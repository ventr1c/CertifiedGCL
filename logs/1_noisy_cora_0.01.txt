Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10498])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0253052711486816 = 1.9393366575241089 + 0.01 * 8.596854209899902
Epoch 0, val loss: 1.9426188468933105
Epoch 10, training loss: 2.0156149864196777 = 1.9296470880508423 + 0.01 * 8.596796035766602
Epoch 10, val loss: 1.933700442314148
Epoch 20, training loss: 2.0037124156951904 = 1.9177465438842773 + 0.01 * 8.596590042114258
Epoch 20, val loss: 1.9224669933319092
Epoch 30, training loss: 1.9871991872787476 = 1.901240348815918 + 0.01 * 8.595888137817383
Epoch 30, val loss: 1.9066671133041382
Epoch 40, training loss: 1.9633395671844482 = 1.877419352531433 + 0.01 * 8.592022895812988
Epoch 40, val loss: 1.8840152025222778
Epoch 50, training loss: 1.9308898448944092 = 1.8452218770980835 + 0.01 * 8.566793441772461
Epoch 50, val loss: 1.8549237251281738
Epoch 60, training loss: 1.895119547843933 = 1.8105827569961548 + 0.01 * 8.453676223754883
Epoch 60, val loss: 1.8261311054229736
Epoch 70, training loss: 1.8610190153121948 = 1.7798068523406982 + 0.01 * 8.121217727661133
Epoch 70, val loss: 1.7988009452819824
Epoch 80, training loss: 1.8209283351898193 = 1.7409141063690186 + 0.01 * 8.001423835754395
Epoch 80, val loss: 1.7602324485778809
Epoch 90, training loss: 1.7645277976989746 = 1.6862361431121826 + 0.01 * 7.829169750213623
Epoch 90, val loss: 1.711073637008667
Epoch 100, training loss: 1.6885366439819336 = 1.6116712093353271 + 0.01 * 7.686547756195068
Epoch 100, val loss: 1.6483696699142456
Epoch 110, training loss: 1.5978024005889893 = 1.52263605594635 + 0.01 * 7.51663875579834
Epoch 110, val loss: 1.5728836059570312
Epoch 120, training loss: 1.5034124851226807 = 1.4294432401657104 + 0.01 * 7.396921157836914
Epoch 120, val loss: 1.4942176342010498
Epoch 130, training loss: 1.4118597507476807 = 1.3387246131896973 + 0.01 * 7.3135175704956055
Epoch 130, val loss: 1.4206843376159668
Epoch 140, training loss: 1.324486255645752 = 1.2520771026611328 + 0.01 * 7.240921497344971
Epoch 140, val loss: 1.35366690158844
Epoch 150, training loss: 1.240635633468628 = 1.1685538291931152 + 0.01 * 7.208179473876953
Epoch 150, val loss: 1.2921881675720215
Epoch 160, training loss: 1.157667875289917 = 1.0857032537460327 + 0.01 * 7.196465015411377
Epoch 160, val loss: 1.2324774265289307
Epoch 170, training loss: 1.0735033750534058 = 1.0015910863876343 + 0.01 * 7.191230773925781
Epoch 170, val loss: 1.172278642654419
Epoch 180, training loss: 0.9889233112335205 = 0.9170668721199036 + 0.01 * 7.185646057128906
Epoch 180, val loss: 1.111910343170166
Epoch 190, training loss: 0.9068150520324707 = 0.8350456357002258 + 0.01 * 7.176942348480225
Epoch 190, val loss: 1.054797649383545
Epoch 200, training loss: 0.8305063247680664 = 0.758857786655426 + 0.01 * 7.16485595703125
Epoch 200, val loss: 1.0045065879821777
Epoch 210, training loss: 0.7618952989578247 = 0.6904242634773254 + 0.01 * 7.147103786468506
Epoch 210, val loss: 0.9634599685668945
Epoch 220, training loss: 0.7013005614280701 = 0.6300537586212158 + 0.01 * 7.124679088592529
Epoch 220, val loss: 0.9324585795402527
Epoch 230, training loss: 0.6481354832649231 = 0.5771071314811707 + 0.01 * 7.102835178375244
Epoch 230, val loss: 0.9106981754302979
Epoch 240, training loss: 0.601159393787384 = 0.5303439497947693 + 0.01 * 7.081546306610107
Epoch 240, val loss: 0.8963288068771362
Epoch 250, training loss: 0.5589563846588135 = 0.48825085163116455 + 0.01 * 7.070553302764893
Epoch 250, val loss: 0.8874285221099854
Epoch 260, training loss: 0.5198667049407959 = 0.44924578070640564 + 0.01 * 7.062091827392578
Epoch 260, val loss: 0.8822435736656189
Epoch 270, training loss: 0.4826018214225769 = 0.4120157063007355 + 0.01 * 7.05861234664917
Epoch 270, val loss: 0.8796244263648987
Epoch 280, training loss: 0.44637951254844666 = 0.3758109509944916 + 0.01 * 7.056856155395508
Epoch 280, val loss: 0.8790496587753296
Epoch 290, training loss: 0.41117745637893677 = 0.34061962366104126 + 0.01 * 7.055783748626709
Epoch 290, val loss: 0.8806506395339966
Epoch 300, training loss: 0.3774765729904175 = 0.30693671107292175 + 0.01 * 7.053987979888916
Epoch 300, val loss: 0.8849661946296692
Epoch 310, training loss: 0.3459567725658417 = 0.2754310071468353 + 0.01 * 7.052575588226318
Epoch 310, val loss: 0.8926108479499817
Epoch 320, training loss: 0.31704598665237427 = 0.24651981890201569 + 0.01 * 7.052616596221924
Epoch 320, val loss: 0.9038755893707275
Epoch 330, training loss: 0.2907932698726654 = 0.22027897834777832 + 0.01 * 7.05142879486084
Epoch 330, val loss: 0.9183719754219055
Epoch 340, training loss: 0.26716673374176025 = 0.1966426521539688 + 0.01 * 7.0524067878723145
Epoch 340, val loss: 0.9353535771369934
Epoch 350, training loss: 0.24594265222549438 = 0.17545919120311737 + 0.01 * 7.048346996307373
Epoch 350, val loss: 0.9542936086654663
Epoch 360, training loss: 0.22699925303459167 = 0.156523659825325 + 0.01 * 7.047560214996338
Epoch 360, val loss: 0.9745596647262573
Epoch 370, training loss: 0.21010786294937134 = 0.13963767886161804 + 0.01 * 7.047019004821777
Epoch 370, val loss: 0.9957410097122192
Epoch 380, training loss: 0.1950434148311615 = 0.12459025532007217 + 0.01 * 7.04531717300415
Epoch 380, val loss: 1.0174850225448608
Epoch 390, training loss: 0.18161550164222717 = 0.11119219660758972 + 0.01 * 7.042331218719482
Epoch 390, val loss: 1.0396063327789307
Epoch 400, training loss: 0.16969801485538483 = 0.0992719978094101 + 0.01 * 7.042601585388184
Epoch 400, val loss: 1.061838984489441
Epoch 410, training loss: 0.15908333659172058 = 0.08868727087974548 + 0.01 * 7.039606094360352
Epoch 410, val loss: 1.0839444398880005
Epoch 420, training loss: 0.14965805411338806 = 0.07930317521095276 + 0.01 * 7.035487651824951
Epoch 420, val loss: 1.105772852897644
Epoch 430, training loss: 0.1413554698228836 = 0.07100339978933334 + 0.01 * 7.035207271575928
Epoch 430, val loss: 1.1272305250167847
Epoch 440, training loss: 0.13395905494689941 = 0.06367788463830948 + 0.01 * 7.028118133544922
Epoch 440, val loss: 1.1482738256454468
Epoch 450, training loss: 0.12745830416679382 = 0.05721026659011841 + 0.01 * 7.024803161621094
Epoch 450, val loss: 1.1688506603240967
Epoch 460, training loss: 0.12169978767633438 = 0.05150613933801651 + 0.01 * 7.019365310668945
Epoch 460, val loss: 1.1888961791992188
Epoch 470, training loss: 0.1166178360581398 = 0.04647882282733917 + 0.01 * 7.013901233673096
Epoch 470, val loss: 1.2083754539489746
Epoch 480, training loss: 0.11214692145586014 = 0.042046308517456055 + 0.01 * 7.010061740875244
Epoch 480, val loss: 1.2272822856903076
Epoch 490, training loss: 0.10825219750404358 = 0.03813822194933891 + 0.01 * 7.011397838592529
Epoch 490, val loss: 1.2456042766571045
Epoch 500, training loss: 0.1046643853187561 = 0.03468848764896393 + 0.01 * 6.997589588165283
Epoch 500, val loss: 1.2633283138275146
Epoch 510, training loss: 0.10171926766633987 = 0.031639717519283295 + 0.01 * 7.007955074310303
Epoch 510, val loss: 1.2804428339004517
Epoch 520, training loss: 0.09874917566776276 = 0.028944818302989006 + 0.01 * 6.980435371398926
Epoch 520, val loss: 1.2968780994415283
Epoch 530, training loss: 0.09647910296916962 = 0.026551077142357826 + 0.01 * 6.99280309677124
Epoch 530, val loss: 1.312811255455017
Epoch 540, training loss: 0.09418244659900665 = 0.02442338690161705 + 0.01 * 6.975906848907471
Epoch 540, val loss: 1.3281240463256836
Epoch 550, training loss: 0.09237450361251831 = 0.022526085376739502 + 0.01 * 6.984842300415039
Epoch 550, val loss: 1.3428936004638672
Epoch 560, training loss: 0.09035225957632065 = 0.0208333358168602 + 0.01 * 6.951892852783203
Epoch 560, val loss: 1.3571330308914185
Epoch 570, training loss: 0.08868701756000519 = 0.01931837387382984 + 0.01 * 6.936864376068115
Epoch 570, val loss: 1.3709051609039307
Epoch 580, training loss: 0.0872776210308075 = 0.017957966774702072 + 0.01 * 6.931966304779053
Epoch 580, val loss: 1.3842368125915527
Epoch 590, training loss: 0.08601057529449463 = 0.0167384035885334 + 0.01 * 6.927217960357666
Epoch 590, val loss: 1.396956205368042
Epoch 600, training loss: 0.0849086344242096 = 0.015643542632460594 + 0.01 * 6.926509380340576
Epoch 600, val loss: 1.4092603921890259
Epoch 610, training loss: 0.08371158689260483 = 0.01465409342199564 + 0.01 * 6.905749320983887
Epoch 610, val loss: 1.4212090969085693
Epoch 620, training loss: 0.08298176527023315 = 0.013755721971392632 + 0.01 * 6.922604084014893
Epoch 620, val loss: 1.4328492879867554
Epoch 630, training loss: 0.08183734863996506 = 0.012940346263349056 + 0.01 * 6.889700412750244
Epoch 630, val loss: 1.4439878463745117
Epoch 640, training loss: 0.081116683781147 = 0.012197927571833134 + 0.01 * 6.891876220703125
Epoch 640, val loss: 1.4547696113586426
Epoch 650, training loss: 0.08038617670536041 = 0.011520331725478172 + 0.01 * 6.886584281921387
Epoch 650, val loss: 1.4652743339538574
Epoch 660, training loss: 0.07973308861255646 = 0.010901890695095062 + 0.01 * 6.883119583129883
Epoch 660, val loss: 1.475450873374939
Epoch 670, training loss: 0.07896702736616135 = 0.010334870778024197 + 0.01 * 6.863215446472168
Epoch 670, val loss: 1.485265851020813
Epoch 680, training loss: 0.07848231494426727 = 0.009813190437853336 + 0.01 * 6.866912364959717
Epoch 680, val loss: 1.494910478591919
Epoch 690, training loss: 0.07814797013998032 = 0.009332164190709591 + 0.01 * 6.881580352783203
Epoch 690, val loss: 1.5041944980621338
Epoch 700, training loss: 0.07736065983772278 = 0.008889567106962204 + 0.01 * 6.847109317779541
Epoch 700, val loss: 1.5131845474243164
Epoch 710, training loss: 0.0769757330417633 = 0.00848090648651123 + 0.01 * 6.849483013153076
Epoch 710, val loss: 1.5220272541046143
Epoch 720, training loss: 0.07656938582658768 = 0.00810175109654665 + 0.01 * 6.8467631340026855
Epoch 720, val loss: 1.5305383205413818
Epoch 730, training loss: 0.07627139240503311 = 0.0077505349181592464 + 0.01 * 6.852086067199707
Epoch 730, val loss: 1.5388816595077515
Epoch 740, training loss: 0.0757136195898056 = 0.007424016948789358 + 0.01 * 6.828959941864014
Epoch 740, val loss: 1.5469534397125244
Epoch 750, training loss: 0.07537419348955154 = 0.007119635120034218 + 0.01 * 6.825456142425537
Epoch 750, val loss: 1.5548635721206665
Epoch 760, training loss: 0.07533852756023407 = 0.006835154257714748 + 0.01 * 6.850337505340576
Epoch 760, val loss: 1.5626763105392456
Epoch 770, training loss: 0.0747467577457428 = 0.006570117548108101 + 0.01 * 6.817664623260498
Epoch 770, val loss: 1.5700387954711914
Epoch 780, training loss: 0.07441647350788116 = 0.006322478409856558 + 0.01 * 6.809399127960205
Epoch 780, val loss: 1.5773152112960815
Epoch 790, training loss: 0.0745137631893158 = 0.006090259179472923 + 0.01 * 6.842350006103516
Epoch 790, val loss: 1.584517240524292
Epoch 800, training loss: 0.0740489810705185 = 0.005872350186109543 + 0.01 * 6.817663192749023
Epoch 800, val loss: 1.5913691520690918
Epoch 810, training loss: 0.07375655323266983 = 0.005667772609740496 + 0.01 * 6.808877944946289
Epoch 810, val loss: 1.5982322692871094
Epoch 820, training loss: 0.07338622957468033 = 0.005474964622408152 + 0.01 * 6.7911272048950195
Epoch 820, val loss: 1.6048245429992676
Epoch 830, training loss: 0.07334284484386444 = 0.005293130874633789 + 0.01 * 6.804971694946289
Epoch 830, val loss: 1.6113007068634033
Epoch 840, training loss: 0.07311400771141052 = 0.0051220678724348545 + 0.01 * 6.7991943359375
Epoch 840, val loss: 1.6175978183746338
Epoch 850, training loss: 0.07293488085269928 = 0.004960625898092985 + 0.01 * 6.797425746917725
Epoch 850, val loss: 1.6238224506378174
Epoch 860, training loss: 0.07268989086151123 = 0.0048080189153552055 + 0.01 * 6.7881879806518555
Epoch 860, val loss: 1.6298354864120483
Epoch 870, training loss: 0.07250028848648071 = 0.004663555417209864 + 0.01 * 6.783673286437988
Epoch 870, val loss: 1.635754942893982
Epoch 880, training loss: 0.07245610654354095 = 0.004526520613580942 + 0.01 * 6.792958736419678
Epoch 880, val loss: 1.6415196657180786
Epoch 890, training loss: 0.07208413630723953 = 0.004396714270114899 + 0.01 * 6.768742084503174
Epoch 890, val loss: 1.6471489667892456
Epoch 900, training loss: 0.07199677079916 = 0.004273357335478067 + 0.01 * 6.772341728210449
Epoch 900, val loss: 1.6526740789413452
Epoch 910, training loss: 0.07189200818538666 = 0.004156266339123249 + 0.01 * 6.773573875427246
Epoch 910, val loss: 1.658079743385315
Epoch 920, training loss: 0.07172421365976334 = 0.004044994246214628 + 0.01 * 6.7679219245910645
Epoch 920, val loss: 1.6633681058883667
Epoch 930, training loss: 0.07160260528326035 = 0.003939146641641855 + 0.01 * 6.766345977783203
Epoch 930, val loss: 1.6686291694641113
Epoch 940, training loss: 0.07143077254295349 = 0.0038382038474082947 + 0.01 * 6.759256839752197
Epoch 940, val loss: 1.6736888885498047
Epoch 950, training loss: 0.07123154401779175 = 0.003741953521966934 + 0.01 * 6.748959064483643
Epoch 950, val loss: 1.6786004304885864
Epoch 960, training loss: 0.07113396376371384 = 0.0036502531729638577 + 0.01 * 6.74837064743042
Epoch 960, val loss: 1.6835640668869019
Epoch 970, training loss: 0.07118310779333115 = 0.0035625589080154896 + 0.01 * 6.762054920196533
Epoch 970, val loss: 1.6883727312088013
Epoch 980, training loss: 0.07106741517782211 = 0.0034787498880177736 + 0.01 * 6.758866786956787
Epoch 980, val loss: 1.692958950996399
Epoch 990, training loss: 0.0709596574306488 = 0.0033988486975431442 + 0.01 * 6.756080627441406
Epoch 990, val loss: 1.6975595951080322
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 2.019807815551758 = 1.9338394403457642 + 0.01 * 8.596846580505371
Epoch 0, val loss: 1.9404140710830688
Epoch 10, training loss: 2.010239839553833 = 1.9242719411849976 + 0.01 * 8.596792221069336
Epoch 10, val loss: 1.9309382438659668
Epoch 20, training loss: 1.9984108209609985 = 1.912445068359375 + 0.01 * 8.596573829650879
Epoch 20, val loss: 1.918736219406128
Epoch 30, training loss: 1.981911301612854 = 1.895953893661499 + 0.01 * 8.595736503601074
Epoch 30, val loss: 1.9014512300491333
Epoch 40, training loss: 1.957993745803833 = 1.8720942735671997 + 0.01 * 8.58995246887207
Epoch 40, val loss: 1.8766363859176636
Epoch 50, training loss: 1.9254199266433716 = 1.8399299383163452 + 0.01 * 8.548996925354004
Epoch 50, val loss: 1.8449381589889526
Epoch 60, training loss: 1.8900806903839111 = 1.8066167831420898 + 0.01 * 8.346390724182129
Epoch 60, val loss: 1.8157025575637817
Epoch 70, training loss: 1.8588169813156128 = 1.7777975797653198 + 0.01 * 8.101940155029297
Epoch 70, val loss: 1.7907918691635132
Epoch 80, training loss: 1.8190975189208984 = 1.7397388219833374 + 0.01 * 7.935863971710205
Epoch 80, val loss: 1.7555196285247803
Epoch 90, training loss: 1.7634509801864624 = 1.6862218379974365 + 0.01 * 7.722919464111328
Epoch 90, val loss: 1.7078197002410889
Epoch 100, training loss: 1.6876616477966309 = 1.6122026443481445 + 0.01 * 7.545902729034424
Epoch 100, val loss: 1.6438363790512085
Epoch 110, training loss: 1.5957010984420776 = 1.5215436220169067 + 0.01 * 7.415748596191406
Epoch 110, val loss: 1.5668398141860962
Epoch 120, training loss: 1.496374487876892 = 1.4228229522705078 + 0.01 * 7.355152130126953
Epoch 120, val loss: 1.4843246936798096
Epoch 130, training loss: 1.3963903188705444 = 1.3234937191009521 + 0.01 * 7.28965950012207
Epoch 130, val loss: 1.403343677520752
Epoch 140, training loss: 1.2991055250167847 = 1.2266664505004883 + 0.01 * 7.243906021118164
Epoch 140, val loss: 1.3265964984893799
Epoch 150, training loss: 1.2073603868484497 = 1.1353223323822021 + 0.01 * 7.203802108764648
Epoch 150, val loss: 1.2571120262145996
Epoch 160, training loss: 1.1234670877456665 = 1.0517096519470215 + 0.01 * 7.175742149353027
Epoch 160, val loss: 1.1966673135757446
Epoch 170, training loss: 1.0472257137298584 = 0.9755821228027344 + 0.01 * 7.164355754852295
Epoch 170, val loss: 1.144592046737671
Epoch 180, training loss: 0.9762883186340332 = 0.904667854309082 + 0.01 * 7.162045955657959
Epoch 180, val loss: 1.098628282546997
Epoch 190, training loss: 0.9080930948257446 = 0.8364928364753723 + 0.01 * 7.160023212432861
Epoch 190, val loss: 1.0563902854919434
Epoch 200, training loss: 0.8410950303077698 = 0.769525945186615 + 0.01 * 7.156910419464111
Epoch 200, val loss: 1.0165317058563232
Epoch 210, training loss: 0.7752532362937927 = 0.7037203907966614 + 0.01 * 7.153286933898926
Epoch 210, val loss: 0.9789678454399109
Epoch 220, training loss: 0.7119237184524536 = 0.6404386758804321 + 0.01 * 7.148502349853516
Epoch 220, val loss: 0.9446196556091309
Epoch 230, training loss: 0.6529643535614014 = 0.5815451145172119 + 0.01 * 7.141927242279053
Epoch 230, val loss: 0.9150598049163818
Epoch 240, training loss: 0.5995188355445862 = 0.528186023235321 + 0.01 * 7.133281230926514
Epoch 240, val loss: 0.891861617565155
Epoch 250, training loss: 0.5513908267021179 = 0.4801841080188751 + 0.01 * 7.120670795440674
Epoch 250, val loss: 0.8754034042358398
Epoch 260, training loss: 0.5076971054077148 = 0.436617374420166 + 0.01 * 7.107972621917725
Epoch 260, val loss: 0.8650797605514526
Epoch 270, training loss: 0.46745961904525757 = 0.39650923013687134 + 0.01 * 7.095037460327148
Epoch 270, val loss: 0.8596965670585632
Epoch 280, training loss: 0.4299861490726471 = 0.35921719670295715 + 0.01 * 7.076894283294678
Epoch 280, val loss: 0.8582242131233215
Epoch 290, training loss: 0.395046204328537 = 0.324357271194458 + 0.01 * 7.068894386291504
Epoch 290, val loss: 0.859957754611969
Epoch 300, training loss: 0.36234617233276367 = 0.29173383116722107 + 0.01 * 7.0612335205078125
Epoch 300, val loss: 0.8642163276672363
Epoch 310, training loss: 0.3317892253398895 = 0.26125046610832214 + 0.01 * 7.053876876831055
Epoch 310, val loss: 0.8707736730575562
Epoch 320, training loss: 0.30334702134132385 = 0.23289178311824799 + 0.01 * 7.045524597167969
Epoch 320, val loss: 0.8793350458145142
Epoch 330, training loss: 0.27713513374328613 = 0.20672953128814697 + 0.01 * 7.040560245513916
Epoch 330, val loss: 0.8897367715835571
Epoch 340, training loss: 0.25325867533683777 = 0.1828598827123642 + 0.01 * 7.039878845214844
Epoch 340, val loss: 0.9018396139144897
Epoch 350, training loss: 0.23165932297706604 = 0.16132000088691711 + 0.01 * 7.033932685852051
Epoch 350, val loss: 0.9156501293182373
Epoch 360, training loss: 0.21244697272777557 = 0.1420738399028778 + 0.01 * 7.037313461303711
Epoch 360, val loss: 0.9310283660888672
Epoch 370, training loss: 0.19531197845935822 = 0.12502527236938477 + 0.01 * 7.028670787811279
Epoch 370, val loss: 0.9477604627609253
Epoch 380, training loss: 0.18021133542060852 = 0.1100105419754982 + 0.01 * 7.020078659057617
Epoch 380, val loss: 0.9655923247337341
Epoch 390, training loss: 0.16707497835159302 = 0.09686566144227982 + 0.01 * 7.020931243896484
Epoch 390, val loss: 0.9843049049377441
Epoch 400, training loss: 0.15550899505615234 = 0.08541909605264664 + 0.01 * 7.008990287780762
Epoch 400, val loss: 1.0034852027893066
Epoch 410, training loss: 0.14550334215164185 = 0.07547275722026825 + 0.01 * 7.003057956695557
Epoch 410, val loss: 1.022950291633606
Epoch 420, training loss: 0.13698875904083252 = 0.06683969497680664 + 0.01 * 7.014906406402588
Epoch 420, val loss: 1.0425525903701782
Epoch 430, training loss: 0.12926295399665833 = 0.05936301499605179 + 0.01 * 6.989995002746582
Epoch 430, val loss: 1.0619971752166748
Epoch 440, training loss: 0.12274983525276184 = 0.05287851020693779 + 0.01 * 6.987133026123047
Epoch 440, val loss: 1.0812177658081055
Epoch 450, training loss: 0.11700189113616943 = 0.04725252464413643 + 0.01 * 6.974937438964844
Epoch 450, val loss: 1.1001399755477905
Epoch 460, training loss: 0.11213450133800507 = 0.0423710010945797 + 0.01 * 6.9763503074646
Epoch 460, val loss: 1.1186401844024658
Epoch 470, training loss: 0.10773652791976929 = 0.03813321515917778 + 0.01 * 6.960330963134766
Epoch 470, val loss: 1.1367157697677612
Epoch 480, training loss: 0.10434184968471527 = 0.03444262221455574 + 0.01 * 6.989922523498535
Epoch 480, val loss: 1.1543093919754028
Epoch 490, training loss: 0.10076936334371567 = 0.0312329214066267 + 0.01 * 6.953644275665283
Epoch 490, val loss: 1.1712062358856201
Epoch 500, training loss: 0.09786750376224518 = 0.02842775546014309 + 0.01 * 6.94397497177124
Epoch 500, val loss: 1.187549114227295
Epoch 510, training loss: 0.0953626036643982 = 0.025965005159378052 + 0.01 * 6.939760208129883
Epoch 510, val loss: 1.2034331560134888
Epoch 520, training loss: 0.09328103810548782 = 0.023796280845999718 + 0.01 * 6.9484758377075195
Epoch 520, val loss: 1.2187414169311523
Epoch 530, training loss: 0.09125784039497375 = 0.021883364766836166 + 0.01 * 6.9374470710754395
Epoch 530, val loss: 1.233433485031128
Epoch 540, training loss: 0.08941228687763214 = 0.02018810249865055 + 0.01 * 6.922418594360352
Epoch 540, val loss: 1.2476603984832764
Epoch 550, training loss: 0.08779391646385193 = 0.018678639084100723 + 0.01 * 6.91152811050415
Epoch 550, val loss: 1.261361837387085
Epoch 560, training loss: 0.08641335368156433 = 0.017327098175883293 + 0.01 * 6.908625602722168
Epoch 560, val loss: 1.2746034860610962
Epoch 570, training loss: 0.08515298366546631 = 0.016114305704832077 + 0.01 * 6.903868198394775
Epoch 570, val loss: 1.2873657941818237
Epoch 580, training loss: 0.08405949920415878 = 0.015022575855255127 + 0.01 * 6.903692245483398
Epoch 580, val loss: 1.2996824979782104
Epoch 590, training loss: 0.08314524590969086 = 0.014037002809345722 + 0.01 * 6.910823822021484
Epoch 590, val loss: 1.3115074634552002
Epoch 600, training loss: 0.08203451335430145 = 0.013145554810762405 + 0.01 * 6.8888959884643555
Epoch 600, val loss: 1.3230317831039429
Epoch 610, training loss: 0.0811774730682373 = 0.012336499989032745 + 0.01 * 6.884097099304199
Epoch 610, val loss: 1.3341072797775269
Epoch 620, training loss: 0.08044107258319855 = 0.011600131168961525 + 0.01 * 6.884094715118408
Epoch 620, val loss: 1.3448253870010376
Epoch 630, training loss: 0.07966100424528122 = 0.010928702540695667 + 0.01 * 6.87322998046875
Epoch 630, val loss: 1.3552254438400269
Epoch 640, training loss: 0.07915975153446198 = 0.01031606737524271 + 0.01 * 6.884368896484375
Epoch 640, val loss: 1.3652501106262207
Epoch 650, training loss: 0.07844888418912888 = 0.009757391177117825 + 0.01 * 6.869149208068848
Epoch 650, val loss: 1.374893069267273
Epoch 660, training loss: 0.0778329074382782 = 0.009244926273822784 + 0.01 * 6.858798027038574
Epoch 660, val loss: 1.3842741250991821
Epoch 670, training loss: 0.07749509811401367 = 0.008773201145231724 + 0.01 * 6.872189998626709
Epoch 670, val loss: 1.3933755159378052
Epoch 680, training loss: 0.07687549293041229 = 0.00833863578736782 + 0.01 * 6.8536858558654785
Epoch 680, val loss: 1.4021656513214111
Epoch 690, training loss: 0.07656583189964294 = 0.007937461137771606 + 0.01 * 6.862837314605713
Epoch 690, val loss: 1.4106777906417847
Epoch 700, training loss: 0.07601363956928253 = 0.007567044347524643 + 0.01 * 6.84466028213501
Epoch 700, val loss: 1.4189273118972778
Epoch 710, training loss: 0.07561196386814117 = 0.007223560009151697 + 0.01 * 6.838840484619141
Epoch 710, val loss: 1.4269323348999023
Epoch 720, training loss: 0.07543475925922394 = 0.006904692854732275 + 0.01 * 6.853006839752197
Epoch 720, val loss: 1.4347323179244995
Epoch 730, training loss: 0.07484431564807892 = 0.006608691066503525 + 0.01 * 6.823562145233154
Epoch 730, val loss: 1.4423187971115112
Epoch 740, training loss: 0.07467588782310486 = 0.006333264987915754 + 0.01 * 6.834262847900391
Epoch 740, val loss: 1.4496742486953735
Epoch 750, training loss: 0.07433465868234634 = 0.006076819729059935 + 0.01 * 6.825784206390381
Epoch 750, val loss: 1.4567503929138184
Epoch 760, training loss: 0.07405103743076324 = 0.005837297532707453 + 0.01 * 6.82137393951416
Epoch 760, val loss: 1.4636205434799194
Epoch 770, training loss: 0.0737476572394371 = 0.005613468121737242 + 0.01 * 6.813418865203857
Epoch 770, val loss: 1.470285177230835
Epoch 780, training loss: 0.07349374145269394 = 0.005403317045420408 + 0.01 * 6.809042453765869
Epoch 780, val loss: 1.4768199920654297
Epoch 790, training loss: 0.073309026658535 = 0.005206521134823561 + 0.01 * 6.810251235961914
Epoch 790, val loss: 1.4831223487854004
Epoch 800, training loss: 0.0732380822300911 = 0.005021200515329838 + 0.01 * 6.821688652038574
Epoch 800, val loss: 1.4892706871032715
Epoch 810, training loss: 0.0731394961476326 = 0.004847418051213026 + 0.01 * 6.8292083740234375
Epoch 810, val loss: 1.4952627420425415
Epoch 820, training loss: 0.07274925708770752 = 0.004683793522417545 + 0.01 * 6.806546688079834
Epoch 820, val loss: 1.5009864568710327
Epoch 830, training loss: 0.07237329334020615 = 0.004529763478785753 + 0.01 * 6.784352779388428
Epoch 830, val loss: 1.5065674781799316
Epoch 840, training loss: 0.07214470207691193 = 0.004384280182421207 + 0.01 * 6.776042461395264
Epoch 840, val loss: 1.5120813846588135
Epoch 850, training loss: 0.07223223894834518 = 0.0042467545717954636 + 0.01 * 6.798548221588135
Epoch 850, val loss: 1.5173962116241455
Epoch 860, training loss: 0.07178635895252228 = 0.004116677213460207 + 0.01 * 6.766968727111816
Epoch 860, val loss: 1.522624135017395
Epoch 870, training loss: 0.07185274362564087 = 0.0039934334345161915 + 0.01 * 6.785930633544922
Epoch 870, val loss: 1.5276137590408325
Epoch 880, training loss: 0.07147451490163803 = 0.0038769186940044165 + 0.01 * 6.759759902954102
Epoch 880, val loss: 1.532578468322754
Epoch 890, training loss: 0.07158298045396805 = 0.003765867790207267 + 0.01 * 6.781711578369141
Epoch 890, val loss: 1.537385106086731
Epoch 900, training loss: 0.07125329226255417 = 0.0036606721114367247 + 0.01 * 6.7592620849609375
Epoch 900, val loss: 1.542065143585205
Epoch 910, training loss: 0.07110182195901871 = 0.003560379147529602 + 0.01 * 6.754144191741943
Epoch 910, val loss: 1.546651840209961
Epoch 920, training loss: 0.07087747007608414 = 0.0034648263826966286 + 0.01 * 6.741264820098877
Epoch 920, val loss: 1.551061749458313
Epoch 930, training loss: 0.07092837244272232 = 0.0033738757483661175 + 0.01 * 6.7554497718811035
Epoch 930, val loss: 1.5553818941116333
Epoch 940, training loss: 0.07072068005800247 = 0.0032872778829187155 + 0.01 * 6.743340492248535
Epoch 940, val loss: 1.5596084594726562
Epoch 950, training loss: 0.07079039514064789 = 0.0032045573461800814 + 0.01 * 6.758584022521973
Epoch 950, val loss: 1.5636718273162842
Epoch 960, training loss: 0.07065457105636597 = 0.0031258189119398594 + 0.01 * 6.752875328063965
Epoch 960, val loss: 1.5677530765533447
Epoch 970, training loss: 0.07039296627044678 = 0.003050456754863262 + 0.01 * 6.734251499176025
Epoch 970, val loss: 1.5716798305511475
Epoch 980, training loss: 0.07045160233974457 = 0.0029784091748297215 + 0.01 * 6.74731969833374
Epoch 980, val loss: 1.5754945278167725
Epoch 990, training loss: 0.07015111297369003 = 0.0029096670914441347 + 0.01 * 6.72414493560791
Epoch 990, val loss: 1.5792655944824219
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 2.0399258136749268 = 1.953957200050354 + 0.01 * 8.596856117248535
Epoch 0, val loss: 1.9535911083221436
Epoch 10, training loss: 2.0288586616516113 = 1.9428905248641968 + 0.01 * 8.596806526184082
Epoch 10, val loss: 1.942077398300171
Epoch 20, training loss: 2.015702486038208 = 1.9297364950180054 + 0.01 * 8.596603393554688
Epoch 20, val loss: 1.9282468557357788
Epoch 30, training loss: 1.9979393482208252 = 1.9119800329208374 + 0.01 * 8.59593391418457
Epoch 30, val loss: 1.9095282554626465
Epoch 40, training loss: 1.9725767374038696 = 1.8866535425186157 + 0.01 * 8.592316627502441
Epoch 40, val loss: 1.8830899000167847
Epoch 50, training loss: 1.9378803968429565 = 1.8522095680236816 + 0.01 * 8.567079544067383
Epoch 50, val loss: 1.8485685586929321
Epoch 60, training loss: 1.8995901346206665 = 1.8150217533111572 + 0.01 * 8.456839561462402
Epoch 60, val loss: 1.8151851892471313
Epoch 70, training loss: 1.8683608770370483 = 1.7868889570236206 + 0.01 * 8.147195816040039
Epoch 70, val loss: 1.7939140796661377
Epoch 80, training loss: 1.8366971015930176 = 1.7561348676681519 + 0.01 * 8.056227684020996
Epoch 80, val loss: 1.7684686183929443
Epoch 90, training loss: 1.7922664880752563 = 1.713897705078125 + 0.01 * 7.836883544921875
Epoch 90, val loss: 1.7319716215133667
Epoch 100, training loss: 1.7305854558944702 = 1.655093789100647 + 0.01 * 7.54917049407959
Epoch 100, val loss: 1.6819493770599365
Epoch 110, training loss: 1.6503472328186035 = 1.5767152309417725 + 0.01 * 7.363198757171631
Epoch 110, val loss: 1.6170037984848022
Epoch 120, training loss: 1.556412696838379 = 1.4835976362228394 + 0.01 * 7.281503200531006
Epoch 120, val loss: 1.5407947301864624
Epoch 130, training loss: 1.4558531045913696 = 1.383671760559082 + 0.01 * 7.218139171600342
Epoch 130, val loss: 1.4601783752441406
Epoch 140, training loss: 1.3532708883285522 = 1.2815415859222412 + 0.01 * 7.1729278564453125
Epoch 140, val loss: 1.379747748374939
Epoch 150, training loss: 1.2504032850265503 = 1.1788959503173828 + 0.01 * 7.150731086730957
Epoch 150, val loss: 1.2998466491699219
Epoch 160, training loss: 1.1487764120101929 = 1.0774147510528564 + 0.01 * 7.136165618896484
Epoch 160, val loss: 1.2223268747329712
Epoch 170, training loss: 1.050749659538269 = 0.979537308216095 + 0.01 * 7.121240615844727
Epoch 170, val loss: 1.1498234272003174
Epoch 180, training loss: 0.95779949426651 = 0.8867512941360474 + 0.01 * 7.104819297790527
Epoch 180, val loss: 1.0830895900726318
Epoch 190, training loss: 0.8703138828277588 = 0.7994263172149658 + 0.01 * 7.088758945465088
Epoch 190, val loss: 1.0213017463684082
Epoch 200, training loss: 0.7885473370552063 = 0.7177776098251343 + 0.01 * 7.076973915100098
Epoch 200, val loss: 0.9642443656921387
Epoch 210, training loss: 0.7131359577178955 = 0.642432689666748 + 0.01 * 7.070329666137695
Epoch 210, val loss: 0.9125049710273743
Epoch 220, training loss: 0.644454836845398 = 0.5737867951393127 + 0.01 * 7.06680154800415
Epoch 220, val loss: 0.86745685338974
Epoch 230, training loss: 0.5822585821151733 = 0.5116092562675476 + 0.01 * 7.064934253692627
Epoch 230, val loss: 0.8296748995780945
Epoch 240, training loss: 0.5256977081298828 = 0.45506733655929565 + 0.01 * 7.063034534454346
Epoch 240, val loss: 0.799197256565094
Epoch 250, training loss: 0.47390854358673096 = 0.40329620242118835 + 0.01 * 7.061232566833496
Epoch 250, val loss: 0.7750352621078491
Epoch 260, training loss: 0.4262714385986328 = 0.3556733727455139 + 0.01 * 7.059806823730469
Epoch 260, val loss: 0.756366491317749
Epoch 270, training loss: 0.3824075758457184 = 0.31182199716567993 + 0.01 * 7.058557510375977
Epoch 270, val loss: 0.7421506643295288
Epoch 280, training loss: 0.342241495847702 = 0.27166637778282166 + 0.01 * 7.057512283325195
Epoch 280, val loss: 0.7319526076316833
Epoch 290, training loss: 0.30594706535339355 = 0.23537787795066833 + 0.01 * 7.056918144226074
Epoch 290, val loss: 0.7252001762390137
Epoch 300, training loss: 0.27372244000434875 = 0.2031475007534027 + 0.01 * 7.057493686676025
Epoch 300, val loss: 0.7216726541519165
Epoch 310, training loss: 0.24561764299869537 = 0.17505274713039398 + 0.01 * 7.056489944458008
Epoch 310, val loss: 0.721333384513855
Epoch 320, training loss: 0.22149652242660522 = 0.15092742443084717 + 0.01 * 7.056910037994385
Epoch 320, val loss: 0.7239810228347778
Epoch 330, training loss: 0.20094573497772217 = 0.13037826120853424 + 0.01 * 7.056746482849121
Epoch 330, val loss: 0.729200541973114
Epoch 340, training loss: 0.18351194262504578 = 0.11293117702007294 + 0.01 * 7.058077335357666
Epoch 340, val loss: 0.7366580963134766
Epoch 350, training loss: 0.16868427395820618 = 0.09811746329069138 + 0.01 * 7.056680679321289
Epoch 350, val loss: 0.7459331154823303
Epoch 360, training loss: 0.15607471764087677 = 0.08550941199064255 + 0.01 * 7.056530952453613
Epoch 360, val loss: 0.7566711902618408
Epoch 370, training loss: 0.14529940485954285 = 0.07473869621753693 + 0.01 * 7.056070804595947
Epoch 370, val loss: 0.7684783935546875
Epoch 380, training loss: 0.136090487241745 = 0.06553119421005249 + 0.01 * 7.055929183959961
Epoch 380, val loss: 0.7811543941497803
Epoch 390, training loss: 0.12821334600448608 = 0.05766157805919647 + 0.01 * 7.055177688598633
Epoch 390, val loss: 0.7945033311843872
Epoch 400, training loss: 0.1214824765920639 = 0.05093706399202347 + 0.01 * 7.05454158782959
Epoch 400, val loss: 0.8082817792892456
Epoch 410, training loss: 0.11572380363941193 = 0.045189280062913895 + 0.01 * 7.053452491760254
Epoch 410, val loss: 0.8223225474357605
Epoch 420, training loss: 0.11080040037631989 = 0.040272656828165054 + 0.01 * 7.052774429321289
Epoch 420, val loss: 0.8364193439483643
Epoch 430, training loss: 0.10657165944576263 = 0.036058422178030014 + 0.01 * 7.051323413848877
Epoch 430, val loss: 0.8504654765129089
Epoch 440, training loss: 0.10293497145175934 = 0.03243483975529671 + 0.01 * 7.050013542175293
Epoch 440, val loss: 0.8643490672111511
Epoch 450, training loss: 0.09979140013456345 = 0.029307842254638672 + 0.01 * 7.048356056213379
Epoch 450, val loss: 0.8779858946800232
Epoch 460, training loss: 0.09707541763782501 = 0.02659793011844158 + 0.01 * 7.047749042510986
Epoch 460, val loss: 0.8913556337356567
Epoch 470, training loss: 0.09470011293888092 = 0.02423945628106594 + 0.01 * 7.046065807342529
Epoch 470, val loss: 0.9043920636177063
Epoch 480, training loss: 0.09261234104633331 = 0.022177787497639656 + 0.01 * 7.043455600738525
Epoch 480, val loss: 0.9171078205108643
Epoch 490, training loss: 0.09078413248062134 = 0.020367251709103584 + 0.01 * 7.041688442230225
Epoch 490, val loss: 0.9294602274894714
Epoch 500, training loss: 0.08916646242141724 = 0.018770182505249977 + 0.01 * 7.039628028869629
Epoch 500, val loss: 0.9414846897125244
Epoch 510, training loss: 0.08775811642408371 = 0.017355695366859436 + 0.01 * 7.0402421951293945
Epoch 510, val loss: 0.9531437754631042
Epoch 520, training loss: 0.08646247535943985 = 0.016098102554678917 + 0.01 * 7.036437034606934
Epoch 520, val loss: 0.9644864797592163
Epoch 530, training loss: 0.08530686795711517 = 0.014975357800722122 + 0.01 * 7.033151626586914
Epoch 530, val loss: 0.9754871726036072
Epoch 540, training loss: 0.08427833020687103 = 0.013969002291560173 + 0.01 * 7.030932426452637
Epoch 540, val loss: 0.9861508011817932
Epoch 550, training loss: 0.08334865421056747 = 0.013063793070614338 + 0.01 * 7.028486251831055
Epoch 550, val loss: 0.9965149760246277
Epoch 560, training loss: 0.0825406163930893 = 0.012246910482645035 + 0.01 * 7.029370307922363
Epoch 560, val loss: 1.006581425666809
Epoch 570, training loss: 0.08175618946552277 = 0.011507625691592693 + 0.01 * 7.0248565673828125
Epoch 570, val loss: 1.0163445472717285
Epoch 580, training loss: 0.08106672763824463 = 0.010836316272616386 + 0.01 * 7.023040771484375
Epoch 580, val loss: 1.0258256196975708
Epoch 590, training loss: 0.08041226118803024 = 0.010225119069218636 + 0.01 * 7.018714427947998
Epoch 590, val loss: 1.03504478931427
Epoch 600, training loss: 0.07981708645820618 = 0.009667007252573967 + 0.01 * 7.015007972717285
Epoch 600, val loss: 1.0440047979354858
Epoch 610, training loss: 0.07934270799160004 = 0.009156065993010998 + 0.01 * 7.018664360046387
Epoch 610, val loss: 1.05271577835083
Epoch 620, training loss: 0.0788084864616394 = 0.008687442168593407 + 0.01 * 7.012104511260986
Epoch 620, val loss: 1.0611947774887085
Epoch 630, training loss: 0.07831065356731415 = 0.008256419561803341 + 0.01 * 7.005423545837402
Epoch 630, val loss: 1.0694632530212402
Epoch 640, training loss: 0.07790765166282654 = 0.007858894765377045 + 0.01 * 7.004875659942627
Epoch 640, val loss: 1.077494740486145
Epoch 650, training loss: 0.07755526900291443 = 0.007491666823625565 + 0.01 * 7.00636100769043
Epoch 650, val loss: 1.0853605270385742
Epoch 660, training loss: 0.07720997929573059 = 0.007151661906391382 + 0.01 * 7.005831718444824
Epoch 660, val loss: 1.0929889678955078
Epoch 670, training loss: 0.07676699757575989 = 0.006836923770606518 + 0.01 * 6.993008136749268
Epoch 670, val loss: 1.1003973484039307
Epoch 680, training loss: 0.07643384486436844 = 0.006544628646224737 + 0.01 * 6.988922119140625
Epoch 680, val loss: 1.1076476573944092
Epoch 690, training loss: 0.07610398530960083 = 0.0062724268063902855 + 0.01 * 6.983155727386475
Epoch 690, val loss: 1.1147221326828003
Epoch 700, training loss: 0.07579714804887772 = 0.006018400192260742 + 0.01 * 6.977875232696533
Epoch 700, val loss: 1.1216506958007812
Epoch 710, training loss: 0.07594484090805054 = 0.00578179769217968 + 0.01 * 7.0163044929504395
Epoch 710, val loss: 1.1283684968948364
Epoch 720, training loss: 0.07541710883378983 = 0.005562422797083855 + 0.01 * 6.98546838760376
Epoch 720, val loss: 1.1347285509109497
Epoch 730, training loss: 0.07511348277330399 = 0.0053576393984258175 + 0.01 * 6.975584506988525
Epoch 730, val loss: 1.1410290002822876
Epoch 740, training loss: 0.07483408600091934 = 0.0051652523688972 + 0.01 * 6.966884136199951
Epoch 740, val loss: 1.1472010612487793
Epoch 750, training loss: 0.07458082586526871 = 0.004983973223716021 + 0.01 * 6.959685325622559
Epoch 750, val loss: 1.1532560586929321
Epoch 760, training loss: 0.0743485763669014 = 0.004813142120838165 + 0.01 * 6.953543663024902
Epoch 760, val loss: 1.1592004299163818
Epoch 770, training loss: 0.07415599375963211 = 0.004651954397559166 + 0.01 * 6.950404167175293
Epoch 770, val loss: 1.1650406122207642
Epoch 780, training loss: 0.07413320243358612 = 0.00450036209076643 + 0.01 * 6.963284492492676
Epoch 780, val loss: 1.17069411277771
Epoch 790, training loss: 0.07383783161640167 = 0.004357501864433289 + 0.01 * 6.948032855987549
Epoch 790, val loss: 1.1761908531188965
Epoch 800, training loss: 0.07358259707689285 = 0.004222500137984753 + 0.01 * 6.936009883880615
Epoch 800, val loss: 1.181575059890747
Epoch 810, training loss: 0.07362150400876999 = 0.004095079377293587 + 0.01 * 6.952642917633057
Epoch 810, val loss: 1.186798095703125
Epoch 820, training loss: 0.07334104180335999 = 0.003975061234086752 + 0.01 * 6.936598300933838
Epoch 820, val loss: 1.1918833255767822
Epoch 830, training loss: 0.07311220467090607 = 0.003861479228362441 + 0.01 * 6.92507266998291
Epoch 830, val loss: 1.1968234777450562
Epoch 840, training loss: 0.07295097410678864 = 0.0037535603623837233 + 0.01 * 6.919742107391357
Epoch 840, val loss: 1.201707363128662
Epoch 850, training loss: 0.07286450266838074 = 0.0036511137150228024 + 0.01 * 6.921339511871338
Epoch 850, val loss: 1.2064580917358398
Epoch 860, training loss: 0.07279867678880692 = 0.0035540920216590166 + 0.01 * 6.9244585037231445
Epoch 860, val loss: 1.2110601663589478
Epoch 870, training loss: 0.07250267267227173 = 0.003462084336206317 + 0.01 * 6.904058933258057
Epoch 870, val loss: 1.2156163454055786
Epoch 880, training loss: 0.07292412221431732 = 0.0033743255771696568 + 0.01 * 6.95497989654541
Epoch 880, val loss: 1.220012903213501
Epoch 890, training loss: 0.07231578975915909 = 0.0032910986337810755 + 0.01 * 6.902469158172607
Epoch 890, val loss: 1.2242701053619385
Epoch 900, training loss: 0.0722990557551384 = 0.0032118933741003275 + 0.01 * 6.908716678619385
Epoch 900, val loss: 1.228542447090149
Epoch 910, training loss: 0.07207557559013367 = 0.0031361181754618883 + 0.01 * 6.893945693969727
Epoch 910, val loss: 1.2326475381851196
Epoch 920, training loss: 0.07200313359498978 = 0.0030640687327831984 + 0.01 * 6.893907070159912
Epoch 920, val loss: 1.2366974353790283
Epoch 930, training loss: 0.0721982941031456 = 0.002995141549035907 + 0.01 * 6.920315742492676
Epoch 930, val loss: 1.2406997680664062
Epoch 940, training loss: 0.07178577035665512 = 0.00292937271296978 + 0.01 * 6.885639667510986
Epoch 940, val loss: 1.2444833517074585
Epoch 950, training loss: 0.07221560925245285 = 0.002866290742531419 + 0.01 * 6.934932231903076
Epoch 950, val loss: 1.2483408451080322
Epoch 960, training loss: 0.07162687927484512 = 0.0028063480276614428 + 0.01 * 6.882053375244141
Epoch 960, val loss: 1.251923680305481
Epoch 970, training loss: 0.07141681760549545 = 0.0027489804197102785 + 0.01 * 6.866783618927002
Epoch 970, val loss: 1.255603551864624
Epoch 980, training loss: 0.07138937711715698 = 0.002693762071430683 + 0.01 * 6.869561672210693
Epoch 980, val loss: 1.2591520547866821
Epoch 990, training loss: 0.07125844061374664 = 0.0026408785488456488 + 0.01 * 6.861756324768066
Epoch 990, val loss: 1.262573003768921
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8070637849235636
The final CL Acc:0.77654, 0.01429, The final GNN Acc:0.80601, 0.00086
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13234])
remove edge: torch.Size([2, 7886])
updated graph: torch.Size([2, 10564])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0317890644073486 = 1.945820689201355 + 0.01 * 8.596840858459473
Epoch 0, val loss: 1.9525517225265503
Epoch 10, training loss: 2.022401809692383 = 1.9364339113235474 + 0.01 * 8.596789360046387
Epoch 10, val loss: 1.9435861110687256
Epoch 20, training loss: 2.011293888092041 = 1.9253283739089966 + 0.01 * 8.59654426574707
Epoch 20, val loss: 1.9324091672897339
Epoch 30, training loss: 1.996270775794983 = 1.9103142023086548 + 0.01 * 8.595661163330078
Epoch 30, val loss: 1.9166254997253418
Epoch 40, training loss: 1.9745875597000122 = 1.8886840343475342 + 0.01 * 8.590347290039062
Epoch 40, val loss: 1.8933120965957642
Epoch 50, training loss: 1.9429734945297241 = 1.857431411743164 + 0.01 * 8.554208755493164
Epoch 50, val loss: 1.8599560260772705
Epoch 60, training loss: 1.9005870819091797 = 1.8166043758392334 + 0.01 * 8.398269653320312
Epoch 60, val loss: 1.8186428546905518
Epoch 70, training loss: 1.854563593864441 = 1.773732304573059 + 0.01 * 8.08313274383545
Epoch 70, val loss: 1.779693365097046
Epoch 80, training loss: 1.8069114685058594 = 1.7283482551574707 + 0.01 * 7.856322765350342
Epoch 80, val loss: 1.7412744760513306
Epoch 90, training loss: 1.7443238496780396 = 1.6688262224197388 + 0.01 * 7.549759864807129
Epoch 90, val loss: 1.6893863677978516
Epoch 100, training loss: 1.663169264793396 = 1.5896263122558594 + 0.01 * 7.354297161102295
Epoch 100, val loss: 1.6191840171813965
Epoch 110, training loss: 1.5625205039978027 = 1.4898580312728882 + 0.01 * 7.266251087188721
Epoch 110, val loss: 1.5332701206207275
Epoch 120, training loss: 1.4484628438949585 = 1.3763138055801392 + 0.01 * 7.214905738830566
Epoch 120, val loss: 1.4371392726898193
Epoch 130, training loss: 1.3324377536773682 = 1.2606420516967773 + 0.01 * 7.1795735359191895
Epoch 130, val loss: 1.340807557106018
Epoch 140, training loss: 1.2224531173706055 = 1.1510134935379028 + 0.01 * 7.14396858215332
Epoch 140, val loss: 1.2493784427642822
Epoch 150, training loss: 1.120835304260254 = 1.049729585647583 + 0.01 * 7.110575199127197
Epoch 150, val loss: 1.165124535560608
Epoch 160, training loss: 1.0265181064605713 = 0.9556692838668823 + 0.01 * 7.084885120391846
Epoch 160, val loss: 1.0867464542388916
Epoch 170, training loss: 0.9375299215316772 = 0.8668674230575562 + 0.01 * 7.066251277923584
Epoch 170, val loss: 1.01277494430542
Epoch 180, training loss: 0.8539320826530457 = 0.7834275960922241 + 0.01 * 7.050449371337891
Epoch 180, val loss: 0.9439047574996948
Epoch 190, training loss: 0.7766655683517456 = 0.7062764167785645 + 0.01 * 7.03891658782959
Epoch 190, val loss: 0.8816767930984497
Epoch 200, training loss: 0.7058659195899963 = 0.6355917453765869 + 0.01 * 7.0274176597595215
Epoch 200, val loss: 0.8268455266952515
Epoch 210, training loss: 0.6405973434448242 = 0.5704095959663391 + 0.01 * 7.018777370452881
Epoch 210, val loss: 0.7793733477592468
Epoch 220, training loss: 0.5795623064041138 = 0.5094388723373413 + 0.01 * 7.0123467445373535
Epoch 220, val loss: 0.7375519871711731
Epoch 230, training loss: 0.5219093561172485 = 0.45182594656944275 + 0.01 * 7.008340358734131
Epoch 230, val loss: 0.6995267868041992
Epoch 240, training loss: 0.4674871563911438 = 0.39744988083839417 + 0.01 * 7.003726005554199
Epoch 240, val loss: 0.6647734642028809
Epoch 250, training loss: 0.4167720377445221 = 0.34677496552467346 + 0.01 * 6.9997076988220215
Epoch 250, val loss: 0.6340240836143494
Epoch 260, training loss: 0.3704398572444916 = 0.3004910945892334 + 0.01 * 6.994876384735107
Epoch 260, val loss: 0.6078380942344666
Epoch 270, training loss: 0.32899147272109985 = 0.25907617807388306 + 0.01 * 6.991530895233154
Epoch 270, val loss: 0.5863966345787048
Epoch 280, training loss: 0.2925851345062256 = 0.22273413836956024 + 0.01 * 6.985098838806152
Epoch 280, val loss: 0.569471538066864
Epoch 290, training loss: 0.26120397448539734 = 0.19141794741153717 + 0.01 * 6.978601932525635
Epoch 290, val loss: 0.5568150281906128
Epoch 300, training loss: 0.23460006713867188 = 0.16484537720680237 + 0.01 * 6.97546911239624
Epoch 300, val loss: 0.5482117533683777
Epoch 310, training loss: 0.2121824473142624 = 0.1425136774778366 + 0.01 * 6.966876983642578
Epoch 310, val loss: 0.5431650876998901
Epoch 320, training loss: 0.19341176748275757 = 0.12380106002092361 + 0.01 * 6.961071491241455
Epoch 320, val loss: 0.541137158870697
Epoch 330, training loss: 0.17767265439033508 = 0.10808872431516647 + 0.01 * 6.9583940505981445
Epoch 330, val loss: 0.5416845679283142
Epoch 340, training loss: 0.16433504223823547 = 0.09483113139867783 + 0.01 * 6.9503912925720215
Epoch 340, val loss: 0.5443601012229919
Epoch 350, training loss: 0.1530519723892212 = 0.08357542753219604 + 0.01 * 6.947654724121094
Epoch 350, val loss: 0.548717737197876
Epoch 360, training loss: 0.143516406416893 = 0.07396958023309708 + 0.01 * 6.95468282699585
Epoch 360, val loss: 0.5544722080230713
Epoch 370, training loss: 0.1351815015077591 = 0.06573791801929474 + 0.01 * 6.9443583488464355
Epoch 370, val loss: 0.5612595677375793
Epoch 380, training loss: 0.12803493440151215 = 0.05864526703953743 + 0.01 * 6.938966751098633
Epoch 380, val loss: 0.5688492655754089
Epoch 390, training loss: 0.12185870110988617 = 0.052508238703012466 + 0.01 * 6.935046195983887
Epoch 390, val loss: 0.5770221948623657
Epoch 400, training loss: 0.1164996474981308 = 0.047177817672491074 + 0.01 * 6.932183742523193
Epoch 400, val loss: 0.5856404900550842
Epoch 410, training loss: 0.1119156926870346 = 0.04253506287932396 + 0.01 * 6.938063144683838
Epoch 410, val loss: 0.5945545434951782
Epoch 420, training loss: 0.10775236040353775 = 0.03848246484994888 + 0.01 * 6.926990032196045
Epoch 420, val loss: 0.6036259531974792
Epoch 430, training loss: 0.10418900847434998 = 0.03493304178118706 + 0.01 * 6.925596237182617
Epoch 430, val loss: 0.6127781271934509
Epoch 440, training loss: 0.10103818774223328 = 0.031814057379961014 + 0.01 * 6.9224138259887695
Epoch 440, val loss: 0.6219592094421387
Epoch 450, training loss: 0.09825820475816727 = 0.029065050184726715 + 0.01 * 6.919315338134766
Epoch 450, val loss: 0.6310717463493347
Epoch 460, training loss: 0.0958021879196167 = 0.02663532830774784 + 0.01 * 6.916686058044434
Epoch 460, val loss: 0.640078604221344
Epoch 470, training loss: 0.09362867474555969 = 0.024481605738401413 + 0.01 * 6.914707660675049
Epoch 470, val loss: 0.6489558219909668
Epoch 480, training loss: 0.0917300283908844 = 0.022567816078662872 + 0.01 * 6.916221618652344
Epoch 480, val loss: 0.6576672196388245
Epoch 490, training loss: 0.0899784117937088 = 0.020863253623247147 + 0.01 * 6.911515712738037
Epoch 490, val loss: 0.6661900877952576
Epoch 500, training loss: 0.08841543644666672 = 0.019339965656399727 + 0.01 * 6.907547473907471
Epoch 500, val loss: 0.6745012402534485
Epoch 510, training loss: 0.08700382709503174 = 0.017974020913243294 + 0.01 * 6.902981281280518
Epoch 510, val loss: 0.6826261878013611
Epoch 520, training loss: 0.0858028456568718 = 0.016745584085583687 + 0.01 * 6.905726432800293
Epoch 520, val loss: 0.6905737519264221
Epoch 530, training loss: 0.08464755117893219 = 0.015639012679457664 + 0.01 * 6.900854110717773
Epoch 530, val loss: 0.6982926726341248
Epoch 540, training loss: 0.0835881307721138 = 0.01463894359767437 + 0.01 * 6.894918918609619
Epoch 540, val loss: 0.7057866454124451
Epoch 550, training loss: 0.08265574276447296 = 0.013732191175222397 + 0.01 * 6.892354965209961
Epoch 550, val loss: 0.7130976915359497
Epoch 560, training loss: 0.08185601979494095 = 0.012907908298075199 + 0.01 * 6.894811153411865
Epoch 560, val loss: 0.7202120423316956
Epoch 570, training loss: 0.0810832679271698 = 0.012157289311289787 + 0.01 * 6.892597675323486
Epoch 570, val loss: 0.7271227240562439
Epoch 580, training loss: 0.08030302822589874 = 0.011472032405436039 + 0.01 * 6.8831000328063965
Epoch 580, val loss: 0.7338589429855347
Epoch 590, training loss: 0.07965226471424103 = 0.010844639502465725 + 0.01 * 6.880762577056885
Epoch 590, val loss: 0.7404276132583618
Epoch 600, training loss: 0.07909397035837173 = 0.01026909053325653 + 0.01 * 6.882487773895264
Epoch 600, val loss: 0.746807336807251
Epoch 610, training loss: 0.07849495112895966 = 0.009740368463099003 + 0.01 * 6.875458717346191
Epoch 610, val loss: 0.7530118227005005
Epoch 620, training loss: 0.07797801494598389 = 0.009253542870283127 + 0.01 * 6.872447967529297
Epoch 620, val loss: 0.7590266466140747
Epoch 630, training loss: 0.07749928534030914 = 0.00880395993590355 + 0.01 * 6.869533061981201
Epoch 630, val loss: 0.7649040222167969
Epoch 640, training loss: 0.0770571231842041 = 0.008388400077819824 + 0.01 * 6.866872787475586
Epoch 640, val loss: 0.7705997228622437
Epoch 650, training loss: 0.07664038240909576 = 0.008003822527825832 + 0.01 * 6.863656044006348
Epoch 650, val loss: 0.7761733531951904
Epoch 660, training loss: 0.07634169608354568 = 0.007646918296813965 + 0.01 * 6.869478225708008
Epoch 660, val loss: 0.7815809845924377
Epoch 670, training loss: 0.07591504603624344 = 0.0073153129778802395 + 0.01 * 6.859972953796387
Epoch 670, val loss: 0.7868422269821167
Epoch 680, training loss: 0.07561100274324417 = 0.007006408181041479 + 0.01 * 6.860459804534912
Epoch 680, val loss: 0.7919740676879883
Epoch 690, training loss: 0.07528049498796463 = 0.006718433927744627 + 0.01 * 6.856205940246582
Epoch 690, val loss: 0.7969502210617065
Epoch 700, training loss: 0.07494806498289108 = 0.006449456326663494 + 0.01 * 6.8498616218566895
Epoch 700, val loss: 0.8018437027931213
Epoch 710, training loss: 0.07473114132881165 = 0.006197815295308828 + 0.01 * 6.85333251953125
Epoch 710, val loss: 0.8065681457519531
Epoch 720, training loss: 0.07440350204706192 = 0.0059622651897370815 + 0.01 * 6.844123363494873
Epoch 720, val loss: 0.8111968040466309
Epoch 730, training loss: 0.07421530038118362 = 0.005741251166909933 + 0.01 * 6.847404956817627
Epoch 730, val loss: 0.815683901309967
Epoch 740, training loss: 0.07390445470809937 = 0.0055338237434625626 + 0.01 * 6.837063312530518
Epoch 740, val loss: 0.8200856447219849
Epoch 750, training loss: 0.0736883282661438 = 0.005338926333934069 + 0.01 * 6.834939956665039
Epoch 750, val loss: 0.824368953704834
Epoch 760, training loss: 0.07354483753442764 = 0.005155342165380716 + 0.01 * 6.838950157165527
Epoch 760, val loss: 0.8285462260246277
Epoch 770, training loss: 0.0732874795794487 = 0.004982432350516319 + 0.01 * 6.830504417419434
Epoch 770, val loss: 0.832596480846405
Epoch 780, training loss: 0.07300173491239548 = 0.004819254390895367 + 0.01 * 6.818248271942139
Epoch 780, val loss: 0.8365741968154907
Epoch 790, training loss: 0.07285468280315399 = 0.0046651167795062065 + 0.01 * 6.818957328796387
Epoch 790, val loss: 0.8404548168182373
Epoch 800, training loss: 0.07275136560201645 = 0.004519672133028507 + 0.01 * 6.823169231414795
Epoch 800, val loss: 0.8442242741584778
Epoch 810, training loss: 0.07251743227243423 = 0.004381945822387934 + 0.01 * 6.813548564910889
Epoch 810, val loss: 0.8478702306747437
Epoch 820, training loss: 0.07245927304029465 = 0.004251773469150066 + 0.01 * 6.820750713348389
Epoch 820, val loss: 0.8514679074287415
Epoch 830, training loss: 0.0722220167517662 = 0.0041281068697571754 + 0.01 * 6.809391498565674
Epoch 830, val loss: 0.8549773097038269
Epoch 840, training loss: 0.07208851724863052 = 0.004010372795164585 + 0.01 * 6.807814598083496
Epoch 840, val loss: 0.858424186706543
Epoch 850, training loss: 0.07189697027206421 = 0.0038983782287687063 + 0.01 * 6.799859523773193
Epoch 850, val loss: 0.8617807030677795
Epoch 860, training loss: 0.07170522212982178 = 0.0037915746215730906 + 0.01 * 6.791364669799805
Epoch 860, val loss: 0.865067720413208
Epoch 870, training loss: 0.07163071632385254 = 0.0036897689569741488 + 0.01 * 6.794095039367676
Epoch 870, val loss: 0.8682522773742676
Epoch 880, training loss: 0.07160788774490356 = 0.003592860884964466 + 0.01 * 6.801502227783203
Epoch 880, val loss: 0.87141352891922
Epoch 890, training loss: 0.07149175554513931 = 0.003500668564811349 + 0.01 * 6.799108505249023
Epoch 890, val loss: 0.874442994594574
Epoch 900, training loss: 0.07119544595479965 = 0.0034126348327845335 + 0.01 * 6.778281211853027
Epoch 900, val loss: 0.8774277567863464
Epoch 910, training loss: 0.07101491093635559 = 0.003328547812998295 + 0.01 * 6.768636703491211
Epoch 910, val loss: 0.8803523778915405
Epoch 920, training loss: 0.07109063118696213 = 0.003248069202527404 + 0.01 * 6.784256935119629
Epoch 920, val loss: 0.8832228183746338
Epoch 930, training loss: 0.07097873091697693 = 0.0031712448690086603 + 0.01 * 6.780749320983887
Epoch 930, val loss: 0.8860092759132385
Epoch 940, training loss: 0.07101117074489594 = 0.0030975830741226673 + 0.01 * 6.791358947753906
Epoch 940, val loss: 0.8887529969215393
Epoch 950, training loss: 0.07072867453098297 = 0.0030275038443505764 + 0.01 * 6.770116806030273
Epoch 950, val loss: 0.8914228081703186
Epoch 960, training loss: 0.0705932080745697 = 0.0029599489644169807 + 0.01 * 6.7633256912231445
Epoch 960, val loss: 0.8940479755401611
Epoch 970, training loss: 0.07045923918485641 = 0.0028955386951565742 + 0.01 * 6.7563700675964355
Epoch 970, val loss: 0.8966386914253235
Epoch 980, training loss: 0.0706406831741333 = 0.002833374310284853 + 0.01 * 6.780731201171875
Epoch 980, val loss: 0.8991561532020569
Epoch 990, training loss: 0.07026244699954987 = 0.0027739708311855793 + 0.01 * 6.748847484588623
Epoch 990, val loss: 0.9016327261924744
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.0431342124938965 = 1.9571654796600342 + 0.01 * 8.596868515014648
Epoch 0, val loss: 1.9502323865890503
Epoch 10, training loss: 2.0329835414886475 = 1.9470151662826538 + 0.01 * 8.596842765808105
Epoch 10, val loss: 1.9399992227554321
Epoch 20, training loss: 2.020986557006836 = 1.935019612312317 + 0.01 * 8.596688270568848
Epoch 20, val loss: 1.927946925163269
Epoch 30, training loss: 2.004533290863037 = 1.9185712337493896 + 0.01 * 8.59619426727295
Epoch 30, val loss: 1.9115242958068848
Epoch 40, training loss: 1.9803763628005981 = 1.8944382667541504 + 0.01 * 8.593814849853516
Epoch 40, val loss: 1.8876941204071045
Epoch 50, training loss: 1.9448354244232178 = 1.8590658903121948 + 0.01 * 8.576953887939453
Epoch 50, val loss: 1.853901743888855
Epoch 60, training loss: 1.8987057209014893 = 1.8138033151626587 + 0.01 * 8.490235328674316
Epoch 60, val loss: 1.8137110471725464
Epoch 70, training loss: 1.8510806560516357 = 1.7700732946395874 + 0.01 * 8.100737571716309
Epoch 70, val loss: 1.7781767845153809
Epoch 80, training loss: 1.8049983978271484 = 1.7251255512237549 + 0.01 * 7.987286567687988
Epoch 80, val loss: 1.7373780012130737
Epoch 90, training loss: 1.7392463684082031 = 1.6614460945129395 + 0.01 * 7.780030250549316
Epoch 90, val loss: 1.6772723197937012
Epoch 100, training loss: 1.6526411771774292 = 1.5765973329544067 + 0.01 * 7.604382514953613
Epoch 100, val loss: 1.602131724357605
Epoch 110, training loss: 1.5488135814666748 = 1.4740586280822754 + 0.01 * 7.475490093231201
Epoch 110, val loss: 1.5146209001541138
Epoch 120, training loss: 1.4396657943725586 = 1.3655612468719482 + 0.01 * 7.410449028015137
Epoch 120, val loss: 1.4235237836837769
Epoch 130, training loss: 1.3357720375061035 = 1.261999249458313 + 0.01 * 7.377279758453369
Epoch 130, val loss: 1.3396281003952026
Epoch 140, training loss: 1.2401632070541382 = 1.1667481660842896 + 0.01 * 7.341501712799072
Epoch 140, val loss: 1.2648478746414185
Epoch 150, training loss: 1.1510062217712402 = 1.077872633934021 + 0.01 * 7.313363075256348
Epoch 150, val loss: 1.1967686414718628
Epoch 160, training loss: 1.0657196044921875 = 0.9928836822509766 + 0.01 * 7.2835893630981445
Epoch 160, val loss: 1.1320812702178955
Epoch 170, training loss: 0.9839016199111938 = 0.9114299416542053 + 0.01 * 7.247169494628906
Epoch 170, val loss: 1.0702767372131348
Epoch 180, training loss: 0.9065923690795898 = 0.8345829844474792 + 0.01 * 7.200936794281006
Epoch 180, val loss: 1.0122928619384766
Epoch 190, training loss: 0.8346019983291626 = 0.7630833983421326 + 0.01 * 7.151860237121582
Epoch 190, val loss: 0.9593527317047119
Epoch 200, training loss: 0.7684422731399536 = 0.6972584128379822 + 0.01 * 7.118386745452881
Epoch 200, val loss: 0.9126016497612
Epoch 210, training loss: 0.7080718278884888 = 0.6370686888694763 + 0.01 * 7.100314617156982
Epoch 210, val loss: 0.8727962970733643
Epoch 220, training loss: 0.6528757810592651 = 0.5819987654685974 + 0.01 * 7.0876994132995605
Epoch 220, val loss: 0.8399366736412048
Epoch 230, training loss: 0.6020504236221313 = 0.5312740802764893 + 0.01 * 7.077635765075684
Epoch 230, val loss: 0.8133124113082886
Epoch 240, training loss: 0.5547942519187927 = 0.48407676815986633 + 0.01 * 7.071746349334717
Epoch 240, val loss: 0.7918626070022583
Epoch 250, training loss: 0.5104854702949524 = 0.43980807065963745 + 0.01 * 7.067737579345703
Epoch 250, val loss: 0.7745368480682373
Epoch 260, training loss: 0.46888449788093567 = 0.39823558926582336 + 0.01 * 7.064889907836914
Epoch 260, val loss: 0.7606072425842285
Epoch 270, training loss: 0.4300805330276489 = 0.35944363474845886 + 0.01 * 7.063689708709717
Epoch 270, val loss: 0.749896764755249
Epoch 280, training loss: 0.3941223621368408 = 0.3235083520412445 + 0.01 * 7.061400890350342
Epoch 280, val loss: 0.7423586249351501
Epoch 290, training loss: 0.3607873320579529 = 0.2901803255081177 + 0.01 * 7.060698986053467
Epoch 290, val loss: 0.737420916557312
Epoch 300, training loss: 0.3296032249927521 = 0.25900375843048096 + 0.01 * 7.059946537017822
Epoch 300, val loss: 0.7342628836631775
Epoch 310, training loss: 0.3002803921699524 = 0.2296852171421051 + 0.01 * 7.059516906738281
Epoch 310, val loss: 0.7325764894485474
Epoch 320, training loss: 0.27295657992362976 = 0.20236323773860931 + 0.01 * 7.059333801269531
Epoch 320, val loss: 0.7322608232498169
Epoch 330, training loss: 0.24807336926460266 = 0.17748163640499115 + 0.01 * 7.059173583984375
Epoch 330, val loss: 0.7335157990455627
Epoch 340, training loss: 0.2260403335094452 = 0.15545043349266052 + 0.01 * 7.05898904800415
Epoch 340, val loss: 0.7366572618484497
Epoch 350, training loss: 0.20693987607955933 = 0.13635215163230896 + 0.01 * 7.05877161026001
Epoch 350, val loss: 0.7416641712188721
Epoch 360, training loss: 0.19056656956672668 = 0.11997104436159134 + 0.01 * 7.0595526695251465
Epoch 360, val loss: 0.748340368270874
Epoch 370, training loss: 0.1765199601650238 = 0.10593730211257935 + 0.01 * 7.0582661628723145
Epoch 370, val loss: 0.7562759518623352
Epoch 380, training loss: 0.164451003074646 = 0.09387121349573135 + 0.01 * 7.05797815322876
Epoch 380, val loss: 0.7651436924934387
Epoch 390, training loss: 0.154011070728302 = 0.08343780785799026 + 0.01 * 7.057326316833496
Epoch 390, val loss: 0.7745639085769653
Epoch 400, training loss: 0.1449301540851593 = 0.07436338067054749 + 0.01 * 7.05667781829834
Epoch 400, val loss: 0.7843153476715088
Epoch 410, training loss: 0.1369917392730713 = 0.06643211096525192 + 0.01 * 7.055962562561035
Epoch 410, val loss: 0.7942339181900024
Epoch 420, training loss: 0.13002543151378632 = 0.05947485193610191 + 0.01 * 7.055058002471924
Epoch 420, val loss: 0.8041505813598633
Epoch 430, training loss: 0.12389892339706421 = 0.05335808917880058 + 0.01 * 7.054082870483398
Epoch 430, val loss: 0.8140118718147278
Epoch 440, training loss: 0.11850487440824509 = 0.04797440767288208 + 0.01 * 7.053047180175781
Epoch 440, val loss: 0.8237332105636597
Epoch 450, training loss: 0.11376345157623291 = 0.0432339608669281 + 0.01 * 7.052948951721191
Epoch 450, val loss: 0.8332903385162354
Epoch 460, training loss: 0.10956482589244843 = 0.039059724658727646 + 0.01 * 7.050509929656982
Epoch 460, val loss: 0.8426453471183777
Epoch 470, training loss: 0.10586918890476227 = 0.035381998866796494 + 0.01 * 7.048718452453613
Epoch 470, val loss: 0.8517647981643677
Epoch 480, training loss: 0.10260900855064392 = 0.032139599323272705 + 0.01 * 7.04694128036499
Epoch 480, val loss: 0.8606348037719727
Epoch 490, training loss: 0.09973230212926865 = 0.029278859496116638 + 0.01 * 7.045344352722168
Epoch 490, val loss: 0.8692317008972168
Epoch 500, training loss: 0.0972076803445816 = 0.026752840727567673 + 0.01 * 7.0454840660095215
Epoch 500, val loss: 0.8775943517684937
Epoch 510, training loss: 0.09492876380681992 = 0.024519076570868492 + 0.01 * 7.040968418121338
Epoch 510, val loss: 0.8856812715530396
Epoch 520, training loss: 0.0929180234670639 = 0.022538308054208755 + 0.01 * 7.037971019744873
Epoch 520, val loss: 0.8935206532478333
Epoch 530, training loss: 0.09114721417427063 = 0.020777564495801926 + 0.01 * 7.0369648933410645
Epoch 530, val loss: 0.9011041522026062
Epoch 540, training loss: 0.08956583589315414 = 0.019208336248993874 + 0.01 * 7.035750389099121
Epoch 540, val loss: 0.9084447622299194
Epoch 550, training loss: 0.08810224384069443 = 0.01780635304749012 + 0.01 * 7.029589653015137
Epoch 550, val loss: 0.9155701398849487
Epoch 560, training loss: 0.08681600540876389 = 0.016550377011299133 + 0.01 * 7.0265631675720215
Epoch 560, val loss: 0.9224706888198853
Epoch 570, training loss: 0.08566854149103165 = 0.015423166565597057 + 0.01 * 7.024538040161133
Epoch 570, val loss: 0.9291621446609497
Epoch 580, training loss: 0.08458270132541656 = 0.014408823102712631 + 0.01 * 7.017387390136719
Epoch 580, val loss: 0.9356667399406433
Epoch 590, training loss: 0.08363654464483261 = 0.013492862693965435 + 0.01 * 7.014368534088135
Epoch 590, val loss: 0.9419811964035034
Epoch 600, training loss: 0.08278002589941025 = 0.012663817964494228 + 0.01 * 7.01162052154541
Epoch 600, val loss: 0.9481122493743896
Epoch 610, training loss: 0.08200189471244812 = 0.01191171258687973 + 0.01 * 7.009018421173096
Epoch 610, val loss: 0.9540826678276062
Epoch 620, training loss: 0.08119450509548187 = 0.011227788403630257 + 0.01 * 6.9966721534729
Epoch 620, val loss: 0.9598686099052429
Epoch 630, training loss: 0.0805279016494751 = 0.01060425490140915 + 0.01 * 6.992364883422852
Epoch 630, val loss: 0.965533435344696
Epoch 640, training loss: 0.07988565415143967 = 0.010033455677330494 + 0.01 * 6.985219478607178
Epoch 640, val loss: 0.9710566997528076
Epoch 650, training loss: 0.07932771742343903 = 0.009509789757430553 + 0.01 * 6.98179292678833
Epoch 650, val loss: 0.9764441847801208
Epoch 660, training loss: 0.07885968685150146 = 0.009027588181197643 + 0.01 * 6.98321008682251
Epoch 660, val loss: 0.981711745262146
Epoch 670, training loss: 0.07826969027519226 = 0.008585161529481411 + 0.01 * 6.9684529304504395
Epoch 670, val loss: 0.9868406057357788
Epoch 680, training loss: 0.07776184380054474 = 0.008177093230187893 + 0.01 * 6.958475112915039
Epoch 680, val loss: 0.9918426275253296
Epoch 690, training loss: 0.07735078036785126 = 0.007799576967954636 + 0.01 * 6.955121040344238
Epoch 690, val loss: 0.9967116713523865
Epoch 700, training loss: 0.07688923180103302 = 0.007449856027960777 + 0.01 * 6.9439377784729
Epoch 700, val loss: 1.0014536380767822
Epoch 710, training loss: 0.07673625648021698 = 0.007124385330826044 + 0.01 * 6.96118688583374
Epoch 710, val loss: 1.006104588508606
Epoch 720, training loss: 0.07612631469964981 = 0.006822685245424509 + 0.01 * 6.930363655090332
Epoch 720, val loss: 1.0106146335601807
Epoch 730, training loss: 0.07566274702548981 = 0.006541679613292217 + 0.01 * 6.912106990814209
Epoch 730, val loss: 1.015028476715088
Epoch 740, training loss: 0.07540100067853928 = 0.006279893219470978 + 0.01 * 6.912110805511475
Epoch 740, val loss: 1.0193203687667847
Epoch 750, training loss: 0.07502467930316925 = 0.006036442704498768 + 0.01 * 6.898823261260986
Epoch 750, val loss: 1.0234897136688232
Epoch 760, training loss: 0.07475125044584274 = 0.005808832589536905 + 0.01 * 6.894241809844971
Epoch 760, val loss: 1.0275899171829224
Epoch 770, training loss: 0.07446807622909546 = 0.005595105700194836 + 0.01 * 6.8872971534729
Epoch 770, val loss: 1.031583309173584
Epoch 780, training loss: 0.07419072091579437 = 0.0053946347907185555 + 0.01 * 6.879608631134033
Epoch 780, val loss: 1.0354565382003784
Epoch 790, training loss: 0.07404356449842453 = 0.005206250119954348 + 0.01 * 6.883731365203857
Epoch 790, val loss: 1.0392571687698364
Epoch 800, training loss: 0.07378185540437698 = 0.00502966670319438 + 0.01 * 6.875219345092773
Epoch 800, val loss: 1.0429632663726807
Epoch 810, training loss: 0.07355708628892899 = 0.004863034002482891 + 0.01 * 6.869405746459961
Epoch 810, val loss: 1.0466045141220093
Epoch 820, training loss: 0.07324469089508057 = 0.004705911036580801 + 0.01 * 6.853877544403076
Epoch 820, val loss: 1.050171971321106
Epoch 830, training loss: 0.07306426018476486 = 0.004557289648801088 + 0.01 * 6.850697040557861
Epoch 830, val loss: 1.053674340248108
Epoch 840, training loss: 0.07285992801189423 = 0.0044168769381940365 + 0.01 * 6.844305038452148
Epoch 840, val loss: 1.0570837259292603
Epoch 850, training loss: 0.07281364500522614 = 0.004283864516764879 + 0.01 * 6.852978229522705
Epoch 850, val loss: 1.0604326725006104
Epoch 860, training loss: 0.07249906659126282 = 0.0041580116376280785 + 0.01 * 6.834105491638184
Epoch 860, val loss: 1.0636696815490723
Epoch 870, training loss: 0.07232584804296494 = 0.004038495477288961 + 0.01 * 6.828734874725342
Epoch 870, val loss: 1.066871166229248
Epoch 880, training loss: 0.07227069139480591 = 0.00392485037446022 + 0.01 * 6.8345842361450195
Epoch 880, val loss: 1.0700041055679321
Epoch 890, training loss: 0.0720686987042427 = 0.003816857933998108 + 0.01 * 6.825183868408203
Epoch 890, val loss: 1.0730493068695068
Epoch 900, training loss: 0.07191966474056244 = 0.0037141868378967047 + 0.01 * 6.820548057556152
Epoch 900, val loss: 1.076034665107727
Epoch 910, training loss: 0.07189657539129257 = 0.003616406349465251 + 0.01 * 6.828017234802246
Epoch 910, val loss: 1.078971028327942
Epoch 920, training loss: 0.07184828817844391 = 0.0035232314839959145 + 0.01 * 6.832505702972412
Epoch 920, val loss: 1.0818202495574951
Epoch 930, training loss: 0.07153162360191345 = 0.003434441052377224 + 0.01 * 6.809718132019043
Epoch 930, val loss: 1.084612488746643
Epoch 940, training loss: 0.07136759161949158 = 0.003349752863869071 + 0.01 * 6.801784515380859
Epoch 940, val loss: 1.0873702764511108
Epoch 950, training loss: 0.0718812644481659 = 0.003268883563578129 + 0.01 * 6.861238479614258
Epoch 950, val loss: 1.090030550956726
Epoch 960, training loss: 0.07127916067838669 = 0.0031916999723762274 + 0.01 * 6.808746337890625
Epoch 960, val loss: 1.0925976037979126
Epoch 970, training loss: 0.07107802480459213 = 0.003118132008239627 + 0.01 * 6.795989036560059
Epoch 970, val loss: 1.0951218605041504
Epoch 980, training loss: 0.07096093147993088 = 0.0030477293767035007 + 0.01 * 6.79132080078125
Epoch 980, val loss: 1.0976192951202393
Epoch 990, training loss: 0.07085218280553818 = 0.002980128163471818 + 0.01 * 6.787205696105957
Epoch 990, val loss: 1.1000860929489136
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.0305123329162598 = 1.9445441961288452 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9527547359466553
Epoch 10, training loss: 2.021022319793701 = 1.9350550174713135 + 0.01 * 8.596741676330566
Epoch 10, val loss: 1.9429404735565186
Epoch 20, training loss: 2.0090510845184326 = 1.9230866432189941 + 0.01 * 8.596433639526367
Epoch 20, val loss: 1.930433750152588
Epoch 30, training loss: 1.9916471242904663 = 1.9056960344314575 + 0.01 * 8.595112800598145
Epoch 30, val loss: 1.9121414422988892
Epoch 40, training loss: 1.9648454189300537 = 1.8789994716644287 + 0.01 * 8.584598541259766
Epoch 40, val loss: 1.8842387199401855
Epoch 50, training loss: 1.9260057210922241 = 1.8409134149551392 + 0.01 * 8.509236335754395
Epoch 50, val loss: 1.8460516929626465
Epoch 60, training loss: 1.8795918226242065 = 1.7983545064926147 + 0.01 * 8.123730659484863
Epoch 60, val loss: 1.8070495128631592
Epoch 70, training loss: 1.8407583236694336 = 1.7611452341079712 + 0.01 * 7.961308002471924
Epoch 70, val loss: 1.7738689184188843
Epoch 80, training loss: 1.7911491394042969 = 1.7142120599746704 + 0.01 * 7.693711757659912
Epoch 80, val loss: 1.7291936874389648
Epoch 90, training loss: 1.7232444286346436 = 1.64898681640625 + 0.01 * 7.425755977630615
Epoch 90, val loss: 1.669094443321228
Epoch 100, training loss: 1.6350129842758179 = 1.562233805656433 + 0.01 * 7.277923583984375
Epoch 100, val loss: 1.5927258729934692
Epoch 110, training loss: 1.5309839248657227 = 1.4588418006896973 + 0.01 * 7.2142133712768555
Epoch 110, val loss: 1.5030020475387573
Epoch 120, training loss: 1.4208208322525024 = 1.3489537239074707 + 0.01 * 7.186707019805908
Epoch 120, val loss: 1.4098135232925415
Epoch 130, training loss: 1.3118513822555542 = 1.2402290105819702 + 0.01 * 7.162239074707031
Epoch 130, val loss: 1.3197556734085083
Epoch 140, training loss: 1.208828091621399 = 1.1373310089111328 + 0.01 * 7.14971399307251
Epoch 140, val loss: 1.236109733581543
Epoch 150, training loss: 1.1160095930099487 = 1.0445671081542969 + 0.01 * 7.144244194030762
Epoch 150, val loss: 1.1613824367523193
Epoch 160, training loss: 1.0347604751586914 = 0.9633499979972839 + 0.01 * 7.141053199768066
Epoch 160, val loss: 1.097021460533142
Epoch 170, training loss: 0.9627538919448853 = 0.8913884162902832 + 0.01 * 7.136545658111572
Epoch 170, val loss: 1.0406454801559448
Epoch 180, training loss: 0.8964117765426636 = 0.8251280784606934 + 0.01 * 7.12837028503418
Epoch 180, val loss: 0.9889971613883972
Epoch 190, training loss: 0.8329099416732788 = 0.7617546319961548 + 0.01 * 7.115532875061035
Epoch 190, val loss: 0.9402248859405518
Epoch 200, training loss: 0.7709856629371643 = 0.7000136375427246 + 0.01 * 7.097203731536865
Epoch 200, val loss: 0.8940127491950989
Epoch 210, training loss: 0.710822343826294 = 0.6400766968727112 + 0.01 * 7.074565410614014
Epoch 210, val loss: 0.8514648079872131
Epoch 220, training loss: 0.6531768441200256 = 0.582596480846405 + 0.01 * 7.0580339431762695
Epoch 220, val loss: 0.8144025802612305
Epoch 230, training loss: 0.5983419418334961 = 0.527928352355957 + 0.01 * 7.041362285614014
Epoch 230, val loss: 0.7834296226501465
Epoch 240, training loss: 0.5461888313293457 = 0.47587621212005615 + 0.01 * 7.031261444091797
Epoch 240, val loss: 0.7579264640808105
Epoch 250, training loss: 0.49623939394950867 = 0.42598724365234375 + 0.01 * 7.025215148925781
Epoch 250, val loss: 0.7371165156364441
Epoch 260, training loss: 0.44824016094207764 = 0.37803518772125244 + 0.01 * 7.020497798919678
Epoch 260, val loss: 0.7202746272087097
Epoch 270, training loss: 0.4022611677646637 = 0.33210527896881104 + 0.01 * 7.015589714050293
Epoch 270, val loss: 0.7070196270942688
Epoch 280, training loss: 0.3587760329246521 = 0.28866761922836304 + 0.01 * 7.010841369628906
Epoch 280, val loss: 0.6971961259841919
Epoch 290, training loss: 0.3185136616230011 = 0.24844563007354736 + 0.01 * 7.006803512573242
Epoch 290, val loss: 0.6905664801597595
Epoch 300, training loss: 0.2822292447090149 = 0.2121782898902893 + 0.01 * 7.005094051361084
Epoch 300, val loss: 0.6868772506713867
Epoch 310, training loss: 0.2504025101661682 = 0.18042823672294617 + 0.01 * 6.997426509857178
Epoch 310, val loss: 0.6861137747764587
Epoch 320, training loss: 0.22325775027275085 = 0.1533443033695221 + 0.01 * 6.991344928741455
Epoch 320, val loss: 0.6881954073905945
Epoch 330, training loss: 0.2005278766155243 = 0.13067744672298431 + 0.01 * 6.985044002532959
Epoch 330, val loss: 0.6929284334182739
Epoch 340, training loss: 0.18190841376781464 = 0.11190459877252579 + 0.01 * 7.000381946563721
Epoch 340, val loss: 0.6999382972717285
Epoch 350, training loss: 0.16622227430343628 = 0.09641781449317932 + 0.01 * 6.980446815490723
Epoch 350, val loss: 0.7085860371589661
Epoch 360, training loss: 0.15329457819461823 = 0.08361426740884781 + 0.01 * 6.968031406402588
Epoch 360, val loss: 0.7183439135551453
Epoch 370, training loss: 0.1426050066947937 = 0.07298748940229416 + 0.01 * 6.9617509841918945
Epoch 370, val loss: 0.728865385055542
Epoch 380, training loss: 0.13364075124263763 = 0.06412036716938019 + 0.01 * 6.952038764953613
Epoch 380, val loss: 0.739819347858429
Epoch 390, training loss: 0.12645649909973145 = 0.05668255314230919 + 0.01 * 6.9773945808410645
Epoch 390, val loss: 0.7509481906890869
Epoch 400, training loss: 0.11985145509243011 = 0.050418294966220856 + 0.01 * 6.9433159828186035
Epoch 400, val loss: 0.7620291709899902
Epoch 410, training loss: 0.11444028466939926 = 0.045104898512363434 + 0.01 * 6.933538913726807
Epoch 410, val loss: 0.7729576826095581
Epoch 420, training loss: 0.10979646444320679 = 0.04056667163968086 + 0.01 * 6.922979831695557
Epoch 420, val loss: 0.7836858630180359
Epoch 430, training loss: 0.1058371365070343 = 0.036666255444288254 + 0.01 * 6.917088508605957
Epoch 430, val loss: 0.7941607236862183
Epoch 440, training loss: 0.10236509144306183 = 0.03329605236649513 + 0.01 * 6.906904220581055
Epoch 440, val loss: 0.8043074011802673
Epoch 450, training loss: 0.0994342714548111 = 0.030365783721208572 + 0.01 * 6.906849384307861
Epoch 450, val loss: 0.8141476511955261
Epoch 460, training loss: 0.09673741459846497 = 0.02780444361269474 + 0.01 * 6.89329719543457
Epoch 460, val loss: 0.8236222863197327
Epoch 470, training loss: 0.09458404779434204 = 0.025551114231348038 + 0.01 * 6.903294086456299
Epoch 470, val loss: 0.8327294588088989
Epoch 480, training loss: 0.09237472712993622 = 0.023562604561448097 + 0.01 * 6.8812127113342285
Epoch 480, val loss: 0.8415526747703552
Epoch 490, training loss: 0.0905240923166275 = 0.021797936409711838 + 0.01 * 6.872616291046143
Epoch 490, val loss: 0.8501114845275879
Epoch 500, training loss: 0.08931833505630493 = 0.020225076004862785 + 0.01 * 6.909326076507568
Epoch 500, val loss: 0.858465313911438
Epoch 510, training loss: 0.08755232393741608 = 0.018821457400918007 + 0.01 * 6.873086929321289
Epoch 510, val loss: 0.8664004802703857
Epoch 520, training loss: 0.08615773171186447 = 0.01756156049668789 + 0.01 * 6.859617233276367
Epoch 520, val loss: 0.8741322755813599
Epoch 530, training loss: 0.08498391509056091 = 0.016426365822553635 + 0.01 * 6.855754375457764
Epoch 530, val loss: 0.881646990776062
Epoch 540, training loss: 0.08389279991388321 = 0.015401378273963928 + 0.01 * 6.849142551422119
Epoch 540, val loss: 0.8888306617736816
Epoch 550, training loss: 0.08284935355186462 = 0.014471877366304398 + 0.01 * 6.837747573852539
Epoch 550, val loss: 0.8958471417427063
Epoch 560, training loss: 0.08200590312480927 = 0.013626023195683956 + 0.01 * 6.837988376617432
Epoch 560, val loss: 0.9027156829833984
Epoch 570, training loss: 0.08120984584093094 = 0.012855572625994682 + 0.01 * 6.835427284240723
Epoch 570, val loss: 0.9092910289764404
Epoch 580, training loss: 0.08043283969163895 = 0.01215180940926075 + 0.01 * 6.828103065490723
Epoch 580, val loss: 0.9157029390335083
Epoch 590, training loss: 0.08004628121852875 = 0.01150666456669569 + 0.01 * 6.853962421417236
Epoch 590, val loss: 0.9219692349433899
Epoch 600, training loss: 0.07901406288146973 = 0.010914992541074753 + 0.01 * 6.809906482696533
Epoch 600, val loss: 0.9279316067695618
Epoch 610, training loss: 0.07849430292844772 = 0.010370220057666302 + 0.01 * 6.812408924102783
Epoch 610, val loss: 0.933792769908905
Epoch 620, training loss: 0.07789812982082367 = 0.009866929613053799 + 0.01 * 6.803120136260986
Epoch 620, val loss: 0.9395037293434143
Epoch 630, training loss: 0.07745041698217392 = 0.009401575662195683 + 0.01 * 6.804884433746338
Epoch 630, val loss: 0.9450812935829163
Epoch 640, training loss: 0.07704981416463852 = 0.008970393799245358 + 0.01 * 6.8079423904418945
Epoch 640, val loss: 0.9504280090332031
Epoch 650, training loss: 0.07655993103981018 = 0.008570871315896511 + 0.01 * 6.798905849456787
Epoch 650, val loss: 0.955693781375885
Epoch 660, training loss: 0.07608497887849808 = 0.008199699223041534 + 0.01 * 6.788527965545654
Epoch 660, val loss: 0.9607350826263428
Epoch 670, training loss: 0.07565885782241821 = 0.007853719405829906 + 0.01 * 6.780514240264893
Epoch 670, val loss: 0.9657198190689087
Epoch 680, training loss: 0.07555866241455078 = 0.007530632894486189 + 0.01 * 6.8028035163879395
Epoch 680, val loss: 0.9705785512924194
Epoch 690, training loss: 0.0750126987695694 = 0.007228847127407789 + 0.01 * 6.778385639190674
Epoch 690, val loss: 0.9751846790313721
Epoch 700, training loss: 0.07497932016849518 = 0.006946538109332323 + 0.01 * 6.803277969360352
Epoch 700, val loss: 0.9798105955123901
Epoch 710, training loss: 0.07439912855625153 = 0.0066819461062550545 + 0.01 * 6.7717180252075195
Epoch 710, val loss: 0.9842272996902466
Epoch 720, training loss: 0.07406004518270493 = 0.006433811970055103 + 0.01 * 6.762622833251953
Epoch 720, val loss: 0.9885701537132263
Epoch 730, training loss: 0.07397407293319702 = 0.006200441159307957 + 0.01 * 6.7773637771606445
Epoch 730, val loss: 0.9928199052810669
Epoch 740, training loss: 0.07354971766471863 = 0.005981061607599258 + 0.01 * 6.756865501403809
Epoch 740, val loss: 0.9968881011009216
Epoch 750, training loss: 0.07361570745706558 = 0.005774216260761023 + 0.01 * 6.784149169921875
Epoch 750, val loss: 1.0009269714355469
Epoch 760, training loss: 0.07315480709075928 = 0.005579377990216017 + 0.01 * 6.757543563842773
Epoch 760, val loss: 1.0048303604125977
Epoch 770, training loss: 0.07290979474782944 = 0.005395376589149237 + 0.01 * 6.7514424324035645
Epoch 770, val loss: 1.008676290512085
Epoch 780, training loss: 0.07280019670724869 = 0.005221442319452763 + 0.01 * 6.757875919342041
Epoch 780, val loss: 1.012420654296875
Epoch 790, training loss: 0.07253827899694443 = 0.005056769121438265 + 0.01 * 6.748150825500488
Epoch 790, val loss: 1.016010046005249
Epoch 800, training loss: 0.0722682923078537 = 0.004900807980448008 + 0.01 * 6.736749172210693
Epoch 800, val loss: 1.019639015197754
Epoch 810, training loss: 0.07240477204322815 = 0.00475297961384058 + 0.01 * 6.76517915725708
Epoch 810, val loss: 1.023114800453186
Epoch 820, training loss: 0.07209531962871552 = 0.004613091237843037 + 0.01 * 6.748222827911377
Epoch 820, val loss: 1.0264692306518555
Epoch 830, training loss: 0.07186098396778107 = 0.004479961469769478 + 0.01 * 6.738102912902832
Epoch 830, val loss: 1.0297467708587646
Epoch 840, training loss: 0.07168827950954437 = 0.004353407770395279 + 0.01 * 6.733487606048584
Epoch 840, val loss: 1.0330536365509033
Epoch 850, training loss: 0.07153961062431335 = 0.0042329588904976845 + 0.01 * 6.730665683746338
Epoch 850, val loss: 1.0361905097961426
Epoch 860, training loss: 0.07159508764743805 = 0.004118101671338081 + 0.01 * 6.74769926071167
Epoch 860, val loss: 1.0393060445785522
Epoch 870, training loss: 0.0712759718298912 = 0.004008769057691097 + 0.01 * 6.726720333099365
Epoch 870, val loss: 1.0422483682632446
Epoch 880, training loss: 0.0712081640958786 = 0.003904575714841485 + 0.01 * 6.730359077453613
Epoch 880, val loss: 1.0452300310134888
Epoch 890, training loss: 0.0710592269897461 = 0.003805055283010006 + 0.01 * 6.725417137145996
Epoch 890, val loss: 1.0481312274932861
Epoch 900, training loss: 0.07092039287090302 = 0.0037100024055689573 + 0.01 * 6.721039295196533
Epoch 900, val loss: 1.0508893728256226
Epoch 910, training loss: 0.07077781856060028 = 0.003619213355705142 + 0.01 * 6.715860366821289
Epoch 910, val loss: 1.0536682605743408
Epoch 920, training loss: 0.07071131467819214 = 0.003532383358106017 + 0.01 * 6.717893600463867
Epoch 920, val loss: 1.0563583374023438
Epoch 930, training loss: 0.07069141417741776 = 0.0034491599071770906 + 0.01 * 6.7242255210876465
Epoch 930, val loss: 1.0590194463729858
Epoch 940, training loss: 0.07042281329631805 = 0.0033694820012897253 + 0.01 * 6.705333232879639
Epoch 940, val loss: 1.0615577697753906
Epoch 950, training loss: 0.07042557001113892 = 0.00329336104914546 + 0.01 * 6.713221549987793
Epoch 950, val loss: 1.0640443563461304
Epoch 960, training loss: 0.07026472687721252 = 0.0032202249858528376 + 0.01 * 6.704450607299805
Epoch 960, val loss: 1.066527247428894
Epoch 970, training loss: 0.07013283669948578 = 0.003149984637275338 + 0.01 * 6.698285102844238
Epoch 970, val loss: 1.0689574480056763
Epoch 980, training loss: 0.07033467292785645 = 0.0030825359281152487 + 0.01 * 6.725213527679443
Epoch 980, val loss: 1.0712876319885254
Epoch 990, training loss: 0.070096455514431 = 0.003017993876710534 + 0.01 * 6.707846164703369
Epoch 990, val loss: 1.0736021995544434
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8413284132841329
The final CL Acc:0.81358, 0.01720, The final GNN Acc:0.83817, 0.00258
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9556])
updated graph: torch.Size([2, 10610])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0428783893585205 = 1.9569098949432373 + 0.01 * 8.596851348876953
Epoch 0, val loss: 1.9595407247543335
Epoch 10, training loss: 2.0323562622070312 = 1.9463881254196167 + 0.01 * 8.596809387207031
Epoch 10, val loss: 1.9492772817611694
Epoch 20, training loss: 2.019307851791382 = 1.9333412647247314 + 0.01 * 8.596656799316406
Epoch 20, val loss: 1.936365008354187
Epoch 30, training loss: 2.0009849071502686 = 1.9150233268737793 + 0.01 * 8.596168518066406
Epoch 30, val loss: 1.9181463718414307
Epoch 40, training loss: 1.9740557670593262 = 1.888120412826538 + 0.01 * 8.59354019165039
Epoch 40, val loss: 1.8919428586959839
Epoch 50, training loss: 1.9370503425598145 = 1.8513062000274658 + 0.01 * 8.57441234588623
Epoch 50, val loss: 1.8582288026809692
Epoch 60, training loss: 1.8975330591201782 = 1.812687635421753 + 0.01 * 8.484543800354004
Epoch 60, val loss: 1.8270539045333862
Epoch 70, training loss: 1.864755630493164 = 1.7836706638336182 + 0.01 * 8.10849666595459
Epoch 70, val loss: 1.8031038045883179
Epoch 80, training loss: 1.8292144536972046 = 1.7488912343978882 + 0.01 * 8.032327651977539
Epoch 80, val loss: 1.7683990001678467
Epoch 90, training loss: 1.7799805402755737 = 1.7006715536117554 + 0.01 * 7.930898189544678
Epoch 90, val loss: 1.7256970405578613
Epoch 100, training loss: 1.711695671081543 = 1.6331617832183838 + 0.01 * 7.853393077850342
Epoch 100, val loss: 1.6698969602584839
Epoch 110, training loss: 1.6272084712982178 = 1.549567461013794 + 0.01 * 7.764101505279541
Epoch 110, val loss: 1.600874900817871
Epoch 120, training loss: 1.537948489189148 = 1.462131142616272 + 0.01 * 7.581737041473389
Epoch 120, val loss: 1.5305498838424683
Epoch 130, training loss: 1.4526252746582031 = 1.3788048028945923 + 0.01 * 7.382051944732666
Epoch 130, val loss: 1.4649603366851807
Epoch 140, training loss: 1.3743274211883545 = 1.3010430335998535 + 0.01 * 7.328444957733154
Epoch 140, val loss: 1.4077950716018677
Epoch 150, training loss: 1.3014918565750122 = 1.2287437915802002 + 0.01 * 7.274801731109619
Epoch 150, val loss: 1.358193278312683
Epoch 160, training loss: 1.2323025465011597 = 1.159985899925232 + 0.01 * 7.231668949127197
Epoch 160, val loss: 1.31336510181427
Epoch 170, training loss: 1.1628999710083008 = 1.0909608602523804 + 0.01 * 7.193911552429199
Epoch 170, val loss: 1.268923282623291
Epoch 180, training loss: 1.0896447896957397 = 1.0179154872894287 + 0.01 * 7.1729350090026855
Epoch 180, val loss: 1.220044732093811
Epoch 190, training loss: 1.0109504461288452 = 0.9393329620361328 + 0.01 * 7.161745548248291
Epoch 190, val loss: 1.1654173135757446
Epoch 200, training loss: 0.9282631278038025 = 0.8567743897438049 + 0.01 * 7.1488728523254395
Epoch 200, val loss: 1.1069433689117432
Epoch 210, training loss: 0.8453943133354187 = 0.7740690112113953 + 0.01 * 7.132528305053711
Epoch 210, val loss: 1.0492562055587769
Epoch 220, training loss: 0.7661281228065491 = 0.6949958205223083 + 0.01 * 7.113232135772705
Epoch 220, val loss: 0.996925950050354
Epoch 230, training loss: 0.6926144361495972 = 0.6216511130332947 + 0.01 * 7.096329689025879
Epoch 230, val loss: 0.9530899524688721
Epoch 240, training loss: 0.6254785060882568 = 0.5546369552612305 + 0.01 * 7.084156036376953
Epoch 240, val loss: 0.9187623858451843
Epoch 250, training loss: 0.5644797086715698 = 0.49373582005500793 + 0.01 * 7.074392318725586
Epoch 250, val loss: 0.893467128276825
Epoch 260, training loss: 0.5092219114303589 = 0.4385320842266083 + 0.01 * 7.068981170654297
Epoch 260, val loss: 0.8759980201721191
Epoch 270, training loss: 0.45926639437675476 = 0.3885946571826935 + 0.01 * 7.067174911499023
Epoch 270, val loss: 0.8653878569602966
Epoch 280, training loss: 0.4139159023761749 = 0.34329017996788025 + 0.01 * 7.062572002410889
Epoch 280, val loss: 0.8605201840400696
Epoch 290, training loss: 0.3725588917732239 = 0.30195152759552 + 0.01 * 7.060734748840332
Epoch 290, val loss: 0.8608502149581909
Epoch 300, training loss: 0.33468103408813477 = 0.26408863067626953 + 0.01 * 7.05924129486084
Epoch 300, val loss: 0.8658848404884338
Epoch 310, training loss: 0.3001602292060852 = 0.22957885265350342 + 0.01 * 7.058139324188232
Epoch 310, val loss: 0.8750208616256714
Epoch 320, training loss: 0.26920947432518005 = 0.19863173365592957 + 0.01 * 7.057774066925049
Epoch 320, val loss: 0.8879942297935486
Epoch 330, training loss: 0.24204513430595398 = 0.1714782416820526 + 0.01 * 7.05668830871582
Epoch 330, val loss: 0.9041193723678589
Epoch 340, training loss: 0.218642920255661 = 0.14808274805545807 + 0.01 * 7.056017875671387
Epoch 340, val loss: 0.9228739142417908
Epoch 350, training loss: 0.19871410727500916 = 0.12816345691680908 + 0.01 * 7.055064678192139
Epoch 350, val loss: 0.943725049495697
Epoch 360, training loss: 0.1818581223487854 = 0.111304372549057 + 0.01 * 7.055376052856445
Epoch 360, val loss: 0.9661069512367249
Epoch 370, training loss: 0.167589008808136 = 0.09704706817865372 + 0.01 * 7.05419397354126
Epoch 370, val loss: 0.9894525408744812
Epoch 380, training loss: 0.1555022895336151 = 0.08496900647878647 + 0.01 * 7.053328514099121
Epoch 380, val loss: 1.013383388519287
Epoch 390, training loss: 0.1452355980873108 = 0.07471168041229248 + 0.01 * 7.052391052246094
Epoch 390, val loss: 1.0374256372451782
Epoch 400, training loss: 0.13650855422019958 = 0.06597626954317093 + 0.01 * 7.053228855133057
Epoch 400, val loss: 1.061354637145996
Epoch 410, training loss: 0.12902872264385223 = 0.058519139885902405 + 0.01 * 7.050958633422852
Epoch 410, val loss: 1.0849571228027344
Epoch 420, training loss: 0.12262564897537231 = 0.05213563144207001 + 0.01 * 7.049001693725586
Epoch 420, val loss: 1.1080788373947144
Epoch 430, training loss: 0.11713062226772308 = 0.04665507376194 + 0.01 * 7.047554969787598
Epoch 430, val loss: 1.1305960416793823
Epoch 440, training loss: 0.11240001767873764 = 0.041935794055461884 + 0.01 * 7.046422481536865
Epoch 440, val loss: 1.1524157524108887
Epoch 450, training loss: 0.10829826444387436 = 0.03785764425992966 + 0.01 * 7.04406213760376
Epoch 450, val loss: 1.1734747886657715
Epoch 460, training loss: 0.10476016998291016 = 0.03431961312890053 + 0.01 * 7.044055461883545
Epoch 460, val loss: 1.1937428712844849
Epoch 470, training loss: 0.10164809226989746 = 0.031238507479429245 + 0.01 * 7.040959358215332
Epoch 470, val loss: 1.2132678031921387
Epoch 480, training loss: 0.09893351793289185 = 0.02854372188448906 + 0.01 * 7.038980007171631
Epoch 480, val loss: 1.2320258617401123
Epoch 490, training loss: 0.09653900563716888 = 0.02617723122239113 + 0.01 * 7.036177158355713
Epoch 490, val loss: 1.250102162361145
Epoch 500, training loss: 0.09445204585790634 = 0.02409118227660656 + 0.01 * 7.036086559295654
Epoch 500, val loss: 1.2674425840377808
Epoch 510, training loss: 0.09256597608327866 = 0.022245021536946297 + 0.01 * 7.032095909118652
Epoch 510, val loss: 1.2841501235961914
Epoch 520, training loss: 0.09089185297489166 = 0.02060333639383316 + 0.01 * 7.028851509094238
Epoch 520, val loss: 1.300244927406311
Epoch 530, training loss: 0.08939354121685028 = 0.019135277718305588 + 0.01 * 7.0258259773254395
Epoch 530, val loss: 1.3157979249954224
Epoch 540, training loss: 0.08804874122142792 = 0.017816955223679543 + 0.01 * 7.023178577423096
Epoch 540, val loss: 1.3308014869689941
Epoch 550, training loss: 0.08683766424655914 = 0.016628438606858253 + 0.01 * 7.020923137664795
Epoch 550, val loss: 1.3453443050384521
Epoch 560, training loss: 0.08572439104318619 = 0.015552236698567867 + 0.01 * 7.017215251922607
Epoch 560, val loss: 1.3593931198120117
Epoch 570, training loss: 0.08469665050506592 = 0.014575283043086529 + 0.01 * 7.012136936187744
Epoch 570, val loss: 1.3730459213256836
Epoch 580, training loss: 0.08388759195804596 = 0.013686553575098515 + 0.01 * 7.020103931427002
Epoch 580, val loss: 1.3862476348876953
Epoch 590, training loss: 0.08295673131942749 = 0.01287842821329832 + 0.01 * 7.00783109664917
Epoch 590, val loss: 1.3989534378051758
Epoch 600, training loss: 0.0821562260389328 = 0.012140288949012756 + 0.01 * 7.001593589782715
Epoch 600, val loss: 1.4112471342086792
Epoch 610, training loss: 0.08144676685333252 = 0.011464341543614864 + 0.01 * 6.9982428550720215
Epoch 610, val loss: 1.4231053590774536
Epoch 620, training loss: 0.08077334612607956 = 0.010847642086446285 + 0.01 * 6.992570400238037
Epoch 620, val loss: 1.434535264968872
Epoch 630, training loss: 0.08016343414783478 = 0.01028307806700468 + 0.01 * 6.988035678863525
Epoch 630, val loss: 1.445670247077942
Epoch 640, training loss: 0.07959351688623428 = 0.00976352859288454 + 0.01 * 6.982998847961426
Epoch 640, val loss: 1.4563897848129272
Epoch 650, training loss: 0.07905491441488266 = 0.009285238571465015 + 0.01 * 6.976967811584473
Epoch 650, val loss: 1.4667761325836182
Epoch 660, training loss: 0.07859311252832413 = 0.008843333460390568 + 0.01 * 6.974978446960449
Epoch 660, val loss: 1.476861834526062
Epoch 670, training loss: 0.07815682888031006 = 0.008434467017650604 + 0.01 * 6.972236156463623
Epoch 670, val loss: 1.4866549968719482
Epoch 680, training loss: 0.0775960385799408 = 0.008056622929871082 + 0.01 * 6.953941345214844
Epoch 680, val loss: 1.4960774183273315
Epoch 690, training loss: 0.07719234377145767 = 0.007706029806286097 + 0.01 * 6.948631286621094
Epoch 690, val loss: 1.505210041999817
Epoch 700, training loss: 0.07692356407642365 = 0.007379672955721617 + 0.01 * 6.9543890953063965
Epoch 700, val loss: 1.5139999389648438
Epoch 710, training loss: 0.07645043730735779 = 0.007075885776430368 + 0.01 * 6.937455177307129
Epoch 710, val loss: 1.5227266550064087
Epoch 720, training loss: 0.07639700174331665 = 0.006792047526687384 + 0.01 * 6.960495948791504
Epoch 720, val loss: 1.531182885169983
Epoch 730, training loss: 0.07587926089763641 = 0.0065279812552034855 + 0.01 * 6.935128211975098
Epoch 730, val loss: 1.5390514135360718
Epoch 740, training loss: 0.07542353123426437 = 0.006280303932726383 + 0.01 * 6.914323329925537
Epoch 740, val loss: 1.5468348264694214
Epoch 750, training loss: 0.07570953667163849 = 0.00604775408282876 + 0.01 * 6.966177940368652
Epoch 750, val loss: 1.5544793605804443
Epoch 760, training loss: 0.07498948276042938 = 0.005830538924783468 + 0.01 * 6.915894508361816
Epoch 760, val loss: 1.5616962909698486
Epoch 770, training loss: 0.07457143068313599 = 0.0056261466816067696 + 0.01 * 6.894528388977051
Epoch 770, val loss: 1.5688401460647583
Epoch 780, training loss: 0.07432445883750916 = 0.005433450918644667 + 0.01 * 6.889101028442383
Epoch 780, val loss: 1.5757871866226196
Epoch 790, training loss: 0.07422691583633423 = 0.005251413676887751 + 0.01 * 6.897550106048584
Epoch 790, val loss: 1.5825752019882202
Epoch 800, training loss: 0.07394377142190933 = 0.005079794209450483 + 0.01 * 6.8863983154296875
Epoch 800, val loss: 1.5890651941299438
Epoch 810, training loss: 0.07366786897182465 = 0.004917510785162449 + 0.01 * 6.875035762786865
Epoch 810, val loss: 1.59550940990448
Epoch 820, training loss: 0.07356030493974686 = 0.0047643608413636684 + 0.01 * 6.879594326019287
Epoch 820, val loss: 1.6015205383300781
Epoch 830, training loss: 0.07338611781597137 = 0.004619251936674118 + 0.01 * 6.876687049865723
Epoch 830, val loss: 1.6075263023376465
Epoch 840, training loss: 0.07318126410245895 = 0.004481533542275429 + 0.01 * 6.869973659515381
Epoch 840, val loss: 1.6133424043655396
Epoch 850, training loss: 0.0728703960776329 = 0.004350965842604637 + 0.01 * 6.851943016052246
Epoch 850, val loss: 1.6189955472946167
Epoch 860, training loss: 0.07316817343235016 = 0.004226700868457556 + 0.01 * 6.8941473960876465
Epoch 860, val loss: 1.6245137453079224
Epoch 870, training loss: 0.0727556049823761 = 0.00410898681730032 + 0.01 * 6.864661693572998
Epoch 870, val loss: 1.6298099756240845
Epoch 880, training loss: 0.07240115106105804 = 0.003996833693236113 + 0.01 * 6.840432167053223
Epoch 880, val loss: 1.635055661201477
Epoch 890, training loss: 0.07255665957927704 = 0.003890021238476038 + 0.01 * 6.866663932800293
Epoch 890, val loss: 1.6401793956756592
Epoch 900, training loss: 0.07213981449604034 = 0.0037883413024246693 + 0.01 * 6.835147380828857
Epoch 900, val loss: 1.6450506448745728
Epoch 910, training loss: 0.07204775512218475 = 0.003691543824970722 + 0.01 * 6.835621356964111
Epoch 910, val loss: 1.6499049663543701
Epoch 920, training loss: 0.0718495324254036 = 0.0035990991163998842 + 0.01 * 6.825043678283691
Epoch 920, val loss: 1.6545220613479614
Epoch 930, training loss: 0.0720050260424614 = 0.003510678419843316 + 0.01 * 6.849435329437256
Epoch 930, val loss: 1.6590665578842163
Epoch 940, training loss: 0.07160014659166336 = 0.0034265073481947184 + 0.01 * 6.817363739013672
Epoch 940, val loss: 1.6634811162948608
Epoch 950, training loss: 0.07141375541687012 = 0.003345722798258066 + 0.01 * 6.8068037033081055
Epoch 950, val loss: 1.6678788661956787
Epoch 960, training loss: 0.07142916321754456 = 0.0032684411853551865 + 0.01 * 6.8160719871521
Epoch 960, val loss: 1.6720925569534302
Epoch 970, training loss: 0.07120352238416672 = 0.0031944583170115948 + 0.01 * 6.800906658172607
Epoch 970, val loss: 1.6761674880981445
Epoch 980, training loss: 0.07111229002475739 = 0.0031235776841640472 + 0.01 * 6.798871040344238
Epoch 980, val loss: 1.680181860923767
Epoch 990, training loss: 0.07098764181137085 = 0.0030557371210306883 + 0.01 * 6.793190956115723
Epoch 990, val loss: 1.6840927600860596
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.019394874572754 = 1.9334264993667603 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9290441274642944
Epoch 10, training loss: 2.0094265937805176 = 1.9234589338302612 + 0.01 * 8.596756935119629
Epoch 10, val loss: 1.9196754693984985
Epoch 20, training loss: 1.9975491762161255 = 1.9115842580795288 + 0.01 * 8.596487998962402
Epoch 20, val loss: 1.9080561399459839
Epoch 30, training loss: 1.981520652770996 = 1.8955656290054321 + 0.01 * 8.595497131347656
Epoch 30, val loss: 1.8919397592544556
Epoch 40, training loss: 1.9587050676345825 = 1.872818946838379 + 0.01 * 8.588607788085938
Epoch 40, val loss: 1.8689748048782349
Epoch 50, training loss: 1.9275424480438232 = 1.842193841934204 + 0.01 * 8.53486442565918
Epoch 50, val loss: 1.8393269777297974
Epoch 60, training loss: 1.8914519548416138 = 1.809428334236145 + 0.01 * 8.202359199523926
Epoch 60, val loss: 1.8110768795013428
Epoch 70, training loss: 1.86008882522583 = 1.7800366878509521 + 0.01 * 8.005218505859375
Epoch 70, val loss: 1.7875080108642578
Epoch 80, training loss: 1.8199421167373657 = 1.7419517040252686 + 0.01 * 7.799037933349609
Epoch 80, val loss: 1.754289150238037
Epoch 90, training loss: 1.7634470462799072 = 1.6875908374786377 + 0.01 * 7.585621356964111
Epoch 90, val loss: 1.7069004774093628
Epoch 100, training loss: 1.6867715120315552 = 1.6126421689987183 + 0.01 * 7.412937164306641
Epoch 100, val loss: 1.6435353755950928
Epoch 110, training loss: 1.5959553718566895 = 1.5226770639419556 + 0.01 * 7.327833652496338
Epoch 110, val loss: 1.5686984062194824
Epoch 120, training loss: 1.5011917352676392 = 1.4284251928329468 + 0.01 * 7.2766499519348145
Epoch 120, val loss: 1.4919466972351074
Epoch 130, training loss: 1.4063687324523926 = 1.3341351747512817 + 0.01 * 7.223355770111084
Epoch 130, val loss: 1.4168225526809692
Epoch 140, training loss: 1.312620997428894 = 1.240890622138977 + 0.01 * 7.173040390014648
Epoch 140, val loss: 1.3458747863769531
Epoch 150, training loss: 1.222393274307251 = 1.1511508226394653 + 0.01 * 7.124246120452881
Epoch 150, val loss: 1.2805136442184448
Epoch 160, training loss: 1.1387156248092651 = 1.0678662061691284 + 0.01 * 7.084944725036621
Epoch 160, val loss: 1.2238764762878418
Epoch 170, training loss: 1.0623691082000732 = 0.9917439222335815 + 0.01 * 7.062521457672119
Epoch 170, val loss: 1.1751048564910889
Epoch 180, training loss: 0.9917331337928772 = 0.9212411642074585 + 0.01 * 7.049197196960449
Epoch 180, val loss: 1.132243037223816
Epoch 190, training loss: 0.9241939783096313 = 0.8537992238998413 + 0.01 * 7.039475440979004
Epoch 190, val loss: 1.0925095081329346
Epoch 200, training loss: 0.8574753999710083 = 0.7871438264846802 + 0.01 * 7.03315544128418
Epoch 200, val loss: 1.0541496276855469
Epoch 210, training loss: 0.7904374599456787 = 0.7201570868492126 + 0.01 * 7.028036117553711
Epoch 210, val loss: 1.016081690788269
Epoch 220, training loss: 0.7236993908882141 = 0.6534976363182068 + 0.01 * 7.0201735496521
Epoch 220, val loss: 0.9786975979804993
Epoch 230, training loss: 0.6593286395072937 = 0.5891906023025513 + 0.01 * 7.013805866241455
Epoch 230, val loss: 0.9437920451164246
Epoch 240, training loss: 0.5994851589202881 = 0.5294108986854553 + 0.01 * 7.007427215576172
Epoch 240, val loss: 0.9139635562896729
Epoch 250, training loss: 0.5451307892799377 = 0.4750879406929016 + 0.01 * 7.004283905029297
Epoch 250, val loss: 0.891040027141571
Epoch 260, training loss: 0.49557363986968994 = 0.42558661103248596 + 0.01 * 6.9987030029296875
Epoch 260, val loss: 0.8750200271606445
Epoch 270, training loss: 0.4495265483856201 = 0.3796008825302124 + 0.01 * 6.992566108703613
Epoch 270, val loss: 0.8648872971534729
Epoch 280, training loss: 0.40614116191864014 = 0.3362284302711487 + 0.01 * 6.991272926330566
Epoch 280, val loss: 0.8596643209457397
Epoch 290, training loss: 0.36537355184555054 = 0.2955225110054016 + 0.01 * 6.985105991363525
Epoch 290, val loss: 0.8587941527366638
Epoch 300, training loss: 0.327812135219574 = 0.2580261528491974 + 0.01 * 6.978597164154053
Epoch 300, val loss: 0.8621218204498291
Epoch 310, training loss: 0.29396694898605347 = 0.22425144910812378 + 0.01 * 6.971550941467285
Epoch 310, val loss: 0.8694174885749817
Epoch 320, training loss: 0.26407769322395325 = 0.19439536333084106 + 0.01 * 6.968232154846191
Epoch 320, val loss: 0.8804473280906677
Epoch 330, training loss: 0.2379971593618393 = 0.1683417111635208 + 0.01 * 6.965544700622559
Epoch 330, val loss: 0.8946782350540161
Epoch 340, training loss: 0.21540334820747375 = 0.14579209685325623 + 0.01 * 6.961126327514648
Epoch 340, val loss: 0.9114026427268982
Epoch 350, training loss: 0.19593296945095062 = 0.12640617787837982 + 0.01 * 6.95267915725708
Epoch 350, val loss: 0.9300625324249268
Epoch 360, training loss: 0.17927952110767365 = 0.10981427878141403 + 0.01 * 6.946524620056152
Epoch 360, val loss: 0.9500967264175415
Epoch 370, training loss: 0.16513188183307648 = 0.09565211087465286 + 0.01 * 6.947977542877197
Epoch 370, val loss: 0.9709697365760803
Epoch 380, training loss: 0.15308600664138794 = 0.08358527719974518 + 0.01 * 6.9500732421875
Epoch 380, val loss: 0.9922053813934326
Epoch 390, training loss: 0.14262712001800537 = 0.07330817729234695 + 0.01 * 6.931894302368164
Epoch 390, val loss: 1.0135053396224976
Epoch 400, training loss: 0.13386890292167664 = 0.06454196572303772 + 0.01 * 6.932693004608154
Epoch 400, val loss: 1.0347646474838257
Epoch 410, training loss: 0.1262768805027008 = 0.05706040933728218 + 0.01 * 6.921647071838379
Epoch 410, val loss: 1.0557523965835571
Epoch 420, training loss: 0.11995477974414825 = 0.05066880211234093 + 0.01 * 6.928598403930664
Epoch 420, val loss: 1.0763804912567139
Epoch 430, training loss: 0.11435923725366592 = 0.045216791331768036 + 0.01 * 6.914244651794434
Epoch 430, val loss: 1.0963879823684692
Epoch 440, training loss: 0.1096750795841217 = 0.040552616119384766 + 0.01 * 6.912246227264404
Epoch 440, val loss: 1.1158324480056763
Epoch 450, training loss: 0.10565243661403656 = 0.036544423550367355 + 0.01 * 6.910801410675049
Epoch 450, val loss: 1.134741187095642
Epoch 460, training loss: 0.1021350771188736 = 0.033084314316511154 + 0.01 * 6.90507698059082
Epoch 460, val loss: 1.1530903577804565
Epoch 470, training loss: 0.09897971898317337 = 0.03008280135691166 + 0.01 * 6.889692306518555
Epoch 470, val loss: 1.1708182096481323
Epoch 480, training loss: 0.09641831368207932 = 0.02746458537876606 + 0.01 * 6.8953728675842285
Epoch 480, val loss: 1.1879069805145264
Epoch 490, training loss: 0.09406581521034241 = 0.02516906149685383 + 0.01 * 6.889675617218018
Epoch 490, val loss: 1.204432725906372
Epoch 500, training loss: 0.09195531904697418 = 0.023148437961935997 + 0.01 * 6.880687713623047
Epoch 500, val loss: 1.2202879190444946
Epoch 510, training loss: 0.09006884694099426 = 0.0213597621768713 + 0.01 * 6.870908260345459
Epoch 510, val loss: 1.2356194257736206
Epoch 520, training loss: 0.0885019451379776 = 0.019770825281739235 + 0.01 * 6.873112201690674
Epoch 520, val loss: 1.2503929138183594
Epoch 530, training loss: 0.08706459403038025 = 0.01835418865084648 + 0.01 * 6.871040344238281
Epoch 530, val loss: 1.2646135091781616
Epoch 540, training loss: 0.08575592935085297 = 0.017086371779441833 + 0.01 * 6.866955757141113
Epoch 540, val loss: 1.2783477306365967
Epoch 550, training loss: 0.08445822447538376 = 0.015948770567774773 + 0.01 * 6.850944995880127
Epoch 550, val loss: 1.2916350364685059
Epoch 560, training loss: 0.08340123295783997 = 0.014923317357897758 + 0.01 * 6.84779167175293
Epoch 560, val loss: 1.3044105768203735
Epoch 570, training loss: 0.08236215263605118 = 0.013996431604027748 + 0.01 * 6.836572170257568
Epoch 570, val loss: 1.3167810440063477
Epoch 580, training loss: 0.0815933495759964 = 0.013155668042600155 + 0.01 * 6.843768119812012
Epoch 580, val loss: 1.3287211656570435
Epoch 590, training loss: 0.08071975409984589 = 0.012392059899866581 + 0.01 * 6.832769870758057
Epoch 590, val loss: 1.3402560949325562
Epoch 600, training loss: 0.08002723008394241 = 0.011697238311171532 + 0.01 * 6.832999229431152
Epoch 600, val loss: 1.3514033555984497
Epoch 610, training loss: 0.07929462194442749 = 0.01106206700205803 + 0.01 * 6.82325553894043
Epoch 610, val loss: 1.3622251749038696
Epoch 620, training loss: 0.07873477041721344 = 0.010479506105184555 + 0.01 * 6.825526237487793
Epoch 620, val loss: 1.3727389574050903
Epoch 630, training loss: 0.07802920043468475 = 0.009944216348230839 + 0.01 * 6.808498382568359
Epoch 630, val loss: 1.3829401731491089
Epoch 640, training loss: 0.07768803834915161 = 0.009451030753552914 + 0.01 * 6.823700904846191
Epoch 640, val loss: 1.3928289413452148
Epoch 650, training loss: 0.07719876617193222 = 0.00899676512926817 + 0.01 * 6.820200443267822
Epoch 650, val loss: 1.4024075269699097
Epoch 660, training loss: 0.0765824019908905 = 0.00857680756598711 + 0.01 * 6.800559043884277
Epoch 660, val loss: 1.4117558002471924
Epoch 670, training loss: 0.07634877413511276 = 0.008187653496861458 + 0.01 * 6.816112518310547
Epoch 670, val loss: 1.420827865600586
Epoch 680, training loss: 0.07583940029144287 = 0.007826846092939377 + 0.01 * 6.801255702972412
Epoch 680, val loss: 1.4296391010284424
Epoch 690, training loss: 0.07538191229104996 = 0.007491039577871561 + 0.01 * 6.789087772369385
Epoch 690, val loss: 1.4382147789001465
Epoch 700, training loss: 0.07505741715431213 = 0.007177987601608038 + 0.01 * 6.787942886352539
Epoch 700, val loss: 1.4465715885162354
Epoch 710, training loss: 0.07483427971601486 = 0.006885948125272989 + 0.01 * 6.794833660125732
Epoch 710, val loss: 1.4546452760696411
Epoch 720, training loss: 0.07450255751609802 = 0.006613690871745348 + 0.01 * 6.788886547088623
Epoch 720, val loss: 1.4625866413116455
Epoch 730, training loss: 0.07416162639856339 = 0.006358969025313854 + 0.01 * 6.780266284942627
Epoch 730, val loss: 1.470288634300232
Epoch 740, training loss: 0.07391222566366196 = 0.006120038218796253 + 0.01 * 6.779218673706055
Epoch 740, val loss: 1.4777928590774536
Epoch 750, training loss: 0.07371895760297775 = 0.005895968992263079 + 0.01 * 6.782298564910889
Epoch 750, val loss: 1.4851306676864624
Epoch 760, training loss: 0.07337683439254761 = 0.0056852311827242374 + 0.01 * 6.769160270690918
Epoch 760, val loss: 1.492295265197754
Epoch 770, training loss: 0.07325983792543411 = 0.005486820358783007 + 0.01 * 6.777301788330078
Epoch 770, val loss: 1.4992258548736572
Epoch 780, training loss: 0.07305315136909485 = 0.005300161428749561 + 0.01 * 6.775299072265625
Epoch 780, val loss: 1.5060033798217773
Epoch 790, training loss: 0.07273898273706436 = 0.005124145187437534 + 0.01 * 6.761484146118164
Epoch 790, val loss: 1.5125981569290161
Epoch 800, training loss: 0.0725579559803009 = 0.004958341829478741 + 0.01 * 6.7599616050720215
Epoch 800, val loss: 1.5190309286117554
Epoch 810, training loss: 0.07232736796140671 = 0.004801390692591667 + 0.01 * 6.752597808837891
Epoch 810, val loss: 1.5253781080245972
Epoch 820, training loss: 0.07244213670492172 = 0.004652529023587704 + 0.01 * 6.778960704803467
Epoch 820, val loss: 1.5314903259277344
Epoch 830, training loss: 0.07208477705717087 = 0.004511789884418249 + 0.01 * 6.757298946380615
Epoch 830, val loss: 1.5375360250473022
Epoch 840, training loss: 0.07183554023504257 = 0.0043783243745565414 + 0.01 * 6.745721340179443
Epoch 840, val loss: 1.5433653593063354
Epoch 850, training loss: 0.07176660746335983 = 0.004251645412296057 + 0.01 * 6.751496315002441
Epoch 850, val loss: 1.5490728616714478
Epoch 860, training loss: 0.07156664878129959 = 0.004131457302719355 + 0.01 * 6.743519306182861
Epoch 860, val loss: 1.5546653270721436
Epoch 870, training loss: 0.07150021195411682 = 0.0040171220898628235 + 0.01 * 6.748309135437012
Epoch 870, val loss: 1.560124397277832
Epoch 880, training loss: 0.07127553224563599 = 0.003908321727067232 + 0.01 * 6.736721038818359
Epoch 880, val loss: 1.565464973449707
Epoch 890, training loss: 0.07116470485925674 = 0.0038046916015446186 + 0.01 * 6.736001491546631
Epoch 890, val loss: 1.5706956386566162
Epoch 900, training loss: 0.07106883823871613 = 0.0037059655878692865 + 0.01 * 6.736287593841553
Epoch 900, val loss: 1.5757955312728882
Epoch 910, training loss: 0.07092198729515076 = 0.003611758118495345 + 0.01 * 6.73102331161499
Epoch 910, val loss: 1.5807867050170898
Epoch 920, training loss: 0.07083221524953842 = 0.0035218624398112297 + 0.01 * 6.7310357093811035
Epoch 920, val loss: 1.5856664180755615
Epoch 930, training loss: 0.0707424134016037 = 0.003436085768043995 + 0.01 * 6.730632305145264
Epoch 930, val loss: 1.5904978513717651
Epoch 940, training loss: 0.07057511061429977 = 0.003353974549099803 + 0.01 * 6.722113609313965
Epoch 940, val loss: 1.595167636871338
Epoch 950, training loss: 0.0709308534860611 = 0.003275464754551649 + 0.01 * 6.765539169311523
Epoch 950, val loss: 1.5997260808944702
Epoch 960, training loss: 0.07054322957992554 = 0.003200430888682604 + 0.01 * 6.734279632568359
Epoch 960, val loss: 1.6042293310165405
Epoch 970, training loss: 0.07031760364770889 = 0.003128520678728819 + 0.01 * 6.718908309936523
Epoch 970, val loss: 1.6086097955703735
Epoch 980, training loss: 0.07025327533483505 = 0.0030595010612159967 + 0.01 * 6.719377517700195
Epoch 980, val loss: 1.6129075288772583
Epoch 990, training loss: 0.07028502970933914 = 0.002993243746459484 + 0.01 * 6.7291789054870605
Epoch 990, val loss: 1.6170927286148071
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 2.0476343631744385 = 1.9616658687591553 + 0.01 * 8.596856117248535
Epoch 0, val loss: 1.963648796081543
Epoch 10, training loss: 2.037152051925659 = 1.9511840343475342 + 0.01 * 8.596810340881348
Epoch 10, val loss: 1.9526439905166626
Epoch 20, training loss: 2.0243544578552246 = 1.9383881092071533 + 0.01 * 8.59663200378418
Epoch 20, val loss: 1.9390419721603394
Epoch 30, training loss: 2.006556749343872 = 1.9205961227416992 + 0.01 * 8.596067428588867
Epoch 30, val loss: 1.9203420877456665
Epoch 40, training loss: 1.9806504249572754 = 1.8947200775146484 + 0.01 * 8.593032836914062
Epoch 40, val loss: 1.8938040733337402
Epoch 50, training loss: 1.944873332977295 = 1.8591595888137817 + 0.01 * 8.571375846862793
Epoch 50, val loss: 1.8593510389328003
Epoch 60, training loss: 1.9056077003479004 = 1.8207449913024902 + 0.01 * 8.486268997192383
Epoch 60, val loss: 1.8268485069274902
Epoch 70, training loss: 1.8739553689956665 = 1.792185664176941 + 0.01 * 8.176965713500977
Epoch 70, val loss: 1.8058122396469116
Epoch 80, training loss: 1.8415780067443848 = 1.76067316532135 + 0.01 * 8.090486526489258
Epoch 80, val loss: 1.7775202989578247
Epoch 90, training loss: 1.7964073419570923 = 1.717589020729065 + 0.01 * 7.881837368011475
Epoch 90, val loss: 1.7395795583724976
Epoch 100, training loss: 1.7322324514389038 = 1.6558830738067627 + 0.01 * 7.6349358558654785
Epoch 100, val loss: 1.6878381967544556
Epoch 110, training loss: 1.6483566761016846 = 1.5730122327804565 + 0.01 * 7.534448623657227
Epoch 110, val loss: 1.6183983087539673
Epoch 120, training loss: 1.5510849952697754 = 1.4759522676467896 + 0.01 * 7.513272285461426
Epoch 120, val loss: 1.5391521453857422
Epoch 130, training loss: 1.4473702907562256 = 1.372373342514038 + 0.01 * 7.49968957901001
Epoch 130, val loss: 1.4554088115692139
Epoch 140, training loss: 1.3372604846954346 = 1.2624536752700806 + 0.01 * 7.480684757232666
Epoch 140, val loss: 1.3670858144760132
Epoch 150, training loss: 1.2203484773635864 = 1.1458381414413452 + 0.01 * 7.451028347015381
Epoch 150, val loss: 1.2750248908996582
Epoch 160, training loss: 1.1019545793533325 = 1.0279412269592285 + 0.01 * 7.401331901550293
Epoch 160, val loss: 1.1847453117370605
Epoch 170, training loss: 0.9911652207374573 = 0.9178657531738281 + 0.01 * 7.329946517944336
Epoch 170, val loss: 1.1044552326202393
Epoch 180, training loss: 0.895270824432373 = 0.8226646780967712 + 0.01 * 7.2606120109558105
Epoch 180, val loss: 1.0400651693344116
Epoch 190, training loss: 0.8155093193054199 = 0.7432616353034973 + 0.01 * 7.224771499633789
Epoch 190, val loss: 0.9922726154327393
Epoch 200, training loss: 0.7483384013175964 = 0.676368236541748 + 0.01 * 7.197018623352051
Epoch 200, val loss: 0.9579646587371826
Epoch 210, training loss: 0.6898003220558167 = 0.6180378794670105 + 0.01 * 7.17624568939209
Epoch 210, val loss: 0.9332541823387146
Epoch 220, training loss: 0.6368538737297058 = 0.5652820467948914 + 0.01 * 7.157181739807129
Epoch 220, val loss: 0.9152966737747192
Epoch 230, training loss: 0.5877630114555359 = 0.5163783431053162 + 0.01 * 7.138468265533447
Epoch 230, val loss: 0.9021289348602295
Epoch 240, training loss: 0.5419086813926697 = 0.47070053219795227 + 0.01 * 7.120815277099609
Epoch 240, val loss: 0.8927685022354126
Epoch 250, training loss: 0.49930983781814575 = 0.42834344506263733 + 0.01 * 7.096639156341553
Epoch 250, val loss: 0.887475311756134
Epoch 260, training loss: 0.46042972803115845 = 0.3896608054637909 + 0.01 * 7.0768914222717285
Epoch 260, val loss: 0.8870342373847961
Epoch 270, training loss: 0.4252760112285614 = 0.35476183891296387 + 0.01 * 7.051417350769043
Epoch 270, val loss: 0.8915431499481201
Epoch 280, training loss: 0.39376020431518555 = 0.3233785033226013 + 0.01 * 7.0381693840026855
Epoch 280, val loss: 0.9003580212593079
Epoch 290, training loss: 0.3651578426361084 = 0.2950081527233124 + 0.01 * 7.014970779418945
Epoch 290, val loss: 0.9124953150749207
Epoch 300, training loss: 0.33901798725128174 = 0.26898688077926636 + 0.01 * 7.003111839294434
Epoch 300, val loss: 0.927111029624939
Epoch 310, training loss: 0.3145540952682495 = 0.24462834000587463 + 0.01 * 6.9925761222839355
Epoch 310, val loss: 0.9436261653900146
Epoch 320, training loss: 0.29126203060150146 = 0.22139523923397064 + 0.01 * 6.986678600311279
Epoch 320, val loss: 0.9614879488945007
Epoch 330, training loss: 0.2687959671020508 = 0.19905498623847961 + 0.01 * 6.9740986824035645
Epoch 330, val loss: 0.980269730091095
Epoch 340, training loss: 0.24738043546676636 = 0.1777300238609314 + 0.01 * 6.965041160583496
Epoch 340, val loss: 1.0000741481781006
Epoch 350, training loss: 0.22739288210868835 = 0.1577395498752594 + 0.01 * 6.965333938598633
Epoch 350, val loss: 1.0210940837860107
Epoch 360, training loss: 0.20888660848140717 = 0.13937391340732574 + 0.01 * 6.951269626617432
Epoch 360, val loss: 1.0433855056762695
Epoch 370, training loss: 0.19246262311935425 = 0.12281634658575058 + 0.01 * 6.96462869644165
Epoch 370, val loss: 1.0670018196105957
Epoch 380, training loss: 0.177545964717865 = 0.10814890265464783 + 0.01 * 6.9397053718566895
Epoch 380, val loss: 1.0919219255447388
Epoch 390, training loss: 0.1647084653377533 = 0.09530719369649887 + 0.01 * 6.940126419067383
Epoch 390, val loss: 1.1179641485214233
Epoch 400, training loss: 0.1534889042377472 = 0.08415620774030685 + 0.01 * 6.933270454406738
Epoch 400, val loss: 1.1449321508407593
Epoch 410, training loss: 0.1438140720129013 = 0.07452326267957687 + 0.01 * 6.929081439971924
Epoch 410, val loss: 1.1724622249603271
Epoch 420, training loss: 0.13543689250946045 = 0.06621561199426651 + 0.01 * 6.922128677368164
Epoch 420, val loss: 1.2002770900726318
Epoch 430, training loss: 0.1282781958580017 = 0.05904887244105339 + 0.01 * 6.922932147979736
Epoch 430, val loss: 1.2281124591827393
Epoch 440, training loss: 0.1220414936542511 = 0.052858587354421616 + 0.01 * 6.918290138244629
Epoch 440, val loss: 1.2556811571121216
Epoch 450, training loss: 0.11660706996917725 = 0.04749807342886925 + 0.01 * 6.9108991622924805
Epoch 450, val loss: 1.282938838005066
Epoch 460, training loss: 0.1118406355381012 = 0.04284119978547096 + 0.01 * 6.8999433517456055
Epoch 460, val loss: 1.3096705675125122
Epoch 470, training loss: 0.1077805608510971 = 0.03878146409988403 + 0.01 * 6.899909973144531
Epoch 470, val loss: 1.335817575454712
Epoch 480, training loss: 0.10437875986099243 = 0.0352376289665699 + 0.01 * 6.914113521575928
Epoch 480, val loss: 1.3613280057907104
Epoch 490, training loss: 0.10110943019390106 = 0.032133303582668304 + 0.01 * 6.897613048553467
Epoch 490, val loss: 1.3861089944839478
Epoch 500, training loss: 0.09823323786258698 = 0.02940000593662262 + 0.01 * 6.8833231925964355
Epoch 500, val loss: 1.4102072715759277
Epoch 510, training loss: 0.09592120349407196 = 0.0269843228161335 + 0.01 * 6.893688678741455
Epoch 510, val loss: 1.4336446523666382
Epoch 520, training loss: 0.09364676475524902 = 0.024844054132699966 + 0.01 * 6.880270481109619
Epoch 520, val loss: 1.4563347101211548
Epoch 530, training loss: 0.09170044213533401 = 0.022940943017601967 + 0.01 * 6.875950336456299
Epoch 530, val loss: 1.4783328771591187
Epoch 540, training loss: 0.08999211341142654 = 0.021243013441562653 + 0.01 * 6.874910354614258
Epoch 540, val loss: 1.4996156692504883
Epoch 550, training loss: 0.08838962018489838 = 0.01972396858036518 + 0.01 * 6.866565704345703
Epoch 550, val loss: 1.5202282667160034
Epoch 560, training loss: 0.08711309731006622 = 0.018360327929258347 + 0.01 * 6.875277042388916
Epoch 560, val loss: 1.540167212486267
Epoch 570, training loss: 0.08578339964151382 = 0.017134055495262146 + 0.01 * 6.864934921264648
Epoch 570, val loss: 1.5594751834869385
Epoch 580, training loss: 0.08461892604827881 = 0.016027186065912247 + 0.01 * 6.8591742515563965
Epoch 580, val loss: 1.5781023502349854
Epoch 590, training loss: 0.08353346586227417 = 0.015025577507913113 + 0.01 * 6.8507890701293945
Epoch 590, val loss: 1.596174955368042
Epoch 600, training loss: 0.08268444985151291 = 0.014116497710347176 + 0.01 * 6.856795787811279
Epoch 600, val loss: 1.6136361360549927
Epoch 610, training loss: 0.08173903822898865 = 0.013289365917444229 + 0.01 * 6.844967365264893
Epoch 610, val loss: 1.6305586099624634
Epoch 620, training loss: 0.08096759766340256 = 0.012534722685813904 + 0.01 * 6.843287467956543
Epoch 620, val loss: 1.6469624042510986
Epoch 630, training loss: 0.0802033394575119 = 0.011845100671052933 + 0.01 * 6.835824012756348
Epoch 630, val loss: 1.6628837585449219
Epoch 640, training loss: 0.07949236780405045 = 0.011213121935725212 + 0.01 * 6.8279242515563965
Epoch 640, val loss: 1.678322196006775
Epoch 650, training loss: 0.07886121422052383 = 0.010632499121129513 + 0.01 * 6.822871208190918
Epoch 650, val loss: 1.6933016777038574
Epoch 660, training loss: 0.07843004912137985 = 0.010098871774971485 + 0.01 * 6.833117485046387
Epoch 660, val loss: 1.70784592628479
Epoch 670, training loss: 0.07782918959856033 = 0.009607315994799137 + 0.01 * 6.822187423706055
Epoch 670, val loss: 1.7218711376190186
Epoch 680, training loss: 0.07726740092039108 = 0.009152800776064396 + 0.01 * 6.811460018157959
Epoch 680, val loss: 1.7355461120605469
Epoch 690, training loss: 0.07696015387773514 = 0.00873155053704977 + 0.01 * 6.8228607177734375
Epoch 690, val loss: 1.748862862586975
Epoch 700, training loss: 0.07648372650146484 = 0.008340972475707531 + 0.01 * 6.814275741577148
Epoch 700, val loss: 1.761797308921814
Epoch 710, training loss: 0.07602652162313461 = 0.007978319190442562 + 0.01 * 6.8048200607299805
Epoch 710, val loss: 1.774344563484192
Epoch 720, training loss: 0.07559967786073685 = 0.007641259115189314 + 0.01 * 6.795842170715332
Epoch 720, val loss: 1.7865402698516846
Epoch 730, training loss: 0.07530312240123749 = 0.007326897233724594 + 0.01 * 6.797622203826904
Epoch 730, val loss: 1.7984451055526733
Epoch 740, training loss: 0.07491844147443771 = 0.007033160421997309 + 0.01 * 6.788527965545654
Epoch 740, val loss: 1.8100159168243408
Epoch 750, training loss: 0.07470561563968658 = 0.006758453790098429 + 0.01 * 6.7947163581848145
Epoch 750, val loss: 1.8213061094284058
Epoch 760, training loss: 0.0746101588010788 = 0.0065013146959245205 + 0.01 * 6.810884475708008
Epoch 760, val loss: 1.8322725296020508
Epoch 770, training loss: 0.07403599470853806 = 0.006260151509195566 + 0.01 * 6.777584552764893
Epoch 770, val loss: 1.8430113792419434
Epoch 780, training loss: 0.07377798110246658 = 0.006033593323081732 + 0.01 * 6.774439334869385
Epoch 780, val loss: 1.8534557819366455
Epoch 790, training loss: 0.07367558032274246 = 0.0058206310495734215 + 0.01 * 6.785495281219482
Epoch 790, val loss: 1.8636044263839722
Epoch 800, training loss: 0.07328919321298599 = 0.0056203678250312805 + 0.01 * 6.76688289642334
Epoch 800, val loss: 1.8735407590866089
Epoch 810, training loss: 0.07312937825918198 = 0.00543147511780262 + 0.01 * 6.7697906494140625
Epoch 810, val loss: 1.8832603693008423
Epoch 820, training loss: 0.07300812005996704 = 0.005253256298601627 + 0.01 * 6.775486946105957
Epoch 820, val loss: 1.8927229642868042
Epoch 830, training loss: 0.07272294163703918 = 0.005084640346467495 + 0.01 * 6.763830184936523
Epoch 830, val loss: 1.9019745588302612
Epoch 840, training loss: 0.07262856513261795 = 0.004925147630274296 + 0.01 * 6.770341873168945
Epoch 840, val loss: 1.9109857082366943
Epoch 850, training loss: 0.07230294495820999 = 0.004774125292897224 + 0.01 * 6.75288200378418
Epoch 850, val loss: 1.919840931892395
Epoch 860, training loss: 0.07221551984548569 = 0.004630991257727146 + 0.01 * 6.758453369140625
Epoch 860, val loss: 1.9284543991088867
Epoch 870, training loss: 0.07208248972892761 = 0.004495398607105017 + 0.01 * 6.75870943069458
Epoch 870, val loss: 1.9368815422058105
Epoch 880, training loss: 0.07203086465597153 = 0.0043663689866662025 + 0.01 * 6.766449928283691
Epoch 880, val loss: 1.9451569318771362
Epoch 890, training loss: 0.07172531634569168 = 0.004243983421474695 + 0.01 * 6.748133182525635
Epoch 890, val loss: 1.9532642364501953
Epoch 900, training loss: 0.07154972106218338 = 0.004127200227230787 + 0.01 * 6.742252349853516
Epoch 900, val loss: 1.9611622095108032
Epoch 910, training loss: 0.07156047224998474 = 0.004016132093966007 + 0.01 * 6.754434108734131
Epoch 910, val loss: 1.9689353704452515
Epoch 920, training loss: 0.0712561160326004 = 0.003910297993570566 + 0.01 * 6.73458194732666
Epoch 920, val loss: 1.9764971733093262
Epoch 930, training loss: 0.07130421698093414 = 0.0038090867456048727 + 0.01 * 6.749513149261475
Epoch 930, val loss: 1.9839434623718262
Epoch 940, training loss: 0.07110048830509186 = 0.003712703473865986 + 0.01 * 6.738778591156006
Epoch 940, val loss: 1.9912785291671753
Epoch 950, training loss: 0.07090152055025101 = 0.0036203654017299414 + 0.01 * 6.728116035461426
Epoch 950, val loss: 1.9983927011489868
Epoch 960, training loss: 0.07084283977746964 = 0.003532213857397437 + 0.01 * 6.731062889099121
Epoch 960, val loss: 2.005417823791504
Epoch 970, training loss: 0.07073225826025009 = 0.003448005300015211 + 0.01 * 6.7284255027771
Epoch 970, val loss: 2.012256383895874
Epoch 980, training loss: 0.07072856277227402 = 0.0033674936275929213 + 0.01 * 6.736107349395752
Epoch 980, val loss: 2.0190181732177734
Epoch 990, training loss: 0.07061752676963806 = 0.0032904241234064102 + 0.01 * 6.732710361480713
Epoch 990, val loss: 2.0255892276763916
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8191881918819188
The final CL Acc:0.76543, 0.02058, The final GNN Acc:0.81831, 0.00066
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13110])
remove edge: torch.Size([2, 7822])
updated graph: torch.Size([2, 10376])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0258729457855225 = 1.9399044513702393 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.9347554445266724
Epoch 10, training loss: 2.0161805152893066 = 1.9302126169204712 + 0.01 * 8.59679126739502
Epoch 10, val loss: 1.924620270729065
Epoch 20, training loss: 2.0044119358062744 = 1.91844642162323 + 0.01 * 8.59656047821045
Epoch 20, val loss: 1.9122848510742188
Epoch 30, training loss: 1.9881411790847778 = 1.902184247970581 + 0.01 * 8.595697402954102
Epoch 30, val loss: 1.8952405452728271
Epoch 40, training loss: 1.9642242193222046 = 1.87831711769104 + 0.01 * 8.590713500976562
Epoch 40, val loss: 1.8702528476715088
Epoch 50, training loss: 1.9299942255020142 = 1.8444180488586426 + 0.01 * 8.557618141174316
Epoch 50, val loss: 1.8357731103897095
Epoch 60, training loss: 1.8878753185272217 = 1.8037188053131104 + 0.01 * 8.415651321411133
Epoch 60, val loss: 1.7980265617370605
Epoch 70, training loss: 1.8448110818862915 = 1.7645927667617798 + 0.01 * 8.021833419799805
Epoch 70, val loss: 1.7661701440811157
Epoch 80, training loss: 1.79707932472229 = 1.71880304813385 + 0.01 * 7.827630043029785
Epoch 80, val loss: 1.727586269378662
Epoch 90, training loss: 1.7301406860351562 = 1.6545454263687134 + 0.01 * 7.559527397155762
Epoch 90, val loss: 1.6704843044281006
Epoch 100, training loss: 1.6421540975570679 = 1.5683542490005493 + 0.01 * 7.3799896240234375
Epoch 100, val loss: 1.5942285060882568
Epoch 110, training loss: 1.5351710319519043 = 1.4621212482452393 + 0.01 * 7.3049774169921875
Epoch 110, val loss: 1.5020031929016113
Epoch 120, training loss: 1.417330265045166 = 1.3446331024169922 + 0.01 * 7.269712924957275
Epoch 120, val loss: 1.4035954475402832
Epoch 130, training loss: 1.2947827577590942 = 1.2223436832427979 + 0.01 * 7.243907451629639
Epoch 130, val loss: 1.3042054176330566
Epoch 140, training loss: 1.17196524143219 = 1.0997209548950195 + 0.01 * 7.224428176879883
Epoch 140, val loss: 1.2068290710449219
Epoch 150, training loss: 1.0540683269500732 = 0.9820034503936768 + 0.01 * 7.20648717880249
Epoch 150, val loss: 1.1140590906143188
Epoch 160, training loss: 0.946406364440918 = 0.8745194673538208 + 0.01 * 7.188689708709717
Epoch 160, val loss: 1.0305589437484741
Epoch 170, training loss: 0.8519529104232788 = 0.7802267670631409 + 0.01 * 7.172611236572266
Epoch 170, val loss: 0.9599120020866394
Epoch 180, training loss: 0.7703040838241577 = 0.6987209320068359 + 0.01 * 7.158312797546387
Epoch 180, val loss: 0.9017260074615479
Epoch 190, training loss: 0.6986541748046875 = 0.6272028088569641 + 0.01 * 7.145139694213867
Epoch 190, val loss: 0.8539396524429321
Epoch 200, training loss: 0.6338534355163574 = 0.5625335574150085 + 0.01 * 7.131988525390625
Epoch 200, val loss: 0.8137577176094055
Epoch 210, training loss: 0.5736649036407471 = 0.5025005340576172 + 0.01 * 7.116438865661621
Epoch 210, val loss: 0.779081404209137
Epoch 220, training loss: 0.517127275466919 = 0.44616401195526123 + 0.01 * 7.0963263511657715
Epoch 220, val loss: 0.7491446733474731
Epoch 230, training loss: 0.46407023072242737 = 0.3933190107345581 + 0.01 * 7.0751214027404785
Epoch 230, val loss: 0.723354697227478
Epoch 240, training loss: 0.4146387279033661 = 0.3441183567047119 + 0.01 * 7.052036285400391
Epoch 240, val loss: 0.7014442086219788
Epoch 250, training loss: 0.3693152666091919 = 0.2988969087600708 + 0.01 * 7.041836261749268
Epoch 250, val loss: 0.6834732890129089
Epoch 260, training loss: 0.32833248376846313 = 0.25806924700737 + 0.01 * 7.026322364807129
Epoch 260, val loss: 0.6696826815605164
Epoch 270, training loss: 0.29206985235214233 = 0.2219201922416687 + 0.01 * 7.01496696472168
Epoch 270, val loss: 0.6602270603179932
Epoch 280, training loss: 0.2606039345264435 = 0.19051501154899597 + 0.01 * 7.008892059326172
Epoch 280, val loss: 0.6552059650421143
Epoch 290, training loss: 0.23369480669498444 = 0.16365520656108856 + 0.01 * 7.003960132598877
Epoch 290, val loss: 0.6544707417488098
Epoch 300, training loss: 0.21104875206947327 = 0.14093294739723206 + 0.01 * 7.011579990386963
Epoch 300, val loss: 0.6574074625968933
Epoch 310, training loss: 0.19177064299583435 = 0.1218356192111969 + 0.01 * 6.993503093719482
Epoch 310, val loss: 0.6634078621864319
Epoch 320, training loss: 0.17571496963500977 = 0.10581797361373901 + 0.01 * 6.989698886871338
Epoch 320, val loss: 0.6717525720596313
Epoch 330, training loss: 0.16225138306617737 = 0.09238018840551376 + 0.01 * 6.987120628356934
Epoch 330, val loss: 0.68177330493927
Epoch 340, training loss: 0.15085363388061523 = 0.08108671754598618 + 0.01 * 6.976690769195557
Epoch 340, val loss: 0.692997932434082
Epoch 350, training loss: 0.14124973118305206 = 0.07156313210725784 + 0.01 * 6.9686598777771
Epoch 350, val loss: 0.704914927482605
Epoch 360, training loss: 0.13313569128513336 = 0.06349758058786392 + 0.01 * 6.963810920715332
Epoch 360, val loss: 0.7172955274581909
Epoch 370, training loss: 0.12618249654769897 = 0.05663645640015602 + 0.01 * 6.954604625701904
Epoch 370, val loss: 0.7298678159713745
Epoch 380, training loss: 0.12025199085474014 = 0.050764985382556915 + 0.01 * 6.948700904846191
Epoch 380, val loss: 0.7424871921539307
Epoch 390, training loss: 0.11511492729187012 = 0.04571187123656273 + 0.01 * 6.940305709838867
Epoch 390, val loss: 0.7550773620605469
Epoch 400, training loss: 0.11089976131916046 = 0.04134122282266617 + 0.01 * 6.9558539390563965
Epoch 400, val loss: 0.7675364017486572
Epoch 410, training loss: 0.1068030297756195 = 0.037548087537288666 + 0.01 * 6.925494194030762
Epoch 410, val loss: 0.7797459959983826
Epoch 420, training loss: 0.10344058275222778 = 0.03423786535859108 + 0.01 * 6.920272350311279
Epoch 420, val loss: 0.7917369604110718
Epoch 430, training loss: 0.10046716034412384 = 0.03133445978164673 + 0.01 * 6.913270473480225
Epoch 430, val loss: 0.803511381149292
Epoch 440, training loss: 0.0978555828332901 = 0.02877892181277275 + 0.01 * 6.907666206359863
Epoch 440, val loss: 0.8149381279945374
Epoch 450, training loss: 0.09550993889570236 = 0.02651774324476719 + 0.01 * 6.899219989776611
Epoch 450, val loss: 0.8261340856552124
Epoch 460, training loss: 0.09346488863229752 = 0.024508165195584297 + 0.01 * 6.89567232131958
Epoch 460, val loss: 0.8370349407196045
Epoch 470, training loss: 0.09161790460348129 = 0.02271571010351181 + 0.01 * 6.890219688415527
Epoch 470, val loss: 0.8476276397705078
Epoch 480, training loss: 0.08999229967594147 = 0.021111974492669106 + 0.01 * 6.888032913208008
Epoch 480, val loss: 0.8579314351081848
Epoch 490, training loss: 0.0885683000087738 = 0.019672423601150513 + 0.01 * 6.889587879180908
Epoch 490, val loss: 0.8679263591766357
Epoch 500, training loss: 0.08717921376228333 = 0.018376043066382408 + 0.01 * 6.880316734313965
Epoch 500, val loss: 0.877685546875
Epoch 510, training loss: 0.08590373396873474 = 0.01720471866428852 + 0.01 * 6.86990213394165
Epoch 510, val loss: 0.8871464133262634
Epoch 520, training loss: 0.08481472730636597 = 0.01614321954548359 + 0.01 * 6.867151260375977
Epoch 520, val loss: 0.8963742852210999
Epoch 530, training loss: 0.08382657915353775 = 0.015178988687694073 + 0.01 * 6.86475944519043
Epoch 530, val loss: 0.9053153991699219
Epoch 540, training loss: 0.08292745053768158 = 0.014300660230219364 + 0.01 * 6.862679481506348
Epoch 540, val loss: 0.9140323996543884
Epoch 550, training loss: 0.08203864097595215 = 0.013498264364898205 + 0.01 * 6.854037761688232
Epoch 550, val loss: 0.9225054979324341
Epoch 560, training loss: 0.08127272129058838 = 0.012763592414557934 + 0.01 * 6.850913047790527
Epoch 560, val loss: 0.9307512640953064
Epoch 570, training loss: 0.08063043653964996 = 0.012090083211660385 + 0.01 * 6.854034900665283
Epoch 570, val loss: 0.9387655258178711
Epoch 580, training loss: 0.07991275936365128 = 0.011470439843833447 + 0.01 * 6.844232559204102
Epoch 580, val loss: 0.9465504288673401
Epoch 590, training loss: 0.0793057382106781 = 0.01089879684150219 + 0.01 * 6.840693950653076
Epoch 590, val loss: 0.9541545510292053
Epoch 600, training loss: 0.07878971844911575 = 0.010370445437729359 + 0.01 * 6.8419270515441895
Epoch 600, val loss: 0.9615686535835266
Epoch 610, training loss: 0.0782884731888771 = 0.009881915524601936 + 0.01 * 6.84065580368042
Epoch 610, val loss: 0.9688243865966797
Epoch 620, training loss: 0.0777137279510498 = 0.009429375641047955 + 0.01 * 6.828435897827148
Epoch 620, val loss: 0.9758365154266357
Epoch 630, training loss: 0.07728265970945358 = 0.00900876522064209 + 0.01 * 6.827389240264893
Epoch 630, val loss: 0.982694149017334
Epoch 640, training loss: 0.0768713727593422 = 0.008617023006081581 + 0.01 * 6.825435161590576
Epoch 640, val loss: 0.9894064664840698
Epoch 650, training loss: 0.07658923417329788 = 0.00825200043618679 + 0.01 * 6.833723068237305
Epoch 650, val loss: 0.9959602355957031
Epoch 660, training loss: 0.07612165808677673 = 0.007911899127066135 + 0.01 * 6.8209757804870605
Epoch 660, val loss: 1.0022882223129272
Epoch 670, training loss: 0.07572797685861588 = 0.00759387481957674 + 0.01 * 6.81341028213501
Epoch 670, val loss: 1.0084995031356812
Epoch 680, training loss: 0.0754081979393959 = 0.007296084426343441 + 0.01 * 6.811211585998535
Epoch 680, val loss: 1.014559030532837
Epoch 690, training loss: 0.07508397102355957 = 0.007016761694103479 + 0.01 * 6.806720733642578
Epoch 690, val loss: 1.0204713344573975
Epoch 700, training loss: 0.07485546171665192 = 0.006754674948751926 + 0.01 * 6.8100786209106445
Epoch 700, val loss: 1.0262701511383057
Epoch 710, training loss: 0.07458900660276413 = 0.006508946418762207 + 0.01 * 6.8080058097839355
Epoch 710, val loss: 1.0318504571914673
Epoch 720, training loss: 0.07422740757465363 = 0.006277722772210836 + 0.01 * 6.794968128204346
Epoch 720, val loss: 1.037340521812439
Epoch 730, training loss: 0.07428616285324097 = 0.006059679202735424 + 0.01 * 6.822649002075195
Epoch 730, val loss: 1.0426846742630005
Epoch 740, training loss: 0.07378299534320831 = 0.005854318849742413 + 0.01 * 6.792868137359619
Epoch 740, val loss: 1.0479259490966797
Epoch 750, training loss: 0.07360485196113586 = 0.005660473369061947 + 0.01 * 6.794437885284424
Epoch 750, val loss: 1.053073763847351
Epoch 760, training loss: 0.07334312796592712 = 0.005477278959006071 + 0.01 * 6.786584854125977
Epoch 760, val loss: 1.0580534934997559
Epoch 770, training loss: 0.07310318946838379 = 0.005303902085870504 + 0.01 * 6.779928684234619
Epoch 770, val loss: 1.0629435777664185
Epoch 780, training loss: 0.07298976927995682 = 0.005139580927789211 + 0.01 * 6.7850189208984375
Epoch 780, val loss: 1.0677361488342285
Epoch 790, training loss: 0.07274220138788223 = 0.004983904305845499 + 0.01 * 6.775830268859863
Epoch 790, val loss: 1.07237708568573
Epoch 800, training loss: 0.07251609116792679 = 0.00483623705804348 + 0.01 * 6.7679853439331055
Epoch 800, val loss: 1.07696533203125
Epoch 810, training loss: 0.07254453003406525 = 0.004695971962064505 + 0.01 * 6.784855365753174
Epoch 810, val loss: 1.0814619064331055
Epoch 820, training loss: 0.0722937136888504 = 0.004563063383102417 + 0.01 * 6.773065090179443
Epoch 820, val loss: 1.0857659578323364
Epoch 830, training loss: 0.07198380678892136 = 0.004436439368873835 + 0.01 * 6.75473690032959
Epoch 830, val loss: 1.0900310277938843
Epoch 840, training loss: 0.07202813774347305 = 0.00431579165160656 + 0.01 * 6.771234512329102
Epoch 840, val loss: 1.094213604927063
Epoch 850, training loss: 0.07172176986932755 = 0.004201066680252552 + 0.01 * 6.752070426940918
Epoch 850, val loss: 1.0982561111450195
Epoch 860, training loss: 0.07158666849136353 = 0.0040914928540587425 + 0.01 * 6.749517917633057
Epoch 860, val loss: 1.1022372245788574
Epoch 870, training loss: 0.07146581262350082 = 0.003986880648881197 + 0.01 * 6.747893333435059
Epoch 870, val loss: 1.1061549186706543
Epoch 880, training loss: 0.07129421085119247 = 0.0038871874567121267 + 0.01 * 6.740703105926514
Epoch 880, val loss: 1.110006332397461
Epoch 890, training loss: 0.07120158523321152 = 0.0037920663598924875 + 0.01 * 6.740952014923096
Epoch 890, val loss: 1.1137272119522095
Epoch 900, training loss: 0.07106778025627136 = 0.0037009972147643566 + 0.01 * 6.736678600311279
Epoch 900, val loss: 1.1174026727676392
Epoch 910, training loss: 0.07092563062906265 = 0.0036140407901257277 + 0.01 * 6.731159687042236
Epoch 910, val loss: 1.120953917503357
Epoch 920, training loss: 0.07081522792577744 = 0.0035305602941662073 + 0.01 * 6.728466510772705
Epoch 920, val loss: 1.1244703531265259
Epoch 930, training loss: 0.07083891332149506 = 0.003450534539297223 + 0.01 * 6.738837718963623
Epoch 930, val loss: 1.1279417276382446
Epoch 940, training loss: 0.07060384750366211 = 0.003373987739905715 + 0.01 * 6.722985744476318
Epoch 940, val loss: 1.1313667297363281
Epoch 950, training loss: 0.07071489095687866 = 0.003300665644928813 + 0.01 * 6.741422653198242
Epoch 950, val loss: 1.1346126794815063
Epoch 960, training loss: 0.07048886269330978 = 0.0032301456667482853 + 0.01 * 6.725871562957764
Epoch 960, val loss: 1.137908697128296
Epoch 970, training loss: 0.07031654566526413 = 0.0031624517869204283 + 0.01 * 6.715409278869629
Epoch 970, val loss: 1.1410915851593018
Epoch 980, training loss: 0.0703851655125618 = 0.003097445471212268 + 0.01 * 6.728772163391113
Epoch 980, val loss: 1.1442313194274902
Epoch 990, training loss: 0.07014597952365875 = 0.0030349288135766983 + 0.01 * 6.7111053466796875
Epoch 990, val loss: 1.1473095417022705
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8481813389562468
=== training gcn model ===
Epoch 0, training loss: 2.018277406692505 = 1.9323089122772217 + 0.01 * 8.596845626831055
Epoch 0, val loss: 1.9243584871292114
Epoch 10, training loss: 2.008352756500244 = 1.9223849773406982 + 0.01 * 8.596774101257324
Epoch 10, val loss: 1.9143298864364624
Epoch 20, training loss: 1.9964008331298828 = 1.9104357957839966 + 0.01 * 8.596503257751465
Epoch 20, val loss: 1.9021841287612915
Epoch 30, training loss: 1.9800961017608643 = 1.8941417932510376 + 0.01 * 8.595428466796875
Epoch 30, val loss: 1.8857842683792114
Epoch 40, training loss: 1.956601619720459 = 1.8707159757614136 + 0.01 * 8.588565826416016
Epoch 40, val loss: 1.8626906871795654
Epoch 50, training loss: 1.9239774942398071 = 1.8385071754455566 + 0.01 * 8.547026634216309
Epoch 50, val loss: 1.8324822187423706
Epoch 60, training loss: 1.8849108219146729 = 1.8011562824249268 + 0.01 * 8.37545394897461
Epoch 60, val loss: 1.8008811473846436
Epoch 70, training loss: 1.8438342809677124 = 1.7633631229400635 + 0.01 * 8.047111511230469
Epoch 70, val loss: 1.7711237668991089
Epoch 80, training loss: 1.7946412563323975 = 1.7152273654937744 + 0.01 * 7.941386699676514
Epoch 80, val loss: 1.7288705110549927
Epoch 90, training loss: 1.7258249521255493 = 1.6480951309204102 + 0.01 * 7.772984981536865
Epoch 90, val loss: 1.6672580242156982
Epoch 100, training loss: 1.6369229555130005 = 1.560718059539795 + 0.01 * 7.620492935180664
Epoch 100, val loss: 1.5904147624969482
Epoch 110, training loss: 1.5358030796051025 = 1.4606668949127197 + 0.01 * 7.513624668121338
Epoch 110, val loss: 1.5070918798446655
Epoch 120, training loss: 1.4320645332336426 = 1.3579435348510742 + 0.01 * 7.412103652954102
Epoch 120, val loss: 1.4239325523376465
Epoch 130, training loss: 1.3306694030761719 = 1.2573415040969849 + 0.01 * 7.332789897918701
Epoch 130, val loss: 1.3443090915679932
Epoch 140, training loss: 1.2342458963394165 = 1.1615735292434692 + 0.01 * 7.2672319412231445
Epoch 140, val loss: 1.2690110206604004
Epoch 150, training loss: 1.146217942237854 = 1.0741580724716187 + 0.01 * 7.205983638763428
Epoch 150, val loss: 1.2010892629623413
Epoch 160, training loss: 1.0693901777267456 = 0.9977273941040039 + 0.01 * 7.166283130645752
Epoch 160, val loss: 1.1440547704696655
Epoch 170, training loss: 1.0026540756225586 = 0.9311399459838867 + 0.01 * 7.1514081954956055
Epoch 170, val loss: 1.0964679718017578
Epoch 180, training loss: 0.9412953853607178 = 0.8698095679283142 + 0.01 * 7.148581504821777
Epoch 180, val loss: 1.053511142730713
Epoch 190, training loss: 0.8800556063652039 = 0.8086056709289551 + 0.01 * 7.14499568939209
Epoch 190, val loss: 1.0097527503967285
Epoch 200, training loss: 0.8153532147407532 = 0.74397873878479 + 0.01 * 7.137448310852051
Epoch 200, val loss: 0.9610063433647156
Epoch 210, training loss: 0.7463909983634949 = 0.6751303672790527 + 0.01 * 7.126060962677002
Epoch 210, val loss: 0.9066824316978455
Epoch 220, training loss: 0.6751499176025391 = 0.604053258895874 + 0.01 * 7.109669208526611
Epoch 220, val loss: 0.8494808673858643
Epoch 230, training loss: 0.6050240993499756 = 0.5341272950172424 + 0.01 * 7.08967924118042
Epoch 230, val loss: 0.7942104339599609
Epoch 240, training loss: 0.53876131772995 = 0.46810320019721985 + 0.01 * 7.065813064575195
Epoch 240, val loss: 0.7451257705688477
Epoch 250, training loss: 0.4777511954307556 = 0.40728944540023804 + 0.01 * 7.0461745262146
Epoch 250, val loss: 0.7045760750770569
Epoch 260, training loss: 0.42268070578575134 = 0.35231491923332214 + 0.01 * 7.036579132080078
Epoch 260, val loss: 0.6727611422538757
Epoch 270, training loss: 0.3738654851913452 = 0.30354684591293335 + 0.01 * 7.03186559677124
Epoch 270, val loss: 0.6486712694168091
Epoch 280, training loss: 0.3313885033130646 = 0.26112493872642517 + 0.01 * 7.026357650756836
Epoch 280, val loss: 0.6313811540603638
Epoch 290, training loss: 0.2950543165206909 = 0.22481241822242737 + 0.01 * 7.024188995361328
Epoch 290, val loss: 0.6200509071350098
Epoch 300, training loss: 0.26425522565841675 = 0.1940181404352188 + 0.01 * 7.023706912994385
Epoch 300, val loss: 0.6137716770172119
Epoch 310, training loss: 0.23821161687374115 = 0.16797520220279694 + 0.01 * 7.023641586303711
Epoch 310, val loss: 0.6115877628326416
Epoch 320, training loss: 0.21616804599761963 = 0.1459304392337799 + 0.01 * 7.0237603187561035
Epoch 320, val loss: 0.6126090288162231
Epoch 330, training loss: 0.19745540618896484 = 0.12721657752990723 + 0.01 * 7.023883819580078
Epoch 330, val loss: 0.6160250902175903
Epoch 340, training loss: 0.18151919543743134 = 0.11127987504005432 + 0.01 * 7.023931980133057
Epoch 340, val loss: 0.6212201714515686
Epoch 350, training loss: 0.16790945827960968 = 0.09767098724842072 + 0.01 * 7.0238471031188965
Epoch 350, val loss: 0.6277880668640137
Epoch 360, training loss: 0.1562575399875641 = 0.08602171391248703 + 0.01 * 7.023582458496094
Epoch 360, val loss: 0.6353045701980591
Epoch 370, training loss: 0.14625594019889832 = 0.07602498680353165 + 0.01 * 7.023095607757568
Epoch 370, val loss: 0.6435363292694092
Epoch 380, training loss: 0.13764286041259766 = 0.06741928309202194 + 0.01 * 7.022358417510986
Epoch 380, val loss: 0.6523464918136597
Epoch 390, training loss: 0.13020461797714233 = 0.05999106541275978 + 0.01 * 7.021355628967285
Epoch 390, val loss: 0.6615079641342163
Epoch 400, training loss: 0.12376707792282104 = 0.05356627330183983 + 0.01 * 7.02008056640625
Epoch 400, val loss: 0.6709905862808228
Epoch 410, training loss: 0.11818067729473114 = 0.04799530282616615 + 0.01 * 7.0185370445251465
Epoch 410, val loss: 0.6806590557098389
Epoch 420, training loss: 0.1133362352848053 = 0.04315221682190895 + 0.01 * 7.018402576446533
Epoch 420, val loss: 0.6904675364494324
Epoch 430, training loss: 0.10908734053373337 = 0.03893101215362549 + 0.01 * 7.015632629394531
Epoch 430, val loss: 0.7003814578056335
Epoch 440, training loss: 0.10537315905094147 = 0.035241637378931046 + 0.01 * 7.013152599334717
Epoch 440, val loss: 0.7103900909423828
Epoch 450, training loss: 0.10211540758609772 = 0.03200910612940788 + 0.01 * 7.010629653930664
Epoch 450, val loss: 0.7203452587127686
Epoch 460, training loss: 0.09924805164337158 = 0.029168765991926193 + 0.01 * 7.00792932510376
Epoch 460, val loss: 0.7302347421646118
Epoch 470, training loss: 0.09671700745820999 = 0.026665858924388885 + 0.01 * 7.005115032196045
Epoch 470, val loss: 0.7400250434875488
Epoch 480, training loss: 0.09448854625225067 = 0.02445402555167675 + 0.01 * 7.003452301025391
Epoch 480, val loss: 0.7496763467788696
Epoch 490, training loss: 0.09249651432037354 = 0.022493721917271614 + 0.01 * 7.000279903411865
Epoch 490, val loss: 0.7591119408607483
Epoch 500, training loss: 0.0907047763466835 = 0.02075079269707203 + 0.01 * 6.99539852142334
Epoch 500, val loss: 0.7683727741241455
Epoch 510, training loss: 0.08910336345434189 = 0.019194813445210457 + 0.01 * 6.9908552169799805
Epoch 510, val loss: 0.777435839176178
Epoch 520, training loss: 0.08771015703678131 = 0.01780037209391594 + 0.01 * 6.990978717803955
Epoch 520, val loss: 0.7862892746925354
Epoch 530, training loss: 0.08636824786663055 = 0.016547342762351036 + 0.01 * 6.982090473175049
Epoch 530, val loss: 0.7949421405792236
Epoch 540, training loss: 0.08519566059112549 = 0.015416684560477734 + 0.01 * 6.977898120880127
Epoch 540, val loss: 0.8033913373947144
Epoch 550, training loss: 0.08411285281181335 = 0.014393199235200882 + 0.01 * 6.971965312957764
Epoch 550, val loss: 0.8116333484649658
Epoch 560, training loss: 0.08330123126506805 = 0.013467802666127682 + 0.01 * 6.98334264755249
Epoch 560, val loss: 0.8196350932121277
Epoch 570, training loss: 0.08223523944616318 = 0.012629196047782898 + 0.01 * 6.960604190826416
Epoch 570, val loss: 0.8273811340332031
Epoch 580, training loss: 0.0813826471567154 = 0.0118661317974329 + 0.01 * 6.9516520500183105
Epoch 580, val loss: 0.8349704742431641
Epoch 590, training loss: 0.0806085467338562 = 0.011169743724167347 + 0.01 * 6.943880081176758
Epoch 590, val loss: 0.8423259854316711
Epoch 600, training loss: 0.07985714823007584 = 0.010533864609897137 + 0.01 * 6.932328701019287
Epoch 600, val loss: 0.8495152592658997
Epoch 610, training loss: 0.0792001336812973 = 0.009952486492693424 + 0.01 * 6.924765110015869
Epoch 610, val loss: 0.8564609289169312
Epoch 620, training loss: 0.07862484455108643 = 0.009419996291399002 + 0.01 * 6.920485019683838
Epoch 620, val loss: 0.8632402420043945
Epoch 630, training loss: 0.07815273851156235 = 0.008931652642786503 + 0.01 * 6.9221086502075195
Epoch 630, val loss: 0.8698082566261292
Epoch 640, training loss: 0.07771085202693939 = 0.008482206612825394 + 0.01 * 6.92286491394043
Epoch 640, val loss: 0.876236081123352
Epoch 650, training loss: 0.07718165218830109 = 0.008067825809121132 + 0.01 * 6.911383152008057
Epoch 650, val loss: 0.8824881911277771
Epoch 660, training loss: 0.07664388418197632 = 0.007685014512389898 + 0.01 * 6.8958868980407715
Epoch 660, val loss: 0.8885774612426758
Epoch 670, training loss: 0.0762421265244484 = 0.007330736611038446 + 0.01 * 6.891139030456543
Epoch 670, val loss: 0.8945198655128479
Epoch 680, training loss: 0.07587816566228867 = 0.00700237276032567 + 0.01 * 6.887578964233398
Epoch 680, val loss: 0.9002902507781982
Epoch 690, training loss: 0.07571233808994293 = 0.006697004660964012 + 0.01 * 6.901533603668213
Epoch 690, val loss: 0.9059443473815918
Epoch 700, training loss: 0.07525236159563065 = 0.006413349416106939 + 0.01 * 6.883901596069336
Epoch 700, val loss: 0.9114392995834351
Epoch 710, training loss: 0.0749063640832901 = 0.006148806307464838 + 0.01 * 6.875755786895752
Epoch 710, val loss: 0.9167978763580322
Epoch 720, training loss: 0.07465435564517975 = 0.0059015871956944466 + 0.01 * 6.875277042388916
Epoch 720, val loss: 0.9220712184906006
Epoch 730, training loss: 0.07435765117406845 = 0.005670517683029175 + 0.01 * 6.86871337890625
Epoch 730, val loss: 0.9271858334541321
Epoch 740, training loss: 0.07409138977527618 = 0.005454238969832659 + 0.01 * 6.863714694976807
Epoch 740, val loss: 0.9321665167808533
Epoch 750, training loss: 0.07393976300954819 = 0.005251395516097546 + 0.01 * 6.868837356567383
Epoch 750, val loss: 0.9370535016059875
Epoch 760, training loss: 0.07362256199121475 = 0.005060896277427673 + 0.01 * 6.856166839599609
Epoch 760, val loss: 0.9417992234230042
Epoch 770, training loss: 0.07355979830026627 = 0.004881782922893763 + 0.01 * 6.867801666259766
Epoch 770, val loss: 0.9464684724807739
Epoch 780, training loss: 0.07327401638031006 = 0.004713378846645355 + 0.01 * 6.8560638427734375
Epoch 780, val loss: 0.9509746432304382
Epoch 790, training loss: 0.07302289456129074 = 0.004554639104753733 + 0.01 * 6.846826076507568
Epoch 790, val loss: 0.9554178714752197
Epoch 800, training loss: 0.07303189486265182 = 0.0044048321433365345 + 0.01 * 6.862706184387207
Epoch 800, val loss: 0.9597622156143188
Epoch 810, training loss: 0.07274949550628662 = 0.004263584036380053 + 0.01 * 6.848591327667236
Epoch 810, val loss: 0.9640011191368103
Epoch 820, training loss: 0.07272854447364807 = 0.004130204673856497 + 0.01 * 6.859833717346191
Epoch 820, val loss: 0.9681273102760315
Epoch 830, training loss: 0.07239232957363129 = 0.004003992769867182 + 0.01 * 6.838833808898926
Epoch 830, val loss: 0.9721634387969971
Epoch 840, training loss: 0.07220806926488876 = 0.0038845099043101072 + 0.01 * 6.8323564529418945
Epoch 840, val loss: 0.97609943151474
Epoch 850, training loss: 0.07225068658590317 = 0.003770984709262848 + 0.01 * 6.847970008850098
Epoch 850, val loss: 0.9800286293029785
Epoch 860, training loss: 0.07188213616609573 = 0.003663176903501153 + 0.01 * 6.821896076202393
Epoch 860, val loss: 0.9837996959686279
Epoch 870, training loss: 0.07169990986585617 = 0.003560833865776658 + 0.01 * 6.813907623291016
Epoch 870, val loss: 0.9874305725097656
Epoch 880, training loss: 0.07184863835573196 = 0.0034634661860764027 + 0.01 * 6.838517189025879
Epoch 880, val loss: 0.9911288619041443
Epoch 890, training loss: 0.07158990204334259 = 0.003371229162439704 + 0.01 * 6.8218674659729
Epoch 890, val loss: 0.994583010673523
Epoch 900, training loss: 0.07139595597982407 = 0.0032834219746291637 + 0.01 * 6.811253547668457
Epoch 900, val loss: 0.9980395436286926
Epoch 910, training loss: 0.07125543057918549 = 0.0031994986347854137 + 0.01 * 6.805593967437744
Epoch 910, val loss: 1.001502275466919
Epoch 920, training loss: 0.07129805535078049 = 0.003119354136288166 + 0.01 * 6.817870140075684
Epoch 920, val loss: 1.0048280954360962
Epoch 930, training loss: 0.07094813138246536 = 0.003042995696887374 + 0.01 * 6.790513515472412
Epoch 930, val loss: 1.008131504058838
Epoch 940, training loss: 0.07099325209856033 = 0.002970192115753889 + 0.01 * 6.802306652069092
Epoch 940, val loss: 1.0112581253051758
Epoch 950, training loss: 0.07100308686494827 = 0.002900726394727826 + 0.01 * 6.81023645401001
Epoch 950, val loss: 1.0144826173782349
Epoch 960, training loss: 0.07073649764060974 = 0.0028344120364636183 + 0.01 * 6.7902092933654785
Epoch 960, val loss: 1.0174161195755005
Epoch 970, training loss: 0.07074549794197083 = 0.0027708495035767555 + 0.01 * 6.797464847564697
Epoch 970, val loss: 1.020432710647583
Epoch 980, training loss: 0.07047790288925171 = 0.002710268832743168 + 0.01 * 6.776763916015625
Epoch 980, val loss: 1.023392677307129
Epoch 990, training loss: 0.07055817544460297 = 0.0026520027313381433 + 0.01 * 6.7906174659729
Epoch 990, val loss: 1.0262051820755005
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.0336475372314453 = 1.9476791620254517 + 0.01 * 8.596831321716309
Epoch 0, val loss: 1.9456050395965576
Epoch 10, training loss: 2.0233712196350098 = 1.937403678894043 + 0.01 * 8.596760749816895
Epoch 10, val loss: 1.9348750114440918
Epoch 20, training loss: 2.010746717453003 = 1.9247816801071167 + 0.01 * 8.596512794494629
Epoch 20, val loss: 1.9216794967651367
Epoch 30, training loss: 1.993119239807129 = 1.9071626663208008 + 0.01 * 8.595656394958496
Epoch 30, val loss: 1.9035671949386597
Epoch 40, training loss: 1.9671331644058228 = 1.8812298774719238 + 0.01 * 8.590330123901367
Epoch 40, val loss: 1.8776345252990723
Epoch 50, training loss: 1.9304896593093872 = 1.8449636697769165 + 0.01 * 8.55259895324707
Epoch 50, val loss: 1.8431981801986694
Epoch 60, training loss: 1.8878909349441528 = 1.804129958152771 + 0.01 * 8.37609577178955
Epoch 60, val loss: 1.8081697225570679
Epoch 70, training loss: 1.8509366512298584 = 1.770635724067688 + 0.01 * 8.0300931930542
Epoch 70, val loss: 1.7810418605804443
Epoch 80, training loss: 1.8100409507751465 = 1.731888771057129 + 0.01 * 7.815223217010498
Epoch 80, val loss: 1.7447428703308105
Epoch 90, training loss: 1.753622055053711 = 1.6781095266342163 + 0.01 * 7.55125617980957
Epoch 90, val loss: 1.6959816217422485
Epoch 100, training loss: 1.679269790649414 = 1.605713963508606 + 0.01 * 7.355576992034912
Epoch 100, val loss: 1.6333245038986206
Epoch 110, training loss: 1.5872153043746948 = 1.5148319005966187 + 0.01 * 7.238340377807617
Epoch 110, val loss: 1.5558303594589233
Epoch 120, training loss: 1.4845598936080933 = 1.4127299785614014 + 0.01 * 7.182990550994873
Epoch 120, val loss: 1.4682366847991943
Epoch 130, training loss: 1.3774906396865845 = 1.3060673475265503 + 0.01 * 7.142325401306152
Epoch 130, val loss: 1.377751111984253
Epoch 140, training loss: 1.2695934772491455 = 1.1985232830047607 + 0.01 * 7.107025623321533
Epoch 140, val loss: 1.2889283895492554
Epoch 150, training loss: 1.1629470586776733 = 1.0921976566314697 + 0.01 * 7.074943542480469
Epoch 150, val loss: 1.2020841836929321
Epoch 160, training loss: 1.0596599578857422 = 0.9891761541366577 + 0.01 * 7.04838228225708
Epoch 160, val loss: 1.119081735610962
Epoch 170, training loss: 0.9627776145935059 = 0.8924909234046936 + 0.01 * 7.028672218322754
Epoch 170, val loss: 1.041936993598938
Epoch 180, training loss: 0.8751665949821472 = 0.8050127029418945 + 0.01 * 7.0153913497924805
Epoch 180, val loss: 0.9730648398399353
Epoch 190, training loss: 0.7982946038246155 = 0.7282102704048157 + 0.01 * 7.008431911468506
Epoch 190, val loss: 0.9138168692588806
Epoch 200, training loss: 0.731115460395813 = 0.6610549688339233 + 0.01 * 7.006052017211914
Epoch 200, val loss: 0.8635090589523315
Epoch 210, training loss: 0.670699954032898 = 0.6006431579589844 + 0.01 * 7.005681037902832
Epoch 210, val loss: 0.8198797106742859
Epoch 220, training loss: 0.613605260848999 = 0.5435507297515869 + 0.01 * 7.005453109741211
Epoch 220, val loss: 0.7801040410995483
Epoch 230, training loss: 0.5573954582214355 = 0.48734334111213684 + 0.01 * 7.005212783813477
Epoch 230, val loss: 0.7422567009925842
Epoch 240, training loss: 0.5014591813087463 = 0.431410014629364 + 0.01 * 7.004915237426758
Epoch 240, val loss: 0.7062863111495972
Epoch 250, training loss: 0.44682830572128296 = 0.3767840266227722 + 0.01 * 7.004426956176758
Epoch 250, val loss: 0.673097550868988
Epoch 260, training loss: 0.39524590969085693 = 0.32520821690559387 + 0.01 * 7.003770351409912
Epoch 260, val loss: 0.6434147953987122
Epoch 270, training loss: 0.34833526611328125 = 0.2783055603504181 + 0.01 * 7.002969741821289
Epoch 270, val loss: 0.6184112429618835
Epoch 280, training loss: 0.30710840225219727 = 0.23708689212799072 + 0.01 * 7.002150058746338
Epoch 280, val loss: 0.5986512303352356
Epoch 290, training loss: 0.2718239724636078 = 0.20180843770503998 + 0.01 * 7.001554012298584
Epoch 290, val loss: 0.5843489766120911
Epoch 300, training loss: 0.242148756980896 = 0.17213986814022064 + 0.01 * 7.000889301300049
Epoch 300, val loss: 0.5748830437660217
Epoch 310, training loss: 0.2174171507358551 = 0.14741367101669312 + 0.01 * 7.000349044799805
Epoch 310, val loss: 0.5697029232978821
Epoch 320, training loss: 0.19686046242713928 = 0.1268569678068161 + 0.01 * 7.000350475311279
Epoch 320, val loss: 0.5680766701698303
Epoch 330, training loss: 0.17972102761268616 = 0.10972253978252411 + 0.01 * 6.999849796295166
Epoch 330, val loss: 0.5693196058273315
Epoch 340, training loss: 0.16537058353424072 = 0.09537952393293381 + 0.01 * 6.9991068840026855
Epoch 340, val loss: 0.5728625059127808
Epoch 350, training loss: 0.15331006050109863 = 0.08332430571317673 + 0.01 * 6.998576641082764
Epoch 350, val loss: 0.5781329274177551
Epoch 360, training loss: 0.14312398433685303 = 0.07315149158239365 + 0.01 * 6.997249126434326
Epoch 360, val loss: 0.5846592783927917
Epoch 370, training loss: 0.13449572026729584 = 0.0645366907119751 + 0.01 * 6.995903015136719
Epoch 370, val loss: 0.592092752456665
Epoch 380, training loss: 0.12716831266880035 = 0.05721411481499672 + 0.01 * 6.995419979095459
Epoch 380, val loss: 0.6001412272453308
Epoch 390, training loss: 0.12090617418289185 = 0.050965968519449234 + 0.01 * 6.994019985198975
Epoch 390, val loss: 0.6085895299911499
Epoch 400, training loss: 0.11552051454782486 = 0.045611970126628876 + 0.01 * 6.990854740142822
Epoch 400, val loss: 0.6172979474067688
Epoch 410, training loss: 0.11093801259994507 = 0.041002992540597916 + 0.01 * 6.993501663208008
Epoch 410, val loss: 0.6261399984359741
Epoch 420, training loss: 0.106890007853508 = 0.037018273025751114 + 0.01 * 6.987173080444336
Epoch 420, val loss: 0.6350305080413818
Epoch 430, training loss: 0.10338360071182251 = 0.03355778753757477 + 0.01 * 6.982581615447998
Epoch 430, val loss: 0.6439061760902405
Epoch 440, training loss: 0.10033155232667923 = 0.03053916245698929 + 0.01 * 6.979239463806152
Epoch 440, val loss: 0.6527388095855713
Epoch 450, training loss: 0.09772265702486038 = 0.02789490669965744 + 0.01 * 6.9827752113342285
Epoch 450, val loss: 0.6614955067634583
Epoch 460, training loss: 0.09529237449169159 = 0.025569552555680275 + 0.01 * 6.972282409667969
Epoch 460, val loss: 0.6701371073722839
Epoch 470, training loss: 0.09317029267549515 = 0.023516424000263214 + 0.01 * 6.965386867523193
Epoch 470, val loss: 0.6786251664161682
Epoch 480, training loss: 0.09133636951446533 = 0.021696755662560463 + 0.01 * 6.963962078094482
Epoch 480, val loss: 0.6869800686836243
Epoch 490, training loss: 0.0895894318819046 = 0.02007809467613697 + 0.01 * 6.951134204864502
Epoch 490, val loss: 0.6951660513877869
Epoch 500, training loss: 0.0881240963935852 = 0.01863291673362255 + 0.01 * 6.949117660522461
Epoch 500, val loss: 0.7032136917114258
Epoch 510, training loss: 0.08673757314682007 = 0.017338616773486137 + 0.01 * 6.9398956298828125
Epoch 510, val loss: 0.7110748887062073
Epoch 520, training loss: 0.08549661189317703 = 0.016176486387848854 + 0.01 * 6.932012557983398
Epoch 520, val loss: 0.7187527418136597
Epoch 530, training loss: 0.08489132672548294 = 0.015129069797694683 + 0.01 * 6.976226329803467
Epoch 530, val loss: 0.7263023853302002
Epoch 540, training loss: 0.08345410972833633 = 0.01418396271765232 + 0.01 * 6.92701530456543
Epoch 540, val loss: 0.7335702180862427
Epoch 550, training loss: 0.08232544362545013 = 0.013327251188457012 + 0.01 * 6.899819374084473
Epoch 550, val loss: 0.7407401204109192
Epoch 560, training loss: 0.08146513998508453 = 0.012547867372632027 + 0.01 * 6.891727447509766
Epoch 560, val loss: 0.7477408051490784
Epoch 570, training loss: 0.08063890784978867 = 0.011837167665362358 + 0.01 * 6.880174160003662
Epoch 570, val loss: 0.7545897364616394
Epoch 580, training loss: 0.08003875613212585 = 0.011188384145498276 + 0.01 * 6.885037899017334
Epoch 580, val loss: 0.7612364888191223
Epoch 590, training loss: 0.07927709817886353 = 0.010593045502901077 + 0.01 * 6.868404865264893
Epoch 590, val loss: 0.7677938342094421
Epoch 600, training loss: 0.07867087423801422 = 0.010046824812889099 + 0.01 * 6.862405300140381
Epoch 600, val loss: 0.7741395235061646
Epoch 610, training loss: 0.07802820205688477 = 0.009544359520077705 + 0.01 * 6.848384380340576
Epoch 610, val loss: 0.7803379893302917
Epoch 620, training loss: 0.07751941680908203 = 0.009080016054213047 + 0.01 * 6.843940258026123
Epoch 620, val loss: 0.7864212393760681
Epoch 630, training loss: 0.07714203745126724 = 0.008651929907500744 + 0.01 * 6.849011421203613
Epoch 630, val loss: 0.7923130393028259
Epoch 640, training loss: 0.07659053057432175 = 0.00825635064393282 + 0.01 * 6.833417892456055
Epoch 640, val loss: 0.7980280518531799
Epoch 650, training loss: 0.07613486051559448 = 0.007889037020504475 + 0.01 * 6.824582099914551
Epoch 650, val loss: 0.8036406636238098
Epoch 660, training loss: 0.07578149437904358 = 0.007548394612967968 + 0.01 * 6.823310375213623
Epoch 660, val loss: 0.8091444373130798
Epoch 670, training loss: 0.07540580630302429 = 0.007231040857732296 + 0.01 * 6.817476272583008
Epoch 670, val loss: 0.8144861459732056
Epoch 680, training loss: 0.075018011033535 = 0.006935625337064266 + 0.01 * 6.808238983154297
Epoch 680, val loss: 0.8196761608123779
Epoch 690, training loss: 0.07457508146762848 = 0.006659283768385649 + 0.01 * 6.7915802001953125
Epoch 690, val loss: 0.8247803449630737
Epoch 700, training loss: 0.07436811178922653 = 0.00640086829662323 + 0.01 * 6.796724796295166
Epoch 700, val loss: 0.8297503590583801
Epoch 710, training loss: 0.0739690288901329 = 0.006159075535833836 + 0.01 * 6.780995845794678
Epoch 710, val loss: 0.834570050239563
Epoch 720, training loss: 0.07395114749670029 = 0.005932189524173737 + 0.01 * 6.801895618438721
Epoch 720, val loss: 0.8393557667732239
Epoch 730, training loss: 0.07354477047920227 = 0.00571963656693697 + 0.01 * 6.782513618469238
Epoch 730, val loss: 0.8438878059387207
Epoch 740, training loss: 0.07337917387485504 = 0.005519524682313204 + 0.01 * 6.785965442657471
Epoch 740, val loss: 0.8484451174736023
Epoch 750, training loss: 0.07305288314819336 = 0.005331384018063545 + 0.01 * 6.77215051651001
Epoch 750, val loss: 0.8527652025222778
Epoch 760, training loss: 0.07272174209356308 = 0.005153631325811148 + 0.01 * 6.756811141967773
Epoch 760, val loss: 0.8570683002471924
Epoch 770, training loss: 0.07263301312923431 = 0.004985540173947811 + 0.01 * 6.764747619628906
Epoch 770, val loss: 0.8613484501838684
Epoch 780, training loss: 0.07239851355552673 = 0.004827003926038742 + 0.01 * 6.7571516036987305
Epoch 780, val loss: 0.8654093146324158
Epoch 790, training loss: 0.07229003310203552 = 0.004676434677094221 + 0.01 * 6.761360168457031
Epoch 790, val loss: 0.8695030212402344
Epoch 800, training loss: 0.07202700525522232 = 0.004534150939434767 + 0.01 * 6.749285697937012
Epoch 800, val loss: 0.873439610004425
Epoch 810, training loss: 0.0718567818403244 = 0.0043990397825837135 + 0.01 * 6.745774269104004
Epoch 810, val loss: 0.8773164749145508
Epoch 820, training loss: 0.07175066322088242 = 0.004271034616976976 + 0.01 * 6.747962474822998
Epoch 820, val loss: 0.8811028003692627
Epoch 830, training loss: 0.07157360762357712 = 0.0041498783975839615 + 0.01 * 6.742373466491699
Epoch 830, val loss: 0.884741485118866
Epoch 840, training loss: 0.07134824991226196 = 0.0040344600565731525 + 0.01 * 6.73137903213501
Epoch 840, val loss: 0.8883692026138306
Epoch 850, training loss: 0.07129035890102386 = 0.003924480173736811 + 0.01 * 6.7365875244140625
Epoch 850, val loss: 0.891901433467865
Epoch 860, training loss: 0.07121365517377853 = 0.0038196691311895847 + 0.01 * 6.739398956298828
Epoch 860, val loss: 0.8954139351844788
Epoch 870, training loss: 0.07099669426679611 = 0.003720309119671583 + 0.01 * 6.727639198303223
Epoch 870, val loss: 0.8987136483192444
Epoch 880, training loss: 0.07091064006090164 = 0.0036251323763281107 + 0.01 * 6.728550910949707
Epoch 880, val loss: 0.9020575881004333
Epoch 890, training loss: 0.0707172155380249 = 0.00353458384051919 + 0.01 * 6.718263149261475
Epoch 890, val loss: 0.9052841663360596
Epoch 900, training loss: 0.07088949531316757 = 0.0034480628091841936 + 0.01 * 6.744143486022949
Epoch 900, val loss: 0.9085150957107544
Epoch 910, training loss: 0.0704624280333519 = 0.0033655641600489616 + 0.01 * 6.709686756134033
Epoch 910, val loss: 0.9115509986877441
Epoch 920, training loss: 0.0704093724489212 = 0.0032866154797375202 + 0.01 * 6.712275981903076
Epoch 920, val loss: 0.9145779609680176
Epoch 930, training loss: 0.07028549164533615 = 0.003210983006283641 + 0.01 * 6.707450866699219
Epoch 930, val loss: 0.9175534844398499
Epoch 940, training loss: 0.07018313556909561 = 0.0031385424081236124 + 0.01 * 6.704459190368652
Epoch 940, val loss: 0.9204786419868469
Epoch 950, training loss: 0.07027889043092728 = 0.0030690182466059923 + 0.01 * 6.720987319946289
Epoch 950, val loss: 0.9233395457267761
Epoch 960, training loss: 0.07002552598714828 = 0.0030025308951735497 + 0.01 * 6.70229959487915
Epoch 960, val loss: 0.9261137247085571
Epoch 970, training loss: 0.06994106620550156 = 0.0029387499671429396 + 0.01 * 6.700232028961182
Epoch 970, val loss: 0.9289072155952454
Epoch 980, training loss: 0.06982911378145218 = 0.0028776349499821663 + 0.01 * 6.69514799118042
Epoch 980, val loss: 0.9315289258956909
Epoch 990, training loss: 0.06970082223415375 = 0.002818693406879902 + 0.01 * 6.6882123947143555
Epoch 990, val loss: 0.9341607689857483
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8444444444444444
0.8376383763837639
The final CL Acc:0.82593, 0.01512, The final GNN Acc:0.84168, 0.00464
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9486])
updated graph: torch.Size([2, 10534])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0530099868774414 = 1.967041254043579 + 0.01 * 8.59687614440918
Epoch 0, val loss: 1.967570185661316
Epoch 10, training loss: 2.0421295166015625 = 1.9561611413955688 + 0.01 * 8.596835136413574
Epoch 10, val loss: 1.9577091932296753
Epoch 20, training loss: 2.028808355331421 = 1.9428415298461914 + 0.01 * 8.596681594848633
Epoch 20, val loss: 1.9454121589660645
Epoch 30, training loss: 2.0104172229766846 = 1.9244556427001953 + 0.01 * 8.596159934997559
Epoch 30, val loss: 1.92832350730896
Epoch 40, training loss: 1.9836034774780273 = 1.8976703882217407 + 0.01 * 8.593306541442871
Epoch 40, val loss: 1.903773546218872
Epoch 50, training loss: 1.9455242156982422 = 1.8597966432571411 + 0.01 * 8.572763442993164
Epoch 50, val loss: 1.870617151260376
Epoch 60, training loss: 1.9001911878585815 = 1.8153952360153198 + 0.01 * 8.479599952697754
Epoch 60, val loss: 1.8342903852462769
Epoch 70, training loss: 1.8599485158920288 = 1.7776848077774048 + 0.01 * 8.226371765136719
Epoch 70, val loss: 1.8017559051513672
Epoch 80, training loss: 1.8211438655853271 = 1.739898681640625 + 0.01 * 8.124519348144531
Epoch 80, val loss: 1.7611443996429443
Epoch 90, training loss: 1.7680286169052124 = 1.688154697418213 + 0.01 * 7.987396717071533
Epoch 90, val loss: 1.7108427286148071
Epoch 100, training loss: 1.695443034172058 = 1.6172243356704712 + 0.01 * 7.821873188018799
Epoch 100, val loss: 1.6482967138290405
Epoch 110, training loss: 1.6044217348098755 = 1.5282506942749023 + 0.01 * 7.617100715637207
Epoch 110, val loss: 1.5709419250488281
Epoch 120, training loss: 1.504639983177185 = 1.4297330379486084 + 0.01 * 7.490694522857666
Epoch 120, val loss: 1.485404133796692
Epoch 130, training loss: 1.404758334159851 = 1.330322504043579 + 0.01 * 7.443583011627197
Epoch 130, val loss: 1.4001182317733765
Epoch 140, training loss: 1.3080040216445923 = 1.234089970588684 + 0.01 * 7.391409397125244
Epoch 140, val loss: 1.319183349609375
Epoch 150, training loss: 1.216118335723877 = 1.1426090002059937 + 0.01 * 7.3509392738342285
Epoch 150, val loss: 1.2436596155166626
Epoch 160, training loss: 1.1308592557907104 = 1.0577300786972046 + 0.01 * 7.312922477722168
Epoch 160, val loss: 1.1760361194610596
Epoch 170, training loss: 1.0527681112289429 = 0.97996985912323 + 0.01 * 7.279830455780029
Epoch 170, val loss: 1.1158796548843384
Epoch 180, training loss: 0.9802775382995605 = 0.9077122211456299 + 0.01 * 7.256529331207275
Epoch 180, val loss: 1.061278223991394
Epoch 190, training loss: 0.9109152555465698 = 0.8385023474693298 + 0.01 * 7.241290092468262
Epoch 190, val loss: 1.01006019115448
Epoch 200, training loss: 0.8432468175888062 = 0.7709523439407349 + 0.01 * 7.229450225830078
Epoch 200, val loss: 0.9616628289222717
Epoch 210, training loss: 0.7775834798812866 = 0.7054109573364258 + 0.01 * 7.217252731323242
Epoch 210, val loss: 0.9175552725791931
Epoch 220, training loss: 0.7154517769813538 = 0.6434177756309509 + 0.01 * 7.203400135040283
Epoch 220, val loss: 0.8799995183944702
Epoch 230, training loss: 0.6581250429153442 = 0.5862255692481995 + 0.01 * 7.189949035644531
Epoch 230, val loss: 0.8501446843147278
Epoch 240, training loss: 0.6054490804672241 = 0.5337284207344055 + 0.01 * 7.172062873840332
Epoch 240, val loss: 0.827400267124176
Epoch 250, training loss: 0.5565191507339478 = 0.48491623997688293 + 0.01 * 7.160293102264404
Epoch 250, val loss: 0.8106971383094788
Epoch 260, training loss: 0.510144054889679 = 0.43870559334754944 + 0.01 * 7.1438446044921875
Epoch 260, val loss: 0.7986775636672974
Epoch 270, training loss: 0.46593156456947327 = 0.3945958614349365 + 0.01 * 7.133571147918701
Epoch 270, val loss: 0.7904747724533081
Epoch 280, training loss: 0.4241541922092438 = 0.35280153155326843 + 0.01 * 7.135265350341797
Epoch 280, val loss: 0.7859266400337219
Epoch 290, training loss: 0.3851640820503235 = 0.31393662095069885 + 0.01 * 7.122745990753174
Epoch 290, val loss: 0.784995436668396
Epoch 300, training loss: 0.3496585488319397 = 0.2784821093082428 + 0.01 * 7.117642402648926
Epoch 300, val loss: 0.7875458002090454
Epoch 310, training loss: 0.3177613615989685 = 0.24662566184997559 + 0.01 * 7.1135687828063965
Epoch 310, val loss: 0.7932612299919128
Epoch 320, training loss: 0.2893680930137634 = 0.21825873851776123 + 0.01 * 7.110934257507324
Epoch 320, val loss: 0.8016476035118103
Epoch 330, training loss: 0.26423442363739014 = 0.19314758479595184 + 0.01 * 7.108682632446289
Epoch 330, val loss: 0.8122063279151917
Epoch 340, training loss: 0.24210740625858307 = 0.17100466787815094 + 0.01 * 7.110274314880371
Epoch 340, val loss: 0.8245121240615845
Epoch 350, training loss: 0.22258946299552917 = 0.1515321433544159 + 0.01 * 7.105731964111328
Epoch 350, val loss: 0.8380846381187439
Epoch 360, training loss: 0.20545758306980133 = 0.1344253271818161 + 0.01 * 7.1032257080078125
Epoch 360, val loss: 0.8524889945983887
Epoch 370, training loss: 0.19043117761611938 = 0.11939141154289246 + 0.01 * 7.103977680206299
Epoch 370, val loss: 0.8673823475837708
Epoch 380, training loss: 0.17717984318733215 = 0.1061662808060646 + 0.01 * 7.101356029510498
Epoch 380, val loss: 0.882481038570404
Epoch 390, training loss: 0.16549837589263916 = 0.09451916813850403 + 0.01 * 7.097919940948486
Epoch 390, val loss: 0.8975780010223389
Epoch 400, training loss: 0.15520885586738586 = 0.08425158262252808 + 0.01 * 7.095727443695068
Epoch 400, val loss: 0.9126205444335938
Epoch 410, training loss: 0.1461462378501892 = 0.07519379258155823 + 0.01 * 7.095244407653809
Epoch 410, val loss: 0.9275622367858887
Epoch 420, training loss: 0.13812781870365143 = 0.06720075011253357 + 0.01 * 7.09270715713501
Epoch 420, val loss: 0.9423239827156067
Epoch 430, training loss: 0.13105031847953796 = 0.06015089526772499 + 0.01 * 7.089942932128906
Epoch 430, val loss: 0.9568998217582703
Epoch 440, training loss: 0.12482523918151855 = 0.05393562838435173 + 0.01 * 7.088961601257324
Epoch 440, val loss: 0.9712176322937012
Epoch 450, training loss: 0.11934611946344376 = 0.04846176505088806 + 0.01 * 7.088435649871826
Epoch 450, val loss: 0.9853357672691345
Epoch 460, training loss: 0.11445433646440506 = 0.04364340752363205 + 0.01 * 7.081092834472656
Epoch 460, val loss: 0.9990847110748291
Epoch 470, training loss: 0.11017133295536041 = 0.039401404559612274 + 0.01 * 7.076992988586426
Epoch 470, val loss: 1.0125184059143066
Epoch 480, training loss: 0.1064499095082283 = 0.03566615283489227 + 0.01 * 7.078375816345215
Epoch 480, val loss: 1.025549292564392
Epoch 490, training loss: 0.1031055748462677 = 0.032375697046518326 + 0.01 * 7.0729875564575195
Epoch 490, val loss: 1.038196325302124
Epoch 500, training loss: 0.10014313459396362 = 0.029473384842276573 + 0.01 * 7.066974639892578
Epoch 500, val loss: 1.0504472255706787
Epoch 510, training loss: 0.09750324487686157 = 0.02690667100250721 + 0.01 * 7.059658050537109
Epoch 510, val loss: 1.062279224395752
Epoch 520, training loss: 0.09523500502109528 = 0.024630531668663025 + 0.01 * 7.0604472160339355
Epoch 520, val loss: 1.0736867189407349
Epoch 530, training loss: 0.09311586618423462 = 0.02261102758347988 + 0.01 * 7.0504841804504395
Epoch 530, val loss: 1.0846729278564453
Epoch 540, training loss: 0.09124032407999039 = 0.020813481882214546 + 0.01 * 7.042684555053711
Epoch 540, val loss: 1.0952211618423462
Epoch 550, training loss: 0.08954224735498428 = 0.019208207726478577 + 0.01 * 7.033404350280762
Epoch 550, val loss: 1.1053699254989624
Epoch 560, training loss: 0.088033527135849 = 0.017770353704690933 + 0.01 * 7.026318073272705
Epoch 560, val loss: 1.1151363849639893
Epoch 570, training loss: 0.08672431111335754 = 0.016479887068271637 + 0.01 * 7.024442672729492
Epoch 570, val loss: 1.124551773071289
Epoch 580, training loss: 0.08538807183504105 = 0.015315212309360504 + 0.01 * 7.007286071777344
Epoch 580, val loss: 1.1335622072219849
Epoch 590, training loss: 0.08443357795476913 = 0.014261438511312008 + 0.01 * 7.017213821411133
Epoch 590, val loss: 1.1422384977340698
Epoch 600, training loss: 0.08343315869569778 = 0.013304835185408592 + 0.01 * 7.012832164764404
Epoch 600, val loss: 1.1504628658294678
Epoch 610, training loss: 0.08252193033695221 = 0.012435337528586388 + 0.01 * 7.0086588859558105
Epoch 610, val loss: 1.1583610773086548
Epoch 620, training loss: 0.0814676433801651 = 0.011644761078059673 + 0.01 * 6.982288360595703
Epoch 620, val loss: 1.1658607721328735
Epoch 630, training loss: 0.08056284487247467 = 0.010924436151981354 + 0.01 * 6.963840961456299
Epoch 630, val loss: 1.1729214191436768
Epoch 640, training loss: 0.07984613627195358 = 0.010268053039908409 + 0.01 * 6.957808494567871
Epoch 640, val loss: 1.1796975135803223
Epoch 650, training loss: 0.07920768111944199 = 0.009669847786426544 + 0.01 * 6.9537835121154785
Epoch 650, val loss: 1.1860569715499878
Epoch 660, training loss: 0.07835368812084198 = 0.009122954681515694 + 0.01 * 6.923073768615723
Epoch 660, val loss: 1.1921226978302002
Epoch 670, training loss: 0.07844028621912003 = 0.008621811866760254 + 0.01 * 6.981847763061523
Epoch 670, val loss: 1.197942852973938
Epoch 680, training loss: 0.07728966325521469 = 0.008163094520568848 + 0.01 * 6.912657260894775
Epoch 680, val loss: 1.2034674882888794
Epoch 690, training loss: 0.07695630192756653 = 0.00774175813421607 + 0.01 * 6.921454906463623
Epoch 690, val loss: 1.2087658643722534
Epoch 700, training loss: 0.07649149745702744 = 0.007354262750595808 + 0.01 * 6.913723945617676
Epoch 700, val loss: 1.2138373851776123
Epoch 710, training loss: 0.07645925134420395 = 0.006996625103056431 + 0.01 * 6.946262836456299
Epoch 710, val loss: 1.2186810970306396
Epoch 720, training loss: 0.07555697858333588 = 0.006666882894933224 + 0.01 * 6.889009475708008
Epoch 720, val loss: 1.223314642906189
Epoch 730, training loss: 0.07502952963113785 = 0.0063619487918913364 + 0.01 * 6.866758346557617
Epoch 730, val loss: 1.2277544736862183
Epoch 740, training loss: 0.07479216903448105 = 0.0060788718983531 + 0.01 * 6.871330261230469
Epoch 740, val loss: 1.232035756111145
Epoch 750, training loss: 0.07449600845575333 = 0.005816299933940172 + 0.01 * 6.8679704666137695
Epoch 750, val loss: 1.2361184358596802
Epoch 760, training loss: 0.07426255941390991 = 0.005572150927037001 + 0.01 * 6.869041442871094
Epoch 760, val loss: 1.2400763034820557
Epoch 770, training loss: 0.07435949146747589 = 0.005344551056623459 + 0.01 * 6.901494026184082
Epoch 770, val loss: 1.2438939809799194
Epoch 780, training loss: 0.0735185369849205 = 0.005131927318871021 + 0.01 * 6.838660717010498
Epoch 780, val loss: 1.2475268840789795
Epoch 790, training loss: 0.07347117364406586 = 0.004933089949190617 + 0.01 * 6.853808403015137
Epoch 790, val loss: 1.2510164976119995
Epoch 800, training loss: 0.07329320162534714 = 0.004747417289763689 + 0.01 * 6.854578018188477
Epoch 800, val loss: 1.2544816732406616
Epoch 810, training loss: 0.0728091448545456 = 0.004573428072035313 + 0.01 * 6.823572158813477
Epoch 810, val loss: 1.2577248811721802
Epoch 820, training loss: 0.07269081473350525 = 0.00441019469872117 + 0.01 * 6.828062057495117
Epoch 820, val loss: 1.2608439922332764
Epoch 830, training loss: 0.07258981466293335 = 0.00425692880526185 + 0.01 * 6.83328914642334
Epoch 830, val loss: 1.2639262676239014
Epoch 840, training loss: 0.07247868180274963 = 0.004112511873245239 + 0.01 * 6.8366169929504395
Epoch 840, val loss: 1.2668395042419434
Epoch 850, training loss: 0.07215571403503418 = 0.003976759035140276 + 0.01 * 6.817895412445068
Epoch 850, val loss: 1.2697468996047974
Epoch 860, training loss: 0.07201356440782547 = 0.0038485373370349407 + 0.01 * 6.816502571105957
Epoch 860, val loss: 1.2724857330322266
Epoch 870, training loss: 0.07173208147287369 = 0.0037275198847055435 + 0.01 * 6.800456523895264
Epoch 870, val loss: 1.2751766443252563
Epoch 880, training loss: 0.07169467955827713 = 0.003613049164414406 + 0.01 * 6.808163642883301
Epoch 880, val loss: 1.277775764465332
Epoch 890, training loss: 0.07136236131191254 = 0.0035048765130341053 + 0.01 * 6.785748481750488
Epoch 890, val loss: 1.2802791595458984
Epoch 900, training loss: 0.071233369410038 = 0.003402451053261757 + 0.01 * 6.783092498779297
Epoch 900, val loss: 1.282718300819397
Epoch 910, training loss: 0.0713084414601326 = 0.0033053182996809483 + 0.01 * 6.800312519073486
Epoch 910, val loss: 1.2851508855819702
Epoch 920, training loss: 0.07108162343502045 = 0.0032130046747624874 + 0.01 * 6.786862373352051
Epoch 920, val loss: 1.2873817682266235
Epoch 930, training loss: 0.07088471949100494 = 0.0031254678033292294 + 0.01 * 6.775925636291504
Epoch 930, val loss: 1.289683222770691
Epoch 940, training loss: 0.07109174132347107 = 0.003042245749384165 + 0.01 * 6.804949760437012
Epoch 940, val loss: 1.2918164730072021
Epoch 950, training loss: 0.07085441052913666 = 0.002963225357234478 + 0.01 * 6.789118766784668
Epoch 950, val loss: 1.2939245700836182
Epoch 960, training loss: 0.07049376517534256 = 0.002887844108045101 + 0.01 * 6.760591983795166
Epoch 960, val loss: 1.2959781885147095
Epoch 970, training loss: 0.0705917626619339 = 0.0028158663772046566 + 0.01 * 6.777590274810791
Epoch 970, val loss: 1.297964334487915
Epoch 980, training loss: 0.07057993859052658 = 0.0027472020592540503 + 0.01 * 6.783273696899414
Epoch 980, val loss: 1.2998234033584595
Epoch 990, training loss: 0.07020986080169678 = 0.0026816800236701965 + 0.01 * 6.752818584442139
Epoch 990, val loss: 1.301723599433899
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 2.0373895168304443 = 1.951420783996582 + 0.01 * 8.59687614440918
Epoch 0, val loss: 1.949726939201355
Epoch 10, training loss: 2.0265731811523438 = 1.9406050443649292 + 0.01 * 8.596823692321777
Epoch 10, val loss: 1.9393484592437744
Epoch 20, training loss: 2.0132226943969727 = 1.9272559881210327 + 0.01 * 8.596668243408203
Epoch 20, val loss: 1.926072359085083
Epoch 30, training loss: 1.994816780090332 = 1.908855676651001 + 0.01 * 8.596111297607422
Epoch 30, val loss: 1.90735924243927
Epoch 40, training loss: 1.9682592153549194 = 1.8823336362838745 + 0.01 * 8.592559814453125
Epoch 40, val loss: 1.8805460929870605
Epoch 50, training loss: 1.9320955276489258 = 1.8464454412460327 + 0.01 * 8.565014839172363
Epoch 50, val loss: 1.8461118936538696
Epoch 60, training loss: 1.8936502933502197 = 1.8090434074401855 + 0.01 * 8.460693359375
Epoch 60, val loss: 1.814622402191162
Epoch 70, training loss: 1.86387038230896 = 1.7814157009124756 + 0.01 * 8.245463371276855
Epoch 70, val loss: 1.7930374145507812
Epoch 80, training loss: 1.8295259475708008 = 1.7481173276901245 + 0.01 * 8.140863418579102
Epoch 80, val loss: 1.7621804475784302
Epoch 90, training loss: 1.7814499139785767 = 1.7015297412872314 + 0.01 * 7.992020130157471
Epoch 90, val loss: 1.7215888500213623
Epoch 100, training loss: 1.7142291069030762 = 1.6366344690322876 + 0.01 * 7.759459495544434
Epoch 100, val loss: 1.6667553186416626
Epoch 110, training loss: 1.6297001838684082 = 1.554617166519165 + 0.01 * 7.508299827575684
Epoch 110, val loss: 1.5985183715820312
Epoch 120, training loss: 1.5396825075149536 = 1.465785026550293 + 0.01 * 7.389744281768799
Epoch 120, val loss: 1.5251985788345337
Epoch 130, training loss: 1.4522172212600708 = 1.3788223266601562 + 0.01 * 7.339488983154297
Epoch 130, val loss: 1.4533792734146118
Epoch 140, training loss: 1.3646429777145386 = 1.2915838956832886 + 0.01 * 7.305910587310791
Epoch 140, val loss: 1.3810806274414062
Epoch 150, training loss: 1.2721362113952637 = 1.1993259191513062 + 0.01 * 7.281025409698486
Epoch 150, val loss: 1.3042179346084595
Epoch 160, training loss: 1.174261212348938 = 1.1015583276748657 + 0.01 * 7.270294189453125
Epoch 160, val loss: 1.2236770391464233
Epoch 170, training loss: 1.075108528137207 = 1.0024356842041016 + 0.01 * 7.267286777496338
Epoch 170, val loss: 1.1433194875717163
Epoch 180, training loss: 0.9809545278549194 = 0.9082961678504944 + 0.01 * 7.265836715698242
Epoch 180, val loss: 1.0685806274414062
Epoch 190, training loss: 0.8968002796173096 = 0.8241584897041321 + 0.01 * 7.264178276062012
Epoch 190, val loss: 1.0040408372879028
Epoch 200, training loss: 0.8245839476585388 = 0.7519738078117371 + 0.01 * 7.26101541519165
Epoch 200, val loss: 0.9513124823570251
Epoch 210, training loss: 0.7630997896194458 = 0.6905422210693359 + 0.01 * 7.2557573318481445
Epoch 210, val loss: 0.9092615842819214
Epoch 220, training loss: 0.7091094851493835 = 0.6366262435913086 + 0.01 * 7.248325824737549
Epoch 220, val loss: 0.8749532699584961
Epoch 230, training loss: 0.6590394973754883 = 0.5866591930389404 + 0.01 * 7.238028049468994
Epoch 230, val loss: 0.8447914123535156
Epoch 240, training loss: 0.6101599335670471 = 0.5379223227500916 + 0.01 * 7.223759651184082
Epoch 240, val loss: 0.8166264295578003
Epoch 250, training loss: 0.5611914396286011 = 0.4891093969345093 + 0.01 * 7.208203315734863
Epoch 250, val loss: 0.7895054817199707
Epoch 260, training loss: 0.5120624899864197 = 0.4401742219924927 + 0.01 * 7.188828468322754
Epoch 260, val loss: 0.7638598680496216
Epoch 270, training loss: 0.46356186270713806 = 0.3918488025665283 + 0.01 * 7.171305179595947
Epoch 270, val loss: 0.7404012680053711
Epoch 280, training loss: 0.416592538356781 = 0.34502166509628296 + 0.01 * 7.157088756561279
Epoch 280, val loss: 0.7196698188781738
Epoch 290, training loss: 0.37193143367767334 = 0.30045220255851746 + 0.01 * 7.147921562194824
Epoch 290, val loss: 0.7021337151527405
Epoch 300, training loss: 0.33028972148895264 = 0.2588566839694977 + 0.01 * 7.143302917480469
Epoch 300, val loss: 0.6882454752922058
Epoch 310, training loss: 0.29251137375831604 = 0.22107429802417755 + 0.01 * 7.143707275390625
Epoch 310, val loss: 0.6783552169799805
Epoch 320, training loss: 0.2591555714607239 = 0.18779030442237854 + 0.01 * 7.136526584625244
Epoch 320, val loss: 0.6728704571723938
Epoch 330, training loss: 0.2305913120508194 = 0.15925416350364685 + 0.01 * 7.1337151527404785
Epoch 330, val loss: 0.6715202331542969
Epoch 340, training loss: 0.20666000247001648 = 0.13527028262615204 + 0.01 * 7.13897180557251
Epoch 340, val loss: 0.6737675666809082
Epoch 350, training loss: 0.18664860725402832 = 0.11536277085542679 + 0.01 * 7.128582954406738
Epoch 350, val loss: 0.6788836717605591
Epoch 360, training loss: 0.17017492651939392 = 0.09891417622566223 + 0.01 * 7.126075744628906
Epoch 360, val loss: 0.685966968536377
Epoch 370, training loss: 0.1565854251384735 = 0.08531467616558075 + 0.01 * 7.127075672149658
Epoch 370, val loss: 0.6944877505302429
Epoch 380, training loss: 0.14523597061634064 = 0.07403452694416046 + 0.01 * 7.120144367218018
Epoch 380, val loss: 0.7038120627403259
Epoch 390, training loss: 0.13579803705215454 = 0.06462746113538742 + 0.01 * 7.117058277130127
Epoch 390, val loss: 0.7136062979698181
Epoch 400, training loss: 0.12792547047138214 = 0.05673777684569359 + 0.01 * 7.118769645690918
Epoch 400, val loss: 0.7236064672470093
Epoch 410, training loss: 0.1212121918797493 = 0.050089187920093536 + 0.01 * 7.112300395965576
Epoch 410, val loss: 0.7336354851722717
Epoch 420, training loss: 0.11550827324390411 = 0.044455863535404205 + 0.01 * 7.105240821838379
Epoch 420, val loss: 0.7436076998710632
Epoch 430, training loss: 0.11070212721824646 = 0.03965997323393822 + 0.01 * 7.104215621948242
Epoch 430, val loss: 0.7534872889518738
Epoch 440, training loss: 0.10653512179851532 = 0.03556160256266594 + 0.01 * 7.097351551055908
Epoch 440, val loss: 0.763245165348053
Epoch 450, training loss: 0.10298189520835876 = 0.0320436917245388 + 0.01 * 7.093821048736572
Epoch 450, val loss: 0.7728398442268372
Epoch 460, training loss: 0.09988418221473694 = 0.029009440913796425 + 0.01 * 7.087474822998047
Epoch 460, val loss: 0.7822503447532654
Epoch 470, training loss: 0.0973123162984848 = 0.026381466537714005 + 0.01 * 7.093084812164307
Epoch 470, val loss: 0.7914451956748962
Epoch 480, training loss: 0.09489211440086365 = 0.024098247289657593 + 0.01 * 7.079387187957764
Epoch 480, val loss: 0.8004527688026428
Epoch 490, training loss: 0.09285388886928558 = 0.022101406008005142 + 0.01 * 7.075248718261719
Epoch 490, val loss: 0.8092318773269653
Epoch 500, training loss: 0.0910332053899765 = 0.02034342847764492 + 0.01 * 7.0689778327941895
Epoch 500, val loss: 0.8178255558013916
Epoch 510, training loss: 0.08945194631814957 = 0.018784070387482643 + 0.01 * 7.066788196563721
Epoch 510, val loss: 0.8263064622879028
Epoch 520, training loss: 0.0880165547132492 = 0.017392467707395554 + 0.01 * 7.062408924102783
Epoch 520, val loss: 0.8346722722053528
Epoch 530, training loss: 0.08675448596477509 = 0.01614440605044365 + 0.01 * 7.061008453369141
Epoch 530, val loss: 0.8429323434829712
Epoch 540, training loss: 0.08552753180265427 = 0.015022799372673035 + 0.01 * 7.050473690032959
Epoch 540, val loss: 0.8511443138122559
Epoch 550, training loss: 0.08448582142591476 = 0.014012346044182777 + 0.01 * 7.0473480224609375
Epoch 550, val loss: 0.8592432141304016
Epoch 560, training loss: 0.08348965644836426 = 0.013099532574415207 + 0.01 * 7.0390119552612305
Epoch 560, val loss: 0.8671222925186157
Epoch 570, training loss: 0.08282627165317535 = 0.012274205684661865 + 0.01 * 7.055206775665283
Epoch 570, val loss: 0.8749275207519531
Epoch 580, training loss: 0.08178055286407471 = 0.011528262868523598 + 0.01 * 7.025229454040527
Epoch 580, val loss: 0.8824588656425476
Epoch 590, training loss: 0.08120889961719513 = 0.01085006631910801 + 0.01 * 7.035883903503418
Epoch 590, val loss: 0.889854907989502
Epoch 600, training loss: 0.08041056245565414 = 0.010231108404695988 + 0.01 * 7.017945766448975
Epoch 600, val loss: 0.8971561193466187
Epoch 610, training loss: 0.07974948734045029 = 0.009662969037890434 + 0.01 * 7.0086517333984375
Epoch 610, val loss: 0.9043912291526794
Epoch 620, training loss: 0.07935940474271774 = 0.00913985725492239 + 0.01 * 7.021954536437988
Epoch 620, val loss: 0.9115071296691895
Epoch 630, training loss: 0.0785788744688034 = 0.00865779910236597 + 0.01 * 6.99210786819458
Epoch 630, val loss: 0.9186092615127563
Epoch 640, training loss: 0.07838835567235947 = 0.00821242667734623 + 0.01 * 7.017592906951904
Epoch 640, val loss: 0.9256166219711304
Epoch 650, training loss: 0.07770675420761108 = 0.0078020887449383736 + 0.01 * 6.990466594696045
Epoch 650, val loss: 0.9323978424072266
Epoch 660, training loss: 0.07726410031318665 = 0.0074223969131708145 + 0.01 * 6.984170913696289
Epoch 660, val loss: 0.9391103982925415
Epoch 670, training loss: 0.07677314430475235 = 0.007070754189044237 + 0.01 * 6.97023868560791
Epoch 670, val loss: 0.9457029104232788
Epoch 680, training loss: 0.07651957869529724 = 0.00674445042386651 + 0.01 * 6.977512836456299
Epoch 680, val loss: 0.9521352052688599
Epoch 690, training loss: 0.07618962973356247 = 0.006443118676543236 + 0.01 * 6.974650859832764
Epoch 690, val loss: 0.95839923620224
Epoch 700, training loss: 0.07570838928222656 = 0.006162846926599741 + 0.01 * 6.954554557800293
Epoch 700, val loss: 0.9645497798919678
Epoch 710, training loss: 0.07544273138046265 = 0.005903075449168682 + 0.01 * 6.953965663909912
Epoch 710, val loss: 0.9705018401145935
Epoch 720, training loss: 0.07504363358020782 = 0.005660815630108118 + 0.01 * 6.938281536102295
Epoch 720, val loss: 0.9764197468757629
Epoch 730, training loss: 0.07470224797725677 = 0.005435165483504534 + 0.01 * 6.926707744598389
Epoch 730, val loss: 0.9821019172668457
Epoch 740, training loss: 0.07472158223390579 = 0.005224467720836401 + 0.01 * 6.949711799621582
Epoch 740, val loss: 0.9877830147743225
Epoch 750, training loss: 0.07432421296834946 = 0.00502686807885766 + 0.01 * 6.929734230041504
Epoch 750, val loss: 0.9930568933486938
Epoch 760, training loss: 0.07426688075065613 = 0.004842686001211405 + 0.01 * 6.942419528961182
Epoch 760, val loss: 0.9984762668609619
Epoch 770, training loss: 0.07366066426038742 = 0.004668913781642914 + 0.01 * 6.89917516708374
Epoch 770, val loss: 1.0035736560821533
Epoch 780, training loss: 0.07397884875535965 = 0.004506500903517008 + 0.01 * 6.947235107421875
Epoch 780, val loss: 1.0087127685546875
Epoch 790, training loss: 0.07326654344797134 = 0.004352874588221312 + 0.01 * 6.891366958618164
Epoch 790, val loss: 1.0135222673416138
Epoch 800, training loss: 0.07335007190704346 = 0.004209994804114103 + 0.01 * 6.914007663726807
Epoch 800, val loss: 1.0184348821640015
Epoch 810, training loss: 0.07283394038677216 = 0.004074401222169399 + 0.01 * 6.8759541511535645
Epoch 810, val loss: 1.0230941772460938
Epoch 820, training loss: 0.07279187440872192 = 0.003946797922253609 + 0.01 * 6.88450813293457
Epoch 820, val loss: 1.0276660919189453
Epoch 830, training loss: 0.07255438715219498 = 0.003826295491307974 + 0.01 * 6.872808933258057
Epoch 830, val loss: 1.032180905342102
Epoch 840, training loss: 0.07245417684316635 = 0.0037120755296200514 + 0.01 * 6.874210834503174
Epoch 840, val loss: 1.0364683866500854
Epoch 850, training loss: 0.07223594188690186 = 0.0036044467706233263 + 0.01 * 6.863149642944336
Epoch 850, val loss: 1.0408345460891724
Epoch 860, training loss: 0.0721256285905838 = 0.0035018750932067633 + 0.01 * 6.862375259399414
Epoch 860, val loss: 1.0448278188705444
Epoch 870, training loss: 0.0718495175242424 = 0.00340539263561368 + 0.01 * 6.844412803649902
Epoch 870, val loss: 1.0489941835403442
Epoch 880, training loss: 0.07179657369852066 = 0.003313658758997917 + 0.01 * 6.848291397094727
Epoch 880, val loss: 1.0529496669769287
Epoch 890, training loss: 0.07157532870769501 = 0.003225708147510886 + 0.01 * 6.834962368011475
Epoch 890, val loss: 1.056792140007019
Epoch 900, training loss: 0.07165024429559708 = 0.003142674220725894 + 0.01 * 6.850756645202637
Epoch 900, val loss: 1.0605989694595337
Epoch 910, training loss: 0.07134892046451569 = 0.0030632757116109133 + 0.01 * 6.828564643859863
Epoch 910, val loss: 1.0643060207366943
Epoch 920, training loss: 0.07126562297344208 = 0.0029878192581236362 + 0.01 * 6.827780723571777
Epoch 920, val loss: 1.0680419206619263
Epoch 930, training loss: 0.07112421095371246 = 0.002915088552981615 + 0.01 * 6.820911884307861
Epoch 930, val loss: 1.0714573860168457
Epoch 940, training loss: 0.07116252183914185 = 0.0028461813926696777 + 0.01 * 6.831634521484375
Epoch 940, val loss: 1.0750656127929688
Epoch 950, training loss: 0.07104811817407608 = 0.0027803671546280384 + 0.01 * 6.826774597167969
Epoch 950, val loss: 1.0784764289855957
Epoch 960, training loss: 0.0707080066204071 = 0.002717570634558797 + 0.01 * 6.799043655395508
Epoch 960, val loss: 1.081765055656433
Epoch 970, training loss: 0.07058370858430862 = 0.002657270524650812 + 0.01 * 6.792644023895264
Epoch 970, val loss: 1.0850286483764648
Epoch 980, training loss: 0.07060174643993378 = 0.002599666826426983 + 0.01 * 6.800208568572998
Epoch 980, val loss: 1.0883073806762695
Epoch 990, training loss: 0.07060997933149338 = 0.002544282004237175 + 0.01 * 6.806569576263428
Epoch 990, val loss: 1.0914700031280518
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 2.0358173847198486 = 1.9498487710952759 + 0.01 * 8.596869468688965
Epoch 0, val loss: 1.9424105882644653
Epoch 10, training loss: 2.025513172149658 = 1.939544916152954 + 0.01 * 8.596817016601562
Epoch 10, val loss: 1.9318335056304932
Epoch 20, training loss: 2.0128345489501953 = 1.9268683195114136 + 0.01 * 8.5966157913208
Epoch 20, val loss: 1.9187036752700806
Epoch 30, training loss: 1.9951889514923096 = 1.9092302322387695 + 0.01 * 8.595870018005371
Epoch 30, val loss: 1.900529384613037
Epoch 40, training loss: 1.969368815422058 = 1.8834565877914429 + 0.01 * 8.59122085571289
Epoch 40, val loss: 1.8745821714401245
Epoch 50, training loss: 1.933680772781372 = 1.848084568977356 + 0.01 * 8.559616088867188
Epoch 50, val loss: 1.8411445617675781
Epoch 60, training loss: 1.894842267036438 = 1.8104928731918335 + 0.01 * 8.4349365234375
Epoch 60, val loss: 1.8106471300125122
Epoch 70, training loss: 1.8629473447799683 = 1.7810100317001343 + 0.01 * 8.193733215332031
Epoch 70, val loss: 1.789879322052002
Epoch 80, training loss: 1.8264427185058594 = 1.7451848983764648 + 0.01 * 8.125784873962402
Epoch 80, val loss: 1.7588293552398682
Epoch 90, training loss: 1.7745442390441895 = 1.6945480108261108 + 0.01 * 7.999621868133545
Epoch 90, val loss: 1.713180422782898
Epoch 100, training loss: 1.7011319398880005 = 1.622624158859253 + 0.01 * 7.8507819175720215
Epoch 100, val loss: 1.6499978303909302
Epoch 110, training loss: 1.6112627983093262 = 1.5346474647521973 + 0.01 * 7.661530017852783
Epoch 110, val loss: 1.5754482746124268
Epoch 120, training loss: 1.5169970989227295 = 1.442768931388855 + 0.01 * 7.422821998596191
Epoch 120, val loss: 1.4999258518218994
Epoch 130, training loss: 1.428061604499817 = 1.3541123867034912 + 0.01 * 7.39492654800415
Epoch 130, val loss: 1.4296715259552002
Epoch 140, training loss: 1.3427810668945312 = 1.269314169883728 + 0.01 * 7.346695423126221
Epoch 140, val loss: 1.3650370836257935
Epoch 150, training loss: 1.2586513757705688 = 1.1854420900344849 + 0.01 * 7.320934295654297
Epoch 150, val loss: 1.3020397424697876
Epoch 160, training loss: 1.1725484132766724 = 1.0996737480163574 + 0.01 * 7.2874627113342285
Epoch 160, val loss: 1.2391712665557861
Epoch 170, training loss: 1.0829174518585205 = 1.0103874206542969 + 0.01 * 7.253008842468262
Epoch 170, val loss: 1.1746234893798828
Epoch 180, training loss: 0.9918187260627747 = 0.9194638729095459 + 0.01 * 7.235485553741455
Epoch 180, val loss: 1.1096793413162231
Epoch 190, training loss: 0.9044795632362366 = 0.8321765661239624 + 0.01 * 7.230300426483154
Epoch 190, val loss: 1.0489095449447632
Epoch 200, training loss: 0.8256251811981201 = 0.7533826231956482 + 0.01 * 7.224255084991455
Epoch 200, val loss: 0.9966306090354919
Epoch 210, training loss: 0.756637692451477 = 0.6844720840454102 + 0.01 * 7.216560363769531
Epoch 210, val loss: 0.95441734790802
Epoch 220, training loss: 0.6956815123558044 = 0.623612642288208 + 0.01 * 7.2068867683410645
Epoch 220, val loss: 0.9206625819206238
Epoch 230, training loss: 0.6401317119598389 = 0.5681756138801575 + 0.01 * 7.195611000061035
Epoch 230, val loss: 0.8930516839027405
Epoch 240, training loss: 0.5881932973861694 = 0.5163789987564087 + 0.01 * 7.181431293487549
Epoch 240, val loss: 0.870077908039093
Epoch 250, training loss: 0.5393269062042236 = 0.46766507625579834 + 0.01 * 7.166180610656738
Epoch 250, val loss: 0.8514963388442993
Epoch 260, training loss: 0.49335741996765137 = 0.4219091534614563 + 0.01 * 7.144827365875244
Epoch 260, val loss: 0.8379665017127991
Epoch 270, training loss: 0.4503965675830841 = 0.3791385591030121 + 0.01 * 7.125800609588623
Epoch 270, val loss: 0.8296961784362793
Epoch 280, training loss: 0.41049712896347046 = 0.3394964635372162 + 0.01 * 7.100068092346191
Epoch 280, val loss: 0.8261706829071045
Epoch 290, training loss: 0.3739572763442993 = 0.30313020944595337 + 0.01 * 7.082705974578857
Epoch 290, val loss: 0.8268612623214722
Epoch 300, training loss: 0.34063655138015747 = 0.270044207572937 + 0.01 * 7.059235572814941
Epoch 300, val loss: 0.8311331272125244
Epoch 310, training loss: 0.3106374740600586 = 0.24010847508907318 + 0.01 * 7.052900314331055
Epoch 310, val loss: 0.838221549987793
Epoch 320, training loss: 0.2834708094596863 = 0.2130957990884781 + 0.01 * 7.037501811981201
Epoch 320, val loss: 0.8472225069999695
Epoch 330, training loss: 0.25905120372772217 = 0.18875008821487427 + 0.01 * 7.030110836029053
Epoch 330, val loss: 0.8575220108032227
Epoch 340, training loss: 0.23708122968673706 = 0.166863352060318 + 0.01 * 7.021788597106934
Epoch 340, val loss: 0.8687189221382141
Epoch 350, training loss: 0.2174106389284134 = 0.14725585281848907 + 0.01 * 7.01547908782959
Epoch 350, val loss: 0.8805685639381409
Epoch 360, training loss: 0.19988983869552612 = 0.12977616488933563 + 0.01 * 7.011368274688721
Epoch 360, val loss: 0.8928617238998413
Epoch 370, training loss: 0.18431955575942993 = 0.1142544224858284 + 0.01 * 7.006512641906738
Epoch 370, val loss: 0.9054928421974182
Epoch 380, training loss: 0.17055267095565796 = 0.10053138434886932 + 0.01 * 7.002129077911377
Epoch 380, val loss: 0.9183462858200073
Epoch 390, training loss: 0.15849721431732178 = 0.08845867216587067 + 0.01 * 7.003854274749756
Epoch 390, val loss: 0.931445837020874
Epoch 400, training loss: 0.1478942334651947 = 0.07790733873844147 + 0.01 * 6.998688697814941
Epoch 400, val loss: 0.9446830153465271
Epoch 410, training loss: 0.13864564895629883 = 0.06873957067728043 + 0.01 * 6.990607738494873
Epoch 410, val loss: 0.9580115675926208
Epoch 420, training loss: 0.13067543506622314 = 0.06080494076013565 + 0.01 * 6.987050533294678
Epoch 420, val loss: 0.9714289903640747
Epoch 430, training loss: 0.12382501363754272 = 0.05394742265343666 + 0.01 * 6.987759113311768
Epoch 430, val loss: 0.9847820997238159
Epoch 440, training loss: 0.11796142160892487 = 0.04802486672997475 + 0.01 * 6.993655681610107
Epoch 440, val loss: 0.9979016184806824
Epoch 450, training loss: 0.1126481220126152 = 0.042908214032649994 + 0.01 * 6.9739909172058105
Epoch 450, val loss: 1.010817050933838
Epoch 460, training loss: 0.1081869974732399 = 0.03847520798444748 + 0.01 * 6.971179008483887
Epoch 460, val loss: 1.023457407951355
Epoch 470, training loss: 0.10430191457271576 = 0.034625228494405746 + 0.01 * 6.967668056488037
Epoch 470, val loss: 1.0357863903045654
Epoch 480, training loss: 0.10099188983440399 = 0.031275395303964615 + 0.01 * 6.971650123596191
Epoch 480, val loss: 1.0478016138076782
Epoch 490, training loss: 0.09797409176826477 = 0.028356138616800308 + 0.01 * 6.961795806884766
Epoch 490, val loss: 1.059430718421936
Epoch 500, training loss: 0.09534326195716858 = 0.02580263838171959 + 0.01 * 6.954062461853027
Epoch 500, val loss: 1.0707166194915771
Epoch 510, training loss: 0.09323462843894958 = 0.023562539368867874 + 0.01 * 6.967209339141846
Epoch 510, val loss: 1.0817042589187622
Epoch 520, training loss: 0.09106148779392242 = 0.02159530110657215 + 0.01 * 6.946619033813477
Epoch 520, val loss: 1.0922966003417969
Epoch 530, training loss: 0.0892728641629219 = 0.019859425723552704 + 0.01 * 6.941344261169434
Epoch 530, val loss: 1.1025978326797485
Epoch 540, training loss: 0.08779116719961166 = 0.018322641029953957 + 0.01 * 6.946853160858154
Epoch 540, val loss: 1.1125596761703491
Epoch 550, training loss: 0.08631926774978638 = 0.016959404572844505 + 0.01 * 6.935986518859863
Epoch 550, val loss: 1.122233510017395
Epoch 560, training loss: 0.0850614607334137 = 0.015743939206004143 + 0.01 * 6.9317522048950195
Epoch 560, val loss: 1.131607174873352
Epoch 570, training loss: 0.08392617106437683 = 0.014656750485301018 + 0.01 * 6.926942348480225
Epoch 570, val loss: 1.1407184600830078
Epoch 580, training loss: 0.08298929035663605 = 0.013681523501873016 + 0.01 * 6.930776596069336
Epoch 580, val loss: 1.149520754814148
Epoch 590, training loss: 0.08198394626379013 = 0.01280404906719923 + 0.01 * 6.917990207672119
Epoch 590, val loss: 1.1580512523651123
Epoch 600, training loss: 0.08115401864051819 = 0.012011796236038208 + 0.01 * 6.914222240447998
Epoch 600, val loss: 1.166297435760498
Epoch 610, training loss: 0.08041542768478394 = 0.011294395662844181 + 0.01 * 6.912103652954102
Epoch 610, val loss: 1.1743276119232178
Epoch 620, training loss: 0.07985251396894455 = 0.010642952285706997 + 0.01 * 6.920956134796143
Epoch 620, val loss: 1.1821069717407227
Epoch 630, training loss: 0.07910122722387314 = 0.010050220414996147 + 0.01 * 6.9051008224487305
Epoch 630, val loss: 1.1896111965179443
Epoch 640, training loss: 0.07847947627305984 = 0.00950932689011097 + 0.01 * 6.89701509475708
Epoch 640, val loss: 1.1969397068023682
Epoch 650, training loss: 0.07794080674648285 = 0.009014476090669632 + 0.01 * 6.892632961273193
Epoch 650, val loss: 1.2040529251098633
Epoch 660, training loss: 0.07745392620563507 = 0.008560225367546082 + 0.01 * 6.889369964599609
Epoch 660, val loss: 1.2109683752059937
Epoch 670, training loss: 0.0770764872431755 = 0.008142386563122272 + 0.01 * 6.8934102058410645
Epoch 670, val loss: 1.217670202255249
Epoch 680, training loss: 0.07652292400598526 = 0.007757445331662893 + 0.01 * 6.876547813415527
Epoch 680, val loss: 1.2241636514663696
Epoch 690, training loss: 0.07609892636537552 = 0.0074017951264977455 + 0.01 * 6.869713306427002
Epoch 690, val loss: 1.2305139303207397
Epoch 700, training loss: 0.0758928507566452 = 0.007072128355503082 + 0.01 * 6.882072448730469
Epoch 700, val loss: 1.2367182970046997
Epoch 710, training loss: 0.07544334977865219 = 0.006766947451978922 + 0.01 * 6.867640972137451
Epoch 710, val loss: 1.2426575422286987
Epoch 720, training loss: 0.07519105076789856 = 0.00648367078974843 + 0.01 * 6.8707380294799805
Epoch 720, val loss: 1.2484883069992065
Epoch 730, training loss: 0.07473098486661911 = 0.006220035720616579 + 0.01 * 6.851095199584961
Epoch 730, val loss: 1.2541512250900269
Epoch 740, training loss: 0.0744214579463005 = 0.005973837338387966 + 0.01 * 6.844761848449707
Epoch 740, val loss: 1.2597122192382812
Epoch 750, training loss: 0.07426491379737854 = 0.005743740126490593 + 0.01 * 6.85211706161499
Epoch 750, val loss: 1.265044093132019
Epoch 760, training loss: 0.0740104466676712 = 0.005529116839170456 + 0.01 * 6.848133087158203
Epoch 760, val loss: 1.270325779914856
Epoch 770, training loss: 0.07367919385433197 = 0.0053278738632798195 + 0.01 * 6.835132598876953
Epoch 770, val loss: 1.2754499912261963
Epoch 780, training loss: 0.07346682250499725 = 0.005138785112649202 + 0.01 * 6.832803726196289
Epoch 780, val loss: 1.2804069519042969
Epoch 790, training loss: 0.07317843288183212 = 0.004960970487445593 + 0.01 * 6.821746349334717
Epoch 790, val loss: 1.2853165864944458
Epoch 800, training loss: 0.07323405146598816 = 0.004793277010321617 + 0.01 * 6.844077110290527
Epoch 800, val loss: 1.2901097536087036
Epoch 810, training loss: 0.07293505221605301 = 0.004635585471987724 + 0.01 * 6.829946517944336
Epoch 810, val loss: 1.2947145700454712
Epoch 820, training loss: 0.07278280705213547 = 0.004486783407628536 + 0.01 * 6.8296027183532715
Epoch 820, val loss: 1.2992891073226929
Epoch 830, training loss: 0.0726761445403099 = 0.004347397945821285 + 0.01 * 6.832874774932861
Epoch 830, val loss: 1.3036367893218994
Epoch 840, training loss: 0.0722363069653511 = 0.004214787390083075 + 0.01 * 6.802152156829834
Epoch 840, val loss: 1.3079313039779663
Epoch 850, training loss: 0.07206156104803085 = 0.004089603200554848 + 0.01 * 6.797196388244629
Epoch 850, val loss: 1.3121545314788818
Epoch 860, training loss: 0.07230718433856964 = 0.003970455843955278 + 0.01 * 6.833672523498535
Epoch 860, val loss: 1.3162634372711182
Epoch 870, training loss: 0.07183187454938889 = 0.0038578854873776436 + 0.01 * 6.797398567199707
Epoch 870, val loss: 1.3202743530273438
Epoch 880, training loss: 0.07158490270376205 = 0.003750754287466407 + 0.01 * 6.783414840698242
Epoch 880, val loss: 1.3242062330245972
Epoch 890, training loss: 0.07175736874341965 = 0.0036489656195044518 + 0.01 * 6.810840129852295
Epoch 890, val loss: 1.3280209302902222
Epoch 900, training loss: 0.07133162766695023 = 0.0035520941019058228 + 0.01 * 6.777953624725342
Epoch 900, val loss: 1.3317642211914062
Epoch 910, training loss: 0.07133249938488007 = 0.0034600149374455214 + 0.01 * 6.7872490882873535
Epoch 910, val loss: 1.335451364517212
Epoch 920, training loss: 0.07110229134559631 = 0.0033720999490469694 + 0.01 * 6.773018836975098
Epoch 920, val loss: 1.339050531387329
Epoch 930, training loss: 0.07105619460344315 = 0.003288109553977847 + 0.01 * 6.776808261871338
Epoch 930, val loss: 1.342513918876648
Epoch 940, training loss: 0.07095851749181747 = 0.0032082917168736458 + 0.01 * 6.775022506713867
Epoch 940, val loss: 1.345988392829895
Epoch 950, training loss: 0.07081565260887146 = 0.0031319106929004192 + 0.01 * 6.768374443054199
Epoch 950, val loss: 1.3493565320968628
Epoch 960, training loss: 0.07082987576723099 = 0.003058690344914794 + 0.01 * 6.777118682861328
Epoch 960, val loss: 1.3526337146759033
Epoch 970, training loss: 0.07059560716152191 = 0.0029887345153838396 + 0.01 * 6.760687828063965
Epoch 970, val loss: 1.3558329343795776
Epoch 980, training loss: 0.0704222097992897 = 0.0029220127034932375 + 0.01 * 6.7500200271606445
Epoch 980, val loss: 1.3590540885925293
Epoch 990, training loss: 0.07043904811143875 = 0.0028578939381986856 + 0.01 * 6.758115291595459
Epoch 990, val loss: 1.3621134757995605
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8102266736953084
The final CL Acc:0.76790, 0.00924, The final GNN Acc:0.80742, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13154])
remove edge: torch.Size([2, 7956])
updated graph: torch.Size([2, 10554])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0249953269958496 = 1.9390267133712769 + 0.01 * 8.59686279296875
Epoch 0, val loss: 1.9449537992477417
Epoch 10, training loss: 2.0149548053741455 = 1.928986668586731 + 0.01 * 8.59680461883545
Epoch 10, val loss: 1.9345027208328247
Epoch 20, training loss: 2.002134084701538 = 1.9161683320999146 + 0.01 * 8.596582412719727
Epoch 20, val loss: 1.920953631401062
Epoch 30, training loss: 1.9837654829025269 = 1.8978081941604614 + 0.01 * 8.595727920532227
Epoch 30, val loss: 1.901418685913086
Epoch 40, training loss: 1.9566776752471924 = 1.870780110359192 + 0.01 * 8.58975887298584
Epoch 40, val loss: 1.8731839656829834
Epoch 50, training loss: 1.9198614358901978 = 1.8343923091888428 + 0.01 * 8.546917915344238
Epoch 50, val loss: 1.8370559215545654
Epoch 60, training loss: 1.8803296089172363 = 1.796582818031311 + 0.01 * 8.374685287475586
Epoch 60, val loss: 1.8031623363494873
Epoch 70, training loss: 1.8450545072555542 = 1.7640509605407715 + 0.01 * 8.100358009338379
Epoch 70, val loss: 1.7743786573410034
Epoch 80, training loss: 1.7996903657913208 = 1.7201541662216187 + 0.01 * 7.953620910644531
Epoch 80, val loss: 1.7331302165985107
Epoch 90, training loss: 1.7363078594207764 = 1.6586843729019165 + 0.01 * 7.762348175048828
Epoch 90, val loss: 1.6778291463851929
Epoch 100, training loss: 1.6532379388809204 = 1.577235460281372 + 0.01 * 7.600242614746094
Epoch 100, val loss: 1.6067620515823364
Epoch 110, training loss: 1.5582761764526367 = 1.4830611944198608 + 0.01 * 7.521494388580322
Epoch 110, val loss: 1.5262635946273804
Epoch 120, training loss: 1.4588955640792847 = 1.3840528726577759 + 0.01 * 7.4842681884765625
Epoch 120, val loss: 1.4422820806503296
Epoch 130, training loss: 1.3572607040405273 = 1.2828967571258545 + 0.01 * 7.436398506164551
Epoch 130, val loss: 1.3581228256225586
Epoch 140, training loss: 1.253456950187683 = 1.179632544517517 + 0.01 * 7.382438659667969
Epoch 140, val loss: 1.272735357284546
Epoch 150, training loss: 1.1497561931610107 = 1.076447606086731 + 0.01 * 7.3308610916137695
Epoch 150, val loss: 1.1879522800445557
Epoch 160, training loss: 1.0508379936218262 = 0.9778856635093689 + 0.01 * 7.295236110687256
Epoch 160, val loss: 1.1075953245162964
Epoch 170, training loss: 0.9607017636299133 = 0.8880089521408081 + 0.01 * 7.269279479980469
Epoch 170, val loss: 1.0357770919799805
Epoch 180, training loss: 0.8809055089950562 = 0.8084131479263306 + 0.01 * 7.249239444732666
Epoch 180, val loss: 0.9743092656135559
Epoch 190, training loss: 0.8110448122024536 = 0.7386932373046875 + 0.01 * 7.23515510559082
Epoch 190, val loss: 0.9238535165786743
Epoch 200, training loss: 0.7497605085372925 = 0.6775075197219849 + 0.01 * 7.225301742553711
Epoch 200, val loss: 0.8840026259422302
Epoch 210, training loss: 0.6956409811973572 = 0.6234727501869202 + 0.01 * 7.216825008392334
Epoch 210, val loss: 0.8538303971290588
Epoch 220, training loss: 0.6477208733558655 = 0.5756303668022156 + 0.01 * 7.20905065536499
Epoch 220, val loss: 0.832109272480011
Epoch 230, training loss: 0.6052438020706177 = 0.533222496509552 + 0.01 * 7.202133655548096
Epoch 230, val loss: 0.8173456788063049
Epoch 240, training loss: 0.5671456456184387 = 0.4951830804347992 + 0.01 * 7.196256637573242
Epoch 240, val loss: 0.8079336285591125
Epoch 250, training loss: 0.5319334268569946 = 0.46001997590065 + 0.01 * 7.191348552703857
Epoch 250, val loss: 0.8020303845405579
Epoch 260, training loss: 0.49784666299819946 = 0.4259759187698364 + 0.01 * 7.187075138092041
Epoch 260, val loss: 0.7977817058563232
Epoch 270, training loss: 0.46331942081451416 = 0.39148035645484924 + 0.01 * 7.183906555175781
Epoch 270, val loss: 0.79350745677948
Epoch 280, training loss: 0.4272991418838501 = 0.3554981052875519 + 0.01 * 7.180102825164795
Epoch 280, val loss: 0.7886953949928284
Epoch 290, training loss: 0.38991448283195496 = 0.3181413412094116 + 0.01 * 7.177314281463623
Epoch 290, val loss: 0.783762514591217
Epoch 300, training loss: 0.35251086950302124 = 0.2807601988315582 + 0.01 * 7.175067901611328
Epoch 300, val loss: 0.779680609703064
Epoch 310, training loss: 0.31700974702835083 = 0.24529123306274414 + 0.01 * 7.17185115814209
Epoch 310, val loss: 0.7776658535003662
Epoch 320, training loss: 0.2850576639175415 = 0.21337798237800598 + 0.01 * 7.16796875
Epoch 320, val loss: 0.7784274816513062
Epoch 330, training loss: 0.25733304023742676 = 0.18568597733974457 + 0.01 * 7.164706230163574
Epoch 330, val loss: 0.7822273969650269
Epoch 340, training loss: 0.23368453979492188 = 0.16207188367843628 + 0.01 * 7.161266326904297
Epoch 340, val loss: 0.7888519763946533
Epoch 350, training loss: 0.21357974410057068 = 0.1419910043478012 + 0.01 * 7.158874988555908
Epoch 350, val loss: 0.7977377772331238
Epoch 360, training loss: 0.19639991223812103 = 0.1248612329363823 + 0.01 * 7.153868198394775
Epoch 360, val loss: 0.808269202709198
Epoch 370, training loss: 0.18165811896324158 = 0.11018058657646179 + 0.01 * 7.14775276184082
Epoch 370, val loss: 0.8199674487113953
Epoch 380, training loss: 0.16896286606788635 = 0.09754230082035065 + 0.01 * 7.142055988311768
Epoch 380, val loss: 0.8324762582778931
Epoch 390, training loss: 0.1579725593328476 = 0.08661247044801712 + 0.01 * 7.1360087394714355
Epoch 390, val loss: 0.8455185294151306
Epoch 400, training loss: 0.14836916327476501 = 0.07710881531238556 + 0.01 * 7.126035690307617
Epoch 400, val loss: 0.858923614025116
Epoch 410, training loss: 0.14009149372577667 = 0.06880703568458557 + 0.01 * 7.128446102142334
Epoch 410, val loss: 0.8725327253341675
Epoch 420, training loss: 0.13259994983673096 = 0.06153818964958191 + 0.01 * 7.106176853179932
Epoch 420, val loss: 0.8862053751945496
Epoch 430, training loss: 0.12622937560081482 = 0.055165790021419525 + 0.01 * 7.106358528137207
Epoch 430, val loss: 0.8998337388038635
Epoch 440, training loss: 0.12039992213249207 = 0.04957889765501022 + 0.01 * 7.082102298736572
Epoch 440, val loss: 0.9132369160652161
Epoch 450, training loss: 0.1152385026216507 = 0.04467521235346794 + 0.01 * 7.056328773498535
Epoch 450, val loss: 0.9264371395111084
Epoch 460, training loss: 0.11078910529613495 = 0.04036780074238777 + 0.01 * 7.042130470275879
Epoch 460, val loss: 0.9393896460533142
Epoch 470, training loss: 0.10704104602336884 = 0.03658371418714523 + 0.01 * 7.0457329750061035
Epoch 470, val loss: 0.9520860314369202
Epoch 480, training loss: 0.1036066859960556 = 0.03325354680418968 + 0.01 * 7.035314559936523
Epoch 480, val loss: 0.964423418045044
Epoch 490, training loss: 0.10054272413253784 = 0.030319245532155037 + 0.01 * 7.022348403930664
Epoch 490, val loss: 0.9765212535858154
Epoch 500, training loss: 0.09780433773994446 = 0.027727600187063217 + 0.01 * 7.007673263549805
Epoch 500, val loss: 0.9881975054740906
Epoch 510, training loss: 0.09531528502702713 = 0.025433937087655067 + 0.01 * 6.98813533782959
Epoch 510, val loss: 0.9996370673179626
Epoch 520, training loss: 0.09315051883459091 = 0.023398444056510925 + 0.01 * 6.975207805633545
Epoch 520, val loss: 1.0106806755065918
Epoch 530, training loss: 0.09127701073884964 = 0.021587558090686798 + 0.01 * 6.968945503234863
Epoch 530, val loss: 1.0214223861694336
Epoch 540, training loss: 0.0897284522652626 = 0.019972404465079308 + 0.01 * 6.97560453414917
Epoch 540, val loss: 1.0318584442138672
Epoch 550, training loss: 0.08811357617378235 = 0.01852802373468876 + 0.01 * 6.958555698394775
Epoch 550, val loss: 1.041937232017517
Epoch 560, training loss: 0.08658677339553833 = 0.017231998965144157 + 0.01 * 6.9354777336120605
Epoch 560, val loss: 1.0517916679382324
Epoch 570, training loss: 0.0858854427933693 = 0.016063526272773743 + 0.01 * 6.982191562652588
Epoch 570, val loss: 1.0613577365875244
Epoch 580, training loss: 0.08425648510456085 = 0.015009872615337372 + 0.01 * 6.924661159515381
Epoch 580, val loss: 1.0705705881118774
Epoch 590, training loss: 0.08318327367305756 = 0.014053935185074806 + 0.01 * 6.912934303283691
Epoch 590, val loss: 1.079556941986084
Epoch 600, training loss: 0.0823374018073082 = 0.013183163478970528 + 0.01 * 6.91542387008667
Epoch 600, val loss: 1.08834969997406
Epoch 610, training loss: 0.08146217465400696 = 0.012388352304697037 + 0.01 * 6.907382965087891
Epoch 610, val loss: 1.0969043970108032
Epoch 620, training loss: 0.08072395622730255 = 0.011661852709949017 + 0.01 * 6.906210422515869
Epoch 620, val loss: 1.1052767038345337
Epoch 630, training loss: 0.08036883175373077 = 0.010996691882610321 + 0.01 * 6.937214374542236
Epoch 630, val loss: 1.1133971214294434
Epoch 640, training loss: 0.07933683693408966 = 0.010387745685875416 + 0.01 * 6.894908905029297
Epoch 640, val loss: 1.121202826499939
Epoch 650, training loss: 0.07866167277097702 = 0.009828837588429451 + 0.01 * 6.883284091949463
Epoch 650, val loss: 1.1288421154022217
Epoch 660, training loss: 0.0781107097864151 = 0.009313778951764107 + 0.01 * 6.879693508148193
Epoch 660, val loss: 1.13627290725708
Epoch 670, training loss: 0.07751234620809555 = 0.00883864052593708 + 0.01 * 6.86737060546875
Epoch 670, val loss: 1.143502116203308
Epoch 680, training loss: 0.07702157646417618 = 0.00840004812926054 + 0.01 * 6.862153053283691
Epoch 680, val loss: 1.1505221128463745
Epoch 690, training loss: 0.07660448551177979 = 0.007993927225470543 + 0.01 * 6.861055850982666
Epoch 690, val loss: 1.157349705696106
Epoch 700, training loss: 0.07613839209079742 = 0.007617838680744171 + 0.01 * 6.852055549621582
Epoch 700, val loss: 1.1639913320541382
Epoch 710, training loss: 0.07576918601989746 = 0.007268939167261124 + 0.01 * 6.850024700164795
Epoch 710, val loss: 1.1704374551773071
Epoch 720, training loss: 0.0753898173570633 = 0.006944537162780762 + 0.01 * 6.8445281982421875
Epoch 720, val loss: 1.1766974925994873
Epoch 730, training loss: 0.07530141621828079 = 0.006642594467848539 + 0.01 * 6.86588191986084
Epoch 730, val loss: 1.1827867031097412
Epoch 740, training loss: 0.07473847270011902 = 0.0063615343533456326 + 0.01 * 6.837693691253662
Epoch 740, val loss: 1.188651204109192
Epoch 750, training loss: 0.07447516918182373 = 0.006099347025156021 + 0.01 * 6.837583065032959
Epoch 750, val loss: 1.1943881511688232
Epoch 760, training loss: 0.07413846254348755 = 0.005854063201695681 + 0.01 * 6.828440189361572
Epoch 760, val loss: 1.1999424695968628
Epoch 770, training loss: 0.0738578513264656 = 0.005624375306069851 + 0.01 * 6.823347568511963
Epoch 770, val loss: 1.205351710319519
Epoch 780, training loss: 0.07425177097320557 = 0.00540874432772398 + 0.01 * 6.884303092956543
Epoch 780, val loss: 1.210587978363037
Epoch 790, training loss: 0.07339254021644592 = 0.0052069248631596565 + 0.01 * 6.81856107711792
Epoch 790, val loss: 1.215711236000061
Epoch 800, training loss: 0.07316193729639053 = 0.005017281509935856 + 0.01 * 6.8144659996032715
Epoch 800, val loss: 1.2206755876541138
Epoch 810, training loss: 0.07292461395263672 = 0.004838672000914812 + 0.01 * 6.80859375
Epoch 810, val loss: 1.2254648208618164
Epoch 820, training loss: 0.07321401685476303 = 0.004670259542763233 + 0.01 * 6.85437536239624
Epoch 820, val loss: 1.2301018238067627
Epoch 830, training loss: 0.07254933565855026 = 0.004511591978371143 + 0.01 * 6.803774356842041
Epoch 830, val loss: 1.2346314191818237
Epoch 840, training loss: 0.07236972451210022 = 0.004361901897937059 + 0.01 * 6.800781726837158
Epoch 840, val loss: 1.2390328645706177
Epoch 850, training loss: 0.07231652736663818 = 0.004220311064273119 + 0.01 * 6.809621334075928
Epoch 850, val loss: 1.2433114051818848
Epoch 860, training loss: 0.07214974611997604 = 0.0040863570757210255 + 0.01 * 6.806338787078857
Epoch 860, val loss: 1.247443675994873
Epoch 870, training loss: 0.07195333391427994 = 0.0039598108269274235 + 0.01 * 6.799352169036865
Epoch 870, val loss: 1.251514196395874
Epoch 880, training loss: 0.07210278511047363 = 0.003839752869680524 + 0.01 * 6.826303005218506
Epoch 880, val loss: 1.255452036857605
Epoch 890, training loss: 0.07175084948539734 = 0.003725970396772027 + 0.01 * 6.802488327026367
Epoch 890, val loss: 1.2592380046844482
Epoch 900, training loss: 0.07143188267946243 = 0.00361809553578496 + 0.01 * 6.781378269195557
Epoch 900, val loss: 1.2629740238189697
Epoch 910, training loss: 0.07153692096471786 = 0.0035155084915459156 + 0.01 * 6.802140712738037
Epoch 910, val loss: 1.2665836811065674
Epoch 920, training loss: 0.0713186115026474 = 0.0034179079812020063 + 0.01 * 6.790070533752441
Epoch 920, val loss: 1.270068645477295
Epoch 930, training loss: 0.07121763378381729 = 0.0033249459229409695 + 0.01 * 6.789268493652344
Epoch 930, val loss: 1.2734981775283813
Epoch 940, training loss: 0.07107578217983246 = 0.003236365970224142 + 0.01 * 6.783941745758057
Epoch 940, val loss: 1.2768288850784302
Epoch 950, training loss: 0.07085804641246796 = 0.0031519655603915453 + 0.01 * 6.770607948303223
Epoch 950, val loss: 1.280089259147644
Epoch 960, training loss: 0.07074163109064102 = 0.003071491839364171 + 0.01 * 6.767014026641846
Epoch 960, val loss: 1.2832460403442383
Epoch 970, training loss: 0.07071800529956818 = 0.002994581125676632 + 0.01 * 6.772342681884766
Epoch 970, val loss: 1.2863231897354126
Epoch 980, training loss: 0.07074156403541565 = 0.0029210250359028578 + 0.01 * 6.7820539474487305
Epoch 980, val loss: 1.2893412113189697
Epoch 990, training loss: 0.07051119208335876 = 0.0028508452232927084 + 0.01 * 6.7660346031188965
Epoch 990, val loss: 1.29222571849823
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.0321285724639893 = 1.9461599588394165 + 0.01 * 8.596860885620117
Epoch 0, val loss: 1.9490333795547485
Epoch 10, training loss: 2.022770643234253 = 1.9368022680282593 + 0.01 * 8.596831321716309
Epoch 10, val loss: 1.9392619132995605
Epoch 20, training loss: 2.0116357803344727 = 1.9256690740585327 + 0.01 * 8.596667289733887
Epoch 20, val loss: 1.9275058507919312
Epoch 30, training loss: 1.9961098432540894 = 1.9101488590240479 + 0.01 * 8.59610366821289
Epoch 30, val loss: 1.9112480878829956
Epoch 40, training loss: 1.9729619026184082 = 1.8870302438735962 + 0.01 * 8.593165397644043
Epoch 40, val loss: 1.8873374462127686
Epoch 50, training loss: 1.9391508102416992 = 1.8534214496612549 + 0.01 * 8.572932243347168
Epoch 50, val loss: 1.8534011840820312
Epoch 60, training loss: 1.8964433670043945 = 1.8116073608398438 + 0.01 * 8.48359489440918
Epoch 60, val loss: 1.8137480020523071
Epoch 70, training loss: 1.8537169694900513 = 1.7721045017242432 + 0.01 * 8.16125202178955
Epoch 70, val loss: 1.7791262865066528
Epoch 80, training loss: 1.8110100030899048 = 1.7307765483856201 + 0.01 * 8.023343086242676
Epoch 80, val loss: 1.740900993347168
Epoch 90, training loss: 1.749914288520813 = 1.6722854375839233 + 0.01 * 7.762887001037598
Epoch 90, val loss: 1.6870737075805664
Epoch 100, training loss: 1.6688778400421143 = 1.5934691429138184 + 0.01 * 7.540874004364014
Epoch 100, val loss: 1.617415428161621
Epoch 110, training loss: 1.5685971975326538 = 1.4945096969604492 + 0.01 * 7.408755302429199
Epoch 110, val loss: 1.5322009325027466
Epoch 120, training loss: 1.4561188220977783 = 1.3824809789657593 + 0.01 * 7.363785743713379
Epoch 120, val loss: 1.4361833333969116
Epoch 130, training loss: 1.3385013341903687 = 1.265184760093689 + 0.01 * 7.331663131713867
Epoch 130, val loss: 1.3374583721160889
Epoch 140, training loss: 1.2219586372375488 = 1.14888334274292 + 0.01 * 7.307528495788574
Epoch 140, val loss: 1.240830659866333
Epoch 150, training loss: 1.110778570175171 = 1.0379384756088257 + 0.01 * 7.284006595611572
Epoch 150, val loss: 1.1497833728790283
Epoch 160, training loss: 1.0090030431747437 = 0.9363231062889099 + 0.01 * 7.267993450164795
Epoch 160, val loss: 1.0668944120407104
Epoch 170, training loss: 0.9197402000427246 = 0.8471505045890808 + 0.01 * 7.258970737457275
Epoch 170, val loss: 0.9948291778564453
Epoch 180, training loss: 0.8440485000610352 = 0.7715122103691101 + 0.01 * 7.253629207611084
Epoch 180, val loss: 0.9350019693374634
Epoch 190, training loss: 0.7809520363807678 = 0.7084627151489258 + 0.01 * 7.248930931091309
Epoch 190, val loss: 0.8875361084938049
Epoch 200, training loss: 0.7272713780403137 = 0.6548412442207336 + 0.01 * 7.243010997772217
Epoch 200, val loss: 0.8506829142570496
Epoch 210, training loss: 0.6787576079368591 = 0.6064208745956421 + 0.01 * 7.233672142028809
Epoch 210, val loss: 0.8206497430801392
Epoch 220, training loss: 0.6318801045417786 = 0.5596906542778015 + 0.01 * 7.218944549560547
Epoch 220, val loss: 0.7941300868988037
Epoch 230, training loss: 0.5847344398498535 = 0.5127456784248352 + 0.01 * 7.198876857757568
Epoch 230, val loss: 0.76918625831604
Epoch 240, training loss: 0.5372198820114136 = 0.46546250581741333 + 0.01 * 7.1757378578186035
Epoch 240, val loss: 0.7458741664886475
Epoch 250, training loss: 0.49057576060295105 = 0.4190739393234253 + 0.01 * 7.150182723999023
Epoch 250, val loss: 0.7252839207649231
Epoch 260, training loss: 0.4463276267051697 = 0.3750406503677368 + 0.01 * 7.128696441650391
Epoch 260, val loss: 0.7084175944328308
Epoch 270, training loss: 0.4052877426147461 = 0.3341868221759796 + 0.01 * 7.110093116760254
Epoch 270, val loss: 0.6958563327789307
Epoch 280, training loss: 0.36749547719955444 = 0.29648688435554504 + 0.01 * 7.100857734680176
Epoch 280, val loss: 0.6872823238372803
Epoch 290, training loss: 0.33246031403541565 = 0.2615390419960022 + 0.01 * 7.092127799987793
Epoch 290, val loss: 0.6820292472839355
Epoch 300, training loss: 0.2999271750450134 = 0.2291034460067749 + 0.01 * 7.082371711730957
Epoch 300, val loss: 0.6795027852058411
Epoch 310, training loss: 0.27008551359176636 = 0.19931761920452118 + 0.01 * 7.076789855957031
Epoch 310, val loss: 0.6794501543045044
Epoch 320, training loss: 0.2433328628540039 = 0.17260928452014923 + 0.01 * 7.072357177734375
Epoch 320, val loss: 0.6817731261253357
Epoch 330, training loss: 0.2200232297182083 = 0.14934928715229034 + 0.01 * 7.067394733428955
Epoch 330, val loss: 0.6864820718765259
Epoch 340, training loss: 0.20019495487213135 = 0.12956362962722778 + 0.01 * 7.063131809234619
Epoch 340, val loss: 0.693425714969635
Epoch 350, training loss: 0.18352004885673523 = 0.11293806880712509 + 0.01 * 7.05819845199585
Epoch 350, val loss: 0.7022525072097778
Epoch 360, training loss: 0.16953735053539276 = 0.0989990308880806 + 0.01 * 7.053832054138184
Epoch 360, val loss: 0.7125741839408875
Epoch 370, training loss: 0.15772375464439392 = 0.08724689483642578 + 0.01 * 7.04768705368042
Epoch 370, val loss: 0.7240009307861328
Epoch 380, training loss: 0.14769932627677917 = 0.07725244760513306 + 0.01 * 7.0446882247924805
Epoch 380, val loss: 0.7362682223320007
Epoch 390, training loss: 0.13907349109649658 = 0.06867645680904388 + 0.01 * 7.039704322814941
Epoch 390, val loss: 0.7491068243980408
Epoch 400, training loss: 0.13157059252262115 = 0.061266083270311356 + 0.01 * 7.030450820922852
Epoch 400, val loss: 0.7622835636138916
Epoch 410, training loss: 0.12512418627738953 = 0.05483070760965347 + 0.01 * 7.029347896575928
Epoch 410, val loss: 0.7756831049919128
Epoch 420, training loss: 0.1194005236029625 = 0.04922384023666382 + 0.01 * 7.017668724060059
Epoch 420, val loss: 0.7890575528144836
Epoch 430, training loss: 0.1144421249628067 = 0.04432489350438118 + 0.01 * 7.011723041534424
Epoch 430, val loss: 0.8023524880409241
Epoch 440, training loss: 0.11015172302722931 = 0.04003925994038582 + 0.01 * 7.011246681213379
Epoch 440, val loss: 0.8155305981636047
Epoch 450, training loss: 0.10624421387910843 = 0.03628362715244293 + 0.01 * 6.996058940887451
Epoch 450, val loss: 0.8284814357757568
Epoch 460, training loss: 0.10287722945213318 = 0.03298453614115715 + 0.01 * 6.989269256591797
Epoch 460, val loss: 0.8411969542503357
Epoch 470, training loss: 0.09992145746946335 = 0.03008096106350422 + 0.01 * 6.984050273895264
Epoch 470, val loss: 0.8536525964736938
Epoch 480, training loss: 0.09729993343353271 = 0.02752051129937172 + 0.01 * 6.977942943572998
Epoch 480, val loss: 0.86579829454422
Epoch 490, training loss: 0.09494543820619583 = 0.025252213701605797 + 0.01 * 6.969322681427002
Epoch 490, val loss: 0.8776621222496033
Epoch 500, training loss: 0.09291934967041016 = 0.02323424629867077 + 0.01 * 6.968510627746582
Epoch 500, val loss: 0.8892533183097839
Epoch 510, training loss: 0.09101198613643646 = 0.021433647722005844 + 0.01 * 6.957834720611572
Epoch 510, val loss: 0.900560736656189
Epoch 520, training loss: 0.08935829997062683 = 0.019820591434836388 + 0.01 * 6.953771114349365
Epoch 520, val loss: 0.9116151332855225
Epoch 530, training loss: 0.08785468339920044 = 0.018373986706137657 + 0.01 * 6.9480695724487305
Epoch 530, val loss: 0.9223870635032654
Epoch 540, training loss: 0.08668974041938782 = 0.01707274094223976 + 0.01 * 6.961699485778809
Epoch 540, val loss: 0.9329028725624084
Epoch 550, training loss: 0.08528170734643936 = 0.01590062864124775 + 0.01 * 6.938108444213867
Epoch 550, val loss: 0.9431668519973755
Epoch 560, training loss: 0.0841677114367485 = 0.014843237586319447 + 0.01 * 6.9324469566345215
Epoch 560, val loss: 0.9530856609344482
Epoch 570, training loss: 0.08312688022851944 = 0.013882446102797985 + 0.01 * 6.92444372177124
Epoch 570, val loss: 0.9628379344940186
Epoch 580, training loss: 0.08247492462396622 = 0.013010317459702492 + 0.01 * 6.946460723876953
Epoch 580, val loss: 0.9723479151725769
Epoch 590, training loss: 0.08136040717363358 = 0.012217625044286251 + 0.01 * 6.914278507232666
Epoch 590, val loss: 0.9815646409988403
Epoch 600, training loss: 0.08062057197093964 = 0.01149531826376915 + 0.01 * 6.912525177001953
Epoch 600, val loss: 0.9905011057853699
Epoch 610, training loss: 0.07989707589149475 = 0.01083403266966343 + 0.01 * 6.906304836273193
Epoch 610, val loss: 0.999308168888092
Epoch 620, training loss: 0.07923056185245514 = 0.010228011757135391 + 0.01 * 6.900254726409912
Epoch 620, val loss: 1.00786554813385
Epoch 630, training loss: 0.07868809998035431 = 0.009671514853835106 + 0.01 * 6.901658535003662
Epoch 630, val loss: 1.0162426233291626
Epoch 640, training loss: 0.07831011712551117 = 0.009160863235592842 + 0.01 * 6.914925575256348
Epoch 640, val loss: 1.024363398551941
Epoch 650, training loss: 0.07761194556951523 = 0.008690347895026207 + 0.01 * 6.892159461975098
Epoch 650, val loss: 1.0322576761245728
Epoch 660, training loss: 0.07711786031723022 = 0.008255301043391228 + 0.01 * 6.886255741119385
Epoch 660, val loss: 1.03996741771698
Epoch 670, training loss: 0.07664836943149567 = 0.00785309262573719 + 0.01 * 6.879528045654297
Epoch 670, val loss: 1.0474961996078491
Epoch 680, training loss: 0.07637622952461243 = 0.007479884661734104 + 0.01 * 6.88963508605957
Epoch 680, val loss: 1.0548889636993408
Epoch 690, training loss: 0.07591785490512848 = 0.007133985869586468 + 0.01 * 6.878387451171875
Epoch 690, val loss: 1.0620687007904053
Epoch 700, training loss: 0.07548951357603073 = 0.006812399718910456 + 0.01 * 6.867711544036865
Epoch 700, val loss: 1.0690743923187256
Epoch 710, training loss: 0.0753173902630806 = 0.006513094995170832 + 0.01 * 6.880429267883301
Epoch 710, val loss: 1.075957179069519
Epoch 720, training loss: 0.0749177560210228 = 0.00623470451682806 + 0.01 * 6.868305206298828
Epoch 720, val loss: 1.082568883895874
Epoch 730, training loss: 0.07455512136220932 = 0.005974888801574707 + 0.01 * 6.858023166656494
Epoch 730, val loss: 1.089019775390625
Epoch 740, training loss: 0.07424645125865936 = 0.005731683690100908 + 0.01 * 6.851476669311523
Epoch 740, val loss: 1.0953844785690308
Epoch 750, training loss: 0.0740291029214859 = 0.005503669381141663 + 0.01 * 6.852543354034424
Epoch 750, val loss: 1.1015965938568115
Epoch 760, training loss: 0.07373663038015366 = 0.00529023353010416 + 0.01 * 6.844639778137207
Epoch 760, val loss: 1.1076573133468628
Epoch 770, training loss: 0.07351750880479813 = 0.00509003596380353 + 0.01 * 6.842747211456299
Epoch 770, val loss: 1.1135231256484985
Epoch 780, training loss: 0.07348122447729111 = 0.004902023356407881 + 0.01 * 6.8579206466674805
Epoch 780, val loss: 1.1192834377288818
Epoch 790, training loss: 0.07313855737447739 = 0.004725765902549028 + 0.01 * 6.841279029846191
Epoch 790, val loss: 1.1248109340667725
Epoch 800, training loss: 0.07286352664232254 = 0.00456001702696085 + 0.01 * 6.83035135269165
Epoch 800, val loss: 1.130232810974121
Epoch 810, training loss: 0.07287374138832092 = 0.004403670784085989 + 0.01 * 6.847006797790527
Epoch 810, val loss: 1.1356000900268555
Epoch 820, training loss: 0.0726161077618599 = 0.004256254993379116 + 0.01 * 6.83598518371582
Epoch 820, val loss: 1.1407091617584229
Epoch 830, training loss: 0.0723358690738678 = 0.004117065574973822 + 0.01 * 6.821880340576172
Epoch 830, val loss: 1.1457792520523071
Epoch 840, training loss: 0.0724739208817482 = 0.003985212650150061 + 0.01 * 6.848871231079102
Epoch 840, val loss: 1.1507771015167236
Epoch 850, training loss: 0.07206230610609055 = 0.003860707627609372 + 0.01 * 6.820160388946533
Epoch 850, val loss: 1.155536413192749
Epoch 860, training loss: 0.07187425345182419 = 0.0037427605129778385 + 0.01 * 6.8131489753723145
Epoch 860, val loss: 1.1602685451507568
Epoch 870, training loss: 0.07176750898361206 = 0.0036309321876615286 + 0.01 * 6.813658237457275
Epoch 870, val loss: 1.1648885011672974
Epoch 880, training loss: 0.07168158888816833 = 0.0035248168278485537 + 0.01 * 6.815677642822266
Epoch 880, val loss: 1.169325590133667
Epoch 890, training loss: 0.0715106949210167 = 0.0034241112880408764 + 0.01 * 6.808658599853516
Epoch 890, val loss: 1.1737335920333862
Epoch 900, training loss: 0.07143767923116684 = 0.003328609047457576 + 0.01 * 6.810907363891602
Epoch 900, val loss: 1.1779476404190063
Epoch 910, training loss: 0.07122843712568283 = 0.0032378952018916607 + 0.01 * 6.7990546226501465
Epoch 910, val loss: 1.1821125745773315
Epoch 920, training loss: 0.07106831669807434 = 0.003151671029627323 + 0.01 * 6.7916646003723145
Epoch 920, val loss: 1.1862448453903198
Epoch 930, training loss: 0.07103316485881805 = 0.0030694850720465183 + 0.01 * 6.796368598937988
Epoch 930, val loss: 1.1900516748428345
Epoch 940, training loss: 0.07089849561452866 = 0.002991237910464406 + 0.01 * 6.7907257080078125
Epoch 940, val loss: 1.193853735923767
Epoch 950, training loss: 0.07094048708677292 = 0.0029166224412620068 + 0.01 * 6.80238676071167
Epoch 950, val loss: 1.1976754665374756
Epoch 960, training loss: 0.07071645557880402 = 0.00284560932777822 + 0.01 * 6.787084579467773
Epoch 960, val loss: 1.201248049736023
Epoch 970, training loss: 0.07054517418146133 = 0.002777770394459367 + 0.01 * 6.776740550994873
Epoch 970, val loss: 1.2048218250274658
Epoch 980, training loss: 0.0704510509967804 = 0.0027129994705319405 + 0.01 * 6.773805141448975
Epoch 980, val loss: 1.2083231210708618
Epoch 990, training loss: 0.07034360617399216 = 0.0026510199531912804 + 0.01 * 6.769258499145508
Epoch 990, val loss: 1.2116509675979614
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.032589912414551 = 1.9466214179992676 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9429969787597656
Epoch 10, training loss: 2.02272629737854 = 1.9367581605911255 + 0.01 * 8.596813201904297
Epoch 10, val loss: 1.9336819648742676
Epoch 20, training loss: 2.010310411453247 = 1.9243441820144653 + 0.01 * 8.596616744995117
Epoch 20, val loss: 1.9215519428253174
Epoch 30, training loss: 1.9924198389053345 = 1.906461238861084 + 0.01 * 8.595863342285156
Epoch 30, val loss: 1.9037595987319946
Epoch 40, training loss: 1.965360164642334 = 1.879448652267456 + 0.01 * 8.591147422790527
Epoch 40, val loss: 1.8771672248840332
Epoch 50, training loss: 1.9269760847091675 = 1.8413742780685425 + 0.01 * 8.560181617736816
Epoch 50, val loss: 1.8416061401367188
Epoch 60, training loss: 1.8832206726074219 = 1.7989232540130615 + 0.01 * 8.429736137390137
Epoch 60, val loss: 1.8061716556549072
Epoch 70, training loss: 1.8438389301300049 = 1.761793851852417 + 0.01 * 8.204506874084473
Epoch 70, val loss: 1.7763944864273071
Epoch 80, training loss: 1.7955807447433472 = 1.7145965099334717 + 0.01 * 8.098420143127441
Epoch 80, val loss: 1.7325478792190552
Epoch 90, training loss: 1.7274550199508667 = 1.6482878923416138 + 0.01 * 7.916708469390869
Epoch 90, val loss: 1.6721433401107788
Epoch 100, training loss: 1.6373504400253296 = 1.5603395700454712 + 0.01 * 7.701085090637207
Epoch 100, val loss: 1.5971791744232178
Epoch 110, training loss: 1.5363632440567017 = 1.4618501663208008 + 0.01 * 7.451313018798828
Epoch 110, val loss: 1.5158419609069824
Epoch 120, training loss: 1.4375296831130981 = 1.3636445999145508 + 0.01 * 7.388505935668945
Epoch 120, val loss: 1.4349700212478638
Epoch 130, training loss: 1.342235803604126 = 1.2688382863998413 + 0.01 * 7.339755058288574
Epoch 130, val loss: 1.3593565225601196
Epoch 140, training loss: 1.2497118711471558 = 1.176689863204956 + 0.01 * 7.3022003173828125
Epoch 140, val loss: 1.2873929738998413
Epoch 150, training loss: 1.1609495878219604 = 1.0882188081741333 + 0.01 * 7.2730793952941895
Epoch 150, val loss: 1.2199897766113281
Epoch 160, training loss: 1.0777233839035034 = 1.005185842514038 + 0.01 * 7.253758907318115
Epoch 160, val loss: 1.1585056781768799
Epoch 170, training loss: 1.0000033378601074 = 0.9275377988815308 + 0.01 * 7.246558666229248
Epoch 170, val loss: 1.1019644737243652
Epoch 180, training loss: 0.9263254404067993 = 0.8538969159126282 + 0.01 * 7.2428507804870605
Epoch 180, val loss: 1.0481899976730347
Epoch 190, training loss: 0.855929970741272 = 0.7835597991943359 + 0.01 * 7.2370147705078125
Epoch 190, val loss: 0.9962530136108398
Epoch 200, training loss: 0.7894072532653809 = 0.7171109914779663 + 0.01 * 7.2296247482299805
Epoch 200, val loss: 0.9471333622932434
Epoch 210, training loss: 0.7276690006256104 = 0.6554594039916992 + 0.01 * 7.220962047576904
Epoch 210, val loss: 0.9026691317558289
Epoch 220, training loss: 0.6705484390258789 = 0.5984280109405518 + 0.01 * 7.212042331695557
Epoch 220, val loss: 0.8638884425163269
Epoch 230, training loss: 0.6171907186508179 = 0.5451682209968567 + 0.01 * 7.202251434326172
Epoch 230, val loss: 0.8310893774032593
Epoch 240, training loss: 0.567172110080719 = 0.4952563941478729 + 0.01 * 7.191571235656738
Epoch 240, val loss: 0.8039371371269226
Epoch 250, training loss: 0.5206248760223389 = 0.44882693886756897 + 0.01 * 7.179795742034912
Epoch 250, val loss: 0.782111644744873
Epoch 260, training loss: 0.4778924882411957 = 0.40621218085289 + 0.01 * 7.168031215667725
Epoch 260, val loss: 0.7657537460327148
Epoch 270, training loss: 0.4390232264995575 = 0.36746034026145935 + 0.01 * 7.1562886238098145
Epoch 270, val loss: 0.7546778917312622
Epoch 280, training loss: 0.4036085605621338 = 0.3321555256843567 + 0.01 * 7.145305156707764
Epoch 280, val loss: 0.7477034330368042
Epoch 290, training loss: 0.37110501527786255 = 0.29978999495506287 + 0.01 * 7.1315016746521
Epoch 290, val loss: 0.7437446713447571
Epoch 300, training loss: 0.3411836624145508 = 0.2699597477912903 + 0.01 * 7.122391700744629
Epoch 300, val loss: 0.7421897053718567
Epoch 310, training loss: 0.3133884370326996 = 0.24239768087863922 + 0.01 * 7.099076747894287
Epoch 310, val loss: 0.7427843809127808
Epoch 320, training loss: 0.28787463903427124 = 0.21694700419902802 + 0.01 * 7.092764854431152
Epoch 320, val loss: 0.7450950741767883
Epoch 330, training loss: 0.2642608880996704 = 0.19360922276973724 + 0.01 * 7.0651679039001465
Epoch 330, val loss: 0.7492098808288574
Epoch 340, training loss: 0.24291211366653442 = 0.17239300906658173 + 0.01 * 7.051910400390625
Epoch 340, val loss: 0.7547533512115479
Epoch 350, training loss: 0.22355268895626068 = 0.15329407155513763 + 0.01 * 7.025862216949463
Epoch 350, val loss: 0.7616053223609924
Epoch 360, training loss: 0.20706428587436676 = 0.13624852895736694 + 0.01 * 7.081575870513916
Epoch 360, val loss: 0.7696434855461121
Epoch 370, training loss: 0.19122529029846191 = 0.12114366888999939 + 0.01 * 7.008162975311279
Epoch 370, val loss: 0.7783485054969788
Epoch 380, training loss: 0.177650585770607 = 0.10771460831165314 + 0.01 * 6.993597984313965
Epoch 380, val loss: 0.7877563238143921
Epoch 390, training loss: 0.16561897099018097 = 0.09576355665922165 + 0.01 * 6.985541820526123
Epoch 390, val loss: 0.7976775169372559
Epoch 400, training loss: 0.15504410862922668 = 0.08513671159744263 + 0.01 * 6.990740776062012
Epoch 400, val loss: 0.8080112338066101
Epoch 410, training loss: 0.1454181671142578 = 0.07572483271360397 + 0.01 * 6.969333171844482
Epoch 410, val loss: 0.8187208771705627
Epoch 420, training loss: 0.1370185911655426 = 0.06741820275783539 + 0.01 * 6.960038185119629
Epoch 420, val loss: 0.8295642137527466
Epoch 430, training loss: 0.129889577627182 = 0.06011710315942764 + 0.01 * 6.977247714996338
Epoch 430, val loss: 0.8405531644821167
Epoch 440, training loss: 0.1233518123626709 = 0.053727664053440094 + 0.01 * 6.9624152183532715
Epoch 440, val loss: 0.8517080545425415
Epoch 450, training loss: 0.11747290194034576 = 0.04811858385801315 + 0.01 * 6.935431957244873
Epoch 450, val loss: 0.8629022836685181
Epoch 460, training loss: 0.11266689747571945 = 0.043232597410678864 + 0.01 * 6.943430423736572
Epoch 460, val loss: 0.8743870258331299
Epoch 470, training loss: 0.10828317701816559 = 0.03896713629364967 + 0.01 * 6.931603908538818
Epoch 470, val loss: 0.8858585357666016
Epoch 480, training loss: 0.10445792973041534 = 0.03524046018719673 + 0.01 * 6.921747207641602
Epoch 480, val loss: 0.897360622882843
Epoch 490, training loss: 0.10106100142002106 = 0.03197789564728737 + 0.01 * 6.908310413360596
Epoch 490, val loss: 0.9088401794433594
Epoch 500, training loss: 0.09827616810798645 = 0.02911391109228134 + 0.01 * 6.916225910186768
Epoch 500, val loss: 0.9202409386634827
Epoch 510, training loss: 0.09557974338531494 = 0.026600543409585953 + 0.01 * 6.897919654846191
Epoch 510, val loss: 0.9315759539604187
Epoch 520, training loss: 0.09341450035572052 = 0.024384036660194397 + 0.01 * 6.903046607971191
Epoch 520, val loss: 0.9426894187927246
Epoch 530, training loss: 0.09135172516107559 = 0.022422581911087036 + 0.01 * 6.892914772033691
Epoch 530, val loss: 0.9536401629447937
Epoch 540, training loss: 0.0895530954003334 = 0.020682642236351967 + 0.01 * 6.887045383453369
Epoch 540, val loss: 0.9642977118492126
Epoch 550, training loss: 0.08794188499450684 = 0.019133435562253 + 0.01 * 6.880845546722412
Epoch 550, val loss: 0.9747679829597473
Epoch 560, training loss: 0.08665573596954346 = 0.017749596387147903 + 0.01 * 6.890614032745361
Epoch 560, val loss: 0.9849388003349304
Epoch 570, training loss: 0.08524765074253082 = 0.01651146076619625 + 0.01 * 6.8736186027526855
Epoch 570, val loss: 0.9948580265045166
Epoch 580, training loss: 0.08404722064733505 = 0.015398882329463959 + 0.01 * 6.864833831787109
Epoch 580, val loss: 1.0044684410095215
Epoch 590, training loss: 0.08298458158969879 = 0.014395964331924915 + 0.01 * 6.858861446380615
Epoch 590, val loss: 1.0138473510742188
Epoch 600, training loss: 0.08201275765895844 = 0.013489591889083385 + 0.01 * 6.8523173332214355
Epoch 600, val loss: 1.0229157209396362
Epoch 610, training loss: 0.08118893951177597 = 0.012668153271079063 + 0.01 * 6.852078914642334
Epoch 610, val loss: 1.031772255897522
Epoch 620, training loss: 0.08050413429737091 = 0.011921484023332596 + 0.01 * 6.858265399932861
Epoch 620, val loss: 1.0403393507003784
Epoch 630, training loss: 0.07957585901021957 = 0.011241918429732323 + 0.01 * 6.8333940505981445
Epoch 630, val loss: 1.0486948490142822
Epoch 640, training loss: 0.07911248505115509 = 0.01062086597084999 + 0.01 * 6.8491621017456055
Epoch 640, val loss: 1.0567829608917236
Epoch 650, training loss: 0.07831641286611557 = 0.01005269680172205 + 0.01 * 6.826372146606445
Epoch 650, val loss: 1.0646330118179321
Epoch 660, training loss: 0.0778631716966629 = 0.009531030431389809 + 0.01 * 6.83321475982666
Epoch 660, val loss: 1.0722826719284058
Epoch 670, training loss: 0.07740907371044159 = 0.009051484055817127 + 0.01 * 6.835759162902832
Epoch 670, val loss: 1.0797157287597656
Epoch 680, training loss: 0.0768001452088356 = 0.008609727956354618 + 0.01 * 6.819041728973389
Epoch 680, val loss: 1.0869460105895996
Epoch 690, training loss: 0.0762740969657898 = 0.008201507851481438 + 0.01 * 6.807258605957031
Epoch 690, val loss: 1.093919038772583
Epoch 700, training loss: 0.07588296383619308 = 0.007823152467608452 + 0.01 * 6.805981159210205
Epoch 700, val loss: 1.1007821559906006
Epoch 710, training loss: 0.07555483281612396 = 0.007472618017345667 + 0.01 * 6.808221340179443
Epoch 710, val loss: 1.1074025630950928
Epoch 720, training loss: 0.07521715760231018 = 0.007147564087063074 + 0.01 * 6.80695915222168
Epoch 720, val loss: 1.1138687133789062
Epoch 730, training loss: 0.07494546473026276 = 0.006845496129244566 + 0.01 * 6.809997081756592
Epoch 730, val loss: 1.1201283931732178
Epoch 740, training loss: 0.07448846101760864 = 0.006563600618392229 + 0.01 * 6.792486190795898
Epoch 740, val loss: 1.1262576580047607
Epoch 750, training loss: 0.07413042336702347 = 0.006301104091107845 + 0.01 * 6.782931804656982
Epoch 750, val loss: 1.1322414875030518
Epoch 760, training loss: 0.07400471717119217 = 0.006055660080164671 + 0.01 * 6.794905662536621
Epoch 760, val loss: 1.1380404233932495
Epoch 770, training loss: 0.0736478790640831 = 0.005825441796332598 + 0.01 * 6.782243251800537
Epoch 770, val loss: 1.1437164545059204
Epoch 780, training loss: 0.07335852831602097 = 0.005609513260424137 + 0.01 * 6.774901390075684
Epoch 780, val loss: 1.1492890119552612
Epoch 790, training loss: 0.07315043359994888 = 0.005407061893492937 + 0.01 * 6.774337291717529
Epoch 790, val loss: 1.1546552181243896
Epoch 800, training loss: 0.07313381135463715 = 0.005216372199356556 + 0.01 * 6.791743755340576
Epoch 800, val loss: 1.1599432229995728
Epoch 810, training loss: 0.07291939854621887 = 0.005037133581936359 + 0.01 * 6.78822660446167
Epoch 810, val loss: 1.1651170253753662
Epoch 820, training loss: 0.07256682217121124 = 0.004868725780397654 + 0.01 * 6.769809722900391
Epoch 820, val loss: 1.1700785160064697
Epoch 830, training loss: 0.07230540364980698 = 0.004709686618298292 + 0.01 * 6.7595720291137695
Epoch 830, val loss: 1.1749322414398193
Epoch 840, training loss: 0.07215377688407898 = 0.0045587485656142235 + 0.01 * 6.75950288772583
Epoch 840, val loss: 1.1797579526901245
Epoch 850, training loss: 0.07205436378717422 = 0.004416593350470066 + 0.01 * 6.763777256011963
Epoch 850, val loss: 1.1843880414962769
Epoch 860, training loss: 0.0719069391489029 = 0.004281847272068262 + 0.01 * 6.762508869171143
Epoch 860, val loss: 1.1889355182647705
Epoch 870, training loss: 0.07176685333251953 = 0.004154121968895197 + 0.01 * 6.76127290725708
Epoch 870, val loss: 1.193419337272644
Epoch 880, training loss: 0.07148696482181549 = 0.004033090081065893 + 0.01 * 6.745387554168701
Epoch 880, val loss: 1.1977680921554565
Epoch 890, training loss: 0.07136120647192001 = 0.003918524365872145 + 0.01 * 6.744268894195557
Epoch 890, val loss: 1.2019659280776978
Epoch 900, training loss: 0.07130604237318039 = 0.0038092774339020252 + 0.01 * 6.74967622756958
Epoch 900, val loss: 1.2061233520507812
Epoch 910, training loss: 0.07113271951675415 = 0.003705306677147746 + 0.01 * 6.742741107940674
Epoch 910, val loss: 1.2102361917495728
Epoch 920, training loss: 0.07107013463973999 = 0.003606724552810192 + 0.01 * 6.746341705322266
Epoch 920, val loss: 1.2141380310058594
Epoch 930, training loss: 0.07093750685453415 = 0.0035123666748404503 + 0.01 * 6.742514133453369
Epoch 930, val loss: 1.2180451154708862
Epoch 940, training loss: 0.0708886906504631 = 0.003422531532123685 + 0.01 * 6.746616363525391
Epoch 940, val loss: 1.2218356132507324
Epoch 950, training loss: 0.07066041231155396 = 0.003336617024615407 + 0.01 * 6.732379913330078
Epoch 950, val loss: 1.2255736589431763
Epoch 960, training loss: 0.07057197391986847 = 0.0032548343297094107 + 0.01 * 6.731714248657227
Epoch 960, val loss: 1.2291887998580933
Epoch 970, training loss: 0.0707140639424324 = 0.003176432801410556 + 0.01 * 6.753763198852539
Epoch 970, val loss: 1.2327485084533691
Epoch 980, training loss: 0.07043800503015518 = 0.0031015658751130104 + 0.01 * 6.733644008636475
Epoch 980, val loss: 1.2362608909606934
Epoch 990, training loss: 0.07024512439966202 = 0.0030304226092994213 + 0.01 * 6.721470355987549
Epoch 990, val loss: 1.2395800352096558
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.838165524512388
The final CL Acc:0.80988, 0.00630, The final GNN Acc:0.83940, 0.00090
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9518])
updated graph: torch.Size([2, 10552])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.041609764099121 = 1.955641508102417 + 0.01 * 8.59681510925293
Epoch 0, val loss: 1.952989101409912
Epoch 10, training loss: 2.0316810607910156 = 1.9457134008407593 + 0.01 * 8.596759796142578
Epoch 10, val loss: 1.9437041282653809
Epoch 20, training loss: 2.0195183753967285 = 1.9335530996322632 + 0.01 * 8.596529960632324
Epoch 20, val loss: 1.9320124387741089
Epoch 30, training loss: 2.0022218227386475 = 1.9162647724151611 + 0.01 * 8.595704078674316
Epoch 30, val loss: 1.9151008129119873
Epoch 40, training loss: 1.9759812355041504 = 1.8900786638259888 + 0.01 * 8.590263366699219
Epoch 40, val loss: 1.8896901607513428
Epoch 50, training loss: 1.938091516494751 = 1.8525773286819458 + 0.01 * 8.551421165466309
Epoch 50, val loss: 1.855265736579895
Epoch 60, training loss: 1.8946057558059692 = 1.8108526468276978 + 0.01 * 8.375306129455566
Epoch 60, val loss: 1.8214797973632812
Epoch 70, training loss: 1.8611358404159546 = 1.779396891593933 + 0.01 * 8.173893928527832
Epoch 70, val loss: 1.7976266145706177
Epoch 80, training loss: 1.8242909908294678 = 1.7439351081848145 + 0.01 * 8.03558349609375
Epoch 80, val loss: 1.764121651649475
Epoch 90, training loss: 1.7729783058166504 = 1.6946884393692017 + 0.01 * 7.828988075256348
Epoch 90, val loss: 1.7209033966064453
Epoch 100, training loss: 1.7035285234451294 = 1.62678861618042 + 0.01 * 7.673994541168213
Epoch 100, val loss: 1.6663190126419067
Epoch 110, training loss: 1.6173243522644043 = 1.5418968200683594 + 0.01 * 7.542759418487549
Epoch 110, val loss: 1.5987964868545532
Epoch 120, training loss: 1.5248390436172485 = 1.4501227140426636 + 0.01 * 7.4716339111328125
Epoch 120, val loss: 1.5253851413726807
Epoch 130, training loss: 1.4337908029556274 = 1.359902262687683 + 0.01 * 7.388852119445801
Epoch 130, val loss: 1.4545297622680664
Epoch 140, training loss: 1.346551537513733 = 1.2733385562896729 + 0.01 * 7.3213019371032715
Epoch 140, val loss: 1.3879026174545288
Epoch 150, training loss: 1.2621089220046997 = 1.1895561218261719 + 0.01 * 7.255282878875732
Epoch 150, val loss: 1.3245033025741577
Epoch 160, training loss: 1.1819883584976196 = 1.1100194454193115 + 0.01 * 7.196894645690918
Epoch 160, val loss: 1.2663781642913818
Epoch 170, training loss: 1.1083829402923584 = 1.0367330312728882 + 0.01 * 7.164985656738281
Epoch 170, val loss: 1.214816927909851
Epoch 180, training loss: 1.0415904521942139 = 0.9700969457626343 + 0.01 * 7.149349212646484
Epoch 180, val loss: 1.169355869293213
Epoch 190, training loss: 0.9796033501625061 = 0.9082260727882385 + 0.01 * 7.137728214263916
Epoch 190, val loss: 1.1277107000350952
Epoch 200, training loss: 0.9196249842643738 = 0.8483760356903076 + 0.01 * 7.124894618988037
Epoch 200, val loss: 1.0874273777008057
Epoch 210, training loss: 0.8598165512084961 = 0.788628339767456 + 0.01 * 7.118818283081055
Epoch 210, val loss: 1.0467606782913208
Epoch 220, training loss: 0.7996023893356323 = 0.7285319566726685 + 0.01 * 7.107040882110596
Epoch 220, val loss: 1.0064935684204102
Epoch 230, training loss: 0.7400048971176147 = 0.6689891815185547 + 0.01 * 7.1015729904174805
Epoch 230, val loss: 0.96834796667099
Epoch 240, training loss: 0.6820957064628601 = 0.6111454963684082 + 0.01 * 7.095019340515137
Epoch 240, val loss: 0.9343538284301758
Epoch 250, training loss: 0.6270310878753662 = 0.5561050772666931 + 0.01 * 7.092601776123047
Epoch 250, val loss: 0.9060448408126831
Epoch 260, training loss: 0.5753437280654907 = 0.504517674446106 + 0.01 * 7.082605361938477
Epoch 260, val loss: 0.8841951489448547
Epoch 270, training loss: 0.5270117521286011 = 0.45624226331710815 + 0.01 * 7.076948642730713
Epoch 270, val loss: 0.86841881275177
Epoch 280, training loss: 0.4816819429397583 = 0.4109381437301636 + 0.01 * 7.074380397796631
Epoch 280, val loss: 0.8582465648651123
Epoch 290, training loss: 0.4392748475074768 = 0.3686302900314331 + 0.01 * 7.064454555511475
Epoch 290, val loss: 0.8532989025115967
Epoch 300, training loss: 0.40013033151626587 = 0.3295467495918274 + 0.01 * 7.0583600997924805
Epoch 300, val loss: 0.8528914451599121
Epoch 310, training loss: 0.36420586705207825 = 0.29365357756614685 + 0.01 * 7.055228233337402
Epoch 310, val loss: 0.8558894991874695
Epoch 320, training loss: 0.3309704065322876 = 0.2604939043521881 + 0.01 * 7.047649383544922
Epoch 320, val loss: 0.8609706163406372
Epoch 330, training loss: 0.3000740110874176 = 0.22965139150619507 + 0.01 * 7.042263031005859
Epoch 330, val loss: 0.8674647808074951
Epoch 340, training loss: 0.2714954912662506 = 0.20112468302249908 + 0.01 * 7.03708028793335
Epoch 340, val loss: 0.8751280903816223
Epoch 350, training loss: 0.24556094408035278 = 0.17522180080413818 + 0.01 * 7.0339155197143555
Epoch 350, val loss: 0.8842204809188843
Epoch 360, training loss: 0.22258244454860687 = 0.15222394466400146 + 0.01 * 7.0358500480651855
Epoch 360, val loss: 0.8948908448219299
Epoch 370, training loss: 0.20242507755756378 = 0.13216182589530945 + 0.01 * 7.026325225830078
Epoch 370, val loss: 0.9071370363235474
Epoch 380, training loss: 0.18508280813694 = 0.1148412823677063 + 0.01 * 7.024152755737305
Epoch 380, val loss: 0.9207683205604553
Epoch 390, training loss: 0.17014580965042114 = 0.09998452663421631 + 0.01 * 7.016129493713379
Epoch 390, val loss: 0.9353269338607788
Epoch 400, training loss: 0.15751086175441742 = 0.08728183060884476 + 0.01 * 7.022902965545654
Epoch 400, val loss: 0.9504532217979431
Epoch 410, training loss: 0.14657285809516907 = 0.07643838226795197 + 0.01 * 7.013448238372803
Epoch 410, val loss: 0.9658621549606323
Epoch 420, training loss: 0.137218177318573 = 0.0671650767326355 + 0.01 * 7.005309581756592
Epoch 420, val loss: 0.9812929630279541
Epoch 430, training loss: 0.12923406064510345 = 0.05921537056565285 + 0.01 * 7.001869201660156
Epoch 430, val loss: 0.9965860843658447
Epoch 440, training loss: 0.12252640724182129 = 0.052388329058885574 + 0.01 * 7.01380729675293
Epoch 440, val loss: 1.0116400718688965
Epoch 450, training loss: 0.1165161058306694 = 0.046515628695487976 + 0.01 * 7.00004768371582
Epoch 450, val loss: 1.0263296365737915
Epoch 460, training loss: 0.11138173192739487 = 0.041450195014476776 + 0.01 * 6.993154048919678
Epoch 460, val loss: 1.0406773090362549
Epoch 470, training loss: 0.1069437563419342 = 0.037078093737363815 + 0.01 * 6.986566066741943
Epoch 470, val loss: 1.0546865463256836
Epoch 480, training loss: 0.10311847180128098 = 0.03330141305923462 + 0.01 * 6.981706142425537
Epoch 480, val loss: 1.0683143138885498
Epoch 490, training loss: 0.09981147199869156 = 0.030034556984901428 + 0.01 * 6.977691650390625
Epoch 490, val loss: 1.0815556049346924
Epoch 500, training loss: 0.09694983065128326 = 0.027204789221286774 + 0.01 * 6.974503993988037
Epoch 500, val loss: 1.0943667888641357
Epoch 510, training loss: 0.09455631673336029 = 0.02475171536207199 + 0.01 * 6.980460166931152
Epoch 510, val loss: 1.1067545413970947
Epoch 520, training loss: 0.09229712188243866 = 0.022613469511270523 + 0.01 * 6.968364715576172
Epoch 520, val loss: 1.118723750114441
Epoch 530, training loss: 0.09036602079868317 = 0.02074017934501171 + 0.01 * 6.962584018707275
Epoch 530, val loss: 1.1302855014801025
Epoch 540, training loss: 0.08875574916601181 = 0.019092479720711708 + 0.01 * 6.96632719039917
Epoch 540, val loss: 1.1414344310760498
Epoch 550, training loss: 0.08715683221817017 = 0.01763835735619068 + 0.01 * 6.951848030090332
Epoch 550, val loss: 1.1521775722503662
Epoch 560, training loss: 0.08583369851112366 = 0.016349349170923233 + 0.01 * 6.948435306549072
Epoch 560, val loss: 1.1625218391418457
Epoch 570, training loss: 0.08481492102146149 = 0.015201590023934841 + 0.01 * 6.96133279800415
Epoch 570, val loss: 1.1725045442581177
Epoch 580, training loss: 0.0835965946316719 = 0.014177500270307064 + 0.01 * 6.941909313201904
Epoch 580, val loss: 1.182122826576233
Epoch 590, training loss: 0.08262486010789871 = 0.013259327039122581 + 0.01 * 6.936553478240967
Epoch 590, val loss: 1.1913989782333374
Epoch 600, training loss: 0.0818600282073021 = 0.012432321906089783 + 0.01 * 6.942770481109619
Epoch 600, val loss: 1.200355887413025
Epoch 610, training loss: 0.08100149035453796 = 0.011684763245284557 + 0.01 * 6.931673049926758
Epoch 610, val loss: 1.2090283632278442
Epoch 620, training loss: 0.08044856786727905 = 0.011008302681148052 + 0.01 * 6.944026947021484
Epoch 620, val loss: 1.217340111732483
Epoch 630, training loss: 0.07958084344863892 = 0.010393472388386726 + 0.01 * 6.918737411499023
Epoch 630, val loss: 1.225398063659668
Epoch 640, training loss: 0.07910766452550888 = 0.009833551943302155 + 0.01 * 6.927411079406738
Epoch 640, val loss: 1.2331371307373047
Epoch 650, training loss: 0.07848858833312988 = 0.009321380406618118 + 0.01 * 6.916721343994141
Epoch 650, val loss: 1.2406177520751953
Epoch 660, training loss: 0.07789571583271027 = 0.008852656930685043 + 0.01 * 6.904305934906006
Epoch 660, val loss: 1.2478642463684082
Epoch 670, training loss: 0.07768145203590393 = 0.00842113234102726 + 0.01 * 6.926032066345215
Epoch 670, val loss: 1.254846453666687
Epoch 680, training loss: 0.07700742036104202 = 0.008023935370147228 + 0.01 * 6.898348808288574
Epoch 680, val loss: 1.2616163492202759
Epoch 690, training loss: 0.0766645222902298 = 0.00765720009803772 + 0.01 * 6.900732517242432
Epoch 690, val loss: 1.2681242227554321
Epoch 700, training loss: 0.0763753205537796 = 0.007317760959267616 + 0.01 * 6.905755996704102
Epoch 700, val loss: 1.2744557857513428
Epoch 710, training loss: 0.07583969831466675 = 0.007003311533480883 + 0.01 * 6.883638858795166
Epoch 710, val loss: 1.2805862426757812
Epoch 720, training loss: 0.07550446689128876 = 0.0067115179263055325 + 0.01 * 6.8792948722839355
Epoch 720, val loss: 1.2864863872528076
Epoch 730, training loss: 0.07519535720348358 = 0.006439358927309513 + 0.01 * 6.8755998611450195
Epoch 730, val loss: 1.2922335863113403
Epoch 740, training loss: 0.07493321597576141 = 0.006186061538755894 + 0.01 * 6.874715805053711
Epoch 740, val loss: 1.2977843284606934
Epoch 750, training loss: 0.07498607039451599 = 0.005948891397565603 + 0.01 * 6.903717517852783
Epoch 750, val loss: 1.3031678199768066
Epoch 760, training loss: 0.07435929030179977 = 0.005727936048060656 + 0.01 * 6.86313533782959
Epoch 760, val loss: 1.3083971738815308
Epoch 770, training loss: 0.07421721518039703 = 0.005520837381482124 + 0.01 * 6.869637966156006
Epoch 770, val loss: 1.3134187459945679
Epoch 780, training loss: 0.07386879622936249 = 0.0053264605812728405 + 0.01 * 6.854233741760254
Epoch 780, val loss: 1.3182910680770874
Epoch 790, training loss: 0.07364122569561005 = 0.005144067108631134 + 0.01 * 6.8497161865234375
Epoch 790, val loss: 1.3230290412902832
Epoch 800, training loss: 0.07340383529663086 = 0.004972046706825495 + 0.01 * 6.843179225921631
Epoch 800, val loss: 1.327662467956543
Epoch 810, training loss: 0.07318301498889923 = 0.004810258746147156 + 0.01 * 6.837275981903076
Epoch 810, val loss: 1.3320823907852173
Epoch 820, training loss: 0.07306905835866928 = 0.00465757492929697 + 0.01 * 6.841148853302002
Epoch 820, val loss: 1.336419701576233
Epoch 830, training loss: 0.07280223816633224 = 0.004513659980148077 + 0.01 * 6.828858375549316
Epoch 830, val loss: 1.3405874967575073
Epoch 840, training loss: 0.07267554104328156 = 0.0043770805932581425 + 0.01 * 6.829845905303955
Epoch 840, val loss: 1.344681978225708
Epoch 850, training loss: 0.07288816571235657 = 0.004248274024575949 + 0.01 * 6.863989353179932
Epoch 850, val loss: 1.3486469984054565
Epoch 860, training loss: 0.07240843027830124 = 0.004126140382140875 + 0.01 * 6.828228950500488
Epoch 860, val loss: 1.352471113204956
Epoch 870, training loss: 0.07242186367511749 = 0.004010111093521118 + 0.01 * 6.841175556182861
Epoch 870, val loss: 1.356162667274475
Epoch 880, training loss: 0.0719921886920929 = 0.003900412004441023 + 0.01 * 6.809177875518799
Epoch 880, val loss: 1.359812617301941
Epoch 890, training loss: 0.07219460606575012 = 0.0037956542801111937 + 0.01 * 6.839895725250244
Epoch 890, val loss: 1.3633439540863037
Epoch 900, training loss: 0.07178382575511932 = 0.0036962851881980896 + 0.01 * 6.8087544441223145
Epoch 900, val loss: 1.3667546510696411
Epoch 910, training loss: 0.07180862128734589 = 0.003601616946980357 + 0.01 * 6.820700168609619
Epoch 910, val loss: 1.3701108694076538
Epoch 920, training loss: 0.07147609442472458 = 0.0035113217309117317 + 0.01 * 6.7964768409729
Epoch 920, val loss: 1.3733683824539185
Epoch 930, training loss: 0.07147979736328125 = 0.003425395581871271 + 0.01 * 6.805440425872803
Epoch 930, val loss: 1.376499056816101
Epoch 940, training loss: 0.07124447077512741 = 0.0033432648051530123 + 0.01 * 6.790121078491211
Epoch 940, val loss: 1.37959885597229
Epoch 950, training loss: 0.0711180791258812 = 0.003265159437432885 + 0.01 * 6.785292625427246
Epoch 950, val loss: 1.3825734853744507
Epoch 960, training loss: 0.07096108794212341 = 0.003189894137904048 + 0.01 * 6.777120113372803
Epoch 960, val loss: 1.3855100870132446
Epoch 970, training loss: 0.07127629965543747 = 0.003118110354989767 + 0.01 * 6.815818786621094
Epoch 970, val loss: 1.3883740901947021
Epoch 980, training loss: 0.07086816430091858 = 0.003049231832846999 + 0.01 * 6.781893253326416
Epoch 980, val loss: 1.391166090965271
Epoch 990, training loss: 0.07087526470422745 = 0.002983638783916831 + 0.01 * 6.789163112640381
Epoch 990, val loss: 1.3938831090927124
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8186610437532947
=== training gcn model ===
Epoch 0, training loss: 2.0139689445495605 = 1.9280004501342773 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.92787766456604
Epoch 10, training loss: 2.004911422729492 = 1.9189435243606567 + 0.01 * 8.596784591674805
Epoch 10, val loss: 1.91831374168396
Epoch 20, training loss: 1.993814468383789 = 1.907848834991455 + 0.01 * 8.596565246582031
Epoch 20, val loss: 1.9064480066299438
Epoch 30, training loss: 1.9785244464874268 = 1.8925665616989136 + 0.01 * 8.595789909362793
Epoch 30, val loss: 1.8902373313903809
Epoch 40, training loss: 1.956327199935913 = 1.8704118728637695 + 0.01 * 8.591538429260254
Epoch 40, val loss: 1.867280125617981
Epoch 50, training loss: 1.925550937652588 = 1.8399155139923096 + 0.01 * 8.563538551330566
Epoch 50, val loss: 1.8372373580932617
Epoch 60, training loss: 1.890466570854187 = 1.8059723377227783 + 0.01 * 8.449420928955078
Epoch 60, val loss: 1.8075603246688843
Epoch 70, training loss: 1.8570035696029663 = 1.7754414081573486 + 0.01 * 8.156218528747559
Epoch 70, val loss: 1.7834241390228271
Epoch 80, training loss: 1.8168175220489502 = 1.7366013526916504 + 0.01 * 8.021622657775879
Epoch 80, val loss: 1.7495436668395996
Epoch 90, training loss: 1.7601534128189087 = 1.6815595626831055 + 0.01 * 7.859383583068848
Epoch 90, val loss: 1.7006388902664185
Epoch 100, training loss: 1.6845380067825317 = 1.6074342727661133 + 0.01 * 7.7103705406188965
Epoch 100, val loss: 1.6370543241500854
Epoch 110, training loss: 1.5945897102355957 = 1.5193846225738525 + 0.01 * 7.520509243011475
Epoch 110, val loss: 1.5638240575790405
Epoch 120, training loss: 1.4994210004806519 = 1.4253026247024536 + 0.01 * 7.411833763122559
Epoch 120, val loss: 1.4866926670074463
Epoch 130, training loss: 1.402948021888733 = 1.3290634155273438 + 0.01 * 7.388463973999023
Epoch 130, val loss: 1.4090962409973145
Epoch 140, training loss: 1.3051209449768066 = 1.231506109237671 + 0.01 * 7.361486434936523
Epoch 140, val loss: 1.3332074880599976
Epoch 150, training loss: 1.2074191570281982 = 1.1340374946594238 + 0.01 * 7.338169097900391
Epoch 150, val loss: 1.2581034898757935
Epoch 160, training loss: 1.1114872694015503 = 1.0382789373397827 + 0.01 * 7.320830345153809
Epoch 160, val loss: 1.1843767166137695
Epoch 170, training loss: 1.0188453197479248 = 0.9457482695579529 + 0.01 * 7.309700012207031
Epoch 170, val loss: 1.1133359670639038
Epoch 180, training loss: 0.9303714632987976 = 0.8573456406593323 + 0.01 * 7.302584171295166
Epoch 180, val loss: 1.0461589097976685
Epoch 190, training loss: 0.8468136191368103 = 0.7738522887229919 + 0.01 * 7.2961320877075195
Epoch 190, val loss: 0.9840103983879089
Epoch 200, training loss: 0.7697526216506958 = 0.6968513131141663 + 0.01 * 7.290130615234375
Epoch 200, val loss: 0.9293861389160156
Epoch 210, training loss: 0.7003993391990662 = 0.6275507211685181 + 0.01 * 7.284863471984863
Epoch 210, val loss: 0.8839768767356873
Epoch 220, training loss: 0.638887345790863 = 0.5660942196846008 + 0.01 * 7.279313087463379
Epoch 220, val loss: 0.8476438522338867
Epoch 230, training loss: 0.5841150283813477 = 0.511390209197998 + 0.01 * 7.272482395172119
Epoch 230, val loss: 0.8189945220947266
Epoch 240, training loss: 0.5340287685394287 = 0.46139630675315857 + 0.01 * 7.263247013092041
Epoch 240, val loss: 0.7955654263496399
Epoch 250, training loss: 0.4864630103111267 = 0.4139568507671356 + 0.01 * 7.250617027282715
Epoch 250, val loss: 0.7754468321800232
Epoch 260, training loss: 0.43995243310928345 = 0.3676178455352783 + 0.01 * 7.233457565307617
Epoch 260, val loss: 0.7577717900276184
Epoch 270, training loss: 0.39435309171676636 = 0.32220447063446045 + 0.01 * 7.214860916137695
Epoch 270, val loss: 0.7424407601356506
Epoch 280, training loss: 0.35077622532844543 = 0.27877259254455566 + 0.01 * 7.200364112854004
Epoch 280, val loss: 0.7299515008926392
Epoch 290, training loss: 0.31074538826942444 = 0.23892448842525482 + 0.01 * 7.182091236114502
Epoch 290, val loss: 0.7210773825645447
Epoch 300, training loss: 0.27558645606040955 = 0.20383861660957336 + 0.01 * 7.174784183502197
Epoch 300, val loss: 0.7163205742835999
Epoch 310, training loss: 0.24550947546958923 = 0.1738877147436142 + 0.01 * 7.162177085876465
Epoch 310, val loss: 0.7156355381011963
Epoch 320, training loss: 0.22031450271606445 = 0.14874786138534546 + 0.01 * 7.156665325164795
Epoch 320, val loss: 0.7186610102653503
Epoch 330, training loss: 0.19928078353405 = 0.12776006758213043 + 0.01 * 7.152071475982666
Epoch 330, val loss: 0.7248547673225403
Epoch 340, training loss: 0.1816886067390442 = 0.11021671444177628 + 0.01 * 7.147188663482666
Epoch 340, val loss: 0.7335031628608704
Epoch 350, training loss: 0.16693013906478882 = 0.09550726413726807 + 0.01 * 7.1422882080078125
Epoch 350, val loss: 0.7440388202667236
Epoch 360, training loss: 0.15452179312705994 = 0.0831361711025238 + 0.01 * 7.138562202453613
Epoch 360, val loss: 0.7558578848838806
Epoch 370, training loss: 0.1440754532814026 = 0.07270423322916031 + 0.01 * 7.137121200561523
Epoch 370, val loss: 0.7686048746109009
Epoch 380, training loss: 0.13521620631217957 = 0.06388555467128754 + 0.01 * 7.133064270019531
Epoch 380, val loss: 0.781927227973938
Epoch 390, training loss: 0.12768371403217316 = 0.05641071870923042 + 0.01 * 7.127299785614014
Epoch 390, val loss: 0.7955572605133057
Epoch 400, training loss: 0.12130130082368851 = 0.050052084028720856 + 0.01 * 7.124921798706055
Epoch 400, val loss: 0.8093165755271912
Epoch 410, training loss: 0.11585548520088196 = 0.044618818908929825 + 0.01 * 7.123666286468506
Epoch 410, val loss: 0.823072612285614
Epoch 420, training loss: 0.11113260686397552 = 0.03995266556739807 + 0.01 * 7.11799430847168
Epoch 420, val loss: 0.8367205858230591
Epoch 430, training loss: 0.10705209523439407 = 0.03591802716255188 + 0.01 * 7.113406658172607
Epoch 430, val loss: 0.8502094745635986
Epoch 440, training loss: 0.10357311367988586 = 0.032408177852630615 + 0.01 * 7.1164937019348145
Epoch 440, val loss: 0.8634740710258484
Epoch 450, training loss: 0.10044554620981216 = 0.029346169903874397 + 0.01 * 7.109938144683838
Epoch 450, val loss: 0.8764824867248535
Epoch 460, training loss: 0.0977063700556755 = 0.026664992794394493 + 0.01 * 7.104137897491455
Epoch 460, val loss: 0.8891921043395996
Epoch 470, training loss: 0.09532264620065689 = 0.024310188367962837 + 0.01 * 7.101246356964111
Epoch 470, val loss: 0.9015992879867554
Epoch 480, training loss: 0.09321831166744232 = 0.022236434742808342 + 0.01 * 7.098187446594238
Epoch 480, val loss: 0.9136672019958496
Epoch 490, training loss: 0.09137026965618134 = 0.02040482871234417 + 0.01 * 7.096543788909912
Epoch 490, val loss: 0.9253988265991211
Epoch 500, training loss: 0.08970247209072113 = 0.0187840536236763 + 0.01 * 7.091842174530029
Epoch 500, val loss: 0.9368046522140503
Epoch 510, training loss: 0.08825147151947021 = 0.01734549179673195 + 0.01 * 7.0905985832214355
Epoch 510, val loss: 0.9478583931922913
Epoch 520, training loss: 0.08692797273397446 = 0.016063502058386803 + 0.01 * 7.086446762084961
Epoch 520, val loss: 0.9586064219474792
Epoch 530, training loss: 0.08573713898658752 = 0.014917123131453991 + 0.01 * 7.08200216293335
Epoch 530, val loss: 0.9690253138542175
Epoch 540, training loss: 0.08484365791082382 = 0.013889135792851448 + 0.01 * 7.095452785491943
Epoch 540, val loss: 0.9791299104690552
Epoch 550, training loss: 0.08371634781360626 = 0.012966353446245193 + 0.01 * 7.075000286102295
Epoch 550, val loss: 0.9889188408851624
Epoch 560, training loss: 0.08287167549133301 = 0.01213416550308466 + 0.01 * 7.073751449584961
Epoch 560, val loss: 0.9983929395675659
Epoch 570, training loss: 0.08207523077726364 = 0.011380760930478573 + 0.01 * 7.069447040557861
Epoch 570, val loss: 1.0075938701629639
Epoch 580, training loss: 0.08135922998189926 = 0.010696926154196262 + 0.01 * 7.066230297088623
Epoch 580, val loss: 1.0165090560913086
Epoch 590, training loss: 0.0807039886713028 = 0.01007457822561264 + 0.01 * 7.062941074371338
Epoch 590, val loss: 1.0251895189285278
Epoch 600, training loss: 0.08014287799596786 = 0.009507796727120876 + 0.01 * 7.063508033752441
Epoch 600, val loss: 1.0335767269134521
Epoch 610, training loss: 0.0795271098613739 = 0.008990622125566006 + 0.01 * 7.053648948669434
Epoch 610, val loss: 1.041701078414917
Epoch 620, training loss: 0.07903184741735458 = 0.008516566827893257 + 0.01 * 7.051528453826904
Epoch 620, val loss: 1.0496035814285278
Epoch 630, training loss: 0.07854661345481873 = 0.008080702275037766 + 0.01 * 7.046590805053711
Epoch 630, val loss: 1.0572999715805054
Epoch 640, training loss: 0.07825612276792526 = 0.007679108064621687 + 0.01 * 7.057701587677002
Epoch 640, val loss: 1.0647779703140259
Epoch 650, training loss: 0.07774196565151215 = 0.007309394888579845 + 0.01 * 7.043257713317871
Epoch 650, val loss: 1.0720338821411133
Epoch 660, training loss: 0.07730913162231445 = 0.00696778716519475 + 0.01 * 7.0341339111328125
Epoch 660, val loss: 1.079107403755188
Epoch 670, training loss: 0.07692732661962509 = 0.006651118863373995 + 0.01 * 7.027620792388916
Epoch 670, val loss: 1.0859909057617188
Epoch 680, training loss: 0.07690346240997314 = 0.00635702395811677 + 0.01 * 7.054644584655762
Epoch 680, val loss: 1.0927006006240845
Epoch 690, training loss: 0.07628948986530304 = 0.006084830034524202 + 0.01 * 7.020465850830078
Epoch 690, val loss: 1.0991491079330444
Epoch 700, training loss: 0.07603277266025543 = 0.005831862334161997 + 0.01 * 7.020091533660889
Epoch 700, val loss: 1.1054681539535522
Epoch 710, training loss: 0.07577840983867645 = 0.005596268456429243 + 0.01 * 7.018214702606201
Epoch 710, val loss: 1.1115548610687256
Epoch 720, training loss: 0.07540710270404816 = 0.005376371555030346 + 0.01 * 7.003073215484619
Epoch 720, val loss: 1.1174702644348145
Epoch 730, training loss: 0.07513216882944107 = 0.005170335061848164 + 0.01 * 6.996182918548584
Epoch 730, val loss: 1.1232855319976807
Epoch 740, training loss: 0.07512281090021133 = 0.004977184813469648 + 0.01 * 7.014563083648682
Epoch 740, val loss: 1.1289138793945312
Epoch 750, training loss: 0.07469511032104492 = 0.004796300083398819 + 0.01 * 6.9898810386657715
Epoch 750, val loss: 1.1344157457351685
Epoch 760, training loss: 0.07442490756511688 = 0.0046268003061413765 + 0.01 * 6.9798102378845215
Epoch 760, val loss: 1.1397171020507812
Epoch 770, training loss: 0.07420472800731659 = 0.004468083381652832 + 0.01 * 6.973664283752441
Epoch 770, val loss: 1.1449000835418701
Epoch 780, training loss: 0.07428589463233948 = 0.00431862473487854 + 0.01 * 6.996726989746094
Epoch 780, val loss: 1.1498727798461914
Epoch 790, training loss: 0.07389213144779205 = 0.004178062081336975 + 0.01 * 6.971407413482666
Epoch 790, val loss: 1.1548100709915161
Epoch 800, training loss: 0.07377917319536209 = 0.0040453290566802025 + 0.01 * 6.973384380340576
Epoch 800, val loss: 1.1595553159713745
Epoch 810, training loss: 0.07351076602935791 = 0.003920193761587143 + 0.01 * 6.959056854248047
Epoch 810, val loss: 1.1640647649765015
Epoch 820, training loss: 0.07326634228229523 = 0.0038025639951229095 + 0.01 * 6.946378231048584
Epoch 820, val loss: 1.1686477661132812
Epoch 830, training loss: 0.07311926782131195 = 0.0036913282237946987 + 0.01 * 6.942793846130371
Epoch 830, val loss: 1.1728695631027222
Epoch 840, training loss: 0.07275068014860153 = 0.0035863027442246675 + 0.01 * 6.916438102722168
Epoch 840, val loss: 1.1771419048309326
Epoch 850, training loss: 0.07282493263483047 = 0.0034869681112468243 + 0.01 * 6.9337968826293945
Epoch 850, val loss: 1.1810804605484009
Epoch 860, training loss: 0.07242414355278015 = 0.0033932814840227365 + 0.01 * 6.903085708618164
Epoch 860, val loss: 1.185136318206787
Epoch 870, training loss: 0.07248087227344513 = 0.00330414529889822 + 0.01 * 6.917673110961914
Epoch 870, val loss: 1.1889150142669678
Epoch 880, training loss: 0.07248251140117645 = 0.003219367004930973 + 0.01 * 6.926314830780029
Epoch 880, val loss: 1.1925835609436035
Epoch 890, training loss: 0.07199795544147491 = 0.0031391263473778963 + 0.01 * 6.885883331298828
Epoch 890, val loss: 1.196207880973816
Epoch 900, training loss: 0.07191523164510727 = 0.0030628463719040155 + 0.01 * 6.8852386474609375
Epoch 900, val loss: 1.199769377708435
Epoch 910, training loss: 0.07194194197654724 = 0.0029900488443672657 + 0.01 * 6.895188808441162
Epoch 910, val loss: 1.203072190284729
Epoch 920, training loss: 0.07133645564317703 = 0.0029208180494606495 + 0.01 * 6.8415632247924805
Epoch 920, val loss: 1.206363320350647
Epoch 930, training loss: 0.07168092578649521 = 0.002854791935533285 + 0.01 * 6.882613182067871
Epoch 930, val loss: 1.2096176147460938
Epoch 940, training loss: 0.07119639962911606 = 0.0027916450053453445 + 0.01 * 6.840475559234619
Epoch 940, val loss: 1.2127169370651245
Epoch 950, training loss: 0.07111585885286331 = 0.002731283660978079 + 0.01 * 6.8384575843811035
Epoch 950, val loss: 1.2157108783721924
Epoch 960, training loss: 0.07098899036645889 = 0.002673586132004857 + 0.01 * 6.831540107727051
Epoch 960, val loss: 1.2187373638153076
Epoch 970, training loss: 0.07088341563940048 = 0.002618285594508052 + 0.01 * 6.826513767242432
Epoch 970, val loss: 1.2215237617492676
Epoch 980, training loss: 0.07083763182163239 = 0.0025652798358350992 + 0.01 * 6.827235698699951
Epoch 980, val loss: 1.2243884801864624
Epoch 990, training loss: 0.07059773802757263 = 0.0025144307874143124 + 0.01 * 6.80833101272583
Epoch 990, val loss: 1.2270888090133667
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 2.0344882011413574 = 1.9485197067260742 + 0.01 * 8.596845626831055
Epoch 0, val loss: 1.9437086582183838
Epoch 10, training loss: 2.0241856575012207 = 1.9382179975509644 + 0.01 * 8.59677791595459
Epoch 10, val loss: 1.934274435043335
Epoch 20, training loss: 2.011212110519409 = 1.9252467155456543 + 0.01 * 8.596538543701172
Epoch 20, val loss: 1.9221080541610718
Epoch 30, training loss: 1.9927234649658203 = 1.9067673683166504 + 0.01 * 8.59560775756836
Epoch 30, val loss: 1.9044920206069946
Epoch 40, training loss: 1.9651141166687012 = 1.879217267036438 + 0.01 * 8.589683532714844
Epoch 40, val loss: 1.8785136938095093
Epoch 50, training loss: 1.926844835281372 = 1.8413037061691284 + 0.01 * 8.554108619689941
Epoch 50, val loss: 1.8447400331497192
Epoch 60, training loss: 1.8851960897445679 = 1.8011821508407593 + 0.01 * 8.401389122009277
Epoch 60, val loss: 1.812878131866455
Epoch 70, training loss: 1.8502297401428223 = 1.7684545516967773 + 0.01 * 8.17752456665039
Epoch 70, val loss: 1.7862952947616577
Epoch 80, training loss: 1.8079863786697388 = 1.7273005247116089 + 0.01 * 8.068582534790039
Epoch 80, val loss: 1.7472184896469116
Epoch 90, training loss: 1.7498334646224976 = 1.6706238985061646 + 0.01 * 7.920953273773193
Epoch 90, val loss: 1.6980793476104736
Epoch 100, training loss: 1.6734280586242676 = 1.5957305431365967 + 0.01 * 7.769746780395508
Epoch 100, val loss: 1.637863039970398
Epoch 110, training loss: 1.5844980478286743 = 1.5083149671554565 + 0.01 * 7.6183037757873535
Epoch 110, val loss: 1.5674649477005005
Epoch 120, training loss: 1.4933536052703857 = 1.4179930686950684 + 0.01 * 7.536048889160156
Epoch 120, val loss: 1.4963947534561157
Epoch 130, training loss: 1.4048571586608887 = 1.3300975561141968 + 0.01 * 7.475963115692139
Epoch 130, val loss: 1.428223729133606
Epoch 140, training loss: 1.3189256191253662 = 1.2448238134384155 + 0.01 * 7.410186290740967
Epoch 140, val loss: 1.3633898496627808
Epoch 150, training loss: 1.2356922626495361 = 1.1621037721633911 + 0.01 * 7.358848571777344
Epoch 150, val loss: 1.302149772644043
Epoch 160, training loss: 1.1561429500579834 = 1.083085298538208 + 0.01 * 7.305764198303223
Epoch 160, val loss: 1.2452033758163452
Epoch 170, training loss: 1.081640601158142 = 1.0089160203933716 + 0.01 * 7.272459506988525
Epoch 170, val loss: 1.1925766468048096
Epoch 180, training loss: 1.0122337341308594 = 0.9396567344665527 + 0.01 * 7.2577056884765625
Epoch 180, val loss: 1.1440743207931519
Epoch 190, training loss: 0.9467285871505737 = 0.8742766380310059 + 0.01 * 7.245193004608154
Epoch 190, val loss: 1.0983099937438965
Epoch 200, training loss: 0.8836621046066284 = 0.8113713264465332 + 0.01 * 7.229079723358154
Epoch 200, val loss: 1.054469347000122
Epoch 210, training loss: 0.8222965598106384 = 0.7501744627952576 + 0.01 * 7.2122111320495605
Epoch 210, val loss: 1.0126953125
Epoch 220, training loss: 0.7629107236862183 = 0.6909535527229309 + 0.01 * 7.195714950561523
Epoch 220, val loss: 0.9739137887954712
Epoch 230, training loss: 0.7062517404556274 = 0.6344356536865234 + 0.01 * 7.181608200073242
Epoch 230, val loss: 0.9395650029182434
Epoch 240, training loss: 0.6530392169952393 = 0.5813309550285339 + 0.01 * 7.17082405090332
Epoch 240, val loss: 0.9108554124832153
Epoch 250, training loss: 0.6034832000732422 = 0.5318398475646973 + 0.01 * 7.164337635040283
Epoch 250, val loss: 0.8883131146430969
Epoch 260, training loss: 0.5569800138473511 = 0.48538896441459656 + 0.01 * 7.159107208251953
Epoch 260, val loss: 0.8716617822647095
Epoch 270, training loss: 0.5123971104621887 = 0.4408375918865204 + 0.01 * 7.1559529304504395
Epoch 270, val loss: 0.8596254587173462
Epoch 280, training loss: 0.4687318205833435 = 0.3971751630306244 + 0.01 * 7.155665874481201
Epoch 280, val loss: 0.8508144617080688
Epoch 290, training loss: 0.4255952835083008 = 0.3540748953819275 + 0.01 * 7.152040481567383
Epoch 290, val loss: 0.844464898109436
Epoch 300, training loss: 0.38362887501716614 = 0.31212836503982544 + 0.01 * 7.150051593780518
Epoch 300, val loss: 0.8407245874404907
Epoch 310, training loss: 0.34387075901031494 = 0.27238863706588745 + 0.01 * 7.148213863372803
Epoch 310, val loss: 0.8398821353912354
Epoch 320, training loss: 0.3073902726173401 = 0.23591038584709167 + 0.01 * 7.1479902267456055
Epoch 320, val loss: 0.8422454595565796
Epoch 330, training loss: 0.2748095691204071 = 0.20337103307247162 + 0.01 * 7.14385461807251
Epoch 330, val loss: 0.8479307889938354
Epoch 340, training loss: 0.24640238285064697 = 0.17498132586479187 + 0.01 * 7.142106056213379
Epoch 340, val loss: 0.8567150831222534
Epoch 350, training loss: 0.22204919159412384 = 0.15060675144195557 + 0.01 * 7.144244194030762
Epoch 350, val loss: 0.8682072758674622
Epoch 360, training loss: 0.20126879215240479 = 0.12990015745162964 + 0.01 * 7.136864185333252
Epoch 360, val loss: 0.8819924592971802
Epoch 370, training loss: 0.18374061584472656 = 0.1123821809887886 + 0.01 * 7.1358442306518555
Epoch 370, val loss: 0.8975591063499451
Epoch 380, training loss: 0.16889673471450806 = 0.09757690876722336 + 0.01 * 7.131982326507568
Epoch 380, val loss: 0.9145585298538208
Epoch 390, training loss: 0.15633855760097504 = 0.08503657579421997 + 0.01 * 7.130198001861572
Epoch 390, val loss: 0.9325834512710571
Epoch 400, training loss: 0.14566978812217712 = 0.07438959926366806 + 0.01 * 7.128018856048584
Epoch 400, val loss: 0.9513434767723083
Epoch 410, training loss: 0.13656845688819885 = 0.06531960517168045 + 0.01 * 7.1248860359191895
Epoch 410, val loss: 0.9705384373664856
Epoch 420, training loss: 0.128795325756073 = 0.05757172033190727 + 0.01 * 7.122360706329346
Epoch 420, val loss: 0.9899480938911438
Epoch 430, training loss: 0.12215960025787354 = 0.050941355526447296 + 0.01 * 7.121824264526367
Epoch 430, val loss: 1.0094407796859741
Epoch 440, training loss: 0.11644671112298965 = 0.04525565356016159 + 0.01 * 7.119105815887451
Epoch 440, val loss: 1.0286749601364136
Epoch 450, training loss: 0.11148708313703537 = 0.040366142988204956 + 0.01 * 7.112093925476074
Epoch 450, val loss: 1.0476540327072144
Epoch 460, training loss: 0.10722184926271439 = 0.03615238517522812 + 0.01 * 7.1069464683532715
Epoch 460, val loss: 1.0661624670028687
Epoch 470, training loss: 0.1035432294011116 = 0.03251246362924576 + 0.01 * 7.103076934814453
Epoch 470, val loss: 1.0841997861862183
Epoch 480, training loss: 0.100324347615242 = 0.029358088970184326 + 0.01 * 7.096626281738281
Epoch 480, val loss: 1.1017060279846191
Epoch 490, training loss: 0.09758047759532928 = 0.026617329567670822 + 0.01 * 7.096314430236816
Epoch 490, val loss: 1.118632197380066
Epoch 500, training loss: 0.09506427496671677 = 0.02423100173473358 + 0.01 * 7.083327770233154
Epoch 500, val loss: 1.134925365447998
Epoch 510, training loss: 0.09289877116680145 = 0.022144820541143417 + 0.01 * 7.075395107269287
Epoch 510, val loss: 1.1505906581878662
Epoch 520, training loss: 0.09102492034435272 = 0.020312221720814705 + 0.01 * 7.071269989013672
Epoch 520, val loss: 1.1656594276428223
Epoch 530, training loss: 0.08946101367473602 = 0.018698975443840027 + 0.01 * 7.076204299926758
Epoch 530, val loss: 1.1800765991210938
Epoch 540, training loss: 0.08778298646211624 = 0.017273500561714172 + 0.01 * 7.0509490966796875
Epoch 540, val loss: 1.1938797235488892
Epoch 550, training loss: 0.08643209934234619 = 0.016005778685212135 + 0.01 * 7.042632579803467
Epoch 550, val loss: 1.207257866859436
Epoch 560, training loss: 0.08519769459962845 = 0.014874172396957874 + 0.01 * 7.032351970672607
Epoch 560, val loss: 1.2200645208358765
Epoch 570, training loss: 0.084144227206707 = 0.013862387277185917 + 0.01 * 7.028184413909912
Epoch 570, val loss: 1.2324466705322266
Epoch 580, training loss: 0.08316630125045776 = 0.012954344972968102 + 0.01 * 7.021195888519287
Epoch 580, val loss: 1.2442588806152344
Epoch 590, training loss: 0.08240626752376556 = 0.012138769961893559 + 0.01 * 7.026750087738037
Epoch 590, val loss: 1.2556700706481934
Epoch 600, training loss: 0.0815076231956482 = 0.011402585543692112 + 0.01 * 7.010504245758057
Epoch 600, val loss: 1.2665021419525146
Epoch 610, training loss: 0.0805601179599762 = 0.010737498290836811 + 0.01 * 6.982261657714844
Epoch 610, val loss: 1.2770545482635498
Epoch 620, training loss: 0.08053138852119446 = 0.010132815688848495 + 0.01 * 7.039857387542725
Epoch 620, val loss: 1.2871454954147339
Epoch 630, training loss: 0.07925985753536224 = 0.00958337727934122 + 0.01 * 6.967648506164551
Epoch 630, val loss: 1.296876072883606
Epoch 640, training loss: 0.07875031232833862 = 0.009081698954105377 + 0.01 * 6.966861724853516
Epoch 640, val loss: 1.306244134902954
Epoch 650, training loss: 0.07826431840658188 = 0.008621676824986935 + 0.01 * 6.964263916015625
Epoch 650, val loss: 1.3152130842208862
Epoch 660, training loss: 0.07830797135829926 = 0.008198771625757217 + 0.01 * 7.01092004776001
Epoch 660, val loss: 1.3239729404449463
Epoch 670, training loss: 0.07733719050884247 = 0.007809948176145554 + 0.01 * 6.952723979949951
Epoch 670, val loss: 1.3324732780456543
Epoch 680, training loss: 0.07677783817052841 = 0.007451288402080536 + 0.01 * 6.932655334472656
Epoch 680, val loss: 1.3405249118804932
Epoch 690, training loss: 0.0763193741440773 = 0.007119691930711269 + 0.01 * 6.919968128204346
Epoch 690, val loss: 1.3483734130859375
Epoch 700, training loss: 0.07681680470705032 = 0.006811910774558783 + 0.01 * 7.000489711761475
Epoch 700, val loss: 1.3559765815734863
Epoch 710, training loss: 0.07590090483427048 = 0.006526758894324303 + 0.01 * 6.937414646148682
Epoch 710, val loss: 1.3633460998535156
Epoch 720, training loss: 0.07518776506185532 = 0.0062614367343485355 + 0.01 * 6.892632961273193
Epoch 720, val loss: 1.3704372644424438
Epoch 730, training loss: 0.0752304345369339 = 0.006013782694935799 + 0.01 * 6.921665668487549
Epoch 730, val loss: 1.3773339986801147
Epoch 740, training loss: 0.0747922956943512 = 0.005782799329608679 + 0.01 * 6.900949954986572
Epoch 740, val loss: 1.38398277759552
Epoch 750, training loss: 0.074553482234478 = 0.005566227715462446 + 0.01 * 6.898725986480713
Epoch 750, val loss: 1.3904953002929688
Epoch 760, training loss: 0.07414388656616211 = 0.005363449454307556 + 0.01 * 6.8780436515808105
Epoch 760, val loss: 1.3967522382736206
Epoch 770, training loss: 0.07428764551877975 = 0.005173062905669212 + 0.01 * 6.9114580154418945
Epoch 770, val loss: 1.4027844667434692
Epoch 780, training loss: 0.0739050805568695 = 0.004994610790163279 + 0.01 * 6.891047477722168
Epoch 780, val loss: 1.408724069595337
Epoch 790, training loss: 0.0734521821141243 = 0.004826732911169529 + 0.01 * 6.862545490264893
Epoch 790, val loss: 1.4144669771194458
Epoch 800, training loss: 0.07350493222475052 = 0.004668365232646465 + 0.01 * 6.883657455444336
Epoch 800, val loss: 1.4199031591415405
Epoch 810, training loss: 0.07326148450374603 = 0.004519575275480747 + 0.01 * 6.874190807342529
Epoch 810, val loss: 1.4253771305084229
Epoch 820, training loss: 0.07294049113988876 = 0.004378794692456722 + 0.01 * 6.856169700622559
Epoch 820, val loss: 1.430569052696228
Epoch 830, training loss: 0.07270172238349915 = 0.004245597869157791 + 0.01 * 6.845612049102783
Epoch 830, val loss: 1.435669183731079
Epoch 840, training loss: 0.07298456132411957 = 0.004119428806006908 + 0.01 * 6.886513710021973
Epoch 840, val loss: 1.440597653388977
Epoch 850, training loss: 0.0724010244011879 = 0.004000293090939522 + 0.01 * 6.840073585510254
Epoch 850, val loss: 1.4454313516616821
Epoch 860, training loss: 0.07225670665502548 = 0.00388702261261642 + 0.01 * 6.836968898773193
Epoch 860, val loss: 1.4500830173492432
Epoch 870, training loss: 0.0722389742732048 = 0.003779448801651597 + 0.01 * 6.84595251083374
Epoch 870, val loss: 1.4546282291412354
Epoch 880, training loss: 0.07201351970434189 = 0.0036773139145225286 + 0.01 * 6.833620548248291
Epoch 880, val loss: 1.459011197090149
Epoch 890, training loss: 0.0718860775232315 = 0.003580450778827071 + 0.01 * 6.830562591552734
Epoch 890, val loss: 1.4633595943450928
Epoch 900, training loss: 0.07215392589569092 = 0.0034879811573773623 + 0.01 * 6.866594314575195
Epoch 900, val loss: 1.467529296875
Epoch 910, training loss: 0.07185815274715424 = 0.003400223795324564 + 0.01 * 6.8457932472229
Epoch 910, val loss: 1.4715911149978638
Epoch 920, training loss: 0.071409210562706 = 0.003316361689940095 + 0.01 * 6.809284687042236
Epoch 920, val loss: 1.4755715131759644
Epoch 930, training loss: 0.07161348313093185 = 0.003236195771023631 + 0.01 * 6.837728977203369
Epoch 930, val loss: 1.4794881343841553
Epoch 940, training loss: 0.0716007724404335 = 0.0031599642243236303 + 0.01 * 6.844081401824951
Epoch 940, val loss: 1.4832700490951538
Epoch 950, training loss: 0.07114262133836746 = 0.0030869068577885628 + 0.01 * 6.80557107925415
Epoch 950, val loss: 1.4868937730789185
Epoch 960, training loss: 0.07138586789369583 = 0.0030171656981110573 + 0.01 * 6.8368706703186035
Epoch 960, val loss: 1.4905133247375488
Epoch 970, training loss: 0.07094234973192215 = 0.0029503859113901854 + 0.01 * 6.799196720123291
Epoch 970, val loss: 1.4940624237060547
Epoch 980, training loss: 0.07096846401691437 = 0.0028864953201264143 + 0.01 * 6.808197021484375
Epoch 980, val loss: 1.497463345527649
Epoch 990, training loss: 0.07069703936576843 = 0.002825229661539197 + 0.01 * 6.787181377410889
Epoch 990, val loss: 1.5007933378219604
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8133895624670533
The final CL Acc:0.76173, 0.00698, The final GNN Acc:0.81444, 0.00310
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13300])
remove edge: torch.Size([2, 7928])
updated graph: torch.Size([2, 10672])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0437231063842773 = 1.9577548503875732 + 0.01 * 8.596824645996094
Epoch 0, val loss: 1.9639942646026611
Epoch 10, training loss: 2.0333292484283447 = 1.947361946105957 + 0.01 * 8.596721649169922
Epoch 10, val loss: 1.9540445804595947
Epoch 20, training loss: 2.0198986530303955 = 1.9339349269866943 + 0.01 * 8.596375465393066
Epoch 20, val loss: 1.9405689239501953
Epoch 30, training loss: 2.0006866455078125 = 1.9147369861602783 + 0.01 * 8.594971656799316
Epoch 30, val loss: 1.9207773208618164
Epoch 40, training loss: 1.9720834493637085 = 1.886237382888794 + 0.01 * 8.584606170654297
Epoch 40, val loss: 1.891312837600708
Epoch 50, training loss: 1.93141770362854 = 1.846176266670227 + 0.01 * 8.524148941040039
Epoch 50, val loss: 1.8513219356536865
Epoch 60, training loss: 1.883793830871582 = 1.8012441396713257 + 0.01 * 8.254966735839844
Epoch 60, val loss: 1.810234785079956
Epoch 70, training loss: 1.8442409038543701 = 1.7632659673690796 + 0.01 * 8.09749984741211
Epoch 70, val loss: 1.7775546312332153
Epoch 80, training loss: 1.7970415353775024 = 1.7175265550613403 + 0.01 * 7.9514970779418945
Epoch 80, val loss: 1.7346478700637817
Epoch 90, training loss: 1.731648564338684 = 1.6538140773773193 + 0.01 * 7.783443927764893
Epoch 90, val loss: 1.6761558055877686
Epoch 100, training loss: 1.6467851400375366 = 1.5704667568206787 + 0.01 * 7.631838798522949
Epoch 100, val loss: 1.603279709815979
Epoch 110, training loss: 1.5496562719345093 = 1.4752565622329712 + 0.01 * 7.439975261688232
Epoch 110, val loss: 1.5227702856063843
Epoch 120, training loss: 1.453377604484558 = 1.3798466920852661 + 0.01 * 7.353095054626465
Epoch 120, val loss: 1.4446065425872803
Epoch 130, training loss: 1.3613742589950562 = 1.2882800102233887 + 0.01 * 7.309426784515381
Epoch 130, val loss: 1.3718864917755127
Epoch 140, training loss: 1.272774577140808 = 1.1999832391738892 + 0.01 * 7.279129505157471
Epoch 140, val loss: 1.3030184507369995
Epoch 150, training loss: 1.188782811164856 = 1.1163287162780762 + 0.01 * 7.2454071044921875
Epoch 150, val loss: 1.2389249801635742
Epoch 160, training loss: 1.1117905378341675 = 1.0396380424499512 + 0.01 * 7.215248107910156
Epoch 160, val loss: 1.1812896728515625
Epoch 170, training loss: 1.0423215627670288 = 0.9703624248504639 + 0.01 * 7.195916652679443
Epoch 170, val loss: 1.1300376653671265
Epoch 180, training loss: 0.9786810874938965 = 0.9068469405174255 + 0.01 * 7.183414936065674
Epoch 180, val loss: 1.0826586484909058
Epoch 190, training loss: 0.9175913333892822 = 0.8459399342536926 + 0.01 * 7.165139675140381
Epoch 190, val loss: 1.0361958742141724
Epoch 200, training loss: 0.8560709953308105 = 0.7846851348876953 + 0.01 * 7.138586521148682
Epoch 200, val loss: 0.9881125092506409
Epoch 210, training loss: 0.7929714918136597 = 0.7218525409698486 + 0.01 * 7.111897945404053
Epoch 210, val loss: 0.938143789768219
Epoch 220, training loss: 0.7291955947875977 = 0.6583613157272339 + 0.01 * 7.083431243896484
Epoch 220, val loss: 0.8881674408912659
Epoch 230, training loss: 0.6668897867202759 = 0.5962545871734619 + 0.01 * 7.06351900100708
Epoch 230, val loss: 0.8407097458839417
Epoch 240, training loss: 0.6077792644500732 = 0.5373546481132507 + 0.01 * 7.042463779449463
Epoch 240, val loss: 0.7981932163238525
Epoch 250, training loss: 0.5528830289840698 = 0.4826073944568634 + 0.01 * 7.027564525604248
Epoch 250, val loss: 0.7621595859527588
Epoch 260, training loss: 0.5024782419204712 = 0.43227317929267883 + 0.01 * 7.020506858825684
Epoch 260, val loss: 0.732722282409668
Epoch 270, training loss: 0.45625773072242737 = 0.38622045516967773 + 0.01 * 7.003727436065674
Epoch 270, val loss: 0.7094036340713501
Epoch 280, training loss: 0.413912832736969 = 0.34396079182624817 + 0.01 * 6.995203495025635
Epoch 280, val loss: 0.6911742687225342
Epoch 290, training loss: 0.3747849762439728 = 0.30482590198516846 + 0.01 * 6.995908260345459
Epoch 290, val loss: 0.676842212677002
Epoch 300, training loss: 0.3379169702529907 = 0.26811692118644714 + 0.01 * 6.98000431060791
Epoch 300, val loss: 0.6654006242752075
Epoch 310, training loss: 0.3031781017780304 = 0.23343989253044128 + 0.01 * 6.973820686340332
Epoch 310, val loss: 0.6562045812606812
Epoch 320, training loss: 0.2707025706768036 = 0.2010347843170166 + 0.01 * 6.9667792320251465
Epoch 320, val loss: 0.6490952372550964
Epoch 330, training loss: 0.24126829206943512 = 0.171625554561615 + 0.01 * 6.964273929595947
Epoch 330, val loss: 0.6444329023361206
Epoch 340, training loss: 0.21549099683761597 = 0.1458883136510849 + 0.01 * 6.960268974304199
Epoch 340, val loss: 0.6425220370292664
Epoch 350, training loss: 0.19365465641021729 = 0.1240844875574112 + 0.01 * 6.9570159912109375
Epoch 350, val loss: 0.6434590220451355
Epoch 360, training loss: 0.1755150854587555 = 0.10598643124103546 + 0.01 * 6.952865123748779
Epoch 360, val loss: 0.6469897031784058
Epoch 370, training loss: 0.16054630279541016 = 0.09105195105075836 + 0.01 * 6.949436187744141
Epoch 370, val loss: 0.6526400446891785
Epoch 380, training loss: 0.1482219696044922 = 0.0786898285150528 + 0.01 * 6.953213691711426
Epoch 380, val loss: 0.65996915102005
Epoch 390, training loss: 0.13788005709648132 = 0.06840577721595764 + 0.01 * 6.947427272796631
Epoch 390, val loss: 0.6683708429336548
Epoch 400, training loss: 0.12921234965324402 = 0.05979904532432556 + 0.01 * 6.941329479217529
Epoch 400, val loss: 0.6776027679443359
Epoch 410, training loss: 0.12192453444004059 = 0.05256090685725212 + 0.01 * 6.936363220214844
Epoch 410, val loss: 0.6872807741165161
Epoch 420, training loss: 0.1157769113779068 = 0.046442292630672455 + 0.01 * 6.933462142944336
Epoch 420, val loss: 0.6972886919975281
Epoch 430, training loss: 0.11065920442342758 = 0.04125513136386871 + 0.01 * 6.940407752990723
Epoch 430, val loss: 0.7073677778244019
Epoch 440, training loss: 0.10614733397960663 = 0.036841314285993576 + 0.01 * 6.930602073669434
Epoch 440, val loss: 0.7174960970878601
Epoch 450, training loss: 0.10229375958442688 = 0.03306425362825394 + 0.01 * 6.922950744628906
Epoch 450, val loss: 0.7275798320770264
Epoch 460, training loss: 0.09902016818523407 = 0.029816381633281708 + 0.01 * 6.920379161834717
Epoch 460, val loss: 0.7375782132148743
Epoch 470, training loss: 0.09617209434509277 = 0.027010362595319748 + 0.01 * 6.916173458099365
Epoch 470, val loss: 0.7474072575569153
Epoch 480, training loss: 0.09373648464679718 = 0.02457611635327339 + 0.01 * 6.916036605834961
Epoch 480, val loss: 0.7570253014564514
Epoch 490, training loss: 0.09160653501749039 = 0.022457575425505638 + 0.01 * 6.914896011352539
Epoch 490, val loss: 0.7663977742195129
Epoch 500, training loss: 0.08966778218746185 = 0.020603951066732407 + 0.01 * 6.906383991241455
Epoch 500, val loss: 0.7755393385887146
Epoch 510, training loss: 0.0880923867225647 = 0.018973739817738533 + 0.01 * 6.911865234375
Epoch 510, val loss: 0.7844279408454895
Epoch 520, training loss: 0.0865340456366539 = 0.017535803839564323 + 0.01 * 6.899824142456055
Epoch 520, val loss: 0.7930817604064941
Epoch 530, training loss: 0.0852244570851326 = 0.016259795054793358 + 0.01 * 6.896466255187988
Epoch 530, val loss: 0.8014703392982483
Epoch 540, training loss: 0.08405905216932297 = 0.015122383832931519 + 0.01 * 6.893667221069336
Epoch 540, val loss: 0.8096426129341125
Epoch 550, training loss: 0.08299451321363449 = 0.014104610309004784 + 0.01 * 6.88899040222168
Epoch 550, val loss: 0.8175795674324036
Epoch 560, training loss: 0.0821184366941452 = 0.013190869241952896 + 0.01 * 6.892756938934326
Epoch 560, val loss: 0.8252521753311157
Epoch 570, training loss: 0.08117766678333282 = 0.01236847322434187 + 0.01 * 6.880919933319092
Epoch 570, val loss: 0.8327425718307495
Epoch 580, training loss: 0.0804913267493248 = 0.011625084094703197 + 0.01 * 6.886624336242676
Epoch 580, val loss: 0.8399946689605713
Epoch 590, training loss: 0.07968155294656754 = 0.010951103642582893 + 0.01 * 6.873045444488525
Epoch 590, val loss: 0.847003698348999
Epoch 600, training loss: 0.07906756550073624 = 0.010338081046938896 + 0.01 * 6.87294864654541
Epoch 600, val loss: 0.853850245475769
Epoch 610, training loss: 0.07854276150465012 = 0.009778310544788837 + 0.01 * 6.876445293426514
Epoch 610, val loss: 0.8605070114135742
Epoch 620, training loss: 0.07795533537864685 = 0.009267066605389118 + 0.01 * 6.868826866149902
Epoch 620, val loss: 0.8669168949127197
Epoch 630, training loss: 0.07739000767469406 = 0.00879835058003664 + 0.01 * 6.859165668487549
Epoch 630, val loss: 0.8731870651245117
Epoch 640, training loss: 0.07706207782030106 = 0.008367148227989674 + 0.01 * 6.869493007659912
Epoch 640, val loss: 0.8792494535446167
Epoch 650, training loss: 0.07650931924581528 = 0.007970369420945644 + 0.01 * 6.8538947105407715
Epoch 650, val loss: 0.8851946592330933
Epoch 660, training loss: 0.07619372755289078 = 0.007603362202644348 + 0.01 * 6.859036445617676
Epoch 660, val loss: 0.8909662961959839
Epoch 670, training loss: 0.07570160925388336 = 0.007264489773660898 + 0.01 * 6.843712329864502
Epoch 670, val loss: 0.896508514881134
Epoch 680, training loss: 0.075388103723526 = 0.006949937902390957 + 0.01 * 6.84381628036499
Epoch 680, val loss: 0.9019563794136047
Epoch 690, training loss: 0.07498187571763992 = 0.00665780995041132 + 0.01 * 6.832406997680664
Epoch 690, val loss: 0.9072715640068054
Epoch 700, training loss: 0.07475703954696655 = 0.006385749205946922 + 0.01 * 6.837128639221191
Epoch 700, val loss: 0.9124536514282227
Epoch 710, training loss: 0.07435232400894165 = 0.006132427137345076 + 0.01 * 6.821990489959717
Epoch 710, val loss: 0.9174410104751587
Epoch 720, training loss: 0.07408430427312851 = 0.005895702634006739 + 0.01 * 6.8188605308532715
Epoch 720, val loss: 0.9223091006278992
Epoch 730, training loss: 0.07387680560350418 = 0.005674436688423157 + 0.01 * 6.820236682891846
Epoch 730, val loss: 0.9270774722099304
Epoch 740, training loss: 0.07385364174842834 = 0.0054674334824085236 + 0.01 * 6.838621616363525
Epoch 740, val loss: 0.931669294834137
Epoch 750, training loss: 0.07330577820539474 = 0.005273579154163599 + 0.01 * 6.803220272064209
Epoch 750, val loss: 0.9361989498138428
Epoch 760, training loss: 0.07319436967372894 = 0.0050908955745399 + 0.01 * 6.810347557067871
Epoch 760, val loss: 0.9405584931373596
Epoch 770, training loss: 0.07296522706747055 = 0.004919031169265509 + 0.01 * 6.804619789123535
Epoch 770, val loss: 0.9448793530464172
Epoch 780, training loss: 0.07277420908212662 = 0.0047572702169418335 + 0.01 * 6.801694393157959
Epoch 780, val loss: 0.9490299820899963
Epoch 790, training loss: 0.07250626385211945 = 0.004604421090334654 + 0.01 * 6.790184020996094
Epoch 790, val loss: 0.9531164169311523
Epoch 800, training loss: 0.07239340990781784 = 0.004459860268980265 + 0.01 * 6.7933549880981445
Epoch 800, val loss: 0.9571525454521179
Epoch 810, training loss: 0.07226081937551498 = 0.004323539324104786 + 0.01 * 6.793727874755859
Epoch 810, val loss: 0.961003839969635
Epoch 820, training loss: 0.07206888496875763 = 0.0041953157633543015 + 0.01 * 6.787356853485107
Epoch 820, val loss: 0.9647122025489807
Epoch 830, training loss: 0.0718180388212204 = 0.004073276184499264 + 0.01 * 6.774476051330566
Epoch 830, val loss: 0.9684301018714905
Epoch 840, training loss: 0.07170063257217407 = 0.003957871347665787 + 0.01 * 6.774276256561279
Epoch 840, val loss: 0.9720359444618225
Epoch 850, training loss: 0.07152405381202698 = 0.003847680054605007 + 0.01 * 6.767637252807617
Epoch 850, val loss: 0.9755514860153198
Epoch 860, training loss: 0.0714481770992279 = 0.0037432925309985876 + 0.01 * 6.770488739013672
Epoch 860, val loss: 0.97901451587677
Epoch 870, training loss: 0.07137411087751389 = 0.00364380213432014 + 0.01 * 6.773030757904053
Epoch 870, val loss: 0.9823719263076782
Epoch 880, training loss: 0.07108910381793976 = 0.00354937044903636 + 0.01 * 6.753973960876465
Epoch 880, val loss: 0.9856910109519958
Epoch 890, training loss: 0.07099887728691101 = 0.0034591276198625565 + 0.01 * 6.7539753913879395
Epoch 890, val loss: 0.9888648986816406
Epoch 900, training loss: 0.07124630361795425 = 0.0033728976268321276 + 0.01 * 6.78734016418457
Epoch 900, val loss: 0.9920299649238586
Epoch 910, training loss: 0.07081561535596848 = 0.0032908220309764147 + 0.01 * 6.752479553222656
Epoch 910, val loss: 0.9951907396316528
Epoch 920, training loss: 0.07086024433374405 = 0.0032122484408318996 + 0.01 * 6.76479959487915
Epoch 920, val loss: 0.9982096552848816
Epoch 930, training loss: 0.07057949900627136 = 0.003137508174404502 + 0.01 * 6.744199752807617
Epoch 930, val loss: 1.001158595085144
Epoch 940, training loss: 0.07051639258861542 = 0.0030656959861516953 + 0.01 * 6.74506950378418
Epoch 940, val loss: 1.0040950775146484
Epoch 950, training loss: 0.07033336162567139 = 0.002996711526066065 + 0.01 * 6.7336649894714355
Epoch 950, val loss: 1.0069355964660645
Epoch 960, training loss: 0.07035772502422333 = 0.0029305960051715374 + 0.01 * 6.74271297454834
Epoch 960, val loss: 1.0098631381988525
Epoch 970, training loss: 0.07024671882390976 = 0.0028679468668997288 + 0.01 * 6.737877368927002
Epoch 970, val loss: 1.0125024318695068
Epoch 980, training loss: 0.07011282444000244 = 0.0028072542045265436 + 0.01 * 6.730557441711426
Epoch 980, val loss: 1.0152158737182617
Epoch 990, training loss: 0.07020007073879242 = 0.002749171108007431 + 0.01 * 6.745089530944824
Epoch 990, val loss: 1.0179105997085571
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.0449676513671875 = 1.9589992761611938 + 0.01 * 8.596848487854004
Epoch 0, val loss: 1.958876371383667
Epoch 10, training loss: 2.034821033477783 = 1.9488530158996582 + 0.01 * 8.596792221069336
Epoch 10, val loss: 1.9485652446746826
Epoch 20, training loss: 2.02239990234375 = 1.936434030532837 + 0.01 * 8.596576690673828
Epoch 20, val loss: 1.9358220100402832
Epoch 30, training loss: 2.0051486492156982 = 1.9191904067993164 + 0.01 * 8.595833778381348
Epoch 30, val loss: 1.9182212352752686
Epoch 40, training loss: 1.9795501232147217 = 1.893637776374817 + 0.01 * 8.59123706817627
Epoch 40, val loss: 1.8924287557601929
Epoch 50, training loss: 1.942257285118103 = 1.8566765785217285 + 0.01 * 8.55806827545166
Epoch 50, val loss: 1.856606364250183
Epoch 60, training loss: 1.8963662385940552 = 1.8122285604476929 + 0.01 * 8.413763999938965
Epoch 60, val loss: 1.8174968957901
Epoch 70, training loss: 1.8562918901443481 = 1.7744899988174438 + 0.01 * 8.180188179016113
Epoch 70, val loss: 1.787805199623108
Epoch 80, training loss: 1.8164616823196411 = 1.7361419200897217 + 0.01 * 8.031976699829102
Epoch 80, val loss: 1.7529855966567993
Epoch 90, training loss: 1.7601622343063354 = 1.6827248334884644 + 0.01 * 7.743740081787109
Epoch 90, val loss: 1.7043542861938477
Epoch 100, training loss: 1.685745358467102 = 1.6110225915908813 + 0.01 * 7.4722723960876465
Epoch 100, val loss: 1.6427137851715088
Epoch 110, training loss: 1.594578504562378 = 1.5213414430618286 + 0.01 * 7.323705196380615
Epoch 110, val loss: 1.567996859550476
Epoch 120, training loss: 1.4971410036087036 = 1.4244165420532227 + 0.01 * 7.272441864013672
Epoch 120, val loss: 1.4877796173095703
Epoch 130, training loss: 1.4009400606155396 = 1.3285720348358154 + 0.01 * 7.236802577972412
Epoch 130, val loss: 1.4093412160873413
Epoch 140, training loss: 1.3064136505126953 = 1.2344062328338623 + 0.01 * 7.200747489929199
Epoch 140, val loss: 1.3327233791351318
Epoch 150, training loss: 1.2120978832244873 = 1.1404682397842407 + 0.01 * 7.162967681884766
Epoch 150, val loss: 1.2562724351882935
Epoch 160, training loss: 1.1194857358932495 = 1.0481780767440796 + 0.01 * 7.130766868591309
Epoch 160, val loss: 1.1817351579666138
Epoch 170, training loss: 1.0308760404586792 = 0.9597519040107727 + 0.01 * 7.112412929534912
Epoch 170, val loss: 1.1108343601226807
Epoch 180, training loss: 0.947490930557251 = 0.8764572143554688 + 0.01 * 7.10337495803833
Epoch 180, val loss: 1.0437695980072021
Epoch 190, training loss: 0.8701392412185669 = 0.7991482615470886 + 0.01 * 7.099100589752197
Epoch 190, val loss: 0.9815317988395691
Epoch 200, training loss: 0.7995887398719788 = 0.7286362648010254 + 0.01 * 7.095247745513916
Epoch 200, val loss: 0.9255622625350952
Epoch 210, training loss: 0.736005425453186 = 0.6650816202163696 + 0.01 * 7.09237813949585
Epoch 210, val loss: 0.8772686123847961
Epoch 220, training loss: 0.6779242753982544 = 0.607024610042572 + 0.01 * 7.089964866638184
Epoch 220, val loss: 0.8365617990493774
Epoch 230, training loss: 0.6231845617294312 = 0.5523037314414978 + 0.01 * 7.088083267211914
Epoch 230, val loss: 0.8020312786102295
Epoch 240, training loss: 0.5703206062316895 = 0.49945834279060364 + 0.01 * 7.0862274169921875
Epoch 240, val loss: 0.7716941237449646
Epoch 250, training loss: 0.5190544724464417 = 0.4482061564922333 + 0.01 * 7.084829330444336
Epoch 250, val loss: 0.7447232007980347
Epoch 260, training loss: 0.4700196087360382 = 0.3991960883140564 + 0.01 * 7.082352638244629
Epoch 260, val loss: 0.7212011218070984
Epoch 270, training loss: 0.42415255308151245 = 0.353353887796402 + 0.01 * 7.0798659324646
Epoch 270, val loss: 0.7019149661064148
Epoch 280, training loss: 0.3821226954460144 = 0.3113483786582947 + 0.01 * 7.077430248260498
Epoch 280, val loss: 0.6875449419021606
Epoch 290, training loss: 0.3441712260246277 = 0.27343493700027466 + 0.01 * 7.073629379272461
Epoch 290, val loss: 0.6782301068305969
Epoch 300, training loss: 0.310253381729126 = 0.2395578771829605 + 0.01 * 7.069551944732666
Epoch 300, val loss: 0.6734498739242554
Epoch 310, training loss: 0.28014278411865234 = 0.20949698984622955 + 0.01 * 7.064581394195557
Epoch 310, val loss: 0.6725163459777832
Epoch 320, training loss: 0.253533273935318 = 0.18294855952262878 + 0.01 * 7.058471202850342
Epoch 320, val loss: 0.6749775409698486
Epoch 330, training loss: 0.23017281293869019 = 0.15960925817489624 + 0.01 * 7.056356430053711
Epoch 330, val loss: 0.6805208325386047
Epoch 340, training loss: 0.20967769622802734 = 0.13920767605304718 + 0.01 * 7.047003269195557
Epoch 340, val loss: 0.6885918378829956
Epoch 350, training loss: 0.19186058640480042 = 0.12147080898284912 + 0.01 * 7.038978576660156
Epoch 350, val loss: 0.6987872123718262
Epoch 360, training loss: 0.1764877885580063 = 0.1061333641409874 + 0.01 * 7.035442352294922
Epoch 360, val loss: 0.710811972618103
Epoch 370, training loss: 0.16318261623382568 = 0.0929255411028862 + 0.01 * 7.0257086753845215
Epoch 370, val loss: 0.7241626977920532
Epoch 380, training loss: 0.15177616477012634 = 0.08158557862043381 + 0.01 * 7.019059181213379
Epoch 380, val loss: 0.73853600025177
Epoch 390, training loss: 0.14200466871261597 = 0.07186884433031082 + 0.01 * 7.013583660125732
Epoch 390, val loss: 0.7536214590072632
Epoch 400, training loss: 0.13363635540008545 = 0.06355205923318863 + 0.01 * 7.008430480957031
Epoch 400, val loss: 0.7691957354545593
Epoch 410, training loss: 0.1264738291501999 = 0.05643446370959282 + 0.01 * 7.003936290740967
Epoch 410, val loss: 0.7850940823554993
Epoch 420, training loss: 0.1203424334526062 = 0.05033494532108307 + 0.01 * 7.000748634338379
Epoch 420, val loss: 0.8011445999145508
Epoch 430, training loss: 0.1150619387626648 = 0.04509539529681206 + 0.01 * 6.996654033660889
Epoch 430, val loss: 0.8171876668930054
Epoch 440, training loss: 0.11057842522859573 = 0.04058031737804413 + 0.01 * 6.999810695648193
Epoch 440, val loss: 0.8330706357955933
Epoch 450, training loss: 0.10657692700624466 = 0.03667696565389633 + 0.01 * 6.989996433258057
Epoch 450, val loss: 0.8487946391105652
Epoch 460, training loss: 0.1031280905008316 = 0.03328697010874748 + 0.01 * 6.98411226272583
Epoch 460, val loss: 0.8641713857650757
Epoch 470, training loss: 0.10015282034873962 = 0.030331024900078773 + 0.01 * 6.982180118560791
Epoch 470, val loss: 0.8792678713798523
Epoch 480, training loss: 0.09751088917255402 = 0.027744458988308907 + 0.01 * 6.976643085479736
Epoch 480, val loss: 0.8939102292060852
Epoch 490, training loss: 0.09519064426422119 = 0.02547009289264679 + 0.01 * 6.972055435180664
Epoch 490, val loss: 0.9082227945327759
Epoch 500, training loss: 0.09317608177661896 = 0.0234615970402956 + 0.01 * 6.97144889831543
Epoch 500, val loss: 0.9221263527870178
Epoch 510, training loss: 0.09132932871580124 = 0.021681224927306175 + 0.01 * 6.964810371398926
Epoch 510, val loss: 0.935610830783844
Epoch 520, training loss: 0.08969543129205704 = 0.02009742707014084 + 0.01 * 6.959800720214844
Epoch 520, val loss: 0.9487406611442566
Epoch 530, training loss: 0.0882752314209938 = 0.018684016540646553 + 0.01 * 6.959122180938721
Epoch 530, val loss: 0.96144038438797
Epoch 540, training loss: 0.08694542199373245 = 0.01741698570549488 + 0.01 * 6.95284366607666
Epoch 540, val loss: 0.9737640619277954
Epoch 550, training loss: 0.0857684463262558 = 0.016277749091386795 + 0.01 * 6.949069976806641
Epoch 550, val loss: 0.985748827457428
Epoch 560, training loss: 0.08472506701946259 = 0.015250012278556824 + 0.01 * 6.947505474090576
Epoch 560, val loss: 0.9973742365837097
Epoch 570, training loss: 0.08371306210756302 = 0.014319819398224354 + 0.01 * 6.939324378967285
Epoch 570, val loss: 1.008628487586975
Epoch 580, training loss: 0.08295144885778427 = 0.0134751470759511 + 0.01 * 6.947630405426025
Epoch 580, val loss: 1.0195982456207275
Epoch 590, training loss: 0.08212412148714066 = 0.012707429938018322 + 0.01 * 6.941669940948486
Epoch 590, val loss: 1.0301727056503296
Epoch 600, training loss: 0.08129376173019409 = 0.012007336132228374 + 0.01 * 6.928643226623535
Epoch 600, val loss: 1.0404574871063232
Epoch 610, training loss: 0.08061089366674423 = 0.011366170831024647 + 0.01 * 6.924471855163574
Epoch 610, val loss: 1.0504978895187378
Epoch 620, training loss: 0.08002378046512604 = 0.010777509771287441 + 0.01 * 6.924627304077148
Epoch 620, val loss: 1.06024169921875
Epoch 630, training loss: 0.07943473011255264 = 0.010235818102955818 + 0.01 * 6.919891357421875
Epoch 630, val loss: 1.0697096586227417
Epoch 640, training loss: 0.0788557380437851 = 0.00973653607070446 + 0.01 * 6.911920070648193
Epoch 640, val loss: 1.0789631605148315
Epoch 650, training loss: 0.07846396416425705 = 0.009274311363697052 + 0.01 * 6.9189653396606445
Epoch 650, val loss: 1.0879716873168945
Epoch 660, training loss: 0.07799457758665085 = 0.008847853168845177 + 0.01 * 6.914672374725342
Epoch 660, val loss: 1.0967705249786377
Epoch 670, training loss: 0.07750522345304489 = 0.008452336303889751 + 0.01 * 6.905289173126221
Epoch 670, val loss: 1.1052947044372559
Epoch 680, training loss: 0.07704700529575348 = 0.008084485307335854 + 0.01 * 6.896252155303955
Epoch 680, val loss: 1.1136105060577393
Epoch 690, training loss: 0.07671593874692917 = 0.007742220535874367 + 0.01 * 6.897372245788574
Epoch 690, val loss: 1.1217753887176514
Epoch 700, training loss: 0.07627888023853302 = 0.0074234819039702415 + 0.01 * 6.885540008544922
Epoch 700, val loss: 1.1296442747116089
Epoch 710, training loss: 0.07608500868082047 = 0.007125317119061947 + 0.01 * 6.895969390869141
Epoch 710, val loss: 1.1373478174209595
Epoch 720, training loss: 0.07568584382534027 = 0.006847251672297716 + 0.01 * 6.883858680725098
Epoch 720, val loss: 1.1449408531188965
Epoch 730, training loss: 0.07538127899169922 = 0.006586677394807339 + 0.01 * 6.879459857940674
Epoch 730, val loss: 1.1521899700164795
Epoch 740, training loss: 0.07504789531230927 = 0.006342009175568819 + 0.01 * 6.870588779449463
Epoch 740, val loss: 1.1593531370162964
Epoch 750, training loss: 0.07499399781227112 = 0.006112628150731325 + 0.01 * 6.888137340545654
Epoch 750, val loss: 1.1664795875549316
Epoch 760, training loss: 0.07460930198431015 = 0.00589774688705802 + 0.01 * 6.871155738830566
Epoch 760, val loss: 1.173181176185608
Epoch 770, training loss: 0.07438801974058151 = 0.005695164669305086 + 0.01 * 6.869286060333252
Epoch 770, val loss: 1.1797916889190674
Epoch 780, training loss: 0.07404237985610962 = 0.005504146683961153 + 0.01 * 6.853823661804199
Epoch 780, val loss: 1.1861997842788696
Epoch 790, training loss: 0.0737726017832756 = 0.0053239548578858376 + 0.01 * 6.844864845275879
Epoch 790, val loss: 1.192556381225586
Epoch 800, training loss: 0.07358458638191223 = 0.005153893027454615 + 0.01 * 6.843069553375244
Epoch 800, val loss: 1.1986653804779053
Epoch 810, training loss: 0.07353551685810089 = 0.004992921371012926 + 0.01 * 6.854259014129639
Epoch 810, val loss: 1.204703688621521
Epoch 820, training loss: 0.07338578999042511 = 0.00484052486717701 + 0.01 * 6.854526519775391
Epoch 820, val loss: 1.2105565071105957
Epoch 830, training loss: 0.0729702040553093 = 0.004696274176239967 + 0.01 * 6.827393531799316
Epoch 830, val loss: 1.2162741422653198
Epoch 840, training loss: 0.07284332066774368 = 0.004559479653835297 + 0.01 * 6.8283843994140625
Epoch 840, val loss: 1.2218350172042847
Epoch 850, training loss: 0.07270916551351547 = 0.004429648630321026 + 0.01 * 6.827951908111572
Epoch 850, val loss: 1.227279782295227
Epoch 860, training loss: 0.07259698212146759 = 0.0043062204495072365 + 0.01 * 6.829076766967773
Epoch 860, val loss: 1.232641339302063
Epoch 870, training loss: 0.07250457257032394 = 0.0041884626261889935 + 0.01 * 6.831611156463623
Epoch 870, val loss: 1.2377880811691284
Epoch 880, training loss: 0.07225459069013596 = 0.004077398218214512 + 0.01 * 6.817719459533691
Epoch 880, val loss: 1.242937445640564
Epoch 890, training loss: 0.07197917252779007 = 0.003970532678067684 + 0.01 * 6.800864219665527
Epoch 890, val loss: 1.2478398084640503
Epoch 900, training loss: 0.07186771184206009 = 0.0038689570501446724 + 0.01 * 6.799875259399414
Epoch 900, val loss: 1.2527215480804443
Epoch 910, training loss: 0.07170351594686508 = 0.0037724170833826065 + 0.01 * 6.793109893798828
Epoch 910, val loss: 1.25753653049469
Epoch 920, training loss: 0.07175609469413757 = 0.00368048925884068 + 0.01 * 6.807560920715332
Epoch 920, val loss: 1.2621407508850098
Epoch 930, training loss: 0.07152845710515976 = 0.003591900458559394 + 0.01 * 6.7936553955078125
Epoch 930, val loss: 1.2666025161743164
Epoch 940, training loss: 0.0713248997926712 = 0.0035078253131359816 + 0.01 * 6.781707763671875
Epoch 940, val loss: 1.2711715698242188
Epoch 950, training loss: 0.07120339572429657 = 0.003427453339099884 + 0.01 * 6.777594566345215
Epoch 950, val loss: 1.2754355669021606
Epoch 960, training loss: 0.07100445032119751 = 0.0033500618301331997 + 0.01 * 6.765438556671143
Epoch 960, val loss: 1.2796539068222046
Epoch 970, training loss: 0.07105741649866104 = 0.0032758989837020636 + 0.01 * 6.778151988983154
Epoch 970, val loss: 1.2838664054870605
Epoch 980, training loss: 0.07084164768457413 = 0.003205309621989727 + 0.01 * 6.763634204864502
Epoch 980, val loss: 1.2880065441131592
Epoch 990, training loss: 0.07072354108095169 = 0.003137558465823531 + 0.01 * 6.758598327636719
Epoch 990, val loss: 1.2919878959655762
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 2.04199481010437 = 1.9560260772705078 + 0.01 * 8.596863746643066
Epoch 0, val loss: 1.9600549936294556
Epoch 10, training loss: 2.0317697525024414 = 1.9458014965057373 + 0.01 * 8.596820831298828
Epoch 10, val loss: 1.9505194425582886
Epoch 20, training loss: 2.019070863723755 = 1.9331045150756836 + 0.01 * 8.596643447875977
Epoch 20, val loss: 1.9383277893066406
Epoch 30, training loss: 2.0012195110321045 = 1.9152590036392212 + 0.01 * 8.59605598449707
Epoch 30, val loss: 1.920857310295105
Epoch 40, training loss: 1.974973201751709 = 1.8890421390533447 + 0.01 * 8.593109130859375
Epoch 40, val loss: 1.895323634147644
Epoch 50, training loss: 1.9379514455795288 = 1.8522121906280518 + 0.01 * 8.573930740356445
Epoch 50, val loss: 1.8606107234954834
Epoch 60, training loss: 1.8937939405441284 = 1.8088785409927368 + 0.01 * 8.491540908813477
Epoch 60, val loss: 1.8222943544387817
Epoch 70, training loss: 1.8515480756759644 = 1.7692512273788452 + 0.01 * 8.229689598083496
Epoch 70, val loss: 1.7876158952713013
Epoch 80, training loss: 1.8069475889205933 = 1.7255810499191284 + 0.01 * 8.1366548538208
Epoch 80, val loss: 1.7454445362091064
Epoch 90, training loss: 1.7450670003890991 = 1.665412187576294 + 0.01 * 7.965487003326416
Epoch 90, val loss: 1.6904429197311401
Epoch 100, training loss: 1.6639660596847534 = 1.5862996578216553 + 0.01 * 7.766643047332764
Epoch 100, val loss: 1.6219974756240845
Epoch 110, training loss: 1.5661265850067139 = 1.4907965660095215 + 0.01 * 7.533008098602295
Epoch 110, val loss: 1.541495680809021
Epoch 120, training loss: 1.461539387702942 = 1.3879671096801758 + 0.01 * 7.357232570648193
Epoch 120, val loss: 1.4546568393707275
Epoch 130, training loss: 1.3564602136611938 = 1.2834036350250244 + 0.01 * 7.305658340454102
Epoch 130, val loss: 1.3687443733215332
Epoch 140, training loss: 1.2512924671173096 = 1.1784695386886597 + 0.01 * 7.282297134399414
Epoch 140, val loss: 1.2840008735656738
Epoch 150, training loss: 1.1477680206298828 = 1.0750000476837158 + 0.01 * 7.276792526245117
Epoch 150, val loss: 1.2014933824539185
Epoch 160, training loss: 1.0482646226882935 = 0.975511908531189 + 0.01 * 7.275274276733398
Epoch 160, val loss: 1.1231802701950073
Epoch 170, training loss: 0.95386803150177 = 0.8811091780662537 + 0.01 * 7.275883197784424
Epoch 170, val loss: 1.0490269660949707
Epoch 180, training loss: 0.8644091486930847 = 0.7916436195373535 + 0.01 * 7.276554584503174
Epoch 180, val loss: 0.9785152673721313
Epoch 190, training loss: 0.7800772786140442 = 0.7073115110397339 + 0.01 * 7.276574611663818
Epoch 190, val loss: 0.9120191335678101
Epoch 200, training loss: 0.702134370803833 = 0.6293805837631226 + 0.01 * 7.275382041931152
Epoch 200, val loss: 0.8512938022613525
Epoch 210, training loss: 0.6318295001983643 = 0.5591027736663818 + 0.01 * 7.272674083709717
Epoch 210, val loss: 0.7985329031944275
Epoch 220, training loss: 0.5692786574363708 = 0.49659574031829834 + 0.01 * 7.26829195022583
Epoch 220, val loss: 0.7547978758811951
Epoch 230, training loss: 0.5136149525642395 = 0.4410008490085602 + 0.01 * 7.26140832901001
Epoch 230, val loss: 0.7197514772415161
Epoch 240, training loss: 0.463725209236145 = 0.3912114202976227 + 0.01 * 7.251377582550049
Epoch 240, val loss: 0.6922422647476196
Epoch 250, training loss: 0.4187237024307251 = 0.3463529050350189 + 0.01 * 7.237081050872803
Epoch 250, val loss: 0.6708812117576599
Epoch 260, training loss: 0.37814706563949585 = 0.30593422055244446 + 0.01 * 7.221282958984375
Epoch 260, val loss: 0.6543059349060059
Epoch 270, training loss: 0.34173277020454407 = 0.2697249948978424 + 0.01 * 7.200777053833008
Epoch 270, val loss: 0.641781210899353
Epoch 280, training loss: 0.30937880277633667 = 0.2374977469444275 + 0.01 * 7.188104152679443
Epoch 280, val loss: 0.6328676342964172
Epoch 290, training loss: 0.2807579040527344 = 0.20897988975048065 + 0.01 * 7.177802085876465
Epoch 290, val loss: 0.6272097229957581
Epoch 300, training loss: 0.25556302070617676 = 0.18383671343326569 + 0.01 * 7.172630786895752
Epoch 300, val loss: 0.6244408488273621
Epoch 310, training loss: 0.23338133096694946 = 0.1617051512002945 + 0.01 * 7.167618751525879
Epoch 310, val loss: 0.6242165565490723
Epoch 320, training loss: 0.2139020562171936 = 0.14226222038269043 + 0.01 * 7.16398286819458
Epoch 320, val loss: 0.626185953617096
Epoch 330, training loss: 0.1968107521533966 = 0.12521234154701233 + 0.01 * 7.159840106964111
Epoch 330, val loss: 0.6300630569458008
Epoch 340, training loss: 0.18185338377952576 = 0.11030246317386627 + 0.01 * 7.155093193054199
Epoch 340, val loss: 0.6355616450309753
Epoch 350, training loss: 0.16881564259529114 = 0.09729760885238647 + 0.01 * 7.151803970336914
Epoch 350, val loss: 0.6424265503883362
Epoch 360, training loss: 0.1574571579694748 = 0.08597658574581146 + 0.01 * 7.148057460784912
Epoch 360, val loss: 0.6504415273666382
Epoch 370, training loss: 0.14756427705287933 = 0.07613816857337952 + 0.01 * 7.142611026763916
Epoch 370, val loss: 0.6593548655509949
Epoch 380, training loss: 0.13898837566375732 = 0.0675944983959198 + 0.01 * 7.139388084411621
Epoch 380, val loss: 0.6689537167549133
Epoch 390, training loss: 0.13150116801261902 = 0.06018364802002907 + 0.01 * 7.131751537322998
Epoch 390, val loss: 0.6790785789489746
Epoch 400, training loss: 0.12501752376556396 = 0.0537530854344368 + 0.01 * 7.126442909240723
Epoch 400, val loss: 0.6895161867141724
Epoch 410, training loss: 0.11935025453567505 = 0.04816724732518196 + 0.01 * 7.118300914764404
Epoch 410, val loss: 0.7001798748970032
Epoch 420, training loss: 0.11442206054925919 = 0.043308988213539124 + 0.01 * 7.111307144165039
Epoch 420, val loss: 0.7109676003456116
Epoch 430, training loss: 0.11010847985744476 = 0.039078738540410995 + 0.01 * 7.1029744148254395
Epoch 430, val loss: 0.7217717170715332
Epoch 440, training loss: 0.1065373420715332 = 0.03538523614406586 + 0.01 * 7.11521053314209
Epoch 440, val loss: 0.7325240969657898
Epoch 450, training loss: 0.10303273797035217 = 0.03215639665722847 + 0.01 * 7.0876336097717285
Epoch 450, val loss: 0.7431291341781616
Epoch 460, training loss: 0.10002116113901138 = 0.02931983955204487 + 0.01 * 7.070132255554199
Epoch 460, val loss: 0.7536137700080872
Epoch 470, training loss: 0.09746450185775757 = 0.02681766077876091 + 0.01 * 7.06468391418457
Epoch 470, val loss: 0.7638940215110779
Epoch 480, training loss: 0.0952390506863594 = 0.024603696539998055 + 0.01 * 7.063535690307617
Epoch 480, val loss: 0.7739995121955872
Epoch 490, training loss: 0.09309209883213043 = 0.022639160975813866 + 0.01 * 7.045293807983398
Epoch 490, val loss: 0.7838629484176636
Epoch 500, training loss: 0.09118132293224335 = 0.02089337632060051 + 0.01 * 7.028794765472412
Epoch 500, val loss: 0.793538510799408
Epoch 510, training loss: 0.08952729403972626 = 0.019337892532348633 + 0.01 * 7.018940448760986
Epoch 510, val loss: 0.8029459118843079
Epoch 520, training loss: 0.08813121914863586 = 0.01794833317399025 + 0.01 * 7.018288612365723
Epoch 520, val loss: 0.8120877146720886
Epoch 530, training loss: 0.08665262162685394 = 0.016702156513929367 + 0.01 * 6.995046138763428
Epoch 530, val loss: 0.8210158348083496
Epoch 540, training loss: 0.08555594086647034 = 0.015579725615680218 + 0.01 * 6.997622013092041
Epoch 540, val loss: 0.8296951055526733
Epoch 550, training loss: 0.08443286269903183 = 0.014566464349627495 + 0.01 * 6.986639499664307
Epoch 550, val loss: 0.838151216506958
Epoch 560, training loss: 0.083341583609581 = 0.013649383559823036 + 0.01 * 6.969220161437988
Epoch 560, val loss: 0.8463184833526611
Epoch 570, training loss: 0.08252594619989395 = 0.012817285023629665 + 0.01 * 6.9708662033081055
Epoch 570, val loss: 0.854328453540802
Epoch 580, training loss: 0.08161956071853638 = 0.012060863897204399 + 0.01 * 6.955869674682617
Epoch 580, val loss: 0.8620480895042419
Epoch 590, training loss: 0.08091330528259277 = 0.011371197178959846 + 0.01 * 6.954211235046387
Epoch 590, val loss: 0.8695309162139893
Epoch 600, training loss: 0.08020614832639694 = 0.010741411708295345 + 0.01 * 6.946473598480225
Epoch 600, val loss: 0.8767930865287781
Epoch 610, training loss: 0.07974081486463547 = 0.010163857601583004 + 0.01 * 6.957695960998535
Epoch 610, val loss: 0.883842945098877
Epoch 620, training loss: 0.0790439248085022 = 0.009634210728108883 + 0.01 * 6.940971374511719
Epoch 620, val loss: 0.8906691670417786
Epoch 630, training loss: 0.07850271463394165 = 0.009146821685135365 + 0.01 * 6.93558931350708
Epoch 630, val loss: 0.897307276725769
Epoch 640, training loss: 0.07795848697423935 = 0.008696185424923897 + 0.01 * 6.926230430603027
Epoch 640, val loss: 0.9037408232688904
Epoch 650, training loss: 0.07755564898252487 = 0.008279712870717049 + 0.01 * 6.927594184875488
Epoch 650, val loss: 0.9100213050842285
Epoch 660, training loss: 0.07703500241041183 = 0.007893591187894344 + 0.01 * 6.9141411781311035
Epoch 660, val loss: 0.9161413311958313
Epoch 670, training loss: 0.07682083547115326 = 0.007534747943282127 + 0.01 * 6.928608417510986
Epoch 670, val loss: 0.9221163392066956
Epoch 680, training loss: 0.07624348253011703 = 0.007200819905847311 + 0.01 * 6.904266357421875
Epoch 680, val loss: 0.9279107451438904
Epoch 690, training loss: 0.07587724924087524 = 0.006889441050589085 + 0.01 * 6.8987812995910645
Epoch 690, val loss: 0.9336129426956177
Epoch 700, training loss: 0.07548486441373825 = 0.006598757114261389 + 0.01 * 6.888611316680908
Epoch 700, val loss: 0.9391999840736389
Epoch 710, training loss: 0.07535704970359802 = 0.006327647715806961 + 0.01 * 6.902940273284912
Epoch 710, val loss: 0.9445444941520691
Epoch 720, training loss: 0.07496319711208344 = 0.006073168944567442 + 0.01 * 6.889003276824951
Epoch 720, val loss: 0.9498510360717773
Epoch 730, training loss: 0.07479874044656754 = 0.005834610667079687 + 0.01 * 6.896413326263428
Epoch 730, val loss: 0.9550405740737915
Epoch 740, training loss: 0.07438012212514877 = 0.005610753316432238 + 0.01 * 6.876936912536621
Epoch 740, val loss: 0.9600880742073059
Epoch 750, training loss: 0.07410401105880737 = 0.005400217603892088 + 0.01 * 6.870379447937012
Epoch 750, val loss: 0.9650222063064575
Epoch 760, training loss: 0.07399282604455948 = 0.0052023110911250114 + 0.01 * 6.879051685333252
Epoch 760, val loss: 0.9699358940124512
Epoch 770, training loss: 0.07377514243125916 = 0.00501682423055172 + 0.01 * 6.8758320808410645
Epoch 770, val loss: 0.9746376276016235
Epoch 780, training loss: 0.0735253244638443 = 0.004842161666601896 + 0.01 * 6.868316650390625
Epoch 780, val loss: 0.979256272315979
Epoch 790, training loss: 0.0731954276561737 = 0.004677293356508017 + 0.01 * 6.851813316345215
Epoch 790, val loss: 0.9837444424629211
Epoch 800, training loss: 0.07320467382669449 = 0.004521308466792107 + 0.01 * 6.868336200714111
Epoch 800, val loss: 0.9881918430328369
Epoch 810, training loss: 0.07295326143503189 = 0.004373678471893072 + 0.01 * 6.8579583168029785
Epoch 810, val loss: 0.9925557374954224
Epoch 820, training loss: 0.07277483493089676 = 0.004233958199620247 + 0.01 * 6.854088306427002
Epoch 820, val loss: 0.9968588352203369
Epoch 830, training loss: 0.07267063856124878 = 0.004101815167814493 + 0.01 * 6.8568830490112305
Epoch 830, val loss: 1.001011610031128
Epoch 840, training loss: 0.07248984277248383 = 0.003976549953222275 + 0.01 * 6.8513288497924805
Epoch 840, val loss: 1.0051106214523315
Epoch 850, training loss: 0.07243503630161285 = 0.003857772098854184 + 0.01 * 6.85772705078125
Epoch 850, val loss: 1.0091124773025513
Epoch 860, training loss: 0.07213519513607025 = 0.0037453712429851294 + 0.01 * 6.838983058929443
Epoch 860, val loss: 1.012975811958313
Epoch 870, training loss: 0.072121761739254 = 0.0036384696140885353 + 0.01 * 6.848329544067383
Epoch 870, val loss: 1.0168180465698242
Epoch 880, training loss: 0.07196059823036194 = 0.0035367903765290976 + 0.01 * 6.842381000518799
Epoch 880, val loss: 1.0205423831939697
Epoch 890, training loss: 0.07168987393379211 = 0.003440192900598049 + 0.01 * 6.824967861175537
Epoch 890, val loss: 1.0241994857788086
Epoch 900, training loss: 0.0720556229352951 = 0.0033481589052826166 + 0.01 * 6.870746612548828
Epoch 900, val loss: 1.0277864933013916
Epoch 910, training loss: 0.07137357443571091 = 0.0032606227323412895 + 0.01 * 6.811295509338379
Epoch 910, val loss: 1.031266689300537
Epoch 920, training loss: 0.07143939286470413 = 0.003177026752382517 + 0.01 * 6.826237201690674
Epoch 920, val loss: 1.0347133874893188
Epoch 930, training loss: 0.07118390500545502 = 0.003097409848123789 + 0.01 * 6.808650016784668
Epoch 930, val loss: 1.0380358695983887
Epoch 940, training loss: 0.0711166262626648 = 0.003021417185664177 + 0.01 * 6.809521198272705
Epoch 940, val loss: 1.0414320230484009
Epoch 950, training loss: 0.07106508314609528 = 0.0029489484149962664 + 0.01 * 6.8116135597229
Epoch 950, val loss: 1.0445640087127686
Epoch 960, training loss: 0.07088662683963776 = 0.002879657316952944 + 0.01 * 6.800697326660156
Epoch 960, val loss: 1.0476809740066528
Epoch 970, training loss: 0.0709155723452568 = 0.002813340863212943 + 0.01 * 6.810223579406738
Epoch 970, val loss: 1.0507869720458984
Epoch 980, training loss: 0.07085970044136047 = 0.002749786013737321 + 0.01 * 6.8109917640686035
Epoch 980, val loss: 1.0537769794464111
Epoch 990, training loss: 0.07066430896520615 = 0.002689243294298649 + 0.01 * 6.797506809234619
Epoch 990, val loss: 1.0566900968551636
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8423827095413812
The final CL Acc:0.81975, 0.01145, The final GNN Acc:0.83729, 0.00366
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10596])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.020982503890991 = 1.9350143671035767 + 0.01 * 8.596819877624512
Epoch 0, val loss: 1.932774543762207
Epoch 10, training loss: 2.011533260345459 = 1.925565481185913 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.9238039255142212
Epoch 20, training loss: 2.000170946121216 = 1.9142050743103027 + 0.01 * 8.596593856811523
Epoch 20, val loss: 1.9126858711242676
Epoch 30, training loss: 1.9843635559082031 = 1.8984036445617676 + 0.01 * 8.59599494934082
Epoch 30, val loss: 1.8970314264297485
Epoch 40, training loss: 1.9613038301467896 = 1.8753747940063477 + 0.01 * 8.592901229858398
Epoch 40, val loss: 1.8744382858276367
Epoch 50, training loss: 1.9296534061431885 = 1.8439586162567139 + 0.01 * 8.569479942321777
Epoch 50, val loss: 1.8451722860336304
Epoch 60, training loss: 1.8951233625411987 = 1.8105976581573486 + 0.01 * 8.452573776245117
Epoch 60, val loss: 1.8180469274520874
Epoch 70, training loss: 1.8643653392791748 = 1.7832423448562622 + 0.01 * 8.112298965454102
Epoch 70, val loss: 1.7977519035339355
Epoch 80, training loss: 1.8273688554763794 = 1.7481609582901 + 0.01 * 7.920794486999512
Epoch 80, val loss: 1.7663557529449463
Epoch 90, training loss: 1.7752825021743774 = 1.6986695528030396 + 0.01 * 7.661292552947998
Epoch 90, val loss: 1.7224618196487427
Epoch 100, training loss: 1.7049593925476074 = 1.6298154592514038 + 0.01 * 7.514395713806152
Epoch 100, val loss: 1.6642321348190308
Epoch 110, training loss: 1.6174139976501465 = 1.5425424575805664 + 0.01 * 7.487155914306641
Epoch 110, val loss: 1.5929200649261475
Epoch 120, training loss: 1.5192395448684692 = 1.4446042776107788 + 0.01 * 7.463529586791992
Epoch 120, val loss: 1.5150730609893799
Epoch 130, training loss: 1.4174888134002686 = 1.3431297540664673 + 0.01 * 7.435911655426025
Epoch 130, val loss: 1.436317801475525
Epoch 140, training loss: 1.3134446144104004 = 1.2395974397659302 + 0.01 * 7.3847222328186035
Epoch 140, val loss: 1.357125997543335
Epoch 150, training loss: 1.2083513736724854 = 1.1353967189788818 + 0.01 * 7.295464038848877
Epoch 150, val loss: 1.2786469459533691
Epoch 160, training loss: 1.1062365770339966 = 1.0342051982879639 + 0.01 * 7.203136920928955
Epoch 160, val loss: 1.2051118612289429
Epoch 170, training loss: 1.011281132698059 = 0.9394562840461731 + 0.01 * 7.182481288909912
Epoch 170, val loss: 1.139548420906067
Epoch 180, training loss: 0.9242395162582397 = 0.8526121973991394 + 0.01 * 7.162729263305664
Epoch 180, val loss: 1.0823254585266113
Epoch 190, training loss: 0.8452206254005432 = 0.7737946510314941 + 0.01 * 7.14259672164917
Epoch 190, val loss: 1.0333034992218018
Epoch 200, training loss: 0.7740488052368164 = 0.7028508186340332 + 0.01 * 7.1197967529296875
Epoch 200, val loss: 0.9924249649047852
Epoch 210, training loss: 0.7101835608482361 = 0.6391082406044006 + 0.01 * 7.1075334548950195
Epoch 210, val loss: 0.9600715041160583
Epoch 220, training loss: 0.6529626250267029 = 0.5820996165275574 + 0.01 * 7.086300849914551
Epoch 220, val loss: 0.9359686970710754
Epoch 230, training loss: 0.6020417213439941 = 0.531287670135498 + 0.01 * 7.07540225982666
Epoch 230, val loss: 0.9194252490997314
Epoch 240, training loss: 0.5565704703330994 = 0.48588672280311584 + 0.01 * 7.068375110626221
Epoch 240, val loss: 0.9090282917022705
Epoch 250, training loss: 0.5155466794967651 = 0.44490283727645874 + 0.01 * 7.064383506774902
Epoch 250, val loss: 0.90370774269104
Epoch 260, training loss: 0.47805288434028625 = 0.40744298696517944 + 0.01 * 7.060990810394287
Epoch 260, val loss: 0.9028116464614868
Epoch 270, training loss: 0.44342976808547974 = 0.3728458285331726 + 0.01 * 7.058395862579346
Epoch 270, val loss: 0.9055268168449402
Epoch 280, training loss: 0.4111785888671875 = 0.34061673283576965 + 0.01 * 7.056186199188232
Epoch 280, val loss: 0.9110696315765381
Epoch 290, training loss: 0.3808702230453491 = 0.3103293478488922 + 0.01 * 7.054088592529297
Epoch 290, val loss: 0.9187543988227844
Epoch 300, training loss: 0.3519994020462036 = 0.2814750373363495 + 0.01 * 7.052436351776123
Epoch 300, val loss: 0.9280217289924622
Epoch 310, training loss: 0.32392433285713196 = 0.25340399146080017 + 0.01 * 7.052035331726074
Epoch 310, val loss: 0.9382718205451965
Epoch 320, training loss: 0.296011358499527 = 0.22549550235271454 + 0.01 * 7.0515851974487305
Epoch 320, val loss: 0.9492883682250977
Epoch 330, training loss: 0.2681623697280884 = 0.1976555436849594 + 0.01 * 7.0506815910339355
Epoch 330, val loss: 0.9612850546836853
Epoch 340, training loss: 0.24113965034484863 = 0.17063170671463013 + 0.01 * 7.05079460144043
Epoch 340, val loss: 0.9748736619949341
Epoch 350, training loss: 0.2161082625389099 = 0.14561043679714203 + 0.01 * 7.0497822761535645
Epoch 350, val loss: 0.9904872179031372
Epoch 360, training loss: 0.19402548670768738 = 0.12352054566144943 + 0.01 * 7.050493240356445
Epoch 360, val loss: 1.0080727338790894
Epoch 370, training loss: 0.17521092295646667 = 0.10471949726343155 + 0.01 * 7.049143314361572
Epoch 370, val loss: 1.0273208618164062
Epoch 380, training loss: 0.1595582365989685 = 0.08907564729452133 + 0.01 * 7.048259735107422
Epoch 380, val loss: 1.047906517982483
Epoch 390, training loss: 0.14666922390460968 = 0.0761944055557251 + 0.01 * 7.047482013702393
Epoch 390, val loss: 1.0693764686584473
Epoch 400, training loss: 0.1361021250486374 = 0.06562785804271698 + 0.01 * 7.047426700592041
Epoch 400, val loss: 1.0911521911621094
Epoch 410, training loss: 0.12742194533348083 = 0.05696175992488861 + 0.01 * 7.046018123626709
Epoch 410, val loss: 1.1128233671188354
Epoch 420, training loss: 0.1202818900346756 = 0.049830857664346695 + 0.01 * 7.045103549957275
Epoch 420, val loss: 1.1341803073883057
Epoch 430, training loss: 0.114376500248909 = 0.04393353313207626 + 0.01 * 7.044297218322754
Epoch 430, val loss: 1.155055284500122
Epoch 440, training loss: 0.10945719480514526 = 0.03902082517743111 + 0.01 * 7.043637275695801
Epoch 440, val loss: 1.1753590106964111
Epoch 450, training loss: 0.10531622171401978 = 0.034892737865448 + 0.01 * 7.042348384857178
Epoch 450, val loss: 1.1950551271438599
Epoch 460, training loss: 0.10180360078811646 = 0.031396567821502686 + 0.01 * 7.040703296661377
Epoch 460, val loss: 1.214038610458374
Epoch 470, training loss: 0.09881077706813812 = 0.028408382087945938 + 0.01 * 7.040240287780762
Epoch 470, val loss: 1.2323557138442993
Epoch 480, training loss: 0.09621407091617584 = 0.025835033506155014 + 0.01 * 7.037904262542725
Epoch 480, val loss: 1.2499901056289673
Epoch 490, training loss: 0.09396519511938095 = 0.023603148758411407 + 0.01 * 7.036204814910889
Epoch 490, val loss: 1.2669609785079956
Epoch 500, training loss: 0.09200737625360489 = 0.021654928103089333 + 0.01 * 7.035244941711426
Epoch 500, val loss: 1.2832951545715332
Epoch 510, training loss: 0.09031475335359573 = 0.019945422187447548 + 0.01 * 7.036933422088623
Epoch 510, val loss: 1.299010157585144
Epoch 520, training loss: 0.08876803517341614 = 0.018437175080180168 + 0.01 * 7.03308629989624
Epoch 520, val loss: 1.314159870147705
Epoch 530, training loss: 0.08739524334669113 = 0.017098041251301765 + 0.01 * 7.029720783233643
Epoch 530, val loss: 1.3288137912750244
Epoch 540, training loss: 0.08617843687534332 = 0.0158988144248724 + 0.01 * 7.0279622077941895
Epoch 540, val loss: 1.3430975675582886
Epoch 550, training loss: 0.08509372174739838 = 0.014815788716077805 + 0.01 * 7.027793884277344
Epoch 550, val loss: 1.357188105583191
Epoch 560, training loss: 0.08408036828041077 = 0.013833696022629738 + 0.01 * 7.024667739868164
Epoch 560, val loss: 1.3711203336715698
Epoch 570, training loss: 0.08316634595394135 = 0.012941505759954453 + 0.01 * 7.022483825683594
Epoch 570, val loss: 1.3848645687103271
Epoch 580, training loss: 0.0823572427034378 = 0.012130066752433777 + 0.01 * 7.022717475891113
Epoch 580, val loss: 1.3983463048934937
Epoch 590, training loss: 0.08158004283905029 = 0.01139139849692583 + 0.01 * 7.018864631652832
Epoch 590, val loss: 1.411550760269165
Epoch 600, training loss: 0.0808774083852768 = 0.010718232952058315 + 0.01 * 7.015917778015137
Epoch 600, val loss: 1.4244464635849
Epoch 610, training loss: 0.08024512976408005 = 0.010103615000844002 + 0.01 * 7.014151573181152
Epoch 610, val loss: 1.437051773071289
Epoch 620, training loss: 0.07973887771368027 = 0.009541519917547703 + 0.01 * 7.019735813140869
Epoch 620, val loss: 1.449333906173706
Epoch 630, training loss: 0.07912229746580124 = 0.009026161395013332 + 0.01 * 7.009613513946533
Epoch 630, val loss: 1.461287498474121
Epoch 640, training loss: 0.07867657393217087 = 0.00855217780917883 + 0.01 * 7.012439727783203
Epoch 640, val loss: 1.4729517698287964
Epoch 650, training loss: 0.07817577570676804 = 0.008114414289593697 + 0.01 * 7.006136417388916
Epoch 650, val loss: 1.484281301498413
Epoch 660, training loss: 0.07773423194885254 = 0.007708150893449783 + 0.01 * 7.002608776092529
Epoch 660, val loss: 1.4953372478485107
Epoch 670, training loss: 0.07735458761453629 = 0.007330046035349369 + 0.01 * 7.00245475769043
Epoch 670, val loss: 1.5060830116271973
Epoch 680, training loss: 0.0769575983285904 = 0.006978337652981281 + 0.01 * 6.997925758361816
Epoch 680, val loss: 1.5165456533432007
Epoch 690, training loss: 0.07657388597726822 = 0.006650294177234173 + 0.01 * 6.992359638214111
Epoch 690, val loss: 1.5267199277877808
Epoch 700, training loss: 0.07636399567127228 = 0.006345741450786591 + 0.01 * 7.00182580947876
Epoch 700, val loss: 1.5365970134735107
Epoch 710, training loss: 0.07592480629682541 = 0.006063153501600027 + 0.01 * 6.9861650466918945
Epoch 710, val loss: 1.5461400747299194
Epoch 720, training loss: 0.07562817633152008 = 0.005799697246402502 + 0.01 * 6.982848167419434
Epoch 720, val loss: 1.5554801225662231
Epoch 730, training loss: 0.07536128908395767 = 0.005553302355110645 + 0.01 * 6.980798721313477
Epoch 730, val loss: 1.5646047592163086
Epoch 740, training loss: 0.0750887542963028 = 0.005322950892150402 + 0.01 * 6.97658109664917
Epoch 740, val loss: 1.5734741687774658
Epoch 750, training loss: 0.07497601211071014 = 0.005107582081109285 + 0.01 * 6.986843109130859
Epoch 750, val loss: 1.5820951461791992
Epoch 760, training loss: 0.07466594129800797 = 0.004906249698251486 + 0.01 * 6.975968837738037
Epoch 760, val loss: 1.5904383659362793
Epoch 770, training loss: 0.07434193789958954 = 0.004717191681265831 + 0.01 * 6.962474822998047
Epoch 770, val loss: 1.598617672920227
Epoch 780, training loss: 0.07432504743337631 = 0.004539707209914923 + 0.01 * 6.97853422164917
Epoch 780, val loss: 1.6066035032272339
Epoch 790, training loss: 0.07406199723482132 = 0.004374225623905659 + 0.01 * 6.968777179718018
Epoch 790, val loss: 1.6142386198043823
Epoch 800, training loss: 0.07380040735006332 = 0.004219412803649902 + 0.01 * 6.958099365234375
Epoch 800, val loss: 1.62168288230896
Epoch 810, training loss: 0.07353068888187408 = 0.004073807038366795 + 0.01 * 6.945688247680664
Epoch 810, val loss: 1.6290018558502197
Epoch 820, training loss: 0.0738895982503891 = 0.003936799243092537 + 0.01 * 6.9952802658081055
Epoch 820, val loss: 1.6361483335494995
Epoch 830, training loss: 0.07313176244497299 = 0.0038079163059592247 + 0.01 * 6.932384967803955
Epoch 830, val loss: 1.6430009603500366
Epoch 840, training loss: 0.073123499751091 = 0.003686638316139579 + 0.01 * 6.943686485290527
Epoch 840, val loss: 1.6496508121490479
Epoch 850, training loss: 0.07297152280807495 = 0.003572744783014059 + 0.01 * 6.939877986907959
Epoch 850, val loss: 1.65601646900177
Epoch 860, training loss: 0.07264197617769241 = 0.003465308342128992 + 0.01 * 6.917666435241699
Epoch 860, val loss: 1.6622508764266968
Epoch 870, training loss: 0.07253969460725784 = 0.0033638044260442257 + 0.01 * 6.91758918762207
Epoch 870, val loss: 1.6683589220046997
Epoch 880, training loss: 0.07311217486858368 = 0.0032678062561899424 + 0.01 * 6.984436988830566
Epoch 880, val loss: 1.6742455959320068
Epoch 890, training loss: 0.07214576005935669 = 0.0031773934606462717 + 0.01 * 6.896836280822754
Epoch 890, val loss: 1.679802417755127
Epoch 900, training loss: 0.0720510333776474 = 0.0030919350683689117 + 0.01 * 6.895910263061523
Epoch 900, val loss: 1.6851975917816162
Epoch 910, training loss: 0.07221190631389618 = 0.0030108969658613205 + 0.01 * 6.920100688934326
Epoch 910, val loss: 1.6906121969223022
Epoch 920, training loss: 0.0716712474822998 = 0.0029342668130993843 + 0.01 * 6.8736982345581055
Epoch 920, val loss: 1.6956862211227417
Epoch 930, training loss: 0.07156451791524887 = 0.002861686749383807 + 0.01 * 6.870283126831055
Epoch 930, val loss: 1.7006264925003052
Epoch 940, training loss: 0.0713832899928093 = 0.0027929109055548906 + 0.01 * 6.85903787612915
Epoch 940, val loss: 1.705380916595459
Epoch 950, training loss: 0.07148176431655884 = 0.0027274983003735542 + 0.01 * 6.875426292419434
Epoch 950, val loss: 1.7099815607070923
Epoch 960, training loss: 0.07122822850942612 = 0.0026653495151549578 + 0.01 * 6.856287956237793
Epoch 960, val loss: 1.7144426107406616
Epoch 970, training loss: 0.07108692079782486 = 0.002606032183393836 + 0.01 * 6.84808874130249
Epoch 970, val loss: 1.7188019752502441
Epoch 980, training loss: 0.07089819014072418 = 0.0025495493318885565 + 0.01 * 6.834864139556885
Epoch 980, val loss: 1.7230116128921509
Epoch 990, training loss: 0.07126695662736893 = 0.002495566150173545 + 0.01 * 6.877139091491699
Epoch 990, val loss: 1.7271136045455933
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 2.0354580879211426 = 1.9494894742965698 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.9551335573196411
Epoch 10, training loss: 2.025224447250366 = 1.9392565488815308 + 0.01 * 8.596778869628906
Epoch 10, val loss: 1.945104956626892
Epoch 20, training loss: 2.012465238571167 = 1.9264994859695435 + 0.01 * 8.596564292907715
Epoch 20, val loss: 1.932052731513977
Epoch 30, training loss: 1.994816780090332 = 1.9088581800460815 + 0.01 * 8.595854759216309
Epoch 30, val loss: 1.9134858846664429
Epoch 40, training loss: 1.969283938407898 = 1.8833646774291992 + 0.01 * 8.591920852661133
Epoch 40, val loss: 1.8866989612579346
Epoch 50, training loss: 1.9342442750930786 = 1.8485772609710693 + 0.01 * 8.566699981689453
Epoch 50, val loss: 1.8514763116836548
Epoch 60, training loss: 1.8950978517532349 = 1.8105124235153198 + 0.01 * 8.458540916442871
Epoch 60, val loss: 1.8163279294967651
Epoch 70, training loss: 1.8613070249557495 = 1.7794245481491089 + 0.01 * 8.188244819641113
Epoch 70, val loss: 1.7902798652648926
Epoch 80, training loss: 1.8237993717193604 = 1.743048071861267 + 0.01 * 8.0751314163208
Epoch 80, val loss: 1.7576956748962402
Epoch 90, training loss: 1.7712894678115845 = 1.6922335624694824 + 0.01 * 7.905594825744629
Epoch 90, val loss: 1.7127007246017456
Epoch 100, training loss: 1.699952483177185 = 1.6227623224258423 + 0.01 * 7.719016075134277
Epoch 100, val loss: 1.6530451774597168
Epoch 110, training loss: 1.6130598783493042 = 1.5384063720703125 + 0.01 * 7.465351581573486
Epoch 110, val loss: 1.5819858312606812
Epoch 120, training loss: 1.5209681987762451 = 1.4476991891860962 + 0.01 * 7.326901435852051
Epoch 120, val loss: 1.5078024864196777
Epoch 130, training loss: 1.4290738105773926 = 1.3561347723007202 + 0.01 * 7.293898105621338
Epoch 130, val loss: 1.435240626335144
Epoch 140, training loss: 1.3363929986953735 = 1.2637417316436768 + 0.01 * 7.265126705169678
Epoch 140, val loss: 1.3651669025421143
Epoch 150, training loss: 1.2424345016479492 = 1.1698665618896484 + 0.01 * 7.2567925453186035
Epoch 150, val loss: 1.2952603101730347
Epoch 160, training loss: 1.1474506855010986 = 1.0749269723892212 + 0.01 * 7.2523698806762695
Epoch 160, val loss: 1.2260479927062988
Epoch 170, training loss: 1.0536448955535889 = 0.981179416179657 + 0.01 * 7.246548652648926
Epoch 170, val loss: 1.158035397529602
Epoch 180, training loss: 0.9641596078872681 = 0.891772985458374 + 0.01 * 7.238665580749512
Epoch 180, val loss: 1.0927789211273193
Epoch 190, training loss: 0.8822281956672668 = 0.8099808096885681 + 0.01 * 7.224740982055664
Epoch 190, val loss: 1.0331634283065796
Epoch 200, training loss: 0.8095129728317261 = 0.7375044226646423 + 0.01 * 7.200855731964111
Epoch 200, val loss: 0.981865644454956
Epoch 210, training loss: 0.745419979095459 = 0.6737587451934814 + 0.01 * 7.166126728057861
Epoch 210, val loss: 0.9391462206840515
Epoch 220, training loss: 0.6880173087120056 = 0.616657018661499 + 0.01 * 7.136031150817871
Epoch 220, val loss: 0.9043300151824951
Epoch 230, training loss: 0.6351312398910522 = 0.5640596747398376 + 0.01 * 7.107158660888672
Epoch 230, val loss: 0.876212477684021
Epoch 240, training loss: 0.5856969356536865 = 0.5147851705551147 + 0.01 * 7.091175079345703
Epoch 240, val loss: 0.8536105155944824
Epoch 250, training loss: 0.539340615272522 = 0.4686223864555359 + 0.01 * 7.071822643280029
Epoch 250, val loss: 0.8358312249183655
Epoch 260, training loss: 0.49653899669647217 = 0.4259236752986908 + 0.01 * 7.061530590057373
Epoch 260, val loss: 0.8226036429405212
Epoch 270, training loss: 0.4576281011104584 = 0.38716763257980347 + 0.01 * 7.046046733856201
Epoch 270, val loss: 0.8134749531745911
Epoch 280, training loss: 0.42290329933166504 = 0.3525483012199402 + 0.01 * 7.0355000495910645
Epoch 280, val loss: 0.8078863620758057
Epoch 290, training loss: 0.392112135887146 = 0.32184287905693054 + 0.01 * 7.026927471160889
Epoch 290, val loss: 0.8052467107772827
Epoch 300, training loss: 0.3646191656589508 = 0.29442378878593445 + 0.01 * 7.019537448883057
Epoch 300, val loss: 0.8049299716949463
Epoch 310, training loss: 0.3395664691925049 = 0.2694557309150696 + 0.01 * 7.011074542999268
Epoch 310, val loss: 0.8064447641372681
Epoch 320, training loss: 0.31614887714385986 = 0.24604614078998566 + 0.01 * 7.010275363922119
Epoch 320, val loss: 0.809264063835144
Epoch 330, training loss: 0.2933928370475769 = 0.22335045039653778 + 0.01 * 7.004238605499268
Epoch 330, val loss: 0.8128790855407715
Epoch 340, training loss: 0.2708066403865814 = 0.20080208778381348 + 0.01 * 7.000455856323242
Epoch 340, val loss: 0.8168696761131287
Epoch 350, training loss: 0.24836263060569763 = 0.17838944494724274 + 0.01 * 6.997319221496582
Epoch 350, val loss: 0.8212486505508423
Epoch 360, training loss: 0.22684499621391296 = 0.1567552536725998 + 0.01 * 7.008973598480225
Epoch 360, val loss: 0.8263089060783386
Epoch 370, training loss: 0.20676153898239136 = 0.13679489493370056 + 0.01 * 6.996664047241211
Epoch 370, val loss: 0.8326058983802795
Epoch 380, training loss: 0.18903237581253052 = 0.11910708993673325 + 0.01 * 6.992527961730957
Epoch 380, val loss: 0.8403854370117188
Epoch 390, training loss: 0.17371852695941925 = 0.10383947193622589 + 0.01 * 6.987905502319336
Epoch 390, val loss: 0.8496244549751282
Epoch 400, training loss: 0.1606471836566925 = 0.09079235047101974 + 0.01 * 6.985482692718506
Epoch 400, val loss: 0.860210657119751
Epoch 410, training loss: 0.1495516002178192 = 0.0796666070818901 + 0.01 * 6.988498687744141
Epoch 410, val loss: 0.87204509973526
Epoch 420, training loss: 0.14000917971134186 = 0.0701739639043808 + 0.01 * 6.983521461486816
Epoch 420, val loss: 0.8849174976348877
Epoch 430, training loss: 0.13185010850429535 = 0.062058910727500916 + 0.01 * 6.979119777679443
Epoch 430, val loss: 0.8986694812774658
Epoch 440, training loss: 0.1248617172241211 = 0.055106762796640396 + 0.01 * 6.9754958152771
Epoch 440, val loss: 0.9130544066429138
Epoch 450, training loss: 0.11886097490787506 = 0.049135830253362656 + 0.01 * 6.972514629364014
Epoch 450, val loss: 0.9279580116271973
Epoch 460, training loss: 0.11369995027780533 = 0.044001758098602295 + 0.01 * 6.96981954574585
Epoch 460, val loss: 0.9430907368659973
Epoch 470, training loss: 0.10926200449466705 = 0.039577387273311615 + 0.01 * 6.968461513519287
Epoch 470, val loss: 0.9583582282066345
Epoch 480, training loss: 0.10537969321012497 = 0.03575021028518677 + 0.01 * 6.962948799133301
Epoch 480, val loss: 0.9735451936721802
Epoch 490, training loss: 0.10204634815454483 = 0.03242611140012741 + 0.01 * 6.962023735046387
Epoch 490, val loss: 0.9886049628257751
Epoch 500, training loss: 0.09910044819116592 = 0.02952844463288784 + 0.01 * 6.957200050354004
Epoch 500, val loss: 1.0034291744232178
Epoch 510, training loss: 0.09654447436332703 = 0.02699103206396103 + 0.01 * 6.955344200134277
Epoch 510, val loss: 1.017967700958252
Epoch 520, training loss: 0.09428109228610992 = 0.024757951498031616 + 0.01 * 6.952314376831055
Epoch 520, val loss: 1.0321670770645142
Epoch 530, training loss: 0.09225478768348694 = 0.022785820066928864 + 0.01 * 6.946897029876709
Epoch 530, val loss: 1.0460394620895386
Epoch 540, training loss: 0.09056500345468521 = 0.021036172285676003 + 0.01 * 6.952883243560791
Epoch 540, val loss: 1.0595577955245972
Epoch 550, training loss: 0.08886506408452988 = 0.01948176883161068 + 0.01 * 6.938329219818115
Epoch 550, val loss: 1.0726516246795654
Epoch 560, training loss: 0.08742476999759674 = 0.018093081191182137 + 0.01 * 6.933169364929199
Epoch 560, val loss: 1.0854099988937378
Epoch 570, training loss: 0.08636555820703506 = 0.01684815250337124 + 0.01 * 6.951740741729736
Epoch 570, val loss: 1.097786784172058
Epoch 580, training loss: 0.08501181751489639 = 0.01573021151125431 + 0.01 * 6.928161144256592
Epoch 580, val loss: 1.1097344160079956
Epoch 590, training loss: 0.0839681476354599 = 0.014721186831593513 + 0.01 * 6.924696445465088
Epoch 590, val loss: 1.1214070320129395
Epoch 600, training loss: 0.083006851375103 = 0.01380719430744648 + 0.01 * 6.919966220855713
Epoch 600, val loss: 1.1326923370361328
Epoch 610, training loss: 0.08213692903518677 = 0.012977463193237782 + 0.01 * 6.9159464836120605
Epoch 610, val loss: 1.1436705589294434
Epoch 620, training loss: 0.08175808936357498 = 0.01222218293696642 + 0.01 * 6.9535908699035645
Epoch 620, val loss: 1.1542974710464478
Epoch 630, training loss: 0.08070269227027893 = 0.011535181663930416 + 0.01 * 6.916751384735107
Epoch 630, val loss: 1.1645811796188354
Epoch 640, training loss: 0.07997798919677734 = 0.010907131247222424 + 0.01 * 6.907086372375488
Epoch 640, val loss: 1.1745562553405762
Epoch 650, training loss: 0.07929906249046326 = 0.010331018827855587 + 0.01 * 6.896804332733154
Epoch 650, val loss: 1.1842771768569946
Epoch 660, training loss: 0.07880357652902603 = 0.009801102802157402 + 0.01 * 6.900247573852539
Epoch 660, val loss: 1.1936293840408325
Epoch 670, training loss: 0.0782395452260971 = 0.009313664399087429 + 0.01 * 6.892588138580322
Epoch 670, val loss: 1.2027068138122559
Epoch 680, training loss: 0.07773122936487198 = 0.008864062838256359 + 0.01 * 6.886717319488525
Epoch 680, val loss: 1.2115262746810913
Epoch 690, training loss: 0.0773070678114891 = 0.008448218926787376 + 0.01 * 6.885884761810303
Epoch 690, val loss: 1.2200771570205688
Epoch 700, training loss: 0.07681673020124435 = 0.008063209243118763 + 0.01 * 6.875351905822754
Epoch 700, val loss: 1.2284272909164429
Epoch 710, training loss: 0.07646897435188293 = 0.0077063823118805885 + 0.01 * 6.876259803771973
Epoch 710, val loss: 1.2365102767944336
Epoch 720, training loss: 0.07626992464065552 = 0.007374571170657873 + 0.01 * 6.889535903930664
Epoch 720, val loss: 1.2442883253097534
Epoch 730, training loss: 0.07572384178638458 = 0.007066136226058006 + 0.01 * 6.8657708168029785
Epoch 730, val loss: 1.2519171237945557
Epoch 740, training loss: 0.07540947198867798 = 0.006778766866773367 + 0.01 * 6.863070487976074
Epoch 740, val loss: 1.2592871189117432
Epoch 750, training loss: 0.07516710460186005 = 0.006510045379400253 + 0.01 * 6.865706443786621
Epoch 750, val loss: 1.266522765159607
Epoch 760, training loss: 0.07488833367824554 = 0.0062592956237494946 + 0.01 * 6.862904071807861
Epoch 760, val loss: 1.2734777927398682
Epoch 770, training loss: 0.0745532363653183 = 0.006024564150720835 + 0.01 * 6.852867603302002
Epoch 770, val loss: 1.2801717519760132
Epoch 780, training loss: 0.07425019890069962 = 0.00580392824485898 + 0.01 * 6.844627380371094
Epoch 780, val loss: 1.2868562936782837
Epoch 790, training loss: 0.07400595396757126 = 0.005596387665718794 + 0.01 * 6.840957164764404
Epoch 790, val loss: 1.293226718902588
Epoch 800, training loss: 0.07383140921592712 = 0.0054008327424526215 + 0.01 * 6.843057632446289
Epoch 800, val loss: 1.2994884252548218
Epoch 810, training loss: 0.07354704290628433 = 0.005216980818659067 + 0.01 * 6.833006858825684
Epoch 810, val loss: 1.3056375980377197
Epoch 820, training loss: 0.07333829998970032 = 0.005043727811425924 + 0.01 * 6.829456806182861
Epoch 820, val loss: 1.3115042448043823
Epoch 830, training loss: 0.07323150336742401 = 0.00488036684691906 + 0.01 * 6.835113525390625
Epoch 830, val loss: 1.3173233270645142
Epoch 840, training loss: 0.07296175509691238 = 0.004725784529000521 + 0.01 * 6.823597431182861
Epoch 840, val loss: 1.3229278326034546
Epoch 850, training loss: 0.07275008410215378 = 0.0045796469785273075 + 0.01 * 6.817043781280518
Epoch 850, val loss: 1.3283838033676147
Epoch 860, training loss: 0.0726306140422821 = 0.004441262222826481 + 0.01 * 6.818935394287109
Epoch 860, val loss: 1.3336706161499023
Epoch 870, training loss: 0.07263779640197754 = 0.004309885203838348 + 0.01 * 6.832791328430176
Epoch 870, val loss: 1.3388724327087402
Epoch 880, training loss: 0.07229767739772797 = 0.004185167606920004 + 0.01 * 6.811250686645508
Epoch 880, val loss: 1.343971848487854
Epoch 890, training loss: 0.07217273861169815 = 0.004066494759172201 + 0.01 * 6.810624599456787
Epoch 890, val loss: 1.3488811254501343
Epoch 900, training loss: 0.07210846990346909 = 0.003953847568482161 + 0.01 * 6.815462589263916
Epoch 900, val loss: 1.3537538051605225
Epoch 910, training loss: 0.07192452251911163 = 0.0038469829596579075 + 0.01 * 6.807754039764404
Epoch 910, val loss: 1.3583356142044067
Epoch 920, training loss: 0.07173793762922287 = 0.0037449803203344345 + 0.01 * 6.799295902252197
Epoch 920, val loss: 1.3629748821258545
Epoch 930, training loss: 0.07164289057254791 = 0.0036477858666330576 + 0.01 * 6.799510478973389
Epoch 930, val loss: 1.3673863410949707
Epoch 940, training loss: 0.07148734480142593 = 0.0035551574546843767 + 0.01 * 6.793219089508057
Epoch 940, val loss: 1.371708631515503
Epoch 950, training loss: 0.0713525265455246 = 0.0034665849525481462 + 0.01 * 6.7885942459106445
Epoch 950, val loss: 1.3759641647338867
Epoch 960, training loss: 0.07159828394651413 = 0.003381946589797735 + 0.01 * 6.821633815765381
Epoch 960, val loss: 1.3800417184829712
Epoch 970, training loss: 0.07115702331066132 = 0.003301207907497883 + 0.01 * 6.785581588745117
Epoch 970, val loss: 1.384102702140808
Epoch 980, training loss: 0.07125044614076614 = 0.0032239186111837626 + 0.01 * 6.802652835845947
Epoch 980, val loss: 1.3879525661468506
Epoch 990, training loss: 0.07111231237649918 = 0.0031501285266131163 + 0.01 * 6.796218395233154
Epoch 990, val loss: 1.3918383121490479
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8212967843964154
=== training gcn model ===
Epoch 0, training loss: 2.0497617721557617 = 1.963793158531189 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.9542536735534668
Epoch 10, training loss: 2.0392777919769287 = 1.9533097743988037 + 0.01 * 8.5968017578125
Epoch 10, val loss: 1.944579839706421
Epoch 20, training loss: 2.0263760089874268 = 1.940409779548645 + 0.01 * 8.59662914276123
Epoch 20, val loss: 1.932577133178711
Epoch 30, training loss: 2.008119583129883 = 1.9221590757369995 + 0.01 * 8.596061706542969
Epoch 30, val loss: 1.9155666828155518
Epoch 40, training loss: 1.9809596538543701 = 1.8950310945510864 + 0.01 * 8.59285831451416
Epoch 40, val loss: 1.8906843662261963
Epoch 50, training loss: 1.9423749446868896 = 1.856666922569275 + 0.01 * 8.570804595947266
Epoch 50, val loss: 1.8574578762054443
Epoch 60, training loss: 1.8993300199508667 = 1.8144378662109375 + 0.01 * 8.489216804504395
Epoch 60, val loss: 1.8257489204406738
Epoch 70, training loss: 1.8668296337127686 = 1.7841646671295166 + 0.01 * 8.26649284362793
Epoch 70, val loss: 1.8048858642578125
Epoch 80, training loss: 1.833433747291565 = 1.751739740371704 + 0.01 * 8.169401168823242
Epoch 80, val loss: 1.772800087928772
Epoch 90, training loss: 1.7873331308364868 = 1.7070939540863037 + 0.01 * 8.023921966552734
Epoch 90, val loss: 1.7308881282806396
Epoch 100, training loss: 1.7232211828231812 = 1.644598126411438 + 0.01 * 7.862301826477051
Epoch 100, val loss: 1.6769517660140991
Epoch 110, training loss: 1.640727162361145 = 1.564340353012085 + 0.01 * 7.638680934906006
Epoch 110, val loss: 1.6098651885986328
Epoch 120, training loss: 1.550994634628296 = 1.4774326086044312 + 0.01 * 7.356198310852051
Epoch 120, val loss: 1.5400056838989258
Epoch 130, training loss: 1.4669195413589478 = 1.393778920173645 + 0.01 * 7.314066410064697
Epoch 130, val loss: 1.474942684173584
Epoch 140, training loss: 1.3871742486953735 = 1.3143565654754639 + 0.01 * 7.281772613525391
Epoch 140, val loss: 1.4148603677749634
Epoch 150, training loss: 1.307978630065918 = 1.235350489616394 + 0.01 * 7.26281213760376
Epoch 150, val loss: 1.3549339771270752
Epoch 160, training loss: 1.2277895212173462 = 1.1553282737731934 + 0.01 * 7.246125221252441
Epoch 160, val loss: 1.295634388923645
Epoch 170, training loss: 1.1477692127227783 = 1.0754495859146118 + 0.01 * 7.231967449188232
Epoch 170, val loss: 1.2382738590240479
Epoch 180, training loss: 1.0692452192306519 = 0.9970057606697083 + 0.01 * 7.223948001861572
Epoch 180, val loss: 1.1838126182556152
Epoch 190, training loss: 0.9924964308738708 = 0.9203107357025146 + 0.01 * 7.218569755554199
Epoch 190, val loss: 1.1312825679779053
Epoch 200, training loss: 0.9168064594268799 = 0.8446660041809082 + 0.01 * 7.214046478271484
Epoch 200, val loss: 1.0796005725860596
Epoch 210, training loss: 0.8413630723953247 = 0.7692640423774719 + 0.01 * 7.209901809692383
Epoch 210, val loss: 1.0282047986984253
Epoch 220, training loss: 0.7662375569343567 = 0.6941842436790466 + 0.01 * 7.205331802368164
Epoch 220, val loss: 0.9772422313690186
Epoch 230, training loss: 0.6927099823951721 = 0.6207167506217957 + 0.01 * 7.1993255615234375
Epoch 230, val loss: 0.9281741976737976
Epoch 240, training loss: 0.6225999593734741 = 0.550691545009613 + 0.01 * 7.19084358215332
Epoch 240, val loss: 0.8834889531135559
Epoch 250, training loss: 0.5573685169219971 = 0.4855784475803375 + 0.01 * 7.179009914398193
Epoch 250, val loss: 0.8447939157485962
Epoch 260, training loss: 0.49754273891448975 = 0.4258846938610077 + 0.01 * 7.165806293487549
Epoch 260, val loss: 0.8131075501441956
Epoch 270, training loss: 0.44327306747436523 = 0.3718176484107971 + 0.01 * 7.145542144775391
Epoch 270, val loss: 0.7880629897117615
Epoch 280, training loss: 0.3946961760520935 = 0.32344016432762146 + 0.01 * 7.125601291656494
Epoch 280, val loss: 0.7690563797950745
Epoch 290, training loss: 0.35164257884025574 = 0.28054845333099365 + 0.01 * 7.10941219329834
Epoch 290, val loss: 0.7556130290031433
Epoch 300, training loss: 0.31386858224868774 = 0.24290908873081207 + 0.01 * 7.095949649810791
Epoch 300, val loss: 0.7473320364952087
Epoch 310, training loss: 0.28097838163375854 = 0.210164412856102 + 0.01 * 7.081395626068115
Epoch 310, val loss: 0.7435188293457031
Epoch 320, training loss: 0.2526123523712158 = 0.18189610540866852 + 0.01 * 7.071625709533691
Epoch 320, val loss: 0.7431959509849548
Epoch 330, training loss: 0.2283269464969635 = 0.15763497352600098 + 0.01 * 7.069197177886963
Epoch 330, val loss: 0.7457942366600037
Epoch 340, training loss: 0.20747233927249908 = 0.13687647879123688 + 0.01 * 7.059586524963379
Epoch 340, val loss: 0.7506596446037292
Epoch 350, training loss: 0.18964071571826935 = 0.11913595348596573 + 0.01 * 7.050476551055908
Epoch 350, val loss: 0.757209837436676
Epoch 360, training loss: 0.1744077205657959 = 0.103978231549263 + 0.01 * 7.042949676513672
Epoch 360, val loss: 0.7650817632675171
Epoch 370, training loss: 0.16141390800476074 = 0.0910215675830841 + 0.01 * 7.039234161376953
Epoch 370, val loss: 0.7740048170089722
Epoch 380, training loss: 0.15024548768997192 = 0.07993817329406738 + 0.01 * 7.030732154846191
Epoch 380, val loss: 0.7836499214172363
Epoch 390, training loss: 0.14090955257415771 = 0.07045450806617737 + 0.01 * 7.045505046844482
Epoch 390, val loss: 0.7938070297241211
Epoch 400, training loss: 0.13254843652248383 = 0.06234316900372505 + 0.01 * 7.02052640914917
Epoch 400, val loss: 0.8044753074645996
Epoch 410, training loss: 0.12551888823509216 = 0.05539105087518692 + 0.01 * 7.012784004211426
Epoch 410, val loss: 0.8153226375579834
Epoch 420, training loss: 0.11965705454349518 = 0.04941747337579727 + 0.01 * 7.023958206176758
Epoch 420, val loss: 0.8263764381408691
Epoch 430, training loss: 0.11434541642665863 = 0.04427495226264 + 0.01 * 7.007046222686768
Epoch 430, val loss: 0.8374179005622864
Epoch 440, training loss: 0.10982205718755722 = 0.03983151167631149 + 0.01 * 6.999054908752441
Epoch 440, val loss: 0.8483920097351074
Epoch 450, training loss: 0.10592126846313477 = 0.035977691411972046 + 0.01 * 6.994357585906982
Epoch 450, val loss: 0.8592413067817688
Epoch 460, training loss: 0.1025240570306778 = 0.03262258693575859 + 0.01 * 6.990147590637207
Epoch 460, val loss: 0.8699880242347717
Epoch 470, training loss: 0.09960557520389557 = 0.02969340607523918 + 0.01 * 6.991216659545898
Epoch 470, val loss: 0.8804193139076233
Epoch 480, training loss: 0.09691932797431946 = 0.027127843350172043 + 0.01 * 6.979148864746094
Epoch 480, val loss: 0.8906633257865906
Epoch 490, training loss: 0.094635970890522 = 0.024870986118912697 + 0.01 * 6.976499080657959
Epoch 490, val loss: 0.900653600692749
Epoch 500, training loss: 0.09257622808218002 = 0.022878045216202736 + 0.01 * 6.969818592071533
Epoch 500, val loss: 0.9103091955184937
Epoch 510, training loss: 0.0907776802778244 = 0.0211115051060915 + 0.01 * 6.966618061065674
Epoch 510, val loss: 0.9197137951850891
Epoch 520, training loss: 0.08926978707313538 = 0.019540969282388687 + 0.01 * 6.972881317138672
Epoch 520, val loss: 0.9288474321365356
Epoch 530, training loss: 0.08776192367076874 = 0.018142886459827423 + 0.01 * 6.961904048919678
Epoch 530, val loss: 0.9376315474510193
Epoch 540, training loss: 0.08645178377628326 = 0.01689300313591957 + 0.01 * 6.955878734588623
Epoch 540, val loss: 0.9461511373519897
Epoch 550, training loss: 0.08526913076639175 = 0.015769919380545616 + 0.01 * 6.949921131134033
Epoch 550, val loss: 0.9543956518173218
Epoch 560, training loss: 0.08419763296842575 = 0.014757170341908932 + 0.01 * 6.944046497344971
Epoch 560, val loss: 0.9624420404434204
Epoch 570, training loss: 0.0833243727684021 = 0.013841233216226101 + 0.01 * 6.948314189910889
Epoch 570, val loss: 0.9702481627464294
Epoch 580, training loss: 0.08238712698221207 = 0.01301124133169651 + 0.01 * 6.937589168548584
Epoch 580, val loss: 0.9777169823646545
Epoch 590, training loss: 0.08156555145978928 = 0.012256795540452003 + 0.01 * 6.930875778198242
Epoch 590, val loss: 0.9850561022758484
Epoch 600, training loss: 0.08098526298999786 = 0.011568982154130936 + 0.01 * 6.941628456115723
Epoch 600, val loss: 0.9922025203704834
Epoch 610, training loss: 0.08019370585680008 = 0.010940758511424065 + 0.01 * 6.925294876098633
Epoch 610, val loss: 0.9990129470825195
Epoch 620, training loss: 0.07952965795993805 = 0.010365364141762257 + 0.01 * 6.91642951965332
Epoch 620, val loss: 1.0056681632995605
Epoch 630, training loss: 0.07897592335939407 = 0.009837229736149311 + 0.01 * 6.9138689041137695
Epoch 630, val loss: 1.012205719947815
Epoch 640, training loss: 0.07842235267162323 = 0.0093520013615489 + 0.01 * 6.9070353507995605
Epoch 640, val loss: 1.0184215307235718
Epoch 650, training loss: 0.07800769805908203 = 0.008904374204576015 + 0.01 * 6.910332202911377
Epoch 650, val loss: 1.0244325399398804
Epoch 660, training loss: 0.0775395855307579 = 0.008490845561027527 + 0.01 * 6.904873847961426
Epoch 660, val loss: 1.030387043952942
Epoch 670, training loss: 0.07705122232437134 = 0.008107965812087059 + 0.01 * 6.8943257331848145
Epoch 670, val loss: 1.0361342430114746
Epoch 680, training loss: 0.07690870016813278 = 0.007752471603453159 + 0.01 * 6.915623188018799
Epoch 680, val loss: 1.0416970252990723
Epoch 690, training loss: 0.07629144936800003 = 0.007422308903187513 + 0.01 * 6.886914253234863
Epoch 690, val loss: 1.047031283378601
Epoch 700, training loss: 0.0758887529373169 = 0.007115081883966923 + 0.01 * 6.8773674964904785
Epoch 700, val loss: 1.0523422956466675
Epoch 710, training loss: 0.0756385549902916 = 0.006828338839113712 + 0.01 * 6.881021499633789
Epoch 710, val loss: 1.0574818849563599
Epoch 720, training loss: 0.07525111734867096 = 0.006560552399605513 + 0.01 * 6.869056701660156
Epoch 720, val loss: 1.062363624572754
Epoch 730, training loss: 0.07489185780286789 = 0.006310025695711374 + 0.01 * 6.85818338394165
Epoch 730, val loss: 1.0672465562820435
Epoch 740, training loss: 0.07477620989084244 = 0.0060754260048270226 + 0.01 * 6.870078086853027
Epoch 740, val loss: 1.0718878507614136
Epoch 750, training loss: 0.07435010373592377 = 0.005855814088135958 + 0.01 * 6.849429130554199
Epoch 750, val loss: 1.0764626264572144
Epoch 760, training loss: 0.074213407933712 = 0.005649259313941002 + 0.01 * 6.856414794921875
Epoch 760, val loss: 1.0809998512268066
Epoch 770, training loss: 0.07397408038377762 = 0.005455258768051863 + 0.01 * 6.851882457733154
Epoch 770, val loss: 1.0853039026260376
Epoch 780, training loss: 0.07366623729467392 = 0.00527286808937788 + 0.01 * 6.839337348937988
Epoch 780, val loss: 1.0894933938980103
Epoch 790, training loss: 0.0733824074268341 = 0.005100523587316275 + 0.01 * 6.828188419342041
Epoch 790, val loss: 1.0935779809951782
Epoch 800, training loss: 0.07318935543298721 = 0.004937439691275358 + 0.01 * 6.825191497802734
Epoch 800, val loss: 1.0976223945617676
Epoch 810, training loss: 0.07300690561532974 = 0.004783319775015116 + 0.01 * 6.822359085083008
Epoch 810, val loss: 1.1015844345092773
Epoch 820, training loss: 0.07285338640213013 = 0.004637596197426319 + 0.01 * 6.8215789794921875
Epoch 820, val loss: 1.1053839921951294
Epoch 830, training loss: 0.07264895737171173 = 0.004499342292547226 + 0.01 * 6.8149614334106445
Epoch 830, val loss: 1.1090883016586304
Epoch 840, training loss: 0.0724298506975174 = 0.004368197172880173 + 0.01 * 6.80616569519043
Epoch 840, val loss: 1.1127558946609497
Epoch 850, training loss: 0.07247141748666763 = 0.004243615083396435 + 0.01 * 6.822780609130859
Epoch 850, val loss: 1.1163612604141235
Epoch 860, training loss: 0.07216012477874756 = 0.004125304985791445 + 0.01 * 6.8034820556640625
Epoch 860, val loss: 1.1198270320892334
Epoch 870, training loss: 0.07200596481561661 = 0.004012967925518751 + 0.01 * 6.799300193786621
Epoch 870, val loss: 1.1231850385665894
Epoch 880, training loss: 0.07186595350503922 = 0.0039058872498571873 + 0.01 * 6.79600715637207
Epoch 880, val loss: 1.1265462636947632
Epoch 890, training loss: 0.07175655663013458 = 0.0038037635385990143 + 0.01 * 6.7952799797058105
Epoch 890, val loss: 1.1297614574432373
Epoch 900, training loss: 0.07158764451742172 = 0.0037063895724713802 + 0.01 * 6.788125514984131
Epoch 900, val loss: 1.1329104900360107
Epoch 910, training loss: 0.07146850973367691 = 0.0036134093534201384 + 0.01 * 6.7855095863342285
Epoch 910, val loss: 1.136052131652832
Epoch 920, training loss: 0.07141295075416565 = 0.0035247686319053173 + 0.01 * 6.788818359375
Epoch 920, val loss: 1.1390297412872314
Epoch 930, training loss: 0.0713290348649025 = 0.0034399668220430613 + 0.01 * 6.788906574249268
Epoch 930, val loss: 1.1420525312423706
Epoch 940, training loss: 0.07114838808774948 = 0.003358700545504689 + 0.01 * 6.7789692878723145
Epoch 940, val loss: 1.1448732614517212
Epoch 950, training loss: 0.07101830095052719 = 0.003281132783740759 + 0.01 * 6.773717403411865
Epoch 950, val loss: 1.1476953029632568
Epoch 960, training loss: 0.07112652063369751 = 0.0032067031133919954 + 0.01 * 6.791982173919678
Epoch 960, val loss: 1.15045166015625
Epoch 970, training loss: 0.07093778997659683 = 0.00313565437681973 + 0.01 * 6.780214309692383
Epoch 970, val loss: 1.153173804283142
Epoch 980, training loss: 0.07084592431783676 = 0.003067342797294259 + 0.01 * 6.777857780456543
Epoch 980, val loss: 1.1557191610336304
Epoch 990, training loss: 0.07067595422267914 = 0.0030017485842108727 + 0.01 * 6.767420768737793
Epoch 990, val loss: 1.1583393812179565
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8202424881391671
The final CL Acc:0.73827, 0.03234, The final GNN Acc:0.82112, 0.00066
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13118])
remove edge: torch.Size([2, 7970])
updated graph: torch.Size([2, 10532])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0449390411376953 = 1.9589706659317017 + 0.01 * 8.59682846069336
Epoch 0, val loss: 1.962723970413208
Epoch 10, training loss: 2.034499168395996 = 1.9485316276550293 + 0.01 * 8.596748352050781
Epoch 10, val loss: 1.9520729780197144
Epoch 20, training loss: 2.021296739578247 = 1.9353315830230713 + 0.01 * 8.59650993347168
Epoch 20, val loss: 1.93848717212677
Epoch 30, training loss: 2.0023515224456787 = 1.9163939952850342 + 0.01 * 8.595744132995605
Epoch 30, val loss: 1.9191261529922485
Epoch 40, training loss: 1.9740923643112183 = 1.8881783485412598 + 0.01 * 8.591398239135742
Epoch 40, val loss: 1.8906989097595215
Epoch 50, training loss: 1.9341291189193726 = 1.8485021591186523 + 0.01 * 8.562694549560547
Epoch 50, val loss: 1.852540135383606
Epoch 60, training loss: 1.8882622718811035 = 1.8038384914398193 + 0.01 * 8.442381858825684
Epoch 60, val loss: 1.8135261535644531
Epoch 70, training loss: 1.847532033920288 = 1.7665907144546509 + 0.01 * 8.094136238098145
Epoch 70, val loss: 1.7827383279800415
Epoch 80, training loss: 1.802377700805664 = 1.7236576080322266 + 0.01 * 7.872015476226807
Epoch 80, val loss: 1.7422081232070923
Epoch 90, training loss: 1.7410304546356201 = 1.6646430492401123 + 0.01 * 7.63873815536499
Epoch 90, val loss: 1.6877703666687012
Epoch 100, training loss: 1.6603009700775146 = 1.5857337713241577 + 0.01 * 7.456722259521484
Epoch 100, val loss: 1.618780255317688
Epoch 110, training loss: 1.5616469383239746 = 1.4881597757339478 + 0.01 * 7.348712921142578
Epoch 110, val loss: 1.5357333421707153
Epoch 120, training loss: 1.4554296731948853 = 1.382360816001892 + 0.01 * 7.306883335113525
Epoch 120, val loss: 1.4480571746826172
Epoch 130, training loss: 1.3511782884597778 = 1.2783349752426147 + 0.01 * 7.284326553344727
Epoch 130, val loss: 1.3636590242385864
Epoch 140, training loss: 1.252380609512329 = 1.1797208786010742 + 0.01 * 7.265975475311279
Epoch 140, val loss: 1.2844573259353638
Epoch 150, training loss: 1.1602985858917236 = 1.0878397226333618 + 0.01 * 7.245891094207764
Epoch 150, val loss: 1.2112126350402832
Epoch 160, training loss: 1.0759098529815674 = 1.0036152601242065 + 0.01 * 7.229458808898926
Epoch 160, val loss: 1.14418363571167
Epoch 170, training loss: 0.9977209568023682 = 0.9255290031433105 + 0.01 * 7.219196796417236
Epoch 170, val loss: 1.0816419124603271
Epoch 180, training loss: 0.9234638214111328 = 0.8513451814651489 + 0.01 * 7.2118635177612305
Epoch 180, val loss: 1.0211290121078491
Epoch 190, training loss: 0.8522579073905945 = 0.780224621295929 + 0.01 * 7.203329563140869
Epoch 190, val loss: 0.9623537659645081
Epoch 200, training loss: 0.7854400277137756 = 0.7135111689567566 + 0.01 * 7.192888259887695
Epoch 200, val loss: 0.9074190258979797
Epoch 210, training loss: 0.7248048782348633 = 0.6530020833015442 + 0.01 * 7.1802802085876465
Epoch 210, val loss: 0.8596718907356262
Epoch 220, training loss: 0.6706240773200989 = 0.5989750027656555 + 0.01 * 7.164906024932861
Epoch 220, val loss: 0.8205615282058716
Epoch 230, training loss: 0.6217690706253052 = 0.5502793192863464 + 0.01 * 7.148972511291504
Epoch 230, val loss: 0.789611279964447
Epoch 240, training loss: 0.5770887732505798 = 0.5057812929153442 + 0.01 * 7.130746841430664
Epoch 240, val loss: 0.7652444839477539
Epoch 250, training loss: 0.535688579082489 = 0.4645310342311859 + 0.01 * 7.115752696990967
Epoch 250, val loss: 0.7453958988189697
Epoch 260, training loss: 0.49646276235580444 = 0.4254383146762848 + 0.01 * 7.102443695068359
Epoch 260, val loss: 0.7283082604408264
Epoch 270, training loss: 0.4582356810569763 = 0.38731199502944946 + 0.01 * 7.092367172241211
Epoch 270, val loss: 0.7127412557601929
Epoch 280, training loss: 0.42022812366485596 = 0.349364310503006 + 0.01 * 7.0863800048828125
Epoch 280, val loss: 0.6986111998558044
Epoch 290, training loss: 0.382501482963562 = 0.31169259548187256 + 0.01 * 7.08089017868042
Epoch 290, val loss: 0.6865452527999878
Epoch 300, training loss: 0.3459792137145996 = 0.2752494513988495 + 0.01 * 7.072977542877197
Epoch 300, val loss: 0.67766273021698
Epoch 310, training loss: 0.31189942359924316 = 0.24121816456317902 + 0.01 * 7.0681257247924805
Epoch 310, val loss: 0.6726235747337341
Epoch 320, training loss: 0.2811312973499298 = 0.2104765772819519 + 0.01 * 7.06547212600708
Epoch 320, val loss: 0.6714818477630615
Epoch 330, training loss: 0.2540214955806732 = 0.18341593444347382 + 0.01 * 7.060555934906006
Epoch 330, val loss: 0.6738325357437134
Epoch 340, training loss: 0.23060844838619232 = 0.16002075374126434 + 0.01 * 7.058769702911377
Epoch 340, val loss: 0.6789929270744324
Epoch 350, training loss: 0.21052837371826172 = 0.13996873795986176 + 0.01 * 7.05596399307251
Epoch 350, val loss: 0.6864631772041321
Epoch 360, training loss: 0.19333234429359436 = 0.12281829118728638 + 0.01 * 7.05140495300293
Epoch 360, val loss: 0.6957377791404724
Epoch 370, training loss: 0.17861929535865784 = 0.10814035683870316 + 0.01 * 7.047893047332764
Epoch 370, val loss: 0.7064523696899414
Epoch 380, training loss: 0.16603173315525055 = 0.09555503726005554 + 0.01 * 7.047669887542725
Epoch 380, val loss: 0.7183079123497009
Epoch 390, training loss: 0.15517747402191162 = 0.08474240452051163 + 0.01 * 7.0435075759887695
Epoch 390, val loss: 0.7310294508934021
Epoch 400, training loss: 0.14583536982536316 = 0.0754299983382225 + 0.01 * 7.040537357330322
Epoch 400, val loss: 0.7443368434906006
Epoch 410, training loss: 0.13778871297836304 = 0.06738782674074173 + 0.01 * 7.0400896072387695
Epoch 410, val loss: 0.7580669522285461
Epoch 420, training loss: 0.13077512383460999 = 0.0604214072227478 + 0.01 * 7.035372257232666
Epoch 420, val loss: 0.7720384001731873
Epoch 430, training loss: 0.12472447007894516 = 0.054368987679481506 + 0.01 * 7.035548686981201
Epoch 430, val loss: 0.7861037254333496
Epoch 440, training loss: 0.11941368877887726 = 0.04909452423453331 + 0.01 * 7.031917095184326
Epoch 440, val loss: 0.8002780675888062
Epoch 450, training loss: 0.11476616561412811 = 0.04448310658335686 + 0.01 * 7.028305530548096
Epoch 450, val loss: 0.8144335150718689
Epoch 460, training loss: 0.1107393205165863 = 0.04043693467974663 + 0.01 * 7.030238628387451
Epoch 460, val loss: 0.8284205794334412
Epoch 470, training loss: 0.10710018873214722 = 0.036875803023576736 + 0.01 * 7.022439002990723
Epoch 470, val loss: 0.8422961235046387
Epoch 480, training loss: 0.10392791777849197 = 0.03372998535633087 + 0.01 * 7.0197930335998535
Epoch 480, val loss: 0.8559728264808655
Epoch 490, training loss: 0.10114333778619766 = 0.030942479148507118 + 0.01 * 7.020086288452148
Epoch 490, val loss: 0.869364321231842
Epoch 500, training loss: 0.09860246628522873 = 0.028465546667575836 + 0.01 * 7.0136919021606445
Epoch 500, val loss: 0.8825087547302246
Epoch 510, training loss: 0.09635451436042786 = 0.02625756338238716 + 0.01 * 7.009694576263428
Epoch 510, val loss: 0.8953123092651367
Epoch 520, training loss: 0.0943613275885582 = 0.024283794686198235 + 0.01 * 7.007753372192383
Epoch 520, val loss: 0.9078646898269653
Epoch 530, training loss: 0.0925690084695816 = 0.022514989599585533 + 0.01 * 7.005402088165283
Epoch 530, val loss: 0.9200757145881653
Epoch 540, training loss: 0.09090980887413025 = 0.02092532068490982 + 0.01 * 6.998449325561523
Epoch 540, val loss: 0.931966245174408
Epoch 550, training loss: 0.08943276852369308 = 0.019490981474518776 + 0.01 * 6.9941792488098145
Epoch 550, val loss: 0.9436137676239014
Epoch 560, training loss: 0.08811686187982559 = 0.018193716183304787 + 0.01 * 6.99231481552124
Epoch 560, val loss: 0.9549100399017334
Epoch 570, training loss: 0.08686727285385132 = 0.017019163817167282 + 0.01 * 6.984811782836914
Epoch 570, val loss: 0.9659073352813721
Epoch 580, training loss: 0.0857740044593811 = 0.01595268025994301 + 0.01 * 6.9821319580078125
Epoch 580, val loss: 0.9766204953193665
Epoch 590, training loss: 0.08469652384519577 = 0.014981917105615139 + 0.01 * 6.971460819244385
Epoch 590, val loss: 0.9870058298110962
Epoch 600, training loss: 0.0838712602853775 = 0.014096859842538834 + 0.01 * 6.97744083404541
Epoch 600, val loss: 0.9970585703849792
Epoch 610, training loss: 0.08285629004240036 = 0.013288424350321293 + 0.01 * 6.956787109375
Epoch 610, val loss: 1.0068717002868652
Epoch 620, training loss: 0.0821748822927475 = 0.012548677623271942 + 0.01 * 6.962620735168457
Epoch 620, val loss: 1.0163698196411133
Epoch 630, training loss: 0.08134741336107254 = 0.01187050435692072 + 0.01 * 6.947690963745117
Epoch 630, val loss: 1.0256218910217285
Epoch 640, training loss: 0.08062758296728134 = 0.01124805398285389 + 0.01 * 6.937952995300293
Epoch 640, val loss: 1.0345460176467896
Epoch 650, training loss: 0.08002623915672302 = 0.010675537399947643 + 0.01 * 6.935070514678955
Epoch 650, val loss: 1.0432343482971191
Epoch 660, training loss: 0.07940977811813354 = 0.010148834437131882 + 0.01 * 6.926095008850098
Epoch 660, val loss: 1.051679015159607
Epoch 670, training loss: 0.07886125147342682 = 0.009662470780313015 + 0.01 * 6.9198784828186035
Epoch 670, val loss: 1.0597777366638184
Epoch 680, training loss: 0.07820850610733032 = 0.009212438017129898 + 0.01 * 6.899607181549072
Epoch 680, val loss: 1.0678495168685913
Epoch 690, training loss: 0.07791110873222351 = 0.008795754984021187 + 0.01 * 6.911535739898682
Epoch 690, val loss: 1.075448751449585
Epoch 700, training loss: 0.07724979519844055 = 0.008409058675169945 + 0.01 * 6.884073257446289
Epoch 700, val loss: 1.083024024963379
Epoch 710, training loss: 0.07690562307834625 = 0.00804977584630251 + 0.01 * 6.885584831237793
Epoch 710, val loss: 1.0902591943740845
Epoch 720, training loss: 0.07641910016536713 = 0.007715250831097364 + 0.01 * 6.87038516998291
Epoch 720, val loss: 1.097347617149353
Epoch 730, training loss: 0.07616610080003738 = 0.00740332156419754 + 0.01 * 6.876277923583984
Epoch 730, val loss: 1.1042602062225342
Epoch 740, training loss: 0.07593651860952377 = 0.007112162187695503 + 0.01 * 6.8824357986450195
Epoch 740, val loss: 1.1110447645187378
Epoch 750, training loss: 0.07542598247528076 = 0.006839951034635305 + 0.01 * 6.858603000640869
Epoch 750, val loss: 1.1175390481948853
Epoch 760, training loss: 0.07492293417453766 = 0.006584762595593929 + 0.01 * 6.833817005157471
Epoch 760, val loss: 1.1239781379699707
Epoch 770, training loss: 0.07455477863550186 = 0.006345100235193968 + 0.01 * 6.820968151092529
Epoch 770, val loss: 1.1302237510681152
Epoch 780, training loss: 0.0745018869638443 = 0.006119982339441776 + 0.01 * 6.83819055557251
Epoch 780, val loss: 1.1363521814346313
Epoch 790, training loss: 0.07409658282995224 = 0.005908307619392872 + 0.01 * 6.818827152252197
Epoch 790, val loss: 1.1423150300979614
Epoch 800, training loss: 0.07410958409309387 = 0.005708599928766489 + 0.01 * 6.840098857879639
Epoch 800, val loss: 1.148106575012207
Epoch 810, training loss: 0.07364249974489212 = 0.005520538426935673 + 0.01 * 6.812196731567383
Epoch 810, val loss: 1.1538710594177246
Epoch 820, training loss: 0.07343023270368576 = 0.005342823453247547 + 0.01 * 6.808740615844727
Epoch 820, val loss: 1.1594488620758057
Epoch 830, training loss: 0.07315047085285187 = 0.005174779333174229 + 0.01 * 6.797569274902344
Epoch 830, val loss: 1.1648855209350586
Epoch 840, training loss: 0.0730891302227974 = 0.005015878938138485 + 0.01 * 6.8073248863220215
Epoch 840, val loss: 1.1703039407730103
Epoch 850, training loss: 0.07271851599216461 = 0.004864833317697048 + 0.01 * 6.785367965698242
Epoch 850, val loss: 1.1755197048187256
Epoch 860, training loss: 0.07241915166378021 = 0.004721472039818764 + 0.01 * 6.769768238067627
Epoch 860, val loss: 1.1806873083114624
Epoch 870, training loss: 0.07242745906114578 = 0.004585628863424063 + 0.01 * 6.784183025360107
Epoch 870, val loss: 1.1856745481491089
Epoch 880, training loss: 0.07221130281686783 = 0.00445678411051631 + 0.01 * 6.775452613830566
Epoch 880, val loss: 1.1905615329742432
Epoch 890, training loss: 0.07206753641366959 = 0.004334206227213144 + 0.01 * 6.773333549499512
Epoch 890, val loss: 1.1953790187835693
Epoch 900, training loss: 0.07192373275756836 = 0.004217051900923252 + 0.01 * 6.7706685066223145
Epoch 900, val loss: 1.200076699256897
Epoch 910, training loss: 0.07179965078830719 = 0.004105511587113142 + 0.01 * 6.76941442489624
Epoch 910, val loss: 1.204756736755371
Epoch 920, training loss: 0.07156895101070404 = 0.003999143838882446 + 0.01 * 6.756980895996094
Epoch 920, val loss: 1.2092229127883911
Epoch 930, training loss: 0.07155101746320724 = 0.0038979158271104097 + 0.01 * 6.765310287475586
Epoch 930, val loss: 1.2136934995651245
Epoch 940, training loss: 0.07148509472608566 = 0.0038009192794561386 + 0.01 * 6.7684173583984375
Epoch 940, val loss: 1.2179546356201172
Epoch 950, training loss: 0.0711558386683464 = 0.0037083798088133335 + 0.01 * 6.744746208190918
Epoch 950, val loss: 1.2222506999969482
Epoch 960, training loss: 0.07099976390600204 = 0.0036197262816131115 + 0.01 * 6.738004207611084
Epoch 960, val loss: 1.2263771295547485
Epoch 970, training loss: 0.0709538534283638 = 0.0035348243545740843 + 0.01 * 6.741903305053711
Epoch 970, val loss: 1.2304651737213135
Epoch 980, training loss: 0.07104711979627609 = 0.0034535482991486788 + 0.01 * 6.759357929229736
Epoch 980, val loss: 1.234541416168213
Epoch 990, training loss: 0.07075101137161255 = 0.0033754254691302776 + 0.01 * 6.737558364868164
Epoch 990, val loss: 1.2384008169174194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8497627833421192
=== training gcn model ===
Epoch 0, training loss: 2.012054204940796 = 1.9260858297348022 + 0.01 * 8.596832275390625
Epoch 0, val loss: 1.9195427894592285
Epoch 10, training loss: 2.0029923915863037 = 1.9170247316360474 + 0.01 * 8.596772193908691
Epoch 10, val loss: 1.9106192588806152
Epoch 20, training loss: 1.9920412302017212 = 1.9060758352279663 + 0.01 * 8.596535682678223
Epoch 20, val loss: 1.89938223361969
Epoch 30, training loss: 1.977005958557129 = 1.891048550605774 + 0.01 * 8.595745086669922
Epoch 30, val loss: 1.8835499286651611
Epoch 40, training loss: 1.9549726247787476 = 1.869057536125183 + 0.01 * 8.591509819030762
Epoch 40, val loss: 1.8604810237884521
Epoch 50, training loss: 1.9238566160202026 = 1.838234782218933 + 0.01 * 8.562180519104004
Epoch 50, val loss: 1.8292946815490723
Epoch 60, training loss: 1.885353684425354 = 1.8013836145401 + 0.01 * 8.397003173828125
Epoch 60, val loss: 1.795346975326538
Epoch 70, training loss: 1.8431572914123535 = 1.7629284858703613 + 0.01 * 8.02287769317627
Epoch 70, val loss: 1.763564944267273
Epoch 80, training loss: 1.791225552558899 = 1.713729977607727 + 0.01 * 7.749558448791504
Epoch 80, val loss: 1.7214404344558716
Epoch 90, training loss: 1.720562219619751 = 1.6454579830169678 + 0.01 * 7.5104217529296875
Epoch 90, val loss: 1.6610323190689087
Epoch 100, training loss: 1.6307064294815063 = 1.5569981336593628 + 0.01 * 7.370834827423096
Epoch 100, val loss: 1.5848461389541626
Epoch 110, training loss: 1.527628779411316 = 1.454417109489441 + 0.01 * 7.321165084838867
Epoch 110, val loss: 1.4992610216140747
Epoch 120, training loss: 1.4188549518585205 = 1.3459473848342896 + 0.01 * 7.290755748748779
Epoch 120, val loss: 1.4118144512176514
Epoch 130, training loss: 1.309328556060791 = 1.236654281616211 + 0.01 * 7.267430782318115
Epoch 130, val loss: 1.3256467580795288
Epoch 140, training loss: 1.2034136056900024 = 1.1309343576431274 + 0.01 * 7.24791955947876
Epoch 140, val loss: 1.2445052862167358
Epoch 150, training loss: 1.105106234550476 = 1.0328483581542969 + 0.01 * 7.225791931152344
Epoch 150, val loss: 1.1711543798446655
Epoch 160, training loss: 1.0152888298034668 = 0.9432782530784607 + 0.01 * 7.201056480407715
Epoch 160, val loss: 1.105712652206421
Epoch 170, training loss: 0.9328002333641052 = 0.8610185384750366 + 0.01 * 7.178169250488281
Epoch 170, val loss: 1.046707034111023
Epoch 180, training loss: 0.8565044403076172 = 0.7849087715148926 + 0.01 * 7.159567356109619
Epoch 180, val loss: 0.9930658340454102
Epoch 190, training loss: 0.7860690355300903 = 0.714631974697113 + 0.01 * 7.14370584487915
Epoch 190, val loss: 0.9447983503341675
Epoch 200, training loss: 0.7208417057991028 = 0.6495569944381714 + 0.01 * 7.128471851348877
Epoch 200, val loss: 0.9019317030906677
Epoch 210, training loss: 0.6595190763473511 = 0.5883782505989075 + 0.01 * 7.114083290100098
Epoch 210, val loss: 0.8644931316375732
Epoch 220, training loss: 0.601195752620697 = 0.5302115082740784 + 0.01 * 7.098422050476074
Epoch 220, val loss: 0.8331630825996399
Epoch 230, training loss: 0.5463017821311951 = 0.47537824511528015 + 0.01 * 7.092356204986572
Epoch 230, val loss: 0.8089158535003662
Epoch 240, training loss: 0.4955470860004425 = 0.42490240931510925 + 0.01 * 7.064467430114746
Epoch 240, val loss: 0.7923276424407959
Epoch 250, training loss: 0.44987836480140686 = 0.37938499450683594 + 0.01 * 7.049337387084961
Epoch 250, val loss: 0.783063530921936
Epoch 260, training loss: 0.4090690016746521 = 0.33868685364723206 + 0.01 * 7.038215637207031
Epoch 260, val loss: 0.7801061868667603
Epoch 270, training loss: 0.3724699914455414 = 0.30219900608062744 + 0.01 * 7.027099609375
Epoch 270, val loss: 0.7819559574127197
Epoch 280, training loss: 0.3393377363681793 = 0.2691654860973358 + 0.01 * 7.0172247886657715
Epoch 280, val loss: 0.7874948382377625
Epoch 290, training loss: 0.3090854585170746 = 0.2390187829732895 + 0.01 * 7.006667137145996
Epoch 290, val loss: 0.7959890961647034
Epoch 300, training loss: 0.28170448541641235 = 0.211567685008049 + 0.01 * 7.013679027557373
Epoch 300, val loss: 0.8065903186798096
Epoch 310, training loss: 0.25683069229125977 = 0.18686971068382263 + 0.01 * 6.996098041534424
Epoch 310, val loss: 0.8190712332725525
Epoch 320, training loss: 0.23478129506111145 = 0.16491955518722534 + 0.01 * 6.986173152923584
Epoch 320, val loss: 0.8332399725914001
Epoch 330, training loss: 0.21538352966308594 = 0.14560365676879883 + 0.01 * 6.977988243103027
Epoch 330, val loss: 0.8486756086349487
Epoch 340, training loss: 0.1984463781118393 = 0.1286998838186264 + 0.01 * 6.974649429321289
Epoch 340, val loss: 0.8652494549751282
Epoch 350, training loss: 0.1836189478635788 = 0.1139434352517128 + 0.01 * 6.9675517082214355
Epoch 350, val loss: 0.8825470805168152
Epoch 360, training loss: 0.17079731822013855 = 0.1010744720697403 + 0.01 * 6.972283840179443
Epoch 360, val loss: 0.9003238081932068
Epoch 370, training loss: 0.159466952085495 = 0.08986810594797134 + 0.01 * 6.959884166717529
Epoch 370, val loss: 0.9182858467102051
Epoch 380, training loss: 0.14965113997459412 = 0.08009704947471619 + 0.01 * 6.955409526824951
Epoch 380, val loss: 0.9363360404968262
Epoch 390, training loss: 0.1410669982433319 = 0.0715668797492981 + 0.01 * 6.95001220703125
Epoch 390, val loss: 0.9543325901031494
Epoch 400, training loss: 0.1336841881275177 = 0.06411390006542206 + 0.01 * 6.957028388977051
Epoch 400, val loss: 0.9721893072128296
Epoch 410, training loss: 0.12702494859695435 = 0.05760082229971886 + 0.01 * 6.942412853240967
Epoch 410, val loss: 0.9897525310516357
Epoch 420, training loss: 0.12130036950111389 = 0.051890552043914795 + 0.01 * 6.940981864929199
Epoch 420, val loss: 1.0069910287857056
Epoch 430, training loss: 0.11623836308717728 = 0.04686850309371948 + 0.01 * 6.936986446380615
Epoch 430, val loss: 1.0239896774291992
Epoch 440, training loss: 0.11178134381771088 = 0.04243908450007439 + 0.01 * 6.934226036071777
Epoch 440, val loss: 1.0406627655029297
Epoch 450, training loss: 0.10782978683710098 = 0.03852531313896179 + 0.01 * 6.930447578430176
Epoch 450, val loss: 1.0569958686828613
Epoch 460, training loss: 0.10435225069522858 = 0.03506294637918472 + 0.01 * 6.928930759429932
Epoch 460, val loss: 1.0730009078979492
Epoch 470, training loss: 0.10136309266090393 = 0.03199365362524986 + 0.01 * 6.936944484710693
Epoch 470, val loss: 1.0885753631591797
Epoch 480, training loss: 0.09849266707897186 = 0.02927105315029621 + 0.01 * 6.922162055969238
Epoch 480, val loss: 1.103765845298767
Epoch 490, training loss: 0.09602509438991547 = 0.026850245893001556 + 0.01 * 6.917484760284424
Epoch 490, val loss: 1.1185656785964966
Epoch 500, training loss: 0.09384749084711075 = 0.024693109095096588 + 0.01 * 6.915438652038574
Epoch 500, val loss: 1.1329787969589233
Epoch 510, training loss: 0.09191370010375977 = 0.02276775613427162 + 0.01 * 6.914594650268555
Epoch 510, val loss: 1.146988868713379
Epoch 520, training loss: 0.09016560018062592 = 0.021045859903097153 + 0.01 * 6.911973476409912
Epoch 520, val loss: 1.1606225967407227
Epoch 530, training loss: 0.08856961131095886 = 0.019502786919474602 + 0.01 * 6.90668249130249
Epoch 530, val loss: 1.1738715171813965
Epoch 540, training loss: 0.08716614544391632 = 0.018116045743227005 + 0.01 * 6.905010223388672
Epoch 540, val loss: 1.1867363452911377
Epoch 550, training loss: 0.08587124198675156 = 0.016867658123373985 + 0.01 * 6.9003586769104
Epoch 550, val loss: 1.1992051601409912
Epoch 560, training loss: 0.08472040295600891 = 0.01574089005589485 + 0.01 * 6.897952079772949
Epoch 560, val loss: 1.2112998962402344
Epoch 570, training loss: 0.08367358893156052 = 0.01472145039588213 + 0.01 * 6.895213603973389
Epoch 570, val loss: 1.2230792045593262
Epoch 580, training loss: 0.08278269320726395 = 0.013796843588352203 + 0.01 * 6.898585319519043
Epoch 580, val loss: 1.2344632148742676
Epoch 590, training loss: 0.08182499557733536 = 0.012957216240465641 + 0.01 * 6.886778354644775
Epoch 590, val loss: 1.245492696762085
Epoch 600, training loss: 0.08104147762060165 = 0.01219204906374216 + 0.01 * 6.884943008422852
Epoch 600, val loss: 1.2562251091003418
Epoch 610, training loss: 0.08029872179031372 = 0.011493591591715813 + 0.01 * 6.8805131912231445
Epoch 610, val loss: 1.2666089534759521
Epoch 620, training loss: 0.07962767034769058 = 0.010854719206690788 + 0.01 * 6.877295017242432
Epoch 620, val loss: 1.276696801185608
Epoch 630, training loss: 0.07899931818246841 = 0.010268790647387505 + 0.01 * 6.873052597045898
Epoch 630, val loss: 1.2864584922790527
Epoch 640, training loss: 0.07856529206037521 = 0.009729892946779728 + 0.01 * 6.883540153503418
Epoch 640, val loss: 1.2959654331207275
Epoch 650, training loss: 0.07797540724277496 = 0.00923430547118187 + 0.01 * 6.874109745025635
Epoch 650, val loss: 1.3051484823226929
Epoch 660, training loss: 0.07741336524486542 = 0.008776978589594364 + 0.01 * 6.863638877868652
Epoch 660, val loss: 1.3140676021575928
Epoch 670, training loss: 0.07698577642440796 = 0.00835395697504282 + 0.01 * 6.863182067871094
Epoch 670, val loss: 1.3227715492248535
Epoch 680, training loss: 0.07654374837875366 = 0.007962251082062721 + 0.01 * 6.858150005340576
Epoch 680, val loss: 1.3311820030212402
Epoch 690, training loss: 0.07617581635713577 = 0.00759916054084897 + 0.01 * 6.857665538787842
Epoch 690, val loss: 1.3393542766571045
Epoch 700, training loss: 0.07578621059656143 = 0.007261499762535095 + 0.01 * 6.852471351623535
Epoch 700, val loss: 1.3473325967788696
Epoch 710, training loss: 0.07556964457035065 = 0.0069475332275033 + 0.01 * 6.86221170425415
Epoch 710, val loss: 1.355067491531372
Epoch 720, training loss: 0.07516226917505264 = 0.006655062548816204 + 0.01 * 6.85072135925293
Epoch 720, val loss: 1.3626219034194946
Epoch 730, training loss: 0.07485520839691162 = 0.0063821724615991116 + 0.01 * 6.84730339050293
Epoch 730, val loss: 1.36991286277771
Epoch 740, training loss: 0.07456598430871964 = 0.006126962136477232 + 0.01 * 6.843902587890625
Epoch 740, val loss: 1.3770370483398438
Epoch 750, training loss: 0.07427676022052765 = 0.005888220388442278 + 0.01 * 6.83885383605957
Epoch 750, val loss: 1.3839682340621948
Epoch 760, training loss: 0.07401096075773239 = 0.005664493422955275 + 0.01 * 6.834647178649902
Epoch 760, val loss: 1.390748143196106
Epoch 770, training loss: 0.07398823648691177 = 0.0054544722661376 + 0.01 * 6.853376388549805
Epoch 770, val loss: 1.397320032119751
Epoch 780, training loss: 0.07359955459833145 = 0.005257333163172007 + 0.01 * 6.834222316741943
Epoch 780, val loss: 1.4037500619888306
Epoch 790, training loss: 0.07328145951032639 = 0.005072084255516529 + 0.01 * 6.820937156677246
Epoch 790, val loss: 1.4099587202072144
Epoch 800, training loss: 0.07335393130779266 = 0.004897490609437227 + 0.01 * 6.845644474029541
Epoch 800, val loss: 1.416066288948059
Epoch 810, training loss: 0.07297728955745697 = 0.004733213223516941 + 0.01 * 6.824407577514648
Epoch 810, val loss: 1.4220044612884521
Epoch 820, training loss: 0.07281573861837387 = 0.004578152671456337 + 0.01 * 6.823758602142334
Epoch 820, val loss: 1.4277231693267822
Epoch 830, training loss: 0.07253610342741013 = 0.004431461915373802 + 0.01 * 6.810464382171631
Epoch 830, val loss: 1.4334481954574585
Epoch 840, training loss: 0.07246068120002747 = 0.0042927260510623455 + 0.01 * 6.816795349121094
Epoch 840, val loss: 1.4388853311538696
Epoch 850, training loss: 0.07224424928426743 = 0.004161318764090538 + 0.01 * 6.808293342590332
Epoch 850, val loss: 1.4442931413650513
Epoch 860, training loss: 0.07206586003303528 = 0.004036935046315193 + 0.01 * 6.802892684936523
Epoch 860, val loss: 1.4495168924331665
Epoch 870, training loss: 0.07217711955308914 = 0.003918846137821674 + 0.01 * 6.825827121734619
Epoch 870, val loss: 1.454634428024292
Epoch 880, training loss: 0.07173413783311844 = 0.0038070238661020994 + 0.01 * 6.7927117347717285
Epoch 880, val loss: 1.4596436023712158
Epoch 890, training loss: 0.07168129086494446 = 0.003700610250234604 + 0.01 * 6.798068523406982
Epoch 890, val loss: 1.4644551277160645
Epoch 900, training loss: 0.07160304486751556 = 0.0035991878248751163 + 0.01 * 6.80038595199585
Epoch 900, val loss: 1.4692729711532593
Epoch 910, training loss: 0.07146339118480682 = 0.003502793377265334 + 0.01 * 6.796060085296631
Epoch 910, val loss: 1.4738843441009521
Epoch 920, training loss: 0.07132262736558914 = 0.003410839242860675 + 0.01 * 6.791179180145264
Epoch 920, val loss: 1.4783953428268433
Epoch 930, training loss: 0.07119475305080414 = 0.0033231647685170174 + 0.01 * 6.787158489227295
Epoch 930, val loss: 1.482878565788269
Epoch 940, training loss: 0.07111160457134247 = 0.0032395166344940662 + 0.01 * 6.787208557128906
Epoch 940, val loss: 1.4871587753295898
Epoch 950, training loss: 0.07091318815946579 = 0.0031599029898643494 + 0.01 * 6.775328636169434
Epoch 950, val loss: 1.4913406372070312
Epoch 960, training loss: 0.07078983634710312 = 0.0030838390812277794 + 0.01 * 6.770599842071533
Epoch 960, val loss: 1.4954650402069092
Epoch 970, training loss: 0.07081852108240128 = 0.003011159598827362 + 0.01 * 6.780736446380615
Epoch 970, val loss: 1.4994370937347412
Epoch 980, training loss: 0.07082269340753555 = 0.002941849874332547 + 0.01 * 6.788084506988525
Epoch 980, val loss: 1.5034221410751343
Epoch 990, training loss: 0.07052963227033615 = 0.002875691279768944 + 0.01 * 6.76539421081543
Epoch 990, val loss: 1.5072144269943237
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 2.02662992477417 = 1.940661907196045 + 0.01 * 8.596809387207031
Epoch 0, val loss: 1.938751459121704
Epoch 10, training loss: 2.0170700550079346 = 1.9311027526855469 + 0.01 * 8.596739768981934
Epoch 10, val loss: 1.9296153783798218
Epoch 20, training loss: 2.0052199363708496 = 1.9192553758621216 + 0.01 * 8.596447944641113
Epoch 20, val loss: 1.917837142944336
Epoch 30, training loss: 1.9884110689163208 = 1.9024574756622314 + 0.01 * 8.595359802246094
Epoch 30, val loss: 1.9008355140686035
Epoch 40, training loss: 1.9632084369659424 = 1.8773301839828491 + 0.01 * 8.587824821472168
Epoch 40, val loss: 1.8756266832351685
Epoch 50, training loss: 1.9274545907974243 = 1.8420796394348145 + 0.01 * 8.537500381469727
Epoch 50, val loss: 1.841928243637085
Epoch 60, training loss: 1.8853973150253296 = 1.8024697303771973 + 0.01 * 8.292757987976074
Epoch 60, val loss: 1.8078263998031616
Epoch 70, training loss: 1.84673273563385 = 1.7655186653137207 + 0.01 * 8.121406555175781
Epoch 70, val loss: 1.777207612991333
Epoch 80, training loss: 1.7975332736968994 = 1.7176432609558105 + 0.01 * 7.9890007972717285
Epoch 80, val loss: 1.7338342666625977
Epoch 90, training loss: 1.7287428379058838 = 1.6499724388122559 + 0.01 * 7.877035140991211
Epoch 90, val loss: 1.6735241413116455
Epoch 100, training loss: 1.6406452655792236 = 1.5627235174179077 + 0.01 * 7.792179584503174
Epoch 100, val loss: 1.6008585691452026
Epoch 110, training loss: 1.5464203357696533 = 1.4702609777450562 + 0.01 * 7.615934371948242
Epoch 110, val loss: 1.5269296169281006
Epoch 120, training loss: 1.4574660062789917 = 1.3841854333877563 + 0.01 * 7.328060150146484
Epoch 120, val loss: 1.4611648321151733
Epoch 130, training loss: 1.3773610591888428 = 1.3044812679290771 + 0.01 * 7.287978172302246
Epoch 130, val loss: 1.40218186378479
Epoch 140, training loss: 1.2998523712158203 = 1.227347731590271 + 0.01 * 7.250470161437988
Epoch 140, val loss: 1.3464345932006836
Epoch 150, training loss: 1.2240731716156006 = 1.1518449783325195 + 0.01 * 7.222823619842529
Epoch 150, val loss: 1.2933344841003418
Epoch 160, training loss: 1.151332974433899 = 1.0792722702026367 + 0.01 * 7.206066608428955
Epoch 160, val loss: 1.244551181793213
Epoch 170, training loss: 1.082521915435791 = 1.0105388164520264 + 0.01 * 7.198315620422363
Epoch 170, val loss: 1.1993266344070435
Epoch 180, training loss: 1.0175808668136597 = 0.94564288854599 + 0.01 * 7.193803310394287
Epoch 180, val loss: 1.156755805015564
Epoch 190, training loss: 0.9555303454399109 = 0.8836377263069153 + 0.01 * 7.1892595291137695
Epoch 190, val loss: 1.1148414611816406
Epoch 200, training loss: 0.8946818113327026 = 0.8228476643562317 + 0.01 * 7.183416366577148
Epoch 200, val loss: 1.0728943347930908
Epoch 210, training loss: 0.8338820338249207 = 0.7621243000030518 + 0.01 * 7.17577600479126
Epoch 210, val loss: 1.0303958654403687
Epoch 220, training loss: 0.7733167409896851 = 0.7016611695289612 + 0.01 * 7.165555953979492
Epoch 220, val loss: 0.9882864356040955
Epoch 230, training loss: 0.713988721370697 = 0.6424806118011475 + 0.01 * 7.150808811187744
Epoch 230, val loss: 0.9477046132087708
Epoch 240, training loss: 0.6568293571472168 = 0.5855319499969482 + 0.01 * 7.129742622375488
Epoch 240, val loss: 0.9100181460380554
Epoch 250, training loss: 0.6022205352783203 = 0.5311666131019592 + 0.01 * 7.10538911819458
Epoch 250, val loss: 0.8758148550987244
Epoch 260, training loss: 0.5500073432922363 = 0.47920873761177063 + 0.01 * 7.079863548278809
Epoch 260, val loss: 0.8448908925056458
Epoch 270, training loss: 0.49965861439704895 = 0.42903587222099304 + 0.01 * 7.062274932861328
Epoch 270, val loss: 0.8169291615486145
Epoch 280, training loss: 0.4503093659877777 = 0.37979021668434143 + 0.01 * 7.051914215087891
Epoch 280, val loss: 0.7920657396316528
Epoch 290, training loss: 0.40150269865989685 = 0.33107325434684753 + 0.01 * 7.042943954467773
Epoch 290, val loss: 0.7709072232246399
Epoch 300, training loss: 0.35416895151138306 = 0.28379055857658386 + 0.01 * 7.037838935852051
Epoch 300, val loss: 0.7548200488090515
Epoch 310, training loss: 0.31041738390922546 = 0.24007566273212433 + 0.01 * 7.0341715812683105
Epoch 310, val loss: 0.745269775390625
Epoch 320, training loss: 0.27234604954719543 = 0.20202414691448212 + 0.01 * 7.032190322875977
Epoch 320, val loss: 0.7428112030029297
Epoch 330, training loss: 0.24080726504325867 = 0.17053227126598358 + 0.01 * 7.027499198913574
Epoch 330, val loss: 0.7465880513191223
Epoch 340, training loss: 0.21531419456005096 = 0.1450728476047516 + 0.01 * 7.024134635925293
Epoch 340, val loss: 0.7545920014381409
Epoch 350, training loss: 0.19468441605567932 = 0.12447444349527359 + 0.01 * 7.020998001098633
Epoch 350, val loss: 0.7651685476303101
Epoch 360, training loss: 0.17774467170238495 = 0.10758373141288757 + 0.01 * 7.016094207763672
Epoch 360, val loss: 0.7773762345314026
Epoch 370, training loss: 0.16361641883850098 = 0.09350898116827011 + 0.01 * 7.010743618011475
Epoch 370, val loss: 0.7907572984695435
Epoch 380, training loss: 0.15183167159557343 = 0.08164434880018234 + 0.01 * 7.01873254776001
Epoch 380, val loss: 0.8050084114074707
Epoch 390, training loss: 0.14162655174732208 = 0.07157237827777863 + 0.01 * 7.005417823791504
Epoch 390, val loss: 0.8197799324989319
Epoch 400, training loss: 0.1329112946987152 = 0.06298401951789856 + 0.01 * 6.992728233337402
Epoch 400, val loss: 0.8349398374557495
Epoch 410, training loss: 0.1255134642124176 = 0.05564693734049797 + 0.01 * 6.986652374267578
Epoch 410, val loss: 0.8503387570381165
Epoch 420, training loss: 0.11922033131122589 = 0.04938188195228577 + 0.01 * 6.983844757080078
Epoch 420, val loss: 0.8657941818237305
Epoch 430, training loss: 0.11380473524332047 = 0.04402764141559601 + 0.01 * 6.9777092933654785
Epoch 430, val loss: 0.8810782432556152
Epoch 440, training loss: 0.10911296308040619 = 0.039433930069208145 + 0.01 * 6.967903137207031
Epoch 440, val loss: 0.8961994647979736
Epoch 450, training loss: 0.10510160028934479 = 0.035476427525281906 + 0.01 * 6.962517738342285
Epoch 450, val loss: 0.9110991954803467
Epoch 460, training loss: 0.10162647068500519 = 0.032054297626018524 + 0.01 * 6.957217693328857
Epoch 460, val loss: 0.9257209897041321
Epoch 470, training loss: 0.0986020639538765 = 0.029084928333759308 + 0.01 * 6.951714038848877
Epoch 470, val loss: 0.9400414228439331
Epoch 480, training loss: 0.09600181132555008 = 0.02649940364062786 + 0.01 * 6.950240612030029
Epoch 480, val loss: 0.9539773464202881
Epoch 490, training loss: 0.09368591010570526 = 0.02423633076250553 + 0.01 * 6.944958209991455
Epoch 490, val loss: 0.9676187038421631
Epoch 500, training loss: 0.09165177494287491 = 0.022246627137064934 + 0.01 * 6.94051456451416
Epoch 500, val loss: 0.9809063673019409
Epoch 510, training loss: 0.08994477987289429 = 0.020489808171987534 + 0.01 * 6.945497035980225
Epoch 510, val loss: 0.9938585162162781
Epoch 520, training loss: 0.08830490708351135 = 0.018933655694127083 + 0.01 * 6.9371256828308105
Epoch 520, val loss: 1.0065124034881592
Epoch 530, training loss: 0.08689508587121964 = 0.01754864864051342 + 0.01 * 6.934643745422363
Epoch 530, val loss: 1.0187972784042358
Epoch 540, training loss: 0.08558177947998047 = 0.016311723738908768 + 0.01 * 6.927005290985107
Epoch 540, val loss: 1.0307683944702148
Epoch 550, training loss: 0.08449822664260864 = 0.015202580019831657 + 0.01 * 6.929564952850342
Epoch 550, val loss: 1.0424238443374634
Epoch 560, training loss: 0.0834260880947113 = 0.01420510932803154 + 0.01 * 6.922097682952881
Epoch 560, val loss: 1.053748369216919
Epoch 570, training loss: 0.08253934234380722 = 0.013304753229022026 + 0.01 * 6.923458576202393
Epoch 570, val loss: 1.064752459526062
Epoch 580, training loss: 0.08163759857416153 = 0.012489995919167995 + 0.01 * 6.914760589599609
Epoch 580, val loss: 1.0754649639129639
Epoch 590, training loss: 0.08094906806945801 = 0.011750197038054466 + 0.01 * 6.919887065887451
Epoch 590, val loss: 1.0858471393585205
Epoch 600, training loss: 0.08017449826002121 = 0.011077282950282097 + 0.01 * 6.909721851348877
Epoch 600, val loss: 1.0959774255752563
Epoch 610, training loss: 0.07955434918403625 = 0.010462934151291847 + 0.01 * 6.909142017364502
Epoch 610, val loss: 1.1058273315429688
Epoch 620, training loss: 0.07892332971096039 = 0.009900467470288277 + 0.01 * 6.902286529541016
Epoch 620, val loss: 1.1154327392578125
Epoch 630, training loss: 0.07853339612483978 = 0.009384213015437126 + 0.01 * 6.914918422698975
Epoch 630, val loss: 1.1247130632400513
Epoch 640, training loss: 0.07792279869318008 = 0.008910228498280048 + 0.01 * 6.901256561279297
Epoch 640, val loss: 1.133817195892334
Epoch 650, training loss: 0.0774422362446785 = 0.008473225869238377 + 0.01 * 6.896901607513428
Epoch 650, val loss: 1.1426244974136353
Epoch 660, training loss: 0.07697582989931107 = 0.008069472387433052 + 0.01 * 6.890636444091797
Epoch 660, val loss: 1.1512154340744019
Epoch 670, training loss: 0.07668138295412064 = 0.0076955645345151424 + 0.01 * 6.898582458496094
Epoch 670, val loss: 1.1595466136932373
Epoch 680, training loss: 0.07624344527721405 = 0.007349606603384018 + 0.01 * 6.8893842697143555
Epoch 680, val loss: 1.1677266359329224
Epoch 690, training loss: 0.07587694376707077 = 0.007028316147625446 + 0.01 * 6.884862899780273
Epoch 690, val loss: 1.1756254434585571
Epoch 700, training loss: 0.07552303373813629 = 0.006729247979819775 + 0.01 * 6.8793792724609375
Epoch 700, val loss: 1.1833724975585938
Epoch 710, training loss: 0.07523706555366516 = 0.006450258195400238 + 0.01 * 6.878681182861328
Epoch 710, val loss: 1.1909147500991821
Epoch 720, training loss: 0.0749325230717659 = 0.006189541891217232 + 0.01 * 6.874298095703125
Epoch 720, val loss: 1.1983246803283691
Epoch 730, training loss: 0.07474296540021896 = 0.0059461346827447414 + 0.01 * 6.879683017730713
Epoch 730, val loss: 1.2053933143615723
Epoch 740, training loss: 0.07439890503883362 = 0.00571906752884388 + 0.01 * 6.867983818054199
Epoch 740, val loss: 1.2124121189117432
Epoch 750, training loss: 0.07418026775121689 = 0.005506064277142286 + 0.01 * 6.8674211502075195
Epoch 750, val loss: 1.2192176580429077
Epoch 760, training loss: 0.07392840832471848 = 0.005305702332407236 + 0.01 * 6.862271308898926
Epoch 760, val loss: 1.2258583307266235
Epoch 770, training loss: 0.07374183088541031 = 0.00511719798669219 + 0.01 * 6.862462997436523
Epoch 770, val loss: 1.2323689460754395
Epoch 780, training loss: 0.07361795753240585 = 0.0049396236427128315 + 0.01 * 6.867833614349365
Epoch 780, val loss: 1.2387278079986572
Epoch 790, training loss: 0.07330947369337082 = 0.00477256067097187 + 0.01 * 6.853691577911377
Epoch 790, val loss: 1.2449803352355957
Epoch 800, training loss: 0.07332485914230347 = 0.004614872857928276 + 0.01 * 6.870998382568359
Epoch 800, val loss: 1.2509872913360596
Epoch 810, training loss: 0.07297234237194061 = 0.004466055892407894 + 0.01 * 6.850628852844238
Epoch 810, val loss: 1.2569608688354492
Epoch 820, training loss: 0.07279417663812637 = 0.0043254210613667965 + 0.01 * 6.8468756675720215
Epoch 820, val loss: 1.2627111673355103
Epoch 830, training loss: 0.07270649075508118 = 0.0041921548545360565 + 0.01 * 6.851433753967285
Epoch 830, val loss: 1.2683650255203247
Epoch 840, training loss: 0.07251013070344925 = 0.004065983928740025 + 0.01 * 6.844414710998535
Epoch 840, val loss: 1.2738624811172485
Epoch 850, training loss: 0.07238060981035233 = 0.003946373704820871 + 0.01 * 6.843423843383789
Epoch 850, val loss: 1.2792469263076782
Epoch 860, training loss: 0.07222560793161392 = 0.00383312557823956 + 0.01 * 6.8392486572265625
Epoch 860, val loss: 1.284521460533142
Epoch 870, training loss: 0.07203824073076248 = 0.003725451184436679 + 0.01 * 6.8312788009643555
Epoch 870, val loss: 1.2896374464035034
Epoch 880, training loss: 0.07188957184553146 = 0.0036230343393981457 + 0.01 * 6.826654434204102
Epoch 880, val loss: 1.294700026512146
Epoch 890, training loss: 0.07192286849021912 = 0.0035255413968116045 + 0.01 * 6.839733123779297
Epoch 890, val loss: 1.2996143102645874
Epoch 900, training loss: 0.07162461429834366 = 0.0034327865578234196 + 0.01 * 6.819182872772217
Epoch 900, val loss: 1.3044100999832153
Epoch 910, training loss: 0.07154291123151779 = 0.0033445144072175026 + 0.01 * 6.819839954376221
Epoch 910, val loss: 1.3090945482254028
Epoch 920, training loss: 0.07137133926153183 = 0.0032602499704807997 + 0.01 * 6.81110954284668
Epoch 920, val loss: 1.3137483596801758
Epoch 930, training loss: 0.07132505625486374 = 0.0031796914990991354 + 0.01 * 6.8145365715026855
Epoch 930, val loss: 1.3181618452072144
Epoch 940, training loss: 0.07135578244924545 = 0.0031028802040964365 + 0.01 * 6.825290203094482
Epoch 940, val loss: 1.3226209878921509
Epoch 950, training loss: 0.07118547707796097 = 0.0030298156198114157 + 0.01 * 6.815566062927246
Epoch 950, val loss: 1.3269317150115967
Epoch 960, training loss: 0.07101402431726456 = 0.002959765726700425 + 0.01 * 6.805426120758057
Epoch 960, val loss: 1.3311455249786377
Epoch 970, training loss: 0.07093919813632965 = 0.0028929163236171007 + 0.01 * 6.804628372192383
Epoch 970, val loss: 1.3352138996124268
Epoch 980, training loss: 0.07080946117639542 = 0.002828746335580945 + 0.01 * 6.798071384429932
Epoch 980, val loss: 1.339263916015625
Epoch 990, training loss: 0.07075771689414978 = 0.002767302095890045 + 0.01 * 6.799041271209717
Epoch 990, val loss: 1.3431799411773682
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8371112282551397
The final CL Acc:0.80494, 0.01720, The final GNN Acc:0.84256, 0.00531
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0421531200408936 = 1.9561846256256104 + 0.01 * 8.596848487854004
Epoch 0, val loss: 1.9538465738296509
Epoch 10, training loss: 2.032424211502075 = 1.9464560747146606 + 0.01 * 8.596817970275879
Epoch 10, val loss: 1.9439586400985718
Epoch 20, training loss: 2.021348476409912 = 1.9353816509246826 + 0.01 * 8.596671104431152
Epoch 20, val loss: 1.9325767755508423
Epoch 30, training loss: 2.0066676139831543 = 1.9207048416137695 + 0.01 * 8.596270561218262
Epoch 30, val loss: 1.9176115989685059
Epoch 40, training loss: 1.9854727983474731 = 1.899526834487915 + 0.01 * 8.594594955444336
Epoch 40, val loss: 1.8963441848754883
Epoch 50, training loss: 1.9543086290359497 = 1.8684850931167603 + 0.01 * 8.582351684570312
Epoch 50, val loss: 1.865843415260315
Epoch 60, training loss: 1.9126676321029663 = 1.827620506286621 + 0.01 * 8.504714965820312
Epoch 60, val loss: 1.827674150466919
Epoch 70, training loss: 1.8683298826217651 = 1.786467432975769 + 0.01 * 8.186240196228027
Epoch 70, val loss: 1.7917011976242065
Epoch 80, training loss: 1.8285812139511108 = 1.7487610578536987 + 0.01 * 7.982015609741211
Epoch 80, val loss: 1.7565850019454956
Epoch 90, training loss: 1.7763322591781616 = 1.698594093322754 + 0.01 * 7.773814678192139
Epoch 90, val loss: 1.7087767124176025
Epoch 100, training loss: 1.7072863578796387 = 1.6309607028961182 + 0.01 * 7.632561206817627
Epoch 100, val loss: 1.649609923362732
Epoch 110, training loss: 1.619511365890503 = 1.5449808835983276 + 0.01 * 7.453049659729004
Epoch 110, val loss: 1.5763264894485474
Epoch 120, training loss: 1.5189841985702515 = 1.4457576274871826 + 0.01 * 7.32265567779541
Epoch 120, val loss: 1.492857575416565
Epoch 130, training loss: 1.4162169694900513 = 1.3437831401824951 + 0.01 * 7.2433881759643555
Epoch 130, val loss: 1.4103901386260986
Epoch 140, training loss: 1.3167322874069214 = 1.2448490858078003 + 0.01 * 7.188323497772217
Epoch 140, val loss: 1.3330552577972412
Epoch 150, training loss: 1.221086025238037 = 1.1495035886764526 + 0.01 * 7.158241271972656
Epoch 150, val loss: 1.2607545852661133
Epoch 160, training loss: 1.1278307437896729 = 1.0563976764678955 + 0.01 * 7.143312931060791
Epoch 160, val loss: 1.19180166721344
Epoch 170, training loss: 1.0365116596221924 = 0.9651569128036499 + 0.01 * 7.135475158691406
Epoch 170, val loss: 1.1250348091125488
Epoch 180, training loss: 0.9482595920562744 = 0.8769335150718689 + 0.01 * 7.132607460021973
Epoch 180, val loss: 1.0615473985671997
Epoch 190, training loss: 0.8653570413589478 = 0.794050931930542 + 0.01 * 7.130610942840576
Epoch 190, val loss: 1.0030871629714966
Epoch 200, training loss: 0.7893228530883789 = 0.7180349826812744 + 0.01 * 7.128787040710449
Epoch 200, val loss: 0.951400101184845
Epoch 210, training loss: 0.7200954556465149 = 0.6488324403762817 + 0.01 * 7.1262993812561035
Epoch 210, val loss: 0.9068374633789062
Epoch 220, training loss: 0.6565060615539551 = 0.5852651000022888 + 0.01 * 7.124096870422363
Epoch 220, val loss: 0.8692097663879395
Epoch 230, training loss: 0.5971496105194092 = 0.5259414911270142 + 0.01 * 7.120812892913818
Epoch 230, val loss: 0.8372781276702881
Epoch 240, training loss: 0.5410460233688354 = 0.4698651134967804 + 0.01 * 7.118094444274902
Epoch 240, val loss: 0.8099832534790039
Epoch 250, training loss: 0.4879918098449707 = 0.41684287786483765 + 0.01 * 7.114893913269043
Epoch 250, val loss: 0.787064790725708
Epoch 260, training loss: 0.43846502900123596 = 0.36735665798187256 + 0.01 * 7.110836982727051
Epoch 260, val loss: 0.7687942981719971
Epoch 270, training loss: 0.39311614632606506 = 0.32204166054725647 + 0.01 * 7.107449054718018
Epoch 270, val loss: 0.7555879354476929
Epoch 280, training loss: 0.35227862000465393 = 0.28124645352363586 + 0.01 * 7.103217601776123
Epoch 280, val loss: 0.7477260231971741
Epoch 290, training loss: 0.3159486949443817 = 0.24497771263122559 + 0.01 * 7.097099304199219
Epoch 290, val loss: 0.7447912693023682
Epoch 300, training loss: 0.2839202582836151 = 0.21295733749866486 + 0.01 * 7.096292972564697
Epoch 300, val loss: 0.7462748885154724
Epoch 310, training loss: 0.2557635009288788 = 0.1849142462015152 + 0.01 * 7.084925651550293
Epoch 310, val loss: 0.7513113021850586
Epoch 320, training loss: 0.23138046264648438 = 0.1605662703514099 + 0.01 * 7.081419467926025
Epoch 320, val loss: 0.7593955397605896
Epoch 330, training loss: 0.21035084128379822 = 0.1396017223596573 + 0.01 * 7.074911117553711
Epoch 330, val loss: 0.7700017690658569
Epoch 340, training loss: 0.19232717156410217 = 0.12166255712509155 + 0.01 * 7.066461086273193
Epoch 340, val loss: 0.7826509475708008
Epoch 350, training loss: 0.1769789457321167 = 0.10635850578546524 + 0.01 * 7.062044143676758
Epoch 350, val loss: 0.7969085574150085
Epoch 360, training loss: 0.1639268845319748 = 0.09330666810274124 + 0.01 * 7.062021732330322
Epoch 360, val loss: 0.8123138546943665
Epoch 370, training loss: 0.1527484655380249 = 0.0821598544716835 + 0.01 * 7.0588603019714355
Epoch 370, val loss: 0.8285347819328308
Epoch 380, training loss: 0.14315274357795715 = 0.07261952012777328 + 0.01 * 7.053321838378906
Epoch 380, val loss: 0.845253050327301
Epoch 390, training loss: 0.13490328192710876 = 0.06443315744400024 + 0.01 * 7.047011852264404
Epoch 390, val loss: 0.8623047471046448
Epoch 400, training loss: 0.1277962028980255 = 0.057386793196201324 + 0.01 * 7.04094123840332
Epoch 400, val loss: 0.879479706287384
Epoch 410, training loss: 0.12168227136135101 = 0.05130600184202194 + 0.01 * 7.037627220153809
Epoch 410, val loss: 0.8966441750526428
Epoch 420, training loss: 0.11644329875707626 = 0.04604192078113556 + 0.01 * 7.040138244628906
Epoch 420, val loss: 0.913748025894165
Epoch 430, training loss: 0.11179758608341217 = 0.0414750911295414 + 0.01 * 7.032249450683594
Epoch 430, val loss: 0.9306114912033081
Epoch 440, training loss: 0.10774888843297958 = 0.0375002846121788 + 0.01 * 7.024860858917236
Epoch 440, val loss: 0.947249174118042
Epoch 450, training loss: 0.10422441363334656 = 0.03402893245220184 + 0.01 * 7.019548416137695
Epoch 450, val loss: 0.9636094570159912
Epoch 460, training loss: 0.10113202780485153 = 0.03098752535879612 + 0.01 * 7.014450550079346
Epoch 460, val loss: 0.9796201586723328
Epoch 470, training loss: 0.0984160378575325 = 0.028314946219325066 + 0.01 * 7.010108947753906
Epoch 470, val loss: 0.9952142834663391
Epoch 480, training loss: 0.09601657092571259 = 0.025959493592381477 + 0.01 * 7.005707740783691
Epoch 480, val loss: 1.0104560852050781
Epoch 490, training loss: 0.09391023963689804 = 0.02387458272278309 + 0.01 * 7.003565788269043
Epoch 490, val loss: 1.0252888202667236
Epoch 500, training loss: 0.09195244312286377 = 0.02202294021844864 + 0.01 * 6.992950439453125
Epoch 500, val loss: 1.0396956205368042
Epoch 510, training loss: 0.0903107225894928 = 0.020372776314616203 + 0.01 * 6.9937944412231445
Epoch 510, val loss: 1.0537147521972656
Epoch 520, training loss: 0.08873550593852997 = 0.018896708264946938 + 0.01 * 6.983880043029785
Epoch 520, val loss: 1.0673836469650269
Epoch 530, training loss: 0.08739206194877625 = 0.01757083088159561 + 0.01 * 6.982123374938965
Epoch 530, val loss: 1.0806678533554077
Epoch 540, training loss: 0.08608406037092209 = 0.016376009210944176 + 0.01 * 6.9708051681518555
Epoch 540, val loss: 1.0936464071273804
Epoch 550, training loss: 0.08514058589935303 = 0.015296434052288532 + 0.01 * 6.984415054321289
Epoch 550, val loss: 1.1062556505203247
Epoch 560, training loss: 0.08392595499753952 = 0.014320231042802334 + 0.01 * 6.960572242736816
Epoch 560, val loss: 1.1185603141784668
Epoch 570, training loss: 0.08298081904649734 = 0.013433784246444702 + 0.01 * 6.9547038078308105
Epoch 570, val loss: 1.1305233240127563
Epoch 580, training loss: 0.0821896493434906 = 0.012627708725631237 + 0.01 * 6.9561944007873535
Epoch 580, val loss: 1.1421504020690918
Epoch 590, training loss: 0.08137909322977066 = 0.011894235387444496 + 0.01 * 6.948485851287842
Epoch 590, val loss: 1.1534180641174316
Epoch 600, training loss: 0.08064636588096619 = 0.011223811656236649 + 0.01 * 6.942255020141602
Epoch 600, val loss: 1.1644315719604492
Epoch 610, training loss: 0.08004611730575562 = 0.010609282180666924 + 0.01 * 6.943683624267578
Epoch 610, val loss: 1.175134539604187
Epoch 620, training loss: 0.07935292273759842 = 0.010045676492154598 + 0.01 * 6.930724620819092
Epoch 620, val loss: 1.1855106353759766
Epoch 630, training loss: 0.07884608954191208 = 0.009527207352221012 + 0.01 * 6.931888580322266
Epoch 630, val loss: 1.195659875869751
Epoch 640, training loss: 0.07833636552095413 = 0.009049862623214722 + 0.01 * 6.928650379180908
Epoch 640, val loss: 1.2054858207702637
Epoch 650, training loss: 0.07786258310079575 = 0.008609467186033726 + 0.01 * 6.92531156539917
Epoch 650, val loss: 1.2150325775146484
Epoch 660, training loss: 0.07744727283716202 = 0.008202788420021534 + 0.01 * 6.924448490142822
Epoch 660, val loss: 1.2243841886520386
Epoch 670, training loss: 0.07690411806106567 = 0.007825739681720734 + 0.01 * 6.907837867736816
Epoch 670, val loss: 1.2334572076797485
Epoch 680, training loss: 0.07646557688713074 = 0.007475532125681639 + 0.01 * 6.8990044593811035
Epoch 680, val loss: 1.2422972917556763
Epoch 690, training loss: 0.07642696797847748 = 0.0071497224271297455 + 0.01 * 6.927724838256836
Epoch 690, val loss: 1.2509814500808716
Epoch 700, training loss: 0.07585122436285019 = 0.006847640965133905 + 0.01 * 6.9003586769104
Epoch 700, val loss: 1.2592880725860596
Epoch 710, training loss: 0.07550927996635437 = 0.006565863266587257 + 0.01 * 6.894341945648193
Epoch 710, val loss: 1.267492413520813
Epoch 720, training loss: 0.07514677941799164 = 0.006302625872194767 + 0.01 * 6.884415626525879
Epoch 720, val loss: 1.2754579782485962
Epoch 730, training loss: 0.07484865933656693 = 0.006056469399482012 + 0.01 * 6.879219055175781
Epoch 730, val loss: 1.2832210063934326
Epoch 740, training loss: 0.0748494490981102 = 0.005826207809150219 + 0.01 * 6.902324676513672
Epoch 740, val loss: 1.290816307067871
Epoch 750, training loss: 0.0744234025478363 = 0.0056107668206095695 + 0.01 * 6.881263732910156
Epoch 750, val loss: 1.2980966567993164
Epoch 760, training loss: 0.07416310161352158 = 0.005408376920968294 + 0.01 * 6.8754730224609375
Epoch 760, val loss: 1.3053264617919922
Epoch 770, training loss: 0.07377587258815765 = 0.0052182115614414215 + 0.01 * 6.855766296386719
Epoch 770, val loss: 1.3123042583465576
Epoch 780, training loss: 0.07376440614461899 = 0.005038954317569733 + 0.01 * 6.87254524230957
Epoch 780, val loss: 1.3191789388656616
Epoch 790, training loss: 0.07337702065706253 = 0.0048703383654356 + 0.01 * 6.850667953491211
Epoch 790, val loss: 1.3258402347564697
Epoch 800, training loss: 0.0733453780412674 = 0.004711229354143143 + 0.01 * 6.863415241241455
Epoch 800, val loss: 1.3324286937713623
Epoch 810, training loss: 0.07315947115421295 = 0.0045613693073391914 + 0.01 * 6.8598103523254395
Epoch 810, val loss: 1.3387038707733154
Epoch 820, training loss: 0.07278873771429062 = 0.004419358912855387 + 0.01 * 6.83693790435791
Epoch 820, val loss: 1.3449729681015015
Epoch 830, training loss: 0.07291514426469803 = 0.004284916445612907 + 0.01 * 6.863022804260254
Epoch 830, val loss: 1.3511275053024292
Epoch 840, training loss: 0.07259997725486755 = 0.004157772287726402 + 0.01 * 6.844220161437988
Epoch 840, val loss: 1.356991171836853
Epoch 850, training loss: 0.07227637618780136 = 0.004037237260490656 + 0.01 * 6.823914051055908
Epoch 850, val loss: 1.362795114517212
Epoch 860, training loss: 0.07223869860172272 = 0.003922662232071161 + 0.01 * 6.83160400390625
Epoch 860, val loss: 1.368620753288269
Epoch 870, training loss: 0.07210444658994675 = 0.0038145584985613823 + 0.01 * 6.828989028930664
Epoch 870, val loss: 1.3739838600158691
Epoch 880, training loss: 0.07203790545463562 = 0.0037109944969415665 + 0.01 * 6.832691669464111
Epoch 880, val loss: 1.3794819116592407
Epoch 890, training loss: 0.07173076272010803 = 0.003612954169511795 + 0.01 * 6.811781406402588
Epoch 890, val loss: 1.3847559690475464
Epoch 900, training loss: 0.07180168479681015 = 0.0035190293565392494 + 0.01 * 6.828266143798828
Epoch 900, val loss: 1.3899809122085571
Epoch 910, training loss: 0.07154227793216705 = 0.003429956967011094 + 0.01 * 6.811232089996338
Epoch 910, val loss: 1.3950918912887573
Epoch 920, training loss: 0.07150658965110779 = 0.003344934433698654 + 0.01 * 6.816165447235107
Epoch 920, val loss: 1.4000540971755981
Epoch 930, training loss: 0.07132422178983688 = 0.0032637270633131266 + 0.01 * 6.806049823760986
Epoch 930, val loss: 1.4048871994018555
Epoch 940, training loss: 0.07120799273252487 = 0.003186168847605586 + 0.01 * 6.802182674407959
Epoch 940, val loss: 1.409725308418274
Epoch 950, training loss: 0.07108934968709946 = 0.0031121643260121346 + 0.01 * 6.797718524932861
Epoch 950, val loss: 1.4143425226211548
Epoch 960, training loss: 0.07128623127937317 = 0.003041272982954979 + 0.01 * 6.824495792388916
Epoch 960, val loss: 1.418956995010376
Epoch 970, training loss: 0.07081979513168335 = 0.0029734978452324867 + 0.01 * 6.784629821777344
Epoch 970, val loss: 1.423406720161438
Epoch 980, training loss: 0.07071349769830704 = 0.002908470807597041 + 0.01 * 6.780502796173096
Epoch 980, val loss: 1.4278173446655273
Epoch 990, training loss: 0.07072222232818604 = 0.0028461222536861897 + 0.01 * 6.7876105308532715
Epoch 990, val loss: 1.4320998191833496
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 2.0295238494873047 = 1.943555235862732 + 0.01 * 8.5968599319458
Epoch 0, val loss: 1.9441189765930176
Epoch 10, training loss: 2.0195813179016113 = 1.9336131811141968 + 0.01 * 8.596809387207031
Epoch 10, val loss: 1.9345440864562988
Epoch 20, training loss: 2.0070927143096924 = 1.921126365661621 + 0.01 * 8.59662914276123
Epoch 20, val loss: 1.9221796989440918
Epoch 30, training loss: 1.9894171953201294 = 1.903457522392273 + 0.01 * 8.595967292785645
Epoch 30, val loss: 1.9044322967529297
Epoch 40, training loss: 1.9634774923324585 = 1.8775594234466553 + 0.01 * 8.591803550720215
Epoch 40, val loss: 1.8787200450897217
Epoch 50, training loss: 1.928175926208496 = 1.8425456285476685 + 0.01 * 8.563033103942871
Epoch 50, val loss: 1.8456337451934814
Epoch 60, training loss: 1.8910460472106934 = 1.8066213130950928 + 0.01 * 8.442474365234375
Epoch 60, val loss: 1.8149182796478271
Epoch 70, training loss: 1.8610708713531494 = 1.7780522108078003 + 0.01 * 8.301871299743652
Epoch 70, val loss: 1.790096640586853
Epoch 80, training loss: 1.8222919702529907 = 1.7405284643173218 + 0.01 * 8.176350593566895
Epoch 80, val loss: 1.754904866218567
Epoch 90, training loss: 1.7665971517562866 = 1.6871381998062134 + 0.01 * 7.94589376449585
Epoch 90, val loss: 1.7082672119140625
Epoch 100, training loss: 1.6909455060958862 = 1.6135503053665161 + 0.01 * 7.739522933959961
Epoch 100, val loss: 1.6475924253463745
Epoch 110, training loss: 1.601029396057129 = 1.524424433708191 + 0.01 * 7.660495758056641
Epoch 110, val loss: 1.5754841566085815
Epoch 120, training loss: 1.5074352025985718 = 1.431377649307251 + 0.01 * 7.605759143829346
Epoch 120, val loss: 1.502004623413086
Epoch 130, training loss: 1.4165971279144287 = 1.3412659168243408 + 0.01 * 7.5331220626831055
Epoch 130, val loss: 1.4315212965011597
Epoch 140, training loss: 1.3275913000106812 = 1.2532175779342651 + 0.01 * 7.437376022338867
Epoch 140, val loss: 1.3643184900283813
Epoch 150, training loss: 1.2407201528549194 = 1.1672189235687256 + 0.01 * 7.350122928619385
Epoch 150, val loss: 1.3006011247634888
Epoch 160, training loss: 1.1582105159759521 = 1.0854557752609253 + 0.01 * 7.275472640991211
Epoch 160, val loss: 1.2417244911193848
Epoch 170, training loss: 1.0818830728530884 = 1.009545087814331 + 0.01 * 7.233795166015625
Epoch 170, val loss: 1.188132405281067
Epoch 180, training loss: 1.0110464096069336 = 0.9390645623207092 + 0.01 * 7.198180675506592
Epoch 180, val loss: 1.1382396221160889
Epoch 190, training loss: 0.9441545009613037 = 0.8723945617675781 + 0.01 * 7.175994396209717
Epoch 190, val loss: 1.0903143882751465
Epoch 200, training loss: 0.8800460696220398 = 0.808427631855011 + 0.01 * 7.161846160888672
Epoch 200, val loss: 1.0442156791687012
Epoch 210, training loss: 0.8186103105545044 = 0.7470722198486328 + 0.01 * 7.153811931610107
Epoch 210, val loss: 1.001172423362732
Epoch 220, training loss: 0.7601104378700256 = 0.6886408925056458 + 0.01 * 7.146954536437988
Epoch 220, val loss: 0.9630927443504333
Epoch 230, training loss: 0.7045575976371765 = 0.633136510848999 + 0.01 * 7.142107009887695
Epoch 230, val loss: 0.9315267205238342
Epoch 240, training loss: 0.6516504883766174 = 0.5801901817321777 + 0.01 * 7.146031856536865
Epoch 240, val loss: 0.9068994522094727
Epoch 250, training loss: 0.6007291078567505 = 0.5293788313865662 + 0.01 * 7.135026454925537
Epoch 250, val loss: 0.8886606097221375
Epoch 260, training loss: 0.552006721496582 = 0.48067402839660645 + 0.01 * 7.133266925811768
Epoch 260, val loss: 0.8763306736946106
Epoch 270, training loss: 0.5056793689727783 = 0.43437546491622925 + 0.01 * 7.130392551422119
Epoch 270, val loss: 0.8694056868553162
Epoch 280, training loss: 0.4624498784542084 = 0.39109838008880615 + 0.01 * 7.135149955749512
Epoch 280, val loss: 0.8674805760383606
Epoch 290, training loss: 0.42267388105392456 = 0.3513873517513275 + 0.01 * 7.128654479980469
Epoch 290, val loss: 0.8703190088272095
Epoch 300, training loss: 0.3866265118122101 = 0.3153681457042694 + 0.01 * 7.125836372375488
Epoch 300, val loss: 0.8775449395179749
Epoch 310, training loss: 0.35391223430633545 = 0.2826758325099945 + 0.01 * 7.123640537261963
Epoch 310, val loss: 0.8882982134819031
Epoch 320, training loss: 0.3239310681819916 = 0.2527162730693817 + 0.01 * 7.1214799880981445
Epoch 320, val loss: 0.9019217491149902
Epoch 330, training loss: 0.2962029278278351 = 0.22500668466091156 + 0.01 * 7.11962366104126
Epoch 330, val loss: 0.9178972244262695
Epoch 340, training loss: 0.27056631445884705 = 0.1993529349565506 + 0.01 * 7.121337413787842
Epoch 340, val loss: 0.9360694885253906
Epoch 350, training loss: 0.24694189429283142 = 0.17578080296516418 + 0.01 * 7.116108417510986
Epoch 350, val loss: 0.9560366272926331
Epoch 360, training loss: 0.2255266010761261 = 0.154377281665802 + 0.01 * 7.114932537078857
Epoch 360, val loss: 0.9776669144630432
Epoch 370, training loss: 0.20635515451431274 = 0.1352269947528839 + 0.01 * 7.1128153800964355
Epoch 370, val loss: 1.0007209777832031
Epoch 380, training loss: 0.18941903114318848 = 0.11830371618270874 + 0.01 * 7.111532688140869
Epoch 380, val loss: 1.0250505208969116
Epoch 390, training loss: 0.17467425763607025 = 0.10350233316421509 + 0.01 * 7.117192268371582
Epoch 390, val loss: 1.0503432750701904
Epoch 400, training loss: 0.1617443561553955 = 0.09065279364585876 + 0.01 * 7.109157085418701
Epoch 400, val loss: 1.0761386156082153
Epoch 410, training loss: 0.15061244368553162 = 0.07954355329275131 + 0.01 * 7.1068902015686035
Epoch 410, val loss: 1.1020050048828125
Epoch 420, training loss: 0.14100900292396545 = 0.06997033208608627 + 0.01 * 7.103868007659912
Epoch 420, val loss: 1.127686619758606
Epoch 430, training loss: 0.13274846971035004 = 0.06173555180430412 + 0.01 * 7.101291656494141
Epoch 430, val loss: 1.1529850959777832
Epoch 440, training loss: 0.12564057111740112 = 0.054655883461236954 + 0.01 * 7.09846830368042
Epoch 440, val loss: 1.1775535345077515
Epoch 450, training loss: 0.11958420276641846 = 0.048564109951257706 + 0.01 * 7.102008819580078
Epoch 450, val loss: 1.2012484073638916
Epoch 460, training loss: 0.11431637406349182 = 0.043312717229127884 + 0.01 * 7.10036563873291
Epoch 460, val loss: 1.224099040031433
Epoch 470, training loss: 0.10968375951051712 = 0.03877904266119003 + 0.01 * 7.0904717445373535
Epoch 470, val loss: 1.2460147142410278
Epoch 480, training loss: 0.1057485044002533 = 0.03485184535384178 + 0.01 * 7.089666366577148
Epoch 480, val loss: 1.2669754028320312
Epoch 490, training loss: 0.10226916521787643 = 0.03144200146198273 + 0.01 * 7.082716464996338
Epoch 490, val loss: 1.2869843244552612
Epoch 500, training loss: 0.09929251670837402 = 0.028472639620304108 + 0.01 * 7.0819878578186035
Epoch 500, val loss: 1.3060868978500366
Epoch 510, training loss: 0.09668436646461487 = 0.025879355147480965 + 0.01 * 7.080501556396484
Epoch 510, val loss: 1.3243149518966675
Epoch 520, training loss: 0.09434162080287933 = 0.02360745146870613 + 0.01 * 7.073416709899902
Epoch 520, val loss: 1.3416080474853516
Epoch 530, training loss: 0.09242184460163116 = 0.021611209958791733 + 0.01 * 7.081063747406006
Epoch 530, val loss: 1.3580797910690308
Epoch 540, training loss: 0.09047883749008179 = 0.019851181656122208 + 0.01 * 7.062765121459961
Epoch 540, val loss: 1.3736803531646729
Epoch 550, training loss: 0.08887017518281937 = 0.01829279027879238 + 0.01 * 7.057738304138184
Epoch 550, val loss: 1.3885166645050049
Epoch 560, training loss: 0.0874815359711647 = 0.016910219565033913 + 0.01 * 7.057132244110107
Epoch 560, val loss: 1.4026347398757935
Epoch 570, training loss: 0.08613049238920212 = 0.015680210664868355 + 0.01 * 7.0450286865234375
Epoch 570, val loss: 1.4161336421966553
Epoch 580, training loss: 0.08498881757259369 = 0.014580325223505497 + 0.01 * 7.040849685668945
Epoch 580, val loss: 1.428968906402588
Epoch 590, training loss: 0.08394352346658707 = 0.013594931922852993 + 0.01 * 7.0348591804504395
Epoch 590, val loss: 1.441224217414856
Epoch 600, training loss: 0.083138607442379 = 0.01270989328622818 + 0.01 * 7.042871475219727
Epoch 600, val loss: 1.4529452323913574
Epoch 610, training loss: 0.08221222460269928 = 0.011912308633327484 + 0.01 * 7.02999210357666
Epoch 610, val loss: 1.464055061340332
Epoch 620, training loss: 0.08140455931425095 = 0.011190572753548622 + 0.01 * 7.021399021148682
Epoch 620, val loss: 1.4747318029403687
Epoch 630, training loss: 0.08055150508880615 = 0.010535197332501411 + 0.01 * 7.001631259918213
Epoch 630, val loss: 1.484924077987671
Epoch 640, training loss: 0.08049257844686508 = 0.009937475435435772 + 0.01 * 7.055510520935059
Epoch 640, val loss: 1.494797706604004
Epoch 650, training loss: 0.07948822528123856 = 0.009392959997057915 + 0.01 * 7.009526252746582
Epoch 650, val loss: 1.5040737390518188
Epoch 660, training loss: 0.07870519161224365 = 0.008894680999219418 + 0.01 * 6.981051445007324
Epoch 660, val loss: 1.5130168199539185
Epoch 670, training loss: 0.07814408838748932 = 0.008437152951955795 + 0.01 * 6.970693111419678
Epoch 670, val loss: 1.5216143131256104
Epoch 680, training loss: 0.07789987325668335 = 0.00801614485681057 + 0.01 * 6.988372802734375
Epoch 680, val loss: 1.529977798461914
Epoch 690, training loss: 0.07725588977336884 = 0.007629085332155228 + 0.01 * 6.962679862976074
Epoch 690, val loss: 1.5378780364990234
Epoch 700, training loss: 0.07676467299461365 = 0.007272019982337952 + 0.01 * 6.949265480041504
Epoch 700, val loss: 1.5454988479614258
Epoch 710, training loss: 0.07654611766338348 = 0.006941923871636391 + 0.01 * 6.960419654846191
Epoch 710, val loss: 1.5528243780136108
Epoch 720, training loss: 0.07621686160564423 = 0.006635561585426331 + 0.01 * 6.958130359649658
Epoch 720, val loss: 1.5599136352539062
Epoch 730, training loss: 0.07577836513519287 = 0.006351302843540907 + 0.01 * 6.942707061767578
Epoch 730, val loss: 1.5667170286178589
Epoch 740, training loss: 0.07549644261598587 = 0.006086899898946285 + 0.01 * 6.940954685211182
Epoch 740, val loss: 1.5733400583267212
Epoch 750, training loss: 0.07506589591503143 = 0.005840471014380455 + 0.01 * 6.922542572021484
Epoch 750, val loss: 1.5796494483947754
Epoch 760, training loss: 0.07486335188150406 = 0.0056106881238520145 + 0.01 * 6.925266265869141
Epoch 760, val loss: 1.5857974290847778
Epoch 770, training loss: 0.07460177689790726 = 0.005396299064159393 + 0.01 * 6.920547962188721
Epoch 770, val loss: 1.5916825532913208
Epoch 780, training loss: 0.07425995171070099 = 0.005195567850023508 + 0.01 * 6.906438827514648
Epoch 780, val loss: 1.5973957777023315
Epoch 790, training loss: 0.07407161593437195 = 0.005007233005017042 + 0.01 * 6.906438827514648
Epoch 790, val loss: 1.6029388904571533
Epoch 800, training loss: 0.07376749813556671 = 0.004830366000533104 + 0.01 * 6.893712997436523
Epoch 800, val loss: 1.6083245277404785
Epoch 810, training loss: 0.07357794791460037 = 0.004664378706365824 + 0.01 * 6.891357421875
Epoch 810, val loss: 1.6134405136108398
Epoch 820, training loss: 0.07355601340532303 = 0.004507950972765684 + 0.01 * 6.904806613922119
Epoch 820, val loss: 1.6185252666473389
Epoch 830, training loss: 0.07316362112760544 = 0.004360666964203119 + 0.01 * 6.880295276641846
Epoch 830, val loss: 1.6233694553375244
Epoch 840, training loss: 0.07305274903774261 = 0.004221788141876459 + 0.01 * 6.883095741271973
Epoch 840, val loss: 1.6281243562698364
Epoch 850, training loss: 0.07288448512554169 = 0.004090573638677597 + 0.01 * 6.879390716552734
Epoch 850, val loss: 1.6325950622558594
Epoch 860, training loss: 0.07279148697853088 = 0.003966442309319973 + 0.01 * 6.882504463195801
Epoch 860, val loss: 1.6370643377304077
Epoch 870, training loss: 0.07269422709941864 = 0.003849012777209282 + 0.01 * 6.884521484375
Epoch 870, val loss: 1.6414023637771606
Epoch 880, training loss: 0.07239626348018646 = 0.0037378096021711826 + 0.01 * 6.865845680236816
Epoch 880, val loss: 1.6454626321792603
Epoch 890, training loss: 0.07232697308063507 = 0.003632250241935253 + 0.01 * 6.869472980499268
Epoch 890, val loss: 1.6496461629867554
Epoch 900, training loss: 0.07211008667945862 = 0.003532039700075984 + 0.01 * 6.857805252075195
Epoch 900, val loss: 1.6534247398376465
Epoch 910, training loss: 0.07196533679962158 = 0.003436651546508074 + 0.01 * 6.852868556976318
Epoch 910, val loss: 1.6573576927185059
Epoch 920, training loss: 0.07185567170381546 = 0.003346038516610861 + 0.01 * 6.850963115692139
Epoch 920, val loss: 1.6609511375427246
Epoch 930, training loss: 0.07173357903957367 = 0.0032595908269286156 + 0.01 * 6.8473992347717285
Epoch 930, val loss: 1.6645417213439941
Epoch 940, training loss: 0.07184058427810669 = 0.0031772812362760305 + 0.01 * 6.866330623626709
Epoch 940, val loss: 1.6682689189910889
Epoch 950, training loss: 0.07158911228179932 = 0.0030991777312010527 + 0.01 * 6.848993301391602
Epoch 950, val loss: 1.671469807624817
Epoch 960, training loss: 0.07149770110845566 = 0.0030244674999266863 + 0.01 * 6.847323417663574
Epoch 960, val loss: 1.67477285861969
Epoch 970, training loss: 0.07128845900297165 = 0.0029530879110097885 + 0.01 * 6.8335371017456055
Epoch 970, val loss: 1.677998423576355
Epoch 980, training loss: 0.07125261425971985 = 0.002884814515709877 + 0.01 * 6.836780548095703
Epoch 980, val loss: 1.681167483329773
Epoch 990, training loss: 0.07113928347826004 = 0.002819555811583996 + 0.01 * 6.831973552703857
Epoch 990, val loss: 1.6840752363204956
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8228782287822879
=== training gcn model ===
Epoch 0, training loss: 2.035048484802246 = 1.949079990386963 + 0.01 * 8.59685230255127
Epoch 0, val loss: 1.952153205871582
Epoch 10, training loss: 2.024681806564331 = 1.9387134313583374 + 0.01 * 8.596833229064941
Epoch 10, val loss: 1.9413971900939941
Epoch 20, training loss: 2.012589454650879 = 1.9266223907470703 + 0.01 * 8.596711158752441
Epoch 20, val loss: 1.9286422729492188
Epoch 30, training loss: 1.9961885213851929 = 1.91022527217865 + 0.01 * 8.596322059631348
Epoch 30, val loss: 1.9113489389419556
Epoch 40, training loss: 1.9724737405776978 = 1.8865292072296143 + 0.01 * 8.594457626342773
Epoch 40, val loss: 1.8866158723831177
Epoch 50, training loss: 1.9394623041152954 = 1.8536583185195923 + 0.01 * 8.580395698547363
Epoch 50, val loss: 1.8532063961029053
Epoch 60, training loss: 1.9009958505630493 = 1.8159023523330688 + 0.01 * 8.509346008300781
Epoch 60, val loss: 1.8174198865890503
Epoch 70, training loss: 1.8653888702392578 = 1.7826206684112549 + 0.01 * 8.276815414428711
Epoch 70, val loss: 1.7886446714401245
Epoch 80, training loss: 1.8261985778808594 = 1.7449026107788086 + 0.01 * 8.129602432250977
Epoch 80, val loss: 1.7551881074905396
Epoch 90, training loss: 1.7701722383499146 = 1.6917692422866821 + 0.01 * 7.840298175811768
Epoch 90, val loss: 1.7076363563537598
Epoch 100, training loss: 1.6947376728057861 = 1.6184920072555542 + 0.01 * 7.624564170837402
Epoch 100, val loss: 1.6431933641433716
Epoch 110, training loss: 1.5996148586273193 = 1.524088978767395 + 0.01 * 7.5525898933410645
Epoch 110, val loss: 1.5634613037109375
Epoch 120, training loss: 1.4901443719863892 = 1.414931297302246 + 0.01 * 7.521302700042725
Epoch 120, val loss: 1.4724280834197998
Epoch 130, training loss: 1.3740713596343994 = 1.299196481704712 + 0.01 * 7.487482070922852
Epoch 130, val loss: 1.3772884607315063
Epoch 140, training loss: 1.2580790519714355 = 1.183537483215332 + 0.01 * 7.454154968261719
Epoch 140, val loss: 1.284375786781311
Epoch 150, training loss: 1.1478452682495117 = 1.0735821723937988 + 0.01 * 7.42631196975708
Epoch 150, val loss: 1.1985182762145996
Epoch 160, training loss: 1.0474541187286377 = 0.9735813736915588 + 0.01 * 7.387276649475098
Epoch 160, val loss: 1.1232441663742065
Epoch 170, training loss: 0.9584778547286987 = 0.885161817073822 + 0.01 * 7.33160400390625
Epoch 170, val loss: 1.060104250907898
Epoch 180, training loss: 0.8813247680664062 = 0.8084911704063416 + 0.01 * 7.283359527587891
Epoch 180, val loss: 1.009880781173706
Epoch 190, training loss: 0.8148906230926514 = 0.7424225807189941 + 0.01 * 7.24680757522583
Epoch 190, val loss: 0.9718790650367737
Epoch 200, training loss: 0.7567704319953918 = 0.6845846772193909 + 0.01 * 7.218576908111572
Epoch 200, val loss: 0.9436684250831604
Epoch 210, training loss: 0.7041996121406555 = 0.6322870850563049 + 0.01 * 7.191254615783691
Epoch 210, val loss: 0.9223881363868713
Epoch 220, training loss: 0.654923141002655 = 0.583296000957489 + 0.01 * 7.162714958190918
Epoch 220, val loss: 0.9053860306739807
Epoch 230, training loss: 0.6074289083480835 = 0.5360227823257446 + 0.01 * 7.140612602233887
Epoch 230, val loss: 0.8908698558807373
Epoch 240, training loss: 0.5609968900680542 = 0.4897300601005554 + 0.01 * 7.126680374145508
Epoch 240, val loss: 0.8787739276885986
Epoch 250, training loss: 0.5157570242881775 = 0.4444023668766022 + 0.01 * 7.135465145111084
Epoch 250, val loss: 0.8703075051307678
Epoch 260, training loss: 0.47186481952667236 = 0.40072742104530334 + 0.01 * 7.113738536834717
Epoch 260, val loss: 0.8668794631958008
Epoch 270, training loss: 0.43063950538635254 = 0.3596009910106659 + 0.01 * 7.103853225708008
Epoch 270, val loss: 0.8688823580741882
Epoch 280, training loss: 0.39273601770401 = 0.32178041338920593 + 0.01 * 7.0955610275268555
Epoch 280, val loss: 0.8760702013969421
Epoch 290, training loss: 0.35850024223327637 = 0.2875479459762573 + 0.01 * 7.095230579376221
Epoch 290, val loss: 0.8880276679992676
Epoch 300, training loss: 0.32761234045028687 = 0.25677698850631714 + 0.01 * 7.0835371017456055
Epoch 300, val loss: 0.9040912985801697
Epoch 310, training loss: 0.29989781975746155 = 0.22910751402378082 + 0.01 * 7.079031467437744
Epoch 310, val loss: 0.9233973026275635
Epoch 320, training loss: 0.27494293451309204 = 0.20423978567123413 + 0.01 * 7.070313930511475
Epoch 320, val loss: 0.9451941847801208
Epoch 330, training loss: 0.2525513470172882 = 0.18192414939403534 + 0.01 * 7.062719345092773
Epoch 330, val loss: 0.9690877199172974
Epoch 340, training loss: 0.23260822892189026 = 0.16195976734161377 + 0.01 * 7.064845561981201
Epoch 340, val loss: 0.9948038458824158
Epoch 350, training loss: 0.21477538347244263 = 0.14420491456985474 + 0.01 * 7.057045936584473
Epoch 350, val loss: 1.0220260620117188
Epoch 360, training loss: 0.1989353895187378 = 0.12849296629428864 + 0.01 * 7.0442423820495605
Epoch 360, val loss: 1.050520896911621
Epoch 370, training loss: 0.1850329041481018 = 0.11464047431945801 + 0.01 * 7.039242744445801
Epoch 370, val loss: 1.080154538154602
Epoch 380, training loss: 0.17283844947814941 = 0.10246399790048599 + 0.01 * 7.037445545196533
Epoch 380, val loss: 1.1105648279190063
Epoch 390, training loss: 0.16201263666152954 = 0.09173471480607986 + 0.01 * 7.027791500091553
Epoch 390, val loss: 1.1416709423065186
Epoch 400, training loss: 0.15249638259410858 = 0.08225345611572266 + 0.01 * 7.024292945861816
Epoch 400, val loss: 1.1733191013336182
Epoch 410, training loss: 0.14409758150577545 = 0.07386235892772675 + 0.01 * 7.02352237701416
Epoch 410, val loss: 1.2052993774414062
Epoch 420, training loss: 0.13669216632843018 = 0.0664292573928833 + 0.01 * 7.026290416717529
Epoch 420, val loss: 1.2374038696289062
Epoch 430, training loss: 0.12997741997241974 = 0.05983712151646614 + 0.01 * 7.014029502868652
Epoch 430, val loss: 1.269456386566162
Epoch 440, training loss: 0.12399645149707794 = 0.0539894700050354 + 0.01 * 7.000698089599609
Epoch 440, val loss: 1.3013908863067627
Epoch 450, training loss: 0.11887563019990921 = 0.04880013316869736 + 0.01 * 7.00754976272583
Epoch 450, val loss: 1.3329662084579468
Epoch 460, training loss: 0.1141711175441742 = 0.0442018061876297 + 0.01 * 6.996931076049805
Epoch 460, val loss: 1.363987922668457
Epoch 470, training loss: 0.11003325134515762 = 0.04012434929609299 + 0.01 * 6.9908905029296875
Epoch 470, val loss: 1.3944246768951416
Epoch 480, training loss: 0.10637153685092926 = 0.03650752827525139 + 0.01 * 6.986400604248047
Epoch 480, val loss: 1.4241409301757812
Epoch 490, training loss: 0.1030735969543457 = 0.03329514339566231 + 0.01 * 6.977845668792725
Epoch 490, val loss: 1.4531240463256836
Epoch 500, training loss: 0.10024502128362656 = 0.030440783128142357 + 0.01 * 6.980423927307129
Epoch 500, val loss: 1.4813395738601685
Epoch 510, training loss: 0.09760496020317078 = 0.027906086295843124 + 0.01 * 6.969888210296631
Epoch 510, val loss: 1.5086942911148071
Epoch 520, training loss: 0.09529563784599304 = 0.025649335235357285 + 0.01 * 6.964630126953125
Epoch 520, val loss: 1.535223364830017
Epoch 530, training loss: 0.0932595506310463 = 0.023635482415556908 + 0.01 * 6.962407112121582
Epoch 530, val loss: 1.560943841934204
Epoch 540, training loss: 0.09145057946443558 = 0.0218364167958498 + 0.01 * 6.961416244506836
Epoch 540, val loss: 1.5857865810394287
Epoch 550, training loss: 0.08985621482133865 = 0.02022690325975418 + 0.01 * 6.9629316329956055
Epoch 550, val loss: 1.609829068183899
Epoch 560, training loss: 0.08824878185987473 = 0.018783139064908028 + 0.01 * 6.946564674377441
Epoch 560, val loss: 1.6330372095108032
Epoch 570, training loss: 0.08720054477453232 = 0.017484212294220924 + 0.01 * 6.971633434295654
Epoch 570, val loss: 1.6555120944976807
Epoch 580, training loss: 0.08580964058637619 = 0.01631457917392254 + 0.01 * 6.949505805969238
Epoch 580, val loss: 1.6770542860031128
Epoch 590, training loss: 0.0846952497959137 = 0.015258299186825752 + 0.01 * 6.943695545196533
Epoch 590, val loss: 1.6979376077651978
Epoch 600, training loss: 0.08364386856555939 = 0.014301221817731857 + 0.01 * 6.93426513671875
Epoch 600, val loss: 1.7181252241134644
Epoch 610, training loss: 0.08288401365280151 = 0.013431940227746964 + 0.01 * 6.945207118988037
Epoch 610, val loss: 1.737605094909668
Epoch 620, training loss: 0.08189954608678818 = 0.01264173537492752 + 0.01 * 6.92578125
Epoch 620, val loss: 1.7563865184783936
Epoch 630, training loss: 0.08122330904006958 = 0.011920426972210407 + 0.01 * 6.930288314819336
Epoch 630, val loss: 1.7746487855911255
Epoch 640, training loss: 0.08045138418674469 = 0.0112617127597332 + 0.01 * 6.918966770172119
Epoch 640, val loss: 1.7922242879867554
Epoch 650, training loss: 0.07991291582584381 = 0.01065785065293312 + 0.01 * 6.925507068634033
Epoch 650, val loss: 1.8092666864395142
Epoch 660, training loss: 0.07918941229581833 = 0.010103034786880016 + 0.01 * 6.9086384773254395
Epoch 660, val loss: 1.8257781267166138
Epoch 670, training loss: 0.07862012833356857 = 0.009592464193701744 + 0.01 * 6.902766227722168
Epoch 670, val loss: 1.8417747020721436
Epoch 680, training loss: 0.07815777510404587 = 0.009121359325945377 + 0.01 * 6.903641700744629
Epoch 680, val loss: 1.857276439666748
Epoch 690, training loss: 0.07771722227334976 = 0.008686400018632412 + 0.01 * 6.903082370758057
Epoch 690, val loss: 1.8723288774490356
Epoch 700, training loss: 0.07751548290252686 = 0.008283327333629131 + 0.01 * 6.923216342926025
Epoch 700, val loss: 1.8869450092315674
Epoch 710, training loss: 0.07690294086933136 = 0.007910240441560745 + 0.01 * 6.8992695808410645
Epoch 710, val loss: 1.901048183441162
Epoch 720, training loss: 0.07643134891986847 = 0.00756312208250165 + 0.01 * 6.8868231773376465
Epoch 720, val loss: 1.914810299873352
Epoch 730, training loss: 0.07608852535486221 = 0.007239521015435457 + 0.01 * 6.8849005699157715
Epoch 730, val loss: 1.9282143115997314
Epoch 740, training loss: 0.07578322291374207 = 0.006938324309885502 + 0.01 * 6.884490013122559
Epoch 740, val loss: 1.9411901235580444
Epoch 750, training loss: 0.07538595050573349 = 0.0066574071533977985 + 0.01 * 6.872854709625244
Epoch 750, val loss: 1.9538110494613647
Epoch 760, training loss: 0.07502482086420059 = 0.006394451018422842 + 0.01 * 6.863037109375
Epoch 760, val loss: 1.9660893678665161
Epoch 770, training loss: 0.07481010258197784 = 0.006147563457489014 + 0.01 * 6.866253852844238
Epoch 770, val loss: 1.9780908823013306
Epoch 780, training loss: 0.07486032694578171 = 0.005916144233196974 + 0.01 * 6.894418716430664
Epoch 780, val loss: 1.9898699522018433
Epoch 790, training loss: 0.07438623160123825 = 0.005699189379811287 + 0.01 * 6.868704795837402
Epoch 790, val loss: 2.001101016998291
Epoch 800, training loss: 0.07402520626783371 = 0.005495377350598574 + 0.01 * 6.852982997894287
Epoch 800, val loss: 2.0121684074401855
Epoch 810, training loss: 0.07381155341863632 = 0.005303359590470791 + 0.01 * 6.8508195877075195
Epoch 810, val loss: 2.0229787826538086
Epoch 820, training loss: 0.07406274229288101 = 0.005122356116771698 + 0.01 * 6.894038677215576
Epoch 820, val loss: 2.0334720611572266
Epoch 830, training loss: 0.07333967089653015 = 0.004951870068907738 + 0.01 * 6.838780403137207
Epoch 830, val loss: 2.043644428253174
Epoch 840, training loss: 0.0732576996088028 = 0.004790713544934988 + 0.01 * 6.84669828414917
Epoch 840, val loss: 2.0536298751831055
Epoch 850, training loss: 0.07317093014717102 = 0.004638469312340021 + 0.01 * 6.853245735168457
Epoch 850, val loss: 2.063408851623535
Epoch 860, training loss: 0.07282226532697678 = 0.004494312684983015 + 0.01 * 6.8327956199646
Epoch 860, val loss: 2.072877883911133
Epoch 870, training loss: 0.07264337688684464 = 0.004357738420367241 + 0.01 * 6.828564167022705
Epoch 870, val loss: 2.082188367843628
Epoch 880, training loss: 0.0726436972618103 = 0.0042283423244953156 + 0.01 * 6.841535568237305
Epoch 880, val loss: 2.0912435054779053
Epoch 890, training loss: 0.07239849865436554 = 0.004105627536773682 + 0.01 * 6.829287528991699
Epoch 890, val loss: 2.100078582763672
Epoch 900, training loss: 0.07223247736692429 = 0.003988946322351694 + 0.01 * 6.8243536949157715
Epoch 900, val loss: 2.1087610721588135
Epoch 910, training loss: 0.07219121605157852 = 0.0038782907649874687 + 0.01 * 6.831292629241943
Epoch 910, val loss: 2.1171791553497314
Epoch 920, training loss: 0.07187053561210632 = 0.003772862022742629 + 0.01 * 6.809767723083496
Epoch 920, val loss: 2.125342607498169
Epoch 930, training loss: 0.07180075347423553 = 0.003672526217997074 + 0.01 * 6.8128228187561035
Epoch 930, val loss: 2.1334388256073
Epoch 940, training loss: 0.07168496400117874 = 0.0035769471433013678 + 0.01 * 6.810801982879639
Epoch 940, val loss: 2.141345977783203
Epoch 950, training loss: 0.0715906098484993 = 0.0034857478458434343 + 0.01 * 6.810486316680908
Epoch 950, val loss: 2.1490273475646973
Epoch 960, training loss: 0.07149945199489594 = 0.003398980712518096 + 0.01 * 6.810047626495361
Epoch 960, val loss: 2.1564669609069824
Epoch 970, training loss: 0.07133778929710388 = 0.0033161062747240067 + 0.01 * 6.802168846130371
Epoch 970, val loss: 2.1638314723968506
Epoch 980, training loss: 0.07111360132694244 = 0.0032369697000831366 + 0.01 * 6.787662982940674
Epoch 980, val loss: 2.1710093021392822
Epoch 990, training loss: 0.07108087837696075 = 0.0031613996252417564 + 0.01 * 6.791947841644287
Epoch 990, val loss: 2.178013801574707
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8165524512387982
The final CL Acc:0.75802, 0.01720, The final GNN Acc:0.81831, 0.00326
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13142])
remove edge: torch.Size([2, 7898])
updated graph: torch.Size([2, 10484])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.041995048522949 = 1.9560264348983765 + 0.01 * 8.596863746643066
Epoch 0, val loss: 1.9523166418075562
Epoch 10, training loss: 2.0308992862701416 = 1.944931149482727 + 0.01 * 8.596806526184082
Epoch 10, val loss: 1.9415267705917358
Epoch 20, training loss: 2.0169503688812256 = 1.9309840202331543 + 0.01 * 8.596625328063965
Epoch 20, val loss: 1.927423119544983
Epoch 30, training loss: 1.9970595836639404 = 1.9110991954803467 + 0.01 * 8.59604263305664
Epoch 30, val loss: 1.9069138765335083
Epoch 40, training loss: 1.967622995376587 = 1.8816943168640137 + 0.01 * 8.592862129211426
Epoch 40, val loss: 1.8769280910491943
Epoch 50, training loss: 1.9274547100067139 = 1.8417370319366455 + 0.01 * 8.571768760681152
Epoch 50, val loss: 1.8384628295898438
Epoch 60, training loss: 1.8857710361480713 = 1.8007822036743164 + 0.01 * 8.49887752532959
Epoch 60, val loss: 1.80427086353302
Epoch 70, training loss: 1.8519632816314697 = 1.7688127756118774 + 0.01 * 8.315052032470703
Epoch 70, val loss: 1.7798594236373901
Epoch 80, training loss: 1.8100193738937378 = 1.7278988361358643 + 0.01 * 8.212057113647461
Epoch 80, val loss: 1.743549108505249
Epoch 90, training loss: 1.750991702079773 = 1.670941710472107 + 0.01 * 8.00500202178955
Epoch 90, val loss: 1.6930274963378906
Epoch 100, training loss: 1.670902132987976 = 1.5934385061264038 + 0.01 * 7.746366024017334
Epoch 100, val loss: 1.6264622211456299
Epoch 110, training loss: 1.5763652324676514 = 1.500388264656067 + 0.01 * 7.597692012786865
Epoch 110, val loss: 1.5475269556045532
Epoch 120, training loss: 1.4796662330627441 = 1.403860330581665 + 0.01 * 7.5805888175964355
Epoch 120, val loss: 1.466997742652893
Epoch 130, training loss: 1.383607029914856 = 1.3080328702926636 + 0.01 * 7.557417392730713
Epoch 130, val loss: 1.3894742727279663
Epoch 140, training loss: 1.2862390279769897 = 1.210857629776001 + 0.01 * 7.538137912750244
Epoch 140, val loss: 1.3110955953598022
Epoch 150, training loss: 1.1869449615478516 = 1.1118113994598389 + 0.01 * 7.513352870941162
Epoch 150, val loss: 1.2312262058258057
Epoch 160, training loss: 1.08921217918396 = 1.014441728591919 + 0.01 * 7.477044105529785
Epoch 160, val loss: 1.153977394104004
Epoch 170, training loss: 0.9981744289398193 = 0.9238517880439758 + 0.01 * 7.4322638511657715
Epoch 170, val loss: 1.0840200185775757
Epoch 180, training loss: 0.9179983735084534 = 0.8440640568733215 + 0.01 * 7.393433570861816
Epoch 180, val loss: 1.0247881412506104
Epoch 190, training loss: 0.8503549098968506 = 0.7767185568809509 + 0.01 * 7.363636016845703
Epoch 190, val loss: 0.9778566956520081
Epoch 200, training loss: 0.7943577170372009 = 0.7211346626281738 + 0.01 * 7.322305679321289
Epoch 200, val loss: 0.9423317313194275
Epoch 210, training loss: 0.7475030422210693 = 0.6746402978897095 + 0.01 * 7.286277770996094
Epoch 210, val loss: 0.9156040549278259
Epoch 220, training loss: 0.7064512372016907 = 0.6338543891906738 + 0.01 * 7.259687423706055
Epoch 220, val loss: 0.8942216634750366
Epoch 230, training loss: 0.6680105328559875 = 0.5955849289894104 + 0.01 * 7.242559432983398
Epoch 230, val loss: 0.8752979636192322
Epoch 240, training loss: 0.6297242641448975 = 0.5574318766593933 + 0.01 * 7.229238986968994
Epoch 240, val loss: 0.8565717935562134
Epoch 250, training loss: 0.5900197625160217 = 0.5178393125534058 + 0.01 * 7.218044281005859
Epoch 250, val loss: 0.8372220396995544
Epoch 260, training loss: 0.548252284526825 = 0.47618427872657776 + 0.01 * 7.206803321838379
Epoch 260, val loss: 0.8175464868545532
Epoch 270, training loss: 0.5045289993286133 = 0.43257877230644226 + 0.01 * 7.19502592086792
Epoch 270, val loss: 0.7988958358764648
Epoch 280, training loss: 0.45967796444892883 = 0.3878415822982788 + 0.01 * 7.183637619018555
Epoch 280, val loss: 0.7831692099571228
Epoch 290, training loss: 0.41513901948928833 = 0.34339639544487 + 0.01 * 7.174261569976807
Epoch 290, val loss: 0.7720521092414856
Epoch 300, training loss: 0.3728184700012207 = 0.3012292683124542 + 0.01 * 7.158920764923096
Epoch 300, val loss: 0.7665867805480957
Epoch 310, training loss: 0.3344579339027405 = 0.26300564408302307 + 0.01 * 7.145230770111084
Epoch 310, val loss: 0.7673071622848511
Epoch 320, training loss: 0.30094748735427856 = 0.22950968146324158 + 0.01 * 7.143781661987305
Epoch 320, val loss: 0.7734574675559998
Epoch 330, training loss: 0.27186036109924316 = 0.20064707100391388 + 0.01 * 7.121328830718994
Epoch 330, val loss: 0.7840856909751892
Epoch 340, training loss: 0.24692869186401367 = 0.1758277863264084 + 0.01 * 7.110090255737305
Epoch 340, val loss: 0.7983590364456177
Epoch 350, training loss: 0.2255784273147583 = 0.15444953739643097 + 0.01 * 7.112890243530273
Epoch 350, val loss: 0.8153947591781616
Epoch 360, training loss: 0.20684929192066193 = 0.13599799573421478 + 0.01 * 7.085129737854004
Epoch 360, val loss: 0.8343793153762817
Epoch 370, training loss: 0.19082596898078918 = 0.12001672387123108 + 0.01 * 7.0809245109558105
Epoch 370, val loss: 0.8547900319099426
Epoch 380, training loss: 0.17689205706119537 = 0.10614056885242462 + 0.01 * 7.075149059295654
Epoch 380, val loss: 0.8762596249580383
Epoch 390, training loss: 0.1647261083126068 = 0.09407497197389603 + 0.01 * 7.065113544464111
Epoch 390, val loss: 0.8982730507850647
Epoch 400, training loss: 0.15412810444831848 = 0.08356692641973495 + 0.01 * 7.056118965148926
Epoch 400, val loss: 0.9206407070159912
Epoch 410, training loss: 0.14501605927944183 = 0.07440133392810822 + 0.01 * 7.0614728927612305
Epoch 410, val loss: 0.9432541728019714
Epoch 420, training loss: 0.1368444263935089 = 0.06640570610761642 + 0.01 * 7.0438714027404785
Epoch 420, val loss: 0.9658075571060181
Epoch 430, training loss: 0.12982876598834991 = 0.059419479221105576 + 0.01 * 7.040928840637207
Epoch 430, val loss: 0.9882488250732422
Epoch 440, training loss: 0.12364605069160461 = 0.053313177078962326 + 0.01 * 7.03328800201416
Epoch 440, val loss: 1.0104039907455444
Epoch 450, training loss: 0.11821137368679047 = 0.047970131039619446 + 0.01 * 7.0241241455078125
Epoch 450, val loss: 1.0322620868682861
Epoch 460, training loss: 0.11345358192920685 = 0.04328426718711853 + 0.01 * 7.016931533813477
Epoch 460, val loss: 1.053633451461792
Epoch 470, training loss: 0.1092580258846283 = 0.03916751965880394 + 0.01 * 7.009051322937012
Epoch 470, val loss: 1.074614405632019
Epoch 480, training loss: 0.10556183755397797 = 0.035550229251384735 + 0.01 * 7.001161098480225
Epoch 480, val loss: 1.0951167345046997
Epoch 490, training loss: 0.1023143082857132 = 0.03237008675932884 + 0.01 * 6.99442195892334
Epoch 490, val loss: 1.115106463432312
Epoch 500, training loss: 0.09952284395694733 = 0.029559064656496048 + 0.01 * 6.996378421783447
Epoch 500, val loss: 1.134588360786438
Epoch 510, training loss: 0.09683579206466675 = 0.027068817988038063 + 0.01 * 6.9766974449157715
Epoch 510, val loss: 1.153516411781311
Epoch 520, training loss: 0.09457780420780182 = 0.02485777996480465 + 0.01 * 6.972002029418945
Epoch 520, val loss: 1.1719310283660889
Epoch 530, training loss: 0.09253633767366409 = 0.022891022264957428 + 0.01 * 6.964531421661377
Epoch 530, val loss: 1.1898621320724487
Epoch 540, training loss: 0.09097450971603394 = 0.02113494649529457 + 0.01 * 6.983956336975098
Epoch 540, val loss: 1.207277536392212
Epoch 550, training loss: 0.08917653560638428 = 0.01956762745976448 + 0.01 * 6.960890293121338
Epoch 550, val loss: 1.2240643501281738
Epoch 560, training loss: 0.08767290413379669 = 0.018164340406656265 + 0.01 * 6.950856685638428
Epoch 560, val loss: 1.2404519319534302
Epoch 570, training loss: 0.08640942722558975 = 0.01690308190882206 + 0.01 * 6.950634956359863
Epoch 570, val loss: 1.2563706636428833
Epoch 580, training loss: 0.08518195152282715 = 0.01576698198914528 + 0.01 * 6.941496849060059
Epoch 580, val loss: 1.2717734575271606
Epoch 590, training loss: 0.08401435613632202 = 0.01473874319344759 + 0.01 * 6.927561283111572
Epoch 590, val loss: 1.2867969274520874
Epoch 600, training loss: 0.08342760056257248 = 0.013804672285914421 + 0.01 * 6.962292671203613
Epoch 600, val loss: 1.3014878034591675
Epoch 610, training loss: 0.08220893144607544 = 0.012956206686794758 + 0.01 * 6.925271987915039
Epoch 610, val loss: 1.3157449960708618
Epoch 620, training loss: 0.0813743993639946 = 0.012181540951132774 + 0.01 * 6.919285774230957
Epoch 620, val loss: 1.3296449184417725
Epoch 630, training loss: 0.0805475264787674 = 0.011472181417047977 + 0.01 * 6.907535076141357
Epoch 630, val loss: 1.3432235717773438
Epoch 640, training loss: 0.08014856278896332 = 0.010821591131389141 + 0.01 * 6.932697772979736
Epoch 640, val loss: 1.3565216064453125
Epoch 650, training loss: 0.07930454611778259 = 0.010225145146250725 + 0.01 * 6.907939910888672
Epoch 650, val loss: 1.3693711757659912
Epoch 660, training loss: 0.07866208255290985 = 0.009677323512732983 + 0.01 * 6.8984761238098145
Epoch 660, val loss: 1.3820230960845947
Epoch 670, training loss: 0.07819390296936035 = 0.009171939454972744 + 0.01 * 6.902196407318115
Epoch 670, val loss: 1.3943560123443604
Epoch 680, training loss: 0.07762887328863144 = 0.008706481195986271 + 0.01 * 6.892239093780518
Epoch 680, val loss: 1.4062232971191406
Epoch 690, training loss: 0.07708869129419327 = 0.008277068845927715 + 0.01 * 6.881162643432617
Epoch 690, val loss: 1.4178651571273804
Epoch 700, training loss: 0.0766599103808403 = 0.007879833690822124 + 0.01 * 6.8780083656311035
Epoch 700, val loss: 1.4292324781417847
Epoch 710, training loss: 0.07632360607385635 = 0.007511718198657036 + 0.01 * 6.881188869476318
Epoch 710, val loss: 1.4402905702590942
Epoch 720, training loss: 0.07590291649103165 = 0.007170131895691156 + 0.01 * 6.8732781410217285
Epoch 720, val loss: 1.4509655237197876
Epoch 730, training loss: 0.07546760141849518 = 0.006852983497083187 + 0.01 * 6.861462116241455
Epoch 730, val loss: 1.4613888263702393
Epoch 740, training loss: 0.07522425800561905 = 0.006557558663189411 + 0.01 * 6.866669654846191
Epoch 740, val loss: 1.471644401550293
Epoch 750, training loss: 0.07487854361534119 = 0.0062820627354085445 + 0.01 * 6.859648704528809
Epoch 750, val loss: 1.4815754890441895
Epoch 760, training loss: 0.07453220337629318 = 0.006024370901286602 + 0.01 * 6.850783348083496
Epoch 760, val loss: 1.4912359714508057
Epoch 770, training loss: 0.0743996724486351 = 0.00578320911154151 + 0.01 * 6.86164665222168
Epoch 770, val loss: 1.5008504390716553
Epoch 780, training loss: 0.07421734929084778 = 0.005557118449360132 + 0.01 * 6.866023063659668
Epoch 780, val loss: 1.5101797580718994
Epoch 790, training loss: 0.07383372634649277 = 0.005344420205801725 + 0.01 * 6.848930358886719
Epoch 790, val loss: 1.5192220211029053
Epoch 800, training loss: 0.07360894232988358 = 0.00514415604993701 + 0.01 * 6.846478462219238
Epoch 800, val loss: 1.5282670259475708
Epoch 810, training loss: 0.07335618883371353 = 0.004955517128109932 + 0.01 * 6.8400678634643555
Epoch 810, val loss: 1.5369839668273926
Epoch 820, training loss: 0.07316353172063828 = 0.004777715541422367 + 0.01 * 6.8385820388793945
Epoch 820, val loss: 1.5455896854400635
Epoch 830, training loss: 0.07304258644580841 = 0.004609823692589998 + 0.01 * 6.843276500701904
Epoch 830, val loss: 1.5540385246276855
Epoch 840, training loss: 0.07264691591262817 = 0.004451595712453127 + 0.01 * 6.8195319175720215
Epoch 840, val loss: 1.5622003078460693
Epoch 850, training loss: 0.07247845828533173 = 0.00430193031206727 + 0.01 * 6.817653179168701
Epoch 850, val loss: 1.570337176322937
Epoch 860, training loss: 0.07243971526622772 = 0.004160798620432615 + 0.01 * 6.827891826629639
Epoch 860, val loss: 1.5781655311584473
Epoch 870, training loss: 0.07213207334280014 = 0.0040271952748298645 + 0.01 * 6.810487747192383
Epoch 870, val loss: 1.585790753364563
Epoch 880, training loss: 0.07189156860113144 = 0.003900193842127919 + 0.01 * 6.799138069152832
Epoch 880, val loss: 1.5932713747024536
Epoch 890, training loss: 0.07183974236249924 = 0.003779806662350893 + 0.01 * 6.805994033813477
Epoch 890, val loss: 1.600764274597168
Epoch 900, training loss: 0.07175798714160919 = 0.0036657287273555994 + 0.01 * 6.809226036071777
Epoch 900, val loss: 1.6077368259429932
Epoch 910, training loss: 0.07150990515947342 = 0.0035575146321207285 + 0.01 * 6.795238494873047
Epoch 910, val loss: 1.6147043704986572
Epoch 920, training loss: 0.07143791764974594 = 0.0034547701943665743 + 0.01 * 6.798315048217773
Epoch 920, val loss: 1.6215336322784424
Epoch 930, training loss: 0.071360282599926 = 0.0033570299856364727 + 0.01 * 6.800325393676758
Epoch 930, val loss: 1.6279808282852173
Epoch 940, training loss: 0.07117786258459091 = 0.0032641023863106966 + 0.01 * 6.791376113891602
Epoch 940, val loss: 1.634456992149353
Epoch 950, training loss: 0.07100710272789001 = 0.0031757319811731577 + 0.01 * 6.78313684463501
Epoch 950, val loss: 1.6407159566879272
Epoch 960, training loss: 0.07106129825115204 = 0.003091606078669429 + 0.01 * 6.796969413757324
Epoch 960, val loss: 1.6466199159622192
Epoch 970, training loss: 0.070695661008358 = 0.0030112380627542734 + 0.01 * 6.768442153930664
Epoch 970, val loss: 1.6524312496185303
Epoch 980, training loss: 0.07052702456712723 = 0.0029346018563956022 + 0.01 * 6.759242057800293
Epoch 980, val loss: 1.658201813697815
Epoch 990, training loss: 0.07053984701633453 = 0.002861399669200182 + 0.01 * 6.767844200134277
Epoch 990, val loss: 1.6635347604751587
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.033783435821533 = 1.9478148221969604 + 0.01 * 8.59686279296875
Epoch 0, val loss: 1.9310834407806396
Epoch 10, training loss: 2.0234835147857666 = 1.9375156164169312 + 0.01 * 8.596797943115234
Epoch 10, val loss: 1.9215517044067383
Epoch 20, training loss: 2.010582208633423 = 1.9246164560317993 + 0.01 * 8.59656810760498
Epoch 20, val loss: 1.9094581604003906
Epoch 30, training loss: 1.9925062656402588 = 1.9065489768981934 + 0.01 * 8.595733642578125
Epoch 30, val loss: 1.8923852443695068
Epoch 40, training loss: 1.9659818410873413 = 1.8800750970840454 + 0.01 * 8.59067440032959
Epoch 40, val loss: 1.8676573038101196
Epoch 50, training loss: 1.928510069847107 = 1.8429635763168335 + 0.01 * 8.554646492004395
Epoch 50, val loss: 1.8343682289123535
Epoch 60, training loss: 1.883975625038147 = 1.8001583814620972 + 0.01 * 8.381730079650879
Epoch 60, val loss: 1.7997859716415405
Epoch 70, training loss: 1.8436007499694824 = 1.761885166168213 + 0.01 * 8.171557426452637
Epoch 70, val loss: 1.7715636491775513
Epoch 80, training loss: 1.7958858013153076 = 1.7167246341705322 + 0.01 * 7.9161200523376465
Epoch 80, val loss: 1.7315658330917358
Epoch 90, training loss: 1.7304128408432007 = 1.653681755065918 + 0.01 * 7.673106670379639
Epoch 90, val loss: 1.6732691526412964
Epoch 100, training loss: 1.6457784175872803 = 1.5700552463531494 + 0.01 * 7.572319507598877
Epoch 100, val loss: 1.5995113849639893
Epoch 110, training loss: 1.5459821224212646 = 1.470701813697815 + 0.01 * 7.528032302856445
Epoch 110, val loss: 1.5160490274429321
Epoch 120, training loss: 1.442075252532959 = 1.3670761585235596 + 0.01 * 7.499914646148682
Epoch 120, val loss: 1.4294679164886475
Epoch 130, training loss: 1.3403699398040771 = 1.2656413316726685 + 0.01 * 7.472862243652344
Epoch 130, val loss: 1.3472167253494263
Epoch 140, training loss: 1.2413102388381958 = 1.1669195890426636 + 0.01 * 7.43906831741333
Epoch 140, val loss: 1.2690502405166626
Epoch 150, training loss: 1.1459202766418457 = 1.0719178915023804 + 0.01 * 7.400234699249268
Epoch 150, val loss: 1.1952764987945557
Epoch 160, training loss: 1.0557948350906372 = 0.9821233749389648 + 0.01 * 7.367147445678711
Epoch 160, val loss: 1.127004623413086
Epoch 170, training loss: 0.9719964265823364 = 0.8986011147499084 + 0.01 * 7.339531898498535
Epoch 170, val loss: 1.0644886493682861
Epoch 180, training loss: 0.8950587511062622 = 0.8219273090362549 + 0.01 * 7.313143730163574
Epoch 180, val loss: 1.0083248615264893
Epoch 190, training loss: 0.8253299593925476 = 0.7523502111434937 + 0.01 * 7.297972679138184
Epoch 190, val loss: 0.9593932032585144
Epoch 200, training loss: 0.7625927925109863 = 0.6897689700126648 + 0.01 * 7.282384872436523
Epoch 200, val loss: 0.9184076189994812
Epoch 210, training loss: 0.7062397003173828 = 0.6335489153862 + 0.01 * 7.269078254699707
Epoch 210, val loss: 0.8855104446411133
Epoch 220, training loss: 0.6551511287689209 = 0.582628607749939 + 0.01 * 7.25225305557251
Epoch 220, val loss: 0.8601205945014954
Epoch 230, training loss: 0.6082742214202881 = 0.5359107851982117 + 0.01 * 7.23634147644043
Epoch 230, val loss: 0.8410113453865051
Epoch 240, training loss: 0.5647356510162354 = 0.4925077259540558 + 0.01 * 7.222793102264404
Epoch 240, val loss: 0.8268583416938782
Epoch 250, training loss: 0.5237874984741211 = 0.45183318853378296 + 0.01 * 7.195429801940918
Epoch 250, val loss: 0.8166778683662415
Epoch 260, training loss: 0.4853080213069916 = 0.41355496644973755 + 0.01 * 7.175304889678955
Epoch 260, val loss: 0.8102406859397888
Epoch 270, training loss: 0.44903531670570374 = 0.3774944543838501 + 0.01 * 7.154086112976074
Epoch 270, val loss: 0.8076481819152832
Epoch 280, training loss: 0.4149767756462097 = 0.3435462713241577 + 0.01 * 7.143049716949463
Epoch 280, val loss: 0.8090474009513855
Epoch 290, training loss: 0.38298583030700684 = 0.3116590678691864 + 0.01 * 7.132676601409912
Epoch 290, val loss: 0.8143805861473083
Epoch 300, training loss: 0.35297220945358276 = 0.28174832463264465 + 0.01 * 7.122387409210205
Epoch 300, val loss: 0.8233773708343506
Epoch 310, training loss: 0.3248882591724396 = 0.25374406576156616 + 0.01 * 7.1144185066223145
Epoch 310, val loss: 0.8358759880065918
Epoch 320, training loss: 0.298840194940567 = 0.22762927412986755 + 0.01 * 7.121092796325684
Epoch 320, val loss: 0.8515162467956543
Epoch 330, training loss: 0.27454546093940735 = 0.20344068109989166 + 0.01 * 7.110478401184082
Epoch 330, val loss: 0.8700199127197266
Epoch 340, training loss: 0.25224432349205017 = 0.18122005462646484 + 0.01 * 7.102427959442139
Epoch 340, val loss: 0.8911170363426208
Epoch 350, training loss: 0.23199068009853363 = 0.16100676357746124 + 0.01 * 7.098392009735107
Epoch 350, val loss: 0.9145767688751221
Epoch 360, training loss: 0.21377266943454742 = 0.14283387362957 + 0.01 * 7.093879699707031
Epoch 360, val loss: 0.9399553537368774
Epoch 370, training loss: 0.19758816063404083 = 0.12667380273342133 + 0.01 * 7.09143590927124
Epoch 370, val loss: 0.9669865369796753
Epoch 380, training loss: 0.18326185643672943 = 0.11242233961820602 + 0.01 * 7.083951950073242
Epoch 380, val loss: 0.9951639771461487
Epoch 390, training loss: 0.17073073983192444 = 0.09992709010839462 + 0.01 * 7.080364227294922
Epoch 390, val loss: 1.0240893363952637
Epoch 400, training loss: 0.1598011553287506 = 0.08900680392980576 + 0.01 * 7.079435348510742
Epoch 400, val loss: 1.0535236597061157
Epoch 410, training loss: 0.15023648738861084 = 0.07948100566864014 + 0.01 * 7.075547218322754
Epoch 410, val loss: 1.0832339525222778
Epoch 420, training loss: 0.14194920659065247 = 0.07117073982954025 + 0.01 * 7.077846527099609
Epoch 420, val loss: 1.1129660606384277
Epoch 430, training loss: 0.13460952043533325 = 0.06391913443803787 + 0.01 * 7.0690388679504395
Epoch 430, val loss: 1.1425825357437134
Epoch 440, training loss: 0.12818625569343567 = 0.057571109384298325 + 0.01 * 7.061514854431152
Epoch 440, val loss: 1.1719461679458618
Epoch 450, training loss: 0.1225699633359909 = 0.05199916288256645 + 0.01 * 7.057080268859863
Epoch 450, val loss: 1.200972557067871
Epoch 460, training loss: 0.11767090857028961 = 0.0470973402261734 + 0.01 * 7.057357311248779
Epoch 460, val loss: 1.2295221090316772
Epoch 470, training loss: 0.11326626688241959 = 0.04277733713388443 + 0.01 * 7.048892974853516
Epoch 470, val loss: 1.257448434829712
Epoch 480, training loss: 0.10934527963399887 = 0.03895682841539383 + 0.01 * 7.038845062255859
Epoch 480, val loss: 1.2848104238510132
Epoch 490, training loss: 0.10596796870231628 = 0.03556324914097786 + 0.01 * 7.04047155380249
Epoch 490, val loss: 1.3116371631622314
Epoch 500, training loss: 0.1028556153178215 = 0.03254711627960205 + 0.01 * 7.030850410461426
Epoch 500, val loss: 1.3377223014831543
Epoch 510, training loss: 0.10014236718416214 = 0.029857216402888298 + 0.01 * 7.028514862060547
Epoch 510, val loss: 1.3631856441497803
Epoch 520, training loss: 0.09769853204488754 = 0.027450166642665863 + 0.01 * 7.024836540222168
Epoch 520, val loss: 1.3881242275238037
Epoch 530, training loss: 0.09543226659297943 = 0.025293152779340744 + 0.01 * 7.013911247253418
Epoch 530, val loss: 1.4124037027359009
Epoch 540, training loss: 0.09354414790868759 = 0.02335895039141178 + 0.01 * 7.018519878387451
Epoch 540, val loss: 1.4359869956970215
Epoch 550, training loss: 0.09163673222064972 = 0.021625246852636337 + 0.01 * 7.001148223876953
Epoch 550, val loss: 1.4588146209716797
Epoch 560, training loss: 0.09006956219673157 = 0.02006359212100506 + 0.01 * 7.00059700012207
Epoch 560, val loss: 1.4809958934783936
Epoch 570, training loss: 0.08853769302368164 = 0.0186535082757473 + 0.01 * 6.988418102264404
Epoch 570, val loss: 1.5025129318237305
Epoch 580, training loss: 0.0872853472828865 = 0.017376594245433807 + 0.01 * 6.990875244140625
Epoch 580, val loss: 1.5234510898590088
Epoch 590, training loss: 0.08610999584197998 = 0.01621892675757408 + 0.01 * 6.989107608795166
Epoch 590, val loss: 1.5437835454940796
Epoch 600, training loss: 0.08495558798313141 = 0.015170316211879253 + 0.01 * 6.978527545928955
Epoch 600, val loss: 1.563360333442688
Epoch 610, training loss: 0.0840352475643158 = 0.014220809563994408 + 0.01 * 6.981443881988525
Epoch 610, val loss: 1.582236409187317
Epoch 620, training loss: 0.0830436572432518 = 0.013357640244066715 + 0.01 * 6.968602180480957
Epoch 620, val loss: 1.6004911661148071
Epoch 630, training loss: 0.08212456852197647 = 0.012572082690894604 + 0.01 * 6.955248832702637
Epoch 630, val loss: 1.6180474758148193
Epoch 640, training loss: 0.08135467767715454 = 0.011854511685669422 + 0.01 * 6.950016498565674
Epoch 640, val loss: 1.6350821256637573
Epoch 650, training loss: 0.08072653412818909 = 0.011198079213500023 + 0.01 * 6.952845096588135
Epoch 650, val loss: 1.6515073776245117
Epoch 660, training loss: 0.08012393116950989 = 0.010594936087727547 + 0.01 * 6.952899932861328
Epoch 660, val loss: 1.6674643754959106
Epoch 670, training loss: 0.0792430117726326 = 0.010041212663054466 + 0.01 * 6.920179843902588
Epoch 670, val loss: 1.682878017425537
Epoch 680, training loss: 0.07904727756977081 = 0.009530197829008102 + 0.01 * 6.95170783996582
Epoch 680, val loss: 1.697798490524292
Epoch 690, training loss: 0.07827077805995941 = 0.00906001403927803 + 0.01 * 6.921075820922852
Epoch 690, val loss: 1.7121825218200684
Epoch 700, training loss: 0.07768294215202332 = 0.00862504355609417 + 0.01 * 6.905789852142334
Epoch 700, val loss: 1.7261725664138794
Epoch 710, training loss: 0.07748045772314072 = 0.00822198111563921 + 0.01 * 6.92584753036499
Epoch 710, val loss: 1.7397801876068115
Epoch 720, training loss: 0.07739809155464172 = 0.007848357781767845 + 0.01 * 6.954973220825195
Epoch 720, val loss: 1.753060221672058
Epoch 730, training loss: 0.07637520134449005 = 0.007501373067498207 + 0.01 * 6.887383460998535
Epoch 730, val loss: 1.7657438516616821
Epoch 740, training loss: 0.07601376622915268 = 0.00717769144102931 + 0.01 * 6.883607864379883
Epoch 740, val loss: 1.7782268524169922
Epoch 750, training loss: 0.07598883658647537 = 0.006876005791127682 + 0.01 * 6.911283493041992
Epoch 750, val loss: 1.790364384651184
Epoch 760, training loss: 0.07548176497220993 = 0.0065950630232691765 + 0.01 * 6.888670921325684
Epoch 760, val loss: 1.8020871877670288
Epoch 770, training loss: 0.07493183016777039 = 0.006332254037261009 + 0.01 * 6.859957695007324
Epoch 770, val loss: 1.813414454460144
Epoch 780, training loss: 0.07483559101819992 = 0.006085770670324564 + 0.01 * 6.8749823570251465
Epoch 780, val loss: 1.8246212005615234
Epoch 790, training loss: 0.07450786978006363 = 0.005855330731719732 + 0.01 * 6.8652544021606445
Epoch 790, val loss: 1.8353179693222046
Epoch 800, training loss: 0.07432219386100769 = 0.005639087408781052 + 0.01 * 6.868310928344727
Epoch 800, val loss: 1.845859169960022
Epoch 810, training loss: 0.07393935322761536 = 0.005435706116259098 + 0.01 * 6.850365161895752
Epoch 810, val loss: 1.8560305833816528
Epoch 820, training loss: 0.07352081686258316 = 0.005244312807917595 + 0.01 * 6.82765007019043
Epoch 820, val loss: 1.8659776449203491
Epoch 830, training loss: 0.07351850718259811 = 0.005064129829406738 + 0.01 * 6.845437526702881
Epoch 830, val loss: 1.8756954669952393
Epoch 840, training loss: 0.07303658872842789 = 0.004894717130810022 + 0.01 * 6.814187526702881
Epoch 840, val loss: 1.8850184679031372
Epoch 850, training loss: 0.07296332716941833 = 0.004734366200864315 + 0.01 * 6.822896480560303
Epoch 850, val loss: 1.894247055053711
Epoch 860, training loss: 0.07272613048553467 = 0.004583284258842468 + 0.01 * 6.814284801483154
Epoch 860, val loss: 1.9030650854110718
Epoch 870, training loss: 0.07249351590871811 = 0.004440372344106436 + 0.01 * 6.805314064025879
Epoch 870, val loss: 1.9117892980575562
Epoch 880, training loss: 0.07231295108795166 = 0.004305030219256878 + 0.01 * 6.800792694091797
Epoch 880, val loss: 1.9202126264572144
Epoch 890, training loss: 0.07214046269655228 = 0.004176330752670765 + 0.01 * 6.796413421630859
Epoch 890, val loss: 1.9285287857055664
Epoch 900, training loss: 0.07183732837438583 = 0.004054387100040913 + 0.01 * 6.778294086456299
Epoch 900, val loss: 1.9365605115890503
Epoch 910, training loss: 0.07176957279443741 = 0.003938213922083378 + 0.01 * 6.783135890960693
Epoch 910, val loss: 1.9444421529769897
Epoch 920, training loss: 0.0719270408153534 = 0.0038273294921964407 + 0.01 * 6.809971809387207
Epoch 920, val loss: 1.9521986246109009
Epoch 930, training loss: 0.0714419037103653 = 0.003722393186762929 + 0.01 * 6.771951675415039
Epoch 930, val loss: 1.9597746133804321
Epoch 940, training loss: 0.07124972343444824 = 0.0036224089562892914 + 0.01 * 6.762731075286865
Epoch 940, val loss: 1.9670581817626953
Epoch 950, training loss: 0.07102993875741959 = 0.0035272156819701195 + 0.01 * 6.750272274017334
Epoch 950, val loss: 1.974120855331421
Epoch 960, training loss: 0.07116037607192993 = 0.0034362617880105972 + 0.01 * 6.772411823272705
Epoch 960, val loss: 1.9810876846313477
Epoch 970, training loss: 0.07126802951097488 = 0.0033493791706860065 + 0.01 * 6.791865348815918
Epoch 970, val loss: 1.9878809452056885
Epoch 980, training loss: 0.07068689912557602 = 0.003266521729528904 + 0.01 * 6.742038249969482
Epoch 980, val loss: 1.9945759773254395
Epoch 990, training loss: 0.07087677717208862 = 0.0031872354447841644 + 0.01 * 6.768954753875732
Epoch 990, val loss: 2.0010581016540527
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.0164175033569336 = 1.9304487705230713 + 0.01 * 8.59687328338623
Epoch 0, val loss: 1.9287467002868652
Epoch 10, training loss: 2.007033109664917 = 1.9210647344589233 + 0.01 * 8.596829414367676
Epoch 10, val loss: 1.919709324836731
Epoch 20, training loss: 1.99559485912323 = 1.9096280336380005 + 0.01 * 8.596678733825684
Epoch 20, val loss: 1.9083014726638794
Epoch 30, training loss: 1.9798179864883423 = 1.8938559293746948 + 0.01 * 8.59620189666748
Epoch 30, val loss: 1.892345666885376
Epoch 40, training loss: 1.9569047689437866 = 1.8709652423858643 + 0.01 * 8.59395694732666
Epoch 40, val loss: 1.8692349195480347
Epoch 50, training loss: 1.9246635437011719 = 1.838893175125122 + 0.01 * 8.577032089233398
Epoch 50, val loss: 1.838131070137024
Epoch 60, training loss: 1.885351300239563 = 1.8005520105361938 + 0.01 * 8.479931831359863
Epoch 60, val loss: 1.804141640663147
Epoch 70, training loss: 1.8437085151672363 = 1.7620328664779663 + 0.01 * 8.167566299438477
Epoch 70, val loss: 1.7723610401153564
Epoch 80, training loss: 1.7929965257644653 = 1.71305251121521 + 0.01 * 7.9944071769714355
Epoch 80, val loss: 1.7288895845413208
Epoch 90, training loss: 1.7219480276107788 = 1.644692063331604 + 0.01 * 7.72559928894043
Epoch 90, val loss: 1.6690645217895508
Epoch 100, training loss: 1.6297974586486816 = 1.5550479888916016 + 0.01 * 7.474943161010742
Epoch 100, val loss: 1.5943690538406372
Epoch 110, training loss: 1.5243250131607056 = 1.450867772102356 + 0.01 * 7.34572696685791
Epoch 110, val loss: 1.5068392753601074
Epoch 120, training loss: 1.4139248132705688 = 1.3407624959945679 + 0.01 * 7.3162360191345215
Epoch 120, val loss: 1.4160542488098145
Epoch 130, training loss: 1.304026484489441 = 1.2312337160110474 + 0.01 * 7.279280662536621
Epoch 130, val loss: 1.3267335891723633
Epoch 140, training loss: 1.1983507871627808 = 1.1257485151290894 + 0.01 * 7.260227203369141
Epoch 140, val loss: 1.2422568798065186
Epoch 150, training loss: 1.0992164611816406 = 1.0267413854599 + 0.01 * 7.247512340545654
Epoch 150, val loss: 1.164339303970337
Epoch 160, training loss: 1.0068997144699097 = 0.934541642665863 + 0.01 * 7.235811710357666
Epoch 160, val loss: 1.09273099899292
Epoch 170, training loss: 0.920025110244751 = 0.8478270173072815 + 0.01 * 7.219806671142578
Epoch 170, val loss: 1.0257868766784668
Epoch 180, training loss: 0.8377796411514282 = 0.7657783031463623 + 0.01 * 7.200134754180908
Epoch 180, val loss: 0.9622988104820251
Epoch 190, training loss: 0.7604444026947021 = 0.6886222958564758 + 0.01 * 7.182211875915527
Epoch 190, val loss: 0.9022997617721558
Epoch 200, training loss: 0.6884238123893738 = 0.6167998909950256 + 0.01 * 7.162391185760498
Epoch 200, val loss: 0.8474819660186768
Epoch 210, training loss: 0.6218662858009338 = 0.5503810048103333 + 0.01 * 7.148526668548584
Epoch 210, val loss: 0.7995578646659851
Epoch 220, training loss: 0.5610529780387878 = 0.48964783549308777 + 0.01 * 7.140512466430664
Epoch 220, val loss: 0.7599586844444275
Epoch 230, training loss: 0.5065033435821533 = 0.435139924287796 + 0.01 * 7.13633918762207
Epoch 230, val loss: 0.7290560007095337
Epoch 240, training loss: 0.458420991897583 = 0.3870792090892792 + 0.01 * 7.1341776847839355
Epoch 240, val loss: 0.7058285474777222
Epoch 250, training loss: 0.41623634099960327 = 0.34495294094085693 + 0.01 * 7.1283392906188965
Epoch 250, val loss: 0.6886388659477234
Epoch 260, training loss: 0.3789155185222626 = 0.3076697289943695 + 0.01 * 7.124579906463623
Epoch 260, val loss: 0.6758238673210144
Epoch 270, training loss: 0.34521806240081787 = 0.27400660514831543 + 0.01 * 7.121147155761719
Epoch 270, val loss: 0.6660856008529663
Epoch 280, training loss: 0.31417402625083923 = 0.24295929074287415 + 0.01 * 7.121473789215088
Epoch 280, val loss: 0.6583292484283447
Epoch 290, training loss: 0.28520089387893677 = 0.21403557062149048 + 0.01 * 7.116532325744629
Epoch 290, val loss: 0.652099609375
Epoch 300, training loss: 0.2584070563316345 = 0.18729694187641144 + 0.01 * 7.111011505126953
Epoch 300, val loss: 0.647331953048706
Epoch 310, training loss: 0.23417335748672485 = 0.163113072514534 + 0.01 * 7.106029510498047
Epoch 310, val loss: 0.6441606879234314
Epoch 320, training loss: 0.21288584172725677 = 0.14180703461170197 + 0.01 * 7.10788106918335
Epoch 320, val loss: 0.6427544355392456
Epoch 330, training loss: 0.19434985518455505 = 0.12341196835041046 + 0.01 * 7.093789577484131
Epoch 330, val loss: 0.6430566310882568
Epoch 340, training loss: 0.17857840657234192 = 0.10769969969987869 + 0.01 * 7.087871551513672
Epoch 340, val loss: 0.6450477242469788
Epoch 350, training loss: 0.16519677639007568 = 0.0943438932299614 + 0.01 * 7.085288047790527
Epoch 350, val loss: 0.6485663056373596
Epoch 360, training loss: 0.15371671319007874 = 0.08299712091684341 + 0.01 * 7.071959495544434
Epoch 360, val loss: 0.653423547744751
Epoch 370, training loss: 0.1440666913986206 = 0.07333312928676605 + 0.01 * 7.0733561515808105
Epoch 370, val loss: 0.6593988537788391
Epoch 380, training loss: 0.13567402958869934 = 0.06508748233318329 + 0.01 * 7.05865478515625
Epoch 380, val loss: 0.6663120985031128
Epoch 390, training loss: 0.12851017713546753 = 0.05802430957555771 + 0.01 * 7.048587322235107
Epoch 390, val loss: 0.6739404797554016
Epoch 400, training loss: 0.12252587080001831 = 0.05194732919335365 + 0.01 * 7.057853698730469
Epoch 400, val loss: 0.682144284248352
Epoch 410, training loss: 0.11726172268390656 = 0.04671182855963707 + 0.01 * 7.054990291595459
Epoch 410, val loss: 0.6907718777656555
Epoch 420, training loss: 0.11248083412647247 = 0.04217873886227608 + 0.01 * 7.030210018157959
Epoch 420, val loss: 0.6996178030967712
Epoch 430, training loss: 0.10847576707601547 = 0.03823014348745346 + 0.01 * 7.024562358856201
Epoch 430, val loss: 0.7086213231086731
Epoch 440, training loss: 0.10493700951337814 = 0.03477583825588226 + 0.01 * 7.016117572784424
Epoch 440, val loss: 0.7177533507347107
Epoch 450, training loss: 0.10184374451637268 = 0.031742315739393234 + 0.01 * 7.010143280029297
Epoch 450, val loss: 0.7269026041030884
Epoch 460, training loss: 0.09924390912055969 = 0.02906954474747181 + 0.01 * 7.017436504364014
Epoch 460, val loss: 0.7360068559646606
Epoch 470, training loss: 0.09671066701412201 = 0.026709258556365967 + 0.01 * 7.000141143798828
Epoch 470, val loss: 0.7450062036514282
Epoch 480, training loss: 0.09464456886053085 = 0.02461457997560501 + 0.01 * 7.0029988288879395
Epoch 480, val loss: 0.7538745403289795
Epoch 490, training loss: 0.09269613772630692 = 0.022751735523343086 + 0.01 * 6.99444055557251
Epoch 490, val loss: 0.7625601291656494
Epoch 500, training loss: 0.09095057100057602 = 0.02108840085566044 + 0.01 * 6.986217021942139
Epoch 500, val loss: 0.7710774540901184
Epoch 510, training loss: 0.08940453827381134 = 0.019597230479121208 + 0.01 * 6.980731010437012
Epoch 510, val loss: 0.7794119715690613
Epoch 520, training loss: 0.08804498612880707 = 0.018255777657032013 + 0.01 * 6.978920936584473
Epoch 520, val loss: 0.7875739336013794
Epoch 530, training loss: 0.08682629466056824 = 0.017048297449946404 + 0.01 * 6.977799415588379
Epoch 530, val loss: 0.795526385307312
Epoch 540, training loss: 0.08561733365058899 = 0.015956319868564606 + 0.01 * 6.96610164642334
Epoch 540, val loss: 0.8032670617103577
Epoch 550, training loss: 0.08458343893289566 = 0.01496694516390562 + 0.01 * 6.9616498947143555
Epoch 550, val loss: 0.8108294010162354
Epoch 560, training loss: 0.0836782306432724 = 0.014066711068153381 + 0.01 * 6.961152076721191
Epoch 560, val loss: 0.8181850910186768
Epoch 570, training loss: 0.08287079632282257 = 0.013248104602098465 + 0.01 * 6.962268829345703
Epoch 570, val loss: 0.8252910375595093
Epoch 580, training loss: 0.08196178078651428 = 0.012500814162194729 + 0.01 * 6.946096420288086
Epoch 580, val loss: 0.8322323560714722
Epoch 590, training loss: 0.08125259727239609 = 0.01181657426059246 + 0.01 * 6.943602085113525
Epoch 590, val loss: 0.8390091061592102
Epoch 600, training loss: 0.08052109181880951 = 0.011188086122274399 + 0.01 * 6.933300971984863
Epoch 600, val loss: 0.8456092476844788
Epoch 610, training loss: 0.07993533462285995 = 0.010610383935272694 + 0.01 * 6.932494640350342
Epoch 610, val loss: 0.8519861102104187
Epoch 620, training loss: 0.07935655862092972 = 0.010079016909003258 + 0.01 * 6.9277544021606445
Epoch 620, val loss: 0.8582658171653748
Epoch 630, training loss: 0.07888580858707428 = 0.009587840177118778 + 0.01 * 6.929797172546387
Epoch 630, val loss: 0.8643277883529663
Epoch 640, training loss: 0.07834800332784653 = 0.009135373868048191 + 0.01 * 6.921263217926025
Epoch 640, val loss: 0.8702168464660645
Epoch 650, training loss: 0.07791201025247574 = 0.008715215139091015 + 0.01 * 6.919680118560791
Epoch 650, val loss: 0.87600177526474
Epoch 660, training loss: 0.07742801308631897 = 0.00832576397806406 + 0.01 * 6.9102253913879395
Epoch 660, val loss: 0.8815509080886841
Epoch 670, training loss: 0.07718862593173981 = 0.007963866926729679 + 0.01 * 6.922475814819336
Epoch 670, val loss: 0.8870524764060974
Epoch 680, training loss: 0.07661190629005432 = 0.007626262027770281 + 0.01 * 6.898564338684082
Epoch 680, val loss: 0.8923226594924927
Epoch 690, training loss: 0.07631657272577286 = 0.007312088273465633 + 0.01 * 6.900448799133301
Epoch 690, val loss: 0.8975070118904114
Epoch 700, training loss: 0.07588311284780502 = 0.0070185172371566296 + 0.01 * 6.886459827423096
Epoch 700, val loss: 0.902533233165741
Epoch 710, training loss: 0.07569889724254608 = 0.006743787322193384 + 0.01 * 6.895511150360107
Epoch 710, val loss: 0.9074616432189941
Epoch 720, training loss: 0.07539842277765274 = 0.006487018428742886 + 0.01 * 6.891140460968018
Epoch 720, val loss: 0.912214994430542
Epoch 730, training loss: 0.0750758945941925 = 0.0062458147294819355 + 0.01 * 6.8830084800720215
Epoch 730, val loss: 0.9168838262557983
Epoch 740, training loss: 0.07474285364151001 = 0.006019758526235819 + 0.01 * 6.872309684753418
Epoch 740, val loss: 0.9213865995407104
Epoch 750, training loss: 0.07449981570243835 = 0.005806851200759411 + 0.01 * 6.869297027587891
Epoch 750, val loss: 0.9258331656455994
Epoch 760, training loss: 0.07418107986450195 = 0.005606715101748705 + 0.01 * 6.857436656951904
Epoch 760, val loss: 0.9301593899726868
Epoch 770, training loss: 0.07401469349861145 = 0.00541792344301939 + 0.01 * 6.859677791595459
Epoch 770, val loss: 0.9343721866607666
Epoch 780, training loss: 0.07386745512485504 = 0.005239947699010372 + 0.01 * 6.862751007080078
Epoch 780, val loss: 0.9384632110595703
Epoch 790, training loss: 0.073452889919281 = 0.005071478895843029 + 0.01 * 6.838141441345215
Epoch 790, val loss: 0.9424487352371216
Epoch 800, training loss: 0.07336590439081192 = 0.004912523552775383 + 0.01 * 6.845337867736816
Epoch 800, val loss: 0.9464098215103149
Epoch 810, training loss: 0.07399933785200119 = 0.004761450923979282 + 0.01 * 6.923789024353027
Epoch 810, val loss: 0.9501532912254333
Epoch 820, training loss: 0.07296431064605713 = 0.004619360901415348 + 0.01 * 6.8344950675964355
Epoch 820, val loss: 0.9538128972053528
Epoch 830, training loss: 0.07277125120162964 = 0.004484562203288078 + 0.01 * 6.828668594360352
Epoch 830, val loss: 0.9575080871582031
Epoch 840, training loss: 0.07262277603149414 = 0.004356593359261751 + 0.01 * 6.826618671417236
Epoch 840, val loss: 0.9609768390655518
Epoch 850, training loss: 0.07239657640457153 = 0.004234248772263527 + 0.01 * 6.816233158111572
Epoch 850, val loss: 0.9644261002540588
Epoch 860, training loss: 0.07220041751861572 = 0.0041183303110301495 + 0.01 * 6.808208465576172
Epoch 860, val loss: 0.9678422808647156
Epoch 870, training loss: 0.07212003320455551 = 0.004007540177553892 + 0.01 * 6.811249256134033
Epoch 870, val loss: 0.9711057543754578
Epoch 880, training loss: 0.07196734845638275 = 0.0039023924618959427 + 0.01 * 6.806495666503906
Epoch 880, val loss: 0.9743697643280029
Epoch 890, training loss: 0.07182247936725616 = 0.0038017763290554285 + 0.01 * 6.802070617675781
Epoch 890, val loss: 0.9775034785270691
Epoch 900, training loss: 0.07208778709173203 = 0.0037060666363686323 + 0.01 * 6.83817195892334
Epoch 900, val loss: 0.980654776096344
Epoch 910, training loss: 0.07169559597969055 = 0.003614661982282996 + 0.01 * 6.808093070983887
Epoch 910, val loss: 0.9835511445999146
Epoch 920, training loss: 0.07144462317228317 = 0.003527296707034111 + 0.01 * 6.791732311248779
Epoch 920, val loss: 0.9865682125091553
Epoch 930, training loss: 0.07127255946397781 = 0.003443995025008917 + 0.01 * 6.7828569412231445
Epoch 930, val loss: 0.9894087314605713
Epoch 940, training loss: 0.0711895003914833 = 0.0033637736923992634 + 0.01 * 6.7825727462768555
Epoch 940, val loss: 0.9921510815620422
Epoch 950, training loss: 0.0711013674736023 = 0.003287374973297119 + 0.01 * 6.781399726867676
Epoch 950, val loss: 0.9949427843093872
Epoch 960, training loss: 0.07121137529611588 = 0.0032137485686689615 + 0.01 * 6.799762725830078
Epoch 960, val loss: 0.9975954294204712
Epoch 970, training loss: 0.0709221288561821 = 0.0031433049589395523 + 0.01 * 6.7778825759887695
Epoch 970, val loss: 1.000226378440857
Epoch 980, training loss: 0.07098118215799332 = 0.003075727028772235 + 0.01 * 6.790545463562012
Epoch 980, val loss: 1.0028104782104492
Epoch 990, training loss: 0.07076282799243927 = 0.0030108229257166386 + 0.01 * 6.775200843811035
Epoch 990, val loss: 1.005273699760437
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8402741170268846
The final CL Acc:0.80370, 0.02722, The final GNN Acc:0.83852, 0.00131
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9550])
updated graph: torch.Size([2, 10606])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.046316385269165 = 1.960348129272461 + 0.01 * 8.596834182739258
Epoch 0, val loss: 1.9554884433746338
Epoch 10, training loss: 2.0358822345733643 = 1.9499143362045288 + 0.01 * 8.596793174743652
Epoch 10, val loss: 1.9447498321533203
Epoch 20, training loss: 2.023420572280884 = 1.9374544620513916 + 0.01 * 8.596609115600586
Epoch 20, val loss: 1.9318259954452515
Epoch 30, training loss: 2.0064265727996826 = 1.9204667806625366 + 0.01 * 8.595982551574707
Epoch 30, val loss: 1.9143096208572388
Epoch 40, training loss: 1.9815936088562012 = 1.8956691026687622 + 0.01 * 8.59245491027832
Epoch 40, val loss: 1.8891299962997437
Epoch 50, training loss: 1.9459043741226196 = 1.860215425491333 + 0.01 * 8.568891525268555
Epoch 50, val loss: 1.854615330696106
Epoch 60, training loss: 1.902730107307434 = 1.8180687427520752 + 0.01 * 8.466136932373047
Epoch 60, val loss: 1.8175735473632812
Epoch 70, training loss: 1.8654922246932983 = 1.7827485799789429 + 0.01 * 8.274361610412598
Epoch 70, val loss: 1.791386604309082
Epoch 80, training loss: 1.8285505771636963 = 1.7467533349990845 + 0.01 * 8.17972469329834
Epoch 80, val loss: 1.7620470523834229
Epoch 90, training loss: 1.7768886089324951 = 1.6976062059402466 + 0.01 * 7.9282402992248535
Epoch 90, val loss: 1.7186387777328491
Epoch 100, training loss: 1.7059060335159302 = 1.6294734477996826 + 0.01 * 7.643261909484863
Epoch 100, val loss: 1.6590452194213867
Epoch 110, training loss: 1.615931510925293 = 1.540753722190857 + 0.01 * 7.517776966094971
Epoch 110, val loss: 1.5842461585998535
Epoch 120, training loss: 1.5113935470581055 = 1.43696928024292 + 0.01 * 7.442424297332764
Epoch 120, val loss: 1.500054121017456
Epoch 130, training loss: 1.4009795188903809 = 1.3273091316223145 + 0.01 * 7.367033958435059
Epoch 130, val loss: 1.413162112236023
Epoch 140, training loss: 1.2936625480651855 = 1.220636248588562 + 0.01 * 7.302626132965088
Epoch 140, val loss: 1.330177903175354
Epoch 150, training loss: 1.1952261924743652 = 1.1227604150772095 + 0.01 * 7.246573448181152
Epoch 150, val loss: 1.2560691833496094
Epoch 160, training loss: 1.1085314750671387 = 1.0364514589309692 + 0.01 * 7.207999229431152
Epoch 160, val loss: 1.1924933195114136
Epoch 170, training loss: 1.0323268175125122 = 0.9604105353355408 + 0.01 * 7.191627025604248
Epoch 170, val loss: 1.137953758239746
Epoch 180, training loss: 0.9632193446159363 = 0.8914045095443726 + 0.01 * 7.181481838226318
Epoch 180, val loss: 1.0892891883850098
Epoch 190, training loss: 0.8982173800468445 = 0.8264911770820618 + 0.01 * 7.172619342803955
Epoch 190, val loss: 1.043837547302246
Epoch 200, training loss: 0.8360660672187805 = 0.7644338011741638 + 0.01 * 7.163227081298828
Epoch 200, val loss: 1.0005524158477783
Epoch 210, training loss: 0.7770347595214844 = 0.7054813504219055 + 0.01 * 7.15533971786499
Epoch 210, val loss: 0.9598761796951294
Epoch 220, training loss: 0.7215518355369568 = 0.6500908136367798 + 0.01 * 7.14610481262207
Epoch 220, val loss: 0.92308109998703
Epoch 230, training loss: 0.6694336533546448 = 0.5980451107025146 + 0.01 * 7.138856410980225
Epoch 230, val loss: 0.890714704990387
Epoch 240, training loss: 0.6199928522109985 = 0.5485769510269165 + 0.01 * 7.141592979431152
Epoch 240, val loss: 0.8626108169555664
Epoch 250, training loss: 0.5724493861198425 = 0.5011882781982422 + 0.01 * 7.126110553741455
Epoch 250, val loss: 0.8383815288543701
Epoch 260, training loss: 0.5270349383354187 = 0.45586448907852173 + 0.01 * 7.1170454025268555
Epoch 260, val loss: 0.8176882266998291
Epoch 270, training loss: 0.48394572734832764 = 0.4128545820713043 + 0.01 * 7.109113693237305
Epoch 270, val loss: 0.8005309700965881
Epoch 280, training loss: 0.44350293278694153 = 0.372482568025589 + 0.01 * 7.1020355224609375
Epoch 280, val loss: 0.7867779731750488
Epoch 290, training loss: 0.4058845639228821 = 0.3349396884441376 + 0.01 * 7.094487190246582
Epoch 290, val loss: 0.7763738036155701
Epoch 300, training loss: 0.37114012241363525 = 0.3002251982688904 + 0.01 * 7.091490745544434
Epoch 300, val loss: 0.7690188884735107
Epoch 310, training loss: 0.3390827178955078 = 0.2682143747806549 + 0.01 * 7.086835861206055
Epoch 310, val loss: 0.7643686532974243
Epoch 320, training loss: 0.30957937240600586 = 0.2387622892856598 + 0.01 * 7.081707954406738
Epoch 320, val loss: 0.7620800137519836
Epoch 330, training loss: 0.28246086835861206 = 0.21172913908958435 + 0.01 * 7.073172092437744
Epoch 330, val loss: 0.761793315410614
Epoch 340, training loss: 0.2577461302280426 = 0.187030628323555 + 0.01 * 7.071549892425537
Epoch 340, val loss: 0.7635666728019714
Epoch 350, training loss: 0.23537056148052216 = 0.164668470621109 + 0.01 * 7.07020902633667
Epoch 350, val loss: 0.7671499848365784
Epoch 360, training loss: 0.21524834632873535 = 0.14460135996341705 + 0.01 * 7.064698219299316
Epoch 360, val loss: 0.7724913954734802
Epoch 370, training loss: 0.1974155753850937 = 0.12678077816963196 + 0.01 * 7.063479900360107
Epoch 370, val loss: 0.7795658111572266
Epoch 380, training loss: 0.1817314326763153 = 0.11112105846405029 + 0.01 * 7.061037063598633
Epoch 380, val loss: 0.7881401777267456
Epoch 390, training loss: 0.1682395339012146 = 0.09749981015920639 + 0.01 * 7.073971748352051
Epoch 390, val loss: 0.7980632185935974
Epoch 400, training loss: 0.156296506524086 = 0.08573168516159058 + 0.01 * 7.056482315063477
Epoch 400, val loss: 0.8090936541557312
Epoch 410, training loss: 0.14611130952835083 = 0.0755820944905281 + 0.01 * 7.052921772003174
Epoch 410, val loss: 0.8210393786430359
Epoch 420, training loss: 0.1373780071735382 = 0.06684210151433945 + 0.01 * 7.053591728210449
Epoch 420, val loss: 0.8337071537971497
Epoch 430, training loss: 0.12979784607887268 = 0.059322454035282135 + 0.01 * 7.047539710998535
Epoch 430, val loss: 0.8468552231788635
Epoch 440, training loss: 0.12328507006168365 = 0.0528477318584919 + 0.01 * 7.043734550476074
Epoch 440, val loss: 0.8603718280792236
Epoch 450, training loss: 0.11772632598876953 = 0.04726922884583473 + 0.01 * 7.04571008682251
Epoch 450, val loss: 0.8739991784095764
Epoch 460, training loss: 0.11289200931787491 = 0.042454853653907776 + 0.01 * 7.043715953826904
Epoch 460, val loss: 0.8876305818557739
Epoch 470, training loss: 0.1086168885231018 = 0.03828687593340874 + 0.01 * 7.033000946044922
Epoch 470, val loss: 0.901221752166748
Epoch 480, training loss: 0.10491029173135757 = 0.034668244421482086 + 0.01 * 7.024204730987549
Epoch 480, val loss: 0.9146475791931152
Epoch 490, training loss: 0.1017892137169838 = 0.031517572700977325 + 0.01 * 7.027163982391357
Epoch 490, val loss: 0.9277944564819336
Epoch 500, training loss: 0.09896872192621231 = 0.028762174770236015 + 0.01 * 7.020654678344727
Epoch 500, val loss: 0.9407767057418823
Epoch 510, training loss: 0.09706225246191025 = 0.02634304203093052 + 0.01 * 7.071921348571777
Epoch 510, val loss: 0.9533993005752563
Epoch 520, training loss: 0.09436196833848953 = 0.0242180023342371 + 0.01 * 7.014397144317627
Epoch 520, val loss: 0.9657378196716309
Epoch 530, training loss: 0.09242385625839233 = 0.022336820140480995 + 0.01 * 7.008703708648682
Epoch 530, val loss: 0.9777429699897766
Epoch 540, training loss: 0.09068311005830765 = 0.02065807580947876 + 0.01 * 7.002503871917725
Epoch 540, val loss: 0.9895420670509338
Epoch 550, training loss: 0.08910435438156128 = 0.01915171928703785 + 0.01 * 6.995264053344727
Epoch 550, val loss: 1.0010343790054321
Epoch 560, training loss: 0.08800105005502701 = 0.017797166481614113 + 0.01 * 7.020388603210449
Epoch 560, val loss: 1.0122824907302856
Epoch 570, training loss: 0.08660604804754257 = 0.016582852229475975 + 0.01 * 7.0023193359375
Epoch 570, val loss: 1.0232309103012085
Epoch 580, training loss: 0.08537065237760544 = 0.015488289296627045 + 0.01 * 6.988236427307129
Epoch 580, val loss: 1.0338704586029053
Epoch 590, training loss: 0.08431966602802277 = 0.01450004056096077 + 0.01 * 6.981963157653809
Epoch 590, val loss: 1.0442172288894653
Epoch 600, training loss: 0.08340322971343994 = 0.013605757616460323 + 0.01 * 6.979747295379639
Epoch 600, val loss: 1.0543183088302612
Epoch 610, training loss: 0.0824911817908287 = 0.01279416587203741 + 0.01 * 6.969701290130615
Epoch 610, val loss: 1.0641578435897827
Epoch 620, training loss: 0.08239741623401642 = 0.012055112980306149 + 0.01 * 7.034230709075928
Epoch 620, val loss: 1.0737074613571167
Epoch 630, training loss: 0.08119256049394608 = 0.01138530857861042 + 0.01 * 6.980725288391113
Epoch 630, val loss: 1.0829771757125854
Epoch 640, training loss: 0.08036935329437256 = 0.010773982852697372 + 0.01 * 6.959536552429199
Epoch 640, val loss: 1.0919426679611206
Epoch 650, training loss: 0.0799115002155304 = 0.010214024223387241 + 0.01 * 6.969747543334961
Epoch 650, val loss: 1.100697636604309
Epoch 660, training loss: 0.0791545882821083 = 0.009700268507003784 + 0.01 * 6.945432186126709
Epoch 660, val loss: 1.1091464757919312
Epoch 670, training loss: 0.07875252515077591 = 0.009227207861840725 + 0.01 * 6.952531814575195
Epoch 670, val loss: 1.117400050163269
Epoch 680, training loss: 0.078451007604599 = 0.008790479972958565 + 0.01 * 6.966053009033203
Epoch 680, val loss: 1.1254699230194092
Epoch 690, training loss: 0.07782706618309021 = 0.008387631736695766 + 0.01 * 6.943943500518799
Epoch 690, val loss: 1.1332576274871826
Epoch 700, training loss: 0.07785454392433167 = 0.008014427497982979 + 0.01 * 6.984012126922607
Epoch 700, val loss: 1.1408487558364868
Epoch 710, training loss: 0.07702124863862991 = 0.007669227663427591 + 0.01 * 6.935202121734619
Epoch 710, val loss: 1.1483004093170166
Epoch 720, training loss: 0.07665853202342987 = 0.007348461542278528 + 0.01 * 6.931007385253906
Epoch 720, val loss: 1.1553744077682495
Epoch 730, training loss: 0.07630883157253265 = 0.007050058338791132 + 0.01 * 6.925877094268799
Epoch 730, val loss: 1.1623135805130005
Epoch 740, training loss: 0.07600502669811249 = 0.006771622225642204 + 0.01 * 6.923340320587158
Epoch 740, val loss: 1.169204831123352
Epoch 750, training loss: 0.07563089579343796 = 0.006511809770017862 + 0.01 * 6.9119086265563965
Epoch 750, val loss: 1.1757270097732544
Epoch 760, training loss: 0.07536761462688446 = 0.006269599311053753 + 0.01 * 6.909801959991455
Epoch 760, val loss: 1.1821413040161133
Epoch 770, training loss: 0.07534375786781311 = 0.006042566150426865 + 0.01 * 6.93011999130249
Epoch 770, val loss: 1.1884596347808838
Epoch 780, training loss: 0.0749264806509018 = 0.005829658359289169 + 0.01 * 6.909682273864746
Epoch 780, val loss: 1.1945339441299438
Epoch 790, training loss: 0.07475125789642334 = 0.005629374645650387 + 0.01 * 6.912188529968262
Epoch 790, val loss: 1.2004979848861694
Epoch 800, training loss: 0.07438531517982483 = 0.005441340617835522 + 0.01 * 6.894398212432861
Epoch 800, val loss: 1.2062695026397705
Epoch 810, training loss: 0.07424880564212799 = 0.005264259874820709 + 0.01 * 6.898454666137695
Epoch 810, val loss: 1.2119266986846924
Epoch 820, training loss: 0.07385244965553284 = 0.005097215063869953 + 0.01 * 6.875523567199707
Epoch 820, val loss: 1.2173951864242554
Epoch 830, training loss: 0.07372352480888367 = 0.004939724691212177 + 0.01 * 6.878379821777344
Epoch 830, val loss: 1.2227925062179565
Epoch 840, training loss: 0.073786661028862 = 0.004790690261870623 + 0.01 * 6.89959716796875
Epoch 840, val loss: 1.2280737161636353
Epoch 850, training loss: 0.07358692586421967 = 0.004649762064218521 + 0.01 * 6.893716812133789
Epoch 850, val loss: 1.2332087755203247
Epoch 860, training loss: 0.07309053838253021 = 0.004516063258051872 + 0.01 * 6.857447624206543
Epoch 860, val loss: 1.2381845712661743
Epoch 870, training loss: 0.07309538125991821 = 0.004389542154967785 + 0.01 * 6.870584487915039
Epoch 870, val loss: 1.243138313293457
Epoch 880, training loss: 0.07298478484153748 = 0.0042695216834545135 + 0.01 * 6.87152624130249
Epoch 880, val loss: 1.2479078769683838
Epoch 890, training loss: 0.0726979672908783 = 0.004155286587774754 + 0.01 * 6.854268550872803
Epoch 890, val loss: 1.2526371479034424
Epoch 900, training loss: 0.07280153036117554 = 0.004046668764203787 + 0.01 * 6.875485897064209
Epoch 900, val loss: 1.2572399377822876
Epoch 910, training loss: 0.072568379342556 = 0.003943232819437981 + 0.01 * 6.862514972686768
Epoch 910, val loss: 1.2617205381393433
Epoch 920, training loss: 0.0724220722913742 = 0.0038448588456958532 + 0.01 * 6.85772180557251
Epoch 920, val loss: 1.26609206199646
Epoch 930, training loss: 0.07219348847866058 = 0.0037507941015064716 + 0.01 * 6.844269275665283
Epoch 930, val loss: 1.2704704999923706
Epoch 940, training loss: 0.07231234759092331 = 0.0036610648967325687 + 0.01 * 6.865128040313721
Epoch 940, val loss: 1.2745897769927979
Epoch 950, training loss: 0.07190762460231781 = 0.003575395792722702 + 0.01 * 6.833222389221191
Epoch 950, val loss: 1.278817057609558
Epoch 960, training loss: 0.07183360308408737 = 0.0034935506992042065 + 0.01 * 6.834005832672119
Epoch 960, val loss: 1.2827916145324707
Epoch 970, training loss: 0.07156666368246078 = 0.0034152264706790447 + 0.01 * 6.815144062042236
Epoch 970, val loss: 1.2868870496749878
Epoch 980, training loss: 0.07182282954454422 = 0.003340119728818536 + 0.01 * 6.848271369934082
Epoch 980, val loss: 1.2906534671783447
Epoch 990, training loss: 0.07163017988204956 = 0.0032682986930012703 + 0.01 * 6.836188316345215
Epoch 990, val loss: 1.294514536857605
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.020662546157837 = 1.9346939325332642 + 0.01 * 8.5968599319458
Epoch 0, val loss: 1.9377902746200562
Epoch 10, training loss: 2.0111982822418213 = 1.9252300262451172 + 0.01 * 8.596817016601562
Epoch 10, val loss: 1.9285353422164917
Epoch 20, training loss: 1.9993592500686646 = 1.9133925437927246 + 0.01 * 8.596668243408203
Epoch 20, val loss: 1.9164224863052368
Epoch 30, training loss: 1.9825552701950073 = 1.8965927362442017 + 0.01 * 8.596248626708984
Epoch 30, val loss: 1.8987571001052856
Epoch 40, training loss: 1.9579541683197021 = 1.8720126152038574 + 0.01 * 8.59415340423584
Epoch 40, val loss: 1.8728920221328735
Epoch 50, training loss: 1.9247281551361084 = 1.838951826095581 + 0.01 * 8.577628135681152
Epoch 50, val loss: 1.8395558595657349
Epoch 60, training loss: 1.8899052143096924 = 1.8048611879348755 + 0.01 * 8.504408836364746
Epoch 60, val loss: 1.808602213859558
Epoch 70, training loss: 1.858771800994873 = 1.7761496305465698 + 0.01 * 8.26222038269043
Epoch 70, val loss: 1.7840662002563477
Epoch 80, training loss: 1.820040225982666 = 1.738426923751831 + 0.01 * 8.16132926940918
Epoch 80, val loss: 1.7505316734313965
Epoch 90, training loss: 1.7652894258499146 = 1.6855316162109375 + 0.01 * 7.975782871246338
Epoch 90, val loss: 1.7045691013336182
Epoch 100, training loss: 1.6898239850997925 = 1.6121366024017334 + 0.01 * 7.768733501434326
Epoch 100, val loss: 1.642703652381897
Epoch 110, training loss: 1.5971224308013916 = 1.521148443222046 + 0.01 * 7.597400665283203
Epoch 110, val loss: 1.5666766166687012
Epoch 120, training loss: 1.4965237379074097 = 1.4210608005523682 + 0.01 * 7.546298027038574
Epoch 120, val loss: 1.486128568649292
Epoch 130, training loss: 1.3938130140304565 = 1.318770170211792 + 0.01 * 7.5042853355407715
Epoch 130, val loss: 1.4075459241867065
Epoch 140, training loss: 1.2916673421859741 = 1.2171276807785034 + 0.01 * 7.4539642333984375
Epoch 140, val loss: 1.3317054510116577
Epoch 150, training loss: 1.1914937496185303 = 1.1174942255020142 + 0.01 * 7.399954795837402
Epoch 150, val loss: 1.258370280265808
Epoch 160, training loss: 1.0942308902740479 = 1.0208791494369507 + 0.01 * 7.335175037384033
Epoch 160, val loss: 1.1868044137954712
Epoch 170, training loss: 1.0017471313476562 = 0.9290425777435303 + 0.01 * 7.27046012878418
Epoch 170, val loss: 1.1180070638656616
Epoch 180, training loss: 0.9167332053184509 = 0.8444713950157166 + 0.01 * 7.22618293762207
Epoch 180, val loss: 1.0551868677139282
Epoch 190, training loss: 0.8410125970840454 = 0.7690126299858093 + 0.01 * 7.199995994567871
Epoch 190, val loss: 1.0011467933654785
Epoch 200, training loss: 0.7744882106781006 = 0.7026748061180115 + 0.01 * 7.181341648101807
Epoch 200, val loss: 0.9571094512939453
Epoch 210, training loss: 0.7153074741363525 = 0.6436143517494202 + 0.01 * 7.1693115234375
Epoch 210, val loss: 0.9223316311836243
Epoch 220, training loss: 0.6609421968460083 = 0.5892966389656067 + 0.01 * 7.164553642272949
Epoch 220, val loss: 0.8947513699531555
Epoch 230, training loss: 0.6094252467155457 = 0.5378285646438599 + 0.01 * 7.159667015075684
Epoch 230, val loss: 0.8720709085464478
Epoch 240, training loss: 0.5601540207862854 = 0.488574743270874 + 0.01 * 7.157927513122559
Epoch 240, val loss: 0.8529207110404968
Epoch 250, training loss: 0.5135758519172668 = 0.4420159161090851 + 0.01 * 7.1559929847717285
Epoch 250, val loss: 0.8374754190444946
Epoch 260, training loss: 0.4706365466117859 = 0.3990944027900696 + 0.01 * 7.154213905334473
Epoch 260, val loss: 0.8268319964408875
Epoch 270, training loss: 0.43222615122795105 = 0.36070334911346436 + 0.01 * 7.152280330657959
Epoch 270, val loss: 0.8218046426773071
Epoch 280, training loss: 0.3985462188720703 = 0.32704222202301025 + 0.01 * 7.150399208068848
Epoch 280, val loss: 0.8223409056663513
Epoch 290, training loss: 0.3691342771053314 = 0.29762837290763855 + 0.01 * 7.150589466094971
Epoch 290, val loss: 0.8277789354324341
Epoch 300, training loss: 0.3430292308330536 = 0.27154773473739624 + 0.01 * 7.148149013519287
Epoch 300, val loss: 0.8369666337966919
Epoch 310, training loss: 0.3192494511604309 = 0.2477966696023941 + 0.01 * 7.145278453826904
Epoch 310, val loss: 0.8485618829727173
Epoch 320, training loss: 0.29692697525024414 = 0.2254929393529892 + 0.01 * 7.143403053283691
Epoch 320, val loss: 0.861518919467926
Epoch 330, training loss: 0.27548089623451233 = 0.2040717601776123 + 0.01 * 7.140913486480713
Epoch 330, val loss: 0.8752917051315308
Epoch 340, training loss: 0.25473737716674805 = 0.18335139751434326 + 0.01 * 7.138596534729004
Epoch 340, val loss: 0.8896108269691467
Epoch 350, training loss: 0.23485508561134338 = 0.16350483894348145 + 0.01 * 7.135024547576904
Epoch 350, val loss: 0.9045366048812866
Epoch 360, training loss: 0.21621888875961304 = 0.14490355551242828 + 0.01 * 7.131532669067383
Epoch 360, val loss: 0.9203246235847473
Epoch 370, training loss: 0.19913220405578613 = 0.12786859273910522 + 0.01 * 7.126362323760986
Epoch 370, val loss: 0.9371780157089233
Epoch 380, training loss: 0.18374483287334442 = 0.11255383491516113 + 0.01 * 7.119100093841553
Epoch 380, val loss: 0.955014169216156
Epoch 390, training loss: 0.1700701117515564 = 0.0989781841635704 + 0.01 * 7.109192371368408
Epoch 390, val loss: 0.9739968776702881
Epoch 400, training loss: 0.15813134610652924 = 0.08707655966281891 + 0.01 * 7.105478763580322
Epoch 400, val loss: 0.9940736889839172
Epoch 410, training loss: 0.14762967824935913 = 0.07672251760959625 + 0.01 * 7.090716361999512
Epoch 410, val loss: 1.0150045156478882
Epoch 420, training loss: 0.1386139988899231 = 0.06777051836252213 + 0.01 * 7.084347724914551
Epoch 420, val loss: 1.0365474224090576
Epoch 430, training loss: 0.13073384761810303 = 0.06007251888513565 + 0.01 * 7.066132068634033
Epoch 430, val loss: 1.0583972930908203
Epoch 440, training loss: 0.12400628626346588 = 0.053449079394340515 + 0.01 * 7.055720806121826
Epoch 440, val loss: 1.0802500247955322
Epoch 450, training loss: 0.11845279484987259 = 0.0477428063750267 + 0.01 * 7.0709991455078125
Epoch 450, val loss: 1.101770281791687
Epoch 460, training loss: 0.11328180134296417 = 0.04282737895846367 + 0.01 * 7.045442581176758
Epoch 460, val loss: 1.122755527496338
Epoch 470, training loss: 0.10890686511993408 = 0.03857090324163437 + 0.01 * 7.033596038818359
Epoch 470, val loss: 1.1431776285171509
Epoch 480, training loss: 0.10513068735599518 = 0.034870948642492294 + 0.01 * 7.025974750518799
Epoch 480, val loss: 1.1630456447601318
Epoch 490, training loss: 0.10182755440473557 = 0.03164216876029968 + 0.01 * 7.018538475036621
Epoch 490, val loss: 1.1823201179504395
Epoch 500, training loss: 0.09893476963043213 = 0.028815317898988724 + 0.01 * 7.0119452476501465
Epoch 500, val loss: 1.2010111808776855
Epoch 510, training loss: 0.09652125090360641 = 0.026333672925829887 + 0.01 * 7.0187578201293945
Epoch 510, val loss: 1.2190872430801392
Epoch 520, training loss: 0.0941610336303711 = 0.024149488657712936 + 0.01 * 7.001155376434326
Epoch 520, val loss: 1.2365269660949707
Epoch 530, training loss: 0.09218470752239227 = 0.022220131009817123 + 0.01 * 6.996457099914551
Epoch 530, val loss: 1.2533522844314575
Epoch 540, training loss: 0.09044013917446136 = 0.020510198548436165 + 0.01 * 6.99299430847168
Epoch 540, val loss: 1.269639015197754
Epoch 550, training loss: 0.08885061740875244 = 0.018988346680998802 + 0.01 * 6.986227035522461
Epoch 550, val loss: 1.28536057472229
Epoch 560, training loss: 0.08753135800361633 = 0.01762879081070423 + 0.01 * 6.9902567863464355
Epoch 560, val loss: 1.3006186485290527
Epoch 570, training loss: 0.08613737672567368 = 0.016411421820521355 + 0.01 * 6.972595691680908
Epoch 570, val loss: 1.3153769969940186
Epoch 580, training loss: 0.08504677563905716 = 0.015317589044570923 + 0.01 * 6.972918510437012
Epoch 580, val loss: 1.3296658992767334
Epoch 590, training loss: 0.08400475978851318 = 0.014332853257656097 + 0.01 * 6.967190742492676
Epoch 590, val loss: 1.3433817625045776
Epoch 600, training loss: 0.083000048995018 = 0.013443047180771828 + 0.01 * 6.955700397491455
Epoch 600, val loss: 1.3566844463348389
Epoch 610, training loss: 0.08212487399578094 = 0.012636296451091766 + 0.01 * 6.94885778427124
Epoch 610, val loss: 1.3695964813232422
Epoch 620, training loss: 0.08138459920883179 = 0.011902891099452972 + 0.01 * 6.948171138763428
Epoch 620, val loss: 1.382072925567627
Epoch 630, training loss: 0.08070939034223557 = 0.011235296726226807 + 0.01 * 6.947409629821777
Epoch 630, val loss: 1.3941993713378906
Epoch 640, training loss: 0.08001608401536942 = 0.010625374503433704 + 0.01 * 6.939070701599121
Epoch 640, val loss: 1.4058890342712402
Epoch 650, training loss: 0.07936032861471176 = 0.010066956281661987 + 0.01 * 6.929337024688721
Epoch 650, val loss: 1.4172512292861938
Epoch 660, training loss: 0.07881959527730942 = 0.009554573334753513 + 0.01 * 6.926502227783203
Epoch 660, val loss: 1.4283270835876465
Epoch 670, training loss: 0.07839849591255188 = 0.009082863107323647 + 0.01 * 6.931563854217529
Epoch 670, val loss: 1.4391450881958008
Epoch 680, training loss: 0.07786285132169724 = 0.008647632785141468 + 0.01 * 6.9215216636657715
Epoch 680, val loss: 1.4494704008102417
Epoch 690, training loss: 0.07740060240030289 = 0.008245938457548618 + 0.01 * 6.915466785430908
Epoch 690, val loss: 1.4596737623214722
Epoch 700, training loss: 0.07693956792354584 = 0.007874167524278164 + 0.01 * 6.9065399169921875
Epoch 700, val loss: 1.4695332050323486
Epoch 710, training loss: 0.07678000628948212 = 0.007528440095484257 + 0.01 * 6.925157070159912
Epoch 710, val loss: 1.4791449308395386
Epoch 720, training loss: 0.07621365785598755 = 0.007207183632999659 + 0.01 * 6.900647163391113
Epoch 720, val loss: 1.4885140657424927
Epoch 730, training loss: 0.07587995380163193 = 0.006908153183758259 + 0.01 * 6.89717960357666
Epoch 730, val loss: 1.497621774673462
Epoch 740, training loss: 0.07553302496671677 = 0.006629052106291056 + 0.01 * 6.890397548675537
Epoch 740, val loss: 1.5064817667007446
Epoch 750, training loss: 0.07525689899921417 = 0.006368035450577736 + 0.01 * 6.888886451721191
Epoch 750, val loss: 1.5151485204696655
Epoch 760, training loss: 0.07505263388156891 = 0.006123713683336973 + 0.01 * 6.892892360687256
Epoch 760, val loss: 1.5236648321151733
Epoch 770, training loss: 0.07468332350254059 = 0.005895301699638367 + 0.01 * 6.878802299499512
Epoch 770, val loss: 1.531886100769043
Epoch 780, training loss: 0.07444676011800766 = 0.005680924281477928 + 0.01 * 6.876583576202393
Epoch 780, val loss: 1.539965271949768
Epoch 790, training loss: 0.07426564395427704 = 0.005479139741510153 + 0.01 * 6.878650665283203
Epoch 790, val loss: 1.5478500127792358
Epoch 800, training loss: 0.07406497746706009 = 0.0052896407432854176 + 0.01 * 6.877533435821533
Epoch 800, val loss: 1.555473804473877
Epoch 810, training loss: 0.07378114759922028 = 0.0051110791973769665 + 0.01 * 6.867007255554199
Epoch 810, val loss: 1.5630360841751099
Epoch 820, training loss: 0.07367372512817383 = 0.004942552652209997 + 0.01 * 6.873117446899414
Epoch 820, val loss: 1.5704407691955566
Epoch 830, training loss: 0.07345923036336899 = 0.004783543758094311 + 0.01 * 6.867569446563721
Epoch 830, val loss: 1.5775401592254639
Epoch 840, training loss: 0.07324424386024475 = 0.0046332827769219875 + 0.01 * 6.861096382141113
Epoch 840, val loss: 1.5845916271209717
Epoch 850, training loss: 0.07307708263397217 = 0.004491080529987812 + 0.01 * 6.85860013961792
Epoch 850, val loss: 1.5915101766586304
Epoch 860, training loss: 0.07284687459468842 = 0.004356409888714552 + 0.01 * 6.8490471839904785
Epoch 860, val loss: 1.598168134689331
Epoch 870, training loss: 0.07269981503486633 = 0.004228565841913223 + 0.01 * 6.847125053405762
Epoch 870, val loss: 1.6047526597976685
Epoch 880, training loss: 0.07253090292215347 = 0.004107336048036814 + 0.01 * 6.842357158660889
Epoch 880, val loss: 1.61124587059021
Epoch 890, training loss: 0.07239192724227905 = 0.003992237150669098 + 0.01 * 6.839969158172607
Epoch 890, val loss: 1.6175365447998047
Epoch 900, training loss: 0.07230306416749954 = 0.003882833756506443 + 0.01 * 6.8420233726501465
Epoch 900, val loss: 1.6237162351608276
Epoch 910, training loss: 0.0721338614821434 = 0.0037786150351166725 + 0.01 * 6.835525035858154
Epoch 910, val loss: 1.6297672986984253
Epoch 920, training loss: 0.07201863080263138 = 0.0036794778425246477 + 0.01 * 6.833915710449219
Epoch 920, val loss: 1.6356842517852783
Epoch 930, training loss: 0.0718756914138794 = 0.0035849434789270163 + 0.01 * 6.829074859619141
Epoch 930, val loss: 1.641528844833374
Epoch 940, training loss: 0.07186327874660492 = 0.003494865261018276 + 0.01 * 6.836841583251953
Epoch 940, val loss: 1.6472634077072144
Epoch 950, training loss: 0.07169345021247864 = 0.0034088394604623318 + 0.01 * 6.828461170196533
Epoch 950, val loss: 1.6527851819992065
Epoch 960, training loss: 0.07153285294771194 = 0.0033267757389694452 + 0.01 * 6.820608139038086
Epoch 960, val loss: 1.658271312713623
Epoch 970, training loss: 0.07149361819028854 = 0.0032480789814144373 + 0.01 * 6.824553966522217
Epoch 970, val loss: 1.6636683940887451
Epoch 980, training loss: 0.07137084007263184 = 0.0031730446498841047 + 0.01 * 6.819779872894287
Epoch 980, val loss: 1.6689560413360596
Epoch 990, training loss: 0.07120645046234131 = 0.0031010920647531748 + 0.01 * 6.810535430908203
Epoch 990, val loss: 1.6741410493850708
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 2.025447368621826 = 1.939479112625122 + 0.01 * 8.596814155578613
Epoch 0, val loss: 1.9349629878997803
Epoch 10, training loss: 2.0141029357910156 = 1.9281357526779175 + 0.01 * 8.596721649169922
Epoch 10, val loss: 1.9239341020584106
Epoch 20, training loss: 1.9995330572128296 = 1.9135689735412598 + 0.01 * 8.596404075622559
Epoch 20, val loss: 1.9092727899551392
Epoch 30, training loss: 1.9791710376739502 = 1.8932195901870728 + 0.01 * 8.595142364501953
Epoch 30, val loss: 1.8885242938995361
Epoch 40, training loss: 1.9507488012313843 = 1.864881157875061 + 0.01 * 8.586767196655273
Epoch 40, val loss: 1.8602914810180664
Epoch 50, training loss: 1.915834903717041 = 1.8303923606872559 + 0.01 * 8.544260025024414
Epoch 50, val loss: 1.8285359144210815
Epoch 60, training loss: 1.8831093311309814 = 1.7994425296783447 + 0.01 * 8.366679191589355
Epoch 60, val loss: 1.80424165725708
Epoch 70, training loss: 1.8533436059951782 = 1.7711372375488281 + 0.01 * 8.220635414123535
Epoch 70, val loss: 1.781829833984375
Epoch 80, training loss: 1.8121817111968994 = 1.7313640117645264 + 0.01 * 8.081769943237305
Epoch 80, val loss: 1.7475959062576294
Epoch 90, training loss: 1.7542544603347778 = 1.6753934621810913 + 0.01 * 7.886099815368652
Epoch 90, val loss: 1.6994028091430664
Epoch 100, training loss: 1.6771583557128906 = 1.6001454591751099 + 0.01 * 7.701293468475342
Epoch 100, val loss: 1.6361773014068604
Epoch 110, training loss: 1.58775794506073 = 1.5121771097183228 + 0.01 * 7.558084487915039
Epoch 110, val loss: 1.5648525953292847
Epoch 120, training loss: 1.4969408512115479 = 1.4223569631576538 + 0.01 * 7.458386421203613
Epoch 120, val loss: 1.4968551397323608
Epoch 130, training loss: 1.4082748889923096 = 1.3345149755477905 + 0.01 * 7.375988006591797
Epoch 130, val loss: 1.4333374500274658
Epoch 140, training loss: 1.3211079835891724 = 1.2478450536727905 + 0.01 * 7.326297283172607
Epoch 140, val loss: 1.3712273836135864
Epoch 150, training loss: 1.2355296611785889 = 1.1625665426254272 + 0.01 * 7.2963080406188965
Epoch 150, val loss: 1.3107285499572754
Epoch 160, training loss: 1.1533924341201782 = 1.0805976390838623 + 0.01 * 7.279477119445801
Epoch 160, val loss: 1.252846121788025
Epoch 170, training loss: 1.0758185386657715 = 1.0031285285949707 + 0.01 * 7.2689971923828125
Epoch 170, val loss: 1.1988840103149414
Epoch 180, training loss: 1.0018315315246582 = 0.92923504114151 + 0.01 * 7.259644985198975
Epoch 180, val loss: 1.148218035697937
Epoch 190, training loss: 0.9289809465408325 = 0.8564924001693726 + 0.01 * 7.248857498168945
Epoch 190, val loss: 1.098419427871704
Epoch 200, training loss: 0.8551294803619385 = 0.7827880382537842 + 0.01 * 7.2341461181640625
Epoch 200, val loss: 1.0476644039154053
Epoch 210, training loss: 0.7798758745193481 = 0.7077268362045288 + 0.01 * 7.21490478515625
Epoch 210, val loss: 0.9955692887306213
Epoch 220, training loss: 0.7048149108886719 = 0.6328797340393066 + 0.01 * 7.193517684936523
Epoch 220, val loss: 0.9441294074058533
Epoch 230, training loss: 0.6324661374092102 = 0.5607195496559143 + 0.01 * 7.174656867980957
Epoch 230, val loss: 0.8961442112922668
Epoch 240, training loss: 0.5652548670768738 = 0.4935937821865082 + 0.01 * 7.166110515594482
Epoch 240, val loss: 0.8547748327255249
Epoch 250, training loss: 0.5046880841255188 = 0.4330939054489136 + 0.01 * 7.159416675567627
Epoch 250, val loss: 0.8217190504074097
Epoch 260, training loss: 0.4511338174343109 = 0.37955841422080994 + 0.01 * 7.157540321350098
Epoch 260, val loss: 0.7970764636993408
Epoch 270, training loss: 0.4038943350315094 = 0.3323265016078949 + 0.01 * 7.156782627105713
Epoch 270, val loss: 0.780179500579834
Epoch 280, training loss: 0.36191678047180176 = 0.2903653085231781 + 0.01 * 7.155148506164551
Epoch 280, val loss: 0.7696765661239624
Epoch 290, training loss: 0.32444989681243896 = 0.2529103755950928 + 0.01 * 7.153953552246094
Epoch 290, val loss: 0.7644898295402527
Epoch 300, training loss: 0.2910131514072418 = 0.21948525309562683 + 0.01 * 7.152789115905762
Epoch 300, val loss: 0.7638283967971802
Epoch 310, training loss: 0.261369526386261 = 0.1898634135723114 + 0.01 * 7.150612831115723
Epoch 310, val loss: 0.7670412659645081
Epoch 320, training loss: 0.23537811636924744 = 0.16389025747776031 + 0.01 * 7.14878511428833
Epoch 320, val loss: 0.7735546827316284
Epoch 330, training loss: 0.21286547183990479 = 0.14139311015605927 + 0.01 * 7.147235870361328
Epoch 330, val loss: 0.7828128933906555
Epoch 340, training loss: 0.1935729682445526 = 0.12210523337125778 + 0.01 * 7.1467742919921875
Epoch 340, val loss: 0.7944098711013794
Epoch 350, training loss: 0.17713141441345215 = 0.10567940771579742 + 0.01 * 7.145201683044434
Epoch 350, val loss: 0.8078967332839966
Epoch 360, training loss: 0.16316556930541992 = 0.09173870831727982 + 0.01 * 7.142686367034912
Epoch 360, val loss: 0.82278972864151
Epoch 370, training loss: 0.15134835243225098 = 0.07992826402187347 + 0.01 * 7.142008304595947
Epoch 370, val loss: 0.8386749625205994
Epoch 380, training loss: 0.14132006466388702 = 0.06992991268634796 + 0.01 * 7.139015197753906
Epoch 380, val loss: 0.8552082180976868
Epoch 390, training loss: 0.13282130658626556 = 0.06146240234375 + 0.01 * 7.135890483856201
Epoch 390, val loss: 0.87211674451828
Epoch 400, training loss: 0.12563781440258026 = 0.05428152158856392 + 0.01 * 7.135629653930664
Epoch 400, val loss: 0.8891593813896179
Epoch 410, training loss: 0.11946931481361389 = 0.04817550256848335 + 0.01 * 7.12938117980957
Epoch 410, val loss: 0.9061402678489685
Epoch 420, training loss: 0.1142195612192154 = 0.042964186519384384 + 0.01 * 7.125537395477295
Epoch 420, val loss: 0.9229380488395691
Epoch 430, training loss: 0.10978671908378601 = 0.038499053567647934 + 0.01 * 7.128766059875488
Epoch 430, val loss: 0.9394263029098511
Epoch 440, training loss: 0.10581155121326447 = 0.034656427800655365 + 0.01 * 7.115512847900391
Epoch 440, val loss: 0.95555180311203
Epoch 450, training loss: 0.10253296047449112 = 0.03133179247379303 + 0.01 * 7.1201171875
Epoch 450, val loss: 0.9712485074996948
Epoch 460, training loss: 0.09951280802488327 = 0.02844228781759739 + 0.01 * 7.107052326202393
Epoch 460, val loss: 0.9864534735679626
Epoch 470, training loss: 0.09690015017986298 = 0.025915224105119705 + 0.01 * 7.09849214553833
Epoch 470, val loss: 1.0012151002883911
Epoch 480, training loss: 0.09462179243564606 = 0.023699145764112473 + 0.01 * 7.092264175415039
Epoch 480, val loss: 1.01559579372406
Epoch 490, training loss: 0.09306313097476959 = 0.021748559549450874 + 0.01 * 7.131457805633545
Epoch 490, val loss: 1.0295076370239258
Epoch 500, training loss: 0.09107004106044769 = 0.02003246545791626 + 0.01 * 7.103757858276367
Epoch 500, val loss: 1.042967677116394
Epoch 510, training loss: 0.08925583958625793 = 0.0185169018805027 + 0.01 * 7.073894023895264
Epoch 510, val loss: 1.055889368057251
Epoch 520, training loss: 0.08782701194286346 = 0.01717047579586506 + 0.01 * 7.0656538009643555
Epoch 520, val loss: 1.0683579444885254
Epoch 530, training loss: 0.08652257919311523 = 0.015966828912496567 + 0.01 * 7.055575847625732
Epoch 530, val loss: 1.0804386138916016
Epoch 540, training loss: 0.0855090320110321 = 0.014888305217027664 + 0.01 * 7.062072277069092
Epoch 540, val loss: 1.0920791625976562
Epoch 550, training loss: 0.0844736397266388 = 0.013920760713517666 + 0.01 * 7.055288314819336
Epoch 550, val loss: 1.1033570766448975
Epoch 560, training loss: 0.08347661793231964 = 0.013048110529780388 + 0.01 * 7.042850494384766
Epoch 560, val loss: 1.114235520362854
Epoch 570, training loss: 0.08268049359321594 = 0.012257777154445648 + 0.01 * 7.042272090911865
Epoch 570, val loss: 1.1248996257781982
Epoch 580, training loss: 0.08186899125576019 = 0.01154035422950983 + 0.01 * 7.032864093780518
Epoch 580, val loss: 1.1350228786468506
Epoch 590, training loss: 0.08102597296237946 = 0.010887379758059978 + 0.01 * 7.013859748840332
Epoch 590, val loss: 1.1450244188308716
Epoch 600, training loss: 0.08050422370433807 = 0.010291620157659054 + 0.01 * 7.021260738372803
Epoch 600, val loss: 1.1545944213867188
Epoch 610, training loss: 0.0798245221376419 = 0.009747093543410301 + 0.01 * 7.007742881774902
Epoch 610, val loss: 1.1640452146530151
Epoch 620, training loss: 0.07933264225721359 = 0.009247265756130219 + 0.01 * 7.008537769317627
Epoch 620, val loss: 1.1730209589004517
Epoch 630, training loss: 0.07879588007926941 = 0.008787662722170353 + 0.01 * 7.000822067260742
Epoch 630, val loss: 1.181864857673645
Epoch 640, training loss: 0.07829838246107101 = 0.008364053443074226 + 0.01 * 6.993433475494385
Epoch 640, val loss: 1.1902754306793213
Epoch 650, training loss: 0.07776912301778793 = 0.007972943596541882 + 0.01 * 6.979618072509766
Epoch 650, val loss: 1.198659062385559
Epoch 660, training loss: 0.07768910378217697 = 0.007611296605318785 + 0.01 * 7.0077805519104
Epoch 660, val loss: 1.2068252563476562
Epoch 670, training loss: 0.07707177102565765 = 0.007277011405676603 + 0.01 * 6.979475975036621
Epoch 670, val loss: 1.2144495248794556
Epoch 680, training loss: 0.07677233219146729 = 0.006966771092265844 + 0.01 * 6.980556488037109
Epoch 680, val loss: 1.222178339958191
Epoch 690, training loss: 0.07630223035812378 = 0.006677895784378052 + 0.01 * 6.962433338165283
Epoch 690, val loss: 1.2293870449066162
Epoch 700, training loss: 0.07593049108982086 = 0.006408595014363527 + 0.01 * 6.952189922332764
Epoch 700, val loss: 1.2366427183151245
Epoch 710, training loss: 0.07569506764411926 = 0.006157102528959513 + 0.01 * 6.953796863555908
Epoch 710, val loss: 1.2436912059783936
Epoch 720, training loss: 0.07549436390399933 = 0.005921602249145508 + 0.01 * 6.957276344299316
Epoch 720, val loss: 1.250313401222229
Epoch 730, training loss: 0.07520606368780136 = 0.005701281595975161 + 0.01 * 6.950478553771973
Epoch 730, val loss: 1.2570338249206543
Epoch 740, training loss: 0.07484807074069977 = 0.00549445953220129 + 0.01 * 6.935361385345459
Epoch 740, val loss: 1.2634893655776978
Epoch 750, training loss: 0.07483957707881927 = 0.005300040822476149 + 0.01 * 6.953954219818115
Epoch 750, val loss: 1.2697837352752686
Epoch 760, training loss: 0.07449893653392792 = 0.005117448512464762 + 0.01 * 6.9381489753723145
Epoch 760, val loss: 1.2759647369384766
Epoch 770, training loss: 0.07436000555753708 = 0.004945290740579367 + 0.01 * 6.941471099853516
Epoch 770, val loss: 1.2819321155548096
Epoch 780, training loss: 0.07399395853281021 = 0.004783503711223602 + 0.01 * 6.921045303344727
Epoch 780, val loss: 1.2877174615859985
Epoch 790, training loss: 0.07378942519426346 = 0.004630701150745153 + 0.01 * 6.915872573852539
Epoch 790, val loss: 1.293395757675171
Epoch 800, training loss: 0.07363562285900116 = 0.0044865235686302185 + 0.01 * 6.914910316467285
Epoch 800, val loss: 1.299150824546814
Epoch 810, training loss: 0.07360750436782837 = 0.004350279923528433 + 0.01 * 6.925722122192383
Epoch 810, val loss: 1.304310917854309
Epoch 820, training loss: 0.07330118119716644 = 0.004221353679895401 + 0.01 * 6.907982349395752
Epoch 820, val loss: 1.309688925743103
Epoch 830, training loss: 0.07309061288833618 = 0.004098887089639902 + 0.01 * 6.899173259735107
Epoch 830, val loss: 1.3149014711380005
Epoch 840, training loss: 0.073096863925457 = 0.003982751630246639 + 0.01 * 6.911411285400391
Epoch 840, val loss: 1.319907307624817
Epoch 850, training loss: 0.07276581972837448 = 0.003872526343911886 + 0.01 * 6.889329433441162
Epoch 850, val loss: 1.3247090578079224
Epoch 860, training loss: 0.07285072654485703 = 0.0037676533684134483 + 0.01 * 6.908307075500488
Epoch 860, val loss: 1.3295916318893433
Epoch 870, training loss: 0.07252366840839386 = 0.0036680595949292183 + 0.01 * 6.885560989379883
Epoch 870, val loss: 1.334197998046875
Epoch 880, training loss: 0.07274367660284042 = 0.003573090536519885 + 0.01 * 6.91705846786499
Epoch 880, val loss: 1.3388581275939941
Epoch 890, training loss: 0.0722932368516922 = 0.00348298461176455 + 0.01 * 6.881025791168213
Epoch 890, val loss: 1.34328293800354
Epoch 900, training loss: 0.07210426032543182 = 0.0033968056086450815 + 0.01 * 6.8707451820373535
Epoch 900, val loss: 1.3476747274398804
Epoch 910, training loss: 0.0724039077758789 = 0.0033144825138151646 + 0.01 * 6.908943176269531
Epoch 910, val loss: 1.3519824743270874
Epoch 920, training loss: 0.07205270975828171 = 0.0032361692283302546 + 0.01 * 6.881654262542725
Epoch 920, val loss: 1.356124997138977
Epoch 930, training loss: 0.0719154104590416 = 0.003161114174872637 + 0.01 * 6.875429630279541
Epoch 930, val loss: 1.3602615594863892
Epoch 940, training loss: 0.0716228112578392 = 0.0030893958173692226 + 0.01 * 6.853341579437256
Epoch 940, val loss: 1.3643728494644165
Epoch 950, training loss: 0.07176154106855392 = 0.0030207873787730932 + 0.01 * 6.874075889587402
Epoch 950, val loss: 1.3680862188339233
Epoch 960, training loss: 0.07148949801921844 = 0.0029552262276411057 + 0.01 * 6.853427410125732
Epoch 960, val loss: 1.3721486330032349
Epoch 970, training loss: 0.07151187211275101 = 0.002892260206863284 + 0.01 * 6.861961364746094
Epoch 970, val loss: 1.3759267330169678
Epoch 980, training loss: 0.07133832573890686 = 0.002831860212609172 + 0.01 * 6.850646495819092
Epoch 980, val loss: 1.3794978857040405
Epoch 990, training loss: 0.07112939655780792 = 0.002773919142782688 + 0.01 * 6.835547924041748
Epoch 990, val loss: 1.383245825767517
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8112809699525567
The final CL Acc:0.75802, 0.01848, The final GNN Acc:0.81673, 0.00400
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13172])
remove edge: torch.Size([2, 7860])
updated graph: torch.Size([2, 10476])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0219626426696777 = 1.935994267463684 + 0.01 * 8.596831321716309
Epoch 0, val loss: 1.9330883026123047
Epoch 10, training loss: 2.0127713680267334 = 1.9268038272857666 + 0.01 * 8.59676456451416
Epoch 10, val loss: 1.9246470928192139
Epoch 20, training loss: 2.0015292167663574 = 1.915563941001892 + 0.01 * 8.59652042388916
Epoch 20, val loss: 1.9140697717666626
Epoch 30, training loss: 1.9858680963516235 = 1.8999111652374268 + 0.01 * 8.595695495605469
Epoch 30, val loss: 1.8992433547973633
Epoch 40, training loss: 1.962744116783142 = 1.8768373727798462 + 0.01 * 8.590677261352539
Epoch 40, val loss: 1.8779828548431396
Epoch 50, training loss: 1.929979681968689 = 1.8444523811340332 + 0.01 * 8.552725791931152
Epoch 50, val loss: 1.849635124206543
Epoch 60, training loss: 1.8895130157470703 = 1.8059699535369873 + 0.01 * 8.354304313659668
Epoch 60, val loss: 1.818452000617981
Epoch 70, training loss: 1.848637580871582 = 1.767100214958191 + 0.01 * 8.153732299804688
Epoch 70, val loss: 1.7865300178527832
Epoch 80, training loss: 1.7983068227767944 = 1.7187364101409912 + 0.01 * 7.957041263580322
Epoch 80, val loss: 1.741500973701477
Epoch 90, training loss: 1.7288792133331299 = 1.6518648862838745 + 0.01 * 7.701432704925537
Epoch 90, val loss: 1.681247353553772
Epoch 100, training loss: 1.6415820121765137 = 1.5667537450790405 + 0.01 * 7.482826232910156
Epoch 100, val loss: 1.6101012229919434
Epoch 110, training loss: 1.5456147193908691 = 1.471449851989746 + 0.01 * 7.416490077972412
Epoch 110, val loss: 1.531793475151062
Epoch 120, training loss: 1.4476138353347778 = 1.3738287687301636 + 0.01 * 7.3785080909729
Epoch 120, val loss: 1.4519885778427124
Epoch 130, training loss: 1.3494309186935425 = 1.2759065628051758 + 0.01 * 7.35243558883667
Epoch 130, val loss: 1.3711636066436768
Epoch 140, training loss: 1.250960111618042 = 1.1776453256607056 + 0.01 * 7.331480026245117
Epoch 140, val loss: 1.2907994985580444
Epoch 150, training loss: 1.1551016569137573 = 1.082041621208191 + 0.01 * 7.306003093719482
Epoch 150, val loss: 1.213901162147522
Epoch 160, training loss: 1.0653518438339233 = 0.9925677180290222 + 0.01 * 7.278412342071533
Epoch 160, val loss: 1.1440280675888062
Epoch 170, training loss: 0.9818674325942993 = 0.9092925190925598 + 0.01 * 7.257491588592529
Epoch 170, val loss: 1.080776333808899
Epoch 180, training loss: 0.902324378490448 = 0.8298863172531128 + 0.01 * 7.243807792663574
Epoch 180, val loss: 1.021108865737915
Epoch 190, training loss: 0.8253312110900879 = 0.7530075311660767 + 0.01 * 7.232365608215332
Epoch 190, val loss: 0.9633892178535461
Epoch 200, training loss: 0.7515660524368286 = 0.6793562769889832 + 0.01 * 7.220980644226074
Epoch 200, val loss: 0.9087474346160889
Epoch 210, training loss: 0.6821883320808411 = 0.6100963950157166 + 0.01 * 7.209195613861084
Epoch 210, val loss: 0.8593901991844177
Epoch 220, training loss: 0.6175968647003174 = 0.5456269383430481 + 0.01 * 7.196991443634033
Epoch 220, val loss: 0.8170702457427979
Epoch 230, training loss: 0.5581339001655579 = 0.48626822233200073 + 0.01 * 7.1865692138671875
Epoch 230, val loss: 0.7828173637390137
Epoch 240, training loss: 0.5042140483856201 = 0.43245410919189453 + 0.01 * 7.175993919372559
Epoch 240, val loss: 0.7566388845443726
Epoch 250, training loss: 0.4560295045375824 = 0.38439545035362244 + 0.01 * 7.163405895233154
Epoch 250, val loss: 0.7379754781723022
Epoch 260, training loss: 0.4133361577987671 = 0.34180596470832825 + 0.01 * 7.153018474578857
Epoch 260, val loss: 0.7258744835853577
Epoch 270, training loss: 0.3755396008491516 = 0.30401715636253357 + 0.01 * 7.152244567871094
Epoch 270, val loss: 0.7192410230636597
Epoch 280, training loss: 0.34160253405570984 = 0.27022966742515564 + 0.01 * 7.1372857093811035
Epoch 280, val loss: 0.7168757915496826
Epoch 290, training loss: 0.3110606074333191 = 0.23973901569843292 + 0.01 * 7.132160186767578
Epoch 290, val loss: 0.7179570198059082
Epoch 300, training loss: 0.28335270285606384 = 0.21208541095256805 + 0.01 * 7.126729488372803
Epoch 300, val loss: 0.7220064401626587
Epoch 310, training loss: 0.2582773268222809 = 0.18705125153064728 + 0.01 * 7.122608184814453
Epoch 310, val loss: 0.7284814715385437
Epoch 320, training loss: 0.23575423657894135 = 0.1645536869764328 + 0.01 * 7.120055198669434
Epoch 320, val loss: 0.7369481921195984
Epoch 330, training loss: 0.21566760540008545 = 0.14451788365840912 + 0.01 * 7.114973068237305
Epoch 330, val loss: 0.7471541166305542
Epoch 340, training loss: 0.1979072093963623 = 0.1268070489168167 + 0.01 * 7.110016822814941
Epoch 340, val loss: 0.7588055729866028
Epoch 350, training loss: 0.18228402733802795 = 0.11123482137918472 + 0.01 * 7.104920387268066
Epoch 350, val loss: 0.7715777158737183
Epoch 360, training loss: 0.16860151290893555 = 0.09760081022977829 + 0.01 * 7.100070953369141
Epoch 360, val loss: 0.7852330207824707
Epoch 370, training loss: 0.15665611624717712 = 0.08570697903633118 + 0.01 * 7.094913959503174
Epoch 370, val loss: 0.7995189428329468
Epoch 380, training loss: 0.1462828814983368 = 0.07537543773651123 + 0.01 * 7.090744495391846
Epoch 380, val loss: 0.8143582940101624
Epoch 390, training loss: 0.13732211291790009 = 0.06643711775541306 + 0.01 * 7.088500022888184
Epoch 390, val loss: 0.829585075378418
Epoch 400, training loss: 0.1295301914215088 = 0.05874199792742729 + 0.01 * 7.07882022857666
Epoch 400, val loss: 0.8451569080352783
Epoch 410, training loss: 0.122842937707901 = 0.05213656648993492 + 0.01 * 7.070636749267578
Epoch 410, val loss: 0.8607547879219055
Epoch 420, training loss: 0.11720849573612213 = 0.046475838869810104 + 0.01 * 7.073266506195068
Epoch 420, val loss: 0.876413881778717
Epoch 430, training loss: 0.11223410069942474 = 0.04162199795246124 + 0.01 * 7.0612101554870605
Epoch 430, val loss: 0.8919151425361633
Epoch 440, training loss: 0.10792700201272964 = 0.037448376417160034 + 0.01 * 7.047863006591797
Epoch 440, val loss: 0.9072968363761902
Epoch 450, training loss: 0.10435256361961365 = 0.03384523093700409 + 0.01 * 7.05073356628418
Epoch 450, val loss: 0.9224535822868347
Epoch 460, training loss: 0.10108143091201782 = 0.03072259947657585 + 0.01 * 7.035883903503418
Epoch 460, val loss: 0.9373117089271545
Epoch 470, training loss: 0.09824950993061066 = 0.028001951053738594 + 0.01 * 7.024755954742432
Epoch 470, val loss: 0.9518821239471436
Epoch 480, training loss: 0.09582614153623581 = 0.025620440021157265 + 0.01 * 7.020570278167725
Epoch 480, val loss: 0.9661258459091187
Epoch 490, training loss: 0.09390783309936523 = 0.02352740243077278 + 0.01 * 7.0380425453186035
Epoch 490, val loss: 0.9800036549568176
Epoch 500, training loss: 0.0917244702577591 = 0.021680857986211777 + 0.01 * 7.004360675811768
Epoch 500, val loss: 0.9935581684112549
Epoch 510, training loss: 0.08999080210924149 = 0.020044101402163506 + 0.01 * 6.9946699142456055
Epoch 510, val loss: 1.0067005157470703
Epoch 520, training loss: 0.08841594308614731 = 0.018587177619338036 + 0.01 * 6.982876300811768
Epoch 520, val loss: 1.0194851160049438
Epoch 530, training loss: 0.08715932816267014 = 0.017285672947764397 + 0.01 * 6.987365245819092
Epoch 530, val loss: 1.0318584442138672
Epoch 540, training loss: 0.08583641052246094 = 0.016119549050927162 + 0.01 * 6.971686840057373
Epoch 540, val loss: 1.043884515762329
Epoch 550, training loss: 0.08461735397577286 = 0.015071791596710682 + 0.01 * 6.954555988311768
Epoch 550, val loss: 1.0555617809295654
Epoch 560, training loss: 0.0835328996181488 = 0.014126686379313469 + 0.01 * 6.940621376037598
Epoch 560, val loss: 1.0667885541915894
Epoch 570, training loss: 0.08266444504261017 = 0.013270832598209381 + 0.01 * 6.939361572265625
Epoch 570, val loss: 1.0776910781860352
Epoch 580, training loss: 0.08174275606870651 = 0.012494683265686035 + 0.01 * 6.924807548522949
Epoch 580, val loss: 1.088271141052246
Epoch 590, training loss: 0.08094129711389542 = 0.011788195930421352 + 0.01 * 6.915309906005859
Epoch 590, val loss: 1.0984975099563599
Epoch 600, training loss: 0.08042329549789429 = 0.01114275585860014 + 0.01 * 6.928054332733154
Epoch 600, val loss: 1.1083909273147583
Epoch 610, training loss: 0.07961948961019516 = 0.010552538558840752 + 0.01 * 6.906695365905762
Epoch 610, val loss: 1.1180341243743896
Epoch 620, training loss: 0.07896195352077484 = 0.010011846199631691 + 0.01 * 6.8950114250183105
Epoch 620, val loss: 1.1272891759872437
Epoch 630, training loss: 0.07840487360954285 = 0.009513975121080875 + 0.01 * 6.889090061187744
Epoch 630, val loss: 1.1363400220870972
Epoch 640, training loss: 0.07787589728832245 = 0.009055238217115402 + 0.01 * 6.88206672668457
Epoch 640, val loss: 1.1450772285461426
Epoch 650, training loss: 0.07737851142883301 = 0.008632194250822067 + 0.01 * 6.874631881713867
Epoch 650, val loss: 1.1535626649856567
Epoch 660, training loss: 0.0770796462893486 = 0.008240374736487865 + 0.01 * 6.883927345275879
Epoch 660, val loss: 1.161780595779419
Epoch 670, training loss: 0.07647208869457245 = 0.00787687674164772 + 0.01 * 6.859521389007568
Epoch 670, val loss: 1.1698073148727417
Epoch 680, training loss: 0.07609206438064575 = 0.007539727259427309 + 0.01 * 6.855233669281006
Epoch 680, val loss: 1.1775250434875488
Epoch 690, training loss: 0.0757322907447815 = 0.007225518114864826 + 0.01 * 6.850677490234375
Epoch 690, val loss: 1.185074806213379
Epoch 700, training loss: 0.07536691427230835 = 0.0069326916709542274 + 0.01 * 6.8434224128723145
Epoch 700, val loss: 1.1924439668655396
Epoch 710, training loss: 0.07526498287916183 = 0.006658812984824181 + 0.01 * 6.860616683959961
Epoch 710, val loss: 1.1995272636413574
Epoch 720, training loss: 0.07477599382400513 = 0.0064030177891254425 + 0.01 * 6.837297439575195
Epoch 720, val loss: 1.2065702676773071
Epoch 730, training loss: 0.07436553388834 = 0.006163481157273054 + 0.01 * 6.8202056884765625
Epoch 730, val loss: 1.2131948471069336
Epoch 740, training loss: 0.074164979159832 = 0.0059385099448263645 + 0.01 * 6.822647571563721
Epoch 740, val loss: 1.2199198007583618
Epoch 750, training loss: 0.07383966445922852 = 0.005727472715079784 + 0.01 * 6.811219692230225
Epoch 750, val loss: 1.2262585163116455
Epoch 760, training loss: 0.07384950667619705 = 0.0055289254523813725 + 0.01 * 6.832057952880859
Epoch 760, val loss: 1.2325080633163452
Epoch 770, training loss: 0.07350944727659225 = 0.005342007614672184 + 0.01 * 6.816743850708008
Epoch 770, val loss: 1.2386401891708374
Epoch 780, training loss: 0.0732661560177803 = 0.005165650974959135 + 0.01 * 6.8100504875183105
Epoch 780, val loss: 1.2444852590560913
Epoch 790, training loss: 0.0730755478143692 = 0.004999049007892609 + 0.01 * 6.807650089263916
Epoch 790, val loss: 1.250356912612915
Epoch 800, training loss: 0.07280169427394867 = 0.0048415781930089 + 0.01 * 6.796011447906494
Epoch 800, val loss: 1.2559535503387451
Epoch 810, training loss: 0.0728798508644104 = 0.004692414775490761 + 0.01 * 6.81874418258667
Epoch 810, val loss: 1.2614734172821045
Epoch 820, training loss: 0.07244184613227844 = 0.0045513613149523735 + 0.01 * 6.78904914855957
Epoch 820, val loss: 1.266912817955017
Epoch 830, training loss: 0.07231473922729492 = 0.004417457152158022 + 0.01 * 6.78972864151001
Epoch 830, val loss: 1.2720590829849243
Epoch 840, training loss: 0.07218824326992035 = 0.004290311131626368 + 0.01 * 6.789793491363525
Epoch 840, val loss: 1.277238368988037
Epoch 850, training loss: 0.0719282403588295 = 0.004169604275375605 + 0.01 * 6.7758636474609375
Epoch 850, val loss: 1.2822654247283936
Epoch 860, training loss: 0.07174544036388397 = 0.00405486673116684 + 0.01 * 6.769057750701904
Epoch 860, val loss: 1.28708016872406
Epoch 870, training loss: 0.07162804901599884 = 0.003945442382246256 + 0.01 * 6.768260955810547
Epoch 870, val loss: 1.2919288873672485
Epoch 880, training loss: 0.07149942219257355 = 0.0038414353039115667 + 0.01 * 6.765798568725586
Epoch 880, val loss: 1.2965428829193115
Epoch 890, training loss: 0.07134749740362167 = 0.003742130706086755 + 0.01 * 6.760537147521973
Epoch 890, val loss: 1.301132082939148
Epoch 900, training loss: 0.07121657580137253 = 0.003647300647571683 + 0.01 * 6.756927967071533
Epoch 900, val loss: 1.3056223392486572
Epoch 910, training loss: 0.0711468830704689 = 0.0035570019390434027 + 0.01 * 6.758987903594971
Epoch 910, val loss: 1.3098777532577515
Epoch 920, training loss: 0.07113057374954224 = 0.003470534924417734 + 0.01 * 6.7660040855407715
Epoch 920, val loss: 1.314256191253662
Epoch 930, training loss: 0.07087479531764984 = 0.0033879794646054506 + 0.01 * 6.748681545257568
Epoch 930, val loss: 1.3183643817901611
Epoch 940, training loss: 0.07082410901784897 = 0.00330883776769042 + 0.01 * 6.751526832580566
Epoch 940, val loss: 1.3224802017211914
Epoch 950, training loss: 0.07093734294176102 = 0.0032330104149878025 + 0.01 * 6.7704339027404785
Epoch 950, val loss: 1.326430082321167
Epoch 960, training loss: 0.07071078568696976 = 0.003160404274240136 + 0.01 * 6.755038261413574
Epoch 960, val loss: 1.3303934335708618
Epoch 970, training loss: 0.07043438404798508 = 0.0030908803455531597 + 0.01 * 6.734350204467773
Epoch 970, val loss: 1.334155797958374
Epoch 980, training loss: 0.07042301446199417 = 0.003024148289114237 + 0.01 * 6.739887237548828
Epoch 980, val loss: 1.3378545045852661
Epoch 990, training loss: 0.07032368332147598 = 0.0029600802809000015 + 0.01 * 6.736360549926758
Epoch 990, val loss: 1.3415822982788086
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.0267066955566406 = 1.940738320350647 + 0.01 * 8.596843719482422
Epoch 0, val loss: 1.9396133422851562
Epoch 10, training loss: 2.017233371734619 = 1.9312653541564941 + 0.01 * 8.596792221069336
Epoch 10, val loss: 1.9298285245895386
Epoch 20, training loss: 2.0059010982513428 = 1.9199349880218506 + 0.01 * 8.596609115600586
Epoch 20, val loss: 1.9179362058639526
Epoch 30, training loss: 1.9903604984283447 = 1.9044004678726196 + 0.01 * 8.596000671386719
Epoch 30, val loss: 1.9016443490982056
Epoch 40, training loss: 1.9675061702728271 = 1.8815809488296509 + 0.01 * 8.592527389526367
Epoch 40, val loss: 1.877958059310913
Epoch 50, training loss: 1.9346004724502563 = 1.8489431142807007 + 0.01 * 8.565741539001465
Epoch 50, val loss: 1.845273733139038
Epoch 60, training loss: 1.8934190273284912 = 1.8091493844985962 + 0.01 * 8.426966667175293
Epoch 60, val loss: 1.8085488080978394
Epoch 70, training loss: 1.8540503978729248 = 1.7720195055007935 + 0.01 * 8.203094482421875
Epoch 70, val loss: 1.777253270149231
Epoch 80, training loss: 1.8107153177261353 = 1.730107307434082 + 0.01 * 8.060802459716797
Epoch 80, val loss: 1.7397149801254272
Epoch 90, training loss: 1.7481812238693237 = 1.6696561574935913 + 0.01 * 7.852501392364502
Epoch 90, val loss: 1.6854757070541382
Epoch 100, training loss: 1.6650352478027344 = 1.5883272886276245 + 0.01 * 7.670800685882568
Epoch 100, val loss: 1.6152023077011108
Epoch 110, training loss: 1.5651570558547974 = 1.4904958009719849 + 0.01 * 7.46612024307251
Epoch 110, val loss: 1.5323585271835327
Epoch 120, training loss: 1.460701823234558 = 1.3870548009872437 + 0.01 * 7.364706039428711
Epoch 120, val loss: 1.4452003240585327
Epoch 130, training loss: 1.357978105545044 = 1.285317301750183 + 0.01 * 7.266075611114502
Epoch 130, val loss: 1.3624082803726196
Epoch 140, training loss: 1.2605994939804077 = 1.1886765956878662 + 0.01 * 7.192294597625732
Epoch 140, val loss: 1.2865945100784302
Epoch 150, training loss: 1.1715797185897827 = 1.100066065788269 + 0.01 * 7.151365756988525
Epoch 150, val loss: 1.218837022781372
Epoch 160, training loss: 1.0923430919647217 = 1.020998239517212 + 0.01 * 7.134491443634033
Epoch 160, val loss: 1.160752534866333
Epoch 170, training loss: 1.0218181610107422 = 0.9505329728126526 + 0.01 * 7.128523826599121
Epoch 170, val loss: 1.1112741231918335
Epoch 180, training loss: 0.9575265049934387 = 0.8862682580947876 + 0.01 * 7.125826358795166
Epoch 180, val loss: 1.0676637887954712
Epoch 190, training loss: 0.8969570994377136 = 0.8257202506065369 + 0.01 * 7.123685836791992
Epoch 190, val loss: 1.0277864933013916
Epoch 200, training loss: 0.8381438255310059 = 0.7669166326522827 + 0.01 * 7.1227192878723145
Epoch 200, val loss: 0.9899014234542847
Epoch 210, training loss: 0.7797238230705261 = 0.7084977626800537 + 0.01 * 7.122605323791504
Epoch 210, val loss: 0.9523165225982666
Epoch 220, training loss: 0.7205849289894104 = 0.6493552327156067 + 0.01 * 7.122971534729004
Epoch 220, val loss: 0.9141100645065308
Epoch 230, training loss: 0.6600098609924316 = 0.5887752175331116 + 0.01 * 7.123464107513428
Epoch 230, val loss: 0.874908983707428
Epoch 240, training loss: 0.598334550857544 = 0.5270951390266418 + 0.01 * 7.123939037322998
Epoch 240, val loss: 0.8344113230705261
Epoch 250, training loss: 0.536966860294342 = 0.4657233655452728 + 0.01 * 7.1243486404418945
Epoch 250, val loss: 0.794310450553894
Epoch 260, training loss: 0.47758036851882935 = 0.40633338689804077 + 0.01 * 7.124698162078857
Epoch 260, val loss: 0.7562512159347534
Epoch 270, training loss: 0.4217281639575958 = 0.35047730803489685 + 0.01 * 7.125086307525635
Epoch 270, val loss: 0.7216670513153076
Epoch 280, training loss: 0.37074026465415955 = 0.29948604106903076 + 0.01 * 7.125422477722168
Epoch 280, val loss: 0.6919628977775574
Epoch 290, training loss: 0.3255884349346161 = 0.2543323338031769 + 0.01 * 7.125609874725342
Epoch 290, val loss: 0.6677390933036804
Epoch 300, training loss: 0.2867152690887451 = 0.21545857191085815 + 0.01 * 7.125668525695801
Epoch 300, val loss: 0.6493748426437378
Epoch 310, training loss: 0.25390687584877014 = 0.18264977633953094 + 0.01 * 7.125710487365723
Epoch 310, val loss: 0.6367216110229492
Epoch 320, training loss: 0.22649748623371124 = 0.15524409711360931 + 0.01 * 7.125339031219482
Epoch 320, val loss: 0.6291818022727966
Epoch 330, training loss: 0.20368969440460205 = 0.13244368135929108 + 0.01 * 7.124602317810059
Epoch 330, val loss: 0.6258363723754883
Epoch 340, training loss: 0.18472766876220703 = 0.1134805828332901 + 0.01 * 7.1247076988220215
Epoch 340, val loss: 0.6257419586181641
Epoch 350, training loss: 0.16890490055084229 = 0.09768141061067581 + 0.01 * 7.122350215911865
Epoch 350, val loss: 0.6281808614730835
Epoch 360, training loss: 0.15570226311683655 = 0.08448902517557144 + 0.01 * 7.121323585510254
Epoch 360, val loss: 0.6324381828308105
Epoch 370, training loss: 0.1446436643600464 = 0.07344865798950195 + 0.01 * 7.119501113891602
Epoch 370, val loss: 0.6380463242530823
Epoch 380, training loss: 0.13534477353096008 = 0.06418387591838837 + 0.01 * 7.1160888671875
Epoch 380, val loss: 0.644568681716919
Epoch 390, training loss: 0.12754446268081665 = 0.05638502538204193 + 0.01 * 7.115943431854248
Epoch 390, val loss: 0.6517740488052368
Epoch 400, training loss: 0.12090057134628296 = 0.04979919642210007 + 0.01 * 7.110137939453125
Epoch 400, val loss: 0.6594780683517456
Epoch 410, training loss: 0.11527514457702637 = 0.04421728849411011 + 0.01 * 7.105785846710205
Epoch 410, val loss: 0.6674658060073853
Epoch 420, training loss: 0.11046653240919113 = 0.039468780159950256 + 0.01 * 7.099775314331055
Epoch 420, val loss: 0.6756717562675476
Epoch 430, training loss: 0.10634999722242355 = 0.03541162610054016 + 0.01 * 7.093837261199951
Epoch 430, val loss: 0.6839474439620972
Epoch 440, training loss: 0.10283677279949188 = 0.03193100914359093 + 0.01 * 7.090577125549316
Epoch 440, val loss: 0.6922686696052551
Epoch 450, training loss: 0.09979040920734406 = 0.028931450098752975 + 0.01 * 7.085895538330078
Epoch 450, val loss: 0.7004837989807129
Epoch 460, training loss: 0.09710182994604111 = 0.02633252926170826 + 0.01 * 7.076930522918701
Epoch 460, val loss: 0.7086815237998962
Epoch 470, training loss: 0.09475450217723846 = 0.024068506434559822 + 0.01 * 7.068600177764893
Epoch 470, val loss: 0.7167901396751404
Epoch 480, training loss: 0.0926826000213623 = 0.022086558863520622 + 0.01 * 7.059604167938232
Epoch 480, val loss: 0.7247154116630554
Epoch 490, training loss: 0.09105993807315826 = 0.020344415679574013 + 0.01 * 7.071552753448486
Epoch 490, val loss: 0.732526957988739
Epoch 500, training loss: 0.08933466672897339 = 0.018807511776685715 + 0.01 * 7.052715301513672
Epoch 500, val loss: 0.7401041388511658
Epoch 510, training loss: 0.08783449977636337 = 0.01744416169822216 + 0.01 * 7.03903341293335
Epoch 510, val loss: 0.7475288510322571
Epoch 520, training loss: 0.08656366914510727 = 0.01622888445854187 + 0.01 * 7.033478260040283
Epoch 520, val loss: 0.7547609806060791
Epoch 530, training loss: 0.08543053269386292 = 0.015141619369387627 + 0.01 * 7.028891563415527
Epoch 530, val loss: 0.761825442314148
Epoch 540, training loss: 0.08449603617191315 = 0.014165704138576984 + 0.01 * 7.03303337097168
Epoch 540, val loss: 0.7687312364578247
Epoch 550, training loss: 0.08336972445249557 = 0.013287023641169071 + 0.01 * 7.008270263671875
Epoch 550, val loss: 0.7754068374633789
Epoch 560, training loss: 0.08251183480024338 = 0.012492640875279903 + 0.01 * 7.001919269561768
Epoch 560, val loss: 0.7819182276725769
Epoch 570, training loss: 0.08181248605251312 = 0.011771946214139462 + 0.01 * 7.004054069519043
Epoch 570, val loss: 0.7882903218269348
Epoch 580, training loss: 0.08102662861347198 = 0.011116603389382362 + 0.01 * 6.991002559661865
Epoch 580, val loss: 0.7945035099983215
Epoch 590, training loss: 0.08044067770242691 = 0.010519043542444706 + 0.01 * 6.99216365814209
Epoch 590, val loss: 0.800478458404541
Epoch 600, training loss: 0.07979647815227509 = 0.00997199211269617 + 0.01 * 6.982448577880859
Epoch 600, val loss: 0.8064018487930298
Epoch 610, training loss: 0.0792572945356369 = 0.009470279328525066 + 0.01 * 6.978702068328857
Epoch 610, val loss: 0.8120484352111816
Epoch 620, training loss: 0.07872695475816727 = 0.009009040892124176 + 0.01 * 6.9717912673950195
Epoch 620, val loss: 0.8176642060279846
Epoch 630, training loss: 0.07826269418001175 = 0.008583626709878445 + 0.01 * 6.967906951904297
Epoch 630, val loss: 0.8230460286140442
Epoch 640, training loss: 0.07783838361501694 = 0.008190988563001156 + 0.01 * 6.964739799499512
Epoch 640, val loss: 0.8283518552780151
Epoch 650, training loss: 0.07733617722988129 = 0.00782767217606306 + 0.01 * 6.950850963592529
Epoch 650, val loss: 0.8334841728210449
Epoch 660, training loss: 0.07696472853422165 = 0.007490396033972502 + 0.01 * 6.9474334716796875
Epoch 660, val loss: 0.8384617567062378
Epoch 670, training loss: 0.07662614434957504 = 0.007177205756306648 + 0.01 * 6.944893836975098
Epoch 670, val loss: 0.8433899879455566
Epoch 680, training loss: 0.07621119916439056 = 0.00688564870506525 + 0.01 * 6.932555198669434
Epoch 680, val loss: 0.8481209874153137
Epoch 690, training loss: 0.07589352130889893 = 0.006613568402826786 + 0.01 * 6.927995204925537
Epoch 690, val loss: 0.8527477979660034
Epoch 700, training loss: 0.07546062767505646 = 0.0063594128005206585 + 0.01 * 6.910121440887451
Epoch 700, val loss: 0.8572752475738525
Epoch 710, training loss: 0.07537047564983368 = 0.006121581885963678 + 0.01 * 6.92488956451416
Epoch 710, val loss: 0.861724317073822
Epoch 720, training loss: 0.07502751052379608 = 0.0058989874087274075 + 0.01 * 6.9128522872924805
Epoch 720, val loss: 0.8659510016441345
Epoch 730, training loss: 0.07459279894828796 = 0.005690264515578747 + 0.01 * 6.890254020690918
Epoch 730, val loss: 0.8701058626174927
Epoch 740, training loss: 0.07460261881351471 = 0.005494128912687302 + 0.01 * 6.910848617553711
Epoch 740, val loss: 0.8742354512214661
Epoch 750, training loss: 0.07410965859889984 = 0.005309815984219313 + 0.01 * 6.879984378814697
Epoch 750, val loss: 0.8780994415283203
Epoch 760, training loss: 0.07395815849304199 = 0.005136031657457352 + 0.01 * 6.882212162017822
Epoch 760, val loss: 0.8819454312324524
Epoch 770, training loss: 0.0736437439918518 = 0.004972201306372881 + 0.01 * 6.867154121398926
Epoch 770, val loss: 0.885772168636322
Epoch 780, training loss: 0.07359864562749863 = 0.004817412234842777 + 0.01 * 6.878123760223389
Epoch 780, val loss: 0.8893089890480042
Epoch 790, training loss: 0.07338644564151764 = 0.004671056289225817 + 0.01 * 6.871539115905762
Epoch 790, val loss: 0.8929866552352905
Epoch 800, training loss: 0.07326385378837585 = 0.004532397259026766 + 0.01 * 6.873146057128906
Epoch 800, val loss: 0.8963861465454102
Epoch 810, training loss: 0.07291901111602783 = 0.0044007678516209126 + 0.01 * 6.851824760437012
Epoch 810, val loss: 0.8998227119445801
Epoch 820, training loss: 0.07309111207723618 = 0.004275461193174124 + 0.01 * 6.881565093994141
Epoch 820, val loss: 0.9031217694282532
Epoch 830, training loss: 0.0725095346570015 = 0.004156267736107111 + 0.01 * 6.835326671600342
Epoch 830, val loss: 0.9063131213188171
Epoch 840, training loss: 0.07270834594964981 = 0.004042757209390402 + 0.01 * 6.86655855178833
Epoch 840, val loss: 0.9094690084457397
Epoch 850, training loss: 0.07207004725933075 = 0.0039347331039607525 + 0.01 * 6.813531398773193
Epoch 850, val loss: 0.9126210808753967
Epoch 860, training loss: 0.07208035886287689 = 0.003831992857158184 + 0.01 * 6.8248372077941895
Epoch 860, val loss: 0.9156791567802429
Epoch 870, training loss: 0.07192115485668182 = 0.0037338680122047663 + 0.01 * 6.818729400634766
Epoch 870, val loss: 0.9186363220214844
Epoch 880, training loss: 0.07202013581991196 = 0.0036401082761585712 + 0.01 * 6.838003158569336
Epoch 880, val loss: 0.9215444922447205
Epoch 890, training loss: 0.07163450866937637 = 0.0035512102767825127 + 0.01 * 6.808330535888672
Epoch 890, val loss: 0.9243432879447937
Epoch 900, training loss: 0.07165291160345078 = 0.0034658953081816435 + 0.01 * 6.818701267242432
Epoch 900, val loss: 0.927098274230957
Epoch 910, training loss: 0.07139105349779129 = 0.0033844783902168274 + 0.01 * 6.800657749176025
Epoch 910, val loss: 0.9298480749130249
Epoch 920, training loss: 0.0712820291519165 = 0.0033065062016248703 + 0.01 * 6.797552108764648
Epoch 920, val loss: 0.9325283169746399
Epoch 930, training loss: 0.071071557700634 = 0.003232038114219904 + 0.01 * 6.783951759338379
Epoch 930, val loss: 0.9351782202720642
Epoch 940, training loss: 0.07091857492923737 = 0.0031605109106749296 + 0.01 * 6.775805950164795
Epoch 940, val loss: 0.9376547336578369
Epoch 950, training loss: 0.07097648084163666 = 0.0030919010750949383 + 0.01 * 6.788457870483398
Epoch 950, val loss: 0.9401847720146179
Epoch 960, training loss: 0.07083068042993546 = 0.00302635389380157 + 0.01 * 6.780433177947998
Epoch 960, val loss: 0.9426071047782898
Epoch 970, training loss: 0.07067983597517014 = 0.00296323886141181 + 0.01 * 6.771660327911377
Epoch 970, val loss: 0.9449945688247681
Epoch 980, training loss: 0.07081577181816101 = 0.002902728272601962 + 0.01 * 6.791304111480713
Epoch 980, val loss: 0.9474599957466125
Epoch 990, training loss: 0.07042358815670013 = 0.002844540635123849 + 0.01 * 6.757904529571533
Epoch 990, val loss: 0.9496912360191345
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.0459578037261963 = 1.959989309310913 + 0.01 * 8.5968599319458
Epoch 0, val loss: 1.959832787513733
Epoch 10, training loss: 2.0348575115203857 = 1.9488894939422607 + 0.01 * 8.596813201904297
Epoch 10, val loss: 1.948617935180664
Epoch 20, training loss: 2.0213801860809326 = 1.9354137182235718 + 0.01 * 8.596656799316406
Epoch 20, val loss: 1.9352084398269653
Epoch 30, training loss: 2.0027806758880615 = 1.9168190956115723 + 0.01 * 8.596153259277344
Epoch 30, val loss: 1.9170337915420532
Epoch 40, training loss: 1.9758477210998535 = 1.8899134397506714 + 0.01 * 8.593424797058105
Epoch 40, val loss: 1.8911478519439697
Epoch 50, training loss: 1.9386570453643799 = 1.8529188632965088 + 0.01 * 8.573820114135742
Epoch 50, val loss: 1.8570709228515625
Epoch 60, training loss: 1.8958903551101685 = 1.8110185861587524 + 0.01 * 8.487178802490234
Epoch 60, val loss: 1.821748971939087
Epoch 70, training loss: 1.856345772743225 = 1.7740973234176636 + 0.01 * 8.224846839904785
Epoch 70, val loss: 1.7917238473892212
Epoch 80, training loss: 1.8128395080566406 = 1.7313940525054932 + 0.01 * 8.144549369812012
Epoch 80, val loss: 1.7514870166778564
Epoch 90, training loss: 1.7509289979934692 = 1.670379400253296 + 0.01 * 8.05495834350586
Epoch 90, val loss: 1.6948126554489136
Epoch 100, training loss: 1.6679540872573853 = 1.5880967378616333 + 0.01 * 7.985740661621094
Epoch 100, val loss: 1.6228082180023193
Epoch 110, training loss: 1.5718783140182495 = 1.4928983449935913 + 0.01 * 7.8979973793029785
Epoch 110, val loss: 1.5445808172225952
Epoch 120, training loss: 1.4761326313018799 = 1.3992100954055786 + 0.01 * 7.692258834838867
Epoch 120, val loss: 1.4694522619247437
Epoch 130, training loss: 1.3889014720916748 = 1.314460277557373 + 0.01 * 7.444121360778809
Epoch 130, val loss: 1.4057093858718872
Epoch 140, training loss: 1.3131883144378662 = 1.2397485971450806 + 0.01 * 7.343973636627197
Epoch 140, val loss: 1.3510925769805908
Epoch 150, training loss: 1.2446812391281128 = 1.1718374490737915 + 0.01 * 7.284384250640869
Epoch 150, val loss: 1.3016128540039062
Epoch 160, training loss: 1.1780890226364136 = 1.105454921722412 + 0.01 * 7.263407230377197
Epoch 160, val loss: 1.2533787488937378
Epoch 170, training loss: 1.1078928709030151 = 1.0354421138763428 + 0.01 * 7.245079040527344
Epoch 170, val loss: 1.2021734714508057
Epoch 180, training loss: 1.0312385559082031 = 0.9590038061141968 + 0.01 * 7.223475933074951
Epoch 180, val loss: 1.1446486711502075
Epoch 190, training loss: 0.9491598606109619 = 0.8771816492080688 + 0.01 * 7.197820663452148
Epoch 190, val loss: 1.0814433097839355
Epoch 200, training loss: 0.8657389879226685 = 0.79405277967453 + 0.01 * 7.1686201095581055
Epoch 200, val loss: 1.0168582201004028
Epoch 210, training loss: 0.7859554886817932 = 0.7144374847412109 + 0.01 * 7.151798248291016
Epoch 210, val loss: 0.9563145637512207
Epoch 220, training loss: 0.7130410075187683 = 0.6417626142501831 + 0.01 * 7.127837181091309
Epoch 220, val loss: 0.9038987159729004
Epoch 230, training loss: 0.6485708355903625 = 0.5774444341659546 + 0.01 * 7.112642765045166
Epoch 230, val loss: 0.8614023923873901
Epoch 240, training loss: 0.5921744108200073 = 0.5211710929870605 + 0.01 * 7.100333213806152
Epoch 240, val loss: 0.8286492228507996
Epoch 250, training loss: 0.5425686836242676 = 0.4716953635215759 + 0.01 * 7.087332248687744
Epoch 250, val loss: 0.80438631772995
Epoch 260, training loss: 0.4982295036315918 = 0.4274740517139435 + 0.01 * 7.075545787811279
Epoch 260, val loss: 0.7869939804077148
Epoch 270, training loss: 0.45784085988998413 = 0.3871895372867584 + 0.01 * 7.065134048461914
Epoch 270, val loss: 0.7746958136558533
Epoch 280, training loss: 0.42057228088378906 = 0.35000717639923096 + 0.01 * 7.056509971618652
Epoch 280, val loss: 0.766204833984375
Epoch 290, training loss: 0.38607507944107056 = 0.3155750036239624 + 0.01 * 7.05000638961792
Epoch 290, val loss: 0.760819137096405
Epoch 300, training loss: 0.35434845089912415 = 0.2838251292705536 + 0.01 * 7.052331924438477
Epoch 300, val loss: 0.7581408619880676
Epoch 310, training loss: 0.32516932487487793 = 0.2547362744808197 + 0.01 * 7.043306350708008
Epoch 310, val loss: 0.7579331994056702
Epoch 320, training loss: 0.29857856035232544 = 0.2281680554151535 + 0.01 * 7.041051387786865
Epoch 320, val loss: 0.759958803653717
Epoch 330, training loss: 0.27431756258010864 = 0.20393380522727966 + 0.01 * 7.038374423980713
Epoch 330, val loss: 0.7640003561973572
Epoch 340, training loss: 0.2522311806678772 = 0.1818636655807495 + 0.01 * 7.036752700805664
Epoch 340, val loss: 0.769835352897644
Epoch 350, training loss: 0.23219987750053406 = 0.1618446558713913 + 0.01 * 7.0355224609375
Epoch 350, val loss: 0.777243435382843
Epoch 360, training loss: 0.2141275703907013 = 0.1437966376543045 + 0.01 * 7.033093452453613
Epoch 360, val loss: 0.7858855128288269
Epoch 370, training loss: 0.19794286787509918 = 0.12763363122940063 + 0.01 * 7.030923843383789
Epoch 370, val loss: 0.7956900596618652
Epoch 380, training loss: 0.18355849385261536 = 0.11324205994606018 + 0.01 * 7.031644344329834
Epoch 380, val loss: 0.8063801527023315
Epoch 390, training loss: 0.17076900601387024 = 0.10049556195735931 + 0.01 * 7.02734375
Epoch 390, val loss: 0.8178065419197083
Epoch 400, training loss: 0.15950515866279602 = 0.08925322443246841 + 0.01 * 7.025192737579346
Epoch 400, val loss: 0.8296962976455688
Epoch 410, training loss: 0.14959421753883362 = 0.07936722040176392 + 0.01 * 7.022698879241943
Epoch 410, val loss: 0.8419712781906128
Epoch 420, training loss: 0.14090168476104736 = 0.07067890465259552 + 0.01 * 7.022279262542725
Epoch 420, val loss: 0.8544490933418274
Epoch 430, training loss: 0.13326284289360046 = 0.06307987868785858 + 0.01 * 7.0182976722717285
Epoch 430, val loss: 0.8669991493225098
Epoch 440, training loss: 0.1265851855278015 = 0.056428249925374985 + 0.01 * 7.0156941413879395
Epoch 440, val loss: 0.8795498013496399
Epoch 450, training loss: 0.12073181569576263 = 0.05060048773884773 + 0.01 * 7.013133525848389
Epoch 450, val loss: 0.8919639587402344
Epoch 460, training loss: 0.11559662967920303 = 0.04549399018287659 + 0.01 * 7.0102643966674805
Epoch 460, val loss: 0.9040839076042175
Epoch 470, training loss: 0.11109405755996704 = 0.04101472347974777 + 0.01 * 7.007933616638184
Epoch 470, val loss: 0.9158986210823059
Epoch 480, training loss: 0.10712659358978271 = 0.03707578405737877 + 0.01 * 7.005081653594971
Epoch 480, val loss: 0.9272828698158264
Epoch 490, training loss: 0.10362336039543152 = 0.033604662865400314 + 0.01 * 7.001869201660156
Epoch 490, val loss: 0.9383609294891357
Epoch 500, training loss: 0.10053248703479767 = 0.03053787164390087 + 0.01 * 6.999461650848389
Epoch 500, val loss: 0.9490554332733154
Epoch 510, training loss: 0.09779847413301468 = 0.027819735929369926 + 0.01 * 6.997873783111572
Epoch 510, val loss: 0.9594155550003052
Epoch 520, training loss: 0.09533674269914627 = 0.025406325235962868 + 0.01 * 6.9930419921875
Epoch 520, val loss: 0.9694752097129822
Epoch 530, training loss: 0.09316659718751907 = 0.023259298875927925 + 0.01 * 6.990729808807373
Epoch 530, val loss: 0.9792498350143433
Epoch 540, training loss: 0.09121554344892502 = 0.0213459599763155 + 0.01 * 6.986958980560303
Epoch 540, val loss: 0.9888625741004944
Epoch 550, training loss: 0.08963213115930557 = 0.01963857002556324 + 0.01 * 6.999356746673584
Epoch 550, val loss: 0.9981679320335388
Epoch 560, training loss: 0.08796979486942291 = 0.018116313964128494 + 0.01 * 6.985348701477051
Epoch 560, val loss: 1.007259488105774
Epoch 570, training loss: 0.08653876930475235 = 0.016754671931266785 + 0.01 * 6.978410243988037
Epoch 570, val loss: 1.0161325931549072
Epoch 580, training loss: 0.0852835476398468 = 0.015534092672169209 + 0.01 * 6.974945545196533
Epoch 580, val loss: 1.024819016456604
Epoch 590, training loss: 0.08415298163890839 = 0.014437771402299404 + 0.01 * 6.971521377563477
Epoch 590, val loss: 1.0333189964294434
Epoch 600, training loss: 0.08319950848817825 = 0.013450601138174534 + 0.01 * 6.974891185760498
Epoch 600, val loss: 1.0416117906570435
Epoch 610, training loss: 0.08222338557243347 = 0.012561379000544548 + 0.01 * 6.966200828552246
Epoch 610, val loss: 1.0496704578399658
Epoch 620, training loss: 0.08139351010322571 = 0.011757866479456425 + 0.01 * 6.963564872741699
Epoch 620, val loss: 1.0574629306793213
Epoch 630, training loss: 0.08061695843935013 = 0.011028775945305824 + 0.01 * 6.958818435668945
Epoch 630, val loss: 1.065068244934082
Epoch 640, training loss: 0.07991737872362137 = 0.010365203954279423 + 0.01 * 6.955217361450195
Epoch 640, val loss: 1.0724605321884155
Epoch 650, training loss: 0.07934930175542831 = 0.009759532287716866 + 0.01 * 6.958977222442627
Epoch 650, val loss: 1.0796175003051758
Epoch 660, training loss: 0.0786975622177124 = 0.009207439608871937 + 0.01 * 6.949012279510498
Epoch 660, val loss: 1.0865801572799683
Epoch 670, training loss: 0.0781717449426651 = 0.008702198043465614 + 0.01 * 6.94695520401001
Epoch 670, val loss: 1.0933152437210083
Epoch 680, training loss: 0.07766850292682648 = 0.008238239213824272 + 0.01 * 6.943026542663574
Epoch 680, val loss: 1.0998743772506714
Epoch 690, training loss: 0.07725544273853302 = 0.007811968680471182 + 0.01 * 6.944347381591797
Epoch 690, val loss: 1.1062078475952148
Epoch 700, training loss: 0.0767744779586792 = 0.007419656030833721 + 0.01 * 6.935482501983643
Epoch 700, val loss: 1.1123700141906738
Epoch 710, training loss: 0.07641113549470901 = 0.007057860027998686 + 0.01 * 6.935327529907227
Epoch 710, val loss: 1.1183571815490723
Epoch 720, training loss: 0.0760292336344719 = 0.006723881233483553 + 0.01 * 6.930535316467285
Epoch 720, val loss: 1.1241391897201538
Epoch 730, training loss: 0.07571861147880554 = 0.006414688657969236 + 0.01 * 6.930392742156982
Epoch 730, val loss: 1.12973952293396
Epoch 740, training loss: 0.07536269724369049 = 0.006127916742116213 + 0.01 * 6.923478126525879
Epoch 740, val loss: 1.1351574659347534
Epoch 750, training loss: 0.07512716948986053 = 0.00586159061640501 + 0.01 * 6.926558494567871
Epoch 750, val loss: 1.140444278717041
Epoch 760, training loss: 0.07482526451349258 = 0.005614564288407564 + 0.01 * 6.921070098876953
Epoch 760, val loss: 1.1455190181732178
Epoch 770, training loss: 0.07452714443206787 = 0.005384479649364948 + 0.01 * 6.914266586303711
Epoch 770, val loss: 1.1504744291305542
Epoch 780, training loss: 0.07426358014345169 = 0.005169820971786976 + 0.01 * 6.9093756675720215
Epoch 780, val loss: 1.155289649963379
Epoch 790, training loss: 0.07413770258426666 = 0.004969317931681871 + 0.01 * 6.916838645935059
Epoch 790, val loss: 1.159946084022522
Epoch 800, training loss: 0.07391107827425003 = 0.004781693685799837 + 0.01 * 6.912938594818115
Epoch 800, val loss: 1.1644865274429321
Epoch 810, training loss: 0.07366576045751572 = 0.004606114234775305 + 0.01 * 6.9059648513793945
Epoch 810, val loss: 1.1688463687896729
Epoch 820, training loss: 0.07341074198484421 = 0.004441320896148682 + 0.01 * 6.896942138671875
Epoch 820, val loss: 1.1730833053588867
Epoch 830, training loss: 0.07347515225410461 = 0.004286367446184158 + 0.01 * 6.918878078460693
Epoch 830, val loss: 1.177247166633606
Epoch 840, training loss: 0.07309029251337051 = 0.004141121171414852 + 0.01 * 6.8949174880981445
Epoch 840, val loss: 1.181241750717163
Epoch 850, training loss: 0.07292424142360687 = 0.004004454240202904 + 0.01 * 6.891978740692139
Epoch 850, val loss: 1.1850807666778564
Epoch 860, training loss: 0.07277283817529678 = 0.0038754816632717848 + 0.01 * 6.889736175537109
Epoch 860, val loss: 1.1888649463653564
Epoch 870, training loss: 0.07258306443691254 = 0.0037536881864070892 + 0.01 * 6.882937908172607
Epoch 870, val loss: 1.1925619840621948
Epoch 880, training loss: 0.07277649641036987 = 0.0036385958082973957 + 0.01 * 6.913790225982666
Epoch 880, val loss: 1.196138620376587
Epoch 890, training loss: 0.07235594838857651 = 0.003530109766870737 + 0.01 * 6.882584571838379
Epoch 890, val loss: 1.199616551399231
Epoch 900, training loss: 0.07218624651432037 = 0.0034273623023182154 + 0.01 * 6.875888347625732
Epoch 900, val loss: 1.2029428482055664
Epoch 910, training loss: 0.07224486023187637 = 0.0033299236092716455 + 0.01 * 6.891493320465088
Epoch 910, val loss: 1.2062290906906128
Epoch 920, training loss: 0.07204966247081757 = 0.0032375743612647057 + 0.01 * 6.881208896636963
Epoch 920, val loss: 1.2094045877456665
Epoch 930, training loss: 0.07185615599155426 = 0.003149846801534295 + 0.01 * 6.870631217956543
Epoch 930, val loss: 1.2125418186187744
Epoch 940, training loss: 0.0717562660574913 = 0.0030664291698485613 + 0.01 * 6.868983745574951
Epoch 940, val loss: 1.2155574560165405
Epoch 950, training loss: 0.0717211440205574 = 0.0029871445149183273 + 0.01 * 6.8734002113342285
Epoch 950, val loss: 1.2185221910476685
Epoch 960, training loss: 0.07154783606529236 = 0.0029118130914866924 + 0.01 * 6.863602638244629
Epoch 960, val loss: 1.2213551998138428
Epoch 970, training loss: 0.07142888754606247 = 0.0028400991577655077 + 0.01 * 6.8588786125183105
Epoch 970, val loss: 1.224131464958191
Epoch 980, training loss: 0.07146475464105606 = 0.0027717086486518383 + 0.01 * 6.86930513381958
Epoch 980, val loss: 1.2268444299697876
Epoch 990, training loss: 0.07121268659830093 = 0.002706569153815508 + 0.01 * 6.850611686706543
Epoch 990, val loss: 1.2294877767562866
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8418555614127571
The final CL Acc:0.81235, 0.00761, The final GNN Acc:0.84027, 0.00129
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9438])
updated graph: torch.Size([2, 10518])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.012922763824463 = 1.926954746246338 + 0.01 * 8.5968017578125
Epoch 0, val loss: 1.9246735572814941
Epoch 10, training loss: 2.004063606262207 = 1.9180961847305298 + 0.01 * 8.596747398376465
Epoch 10, val loss: 1.9154000282287598
Epoch 20, training loss: 1.9933511018753052 = 1.9073858261108398 + 0.01 * 8.596529006958008
Epoch 20, val loss: 1.9040945768356323
Epoch 30, training loss: 1.9784494638442993 = 1.8924912214279175 + 0.01 * 8.59582805633545
Epoch 30, val loss: 1.8884050846099854
Epoch 40, training loss: 1.956950306892395 = 1.8710286617279053 + 0.01 * 8.592167854309082
Epoch 40, val loss: 1.8659679889678955
Epoch 50, training loss: 1.927199125289917 = 1.841524362564087 + 0.01 * 8.567475318908691
Epoch 50, val loss: 1.8362551927566528
Epoch 60, training loss: 1.8929951190948486 = 1.808717966079712 + 0.01 * 8.427712440490723
Epoch 60, val loss: 1.806174874305725
Epoch 70, training loss: 1.8603640794754028 = 1.7785903215408325 + 0.01 * 8.177377700805664
Epoch 70, val loss: 1.781074047088623
Epoch 80, training loss: 1.8186368942260742 = 1.740169644355774 + 0.01 * 7.846729278564453
Epoch 80, val loss: 1.74794602394104
Epoch 90, training loss: 1.7611961364746094 = 1.685975193977356 + 0.01 * 7.522097110748291
Epoch 90, val loss: 1.7001088857650757
Epoch 100, training loss: 1.6861497163772583 = 1.6122796535491943 + 0.01 * 7.387005805969238
Epoch 100, val loss: 1.636579990386963
Epoch 110, training loss: 1.5951297283172607 = 1.5217821598052979 + 0.01 * 7.334757328033447
Epoch 110, val loss: 1.5628650188446045
Epoch 120, training loss: 1.495482325553894 = 1.4223508834838867 + 0.01 * 7.31314754486084
Epoch 120, val loss: 1.4856445789337158
Epoch 130, training loss: 1.3941400051116943 = 1.321182131767273 + 0.01 * 7.295786380767822
Epoch 130, val loss: 1.4115372896194458
Epoch 140, training loss: 1.2934306859970093 = 1.2206369638442993 + 0.01 * 7.279374599456787
Epoch 140, val loss: 1.3389836549758911
Epoch 150, training loss: 1.1938331127166748 = 1.121170163154602 + 0.01 * 7.266294956207275
Epoch 150, val loss: 1.2668967247009277
Epoch 160, training loss: 1.0965056419372559 = 1.0239673852920532 + 0.01 * 7.253829002380371
Epoch 160, val loss: 1.1959837675094604
Epoch 170, training loss: 1.0028371810913086 = 0.9304494857788086 + 0.01 * 7.238771438598633
Epoch 170, val loss: 1.1281036138534546
Epoch 180, training loss: 0.9144023656845093 = 0.8422331809997559 + 0.01 * 7.216918468475342
Epoch 180, val loss: 1.0643614530563354
Epoch 190, training loss: 0.8330413103103638 = 0.7611966133117676 + 0.01 * 7.184472560882568
Epoch 190, val loss: 1.0073015689849854
Epoch 200, training loss: 0.7605850100517273 = 0.689051628112793 + 0.01 * 7.153336048126221
Epoch 200, val loss: 0.9579358100891113
Epoch 210, training loss: 0.6974306702613831 = 0.6261410713195801 + 0.01 * 7.128959655761719
Epoch 210, val loss: 0.9170209765434265
Epoch 220, training loss: 0.6421481370925903 = 0.5709937810897827 + 0.01 * 7.115437030792236
Epoch 220, val loss: 0.8843138813972473
Epoch 230, training loss: 0.5924574136734009 = 0.5214413404464722 + 0.01 * 7.101606845855713
Epoch 230, val loss: 0.8583551645278931
Epoch 240, training loss: 0.5463400483131409 = 0.47543245553970337 + 0.01 * 7.090758323669434
Epoch 240, val loss: 0.8376103043556213
Epoch 250, training loss: 0.502265453338623 = 0.4314537048339844 + 0.01 * 7.081174373626709
Epoch 250, val loss: 0.8208775520324707
Epoch 260, training loss: 0.45943784713745117 = 0.38867491483688354 + 0.01 * 7.076294422149658
Epoch 260, val loss: 0.8074889183044434
Epoch 270, training loss: 0.4176235496997833 = 0.34692543745040894 + 0.01 * 7.069811820983887
Epoch 270, val loss: 0.7971220016479492
Epoch 280, training loss: 0.3771239221096039 = 0.3065164089202881 + 0.01 * 7.060750961303711
Epoch 280, val loss: 0.7898550033569336
Epoch 290, training loss: 0.33877289295196533 = 0.2682477533817291 + 0.01 * 7.052513599395752
Epoch 290, val loss: 0.7861048579216003
Epoch 300, training loss: 0.303629606962204 = 0.23301033675670624 + 0.01 * 7.061927318572998
Epoch 300, val loss: 0.7863039374351501
Epoch 310, training loss: 0.2720223069190979 = 0.20150980353355408 + 0.01 * 7.051251411437988
Epoch 310, val loss: 0.7904839515686035
Epoch 320, training loss: 0.24438756704330444 = 0.17397953569889069 + 0.01 * 7.0408034324646
Epoch 320, val loss: 0.7981812953948975
Epoch 330, training loss: 0.2206101268529892 = 0.150273397564888 + 0.01 * 7.033673286437988
Epoch 330, val loss: 0.8088932037353516
Epoch 340, training loss: 0.20029176771640778 = 0.129999041557312 + 0.01 * 7.02927303314209
Epoch 340, val loss: 0.8219038844108582
Epoch 350, training loss: 0.18310874700546265 = 0.11270279437303543 + 0.01 * 7.040595054626465
Epoch 350, val loss: 0.8367579579353333
Epoch 360, training loss: 0.16827929019927979 = 0.09797722846269608 + 0.01 * 7.030205726623535
Epoch 360, val loss: 0.8528338670730591
Epoch 370, training loss: 0.15563154220581055 = 0.08541084080934525 + 0.01 * 7.022069931030273
Epoch 370, val loss: 0.869878888130188
Epoch 380, training loss: 0.14485682547092438 = 0.07469377666711807 + 0.01 * 7.016304969787598
Epoch 380, val loss: 0.8875052332878113
Epoch 390, training loss: 0.13568425178527832 = 0.0655645802617073 + 0.01 * 7.011966228485107
Epoch 390, val loss: 0.9054608345031738
Epoch 400, training loss: 0.12787768244743347 = 0.05779082328081131 + 0.01 * 7.008685111999512
Epoch 400, val loss: 0.923577070236206
Epoch 410, training loss: 0.12120422720909119 = 0.051157888025045395 + 0.01 * 7.004634380340576
Epoch 410, val loss: 0.9415750503540039
Epoch 420, training loss: 0.11551174521446228 = 0.04549339413642883 + 0.01 * 7.001835346221924
Epoch 420, val loss: 0.9593244791030884
Epoch 430, training loss: 0.11068332940340042 = 0.04064396768808365 + 0.01 * 7.003936290740967
Epoch 430, val loss: 0.9766625165939331
Epoch 440, training loss: 0.10642495006322861 = 0.03647208958864212 + 0.01 * 6.995286464691162
Epoch 440, val loss: 0.9935291409492493
Epoch 450, training loss: 0.10282466560602188 = 0.03286833316087723 + 0.01 * 6.995633602142334
Epoch 450, val loss: 1.0098544359207153
Epoch 460, training loss: 0.09961754083633423 = 0.029741819947957993 + 0.01 * 6.98757266998291
Epoch 460, val loss: 1.0256726741790771
Epoch 470, training loss: 0.09681300818920135 = 0.02702152170240879 + 0.01 * 6.979148864746094
Epoch 470, val loss: 1.0409553050994873
Epoch 480, training loss: 0.0943971574306488 = 0.02464573085308075 + 0.01 * 6.975142955780029
Epoch 480, val loss: 1.0556857585906982
Epoch 490, training loss: 0.09228935092687607 = 0.0225616917014122 + 0.01 * 6.972765922546387
Epoch 490, val loss: 1.0698766708374023
Epoch 500, training loss: 0.09041353315114975 = 0.020727114751935005 + 0.01 * 6.968642234802246
Epoch 500, val loss: 1.0835391283035278
Epoch 510, training loss: 0.08872298896312714 = 0.01910547912120819 + 0.01 * 6.961751461029053
Epoch 510, val loss: 1.0967328548431396
Epoch 520, training loss: 0.08723864704370499 = 0.017666658386588097 + 0.01 * 6.957198619842529
Epoch 520, val loss: 1.1094036102294922
Epoch 530, training loss: 0.08598128706216812 = 0.016386620700359344 + 0.01 * 6.959466934204102
Epoch 530, val loss: 1.121600866317749
Epoch 540, training loss: 0.08474736660718918 = 0.015245078131556511 + 0.01 * 6.950229167938232
Epoch 540, val loss: 1.1331841945648193
Epoch 550, training loss: 0.08371075987815857 = 0.014221284538507462 + 0.01 * 6.948947429656982
Epoch 550, val loss: 1.1444865465164185
Epoch 560, training loss: 0.08273962140083313 = 0.013299939222633839 + 0.01 * 6.943967819213867
Epoch 560, val loss: 1.1553422212600708
Epoch 570, training loss: 0.08191506564617157 = 0.012467984110116959 + 0.01 * 6.944708824157715
Epoch 570, val loss: 1.1657339334487915
Epoch 580, training loss: 0.0809662863612175 = 0.01171453483402729 + 0.01 * 6.925175189971924
Epoch 580, val loss: 1.1759004592895508
Epoch 590, training loss: 0.08030679076910019 = 0.011030043475329876 + 0.01 * 6.927675247192383
Epoch 590, val loss: 1.1855778694152832
Epoch 600, training loss: 0.07957621663808823 = 0.010405701585114002 + 0.01 * 6.917051315307617
Epoch 600, val loss: 1.19504976272583
Epoch 610, training loss: 0.07886810600757599 = 0.00983455777168274 + 0.01 * 6.903354644775391
Epoch 610, val loss: 1.204148769378662
Epoch 620, training loss: 0.07831385731697083 = 0.009310276247560978 + 0.01 * 6.900358200073242
Epoch 620, val loss: 1.2129448652267456
Epoch 630, training loss: 0.07797742635011673 = 0.00882897712290287 + 0.01 * 6.9148454666137695
Epoch 630, val loss: 1.221508502960205
Epoch 640, training loss: 0.07725799083709717 = 0.008386322297155857 + 0.01 * 6.887167453765869
Epoch 640, val loss: 1.2297167778015137
Epoch 650, training loss: 0.07699901610612869 = 0.007977641187608242 + 0.01 * 6.902137279510498
Epoch 650, val loss: 1.2377305030822754
Epoch 660, training loss: 0.07647145539522171 = 0.007600859273225069 + 0.01 * 6.887059688568115
Epoch 660, val loss: 1.2454663515090942
Epoch 670, training loss: 0.07597985863685608 = 0.007251601666212082 + 0.01 * 6.872826099395752
Epoch 670, val loss: 1.2529492378234863
Epoch 680, training loss: 0.0756184309720993 = 0.00692781014367938 + 0.01 * 6.869062423706055
Epoch 680, val loss: 1.2602578401565552
Epoch 690, training loss: 0.07521547377109528 = 0.0066264308989048 + 0.01 * 6.858904838562012
Epoch 690, val loss: 1.2673439979553223
Epoch 700, training loss: 0.0749068632721901 = 0.006345872301608324 + 0.01 * 6.8560991287231445
Epoch 700, val loss: 1.274170994758606
Epoch 710, training loss: 0.07472053915262222 = 0.0060845171101391315 + 0.01 * 6.863602638244629
Epoch 710, val loss: 1.280889630317688
Epoch 720, training loss: 0.07444766908884048 = 0.005840230267494917 + 0.01 * 6.860743999481201
Epoch 720, val loss: 1.2873069047927856
Epoch 730, training loss: 0.07404699921607971 = 0.005612799432128668 + 0.01 * 6.843420028686523
Epoch 730, val loss: 1.293708086013794
Epoch 740, training loss: 0.07374629378318787 = 0.005399288609623909 + 0.01 * 6.834700584411621
Epoch 740, val loss: 1.299766182899475
Epoch 750, training loss: 0.07392287999391556 = 0.0051988158375024796 + 0.01 * 6.872406482696533
Epoch 750, val loss: 1.3057383298873901
Epoch 760, training loss: 0.07335272431373596 = 0.005011069122701883 + 0.01 * 6.834166049957275
Epoch 760, val loss: 1.311503291130066
Epoch 770, training loss: 0.073104128241539 = 0.004834792111068964 + 0.01 * 6.82693338394165
Epoch 770, val loss: 1.3171018362045288
Epoch 780, training loss: 0.07293310761451721 = 0.004668626468628645 + 0.01 * 6.826448917388916
Epoch 780, val loss: 1.3225514888763428
Epoch 790, training loss: 0.07279060781002045 = 0.004512075334787369 + 0.01 * 6.827853679656982
Epoch 790, val loss: 1.327906608581543
Epoch 800, training loss: 0.07247723639011383 = 0.004364405293017626 + 0.01 * 6.811282634735107
Epoch 800, val loss: 1.3330312967300415
Epoch 810, training loss: 0.07253297418355942 = 0.0042248317040503025 + 0.01 * 6.830814361572266
Epoch 810, val loss: 1.3381305932998657
Epoch 820, training loss: 0.07224887609481812 = 0.004092615097761154 + 0.01 * 6.8156256675720215
Epoch 820, val loss: 1.3430359363555908
Epoch 830, training loss: 0.0721021294593811 = 0.003967937082052231 + 0.01 * 6.813418865203857
Epoch 830, val loss: 1.3477156162261963
Epoch 840, training loss: 0.07201241701841354 = 0.0038497319910675287 + 0.01 * 6.8162689208984375
Epoch 840, val loss: 1.352512001991272
Epoch 850, training loss: 0.07175891101360321 = 0.003737576538696885 + 0.01 * 6.802133560180664
Epoch 850, val loss: 1.356990098953247
Epoch 860, training loss: 0.07166603207588196 = 0.003631059778854251 + 0.01 * 6.803497314453125
Epoch 860, val loss: 1.3613919019699097
Epoch 870, training loss: 0.07145456969738007 = 0.003529881127178669 + 0.01 * 6.792469024658203
Epoch 870, val loss: 1.3657335042953491
Epoch 880, training loss: 0.07142098993062973 = 0.003433479694649577 + 0.01 * 6.798751354217529
Epoch 880, val loss: 1.3698400259017944
Epoch 890, training loss: 0.07134578377008438 = 0.0033422342967242002 + 0.01 * 6.800355434417725
Epoch 890, val loss: 1.3740267753601074
Epoch 900, training loss: 0.07118625193834305 = 0.0032550825271755457 + 0.01 * 6.793117046356201
Epoch 900, val loss: 1.3778789043426514
Epoch 910, training loss: 0.07096526026725769 = 0.003171902149915695 + 0.01 * 6.7793354988098145
Epoch 910, val loss: 1.381980299949646
Epoch 920, training loss: 0.07114148139953613 = 0.0030924875754863024 + 0.01 * 6.804899215698242
Epoch 920, val loss: 1.3856059312820435
Epoch 930, training loss: 0.0707361176609993 = 0.0030171324033290148 + 0.01 * 6.7718987464904785
Epoch 930, val loss: 1.3893457651138306
Epoch 940, training loss: 0.0711117759346962 = 0.0029447772540152073 + 0.01 * 6.816700458526611
Epoch 940, val loss: 1.3930052518844604
Epoch 950, training loss: 0.07061231136322021 = 0.0028762619476765394 + 0.01 * 6.7736053466796875
Epoch 950, val loss: 1.3965113162994385
Epoch 960, training loss: 0.07046212255954742 = 0.0028103727381676435 + 0.01 * 6.7651753425598145
Epoch 960, val loss: 1.3999158143997192
Epoch 970, training loss: 0.07034967839717865 = 0.002747190883383155 + 0.01 * 6.76024866104126
Epoch 970, val loss: 1.403235673904419
Epoch 980, training loss: 0.07041347771883011 = 0.002686689607799053 + 0.01 * 6.772679328918457
Epoch 980, val loss: 1.406612753868103
Epoch 990, training loss: 0.07025155425071716 = 0.0026288365479558706 + 0.01 * 6.762272357940674
Epoch 990, val loss: 1.40972101688385
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 2.0315802097320557 = 1.9456119537353516 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9430299997329712
Epoch 10, training loss: 2.021247148513794 = 1.9352794885635376 + 0.01 * 8.59676456451416
Epoch 10, val loss: 1.9335445165634155
Epoch 20, training loss: 2.0085856914520264 = 1.9226202964782715 + 0.01 * 8.596539497375488
Epoch 20, val loss: 1.9213846921920776
Epoch 30, training loss: 1.990903377532959 = 1.904944896697998 + 0.01 * 8.595845222473145
Epoch 30, val loss: 1.9041908979415894
Epoch 40, training loss: 1.9653339385986328 = 1.8794116973876953 + 0.01 * 8.5922212600708
Epoch 40, val loss: 1.8796436786651611
Epoch 50, training loss: 1.9309889078140259 = 1.8453096151351929 + 0.01 * 8.567924499511719
Epoch 50, val loss: 1.8484324216842651
Epoch 60, training loss: 1.8945813179016113 = 1.8101615905761719 + 0.01 * 8.441967964172363
Epoch 60, val loss: 1.8192800283432007
Epoch 70, training loss: 1.8646392822265625 = 1.7821581363677979 + 0.01 * 8.248115539550781
Epoch 70, val loss: 1.7948962450027466
Epoch 80, training loss: 1.8276751041412354 = 1.7470242977142334 + 0.01 * 8.065084457397461
Epoch 80, val loss: 1.7601454257965088
Epoch 90, training loss: 1.7752739191055298 = 1.6978275775909424 + 0.01 * 7.744636058807373
Epoch 90, val loss: 1.7158476114273071
Epoch 100, training loss: 1.7036901712417603 = 1.629313588142395 + 0.01 * 7.437656402587891
Epoch 100, val loss: 1.657619833946228
Epoch 110, training loss: 1.6171492338180542 = 1.5442719459533691 + 0.01 * 7.28772497177124
Epoch 110, val loss: 1.5870709419250488
Epoch 120, training loss: 1.525416612625122 = 1.4528530836105347 + 0.01 * 7.2563581466674805
Epoch 120, val loss: 1.513812780380249
Epoch 130, training loss: 1.4336678981781006 = 1.3612303733825684 + 0.01 * 7.243756294250488
Epoch 130, val loss: 1.4432623386383057
Epoch 140, training loss: 1.341102123260498 = 1.2687495946884155 + 0.01 * 7.2352519035339355
Epoch 140, val loss: 1.3748207092285156
Epoch 150, training loss: 1.2461357116699219 = 1.1738325357437134 + 0.01 * 7.230317115783691
Epoch 150, val loss: 1.306015968322754
Epoch 160, training loss: 1.149271845817566 = 1.0770169496536255 + 0.01 * 7.225494861602783
Epoch 160, val loss: 1.2372936010360718
Epoch 170, training loss: 1.052611231803894 = 0.9804356098175049 + 0.01 * 7.2175612449646
Epoch 170, val loss: 1.1694056987762451
Epoch 180, training loss: 0.9595258831977844 = 0.8874936103820801 + 0.01 * 7.203225612640381
Epoch 180, val loss: 1.1044780015945435
Epoch 190, training loss: 0.873831033706665 = 0.8020347356796265 + 0.01 * 7.179632663726807
Epoch 190, val loss: 1.0451099872589111
Epoch 200, training loss: 0.797920823097229 = 0.7263780832290649 + 0.01 * 7.154273509979248
Epoch 200, val loss: 0.9930053949356079
Epoch 210, training loss: 0.731315016746521 = 0.660057008266449 + 0.01 * 7.125801086425781
Epoch 210, val loss: 0.9493160843849182
Epoch 220, training loss: 0.6719527840614319 = 0.6008552312850952 + 0.01 * 7.109755516052246
Epoch 220, val loss: 0.912720263004303
Epoch 230, training loss: 0.6175786256790161 = 0.5465458035469055 + 0.01 * 7.1032819747924805
Epoch 230, val loss: 0.8825970888137817
Epoch 240, training loss: 0.5667741298675537 = 0.4958464503288269 + 0.01 * 7.092767715454102
Epoch 240, val loss: 0.8583311438560486
Epoch 250, training loss: 0.5189802050590515 = 0.44813957810401917 + 0.01 * 7.0840630531311035
Epoch 250, val loss: 0.8394267559051514
Epoch 260, training loss: 0.4738912880420685 = 0.4031161665916443 + 0.01 * 7.077513217926025
Epoch 260, val loss: 0.8254421353340149
Epoch 270, training loss: 0.4312848150730133 = 0.3605923652648926 + 0.01 * 7.069244861602783
Epoch 270, val loss: 0.8155097961425781
Epoch 280, training loss: 0.391165167093277 = 0.3205709457397461 + 0.01 * 7.059421539306641
Epoch 280, val loss: 0.8088128566741943
Epoch 290, training loss: 0.353832870721817 = 0.2832891643047333 + 0.01 * 7.054369926452637
Epoch 290, val loss: 0.8049851059913635
Epoch 300, training loss: 0.31965234875679016 = 0.24914516508579254 + 0.01 * 7.050719261169434
Epoch 300, val loss: 0.8040509819984436
Epoch 310, training loss: 0.2889133393764496 = 0.21847525238990784 + 0.01 * 7.0438079833984375
Epoch 310, val loss: 0.8058676719665527
Epoch 320, training loss: 0.2617843449115753 = 0.19140854477882385 + 0.01 * 7.0375800132751465
Epoch 320, val loss: 0.8102313280105591
Epoch 330, training loss: 0.23816370964050293 = 0.16780194640159607 + 0.01 * 7.036176681518555
Epoch 330, val loss: 0.8167673945426941
Epoch 340, training loss: 0.21766397356987 = 0.14732305705547333 + 0.01 * 7.034092426300049
Epoch 340, val loss: 0.8251609802246094
Epoch 350, training loss: 0.19983533024787903 = 0.1295568346977234 + 0.01 * 7.027850151062012
Epoch 350, val loss: 0.8351598381996155
Epoch 360, training loss: 0.18445196747779846 = 0.11411570757627487 + 0.01 * 7.03362512588501
Epoch 360, val loss: 0.8463844656944275
Epoch 370, training loss: 0.17089778184890747 = 0.1006724163889885 + 0.01 * 7.0225372314453125
Epoch 370, val loss: 0.8584780097007751
Epoch 380, training loss: 0.1591358780860901 = 0.08894461393356323 + 0.01 * 7.019125938415527
Epoch 380, val loss: 0.8712664842605591
Epoch 390, training loss: 0.14885777235031128 = 0.07870014756917953 + 0.01 * 7.015761852264404
Epoch 390, val loss: 0.8844896554946899
Epoch 400, training loss: 0.14004993438720703 = 0.06975024193525314 + 0.01 * 7.029968738555908
Epoch 400, val loss: 0.898019015789032
Epoch 410, training loss: 0.13205255568027496 = 0.06193745508790016 + 0.01 * 7.011510848999023
Epoch 410, val loss: 0.9116749167442322
Epoch 420, training loss: 0.12523284554481506 = 0.05510646477341652 + 0.01 * 7.012637615203857
Epoch 420, val loss: 0.9253917336463928
Epoch 430, training loss: 0.11924176663160324 = 0.04913865029811859 + 0.01 * 7.010312080383301
Epoch 430, val loss: 0.9390881657600403
Epoch 440, training loss: 0.11396868526935577 = 0.04392983019351959 + 0.01 * 7.003885746002197
Epoch 440, val loss: 0.952636182308197
Epoch 450, training loss: 0.10961126536130905 = 0.03938637673854828 + 0.01 * 7.022489070892334
Epoch 450, val loss: 0.9659836292266846
Epoch 460, training loss: 0.10542269051074982 = 0.035432226955890656 + 0.01 * 6.999046325683594
Epoch 460, val loss: 0.9790534377098083
Epoch 470, training loss: 0.10199014842510223 = 0.031984053552150726 + 0.01 * 7.000609397888184
Epoch 470, val loss: 0.9918810725212097
Epoch 480, training loss: 0.09893020242452621 = 0.028972772881388664 + 0.01 * 6.995743274688721
Epoch 480, val loss: 1.0043861865997314
Epoch 490, training loss: 0.09625419229269028 = 0.026341108605265617 + 0.01 * 6.991308689117432
Epoch 490, val loss: 1.0165644884109497
Epoch 500, training loss: 0.09403130412101746 = 0.02403264120221138 + 0.01 * 6.999866962432861
Epoch 500, val loss: 1.0283517837524414
Epoch 510, training loss: 0.09191758930683136 = 0.022003088146448135 + 0.01 * 6.991450309753418
Epoch 510, val loss: 1.0397608280181885
Epoch 520, training loss: 0.09007009863853455 = 0.020212043076753616 + 0.01 * 6.985805034637451
Epoch 520, val loss: 1.0508166551589966
Epoch 530, training loss: 0.08841104060411453 = 0.01862606592476368 + 0.01 * 6.978497505187988
Epoch 530, val loss: 1.0614851713180542
Epoch 540, training loss: 0.0870361253619194 = 0.017216680571436882 + 0.01 * 6.9819440841674805
Epoch 540, val loss: 1.0717968940734863
Epoch 550, training loss: 0.08568911254405975 = 0.01596076413989067 + 0.01 * 6.972835540771484
Epoch 550, val loss: 1.0817850828170776
Epoch 560, training loss: 0.08459586650133133 = 0.014837832190096378 + 0.01 * 6.975803375244141
Epoch 560, val loss: 1.0914326906204224
Epoch 570, training loss: 0.08354602754116058 = 0.01383216306567192 + 0.01 * 6.971386909484863
Epoch 570, val loss: 1.1007205247879028
Epoch 580, training loss: 0.08255748450756073 = 0.0129278264939785 + 0.01 * 6.962966442108154
Epoch 580, val loss: 1.1097455024719238
Epoch 590, training loss: 0.08171914517879486 = 0.01211195345968008 + 0.01 * 6.960719585418701
Epoch 590, val loss: 1.1184399127960205
Epoch 600, training loss: 0.0809633731842041 = 0.01137477345764637 + 0.01 * 6.958860397338867
Epoch 600, val loss: 1.1268354654312134
Epoch 610, training loss: 0.08021189272403717 = 0.010705549269914627 + 0.01 * 6.950634956359863
Epoch 610, val loss: 1.1349716186523438
Epoch 620, training loss: 0.07968582957983017 = 0.010096045210957527 + 0.01 * 6.958978652954102
Epoch 620, val loss: 1.1428031921386719
Epoch 630, training loss: 0.07902773469686508 = 0.009540506638586521 + 0.01 * 6.948723316192627
Epoch 630, val loss: 1.1504534482955933
Epoch 640, training loss: 0.07848849892616272 = 0.009032421745359898 + 0.01 * 6.945608139038086
Epoch 640, val loss: 1.157771110534668
Epoch 650, training loss: 0.0779682844877243 = 0.008567252196371555 + 0.01 * 6.940103530883789
Epoch 650, val loss: 1.1648989915847778
Epoch 660, training loss: 0.07756055891513824 = 0.008139797486364841 + 0.01 * 6.942076683044434
Epoch 660, val loss: 1.171770453453064
Epoch 670, training loss: 0.07716203480958939 = 0.007746705319732428 + 0.01 * 6.94153356552124
Epoch 670, val loss: 1.1784579753875732
Epoch 680, training loss: 0.07669872790575027 = 0.007383954245597124 + 0.01 * 6.9314775466918945
Epoch 680, val loss: 1.1848406791687012
Epoch 690, training loss: 0.076535664498806 = 0.007048304192721844 + 0.01 * 6.948736667633057
Epoch 690, val loss: 1.1910814046859741
Epoch 700, training loss: 0.07590758800506592 = 0.006738053634762764 + 0.01 * 6.916954040527344
Epoch 700, val loss: 1.197135329246521
Epoch 710, training loss: 0.07578062266111374 = 0.006450104992836714 + 0.01 * 6.9330525398254395
Epoch 710, val loss: 1.2029982805252075
Epoch 720, training loss: 0.07533954083919525 = 0.006182864774018526 + 0.01 * 6.91566801071167
Epoch 720, val loss: 1.2086548805236816
Epoch 730, training loss: 0.07499370723962784 = 0.005934074055403471 + 0.01 * 6.90596342086792
Epoch 730, val loss: 1.214119553565979
Epoch 740, training loss: 0.0747566893696785 = 0.005701899528503418 + 0.01 * 6.9054789543151855
Epoch 740, val loss: 1.2194457054138184
Epoch 750, training loss: 0.074412040412426 = 0.005484755150973797 + 0.01 * 6.892728805541992
Epoch 750, val loss: 1.2246243953704834
Epoch 760, training loss: 0.07411288470029831 = 0.005281414370983839 + 0.01 * 6.883147716522217
Epoch 760, val loss: 1.2296693325042725
Epoch 770, training loss: 0.07387793809175491 = 0.005091608967632055 + 0.01 * 6.878633499145508
Epoch 770, val loss: 1.2345178127288818
Epoch 780, training loss: 0.0738745778799057 = 0.004913769196718931 + 0.01 * 6.89608097076416
Epoch 780, val loss: 1.2391051054000854
Epoch 790, training loss: 0.07341643422842026 = 0.004746987018734217 + 0.01 * 6.866944789886475
Epoch 790, val loss: 1.2436283826828003
Epoch 800, training loss: 0.07317327708005905 = 0.004589837975800037 + 0.01 * 6.858344078063965
Epoch 800, val loss: 1.248030185699463
Epoch 810, training loss: 0.0733303651213646 = 0.004441549070179462 + 0.01 * 6.888881683349609
Epoch 810, val loss: 1.2523736953735352
Epoch 820, training loss: 0.07291886955499649 = 0.004301903303712606 + 0.01 * 6.861696720123291
Epoch 820, val loss: 1.2565940618515015
Epoch 830, training loss: 0.07267750799655914 = 0.0041700974106788635 + 0.01 * 6.850741386413574
Epoch 830, val loss: 1.260603427886963
Epoch 840, training loss: 0.07296343147754669 = 0.004045397974550724 + 0.01 * 6.89180326461792
Epoch 840, val loss: 1.2645326852798462
Epoch 850, training loss: 0.07222476601600647 = 0.0039276862516999245 + 0.01 * 6.829708099365234
Epoch 850, val loss: 1.2683415412902832
Epoch 860, training loss: 0.07221042364835739 = 0.0038159850519150496 + 0.01 * 6.839444160461426
Epoch 860, val loss: 1.271973967552185
Epoch 870, training loss: 0.07206835597753525 = 0.003710022661834955 + 0.01 * 6.835833549499512
Epoch 870, val loss: 1.275633692741394
Epoch 880, training loss: 0.07201793789863586 = 0.0036092011723667383 + 0.01 * 6.840873718261719
Epoch 880, val loss: 1.2790957689285278
Epoch 890, training loss: 0.07184591144323349 = 0.00351347285322845 + 0.01 * 6.833244323730469
Epoch 890, val loss: 1.2825580835342407
Epoch 900, training loss: 0.07165142148733139 = 0.0034224933478981256 + 0.01 * 6.822893142700195
Epoch 900, val loss: 1.2858543395996094
Epoch 910, training loss: 0.07162126153707504 = 0.003335904097184539 + 0.01 * 6.828536033630371
Epoch 910, val loss: 1.289121389389038
Epoch 920, training loss: 0.07128128409385681 = 0.0032532925251871347 + 0.01 * 6.802799224853516
Epoch 920, val loss: 1.2922227382659912
Epoch 930, training loss: 0.07138948887586594 = 0.0031743759755045176 + 0.01 * 6.821511268615723
Epoch 930, val loss: 1.2952418327331543
Epoch 940, training loss: 0.07124125212430954 = 0.003099072491750121 + 0.01 * 6.814218521118164
Epoch 940, val loss: 1.2983371019363403
Epoch 950, training loss: 0.07097278535366058 = 0.0030270565766841173 + 0.01 * 6.794572830200195
Epoch 950, val loss: 1.3011903762817383
Epoch 960, training loss: 0.0709071084856987 = 0.0029583105351775885 + 0.01 * 6.794879913330078
Epoch 960, val loss: 1.3040401935577393
Epoch 970, training loss: 0.07067884504795074 = 0.002892452757805586 + 0.01 * 6.778639793395996
Epoch 970, val loss: 1.3067575693130493
Epoch 980, training loss: 0.07073055952787399 = 0.0028293803334236145 + 0.01 * 6.790118217468262
Epoch 980, val loss: 1.3094464540481567
Epoch 990, training loss: 0.07058645784854889 = 0.002768837148323655 + 0.01 * 6.78176212310791
Epoch 990, val loss: 1.3121070861816406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8212967843964154
=== training gcn model ===
Epoch 0, training loss: 2.018482208251953 = 1.9325140714645386 + 0.01 * 8.59681510925293
Epoch 0, val loss: 1.9303709268569946
Epoch 10, training loss: 2.0088138580322266 = 1.9228461980819702 + 0.01 * 8.596758842468262
Epoch 10, val loss: 1.9201890230178833
Epoch 20, training loss: 1.9970678091049194 = 1.911102533340454 + 0.01 * 8.596525192260742
Epoch 20, val loss: 1.9076495170593262
Epoch 30, training loss: 1.980716347694397 = 1.894758939743042 + 0.01 * 8.595742225646973
Epoch 30, val loss: 1.8900166749954224
Epoch 40, training loss: 1.9571412801742554 = 1.8712302446365356 + 0.01 * 8.591108322143555
Epoch 40, val loss: 1.864797592163086
Epoch 50, training loss: 1.9256254434585571 = 1.8400514125823975 + 0.01 * 8.557404518127441
Epoch 50, val loss: 1.8329015970230103
Epoch 60, training loss: 1.8919193744659424 = 1.8082478046417236 + 0.01 * 8.367151260375977
Epoch 60, val loss: 1.8041441440582275
Epoch 70, training loss: 1.8621991872787476 = 1.780340552330017 + 0.01 * 8.185861587524414
Epoch 70, val loss: 1.781585931777954
Epoch 80, training loss: 1.821488857269287 = 1.7423418760299683 + 0.01 * 7.914702892303467
Epoch 80, val loss: 1.7488672733306885
Epoch 90, training loss: 1.7644926309585571 = 1.6885408163070679 + 0.01 * 7.595176696777344
Epoch 90, val loss: 1.7014405727386475
Epoch 100, training loss: 1.689320683479309 = 1.615340232849121 + 0.01 * 7.39804220199585
Epoch 100, val loss: 1.6385189294815063
Epoch 110, training loss: 1.6025704145431519 = 1.5295335054397583 + 0.01 * 7.303690433502197
Epoch 110, val loss: 1.5689654350280762
Epoch 120, training loss: 1.5126999616622925 = 1.4401166439056396 + 0.01 * 7.258327484130859
Epoch 120, val loss: 1.5010910034179688
Epoch 130, training loss: 1.4208004474639893 = 1.3485336303710938 + 0.01 * 7.226682662963867
Epoch 130, val loss: 1.4340578317642212
Epoch 140, training loss: 1.3241298198699951 = 1.252152442932129 + 0.01 * 7.197741985321045
Epoch 140, val loss: 1.3640583753585815
Epoch 150, training loss: 1.2227705717086792 = 1.151064157485962 + 0.01 * 7.170639991760254
Epoch 150, val loss: 1.2909932136535645
Epoch 160, training loss: 1.1213099956512451 = 1.0498178005218506 + 0.01 * 7.14921760559082
Epoch 160, val loss: 1.2189399003982544
Epoch 170, training loss: 1.026002049446106 = 0.9546124339103699 + 0.01 * 7.1389594078063965
Epoch 170, val loss: 1.1536178588867188
Epoch 180, training loss: 0.9412485361099243 = 0.8699714541435242 + 0.01 * 7.127707481384277
Epoch 180, val loss: 1.0984764099121094
Epoch 190, training loss: 0.8689586520195007 = 0.7977714538574219 + 0.01 * 7.118721961975098
Epoch 190, val loss: 1.0548728704452515
Epoch 200, training loss: 0.8087494373321533 = 0.7375920414924622 + 0.01 * 7.115736961364746
Epoch 200, val loss: 1.0224599838256836
Epoch 210, training loss: 0.758036732673645 = 0.6869556903839111 + 0.01 * 7.1081013679504395
Epoch 210, val loss: 0.9992587566375732
Epoch 220, training loss: 0.7135530114173889 = 0.6425178647041321 + 0.01 * 7.103514671325684
Epoch 220, val loss: 0.9819605946540833
Epoch 230, training loss: 0.6722257733345032 = 0.6012213230133057 + 0.01 * 7.100444316864014
Epoch 230, val loss: 0.9676417708396912
Epoch 240, training loss: 0.6316140294075012 = 0.5606763362884521 + 0.01 * 7.09376859664917
Epoch 240, val loss: 0.9542652368545532
Epoch 250, training loss: 0.5902320742607117 = 0.5193606615066528 + 0.01 * 7.087141990661621
Epoch 250, val loss: 0.9409147500991821
Epoch 260, training loss: 0.5476662516593933 = 0.4768628776073456 + 0.01 * 7.0803375244140625
Epoch 260, val loss: 0.9279735684394836
Epoch 270, training loss: 0.5046260356903076 = 0.4338955283164978 + 0.01 * 7.073052883148193
Epoch 270, val loss: 0.9170383214950562
Epoch 280, training loss: 0.4625489115715027 = 0.39185604453086853 + 0.01 * 7.0692853927612305
Epoch 280, val loss: 0.9099581241607666
Epoch 290, training loss: 0.42265206575393677 = 0.3520718812942505 + 0.01 * 7.058018207550049
Epoch 290, val loss: 0.9082666039466858
Epoch 300, training loss: 0.38597699999809265 = 0.31538206338882446 + 0.01 * 7.059493541717529
Epoch 300, val loss: 0.9127042293548584
Epoch 310, training loss: 0.35242441296577454 = 0.28198134899139404 + 0.01 * 7.044307231903076
Epoch 310, val loss: 0.922785758972168
Epoch 320, training loss: 0.3219480812549591 = 0.2516205310821533 + 0.01 * 7.0327558517456055
Epoch 320, val loss: 0.9375685453414917
Epoch 330, training loss: 0.29427385330200195 = 0.22401808202266693 + 0.01 * 7.025576591491699
Epoch 330, val loss: 0.9561249017715454
Epoch 340, training loss: 0.2692413926124573 = 0.19902729988098145 + 0.01 * 7.0214104652404785
Epoch 340, val loss: 0.9775702357292175
Epoch 350, training loss: 0.24668964743614197 = 0.1765371412038803 + 0.01 * 7.0152506828308105
Epoch 350, val loss: 1.001209020614624
Epoch 360, training loss: 0.22649434208869934 = 0.156437948346138 + 0.01 * 7.005640506744385
Epoch 360, val loss: 1.0264405012130737
Epoch 370, training loss: 0.20856556296348572 = 0.13862664997577667 + 0.01 * 6.993890762329102
Epoch 370, val loss: 1.0527390241622925
Epoch 380, training loss: 0.19283148646354675 = 0.12292461842298508 + 0.01 * 6.990687370300293
Epoch 380, val loss: 1.0797072649002075
Epoch 390, training loss: 0.1791345477104187 = 0.10911150276660919 + 0.01 * 7.0023040771484375
Epoch 390, val loss: 1.1069209575653076
Epoch 400, training loss: 0.16675060987472534 = 0.09699748456478119 + 0.01 * 6.975311756134033
Epoch 400, val loss: 1.1340450048446655
Epoch 410, training loss: 0.1563756763935089 = 0.08637014776468277 + 0.01 * 7.000552177429199
Epoch 410, val loss: 1.160792350769043
Epoch 420, training loss: 0.14674407243728638 = 0.07706744223833084 + 0.01 * 6.967662334442139
Epoch 420, val loss: 1.1871031522750854
Epoch 430, training loss: 0.13857942819595337 = 0.06891999393701553 + 0.01 * 6.965943336486816
Epoch 430, val loss: 1.2128276824951172
Epoch 440, training loss: 0.13130177557468414 = 0.061781249940395355 + 0.01 * 6.952052593231201
Epoch 440, val loss: 1.2379108667373657
Epoch 450, training loss: 0.12518994510173798 = 0.05552873760461807 + 0.01 * 6.966120719909668
Epoch 450, val loss: 1.2622376680374146
Epoch 460, training loss: 0.11957302689552307 = 0.05006235092878342 + 0.01 * 6.951067924499512
Epoch 460, val loss: 1.2857749462127686
Epoch 470, training loss: 0.11468593776226044 = 0.045271433889865875 + 0.01 * 6.941450595855713
Epoch 470, val loss: 1.308442234992981
Epoch 480, training loss: 0.11039558798074722 = 0.04106118530035019 + 0.01 * 6.933440685272217
Epoch 480, val loss: 1.330439805984497
Epoch 490, training loss: 0.10667873173952103 = 0.0373491570353508 + 0.01 * 6.932957649230957
Epoch 490, val loss: 1.3516899347305298
Epoch 500, training loss: 0.10335437953472137 = 0.0340660884976387 + 0.01 * 6.928829193115234
Epoch 500, val loss: 1.372337818145752
Epoch 510, training loss: 0.1003522127866745 = 0.03114989772439003 + 0.01 * 6.920231819152832
Epoch 510, val loss: 1.3924919366836548
Epoch 520, training loss: 0.09783696383237839 = 0.028557004407048225 + 0.01 * 6.9279961585998535
Epoch 520, val loss: 1.4120829105377197
Epoch 530, training loss: 0.09550968557596207 = 0.026250138878822327 + 0.01 * 6.925954818725586
Epoch 530, val loss: 1.4311336278915405
Epoch 540, training loss: 0.09324542433023453 = 0.02419036440551281 + 0.01 * 6.905506134033203
Epoch 540, val loss: 1.4496296644210815
Epoch 550, training loss: 0.0914396345615387 = 0.022346576675772667 + 0.01 * 6.909305572509766
Epoch 550, val loss: 1.4676883220672607
Epoch 560, training loss: 0.08965066820383072 = 0.02069380320608616 + 0.01 * 6.895686626434326
Epoch 560, val loss: 1.485248327255249
Epoch 570, training loss: 0.08823134750127792 = 0.019209271296858788 + 0.01 * 6.902207851409912
Epoch 570, val loss: 1.502226710319519
Epoch 580, training loss: 0.08686709403991699 = 0.017875391989946365 + 0.01 * 6.899170875549316
Epoch 580, val loss: 1.518677830696106
Epoch 590, training loss: 0.08553615212440491 = 0.01667347550392151 + 0.01 * 6.88626766204834
Epoch 590, val loss: 1.5345518589019775
Epoch 600, training loss: 0.08458547294139862 = 0.015587597154080868 + 0.01 * 6.899787425994873
Epoch 600, val loss: 1.5499122142791748
Epoch 610, training loss: 0.08341798186302185 = 0.014604102820158005 + 0.01 * 6.8813886642456055
Epoch 610, val loss: 1.5648860931396484
Epoch 620, training loss: 0.08251296728849411 = 0.013711734674870968 + 0.01 * 6.880123615264893
Epoch 620, val loss: 1.5792732238769531
Epoch 630, training loss: 0.08165670186281204 = 0.012899957597255707 + 0.01 * 6.875674724578857
Epoch 630, val loss: 1.5932499170303345
Epoch 640, training loss: 0.08093816041946411 = 0.012159635312855244 + 0.01 * 6.877852439880371
Epoch 640, val loss: 1.6067984104156494
Epoch 650, training loss: 0.08024106919765472 = 0.011483416892588139 + 0.01 * 6.875765800476074
Epoch 650, val loss: 1.6198304891586304
Epoch 660, training loss: 0.0794842466711998 = 0.010864662937819958 + 0.01 * 6.8619585037231445
Epoch 660, val loss: 1.6324334144592285
Epoch 670, training loss: 0.0789494439959526 = 0.010296602733433247 + 0.01 * 6.865283966064453
Epoch 670, val loss: 1.6445857286453247
Epoch 680, training loss: 0.07826262712478638 = 0.009773721918463707 + 0.01 * 6.848890781402588
Epoch 680, val loss: 1.6565253734588623
Epoch 690, training loss: 0.07773864269256592 = 0.00929116178303957 + 0.01 * 6.844748020172119
Epoch 690, val loss: 1.6679911613464355
Epoch 700, training loss: 0.07733843475580215 = 0.008846046403050423 + 0.01 * 6.849239349365234
Epoch 700, val loss: 1.6791036128997803
Epoch 710, training loss: 0.07689361274242401 = 0.008434241637587547 + 0.01 * 6.845937252044678
Epoch 710, val loss: 1.6898890733718872
Epoch 720, training loss: 0.07647889852523804 = 0.008052715100347996 + 0.01 * 6.842618942260742
Epoch 720, val loss: 1.7004046440124512
Epoch 730, training loss: 0.07603740692138672 = 0.0076986621133983135 + 0.01 * 6.833874702453613
Epoch 730, val loss: 1.7104969024658203
Epoch 740, training loss: 0.07565709948539734 = 0.007369446102529764 + 0.01 * 6.828765392303467
Epoch 740, val loss: 1.720292091369629
Epoch 750, training loss: 0.07560741156339645 = 0.007062806282192469 + 0.01 * 6.8544602394104
Epoch 750, val loss: 1.7298349142074585
Epoch 760, training loss: 0.07515417039394379 = 0.006777085363864899 + 0.01 * 6.837708950042725
Epoch 760, val loss: 1.739037275314331
Epoch 770, training loss: 0.07469248026609421 = 0.006509721744805574 + 0.01 * 6.818276405334473
Epoch 770, val loss: 1.7480432987213135
Epoch 780, training loss: 0.07451736927032471 = 0.006259389221668243 + 0.01 * 6.825798034667969
Epoch 780, val loss: 1.7567594051361084
Epoch 790, training loss: 0.07419542968273163 = 0.006024881731718779 + 0.01 * 6.8170552253723145
Epoch 790, val loss: 1.7651854753494263
Epoch 800, training loss: 0.07400327920913696 = 0.0058046504855155945 + 0.01 * 6.8198628425598145
Epoch 800, val loss: 1.7734755277633667
Epoch 810, training loss: 0.07377594709396362 = 0.005597472656518221 + 0.01 * 6.817847728729248
Epoch 810, val loss: 1.7814183235168457
Epoch 820, training loss: 0.0734652727842331 = 0.005402694456279278 + 0.01 * 6.806258201599121
Epoch 820, val loss: 1.7891714572906494
Epoch 830, training loss: 0.07348164916038513 = 0.005219088867306709 + 0.01 * 6.826256275177002
Epoch 830, val loss: 1.7965905666351318
Epoch 840, training loss: 0.07324566692113876 = 0.005045796278864145 + 0.01 * 6.819987773895264
Epoch 840, val loss: 1.8040928840637207
Epoch 850, training loss: 0.07283277064561844 = 0.004881988745182753 + 0.01 * 6.795078754425049
Epoch 850, val loss: 1.8111701011657715
Epoch 860, training loss: 0.07281209528446198 = 0.004727617371827364 + 0.01 * 6.80844783782959
Epoch 860, val loss: 1.818003535270691
Epoch 870, training loss: 0.07256046682596207 = 0.004581564571708441 + 0.01 * 6.7978901863098145
Epoch 870, val loss: 1.824916124343872
Epoch 880, training loss: 0.0723719671368599 = 0.004442339763045311 + 0.01 * 6.792963027954102
Epoch 880, val loss: 1.8314801454544067
Epoch 890, training loss: 0.07229918241500854 = 0.004310613963752985 + 0.01 * 6.798856735229492
Epoch 890, val loss: 1.83785080909729
Epoch 900, training loss: 0.07244252413511276 = 0.004186131991446018 + 0.01 * 6.825639247894287
Epoch 900, val loss: 1.8439981937408447
Epoch 910, training loss: 0.07200921326875687 = 0.004067901987582445 + 0.01 * 6.794131278991699
Epoch 910, val loss: 1.8500375747680664
Epoch 920, training loss: 0.07204551249742508 = 0.003955270629376173 + 0.01 * 6.809024333953857
Epoch 920, val loss: 1.8558976650238037
Epoch 930, training loss: 0.07167793065309525 = 0.003848749678581953 + 0.01 * 6.782918930053711
Epoch 930, val loss: 1.8615249395370483
Epoch 940, training loss: 0.07146434485912323 = 0.003747069276869297 + 0.01 * 6.771728038787842
Epoch 940, val loss: 1.8670419454574585
Epoch 950, training loss: 0.07135406136512756 = 0.0036496322136372328 + 0.01 * 6.770442962646484
Epoch 950, val loss: 1.8723889589309692
Epoch 960, training loss: 0.07134197652339935 = 0.0035575306974351406 + 0.01 * 6.778444290161133
Epoch 960, val loss: 1.8775503635406494
Epoch 970, training loss: 0.07114027440547943 = 0.003469446673989296 + 0.01 * 6.767083168029785
Epoch 970, val loss: 1.8826531171798706
Epoch 980, training loss: 0.07118277996778488 = 0.0033852828200906515 + 0.01 * 6.779749870300293
Epoch 980, val loss: 1.8875939846038818
Epoch 990, training loss: 0.07101033627986908 = 0.0033050612546503544 + 0.01 * 6.7705278396606445
Epoch 990, val loss: 1.8925261497497559
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.823405376910912
The final CL Acc:0.77778, 0.00524, The final GNN Acc:0.82007, 0.00334
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13172])
remove edge: torch.Size([2, 7898])
updated graph: torch.Size([2, 10514])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.036651134490967 = 1.9506831169128418 + 0.01 * 8.59679889678955
Epoch 0, val loss: 1.9480794668197632
Epoch 10, training loss: 2.025679588317871 = 1.9397119283676147 + 0.01 * 8.59675407409668
Epoch 10, val loss: 1.9377048015594482
Epoch 20, training loss: 2.0120766162872314 = 1.9261113405227661 + 0.01 * 8.59652328491211
Epoch 20, val loss: 1.924500584602356
Epoch 30, training loss: 1.9930742979049683 = 1.9071178436279297 + 0.01 * 8.595643043518066
Epoch 30, val loss: 1.90577232837677
Epoch 40, training loss: 1.9654600620269775 = 1.8795591592788696 + 0.01 * 8.590092658996582
Epoch 40, val loss: 1.8787293434143066
Epoch 50, training loss: 1.927498459815979 = 1.8419828414916992 + 0.01 * 8.551563262939453
Epoch 50, val loss: 1.8432258367538452
Epoch 60, training loss: 1.8841627836227417 = 1.8005110025405884 + 0.01 * 8.365174293518066
Epoch 60, val loss: 1.8073850870132446
Epoch 70, training loss: 1.8454761505126953 = 1.7635654211044312 + 0.01 * 8.191075325012207
Epoch 70, val loss: 1.7765936851501465
Epoch 80, training loss: 1.7978885173797607 = 1.7174687385559082 + 0.01 * 8.041973114013672
Epoch 80, val loss: 1.7335542440414429
Epoch 90, training loss: 1.7312443256378174 = 1.652618646621704 + 0.01 * 7.862562656402588
Epoch 90, val loss: 1.6734986305236816
Epoch 100, training loss: 1.6434067487716675 = 1.566759467124939 + 0.01 * 7.664724349975586
Epoch 100, val loss: 1.5984523296356201
Epoch 110, training loss: 1.544066071510315 = 1.4698631763458252 + 0.01 * 7.420290470123291
Epoch 110, val loss: 1.5175610780715942
Epoch 120, training loss: 1.4469393491744995 = 1.3736594915390015 + 0.01 * 7.327989101409912
Epoch 120, val loss: 1.4383848905563354
Epoch 130, training loss: 1.3521623611450195 = 1.2796530723571777 + 0.01 * 7.2509260177612305
Epoch 130, val loss: 1.363342523574829
Epoch 140, training loss: 1.2588694095611572 = 1.186740517616272 + 0.01 * 7.212886333465576
Epoch 140, val loss: 1.2914656400680542
Epoch 150, training loss: 1.1672465801239014 = 1.095179796218872 + 0.01 * 7.206679821014404
Epoch 150, val loss: 1.2217199802398682
Epoch 160, training loss: 1.077796220779419 = 1.0057661533355713 + 0.01 * 7.203002452850342
Epoch 160, val loss: 1.1551835536956787
Epoch 170, training loss: 0.9909696578979492 = 0.918953537940979 + 0.01 * 7.201611042022705
Epoch 170, val loss: 1.0911316871643066
Epoch 180, training loss: 0.9071998596191406 = 0.8352090716362 + 0.01 * 7.199075698852539
Epoch 180, val loss: 1.0289607048034668
Epoch 190, training loss: 0.8279463648796082 = 0.7559944987297058 + 0.01 * 7.195187091827393
Epoch 190, val loss: 0.9704024791717529
Epoch 200, training loss: 0.7550081014633179 = 0.6831198930740356 + 0.01 * 7.188821315765381
Epoch 200, val loss: 0.9179410934448242
Epoch 210, training loss: 0.6898753046989441 = 0.6180880069732666 + 0.01 * 7.1787309646606445
Epoch 210, val loss: 0.8738236427307129
Epoch 220, training loss: 0.6329822540283203 = 0.5613394975662231 + 0.01 * 7.164277076721191
Epoch 220, val loss: 0.8393678069114685
Epoch 230, training loss: 0.5836744904518127 = 0.5121970176696777 + 0.01 * 7.147746562957764
Epoch 230, val loss: 0.8136200904846191
Epoch 240, training loss: 0.5404707193374634 = 0.4691590964794159 + 0.01 * 7.13115930557251
Epoch 240, val loss: 0.7949007749557495
Epoch 250, training loss: 0.5015525817871094 = 0.43039819598197937 + 0.01 * 7.115437030792236
Epoch 250, val loss: 0.7813544869422913
Epoch 260, training loss: 0.4653054475784302 = 0.3942231833934784 + 0.01 * 7.1082258224487305
Epoch 260, val loss: 0.7712381482124329
Epoch 270, training loss: 0.4303520917892456 = 0.35937434434890747 + 0.01 * 7.097775936126709
Epoch 270, val loss: 0.763145923614502
Epoch 280, training loss: 0.39609968662261963 = 0.32516470551490784 + 0.01 * 7.093497276306152
Epoch 280, val loss: 0.7563139200210571
Epoch 290, training loss: 0.3624107837677002 = 0.2915017902851105 + 0.01 * 7.090900421142578
Epoch 290, val loss: 0.7503013610839844
Epoch 300, training loss: 0.3296682834625244 = 0.25877776741981506 + 0.01 * 7.089051723480225
Epoch 300, val loss: 0.7451366782188416
Epoch 310, training loss: 0.2985422909259796 = 0.22766880691051483 + 0.01 * 7.0873494148254395
Epoch 310, val loss: 0.741028368473053
Epoch 320, training loss: 0.2696835994720459 = 0.1988295614719391 + 0.01 * 7.085404872894287
Epoch 320, val loss: 0.7383970618247986
Epoch 330, training loss: 0.2435590922832489 = 0.17273442447185516 + 0.01 * 7.082468032836914
Epoch 330, val loss: 0.7374884486198425
Epoch 340, training loss: 0.2204529047012329 = 0.14958937466144562 + 0.01 * 7.0863542556762695
Epoch 340, val loss: 0.7383126616477966
Epoch 350, training loss: 0.20016008615493774 = 0.12934623658657074 + 0.01 * 7.081385612487793
Epoch 350, val loss: 0.7409717440605164
Epoch 360, training loss: 0.18252688646316528 = 0.11179132014513016 + 0.01 * 7.073556423187256
Epoch 360, val loss: 0.7454248070716858
Epoch 370, training loss: 0.1673576682806015 = 0.09666095674037933 + 0.01 * 7.069671154022217
Epoch 370, val loss: 0.7515245079994202
Epoch 380, training loss: 0.15434032678604126 = 0.08368762582540512 + 0.01 * 7.065269947052002
Epoch 380, val loss: 0.7590395212173462
Epoch 390, training loss: 0.1432584822177887 = 0.07261644303798676 + 0.01 * 7.064203262329102
Epoch 390, val loss: 0.7677258253097534
Epoch 400, training loss: 0.13378086686134338 = 0.06321408599615097 + 0.01 * 7.0566792488098145
Epoch 400, val loss: 0.777259111404419
Epoch 410, training loss: 0.12575505673885345 = 0.05525083467364311 + 0.01 * 7.050421714782715
Epoch 410, val loss: 0.7874006628990173
Epoch 420, training loss: 0.11902187764644623 = 0.04851805046200752 + 0.01 * 7.0503830909729
Epoch 420, val loss: 0.7980711460113525
Epoch 430, training loss: 0.11322405934333801 = 0.04283202067017555 + 0.01 * 7.039203643798828
Epoch 430, val loss: 0.8089970946311951
Epoch 440, training loss: 0.10834677517414093 = 0.03801460564136505 + 0.01 * 7.033217430114746
Epoch 440, val loss: 0.8199982047080994
Epoch 450, training loss: 0.10416684299707413 = 0.03391924500465393 + 0.01 * 7.0247602462768555
Epoch 450, val loss: 0.8309412598609924
Epoch 460, training loss: 0.10063166171312332 = 0.030427901074290276 + 0.01 * 7.020376205444336
Epoch 460, val loss: 0.8417989611625671
Epoch 470, training loss: 0.0975995659828186 = 0.027438219636678696 + 0.01 * 7.016134262084961
Epoch 470, val loss: 0.8524118661880493
Epoch 480, training loss: 0.09488947689533234 = 0.024861861020326614 + 0.01 * 7.002761363983154
Epoch 480, val loss: 0.8627896308898926
Epoch 490, training loss: 0.09257346391677856 = 0.022628681734204292 + 0.01 * 6.994478225708008
Epoch 490, val loss: 0.8728848695755005
Epoch 500, training loss: 0.09090691804885864 = 0.020683979615569115 + 0.01 * 7.022293567657471
Epoch 500, val loss: 0.882710874080658
Epoch 510, training loss: 0.08886940777301788 = 0.018988756462931633 + 0.01 * 6.988065242767334
Epoch 510, val loss: 0.8920791149139404
Epoch 520, training loss: 0.08726470172405243 = 0.017498839646577835 + 0.01 * 6.976586818695068
Epoch 520, val loss: 0.9011325836181641
Epoch 530, training loss: 0.08585081994533539 = 0.01618145778775215 + 0.01 * 6.9669365882873535
Epoch 530, val loss: 0.9099555015563965
Epoch 540, training loss: 0.08464482426643372 = 0.015012841671705246 + 0.01 * 6.963197708129883
Epoch 540, val loss: 0.9184221625328064
Epoch 550, training loss: 0.08353415131568909 = 0.013971076346933842 + 0.01 * 6.956307888031006
Epoch 550, val loss: 0.9266339540481567
Epoch 560, training loss: 0.08265244960784912 = 0.01303731370717287 + 0.01 * 6.961513519287109
Epoch 560, val loss: 0.9345577955245972
Epoch 570, training loss: 0.08166709542274475 = 0.012197216972708702 + 0.01 * 6.946987628936768
Epoch 570, val loss: 0.9422714710235596
Epoch 580, training loss: 0.08079859614372253 = 0.011438892222940922 + 0.01 * 6.935970783233643
Epoch 580, val loss: 0.9497159719467163
Epoch 590, training loss: 0.0800309106707573 = 0.010751116089522839 + 0.01 * 6.927979946136475
Epoch 590, val loss: 0.9569215178489685
Epoch 600, training loss: 0.07936418801546097 = 0.01012676302343607 + 0.01 * 6.923742771148682
Epoch 600, val loss: 0.9639186263084412
Epoch 610, training loss: 0.078826904296875 = 0.009557748213410378 + 0.01 * 6.926916122436523
Epoch 610, val loss: 0.970720648765564
Epoch 620, training loss: 0.07817365974187851 = 0.00903860479593277 + 0.01 * 6.913505554199219
Epoch 620, val loss: 0.9771946668624878
Epoch 630, training loss: 0.07750972360372543 = 0.008562512695789337 + 0.01 * 6.894721508026123
Epoch 630, val loss: 0.9835671782493591
Epoch 640, training loss: 0.07719945162534714 = 0.008125310763716698 + 0.01 * 6.907413959503174
Epoch 640, val loss: 0.9897352457046509
Epoch 650, training loss: 0.07669280469417572 = 0.007723579183220863 + 0.01 * 6.896922588348389
Epoch 650, val loss: 0.9956682920455933
Epoch 660, training loss: 0.07616204023361206 = 0.007353418506681919 + 0.01 * 6.880862236022949
Epoch 660, val loss: 1.0014408826828003
Epoch 670, training loss: 0.07572591304779053 = 0.0070113008841872215 + 0.01 * 6.871460914611816
Epoch 670, val loss: 1.0070985555648804
Epoch 680, training loss: 0.07544952630996704 = 0.00669447798281908 + 0.01 * 6.875504493713379
Epoch 680, val loss: 1.0124962329864502
Epoch 690, training loss: 0.07513903081417084 = 0.006400657817721367 + 0.01 * 6.873836994171143
Epoch 690, val loss: 1.0178062915802002
Epoch 700, training loss: 0.07479140162467957 = 0.006127630826085806 + 0.01 * 6.866376876831055
Epoch 700, val loss: 1.022894024848938
Epoch 710, training loss: 0.07460352778434753 = 0.0058733620680868626 + 0.01 * 6.873017311096191
Epoch 710, val loss: 1.0279312133789062
Epoch 720, training loss: 0.07417965680360794 = 0.005636671092361212 + 0.01 * 6.8542985916137695
Epoch 720, val loss: 1.032640814781189
Epoch 730, training loss: 0.07387036085128784 = 0.005415433086454868 + 0.01 * 6.845493316650391
Epoch 730, val loss: 1.0373817682266235
Epoch 740, training loss: 0.07360942661762238 = 0.0052085695788264275 + 0.01 * 6.840085983276367
Epoch 740, val loss: 1.041909098625183
Epoch 750, training loss: 0.07367559522390366 = 0.005014396272599697 + 0.01 * 6.866119861602783
Epoch 750, val loss: 1.0464314222335815
Epoch 760, training loss: 0.07316962629556656 = 0.004832854028791189 + 0.01 * 6.833676815032959
Epoch 760, val loss: 1.0506889820098877
Epoch 770, training loss: 0.07290740311145782 = 0.0046622673980891705 + 0.01 * 6.8245134353637695
Epoch 770, val loss: 1.0548492670059204
Epoch 780, training loss: 0.07296080887317657 = 0.004501515533775091 + 0.01 * 6.8459296226501465
Epoch 780, val loss: 1.0589793920516968
Epoch 790, training loss: 0.07262876629829407 = 0.004350535571575165 + 0.01 * 6.827823162078857
Epoch 790, val loss: 1.062874674797058
Epoch 800, training loss: 0.07238654792308807 = 0.004208137281239033 + 0.01 * 6.817840576171875
Epoch 800, val loss: 1.0667452812194824
Epoch 810, training loss: 0.07225487381219864 = 0.0040735467337071896 + 0.01 * 6.8181328773498535
Epoch 810, val loss: 1.070487380027771
Epoch 820, training loss: 0.07210125029087067 = 0.003946179058402777 + 0.01 * 6.815507411956787
Epoch 820, val loss: 1.0742290019989014
Epoch 830, training loss: 0.07197693735361099 = 0.003825935535132885 + 0.01 * 6.81510066986084
Epoch 830, val loss: 1.077737808227539
Epoch 840, training loss: 0.07183778285980225 = 0.0037119591142982244 + 0.01 * 6.812582015991211
Epoch 840, val loss: 1.0811753273010254
Epoch 850, training loss: 0.0716557502746582 = 0.003603905439376831 + 0.01 * 6.805184841156006
Epoch 850, val loss: 1.0844817161560059
Epoch 860, training loss: 0.07145901769399643 = 0.0035012734588235617 + 0.01 * 6.795774459838867
Epoch 860, val loss: 1.087843418121338
Epoch 870, training loss: 0.07133941352367401 = 0.0034037833102047443 + 0.01 * 6.793562889099121
Epoch 870, val loss: 1.0909886360168457
Epoch 880, training loss: 0.07127366214990616 = 0.003311087377369404 + 0.01 * 6.796257495880127
Epoch 880, val loss: 1.0941418409347534
Epoch 890, training loss: 0.07107007503509521 = 0.0032230298966169357 + 0.01 * 6.784704685211182
Epoch 890, val loss: 1.0970914363861084
Epoch 900, training loss: 0.07111188769340515 = 0.0031391121447086334 + 0.01 * 6.797277927398682
Epoch 900, val loss: 1.1001313924789429
Epoch 910, training loss: 0.07096334546804428 = 0.0030592670664191246 + 0.01 * 6.790408134460449
Epoch 910, val loss: 1.1028578281402588
Epoch 920, training loss: 0.0708124116063118 = 0.0029830753337591887 + 0.01 * 6.782933235168457
Epoch 920, val loss: 1.1056925058364868
Epoch 930, training loss: 0.07069016247987747 = 0.002910217270255089 + 0.01 * 6.777994155883789
Epoch 930, val loss: 1.1084188222885132
Epoch 940, training loss: 0.07083296030759811 = 0.0028405110351741314 + 0.01 * 6.7992448806762695
Epoch 940, val loss: 1.1111353635787964
Epoch 950, training loss: 0.07053679972887039 = 0.0027740809600800276 + 0.01 * 6.776271820068359
Epoch 950, val loss: 1.1135940551757812
Epoch 960, training loss: 0.07063917815685272 = 0.0027104897890239954 + 0.01 * 6.7928690910339355
Epoch 960, val loss: 1.1161847114562988
Epoch 970, training loss: 0.07036656141281128 = 0.0026495938654989004 + 0.01 * 6.771697521209717
Epoch 970, val loss: 1.1186140775680542
Epoch 980, training loss: 0.07030484080314636 = 0.002591250231489539 + 0.01 * 6.771358966827393
Epoch 980, val loss: 1.120951533317566
Epoch 990, training loss: 0.07016614824533463 = 0.0025353534147143364 + 0.01 * 6.76308012008667
Epoch 990, val loss: 1.1232829093933105
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.023406982421875 = 1.9374390840530396 + 0.01 * 8.5968017578125
Epoch 0, val loss: 1.925126314163208
Epoch 10, training loss: 2.013859987258911 = 1.9278925657272339 + 0.01 * 8.596739768981934
Epoch 10, val loss: 1.9161732196807861
Epoch 20, training loss: 2.002303123474121 = 1.9163380861282349 + 0.01 * 8.596495628356934
Epoch 20, val loss: 1.9049327373504639
Epoch 30, training loss: 1.9861050844192505 = 1.900147795677185 + 0.01 * 8.595734596252441
Epoch 30, val loss: 1.8889800310134888
Epoch 40, training loss: 1.9621490240097046 = 1.8762316703796387 + 0.01 * 8.5917387008667
Epoch 40, val loss: 1.8657015562057495
Epoch 50, training loss: 1.9284368753433228 = 1.8427934646606445 + 0.01 * 8.564337730407715
Epoch 50, val loss: 1.8346197605133057
Epoch 60, training loss: 1.8883328437805176 = 1.8041019439697266 + 0.01 * 8.423093795776367
Epoch 60, val loss: 1.8021957874298096
Epoch 70, training loss: 1.8499774932861328 = 1.7686657905578613 + 0.01 * 8.131171226501465
Epoch 70, val loss: 1.7751595973968506
Epoch 80, training loss: 1.8052762746810913 = 1.726726770401001 + 0.01 * 7.854949951171875
Epoch 80, val loss: 1.7388513088226318
Epoch 90, training loss: 1.743586778640747 = 1.6682723760604858 + 0.01 * 7.531445503234863
Epoch 90, val loss: 1.68588387966156
Epoch 100, training loss: 1.6651312112808228 = 1.5915567874908447 + 0.01 * 7.3574442863464355
Epoch 100, val loss: 1.6182751655578613
Epoch 110, training loss: 1.5741194486618042 = 1.5013096332550049 + 0.01 * 7.280986785888672
Epoch 110, val loss: 1.541951298713684
Epoch 120, training loss: 1.479164958000183 = 1.4068236351013184 + 0.01 * 7.234130382537842
Epoch 120, val loss: 1.4642739295959473
Epoch 130, training loss: 1.3834277391433716 = 1.3113898038864136 + 0.01 * 7.203796863555908
Epoch 130, val loss: 1.387873649597168
Epoch 140, training loss: 1.2868050336837769 = 1.2149714231491089 + 0.01 * 7.1833600997924805
Epoch 140, val loss: 1.3121767044067383
Epoch 150, training loss: 1.1918071508407593 = 1.1201295852661133 + 0.01 * 7.167755126953125
Epoch 150, val loss: 1.2382359504699707
Epoch 160, training loss: 1.102340579032898 = 1.030853509902954 + 0.01 * 7.148709774017334
Epoch 160, val loss: 1.1692559719085693
Epoch 170, training loss: 1.0201349258422852 = 0.9488912224769592 + 0.01 * 7.124373912811279
Epoch 170, val loss: 1.1062244176864624
Epoch 180, training loss: 0.9430580139160156 = 0.8720912337303162 + 0.01 * 7.096678256988525
Epoch 180, val loss: 1.0468153953552246
Epoch 190, training loss: 0.8679171204566956 = 0.797162652015686 + 0.01 * 7.07544469833374
Epoch 190, val loss: 0.9877537488937378
Epoch 200, training loss: 0.7933984994888306 = 0.7227936387062073 + 0.01 * 7.060484886169434
Epoch 200, val loss: 0.9291614294052124
Epoch 210, training loss: 0.7209999561309814 = 0.6504891514778137 + 0.01 * 7.051082611083984
Epoch 210, val loss: 0.8734042644500732
Epoch 220, training loss: 0.653438925743103 = 0.5829886794090271 + 0.01 * 7.045025825500488
Epoch 220, val loss: 0.8236209750175476
Epoch 230, training loss: 0.5923978090286255 = 0.5219964385032654 + 0.01 * 7.040136814117432
Epoch 230, val loss: 0.7820890545845032
Epoch 240, training loss: 0.5377854704856873 = 0.4674246609210968 + 0.01 * 7.036079406738281
Epoch 240, val loss: 0.7492778301239014
Epoch 250, training loss: 0.4886021018028259 = 0.41824716329574585 + 0.01 * 7.035493850708008
Epoch 250, val loss: 0.7242624759674072
Epoch 260, training loss: 0.44374722242355347 = 0.3734533488750458 + 0.01 * 7.029388904571533
Epoch 260, val loss: 0.7054589986801147
Epoch 270, training loss: 0.4026035964488983 = 0.332346111536026 + 0.01 * 7.025748252868652
Epoch 270, val loss: 0.6915310621261597
Epoch 280, training loss: 0.3647209703922272 = 0.2945057451725006 + 0.01 * 7.02152156829834
Epoch 280, val loss: 0.6817473769187927
Epoch 290, training loss: 0.329961895942688 = 0.25973421335220337 + 0.01 * 7.022766590118408
Epoch 290, val loss: 0.6754242777824402
Epoch 300, training loss: 0.2981288433074951 = 0.2279910296201706 + 0.01 * 7.013781547546387
Epoch 300, val loss: 0.6721943020820618
Epoch 310, training loss: 0.26935768127441406 = 0.19927598536014557 + 0.01 * 7.0081706047058105
Epoch 310, val loss: 0.6716792583465576
Epoch 320, training loss: 0.24359005689620972 = 0.173569455742836 + 0.01 * 7.002060413360596
Epoch 320, val loss: 0.6735608577728271
Epoch 330, training loss: 0.2209412157535553 = 0.15080712735652924 + 0.01 * 7.01340913772583
Epoch 330, val loss: 0.6775702834129333
Epoch 340, training loss: 0.20086149871349335 = 0.13089656829833984 + 0.01 * 6.996493339538574
Epoch 340, val loss: 0.6833961606025696
Epoch 350, training loss: 0.18350443243980408 = 0.11363431811332703 + 0.01 * 6.987011909484863
Epoch 350, val loss: 0.6907358765602112
Epoch 360, training loss: 0.16857731342315674 = 0.09877382218837738 + 0.01 * 6.980349540710449
Epoch 360, val loss: 0.6993555426597595
Epoch 370, training loss: 0.15579968690872192 = 0.08604519069194794 + 0.01 * 6.975449562072754
Epoch 370, val loss: 0.7089800238609314
Epoch 380, training loss: 0.14493544399738312 = 0.07518418878316879 + 0.01 * 6.975125789642334
Epoch 380, val loss: 0.7194058895111084
Epoch 390, training loss: 0.1356208622455597 = 0.06593319028615952 + 0.01 * 6.968768119812012
Epoch 390, val loss: 0.7303617000579834
Epoch 400, training loss: 0.12771612405776978 = 0.05804934352636337 + 0.01 * 6.966677665710449
Epoch 400, val loss: 0.7416375279426575
Epoch 410, training loss: 0.12095233052968979 = 0.051325730979442596 + 0.01 * 6.96265983581543
Epoch 410, val loss: 0.753097653388977
Epoch 420, training loss: 0.11510539054870605 = 0.045577529817819595 + 0.01 * 6.952785968780518
Epoch 420, val loss: 0.7646008133888245
Epoch 430, training loss: 0.11017404496669769 = 0.040650177747011185 + 0.01 * 6.952386856079102
Epoch 430, val loss: 0.7760794758796692
Epoch 440, training loss: 0.10587824881076813 = 0.0364166758954525 + 0.01 * 6.946156978607178
Epoch 440, val loss: 0.7874138951301575
Epoch 450, training loss: 0.10239993035793304 = 0.03276925906538963 + 0.01 * 6.963066577911377
Epoch 450, val loss: 0.798610508441925
Epoch 460, training loss: 0.09904041141271591 = 0.029621712863445282 + 0.01 * 6.941869735717773
Epoch 460, val loss: 0.809511661529541
Epoch 470, training loss: 0.09620313346385956 = 0.02689002826809883 + 0.01 * 6.931310176849365
Epoch 470, val loss: 0.8201592564582825
Epoch 480, training loss: 0.09376709163188934 = 0.024508457630872726 + 0.01 * 6.925863265991211
Epoch 480, val loss: 0.8305202126502991
Epoch 490, training loss: 0.09180381894111633 = 0.02242329530417919 + 0.01 * 6.938052177429199
Epoch 490, val loss: 0.8406161069869995
Epoch 500, training loss: 0.0897994115948677 = 0.02059320919215679 + 0.01 * 6.920619964599609
Epoch 500, val loss: 0.8504027724266052
Epoch 510, training loss: 0.08808769285678864 = 0.018978040665388107 + 0.01 * 6.910965442657471
Epoch 510, val loss: 0.8599139451980591
Epoch 520, training loss: 0.08678677678108215 = 0.017547210678458214 + 0.01 * 6.923956871032715
Epoch 520, val loss: 0.8691082000732422
Epoch 530, training loss: 0.08526233583688736 = 0.016275258734822273 + 0.01 * 6.898708343505859
Epoch 530, val loss: 0.8779785633087158
Epoch 540, training loss: 0.08416789025068283 = 0.0151384761556983 + 0.01 * 6.902941703796387
Epoch 540, val loss: 0.8865908980369568
Epoch 550, training loss: 0.08304571360349655 = 0.01411963440477848 + 0.01 * 6.892608165740967
Epoch 550, val loss: 0.8948708176612854
Epoch 560, training loss: 0.0822676345705986 = 0.01320361252874136 + 0.01 * 6.906402587890625
Epoch 560, val loss: 0.9029192924499512
Epoch 570, training loss: 0.08114811033010483 = 0.012376904487609863 + 0.01 * 6.877120494842529
Epoch 570, val loss: 0.9107363224029541
Epoch 580, training loss: 0.08036284893751144 = 0.011628429405391216 + 0.01 * 6.87344217300415
Epoch 580, val loss: 0.9182680249214172
Epoch 590, training loss: 0.07956043630838394 = 0.010948212817311287 + 0.01 * 6.861222743988037
Epoch 590, val loss: 0.9255580306053162
Epoch 600, training loss: 0.07904678583145142 = 0.010328060016036034 + 0.01 * 6.871872425079346
Epoch 600, val loss: 0.9326788187026978
Epoch 610, training loss: 0.0783349946141243 = 0.009762086905539036 + 0.01 * 6.857291221618652
Epoch 610, val loss: 0.9395231008529663
Epoch 620, training loss: 0.07791238278150558 = 0.009244068525731564 + 0.01 * 6.8668317794799805
Epoch 620, val loss: 0.946160078048706
Epoch 630, training loss: 0.07719500362873077 = 0.00876804068684578 + 0.01 * 6.842696666717529
Epoch 630, val loss: 0.9526317715644836
Epoch 640, training loss: 0.0767463743686676 = 0.008330516517162323 + 0.01 * 6.841585636138916
Epoch 640, val loss: 0.9589069485664368
Epoch 650, training loss: 0.07621802389621735 = 0.007927630096673965 + 0.01 * 6.829039573669434
Epoch 650, val loss: 0.9650130867958069
Epoch 660, training loss: 0.07588539272546768 = 0.007555142976343632 + 0.01 * 6.833024978637695
Epoch 660, val loss: 0.970987856388092
Epoch 670, training loss: 0.07542804628610611 = 0.00721051637083292 + 0.01 * 6.82175350189209
Epoch 670, val loss: 0.9767211675643921
Epoch 680, training loss: 0.07513804733753204 = 0.006890655495226383 + 0.01 * 6.824739456176758
Epoch 680, val loss: 0.982284665107727
Epoch 690, training loss: 0.07479248940944672 = 0.006594691425561905 + 0.01 * 6.819779396057129
Epoch 690, val loss: 0.9877658486366272
Epoch 700, training loss: 0.0743492990732193 = 0.00631908280774951 + 0.01 * 6.803021430969238
Epoch 700, val loss: 0.9930545091629028
Epoch 710, training loss: 0.07408767938613892 = 0.0060619572177529335 + 0.01 * 6.802572727203369
Epoch 710, val loss: 0.998191773891449
Epoch 720, training loss: 0.0737643763422966 = 0.005822304170578718 + 0.01 * 6.794207572937012
Epoch 720, val loss: 1.003237247467041
Epoch 730, training loss: 0.07363656908273697 = 0.005598262418061495 + 0.01 * 6.803831100463867
Epoch 730, val loss: 1.008104681968689
Epoch 740, training loss: 0.07326012849807739 = 0.005388882011175156 + 0.01 * 6.787125110626221
Epoch 740, val loss: 1.0128265619277954
Epoch 750, training loss: 0.07308666408061981 = 0.005192183423787355 + 0.01 * 6.789448261260986
Epoch 750, val loss: 1.0174713134765625
Epoch 760, training loss: 0.07283378392457962 = 0.0050079128704965115 + 0.01 * 6.782587051391602
Epoch 760, val loss: 1.0219563245773315
Epoch 770, training loss: 0.07261746376752853 = 0.004834653344005346 + 0.01 * 6.778281211853027
Epoch 770, val loss: 1.0263043642044067
Epoch 780, training loss: 0.07244642823934555 = 0.0046720015816390514 + 0.01 * 6.777442932128906
Epoch 780, val loss: 1.0306017398834229
Epoch 790, training loss: 0.07211661338806152 = 0.0045186481438577175 + 0.01 * 6.759796619415283
Epoch 790, val loss: 1.0347853899002075
Epoch 800, training loss: 0.07226675003767014 = 0.004373999312520027 + 0.01 * 6.789275646209717
Epoch 800, val loss: 1.0388456583023071
Epoch 810, training loss: 0.07185027748346329 = 0.0042372168973088264 + 0.01 * 6.761306285858154
Epoch 810, val loss: 1.042817234992981
Epoch 820, training loss: 0.07196655869483948 = 0.00410770159214735 + 0.01 * 6.785885810852051
Epoch 820, val loss: 1.0467190742492676
Epoch 830, training loss: 0.07159647345542908 = 0.003986193798482418 + 0.01 * 6.761027812957764
Epoch 830, val loss: 1.0504647493362427
Epoch 840, training loss: 0.07133623957633972 = 0.003870224580168724 + 0.01 * 6.746601581573486
Epoch 840, val loss: 1.0542184114456177
Epoch 850, training loss: 0.07183206081390381 = 0.0037607529666274786 + 0.01 * 6.807130813598633
Epoch 850, val loss: 1.0578382015228271
Epoch 860, training loss: 0.07120931148529053 = 0.0036571514792740345 + 0.01 * 6.755216121673584
Epoch 860, val loss: 1.0613150596618652
Epoch 870, training loss: 0.07101719826459885 = 0.0035582291893661022 + 0.01 * 6.74589729309082
Epoch 870, val loss: 1.0647778511047363
Epoch 880, training loss: 0.07081545889377594 = 0.003464498557150364 + 0.01 * 6.735095977783203
Epoch 880, val loss: 1.0681060552597046
Epoch 890, training loss: 0.07066254317760468 = 0.0033751532901078463 + 0.01 * 6.7287397384643555
Epoch 890, val loss: 1.0714151859283447
Epoch 900, training loss: 0.07064349204301834 = 0.0032895931508392096 + 0.01 * 6.7353901863098145
Epoch 900, val loss: 1.074630618095398
Epoch 910, training loss: 0.07062079757452011 = 0.0032085913699120283 + 0.01 * 6.741220951080322
Epoch 910, val loss: 1.077866554260254
Epoch 920, training loss: 0.07026708871126175 = 0.0031311134807765484 + 0.01 * 6.713597297668457
Epoch 920, val loss: 1.080915927886963
Epoch 930, training loss: 0.07022393494844437 = 0.0030573895201087 + 0.01 * 6.7166547775268555
Epoch 930, val loss: 1.083959698677063
Epoch 940, training loss: 0.0700925886631012 = 0.002986649051308632 + 0.01 * 6.710594654083252
Epoch 940, val loss: 1.0869027376174927
Epoch 950, training loss: 0.07001131027936935 = 0.002918963087722659 + 0.01 * 6.709234714508057
Epoch 950, val loss: 1.0898929834365845
Epoch 960, training loss: 0.06986071169376373 = 0.002854529768228531 + 0.01 * 6.700617790222168
Epoch 960, val loss: 1.0927178859710693
Epoch 970, training loss: 0.06987591832876205 = 0.0027928659692406654 + 0.01 * 6.708305358886719
Epoch 970, val loss: 1.095523476600647
Epoch 980, training loss: 0.0700242891907692 = 0.002733598928898573 + 0.01 * 6.729069232940674
Epoch 980, val loss: 1.0983080863952637
Epoch 990, training loss: 0.06958483159542084 = 0.002676586853340268 + 0.01 * 6.69082498550415
Epoch 990, val loss: 1.1010249853134155
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 2.025984764099121 = 1.940016746520996 + 0.01 * 8.5968017578125
Epoch 0, val loss: 1.9386253356933594
Epoch 10, training loss: 2.016458511352539 = 1.9304914474487305 + 0.01 * 8.596713066101074
Epoch 10, val loss: 1.9286155700683594
Epoch 20, training loss: 2.004694938659668 = 1.9187307357788086 + 0.01 * 8.596420288085938
Epoch 20, val loss: 1.9161553382873535
Epoch 30, training loss: 1.9881621599197388 = 1.9022074937820435 + 0.01 * 8.59547233581543
Epoch 30, val loss: 1.8988274335861206
Epoch 40, training loss: 1.9638242721557617 = 1.8779247999191284 + 0.01 * 8.589948654174805
Epoch 40, val loss: 1.8737188577651978
Epoch 50, training loss: 1.929090976715088 = 1.8435900211334229 + 0.01 * 8.550095558166504
Epoch 50, val loss: 1.8395448923110962
Epoch 60, training loss: 1.886013388633728 = 1.8026129007339478 + 0.01 * 8.340046882629395
Epoch 60, val loss: 1.8021608591079712
Epoch 70, training loss: 1.8418068885803223 = 1.7616316080093384 + 0.01 * 8.017533302307129
Epoch 70, val loss: 1.7673810720443726
Epoch 80, training loss: 1.7894452810287476 = 1.7118762731552124 + 0.01 * 7.75689697265625
Epoch 80, val loss: 1.7232410907745361
Epoch 90, training loss: 1.7188549041748047 = 1.6437602043151855 + 0.01 * 7.509466648101807
Epoch 90, val loss: 1.6623284816741943
Epoch 100, training loss: 1.629226565361023 = 1.5553903579711914 + 0.01 * 7.3836188316345215
Epoch 100, val loss: 1.5849189758300781
Epoch 110, training loss: 1.5251781940460205 = 1.451818823814392 + 0.01 * 7.335936546325684
Epoch 110, val loss: 1.496251106262207
Epoch 120, training loss: 1.4160245656967163 = 1.3431044816970825 + 0.01 * 7.292007923126221
Epoch 120, val loss: 1.4049856662750244
Epoch 130, training loss: 1.307649850845337 = 1.2350258827209473 + 0.01 * 7.262396335601807
Epoch 130, val loss: 1.3158295154571533
Epoch 140, training loss: 1.2033262252807617 = 1.1309314966201782 + 0.01 * 7.239470958709717
Epoch 140, val loss: 1.2315340042114258
Epoch 150, training loss: 1.1059304475784302 = 1.033711552619934 + 0.01 * 7.221892833709717
Epoch 150, val loss: 1.1542794704437256
Epoch 160, training loss: 1.0165632963180542 = 0.9444321990013123 + 0.01 * 7.213107585906982
Epoch 160, val loss: 1.084235429763794
Epoch 170, training loss: 0.9352324604988098 = 0.8631572723388672 + 0.01 * 7.207518577575684
Epoch 170, val loss: 1.0210566520690918
Epoch 180, training loss: 0.8617773056030273 = 0.78977370262146 + 0.01 * 7.200361251831055
Epoch 180, val loss: 0.9648172855377197
Epoch 190, training loss: 0.7959944009780884 = 0.7240942716598511 + 0.01 * 7.190011024475098
Epoch 190, val loss: 0.9156538844108582
Epoch 200, training loss: 0.736907958984375 = 0.665153443813324 + 0.01 * 7.175449371337891
Epoch 200, val loss: 0.8736017346382141
Epoch 210, training loss: 0.6829994916915894 = 0.6114441752433777 + 0.01 * 7.155529975891113
Epoch 210, val loss: 0.8381581902503967
Epoch 220, training loss: 0.6329129934310913 = 0.5616052746772766 + 0.01 * 7.130774021148682
Epoch 220, val loss: 0.8082418441772461
Epoch 230, training loss: 0.5858685970306396 = 0.5148400068283081 + 0.01 * 7.10285758972168
Epoch 230, val loss: 0.7833101153373718
Epoch 240, training loss: 0.541581392288208 = 0.47077518701553345 + 0.01 * 7.080618858337402
Epoch 240, val loss: 0.7635869383811951
Epoch 250, training loss: 0.49982941150665283 = 0.42928972840309143 + 0.01 * 7.053968906402588
Epoch 250, val loss: 0.7494797706604004
Epoch 260, training loss: 0.4608977138996124 = 0.39051076769828796 + 0.01 * 7.038694381713867
Epoch 260, val loss: 0.7409041523933411
Epoch 270, training loss: 0.42482367157936096 = 0.35459578037261963 + 0.01 * 7.022789001464844
Epoch 270, val loss: 0.7372956275939941
Epoch 280, training loss: 0.39166319370269775 = 0.3215378522872925 + 0.01 * 7.012535095214844
Epoch 280, val loss: 0.7375375628471375
Epoch 290, training loss: 0.36097395420074463 = 0.29094675183296204 + 0.01 * 7.002718925476074
Epoch 290, val loss: 0.7406148314476013
Epoch 300, training loss: 0.3320064842700958 = 0.2620721757411957 + 0.01 * 6.99343204498291
Epoch 300, val loss: 0.7454327344894409
Epoch 310, training loss: 0.3041015863418579 = 0.23424841463565826 + 0.01 * 6.985318183898926
Epoch 310, val loss: 0.7511113882064819
Epoch 320, training loss: 0.27720433473587036 = 0.20741870999336243 + 0.01 * 6.978561878204346
Epoch 320, val loss: 0.7575130462646484
Epoch 330, training loss: 0.2519534230232239 = 0.1821969747543335 + 0.01 * 6.975643634796143
Epoch 330, val loss: 0.7648026943206787
Epoch 340, training loss: 0.2291354238986969 = 0.15938742458820343 + 0.01 * 6.974799156188965
Epoch 340, val loss: 0.7731848359107971
Epoch 350, training loss: 0.20910689234733582 = 0.1394052803516388 + 0.01 * 6.9701619148254395
Epoch 350, val loss: 0.7830394506454468
Epoch 360, training loss: 0.1918422281742096 = 0.12217453122138977 + 0.01 * 6.966770172119141
Epoch 360, val loss: 0.7941842079162598
Epoch 370, training loss: 0.17702239751815796 = 0.10739149153232574 + 0.01 * 6.963090419769287
Epoch 370, val loss: 0.8066121339797974
Epoch 380, training loss: 0.16434043645858765 = 0.09473513066768646 + 0.01 * 6.960530757904053
Epoch 380, val loss: 0.8201830387115479
Epoch 390, training loss: 0.1534940004348755 = 0.08389712870121002 + 0.01 * 6.959688186645508
Epoch 390, val loss: 0.8345943689346313
Epoch 400, training loss: 0.1441521942615509 = 0.0745958536863327 + 0.01 * 6.955633163452148
Epoch 400, val loss: 0.8496294021606445
Epoch 410, training loss: 0.13610804080963135 = 0.06657812744379044 + 0.01 * 6.952991962432861
Epoch 410, val loss: 0.8651539087295532
Epoch 420, training loss: 0.12916819751262665 = 0.05961634963750839 + 0.01 * 6.9551849365234375
Epoch 420, val loss: 0.8809120059013367
Epoch 430, training loss: 0.12302866578102112 = 0.05354669317603111 + 0.01 * 6.948197364807129
Epoch 430, val loss: 0.8967760801315308
Epoch 440, training loss: 0.11769114434719086 = 0.04822594299912453 + 0.01 * 6.9465203285217285
Epoch 440, val loss: 0.9126735329627991
Epoch 450, training loss: 0.11298997700214386 = 0.04354427009820938 + 0.01 * 6.944571018218994
Epoch 450, val loss: 0.9285293221473694
Epoch 460, training loss: 0.10882914811372757 = 0.03941596299409866 + 0.01 * 6.941318988800049
Epoch 460, val loss: 0.9442305564880371
Epoch 470, training loss: 0.10515603423118591 = 0.03576512262225151 + 0.01 * 6.939090728759766
Epoch 470, val loss: 0.9597275257110596
Epoch 480, training loss: 0.10190057754516602 = 0.03253216668963432 + 0.01 * 6.93684196472168
Epoch 480, val loss: 0.9749554395675659
Epoch 490, training loss: 0.0990075170993805 = 0.029667513445019722 + 0.01 * 6.934000492095947
Epoch 490, val loss: 0.9898843169212341
Epoch 500, training loss: 0.0964483991265297 = 0.027125880122184753 + 0.01 * 6.932251930236816
Epoch 500, val loss: 1.004481315612793
Epoch 510, training loss: 0.0941663533449173 = 0.024868257343769073 + 0.01 * 6.929810047149658
Epoch 510, val loss: 1.0187214612960815
Epoch 520, training loss: 0.0921245664358139 = 0.022859741002321243 + 0.01 * 6.926482677459717
Epoch 520, val loss: 1.0326107740402222
Epoch 530, training loss: 0.0903174877166748 = 0.02106868289411068 + 0.01 * 6.924880504608154
Epoch 530, val loss: 1.0461472272872925
Epoch 540, training loss: 0.08868879079818726 = 0.01946941949427128 + 0.01 * 6.921937465667725
Epoch 540, val loss: 1.059253215789795
Epoch 550, training loss: 0.08722491562366486 = 0.018038205802440643 + 0.01 * 6.918671131134033
Epoch 550, val loss: 1.0719903707504272
Epoch 560, training loss: 0.0859130397439003 = 0.016754036769270897 + 0.01 * 6.915900230407715
Epoch 560, val loss: 1.084337592124939
Epoch 570, training loss: 0.08478190749883652 = 0.015598016791045666 + 0.01 * 6.918389320373535
Epoch 570, val loss: 1.096330165863037
Epoch 580, training loss: 0.08367712795734406 = 0.014554205350577831 + 0.01 * 6.91229248046875
Epoch 580, val loss: 1.1080063581466675
Epoch 590, training loss: 0.08267845213413239 = 0.013606786727905273 + 0.01 * 6.907166481018066
Epoch 590, val loss: 1.119390845298767
Epoch 600, training loss: 0.0817955806851387 = 0.012741828337311745 + 0.01 * 6.9053754806518555
Epoch 600, val loss: 1.130601406097412
Epoch 610, training loss: 0.08099059760570526 = 0.011950320564210415 + 0.01 * 6.904027938842773
Epoch 610, val loss: 1.141607403755188
Epoch 620, training loss: 0.08023341745138168 = 0.011226004920899868 + 0.01 * 6.900741100311279
Epoch 620, val loss: 1.1523998975753784
Epoch 630, training loss: 0.07954040914773941 = 0.010562308132648468 + 0.01 * 6.897810459136963
Epoch 630, val loss: 1.1630243062973022
Epoch 640, training loss: 0.07896097749471664 = 0.009954185225069523 + 0.01 * 6.900679111480713
Epoch 640, val loss: 1.1733871698379517
Epoch 650, training loss: 0.07834455370903015 = 0.009396538138389587 + 0.01 * 6.894801616668701
Epoch 650, val loss: 1.1835262775421143
Epoch 660, training loss: 0.07779346406459808 = 0.008885016664862633 + 0.01 * 6.890844821929932
Epoch 660, val loss: 1.193407654762268
Epoch 670, training loss: 0.0772760882973671 = 0.008415164425969124 + 0.01 * 6.886092662811279
Epoch 670, val loss: 1.2030165195465088
Epoch 680, training loss: 0.0767919197678566 = 0.00798255205154419 + 0.01 * 6.880937099456787
Epoch 680, val loss: 1.212416648864746
Epoch 690, training loss: 0.07642879337072372 = 0.0075835431925952435 + 0.01 * 6.884525299072266
Epoch 690, val loss: 1.2215983867645264
Epoch 700, training loss: 0.07595476508140564 = 0.007215467747300863 + 0.01 * 6.873929500579834
Epoch 700, val loss: 1.2304997444152832
Epoch 710, training loss: 0.07573059946298599 = 0.006875059567391872 + 0.01 * 6.88555383682251
Epoch 710, val loss: 1.239145040512085
Epoch 720, training loss: 0.07524192333221436 = 0.006560673471540213 + 0.01 * 6.868124961853027
Epoch 720, val loss: 1.2475377321243286
Epoch 730, training loss: 0.07488104701042175 = 0.0062688481993973255 + 0.01 * 6.861219882965088
Epoch 730, val loss: 1.2557752132415771
Epoch 740, training loss: 0.07477091997861862 = 0.005997628439217806 + 0.01 * 6.877329349517822
Epoch 740, val loss: 1.2637757062911987
Epoch 750, training loss: 0.07437088340520859 = 0.005745654460042715 + 0.01 * 6.862522602081299
Epoch 750, val loss: 1.2714792490005493
Epoch 760, training loss: 0.07405409961938858 = 0.00551092391833663 + 0.01 * 6.854317665100098
Epoch 760, val loss: 1.279090166091919
Epoch 770, training loss: 0.07388220727443695 = 0.005291786510497332 + 0.01 * 6.859041690826416
Epoch 770, val loss: 1.2864266633987427
Epoch 780, training loss: 0.07364203780889511 = 0.005087047349661589 + 0.01 * 6.855498790740967
Epoch 780, val loss: 1.2935365438461304
Epoch 790, training loss: 0.07332900911569595 = 0.004895535763353109 + 0.01 * 6.843347549438477
Epoch 790, val loss: 1.3005273342132568
Epoch 800, training loss: 0.07331021130084991 = 0.004716347903013229 + 0.01 * 6.859386920928955
Epoch 800, val loss: 1.3072677850723267
Epoch 810, training loss: 0.07289176434278488 = 0.004548302385956049 + 0.01 * 6.834346294403076
Epoch 810, val loss: 1.313843846321106
Epoch 820, training loss: 0.07276687026023865 = 0.004390395246446133 + 0.01 * 6.837647438049316
Epoch 820, val loss: 1.3203134536743164
Epoch 830, training loss: 0.0726114884018898 = 0.0042421091347932816 + 0.01 * 6.83693790435791
Epoch 830, val loss: 1.3265005350112915
Epoch 840, training loss: 0.07237464189529419 = 0.004102170001715422 + 0.01 * 6.827247619628906
Epoch 840, val loss: 1.3326150178909302
Epoch 850, training loss: 0.07231375575065613 = 0.003970526624470949 + 0.01 * 6.834323406219482
Epoch 850, val loss: 1.3385282754898071
Epoch 860, training loss: 0.07203046977519989 = 0.003846453269943595 + 0.01 * 6.818402290344238
Epoch 860, val loss: 1.3442513942718506
Epoch 870, training loss: 0.07195395976305008 = 0.003729390911757946 + 0.01 * 6.822457313537598
Epoch 870, val loss: 1.3498672246932983
Epoch 880, training loss: 0.07178758829832077 = 0.003618901129812002 + 0.01 * 6.816868305206299
Epoch 880, val loss: 1.3552824258804321
Epoch 890, training loss: 0.07157178968191147 = 0.0035142344422638416 + 0.01 * 6.805755615234375
Epoch 890, val loss: 1.3605859279632568
Epoch 900, training loss: 0.0716702863574028 = 0.003415194572880864 + 0.01 * 6.825509071350098
Epoch 900, val loss: 1.3657804727554321
Epoch 910, training loss: 0.07122324407100677 = 0.003321294439956546 + 0.01 * 6.790194511413574
Epoch 910, val loss: 1.3707467317581177
Epoch 920, training loss: 0.07113005220890045 = 0.0032321198377758265 + 0.01 * 6.789793014526367
Epoch 920, val loss: 1.3757143020629883
Epoch 930, training loss: 0.07111199200153351 = 0.0031474337447434664 + 0.01 * 6.796456336975098
Epoch 930, val loss: 1.380610466003418
Epoch 940, training loss: 0.07103551179170609 = 0.0030668203253299 + 0.01 * 6.796869277954102
Epoch 940, val loss: 1.385246992111206
Epoch 950, training loss: 0.0707722082734108 = 0.002989994129166007 + 0.01 * 6.77822208404541
Epoch 950, val loss: 1.3898645639419556
Epoch 960, training loss: 0.07066121697425842 = 0.002916917437687516 + 0.01 * 6.774429798126221
Epoch 960, val loss: 1.3943965435028076
Epoch 970, training loss: 0.07065153867006302 = 0.002846989082172513 + 0.01 * 6.780455112457275
Epoch 970, val loss: 1.3988137245178223
Epoch 980, training loss: 0.07049639523029327 = 0.0027803853154182434 + 0.01 * 6.77160120010376
Epoch 980, val loss: 1.4031306505203247
Epoch 990, training loss: 0.07032257318496704 = 0.0027167892549186945 + 0.01 * 6.7605791091918945
Epoch 990, val loss: 1.4073913097381592
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8339483394833949
The final CL Acc:0.78889, 0.00907, The final GNN Acc:0.83887, 0.00358
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10580])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0388987064361572 = 1.9529300928115845 + 0.01 * 8.596866607666016
Epoch 0, val loss: 1.9508165121078491
Epoch 10, training loss: 2.027832269668579 = 1.941864252090454 + 0.01 * 8.5968017578125
Epoch 10, val loss: 1.939764142036438
Epoch 20, training loss: 2.013488531112671 = 1.9275227785110474 + 0.01 * 8.59657096862793
Epoch 20, val loss: 1.9247732162475586
Epoch 30, training loss: 1.9929701089859009 = 1.9070137739181519 + 0.01 * 8.595629692077637
Epoch 30, val loss: 1.902847409248352
Epoch 40, training loss: 1.9634488821029663 = 1.8775498867034912 + 0.01 * 8.5899019241333
Epoch 40, val loss: 1.8716002702713013
Epoch 50, training loss: 1.9264382123947144 = 1.8408434391021729 + 0.01 * 8.559475898742676
Epoch 50, val loss: 1.835211157798767
Epoch 60, training loss: 1.8937151432037354 = 1.8092010021209717 + 0.01 * 8.451411247253418
Epoch 60, val loss: 1.809338927268982
Epoch 70, training loss: 1.8666213750839233 = 1.7834978103637695 + 0.01 * 8.312353134155273
Epoch 70, val loss: 1.790593147277832
Epoch 80, training loss: 1.8318874835968018 = 1.749657392501831 + 0.01 * 8.223014831542969
Epoch 80, val loss: 1.7631094455718994
Epoch 90, training loss: 1.7825974225997925 = 1.7013782262802124 + 0.01 * 8.121923446655273
Epoch 90, val loss: 1.7219654321670532
Epoch 100, training loss: 1.7142387628555298 = 1.6338961124420166 + 0.01 * 8.03426742553711
Epoch 100, val loss: 1.6657131910324097
Epoch 110, training loss: 1.6328734159469604 = 1.553598165512085 + 0.01 * 7.927530765533447
Epoch 110, val loss: 1.601743221282959
Epoch 120, training loss: 1.5480448007583618 = 1.4707986116409302 + 0.01 * 7.724615097045898
Epoch 120, val loss: 1.5393521785736084
Epoch 130, training loss: 1.4659404754638672 = 1.3895368576049805 + 0.01 * 7.640364646911621
Epoch 130, val loss: 1.4814023971557617
Epoch 140, training loss: 1.384021520614624 = 1.3082008361816406 + 0.01 * 7.582062721252441
Epoch 140, val loss: 1.426378607749939
Epoch 150, training loss: 1.2998616695404053 = 1.2246671915054321 + 0.01 * 7.519443988800049
Epoch 150, val loss: 1.3695842027664185
Epoch 160, training loss: 1.214442253112793 = 1.1397737264633179 + 0.01 * 7.466856002807617
Epoch 160, val loss: 1.3119813203811646
Epoch 170, training loss: 1.1308069229125977 = 1.056607961654663 + 0.01 * 7.41989278793335
Epoch 170, val loss: 1.2567635774612427
Epoch 180, training loss: 1.051259994506836 = 0.9773655533790588 + 0.01 * 7.389439105987549
Epoch 180, val loss: 1.2063478231430054
Epoch 190, training loss: 0.9758139848709106 = 0.9020317792892456 + 0.01 * 7.378218173980713
Epoch 190, val loss: 1.1609039306640625
Epoch 200, training loss: 0.9039276242256165 = 0.8302017450332642 + 0.01 * 7.372589588165283
Epoch 200, val loss: 1.119156837463379
Epoch 210, training loss: 0.8364239931106567 = 0.7627537846565247 + 0.01 * 7.36702299118042
Epoch 210, val loss: 1.0814390182495117
Epoch 220, training loss: 0.7745416760444641 = 0.7009320855140686 + 0.01 * 7.360960006713867
Epoch 220, val loss: 1.048588514328003
Epoch 230, training loss: 0.718092679977417 = 0.6445541977882385 + 0.01 * 7.353847026824951
Epoch 230, val loss: 1.0205628871917725
Epoch 240, training loss: 0.6655853390693665 = 0.5921295285224915 + 0.01 * 7.345579147338867
Epoch 240, val loss: 0.9966375231742859
Epoch 250, training loss: 0.6156003475189209 = 0.5422332286834717 + 0.01 * 7.336710453033447
Epoch 250, val loss: 0.9764792919158936
Epoch 260, training loss: 0.5677222609519958 = 0.49449267983436584 + 0.01 * 7.322957992553711
Epoch 260, val loss: 0.9603947401046753
Epoch 270, training loss: 0.5223783254623413 = 0.44929027557373047 + 0.01 * 7.308807849884033
Epoch 270, val loss: 0.9490500092506409
Epoch 280, training loss: 0.47982341051101685 = 0.4068913161754608 + 0.01 * 7.293210983276367
Epoch 280, val loss: 0.9427882432937622
Epoch 290, training loss: 0.44004160165786743 = 0.3672942519187927 + 0.01 * 7.274734020233154
Epoch 290, val loss: 0.9415958523750305
Epoch 300, training loss: 0.40305468440055847 = 0.33043569326400757 + 0.01 * 7.261899471282959
Epoch 300, val loss: 0.9451438188552856
Epoch 310, training loss: 0.3686082363128662 = 0.29614776372909546 + 0.01 * 7.246048927307129
Epoch 310, val loss: 0.9528802037239075
Epoch 320, training loss: 0.33634573221206665 = 0.2640167772769928 + 0.01 * 7.2328948974609375
Epoch 320, val loss: 0.9642655253410339
Epoch 330, training loss: 0.3059532642364502 = 0.23370353877544403 + 0.01 * 7.224971294403076
Epoch 330, val loss: 0.9786364436149597
Epoch 340, training loss: 0.27735066413879395 = 0.20517165958881378 + 0.01 * 7.217898845672607
Epoch 340, val loss: 0.9954319596290588
Epoch 350, training loss: 0.2508857250213623 = 0.17875519394874573 + 0.01 * 7.213052749633789
Epoch 350, val loss: 1.0145134925842285
Epoch 360, training loss: 0.2270025759935379 = 0.15491792559623718 + 0.01 * 7.208465099334717
Epoch 360, val loss: 1.0360454320907593
Epoch 370, training loss: 0.20605403184890747 = 0.13399839401245117 + 0.01 * 7.205564022064209
Epoch 370, val loss: 1.0601577758789062
Epoch 380, training loss: 0.18804693222045898 = 0.11604781448841095 + 0.01 * 7.199911594390869
Epoch 380, val loss: 1.0865402221679688
Epoch 390, training loss: 0.17278018593788147 = 0.10084232687950134 + 0.01 * 7.193785667419434
Epoch 390, val loss: 1.1145602464675903
Epoch 400, training loss: 0.15991926193237305 = 0.08802197128534317 + 0.01 * 7.189728260040283
Epoch 400, val loss: 1.1437662839889526
Epoch 410, training loss: 0.14907214045524597 = 0.07721076160669327 + 0.01 * 7.186139106750488
Epoch 410, val loss: 1.173555612564087
Epoch 420, training loss: 0.13988405466079712 = 0.0680675357580185 + 0.01 * 7.1816511154174805
Epoch 420, val loss: 1.203457236289978
Epoch 430, training loss: 0.13204684853553772 = 0.060299087315797806 + 0.01 * 7.174776554107666
Epoch 430, val loss: 1.2331372499465942
Epoch 440, training loss: 0.12542128562927246 = 0.05366705730557442 + 0.01 * 7.1754231452941895
Epoch 440, val loss: 1.2623887062072754
Epoch 450, training loss: 0.11965125799179077 = 0.04797843098640442 + 0.01 * 7.167282581329346
Epoch 450, val loss: 1.290981411933899
Epoch 460, training loss: 0.1146862730383873 = 0.04307064414024353 + 0.01 * 7.161563396453857
Epoch 460, val loss: 1.3189072608947754
Epoch 470, training loss: 0.1103607788681984 = 0.038811177015304565 + 0.01 * 7.1549601554870605
Epoch 470, val loss: 1.3462135791778564
Epoch 480, training loss: 0.10672049969434738 = 0.0350971594452858 + 0.01 * 7.162334442138672
Epoch 480, val loss: 1.3729329109191895
Epoch 490, training loss: 0.10334759205579758 = 0.03185223042964935 + 0.01 * 7.149536609649658
Epoch 490, val loss: 1.398918628692627
Epoch 500, training loss: 0.10038168728351593 = 0.02900608628988266 + 0.01 * 7.1375603675842285
Epoch 500, val loss: 1.424214482307434
Epoch 510, training loss: 0.09795735776424408 = 0.026498842984437943 + 0.01 * 7.145851135253906
Epoch 510, val loss: 1.448805332183838
Epoch 520, training loss: 0.09555773437023163 = 0.024287918582558632 + 0.01 * 7.126981735229492
Epoch 520, val loss: 1.472672462463379
Epoch 530, training loss: 0.09353141486644745 = 0.02232939749956131 + 0.01 * 7.120201587677002
Epoch 530, val loss: 1.4958159923553467
Epoch 540, training loss: 0.09172103554010391 = 0.020586436614394188 + 0.01 * 7.113460063934326
Epoch 540, val loss: 1.5183290243148804
Epoch 550, training loss: 0.09020694345235825 = 0.019036702811717987 + 0.01 * 7.1170244216918945
Epoch 550, val loss: 1.5400532484054565
Epoch 560, training loss: 0.08869227766990662 = 0.017654646188020706 + 0.01 * 7.103763103485107
Epoch 560, val loss: 1.5610435009002686
Epoch 570, training loss: 0.08737234771251678 = 0.016415338963270187 + 0.01 * 7.095701694488525
Epoch 570, val loss: 1.5814160108566284
Epoch 580, training loss: 0.08630786836147308 = 0.01529838889837265 + 0.01 * 7.100947856903076
Epoch 580, val loss: 1.601265788078308
Epoch 590, training loss: 0.08513498306274414 = 0.01428929716348648 + 0.01 * 7.084568977355957
Epoch 590, val loss: 1.6205272674560547
Epoch 600, training loss: 0.08417244255542755 = 0.013371938839554787 + 0.01 * 7.080050468444824
Epoch 600, val loss: 1.639415979385376
Epoch 610, training loss: 0.08320818841457367 = 0.012536510825157166 + 0.01 * 7.06716775894165
Epoch 610, val loss: 1.6578820943832397
Epoch 620, training loss: 0.08237143605947495 = 0.011774453334510326 + 0.01 * 7.059699058532715
Epoch 620, val loss: 1.6759451627731323
Epoch 630, training loss: 0.0816856175661087 = 0.011080670170485973 + 0.01 * 7.060495376586914
Epoch 630, val loss: 1.6935038566589355
Epoch 640, training loss: 0.08094097673892975 = 0.01044535543769598 + 0.01 * 7.049561977386475
Epoch 640, val loss: 1.7107245922088623
Epoch 650, training loss: 0.0802232101559639 = 0.00986245833337307 + 0.01 * 7.036075115203857
Epoch 650, val loss: 1.7275713682174683
Epoch 660, training loss: 0.07966796308755875 = 0.009326082654297352 + 0.01 * 7.034188270568848
Epoch 660, val loss: 1.7441093921661377
Epoch 670, training loss: 0.07963989675045013 = 0.008835979737341404 + 0.01 * 7.080391883850098
Epoch 670, val loss: 1.7599080801010132
Epoch 680, training loss: 0.0787874162197113 = 0.008387413807213306 + 0.01 * 7.039999961853027
Epoch 680, val loss: 1.7752114534378052
Epoch 690, training loss: 0.07813306152820587 = 0.007974034175276756 + 0.01 * 7.015902519226074
Epoch 690, val loss: 1.7901614904403687
Epoch 700, training loss: 0.07789259403944016 = 0.007591418456286192 + 0.01 * 7.030117511749268
Epoch 700, val loss: 1.8047287464141846
Epoch 710, training loss: 0.07738704979419708 = 0.007237124722450972 + 0.01 * 7.014993190765381
Epoch 710, val loss: 1.8189873695373535
Epoch 720, training loss: 0.07692207396030426 = 0.006909117568284273 + 0.01 * 7.001296043395996
Epoch 720, val loss: 1.8327763080596924
Epoch 730, training loss: 0.07642111927270889 = 0.006604102905839682 + 0.01 * 6.981701850891113
Epoch 730, val loss: 1.84629487991333
Epoch 740, training loss: 0.07612966001033783 = 0.006320327054709196 + 0.01 * 6.980933666229248
Epoch 740, val loss: 1.8593841791152954
Epoch 750, training loss: 0.07600648701190948 = 0.006056086625903845 + 0.01 * 6.995040416717529
Epoch 750, val loss: 1.8721517324447632
Epoch 760, training loss: 0.07554499059915543 = 0.005810115020722151 + 0.01 * 6.9734883308410645
Epoch 760, val loss: 1.8845628499984741
Epoch 770, training loss: 0.07525402307510376 = 0.005580422934144735 + 0.01 * 6.967360019683838
Epoch 770, val loss: 1.8966702222824097
Epoch 780, training loss: 0.07484500855207443 = 0.005365257151424885 + 0.01 * 6.947974681854248
Epoch 780, val loss: 1.9084488153457642
Epoch 790, training loss: 0.07474878430366516 = 0.005163851194083691 + 0.01 * 6.958493709564209
Epoch 790, val loss: 1.9199740886688232
Epoch 800, training loss: 0.07457306981086731 = 0.004974940326064825 + 0.01 * 6.959813117980957
Epoch 800, val loss: 1.9312065839767456
Epoch 810, training loss: 0.0741366297006607 = 0.00479759369045496 + 0.01 * 6.933903694152832
Epoch 810, val loss: 1.9420859813690186
Epoch 820, training loss: 0.07391347736120224 = 0.004630871582776308 + 0.01 * 6.928260803222656
Epoch 820, val loss: 1.952754020690918
Epoch 830, training loss: 0.07370905578136444 = 0.0044735693372786045 + 0.01 * 6.923548698425293
Epoch 830, val loss: 1.9632033109664917
Epoch 840, training loss: 0.07361212372779846 = 0.004325335845351219 + 0.01 * 6.9286789894104
Epoch 840, val loss: 1.9733511209487915
Epoch 850, training loss: 0.07342971861362457 = 0.004185769706964493 + 0.01 * 6.9243950843811035
Epoch 850, val loss: 1.9832044839859009
Epoch 860, training loss: 0.07333063334226608 = 0.0040543037466704845 + 0.01 * 6.927633762359619
Epoch 860, val loss: 1.992823839187622
Epoch 870, training loss: 0.07305582612752914 = 0.003929920494556427 + 0.01 * 6.912590980529785
Epoch 870, val loss: 2.0022835731506348
Epoch 880, training loss: 0.07292492687702179 = 0.0038123559206724167 + 0.01 * 6.911256790161133
Epoch 880, val loss: 2.0114119052886963
Epoch 890, training loss: 0.07272189110517502 = 0.003700883826240897 + 0.01 * 6.902101039886475
Epoch 890, val loss: 2.020399332046509
Epoch 900, training loss: 0.07252778857946396 = 0.0035952262114733458 + 0.01 * 6.893256664276123
Epoch 900, val loss: 2.0291507244110107
Epoch 910, training loss: 0.07236309349536896 = 0.0034950331319123507 + 0.01 * 6.886806488037109
Epoch 910, val loss: 2.0376739501953125
Epoch 920, training loss: 0.07235871255397797 = 0.0033995083067566156 + 0.01 * 6.895920753479004
Epoch 920, val loss: 2.046062469482422
Epoch 930, training loss: 0.07210466265678406 = 0.003308917861431837 + 0.01 * 6.879575252532959
Epoch 930, val loss: 2.0542242527008057
Epoch 940, training loss: 0.0720774382352829 = 0.0032224170863628387 + 0.01 * 6.885502815246582
Epoch 940, val loss: 2.062246561050415
Epoch 950, training loss: 0.072076715528965 = 0.0031402341555804014 + 0.01 * 6.893648147583008
Epoch 950, val loss: 2.0700230598449707
Epoch 960, training loss: 0.07181496918201447 = 0.003062053583562374 + 0.01 * 6.87529182434082
Epoch 960, val loss: 2.077674627304077
Epoch 970, training loss: 0.07170918583869934 = 0.0029873063322156668 + 0.01 * 6.872188568115234
Epoch 970, val loss: 2.085181951522827
Epoch 980, training loss: 0.07161086797714233 = 0.00291589368134737 + 0.01 * 6.869497299194336
Epoch 980, val loss: 2.0925235748291016
Epoch 990, training loss: 0.07149772346019745 = 0.0028478214517235756 + 0.01 * 6.864990234375
Epoch 990, val loss: 2.0996670722961426
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8228782287822879
=== training gcn model ===
Epoch 0, training loss: 2.056755542755127 = 1.9707868099212646 + 0.01 * 8.596866607666016
Epoch 0, val loss: 1.974055290222168
Epoch 10, training loss: 2.0451934337615967 = 1.9592251777648926 + 0.01 * 8.596823692321777
Epoch 10, val loss: 1.9620169401168823
Epoch 20, training loss: 2.0308642387390137 = 1.9448975324630737 + 0.01 * 8.59667682647705
Epoch 20, val loss: 1.9468358755111694
Epoch 30, training loss: 2.0107038021087646 = 1.9247422218322754 + 0.01 * 8.596168518066406
Epoch 30, val loss: 1.925054907798767
Epoch 40, training loss: 1.9811351299285889 = 1.8952012062072754 + 0.01 * 8.593392372131348
Epoch 40, val loss: 1.8929243087768555
Epoch 50, training loss: 1.9406514167785645 = 1.8549118041992188 + 0.01 * 8.573962211608887
Epoch 50, val loss: 1.8505406379699707
Epoch 60, training loss: 1.8980950117111206 = 1.8130782842636108 + 0.01 * 8.501676559448242
Epoch 60, val loss: 1.8113055229187012
Epoch 70, training loss: 1.8668360710144043 = 1.7836055755615234 + 0.01 * 8.323049545288086
Epoch 70, val loss: 1.788735270500183
Epoch 80, training loss: 1.8320910930633545 = 1.7497557401657104 + 0.01 * 8.233534812927246
Epoch 80, val loss: 1.761237382888794
Epoch 90, training loss: 1.7836456298828125 = 1.7028257846832275 + 0.01 * 8.08198070526123
Epoch 90, val loss: 1.7208975553512573
Epoch 100, training loss: 1.7154271602630615 = 1.636821985244751 + 0.01 * 7.86051607131958
Epoch 100, val loss: 1.6645132303237915
Epoch 110, training loss: 1.6291998624801636 = 1.5522327423095703 + 0.01 * 7.696716785430908
Epoch 110, val loss: 1.595187783241272
Epoch 120, training loss: 1.5353784561157227 = 1.4588868618011475 + 0.01 * 7.649153709411621
Epoch 120, val loss: 1.5207788944244385
Epoch 130, training loss: 1.440355896949768 = 1.3642209768295288 + 0.01 * 7.613488674163818
Epoch 130, val loss: 1.4490597248077393
Epoch 140, training loss: 1.3450303077697754 = 1.2693593502044678 + 0.01 * 7.5670976638793945
Epoch 140, val loss: 1.3807047605514526
Epoch 150, training loss: 1.2490723133087158 = 1.1738789081573486 + 0.01 * 7.5193400382995605
Epoch 150, val loss: 1.3130046129226685
Epoch 160, training loss: 1.1543993949890137 = 1.0795892477035522 + 0.01 * 7.481016635894775
Epoch 160, val loss: 1.2478607892990112
Epoch 170, training loss: 1.0640116930007935 = 0.9896147847175598 + 0.01 * 7.439688205718994
Epoch 170, val loss: 1.1876230239868164
Epoch 180, training loss: 0.9796023368835449 = 0.9056488275527954 + 0.01 * 7.395350456237793
Epoch 180, val loss: 1.133023977279663
Epoch 190, training loss: 0.9021186232566833 = 0.8284834623336792 + 0.01 * 7.363518238067627
Epoch 190, val loss: 1.0847703218460083
Epoch 200, training loss: 0.8321329951286316 = 0.7587507367134094 + 0.01 * 7.338225364685059
Epoch 200, val loss: 1.0432380437850952
Epoch 210, training loss: 0.7695111036300659 = 0.696377694606781 + 0.01 * 7.313343524932861
Epoch 210, val loss: 1.008902907371521
Epoch 220, training loss: 0.713064432144165 = 0.6402252912521362 + 0.01 * 7.283912181854248
Epoch 220, val loss: 0.9815407991409302
Epoch 230, training loss: 0.6612290740013123 = 0.5885633826255798 + 0.01 * 7.266571044921875
Epoch 230, val loss: 0.9599592685699463
Epoch 240, training loss: 0.612525463104248 = 0.5400648713111877 + 0.01 * 7.2460618019104
Epoch 240, val loss: 0.9427288174629211
Epoch 250, training loss: 0.5664705634117126 = 0.49412110447883606 + 0.01 * 7.2349467277526855
Epoch 250, val loss: 0.9286607503890991
Epoch 260, training loss: 0.5230075716972351 = 0.45073679089546204 + 0.01 * 7.227076053619385
Epoch 260, val loss: 0.9173399209976196
Epoch 270, training loss: 0.48242658376693726 = 0.4101894795894623 + 0.01 * 7.223710536956787
Epoch 270, val loss: 0.909208357334137
Epoch 280, training loss: 0.44476598501205444 = 0.3725719749927521 + 0.01 * 7.2194013595581055
Epoch 280, val loss: 0.904541552066803
Epoch 290, training loss: 0.40986648201942444 = 0.3377191126346588 + 0.01 * 7.214735984802246
Epoch 290, val loss: 0.9030999541282654
Epoch 300, training loss: 0.37731796503067017 = 0.3051992356777191 + 0.01 * 7.211873531341553
Epoch 300, val loss: 0.9044718146324158
Epoch 310, training loss: 0.346664160490036 = 0.27456238865852356 + 0.01 * 7.210178375244141
Epoch 310, val loss: 0.9082140922546387
Epoch 320, training loss: 0.3176121413707733 = 0.2455267459154129 + 0.01 * 7.208540439605713
Epoch 320, val loss: 0.9139605760574341
Epoch 330, training loss: 0.2902180850505829 = 0.2181413173675537 + 0.01 * 7.207677364349365
Epoch 330, val loss: 0.9215043783187866
Epoch 340, training loss: 0.2647933065891266 = 0.19270817935466766 + 0.01 * 7.208512306213379
Epoch 340, val loss: 0.930832028388977
Epoch 350, training loss: 0.24162665009498596 = 0.1695534586906433 + 0.01 * 7.207319736480713
Epoch 350, val loss: 0.9418852925300598
Epoch 360, training loss: 0.2209547758102417 = 0.1488972008228302 + 0.01 * 7.2057576179504395
Epoch 360, val loss: 0.954643964767456
Epoch 370, training loss: 0.20279747247695923 = 0.1307457685470581 + 0.01 * 7.205171585083008
Epoch 370, val loss: 0.9689133167266846
Epoch 380, training loss: 0.18700169026851654 = 0.11493141949176788 + 0.01 * 7.207026958465576
Epoch 380, val loss: 0.9843617677688599
Epoch 390, training loss: 0.17329668998718262 = 0.10125485807657242 + 0.01 * 7.204183578491211
Epoch 390, val loss: 1.000758171081543
Epoch 400, training loss: 0.16146039962768555 = 0.08943762630224228 + 0.01 * 7.202278137207031
Epoch 400, val loss: 1.0176101922988892
Epoch 410, training loss: 0.15122920274734497 = 0.0792282372713089 + 0.01 * 7.2000956535339355
Epoch 410, val loss: 1.034658432006836
Epoch 420, training loss: 0.14237836003303528 = 0.07039690762758255 + 0.01 * 7.198144435882568
Epoch 420, val loss: 1.0517334938049316
Epoch 430, training loss: 0.13470076024532318 = 0.0627434179186821 + 0.01 * 7.19573450088501
Epoch 430, val loss: 1.0686509609222412
Epoch 440, training loss: 0.12808473408222198 = 0.056097690016031265 + 0.01 * 7.198705196380615
Epoch 440, val loss: 1.0853170156478882
Epoch 450, training loss: 0.12222585827112198 = 0.05031770467758179 + 0.01 * 7.190815448760986
Epoch 450, val loss: 1.1016550064086914
Epoch 460, training loss: 0.1171623095870018 = 0.045275598764419556 + 0.01 * 7.188671112060547
Epoch 460, val loss: 1.1176164150238037
Epoch 470, training loss: 0.11270777881145477 = 0.040866490453481674 + 0.01 * 7.184128761291504
Epoch 470, val loss: 1.1332138776779175
Epoch 480, training loss: 0.10880015045404434 = 0.03700113296508789 + 0.01 * 7.179901599884033
Epoch 480, val loss: 1.1484049558639526
Epoch 490, training loss: 0.10542351007461548 = 0.033605944365262985 + 0.01 * 7.18175745010376
Epoch 490, val loss: 1.163214921951294
Epoch 500, training loss: 0.10237926989793777 = 0.03061935491859913 + 0.01 * 7.175991535186768
Epoch 500, val loss: 1.1775565147399902
Epoch 510, training loss: 0.0996454581618309 = 0.027984341606497765 + 0.01 * 7.166111946105957
Epoch 510, val loss: 1.1914867162704468
Epoch 520, training loss: 0.09729434549808502 = 0.025652989745140076 + 0.01 * 7.164135932922363
Epoch 520, val loss: 1.205014705657959
Epoch 530, training loss: 0.0951407253742218 = 0.023584390059113503 + 0.01 * 7.155633926391602
Epoch 530, val loss: 1.2181391716003418
Epoch 540, training loss: 0.09359274059534073 = 0.021746596321463585 + 0.01 * 7.184614658355713
Epoch 540, val loss: 1.2308387756347656
Epoch 550, training loss: 0.09162832796573639 = 0.020113764330744743 + 0.01 * 7.151456356048584
Epoch 550, val loss: 1.243080735206604
Epoch 560, training loss: 0.090028777718544 = 0.018655460327863693 + 0.01 * 7.137331485748291
Epoch 560, val loss: 1.254979133605957
Epoch 570, training loss: 0.08865102380514145 = 0.017347505316138268 + 0.01 * 7.130351543426514
Epoch 570, val loss: 1.266560435295105
Epoch 580, training loss: 0.08764829486608505 = 0.016170935705304146 + 0.01 * 7.147736072540283
Epoch 580, val loss: 1.2777833938598633
Epoch 590, training loss: 0.08632495254278183 = 0.015112309716641903 + 0.01 * 7.1212639808654785
Epoch 590, val loss: 1.2886054515838623
Epoch 600, training loss: 0.0853031650185585 = 0.014156823977828026 + 0.01 * 7.114634037017822
Epoch 600, val loss: 1.2990630865097046
Epoch 610, training loss: 0.08463604003190994 = 0.013291274197399616 + 0.01 * 7.134476184844971
Epoch 610, val loss: 1.3092041015625
Epoch 620, training loss: 0.08340208232402802 = 0.012507397681474686 + 0.01 * 7.089468479156494
Epoch 620, val loss: 1.3190110921859741
Epoch 630, training loss: 0.08260674774646759 = 0.01179440412670374 + 0.01 * 7.081234931945801
Epoch 630, val loss: 1.3284497261047363
Epoch 640, training loss: 0.08197921514511108 = 0.01114386972039938 + 0.01 * 7.083534240722656
Epoch 640, val loss: 1.337630033493042
Epoch 650, training loss: 0.08157326281070709 = 0.010548670776188374 + 0.01 * 7.102458953857422
Epoch 650, val loss: 1.3465033769607544
Epoch 660, training loss: 0.08076789230108261 = 0.010004089213907719 + 0.01 * 7.076380729675293
Epoch 660, val loss: 1.3551108837127686
Epoch 670, training loss: 0.07999071478843689 = 0.00950318668037653 + 0.01 * 7.048752784729004
Epoch 670, val loss: 1.3634456396102905
Epoch 680, training loss: 0.07952915132045746 = 0.009041807614266872 + 0.01 * 7.048734188079834
Epoch 680, val loss: 1.3714993000030518
Epoch 690, training loss: 0.07928559184074402 = 0.008616965264081955 + 0.01 * 7.066862106323242
Epoch 690, val loss: 1.3793245553970337
Epoch 700, training loss: 0.07854703068733215 = 0.008223598822951317 + 0.01 * 7.03234338760376
Epoch 700, val loss: 1.386907935142517
Epoch 710, training loss: 0.07815410941839218 = 0.007858381606638432 + 0.01 * 7.0295729637146
Epoch 710, val loss: 1.3943150043487549
Epoch 720, training loss: 0.07791560888290405 = 0.007519816514104605 + 0.01 * 7.039579391479492
Epoch 720, val loss: 1.4014664888381958
Epoch 730, training loss: 0.07750870287418365 = 0.007203766144812107 + 0.01 * 7.030494213104248
Epoch 730, val loss: 1.4084330797195435
Epoch 740, training loss: 0.07693534344434738 = 0.006910252384841442 + 0.01 * 7.002509593963623
Epoch 740, val loss: 1.4152566194534302
Epoch 750, training loss: 0.0772428810596466 = 0.006636243313550949 + 0.01 * 7.06066370010376
Epoch 750, val loss: 1.4218039512634277
Epoch 760, training loss: 0.07635371387004852 = 0.006380167789757252 + 0.01 * 6.997354507446289
Epoch 760, val loss: 1.428214430809021
Epoch 770, training loss: 0.07627036422491074 = 0.0061409021727740765 + 0.01 * 7.012946128845215
Epoch 770, val loss: 1.4343492984771729
Epoch 780, training loss: 0.07593034207820892 = 0.005916431080549955 + 0.01 * 7.0013909339904785
Epoch 780, val loss: 1.4404462575912476
Epoch 790, training loss: 0.07549671083688736 = 0.005706189200282097 + 0.01 * 6.9790520668029785
Epoch 790, val loss: 1.4462841749191284
Epoch 800, training loss: 0.07523325085639954 = 0.005508264526724815 + 0.01 * 6.972498893737793
Epoch 800, val loss: 1.4519847631454468
Epoch 810, training loss: 0.07498953491449356 = 0.00532215740531683 + 0.01 * 6.966737747192383
Epoch 810, val loss: 1.4575566053390503
Epoch 820, training loss: 0.07505281269550323 = 0.00514685083180666 + 0.01 * 6.990596294403076
Epoch 820, val loss: 1.4629366397857666
Epoch 830, training loss: 0.07476159185171127 = 0.0049812584184110165 + 0.01 * 6.978033542633057
Epoch 830, val loss: 1.4682104587554932
Epoch 840, training loss: 0.0742904543876648 = 0.00482538715004921 + 0.01 * 6.946506500244141
Epoch 840, val loss: 1.473367691040039
Epoch 850, training loss: 0.07430441677570343 = 0.004677820019423962 + 0.01 * 6.96265983581543
Epoch 850, val loss: 1.4782651662826538
Epoch 860, training loss: 0.07404938340187073 = 0.004538267385214567 + 0.01 * 6.951111793518066
Epoch 860, val loss: 1.4831879138946533
Epoch 870, training loss: 0.07380897551774979 = 0.004405961371958256 + 0.01 * 6.940301895141602
Epoch 870, val loss: 1.4879159927368164
Epoch 880, training loss: 0.07363518327474594 = 0.0042804847471416 + 0.01 * 6.935470104217529
Epoch 880, val loss: 1.4924734830856323
Epoch 890, training loss: 0.07348711788654327 = 0.004161503631621599 + 0.01 * 6.93256139755249
Epoch 890, val loss: 1.497005581855774
Epoch 900, training loss: 0.07334060221910477 = 0.0040483842603862286 + 0.01 * 6.929222106933594
Epoch 900, val loss: 1.5013564825057983
Epoch 910, training loss: 0.07369637489318848 = 0.00394078716635704 + 0.01 * 6.975558280944824
Epoch 910, val loss: 1.505599021911621
Epoch 920, training loss: 0.07320783287286758 = 0.003838455770164728 + 0.01 * 6.9369378089904785
Epoch 920, val loss: 1.509750485420227
Epoch 930, training loss: 0.0729798972606659 = 0.003740884829312563 + 0.01 * 6.923901557922363
Epoch 930, val loss: 1.5137971639633179
Epoch 940, training loss: 0.07284431159496307 = 0.00364803452976048 + 0.01 * 6.919627666473389
Epoch 940, val loss: 1.5177069902420044
Epoch 950, training loss: 0.07317126542329788 = 0.0035592690110206604 + 0.01 * 6.961199760437012
Epoch 950, val loss: 1.5216306447982788
Epoch 960, training loss: 0.07262862473726273 = 0.0034746606834232807 + 0.01 * 6.915396690368652
Epoch 960, val loss: 1.5253111124038696
Epoch 970, training loss: 0.07252924144268036 = 0.0033934754319489002 + 0.01 * 6.913576126098633
Epoch 970, val loss: 1.5289177894592285
Epoch 980, training loss: 0.07219129800796509 = 0.0033166236244142056 + 0.01 * 6.887467384338379
Epoch 980, val loss: 1.532547950744629
Epoch 990, training loss: 0.07238098978996277 = 0.0032421608921140432 + 0.01 * 6.913883686065674
Epoch 990, val loss: 1.535975456237793
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8007380073800738
=== training gcn model ===
Epoch 0, training loss: 2.0341482162475586 = 1.9481797218322754 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.9377081394195557
Epoch 10, training loss: 2.0241191387176514 = 1.9381510019302368 + 0.01 * 8.596806526184082
Epoch 10, val loss: 1.9281315803527832
Epoch 20, training loss: 2.0121214389801025 = 1.9261555671691895 + 0.01 * 8.596596717834473
Epoch 20, val loss: 1.916182041168213
Epoch 30, training loss: 1.9959089756011963 = 1.909951090812683 + 0.01 * 8.595789909362793
Epoch 30, val loss: 1.8996453285217285
Epoch 40, training loss: 1.97256600856781 = 1.8866547346115112 + 0.01 * 8.591130256652832
Epoch 40, val loss: 1.8758705854415894
Epoch 50, training loss: 1.9398678541183472 = 1.8542407751083374 + 0.01 * 8.562708854675293
Epoch 50, val loss: 1.8440295457839966
Epoch 60, training loss: 1.9010694026947021 = 1.816792607307434 + 0.01 * 8.427680969238281
Epoch 60, val loss: 1.8110710382461548
Epoch 70, training loss: 1.865929126739502 = 1.7835623025894165 + 0.01 * 8.23668384552002
Epoch 70, val loss: 1.786244511604309
Epoch 80, training loss: 1.8279728889465332 = 1.7473270893096924 + 0.01 * 8.06457805633545
Epoch 80, val loss: 1.7564969062805176
Epoch 90, training loss: 1.7749850749969482 = 1.6968005895614624 + 0.01 * 7.818454265594482
Epoch 90, val loss: 1.7111088037490845
Epoch 100, training loss: 1.7033421993255615 = 1.627053141593933 + 0.01 * 7.628910541534424
Epoch 100, val loss: 1.650815486907959
Epoch 110, training loss: 1.6134127378463745 = 1.5384821891784668 + 0.01 * 7.493054389953613
Epoch 110, val loss: 1.5786348581314087
Epoch 120, training loss: 1.5136536359786987 = 1.4394572973251343 + 0.01 * 7.419636249542236
Epoch 120, val loss: 1.4995713233947754
Epoch 130, training loss: 1.411588191986084 = 1.337968349456787 + 0.01 * 7.361988544464111
Epoch 130, val loss: 1.4219770431518555
Epoch 140, training loss: 1.3108409643173218 = 1.2375073432922363 + 0.01 * 7.333358287811279
Epoch 140, val loss: 1.3486027717590332
Epoch 150, training loss: 1.212747573852539 = 1.139522910118103 + 0.01 * 7.322471618652344
Epoch 150, val loss: 1.2780133485794067
Epoch 160, training loss: 1.1180516481399536 = 1.0448778867721558 + 0.01 * 7.317379951477051
Epoch 160, val loss: 1.2105244398117065
Epoch 170, training loss: 1.0284044742584229 = 0.955282986164093 + 0.01 * 7.3121514320373535
Epoch 170, val loss: 1.1470866203308105
Epoch 180, training loss: 0.9456502199172974 = 0.8726081848144531 + 0.01 * 7.304205417633057
Epoch 180, val loss: 1.0897623300552368
Epoch 190, training loss: 0.8714287281036377 = 0.7985031008720398 + 0.01 * 7.292562007904053
Epoch 190, val loss: 1.0402840375900269
Epoch 200, training loss: 0.8062300682067871 = 0.7334670424461365 + 0.01 * 7.276299953460693
Epoch 200, val loss: 0.9996076822280884
Epoch 210, training loss: 0.748958170413971 = 0.6763824224472046 + 0.01 * 7.257577419281006
Epoch 210, val loss: 0.9674494862556458
Epoch 220, training loss: 0.6972231268882751 = 0.6248364448547363 + 0.01 * 7.23866605758667
Epoch 220, val loss: 0.9417158365249634
Epoch 230, training loss: 0.6484858989715576 = 0.5762535929679871 + 0.01 * 7.223228931427002
Epoch 230, val loss: 0.9198159575462341
Epoch 240, training loss: 0.6009166240692139 = 0.5287912487983704 + 0.01 * 7.2125372886657715
Epoch 240, val loss: 0.8997749090194702
Epoch 250, training loss: 0.5537473559379578 = 0.4816763699054718 + 0.01 * 7.207098960876465
Epoch 250, val loss: 0.880849301815033
Epoch 260, training loss: 0.5071040391921997 = 0.4350799024105072 + 0.01 * 7.20241641998291
Epoch 260, val loss: 0.8630651235580444
Epoch 270, training loss: 0.4616825580596924 = 0.38969939947128296 + 0.01 * 7.198314666748047
Epoch 270, val loss: 0.8474453687667847
Epoch 280, training loss: 0.41834110021591187 = 0.3463778793811798 + 0.01 * 7.196320533752441
Epoch 280, val loss: 0.8347597122192383
Epoch 290, training loss: 0.3778209090232849 = 0.30589672923088074 + 0.01 * 7.192418098449707
Epoch 290, val loss: 0.8255624771118164
Epoch 300, training loss: 0.34071439504623413 = 0.2688045799732208 + 0.01 * 7.190981864929199
Epoch 300, val loss: 0.820206344127655
Epoch 310, training loss: 0.30726706981658936 = 0.23540283739566803 + 0.01 * 7.186422348022461
Epoch 310, val loss: 0.8188284635543823
Epoch 320, training loss: 0.27749261260032654 = 0.20568199455738068 + 0.01 * 7.181062698364258
Epoch 320, val loss: 0.8212608695030212
Epoch 330, training loss: 0.25125935673713684 = 0.1794232428073883 + 0.01 * 7.183611869812012
Epoch 330, val loss: 0.8270715475082397
Epoch 340, training loss: 0.2280413955450058 = 0.15632818639278412 + 0.01 * 7.171320915222168
Epoch 340, val loss: 0.8355770707130432
Epoch 350, training loss: 0.2077658474445343 = 0.13604822754859924 + 0.01 * 7.1717610359191895
Epoch 350, val loss: 0.8463209867477417
Epoch 360, training loss: 0.1899099349975586 = 0.11830366402864456 + 0.01 * 7.160628318786621
Epoch 360, val loss: 0.858814537525177
Epoch 370, training loss: 0.17440985143184662 = 0.10286350548267365 + 0.01 * 7.154634952545166
Epoch 370, val loss: 0.8725556135177612
Epoch 380, training loss: 0.16104811429977417 = 0.0895107313990593 + 0.01 * 7.153738975524902
Epoch 380, val loss: 0.8871381282806396
Epoch 390, training loss: 0.14944951236248016 = 0.07801643759012222 + 0.01 * 7.143307685852051
Epoch 390, val loss: 0.9024026393890381
Epoch 400, training loss: 0.1397935450077057 = 0.06816694885492325 + 0.01 * 7.16265869140625
Epoch 400, val loss: 0.9180935621261597
Epoch 410, training loss: 0.13101787865161896 = 0.05976628512144089 + 0.01 * 7.12515926361084
Epoch 410, val loss: 0.9339970946311951
Epoch 420, training loss: 0.12402655184268951 = 0.05261101946234703 + 0.01 * 7.14155387878418
Epoch 420, val loss: 0.9499382972717285
Epoch 430, training loss: 0.11764703691005707 = 0.04653375968337059 + 0.01 * 7.111327171325684
Epoch 430, val loss: 0.965675950050354
Epoch 440, training loss: 0.11245687305927277 = 0.04135933890938759 + 0.01 * 7.109753608703613
Epoch 440, val loss: 0.9810884594917297
Epoch 450, training loss: 0.10788002610206604 = 0.036941368132829666 + 0.01 * 7.093865871429443
Epoch 450, val loss: 0.9962324500083923
Epoch 460, training loss: 0.10418322682380676 = 0.03316193446516991 + 0.01 * 7.102129936218262
Epoch 460, val loss: 1.0109528303146362
Epoch 470, training loss: 0.10079296678304672 = 0.029922157526016235 + 0.01 * 7.087081432342529
Epoch 470, val loss: 1.0252290964126587
Epoch 480, training loss: 0.09785478562116623 = 0.027126312255859375 + 0.01 * 7.072847366333008
Epoch 480, val loss: 1.039026141166687
Epoch 490, training loss: 0.09542518854141235 = 0.024700116366147995 + 0.01 * 7.072507858276367
Epoch 490, val loss: 1.0523791313171387
Epoch 500, training loss: 0.09318952262401581 = 0.02258373610675335 + 0.01 * 7.060578346252441
Epoch 500, val loss: 1.0652530193328857
Epoch 510, training loss: 0.09126758575439453 = 0.02072570100426674 + 0.01 * 7.054188251495361
Epoch 510, val loss: 1.0776982307434082
Epoch 520, training loss: 0.08960865437984467 = 0.019094111397862434 + 0.01 * 7.051454544067383
Epoch 520, val loss: 1.0896447896957397
Epoch 530, training loss: 0.088068388402462 = 0.017656227573752403 + 0.01 * 7.041215896606445
Epoch 530, val loss: 1.101128101348877
Epoch 540, training loss: 0.08685695379972458 = 0.016379600390791893 + 0.01 * 7.047735214233398
Epoch 540, val loss: 1.112176775932312
Epoch 550, training loss: 0.08561909198760986 = 0.015243382193148136 + 0.01 * 7.037570953369141
Epoch 550, val loss: 1.1228591203689575
Epoch 560, training loss: 0.08451295644044876 = 0.014227226376533508 + 0.01 * 7.028573513031006
Epoch 560, val loss: 1.133145809173584
Epoch 570, training loss: 0.08352146297693253 = 0.01331472396850586 + 0.01 * 7.020673751831055
Epoch 570, val loss: 1.1431018114089966
Epoch 580, training loss: 0.08279727399349213 = 0.01249245647341013 + 0.01 * 7.030482292175293
Epoch 580, val loss: 1.1527206897735596
Epoch 590, training loss: 0.08188554644584656 = 0.01174909994006157 + 0.01 * 7.013644218444824
Epoch 590, val loss: 1.1618750095367432
Epoch 600, training loss: 0.08121998608112335 = 0.011074485257267952 + 0.01 * 7.01455020904541
Epoch 600, val loss: 1.1708083152770996
Epoch 610, training loss: 0.08045975118875504 = 0.010461254045367241 + 0.01 * 6.999849319458008
Epoch 610, val loss: 1.1794519424438477
Epoch 620, training loss: 0.07990024238824844 = 0.009901873767375946 + 0.01 * 6.9998369216918945
Epoch 620, val loss: 1.187781810760498
Epoch 630, training loss: 0.07929956167936325 = 0.00938915554434061 + 0.01 * 6.9910407066345215
Epoch 630, val loss: 1.195830225944519
Epoch 640, training loss: 0.07875063270330429 = 0.008919058367609978 + 0.01 * 6.983157634735107
Epoch 640, val loss: 1.2036710977554321
Epoch 650, training loss: 0.07838743925094604 = 0.0084870345890522 + 0.01 * 6.9900407791137695
Epoch 650, val loss: 1.211178183555603
Epoch 660, training loss: 0.0778360515832901 = 0.008088159374892712 + 0.01 * 6.974789619445801
Epoch 660, val loss: 1.218444585800171
Epoch 670, training loss: 0.07758615165948868 = 0.007717932108789682 + 0.01 * 6.986822128295898
Epoch 670, val loss: 1.225593090057373
Epoch 680, training loss: 0.07709433883428574 = 0.007377365604043007 + 0.01 * 6.971697807312012
Epoch 680, val loss: 1.232462763786316
Epoch 690, training loss: 0.0766865611076355 = 0.007060348056256771 + 0.01 * 6.962621688842773
Epoch 690, val loss: 1.2391655445098877
Epoch 700, training loss: 0.07654614001512527 = 0.006765876431018114 + 0.01 * 6.978026390075684
Epoch 700, val loss: 1.2457008361816406
Epoch 710, training loss: 0.07610229402780533 = 0.0064915758557617664 + 0.01 * 6.961071491241455
Epoch 710, val loss: 1.251906156539917
Epoch 720, training loss: 0.07588520646095276 = 0.0062362514436244965 + 0.01 * 6.964895725250244
Epoch 720, val loss: 1.25809645652771
Epoch 730, training loss: 0.07547080516815186 = 0.00599741842597723 + 0.01 * 6.947339057922363
Epoch 730, val loss: 1.2639479637145996
Epoch 740, training loss: 0.07547341287136078 = 0.005773698911070824 + 0.01 * 6.969971656799316
Epoch 740, val loss: 1.269730806350708
Epoch 750, training loss: 0.07503286004066467 = 0.005565112456679344 + 0.01 * 6.946774959564209
Epoch 750, val loss: 1.2753729820251465
Epoch 760, training loss: 0.07466133683919907 = 0.005368313752114773 + 0.01 * 6.929302215576172
Epoch 760, val loss: 1.2808005809783936
Epoch 770, training loss: 0.07448694109916687 = 0.005183669738471508 + 0.01 * 6.930327892303467
Epoch 770, val loss: 1.286180853843689
Epoch 780, training loss: 0.07427296787500381 = 0.005010141059756279 + 0.01 * 6.92628288269043
Epoch 780, val loss: 1.2913157939910889
Epoch 790, training loss: 0.07402317225933075 = 0.00484645226970315 + 0.01 * 6.917672157287598
Epoch 790, val loss: 1.2963794469833374
Epoch 800, training loss: 0.07388327270746231 = 0.004691801033914089 + 0.01 * 6.91914701461792
Epoch 800, val loss: 1.3012481927871704
Epoch 810, training loss: 0.07373286783695221 = 0.004546032752841711 + 0.01 * 6.9186835289001465
Epoch 810, val loss: 1.3060486316680908
Epoch 820, training loss: 0.0734131932258606 = 0.004408183973282576 + 0.01 * 6.900501728057861
Epoch 820, val loss: 1.310621738433838
Epoch 830, training loss: 0.07333969324827194 = 0.004277598578482866 + 0.01 * 6.906209468841553
Epoch 830, val loss: 1.3151625394821167
Epoch 840, training loss: 0.07326965034008026 = 0.004153707530349493 + 0.01 * 6.911594390869141
Epoch 840, val loss: 1.3194797039031982
Epoch 850, training loss: 0.07307171076536179 = 0.00403703935444355 + 0.01 * 6.903467178344727
Epoch 850, val loss: 1.3238509893417358
Epoch 860, training loss: 0.07281468063592911 = 0.003925601486116648 + 0.01 * 6.888908386230469
Epoch 860, val loss: 1.3280045986175537
Epoch 870, training loss: 0.0730915516614914 = 0.0038197776302695274 + 0.01 * 6.927177429199219
Epoch 870, val loss: 1.3320964574813843
Epoch 880, training loss: 0.07258667796850204 = 0.0037189582362771034 + 0.01 * 6.886772155761719
Epoch 880, val loss: 1.3359721899032593
Epoch 890, training loss: 0.07261431217193604 = 0.003623157972469926 + 0.01 * 6.899115562438965
Epoch 890, val loss: 1.339884638786316
Epoch 900, training loss: 0.07240359485149384 = 0.0035317419096827507 + 0.01 * 6.887186050415039
Epoch 900, val loss: 1.3435883522033691
Epoch 910, training loss: 0.07244525849819183 = 0.0034447461366653442 + 0.01 * 6.900051593780518
Epoch 910, val loss: 1.3472976684570312
Epoch 920, training loss: 0.07211367040872574 = 0.0033617522567510605 + 0.01 * 6.875191688537598
Epoch 920, val loss: 1.350926399230957
Epoch 930, training loss: 0.07200457155704498 = 0.0032824298832565546 + 0.01 * 6.872214317321777
Epoch 930, val loss: 1.3544352054595947
Epoch 940, training loss: 0.07195334136486053 = 0.003206644207239151 + 0.01 * 6.874669551849365
Epoch 940, val loss: 1.3579250574111938
Epoch 950, training loss: 0.07195582985877991 = 0.003133938880637288 + 0.01 * 6.8821892738342285
Epoch 950, val loss: 1.361114740371704
Epoch 960, training loss: 0.07155132293701172 = 0.003064764430746436 + 0.01 * 6.848655700683594
Epoch 960, val loss: 1.364486813545227
Epoch 970, training loss: 0.07154332101345062 = 0.00299835205078125 + 0.01 * 6.854496955871582
Epoch 970, val loss: 1.367658019065857
Epoch 980, training loss: 0.07153787463903427 = 0.002934585092589259 + 0.01 * 6.860329627990723
Epoch 980, val loss: 1.3708232641220093
Epoch 990, training loss: 0.07151191681623459 = 0.0028731825295835733 + 0.01 * 6.863873481750488
Epoch 990, val loss: 1.373680830001831
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8218239325250396
The final CL Acc:0.74074, 0.01600, The final GNN Acc:0.81515, 0.01020
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13098])
remove edge: torch.Size([2, 7886])
updated graph: torch.Size([2, 10428])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0284786224365234 = 1.9425101280212402 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.9432142972946167
Epoch 10, training loss: 2.0183603763580322 = 1.9323924779891968 + 0.01 * 8.596794128417969
Epoch 10, val loss: 1.9328489303588867
Epoch 20, training loss: 2.0061933994293213 = 1.9202274084091187 + 0.01 * 8.596597671508789
Epoch 20, val loss: 1.920253038406372
Epoch 30, training loss: 1.9893943071365356 = 1.9034349918365479 + 0.01 * 8.595928192138672
Epoch 30, val loss: 1.9029202461242676
Epoch 40, training loss: 1.9649672508239746 = 1.8790442943572998 + 0.01 * 8.592292785644531
Epoch 40, val loss: 1.8783103227615356
Epoch 50, training loss: 1.9310301542282104 = 1.8453421592712402 + 0.01 * 8.568799018859863
Epoch 50, val loss: 1.8460804224014282
Epoch 60, training loss: 1.8911389112472534 = 1.8064030408859253 + 0.01 * 8.473590850830078
Epoch 60, val loss: 1.8122926950454712
Epoch 70, training loss: 1.8529114723205566 = 1.7706060409545898 + 0.01 * 8.23054027557373
Epoch 70, val loss: 1.783376932144165
Epoch 80, training loss: 1.8102991580963135 = 1.7290080785751343 + 0.01 * 8.129108428955078
Epoch 80, val loss: 1.745242953300476
Epoch 90, training loss: 1.7499765157699585 = 1.6704182624816895 + 0.01 * 7.955821514129639
Epoch 90, val loss: 1.6906285285949707
Epoch 100, training loss: 1.6703585386276245 = 1.5929641723632812 + 0.01 * 7.739441394805908
Epoch 100, val loss: 1.6210477352142334
Epoch 110, training loss: 1.5752441883087158 = 1.5001099109649658 + 0.01 * 7.513430595397949
Epoch 110, val loss: 1.5416818857192993
Epoch 120, training loss: 1.4748228788375854 = 1.4008578062057495 + 0.01 * 7.396512985229492
Epoch 120, val loss: 1.456427812576294
Epoch 130, training loss: 1.371254801750183 = 1.297716498374939 + 0.01 * 7.3538312911987305
Epoch 130, val loss: 1.3688548803329468
Epoch 140, training loss: 1.2643170356750488 = 1.191083550453186 + 0.01 * 7.32335090637207
Epoch 140, val loss: 1.2800217866897583
Epoch 150, training loss: 1.1560757160186768 = 1.0829945802688599 + 0.01 * 7.308114051818848
Epoch 150, val loss: 1.1912964582443237
Epoch 160, training loss: 1.0501996278762817 = 0.9772440195083618 + 0.01 * 7.295556545257568
Epoch 160, val loss: 1.1061856746673584
Epoch 170, training loss: 0.9503320455551147 = 0.8774612545967102 + 0.01 * 7.28708028793335
Epoch 170, val loss: 1.0267390012741089
Epoch 180, training loss: 0.8592097163200378 = 0.7864094972610474 + 0.01 * 7.280023574829102
Epoch 180, val loss: 0.9551227688789368
Epoch 190, training loss: 0.7782515287399292 = 0.705511748790741 + 0.01 * 7.273978233337402
Epoch 190, val loss: 0.8935577869415283
Epoch 200, training loss: 0.7069335579872131 = 0.634247362613678 + 0.01 * 7.268617153167725
Epoch 200, val loss: 0.8425925374031067
Epoch 210, training loss: 0.6433336138725281 = 0.5706942677497864 + 0.01 * 7.263936519622803
Epoch 210, val loss: 0.801496684551239
Epoch 220, training loss: 0.5856324434280396 = 0.513033390045166 + 0.01 * 7.2599053382873535
Epoch 220, val loss: 0.7690685987472534
Epoch 230, training loss: 0.5330075025558472 = 0.4604446589946747 + 0.01 * 7.256283283233643
Epoch 230, val loss: 0.7441637516021729
Epoch 240, training loss: 0.4850315451622009 = 0.41250374913215637 + 0.01 * 7.252780914306641
Epoch 240, val loss: 0.7255491018295288
Epoch 250, training loss: 0.4413004517555237 = 0.368807852268219 + 0.01 * 7.249259948730469
Epoch 250, val loss: 0.7121143341064453
Epoch 260, training loss: 0.40138524770736694 = 0.3289329409599304 + 0.01 * 7.2452311515808105
Epoch 260, val loss: 0.7030928730964661
Epoch 270, training loss: 0.3649612069129944 = 0.2925562858581543 + 0.01 * 7.240493297576904
Epoch 270, val loss: 0.6979018449783325
Epoch 280, training loss: 0.3317755162715912 = 0.2594328224658966 + 0.01 * 7.234269618988037
Epoch 280, val loss: 0.6959238648414612
Epoch 290, training loss: 0.30166739225387573 = 0.2293887734413147 + 0.01 * 7.2278618812561035
Epoch 290, val loss: 0.6967563033103943
Epoch 300, training loss: 0.27448177337646484 = 0.20230987668037415 + 0.01 * 7.217188358306885
Epoch 300, val loss: 0.7000670433044434
Epoch 310, training loss: 0.2501506209373474 = 0.17808908224105835 + 0.01 * 7.206155300140381
Epoch 310, val loss: 0.7056239247322083
Epoch 320, training loss: 0.22849507629871368 = 0.1565638780593872 + 0.01 * 7.193120002746582
Epoch 320, val loss: 0.7131539583206177
Epoch 330, training loss: 0.20935501158237457 = 0.13754121959209442 + 0.01 * 7.181379318237305
Epoch 330, val loss: 0.7222467064857483
Epoch 340, training loss: 0.19254399836063385 = 0.1208089217543602 + 0.01 * 7.1735076904296875
Epoch 340, val loss: 0.7326152324676514
Epoch 350, training loss: 0.17782603204250336 = 0.10616519302129745 + 0.01 * 7.166083812713623
Epoch 350, val loss: 0.7440131306648254
Epoch 360, training loss: 0.16496381163597107 = 0.09341032058000565 + 0.01 * 7.1553497314453125
Epoch 360, val loss: 0.7561081051826477
Epoch 370, training loss: 0.15385720133781433 = 0.08234459161758423 + 0.01 * 7.1512603759765625
Epoch 370, val loss: 0.7687328457832336
Epoch 380, training loss: 0.14424261450767517 = 0.07277390360832214 + 0.01 * 7.146872043609619
Epoch 380, val loss: 0.7816441655158997
Epoch 390, training loss: 0.13593125343322754 = 0.06451400369405746 + 0.01 * 7.141726016998291
Epoch 390, val loss: 0.7947216629981995
Epoch 400, training loss: 0.12875092029571533 = 0.057386551052331924 + 0.01 * 7.13643741607666
Epoch 400, val loss: 0.8077600598335266
Epoch 410, training loss: 0.12257736921310425 = 0.05123044550418854 + 0.01 * 7.134692668914795
Epoch 410, val loss: 0.8207183480262756
Epoch 420, training loss: 0.11722423136234283 = 0.045904260128736496 + 0.01 * 7.131997585296631
Epoch 420, val loss: 0.8335369229316711
Epoch 430, training loss: 0.11252780258655548 = 0.041283853352069855 + 0.01 * 7.124395370483398
Epoch 430, val loss: 0.846113383769989
Epoch 440, training loss: 0.10847301781177521 = 0.03726488724350929 + 0.01 * 7.120813846588135
Epoch 440, val loss: 0.8585183620452881
Epoch 450, training loss: 0.10494823008775711 = 0.03375854343175888 + 0.01 * 7.118968963623047
Epoch 450, val loss: 0.8706919550895691
Epoch 460, training loss: 0.10181931406259537 = 0.030686601996421814 + 0.01 * 7.113271236419678
Epoch 460, val loss: 0.8826196789741516
Epoch 470, training loss: 0.09906098246574402 = 0.02798553556203842 + 0.01 * 7.107544898986816
Epoch 470, val loss: 0.8942850828170776
Epoch 480, training loss: 0.09665067493915558 = 0.025605881586670876 + 0.01 * 7.104479789733887
Epoch 480, val loss: 0.9057452082633972
Epoch 490, training loss: 0.09451622515916824 = 0.02350412867963314 + 0.01 * 7.10120964050293
Epoch 490, val loss: 0.9168630838394165
Epoch 500, training loss: 0.09258495271205902 = 0.02163832075893879 + 0.01 * 7.094663619995117
Epoch 500, val loss: 0.9277303814888
Epoch 510, training loss: 0.09091770648956299 = 0.019974973052740097 + 0.01 * 7.094274044036865
Epoch 510, val loss: 0.938269317150116
Epoch 520, training loss: 0.08934523910284042 = 0.01848992146551609 + 0.01 * 7.085532188415527
Epoch 520, val loss: 0.9485968947410583
Epoch 530, training loss: 0.08809461444616318 = 0.01715843565762043 + 0.01 * 7.093617916107178
Epoch 530, val loss: 0.9586460590362549
Epoch 540, training loss: 0.08677497506141663 = 0.015963204205036163 + 0.01 * 7.081177234649658
Epoch 540, val loss: 0.9683425426483154
Epoch 550, training loss: 0.08561110496520996 = 0.014887171797454357 + 0.01 * 7.072393894195557
Epoch 550, val loss: 0.9777336120605469
Epoch 560, training loss: 0.0846572294831276 = 0.01391341257840395 + 0.01 * 7.0743818283081055
Epoch 560, val loss: 0.9868977665901184
Epoch 570, training loss: 0.08369948714971542 = 0.013032256625592709 + 0.01 * 7.066723346710205
Epoch 570, val loss: 0.9957792162895203
Epoch 580, training loss: 0.08280263096094131 = 0.0122305853292346 + 0.01 * 7.057204723358154
Epoch 580, val loss: 1.00444495677948
Epoch 590, training loss: 0.0821043998003006 = 0.011498858220875263 + 0.01 * 7.060554027557373
Epoch 590, val loss: 1.012845754623413
Epoch 600, training loss: 0.08136256039142609 = 0.010832791216671467 + 0.01 * 7.052976608276367
Epoch 600, val loss: 1.0208969116210938
Epoch 610, training loss: 0.08064670860767365 = 0.010223973542451859 + 0.01 * 7.04227352142334
Epoch 610, val loss: 1.02872896194458
Epoch 620, training loss: 0.08002280443906784 = 0.009666471742093563 + 0.01 * 7.035633563995361
Epoch 620, val loss: 1.0362977981567383
Epoch 630, training loss: 0.079596608877182 = 0.009153545834124088 + 0.01 * 7.04430627822876
Epoch 630, val loss: 1.043623447418213
Epoch 640, training loss: 0.07900340110063553 = 0.008681736886501312 + 0.01 * 7.032166481018066
Epoch 640, val loss: 1.0507451295852661
Epoch 650, training loss: 0.07851088792085648 = 0.00824726838618517 + 0.01 * 7.02636194229126
Epoch 650, val loss: 1.0576814413070679
Epoch 660, training loss: 0.07815506309270859 = 0.007845553569495678 + 0.01 * 7.030951499938965
Epoch 660, val loss: 1.064380168914795
Epoch 670, training loss: 0.07758380472660065 = 0.0074745784513652325 + 0.01 * 7.010922431945801
Epoch 670, val loss: 1.0709244012832642
Epoch 680, training loss: 0.07724261283874512 = 0.007130911108106375 + 0.01 * 7.011170387268066
Epoch 680, val loss: 1.077242136001587
Epoch 690, training loss: 0.07691172510385513 = 0.006812631152570248 + 0.01 * 7.009909629821777
Epoch 690, val loss: 1.0832833051681519
Epoch 700, training loss: 0.0765337198972702 = 0.006516230292618275 + 0.01 * 7.001749038696289
Epoch 700, val loss: 1.0892966985702515
Epoch 710, training loss: 0.07631367444992065 = 0.006240055896341801 + 0.01 * 7.007362365722656
Epoch 710, val loss: 1.0950491428375244
Epoch 720, training loss: 0.07573804259300232 = 0.005984114017337561 + 0.01 * 6.975393295288086
Epoch 720, val loss: 1.100597858428955
Epoch 730, training loss: 0.07558932155370712 = 0.005744690541177988 + 0.01 * 6.984463691711426
Epoch 730, val loss: 1.1061080694198608
Epoch 740, training loss: 0.07533961534500122 = 0.005521188955754042 + 0.01 * 6.9818434715271
Epoch 740, val loss: 1.1112903356552124
Epoch 750, training loss: 0.07523450255393982 = 0.005312376655638218 + 0.01 * 6.992212772369385
Epoch 750, val loss: 1.1165319681167603
Epoch 760, training loss: 0.07480297237634659 = 0.005117257125675678 + 0.01 * 6.968571186065674
Epoch 760, val loss: 1.1213330030441284
Epoch 770, training loss: 0.07451018691062927 = 0.004934384487569332 + 0.01 * 6.95758056640625
Epoch 770, val loss: 1.1262871026992798
Epoch 780, training loss: 0.07412797212600708 = 0.004762323573231697 + 0.01 * 6.936564922332764
Epoch 780, val loss: 1.1308997869491577
Epoch 790, training loss: 0.07412447035312653 = 0.004600341431796551 + 0.01 * 6.952413082122803
Epoch 790, val loss: 1.1354814767837524
Epoch 800, training loss: 0.07360775023698807 = 0.004447943530976772 + 0.01 * 6.915980815887451
Epoch 800, val loss: 1.1398285627365112
Epoch 810, training loss: 0.07355276495218277 = 0.004304153379052877 + 0.01 * 6.924861431121826
Epoch 810, val loss: 1.1442368030548096
Epoch 820, training loss: 0.07348907738924026 = 0.00416893046349287 + 0.01 * 6.9320149421691895
Epoch 820, val loss: 1.1482964754104614
Epoch 830, training loss: 0.07305972278118134 = 0.004040832165628672 + 0.01 * 6.901889801025391
Epoch 830, val loss: 1.152377724647522
Epoch 840, training loss: 0.07286977767944336 = 0.003919688984751701 + 0.01 * 6.895008563995361
Epoch 840, val loss: 1.1562637090682983
Epoch 850, training loss: 0.07273496687412262 = 0.0038048499263823032 + 0.01 * 6.893012046813965
Epoch 850, val loss: 1.1600558757781982
Epoch 860, training loss: 0.07267488539218903 = 0.0036956893745809793 + 0.01 * 6.897919654846191
Epoch 860, val loss: 1.1637450456619263
Epoch 870, training loss: 0.0724649727344513 = 0.003592605236917734 + 0.01 * 6.887237071990967
Epoch 870, val loss: 1.1672559976577759
Epoch 880, training loss: 0.07248418033123016 = 0.0034942857455462217 + 0.01 * 6.898989677429199
Epoch 880, val loss: 1.170850396156311
Epoch 890, training loss: 0.07213682681322098 = 0.0034008559305220842 + 0.01 * 6.873597621917725
Epoch 890, val loss: 1.1741749048233032
Epoch 900, training loss: 0.0721127986907959 = 0.0033116170670837164 + 0.01 * 6.880118370056152
Epoch 900, val loss: 1.177527904510498
Epoch 910, training loss: 0.07200711220502853 = 0.0032266441266983747 + 0.01 * 6.87804651260376
Epoch 910, val loss: 1.1807218790054321
Epoch 920, training loss: 0.07195893675088882 = 0.0031459289602935314 + 0.01 * 6.881301403045654
Epoch 920, val loss: 1.1839054822921753
Epoch 930, training loss: 0.07184307277202606 = 0.0030687986873090267 + 0.01 * 6.877427577972412
Epoch 930, val loss: 1.1869019269943237
Epoch 940, training loss: 0.07159373909235 = 0.0029952446930110455 + 0.01 * 6.859849452972412
Epoch 940, val loss: 1.1899043321609497
Epoch 950, training loss: 0.07165704667568207 = 0.0029251431114971638 + 0.01 * 6.873190879821777
Epoch 950, val loss: 1.192779302597046
Epoch 960, training loss: 0.0713362991809845 = 0.002858019433915615 + 0.01 * 6.847827911376953
Epoch 960, val loss: 1.1956558227539062
Epoch 970, training loss: 0.07141897082328796 = 0.002793616848066449 + 0.01 * 6.86253547668457
Epoch 970, val loss: 1.1984494924545288
Epoch 980, training loss: 0.07122772186994553 = 0.002732104854658246 + 0.01 * 6.84956169128418
Epoch 980, val loss: 1.2010738849639893
Epoch 990, training loss: 0.07100176066160202 = 0.0026729688979685307 + 0.01 * 6.832879066467285
Epoch 990, val loss: 1.2037713527679443
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.0349090099334717 = 1.9489405155181885 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.9435815811157227
Epoch 10, training loss: 2.0242419242858887 = 1.9382741451263428 + 0.01 * 8.59677505493164
Epoch 10, val loss: 1.933053731918335
Epoch 20, training loss: 2.01107120513916 = 1.9251059293746948 + 0.01 * 8.596517562866211
Epoch 20, val loss: 1.9196285009384155
Epoch 30, training loss: 1.99274480342865 = 1.9067888259887695 + 0.01 * 8.595593452453613
Epoch 30, val loss: 1.900575041770935
Epoch 40, training loss: 1.9660499095916748 = 1.8801478147506714 + 0.01 * 8.590211868286133
Epoch 40, val loss: 1.8729093074798584
Epoch 50, training loss: 1.9289238452911377 = 1.843354344367981 + 0.01 * 8.556944847106934
Epoch 50, val loss: 1.836215853691101
Epoch 60, training loss: 1.885582447052002 = 1.8013674020767212 + 0.01 * 8.42150592803955
Epoch 60, val loss: 1.7986195087432861
Epoch 70, training loss: 1.8449339866638184 = 1.7629516124725342 + 0.01 * 8.198241233825684
Epoch 70, val loss: 1.7686805725097656
Epoch 80, training loss: 1.7974730730056763 = 1.7173064947128296 + 0.01 * 8.016657829284668
Epoch 80, val loss: 1.7306467294692993
Epoch 90, training loss: 1.7310067415237427 = 1.6536567211151123 + 0.01 * 7.7349982261657715
Epoch 90, val loss: 1.6736116409301758
Epoch 100, training loss: 1.644586443901062 = 1.5686458349227905 + 0.01 * 7.594058513641357
Epoch 100, val loss: 1.5984967947006226
Epoch 110, training loss: 1.5393991470336914 = 1.4639923572540283 + 0.01 * 7.540683746337891
Epoch 110, val loss: 1.5089565515518188
Epoch 120, training loss: 1.4234381914138794 = 1.3484745025634766 + 0.01 * 7.496368408203125
Epoch 120, val loss: 1.411226511001587
Epoch 130, training loss: 1.305901050567627 = 1.2314062118530273 + 0.01 * 7.44948673248291
Epoch 130, val loss: 1.314870834350586
Epoch 140, training loss: 1.1934548616409302 = 1.1194344758987427 + 0.01 * 7.402037143707275
Epoch 140, val loss: 1.22393000125885
Epoch 150, training loss: 1.0900713205337524 = 1.016356110572815 + 0.01 * 7.371520519256592
Epoch 150, val loss: 1.1406925916671753
Epoch 160, training loss: 0.9965920448303223 = 0.9230550527572632 + 0.01 * 7.353701591491699
Epoch 160, val loss: 1.0655397176742554
Epoch 170, training loss: 0.9127981662750244 = 0.839404821395874 + 0.01 * 7.339337348937988
Epoch 170, val loss: 0.9984807968139648
Epoch 180, training loss: 0.8383108377456665 = 0.7650220990180969 + 0.01 * 7.3288750648498535
Epoch 180, val loss: 0.9396421313285828
Epoch 190, training loss: 0.7721513509750366 = 0.6989403963088989 + 0.01 * 7.321094036102295
Epoch 190, val loss: 0.8887069821357727
Epoch 200, training loss: 0.7125310301780701 = 0.6393926739692688 + 0.01 * 7.313837051391602
Epoch 200, val loss: 0.8450669050216675
Epoch 210, training loss: 0.657672107219696 = 0.5846051573753357 + 0.01 * 7.306694030761719
Epoch 210, val loss: 0.8079801201820374
Epoch 220, training loss: 0.6063094735145569 = 0.5333133935928345 + 0.01 * 7.299610614776611
Epoch 220, val loss: 0.776672899723053
Epoch 230, training loss: 0.5579887628555298 = 0.485061913728714 + 0.01 * 7.292684555053711
Epoch 230, val loss: 0.7502355575561523
Epoch 240, training loss: 0.5127730965614319 = 0.43990883231163025 + 0.01 * 7.2864274978637695
Epoch 240, val loss: 0.728245735168457
Epoch 250, training loss: 0.470801442861557 = 0.3980051875114441 + 0.01 * 7.279625415802002
Epoch 250, val loss: 0.7105923891067505
Epoch 260, training loss: 0.43214139342308044 = 0.35941314697265625 + 0.01 * 7.272825241088867
Epoch 260, val loss: 0.6972525715827942
Epoch 270, training loss: 0.39664027094841003 = 0.3239823281764984 + 0.01 * 7.2657952308654785
Epoch 270, val loss: 0.687794029712677
Epoch 280, training loss: 0.3638302683830261 = 0.2912576198577881 + 0.01 * 7.257266521453857
Epoch 280, val loss: 0.6817435026168823
Epoch 290, training loss: 0.33301177620887756 = 0.26052841544151306 + 0.01 * 7.248335361480713
Epoch 290, val loss: 0.6784508228302002
Epoch 300, training loss: 0.3036031126976013 = 0.23125500977039337 + 0.01 * 7.234809875488281
Epoch 300, val loss: 0.6772512793540955
Epoch 310, training loss: 0.2756800651550293 = 0.20348909497261047 + 0.01 * 7.21909761428833
Epoch 310, val loss: 0.6779109239578247
Epoch 320, training loss: 0.24989163875579834 = 0.17783838510513306 + 0.01 * 7.205326080322266
Epoch 320, val loss: 0.680630624294281
Epoch 330, training loss: 0.22680515050888062 = 0.1549854278564453 + 0.01 * 7.181972026824951
Epoch 330, val loss: 0.6855648756027222
Epoch 340, training loss: 0.2068444788455963 = 0.13516440987586975 + 0.01 * 7.16800594329834
Epoch 340, val loss: 0.6928149461746216
Epoch 350, training loss: 0.18989834189414978 = 0.11820396035909653 + 0.01 * 7.169438362121582
Epoch 350, val loss: 0.7021391987800598
Epoch 360, training loss: 0.1751917004585266 = 0.10371238738298416 + 0.01 * 7.147932529449463
Epoch 360, val loss: 0.7131434679031372
Epoch 370, training loss: 0.1625477373600006 = 0.0912787988781929 + 0.01 * 7.126893997192383
Epoch 370, val loss: 0.7254074215888977
Epoch 380, training loss: 0.151760533452034 = 0.08057572692632675 + 0.01 * 7.118481159210205
Epoch 380, val loss: 0.7386190295219421
Epoch 390, training loss: 0.14242234826087952 = 0.07134562730789185 + 0.01 * 7.107672691345215
Epoch 390, val loss: 0.7524789571762085
Epoch 400, training loss: 0.1343649923801422 = 0.06338173896074295 + 0.01 * 7.098325252532959
Epoch 400, val loss: 0.7667099833488464
Epoch 410, training loss: 0.12742507457733154 = 0.05649752542376518 + 0.01 * 7.0927557945251465
Epoch 410, val loss: 0.7811272740364075
Epoch 420, training loss: 0.12138722836971283 = 0.050545696169137955 + 0.01 * 7.084153652191162
Epoch 420, val loss: 0.7955865859985352
Epoch 430, training loss: 0.11617523431777954 = 0.04538986086845398 + 0.01 * 7.078537464141846
Epoch 430, val loss: 0.8101316094398499
Epoch 440, training loss: 0.11139386892318726 = 0.04091542586684227 + 0.01 * 7.047844409942627
Epoch 440, val loss: 0.8245803117752075
Epoch 450, training loss: 0.10760627686977386 = 0.03701862320303917 + 0.01 * 7.058765888214111
Epoch 450, val loss: 0.8389448523521423
Epoch 460, training loss: 0.10389989614486694 = 0.03361721709370613 + 0.01 * 7.028268337249756
Epoch 460, val loss: 0.8529934287071228
Epoch 470, training loss: 0.10093710571527481 = 0.03063717670738697 + 0.01 * 7.029993534088135
Epoch 470, val loss: 0.8668041825294495
Epoch 480, training loss: 0.09814748167991638 = 0.028018813580274582 + 0.01 * 7.012867450714111
Epoch 480, val loss: 0.8803987503051758
Epoch 490, training loss: 0.09577814489603043 = 0.025709077715873718 + 0.01 * 7.006906509399414
Epoch 490, val loss: 0.8935943245887756
Epoch 500, training loss: 0.09355784207582474 = 0.02366541139781475 + 0.01 * 6.989243030548096
Epoch 500, val loss: 0.9064372181892395
Epoch 510, training loss: 0.09179312735795975 = 0.02185072936117649 + 0.01 * 6.994239807128906
Epoch 510, val loss: 0.9188989400863647
Epoch 520, training loss: 0.08995414525270462 = 0.020234480500221252 + 0.01 * 6.971966743469238
Epoch 520, val loss: 0.9310395121574402
Epoch 530, training loss: 0.08846386522054672 = 0.01879057288169861 + 0.01 * 6.967329025268555
Epoch 530, val loss: 0.9427831768989563
Epoch 540, training loss: 0.08712952584028244 = 0.017495976760983467 + 0.01 * 6.96335506439209
Epoch 540, val loss: 0.9542691707611084
Epoch 550, training loss: 0.08584495633840561 = 0.016332758590579033 + 0.01 * 6.95121955871582
Epoch 550, val loss: 0.9652292728424072
Epoch 560, training loss: 0.084559366106987 = 0.01528378389775753 + 0.01 * 6.927558422088623
Epoch 560, val loss: 0.9760127067565918
Epoch 570, training loss: 0.08362516015768051 = 0.014334137551486492 + 0.01 * 6.929102420806885
Epoch 570, val loss: 0.9864558577537537
Epoch 580, training loss: 0.08271471410989761 = 0.01347365416586399 + 0.01 * 6.924106597900391
Epoch 580, val loss: 0.9964562058448792
Epoch 590, training loss: 0.08198674023151398 = 0.012691660784184933 + 0.01 * 6.929507732391357
Epoch 590, val loss: 1.0062522888183594
Epoch 600, training loss: 0.08098901808261871 = 0.011978782713413239 + 0.01 * 6.901023864746094
Epoch 600, val loss: 1.015635371208191
Epoch 610, training loss: 0.08037643879652023 = 0.011327182874083519 + 0.01 * 6.90492582321167
Epoch 610, val loss: 1.0248744487762451
Epoch 620, training loss: 0.07968994975090027 = 0.010730449110269547 + 0.01 * 6.8959503173828125
Epoch 620, val loss: 1.033760666847229
Epoch 630, training loss: 0.07906746864318848 = 0.010182150639593601 + 0.01 * 6.888531684875488
Epoch 630, val loss: 1.0423660278320312
Epoch 640, training loss: 0.07849639654159546 = 0.009677164256572723 + 0.01 * 6.881923675537109
Epoch 640, val loss: 1.0507687330245972
Epoch 650, training loss: 0.07793014496564865 = 0.009211565367877483 + 0.01 * 6.8718581199646
Epoch 650, val loss: 1.058897852897644
Epoch 660, training loss: 0.07745502889156342 = 0.008780989795923233 + 0.01 * 6.867403507232666
Epoch 660, val loss: 1.0668015480041504
Epoch 670, training loss: 0.07691364735364914 = 0.008382433094084263 + 0.01 * 6.853121757507324
Epoch 670, val loss: 1.0744764804840088
Epoch 680, training loss: 0.07648584991693497 = 0.008012437261641026 + 0.01 * 6.847341537475586
Epoch 680, val loss: 1.081892967224121
Epoch 690, training loss: 0.07615911960601807 = 0.007668337319046259 + 0.01 * 6.849078178405762
Epoch 690, val loss: 1.0891591310501099
Epoch 700, training loss: 0.07573874294757843 = 0.007348066661506891 + 0.01 * 6.839067459106445
Epoch 700, val loss: 1.0961487293243408
Epoch 710, training loss: 0.0753321647644043 = 0.007049188949167728 + 0.01 * 6.828298091888428
Epoch 710, val loss: 1.1030751466751099
Epoch 720, training loss: 0.07510188221931458 = 0.0067698173224925995 + 0.01 * 6.833206653594971
Epoch 720, val loss: 1.1096724271774292
Epoch 730, training loss: 0.0748051106929779 = 0.0065087489783763885 + 0.01 * 6.829636573791504
Epoch 730, val loss: 1.1162388324737549
Epoch 740, training loss: 0.0743851587176323 = 0.006264299154281616 + 0.01 * 6.81208610534668
Epoch 740, val loss: 1.1225025653839111
Epoch 750, training loss: 0.07423708587884903 = 0.006034657824784517 + 0.01 * 6.8202433586120605
Epoch 750, val loss: 1.1285955905914307
Epoch 760, training loss: 0.07390034943819046 = 0.005818815436214209 + 0.01 * 6.8081536293029785
Epoch 760, val loss: 1.1346211433410645
Epoch 770, training loss: 0.07365269213914871 = 0.005615512374788523 + 0.01 * 6.803718090057373
Epoch 770, val loss: 1.1404615640640259
Epoch 780, training loss: 0.07363159954547882 = 0.00542406365275383 + 0.01 * 6.820753574371338
Epoch 780, val loss: 1.1462020874023438
Epoch 790, training loss: 0.07331530749797821 = 0.005243781954050064 + 0.01 * 6.80715274810791
Epoch 790, val loss: 1.1516764163970947
Epoch 800, training loss: 0.07303731888532639 = 0.005073311273008585 + 0.01 * 6.796401500701904
Epoch 800, val loss: 1.157148838043213
Epoch 810, training loss: 0.0727401077747345 = 0.004912086762487888 + 0.01 * 6.782802104949951
Epoch 810, val loss: 1.1624369621276855
Epoch 820, training loss: 0.07286445051431656 = 0.004759472329169512 + 0.01 * 6.810497760772705
Epoch 820, val loss: 1.167648196220398
Epoch 830, training loss: 0.07250689715147018 = 0.004614946432411671 + 0.01 * 6.7891950607299805
Epoch 830, val loss: 1.1726465225219727
Epoch 840, training loss: 0.07250884920358658 = 0.004477859009057283 + 0.01 * 6.803099155426025
Epoch 840, val loss: 1.1776281595230103
Epoch 850, training loss: 0.07207293808460236 = 0.00434782775118947 + 0.01 * 6.772510528564453
Epoch 850, val loss: 1.1824185848236084
Epoch 860, training loss: 0.07201121747493744 = 0.004224243573844433 + 0.01 * 6.778697967529297
Epoch 860, val loss: 1.1871628761291504
Epoch 870, training loss: 0.07188016176223755 = 0.004106797277927399 + 0.01 * 6.777336597442627
Epoch 870, val loss: 1.191692590713501
Epoch 880, training loss: 0.07172567397356033 = 0.0039950269274413586 + 0.01 * 6.773065090179443
Epoch 880, val loss: 1.1962491273880005
Epoch 890, training loss: 0.07162472605705261 = 0.003888496896252036 + 0.01 * 6.773623466491699
Epoch 890, val loss: 1.2005786895751953
Epoch 900, training loss: 0.07139664143323898 = 0.003786978544667363 + 0.01 * 6.7609663009643555
Epoch 900, val loss: 1.204894781112671
Epoch 910, training loss: 0.07136325538158417 = 0.0036900367122143507 + 0.01 * 6.767322063446045
Epoch 910, val loss: 1.2091096639633179
Epoch 920, training loss: 0.0710994079709053 = 0.0035975861828774214 + 0.01 * 6.750182151794434
Epoch 920, val loss: 1.213198184967041
Epoch 930, training loss: 0.07109835743904114 = 0.0035091699101030827 + 0.01 * 6.758918762207031
Epoch 930, val loss: 1.217241644859314
Epoch 940, training loss: 0.07102560997009277 = 0.003424672409892082 + 0.01 * 6.760093688964844
Epoch 940, val loss: 1.221178650856018
Epoch 950, training loss: 0.07086075842380524 = 0.0033438103273510933 + 0.01 * 6.751694679260254
Epoch 950, val loss: 1.2250512838363647
Epoch 960, training loss: 0.07089104503393173 = 0.0032664737664163113 + 0.01 * 6.762457370758057
Epoch 960, val loss: 1.2288655042648315
Epoch 970, training loss: 0.07065477222204208 = 0.0031922697089612484 + 0.01 * 6.746250629425049
Epoch 970, val loss: 1.232483983039856
Epoch 980, training loss: 0.07058361917734146 = 0.0031211618334054947 + 0.01 * 6.746246337890625
Epoch 980, val loss: 1.2361719608306885
Epoch 990, training loss: 0.07047457993030548 = 0.003053051419556141 + 0.01 * 6.742152690887451
Epoch 990, val loss: 1.2396918535232544
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.0447394847869873 = 1.958770751953125 + 0.01 * 8.5968656539917
Epoch 0, val loss: 1.955796480178833
Epoch 10, training loss: 2.0337564945220947 = 1.9477884769439697 + 0.01 * 8.59681224822998
Epoch 10, val loss: 1.9454827308654785
Epoch 20, training loss: 2.020411252975464 = 1.9344449043273926 + 0.01 * 8.596632957458496
Epoch 20, val loss: 1.9325506687164307
Epoch 30, training loss: 2.002037763595581 = 1.9160774946212769 + 0.01 * 8.596028327941895
Epoch 30, val loss: 1.9145543575286865
Epoch 40, training loss: 1.975474238395691 = 1.889549732208252 + 0.01 * 8.592453002929688
Epoch 40, val loss: 1.8888996839523315
Epoch 50, training loss: 1.9388166666030884 = 1.853156328201294 + 0.01 * 8.566034317016602
Epoch 50, val loss: 1.855272889137268
Epoch 60, training loss: 1.896704077720642 = 1.8122926950454712 + 0.01 * 8.441137313842773
Epoch 60, val loss: 1.820940375328064
Epoch 70, training loss: 1.860800862312317 = 1.7779009342193604 + 0.01 * 8.289992332458496
Epoch 70, val loss: 1.7931371927261353
Epoch 80, training loss: 1.8217657804489136 = 1.7399927377700806 + 0.01 * 8.177305221557617
Epoch 80, val loss: 1.7567270994186401
Epoch 90, training loss: 1.765880823135376 = 1.6856045722961426 + 0.01 * 8.027628898620605
Epoch 90, val loss: 1.70624840259552
Epoch 100, training loss: 1.6905320882797241 = 1.611382246017456 + 0.01 * 7.914984703063965
Epoch 100, val loss: 1.6415351629257202
Epoch 110, training loss: 1.6007922887802124 = 1.5230151414871216 + 0.01 * 7.777715682983398
Epoch 110, val loss: 1.5693085193634033
Epoch 120, training loss: 1.5112351179122925 = 1.4362881183624268 + 0.01 * 7.494703769683838
Epoch 120, val loss: 1.499741792678833
Epoch 130, training loss: 1.4316928386688232 = 1.3577982187271118 + 0.01 * 7.38946008682251
Epoch 130, val loss: 1.4376870393753052
Epoch 140, training loss: 1.3565882444381714 = 1.2831937074661255 + 0.01 * 7.339457035064697
Epoch 140, val loss: 1.3788084983825684
Epoch 150, training loss: 1.2807307243347168 = 1.2076237201690674 + 0.01 * 7.310702323913574
Epoch 150, val loss: 1.3187285661697388
Epoch 160, training loss: 1.2030423879623413 = 1.1300642490386963 + 0.01 * 7.297809600830078
Epoch 160, val loss: 1.2575504779815674
Epoch 170, training loss: 1.1240488290786743 = 1.0511317253112793 + 0.01 * 7.291711330413818
Epoch 170, val loss: 1.1959569454193115
Epoch 180, training loss: 1.0450323820114136 = 0.9721912145614624 + 0.01 * 7.284121036529541
Epoch 180, val loss: 1.1342238187789917
Epoch 190, training loss: 0.9675804376602173 = 0.8948583006858826 + 0.01 * 7.272213935852051
Epoch 190, val loss: 1.073290467262268
Epoch 200, training loss: 0.893610954284668 = 0.8210896253585815 + 0.01 * 7.25213623046875
Epoch 200, val loss: 1.014391541481018
Epoch 210, training loss: 0.8250395059585571 = 0.7528052926063538 + 0.01 * 7.223423480987549
Epoch 210, val loss: 0.9597663283348083
Epoch 220, training loss: 0.763169527053833 = 0.6912087202072144 + 0.01 * 7.196079254150391
Epoch 220, val loss: 0.9110268354415894
Epoch 230, training loss: 0.7073779106140137 = 0.6356121897697449 + 0.01 * 7.176570892333984
Epoch 230, val loss: 0.8683942556381226
Epoch 240, training loss: 0.6551821231842041 = 0.5835649967193604 + 0.01 * 7.161712646484375
Epoch 240, val loss: 0.829908549785614
Epoch 250, training loss: 0.603762149810791 = 0.5322123765945435 + 0.01 * 7.154978275299072
Epoch 250, val loss: 0.79358971118927
Epoch 260, training loss: 0.5513404607772827 = 0.47986048460006714 + 0.01 * 7.14799690246582
Epoch 260, val loss: 0.7588157653808594
Epoch 270, training loss: 0.49834153056144714 = 0.42691171169281006 + 0.01 * 7.142982006072998
Epoch 270, val loss: 0.7265181541442871
Epoch 280, training loss: 0.44677168130874634 = 0.3753895163536072 + 0.01 * 7.138216495513916
Epoch 280, val loss: 0.6989965438842773
Epoch 290, training loss: 0.3987525701522827 = 0.32740306854248047 + 0.01 * 7.134950160980225
Epoch 290, val loss: 0.6775070428848267
Epoch 300, training loss: 0.3555251955986023 = 0.2842547297477722 + 0.01 * 7.127045154571533
Epoch 300, val loss: 0.6625374555587769
Epoch 310, training loss: 0.3175275921821594 = 0.24631360173225403 + 0.01 * 7.121400833129883
Epoch 310, val loss: 0.6533363461494446
Epoch 320, training loss: 0.2845380902290344 = 0.21337223052978516 + 0.01 * 7.116587162017822
Epoch 320, val loss: 0.6491733193397522
Epoch 330, training loss: 0.25621551275253296 = 0.18503083288669586 + 0.01 * 7.118466854095459
Epoch 330, val loss: 0.6489046216011047
Epoch 340, training loss: 0.23187702894210815 = 0.16079463064670563 + 0.01 * 7.108239650726318
Epoch 340, val loss: 0.6518815159797668
Epoch 350, training loss: 0.21113428473472595 = 0.14012303948402405 + 0.01 * 7.101125240325928
Epoch 350, val loss: 0.657348096370697
Epoch 360, training loss: 0.19347378611564636 = 0.12251348048448563 + 0.01 * 7.096029758453369
Epoch 360, val loss: 0.6647094488143921
Epoch 370, training loss: 0.17840133607387543 = 0.1074916273355484 + 0.01 * 7.090970993041992
Epoch 370, val loss: 0.6734828948974609
Epoch 380, training loss: 0.16561686992645264 = 0.09464719891548157 + 0.01 * 7.096966743469238
Epoch 380, val loss: 0.6832510232925415
Epoch 390, training loss: 0.1545017957687378 = 0.08365720510482788 + 0.01 * 7.084460258483887
Epoch 390, val loss: 0.6937001347541809
Epoch 400, training loss: 0.14500945806503296 = 0.07421544939279556 + 0.01 * 7.07940149307251
Epoch 400, val loss: 0.7046067118644714
Epoch 410, training loss: 0.13679751753807068 = 0.06606197357177734 + 0.01 * 7.073554515838623
Epoch 410, val loss: 0.7157974243164062
Epoch 420, training loss: 0.12972012162208557 = 0.05898672342300415 + 0.01 * 7.073340892791748
Epoch 420, val loss: 0.7270635962486267
Epoch 430, training loss: 0.12348096072673798 = 0.052824947983026505 + 0.01 * 7.065601825714111
Epoch 430, val loss: 0.7383436560630798
Epoch 440, training loss: 0.11805335432291031 = 0.04743900150060654 + 0.01 * 7.061435222625732
Epoch 440, val loss: 0.7494924068450928
Epoch 450, training loss: 0.11339106410741806 = 0.04271937161684036 + 0.01 * 7.067169666290283
Epoch 450, val loss: 0.7604691982269287
Epoch 460, training loss: 0.10912926495075226 = 0.038578055799007416 + 0.01 * 7.055121421813965
Epoch 460, val loss: 0.771335780620575
Epoch 470, training loss: 0.10545012354850769 = 0.03493603691458702 + 0.01 * 7.051408767700195
Epoch 470, val loss: 0.7819910645484924
Epoch 480, training loss: 0.10223051905632019 = 0.03172894939780235 + 0.01 * 7.050156593322754
Epoch 480, val loss: 0.7924962639808655
Epoch 490, training loss: 0.09931885451078415 = 0.0289045050740242 + 0.01 * 7.041435241699219
Epoch 490, val loss: 0.8028712272644043
Epoch 500, training loss: 0.09678031504154205 = 0.026412740349769592 + 0.01 * 7.036757946014404
Epoch 500, val loss: 0.8130513429641724
Epoch 510, training loss: 0.09461577981710434 = 0.024209508672356606 + 0.01 * 7.040627479553223
Epoch 510, val loss: 0.8230459690093994
Epoch 520, training loss: 0.09262864291667938 = 0.022260714322328568 + 0.01 * 7.036793231964111
Epoch 520, val loss: 0.8329287767410278
Epoch 530, training loss: 0.09081043303012848 = 0.020531591027975082 + 0.01 * 7.027884483337402
Epoch 530, val loss: 0.842585027217865
Epoch 540, training loss: 0.08921326696872711 = 0.01899142935872078 + 0.01 * 7.022183895111084
Epoch 540, val loss: 0.8520229458808899
Epoch 550, training loss: 0.08791423588991165 = 0.017613932490348816 + 0.01 * 7.030030727386475
Epoch 550, val loss: 0.8613119721412659
Epoch 560, training loss: 0.08654077351093292 = 0.016377069056034088 + 0.01 * 7.01637077331543
Epoch 560, val loss: 0.8703683614730835
Epoch 570, training loss: 0.0853554829955101 = 0.015260474756360054 + 0.01 * 7.0095014572143555
Epoch 570, val loss: 0.8792980909347534
Epoch 580, training loss: 0.08427929133176804 = 0.014247818849980831 + 0.01 * 7.003147125244141
Epoch 580, val loss: 0.888103723526001
Epoch 590, training loss: 0.08337242901325226 = 0.013327048160135746 + 0.01 * 7.004538059234619
Epoch 590, val loss: 0.8967269062995911
Epoch 600, training loss: 0.08245642483234406 = 0.012486312538385391 + 0.01 * 6.997011184692383
Epoch 600, val loss: 0.9053087830543518
Epoch 610, training loss: 0.08165262639522552 = 0.011717166751623154 + 0.01 * 6.993546009063721
Epoch 610, val loss: 0.9137127995491028
Epoch 620, training loss: 0.08088073879480362 = 0.011012928560376167 + 0.01 * 6.986781120300293
Epoch 620, val loss: 0.9219828248023987
Epoch 630, training loss: 0.0805078074336052 = 0.010367774404585361 + 0.01 * 7.014003276824951
Epoch 630, val loss: 0.9300456643104553
Epoch 640, training loss: 0.07960566878318787 = 0.00977927166968584 + 0.01 * 6.982639312744141
Epoch 640, val loss: 0.937897801399231
Epoch 650, training loss: 0.07903165370225906 = 0.009239126928150654 + 0.01 * 6.979252815246582
Epoch 650, val loss: 0.9456254839897156
Epoch 660, training loss: 0.07845477759838104 = 0.008741949684917927 + 0.01 * 6.971282958984375
Epoch 660, val loss: 0.9531209468841553
Epoch 670, training loss: 0.07800140976905823 = 0.008283963426947594 + 0.01 * 6.971744537353516
Epoch 670, val loss: 0.9604334235191345
Epoch 680, training loss: 0.07749813050031662 = 0.007861914113163948 + 0.01 * 6.963621616363525
Epoch 680, val loss: 0.9675683975219727
Epoch 690, training loss: 0.0771680399775505 = 0.007472213823348284 + 0.01 * 6.9695820808410645
Epoch 690, val loss: 0.9745352268218994
Epoch 700, training loss: 0.07666334509849548 = 0.007112626451998949 + 0.01 * 6.955071449279785
Epoch 700, val loss: 0.9812763333320618
Epoch 710, training loss: 0.07633490860462189 = 0.0067790811881423 + 0.01 * 6.955582618713379
Epoch 710, val loss: 0.9878603219985962
Epoch 720, training loss: 0.07591847330331802 = 0.00646985275670886 + 0.01 * 6.944862365722656
Epoch 720, val loss: 0.9941830039024353
Epoch 730, training loss: 0.07567262649536133 = 0.006183269899338484 + 0.01 * 6.948936462402344
Epoch 730, val loss: 1.0004061460494995
Epoch 740, training loss: 0.0754256322979927 = 0.005916133988648653 + 0.01 * 6.950949668884277
Epoch 740, val loss: 1.0064014196395874
Epoch 750, training loss: 0.07501745223999023 = 0.005667133256793022 + 0.01 * 6.935031890869141
Epoch 750, val loss: 1.0122947692871094
Epoch 760, training loss: 0.07498226314783096 = 0.005434887483716011 + 0.01 * 6.954737663269043
Epoch 760, val loss: 1.0180656909942627
Epoch 770, training loss: 0.07455555349588394 = 0.00521807512268424 + 0.01 * 6.9337477684021
Epoch 770, val loss: 1.0235106945037842
Epoch 780, training loss: 0.07425973564386368 = 0.0050159599632024765 + 0.01 * 6.92437744140625
Epoch 780, val loss: 1.0289210081100464
Epoch 790, training loss: 0.07400974631309509 = 0.004825982730835676 + 0.01 * 6.918376922607422
Epoch 790, val loss: 1.0341627597808838
Epoch 800, training loss: 0.07385917752981186 = 0.004647915251553059 + 0.01 * 6.921126365661621
Epoch 800, val loss: 1.0392992496490479
Epoch 810, training loss: 0.0736517533659935 = 0.004480372183024883 + 0.01 * 6.917138576507568
Epoch 810, val loss: 1.044191837310791
Epoch 820, training loss: 0.07355084270238876 = 0.004323419649153948 + 0.01 * 6.9227423667907715
Epoch 820, val loss: 1.0491372346878052
Epoch 830, training loss: 0.07319991290569305 = 0.004175736103206873 + 0.01 * 6.9024176597595215
Epoch 830, val loss: 1.0537915229797363
Epoch 840, training loss: 0.07309825718402863 = 0.00403638556599617 + 0.01 * 6.906187057495117
Epoch 840, val loss: 1.0583419799804688
Epoch 850, training loss: 0.0728658139705658 = 0.0039048767648637295 + 0.01 * 6.896093368530273
Epoch 850, val loss: 1.0627657175064087
Epoch 860, training loss: 0.07265417277812958 = 0.0037806734908372164 + 0.01 * 6.887350559234619
Epoch 860, val loss: 1.0672225952148438
Epoch 870, training loss: 0.07244765758514404 = 0.0036634807474911213 + 0.01 * 6.87841796875
Epoch 870, val loss: 1.0714730024337769
Epoch 880, training loss: 0.07237105816602707 = 0.0035516899079084396 + 0.01 * 6.881936550140381
Epoch 880, val loss: 1.0755857229232788
Epoch 890, training loss: 0.07233990728855133 = 0.003446682123467326 + 0.01 * 6.889322280883789
Epoch 890, val loss: 1.079659104347229
Epoch 900, training loss: 0.07205236703157425 = 0.003346710931509733 + 0.01 * 6.870565891265869
Epoch 900, val loss: 1.0835418701171875
Epoch 910, training loss: 0.07203733921051025 = 0.003251866204664111 + 0.01 * 6.878547668457031
Epoch 910, val loss: 1.087349534034729
Epoch 920, training loss: 0.07181250303983688 = 0.0031618447974324226 + 0.01 * 6.865065574645996
Epoch 920, val loss: 1.0911468267440796
Epoch 930, training loss: 0.07181770354509354 = 0.0030760185327380896 + 0.01 * 6.874168872833252
Epoch 930, val loss: 1.0948758125305176
Epoch 940, training loss: 0.07161981612443924 = 0.0029945371206849813 + 0.01 * 6.862528324127197
Epoch 940, val loss: 1.098448395729065
Epoch 950, training loss: 0.07143184542655945 = 0.0029171311762183905 + 0.01 * 6.851471424102783
Epoch 950, val loss: 1.1019965410232544
Epoch 960, training loss: 0.07129594683647156 = 0.002843245165422559 + 0.01 * 6.845270156860352
Epoch 960, val loss: 1.105289101600647
Epoch 970, training loss: 0.07131688296794891 = 0.002773015294224024 + 0.01 * 6.854386806488037
Epoch 970, val loss: 1.1086153984069824
Epoch 980, training loss: 0.07103563845157623 = 0.0027059719432145357 + 0.01 * 6.832967281341553
Epoch 980, val loss: 1.111877202987671
Epoch 990, training loss: 0.07116314768791199 = 0.002641927683725953 + 0.01 * 6.852121829986572
Epoch 990, val loss: 1.115012764930725
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8365840801265156
The final CL Acc:0.82222, 0.00302, The final GNN Acc:0.83904, 0.00194
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10528])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0328633785247803 = 1.9468953609466553 + 0.01 * 8.59681224822998
Epoch 0, val loss: 1.9397499561309814
Epoch 10, training loss: 2.022958993911743 = 1.9369914531707764 + 0.01 * 8.596752166748047
Epoch 10, val loss: 1.9305490255355835
Epoch 20, training loss: 2.010852813720703 = 1.9248875379562378 + 0.01 * 8.596529960632324
Epoch 20, val loss: 1.9189914464950562
Epoch 30, training loss: 1.9942141771316528 = 1.90825617313385 + 0.01 * 8.595804214477539
Epoch 30, val loss: 1.9029629230499268
Epoch 40, training loss: 1.9702439308166504 = 1.884325385093689 + 0.01 * 8.591854095458984
Epoch 40, val loss: 1.8801872730255127
Epoch 50, training loss: 1.9369161128997803 = 1.851290225982666 + 0.01 * 8.562586784362793
Epoch 50, val loss: 1.8499445915222168
Epoch 60, training loss: 1.8967814445495605 = 1.8129125833511353 + 0.01 * 8.386890411376953
Epoch 60, val loss: 1.8172307014465332
Epoch 70, training loss: 1.8603166341781616 = 1.778442144393921 + 0.01 * 8.187450408935547
Epoch 70, val loss: 1.7886862754821777
Epoch 80, training loss: 1.8205803632736206 = 1.7412173748016357 + 0.01 * 7.936300277709961
Epoch 80, val loss: 1.753244400024414
Epoch 90, training loss: 1.7657297849655151 = 1.6888824701309204 + 0.01 * 7.684736251831055
Epoch 90, val loss: 1.7036851644515991
Epoch 100, training loss: 1.6925193071365356 = 1.6173014640808105 + 0.01 * 7.521788597106934
Epoch 100, val loss: 1.6409022808074951
Epoch 110, training loss: 1.6014243364334106 = 1.5272263288497925 + 0.01 * 7.41980504989624
Epoch 110, val loss: 1.5646871328353882
Epoch 120, training loss: 1.5012410879135132 = 1.4277743101119995 + 0.01 * 7.346673965454102
Epoch 120, val loss: 1.4802041053771973
Epoch 130, training loss: 1.399167776107788 = 1.3260020017623901 + 0.01 * 7.316575527191162
Epoch 130, val loss: 1.3956865072250366
Epoch 140, training loss: 1.2972301244735718 = 1.224177360534668 + 0.01 * 7.305281162261963
Epoch 140, val loss: 1.3134554624557495
Epoch 150, training loss: 1.1962487697601318 = 1.1232702732086182 + 0.01 * 7.297852993011475
Epoch 150, val loss: 1.234984278678894
Epoch 160, training loss: 1.0969607830047607 = 1.0240477323532104 + 0.01 * 7.291299819946289
Epoch 160, val loss: 1.1599316596984863
Epoch 170, training loss: 1.0013667345046997 = 0.9285446405410767 + 0.01 * 7.282211780548096
Epoch 170, val loss: 1.089208960533142
Epoch 180, training loss: 0.912091851234436 = 0.839418351650238 + 0.01 * 7.26734733581543
Epoch 180, val loss: 1.024501085281372
Epoch 190, training loss: 0.8309969902038574 = 0.7585397958755493 + 0.01 * 7.2457194328308105
Epoch 190, val loss: 0.9675979018211365
Epoch 200, training loss: 0.7583417296409607 = 0.6861477494239807 + 0.01 * 7.219400405883789
Epoch 200, val loss: 0.919581413269043
Epoch 210, training loss: 0.693493127822876 = 0.6215448975563049 + 0.01 * 7.194819927215576
Epoch 210, val loss: 0.8806062936782837
Epoch 220, training loss: 0.635326087474823 = 0.5634987354278564 + 0.01 * 7.182733058929443
Epoch 220, val loss: 0.8500241637229919
Epoch 230, training loss: 0.5825760960578918 = 0.5108538866043091 + 0.01 * 7.17222261428833
Epoch 230, val loss: 0.8263890743255615
Epoch 240, training loss: 0.5344721078872681 = 0.46281811594963074 + 0.01 * 7.165401935577393
Epoch 240, val loss: 0.8085225820541382
Epoch 250, training loss: 0.49051520228385925 = 0.41890066862106323 + 0.01 * 7.161452770233154
Epoch 250, val loss: 0.7951880693435669
Epoch 260, training loss: 0.4502648711204529 = 0.37870723009109497 + 0.01 * 7.155766010284424
Epoch 260, val loss: 0.7857241034507751
Epoch 270, training loss: 0.4134807288646698 = 0.34186485409736633 + 0.01 * 7.161588668823242
Epoch 270, val loss: 0.7797088623046875
Epoch 280, training loss: 0.37942609190940857 = 0.3079817295074463 + 0.01 * 7.144435882568359
Epoch 280, val loss: 0.776435375213623
Epoch 290, training loss: 0.3480370044708252 = 0.2766449451446533 + 0.01 * 7.13920783996582
Epoch 290, val loss: 0.7752581834793091
Epoch 300, training loss: 0.31885820627212524 = 0.24754437804222107 + 0.01 * 7.131381511688232
Epoch 300, val loss: 0.7758962512016296
Epoch 310, training loss: 0.29175126552581787 = 0.22052784264087677 + 0.01 * 7.122342109680176
Epoch 310, val loss: 0.7783492803573608
Epoch 320, training loss: 0.2667921781539917 = 0.19561637938022614 + 0.01 * 7.117581367492676
Epoch 320, val loss: 0.7826427221298218
Epoch 330, training loss: 0.24399420619010925 = 0.17294225096702576 + 0.01 * 7.105195045471191
Epoch 330, val loss: 0.7887755632400513
Epoch 340, training loss: 0.22360792756080627 = 0.15257446467876434 + 0.01 * 7.103346824645996
Epoch 340, val loss: 0.7967733144760132
Epoch 350, training loss: 0.20546162128448486 = 0.13451507687568665 + 0.01 * 7.094654083251953
Epoch 350, val loss: 0.8065340518951416
Epoch 360, training loss: 0.18955537676811218 = 0.11866499483585358 + 0.01 * 7.089038848876953
Epoch 360, val loss: 0.8178943395614624
Epoch 370, training loss: 0.17564994096755981 = 0.10484065860509872 + 0.01 * 7.080928325653076
Epoch 370, val loss: 0.8306485414505005
Epoch 380, training loss: 0.16363021731376648 = 0.0928359180688858 + 0.01 * 7.079431056976318
Epoch 380, val loss: 0.8445051908493042
Epoch 390, training loss: 0.15308496356010437 = 0.08243367820978165 + 0.01 * 7.065128803253174
Epoch 390, val loss: 0.859292209148407
Epoch 400, training loss: 0.14407819509506226 = 0.07341723144054413 + 0.01 * 7.06609582901001
Epoch 400, val loss: 0.8747197389602661
Epoch 410, training loss: 0.13611185550689697 = 0.06560049951076508 + 0.01 * 7.051136016845703
Epoch 410, val loss: 0.8905856609344482
Epoch 420, training loss: 0.12929506599903107 = 0.05880360305309296 + 0.01 * 7.0491461753845215
Epoch 420, val loss: 0.9067544341087341
Epoch 430, training loss: 0.12325751781463623 = 0.05288156494498253 + 0.01 * 7.037595272064209
Epoch 430, val loss: 0.9230667948722839
Epoch 440, training loss: 0.11796510219573975 = 0.04770490527153015 + 0.01 * 7.02601957321167
Epoch 440, val loss: 0.9393714070320129
Epoch 450, training loss: 0.11356247961521149 = 0.04316773638129234 + 0.01 * 7.039474964141846
Epoch 450, val loss: 0.9555842876434326
Epoch 460, training loss: 0.10934030264616013 = 0.03918677568435669 + 0.01 * 7.015352725982666
Epoch 460, val loss: 0.9716197848320007
Epoch 470, training loss: 0.10582206398248672 = 0.035680197179317474 + 0.01 * 7.014186859130859
Epoch 470, val loss: 0.9874383807182312
Epoch 480, training loss: 0.10254927724599838 = 0.03258612006902695 + 0.01 * 6.996315956115723
Epoch 480, val loss: 1.003037452697754
Epoch 490, training loss: 0.09968706220388412 = 0.029849208891391754 + 0.01 * 6.983785629272461
Epoch 490, val loss: 1.0183268785476685
Epoch 500, training loss: 0.09732753038406372 = 0.027423391118645668 + 0.01 * 6.990414619445801
Epoch 500, val loss: 1.0333000421524048
Epoch 510, training loss: 0.09506642073392868 = 0.025267094373703003 + 0.01 * 6.97993278503418
Epoch 510, val loss: 1.047959804534912
Epoch 520, training loss: 0.09311734139919281 = 0.023346934467554092 + 0.01 * 6.977041244506836
Epoch 520, val loss: 1.0622490644454956
Epoch 530, training loss: 0.09138953685760498 = 0.021629882976412773 + 0.01 * 6.9759650230407715
Epoch 530, val loss: 1.0761833190917969
Epoch 540, training loss: 0.08973488956689835 = 0.020091364160180092 + 0.01 * 6.964353084564209
Epoch 540, val loss: 1.0897706747055054
Epoch 550, training loss: 0.08818544447422028 = 0.01870885118842125 + 0.01 * 6.947659969329834
Epoch 550, val loss: 1.1029568910598755
Epoch 560, training loss: 0.08690406382083893 = 0.01746288314461708 + 0.01 * 6.944118022918701
Epoch 560, val loss: 1.1157807111740112
Epoch 570, training loss: 0.0857214406132698 = 0.016338123008608818 + 0.01 * 6.9383320808410645
Epoch 570, val loss: 1.1282256841659546
Epoch 580, training loss: 0.08458760380744934 = 0.015320670790970325 + 0.01 * 6.926692962646484
Epoch 580, val loss: 1.1402862071990967
Epoch 590, training loss: 0.0836273580789566 = 0.014397530816495419 + 0.01 * 6.922982692718506
Epoch 590, val loss: 1.1519724130630493
Epoch 600, training loss: 0.08295182138681412 = 0.013557897880673409 + 0.01 * 6.939392566680908
Epoch 600, val loss: 1.1633747816085815
Epoch 610, training loss: 0.08185650408267975 = 0.012792745605111122 + 0.01 * 6.906375885009766
Epoch 610, val loss: 1.1743322610855103
Epoch 620, training loss: 0.08118152618408203 = 0.012093150056898594 + 0.01 * 6.908837795257568
Epoch 620, val loss: 1.184989094734192
Epoch 630, training loss: 0.08054563403129578 = 0.01145152747631073 + 0.01 * 6.90941047668457
Epoch 630, val loss: 1.1954193115234375
Epoch 640, training loss: 0.07983295619487762 = 0.010862696915864944 + 0.01 * 6.897026062011719
Epoch 640, val loss: 1.205412745475769
Epoch 650, training loss: 0.07923141866922379 = 0.010320918634533882 + 0.01 * 6.891050338745117
Epoch 650, val loss: 1.2151838541030884
Epoch 660, training loss: 0.07858866453170776 = 0.009820985607802868 + 0.01 * 6.876768112182617
Epoch 660, val loss: 1.2246464490890503
Epoch 670, training loss: 0.07836683094501495 = 0.009358573704957962 + 0.01 * 6.9008259773254395
Epoch 670, val loss: 1.2338306903839111
Epoch 680, training loss: 0.0777117908000946 = 0.008930718526244164 + 0.01 * 6.878107070922852
Epoch 680, val loss: 1.2427852153778076
Epoch 690, training loss: 0.07716993987560272 = 0.008533823303878307 + 0.01 * 6.863612174987793
Epoch 690, val loss: 1.2514463663101196
Epoch 700, training loss: 0.0768938809633255 = 0.008164707571268082 + 0.01 * 6.872918128967285
Epoch 700, val loss: 1.2598680257797241
Epoch 710, training loss: 0.0764538124203682 = 0.00782176572829485 + 0.01 * 6.863204479217529
Epoch 710, val loss: 1.2680143117904663
Epoch 720, training loss: 0.07599151879549026 = 0.007502174004912376 + 0.01 * 6.848934173583984
Epoch 720, val loss: 1.2759610414505005
Epoch 730, training loss: 0.07578922808170319 = 0.007203850895166397 + 0.01 * 6.8585381507873535
Epoch 730, val loss: 1.2836742401123047
Epoch 740, training loss: 0.07533930242061615 = 0.0069245994091033936 + 0.01 * 6.841470241546631
Epoch 740, val loss: 1.2912126779556274
Epoch 750, training loss: 0.07503625005483627 = 0.0066629741340875626 + 0.01 * 6.83732795715332
Epoch 750, val loss: 1.2984830141067505
Epoch 760, training loss: 0.07488562166690826 = 0.006417766213417053 + 0.01 * 6.846785545349121
Epoch 760, val loss: 1.3056100606918335
Epoch 770, training loss: 0.07444871962070465 = 0.0061873784288764 + 0.01 * 6.826134204864502
Epoch 770, val loss: 1.3125371932983398
Epoch 780, training loss: 0.07428690791130066 = 0.005970401223748922 + 0.01 * 6.831650257110596
Epoch 780, val loss: 1.3193188905715942
Epoch 790, training loss: 0.07425948232412338 = 0.005765798967331648 + 0.01 * 6.849369049072266
Epoch 790, val loss: 1.325886845588684
Epoch 800, training loss: 0.07384355366230011 = 0.005572526250034571 + 0.01 * 6.8271026611328125
Epoch 800, val loss: 1.332221269607544
Epoch 810, training loss: 0.07368506491184235 = 0.005390124395489693 + 0.01 * 6.829494476318359
Epoch 810, val loss: 1.3385090827941895
Epoch 820, training loss: 0.07332143187522888 = 0.005217383615672588 + 0.01 * 6.810405254364014
Epoch 820, val loss: 1.3445606231689453
Epoch 830, training loss: 0.0730719044804573 = 0.005054053850471973 + 0.01 * 6.801784992218018
Epoch 830, val loss: 1.3505059480667114
Epoch 840, training loss: 0.07284216582775116 = 0.004899463150650263 + 0.01 * 6.7942705154418945
Epoch 840, val loss: 1.3562902212142944
Epoch 850, training loss: 0.07289911806583405 = 0.004752911161631346 + 0.01 * 6.8146209716796875
Epoch 850, val loss: 1.361911416053772
Epoch 860, training loss: 0.07272417098283768 = 0.004613971803337336 + 0.01 * 6.8110198974609375
Epoch 860, val loss: 1.3674049377441406
Epoch 870, training loss: 0.07250512391328812 = 0.004481747280806303 + 0.01 * 6.802338123321533
Epoch 870, val loss: 1.3727166652679443
Epoch 880, training loss: 0.07213769853115082 = 0.004356278572231531 + 0.01 * 6.778141975402832
Epoch 880, val loss: 1.3779447078704834
Epoch 890, training loss: 0.0721493810415268 = 0.004236987791955471 + 0.01 * 6.791239261627197
Epoch 890, val loss: 1.3830335140228271
Epoch 900, training loss: 0.0719325914978981 = 0.004123380873352289 + 0.01 * 6.78092098236084
Epoch 900, val loss: 1.3879450559616089
Epoch 910, training loss: 0.0716799944639206 = 0.004015036392956972 + 0.01 * 6.766496181488037
Epoch 910, val loss: 1.392799735069275
Epoch 920, training loss: 0.07165329158306122 = 0.003912109881639481 + 0.01 * 6.774118423461914
Epoch 920, val loss: 1.3975001573562622
Epoch 930, training loss: 0.07162424921989441 = 0.0038133615162223577 + 0.01 * 6.781088829040527
Epoch 930, val loss: 1.4020406007766724
Epoch 940, training loss: 0.07134661823511124 = 0.0037195414770394564 + 0.01 * 6.7627081871032715
Epoch 940, val loss: 1.406550645828247
Epoch 950, training loss: 0.07122818380594254 = 0.0036297389306128025 + 0.01 * 6.759844779968262
Epoch 950, val loss: 1.4108771085739136
Epoch 960, training loss: 0.07105936855077744 = 0.003543790662661195 + 0.01 * 6.75155782699585
Epoch 960, val loss: 1.4150699377059937
Epoch 970, training loss: 0.0708758756518364 = 0.003461852204054594 + 0.01 * 6.7414021492004395
Epoch 970, val loss: 1.4191906452178955
Epoch 980, training loss: 0.07094010710716248 = 0.0033831680193543434 + 0.01 * 6.7556939125061035
Epoch 980, val loss: 1.42323637008667
Epoch 990, training loss: 0.07084397226572037 = 0.0033082126174122095 + 0.01 * 6.753575801849365
Epoch 990, val loss: 1.4271870851516724
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 2.028061866760254 = 1.9420934915542603 + 0.01 * 8.596831321716309
Epoch 0, val loss: 1.9377126693725586
Epoch 10, training loss: 2.0185370445251465 = 1.932569146156311 + 0.01 * 8.596789360046387
Epoch 10, val loss: 1.9284658432006836
Epoch 20, training loss: 2.0071537494659424 = 1.9211875200271606 + 0.01 * 8.59663200378418
Epoch 20, val loss: 1.9170103073120117
Epoch 30, training loss: 1.991715669631958 = 1.9057540893554688 + 0.01 * 8.59615707397461
Epoch 30, val loss: 1.9011167287826538
Epoch 40, training loss: 1.9694817066192627 = 1.8835433721542358 + 0.01 * 8.593835830688477
Epoch 40, val loss: 1.878144383430481
Epoch 50, training loss: 1.938232421875 = 1.8524494171142578 + 0.01 * 8.578303337097168
Epoch 50, val loss: 1.8468207120895386
Epoch 60, training loss: 1.9007999897003174 = 1.815711259841919 + 0.01 * 8.50887680053711
Epoch 60, val loss: 1.8130109310150146
Epoch 70, training loss: 1.866600751876831 = 1.7836631536483765 + 0.01 * 8.293758392333984
Epoch 70, val loss: 1.7878262996673584
Epoch 80, training loss: 1.8329527378082275 = 1.7511656284332275 + 0.01 * 8.178706169128418
Epoch 80, val loss: 1.761337399482727
Epoch 90, training loss: 1.7839629650115967 = 1.7050282955169678 + 0.01 * 7.893467903137207
Epoch 90, val loss: 1.7218681573867798
Epoch 100, training loss: 1.717751383781433 = 1.6409227848052979 + 0.01 * 7.682858467102051
Epoch 100, val loss: 1.6678471565246582
Epoch 110, training loss: 1.6298788785934448 = 1.5539774894714355 + 0.01 * 7.590137004852295
Epoch 110, val loss: 1.595266342163086
Epoch 120, training loss: 1.5232853889465332 = 1.448146104812622 + 0.01 * 7.5139241218566895
Epoch 120, val loss: 1.508077621459961
Epoch 130, training loss: 1.4088772535324097 = 1.3344180583953857 + 0.01 * 7.445921897888184
Epoch 130, val loss: 1.4160984754562378
Epoch 140, training loss: 1.2954527139663696 = 1.2214796543121338 + 0.01 * 7.397306442260742
Epoch 140, val loss: 1.3245415687561035
Epoch 150, training loss: 1.1864897012710571 = 1.112782597541809 + 0.01 * 7.370712757110596
Epoch 150, val loss: 1.2373766899108887
Epoch 160, training loss: 1.083835482597351 = 1.0102916955947876 + 0.01 * 7.354381084442139
Epoch 160, val loss: 1.1573442220687866
Epoch 170, training loss: 0.9891856908798218 = 0.9157413840293884 + 0.01 * 7.344430923461914
Epoch 170, val loss: 1.0859242677688599
Epoch 180, training loss: 0.9044455885887146 = 0.831124484539032 + 0.01 * 7.332111358642578
Epoch 180, val loss: 1.0247483253479004
Epoch 190, training loss: 0.8305662870407104 = 0.7574414610862732 + 0.01 * 7.312483787536621
Epoch 190, val loss: 0.9748051166534424
Epoch 200, training loss: 0.766495943069458 = 0.6936606168746948 + 0.01 * 7.283534049987793
Epoch 200, val loss: 0.9356850385665894
Epoch 210, training loss: 0.7099416255950928 = 0.6374274492263794 + 0.01 * 7.2514190673828125
Epoch 210, val loss: 0.9058079123497009
Epoch 220, training loss: 0.6587293148040771 = 0.5864797830581665 + 0.01 * 7.224956035614014
Epoch 220, val loss: 0.8833813667297363
Epoch 230, training loss: 0.6114931702613831 = 0.5393792986869812 + 0.01 * 7.211389541625977
Epoch 230, val loss: 0.8666239380836487
Epoch 240, training loss: 0.5674272179603577 = 0.49549365043640137 + 0.01 * 7.193357944488525
Epoch 240, val loss: 0.8546704649925232
Epoch 250, training loss: 0.5264381766319275 = 0.45456498861312866 + 0.01 * 7.187321186065674
Epoch 250, val loss: 0.847419261932373
Epoch 260, training loss: 0.4884266257286072 = 0.41661906242370605 + 0.01 * 7.18075704574585
Epoch 260, val loss: 0.8449293971061707
Epoch 270, training loss: 0.4535326063632965 = 0.38178011775016785 + 0.01 * 7.175248622894287
Epoch 270, val loss: 0.8471794724464417
Epoch 280, training loss: 0.42187047004699707 = 0.3501303195953369 + 0.01 * 7.174014091491699
Epoch 280, val loss: 0.8538978099822998
Epoch 290, training loss: 0.3933067321777344 = 0.32162681221961975 + 0.01 * 7.167993068695068
Epoch 290, val loss: 0.8645613789558411
Epoch 300, training loss: 0.3677394986152649 = 0.2960907518863678 + 0.01 * 7.16487455368042
Epoch 300, val loss: 0.8783929944038391
Epoch 310, training loss: 0.34484386444091797 = 0.27328187227249146 + 0.01 * 7.156200885772705
Epoch 310, val loss: 0.89506596326828
Epoch 320, training loss: 0.3244573473930359 = 0.2529040575027466 + 0.01 * 7.155328273773193
Epoch 320, val loss: 0.9141480326652527
Epoch 330, training loss: 0.3060773015022278 = 0.23460917174816132 + 0.01 * 7.146811485290527
Epoch 330, val loss: 0.9350841641426086
Epoch 340, training loss: 0.28934749960899353 = 0.21792948246002197 + 0.01 * 7.141802787780762
Epoch 340, val loss: 0.9573910236358643
Epoch 350, training loss: 0.27363860607147217 = 0.20227450132369995 + 0.01 * 7.136410713195801
Epoch 350, val loss: 0.9806422591209412
Epoch 360, training loss: 0.25825566053390503 = 0.18692699074745178 + 0.01 * 7.132868766784668
Epoch 360, val loss: 1.0045397281646729
Epoch 370, training loss: 0.2424529641866684 = 0.17112997174263 + 0.01 * 7.132299423217773
Epoch 370, val loss: 1.0288468599319458
Epoch 380, training loss: 0.22568780183792114 = 0.15441551804542542 + 0.01 * 7.1272292137146
Epoch 380, val loss: 1.0537854433059692
Epoch 390, training loss: 0.20837177336215973 = 0.13715848326683044 + 0.01 * 7.121328830718994
Epoch 390, val loss: 1.0800529718399048
Epoch 400, training loss: 0.19166547060012817 = 0.12051331251859665 + 0.01 * 7.1152167320251465
Epoch 400, val loss: 1.108019471168518
Epoch 410, training loss: 0.17663460969924927 = 0.10555027425289154 + 0.01 * 7.108432769775391
Epoch 410, val loss: 1.1378798484802246
Epoch 420, training loss: 0.16373950242996216 = 0.09267745912075043 + 0.01 * 7.106204509735107
Epoch 420, val loss: 1.1693347692489624
Epoch 430, training loss: 0.1528138816356659 = 0.08178503811359406 + 0.01 * 7.102884292602539
Epoch 430, val loss: 1.2016689777374268
Epoch 440, training loss: 0.14347541332244873 = 0.07255227118730545 + 0.01 * 7.092315196990967
Epoch 440, val loss: 1.2343072891235352
Epoch 450, training loss: 0.13547593355178833 = 0.06467832624912262 + 0.01 * 7.079761981964111
Epoch 450, val loss: 1.2667579650878906
Epoch 460, training loss: 0.12880223989486694 = 0.05792222172021866 + 0.01 * 7.0880022048950195
Epoch 460, val loss: 1.2985526323318481
Epoch 470, training loss: 0.12285351753234863 = 0.052098244428634644 + 0.01 * 7.075527667999268
Epoch 470, val loss: 1.3294473886489868
Epoch 480, training loss: 0.11774402856826782 = 0.04705110192298889 + 0.01 * 7.069293022155762
Epoch 480, val loss: 1.3592243194580078
Epoch 490, training loss: 0.11329755187034607 = 0.042653005570173264 + 0.01 * 7.064454555511475
Epoch 490, val loss: 1.387913465499878
Epoch 500, training loss: 0.10941680520772934 = 0.03880566358566284 + 0.01 * 7.061114311218262
Epoch 500, val loss: 1.4154376983642578
Epoch 510, training loss: 0.10582572221755981 = 0.03542538359761238 + 0.01 * 7.04003381729126
Epoch 510, val loss: 1.4419480562210083
Epoch 520, training loss: 0.10272658616304398 = 0.032445915043354034 + 0.01 * 7.028067111968994
Epoch 520, val loss: 1.4673913717269897
Epoch 530, training loss: 0.10010705888271332 = 0.029809441417455673 + 0.01 * 7.02976131439209
Epoch 530, val loss: 1.4918591976165771
Epoch 540, training loss: 0.09759947657585144 = 0.027468036860227585 + 0.01 * 7.013144016265869
Epoch 540, val loss: 1.5153727531433105
Epoch 550, training loss: 0.09551185369491577 = 0.025381913408637047 + 0.01 * 7.012993812561035
Epoch 550, val loss: 1.5379501581192017
Epoch 560, training loss: 0.09369038045406342 = 0.02352145127952099 + 0.01 * 7.016892910003662
Epoch 560, val loss: 1.559561848640442
Epoch 570, training loss: 0.09185248613357544 = 0.021853594109416008 + 0.01 * 6.999888896942139
Epoch 570, val loss: 1.580262541770935
Epoch 580, training loss: 0.09026096761226654 = 0.020352479070425034 + 0.01 * 6.990848541259766
Epoch 580, val loss: 1.6002464294433594
Epoch 590, training loss: 0.08895517885684967 = 0.01899874582886696 + 0.01 * 6.995643615722656
Epoch 590, val loss: 1.6193907260894775
Epoch 600, training loss: 0.08762206137180328 = 0.01777510903775692 + 0.01 * 6.984694957733154
Epoch 600, val loss: 1.6378283500671387
Epoch 610, training loss: 0.08642847836017609 = 0.016667082905769348 + 0.01 * 6.976139545440674
Epoch 610, val loss: 1.655457615852356
Epoch 620, training loss: 0.08534292876720428 = 0.01566123776137829 + 0.01 * 6.968169212341309
Epoch 620, val loss: 1.6724586486816406
Epoch 630, training loss: 0.08444757759571075 = 0.014744660817086697 + 0.01 * 6.970292091369629
Epoch 630, val loss: 1.6888017654418945
Epoch 640, training loss: 0.08349441736936569 = 0.013907967135310173 + 0.01 * 6.958645343780518
Epoch 640, val loss: 1.704492211341858
Epoch 650, training loss: 0.08265943080186844 = 0.01314175222069025 + 0.01 * 6.951768398284912
Epoch 650, val loss: 1.719694972038269
Epoch 660, training loss: 0.0819944515824318 = 0.012439598329365253 + 0.01 * 6.955484867095947
Epoch 660, val loss: 1.7341986894607544
Epoch 670, training loss: 0.08128111064434052 = 0.01179581694304943 + 0.01 * 6.948529243469238
Epoch 670, val loss: 1.7481340169906616
Epoch 680, training loss: 0.08050079643726349 = 0.011203315109014511 + 0.01 * 6.929748058319092
Epoch 680, val loss: 1.761522650718689
Epoch 690, training loss: 0.08003655821084976 = 0.01065598614513874 + 0.01 * 6.938057899475098
Epoch 690, val loss: 1.7744396924972534
Epoch 700, training loss: 0.07942534983158112 = 0.010150086134672165 + 0.01 * 6.927525997161865
Epoch 700, val loss: 1.7869597673416138
Epoch 710, training loss: 0.07889176160097122 = 0.009681494906544685 + 0.01 * 6.921027183532715
Epoch 710, val loss: 1.7989814281463623
Epoch 720, training loss: 0.0783427506685257 = 0.009247123263776302 + 0.01 * 6.909563064575195
Epoch 720, val loss: 1.8104979991912842
Epoch 730, training loss: 0.07803694903850555 = 0.008843638934195042 + 0.01 * 6.9193315505981445
Epoch 730, val loss: 1.821679949760437
Epoch 740, training loss: 0.07756233960390091 = 0.00846810732036829 + 0.01 * 6.909423828125
Epoch 740, val loss: 1.8324171304702759
Epoch 750, training loss: 0.07716153562068939 = 0.008118443191051483 + 0.01 * 6.9043097496032715
Epoch 750, val loss: 1.8427159786224365
Epoch 760, training loss: 0.0769290179014206 = 0.007792044430971146 + 0.01 * 6.913697242736816
Epoch 760, val loss: 1.8527588844299316
Epoch 770, training loss: 0.07653400301933289 = 0.007487033493816853 + 0.01 * 6.904696464538574
Epoch 770, val loss: 1.8623262643814087
Epoch 780, training loss: 0.07597742229700089 = 0.007201606873422861 + 0.01 * 6.87758207321167
Epoch 780, val loss: 1.8716247081756592
Epoch 790, training loss: 0.07598530501127243 = 0.00693397456780076 + 0.01 * 6.905133247375488
Epoch 790, val loss: 1.880599856376648
Epoch 800, training loss: 0.07558301836252213 = 0.006682653911411762 + 0.01 * 6.890036106109619
Epoch 800, val loss: 1.8892898559570312
Epoch 810, training loss: 0.07509154826402664 = 0.006446319166570902 + 0.01 * 6.864522933959961
Epoch 810, val loss: 1.8976768255233765
Epoch 820, training loss: 0.0749204084277153 = 0.006223694887012243 + 0.01 * 6.86967134475708
Epoch 820, val loss: 1.9057672023773193
Epoch 830, training loss: 0.07475989311933517 = 0.006014023907482624 + 0.01 * 6.874586582183838
Epoch 830, val loss: 1.913514494895935
Epoch 840, training loss: 0.07435203343629837 = 0.0058164652436971664 + 0.01 * 6.8535566329956055
Epoch 840, val loss: 1.9210638999938965
Epoch 850, training loss: 0.07455386966466904 = 0.0056299432180821896 + 0.01 * 6.892393112182617
Epoch 850, val loss: 1.928385615348816
Epoch 860, training loss: 0.07404642552137375 = 0.005453629419207573 + 0.01 * 6.859279632568359
Epoch 860, val loss: 1.9353280067443848
Epoch 870, training loss: 0.07383130490779877 = 0.005286807660013437 + 0.01 * 6.854450225830078
Epoch 870, val loss: 1.9422069787979126
Epoch 880, training loss: 0.07352137565612793 = 0.005128621589392424 + 0.01 * 6.839275360107422
Epoch 880, val loss: 1.9487334489822388
Epoch 890, training loss: 0.07342205941677094 = 0.004978398326784372 + 0.01 * 6.844366550445557
Epoch 890, val loss: 1.9551339149475098
Epoch 900, training loss: 0.07344511151313782 = 0.004835898522287607 + 0.01 * 6.860921859741211
Epoch 900, val loss: 1.9613556861877441
Epoch 910, training loss: 0.07339511811733246 = 0.004700539167970419 + 0.01 * 6.869457721710205
Epoch 910, val loss: 1.9671721458435059
Epoch 920, training loss: 0.07280019670724869 = 0.0045722476206719875 + 0.01 * 6.8227949142456055
Epoch 920, val loss: 1.9729336500167847
Epoch 930, training loss: 0.07275141775608063 = 0.004450102336704731 + 0.01 * 6.830131530761719
Epoch 930, val loss: 1.978541612625122
Epoch 940, training loss: 0.07246512919664383 = 0.0043335529044270515 + 0.01 * 6.813157558441162
Epoch 940, val loss: 1.9839162826538086
Epoch 950, training loss: 0.07296760380268097 = 0.004222366958856583 + 0.01 * 6.874523639678955
Epoch 950, val loss: 1.9891940355300903
Epoch 960, training loss: 0.07225988805294037 = 0.004116138909012079 + 0.01 * 6.814374923706055
Epoch 960, val loss: 1.9942513704299927
Epoch 970, training loss: 0.07232439517974854 = 0.004014762118458748 + 0.01 * 6.830963611602783
Epoch 970, val loss: 1.9990917444229126
Epoch 980, training loss: 0.07211800664663315 = 0.003918155562132597 + 0.01 * 6.8199849128723145
Epoch 980, val loss: 2.0036890506744385
Epoch 990, training loss: 0.07177826017141342 = 0.003825769294053316 + 0.01 * 6.795248985290527
Epoch 990, val loss: 2.008338212966919
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 2.03061580657959 = 1.9446470737457275 + 0.01 * 8.596867561340332
Epoch 0, val loss: 1.9453190565109253
Epoch 10, training loss: 2.019676685333252 = 1.9337084293365479 + 0.01 * 8.596823692321777
Epoch 10, val loss: 1.933782696723938
Epoch 20, training loss: 2.0063438415527344 = 1.9203770160675049 + 0.01 * 8.596687316894531
Epoch 20, val loss: 1.9196046590805054
Epoch 30, training loss: 1.988208293914795 = 1.9022456407546997 + 0.01 * 8.596271514892578
Epoch 30, val loss: 1.900263786315918
Epoch 40, training loss: 1.9627543687820435 = 1.8768113851547241 + 0.01 * 8.594301223754883
Epoch 40, val loss: 1.8734370470046997
Epoch 50, training loss: 1.9291733503341675 = 1.8433661460876465 + 0.01 * 8.580720901489258
Epoch 50, val loss: 1.8398211002349854
Epoch 60, training loss: 1.893022894859314 = 1.8079043626785278 + 0.01 * 8.511855125427246
Epoch 60, val loss: 1.8082140684127808
Epoch 70, training loss: 1.8597499132156372 = 1.777161717414856 + 0.01 * 8.258817672729492
Epoch 70, val loss: 1.7842280864715576
Epoch 80, training loss: 1.8198018074035645 = 1.738814115524292 + 0.01 * 8.098764419555664
Epoch 80, val loss: 1.7514255046844482
Epoch 90, training loss: 1.7635036706924438 = 1.6853864192962646 + 0.01 * 7.8117265701293945
Epoch 90, val loss: 1.703678011894226
Epoch 100, training loss: 1.6893250942230225 = 1.6130094528198242 + 0.01 * 7.631566047668457
Epoch 100, val loss: 1.640316367149353
Epoch 110, training loss: 1.6002731323242188 = 1.5242582559585571 + 0.01 * 7.6014885902404785
Epoch 110, val loss: 1.5658674240112305
Epoch 120, training loss: 1.5017979145050049 = 1.4259315729141235 + 0.01 * 7.586634635925293
Epoch 120, val loss: 1.484984278678894
Epoch 130, training loss: 1.3977363109588623 = 1.3219125270843506 + 0.01 * 7.582379341125488
Epoch 130, val loss: 1.4009208679199219
Epoch 140, training loss: 1.2882130146026611 = 1.2124477624893188 + 0.01 * 7.576526641845703
Epoch 140, val loss: 1.3131680488586426
Epoch 150, training loss: 1.1757451295852661 = 1.100102186203003 + 0.01 * 7.564298629760742
Epoch 150, val loss: 1.2246400117874146
Epoch 160, training loss: 1.0666974782943726 = 0.9912753701210022 + 0.01 * 7.542215824127197
Epoch 160, val loss: 1.1415139436721802
Epoch 170, training loss: 0.968150794506073 = 0.8930479884147644 + 0.01 * 7.510280609130859
Epoch 170, val loss: 1.0706440210342407
Epoch 180, training loss: 0.8837228417396545 = 0.8089244961738586 + 0.01 * 7.47983455657959
Epoch 180, val loss: 1.0152806043624878
Epoch 190, training loss: 0.8123274445533752 = 0.7378374934196472 + 0.01 * 7.448995113372803
Epoch 190, val loss: 0.9737968444824219
Epoch 200, training loss: 0.7503789067268372 = 0.6764665842056274 + 0.01 * 7.39123010635376
Epoch 200, val loss: 0.9431197643280029
Epoch 210, training loss: 0.6946787238121033 = 0.6213299632072449 + 0.01 * 7.334874153137207
Epoch 210, val loss: 0.9202447533607483
Epoch 220, training loss: 0.6429126858711243 = 0.569991946220398 + 0.01 * 7.2920756340026855
Epoch 220, val loss: 0.9030742049217224
Epoch 230, training loss: 0.5938966274261475 = 0.5212790369987488 + 0.01 * 7.26176118850708
Epoch 230, val loss: 0.8905177116394043
Epoch 240, training loss: 0.5473136901855469 = 0.4748665392398834 + 0.01 * 7.244714736938477
Epoch 240, val loss: 0.8823451995849609
Epoch 250, training loss: 0.5029911994934082 = 0.43076467514038086 + 0.01 * 7.222650527954102
Epoch 250, val loss: 0.8787806630134583
Epoch 260, training loss: 0.46133214235305786 = 0.38927003741264343 + 0.01 * 7.206211566925049
Epoch 260, val loss: 0.8803021907806396
Epoch 270, training loss: 0.42262279987335205 = 0.3507157862186432 + 0.01 * 7.190700054168701
Epoch 270, val loss: 0.8871610760688782
Epoch 280, training loss: 0.38702407479286194 = 0.315245658159256 + 0.01 * 7.1778411865234375
Epoch 280, val loss: 0.8993014097213745
Epoch 290, training loss: 0.3547247648239136 = 0.2828870713710785 + 0.01 * 7.183769702911377
Epoch 290, val loss: 0.9165811538696289
Epoch 300, training loss: 0.32513436675071716 = 0.25355181097984314 + 0.01 * 7.158256530761719
Epoch 300, val loss: 0.938431441783905
Epoch 310, training loss: 0.298470139503479 = 0.22700268030166626 + 0.01 * 7.146745204925537
Epoch 310, val loss: 0.9643418788909912
Epoch 320, training loss: 0.27446073293685913 = 0.20301465690135956 + 0.01 * 7.1446075439453125
Epoch 320, val loss: 0.9936750531196594
Epoch 330, training loss: 0.25271496176719666 = 0.18139714002609253 + 0.01 * 7.131782531738281
Epoch 330, val loss: 1.0258523225784302
Epoch 340, training loss: 0.23317134380340576 = 0.1619693487882614 + 0.01 * 7.1202006340026855
Epoch 340, val loss: 1.0602269172668457
Epoch 350, training loss: 0.2156953066587448 = 0.1445818692445755 + 0.01 * 7.111343860626221
Epoch 350, val loss: 1.0962390899658203
Epoch 360, training loss: 0.2001933753490448 = 0.1291014850139618 + 0.01 * 7.109189510345459
Epoch 360, val loss: 1.1333237886428833
Epoch 370, training loss: 0.18635037541389465 = 0.11538957804441452 + 0.01 * 7.096078872680664
Epoch 370, val loss: 1.170963168144226
Epoch 380, training loss: 0.17419570684432983 = 0.10328064858913422 + 0.01 * 7.0915069580078125
Epoch 380, val loss: 1.2088407278060913
Epoch 390, training loss: 0.16339920461177826 = 0.09262748062610626 + 0.01 * 7.077172756195068
Epoch 390, val loss: 1.246622920036316
Epoch 400, training loss: 0.15400782227516174 = 0.08326780050992966 + 0.01 * 7.074002265930176
Epoch 400, val loss: 1.2839579582214355
Epoch 410, training loss: 0.14568133652210236 = 0.0750395879149437 + 0.01 * 7.064174652099609
Epoch 410, val loss: 1.3208937644958496
Epoch 420, training loss: 0.13831394910812378 = 0.06779377907514572 + 0.01 * 7.0520172119140625
Epoch 420, val loss: 1.3572853803634644
Epoch 430, training loss: 0.1318504810333252 = 0.061405476182699203 + 0.01 * 7.044501304626465
Epoch 430, val loss: 1.3930635452270508
Epoch 440, training loss: 0.12625224888324738 = 0.05575713887810707 + 0.01 * 7.049510478973389
Epoch 440, val loss: 1.428171157836914
Epoch 450, training loss: 0.12108953297138214 = 0.050755225121974945 + 0.01 * 7.033431053161621
Epoch 450, val loss: 1.4625753164291382
Epoch 460, training loss: 0.11660006642341614 = 0.04632021114230156 + 0.01 * 7.027985572814941
Epoch 460, val loss: 1.4962137937545776
Epoch 470, training loss: 0.11264017969369888 = 0.042381398379802704 + 0.01 * 7.025877952575684
Epoch 470, val loss: 1.5290015935897827
Epoch 480, training loss: 0.10902537405490875 = 0.03886958211660385 + 0.01 * 7.0155792236328125
Epoch 480, val loss: 1.5610063076019287
Epoch 490, training loss: 0.10586853325366974 = 0.035732418298721313 + 0.01 * 7.013611316680908
Epoch 490, val loss: 1.592205286026001
Epoch 500, training loss: 0.10297752916812897 = 0.03292590007185936 + 0.01 * 7.005162715911865
Epoch 500, val loss: 1.6225463151931763
Epoch 510, training loss: 0.10042787343263626 = 0.03040907345712185 + 0.01 * 7.001880168914795
Epoch 510, val loss: 1.6520977020263672
Epoch 520, training loss: 0.09816043078899384 = 0.028146695345640182 + 0.01 * 7.001373767852783
Epoch 520, val loss: 1.6808041334152222
Epoch 530, training loss: 0.09604741632938385 = 0.026110360398888588 + 0.01 * 6.993706226348877
Epoch 530, val loss: 1.7087146043777466
Epoch 540, training loss: 0.09417068958282471 = 0.02427498809993267 + 0.01 * 6.989570140838623
Epoch 540, val loss: 1.7357717752456665
Epoch 550, training loss: 0.09239841997623444 = 0.022615736350417137 + 0.01 * 6.978268623352051
Epoch 550, val loss: 1.7620153427124023
Epoch 560, training loss: 0.09084874391555786 = 0.02111176960170269 + 0.01 * 6.973697662353516
Epoch 560, val loss: 1.787429690361023
Epoch 570, training loss: 0.08953580260276794 = 0.01974700577557087 + 0.01 * 6.978879928588867
Epoch 570, val loss: 1.8121222257614136
Epoch 580, training loss: 0.08814992010593414 = 0.01850597932934761 + 0.01 * 6.964393615722656
Epoch 580, val loss: 1.8360506296157837
Epoch 590, training loss: 0.08698739856481552 = 0.01737532578408718 + 0.01 * 6.961206912994385
Epoch 590, val loss: 1.8592406511306763
Epoch 600, training loss: 0.0859624445438385 = 0.0163428895175457 + 0.01 * 6.9619550704956055
Epoch 600, val loss: 1.8816953897476196
Epoch 610, training loss: 0.08499864488840103 = 0.01539786346256733 + 0.01 * 6.960078239440918
Epoch 610, val loss: 1.9034913778305054
Epoch 620, training loss: 0.08403678983449936 = 0.014531564898788929 + 0.01 * 6.950522422790527
Epoch 620, val loss: 1.9246140718460083
Epoch 630, training loss: 0.08314114809036255 = 0.013735300861299038 + 0.01 * 6.940585136413574
Epoch 630, val loss: 1.9451391696929932
Epoch 640, training loss: 0.08241259306669235 = 0.013001441024243832 + 0.01 * 6.941115379333496
Epoch 640, val loss: 1.9650788307189941
Epoch 650, training loss: 0.08186452090740204 = 0.012323256582021713 + 0.01 * 6.954126834869385
Epoch 650, val loss: 1.98454749584198
Epoch 660, training loss: 0.08109451085329056 = 0.011696684174239635 + 0.01 * 6.939783096313477
Epoch 660, val loss: 2.0033679008483887
Epoch 670, training loss: 0.08037950843572617 = 0.01111760176718235 + 0.01 * 6.926191329956055
Epoch 670, val loss: 2.021688222885132
Epoch 680, training loss: 0.07987608760595322 = 0.010580451227724552 + 0.01 * 6.929563999176025
Epoch 680, val loss: 2.039487361907959
Epoch 690, training loss: 0.0792480856180191 = 0.010082419961690903 + 0.01 * 6.916566371917725
Epoch 690, val loss: 2.0567779541015625
Epoch 700, training loss: 0.07885380834341049 = 0.009618864394724369 + 0.01 * 6.923494815826416
Epoch 700, val loss: 2.073641061782837
Epoch 710, training loss: 0.07828789949417114 = 0.009188150055706501 + 0.01 * 6.909975051879883
Epoch 710, val loss: 2.0899696350097656
Epoch 720, training loss: 0.07801157236099243 = 0.008787097409367561 + 0.01 * 6.92244815826416
Epoch 720, val loss: 2.1058273315429688
Epoch 730, training loss: 0.07752982527017593 = 0.008414017967879772 + 0.01 * 6.911581516265869
Epoch 730, val loss: 2.1212046146392822
Epoch 740, training loss: 0.0770251527428627 = 0.008064789697527885 + 0.01 * 6.896036148071289
Epoch 740, val loss: 2.1362099647521973
Epoch 750, training loss: 0.07666977494955063 = 0.00773786474019289 + 0.01 * 6.893190860748291
Epoch 750, val loss: 2.1508233547210693
Epoch 760, training loss: 0.07639957219362259 = 0.00743148336187005 + 0.01 * 6.896808624267578
Epoch 760, val loss: 2.1650390625
Epoch 770, training loss: 0.07608816027641296 = 0.0071445950306952 + 0.01 * 6.894356727600098
Epoch 770, val loss: 2.1788926124572754
Epoch 780, training loss: 0.07572543621063232 = 0.006874230224639177 + 0.01 * 6.885120391845703
Epoch 780, val loss: 2.1923835277557373
Epoch 790, training loss: 0.07538513839244843 = 0.006620725151151419 + 0.01 * 6.876441478729248
Epoch 790, val loss: 2.2055504322052
Epoch 800, training loss: 0.07524414360523224 = 0.0063826232217252254 + 0.01 * 6.886152267456055
Epoch 800, val loss: 2.218341827392578
Epoch 810, training loss: 0.07496755570173264 = 0.006159184500575066 + 0.01 * 6.880837440490723
Epoch 810, val loss: 2.2306647300720215
Epoch 820, training loss: 0.07468483597040176 = 0.005948534235358238 + 0.01 * 6.873630046844482
Epoch 820, val loss: 2.242755889892578
Epoch 830, training loss: 0.07434671372175217 = 0.0057494984939694405 + 0.01 * 6.8597211837768555
Epoch 830, val loss: 2.2545406818389893
Epoch 840, training loss: 0.0742182582616806 = 0.005561366677284241 + 0.01 * 6.865689277648926
Epoch 840, val loss: 2.2660868167877197
Epoch 850, training loss: 0.07394096255302429 = 0.005383357405662537 + 0.01 * 6.85576057434082
Epoch 850, val loss: 2.277331590652466
Epoch 860, training loss: 0.07381299138069153 = 0.005214665550738573 + 0.01 * 6.859832286834717
Epoch 860, val loss: 2.2881979942321777
Epoch 870, training loss: 0.07356590777635574 = 0.005055106710642576 + 0.01 * 6.851080417633057
Epoch 870, val loss: 2.2989652156829834
Epoch 880, training loss: 0.07337333261966705 = 0.004903670400381088 + 0.01 * 6.846966743469238
Epoch 880, val loss: 2.3093137741088867
Epoch 890, training loss: 0.07331858575344086 = 0.004760460462421179 + 0.01 * 6.8558125495910645
Epoch 890, val loss: 2.3194899559020996
Epoch 900, training loss: 0.07311233133077621 = 0.004624196793884039 + 0.01 * 6.848813533782959
Epoch 900, val loss: 2.3294055461883545
Epoch 910, training loss: 0.07281220704317093 = 0.004494952037930489 + 0.01 * 6.83172607421875
Epoch 910, val loss: 2.338996171951294
Epoch 920, training loss: 0.07292970269918442 = 0.00437211012467742 + 0.01 * 6.855759143829346
Epoch 920, val loss: 2.3483664989471436
Epoch 930, training loss: 0.07260411977767944 = 0.004255179315805435 + 0.01 * 6.83489465713501
Epoch 930, val loss: 2.3575663566589355
Epoch 940, training loss: 0.0725736990571022 = 0.004143848083913326 + 0.01 * 6.842985153198242
Epoch 940, val loss: 2.3664703369140625
Epoch 950, training loss: 0.07243937253952026 = 0.004037592094391584 + 0.01 * 6.840178489685059
Epoch 950, val loss: 2.375253438949585
Epoch 960, training loss: 0.07223524898290634 = 0.003936127293854952 + 0.01 * 6.829912185668945
Epoch 960, val loss: 2.3837268352508545
Epoch 970, training loss: 0.07207751274108887 = 0.003839291399344802 + 0.01 * 6.823822021484375
Epoch 970, val loss: 2.392087697982788
Epoch 980, training loss: 0.0719369649887085 = 0.003746663685888052 + 0.01 * 6.819029808044434
Epoch 980, val loss: 2.400254011154175
Epoch 990, training loss: 0.0717356950044632 = 0.003658042987808585 + 0.01 * 6.807765483856201
Epoch 990, val loss: 2.4082181453704834
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8176067474960464
The final CL Acc:0.75556, 0.01048, The final GNN Acc:0.81655, 0.00114
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13230])
remove edge: torch.Size([2, 7950])
updated graph: torch.Size([2, 10624])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0463924407958984 = 1.9604241847991943 + 0.01 * 8.596817970275879
Epoch 0, val loss: 1.9600027799606323
Epoch 10, training loss: 2.0350594520568848 = 1.949092149734497 + 0.01 * 8.596742630004883
Epoch 10, val loss: 1.9483892917633057
Epoch 20, training loss: 2.020940065383911 = 1.9349751472473145 + 0.01 * 8.596490859985352
Epoch 20, val loss: 1.9337599277496338
Epoch 30, training loss: 2.0012240409851074 = 1.9152685403823853 + 0.01 * 8.595547676086426
Epoch 30, val loss: 1.913153052330017
Epoch 40, training loss: 1.9724558591842651 = 1.8865591287612915 + 0.01 * 8.589674949645996
Epoch 40, val loss: 1.8835002183914185
Epoch 50, training loss: 1.932721734046936 = 1.8471851348876953 + 0.01 * 8.55366039276123
Epoch 50, val loss: 1.8449947834014893
Epoch 60, training loss: 1.888493299484253 = 1.8045350313186646 + 0.01 * 8.39582347869873
Epoch 60, val loss: 1.8083012104034424
Epoch 70, training loss: 1.8528563976287842 = 1.7700974941253662 + 0.01 * 8.275890350341797
Epoch 70, val loss: 1.7822463512420654
Epoch 80, training loss: 1.8105863332748413 = 1.7290276288986206 + 0.01 * 8.155869483947754
Epoch 80, val loss: 1.7465758323669434
Epoch 90, training loss: 1.7506877183914185 = 1.671630859375 + 0.01 * 7.905691623687744
Epoch 90, val loss: 1.6953953504562378
Epoch 100, training loss: 1.6693589687347412 = 1.5930700302124023 + 0.01 * 7.628890037536621
Epoch 100, val loss: 1.626787543296814
Epoch 110, training loss: 1.572446346282959 = 1.4973987340927124 + 0.01 * 7.504756450653076
Epoch 110, val loss: 1.546330213546753
Epoch 120, training loss: 1.4707579612731934 = 1.3963650465011597 + 0.01 * 7.4392900466918945
Epoch 120, val loss: 1.4638577699661255
Epoch 130, training loss: 1.3704208135604858 = 1.296815037727356 + 0.01 * 7.3605780601501465
Epoch 130, val loss: 1.3857481479644775
Epoch 140, training loss: 1.2705297470092773 = 1.1974581480026245 + 0.01 * 7.307161331176758
Epoch 140, val loss: 1.3086522817611694
Epoch 150, training loss: 1.17026948928833 = 1.097558856010437 + 0.01 * 7.271059989929199
Epoch 150, val loss: 1.231657862663269
Epoch 160, training loss: 1.0726689100265503 = 1.0002717971801758 + 0.01 * 7.239707946777344
Epoch 160, val loss: 1.1574375629425049
Epoch 170, training loss: 0.98226398229599 = 0.9101687073707581 + 0.01 * 7.209526062011719
Epoch 170, val loss: 1.0897541046142578
Epoch 180, training loss: 0.902895450592041 = 0.831022322177887 + 0.01 * 7.187314510345459
Epoch 180, val loss: 1.0320649147033691
Epoch 190, training loss: 0.8364406824111938 = 0.7647517919540405 + 0.01 * 7.168891906738281
Epoch 190, val loss: 0.9858883619308472
Epoch 200, training loss: 0.7822098135948181 = 0.7106708884239197 + 0.01 * 7.153894424438477
Epoch 200, val loss: 0.9510963559150696
Epoch 210, training loss: 0.7374045848846436 = 0.6659591197967529 + 0.01 * 7.144548416137695
Epoch 210, val loss: 0.9247610569000244
Epoch 220, training loss: 0.6983239650726318 = 0.6269545555114746 + 0.01 * 7.136939525604248
Epoch 220, val loss: 0.9033235311508179
Epoch 230, training loss: 0.6616979241371155 = 0.5903977155685425 + 0.01 * 7.130021095275879
Epoch 230, val loss: 0.8836995363235474
Epoch 240, training loss: 0.6251009702682495 = 0.5538783669471741 + 0.01 * 7.122262001037598
Epoch 240, val loss: 0.8640584945678711
Epoch 250, training loss: 0.5873243808746338 = 0.5161946415901184 + 0.01 * 7.11297082901001
Epoch 250, val loss: 0.8438276052474976
Epoch 260, training loss: 0.5483607053756714 = 0.47734466195106506 + 0.01 * 7.101602554321289
Epoch 260, val loss: 0.82375168800354
Epoch 270, training loss: 0.5090079307556152 = 0.4380923807621002 + 0.01 * 7.091554164886475
Epoch 270, val loss: 0.8051018714904785
Epoch 280, training loss: 0.4700729548931122 = 0.3992502689361572 + 0.01 * 7.082268238067627
Epoch 280, val loss: 0.7888522148132324
Epoch 290, training loss: 0.43178609013557434 = 0.3610767722129822 + 0.01 * 7.0709309577941895
Epoch 290, val loss: 0.7752212882041931
Epoch 300, training loss: 0.3942054212093353 = 0.32361093163490295 + 0.01 * 7.059449195861816
Epoch 300, val loss: 0.7642980217933655
Epoch 310, training loss: 0.357898473739624 = 0.28732800483703613 + 0.01 * 7.057048797607422
Epoch 310, val loss: 0.7566333413124084
Epoch 320, training loss: 0.32356297969818115 = 0.2531227767467499 + 0.01 * 7.044020175933838
Epoch 320, val loss: 0.7527503967285156
Epoch 330, training loss: 0.2922596335411072 = 0.22187156975269318 + 0.01 * 7.03880500793457
Epoch 330, val loss: 0.7527782320976257
Epoch 340, training loss: 0.2643442749977112 = 0.19397054612636566 + 0.01 * 7.037374496459961
Epoch 340, val loss: 0.7567002177238464
Epoch 350, training loss: 0.23972952365875244 = 0.16940715909004211 + 0.01 * 7.032235622406006
Epoch 350, val loss: 0.763857364654541
Epoch 360, training loss: 0.21831196546554565 = 0.14799848198890686 + 0.01 * 7.031349182128906
Epoch 360, val loss: 0.7736745476722717
Epoch 370, training loss: 0.19974948465824127 = 0.1294824630022049 + 0.01 * 7.026702404022217
Epoch 370, val loss: 0.7858347296714783
Epoch 380, training loss: 0.18377530574798584 = 0.11352881789207458 + 0.01 * 7.024649143218994
Epoch 380, val loss: 0.79977947473526
Epoch 390, training loss: 0.17009897530078888 = 0.09980396926403046 + 0.01 * 7.029500961303711
Epoch 390, val loss: 0.8151028752326965
Epoch 400, training loss: 0.15823771059513092 = 0.08799684792757034 + 0.01 * 7.0240864753723145
Epoch 400, val loss: 0.8313009738922119
Epoch 410, training loss: 0.14800961315631866 = 0.07783449441194534 + 0.01 * 7.017512321472168
Epoch 410, val loss: 0.8481130003929138
Epoch 420, training loss: 0.13922211527824402 = 0.06907381862401962 + 0.01 * 7.014830589294434
Epoch 420, val loss: 0.8652330636978149
Epoch 430, training loss: 0.13168421387672424 = 0.061507925391197205 + 0.01 * 7.0176286697387695
Epoch 430, val loss: 0.8825172185897827
Epoch 440, training loss: 0.1250954121351242 = 0.054965000599622726 + 0.01 * 7.0130414962768555
Epoch 440, val loss: 0.899752140045166
Epoch 450, training loss: 0.11938133835792542 = 0.049289729446172714 + 0.01 * 7.009160995483398
Epoch 450, val loss: 0.916916012763977
Epoch 460, training loss: 0.11443177610635757 = 0.04435458034276962 + 0.01 * 7.007719993591309
Epoch 460, val loss: 0.933874785900116
Epoch 470, training loss: 0.11010714620351791 = 0.04004960507154465 + 0.01 * 7.005754470825195
Epoch 470, val loss: 0.9505822062492371
Epoch 480, training loss: 0.10628324747085571 = 0.036284055560827255 + 0.01 * 6.9999189376831055
Epoch 480, val loss: 0.9669051766395569
Epoch 490, training loss: 0.10295455157756805 = 0.03298086300492287 + 0.01 * 6.997369289398193
Epoch 490, val loss: 0.982836902141571
Epoch 500, training loss: 0.10001987218856812 = 0.030077224597334862 + 0.01 * 6.994265079498291
Epoch 500, val loss: 0.9983533024787903
Epoch 510, training loss: 0.09744826704263687 = 0.027518346905708313 + 0.01 * 6.992992401123047
Epoch 510, val loss: 1.013427734375
Epoch 520, training loss: 0.09513415396213531 = 0.02525424025952816 + 0.01 * 6.9879913330078125
Epoch 520, val loss: 1.0280476808547974
Epoch 530, training loss: 0.09307775646448135 = 0.02324158139526844 + 0.01 * 6.983617782592773
Epoch 530, val loss: 1.042256474494934
Epoch 540, training loss: 0.09131260216236115 = 0.02144494839012623 + 0.01 * 6.9867658615112305
Epoch 540, val loss: 1.0560777187347412
Epoch 550, training loss: 0.08966658264398575 = 0.019840897992253304 + 0.01 * 6.982568740844727
Epoch 550, val loss: 1.0694782733917236
Epoch 560, training loss: 0.08813340961933136 = 0.01840176060795784 + 0.01 * 6.973165512084961
Epoch 560, val loss: 1.082483172416687
Epoch 570, training loss: 0.08685257285833359 = 0.017106497660279274 + 0.01 * 6.974607944488525
Epoch 570, val loss: 1.0950943231582642
Epoch 580, training loss: 0.08569228649139404 = 0.015940576791763306 + 0.01 * 6.975171089172363
Epoch 580, val loss: 1.1072280406951904
Epoch 590, training loss: 0.084529809653759 = 0.014890131540596485 + 0.01 * 6.963967800140381
Epoch 590, val loss: 1.1190297603607178
Epoch 600, training loss: 0.08354513347148895 = 0.013938838616013527 + 0.01 * 6.960629940032959
Epoch 600, val loss: 1.1304556131362915
Epoch 610, training loss: 0.08280351012945175 = 0.013074537739157677 + 0.01 * 6.972897052764893
Epoch 610, val loss: 1.1415446996688843
Epoch 620, training loss: 0.08184140920639038 = 0.012289518490433693 + 0.01 * 6.955189228057861
Epoch 620, val loss: 1.1522481441497803
Epoch 630, training loss: 0.08108203858137131 = 0.01157299242913723 + 0.01 * 6.950904369354248
Epoch 630, val loss: 1.1626462936401367
Epoch 640, training loss: 0.08055758476257324 = 0.01091897301375866 + 0.01 * 6.963861465454102
Epoch 640, val loss: 1.1727410554885864
Epoch 650, training loss: 0.07976552844047546 = 0.010321879759430885 + 0.01 * 6.944365501403809
Epoch 650, val loss: 1.1823655366897583
Epoch 660, training loss: 0.07924669235944748 = 0.009774693287909031 + 0.01 * 6.947199821472168
Epoch 660, val loss: 1.1917160749435425
Epoch 670, training loss: 0.07862288504838943 = 0.009271861054003239 + 0.01 * 6.935102462768555
Epoch 670, val loss: 1.2008121013641357
Epoch 680, training loss: 0.07819719612598419 = 0.008809033781290054 + 0.01 * 6.938817024230957
Epoch 680, val loss: 1.209661841392517
Epoch 690, training loss: 0.07768498361110687 = 0.008381785824894905 + 0.01 * 6.930319786071777
Epoch 690, val loss: 1.2181316614151
Epoch 700, training loss: 0.07726508378982544 = 0.007986639626324177 + 0.01 * 6.927844047546387
Epoch 700, val loss: 1.226408839225769
Epoch 710, training loss: 0.07689544558525085 = 0.007620725780725479 + 0.01 * 6.92747163772583
Epoch 710, val loss: 1.2344520092010498
Epoch 720, training loss: 0.07644453644752502 = 0.007281494326889515 + 0.01 * 6.916304111480713
Epoch 720, val loss: 1.2421497106552124
Epoch 730, training loss: 0.07629401981830597 = 0.006966277491301298 + 0.01 * 6.932774066925049
Epoch 730, val loss: 1.2496459484100342
Epoch 740, training loss: 0.075821153819561 = 0.006673416588455439 + 0.01 * 6.914773464202881
Epoch 740, val loss: 1.2569798231124878
Epoch 750, training loss: 0.07551904767751694 = 0.006400524638593197 + 0.01 * 6.9118523597717285
Epoch 750, val loss: 1.2639896869659424
Epoch 760, training loss: 0.07522974908351898 = 0.006145688705146313 + 0.01 * 6.9084062576293945
Epoch 760, val loss: 1.2708948850631714
Epoch 770, training loss: 0.07502255588769913 = 0.005907304584980011 + 0.01 * 6.911525249481201
Epoch 770, val loss: 1.2775542736053467
Epoch 780, training loss: 0.07472807914018631 = 0.005684005096554756 + 0.01 * 6.904407978057861
Epoch 780, val loss: 1.2840632200241089
Epoch 790, training loss: 0.0744980052113533 = 0.0054739066399633884 + 0.01 * 6.902410507202148
Epoch 790, val loss: 1.2903996706008911
Epoch 800, training loss: 0.07425925880670547 = 0.00527577381581068 + 0.01 * 6.898348808288574
Epoch 800, val loss: 1.2963507175445557
Epoch 810, training loss: 0.0739903375506401 = 0.005088256672024727 + 0.01 * 6.890208721160889
Epoch 810, val loss: 1.3024171590805054
Epoch 820, training loss: 0.0737871527671814 = 0.004909717012196779 + 0.01 * 6.8877434730529785
Epoch 820, val loss: 1.3082677125930786
Epoch 830, training loss: 0.07379914075136185 = 0.004739424213767052 + 0.01 * 6.905971527099609
Epoch 830, val loss: 1.3138622045516968
Epoch 840, training loss: 0.07332931458950043 = 0.004577173851430416 + 0.01 * 6.875214099884033
Epoch 840, val loss: 1.3192288875579834
Epoch 850, training loss: 0.07328563928604126 = 0.004422291181981564 + 0.01 * 6.8863348960876465
Epoch 850, val loss: 1.3246819972991943
Epoch 860, training loss: 0.0729752779006958 = 0.004274642560631037 + 0.01 * 6.870063781738281
Epoch 860, val loss: 1.3298877477645874
Epoch 870, training loss: 0.07298218458890915 = 0.004133761860430241 + 0.01 * 6.884842872619629
Epoch 870, val loss: 1.3349603414535522
Epoch 880, training loss: 0.07274168729782104 = 0.003999765962362289 + 0.01 * 6.874192237854004
Epoch 880, val loss: 1.3400107622146606
Epoch 890, training loss: 0.07252977043390274 = 0.0038722737226635218 + 0.01 * 6.865749359130859
Epoch 890, val loss: 1.3448455333709717
Epoch 900, training loss: 0.07246846705675125 = 0.003751078387722373 + 0.01 * 6.871738910675049
Epoch 900, val loss: 1.3496623039245605
Epoch 910, training loss: 0.07221636921167374 = 0.00363564002327621 + 0.01 * 6.8580732345581055
Epoch 910, val loss: 1.3543734550476074
Epoch 920, training loss: 0.07219471037387848 = 0.0035262631718069315 + 0.01 * 6.866844654083252
Epoch 920, val loss: 1.3587961196899414
Epoch 930, training loss: 0.07198004424571991 = 0.003422402311116457 + 0.01 * 6.855764389038086
Epoch 930, val loss: 1.3631073236465454
Epoch 940, training loss: 0.07198727130889893 = 0.0033236786257475615 + 0.01 * 6.866358757019043
Epoch 940, val loss: 1.3675105571746826
Epoch 950, training loss: 0.07166022062301636 = 0.0032296054996550083 + 0.01 * 6.843061923980713
Epoch 950, val loss: 1.3716647624969482
Epoch 960, training loss: 0.07166382670402527 = 0.003140200860798359 + 0.01 * 6.852362155914307
Epoch 960, val loss: 1.3757216930389404
Epoch 970, training loss: 0.07154665887355804 = 0.003055138746276498 + 0.01 * 6.849152088165283
Epoch 970, val loss: 1.379805326461792
Epoch 980, training loss: 0.07137542217969894 = 0.002974054543301463 + 0.01 * 6.840136528015137
Epoch 980, val loss: 1.3835113048553467
Epoch 990, training loss: 0.07130308449268341 = 0.002897075144574046 + 0.01 * 6.840600967407227
Epoch 990, val loss: 1.3873956203460693
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 2.050686836242676 = 1.9647183418273926 + 0.01 * 8.596840858459473
Epoch 0, val loss: 1.970240831375122
Epoch 10, training loss: 2.0391056537628174 = 1.9531378746032715 + 0.01 * 8.596769332885742
Epoch 10, val loss: 1.958388090133667
Epoch 20, training loss: 2.0245838165283203 = 1.9386183023452759 + 0.01 * 8.596562385559082
Epoch 20, val loss: 1.943541169166565
Epoch 30, training loss: 2.0040550231933594 = 1.9180964231491089 + 0.01 * 8.595854759216309
Epoch 30, val loss: 1.9227731227874756
Epoch 40, training loss: 1.9736918210983276 = 1.8877763748168945 + 0.01 * 8.591540336608887
Epoch 40, val loss: 1.8927443027496338
Epoch 50, training loss: 1.9317476749420166 = 1.8461483716964722 + 0.01 * 8.55993366241455
Epoch 50, val loss: 1.8536165952682495
Epoch 60, training loss: 1.886409878730774 = 1.8023110628128052 + 0.01 * 8.409884452819824
Epoch 60, val loss: 1.8162299394607544
Epoch 70, training loss: 1.8487071990966797 = 1.7663921117782593 + 0.01 * 8.23150634765625
Epoch 70, val loss: 1.7852790355682373
Epoch 80, training loss: 1.8031986951828003 = 1.7223109006881714 + 0.01 * 8.088785171508789
Epoch 80, val loss: 1.7422244548797607
Epoch 90, training loss: 1.7419697046279907 = 1.6625816822052002 + 0.01 * 7.938807964324951
Epoch 90, val loss: 1.6871050596237183
Epoch 100, training loss: 1.6620864868164062 = 1.5841374397277832 + 0.01 * 7.7949066162109375
Epoch 100, val loss: 1.619060754776001
Epoch 110, training loss: 1.5731042623519897 = 1.497498631477356 + 0.01 * 7.5605669021606445
Epoch 110, val loss: 1.5464988946914673
Epoch 120, training loss: 1.4884103536605835 = 1.4144912958145142 + 0.01 * 7.391904354095459
Epoch 120, val loss: 1.4797074794769287
Epoch 130, training loss: 1.4109594821929932 = 1.3382010459899902 + 0.01 * 7.275838375091553
Epoch 130, val loss: 1.4198077917099
Epoch 140, training loss: 1.3378238677978516 = 1.2656515836715698 + 0.01 * 7.217223644256592
Epoch 140, val loss: 1.3638691902160645
Epoch 150, training loss: 1.2660408020019531 = 1.1942564249038696 + 0.01 * 7.178432464599609
Epoch 150, val loss: 1.3104358911514282
Epoch 160, training loss: 1.1943305730819702 = 1.1228595972061157 + 0.01 * 7.147095680236816
Epoch 160, val loss: 1.2587804794311523
Epoch 170, training loss: 1.1216634511947632 = 1.0503414869308472 + 0.01 * 7.132191181182861
Epoch 170, val loss: 1.2062454223632812
Epoch 180, training loss: 1.0479986667633057 = 0.9767125248908997 + 0.01 * 7.128617763519287
Epoch 180, val loss: 1.152436375617981
Epoch 190, training loss: 0.9744628071784973 = 0.9031884670257568 + 0.01 * 7.12743616104126
Epoch 190, val loss: 1.0980916023254395
Epoch 200, training loss: 0.9024341702461243 = 0.8311727643013 + 0.01 * 7.126142501831055
Epoch 200, val loss: 1.0447697639465332
Epoch 210, training loss: 0.8328115344047546 = 0.7615591287612915 + 0.01 * 7.125240802764893
Epoch 210, val loss: 0.9939793348312378
Epoch 220, training loss: 0.766393780708313 = 0.6951477527618408 + 0.01 * 7.1246018409729
Epoch 220, val loss: 0.947002112865448
Epoch 230, training loss: 0.7041974663734436 = 0.6329568028450012 + 0.01 * 7.124068737030029
Epoch 230, val loss: 0.9052821397781372
Epoch 240, training loss: 0.6472507119178772 = 0.5760092735290527 + 0.01 * 7.124142169952393
Epoch 240, val loss: 0.8703034520149231
Epoch 250, training loss: 0.5958206653594971 = 0.5245934128761292 + 0.01 * 7.122727870941162
Epoch 250, val loss: 0.8426690697669983
Epoch 260, training loss: 0.5492596626281738 = 0.47804123163223267 + 0.01 * 7.1218438148498535
Epoch 260, val loss: 0.8215906620025635
Epoch 270, training loss: 0.5062352418899536 = 0.43503057956695557 + 0.01 * 7.120463848114014
Epoch 270, val loss: 0.8054177761077881
Epoch 280, training loss: 0.4654737412929535 = 0.39427849650382996 + 0.01 * 7.119524002075195
Epoch 280, val loss: 0.7927228212356567
Epoch 290, training loss: 0.42623284459114075 = 0.3550560474395752 + 0.01 * 7.117679119110107
Epoch 290, val loss: 0.7830021977424622
Epoch 300, training loss: 0.3884715437889099 = 0.3173070251941681 + 0.01 * 7.116450309753418
Epoch 300, val loss: 0.7761833071708679
Epoch 310, training loss: 0.3525207042694092 = 0.2813538908958435 + 0.01 * 7.116679668426514
Epoch 310, val loss: 0.7722824215888977
Epoch 320, training loss: 0.31880807876586914 = 0.24766004085540771 + 0.01 * 7.114802837371826
Epoch 320, val loss: 0.7715305089950562
Epoch 330, training loss: 0.28780752420425415 = 0.21666835248470306 + 0.01 * 7.113918781280518
Epoch 330, val loss: 0.7739635109901428
Epoch 340, training loss: 0.25986436009407043 = 0.18871694803237915 + 0.01 * 7.114742279052734
Epoch 340, val loss: 0.7796842455863953
Epoch 350, training loss: 0.23507016897201538 = 0.16394704580307007 + 0.01 * 7.112311840057373
Epoch 350, val loss: 0.7885240316390991
Epoch 360, training loss: 0.21340063214302063 = 0.14228232204914093 + 0.01 * 7.111832141876221
Epoch 360, val loss: 0.8000512719154358
Epoch 370, training loss: 0.19461849331855774 = 0.12350497394800186 + 0.01 * 7.11135196685791
Epoch 370, val loss: 0.8136864304542542
Epoch 380, training loss: 0.1785058081150055 = 0.10735990107059479 + 0.01 * 7.114590644836426
Epoch 380, val loss: 0.8288527131080627
Epoch 390, training loss: 0.1646498143672943 = 0.09354803711175919 + 0.01 * 7.110178470611572
Epoch 390, val loss: 0.8450460433959961
Epoch 400, training loss: 0.15284350514411926 = 0.08176107704639435 + 0.01 * 7.108243942260742
Epoch 400, val loss: 0.8617919087409973
Epoch 410, training loss: 0.14280202984809875 = 0.07171038538217545 + 0.01 * 7.109164237976074
Epoch 410, val loss: 0.8787568807601929
Epoch 420, training loss: 0.1342112123966217 = 0.06314212828874588 + 0.01 * 7.106908321380615
Epoch 420, val loss: 0.8957857489585876
Epoch 430, training loss: 0.12687131762504578 = 0.05583084747195244 + 0.01 * 7.104047775268555
Epoch 430, val loss: 0.9125590920448303
Epoch 440, training loss: 0.12057922780513763 = 0.04958002269268036 + 0.01 * 7.099920749664307
Epoch 440, val loss: 0.9290961623191833
Epoch 450, training loss: 0.11521779000759125 = 0.04422055929899216 + 0.01 * 7.0997233390808105
Epoch 450, val loss: 0.9452211856842041
Epoch 460, training loss: 0.11055111140012741 = 0.03960968554019928 + 0.01 * 7.094142913818359
Epoch 460, val loss: 0.960982620716095
Epoch 470, training loss: 0.10652019828557968 = 0.03562808036804199 + 0.01 * 7.089211940765381
Epoch 470, val loss: 0.9762383103370667
Epoch 480, training loss: 0.10301420837640762 = 0.032177090644836426 + 0.01 * 7.083712100982666
Epoch 480, val loss: 0.9910365343093872
Epoch 490, training loss: 0.10001542419195175 = 0.029175221920013428 + 0.01 * 7.084020137786865
Epoch 490, val loss: 1.0053281784057617
Epoch 500, training loss: 0.09729978442192078 = 0.026553433388471603 + 0.01 * 7.0746355056762695
Epoch 500, val loss: 1.019089937210083
Epoch 510, training loss: 0.09494035691022873 = 0.02425687573850155 + 0.01 * 7.068348407745361
Epoch 510, val loss: 1.0322940349578857
Epoch 520, training loss: 0.09279942512512207 = 0.022237006574869156 + 0.01 * 7.056241512298584
Epoch 520, val loss: 1.0449514389038086
Epoch 530, training loss: 0.0911044105887413 = 0.020453045144677162 + 0.01 * 7.065136909484863
Epoch 530, val loss: 1.057123064994812
Epoch 540, training loss: 0.08929275721311569 = 0.0188764501363039 + 0.01 * 7.041630744934082
Epoch 540, val loss: 1.0688817501068115
Epoch 550, training loss: 0.08792154490947723 = 0.017478108406066895 + 0.01 * 7.0443434715271
Epoch 550, val loss: 1.079999327659607
Epoch 560, training loss: 0.08644973486661911 = 0.016231290996074677 + 0.01 * 7.021844863891602
Epoch 560, val loss: 1.0907673835754395
Epoch 570, training loss: 0.08528491109609604 = 0.015114685520529747 + 0.01 * 7.017022609710693
Epoch 570, val loss: 1.1012707948684692
Epoch 580, training loss: 0.08425182104110718 = 0.014113323763012886 + 0.01 * 7.01384973526001
Epoch 580, val loss: 1.111143708229065
Epoch 590, training loss: 0.08311872184276581 = 0.013210687786340714 + 0.01 * 6.990804195404053
Epoch 590, val loss: 1.120694875717163
Epoch 600, training loss: 0.08216903358697891 = 0.012393408454954624 + 0.01 * 6.97756290435791
Epoch 600, val loss: 1.1300047636032104
Epoch 610, training loss: 0.08137661218643188 = 0.011651905253529549 + 0.01 * 6.972471237182617
Epoch 610, val loss: 1.13900887966156
Epoch 620, training loss: 0.08072932064533234 = 0.01097913458943367 + 0.01 * 6.975019454956055
Epoch 620, val loss: 1.1474813222885132
Epoch 630, training loss: 0.07989992946386337 = 0.010365994647145271 + 0.01 * 6.953393936157227
Epoch 630, val loss: 1.1558914184570312
Epoch 640, training loss: 0.07945305109024048 = 0.009805861860513687 + 0.01 * 6.964718341827393
Epoch 640, val loss: 1.163805603981018
Epoch 650, training loss: 0.07875066250562668 = 0.009292179718613625 + 0.01 * 6.9458489418029785
Epoch 650, val loss: 1.171541690826416
Epoch 660, training loss: 0.07823467254638672 = 0.008819486945867538 + 0.01 * 6.941519260406494
Epoch 660, val loss: 1.1790077686309814
Epoch 670, training loss: 0.07762786746025085 = 0.00838388130068779 + 0.01 * 6.924399375915527
Epoch 670, val loss: 1.1862741708755493
Epoch 680, training loss: 0.07727449387311935 = 0.007981427013874054 + 0.01 * 6.929306507110596
Epoch 680, val loss: 1.1933436393737793
Epoch 690, training loss: 0.07679694890975952 = 0.007609331980347633 + 0.01 * 6.91876220703125
Epoch 690, val loss: 1.200127124786377
Epoch 700, training loss: 0.07633357495069504 = 0.00726438919082284 + 0.01 * 6.906918525695801
Epoch 700, val loss: 1.2067216634750366
Epoch 710, training loss: 0.07590785622596741 = 0.006943578831851482 + 0.01 * 6.896428108215332
Epoch 710, val loss: 1.213091254234314
Epoch 720, training loss: 0.07554781436920166 = 0.006645623128861189 + 0.01 * 6.890219688415527
Epoch 720, val loss: 1.2193710803985596
Epoch 730, training loss: 0.07533171027898788 = 0.006367004942148924 + 0.01 * 6.896470546722412
Epoch 730, val loss: 1.2253713607788086
Epoch 740, training loss: 0.07497702538967133 = 0.006107867229729891 + 0.01 * 6.886915683746338
Epoch 740, val loss: 1.2311899662017822
Epoch 750, training loss: 0.07487135380506516 = 0.005865240935236216 + 0.01 * 6.900611400604248
Epoch 750, val loss: 1.2370704412460327
Epoch 760, training loss: 0.07429049909114838 = 0.005638270638883114 + 0.01 * 6.865222930908203
Epoch 760, val loss: 1.242462396621704
Epoch 770, training loss: 0.0740094780921936 = 0.005426070187240839 + 0.01 * 6.858340740203857
Epoch 770, val loss: 1.2479608058929443
Epoch 780, training loss: 0.07386422157287598 = 0.0052268048748373985 + 0.01 * 6.863741874694824
Epoch 780, val loss: 1.2532469034194946
Epoch 790, training loss: 0.07379940897226334 = 0.0050397166050970554 + 0.01 * 6.875969409942627
Epoch 790, val loss: 1.2583534717559814
Epoch 800, training loss: 0.07328139245510101 = 0.00486411526799202 + 0.01 * 6.841728210449219
Epoch 800, val loss: 1.2633761167526245
Epoch 810, training loss: 0.07299774140119553 = 0.004698519594967365 + 0.01 * 6.829922199249268
Epoch 810, val loss: 1.2682081460952759
Epoch 820, training loss: 0.07303968816995621 = 0.004542524926364422 + 0.01 * 6.849716663360596
Epoch 820, val loss: 1.2729593515396118
Epoch 830, training loss: 0.07281419634819031 = 0.004395422991365194 + 0.01 * 6.841876983642578
Epoch 830, val loss: 1.2775650024414062
Epoch 840, training loss: 0.07251804322004318 = 0.004256455693393946 + 0.01 * 6.82615852355957
Epoch 840, val loss: 1.2820295095443726
Epoch 850, training loss: 0.07237376272678375 = 0.004124998115003109 + 0.01 * 6.824876308441162
Epoch 850, val loss: 1.2865262031555176
Epoch 860, training loss: 0.07214673608541489 = 0.004000477958470583 + 0.01 * 6.814626216888428
Epoch 860, val loss: 1.2907838821411133
Epoch 870, training loss: 0.07195502519607544 = 0.0038829066324979067 + 0.01 * 6.807211875915527
Epoch 870, val loss: 1.295006275177002
Epoch 880, training loss: 0.07188808172941208 = 0.003771200543269515 + 0.01 * 6.81168794631958
Epoch 880, val loss: 1.2991342544555664
Epoch 890, training loss: 0.07175199687480927 = 0.003665252821519971 + 0.01 * 6.808674335479736
Epoch 890, val loss: 1.3030732870101929
Epoch 900, training loss: 0.07152850925922394 = 0.0035646562464535236 + 0.01 * 6.796385288238525
Epoch 900, val loss: 1.307037353515625
Epoch 910, training loss: 0.0713568776845932 = 0.003469139337539673 + 0.01 * 6.788774013519287
Epoch 910, val loss: 1.310801386833191
Epoch 920, training loss: 0.07153033465147018 = 0.0033780799712985754 + 0.01 * 6.815225601196289
Epoch 920, val loss: 1.3145523071289062
Epoch 930, training loss: 0.07119216024875641 = 0.0032914297189563513 + 0.01 * 6.790073394775391
Epoch 930, val loss: 1.318198323249817
Epoch 940, training loss: 0.07119918614625931 = 0.0032088791485875845 + 0.01 * 6.7990312576293945
Epoch 940, val loss: 1.3217812776565552
Epoch 950, training loss: 0.07099118828773499 = 0.0031300578266382217 + 0.01 * 6.786113262176514
Epoch 950, val loss: 1.3252911567687988
Epoch 960, training loss: 0.07074817270040512 = 0.003054925473406911 + 0.01 * 6.769324779510498
Epoch 960, val loss: 1.3286761045455933
Epoch 970, training loss: 0.07086911052465439 = 0.002983279060572386 + 0.01 * 6.788583278656006
Epoch 970, val loss: 1.3321623802185059
Epoch 980, training loss: 0.0708482638001442 = 0.0029146624729037285 + 0.01 * 6.793360233306885
Epoch 980, val loss: 1.3353514671325684
Epoch 990, training loss: 0.07050905376672745 = 0.0028490396216511726 + 0.01 * 6.766001224517822
Epoch 990, val loss: 1.3385605812072754
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 2.037576913833618 = 1.9516088962554932 + 0.01 * 8.596807479858398
Epoch 0, val loss: 1.9514503479003906
Epoch 10, training loss: 2.02744460105896 = 1.9414770603179932 + 0.01 * 8.59675407409668
Epoch 10, val loss: 1.9418617486953735
Epoch 20, training loss: 2.015364408493042 = 1.9293991327285767 + 0.01 * 8.59653377532959
Epoch 20, val loss: 1.929800271987915
Epoch 30, training loss: 1.9986629486083984 = 1.9127048254013062 + 0.01 * 8.595813751220703
Epoch 30, val loss: 1.912643313407898
Epoch 40, training loss: 1.9741450548171997 = 1.8882250785827637 + 0.01 * 8.592001914978027
Epoch 40, val loss: 1.8873252868652344
Epoch 50, training loss: 1.9387555122375488 = 1.8530921936035156 + 0.01 * 8.566327095031738
Epoch 50, val loss: 1.8519419431686401
Epoch 60, training loss: 1.895031213760376 = 1.8106145858764648 + 0.01 * 8.441668510437012
Epoch 60, val loss: 1.8122788667678833
Epoch 70, training loss: 1.8550642728805542 = 1.7726786136627197 + 0.01 * 8.238563537597656
Epoch 70, val loss: 1.7801655530929565
Epoch 80, training loss: 1.8125897645950317 = 1.7323622703552246 + 0.01 * 8.022754669189453
Epoch 80, val loss: 1.7439675331115723
Epoch 90, training loss: 1.7530229091644287 = 1.6757361888885498 + 0.01 * 7.728677749633789
Epoch 90, val loss: 1.6928284168243408
Epoch 100, training loss: 1.6751136779785156 = 1.5997205972671509 + 0.01 * 7.539307594299316
Epoch 100, val loss: 1.6267707347869873
Epoch 110, training loss: 1.5795698165893555 = 1.5052366256713867 + 0.01 * 7.433325290679932
Epoch 110, val loss: 1.546783447265625
Epoch 120, training loss: 1.4754217863082886 = 1.401620626449585 + 0.01 * 7.380110740661621
Epoch 120, val loss: 1.4593276977539062
Epoch 130, training loss: 1.3693984746932983 = 1.295925259590149 + 0.01 * 7.347318649291992
Epoch 130, val loss: 1.3712705373764038
Epoch 140, training loss: 1.2633306980133057 = 1.1901772022247314 + 0.01 * 7.315344333648682
Epoch 140, val loss: 1.2841193675994873
Epoch 150, training loss: 1.1586289405822754 = 1.085745096206665 + 0.01 * 7.2883830070495605
Epoch 150, val loss: 1.1986987590789795
Epoch 160, training loss: 1.0574939250946045 = 0.9848341941833496 + 0.01 * 7.26597261428833
Epoch 160, val loss: 1.1165239810943604
Epoch 170, training loss: 0.9626836776733398 = 0.8902835249900818 + 0.01 * 7.240016937255859
Epoch 170, val loss: 1.040031909942627
Epoch 180, training loss: 0.8766849040985107 = 0.8045581579208374 + 0.01 * 7.212677478790283
Epoch 180, val loss: 0.9717394709587097
Epoch 190, training loss: 0.8008989095687866 = 0.7289931774139404 + 0.01 * 7.190570831298828
Epoch 190, val loss: 0.9130297899246216
Epoch 200, training loss: 0.7347477078437805 = 0.663024365901947 + 0.01 * 7.172333717346191
Epoch 200, val loss: 0.8641504645347595
Epoch 210, training loss: 0.6758543252944946 = 0.604131281375885 + 0.01 * 7.17230749130249
Epoch 210, val loss: 0.824083149433136
Epoch 220, training loss: 0.6210272312164307 = 0.549504816532135 + 0.01 * 7.152242660522461
Epoch 220, val loss: 0.7911638021469116
Epoch 230, training loss: 0.5687603950500488 = 0.4973152279853821 + 0.01 * 7.144515037536621
Epoch 230, val loss: 0.7635661959648132
Epoch 240, training loss: 0.5181602835655212 = 0.4467848539352417 + 0.01 * 7.137545108795166
Epoch 240, val loss: 0.739797830581665
Epoch 250, training loss: 0.4691016674041748 = 0.39777255058288574 + 0.01 * 7.132911682128906
Epoch 250, val loss: 0.7188925743103027
Epoch 260, training loss: 0.4216749966144562 = 0.35038480162620544 + 0.01 * 7.1290202140808105
Epoch 260, val loss: 0.7003871202468872
Epoch 270, training loss: 0.37638476490974426 = 0.3051299750804901 + 0.01 * 7.1254801750183105
Epoch 270, val loss: 0.6844750046730042
Epoch 280, training loss: 0.33401793241500854 = 0.2628232538700104 + 0.01 * 7.119469165802002
Epoch 280, val loss: 0.6713995933532715
Epoch 290, training loss: 0.2956295609474182 = 0.22442626953125 + 0.01 * 7.120330333709717
Epoch 290, val loss: 0.6616637110710144
Epoch 300, training loss: 0.2618594467639923 = 0.19072313606739044 + 0.01 * 7.113630294799805
Epoch 300, val loss: 0.6556161046028137
Epoch 310, training loss: 0.23299527168273926 = 0.16196738183498383 + 0.01 * 7.102789402008057
Epoch 310, val loss: 0.6532376408576965
Epoch 320, training loss: 0.20903387665748596 = 0.1379029005765915 + 0.01 * 7.113097190856934
Epoch 320, val loss: 0.6542931795120239
Epoch 330, training loss: 0.18889018893241882 = 0.11795734614133835 + 0.01 * 7.0932841300964355
Epoch 330, val loss: 0.6583908796310425
Epoch 340, training loss: 0.17221464216709137 = 0.10140836983919144 + 0.01 * 7.08062744140625
Epoch 340, val loss: 0.6648310422897339
Epoch 350, training loss: 0.15840722620487213 = 0.08762690424919128 + 0.01 * 7.078032493591309
Epoch 350, val loss: 0.6730648875236511
Epoch 360, training loss: 0.1467430591583252 = 0.07608798146247864 + 0.01 * 7.065508842468262
Epoch 360, val loss: 0.6825780868530273
Epoch 370, training loss: 0.13696187734603882 = 0.06639855355024338 + 0.01 * 7.056332588195801
Epoch 370, val loss: 0.6928711533546448
Epoch 380, training loss: 0.12875455617904663 = 0.05823495239019394 + 0.01 * 7.0519609451293945
Epoch 380, val loss: 0.7037191987037659
Epoch 390, training loss: 0.12167076766490936 = 0.05131987854838371 + 0.01 * 7.035088539123535
Epoch 390, val loss: 0.7149444222450256
Epoch 400, training loss: 0.11596395075321198 = 0.04544324055314064 + 0.01 * 7.052070617675781
Epoch 400, val loss: 0.7265044450759888
Epoch 410, training loss: 0.11064021289348602 = 0.040449079126119614 + 0.01 * 7.019114017486572
Epoch 410, val loss: 0.7381011843681335
Epoch 420, training loss: 0.10633210092782974 = 0.03618772327899933 + 0.01 * 7.014437675476074
Epoch 420, val loss: 0.7495823502540588
Epoch 430, training loss: 0.10259465873241425 = 0.03253662586212158 + 0.01 * 7.005803108215332
Epoch 430, val loss: 0.761029064655304
Epoch 440, training loss: 0.09937602281570435 = 0.0293952114880085 + 0.01 * 6.998081207275391
Epoch 440, val loss: 0.772286593914032
Epoch 450, training loss: 0.09653548151254654 = 0.02668517269194126 + 0.01 * 6.9850311279296875
Epoch 450, val loss: 0.7833249568939209
Epoch 460, training loss: 0.09409604966640472 = 0.02433784306049347 + 0.01 * 6.975821018218994
Epoch 460, val loss: 0.7940107583999634
Epoch 470, training loss: 0.09213219583034515 = 0.022290648892521858 + 0.01 * 6.98415470123291
Epoch 470, val loss: 0.8044013977050781
Epoch 480, training loss: 0.09000860154628754 = 0.020498644560575485 + 0.01 * 6.950995922088623
Epoch 480, val loss: 0.8145625591278076
Epoch 490, training loss: 0.08843990415334702 = 0.01891970820724964 + 0.01 * 6.952019691467285
Epoch 490, val loss: 0.8243379592895508
Epoch 500, training loss: 0.08690381795167923 = 0.017524218186736107 + 0.01 * 6.937960147857666
Epoch 500, val loss: 0.8338846564292908
Epoch 510, training loss: 0.08563035726547241 = 0.016284335404634476 + 0.01 * 6.934601783752441
Epoch 510, val loss: 0.8431092500686646
Epoch 520, training loss: 0.0847240686416626 = 0.015176129527390003 + 0.01 * 6.954793930053711
Epoch 520, val loss: 0.8520458936691284
Epoch 530, training loss: 0.08340928703546524 = 0.014184285886585712 + 0.01 * 6.922499656677246
Epoch 530, val loss: 0.860781192779541
Epoch 540, training loss: 0.08254684507846832 = 0.013291199691593647 + 0.01 * 6.925564765930176
Epoch 540, val loss: 0.8691990971565247
Epoch 550, training loss: 0.08150507509708405 = 0.012484046630561352 + 0.01 * 6.902102947235107
Epoch 550, val loss: 0.8774735331535339
Epoch 560, training loss: 0.08075588196516037 = 0.011752395890653133 + 0.01 * 6.900348663330078
Epoch 560, val loss: 0.885374903678894
Epoch 570, training loss: 0.07998284697532654 = 0.0110881756991148 + 0.01 * 6.889466762542725
Epoch 570, val loss: 0.8931547403335571
Epoch 580, training loss: 0.07926974445581436 = 0.010481426492333412 + 0.01 * 6.8788323402404785
Epoch 580, val loss: 0.9006739854812622
Epoch 590, training loss: 0.07882697135210037 = 0.00992500502616167 + 0.01 * 6.890196323394775
Epoch 590, val loss: 0.9080603122711182
Epoch 600, training loss: 0.07814270257949829 = 0.009414272382855415 + 0.01 * 6.872843265533447
Epoch 600, val loss: 0.9152204990386963
Epoch 610, training loss: 0.07765866816043854 = 0.008944232016801834 + 0.01 * 6.871444225311279
Epoch 610, val loss: 0.922223687171936
Epoch 620, training loss: 0.0771864727139473 = 0.008510406129062176 + 0.01 * 6.867607116699219
Epoch 620, val loss: 0.9290666580200195
Epoch 630, training loss: 0.07672072947025299 = 0.008109080605208874 + 0.01 * 6.861165523529053
Epoch 630, val loss: 0.9357190728187561
Epoch 640, training loss: 0.07640084624290466 = 0.00773744797334075 + 0.01 * 6.866339683532715
Epoch 640, val loss: 0.9421505331993103
Epoch 650, training loss: 0.0758286714553833 = 0.00739278644323349 + 0.01 * 6.843588829040527
Epoch 650, val loss: 0.9485371708869934
Epoch 660, training loss: 0.07560180872678757 = 0.007072109263390303 + 0.01 * 6.852970123291016
Epoch 660, val loss: 0.9547255635261536
Epoch 670, training loss: 0.07535643130540848 = 0.006773925386369228 + 0.01 * 6.858250617980957
Epoch 670, val loss: 0.9607454538345337
Epoch 680, training loss: 0.07480841130018234 = 0.006496114190667868 + 0.01 * 6.8312296867370605
Epoch 680, val loss: 0.9666438102722168
Epoch 690, training loss: 0.07449617236852646 = 0.00623615738004446 + 0.01 * 6.8260016441345215
Epoch 690, val loss: 0.9724043011665344
Epoch 700, training loss: 0.07481420785188675 = 0.005992950405925512 + 0.01 * 6.882126331329346
Epoch 700, val loss: 0.9780951738357544
Epoch 710, training loss: 0.0740237608551979 = 0.0057652052491903305 + 0.01 * 6.825855255126953
Epoch 710, val loss: 0.9835468530654907
Epoch 720, training loss: 0.0739036500453949 = 0.005551692098379135 + 0.01 * 6.835196018218994
Epoch 720, val loss: 0.98896723985672
Epoch 730, training loss: 0.07356436550617218 = 0.0053511010482907295 + 0.01 * 6.82132625579834
Epoch 730, val loss: 0.9941980242729187
Epoch 740, training loss: 0.07332424819469452 = 0.005162594374269247 + 0.01 * 6.816165447235107
Epoch 740, val loss: 0.9993380904197693
Epoch 750, training loss: 0.0730808675289154 = 0.004985107108950615 + 0.01 * 6.809576034545898
Epoch 750, val loss: 1.0043681859970093
Epoch 760, training loss: 0.07282866537570953 = 0.004817879758775234 + 0.01 * 6.801078796386719
Epoch 760, val loss: 1.0093300342559814
Epoch 770, training loss: 0.07275579124689102 = 0.004659891594201326 + 0.01 * 6.8095903396606445
Epoch 770, val loss: 1.0140981674194336
Epoch 780, training loss: 0.07259230315685272 = 0.004510841332376003 + 0.01 * 6.8081464767456055
Epoch 780, val loss: 1.018830418586731
Epoch 790, training loss: 0.0724063515663147 = 0.004369605798274279 + 0.01 * 6.803674697875977
Epoch 790, val loss: 1.0234551429748535
Epoch 800, training loss: 0.07235866039991379 = 0.004235921427607536 + 0.01 * 6.8122735023498535
Epoch 800, val loss: 1.0279899835586548
Epoch 810, training loss: 0.0720711275935173 = 0.0041093663312494755 + 0.01 * 6.796175956726074
Epoch 810, val loss: 1.0323697328567505
Epoch 820, training loss: 0.07214267551898956 = 0.0039894175715744495 + 0.01 * 6.815325736999512
Epoch 820, val loss: 1.0366867780685425
Epoch 830, training loss: 0.07174330949783325 = 0.00387558969669044 + 0.01 * 6.786771774291992
Epoch 830, val loss: 1.0409693717956543
Epoch 840, training loss: 0.07152208685874939 = 0.0037672589533030987 + 0.01 * 6.775482654571533
Epoch 840, val loss: 1.045114278793335
Epoch 850, training loss: 0.07154761999845505 = 0.003664152231067419 + 0.01 * 6.788346767425537
Epoch 850, val loss: 1.0491578578948975
Epoch 860, training loss: 0.07128017395734787 = 0.003566119121387601 + 0.01 * 6.771406173706055
Epoch 860, val loss: 1.0531781911849976
Epoch 870, training loss: 0.07151824235916138 = 0.003472614800557494 + 0.01 * 6.804563522338867
Epoch 870, val loss: 1.0570709705352783
Epoch 880, training loss: 0.07114220410585403 = 0.0033837577793747187 + 0.01 * 6.775845050811768
Epoch 880, val loss: 1.0609514713287354
Epoch 890, training loss: 0.07098443061113358 = 0.003298881696537137 + 0.01 * 6.768555164337158
Epoch 890, val loss: 1.0646785497665405
Epoch 900, training loss: 0.07090166211128235 = 0.003217639634385705 + 0.01 * 6.768402576446533
Epoch 900, val loss: 1.0683813095092773
Epoch 910, training loss: 0.07093895971775055 = 0.003140188055112958 + 0.01 * 6.779877185821533
Epoch 910, val loss: 1.0720250606536865
Epoch 920, training loss: 0.07059270888566971 = 0.003066067351028323 + 0.01 * 6.752664089202881
Epoch 920, val loss: 1.0755962133407593
Epoch 930, training loss: 0.07067801803350449 = 0.0029950421303510666 + 0.01 * 6.7682976722717285
Epoch 930, val loss: 1.0790597200393677
Epoch 940, training loss: 0.07061447948217392 = 0.002927144058048725 + 0.01 * 6.768733501434326
Epoch 940, val loss: 1.0825204849243164
Epoch 950, training loss: 0.07035787403583527 = 0.002862176625058055 + 0.01 * 6.749569892883301
Epoch 950, val loss: 1.0858275890350342
Epoch 960, training loss: 0.07031507045030594 = 0.00280002667568624 + 0.01 * 6.751504421234131
Epoch 960, val loss: 1.089132308959961
Epoch 970, training loss: 0.07044287025928497 = 0.002740493742749095 + 0.01 * 6.770237922668457
Epoch 970, val loss: 1.0924103260040283
Epoch 980, training loss: 0.070160411298275 = 0.002683190628886223 + 0.01 * 6.747722148895264
Epoch 980, val loss: 1.0955225229263306
Epoch 990, training loss: 0.070367731153965 = 0.0026282065082341433 + 0.01 * 6.773952484130859
Epoch 990, val loss: 1.0986469984054565
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8408012651555088
The final CL Acc:0.80617, 0.00972, The final GNN Acc:0.84010, 0.00179
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9414])
updated graph: torch.Size([2, 10478])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0565054416656494 = 1.970537543296814 + 0.01 * 8.596790313720703
Epoch 0, val loss: 1.976362705230713
Epoch 10, training loss: 2.0456159114837646 = 1.959648609161377 + 0.01 * 8.596726417541504
Epoch 10, val loss: 1.9651968479156494
Epoch 20, training loss: 2.0322718620300293 = 1.9463070631027222 + 0.01 * 8.596487045288086
Epoch 20, val loss: 1.9512860774993896
Epoch 30, training loss: 2.0134243965148926 = 1.927468180656433 + 0.01 * 8.595632553100586
Epoch 30, val loss: 1.931626558303833
Epoch 40, training loss: 1.9854695796966553 = 1.8995634317398071 + 0.01 * 8.590612411499023
Epoch 40, val loss: 1.9028244018554688
Epoch 50, training loss: 1.946285367012024 = 1.8606775999069214 + 0.01 * 8.560773849487305
Epoch 50, val loss: 1.8641979694366455
Epoch 60, training loss: 1.9031407833099365 = 1.8187224864959717 + 0.01 * 8.441834449768066
Epoch 60, val loss: 1.8260833024978638
Epoch 70, training loss: 1.872434377670288 = 1.789445161819458 + 0.01 * 8.29891586303711
Epoch 70, val loss: 1.8017297983169556
Epoch 80, training loss: 1.840795874595642 = 1.7593830823898315 + 0.01 * 8.141278266906738
Epoch 80, val loss: 1.7742429971694946
Epoch 90, training loss: 1.7963073253631592 = 1.7177811861038208 + 0.01 * 7.8526082038879395
Epoch 90, val loss: 1.7374072074890137
Epoch 100, training loss: 1.7353593111038208 = 1.6592189073562622 + 0.01 * 7.614035606384277
Epoch 100, val loss: 1.6867626905441284
Epoch 110, training loss: 1.6551049947738647 = 1.5804853439331055 + 0.01 * 7.461965084075928
Epoch 110, val loss: 1.6195073127746582
Epoch 120, training loss: 1.5611671209335327 = 1.4875036478042603 + 0.01 * 7.36634635925293
Epoch 120, val loss: 1.5409505367279053
Epoch 130, training loss: 1.4629050493240356 = 1.3899465799331665 + 0.01 * 7.295846939086914
Epoch 130, val loss: 1.459635615348816
Epoch 140, training loss: 1.3656444549560547 = 1.293096899986267 + 0.01 * 7.254749774932861
Epoch 140, val loss: 1.3802964687347412
Epoch 150, training loss: 1.269693374633789 = 1.197424054145813 + 0.01 * 7.226934432983398
Epoch 150, val loss: 1.3027374744415283
Epoch 160, training loss: 1.1772844791412354 = 1.1053146123886108 + 0.01 * 7.196989059448242
Epoch 160, val loss: 1.229476809501648
Epoch 170, training loss: 1.0914379358291626 = 1.019755482673645 + 0.01 * 7.168243885040283
Epoch 170, val loss: 1.1633522510528564
Epoch 180, training loss: 1.0120149850845337 = 0.9404481649398804 + 0.01 * 7.15668249130249
Epoch 180, val loss: 1.1041101217269897
Epoch 190, training loss: 0.9356157183647156 = 0.8641324043273926 + 0.01 * 7.148329257965088
Epoch 190, val loss: 1.0485810041427612
Epoch 200, training loss: 0.8595446944236755 = 0.7881216406822205 + 0.01 * 7.142306804656982
Epoch 200, val loss: 0.9942488074302673
Epoch 210, training loss: 0.7837694883346558 = 0.7124063372612 + 0.01 * 7.13631534576416
Epoch 210, val loss: 0.9413715600967407
Epoch 220, training loss: 0.7106434106826782 = 0.6393374800682068 + 0.01 * 7.130590915679932
Epoch 220, val loss: 0.8924926519393921
Epoch 230, training loss: 0.6427472233772278 = 0.5714854001998901 + 0.01 * 7.12618350982666
Epoch 230, val loss: 0.850832462310791
Epoch 240, training loss: 0.5812327861785889 = 0.5100312232971191 + 0.01 * 7.12015438079834
Epoch 240, val loss: 0.8180782794952393
Epoch 250, training loss: 0.5263785123825073 = 0.45523205399513245 + 0.01 * 7.114646911621094
Epoch 250, val loss: 0.7944415807723999
Epoch 260, training loss: 0.47794803977012634 = 0.4068688154220581 + 0.01 * 7.10792350769043
Epoch 260, val loss: 0.7787385582923889
Epoch 270, training loss: 0.4355402886867523 = 0.36452600359916687 + 0.01 * 7.101429462432861
Epoch 270, val loss: 0.7693133354187012
Epoch 280, training loss: 0.3987021744251251 = 0.3276572525501251 + 0.01 * 7.1044921875
Epoch 280, val loss: 0.7650892734527588
Epoch 290, training loss: 0.3664432168006897 = 0.2955198287963867 + 0.01 * 7.092338562011719
Epoch 290, val loss: 0.765182375907898
Epoch 300, training loss: 0.3380874991416931 = 0.26724958419799805 + 0.01 * 7.0837907791137695
Epoch 300, val loss: 0.7686328291893005
Epoch 310, training loss: 0.3127202093601227 = 0.2419726699590683 + 0.01 * 7.074753761291504
Epoch 310, val loss: 0.7746652960777283
Epoch 320, training loss: 0.2896242141723633 = 0.21892942488193512 + 0.01 * 7.069479942321777
Epoch 320, val loss: 0.7827073335647583
Epoch 330, training loss: 0.26820841431617737 = 0.1975756287574768 + 0.01 * 7.063279628753662
Epoch 330, val loss: 0.7922760248184204
Epoch 340, training loss: 0.24818024039268494 = 0.17762413620948792 + 0.01 * 7.0556111335754395
Epoch 340, val loss: 0.8030499219894409
Epoch 350, training loss: 0.22956261038780212 = 0.15905340015888214 + 0.01 * 7.050920486450195
Epoch 350, val loss: 0.8149060606956482
Epoch 360, training loss: 0.2124490886926651 = 0.14198848605155945 + 0.01 * 7.046060085296631
Epoch 360, val loss: 0.8276888132095337
Epoch 370, training loss: 0.19698885083198547 = 0.12651950120925903 + 0.01 * 7.04693603515625
Epoch 370, val loss: 0.8413317799568176
Epoch 380, training loss: 0.18307486176490784 = 0.11265594512224197 + 0.01 * 7.041891098022461
Epoch 380, val loss: 0.8558568358421326
Epoch 390, training loss: 0.1706792563199997 = 0.10033419728279114 + 0.01 * 7.034506320953369
Epoch 390, val loss: 0.8711945414543152
Epoch 400, training loss: 0.15979671478271484 = 0.08944140374660492 + 0.01 * 7.035531044006348
Epoch 400, val loss: 0.8869989514350891
Epoch 410, training loss: 0.15011340379714966 = 0.07983774691820145 + 0.01 * 7.027566432952881
Epoch 410, val loss: 0.9031584858894348
Epoch 420, training loss: 0.14162670075893402 = 0.0713842362165451 + 0.01 * 7.024246692657471
Epoch 420, val loss: 0.9194989800453186
Epoch 430, training loss: 0.13415151834487915 = 0.06395380944013596 + 0.01 * 7.019770622253418
Epoch 430, val loss: 0.935929000377655
Epoch 440, training loss: 0.1276252269744873 = 0.057430148124694824 + 0.01 * 7.019507884979248
Epoch 440, val loss: 0.9522212147712708
Epoch 450, training loss: 0.12183408439159393 = 0.05170547217130661 + 0.01 * 7.012861728668213
Epoch 450, val loss: 0.9683676362037659
Epoch 460, training loss: 0.11680878698825836 = 0.04667895659804344 + 0.01 * 7.0129828453063965
Epoch 460, val loss: 0.9842579960823059
Epoch 470, training loss: 0.11234042048454285 = 0.04226446524262428 + 0.01 * 7.007595062255859
Epoch 470, val loss: 0.9999338984489441
Epoch 480, training loss: 0.10841435194015503 = 0.03838418796658516 + 0.01 * 7.003015995025635
Epoch 480, val loss: 1.0153305530548096
Epoch 490, training loss: 0.10511814057826996 = 0.03496994078159332 + 0.01 * 7.014820098876953
Epoch 490, val loss: 1.0304241180419922
Epoch 500, training loss: 0.10199850052595139 = 0.03196188062429428 + 0.01 * 7.003662109375
Epoch 500, val loss: 1.0451326370239258
Epoch 510, training loss: 0.09929124265909195 = 0.029302282258868217 + 0.01 * 6.998895645141602
Epoch 510, val loss: 1.0594924688339233
Epoch 520, training loss: 0.09686123579740524 = 0.026944255456328392 + 0.01 * 6.991698265075684
Epoch 520, val loss: 1.0734150409698486
Epoch 530, training loss: 0.09475947916507721 = 0.024846017360687256 + 0.01 * 6.99134635925293
Epoch 530, val loss: 1.0869388580322266
Epoch 540, training loss: 0.09295976161956787 = 0.022976061329245567 + 0.01 * 6.9983696937561035
Epoch 540, val loss: 1.0999186038970947
Epoch 550, training loss: 0.09108222275972366 = 0.02130318619310856 + 0.01 * 6.977903842926025
Epoch 550, val loss: 1.1125541925430298
Epoch 560, training loss: 0.08966369926929474 = 0.019800784066319466 + 0.01 * 6.986291885375977
Epoch 560, val loss: 1.1247456073760986
Epoch 570, training loss: 0.08827443420886993 = 0.01844961568713188 + 0.01 * 6.982481479644775
Epoch 570, val loss: 1.1364490985870361
Epoch 580, training loss: 0.08688758313655853 = 0.017230242490768433 + 0.01 * 6.965734481811523
Epoch 580, val loss: 1.1477429866790771
Epoch 590, training loss: 0.08585748076438904 = 0.016125913709402084 + 0.01 * 6.973156452178955
Epoch 590, val loss: 1.1586377620697021
Epoch 600, training loss: 0.0848766416311264 = 0.01512459572404623 + 0.01 * 6.975204944610596
Epoch 600, val loss: 1.1691426038742065
Epoch 610, training loss: 0.08380796760320663 = 0.014214722439646721 + 0.01 * 6.959324836730957
Epoch 610, val loss: 1.179275393486023
Epoch 620, training loss: 0.08290598541498184 = 0.013384249992668629 + 0.01 * 6.952173709869385
Epoch 620, val loss: 1.1890640258789062
Epoch 630, training loss: 0.08206819742918015 = 0.012624050490558147 + 0.01 * 6.944415092468262
Epoch 630, val loss: 1.1985328197479248
Epoch 640, training loss: 0.0814133808016777 = 0.011927729472517967 + 0.01 * 6.9485650062561035
Epoch 640, val loss: 1.2077503204345703
Epoch 650, training loss: 0.08081472665071487 = 0.011289931833744049 + 0.01 * 6.952479839324951
Epoch 650, val loss: 1.2165173292160034
Epoch 660, training loss: 0.08022569864988327 = 0.01070437766611576 + 0.01 * 6.952132225036621
Epoch 660, val loss: 1.224952220916748
Epoch 670, training loss: 0.07945674657821655 = 0.010166856460273266 + 0.01 * 6.928989410400391
Epoch 670, val loss: 1.2332251071929932
Epoch 680, training loss: 0.07892094552516937 = 0.009669993072748184 + 0.01 * 6.925096035003662
Epoch 680, val loss: 1.2411493062973022
Epoch 690, training loss: 0.07864546775817871 = 0.00921077560633421 + 0.01 * 6.943469524383545
Epoch 690, val loss: 1.2487891912460327
Epoch 700, training loss: 0.07800232619047165 = 0.008786387741565704 + 0.01 * 6.921594142913818
Epoch 700, val loss: 1.2561931610107422
Epoch 710, training loss: 0.07777848839759827 = 0.008392687886953354 + 0.01 * 6.938580513000488
Epoch 710, val loss: 1.2633306980133057
Epoch 720, training loss: 0.07709857076406479 = 0.008027457632124424 + 0.01 * 6.907111644744873
Epoch 720, val loss: 1.2703219652175903
Epoch 730, training loss: 0.0766105055809021 = 0.007687476929277182 + 0.01 * 6.892302513122559
Epoch 730, val loss: 1.276953101158142
Epoch 740, training loss: 0.07645972818136215 = 0.0073706586845219135 + 0.01 * 6.908906936645508
Epoch 740, val loss: 1.2835240364074707
Epoch 750, training loss: 0.07604484260082245 = 0.007074989378452301 + 0.01 * 6.8969855308532715
Epoch 750, val loss: 1.2897707223892212
Epoch 760, training loss: 0.07559175044298172 = 0.006798886694014072 + 0.01 * 6.879286289215088
Epoch 760, val loss: 1.295824408531189
Epoch 770, training loss: 0.07537095993757248 = 0.006540557835251093 + 0.01 * 6.883040428161621
Epoch 770, val loss: 1.3018066883087158
Epoch 780, training loss: 0.07519660145044327 = 0.006298184860497713 + 0.01 * 6.889841556549072
Epoch 780, val loss: 1.307506799697876
Epoch 790, training loss: 0.07474474608898163 = 0.006070935633033514 + 0.01 * 6.867381572723389
Epoch 790, val loss: 1.3129901885986328
Epoch 800, training loss: 0.07448659837245941 = 0.005857469514012337 + 0.01 * 6.862913131713867
Epoch 800, val loss: 1.3184071779251099
Epoch 810, training loss: 0.07427917420864105 = 0.005656752735376358 + 0.01 * 6.862241744995117
Epoch 810, val loss: 1.323589563369751
Epoch 820, training loss: 0.07404979318380356 = 0.005467826500535011 + 0.01 * 6.858197212219238
Epoch 820, val loss: 1.3287410736083984
Epoch 830, training loss: 0.07375030219554901 = 0.005289566703140736 + 0.01 * 6.846073627471924
Epoch 830, val loss: 1.3335732221603394
Epoch 840, training loss: 0.07366908341646194 = 0.005121102090924978 + 0.01 * 6.854797840118408
Epoch 840, val loss: 1.33841872215271
Epoch 850, training loss: 0.07343288511037827 = 0.004961860366165638 + 0.01 * 6.847102165222168
Epoch 850, val loss: 1.3430591821670532
Epoch 860, training loss: 0.07323262095451355 = 0.004811236169189215 + 0.01 * 6.842138767242432
Epoch 860, val loss: 1.3474903106689453
Epoch 870, training loss: 0.07296917587518692 = 0.0046687498688697815 + 0.01 * 6.830042839050293
Epoch 870, val loss: 1.3519172668457031
Epoch 880, training loss: 0.07285012304782867 = 0.00453355023637414 + 0.01 * 6.831657409667969
Epoch 880, val loss: 1.356224536895752
Epoch 890, training loss: 0.0727885365486145 = 0.004405208397656679 + 0.01 * 6.8383331298828125
Epoch 890, val loss: 1.3605097532272339
Epoch 900, training loss: 0.07279741019010544 = 0.004283246584236622 + 0.01 * 6.85141658782959
Epoch 900, val loss: 1.3644262552261353
Epoch 910, training loss: 0.07239009439945221 = 0.0041672163642942905 + 0.01 * 6.822288513183594
Epoch 910, val loss: 1.3684263229370117
Epoch 920, training loss: 0.0723894014954567 = 0.004056885372847319 + 0.01 * 6.833251476287842
Epoch 920, val loss: 1.3722530603408813
Epoch 930, training loss: 0.07207931578159332 = 0.003951855003833771 + 0.01 * 6.812746524810791
Epoch 930, val loss: 1.3760621547698975
Epoch 940, training loss: 0.07198353111743927 = 0.003851605812087655 + 0.01 * 6.813192367553711
Epoch 940, val loss: 1.3797889947891235
Epoch 950, training loss: 0.07175296545028687 = 0.003755891229957342 + 0.01 * 6.799707889556885
Epoch 950, val loss: 1.3832722902297974
Epoch 960, training loss: 0.07165983319282532 = 0.003664660034701228 + 0.01 * 6.799517631530762
Epoch 960, val loss: 1.3867943286895752
Epoch 970, training loss: 0.07178692519664764 = 0.0035775136202573776 + 0.01 * 6.82094144821167
Epoch 970, val loss: 1.390135645866394
Epoch 980, training loss: 0.07152533531188965 = 0.0034943826030939817 + 0.01 * 6.803095817565918
Epoch 980, val loss: 1.3935844898223877
Epoch 990, training loss: 0.07126697152853012 = 0.003414719132706523 + 0.01 * 6.7852253913879395
Epoch 990, val loss: 1.3967739343643188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 2.0332484245300293 = 1.9472805261611938 + 0.01 * 8.596799850463867
Epoch 0, val loss: 1.9567276239395142
Epoch 10, training loss: 2.023772954940796 = 1.9378055334091187 + 0.01 * 8.596748352050781
Epoch 10, val loss: 1.9473029375076294
Epoch 20, training loss: 2.011964797973633 = 1.925999402999878 + 0.01 * 8.59654426574707
Epoch 20, val loss: 1.9349594116210938
Epoch 30, training loss: 1.9951317310333252 = 1.9091733694076538 + 0.01 * 8.595832824707031
Epoch 30, val loss: 1.9169808626174927
Epoch 40, training loss: 1.9701818227767944 = 1.8842626810073853 + 0.01 * 8.591919898986816
Epoch 40, val loss: 1.8903942108154297
Epoch 50, training loss: 1.935315728187561 = 1.8496454954147339 + 0.01 * 8.56701946258545
Epoch 50, val loss: 1.8547534942626953
Epoch 60, training loss: 1.8961892127990723 = 1.811688780784607 + 0.01 * 8.450039863586426
Epoch 60, val loss: 1.8186395168304443
Epoch 70, training loss: 1.8633674383163452 = 1.7803558111190796 + 0.01 * 8.301163673400879
Epoch 70, val loss: 1.7903064489364624
Epoch 80, training loss: 1.824416160583496 = 1.743312954902649 + 0.01 * 8.11031723022461
Epoch 80, val loss: 1.757028579711914
Epoch 90, training loss: 1.7699999809265137 = 1.6920325756072998 + 0.01 * 7.796740531921387
Epoch 90, val loss: 1.712448000907898
Epoch 100, training loss: 1.6973841190338135 = 1.6213264465332031 + 0.01 * 7.605762958526611
Epoch 100, val loss: 1.6511564254760742
Epoch 110, training loss: 1.607894778251648 = 1.5327134132385254 + 0.01 * 7.518136024475098
Epoch 110, val loss: 1.5753757953643799
Epoch 120, training loss: 1.5079247951507568 = 1.4333362579345703 + 0.01 * 7.45885705947876
Epoch 120, val loss: 1.491392731666565
Epoch 130, training loss: 1.4046564102172852 = 1.3305649757385254 + 0.01 * 7.409137725830078
Epoch 130, val loss: 1.4062074422836304
Epoch 140, training loss: 1.3025840520858765 = 1.2289199829101562 + 0.01 * 7.366408348083496
Epoch 140, val loss: 1.324210524559021
Epoch 150, training loss: 1.2064567804336548 = 1.1331983804702759 + 0.01 * 7.32583475112915
Epoch 150, val loss: 1.2496240139007568
Epoch 160, training loss: 1.1205536127090454 = 1.0476016998291016 + 0.01 * 7.295194149017334
Epoch 160, val loss: 1.1856379508972168
Epoch 170, training loss: 1.044999599456787 = 0.9722867608070374 + 0.01 * 7.271287441253662
Epoch 170, val loss: 1.1326289176940918
Epoch 180, training loss: 0.9758347272872925 = 0.9033534526824951 + 0.01 * 7.2481303215026855
Epoch 180, val loss: 1.086691975593567
Epoch 190, training loss: 0.9081430435180664 = 0.8359139561653137 + 0.01 * 7.222907066345215
Epoch 190, val loss: 1.0436179637908936
Epoch 200, training loss: 0.8385037183761597 = 0.7665144801139832 + 0.01 * 7.198920726776123
Epoch 200, val loss: 1.0000357627868652
Epoch 210, training loss: 0.7659856081008911 = 0.6941728591918945 + 0.01 * 7.181273937225342
Epoch 210, val loss: 0.9549622535705566
Epoch 220, training loss: 0.6918845176696777 = 0.6202176809310913 + 0.01 * 7.16668701171875
Epoch 220, val loss: 0.9092916250228882
Epoch 230, training loss: 0.6190672516822815 = 0.5475018620491028 + 0.01 * 7.156540393829346
Epoch 230, val loss: 0.8651567101478577
Epoch 240, training loss: 0.5510184168815613 = 0.4794982373714447 + 0.01 * 7.152020454406738
Epoch 240, val loss: 0.825812041759491
Epoch 250, training loss: 0.49030208587646484 = 0.4188854992389679 + 0.01 * 7.141657829284668
Epoch 250, val loss: 0.7939631342887878
Epoch 260, training loss: 0.43791836500167847 = 0.3665580749511719 + 0.01 * 7.1360297203063965
Epoch 260, val loss: 0.7707133889198303
Epoch 270, training loss: 0.3929610848426819 = 0.32157212495803833 + 0.01 * 7.138896942138672
Epoch 270, val loss: 0.7552904486656189
Epoch 280, training loss: 0.3536212146282196 = 0.2823188900947571 + 0.01 * 7.130231857299805
Epoch 280, val loss: 0.7457946538925171
Epoch 290, training loss: 0.3185685873031616 = 0.24733410775661469 + 0.01 * 7.123449325561523
Epoch 290, val loss: 0.7409152388572693
Epoch 300, training loss: 0.2868569493293762 = 0.21569088101387024 + 0.01 * 7.11660623550415
Epoch 300, val loss: 0.7393608689308167
Epoch 310, training loss: 0.2581675052642822 = 0.18704234063625336 + 0.01 * 7.112517833709717
Epoch 310, val loss: 0.7402908802032471
Epoch 320, training loss: 0.23248782753944397 = 0.16145066916942596 + 0.01 * 7.103715896606445
Epoch 320, val loss: 0.7436431050300598
Epoch 330, training loss: 0.21028882265090942 = 0.13903497159481049 + 0.01 * 7.12538480758667
Epoch 330, val loss: 0.7494238018989563
Epoch 340, training loss: 0.19074130058288574 = 0.1197945848107338 + 0.01 * 7.094672203063965
Epoch 340, val loss: 0.7575669288635254
Epoch 350, training loss: 0.1743704080581665 = 0.1034986600279808 + 0.01 * 7.0871758460998535
Epoch 350, val loss: 0.767658531665802
Epoch 360, training loss: 0.16065838932991028 = 0.08980971574783325 + 0.01 * 7.084866523742676
Epoch 360, val loss: 0.7792399525642395
Epoch 370, training loss: 0.1491135209798813 = 0.07836887985467911 + 0.01 * 7.074464321136475
Epoch 370, val loss: 0.7919084429740906
Epoch 380, training loss: 0.13941287994384766 = 0.06878339499235153 + 0.01 * 7.062948703765869
Epoch 380, val loss: 0.8052495718002319
Epoch 390, training loss: 0.13129499554634094 = 0.060730382800102234 + 0.01 * 7.056461811065674
Epoch 390, val loss: 0.8189572095870972
Epoch 400, training loss: 0.1243644654750824 = 0.053928274661302567 + 0.01 * 7.043619155883789
Epoch 400, val loss: 0.8328532576560974
Epoch 410, training loss: 0.118598073720932 = 0.0481562577188015 + 0.01 * 7.044181823730469
Epoch 410, val loss: 0.8466430306434631
Epoch 420, training loss: 0.11353051662445068 = 0.043229833245277405 + 0.01 * 7.030068397521973
Epoch 420, val loss: 0.8602169752120972
Epoch 430, training loss: 0.10921783000230789 = 0.0389983132481575 + 0.01 * 7.021952152252197
Epoch 430, val loss: 0.8735385537147522
Epoch 440, training loss: 0.10557019710540771 = 0.03534514456987381 + 0.01 * 7.022505283355713
Epoch 440, val loss: 0.8865780830383301
Epoch 450, training loss: 0.10240919888019562 = 0.03217501938343048 + 0.01 * 7.023417949676514
Epoch 450, val loss: 0.8992677330970764
Epoch 460, training loss: 0.09946614503860474 = 0.02941199392080307 + 0.01 * 7.005414962768555
Epoch 460, val loss: 0.9115908741950989
Epoch 470, training loss: 0.09693482518196106 = 0.026988836005330086 + 0.01 * 6.994599342346191
Epoch 470, val loss: 0.923621416091919
Epoch 480, training loss: 0.0951857641339302 = 0.024852311238646507 + 0.01 * 7.0333452224731445
Epoch 480, val loss: 0.9352722764015198
Epoch 490, training loss: 0.09285128116607666 = 0.02296629175543785 + 0.01 * 6.988498687744141
Epoch 490, val loss: 0.9465497136116028
Epoch 500, training loss: 0.09118147194385529 = 0.021288493648171425 + 0.01 * 6.989297866821289
Epoch 500, val loss: 0.9575101733207703
Epoch 510, training loss: 0.08944518864154816 = 0.019791092723608017 + 0.01 * 6.965409278869629
Epoch 510, val loss: 0.9681200981140137
Epoch 520, training loss: 0.08838008344173431 = 0.01844753697514534 + 0.01 * 6.993254661560059
Epoch 520, val loss: 0.9784046411514282
Epoch 530, training loss: 0.08680952340364456 = 0.017241785302758217 + 0.01 * 6.9567742347717285
Epoch 530, val loss: 0.9883770942687988
Epoch 540, training loss: 0.08566981554031372 = 0.016154808923602104 + 0.01 * 6.951500415802002
Epoch 540, val loss: 0.9980501532554626
Epoch 550, training loss: 0.08463311195373535 = 0.015171024948358536 + 0.01 * 6.94620943069458
Epoch 550, val loss: 1.0074076652526855
Epoch 560, training loss: 0.08402757346630096 = 0.014278015121817589 + 0.01 * 6.9749555587768555
Epoch 560, val loss: 1.0164865255355835
Epoch 570, training loss: 0.08281863480806351 = 0.013466203585267067 + 0.01 * 6.935243606567383
Epoch 570, val loss: 1.0253026485443115
Epoch 580, training loss: 0.08230382949113846 = 0.01272542029619217 + 0.01 * 6.957840919494629
Epoch 580, val loss: 1.0338433980941772
Epoch 590, training loss: 0.08128511160612106 = 0.012049518525600433 + 0.01 * 6.923559188842773
Epoch 590, val loss: 1.0420811176300049
Epoch 600, training loss: 0.08053909242153168 = 0.011429534293711185 + 0.01 * 6.910955905914307
Epoch 600, val loss: 1.0501184463500977
Epoch 610, training loss: 0.07991782575845718 = 0.010858878493309021 + 0.01 * 6.905895233154297
Epoch 610, val loss: 1.0578895807266235
Epoch 620, training loss: 0.07942908257246017 = 0.010332023724913597 + 0.01 * 6.909706115722656
Epoch 620, val loss: 1.0654950141906738
Epoch 630, training loss: 0.0789572149515152 = 0.009845958091318607 + 0.01 * 6.911125659942627
Epoch 630, val loss: 1.072869062423706
Epoch 640, training loss: 0.07831019163131714 = 0.009396500885486603 + 0.01 * 6.891369342803955
Epoch 640, val loss: 1.0799872875213623
Epoch 650, training loss: 0.07777093350887299 = 0.008980724960565567 + 0.01 * 6.879021644592285
Epoch 650, val loss: 1.08689284324646
Epoch 660, training loss: 0.07756626605987549 = 0.00859425961971283 + 0.01 * 6.897201061248779
Epoch 660, val loss: 1.0936647653579712
Epoch 670, training loss: 0.07693475484848022 = 0.008234834298491478 + 0.01 * 6.869991779327393
Epoch 670, val loss: 1.1002647876739502
Epoch 680, training loss: 0.07677571475505829 = 0.007898952811956406 + 0.01 * 6.887676239013672
Epoch 680, val loss: 1.1066547632217407
Epoch 690, training loss: 0.076323501765728 = 0.007584054488688707 + 0.01 * 6.8739447593688965
Epoch 690, val loss: 1.1129122972488403
Epoch 700, training loss: 0.07583210617303848 = 0.007288441993296146 + 0.01 * 6.854366779327393
Epoch 700, val loss: 1.119032621383667
Epoch 710, training loss: 0.07563816010951996 = 0.007012099958956242 + 0.01 * 6.862606048583984
Epoch 710, val loss: 1.1248960494995117
Epoch 720, training loss: 0.07529181987047195 = 0.006751467008143663 + 0.01 * 6.854034900665283
Epoch 720, val loss: 1.1307144165039062
Epoch 730, training loss: 0.07529252022504807 = 0.0065075065940618515 + 0.01 * 6.8785014152526855
Epoch 730, val loss: 1.1363050937652588
Epoch 740, training loss: 0.07461421936750412 = 0.006278402172029018 + 0.01 * 6.833581924438477
Epoch 740, val loss: 1.1417789459228516
Epoch 750, training loss: 0.07434455305337906 = 0.006062356289476156 + 0.01 * 6.828219890594482
Epoch 750, val loss: 1.1470965147018433
Epoch 760, training loss: 0.07434433698654175 = 0.005858516786247492 + 0.01 * 6.8485822677612305
Epoch 760, val loss: 1.152306079864502
Epoch 770, training loss: 0.07393471151590347 = 0.005666804499924183 + 0.01 * 6.8267903327941895
Epoch 770, val loss: 1.1574245691299438
Epoch 780, training loss: 0.07403893768787384 = 0.005485234782099724 + 0.01 * 6.85537052154541
Epoch 780, val loss: 1.1623414754867554
Epoch 790, training loss: 0.07347655296325684 = 0.005313446745276451 + 0.01 * 6.816310405731201
Epoch 790, val loss: 1.1671994924545288
Epoch 800, training loss: 0.073459692299366 = 0.005150605458766222 + 0.01 * 6.83090877532959
Epoch 800, val loss: 1.1719205379486084
Epoch 810, training loss: 0.07324296981096268 = 0.004996811039745808 + 0.01 * 6.824615478515625
Epoch 810, val loss: 1.176572561264038
Epoch 820, training loss: 0.07322242110967636 = 0.004850235767662525 + 0.01 * 6.837218284606934
Epoch 820, val loss: 1.1810425519943237
Epoch 830, training loss: 0.07290016114711761 = 0.004711877554655075 + 0.01 * 6.818828105926514
Epoch 830, val loss: 1.1854826211929321
Epoch 840, training loss: 0.0726642981171608 = 0.004579802975058556 + 0.01 * 6.8084492683410645
Epoch 840, val loss: 1.1897352933883667
Epoch 850, training loss: 0.07264073193073273 = 0.00445457873865962 + 0.01 * 6.818615913391113
Epoch 850, val loss: 1.1939481496810913
Epoch 860, training loss: 0.07227538526058197 = 0.004335029516369104 + 0.01 * 6.7940354347229
Epoch 860, val loss: 1.1980851888656616
Epoch 870, training loss: 0.07213477045297623 = 0.004220969043672085 + 0.01 * 6.791380405426025
Epoch 870, val loss: 1.2020819187164307
Epoch 880, training loss: 0.07221806794404984 = 0.004112243186682463 + 0.01 * 6.810582637786865
Epoch 880, val loss: 1.2060344219207764
Epoch 890, training loss: 0.07202666252851486 = 0.004008329939097166 + 0.01 * 6.801833629608154
Epoch 890, val loss: 1.2098581790924072
Epoch 900, training loss: 0.07213514298200607 = 0.003909161314368248 + 0.01 * 6.822597980499268
Epoch 900, val loss: 1.2136720418930054
Epoch 910, training loss: 0.07168662548065186 = 0.00381462718360126 + 0.01 * 6.787199974060059
Epoch 910, val loss: 1.217350721359253
Epoch 920, training loss: 0.07146632671356201 = 0.0037239519879221916 + 0.01 * 6.774238109588623
Epoch 920, val loss: 1.2209190130233765
Epoch 930, training loss: 0.07131411880254745 = 0.003637297311797738 + 0.01 * 6.767682075500488
Epoch 930, val loss: 1.2244350910186768
Epoch 940, training loss: 0.0712909996509552 = 0.0035544803831726313 + 0.01 * 6.773652076721191
Epoch 940, val loss: 1.2279024124145508
Epoch 950, training loss: 0.07157144695520401 = 0.0034748599864542484 + 0.01 * 6.809659004211426
Epoch 950, val loss: 1.2312629222869873
Epoch 960, training loss: 0.07120756804943085 = 0.0033986547496169806 + 0.01 * 6.780891418457031
Epoch 960, val loss: 1.2346155643463135
Epoch 970, training loss: 0.07104892283678055 = 0.00332558318041265 + 0.01 * 6.772334575653076
Epoch 970, val loss: 1.2377965450286865
Epoch 980, training loss: 0.07081712782382965 = 0.0032552636694163084 + 0.01 * 6.756186485290527
Epoch 980, val loss: 1.240966796875
Epoch 990, training loss: 0.07080340385437012 = 0.0031877930741757154 + 0.01 * 6.761561393737793
Epoch 990, val loss: 1.2440861463546753
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 2.0241146087646484 = 1.9381464719772339 + 0.01 * 8.59681224822998
Epoch 0, val loss: 1.9407312870025635
Epoch 10, training loss: 2.0149710178375244 = 1.929003357887268 + 0.01 * 8.596776008605957
Epoch 10, val loss: 1.93128502368927
Epoch 20, training loss: 2.0041348934173584 = 1.9181689023971558 + 0.01 * 8.596595764160156
Epoch 20, val loss: 1.9197909832000732
Epoch 30, training loss: 1.9892351627349854 = 1.9032745361328125 + 0.01 * 8.596068382263184
Epoch 30, val loss: 1.90370774269104
Epoch 40, training loss: 1.9673620462417603 = 1.8814257383346558 + 0.01 * 8.593626022338867
Epoch 40, val loss: 1.879977822303772
Epoch 50, training loss: 1.936234474182129 = 1.8504751920700073 + 0.01 * 8.575923919677734
Epoch 50, val loss: 1.8471559286117554
Epoch 60, training loss: 1.8983256816864014 = 1.8135349750518799 + 0.01 * 8.479072570800781
Epoch 60, val loss: 1.8110884428024292
Epoch 70, training loss: 1.861595869064331 = 1.779384970664978 + 0.01 * 8.221087455749512
Epoch 70, val loss: 1.7826849222183228
Epoch 80, training loss: 1.81900155544281 = 1.7403926849365234 + 0.01 * 7.860889434814453
Epoch 80, val loss: 1.751518726348877
Epoch 90, training loss: 1.7616074085235596 = 1.6858290433883667 + 0.01 * 7.577835559844971
Epoch 90, val loss: 1.704411506652832
Epoch 100, training loss: 1.6859707832336426 = 1.6108970642089844 + 0.01 * 7.5073747634887695
Epoch 100, val loss: 1.6376056671142578
Epoch 110, training loss: 1.5911275148391724 = 1.5165693759918213 + 0.01 * 7.455815315246582
Epoch 110, val loss: 1.5580989122390747
Epoch 120, training loss: 1.4846593141555786 = 1.4105713367462158 + 0.01 * 7.408798694610596
Epoch 120, val loss: 1.4707591533660889
Epoch 130, training loss: 1.3769983053207397 = 1.3033055067062378 + 0.01 * 7.369281768798828
Epoch 130, val loss: 1.3846932649612427
Epoch 140, training loss: 1.2740811109542847 = 1.2007807493209839 + 0.01 * 7.330033779144287
Epoch 140, val loss: 1.3038115501403809
Epoch 150, training loss: 1.1782021522521973 = 1.105329990386963 + 0.01 * 7.2872161865234375
Epoch 150, val loss: 1.2302465438842773
Epoch 160, training loss: 1.0906531810760498 = 1.0181231498718262 + 0.01 * 7.253004550933838
Epoch 160, val loss: 1.1650744676589966
Epoch 170, training loss: 1.0101866722106934 = 0.9379536509513855 + 0.01 * 7.223300933837891
Epoch 170, val loss: 1.1077848672866821
Epoch 180, training loss: 0.9339137077331543 = 0.8619050979614258 + 0.01 * 7.200862407684326
Epoch 180, val loss: 1.0555553436279297
Epoch 190, training loss: 0.8596223592758179 = 0.7878656983375549 + 0.01 * 7.175669193267822
Epoch 190, val loss: 1.0061088800430298
Epoch 200, training loss: 0.7872427701950073 = 0.7156418561935425 + 0.01 * 7.160091876983643
Epoch 200, val loss: 0.9590862393379211
Epoch 210, training loss: 0.7176671624183655 = 0.6462131142616272 + 0.01 * 7.145405292510986
Epoch 210, val loss: 0.9157008528709412
Epoch 220, training loss: 0.6517916917800903 = 0.5804072618484497 + 0.01 * 7.138444423675537
Epoch 220, val loss: 0.8773943185806274
Epoch 230, training loss: 0.5899693965911865 = 0.5187126994132996 + 0.01 * 7.125669956207275
Epoch 230, val loss: 0.8453265428543091
Epoch 240, training loss: 0.5326725840568542 = 0.4614429473876953 + 0.01 * 7.122964382171631
Epoch 240, val loss: 0.8196706771850586
Epoch 250, training loss: 0.4798888862133026 = 0.4087851047515869 + 0.01 * 7.110378265380859
Epoch 250, val loss: 0.7999671697616577
Epoch 260, training loss: 0.4317462146282196 = 0.3607279062271118 + 0.01 * 7.101831912994385
Epoch 260, val loss: 0.7852557301521301
Epoch 270, training loss: 0.38816913962364197 = 0.3171689212322235 + 0.01 * 7.100021839141846
Epoch 270, val loss: 0.7750639319419861
Epoch 280, training loss: 0.3488936722278595 = 0.2780321538448334 + 0.01 * 7.086152076721191
Epoch 280, val loss: 0.7689513564109802
Epoch 290, training loss: 0.31399452686309814 = 0.24320822954177856 + 0.01 * 7.078631401062012
Epoch 290, val loss: 0.7666075229644775
Epoch 300, training loss: 0.2834004759788513 = 0.21258032321929932 + 0.01 * 7.082015037536621
Epoch 300, val loss: 0.7677628397941589
Epoch 310, training loss: 0.2565169036388397 = 0.18586724996566772 + 0.01 * 7.06496524810791
Epoch 310, val loss: 0.7721688151359558
Epoch 320, training loss: 0.23324888944625854 = 0.16269411146640778 + 0.01 * 7.055479049682617
Epoch 320, val loss: 0.7795926928520203
Epoch 330, training loss: 0.21319833397865295 = 0.14266365766525269 + 0.01 * 7.053468227386475
Epoch 330, val loss: 0.7897517681121826
Epoch 340, training loss: 0.19600221514701843 = 0.12541332840919495 + 0.01 * 7.0588884353637695
Epoch 340, val loss: 0.8021185994148254
Epoch 350, training loss: 0.18095101416110992 = 0.11055389791727066 + 0.01 * 7.0397114753723145
Epoch 350, val loss: 0.8162068128585815
Epoch 360, training loss: 0.16802701354026794 = 0.09772689640522003 + 0.01 * 7.0300116539001465
Epoch 360, val loss: 0.8315461277961731
Epoch 370, training loss: 0.15686073899269104 = 0.0866437703371048 + 0.01 * 7.021696090698242
Epoch 370, val loss: 0.8477159738540649
Epoch 380, training loss: 0.14727506041526794 = 0.07705524563789368 + 0.01 * 7.021982669830322
Epoch 380, val loss: 0.8642760515213013
Epoch 390, training loss: 0.13902223110198975 = 0.06873892992734909 + 0.01 * 7.028330326080322
Epoch 390, val loss: 0.880987286567688
Epoch 400, training loss: 0.13155591487884521 = 0.0615200400352478 + 0.01 * 7.003588676452637
Epoch 400, val loss: 0.8975773453712463
Epoch 410, training loss: 0.12524734437465668 = 0.05523679777979851 + 0.01 * 7.0010552406311035
Epoch 410, val loss: 0.9139519333839417
Epoch 420, training loss: 0.11970522999763489 = 0.04975434020161629 + 0.01 * 6.995089054107666
Epoch 420, val loss: 0.930016279220581
Epoch 430, training loss: 0.11492280662059784 = 0.044963084161281586 + 0.01 * 6.995972633361816
Epoch 430, val loss: 0.945738673210144
Epoch 440, training loss: 0.11065586656332016 = 0.04076618701219559 + 0.01 * 6.9889678955078125
Epoch 440, val loss: 0.961008608341217
Epoch 450, training loss: 0.10687640309333801 = 0.03708046302199364 + 0.01 * 6.9795942306518555
Epoch 450, val loss: 0.9758465886116028
Epoch 460, training loss: 0.10360637307167053 = 0.03383607789874077 + 0.01 * 6.977030277252197
Epoch 460, val loss: 0.9902585744857788
Epoch 470, training loss: 0.10064326226711273 = 0.030972305685281754 + 0.01 * 6.967095851898193
Epoch 470, val loss: 1.0041942596435547
Epoch 480, training loss: 0.09823820739984512 = 0.02843797765672207 + 0.01 * 6.98002290725708
Epoch 480, val loss: 1.0177346467971802
Epoch 490, training loss: 0.09578025341033936 = 0.026190873235464096 + 0.01 * 6.9589385986328125
Epoch 490, val loss: 1.0307759046554565
Epoch 500, training loss: 0.09377838671207428 = 0.024189617484807968 + 0.01 * 6.9588775634765625
Epoch 500, val loss: 1.0434191226959229
Epoch 510, training loss: 0.09192972630262375 = 0.02240302972495556 + 0.01 * 6.952669620513916
Epoch 510, val loss: 1.055670142173767
Epoch 520, training loss: 0.09030455350875854 = 0.02080199122428894 + 0.01 * 6.95025634765625
Epoch 520, val loss: 1.0674998760223389
Epoch 530, training loss: 0.08874645829200745 = 0.019365746527910233 + 0.01 * 6.938071250915527
Epoch 530, val loss: 1.0789434909820557
Epoch 540, training loss: 0.08746175467967987 = 0.018073072656989098 + 0.01 * 6.938868045806885
Epoch 540, val loss: 1.0900323390960693
Epoch 550, training loss: 0.08633862435817719 = 0.016904814168810844 + 0.01 * 6.943380832672119
Epoch 550, val loss: 1.1007155179977417
Epoch 560, training loss: 0.0851290300488472 = 0.01584766060113907 + 0.01 * 6.928136825561523
Epoch 560, val loss: 1.1111198663711548
Epoch 570, training loss: 0.0843268409371376 = 0.014885746873915195 + 0.01 * 6.9441094398498535
Epoch 570, val loss: 1.1211674213409424
Epoch 580, training loss: 0.08323319256305695 = 0.014011950232088566 + 0.01 * 6.92212438583374
Epoch 580, val loss: 1.1308773756027222
Epoch 590, training loss: 0.08242463320493698 = 0.013214826583862305 + 0.01 * 6.920980453491211
Epoch 590, val loss: 1.140316367149353
Epoch 600, training loss: 0.0815950259566307 = 0.012485245242714882 + 0.01 * 6.910978317260742
Epoch 600, val loss: 1.149399995803833
Epoch 610, training loss: 0.08095438778400421 = 0.011816603131592274 + 0.01 * 6.913778305053711
Epoch 610, val loss: 1.1582081317901611
Epoch 620, training loss: 0.08021797984838486 = 0.011202274821698666 + 0.01 * 6.9015703201293945
Epoch 620, val loss: 1.1667250394821167
Epoch 630, training loss: 0.0796433836221695 = 0.010637473315000534 + 0.01 * 6.900590896606445
Epoch 630, val loss: 1.1750166416168213
Epoch 640, training loss: 0.07908277958631516 = 0.010116119869053364 + 0.01 * 6.896666049957275
Epoch 640, val loss: 1.1829811334609985
Epoch 650, training loss: 0.07850238680839539 = 0.00963460374623537 + 0.01 * 6.886778354644775
Epoch 650, val loss: 1.1908209323883057
Epoch 660, training loss: 0.07818369567394257 = 0.009188373573124409 + 0.01 * 6.899532318115234
Epoch 660, val loss: 1.19841730594635
Epoch 670, training loss: 0.07757198065519333 = 0.008775402791798115 + 0.01 * 6.879657745361328
Epoch 670, val loss: 1.2057162523269653
Epoch 680, training loss: 0.0772678330540657 = 0.008390789851546288 + 0.01 * 6.887704372406006
Epoch 680, val loss: 1.212830901145935
Epoch 690, training loss: 0.07681907713413239 = 0.00803395640105009 + 0.01 * 6.878512382507324
Epoch 690, val loss: 1.2197860479354858
Epoch 700, training loss: 0.0766996219754219 = 0.007700743619352579 + 0.01 * 6.899888515472412
Epoch 700, val loss: 1.2265177965164185
Epoch 710, training loss: 0.07604951411485672 = 0.007390245329588652 + 0.01 * 6.865926742553711
Epoch 710, val loss: 1.2330365180969238
Epoch 720, training loss: 0.07577767968177795 = 0.007099559064954519 + 0.01 * 6.867812156677246
Epoch 720, val loss: 1.2394427061080933
Epoch 730, training loss: 0.07547950744628906 = 0.006827444303780794 + 0.01 * 6.865206718444824
Epoch 730, val loss: 1.2456283569335938
Epoch 740, training loss: 0.07519747316837311 = 0.0065722097642719746 + 0.01 * 6.862526893615723
Epoch 740, val loss: 1.2516684532165527
Epoch 750, training loss: 0.07489226013422012 = 0.006332944612950087 + 0.01 * 6.855932235717773
Epoch 750, val loss: 1.2575106620788574
Epoch 760, training loss: 0.07458539307117462 = 0.0061075324192643166 + 0.01 * 6.847785949707031
Epoch 760, val loss: 1.263177752494812
Epoch 770, training loss: 0.07440243661403656 = 0.005896029993891716 + 0.01 * 6.850640773773193
Epoch 770, val loss: 1.2687668800354004
Epoch 780, training loss: 0.07425538450479507 = 0.005696390755474567 + 0.01 * 6.855899333953857
Epoch 780, val loss: 1.2741305828094482
Epoch 790, training loss: 0.0740223079919815 = 0.005508324131369591 + 0.01 * 6.85139799118042
Epoch 790, val loss: 1.2794245481491089
Epoch 800, training loss: 0.07372920215129852 = 0.005330375861376524 + 0.01 * 6.8398823738098145
Epoch 800, val loss: 1.2845432758331299
Epoch 810, training loss: 0.07365364581346512 = 0.005162067245692015 + 0.01 * 6.849157810211182
Epoch 810, val loss: 1.2895591259002686
Epoch 820, training loss: 0.07333613932132721 = 0.005002681631594896 + 0.01 * 6.833345413208008
Epoch 820, val loss: 1.2944080829620361
Epoch 830, training loss: 0.07317288964986801 = 0.004851901903748512 + 0.01 * 6.832098960876465
Epoch 830, val loss: 1.2991679906845093
Epoch 840, training loss: 0.07311320304870605 = 0.004709542728960514 + 0.01 * 6.840366363525391
Epoch 840, val loss: 1.3038588762283325
Epoch 850, training loss: 0.07294433563947678 = 0.004573628306388855 + 0.01 * 6.837070941925049
Epoch 850, val loss: 1.3082693815231323
Epoch 860, training loss: 0.07265476882457733 = 0.004444683436304331 + 0.01 * 6.821009159088135
Epoch 860, val loss: 1.3127416372299194
Epoch 870, training loss: 0.07240702956914902 = 0.004322108346968889 + 0.01 * 6.8084917068481445
Epoch 870, val loss: 1.3170253038406372
Epoch 880, training loss: 0.07248163968324661 = 0.0042055449448525906 + 0.01 * 6.827609062194824
Epoch 880, val loss: 1.3212616443634033
Epoch 890, training loss: 0.07215959578752518 = 0.004094302654266357 + 0.01 * 6.806529521942139
Epoch 890, val loss: 1.325234293937683
Epoch 900, training loss: 0.07252002507448196 = 0.0039880545809865 + 0.01 * 6.853196620941162
Epoch 900, val loss: 1.3292176723480225
Epoch 910, training loss: 0.07189162075519562 = 0.003887694329023361 + 0.01 * 6.800393104553223
Epoch 910, val loss: 1.3331201076507568
Epoch 920, training loss: 0.0720066949725151 = 0.003790902206674218 + 0.01 * 6.821579456329346
Epoch 920, val loss: 1.336897611618042
Epoch 930, training loss: 0.07173751294612885 = 0.0036993653047829866 + 0.01 * 6.803814888000488
Epoch 930, val loss: 1.3406163454055786
Epoch 940, training loss: 0.07159008830785751 = 0.003611413761973381 + 0.01 * 6.797867298126221
Epoch 940, val loss: 1.3442046642303467
Epoch 950, training loss: 0.07162786275148392 = 0.0035269418731331825 + 0.01 * 6.810092449188232
Epoch 950, val loss: 1.3477270603179932
Epoch 960, training loss: 0.07137437164783478 = 0.0034463696647435427 + 0.01 * 6.792799949645996
Epoch 960, val loss: 1.3511955738067627
Epoch 970, training loss: 0.07129710167646408 = 0.0033689443953335285 + 0.01 * 6.792815685272217
Epoch 970, val loss: 1.3545739650726318
Epoch 980, training loss: 0.07126439362764359 = 0.0032952462788671255 + 0.01 * 6.796915054321289
Epoch 980, val loss: 1.3579514026641846
Epoch 990, training loss: 0.07103527337312698 = 0.0032240801956504583 + 0.01 * 6.781119346618652
Epoch 990, val loss: 1.3611706495285034
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8223510806536637
The final CL Acc:0.78395, 0.01552, The final GNN Acc:0.81884, 0.00252
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13198])
remove edge: torch.Size([2, 7890])
updated graph: torch.Size([2, 10532])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0138041973114014 = 1.9278361797332764 + 0.01 * 8.596803665161133
Epoch 0, val loss: 1.9245408773422241
Epoch 10, training loss: 2.0049240589141846 = 1.9189566373825073 + 0.01 * 8.596736907958984
Epoch 10, val loss: 1.916183590888977
Epoch 20, training loss: 1.9941848516464233 = 1.9082202911376953 + 0.01 * 8.596460342407227
Epoch 20, val loss: 1.9056739807128906
Epoch 30, training loss: 1.9791975021362305 = 1.893242359161377 + 0.01 * 8.595518112182617
Epoch 30, val loss: 1.8907023668289185
Epoch 40, training loss: 1.9571774005889893 = 1.8712780475616455 + 0.01 * 8.589930534362793
Epoch 40, val loss: 1.868920087814331
Epoch 50, training loss: 1.9253039360046387 = 1.8398524522781372 + 0.01 * 8.54515266418457
Epoch 50, val loss: 1.8389079570770264
Epoch 60, training loss: 1.8834028244018555 = 1.800970196723938 + 0.01 * 8.243268013000488
Epoch 60, val loss: 1.804348111152649
Epoch 70, training loss: 1.840704321861267 = 1.761012077331543 + 0.01 * 7.969225883483887
Epoch 70, val loss: 1.7707691192626953
Epoch 80, training loss: 1.7885056734085083 = 1.7121905088424683 + 0.01 * 7.631511211395264
Epoch 80, val loss: 1.7270406484603882
Epoch 90, training loss: 1.7181329727172852 = 1.6434450149536133 + 0.01 * 7.468799591064453
Epoch 90, val loss: 1.6651699542999268
Epoch 100, training loss: 1.6257213354110718 = 1.5515459775924683 + 0.01 * 7.417535781860352
Epoch 100, val loss: 1.5847171545028687
Epoch 110, training loss: 1.5116556882858276 = 1.4379725456237793 + 0.01 * 7.368309497833252
Epoch 110, val loss: 1.4874629974365234
Epoch 120, training loss: 1.3849924802780151 = 1.3120592832565308 + 0.01 * 7.293319225311279
Epoch 120, val loss: 1.3806838989257812
Epoch 130, training loss: 1.2556761503219604 = 1.1835973262786865 + 0.01 * 7.207888126373291
Epoch 130, val loss: 1.2730622291564941
Epoch 140, training loss: 1.1322375535964966 = 1.0605813264846802 + 0.01 * 7.165625095367432
Epoch 140, val loss: 1.171876072883606
Epoch 150, training loss: 1.019073724746704 = 0.9476759433746338 + 0.01 * 7.139773845672607
Epoch 150, val loss: 1.0803265571594238
Epoch 160, training loss: 0.9186378717422485 = 0.8474732637405396 + 0.01 * 7.1164631843566895
Epoch 160, val loss: 1.000628113746643
Epoch 170, training loss: 0.8316575884819031 = 0.7606947422027588 + 0.01 * 7.096287250518799
Epoch 170, val loss: 0.934279203414917
Epoch 180, training loss: 0.7563011050224304 = 0.6854866147041321 + 0.01 * 7.081447124481201
Epoch 180, val loss: 0.8808927536010742
Epoch 190, training loss: 0.6894423961639404 = 0.6187381148338318 + 0.01 * 7.0704264640808105
Epoch 190, val loss: 0.838378369808197
Epoch 200, training loss: 0.6282328963279724 = 0.5576216578483582 + 0.01 * 7.0611252784729
Epoch 200, val loss: 0.8040449619293213
Epoch 210, training loss: 0.5710920691490173 = 0.5005909204483032 + 0.01 * 7.050117015838623
Epoch 210, val loss: 0.7758331298828125
Epoch 220, training loss: 0.5178391337394714 = 0.44743916392326355 + 0.01 * 7.039994716644287
Epoch 220, val loss: 0.7532386779785156
Epoch 230, training loss: 0.4690408706665039 = 0.398716002702713 + 0.01 * 7.0324883460998535
Epoch 230, val loss: 0.7369921803474426
Epoch 240, training loss: 0.4252254366874695 = 0.35494717955589294 + 0.01 * 7.02782678604126
Epoch 240, val loss: 0.7274720072746277
Epoch 250, training loss: 0.3863506317138672 = 0.31612715125083923 + 0.01 * 7.022348880767822
Epoch 250, val loss: 0.7238671779632568
Epoch 260, training loss: 0.35190775990486145 = 0.2817506194114685 + 0.01 * 7.015714168548584
Epoch 260, val loss: 0.7251980304718018
Epoch 270, training loss: 0.3214859068393707 = 0.25117579102516174 + 0.01 * 7.031010627746582
Epoch 270, val loss: 0.7306775450706482
Epoch 280, training loss: 0.29397252202033997 = 0.22388547658920288 + 0.01 * 7.008703708648682
Epoch 280, val loss: 0.7394567131996155
Epoch 290, training loss: 0.26950398087501526 = 0.19944709539413452 + 0.01 * 7.005687713623047
Epoch 290, val loss: 0.7510477900505066
Epoch 300, training loss: 0.24759481847286224 = 0.177574023604393 + 0.01 * 7.002079963684082
Epoch 300, val loss: 0.7649050354957581
Epoch 310, training loss: 0.22804208099842072 = 0.15805676579475403 + 0.01 * 6.998531341552734
Epoch 310, val loss: 0.7805755734443665
Epoch 320, training loss: 0.2106650322675705 = 0.14071059226989746 + 0.01 * 6.995444297790527
Epoch 320, val loss: 0.7976703643798828
Epoch 330, training loss: 0.19539029896259308 = 0.12536020576953888 + 0.01 * 7.003009796142578
Epoch 330, val loss: 0.8160085082054138
Epoch 340, training loss: 0.18172520399093628 = 0.11181539297103882 + 0.01 * 6.990980625152588
Epoch 340, val loss: 0.8352298736572266
Epoch 350, training loss: 0.16972915828227997 = 0.09986931085586548 + 0.01 * 6.985984802246094
Epoch 350, val loss: 0.8551911115646362
Epoch 360, training loss: 0.15916313230991364 = 0.08933701366186142 + 0.01 * 6.982612133026123
Epoch 360, val loss: 0.875626266002655
Epoch 370, training loss: 0.1498432755470276 = 0.08005165308713913 + 0.01 * 6.979161739349365
Epoch 370, val loss: 0.896351158618927
Epoch 380, training loss: 0.14167889952659607 = 0.07186732441186905 + 0.01 * 6.981158256530762
Epoch 380, val loss: 0.9172356724739075
Epoch 390, training loss: 0.13437879085540771 = 0.06464999169111252 + 0.01 * 6.9728803634643555
Epoch 390, val loss: 0.9379943609237671
Epoch 400, training loss: 0.1279722899198532 = 0.05827648937702179 + 0.01 * 6.969580173492432
Epoch 400, val loss: 0.9585604071617126
Epoch 410, training loss: 0.12230534851551056 = 0.05264590308070183 + 0.01 * 6.965944290161133
Epoch 410, val loss: 0.9788092970848083
Epoch 420, training loss: 0.1172400712966919 = 0.04766518622636795 + 0.01 * 6.957488536834717
Epoch 420, val loss: 0.9987224340438843
Epoch 430, training loss: 0.11276926845312119 = 0.043252356350421906 + 0.01 * 6.951691150665283
Epoch 430, val loss: 1.0182342529296875
Epoch 440, training loss: 0.10892033576965332 = 0.039340417832136154 + 0.01 * 6.957991600036621
Epoch 440, val loss: 1.0372543334960938
Epoch 450, training loss: 0.10534639656543732 = 0.03587257117033005 + 0.01 * 6.94738245010376
Epoch 450, val loss: 1.0558024644851685
Epoch 460, training loss: 0.1021587923169136 = 0.03279169648885727 + 0.01 * 6.936709403991699
Epoch 460, val loss: 1.0738681554794312
Epoch 470, training loss: 0.0995028167963028 = 0.03005099669098854 + 0.01 * 6.9451823234558105
Epoch 470, val loss: 1.0914727449417114
Epoch 480, training loss: 0.09687477350234985 = 0.027611400932073593 + 0.01 * 6.926337718963623
Epoch 480, val loss: 1.1085808277130127
Epoch 490, training loss: 0.09477905184030533 = 0.025436224415898323 + 0.01 * 6.934283256530762
Epoch 490, val loss: 1.1252225637435913
Epoch 500, training loss: 0.09274016320705414 = 0.023494144901633263 + 0.01 * 6.924602031707764
Epoch 500, val loss: 1.141337275505066
Epoch 510, training loss: 0.09092793613672256 = 0.021755777299404144 + 0.01 * 6.917215824127197
Epoch 510, val loss: 1.156922698020935
Epoch 520, training loss: 0.08928122371435165 = 0.020196961238980293 + 0.01 * 6.908426761627197
Epoch 520, val loss: 1.1720958948135376
Epoch 530, training loss: 0.08779749274253845 = 0.018794532865285873 + 0.01 * 6.900296211242676
Epoch 530, val loss: 1.1868276596069336
Epoch 540, training loss: 0.0866914764046669 = 0.017527202144265175 + 0.01 * 6.916428089141846
Epoch 540, val loss: 1.2010726928710938
Epoch 550, training loss: 0.0853230282664299 = 0.016380786895751953 + 0.01 * 6.894224643707275
Epoch 550, val loss: 1.2149039506912231
Epoch 560, training loss: 0.08421988785266876 = 0.015337904915213585 + 0.01 * 6.888197898864746
Epoch 560, val loss: 1.2282854318618774
Epoch 570, training loss: 0.08317278325557709 = 0.014385422691702843 + 0.01 * 6.8787360191345215
Epoch 570, val loss: 1.241270899772644
Epoch 580, training loss: 0.08244792371988297 = 0.013517077080905437 + 0.01 * 6.89308500289917
Epoch 580, val loss: 1.253921389579773
Epoch 590, training loss: 0.08150584995746613 = 0.01272471621632576 + 0.01 * 6.87811279296875
Epoch 590, val loss: 1.266019582748413
Epoch 600, training loss: 0.08071281760931015 = 0.011999630369246006 + 0.01 * 6.871318817138672
Epoch 600, val loss: 1.2777575254440308
Epoch 610, training loss: 0.08002859354019165 = 0.01133463904261589 + 0.01 * 6.869395732879639
Epoch 610, val loss: 1.2890880107879639
Epoch 620, training loss: 0.07930567115545273 = 0.010723217390477657 + 0.01 * 6.858245372772217
Epoch 620, val loss: 1.3001022338867188
Epoch 630, training loss: 0.07869788259267807 = 0.010159957222640514 + 0.01 * 6.853793144226074
Epoch 630, val loss: 1.310759425163269
Epoch 640, training loss: 0.07836132496595383 = 0.009640208445489407 + 0.01 * 6.872111797332764
Epoch 640, val loss: 1.3211020231246948
Epoch 650, training loss: 0.07770459353923798 = 0.009160403162240982 + 0.01 * 6.854419231414795
Epoch 650, val loss: 1.3310792446136475
Epoch 660, training loss: 0.07737979292869568 = 0.008716754615306854 + 0.01 * 6.86630392074585
Epoch 660, val loss: 1.3408540487289429
Epoch 670, training loss: 0.07674228399991989 = 0.008306666277348995 + 0.01 * 6.843562126159668
Epoch 670, val loss: 1.3501816987991333
Epoch 680, training loss: 0.07632581889629364 = 0.007926477119326591 + 0.01 * 6.839934349060059
Epoch 680, val loss: 1.3593164682388306
Epoch 690, training loss: 0.07596427202224731 = 0.007573000155389309 + 0.01 * 6.839127540588379
Epoch 690, val loss: 1.3681795597076416
Epoch 700, training loss: 0.07570714503526688 = 0.0072443196550011635 + 0.01 * 6.846282958984375
Epoch 700, val loss: 1.376827359199524
Epoch 710, training loss: 0.07528254389762878 = 0.006938363891094923 + 0.01 * 6.834418296813965
Epoch 710, val loss: 1.3850998878479004
Epoch 720, training loss: 0.07503600418567657 = 0.0066530960611999035 + 0.01 * 6.838291645050049
Epoch 720, val loss: 1.3932898044586182
Epoch 730, training loss: 0.07456335425376892 = 0.006386552471667528 + 0.01 * 6.817680358886719
Epoch 730, val loss: 1.4010933637619019
Epoch 740, training loss: 0.0744943618774414 = 0.006136939860880375 + 0.01 * 6.835742473602295
Epoch 740, val loss: 1.4088239669799805
Epoch 750, training loss: 0.07410497963428497 = 0.005903261713683605 + 0.01 * 6.820172309875488
Epoch 750, val loss: 1.4162018299102783
Epoch 760, training loss: 0.07376817613840103 = 0.005684202071279287 + 0.01 * 6.80839729309082
Epoch 760, val loss: 1.4234310388565063
Epoch 770, training loss: 0.07381392270326614 = 0.00547818886116147 + 0.01 * 6.833573341369629
Epoch 770, val loss: 1.4304821491241455
Epoch 780, training loss: 0.07335920631885529 = 0.00528539065271616 + 0.01 * 6.807382106781006
Epoch 780, val loss: 1.4373353719711304
Epoch 790, training loss: 0.07309838384389877 = 0.005104149226099253 + 0.01 * 6.799423694610596
Epoch 790, val loss: 1.443984866142273
Epoch 800, training loss: 0.07292281836271286 = 0.004932998679578304 + 0.01 * 6.798981666564941
Epoch 800, val loss: 1.4504587650299072
Epoch 810, training loss: 0.07277147471904755 = 0.004771471489220858 + 0.01 * 6.800000190734863
Epoch 810, val loss: 1.4567787647247314
Epoch 820, training loss: 0.07262595742940903 = 0.004618984181433916 + 0.01 * 6.800697326660156
Epoch 820, val loss: 1.462921142578125
Epoch 830, training loss: 0.07243973016738892 = 0.0044749341905117035 + 0.01 * 6.79647970199585
Epoch 830, val loss: 1.4688962697982788
Epoch 840, training loss: 0.07225152105093002 = 0.004338463768362999 + 0.01 * 6.7913055419921875
Epoch 840, val loss: 1.4747130870819092
Epoch 850, training loss: 0.07204795628786087 = 0.004209127742797136 + 0.01 * 6.7838826179504395
Epoch 850, val loss: 1.480455756187439
Epoch 860, training loss: 0.07186737656593323 = 0.004086523782461882 + 0.01 * 6.778085231781006
Epoch 860, val loss: 1.4859727621078491
Epoch 870, training loss: 0.07187574356794357 = 0.003970217425376177 + 0.01 * 6.790552616119385
Epoch 870, val loss: 1.4913591146469116
Epoch 880, training loss: 0.0716937929391861 = 0.0038598976098001003 + 0.01 * 6.783389568328857
Epoch 880, val loss: 1.4966284036636353
Epoch 890, training loss: 0.07144966721534729 = 0.0037550912238657475 + 0.01 * 6.7694573402404785
Epoch 890, val loss: 1.5017368793487549
Epoch 900, training loss: 0.07141444087028503 = 0.00365523062646389 + 0.01 * 6.77592134475708
Epoch 900, val loss: 1.5066981315612793
Epoch 910, training loss: 0.07124770432710648 = 0.003559992415830493 + 0.01 * 6.768771648406982
Epoch 910, val loss: 1.511637806892395
Epoch 920, training loss: 0.07114221155643463 = 0.003469370771199465 + 0.01 * 6.767284393310547
Epoch 920, val loss: 1.5163475275039673
Epoch 930, training loss: 0.07118743658065796 = 0.003382645081728697 + 0.01 * 6.7804789543151855
Epoch 930, val loss: 1.5210542678833008
Epoch 940, training loss: 0.07109947502613068 = 0.0033000470139086246 + 0.01 * 6.779942989349365
Epoch 940, val loss: 1.5256272554397583
Epoch 950, training loss: 0.0707654356956482 = 0.0032213623635470867 + 0.01 * 6.7544074058532715
Epoch 950, val loss: 1.5299816131591797
Epoch 960, training loss: 0.07074739784002304 = 0.0031461280304938555 + 0.01 * 6.760127067565918
Epoch 960, val loss: 1.5343466997146606
Epoch 970, training loss: 0.07063503563404083 = 0.0030741654336452484 + 0.01 * 6.756087779998779
Epoch 970, val loss: 1.5384860038757324
Epoch 980, training loss: 0.07048414647579193 = 0.0030051828362047672 + 0.01 * 6.747896671295166
Epoch 980, val loss: 1.5426279306411743
Epoch 990, training loss: 0.07058677077293396 = 0.0029388482216745615 + 0.01 * 6.764791965484619
Epoch 990, val loss: 1.5467883348464966
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.0434281826019287 = 1.9574601650238037 + 0.01 * 8.596797943115234
Epoch 0, val loss: 1.95626962184906
Epoch 10, training loss: 2.0320005416870117 = 1.946033239364624 + 0.01 * 8.596721649169922
Epoch 10, val loss: 1.9446191787719727
Epoch 20, training loss: 2.0176501274108887 = 1.9316855669021606 + 0.01 * 8.596458435058594
Epoch 20, val loss: 1.929993748664856
Epoch 30, training loss: 1.9975571632385254 = 1.911601185798645 + 0.01 * 8.595593452453613
Epoch 30, val loss: 1.9097524881362915
Epoch 40, training loss: 1.96845281124115 = 1.8825427293777466 + 0.01 * 8.591004371643066
Epoch 40, val loss: 1.8811196088790894
Epoch 50, training loss: 1.928993821144104 = 1.84335458278656 + 0.01 * 8.563922882080078
Epoch 50, val loss: 1.8446346521377563
Epoch 60, training loss: 1.88652503490448 = 1.8019658327102661 + 0.01 * 8.455924987792969
Epoch 60, val loss: 1.8101834058761597
Epoch 70, training loss: 1.8511199951171875 = 1.7688145637512207 + 0.01 * 8.23054027557373
Epoch 70, val loss: 1.7834597826004028
Epoch 80, training loss: 1.8093538284301758 = 1.7286875247955322 + 0.01 * 8.066624641418457
Epoch 80, val loss: 1.7460041046142578
Epoch 90, training loss: 1.751453161239624 = 1.6727327108383179 + 0.01 * 7.872039794921875
Epoch 90, val loss: 1.696315884590149
Epoch 100, training loss: 1.6742351055145264 = 1.5974335670471191 + 0.01 * 7.680156707763672
Epoch 100, val loss: 1.6331937313079834
Epoch 110, training loss: 1.583657145500183 = 1.5093472003936768 + 0.01 * 7.430996894836426
Epoch 110, val loss: 1.5618219375610352
Epoch 120, training loss: 1.4936844110488892 = 1.420636534690857 + 0.01 * 7.304788112640381
Epoch 120, val loss: 1.4899945259094238
Epoch 130, training loss: 1.4086506366729736 = 1.336085319519043 + 0.01 * 7.256535530090332
Epoch 130, val loss: 1.421199083328247
Epoch 140, training loss: 1.3255841732025146 = 1.2533677816390991 + 0.01 * 7.221644878387451
Epoch 140, val loss: 1.3540054559707642
Epoch 150, training loss: 1.2428113222122192 = 1.1708452701568604 + 0.01 * 7.19660758972168
Epoch 150, val loss: 1.2865533828735352
Epoch 160, training loss: 1.161831259727478 = 1.0900590419769287 + 0.01 * 7.177216529846191
Epoch 160, val loss: 1.221634030342102
Epoch 170, training loss: 1.0850470066070557 = 1.0134366750717163 + 0.01 * 7.161033630371094
Epoch 170, val loss: 1.1613519191741943
Epoch 180, training loss: 1.0129112005233765 = 0.9414606690406799 + 0.01 * 7.145057678222656
Epoch 180, val loss: 1.1059678792953491
Epoch 190, training loss: 0.9436001777648926 = 0.872323215007782 + 0.01 * 7.127693176269531
Epoch 190, val loss: 1.0530270338058472
Epoch 200, training loss: 0.874842643737793 = 0.8037447333335876 + 0.01 * 7.109793186187744
Epoch 200, val loss: 1.0007007122039795
Epoch 210, training loss: 0.8058116436004639 = 0.7348585724830627 + 0.01 * 7.095309257507324
Epoch 210, val loss: 0.9485384225845337
Epoch 220, training loss: 0.7378838062286377 = 0.6670332551002502 + 0.01 * 7.085054397583008
Epoch 220, val loss: 0.8976884484291077
Epoch 230, training loss: 0.6737068295478821 = 0.6029443740844727 + 0.01 * 7.076246738433838
Epoch 230, val loss: 0.8508732318878174
Epoch 240, training loss: 0.6159436106681824 = 0.5452563762664795 + 0.01 * 7.0687255859375
Epoch 240, val loss: 0.8108159899711609
Epoch 250, training loss: 0.5657128095626831 = 0.4950922429561615 + 0.01 * 7.06205415725708
Epoch 250, val loss: 0.7788876891136169
Epoch 260, training loss: 0.52222740650177 = 0.45169907808303833 + 0.01 * 7.052830219268799
Epoch 260, val loss: 0.7546255588531494
Epoch 270, training loss: 0.48361527919769287 = 0.4131777584552765 + 0.01 * 7.043753147125244
Epoch 270, val loss: 0.7360722422599792
Epoch 280, training loss: 0.4478590786457062 = 0.37750571966171265 + 0.01 * 7.035335063934326
Epoch 280, val loss: 0.7213401794433594
Epoch 290, training loss: 0.41352421045303345 = 0.3432473838329315 + 0.01 * 7.027681827545166
Epoch 290, val loss: 0.7090120315551758
Epoch 300, training loss: 0.3799082338809967 = 0.3096902668476105 + 0.01 * 7.021795749664307
Epoch 300, val loss: 0.6985176801681519
Epoch 310, training loss: 0.34680378437042236 = 0.27662503719329834 + 0.01 * 7.017874240875244
Epoch 310, val loss: 0.6900802850723267
Epoch 320, training loss: 0.314465194940567 = 0.24432742595672607 + 0.01 * 7.013777256011963
Epoch 320, val loss: 0.683922290802002
Epoch 330, training loss: 0.28363558650016785 = 0.2135302722454071 + 0.01 * 7.010531425476074
Epoch 330, val loss: 0.6805996894836426
Epoch 340, training loss: 0.255290687084198 = 0.18518218398094177 + 0.01 * 7.0108489990234375
Epoch 340, val loss: 0.6802400946617126
Epoch 350, training loss: 0.23013967275619507 = 0.16006828844547272 + 0.01 * 7.007138729095459
Epoch 350, val loss: 0.6827499866485596
Epoch 360, training loss: 0.20850779116153717 = 0.138460174202919 + 0.01 * 7.004761695861816
Epoch 360, val loss: 0.687664270401001
Epoch 370, training loss: 0.19018414616584778 = 0.12015367299318314 + 0.01 * 7.003046989440918
Epoch 370, val loss: 0.6945253610610962
Epoch 380, training loss: 0.17470023036003113 = 0.10469084978103638 + 0.01 * 7.000937461853027
Epoch 380, val loss: 0.702588677406311
Epoch 390, training loss: 0.16156038641929626 = 0.09156858921051025 + 0.01 * 6.999178886413574
Epoch 390, val loss: 0.7113283276557922
Epoch 400, training loss: 0.15031346678733826 = 0.08033935725688934 + 0.01 * 6.997410774230957
Epoch 400, val loss: 0.7204726338386536
Epoch 410, training loss: 0.14061617851257324 = 0.07065978646278381 + 0.01 * 6.995639324188232
Epoch 410, val loss: 0.7298594117164612
Epoch 420, training loss: 0.1322418451309204 = 0.06227558106184006 + 0.01 * 6.996627330780029
Epoch 420, val loss: 0.739409863948822
Epoch 430, training loss: 0.12494823336601257 = 0.054999206215143204 + 0.01 * 6.994902610778809
Epoch 430, val loss: 0.7490219473838806
Epoch 440, training loss: 0.11859771609306335 = 0.04868254065513611 + 0.01 * 6.991518020629883
Epoch 440, val loss: 0.7586908936500549
Epoch 450, training loss: 0.11308459937572479 = 0.04320230334997177 + 0.01 * 6.988229751586914
Epoch 450, val loss: 0.768336832523346
Epoch 460, training loss: 0.10831039398908615 = 0.03845030814409256 + 0.01 * 6.986008644104004
Epoch 460, val loss: 0.777941882610321
Epoch 470, training loss: 0.10415835678577423 = 0.034326422959566116 + 0.01 * 6.983193397521973
Epoch 470, val loss: 0.7874942421913147
Epoch 480, training loss: 0.10067294538021088 = 0.03074241615831852 + 0.01 * 6.993052959442139
Epoch 480, val loss: 0.7968983054161072
Epoch 490, training loss: 0.09743275493383408 = 0.02763056568801403 + 0.01 * 6.980218887329102
Epoch 490, val loss: 0.8061365485191345
Epoch 500, training loss: 0.09468388557434082 = 0.02492520958185196 + 0.01 * 6.975867748260498
Epoch 500, val loss: 0.8152113556861877
Epoch 510, training loss: 0.09230058640241623 = 0.022570550441741943 + 0.01 * 6.97300386428833
Epoch 510, val loss: 0.8241240382194519
Epoch 520, training loss: 0.09026671946048737 = 0.020517751574516296 + 0.01 * 6.9748969078063965
Epoch 520, val loss: 0.832886278629303
Epoch 530, training loss: 0.08840201795101166 = 0.018724514171481133 + 0.01 * 6.967750072479248
Epoch 530, val loss: 0.8413506746292114
Epoch 540, training loss: 0.08679232001304626 = 0.017153289169073105 + 0.01 * 6.963902950286865
Epoch 540, val loss: 0.8496084213256836
Epoch 550, training loss: 0.08538892865180969 = 0.01577148586511612 + 0.01 * 6.96174430847168
Epoch 550, val loss: 0.8576520085334778
Epoch 560, training loss: 0.08413001149892807 = 0.014552496373653412 + 0.01 * 6.957751750946045
Epoch 560, val loss: 0.8654372692108154
Epoch 570, training loss: 0.08308358490467072 = 0.013473013415932655 + 0.01 * 6.961057186126709
Epoch 570, val loss: 0.8730357885360718
Epoch 580, training loss: 0.08203121274709702 = 0.0125138433650136 + 0.01 * 6.9517364501953125
Epoch 580, val loss: 0.8803558945655823
Epoch 590, training loss: 0.08120822161436081 = 0.011658156290650368 + 0.01 * 6.955007076263428
Epoch 590, val loss: 0.8874613046646118
Epoch 600, training loss: 0.08029717206954956 = 0.010892149060964584 + 0.01 * 6.940503120422363
Epoch 600, val loss: 0.8943649530410767
Epoch 610, training loss: 0.07968784123659134 = 0.010204038582742214 + 0.01 * 6.948380470275879
Epoch 610, val loss: 0.9010255932807922
Epoch 620, training loss: 0.07892262190580368 = 0.009583533741533756 + 0.01 * 6.933908939361572
Epoch 620, val loss: 0.907477617263794
Epoch 630, training loss: 0.07835790514945984 = 0.009021980687975883 + 0.01 * 6.933592319488525
Epoch 630, val loss: 0.9137753248214722
Epoch 640, training loss: 0.07779376208782196 = 0.008512375876307487 + 0.01 * 6.928138732910156
Epoch 640, val loss: 0.9198288321495056
Epoch 650, training loss: 0.07727797329425812 = 0.008048556745052338 + 0.01 * 6.922942161560059
Epoch 650, val loss: 0.92573481798172
Epoch 660, training loss: 0.07679825276136398 = 0.00762492511421442 + 0.01 * 6.917332649230957
Epoch 660, val loss: 0.9314090609550476
Epoch 670, training loss: 0.07643488049507141 = 0.007237305864691734 + 0.01 * 6.91975736618042
Epoch 670, val loss: 0.9370349645614624
Epoch 680, training loss: 0.07594845443964005 = 0.006881689187139273 + 0.01 * 6.906676769256592
Epoch 680, val loss: 0.9423435926437378
Epoch 690, training loss: 0.07558736950159073 = 0.006554493214935064 + 0.01 * 6.903287887573242
Epoch 690, val loss: 0.9476135969161987
Epoch 700, training loss: 0.07524285465478897 = 0.006252897437661886 + 0.01 * 6.898995399475098
Epoch 700, val loss: 0.9526544809341431
Epoch 710, training loss: 0.07486826926469803 = 0.005974318366497755 + 0.01 * 6.889395236968994
Epoch 710, val loss: 0.9575338959693909
Epoch 720, training loss: 0.07468046993017197 = 0.0057163843885064125 + 0.01 * 6.896409034729004
Epoch 720, val loss: 0.9624515175819397
Epoch 730, training loss: 0.07434965670108795 = 0.005477362312376499 + 0.01 * 6.887229919433594
Epoch 730, val loss: 0.9669703841209412
Epoch 740, training loss: 0.07404016703367233 = 0.005255045369267464 + 0.01 * 6.878512859344482
Epoch 740, val loss: 0.9715229272842407
Epoch 750, training loss: 0.0737336203455925 = 0.00504788663238287 + 0.01 * 6.8685736656188965
Epoch 750, val loss: 0.9759462475776672
Epoch 760, training loss: 0.07372912764549255 = 0.004854485858231783 + 0.01 * 6.88746452331543
Epoch 760, val loss: 0.980193555355072
Epoch 770, training loss: 0.07330062240362167 = 0.004674035124480724 + 0.01 * 6.862659454345703
Epoch 770, val loss: 0.9843845963478088
Epoch 780, training loss: 0.07315247505903244 = 0.004505092278122902 + 0.01 * 6.864738941192627
Epoch 780, val loss: 0.9883608222007751
Epoch 790, training loss: 0.07288309186697006 = 0.004346680361777544 + 0.01 * 6.853641033172607
Epoch 790, val loss: 0.9922832250595093
Epoch 800, training loss: 0.07302405685186386 = 0.004197787959128618 + 0.01 * 6.882627010345459
Epoch 800, val loss: 0.9961705207824707
Epoch 810, training loss: 0.07249367237091064 = 0.00405810447409749 + 0.01 * 6.8435564041137695
Epoch 810, val loss: 0.9998248219490051
Epoch 820, training loss: 0.07242731004953384 = 0.003926544450223446 + 0.01 * 6.850076675415039
Epoch 820, val loss: 1.0034576654434204
Epoch 830, training loss: 0.07215775549411774 = 0.0038024322129786015 + 0.01 * 6.8355326652526855
Epoch 830, val loss: 1.0069377422332764
Epoch 840, training loss: 0.07202555239200592 = 0.0036849912721663713 + 0.01 * 6.834056377410889
Epoch 840, val loss: 1.0104033946990967
Epoch 850, training loss: 0.07185950130224228 = 0.0035740856546908617 + 0.01 * 6.8285417556762695
Epoch 850, val loss: 1.0138169527053833
Epoch 860, training loss: 0.07185497134923935 = 0.003469329560175538 + 0.01 * 6.838563919067383
Epoch 860, val loss: 1.0169507265090942
Epoch 870, training loss: 0.07164468616247177 = 0.0033700724598020315 + 0.01 * 6.827462196350098
Epoch 870, val loss: 1.0201139450073242
Epoch 880, training loss: 0.07153953611850739 = 0.0032757623121142387 + 0.01 * 6.826377868652344
Epoch 880, val loss: 1.0232229232788086
Epoch 890, training loss: 0.07139123976230621 = 0.003186273854225874 + 0.01 * 6.820497035980225
Epoch 890, val loss: 1.0262200832366943
Epoch 900, training loss: 0.07120200246572495 = 0.0031011104583740234 + 0.01 * 6.810089588165283
Epoch 900, val loss: 1.0291513204574585
Epoch 910, training loss: 0.07125519216060638 = 0.0030201016925275326 + 0.01 * 6.823509693145752
Epoch 910, val loss: 1.0320708751678467
Epoch 920, training loss: 0.07102155685424805 = 0.0029430489521473646 + 0.01 * 6.807851314544678
Epoch 920, val loss: 1.0348068475723267
Epoch 930, training loss: 0.07093192636966705 = 0.002869709860533476 + 0.01 * 6.8062214851379395
Epoch 930, val loss: 1.0375717878341675
Epoch 940, training loss: 0.07076510787010193 = 0.0027997582219541073 + 0.01 * 6.796535015106201
Epoch 940, val loss: 1.0401543378829956
Epoch 950, training loss: 0.07075785845518112 = 0.002732811262831092 + 0.01 * 6.802505016326904
Epoch 950, val loss: 1.04275381565094
Epoch 960, training loss: 0.07072339951992035 = 0.002669017529115081 + 0.01 * 6.805438995361328
Epoch 960, val loss: 1.0453555583953857
Epoch 970, training loss: 0.07048296928405762 = 0.0026079367380589247 + 0.01 * 6.787503719329834
Epoch 970, val loss: 1.047864556312561
Epoch 980, training loss: 0.07045270502567291 = 0.0025495949666947126 + 0.01 * 6.790311336517334
Epoch 980, val loss: 1.0501976013183594
Epoch 990, training loss: 0.07050278037786484 = 0.0024937829002738 + 0.01 * 6.800899982452393
Epoch 990, val loss: 1.0526080131530762
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.0456767082214355 = 1.9597086906433105 + 0.01 * 8.596809387207031
Epoch 0, val loss: 1.9700672626495361
Epoch 10, training loss: 2.0357041358947754 = 1.9497365951538086 + 0.01 * 8.596765518188477
Epoch 10, val loss: 1.9603874683380127
Epoch 20, training loss: 2.023469924926758 = 1.937504529953003 + 0.01 * 8.596549034118652
Epoch 20, val loss: 1.948042631149292
Epoch 30, training loss: 2.006352663040161 = 1.920394778251648 + 0.01 * 8.59579849243164
Epoch 30, val loss: 1.9303706884384155
Epoch 40, training loss: 1.9809318780899048 = 1.895014762878418 + 0.01 * 8.591715812683105
Epoch 40, val loss: 1.9041122198104858
Epoch 50, training loss: 1.9440393447875977 = 1.8583821058273315 + 0.01 * 8.565718650817871
Epoch 50, val loss: 1.8673175573349
Epoch 60, training loss: 1.8982470035552979 = 1.8135981559753418 + 0.01 * 8.464888572692871
Epoch 60, val loss: 1.8249045610427856
Epoch 70, training loss: 1.855892539024353 = 1.773351788520813 + 0.01 * 8.254074096679688
Epoch 70, val loss: 1.78821861743927
Epoch 80, training loss: 1.8133584260940552 = 1.7325410842895508 + 0.01 * 8.081730842590332
Epoch 80, val loss: 1.7478947639465332
Epoch 90, training loss: 1.7537128925323486 = 1.6759175062179565 + 0.01 * 7.779540538787842
Epoch 90, val loss: 1.695289134979248
Epoch 100, training loss: 1.674069881439209 = 1.598798155784607 + 0.01 * 7.527177810668945
Epoch 100, val loss: 1.6262741088867188
Epoch 110, training loss: 1.5760623216629028 = 1.5021437406539917 + 0.01 * 7.391855239868164
Epoch 110, val loss: 1.5403881072998047
Epoch 120, training loss: 1.469658374786377 = 1.39644193649292 + 0.01 * 7.321649551391602
Epoch 120, val loss: 1.4496002197265625
Epoch 130, training loss: 1.3640382289886475 = 1.2912431955337524 + 0.01 * 7.279505729675293
Epoch 130, val loss: 1.3617454767227173
Epoch 140, training loss: 1.2615044116973877 = 1.1888833045959473 + 0.01 * 7.26211404800415
Epoch 140, val loss: 1.2788918018341064
Epoch 150, training loss: 1.1627380847930908 = 1.0902186632156372 + 0.01 * 7.251946449279785
Epoch 150, val loss: 1.2007005214691162
Epoch 160, training loss: 1.0681287050247192 = 0.9956734776496887 + 0.01 * 7.245525360107422
Epoch 160, val loss: 1.1275171041488647
Epoch 170, training loss: 0.9787377715110779 = 0.9063400626182556 + 0.01 * 7.239768981933594
Epoch 170, val loss: 1.059402346611023
Epoch 180, training loss: 0.8958629369735718 = 0.8235315680503845 + 0.01 * 7.23313570022583
Epoch 180, val loss: 0.9973568320274353
Epoch 190, training loss: 0.8201971054077148 = 0.7479526400566101 + 0.01 * 7.224447727203369
Epoch 190, val loss: 0.9421637058258057
Epoch 200, training loss: 0.7518153786659241 = 0.67969810962677 + 0.01 * 7.211724758148193
Epoch 200, val loss: 0.8945059180259705
Epoch 210, training loss: 0.690450131893158 = 0.6185210943222046 + 0.01 * 7.192904949188232
Epoch 210, val loss: 0.8546176552772522
Epoch 220, training loss: 0.6357036828994751 = 0.5639815330505371 + 0.01 * 7.172216892242432
Epoch 220, val loss: 0.8220198750495911
Epoch 230, training loss: 0.5867125391960144 = 0.515272319316864 + 0.01 * 7.144021987915039
Epoch 230, val loss: 0.7958599925041199
Epoch 240, training loss: 0.5423300266265869 = 0.4711208641529083 + 0.01 * 7.120916366577148
Epoch 240, val loss: 0.7748826146125793
Epoch 250, training loss: 0.5010020732879639 = 0.43000558018684387 + 0.01 * 7.0996479988098145
Epoch 250, val loss: 0.7579042911529541
Epoch 260, training loss: 0.46143633127212524 = 0.3905878961086273 + 0.01 * 7.084842681884766
Epoch 260, val loss: 0.7440703511238098
Epoch 270, training loss: 0.42284584045410156 = 0.3521846830844879 + 0.01 * 7.066114902496338
Epoch 270, val loss: 0.7329180240631104
Epoch 280, training loss: 0.38547879457473755 = 0.31495705246925354 + 0.01 * 7.052173614501953
Epoch 280, val loss: 0.7243896722793579
Epoch 290, training loss: 0.3501080870628357 = 0.27969104051589966 + 0.01 * 7.041703701019287
Epoch 290, val loss: 0.7187521457672119
Epoch 300, training loss: 0.3175700902938843 = 0.24725350737571716 + 0.01 * 7.0316572189331055
Epoch 300, val loss: 0.7163325548171997
Epoch 310, training loss: 0.2884039282798767 = 0.2181517779827118 + 0.01 * 7.025214672088623
Epoch 310, val loss: 0.7170447111129761
Epoch 320, training loss: 0.2626550793647766 = 0.1924581080675125 + 0.01 * 7.0196990966796875
Epoch 320, val loss: 0.7204549312591553
Epoch 330, training loss: 0.24012678861618042 = 0.16998493671417236 + 0.01 * 7.014185428619385
Epoch 330, val loss: 0.7263095378875732
Epoch 340, training loss: 0.22054922580718994 = 0.15044791996479034 + 0.01 * 7.010130405426025
Epoch 340, val loss: 0.7344039082527161
Epoch 350, training loss: 0.20363010466098785 = 0.13353022933006287 + 0.01 * 7.009987831115723
Epoch 350, val loss: 0.7444165349006653
Epoch 360, training loss: 0.18894577026367188 = 0.1188981756567955 + 0.01 * 7.004759311676025
Epoch 360, val loss: 0.7559980154037476
Epoch 370, training loss: 0.1762261986732483 = 0.10622166842222214 + 0.01 * 7.000453472137451
Epoch 370, val loss: 0.7687551379203796
Epoch 380, training loss: 0.16518045961856842 = 0.09520197659730911 + 0.01 * 6.9978485107421875
Epoch 380, val loss: 0.7823677062988281
Epoch 390, training loss: 0.15550433099269867 = 0.0855848491191864 + 0.01 * 6.99194860458374
Epoch 390, val loss: 0.796592652797699
Epoch 400, training loss: 0.14703381061553955 = 0.07715482264757156 + 0.01 * 6.987898826599121
Epoch 400, val loss: 0.8112560510635376
Epoch 410, training loss: 0.13957226276397705 = 0.06973876059055328 + 0.01 * 6.98335075378418
Epoch 410, val loss: 0.8261963725090027
Epoch 420, training loss: 0.13297203183174133 = 0.0631902664899826 + 0.01 * 6.978175640106201
Epoch 420, val loss: 0.841296374797821
Epoch 430, training loss: 0.12712571024894714 = 0.05739007517695427 + 0.01 * 6.973563194274902
Epoch 430, val loss: 0.8564686179161072
Epoch 440, training loss: 0.12208335846662521 = 0.05224097520112991 + 0.01 * 6.984238624572754
Epoch 440, val loss: 0.871549665927887
Epoch 450, training loss: 0.11733820289373398 = 0.04766710847616196 + 0.01 * 6.967109680175781
Epoch 450, val loss: 0.8865200281143188
Epoch 460, training loss: 0.11319506168365479 = 0.04359227046370506 + 0.01 * 6.96027946472168
Epoch 460, val loss: 0.9013113975524902
Epoch 470, training loss: 0.10950450599193573 = 0.03995470330119133 + 0.01 * 6.954980850219727
Epoch 470, val loss: 0.9158798456192017
Epoch 480, training loss: 0.10620784014463425 = 0.036699309945106506 + 0.01 * 6.950852870941162
Epoch 480, val loss: 0.9302393198013306
Epoch 490, training loss: 0.10337760299444199 = 0.03378451615571976 + 0.01 * 6.959308624267578
Epoch 490, val loss: 0.9442492723464966
Epoch 500, training loss: 0.10062729567289352 = 0.0311712846159935 + 0.01 * 6.945601463317871
Epoch 500, val loss: 0.9580172896385193
Epoch 510, training loss: 0.09821052849292755 = 0.02882089652121067 + 0.01 * 6.938963890075684
Epoch 510, val loss: 0.9714911580085754
Epoch 520, training loss: 0.09603405743837357 = 0.026701627299189568 + 0.01 * 6.933243274688721
Epoch 520, val loss: 0.984667956829071
Epoch 530, training loss: 0.09407936036586761 = 0.024786751717329025 + 0.01 * 6.929261684417725
Epoch 530, val loss: 0.9975653886795044
Epoch 540, training loss: 0.09249166399240494 = 0.023054776713252068 + 0.01 * 6.943688869476318
Epoch 540, val loss: 1.0101770162582397
Epoch 550, training loss: 0.09074229001998901 = 0.021488411352038383 + 0.01 * 6.925387859344482
Epoch 550, val loss: 1.0223990678787231
Epoch 560, training loss: 0.0892675518989563 = 0.0200667642056942 + 0.01 * 6.920078754425049
Epoch 560, val loss: 1.0343711376190186
Epoch 570, training loss: 0.08791352808475494 = 0.018773574382066727 + 0.01 * 6.91399621963501
Epoch 570, val loss: 1.0460638999938965
Epoch 580, training loss: 0.08669246733188629 = 0.017595091834664345 + 0.01 * 6.909737586975098
Epoch 580, val loss: 1.0574660301208496
Epoch 590, training loss: 0.08559897541999817 = 0.016518153250217438 + 0.01 * 6.908082008361816
Epoch 590, val loss: 1.0685851573944092
Epoch 600, training loss: 0.0845799669623375 = 0.015531929209828377 + 0.01 * 6.90480375289917
Epoch 600, val loss: 1.079498052597046
Epoch 610, training loss: 0.08364201337099075 = 0.014624428935348988 + 0.01 * 6.901758670806885
Epoch 610, val loss: 1.0900754928588867
Epoch 620, training loss: 0.08274957537651062 = 0.013786200433969498 + 0.01 * 6.896337032318115
Epoch 620, val loss: 1.10053288936615
Epoch 630, training loss: 0.08194420486688614 = 0.013010449707508087 + 0.01 * 6.893375873565674
Epoch 630, val loss: 1.110790491104126
Epoch 640, training loss: 0.08122925460338593 = 0.012293335981667042 + 0.01 * 6.89359188079834
Epoch 640, val loss: 1.1208511590957642
Epoch 650, training loss: 0.08050922304391861 = 0.011630917899310589 + 0.01 * 6.88783073425293
Epoch 650, val loss: 1.1307041645050049
Epoch 660, training loss: 0.07985289394855499 = 0.011017588898539543 + 0.01 * 6.883530616760254
Epoch 660, val loss: 1.1403923034667969
Epoch 670, training loss: 0.07934238016605377 = 0.010449064895510674 + 0.01 * 6.889331817626953
Epoch 670, val loss: 1.149915099143982
Epoch 680, training loss: 0.07874905318021774 = 0.009923307225108147 + 0.01 * 6.882574558258057
Epoch 680, val loss: 1.1590664386749268
Epoch 690, training loss: 0.0781775489449501 = 0.009436143562197685 + 0.01 * 6.87414026260376
Epoch 690, val loss: 1.168157696723938
Epoch 700, training loss: 0.07768715918064117 = 0.00898340716958046 + 0.01 * 6.87037467956543
Epoch 700, val loss: 1.1769689321517944
Epoch 710, training loss: 0.07742449641227722 = 0.008562412112951279 + 0.01 * 6.886208534240723
Epoch 710, val loss: 1.185679316520691
Epoch 720, training loss: 0.07682491838932037 = 0.00817099492996931 + 0.01 * 6.865392684936523
Epoch 720, val loss: 1.1940248012542725
Epoch 730, training loss: 0.07642243802547455 = 0.007806465961039066 + 0.01 * 6.861597537994385
Epoch 730, val loss: 1.2022510766983032
Epoch 740, training loss: 0.07608192414045334 = 0.007466216571629047 + 0.01 * 6.861570358276367
Epoch 740, val loss: 1.2103325128555298
Epoch 750, training loss: 0.07570985704660416 = 0.007148695643991232 + 0.01 * 6.85611629486084
Epoch 750, val loss: 1.2181124687194824
Epoch 760, training loss: 0.0754147619009018 = 0.00685192970559001 + 0.01 * 6.856283664703369
Epoch 760, val loss: 1.2257418632507324
Epoch 770, training loss: 0.07511831820011139 = 0.006574425380676985 + 0.01 * 6.854389667510986
Epoch 770, val loss: 1.2332640886306763
Epoch 780, training loss: 0.0748332291841507 = 0.006314288824796677 + 0.01 * 6.851893901824951
Epoch 780, val loss: 1.2405160665512085
Epoch 790, training loss: 0.07450222223997116 = 0.00607018219307065 + 0.01 * 6.843204498291016
Epoch 790, val loss: 1.2476162910461426
Epoch 800, training loss: 0.07424610108137131 = 0.005840621422976255 + 0.01 * 6.840548038482666
Epoch 800, val loss: 1.2545751333236694
Epoch 810, training loss: 0.07421842962503433 = 0.005624569486826658 + 0.01 * 6.859386444091797
Epoch 810, val loss: 1.2613139152526855
Epoch 820, training loss: 0.07381847500801086 = 0.0054221004247665405 + 0.01 * 6.839637279510498
Epoch 820, val loss: 1.2679964303970337
Epoch 830, training loss: 0.07357054203748703 = 0.00523138465359807 + 0.01 * 6.8339152336120605
Epoch 830, val loss: 1.2744237184524536
Epoch 840, training loss: 0.073357492685318 = 0.005051265936344862 + 0.01 * 6.830623149871826
Epoch 840, val loss: 1.280745267868042
Epoch 850, training loss: 0.07330425828695297 = 0.0048810821026563644 + 0.01 * 6.842317581176758
Epoch 850, val loss: 1.2869759798049927
Epoch 860, training loss: 0.07304329425096512 = 0.004720347002148628 + 0.01 * 6.832294940948486
Epoch 860, val loss: 1.2928980588912964
Epoch 870, training loss: 0.07285456359386444 = 0.004568358883261681 + 0.01 * 6.828620433807373
Epoch 870, val loss: 1.2988386154174805
Epoch 880, training loss: 0.07265225052833557 = 0.0044243610464036465 + 0.01 * 6.822789192199707
Epoch 880, val loss: 1.3045519590377808
Epoch 890, training loss: 0.07256831228733063 = 0.004287904594093561 + 0.01 * 6.828041076660156
Epoch 890, val loss: 1.3102197647094727
Epoch 900, training loss: 0.07236402481794357 = 0.004158624913543463 + 0.01 * 6.820540428161621
Epoch 900, val loss: 1.315646767616272
Epoch 910, training loss: 0.07216817885637283 = 0.004036029800772667 + 0.01 * 6.8132147789001465
Epoch 910, val loss: 1.3210479021072388
Epoch 920, training loss: 0.07211292535066605 = 0.003919404931366444 + 0.01 * 6.819351673126221
Epoch 920, val loss: 1.3263444900512695
Epoch 930, training loss: 0.07193649560213089 = 0.0038085132837295532 + 0.01 * 6.812798500061035
Epoch 930, val loss: 1.3314359188079834
Epoch 940, training loss: 0.07183842360973358 = 0.0037029054947197437 + 0.01 * 6.813551425933838
Epoch 940, val loss: 1.336496353149414
Epoch 950, training loss: 0.07164851576089859 = 0.0036024064756929874 + 0.01 * 6.804610729217529
Epoch 950, val loss: 1.3414371013641357
Epoch 960, training loss: 0.07186120748519897 = 0.0035066374111920595 + 0.01 * 6.835456848144531
Epoch 960, val loss: 1.3461517095565796
Epoch 970, training loss: 0.07148607820272446 = 0.0034154255408793688 + 0.01 * 6.807065486907959
Epoch 970, val loss: 1.350866675376892
Epoch 980, training loss: 0.07128286361694336 = 0.003328378777951002 + 0.01 * 6.7954487800598145
Epoch 980, val loss: 1.3554171323776245
Epoch 990, training loss: 0.07125497609376907 = 0.0032451730221509933 + 0.01 * 6.800980091094971
Epoch 990, val loss: 1.3599121570587158
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8360569319978914
The final CL Acc:0.80988, 0.00462, The final GNN Acc:0.83852, 0.00217
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9432])
updated graph: torch.Size([2, 10502])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0307772159576416 = 1.9448084831237793 + 0.01 * 8.596866607666016
Epoch 0, val loss: 1.94428551197052
Epoch 10, training loss: 2.021388530731201 = 1.935420274734497 + 0.01 * 8.596837043762207
Epoch 10, val loss: 1.9344580173492432
Epoch 20, training loss: 2.010179042816162 = 1.9242117404937744 + 0.01 * 8.596722602844238
Epoch 20, val loss: 1.9224159717559814
Epoch 30, training loss: 1.9944452047348022 = 1.9084811210632324 + 0.01 * 8.59640884399414
Epoch 30, val loss: 1.9054147005081177
Epoch 40, training loss: 1.9708460569381714 = 1.8848942518234253 + 0.01 * 8.595182418823242
Epoch 40, val loss: 1.8802409172058105
Epoch 50, training loss: 1.9371284246444702 = 1.8512566089630127 + 0.01 * 8.587179183959961
Epoch 50, val loss: 1.8455299139022827
Epoch 60, training loss: 1.8978334665298462 = 1.8124096393585205 + 0.01 * 8.542384147644043
Epoch 60, val loss: 1.8086247444152832
Epoch 70, training loss: 1.8622066974639893 = 1.779123067855835 + 0.01 * 8.30836009979248
Epoch 70, val loss: 1.7794442176818848
Epoch 80, training loss: 1.819750428199768 = 1.7390731573104858 + 0.01 * 8.067729949951172
Epoch 80, val loss: 1.7421369552612305
Epoch 90, training loss: 1.7603583335876465 = 1.6817700862884521 + 0.01 * 7.85883092880249
Epoch 90, val loss: 1.6911052465438843
Epoch 100, training loss: 1.6819655895233154 = 1.6040841341018677 + 0.01 * 7.788143634796143
Epoch 100, val loss: 1.625266194343567
Epoch 110, training loss: 1.5875244140625 = 1.5103501081466675 + 0.01 * 7.717432022094727
Epoch 110, val loss: 1.545136570930481
Epoch 120, training loss: 1.4889377355575562 = 1.4122039079666138 + 0.01 * 7.673385143280029
Epoch 120, val loss: 1.4635390043258667
Epoch 130, training loss: 1.3950070142745972 = 1.3186800479888916 + 0.01 * 7.632697105407715
Epoch 130, val loss: 1.3885029554367065
Epoch 140, training loss: 1.3077986240386963 = 1.2318388223648071 + 0.01 * 7.595979690551758
Epoch 140, val loss: 1.3219209909439087
Epoch 150, training loss: 1.2242481708526611 = 1.1485795974731445 + 0.01 * 7.566851615905762
Epoch 150, val loss: 1.2594233751296997
Epoch 160, training loss: 1.1398184299468994 = 1.064409852027893 + 0.01 * 7.540859699249268
Epoch 160, val loss: 1.1964269876480103
Epoch 170, training loss: 1.0520437955856323 = 0.9769260287284851 + 0.01 * 7.511772155761719
Epoch 170, val loss: 1.1305729150772095
Epoch 180, training loss: 0.9629226922988892 = 0.8882677555084229 + 0.01 * 7.465497016906738
Epoch 180, val loss: 1.064151406288147
Epoch 190, training loss: 0.8774033784866333 = 0.803551197052002 + 0.01 * 7.385215759277344
Epoch 190, val loss: 1.0020272731781006
Epoch 200, training loss: 0.800290048122406 = 0.7270369529724121 + 0.01 * 7.325309753417969
Epoch 200, val loss: 0.9485443234443665
Epoch 210, training loss: 0.7326232194900513 = 0.6597462296485901 + 0.01 * 7.287698268890381
Epoch 210, val loss: 0.9055153131484985
Epoch 220, training loss: 0.6730251908302307 = 0.6003319025039673 + 0.01 * 7.269329071044922
Epoch 220, val loss: 0.8723326325416565
Epoch 230, training loss: 0.6195019483566284 = 0.5469207763671875 + 0.01 * 7.258120536804199
Epoch 230, val loss: 0.8472995162010193
Epoch 240, training loss: 0.5704467296600342 = 0.4979352355003357 + 0.01 * 7.2511467933654785
Epoch 240, val loss: 0.8287166357040405
Epoch 250, training loss: 0.524585485458374 = 0.4521203637123108 + 0.01 * 7.2465105056762695
Epoch 250, val loss: 0.8150196075439453
Epoch 260, training loss: 0.48114198446273804 = 0.4087379574775696 + 0.01 * 7.240403175354004
Epoch 260, val loss: 0.8053969740867615
Epoch 270, training loss: 0.4400565028190613 = 0.36762380599975586 + 0.01 * 7.243269443511963
Epoch 270, val loss: 0.7996243834495544
Epoch 280, training loss: 0.4013037383556366 = 0.3289801776409149 + 0.01 * 7.232357025146484
Epoch 280, val loss: 0.797774076461792
Epoch 290, training loss: 0.3653901219367981 = 0.29313474893569946 + 0.01 * 7.225536823272705
Epoch 290, val loss: 0.7998830676078796
Epoch 300, training loss: 0.33263105154037476 = 0.26039189100265503 + 0.01 * 7.2239155769348145
Epoch 300, val loss: 0.8060297966003418
Epoch 310, training loss: 0.303136944770813 = 0.23093527555465698 + 0.01 * 7.220168590545654
Epoch 310, val loss: 0.8159844279289246
Epoch 320, training loss: 0.27686038613319397 = 0.2047359198331833 + 0.01 * 7.212447643280029
Epoch 320, val loss: 0.8293353319168091
Epoch 330, training loss: 0.2537652552127838 = 0.18161141872406006 + 0.01 * 7.215383052825928
Epoch 330, val loss: 0.8456053137779236
Epoch 340, training loss: 0.2333320677280426 = 0.1612897366285324 + 0.01 * 7.204232692718506
Epoch 340, val loss: 0.8644877076148987
Epoch 350, training loss: 0.21542015671730042 = 0.14343921840190887 + 0.01 * 7.198093414306641
Epoch 350, val loss: 0.8854714632034302
Epoch 360, training loss: 0.1998591423034668 = 0.1277492195367813 + 0.01 * 7.210992336273193
Epoch 360, val loss: 0.9082194566726685
Epoch 370, training loss: 0.18592213094234467 = 0.11395154148340225 + 0.01 * 7.197059154510498
Epoch 370, val loss: 0.9321749806404114
Epoch 380, training loss: 0.1736266314983368 = 0.10176879167556763 + 0.01 * 7.185783386230469
Epoch 380, val loss: 0.9570481777191162
Epoch 390, training loss: 0.16273969411849976 = 0.09096576273441315 + 0.01 * 7.177393913269043
Epoch 390, val loss: 0.9826036095619202
Epoch 400, training loss: 0.15309849381446838 = 0.08136769384145737 + 0.01 * 7.173079967498779
Epoch 400, val loss: 1.0086811780929565
Epoch 410, training loss: 0.14457552134990692 = 0.07284725457429886 + 0.01 * 7.172826766967773
Epoch 410, val loss: 1.035038709640503
Epoch 420, training loss: 0.13688117265701294 = 0.06529659032821655 + 0.01 * 7.1584577560424805
Epoch 420, val loss: 1.0615262985229492
Epoch 430, training loss: 0.13011807203292847 = 0.05861328914761543 + 0.01 * 7.150478363037109
Epoch 430, val loss: 1.0879359245300293
Epoch 440, training loss: 0.12418098747730255 = 0.05271682143211365 + 0.01 * 7.146416664123535
Epoch 440, val loss: 1.1142513751983643
Epoch 450, training loss: 0.1188971996307373 = 0.04752736911177635 + 0.01 * 7.136983871459961
Epoch 450, val loss: 1.1402151584625244
Epoch 460, training loss: 0.11425358802080154 = 0.04296477138996124 + 0.01 * 7.128881454467773
Epoch 460, val loss: 1.1658035516738892
Epoch 470, training loss: 0.11031757295131683 = 0.038958195596933365 + 0.01 * 7.1359381675720215
Epoch 470, val loss: 1.1909347772598267
Epoch 480, training loss: 0.10657735913991928 = 0.03543885797262192 + 0.01 * 7.113850116729736
Epoch 480, val loss: 1.215415120124817
Epoch 490, training loss: 0.10338591039180756 = 0.03233972191810608 + 0.01 * 7.104619026184082
Epoch 490, val loss: 1.2393308877944946
Epoch 500, training loss: 0.10062253475189209 = 0.029606500640511513 + 0.01 * 7.101603984832764
Epoch 500, val loss: 1.2626665830612183
Epoch 510, training loss: 0.09819579869508743 = 0.027190620079636574 + 0.01 * 7.100518226623535
Epoch 510, val loss: 1.285262107849121
Epoch 520, training loss: 0.09591571986675262 = 0.025048626586794853 + 0.01 * 7.086709022521973
Epoch 520, val loss: 1.3073091506958008
Epoch 530, training loss: 0.09394749999046326 = 0.023142924532294273 + 0.01 * 7.0804572105407715
Epoch 530, val loss: 1.3286770582199097
Epoch 540, training loss: 0.09246446937322617 = 0.021440325304865837 + 0.01 * 7.102414131164551
Epoch 540, val loss: 1.3494200706481934
Epoch 550, training loss: 0.09066294133663177 = 0.01991715095937252 + 0.01 * 7.074579238891602
Epoch 550, val loss: 1.3696157932281494
Epoch 560, training loss: 0.0890820324420929 = 0.01854792609810829 + 0.01 * 7.053410053253174
Epoch 560, val loss: 1.3891505002975464
Epoch 570, training loss: 0.08789646625518799 = 0.017311807721853256 + 0.01 * 7.05846643447876
Epoch 570, val loss: 1.4081741571426392
Epoch 580, training loss: 0.08666667342185974 = 0.016193632036447525 + 0.01 * 7.047304153442383
Epoch 580, val loss: 1.426758885383606
Epoch 590, training loss: 0.08556801825761795 = 0.01518018078058958 + 0.01 * 7.038784503936768
Epoch 590, val loss: 1.4447119235992432
Epoch 600, training loss: 0.08469018340110779 = 0.014258824288845062 + 0.01 * 7.043136119842529
Epoch 600, val loss: 1.462240219116211
Epoch 610, training loss: 0.08368272334337234 = 0.01341996155679226 + 0.01 * 7.026276111602783
Epoch 610, val loss: 1.4792522192001343
Epoch 620, training loss: 0.08302903175354004 = 0.012654134072363377 + 0.01 * 7.037489891052246
Epoch 620, val loss: 1.4958319664001465
Epoch 630, training loss: 0.08203750103712082 = 0.011954503133893013 + 0.01 * 7.008299827575684
Epoch 630, val loss: 1.5118229389190674
Epoch 640, training loss: 0.08152952790260315 = 0.011313446797430515 + 0.01 * 7.021608352661133
Epoch 640, val loss: 1.5273969173431396
Epoch 650, training loss: 0.08076585829257965 = 0.010725037194788456 + 0.01 * 7.004082202911377
Epoch 650, val loss: 1.5426172018051147
Epoch 660, training loss: 0.0801725834608078 = 0.010182920843362808 + 0.01 * 6.998966217041016
Epoch 660, val loss: 1.5573248863220215
Epoch 670, training loss: 0.07953956723213196 = 0.009683162905275822 + 0.01 * 6.985640525817871
Epoch 670, val loss: 1.5717453956604004
Epoch 680, training loss: 0.07903363555669785 = 0.009221391752362251 + 0.01 * 6.981224536895752
Epoch 680, val loss: 1.5857386589050293
Epoch 690, training loss: 0.07854803651571274 = 0.00879369955509901 + 0.01 * 6.975434303283691
Epoch 690, val loss: 1.5993294715881348
Epoch 700, training loss: 0.07831114530563354 = 0.008398221805691719 + 0.01 * 6.991292953491211
Epoch 700, val loss: 1.6125398874282837
Epoch 710, training loss: 0.0777016431093216 = 0.008032072335481644 + 0.01 * 6.966956615447998
Epoch 710, val loss: 1.6253796815872192
Epoch 720, training loss: 0.07729653269052505 = 0.007691023871302605 + 0.01 * 6.960550785064697
Epoch 720, val loss: 1.6378331184387207
Epoch 730, training loss: 0.0769602358341217 = 0.007372923195362091 + 0.01 * 6.958731651306152
Epoch 730, val loss: 1.6500213146209717
Epoch 740, training loss: 0.07653545588254929 = 0.007076336536556482 + 0.01 * 6.945911884307861
Epoch 740, val loss: 1.6619385480880737
Epoch 750, training loss: 0.07631328701972961 = 0.006799391936510801 + 0.01 * 6.951389312744141
Epoch 750, val loss: 1.6734904050827026
Epoch 760, training loss: 0.0760486051440239 = 0.006540595553815365 + 0.01 * 6.950800895690918
Epoch 760, val loss: 1.68471097946167
Epoch 770, training loss: 0.0755816176533699 = 0.006297824438661337 + 0.01 * 6.928379058837891
Epoch 770, val loss: 1.6957558393478394
Epoch 780, training loss: 0.07553891092538834 = 0.006069923751056194 + 0.01 * 6.946898937225342
Epoch 780, val loss: 1.7065162658691406
Epoch 790, training loss: 0.07519207149744034 = 0.005856009665876627 + 0.01 * 6.9336066246032715
Epoch 790, val loss: 1.7169760465621948
Epoch 800, training loss: 0.07489672303199768 = 0.005654850043356419 + 0.01 * 6.924187660217285
Epoch 800, val loss: 1.7272474765777588
Epoch 810, training loss: 0.07471002638339996 = 0.005465242080390453 + 0.01 * 6.924478530883789
Epoch 810, val loss: 1.7372488975524902
Epoch 820, training loss: 0.07452894747257233 = 0.005286599043756723 + 0.01 * 6.924234867095947
Epoch 820, val loss: 1.7469409704208374
Epoch 830, training loss: 0.07419804483652115 = 0.005117804743349552 + 0.01 * 6.908024311065674
Epoch 830, val loss: 1.756424903869629
Epoch 840, training loss: 0.07388961315155029 = 0.004958398640155792 + 0.01 * 6.893121719360352
Epoch 840, val loss: 1.7657651901245117
Epoch 850, training loss: 0.07368674129247665 = 0.004807527642697096 + 0.01 * 6.8879218101501465
Epoch 850, val loss: 1.7747915983200073
Epoch 860, training loss: 0.0736616998910904 = 0.004664934705942869 + 0.01 * 6.899676322937012
Epoch 860, val loss: 1.7837495803833008
Epoch 870, training loss: 0.07337655127048492 = 0.004529586061835289 + 0.01 * 6.8846964836120605
Epoch 870, val loss: 1.792373538017273
Epoch 880, training loss: 0.07337331771850586 = 0.004400897771120071 + 0.01 * 6.897242069244385
Epoch 880, val loss: 1.800959587097168
Epoch 890, training loss: 0.07313080877065659 = 0.004278723616153002 + 0.01 * 6.8852081298828125
Epoch 890, val loss: 1.809275507926941
Epoch 900, training loss: 0.07318815588951111 = 0.004162587225437164 + 0.01 * 6.902556896209717
Epoch 900, val loss: 1.8173476457595825
Epoch 910, training loss: 0.07271190732717514 = 0.004052201751619577 + 0.01 * 6.865970611572266
Epoch 910, val loss: 1.825322151184082
Epoch 920, training loss: 0.07262710481882095 = 0.00394718861207366 + 0.01 * 6.8679914474487305
Epoch 920, val loss: 1.8330334424972534
Epoch 930, training loss: 0.07243724912405014 = 0.0038469000719487667 + 0.01 * 6.859035015106201
Epoch 930, val loss: 1.8407244682312012
Epoch 940, training loss: 0.07224463671445847 = 0.003751484677195549 + 0.01 * 6.849315166473389
Epoch 940, val loss: 1.8481885194778442
Epoch 950, training loss: 0.0723116397857666 = 0.0036600360181182623 + 0.01 * 6.8651604652404785
Epoch 950, val loss: 1.8554459810256958
Epoch 960, training loss: 0.07207648456096649 = 0.003572904970496893 + 0.01 * 6.850358009338379
Epoch 960, val loss: 1.8627605438232422
Epoch 970, training loss: 0.07189176231622696 = 0.003489470575004816 + 0.01 * 6.840229511260986
Epoch 970, val loss: 1.8697749376296997
Epoch 980, training loss: 0.0720527321100235 = 0.0034095209557563066 + 0.01 * 6.864321231842041
Epoch 980, val loss: 1.876627802848816
Epoch 990, training loss: 0.0716845691204071 = 0.003333186963573098 + 0.01 * 6.835138320922852
Epoch 990, val loss: 1.8834397792816162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8186610437532947
=== training gcn model ===
Epoch 0, training loss: 2.0379838943481445 = 1.9520152807235718 + 0.01 * 8.596858978271484
Epoch 0, val loss: 1.948060154914856
Epoch 10, training loss: 2.026742696762085 = 1.94077467918396 + 0.01 * 8.596800804138184
Epoch 10, val loss: 1.9365471601486206
Epoch 20, training loss: 2.0126686096191406 = 1.9267024993896484 + 0.01 * 8.596610069274902
Epoch 20, val loss: 1.9220472574234009
Epoch 30, training loss: 1.9929888248443604 = 1.9070289134979248 + 0.01 * 8.595991134643555
Epoch 30, val loss: 1.9018733501434326
Epoch 40, training loss: 1.9646207094192505 = 1.8786920309066772 + 0.01 * 8.592864036560059
Epoch 40, val loss: 1.8734639883041382
Epoch 50, training loss: 1.9274460077285767 = 1.8417104482650757 + 0.01 * 8.573558807373047
Epoch 50, val loss: 1.838683009147644
Epoch 60, training loss: 1.890629529953003 = 1.805728793144226 + 0.01 * 8.490068435668945
Epoch 60, val loss: 1.808995008468628
Epoch 70, training loss: 1.8612606525421143 = 1.7784743309020996 + 0.01 * 8.278632164001465
Epoch 70, val loss: 1.7870069742202759
Epoch 80, training loss: 1.824411392211914 = 1.7426918745040894 + 0.01 * 8.171957015991211
Epoch 80, val loss: 1.7547367811203003
Epoch 90, training loss: 1.7726926803588867 = 1.6920334100723267 + 0.01 * 8.065933227539062
Epoch 90, val loss: 1.7109301090240479
Epoch 100, training loss: 1.7022645473480225 = 1.6221288442611694 + 0.01 * 8.013570785522461
Epoch 100, val loss: 1.652950406074524
Epoch 110, training loss: 1.6197088956832886 = 1.5398390293121338 + 0.01 * 7.986983776092529
Epoch 110, val loss: 1.5870614051818848
Epoch 120, training loss: 1.5367827415466309 = 1.4572988748550415 + 0.01 * 7.948389053344727
Epoch 120, val loss: 1.5228718519210815
Epoch 130, training loss: 1.4584614038467407 = 1.3802238702774048 + 0.01 * 7.823751449584961
Epoch 130, val loss: 1.464991569519043
Epoch 140, training loss: 1.3812766075134277 = 1.3058462142944336 + 0.01 * 7.543038368225098
Epoch 140, val loss: 1.4097810983657837
Epoch 150, training loss: 1.3055874109268188 = 1.2305197715759277 + 0.01 * 7.506760597229004
Epoch 150, val loss: 1.3533554077148438
Epoch 160, training loss: 1.2282119989395142 = 1.153590440750122 + 0.01 * 7.462156295776367
Epoch 160, val loss: 1.2965431213378906
Epoch 170, training loss: 1.1498562097549438 = 1.0754647254943848 + 0.01 * 7.439144134521484
Epoch 170, val loss: 1.239004135131836
Epoch 180, training loss: 1.071351170539856 = 0.9971143007278442 + 0.01 * 7.423684597015381
Epoch 180, val loss: 1.1814665794372559
Epoch 190, training loss: 0.9937387704849243 = 0.9196087718009949 + 0.01 * 7.412997722625732
Epoch 190, val loss: 1.1237151622772217
Epoch 200, training loss: 0.9181625247001648 = 0.8441133499145508 + 0.01 * 7.4049153327941895
Epoch 200, val loss: 1.0669713020324707
Epoch 210, training loss: 0.8458614945411682 = 0.7718999981880188 + 0.01 * 7.396151065826416
Epoch 210, val loss: 1.0125569105148315
Epoch 220, training loss: 0.778093159198761 = 0.7042350769042969 + 0.01 * 7.38580846786499
Epoch 220, val loss: 0.9618529081344604
Epoch 230, training loss: 0.7155758738517761 = 0.6418566703796387 + 0.01 * 7.371918678283691
Epoch 230, val loss: 0.9165617823600769
Epoch 240, training loss: 0.6581278443336487 = 0.5846086740493774 + 0.01 * 7.351918697357178
Epoch 240, val loss: 0.8775189518928528
Epoch 250, training loss: 0.6050018668174744 = 0.5317590832710266 + 0.01 * 7.32427978515625
Epoch 250, val loss: 0.844811201095581
Epoch 260, training loss: 0.5554313659667969 = 0.48248425126075745 + 0.01 * 7.29471492767334
Epoch 260, val loss: 0.8176599740982056
Epoch 270, training loss: 0.50881028175354 = 0.4360673725605011 + 0.01 * 7.274292945861816
Epoch 270, val loss: 0.7950564622879028
Epoch 280, training loss: 0.4645514488220215 = 0.39192482829093933 + 0.01 * 7.262660503387451
Epoch 280, val loss: 0.7759659290313721
Epoch 290, training loss: 0.4221239686012268 = 0.3495592176914215 + 0.01 * 7.25647497177124
Epoch 290, val loss: 0.7595676183700562
Epoch 300, training loss: 0.3813832402229309 = 0.30884844064712524 + 0.01 * 7.253480434417725
Epoch 300, val loss: 0.7453364729881287
Epoch 310, training loss: 0.3427221179008484 = 0.2702081799507141 + 0.01 * 7.2513933181762695
Epoch 310, val loss: 0.7334991693496704
Epoch 320, training loss: 0.30698180198669434 = 0.23447135090827942 + 0.01 * 7.251044273376465
Epoch 320, val loss: 0.7244213819503784
Epoch 330, training loss: 0.2748633623123169 = 0.2023710161447525 + 0.01 * 7.249233722686768
Epoch 330, val loss: 0.7186826467514038
Epoch 340, training loss: 0.24673834443092346 = 0.17425274848937988 + 0.01 * 7.24855899810791
Epoch 340, val loss: 0.7165769934654236
Epoch 350, training loss: 0.22247405350208282 = 0.14999571442604065 + 0.01 * 7.247833728790283
Epoch 350, val loss: 0.7180522680282593
Epoch 360, training loss: 0.20173461735248566 = 0.12925834953784943 + 0.01 * 7.247626781463623
Epoch 360, val loss: 0.7227674126625061
Epoch 370, training loss: 0.18408891558647156 = 0.11162082105875015 + 0.01 * 7.246809005737305
Epoch 370, val loss: 0.7300540208816528
Epoch 380, training loss: 0.1691264808177948 = 0.0966644138097763 + 0.01 * 7.246206283569336
Epoch 380, val loss: 0.7392726540565491
Epoch 390, training loss: 0.15648064017295837 = 0.08402065932750702 + 0.01 * 7.245998382568359
Epoch 390, val loss: 0.7498728036880493
Epoch 400, training loss: 0.14580145478248596 = 0.07334446161985397 + 0.01 * 7.24569845199585
Epoch 400, val loss: 0.7613127827644348
Epoch 410, training loss: 0.13674584031105042 = 0.06431275606155396 + 0.01 * 7.243309497833252
Epoch 410, val loss: 0.7733103036880493
Epoch 420, training loss: 0.1290678232908249 = 0.056652721017599106 + 0.01 * 7.24151086807251
Epoch 420, val loss: 0.7855722904205322
Epoch 430, training loss: 0.12253598123788834 = 0.05012909322977066 + 0.01 * 7.240688800811768
Epoch 430, val loss: 0.7979524731636047
Epoch 440, training loss: 0.11693644523620605 = 0.044554587453603745 + 0.01 * 7.238185882568359
Epoch 440, val loss: 0.8102771043777466
Epoch 450, training loss: 0.11212927103042603 = 0.03977491334080696 + 0.01 * 7.235435962677002
Epoch 450, val loss: 0.8224847316741943
Epoch 460, training loss: 0.10800613462924957 = 0.03566384315490723 + 0.01 * 7.23422908782959
Epoch 460, val loss: 0.8345069885253906
Epoch 470, training loss: 0.10442331433296204 = 0.0321182943880558 + 0.01 * 7.230501651763916
Epoch 470, val loss: 0.8463588356971741
Epoch 480, training loss: 0.10131633281707764 = 0.02905169129371643 + 0.01 * 7.22646427154541
Epoch 480, val loss: 0.8579756617546082
Epoch 490, training loss: 0.09861645847558975 = 0.02639070898294449 + 0.01 * 7.2225751876831055
Epoch 490, val loss: 0.8693316578865051
Epoch 500, training loss: 0.09629381448030472 = 0.024073401466012 + 0.01 * 7.222041606903076
Epoch 500, val loss: 0.8804227709770203
Epoch 510, training loss: 0.09420600533485413 = 0.022048387676477432 + 0.01 * 7.215761661529541
Epoch 510, val loss: 0.8912086486816406
Epoch 520, training loss: 0.09238486737012863 = 0.02026953548192978 + 0.01 * 7.211533069610596
Epoch 520, val loss: 0.9017217755317688
Epoch 530, training loss: 0.09076711535453796 = 0.018699953332543373 + 0.01 * 7.206716537475586
Epoch 530, val loss: 0.9119479656219482
Epoch 540, training loss: 0.08933407068252563 = 0.017308944836258888 + 0.01 * 7.202513217926025
Epoch 540, val loss: 0.9218926429748535
Epoch 550, training loss: 0.08809448778629303 = 0.016073428094387054 + 0.01 * 7.202106475830078
Epoch 550, val loss: 0.931561291217804
Epoch 560, training loss: 0.08691426366567612 = 0.014972090721130371 + 0.01 * 7.194217681884766
Epoch 560, val loss: 0.9409016966819763
Epoch 570, training loss: 0.08588307350873947 = 0.01398489810526371 + 0.01 * 7.189817905426025
Epoch 570, val loss: 0.9499702453613281
Epoch 580, training loss: 0.08493383228778839 = 0.013096119277179241 + 0.01 * 7.18377161026001
Epoch 580, val loss: 0.9587977528572083
Epoch 590, training loss: 0.08408820629119873 = 0.01229335367679596 + 0.01 * 7.179485321044922
Epoch 590, val loss: 0.9673980474472046
Epoch 600, training loss: 0.0833168476819992 = 0.011566313914954662 + 0.01 * 7.17505407333374
Epoch 600, val loss: 0.9757401347160339
Epoch 610, training loss: 0.08260076493024826 = 0.010906151495873928 + 0.01 * 7.169461727142334
Epoch 610, val loss: 0.9838566780090332
Epoch 620, training loss: 0.0819941982626915 = 0.010304292663931847 + 0.01 * 7.168990612030029
Epoch 620, val loss: 0.9917469024658203
Epoch 630, training loss: 0.0813237726688385 = 0.009754994884133339 + 0.01 * 7.1568779945373535
Epoch 630, val loss: 0.9994296431541443
Epoch 640, training loss: 0.08080089092254639 = 0.009252226911485195 + 0.01 * 7.154866695404053
Epoch 640, val loss: 1.0069068670272827
Epoch 650, training loss: 0.08030682802200317 = 0.008790262043476105 + 0.01 * 7.1516571044921875
Epoch 650, val loss: 1.014182448387146
Epoch 660, training loss: 0.07977363467216492 = 0.008364753797650337 + 0.01 * 7.140888690948486
Epoch 660, val loss: 1.021237850189209
Epoch 670, training loss: 0.07943947613239288 = 0.007972855120897293 + 0.01 * 7.14666223526001
Epoch 670, val loss: 1.0281182527542114
Epoch 680, training loss: 0.07892818003892899 = 0.007610843051224947 + 0.01 * 7.1317338943481445
Epoch 680, val loss: 1.0347652435302734
Epoch 690, training loss: 0.07858837395906448 = 0.007275346200913191 + 0.01 * 7.131303310394287
Epoch 690, val loss: 1.0412604808807373
Epoch 700, training loss: 0.0780821368098259 = 0.006963524501770735 + 0.01 * 7.111861705780029
Epoch 700, val loss: 1.0476012229919434
Epoch 710, training loss: 0.0779135674238205 = 0.006674129981547594 + 0.01 * 7.123943328857422
Epoch 710, val loss: 1.0537961721420288
Epoch 720, training loss: 0.0775413066148758 = 0.006405520718544722 + 0.01 * 7.113578796386719
Epoch 720, val loss: 1.0596028566360474
Epoch 730, training loss: 0.07710909843444824 = 0.0061549111269414425 + 0.01 * 7.095418930053711
Epoch 730, val loss: 1.0652954578399658
Epoch 740, training loss: 0.07674907147884369 = 0.005920152645558119 + 0.01 * 7.082892417907715
Epoch 740, val loss: 1.0708343982696533
Epoch 750, training loss: 0.0765979140996933 = 0.005699945148080587 + 0.01 * 7.089796543121338
Epoch 750, val loss: 1.0762308835983276
Epoch 760, training loss: 0.07651910185813904 = 0.005494880024343729 + 0.01 * 7.102422714233398
Epoch 760, val loss: 1.081458330154419
Epoch 770, training loss: 0.07600126415491104 = 0.005303016398102045 + 0.01 * 7.069824695587158
Epoch 770, val loss: 1.0864756107330322
Epoch 780, training loss: 0.07568176835775375 = 0.005122292321175337 + 0.01 * 7.055948257446289
Epoch 780, val loss: 1.0914156436920166
Epoch 790, training loss: 0.07554031908512115 = 0.00495184725150466 + 0.01 * 7.058847427368164
Epoch 790, val loss: 1.0962501764297485
Epoch 800, training loss: 0.07526116818189621 = 0.004791281186044216 + 0.01 * 7.0469889640808105
Epoch 800, val loss: 1.1008957624435425
Epoch 810, training loss: 0.07528862357139587 = 0.004640014376491308 + 0.01 * 7.064861297607422
Epoch 810, val loss: 1.105457067489624
Epoch 820, training loss: 0.07489063590765 = 0.004497260320931673 + 0.01 * 7.039337635040283
Epoch 820, val loss: 1.1098142862319946
Epoch 830, training loss: 0.07478660345077515 = 0.004362117033451796 + 0.01 * 7.042449474334717
Epoch 830, val loss: 1.114187479019165
Epoch 840, training loss: 0.0744389221072197 = 0.004234159365296364 + 0.01 * 7.020476818084717
Epoch 840, val loss: 1.1183665990829468
Epoch 850, training loss: 0.07424163818359375 = 0.004112968221306801 + 0.01 * 7.012866973876953
Epoch 850, val loss: 1.122532606124878
Epoch 860, training loss: 0.07405221462249756 = 0.00399828003719449 + 0.01 * 7.005393981933594
Epoch 860, val loss: 1.1264674663543701
Epoch 870, training loss: 0.07391193509101868 = 0.003889421932399273 + 0.01 * 7.002251625061035
Epoch 870, val loss: 1.1303770542144775
Epoch 880, training loss: 0.0736689642071724 = 0.0037858786527067423 + 0.01 * 6.988308429718018
Epoch 880, val loss: 1.1341561079025269
Epoch 890, training loss: 0.0738142803311348 = 0.003687203861773014 + 0.01 * 7.0127081871032715
Epoch 890, val loss: 1.1379413604736328
Epoch 900, training loss: 0.07355702668428421 = 0.003593528177589178 + 0.01 * 6.996350288391113
Epoch 900, val loss: 1.1414545774459839
Epoch 910, training loss: 0.07331093400716782 = 0.0035043859388679266 + 0.01 * 6.980654716491699
Epoch 910, val loss: 1.1450965404510498
Epoch 920, training loss: 0.07328353822231293 = 0.0034195089247077703 + 0.01 * 6.986402988433838
Epoch 920, val loss: 1.1484014987945557
Epoch 930, training loss: 0.07305227965116501 = 0.0033383285626769066 + 0.01 * 6.971395492553711
Epoch 930, val loss: 1.151830792427063
Epoch 940, training loss: 0.07324148714542389 = 0.0032608667388558388 + 0.01 * 6.998062610626221
Epoch 940, val loss: 1.1550486087799072
Epoch 950, training loss: 0.07274585962295532 = 0.0031866265926510096 + 0.01 * 6.955923080444336
Epoch 950, val loss: 1.158275842666626
Epoch 960, training loss: 0.07292017340660095 = 0.0031157485209405422 + 0.01 * 6.980442523956299
Epoch 960, val loss: 1.161412239074707
Epoch 970, training loss: 0.07255531847476959 = 0.0030478646513074636 + 0.01 * 6.950746059417725
Epoch 970, val loss: 1.1644227504730225
Epoch 980, training loss: 0.07242871075868607 = 0.002982925856485963 + 0.01 * 6.944578170776367
Epoch 980, val loss: 1.167397379875183
Epoch 990, training loss: 0.0723046064376831 = 0.0029206289909780025 + 0.01 * 6.938398361206055
Epoch 990, val loss: 1.170331358909607
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8186610437532947
=== training gcn model ===
Epoch 0, training loss: 2.044097661972046 = 1.9581291675567627 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.9632225036621094
Epoch 10, training loss: 2.0335214138031006 = 1.9475535154342651 + 0.01 * 8.596796035766602
Epoch 10, val loss: 1.9530826807022095
Epoch 20, training loss: 2.0201926231384277 = 1.934226393699646 + 0.01 * 8.596626281738281
Epoch 20, val loss: 1.9397259950637817
Epoch 30, training loss: 2.001281499862671 = 1.9153203964233398 + 0.01 * 8.596109390258789
Epoch 30, val loss: 1.9201546907424927
Epoch 40, training loss: 1.9734755754470825 = 1.8875408172607422 + 0.01 * 8.593481063842773
Epoch 40, val loss: 1.8912440538406372
Epoch 50, training loss: 1.9355732202529907 = 1.8498247861862183 + 0.01 * 8.574848175048828
Epoch 50, val loss: 1.8529400825500488
Epoch 60, training loss: 1.8952381610870361 = 1.8103764057159424 + 0.01 * 8.486177444458008
Epoch 60, val loss: 1.8152236938476562
Epoch 70, training loss: 1.8622103929519653 = 1.7792084217071533 + 0.01 * 8.300202369689941
Epoch 70, val loss: 1.7860331535339355
Epoch 80, training loss: 1.8221595287322998 = 1.7409454584121704 + 0.01 * 8.121411323547363
Epoch 80, val loss: 1.75102961063385
Epoch 90, training loss: 1.768257975578308 = 1.6883351802825928 + 0.01 * 7.992285251617432
Epoch 90, val loss: 1.7063770294189453
Epoch 100, training loss: 1.6985759735107422 = 1.6190992593765259 + 0.01 * 7.947666645050049
Epoch 100, val loss: 1.6501119136810303
Epoch 110, training loss: 1.6203020811080933 = 1.541276216506958 + 0.01 * 7.902591705322266
Epoch 110, val loss: 1.5868936777114868
Epoch 120, training loss: 1.5431734323501587 = 1.4656165838241577 + 0.01 * 7.755684852600098
Epoch 120, val loss: 1.5281360149383545
Epoch 130, training loss: 1.471766471862793 = 1.396039605140686 + 0.01 * 7.572692394256592
Epoch 130, val loss: 1.4745951890945435
Epoch 140, training loss: 1.4056956768035889 = 1.3310754299163818 + 0.01 * 7.462021350860596
Epoch 140, val loss: 1.4259121417999268
Epoch 150, training loss: 1.343329906463623 = 1.2695585489273071 + 0.01 * 7.377131938934326
Epoch 150, val loss: 1.3801698684692383
Epoch 160, training loss: 1.2842210531234741 = 1.210960865020752 + 0.01 * 7.326022624969482
Epoch 160, val loss: 1.3382503986358643
Epoch 170, training loss: 1.2257767915725708 = 1.1528735160827637 + 0.01 * 7.290333271026611
Epoch 170, val loss: 1.2975128889083862
Epoch 180, training loss: 1.1643671989440918 = 1.091715693473816 + 0.01 * 7.265156269073486
Epoch 180, val loss: 1.2533411979675293
Epoch 190, training loss: 1.0977412462234497 = 1.0253204107284546 + 0.01 * 7.242086410522461
Epoch 190, val loss: 1.20406174659729
Epoch 200, training loss: 1.026183843612671 = 0.9539701342582703 + 0.01 * 7.22137451171875
Epoch 200, val loss: 1.1507900953292847
Epoch 210, training loss: 0.9522644281387329 = 0.8802138566970825 + 0.01 * 7.2050604820251465
Epoch 210, val loss: 1.0962140560150146
Epoch 220, training loss: 0.8793896436691284 = 0.8074472546577454 + 0.01 * 7.194236755371094
Epoch 220, val loss: 1.0439311265945435
Epoch 230, training loss: 0.8097110986709595 = 0.7378292679786682 + 0.01 * 7.188185214996338
Epoch 230, val loss: 0.9962190985679626
Epoch 240, training loss: 0.7438890933990479 = 0.6720507740974426 + 0.01 * 7.183834552764893
Epoch 240, val loss: 0.9540503621101379
Epoch 250, training loss: 0.6817612051963806 = 0.6099399924278259 + 0.01 * 7.182122230529785
Epoch 250, val loss: 0.917035698890686
Epoch 260, training loss: 0.6231076717376709 = 0.5512945055961609 + 0.01 * 7.181315898895264
Epoch 260, val loss: 0.8849489688873291
Epoch 270, training loss: 0.5681219696998596 = 0.4963313937187195 + 0.01 * 7.179056167602539
Epoch 270, val loss: 0.8582518696784973
Epoch 280, training loss: 0.5170122385025024 = 0.44525450468063354 + 0.01 * 7.17577600479126
Epoch 280, val loss: 0.8377858400344849
Epoch 290, training loss: 0.46969321370124817 = 0.39794886112213135 + 0.01 * 7.174435138702393
Epoch 290, val loss: 0.8244544267654419
Epoch 300, training loss: 0.42581474781036377 = 0.35408127307891846 + 0.01 * 7.173346996307373
Epoch 300, val loss: 0.817733645439148
Epoch 310, training loss: 0.3850554823875427 = 0.3133394420146942 + 0.01 * 7.171605110168457
Epoch 310, val loss: 0.8165149092674255
Epoch 320, training loss: 0.34722304344177246 = 0.2755427658557892 + 0.01 * 7.168026447296143
Epoch 320, val loss: 0.8195236325263977
Epoch 330, training loss: 0.31251540780067444 = 0.24075409770011902 + 0.01 * 7.176131725311279
Epoch 330, val loss: 0.8261080980300903
Epoch 340, training loss: 0.2809467911720276 = 0.20927099883556366 + 0.01 * 7.167579174041748
Epoch 340, val loss: 0.83574378490448
Epoch 350, training loss: 0.253006249666214 = 0.1813640147447586 + 0.01 * 7.1642231941223145
Epoch 350, val loss: 0.8479982018470764
Epoch 360, training loss: 0.22868509590625763 = 0.1570630818605423 + 0.01 * 7.162201404571533
Epoch 360, val loss: 0.8624643683433533
Epoch 370, training loss: 0.20773248374462128 = 0.13612619042396545 + 0.01 * 7.1606292724609375
Epoch 370, val loss: 0.879040002822876
Epoch 380, training loss: 0.18978041410446167 = 0.11819387227296829 + 0.01 * 7.158654689788818
Epoch 380, val loss: 0.8972617387771606
Epoch 390, training loss: 0.17446094751358032 = 0.10288550704717636 + 0.01 * 7.1575446128845215
Epoch 390, val loss: 0.9168307781219482
Epoch 400, training loss: 0.16134916245937347 = 0.08983961492776871 + 0.01 * 7.150954723358154
Epoch 400, val loss: 0.9373331665992737
Epoch 410, training loss: 0.15020792186260223 = 0.07873238623142242 + 0.01 * 7.14755392074585
Epoch 410, val loss: 0.9583950638771057
Epoch 420, training loss: 0.14080676436424255 = 0.0692831352353096 + 0.01 * 7.152363300323486
Epoch 420, val loss: 0.9796617031097412
Epoch 430, training loss: 0.13263368606567383 = 0.061238352209329605 + 0.01 * 7.139533519744873
Epoch 430, val loss: 1.0009968280792236
Epoch 440, training loss: 0.125737264752388 = 0.05437415838241577 + 0.01 * 7.136310577392578
Epoch 440, val loss: 1.0220690965652466
Epoch 450, training loss: 0.11984650790691376 = 0.048503752797842026 + 0.01 * 7.134276390075684
Epoch 450, val loss: 1.0427336692810059
Epoch 460, training loss: 0.11477412283420563 = 0.04346847161650658 + 0.01 * 7.130565643310547
Epoch 460, val loss: 1.0628670454025269
Epoch 470, training loss: 0.11038628220558167 = 0.039133939892053604 + 0.01 * 7.125235080718994
Epoch 470, val loss: 1.0824257135391235
Epoch 480, training loss: 0.106668621301651 = 0.035381656140089035 + 0.01 * 7.128696441650391
Epoch 480, val loss: 1.1013978719711304
Epoch 490, training loss: 0.10316584259271622 = 0.03212336450815201 + 0.01 * 7.104248046875
Epoch 490, val loss: 1.119679570198059
Epoch 500, training loss: 0.1003076508641243 = 0.029276743531227112 + 0.01 * 7.103091239929199
Epoch 500, val loss: 1.1372966766357422
Epoch 510, training loss: 0.09773585200309753 = 0.026779385283589363 + 0.01 * 7.09564733505249
Epoch 510, val loss: 1.1543153524398804
Epoch 520, training loss: 0.09561201184988022 = 0.024578751996159554 + 0.01 * 7.103326320648193
Epoch 520, val loss: 1.1706759929656982
Epoch 530, training loss: 0.09351259469985962 = 0.022633519023656845 + 0.01 * 7.087907791137695
Epoch 530, val loss: 1.1864122152328491
Epoch 540, training loss: 0.09176625311374664 = 0.02090691216289997 + 0.01 * 7.085934638977051
Epoch 540, val loss: 1.2015737295150757
Epoch 550, training loss: 0.09013547748327255 = 0.019369537010788918 + 0.01 * 7.076594352722168
Epoch 550, val loss: 1.216154932975769
Epoch 560, training loss: 0.08863677084445953 = 0.017995811998844147 + 0.01 * 7.064095973968506
Epoch 560, val loss: 1.2301807403564453
Epoch 570, training loss: 0.08729364722967148 = 0.01676437444984913 + 0.01 * 7.052927494049072
Epoch 570, val loss: 1.243685245513916
Epoch 580, training loss: 0.08613693714141846 = 0.015657002106308937 + 0.01 * 7.047994136810303
Epoch 580, val loss: 1.2566064596176147
Epoch 590, training loss: 0.0851624459028244 = 0.01465802825987339 + 0.01 * 7.050442218780518
Epoch 590, val loss: 1.2690075635910034
Epoch 600, training loss: 0.08430047333240509 = 0.013754326850175858 + 0.01 * 7.054614543914795
Epoch 600, val loss: 1.2809839248657227
Epoch 610, training loss: 0.08345425128936768 = 0.01293470524251461 + 0.01 * 7.051954746246338
Epoch 610, val loss: 1.2924772500991821
Epoch 620, training loss: 0.08263511955738068 = 0.012189473025500774 + 0.01 * 7.044565200805664
Epoch 620, val loss: 1.3035939931869507
Epoch 630, training loss: 0.08187735825777054 = 0.01150958240032196 + 0.01 * 7.036777496337891
Epoch 630, val loss: 1.3143112659454346
Epoch 640, training loss: 0.08108578622341156 = 0.01088804192841053 + 0.01 * 7.019774913787842
Epoch 640, val loss: 1.3246031999588013
Epoch 650, training loss: 0.0804218202829361 = 0.010318074375391006 + 0.01 * 7.010374069213867
Epoch 650, val loss: 1.3345391750335693
Epoch 660, training loss: 0.08000259846448898 = 0.009794473648071289 + 0.01 * 7.02081298828125
Epoch 660, val loss: 1.344029426574707
Epoch 670, training loss: 0.0794883593916893 = 0.009312831796705723 + 0.01 * 7.017553329467773
Epoch 670, val loss: 1.3532471656799316
Epoch 680, training loss: 0.07910394668579102 = 0.008868531323969364 + 0.01 * 7.0235419273376465
Epoch 680, val loss: 1.3621193170547485
Epoch 690, training loss: 0.07860803604125977 = 0.008457893505692482 + 0.01 * 7.0150146484375
Epoch 690, val loss: 1.3707605600357056
Epoch 700, training loss: 0.07827797532081604 = 0.008077719248831272 + 0.01 * 7.020025730133057
Epoch 700, val loss: 1.37901771068573
Epoch 710, training loss: 0.07755802571773529 = 0.007724929600954056 + 0.01 * 6.983310222625732
Epoch 710, val loss: 1.3870939016342163
Epoch 720, training loss: 0.07726253569126129 = 0.007397016044706106 + 0.01 * 6.986551761627197
Epoch 720, val loss: 1.3948147296905518
Epoch 730, training loss: 0.07698319852352142 = 0.007091457024216652 + 0.01 * 6.989174842834473
Epoch 730, val loss: 1.4023261070251465
Epoch 740, training loss: 0.07662735879421234 = 0.006806218530982733 + 0.01 * 6.982114315032959
Epoch 740, val loss: 1.4096050262451172
Epoch 750, training loss: 0.0762542113661766 = 0.006540335714817047 + 0.01 * 6.9713873863220215
Epoch 750, val loss: 1.4165794849395752
Epoch 760, training loss: 0.07601714134216309 = 0.006291416008025408 + 0.01 * 6.972572326660156
Epoch 760, val loss: 1.4232641458511353
Epoch 770, training loss: 0.07574570178985596 = 0.006058505270630121 + 0.01 * 6.968719482421875
Epoch 770, val loss: 1.4298200607299805
Epoch 780, training loss: 0.07551007717847824 = 0.0058401054702699184 + 0.01 * 6.966997146606445
Epoch 780, val loss: 1.4361759424209595
Epoch 790, training loss: 0.07526689022779465 = 0.005635029170662165 + 0.01 * 6.963186264038086
Epoch 790, val loss: 1.442378044128418
Epoch 800, training loss: 0.0750405341386795 = 0.005442062858492136 + 0.01 * 6.959847450256348
Epoch 800, val loss: 1.448325753211975
Epoch 810, training loss: 0.07507672160863876 = 0.005260454025119543 + 0.01 * 6.981626987457275
Epoch 810, val loss: 1.454057216644287
Epoch 820, training loss: 0.07437177747488022 = 0.00508928345516324 + 0.01 * 6.928249835968018
Epoch 820, val loss: 1.45963716506958
Epoch 830, training loss: 0.07409296929836273 = 0.0049278950318694115 + 0.01 * 6.916507244110107
Epoch 830, val loss: 1.4649730920791626
Epoch 840, training loss: 0.07395540922880173 = 0.0047753360122442245 + 0.01 * 6.9180073738098145
Epoch 840, val loss: 1.4701855182647705
Epoch 850, training loss: 0.07433716952800751 = 0.004630953539162874 + 0.01 * 6.9706220626831055
Epoch 850, val loss: 1.4752713441848755
Epoch 860, training loss: 0.07353271543979645 = 0.0044938731007277966 + 0.01 * 6.903884410858154
Epoch 860, val loss: 1.4801979064941406
Epoch 870, training loss: 0.07355369627475739 = 0.004364196676760912 + 0.01 * 6.918949604034424
Epoch 870, val loss: 1.4849480390548706
Epoch 880, training loss: 0.07327469438314438 = 0.004241335205733776 + 0.01 * 6.903336524963379
Epoch 880, val loss: 1.4896916151046753
Epoch 890, training loss: 0.07294241338968277 = 0.004124530591070652 + 0.01 * 6.881788730621338
Epoch 890, val loss: 1.4940674304962158
Epoch 900, training loss: 0.07305007427930832 = 0.004013508092612028 + 0.01 * 6.903656482696533
Epoch 900, val loss: 1.4983546733856201
Epoch 910, training loss: 0.0728338360786438 = 0.003907645121216774 + 0.01 * 6.892619609832764
Epoch 910, val loss: 1.5026930570602417
Epoch 920, training loss: 0.072663314640522 = 0.0038069996517151594 + 0.01 * 6.885631561279297
Epoch 920, val loss: 1.5067412853240967
Epoch 930, training loss: 0.07241620868444443 = 0.0037112452555447817 + 0.01 * 6.8704962730407715
Epoch 930, val loss: 1.5108239650726318
Epoch 940, training loss: 0.07238515466451645 = 0.0036197311710566282 + 0.01 * 6.876542568206787
Epoch 940, val loss: 1.5147993564605713
Epoch 950, training loss: 0.07237476855516434 = 0.003532364498823881 + 0.01 * 6.884240627288818
Epoch 950, val loss: 1.5186740159988403
Epoch 960, training loss: 0.07218746840953827 = 0.0034487578086555004 + 0.01 * 6.873871803283691
Epoch 960, val loss: 1.5224720239639282
Epoch 970, training loss: 0.07212722301483154 = 0.003368902951478958 + 0.01 * 6.8758320808410645
Epoch 970, val loss: 1.5260030031204224
Epoch 980, training loss: 0.07191374152898788 = 0.003292574780061841 + 0.01 * 6.862116813659668
Epoch 980, val loss: 1.5296165943145752
Epoch 990, training loss: 0.07185163348913193 = 0.0032194575760513544 + 0.01 * 6.863217830657959
Epoch 990, val loss: 1.5329630374908447
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8191881918819188
The final CL Acc:0.75679, 0.00761, The final GNN Acc:0.81884, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13114])
remove edge: torch.Size([2, 7888])
updated graph: torch.Size([2, 10446])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0430610179901123 = 1.9570926427841187 + 0.01 * 8.596833229064941
Epoch 0, val loss: 1.9600244760513306
Epoch 10, training loss: 2.03214430809021 = 1.9461771249771118 + 0.01 * 8.59672737121582
Epoch 10, val loss: 1.9487779140472412
Epoch 20, training loss: 2.0180654525756836 = 1.9321011304855347 + 0.01 * 8.596424102783203
Epoch 20, val loss: 1.9343422651290894
Epoch 30, training loss: 1.9979106187820435 = 1.9119575023651123 + 0.01 * 8.595315933227539
Epoch 30, val loss: 1.9135398864746094
Epoch 40, training loss: 1.9680426120758057 = 1.8821690082550049 + 0.01 * 8.587359428405762
Epoch 40, val loss: 1.8829944133758545
Epoch 50, training loss: 1.92673921585083 = 1.8414270877838135 + 0.01 * 8.531210899353027
Epoch 50, val loss: 1.8430191278457642
Epoch 60, training loss: 1.881402850151062 = 1.798518180847168 + 0.01 * 8.288468360900879
Epoch 60, val loss: 1.8050708770751953
Epoch 70, training loss: 1.8453762531280518 = 1.7636910676956177 + 0.01 * 8.16851806640625
Epoch 70, val loss: 1.7761108875274658
Epoch 80, training loss: 1.797933578491211 = 1.719038486480713 + 0.01 * 7.8895063400268555
Epoch 80, val loss: 1.735493779182434
Epoch 90, training loss: 1.7337909936904907 = 1.657719612121582 + 0.01 * 7.6071391105651855
Epoch 90, val loss: 1.6813362836837769
Epoch 100, training loss: 1.6516237258911133 = 1.5764203071594238 + 0.01 * 7.5203447341918945
Epoch 100, val loss: 1.6124155521392822
Epoch 110, training loss: 1.5572093725204468 = 1.4823029041290283 + 0.01 * 7.4906511306762695
Epoch 110, val loss: 1.5337870121002197
Epoch 120, training loss: 1.4599494934082031 = 1.3853223323822021 + 0.01 * 7.462717533111572
Epoch 120, val loss: 1.4557641744613647
Epoch 130, training loss: 1.3622450828552246 = 1.2879002094268799 + 0.01 * 7.434492111206055
Epoch 130, val loss: 1.3785046339035034
Epoch 140, training loss: 1.2613778114318848 = 1.1873705387115479 + 0.01 * 7.400732517242432
Epoch 140, val loss: 1.2995612621307373
Epoch 150, training loss: 1.1566468477249146 = 1.0832695960998535 + 0.01 * 7.337730407714844
Epoch 150, val loss: 1.2175332307815552
Epoch 160, training loss: 1.0514609813690186 = 0.9791009426116943 + 0.01 * 7.23599910736084
Epoch 160, val loss: 1.1363837718963623
Epoch 170, training loss: 0.9527153968811035 = 0.8807752132415771 + 0.01 * 7.194018363952637
Epoch 170, val loss: 1.0619429349899292
Epoch 180, training loss: 0.8642346858978271 = 0.7926419973373413 + 0.01 * 7.159265995025635
Epoch 180, val loss: 0.9979382753372192
Epoch 190, training loss: 0.7881720066070557 = 0.7166107892990112 + 0.01 * 7.1561198234558105
Epoch 190, val loss: 0.9468104839324951
Epoch 200, training loss: 0.7234797477722168 = 0.6521954536437988 + 0.01 * 7.128429412841797
Epoch 200, val loss: 0.9088274240493774
Epoch 210, training loss: 0.6683810949325562 = 0.5973133444786072 + 0.01 * 7.106773853302002
Epoch 210, val loss: 0.8822588920593262
Epoch 220, training loss: 0.6209309101104736 = 0.5499864816665649 + 0.01 * 7.094445705413818
Epoch 220, val loss: 0.864813506603241
Epoch 230, training loss: 0.5797150731086731 = 0.5087994933128357 + 0.01 * 7.091556072235107
Epoch 230, val loss: 0.8541949391365051
Epoch 240, training loss: 0.5429778099060059 = 0.47221383452415466 + 0.01 * 7.0763959884643555
Epoch 240, val loss: 0.8481972217559814
Epoch 250, training loss: 0.5091124773025513 = 0.43846964836120605 + 0.01 * 7.064280033111572
Epoch 250, val loss: 0.8450227379798889
Epoch 260, training loss: 0.47637805342674255 = 0.4058561623096466 + 0.01 * 7.052188873291016
Epoch 260, val loss: 0.8433160781860352
Epoch 270, training loss: 0.4433080852031708 = 0.37287500500679016 + 0.01 * 7.043309211730957
Epoch 270, val loss: 0.8424561023712158
Epoch 280, training loss: 0.4094427824020386 = 0.33876463770866394 + 0.01 * 7.067813873291016
Epoch 280, val loss: 0.8427870869636536
Epoch 290, training loss: 0.3742925524711609 = 0.30398279428482056 + 0.01 * 7.030974864959717
Epoch 290, val loss: 0.8452578783035278
Epoch 300, training loss: 0.3400762975215912 = 0.26969286799430847 + 0.01 * 7.038342475891113
Epoch 300, val loss: 0.8502240180969238
Epoch 310, training loss: 0.3074258267879486 = 0.2372392863035202 + 0.01 * 7.018654823303223
Epoch 310, val loss: 0.8577570915222168
Epoch 320, training loss: 0.2776973843574524 = 0.2075922191143036 + 0.01 * 7.0105156898498535
Epoch 320, val loss: 0.8677164912223816
Epoch 330, training loss: 0.25148504972457886 = 0.18138277530670166 + 0.01 * 7.010226249694824
Epoch 330, val loss: 0.880323052406311
Epoch 340, training loss: 0.22888755798339844 = 0.15876194834709167 + 0.01 * 7.012560844421387
Epoch 340, val loss: 0.8957074284553528
Epoch 350, training loss: 0.20946067571640015 = 0.1394953578710556 + 0.01 * 6.996532917022705
Epoch 350, val loss: 0.9135899543762207
Epoch 360, training loss: 0.1929285228252411 = 0.1231270581483841 + 0.01 * 6.980146884918213
Epoch 360, val loss: 0.9334793090820312
Epoch 370, training loss: 0.17905008792877197 = 0.10916733741760254 + 0.01 * 6.988275527954102
Epoch 370, val loss: 0.9548695683479309
Epoch 380, training loss: 0.16706375777721405 = 0.09720144420862198 + 0.01 * 6.986231327056885
Epoch 380, val loss: 0.9771181344985962
Epoch 390, training loss: 0.15651769936084747 = 0.08687634766101837 + 0.01 * 6.96413516998291
Epoch 390, val loss: 0.9998487830162048
Epoch 400, training loss: 0.1477210968732834 = 0.07792098075151443 + 0.01 * 6.980011463165283
Epoch 400, val loss: 1.02273690700531
Epoch 410, training loss: 0.13969871401786804 = 0.07012961059808731 + 0.01 * 6.956911563873291
Epoch 410, val loss: 1.0455151796340942
Epoch 420, training loss: 0.13292735815048218 = 0.06331595778465271 + 0.01 * 6.9611406326293945
Epoch 420, val loss: 1.068024754524231
Epoch 430, training loss: 0.12696832418441772 = 0.05733947083353996 + 0.01 * 6.96288537979126
Epoch 430, val loss: 1.0901868343353271
Epoch 440, training loss: 0.12162907421588898 = 0.052082885056734085 + 0.01 * 6.954619407653809
Epoch 440, val loss: 1.1118271350860596
Epoch 450, training loss: 0.1171911209821701 = 0.047445718199014664 + 0.01 * 6.974539756774902
Epoch 450, val loss: 1.1330013275146484
Epoch 460, training loss: 0.11277855932712555 = 0.04334854707121849 + 0.01 * 6.943000793457031
Epoch 460, val loss: 1.1535594463348389
Epoch 470, training loss: 0.109035924077034 = 0.039716292172670364 + 0.01 * 6.9319634437561035
Epoch 470, val loss: 1.1735916137695312
Epoch 480, training loss: 0.1057189479470253 = 0.036486588418483734 + 0.01 * 6.92323637008667
Epoch 480, val loss: 1.1931140422821045
Epoch 490, training loss: 0.10302643477916718 = 0.033604491502046585 + 0.01 * 6.942194938659668
Epoch 490, val loss: 1.212182641029358
Epoch 500, training loss: 0.10032269358634949 = 0.03103168122470379 + 0.01 * 6.92910099029541
Epoch 500, val loss: 1.2306691408157349
Epoch 510, training loss: 0.09782521426677704 = 0.028729382902383804 + 0.01 * 6.909582614898682
Epoch 510, val loss: 1.2486236095428467
Epoch 520, training loss: 0.09590011835098267 = 0.026659443974494934 + 0.01 * 6.924067497253418
Epoch 520, val loss: 1.2661188840866089
Epoch 530, training loss: 0.09384094178676605 = 0.024795420467853546 + 0.01 * 6.904551982879639
Epoch 530, val loss: 1.2831519842147827
Epoch 540, training loss: 0.09233801066875458 = 0.02311062440276146 + 0.01 * 6.922738075256348
Epoch 540, val loss: 1.299745798110962
Epoch 550, training loss: 0.09071332216262817 = 0.021587785333395004 + 0.01 * 6.9125542640686035
Epoch 550, val loss: 1.3158764839172363
Epoch 560, training loss: 0.08905579894781113 = 0.020206494256854057 + 0.01 * 6.88493013381958
Epoch 560, val loss: 1.3315932750701904
Epoch 570, training loss: 0.0879683792591095 = 0.018950682133436203 + 0.01 * 6.901769638061523
Epoch 570, val loss: 1.3468530178070068
Epoch 580, training loss: 0.08684150129556656 = 0.017809100449085236 + 0.01 * 6.903240203857422
Epoch 580, val loss: 1.3617135286331177
Epoch 590, training loss: 0.08551697432994843 = 0.016766933724284172 + 0.01 * 6.875004291534424
Epoch 590, val loss: 1.3761669397354126
Epoch 600, training loss: 0.08455871045589447 = 0.01581340655684471 + 0.01 * 6.874531269073486
Epoch 600, val loss: 1.3903007507324219
Epoch 610, training loss: 0.0835997462272644 = 0.014939920045435429 + 0.01 * 6.865983009338379
Epoch 610, val loss: 1.4040493965148926
Epoch 620, training loss: 0.08304142206907272 = 0.014138124883174896 + 0.01 * 6.890329837799072
Epoch 620, val loss: 1.4174312353134155
Epoch 630, training loss: 0.08223813772201538 = 0.013400704599916935 + 0.01 * 6.8837432861328125
Epoch 630, val loss: 1.4304360151290894
Epoch 640, training loss: 0.08130432665348053 = 0.012722277082502842 + 0.01 * 6.8582048416137695
Epoch 640, val loss: 1.4430643320083618
Epoch 650, training loss: 0.0805273950099945 = 0.01209568977355957 + 0.01 * 6.843170642852783
Epoch 650, val loss: 1.4553707838058472
Epoch 660, training loss: 0.08019543439149857 = 0.01151634193956852 + 0.01 * 6.867909908294678
Epoch 660, val loss: 1.4673575162887573
Epoch 670, training loss: 0.07963734120130539 = 0.010979288257658482 + 0.01 * 6.865805625915527
Epoch 670, val loss: 1.479034662246704
Epoch 680, training loss: 0.07889145612716675 = 0.010481334291398525 + 0.01 * 6.841012001037598
Epoch 680, val loss: 1.4904557466506958
Epoch 690, training loss: 0.07840239256620407 = 0.01001835148781538 + 0.01 * 6.838404178619385
Epoch 690, val loss: 1.5015286207199097
Epoch 700, training loss: 0.07780896872282028 = 0.0095874248072505 + 0.01 * 6.822154521942139
Epoch 700, val loss: 1.512365460395813
Epoch 710, training loss: 0.07749070972204208 = 0.009185961447656155 + 0.01 * 6.830474853515625
Epoch 710, val loss: 1.5229345560073853
Epoch 720, training loss: 0.0770559310913086 = 0.00881113950163126 + 0.01 * 6.824479103088379
Epoch 720, val loss: 1.5332260131835938
Epoch 730, training loss: 0.07666122168302536 = 0.00846089143306017 + 0.01 * 6.820032596588135
Epoch 730, val loss: 1.543222188949585
Epoch 740, training loss: 0.07623256742954254 = 0.008133062161505222 + 0.01 * 6.809950828552246
Epoch 740, val loss: 1.5530225038528442
Epoch 750, training loss: 0.07584774494171143 = 0.007825636304914951 + 0.01 * 6.802211284637451
Epoch 750, val loss: 1.5625579357147217
Epoch 760, training loss: 0.07576218992471695 = 0.007537215016782284 + 0.01 * 6.822497844696045
Epoch 760, val loss: 1.5719071626663208
Epoch 770, training loss: 0.07521475851535797 = 0.007266402244567871 + 0.01 * 6.794836044311523
Epoch 770, val loss: 1.581000804901123
Epoch 780, training loss: 0.074928879737854 = 0.007011485751718283 + 0.01 * 6.791739463806152
Epoch 780, val loss: 1.5899085998535156
Epoch 790, training loss: 0.07462463527917862 = 0.006771270651370287 + 0.01 * 6.785336494445801
Epoch 790, val loss: 1.5985995531082153
Epoch 800, training loss: 0.07439552992582321 = 0.006544574163854122 + 0.01 * 6.785095691680908
Epoch 800, val loss: 1.607148289680481
Epoch 810, training loss: 0.07418186217546463 = 0.006330383475869894 + 0.01 * 6.785147666931152
Epoch 810, val loss: 1.6154428720474243
Epoch 820, training loss: 0.07382795959711075 = 0.006128291133791208 + 0.01 * 6.769967555999756
Epoch 820, val loss: 1.623569130897522
Epoch 830, training loss: 0.07417348772287369 = 0.005937003064900637 + 0.01 * 6.823648929595947
Epoch 830, val loss: 1.6315791606903076
Epoch 840, training loss: 0.073258176445961 = 0.005755580961704254 + 0.01 * 6.7502593994140625
Epoch 840, val loss: 1.6393744945526123
Epoch 850, training loss: 0.07323330640792847 = 0.005583618301898241 + 0.01 * 6.764968395233154
Epoch 850, val loss: 1.6469718217849731
Epoch 860, training loss: 0.07296682149171829 = 0.005420466419309378 + 0.01 * 6.754635334014893
Epoch 860, val loss: 1.6544171571731567
Epoch 870, training loss: 0.0727381706237793 = 0.005265665240585804 + 0.01 * 6.747250556945801
Epoch 870, val loss: 1.6617070436477661
Epoch 880, training loss: 0.0725988894701004 = 0.005118379835039377 + 0.01 * 6.748051166534424
Epoch 880, val loss: 1.6688681840896606
Epoch 890, training loss: 0.07228171825408936 = 0.00497800437733531 + 0.01 * 6.730371952056885
Epoch 890, val loss: 1.6758713722229004
Epoch 900, training loss: 0.07234708964824677 = 0.004844427574425936 + 0.01 * 6.7502665519714355
Epoch 900, val loss: 1.6827203035354614
Epoch 910, training loss: 0.07222726941108704 = 0.004717082716524601 + 0.01 * 6.75101900100708
Epoch 910, val loss: 1.689460277557373
Epoch 920, training loss: 0.07190893590450287 = 0.004595309495925903 + 0.01 * 6.731362819671631
Epoch 920, val loss: 1.696054220199585
Epoch 930, training loss: 0.0717482641339302 = 0.004479208495467901 + 0.01 * 6.726905822753906
Epoch 930, val loss: 1.7025359869003296
Epoch 940, training loss: 0.07183386385440826 = 0.004368076100945473 + 0.01 * 6.746579170227051
Epoch 940, val loss: 1.7088422775268555
Epoch 950, training loss: 0.07173041254281998 = 0.004262204747647047 + 0.01 * 6.746821403503418
Epoch 950, val loss: 1.7151330709457397
Epoch 960, training loss: 0.0714188739657402 = 0.004160604439675808 + 0.01 * 6.725826740264893
Epoch 960, val loss: 1.7211999893188477
Epoch 970, training loss: 0.07125455141067505 = 0.0040636747144162655 + 0.01 * 6.719087600708008
Epoch 970, val loss: 1.7272382974624634
Epoch 980, training loss: 0.07132439315319061 = 0.003970650024712086 + 0.01 * 6.735374450683594
Epoch 980, val loss: 1.7331002950668335
Epoch 990, training loss: 0.07111499458551407 = 0.0038817140739411116 + 0.01 * 6.723328113555908
Epoch 990, val loss: 1.738951563835144
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.028245449066162 = 1.942277193069458 + 0.01 * 8.59683609008789
Epoch 0, val loss: 1.9450932741165161
Epoch 10, training loss: 2.018562078475952 = 1.9325944185256958 + 0.01 * 8.596766471862793
Epoch 10, val loss: 1.9348633289337158
Epoch 20, training loss: 2.00679349899292 = 1.9208282232284546 + 0.01 * 8.596522331237793
Epoch 20, val loss: 1.9222291707992554
Epoch 30, training loss: 1.990658164024353 = 1.9047014713287354 + 0.01 * 8.595664978027344
Epoch 30, val loss: 1.904890775680542
Epoch 40, training loss: 1.9671611785888672 = 1.88125479221344 + 0.01 * 8.590641021728516
Epoch 40, val loss: 1.8800880908966064
Epoch 50, training loss: 1.9337739944458008 = 1.8482062816619873 + 0.01 * 8.556766510009766
Epoch 50, val loss: 1.8463494777679443
Epoch 60, training loss: 1.8922510147094727 = 1.808495044708252 + 0.01 * 8.375598907470703
Epoch 60, val loss: 1.808911681175232
Epoch 70, training loss: 1.8530521392822266 = 1.7708479166030884 + 0.01 * 8.220423698425293
Epoch 70, val loss: 1.7773215770721436
Epoch 80, training loss: 1.8078434467315674 = 1.728296160697937 + 0.01 * 7.9547319412231445
Epoch 80, val loss: 1.7405201196670532
Epoch 90, training loss: 1.7444477081298828 = 1.6684306859970093 + 0.01 * 7.60169792175293
Epoch 90, val loss: 1.6866830587387085
Epoch 100, training loss: 1.6613166332244873 = 1.5869888067245483 + 0.01 * 7.432783126831055
Epoch 100, val loss: 1.6137878894805908
Epoch 110, training loss: 1.5587633848190308 = 1.4851508140563965 + 0.01 * 7.361257076263428
Epoch 110, val loss: 1.5254361629486084
Epoch 120, training loss: 1.4450807571411133 = 1.3720542192459106 + 0.01 * 7.3026533126831055
Epoch 120, val loss: 1.4293479919433594
Epoch 130, training loss: 1.3298314809799194 = 1.257054090499878 + 0.01 * 7.277744770050049
Epoch 130, val loss: 1.3339742422103882
Epoch 140, training loss: 1.2190231084823608 = 1.1464108228683472 + 0.01 * 7.261226654052734
Epoch 140, val loss: 1.243916630744934
Epoch 150, training loss: 1.1176352500915527 = 1.045216679573059 + 0.01 * 7.2418532371521
Epoch 150, val loss: 1.1634414196014404
Epoch 160, training loss: 1.026928424835205 = 0.9547240734100342 + 0.01 * 7.220438480377197
Epoch 160, val loss: 1.0934404134750366
Epoch 170, training loss: 0.9442445635795593 = 0.8722981214523315 + 0.01 * 7.194642543792725
Epoch 170, val loss: 1.0311002731323242
Epoch 180, training loss: 0.8665547370910645 = 0.7948204278945923 + 0.01 * 7.173431396484375
Epoch 180, val loss: 0.9731889367103577
Epoch 190, training loss: 0.7927559018135071 = 0.7212482690811157 + 0.01 * 7.150762557983398
Epoch 190, val loss: 0.9191921949386597
Epoch 200, training loss: 0.7232699990272522 = 0.6518579125404358 + 0.01 * 7.141210556030273
Epoch 200, val loss: 0.8703400492668152
Epoch 210, training loss: 0.6583743095397949 = 0.5871971249580383 + 0.01 * 7.11771821975708
Epoch 210, val loss: 0.8282945156097412
Epoch 220, training loss: 0.5988446474075317 = 0.5277671217918396 + 0.01 * 7.107754707336426
Epoch 220, val loss: 0.7940704822540283
Epoch 230, training loss: 0.5448524355888367 = 0.4738491475582123 + 0.01 * 7.10032844543457
Epoch 230, val loss: 0.7674726247787476
Epoch 240, training loss: 0.4963565766811371 = 0.42543718218803406 + 0.01 * 7.091940402984619
Epoch 240, val loss: 0.7477431893348694
Epoch 250, training loss: 0.45283806324005127 = 0.38197579979896545 + 0.01 * 7.0862274169921875
Epoch 250, val loss: 0.733882486820221
Epoch 260, training loss: 0.4136044979095459 = 0.34279799461364746 + 0.01 * 7.080652236938477
Epoch 260, val loss: 0.7251076698303223
Epoch 270, training loss: 0.3780430555343628 = 0.30729377269744873 + 0.01 * 7.074926853179932
Epoch 270, val loss: 0.7207425236701965
Epoch 280, training loss: 0.3457244038581848 = 0.2750055491924286 + 0.01 * 7.071884632110596
Epoch 280, val loss: 0.7200515866279602
Epoch 290, training loss: 0.3163275122642517 = 0.24564820528030396 + 0.01 * 7.067929267883301
Epoch 290, val loss: 0.7222899198532104
Epoch 300, training loss: 0.28965574502944946 = 0.21903635561466217 + 0.01 * 7.0619378089904785
Epoch 300, val loss: 0.7271570563316345
Epoch 310, training loss: 0.26571983098983765 = 0.19501963257789612 + 0.01 * 7.070021629333496
Epoch 310, val loss: 0.7342777848243713
Epoch 320, training loss: 0.24404244124889374 = 0.17349641025066376 + 0.01 * 7.054603576660156
Epoch 320, val loss: 0.7431379556655884
Epoch 330, training loss: 0.22483664751052856 = 0.15431416034698486 + 0.01 * 7.052248001098633
Epoch 330, val loss: 0.753607451915741
Epoch 340, training loss: 0.2077655792236328 = 0.13730163872241974 + 0.01 * 7.046395301818848
Epoch 340, val loss: 0.765440821647644
Epoch 350, training loss: 0.19278009235858917 = 0.12226715683937073 + 0.01 * 7.051293849945068
Epoch 350, val loss: 0.7783803343772888
Epoch 360, training loss: 0.1794372797012329 = 0.10903105139732361 + 0.01 * 7.040623188018799
Epoch 360, val loss: 0.792141318321228
Epoch 370, training loss: 0.16773690283298492 = 0.09739381074905396 + 0.01 * 7.034309387207031
Epoch 370, val loss: 0.8065427541732788
Epoch 380, training loss: 0.15745845437049866 = 0.08716056495904922 + 0.01 * 7.029788494110107
Epoch 380, val loss: 0.8213817477226257
Epoch 390, training loss: 0.14837807416915894 = 0.0781644806265831 + 0.01 * 7.021358966827393
Epoch 390, val loss: 0.8365108966827393
Epoch 400, training loss: 0.1404392421245575 = 0.07025349885225296 + 0.01 * 7.018574237823486
Epoch 400, val loss: 0.8517938852310181
Epoch 410, training loss: 0.13338546454906464 = 0.06328871846199036 + 0.01 * 7.009674549102783
Epoch 410, val loss: 0.8671340346336365
Epoch 420, training loss: 0.12718820571899414 = 0.05714257434010506 + 0.01 * 7.004563808441162
Epoch 420, val loss: 0.8824793100357056
Epoch 430, training loss: 0.12186950445175171 = 0.05170583724975586 + 0.01 * 7.016366958618164
Epoch 430, val loss: 0.8977192044258118
Epoch 440, training loss: 0.11685097962617874 = 0.04689965397119522 + 0.01 * 6.995132923126221
Epoch 440, val loss: 0.9127564430236816
Epoch 450, training loss: 0.11256459355354309 = 0.042638976126909256 + 0.01 * 6.9925618171691895
Epoch 450, val loss: 0.9276295304298401
Epoch 460, training loss: 0.10869432985782623 = 0.03885013237595558 + 0.01 * 6.984419345855713
Epoch 460, val loss: 0.9422758221626282
Epoch 470, training loss: 0.10528597235679626 = 0.03547624871134758 + 0.01 * 6.980971813201904
Epoch 470, val loss: 0.956641435623169
Epoch 480, training loss: 0.10213889926671982 = 0.032470665872097015 + 0.01 * 6.966823577880859
Epoch 480, val loss: 0.9706891179084778
Epoch 490, training loss: 0.09960590302944183 = 0.02978857234120369 + 0.01 * 6.981733798980713
Epoch 490, val loss: 0.9844470024108887
Epoch 500, training loss: 0.09692173451185226 = 0.027394352480769157 + 0.01 * 6.952738285064697
Epoch 500, val loss: 0.9978110790252686
Epoch 510, training loss: 0.09478132426738739 = 0.025247346609830856 + 0.01 * 6.953397274017334
Epoch 510, val loss: 1.010941982269287
Epoch 520, training loss: 0.09288289397954941 = 0.023317715153098106 + 0.01 * 6.956517696380615
Epoch 520, val loss: 1.0238007307052612
Epoch 530, training loss: 0.09102479368448257 = 0.02158290706574917 + 0.01 * 6.944188594818115
Epoch 530, val loss: 1.0362573862075806
Epoch 540, training loss: 0.08933518826961517 = 0.020024066790938377 + 0.01 * 6.931111812591553
Epoch 540, val loss: 1.048427939414978
Epoch 550, training loss: 0.08794715255498886 = 0.018619274720549583 + 0.01 * 6.9327874183654785
Epoch 550, val loss: 1.0602641105651855
Epoch 560, training loss: 0.08654124289751053 = 0.01735015958547592 + 0.01 * 6.9191083908081055
Epoch 560, val loss: 1.0717594623565674
Epoch 570, training loss: 0.08543451130390167 = 0.016202325001358986 + 0.01 * 6.923218727111816
Epoch 570, val loss: 1.0829825401306152
Epoch 580, training loss: 0.0842556282877922 = 0.015163546428084373 + 0.01 * 6.909207820892334
Epoch 580, val loss: 1.093825101852417
Epoch 590, training loss: 0.08343090116977692 = 0.01421972457319498 + 0.01 * 6.921117782592773
Epoch 590, val loss: 1.1043977737426758
Epoch 600, training loss: 0.08235595375299454 = 0.0133613096550107 + 0.01 * 6.8994646072387695
Epoch 600, val loss: 1.1146929264068604
Epoch 610, training loss: 0.08188772201538086 = 0.01257831510156393 + 0.01 * 6.930941104888916
Epoch 610, val loss: 1.1247526407241821
Epoch 620, training loss: 0.08080554753541946 = 0.011863933876156807 + 0.01 * 6.894161701202393
Epoch 620, val loss: 1.1343722343444824
Epoch 630, training loss: 0.08012594282627106 = 0.011210761032998562 + 0.01 * 6.891518592834473
Epoch 630, val loss: 1.1438177824020386
Epoch 640, training loss: 0.079460009932518 = 0.01061142049729824 + 0.01 * 6.884859085083008
Epoch 640, val loss: 1.1529422998428345
Epoch 650, training loss: 0.07884971797466278 = 0.010060288943350315 + 0.01 * 6.87894344329834
Epoch 650, val loss: 1.1619091033935547
Epoch 660, training loss: 0.07828378677368164 = 0.009553398936986923 + 0.01 * 6.873038291931152
Epoch 660, val loss: 1.170505404472351
Epoch 670, training loss: 0.07782860100269318 = 0.009085959754884243 + 0.01 * 6.874264240264893
Epoch 670, val loss: 1.1789262294769287
Epoch 680, training loss: 0.07734642922878265 = 0.008654131554067135 + 0.01 * 6.869229793548584
Epoch 680, val loss: 1.1870301961898804
Epoch 690, training loss: 0.07709349691867828 = 0.008254192769527435 + 0.01 * 6.883930206298828
Epoch 690, val loss: 1.1950149536132812
Epoch 700, training loss: 0.07662034779787064 = 0.007883308455348015 + 0.01 * 6.873703956604004
Epoch 700, val loss: 1.2026381492614746
Epoch 710, training loss: 0.07606203109025955 = 0.007538663223385811 + 0.01 * 6.852336883544922
Epoch 710, val loss: 1.2102150917053223
Epoch 720, training loss: 0.07581815868616104 = 0.0072174291126430035 + 0.01 * 6.860073089599609
Epoch 720, val loss: 1.2175863981246948
Epoch 730, training loss: 0.07555510103702545 = 0.006917682010680437 + 0.01 * 6.863741397857666
Epoch 730, val loss: 1.2246501445770264
Epoch 740, training loss: 0.0750838965177536 = 0.006637688726186752 + 0.01 * 6.844620227813721
Epoch 740, val loss: 1.2316653728485107
Epoch 750, training loss: 0.0750434398651123 = 0.0063755507580935955 + 0.01 * 6.8667893409729
Epoch 750, val loss: 1.238502025604248
Epoch 760, training loss: 0.0744524821639061 = 0.006130668334662914 + 0.01 * 6.832181930541992
Epoch 760, val loss: 1.244986653327942
Epoch 770, training loss: 0.07432571798563004 = 0.005901088006794453 + 0.01 * 6.842462539672852
Epoch 770, val loss: 1.2514303922653198
Epoch 780, training loss: 0.07406903058290482 = 0.005685354582965374 + 0.01 * 6.838367938995361
Epoch 780, val loss: 1.2576806545257568
Epoch 790, training loss: 0.07394775003194809 = 0.005482300650328398 + 0.01 * 6.8465447425842285
Epoch 790, val loss: 1.2638124227523804
Epoch 800, training loss: 0.07361418008804321 = 0.005291161127388477 + 0.01 * 6.832302093505859
Epoch 800, val loss: 1.269731044769287
Epoch 810, training loss: 0.07353542745113373 = 0.005110797937959433 + 0.01 * 6.84246301651001
Epoch 810, val loss: 1.2755314111709595
Epoch 820, training loss: 0.07309472560882568 = 0.004940772894769907 + 0.01 * 6.815395832061768
Epoch 820, val loss: 1.2812068462371826
Epoch 830, training loss: 0.07296636700630188 = 0.00477991346269846 + 0.01 * 6.818645477294922
Epoch 830, val loss: 1.286813497543335
Epoch 840, training loss: 0.07298272103071213 = 0.004627577494829893 + 0.01 * 6.835514545440674
Epoch 840, val loss: 1.29213547706604
Epoch 850, training loss: 0.07252517342567444 = 0.004483459983021021 + 0.01 * 6.804171085357666
Epoch 850, val loss: 1.2975131273269653
Epoch 860, training loss: 0.07252463698387146 = 0.004346679896116257 + 0.01 * 6.817795276641846
Epoch 860, val loss: 1.3027089834213257
Epoch 870, training loss: 0.07232287526130676 = 0.0042174081318080425 + 0.01 * 6.810546875
Epoch 870, val loss: 1.307669997215271
Epoch 880, training loss: 0.07205723226070404 = 0.004094899632036686 + 0.01 * 6.796233177185059
Epoch 880, val loss: 1.3125934600830078
Epoch 890, training loss: 0.0721033364534378 = 0.003978395834565163 + 0.01 * 6.812494277954102
Epoch 890, val loss: 1.3174296617507935
Epoch 900, training loss: 0.07185444235801697 = 0.003867641557008028 + 0.01 * 6.798680305480957
Epoch 900, val loss: 1.3221244812011719
Epoch 910, training loss: 0.07176444679498672 = 0.003762207692489028 + 0.01 * 6.800224304199219
Epoch 910, val loss: 1.3267600536346436
Epoch 920, training loss: 0.07169132679700851 = 0.00366177293471992 + 0.01 * 6.802955150604248
Epoch 920, val loss: 1.3311551809310913
Epoch 930, training loss: 0.07161080837249756 = 0.003566298633813858 + 0.01 * 6.804450988769531
Epoch 930, val loss: 1.3355575799942017
Epoch 940, training loss: 0.07122465968132019 = 0.0034751847852021456 + 0.01 * 6.774947643280029
Epoch 940, val loss: 1.3398648500442505
Epoch 950, training loss: 0.07135623693466187 = 0.0033882714342325926 + 0.01 * 6.796796798706055
Epoch 950, val loss: 1.3440552949905396
Epoch 960, training loss: 0.07127015292644501 = 0.0033052905928343534 + 0.01 * 6.796485900878906
Epoch 960, val loss: 1.3480610847473145
Epoch 970, training loss: 0.07113136351108551 = 0.0032259714789688587 + 0.01 * 6.790539264678955
Epoch 970, val loss: 1.3521299362182617
Epoch 980, training loss: 0.07081178575754166 = 0.0031502048950642347 + 0.01 * 6.766158580780029
Epoch 980, val loss: 1.3558977842330933
Epoch 990, training loss: 0.07085425406694412 = 0.003077630652114749 + 0.01 * 6.77766227722168
Epoch 990, val loss: 1.3598512411117554
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.0548839569091797 = 1.968915343284607 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9753851890563965
Epoch 10, training loss: 2.0432255268096924 = 1.9572573900222778 + 0.01 * 8.596802711486816
Epoch 10, val loss: 1.9638192653656006
Epoch 20, training loss: 2.0287926197052 = 1.9428266286849976 + 0.01 * 8.59659194946289
Epoch 20, val loss: 1.9487580060958862
Epoch 30, training loss: 2.008378028869629 = 1.9224196672439575 + 0.01 * 8.595841407775879
Epoch 30, val loss: 1.9267698526382446
Epoch 40, training loss: 1.978163719177246 = 1.8922502994537354 + 0.01 * 8.59134292602539
Epoch 40, val loss: 1.894178867340088
Epoch 50, training loss: 1.9355868101119995 = 1.8499687910079956 + 0.01 * 8.561802864074707
Epoch 50, val loss: 1.850258708000183
Epoch 60, training loss: 1.8878424167633057 = 1.8037457466125488 + 0.01 * 8.409666061401367
Epoch 60, val loss: 1.8071495294570923
Epoch 70, training loss: 1.852325439453125 = 1.769380807876587 + 0.01 * 8.294464111328125
Epoch 70, val loss: 1.7787901163101196
Epoch 80, training loss: 1.8109617233276367 = 1.7297263145446777 + 0.01 * 8.123536109924316
Epoch 80, val loss: 1.742787480354309
Epoch 90, training loss: 1.7533196210861206 = 1.6752785444259644 + 0.01 * 7.804102897644043
Epoch 90, val loss: 1.693753957748413
Epoch 100, training loss: 1.6778463125228882 = 1.6026418209075928 + 0.01 * 7.520447731018066
Epoch 100, val loss: 1.6296107769012451
Epoch 110, training loss: 1.588847041130066 = 1.5147390365600586 + 0.01 * 7.410798072814941
Epoch 110, val loss: 1.5537664890289307
Epoch 120, training loss: 1.4958559274673462 = 1.4222439527511597 + 0.01 * 7.361198902130127
Epoch 120, val loss: 1.4747377634048462
Epoch 130, training loss: 1.403422236442566 = 1.3300575017929077 + 0.01 * 7.336472511291504
Epoch 130, val loss: 1.3978945016860962
Epoch 140, training loss: 1.308759331703186 = 1.2355068922042847 + 0.01 * 7.325247287750244
Epoch 140, val loss: 1.3204104900360107
Epoch 150, training loss: 1.2094577550888062 = 1.1362218856811523 + 0.01 * 7.32358980178833
Epoch 150, val loss: 1.2396492958068848
Epoch 160, training loss: 1.1060084104537964 = 1.0327575206756592 + 0.01 * 7.325092792510986
Epoch 160, val loss: 1.1551072597503662
Epoch 170, training loss: 1.0029805898666382 = 0.9297163486480713 + 0.01 * 7.326420783996582
Epoch 170, val loss: 1.0718234777450562
Epoch 180, training loss: 0.9062806367874146 = 0.8330173492431641 + 0.01 * 7.3263258934021
Epoch 180, val loss: 0.995345950126648
Epoch 190, training loss: 0.8198011517524719 = 0.7465702891349792 + 0.01 * 7.323086261749268
Epoch 190, val loss: 0.9304401278495789
Epoch 200, training loss: 0.744505763053894 = 0.67134028673172 + 0.01 * 7.316544532775879
Epoch 200, val loss: 0.8786015510559082
Epoch 210, training loss: 0.6793323755264282 = 0.6062767505645752 + 0.01 * 7.305561542510986
Epoch 210, val loss: 0.8390970826148987
Epoch 220, training loss: 0.62267005443573 = 0.5497809648513794 + 0.01 * 7.288908004760742
Epoch 220, val loss: 0.8095968961715698
Epoch 230, training loss: 0.5728219151496887 = 0.5001183748245239 + 0.01 * 7.270356178283691
Epoch 230, val loss: 0.7874206900596619
Epoch 240, training loss: 0.5279700756072998 = 0.45547524094581604 + 0.01 * 7.249484062194824
Epoch 240, val loss: 0.7704821228981018
Epoch 250, training loss: 0.48647430539131165 = 0.414183646440506 + 0.01 * 7.229066371917725
Epoch 250, val loss: 0.7574353814125061
Epoch 260, training loss: 0.44714513421058655 = 0.37506774067878723 + 0.01 * 7.207738876342773
Epoch 260, val loss: 0.7474089860916138
Epoch 270, training loss: 0.40976303815841675 = 0.33759966492652893 + 0.01 * 7.216336727142334
Epoch 270, val loss: 0.7396501302719116
Epoch 280, training loss: 0.37363120913505554 = 0.30173438787460327 + 0.01 * 7.189682483673096
Epoch 280, val loss: 0.7339152097702026
Epoch 290, training loss: 0.33954858779907227 = 0.26774653792381287 + 0.01 * 7.180206775665283
Epoch 290, val loss: 0.7300072908401489
Epoch 300, training loss: 0.3078470826148987 = 0.23610486090183258 + 0.01 * 7.174223899841309
Epoch 300, val loss: 0.7281403541564941
Epoch 310, training loss: 0.27895307540893555 = 0.20724564790725708 + 0.01 * 7.170742511749268
Epoch 310, val loss: 0.728444516658783
Epoch 320, training loss: 0.2530718445777893 = 0.18145902454853058 + 0.01 * 7.161282539367676
Epoch 320, val loss: 0.7311320304870605
Epoch 330, training loss: 0.23033086955547333 = 0.1587851196527481 + 0.01 * 7.154575347900391
Epoch 330, val loss: 0.7359545230865479
Epoch 340, training loss: 0.2105090618133545 = 0.1390492171049118 + 0.01 * 7.145983695983887
Epoch 340, val loss: 0.7428773641586304
Epoch 350, training loss: 0.1933785080909729 = 0.12197259813547134 + 0.01 * 7.140590190887451
Epoch 350, val loss: 0.7515242695808411
Epoch 360, training loss: 0.17853125929832458 = 0.10723163932561874 + 0.01 * 7.129961967468262
Epoch 360, val loss: 0.7615075707435608
Epoch 370, training loss: 0.16574245691299438 = 0.09449748694896698 + 0.01 * 7.1244964599609375
Epoch 370, val loss: 0.7725032567977905
Epoch 380, training loss: 0.15468735992908478 = 0.08349310606718063 + 0.01 * 7.119425296783447
Epoch 380, val loss: 0.784109354019165
Epoch 390, training loss: 0.14514058828353882 = 0.07397276163101196 + 0.01 * 7.116783618927002
Epoch 390, val loss: 0.7960565686225891
Epoch 400, training loss: 0.13674025237560272 = 0.0657283216714859 + 0.01 * 7.101192951202393
Epoch 400, val loss: 0.8081923723220825
Epoch 410, training loss: 0.12962521612644196 = 0.05858198553323746 + 0.01 * 7.104323387145996
Epoch 410, val loss: 0.8203860521316528
Epoch 420, training loss: 0.1232592761516571 = 0.05238867178559303 + 0.01 * 7.087060928344727
Epoch 420, val loss: 0.8325040936470032
Epoch 430, training loss: 0.1176704466342926 = 0.047013405710458755 + 0.01 * 7.065704345703125
Epoch 430, val loss: 0.8444405794143677
Epoch 440, training loss: 0.11314404010772705 = 0.0423416830599308 + 0.01 * 7.080235958099365
Epoch 440, val loss: 0.8561682105064392
Epoch 450, training loss: 0.10888437926769257 = 0.038277093321084976 + 0.01 * 7.060728549957275
Epoch 450, val loss: 0.8676233887672424
Epoch 460, training loss: 0.10506701469421387 = 0.034730542451143265 + 0.01 * 7.0336480140686035
Epoch 460, val loss: 0.8787854909896851
Epoch 470, training loss: 0.10195140540599823 = 0.031628236174583435 + 0.01 * 7.032317161560059
Epoch 470, val loss: 0.8897010684013367
Epoch 480, training loss: 0.09912285953760147 = 0.028908230364322662 + 0.01 * 7.021463394165039
Epoch 480, val loss: 0.9002957344055176
Epoch 490, training loss: 0.09658849239349365 = 0.02651503123342991 + 0.01 * 7.0073466300964355
Epoch 490, val loss: 0.910563588142395
Epoch 500, training loss: 0.094307541847229 = 0.02440314181149006 + 0.01 * 6.990440368652344
Epoch 500, val loss: 0.9205142259597778
Epoch 510, training loss: 0.09251571446657181 = 0.022532416507601738 + 0.01 * 6.9983296394348145
Epoch 510, val loss: 0.9301679730415344
Epoch 520, training loss: 0.0907476544380188 = 0.020869622007012367 + 0.01 * 6.9878034591674805
Epoch 520, val loss: 0.9394646883010864
Epoch 530, training loss: 0.08916113525629044 = 0.019387247040867805 + 0.01 * 6.977388858795166
Epoch 530, val loss: 0.94855797290802
Epoch 540, training loss: 0.08770468831062317 = 0.018060022965073586 + 0.01 * 6.9644670486450195
Epoch 540, val loss: 0.9572872519493103
Epoch 550, training loss: 0.08637965470552444 = 0.016868026927113533 + 0.01 * 6.951163291931152
Epoch 550, val loss: 0.9657778143882751
Epoch 560, training loss: 0.08521082252264023 = 0.015793437138199806 + 0.01 * 6.941738605499268
Epoch 560, val loss: 0.9740334749221802
Epoch 570, training loss: 0.08416177332401276 = 0.014820914715528488 + 0.01 * 6.934086322784424
Epoch 570, val loss: 0.9820252656936646
Epoch 580, training loss: 0.08363399654626846 = 0.013939647935330868 + 0.01 * 6.969435214996338
Epoch 580, val loss: 0.9897999167442322
Epoch 590, training loss: 0.08241195976734161 = 0.01313902996480465 + 0.01 * 6.927292823791504
Epoch 590, val loss: 0.9973548054695129
Epoch 600, training loss: 0.08174572885036469 = 0.012408326379954815 + 0.01 * 6.933740615844727
Epoch 600, val loss: 1.0046812295913696
Epoch 610, training loss: 0.08094514161348343 = 0.011740582063794136 + 0.01 * 6.920456409454346
Epoch 610, val loss: 1.0117323398590088
Epoch 620, training loss: 0.08048570156097412 = 0.011128490790724754 + 0.01 * 6.935721397399902
Epoch 620, val loss: 1.018628478050232
Epoch 630, training loss: 0.07954609394073486 = 0.010566139593720436 + 0.01 * 6.897995948791504
Epoch 630, val loss: 1.025323510169983
Epoch 640, training loss: 0.07916709780693054 = 0.010048463009297848 + 0.01 * 6.911863803863525
Epoch 640, val loss: 1.0317902565002441
Epoch 650, training loss: 0.07860079407691956 = 0.00957094132900238 + 0.01 * 6.902985572814941
Epoch 650, val loss: 1.0381731986999512
Epoch 660, training loss: 0.07812920212745667 = 0.009129009209573269 + 0.01 * 6.90001916885376
Epoch 660, val loss: 1.044323444366455
Epoch 670, training loss: 0.07752165198326111 = 0.0087197907269001 + 0.01 * 6.880185604095459
Epoch 670, val loss: 1.0502753257751465
Epoch 680, training loss: 0.07718399167060852 = 0.008339617401361465 + 0.01 * 6.884437084197998
Epoch 680, val loss: 1.0560829639434814
Epoch 690, training loss: 0.0767502710223198 = 0.007986434735357761 + 0.01 * 6.8763837814331055
Epoch 690, val loss: 1.061776041984558
Epoch 700, training loss: 0.07631731033325195 = 0.007657359354197979 + 0.01 * 6.86599588394165
Epoch 700, val loss: 1.0672487020492554
Epoch 710, training loss: 0.0761590301990509 = 0.007350261323153973 + 0.01 * 6.880877494812012
Epoch 710, val loss: 1.072610855102539
Epoch 720, training loss: 0.0755925253033638 = 0.007063244469463825 + 0.01 * 6.8529276847839355
Epoch 720, val loss: 1.0778400897979736
Epoch 730, training loss: 0.0757053792476654 = 0.006794619373977184 + 0.01 * 6.891076564788818
Epoch 730, val loss: 1.0829654932022095
Epoch 740, training loss: 0.074958436191082 = 0.006542717572301626 + 0.01 * 6.841571807861328
Epoch 740, val loss: 1.0878757238388062
Epoch 750, training loss: 0.07488127052783966 = 0.006306294351816177 + 0.01 * 6.8574981689453125
Epoch 750, val loss: 1.0927090644836426
Epoch 760, training loss: 0.0744851753115654 = 0.00608410919085145 + 0.01 * 6.840106964111328
Epoch 760, val loss: 1.0974620580673218
Epoch 770, training loss: 0.07426901906728745 = 0.0058748251758515835 + 0.01 * 6.839419364929199
Epoch 770, val loss: 1.1020129919052124
Epoch 780, training loss: 0.07398854941129684 = 0.0056778197176754475 + 0.01 * 6.83107328414917
Epoch 780, val loss: 1.106507420539856
Epoch 790, training loss: 0.07387705892324448 = 0.0054915789514780045 + 0.01 * 6.838548183441162
Epoch 790, val loss: 1.1108975410461426
Epoch 800, training loss: 0.07360392808914185 = 0.00531570240855217 + 0.01 * 6.828822135925293
Epoch 800, val loss: 1.1152064800262451
Epoch 810, training loss: 0.07342764735221863 = 0.005149316042661667 + 0.01 * 6.827833652496338
Epoch 810, val loss: 1.119341492652893
Epoch 820, training loss: 0.07345817983150482 = 0.004991966765373945 + 0.01 * 6.846621990203857
Epoch 820, val loss: 1.123381495475769
Epoch 830, training loss: 0.07293060421943665 = 0.004842781461775303 + 0.01 * 6.808782577514648
Epoch 830, val loss: 1.1274054050445557
Epoch 840, training loss: 0.07276787608861923 = 0.004701222293078899 + 0.01 * 6.806665897369385
Epoch 840, val loss: 1.1313040256500244
Epoch 850, training loss: 0.07263796776533127 = 0.004566799383610487 + 0.01 * 6.807117462158203
Epoch 850, val loss: 1.1351286172866821
Epoch 860, training loss: 0.07243680208921432 = 0.004438972100615501 + 0.01 * 6.799782752990723
Epoch 860, val loss: 1.1388224363327026
Epoch 870, training loss: 0.07245239615440369 = 0.004317416809499264 + 0.01 * 6.813498020172119
Epoch 870, val loss: 1.1424616575241089
Epoch 880, training loss: 0.07247738540172577 = 0.00420163432136178 + 0.01 * 6.827575206756592
Epoch 880, val loss: 1.1460062265396118
Epoch 890, training loss: 0.07205464690923691 = 0.004091338254511356 + 0.01 * 6.79633092880249
Epoch 890, val loss: 1.149501919746399
Epoch 900, training loss: 0.07185887545347214 = 0.003986233379691839 + 0.01 * 6.787264823913574
Epoch 900, val loss: 1.152862310409546
Epoch 910, training loss: 0.07168280333280563 = 0.003885880811139941 + 0.01 * 6.77969217300415
Epoch 910, val loss: 1.156175971031189
Epoch 920, training loss: 0.07179727405309677 = 0.003789912210777402 + 0.01 * 6.800736904144287
Epoch 920, val loss: 1.1593961715698242
Epoch 930, training loss: 0.07161961495876312 = 0.0036982891615480185 + 0.01 * 6.79213285446167
Epoch 930, val loss: 1.1625840663909912
Epoch 940, training loss: 0.07139655947685242 = 0.003610560903325677 + 0.01 * 6.778600215911865
Epoch 940, val loss: 1.16567063331604
Epoch 950, training loss: 0.071696937084198 = 0.0035267292987555265 + 0.01 * 6.817020416259766
Epoch 950, val loss: 1.1687688827514648
Epoch 960, training loss: 0.0712568461894989 = 0.0034462702460587025 + 0.01 * 6.781057357788086
Epoch 960, val loss: 1.171689510345459
Epoch 970, training loss: 0.07138405740261078 = 0.0033692014403641224 + 0.01 * 6.801485061645508
Epoch 970, val loss: 1.1746158599853516
Epoch 980, training loss: 0.07094941288232803 = 0.0032952437177300453 + 0.01 * 6.765417098999023
Epoch 980, val loss: 1.177510142326355
Epoch 990, training loss: 0.07085026800632477 = 0.0032242550514638424 + 0.01 * 6.762601375579834
Epoch 990, val loss: 1.1802908182144165
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8439641539272537
The final CL Acc:0.79630, 0.01983, The final GNN Acc:0.84027, 0.00282
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10624])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.027200698852539 = 1.9412320852279663 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9447823762893677
Epoch 10, training loss: 2.017723560333252 = 1.9317556619644165 + 0.01 * 8.5968017578125
Epoch 10, val loss: 1.9349440336227417
Epoch 20, training loss: 2.0059332847595215 = 1.9199671745300293 + 0.01 * 8.5966215133667
Epoch 20, val loss: 1.922532558441162
Epoch 30, training loss: 1.9895914793014526 = 1.9036307334899902 + 0.01 * 8.59607982635498
Epoch 30, val loss: 1.905288577079773
Epoch 40, training loss: 1.9659780263900757 = 1.88004732131958 + 0.01 * 8.593070030212402
Epoch 40, val loss: 1.8804904222488403
Epoch 50, training loss: 1.9333317279815674 = 1.8476258516311646 + 0.01 * 8.570582389831543
Epoch 50, val loss: 1.8473012447357178
Epoch 60, training loss: 1.8963714838027954 = 1.8118865489959717 + 0.01 * 8.448495864868164
Epoch 60, val loss: 1.813645362854004
Epoch 70, training loss: 1.8643635511398315 = 1.7826831340789795 + 0.01 * 8.168041229248047
Epoch 70, val loss: 1.7889589071273804
Epoch 80, training loss: 1.8283543586730957 = 1.7482534646987915 + 0.01 * 8.010095596313477
Epoch 80, val loss: 1.7587634325027466
Epoch 90, training loss: 1.7774850130081177 = 1.6996124982833862 + 0.01 * 7.787247180938721
Epoch 90, val loss: 1.7164682149887085
Epoch 100, training loss: 1.7076104879379272 = 1.6319586038589478 + 0.01 * 7.565186977386475
Epoch 100, val loss: 1.6593623161315918
Epoch 110, training loss: 1.6186895370483398 = 1.544830322265625 + 0.01 * 7.38592529296875
Epoch 110, val loss: 1.5862696170806885
Epoch 120, training loss: 1.5181710720062256 = 1.4450507164001465 + 0.01 * 7.31203031539917
Epoch 120, val loss: 1.5019456148147583
Epoch 130, training loss: 1.4128668308258057 = 1.340291976928711 + 0.01 * 7.257482528686523
Epoch 130, val loss: 1.415817141532898
Epoch 140, training loss: 1.308297038078308 = 1.2359586954116821 + 0.01 * 7.233835220336914
Epoch 140, val loss: 1.333062767982483
Epoch 150, training loss: 1.2077308893203735 = 1.1355096101760864 + 0.01 * 7.222126483917236
Epoch 150, val loss: 1.2577978372573853
Epoch 160, training loss: 1.1126307249069214 = 1.0405153036117554 + 0.01 * 7.211547374725342
Epoch 160, val loss: 1.18940269947052
Epoch 170, training loss: 1.0218499898910522 = 0.9498642086982727 + 0.01 * 7.198578834533691
Epoch 170, val loss: 1.125424861907959
Epoch 180, training loss: 0.9342173933982849 = 0.862408459186554 + 0.01 * 7.1808953285217285
Epoch 180, val loss: 1.0636897087097168
Epoch 190, training loss: 0.850525438785553 = 0.7788994312286377 + 0.01 * 7.162601947784424
Epoch 190, val loss: 1.0050380229949951
Epoch 200, training loss: 0.7729899883270264 = 0.7015451788902283 + 0.01 * 7.144477844238281
Epoch 200, val loss: 0.9522607922554016
Epoch 210, training loss: 0.7032612562179565 = 0.6319891214370728 + 0.01 * 7.127211093902588
Epoch 210, val loss: 0.9079002141952515
Epoch 220, training loss: 0.6414251923561096 = 0.5702488422393799 + 0.01 * 7.117637634277344
Epoch 220, val loss: 0.8731919527053833
Epoch 230, training loss: 0.5863678455352783 = 0.5153118968009949 + 0.01 * 7.105594158172607
Epoch 230, val loss: 0.8473994731903076
Epoch 240, training loss: 0.5367187857627869 = 0.46572238206863403 + 0.01 * 7.099639415740967
Epoch 240, val loss: 0.8289387822151184
Epoch 250, training loss: 0.49097931385040283 = 0.4200443923473358 + 0.01 * 7.093491554260254
Epoch 250, val loss: 0.8164088129997253
Epoch 260, training loss: 0.44808444380760193 = 0.37720030546188354 + 0.01 * 7.088414669036865
Epoch 260, val loss: 0.8083925247192383
Epoch 270, training loss: 0.4074282944202423 = 0.3366060256958008 + 0.01 * 7.082226753234863
Epoch 270, val loss: 0.8041114807128906
Epoch 280, training loss: 0.3689422011375427 = 0.2981175482273102 + 0.01 * 7.082465171813965
Epoch 280, val loss: 0.8031448721885681
Epoch 290, training loss: 0.3327174186706543 = 0.2619820833206177 + 0.01 * 7.0735321044921875
Epoch 290, val loss: 0.8052176833152771
Epoch 300, training loss: 0.29936206340789795 = 0.22867225110530853 + 0.01 * 7.0689826011657715
Epoch 300, val loss: 0.8100699782371521
Epoch 310, training loss: 0.26936158537864685 = 0.19863997399806976 + 0.01 * 7.072160720825195
Epoch 310, val loss: 0.8174413442611694
Epoch 320, training loss: 0.24275043606758118 = 0.17211732268333435 + 0.01 * 7.06331205368042
Epoch 320, val loss: 0.8269564509391785
Epoch 330, training loss: 0.21960189938545227 = 0.14901165664196014 + 0.01 * 7.059024810791016
Epoch 330, val loss: 0.8379908204078674
Epoch 340, training loss: 0.1995970755815506 = 0.12903735041618347 + 0.01 * 7.055972576141357
Epoch 340, val loss: 0.8501089215278625
Epoch 350, training loss: 0.1823730617761612 = 0.11183689534664154 + 0.01 * 7.053617000579834
Epoch 350, val loss: 0.8629974126815796
Epoch 360, training loss: 0.16756895184516907 = 0.09706491976976395 + 0.01 * 7.050402641296387
Epoch 360, val loss: 0.876331627368927
Epoch 370, training loss: 0.15488161146640778 = 0.08441049605607986 + 0.01 * 7.047111511230469
Epoch 370, val loss: 0.8901462554931641
Epoch 380, training loss: 0.14406177401542664 = 0.07360643893480301 + 0.01 * 7.045533180236816
Epoch 380, val loss: 0.9042810797691345
Epoch 390, training loss: 0.13481590151786804 = 0.06440774351358414 + 0.01 * 7.0408148765563965
Epoch 390, val loss: 0.9187228083610535
Epoch 400, training loss: 0.12697821855545044 = 0.05659090727567673 + 0.01 * 7.038732051849365
Epoch 400, val loss: 0.9333081245422363
Epoch 410, training loss: 0.1203068271279335 = 0.04995449632406235 + 0.01 * 7.035233497619629
Epoch 410, val loss: 0.9479235410690308
Epoch 420, training loss: 0.11465856432914734 = 0.04431666061282158 + 0.01 * 7.034191131591797
Epoch 420, val loss: 0.9624484777450562
Epoch 430, training loss: 0.10982400178909302 = 0.03951917961239815 + 0.01 * 7.030481815338135
Epoch 430, val loss: 0.9767925143241882
Epoch 440, training loss: 0.1057371199131012 = 0.03542419523000717 + 0.01 * 7.03129243850708
Epoch 440, val loss: 0.9908891320228577
Epoch 450, training loss: 0.10217428207397461 = 0.03191535919904709 + 0.01 * 7.02589225769043
Epoch 450, val loss: 1.0047342777252197
Epoch 460, training loss: 0.09912919998168945 = 0.028895413503050804 + 0.01 * 7.023378372192383
Epoch 460, val loss: 1.0181818008422852
Epoch 470, training loss: 0.09652179479598999 = 0.026282167062163353 + 0.01 * 7.023962497711182
Epoch 470, val loss: 1.0312273502349854
Epoch 480, training loss: 0.09420324116945267 = 0.02401021122932434 + 0.01 * 7.019303321838379
Epoch 480, val loss: 1.0438518524169922
Epoch 490, training loss: 0.09217902272939682 = 0.02202516980469227 + 0.01 * 7.015385627746582
Epoch 490, val loss: 1.0560916662216187
Epoch 500, training loss: 0.09039493650197983 = 0.020281195640563965 + 0.01 * 7.011373996734619
Epoch 500, val loss: 1.0679075717926025
Epoch 510, training loss: 0.08892513066530228 = 0.01874186284840107 + 0.01 * 7.018326759338379
Epoch 510, val loss: 1.0793310403823853
Epoch 520, training loss: 0.08747562766075134 = 0.017378265038132668 + 0.01 * 7.00973653793335
Epoch 520, val loss: 1.090419888496399
Epoch 530, training loss: 0.08619973063468933 = 0.01616435870528221 + 0.01 * 7.003537654876709
Epoch 530, val loss: 1.1011391878128052
Epoch 540, training loss: 0.08509372174739838 = 0.01507882121950388 + 0.01 * 7.001490116119385
Epoch 540, val loss: 1.1114915609359741
Epoch 550, training loss: 0.08408278226852417 = 0.014104025438427925 + 0.01 * 6.997875690460205
Epoch 550, val loss: 1.1215401887893677
Epoch 560, training loss: 0.08334401249885559 = 0.013225906528532505 + 0.01 * 7.011810779571533
Epoch 560, val loss: 1.1312382221221924
Epoch 570, training loss: 0.082368865609169 = 0.012432683259248734 + 0.01 * 6.993618965148926
Epoch 570, val loss: 1.1406323909759521
Epoch 580, training loss: 0.08159560710191727 = 0.01171327568590641 + 0.01 * 6.98823356628418
Epoch 580, val loss: 1.1497317552566528
Epoch 590, training loss: 0.0809234231710434 = 0.011058511212468147 + 0.01 * 6.9864912033081055
Epoch 590, val loss: 1.1585602760314941
Epoch 600, training loss: 0.08035528659820557 = 0.010460834950208664 + 0.01 * 6.989445209503174
Epoch 600, val loss: 1.1671265363693237
Epoch 610, training loss: 0.07973405718803406 = 0.009914152324199677 + 0.01 * 6.981990337371826
Epoch 610, val loss: 1.1754605770111084
Epoch 620, training loss: 0.07917878776788712 = 0.009412583895027637 + 0.01 * 6.976620197296143
Epoch 620, val loss: 1.1835228204727173
Epoch 630, training loss: 0.07871496677398682 = 0.008951138705015182 + 0.01 * 6.976383686065674
Epoch 630, val loss: 1.1913697719573975
Epoch 640, training loss: 0.07827364653348923 = 0.008526044897735119 + 0.01 * 6.97476053237915
Epoch 640, val loss: 1.1989918947219849
Epoch 650, training loss: 0.0778326541185379 = 0.00813355389982462 + 0.01 * 6.969910144805908
Epoch 650, val loss: 1.2063664197921753
Epoch 660, training loss: 0.07740814983844757 = 0.007770087104290724 + 0.01 * 6.963807106018066
Epoch 660, val loss: 1.2135785818099976
Epoch 670, training loss: 0.07711717486381531 = 0.007432782556861639 + 0.01 * 6.96843957901001
Epoch 670, val loss: 1.2205588817596436
Epoch 680, training loss: 0.07671572268009186 = 0.007119322195649147 + 0.01 * 6.9596405029296875
Epoch 680, val loss: 1.2273513078689575
Epoch 690, training loss: 0.07638435065746307 = 0.006827403791248798 + 0.01 * 6.955694675445557
Epoch 690, val loss: 1.2340083122253418
Epoch 700, training loss: 0.07615865021944046 = 0.006555007770657539 + 0.01 * 6.960364818572998
Epoch 700, val loss: 1.2404751777648926
Epoch 710, training loss: 0.07579182088375092 = 0.006300530396401882 + 0.01 * 6.949129104614258
Epoch 710, val loss: 1.2467352151870728
Epoch 720, training loss: 0.075553759932518 = 0.006062389817088842 + 0.01 * 6.949137210845947
Epoch 720, val loss: 1.252860188484192
Epoch 730, training loss: 0.07525651156902313 = 0.005839348305016756 + 0.01 * 6.941716194152832
Epoch 730, val loss: 1.258847951889038
Epoch 740, training loss: 0.07502509653568268 = 0.005629976745694876 + 0.01 * 6.939512252807617
Epoch 740, val loss: 1.264636516571045
Epoch 750, training loss: 0.07481151819229126 = 0.005433217156678438 + 0.01 * 6.937829971313477
Epoch 750, val loss: 1.2703280448913574
Epoch 760, training loss: 0.07475487142801285 = 0.005248188506811857 + 0.01 * 6.9506683349609375
Epoch 760, val loss: 1.2758355140686035
Epoch 770, training loss: 0.07436181604862213 = 0.005073871463537216 + 0.01 * 6.928795337677002
Epoch 770, val loss: 1.2812138795852661
Epoch 780, training loss: 0.07414603978395462 = 0.00490951631218195 + 0.01 * 6.923652648925781
Epoch 780, val loss: 1.2864964008331299
Epoch 790, training loss: 0.07408160716295242 = 0.004754193127155304 + 0.01 * 6.932741641998291
Epoch 790, val loss: 1.2915894985198975
Epoch 800, training loss: 0.07383428514003754 = 0.004607575014233589 + 0.01 * 6.922671318054199
Epoch 800, val loss: 1.2966374158859253
Epoch 810, training loss: 0.07378227263689041 = 0.004468850791454315 + 0.01 * 6.931342601776123
Epoch 810, val loss: 1.3014487028121948
Epoch 820, training loss: 0.073470838367939 = 0.0043373676016926765 + 0.01 * 6.913346767425537
Epoch 820, val loss: 1.3062366247177124
Epoch 830, training loss: 0.07333901524543762 = 0.004212696570903063 + 0.01 * 6.912631511688232
Epoch 830, val loss: 1.3108975887298584
Epoch 840, training loss: 0.07311886548995972 = 0.004094380419701338 + 0.01 * 6.902449131011963
Epoch 840, val loss: 1.315436601638794
Epoch 850, training loss: 0.07299763709306717 = 0.0039818924851715565 + 0.01 * 6.901574611663818
Epoch 850, val loss: 1.3198161125183105
Epoch 860, training loss: 0.07289106398820877 = 0.00387517549097538 + 0.01 * 6.901589393615723
Epoch 860, val loss: 1.3241815567016602
Epoch 870, training loss: 0.07269065827131271 = 0.003773710923269391 + 0.01 * 6.891695022583008
Epoch 870, val loss: 1.3284010887145996
Epoch 880, training loss: 0.07267428934574127 = 0.003676864318549633 + 0.01 * 6.899742603302002
Epoch 880, val loss: 1.332502841949463
Epoch 890, training loss: 0.07238232344388962 = 0.0035846875980496407 + 0.01 * 6.879763603210449
Epoch 890, val loss: 1.3365668058395386
Epoch 900, training loss: 0.07267630100250244 = 0.0034968010149896145 + 0.01 * 6.917949676513672
Epoch 900, val loss: 1.3404303789138794
Epoch 910, training loss: 0.07212945073843002 = 0.0034127768594771624 + 0.01 * 6.871667861938477
Epoch 910, val loss: 1.3442919254302979
Epoch 920, training loss: 0.072038933634758 = 0.0033326640259474516 + 0.01 * 6.870626449584961
Epoch 920, val loss: 1.3479928970336914
Epoch 930, training loss: 0.0718507170677185 = 0.003255974967032671 + 0.01 * 6.859474182128906
Epoch 930, val loss: 1.3516842126846313
Epoch 940, training loss: 0.07186125218868256 = 0.003182521788403392 + 0.01 * 6.867872714996338
Epoch 940, val loss: 1.355233907699585
Epoch 950, training loss: 0.07175019383430481 = 0.003112452570348978 + 0.01 * 6.863774299621582
Epoch 950, val loss: 1.3587080240249634
Epoch 960, training loss: 0.07153931260108948 = 0.0030452292412519455 + 0.01 * 6.8494086265563965
Epoch 960, val loss: 1.3621236085891724
Epoch 970, training loss: 0.07168953120708466 = 0.002980786142870784 + 0.01 * 6.870874881744385
Epoch 970, val loss: 1.3654305934906006
Epoch 980, training loss: 0.07128816097974777 = 0.0029191081412136555 + 0.01 * 6.836905479431152
Epoch 980, val loss: 1.3687124252319336
Epoch 990, training loss: 0.0712328851222992 = 0.00285989954136312 + 0.01 * 6.83729887008667
Epoch 990, val loss: 1.3719048500061035
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 2.0337860584259033 = 1.9478175640106201 + 0.01 * 8.596846580505371
Epoch 0, val loss: 1.9542518854141235
Epoch 10, training loss: 2.0246827602386475 = 1.938714861869812 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.9448511600494385
Epoch 20, training loss: 2.0137972831726074 = 1.9278321266174316 + 0.01 * 8.596525192260742
Epoch 20, val loss: 1.9333902597427368
Epoch 30, training loss: 1.9986836910247803 = 1.9127287864685059 + 0.01 * 8.595484733581543
Epoch 30, val loss: 1.9173593521118164
Epoch 40, training loss: 1.9763445854187012 = 1.890463948249817 + 0.01 * 8.588068008422852
Epoch 40, val loss: 1.8938316106796265
Epoch 50, training loss: 1.9439074993133545 = 1.8585004806518555 + 0.01 * 8.540706634521484
Epoch 50, val loss: 1.860689401626587
Epoch 60, training loss: 1.9029213190078735 = 1.819634199142456 + 0.01 * 8.328706741333008
Epoch 60, val loss: 1.8231090307235718
Epoch 70, training loss: 1.8659169673919678 = 1.7846897840499878 + 0.01 * 8.12271785736084
Epoch 70, val loss: 1.7925251722335815
Epoch 80, training loss: 1.8270068168640137 = 1.7484462261199951 + 0.01 * 7.856057167053223
Epoch 80, val loss: 1.760140061378479
Epoch 90, training loss: 1.7751476764678955 = 1.6995081901550293 + 0.01 * 7.563945293426514
Epoch 90, val loss: 1.716729998588562
Epoch 100, training loss: 1.706649899482727 = 1.6326476335525513 + 0.01 * 7.400232315063477
Epoch 100, val loss: 1.6586275100708008
Epoch 110, training loss: 1.6194666624069214 = 1.5460400581359863 + 0.01 * 7.342658042907715
Epoch 110, val loss: 1.5851821899414062
Epoch 120, training loss: 1.5192558765411377 = 1.4462970495224 + 0.01 * 7.295886039733887
Epoch 120, val loss: 1.502831220626831
Epoch 130, training loss: 1.414504051208496 = 1.3419029712677002 + 0.01 * 7.26010799407959
Epoch 130, val loss: 1.4180903434753418
Epoch 140, training loss: 1.3106350898742676 = 1.2382419109344482 + 0.01 * 7.239317417144775
Epoch 140, val loss: 1.3361821174621582
Epoch 150, training loss: 1.2111512422561646 = 1.1388763189315796 + 0.01 * 7.227497100830078
Epoch 150, val loss: 1.2603182792663574
Epoch 160, training loss: 1.1190621852874756 = 1.0469143390655518 + 0.01 * 7.214787483215332
Epoch 160, val loss: 1.1929991245269775
Epoch 170, training loss: 1.0348984003067017 = 0.9629145264625549 + 0.01 * 7.198391914367676
Epoch 170, val loss: 1.13442063331604
Epoch 180, training loss: 0.9567081332206726 = 0.8848998546600342 + 0.01 * 7.18082857131958
Epoch 180, val loss: 1.0822335481643677
Epoch 190, training loss: 0.8819130659103394 = 0.8103246092796326 + 0.01 * 7.15884256362915
Epoch 190, val loss: 1.0329334735870361
Epoch 200, training loss: 0.8093560338020325 = 0.7379812598228455 + 0.01 * 7.137477874755859
Epoch 200, val loss: 0.9853243231773376
Epoch 210, training loss: 0.7392783761024475 = 0.6680818796157837 + 0.01 * 7.11964750289917
Epoch 210, val loss: 0.9397258758544922
Epoch 220, training loss: 0.6723541617393494 = 0.601227343082428 + 0.01 * 7.112682342529297
Epoch 220, val loss: 0.8972280025482178
Epoch 230, training loss: 0.6086405515670776 = 0.5376522541046143 + 0.01 * 7.098828315734863
Epoch 230, val loss: 0.8589178323745728
Epoch 240, training loss: 0.5483449101448059 = 0.47744202613830566 + 0.01 * 7.090290546417236
Epoch 240, val loss: 0.82500159740448
Epoch 250, training loss: 0.4917614459991455 = 0.42092904448509216 + 0.01 * 7.083240032196045
Epoch 250, val loss: 0.7957912683486938
Epoch 260, training loss: 0.4398253560066223 = 0.36884811520576477 + 0.01 * 7.09772253036499
Epoch 260, val loss: 0.7717040777206421
Epoch 270, training loss: 0.39285504817962646 = 0.3220929503440857 + 0.01 * 7.076211452484131
Epoch 270, val loss: 0.753233790397644
Epoch 280, training loss: 0.3517179489135742 = 0.28101831674575806 + 0.01 * 7.069962501525879
Epoch 280, val loss: 0.7403359413146973
Epoch 290, training loss: 0.3160897493362427 = 0.2454306036233902 + 0.01 * 7.065915584564209
Epoch 290, val loss: 0.7327308654785156
Epoch 300, training loss: 0.2853001356124878 = 0.21467578411102295 + 0.01 * 7.062434196472168
Epoch 300, val loss: 0.7295279502868652
Epoch 310, training loss: 0.25860220193862915 = 0.18802078068256378 + 0.01 * 7.058142185211182
Epoch 310, val loss: 0.7301182150840759
Epoch 320, training loss: 0.23543381690979004 = 0.1648288518190384 + 0.01 * 7.060497760772705
Epoch 320, val loss: 0.7338806390762329
Epoch 330, training loss: 0.21521489322185516 = 0.14462482929229736 + 0.01 * 7.059006690979004
Epoch 330, val loss: 0.7402735948562622
Epoch 340, training loss: 0.197488933801651 = 0.12700511515140533 + 0.01 * 7.04838228225708
Epoch 340, val loss: 0.7486695051193237
Epoch 350, training loss: 0.18206392228603363 = 0.11164481192827225 + 0.01 * 7.0419111251831055
Epoch 350, val loss: 0.7588778734207153
Epoch 360, training loss: 0.16864120960235596 = 0.09827351570129395 + 0.01 * 7.036768913269043
Epoch 360, val loss: 0.7703598141670227
Epoch 370, training loss: 0.15716101229190826 = 0.08665573596954346 + 0.01 * 7.050528049468994
Epoch 370, val loss: 0.7828091979026794
Epoch 380, training loss: 0.146885484457016 = 0.07659998536109924 + 0.01 * 7.028549671173096
Epoch 380, val loss: 0.7959042191505432
Epoch 390, training loss: 0.1381167769432068 = 0.0678987130522728 + 0.01 * 7.021805763244629
Epoch 390, val loss: 0.8093476295471191
Epoch 400, training loss: 0.1305263340473175 = 0.060368817299604416 + 0.01 * 7.01575231552124
Epoch 400, val loss: 0.8230763077735901
Epoch 410, training loss: 0.12401001900434494 = 0.05385474115610123 + 0.01 * 7.015528202056885
Epoch 410, val loss: 0.8368949890136719
Epoch 420, training loss: 0.11827914416790009 = 0.048224445432424545 + 0.01 * 7.005470275878906
Epoch 420, val loss: 0.8506225943565369
Epoch 430, training loss: 0.11330939084291458 = 0.04334814101457596 + 0.01 * 6.996125221252441
Epoch 430, val loss: 0.8641369938850403
Epoch 440, training loss: 0.10903847217559814 = 0.03910841792821884 + 0.01 * 6.993005275726318
Epoch 440, val loss: 0.8774644732475281
Epoch 450, training loss: 0.1052844226360321 = 0.035411275923252106 + 0.01 * 6.987314701080322
Epoch 450, val loss: 0.8904762268066406
Epoch 460, training loss: 0.10200098901987076 = 0.03217892348766327 + 0.01 * 6.98220682144165
Epoch 460, val loss: 0.9031435251235962
Epoch 470, training loss: 0.09910736232995987 = 0.029345937073230743 + 0.01 * 6.976142406463623
Epoch 470, val loss: 0.9154584407806396
Epoch 480, training loss: 0.09651283919811249 = 0.0268532857298851 + 0.01 * 6.96595573425293
Epoch 480, val loss: 0.927387535572052
Epoch 490, training loss: 0.09425956010818481 = 0.024655383080244064 + 0.01 * 6.960418224334717
Epoch 490, val loss: 0.9389443397521973
Epoch 500, training loss: 0.09228091686964035 = 0.022710124030709267 + 0.01 * 6.9570794105529785
Epoch 500, val loss: 0.9500835537910461
Epoch 510, training loss: 0.09052718430757523 = 0.020981373265385628 + 0.01 * 6.9545817375183105
Epoch 510, val loss: 0.9608104825019836
Epoch 520, training loss: 0.08887220174074173 = 0.01943979412317276 + 0.01 * 6.943241119384766
Epoch 520, val loss: 0.9712240695953369
Epoch 530, training loss: 0.08744047582149506 = 0.018061552196741104 + 0.01 * 6.937892913818359
Epoch 530, val loss: 0.9812964797019958
Epoch 540, training loss: 0.08612160384654999 = 0.01682492159307003 + 0.01 * 6.929668426513672
Epoch 540, val loss: 0.9909853339195251
Epoch 550, training loss: 0.08532609790563583 = 0.015711655840277672 + 0.01 * 6.96144437789917
Epoch 550, val loss: 1.0003610849380493
Epoch 560, training loss: 0.08402056246995926 = 0.014709665440022945 + 0.01 * 6.931089401245117
Epoch 560, val loss: 1.0094879865646362
Epoch 570, training loss: 0.08294486999511719 = 0.013802947476506233 + 0.01 * 6.9141926765441895
Epoch 570, val loss: 1.0182576179504395
Epoch 580, training loss: 0.08217879384756088 = 0.012979590333998203 + 0.01 * 6.919920444488525
Epoch 580, val loss: 1.0267785787582397
Epoch 590, training loss: 0.0812918171286583 = 0.012230482883751392 + 0.01 * 6.906133651733398
Epoch 590, val loss: 1.0350279808044434
Epoch 600, training loss: 0.08058083057403564 = 0.011546526104211807 + 0.01 * 6.903430461883545
Epoch 600, val loss: 1.0430200099945068
Epoch 610, training loss: 0.079985111951828 = 0.010920696891844273 + 0.01 * 6.906441688537598
Epoch 610, val loss: 1.050729751586914
Epoch 620, training loss: 0.07924894243478775 = 0.010347025468945503 + 0.01 * 6.890192031860352
Epoch 620, val loss: 1.058251976966858
Epoch 630, training loss: 0.07877828925848007 = 0.009820000268518925 + 0.01 * 6.895829200744629
Epoch 630, val loss: 1.065565586090088
Epoch 640, training loss: 0.07823231816291809 = 0.009334703907370567 + 0.01 * 6.889761447906494
Epoch 640, val loss: 1.0725889205932617
Epoch 650, training loss: 0.07769091427326202 = 0.008886913768947124 + 0.01 * 6.880400657653809
Epoch 650, val loss: 1.0794740915298462
Epoch 660, training loss: 0.07735093683004379 = 0.008472549729049206 + 0.01 * 6.887839317321777
Epoch 660, val loss: 1.0860989093780518
Epoch 670, training loss: 0.07689003646373749 = 0.008089672774076462 + 0.01 * 6.8800368309021
Epoch 670, val loss: 1.092622995376587
Epoch 680, training loss: 0.07647622376680374 = 0.0077341278083622456 + 0.01 * 6.874209880828857
Epoch 680, val loss: 1.0988664627075195
Epoch 690, training loss: 0.07605947554111481 = 0.007403410505503416 + 0.01 * 6.865606307983398
Epoch 690, val loss: 1.105024814605713
Epoch 700, training loss: 0.07577188313007355 = 0.0070948610082268715 + 0.01 * 6.867702484130859
Epoch 700, val loss: 1.1110131740570068
Epoch 710, training loss: 0.07544761896133423 = 0.006807205267250538 + 0.01 * 6.864041328430176
Epoch 710, val loss: 1.1167656183242798
Epoch 720, training loss: 0.07506491243839264 = 0.006538368761539459 + 0.01 * 6.852654457092285
Epoch 720, val loss: 1.1224416494369507
Epoch 730, training loss: 0.07474662363529205 = 0.0062861572951078415 + 0.01 * 6.846046447753906
Epoch 730, val loss: 1.1280248165130615
Epoch 740, training loss: 0.07472293823957443 = 0.006049059331417084 + 0.01 * 6.867387771606445
Epoch 740, val loss: 1.133432149887085
Epoch 750, training loss: 0.07427482306957245 = 0.005825791973620653 + 0.01 * 6.844902992248535
Epoch 750, val loss: 1.138786792755127
Epoch 760, training loss: 0.07420021295547485 = 0.005615638568997383 + 0.01 * 6.858457565307617
Epoch 760, val loss: 1.1438695192337036
Epoch 770, training loss: 0.07375898957252502 = 0.005416440311819315 + 0.01 * 6.834255218505859
Epoch 770, val loss: 1.1490812301635742
Epoch 780, training loss: 0.07364238053560257 = 0.005227779969573021 + 0.01 * 6.84145975112915
Epoch 780, val loss: 1.154150366783142
Epoch 790, training loss: 0.07332536578178406 = 0.005048368126153946 + 0.01 * 6.827699184417725
Epoch 790, val loss: 1.1591815948486328
Epoch 800, training loss: 0.07341978698968887 = 0.004877759143710136 + 0.01 * 6.854202747344971
Epoch 800, val loss: 1.1640548706054688
Epoch 810, training loss: 0.07294292002916336 = 0.0047158170491456985 + 0.01 * 6.8227105140686035
Epoch 810, val loss: 1.1690279245376587
Epoch 820, training loss: 0.07286430895328522 = 0.0045613935217261314 + 0.01 * 6.830291271209717
Epoch 820, val loss: 1.1737658977508545
Epoch 830, training loss: 0.07267545908689499 = 0.0044144573621451855 + 0.01 * 6.826100826263428
Epoch 830, val loss: 1.178633213043213
Epoch 840, training loss: 0.07241571694612503 = 0.004274068400263786 + 0.01 * 6.814165115356445
Epoch 840, val loss: 1.1832503080368042
Epoch 850, training loss: 0.0722576230764389 = 0.004140510223805904 + 0.01 * 6.811711311340332
Epoch 850, val loss: 1.1879329681396484
Epoch 860, training loss: 0.07210706919431686 = 0.004012671299278736 + 0.01 * 6.809439659118652
Epoch 860, val loss: 1.192534327507019
Epoch 870, training loss: 0.07197423279285431 = 0.0038909714203327894 + 0.01 * 6.808326721191406
Epoch 870, val loss: 1.1970750093460083
Epoch 880, training loss: 0.07194013893604279 = 0.003775270888581872 + 0.01 * 6.816486835479736
Epoch 880, val loss: 1.201530933380127
Epoch 890, training loss: 0.0716710314154625 = 0.0036642304621636868 + 0.01 * 6.800680160522461
Epoch 890, val loss: 1.2059236764907837
Epoch 900, training loss: 0.0716857984662056 = 0.0035581651609390974 + 0.01 * 6.812763690948486
Epoch 900, val loss: 1.2103195190429688
Epoch 910, training loss: 0.07144489139318466 = 0.003457297571003437 + 0.01 * 6.7987589836120605
Epoch 910, val loss: 1.214512586593628
Epoch 920, training loss: 0.0715174674987793 = 0.0033611422404646873 + 0.01 * 6.815632343292236
Epoch 920, val loss: 1.218660593032837
Epoch 930, training loss: 0.0712805837392807 = 0.0032699869479984045 + 0.01 * 6.801059722900391
Epoch 930, val loss: 1.2227004766464233
Epoch 940, training loss: 0.07113203406333923 = 0.0031833830289542675 + 0.01 * 6.794865608215332
Epoch 940, val loss: 1.2266019582748413
Epoch 950, training loss: 0.07089650630950928 = 0.0031008722726255655 + 0.01 * 6.7795634269714355
Epoch 950, val loss: 1.230492115020752
Epoch 960, training loss: 0.07118190079927444 = 0.0030220274347811937 + 0.01 * 6.815987586975098
Epoch 960, val loss: 1.234231948852539
Epoch 970, training loss: 0.0707242339849472 = 0.002946687862277031 + 0.01 * 6.777754783630371
Epoch 970, val loss: 1.2379783391952515
Epoch 980, training loss: 0.07058566808700562 = 0.002874936442822218 + 0.01 * 6.771073341369629
Epoch 980, val loss: 1.2415286302566528
Epoch 990, training loss: 0.07061351835727692 = 0.0028064590878784657 + 0.01 * 6.78070592880249
Epoch 990, val loss: 1.245111346244812
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 2.036639451980591 = 1.9506711959838867 + 0.01 * 8.596824645996094
Epoch 0, val loss: 1.952656626701355
Epoch 10, training loss: 2.02640700340271 = 1.9404393434524536 + 0.01 * 8.596763610839844
Epoch 10, val loss: 1.9419924020767212
Epoch 20, training loss: 2.0141749382019043 = 1.9282102584838867 + 0.01 * 8.596480369567871
Epoch 20, val loss: 1.928849458694458
Epoch 30, training loss: 1.9970037937164307 = 1.9110486507415771 + 0.01 * 8.595512390136719
Epoch 30, val loss: 1.910374641418457
Epoch 40, training loss: 1.9716241359710693 = 1.885724663734436 + 0.01 * 8.589943885803223
Epoch 40, val loss: 1.8835349082946777
Epoch 50, training loss: 1.9361152648925781 = 1.8505806922912598 + 0.01 * 8.553452491760254
Epoch 50, val loss: 1.847864031791687
Epoch 60, training loss: 1.8956453800201416 = 1.8121509552001953 + 0.01 * 8.349438667297363
Epoch 60, val loss: 1.812548279762268
Epoch 70, training loss: 1.8617560863494873 = 1.780518651008606 + 0.01 * 8.123745918273926
Epoch 70, val loss: 1.7858684062957764
Epoch 80, training loss: 1.8207685947418213 = 1.742196798324585 + 0.01 * 7.857185363769531
Epoch 80, val loss: 1.751436471939087
Epoch 90, training loss: 1.7645965814590454 = 1.6886053085327148 + 0.01 * 7.599130153656006
Epoch 90, val loss: 1.7038394212722778
Epoch 100, training loss: 1.688464879989624 = 1.6138046979904175 + 0.01 * 7.466014862060547
Epoch 100, val loss: 1.6388522386550903
Epoch 110, training loss: 1.594030499458313 = 1.5197254419326782 + 0.01 * 7.430508136749268
Epoch 110, val loss: 1.5579620599746704
Epoch 120, training loss: 1.489330530166626 = 1.4153040647506714 + 0.01 * 7.402646541595459
Epoch 120, val loss: 1.469305157661438
Epoch 130, training loss: 1.3815997838974 = 1.307881236076355 + 0.01 * 7.37185525894165
Epoch 130, val loss: 1.3811804056167603
Epoch 140, training loss: 1.275098204612732 = 1.2017521858215332 + 0.01 * 7.334598541259766
Epoch 140, val loss: 1.297990083694458
Epoch 150, training loss: 1.1732735633850098 = 1.10037100315094 + 0.01 * 7.290257453918457
Epoch 150, val loss: 1.2221754789352417
Epoch 160, training loss: 1.0785555839538574 = 1.0060737133026123 + 0.01 * 7.248186111450195
Epoch 160, val loss: 1.1539568901062012
Epoch 170, training loss: 0.991856038570404 = 0.9195915460586548 + 0.01 * 7.226449012756348
Epoch 170, val loss: 1.0924361944198608
Epoch 180, training loss: 0.91336590051651 = 0.8412096500396729 + 0.01 * 7.215623378753662
Epoch 180, val loss: 1.0376938581466675
Epoch 190, training loss: 0.8430747389793396 = 0.7710081338882446 + 0.01 * 7.206661224365234
Epoch 190, val loss: 0.9901911020278931
Epoch 200, training loss: 0.78031325340271 = 0.7083662152290344 + 0.01 * 7.194701671600342
Epoch 200, val loss: 0.9503619074821472
Epoch 210, training loss: 0.7236815094947815 = 0.6518839001655579 + 0.01 * 7.1797614097595215
Epoch 210, val loss: 0.9178712964057922
Epoch 220, training loss: 0.6714282035827637 = 0.599797785282135 + 0.01 * 7.163043975830078
Epoch 220, val loss: 0.8918346166610718
Epoch 230, training loss: 0.6224265098571777 = 0.5509632229804993 + 0.01 * 7.146328926086426
Epoch 230, val loss: 0.8717080354690552
Epoch 240, training loss: 0.5763856172561646 = 0.5050803422927856 + 0.01 * 7.13053035736084
Epoch 240, val loss: 0.8570638298988342
Epoch 250, training loss: 0.5335644483566284 = 0.46239349246025085 + 0.01 * 7.117093563079834
Epoch 250, val loss: 0.8475326299667358
Epoch 260, training loss: 0.49421417713165283 = 0.4231599271297455 + 0.01 * 7.105425834655762
Epoch 260, val loss: 0.8426510095596313
Epoch 270, training loss: 0.4584760367870331 = 0.38752174377441406 + 0.01 * 7.095430374145508
Epoch 270, val loss: 0.8420202136039734
Epoch 280, training loss: 0.4262741208076477 = 0.35532569885253906 + 0.01 * 7.09484338760376
Epoch 280, val loss: 0.8447151184082031
Epoch 290, training loss: 0.39695560932159424 = 0.32614263892173767 + 0.01 * 7.081295490264893
Epoch 290, val loss: 0.8497569561004639
Epoch 300, training loss: 0.3699684143066406 = 0.2992750406265259 + 0.01 * 7.069338798522949
Epoch 300, val loss: 0.8564819693565369
Epoch 310, training loss: 0.34456828236579895 = 0.27396073937416077 + 0.01 * 7.060755252838135
Epoch 310, val loss: 0.8642790913581848
Epoch 320, training loss: 0.32007190585136414 = 0.24947500228881836 + 0.01 * 7.059690475463867
Epoch 320, val loss: 0.8726158142089844
Epoch 330, training loss: 0.29579290747642517 = 0.22533643245697021 + 0.01 * 7.045648574829102
Epoch 330, val loss: 0.8809635043144226
Epoch 340, training loss: 0.271911084651947 = 0.20151138305664062 + 0.01 * 7.039968967437744
Epoch 340, val loss: 0.889409065246582
Epoch 350, training loss: 0.24897456169128418 = 0.1785212755203247 + 0.01 * 7.045328617095947
Epoch 350, val loss: 0.8983451724052429
Epoch 360, training loss: 0.22740384936332703 = 0.15713649988174438 + 0.01 * 7.026734828948975
Epoch 360, val loss: 0.908395528793335
Epoch 370, training loss: 0.20802408456802368 = 0.13782043755054474 + 0.01 * 7.0203657150268555
Epoch 370, val loss: 0.919946551322937
Epoch 380, training loss: 0.19082507491111755 = 0.12072049081325531 + 0.01 * 7.010458469390869
Epoch 380, val loss: 0.9332385063171387
Epoch 390, training loss: 0.1758921891450882 = 0.10580425709486008 + 0.01 * 7.008793354034424
Epoch 390, val loss: 0.9481567144393921
Epoch 400, training loss: 0.16294792294502258 = 0.0929456353187561 + 0.01 * 7.000229358673096
Epoch 400, val loss: 0.9643151164054871
Epoch 410, training loss: 0.15189531445503235 = 0.08192402124404907 + 0.01 * 6.997129440307617
Epoch 410, val loss: 0.9816526174545288
Epoch 420, training loss: 0.14237213134765625 = 0.07250048965215683 + 0.01 * 6.987164497375488
Epoch 420, val loss: 0.9998881220817566
Epoch 430, training loss: 0.13424956798553467 = 0.06443241238594055 + 0.01 * 6.981715679168701
Epoch 430, val loss: 1.0186762809753418
Epoch 440, training loss: 0.12727518379688263 = 0.05750412121415138 + 0.01 * 6.977106094360352
Epoch 440, val loss: 1.037644624710083
Epoch 450, training loss: 0.1214144229888916 = 0.05152825638651848 + 0.01 * 6.988617420196533
Epoch 450, val loss: 1.0566692352294922
Epoch 460, training loss: 0.1160699874162674 = 0.0463593415915966 + 0.01 * 6.971065044403076
Epoch 460, val loss: 1.0753185749053955
Epoch 470, training loss: 0.11152224987745285 = 0.041866593062877655 + 0.01 * 6.965566158294678
Epoch 470, val loss: 1.0937546491622925
Epoch 480, training loss: 0.10758943110704422 = 0.03794874995946884 + 0.01 * 6.964068412780762
Epoch 480, val loss: 1.1117299795150757
Epoch 490, training loss: 0.10400821268558502 = 0.03451726213097572 + 0.01 * 6.949095249176025
Epoch 490, val loss: 1.1292005777359009
Epoch 500, training loss: 0.10100291669368744 = 0.03149866685271263 + 0.01 * 6.950424671173096
Epoch 500, val loss: 1.1462198495864868
Epoch 510, training loss: 0.098294198513031 = 0.028838275000452995 + 0.01 * 6.945592403411865
Epoch 510, val loss: 1.1626540422439575
Epoch 520, training loss: 0.09593481570482254 = 0.026485586538910866 + 0.01 * 6.944922924041748
Epoch 520, val loss: 1.178579330444336
Epoch 530, training loss: 0.0937945619225502 = 0.0243997722864151 + 0.01 * 6.939478874206543
Epoch 530, val loss: 1.1940480470657349
Epoch 540, training loss: 0.09179139882326126 = 0.022544361650943756 + 0.01 * 6.924704074859619
Epoch 540, val loss: 1.208916425704956
Epoch 550, training loss: 0.09010836482048035 = 0.020888015627861023 + 0.01 * 6.922034740447998
Epoch 550, val loss: 1.2233188152313232
Epoch 560, training loss: 0.08869090676307678 = 0.019404392689466476 + 0.01 * 6.928651809692383
Epoch 560, val loss: 1.237175464630127
Epoch 570, training loss: 0.08730386197566986 = 0.018073050305247307 + 0.01 * 6.923080921173096
Epoch 570, val loss: 1.2505077123641968
Epoch 580, training loss: 0.08599192649126053 = 0.016875488683581352 + 0.01 * 6.911643981933594
Epoch 580, val loss: 1.263387680053711
Epoch 590, training loss: 0.08502539247274399 = 0.015794090926647186 + 0.01 * 6.923130512237549
Epoch 590, val loss: 1.275877594947815
Epoch 600, training loss: 0.08382480591535568 = 0.01481619942933321 + 0.01 * 6.900860786437988
Epoch 600, val loss: 1.2878086566925049
Epoch 610, training loss: 0.0829964280128479 = 0.01392865926027298 + 0.01 * 6.9067769050598145
Epoch 610, val loss: 1.2994178533554077
Epoch 620, training loss: 0.08189990371465683 = 0.013120986521244049 + 0.01 * 6.877892017364502
Epoch 620, val loss: 1.310603380203247
Epoch 630, training loss: 0.08153008669614792 = 0.012383493594825268 + 0.01 * 6.91465950012207
Epoch 630, val loss: 1.3214397430419922
Epoch 640, training loss: 0.08040686696767807 = 0.011709829792380333 + 0.01 * 6.869703769683838
Epoch 640, val loss: 1.3318145275115967
Epoch 650, training loss: 0.07976189255714417 = 0.011092358268797398 + 0.01 * 6.8669538497924805
Epoch 650, val loss: 1.3418631553649902
Epoch 660, training loss: 0.07921430468559265 = 0.010525011457502842 + 0.01 * 6.868929862976074
Epoch 660, val loss: 1.351649522781372
Epoch 670, training loss: 0.07851418852806091 = 0.01000248547643423 + 0.01 * 6.851170063018799
Epoch 670, val loss: 1.3610584735870361
Epoch 680, training loss: 0.07810527831315994 = 0.009519466198980808 + 0.01 * 6.85858154296875
Epoch 680, val loss: 1.3702534437179565
Epoch 690, training loss: 0.07769646495580673 = 0.009073739871382713 + 0.01 * 6.8622727394104
Epoch 690, val loss: 1.3790502548217773
Epoch 700, training loss: 0.07713915407657623 = 0.008661511354148388 + 0.01 * 6.84776496887207
Epoch 700, val loss: 1.3875683546066284
Epoch 710, training loss: 0.07660754770040512 = 0.008279171772301197 + 0.01 * 6.8328375816345215
Epoch 710, val loss: 1.3958380222320557
Epoch 720, training loss: 0.07660475373268127 = 0.007923615165054798 + 0.01 * 6.8681135177612305
Epoch 720, val loss: 1.4037666320800781
Epoch 730, training loss: 0.07597798109054565 = 0.007592801470309496 + 0.01 * 6.838518142700195
Epoch 730, val loss: 1.4116662740707397
Epoch 740, training loss: 0.07564017921686172 = 0.0072840494103729725 + 0.01 * 6.835612773895264
Epoch 740, val loss: 1.41917085647583
Epoch 750, training loss: 0.07529851794242859 = 0.006995567586272955 + 0.01 * 6.830295085906982
Epoch 750, val loss: 1.4266102313995361
Epoch 760, training loss: 0.07509617507457733 = 0.006725637707859278 + 0.01 * 6.8370537757873535
Epoch 760, val loss: 1.4336769580841064
Epoch 770, training loss: 0.07474274933338165 = 0.006472854409366846 + 0.01 * 6.826989650726318
Epoch 770, val loss: 1.440751314163208
Epoch 780, training loss: 0.07437652349472046 = 0.006235766224563122 + 0.01 * 6.8140764236450195
Epoch 780, val loss: 1.4474691152572632
Epoch 790, training loss: 0.07416172325611115 = 0.006012873258441687 + 0.01 * 6.81488561630249
Epoch 790, val loss: 1.4540634155273438
Epoch 800, training loss: 0.07381770014762878 = 0.005803226958960295 + 0.01 * 6.80144739151001
Epoch 800, val loss: 1.4603909254074097
Epoch 810, training loss: 0.07367363572120667 = 0.005605920217931271 + 0.01 * 6.806771755218506
Epoch 810, val loss: 1.4665635824203491
Epoch 820, training loss: 0.0735490545630455 = 0.005419823341071606 + 0.01 * 6.812923908233643
Epoch 820, val loss: 1.4726866483688354
Epoch 830, training loss: 0.07323122024536133 = 0.005244227591902018 + 0.01 * 6.798699378967285
Epoch 830, val loss: 1.4785858392715454
Epoch 840, training loss: 0.0729442909359932 = 0.005078108049929142 + 0.01 * 6.786618709564209
Epoch 840, val loss: 1.4843389987945557
Epoch 850, training loss: 0.07305089384317398 = 0.004920801613479853 + 0.01 * 6.813009262084961
Epoch 850, val loss: 1.4898505210876465
Epoch 860, training loss: 0.07274375855922699 = 0.004772115033119917 + 0.01 * 6.7971649169921875
Epoch 860, val loss: 1.4954112768173218
Epoch 870, training loss: 0.0726376473903656 = 0.0046310219913721085 + 0.01 * 6.800662994384766
Epoch 870, val loss: 1.5007466077804565
Epoch 880, training loss: 0.07228581607341766 = 0.0044972556643188 + 0.01 * 6.7788567543029785
Epoch 880, val loss: 1.5058847665786743
Epoch 890, training loss: 0.07221920788288116 = 0.004370155278593302 + 0.01 * 6.784905910491943
Epoch 890, val loss: 1.5108479261398315
Epoch 900, training loss: 0.07206569612026215 = 0.004249341320246458 + 0.01 * 6.781635761260986
Epoch 900, val loss: 1.5158418416976929
Epoch 910, training loss: 0.07186552882194519 = 0.004134305752813816 + 0.01 * 6.773122787475586
Epoch 910, val loss: 1.5206799507141113
Epoch 920, training loss: 0.07179892063140869 = 0.004024712834507227 + 0.01 * 6.777420997619629
Epoch 920, val loss: 1.5252447128295898
Epoch 930, training loss: 0.07177354395389557 = 0.003920502960681915 + 0.01 * 6.785304546356201
Epoch 930, val loss: 1.5298917293548584
Epoch 940, training loss: 0.07144489884376526 = 0.003821111284196377 + 0.01 * 6.762379169464111
Epoch 940, val loss: 1.5343366861343384
Epoch 950, training loss: 0.07135581970214844 = 0.00372612988576293 + 0.01 * 6.762969017028809
Epoch 950, val loss: 1.5386213064193726
Epoch 960, training loss: 0.07130349427461624 = 0.00363540300168097 + 0.01 * 6.766808986663818
Epoch 960, val loss: 1.542985439300537
Epoch 970, training loss: 0.0712747871875763 = 0.0035486093256622553 + 0.01 * 6.772617816925049
Epoch 970, val loss: 1.5470129251480103
Epoch 980, training loss: 0.07123628258705139 = 0.003465614514425397 + 0.01 * 6.777066707611084
Epoch 980, val loss: 1.5510812997817993
Epoch 990, training loss: 0.07104962319135666 = 0.0033863368444144726 + 0.01 * 6.766329288482666
Epoch 990, val loss: 1.555095911026001
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8133895624670533
The final CL Acc:0.74691, 0.01492, The final GNN Acc:0.81234, 0.00114
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13204])
remove edge: torch.Size([2, 7824])
updated graph: torch.Size([2, 10472])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.021177291870117 = 1.9352086782455444 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9344059228897095
Epoch 10, training loss: 2.0117759704589844 = 1.9258078336715698 + 0.01 * 8.596819877624512
Epoch 10, val loss: 1.9254026412963867
Epoch 20, training loss: 2.0004355907440186 = 1.9144691228866577 + 0.01 * 8.596641540527344
Epoch 20, val loss: 1.914067029953003
Epoch 30, training loss: 1.9849873781204224 = 1.8990271091461182 + 0.01 * 8.596022605895996
Epoch 30, val loss: 1.8983426094055176
Epoch 40, training loss: 1.962709903717041 = 1.8767848014831543 + 0.01 * 8.592511177062988
Epoch 40, val loss: 1.8758209943771362
Epoch 50, training loss: 1.9313238859176636 = 1.8456591367721558 + 0.01 * 8.566472053527832
Epoch 50, val loss: 1.8451417684555054
Epoch 60, training loss: 1.8925906419754028 = 1.8081164360046387 + 0.01 * 8.447421073913574
Epoch 60, val loss: 1.8106220960617065
Epoch 70, training loss: 1.8531498908996582 = 1.7716032266616821 + 0.01 * 8.15466594696045
Epoch 70, val loss: 1.7795233726501465
Epoch 80, training loss: 1.8090713024139404 = 1.7290775775909424 + 0.01 * 7.999377250671387
Epoch 80, val loss: 1.7415789365768433
Epoch 90, training loss: 1.7462434768676758 = 1.6688495874404907 + 0.01 * 7.7393903732299805
Epoch 90, val loss: 1.6877962350845337
Epoch 100, training loss: 1.6616613864898682 = 1.5865439176559448 + 0.01 * 7.511745929718018
Epoch 100, val loss: 1.6162570714950562
Epoch 110, training loss: 1.5577648878097534 = 1.4839158058166504 + 0.01 * 7.384904861450195
Epoch 110, val loss: 1.52791166305542
Epoch 120, training loss: 1.4419747591018677 = 1.3688786029815674 + 0.01 * 7.3096160888671875
Epoch 120, val loss: 1.4307940006256104
Epoch 130, training loss: 1.3200864791870117 = 1.2475463151931763 + 0.01 * 7.254018783569336
Epoch 130, val loss: 1.3307197093963623
Epoch 140, training loss: 1.1971030235290527 = 1.124882698059082 + 0.01 * 7.222034454345703
Epoch 140, val loss: 1.2314178943634033
Epoch 150, training loss: 1.0789514780044556 = 1.0069292783737183 + 0.01 * 7.202224254608154
Epoch 150, val loss: 1.1376769542694092
Epoch 160, training loss: 0.9713526368141174 = 0.8995293378829956 + 0.01 * 7.18233060836792
Epoch 160, val loss: 1.0533032417297363
Epoch 170, training loss: 0.8780145645141602 = 0.8064178824424744 + 0.01 * 7.159665107727051
Epoch 170, val loss: 0.9811107516288757
Epoch 180, training loss: 0.7995155453681946 = 0.728138267993927 + 0.01 * 7.137726783752441
Epoch 180, val loss: 0.9220567941665649
Epoch 190, training loss: 0.7335050702095032 = 0.6622843742370605 + 0.01 * 7.122071743011475
Epoch 190, val loss: 0.8744198679924011
Epoch 200, training loss: 0.6759107708930969 = 0.6047698259353638 + 0.01 * 7.114096164703369
Epoch 200, val loss: 0.8350328207015991
Epoch 210, training loss: 0.6230934262275696 = 0.5519903898239136 + 0.01 * 7.1103057861328125
Epoch 210, val loss: 0.8007872700691223
Epoch 220, training loss: 0.5730976462364197 = 0.5020273327827454 + 0.01 * 7.107031345367432
Epoch 220, val loss: 0.7702851891517639
Epoch 230, training loss: 0.5254710912704468 = 0.4544248878955841 + 0.01 * 7.10461950302124
Epoch 230, val loss: 0.7435142397880554
Epoch 240, training loss: 0.48065054416656494 = 0.40962642431259155 + 0.01 * 7.102411270141602
Epoch 240, val loss: 0.7207396626472473
Epoch 250, training loss: 0.4391712248325348 = 0.36816850304603577 + 0.01 * 7.1002726554870605
Epoch 250, val loss: 0.7021459937095642
Epoch 260, training loss: 0.40115848183631897 = 0.33017900586128235 + 0.01 * 7.097947597503662
Epoch 260, val loss: 0.6873884201049805
Epoch 270, training loss: 0.3662506341934204 = 0.29529646039009094 + 0.01 * 7.095418930053711
Epoch 270, val loss: 0.6757072806358337
Epoch 280, training loss: 0.3337920308113098 = 0.2628612220287323 + 0.01 * 7.093081474304199
Epoch 280, val loss: 0.6663413643836975
Epoch 290, training loss: 0.30323508381843567 = 0.2323380410671234 + 0.01 * 7.089704513549805
Epoch 290, val loss: 0.6587942242622375
Epoch 300, training loss: 0.27448365092277527 = 0.20361720025539398 + 0.01 * 7.086644172668457
Epoch 300, val loss: 0.6528769731521606
Epoch 310, training loss: 0.24790191650390625 = 0.1770474761724472 + 0.01 * 7.085444927215576
Epoch 310, val loss: 0.64860600233078
Epoch 320, training loss: 0.2239668071269989 = 0.15315686166286469 + 0.01 * 7.080995559692383
Epoch 320, val loss: 0.6461318135261536
Epoch 330, training loss: 0.20308619737625122 = 0.1323091983795166 + 0.01 * 7.077700614929199
Epoch 330, val loss: 0.6457643508911133
Epoch 340, training loss: 0.185291588306427 = 0.11451569944620132 + 0.01 * 7.077588081359863
Epoch 340, val loss: 0.6475532650947571
Epoch 350, training loss: 0.17021368443965912 = 0.09950131177902222 + 0.01 * 7.071237087249756
Epoch 350, val loss: 0.6512748599052429
Epoch 360, training loss: 0.15754517912864685 = 0.08686955273151398 + 0.01 * 7.067562103271484
Epoch 360, val loss: 0.6566558480262756
Epoch 370, training loss: 0.14685148000717163 = 0.07622058689594269 + 0.01 * 7.063089847564697
Epoch 370, val loss: 0.6634063124656677
Epoch 380, training loss: 0.13782045245170593 = 0.06720353662967682 + 0.01 * 7.0616912841796875
Epoch 380, val loss: 0.6712321043014526
Epoch 390, training loss: 0.13012045621871948 = 0.05953099578619003 + 0.01 * 7.0589470863342285
Epoch 390, val loss: 0.6798221468925476
Epoch 400, training loss: 0.12346277385950089 = 0.05296769738197327 + 0.01 * 7.049508094787598
Epoch 400, val loss: 0.6889641284942627
Epoch 410, training loss: 0.1177746057510376 = 0.04733088240027428 + 0.01 * 7.044372081756592
Epoch 410, val loss: 0.6984862685203552
Epoch 420, training loss: 0.11290611326694489 = 0.042475078254938126 + 0.01 * 7.04310417175293
Epoch 420, val loss: 0.7081599831581116
Epoch 430, training loss: 0.10860686004161835 = 0.038276877254247665 + 0.01 * 7.032999038696289
Epoch 430, val loss: 0.7179288268089294
Epoch 440, training loss: 0.10510149598121643 = 0.03463490679860115 + 0.01 * 7.046658992767334
Epoch 440, val loss: 0.727712869644165
Epoch 450, training loss: 0.10162321478128433 = 0.03146786242723465 + 0.01 * 7.015535354614258
Epoch 450, val loss: 0.7374137043952942
Epoch 460, training loss: 0.09885665774345398 = 0.028702232986688614 + 0.01 * 7.015442371368408
Epoch 460, val loss: 0.7470118999481201
Epoch 470, training loss: 0.09627331793308258 = 0.026277732104063034 + 0.01 * 6.999558448791504
Epoch 470, val loss: 0.7564537525177002
Epoch 480, training loss: 0.0940384566783905 = 0.024144161492586136 + 0.01 * 6.989429473876953
Epoch 480, val loss: 0.7657294273376465
Epoch 490, training loss: 0.0920746698975563 = 0.022260621190071106 + 0.01 * 6.981404781341553
Epoch 490, val loss: 0.7747697234153748
Epoch 500, training loss: 0.09089730679988861 = 0.02059195563197136 + 0.01 * 7.0305352210998535
Epoch 500, val loss: 0.783585786819458
Epoch 510, training loss: 0.08873836696147919 = 0.01911238394677639 + 0.01 * 6.9625983238220215
Epoch 510, val loss: 0.7921435832977295
Epoch 520, training loss: 0.08732623606920242 = 0.017790978774428368 + 0.01 * 6.953526020050049
Epoch 520, val loss: 0.8004401922225952
Epoch 530, training loss: 0.08628733456134796 = 0.01660473644733429 + 0.01 * 6.968260288238525
Epoch 530, val loss: 0.8085453510284424
Epoch 540, training loss: 0.08500486612319946 = 0.015537318773567677 + 0.01 * 6.9467549324035645
Epoch 540, val loss: 0.8164417743682861
Epoch 550, training loss: 0.08399642258882523 = 0.014573396183550358 + 0.01 * 6.94230318069458
Epoch 550, val loss: 0.8241046071052551
Epoch 560, training loss: 0.08322407305240631 = 0.013699912466108799 + 0.01 * 6.95241641998291
Epoch 560, val loss: 0.8315609693527222
Epoch 570, training loss: 0.08201149851083755 = 0.012906867079436779 + 0.01 * 6.910463333129883
Epoch 570, val loss: 0.8388046026229858
Epoch 580, training loss: 0.08127370476722717 = 0.01218439545482397 + 0.01 * 6.908930778503418
Epoch 580, val loss: 0.8458337187767029
Epoch 590, training loss: 0.08072517812252045 = 0.01152496412396431 + 0.01 * 6.920022010803223
Epoch 590, val loss: 0.8526681065559387
Epoch 600, training loss: 0.07979486137628555 = 0.010921728797256947 + 0.01 * 6.887312889099121
Epoch 600, val loss: 0.8593094348907471
Epoch 610, training loss: 0.07945970445871353 = 0.01036777999252081 + 0.01 * 6.9091925621032715
Epoch 610, val loss: 0.8657469153404236
Epoch 620, training loss: 0.0786224827170372 = 0.00985858403146267 + 0.01 * 6.876389503479004
Epoch 620, val loss: 0.8720088005065918
Epoch 630, training loss: 0.0781821459531784 = 0.009388595819473267 + 0.01 * 6.879355430603027
Epoch 630, val loss: 0.878095805644989
Epoch 640, training loss: 0.07825221121311188 = 0.008954313583672047 + 0.01 * 6.9297895431518555
Epoch 640, val loss: 0.8840294480323792
Epoch 650, training loss: 0.07730312645435333 = 0.008553329855203629 + 0.01 * 6.874979496002197
Epoch 650, val loss: 0.8897638916969299
Epoch 660, training loss: 0.07670111954212189 = 0.008181172423064709 + 0.01 * 6.85199499130249
Epoch 660, val loss: 0.8953238725662231
Epoch 670, training loss: 0.07638746500015259 = 0.007834410294890404 + 0.01 * 6.8553056716918945
Epoch 670, val loss: 0.9007443189620972
Epoch 680, training loss: 0.07606826722621918 = 0.0075112744234502316 + 0.01 * 6.85569953918457
Epoch 680, val loss: 0.9060282111167908
Epoch 690, training loss: 0.07593600451946259 = 0.0072098481468856335 + 0.01 * 6.872616291046143
Epoch 690, val loss: 0.9111698865890503
Epoch 700, training loss: 0.0753118097782135 = 0.006928734946995974 + 0.01 * 6.838307857513428
Epoch 700, val loss: 0.916172981262207
Epoch 710, training loss: 0.07500141113996506 = 0.006665438879281282 + 0.01 * 6.833597660064697
Epoch 710, val loss: 0.9210454225540161
Epoch 720, training loss: 0.07481639832258224 = 0.006418606732040644 + 0.01 * 6.839778900146484
Epoch 720, val loss: 0.9257809519767761
Epoch 730, training loss: 0.07477336376905441 = 0.0061866813339293 + 0.01 * 6.858668804168701
Epoch 730, val loss: 0.9303993582725525
Epoch 740, training loss: 0.07411728799343109 = 0.005969259887933731 + 0.01 * 6.814803600311279
Epoch 740, val loss: 0.9348906874656677
Epoch 750, training loss: 0.07406453788280487 = 0.005763995461165905 + 0.01 * 6.830054759979248
Epoch 750, val loss: 0.9392706155776978
Epoch 760, training loss: 0.07362479716539383 = 0.005571076180785894 + 0.01 * 6.805372714996338
Epoch 760, val loss: 0.9435257315635681
Epoch 770, training loss: 0.07371910661458969 = 0.005388872232288122 + 0.01 * 6.833023548126221
Epoch 770, val loss: 0.9476976990699768
Epoch 780, training loss: 0.07331954687833786 = 0.005216964054852724 + 0.01 * 6.810258388519287
Epoch 780, val loss: 0.9517335295677185
Epoch 790, training loss: 0.0736476257443428 = 0.0050544822588562965 + 0.01 * 6.859314918518066
Epoch 790, val loss: 0.9556872248649597
Epoch 800, training loss: 0.07292000949382782 = 0.004900898318737745 + 0.01 * 6.8019118309021
Epoch 800, val loss: 0.9594994187355042
Epoch 810, training loss: 0.07259722054004669 = 0.004755406174808741 + 0.01 * 6.784181118011475
Epoch 810, val loss: 0.9632276892662048
Epoch 820, training loss: 0.07264543324708939 = 0.0046171401627361774 + 0.01 * 6.802829265594482
Epoch 820, val loss: 0.96688312292099
Epoch 830, training loss: 0.07231099158525467 = 0.004485900979489088 + 0.01 * 6.7825093269348145
Epoch 830, val loss: 0.9704424738883972
Epoch 840, training loss: 0.07210539281368256 = 0.004361370578408241 + 0.01 * 6.774402618408203
Epoch 840, val loss: 0.9739238619804382
Epoch 850, training loss: 0.07212252169847488 = 0.004242313094437122 + 0.01 * 6.788021087646484
Epoch 850, val loss: 0.9773419499397278
Epoch 860, training loss: 0.07195111364126205 = 0.004129077307879925 + 0.01 * 6.7822041511535645
Epoch 860, val loss: 0.9806634783744812
Epoch 870, training loss: 0.07167918980121613 = 0.004021212458610535 + 0.01 * 6.765798091888428
Epoch 870, val loss: 0.9839203953742981
Epoch 880, training loss: 0.0716019943356514 = 0.003918402828276157 + 0.01 * 6.768359661102295
Epoch 880, val loss: 0.9871005415916443
Epoch 890, training loss: 0.0714287981390953 = 0.003820054465904832 + 0.01 * 6.7608747482299805
Epoch 890, val loss: 0.9902203679084778
Epoch 900, training loss: 0.07141349464654922 = 0.0037262083496898413 + 0.01 * 6.768729209899902
Epoch 900, val loss: 0.9932826161384583
Epoch 910, training loss: 0.07112937420606613 = 0.0036364321131259203 + 0.01 * 6.749293804168701
Epoch 910, val loss: 0.996243953704834
Epoch 920, training loss: 0.07124444842338562 = 0.003550577210262418 + 0.01 * 6.769387722015381
Epoch 920, val loss: 0.9991570115089417
Epoch 930, training loss: 0.07092473655939102 = 0.0034684932325035334 + 0.01 * 6.745625019073486
Epoch 930, val loss: 1.0020105838775635
Epoch 940, training loss: 0.07082123309373856 = 0.003389876103028655 + 0.01 * 6.743136405944824
Epoch 940, val loss: 1.0047883987426758
Epoch 950, training loss: 0.07069814205169678 = 0.0033145213965326548 + 0.01 * 6.738361835479736
Epoch 950, val loss: 1.0075244903564453
Epoch 960, training loss: 0.070558100938797 = 0.0032421895302832127 + 0.01 * 6.731591701507568
Epoch 960, val loss: 1.0101853609085083
Epoch 970, training loss: 0.07053434103727341 = 0.0031728337053209543 + 0.01 * 6.736150741577148
Epoch 970, val loss: 1.012786865234375
Epoch 980, training loss: 0.07031869143247604 = 0.003106452291831374 + 0.01 * 6.721224308013916
Epoch 980, val loss: 1.0153554677963257
Epoch 990, training loss: 0.07028960436582565 = 0.0030425109434872866 + 0.01 * 6.724709510803223
Epoch 990, val loss: 1.0178660154342651
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.045941114425659 = 1.9599725008010864 + 0.01 * 8.596860885620117
Epoch 0, val loss: 1.954767107963562
Epoch 10, training loss: 2.034869909286499 = 1.9489017724990845 + 0.01 * 8.596816062927246
Epoch 10, val loss: 1.9439563751220703
Epoch 20, training loss: 2.0213191509246826 = 1.9353525638580322 + 0.01 * 8.596651077270508
Epoch 20, val loss: 1.930135726928711
Epoch 30, training loss: 2.0025367736816406 = 1.9165756702423096 + 0.01 * 8.59611701965332
Epoch 30, val loss: 1.9104515314102173
Epoch 40, training loss: 1.9748470783233643 = 1.8889172077178955 + 0.01 * 8.592985153198242
Epoch 40, val loss: 1.8812230825424194
Epoch 50, training loss: 1.9355578422546387 = 1.849882960319519 + 0.01 * 8.567485809326172
Epoch 50, val loss: 1.8411896228790283
Epoch 60, training loss: 1.8896528482437134 = 1.8051366806030273 + 0.01 * 8.451622009277344
Epoch 60, val loss: 1.7993327379226685
Epoch 70, training loss: 1.8468655347824097 = 1.7654837369918823 + 0.01 * 8.138178825378418
Epoch 70, val loss: 1.7667466402053833
Epoch 80, training loss: 1.7993884086608887 = 1.7194206714630127 + 0.01 * 7.996767997741699
Epoch 80, val loss: 1.7279378175735474
Epoch 90, training loss: 1.7357878684997559 = 1.6578309535980225 + 0.01 * 7.795689105987549
Epoch 90, val loss: 1.673682689666748
Epoch 100, training loss: 1.6537880897521973 = 1.5776015520095825 + 0.01 * 7.618657112121582
Epoch 100, val loss: 1.6038466691970825
Epoch 110, training loss: 1.560852289199829 = 1.4863392114639282 + 0.01 * 7.451308727264404
Epoch 110, val loss: 1.527103066444397
Epoch 120, training loss: 1.467198133468628 = 1.3935365676879883 + 0.01 * 7.366156101226807
Epoch 120, val loss: 1.4541021585464478
Epoch 130, training loss: 1.3761584758758545 = 1.3029112815856934 + 0.01 * 7.324718952178955
Epoch 130, val loss: 1.386996865272522
Epoch 140, training loss: 1.2864329814910889 = 1.213467001914978 + 0.01 * 7.296599388122559
Epoch 140, val loss: 1.322930932044983
Epoch 150, training loss: 1.1974496841430664 = 1.1247427463531494 + 0.01 * 7.270688056945801
Epoch 150, val loss: 1.260642409324646
Epoch 160, training loss: 1.1103057861328125 = 1.0377192497253418 + 0.01 * 7.258654594421387
Epoch 160, val loss: 1.1993714570999146
Epoch 170, training loss: 1.0260974168777466 = 0.9535666704177856 + 0.01 * 7.253076076507568
Epoch 170, val loss: 1.1393216848373413
Epoch 180, training loss: 0.9447889924049377 = 0.872298002243042 + 0.01 * 7.2490973472595215
Epoch 180, val loss: 1.0802655220031738
Epoch 190, training loss: 0.8655256032943726 = 0.7930777668952942 + 0.01 * 7.244781494140625
Epoch 190, val loss: 1.0220248699188232
Epoch 200, training loss: 0.7881843447685242 = 0.7157914638519287 + 0.01 * 7.239287853240967
Epoch 200, val loss: 0.9655654430389404
Epoch 210, training loss: 0.7136458158493042 = 0.6413273811340332 + 0.01 * 7.231846332550049
Epoch 210, val loss: 0.9126463532447815
Epoch 220, training loss: 0.6431273818016052 = 0.5709108710289001 + 0.01 * 7.221651554107666
Epoch 220, val loss: 0.8654267191886902
Epoch 230, training loss: 0.5776309370994568 = 0.5055400133132935 + 0.01 * 7.209092617034912
Epoch 230, val loss: 0.8253774642944336
Epoch 240, training loss: 0.5175245404243469 = 0.44560742378234863 + 0.01 * 7.19171142578125
Epoch 240, val loss: 0.7932255268096924
Epoch 250, training loss: 0.4629020690917969 = 0.3911897540092468 + 0.01 * 7.171231269836426
Epoch 250, val loss: 0.7686877846717834
Epoch 260, training loss: 0.41370686888694763 = 0.3421594798564911 + 0.01 * 7.154737949371338
Epoch 260, val loss: 0.7504651546478271
Epoch 270, training loss: 0.36954575777053833 = 0.2982303500175476 + 0.01 * 7.131541728973389
Epoch 270, val loss: 0.7374197244644165
Epoch 280, training loss: 0.3303492069244385 = 0.25914737582206726 + 0.01 * 7.120183944702148
Epoch 280, val loss: 0.7288825511932373
Epoch 290, training loss: 0.2956981658935547 = 0.22465264797210693 + 0.01 * 7.104551315307617
Epoch 290, val loss: 0.7243609428405762
Epoch 300, training loss: 0.2655438184738159 = 0.19452930986881256 + 0.01 * 7.101451396942139
Epoch 300, val loss: 0.7232573628425598
Epoch 310, training loss: 0.23942384123802185 = 0.16852477192878723 + 0.01 * 7.089907169342041
Epoch 310, val loss: 0.7252086400985718
Epoch 320, training loss: 0.21711231768131256 = 0.1462819129228592 + 0.01 * 7.083040714263916
Epoch 320, val loss: 0.7297043800354004
Epoch 330, training loss: 0.19815723598003387 = 0.1273733228445053 + 0.01 * 7.0783915519714355
Epoch 330, val loss: 0.7362180352210999
Epoch 340, training loss: 0.18208040297031403 = 0.11133276671171188 + 0.01 * 7.074763774871826
Epoch 340, val loss: 0.7444162368774414
Epoch 350, training loss: 0.16847190260887146 = 0.09770587086677551 + 0.01 * 7.076603889465332
Epoch 350, val loss: 0.754028856754303
Epoch 360, training loss: 0.15678636729717255 = 0.08609379827976227 + 0.01 * 7.0692572593688965
Epoch 360, val loss: 0.7646403312683105
Epoch 370, training loss: 0.14677056670188904 = 0.07616180926561356 + 0.01 * 7.060876369476318
Epoch 370, val loss: 0.7759791016578674
Epoch 380, training loss: 0.13822129368782043 = 0.06763859838247299 + 0.01 * 7.05826997756958
Epoch 380, val loss: 0.7878794074058533
Epoch 390, training loss: 0.13082823157310486 = 0.0603044331073761 + 0.01 * 7.052379608154297
Epoch 390, val loss: 0.8001258373260498
Epoch 400, training loss: 0.12442775815725327 = 0.05396600812673569 + 0.01 * 7.046175479888916
Epoch 400, val loss: 0.8125669956207275
Epoch 410, training loss: 0.1189209520816803 = 0.048471786081790924 + 0.01 * 7.04491662979126
Epoch 410, val loss: 0.825127363204956
Epoch 420, training loss: 0.11403479427099228 = 0.043698251247406006 + 0.01 * 7.033654689788818
Epoch 420, val loss: 0.837714433670044
Epoch 430, training loss: 0.10991854965686798 = 0.03953680768609047 + 0.01 * 7.038173675537109
Epoch 430, val loss: 0.8502385020256042
Epoch 440, training loss: 0.10616359859704971 = 0.035899557173252106 + 0.01 * 7.02640438079834
Epoch 440, val loss: 0.8626046776771545
Epoch 450, training loss: 0.10290650278329849 = 0.03270701318979263 + 0.01 * 7.019949436187744
Epoch 450, val loss: 0.8748171925544739
Epoch 460, training loss: 0.10007227212190628 = 0.029897673055529594 + 0.01 * 7.017459869384766
Epoch 460, val loss: 0.8867629766464233
Epoch 470, training loss: 0.09749038517475128 = 0.027422655373811722 + 0.01 * 7.006772518157959
Epoch 470, val loss: 0.898463249206543
Epoch 480, training loss: 0.09527723491191864 = 0.025228817015886307 + 0.01 * 7.004842281341553
Epoch 480, val loss: 0.9098943471908569
Epoch 490, training loss: 0.09312895685434341 = 0.02327897399663925 + 0.01 * 6.98499870300293
Epoch 490, val loss: 0.9210688471794128
Epoch 500, training loss: 0.09130352735519409 = 0.021541547030210495 + 0.01 * 6.976198673248291
Epoch 500, val loss: 0.9319620728492737
Epoch 510, training loss: 0.08984678983688354 = 0.0199921652674675 + 0.01 * 6.985462665557861
Epoch 510, val loss: 0.9425410032272339
Epoch 520, training loss: 0.08817119151353836 = 0.018604567274451256 + 0.01 * 6.956662654876709
Epoch 520, val loss: 0.9527990221977234
Epoch 530, training loss: 0.08686543256044388 = 0.017356155440211296 + 0.01 * 6.950927734375
Epoch 530, val loss: 0.9628009796142578
Epoch 540, training loss: 0.08578537404537201 = 0.01622895523905754 + 0.01 * 6.955642223358154
Epoch 540, val loss: 0.9725499749183655
Epoch 550, training loss: 0.08473816514015198 = 0.01521228812634945 + 0.01 * 6.952587604522705
Epoch 550, val loss: 0.9820140600204468
Epoch 560, training loss: 0.08373162150382996 = 0.0142911896109581 + 0.01 * 6.944043159484863
Epoch 560, val loss: 0.9911224246025085
Epoch 570, training loss: 0.08256611227989197 = 0.013455327600240707 + 0.01 * 6.911077976226807
Epoch 570, val loss: 1.0000287294387817
Epoch 580, training loss: 0.08220917731523514 = 0.01269279420375824 + 0.01 * 6.951638221740723
Epoch 580, val loss: 1.0087080001831055
Epoch 590, training loss: 0.0812726765871048 = 0.011997217312455177 + 0.01 * 6.927546501159668
Epoch 590, val loss: 1.0170354843139648
Epoch 600, training loss: 0.08027328550815582 = 0.01136122178286314 + 0.01 * 6.891206741333008
Epoch 600, val loss: 1.0251470804214478
Epoch 610, training loss: 0.07970542460680008 = 0.010778621770441532 + 0.01 * 6.8926801681518555
Epoch 610, val loss: 1.0330079793930054
Epoch 620, training loss: 0.0789603739976883 = 0.010242331773042679 + 0.01 * 6.871804237365723
Epoch 620, val loss: 1.0405815839767456
Epoch 630, training loss: 0.07839207351207733 = 0.009746953845024109 + 0.01 * 6.8645124435424805
Epoch 630, val loss: 1.0480893850326538
Epoch 640, training loss: 0.07808329164981842 = 0.009290192276239395 + 0.01 * 6.879310131072998
Epoch 640, val loss: 1.0552701950073242
Epoch 650, training loss: 0.0774163082242012 = 0.008866800926625729 + 0.01 * 6.854950904846191
Epoch 650, val loss: 1.062302589416504
Epoch 660, training loss: 0.07695162296295166 = 0.00847452413290739 + 0.01 * 6.847710132598877
Epoch 660, val loss: 1.069101095199585
Epoch 670, training loss: 0.07656185328960419 = 0.00811035092920065 + 0.01 * 6.845149993896484
Epoch 670, val loss: 1.0758183002471924
Epoch 680, training loss: 0.07614278793334961 = 0.007770707365125418 + 0.01 * 6.837208271026611
Epoch 680, val loss: 1.0823332071304321
Epoch 690, training loss: 0.07587031275033951 = 0.007454029284417629 + 0.01 * 6.841628074645996
Epoch 690, val loss: 1.0885435342788696
Epoch 700, training loss: 0.07555609196424484 = 0.007158590015023947 + 0.01 * 6.839749813079834
Epoch 700, val loss: 1.0947092771530151
Epoch 710, training loss: 0.07512958347797394 = 0.006882170215249062 + 0.01 * 6.824741363525391
Epoch 710, val loss: 1.1007330417633057
Epoch 720, training loss: 0.07480210810899734 = 0.00662346463650465 + 0.01 * 6.817864418029785
Epoch 720, val loss: 1.1065263748168945
Epoch 730, training loss: 0.07438734918832779 = 0.0063802809454500675 + 0.01 * 6.80070686340332
Epoch 730, val loss: 1.1122310161590576
Epoch 740, training loss: 0.07426272332668304 = 0.006151643116027117 + 0.01 * 6.811107635498047
Epoch 740, val loss: 1.1178159713745117
Epoch 750, training loss: 0.07382809370756149 = 0.0059368242509663105 + 0.01 * 6.789127349853516
Epoch 750, val loss: 1.1232411861419678
Epoch 760, training loss: 0.07376544922590256 = 0.0057344078086316586 + 0.01 * 6.803104400634766
Epoch 760, val loss: 1.1284503936767578
Epoch 770, training loss: 0.07334833592176437 = 0.005543813109397888 + 0.01 * 6.780452251434326
Epoch 770, val loss: 1.133684754371643
Epoch 780, training loss: 0.07317379862070084 = 0.005363382399082184 + 0.01 * 6.781041622161865
Epoch 780, val loss: 1.1386643648147583
Epoch 790, training loss: 0.07294067740440369 = 0.0051932246424257755 + 0.01 * 6.774745464324951
Epoch 790, val loss: 1.143682599067688
Epoch 800, training loss: 0.07273056358098984 = 0.005031862296164036 + 0.01 * 6.769870281219482
Epoch 800, val loss: 1.1484766006469727
Epoch 810, training loss: 0.0727216973900795 = 0.004879191052168608 + 0.01 * 6.784250736236572
Epoch 810, val loss: 1.1532254219055176
Epoch 820, training loss: 0.07236064970493317 = 0.00473407469689846 + 0.01 * 6.762657165527344
Epoch 820, val loss: 1.157868504524231
Epoch 830, training loss: 0.07221676409244537 = 0.004596223589032888 + 0.01 * 6.762053966522217
Epoch 830, val loss: 1.1624702215194702
Epoch 840, training loss: 0.07206026464700699 = 0.004465268459171057 + 0.01 * 6.759500026702881
Epoch 840, val loss: 1.1668002605438232
Epoch 850, training loss: 0.07186776399612427 = 0.004340591840445995 + 0.01 * 6.7527174949646
Epoch 850, val loss: 1.171258568763733
Epoch 860, training loss: 0.0717434510588646 = 0.004221145994961262 + 0.01 * 6.752230167388916
Epoch 860, val loss: 1.1755588054656982
Epoch 870, training loss: 0.07157228887081146 = 0.004107643384486437 + 0.01 * 6.746464252471924
Epoch 870, val loss: 1.1798222064971924
Epoch 880, training loss: 0.07159220427274704 = 0.003999182488769293 + 0.01 * 6.759302616119385
Epoch 880, val loss: 1.1838326454162598
Epoch 890, training loss: 0.07130651921033859 = 0.0038958291988819838 + 0.01 * 6.741069316864014
Epoch 890, val loss: 1.1879198551177979
Epoch 900, training loss: 0.07155589014291763 = 0.0037976170424371958 + 0.01 * 6.775827407836914
Epoch 900, val loss: 1.191847801208496
Epoch 910, training loss: 0.07109799981117249 = 0.003703585360199213 + 0.01 * 6.739441394805908
Epoch 910, val loss: 1.1957604885101318
Epoch 920, training loss: 0.07086539268493652 = 0.003613546025007963 + 0.01 * 6.725184917449951
Epoch 920, val loss: 1.1995038986206055
Epoch 930, training loss: 0.07098114490509033 = 0.0035271011292934418 + 0.01 * 6.745404243469238
Epoch 930, val loss: 1.2032102346420288
Epoch 940, training loss: 0.07080912590026855 = 0.0034441486932337284 + 0.01 * 6.736497402191162
Epoch 940, val loss: 1.2069770097732544
Epoch 950, training loss: 0.07104013115167618 = 0.003364193020388484 + 0.01 * 6.767593860626221
Epoch 950, val loss: 1.2105342149734497
Epoch 960, training loss: 0.07046746462583542 = 0.0032883521635085344 + 0.01 * 6.717911720275879
Epoch 960, val loss: 1.2140560150146484
Epoch 970, training loss: 0.07078953832387924 = 0.0032155367080122232 + 0.01 * 6.757400035858154
Epoch 970, val loss: 1.217550277709961
Epoch 980, training loss: 0.07042080909013748 = 0.003145853290334344 + 0.01 * 6.7274956703186035
Epoch 980, val loss: 1.2208560705184937
Epoch 990, training loss: 0.07017286866903305 = 0.003078733803704381 + 0.01 * 6.709413528442383
Epoch 990, val loss: 1.2241896390914917
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.040443181991577 = 1.9544748067855835 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.958495020866394
Epoch 10, training loss: 2.030383586883545 = 1.944415807723999 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.9481910467147827
Epoch 20, training loss: 2.0179145336151123 = 1.931949496269226 + 0.01 * 8.596513748168945
Epoch 20, val loss: 1.9351686239242554
Epoch 30, training loss: 1.9999629259109497 = 1.9140089750289917 + 0.01 * 8.595396995544434
Epoch 30, val loss: 1.9163480997085571
Epoch 40, training loss: 1.9728623628616333 = 1.8869870901107788 + 0.01 * 8.587531089782715
Epoch 40, val loss: 1.8884153366088867
Epoch 50, training loss: 1.9339336156845093 = 1.848602294921875 + 0.01 * 8.533129692077637
Epoch 50, val loss: 1.8501685857772827
Epoch 60, training loss: 1.8867874145507812 = 1.8041964769363403 + 0.01 * 8.259088516235352
Epoch 60, val loss: 1.8092933893203735
Epoch 70, training loss: 1.8456978797912598 = 1.764740228652954 + 0.01 * 8.095766067504883
Epoch 70, val loss: 1.775382399559021
Epoch 80, training loss: 1.7975709438323975 = 1.7183640003204346 + 0.01 * 7.9207000732421875
Epoch 80, val loss: 1.7323111295700073
Epoch 90, training loss: 1.7313600778579712 = 1.6540405750274658 + 0.01 * 7.731954574584961
Epoch 90, val loss: 1.6735658645629883
Epoch 100, training loss: 1.6443654298782349 = 1.5685393810272217 + 0.01 * 7.582608222961426
Epoch 100, val loss: 1.5995351076126099
Epoch 110, training loss: 1.5398286581039429 = 1.4649088382720947 + 0.01 * 7.491982460021973
Epoch 110, val loss: 1.5106556415557861
Epoch 120, training loss: 1.42807936668396 = 1.3536756038665771 + 0.01 * 7.440377712249756
Epoch 120, val loss: 1.4180171489715576
Epoch 130, training loss: 1.3174504041671753 = 1.243619441986084 + 0.01 * 7.383094310760498
Epoch 130, val loss: 1.3285741806030273
Epoch 140, training loss: 1.213093638420105 = 1.1397528648376465 + 0.01 * 7.334076404571533
Epoch 140, val loss: 1.246018409729004
Epoch 150, training loss: 1.1167707443237305 = 1.0439025163650513 + 0.01 * 7.286822319030762
Epoch 150, val loss: 1.1710461378097534
Epoch 160, training loss: 1.028467059135437 = 0.9560938477516174 + 0.01 * 7.2373175621032715
Epoch 160, val loss: 1.1020879745483398
Epoch 170, training loss: 0.947529673576355 = 0.8755660057067871 + 0.01 * 7.196364879608154
Epoch 170, val loss: 1.0383293628692627
Epoch 180, training loss: 0.8735357522964478 = 0.8017715215682983 + 0.01 * 7.176423072814941
Epoch 180, val loss: 0.9796976447105408
Epoch 190, training loss: 0.8058896660804749 = 0.7342941761016846 + 0.01 * 7.159550189971924
Epoch 190, val loss: 0.9271993637084961
Epoch 200, training loss: 0.7438231110572815 = 0.6723620891571045 + 0.01 * 7.146101951599121
Epoch 200, val loss: 0.8819060921669006
Epoch 210, training loss: 0.6861969828605652 = 0.6148689985275269 + 0.01 * 7.132800579071045
Epoch 210, val loss: 0.844035267829895
Epoch 220, training loss: 0.6317648887634277 = 0.5605597496032715 + 0.01 * 7.120516777038574
Epoch 220, val loss: 0.8125805854797363
Epoch 230, training loss: 0.5797123312950134 = 0.5086477398872375 + 0.01 * 7.106461524963379
Epoch 230, val loss: 0.7867512106895447
Epoch 240, training loss: 0.5303800702095032 = 0.45940354466438293 + 0.01 * 7.097653388977051
Epoch 240, val loss: 0.7667606472969055
Epoch 250, training loss: 0.4844115078449249 = 0.4135729670524597 + 0.01 * 7.0838541984558105
Epoch 250, val loss: 0.7529208660125732
Epoch 260, training loss: 0.44238466024398804 = 0.3716776967048645 + 0.01 * 7.07069730758667
Epoch 260, val loss: 0.7449724078178406
Epoch 270, training loss: 0.4043993055820465 = 0.33379626274108887 + 0.01 * 7.060304164886475
Epoch 270, val loss: 0.7424724698066711
Epoch 280, training loss: 0.37007761001586914 = 0.2995872497558594 + 0.01 * 7.049036979675293
Epoch 280, val loss: 0.744436502456665
Epoch 290, training loss: 0.33889707922935486 = 0.2685386538505554 + 0.01 * 7.035843372344971
Epoch 290, val loss: 0.7498528957366943
Epoch 300, training loss: 0.3107551634311676 = 0.24019946157932281 + 0.01 * 7.055569648742676
Epoch 300, val loss: 0.758103609085083
Epoch 310, training loss: 0.2844970226287842 = 0.214310422539711 + 0.01 * 7.018660068511963
Epoch 310, val loss: 0.7686704993247986
Epoch 320, training loss: 0.2608066499233246 = 0.19070403277873993 + 0.01 * 7.010262966156006
Epoch 320, val loss: 0.7811833620071411
Epoch 330, training loss: 0.2393358200788498 = 0.1693008542060852 + 0.01 * 7.0034966468811035
Epoch 330, val loss: 0.7952984571456909
Epoch 340, training loss: 0.21998867392539978 = 0.15003710985183716 + 0.01 * 6.995156764984131
Epoch 340, val loss: 0.8107036352157593
Epoch 350, training loss: 0.20305489003658295 = 0.13282367587089539 + 0.01 * 7.023121356964111
Epoch 350, val loss: 0.8273423910140991
Epoch 360, training loss: 0.187520831823349 = 0.11757168918848038 + 0.01 * 6.994915008544922
Epoch 360, val loss: 0.844727098941803
Epoch 370, training loss: 0.1739267110824585 = 0.10410109907388687 + 0.01 * 6.9825615882873535
Epoch 370, val loss: 0.8627572059631348
Epoch 380, training loss: 0.16199469566345215 = 0.09223376214504242 + 0.01 * 6.976093769073486
Epoch 380, val loss: 0.881224513053894
Epoch 390, training loss: 0.15152767300605774 = 0.0818140059709549 + 0.01 * 6.9713664054870605
Epoch 390, val loss: 0.8999428153038025
Epoch 400, training loss: 0.14241139590740204 = 0.07269012182950974 + 0.01 * 6.972127437591553
Epoch 400, val loss: 0.9186974167823792
Epoch 410, training loss: 0.13440725207328796 = 0.06472330540418625 + 0.01 * 6.968395233154297
Epoch 410, val loss: 0.9372915625572205
Epoch 420, training loss: 0.12738388776779175 = 0.057771552354097366 + 0.01 * 6.961233615875244
Epoch 420, val loss: 0.9556964039802551
Epoch 430, training loss: 0.12130594998598099 = 0.051707297563552856 + 0.01 * 6.959865093231201
Epoch 430, val loss: 0.9737356901168823
Epoch 440, training loss: 0.11594259738922119 = 0.04640916362404823 + 0.01 * 6.953343391418457
Epoch 440, val loss: 0.9913756251335144
Epoch 450, training loss: 0.11134061217308044 = 0.041770365089178085 + 0.01 * 6.957025527954102
Epoch 450, val loss: 1.0084835290908813
Epoch 460, training loss: 0.10722829401493073 = 0.03770627826452255 + 0.01 * 6.952201843261719
Epoch 460, val loss: 1.0250723361968994
Epoch 470, training loss: 0.10354391485452652 = 0.034137070178985596 + 0.01 * 6.9406843185424805
Epoch 470, val loss: 1.0410125255584717
Epoch 480, training loss: 0.10039521753787994 = 0.03098800592124462 + 0.01 * 6.940721035003662
Epoch 480, val loss: 1.0564051866531372
Epoch 490, training loss: 0.09765537828207016 = 0.02819993533194065 + 0.01 * 6.945544719696045
Epoch 490, val loss: 1.0711240768432617
Epoch 500, training loss: 0.09503821283578873 = 0.02572414092719555 + 0.01 * 6.9314069747924805
Epoch 500, val loss: 1.0852587223052979
Epoch 510, training loss: 0.09277097135782242 = 0.023516042158007622 + 0.01 * 6.925492763519287
Epoch 510, val loss: 1.0988177061080933
Epoch 520, training loss: 0.09078705310821533 = 0.02154148742556572 + 0.01 * 6.924556255340576
Epoch 520, val loss: 1.11176335811615
Epoch 530, training loss: 0.08899523317813873 = 0.01977507956326008 + 0.01 * 6.92201566696167
Epoch 530, val loss: 1.1241662502288818
Epoch 540, training loss: 0.0873464047908783 = 0.018196608871221542 + 0.01 * 6.914979934692383
Epoch 540, val loss: 1.1360199451446533
Epoch 550, training loss: 0.08601035922765732 = 0.016784382984042168 + 0.01 * 6.922597408294678
Epoch 550, val loss: 1.1474034786224365
Epoch 560, training loss: 0.08463776111602783 = 0.015522787347435951 + 0.01 * 6.911497592926025
Epoch 560, val loss: 1.1582494974136353
Epoch 570, training loss: 0.08346527069807053 = 0.014393107034265995 + 0.01 * 6.907216548919678
Epoch 570, val loss: 1.1686782836914062
Epoch 580, training loss: 0.08238139748573303 = 0.013378433883190155 + 0.01 * 6.900296211242676
Epoch 580, val loss: 1.1786129474639893
Epoch 590, training loss: 0.08148175477981567 = 0.012466137297451496 + 0.01 * 6.901561737060547
Epoch 590, val loss: 1.1880924701690674
Epoch 600, training loss: 0.08054862171411514 = 0.011645177379250526 + 0.01 * 6.890345096588135
Epoch 600, val loss: 1.1972262859344482
Epoch 610, training loss: 0.07977328449487686 = 0.01090320385992527 + 0.01 * 6.8870086669921875
Epoch 610, val loss: 1.2060023546218872
Epoch 620, training loss: 0.07905460149049759 = 0.010231263935565948 + 0.01 * 6.882334232330322
Epoch 620, val loss: 1.2144845724105835
Epoch 630, training loss: 0.07844280451536179 = 0.00962211936712265 + 0.01 * 6.882068634033203
Epoch 630, val loss: 1.222554326057434
Epoch 640, training loss: 0.07782556116580963 = 0.009067808277904987 + 0.01 * 6.875775337219238
Epoch 640, val loss: 1.230310082435608
Epoch 650, training loss: 0.07728026807308197 = 0.008562655188143253 + 0.01 * 6.871761322021484
Epoch 650, val loss: 1.237789273262024
Epoch 660, training loss: 0.07676032930612564 = 0.00810079462826252 + 0.01 * 6.86595344543457
Epoch 660, val loss: 1.2450224161148071
Epoch 670, training loss: 0.07647287100553513 = 0.00767737440764904 + 0.01 * 6.879549980163574
Epoch 670, val loss: 1.251975178718567
Epoch 680, training loss: 0.07593555003404617 = 0.007287880871444941 + 0.01 * 6.864767074584961
Epoch 680, val loss: 1.2587121725082397
Epoch 690, training loss: 0.07556098699569702 = 0.006929587572813034 + 0.01 * 6.863139629364014
Epoch 690, val loss: 1.265204906463623
Epoch 700, training loss: 0.07505641877651215 = 0.006600614171475172 + 0.01 * 6.845580577850342
Epoch 700, val loss: 1.2714378833770752
Epoch 710, training loss: 0.07473035901784897 = 0.006296795327216387 + 0.01 * 6.843356132507324
Epoch 710, val loss: 1.2774713039398193
Epoch 720, training loss: 0.0743870884180069 = 0.006015213206410408 + 0.01 * 6.837187767028809
Epoch 720, val loss: 1.283370852470398
Epoch 730, training loss: 0.07413135468959808 = 0.005753828678280115 + 0.01 * 6.837752819061279
Epoch 730, val loss: 1.2890278100967407
Epoch 740, training loss: 0.07389948517084122 = 0.00551035488024354 + 0.01 * 6.838913440704346
Epoch 740, val loss: 1.2944731712341309
Epoch 750, training loss: 0.07370110601186752 = 0.005283891689032316 + 0.01 * 6.841721534729004
Epoch 750, val loss: 1.299823522567749
Epoch 760, training loss: 0.07329493016004562 = 0.00507265143096447 + 0.01 * 6.822227954864502
Epoch 760, val loss: 1.3048633337020874
Epoch 770, training loss: 0.07301410287618637 = 0.0048753004521131516 + 0.01 * 6.813880443572998
Epoch 770, val loss: 1.3098500967025757
Epoch 780, training loss: 0.07282889634370804 = 0.004690973553806543 + 0.01 * 6.8137922286987305
Epoch 780, val loss: 1.3147180080413818
Epoch 790, training loss: 0.07257357984781265 = 0.004518269095569849 + 0.01 * 6.805531024932861
Epoch 790, val loss: 1.3192986249923706
Epoch 800, training loss: 0.07260115444660187 = 0.0043561565689742565 + 0.01 * 6.82450008392334
Epoch 800, val loss: 1.3238295316696167
Epoch 810, training loss: 0.07239918410778046 = 0.004204204306006432 + 0.01 * 6.819497585296631
Epoch 810, val loss: 1.328140139579773
Epoch 820, training loss: 0.0719984695315361 = 0.004061663523316383 + 0.01 * 6.7936811447143555
Epoch 820, val loss: 1.332350254058838
Epoch 830, training loss: 0.07187343388795853 = 0.003927129320800304 + 0.01 * 6.794630527496338
Epoch 830, val loss: 1.336517095565796
Epoch 840, training loss: 0.07175637781620026 = 0.0038005411624908447 + 0.01 * 6.795583724975586
Epoch 840, val loss: 1.3403517007827759
Epoch 850, training loss: 0.07152824103832245 = 0.0036809793673455715 + 0.01 * 6.784726142883301
Epoch 850, val loss: 1.3442116975784302
Epoch 860, training loss: 0.07146172970533371 = 0.0035679966676980257 + 0.01 * 6.789373397827148
Epoch 860, val loss: 1.348006248474121
Epoch 870, training loss: 0.07131310552358627 = 0.003461061976850033 + 0.01 * 6.7852044105529785
Epoch 870, val loss: 1.3515748977661133
Epoch 880, training loss: 0.07117301225662231 = 0.0033600053284317255 + 0.01 * 6.781301021575928
Epoch 880, val loss: 1.3551236391067505
Epoch 890, training loss: 0.07114776968955994 = 0.003264302620664239 + 0.01 * 6.788346767425537
Epoch 890, val loss: 1.3584977388381958
Epoch 900, training loss: 0.07082696259021759 = 0.0031733468640595675 + 0.01 * 6.76536226272583
Epoch 900, val loss: 1.3618639707565308
Epoch 910, training loss: 0.07097381353378296 = 0.003086753888055682 + 0.01 * 6.788706302642822
Epoch 910, val loss: 1.3650999069213867
Epoch 920, training loss: 0.07064028084278107 = 0.003004663623869419 + 0.01 * 6.763562202453613
Epoch 920, val loss: 1.3681628704071045
Epoch 930, training loss: 0.07078084349632263 = 0.00292629050090909 + 0.01 * 6.785455226898193
Epoch 930, val loss: 1.3712750673294067
Epoch 940, training loss: 0.070602647960186 = 0.002852321369573474 + 0.01 * 6.7750325202941895
Epoch 940, val loss: 1.3741858005523682
Epoch 950, training loss: 0.07032626867294312 = 0.0027813331689685583 + 0.01 * 6.754493713378906
Epoch 950, val loss: 1.3770079612731934
Epoch 960, training loss: 0.07023485749959946 = 0.0027137643191963434 + 0.01 * 6.752109527587891
Epoch 960, val loss: 1.3798562288284302
Epoch 970, training loss: 0.07015713304281235 = 0.0026495640631765127 + 0.01 * 6.750757694244385
Epoch 970, val loss: 1.3824918270111084
Epoch 980, training loss: 0.07032769173383713 = 0.0025879009626805782 + 0.01 * 6.773979663848877
Epoch 980, val loss: 1.3850929737091064
Epoch 990, training loss: 0.07013556361198425 = 0.002529134741052985 + 0.01 * 6.760643005371094
Epoch 990, val loss: 1.3876261711120605
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8386926726410122
The final CL Acc:0.80617, 0.01062, The final GNN Acc:0.83904, 0.00050
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10506])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.045440435409546 = 1.9594721794128418 + 0.01 * 8.596837043762207
Epoch 0, val loss: 1.9554284811019897
Epoch 10, training loss: 2.03538179397583 = 1.9494141340255737 + 0.01 * 8.596773147583008
Epoch 10, val loss: 1.945684790611267
Epoch 20, training loss: 2.0227973461151123 = 1.9368314743041992 + 0.01 * 8.596582412719727
Epoch 20, val loss: 1.9331728219985962
Epoch 30, training loss: 2.004875898361206 = 1.9189164638519287 + 0.01 * 8.595953941345215
Epoch 30, val loss: 1.9149165153503418
Epoch 40, training loss: 1.9779384136199951 = 1.892014503479004 + 0.01 * 8.592385292053223
Epoch 40, val loss: 1.8874926567077637
Epoch 50, training loss: 1.9397618770599365 = 1.8540966510772705 + 0.01 * 8.566522598266602
Epoch 50, val loss: 1.8501017093658447
Epoch 60, training loss: 1.8972344398498535 = 1.8126952648162842 + 0.01 * 8.453913688659668
Epoch 60, val loss: 1.8136231899261475
Epoch 70, training loss: 1.864508867263794 = 1.7826192378997803 + 0.01 * 8.188962936401367
Epoch 70, val loss: 1.791260838508606
Epoch 80, training loss: 1.8286185264587402 = 1.748393177986145 + 0.01 * 8.02253532409668
Epoch 80, val loss: 1.761710286140442
Epoch 90, training loss: 1.7779483795166016 = 1.7007652521133423 + 0.01 * 7.718310832977295
Epoch 90, val loss: 1.7201532125473022
Epoch 100, training loss: 1.7090469598770142 = 1.6341913938522339 + 0.01 * 7.485556125640869
Epoch 100, val loss: 1.6645622253417969
Epoch 110, training loss: 1.6228359937667847 = 1.5489450693130493 + 0.01 * 7.389092445373535
Epoch 110, val loss: 1.5945435762405396
Epoch 120, training loss: 1.5270771980285645 = 1.4536808729171753 + 0.01 * 7.339638710021973
Epoch 120, val loss: 1.5182065963745117
Epoch 130, training loss: 1.4291412830352783 = 1.3560532331466675 + 0.01 * 7.3087992668151855
Epoch 130, val loss: 1.4419232606887817
Epoch 140, training loss: 1.3297842741012573 = 1.2568734884262085 + 0.01 * 7.291073322296143
Epoch 140, val loss: 1.3660995960235596
Epoch 150, training loss: 1.2276469469070435 = 1.1549466848373413 + 0.01 * 7.270031452178955
Epoch 150, val loss: 1.288969874382019
Epoch 160, training loss: 1.1250172853469849 = 1.052500605583191 + 0.01 * 7.2516703605651855
Epoch 160, val loss: 1.2120847702026367
Epoch 170, training loss: 1.0264427661895752 = 0.9540653228759766 + 0.01 * 7.237748622894287
Epoch 170, val loss: 1.1389472484588623
Epoch 180, training loss: 0.9356421232223511 = 0.8633963465690613 + 0.01 * 7.2245774269104
Epoch 180, val loss: 1.0724446773529053
Epoch 190, training loss: 0.8543804287910461 = 0.7823063731193542 + 0.01 * 7.207407474517822
Epoch 190, val loss: 1.0142865180969238
Epoch 200, training loss: 0.7826290726661682 = 0.7107997536659241 + 0.01 * 7.1829328536987305
Epoch 200, val loss: 0.9658992290496826
Epoch 210, training loss: 0.7191712856292725 = 0.6476163268089294 + 0.01 * 7.155497074127197
Epoch 210, val loss: 0.9266758561134338
Epoch 220, training loss: 0.6624796986579895 = 0.5912085175514221 + 0.01 * 7.127119541168213
Epoch 220, val loss: 0.8958196043968201
Epoch 230, training loss: 0.6113690733909607 = 0.5403526425361633 + 0.01 * 7.101642608642578
Epoch 230, val loss: 0.8716347217559814
Epoch 240, training loss: 0.5648440718650818 = 0.49394652247428894 + 0.01 * 7.089752674102783
Epoch 240, val loss: 0.8527923226356506
Epoch 250, training loss: 0.5215277075767517 = 0.450759619474411 + 0.01 * 7.076810836791992
Epoch 250, val loss: 0.8380458950996399
Epoch 260, training loss: 0.4802328646183014 = 0.4095059335231781 + 0.01 * 7.072693824768066
Epoch 260, val loss: 0.8261112570762634
Epoch 270, training loss: 0.43985992670059204 = 0.36917415261268616 + 0.01 * 7.068576812744141
Epoch 270, val loss: 0.8159632682800293
Epoch 280, training loss: 0.3999204635620117 = 0.329272598028183 + 0.01 * 7.064786434173584
Epoch 280, val loss: 0.8069247603416443
Epoch 290, training loss: 0.36050111055374146 = 0.2898882031440735 + 0.01 * 7.0612897872924805
Epoch 290, val loss: 0.7987042665481567
Epoch 300, training loss: 0.322238028049469 = 0.2516704201698303 + 0.01 * 7.056760311126709
Epoch 300, val loss: 0.7915549874305725
Epoch 310, training loss: 0.28624260425567627 = 0.21570537984371185 + 0.01 * 7.0537238121032715
Epoch 310, val loss: 0.7861130237579346
Epoch 320, training loss: 0.25373443961143494 = 0.18320974707603455 + 0.01 * 7.052468776702881
Epoch 320, val loss: 0.7831883430480957
Epoch 330, training loss: 0.22549563646316528 = 0.15500794351100922 + 0.01 * 7.048769474029541
Epoch 330, val loss: 0.7833781838417053
Epoch 340, training loss: 0.20187464356422424 = 0.13127505779266357 + 0.01 * 7.059957981109619
Epoch 340, val loss: 0.7869951725006104
Epoch 350, training loss: 0.18213006854057312 = 0.11166049540042877 + 0.01 * 7.046957015991211
Epoch 350, val loss: 0.7939248085021973
Epoch 360, training loss: 0.16592097282409668 = 0.09551529586315155 + 0.01 * 7.0405683517456055
Epoch 360, val loss: 0.8036743402481079
Epoch 370, training loss: 0.15255559980869293 = 0.08218653500080109 + 0.01 * 7.036906719207764
Epoch 370, val loss: 0.8157519698143005
Epoch 380, training loss: 0.14147020876407623 = 0.07112208008766174 + 0.01 * 7.034812927246094
Epoch 380, val loss: 0.8295559883117676
Epoch 390, training loss: 0.132186621427536 = 0.061895951628685 + 0.01 * 7.029067516326904
Epoch 390, val loss: 0.8445983529090881
Epoch 400, training loss: 0.12443035840988159 = 0.054176751524209976 + 0.01 * 7.025361061096191
Epoch 400, val loss: 0.8604810237884521
Epoch 410, training loss: 0.11790639907121658 = 0.047704584896564484 + 0.01 * 7.020181655883789
Epoch 410, val loss: 0.8767892718315125
Epoch 420, training loss: 0.11238626390695572 = 0.042257294058799744 + 0.01 * 7.01289701461792
Epoch 420, val loss: 0.893242359161377
Epoch 430, training loss: 0.1076958179473877 = 0.03764806315302849 + 0.01 * 7.0047760009765625
Epoch 430, val loss: 0.909598708152771
Epoch 440, training loss: 0.10372084379196167 = 0.03372662886977196 + 0.01 * 6.9994215965271
Epoch 440, val loss: 0.9257515668869019
Epoch 450, training loss: 0.10042693465948105 = 0.03037881851196289 + 0.01 * 7.004811763763428
Epoch 450, val loss: 0.9415190815925598
Epoch 460, training loss: 0.09738887846469879 = 0.02750452235341072 + 0.01 * 6.988436222076416
Epoch 460, val loss: 0.956825852394104
Epoch 470, training loss: 0.09513765573501587 = 0.025019271299242973 + 0.01 * 7.011838436126709
Epoch 470, val loss: 0.9716734886169434
Epoch 480, training loss: 0.09263540804386139 = 0.022861719131469727 + 0.01 * 6.9773688316345215
Epoch 480, val loss: 0.9860497117042542
Epoch 490, training loss: 0.09060270339250565 = 0.020974621176719666 + 0.01 * 6.962808132171631
Epoch 490, val loss: 0.9999217987060547
Epoch 500, training loss: 0.0888972207903862 = 0.019312851130962372 + 0.01 * 6.958437442779541
Epoch 500, val loss: 1.0134096145629883
Epoch 510, training loss: 0.08734117448329926 = 0.017843393608927727 + 0.01 * 6.949778079986572
Epoch 510, val loss: 1.0264441967010498
Epoch 520, training loss: 0.08597155660390854 = 0.016540104523301125 + 0.01 * 6.943145751953125
Epoch 520, val loss: 1.0390772819519043
Epoch 530, training loss: 0.08483307808637619 = 0.015377054922282696 + 0.01 * 6.9456024169921875
Epoch 530, val loss: 1.0513410568237305
Epoch 540, training loss: 0.0837230384349823 = 0.014335822314023972 + 0.01 * 6.938722133636475
Epoch 540, val loss: 1.063265085220337
Epoch 550, training loss: 0.08273384720087051 = 0.013402141630649567 + 0.01 * 6.933170795440674
Epoch 550, val loss: 1.074766993522644
Epoch 560, training loss: 0.08184199035167694 = 0.012560507282614708 + 0.01 * 6.92814826965332
Epoch 560, val loss: 1.0858972072601318
Epoch 570, training loss: 0.08097077161073685 = 0.011799256317317486 + 0.01 * 6.917151927947998
Epoch 570, val loss: 1.096717119216919
Epoch 580, training loss: 0.08039311319589615 = 0.01110847294330597 + 0.01 * 6.928463935852051
Epoch 580, val loss: 1.10712468624115
Epoch 590, training loss: 0.07965237647294998 = 0.010480598546564579 + 0.01 * 6.917178153991699
Epoch 590, val loss: 1.1173259019851685
Epoch 600, training loss: 0.07895224541425705 = 0.00990730244666338 + 0.01 * 6.904493808746338
Epoch 600, val loss: 1.1271843910217285
Epoch 610, training loss: 0.07867632061243057 = 0.009382033720612526 + 0.01 * 6.929429054260254
Epoch 610, val loss: 1.1367379426956177
Epoch 620, training loss: 0.07793611288070679 = 0.008901339024305344 + 0.01 * 6.903477191925049
Epoch 620, val loss: 1.146087646484375
Epoch 630, training loss: 0.07742513716220856 = 0.008459635078907013 + 0.01 * 6.89655065536499
Epoch 630, val loss: 1.155060052871704
Epoch 640, training loss: 0.07695475965738297 = 0.00805197935551405 + 0.01 * 6.8902788162231445
Epoch 640, val loss: 1.163872241973877
Epoch 650, training loss: 0.07678307592868805 = 0.0076750158332288265 + 0.01 * 6.910806655883789
Epoch 650, val loss: 1.1723921298980713
Epoch 660, training loss: 0.07626190781593323 = 0.007327088620513678 + 0.01 * 6.893482208251953
Epoch 660, val loss: 1.1807674169540405
Epoch 670, training loss: 0.07586205005645752 = 0.007005096413195133 + 0.01 * 6.885695934295654
Epoch 670, val loss: 1.188763976097107
Epoch 680, training loss: 0.07553373277187347 = 0.006705373525619507 + 0.01 * 6.88283634185791
Epoch 680, val loss: 1.196688175201416
Epoch 690, training loss: 0.07518076151609421 = 0.006426363252103329 + 0.01 * 6.875439643859863
Epoch 690, val loss: 1.2043448686599731
Epoch 700, training loss: 0.07497318089008331 = 0.00616604695096612 + 0.01 * 6.88071346282959
Epoch 700, val loss: 1.2118451595306396
Epoch 710, training loss: 0.07468461245298386 = 0.005922909360378981 + 0.01 * 6.876171112060547
Epoch 710, val loss: 1.219180703163147
Epoch 720, training loss: 0.07448025792837143 = 0.005695751868188381 + 0.01 * 6.878450393676758
Epoch 720, val loss: 1.2262463569641113
Epoch 730, training loss: 0.07439296692609787 = 0.005483432672917843 + 0.01 * 6.890953540802002
Epoch 730, val loss: 1.233132243156433
Epoch 740, training loss: 0.07387851923704147 = 0.00528482673689723 + 0.01 * 6.85936975479126
Epoch 740, val loss: 1.2398539781570435
Epoch 750, training loss: 0.07368157804012299 = 0.005097938235849142 + 0.01 * 6.858363628387451
Epoch 750, val loss: 1.2464936971664429
Epoch 760, training loss: 0.07353175431489944 = 0.004921764601022005 + 0.01 * 6.86099910736084
Epoch 760, val loss: 1.2529181241989136
Epoch 770, training loss: 0.07349451631307602 = 0.004755827132612467 + 0.01 * 6.873868942260742
Epoch 770, val loss: 1.259214162826538
Epoch 780, training loss: 0.07320766896009445 = 0.004599696956574917 + 0.01 * 6.860796928405762
Epoch 780, val loss: 1.2653838396072388
Epoch 790, training loss: 0.07298480719327927 = 0.004452168475836515 + 0.01 * 6.853264331817627
Epoch 790, val loss: 1.2713309526443481
Epoch 800, training loss: 0.07274779677391052 = 0.00431278208270669 + 0.01 * 6.843501567840576
Epoch 800, val loss: 1.2772387266159058
Epoch 810, training loss: 0.07278455793857574 = 0.004180889576673508 + 0.01 * 6.860367298126221
Epoch 810, val loss: 1.2829644680023193
Epoch 820, training loss: 0.07261326164007187 = 0.004056079313158989 + 0.01 * 6.85571813583374
Epoch 820, val loss: 1.2885961532592773
Epoch 830, training loss: 0.0722823441028595 = 0.003938000649213791 + 0.01 * 6.834434509277344
Epoch 830, val loss: 1.2940568923950195
Epoch 840, training loss: 0.07231627404689789 = 0.003825690597295761 + 0.01 * 6.849059104919434
Epoch 840, val loss: 1.2992428541183472
Epoch 850, training loss: 0.07199395447969437 = 0.0037196907214820385 + 0.01 * 6.827426910400391
Epoch 850, val loss: 1.3046987056732178
Epoch 860, training loss: 0.07209031283855438 = 0.003618744434788823 + 0.01 * 6.847157001495361
Epoch 860, val loss: 1.3096340894699097
Epoch 870, training loss: 0.07191038131713867 = 0.0035228391643613577 + 0.01 * 6.838754653930664
Epoch 870, val loss: 1.3147261142730713
Epoch 880, training loss: 0.07172922044992447 = 0.003431598888710141 + 0.01 * 6.829762935638428
Epoch 880, val loss: 1.3195911645889282
Epoch 890, training loss: 0.07155582308769226 = 0.0033443698193877935 + 0.01 * 6.821145534515381
Epoch 890, val loss: 1.324351191520691
Epoch 900, training loss: 0.07152708619832993 = 0.003261258825659752 + 0.01 * 6.826582908630371
Epoch 900, val loss: 1.3290932178497314
Epoch 910, training loss: 0.07133086025714874 = 0.0031818447168916464 + 0.01 * 6.814901828765869
Epoch 910, val loss: 1.3336747884750366
Epoch 920, training loss: 0.07121863961219788 = 0.0031059233006089926 + 0.01 * 6.811271667480469
Epoch 920, val loss: 1.3381651639938354
Epoch 930, training loss: 0.07119651138782501 = 0.003033284330740571 + 0.01 * 6.816322326660156
Epoch 930, val loss: 1.3426891565322876
Epoch 940, training loss: 0.07111898809671402 = 0.0029639066196978092 + 0.01 * 6.8155083656311035
Epoch 940, val loss: 1.3469213247299194
Epoch 950, training loss: 0.0710870772600174 = 0.002897643018513918 + 0.01 * 6.818943977355957
Epoch 950, val loss: 1.3512645959854126
Epoch 960, training loss: 0.07100369036197662 = 0.002834095386788249 + 0.01 * 6.816959857940674
Epoch 960, val loss: 1.3553123474121094
Epoch 970, training loss: 0.07090606540441513 = 0.0027733820024877787 + 0.01 * 6.813268661499023
Epoch 970, val loss: 1.359574317932129
Epoch 980, training loss: 0.07072947174310684 = 0.0027149259112775326 + 0.01 * 6.801454544067383
Epoch 980, val loss: 1.363444447517395
Epoch 990, training loss: 0.07051975280046463 = 0.0026589760091155767 + 0.01 * 6.786077976226807
Epoch 990, val loss: 1.367349624633789
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 2.018401622772217 = 1.9324332475662231 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9323443174362183
Epoch 10, training loss: 2.009472131729126 = 1.92350435256958 + 0.01 * 8.596773147583008
Epoch 10, val loss: 1.9228836297988892
Epoch 20, training loss: 1.9986211061477661 = 1.9126554727554321 + 0.01 * 8.596559524536133
Epoch 20, val loss: 1.9112119674682617
Epoch 30, training loss: 1.9834458827972412 = 1.897486925125122 + 0.01 * 8.595894813537598
Epoch 30, val loss: 1.8948626518249512
Epoch 40, training loss: 1.9610636234283447 = 1.8751399517059326 + 0.01 * 8.592364311218262
Epoch 40, val loss: 1.870890736579895
Epoch 50, training loss: 1.9296083450317383 = 1.8439404964447021 + 0.01 * 8.566780090332031
Epoch 50, val loss: 1.8385255336761475
Epoch 60, training loss: 1.8926897048950195 = 1.808593988418579 + 0.01 * 8.409567832946777
Epoch 60, val loss: 1.8054015636444092
Epoch 70, training loss: 1.8560494184494019 = 1.7752481698989868 + 0.01 * 8.08012580871582
Epoch 70, val loss: 1.7785370349884033
Epoch 80, training loss: 1.8118153810501099 = 1.733781337738037 + 0.01 * 7.803401947021484
Epoch 80, val loss: 1.7444915771484375
Epoch 90, training loss: 1.7513667345046997 = 1.6760801076889038 + 0.01 * 7.528665542602539
Epoch 90, val loss: 1.694031834602356
Epoch 100, training loss: 1.6723638772964478 = 1.5985827445983887 + 0.01 * 7.378109931945801
Epoch 100, val loss: 1.627569317817688
Epoch 110, training loss: 1.577836513519287 = 1.5046212673187256 + 0.01 * 7.321528911590576
Epoch 110, val loss: 1.5507831573486328
Epoch 120, training loss: 1.474677324295044 = 1.401681661605835 + 0.01 * 7.2995686531066895
Epoch 120, val loss: 1.4706270694732666
Epoch 130, training loss: 1.368436574935913 = 1.2956821918487549 + 0.01 * 7.275441646575928
Epoch 130, val loss: 1.3918001651763916
Epoch 140, training loss: 1.263027548789978 = 1.1904921531677246 + 0.01 * 7.253536224365234
Epoch 140, val loss: 1.3165075778961182
Epoch 150, training loss: 1.1632118225097656 = 1.0908104181289673 + 0.01 * 7.240137577056885
Epoch 150, val loss: 1.2478262186050415
Epoch 160, training loss: 1.0717570781707764 = 0.9994575381278992 + 0.01 * 7.229959487915039
Epoch 160, val loss: 1.1860102415084839
Epoch 170, training loss: 0.9880883097648621 = 0.9159209728240967 + 0.01 * 7.216732025146484
Epoch 170, val loss: 1.1299179792404175
Epoch 180, training loss: 0.9098489284515381 = 0.8378827571868896 + 0.01 * 7.196616172790527
Epoch 180, val loss: 1.077591896057129
Epoch 190, training loss: 0.8354198932647705 = 0.76372891664505 + 0.01 * 7.169095993041992
Epoch 190, val loss: 1.0283578634262085
Epoch 200, training loss: 0.7647881507873535 = 0.6934002637863159 + 0.01 * 7.138787269592285
Epoch 200, val loss: 0.9826732277870178
Epoch 210, training loss: 0.6988612413406372 = 0.627692699432373 + 0.01 * 7.116855621337891
Epoch 210, val loss: 0.9420689940452576
Epoch 220, training loss: 0.6379619836807251 = 0.5669286847114563 + 0.01 * 7.103330135345459
Epoch 220, val loss: 0.9077197909355164
Epoch 230, training loss: 0.5815384984016418 = 0.5105800032615662 + 0.01 * 7.095849990844727
Epoch 230, val loss: 0.8791682720184326
Epoch 240, training loss: 0.5285813212394714 = 0.4576445519924164 + 0.01 * 7.0936760902404785
Epoch 240, val loss: 0.8549323081970215
Epoch 250, training loss: 0.4779970347881317 = 0.4070869982242584 + 0.01 * 7.091003894805908
Epoch 250, val loss: 0.8337929248809814
Epoch 260, training loss: 0.4293980598449707 = 0.35849618911743164 + 0.01 * 7.090187072753906
Epoch 260, val loss: 0.8151273131370544
Epoch 270, training loss: 0.3832695484161377 = 0.3123660683631897 + 0.01 * 7.090347766876221
Epoch 270, val loss: 0.7994375228881836
Epoch 280, training loss: 0.34071964025497437 = 0.26982107758522034 + 0.01 * 7.089858055114746
Epoch 280, val loss: 0.7874346375465393
Epoch 290, training loss: 0.30283963680267334 = 0.23194627463817596 + 0.01 * 7.089337348937988
Epoch 290, val loss: 0.7795650959014893
Epoch 300, training loss: 0.2701246440410614 = 0.19923293590545654 + 0.01 * 7.089170455932617
Epoch 300, val loss: 0.7759224772453308
Epoch 310, training loss: 0.24243968725204468 = 0.17153891921043396 + 0.01 * 7.090075969696045
Epoch 310, val loss: 0.7764125466346741
Epoch 320, training loss: 0.21917206048965454 = 0.1482873111963272 + 0.01 * 7.088474750518799
Epoch 320, val loss: 0.780597448348999
Epoch 330, training loss: 0.19964328408241272 = 0.12876881659030914 + 0.01 * 7.087446212768555
Epoch 330, val loss: 0.7878491282463074
Epoch 340, training loss: 0.183254212141037 = 0.11232248693704605 + 0.01 * 7.0931715965271
Epoch 340, val loss: 0.7975357174873352
Epoch 350, training loss: 0.16927078366279602 = 0.09841450303792953 + 0.01 * 7.085628509521484
Epoch 350, val loss: 0.8091236352920532
Epoch 360, training loss: 0.15743426978588104 = 0.0865929052233696 + 0.01 * 7.084136962890625
Epoch 360, val loss: 0.8220886588096619
Epoch 370, training loss: 0.1473216414451599 = 0.07649841904640198 + 0.01 * 7.082321643829346
Epoch 370, val loss: 0.8360289335250854
Epoch 380, training loss: 0.13864636421203613 = 0.06784217804670334 + 0.01 * 7.080418586730957
Epoch 380, val loss: 0.8506308197975159
Epoch 390, training loss: 0.13115929067134857 = 0.060379091650247574 + 0.01 * 7.078019618988037
Epoch 390, val loss: 0.8656554222106934
Epoch 400, training loss: 0.1246853694319725 = 0.0539264902472496 + 0.01 * 7.075888156890869
Epoch 400, val loss: 0.8809313178062439
Epoch 410, training loss: 0.11909405142068863 = 0.04833722859621048 + 0.01 * 7.075682640075684
Epoch 410, val loss: 0.8962560296058655
Epoch 420, training loss: 0.11418088525533676 = 0.04348638653755188 + 0.01 * 7.0694499015808105
Epoch 420, val loss: 0.911447286605835
Epoch 430, training loss: 0.10992750525474548 = 0.03926337882876396 + 0.01 * 7.066413402557373
Epoch 430, val loss: 0.9264879822731018
Epoch 440, training loss: 0.10628476738929749 = 0.03557741641998291 + 0.01 * 7.070734977722168
Epoch 440, val loss: 0.9413254857063293
Epoch 450, training loss: 0.10295302420854568 = 0.032352231442928314 + 0.01 * 7.060079574584961
Epoch 450, val loss: 0.9558557868003845
Epoch 460, training loss: 0.10004556179046631 = 0.0295186135917902 + 0.01 * 7.052695274353027
Epoch 460, val loss: 0.9700215458869934
Epoch 470, training loss: 0.09751000255346298 = 0.027022594586014748 + 0.01 * 7.048740863800049
Epoch 470, val loss: 0.9838425517082214
Epoch 480, training loss: 0.09529459476470947 = 0.024824338033795357 + 0.01 * 7.047025680541992
Epoch 480, val loss: 0.9972027540206909
Epoch 490, training loss: 0.09324363619089127 = 0.022875936701893806 + 0.01 * 7.036770343780518
Epoch 490, val loss: 1.0101914405822754
Epoch 500, training loss: 0.09145109355449677 = 0.02114177867770195 + 0.01 * 7.0309319496154785
Epoch 500, val loss: 1.022794246673584
Epoch 510, training loss: 0.08985859155654907 = 0.019594719633460045 + 0.01 * 7.0263872146606445
Epoch 510, val loss: 1.0350453853607178
Epoch 520, training loss: 0.08831902593374252 = 0.018212661147117615 + 0.01 * 7.010636806488037
Epoch 520, val loss: 1.0469045639038086
Epoch 530, training loss: 0.08715458959341049 = 0.016974138095974922 + 0.01 * 7.018045425415039
Epoch 530, val loss: 1.0584040880203247
Epoch 540, training loss: 0.0860612764954567 = 0.015861408784985542 + 0.01 * 7.019987106323242
Epoch 540, val loss: 1.0694220066070557
Epoch 550, training loss: 0.08499883115291595 = 0.014858869835734367 + 0.01 * 7.013996124267578
Epoch 550, val loss: 1.0801560878753662
Epoch 560, training loss: 0.08416267484426498 = 0.013952836394309998 + 0.01 * 7.020983695983887
Epoch 560, val loss: 1.0904521942138672
Epoch 570, training loss: 0.08284981548786163 = 0.013132236897945404 + 0.01 * 6.9717583656311035
Epoch 570, val loss: 1.1004782915115356
Epoch 580, training loss: 0.08201998472213745 = 0.012386367656290531 + 0.01 * 6.963362216949463
Epoch 580, val loss: 1.1100958585739136
Epoch 590, training loss: 0.08118126541376114 = 0.011706853285431862 + 0.01 * 6.947441577911377
Epoch 590, val loss: 1.119484543800354
Epoch 600, training loss: 0.08050298690795898 = 0.011085034348070621 + 0.01 * 6.941795349121094
Epoch 600, val loss: 1.1286132335662842
Epoch 610, training loss: 0.07992424070835114 = 0.010514738969504833 + 0.01 * 6.940950870513916
Epoch 610, val loss: 1.1373602151870728
Epoch 620, training loss: 0.07919912785291672 = 0.009990119375288486 + 0.01 * 6.920901298522949
Epoch 620, val loss: 1.1458464860916138
Epoch 630, training loss: 0.07865668833255768 = 0.009506378322839737 + 0.01 * 6.9150309562683105
Epoch 630, val loss: 1.1541540622711182
Epoch 640, training loss: 0.07818365842103958 = 0.009058200754225254 + 0.01 * 6.912546157836914
Epoch 640, val loss: 1.1622525453567505
Epoch 650, training loss: 0.07803936302661896 = 0.00864284299314022 + 0.01 * 6.939651966094971
Epoch 650, val loss: 1.1700739860534668
Epoch 660, training loss: 0.07725255936384201 = 0.008256690576672554 + 0.01 * 6.899587154388428
Epoch 660, val loss: 1.177802324295044
Epoch 670, training loss: 0.07700135558843613 = 0.007896514609456062 + 0.01 * 6.910484790802002
Epoch 670, val loss: 1.1853498220443726
Epoch 680, training loss: 0.07659144699573517 = 0.007560369558632374 + 0.01 * 6.903107643127441
Epoch 680, val loss: 1.192633867263794
Epoch 690, training loss: 0.07621823996305466 = 0.007246408145874739 + 0.01 * 6.897183895111084
Epoch 690, val loss: 1.1997900009155273
Epoch 700, training loss: 0.0762740820646286 = 0.006952647119760513 + 0.01 * 6.932143688201904
Epoch 700, val loss: 1.206652283668518
Epoch 710, training loss: 0.07553534209728241 = 0.006677271332591772 + 0.01 * 6.885807037353516
Epoch 710, val loss: 1.2135632038116455
Epoch 720, training loss: 0.07504167407751083 = 0.006418921984732151 + 0.01 * 6.86227560043335
Epoch 720, val loss: 1.2201265096664429
Epoch 730, training loss: 0.07493828237056732 = 0.006176564376801252 + 0.01 * 6.8761725425720215
Epoch 730, val loss: 1.2265067100524902
Epoch 740, training loss: 0.07456032186746597 = 0.005948629695922136 + 0.01 * 6.861169338226318
Epoch 740, val loss: 1.2328836917877197
Epoch 750, training loss: 0.0744895488023758 = 0.005734425038099289 + 0.01 * 6.875512599945068
Epoch 750, val loss: 1.2388665676116943
Epoch 760, training loss: 0.07421563565731049 = 0.005532712675631046 + 0.01 * 6.868292331695557
Epoch 760, val loss: 1.2449158430099487
Epoch 770, training loss: 0.07407516986131668 = 0.005341981537640095 + 0.01 * 6.873318672180176
Epoch 770, val loss: 1.2507902383804321
Epoch 780, training loss: 0.07393211871385574 = 0.005162627901881933 + 0.01 * 6.876949310302734
Epoch 780, val loss: 1.256397008895874
Epoch 790, training loss: 0.07335597276687622 = 0.00499268714338541 + 0.01 * 6.836328983306885
Epoch 790, val loss: 1.2619762420654297
Epoch 800, training loss: 0.07314553111791611 = 0.004832678474485874 + 0.01 * 6.83128547668457
Epoch 800, val loss: 1.2674226760864258
Epoch 810, training loss: 0.07319901883602142 = 0.004681393504142761 + 0.01 * 6.851762771606445
Epoch 810, val loss: 1.2725054025650024
Epoch 820, training loss: 0.07301322370767593 = 0.004537975415587425 + 0.01 * 6.847524642944336
Epoch 820, val loss: 1.2776598930358887
Epoch 830, training loss: 0.07284410297870636 = 0.0044021690264344215 + 0.01 * 6.844193935394287
Epoch 830, val loss: 1.2827998399734497
Epoch 840, training loss: 0.07233770936727524 = 0.004273499362170696 + 0.01 * 6.806421756744385
Epoch 840, val loss: 1.2874703407287598
Epoch 850, training loss: 0.0721665620803833 = 0.0041514975018799305 + 0.01 * 6.801506519317627
Epoch 850, val loss: 1.2921737432479858
Epoch 860, training loss: 0.07233503460884094 = 0.004035013262182474 + 0.01 * 6.830002784729004
Epoch 860, val loss: 1.2969770431518555
Epoch 870, training loss: 0.07206916064023972 = 0.0039246282540261745 + 0.01 * 6.814453125
Epoch 870, val loss: 1.301406979560852
Epoch 880, training loss: 0.07181989401578903 = 0.0038194642402231693 + 0.01 * 6.800042629241943
Epoch 880, val loss: 1.3058512210845947
Epoch 890, training loss: 0.07167957723140717 = 0.0037201906088739634 + 0.01 * 6.795938491821289
Epoch 890, val loss: 1.3100751638412476
Epoch 900, training loss: 0.07143648713827133 = 0.0036243898794054985 + 0.01 * 6.781210422515869
Epoch 900, val loss: 1.3144086599349976
Epoch 910, training loss: 0.07178591936826706 = 0.003534190822392702 + 0.01 * 6.825173377990723
Epoch 910, val loss: 1.3184376955032349
Epoch 920, training loss: 0.07116187363862991 = 0.0034472988918423653 + 0.01 * 6.771457672119141
Epoch 920, val loss: 1.3225222826004028
Epoch 930, training loss: 0.07107966393232346 = 0.0033647327218204737 + 0.01 * 6.771492958068848
Epoch 930, val loss: 1.3263784646987915
Epoch 940, training loss: 0.07141680270433426 = 0.0032860066276043653 + 0.01 * 6.813079833984375
Epoch 940, val loss: 1.3302713632583618
Epoch 950, training loss: 0.07078610360622406 = 0.0032104933634400368 + 0.01 * 6.757561206817627
Epoch 950, val loss: 1.3339767456054688
Epoch 960, training loss: 0.07069166749715805 = 0.0031383538153022528 + 0.01 * 6.755331516265869
Epoch 960, val loss: 1.3376891613006592
Epoch 970, training loss: 0.07039473950862885 = 0.003068794496357441 + 0.01 * 6.732594013214111
Epoch 970, val loss: 1.3413256406784058
Epoch 980, training loss: 0.07060183584690094 = 0.003002813085913658 + 0.01 * 6.759902477264404
Epoch 980, val loss: 1.3447037935256958
Epoch 990, training loss: 0.07051254063844681 = 0.002939322032034397 + 0.01 * 6.757321834564209
Epoch 990, val loss: 1.3483030796051025
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 2.018303394317627 = 1.9323351383209229 + 0.01 * 8.596835136413574
Epoch 0, val loss: 1.9292724132537842
Epoch 10, training loss: 2.009019136428833 = 1.9230514764785767 + 0.01 * 8.596776008605957
Epoch 10, val loss: 1.919748306274414
Epoch 20, training loss: 1.9974074363708496 = 1.9114419221878052 + 0.01 * 8.5965576171875
Epoch 20, val loss: 1.9078837633132935
Epoch 30, training loss: 1.9810256958007812 = 1.8950668573379517 + 0.01 * 8.595878601074219
Epoch 30, val loss: 1.8912802934646606
Epoch 40, training loss: 1.9568052291870117 = 1.87088143825531 + 0.01 * 8.592376708984375
Epoch 40, val loss: 1.8670814037322998
Epoch 50, training loss: 1.9235421419143677 = 1.8378653526306152 + 0.01 * 8.567676544189453
Epoch 50, val loss: 1.835680365562439
Epoch 60, training loss: 1.886777400970459 = 1.8023594617843628 + 0.01 * 8.441797256469727
Epoch 60, val loss: 1.8055301904678345
Epoch 70, training loss: 1.851257562637329 = 1.770296573638916 + 0.01 * 8.09610366821289
Epoch 70, val loss: 1.7793153524398804
Epoch 80, training loss: 1.8063757419586182 = 1.7273751497268677 + 0.01 * 7.9000630378723145
Epoch 80, val loss: 1.7403684854507446
Epoch 90, training loss: 1.7439560890197754 = 1.6672358512878418 + 0.01 * 7.672018527984619
Epoch 90, val loss: 1.6870067119598389
Epoch 100, training loss: 1.6621311902999878 = 1.586851716041565 + 0.01 * 7.527953147888184
Epoch 100, val loss: 1.6191400289535522
Epoch 110, training loss: 1.5677244663238525 = 1.4929226636886597 + 0.01 * 7.480177402496338
Epoch 110, val loss: 1.5409612655639648
Epoch 120, training loss: 1.467431902885437 = 1.3930368423461914 + 0.01 * 7.439506530761719
Epoch 120, val loss: 1.460647702217102
Epoch 130, training loss: 1.3639369010925293 = 1.290120244026184 + 0.01 * 7.381661415100098
Epoch 130, val loss: 1.3811172246932983
Epoch 140, training loss: 1.25740647315979 = 1.1843663454055786 + 0.01 * 7.30401611328125
Epoch 140, val loss: 1.302608847618103
Epoch 150, training loss: 1.1498242616653442 = 1.0774827003479004 + 0.01 * 7.234161376953125
Epoch 150, val loss: 1.225757360458374
Epoch 160, training loss: 1.045088291168213 = 0.9731044769287109 + 0.01 * 7.198385715484619
Epoch 160, val loss: 1.1525424718856812
Epoch 170, training loss: 0.9477565288543701 = 0.8760506510734558 + 0.01 * 7.170586585998535
Epoch 170, val loss: 1.0871201753616333
Epoch 180, training loss: 0.861761212348938 = 0.7903559803962708 + 0.01 * 7.140523910522461
Epoch 180, val loss: 1.0332250595092773
Epoch 190, training loss: 0.7879124283790588 = 0.7167094945907593 + 0.01 * 7.120296001434326
Epoch 190, val loss: 0.9916674494743347
Epoch 200, training loss: 0.7239732146263123 = 0.6529437303543091 + 0.01 * 7.102947235107422
Epoch 200, val loss: 0.9607661366462708
Epoch 210, training loss: 0.6671428084373474 = 0.596200704574585 + 0.01 * 7.094213008880615
Epoch 210, val loss: 0.9383914470672607
Epoch 220, training loss: 0.6152681112289429 = 0.5443688035011292 + 0.01 * 7.089929580688477
Epoch 220, val loss: 0.9229789972305298
Epoch 230, training loss: 0.5672882199287415 = 0.49641668796539307 + 0.01 * 7.087153911590576
Epoch 230, val loss: 0.9137982130050659
Epoch 240, training loss: 0.5228848457336426 = 0.4520372450351715 + 0.01 * 7.084758758544922
Epoch 240, val loss: 0.9105356335639954
Epoch 250, training loss: 0.4820714592933655 = 0.4112367630004883 + 0.01 * 7.083467960357666
Epoch 250, val loss: 0.9132609963417053
Epoch 260, training loss: 0.4447556138038635 = 0.3739498555660248 + 0.01 * 7.080576419830322
Epoch 260, val loss: 0.9220288991928101
Epoch 270, training loss: 0.4106287360191345 = 0.33983781933784485 + 0.01 * 7.0790934562683105
Epoch 270, val loss: 0.9363278150558472
Epoch 280, training loss: 0.3791574537754059 = 0.3083948493003845 + 0.01 * 7.076261043548584
Epoch 280, val loss: 0.955277144908905
Epoch 290, training loss: 0.3498972952365875 = 0.2791577875614166 + 0.01 * 7.073949813842773
Epoch 290, val loss: 0.9782062768936157
Epoch 300, training loss: 0.3225686252117157 = 0.25183185935020447 + 0.01 * 7.073676109313965
Epoch 300, val loss: 1.0044407844543457
Epoch 310, training loss: 0.2970850467681885 = 0.226395383477211 + 0.01 * 7.068967819213867
Epoch 310, val loss: 1.033524513244629
Epoch 320, training loss: 0.2736355662345886 = 0.2029769867658615 + 0.01 * 7.065856456756592
Epoch 320, val loss: 1.064924955368042
Epoch 330, training loss: 0.25231316685676575 = 0.18168535828590393 + 0.01 * 7.06278133392334
Epoch 330, val loss: 1.0983654260635376
Epoch 340, training loss: 0.2331087589263916 = 0.1625160574913025 + 0.01 * 7.059269905090332
Epoch 340, val loss: 1.1334640979766846
Epoch 350, training loss: 0.2158876657485962 = 0.14533793926239014 + 0.01 * 7.054972171783447
Epoch 350, val loss: 1.1699252128601074
Epoch 360, training loss: 0.20049253106117249 = 0.12998133897781372 + 0.01 * 7.051118850708008
Epoch 360, val loss: 1.207538366317749
Epoch 370, training loss: 0.1867378056049347 = 0.1162780225276947 + 0.01 * 7.045979022979736
Epoch 370, val loss: 1.2459152936935425
Epoch 380, training loss: 0.1744900345802307 = 0.10406819730997086 + 0.01 * 7.042183876037598
Epoch 380, val loss: 1.2848906517028809
Epoch 390, training loss: 0.16357284784317017 = 0.09321294724941254 + 0.01 * 7.035989284515381
Epoch 390, val loss: 1.3242697715759277
Epoch 400, training loss: 0.15396425127983093 = 0.08358311653137207 + 0.01 * 7.038112640380859
Epoch 400, val loss: 1.3638274669647217
Epoch 410, training loss: 0.1453736126422882 = 0.07506908476352692 + 0.01 * 7.030453681945801
Epoch 410, val loss: 1.4033359289169312
Epoch 420, training loss: 0.1377655565738678 = 0.06755734980106354 + 0.01 * 7.020820617675781
Epoch 420, val loss: 1.4425089359283447
Epoch 430, training loss: 0.13107165694236755 = 0.06093919649720192 + 0.01 * 7.013246059417725
Epoch 430, val loss: 1.4811712503433228
Epoch 440, training loss: 0.1252141296863556 = 0.05511027202010155 + 0.01 * 7.010385513305664
Epoch 440, val loss: 1.519176959991455
Epoch 450, training loss: 0.12008127570152283 = 0.04997964948415756 + 0.01 * 7.010162830352783
Epoch 450, val loss: 1.5563267469406128
Epoch 460, training loss: 0.11545805633068085 = 0.0454600527882576 + 0.01 * 6.999800682067871
Epoch 460, val loss: 1.5925300121307373
Epoch 470, training loss: 0.11139626801013947 = 0.041466034948825836 + 0.01 * 6.99302339553833
Epoch 470, val loss: 1.6277858018875122
Epoch 480, training loss: 0.10777048766613007 = 0.03792725130915642 + 0.01 * 6.9843244552612305
Epoch 480, val loss: 1.6620399951934814
Epoch 490, training loss: 0.10465981811285019 = 0.034786246716976166 + 0.01 * 6.987357139587402
Epoch 490, val loss: 1.6952261924743652
Epoch 500, training loss: 0.10185220837593079 = 0.03199660778045654 + 0.01 * 6.985560417175293
Epoch 500, val loss: 1.7272682189941406
Epoch 510, training loss: 0.09923845529556274 = 0.029507344588637352 + 0.01 * 6.973110675811768
Epoch 510, val loss: 1.7583400011062622
Epoch 520, training loss: 0.09695012867450714 = 0.02727835811674595 + 0.01 * 6.967177391052246
Epoch 520, val loss: 1.7884434461593628
Epoch 530, training loss: 0.09488701075315475 = 0.02527645416557789 + 0.01 * 6.961055755615234
Epoch 530, val loss: 1.8175957202911377
Epoch 540, training loss: 0.09306559711694717 = 0.023474758490920067 + 0.01 * 6.959083557128906
Epoch 540, val loss: 1.8458175659179688
Epoch 550, training loss: 0.0914379209280014 = 0.021851234138011932 + 0.01 * 6.958669185638428
Epoch 550, val loss: 1.8730731010437012
Epoch 560, training loss: 0.08991910517215729 = 0.020384911447763443 + 0.01 * 6.953419208526611
Epoch 560, val loss: 1.8994190692901611
Epoch 570, training loss: 0.08850806951522827 = 0.01905743032693863 + 0.01 * 6.945064067840576
Epoch 570, val loss: 1.924936294555664
Epoch 580, training loss: 0.08725182712078094 = 0.017852891236543655 + 0.01 * 6.939894199371338
Epoch 580, val loss: 1.949563980102539
Epoch 590, training loss: 0.08612548559904099 = 0.01675674505531788 + 0.01 * 6.9368743896484375
Epoch 590, val loss: 1.9734137058258057
Epoch 600, training loss: 0.08515795320272446 = 0.015757612884044647 + 0.01 * 6.94003438949585
Epoch 600, val loss: 1.9965065717697144
Epoch 610, training loss: 0.08411669731140137 = 0.014845447614789009 + 0.01 * 6.927124977111816
Epoch 610, val loss: 2.0187768936157227
Epoch 620, training loss: 0.08325731009244919 = 0.014010249637067318 + 0.01 * 6.92470645904541
Epoch 620, val loss: 2.040390729904175
Epoch 630, training loss: 0.08275409042835236 = 0.013243704102933407 + 0.01 * 6.951038837432861
Epoch 630, val loss: 2.06129789352417
Epoch 640, training loss: 0.08178676664829254 = 0.01254125777631998 + 0.01 * 6.924551486968994
Epoch 640, val loss: 2.0815253257751465
Epoch 650, training loss: 0.08097968250513077 = 0.011894727125763893 + 0.01 * 6.908495903015137
Epoch 650, val loss: 2.101118326187134
Epoch 660, training loss: 0.08035171031951904 = 0.011297887191176414 + 0.01 * 6.90538215637207
Epoch 660, val loss: 2.1201279163360596
Epoch 670, training loss: 0.07975732535123825 = 0.010745873674750328 + 0.01 * 6.901144981384277
Epoch 670, val loss: 2.138582468032837
Epoch 680, training loss: 0.0796264261007309 = 0.010234875604510307 + 0.01 * 6.939155101776123
Epoch 680, val loss: 2.156397581100464
Epoch 690, training loss: 0.07883255183696747 = 0.00976268108934164 + 0.01 * 6.906987190246582
Epoch 690, val loss: 2.1737098693847656
Epoch 700, training loss: 0.07838325947523117 = 0.009324677288532257 + 0.01 * 6.905858039855957
Epoch 700, val loss: 2.1904263496398926
Epoch 710, training loss: 0.07777152210474014 = 0.008917571976780891 + 0.01 * 6.885395050048828
Epoch 710, val loss: 2.2066497802734375
Epoch 720, training loss: 0.07736767828464508 = 0.008538175374269485 + 0.01 * 6.882950305938721
Epoch 720, val loss: 2.222470283508301
Epoch 730, training loss: 0.07704608887434006 = 0.008183849044144154 + 0.01 * 6.886223793029785
Epoch 730, val loss: 2.2377774715423584
Epoch 740, training loss: 0.07667141407728195 = 0.007853086106479168 + 0.01 * 6.881833076477051
Epoch 740, val loss: 2.252753257751465
Epoch 750, training loss: 0.07628314942121506 = 0.007543561980128288 + 0.01 * 6.873958587646484
Epoch 750, val loss: 2.2671430110931396
Epoch 760, training loss: 0.07591824233531952 = 0.0072535136714577675 + 0.01 * 6.866473197937012
Epoch 760, val loss: 2.2812352180480957
Epoch 770, training loss: 0.07559444010257721 = 0.006981309037655592 + 0.01 * 6.861313343048096
Epoch 770, val loss: 2.294851064682007
Epoch 780, training loss: 0.07530435919761658 = 0.006725820247083902 + 0.01 * 6.85785436630249
Epoch 780, val loss: 2.308093786239624
Epoch 790, training loss: 0.07510500401258469 = 0.006485657766461372 + 0.01 * 6.861934661865234
Epoch 790, val loss: 2.320875883102417
Epoch 800, training loss: 0.07484818249940872 = 0.006259842310100794 + 0.01 * 6.8588337898254395
Epoch 800, val loss: 2.333354949951172
Epoch 810, training loss: 0.0744815319776535 = 0.006047055125236511 + 0.01 * 6.843448162078857
Epoch 810, val loss: 2.3455569744110107
Epoch 820, training loss: 0.07455947995185852 = 0.0058461567386984825 + 0.01 * 6.871332168579102
Epoch 820, val loss: 2.357362985610962
Epoch 830, training loss: 0.07401526719331741 = 0.005656724330037832 + 0.01 * 6.835854530334473
Epoch 830, val loss: 2.368910074234009
Epoch 840, training loss: 0.07375933974981308 = 0.005477582570165396 + 0.01 * 6.8281755447387695
Epoch 840, val loss: 2.3801729679107666
Epoch 850, training loss: 0.07376458495855331 = 0.005307885818183422 + 0.01 * 6.845670700073242
Epoch 850, val loss: 2.391014814376831
Epoch 860, training loss: 0.07348792999982834 = 0.005147252697497606 + 0.01 * 6.8340678215026855
Epoch 860, val loss: 2.401754140853882
Epoch 870, training loss: 0.0731983482837677 = 0.004994766321033239 + 0.01 * 6.820358753204346
Epoch 870, val loss: 2.412083864212036
Epoch 880, training loss: 0.0730881541967392 = 0.004849948920309544 + 0.01 * 6.823821067810059
Epoch 880, val loss: 2.4221129417419434
Epoch 890, training loss: 0.07287587970495224 = 0.004712166730314493 + 0.01 * 6.816371440887451
Epoch 890, val loss: 2.4320945739746094
Epoch 900, training loss: 0.0726940929889679 = 0.0045814174227416515 + 0.01 * 6.811267852783203
Epoch 900, val loss: 2.4416613578796387
Epoch 910, training loss: 0.07257556915283203 = 0.004457154776901007 + 0.01 * 6.81184196472168
Epoch 910, val loss: 2.4509072303771973
Epoch 920, training loss: 0.07235179096460342 = 0.004338802769780159 + 0.01 * 6.801299095153809
Epoch 920, val loss: 2.46008563041687
Epoch 930, training loss: 0.07232359051704407 = 0.0042260270565748215 + 0.01 * 6.809756278991699
Epoch 930, val loss: 2.46895432472229
Epoch 940, training loss: 0.07207931578159332 = 0.0041183908469974995 + 0.01 * 6.796092987060547
Epoch 940, val loss: 2.4776906967163086
Epoch 950, training loss: 0.07203784584999084 = 0.00401553139090538 + 0.01 * 6.802232265472412
Epoch 950, val loss: 2.48638916015625
Epoch 960, training loss: 0.07208962738513947 = 0.003917215391993523 + 0.01 * 6.817241191864014
Epoch 960, val loss: 2.494626760482788
Epoch 970, training loss: 0.07188890129327774 = 0.0038234221283346415 + 0.01 * 6.806548118591309
Epoch 970, val loss: 2.5028200149536133
Epoch 980, training loss: 0.07164217531681061 = 0.0037336547393351793 + 0.01 * 6.790851593017578
Epoch 980, val loss: 2.5108330249786377
Epoch 990, training loss: 0.07159797102212906 = 0.003647714387625456 + 0.01 * 6.795025825500488
Epoch 990, val loss: 2.5186517238616943
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8144438587243016
The final CL Acc:0.78889, 0.02636, The final GNN Acc:0.81339, 0.00188
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13180])
remove edge: torch.Size([2, 7972])
updated graph: torch.Size([2, 10596])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0268137454986572 = 1.9408453702926636 + 0.01 * 8.59683895111084
Epoch 0, val loss: 1.9430180788040161
Epoch 10, training loss: 2.0164804458618164 = 1.9305129051208496 + 0.01 * 8.59676456451416
Epoch 10, val loss: 1.9322993755340576
Epoch 20, training loss: 2.0037243366241455 = 1.9177589416503906 + 0.01 * 8.596529960632324
Epoch 20, val loss: 1.9189351797103882
Epoch 30, training loss: 1.9858492612838745 = 1.899891972541809 + 0.01 * 8.595733642578125
Epoch 30, val loss: 1.9002301692962646
Epoch 40, training loss: 1.9596980810165405 = 1.8737916946411133 + 0.01 * 8.5906400680542
Epoch 40, val loss: 1.8734227418899536
Epoch 50, training loss: 1.9235087633132935 = 1.8380234241485596 + 0.01 * 8.548539161682129
Epoch 50, val loss: 1.8384740352630615
Epoch 60, training loss: 1.8817589282989502 = 1.7987014055252075 + 0.01 * 8.305757522583008
Epoch 60, val loss: 1.8038215637207031
Epoch 70, training loss: 1.8429911136627197 = 1.7626636028289795 + 0.01 * 8.032754898071289
Epoch 70, val loss: 1.7732913494110107
Epoch 80, training loss: 1.7933063507080078 = 1.7155473232269287 + 0.01 * 7.7758989334106445
Epoch 80, val loss: 1.7300126552581787
Epoch 90, training loss: 1.7253626585006714 = 1.6500664949417114 + 0.01 * 7.5296196937561035
Epoch 90, val loss: 1.6707851886749268
Epoch 100, training loss: 1.6387311220169067 = 1.5650684833526611 + 0.01 * 7.366266250610352
Epoch 100, val loss: 1.5978565216064453
Epoch 110, training loss: 1.5427495241165161 = 1.469849705696106 + 0.01 * 7.289981365203857
Epoch 110, val loss: 1.5185894966125488
Epoch 120, training loss: 1.445955753326416 = 1.3734508752822876 + 0.01 * 7.250488758087158
Epoch 120, val loss: 1.440428376197815
Epoch 130, training loss: 1.3512747287750244 = 1.279079794883728 + 0.01 * 7.219491958618164
Epoch 130, val loss: 1.3660740852355957
Epoch 140, training loss: 1.25846266746521 = 1.186546802520752 + 0.01 * 7.191585063934326
Epoch 140, val loss: 1.294620394706726
Epoch 150, training loss: 1.169862985610962 = 1.0981520414352417 + 0.01 * 7.171088695526123
Epoch 150, val loss: 1.2276320457458496
Epoch 160, training loss: 1.0887397527694702 = 1.0171279907226562 + 0.01 * 7.16117525100708
Epoch 160, val loss: 1.1685030460357666
Epoch 170, training loss: 1.0153952836990356 = 0.9438430666923523 + 0.01 * 7.155219078063965
Epoch 170, val loss: 1.116657018661499
Epoch 180, training loss: 0.94715815782547 = 0.8756844997406006 + 0.01 * 7.147368431091309
Epoch 180, val loss: 1.069131851196289
Epoch 190, training loss: 0.8809683322906494 = 0.809614896774292 + 0.01 * 7.135341167449951
Epoch 190, val loss: 1.022688627243042
Epoch 200, training loss: 0.8150830864906311 = 0.7438982129096985 + 0.01 * 7.118484973907471
Epoch 200, val loss: 0.9758966565132141
Epoch 210, training loss: 0.7496843338012695 = 0.6786953806877136 + 0.01 * 7.0988922119140625
Epoch 210, val loss: 0.9296438097953796
Epoch 220, training loss: 0.6861621737480164 = 0.6153708696365356 + 0.01 * 7.079128742218018
Epoch 220, val loss: 0.8858662843704224
Epoch 230, training loss: 0.6255747079849243 = 0.55495685338974 + 0.01 * 7.061782360076904
Epoch 230, val loss: 0.8465678095817566
Epoch 240, training loss: 0.5684696435928345 = 0.49795979261398315 + 0.01 * 7.050985813140869
Epoch 240, val loss: 0.8127002120018005
Epoch 250, training loss: 0.5149483680725098 = 0.44451451301574707 + 0.01 * 7.043384552001953
Epoch 250, val loss: 0.7844322323799133
Epoch 260, training loss: 0.46495288610458374 = 0.39455124735832214 + 0.01 * 7.040163040161133
Epoch 260, val loss: 0.7612249255180359
Epoch 270, training loss: 0.41845569014549255 = 0.34807640314102173 + 0.01 * 7.037929058074951
Epoch 270, val loss: 0.7430811524391174
Epoch 280, training loss: 0.3755457103252411 = 0.3052082657814026 + 0.01 * 7.033745288848877
Epoch 280, val loss: 0.7298094630241394
Epoch 290, training loss: 0.3364180624485016 = 0.2660883367061615 + 0.01 * 7.032973766326904
Epoch 290, val loss: 0.7209874987602234
Epoch 300, training loss: 0.3011789917945862 = 0.2308720201253891 + 0.01 * 7.030698776245117
Epoch 300, val loss: 0.716080367565155
Epoch 310, training loss: 0.26994550228118896 = 0.19965286552906036 + 0.01 * 7.029262065887451
Epoch 310, val loss: 0.7149094939231873
Epoch 320, training loss: 0.24267026782035828 = 0.17239463329315186 + 0.01 * 7.027563095092773
Epoch 320, val loss: 0.7169433236122131
Epoch 330, training loss: 0.21918916702270508 = 0.14891035854816437 + 0.01 * 7.027881622314453
Epoch 330, val loss: 0.7217662334442139
Epoch 340, training loss: 0.19911807775497437 = 0.12885110080242157 + 0.01 * 7.026698112487793
Epoch 340, val loss: 0.7289974093437195
Epoch 350, training loss: 0.1820267140865326 = 0.11178362369537354 + 0.01 * 7.024308681488037
Epoch 350, val loss: 0.7382024526596069
Epoch 360, training loss: 0.16750536859035492 = 0.09727311879396439 + 0.01 * 7.0232253074646
Epoch 360, val loss: 0.7488654851913452
Epoch 370, training loss: 0.1551424264907837 = 0.08493001759052277 + 0.01 * 7.021241664886475
Epoch 370, val loss: 0.760630190372467
Epoch 380, training loss: 0.14461413025856018 = 0.0744214728474617 + 0.01 * 7.019266605377197
Epoch 380, val loss: 0.7732547521591187
Epoch 390, training loss: 0.135719895362854 = 0.065467469394207 + 0.01 * 7.025241851806641
Epoch 390, val loss: 0.7865801453590393
Epoch 400, training loss: 0.12799574434757233 = 0.057832472026348114 + 0.01 * 7.016327381134033
Epoch 400, val loss: 0.8003153800964355
Epoch 410, training loss: 0.12145911157131195 = 0.05131152644753456 + 0.01 * 7.014758110046387
Epoch 410, val loss: 0.8142359852790833
Epoch 420, training loss: 0.11585544049739838 = 0.045727554708719254 + 0.01 * 7.012788772583008
Epoch 420, val loss: 0.8282235264778137
Epoch 430, training loss: 0.1110495775938034 = 0.04092990979552269 + 0.01 * 7.011966228485107
Epoch 430, val loss: 0.8421226143836975
Epoch 440, training loss: 0.10688693821430206 = 0.03679325804114342 + 0.01 * 7.0093674659729
Epoch 440, val loss: 0.8559332489967346
Epoch 450, training loss: 0.10328245162963867 = 0.03321433812379837 + 0.01 * 7.006811141967773
Epoch 450, val loss: 0.8695992231369019
Epoch 460, training loss: 0.10019074380397797 = 0.03010597452521324 + 0.01 * 7.008477210998535
Epoch 460, val loss: 0.8830477595329285
Epoch 470, training loss: 0.0974309891462326 = 0.027396824210882187 + 0.01 * 7.003416538238525
Epoch 470, val loss: 0.8962255716323853
Epoch 480, training loss: 0.09502182900905609 = 0.025026138871908188 + 0.01 * 6.999569892883301
Epoch 480, val loss: 0.9091567993164062
Epoch 490, training loss: 0.09291711449623108 = 0.02294253371655941 + 0.01 * 6.997457981109619
Epoch 490, val loss: 0.921731173992157
Epoch 500, training loss: 0.09114310890436172 = 0.0211042370647192 + 0.01 * 7.003887176513672
Epoch 500, val loss: 0.9339808225631714
Epoch 510, training loss: 0.08938546478748322 = 0.019477132707834244 + 0.01 * 6.990832805633545
Epoch 510, val loss: 0.9459452033042908
Epoch 520, training loss: 0.08790376037359238 = 0.018030626699328423 + 0.01 * 6.987313270568848
Epoch 520, val loss: 0.9575287103652954
Epoch 530, training loss: 0.08661479502916336 = 0.016739891842007637 + 0.01 * 6.987490653991699
Epoch 530, val loss: 0.9688233733177185
Epoch 540, training loss: 0.08541283756494522 = 0.015584385022521019 + 0.01 * 6.982845306396484
Epoch 540, val loss: 0.979743242263794
Epoch 550, training loss: 0.08431517332792282 = 0.014546769671142101 + 0.01 * 6.976840972900391
Epoch 550, val loss: 0.9903646111488342
Epoch 560, training loss: 0.0834004282951355 = 0.013611529022455215 + 0.01 * 6.9788899421691895
Epoch 560, val loss: 1.0006693601608276
Epoch 570, training loss: 0.08248786628246307 = 0.012766522355377674 + 0.01 * 6.972134113311768
Epoch 570, val loss: 1.0106323957443237
Epoch 580, training loss: 0.08165574818849564 = 0.012000584974884987 + 0.01 * 6.965516567230225
Epoch 580, val loss: 1.0203238725662231
Epoch 590, training loss: 0.08097490668296814 = 0.011303658597171307 + 0.01 * 6.967125415802002
Epoch 590, val loss: 1.0297125577926636
Epoch 600, training loss: 0.08024973422288895 = 0.010668443515896797 + 0.01 * 6.958129405975342
Epoch 600, val loss: 1.0387910604476929
Epoch 610, training loss: 0.07957112044095993 = 0.010087795555591583 + 0.01 * 6.948332786560059
Epoch 610, val loss: 1.0476014614105225
Epoch 620, training loss: 0.07912524044513702 = 0.009555893950164318 + 0.01 * 6.956935405731201
Epoch 620, val loss: 1.0560376644134521
Epoch 630, training loss: 0.07843568176031113 = 0.00906784925609827 + 0.01 * 6.936783790588379
Epoch 630, val loss: 1.0644530057907104
Epoch 640, training loss: 0.07838389277458191 = 0.008619225583970547 + 0.01 * 6.976466655731201
Epoch 640, val loss: 1.0723075866699219
Epoch 650, training loss: 0.07754206657409668 = 0.008206142112612724 + 0.01 * 6.933592319488525
Epoch 650, val loss: 1.0801620483398438
Epoch 660, training loss: 0.07703561335802078 = 0.007825106382369995 + 0.01 * 6.921051025390625
Epoch 660, val loss: 1.08761727809906
Epoch 670, training loss: 0.07690685242414474 = 0.007471775636076927 + 0.01 * 6.943507671356201
Epoch 670, val loss: 1.0949270725250244
Epoch 680, training loss: 0.07619179040193558 = 0.007144729141145945 + 0.01 * 6.9047064781188965
Epoch 680, val loss: 1.1020175218582153
Epoch 690, training loss: 0.07598209381103516 = 0.006840717047452927 + 0.01 * 6.914137363433838
Epoch 690, val loss: 1.1088435649871826
Epoch 700, training loss: 0.07562814652919769 = 0.00655839778482914 + 0.01 * 6.906975269317627
Epoch 700, val loss: 1.1154484748840332
Epoch 710, training loss: 0.07533969730138779 = 0.00629584351554513 + 0.01 * 6.904385566711426
Epoch 710, val loss: 1.1219244003295898
Epoch 720, training loss: 0.07502854615449905 = 0.006050579249858856 + 0.01 * 6.897797107696533
Epoch 720, val loss: 1.1281967163085938
Epoch 730, training loss: 0.07467768341302872 = 0.005821622908115387 + 0.01 * 6.885606288909912
Epoch 730, val loss: 1.1343098878860474
Epoch 740, training loss: 0.07431714236736298 = 0.005607394967228174 + 0.01 * 6.870974540710449
Epoch 740, val loss: 1.1402112245559692
Epoch 750, training loss: 0.07394533604383469 = 0.00540658924728632 + 0.01 * 6.853875160217285
Epoch 750, val loss: 1.1459165811538696
Epoch 760, training loss: 0.07373160123825073 = 0.005218310281634331 + 0.01 * 6.8513288497924805
Epoch 760, val loss: 1.1516298055648804
Epoch 770, training loss: 0.07354342192411423 = 0.0050413161516189575 + 0.01 * 6.850210666656494
Epoch 770, val loss: 1.1568876504898071
Epoch 780, training loss: 0.0731780081987381 = 0.0048747300170362 + 0.01 * 6.830328464508057
Epoch 780, val loss: 1.1624176502227783
Epoch 790, training loss: 0.07317841798067093 = 0.004717716947197914 + 0.01 * 6.846070766448975
Epoch 790, val loss: 1.167370080947876
Epoch 800, training loss: 0.07287045568227768 = 0.004569579381495714 + 0.01 * 6.830087184906006
Epoch 800, val loss: 1.172493815422058
Epoch 810, training loss: 0.07262174040079117 = 0.0044296253472566605 + 0.01 * 6.819211959838867
Epoch 810, val loss: 1.1774237155914307
Epoch 820, training loss: 0.0724572166800499 = 0.004297222010791302 + 0.01 * 6.815999984741211
Epoch 820, val loss: 1.1820882558822632
Epoch 830, training loss: 0.07210730016231537 = 0.004171834327280521 + 0.01 * 6.7935471534729
Epoch 830, val loss: 1.186944603919983
Epoch 840, training loss: 0.0724313035607338 = 0.00405257660895586 + 0.01 * 6.837872505187988
Epoch 840, val loss: 1.1914808750152588
Epoch 850, training loss: 0.07185623049736023 = 0.0039395177736878395 + 0.01 * 6.7916717529296875
Epoch 850, val loss: 1.1959508657455444
Epoch 860, training loss: 0.07172469049692154 = 0.003832044545561075 + 0.01 * 6.789265155792236
Epoch 860, val loss: 1.200419306755066
Epoch 870, training loss: 0.07175866514444351 = 0.0037298458628356457 + 0.01 * 6.802882671356201
Epoch 870, val loss: 1.204498529434204
Epoch 880, training loss: 0.07146808505058289 = 0.0036326914560049772 + 0.01 * 6.783539295196533
Epoch 880, val loss: 1.2088607549667358
Epoch 890, training loss: 0.07124064117670059 = 0.0035401780623942614 + 0.01 * 6.770046234130859
Epoch 890, val loss: 1.2127488851547241
Epoch 900, training loss: 0.07107516378164291 = 0.003451964119449258 + 0.01 * 6.762320041656494
Epoch 900, val loss: 1.2168916463851929
Epoch 910, training loss: 0.07088077813386917 = 0.003367602126672864 + 0.01 * 6.751317501068115
Epoch 910, val loss: 1.2207229137420654
Epoch 920, training loss: 0.07080252468585968 = 0.003286991035565734 + 0.01 * 6.751553535461426
Epoch 920, val loss: 1.224645733833313
Epoch 930, training loss: 0.07077141851186752 = 0.0032096959184855223 + 0.01 * 6.756172180175781
Epoch 930, val loss: 1.2283625602722168
Epoch 940, training loss: 0.07060938328504562 = 0.0031357237603515387 + 0.01 * 6.747365951538086
Epoch 940, val loss: 1.2320579290390015
Epoch 950, training loss: 0.07052111625671387 = 0.003065053839236498 + 0.01 * 6.745605945587158
Epoch 950, val loss: 1.2357311248779297
Epoch 960, training loss: 0.07060496509075165 = 0.0029973729979246855 + 0.01 * 6.760759353637695
Epoch 960, val loss: 1.2391636371612549
Epoch 970, training loss: 0.07033863663673401 = 0.0029324828647077084 + 0.01 * 6.7406158447265625
Epoch 970, val loss: 1.2427507638931274
Epoch 980, training loss: 0.07014521211385727 = 0.0028701741248369217 + 0.01 * 6.727503776550293
Epoch 980, val loss: 1.246061086654663
Epoch 990, training loss: 0.07024232298135757 = 0.0028103694785386324 + 0.01 * 6.743195533752441
Epoch 990, val loss: 1.2494502067565918
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.0440046787261963 = 1.9580366611480713 + 0.01 * 8.596807479858398
Epoch 0, val loss: 1.9635272026062012
Epoch 10, training loss: 2.033865451812744 = 1.947898268699646 + 0.01 * 8.596725463867188
Epoch 10, val loss: 1.9535671472549438
Epoch 20, training loss: 2.021444797515869 = 1.9354805946350098 + 0.01 * 8.596413612365723
Epoch 20, val loss: 1.9408273696899414
Epoch 30, training loss: 2.003840208053589 = 1.9178870916366577 + 0.01 * 8.595315933227539
Epoch 30, val loss: 1.9222182035446167
Epoch 40, training loss: 1.9775701761245728 = 1.8916891813278198 + 0.01 * 8.58809757232666
Epoch 40, val loss: 1.8942619562149048
Epoch 50, training loss: 1.9395817518234253 = 1.8541910648345947 + 0.01 * 8.539068222045898
Epoch 50, val loss: 1.855256199836731
Epoch 60, training loss: 1.8931337594985962 = 1.8102271556854248 + 0.01 * 8.290658950805664
Epoch 60, val loss: 1.8132325410842896
Epoch 70, training loss: 1.8531850576400757 = 1.7730915546417236 + 0.01 * 8.009350776672363
Epoch 70, val loss: 1.781816840171814
Epoch 80, training loss: 1.810655117034912 = 1.7334685325622559 + 0.01 * 7.718660354614258
Epoch 80, val loss: 1.7466530799865723
Epoch 90, training loss: 1.7543625831604004 = 1.6794605255126953 + 0.01 * 7.49020528793335
Epoch 90, val loss: 1.6974164247512817
Epoch 100, training loss: 1.6804907321929932 = 1.6068142652511597 + 0.01 * 7.3676438331604
Epoch 100, val loss: 1.6322638988494873
Epoch 110, training loss: 1.5876927375793457 = 1.514672875404358 + 0.01 * 7.3019843101501465
Epoch 110, val loss: 1.5526763200759888
Epoch 120, training loss: 1.4821614027023315 = 1.4094951152801514 + 0.01 * 7.266634464263916
Epoch 120, val loss: 1.4631139039993286
Epoch 130, training loss: 1.371413230895996 = 1.2989391088485718 + 0.01 * 7.247408390045166
Epoch 130, val loss: 1.3711280822753906
Epoch 140, training loss: 1.262927532196045 = 1.1906243562698364 + 0.01 * 7.230317115783691
Epoch 140, val loss: 1.2813149690628052
Epoch 150, training loss: 1.160438895225525 = 1.088309407234192 + 0.01 * 7.212947368621826
Epoch 150, val loss: 1.1967387199401855
Epoch 160, training loss: 1.065476655960083 = 0.9935228228569031 + 0.01 * 7.1953816413879395
Epoch 160, val loss: 1.1189141273498535
Epoch 170, training loss: 0.9780789017677307 = 0.9062912464141846 + 0.01 * 7.178765773773193
Epoch 170, val loss: 1.0481102466583252
Epoch 180, training loss: 0.8982805013656616 = 0.8266751766204834 + 0.01 * 7.160533428192139
Epoch 180, val loss: 0.984296441078186
Epoch 190, training loss: 0.8266169428825378 = 0.7552456855773926 + 0.01 * 7.1371283531188965
Epoch 190, val loss: 0.9286831617355347
Epoch 200, training loss: 0.7630764842033386 = 0.6920042037963867 + 0.01 * 7.1072306632995605
Epoch 200, val loss: 0.8815615177154541
Epoch 210, training loss: 0.7060897946357727 = 0.6352943181991577 + 0.01 * 7.079549312591553
Epoch 210, val loss: 0.8418989181518555
Epoch 220, training loss: 0.6533940434455872 = 0.582828938961029 + 0.01 * 7.056509017944336
Epoch 220, val loss: 0.8084123134613037
Epoch 230, training loss: 0.6033555865287781 = 0.5329537987709045 + 0.01 * 7.040177345275879
Epoch 230, val loss: 0.7798983454704285
Epoch 240, training loss: 0.555431067943573 = 0.4851365387439728 + 0.01 * 7.029451847076416
Epoch 240, val loss: 0.7552460432052612
Epoch 250, training loss: 0.5096435546875 = 0.4394177794456482 + 0.01 * 7.0225749015808105
Epoch 250, val loss: 0.7341355681419373
Epoch 260, training loss: 0.46607834100723267 = 0.395921915769577 + 0.01 * 7.015641212463379
Epoch 260, val loss: 0.7164050340652466
Epoch 270, training loss: 0.4249088764190674 = 0.35480818152427673 + 0.01 * 7.010068416595459
Epoch 270, val loss: 0.7020230889320374
Epoch 280, training loss: 0.38647326827049255 = 0.31631144881248474 + 0.01 * 7.0161824226379395
Epoch 280, val loss: 0.6909106969833374
Epoch 290, training loss: 0.35070884227752686 = 0.28068819642066956 + 0.01 * 7.002065658569336
Epoch 290, val loss: 0.6830048561096191
Epoch 300, training loss: 0.31805187463760376 = 0.2480737566947937 + 0.01 * 6.997811794281006
Epoch 300, val loss: 0.6782515645027161
Epoch 310, training loss: 0.2883908152580261 = 0.21849766373634338 + 0.01 * 6.989314079284668
Epoch 310, val loss: 0.6763731241226196
Epoch 320, training loss: 0.26176148653030396 = 0.1919434368610382 + 0.01 * 6.981805801391602
Epoch 320, val loss: 0.6768994331359863
Epoch 330, training loss: 0.238224595785141 = 0.16837210953235626 + 0.01 * 6.9852495193481445
Epoch 330, val loss: 0.6796084642410278
Epoch 340, training loss: 0.21740609407424927 = 0.14771485328674316 + 0.01 * 6.969124794006348
Epoch 340, val loss: 0.6844178438186646
Epoch 350, training loss: 0.19941379129886627 = 0.1297847181558609 + 0.01 * 6.962907791137695
Epoch 350, val loss: 0.6909255981445312
Epoch 360, training loss: 0.1838865876197815 = 0.1143120601773262 + 0.01 * 6.957453727722168
Epoch 360, val loss: 0.6988644003868103
Epoch 370, training loss: 0.1704961359500885 = 0.10099775344133377 + 0.01 * 6.949839115142822
Epoch 370, val loss: 0.7078956365585327
Epoch 380, training loss: 0.158986896276474 = 0.08955173194408417 + 0.01 * 6.943516254425049
Epoch 380, val loss: 0.7176950573921204
Epoch 390, training loss: 0.1490856409072876 = 0.0796930342912674 + 0.01 * 6.939260482788086
Epoch 390, val loss: 0.7280248403549194
Epoch 400, training loss: 0.1406250298023224 = 0.0711650401353836 + 0.01 * 6.945999622344971
Epoch 400, val loss: 0.738691508769989
Epoch 410, training loss: 0.13303837180137634 = 0.06376875936985016 + 0.01 * 6.9269609451293945
Epoch 410, val loss: 0.7495906949043274
Epoch 420, training loss: 0.12655511498451233 = 0.05733026936650276 + 0.01 * 6.922484397888184
Epoch 420, val loss: 0.7606236934661865
Epoch 430, training loss: 0.12085497379302979 = 0.051704224199056625 + 0.01 * 6.915074825286865
Epoch 430, val loss: 0.7716772556304932
Epoch 440, training loss: 0.11609946191310883 = 0.046771809458732605 + 0.01 * 6.932765483856201
Epoch 440, val loss: 0.7827220559120178
Epoch 450, training loss: 0.11150634288787842 = 0.04244093969464302 + 0.01 * 6.9065399169921875
Epoch 450, val loss: 0.7936418652534485
Epoch 460, training loss: 0.10783902555704117 = 0.03862370550632477 + 0.01 * 6.921532154083252
Epoch 460, val loss: 0.8044351935386658
Epoch 470, training loss: 0.10429185628890991 = 0.0352494977414608 + 0.01 * 6.904236316680908
Epoch 470, val loss: 0.8150556683540344
Epoch 480, training loss: 0.10120938718318939 = 0.03225800022482872 + 0.01 * 6.895138263702393
Epoch 480, val loss: 0.825513482093811
Epoch 490, training loss: 0.09850732237100601 = 0.029599977657198906 + 0.01 * 6.890734672546387
Epoch 490, val loss: 0.8357862830162048
Epoch 500, training loss: 0.09613104909658432 = 0.02723422832787037 + 0.01 * 6.889682769775391
Epoch 500, val loss: 0.8458349108695984
Epoch 510, training loss: 0.09399577975273132 = 0.025121327489614487 + 0.01 * 6.887445449829102
Epoch 510, val loss: 0.8556690216064453
Epoch 520, training loss: 0.09208329021930695 = 0.02323117107152939 + 0.01 * 6.885212421417236
Epoch 520, val loss: 0.86528080701828
Epoch 530, training loss: 0.0903024971485138 = 0.021536126732826233 + 0.01 * 6.876636981964111
Epoch 530, val loss: 0.8746199607849121
Epoch 540, training loss: 0.08872397243976593 = 0.02001183293759823 + 0.01 * 6.871213912963867
Epoch 540, val loss: 0.8837296366691589
Epoch 550, training loss: 0.08732499182224274 = 0.018637999892234802 + 0.01 * 6.868699073791504
Epoch 550, val loss: 0.8926053643226624
Epoch 560, training loss: 0.08602893352508545 = 0.01739722117781639 + 0.01 * 6.863171577453613
Epoch 560, val loss: 0.9012053608894348
Epoch 570, training loss: 0.08479401469230652 = 0.016273949295282364 + 0.01 * 6.852006435394287
Epoch 570, val loss: 0.9095654487609863
Epoch 580, training loss: 0.0839548334479332 = 0.015254569239914417 + 0.01 * 6.870026111602783
Epoch 580, val loss: 0.917707085609436
Epoch 590, training loss: 0.0828588455915451 = 0.01432755496352911 + 0.01 * 6.853128910064697
Epoch 590, val loss: 0.9255496859550476
Epoch 600, training loss: 0.08193456381559372 = 0.013482720591127872 + 0.01 * 6.845184326171875
Epoch 600, val loss: 0.9331795573234558
Epoch 610, training loss: 0.08110150694847107 = 0.012710370123386383 + 0.01 * 6.839113712310791
Epoch 610, val loss: 0.9406067132949829
Epoch 620, training loss: 0.0803842842578888 = 0.012003462761640549 + 0.01 * 6.838082313537598
Epoch 620, val loss: 0.9477890133857727
Epoch 630, training loss: 0.0797734409570694 = 0.011355370283126831 + 0.01 * 6.8418073654174805
Epoch 630, val loss: 0.9547985196113586
Epoch 640, training loss: 0.07909998297691345 = 0.010759187862277031 + 0.01 * 6.834079265594482
Epoch 640, val loss: 0.9615619778633118
Epoch 650, training loss: 0.07849011570215225 = 0.010210664942860603 + 0.01 * 6.827945232391357
Epoch 650, val loss: 0.968122124671936
Epoch 660, training loss: 0.07787308841943741 = 0.009704910218715668 + 0.01 * 6.816817760467529
Epoch 660, val loss: 0.974502682685852
Epoch 670, training loss: 0.07739536464214325 = 0.009237119928002357 + 0.01 * 6.81582498550415
Epoch 670, val loss: 0.980681836605072
Epoch 680, training loss: 0.07690781354904175 = 0.00880400650203228 + 0.01 * 6.810380935668945
Epoch 680, val loss: 0.9866740107536316
Epoch 690, training loss: 0.07647620141506195 = 0.008402586914598942 + 0.01 * 6.807361602783203
Epoch 690, val loss: 0.9925059080123901
Epoch 700, training loss: 0.07598958909511566 = 0.008029278367757797 + 0.01 * 6.796031951904297
Epoch 700, val loss: 0.9981442093849182
Epoch 710, training loss: 0.07572918385267258 = 0.007681882940232754 + 0.01 * 6.80472993850708
Epoch 710, val loss: 1.0036503076553345
Epoch 720, training loss: 0.07532546669244766 = 0.007358278147876263 + 0.01 * 6.796719074249268
Epoch 720, val loss: 1.0089749097824097
Epoch 730, training loss: 0.07510455697774887 = 0.007055997848510742 + 0.01 * 6.804856300354004
Epoch 730, val loss: 1.0141628980636597
Epoch 740, training loss: 0.07461554557085037 = 0.006773678120225668 + 0.01 * 6.784186840057373
Epoch 740, val loss: 1.0192244052886963
Epoch 750, training loss: 0.07433770596981049 = 0.006509155035018921 + 0.01 * 6.78285551071167
Epoch 750, val loss: 1.02408766746521
Epoch 760, training loss: 0.07408042252063751 = 0.006261195056140423 + 0.01 * 6.781922817230225
Epoch 760, val loss: 1.0288245677947998
Epoch 770, training loss: 0.07380349189043045 = 0.006028559524565935 + 0.01 * 6.777493000030518
Epoch 770, val loss: 1.0334632396697998
Epoch 780, training loss: 0.07350683957338333 = 0.005809910595417023 + 0.01 * 6.769692897796631
Epoch 780, val loss: 1.0379093885421753
Epoch 790, training loss: 0.07324633747339249 = 0.005604246631264687 + 0.01 * 6.764209270477295
Epoch 790, val loss: 1.0423237085342407
Epoch 800, training loss: 0.07313358783721924 = 0.005409943405538797 + 0.01 * 6.772365093231201
Epoch 800, val loss: 1.0465878248214722
Epoch 810, training loss: 0.07278074324131012 = 0.005227158777415752 + 0.01 * 6.755358695983887
Epoch 810, val loss: 1.0507640838623047
Epoch 820, training loss: 0.07257816195487976 = 0.005054919980466366 + 0.01 * 6.75232458114624
Epoch 820, val loss: 1.0547988414764404
Epoch 830, training loss: 0.07246219366788864 = 0.004891594406217337 + 0.01 * 6.7570600509643555
Epoch 830, val loss: 1.0587081909179688
Epoch 840, training loss: 0.07222531735897064 = 0.0047371285036206245 + 0.01 * 6.748818397521973
Epoch 840, val loss: 1.062536358833313
Epoch 850, training loss: 0.07204655557870865 = 0.004591289442032576 + 0.01 * 6.7455267906188965
Epoch 850, val loss: 1.0663039684295654
Epoch 860, training loss: 0.07214677333831787 = 0.004452662076801062 + 0.01 * 6.769411563873291
Epoch 860, val loss: 1.0699281692504883
Epoch 870, training loss: 0.07181667536497116 = 0.004321264568716288 + 0.01 * 6.749541282653809
Epoch 870, val loss: 1.0735232830047607
Epoch 880, training loss: 0.071524478495121 = 0.004196589812636375 + 0.01 * 6.732789039611816
Epoch 880, val loss: 1.0769723653793335
Epoch 890, training loss: 0.07139275968074799 = 0.004077815916389227 + 0.01 * 6.731494903564453
Epoch 890, val loss: 1.080361247062683
Epoch 900, training loss: 0.0716254860162735 = 0.003965151496231556 + 0.01 * 6.766033172607422
Epoch 900, val loss: 1.0836650133132935
Epoch 910, training loss: 0.07118100672960281 = 0.0038577488157898188 + 0.01 * 6.732326507568359
Epoch 910, val loss: 1.086883783340454
Epoch 920, training loss: 0.07100359350442886 = 0.003755496349185705 + 0.01 * 6.724809646606445
Epoch 920, val loss: 1.0899972915649414
Epoch 930, training loss: 0.07110823690891266 = 0.0036579007282853127 + 0.01 * 6.745034217834473
Epoch 930, val loss: 1.0931047201156616
Epoch 940, training loss: 0.07084880024194717 = 0.003564986865967512 + 0.01 * 6.728382110595703
Epoch 940, val loss: 1.0960960388183594
Epoch 950, training loss: 0.07061871886253357 = 0.0034758937545120716 + 0.01 * 6.714282512664795
Epoch 950, val loss: 1.0990240573883057
Epoch 960, training loss: 0.07062952220439911 = 0.003391135483980179 + 0.01 * 6.723838806152344
Epoch 960, val loss: 1.1018531322479248
Epoch 970, training loss: 0.07042228430509567 = 0.0033099285792559385 + 0.01 * 6.711236000061035
Epoch 970, val loss: 1.104652762413025
Epoch 980, training loss: 0.07030433416366577 = 0.0032322481274604797 + 0.01 * 6.70720911026001
Epoch 980, val loss: 1.1073784828186035
Epoch 990, training loss: 0.07030346989631653 = 0.003157828003168106 + 0.01 * 6.714563846588135
Epoch 990, val loss: 1.1100372076034546
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 2.032599449157715 = 1.9466310739517212 + 0.01 * 8.596831321716309
Epoch 0, val loss: 1.948480486869812
Epoch 10, training loss: 2.022382974624634 = 1.9364153146743774 + 0.01 * 8.596770286560059
Epoch 10, val loss: 1.9377567768096924
Epoch 20, training loss: 2.0098137855529785 = 1.9238486289978027 + 0.01 * 8.596510887145996
Epoch 20, val loss: 1.924260139465332
Epoch 30, training loss: 1.9919670820236206 = 1.9060114622116089 + 0.01 * 8.595565795898438
Epoch 30, val loss: 1.905119776725769
Epoch 40, training loss: 1.9656492471694946 = 1.8797508478164673 + 0.01 * 8.589836120605469
Epoch 40, val loss: 1.877208948135376
Epoch 50, training loss: 1.9288512468338013 = 1.8433775901794434 + 0.01 * 8.547367095947266
Epoch 50, val loss: 1.8400291204452515
Epoch 60, training loss: 1.8850533962249756 = 1.8023134469985962 + 0.01 * 8.273991584777832
Epoch 60, val loss: 1.8020402193069458
Epoch 70, training loss: 1.8443289995193481 = 1.7645126581192017 + 0.01 * 7.981629848480225
Epoch 70, val loss: 1.7703912258148193
Epoch 80, training loss: 1.7945388555526733 = 1.7180386781692505 + 0.01 * 7.650018215179443
Epoch 80, val loss: 1.729264736175537
Epoch 90, training loss: 1.7285659313201904 = 1.6544264554977417 + 0.01 * 7.413951873779297
Epoch 90, val loss: 1.672211766242981
Epoch 100, training loss: 1.64218008518219 = 1.5691657066345215 + 0.01 * 7.3014397621154785
Epoch 100, val loss: 1.5965521335601807
Epoch 110, training loss: 1.5388751029968262 = 1.4663363695144653 + 0.01 * 7.253868103027344
Epoch 110, val loss: 1.5069403648376465
Epoch 120, training loss: 1.4276667833328247 = 1.3555766344070435 + 0.01 * 7.209018707275391
Epoch 120, val loss: 1.415311574935913
Epoch 130, training loss: 1.3153774738311768 = 1.2437275648117065 + 0.01 * 7.1649885177612305
Epoch 130, val loss: 1.3264237642288208
Epoch 140, training loss: 1.205004096031189 = 1.1337018013000488 + 0.01 * 7.13023042678833
Epoch 140, val loss: 1.2409683465957642
Epoch 150, training loss: 1.099804162979126 = 1.0287500619888306 + 0.01 * 7.105404376983643
Epoch 150, val loss: 1.160201907157898
Epoch 160, training loss: 1.0026429891586304 = 0.9317898154258728 + 0.01 * 7.085316181182861
Epoch 160, val loss: 1.0855685472488403
Epoch 170, training loss: 0.9156888127326965 = 0.8450213074684143 + 0.01 * 7.0667524337768555
Epoch 170, val loss: 1.0192378759384155
Epoch 180, training loss: 0.8397619128227234 = 0.7692180275917053 + 0.01 * 7.0543904304504395
Epoch 180, val loss: 0.9628331065177917
Epoch 190, training loss: 0.7737882733345032 = 0.7033110857009888 + 0.01 * 7.047719955444336
Epoch 190, val loss: 0.9162254333496094
Epoch 200, training loss: 0.7152234315872192 = 0.6447761058807373 + 0.01 * 7.044734954833984
Epoch 200, val loss: 0.8782326579093933
Epoch 210, training loss: 0.6611837148666382 = 0.5907636284828186 + 0.01 * 7.042009353637695
Epoch 210, val loss: 0.8468339443206787
Epoch 220, training loss: 0.6094884276390076 = 0.5390920639038086 + 0.01 * 7.039638042449951
Epoch 220, val loss: 0.8199045062065125
Epoch 230, training loss: 0.5590112209320068 = 0.4886382222175598 + 0.01 * 7.037299156188965
Epoch 230, val loss: 0.7960479855537415
Epoch 240, training loss: 0.5094953179359436 = 0.4391500651836395 + 0.01 * 7.0345234870910645
Epoch 240, val loss: 0.7747958898544312
Epoch 250, training loss: 0.46128153800964355 = 0.3909609913825989 + 0.01 * 7.0320563316345215
Epoch 250, val loss: 0.7558684945106506
Epoch 260, training loss: 0.41502684354782104 = 0.34474968910217285 + 0.01 * 7.027714252471924
Epoch 260, val loss: 0.7395262718200684
Epoch 270, training loss: 0.37161609530448914 = 0.3013816773891449 + 0.01 * 7.023442268371582
Epoch 270, val loss: 0.7260602116584778
Epoch 280, training loss: 0.33194369077682495 = 0.26175522804260254 + 0.01 * 7.018845081329346
Epoch 280, val loss: 0.715739369392395
Epoch 290, training loss: 0.2966497838497162 = 0.22651566565036774 + 0.01 * 7.013411521911621
Epoch 290, val loss: 0.7086651921272278
Epoch 300, training loss: 0.2659631669521332 = 0.19585362076759338 + 0.01 * 7.010953903198242
Epoch 300, val loss: 0.7049598693847656
Epoch 310, training loss: 0.23957857489585876 = 0.16955283284187317 + 0.01 * 7.0025739669799805
Epoch 310, val loss: 0.7044100761413574
Epoch 320, training loss: 0.21707308292388916 = 0.14709776639938354 + 0.01 * 6.997532367706299
Epoch 320, val loss: 0.70674067735672
Epoch 330, training loss: 0.19787858426570892 = 0.12793086469173431 + 0.01 * 6.994772434234619
Epoch 330, val loss: 0.7114638686180115
Epoch 340, training loss: 0.18146175146102905 = 0.11155464500188828 + 0.01 * 6.990711688995361
Epoch 340, val loss: 0.7180747985839844
Epoch 350, training loss: 0.16736635565757751 = 0.09751461446285248 + 0.01 * 6.985174179077148
Epoch 350, val loss: 0.7261055111885071
Epoch 360, training loss: 0.1552603393793106 = 0.08545144647359848 + 0.01 * 6.980889797210693
Epoch 360, val loss: 0.7351593971252441
Epoch 370, training loss: 0.14484474062919617 = 0.0750662237405777 + 0.01 * 6.977851390838623
Epoch 370, val loss: 0.744941234588623
Epoch 380, training loss: 0.13593518733978271 = 0.0661189928650856 + 0.01 * 6.981619834899902
Epoch 380, val loss: 0.7551995515823364
Epoch 390, training loss: 0.12813913822174072 = 0.058417364954948425 + 0.01 * 6.972177028656006
Epoch 390, val loss: 0.765790581703186
Epoch 400, training loss: 0.12150317430496216 = 0.05179200693964958 + 0.01 * 6.9711174964904785
Epoch 400, val loss: 0.7766485214233398
Epoch 410, training loss: 0.11577065289020538 = 0.04609072953462601 + 0.01 * 6.967992782592773
Epoch 410, val loss: 0.7876336574554443
Epoch 420, training loss: 0.11082723736763 = 0.04117472097277641 + 0.01 * 6.965251922607422
Epoch 420, val loss: 0.7986510396003723
Epoch 430, training loss: 0.10656525939702988 = 0.036930181086063385 + 0.01 * 6.963507652282715
Epoch 430, val loss: 0.8096428513526917
Epoch 440, training loss: 0.10286134481430054 = 0.03325800225138664 + 0.01 * 6.960334300994873
Epoch 440, val loss: 0.8205199241638184
Epoch 450, training loss: 0.09963740408420563 = 0.03006744012236595 + 0.01 * 6.956996440887451
Epoch 450, val loss: 0.8313040733337402
Epoch 460, training loss: 0.09683243930339813 = 0.027287596836686134 + 0.01 * 6.954484939575195
Epoch 460, val loss: 0.8419126868247986
Epoch 470, training loss: 0.09437991678714752 = 0.02485761046409607 + 0.01 * 6.952230453491211
Epoch 470, val loss: 0.8523127436637878
Epoch 480, training loss: 0.09223049879074097 = 0.022727686911821365 + 0.01 * 6.950281143188477
Epoch 480, val loss: 0.8624529838562012
Epoch 490, training loss: 0.09033748507499695 = 0.020854946225881577 + 0.01 * 6.948253631591797
Epoch 490, val loss: 0.8723706007003784
Epoch 500, training loss: 0.08863586187362671 = 0.019200297072529793 + 0.01 * 6.943556785583496
Epoch 500, val loss: 0.8820344805717468
Epoch 510, training loss: 0.0871451199054718 = 0.01773335225880146 + 0.01 * 6.941176891326904
Epoch 510, val loss: 0.8914800882339478
Epoch 520, training loss: 0.08587528020143509 = 0.016427554190158844 + 0.01 * 6.944772720336914
Epoch 520, val loss: 0.9006824493408203
Epoch 530, training loss: 0.0846429318189621 = 0.015262370929121971 + 0.01 * 6.938055992126465
Epoch 530, val loss: 0.9096043109893799
Epoch 540, training loss: 0.08354310691356659 = 0.01421829778701067 + 0.01 * 6.932480812072754
Epoch 540, val loss: 0.9183056950569153
Epoch 550, training loss: 0.08258600533008575 = 0.01327802799642086 + 0.01 * 6.930798053741455
Epoch 550, val loss: 0.9268063306808472
Epoch 560, training loss: 0.08174456655979156 = 0.01243081409484148 + 0.01 * 6.931375026702881
Epoch 560, val loss: 0.9350526332855225
Epoch 570, training loss: 0.08092331886291504 = 0.01166519708931446 + 0.01 * 6.925812721252441
Epoch 570, val loss: 0.9430769085884094
Epoch 580, training loss: 0.08022142946720123 = 0.010970156639814377 + 0.01 * 6.9251275062561035
Epoch 580, val loss: 0.9508775472640991
Epoch 590, training loss: 0.07952247560024261 = 0.010337986052036285 + 0.01 * 6.9184489250183105
Epoch 590, val loss: 0.9584447741508484
Epoch 600, training loss: 0.07892312109470367 = 0.009761398658156395 + 0.01 * 6.916172504425049
Epoch 600, val loss: 0.9657970070838928
Epoch 610, training loss: 0.07837382704019547 = 0.009233695454895496 + 0.01 * 6.914013385772705
Epoch 610, val loss: 0.9729838967323303
Epoch 620, training loss: 0.0780344307422638 = 0.008750006556510925 + 0.01 * 6.928442478179932
Epoch 620, val loss: 0.9799591898918152
Epoch 630, training loss: 0.07740914821624756 = 0.008306656032800674 + 0.01 * 6.910249710083008
Epoch 630, val loss: 0.9867117404937744
Epoch 640, training loss: 0.07694320380687714 = 0.007898261770606041 + 0.01 * 6.904493808746338
Epoch 640, val loss: 0.9933051466941833
Epoch 650, training loss: 0.07652981579303741 = 0.007521282881498337 + 0.01 * 6.900854110717773
Epoch 650, val loss: 0.9997278451919556
Epoch 660, training loss: 0.07614293694496155 = 0.007172180339694023 + 0.01 * 6.897075653076172
Epoch 660, val loss: 1.0060023069381714
Epoch 670, training loss: 0.07591421902179718 = 0.0068489909172058105 + 0.01 * 6.90652322769165
Epoch 670, val loss: 1.0121155977249146
Epoch 680, training loss: 0.0754791647195816 = 0.00654941750690341 + 0.01 * 6.892974853515625
Epoch 680, val loss: 1.0180637836456299
Epoch 690, training loss: 0.07518671452999115 = 0.0062707578763365746 + 0.01 * 6.891595840454102
Epoch 690, val loss: 1.0238511562347412
Epoch 700, training loss: 0.07487007975578308 = 0.006010841578245163 + 0.01 * 6.885923862457275
Epoch 700, val loss: 1.0295112133026123
Epoch 710, training loss: 0.07466040551662445 = 0.0057685403153300285 + 0.01 * 6.889186859130859
Epoch 710, val loss: 1.0350114107131958
Epoch 720, training loss: 0.07438909262418747 = 0.005541895050555468 + 0.01 * 6.884719371795654
Epoch 720, val loss: 1.0403996706008911
Epoch 730, training loss: 0.07411982864141464 = 0.005330003798007965 + 0.01 * 6.8789825439453125
Epoch 730, val loss: 1.0456465482711792
Epoch 740, training loss: 0.07400281727313995 = 0.0051316916942596436 + 0.01 * 6.887112617492676
Epoch 740, val loss: 1.0507380962371826
Epoch 750, training loss: 0.07370531558990479 = 0.004944858141243458 + 0.01 * 6.8760457038879395
Epoch 750, val loss: 1.0557472705841064
Epoch 760, training loss: 0.07348784804344177 = 0.004769673105329275 + 0.01 * 6.871817111968994
Epoch 760, val loss: 1.060640573501587
Epoch 770, training loss: 0.07329414784908295 = 0.0046044304035604 + 0.01 * 6.868971824645996
Epoch 770, val loss: 1.065358281135559
Epoch 780, training loss: 0.07309295982122421 = 0.004449163097888231 + 0.01 * 6.864379405975342
Epoch 780, val loss: 1.0700109004974365
Epoch 790, training loss: 0.07296052575111389 = 0.004302703309804201 + 0.01 * 6.865782260894775
Epoch 790, val loss: 1.074500322341919
Epoch 800, training loss: 0.07280442863702774 = 0.004164000507444143 + 0.01 * 6.86404275894165
Epoch 800, val loss: 1.0789397954940796
Epoch 810, training loss: 0.07257402688264847 = 0.004032961558550596 + 0.01 * 6.854106903076172
Epoch 810, val loss: 1.0832488536834717
Epoch 820, training loss: 0.07248926907777786 = 0.003909141756594181 + 0.01 * 6.858013153076172
Epoch 820, val loss: 1.0874732732772827
Epoch 830, training loss: 0.07232493907213211 = 0.0037917790468782187 + 0.01 * 6.853316307067871
Epoch 830, val loss: 1.0916038751602173
Epoch 840, training loss: 0.07227922230958939 = 0.003680712543427944 + 0.01 * 6.859850883483887
Epoch 840, val loss: 1.09566068649292
Epoch 850, training loss: 0.0719766765832901 = 0.003575272159650922 + 0.01 * 6.8401408195495605
Epoch 850, val loss: 1.0996280908584595
Epoch 860, training loss: 0.07191489636898041 = 0.0034751605708152056 + 0.01 * 6.843973636627197
Epoch 860, val loss: 1.1034331321716309
Epoch 870, training loss: 0.07176712155342102 = 0.003380212001502514 + 0.01 * 6.838691234588623
Epoch 870, val loss: 1.1072105169296265
Epoch 880, training loss: 0.07168455421924591 = 0.003290211781859398 + 0.01 * 6.8394341468811035
Epoch 880, val loss: 1.1108572483062744
Epoch 890, training loss: 0.0715278685092926 = 0.0032040670048445463 + 0.01 * 6.832380294799805
Epoch 890, val loss: 1.1144778728485107
Epoch 900, training loss: 0.07137136161327362 = 0.003122703405097127 + 0.01 * 6.82486629486084
Epoch 900, val loss: 1.118021011352539
Epoch 910, training loss: 0.07123904675245285 = 0.0030449056066572666 + 0.01 * 6.8194146156311035
Epoch 910, val loss: 1.1214067935943604
Epoch 920, training loss: 0.07106127589941025 = 0.0029704803600907326 + 0.01 * 6.809079647064209
Epoch 920, val loss: 1.1248177289962769
Epoch 930, training loss: 0.0712052434682846 = 0.0029000570066273212 + 0.01 * 6.830519199371338
Epoch 930, val loss: 1.1280326843261719
Epoch 940, training loss: 0.07097282260656357 = 0.002832630882039666 + 0.01 * 6.814019680023193
Epoch 940, val loss: 1.1312825679779053
Epoch 950, training loss: 0.0708591639995575 = 0.002768642967566848 + 0.01 * 6.809052467346191
Epoch 950, val loss: 1.1343580484390259
Epoch 960, training loss: 0.07094331830739975 = 0.002707125386223197 + 0.01 * 6.823619842529297
Epoch 960, val loss: 1.1374258995056152
Epoch 970, training loss: 0.07056418806314468 = 0.002648342866450548 + 0.01 * 6.791584491729736
Epoch 970, val loss: 1.1404597759246826
Epoch 980, training loss: 0.07062271237373352 = 0.0025921231135725975 + 0.01 * 6.803058624267578
Epoch 980, val loss: 1.1433767080307007
Epoch 990, training loss: 0.07052292674779892 = 0.002538108266890049 + 0.01 * 6.7984819412231445
Epoch 990, val loss: 1.1463083028793335
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8344754876120191
The final CL Acc:0.82099, 0.01145, The final GNN Acc:0.83606, 0.00224
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9538])
updated graph: torch.Size([2, 10574])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.054941177368164 = 1.9689724445343018 + 0.01 * 8.596881866455078
Epoch 0, val loss: 1.9720209836959839
Epoch 10, training loss: 2.0440173149108887 = 1.9580488204956055 + 0.01 * 8.596837997436523
Epoch 10, val loss: 1.9609410762786865
Epoch 20, training loss: 2.0304741859436035 = 1.9445072412490845 + 0.01 * 8.596697807312012
Epoch 20, val loss: 1.9471569061279297
Epoch 30, training loss: 2.011564016342163 = 1.9256012439727783 + 0.01 * 8.596269607543945
Epoch 30, val loss: 1.9280725717544556
Epoch 40, training loss: 1.9835293292999268 = 1.8975880146026611 + 0.01 * 8.594132423400879
Epoch 40, val loss: 1.9001951217651367
Epoch 50, training loss: 1.9438230991363525 = 1.8580385446548462 + 0.01 * 8.578457832336426
Epoch 50, val loss: 1.8622828722000122
Epoch 60, training loss: 1.8986444473266602 = 1.8135898113250732 + 0.01 * 8.505468368530273
Epoch 60, val loss: 1.8231167793273926
Epoch 70, training loss: 1.861678123474121 = 1.77888023853302 + 0.01 * 8.279784202575684
Epoch 70, val loss: 1.7949267625808716
Epoch 80, training loss: 1.8227208852767944 = 1.7410833835601807 + 0.01 * 8.16374683380127
Epoch 80, val loss: 1.7607221603393555
Epoch 90, training loss: 1.7692859172821045 = 1.689177393913269 + 0.01 * 8.010848999023438
Epoch 90, val loss: 1.715512752532959
Epoch 100, training loss: 1.6974287033081055 = 1.6183445453643799 + 0.01 * 7.908421516418457
Epoch 100, val loss: 1.6571216583251953
Epoch 110, training loss: 1.6081260442733765 = 1.5303215980529785 + 0.01 * 7.780446529388428
Epoch 110, val loss: 1.5862549543380737
Epoch 120, training loss: 1.5106172561645508 = 1.4351409673690796 + 0.01 * 7.547629356384277
Epoch 120, val loss: 1.5123926401138306
Epoch 130, training loss: 1.4170459508895874 = 1.3422300815582275 + 0.01 * 7.481587886810303
Epoch 130, val loss: 1.4413888454437256
Epoch 140, training loss: 1.3283145427703857 = 1.2539799213409424 + 0.01 * 7.433462142944336
Epoch 140, val loss: 1.3752559423446655
Epoch 150, training loss: 1.2440348863601685 = 1.1699347496032715 + 0.01 * 7.410013198852539
Epoch 150, val loss: 1.313367247581482
Epoch 160, training loss: 1.164137840270996 = 1.090317964553833 + 0.01 * 7.381989479064941
Epoch 160, val loss: 1.2562371492385864
Epoch 170, training loss: 1.0893479585647583 = 1.0157850980758667 + 0.01 * 7.3562822341918945
Epoch 170, val loss: 1.2047598361968994
Epoch 180, training loss: 1.0201457738876343 = 0.9467265605926514 + 0.01 * 7.341919422149658
Epoch 180, val loss: 1.1580170392990112
Epoch 190, training loss: 0.9559035301208496 = 0.8825485110282898 + 0.01 * 7.3354997634887695
Epoch 190, val loss: 1.114924430847168
Epoch 200, training loss: 0.8952549695968628 = 0.8219538927078247 + 0.01 * 7.330105304718018
Epoch 200, val loss: 1.0748809576034546
Epoch 210, training loss: 0.8375728130340576 = 0.764328122138977 + 0.01 * 7.324466705322266
Epoch 210, val loss: 1.0370784997940063
Epoch 220, training loss: 0.7830069065093994 = 0.7098223567008972 + 0.01 * 7.318454265594482
Epoch 220, val loss: 1.0020374059677124
Epoch 230, training loss: 0.731980562210083 = 0.6588753461837769 + 0.01 * 7.310523509979248
Epoch 230, val loss: 0.97096186876297
Epoch 240, training loss: 0.6844932436943054 = 0.6115082502365112 + 0.01 * 7.2985005378723145
Epoch 240, val loss: 0.9443761110305786
Epoch 250, training loss: 0.6402115225791931 = 0.5673952102661133 + 0.01 * 7.2816290855407715
Epoch 250, val loss: 0.9225592613220215
Epoch 260, training loss: 0.5987141132354736 = 0.5261529088020325 + 0.01 * 7.256121635437012
Epoch 260, val loss: 0.9051060080528259
Epoch 270, training loss: 0.5595122575759888 = 0.48722678422927856 + 0.01 * 7.228545188903809
Epoch 270, val loss: 0.891539990901947
Epoch 280, training loss: 0.5218945741653442 = 0.4499187767505646 + 0.01 * 7.197576999664307
Epoch 280, val loss: 0.8807082772254944
Epoch 290, training loss: 0.4852018356323242 = 0.41345879435539246 + 0.01 * 7.174302577972412
Epoch 290, val loss: 0.8719754219055176
Epoch 300, training loss: 0.44886329770088196 = 0.37726789712905884 + 0.01 * 7.159540176391602
Epoch 300, val loss: 0.8643912672996521
Epoch 310, training loss: 0.4125756025314331 = 0.3411306142807007 + 0.01 * 7.144500732421875
Epoch 310, val loss: 0.857852578163147
Epoch 320, training loss: 0.376745343208313 = 0.3053772449493408 + 0.01 * 7.136810779571533
Epoch 320, val loss: 0.852850079536438
Epoch 330, training loss: 0.3421142101287842 = 0.27078256011009216 + 0.01 * 7.1331658363342285
Epoch 330, val loss: 0.8498862385749817
Epoch 340, training loss: 0.30949223041534424 = 0.23828743398189545 + 0.01 * 7.120480537414551
Epoch 340, val loss: 0.8495316505432129
Epoch 350, training loss: 0.2798817753791809 = 0.20868298411369324 + 0.01 * 7.119879722595215
Epoch 350, val loss: 0.8520876169204712
Epoch 360, training loss: 0.2535153329372406 = 0.1824418306350708 + 0.01 * 7.107350826263428
Epoch 360, val loss: 0.8576658368110657
Epoch 370, training loss: 0.2306031435728073 = 0.15957500040531158 + 0.01 * 7.102814197540283
Epoch 370, val loss: 0.8658135533332825
Epoch 380, training loss: 0.21073152124881744 = 0.13974322378635406 + 0.01 * 7.098830223083496
Epoch 380, val loss: 0.8761441111564636
Epoch 390, training loss: 0.1934470385313034 = 0.12253367155790329 + 0.01 * 7.091336727142334
Epoch 390, val loss: 0.8879901766777039
Epoch 400, training loss: 0.17843303084373474 = 0.10756214708089828 + 0.01 * 7.087087631225586
Epoch 400, val loss: 0.901046633720398
Epoch 410, training loss: 0.16533657908439636 = 0.0945105329155922 + 0.01 * 7.082605361938477
Epoch 410, val loss: 0.9150256514549255
Epoch 420, training loss: 0.15392538905143738 = 0.08312689512968063 + 0.01 * 7.0798492431640625
Epoch 420, val loss: 0.929779589176178
Epoch 430, training loss: 0.14402678608894348 = 0.07321304827928543 + 0.01 * 7.081374168395996
Epoch 430, val loss: 0.9451737403869629
Epoch 440, training loss: 0.13532841205596924 = 0.064598448574543 + 0.01 * 7.072997570037842
Epoch 440, val loss: 0.9609171748161316
Epoch 450, training loss: 0.12778544425964355 = 0.057127054780721664 + 0.01 * 7.0658392906188965
Epoch 450, val loss: 0.9768725633621216
Epoch 460, training loss: 0.1212875097990036 = 0.05065536126494408 + 0.01 * 7.063215255737305
Epoch 460, val loss: 0.99295574426651
Epoch 470, training loss: 0.1156550794839859 = 0.045054592192173004 + 0.01 * 7.060049057006836
Epoch 470, val loss: 1.0089423656463623
Epoch 480, training loss: 0.11077387630939484 = 0.04020606353878975 + 0.01 * 7.05678129196167
Epoch 480, val loss: 1.0248053073883057
Epoch 490, training loss: 0.10649771988391876 = 0.036008406430482864 + 0.01 * 7.048932075500488
Epoch 490, val loss: 1.0403918027877808
Epoch 500, training loss: 0.10280922800302505 = 0.03237152099609375 + 0.01 * 7.043770790100098
Epoch 500, val loss: 1.0557001829147339
Epoch 510, training loss: 0.09963881969451904 = 0.029214564710855484 + 0.01 * 7.042426109313965
Epoch 510, val loss: 1.0707015991210938
Epoch 520, training loss: 0.09679572284221649 = 0.026468904688954353 + 0.01 * 7.032682418823242
Epoch 520, val loss: 1.0853129625320435
Epoch 530, training loss: 0.09446723014116287 = 0.024073852226138115 + 0.01 * 7.039337635040283
Epoch 530, val loss: 1.099581003189087
Epoch 540, training loss: 0.09224463999271393 = 0.02198139950633049 + 0.01 * 7.0263237953186035
Epoch 540, val loss: 1.113432765007019
Epoch 550, training loss: 0.0903543159365654 = 0.020145496353507042 + 0.01 * 7.0208821296691895
Epoch 550, val loss: 1.126863956451416
Epoch 560, training loss: 0.08866701275110245 = 0.018527379259467125 + 0.01 * 7.013963222503662
Epoch 560, val loss: 1.1399352550506592
Epoch 570, training loss: 0.08723388612270355 = 0.017096180468797684 + 0.01 * 7.013771057128906
Epoch 570, val loss: 1.1525776386260986
Epoch 580, training loss: 0.08589336276054382 = 0.015826085582375526 + 0.01 * 7.006727695465088
Epoch 580, val loss: 1.1648279428482056
Epoch 590, training loss: 0.08469846844673157 = 0.014695028774440289 + 0.01 * 7.000344753265381
Epoch 590, val loss: 1.1766964197158813
Epoch 600, training loss: 0.0837411880493164 = 0.01368502713739872 + 0.01 * 7.005616664886475
Epoch 600, val loss: 1.1881096363067627
Epoch 610, training loss: 0.082657091319561 = 0.012779622338712215 + 0.01 * 6.9877471923828125
Epoch 610, val loss: 1.199178695678711
Epoch 620, training loss: 0.08179889619350433 = 0.011964749544858932 + 0.01 * 6.983414649963379
Epoch 620, val loss: 1.2098534107208252
Epoch 630, training loss: 0.08106347173452377 = 0.011228601448237896 + 0.01 * 6.983487606048584
Epoch 630, val loss: 1.220215916633606
Epoch 640, training loss: 0.08032265305519104 = 0.010562221519649029 + 0.01 * 6.976043701171875
Epoch 640, val loss: 1.2302347421646118
Epoch 650, training loss: 0.07974231988191605 = 0.009957035072147846 + 0.01 * 6.9785284996032715
Epoch 650, val loss: 1.2398873567581177
Epoch 660, training loss: 0.07906858623027802 = 0.009406135417521 + 0.01 * 6.966245651245117
Epoch 660, val loss: 1.24933660030365
Epoch 670, training loss: 0.07849229127168655 = 0.0089035052806139 + 0.01 * 6.958878993988037
Epoch 670, val loss: 1.2584320306777954
Epoch 680, training loss: 0.07807258516550064 = 0.008443004451692104 + 0.01 * 6.962958335876465
Epoch 680, val loss: 1.2672722339630127
Epoch 690, training loss: 0.07754860818386078 = 0.008020441047847271 + 0.01 * 6.952817440032959
Epoch 690, val loss: 1.275829553604126
Epoch 700, training loss: 0.0771673247218132 = 0.007631904911249876 + 0.01 * 6.9535417556762695
Epoch 700, val loss: 1.2841206789016724
Epoch 710, training loss: 0.07678166031837463 = 0.007273622788488865 + 0.01 * 6.950803756713867
Epoch 710, val loss: 1.2922483682632446
Epoch 720, training loss: 0.07628509402275085 = 0.0069432039745152 + 0.01 * 6.934189319610596
Epoch 720, val loss: 1.300029993057251
Epoch 730, training loss: 0.07592109590768814 = 0.0066369702108204365 + 0.01 * 6.928412437438965
Epoch 730, val loss: 1.3076214790344238
Epoch 740, training loss: 0.07572472095489502 = 0.0063527533784508705 + 0.01 * 6.937196731567383
Epoch 740, val loss: 1.315037727355957
Epoch 750, training loss: 0.07528306543827057 = 0.006088726222515106 + 0.01 * 6.919434070587158
Epoch 750, val loss: 1.3221861124038696
Epoch 760, training loss: 0.07525128871202469 = 0.005843023303896189 + 0.01 * 6.940826892852783
Epoch 760, val loss: 1.3291575908660889
Epoch 770, training loss: 0.07470902800559998 = 0.005613917484879494 + 0.01 * 6.909511089324951
Epoch 770, val loss: 1.3359800577163696
Epoch 780, training loss: 0.07448331266641617 = 0.005400087218731642 + 0.01 * 6.908323287963867
Epoch 780, val loss: 1.3425586223602295
Epoch 790, training loss: 0.07432855665683746 = 0.005200009327381849 + 0.01 * 6.91285514831543
Epoch 790, val loss: 1.3490012884140015
Epoch 800, training loss: 0.07393564283847809 = 0.005012520123273134 + 0.01 * 6.892312526702881
Epoch 800, val loss: 1.355225920677185
Epoch 810, training loss: 0.07375781238079071 = 0.0048365346156060696 + 0.01 * 6.892127990722656
Epoch 810, val loss: 1.361287236213684
Epoch 820, training loss: 0.0736922100186348 = 0.0046713994815945625 + 0.01 * 6.90208101272583
Epoch 820, val loss: 1.3672338724136353
Epoch 830, training loss: 0.07335320860147476 = 0.004516035318374634 + 0.01 * 6.8837175369262695
Epoch 830, val loss: 1.373018741607666
Epoch 840, training loss: 0.07312143594026566 = 0.004369660280644894 + 0.01 * 6.875177383422852
Epoch 840, val loss: 1.3786166906356812
Epoch 850, training loss: 0.07301279157400131 = 0.004231484141200781 + 0.01 * 6.87813138961792
Epoch 850, val loss: 1.384090542793274
Epoch 860, training loss: 0.07281337678432465 = 0.004101073369383812 + 0.01 * 6.871230602264404
Epoch 860, val loss: 1.3895087242126465
Epoch 870, training loss: 0.07287098467350006 = 0.003977958578616381 + 0.01 * 6.889302730560303
Epoch 870, val loss: 1.3946986198425293
Epoch 880, training loss: 0.07250267267227173 = 0.0038615502417087555 + 0.01 * 6.864112854003906
Epoch 880, val loss: 1.3997972011566162
Epoch 890, training loss: 0.07234270870685577 = 0.003751204814761877 + 0.01 * 6.859150409698486
Epoch 890, val loss: 1.4047651290893555
Epoch 900, training loss: 0.07203356176614761 = 0.003646474564447999 + 0.01 * 6.838708400726318
Epoch 900, val loss: 1.4095829725265503
Epoch 910, training loss: 0.07210085541009903 = 0.003546891501173377 + 0.01 * 6.855396270751953
Epoch 910, val loss: 1.4143097400665283
Epoch 920, training loss: 0.07223179191350937 = 0.003452243283390999 + 0.01 * 6.877955436706543
Epoch 920, val loss: 1.4189990758895874
Epoch 930, training loss: 0.07183258980512619 = 0.003362614195793867 + 0.01 * 6.84699821472168
Epoch 930, val loss: 1.4234957695007324
Epoch 940, training loss: 0.07168043404817581 = 0.0032770810648798943 + 0.01 * 6.840335369110107
Epoch 940, val loss: 1.427873134613037
Epoch 950, training loss: 0.07150810956954956 = 0.003195619909092784 + 0.01 * 6.831248760223389
Epoch 950, val loss: 1.4321856498718262
Epoch 960, training loss: 0.07139039784669876 = 0.0031177797354757786 + 0.01 * 6.8272624015808105
Epoch 960, val loss: 1.436383605003357
Epoch 970, training loss: 0.07126819342374802 = 0.003043459728360176 + 0.01 * 6.822474002838135
Epoch 970, val loss: 1.4405251741409302
Epoch 980, training loss: 0.07134326547384262 = 0.002972543938085437 + 0.01 * 6.837072372436523
Epoch 980, val loss: 1.4446001052856445
Epoch 990, training loss: 0.0712175965309143 = 0.00290474365465343 + 0.01 * 6.83128547668457
Epoch 990, val loss: 1.4485111236572266
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8202424881391671
=== training gcn model ===
Epoch 0, training loss: 2.0166494846343994 = 1.930680751800537 + 0.01 * 8.596882820129395
Epoch 0, val loss: 1.932063341140747
Epoch 10, training loss: 2.0072224140167236 = 1.9212538003921509 + 0.01 * 8.596855163574219
Epoch 10, val loss: 1.9227222204208374
Epoch 20, training loss: 1.9961488246917725 = 1.9101812839508057 + 0.01 * 8.596755027770996
Epoch 20, val loss: 1.9110665321350098
Epoch 30, training loss: 1.9812670946121216 = 1.8953025341033936 + 0.01 * 8.596458435058594
Epoch 30, val loss: 1.894836664199829
Epoch 40, training loss: 1.9600046873092651 = 1.8740525245666504 + 0.01 * 8.595213890075684
Epoch 40, val loss: 1.8713339567184448
Epoch 50, training loss: 1.9304378032684326 = 1.8445641994476318 + 0.01 * 8.587362289428711
Epoch 50, val loss: 1.8396246433258057
Epoch 60, training loss: 1.8957065343856812 = 1.810250997543335 + 0.01 * 8.545557975769043
Epoch 60, val loss: 1.8062297105789185
Epoch 70, training loss: 1.86202871799469 = 1.7785815000534058 + 0.01 * 8.34472370147705
Epoch 70, val loss: 1.780837893486023
Epoch 80, training loss: 1.8224220275878906 = 1.7404452562332153 + 0.01 * 8.19768238067627
Epoch 80, val loss: 1.7513656616210938
Epoch 90, training loss: 1.7665956020355225 = 1.686388611793518 + 0.01 * 8.02070140838623
Epoch 90, val loss: 1.7072947025299072
Epoch 100, training loss: 1.6898322105407715 = 1.612208604812622 + 0.01 * 7.762364864349365
Epoch 100, val loss: 1.6456496715545654
Epoch 110, training loss: 1.5945688486099243 = 1.5196104049682617 + 0.01 * 7.495846748352051
Epoch 110, val loss: 1.5703492164611816
Epoch 120, training loss: 1.4905515909194946 = 1.4165866374969482 + 0.01 * 7.396500110626221
Epoch 120, val loss: 1.487920880317688
Epoch 130, training loss: 1.3826394081115723 = 1.3088657855987549 + 0.01 * 7.377361297607422
Epoch 130, val loss: 1.403933048248291
Epoch 140, training loss: 1.271946668624878 = 1.198387622833252 + 0.01 * 7.355908393859863
Epoch 140, val loss: 1.31893789768219
Epoch 150, training loss: 1.1614636182785034 = 1.0880367755889893 + 0.01 * 7.342679023742676
Epoch 150, val loss: 1.2352503538131714
Epoch 160, training loss: 1.0558937788009644 = 0.9825543165206909 + 0.01 * 7.3339409828186035
Epoch 160, val loss: 1.1563551425933838
Epoch 170, training loss: 0.9595341682434082 = 0.88625168800354 + 0.01 * 7.328246116638184
Epoch 170, val loss: 1.08577299118042
Epoch 180, training loss: 0.8747144341468811 = 0.8014668822288513 + 0.01 * 7.3247551918029785
Epoch 180, val loss: 1.025585412979126
Epoch 190, training loss: 0.8015658855438232 = 0.7283468246459961 + 0.01 * 7.3219075202941895
Epoch 190, val loss: 0.9762054681777954
Epoch 200, training loss: 0.738069474697113 = 0.6648824214935303 + 0.01 * 7.3187055587768555
Epoch 200, val loss: 0.9367538094520569
Epoch 210, training loss: 0.680921733379364 = 0.6077742576599121 + 0.01 * 7.314745903015137
Epoch 210, val loss: 0.9045847654342651
Epoch 220, training loss: 0.6271193623542786 = 0.5540178418159485 + 0.01 * 7.310150623321533
Epoch 220, val loss: 0.8769218921661377
Epoch 230, training loss: 0.5748085975646973 = 0.5017576813697815 + 0.01 * 7.305088520050049
Epoch 230, val loss: 0.8521729707717896
Epoch 240, training loss: 0.5233414769172668 = 0.45034360885620117 + 0.01 * 7.299788475036621
Epoch 240, val loss: 0.8300512433052063
Epoch 250, training loss: 0.47312459349632263 = 0.4001794457435608 + 0.01 * 7.294514179229736
Epoch 250, val loss: 0.8109744787216187
Epoch 260, training loss: 0.42521387338638306 = 0.3523290753364563 + 0.01 * 7.288479804992676
Epoch 260, val loss: 0.7957296967506409
Epoch 270, training loss: 0.3807081878185272 = 0.3078967034816742 + 0.01 * 7.281149387359619
Epoch 270, val loss: 0.7848289608955383
Epoch 280, training loss: 0.3402957022190094 = 0.2675694525241852 + 0.01 * 7.2726240158081055
Epoch 280, val loss: 0.7785986661911011
Epoch 290, training loss: 0.3041972219944 = 0.23162028193473816 + 0.01 * 7.257694721221924
Epoch 290, val loss: 0.7768422365188599
Epoch 300, training loss: 0.2723897397518158 = 0.19997645914554596 + 0.01 * 7.24132776260376
Epoch 300, val loss: 0.7792925834655762
Epoch 310, training loss: 0.24465304613113403 = 0.17244546115398407 + 0.01 * 7.220759391784668
Epoch 310, val loss: 0.7852909564971924
Epoch 320, training loss: 0.22069349884986877 = 0.148697629570961 + 0.01 * 7.199586391448975
Epoch 320, val loss: 0.794219434261322
Epoch 330, training loss: 0.20016226172447205 = 0.1283571422100067 + 0.01 * 7.180510997772217
Epoch 330, val loss: 0.8054043650627136
Epoch 340, training loss: 0.18289731442928314 = 0.1110273078083992 + 0.01 * 7.187000751495361
Epoch 340, val loss: 0.8183534741401672
Epoch 350, training loss: 0.1678294837474823 = 0.09632782638072968 + 0.01 * 7.150166034698486
Epoch 350, val loss: 0.8325741291046143
Epoch 360, training loss: 0.1551591455936432 = 0.0838666558265686 + 0.01 * 7.1292500495910645
Epoch 360, val loss: 0.8476069569587708
Epoch 370, training loss: 0.14450037479400635 = 0.07330391556024551 + 0.01 * 7.119645118713379
Epoch 370, val loss: 0.8632502555847168
Epoch 380, training loss: 0.13536229729652405 = 0.06435486674308777 + 0.01 * 7.100744247436523
Epoch 380, val loss: 0.8789708614349365
Epoch 390, training loss: 0.1280938684940338 = 0.056760743260383606 + 0.01 * 7.133312225341797
Epoch 390, val loss: 0.8947661519050598
Epoch 400, training loss: 0.12118978798389435 = 0.05031309649348259 + 0.01 * 7.0876688957214355
Epoch 400, val loss: 0.9103589057922363
Epoch 410, training loss: 0.11554883420467377 = 0.04481292515993118 + 0.01 * 7.073591232299805
Epoch 410, val loss: 0.9256699085235596
Epoch 420, training loss: 0.1107843667268753 = 0.04010375589132309 + 0.01 * 7.068060874938965
Epoch 420, val loss: 0.9407698512077332
Epoch 430, training loss: 0.10663606226444244 = 0.0360553003847599 + 0.01 * 7.05807638168335
Epoch 430, val loss: 0.9554886221885681
Epoch 440, training loss: 0.10309553146362305 = 0.0325620211660862 + 0.01 * 7.053350925445557
Epoch 440, val loss: 0.9698790311813354
Epoch 450, training loss: 0.10004659742116928 = 0.029538480564951897 + 0.01 * 7.050811290740967
Epoch 450, val loss: 0.9837769865989685
Epoch 460, training loss: 0.0973467230796814 = 0.026912124827504158 + 0.01 * 7.043460369110107
Epoch 460, val loss: 0.9972604513168335
Epoch 470, training loss: 0.09495379030704498 = 0.024617549031972885 + 0.01 * 7.033623695373535
Epoch 470, val loss: 1.0104516744613647
Epoch 480, training loss: 0.09286902844905853 = 0.022604160010814667 + 0.01 * 7.026486873626709
Epoch 480, val loss: 1.023219108581543
Epoch 490, training loss: 0.09103932976722717 = 0.020830631256103516 + 0.01 * 7.020869731903076
Epoch 490, val loss: 1.035550832748413
Epoch 500, training loss: 0.0894446074962616 = 0.019260575994849205 + 0.01 * 7.018403053283691
Epoch 500, val loss: 1.0475878715515137
Epoch 510, training loss: 0.08801274746656418 = 0.017864765599370003 + 0.01 * 7.014798164367676
Epoch 510, val loss: 1.0591665506362915
Epoch 520, training loss: 0.08665546774864197 = 0.016618864610791206 + 0.01 * 7.003660678863525
Epoch 520, val loss: 1.0704596042633057
Epoch 530, training loss: 0.08564004302024841 = 0.01550220139324665 + 0.01 * 7.013784408569336
Epoch 530, val loss: 1.0813522338867188
Epoch 540, training loss: 0.08443382382392883 = 0.014500569552183151 + 0.01 * 6.993325710296631
Epoch 540, val loss: 1.0919269323349
Epoch 550, training loss: 0.08346888422966003 = 0.013596298173069954 + 0.01 * 6.9872589111328125
Epoch 550, val loss: 1.1022496223449707
Epoch 560, training loss: 0.08264385163784027 = 0.012776580639183521 + 0.01 * 6.986726760864258
Epoch 560, val loss: 1.1122053861618042
Epoch 570, training loss: 0.08179367333650589 = 0.012032006867229939 + 0.01 * 6.976166248321533
Epoch 570, val loss: 1.1218920946121216
Epoch 580, training loss: 0.08106689900159836 = 0.011353305540978909 + 0.01 * 6.9713592529296875
Epoch 580, val loss: 1.131285309791565
Epoch 590, training loss: 0.0806916281580925 = 0.010732432827353477 + 0.01 * 6.995919227600098
Epoch 590, val loss: 1.140376091003418
Epoch 600, training loss: 0.07984264194965363 = 0.010166148655116558 + 0.01 * 6.967649459838867
Epoch 600, val loss: 1.1492379903793335
Epoch 610, training loss: 0.07922638207674026 = 0.009646439924836159 + 0.01 * 6.95799446105957
Epoch 610, val loss: 1.15785551071167
Epoch 620, training loss: 0.07879097014665604 = 0.009167834185063839 + 0.01 * 6.962313652038574
Epoch 620, val loss: 1.1663271188735962
Epoch 630, training loss: 0.07823292165994644 = 0.008726556785404682 + 0.01 * 6.950636386871338
Epoch 630, val loss: 1.1743828058242798
Epoch 640, training loss: 0.07777686417102814 = 0.008318479172885418 + 0.01 * 6.945838928222656
Epoch 640, val loss: 1.182326316833496
Epoch 650, training loss: 0.07743623852729797 = 0.0079398974776268 + 0.01 * 6.949634075164795
Epoch 650, val loss: 1.1901309490203857
Epoch 660, training loss: 0.07708929479122162 = 0.007588406093418598 + 0.01 * 6.9500885009765625
Epoch 660, val loss: 1.1977505683898926
Epoch 670, training loss: 0.0765993669629097 = 0.007261265534907579 + 0.01 * 6.933809757232666
Epoch 670, val loss: 1.2049912214279175
Epoch 680, training loss: 0.07632932811975479 = 0.006956859026104212 + 0.01 * 6.937246799468994
Epoch 680, val loss: 1.2123186588287354
Epoch 690, training loss: 0.07586698979139328 = 0.00667280750349164 + 0.01 * 6.9194183349609375
Epoch 690, val loss: 1.219294548034668
Epoch 700, training loss: 0.07582886517047882 = 0.006406727246940136 + 0.01 * 6.942214488983154
Epoch 700, val loss: 1.2261149883270264
Epoch 710, training loss: 0.07545530050992966 = 0.006158424075692892 + 0.01 * 6.9296875
Epoch 710, val loss: 1.2329323291778564
Epoch 720, training loss: 0.07507137209177017 = 0.0059257084503769875 + 0.01 * 6.914566516876221
Epoch 720, val loss: 1.239446997642517
Epoch 730, training loss: 0.07481355220079422 = 0.005707091186195612 + 0.01 * 6.910646438598633
Epoch 730, val loss: 1.2459481954574585
Epoch 740, training loss: 0.0745200663805008 = 0.005501892417669296 + 0.01 * 6.901817321777344
Epoch 740, val loss: 1.2520657777786255
Epoch 750, training loss: 0.07438801974058151 = 0.005308651365339756 + 0.01 * 6.907937049865723
Epoch 750, val loss: 1.258363127708435
Epoch 760, training loss: 0.07410617172718048 = 0.005126193165779114 + 0.01 * 6.897998332977295
Epoch 760, val loss: 1.2642484903335571
Epoch 770, training loss: 0.07398161292076111 = 0.004954256117343903 + 0.01 * 6.902736186981201
Epoch 770, val loss: 1.2701901197433472
Epoch 780, training loss: 0.07386992126703262 = 0.0047919717617332935 + 0.01 * 6.907794952392578
Epoch 780, val loss: 1.2759971618652344
Epoch 790, training loss: 0.07351414859294891 = 0.004638278856873512 + 0.01 * 6.887587070465088
Epoch 790, val loss: 1.2815831899642944
Epoch 800, training loss: 0.07353778183460236 = 0.004492640029639006 + 0.01 * 6.904514312744141
Epoch 800, val loss: 1.2871527671813965
Epoch 810, training loss: 0.07317158579826355 = 0.004355252720415592 + 0.01 * 6.881633758544922
Epoch 810, val loss: 1.2925559282302856
Epoch 820, training loss: 0.07318747788667679 = 0.004224629141390324 + 0.01 * 6.896285057067871
Epoch 820, val loss: 1.2979164123535156
Epoch 830, training loss: 0.07273782789707184 = 0.004100650083273649 + 0.01 * 6.863718032836914
Epoch 830, val loss: 1.3030329942703247
Epoch 840, training loss: 0.07284832745790482 = 0.0039828140288591385 + 0.01 * 6.886551856994629
Epoch 840, val loss: 1.3082020282745361
Epoch 850, training loss: 0.07245735824108124 = 0.003871328430250287 + 0.01 * 6.858603000640869
Epoch 850, val loss: 1.313140869140625
Epoch 860, training loss: 0.07233616709709167 = 0.0037649560254067183 + 0.01 * 6.857121467590332
Epoch 860, val loss: 1.3180662393569946
Epoch 870, training loss: 0.07237083464860916 = 0.003663580399006605 + 0.01 * 6.870725631713867
Epoch 870, val loss: 1.3228751420974731
Epoch 880, training loss: 0.0721442773938179 = 0.0035675084218382835 + 0.01 * 6.857676982879639
Epoch 880, val loss: 1.327531337738037
Epoch 890, training loss: 0.0719565749168396 = 0.0034756253007799387 + 0.01 * 6.848094463348389
Epoch 890, val loss: 1.3322473764419556
Epoch 900, training loss: 0.0719979777932167 = 0.003387890523299575 + 0.01 * 6.861009120941162
Epoch 900, val loss: 1.3365205526351929
Epoch 910, training loss: 0.07172764092683792 = 0.00330436322838068 + 0.01 * 6.842328071594238
Epoch 910, val loss: 1.3409674167633057
Epoch 920, training loss: 0.07183042168617249 = 0.0032244562171399593 + 0.01 * 6.860597133636475
Epoch 920, val loss: 1.3454172611236572
Epoch 930, training loss: 0.07156207412481308 = 0.00314813363365829 + 0.01 * 6.841394424438477
Epoch 930, val loss: 1.3495056629180908
Epoch 940, training loss: 0.07124866545200348 = 0.003075237153097987 + 0.01 * 6.817343235015869
Epoch 940, val loss: 1.3537734746932983
Epoch 950, training loss: 0.07130534201860428 = 0.0030053642112761736 + 0.01 * 6.829998016357422
Epoch 950, val loss: 1.357736587524414
Epoch 960, training loss: 0.07117216289043427 = 0.002938420744612813 + 0.01 * 6.8233747482299805
Epoch 960, val loss: 1.3617174625396729
Epoch 970, training loss: 0.07126840204000473 = 0.002874585334211588 + 0.01 * 6.839382171630859
Epoch 970, val loss: 1.3653829097747803
Epoch 980, training loss: 0.07093711197376251 = 0.002813490806147456 + 0.01 * 6.8123626708984375
Epoch 980, val loss: 1.3692322969436646
Epoch 990, training loss: 0.07091343402862549 = 0.0027548111975193024 + 0.01 * 6.81586217880249
Epoch 990, val loss: 1.3730385303497314
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 2.0452628135681152 = 1.9592944383621216 + 0.01 * 8.596826553344727
Epoch 0, val loss: 1.9591805934906006
Epoch 10, training loss: 2.0348453521728516 = 1.9488780498504639 + 0.01 * 8.596731185913086
Epoch 10, val loss: 1.9495617151260376
Epoch 20, training loss: 2.021634101867676 = 1.935670256614685 + 0.01 * 8.596390724182129
Epoch 20, val loss: 1.9369561672210693
Epoch 30, training loss: 2.0027658939361572 = 1.916816234588623 + 0.01 * 8.59497356414795
Epoch 30, val loss: 1.9187350273132324
Epoch 40, training loss: 1.9745285511016846 = 1.8886756896972656 + 0.01 * 8.585287094116211
Epoch 40, val loss: 1.8916722536087036
Epoch 50, training loss: 1.9347851276397705 = 1.8494625091552734 + 0.01 * 8.532256126403809
Epoch 50, val loss: 1.8555195331573486
Epoch 60, training loss: 1.8907277584075928 = 1.8076666593551636 + 0.01 * 8.306116104125977
Epoch 60, val loss: 1.82063627243042
Epoch 70, training loss: 1.8587640523910522 = 1.7767133712768555 + 0.01 * 8.205069541931152
Epoch 70, val loss: 1.795230507850647
Epoch 80, training loss: 1.8214930295944214 = 1.7405931949615479 + 0.01 * 8.08997917175293
Epoch 80, val loss: 1.7610437870025635
Epoch 90, training loss: 1.7696517705917358 = 1.6901556253433228 + 0.01 * 7.949615955352783
Epoch 90, val loss: 1.7174427509307861
Epoch 100, training loss: 1.6990470886230469 = 1.6208215951919556 + 0.01 * 7.822551250457764
Epoch 100, val loss: 1.661116600036621
Epoch 110, training loss: 1.6147305965423584 = 1.5383862257003784 + 0.01 * 7.634442329406738
Epoch 110, val loss: 1.596211314201355
Epoch 120, training loss: 1.5305250883102417 = 1.4557799100875854 + 0.01 * 7.474512577056885
Epoch 120, val loss: 1.5325567722320557
Epoch 130, training loss: 1.4521968364715576 = 1.378111720085144 + 0.01 * 7.408515453338623
Epoch 130, val loss: 1.4746700525283813
Epoch 140, training loss: 1.374817967414856 = 1.3015109300613403 + 0.01 * 7.330699920654297
Epoch 140, val loss: 1.4180899858474731
Epoch 150, training loss: 1.2939589023590088 = 1.221158504486084 + 0.01 * 7.280045509338379
Epoch 150, val loss: 1.3588027954101562
Epoch 160, training loss: 1.2086601257324219 = 1.136184811592102 + 0.01 * 7.247533321380615
Epoch 160, val loss: 1.2970901727676392
Epoch 170, training loss: 1.1212042570114136 = 1.0489126443862915 + 0.01 * 7.229163646697998
Epoch 170, val loss: 1.2353068590164185
Epoch 180, training loss: 1.0346808433532715 = 0.9625611901283264 + 0.01 * 7.21196174621582
Epoch 180, val loss: 1.1765083074569702
Epoch 190, training loss: 0.9514731764793396 = 0.8795327544212341 + 0.01 * 7.194044589996338
Epoch 190, val loss: 1.1226621866226196
Epoch 200, training loss: 0.8734351396560669 = 0.8016290664672852 + 0.01 * 7.180606365203857
Epoch 200, val loss: 1.0746766328811646
Epoch 210, training loss: 0.8019539713859558 = 0.730265200138092 + 0.01 * 7.168876647949219
Epoch 210, val loss: 1.0340632200241089
Epoch 220, training loss: 0.7376434803009033 = 0.6660351157188416 + 0.01 * 7.160836219787598
Epoch 220, val loss: 1.001563310623169
Epoch 230, training loss: 0.6799390316009521 = 0.6083845496177673 + 0.01 * 7.15545129776001
Epoch 230, val loss: 0.9770174622535706
Epoch 240, training loss: 0.6279457807540894 = 0.5564212799072266 + 0.01 * 7.152449607849121
Epoch 240, val loss: 0.9598672389984131
Epoch 250, training loss: 0.580793559551239 = 0.5092710256576538 + 0.01 * 7.152252197265625
Epoch 250, val loss: 0.9491198658943176
Epoch 260, training loss: 0.5376096963882446 = 0.4661271274089813 + 0.01 * 7.148255825042725
Epoch 260, val loss: 0.9437257051467896
Epoch 270, training loss: 0.497736394405365 = 0.4262658655643463 + 0.01 * 7.147051811218262
Epoch 270, val loss: 0.9427531957626343
Epoch 280, training loss: 0.46055907011032104 = 0.38910359144210815 + 0.01 * 7.145549297332764
Epoch 280, val loss: 0.9455850720405579
Epoch 290, training loss: 0.42567819356918335 = 0.35421809554100037 + 0.01 * 7.146010398864746
Epoch 290, val loss: 0.9518427848815918
Epoch 300, training loss: 0.39278948307037354 = 0.3213536739349365 + 0.01 * 7.143582820892334
Epoch 300, val loss: 0.9611456990242004
Epoch 310, training loss: 0.3618367314338684 = 0.29041266441345215 + 0.01 * 7.1424078941345215
Epoch 310, val loss: 0.9733814597129822
Epoch 320, training loss: 0.33286428451538086 = 0.261444091796875 + 0.01 * 7.1420183181762695
Epoch 320, val loss: 0.9884547591209412
Epoch 330, training loss: 0.30597537755966187 = 0.234565868973732 + 0.01 * 7.140949249267578
Epoch 330, val loss: 1.00606107711792
Epoch 340, training loss: 0.28130409121513367 = 0.20990091562271118 + 0.01 * 7.140317440032959
Epoch 340, val loss: 1.02616548538208
Epoch 350, training loss: 0.25891929864883423 = 0.1874953806400299 + 0.01 * 7.142391681671143
Epoch 350, val loss: 1.0486738681793213
Epoch 360, training loss: 0.2387271523475647 = 0.16733425855636597 + 0.01 * 7.139289855957031
Epoch 360, val loss: 1.0734188556671143
Epoch 370, training loss: 0.22070488333702087 = 0.14931976795196533 + 0.01 * 7.138510704040527
Epoch 370, val loss: 1.1002339124679565
Epoch 380, training loss: 0.20468345284461975 = 0.13330428302288055 + 0.01 * 7.137916088104248
Epoch 380, val loss: 1.1288024187088013
Epoch 390, training loss: 0.19050821661949158 = 0.11911742389202118 + 0.01 * 7.139078617095947
Epoch 390, val loss: 1.1588211059570312
Epoch 400, training loss: 0.1779523491859436 = 0.10658075660467148 + 0.01 * 7.137159824371338
Epoch 400, val loss: 1.1898269653320312
Epoch 410, training loss: 0.16687041521072388 = 0.0955149233341217 + 0.01 * 7.1355485916137695
Epoch 410, val loss: 1.2214924097061157
Epoch 420, training loss: 0.15709465742111206 = 0.08574764430522919 + 0.01 * 7.134700775146484
Epoch 420, val loss: 1.253433346748352
Epoch 430, training loss: 0.1484607309103012 = 0.0771242305636406 + 0.01 * 7.133650302886963
Epoch 430, val loss: 1.2853124141693115
Epoch 440, training loss: 0.14082013070583344 = 0.06950722634792328 + 0.01 * 7.131290435791016
Epoch 440, val loss: 1.3168649673461914
Epoch 450, training loss: 0.13408663868904114 = 0.06277403235435486 + 0.01 * 7.131260871887207
Epoch 450, val loss: 1.3478543758392334
Epoch 460, training loss: 0.12811815738677979 = 0.05682070180773735 + 0.01 * 7.129746437072754
Epoch 460, val loss: 1.3781864643096924
Epoch 470, training loss: 0.1228206604719162 = 0.051553916186094284 + 0.01 * 7.126674652099609
Epoch 470, val loss: 1.4078065156936646
Epoch 480, training loss: 0.11813627183437347 = 0.04689041152596474 + 0.01 * 7.1245856285095215
Epoch 480, val loss: 1.436552882194519
Epoch 490, training loss: 0.11402462422847748 = 0.04275751858949661 + 0.01 * 7.126710414886475
Epoch 490, val loss: 1.464426040649414
Epoch 500, training loss: 0.11029844731092453 = 0.03909114748239517 + 0.01 * 7.120729923248291
Epoch 500, val loss: 1.4914048910140991
Epoch 510, training loss: 0.10700908303260803 = 0.03583376109600067 + 0.01 * 7.117532253265381
Epoch 510, val loss: 1.5175098180770874
Epoch 520, training loss: 0.10411358624696732 = 0.03293386846780777 + 0.01 * 7.117971897125244
Epoch 520, val loss: 1.542758584022522
Epoch 530, training loss: 0.1014849841594696 = 0.0303473062813282 + 0.01 * 7.113768100738525
Epoch 530, val loss: 1.567150592803955
Epoch 540, training loss: 0.09913863241672516 = 0.028036031872034073 + 0.01 * 7.110260486602783
Epoch 540, val loss: 1.5907373428344727
Epoch 550, training loss: 0.0970664992928505 = 0.025966180488467216 + 0.01 * 7.110032081604004
Epoch 550, val loss: 1.6135365962982178
Epoch 560, training loss: 0.09516231715679169 = 0.024108516052365303 + 0.01 * 7.105380535125732
Epoch 560, val loss: 1.6356019973754883
Epoch 570, training loss: 0.09342436492443085 = 0.022437572479248047 + 0.01 * 7.098679065704346
Epoch 570, val loss: 1.6569740772247314
Epoch 580, training loss: 0.09191641211509705 = 0.02093050628900528 + 0.01 * 7.098590850830078
Epoch 580, val loss: 1.6776583194732666
Epoch 590, training loss: 0.0904809832572937 = 0.01956832781434059 + 0.01 * 7.091265678405762
Epoch 590, val loss: 1.6976420879364014
Epoch 600, training loss: 0.08918838948011398 = 0.018334081396460533 + 0.01 * 7.085431098937988
Epoch 600, val loss: 1.7169747352600098
Epoch 610, training loss: 0.08802542835474014 = 0.017213212326169014 + 0.01 * 7.081222057342529
Epoch 610, val loss: 1.735695719718933
Epoch 620, training loss: 0.08704142272472382 = 0.01619303598999977 + 0.01 * 7.084838390350342
Epoch 620, val loss: 1.7538268566131592
Epoch 630, training loss: 0.08612755686044693 = 0.01526255626231432 + 0.01 * 7.0864996910095215
Epoch 630, val loss: 1.7714124917984009
Epoch 640, training loss: 0.08508700877428055 = 0.014412181451916695 + 0.01 * 7.067482948303223
Epoch 640, val loss: 1.7883856296539307
Epoch 650, training loss: 0.08432646095752716 = 0.01363303605467081 + 0.01 * 7.069343090057373
Epoch 650, val loss: 1.8048677444458008
Epoch 660, training loss: 0.08351597934961319 = 0.012917931191623211 + 0.01 * 7.059805393218994
Epoch 660, val loss: 1.8207975625991821
Epoch 670, training loss: 0.08290067315101624 = 0.012260372750461102 + 0.01 * 7.064030170440674
Epoch 670, val loss: 1.8363004922866821
Epoch 680, training loss: 0.082140251994133 = 0.011654770001769066 + 0.01 * 7.048548221588135
Epoch 680, val loss: 1.8512227535247803
Epoch 690, training loss: 0.08147681504487991 = 0.011095463298261166 + 0.01 * 7.038135051727295
Epoch 690, val loss: 1.8657435178756714
Epoch 700, training loss: 0.08103907853364944 = 0.010577468201518059 + 0.01 * 7.046160697937012
Epoch 700, val loss: 1.8798198699951172
Epoch 710, training loss: 0.08058056235313416 = 0.010096659883856773 + 0.01 * 7.0483903884887695
Epoch 710, val loss: 1.893492341041565
Epoch 720, training loss: 0.07993989437818527 = 0.00965095404535532 + 0.01 * 7.028894424438477
Epoch 720, val loss: 1.9065163135528564
Epoch 730, training loss: 0.079627126455307 = 0.009236264042556286 + 0.01 * 7.03908634185791
Epoch 730, val loss: 1.91929030418396
Epoch 740, training loss: 0.07884479314088821 = 0.008850164711475372 + 0.01 * 6.999463081359863
Epoch 740, val loss: 1.9315651655197144
Epoch 750, training loss: 0.07833661139011383 = 0.008489426225423813 + 0.01 * 6.9847187995910645
Epoch 750, val loss: 1.9434694051742554
Epoch 760, training loss: 0.07822070270776749 = 0.00815252959728241 + 0.01 * 7.006817817687988
Epoch 760, val loss: 1.9550528526306152
Epoch 770, training loss: 0.0778089165687561 = 0.007837341167032719 + 0.01 * 6.997157573699951
Epoch 770, val loss: 1.9660967588424683
Epoch 780, training loss: 0.07729227095842361 = 0.007542372215539217 + 0.01 * 6.974989891052246
Epoch 780, val loss: 1.9768142700195312
Epoch 790, training loss: 0.07698585838079453 = 0.007265512831509113 + 0.01 * 6.972034454345703
Epoch 790, val loss: 1.9872705936431885
Epoch 800, training loss: 0.07669700682163239 = 0.007005742285400629 + 0.01 * 6.969126224517822
Epoch 800, val loss: 1.9972586631774902
Epoch 810, training loss: 0.07651682943105698 = 0.006761273369193077 + 0.01 * 6.975555419921875
Epoch 810, val loss: 2.0072288513183594
Epoch 820, training loss: 0.07628083229064941 = 0.006531233433634043 + 0.01 * 6.9749603271484375
Epoch 820, val loss: 2.016648054122925
Epoch 830, training loss: 0.07576240599155426 = 0.006314371712505817 + 0.01 * 6.944803237915039
Epoch 830, val loss: 2.025702476501465
Epoch 840, training loss: 0.07555169612169266 = 0.006109654903411865 + 0.01 * 6.944204330444336
Epoch 840, val loss: 2.0349056720733643
Epoch 850, training loss: 0.07537601888179779 = 0.005916493944823742 + 0.01 * 6.945952892303467
Epoch 850, val loss: 2.043313980102539
Epoch 860, training loss: 0.07497226446866989 = 0.005733798258006573 + 0.01 * 6.92384672164917
Epoch 860, val loss: 2.0518734455108643
Epoch 870, training loss: 0.07484883069992065 = 0.005560668185353279 + 0.01 * 6.928816318511963
Epoch 870, val loss: 2.060133695602417
Epoch 880, training loss: 0.07487589120864868 = 0.005396616645157337 + 0.01 * 6.947927474975586
Epoch 880, val loss: 2.0681254863739014
Epoch 890, training loss: 0.07429686188697815 = 0.005241057835519314 + 0.01 * 6.905580520629883
Epoch 890, val loss: 2.075939178466797
Epoch 900, training loss: 0.07400399446487427 = 0.005093377083539963 + 0.01 * 6.891061782836914
Epoch 900, val loss: 2.083639621734619
Epoch 910, training loss: 0.07399506121873856 = 0.004953035153448582 + 0.01 * 6.904202938079834
Epoch 910, val loss: 2.090970277786255
Epoch 920, training loss: 0.07372234016656876 = 0.004819327965378761 + 0.01 * 6.89030122756958
Epoch 920, val loss: 2.0981321334838867
Epoch 930, training loss: 0.07360219955444336 = 0.004691981710493565 + 0.01 * 6.891022205352783
Epoch 930, val loss: 2.105405569076538
Epoch 940, training loss: 0.07359311729669571 = 0.004570605233311653 + 0.01 * 6.902251243591309
Epoch 940, val loss: 2.1121814250946045
Epoch 950, training loss: 0.07321861386299133 = 0.004454697482287884 + 0.01 * 6.876392364501953
Epoch 950, val loss: 2.118939161300659
Epoch 960, training loss: 0.07311274111270905 = 0.004344052169471979 + 0.01 * 6.876869201660156
Epoch 960, val loss: 2.1255831718444824
Epoch 970, training loss: 0.07283540815114975 = 0.004238513298332691 + 0.01 * 6.859689235687256
Epoch 970, val loss: 2.131833791732788
Epoch 980, training loss: 0.07276219129562378 = 0.004137542564421892 + 0.01 * 6.862464427947998
Epoch 980, val loss: 2.1382691860198975
Epoch 990, training loss: 0.07274236530065536 = 0.004040910862386227 + 0.01 * 6.870145797729492
Epoch 990, val loss: 2.1442973613739014
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8154981549815499
The final CL Acc:0.77407, 0.02885, The final GNN Acc:0.81761, 0.00197
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13208])
remove edge: torch.Size([2, 7922])
updated graph: torch.Size([2, 10574])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0360429286956787 = 1.9500741958618164 + 0.01 * 8.596868515014648
Epoch 0, val loss: 1.948815941810608
Epoch 10, training loss: 2.0259339809417725 = 1.9399657249450684 + 0.01 * 8.596827507019043
Epoch 10, val loss: 1.939060926437378
Epoch 20, training loss: 2.0138936042785645 = 1.927927017211914 + 0.01 * 8.596668243408203
Epoch 20, val loss: 1.927078366279602
Epoch 30, training loss: 1.997618317604065 = 1.9116569757461548 + 0.01 * 8.596131324768066
Epoch 30, val loss: 1.910476803779602
Epoch 40, training loss: 1.9742064476013184 = 1.8882722854614258 + 0.01 * 8.593416213989258
Epoch 40, val loss: 1.8865277767181396
Epoch 50, training loss: 1.940913438796997 = 1.8551549911499023 + 0.01 * 8.575840950012207
Epoch 50, val loss: 1.853540301322937
Epoch 60, training loss: 1.8998169898986816 = 1.814748764038086 + 0.01 * 8.506818771362305
Epoch 60, val loss: 1.8165708780288696
Epoch 70, training loss: 1.8613219261169434 = 1.7783538103103638 + 0.01 * 8.296809196472168
Epoch 70, val loss: 1.7881542444229126
Epoch 80, training loss: 1.8227264881134033 = 1.7411946058273315 + 0.01 * 8.153189659118652
Epoch 80, val loss: 1.7567154169082642
Epoch 90, training loss: 1.7672840356826782 = 1.6880959272384644 + 0.01 * 7.918808937072754
Epoch 90, val loss: 1.7083964347839355
Epoch 100, training loss: 1.6921380758285522 = 1.6156905889511108 + 0.01 * 7.644747257232666
Epoch 100, val loss: 1.6445688009262085
Epoch 110, training loss: 1.5975170135498047 = 1.523837924003601 + 0.01 * 7.367915153503418
Epoch 110, val loss: 1.5666733980178833
Epoch 120, training loss: 1.4956519603729248 = 1.423088550567627 + 0.01 * 7.256337642669678
Epoch 120, val loss: 1.4821693897247314
Epoch 130, training loss: 1.3940974473953247 = 1.321847677230835 + 0.01 * 7.224977970123291
Epoch 130, val loss: 1.4005860090255737
Epoch 140, training loss: 1.2927627563476562 = 1.2208919525146484 + 0.01 * 7.187078475952148
Epoch 140, val loss: 1.3204450607299805
Epoch 150, training loss: 1.1913858652114868 = 1.1197619438171387 + 0.01 * 7.162395000457764
Epoch 150, val loss: 1.2405427694320679
Epoch 160, training loss: 1.092408299446106 = 1.0209711790084839 + 0.01 * 7.143716335296631
Epoch 160, val loss: 1.163182258605957
Epoch 170, training loss: 0.9993289113044739 = 0.9280434846878052 + 0.01 * 7.128540515899658
Epoch 170, val loss: 1.0914040803909302
Epoch 180, training loss: 0.9141296744346619 = 0.842959463596344 + 0.01 * 7.117021560668945
Epoch 180, val loss: 1.0263475179672241
Epoch 190, training loss: 0.8377941250801086 = 0.7667080760002136 + 0.01 * 7.108607292175293
Epoch 190, val loss: 0.9684638977050781
Epoch 200, training loss: 0.7706801295280457 = 0.6996418237686157 + 0.01 * 7.103828430175781
Epoch 200, val loss: 0.9183027744293213
Epoch 210, training loss: 0.7117007970809937 = 0.6406911015510559 + 0.01 * 7.100966453552246
Epoch 210, val loss: 0.8749832510948181
Epoch 220, training loss: 0.6581729650497437 = 0.5871753692626953 + 0.01 * 7.099760055541992
Epoch 220, val loss: 0.8366261720657349
Epoch 230, training loss: 0.6070787906646729 = 0.5360863208770752 + 0.01 * 7.099245071411133
Epoch 230, val loss: 0.8009229302406311
Epoch 240, training loss: 0.5564072728157043 = 0.48541632294654846 + 0.01 * 7.099093437194824
Epoch 240, val loss: 0.7667778134346008
Epoch 250, training loss: 0.505746066570282 = 0.4347563087940216 + 0.01 * 7.098977565765381
Epoch 250, val loss: 0.734550416469574
Epoch 260, training loss: 0.4558761715888977 = 0.3848873972892761 + 0.01 * 7.098876476287842
Epoch 260, val loss: 0.7053760886192322
Epoch 270, training loss: 0.40810102224349976 = 0.3371134400367737 + 0.01 * 7.098758697509766
Epoch 270, val loss: 0.6801536679267883
Epoch 280, training loss: 0.3636389672756195 = 0.29265332221984863 + 0.01 * 7.098565578460693
Epoch 280, val loss: 0.6595141291618347
Epoch 290, training loss: 0.32324284315109253 = 0.25226038694381714 + 0.01 * 7.098247528076172
Epoch 290, val loss: 0.6436769366264343
Epoch 300, training loss: 0.28729161620140076 = 0.21631066501140594 + 0.01 * 7.098095893859863
Epoch 300, val loss: 0.6324852705001831
Epoch 310, training loss: 0.2558932900428772 = 0.18491534888744354 + 0.01 * 7.097794055938721
Epoch 310, val loss: 0.6257472038269043
Epoch 320, training loss: 0.228926420211792 = 0.15795469284057617 + 0.01 * 7.097172260284424
Epoch 320, val loss: 0.6231963634490967
Epoch 330, training loss: 0.2060924470424652 = 0.13512766361236572 + 0.01 * 7.0964789390563965
Epoch 330, val loss: 0.6244488954544067
Epoch 340, training loss: 0.18698452413082123 = 0.11601263284683228 + 0.01 * 7.097188949584961
Epoch 340, val loss: 0.6288560628890991
Epoch 350, training loss: 0.17106422781944275 = 0.10011358559131622 + 0.01 * 7.095064163208008
Epoch 350, val loss: 0.635854184627533
Epoch 360, training loss: 0.15786972641944885 = 0.08692698180675507 + 0.01 * 7.094274997711182
Epoch 360, val loss: 0.6448189616203308
Epoch 370, training loss: 0.14691346883773804 = 0.07597971707582474 + 0.01 * 7.093376159667969
Epoch 370, val loss: 0.655096709728241
Epoch 380, training loss: 0.1377721130847931 = 0.06685294210910797 + 0.01 * 7.091917991638184
Epoch 380, val loss: 0.6663031578063965
Epoch 390, training loss: 0.1301368921995163 = 0.05919772759079933 + 0.01 * 7.093915939331055
Epoch 390, val loss: 0.6780477166175842
Epoch 400, training loss: 0.12364152818918228 = 0.0527314618229866 + 0.01 * 7.091006755828857
Epoch 400, val loss: 0.6900290846824646
Epoch 410, training loss: 0.11811204254627228 = 0.04722560569643974 + 0.01 * 7.088643550872803
Epoch 410, val loss: 0.7021052241325378
Epoch 420, training loss: 0.11336509883403778 = 0.04249947890639305 + 0.01 * 7.086562156677246
Epoch 420, val loss: 0.7141526341438293
Epoch 430, training loss: 0.10925593972206116 = 0.03841160982847214 + 0.01 * 7.084433078765869
Epoch 430, val loss: 0.7261678576469421
Epoch 440, training loss: 0.10567809641361237 = 0.03485416620969772 + 0.01 * 7.082393169403076
Epoch 440, val loss: 0.7380325794219971
Epoch 450, training loss: 0.10263349115848541 = 0.03174349665641785 + 0.01 * 7.0889997482299805
Epoch 450, val loss: 0.7497671246528625
Epoch 460, training loss: 0.09980764985084534 = 0.02901626005768776 + 0.01 * 7.07913875579834
Epoch 460, val loss: 0.7612488865852356
Epoch 470, training loss: 0.09737107157707214 = 0.02661382406949997 + 0.01 * 7.075725078582764
Epoch 470, val loss: 0.7724695205688477
Epoch 480, training loss: 0.0952204167842865 = 0.024488791823387146 + 0.01 * 7.07316255569458
Epoch 480, val loss: 0.7834426164627075
Epoch 490, training loss: 0.09331589937210083 = 0.022602196782827377 + 0.01 * 7.071371078491211
Epoch 490, val loss: 0.7941767573356628
Epoch 500, training loss: 0.09160217642784119 = 0.020923126488924026 + 0.01 * 7.067905426025391
Epoch 500, val loss: 0.8046602010726929
Epoch 510, training loss: 0.09007733315229416 = 0.019423628225922585 + 0.01 * 7.065370559692383
Epoch 510, val loss: 0.8148404359817505
Epoch 520, training loss: 0.08867943286895752 = 0.018079642206430435 + 0.01 * 7.059978485107422
Epoch 520, val loss: 0.824776828289032
Epoch 530, training loss: 0.08744269609451294 = 0.016871821135282516 + 0.01 * 7.057087421417236
Epoch 530, val loss: 0.834442675113678
Epoch 540, training loss: 0.08631249517202377 = 0.01578405313193798 + 0.01 * 7.052844047546387
Epoch 540, val loss: 0.8438223004341125
Epoch 550, training loss: 0.08528269827365875 = 0.014800225384533405 + 0.01 * 7.04824686050415
Epoch 550, val loss: 0.852983295917511
Epoch 560, training loss: 0.08433544635772705 = 0.013907927088439465 + 0.01 * 7.042752265930176
Epoch 560, val loss: 0.8618935346603394
Epoch 570, training loss: 0.08359163254499435 = 0.013096611015498638 + 0.01 * 7.049502372741699
Epoch 570, val loss: 0.8705808520317078
Epoch 580, training loss: 0.08275964111089706 = 0.012359094806015491 + 0.01 * 7.040054798126221
Epoch 580, val loss: 0.8789674639701843
Epoch 590, training loss: 0.08193949609994888 = 0.011685856617987156 + 0.01 * 7.025363922119141
Epoch 590, val loss: 0.8871064186096191
Epoch 600, training loss: 0.08124234527349472 = 0.011068813502788544 + 0.01 * 7.017353057861328
Epoch 600, val loss: 0.8950387239456177
Epoch 610, training loss: 0.08077849447727203 = 0.010501964017748833 + 0.01 * 7.027653217315674
Epoch 610, val loss: 0.902801513671875
Epoch 620, training loss: 0.07998734712600708 = 0.009980715811252594 + 0.01 * 7.0006632804870605
Epoch 620, val loss: 0.9102897047996521
Epoch 630, training loss: 0.07957444339990616 = 0.00950110238045454 + 0.01 * 7.0073347091674805
Epoch 630, val loss: 0.9176341891288757
Epoch 640, training loss: 0.07896552979946136 = 0.009058890864253044 + 0.01 * 6.990664005279541
Epoch 640, val loss: 0.9246693849563599
Epoch 650, training loss: 0.07842317223548889 = 0.008648612536489964 + 0.01 * 6.977456569671631
Epoch 650, val loss: 0.9315236210823059
Epoch 660, training loss: 0.07802598923444748 = 0.00826641358435154 + 0.01 * 6.975957870483398
Epoch 660, val loss: 0.9382656216621399
Epoch 670, training loss: 0.07766293734312057 = 0.00790952704846859 + 0.01 * 6.975341320037842
Epoch 670, val loss: 0.9447987675666809
Epoch 680, training loss: 0.07719835638999939 = 0.007577028125524521 + 0.01 * 6.962132453918457
Epoch 680, val loss: 0.951209306716919
Epoch 690, training loss: 0.07674294710159302 = 0.007266396190971136 + 0.01 * 6.94765567779541
Epoch 690, val loss: 0.957466185092926
Epoch 700, training loss: 0.07642637938261032 = 0.006974929943680763 + 0.01 * 6.945145130157471
Epoch 700, val loss: 0.9635546803474426
Epoch 710, training loss: 0.07609377056360245 = 0.006700378842651844 + 0.01 * 6.9393391609191895
Epoch 710, val loss: 0.9695488810539246
Epoch 720, training loss: 0.0757719874382019 = 0.0064420877024531364 + 0.01 * 6.932990550994873
Epoch 720, val loss: 0.9754740595817566
Epoch 730, training loss: 0.07582041621208191 = 0.006198124028742313 + 0.01 * 6.9622297286987305
Epoch 730, val loss: 0.9812445640563965
Epoch 740, training loss: 0.0751681998372078 = 0.005968867801129818 + 0.01 * 6.919933319091797
Epoch 740, val loss: 0.986821711063385
Epoch 750, training loss: 0.07501444220542908 = 0.005753178149461746 + 0.01 * 6.926126003265381
Epoch 750, val loss: 0.9923964738845825
Epoch 760, training loss: 0.07469359040260315 = 0.005550285801291466 + 0.01 * 6.91433048248291
Epoch 760, val loss: 0.9977734684944153
Epoch 770, training loss: 0.07432293891906738 = 0.005358761176466942 + 0.01 * 6.89641809463501
Epoch 770, val loss: 1.0031088590621948
Epoch 780, training loss: 0.07468518614768982 = 0.005177700892090797 + 0.01 * 6.950748920440674
Epoch 780, val loss: 1.0083104372024536
Epoch 790, training loss: 0.07391034066677094 = 0.005007395055145025 + 0.01 * 6.890294551849365
Epoch 790, val loss: 1.0132471323013306
Epoch 800, training loss: 0.07389364391565323 = 0.004846332594752312 + 0.01 * 6.904730796813965
Epoch 800, val loss: 1.0183308124542236
Epoch 810, training loss: 0.07348067313432693 = 0.004693715833127499 + 0.01 * 6.878695487976074
Epoch 810, val loss: 1.0230683088302612
Epoch 820, training loss: 0.07325242459774017 = 0.0045493426732718945 + 0.01 * 6.870308876037598
Epoch 820, val loss: 1.0278573036193848
Epoch 830, training loss: 0.0730907991528511 = 0.0044127837754786015 + 0.01 * 6.867801666259766
Epoch 830, val loss: 1.0324901342391968
Epoch 840, training loss: 0.07278471440076828 = 0.004283176269382238 + 0.01 * 6.850153923034668
Epoch 840, val loss: 1.0370373725891113
Epoch 850, training loss: 0.07284776121377945 = 0.004160022828727961 + 0.01 * 6.868773937225342
Epoch 850, val loss: 1.0414113998413086
Epoch 860, training loss: 0.07250199466943741 = 0.0040432061068713665 + 0.01 * 6.845879077911377
Epoch 860, val loss: 1.0457321405410767
Epoch 870, training loss: 0.07343064248561859 = 0.0039318460039794445 + 0.01 * 6.949880123138428
Epoch 870, val loss: 1.050102949142456
Epoch 880, training loss: 0.07233106344938278 = 0.003827085718512535 + 0.01 * 6.850398063659668
Epoch 880, val loss: 1.0540693998336792
Epoch 890, training loss: 0.0721384808421135 = 0.003727235598489642 + 0.01 * 6.841124534606934
Epoch 890, val loss: 1.0581328868865967
Epoch 900, training loss: 0.07178761065006256 = 0.0036319666542112827 + 0.01 * 6.81556510925293
Epoch 900, val loss: 1.061963438987732
Epoch 910, training loss: 0.07168769091367722 = 0.0035409608390182257 + 0.01 * 6.814672946929932
Epoch 910, val loss: 1.0659910440444946
Epoch 920, training loss: 0.07175181061029434 = 0.003454225603491068 + 0.01 * 6.829759120941162
Epoch 920, val loss: 1.0696067810058594
Epoch 930, training loss: 0.07172564417123795 = 0.0033712568692862988 + 0.01 * 6.835439205169678
Epoch 930, val loss: 1.073380947113037
Epoch 940, training loss: 0.07142963260412216 = 0.0032919563818722963 + 0.01 * 6.813767910003662
Epoch 940, val loss: 1.0768320560455322
Epoch 950, training loss: 0.07117114961147308 = 0.003216042649000883 + 0.01 * 6.795510768890381
Epoch 950, val loss: 1.080453872680664
Epoch 960, training loss: 0.07113493233919144 = 0.00314333732239902 + 0.01 * 6.799159526824951
Epoch 960, val loss: 1.08391273021698
Epoch 970, training loss: 0.0712442398071289 = 0.0030736590269953012 + 0.01 * 6.817058086395264
Epoch 970, val loss: 1.087294101715088
Epoch 980, training loss: 0.07088987529277802 = 0.003007106715813279 + 0.01 * 6.7882771492004395
Epoch 980, val loss: 1.0905711650848389
Epoch 990, training loss: 0.0707908645272255 = 0.002943095052614808 + 0.01 * 6.78477668762207
Epoch 990, val loss: 1.0938245058059692
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.03849196434021 = 1.9525234699249268 + 0.01 * 8.596846580505371
Epoch 0, val loss: 1.9595954418182373
Epoch 10, training loss: 2.028064250946045 = 1.94209623336792 + 0.01 * 8.596793174743652
Epoch 10, val loss: 1.9486244916915894
Epoch 20, training loss: 2.0155723094940186 = 1.9296064376831055 + 0.01 * 8.596589088439941
Epoch 20, val loss: 1.9350782632827759
Epoch 30, training loss: 1.9984722137451172 = 1.9125137329101562 + 0.01 * 8.595852851867676
Epoch 30, val loss: 1.9163382053375244
Epoch 40, training loss: 1.9734809398651123 = 1.8875691890716553 + 0.01 * 8.591180801391602
Epoch 40, val loss: 1.8891268968582153
Epoch 50, training loss: 1.9378273487091064 = 1.8522367477416992 + 0.01 * 8.559060096740723
Epoch 50, val loss: 1.8517299890518188
Epoch 60, training loss: 1.8944900035858154 = 1.8101818561553955 + 0.01 * 8.43081283569336
Epoch 60, val loss: 1.81047523021698
Epoch 70, training loss: 1.8538225889205933 = 1.7716186046600342 + 0.01 * 8.220394134521484
Epoch 70, val loss: 1.776600956916809
Epoch 80, training loss: 1.8097549676895142 = 1.7288748025894165 + 0.01 * 8.088021278381348
Epoch 80, val loss: 1.7394031286239624
Epoch 90, training loss: 1.7481794357299805 = 1.6698578596115112 + 0.01 * 7.832156658172607
Epoch 90, val loss: 1.6867640018463135
Epoch 100, training loss: 1.6675453186035156 = 1.591496467590332 + 0.01 * 7.6048903465271
Epoch 100, val loss: 1.6181737184524536
Epoch 110, training loss: 1.5716829299926758 = 1.497323751449585 + 0.01 * 7.435923099517822
Epoch 110, val loss: 1.5377728939056396
Epoch 120, training loss: 1.4715576171875 = 1.3980954885482788 + 0.01 * 7.346209526062012
Epoch 120, val loss: 1.4544544219970703
Epoch 130, training loss: 1.3718606233596802 = 1.298958659172058 + 0.01 * 7.2902021408081055
Epoch 130, val loss: 1.3742542266845703
Epoch 140, training loss: 1.2729822397232056 = 1.2004352807998657 + 0.01 * 7.254693031311035
Epoch 140, val loss: 1.2976185083389282
Epoch 150, training loss: 1.1773751974105835 = 1.1050257682800293 + 0.01 * 7.234940528869629
Epoch 150, val loss: 1.2255035638809204
Epoch 160, training loss: 1.0882512331008911 = 1.0160242319107056 + 0.01 * 7.222699165344238
Epoch 160, val loss: 1.1591116189956665
Epoch 170, training loss: 1.0065505504608154 = 0.9344453811645508 + 0.01 * 7.210512638092041
Epoch 170, val loss: 1.098901629447937
Epoch 180, training loss: 0.9309774041175842 = 0.8590425252914429 + 0.01 * 7.193490505218506
Epoch 180, val loss: 1.0429497957229614
Epoch 190, training loss: 0.8595170378684998 = 0.7878249883651733 + 0.01 * 7.169204235076904
Epoch 190, val loss: 0.9895818829536438
Epoch 200, training loss: 0.7913663387298584 = 0.7199417352676392 + 0.01 * 7.142459869384766
Epoch 200, val loss: 0.9382308721542358
Epoch 210, training loss: 0.7264723181724548 = 0.6552944779396057 + 0.01 * 7.117781639099121
Epoch 210, val loss: 0.8902231454849243
Epoch 220, training loss: 0.6647390127182007 = 0.5937511324882507 + 0.01 * 7.098788261413574
Epoch 220, val loss: 0.8471142053604126
Epoch 230, training loss: 0.6059163808822632 = 0.5351327061653137 + 0.01 * 7.078364372253418
Epoch 230, val loss: 0.8105291128158569
Epoch 240, training loss: 0.5504347085952759 = 0.47981011867523193 + 0.01 * 7.062457084655762
Epoch 240, val loss: 0.7811816334724426
Epoch 250, training loss: 0.49925610423088074 = 0.42869460582733154 + 0.01 * 7.056149959564209
Epoch 250, val loss: 0.7589674592018127
Epoch 260, training loss: 0.4528733491897583 = 0.3824215829372406 + 0.01 * 7.045178413391113
Epoch 260, val loss: 0.7431101202964783
Epoch 270, training loss: 0.41130632162094116 = 0.34088224172592163 + 0.01 * 7.042407512664795
Epoch 270, val loss: 0.7326103448867798
Epoch 280, training loss: 0.3737921416759491 = 0.3034992516040802 + 0.01 * 7.029289722442627
Epoch 280, val loss: 0.7264049053192139
Epoch 290, training loss: 0.3397781550884247 = 0.26954150199890137 + 0.01 * 7.023665428161621
Epoch 290, val loss: 0.7236003279685974
Epoch 300, training loss: 0.3087816834449768 = 0.23855768144130707 + 0.01 * 7.022401809692383
Epoch 300, val loss: 0.723199188709259
Epoch 310, training loss: 0.28060898184776306 = 0.21045152842998505 + 0.01 * 7.015746593475342
Epoch 310, val loss: 0.7247090935707092
Epoch 320, training loss: 0.2553347945213318 = 0.18525928258895874 + 0.01 * 7.007551193237305
Epoch 320, val loss: 0.7279545068740845
Epoch 330, training loss: 0.23308035731315613 = 0.1629888117313385 + 0.01 * 7.0091552734375
Epoch 330, val loss: 0.7329322099685669
Epoch 340, training loss: 0.21348872780799866 = 0.14353938400745392 + 0.01 * 6.994935512542725
Epoch 340, val loss: 0.7396239638328552
Epoch 350, training loss: 0.1965900957584381 = 0.12666605412960052 + 0.01 * 6.992404937744141
Epoch 350, val loss: 0.7479028701782227
Epoch 360, training loss: 0.18194223940372467 = 0.11207867413759232 + 0.01 * 6.986356735229492
Epoch 360, val loss: 0.7576246857643127
Epoch 370, training loss: 0.16936971247196198 = 0.09948302805423737 + 0.01 * 6.988668918609619
Epoch 370, val loss: 0.7685849070549011
Epoch 380, training loss: 0.1583980768918991 = 0.08860079199075699 + 0.01 * 6.979728698730469
Epoch 380, val loss: 0.7805114388465881
Epoch 390, training loss: 0.148915097117424 = 0.07916612923145294 + 0.01 * 6.9748969078063965
Epoch 390, val loss: 0.7930929064750671
Epoch 400, training loss: 0.1408534049987793 = 0.070957250893116 + 0.01 * 6.9896159172058105
Epoch 400, val loss: 0.8061413764953613
Epoch 410, training loss: 0.13349831104278564 = 0.06380431354045868 + 0.01 * 6.9693989753723145
Epoch 410, val loss: 0.8194833993911743
Epoch 420, training loss: 0.12718641757965088 = 0.05754692107439041 + 0.01 * 6.9639506340026855
Epoch 420, val loss: 0.8329102396965027
Epoch 430, training loss: 0.12163496017456055 = 0.052053432911634445 + 0.01 * 6.958152770996094
Epoch 430, val loss: 0.8463738560676575
Epoch 440, training loss: 0.11681371927261353 = 0.04721730202436447 + 0.01 * 6.959641933441162
Epoch 440, val loss: 0.8597579598426819
Epoch 450, training loss: 0.11247377097606659 = 0.042954374104738235 + 0.01 * 6.951939105987549
Epoch 450, val loss: 0.8729894757270813
Epoch 460, training loss: 0.10864181816577911 = 0.03918645158410072 + 0.01 * 6.945536136627197
Epoch 460, val loss: 0.8860208988189697
Epoch 470, training loss: 0.10534864664077759 = 0.035847533494234085 + 0.01 * 6.950110912322998
Epoch 470, val loss: 0.8988342881202698
Epoch 480, training loss: 0.10230562090873718 = 0.032884128391742706 + 0.01 * 6.942149639129639
Epoch 480, val loss: 0.9113801717758179
Epoch 490, training loss: 0.09958647191524506 = 0.030246809124946594 + 0.01 * 6.933966636657715
Epoch 490, val loss: 0.9236902594566345
Epoch 500, training loss: 0.09739922732114792 = 0.02789396047592163 + 0.01 * 6.950526714324951
Epoch 500, val loss: 0.9356836676597595
Epoch 510, training loss: 0.09505730122327805 = 0.02579275704920292 + 0.01 * 6.926454544067383
Epoch 510, val loss: 0.9473915100097656
Epoch 520, training loss: 0.0932035967707634 = 0.023909524083137512 + 0.01 * 6.929407596588135
Epoch 520, val loss: 0.958782970905304
Epoch 530, training loss: 0.09143486618995667 = 0.022217782214283943 + 0.01 * 6.921709060668945
Epoch 530, val loss: 0.9698720574378967
Epoch 540, training loss: 0.08992958813905716 = 0.020693732425570488 + 0.01 * 6.923585414886475
Epoch 540, val loss: 0.9806584119796753
Epoch 550, training loss: 0.08848180621862411 = 0.019317641854286194 + 0.01 * 6.916416645050049
Epoch 550, val loss: 0.9911753535270691
Epoch 560, training loss: 0.0871795192360878 = 0.01807195506989956 + 0.01 * 6.9107561111450195
Epoch 560, val loss: 1.0013726949691772
Epoch 570, training loss: 0.08613094687461853 = 0.01694110408425331 + 0.01 * 6.9189839363098145
Epoch 570, val loss: 1.0112978219985962
Epoch 580, training loss: 0.08503791689872742 = 0.015914157032966614 + 0.01 * 6.9123759269714355
Epoch 580, val loss: 1.0209197998046875
Epoch 590, training loss: 0.08398367464542389 = 0.014978015795350075 + 0.01 * 6.900566577911377
Epoch 590, val loss: 1.0302767753601074
Epoch 600, training loss: 0.083133265376091 = 0.014122717082500458 + 0.01 * 6.901054859161377
Epoch 600, val loss: 1.0393640995025635
Epoch 610, training loss: 0.08226504176855087 = 0.013339866884052753 + 0.01 * 6.892517566680908
Epoch 610, val loss: 1.0482091903686523
Epoch 620, training loss: 0.08149620145559311 = 0.01262127049267292 + 0.01 * 6.887493133544922
Epoch 620, val loss: 1.0568186044692993
Epoch 630, training loss: 0.08089590072631836 = 0.011960206553339958 + 0.01 * 6.893569469451904
Epoch 630, val loss: 1.0651758909225464
Epoch 640, training loss: 0.08027619868516922 = 0.011351672932505608 + 0.01 * 6.892452716827393
Epoch 640, val loss: 1.0733084678649902
Epoch 650, training loss: 0.07958681136369705 = 0.010790475644171238 + 0.01 * 6.879633903503418
Epoch 650, val loss: 1.0811978578567505
Epoch 660, training loss: 0.07904444634914398 = 0.010271675884723663 + 0.01 * 6.877277374267578
Epoch 660, val loss: 1.0888442993164062
Epoch 670, training loss: 0.07853231579065323 = 0.009790940210223198 + 0.01 * 6.8741374015808105
Epoch 670, val loss: 1.096291184425354
Epoch 680, training loss: 0.07800567895174026 = 0.009344538673758507 + 0.01 * 6.866114139556885
Epoch 680, val loss: 1.1035327911376953
Epoch 690, training loss: 0.07785634696483612 = 0.008929532021284103 + 0.01 * 6.892682075500488
Epoch 690, val loss: 1.1105881929397583
Epoch 700, training loss: 0.07727275788784027 = 0.008544480428099632 + 0.01 * 6.872827529907227
Epoch 700, val loss: 1.1174241304397583
Epoch 710, training loss: 0.07676281034946442 = 0.008185512386262417 + 0.01 * 6.857729911804199
Epoch 710, val loss: 1.1240839958190918
Epoch 720, training loss: 0.07641023397445679 = 0.007849965244531631 + 0.01 * 6.856027603149414
Epoch 720, val loss: 1.130553126335144
Epoch 730, training loss: 0.07627132534980774 = 0.007535920478403568 + 0.01 * 6.873540878295898
Epoch 730, val loss: 1.1368809938430786
Epoch 740, training loss: 0.07574979215860367 = 0.007242043036967516 + 0.01 * 6.850774765014648
Epoch 740, val loss: 1.1430599689483643
Epoch 750, training loss: 0.07545419037342072 = 0.006966384127736092 + 0.01 * 6.848781108856201
Epoch 750, val loss: 1.1490787267684937
Epoch 760, training loss: 0.07510615885257721 = 0.006707560271024704 + 0.01 * 6.839859485626221
Epoch 760, val loss: 1.1549158096313477
Epoch 770, training loss: 0.07494828850030899 = 0.006464326288551092 + 0.01 * 6.848396301269531
Epoch 770, val loss: 1.160628080368042
Epoch 780, training loss: 0.07470256090164185 = 0.006235126871615648 + 0.01 * 6.846743106842041
Epoch 780, val loss: 1.1662178039550781
Epoch 790, training loss: 0.07447779178619385 = 0.006019818130880594 + 0.01 * 6.845797538757324
Epoch 790, val loss: 1.171626329421997
Epoch 800, training loss: 0.07407303899526596 = 0.005816787946969271 + 0.01 * 6.825624942779541
Epoch 800, val loss: 1.1769202947616577
Epoch 810, training loss: 0.07397955656051636 = 0.005624737590551376 + 0.01 * 6.835481643676758
Epoch 810, val loss: 1.1820976734161377
Epoch 820, training loss: 0.07366666197776794 = 0.005443147383630276 + 0.01 * 6.822351455688477
Epoch 820, val loss: 1.1871522665023804
Epoch 830, training loss: 0.07351265847682953 = 0.005271272733807564 + 0.01 * 6.82413911819458
Epoch 830, val loss: 1.1920640468597412
Epoch 840, training loss: 0.07355614751577377 = 0.005108631681650877 + 0.01 * 6.844751834869385
Epoch 840, val loss: 1.1968635320663452
Epoch 850, training loss: 0.07308513671159744 = 0.004954764153808355 + 0.01 * 6.813037872314453
Epoch 850, val loss: 1.2015835046768188
Epoch 860, training loss: 0.07300686836242676 = 0.004808499477803707 + 0.01 * 6.8198370933532715
Epoch 860, val loss: 1.2061567306518555
Epoch 870, training loss: 0.07280857861042023 = 0.004669556859880686 + 0.01 * 6.813902854919434
Epoch 870, val loss: 1.2106435298919678
Epoch 880, training loss: 0.07271423190832138 = 0.004537446424365044 + 0.01 * 6.817678451538086
Epoch 880, val loss: 1.2150453329086304
Epoch 890, training loss: 0.07240831106901169 = 0.004411682486534119 + 0.01 * 6.799663066864014
Epoch 890, val loss: 1.2192890644073486
Epoch 900, training loss: 0.07242541760206223 = 0.004292021039873362 + 0.01 * 6.813340187072754
Epoch 900, val loss: 1.2234790325164795
Epoch 910, training loss: 0.07216023653745651 = 0.004178265575319529 + 0.01 * 6.7981977462768555
Epoch 910, val loss: 1.2275460958480835
Epoch 920, training loss: 0.07205941528081894 = 0.004069515969604254 + 0.01 * 6.798990249633789
Epoch 920, val loss: 1.2315785884857178
Epoch 930, training loss: 0.07193960249423981 = 0.00396574754267931 + 0.01 * 6.7973856925964355
Epoch 930, val loss: 1.2354440689086914
Epoch 940, training loss: 0.0717463418841362 = 0.003866886254400015 + 0.01 * 6.787945747375488
Epoch 940, val loss: 1.2392467260360718
Epoch 950, training loss: 0.0717238262295723 = 0.003772241994738579 + 0.01 * 6.795158386230469
Epoch 950, val loss: 1.2429883480072021
Epoch 960, training loss: 0.07146535813808441 = 0.0036817032378166914 + 0.01 * 6.778365612030029
Epoch 960, val loss: 1.2466496229171753
Epoch 970, training loss: 0.07141941785812378 = 0.003595123067498207 + 0.01 * 6.7824296951293945
Epoch 970, val loss: 1.2501437664031982
Epoch 980, training loss: 0.07134904712438583 = 0.00351221882738173 + 0.01 * 6.783682823181152
Epoch 980, val loss: 1.2536693811416626
Epoch 990, training loss: 0.07118438184261322 = 0.003432851517572999 + 0.01 * 6.775153160095215
Epoch 990, val loss: 1.2570207118988037
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 2.027520179748535 = 1.9415515661239624 + 0.01 * 8.596872329711914
Epoch 0, val loss: 1.9407469034194946
Epoch 10, training loss: 2.0173964500427246 = 1.93142831325531 + 0.01 * 8.59682559967041
Epoch 10, val loss: 1.930435299873352
Epoch 20, training loss: 2.004973888397217 = 1.9190071821212769 + 0.01 * 8.596674919128418
Epoch 20, val loss: 1.917688250541687
Epoch 30, training loss: 1.9878344535827637 = 1.9018726348876953 + 0.01 * 8.596186637878418
Epoch 30, val loss: 1.9002251625061035
Epoch 40, training loss: 1.9630225896835327 = 1.8770875930786133 + 0.01 * 8.59350299835205
Epoch 40, val loss: 1.8753548860549927
Epoch 50, training loss: 1.9287183284759521 = 1.8429882526397705 + 0.01 * 8.573009490966797
Epoch 50, val loss: 1.8424354791641235
Epoch 60, training loss: 1.888339638710022 = 1.8036381006240845 + 0.01 * 8.4701566696167
Epoch 60, val loss: 1.8072030544281006
Epoch 70, training loss: 1.8493705987930298 = 1.7669761180877686 + 0.01 * 8.239444732666016
Epoch 70, val loss: 1.7754905223846436
Epoch 80, training loss: 1.8033983707427979 = 1.7229089736938477 + 0.01 * 8.048944473266602
Epoch 80, val loss: 1.7333029508590698
Epoch 90, training loss: 1.7385282516479492 = 1.6607096195220947 + 0.01 * 7.781861305236816
Epoch 90, val loss: 1.6747580766677856
Epoch 100, training loss: 1.6544464826583862 = 1.578782558441162 + 0.01 * 7.5663886070251465
Epoch 100, val loss: 1.6018339395523071
Epoch 110, training loss: 1.5555962324142456 = 1.4813638925552368 + 0.01 * 7.423229217529297
Epoch 110, val loss: 1.517048954963684
Epoch 120, training loss: 1.4496251344680786 = 1.3761165142059326 + 0.01 * 7.350864410400391
Epoch 120, val loss: 1.4258424043655396
Epoch 130, training loss: 1.3405206203460693 = 1.267684817314148 + 0.01 * 7.283586025238037
Epoch 130, val loss: 1.33416748046875
Epoch 140, training loss: 1.231097936630249 = 1.158582091331482 + 0.01 * 7.25158166885376
Epoch 140, val loss: 1.2444771528244019
Epoch 150, training loss: 1.1238954067230225 = 1.051520824432373 + 0.01 * 7.237452983856201
Epoch 150, val loss: 1.1588082313537598
Epoch 160, training loss: 1.0212678909301758 = 0.9490306973457336 + 0.01 * 7.223720550537109
Epoch 160, val loss: 1.0778534412384033
Epoch 170, training loss: 0.9247796535491943 = 0.8527050614356995 + 0.01 * 7.207459449768066
Epoch 170, val loss: 1.002631664276123
Epoch 180, training loss: 0.8354382514953613 = 0.7635578513145447 + 0.01 * 7.188042163848877
Epoch 180, val loss: 0.9333096146583557
Epoch 190, training loss: 0.7539873123168945 = 0.6822813749313354 + 0.01 * 7.170590877532959
Epoch 190, val loss: 0.8710135817527771
Epoch 200, training loss: 0.6805115342140198 = 0.6089562773704529 + 0.01 * 7.155524253845215
Epoch 200, val loss: 0.8170150518417358
Epoch 210, training loss: 0.6143531799316406 = 0.5429099202156067 + 0.01 * 7.144322872161865
Epoch 210, val loss: 0.7716734409332275
Epoch 220, training loss: 0.5546479225158691 = 0.4833095967769623 + 0.01 * 7.133831977844238
Epoch 220, val loss: 0.7345929741859436
Epoch 230, training loss: 0.500771164894104 = 0.4294935464859009 + 0.01 * 7.127761363983154
Epoch 230, val loss: 0.7047168016433716
Epoch 240, training loss: 0.4521848261356354 = 0.38092440366744995 + 0.01 * 7.12604284286499
Epoch 240, val loss: 0.6807546019554138
Epoch 250, training loss: 0.40831249952316284 = 0.3370995819568634 + 0.01 * 7.121292591094971
Epoch 250, val loss: 0.6617894768714905
Epoch 260, training loss: 0.36881691217422485 = 0.297627717256546 + 0.01 * 7.118918418884277
Epoch 260, val loss: 0.6471481919288635
Epoch 270, training loss: 0.333379864692688 = 0.26220929622650146 + 0.01 * 7.1170573234558105
Epoch 270, val loss: 0.6362624168395996
Epoch 280, training loss: 0.3017491400241852 = 0.23059582710266113 + 0.01 * 7.115332126617432
Epoch 280, val loss: 0.628677487373352
Epoch 290, training loss: 0.27374666929244995 = 0.20259521901607513 + 0.01 * 7.115146160125732
Epoch 290, val loss: 0.6240079402923584
Epoch 300, training loss: 0.2491230070590973 = 0.17799976468086243 + 0.01 * 7.1123247146606445
Epoch 300, val loss: 0.6218565106391907
Epoch 310, training loss: 0.22766481339931488 = 0.15655498206615448 + 0.01 * 7.110983371734619
Epoch 310, val loss: 0.6219493746757507
Epoch 320, training loss: 0.20905840396881104 = 0.13795799016952515 + 0.01 * 7.110042095184326
Epoch 320, val loss: 0.6239884495735168
Epoch 330, training loss: 0.19295495748519897 = 0.12187441438436508 + 0.01 * 7.108053684234619
Epoch 330, val loss: 0.6277210116386414
Epoch 340, training loss: 0.17904675006866455 = 0.10797947645187378 + 0.01 * 7.106727123260498
Epoch 340, val loss: 0.6328718066215515
Epoch 350, training loss: 0.16701486706733704 = 0.09596459567546844 + 0.01 * 7.105027198791504
Epoch 350, val loss: 0.6391876339912415
Epoch 360, training loss: 0.15658003091812134 = 0.08555303514003754 + 0.01 * 7.102698802947998
Epoch 360, val loss: 0.6464378237724304
Epoch 370, training loss: 0.1475401222705841 = 0.07650592178106308 + 0.01 * 7.103421211242676
Epoch 370, val loss: 0.654421865940094
Epoch 380, training loss: 0.13960522413253784 = 0.06861673295497894 + 0.01 * 7.098848819732666
Epoch 380, val loss: 0.6629480123519897
Epoch 390, training loss: 0.13267888128757477 = 0.0617123618721962 + 0.01 * 7.096652030944824
Epoch 390, val loss: 0.6719347834587097
Epoch 400, training loss: 0.12658359110355377 = 0.05564609169960022 + 0.01 * 7.09375
Epoch 400, val loss: 0.6812518239021301
Epoch 410, training loss: 0.12120676040649414 = 0.050299469381570816 + 0.01 * 7.090728759765625
Epoch 410, val loss: 0.6908069849014282
Epoch 420, training loss: 0.11645415425300598 = 0.04557863622903824 + 0.01 * 7.087551593780518
Epoch 420, val loss: 0.7005465626716614
Epoch 430, training loss: 0.11224769800901413 = 0.04140438884496689 + 0.01 * 7.084331035614014
Epoch 430, val loss: 0.7103568315505981
Epoch 440, training loss: 0.10851229727268219 = 0.03770636394619942 + 0.01 * 7.080593109130859
Epoch 440, val loss: 0.7201878428459167
Epoch 450, training loss: 0.10521625727415085 = 0.03442661464214325 + 0.01 * 7.0789642333984375
Epoch 450, val loss: 0.7300081253051758
Epoch 460, training loss: 0.10224318504333496 = 0.031513042747974396 + 0.01 * 7.073014259338379
Epoch 460, val loss: 0.7397594451904297
Epoch 470, training loss: 0.09959772229194641 = 0.028917761519551277 + 0.01 * 7.067996501922607
Epoch 470, val loss: 0.7494000196456909
Epoch 480, training loss: 0.09725183248519897 = 0.026603832840919495 + 0.01 * 7.064799785614014
Epoch 480, val loss: 0.7589368224143982
Epoch 490, training loss: 0.09512434154748917 = 0.02453792653977871 + 0.01 * 7.05864143371582
Epoch 490, val loss: 0.768294632434845
Epoch 500, training loss: 0.0932215005159378 = 0.022687815129756927 + 0.01 * 7.053369045257568
Epoch 500, val loss: 0.7774791121482849
Epoch 510, training loss: 0.09152628481388092 = 0.021027617156505585 + 0.01 * 7.049866676330566
Epoch 510, val loss: 0.7864758968353271
Epoch 520, training loss: 0.09001457691192627 = 0.019536715000867844 + 0.01 * 7.047786712646484
Epoch 520, val loss: 0.7952515482902527
Epoch 530, training loss: 0.08851367980241776 = 0.018194211646914482 + 0.01 * 7.031946659088135
Epoch 530, val loss: 0.8038490414619446
Epoch 540, training loss: 0.08727174997329712 = 0.016981549561023712 + 0.01 * 7.029019832611084
Epoch 540, val loss: 0.8122220039367676
Epoch 550, training loss: 0.08602465689182281 = 0.015884479507803917 + 0.01 * 7.0140180587768555
Epoch 550, val loss: 0.8204402327537537
Epoch 560, training loss: 0.08499952405691147 = 0.014890202321112156 + 0.01 * 7.010932445526123
Epoch 560, val loss: 0.8283886909484863
Epoch 570, training loss: 0.08406595885753632 = 0.013986976817250252 + 0.01 * 7.007898807525635
Epoch 570, val loss: 0.8361573815345764
Epoch 580, training loss: 0.08325691521167755 = 0.013164246454834938 + 0.01 * 7.009267330169678
Epoch 580, val loss: 0.843723475933075
Epoch 590, training loss: 0.08222741633653641 = 0.01241271197795868 + 0.01 * 6.981470584869385
Epoch 590, val loss: 0.8510998487472534
Epoch 600, training loss: 0.08212772011756897 = 0.011725064367055893 + 0.01 * 7.040266036987305
Epoch 600, val loss: 0.858256995677948
Epoch 610, training loss: 0.08096400648355484 = 0.011095124296844006 + 0.01 * 6.986888885498047
Epoch 610, val loss: 0.8652406930923462
Epoch 620, training loss: 0.08014656603336334 = 0.010516364127397537 + 0.01 * 6.9630208015441895
Epoch 620, val loss: 0.8719985485076904
Epoch 630, training loss: 0.07968869060277939 = 0.009982503950595856 + 0.01 * 6.970618724822998
Epoch 630, val loss: 0.87861168384552
Epoch 640, training loss: 0.07904346287250519 = 0.009488832205533981 + 0.01 * 6.955463886260986
Epoch 640, val loss: 0.8851152658462524
Epoch 650, training loss: 0.07857006788253784 = 0.009032865054905415 + 0.01 * 6.9537200927734375
Epoch 650, val loss: 0.8913830518722534
Epoch 660, training loss: 0.07814880460500717 = 0.00861019641160965 + 0.01 * 6.953860759735107
Epoch 660, val loss: 0.8975554704666138
Epoch 670, training loss: 0.07761162519454956 = 0.00821811705827713 + 0.01 * 6.939350605010986
Epoch 670, val loss: 0.9035553932189941
Epoch 680, training loss: 0.07729703187942505 = 0.00785356666892767 + 0.01 * 6.9443464279174805
Epoch 680, val loss: 0.9094077944755554
Epoch 690, training loss: 0.07673976570367813 = 0.007513838820159435 + 0.01 * 6.922593116760254
Epoch 690, val loss: 0.91513592004776
Epoch 700, training loss: 0.0765838548541069 = 0.00719714118167758 + 0.01 * 6.938671112060547
Epoch 700, val loss: 0.9206851124763489
Epoch 710, training loss: 0.07610173523426056 = 0.006901109125465155 + 0.01 * 6.920063018798828
Epoch 710, val loss: 0.9261760711669922
Epoch 720, training loss: 0.07586628943681717 = 0.006624103989452124 + 0.01 * 6.924219131469727
Epoch 720, val loss: 0.9314940571784973
Epoch 730, training loss: 0.07544320821762085 = 0.006365292705595493 + 0.01 * 6.907792091369629
Epoch 730, val loss: 0.936678409576416
Epoch 740, training loss: 0.07509259879589081 = 0.006122267805039883 + 0.01 * 6.897033214569092
Epoch 740, val loss: 0.9417881965637207
Epoch 750, training loss: 0.07504706084728241 = 0.005893636494874954 + 0.01 * 6.915341854095459
Epoch 750, val loss: 0.9467635750770569
Epoch 760, training loss: 0.07459741830825806 = 0.005679381545633078 + 0.01 * 6.89180326461792
Epoch 760, val loss: 0.9515673518180847
Epoch 770, training loss: 0.07432293891906738 = 0.0054778228513896465 + 0.01 * 6.884511470794678
Epoch 770, val loss: 0.9562791585922241
Epoch 780, training loss: 0.07427424192428589 = 0.005287481937557459 + 0.01 * 6.898675918579102
Epoch 780, val loss: 0.9609079957008362
Epoch 790, training loss: 0.07403668016195297 = 0.005108751822263002 + 0.01 * 6.892792701721191
Epoch 790, val loss: 0.9653794169425964
Epoch 800, training loss: 0.07377662509679794 = 0.004939640872180462 + 0.01 * 6.8836989402771
Epoch 800, val loss: 0.9698401689529419
Epoch 810, training loss: 0.07349193096160889 = 0.004780016373842955 + 0.01 * 6.871191501617432
Epoch 810, val loss: 0.9741321802139282
Epoch 820, training loss: 0.07345926761627197 = 0.004629080183804035 + 0.01 * 6.883018493652344
Epoch 820, val loss: 0.9783189296722412
Epoch 830, training loss: 0.07305782288312912 = 0.004486474208533764 + 0.01 * 6.85713529586792
Epoch 830, val loss: 0.9824373722076416
Epoch 840, training loss: 0.07288655638694763 = 0.004351201467216015 + 0.01 * 6.8535356521606445
Epoch 840, val loss: 0.9864500761032104
Epoch 850, training loss: 0.07301986962556839 = 0.004222716670483351 + 0.01 * 6.879715442657471
Epoch 850, val loss: 0.9903919100761414
Epoch 860, training loss: 0.0725352019071579 = 0.00410061189904809 + 0.01 * 6.843458652496338
Epoch 860, val loss: 0.9942311644554138
Epoch 870, training loss: 0.07259819656610489 = 0.0039848159067332745 + 0.01 * 6.861338138580322
Epoch 870, val loss: 0.9980118274688721
Epoch 880, training loss: 0.07244879007339478 = 0.0038749638479202986 + 0.01 * 6.857382774353027
Epoch 880, val loss: 1.0016227960586548
Epoch 890, training loss: 0.0721522644162178 = 0.0037700531538575888 + 0.01 * 6.838221549987793
Epoch 890, val loss: 1.005262017250061
Epoch 900, training loss: 0.07193409651517868 = 0.0036706789396703243 + 0.01 * 6.8263421058654785
Epoch 900, val loss: 1.0086853504180908
Epoch 910, training loss: 0.07191944122314453 = 0.0035754467826336622 + 0.01 * 6.834400177001953
Epoch 910, val loss: 1.0121419429779053
Epoch 920, training loss: 0.07188285887241364 = 0.0034847010392695665 + 0.01 * 6.839815616607666
Epoch 920, val loss: 1.0154732465744019
Epoch 930, training loss: 0.0716230720281601 = 0.003398626111447811 + 0.01 * 6.822444438934326
Epoch 930, val loss: 1.0186560153961182
Epoch 940, training loss: 0.07166779041290283 = 0.003315775655210018 + 0.01 * 6.835201740264893
Epoch 940, val loss: 1.0218983888626099
Epoch 950, training loss: 0.0713261067867279 = 0.0032373901922255754 + 0.01 * 6.808872222900391
Epoch 950, val loss: 1.0249439477920532
Epoch 960, training loss: 0.07119489461183548 = 0.0031617283821105957 + 0.01 * 6.803317070007324
Epoch 960, val loss: 1.027986764907837
Epoch 970, training loss: 0.07174776494503021 = 0.003089465433731675 + 0.01 * 6.865829944610596
Epoch 970, val loss: 1.031010627746582
Epoch 980, training loss: 0.07092570513486862 = 0.00302053801715374 + 0.01 * 6.790516376495361
Epoch 980, val loss: 1.0338475704193115
Epoch 990, training loss: 0.07101797312498093 = 0.002954205498099327 + 0.01 * 6.8063764572143555
Epoch 990, val loss: 1.0367439985275269
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.838165524512388
The final CL Acc:0.82099, 0.01429, The final GNN Acc:0.83658, 0.00155
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9428])
updated graph: torch.Size([2, 10474])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0249385833740234 = 1.9389704465866089 + 0.01 * 8.596819877624512
Epoch 0, val loss: 1.938904047012329
Epoch 10, training loss: 2.015427589416504 = 1.929460048675537 + 0.01 * 8.596753120422363
Epoch 10, val loss: 1.9296163320541382
Epoch 20, training loss: 2.003439426422119 = 1.917474389076233 + 0.01 * 8.5964937210083
Epoch 20, val loss: 1.9173871278762817
Epoch 30, training loss: 1.9866594076156616 = 1.9007033109664917 + 0.01 * 8.595612525939941
Epoch 30, val loss: 1.900136947631836
Epoch 40, training loss: 1.9623034000396729 = 1.876395583152771 + 0.01 * 8.59078598022461
Epoch 40, val loss: 1.875611662864685
Epoch 50, training loss: 1.9294768571853638 = 1.8438700437545776 + 0.01 * 8.560680389404297
Epoch 50, val loss: 1.8447121381759644
Epoch 60, training loss: 1.8934922218322754 = 1.8092597723007202 + 0.01 * 8.423246383666992
Epoch 60, val loss: 1.8158905506134033
Epoch 70, training loss: 1.859859585762024 = 1.7779438495635986 + 0.01 * 8.191573143005371
Epoch 70, val loss: 1.7914586067199707
Epoch 80, training loss: 1.8173102140426636 = 1.7370367050170898 + 0.01 * 8.027348518371582
Epoch 80, val loss: 1.7554537057876587
Epoch 90, training loss: 1.7574135065078735 = 1.6788731813430786 + 0.01 * 7.854033946990967
Epoch 90, val loss: 1.7051533460617065
Epoch 100, training loss: 1.6774812936782837 = 1.6006138324737549 + 0.01 * 7.6867499351501465
Epoch 100, val loss: 1.641265630722046
Epoch 110, training loss: 1.5848788022994995 = 1.5097813606262207 + 0.01 * 7.509746551513672
Epoch 110, val loss: 1.5696110725402832
Epoch 120, training loss: 1.4909393787384033 = 1.416947364807129 + 0.01 * 7.39919900894165
Epoch 120, val loss: 1.4990060329437256
Epoch 130, training loss: 1.3993788957595825 = 1.3261910676956177 + 0.01 * 7.318778038024902
Epoch 130, val loss: 1.4326587915420532
Epoch 140, training loss: 1.3098663091659546 = 1.237345576286316 + 0.01 * 7.252074241638184
Epoch 140, val loss: 1.3694697618484497
Epoch 150, training loss: 1.224211573600769 = 1.1521469354629517 + 0.01 * 7.206460475921631
Epoch 150, val loss: 1.3109670877456665
Epoch 160, training loss: 1.1465208530426025 = 1.0747636556625366 + 0.01 * 7.17571496963501
Epoch 160, val loss: 1.261338233947754
Epoch 170, training loss: 1.078749418258667 = 1.0072147846221924 + 0.01 * 7.153458595275879
Epoch 170, val loss: 1.2216395139694214
Epoch 180, training loss: 1.0197439193725586 = 0.9483398795127869 + 0.01 * 7.1403985023498535
Epoch 180, val loss: 1.1898267269134521
Epoch 190, training loss: 0.9662707448005676 = 0.8949668407440186 + 0.01 * 7.13038969039917
Epoch 190, val loss: 1.1625051498413086
Epoch 200, training loss: 0.9145687818527222 = 0.8433186411857605 + 0.01 * 7.125014305114746
Epoch 200, val loss: 1.1361619234085083
Epoch 210, training loss: 0.8618285059928894 = 0.7906082272529602 + 0.01 * 7.1220269203186035
Epoch 210, val loss: 1.1075867414474487
Epoch 220, training loss: 0.8069725632667542 = 0.7357669472694397 + 0.01 * 7.1205596923828125
Epoch 220, val loss: 1.0760705471038818
Epoch 230, training loss: 0.750403642654419 = 0.6792018413543701 + 0.01 * 7.120182037353516
Epoch 230, val loss: 1.042112946510315
Epoch 240, training loss: 0.6930440068244934 = 0.6218396425247192 + 0.01 * 7.120438575744629
Epoch 240, val loss: 1.0072543621063232
Epoch 250, training loss: 0.6356272101402283 = 0.5644187927246094 + 0.01 * 7.120839595794678
Epoch 250, val loss: 0.9735152721405029
Epoch 260, training loss: 0.5785999298095703 = 0.5073884129524231 + 0.01 * 7.121154308319092
Epoch 260, val loss: 0.9428467750549316
Epoch 270, training loss: 0.5227746963500977 = 0.45156094431877136 + 0.01 * 7.121372699737549
Epoch 270, val loss: 0.9177592992782593
Epoch 280, training loss: 0.46983644366264343 = 0.39862099289894104 + 0.01 * 7.121545314788818
Epoch 280, val loss: 0.900559663772583
Epoch 290, training loss: 0.4215623736381531 = 0.35033243894577026 + 0.01 * 7.122994899749756
Epoch 290, val loss: 0.8918871283531189
Epoch 300, training loss: 0.37870460748672485 = 0.3074774742126465 + 0.01 * 7.122713565826416
Epoch 300, val loss: 0.890953540802002
Epoch 310, training loss: 0.34099963307380676 = 0.2697703540325165 + 0.01 * 7.122927188873291
Epoch 310, val loss: 0.8961142897605896
Epoch 320, training loss: 0.30779528617858887 = 0.23656217753887177 + 0.01 * 7.123310565948486
Epoch 320, val loss: 0.9060458540916443
Epoch 330, training loss: 0.278544545173645 = 0.2072978913784027 + 0.01 * 7.124663829803467
Epoch 330, val loss: 0.9195360541343689
Epoch 340, training loss: 0.25283050537109375 = 0.18158987164497375 + 0.01 * 7.124063014984131
Epoch 340, val loss: 0.935749351978302
Epoch 350, training loss: 0.23030611872673035 = 0.15906870365142822 + 0.01 * 7.12374210357666
Epoch 350, val loss: 0.9541007876396179
Epoch 360, training loss: 0.21059104800224304 = 0.13933923840522766 + 0.01 * 7.125180244445801
Epoch 360, val loss: 0.9739828705787659
Epoch 370, training loss: 0.19331592321395874 = 0.12207838147878647 + 0.01 * 7.123753547668457
Epoch 370, val loss: 0.995035707950592
Epoch 380, training loss: 0.17824244499206543 = 0.10701537132263184 + 0.01 * 7.122706890106201
Epoch 380, val loss: 1.0168640613555908
Epoch 390, training loss: 0.16512668132781982 = 0.09390852600336075 + 0.01 * 7.121814727783203
Epoch 390, val loss: 1.0391850471496582
Epoch 400, training loss: 0.15378403663635254 = 0.08256412297487259 + 0.01 * 7.12199068069458
Epoch 400, val loss: 1.0617843866348267
Epoch 410, training loss: 0.14399579167366028 = 0.07279758900403976 + 0.01 * 7.1198201179504395
Epoch 410, val loss: 1.0843966007232666
Epoch 420, training loss: 0.13560882210731506 = 0.06442825496196747 + 0.01 * 7.118055820465088
Epoch 420, val loss: 1.1070754528045654
Epoch 430, training loss: 0.12847593426704407 = 0.057262107729911804 + 0.01 * 7.1213836669921875
Epoch 430, val loss: 1.1297613382339478
Epoch 440, training loss: 0.12225975096225739 = 0.05111498758196831 + 0.01 * 7.114476203918457
Epoch 440, val loss: 1.1523147821426392
Epoch 450, training loss: 0.1169433742761612 = 0.04582428187131882 + 0.01 * 7.11190938949585
Epoch 450, val loss: 1.1746861934661865
Epoch 460, training loss: 0.11235661804676056 = 0.04125450178980827 + 0.01 * 7.110211372375488
Epoch 460, val loss: 1.1967623233795166
Epoch 470, training loss: 0.1083725094795227 = 0.037293143570423126 + 0.01 * 7.107936859130859
Epoch 470, val loss: 1.218444585800171
Epoch 480, training loss: 0.10486398637294769 = 0.03384589031338692 + 0.01 * 7.101809978485107
Epoch 480, val loss: 1.2396708726882935
Epoch 490, training loss: 0.10180146247148514 = 0.030834609642624855 + 0.01 * 7.096685409545898
Epoch 490, val loss: 1.2603552341461182
Epoch 500, training loss: 0.09914718568325043 = 0.028194479644298553 + 0.01 * 7.095271110534668
Epoch 500, val loss: 1.2804861068725586
Epoch 510, training loss: 0.09672263264656067 = 0.02587132528424263 + 0.01 * 7.085130214691162
Epoch 510, val loss: 1.3000035285949707
Epoch 520, training loss: 0.09459216892719269 = 0.023818403482437134 + 0.01 * 7.077376842498779
Epoch 520, val loss: 1.3189952373504639
Epoch 530, training loss: 0.09302482008934021 = 0.021997934207320213 + 0.01 * 7.102688789367676
Epoch 530, val loss: 1.337319016456604
Epoch 540, training loss: 0.09101013839244843 = 0.02037961408495903 + 0.01 * 7.063052177429199
Epoch 540, val loss: 1.3551381826400757
Epoch 550, training loss: 0.08946631103754044 = 0.01893472671508789 + 0.01 * 7.053158283233643
Epoch 550, val loss: 1.3723078966140747
Epoch 560, training loss: 0.08810488879680634 = 0.017639506608247757 + 0.01 * 7.04653787612915
Epoch 560, val loss: 1.388854742050171
Epoch 570, training loss: 0.08683943748474121 = 0.016475312411785126 + 0.01 * 7.036412715911865
Epoch 570, val loss: 1.405078411102295
Epoch 580, training loss: 0.08570502698421478 = 0.015426214784383774 + 0.01 * 7.027881145477295
Epoch 580, val loss: 1.420536994934082
Epoch 590, training loss: 0.08462681621313095 = 0.014477490447461605 + 0.01 * 7.014932632446289
Epoch 590, val loss: 1.435642123222351
Epoch 600, training loss: 0.08363469690084457 = 0.013617264106869698 + 0.01 * 7.001743793487549
Epoch 600, val loss: 1.4500318765640259
Epoch 610, training loss: 0.08285823464393616 = 0.012835010886192322 + 0.01 * 7.002322196960449
Epoch 610, val loss: 1.4640027284622192
Epoch 620, training loss: 0.08206671476364136 = 0.012121494859457016 + 0.01 * 6.994522571563721
Epoch 620, val loss: 1.4775944948196411
Epoch 630, training loss: 0.08140911161899567 = 0.011469123885035515 + 0.01 * 6.993999004364014
Epoch 630, val loss: 1.4906467199325562
Epoch 640, training loss: 0.08073893934488297 = 0.010871089063584805 + 0.01 * 6.986785411834717
Epoch 640, val loss: 1.503424882888794
Epoch 650, training loss: 0.08005204051733017 = 0.010322020389139652 + 0.01 * 6.9730024337768555
Epoch 650, val loss: 1.5156795978546143
Epoch 660, training loss: 0.07955630123615265 = 0.009816019795835018 + 0.01 * 6.97402811050415
Epoch 660, val loss: 1.5275745391845703
Epoch 670, training loss: 0.07892380654811859 = 0.00934896431863308 + 0.01 * 6.957484245300293
Epoch 670, val loss: 1.53915536403656
Epoch 680, training loss: 0.07847592979669571 = 0.008916900493204594 + 0.01 * 6.955903053283691
Epoch 680, val loss: 1.5502957105636597
Epoch 690, training loss: 0.07793368399143219 = 0.008515960536897182 + 0.01 * 6.9417724609375
Epoch 690, val loss: 1.5611417293548584
Epoch 700, training loss: 0.07755864411592484 = 0.008143137209117413 + 0.01 * 6.941551208496094
Epoch 700, val loss: 1.5715851783752441
Epoch 710, training loss: 0.07720128446817398 = 0.007796377409249544 + 0.01 * 6.94049072265625
Epoch 710, val loss: 1.5818487405776978
Epoch 720, training loss: 0.07710209488868713 = 0.007473180536180735 + 0.01 * 6.962892055511475
Epoch 720, val loss: 1.5918534994125366
Epoch 730, training loss: 0.07643421739339828 = 0.007172281853854656 + 0.01 * 6.926193714141846
Epoch 730, val loss: 1.6013009548187256
Epoch 740, training loss: 0.07601604610681534 = 0.006890655495226383 + 0.01 * 6.912539005279541
Epoch 740, val loss: 1.6107661724090576
Epoch 750, training loss: 0.07577873766422272 = 0.006626931484788656 + 0.01 * 6.915180206298828
Epoch 750, val loss: 1.6198915243148804
Epoch 760, training loss: 0.07549501210451126 = 0.006379994098097086 + 0.01 * 6.911501884460449
Epoch 760, val loss: 1.6285899877548218
Epoch 770, training loss: 0.07524120062589645 = 0.006148043554276228 + 0.01 * 6.909316062927246
Epoch 770, val loss: 1.6373718976974487
Epoch 780, training loss: 0.07490651309490204 = 0.00592992827296257 + 0.01 * 6.8976593017578125
Epoch 780, val loss: 1.645648717880249
Epoch 790, training loss: 0.074781633913517 = 0.005725083872675896 + 0.01 * 6.9056549072265625
Epoch 790, val loss: 1.6536821126937866
Epoch 800, training loss: 0.0746506005525589 = 0.00553183164447546 + 0.01 * 6.911877155303955
Epoch 800, val loss: 1.6617039442062378
Epoch 810, training loss: 0.07423090934753418 = 0.005349906161427498 + 0.01 * 6.888100624084473
Epoch 810, val loss: 1.6692101955413818
Epoch 820, training loss: 0.07400964200496674 = 0.005178350489586592 + 0.01 * 6.883129596710205
Epoch 820, val loss: 1.67689847946167
Epoch 830, training loss: 0.07368292659521103 = 0.005016012117266655 + 0.01 * 6.8666911125183105
Epoch 830, val loss: 1.6841589212417603
Epoch 840, training loss: 0.0736718699336052 = 0.0048625050112605095 + 0.01 * 6.880936622619629
Epoch 840, val loss: 1.6911996603012085
Epoch 850, training loss: 0.07346643507480621 = 0.0047168307937681675 + 0.01 * 6.874960899353027
Epoch 850, val loss: 1.6982519626617432
Epoch 860, training loss: 0.07321538031101227 = 0.004578655585646629 + 0.01 * 6.863672733306885
Epoch 860, val loss: 1.7047016620635986
Epoch 870, training loss: 0.0729953721165657 = 0.00444748904556036 + 0.01 * 6.854788303375244
Epoch 870, val loss: 1.7114957571029663
Epoch 880, training loss: 0.07291265577077866 = 0.004322646651417017 + 0.01 * 6.859001159667969
Epoch 880, val loss: 1.7178205251693726
Epoch 890, training loss: 0.07278887927532196 = 0.004204170778393745 + 0.01 * 6.858470916748047
Epoch 890, val loss: 1.724010944366455
Epoch 900, training loss: 0.07250193506479263 = 0.004091351293027401 + 0.01 * 6.841058254241943
Epoch 900, val loss: 1.7303115129470825
Epoch 910, training loss: 0.0723445937037468 = 0.003983823582530022 + 0.01 * 6.836076736450195
Epoch 910, val loss: 1.7362117767333984
Epoch 920, training loss: 0.0723155215382576 = 0.0038816628511995077 + 0.01 * 6.843386173248291
Epoch 920, val loss: 1.7419313192367554
Epoch 930, training loss: 0.072313591837883 = 0.003783745924010873 + 0.01 * 6.852984428405762
Epoch 930, val loss: 1.7476121187210083
Epoch 940, training loss: 0.0722067579627037 = 0.003690659534186125 + 0.01 * 6.85161018371582
Epoch 940, val loss: 1.7530686855316162
Epoch 950, training loss: 0.07188200950622559 = 0.0036017736420035362 + 0.01 * 6.828023910522461
Epoch 950, val loss: 1.758530855178833
Epoch 960, training loss: 0.07192326337099075 = 0.0035167059395462275 + 0.01 * 6.84065580368042
Epoch 960, val loss: 1.763778805732727
Epoch 970, training loss: 0.07164705544710159 = 0.0034352170769125223 + 0.01 * 6.821184158325195
Epoch 970, val loss: 1.7687554359436035
Epoch 980, training loss: 0.07152259349822998 = 0.0033574046101421118 + 0.01 * 6.816519260406494
Epoch 980, val loss: 1.7735837697982788
Epoch 990, training loss: 0.07160148024559021 = 0.003282965859398246 + 0.01 * 6.831851482391357
Epoch 990, val loss: 1.7786314487457275
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 2.037761688232422 = 1.9517930746078491 + 0.01 * 8.596851348876953
Epoch 0, val loss: 1.9596426486968994
Epoch 10, training loss: 2.027930736541748 = 1.941962718963623 + 0.01 * 8.59680461883545
Epoch 10, val loss: 1.9495748281478882
Epoch 20, training loss: 2.015960454940796 = 1.9299938678741455 + 0.01 * 8.596652030944824
Epoch 20, val loss: 1.9371088743209839
Epoch 30, training loss: 1.9994564056396484 = 1.9134950637817383 + 0.01 * 8.596128463745117
Epoch 30, val loss: 1.9199639558792114
Epoch 40, training loss: 1.975353717803955 = 1.8894236087799072 + 0.01 * 8.593005180358887
Epoch 40, val loss: 1.8951905965805054
Epoch 50, training loss: 1.941478967666626 = 1.8557969331741333 + 0.01 * 8.56820011138916
Epoch 50, val loss: 1.8619400262832642
Epoch 60, training loss: 1.9018601179122925 = 1.8176705837249756 + 0.01 * 8.418950080871582
Epoch 60, val loss: 1.8277490139007568
Epoch 70, training loss: 1.8677680492401123 = 1.7853375673294067 + 0.01 * 8.243045806884766
Epoch 70, val loss: 1.800896167755127
Epoch 80, training loss: 1.8293389081954956 = 1.7486367225646973 + 0.01 * 8.07021713256836
Epoch 80, val loss: 1.767736554145813
Epoch 90, training loss: 1.7756849527359009 = 1.6980414390563965 + 0.01 * 7.764346122741699
Epoch 90, val loss: 1.7237648963928223
Epoch 100, training loss: 1.703115701675415 = 1.6277014017105103 + 0.01 * 7.541426658630371
Epoch 100, val loss: 1.6644450426101685
Epoch 110, training loss: 1.6113349199295044 = 1.5369913578033447 + 0.01 * 7.434352397918701
Epoch 110, val loss: 1.5897457599639893
Epoch 120, training loss: 1.507936716079712 = 1.4344186782836914 + 0.01 * 7.351800441741943
Epoch 120, val loss: 1.5077152252197266
Epoch 130, training loss: 1.4023616313934326 = 1.3294329643249512 + 0.01 * 7.292871952056885
Epoch 130, val loss: 1.4246052503585815
Epoch 140, training loss: 1.3012334108352661 = 1.2287052869796753 + 0.01 * 7.252808570861816
Epoch 140, val loss: 1.3466867208480835
Epoch 150, training loss: 1.2085814476013184 = 1.1363940238952637 + 0.01 * 7.218745708465576
Epoch 150, val loss: 1.2769969701766968
Epoch 160, training loss: 1.1274102926254272 = 1.0555113554000854 + 0.01 * 7.189896583557129
Epoch 160, val loss: 1.2189081907272339
Epoch 170, training loss: 1.0578088760375977 = 0.9860755205154419 + 0.01 * 7.173333168029785
Epoch 170, val loss: 1.1729110479354858
Epoch 180, training loss: 0.9961536526679993 = 0.9244991540908813 + 0.01 * 7.1654486656188965
Epoch 180, val loss: 1.1359214782714844
Epoch 190, training loss: 0.9373769760131836 = 0.8657918572425842 + 0.01 * 7.158514022827148
Epoch 190, val loss: 1.1021794080734253
Epoch 200, training loss: 0.8771361708641052 = 0.8055974841117859 + 0.01 * 7.153868198394775
Epoch 200, val loss: 1.0670242309570312
Epoch 210, training loss: 0.8128628730773926 = 0.7413790225982666 + 0.01 * 7.148388385772705
Epoch 210, val loss: 1.0275893211364746
Epoch 220, training loss: 0.7447735071182251 = 0.6733539700508118 + 0.01 * 7.141952037811279
Epoch 220, val loss: 0.9840206503868103
Epoch 230, training loss: 0.6755598783493042 = 0.6042000651359558 + 0.01 * 7.135980129241943
Epoch 230, val loss: 0.9397165775299072
Epoch 240, training loss: 0.6089645624160767 = 0.5376682281494141 + 0.01 * 7.129632949829102
Epoch 240, val loss: 0.8990404605865479
Epoch 250, training loss: 0.5479485392570496 = 0.476718008518219 + 0.01 * 7.123052597045898
Epoch 250, val loss: 0.8651272654533386
Epoch 260, training loss: 0.4936404228210449 = 0.42249420285224915 + 0.01 * 7.114621162414551
Epoch 260, val loss: 0.8394836187362671
Epoch 270, training loss: 0.4457290470600128 = 0.37459397315979004 + 0.01 * 7.113508224487305
Epoch 270, val loss: 0.8215439915657043
Epoch 280, training loss: 0.4028509557247162 = 0.33183547854423523 + 0.01 * 7.101547718048096
Epoch 280, val loss: 0.8099361062049866
Epoch 290, training loss: 0.3640294373035431 = 0.2930232584476471 + 0.01 * 7.100616931915283
Epoch 290, val loss: 0.803347110748291
Epoch 300, training loss: 0.3283185362815857 = 0.2573643922805786 + 0.01 * 7.095416069030762
Epoch 300, val loss: 0.8008576035499573
Epoch 310, training loss: 0.2954282760620117 = 0.2245100438594818 + 0.01 * 7.091824054718018
Epoch 310, val loss: 0.8017780184745789
Epoch 320, training loss: 0.26539433002471924 = 0.1945086121559143 + 0.01 * 7.088572978973389
Epoch 320, val loss: 0.8056859970092773
Epoch 330, training loss: 0.23844443261623383 = 0.16760241985321045 + 0.01 * 7.084201335906982
Epoch 330, val loss: 0.8124039769172668
Epoch 340, training loss: 0.21482989192008972 = 0.14403411746025085 + 0.01 * 7.079577922821045
Epoch 340, val loss: 0.821571946144104
Epoch 350, training loss: 0.19456736743450165 = 0.12381584942340851 + 0.01 * 7.0751519203186035
Epoch 350, val loss: 0.8331087231636047
Epoch 360, training loss: 0.17745235562324524 = 0.10671689361333847 + 0.01 * 7.073546409606934
Epoch 360, val loss: 0.8466841578483582
Epoch 370, training loss: 0.1630645990371704 = 0.09238065779209137 + 0.01 * 7.068393707275391
Epoch 370, val loss: 0.8618934154510498
Epoch 380, training loss: 0.15106849372386932 = 0.08040323108434677 + 0.01 * 7.066526412963867
Epoch 380, val loss: 0.878147542476654
Epoch 390, training loss: 0.14110663533210754 = 0.07039374113082886 + 0.01 * 7.071290493011475
Epoch 390, val loss: 0.8951277732849121
Epoch 400, training loss: 0.13263660669326782 = 0.06200329214334488 + 0.01 * 7.06333065032959
Epoch 400, val loss: 0.912386417388916
Epoch 410, training loss: 0.12548282742500305 = 0.05492658168077469 + 0.01 * 7.055624961853027
Epoch 410, val loss: 0.9296685457229614
Epoch 420, training loss: 0.11942218244075775 = 0.048918891698122025 + 0.01 * 7.050329685211182
Epoch 420, val loss: 0.9467393755912781
Epoch 430, training loss: 0.1142958328127861 = 0.04378758370876312 + 0.01 * 7.050825119018555
Epoch 430, val loss: 0.9634767770767212
Epoch 440, training loss: 0.10981259495019913 = 0.03937634080648422 + 0.01 * 7.043625831604004
Epoch 440, val loss: 0.9797905683517456
Epoch 450, training loss: 0.10594787448644638 = 0.03555707633495331 + 0.01 * 7.039079666137695
Epoch 450, val loss: 0.9956643581390381
Epoch 460, training loss: 0.1025538519024849 = 0.03222968429327011 + 0.01 * 7.032416820526123
Epoch 460, val loss: 1.0111033916473389
Epoch 470, training loss: 0.09966256469488144 = 0.029316430911421776 + 0.01 * 7.034613609313965
Epoch 470, val loss: 1.0261805057525635
Epoch 480, training loss: 0.09707707166671753 = 0.026759209111332893 + 0.01 * 7.0317864418029785
Epoch 480, val loss: 1.0408285856246948
Epoch 490, training loss: 0.09471015632152557 = 0.024504976347088814 + 0.01 * 7.0205183029174805
Epoch 490, val loss: 1.0550721883773804
Epoch 500, training loss: 0.09269464761018753 = 0.02251267433166504 + 0.01 * 7.018197536468506
Epoch 500, val loss: 1.0688661336898804
Epoch 510, training loss: 0.09085695445537567 = 0.020749041810631752 + 0.01 * 7.010791301727295
Epoch 510, val loss: 1.0822316408157349
Epoch 520, training loss: 0.08924172818660736 = 0.01917995512485504 + 0.01 * 7.0061774253845215
Epoch 520, val loss: 1.095227599143982
Epoch 530, training loss: 0.0878164991736412 = 0.017779996618628502 + 0.01 * 7.003650188446045
Epoch 530, val loss: 1.1078025102615356
Epoch 540, training loss: 0.08648131042718887 = 0.016528120264410973 + 0.01 * 6.99531888961792
Epoch 540, val loss: 1.1199828386306763
Epoch 550, training loss: 0.08546565473079681 = 0.015404918231070042 + 0.01 * 7.006073951721191
Epoch 550, val loss: 1.1317853927612305
Epoch 560, training loss: 0.08419676125049591 = 0.014396165497601032 + 0.01 * 6.9800591468811035
Epoch 560, val loss: 1.1431962251663208
Epoch 570, training loss: 0.08332672715187073 = 0.013486091047525406 + 0.01 * 6.984063148498535
Epoch 570, val loss: 1.1542249917984009
Epoch 580, training loss: 0.08242383599281311 = 0.012663427740335464 + 0.01 * 6.976040363311768
Epoch 580, val loss: 1.1649082899093628
Epoch 590, training loss: 0.0815638080239296 = 0.011917931959033012 + 0.01 * 6.964587688446045
Epoch 590, val loss: 1.1752218008041382
Epoch 600, training loss: 0.08100156486034393 = 0.011239646933972836 + 0.01 * 6.976191520690918
Epoch 600, val loss: 1.1852487325668335
Epoch 610, training loss: 0.08019771426916122 = 0.0106210233643651 + 0.01 * 6.957668781280518
Epoch 610, val loss: 1.1949734687805176
Epoch 620, training loss: 0.07952933013439178 = 0.01005560252815485 + 0.01 * 6.947372913360596
Epoch 620, val loss: 1.2043828964233398
Epoch 630, training loss: 0.07948280870914459 = 0.00953699741512537 + 0.01 * 6.99458122253418
Epoch 630, val loss: 1.2134791612625122
Epoch 640, training loss: 0.07842957228422165 = 0.009061932563781738 + 0.01 * 6.936764240264893
Epoch 640, val loss: 1.2223451137542725
Epoch 650, training loss: 0.077840156853199 = 0.008624695241451263 + 0.01 * 6.92154598236084
Epoch 650, val loss: 1.2308275699615479
Epoch 660, training loss: 0.07772216945886612 = 0.008220791816711426 + 0.01 * 6.950138092041016
Epoch 660, val loss: 1.2390897274017334
Epoch 670, training loss: 0.07705900073051453 = 0.007848408073186874 + 0.01 * 6.921060085296631
Epoch 670, val loss: 1.2471755743026733
Epoch 680, training loss: 0.07654671370983124 = 0.007503437343984842 + 0.01 * 6.904327869415283
Epoch 680, val loss: 1.2549068927764893
Epoch 690, training loss: 0.07613447308540344 = 0.0071830544620752335 + 0.01 * 6.895142555236816
Epoch 690, val loss: 1.262480616569519
Epoch 700, training loss: 0.07581454515457153 = 0.00688498979434371 + 0.01 * 6.892955303192139
Epoch 700, val loss: 1.269826054573059
Epoch 710, training loss: 0.0757235512137413 = 0.0066071790643036366 + 0.01 * 6.911637306213379
Epoch 710, val loss: 1.27693772315979
Epoch 720, training loss: 0.07518589496612549 = 0.006348324939608574 + 0.01 * 6.8837571144104
Epoch 720, val loss: 1.2839323282241821
Epoch 730, training loss: 0.07511356472969055 = 0.006106413900852203 + 0.01 * 6.900714874267578
Epoch 730, val loss: 1.2905614376068115
Epoch 740, training loss: 0.07466959953308105 = 0.005880499258637428 + 0.01 * 6.878910541534424
Epoch 740, val loss: 1.2971827983856201
Epoch 750, training loss: 0.07434558123350143 = 0.005668497644364834 + 0.01 * 6.867708683013916
Epoch 750, val loss: 1.3034915924072266
Epoch 760, training loss: 0.07413671910762787 = 0.005469221156090498 + 0.01 * 6.866750240325928
Epoch 760, val loss: 1.3097134828567505
Epoch 770, training loss: 0.07407303154468536 = 0.005281719844788313 + 0.01 * 6.87913179397583
Epoch 770, val loss: 1.3157588243484497
Epoch 780, training loss: 0.07378695160150528 = 0.005104947369545698 + 0.01 * 6.86820125579834
Epoch 780, val loss: 1.3216711282730103
Epoch 790, training loss: 0.07397286593914032 = 0.004938471596688032 + 0.01 * 6.903439998626709
Epoch 790, val loss: 1.3273305892944336
Epoch 800, training loss: 0.07331608980894089 = 0.004781648050993681 + 0.01 * 6.853444576263428
Epoch 800, val loss: 1.3329342603683472
Epoch 810, training loss: 0.07311046868562698 = 0.0046333130449056625 + 0.01 * 6.847715854644775
Epoch 810, val loss: 1.3382982015609741
Epoch 820, training loss: 0.07293250411748886 = 0.004492755047976971 + 0.01 * 6.84397554397583
Epoch 820, val loss: 1.3436217308044434
Epoch 830, training loss: 0.07285495102405548 = 0.004359885584563017 + 0.01 * 6.849506855010986
Epoch 830, val loss: 1.3486799001693726
Epoch 840, training loss: 0.07272817194461823 = 0.004233997780829668 + 0.01 * 6.849417209625244
Epoch 840, val loss: 1.3537172079086304
Epoch 850, training loss: 0.07245346903800964 = 0.004114582668989897 + 0.01 * 6.833888530731201
Epoch 850, val loss: 1.3585652112960815
Epoch 860, training loss: 0.07240872830152512 = 0.004001075401902199 + 0.01 * 6.840765476226807
Epoch 860, val loss: 1.3633497953414917
Epoch 870, training loss: 0.07217232882976532 = 0.00389315583743155 + 0.01 * 6.827917098999023
Epoch 870, val loss: 1.3679534196853638
Epoch 880, training loss: 0.07231708616018295 = 0.0037903764750808477 + 0.01 * 6.852671146392822
Epoch 880, val loss: 1.3724855184555054
Epoch 890, training loss: 0.0718313530087471 = 0.0036926341708749533 + 0.01 * 6.813872337341309
Epoch 890, val loss: 1.3769035339355469
Epoch 900, training loss: 0.07168982177972794 = 0.003599380375817418 + 0.01 * 6.809044361114502
Epoch 900, val loss: 1.3811501264572144
Epoch 910, training loss: 0.0719572976231575 = 0.003510323353111744 + 0.01 * 6.84469747543335
Epoch 910, val loss: 1.3853756189346313
Epoch 920, training loss: 0.07148213684558868 = 0.0034255238715559244 + 0.01 * 6.805661201477051
Epoch 920, val loss: 1.3894760608673096
Epoch 930, training loss: 0.07167559862136841 = 0.0033444834407418966 + 0.01 * 6.833111763000488
Epoch 930, val loss: 1.393439769744873
Epoch 940, training loss: 0.07128389179706573 = 0.0032669559586793184 + 0.01 * 6.801694393157959
Epoch 940, val loss: 1.3973630666732788
Epoch 950, training loss: 0.07135263085365295 = 0.003192831762135029 + 0.01 * 6.815979957580566
Epoch 950, val loss: 1.4011110067367554
Epoch 960, training loss: 0.0711594894528389 = 0.0031219334341585636 + 0.01 * 6.803755283355713
Epoch 960, val loss: 1.4049054384231567
Epoch 970, training loss: 0.07104858011007309 = 0.003053753636777401 + 0.01 * 6.799482822418213
Epoch 970, val loss: 1.4084426164627075
Epoch 980, training loss: 0.07104379683732986 = 0.002988559426739812 + 0.01 * 6.805523872375488
Epoch 980, val loss: 1.4120253324508667
Epoch 990, training loss: 0.07080557197332382 = 0.0029260299634188414 + 0.01 * 6.787954330444336
Epoch 990, val loss: 1.4154728651046753
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 2.0420315265655518 = 1.9560630321502686 + 0.01 * 8.596848487854004
Epoch 0, val loss: 1.9588396549224854
Epoch 10, training loss: 2.0311925411224365 = 1.9452245235443115 + 0.01 * 8.596793174743652
Epoch 10, val loss: 1.9472638368606567
Epoch 20, training loss: 2.018131971359253 = 1.9321658611297607 + 0.01 * 8.596607208251953
Epoch 20, val loss: 1.9329017400741577
Epoch 30, training loss: 2.0001766681671143 = 1.9142166376113892 + 0.01 * 8.5960054397583
Epoch 30, val loss: 1.912907600402832
Epoch 40, training loss: 1.9743225574493408 = 1.8883970975875854 + 0.01 * 8.592546463012695
Epoch 40, val loss: 1.8843249082565308
Epoch 50, training loss: 1.9391320943832397 = 1.8534563779830933 + 0.01 * 8.567566871643066
Epoch 50, val loss: 1.8473848104476929
Epoch 60, training loss: 1.900641679763794 = 1.816007375717163 + 0.01 * 8.463431358337402
Epoch 60, val loss: 1.8127305507659912
Epoch 70, training loss: 1.8693491220474243 = 1.7865036725997925 + 0.01 * 8.2845458984375
Epoch 70, val loss: 1.791014552116394
Epoch 80, training loss: 1.834289789199829 = 1.7530934810638428 + 0.01 * 8.119633674621582
Epoch 80, val loss: 1.7650851011276245
Epoch 90, training loss: 1.7837148904800415 = 1.7063016891479492 + 0.01 * 7.741316318511963
Epoch 90, val loss: 1.7254345417022705
Epoch 100, training loss: 1.716150164604187 = 1.6415677070617676 + 0.01 * 7.458251476287842
Epoch 100, val loss: 1.6708494424819946
Epoch 110, training loss: 1.631580114364624 = 1.5578984022140503 + 0.01 * 7.368168354034424
Epoch 110, val loss: 1.602094292640686
Epoch 120, training loss: 1.5356172323226929 = 1.462394118309021 + 0.01 * 7.322309970855713
Epoch 120, val loss: 1.5256823301315308
Epoch 130, training loss: 1.4356507062911987 = 1.362646222114563 + 0.01 * 7.300449848175049
Epoch 130, val loss: 1.4472116231918335
Epoch 140, training loss: 1.3354449272155762 = 1.262658715248108 + 0.01 * 7.27862024307251
Epoch 140, val loss: 1.369361400604248
Epoch 150, training loss: 1.2372806072235107 = 1.164673924446106 + 0.01 * 7.2606706619262695
Epoch 150, val loss: 1.2938873767852783
Epoch 160, training loss: 1.1431113481521606 = 1.0706254243850708 + 0.01 * 7.248587131500244
Epoch 160, val loss: 1.2223331928253174
Epoch 170, training loss: 1.0541377067565918 = 0.98176109790802 + 0.01 * 7.237666606903076
Epoch 170, val loss: 1.1561312675476074
Epoch 180, training loss: 0.9699268937110901 = 0.8976815342903137 + 0.01 * 7.224534034729004
Epoch 180, val loss: 1.0959025621414185
Epoch 190, training loss: 0.8892667293548584 = 0.8171716332435608 + 0.01 * 7.209507942199707
Epoch 190, val loss: 1.040152668952942
Epoch 200, training loss: 0.8120328187942505 = 0.740057647228241 + 0.01 * 7.197516441345215
Epoch 200, val loss: 0.9883890748023987
Epoch 210, training loss: 0.7394747734069824 = 0.6675956845283508 + 0.01 * 7.1879096031188965
Epoch 210, val loss: 0.9415248036384583
Epoch 220, training loss: 0.6734349131584167 = 0.6016469597816467 + 0.01 * 7.178792953491211
Epoch 220, val loss: 0.9010941982269287
Epoch 230, training loss: 0.6148918271064758 = 0.5431700348854065 + 0.01 * 7.172178268432617
Epoch 230, val loss: 0.8689311146736145
Epoch 240, training loss: 0.5635586977005005 = 0.4918616712093353 + 0.01 * 7.169701099395752
Epoch 240, val loss: 0.8452834486961365
Epoch 250, training loss: 0.5181103944778442 = 0.44644907116889954 + 0.01 * 7.166133880615234
Epoch 250, val loss: 0.8288677930831909
Epoch 260, training loss: 0.476830393075943 = 0.40520015358924866 + 0.01 * 7.163023948669434
Epoch 260, val loss: 0.8176209330558777
Epoch 270, training loss: 0.4381061792373657 = 0.3664950132369995 + 0.01 * 7.1611151695251465
Epoch 270, val loss: 0.8093904256820679
Epoch 280, training loss: 0.40078651905059814 = 0.3291906416416168 + 0.01 * 7.15958833694458
Epoch 280, val loss: 0.8026536107063293
Epoch 290, training loss: 0.364391565322876 = 0.2928043007850647 + 0.01 * 7.158728122711182
Epoch 290, val loss: 0.7967763543128967
Epoch 300, training loss: 0.3290959894657135 = 0.2575143277645111 + 0.01 * 7.158165454864502
Epoch 300, val loss: 0.7918583750724792
Epoch 310, training loss: 0.29565495252609253 = 0.22407272458076477 + 0.01 * 7.158224582672119
Epoch 310, val loss: 0.7883490920066833
Epoch 320, training loss: 0.26498353481292725 = 0.19341401755809784 + 0.01 * 7.156950950622559
Epoch 320, val loss: 0.7870805859565735
Epoch 330, training loss: 0.23778881132602692 = 0.16621050238609314 + 0.01 * 7.157830715179443
Epoch 330, val loss: 0.7886499166488647
Epoch 340, training loss: 0.21421697735786438 = 0.14267022907733917 + 0.01 * 7.1546759605407715
Epoch 340, val loss: 0.7933677434921265
Epoch 350, training loss: 0.19416475296020508 = 0.12263532727956772 + 0.01 * 7.1529436111450195
Epoch 350, val loss: 0.8009442687034607
Epoch 360, training loss: 0.17729757726192474 = 0.10578885674476624 + 0.01 * 7.150872230529785
Epoch 360, val loss: 0.8111139535903931
Epoch 370, training loss: 0.16319969296455383 = 0.09170882403850555 + 0.01 * 7.149087905883789
Epoch 370, val loss: 0.8234277963638306
Epoch 380, training loss: 0.15140888094902039 = 0.07995951920747757 + 0.01 * 7.144937038421631
Epoch 380, val loss: 0.8374072909355164
Epoch 390, training loss: 0.14158812165260315 = 0.07013863325119019 + 0.01 * 7.1449480056762695
Epoch 390, val loss: 0.8525710105895996
Epoch 400, training loss: 0.13326558470726013 = 0.0618915893137455 + 0.01 * 7.137399673461914
Epoch 400, val loss: 0.8685046434402466
Epoch 410, training loss: 0.12626999616622925 = 0.05492817610502243 + 0.01 * 7.134181499481201
Epoch 410, val loss: 0.8849261999130249
Epoch 420, training loss: 0.12024521827697754 = 0.04901754483580589 + 0.01 * 7.122766971588135
Epoch 420, val loss: 0.9014589786529541
Epoch 430, training loss: 0.1151203066110611 = 0.04397125169634819 + 0.01 * 7.11490535736084
Epoch 430, val loss: 0.9178606867790222
Epoch 440, training loss: 0.11081323027610779 = 0.039640508592128754 + 0.01 * 7.11727237701416
Epoch 440, val loss: 0.9340078234672546
Epoch 450, training loss: 0.10687640309333801 = 0.03590571880340576 + 0.01 * 7.0970683097839355
Epoch 450, val loss: 0.949739933013916
Epoch 460, training loss: 0.10367630422115326 = 0.03266524896025658 + 0.01 * 7.101105213165283
Epoch 460, val loss: 0.9650557637214661
Epoch 470, training loss: 0.10063585638999939 = 0.02984187938272953 + 0.01 * 7.079397678375244
Epoch 470, val loss: 0.9798587560653687
Epoch 480, training loss: 0.09810051321983337 = 0.027368899434804916 + 0.01 * 7.073162078857422
Epoch 480, val loss: 0.9941985011100769
Epoch 490, training loss: 0.0958496481180191 = 0.025193162262439728 + 0.01 * 7.065649032592773
Epoch 490, val loss: 1.0080218315124512
Epoch 500, training loss: 0.0937672033905983 = 0.02326950430870056 + 0.01 * 7.049769878387451
Epoch 500, val loss: 1.021349310874939
Epoch 510, training loss: 0.09200423955917358 = 0.021561583504080772 + 0.01 * 7.044265270233154
Epoch 510, val loss: 1.0342334508895874
Epoch 520, training loss: 0.09044155478477478 = 0.020045233890414238 + 0.01 * 7.039632320404053
Epoch 520, val loss: 1.0465757846832275
Epoch 530, training loss: 0.08896207809448242 = 0.018688427284359932 + 0.01 * 7.027365207672119
Epoch 530, val loss: 1.05846107006073
Epoch 540, training loss: 0.08771899342536926 = 0.01746801659464836 + 0.01 * 7.025097846984863
Epoch 540, val loss: 1.069952130317688
Epoch 550, training loss: 0.08662917464971542 = 0.01636665314435959 + 0.01 * 7.026252269744873
Epoch 550, val loss: 1.0811233520507812
Epoch 560, training loss: 0.08548174798488617 = 0.01537082064896822 + 0.01 * 7.011092662811279
Epoch 560, val loss: 1.0918723344802856
Epoch 570, training loss: 0.08458460867404938 = 0.014468126930296421 + 0.01 * 7.011648654937744
Epoch 570, val loss: 1.1022703647613525
Epoch 580, training loss: 0.08375222980976105 = 0.013646597974002361 + 0.01 * 7.010563373565674
Epoch 580, val loss: 1.1123634576797485
Epoch 590, training loss: 0.08277945220470428 = 0.012897368520498276 + 0.01 * 6.988208770751953
Epoch 590, val loss: 1.1220855712890625
Epoch 600, training loss: 0.0819871723651886 = 0.012212269939482212 + 0.01 * 6.977490425109863
Epoch 600, val loss: 1.1315093040466309
Epoch 610, training loss: 0.08142241835594177 = 0.011584380641579628 + 0.01 * 6.983804225921631
Epoch 610, val loss: 1.1406406164169312
Epoch 620, training loss: 0.08085545152425766 = 0.011006684973835945 + 0.01 * 6.98487663269043
Epoch 620, val loss: 1.149452805519104
Epoch 630, training loss: 0.08011338859796524 = 0.010475085116922855 + 0.01 * 6.963830947875977
Epoch 630, val loss: 1.1580066680908203
Epoch 640, training loss: 0.07967983186244965 = 0.009984130971133709 + 0.01 * 6.969570636749268
Epoch 640, val loss: 1.1663379669189453
Epoch 650, training loss: 0.07918252795934677 = 0.009529726579785347 + 0.01 * 6.965280532836914
Epoch 650, val loss: 1.1743642091751099
Epoch 660, training loss: 0.0784064456820488 = 0.009108536876738071 + 0.01 * 6.929791450500488
Epoch 660, val loss: 1.1821385622024536
Epoch 670, training loss: 0.07857729494571686 = 0.008716852404177189 + 0.01 * 6.986044406890869
Epoch 670, val loss: 1.1897640228271484
Epoch 680, training loss: 0.0775928795337677 = 0.008353319019079208 + 0.01 * 6.923956394195557
Epoch 680, val loss: 1.1970574855804443
Epoch 690, training loss: 0.07718393206596375 = 0.008014535531401634 + 0.01 * 6.9169392585754395
Epoch 690, val loss: 1.2041815519332886
Epoch 700, training loss: 0.07691039144992828 = 0.007698141038417816 + 0.01 * 6.921225547790527
Epoch 700, val loss: 1.211081624031067
Epoch 710, training loss: 0.07645169645547867 = 0.007402240298688412 + 0.01 * 6.9049458503723145
Epoch 710, val loss: 1.2177919149398804
Epoch 720, training loss: 0.07620491832494736 = 0.007125183008611202 + 0.01 * 6.907973766326904
Epoch 720, val loss: 1.2243118286132812
Epoch 730, training loss: 0.07582803815603256 = 0.0068653905764222145 + 0.01 * 6.896265029907227
Epoch 730, val loss: 1.2306418418884277
Epoch 740, training loss: 0.07566236704587936 = 0.0066216071136295795 + 0.01 * 6.904076099395752
Epoch 740, val loss: 1.236758828163147
Epoch 750, training loss: 0.07518157362937927 = 0.006392415147274733 + 0.01 * 6.878916263580322
Epoch 750, val loss: 1.2427105903625488
Epoch 760, training loss: 0.07530011981725693 = 0.006176820490509272 + 0.01 * 6.912330150604248
Epoch 760, val loss: 1.2485586404800415
Epoch 770, training loss: 0.07481709867715836 = 0.005973587278276682 + 0.01 * 6.88435173034668
Epoch 770, val loss: 1.254142165184021
Epoch 780, training loss: 0.0744711384177208 = 0.005781581159681082 + 0.01 * 6.868955612182617
Epoch 780, val loss: 1.2596360445022583
Epoch 790, training loss: 0.07423193007707596 = 0.005600355565547943 + 0.01 * 6.863157272338867
Epoch 790, val loss: 1.2649787664413452
Epoch 800, training loss: 0.07389017194509506 = 0.005428792908787727 + 0.01 * 6.846138000488281
Epoch 800, val loss: 1.2702102661132812
Epoch 810, training loss: 0.07395302504301071 = 0.005266106687486172 + 0.01 * 6.868691921234131
Epoch 810, val loss: 1.275282621383667
Epoch 820, training loss: 0.0735744833946228 = 0.00511191925033927 + 0.01 * 6.846256732940674
Epoch 820, val loss: 1.2801686525344849
Epoch 830, training loss: 0.07332096993923187 = 0.004965664353221655 + 0.01 * 6.835530757904053
Epoch 830, val loss: 1.2850240468978882
Epoch 840, training loss: 0.07325833290815353 = 0.004826800897717476 + 0.01 * 6.843153476715088
Epoch 840, val loss: 1.289721131324768
Epoch 850, training loss: 0.07310385257005692 = 0.004694819450378418 + 0.01 * 6.840903282165527
Epoch 850, val loss: 1.2942534685134888
Epoch 860, training loss: 0.07275507599115372 = 0.004569099750369787 + 0.01 * 6.818597793579102
Epoch 860, val loss: 1.2987834215164185
Epoch 870, training loss: 0.07265254110097885 = 0.004449283704161644 + 0.01 * 6.82032585144043
Epoch 870, val loss: 1.3031402826309204
Epoch 880, training loss: 0.07288212329149246 = 0.004334990866482258 + 0.01 * 6.854712963104248
Epoch 880, val loss: 1.3074206113815308
Epoch 890, training loss: 0.07235697656869888 = 0.0042260801419615746 + 0.01 * 6.813089847564697
Epoch 890, val loss: 1.3116047382354736
Epoch 900, training loss: 0.07222197949886322 = 0.004121910315006971 + 0.01 * 6.810007095336914
Epoch 900, val loss: 1.3157529830932617
Epoch 910, training loss: 0.07233347743749619 = 0.00402236869558692 + 0.01 * 6.831110954284668
Epoch 910, val loss: 1.3197665214538574
Epoch 920, training loss: 0.07200329005718231 = 0.003927342128008604 + 0.01 * 6.8075947761535645
Epoch 920, val loss: 1.3236570358276367
Epoch 930, training loss: 0.0718291848897934 = 0.0038363859057426453 + 0.01 * 6.799279689788818
Epoch 930, val loss: 1.3275333642959595
Epoch 940, training loss: 0.07184071838855743 = 0.0037491784896701574 + 0.01 * 6.809154033660889
Epoch 940, val loss: 1.3312944173812866
Epoch 950, training loss: 0.07179802656173706 = 0.0036655287258327007 + 0.01 * 6.8132500648498535
Epoch 950, val loss: 1.3349648714065552
Epoch 960, training loss: 0.07136645913124084 = 0.0035854200832545757 + 0.01 * 6.778104305267334
Epoch 960, val loss: 1.3385543823242188
Epoch 970, training loss: 0.07134035229682922 = 0.003508551511913538 + 0.01 * 6.783179759979248
Epoch 970, val loss: 1.3421099185943604
Epoch 980, training loss: 0.07125934213399887 = 0.003434841288253665 + 0.01 * 6.782450199127197
Epoch 980, val loss: 1.345504879951477
Epoch 990, training loss: 0.07115571945905685 = 0.00336391176097095 + 0.01 * 6.779180526733398
Epoch 990, val loss: 1.3488409519195557
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8118081180811808
The final CL Acc:0.75309, 0.01397, The final GNN Acc:0.80759, 0.00325
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 7918])
updated graph: torch.Size([2, 10514])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.01896595954895 = 1.9329975843429565 + 0.01 * 8.596835136413574
Epoch 0, val loss: 1.9384580850601196
Epoch 10, training loss: 2.00946307182312 = 1.9234952926635742 + 0.01 * 8.596776008605957
Epoch 10, val loss: 1.9285223484039307
Epoch 20, training loss: 1.997992992401123 = 1.9120274782180786 + 0.01 * 8.5965576171875
Epoch 20, val loss: 1.9161591529846191
Epoch 30, training loss: 1.982313871383667 = 1.8963550329208374 + 0.01 * 8.595884323120117
Epoch 30, val loss: 1.8990707397460938
Epoch 40, training loss: 1.9595839977264404 = 1.8736591339111328 + 0.01 * 8.592486381530762
Epoch 40, val loss: 1.8743830919265747
Epoch 50, training loss: 1.927768349647522 = 1.8420777320861816 + 0.01 * 8.569061279296875
Epoch 50, val loss: 1.8411033153533936
Epoch 60, training loss: 1.8897559642791748 = 1.8052608966827393 + 0.01 * 8.449507713317871
Epoch 60, val loss: 1.805330753326416
Epoch 70, training loss: 1.849692463874817 = 1.7685626745224 + 0.01 * 8.112980842590332
Epoch 70, val loss: 1.7726821899414062
Epoch 80, training loss: 1.8002281188964844 = 1.7217090129852295 + 0.01 * 7.851909160614014
Epoch 80, val loss: 1.7307319641113281
Epoch 90, training loss: 1.7315618991851807 = 1.6562960147857666 + 0.01 * 7.526592254638672
Epoch 90, val loss: 1.672430396080017
Epoch 100, training loss: 1.644352912902832 = 1.5709935426712036 + 0.01 * 7.335931777954102
Epoch 100, val loss: 1.5982882976531982
Epoch 110, training loss: 1.5462815761566162 = 1.4735374450683594 + 0.01 * 7.274409770965576
Epoch 110, val loss: 1.515108585357666
Epoch 120, training loss: 1.444123387336731 = 1.3715178966522217 + 0.01 * 7.260547161102295
Epoch 120, val loss: 1.4314273595809937
Epoch 130, training loss: 1.3394759893417358 = 1.266957402229309 + 0.01 * 7.251853942871094
Epoch 130, val loss: 1.3483026027679443
Epoch 140, training loss: 1.2325917482376099 = 1.1601461172103882 + 0.01 * 7.244564533233643
Epoch 140, val loss: 1.2649152278900146
Epoch 150, training loss: 1.1255316734313965 = 1.053100347518921 + 0.01 * 7.243129253387451
Epoch 150, val loss: 1.1824696063995361
Epoch 160, training loss: 1.0222606658935547 = 0.9498353004455566 + 0.01 * 7.242536544799805
Epoch 160, val loss: 1.102950930595398
Epoch 170, training loss: 0.9269858598709106 = 0.8545822501182556 + 0.01 * 7.240362644195557
Epoch 170, val loss: 1.0302990674972534
Epoch 180, training loss: 0.8429524302482605 = 0.7706045508384705 + 0.01 * 7.234787940979004
Epoch 180, val loss: 0.9670940041542053
Epoch 190, training loss: 0.7708498239517212 = 0.6985991597175598 + 0.01 * 7.225068092346191
Epoch 190, val loss: 0.9142367839813232
Epoch 200, training loss: 0.7086594700813293 = 0.6365541815757751 + 0.01 * 7.210526943206787
Epoch 200, val loss: 0.8704632520675659
Epoch 210, training loss: 0.6531949043273926 = 0.5812918543815613 + 0.01 * 7.190306186676025
Epoch 210, val loss: 0.8334642648696899
Epoch 220, training loss: 0.601993978023529 = 0.5302984118461609 + 0.01 * 7.169558048248291
Epoch 220, val loss: 0.8015879392623901
Epoch 230, training loss: 0.5538696050643921 = 0.4824460744857788 + 0.01 * 7.142353057861328
Epoch 230, val loss: 0.7741195559501648
Epoch 240, training loss: 0.5088488459587097 = 0.4376142621040344 + 0.01 * 7.123459339141846
Epoch 240, val loss: 0.751469612121582
Epoch 250, training loss: 0.4671558737754822 = 0.3960890769958496 + 0.01 * 7.1066789627075195
Epoch 250, val loss: 0.7338774800300598
Epoch 260, training loss: 0.42900964617729187 = 0.35801011323928833 + 0.01 * 7.099954128265381
Epoch 260, val loss: 0.721025824546814
Epoch 270, training loss: 0.39405882358551025 = 0.323166161775589 + 0.01 * 7.089265823364258
Epoch 270, val loss: 0.71202152967453
Epoch 280, training loss: 0.36185213923454285 = 0.29101797938346863 + 0.01 * 7.08341646194458
Epoch 280, val loss: 0.7058230638504028
Epoch 290, training loss: 0.3316642940044403 = 0.2609073519706726 + 0.01 * 7.075695037841797
Epoch 290, val loss: 0.7016977667808533
Epoch 300, training loss: 0.3030731976032257 = 0.23242440819740295 + 0.01 * 7.064879417419434
Epoch 300, val loss: 0.6992301940917969
Epoch 310, training loss: 0.27614712715148926 = 0.20558574795722961 + 0.01 * 7.056138515472412
Epoch 310, val loss: 0.6983215808868408
Epoch 320, training loss: 0.2512660324573517 = 0.18072378635406494 + 0.01 * 7.054224967956543
Epoch 320, val loss: 0.6990668773651123
Epoch 330, training loss: 0.22872185707092285 = 0.15824034810066223 + 0.01 * 7.04815149307251
Epoch 330, val loss: 0.70159912109375
Epoch 340, training loss: 0.20865079760551453 = 0.13832233846187592 + 0.01 * 7.032846450805664
Epoch 340, val loss: 0.7059827446937561
Epoch 350, training loss: 0.1912568360567093 = 0.12092144787311554 + 0.01 * 7.033539295196533
Epoch 350, val loss: 0.7121933102607727
Epoch 360, training loss: 0.17603731155395508 = 0.10586196929216385 + 0.01 * 7.017535209655762
Epoch 360, val loss: 0.7201903462409973
Epoch 370, training loss: 0.162934809923172 = 0.09288075566291809 + 0.01 * 7.005404949188232
Epoch 370, val loss: 0.7297777533531189
Epoch 380, training loss: 0.15170079469680786 = 0.08171118050813675 + 0.01 * 6.998961448669434
Epoch 380, val loss: 0.7405641674995422
Epoch 390, training loss: 0.14214596152305603 = 0.07210119068622589 + 0.01 * 7.004477500915527
Epoch 390, val loss: 0.7523292899131775
Epoch 400, training loss: 0.13390624523162842 = 0.0638522058725357 + 0.01 * 7.005404949188232
Epoch 400, val loss: 0.7647199630737305
Epoch 410, training loss: 0.12662363052368164 = 0.0567576065659523 + 0.01 * 6.986603260040283
Epoch 410, val loss: 0.7775424122810364
Epoch 420, training loss: 0.12042759358882904 = 0.050634611397981644 + 0.01 * 6.9792985916137695
Epoch 420, val loss: 0.7905943393707275
Epoch 430, training loss: 0.11508823931217194 = 0.04533892497420311 + 0.01 * 6.9749321937561035
Epoch 430, val loss: 0.8037554025650024
Epoch 440, training loss: 0.11044007539749146 = 0.04074624180793762 + 0.01 * 6.969383239746094
Epoch 440, val loss: 0.816829264163971
Epoch 450, training loss: 0.10641489923000336 = 0.036753762513399124 + 0.01 * 6.966113090515137
Epoch 450, val loss: 0.8297715187072754
Epoch 460, training loss: 0.10310808569192886 = 0.033279888331890106 + 0.01 * 6.9828200340271
Epoch 460, val loss: 0.8424815535545349
Epoch 470, training loss: 0.0999068096280098 = 0.030250461772084236 + 0.01 * 6.965635299682617
Epoch 470, val loss: 0.8548705577850342
Epoch 480, training loss: 0.097151018679142 = 0.02759505994617939 + 0.01 * 6.955595970153809
Epoch 480, val loss: 0.8669533133506775
Epoch 490, training loss: 0.09476429969072342 = 0.02525867335498333 + 0.01 * 6.950562477111816
Epoch 490, val loss: 0.8787645697593689
Epoch 500, training loss: 0.09267027676105499 = 0.023196136578917503 + 0.01 * 6.947413921356201
Epoch 500, val loss: 0.8902333974838257
Epoch 510, training loss: 0.09083231538534164 = 0.02137017995119095 + 0.01 * 6.946213722229004
Epoch 510, val loss: 0.901405394077301
Epoch 520, training loss: 0.08910787105560303 = 0.019748078659176826 + 0.01 * 6.93597936630249
Epoch 520, val loss: 0.9121772646903992
Epoch 530, training loss: 0.08800984174013138 = 0.01830124668776989 + 0.01 * 6.970859527587891
Epoch 530, val loss: 0.9226692914962769
Epoch 540, training loss: 0.08630356192588806 = 0.017010221257805824 + 0.01 * 6.92933464050293
Epoch 540, val loss: 0.9327436685562134
Epoch 550, training loss: 0.08517402410507202 = 0.0158525537699461 + 0.01 * 6.932147026062012
Epoch 550, val loss: 0.9425566792488098
Epoch 560, training loss: 0.08404846489429474 = 0.014808591455221176 + 0.01 * 6.92398738861084
Epoch 560, val loss: 0.9520258903503418
Epoch 570, training loss: 0.08306187391281128 = 0.01386469230055809 + 0.01 * 6.919717788696289
Epoch 570, val loss: 0.9612523913383484
Epoch 580, training loss: 0.08222224563360214 = 0.013009135611355305 + 0.01 * 6.921310901641846
Epoch 580, val loss: 0.9701527953147888
Epoch 590, training loss: 0.08132912218570709 = 0.01223032921552658 + 0.01 * 6.909879207611084
Epoch 590, val loss: 0.9787865281105042
Epoch 600, training loss: 0.08100957423448563 = 0.011519613675773144 + 0.01 * 6.948996543884277
Epoch 600, val loss: 0.9872046709060669
Epoch 610, training loss: 0.07992033660411835 = 0.010871422477066517 + 0.01 * 6.904891014099121
Epoch 610, val loss: 0.9952972531318665
Epoch 620, training loss: 0.07929012179374695 = 0.010278643108904362 + 0.01 * 6.901148319244385
Epoch 620, val loss: 1.0031346082687378
Epoch 630, training loss: 0.07876725494861603 = 0.009733956307172775 + 0.01 * 6.903330326080322
Epoch 630, val loss: 1.0107709169387817
Epoch 640, training loss: 0.07816428691148758 = 0.009232931770384312 + 0.01 * 6.893136024475098
Epoch 640, val loss: 1.0181432962417603
Epoch 650, training loss: 0.07779081165790558 = 0.008770260028541088 + 0.01 * 6.902055263519287
Epoch 650, val loss: 1.0253517627716064
Epoch 660, training loss: 0.07736626267433167 = 0.00834379531443119 + 0.01 * 6.902246952056885
Epoch 660, val loss: 1.0323268175125122
Epoch 670, training loss: 0.07681802660226822 = 0.007948762737214565 + 0.01 * 6.886926174163818
Epoch 670, val loss: 1.0390634536743164
Epoch 680, training loss: 0.07641714811325073 = 0.007582805585116148 + 0.01 * 6.883434295654297
Epoch 680, val loss: 1.045686960220337
Epoch 690, training loss: 0.07599327713251114 = 0.00724254222586751 + 0.01 * 6.875073432922363
Epoch 690, val loss: 1.0520849227905273
Epoch 700, training loss: 0.07580927759408951 = 0.006926033180207014 + 0.01 * 6.888324737548828
Epoch 700, val loss: 1.05832040309906
Epoch 710, training loss: 0.07531414926052094 = 0.006631558761000633 + 0.01 * 6.868258953094482
Epoch 710, val loss: 1.064391851425171
Epoch 720, training loss: 0.07525762915611267 = 0.006357093807309866 + 0.01 * 6.890053749084473
Epoch 720, val loss: 1.0703098773956299
Epoch 730, training loss: 0.07479821145534515 = 0.0061013363301754 + 0.01 * 6.869687080383301
Epoch 730, val loss: 1.0759468078613281
Epoch 740, training loss: 0.0745169073343277 = 0.005861928220838308 + 0.01 * 6.865498065948486
Epoch 740, val loss: 1.0814663171768188
Epoch 750, training loss: 0.0742209404706955 = 0.005637776106595993 + 0.01 * 6.858315944671631
Epoch 750, val loss: 1.0867680311203003
Epoch 760, training loss: 0.07386142015457153 = 0.005427490454167128 + 0.01 * 6.843393802642822
Epoch 760, val loss: 1.092039704322815
Epoch 770, training loss: 0.07396893203258514 = 0.0052298386581242085 + 0.01 * 6.8739094734191895
Epoch 770, val loss: 1.0971121788024902
Epoch 780, training loss: 0.07344038784503937 = 0.00504482164978981 + 0.01 * 6.839556694030762
Epoch 780, val loss: 1.1019388437271118
Epoch 790, training loss: 0.0733204334974289 = 0.004869814030826092 + 0.01 * 6.845062732696533
Epoch 790, val loss: 1.1067923307418823
Epoch 800, training loss: 0.073115274310112 = 0.004705630708485842 + 0.01 * 6.840964317321777
Epoch 800, val loss: 1.1113969087600708
Epoch 810, training loss: 0.07287165522575378 = 0.004549919627606869 + 0.01 * 6.8321733474731445
Epoch 810, val loss: 1.1160420179367065
Epoch 820, training loss: 0.07288946211338043 = 0.004403030965477228 + 0.01 * 6.848642826080322
Epoch 820, val loss: 1.120498538017273
Epoch 830, training loss: 0.07256124168634415 = 0.004264700226485729 + 0.01 * 6.829654693603516
Epoch 830, val loss: 1.1247144937515259
Epoch 840, training loss: 0.07220662385225296 = 0.004133469890803099 + 0.01 * 6.807315349578857
Epoch 840, val loss: 1.1289784908294678
Epoch 850, training loss: 0.07212281972169876 = 0.0040101585909724236 + 0.01 * 6.8112664222717285
Epoch 850, val loss: 1.1329536437988281
Epoch 860, training loss: 0.07201016694307327 = 0.0038926368579268456 + 0.01 * 6.811752796173096
Epoch 860, val loss: 1.1369577646255493
Epoch 870, training loss: 0.07194678485393524 = 0.0037820604629814625 + 0.01 * 6.81647253036499
Epoch 870, val loss: 1.1408077478408813
Epoch 880, training loss: 0.07164865732192993 = 0.0036754633765667677 + 0.01 * 6.7973198890686035
Epoch 880, val loss: 1.1445871591567993
Epoch 890, training loss: 0.07172892987728119 = 0.0035749953240156174 + 0.01 * 6.815393447875977
Epoch 890, val loss: 1.1482511758804321
Epoch 900, training loss: 0.07140955328941345 = 0.0034793925005942583 + 0.01 * 6.79301643371582
Epoch 900, val loss: 1.1518220901489258
Epoch 910, training loss: 0.07145252823829651 = 0.003388106357306242 + 0.01 * 6.8064422607421875
Epoch 910, val loss: 1.1553318500518799
Epoch 920, training loss: 0.07105232030153275 = 0.0033001897390931845 + 0.01 * 6.775213241577148
Epoch 920, val loss: 1.1587026119232178
Epoch 930, training loss: 0.07104761898517609 = 0.003217077814042568 + 0.01 * 6.783054351806641
Epoch 930, val loss: 1.1620770692825317
Epoch 940, training loss: 0.0708354040980339 = 0.003137642052024603 + 0.01 * 6.769776821136475
Epoch 940, val loss: 1.165230631828308
Epoch 950, training loss: 0.07081862539052963 = 0.0030611762776970863 + 0.01 * 6.775745391845703
Epoch 950, val loss: 1.1685097217559814
Epoch 960, training loss: 0.07075916230678558 = 0.002988318679854274 + 0.01 * 6.777084827423096
Epoch 960, val loss: 1.1714556217193604
Epoch 970, training loss: 0.07051485776901245 = 0.002917391015216708 + 0.01 * 6.759746551513672
Epoch 970, val loss: 1.174500823020935
Epoch 980, training loss: 0.07066618651151657 = 0.0028485471848398447 + 0.01 * 6.781763553619385
Epoch 980, val loss: 1.1775928735733032
Epoch 990, training loss: 0.07041782885789871 = 0.002782910829409957 + 0.01 * 6.763492107391357
Epoch 990, val loss: 1.180483341217041
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8318397469688983
=== training gcn model ===
Epoch 0, training loss: 2.026245355606079 = 1.940277099609375 + 0.01 * 8.59682846069336
Epoch 0, val loss: 1.942936658859253
Epoch 10, training loss: 2.0164990425109863 = 1.93053138256073 + 0.01 * 8.596769332885742
Epoch 10, val loss: 1.9331945180892944
Epoch 20, training loss: 2.0041604042053223 = 1.9181948900222778 + 0.01 * 8.596545219421387
Epoch 20, val loss: 1.9203145503997803
Epoch 30, training loss: 1.9863406419754028 = 1.9003827571868896 + 0.01 * 8.595786094665527
Epoch 30, val loss: 1.9013255834579468
Epoch 40, training loss: 1.9596216678619385 = 1.8737070560455322 + 0.01 * 8.591459274291992
Epoch 40, val loss: 1.8728798627853394
Epoch 50, training loss: 1.9222230911254883 = 1.836598515510559 + 0.01 * 8.562461853027344
Epoch 50, val loss: 1.8349251747131348
Epoch 60, training loss: 1.879764437675476 = 1.7953741550445557 + 0.01 * 8.439030647277832
Epoch 60, val loss: 1.796845555305481
Epoch 70, training loss: 1.838164210319519 = 1.7561657428741455 + 0.01 * 8.199844360351562
Epoch 70, val loss: 1.764474630355835
Epoch 80, training loss: 1.7859400510787964 = 1.7057230472564697 + 0.01 * 8.021700859069824
Epoch 80, val loss: 1.722070336341858
Epoch 90, training loss: 1.7138248682022095 = 1.635887622833252 + 0.01 * 7.793728828430176
Epoch 90, val loss: 1.6610972881317139
Epoch 100, training loss: 1.623356580734253 = 1.546705722808838 + 0.01 * 7.665082931518555
Epoch 100, val loss: 1.5848822593688965
Epoch 110, training loss: 1.5235024690628052 = 1.4474973678588867 + 0.01 * 7.6005120277404785
Epoch 110, val loss: 1.504335641860962
Epoch 120, training loss: 1.4230475425720215 = 1.3476779460906982 + 0.01 * 7.536956310272217
Epoch 120, val loss: 1.4256004095077515
Epoch 130, training loss: 1.3260606527328491 = 1.2516652345657349 + 0.01 * 7.439546585083008
Epoch 130, val loss: 1.3521889448165894
Epoch 140, training loss: 1.2348026037216187 = 1.1612977981567383 + 0.01 * 7.350484848022461
Epoch 140, val loss: 1.2835733890533447
Epoch 150, training loss: 1.1518361568450928 = 1.0787146091461182 + 0.01 * 7.312160968780518
Epoch 150, val loss: 1.221429705619812
Epoch 160, training loss: 1.0782291889190674 = 1.0052225589752197 + 0.01 * 7.300667762756348
Epoch 160, val loss: 1.166524887084961
Epoch 170, training loss: 1.0120761394500732 = 0.9391734004020691 + 0.01 * 7.290271759033203
Epoch 170, val loss: 1.1179099082946777
Epoch 180, training loss: 0.949736475944519 = 0.8769273161888123 + 0.01 * 7.280916690826416
Epoch 180, val loss: 1.0717641115188599
Epoch 190, training loss: 0.8878276348114014 = 0.8151048421859741 + 0.01 * 7.272282123565674
Epoch 190, val loss: 1.0260815620422363
Epoch 200, training loss: 0.8245577812194824 = 0.751933753490448 + 0.01 * 7.262402534484863
Epoch 200, val loss: 0.9797407388687134
Epoch 210, training loss: 0.7597416043281555 = 0.6872387528419495 + 0.01 * 7.250285625457764
Epoch 210, val loss: 0.9326531887054443
Epoch 220, training loss: 0.6942949295043945 = 0.6219373345375061 + 0.01 * 7.235758304595947
Epoch 220, val loss: 0.8860335350036621
Epoch 230, training loss: 0.6297197937965393 = 0.5575554966926575 + 0.01 * 7.216428756713867
Epoch 230, val loss: 0.841619610786438
Epoch 240, training loss: 0.5679051280021667 = 0.4959287941455841 + 0.01 * 7.197631359100342
Epoch 240, val loss: 0.8009196519851685
Epoch 250, training loss: 0.5103588700294495 = 0.4386178255081177 + 0.01 * 7.174103260040283
Epoch 250, val loss: 0.7655304074287415
Epoch 260, training loss: 0.45796555280685425 = 0.38643786311149597 + 0.01 * 7.152769088745117
Epoch 260, val loss: 0.736496090888977
Epoch 270, training loss: 0.41069895029067993 = 0.33933621644973755 + 0.01 * 7.136274337768555
Epoch 270, val loss: 0.7139257788658142
Epoch 280, training loss: 0.36793631315231323 = 0.2967289984226227 + 0.01 * 7.12073278427124
Epoch 280, val loss: 0.6971556544303894
Epoch 290, training loss: 0.3291413187980652 = 0.2580679655075073 + 0.01 * 7.107336521148682
Epoch 290, val loss: 0.6851431131362915
Epoch 300, training loss: 0.2942643165588379 = 0.22315944731235504 + 0.01 * 7.110488414764404
Epoch 300, val loss: 0.6769916415214539
Epoch 310, training loss: 0.26303672790527344 = 0.19210588932037354 + 0.01 * 7.093083381652832
Epoch 310, val loss: 0.6722234487533569
Epoch 320, training loss: 0.2357930839061737 = 0.16498009860515594 + 0.01 * 7.081299781799316
Epoch 320, val loss: 0.6704622507095337
Epoch 330, training loss: 0.21243570744991302 = 0.141683429479599 + 0.01 * 7.075228214263916
Epoch 330, val loss: 0.6714869141578674
Epoch 340, training loss: 0.1926005482673645 = 0.12193645536899567 + 0.01 * 7.066409111022949
Epoch 340, val loss: 0.6749809980392456
Epoch 350, training loss: 0.17587685585021973 = 0.10531100630760193 + 0.01 * 7.056585788726807
Epoch 350, val loss: 0.6805603504180908
Epoch 360, training loss: 0.16193178296089172 = 0.09133826196193695 + 0.01 * 7.059352397918701
Epoch 360, val loss: 0.6878567934036255
Epoch 370, training loss: 0.15003913640975952 = 0.07958579063415527 + 0.01 * 7.04533576965332
Epoch 370, val loss: 0.6964675784111023
Epoch 380, training loss: 0.1400637924671173 = 0.06967317312955856 + 0.01 * 7.0390625
Epoch 380, val loss: 0.7060466408729553
Epoch 390, training loss: 0.13167396187782288 = 0.06128820776939392 + 0.01 * 7.038575649261475
Epoch 390, val loss: 0.716324508190155
Epoch 400, training loss: 0.12441159039735794 = 0.05416332185268402 + 0.01 * 7.024827003479004
Epoch 400, val loss: 0.7271493673324585
Epoch 410, training loss: 0.11826950311660767 = 0.04807418957352638 + 0.01 * 7.01953125
Epoch 410, val loss: 0.7383154034614563
Epoch 420, training loss: 0.11307898163795471 = 0.04284802824258804 + 0.01 * 7.023095607757568
Epoch 420, val loss: 0.7497057914733887
Epoch 430, training loss: 0.10847776383161545 = 0.038349322974681854 + 0.01 * 7.012844562530518
Epoch 430, val loss: 0.7611097693443298
Epoch 440, training loss: 0.10456445813179016 = 0.034465521574020386 + 0.01 * 7.009893894195557
Epoch 440, val loss: 0.772555410861969
Epoch 450, training loss: 0.10113707184791565 = 0.03110305219888687 + 0.01 * 7.003401756286621
Epoch 450, val loss: 0.7838303446769714
Epoch 460, training loss: 0.09812818467617035 = 0.028182201087474823 + 0.01 * 6.994598388671875
Epoch 460, val loss: 0.7949872612953186
Epoch 470, training loss: 0.0955275446176529 = 0.02563324198126793 + 0.01 * 6.989430904388428
Epoch 470, val loss: 0.805967390537262
Epoch 480, training loss: 0.09333827346563339 = 0.02340446412563324 + 0.01 * 6.993381023406982
Epoch 480, val loss: 0.8166757822036743
Epoch 490, training loss: 0.091254323720932 = 0.021449614316225052 + 0.01 * 6.980471611022949
Epoch 490, val loss: 0.8271901607513428
Epoch 500, training loss: 0.08944739401340485 = 0.019725751131772995 + 0.01 * 6.972165107727051
Epoch 500, val loss: 0.8374424576759338
Epoch 510, training loss: 0.08803549408912659 = 0.018197637051343918 + 0.01 * 6.983786106109619
Epoch 510, val loss: 0.8474359512329102
Epoch 520, training loss: 0.086491659283638 = 0.016841571778059006 + 0.01 * 6.96500825881958
Epoch 520, val loss: 0.8571688532829285
Epoch 530, training loss: 0.08522459864616394 = 0.015631986781954765 + 0.01 * 6.959261417388916
Epoch 530, val loss: 0.8666146397590637
Epoch 540, training loss: 0.08430340886116028 = 0.014548469334840775 + 0.01 * 6.975493431091309
Epoch 540, val loss: 0.8757732510566711
Epoch 550, training loss: 0.08306252956390381 = 0.013577014207839966 + 0.01 * 6.948551654815674
Epoch 550, val loss: 0.884680986404419
Epoch 560, training loss: 0.0821845754981041 = 0.012701613828539848 + 0.01 * 6.948296070098877
Epoch 560, val loss: 0.893391489982605
Epoch 570, training loss: 0.08148554712533951 = 0.011909401044249535 + 0.01 * 6.957614421844482
Epoch 570, val loss: 0.9017901420593262
Epoch 580, training loss: 0.08057674765586853 = 0.01119233574718237 + 0.01 * 6.938441276550293
Epoch 580, val loss: 0.9099181294441223
Epoch 590, training loss: 0.07990608364343643 = 0.010540822520852089 + 0.01 * 6.936526298522949
Epoch 590, val loss: 0.917884349822998
Epoch 600, training loss: 0.07930438220500946 = 0.009946422651410103 + 0.01 * 6.93579626083374
Epoch 600, val loss: 0.9255529046058655
Epoch 610, training loss: 0.07872017472982407 = 0.009403197094798088 + 0.01 * 6.931697368621826
Epoch 610, val loss: 0.933039128780365
Epoch 620, training loss: 0.07816650718450546 = 0.0089055011048913 + 0.01 * 6.926101207733154
Epoch 620, val loss: 0.9402757883071899
Epoch 630, training loss: 0.07776414602994919 = 0.008448460139334202 + 0.01 * 6.9315690994262695
Epoch 630, val loss: 0.947350800037384
Epoch 640, training loss: 0.07715743035078049 = 0.008028438314795494 + 0.01 * 6.912899494171143
Epoch 640, val loss: 0.954129695892334
Epoch 650, training loss: 0.0767478197813034 = 0.007641636300832033 + 0.01 * 6.910618305206299
Epoch 650, val loss: 0.9607462286949158
Epoch 660, training loss: 0.07643929123878479 = 0.007283841259777546 + 0.01 * 6.915544509887695
Epoch 660, val loss: 0.9671759605407715
Epoch 670, training loss: 0.07607822865247726 = 0.006953075062483549 + 0.01 * 6.912515163421631
Epoch 670, val loss: 0.9734582901000977
Epoch 680, training loss: 0.07564288377761841 = 0.00664630439132452 + 0.01 * 6.899658203125
Epoch 680, val loss: 0.9795253276824951
Epoch 690, training loss: 0.07542752474546432 = 0.0063612693920731544 + 0.01 * 6.906625747680664
Epoch 690, val loss: 0.9854567050933838
Epoch 700, training loss: 0.07500472664833069 = 0.00609597098082304 + 0.01 * 6.890875816345215
Epoch 700, val loss: 0.991188108921051
Epoch 710, training loss: 0.07476582378149033 = 0.005848773755133152 + 0.01 * 6.891705513000488
Epoch 710, val loss: 0.9967921376228333
Epoch 720, training loss: 0.0744933933019638 = 0.005617712624371052 + 0.01 * 6.88756799697876
Epoch 720, val loss: 1.002217411994934
Epoch 730, training loss: 0.07419321686029434 = 0.005401718430221081 + 0.01 * 6.879150390625
Epoch 730, val loss: 1.0075372457504272
Epoch 740, training loss: 0.07430974394083023 = 0.005199110601097345 + 0.01 * 6.9110636711120605
Epoch 740, val loss: 1.0127203464508057
Epoch 750, training loss: 0.07386637479066849 = 0.005009791813790798 + 0.01 * 6.885658264160156
Epoch 750, val loss: 1.0176687240600586
Epoch 760, training loss: 0.07354637235403061 = 0.004831792786717415 + 0.01 * 6.871458530426025
Epoch 760, val loss: 1.0225350856781006
Epoch 770, training loss: 0.07337261736392975 = 0.004664179868996143 + 0.01 * 6.870843887329102
Epoch 770, val loss: 1.0273045301437378
Epoch 780, training loss: 0.07316216826438904 = 0.0045065260492265224 + 0.01 * 6.865563869476318
Epoch 780, val loss: 1.0319440364837646
Epoch 790, training loss: 0.07296785712242126 = 0.0043577272444963455 + 0.01 * 6.861013412475586
Epoch 790, val loss: 1.0364288091659546
Epoch 800, training loss: 0.07273906469345093 = 0.00421735318377614 + 0.01 * 6.852170944213867
Epoch 800, val loss: 1.0408334732055664
Epoch 810, training loss: 0.07297208160161972 = 0.004084615968167782 + 0.01 * 6.888746738433838
Epoch 810, val loss: 1.0451805591583252
Epoch 820, training loss: 0.07249817997217178 = 0.003959446679800749 + 0.01 * 6.853873252868652
Epoch 820, val loss: 1.0492769479751587
Epoch 830, training loss: 0.07271889597177505 = 0.003840756369754672 + 0.01 * 6.887813568115234
Epoch 830, val loss: 1.0533066987991333
Epoch 840, training loss: 0.07221266627311707 = 0.003728704759851098 + 0.01 * 6.848396301269531
Epoch 840, val loss: 1.0572593212127686
Epoch 850, training loss: 0.0721101313829422 = 0.0036223940551280975 + 0.01 * 6.848773956298828
Epoch 850, val loss: 1.0611228942871094
Epoch 860, training loss: 0.07187718898057938 = 0.0035211180802434683 + 0.01 * 6.835607528686523
Epoch 860, val loss: 1.0648788213729858
Epoch 870, training loss: 0.07213754206895828 = 0.003424799768254161 + 0.01 * 6.871273994445801
Epoch 870, val loss: 1.0686317682266235
Epoch 880, training loss: 0.07159799337387085 = 0.003333290573209524 + 0.01 * 6.826470375061035
Epoch 880, val loss: 1.072142243385315
Epoch 890, training loss: 0.07147806882858276 = 0.00324625289067626 + 0.01 * 6.823181629180908
Epoch 890, val loss: 1.0756891965866089
Epoch 900, training loss: 0.0715588927268982 = 0.0031632615718990564 + 0.01 * 6.839562892913818
Epoch 900, val loss: 1.0791163444519043
Epoch 910, training loss: 0.07126303017139435 = 0.003084024181589484 + 0.01 * 6.817900657653809
Epoch 910, val loss: 1.0824332237243652
Epoch 920, training loss: 0.07120576500892639 = 0.0030084236059337854 + 0.01 * 6.8197340965271
Epoch 920, val loss: 1.085687279701233
Epoch 930, training loss: 0.07158323377370834 = 0.0029362088534981012 + 0.01 * 6.864703178405762
Epoch 930, val loss: 1.0889393091201782
Epoch 940, training loss: 0.0709715411067009 = 0.0028674122877418995 + 0.01 * 6.810412406921387
Epoch 940, val loss: 1.0919921398162842
Epoch 950, training loss: 0.07091204077005386 = 0.002801547758281231 + 0.01 * 6.811049938201904
Epoch 950, val loss: 1.0950559377670288
Epoch 960, training loss: 0.07099228352308273 = 0.002738539595156908 + 0.01 * 6.825374126434326
Epoch 960, val loss: 1.0980714559555054
Epoch 970, training loss: 0.07088835537433624 = 0.0026780562475323677 + 0.01 * 6.821030139923096
Epoch 970, val loss: 1.1009093523025513
Epoch 980, training loss: 0.07066699117422104 = 0.002620392944663763 + 0.01 * 6.804659843444824
Epoch 980, val loss: 1.1038089990615845
Epoch 990, training loss: 0.0704406127333641 = 0.002565065398812294 + 0.01 * 6.787554740905762
Epoch 990, val loss: 1.106539011001587
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.0260653495788574 = 1.9400973320007324 + 0.01 * 8.596810340881348
Epoch 0, val loss: 1.9407389163970947
Epoch 10, training loss: 2.0162312984466553 = 1.9302639961242676 + 0.01 * 8.596731185913086
Epoch 10, val loss: 1.9315569400787354
Epoch 20, training loss: 2.003610372543335 = 1.917645812034607 + 0.01 * 8.596458435058594
Epoch 20, val loss: 1.919427514076233
Epoch 30, training loss: 1.9851648807525635 = 1.8992100954055786 + 0.01 * 8.595484733581543
Epoch 30, val loss: 1.9014978408813477
Epoch 40, training loss: 1.9572941064834595 = 1.8714027404785156 + 0.01 * 8.589132308959961
Epoch 40, val loss: 1.8747445344924927
Epoch 50, training loss: 1.918470025062561 = 1.833052635192871 + 0.01 * 8.541738510131836
Epoch 50, val loss: 1.839529037475586
Epoch 60, training loss: 1.874151587486267 = 1.7916111946105957 + 0.01 * 8.254043579101562
Epoch 60, val loss: 1.8043749332427979
Epoch 70, training loss: 1.8330036401748657 = 1.7518651485443115 + 0.01 * 8.113848686218262
Epoch 70, val loss: 1.7684993743896484
Epoch 80, training loss: 1.77748703956604 = 1.6981889009475708 + 0.01 * 7.929815769195557
Epoch 80, val loss: 1.7171729803085327
Epoch 90, training loss: 1.7024272680282593 = 1.625152587890625 + 0.01 * 7.727470397949219
Epoch 90, val loss: 1.6522616147994995
Epoch 100, training loss: 1.6100592613220215 = 1.534592628479004 + 0.01 * 7.54666805267334
Epoch 100, val loss: 1.576195478439331
Epoch 110, training loss: 1.5128540992736816 = 1.438011884689331 + 0.01 * 7.484227657318115
Epoch 110, val loss: 1.4946707487106323
Epoch 120, training loss: 1.418373703956604 = 1.344085693359375 + 0.01 * 7.428802967071533
Epoch 120, val loss: 1.4181716442108154
Epoch 130, training loss: 1.3267873525619507 = 1.2530994415283203 + 0.01 * 7.3687944412231445
Epoch 130, val loss: 1.3452121019363403
Epoch 140, training loss: 1.2379480600357056 = 1.1648342609405518 + 0.01 * 7.311380386352539
Epoch 140, val loss: 1.2759424448013306
Epoch 150, training loss: 1.1540184020996094 = 1.0813546180725098 + 0.01 * 7.266373634338379
Epoch 150, val loss: 1.212191104888916
Epoch 160, training loss: 1.0767121315002441 = 1.0042895078659058 + 0.01 * 7.242259502410889
Epoch 160, val loss: 1.1547118425369263
Epoch 170, training loss: 1.0051771402359009 = 0.9329996705055237 + 0.01 * 7.217751502990723
Epoch 170, val loss: 1.1019178628921509
Epoch 180, training loss: 0.9373961091041565 = 0.8654592633247375 + 0.01 * 7.193686485290527
Epoch 180, val loss: 1.0512841939926147
Epoch 190, training loss: 0.8721609115600586 = 0.8004617094993591 + 0.01 * 7.169918060302734
Epoch 190, val loss: 1.0017902851104736
Epoch 200, training loss: 0.810055136680603 = 0.7385721802711487 + 0.01 * 7.1482930183410645
Epoch 200, val loss: 0.9543341994285583
Epoch 210, training loss: 0.7523521184921265 = 0.6810499429702759 + 0.01 * 7.130216598510742
Epoch 210, val loss: 0.9113539457321167
Epoch 220, training loss: 0.6991890668869019 = 0.6280638575553894 + 0.01 * 7.112517833709717
Epoch 220, val loss: 0.8737499713897705
Epoch 230, training loss: 0.6494208574295044 = 0.5784220695495605 + 0.01 * 7.099881649017334
Epoch 230, val loss: 0.8417755961418152
Epoch 240, training loss: 0.6018462777137756 = 0.5310050845146179 + 0.01 * 7.08411979675293
Epoch 240, val loss: 0.8152791261672974
Epoch 250, training loss: 0.556148886680603 = 0.4853460490703583 + 0.01 * 7.080285549163818
Epoch 250, val loss: 0.7942798733711243
Epoch 260, training loss: 0.5117791891098022 = 0.4411255419254303 + 0.01 * 7.065363883972168
Epoch 260, val loss: 0.7785170674324036
Epoch 270, training loss: 0.4685952961444855 = 0.39800381660461426 + 0.01 * 7.059147357940674
Epoch 270, val loss: 0.767341136932373
Epoch 280, training loss: 0.42690545320510864 = 0.3561698794364929 + 0.01 * 7.073558807373047
Epoch 280, val loss: 0.7601605653762817
Epoch 290, training loss: 0.3870474696159363 = 0.3164902925491333 + 0.01 * 7.055718898773193
Epoch 290, val loss: 0.7566170692443848
Epoch 300, training loss: 0.3503924012184143 = 0.2798600494861603 + 0.01 * 7.053234577178955
Epoch 300, val loss: 0.7566270232200623
Epoch 310, training loss: 0.31733182072639465 = 0.2469264715909958 + 0.01 * 7.040534496307373
Epoch 310, val loss: 0.7602986693382263
Epoch 320, training loss: 0.288174033164978 = 0.21785257756710052 + 0.01 * 7.032146453857422
Epoch 320, val loss: 0.7675143480300903
Epoch 330, training loss: 0.262666255235672 = 0.19246681034564972 + 0.01 * 7.01994514465332
Epoch 330, val loss: 0.7780330181121826
Epoch 340, training loss: 0.24066942930221558 = 0.17044712603092194 + 0.01 * 7.022231578826904
Epoch 340, val loss: 0.7913927435874939
Epoch 350, training loss: 0.22158202528953552 = 0.151420459151268 + 0.01 * 7.01615571975708
Epoch 350, val loss: 0.8070418238639832
Epoch 360, training loss: 0.2050480842590332 = 0.13495409488677979 + 0.01 * 7.009398460388184
Epoch 360, val loss: 0.8244842290878296
Epoch 370, training loss: 0.19064074754714966 = 0.12063895910978317 + 0.01 * 7.000178813934326
Epoch 370, val loss: 0.8433094024658203
Epoch 380, training loss: 0.1780599057674408 = 0.10813004523515701 + 0.01 * 6.992987155914307
Epoch 380, val loss: 0.863067626953125
Epoch 390, training loss: 0.16731062531471252 = 0.09715450555086136 + 0.01 * 7.01561164855957
Epoch 390, val loss: 0.8834746479988098
Epoch 400, training loss: 0.15735498070716858 = 0.08749019354581833 + 0.01 * 6.986478805541992
Epoch 400, val loss: 0.904312789440155
Epoch 410, training loss: 0.1487254649400711 = 0.0789308175444603 + 0.01 * 6.979464530944824
Epoch 410, val loss: 0.9254316687583923
Epoch 420, training loss: 0.14106401801109314 = 0.07132058590650558 + 0.01 * 6.974343776702881
Epoch 420, val loss: 0.9467058181762695
Epoch 430, training loss: 0.13422401249408722 = 0.06454086303710938 + 0.01 * 6.968315124511719
Epoch 430, val loss: 0.968061089515686
Epoch 440, training loss: 0.12828956544399261 = 0.05849321559071541 + 0.01 * 6.979634761810303
Epoch 440, val loss: 0.989399790763855
Epoch 450, training loss: 0.12277449667453766 = 0.05311017110943794 + 0.01 * 6.966432571411133
Epoch 450, val loss: 1.0104550123214722
Epoch 460, training loss: 0.1179465800523758 = 0.04831157624721527 + 0.01 * 6.963500499725342
Epoch 460, val loss: 1.0311932563781738
Epoch 470, training loss: 0.1135607361793518 = 0.044020336121320724 + 0.01 * 6.954039573669434
Epoch 470, val loss: 1.051627278327942
Epoch 480, training loss: 0.10965617001056671 = 0.04017529636621475 + 0.01 * 6.948087692260742
Epoch 480, val loss: 1.0718014240264893
Epoch 490, training loss: 0.10622496157884598 = 0.036731570959091187 + 0.01 * 6.949338912963867
Epoch 490, val loss: 1.0915898084640503
Epoch 500, training loss: 0.10311488807201385 = 0.0336475595831871 + 0.01 * 6.946732997894287
Epoch 500, val loss: 1.1109578609466553
Epoch 510, training loss: 0.10024455934762955 = 0.030881797894835472 + 0.01 * 6.936275959014893
Epoch 510, val loss: 1.1299015283584595
Epoch 520, training loss: 0.0977601557970047 = 0.028396127745509148 + 0.01 * 6.936402797698975
Epoch 520, val loss: 1.148432970046997
Epoch 530, training loss: 0.09545595943927765 = 0.026162926107645035 + 0.01 * 6.929303169250488
Epoch 530, val loss: 1.1664913892745972
Epoch 540, training loss: 0.0934116393327713 = 0.02415207028388977 + 0.01 * 6.925957202911377
Epoch 540, val loss: 1.1841593980789185
Epoch 550, training loss: 0.09156890958547592 = 0.02233818918466568 + 0.01 * 6.923072338104248
Epoch 550, val loss: 1.2013930082321167
Epoch 560, training loss: 0.08985751867294312 = 0.020699601620435715 + 0.01 * 6.915791988372803
Epoch 560, val loss: 1.2182284593582153
Epoch 570, training loss: 0.08837927877902985 = 0.019218703731894493 + 0.01 * 6.916057109832764
Epoch 570, val loss: 1.2346338033676147
Epoch 580, training loss: 0.08698847144842148 = 0.017879610881209373 + 0.01 * 6.910885810852051
Epoch 580, val loss: 1.2506330013275146
Epoch 590, training loss: 0.08581432700157166 = 0.016666380688548088 + 0.01 * 6.914794921875
Epoch 590, val loss: 1.2661586999893188
Epoch 600, training loss: 0.08466167747974396 = 0.015565222129225731 + 0.01 * 6.9096455574035645
Epoch 600, val loss: 1.2813454866409302
Epoch 610, training loss: 0.0837811529636383 = 0.014563865028321743 + 0.01 * 6.92172908782959
Epoch 610, val loss: 1.2961013317108154
Epoch 620, training loss: 0.08270567655563354 = 0.01365302037447691 + 0.01 * 6.905266284942627
Epoch 620, val loss: 1.3104262351989746
Epoch 630, training loss: 0.08177404850721359 = 0.012822634540498257 + 0.01 * 6.895141124725342
Epoch 630, val loss: 1.3243480920791626
Epoch 640, training loss: 0.08109553158283234 = 0.012063688598573208 + 0.01 * 6.903184413909912
Epoch 640, val loss: 1.3378974199295044
Epoch 650, training loss: 0.08027446269989014 = 0.011369423940777779 + 0.01 * 6.890503883361816
Epoch 650, val loss: 1.3511015176773071
Epoch 660, training loss: 0.07970982044935226 = 0.010733149014413357 + 0.01 * 6.897666931152344
Epoch 660, val loss: 1.363896131515503
Epoch 670, training loss: 0.07905887067317963 = 0.010150312446057796 + 0.01 * 6.89085578918457
Epoch 670, val loss: 1.376267910003662
Epoch 680, training loss: 0.0783773884177208 = 0.00961493793874979 + 0.01 * 6.876245498657227
Epoch 680, val loss: 1.3882970809936523
Epoch 690, training loss: 0.07785461843013763 = 0.009121080860495567 + 0.01 * 6.873353958129883
Epoch 690, val loss: 1.4000555276870728
Epoch 700, training loss: 0.07735344767570496 = 0.008665050379931927 + 0.01 * 6.868840217590332
Epoch 700, val loss: 1.4114311933517456
Epoch 710, training loss: 0.07691993564367294 = 0.008243896998465061 + 0.01 * 6.8676042556762695
Epoch 710, val loss: 1.42249596118927
Epoch 720, training loss: 0.07651832699775696 = 0.007854494266211987 + 0.01 * 6.866384029388428
Epoch 720, val loss: 1.4331897497177124
Epoch 730, training loss: 0.0761905312538147 = 0.007493550423532724 + 0.01 * 6.869698524475098
Epoch 730, val loss: 1.443656086921692
Epoch 740, training loss: 0.07573731988668442 = 0.007158319000154734 + 0.01 * 6.857900142669678
Epoch 740, val loss: 1.4537619352340698
Epoch 750, training loss: 0.07554963231086731 = 0.006846347823739052 + 0.01 * 6.870328903198242
Epoch 750, val loss: 1.463654637336731
Epoch 760, training loss: 0.07511164247989655 = 0.0065557691268622875 + 0.01 * 6.855587482452393
Epoch 760, val loss: 1.473231554031372
Epoch 770, training loss: 0.07485996931791306 = 0.006284876260906458 + 0.01 * 6.857509613037109
Epoch 770, val loss: 1.4825397729873657
Epoch 780, training loss: 0.0747302994132042 = 0.006031980738043785 + 0.01 * 6.869831562042236
Epoch 780, val loss: 1.4916478395462036
Epoch 790, training loss: 0.0743255615234375 = 0.0057955291122198105 + 0.01 * 6.85300350189209
Epoch 790, val loss: 1.5004438161849976
Epoch 800, training loss: 0.07390992343425751 = 0.005574140697717667 + 0.01 * 6.833578109741211
Epoch 800, val loss: 1.5090221166610718
Epoch 810, training loss: 0.07378865033388138 = 0.005366241559386253 + 0.01 * 6.842240810394287
Epoch 810, val loss: 1.5174280405044556
Epoch 820, training loss: 0.07380275428295135 = 0.0051712156273424625 + 0.01 * 6.86315393447876
Epoch 820, val loss: 1.5255615711212158
Epoch 830, training loss: 0.0733475536108017 = 0.004988327156752348 + 0.01 * 6.835922718048096
Epoch 830, val loss: 1.5333669185638428
Epoch 840, training loss: 0.07301503419876099 = 0.004816180560737848 + 0.01 * 6.81988525390625
Epoch 840, val loss: 1.5410947799682617
Epoch 850, training loss: 0.07313032448291779 = 0.004653753247112036 + 0.01 * 6.847657680511475
Epoch 850, val loss: 1.5486445426940918
Epoch 860, training loss: 0.07280522584915161 = 0.0045006126165390015 + 0.01 * 6.830461502075195
Epoch 860, val loss: 1.5559003353118896
Epoch 870, training loss: 0.07279255241155624 = 0.004355983808636665 + 0.01 * 6.84365701675415
Epoch 870, val loss: 1.5630669593811035
Epoch 880, training loss: 0.07245281338691711 = 0.004219347145408392 + 0.01 * 6.8233466148376465
Epoch 880, val loss: 1.5699902772903442
Epoch 890, training loss: 0.0722910687327385 = 0.004090053495019674 + 0.01 * 6.820101737976074
Epoch 890, val loss: 1.5767852067947388
Epoch 900, training loss: 0.07229844480752945 = 0.003967756405472755 + 0.01 * 6.83306884765625
Epoch 900, val loss: 1.583407998085022
Epoch 910, training loss: 0.0719693973660469 = 0.0038521033711731434 + 0.01 * 6.8117289543151855
Epoch 910, val loss: 1.5897969007492065
Epoch 920, training loss: 0.07176981121301651 = 0.003742286702618003 + 0.01 * 6.80275297164917
Epoch 920, val loss: 1.5960899591445923
Epoch 930, training loss: 0.07166656106710434 = 0.0036380363162606955 + 0.01 * 6.802852630615234
Epoch 930, val loss: 1.6022310256958008
Epoch 940, training loss: 0.07156255841255188 = 0.0035389119293540716 + 0.01 * 6.802364349365234
Epoch 940, val loss: 1.608217477798462
Epoch 950, training loss: 0.07141078263521194 = 0.0034445147030055523 + 0.01 * 6.796626567840576
Epoch 950, val loss: 1.6140581369400024
Epoch 960, training loss: 0.07135263085365295 = 0.0033546825870871544 + 0.01 * 6.799795150756836
Epoch 960, val loss: 1.6198217868804932
Epoch 970, training loss: 0.07147721946239471 = 0.0032690505031496286 + 0.01 * 6.820816993713379
Epoch 970, val loss: 1.6254017353057861
Epoch 980, training loss: 0.07092605531215668 = 0.0031874231062829494 + 0.01 * 6.773862838745117
Epoch 980, val loss: 1.6307964324951172
Epoch 990, training loss: 0.071042001247406 = 0.0031094513833522797 + 0.01 * 6.793255805969238
Epoch 990, val loss: 1.6361907720565796
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8344754876120191
The final CL Acc:0.81728, 0.01492, The final GNN Acc:0.83448, 0.00215
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9418])
updated graph: torch.Size([2, 10454])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.035966634750366 = 1.949998378753662 + 0.01 * 8.596822738647461
Epoch 0, val loss: 1.9527552127838135
Epoch 10, training loss: 2.025111436843872 = 1.9391440153121948 + 0.01 * 8.596741676330566
Epoch 10, val loss: 1.9424413442611694
Epoch 20, training loss: 2.0112521648406982 = 1.9252873659133911 + 0.01 * 8.596477508544922
Epoch 20, val loss: 1.9289478063583374
Epoch 30, training loss: 1.9917118549346924 = 1.9057564735412598 + 0.01 * 8.595536231994629
Epoch 30, val loss: 1.9097970724105835
Epoch 40, training loss: 1.9636226892471313 = 1.877724051475525 + 0.01 * 8.589861869812012
Epoch 40, val loss: 1.882798671722412
Epoch 50, training loss: 1.926651954650879 = 1.8410758972167969 + 0.01 * 8.557600021362305
Epoch 50, val loss: 1.8493856191635132
Epoch 60, training loss: 1.888899564743042 = 1.8047810792922974 + 0.01 * 8.41185188293457
Epoch 60, val loss: 1.8192493915557861
Epoch 70, training loss: 1.8582861423492432 = 1.7758879661560059 + 0.01 * 8.239814758300781
Epoch 70, val loss: 1.7942688465118408
Epoch 80, training loss: 1.8197524547576904 = 1.738771915435791 + 0.01 * 8.09805679321289
Epoch 80, val loss: 1.7606202363967896
Epoch 90, training loss: 1.7655962705612183 = 1.6866865158081055 + 0.01 * 7.890979766845703
Epoch 90, val loss: 1.7179062366485596
Epoch 100, training loss: 1.6912825107574463 = 1.6149725914001465 + 0.01 * 7.630990505218506
Epoch 100, val loss: 1.662184715270996
Epoch 110, training loss: 1.6004360914230347 = 1.5253701210021973 + 0.01 * 7.506592750549316
Epoch 110, val loss: 1.5914721488952637
Epoch 120, training loss: 1.5028878450393677 = 1.428135871887207 + 0.01 * 7.475199222564697
Epoch 120, val loss: 1.5167135000228882
Epoch 130, training loss: 1.40579354763031 = 1.3315317630767822 + 0.01 * 7.426182746887207
Epoch 130, val loss: 1.4430441856384277
Epoch 140, training loss: 1.310246467590332 = 1.2364888191223145 + 0.01 * 7.375760078430176
Epoch 140, val loss: 1.3725228309631348
Epoch 150, training loss: 1.2164018154144287 = 1.1431456804275513 + 0.01 * 7.325617790222168
Epoch 150, val loss: 1.3046133518218994
Epoch 160, training loss: 1.1258612871170044 = 1.0531007051467896 + 0.01 * 7.276062488555908
Epoch 160, val loss: 1.2408446073532104
Epoch 170, training loss: 1.039786696434021 = 0.9675205945968628 + 0.01 * 7.226606845855713
Epoch 170, val loss: 1.1823501586914062
Epoch 180, training loss: 0.9581767320632935 = 0.8863322138786316 + 0.01 * 7.184452056884766
Epoch 180, val loss: 1.1287449598312378
Epoch 190, training loss: 0.8808252811431885 = 0.8092901706695557 + 0.01 * 7.153512954711914
Epoch 190, val loss: 1.0801293849945068
Epoch 200, training loss: 0.8083580732345581 = 0.7370890378952026 + 0.01 * 7.126901149749756
Epoch 200, val loss: 1.037474513053894
Epoch 210, training loss: 0.7423118948936462 = 0.6712104678153992 + 0.01 * 7.110145092010498
Epoch 210, val loss: 1.0028717517852783
Epoch 220, training loss: 0.6834213137626648 = 0.6125057339668274 + 0.01 * 7.091557502746582
Epoch 220, val loss: 0.9777239561080933
Epoch 230, training loss: 0.6313698291778564 = 0.5605854988098145 + 0.01 * 7.078436374664307
Epoch 230, val loss: 0.9619634747505188
Epoch 240, training loss: 0.5850682854652405 = 0.5143840312957764 + 0.01 * 7.068427085876465
Epoch 240, val loss: 0.954279899597168
Epoch 250, training loss: 0.5431991815567017 = 0.4725908041000366 + 0.01 * 7.060838222503662
Epoch 250, val loss: 0.9527343511581421
Epoch 260, training loss: 0.5045221447944641 = 0.4339802861213684 + 0.01 * 7.0541839599609375
Epoch 260, val loss: 0.9554364085197449
Epoch 270, training loss: 0.4682278633117676 = 0.3977136015892029 + 0.01 * 7.051427364349365
Epoch 270, val loss: 0.961516797542572
Epoch 280, training loss: 0.4341031014919281 = 0.3636745512485504 + 0.01 * 7.042855262756348
Epoch 280, val loss: 0.9711320400238037
Epoch 290, training loss: 0.402560830116272 = 0.33214884996414185 + 0.01 * 7.04119873046875
Epoch 290, val loss: 0.9846310019493103
Epoch 300, training loss: 0.3737543523311615 = 0.303408145904541 + 0.01 * 7.034620761871338
Epoch 300, val loss: 1.0019348859786987
Epoch 310, training loss: 0.34765130281448364 = 0.27733030915260315 + 0.01 * 7.03209924697876
Epoch 310, val loss: 1.0223138332366943
Epoch 320, training loss: 0.32380861043930054 = 0.25350892543792725 + 0.01 * 7.029967308044434
Epoch 320, val loss: 1.0450443029403687
Epoch 330, training loss: 0.3017352223396301 = 0.23145873844623566 + 0.01 * 7.02764892578125
Epoch 330, val loss: 1.069718837738037
Epoch 340, training loss: 0.28108203411102295 = 0.21076840162277222 + 0.01 * 7.031362533569336
Epoch 340, val loss: 1.096107006072998
Epoch 350, training loss: 0.2614150047302246 = 0.19116666913032532 + 0.01 * 7.024832248687744
Epoch 350, val loss: 1.124097466468811
Epoch 360, training loss: 0.24277280271053314 = 0.17254625260829926 + 0.01 * 7.022655010223389
Epoch 360, val loss: 1.1534308195114136
Epoch 370, training loss: 0.2251705825328827 = 0.15495310723781586 + 0.01 * 7.021748065948486
Epoch 370, val loss: 1.1838812828063965
Epoch 380, training loss: 0.20874351263046265 = 0.13850921392440796 + 0.01 * 7.023428916931152
Epoch 380, val loss: 1.215044379234314
Epoch 390, training loss: 0.19353783130645752 = 0.12334593385457993 + 0.01 * 7.019189357757568
Epoch 390, val loss: 1.2468417882919312
Epoch 400, training loss: 0.17966914176940918 = 0.10951617360115051 + 0.01 * 7.01529598236084
Epoch 400, val loss: 1.2790037393569946
Epoch 410, training loss: 0.1671437919139862 = 0.09700919687747955 + 0.01 * 7.0134596824646
Epoch 410, val loss: 1.311273455619812
Epoch 420, training loss: 0.1559847593307495 = 0.0857987254858017 + 0.01 * 7.018603801727295
Epoch 420, val loss: 1.3433822393417358
Epoch 430, training loss: 0.14593680202960968 = 0.0758374035358429 + 0.01 * 7.009940147399902
Epoch 430, val loss: 1.3751349449157715
Epoch 440, training loss: 0.13709375262260437 = 0.0670463889837265 + 0.01 * 7.00473690032959
Epoch 440, val loss: 1.406401515007019
Epoch 450, training loss: 0.12935227155685425 = 0.05933358147740364 + 0.01 * 7.001868724822998
Epoch 450, val loss: 1.4370067119598389
Epoch 460, training loss: 0.12282931059598923 = 0.052599407732486725 + 0.01 * 7.0229902267456055
Epoch 460, val loss: 1.4667553901672363
Epoch 470, training loss: 0.11676926910877228 = 0.046755824238061905 + 0.01 * 7.001344203948975
Epoch 470, val loss: 1.4955440759658813
Epoch 480, training loss: 0.1116238534450531 = 0.04169793054461479 + 0.01 * 6.992592811584473
Epoch 480, val loss: 1.523262858390808
Epoch 490, training loss: 0.1071934625506401 = 0.037325337529182434 + 0.01 * 6.986812591552734
Epoch 490, val loss: 1.5497732162475586
Epoch 500, training loss: 0.10351742058992386 = 0.033547498285770416 + 0.01 * 6.996992111206055
Epoch 500, val loss: 1.5750168561935425
Epoch 510, training loss: 0.10013225674629211 = 0.03028346411883831 + 0.01 * 6.984879016876221
Epoch 510, val loss: 1.5989596843719482
Epoch 520, training loss: 0.09724120795726776 = 0.027452990412712097 + 0.01 * 6.978822231292725
Epoch 520, val loss: 1.6216176748275757
Epoch 530, training loss: 0.09469722211360931 = 0.024988871067762375 + 0.01 * 6.9708356857299805
Epoch 530, val loss: 1.6430333852767944
Epoch 540, training loss: 0.09255613386631012 = 0.022835178300738335 + 0.01 * 6.972095489501953
Epoch 540, val loss: 1.663261890411377
Epoch 550, training loss: 0.0906120166182518 = 0.020945360884070396 + 0.01 * 6.966665267944336
Epoch 550, val loss: 1.6823854446411133
Epoch 560, training loss: 0.08902071416378021 = 0.019281698390841484 + 0.01 * 6.973901748657227
Epoch 560, val loss: 1.7004238367080688
Epoch 570, training loss: 0.08736199140548706 = 0.017811747267842293 + 0.01 * 6.955024719238281
Epoch 570, val loss: 1.7174444198608398
Epoch 580, training loss: 0.08599522709846497 = 0.0165058933198452 + 0.01 * 6.9489336013793945
Epoch 580, val loss: 1.73357355594635
Epoch 590, training loss: 0.08478041738271713 = 0.015340366400778294 + 0.01 * 6.944005489349365
Epoch 590, val loss: 1.7488824129104614
Epoch 600, training loss: 0.08380009233951569 = 0.014297647401690483 + 0.01 * 6.950244426727295
Epoch 600, val loss: 1.7633564472198486
Epoch 610, training loss: 0.08274224400520325 = 0.013361827470362186 + 0.01 * 6.938042163848877
Epoch 610, val loss: 1.7771624326705933
Epoch 620, training loss: 0.08180934190750122 = 0.012518488802015781 + 0.01 * 6.929085731506348
Epoch 620, val loss: 1.7903016805648804
Epoch 630, training loss: 0.08103196322917938 = 0.011755889281630516 + 0.01 * 6.92760705947876
Epoch 630, val loss: 1.8028335571289062
Epoch 640, training loss: 0.08035658299922943 = 0.011064973659813404 + 0.01 * 6.929161071777344
Epoch 640, val loss: 1.814810872077942
Epoch 650, training loss: 0.07964976876974106 = 0.010438206605613232 + 0.01 * 6.921156406402588
Epoch 650, val loss: 1.8262112140655518
Epoch 660, training loss: 0.07889895141124725 = 0.009866707026958466 + 0.01 * 6.903224945068359
Epoch 660, val loss: 1.83719801902771
Epoch 670, training loss: 0.07880530506372452 = 0.009343636222183704 + 0.01 * 6.946166515350342
Epoch 670, val loss: 1.8475978374481201
Epoch 680, training loss: 0.07791218161582947 = 0.008863760158419609 + 0.01 * 6.904841899871826
Epoch 680, val loss: 1.85773503780365
Epoch 690, training loss: 0.0773538276553154 = 0.00842366088181734 + 0.01 * 6.893016338348389
Epoch 690, val loss: 1.8673533201217651
Epoch 700, training loss: 0.07694611698389053 = 0.008018003776669502 + 0.01 * 6.8928117752075195
Epoch 700, val loss: 1.8766127824783325
Epoch 710, training loss: 0.07654020190238953 = 0.007643625605851412 + 0.01 * 6.889657497406006
Epoch 710, val loss: 1.8855258226394653
Epoch 720, training loss: 0.07613686472177505 = 0.007297344505786896 + 0.01 * 6.8839521408081055
Epoch 720, val loss: 1.8941327333450317
Epoch 730, training loss: 0.07569582015275955 = 0.006976775825023651 + 0.01 * 6.871904373168945
Epoch 730, val loss: 1.902231216430664
Epoch 740, training loss: 0.07540752738714218 = 0.006679318845272064 + 0.01 * 6.872820854187012
Epoch 740, val loss: 1.910211443901062
Epoch 750, training loss: 0.07520721107721329 = 0.006402663886547089 + 0.01 * 6.8804545402526855
Epoch 750, val loss: 1.9179348945617676
Epoch 760, training loss: 0.07478150725364685 = 0.006144596263766289 + 0.01 * 6.863691329956055
Epoch 760, val loss: 1.9253000020980835
Epoch 770, training loss: 0.07450886070728302 = 0.005903644021600485 + 0.01 * 6.8605217933654785
Epoch 770, val loss: 1.932432770729065
Epoch 780, training loss: 0.07422614097595215 = 0.005678514484316111 + 0.01 * 6.854763031005859
Epoch 780, val loss: 1.9392666816711426
Epoch 790, training loss: 0.0741715356707573 = 0.005467756185680628 + 0.01 * 6.870378017425537
Epoch 790, val loss: 1.9459391832351685
Epoch 800, training loss: 0.0737505853176117 = 0.0052699800580739975 + 0.01 * 6.848060131072998
Epoch 800, val loss: 1.9523963928222656
Epoch 810, training loss: 0.07379462569952011 = 0.005084181670099497 + 0.01 * 6.871044635772705
Epoch 810, val loss: 1.9585211277008057
Epoch 820, training loss: 0.07339178025722504 = 0.004909798968583345 + 0.01 * 6.848198413848877
Epoch 820, val loss: 1.9645498991012573
Epoch 830, training loss: 0.07316349446773529 = 0.004745436832308769 + 0.01 * 6.841805934906006
Epoch 830, val loss: 1.970433235168457
Epoch 840, training loss: 0.07300262898206711 = 0.004590551368892193 + 0.01 * 6.841207981109619
Epoch 840, val loss: 1.9759647846221924
Epoch 850, training loss: 0.07271680980920792 = 0.004444212187081575 + 0.01 * 6.8272600173950195
Epoch 850, val loss: 1.9814629554748535
Epoch 860, training loss: 0.07256928086280823 = 0.0043060616590082645 + 0.01 * 6.826321601867676
Epoch 860, val loss: 1.9866256713867188
Epoch 870, training loss: 0.07239525020122528 = 0.004175475798547268 + 0.01 * 6.821977138519287
Epoch 870, val loss: 1.9917746782302856
Epoch 880, training loss: 0.07228003442287445 = 0.004051858093589544 + 0.01 * 6.822817802429199
Epoch 880, val loss: 1.9966181516647339
Epoch 890, training loss: 0.07228866964578629 = 0.003934806678444147 + 0.01 * 6.835386276245117
Epoch 890, val loss: 2.0013320446014404
Epoch 900, training loss: 0.07189398258924484 = 0.003823670791462064 + 0.01 * 6.807031631469727
Epoch 900, val loss: 2.0060763359069824
Epoch 910, training loss: 0.07185479998588562 = 0.0037181051447987556 + 0.01 * 6.813669204711914
Epoch 910, val loss: 2.0104856491088867
Epoch 920, training loss: 0.07172513753175735 = 0.003617687150835991 + 0.01 * 6.8107452392578125
Epoch 920, val loss: 2.0148961544036865
Epoch 930, training loss: 0.07149957120418549 = 0.0035221767611801624 + 0.01 * 6.797739505767822
Epoch 930, val loss: 2.019062042236328
Epoch 940, training loss: 0.07154687494039536 = 0.0034311655908823013 + 0.01 * 6.8115715980529785
Epoch 940, val loss: 2.0231950283050537
Epoch 950, training loss: 0.07133633643388748 = 0.0033445036970078945 + 0.01 * 6.799183368682861
Epoch 950, val loss: 2.027064800262451
Epoch 960, training loss: 0.071429044008255 = 0.003261975245550275 + 0.01 * 6.816706657409668
Epoch 960, val loss: 2.0309643745422363
Epoch 970, training loss: 0.07117364555597305 = 0.0031830111984163523 + 0.01 * 6.799063205718994
Epoch 970, val loss: 2.0346810817718506
Epoch 980, training loss: 0.07120509445667267 = 0.0031077140010893345 + 0.01 * 6.8097381591796875
Epoch 980, val loss: 2.0382089614868164
Epoch 990, training loss: 0.07099518179893494 = 0.0030356459319591522 + 0.01 * 6.795953273773193
Epoch 990, val loss: 2.0418519973754883
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 2.035771131515503 = 1.9498028755187988 + 0.01 * 8.596818923950195
Epoch 0, val loss: 1.9498295783996582
Epoch 10, training loss: 2.0251288414001465 = 1.9391614198684692 + 0.01 * 8.596735000610352
Epoch 10, val loss: 1.9398016929626465
Epoch 20, training loss: 2.011713981628418 = 1.9257491827011108 + 0.01 * 8.596476554870605
Epoch 20, val loss: 1.927092432975769
Epoch 30, training loss: 1.9926772117614746 = 1.9067211151123047 + 0.01 * 8.595605850219727
Epoch 30, val loss: 1.909169316291809
Epoch 40, training loss: 1.9650378227233887 = 1.8791345357894897 + 0.01 * 8.590332984924316
Epoch 40, val loss: 1.884013056755066
Epoch 50, training loss: 1.92831552028656 = 1.8427414894104004 + 0.01 * 8.557403564453125
Epoch 50, val loss: 1.8527532815933228
Epoch 60, training loss: 1.8888840675354004 = 1.805029034614563 + 0.01 * 8.385499954223633
Epoch 60, val loss: 1.8217828273773193
Epoch 70, training loss: 1.8545984029769897 = 1.7726434469223022 + 0.01 * 8.195500373840332
Epoch 70, val loss: 1.7909671068191528
Epoch 80, training loss: 1.8132997751235962 = 1.7329992055892944 + 0.01 * 8.030052185058594
Epoch 80, val loss: 1.7512332201004028
Epoch 90, training loss: 1.7559590339660645 = 1.6771388053894043 + 0.01 * 7.882029056549072
Epoch 90, val loss: 1.7035363912582397
Epoch 100, training loss: 1.6797668933868408 = 1.6020938158035278 + 0.01 * 7.767312526702881
Epoch 100, val loss: 1.6434826850891113
Epoch 110, training loss: 1.5896899700164795 = 1.5134189128875732 + 0.01 * 7.627103805541992
Epoch 110, val loss: 1.5728788375854492
Epoch 120, training loss: 1.497246265411377 = 1.4219383001327515 + 0.01 * 7.530798435211182
Epoch 120, val loss: 1.5011839866638184
Epoch 130, training loss: 1.4106266498565674 = 1.3363687992095947 + 0.01 * 7.425782680511475
Epoch 130, val loss: 1.4365723133087158
Epoch 140, training loss: 1.3335782289505005 = 1.2604539394378662 + 0.01 * 7.3124260902404785
Epoch 140, val loss: 1.3814700841903687
Epoch 150, training loss: 1.2652069330215454 = 1.1928447484970093 + 0.01 * 7.236219882965088
Epoch 150, val loss: 1.3337855339050293
Epoch 160, training loss: 1.200596570968628 = 1.1286271810531616 + 0.01 * 7.196935176849365
Epoch 160, val loss: 1.2883834838867188
Epoch 170, training loss: 1.1345499753952026 = 1.0628933906555176 + 0.01 * 7.165658950805664
Epoch 170, val loss: 1.2410452365875244
Epoch 180, training loss: 1.0643872022628784 = 0.992932915687561 + 0.01 * 7.14542818069458
Epoch 180, val loss: 1.1901147365570068
Epoch 190, training loss: 0.9899410009384155 = 0.918636679649353 + 0.01 * 7.130430221557617
Epoch 190, val loss: 1.1363948583602905
Epoch 200, training loss: 0.9132237434387207 = 0.8420096635818481 + 0.01 * 7.121405124664307
Epoch 200, val loss: 1.0825366973876953
Epoch 210, training loss: 0.8376362323760986 = 0.7665036916732788 + 0.01 * 7.113255023956299
Epoch 210, val loss: 1.032355785369873
Epoch 220, training loss: 0.7667024731636047 = 0.6956286430358887 + 0.01 * 7.107384204864502
Epoch 220, val loss: 0.989642322063446
Epoch 230, training loss: 0.7027997374534607 = 0.6318370699882507 + 0.01 * 7.096266269683838
Epoch 230, val loss: 0.9570683240890503
Epoch 240, training loss: 0.6467365622520447 = 0.5758792161941528 + 0.01 * 7.085733890533447
Epoch 240, val loss: 0.9352803826332092
Epoch 250, training loss: 0.5978439450263977 = 0.5270836353302002 + 0.01 * 7.0760297775268555
Epoch 250, val loss: 0.9231799840927124
Epoch 260, training loss: 0.5546131134033203 = 0.483982115983963 + 0.01 * 7.063101291656494
Epoch 260, val loss: 0.9182038903236389
Epoch 270, training loss: 0.5153258442878723 = 0.4448270797729492 + 0.01 * 7.0498785972595215
Epoch 270, val loss: 0.9178610444068909
Epoch 280, training loss: 0.4785175919532776 = 0.4081534445285797 + 0.01 * 7.03641414642334
Epoch 280, val loss: 0.9205422401428223
Epoch 290, training loss: 0.4433262348175049 = 0.3730238676071167 + 0.01 * 7.030236721038818
Epoch 290, val loss: 0.9253977537155151
Epoch 300, training loss: 0.409290075302124 = 0.3391430377960205 + 0.01 * 7.014704704284668
Epoch 300, val loss: 0.9319168925285339
Epoch 310, training loss: 0.37667229771614075 = 0.3066345155239105 + 0.01 * 7.003777503967285
Epoch 310, val loss: 0.9401037693023682
Epoch 320, training loss: 0.3459635078907013 = 0.2758450508117676 + 0.01 * 7.011845588684082
Epoch 320, val loss: 0.9500271081924438
Epoch 330, training loss: 0.31693485379219055 = 0.2470093071460724 + 0.01 * 6.992555141448975
Epoch 330, val loss: 0.9616231322288513
Epoch 340, training loss: 0.29001230001449585 = 0.2201395183801651 + 0.01 * 6.987279891967773
Epoch 340, val loss: 0.974690854549408
Epoch 350, training loss: 0.26505163311958313 = 0.19522921741008759 + 0.01 * 6.982242107391357
Epoch 350, val loss: 0.9894216060638428
Epoch 360, training loss: 0.24207907915115356 = 0.17228281497955322 + 0.01 * 6.97962760925293
Epoch 360, val loss: 1.005659818649292
Epoch 370, training loss: 0.2211536467075348 = 0.15133675932884216 + 0.01 * 6.981688976287842
Epoch 370, val loss: 1.023728370666504
Epoch 380, training loss: 0.20215824246406555 = 0.13241571187973022 + 0.01 * 6.974253177642822
Epoch 380, val loss: 1.043631911277771
Epoch 390, training loss: 0.1852254867553711 = 0.11551156640052795 + 0.01 * 6.97139310836792
Epoch 390, val loss: 1.0653352737426758
Epoch 400, training loss: 0.1703110635280609 = 0.10063251852989197 + 0.01 * 6.967855453491211
Epoch 400, val loss: 1.088791847229004
Epoch 410, training loss: 0.15750762820243835 = 0.08770518004894257 + 0.01 * 6.980244159698486
Epoch 410, val loss: 1.113829493522644
Epoch 420, training loss: 0.14630156755447388 = 0.07661265134811401 + 0.01 * 6.968892574310303
Epoch 420, val loss: 1.1398290395736694
Epoch 430, training loss: 0.13679711520671844 = 0.06716528534889221 + 0.01 * 6.963183403015137
Epoch 430, val loss: 1.166154146194458
Epoch 440, training loss: 0.1287156194448471 = 0.05913860350847244 + 0.01 * 6.957701683044434
Epoch 440, val loss: 1.1924405097961426
Epoch 450, training loss: 0.12186197936534882 = 0.05232083797454834 + 0.01 * 6.9541144371032715
Epoch 450, val loss: 1.2182074785232544
Epoch 460, training loss: 0.11602777242660522 = 0.04651825502514839 + 0.01 * 6.950952529907227
Epoch 460, val loss: 1.2431817054748535
Epoch 470, training loss: 0.11106429994106293 = 0.041563790291547775 + 0.01 * 6.9500508308410645
Epoch 470, val loss: 1.267258644104004
Epoch 480, training loss: 0.10677622258663177 = 0.03731932118535042 + 0.01 * 6.945690155029297
Epoch 480, val loss: 1.2904430627822876
Epoch 490, training loss: 0.10310938954353333 = 0.03366217017173767 + 0.01 * 6.9447221755981445
Epoch 490, val loss: 1.3125478029251099
Epoch 500, training loss: 0.0998852327466011 = 0.03048846684396267 + 0.01 * 6.939676761627197
Epoch 500, val loss: 1.3338444232940674
Epoch 510, training loss: 0.0970958024263382 = 0.027715209871530533 + 0.01 * 6.938058853149414
Epoch 510, val loss: 1.3542442321777344
Epoch 520, training loss: 0.09461735188961029 = 0.025284038856625557 + 0.01 * 6.9333319664001465
Epoch 520, val loss: 1.3738254308700562
Epoch 530, training loss: 0.09264039993286133 = 0.02314421348273754 + 0.01 * 6.949618339538574
Epoch 530, val loss: 1.3926279544830322
Epoch 540, training loss: 0.09052678197622299 = 0.02125813253223896 + 0.01 * 6.926865100860596
Epoch 540, val loss: 1.4105302095413208
Epoch 550, training loss: 0.08885271847248077 = 0.019588645547628403 + 0.01 * 6.926407814025879
Epoch 550, val loss: 1.4276652336120605
Epoch 560, training loss: 0.0873262882232666 = 0.018104439601302147 + 0.01 * 6.922184944152832
Epoch 560, val loss: 1.4441190958023071
Epoch 570, training loss: 0.08596831560134888 = 0.016780592501163483 + 0.01 * 6.9187726974487305
Epoch 570, val loss: 1.4599255323410034
Epoch 580, training loss: 0.08475235849618912 = 0.015596194192767143 + 0.01 * 6.915616035461426
Epoch 580, val loss: 1.475071668624878
Epoch 590, training loss: 0.0838012546300888 = 0.01453376654535532 + 0.01 * 6.926749229431152
Epoch 590, val loss: 1.4896444082260132
Epoch 600, training loss: 0.08271486312150955 = 0.013578302226960659 + 0.01 * 6.913656234741211
Epoch 600, val loss: 1.5035604238510132
Epoch 610, training loss: 0.08180336654186249 = 0.012715883553028107 + 0.01 * 6.908748149871826
Epoch 610, val loss: 1.5170267820358276
Epoch 620, training loss: 0.08096681535243988 = 0.011934809386730194 + 0.01 * 6.903200626373291
Epoch 620, val loss: 1.5299640893936157
Epoch 630, training loss: 0.08028067648410797 = 0.01122545637190342 + 0.01 * 6.905522346496582
Epoch 630, val loss: 1.5424619913101196
Epoch 640, training loss: 0.07966150343418121 = 0.010580438189208508 + 0.01 * 6.908107280731201
Epoch 640, val loss: 1.554381251335144
Epoch 650, training loss: 0.07898736000061035 = 0.009992596693336964 + 0.01 * 6.899476528167725
Epoch 650, val loss: 1.5659608840942383
Epoch 660, training loss: 0.07838534563779831 = 0.009454782120883465 + 0.01 * 6.893056869506836
Epoch 660, val loss: 1.5771149396896362
Epoch 670, training loss: 0.07786841690540314 = 0.008961379528045654 + 0.01 * 6.890704154968262
Epoch 670, val loss: 1.5879367589950562
Epoch 680, training loss: 0.07741563767194748 = 0.008507906459271908 + 0.01 * 6.890773296356201
Epoch 680, val loss: 1.598303198814392
Epoch 690, training loss: 0.07699261605739594 = 0.008090482093393803 + 0.01 * 6.890213966369629
Epoch 690, val loss: 1.6083204746246338
Epoch 700, training loss: 0.0765414834022522 = 0.007705072406679392 + 0.01 * 6.883641719818115
Epoch 700, val loss: 1.6180858612060547
Epoch 710, training loss: 0.07615398615598679 = 0.007348612882196903 + 0.01 * 6.880537509918213
Epoch 710, val loss: 1.6274516582489014
Epoch 720, training loss: 0.07580722123384476 = 0.007018352393060923 + 0.01 * 6.878887176513672
Epoch 720, val loss: 1.6365242004394531
Epoch 730, training loss: 0.07544370740652084 = 0.0067119537852704525 + 0.01 * 6.873175621032715
Epoch 730, val loss: 1.6453526020050049
Epoch 740, training loss: 0.07518857717514038 = 0.00642720190808177 + 0.01 * 6.876138210296631
Epoch 740, val loss: 1.653874158859253
Epoch 750, training loss: 0.07485026866197586 = 0.006162120029330254 + 0.01 * 6.868814468383789
Epoch 750, val loss: 1.6621451377868652
Epoch 760, training loss: 0.07459291070699692 = 0.005914747249335051 + 0.01 * 6.86781644821167
Epoch 760, val loss: 1.6701620817184448
Epoch 770, training loss: 0.07435742020606995 = 0.005683669354766607 + 0.01 * 6.867375373840332
Epoch 770, val loss: 1.677900791168213
Epoch 780, training loss: 0.07411455363035202 = 0.005467576440423727 + 0.01 * 6.864697456359863
Epoch 780, val loss: 1.6854877471923828
Epoch 790, training loss: 0.07388640195131302 = 0.005265132989734411 + 0.01 * 6.862127304077148
Epoch 790, val loss: 1.692785382270813
Epoch 800, training loss: 0.07363101094961166 = 0.0050750356167554855 + 0.01 * 6.855597496032715
Epoch 800, val loss: 1.6999281644821167
Epoch 810, training loss: 0.07348968833684921 = 0.004896312020719051 + 0.01 * 6.859337329864502
Epoch 810, val loss: 1.706855058670044
Epoch 820, training loss: 0.07332349568605423 = 0.004728264641016722 + 0.01 * 6.859523296356201
Epoch 820, val loss: 1.71355402469635
Epoch 830, training loss: 0.07309882342815399 = 0.004570191726088524 + 0.01 * 6.852863311767578
Epoch 830, val loss: 1.7201519012451172
Epoch 840, training loss: 0.07286694645881653 = 0.004421164747327566 + 0.01 * 6.844577789306641
Epoch 840, val loss: 1.7264851331710815
Epoch 850, training loss: 0.07284057885408401 = 0.004280385095626116 + 0.01 * 6.856019020080566
Epoch 850, val loss: 1.7326513528823853
Epoch 860, training loss: 0.07261453568935394 = 0.004147343337535858 + 0.01 * 6.846719264984131
Epoch 860, val loss: 1.738756775856018
Epoch 870, training loss: 0.07239732146263123 = 0.004021480679512024 + 0.01 * 6.837584018707275
Epoch 870, val loss: 1.7445671558380127
Epoch 880, training loss: 0.07227706909179688 = 0.003902234835550189 + 0.01 * 6.837483882904053
Epoch 880, val loss: 1.7502989768981934
Epoch 890, training loss: 0.07231605798006058 = 0.003789214650169015 + 0.01 * 6.852684020996094
Epoch 890, val loss: 1.7558543682098389
Epoch 900, training loss: 0.07202211767435074 = 0.0036820045206695795 + 0.01 * 6.834011554718018
Epoch 900, val loss: 1.7613250017166138
Epoch 910, training loss: 0.07190372794866562 = 0.003580320393666625 + 0.01 * 6.832341194152832
Epoch 910, val loss: 1.7665692567825317
Epoch 920, training loss: 0.07173548638820648 = 0.003483597422018647 + 0.01 * 6.825188636779785
Epoch 920, val loss: 1.771746039390564
Epoch 930, training loss: 0.071906179189682 = 0.0033915527164936066 + 0.01 * 6.851463317871094
Epoch 930, val loss: 1.7768086194992065
Epoch 940, training loss: 0.07163458317518234 = 0.0033040132839232683 + 0.01 * 6.833056926727295
Epoch 940, val loss: 1.7815971374511719
Epoch 950, training loss: 0.07147156447172165 = 0.003220613580197096 + 0.01 * 6.8250956535339355
Epoch 950, val loss: 1.7863945960998535
Epoch 960, training loss: 0.07132212072610855 = 0.003140971763059497 + 0.01 * 6.818115234375
Epoch 960, val loss: 1.7910765409469604
Epoch 970, training loss: 0.07137349992990494 = 0.0030649087857455015 + 0.01 * 6.830859661102295
Epoch 970, val loss: 1.7956817150115967
Epoch 980, training loss: 0.07114875316619873 = 0.002992278430610895 + 0.01 * 6.815647602081299
Epoch 980, val loss: 1.8000556230545044
Epoch 990, training loss: 0.07114067673683167 = 0.002922986401244998 + 0.01 * 6.8217692375183105
Epoch 990, val loss: 1.804376244544983
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 2.036362648010254 = 1.9503945112228394 + 0.01 * 8.596821784973145
Epoch 0, val loss: 1.951101541519165
Epoch 10, training loss: 2.0261313915252686 = 1.940163493156433 + 0.01 * 8.596784591674805
Epoch 10, val loss: 1.941618800163269
Epoch 20, training loss: 2.0139999389648438 = 1.9280335903167725 + 0.01 * 8.596641540527344
Epoch 20, val loss: 1.9299770593643188
Epoch 30, training loss: 1.9975128173828125 = 1.9115506410598755 + 0.01 * 8.596221923828125
Epoch 30, val loss: 1.9137672185897827
Epoch 40, training loss: 1.9737294912338257 = 1.8877875804901123 + 0.01 * 8.594185829162598
Epoch 40, val loss: 1.8902289867401123
Epoch 50, training loss: 1.9403691291809082 = 1.8545773029327393 + 0.01 * 8.579182624816895
Epoch 50, val loss: 1.8581076860427856
Epoch 60, training loss: 1.9009921550750732 = 1.8160066604614258 + 0.01 * 8.498553276062012
Epoch 60, val loss: 1.8235442638397217
Epoch 70, training loss: 1.8667428493499756 = 1.7841370105743408 + 0.01 * 8.260580062866211
Epoch 70, val loss: 1.7978345155715942
Epoch 80, training loss: 1.8315057754516602 = 1.7503138780593872 + 0.01 * 8.119194984436035
Epoch 80, val loss: 1.767987608909607
Epoch 90, training loss: 1.7808177471160889 = 1.702117919921875 + 0.01 * 7.8699870109558105
Epoch 90, val loss: 1.726717472076416
Epoch 100, training loss: 1.711375117301941 = 1.634588599205017 + 0.01 * 7.678651809692383
Epoch 100, val loss: 1.6717579364776611
Epoch 110, training loss: 1.6213417053222656 = 1.5463436841964722 + 0.01 * 7.4997992515563965
Epoch 110, val loss: 1.6016839742660522
Epoch 120, training loss: 1.5191954374313354 = 1.445457100868225 + 0.01 * 7.373838424682617
Epoch 120, val loss: 1.5217006206512451
Epoch 130, training loss: 1.4150983095169067 = 1.3422240018844604 + 0.01 * 7.28742790222168
Epoch 130, val loss: 1.4415303468704224
Epoch 140, training loss: 1.3147116899490356 = 1.242247223854065 + 0.01 * 7.246448993682861
Epoch 140, val loss: 1.365270972251892
Epoch 150, training loss: 1.2199230194091797 = 1.1476545333862305 + 0.01 * 7.226849555969238
Epoch 150, val loss: 1.294940710067749
Epoch 160, training loss: 1.1323257684707642 = 1.0602387189865112 + 0.01 * 7.20870304107666
Epoch 160, val loss: 1.2314863204956055
Epoch 170, training loss: 1.0518999099731445 = 0.9800291061401367 + 0.01 * 7.187084197998047
Epoch 170, val loss: 1.1746158599853516
Epoch 180, training loss: 0.9768834114074707 = 0.9052245616912842 + 0.01 * 7.165886402130127
Epoch 180, val loss: 1.1224935054779053
Epoch 190, training loss: 0.9053786993026733 = 0.8338003158569336 + 0.01 * 7.157840728759766
Epoch 190, val loss: 1.0731217861175537
Epoch 200, training loss: 0.8364536762237549 = 0.7649433612823486 + 0.01 * 7.151033878326416
Epoch 200, val loss: 1.0263065099716187
Epoch 210, training loss: 0.7706274390220642 = 0.6991548538208008 + 0.01 * 7.147258758544922
Epoch 210, val loss: 0.9833012223243713
Epoch 220, training loss: 0.709053099155426 = 0.6376029253005981 + 0.01 * 7.145020008087158
Epoch 220, val loss: 0.9467052817344666
Epoch 230, training loss: 0.6525261402130127 = 0.5811010599136353 + 0.01 * 7.142507076263428
Epoch 230, val loss: 0.9179911613464355
Epoch 240, training loss: 0.6012569665908813 = 0.5298646688461304 + 0.01 * 7.139232635498047
Epoch 240, val loss: 0.8970783352851868
Epoch 250, training loss: 0.5549134016036987 = 0.48352766036987305 + 0.01 * 7.1385722160339355
Epoch 250, val loss: 0.8829376697540283
Epoch 260, training loss: 0.5126240253448486 = 0.4412938952445984 + 0.01 * 7.133013725280762
Epoch 260, val loss: 0.8741073608398438
Epoch 270, training loss: 0.47341421246528625 = 0.4021260440349579 + 0.01 * 7.128816604614258
Epoch 270, val loss: 0.8689185380935669
Epoch 280, training loss: 0.43646764755249023 = 0.3652334213256836 + 0.01 * 7.123424530029297
Epoch 280, val loss: 0.8662638068199158
Epoch 290, training loss: 0.4016532301902771 = 0.33047977089881897 + 0.01 * 7.117344379425049
Epoch 290, val loss: 0.8657800555229187
Epoch 300, training loss: 0.36952701210975647 = 0.298339307308197 + 0.01 * 7.118770599365234
Epoch 300, val loss: 0.8669333457946777
Epoch 310, training loss: 0.34053096175193787 = 0.26948875188827515 + 0.01 * 7.104220867156982
Epoch 310, val loss: 0.8698457479476929
Epoch 320, training loss: 0.31516021490097046 = 0.24421006441116333 + 0.01 * 7.095013618469238
Epoch 320, val loss: 0.8747232556343079
Epoch 330, training loss: 0.2930750250816345 = 0.22220395505428314 + 0.01 * 7.087108135223389
Epoch 330, val loss: 0.8813706636428833
Epoch 340, training loss: 0.2735903859138489 = 0.20281760394573212 + 0.01 * 7.077280044555664
Epoch 340, val loss: 0.8893177509307861
Epoch 350, training loss: 0.25586751103401184 = 0.18521593511104584 + 0.01 * 7.065158843994141
Epoch 350, val loss: 0.8980743288993835
Epoch 360, training loss: 0.2391783595085144 = 0.1686377376317978 + 0.01 * 7.054061412811279
Epoch 360, val loss: 0.9070242643356323
Epoch 370, training loss: 0.22316399216651917 = 0.1525105983018875 + 0.01 * 7.065340042114258
Epoch 370, val loss: 0.9156700968742371
Epoch 380, training loss: 0.20694193243980408 = 0.1365671306848526 + 0.01 * 7.03748083114624
Epoch 380, val loss: 0.9236570596694946
Epoch 390, training loss: 0.1911674588918686 = 0.12089582532644272 + 0.01 * 7.027163505554199
Epoch 390, val loss: 0.9310566782951355
Epoch 400, training loss: 0.17612668871879578 = 0.10595045238733292 + 0.01 * 7.017624378204346
Epoch 400, val loss: 0.9382822513580322
Epoch 410, training loss: 0.16259115934371948 = 0.09228955209255219 + 0.01 * 7.030160903930664
Epoch 410, val loss: 0.9458926320075989
Epoch 420, training loss: 0.15028882026672363 = 0.08028554916381836 + 0.01 * 7.000327110290527
Epoch 420, val loss: 0.9540653824806213
Epoch 430, training loss: 0.13991224765777588 = 0.06996709853410721 + 0.01 * 6.994515419006348
Epoch 430, val loss: 0.9630544185638428
Epoch 440, training loss: 0.13107803463935852 = 0.0611649826169014 + 0.01 * 6.991305351257324
Epoch 440, val loss: 0.9728599190711975
Epoch 450, training loss: 0.12342901527881622 = 0.05365710332989693 + 0.01 * 6.977191925048828
Epoch 450, val loss: 0.9834421873092651
Epoch 460, training loss: 0.11743217706680298 = 0.04723258689045906 + 0.01 * 7.019959449768066
Epoch 460, val loss: 0.9945788383483887
Epoch 470, training loss: 0.11153566837310791 = 0.041729819029569626 + 0.01 * 6.98058557510376
Epoch 470, val loss: 1.006118893623352
Epoch 480, training loss: 0.10665263235569 = 0.03700440004467964 + 0.01 * 6.964823246002197
Epoch 480, val loss: 1.017992377281189
Epoch 490, training loss: 0.10253403335809708 = 0.03292901813983917 + 0.01 * 6.960501670837402
Epoch 490, val loss: 1.0300942659378052
Epoch 500, training loss: 0.09891501069068909 = 0.029413912445306778 + 0.01 * 6.950109958648682
Epoch 500, val loss: 1.0423225164413452
Epoch 510, training loss: 0.09586410224437714 = 0.02637849561870098 + 0.01 * 6.94856071472168
Epoch 510, val loss: 1.0546308755874634
Epoch 520, training loss: 0.0931534618139267 = 0.02375226840376854 + 0.01 * 6.940118789672852
Epoch 520, val loss: 1.067065954208374
Epoch 530, training loss: 0.09094440191984177 = 0.02147850953042507 + 0.01 * 6.94658899307251
Epoch 530, val loss: 1.079461932182312
Epoch 540, training loss: 0.08883552998304367 = 0.019512878730893135 + 0.01 * 6.932265758514404
Epoch 540, val loss: 1.091750144958496
Epoch 550, training loss: 0.08707637339830399 = 0.017804427072405815 + 0.01 * 6.927194595336914
Epoch 550, val loss: 1.1039265394210815
Epoch 560, training loss: 0.08563510328531265 = 0.01631411723792553 + 0.01 * 6.932098388671875
Epoch 560, val loss: 1.1158721446990967
Epoch 570, training loss: 0.08421336859464645 = 0.015010466799139977 + 0.01 * 6.920290470123291
Epoch 570, val loss: 1.1274430751800537
Epoch 580, training loss: 0.08300412446260452 = 0.013865293934941292 + 0.01 * 6.913883686065674
Epoch 580, val loss: 1.1387218236923218
Epoch 590, training loss: 0.08204131573438644 = 0.012854417786002159 + 0.01 * 6.918689727783203
Epoch 590, val loss: 1.1496357917785645
Epoch 600, training loss: 0.08102156221866608 = 0.011958382092416286 + 0.01 * 6.906317710876465
Epoch 600, val loss: 1.1601582765579224
Epoch 610, training loss: 0.08050063997507095 = 0.011160647496581078 + 0.01 * 6.933999538421631
Epoch 610, val loss: 1.1703143119812012
Epoch 620, training loss: 0.07948797196149826 = 0.010449530556797981 + 0.01 * 6.903844356536865
Epoch 620, val loss: 1.1800923347473145
Epoch 630, training loss: 0.07878506928682327 = 0.009811476804316044 + 0.01 * 6.897359848022461
Epoch 630, val loss: 1.189507246017456
Epoch 640, training loss: 0.0782342255115509 = 0.009236251935362816 + 0.01 * 6.899796962738037
Epoch 640, val loss: 1.198606014251709
Epoch 650, training loss: 0.07760678976774216 = 0.008715814910829067 + 0.01 * 6.889097213745117
Epoch 650, val loss: 1.2073884010314941
Epoch 660, training loss: 0.07721877843141556 = 0.00824320875108242 + 0.01 * 6.897556781768799
Epoch 660, val loss: 1.2158888578414917
Epoch 670, training loss: 0.07664965093135834 = 0.007813036441802979 + 0.01 * 6.88366174697876
Epoch 670, val loss: 1.2241028547286987
Epoch 680, training loss: 0.07627596706151962 = 0.007419738452881575 + 0.01 * 6.885622978210449
Epoch 680, val loss: 1.2320153713226318
Epoch 690, training loss: 0.07586269080638885 = 0.007059294730424881 + 0.01 * 6.8803391456604
Epoch 690, val loss: 1.2397023439407349
Epoch 700, training loss: 0.07544875890016556 = 0.006728101056069136 + 0.01 * 6.872066497802734
Epoch 700, val loss: 1.2471197843551636
Epoch 710, training loss: 0.07514502108097076 = 0.006422702688723803 + 0.01 * 6.872231960296631
Epoch 710, val loss: 1.2543634176254272
Epoch 720, training loss: 0.07493637502193451 = 0.00614065071567893 + 0.01 * 6.87957239151001
Epoch 720, val loss: 1.2613301277160645
Epoch 730, training loss: 0.07460267841815948 = 0.005879838485270739 + 0.01 * 6.872283458709717
Epoch 730, val loss: 1.268030047416687
Epoch 740, training loss: 0.07425692677497864 = 0.005638066679239273 + 0.01 * 6.861886024475098
Epoch 740, val loss: 1.274603247642517
Epoch 750, training loss: 0.07394576817750931 = 0.005412844009697437 + 0.01 * 6.853292465209961
Epoch 750, val loss: 1.2809677124023438
Epoch 760, training loss: 0.07375477999448776 = 0.005202863831073046 + 0.01 * 6.855191707611084
Epoch 760, val loss: 1.2871910333633423
Epoch 770, training loss: 0.0734453871846199 = 0.00500729912891984 + 0.01 * 6.843809127807617
Epoch 770, val loss: 1.2931427955627441
Epoch 780, training loss: 0.07331446558237076 = 0.0048244427889585495 + 0.01 * 6.849001884460449
Epoch 780, val loss: 1.2990063428878784
Epoch 790, training loss: 0.07297458499670029 = 0.004653004929423332 + 0.01 * 6.832158088684082
Epoch 790, val loss: 1.304705262184143
Epoch 800, training loss: 0.07311394810676575 = 0.004492159467190504 + 0.01 * 6.862179279327393
Epoch 800, val loss: 1.3102799654006958
Epoch 810, training loss: 0.07270897179841995 = 0.004341240040957928 + 0.01 * 6.83677339553833
Epoch 810, val loss: 1.3155990839004517
Epoch 820, training loss: 0.07248129695653915 = 0.004199309274554253 + 0.01 * 6.828198432922363
Epoch 820, val loss: 1.3208656311035156
Epoch 830, training loss: 0.07226265966892242 = 0.0040654675103724 + 0.01 * 6.819719314575195
Epoch 830, val loss: 1.3259533643722534
Epoch 840, training loss: 0.07228119671344757 = 0.00393919600173831 + 0.01 * 6.834200382232666
Epoch 840, val loss: 1.330919623374939
Epoch 850, training loss: 0.07198876142501831 = 0.003820318728685379 + 0.01 * 6.8168439865112305
Epoch 850, val loss: 1.3357359170913696
Epoch 860, training loss: 0.07185756415128708 = 0.0037077844608575106 + 0.01 * 6.814978122711182
Epoch 860, val loss: 1.3404098749160767
Epoch 870, training loss: 0.07190178334712982 = 0.003601078875362873 + 0.01 * 6.8300700187683105
Epoch 870, val loss: 1.344985008239746
Epoch 880, training loss: 0.07167886942625046 = 0.003500172635540366 + 0.01 * 6.817870140075684
Epoch 880, val loss: 1.3494884967803955
Epoch 890, training loss: 0.07154763489961624 = 0.003404271323233843 + 0.01 * 6.81433629989624
Epoch 890, val loss: 1.353857398033142
Epoch 900, training loss: 0.07127494364976883 = 0.0033132664393633604 + 0.01 * 6.796167850494385
Epoch 900, val loss: 1.3580756187438965
Epoch 910, training loss: 0.07124244421720505 = 0.003226665547117591 + 0.01 * 6.801578044891357
Epoch 910, val loss: 1.3622162342071533
Epoch 920, training loss: 0.07118454575538635 = 0.0031442721374332905 + 0.01 * 6.804027557373047
Epoch 920, val loss: 1.3662810325622559
Epoch 930, training loss: 0.07120979577302933 = 0.0030660051852464676 + 0.01 * 6.8143792152404785
Epoch 930, val loss: 1.3702244758605957
Epoch 940, training loss: 0.07100795954465866 = 0.0029913915786892176 + 0.01 * 6.801656723022461
Epoch 940, val loss: 1.3740060329437256
Epoch 950, training loss: 0.07075709104537964 = 0.0029203298036009073 + 0.01 * 6.7836761474609375
Epoch 950, val loss: 1.3777832984924316
Epoch 960, training loss: 0.07064345479011536 = 0.002852264791727066 + 0.01 * 6.779119491577148
Epoch 960, val loss: 1.3814069032669067
Epoch 970, training loss: 0.07068593055009842 = 0.0027871292550116777 + 0.01 * 6.789880275726318
Epoch 970, val loss: 1.384992003440857
Epoch 980, training loss: 0.0705740824341774 = 0.002724854974076152 + 0.01 * 6.7849225997924805
Epoch 980, val loss: 1.388493299484253
Epoch 990, training loss: 0.07059681415557861 = 0.0026654512621462345 + 0.01 * 6.793136119842529
Epoch 990, val loss: 1.3919483423233032
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8144438587243016
The final CL Acc:0.73086, 0.01492, The final GNN Acc:0.81198, 0.00179
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13252])
remove edge: torch.Size([2, 7906])
updated graph: torch.Size([2, 10602])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.041156768798828 = 1.955188512802124 + 0.01 * 8.596814155578613
Epoch 0, val loss: 1.9487930536270142
Epoch 10, training loss: 2.031144618988037 = 1.9451770782470703 + 0.01 * 8.596750259399414
Epoch 10, val loss: 1.9395724534988403
Epoch 20, training loss: 2.0190534591674805 = 1.9330883026123047 + 0.01 * 8.596508026123047
Epoch 20, val loss: 1.9280236959457397
Epoch 30, training loss: 2.0025906562805176 = 1.9166340827941895 + 0.01 * 8.595659255981445
Epoch 30, val loss: 1.9120314121246338
Epoch 40, training loss: 1.9785981178283691 = 1.892690896987915 + 0.01 * 8.590723037719727
Epoch 40, val loss: 1.8887593746185303
Epoch 50, training loss: 1.943914532661438 = 1.8583356142044067 + 0.01 * 8.557889938354492
Epoch 50, val loss: 1.856111764907837
Epoch 60, training loss: 1.8992834091186523 = 1.8151237964630127 + 0.01 * 8.415960311889648
Epoch 60, val loss: 1.8175700902938843
Epoch 70, training loss: 1.8553674221038818 = 1.7735196352005005 + 0.01 * 8.184773445129395
Epoch 70, val loss: 1.7832074165344238
Epoch 80, training loss: 1.8124351501464844 = 1.7329394817352295 + 0.01 * 7.949570655822754
Epoch 80, val loss: 1.7471609115600586
Epoch 90, training loss: 1.7537921667099 = 1.6776540279388428 + 0.01 * 7.613811016082764
Epoch 90, val loss: 1.6963152885437012
Epoch 100, training loss: 1.6776361465454102 = 1.6034592390060425 + 0.01 * 7.417694568634033
Epoch 100, val loss: 1.6299333572387695
Epoch 110, training loss: 1.5816564559936523 = 1.508234977722168 + 0.01 * 7.34214448928833
Epoch 110, val loss: 1.5469413995742798
Epoch 120, training loss: 1.4727914333343506 = 1.3997604846954346 + 0.01 * 7.303093433380127
Epoch 120, val loss: 1.4547525644302368
Epoch 130, training loss: 1.3611671924591064 = 1.2883684635162354 + 0.01 * 7.2798686027526855
Epoch 130, val loss: 1.359702706336975
Epoch 140, training loss: 1.255204677581787 = 1.182658314704895 + 0.01 * 7.254639625549316
Epoch 140, val loss: 1.2705512046813965
Epoch 150, training loss: 1.1594113111495972 = 1.0870940685272217 + 0.01 * 7.231728553771973
Epoch 150, val loss: 1.1912935972213745
Epoch 160, training loss: 1.074196457862854 = 1.0020488500595093 + 0.01 * 7.214762210845947
Epoch 160, val loss: 1.122499704360962
Epoch 170, training loss: 0.9970630407333374 = 0.9250698685646057 + 0.01 * 7.199316501617432
Epoch 170, val loss: 1.061341643333435
Epoch 180, training loss: 0.924412190914154 = 0.8526408672332764 + 0.01 * 7.177131175994873
Epoch 180, val loss: 1.0041933059692383
Epoch 190, training loss: 0.8539009094238281 = 0.7824244499206543 + 0.01 * 7.147643089294434
Epoch 190, val loss: 0.9489818215370178
Epoch 200, training loss: 0.785251259803772 = 0.7140447497367859 + 0.01 * 7.12064790725708
Epoch 200, val loss: 0.8954974412918091
Epoch 210, training loss: 0.7194029688835144 = 0.648429274559021 + 0.01 * 7.097367763519287
Epoch 210, val loss: 0.8453713059425354
Epoch 220, training loss: 0.6570594906806946 = 0.5862592458724976 + 0.01 * 7.080023288726807
Epoch 220, val loss: 0.7998217940330505
Epoch 230, training loss: 0.5983068943023682 = 0.527587890625 + 0.01 * 7.071901321411133
Epoch 230, val loss: 0.759674608707428
Epoch 240, training loss: 0.5428575277328491 = 0.47223415970802307 + 0.01 * 7.062339782714844
Epoch 240, val loss: 0.7249154448509216
Epoch 250, training loss: 0.4910954535007477 = 0.4205071032047272 + 0.01 * 7.058834552764893
Epoch 250, val loss: 0.6952380537986755
Epoch 260, training loss: 0.4435268044471741 = 0.37298083305358887 + 0.01 * 7.054596424102783
Epoch 260, val loss: 0.6704554557800293
Epoch 270, training loss: 0.4005444645881653 = 0.33003848791122437 + 0.01 * 7.050597190856934
Epoch 270, val loss: 0.6505875587463379
Epoch 280, training loss: 0.3620389997959137 = 0.29155728220939636 + 0.01 * 7.048172950744629
Epoch 280, val loss: 0.6353645920753479
Epoch 290, training loss: 0.3274158239364624 = 0.2569565176963806 + 0.01 * 7.045929908752441
Epoch 290, val loss: 0.6242735981941223
Epoch 300, training loss: 0.2960917353630066 = 0.22567252814769745 + 0.01 * 7.041921138763428
Epoch 300, val loss: 0.6165708303451538
Epoch 310, training loss: 0.2678639888763428 = 0.1974630206823349 + 0.01 * 7.040096759796143
Epoch 310, val loss: 0.6118332147598267
Epoch 320, training loss: 0.24271373450756073 = 0.1723470836877823 + 0.01 * 7.036665439605713
Epoch 320, val loss: 0.6097937226295471
Epoch 330, training loss: 0.22067198157310486 = 0.15034835040569305 + 0.01 * 7.032363414764404
Epoch 330, val loss: 0.6101773977279663
Epoch 340, training loss: 0.20171037316322327 = 0.13132518529891968 + 0.01 * 7.038519859313965
Epoch 340, val loss: 0.6126776933670044
Epoch 350, training loss: 0.18528138101100922 = 0.11499136686325073 + 0.01 * 7.029001235961914
Epoch 350, val loss: 0.6169041991233826
Epoch 360, training loss: 0.17121335864067078 = 0.10098084807395935 + 0.01 * 7.023251056671143
Epoch 360, val loss: 0.6225093007087708
Epoch 370, training loss: 0.159143328666687 = 0.08894982933998108 + 0.01 * 7.019351005554199
Epoch 370, val loss: 0.6292577385902405
Epoch 380, training loss: 0.14881467819213867 = 0.07860492914915085 + 0.01 * 7.020975112915039
Epoch 380, val loss: 0.6369012594223022
Epoch 390, training loss: 0.13984815776348114 = 0.06970426440238953 + 0.01 * 7.014389514923096
Epoch 390, val loss: 0.6452515125274658
Epoch 400, training loss: 0.1321142166852951 = 0.062038224190473557 + 0.01 * 7.007598876953125
Epoch 400, val loss: 0.6542059779167175
Epoch 410, training loss: 0.1255296766757965 = 0.05542461574077606 + 0.01 * 7.010505676269531
Epoch 410, val loss: 0.6635371446609497
Epoch 420, training loss: 0.11971646547317505 = 0.0497046522796154 + 0.01 * 7.0011820793151855
Epoch 420, val loss: 0.673162579536438
Epoch 430, training loss: 0.1146557480096817 = 0.044736459851264954 + 0.01 * 6.991929054260254
Epoch 430, val loss: 0.6829973459243774
Epoch 440, training loss: 0.1107417643070221 = 0.04040464386343956 + 0.01 * 7.033712863922119
Epoch 440, val loss: 0.6929980516433716
Epoch 450, training loss: 0.10647410154342651 = 0.03662162646651268 + 0.01 * 6.985247611999512
Epoch 450, val loss: 0.7029684782028198
Epoch 460, training loss: 0.10308866202831268 = 0.03329868242144585 + 0.01 * 6.978998184204102
Epoch 460, val loss: 0.7130557298660278
Epoch 470, training loss: 0.10016633570194244 = 0.030365310609340668 + 0.01 * 6.980103015899658
Epoch 470, val loss: 0.72310870885849
Epoch 480, training loss: 0.09740375727415085 = 0.027767732739448547 + 0.01 * 6.963602542877197
Epoch 480, val loss: 0.7331128716468811
Epoch 490, training loss: 0.09507311880588531 = 0.025461530312895775 + 0.01 * 6.961158752441406
Epoch 490, val loss: 0.743037760257721
Epoch 500, training loss: 0.0929078534245491 = 0.023411022499203682 + 0.01 * 6.94968318939209
Epoch 500, val loss: 0.7527874112129211
Epoch 510, training loss: 0.09093902260065079 = 0.0215813796967268 + 0.01 * 6.935764312744141
Epoch 510, val loss: 0.7624073624610901
Epoch 520, training loss: 0.08937602490186691 = 0.01994972862303257 + 0.01 * 6.942630290985107
Epoch 520, val loss: 0.7718367576599121
Epoch 530, training loss: 0.08775704354047775 = 0.01849270798265934 + 0.01 * 6.92643404006958
Epoch 530, val loss: 0.7809340953826904
Epoch 540, training loss: 0.08635485172271729 = 0.017182696610689163 + 0.01 * 6.917215824127197
Epoch 540, val loss: 0.7898765206336975
Epoch 550, training loss: 0.08509372919797897 = 0.01600237749516964 + 0.01 * 6.909135341644287
Epoch 550, val loss: 0.7986145615577698
Epoch 560, training loss: 0.08411212265491486 = 0.014938585460186005 + 0.01 * 6.917354106903076
Epoch 560, val loss: 0.807064414024353
Epoch 570, training loss: 0.08293305337429047 = 0.01397590059787035 + 0.01 * 6.895715236663818
Epoch 570, val loss: 0.8153446316719055
Epoch 580, training loss: 0.0824531838297844 = 0.013102175667881966 + 0.01 * 6.93510103225708
Epoch 580, val loss: 0.8233486413955688
Epoch 590, training loss: 0.08123570680618286 = 0.012310286983847618 + 0.01 * 6.892541885375977
Epoch 590, val loss: 0.8311007618904114
Epoch 600, training loss: 0.08035970479249954 = 0.011588990688323975 + 0.01 * 6.877071380615234
Epoch 600, val loss: 0.8386767506599426
Epoch 610, training loss: 0.07965867221355438 = 0.010930037125945091 + 0.01 * 6.87286376953125
Epoch 610, val loss: 0.846057653427124
Epoch 620, training loss: 0.07902763038873672 = 0.01032741367816925 + 0.01 * 6.870021820068359
Epoch 620, val loss: 0.8531798124313354
Epoch 630, training loss: 0.07840164750814438 = 0.009775226004421711 + 0.01 * 6.862642288208008
Epoch 630, val loss: 0.8601973652839661
Epoch 640, training loss: 0.0779893696308136 = 0.009269099682569504 + 0.01 * 6.872027397155762
Epoch 640, val loss: 0.8668811917304993
Epoch 650, training loss: 0.07742688059806824 = 0.008802792988717556 + 0.01 * 6.862408638000488
Epoch 650, val loss: 0.8734690546989441
Epoch 660, training loss: 0.07688217610120773 = 0.00837207306176424 + 0.01 * 6.851009845733643
Epoch 660, val loss: 0.8798604607582092
Epoch 670, training loss: 0.07638806104660034 = 0.00797443836927414 + 0.01 * 6.841362476348877
Epoch 670, val loss: 0.8861055374145508
Epoch 680, training loss: 0.07594974339008331 = 0.0076063512824475765 + 0.01 * 6.834339618682861
Epoch 680, val loss: 0.8921532034873962
Epoch 690, training loss: 0.07576879858970642 = 0.007264520041644573 + 0.01 * 6.850428581237793
Epoch 690, val loss: 0.8980442881584167
Epoch 700, training loss: 0.07535377144813538 = 0.006946985609829426 + 0.01 * 6.8406782150268555
Epoch 700, val loss: 0.9038241505622864
Epoch 710, training loss: 0.07491298019886017 = 0.0066513968631625175 + 0.01 * 6.82615852355957
Epoch 710, val loss: 0.9093692302703857
Epoch 720, training loss: 0.07453139126300812 = 0.006375741679221392 + 0.01 * 6.81556510925293
Epoch 720, val loss: 0.9148397445678711
Epoch 730, training loss: 0.0742281898856163 = 0.006118656136095524 + 0.01 * 6.810953617095947
Epoch 730, val loss: 0.9201472997665405
Epoch 740, training loss: 0.07388339191675186 = 0.005878348834812641 + 0.01 * 6.800504207611084
Epoch 740, val loss: 0.925293505191803
Epoch 750, training loss: 0.07371523976325989 = 0.005653181578963995 + 0.01 * 6.806206226348877
Epoch 750, val loss: 0.9303658604621887
Epoch 760, training loss: 0.07347171008586884 = 0.005442331079393625 + 0.01 * 6.802938461303711
Epoch 760, val loss: 0.9352400898933411
Epoch 770, training loss: 0.07317148149013519 = 0.005244245287030935 + 0.01 * 6.792723178863525
Epoch 770, val loss: 0.9400332570075989
Epoch 780, training loss: 0.0729842409491539 = 0.0050581395626068115 + 0.01 * 6.7926106452941895
Epoch 780, val loss: 0.9446519017219543
Epoch 790, training loss: 0.07297161221504211 = 0.0048829130828380585 + 0.01 * 6.808870792388916
Epoch 790, val loss: 0.9492452144622803
Epoch 800, training loss: 0.072508305311203 = 0.004717745818197727 + 0.01 * 6.779056072235107
Epoch 800, val loss: 0.9536397457122803
Epoch 810, training loss: 0.07223355025053024 = 0.00456192996352911 + 0.01 * 6.767162322998047
Epoch 810, val loss: 0.9580002427101135
Epoch 820, training loss: 0.07227658480405807 = 0.00441480940207839 + 0.01 * 6.786177158355713
Epoch 820, val loss: 0.9622133374214172
Epoch 830, training loss: 0.07206667959690094 = 0.004275854676961899 + 0.01 * 6.779083251953125
Epoch 830, val loss: 0.9662823677062988
Epoch 840, training loss: 0.07192245870828629 = 0.0041441298089921474 + 0.01 * 6.777832508087158
Epoch 840, val loss: 0.9703072905540466
Epoch 850, training loss: 0.07177066802978516 = 0.004019349347800016 + 0.01 * 6.775132179260254
Epoch 850, val loss: 0.9742190837860107
Epoch 860, training loss: 0.07166513800621033 = 0.0039009449537843466 + 0.01 * 6.776419639587402
Epoch 860, val loss: 0.9780949354171753
Epoch 870, training loss: 0.07147300988435745 = 0.0037886325735598803 + 0.01 * 6.76843786239624
Epoch 870, val loss: 0.9818154573440552
Epoch 880, training loss: 0.07125650346279144 = 0.0036818149965256453 + 0.01 * 6.757469177246094
Epoch 880, val loss: 0.9854041337966919
Epoch 890, training loss: 0.07110914587974548 = 0.003580324584618211 + 0.01 * 6.75288200378418
Epoch 890, val loss: 0.9890084862709045
Epoch 900, training loss: 0.07142488658428192 = 0.0034835070837289095 + 0.01 * 6.794138431549072
Epoch 900, val loss: 0.9924896359443665
Epoch 910, training loss: 0.07089182734489441 = 0.003391479840502143 + 0.01 * 6.750034332275391
Epoch 910, val loss: 0.9959123134613037
Epoch 920, training loss: 0.07067161798477173 = 0.0033037785906344652 + 0.01 * 6.736783981323242
Epoch 920, val loss: 0.9992361664772034
Epoch 930, training loss: 0.07060471922159195 = 0.0032201020512729883 + 0.01 * 6.738461971282959
Epoch 930, val loss: 1.0024752616882324
Epoch 940, training loss: 0.07040076702833176 = 0.0031402658205479383 + 0.01 * 6.72605037689209
Epoch 940, val loss: 1.0056536197662354
Epoch 950, training loss: 0.07034044712781906 = 0.0030638212338089943 + 0.01 * 6.727663040161133
Epoch 950, val loss: 1.008810043334961
Epoch 960, training loss: 0.07061668485403061 = 0.002990859327837825 + 0.01 * 6.762582778930664
Epoch 960, val loss: 1.011842131614685
Epoch 970, training loss: 0.0701759085059166 = 0.0029210189823061228 + 0.01 * 6.725488662719727
Epoch 970, val loss: 1.014838457107544
Epoch 980, training loss: 0.07022202014923096 = 0.0028542340733110905 + 0.01 * 6.736778736114502
Epoch 980, val loss: 1.0176937580108643
Epoch 990, training loss: 0.06997563689947128 = 0.002790240803733468 + 0.01 * 6.718539237976074
Epoch 990, val loss: 1.0206133127212524
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 2.0375218391418457 = 1.9515537023544312 + 0.01 * 8.596818923950195
Epoch 0, val loss: 1.9438635110855103
Epoch 10, training loss: 2.0270605087280273 = 1.94109308719635 + 0.01 * 8.596735954284668
Epoch 10, val loss: 1.9338887929916382
Epoch 20, training loss: 2.013983726501465 = 1.9280189275741577 + 0.01 * 8.596479415893555
Epoch 20, val loss: 1.9210187196731567
Epoch 30, training loss: 1.9957735538482666 = 1.9098173379898071 + 0.01 * 8.595620155334473
Epoch 30, val loss: 1.9027760028839111
Epoch 40, training loss: 1.969216227531433 = 1.8833121061325073 + 0.01 * 8.590408325195312
Epoch 40, val loss: 1.8762718439102173
Epoch 50, training loss: 1.9324700832366943 = 1.846926212310791 + 0.01 * 8.554388999938965
Epoch 50, val loss: 1.841625452041626
Epoch 60, training loss: 1.8899993896484375 = 1.8063464164733887 + 0.01 * 8.365297317504883
Epoch 60, val loss: 1.8072811365127563
Epoch 70, training loss: 1.8502684831619263 = 1.7687009572982788 + 0.01 * 8.156753540039062
Epoch 70, val loss: 1.7786865234375
Epoch 80, training loss: 1.8026607036590576 = 1.7235491275787354 + 0.01 * 7.911161422729492
Epoch 80, val loss: 1.7410227060317993
Epoch 90, training loss: 1.7374794483184814 = 1.6610432863235474 + 0.01 * 7.643620014190674
Epoch 90, val loss: 1.6859432458877563
Epoch 100, training loss: 1.653630256652832 = 1.5787742137908936 + 0.01 * 7.4856038093566895
Epoch 100, val loss: 1.615004539489746
Epoch 110, training loss: 1.5595937967300415 = 1.4857012033462524 + 0.01 * 7.3892645835876465
Epoch 110, val loss: 1.5388330221176147
Epoch 120, training loss: 1.4658008813858032 = 1.3927860260009766 + 0.01 * 7.301481246948242
Epoch 120, val loss: 1.4681810140609741
Epoch 130, training loss: 1.3759236335754395 = 1.3038136959075928 + 0.01 * 7.210989475250244
Epoch 130, val loss: 1.4040067195892334
Epoch 140, training loss: 1.2893891334533691 = 1.2177495956420898 + 0.01 * 7.1639556884765625
Epoch 140, val loss: 1.3424526453018188
Epoch 150, training loss: 1.2065378427505493 = 1.1351820230484009 + 0.01 * 7.135581970214844
Epoch 150, val loss: 1.2833843231201172
Epoch 160, training loss: 1.1295651197433472 = 1.0584285259246826 + 0.01 * 7.1136603355407715
Epoch 160, val loss: 1.228980302810669
Epoch 170, training loss: 1.0599827766418457 = 0.9890364408493042 + 0.01 * 7.094631195068359
Epoch 170, val loss: 1.180337905883789
Epoch 180, training loss: 0.9964993596076965 = 0.9256609678268433 + 0.01 * 7.083841323852539
Epoch 180, val loss: 1.1358016729354858
Epoch 190, training loss: 0.9355424046516418 = 0.8648034334182739 + 0.01 * 7.073896408081055
Epoch 190, val loss: 1.092349886894226
Epoch 200, training loss: 0.8738449811935425 = 0.8031545281410217 + 0.01 * 7.069047927856445
Epoch 200, val loss: 1.0476542711257935
Epoch 210, training loss: 0.8101168274879456 = 0.7394640445709229 + 0.01 * 7.065276622772217
Epoch 210, val loss: 1.0012521743774414
Epoch 220, training loss: 0.7453446388244629 = 0.6747298240661621 + 0.01 * 7.061481475830078
Epoch 220, val loss: 0.9554533958435059
Epoch 230, training loss: 0.6815751194953918 = 0.6109951138496399 + 0.01 * 7.05800199508667
Epoch 230, val loss: 0.9132587909698486
Epoch 240, training loss: 0.6202083826065063 = 0.549645721912384 + 0.01 * 7.056266784667969
Epoch 240, val loss: 0.877038836479187
Epoch 250, training loss: 0.5618833899497986 = 0.49133041501045227 + 0.01 * 7.055300235748291
Epoch 250, val loss: 0.8478801846504211
Epoch 260, training loss: 0.5071754455566406 = 0.43662333488464355 + 0.01 * 7.055209636688232
Epoch 260, val loss: 0.8260961174964905
Epoch 270, training loss: 0.4565092921257019 = 0.3859541118144989 + 0.01 * 7.0555195808410645
Epoch 270, val loss: 0.8111302256584167
Epoch 280, training loss: 0.4101094603538513 = 0.3395501375198364 + 0.01 * 7.0559306144714355
Epoch 280, val loss: 0.8017516136169434
Epoch 290, training loss: 0.36810028553009033 = 0.2975366413593292 + 0.01 * 7.056365489959717
Epoch 290, val loss: 0.796648383140564
Epoch 300, training loss: 0.33031439781188965 = 0.2597491443157196 + 0.01 * 7.056525230407715
Epoch 300, val loss: 0.7946417927742004
Epoch 310, training loss: 0.2964647710323334 = 0.22590193152427673 + 0.01 * 7.056283950805664
Epoch 310, val loss: 0.7948544025421143
Epoch 320, training loss: 0.2662917375564575 = 0.19573485851287842 + 0.01 * 7.055686950683594
Epoch 320, val loss: 0.7967294454574585
Epoch 330, training loss: 0.23961198329925537 = 0.16906554996967316 + 0.01 * 7.054644584655762
Epoch 330, val loss: 0.8000344634056091
Epoch 340, training loss: 0.2163274586200714 = 0.14579446613788605 + 0.01 * 7.053299903869629
Epoch 340, val loss: 0.8047913312911987
Epoch 350, training loss: 0.19627077877521515 = 0.12575991451740265 + 0.01 * 7.05108642578125
Epoch 350, val loss: 0.8110687732696533
Epoch 360, training loss: 0.17920464277267456 = 0.10871811211109161 + 0.01 * 7.048652172088623
Epoch 360, val loss: 0.818818211555481
Epoch 370, training loss: 0.16480430960655212 = 0.09434914588928223 + 0.01 * 7.0455169677734375
Epoch 370, val loss: 0.8280737996101379
Epoch 380, training loss: 0.15269526839256287 = 0.08227068930864334 + 0.01 * 7.0424580574035645
Epoch 380, val loss: 0.8386085629463196
Epoch 390, training loss: 0.1424793303012848 = 0.07211290299892426 + 0.01 * 7.036642551422119
Epoch 390, val loss: 0.8502021431922913
Epoch 400, training loss: 0.13388407230377197 = 0.06354371458292007 + 0.01 * 7.034036159515381
Epoch 400, val loss: 0.8625984191894531
Epoch 410, training loss: 0.12654995918273926 = 0.056284137070178986 + 0.01 * 7.026581287384033
Epoch 410, val loss: 0.8754889369010925
Epoch 420, training loss: 0.12036889791488647 = 0.050104521214962006 + 0.01 * 7.026437759399414
Epoch 420, val loss: 0.8886920809745789
Epoch 430, training loss: 0.11492703855037689 = 0.04481806233525276 + 0.01 * 7.010897636413574
Epoch 430, val loss: 0.9020122289657593
Epoch 440, training loss: 0.11030703783035278 = 0.040268901735544205 + 0.01 * 7.003813743591309
Epoch 440, val loss: 0.915292501449585
Epoch 450, training loss: 0.10636404156684875 = 0.036335717886686325 + 0.01 * 7.002832889556885
Epoch 450, val loss: 0.9283843040466309
Epoch 460, training loss: 0.10285623371601105 = 0.03291674330830574 + 0.01 * 6.993948459625244
Epoch 460, val loss: 0.9412052631378174
Epoch 470, training loss: 0.09989172220230103 = 0.029929853975772858 + 0.01 * 6.996187210083008
Epoch 470, val loss: 0.9537261724472046
Epoch 480, training loss: 0.09716293215751648 = 0.027311190962791443 + 0.01 * 6.985174179077148
Epoch 480, val loss: 0.9658989906311035
Epoch 490, training loss: 0.09479739516973495 = 0.025005092844367027 + 0.01 * 6.9792304039001465
Epoch 490, val loss: 0.9776862263679504
Epoch 500, training loss: 0.09274023771286011 = 0.022968370467424393 + 0.01 * 6.977187156677246
Epoch 500, val loss: 0.9890556335449219
Epoch 510, training loss: 0.09087317436933517 = 0.0211629718542099 + 0.01 * 6.971020221710205
Epoch 510, val loss: 0.9999830722808838
Epoch 520, training loss: 0.08921371400356293 = 0.019555747509002686 + 0.01 * 6.965796947479248
Epoch 520, val loss: 1.010589838027954
Epoch 530, training loss: 0.08773472905158997 = 0.01812080852687359 + 0.01 * 6.961391925811768
Epoch 530, val loss: 1.0207725763320923
Epoch 540, training loss: 0.0864398404955864 = 0.016835851594805717 + 0.01 * 6.960399150848389
Epoch 540, val loss: 1.0306105613708496
Epoch 550, training loss: 0.08519904315471649 = 0.015680890530347824 + 0.01 * 6.951815128326416
Epoch 550, val loss: 1.040096640586853
Epoch 560, training loss: 0.0841507613658905 = 0.014640464447438717 + 0.01 * 6.9510297775268555
Epoch 560, val loss: 1.0492128133773804
Epoch 570, training loss: 0.08315279334783554 = 0.013701547868549824 + 0.01 * 6.94512414932251
Epoch 570, val loss: 1.0580010414123535
Epoch 580, training loss: 0.08224599808454514 = 0.012849112041294575 + 0.01 * 6.939688682556152
Epoch 580, val loss: 1.06649911403656
Epoch 590, training loss: 0.08151916414499283 = 0.012070650234818459 + 0.01 * 6.944851398468018
Epoch 590, val loss: 1.0747132301330566
Epoch 600, training loss: 0.08075222373008728 = 0.01135585829615593 + 0.01 * 6.93963623046875
Epoch 600, val loss: 1.08262300491333
Epoch 610, training loss: 0.07994543015956879 = 0.010697167366743088 + 0.01 * 6.9248270988464355
Epoch 610, val loss: 1.0902462005615234
Epoch 620, training loss: 0.07947243005037308 = 0.010088934563100338 + 0.01 * 6.93834924697876
Epoch 620, val loss: 1.097586750984192
Epoch 630, training loss: 0.07872713357210159 = 0.009527801536023617 + 0.01 * 6.919933319091797
Epoch 630, val loss: 1.1047135591506958
Epoch 640, training loss: 0.0781572014093399 = 0.009009888395667076 + 0.01 * 6.914731502532959
Epoch 640, val loss: 1.1116191148757935
Epoch 650, training loss: 0.07763952761888504 = 0.008531766943633556 + 0.01 * 6.910776615142822
Epoch 650, val loss: 1.1182622909545898
Epoch 660, training loss: 0.07711398601531982 = 0.00809049978852272 + 0.01 * 6.902349472045898
Epoch 660, val loss: 1.1247096061706543
Epoch 670, training loss: 0.07677750289440155 = 0.007682594005018473 + 0.01 * 6.909491062164307
Epoch 670, val loss: 1.1309832334518433
Epoch 680, training loss: 0.0762460008263588 = 0.007305307779461145 + 0.01 * 6.894069671630859
Epoch 680, val loss: 1.1370800733566284
Epoch 690, training loss: 0.07592400163412094 = 0.006955891381949186 + 0.01 * 6.896811485290527
Epoch 690, val loss: 1.1429615020751953
Epoch 700, training loss: 0.07556979358196259 = 0.006632075645029545 + 0.01 * 6.893772125244141
Epoch 700, val loss: 1.1486650705337524
Epoch 710, training loss: 0.07524605095386505 = 0.006331380922347307 + 0.01 * 6.891467094421387
Epoch 710, val loss: 1.1541894674301147
Epoch 720, training loss: 0.07484162598848343 = 0.006051958072930574 + 0.01 * 6.87896728515625
Epoch 720, val loss: 1.1594983339309692
Epoch 730, training loss: 0.07452016323804855 = 0.005791765637695789 + 0.01 * 6.87283992767334
Epoch 730, val loss: 1.1646727323532104
Epoch 740, training loss: 0.07433187961578369 = 0.005549515597522259 + 0.01 * 6.878236770629883
Epoch 740, val loss: 1.1696289777755737
Epoch 750, training loss: 0.0740659162402153 = 0.005323497578501701 + 0.01 * 6.874241828918457
Epoch 750, val loss: 1.1745449304580688
Epoch 760, training loss: 0.07374978065490723 = 0.005112471058964729 + 0.01 * 6.863731384277344
Epoch 760, val loss: 1.179216742515564
Epoch 770, training loss: 0.07358236610889435 = 0.004915027413517237 + 0.01 * 6.866733551025391
Epoch 770, val loss: 1.1837292909622192
Epoch 780, training loss: 0.07324342429637909 = 0.004730223678052425 + 0.01 * 6.851319789886475
Epoch 780, val loss: 1.1881780624389648
Epoch 790, training loss: 0.07303611189126968 = 0.004556857980787754 + 0.01 * 6.847925662994385
Epoch 790, val loss: 1.1924711465835571
Epoch 800, training loss: 0.0727703869342804 = 0.004393941722810268 + 0.01 * 6.837644577026367
Epoch 800, val loss: 1.1966285705566406
Epoch 810, training loss: 0.07269648462533951 = 0.004241001792252064 + 0.01 * 6.845548152923584
Epoch 810, val loss: 1.2007418870925903
Epoch 820, training loss: 0.07239092141389847 = 0.004097628407180309 + 0.01 * 6.829329490661621
Epoch 820, val loss: 1.2046781778335571
Epoch 830, training loss: 0.07237181067466736 = 0.00396260991692543 + 0.01 * 6.840919494628906
Epoch 830, val loss: 1.2085014581680298
Epoch 840, training loss: 0.07208891212940216 = 0.0038351716939359903 + 0.01 * 6.825374126434326
Epoch 840, val loss: 1.2123050689697266
Epoch 850, training loss: 0.07220221310853958 = 0.0037146909162402153 + 0.01 * 6.848752498626709
Epoch 850, val loss: 1.2159802913665771
Epoch 860, training loss: 0.07190864533185959 = 0.0036008425522595644 + 0.01 * 6.830780506134033
Epoch 860, val loss: 1.219569444656372
Epoch 870, training loss: 0.07166748493909836 = 0.0034931248519569635 + 0.01 * 6.817436218261719
Epoch 870, val loss: 1.2230310440063477
Epoch 880, training loss: 0.07148218899965286 = 0.003391165053471923 + 0.01 * 6.809103012084961
Epoch 880, val loss: 1.226465106010437
Epoch 890, training loss: 0.07139788568019867 = 0.0032944201957434416 + 0.01 * 6.8103461265563965
Epoch 890, val loss: 1.2297765016555786
Epoch 900, training loss: 0.07122696191072464 = 0.0032027335837483406 + 0.01 * 6.802422523498535
Epoch 900, val loss: 1.2330167293548584
Epoch 910, training loss: 0.07116596400737762 = 0.003115854226052761 + 0.01 * 6.80501127243042
Epoch 910, val loss: 1.236124038696289
Epoch 920, training loss: 0.07122410833835602 = 0.0030332780443131924 + 0.01 * 6.819082736968994
Epoch 920, val loss: 1.2392430305480957
Epoch 930, training loss: 0.07108568400144577 = 0.0029547533486038446 + 0.01 * 6.813093662261963
Epoch 930, val loss: 1.2421494722366333
Epoch 940, training loss: 0.07091011106967926 = 0.00288008782081306 + 0.01 * 6.80300235748291
Epoch 940, val loss: 1.2450870275497437
Epoch 950, training loss: 0.07094886898994446 = 0.002808826044201851 + 0.01 * 6.814004421234131
Epoch 950, val loss: 1.2478632926940918
Epoch 960, training loss: 0.07071725279092789 = 0.002740829484537244 + 0.01 * 6.797642230987549
Epoch 960, val loss: 1.250699520111084
Epoch 970, training loss: 0.07059721648693085 = 0.002676081145182252 + 0.01 * 6.792113304138184
Epoch 970, val loss: 1.2533504962921143
Epoch 980, training loss: 0.07044949382543564 = 0.0026143849827349186 + 0.01 * 6.783511161804199
Epoch 980, val loss: 1.2560096979141235
Epoch 990, training loss: 0.07062100619077682 = 0.00255552283488214 + 0.01 * 6.806549072265625
Epoch 990, val loss: 1.2586041688919067
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.034440755844116 = 1.9484727382659912 + 0.01 * 8.59681224822998
Epoch 0, val loss: 1.9463422298431396
Epoch 10, training loss: 2.0244429111480713 = 1.938475489616394 + 0.01 * 8.596741676330566
Epoch 10, val loss: 1.9362634420394897
Epoch 20, training loss: 2.012265920639038 = 1.9263008832931519 + 0.01 * 8.5964937210083
Epoch 20, val loss: 1.9239355325698853
Epoch 30, training loss: 1.9952335357666016 = 1.909276008605957 + 0.01 * 8.595747947692871
Epoch 30, val loss: 1.906814694404602
Epoch 40, training loss: 1.9699065685272217 = 1.8839880228042603 + 0.01 * 8.591856956481934
Epoch 40, val loss: 1.881665587425232
Epoch 50, training loss: 1.9335334300994873 = 1.8478885889053345 + 0.01 * 8.564485549926758
Epoch 50, val loss: 1.8470048904418945
Epoch 60, training loss: 1.8885904550552368 = 1.8043160438537598 + 0.01 * 8.427438735961914
Epoch 60, val loss: 1.8081293106079102
Epoch 70, training loss: 1.8432573080062866 = 1.762266755104065 + 0.01 * 8.099052429199219
Epoch 70, val loss: 1.772998332977295
Epoch 80, training loss: 1.7934885025024414 = 1.714959740638733 + 0.01 * 7.852874279022217
Epoch 80, val loss: 1.731163501739502
Epoch 90, training loss: 1.7268187999725342 = 1.651100754737854 + 0.01 * 7.571808338165283
Epoch 90, val loss: 1.6745485067367554
Epoch 100, training loss: 1.6418898105621338 = 1.567779302597046 + 0.01 * 7.4110517501831055
Epoch 100, val loss: 1.6035248041152954
Epoch 110, training loss: 1.5441648960113525 = 1.4709843397140503 + 0.01 * 7.318056106567383
Epoch 110, val loss: 1.5238287448883057
Epoch 120, training loss: 1.4429833889007568 = 1.370497465133667 + 0.01 * 7.248594760894775
Epoch 120, val loss: 1.4433172941207886
Epoch 130, training loss: 1.3437809944152832 = 1.271847128868103 + 0.01 * 7.193384647369385
Epoch 130, val loss: 1.3650952577590942
Epoch 140, training loss: 1.2494405508041382 = 1.1778780221939087 + 0.01 * 7.156248569488525
Epoch 140, val loss: 1.292057752609253
Epoch 150, training loss: 1.1626546382904053 = 1.0913530588150024 + 0.01 * 7.130159854888916
Epoch 150, val loss: 1.2262989282608032
Epoch 160, training loss: 1.0851502418518066 = 1.0140364170074463 + 0.01 * 7.111381530761719
Epoch 160, val loss: 1.1691111326217651
Epoch 170, training loss: 1.0153040885925293 = 0.9442964792251587 + 0.01 * 7.1007561683654785
Epoch 170, val loss: 1.118533968925476
Epoch 180, training loss: 0.9493411779403687 = 0.8783828020095825 + 0.01 * 7.095837116241455
Epoch 180, val loss: 1.0708280801773071
Epoch 190, training loss: 0.8832610845565796 = 0.8123400211334229 + 0.01 * 7.092105388641357
Epoch 190, val loss: 1.0225865840911865
Epoch 200, training loss: 0.8145955801010132 = 0.7437119483947754 + 0.01 * 7.088364124298096
Epoch 200, val loss: 0.9719684720039368
Epoch 210, training loss: 0.7432382106781006 = 0.6723930835723877 + 0.01 * 7.08451509475708
Epoch 210, val loss: 0.9198572039604187
Epoch 220, training loss: 0.6710242033004761 = 0.6002197861671448 + 0.01 * 7.080442428588867
Epoch 220, val loss: 0.8691070079803467
Epoch 230, training loss: 0.6003037095069885 = 0.5295441746711731 + 0.01 * 7.075954437255859
Epoch 230, val loss: 0.8232165575027466
Epoch 240, training loss: 0.5331035256385803 = 0.4623940885066986 + 0.01 * 7.070941925048828
Epoch 240, val loss: 0.7842689156532288
Epoch 250, training loss: 0.47095736861228943 = 0.40030378103256226 + 0.01 * 7.0653581619262695
Epoch 250, val loss: 0.7528098821640015
Epoch 260, training loss: 0.41488760709762573 = 0.34430527687072754 + 0.01 * 7.058234214782715
Epoch 260, val loss: 0.7282766103744507
Epoch 270, training loss: 0.36540544033050537 = 0.29488563537597656 + 0.01 * 7.051979064941406
Epoch 270, val loss: 0.7101638317108154
Epoch 280, training loss: 0.32253578305244446 = 0.25205764174461365 + 0.01 * 7.047814846038818
Epoch 280, val loss: 0.6978254914283752
Epoch 290, training loss: 0.28583183884620667 = 0.2154417783021927 + 0.01 * 7.039006233215332
Epoch 290, val loss: 0.6903825402259827
Epoch 300, training loss: 0.25476738810539246 = 0.18444879353046417 + 0.01 * 7.031859874725342
Epoch 300, val loss: 0.6870546340942383
Epoch 310, training loss: 0.22862491011619568 = 0.1583559364080429 + 0.01 * 7.02689790725708
Epoch 310, val loss: 0.687103807926178
Epoch 320, training loss: 0.20666995644569397 = 0.13643474876880646 + 0.01 * 7.0235209465026855
Epoch 320, val loss: 0.689894437789917
Epoch 330, training loss: 0.18821638822555542 = 0.11802218109369278 + 0.01 * 7.019420146942139
Epoch 330, val loss: 0.6949057579040527
Epoch 340, training loss: 0.17269296944141388 = 0.10252974927425385 + 0.01 * 7.016322135925293
Epoch 340, val loss: 0.7015522718429565
Epoch 350, training loss: 0.15959589183330536 = 0.08945025503635406 + 0.01 * 7.014564037322998
Epoch 350, val loss: 0.7094018459320068
Epoch 360, training loss: 0.14848625659942627 = 0.07836324721574783 + 0.01 * 7.012301445007324
Epoch 360, val loss: 0.7181217074394226
Epoch 370, training loss: 0.1390303075313568 = 0.0689292699098587 + 0.01 * 7.010103225708008
Epoch 370, val loss: 0.7273980379104614
Epoch 380, training loss: 0.13094361126422882 = 0.06087617948651314 + 0.01 * 7.00674295425415
Epoch 380, val loss: 0.7370321750640869
Epoch 390, training loss: 0.12402187287807465 = 0.053984031081199646 + 0.01 * 7.0037841796875
Epoch 390, val loss: 0.746802568435669
Epoch 400, training loss: 0.11810418963432312 = 0.048073235899209976 + 0.01 * 7.003095626831055
Epoch 400, val loss: 0.7566145658493042
Epoch 410, training loss: 0.1129978746175766 = 0.04299331456422806 + 0.01 * 7.000455856323242
Epoch 410, val loss: 0.7663941383361816
Epoch 420, training loss: 0.10858578234910965 = 0.038616783916950226 + 0.01 * 6.9969000816345215
Epoch 420, val loss: 0.7761256098747253
Epoch 430, training loss: 0.1047951728105545 = 0.03483555093407631 + 0.01 * 6.995961666107178
Epoch 430, val loss: 0.7857193946838379
Epoch 440, training loss: 0.10148029029369354 = 0.031558092683553696 + 0.01 * 6.992220401763916
Epoch 440, val loss: 0.7951358556747437
Epoch 450, training loss: 0.09860559552907944 = 0.028704656288027763 + 0.01 * 6.990094184875488
Epoch 450, val loss: 0.8043559193611145
Epoch 460, training loss: 0.09608958661556244 = 0.026209941133856773 + 0.01 * 6.987964630126953
Epoch 460, val loss: 0.8133509755134583
Epoch 470, training loss: 0.09389783442020416 = 0.02402009814977646 + 0.01 * 6.987773418426514
Epoch 470, val loss: 0.8221270442008972
Epoch 480, training loss: 0.09193210303783417 = 0.022090500220656395 + 0.01 * 6.98415994644165
Epoch 480, val loss: 0.8306990265846252
Epoch 490, training loss: 0.09018957614898682 = 0.020383019000291824 + 0.01 * 6.980655193328857
Epoch 490, val loss: 0.8390002846717834
Epoch 500, training loss: 0.08864452689886093 = 0.018865911290049553 + 0.01 * 6.977861404418945
Epoch 500, val loss: 0.8470624685287476
Epoch 510, training loss: 0.08727926760911942 = 0.017513126134872437 + 0.01 * 6.976614475250244
Epoch 510, val loss: 0.8549102544784546
Epoch 520, training loss: 0.08603785932064056 = 0.016302896663546562 + 0.01 * 6.973496437072754
Epoch 520, val loss: 0.8625296354293823
Epoch 530, training loss: 0.08490991592407227 = 0.015216032043099403 + 0.01 * 6.969388961791992
Epoch 530, val loss: 0.8699226975440979
Epoch 540, training loss: 0.0839572474360466 = 0.014236578717827797 + 0.01 * 6.972067356109619
Epoch 540, val loss: 0.8771243095397949
Epoch 550, training loss: 0.08298908919095993 = 0.013351531699299812 + 0.01 * 6.963756084442139
Epoch 550, val loss: 0.8840895891189575
Epoch 560, training loss: 0.0821651816368103 = 0.01254909299314022 + 0.01 * 6.96160888671875
Epoch 560, val loss: 0.8908723592758179
Epoch 570, training loss: 0.08142342418432236 = 0.011819086037576199 + 0.01 * 6.9604339599609375
Epoch 570, val loss: 0.8974584341049194
Epoch 580, training loss: 0.08071274310350418 = 0.011153501458466053 + 0.01 * 6.955924034118652
Epoch 580, val loss: 0.9038671851158142
Epoch 590, training loss: 0.08008109033107758 = 0.010544506832957268 + 0.01 * 6.953658580780029
Epoch 590, val loss: 0.9100977182388306
Epoch 600, training loss: 0.07952684164047241 = 0.009987583383917809 + 0.01 * 6.953925609588623
Epoch 600, val loss: 0.9162287712097168
Epoch 610, training loss: 0.07896662503480911 = 0.009475382044911385 + 0.01 * 6.949124813079834
Epoch 610, val loss: 0.9221527576446533
Epoch 620, training loss: 0.07844527065753937 = 0.009004415944218636 + 0.01 * 6.944086074829102
Epoch 620, val loss: 0.9279071688652039
Epoch 630, training loss: 0.07797392457723618 = 0.008569267578423023 + 0.01 * 6.940465450286865
Epoch 630, val loss: 0.9335593581199646
Epoch 640, training loss: 0.07755070179700851 = 0.008167554624378681 + 0.01 * 6.938314914703369
Epoch 640, val loss: 0.9389915466308594
Epoch 650, training loss: 0.07716827839612961 = 0.007795542944222689 + 0.01 * 6.9372735023498535
Epoch 650, val loss: 0.9443889856338501
Epoch 660, training loss: 0.07676132023334503 = 0.007450357545167208 + 0.01 * 6.93109655380249
Epoch 660, val loss: 0.9496289491653442
Epoch 670, training loss: 0.07644564658403397 = 0.007129569537937641 + 0.01 * 6.931607246398926
Epoch 670, val loss: 0.9547467231750488
Epoch 680, training loss: 0.07611725479364395 = 0.006830580998212099 + 0.01 * 6.9286675453186035
Epoch 680, val loss: 0.959676206111908
Epoch 690, training loss: 0.07576171308755875 = 0.006551976781338453 + 0.01 * 6.920974254608154
Epoch 690, val loss: 0.964556097984314
Epoch 700, training loss: 0.07547254115343094 = 0.006291613914072514 + 0.01 * 6.918093204498291
Epoch 700, val loss: 0.96929931640625
Epoch 710, training loss: 0.07524317502975464 = 0.006047694478183985 + 0.01 * 6.919548034667969
Epoch 710, val loss: 0.9739367365837097
Epoch 720, training loss: 0.07502997666597366 = 0.00581997400149703 + 0.01 * 6.9210004806518555
Epoch 720, val loss: 0.9784972071647644
Epoch 730, training loss: 0.07471292465925217 = 0.005606038961559534 + 0.01 * 6.910688400268555
Epoch 730, val loss: 0.9829151034355164
Epoch 740, training loss: 0.07454179227352142 = 0.0054056099615991116 + 0.01 * 6.913618087768555
Epoch 740, val loss: 0.9871567487716675
Epoch 750, training loss: 0.07427258789539337 = 0.005216815043240786 + 0.01 * 6.905577659606934
Epoch 750, val loss: 0.991473376750946
Epoch 760, training loss: 0.07401661574840546 = 0.005038907751441002 + 0.01 * 6.897770881652832
Epoch 760, val loss: 0.9955199360847473
Epoch 770, training loss: 0.0737834945321083 = 0.004871218930929899 + 0.01 * 6.891227722167969
Epoch 770, val loss: 0.9995911717414856
Epoch 780, training loss: 0.07381035387516022 = 0.004712868947535753 + 0.01 * 6.909748077392578
Epoch 780, val loss: 1.0035200119018555
Epoch 790, training loss: 0.07349510490894318 = 0.004563193768262863 + 0.01 * 6.893190860748291
Epoch 790, val loss: 1.007327914237976
Epoch 800, training loss: 0.07321824878454208 = 0.004422168713063002 + 0.01 * 6.879608631134033
Epoch 800, val loss: 1.0111085176467896
Epoch 810, training loss: 0.07368312031030655 = 0.004288411699235439 + 0.01 * 6.939471244812012
Epoch 810, val loss: 1.014666199684143
Epoch 820, training loss: 0.07297857105731964 = 0.004162234254181385 + 0.01 * 6.881633758544922
Epoch 820, val loss: 1.0182650089263916
Epoch 830, training loss: 0.07270776480436325 = 0.004042745102196932 + 0.01 * 6.866502285003662
Epoch 830, val loss: 1.0217630863189697
Epoch 840, training loss: 0.07257682830095291 = 0.003929312340915203 + 0.01 * 6.864751815795898
Epoch 840, val loss: 1.0251610279083252
Epoch 850, training loss: 0.07254256308078766 = 0.0038212516810745 + 0.01 * 6.872130870819092
Epoch 850, val loss: 1.0285710096359253
Epoch 860, training loss: 0.0722525417804718 = 0.0037186378613114357 + 0.01 * 6.853390693664551
Epoch 860, val loss: 1.031815767288208
Epoch 870, training loss: 0.07252874225378036 = 0.0036209544632583857 + 0.01 * 6.8907790184021
Epoch 870, val loss: 1.0349147319793701
Epoch 880, training loss: 0.0720628872513771 = 0.0035278822761029005 + 0.01 * 6.853500843048096
Epoch 880, val loss: 1.0380277633666992
Epoch 890, training loss: 0.0719422772526741 = 0.003439289750531316 + 0.01 * 6.850298881530762
Epoch 890, val loss: 1.0411802530288696
Epoch 900, training loss: 0.07171551138162613 = 0.003354604123160243 + 0.01 * 6.836091041564941
Epoch 900, val loss: 1.0440763235092163
Epoch 910, training loss: 0.07158031314611435 = 0.0032741413451731205 + 0.01 * 6.8306169509887695
Epoch 910, val loss: 1.0468826293945312
Epoch 920, training loss: 0.07160486280918121 = 0.0031973482109606266 + 0.01 * 6.8407511711120605
Epoch 920, val loss: 1.0499008893966675
Epoch 930, training loss: 0.07154121994972229 = 0.0031236265785992146 + 0.01 * 6.841759204864502
Epoch 930, val loss: 1.0524742603302002
Epoch 940, training loss: 0.0711231604218483 = 0.003053107066079974 + 0.01 * 6.807005405426025
Epoch 940, val loss: 1.0552592277526855
Epoch 950, training loss: 0.07121425867080688 = 0.0029857102781534195 + 0.01 * 6.822855472564697
Epoch 950, val loss: 1.0578558444976807
Epoch 960, training loss: 0.07092184573411942 = 0.0029210939537733793 + 0.01 * 6.800075054168701
Epoch 960, val loss: 1.0604044198989868
Epoch 970, training loss: 0.07101702690124512 = 0.002859506057575345 + 0.01 * 6.8157525062561035
Epoch 970, val loss: 1.0629149675369263
Epoch 980, training loss: 0.070707306265831 = 0.0028002795297652483 + 0.01 * 6.790703296661377
Epoch 980, val loss: 1.0654603242874146
Epoch 990, training loss: 0.07066243141889572 = 0.002743421122431755 + 0.01 * 6.791900634765625
Epoch 990, val loss: 1.0678589344024658
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8360569319978914
The final CL Acc:0.81235, 0.00349, The final GNN Acc:0.83975, 0.00383
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10514])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0244247913360596 = 1.9384565353393555 + 0.01 * 8.596837043762207
Epoch 0, val loss: 1.9431202411651611
Epoch 10, training loss: 2.014932870864868 = 1.9289649724960327 + 0.01 * 8.59678840637207
Epoch 10, val loss: 1.9336893558502197
Epoch 20, training loss: 2.00325870513916 = 1.9172927141189575 + 0.01 * 8.596601486206055
Epoch 20, val loss: 1.9214649200439453
Epoch 30, training loss: 1.9869986772537231 = 1.9010381698608398 + 0.01 * 8.596047401428223
Epoch 30, val loss: 1.9038512706756592
Epoch 40, training loss: 1.9633851051330566 = 1.8774538040161133 + 0.01 * 8.593133926391602
Epoch 40, val loss: 1.8780680894851685
Epoch 50, training loss: 1.9308840036392212 = 1.845166563987732 + 0.01 * 8.57174301147461
Epoch 50, val loss: 1.8438000679016113
Epoch 60, training loss: 1.8946397304534912 = 1.8101252317428589 + 0.01 * 8.451454162597656
Epoch 60, val loss: 1.8100160360336304
Epoch 70, training loss: 1.8625582456588745 = 1.7801172733306885 + 0.01 * 8.244096755981445
Epoch 70, val loss: 1.7846013307571411
Epoch 80, training loss: 1.8221094608306885 = 1.7421914339065552 + 0.01 * 7.9918036460876465
Epoch 80, val loss: 1.7519662380218506
Epoch 90, training loss: 1.7653391361236572 = 1.6886028051376343 + 0.01 * 7.673635959625244
Epoch 90, val loss: 1.7055410146713257
Epoch 100, training loss: 1.6901706457138062 = 1.6149623394012451 + 0.01 * 7.520827770233154
Epoch 100, val loss: 1.6421687602996826
Epoch 110, training loss: 1.5995721817016602 = 1.5248326063156128 + 0.01 * 7.473962306976318
Epoch 110, val loss: 1.56485915184021
Epoch 120, training loss: 1.5010451078414917 = 1.4268712997436523 + 0.01 * 7.417379856109619
Epoch 120, val loss: 1.4824005365371704
Epoch 130, training loss: 1.3982568979263306 = 1.324794054031372 + 0.01 * 7.346285820007324
Epoch 130, val loss: 1.4006446599960327
Epoch 140, training loss: 1.291355848312378 = 1.218445897102356 + 0.01 * 7.290990352630615
Epoch 140, val loss: 1.317674994468689
Epoch 150, training loss: 1.1826131343841553 = 1.110029697418213 + 0.01 * 7.25834321975708
Epoch 150, val loss: 1.2349027395248413
Epoch 160, training loss: 1.0769813060760498 = 1.0046688318252563 + 0.01 * 7.231245517730713
Epoch 160, val loss: 1.1561788320541382
Epoch 170, training loss: 0.9789222478866577 = 0.9068432450294495 + 0.01 * 7.207902431488037
Epoch 170, val loss: 1.0849559307098389
Epoch 180, training loss: 0.8909590244293213 = 0.8190694451332092 + 0.01 * 7.188961029052734
Epoch 180, val loss: 1.023377776145935
Epoch 190, training loss: 0.8135700225830078 = 0.741797924041748 + 0.01 * 7.177209377288818
Epoch 190, val loss: 0.9721436500549316
Epoch 200, training loss: 0.745373547077179 = 0.6736981272697449 + 0.01 * 7.167543888092041
Epoch 200, val loss: 0.9306873083114624
Epoch 210, training loss: 0.6841945052146912 = 0.6125739812850952 + 0.01 * 7.162050247192383
Epoch 210, val loss: 0.8978513479232788
Epoch 220, training loss: 0.6279313564300537 = 0.5563679337501526 + 0.01 * 7.156339168548584
Epoch 220, val loss: 0.8727301955223083
Epoch 230, training loss: 0.5751721262931824 = 0.5036472678184509 + 0.01 * 7.152487754821777
Epoch 230, val loss: 0.8545017242431641
Epoch 240, training loss: 0.5251032114028931 = 0.45360302925109863 + 0.01 * 7.150021076202393
Epoch 240, val loss: 0.8422064781188965
Epoch 250, training loss: 0.47755521535873413 = 0.40608277916908264 + 0.01 * 7.147242069244385
Epoch 250, val loss: 0.8347423672676086
Epoch 260, training loss: 0.4328174591064453 = 0.36137595772743225 + 0.01 * 7.1441497802734375
Epoch 260, val loss: 0.8313085436820984
Epoch 270, training loss: 0.3913736939430237 = 0.31991976499557495 + 0.01 * 7.145392894744873
Epoch 270, val loss: 0.8317843675613403
Epoch 280, training loss: 0.3535487949848175 = 0.2821383774280548 + 0.01 * 7.1410417556762695
Epoch 280, val loss: 0.8360751271247864
Epoch 290, training loss: 0.31961819529533386 = 0.2482110559940338 + 0.01 * 7.140714168548584
Epoch 290, val loss: 0.8438657522201538
Epoch 300, training loss: 0.2894349694252014 = 0.21804988384246826 + 0.01 * 7.138507843017578
Epoch 300, val loss: 0.8545310497283936
Epoch 310, training loss: 0.2628021836280823 = 0.19144578278064728 + 0.01 * 7.135639667510986
Epoch 310, val loss: 0.8676003813743591
Epoch 320, training loss: 0.23948951065540314 = 0.16811099648475647 + 0.01 * 7.137851715087891
Epoch 320, val loss: 0.8827415108680725
Epoch 330, training loss: 0.21904441714286804 = 0.14772766828536987 + 0.01 * 7.131675720214844
Epoch 330, val loss: 0.8996046781539917
Epoch 340, training loss: 0.20124107599258423 = 0.12995941936969757 + 0.01 * 7.128166198730469
Epoch 340, val loss: 0.9179362058639526
Epoch 350, training loss: 0.18576347827911377 = 0.11448612809181213 + 0.01 * 7.127734661102295
Epoch 350, val loss: 0.9373316168785095
Epoch 360, training loss: 0.17221561074256897 = 0.10101819038391113 + 0.01 * 7.119741439819336
Epoch 360, val loss: 0.9574971795082092
Epoch 370, training loss: 0.1604596972465515 = 0.08928696066141129 + 0.01 * 7.117273330688477
Epoch 370, val loss: 0.9781878590583801
Epoch 380, training loss: 0.15016154944896698 = 0.07906285673379898 + 0.01 * 7.109869480133057
Epoch 380, val loss: 0.9991233348846436
Epoch 390, training loss: 0.14120057225227356 = 0.0701575055718422 + 0.01 * 7.104307174682617
Epoch 390, val loss: 1.0201743841171265
Epoch 400, training loss: 0.13343371450901031 = 0.062404338270425797 + 0.01 * 7.102937698364258
Epoch 400, val loss: 1.041050910949707
Epoch 410, training loss: 0.12655393779277802 = 0.055666353553533554 + 0.01 * 7.0887579917907715
Epoch 410, val loss: 1.0615792274475098
Epoch 420, training loss: 0.12064371258020401 = 0.049805499613285065 + 0.01 * 7.0838212966918945
Epoch 420, val loss: 1.081729531288147
Epoch 430, training loss: 0.11559155583381653 = 0.04471523314714432 + 0.01 * 7.087632179260254
Epoch 430, val loss: 1.1013455390930176
Epoch 440, training loss: 0.11098173260688782 = 0.04029541462659836 + 0.01 * 7.068631649017334
Epoch 440, val loss: 1.1204506158828735
Epoch 450, training loss: 0.10704530030488968 = 0.036444149911403656 + 0.01 * 7.060115337371826
Epoch 450, val loss: 1.1389018297195435
Epoch 460, training loss: 0.10355567187070847 = 0.03308134526014328 + 0.01 * 7.047432899475098
Epoch 460, val loss: 1.1567587852478027
Epoch 470, training loss: 0.10055658221244812 = 0.030139366164803505 + 0.01 * 7.041722297668457
Epoch 470, val loss: 1.1739870309829712
Epoch 480, training loss: 0.09821371734142303 = 0.02755451761186123 + 0.01 * 7.065919876098633
Epoch 480, val loss: 1.1905739307403564
Epoch 490, training loss: 0.09568329900503159 = 0.025280555710196495 + 0.01 * 7.040274620056152
Epoch 490, val loss: 1.2065753936767578
Epoch 500, training loss: 0.09338825941085815 = 0.023272549733519554 + 0.01 * 7.011570930480957
Epoch 500, val loss: 1.2219493389129639
Epoch 510, training loss: 0.09153151512145996 = 0.021491892635822296 + 0.01 * 7.003962516784668
Epoch 510, val loss: 1.2367477416992188
Epoch 520, training loss: 0.08987945318222046 = 0.01990749128162861 + 0.01 * 6.997196197509766
Epoch 520, val loss: 1.250978708267212
Epoch 530, training loss: 0.08862969279289246 = 0.018492236733436584 + 0.01 * 7.0137457847595215
Epoch 530, val loss: 1.2646664381027222
Epoch 540, training loss: 0.08702678978443146 = 0.017225053161382675 + 0.01 * 6.980173110961914
Epoch 540, val loss: 1.2778478860855103
Epoch 550, training loss: 0.0861222967505455 = 0.01608595997095108 + 0.01 * 7.003633499145508
Epoch 550, val loss: 1.2905209064483643
Epoch 560, training loss: 0.0848892480134964 = 0.015060262754559517 + 0.01 * 6.982898712158203
Epoch 560, val loss: 1.3026957511901855
Epoch 570, training loss: 0.08371001482009888 = 0.014133070595562458 + 0.01 * 6.957694053649902
Epoch 570, val loss: 1.3144930601119995
Epoch 580, training loss: 0.08273159712553024 = 0.013291674666106701 + 0.01 * 6.943992614746094
Epoch 580, val loss: 1.3258213996887207
Epoch 590, training loss: 0.08196692168712616 = 0.012526378966867924 + 0.01 * 6.94405460357666
Epoch 590, val loss: 1.3367552757263184
Epoch 600, training loss: 0.08125053346157074 = 0.01182882022112608 + 0.01 * 6.942172050476074
Epoch 600, val loss: 1.347305417060852
Epoch 610, training loss: 0.08058685064315796 = 0.011191317811608315 + 0.01 * 6.939553737640381
Epoch 610, val loss: 1.3575456142425537
Epoch 620, training loss: 0.08003664016723633 = 0.010607610456645489 + 0.01 * 6.942903518676758
Epoch 620, val loss: 1.3673901557922363
Epoch 630, training loss: 0.07933134585618973 = 0.010071038268506527 + 0.01 * 6.92603063583374
Epoch 630, val loss: 1.3769334554672241
Epoch 640, training loss: 0.0786394476890564 = 0.009576912969350815 + 0.01 * 6.906253337860107
Epoch 640, val loss: 1.3861982822418213
Epoch 650, training loss: 0.07824055105447769 = 0.009121013805270195 + 0.01 * 6.911953449249268
Epoch 650, val loss: 1.3950988054275513
Epoch 660, training loss: 0.07777862995862961 = 0.008700499311089516 + 0.01 * 6.907813549041748
Epoch 660, val loss: 1.4037340879440308
Epoch 670, training loss: 0.07729277014732361 = 0.008310895413160324 + 0.01 * 6.898187637329102
Epoch 670, val loss: 1.4121545553207397
Epoch 680, training loss: 0.0770154744386673 = 0.007949041202664375 + 0.01 * 6.906643867492676
Epoch 680, val loss: 1.4202924966812134
Epoch 690, training loss: 0.07663831859827042 = 0.007612411864101887 + 0.01 * 6.902590274810791
Epoch 690, val loss: 1.4281995296478271
Epoch 700, training loss: 0.07638033479452133 = 0.007298567332327366 + 0.01 * 6.908176898956299
Epoch 700, val loss: 1.4358391761779785
Epoch 710, training loss: 0.07576191425323486 = 0.007006179541349411 + 0.01 * 6.875574111938477
Epoch 710, val loss: 1.4433125257492065
Epoch 720, training loss: 0.07541771233081818 = 0.006732713431119919 + 0.01 * 6.868500232696533
Epoch 720, val loss: 1.450532078742981
Epoch 730, training loss: 0.07518406957387924 = 0.0064768558368086815 + 0.01 * 6.870721340179443
Epoch 730, val loss: 1.4575097560882568
Epoch 740, training loss: 0.07492979615926743 = 0.0062371171079576015 + 0.01 * 6.869267463684082
Epoch 740, val loss: 1.464388370513916
Epoch 750, training loss: 0.07470526546239853 = 0.00601228978484869 + 0.01 * 6.869297504425049
Epoch 750, val loss: 1.471064567565918
Epoch 760, training loss: 0.07420919090509415 = 0.00580068351700902 + 0.01 * 6.840850353240967
Epoch 760, val loss: 1.477543592453003
Epoch 770, training loss: 0.0741848275065422 = 0.005601533688604832 + 0.01 * 6.8583292961120605
Epoch 770, val loss: 1.4838049411773682
Epoch 780, training loss: 0.07388918846845627 = 0.005414142739027739 + 0.01 * 6.847504615783691
Epoch 780, val loss: 1.4900031089782715
Epoch 790, training loss: 0.07356458902359009 = 0.0052374848164618015 + 0.01 * 6.8327107429504395
Epoch 790, val loss: 1.4960014820098877
Epoch 800, training loss: 0.0734538659453392 = 0.005070330109447241 + 0.01 * 6.838354110717773
Epoch 800, val loss: 1.5018010139465332
Epoch 810, training loss: 0.0731099396944046 = 0.004912318661808968 + 0.01 * 6.819762706756592
Epoch 810, val loss: 1.5075581073760986
Epoch 820, training loss: 0.07345620542764664 = 0.004762588534504175 + 0.01 * 6.869361400604248
Epoch 820, val loss: 1.5131422281265259
Epoch 830, training loss: 0.07296408712863922 = 0.004620842635631561 + 0.01 * 6.834324836730957
Epoch 830, val loss: 1.5185997486114502
Epoch 840, training loss: 0.07269295305013657 = 0.00448647141456604 + 0.01 * 6.820648670196533
Epoch 840, val loss: 1.5239105224609375
Epoch 850, training loss: 0.07260505110025406 = 0.004358978942036629 + 0.01 * 6.824607849121094
Epoch 850, val loss: 1.5291348695755005
Epoch 860, training loss: 0.07226362079381943 = 0.004237576853483915 + 0.01 * 6.8026041984558105
Epoch 860, val loss: 1.5341720581054688
Epoch 870, training loss: 0.07232799381017685 = 0.004122120328247547 + 0.01 * 6.820587158203125
Epoch 870, val loss: 1.5390952825546265
Epoch 880, training loss: 0.07221458852291107 = 0.004012275952845812 + 0.01 * 6.8202314376831055
Epoch 880, val loss: 1.5439903736114502
Epoch 890, training loss: 0.07203266769647598 = 0.003907638601958752 + 0.01 * 6.812503337860107
Epoch 890, val loss: 1.5486652851104736
Epoch 900, training loss: 0.07171226292848587 = 0.0038077430799603462 + 0.01 * 6.790451526641846
Epoch 900, val loss: 1.553310751914978
Epoch 910, training loss: 0.07190181314945221 = 0.003712441772222519 + 0.01 * 6.8189377784729
Epoch 910, val loss: 1.5578612089157104
Epoch 920, training loss: 0.07143134623765945 = 0.0036214725114405155 + 0.01 * 6.78098726272583
Epoch 920, val loss: 1.562278389930725
Epoch 930, training loss: 0.07156364619731903 = 0.003534383838996291 + 0.01 * 6.802926063537598
Epoch 930, val loss: 1.566545844078064
Epoch 940, training loss: 0.07127358764410019 = 0.0034514085855334997 + 0.01 * 6.7822184562683105
Epoch 940, val loss: 1.5707508325576782
Epoch 950, training loss: 0.0713937059044838 = 0.0033718778286129236 + 0.01 * 6.802183151245117
Epoch 950, val loss: 1.574905276298523
Epoch 960, training loss: 0.0710286796092987 = 0.00329566583968699 + 0.01 * 6.773301601409912
Epoch 960, val loss: 1.5789108276367188
Epoch 970, training loss: 0.07097755372524261 = 0.003222543513402343 + 0.01 * 6.775501728057861
Epoch 970, val loss: 1.5828800201416016
Epoch 980, training loss: 0.07086597383022308 = 0.0031525157392024994 + 0.01 * 6.771345615386963
Epoch 980, val loss: 1.5867406129837036
Epoch 990, training loss: 0.07088715583086014 = 0.0030852623749524355 + 0.01 * 6.780189514160156
Epoch 990, val loss: 1.5905253887176514
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 2.014634609222412 = 1.9286662340164185 + 0.01 * 8.596837997436523
Epoch 0, val loss: 1.9243721961975098
Epoch 10, training loss: 2.0052855014801025 = 1.919317603111267 + 0.01 * 8.596785545349121
Epoch 10, val loss: 1.9146744012832642
Epoch 20, training loss: 1.993911623954773 = 1.9079458713531494 + 0.01 * 8.596580505371094
Epoch 20, val loss: 1.9029220342636108
Epoch 30, training loss: 1.9779319763183594 = 1.8919731378555298 + 0.01 * 8.595887184143066
Epoch 30, val loss: 1.8865727186203003
Epoch 40, training loss: 1.9543299674987793 = 1.8684123754501343 + 0.01 * 8.591756820678711
Epoch 40, val loss: 1.862796425819397
Epoch 50, training loss: 1.9218369722366333 = 1.8362160921096802 + 0.01 * 8.562090873718262
Epoch 50, val loss: 1.8318716287612915
Epoch 60, training loss: 1.886202335357666 = 1.80194091796875 + 0.01 * 8.42614459991455
Epoch 60, val loss: 1.8025702238082886
Epoch 70, training loss: 1.8541179895401 = 1.7712011337280273 + 0.01 * 8.291687965393066
Epoch 70, val loss: 1.7781556844711304
Epoch 80, training loss: 1.8109326362609863 = 1.729905128479004 + 0.01 * 8.102752685546875
Epoch 80, val loss: 1.741990089416504
Epoch 90, training loss: 1.750034213066101 = 1.6718592643737793 + 0.01 * 7.817491054534912
Epoch 90, val loss: 1.690434455871582
Epoch 100, training loss: 1.669933557510376 = 1.5939562320709229 + 0.01 * 7.597726821899414
Epoch 100, val loss: 1.6250237226486206
Epoch 110, training loss: 1.5776163339614868 = 1.5026624202728271 + 0.01 * 7.495386600494385
Epoch 110, val loss: 1.549948811531067
Epoch 120, training loss: 1.4825745820999146 = 1.408150315284729 + 0.01 * 7.442424297332764
Epoch 120, val loss: 1.473474383354187
Epoch 130, training loss: 1.3882108926773071 = 1.3141217231750488 + 0.01 * 7.408912658691406
Epoch 130, val loss: 1.3996573686599731
Epoch 140, training loss: 1.2929787635803223 = 1.2191696166992188 + 0.01 * 7.380910396575928
Epoch 140, val loss: 1.3257559537887573
Epoch 150, training loss: 1.1983563899993896 = 1.1247080564498901 + 0.01 * 7.3648362159729
Epoch 150, val loss: 1.2536295652389526
Epoch 160, training loss: 1.106825351715088 = 1.0332226753234863 + 0.01 * 7.360263347625732
Epoch 160, val loss: 1.1848876476287842
Epoch 170, training loss: 1.0198169946670532 = 0.9462210536003113 + 0.01 * 7.359589576721191
Epoch 170, val loss: 1.1212379932403564
Epoch 180, training loss: 0.9371520280838013 = 0.8635665774345398 + 0.01 * 7.358541965484619
Epoch 180, val loss: 1.061550259590149
Epoch 190, training loss: 0.8582947254180908 = 0.7847274541854858 + 0.01 * 7.356724262237549
Epoch 190, val loss: 1.005382776260376
Epoch 200, training loss: 0.783845067024231 = 0.7103122472763062 + 0.01 * 7.3532819747924805
Epoch 200, val loss: 0.9535073637962341
Epoch 210, training loss: 0.7151037454605103 = 0.6416319608688354 + 0.01 * 7.347175598144531
Epoch 210, val loss: 0.9076353907585144
Epoch 220, training loss: 0.6525182723999023 = 0.5791357159614563 + 0.01 * 7.338252544403076
Epoch 220, val loss: 0.8686718344688416
Epoch 230, training loss: 0.5953991413116455 = 0.5221489667892456 + 0.01 * 7.325016498565674
Epoch 230, val loss: 0.8365722894668579
Epoch 240, training loss: 0.5424783229827881 = 0.46940720081329346 + 0.01 * 7.307110786437988
Epoch 240, val loss: 0.8104805946350098
Epoch 250, training loss: 0.49274975061416626 = 0.4198854863643646 + 0.01 * 7.286427021026611
Epoch 250, val loss: 0.7896695733070374
Epoch 260, training loss: 0.44586291909217834 = 0.37331312894821167 + 0.01 * 7.254979133605957
Epoch 260, val loss: 0.7737181782722473
Epoch 270, training loss: 0.4024299681186676 = 0.3300825357437134 + 0.01 * 7.234742641448975
Epoch 270, val loss: 0.7621153593063354
Epoch 280, training loss: 0.36279627680778503 = 0.29070255160331726 + 0.01 * 7.209373474121094
Epoch 280, val loss: 0.7542892694473267
Epoch 290, training loss: 0.32690420746803284 = 0.25510066747665405 + 0.01 * 7.180354595184326
Epoch 290, val loss: 0.7495211958885193
Epoch 300, training loss: 0.2947780191898346 = 0.2229214608669281 + 0.01 * 7.185657024383545
Epoch 300, val loss: 0.7472906708717346
Epoch 310, training loss: 0.2653665840625763 = 0.1939355731010437 + 0.01 * 7.143102169036865
Epoch 310, val loss: 0.7471637725830078
Epoch 320, training loss: 0.23953145742416382 = 0.16806131601333618 + 0.01 * 7.147014141082764
Epoch 320, val loss: 0.7489005327224731
Epoch 330, training loss: 0.21664085984230042 = 0.14536736905574799 + 0.01 * 7.127349853515625
Epoch 330, val loss: 0.752487063407898
Epoch 340, training loss: 0.19690144062042236 = 0.12580227851867676 + 0.01 * 7.109915733337402
Epoch 340, val loss: 0.7577120065689087
Epoch 350, training loss: 0.1801745593547821 = 0.10912881046533585 + 0.01 * 7.1045756340026855
Epoch 350, val loss: 0.7645053863525391
Epoch 360, training loss: 0.1660974621772766 = 0.0950000211596489 + 0.01 * 7.109744071960449
Epoch 360, val loss: 0.7725728750228882
Epoch 370, training loss: 0.15382997691631317 = 0.08307519555091858 + 0.01 * 7.075478553771973
Epoch 370, val loss: 0.7815936207771301
Epoch 380, training loss: 0.1439078450202942 = 0.07297759503126144 + 0.01 * 7.093024253845215
Epoch 380, val loss: 0.7913526296615601
Epoch 390, training loss: 0.13498945534229279 = 0.06441207975149155 + 0.01 * 7.057737350463867
Epoch 390, val loss: 0.8016148805618286
Epoch 400, training loss: 0.12761059403419495 = 0.05711961165070534 + 0.01 * 7.049098968505859
Epoch 400, val loss: 0.81211256980896
Epoch 410, training loss: 0.12158723175525665 = 0.0508953221142292 + 0.01 * 7.0691914558410645
Epoch 410, val loss: 0.8228409886360168
Epoch 420, training loss: 0.11601323634386063 = 0.04557017982006073 + 0.01 * 7.044305801391602
Epoch 420, val loss: 0.8335888981819153
Epoch 430, training loss: 0.11134004592895508 = 0.04098572954535484 + 0.01 * 7.0354323387146
Epoch 430, val loss: 0.844275951385498
Epoch 440, training loss: 0.10741356015205383 = 0.03702801838517189 + 0.01 * 7.038554668426514
Epoch 440, val loss: 0.8548477292060852
Epoch 450, training loss: 0.1037115752696991 = 0.033596377819776535 + 0.01 * 7.011519432067871
Epoch 450, val loss: 0.8652234673500061
Epoch 460, training loss: 0.10091587901115417 = 0.030606208369135857 + 0.01 * 7.0309672355651855
Epoch 460, val loss: 0.8753857612609863
Epoch 470, training loss: 0.09823406487703323 = 0.027990570291876793 + 0.01 * 7.024349212646484
Epoch 470, val loss: 0.8852863311767578
Epoch 480, training loss: 0.0957326889038086 = 0.025696581229567528 + 0.01 * 7.003610610961914
Epoch 480, val loss: 0.8949577212333679
Epoch 490, training loss: 0.09358853846788406 = 0.02366647869348526 + 0.01 * 6.99220609664917
Epoch 490, val loss: 0.9044180512428284
Epoch 500, training loss: 0.09149141609668732 = 0.021867338567972183 + 0.01 * 6.962408542633057
Epoch 500, val loss: 0.913548469543457
Epoch 510, training loss: 0.08993536233901978 = 0.020270079374313354 + 0.01 * 6.966528415679932
Epoch 510, val loss: 0.9223854541778564
Epoch 520, training loss: 0.08844184875488281 = 0.01884341984987259 + 0.01 * 6.959843158721924
Epoch 520, val loss: 0.9310266971588135
Epoch 530, training loss: 0.08711014688014984 = 0.01756318286061287 + 0.01 * 6.954697132110596
Epoch 530, val loss: 0.9394489526748657
Epoch 540, training loss: 0.08614739030599594 = 0.016410619020462036 + 0.01 * 6.973677158355713
Epoch 540, val loss: 0.947670042514801
Epoch 550, training loss: 0.08465392142534256 = 0.01537240855395794 + 0.01 * 6.9281511306762695
Epoch 550, val loss: 0.955551266670227
Epoch 560, training loss: 0.08400814235210419 = 0.014431708492338657 + 0.01 * 6.957643508911133
Epoch 560, val loss: 0.9632962942123413
Epoch 570, training loss: 0.08285029232501984 = 0.01357845589518547 + 0.01 * 6.927183151245117
Epoch 570, val loss: 0.9708638787269592
Epoch 580, training loss: 0.08192337304353714 = 0.012800430878996849 + 0.01 * 6.912293910980225
Epoch 580, val loss: 0.9781761765480042
Epoch 590, training loss: 0.08136828988790512 = 0.012088912539184093 + 0.01 * 6.927938461303711
Epoch 590, val loss: 0.9853032827377319
Epoch 600, training loss: 0.0808236375451088 = 0.011437447741627693 + 0.01 * 6.938619136810303
Epoch 600, val loss: 0.9921777844429016
Epoch 610, training loss: 0.07977750152349472 = 0.010840700939297676 + 0.01 * 6.893680095672607
Epoch 610, val loss: 0.9989398121833801
Epoch 620, training loss: 0.07945805788040161 = 0.010291356593370438 + 0.01 * 6.916670322418213
Epoch 620, val loss: 1.00557279586792
Epoch 630, training loss: 0.07875817269086838 = 0.009785476140677929 + 0.01 * 6.897270202636719
Epoch 630, val loss: 1.0118789672851562
Epoch 640, training loss: 0.07820603251457214 = 0.009318058378994465 + 0.01 * 6.888797283172607
Epoch 640, val loss: 1.0181299448013306
Epoch 650, training loss: 0.07765965163707733 = 0.00888599082827568 + 0.01 * 6.877365589141846
Epoch 650, val loss: 1.0241706371307373
Epoch 660, training loss: 0.07754654437303543 = 0.008485388942062855 + 0.01 * 6.906115531921387
Epoch 660, val loss: 1.0299959182739258
Epoch 670, training loss: 0.07669667154550552 = 0.00811389647424221 + 0.01 * 6.858277797698975
Epoch 670, val loss: 1.0357403755187988
Epoch 680, training loss: 0.07638712227344513 = 0.007768233306705952 + 0.01 * 6.861888885498047
Epoch 680, val loss: 1.0413298606872559
Epoch 690, training loss: 0.07618559151887894 = 0.007446122355759144 + 0.01 * 6.873946666717529
Epoch 690, val loss: 1.0467625856399536
Epoch 700, training loss: 0.07579910755157471 = 0.007145444862544537 + 0.01 * 6.8653669357299805
Epoch 700, val loss: 1.0521043539047241
Epoch 710, training loss: 0.07538576424121857 = 0.0068648625165224075 + 0.01 * 6.852090835571289
Epoch 710, val loss: 1.0572664737701416
Epoch 720, training loss: 0.07486412674188614 = 0.006601897068321705 + 0.01 * 6.826222896575928
Epoch 720, val loss: 1.0622752904891968
Epoch 730, training loss: 0.07483765482902527 = 0.0063551925122737885 + 0.01 * 6.848246097564697
Epoch 730, val loss: 1.0672043561935425
Epoch 740, training loss: 0.07446976751089096 = 0.006123868748545647 + 0.01 * 6.83458948135376
Epoch 740, val loss: 1.0720125436782837
Epoch 750, training loss: 0.07474832981824875 = 0.005906275939196348 + 0.01 * 6.8842058181762695
Epoch 750, val loss: 1.0766396522521973
Epoch 760, training loss: 0.07404324412345886 = 0.005701806861907244 + 0.01 * 6.83414363861084
Epoch 760, val loss: 1.081264615058899
Epoch 770, training loss: 0.07425400614738464 = 0.005508815869688988 + 0.01 * 6.874519348144531
Epoch 770, val loss: 1.0857274532318115
Epoch 780, training loss: 0.0733664408326149 = 0.0053268913179636 + 0.01 * 6.803955078125
Epoch 780, val loss: 1.0900956392288208
Epoch 790, training loss: 0.07331112027168274 = 0.005154910497367382 + 0.01 * 6.815621376037598
Epoch 790, val loss: 1.0943604707717896
Epoch 800, training loss: 0.07317361235618591 = 0.004992355592548847 + 0.01 * 6.8181257247924805
Epoch 800, val loss: 1.0985198020935059
Epoch 810, training loss: 0.07274606078863144 = 0.004838436376303434 + 0.01 * 6.790762901306152
Epoch 810, val loss: 1.1025952100753784
Epoch 820, training loss: 0.07271336019039154 = 0.004692280665040016 + 0.01 * 6.802108287811279
Epoch 820, val loss: 1.1065964698791504
Epoch 830, training loss: 0.07245039194822311 = 0.004553840029984713 + 0.01 * 6.7896552085876465
Epoch 830, val loss: 1.1105328798294067
Epoch 840, training loss: 0.0724949762225151 = 0.004422388970851898 + 0.01 * 6.8072590827941895
Epoch 840, val loss: 1.1143078804016113
Epoch 850, training loss: 0.07232675701379776 = 0.004297431092709303 + 0.01 * 6.802933216094971
Epoch 850, val loss: 1.118029236793518
Epoch 860, training loss: 0.07199452817440033 = 0.004178578965365887 + 0.01 * 6.781595230102539
Epoch 860, val loss: 1.121687412261963
Epoch 870, training loss: 0.07198785245418549 = 0.004065270535647869 + 0.01 * 6.7922587394714355
Epoch 870, val loss: 1.125300407409668
Epoch 880, training loss: 0.07200482487678528 = 0.003957455512136221 + 0.01 * 6.804736614227295
Epoch 880, val loss: 1.128828525543213
Epoch 890, training loss: 0.07175477594137192 = 0.0038545255083590746 + 0.01 * 6.790025234222412
Epoch 890, val loss: 1.132252812385559
Epoch 900, training loss: 0.07157963514328003 = 0.003756414633244276 + 0.01 * 6.782322883605957
Epoch 900, val loss: 1.135634422302246
Epoch 910, training loss: 0.07145275175571442 = 0.003662600414827466 + 0.01 * 6.77901554107666
Epoch 910, val loss: 1.1388663053512573
Epoch 920, training loss: 0.07151816040277481 = 0.003572884015738964 + 0.01 * 6.794528007507324
Epoch 920, val loss: 1.1421340703964233
Epoch 930, training loss: 0.0710148885846138 = 0.0034872214309871197 + 0.01 * 6.752767086029053
Epoch 930, val loss: 1.145362377166748
Epoch 940, training loss: 0.07126817852258682 = 0.003405261319130659 + 0.01 * 6.78629207611084
Epoch 940, val loss: 1.148433804512024
Epoch 950, training loss: 0.07098130136728287 = 0.0033268718980252743 + 0.01 * 6.765443325042725
Epoch 950, val loss: 1.1514551639556885
Epoch 960, training loss: 0.07085011899471283 = 0.003251735121011734 + 0.01 * 6.759839057922363
Epoch 960, val loss: 1.1544227600097656
Epoch 970, training loss: 0.07065224647521973 = 0.003179617691785097 + 0.01 * 6.747263431549072
Epoch 970, val loss: 1.1573609113693237
Epoch 980, training loss: 0.07106412947177887 = 0.0031103806104511023 + 0.01 * 6.795374870300293
Epoch 980, val loss: 1.1601731777191162
Epoch 990, training loss: 0.07052567601203918 = 0.003043930046260357 + 0.01 * 6.748175144195557
Epoch 990, val loss: 1.163085699081421
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 2.030564785003662 = 1.944596290588379 + 0.01 * 8.596839904785156
Epoch 0, val loss: 1.9503343105316162
Epoch 10, training loss: 2.020768165588379 = 1.9348002672195435 + 0.01 * 8.596792221069336
Epoch 10, val loss: 1.9398632049560547
Epoch 20, training loss: 2.0085606575012207 = 1.9225945472717285 + 0.01 * 8.5966157913208
Epoch 20, val loss: 1.9265573024749756
Epoch 30, training loss: 1.9911916255950928 = 1.90523099899292 + 0.01 * 8.596060752868652
Epoch 30, val loss: 1.907572865486145
Epoch 40, training loss: 1.9653949737548828 = 1.8794625997543335 + 0.01 * 8.593235969543457
Epoch 40, val loss: 1.8798730373382568
Epoch 50, training loss: 1.9296414852142334 = 1.8438897132873535 + 0.01 * 8.575175285339355
Epoch 50, val loss: 1.8435723781585693
Epoch 60, training loss: 1.8908722400665283 = 1.8059260845184326 + 0.01 * 8.494621276855469
Epoch 60, val loss: 1.8096033334732056
Epoch 70, training loss: 1.8567243814468384 = 1.7741212844848633 + 0.01 * 8.260313987731934
Epoch 70, val loss: 1.784714937210083
Epoch 80, training loss: 1.814557671546936 = 1.7329163551330566 + 0.01 * 8.16413402557373
Epoch 80, val loss: 1.7498997449874878
Epoch 90, training loss: 1.7550877332687378 = 1.6745684146881104 + 0.01 * 8.051934242248535
Epoch 90, val loss: 1.7000007629394531
Epoch 100, training loss: 1.675359845161438 = 1.5960869789123535 + 0.01 * 7.927289009094238
Epoch 100, val loss: 1.63405442237854
Epoch 110, training loss: 1.5824990272521973 = 1.5050843954086304 + 0.01 * 7.741466045379639
Epoch 110, val loss: 1.5603899955749512
Epoch 120, training loss: 1.4889321327209473 = 1.4134091138839722 + 0.01 * 7.552298545837402
Epoch 120, val loss: 1.4884220361709595
Epoch 130, training loss: 1.3987988233566284 = 1.324084997177124 + 0.01 * 7.471388339996338
Epoch 130, val loss: 1.4206976890563965
Epoch 140, training loss: 1.309138298034668 = 1.2351003885269165 + 0.01 * 7.403796195983887
Epoch 140, val loss: 1.354467511177063
Epoch 150, training loss: 1.220182180404663 = 1.1464006900787354 + 0.01 * 7.378147125244141
Epoch 150, val loss: 1.2884997129440308
Epoch 160, training loss: 1.1337989568710327 = 1.0601211786270142 + 0.01 * 7.367775917053223
Epoch 160, val loss: 1.2232162952423096
Epoch 170, training loss: 1.0516257286071777 = 0.9779942631721497 + 0.01 * 7.3631439208984375
Epoch 170, val loss: 1.1599054336547852
Epoch 180, training loss: 0.973488986492157 = 0.8998681306838989 + 0.01 * 7.362083911895752
Epoch 180, val loss: 1.099524974822998
Epoch 190, training loss: 0.898231029510498 = 0.8246033787727356 + 0.01 * 7.362764835357666
Epoch 190, val loss: 1.0426644086837769
Epoch 200, training loss: 0.825518786907196 = 0.7518776655197144 + 0.01 * 7.36411190032959
Epoch 200, val loss: 0.9898084998130798
Epoch 210, training loss: 0.7564657926559448 = 0.6828159093856812 + 0.01 * 7.364988803863525
Epoch 210, val loss: 0.9429032206535339
Epoch 220, training loss: 0.6930441856384277 = 0.6193959712982178 + 0.01 * 7.3648200035095215
Epoch 220, val loss: 0.9036587476730347
Epoch 230, training loss: 0.6366516947746277 = 0.5630163550376892 + 0.01 * 7.363533020019531
Epoch 230, val loss: 0.8725833892822266
Epoch 240, training loss: 0.5871961712837219 = 0.5135851502418518 + 0.01 * 7.361100196838379
Epoch 240, val loss: 0.8487332463264465
Epoch 250, training loss: 0.5432607531547546 = 0.4696851670742035 + 0.01 * 7.357558250427246
Epoch 250, val loss: 0.8304591178894043
Epoch 260, training loss: 0.5028591752052307 = 0.42931899428367615 + 0.01 * 7.3540167808532715
Epoch 260, val loss: 0.8159751892089844
Epoch 270, training loss: 0.46431684494018555 = 0.3908320367336273 + 0.01 * 7.348481178283691
Epoch 270, val loss: 0.804283857345581
Epoch 280, training loss: 0.4270787835121155 = 0.35364145040512085 + 0.01 * 7.343735218048096
Epoch 280, val loss: 0.795134961605072
Epoch 290, training loss: 0.39186495542526245 = 0.3184770941734314 + 0.01 * 7.338787078857422
Epoch 290, val loss: 0.7890903353691101
Epoch 300, training loss: 0.3600613474845886 = 0.2866915166378021 + 0.01 * 7.336982250213623
Epoch 300, val loss: 0.7866700887680054
Epoch 310, training loss: 0.33230531215667725 = 0.25899720191955566 + 0.01 * 7.330810070037842
Epoch 310, val loss: 0.7878799438476562
Epoch 320, training loss: 0.3083389401435852 = 0.2350732982158661 + 0.01 * 7.326564311981201
Epoch 320, val loss: 0.7921470403671265
Epoch 330, training loss: 0.28714028000831604 = 0.21393227577209473 + 0.01 * 7.320801258087158
Epoch 330, val loss: 0.7986797094345093
Epoch 340, training loss: 0.2676261067390442 = 0.1944049745798111 + 0.01 * 7.322113990783691
Epoch 340, val loss: 0.8067169785499573
Epoch 350, training loss: 0.24858206510543823 = 0.1754809468984604 + 0.01 * 7.310111999511719
Epoch 350, val loss: 0.8156733512878418
Epoch 360, training loss: 0.22951875627040863 = 0.15653154253959656 + 0.01 * 7.2987213134765625
Epoch 360, val loss: 0.8252254128456116
Epoch 370, training loss: 0.21046623587608337 = 0.13755744695663452 + 0.01 * 7.290879726409912
Epoch 370, val loss: 0.8353652358055115
Epoch 380, training loss: 0.19197601079940796 = 0.11920292675495148 + 0.01 * 7.277308940887451
Epoch 380, val loss: 0.8463650345802307
Epoch 390, training loss: 0.17486116290092468 = 0.1023118868470192 + 0.01 * 7.254926681518555
Epoch 390, val loss: 0.8585215210914612
Epoch 400, training loss: 0.159744992852211 = 0.08750996738672256 + 0.01 * 7.2235026359558105
Epoch 400, val loss: 0.8717764019966125
Epoch 410, training loss: 0.14699804782867432 = 0.07499559223651886 + 0.01 * 7.2002458572387695
Epoch 410, val loss: 0.8858319520950317
Epoch 420, training loss: 0.13632047176361084 = 0.06458688527345657 + 0.01 * 7.173357963562012
Epoch 420, val loss: 0.9002051949501038
Epoch 430, training loss: 0.1274668276309967 = 0.05598068982362747 + 0.01 * 7.148613452911377
Epoch 430, val loss: 0.9145397543907166
Epoch 440, training loss: 0.12036623060703278 = 0.048854876309633255 + 0.01 * 7.151134967803955
Epoch 440, val loss: 0.9288302659988403
Epoch 450, training loss: 0.11427938938140869 = 0.04293697699904442 + 0.01 * 7.134241104125977
Epoch 450, val loss: 0.9427577257156372
Epoch 460, training loss: 0.10918782651424408 = 0.03798089548945427 + 0.01 * 7.120693206787109
Epoch 460, val loss: 0.9564717411994934
Epoch 470, training loss: 0.10464257001876831 = 0.03381384536623955 + 0.01 * 7.082873344421387
Epoch 470, val loss: 0.969795286655426
Epoch 480, training loss: 0.10102132707834244 = 0.03027566522359848 + 0.01 * 7.07456636428833
Epoch 480, val loss: 0.9827598333358765
Epoch 490, training loss: 0.09784504771232605 = 0.027257075533270836 + 0.01 * 7.0587968826293945
Epoch 490, val loss: 0.9953119158744812
Epoch 500, training loss: 0.09503470361232758 = 0.024665016680955887 + 0.01 * 7.036969184875488
Epoch 500, val loss: 1.0074175596237183
Epoch 510, training loss: 0.09263430535793304 = 0.022429153323173523 + 0.01 * 7.020515441894531
Epoch 510, val loss: 1.019212007522583
Epoch 520, training loss: 0.09056124836206436 = 0.020490234717726707 + 0.01 * 7.007101058959961
Epoch 520, val loss: 1.0305284261703491
Epoch 530, training loss: 0.08895818889141083 = 0.018796538934111595 + 0.01 * 7.016165256500244
Epoch 530, val loss: 1.0415236949920654
Epoch 540, training loss: 0.0873165875673294 = 0.01731291227042675 + 0.01 * 7.000367164611816
Epoch 540, val loss: 1.0522080659866333
Epoch 550, training loss: 0.08583517372608185 = 0.016005253419280052 + 0.01 * 6.982992172241211
Epoch 550, val loss: 1.0625052452087402
Epoch 560, training loss: 0.08458684384822845 = 0.01484828069806099 + 0.01 * 6.9738569259643555
Epoch 560, val loss: 1.0724824666976929
Epoch 570, training loss: 0.0835220068693161 = 0.013822069391608238 + 0.01 * 6.969994068145752
Epoch 570, val loss: 1.0821421146392822
Epoch 580, training loss: 0.08255556970834732 = 0.012906412594020367 + 0.01 * 6.964916229248047
Epoch 580, val loss: 1.0914781093597412
Epoch 590, training loss: 0.08157733827829361 = 0.012086109258234501 + 0.01 * 6.949122905731201
Epoch 590, val loss: 1.1005005836486816
Epoch 600, training loss: 0.08061031252145767 = 0.01134799886494875 + 0.01 * 6.926231384277344
Epoch 600, val loss: 1.1092448234558105
Epoch 610, training loss: 0.08002544939517975 = 0.010681881569325924 + 0.01 * 6.934356689453125
Epoch 610, val loss: 1.1177310943603516
Epoch 620, training loss: 0.07938659936189651 = 0.010078629478812218 + 0.01 * 6.9307966232299805
Epoch 620, val loss: 1.1259609460830688
Epoch 630, training loss: 0.07864033430814743 = 0.009530737064778805 + 0.01 * 6.910959720611572
Epoch 630, val loss: 1.1339260339736938
Epoch 640, training loss: 0.07814781367778778 = 0.009031586349010468 + 0.01 * 6.911623001098633
Epoch 640, val loss: 1.1416566371917725
Epoch 650, training loss: 0.07777485996484756 = 0.00857466273009777 + 0.01 * 6.920019626617432
Epoch 650, val loss: 1.1491694450378418
Epoch 660, training loss: 0.07723569124937057 = 0.008155958727002144 + 0.01 * 6.907973766326904
Epoch 660, val loss: 1.1564667224884033
Epoch 670, training loss: 0.07680448144674301 = 0.0077715422958135605 + 0.01 * 6.903294086456299
Epoch 670, val loss: 1.1635288000106812
Epoch 680, training loss: 0.07621170580387115 = 0.0074171824380755424 + 0.01 * 6.879452705383301
Epoch 680, val loss: 1.1704007387161255
Epoch 690, training loss: 0.07606913894414902 = 0.007089302409440279 + 0.01 * 6.897983551025391
Epoch 690, val loss: 1.1770790815353394
Epoch 700, training loss: 0.07561914622783661 = 0.006786065641790628 + 0.01 * 6.883308410644531
Epoch 700, val loss: 1.1836203336715698
Epoch 710, training loss: 0.075160913169384 = 0.00650444719940424 + 0.01 * 6.865646839141846
Epoch 710, val loss: 1.1898845434188843
Epoch 720, training loss: 0.07483166456222534 = 0.006242292933166027 + 0.01 * 6.8589372634887695
Epoch 720, val loss: 1.1960272789001465
Epoch 730, training loss: 0.07477467507123947 = 0.005997895263135433 + 0.01 * 6.877677917480469
Epoch 730, val loss: 1.2020426988601685
Epoch 740, training loss: 0.07436822354793549 = 0.005770175717771053 + 0.01 * 6.859804630279541
Epoch 740, val loss: 1.2078558206558228
Epoch 750, training loss: 0.07396574318408966 = 0.005557200871407986 + 0.01 * 6.840854644775391
Epoch 750, val loss: 1.2135130167007446
Epoch 760, training loss: 0.0737152025103569 = 0.005357550922781229 + 0.01 * 6.835765838623047
Epoch 760, val loss: 1.2190004587173462
Epoch 770, training loss: 0.07380131632089615 = 0.0051699792966246605 + 0.01 * 6.863133907318115
Epoch 770, val loss: 1.22438645362854
Epoch 780, training loss: 0.07344208657741547 = 0.004994035232812166 + 0.01 * 6.8448052406311035
Epoch 780, val loss: 1.2295883893966675
Epoch 790, training loss: 0.07306143641471863 = 0.004828514996916056 + 0.01 * 6.8232927322387695
Epoch 790, val loss: 1.2346882820129395
Epoch 800, training loss: 0.07309670746326447 = 0.004672178998589516 + 0.01 * 6.8424530029296875
Epoch 800, val loss: 1.2396457195281982
Epoch 810, training loss: 0.07292242348194122 = 0.004524984396994114 + 0.01 * 6.8397440910339355
Epoch 810, val loss: 1.244489073753357
Epoch 820, training loss: 0.07255436480045319 = 0.004385754000395536 + 0.01 * 6.816861152648926
Epoch 820, val loss: 1.2492179870605469
Epoch 830, training loss: 0.07248125225305557 = 0.004254116211086512 + 0.01 * 6.822713375091553
Epoch 830, val loss: 1.2538375854492188
Epoch 840, training loss: 0.07221493124961853 = 0.004129346460103989 + 0.01 * 6.808558940887451
Epoch 840, val loss: 1.258310079574585
Epoch 850, training loss: 0.07214044034481049 = 0.004010935313999653 + 0.01 * 6.812950134277344
Epoch 850, val loss: 1.2627516984939575
Epoch 860, training loss: 0.07194361090660095 = 0.0038987770676612854 + 0.01 * 6.804483890533447
Epoch 860, val loss: 1.267031192779541
Epoch 870, training loss: 0.07174713909626007 = 0.0037921105977147818 + 0.01 * 6.79550313949585
Epoch 870, val loss: 1.271223783493042
Epoch 880, training loss: 0.07186418771743774 = 0.003690779907628894 + 0.01 * 6.817341327667236
Epoch 880, val loss: 1.2753440141677856
Epoch 890, training loss: 0.07149233669042587 = 0.0035942813847213984 + 0.01 * 6.789805889129639
Epoch 890, val loss: 1.2793185710906982
Epoch 900, training loss: 0.0712871253490448 = 0.0035022071097046137 + 0.01 * 6.778491973876953
Epoch 900, val loss: 1.2832555770874023
Epoch 910, training loss: 0.07115153968334198 = 0.0034144590608775616 + 0.01 * 6.773707866668701
Epoch 910, val loss: 1.2870858907699585
Epoch 920, training loss: 0.07104681432247162 = 0.003330701030790806 + 0.01 * 6.771611213684082
Epoch 920, val loss: 1.290856957435608
Epoch 930, training loss: 0.07092759013175964 = 0.0032508515287190676 + 0.01 * 6.767673969268799
Epoch 930, val loss: 1.294524073600769
Epoch 940, training loss: 0.07108169049024582 = 0.003174416720867157 + 0.01 * 6.790727615356445
Epoch 940, val loss: 1.2981476783752441
Epoch 950, training loss: 0.07085384428501129 = 0.003101389156654477 + 0.01 * 6.7752461433410645
Epoch 950, val loss: 1.3016570806503296
Epoch 960, training loss: 0.07076683640480042 = 0.0030315022449940443 + 0.01 * 6.773533821105957
Epoch 960, val loss: 1.305107831954956
Epoch 970, training loss: 0.07058921456336975 = 0.0029645543545484543 + 0.01 * 6.762465953826904
Epoch 970, val loss: 1.3085023164749146
Epoch 980, training loss: 0.07054994255304337 = 0.002900376683101058 + 0.01 * 6.764956951141357
Epoch 980, val loss: 1.3117954730987549
Epoch 990, training loss: 0.07077328860759735 = 0.0028387794736772776 + 0.01 * 6.793450355529785
Epoch 990, val loss: 1.3149824142456055
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8207696362677913
The final CL Acc:0.76296, 0.02400, The final GNN Acc:0.81638, 0.00366
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13248])
remove edge: torch.Size([2, 7830])
updated graph: torch.Size([2, 10522])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.029693126678467 = 1.943724274635315 + 0.01 * 8.596879959106445
Epoch 0, val loss: 1.9428396224975586
Epoch 10, training loss: 2.0196030139923096 = 1.933634638786316 + 0.01 * 8.596835136413574
Epoch 10, val loss: 1.9323924779891968
Epoch 20, training loss: 2.007145643234253 = 1.921178936958313 + 0.01 * 8.596670150756836
Epoch 20, val loss: 1.9193894863128662
Epoch 30, training loss: 1.9898152351379395 = 1.9038538932800293 + 0.01 * 8.59613037109375
Epoch 30, val loss: 1.9013253450393677
Epoch 40, training loss: 1.9645864963531494 = 1.8786530494689941 + 0.01 * 8.593338966369629
Epoch 40, val loss: 1.8754152059555054
Epoch 50, training loss: 1.9293696880340576 = 1.843631386756897 + 0.01 * 8.57382583618164
Epoch 50, val loss: 1.840863585472107
Epoch 60, training loss: 1.8874852657318115 = 1.802707552909851 + 0.01 * 8.477766036987305
Epoch 60, val loss: 1.8037134408950806
Epoch 70, training loss: 1.8468554019927979 = 1.7643977403640747 + 0.01 * 8.245768547058105
Epoch 70, val loss: 1.7716186046600342
Epoch 80, training loss: 1.8002170324325562 = 1.718963861465454 + 0.01 * 8.125321388244629
Epoch 80, val loss: 1.7309644222259521
Epoch 90, training loss: 1.7344167232513428 = 1.6551659107208252 + 0.01 * 7.925079822540283
Epoch 90, val loss: 1.6722956895828247
Epoch 100, training loss: 1.647676944732666 = 1.5704729557037354 + 0.01 * 7.720405101776123
Epoch 100, val loss: 1.5969597101211548
Epoch 110, training loss: 1.544655203819275 = 1.469356656074524 + 0.01 * 7.529860496520996
Epoch 110, val loss: 1.5108195543289185
Epoch 120, training loss: 1.4368114471435547 = 1.3621017932891846 + 0.01 * 7.470961093902588
Epoch 120, val loss: 1.420708417892456
Epoch 130, training loss: 1.3278470039367676 = 1.2536181211471558 + 0.01 * 7.422887325286865
Epoch 130, val loss: 1.3324429988861084
Epoch 140, training loss: 1.2202682495117188 = 1.1463444232940674 + 0.01 * 7.3923797607421875
Epoch 140, val loss: 1.2471098899841309
Epoch 150, training loss: 1.1173051595687866 = 1.0436959266662598 + 0.01 * 7.360921382904053
Epoch 150, val loss: 1.1668845415115356
Epoch 160, training loss: 1.0214288234710693 = 0.9480448961257935 + 0.01 * 7.338397026062012
Epoch 160, val loss: 1.0933103561401367
Epoch 170, training loss: 0.9334704279899597 = 0.8601996302604675 + 0.01 * 7.32708215713501
Epoch 170, val loss: 1.0263842344284058
Epoch 180, training loss: 0.8526166081428528 = 0.779437780380249 + 0.01 * 7.3178815841674805
Epoch 180, val loss: 0.9659765362739563
Epoch 190, training loss: 0.7782179117202759 = 0.7051399350166321 + 0.01 * 7.307798862457275
Epoch 190, val loss: 0.9119051098823547
Epoch 200, training loss: 0.7099807262420654 = 0.6369902491569519 + 0.01 * 7.299045562744141
Epoch 200, val loss: 0.8648926615715027
Epoch 210, training loss: 0.6473145484924316 = 0.5744175314903259 + 0.01 * 7.289703369140625
Epoch 210, val loss: 0.8254997134208679
Epoch 220, training loss: 0.5896055698394775 = 0.5168028473854065 + 0.01 * 7.280270576477051
Epoch 220, val loss: 0.7935243844985962
Epoch 230, training loss: 0.5364859700202942 = 0.4637819528579712 + 0.01 * 7.270400047302246
Epoch 230, val loss: 0.7684940099716187
Epoch 240, training loss: 0.488211989402771 = 0.4156286418437958 + 0.01 * 7.258336544036865
Epoch 240, val loss: 0.7499454021453857
Epoch 250, training loss: 0.44550490379333496 = 0.37303683161735535 + 0.01 * 7.2468061447143555
Epoch 250, val loss: 0.7375130653381348
Epoch 260, training loss: 0.4086725115776062 = 0.3363408148288727 + 0.01 * 7.233170509338379
Epoch 260, val loss: 0.730600893497467
Epoch 270, training loss: 0.3771108388900757 = 0.30494365096092224 + 0.01 * 7.216720104217529
Epoch 270, val loss: 0.7281466722488403
Epoch 280, training loss: 0.3496488034725189 = 0.27757036685943604 + 0.01 * 7.20784330368042
Epoch 280, val loss: 0.7286933660507202
Epoch 290, training loss: 0.3246455788612366 = 0.2527584433555603 + 0.01 * 7.188712120056152
Epoch 290, val loss: 0.7308351397514343
Epoch 300, training loss: 0.301041841506958 = 0.229237899184227 + 0.01 * 7.180394172668457
Epoch 300, val loss: 0.7337632179260254
Epoch 310, training loss: 0.2780013382434845 = 0.20625022053718567 + 0.01 * 7.175112724304199
Epoch 310, val loss: 0.7368874549865723
Epoch 320, training loss: 0.25545573234558105 = 0.18376797437667847 + 0.01 * 7.1687750816345215
Epoch 320, val loss: 0.7401970624923706
Epoch 330, training loss: 0.23400017619132996 = 0.16238068044185638 + 0.01 * 7.161950588226318
Epoch 330, val loss: 0.7440077066421509
Epoch 340, training loss: 0.2144058346748352 = 0.14286494255065918 + 0.01 * 7.154088973999023
Epoch 340, val loss: 0.7487264275550842
Epoch 350, training loss: 0.19714614748954773 = 0.12565825879573822 + 0.01 * 7.1487884521484375
Epoch 350, val loss: 0.7545916438102722
Epoch 360, training loss: 0.18225519359111786 = 0.11078286170959473 + 0.01 * 7.147233009338379
Epoch 360, val loss: 0.7616878151893616
Epoch 370, training loss: 0.16940540075302124 = 0.0979972556233406 + 0.01 * 7.140813827514648
Epoch 370, val loss: 0.7700767517089844
Epoch 380, training loss: 0.15838339924812317 = 0.08702217042446136 + 0.01 * 7.136122703552246
Epoch 380, val loss: 0.7796974778175354
Epoch 390, training loss: 0.14891108870506287 = 0.07758460938930511 + 0.01 * 7.1326470375061035
Epoch 390, val loss: 0.7902761697769165
Epoch 400, training loss: 0.14070716500282288 = 0.06943347305059433 + 0.01 * 7.1273698806762695
Epoch 400, val loss: 0.8016166090965271
Epoch 410, training loss: 0.1336357593536377 = 0.062360987067222595 + 0.01 * 7.127478122711182
Epoch 410, val loss: 0.8134955763816833
Epoch 420, training loss: 0.12745724618434906 = 0.05620919540524483 + 0.01 * 7.124804973602295
Epoch 420, val loss: 0.8257384300231934
Epoch 430, training loss: 0.12194696068763733 = 0.05083116143941879 + 0.01 * 7.1115803718566895
Epoch 430, val loss: 0.8381054997444153
Epoch 440, training loss: 0.11713643372058868 = 0.04609982669353485 + 0.01 * 7.103661060333252
Epoch 440, val loss: 0.8506778478622437
Epoch 450, training loss: 0.11296749114990234 = 0.041918154805898666 + 0.01 * 7.104933261871338
Epoch 450, val loss: 0.8633826375007629
Epoch 460, training loss: 0.10921327769756317 = 0.03821459412574768 + 0.01 * 7.099868297576904
Epoch 460, val loss: 0.8761174082756042
Epoch 470, training loss: 0.10576649010181427 = 0.03492606803774834 + 0.01 * 7.084042072296143
Epoch 470, val loss: 0.8888212442398071
Epoch 480, training loss: 0.10273037850856781 = 0.03199802711606026 + 0.01 * 7.073235034942627
Epoch 480, val loss: 0.9014779925346375
Epoch 490, training loss: 0.10024803131818771 = 0.029379231855273247 + 0.01 * 7.086880207061768
Epoch 490, val loss: 0.9141297340393066
Epoch 500, training loss: 0.09770077466964722 = 0.027034718543291092 + 0.01 * 7.066605091094971
Epoch 500, val loss: 0.9266663789749146
Epoch 510, training loss: 0.0955340713262558 = 0.02492903731763363 + 0.01 * 7.0605034828186035
Epoch 510, val loss: 0.9390953183174133
Epoch 520, training loss: 0.09344267845153809 = 0.023035435006022453 + 0.01 * 7.040724277496338
Epoch 520, val loss: 0.951487123966217
Epoch 530, training loss: 0.09169761091470718 = 0.021329326555132866 + 0.01 * 7.036828517913818
Epoch 530, val loss: 0.9636847376823425
Epoch 540, training loss: 0.09011149406433105 = 0.019791563972830772 + 0.01 * 7.0319929122924805
Epoch 540, val loss: 0.9757890701293945
Epoch 550, training loss: 0.08864227682352066 = 0.018401240929961205 + 0.01 * 7.02410364151001
Epoch 550, val loss: 0.9876938462257385
Epoch 560, training loss: 0.08722921460866928 = 0.01714302785694599 + 0.01 * 7.0086188316345215
Epoch 560, val loss: 0.9994288682937622
Epoch 570, training loss: 0.0861976370215416 = 0.016003360971808434 + 0.01 * 7.019427299499512
Epoch 570, val loss: 1.0108857154846191
Epoch 580, training loss: 0.08496741205453873 = 0.014971209689974785 + 0.01 * 6.999619960784912
Epoch 580, val loss: 1.0220947265625
Epoch 590, training loss: 0.08410564810037613 = 0.014032595790922642 + 0.01 * 7.007305145263672
Epoch 590, val loss: 1.0331028699874878
Epoch 600, training loss: 0.08314535021781921 = 0.013178554363548756 + 0.01 * 6.996679306030273
Epoch 600, val loss: 1.0437345504760742
Epoch 610, training loss: 0.08232391625642776 = 0.012399381026625633 + 0.01 * 6.992453575134277
Epoch 610, val loss: 1.054214596748352
Epoch 620, training loss: 0.0814056545495987 = 0.01168646290898323 + 0.01 * 6.971919536590576
Epoch 620, val loss: 1.064391016960144
Epoch 630, training loss: 0.08064813166856766 = 0.01103293988853693 + 0.01 * 6.961519241333008
Epoch 630, val loss: 1.074362874031067
Epoch 640, training loss: 0.08010392636060715 = 0.01043267734348774 + 0.01 * 6.967125415802002
Epoch 640, val loss: 1.084133505821228
Epoch 650, training loss: 0.07974105328321457 = 0.009880781173706055 + 0.01 * 6.986027240753174
Epoch 650, val loss: 1.0935695171356201
Epoch 660, training loss: 0.07889268547296524 = 0.009373440407216549 + 0.01 * 6.951924800872803
Epoch 660, val loss: 1.1028060913085938
Epoch 670, training loss: 0.07837720215320587 = 0.008904942311346531 + 0.01 * 6.947226524353027
Epoch 670, val loss: 1.111736536026001
Epoch 680, training loss: 0.07789832353591919 = 0.008471612818539143 + 0.01 * 6.942671298980713
Epoch 680, val loss: 1.1205453872680664
Epoch 690, training loss: 0.07737047225236893 = 0.008070254698395729 + 0.01 * 6.930022239685059
Epoch 690, val loss: 1.1290338039398193
Epoch 700, training loss: 0.07701746374368668 = 0.007698885165154934 + 0.01 * 6.931858062744141
Epoch 700, val loss: 1.1373099088668823
Epoch 710, training loss: 0.07651491463184357 = 0.0073542059399187565 + 0.01 * 6.916070938110352
Epoch 710, val loss: 1.1453274488449097
Epoch 720, training loss: 0.07619836926460266 = 0.007033322472125292 + 0.01 * 6.916504383087158
Epoch 720, val loss: 1.153153896331787
Epoch 730, training loss: 0.0759103000164032 = 0.00673481822013855 + 0.01 * 6.917548656463623
Epoch 730, val loss: 1.1608010530471802
Epoch 740, training loss: 0.07552120089530945 = 0.006456261966377497 + 0.01 * 6.906494140625
Epoch 740, val loss: 1.168164849281311
Epoch 750, training loss: 0.07526549696922302 = 0.006196167320013046 + 0.01 * 6.906932830810547
Epoch 750, val loss: 1.1753723621368408
Epoch 760, training loss: 0.07484107464551926 = 0.005952584557235241 + 0.01 * 6.888849258422852
Epoch 760, val loss: 1.1823973655700684
Epoch 770, training loss: 0.07455513626337051 = 0.00572448642924428 + 0.01 * 6.8830647468566895
Epoch 770, val loss: 1.1892595291137695
Epoch 780, training loss: 0.07426050305366516 = 0.005510547664016485 + 0.01 * 6.874996185302734
Epoch 780, val loss: 1.195892572402954
Epoch 790, training loss: 0.0739784985780716 = 0.005309658590704203 + 0.01 * 6.866884231567383
Epoch 790, val loss: 1.202414631843567
Epoch 800, training loss: 0.07387283444404602 = 0.005120991729199886 + 0.01 * 6.875184535980225
Epoch 800, val loss: 1.2086454629898071
Epoch 810, training loss: 0.0736067146062851 = 0.004943411331623793 + 0.01 * 6.866330623626709
Epoch 810, val loss: 1.2147879600524902
Epoch 820, training loss: 0.0733390673995018 = 0.0047763618640601635 + 0.01 * 6.856270790100098
Epoch 820, val loss: 1.220738172531128
Epoch 830, training loss: 0.07325295358896255 = 0.004618771839886904 + 0.01 * 6.863418102264404
Epoch 830, val loss: 1.2265349626541138
Epoch 840, training loss: 0.07294908910989761 = 0.004469925072044134 + 0.01 * 6.847916603088379
Epoch 840, val loss: 1.2321189641952515
Epoch 850, training loss: 0.07266629487276077 = 0.004329213872551918 + 0.01 * 6.833708763122559
Epoch 850, val loss: 1.2376362085342407
Epoch 860, training loss: 0.07276972383260727 = 0.00419587641954422 + 0.01 * 6.857385158538818
Epoch 860, val loss: 1.2430180311203003
Epoch 870, training loss: 0.0724983885884285 = 0.004069516900926828 + 0.01 * 6.8428874015808105
Epoch 870, val loss: 1.2481814622879028
Epoch 880, training loss: 0.07235895097255707 = 0.003949797712266445 + 0.01 * 6.840915203094482
Epoch 880, val loss: 1.2533268928527832
Epoch 890, training loss: 0.07207883149385452 = 0.003836098127067089 + 0.01 * 6.824273586273193
Epoch 890, val loss: 1.2582234144210815
Epoch 900, training loss: 0.07207287847995758 = 0.0037281932309269905 + 0.01 * 6.834468364715576
Epoch 900, val loss: 1.263065218925476
Epoch 910, training loss: 0.07174409180879593 = 0.0036256916355341673 + 0.01 * 6.811840534210205
Epoch 910, val loss: 1.2677744626998901
Epoch 920, training loss: 0.0720682144165039 = 0.003528126049786806 + 0.01 * 6.854008674621582
Epoch 920, val loss: 1.2723639011383057
Epoch 930, training loss: 0.07163473218679428 = 0.003435318823903799 + 0.01 * 6.819941520690918
Epoch 930, val loss: 1.2768217325210571
Epoch 940, training loss: 0.07148490101099014 = 0.003346869023516774 + 0.01 * 6.813803672790527
Epoch 940, val loss: 1.2811863422393799
Epoch 950, training loss: 0.07144442200660706 = 0.0032624376472085714 + 0.01 * 6.818198204040527
Epoch 950, val loss: 1.2854293584823608
Epoch 960, training loss: 0.07133281230926514 = 0.003181933891028166 + 0.01 * 6.815088272094727
Epoch 960, val loss: 1.2896008491516113
Epoch 970, training loss: 0.07112614065408707 = 0.0031049030367285013 + 0.01 * 6.8021240234375
Epoch 970, val loss: 1.2936948537826538
Epoch 980, training loss: 0.0710560753941536 = 0.0030312296003103256 + 0.01 * 6.802484512329102
Epoch 980, val loss: 1.2976505756378174
Epoch 990, training loss: 0.0708954855799675 = 0.0029607187025249004 + 0.01 * 6.793476581573486
Epoch 990, val loss: 1.3014971017837524
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.037294864654541 = 1.9513262510299683 + 0.01 * 8.596855163574219
Epoch 0, val loss: 1.9553321599960327
Epoch 10, training loss: 2.026554584503174 = 1.9405864477157593 + 0.01 * 8.596811294555664
Epoch 10, val loss: 1.944560170173645
Epoch 20, training loss: 2.0133492946624756 = 1.9273830652236938 + 0.01 * 8.59662914276123
Epoch 20, val loss: 1.9311954975128174
Epoch 30, training loss: 1.9947588443756104 = 1.9087985754013062 + 0.01 * 8.59603214263916
Epoch 30, val loss: 1.9125293493270874
Epoch 40, training loss: 1.9671251773834229 = 1.881197214126587 + 0.01 * 8.592795372009277
Epoch 40, val loss: 1.8855185508728027
Epoch 50, training loss: 1.928229808807373 = 1.8425164222717285 + 0.01 * 8.571338653564453
Epoch 50, val loss: 1.8499276638031006
Epoch 60, training loss: 1.8844785690307617 = 1.7997088432312012 + 0.01 * 8.47697639465332
Epoch 60, val loss: 1.8146367073059082
Epoch 70, training loss: 1.8454210758209229 = 1.7622885704040527 + 0.01 * 8.313254356384277
Epoch 70, val loss: 1.7832255363464355
Epoch 80, training loss: 1.7961640357971191 = 1.7143282890319824 + 0.01 * 8.183573722839355
Epoch 80, val loss: 1.737747311592102
Epoch 90, training loss: 1.7274668216705322 = 1.6472132205963135 + 0.01 * 8.025364875793457
Epoch 90, val loss: 1.679510474205017
Epoch 100, training loss: 1.6423158645629883 = 1.5628576278686523 + 0.01 * 7.9458208084106445
Epoch 100, val loss: 1.611807107925415
Epoch 110, training loss: 1.5543925762176514 = 1.4758481979370117 + 0.01 * 7.854438781738281
Epoch 110, val loss: 1.542740821838379
Epoch 120, training loss: 1.4748324155807495 = 1.3989707231521606 + 0.01 * 7.586172580718994
Epoch 120, val loss: 1.485494613647461
Epoch 130, training loss: 1.405019760131836 = 1.3305224180221558 + 0.01 * 7.449732303619385
Epoch 130, val loss: 1.4348304271697998
Epoch 140, training loss: 1.3382208347320557 = 1.2646071910858154 + 0.01 * 7.361369609832764
Epoch 140, val loss: 1.3868067264556885
Epoch 150, training loss: 1.2706396579742432 = 1.197573184967041 + 0.01 * 7.306644916534424
Epoch 150, val loss: 1.3369373083114624
Epoch 160, training loss: 1.2015799283981323 = 1.1287974119186401 + 0.01 * 7.2782464027404785
Epoch 160, val loss: 1.285291075706482
Epoch 170, training loss: 1.1317001581192017 = 1.059064269065857 + 0.01 * 7.263585090637207
Epoch 170, val loss: 1.232649803161621
Epoch 180, training loss: 1.0621691942214966 = 0.9896023869514465 + 0.01 * 7.256678581237793
Epoch 180, val loss: 1.1796326637268066
Epoch 190, training loss: 0.9935060739517212 = 0.9210010766983032 + 0.01 * 7.2505011558532715
Epoch 190, val loss: 1.1269556283950806
Epoch 200, training loss: 0.9244248867034912 = 0.8519827723503113 + 0.01 * 7.2442097663879395
Epoch 200, val loss: 1.0736486911773682
Epoch 210, training loss: 0.8525718450546265 = 0.7801982164382935 + 0.01 * 7.237362861633301
Epoch 210, val loss: 1.0178178548812866
Epoch 220, training loss: 0.7769175171852112 = 0.7046278119087219 + 0.01 * 7.228968620300293
Epoch 220, val loss: 0.9591453075408936
Epoch 230, training loss: 0.699333667755127 = 0.6271515488624573 + 0.01 * 7.218214511871338
Epoch 230, val loss: 0.9002775549888611
Epoch 240, training loss: 0.6240767240524292 = 0.5520241856575012 + 0.01 * 7.205256938934326
Epoch 240, val loss: 0.8456734418869019
Epoch 250, training loss: 0.5548663139343262 = 0.48295387625694275 + 0.01 * 7.191246032714844
Epoch 250, val loss: 0.799308717250824
Epoch 260, training loss: 0.4932284951210022 = 0.4214552044868469 + 0.01 * 7.177327632904053
Epoch 260, val loss: 0.7626531720161438
Epoch 270, training loss: 0.43886280059814453 = 0.3672402799129486 + 0.01 * 7.162253379821777
Epoch 270, val loss: 0.7349651455879211
Epoch 280, training loss: 0.39082181453704834 = 0.3193686604499817 + 0.01 * 7.145315170288086
Epoch 280, val loss: 0.7144142389297485
Epoch 290, training loss: 0.3482521176338196 = 0.2769230008125305 + 0.01 * 7.132910251617432
Epoch 290, val loss: 0.6995794177055359
Epoch 300, training loss: 0.310545951128006 = 0.23927783966064453 + 0.01 * 7.126811504364014
Epoch 300, val loss: 0.6891250014305115
Epoch 310, training loss: 0.27720576524734497 = 0.2060645967721939 + 0.01 * 7.114117622375488
Epoch 310, val loss: 0.6823501586914062
Epoch 320, training loss: 0.24809885025024414 = 0.17706868052482605 + 0.01 * 7.103017807006836
Epoch 320, val loss: 0.6788078546524048
Epoch 330, training loss: 0.22301393747329712 = 0.152043417096138 + 0.01 * 7.097051620483398
Epoch 330, val loss: 0.6781789660453796
Epoch 340, training loss: 0.20157334208488464 = 0.13066044449806213 + 0.01 * 7.0912909507751465
Epoch 340, val loss: 0.6801401376724243
Epoch 350, training loss: 0.18340608477592468 = 0.11253231763839722 + 0.01 * 7.087377071380615
Epoch 350, val loss: 0.6843008995056152
Epoch 360, training loss: 0.16801273822784424 = 0.09723782539367676 + 0.01 * 7.077491760253906
Epoch 360, val loss: 0.6902570128440857
Epoch 370, training loss: 0.15507379174232483 = 0.08436651527881622 + 0.01 * 7.070727825164795
Epoch 370, val loss: 0.6976348161697388
Epoch 380, training loss: 0.14420509338378906 = 0.07353828847408295 + 0.01 * 7.066679954528809
Epoch 380, val loss: 0.7060865759849548
Epoch 390, training loss: 0.13504207134246826 = 0.0644245371222496 + 0.01 * 7.061752796173096
Epoch 390, val loss: 0.7153164148330688
Epoch 400, training loss: 0.12732459604740143 = 0.05673642084002495 + 0.01 * 7.058817386627197
Epoch 400, val loss: 0.7251249551773071
Epoch 410, training loss: 0.12074372172355652 = 0.05022246763110161 + 0.01 * 7.052125453948975
Epoch 410, val loss: 0.7352677583694458
Epoch 420, training loss: 0.11534668505191803 = 0.044673237949609756 + 0.01 * 7.067345142364502
Epoch 420, val loss: 0.7455997467041016
Epoch 430, training loss: 0.1104123592376709 = 0.03993231803178787 + 0.01 * 7.048004150390625
Epoch 430, val loss: 0.7559316158294678
Epoch 440, training loss: 0.10627289116382599 = 0.03585946187376976 + 0.01 * 7.041343688964844
Epoch 440, val loss: 0.7662003040313721
Epoch 450, training loss: 0.10269992053508759 = 0.03234289959073067 + 0.01 * 7.035702705383301
Epoch 450, val loss: 0.7764017581939697
Epoch 460, training loss: 0.09970778226852417 = 0.029293561354279518 + 0.01 * 7.041422367095947
Epoch 460, val loss: 0.7864527106285095
Epoch 470, training loss: 0.0969386026263237 = 0.02664286457002163 + 0.01 * 7.029573917388916
Epoch 470, val loss: 0.7962700724601746
Epoch 480, training loss: 0.0945897027850151 = 0.024327270686626434 + 0.01 * 7.026243686676025
Epoch 480, val loss: 0.8058579564094543
Epoch 490, training loss: 0.09255512803792953 = 0.02229483611881733 + 0.01 * 7.026029586791992
Epoch 490, val loss: 0.8152247071266174
Epoch 500, training loss: 0.09066846966743469 = 0.020504724234342575 + 0.01 * 7.0163750648498535
Epoch 500, val loss: 0.824303925037384
Epoch 510, training loss: 0.08902716636657715 = 0.018921326845884323 + 0.01 * 7.010583877563477
Epoch 510, val loss: 0.8331803679466248
Epoch 520, training loss: 0.08769209682941437 = 0.017514558508992195 + 0.01 * 7.017754077911377
Epoch 520, val loss: 0.8418130278587341
Epoch 530, training loss: 0.08636662364006042 = 0.016261551529169083 + 0.01 * 7.010507106781006
Epoch 530, val loss: 0.8501294255256653
Epoch 540, training loss: 0.0851239487528801 = 0.01514094602316618 + 0.01 * 6.998300552368164
Epoch 540, val loss: 0.8582479357719421
Epoch 550, training loss: 0.08409655839204788 = 0.014134167693555355 + 0.01 * 6.996239185333252
Epoch 550, val loss: 0.8661073446273804
Epoch 560, training loss: 0.08313872665166855 = 0.013227453455328941 + 0.01 * 6.991127014160156
Epoch 560, val loss: 0.8737710118293762
Epoch 570, training loss: 0.0823022797703743 = 0.012408039532601833 + 0.01 * 6.989423751831055
Epoch 570, val loss: 0.8811894059181213
Epoch 580, training loss: 0.08148322254419327 = 0.01166567299515009 + 0.01 * 6.981755256652832
Epoch 580, val loss: 0.8883900046348572
Epoch 590, training loss: 0.08080720156431198 = 0.010990838520228863 + 0.01 * 6.9816365242004395
Epoch 590, val loss: 0.8953556418418884
Epoch 600, training loss: 0.08008481562137604 = 0.010375608690083027 + 0.01 * 6.970920562744141
Epoch 600, val loss: 0.9021285176277161
Epoch 610, training loss: 0.07981148362159729 = 0.009812900796532631 + 0.01 * 6.999858379364014
Epoch 610, val loss: 0.9087562561035156
Epoch 620, training loss: 0.07893121987581253 = 0.009298142977058887 + 0.01 * 6.9633073806762695
Epoch 620, val loss: 0.9150975942611694
Epoch 630, training loss: 0.0784781277179718 = 0.008825606666505337 + 0.01 * 6.965251922607422
Epoch 630, val loss: 0.9213066101074219
Epoch 640, training loss: 0.07809410244226456 = 0.008390835486352444 + 0.01 * 6.9703264236450195
Epoch 640, val loss: 0.9273618459701538
Epoch 650, training loss: 0.07749556750059128 = 0.00798992719501257 + 0.01 * 6.950563907623291
Epoch 650, val loss: 0.9331838488578796
Epoch 660, training loss: 0.07709366083145142 = 0.007618865463882685 + 0.01 * 6.947479724884033
Epoch 660, val loss: 0.9388954639434814
Epoch 670, training loss: 0.07673407346010208 = 0.007274992763996124 + 0.01 * 6.945908069610596
Epoch 670, val loss: 0.944465696811676
Epoch 680, training loss: 0.07637942582368851 = 0.0069559975527226925 + 0.01 * 6.942343235015869
Epoch 680, val loss: 0.9498356580734253
Epoch 690, training loss: 0.07609519362449646 = 0.006659230217337608 + 0.01 * 6.943596363067627
Epoch 690, val loss: 0.9550984501838684
Epoch 700, training loss: 0.07572643458843231 = 0.006382784340530634 + 0.01 * 6.9343647956848145
Epoch 700, val loss: 0.9602470993995667
Epoch 710, training loss: 0.07534461468458176 = 0.006125107407569885 + 0.01 * 6.921950817108154
Epoch 710, val loss: 0.9652283787727356
Epoch 720, training loss: 0.07514934986829758 = 0.005884351674467325 + 0.01 * 6.92650032043457
Epoch 720, val loss: 0.9701221585273743
Epoch 730, training loss: 0.07476919889450073 = 0.00565933296456933 + 0.01 * 6.91098690032959
Epoch 730, val loss: 0.9748014211654663
Epoch 740, training loss: 0.07458144426345825 = 0.005448111332952976 + 0.01 * 6.913333892822266
Epoch 740, val loss: 0.9794356822967529
Epoch 750, training loss: 0.07424594461917877 = 0.005249940790235996 + 0.01 * 6.899600505828857
Epoch 750, val loss: 0.9839069247245789
Epoch 760, training loss: 0.07405312359333038 = 0.005063842982053757 + 0.01 * 6.898928642272949
Epoch 760, val loss: 0.9882953763008118
Epoch 770, training loss: 0.07388456165790558 = 0.00488872779533267 + 0.01 * 6.899583339691162
Epoch 770, val loss: 0.992576539516449
Epoch 780, training loss: 0.07365673780441284 = 0.004723813850432634 + 0.01 * 6.89329195022583
Epoch 780, val loss: 0.9966939091682434
Epoch 790, training loss: 0.07343561947345734 = 0.004568041767925024 + 0.01 * 6.886758327484131
Epoch 790, val loss: 1.0007903575897217
Epoch 800, training loss: 0.07349266856908798 = 0.004420710261911154 + 0.01 * 6.907196044921875
Epoch 800, val loss: 1.0047329664230347
Epoch 810, training loss: 0.07304047048091888 = 0.004281928762793541 + 0.01 * 6.875854015350342
Epoch 810, val loss: 1.008620023727417
Epoch 820, training loss: 0.0728946104645729 = 0.004150666296482086 + 0.01 * 6.87439489364624
Epoch 820, val loss: 1.0122944116592407
Epoch 830, training loss: 0.07263626903295517 = 0.004026147071272135 + 0.01 * 6.861012935638428
Epoch 830, val loss: 1.0159984827041626
Epoch 840, training loss: 0.0727105513215065 = 0.003907980863004923 + 0.01 * 6.880257606506348
Epoch 840, val loss: 1.019615650177002
Epoch 850, training loss: 0.07243788242340088 = 0.003796045435592532 + 0.01 * 6.8641839027404785
Epoch 850, val loss: 1.023016095161438
Epoch 860, training loss: 0.07216189056634903 = 0.0036896602250635624 + 0.01 * 6.847223281860352
Epoch 860, val loss: 1.0264456272125244
Epoch 870, training loss: 0.07210928946733475 = 0.0035883726086467505 + 0.01 * 6.8520917892456055
Epoch 870, val loss: 1.0297681093215942
Epoch 880, training loss: 0.0721205621957779 = 0.00349183171056211 + 0.01 * 6.862873077392578
Epoch 880, val loss: 1.0329517126083374
Epoch 890, training loss: 0.0718427300453186 = 0.0034001674503087997 + 0.01 * 6.844256401062012
Epoch 890, val loss: 1.0361852645874023
Epoch 900, training loss: 0.07194004207849503 = 0.00331274070776999 + 0.01 * 6.862730026245117
Epoch 900, val loss: 1.039233684539795
Epoch 910, training loss: 0.07154283672571182 = 0.0032292755786329508 + 0.01 * 6.831356048583984
Epoch 910, val loss: 1.0422639846801758
Epoch 920, training loss: 0.07157894223928452 = 0.0031494502909481525 + 0.01 * 6.842949390411377
Epoch 920, val loss: 1.0451569557189941
Epoch 930, training loss: 0.07141458243131638 = 0.0030733284074813128 + 0.01 * 6.834125518798828
Epoch 930, val loss: 1.0480971336364746
Epoch 940, training loss: 0.07134237885475159 = 0.0030003793071955442 + 0.01 * 6.834199905395508
Epoch 940, val loss: 1.0508304834365845
Epoch 950, training loss: 0.07121320813894272 = 0.002930666785687208 + 0.01 * 6.828254699707031
Epoch 950, val loss: 1.0536495447158813
Epoch 960, training loss: 0.07108639925718307 = 0.0028638055082410574 + 0.01 * 6.822259902954102
Epoch 960, val loss: 1.0562421083450317
Epoch 970, training loss: 0.07113124430179596 = 0.0027997076977044344 + 0.01 * 6.833154201507568
Epoch 970, val loss: 1.0589723587036133
Epoch 980, training loss: 0.07077647745609283 = 0.0027383400592952967 + 0.01 * 6.803813934326172
Epoch 980, val loss: 1.0615105628967285
Epoch 990, training loss: 0.07066388428211212 = 0.0026794024743139744 + 0.01 * 6.79844856262207
Epoch 990, val loss: 1.0640681982040405
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.0332136154174805 = 1.9472453594207764 + 0.01 * 8.59681510925293
Epoch 0, val loss: 1.9488986730575562
Epoch 10, training loss: 2.0229952335357666 = 1.9370276927947998 + 0.01 * 8.59675407409668
Epoch 10, val loss: 1.9389700889587402
Epoch 20, training loss: 2.010289192199707 = 1.9243242740631104 + 0.01 * 8.596498489379883
Epoch 20, val loss: 1.9260679483413696
Epoch 30, training loss: 1.992334246635437 = 1.9063788652420044 + 0.01 * 8.59554386138916
Epoch 30, val loss: 1.9074220657348633
Epoch 40, training loss: 1.9658055305480957 = 1.8799093961715698 + 0.01 * 8.589614868164062
Epoch 40, val loss: 1.8799874782562256
Epoch 50, training loss: 1.9286870956420898 = 1.8431551456451416 + 0.01 * 8.553199768066406
Epoch 50, val loss: 1.843400239944458
Epoch 60, training loss: 1.8866627216339111 = 1.8027228116989136 + 0.01 * 8.393988609313965
Epoch 60, val loss: 1.8073322772979736
Epoch 70, training loss: 1.8503634929656982 = 1.7682889699935913 + 0.01 * 8.20744800567627
Epoch 70, val loss: 1.779173731803894
Epoch 80, training loss: 1.8057303428649902 = 1.7256780862808228 + 0.01 * 8.005228996276855
Epoch 80, val loss: 1.7407560348510742
Epoch 90, training loss: 1.7436994314193726 = 1.6665765047073364 + 0.01 * 7.712291717529297
Epoch 90, val loss: 1.6873724460601807
Epoch 100, training loss: 1.661249041557312 = 1.5867983102798462 + 0.01 * 7.445075988769531
Epoch 100, val loss: 1.6178301572799683
Epoch 110, training loss: 1.564431071281433 = 1.4915674924850464 + 0.01 * 7.286359786987305
Epoch 110, val loss: 1.5378997325897217
Epoch 120, training loss: 1.4641188383102417 = 1.3917261362075806 + 0.01 * 7.239274024963379
Epoch 120, val loss: 1.4569400548934937
Epoch 130, training loss: 1.3640066385269165 = 1.2920304536819458 + 0.01 * 7.197621822357178
Epoch 130, val loss: 1.3782864809036255
Epoch 140, training loss: 1.264383316040039 = 1.1927204132080078 + 0.01 * 7.166294097900391
Epoch 140, val loss: 1.3015103340148926
Epoch 150, training loss: 1.1672725677490234 = 1.0958325862884521 + 0.01 * 7.144000053405762
Epoch 150, val loss: 1.2282302379608154
Epoch 160, training loss: 1.0762035846710205 = 1.0049399137496948 + 0.01 * 7.126370429992676
Epoch 160, val loss: 1.161149263381958
Epoch 170, training loss: 0.9928399920463562 = 0.921688437461853 + 0.01 * 7.11515474319458
Epoch 170, val loss: 1.1014001369476318
Epoch 180, training loss: 0.9159843921661377 = 0.8449034094810486 + 0.01 * 7.1081013679504395
Epoch 180, val loss: 1.0469114780426025
Epoch 190, training loss: 0.8435580730438232 = 0.7725269198417664 + 0.01 * 7.103113651275635
Epoch 190, val loss: 0.9958499670028687
Epoch 200, training loss: 0.7742611765861511 = 0.7032604217529297 + 0.01 * 7.1000752449035645
Epoch 200, val loss: 0.9473058581352234
Epoch 210, training loss: 0.7080453634262085 = 0.63706374168396 + 0.01 * 7.098162651062012
Epoch 210, val loss: 0.9016088843345642
Epoch 220, training loss: 0.6454970240592957 = 0.5745068788528442 + 0.01 * 7.099017143249512
Epoch 220, val loss: 0.8598603010177612
Epoch 230, training loss: 0.5873788595199585 = 0.5164044499397278 + 0.01 * 7.097438812255859
Epoch 230, val loss: 0.8229378461837769
Epoch 240, training loss: 0.5340666770935059 = 0.46310123801231384 + 0.01 * 7.0965423583984375
Epoch 240, val loss: 0.7915288209915161
Epoch 250, training loss: 0.4852861762046814 = 0.4143258333206177 + 0.01 * 7.096032619476318
Epoch 250, val loss: 0.7656766176223755
Epoch 260, training loss: 0.44058698415756226 = 0.36963215470314026 + 0.01 * 7.095482349395752
Epoch 260, val loss: 0.7452813982963562
Epoch 270, training loss: 0.39968791604042053 = 0.32873162627220154 + 0.01 * 7.09562873840332
Epoch 270, val loss: 0.7297951579093933
Epoch 280, training loss: 0.3624350428581238 = 0.2914768159389496 + 0.01 * 7.095824241638184
Epoch 280, val loss: 0.7187501788139343
Epoch 290, training loss: 0.3286939561367035 = 0.2577407956123352 + 0.01 * 7.095316410064697
Epoch 290, val loss: 0.7116209268569946
Epoch 300, training loss: 0.2983083128929138 = 0.22735822200775146 + 0.01 * 7.09500789642334
Epoch 300, val loss: 0.7080640196800232
Epoch 310, training loss: 0.2710917890071869 = 0.2001415342092514 + 0.01 * 7.095025062561035
Epoch 310, val loss: 0.7077310085296631
Epoch 320, training loss: 0.24688631296157837 = 0.1759408414363861 + 0.01 * 7.094547748565674
Epoch 320, val loss: 0.7102844715118408
Epoch 330, training loss: 0.22553899884223938 = 0.15459518134593964 + 0.01 * 7.094381809234619
Epoch 330, val loss: 0.7154210805892944
Epoch 340, training loss: 0.20688468217849731 = 0.13591542840003967 + 0.01 * 7.096926212310791
Epoch 340, val loss: 0.7228749394416809
Epoch 350, training loss: 0.1905994713306427 = 0.11966373771429062 + 0.01 * 7.093574047088623
Epoch 350, val loss: 0.7323212027549744
Epoch 360, training loss: 0.17650315165519714 = 0.10557585209608078 + 0.01 * 7.0927300453186035
Epoch 360, val loss: 0.7434676289558411
Epoch 370, training loss: 0.1643235683441162 = 0.0933799296617508 + 0.01 * 7.094363689422607
Epoch 370, val loss: 0.755940854549408
Epoch 380, training loss: 0.15372994542121887 = 0.08281969279050827 + 0.01 * 7.091024875640869
Epoch 380, val loss: 0.7694500684738159
Epoch 390, training loss: 0.14455336332321167 = 0.0736650750041008 + 0.01 * 7.088829040527344
Epoch 390, val loss: 0.7837308645248413
Epoch 400, training loss: 0.13658452033996582 = 0.0657183825969696 + 0.01 * 7.086613655090332
Epoch 400, val loss: 0.7985115647315979
Epoch 410, training loss: 0.12965549528598785 = 0.058809299021959305 + 0.01 * 7.084619522094727
Epoch 410, val loss: 0.813650906085968
Epoch 420, training loss: 0.1236141100525856 = 0.052790768444538116 + 0.01 * 7.082334041595459
Epoch 420, val loss: 0.8289754986763
Epoch 430, training loss: 0.11838746070861816 = 0.04753813147544861 + 0.01 * 7.084933280944824
Epoch 430, val loss: 0.8443779945373535
Epoch 440, training loss: 0.1137160062789917 = 0.042945701628923416 + 0.01 * 7.077030181884766
Epoch 440, val loss: 0.859693169593811
Epoch 450, training loss: 0.10964153707027435 = 0.03892220929265022 + 0.01 * 7.071932315826416
Epoch 450, val loss: 0.874890923500061
Epoch 460, training loss: 0.10618104040622711 = 0.035389699041843414 + 0.01 * 7.079133987426758
Epoch 460, val loss: 0.8898791670799255
Epoch 470, training loss: 0.1029207780957222 = 0.03228241950273514 + 0.01 * 7.063836097717285
Epoch 470, val loss: 0.9046525955200195
Epoch 480, training loss: 0.10013333708047867 = 0.02954074740409851 + 0.01 * 7.059259414672852
Epoch 480, val loss: 0.9190477132797241
Epoch 490, training loss: 0.09768398106098175 = 0.02711435593664646 + 0.01 * 7.056962966918945
Epoch 490, val loss: 0.9331650733947754
Epoch 500, training loss: 0.0954279974102974 = 0.02496200054883957 + 0.01 * 7.046599864959717
Epoch 500, val loss: 0.9469114542007446
Epoch 510, training loss: 0.09355463087558746 = 0.023047689348459244 + 0.01 * 7.050694465637207
Epoch 510, val loss: 0.9602434039115906
Epoch 520, training loss: 0.0916832685470581 = 0.02133866958320141 + 0.01 * 7.034459590911865
Epoch 520, val loss: 0.973206102848053
Epoch 530, training loss: 0.09012837707996368 = 0.019810643047094345 + 0.01 * 7.031774044036865
Epoch 530, val loss: 0.985829770565033
Epoch 540, training loss: 0.08865047246217728 = 0.018439309671521187 + 0.01 * 7.021116256713867
Epoch 540, val loss: 0.9980064034461975
Epoch 550, training loss: 0.08735671639442444 = 0.017204437404870987 + 0.01 * 7.015227317810059
Epoch 550, val loss: 1.0099005699157715
Epoch 560, training loss: 0.08604810386896133 = 0.016090290620923042 + 0.01 * 6.995781898498535
Epoch 560, val loss: 1.0214399099349976
Epoch 570, training loss: 0.08531426638364792 = 0.015081770718097687 + 0.01 * 7.023249626159668
Epoch 570, val loss: 1.0325615406036377
Epoch 580, training loss: 0.08400251716375351 = 0.014167222194373608 + 0.01 * 6.983529567718506
Epoch 580, val loss: 1.0433756113052368
Epoch 590, training loss: 0.08332280814647675 = 0.013334836810827255 + 0.01 * 6.998797416687012
Epoch 590, val loss: 1.0538197755813599
Epoch 600, training loss: 0.0821942687034607 = 0.01257777214050293 + 0.01 * 6.9616498947143555
Epoch 600, val loss: 1.0639269351959229
Epoch 610, training loss: 0.08158565312623978 = 0.011885027401149273 + 0.01 * 6.970062732696533
Epoch 610, val loss: 1.0736970901489258
Epoch 620, training loss: 0.08073993772268295 = 0.011251685209572315 + 0.01 * 6.948824882507324
Epoch 620, val loss: 1.083168625831604
Epoch 630, training loss: 0.08023924380540848 = 0.010669930838048458 + 0.01 * 6.9569315910339355
Epoch 630, val loss: 1.092278242111206
Epoch 640, training loss: 0.07965103536844254 = 0.010135185904800892 + 0.01 * 6.951584815979004
Epoch 640, val loss: 1.1011974811553955
Epoch 650, training loss: 0.07886552810668945 = 0.009642527438700199 + 0.01 * 6.922300338745117
Epoch 650, val loss: 1.1098133325576782
Epoch 660, training loss: 0.07830601185560226 = 0.009187722578644753 + 0.01 * 6.911829471588135
Epoch 660, val loss: 1.1181247234344482
Epoch 670, training loss: 0.07779984921216965 = 0.008766377344727516 + 0.01 * 6.903347492218018
Epoch 670, val loss: 1.1262158155441284
Epoch 680, training loss: 0.07766101509332657 = 0.008375112898647785 + 0.01 * 6.928590297698975
Epoch 680, val loss: 1.1341158151626587
Epoch 690, training loss: 0.07693666219711304 = 0.008012112230062485 + 0.01 * 6.892455577850342
Epoch 690, val loss: 1.1417391300201416
Epoch 700, training loss: 0.07663636654615402 = 0.007673731539398432 + 0.01 * 6.896263599395752
Epoch 700, val loss: 1.149147629737854
Epoch 710, training loss: 0.07615764439105988 = 0.0073590390384197235 + 0.01 * 6.879860877990723
Epoch 710, val loss: 1.1563735008239746
Epoch 720, training loss: 0.07589180767536163 = 0.007064709439873695 + 0.01 * 6.882709980010986
Epoch 720, val loss: 1.1633288860321045
Epoch 730, training loss: 0.07551708817481995 = 0.006789190229028463 + 0.01 * 6.87278938293457
Epoch 730, val loss: 1.1701738834381104
Epoch 740, training loss: 0.07533005625009537 = 0.006531273014843464 + 0.01 * 6.879878520965576
Epoch 740, val loss: 1.1767737865447998
Epoch 750, training loss: 0.07495518028736115 = 0.006289525423198938 + 0.01 * 6.866565704345703
Epoch 750, val loss: 1.1832517385482788
Epoch 760, training loss: 0.07471577823162079 = 0.006062774918973446 + 0.01 * 6.86530065536499
Epoch 760, val loss: 1.1895051002502441
Epoch 770, training loss: 0.0744282677769661 = 0.005849584937095642 + 0.01 * 6.857868194580078
Epoch 770, val loss: 1.1956244707107544
Epoch 780, training loss: 0.07413153350353241 = 0.005648430436849594 + 0.01 * 6.848310947418213
Epoch 780, val loss: 1.2015234231948853
Epoch 790, training loss: 0.07406017929315567 = 0.005458846688270569 + 0.01 * 6.860133647918701
Epoch 790, val loss: 1.2073631286621094
Epoch 800, training loss: 0.07367518544197083 = 0.005280151031911373 + 0.01 * 6.839503765106201
Epoch 800, val loss: 1.2129600048065186
Epoch 810, training loss: 0.07385259866714478 = 0.0051112729124724865 + 0.01 * 6.874133110046387
Epoch 810, val loss: 1.2184538841247559
Epoch 820, training loss: 0.07319222390651703 = 0.004951625131070614 + 0.01 * 6.824059963226318
Epoch 820, val loss: 1.2238200902938843
Epoch 830, training loss: 0.07304951548576355 = 0.004800158552825451 + 0.01 * 6.824936389923096
Epoch 830, val loss: 1.2290207147598267
Epoch 840, training loss: 0.07304474711418152 = 0.004657011944800615 + 0.01 * 6.838773250579834
Epoch 840, val loss: 1.2340600490570068
Epoch 850, training loss: 0.07289646565914154 = 0.0045210509561002254 + 0.01 * 6.837541580200195
Epoch 850, val loss: 1.2390621900558472
Epoch 860, training loss: 0.07268484681844711 = 0.004391901195049286 + 0.01 * 6.829294681549072
Epoch 860, val loss: 1.2438515424728394
Epoch 870, training loss: 0.07240736484527588 = 0.0042692432180047035 + 0.01 * 6.813812732696533
Epoch 870, val loss: 1.248562216758728
Epoch 880, training loss: 0.07236792147159576 = 0.00415275152772665 + 0.01 * 6.821516990661621
Epoch 880, val loss: 1.253145694732666
Epoch 890, training loss: 0.07215302437543869 = 0.004041518084704876 + 0.01 * 6.811151027679443
Epoch 890, val loss: 1.2576135396957397
Epoch 900, training loss: 0.07195498794317245 = 0.003935723565518856 + 0.01 * 6.801926612854004
Epoch 900, val loss: 1.2619636058807373
Epoch 910, training loss: 0.07194352895021439 = 0.0038346608635038137 + 0.01 * 6.810887336730957
Epoch 910, val loss: 1.266223430633545
Epoch 920, training loss: 0.07193367183208466 = 0.0037382824812084436 + 0.01 * 6.819538593292236
Epoch 920, val loss: 1.2703958749771118
Epoch 930, training loss: 0.07149741053581238 = 0.0036464144941419363 + 0.01 * 6.78510046005249
Epoch 930, val loss: 1.2743932008743286
Epoch 940, training loss: 0.07140745222568512 = 0.0035585942678153515 + 0.01 * 6.784886360168457
Epoch 940, val loss: 1.278364658355713
Epoch 950, training loss: 0.0713849663734436 = 0.0034746406599879265 + 0.01 * 6.791032791137695
Epoch 950, val loss: 1.2821646928787231
Epoch 960, training loss: 0.07151185721158981 = 0.0033944305032491684 + 0.01 * 6.811742782592773
Epoch 960, val loss: 1.2859846353530884
Epoch 970, training loss: 0.07104717940092087 = 0.0033178417943418026 + 0.01 * 6.7729339599609375
Epoch 970, val loss: 1.2896003723144531
Epoch 980, training loss: 0.07107647508382797 = 0.0032442088704556227 + 0.01 * 6.78322696685791
Epoch 980, val loss: 1.2932459115982056
Epoch 990, training loss: 0.07089182734489441 = 0.003173807868734002 + 0.01 * 6.7718024253845215
Epoch 990, val loss: 1.2967021465301514
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8423827095413812
The final CL Acc:0.79877, 0.01492, The final GNN Acc:0.84027, 0.00228
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11702])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10656])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0465688705444336 = 1.9606003761291504 + 0.01 * 8.59685230255127
Epoch 0, val loss: 1.9626762866973877
Epoch 10, training loss: 2.036130905151367 = 1.9501630067825317 + 0.01 * 8.596780776977539
Epoch 10, val loss: 1.9523111581802368
Epoch 20, training loss: 2.0229380130767822 = 1.9369722604751587 + 0.01 * 8.596564292907715
Epoch 20, val loss: 1.938658356666565
Epoch 30, training loss: 2.0040283203125 = 1.9180700778961182 + 0.01 * 8.595827102661133
Epoch 30, val loss: 1.9186328649520874
Epoch 40, training loss: 1.9756273031234741 = 1.8897106647491455 + 0.01 * 8.591659545898438
Epoch 40, val loss: 1.8889822959899902
Epoch 50, training loss: 1.936310887336731 = 1.8506633043289185 + 0.01 * 8.56475830078125
Epoch 50, val loss: 1.8503320217132568
Epoch 60, training loss: 1.8946932554244995 = 1.8103642463684082 + 0.01 * 8.432901382446289
Epoch 60, val loss: 1.815224051475525
Epoch 70, training loss: 1.8622443675994873 = 1.7791303396224976 + 0.01 * 8.311405181884766
Epoch 70, val loss: 1.7908344268798828
Epoch 80, training loss: 1.8230963945388794 = 1.7412534952163696 + 0.01 * 8.184287071228027
Epoch 80, val loss: 1.7583532333374023
Epoch 90, training loss: 1.7682613134384155 = 1.6881520748138428 + 0.01 * 8.010923385620117
Epoch 90, val loss: 1.7123093605041504
Epoch 100, training loss: 1.6936885118484497 = 1.6154426336288452 + 0.01 * 7.824587821960449
Epoch 100, val loss: 1.650743007659912
Epoch 110, training loss: 1.6048939228057861 = 1.5293723344802856 + 0.01 * 7.552157402038574
Epoch 110, val loss: 1.5798461437225342
Epoch 120, training loss: 1.5171623229980469 = 1.442556381225586 + 0.01 * 7.460593223571777
Epoch 120, val loss: 1.5119695663452148
Epoch 130, training loss: 1.4351731538772583 = 1.3611481189727783 + 0.01 * 7.402500629425049
Epoch 130, val loss: 1.4525960683822632
Epoch 140, training loss: 1.3583714962005615 = 1.2847576141357422 + 0.01 * 7.361384391784668
Epoch 140, val loss: 1.4014192819595337
Epoch 150, training loss: 1.2854772806167603 = 1.2121590375900269 + 0.01 * 7.3318257331848145
Epoch 150, val loss: 1.355843186378479
Epoch 160, training loss: 1.2144458293914795 = 1.1412932872772217 + 0.01 * 7.315258026123047
Epoch 160, val loss: 1.3126978874206543
Epoch 170, training loss: 1.1432744264602661 = 1.070271611213684 + 0.01 * 7.300282001495361
Epoch 170, val loss: 1.2690707445144653
Epoch 180, training loss: 1.0714789628982544 = 0.9986883401870728 + 0.01 * 7.279066562652588
Epoch 180, val loss: 1.22361159324646
Epoch 190, training loss: 0.999850869178772 = 0.9273130297660828 + 0.01 * 7.253785610198975
Epoch 190, val loss: 1.1768831014633179
Epoch 200, training loss: 0.9298167824745178 = 0.8574214577674866 + 0.01 * 7.2395339012146
Epoch 200, val loss: 1.130213737487793
Epoch 210, training loss: 0.862357497215271 = 0.7901347279548645 + 0.01 * 7.222278118133545
Epoch 210, val loss: 1.0852980613708496
Epoch 220, training loss: 0.7984685301780701 = 0.7263759970664978 + 0.01 * 7.209253787994385
Epoch 220, val loss: 1.042983889579773
Epoch 230, training loss: 0.7388923764228821 = 0.666901171207428 + 0.01 * 7.199121952056885
Epoch 230, val loss: 1.0042520761489868
Epoch 240, training loss: 0.6840991377830505 = 0.6121816039085388 + 0.01 * 7.191755771636963
Epoch 240, val loss: 0.9699900150299072
Epoch 250, training loss: 0.6339433193206787 = 0.5620807409286499 + 0.01 * 7.186257839202881
Epoch 250, val loss: 0.9406428933143616
Epoch 260, training loss: 0.5876842737197876 = 0.5158919095993042 + 0.01 * 7.179238796234131
Epoch 260, val loss: 0.9161345362663269
Epoch 270, training loss: 0.5442398190498352 = 0.47250422835350037 + 0.01 * 7.173559665679932
Epoch 270, val loss: 0.895708441734314
Epoch 280, training loss: 0.5022886395454407 = 0.4305902421474457 + 0.01 * 7.1698408126831055
Epoch 280, val loss: 0.8783478140830994
Epoch 290, training loss: 0.46055975556373596 = 0.38895469903945923 + 0.01 * 7.160506725311279
Epoch 290, val loss: 0.863322913646698
Epoch 300, training loss: 0.41862913966178894 = 0.347067266702652 + 0.01 * 7.156188488006592
Epoch 300, val loss: 0.850346028804779
Epoch 310, training loss: 0.37690210342407227 = 0.3054201006889343 + 0.01 * 7.148199558258057
Epoch 310, val loss: 0.8397710919380188
Epoch 320, training loss: 0.33670955896377563 = 0.26526251435279846 + 0.01 * 7.144704341888428
Epoch 320, val loss: 0.8325031399726868
Epoch 330, training loss: 0.29948967695236206 = 0.2280931919813156 + 0.01 * 7.139649868011475
Epoch 330, val loss: 0.8297607898712158
Epoch 340, training loss: 0.2664143443107605 = 0.1950758695602417 + 0.01 * 7.133846759796143
Epoch 340, val loss: 0.8322663307189941
Epoch 350, training loss: 0.23790350556373596 = 0.16662585735321045 + 0.01 * 7.12776517868042
Epoch 350, val loss: 0.8399699330329895
Epoch 360, training loss: 0.21381577849388123 = 0.14259983599185944 + 0.01 * 7.121593475341797
Epoch 360, val loss: 0.8523336052894592
Epoch 370, training loss: 0.19371269643306732 = 0.12252014875411987 + 0.01 * 7.119255065917969
Epoch 370, val loss: 0.8683369159698486
Epoch 380, training loss: 0.1768975853919983 = 0.10576122254133224 + 0.01 * 7.113636493682861
Epoch 380, val loss: 0.886667788028717
Epoch 390, training loss: 0.16293351352214813 = 0.0917443186044693 + 0.01 * 7.118919849395752
Epoch 390, val loss: 0.9067046046257019
Epoch 400, training loss: 0.15104587376117706 = 0.07999452203512192 + 0.01 * 7.105135440826416
Epoch 400, val loss: 0.9274860620498657
Epoch 410, training loss: 0.14111995697021484 = 0.07009875029325485 + 0.01 * 7.102121353149414
Epoch 410, val loss: 0.9486111998558044
Epoch 420, training loss: 0.13267803192138672 = 0.06172192096710205 + 0.01 * 7.095610618591309
Epoch 420, val loss: 0.9697023630142212
Epoch 430, training loss: 0.1255275458097458 = 0.05460643395781517 + 0.01 * 7.092112064361572
Epoch 430, val loss: 0.9904483556747437
Epoch 440, training loss: 0.11956484615802765 = 0.04854314401745796 + 0.01 * 7.102170944213867
Epoch 440, val loss: 1.0106704235076904
Epoch 450, training loss: 0.11423042416572571 = 0.04336108639836311 + 0.01 * 7.0869340896606445
Epoch 450, val loss: 1.0302191972732544
Epoch 460, training loss: 0.10969559848308563 = 0.038910843431949615 + 0.01 * 7.0784759521484375
Epoch 460, val loss: 1.0491212606430054
Epoch 470, training loss: 0.10593785345554352 = 0.03507469967007637 + 0.01 * 7.086315155029297
Epoch 470, val loss: 1.0673589706420898
Epoch 480, training loss: 0.10246409475803375 = 0.031757816672325134 + 0.01 * 7.0706281661987305
Epoch 480, val loss: 1.0849177837371826
Epoch 490, training loss: 0.0995405912399292 = 0.028875432908535004 + 0.01 * 7.066515922546387
Epoch 490, val loss: 1.101889967918396
Epoch 500, training loss: 0.09696994721889496 = 0.026359323412179947 + 0.01 * 7.061061859130859
Epoch 500, val loss: 1.118167757987976
Epoch 510, training loss: 0.0948132872581482 = 0.024153459817171097 + 0.01 * 7.065982818603516
Epoch 510, val loss: 1.1338222026824951
Epoch 520, training loss: 0.09280380606651306 = 0.02221306972205639 + 0.01 * 7.0590739250183105
Epoch 520, val loss: 1.1489564180374146
Epoch 530, training loss: 0.09103304147720337 = 0.02049821987748146 + 0.01 * 7.053483009338379
Epoch 530, val loss: 1.1634416580200195
Epoch 540, training loss: 0.08951737731695175 = 0.018976688385009766 + 0.01 * 7.0540690422058105
Epoch 540, val loss: 1.177469253540039
Epoch 550, training loss: 0.08795943856239319 = 0.01762164756655693 + 0.01 * 7.033778667449951
Epoch 550, val loss: 1.1909093856811523
Epoch 560, training loss: 0.08711207658052444 = 0.016409488394856453 + 0.01 * 7.070259094238281
Epoch 560, val loss: 1.2038846015930176
Epoch 570, training loss: 0.08567396551370621 = 0.015323451720178127 + 0.01 * 7.035051345825195
Epoch 570, val loss: 1.216390609741211
Epoch 580, training loss: 0.08457045257091522 = 0.014345264993607998 + 0.01 * 7.022518634796143
Epoch 580, val loss: 1.228407621383667
Epoch 590, training loss: 0.08393514156341553 = 0.013461112976074219 + 0.01 * 7.047402858734131
Epoch 590, val loss: 1.2400163412094116
Epoch 600, training loss: 0.08274485170841217 = 0.01266139093786478 + 0.01 * 7.008346080780029
Epoch 600, val loss: 1.2512398958206177
Epoch 610, training loss: 0.08196541666984558 = 0.01193457655608654 + 0.01 * 7.0030837059021
Epoch 610, val loss: 1.2620184421539307
Epoch 620, training loss: 0.08126729726791382 = 0.011272020637989044 + 0.01 * 6.999527454376221
Epoch 620, val loss: 1.2725032567977905
Epoch 630, training loss: 0.08083212375640869 = 0.010666921734809875 + 0.01 * 7.0165205001831055
Epoch 630, val loss: 1.2825719118118286
Epoch 640, training loss: 0.08004523068666458 = 0.01011325977742672 + 0.01 * 6.993196964263916
Epoch 640, val loss: 1.292296290397644
Epoch 650, training loss: 0.07940705120563507 = 0.00960476603358984 + 0.01 * 6.980228424072266
Epoch 650, val loss: 1.3017539978027344
Epoch 660, training loss: 0.07914599031209946 = 0.009137186221778393 + 0.01 * 7.000881195068359
Epoch 660, val loss: 1.3108422756195068
Epoch 670, training loss: 0.0784212201833725 = 0.008706671185791492 + 0.01 * 6.971455097198486
Epoch 670, val loss: 1.3196724653244019
Epoch 680, training loss: 0.07808549702167511 = 0.008308465592563152 + 0.01 * 6.97770357131958
Epoch 680, val loss: 1.3282688856124878
Epoch 690, training loss: 0.07758535444736481 = 0.007939991541206837 + 0.01 * 6.964536190032959
Epoch 690, val loss: 1.3365453481674194
Epoch 700, training loss: 0.07713403552770615 = 0.007597796153277159 + 0.01 * 6.9536237716674805
Epoch 700, val loss: 1.3446040153503418
Epoch 710, training loss: 0.07672146707773209 = 0.007279833313077688 + 0.01 * 6.9441633224487305
Epoch 710, val loss: 1.352445125579834
Epoch 720, training loss: 0.07647240906953812 = 0.006983782630413771 + 0.01 * 6.9488630294799805
Epoch 720, val loss: 1.3600434064865112
Epoch 730, training loss: 0.0760325938463211 = 0.0067075216211378574 + 0.01 * 6.932507514953613
Epoch 730, val loss: 1.3673956394195557
Epoch 740, training loss: 0.07591618597507477 = 0.006449121981859207 + 0.01 * 6.9467058181762695
Epoch 740, val loss: 1.3746141195297241
Epoch 750, training loss: 0.07544804364442825 = 0.006207374855875969 + 0.01 * 6.924066543579102
Epoch 750, val loss: 1.3815152645111084
Epoch 760, training loss: 0.07515917718410492 = 0.0059806653298437595 + 0.01 * 6.91785192489624
Epoch 760, val loss: 1.3883236646652222
Epoch 770, training loss: 0.07529345899820328 = 0.0057677412405610085 + 0.01 * 6.952572345733643
Epoch 770, val loss: 1.3949286937713623
Epoch 780, training loss: 0.07475773990154266 = 0.0055678123608231544 + 0.01 * 6.91899299621582
Epoch 780, val loss: 1.4012680053710938
Epoch 790, training loss: 0.07444426417350769 = 0.0053795576095581055 + 0.01 * 6.906470775604248
Epoch 790, val loss: 1.4075261354446411
Epoch 800, training loss: 0.07441778481006622 = 0.00520198093727231 + 0.01 * 6.921580791473389
Epoch 800, val loss: 1.4136334657669067
Epoch 810, training loss: 0.07405144721269608 = 0.0050346809439361095 + 0.01 * 6.901677131652832
Epoch 810, val loss: 1.4194698333740234
Epoch 820, training loss: 0.07396823912858963 = 0.0048765866085886955 + 0.01 * 6.909165859222412
Epoch 820, val loss: 1.4252136945724487
Epoch 830, training loss: 0.07360713183879852 = 0.004726876504719257 + 0.01 * 6.888025283813477
Epoch 830, val loss: 1.4308120012283325
Epoch 840, training loss: 0.0736037865281105 = 0.004584940150380135 + 0.01 * 6.901885032653809
Epoch 840, val loss: 1.4363032579421997
Epoch 850, training loss: 0.07334426790475845 = 0.004450685810297728 + 0.01 * 6.8893585205078125
Epoch 850, val loss: 1.4416272640228271
Epoch 860, training loss: 0.07307320088148117 = 0.004323294386267662 + 0.01 * 6.874990463256836
Epoch 860, val loss: 1.446838140487671
Epoch 870, training loss: 0.07334111630916595 = 0.004202061798423529 + 0.01 * 6.913905620574951
Epoch 870, val loss: 1.4519284963607788
Epoch 880, training loss: 0.07280270010232925 = 0.004087088163942099 + 0.01 * 6.871561050415039
Epoch 880, val loss: 1.4568473100662231
Epoch 890, training loss: 0.07287892699241638 = 0.003977620974183083 + 0.01 * 6.890130996704102
Epoch 890, val loss: 1.4617125988006592
Epoch 900, training loss: 0.0725574642419815 = 0.0038733857218176126 + 0.01 * 6.868407726287842
Epoch 900, val loss: 1.4663697481155396
Epoch 910, training loss: 0.07248925417661667 = 0.0037740387488156557 + 0.01 * 6.871521949768066
Epoch 910, val loss: 1.470982551574707
Epoch 920, training loss: 0.07223819196224213 = 0.0036792552564293146 + 0.01 * 6.855894088745117
Epoch 920, val loss: 1.4754793643951416
Epoch 930, training loss: 0.07246042042970657 = 0.0035886738914996386 + 0.01 * 6.887174606323242
Epoch 930, val loss: 1.4798905849456787
Epoch 940, training loss: 0.07212875783443451 = 0.0035023544915020466 + 0.01 * 6.862640857696533
Epoch 940, val loss: 1.4841341972351074
Epoch 950, training loss: 0.07191573828458786 = 0.003419792978093028 + 0.01 * 6.849594593048096
Epoch 950, val loss: 1.4883239269256592
Epoch 960, training loss: 0.07187861204147339 = 0.003340718802064657 + 0.01 * 6.853789329528809
Epoch 960, val loss: 1.4924310445785522
Epoch 970, training loss: 0.07163967192173004 = 0.00326505396515131 + 0.01 * 6.837461948394775
Epoch 970, val loss: 1.4963793754577637
Epoch 980, training loss: 0.07196161150932312 = 0.0031924834474921227 + 0.01 * 6.876912593841553
Epoch 980, val loss: 1.5003409385681152
Epoch 990, training loss: 0.07148586213588715 = 0.0031230992171913385 + 0.01 * 6.836276531219482
Epoch 990, val loss: 1.5040748119354248
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.7996837111228255
=== training gcn model ===
Epoch 0, training loss: 2.0421552658081055 = 1.9561868906021118 + 0.01 * 8.596827507019043
Epoch 0, val loss: 1.9453551769256592
Epoch 10, training loss: 2.031697988510132 = 1.9457303285598755 + 0.01 * 8.596758842468262
Epoch 10, val loss: 1.9357637166976929
Epoch 20, training loss: 2.018787384033203 = 1.932822346687317 + 0.01 * 8.596514701843262
Epoch 20, val loss: 1.9235895872116089
Epoch 30, training loss: 2.000765085220337 = 1.9148093461990356 + 0.01 * 8.5955810546875
Epoch 30, val loss: 1.9063688516616821
Epoch 40, training loss: 1.974160075187683 = 1.8882622718811035 + 0.01 * 8.589776039123535
Epoch 40, val loss: 1.8811296224594116
Epoch 50, training loss: 1.9368054866790771 = 1.8512543439865112 + 0.01 * 8.555118560791016
Epoch 50, val loss: 1.8474916219711304
Epoch 60, training loss: 1.8952937126159668 = 1.8111752271652222 + 0.01 * 8.411848068237305
Epoch 60, val loss: 1.815481185913086
Epoch 70, training loss: 1.864264726638794 = 1.781027913093567 + 0.01 * 8.32368278503418
Epoch 70, val loss: 1.7944982051849365
Epoch 80, training loss: 1.8299598693847656 = 1.7476520538330078 + 0.01 * 8.23078441619873
Epoch 80, val loss: 1.765553593635559
Epoch 90, training loss: 1.7828632593154907 = 1.7020745277404785 + 0.01 * 8.07886791229248
Epoch 90, val loss: 1.725149393081665
Epoch 100, training loss: 1.7170718908309937 = 1.6386306285858154 + 0.01 * 7.844124794006348
Epoch 100, val loss: 1.6702227592468262
Epoch 110, training loss: 1.6340289115905762 = 1.5587035417556763 + 0.01 * 7.5325422286987305
Epoch 110, val loss: 1.6025197505950928
Epoch 120, training loss: 1.5472674369812012 = 1.47322678565979 + 0.01 * 7.40406608581543
Epoch 120, val loss: 1.5327261686325073
Epoch 130, training loss: 1.4635570049285889 = 1.3901468515396118 + 0.01 * 7.3410162925720215
Epoch 130, val loss: 1.467782735824585
Epoch 140, training loss: 1.3800863027572632 = 1.3070094585418701 + 0.01 * 7.307680606842041
Epoch 140, val loss: 1.402806282043457
Epoch 150, training loss: 1.2918115854263306 = 1.2190250158309937 + 0.01 * 7.278654098510742
Epoch 150, val loss: 1.33463716506958
Epoch 160, training loss: 1.1966273784637451 = 1.1240925788879395 + 0.01 * 7.253480434417725
Epoch 160, val loss: 1.2615793943405151
Epoch 170, training loss: 1.0959279537200928 = 1.0235991477966309 + 0.01 * 7.232880592346191
Epoch 170, val loss: 1.1856844425201416
Epoch 180, training loss: 0.99349445104599 = 0.9212756752967834 + 0.01 * 7.221879482269287
Epoch 180, val loss: 1.1093188524246216
Epoch 190, training loss: 0.8940743803977966 = 0.8219027519226074 + 0.01 * 7.2171630859375
Epoch 190, val loss: 1.036441445350647
Epoch 200, training loss: 0.8023120760917664 = 0.7301944494247437 + 0.01 * 7.211760997772217
Epoch 200, val loss: 0.9706096649169922
Epoch 210, training loss: 0.7211413979530334 = 0.6490757465362549 + 0.01 * 7.2065653800964355
Epoch 210, val loss: 0.9148563742637634
Epoch 220, training loss: 0.6510418057441711 = 0.5790270566940308 + 0.01 * 7.201473236083984
Epoch 220, val loss: 0.8699463605880737
Epoch 230, training loss: 0.5908336043357849 = 0.518880307674408 + 0.01 * 7.195329666137695
Epoch 230, val loss: 0.8353588581085205
Epoch 240, training loss: 0.5386279821395874 = 0.4667385220527649 + 0.01 * 7.188946723937988
Epoch 240, val loss: 0.8094321489334106
Epoch 250, training loss: 0.49238258600234985 = 0.4205637574195862 + 0.01 * 7.181881427764893
Epoch 250, val loss: 0.7907289266586304
Epoch 260, training loss: 0.4502713084220886 = 0.3785335123538971 + 0.01 * 7.173779010772705
Epoch 260, val loss: 0.7775132656097412
Epoch 270, training loss: 0.41099250316619873 = 0.3393068015575409 + 0.01 * 7.1685686111450195
Epoch 270, val loss: 0.7687352895736694
Epoch 280, training loss: 0.373719185590744 = 0.30215147137641907 + 0.01 * 7.156771183013916
Epoch 280, val loss: 0.7635175585746765
Epoch 290, training loss: 0.3382966220378876 = 0.26679861545562744 + 0.01 * 7.149801731109619
Epoch 290, val loss: 0.7617594003677368
Epoch 300, training loss: 0.3049619793891907 = 0.2335272878408432 + 0.01 * 7.143467903137207
Epoch 300, val loss: 0.7634190917015076
Epoch 310, training loss: 0.27434802055358887 = 0.20295557379722595 + 0.01 * 7.139243125915527
Epoch 310, val loss: 0.7686086893081665
Epoch 320, training loss: 0.2471064329147339 = 0.17577312886714935 + 0.01 * 7.133330821990967
Epoch 320, val loss: 0.7769955396652222
Epoch 330, training loss: 0.22358521819114685 = 0.15229454636573792 + 0.01 * 7.1290669441223145
Epoch 330, val loss: 0.7883371710777283
Epoch 340, training loss: 0.20357629656791687 = 0.13231465220451355 + 0.01 * 7.126164436340332
Epoch 340, val loss: 0.8019447326660156
Epoch 350, training loss: 0.18659362196922302 = 0.1153922751545906 + 0.01 * 7.12013578414917
Epoch 350, val loss: 0.8172782063484192
Epoch 360, training loss: 0.17220591008663177 = 0.10105632245540619 + 0.01 * 7.114958763122559
Epoch 360, val loss: 0.8337177634239197
Epoch 370, training loss: 0.15997058153152466 = 0.08888959884643555 + 0.01 * 7.108098030090332
Epoch 370, val loss: 0.8509007096290588
Epoch 380, training loss: 0.1495560109615326 = 0.0785372406244278 + 0.01 * 7.101876258850098
Epoch 380, val loss: 0.8684160113334656
Epoch 390, training loss: 0.140676811337471 = 0.06969401985406876 + 0.01 * 7.0982794761657715
Epoch 390, val loss: 0.8861716389656067
Epoch 400, training loss: 0.13316814601421356 = 0.0621039979159832 + 0.01 * 7.106414318084717
Epoch 400, val loss: 0.9039061665534973
Epoch 410, training loss: 0.12649273872375488 = 0.055560026317834854 + 0.01 * 7.093270778656006
Epoch 410, val loss: 0.921362578868866
Epoch 420, training loss: 0.12077505886554718 = 0.049878280609846115 + 0.01 * 7.089677810668945
Epoch 420, val loss: 0.9385322332382202
Epoch 430, training loss: 0.11574216187000275 = 0.04491826146841049 + 0.01 * 7.082390308380127
Epoch 430, val loss: 0.9553431868553162
Epoch 440, training loss: 0.11170211434364319 = 0.04056796431541443 + 0.01 * 7.113415241241455
Epoch 440, val loss: 0.9717379808425903
Epoch 450, training loss: 0.10758155584335327 = 0.036748554557561874 + 0.01 * 7.083300590515137
Epoch 450, val loss: 0.9877101182937622
Epoch 460, training loss: 0.10411570966243744 = 0.03337889537215233 + 0.01 * 7.073681831359863
Epoch 460, val loss: 1.00321364402771
Epoch 470, training loss: 0.10105404257774353 = 0.030396906659007072 + 0.01 * 7.065713882446289
Epoch 470, val loss: 1.0183497667312622
Epoch 480, training loss: 0.09837471693754196 = 0.02775292843580246 + 0.01 * 7.062179088592529
Epoch 480, val loss: 1.0330458879470825
Epoch 490, training loss: 0.09608487039804459 = 0.02540590614080429 + 0.01 * 7.067896366119385
Epoch 490, val loss: 1.047309160232544
Epoch 500, training loss: 0.09391441196203232 = 0.02332255244255066 + 0.01 * 7.0591864585876465
Epoch 500, val loss: 1.0612727403640747
Epoch 510, training loss: 0.09203676879405975 = 0.02146972343325615 + 0.01 * 7.056704044342041
Epoch 510, val loss: 1.0746983289718628
Epoch 520, training loss: 0.09025313705205917 = 0.01981797069311142 + 0.01 * 7.043516635894775
Epoch 520, val loss: 1.0878604650497437
Epoch 530, training loss: 0.08880544453859329 = 0.018343880772590637 + 0.01 * 7.046156406402588
Epoch 530, val loss: 1.100588321685791
Epoch 540, training loss: 0.08736275881528854 = 0.01702646166086197 + 0.01 * 7.033629894256592
Epoch 540, val loss: 1.112845540046692
Epoch 550, training loss: 0.086252860724926 = 0.015844402834773064 + 0.01 * 7.0408453941345215
Epoch 550, val loss: 1.1248571872711182
Epoch 560, training loss: 0.0850917398929596 = 0.014782674610614777 + 0.01 * 7.030906677246094
Epoch 560, val loss: 1.1363903284072876
Epoch 570, training loss: 0.08400936424732208 = 0.013825423084199429 + 0.01 * 7.018394470214844
Epoch 570, val loss: 1.1476645469665527
Epoch 580, training loss: 0.08339418470859528 = 0.012959924526512623 + 0.01 * 7.043426036834717
Epoch 580, val loss: 1.1585958003997803
Epoch 590, training loss: 0.08224693685770035 = 0.012177499011158943 + 0.01 * 7.006944179534912
Epoch 590, val loss: 1.1690597534179688
Epoch 600, training loss: 0.08152709901332855 = 0.011466574855148792 + 0.01 * 7.006052494049072
Epoch 600, val loss: 1.1792114973068237
Epoch 610, training loss: 0.08080306649208069 = 0.010819138959050179 + 0.01 * 6.9983930587768555
Epoch 610, val loss: 1.1891252994537354
Epoch 620, training loss: 0.08013830333948135 = 0.010228007100522518 + 0.01 * 6.991029739379883
Epoch 620, val loss: 1.198670744895935
Epoch 630, training loss: 0.07963930070400238 = 0.00968701671808958 + 0.01 * 6.9952287673950195
Epoch 630, val loss: 1.2079238891601562
Epoch 640, training loss: 0.07903582602739334 = 0.009191364049911499 + 0.01 * 6.984446048736572
Epoch 640, val loss: 1.2167878150939941
Epoch 650, training loss: 0.07857274264097214 = 0.008736041374504566 + 0.01 * 6.983670234680176
Epoch 650, val loss: 1.2253799438476562
Epoch 660, training loss: 0.07794637978076935 = 0.008316786028444767 + 0.01 * 6.9629597663879395
Epoch 660, val loss: 1.233769416809082
Epoch 670, training loss: 0.07788887619972229 = 0.007929430343210697 + 0.01 * 6.995944976806641
Epoch 670, val loss: 1.2418794631958008
Epoch 680, training loss: 0.07702764123678207 = 0.00757165951654315 + 0.01 * 6.945598125457764
Epoch 680, val loss: 1.2496609687805176
Epoch 690, training loss: 0.076663538813591 = 0.007240163628011942 + 0.01 * 6.942337989807129
Epoch 690, val loss: 1.2571111917495728
Epoch 700, training loss: 0.07640894502401352 = 0.006932413205504417 + 0.01 * 6.947652816772461
Epoch 700, val loss: 1.2645255327224731
Epoch 710, training loss: 0.07597091794013977 = 0.006646273657679558 + 0.01 * 6.932464122772217
Epoch 710, val loss: 1.2715306282043457
Epoch 720, training loss: 0.07560423016548157 = 0.0063795181922614574 + 0.01 * 6.92247200012207
Epoch 720, val loss: 1.2783410549163818
Epoch 730, training loss: 0.07531121373176575 = 0.0061305612325668335 + 0.01 * 6.918065071105957
Epoch 730, val loss: 1.285061240196228
Epoch 740, training loss: 0.07502743601799011 = 0.005897922907024622 + 0.01 * 6.912951469421387
Epoch 740, val loss: 1.2913426160812378
Epoch 750, training loss: 0.07493295520544052 = 0.005680365487933159 + 0.01 * 6.925259113311768
Epoch 750, val loss: 1.2977268695831299
Epoch 760, training loss: 0.0744411051273346 = 0.005476456135511398 + 0.01 * 6.896464824676514
Epoch 760, val loss: 1.3036861419677734
Epoch 770, training loss: 0.07436399161815643 = 0.005284566432237625 + 0.01 * 6.907943248748779
Epoch 770, val loss: 1.3095589876174927
Epoch 780, training loss: 0.0741337239742279 = 0.005104553420096636 + 0.01 * 6.90291690826416
Epoch 780, val loss: 1.3152953386306763
Epoch 790, training loss: 0.07384064793586731 = 0.004934600554406643 + 0.01 * 6.890604496002197
Epoch 790, val loss: 1.3208285570144653
Epoch 800, training loss: 0.07362615317106247 = 0.004774953238666058 + 0.01 * 6.885120391845703
Epoch 800, val loss: 1.326162576675415
Epoch 810, training loss: 0.07332421839237213 = 0.004624100401997566 + 0.01 * 6.870011806488037
Epoch 810, val loss: 1.3314048051834106
Epoch 820, training loss: 0.07320087403059006 = 0.004481284413486719 + 0.01 * 6.871959209442139
Epoch 820, val loss: 1.3365275859832764
Epoch 830, training loss: 0.0730220302939415 = 0.004346021451056004 + 0.01 * 6.86760139465332
Epoch 830, val loss: 1.3414466381072998
Epoch 840, training loss: 0.07289042323827744 = 0.0042184218764305115 + 0.01 * 6.8672003746032715
Epoch 840, val loss: 1.34632408618927
Epoch 850, training loss: 0.0727236270904541 = 0.00409689312800765 + 0.01 * 6.862673282623291
Epoch 850, val loss: 1.3510276079177856
Epoch 860, training loss: 0.07261838018894196 = 0.003982256166636944 + 0.01 * 6.863612651824951
Epoch 860, val loss: 1.355444312095642
Epoch 870, training loss: 0.0724639892578125 = 0.0038733023684471846 + 0.01 * 6.859068870544434
Epoch 870, val loss: 1.359990119934082
Epoch 880, training loss: 0.07218432426452637 = 0.003769099013879895 + 0.01 * 6.841522693634033
Epoch 880, val loss: 1.3643025159835815
Epoch 890, training loss: 0.07215277850627899 = 0.0036703527439385653 + 0.01 * 6.848243236541748
Epoch 890, val loss: 1.3685383796691895
Epoch 900, training loss: 0.07203569263219833 = 0.003576300572603941 + 0.01 * 6.845939636230469
Epoch 900, val loss: 1.3727083206176758
Epoch 910, training loss: 0.07182314246892929 = 0.0034865224733948708 + 0.01 * 6.833662509918213
Epoch 910, val loss: 1.376641035079956
Epoch 920, training loss: 0.07165833562612534 = 0.0034009094815701246 + 0.01 * 6.825743198394775
Epoch 920, val loss: 1.3806451559066772
Epoch 930, training loss: 0.07167404890060425 = 0.0033192038536071777 + 0.01 * 6.835484981536865
Epoch 930, val loss: 1.3844037055969238
Epoch 940, training loss: 0.07149520516395569 = 0.0032411040738224983 + 0.01 * 6.825409889221191
Epoch 940, val loss: 1.3881069421768188
Epoch 950, training loss: 0.07133902609348297 = 0.003166358917951584 + 0.01 * 6.817266464233398
Epoch 950, val loss: 1.3917009830474854
Epoch 960, training loss: 0.07130895555019379 = 0.003094881074503064 + 0.01 * 6.821407794952393
Epoch 960, val loss: 1.3952412605285645
Epoch 970, training loss: 0.07118349522352219 = 0.003026495687663555 + 0.01 * 6.815700531005859
Epoch 970, val loss: 1.3986390829086304
Epoch 980, training loss: 0.07096631824970245 = 0.002960928250104189 + 0.01 * 6.800539493560791
Epoch 980, val loss: 1.4020684957504272
Epoch 990, training loss: 0.07112737745046616 = 0.002897868864238262 + 0.01 * 6.822951316833496
Epoch 990, val loss: 1.4052178859710693
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 2.042809247970581 = 1.956841230392456 + 0.01 * 8.596810340881348
Epoch 0, val loss: 1.948289394378662
Epoch 10, training loss: 2.031381130218506 = 1.945413589477539 + 0.01 * 8.59675121307373
Epoch 10, val loss: 1.9371088743209839
Epoch 20, training loss: 2.0172536373138428 = 1.9312880039215088 + 0.01 * 8.596552848815918
Epoch 20, val loss: 1.9233688116073608
Epoch 30, training loss: 1.9975658655166626 = 1.9116069078445435 + 0.01 * 8.595901489257812
Epoch 30, val loss: 1.9044418334960938
Epoch 40, training loss: 1.9689258337020874 = 1.8830056190490723 + 0.01 * 8.592019081115723
Epoch 40, val loss: 1.8776379823684692
Epoch 50, training loss: 1.9300211668014526 = 1.8443795442581177 + 0.01 * 8.564160346984863
Epoch 50, val loss: 1.8438822031021118
Epoch 60, training loss: 1.8890247344970703 = 1.80478036403656 + 0.01 * 8.424437522888184
Epoch 60, val loss: 1.81423020362854
Epoch 70, training loss: 1.857227087020874 = 1.7745085954666138 + 0.01 * 8.271849632263184
Epoch 70, val loss: 1.7917709350585938
Epoch 80, training loss: 1.8178094625473022 = 1.7368916273117065 + 0.01 * 8.091787338256836
Epoch 80, val loss: 1.7562029361724854
Epoch 90, training loss: 1.7636330127716064 = 1.6851145029067993 + 0.01 * 7.851846694946289
Epoch 90, val loss: 1.7093859910964966
Epoch 100, training loss: 1.6908502578735352 = 1.6142523288726807 + 0.01 * 7.659789562225342
Epoch 100, val loss: 1.6492574214935303
Epoch 110, training loss: 1.6039522886276245 = 1.5290746688842773 + 0.01 * 7.487765312194824
Epoch 110, val loss: 1.5782873630523682
Epoch 120, training loss: 1.5158144235610962 = 1.4420231580734253 + 0.01 * 7.379123210906982
Epoch 120, val loss: 1.508203148841858
Epoch 130, training loss: 1.4334779977798462 = 1.360357403755188 + 0.01 * 7.312058448791504
Epoch 130, val loss: 1.444196343421936
Epoch 140, training loss: 1.3550360202789307 = 1.2821820974349976 + 0.01 * 7.285395622253418
Epoch 140, val loss: 1.3847230672836304
Epoch 150, training loss: 1.2783175706863403 = 1.2055907249450684 + 0.01 * 7.272683143615723
Epoch 150, val loss: 1.3273892402648926
Epoch 160, training loss: 1.202279806137085 = 1.12968909740448 + 0.01 * 7.259066104888916
Epoch 160, val loss: 1.2709457874298096
Epoch 170, training loss: 1.1266473531723022 = 1.054230809211731 + 0.01 * 7.241656303405762
Epoch 170, val loss: 1.2150954008102417
Epoch 180, training loss: 1.0517311096191406 = 0.9795079827308655 + 0.01 * 7.222307205200195
Epoch 180, val loss: 1.1608425378799438
Epoch 190, training loss: 0.9783716201782227 = 0.9063299298286438 + 0.01 * 7.20416784286499
Epoch 190, val loss: 1.1095846891403198
Epoch 200, training loss: 0.9067493081092834 = 0.834876537322998 + 0.01 * 7.187276840209961
Epoch 200, val loss: 1.062132477760315
Epoch 210, training loss: 0.8360166549682617 = 0.7643022537231445 + 0.01 * 7.171443462371826
Epoch 210, val loss: 1.0182921886444092
Epoch 220, training loss: 0.766176700592041 = 0.694541335105896 + 0.01 * 7.163538932800293
Epoch 220, val loss: 0.9773139953613281
Epoch 230, training loss: 0.6986410617828369 = 0.6271758079528809 + 0.01 * 7.146528244018555
Epoch 230, val loss: 0.9401634335517883
Epoch 240, training loss: 0.6357001662254333 = 0.5643863081932068 + 0.01 * 7.131385803222656
Epoch 240, val loss: 0.9079954624176025
Epoch 250, training loss: 0.5788552165031433 = 0.507625162601471 + 0.01 * 7.123007774353027
Epoch 250, val loss: 0.8816956877708435
Epoch 260, training loss: 0.528030276298523 = 0.45691075921058655 + 0.01 * 7.111952781677246
Epoch 260, val loss: 0.8613124489784241
Epoch 270, training loss: 0.4823768138885498 = 0.4111967384815216 + 0.01 * 7.118007183074951
Epoch 270, val loss: 0.8460684418678284
Epoch 280, training loss: 0.44047829508781433 = 0.36942631006240845 + 0.01 * 7.105197906494141
Epoch 280, val loss: 0.8351759910583496
Epoch 290, training loss: 0.40183335542678833 = 0.3308767080307007 + 0.01 * 7.095664024353027
Epoch 290, val loss: 0.8281520009040833
Epoch 300, training loss: 0.3662368655204773 = 0.29526597261428833 + 0.01 * 7.097090721130371
Epoch 300, val loss: 0.8247089982032776
Epoch 310, training loss: 0.33334407210350037 = 0.2624872922897339 + 0.01 * 7.085677623748779
Epoch 310, val loss: 0.8246675729751587
Epoch 320, training loss: 0.3031684160232544 = 0.232366144657135 + 0.01 * 7.080227851867676
Epoch 320, val loss: 0.8276508450508118
Epoch 330, training loss: 0.27552032470703125 = 0.2047872245311737 + 0.01 * 7.07330846786499
Epoch 330, val loss: 0.8332990407943726
Epoch 340, training loss: 0.25065693259239197 = 0.17979510128498077 + 0.01 * 7.086184024810791
Epoch 340, val loss: 0.8412766456604004
Epoch 350, training loss: 0.2281496226787567 = 0.15748395025730133 + 0.01 * 7.066567897796631
Epoch 350, val loss: 0.8513586521148682
Epoch 360, training loss: 0.2085212618112564 = 0.13788698613643646 + 0.01 * 7.063427448272705
Epoch 360, val loss: 0.8632978796958923
Epoch 370, training loss: 0.19143518805503845 = 0.12081844359636307 + 0.01 * 7.06167459487915
Epoch 370, val loss: 0.8768990635871887
Epoch 380, training loss: 0.17662456631660461 = 0.10604339838027954 + 0.01 * 7.0581159591674805
Epoch 380, val loss: 0.8920598030090332
Epoch 390, training loss: 0.16382604837417603 = 0.09329105168581009 + 0.01 * 7.053499698638916
Epoch 390, val loss: 0.9083847999572754
Epoch 400, training loss: 0.15269726514816284 = 0.08228161185979843 + 0.01 * 7.04156494140625
Epoch 400, val loss: 0.925564169883728
Epoch 410, training loss: 0.14313849806785583 = 0.07276616245508194 + 0.01 * 7.037234783172607
Epoch 410, val loss: 0.9433703422546387
Epoch 420, training loss: 0.1349639892578125 = 0.06452536582946777 + 0.01 * 7.043862819671631
Epoch 420, val loss: 0.9615797400474548
Epoch 430, training loss: 0.12770922482013702 = 0.05739510804414749 + 0.01 * 7.031411647796631
Epoch 430, val loss: 0.979843258857727
Epoch 440, training loss: 0.1215042769908905 = 0.05122369900345802 + 0.01 * 7.02805757522583
Epoch 440, val loss: 0.9980908632278442
Epoch 450, training loss: 0.11621269583702087 = 0.04587789997458458 + 0.01 * 7.0334792137146
Epoch 450, val loss: 1.0161011219024658
Epoch 460, training loss: 0.11137747019529343 = 0.04124709963798523 + 0.01 * 7.013037204742432
Epoch 460, val loss: 1.0337581634521484
Epoch 470, training loss: 0.10742267966270447 = 0.037224914878606796 + 0.01 * 7.019776344299316
Epoch 470, val loss: 1.050966739654541
Epoch 480, training loss: 0.10388113558292389 = 0.03373175486922264 + 0.01 * 7.0149383544921875
Epoch 480, val loss: 1.0676945447921753
Epoch 490, training loss: 0.10065584629774094 = 0.030685117468237877 + 0.01 * 6.997073173522949
Epoch 490, val loss: 1.0837984085083008
Epoch 500, training loss: 0.0978822186589241 = 0.028020024299621582 + 0.01 * 6.986219882965088
Epoch 500, val loss: 1.0994322299957275
Epoch 510, training loss: 0.09577707946300507 = 0.025678683072328568 + 0.01 * 7.009840488433838
Epoch 510, val loss: 1.1144750118255615
Epoch 520, training loss: 0.09352603554725647 = 0.0236199963837862 + 0.01 * 6.990604400634766
Epoch 520, val loss: 1.1290500164031982
Epoch 530, training loss: 0.09153936058282852 = 0.02179856412112713 + 0.01 * 6.974079608917236
Epoch 530, val loss: 1.1430706977844238
Epoch 540, training loss: 0.08989739418029785 = 0.020180940628051758 + 0.01 * 6.971645832061768
Epoch 540, val loss: 1.1566082239151
Epoch 550, training loss: 0.08834771811962128 = 0.018739236518740654 + 0.01 * 6.960847854614258
Epoch 550, val loss: 1.1697449684143066
Epoch 560, training loss: 0.08698888123035431 = 0.01744949445128441 + 0.01 * 6.9539384841918945
Epoch 560, val loss: 1.1823676824569702
Epoch 570, training loss: 0.08587932586669922 = 0.016293048858642578 + 0.01 * 6.958628177642822
Epoch 570, val loss: 1.1946262121200562
Epoch 580, training loss: 0.0854191705584526 = 0.015251240693032742 + 0.01 * 7.016793251037598
Epoch 580, val loss: 1.206404447555542
Epoch 590, training loss: 0.08388974517583847 = 0.014312141574919224 + 0.01 * 6.957760810852051
Epoch 590, val loss: 1.2177826166152954
Epoch 600, training loss: 0.08276024460792542 = 0.013461091555655003 + 0.01 * 6.929915428161621
Epoch 600, val loss: 1.2287495136260986
Epoch 610, training loss: 0.08191648125648499 = 0.012686374597251415 + 0.01 * 6.923011302947998
Epoch 610, val loss: 1.239423394203186
Epoch 620, training loss: 0.08117086440324783 = 0.0119804497808218 + 0.01 * 6.919041633605957
Epoch 620, val loss: 1.2496733665466309
Epoch 630, training loss: 0.08039253205060959 = 0.011335870251059532 + 0.01 * 6.905666351318359
Epoch 630, val loss: 1.2597053050994873
Epoch 640, training loss: 0.08018940687179565 = 0.010744953528046608 + 0.01 * 6.944445610046387
Epoch 640, val loss: 1.2692557573318481
Epoch 650, training loss: 0.07923812419176102 = 0.010204190388321877 + 0.01 * 6.903393745422363
Epoch 650, val loss: 1.2785848379135132
Epoch 660, training loss: 0.07862705737352371 = 0.009705999866127968 + 0.01 * 6.892106056213379
Epoch 660, val loss: 1.2875874042510986
Epoch 670, training loss: 0.07829068601131439 = 0.009246012195944786 + 0.01 * 6.904467582702637
Epoch 670, val loss: 1.2963004112243652
Epoch 680, training loss: 0.07807459682226181 = 0.008820928633213043 + 0.01 * 6.9253668785095215
Epoch 680, val loss: 1.3048436641693115
Epoch 690, training loss: 0.07737463712692261 = 0.008428347297012806 + 0.01 * 6.894629001617432
Epoch 690, val loss: 1.312991976737976
Epoch 700, training loss: 0.07683607190847397 = 0.00806370284408331 + 0.01 * 6.877237319946289
Epoch 700, val loss: 1.3208650350570679
Epoch 710, training loss: 0.07644831389188766 = 0.007724651135504246 + 0.01 * 6.872366428375244
Epoch 710, val loss: 1.328673243522644
Epoch 720, training loss: 0.07612814009189606 = 0.007408309727907181 + 0.01 * 6.871983051300049
Epoch 720, val loss: 1.3361924886703491
Epoch 730, training loss: 0.07582952082157135 = 0.0071131386794149876 + 0.01 * 6.871638774871826
Epoch 730, val loss: 1.343600869178772
Epoch 740, training loss: 0.07543366402387619 = 0.006837477441877127 + 0.01 * 6.859618663787842
Epoch 740, val loss: 1.3506522178649902
Epoch 750, training loss: 0.07518220692873001 = 0.006579297594726086 + 0.01 * 6.860291004180908
Epoch 750, val loss: 1.35760498046875
Epoch 760, training loss: 0.07514265179634094 = 0.006337244529277086 + 0.01 * 6.880540370941162
Epoch 760, val loss: 1.3643320798873901
Epoch 770, training loss: 0.07468007504940033 = 0.006110703572630882 + 0.01 * 6.856936931610107
Epoch 770, val loss: 1.370901107788086
Epoch 780, training loss: 0.07438018172979355 = 0.005897472146898508 + 0.01 * 6.848271369934082
Epoch 780, val loss: 1.377211332321167
Epoch 790, training loss: 0.0740763247013092 = 0.00569663243368268 + 0.01 * 6.837969779968262
Epoch 790, val loss: 1.3834478855133057
Epoch 800, training loss: 0.0739290788769722 = 0.005507224705070257 + 0.01 * 6.842185974121094
Epoch 800, val loss: 1.3895422220230103
Epoch 810, training loss: 0.07358813285827637 = 0.0053286198526620865 + 0.01 * 6.82595157623291
Epoch 810, val loss: 1.3953520059585571
Epoch 820, training loss: 0.07346542179584503 = 0.005159824155271053 + 0.01 * 6.830559730529785
Epoch 820, val loss: 1.4011521339416504
Epoch 830, training loss: 0.07322901487350464 = 0.005000085104256868 + 0.01 * 6.822893142700195
Epoch 830, val loss: 1.4066983461380005
Epoch 840, training loss: 0.07302777469158173 = 0.0048491074703633785 + 0.01 * 6.817866802215576
Epoch 840, val loss: 1.4122004508972168
Epoch 850, training loss: 0.0729188323020935 = 0.004705843515694141 + 0.01 * 6.821299076080322
Epoch 850, val loss: 1.41748046875
Epoch 860, training loss: 0.07268241792917252 = 0.004570274613797665 + 0.01 * 6.811214447021484
Epoch 860, val loss: 1.4226577281951904
Epoch 870, training loss: 0.07281836867332458 = 0.004441332537680864 + 0.01 * 6.837703704833984
Epoch 870, val loss: 1.4276808500289917
Epoch 880, training loss: 0.07233894616365433 = 0.004319145809859037 + 0.01 * 6.801980018615723
Epoch 880, val loss: 1.4326999187469482
Epoch 890, training loss: 0.07236209511756897 = 0.004202691372483969 + 0.01 * 6.8159403800964355
Epoch 890, val loss: 1.43742835521698
Epoch 900, training loss: 0.07196938991546631 = 0.004091883543878794 + 0.01 * 6.787751197814941
Epoch 900, val loss: 1.4422378540039062
Epoch 910, training loss: 0.07223550975322723 = 0.003986341413110495 + 0.01 * 6.824916839599609
Epoch 910, val loss: 1.4466623067855835
Epoch 920, training loss: 0.07171732187271118 = 0.003885620040819049 + 0.01 * 6.783170700073242
Epoch 920, val loss: 1.4512903690338135
Epoch 930, training loss: 0.07207485288381577 = 0.003789125708863139 + 0.01 * 6.828572750091553
Epoch 930, val loss: 1.4555354118347168
Epoch 940, training loss: 0.07148975878953934 = 0.0036976588889956474 + 0.01 * 6.779210090637207
Epoch 940, val loss: 1.4599162340164185
Epoch 950, training loss: 0.07137696444988251 = 0.0036098016425967216 + 0.01 * 6.776716709136963
Epoch 950, val loss: 1.464064598083496
Epoch 960, training loss: 0.07139617204666138 = 0.003525950014591217 + 0.01 * 6.787022590637207
Epoch 960, val loss: 1.468186855316162
Epoch 970, training loss: 0.07126697897911072 = 0.003445553360506892 + 0.01 * 6.782142639160156
Epoch 970, val loss: 1.4722102880477905
Epoch 980, training loss: 0.07094833254814148 = 0.0033686524257063866 + 0.01 * 6.757967948913574
Epoch 980, val loss: 1.4761195182800293
Epoch 990, training loss: 0.0709647387266159 = 0.0032950208988040686 + 0.01 * 6.766972064971924
Epoch 990, val loss: 1.479933738708496
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8007380073800738
The final CL Acc:0.76790, 0.00761, The final GNN Acc:0.80285, 0.00375
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13232])
remove edge: torch.Size([2, 7886])
updated graph: torch.Size([2, 10562])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.038179874420166 = 1.9522113800048828 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.942315936088562
Epoch 10, training loss: 2.0269787311553955 = 1.9410107135772705 + 0.01 * 8.596797943115234
Epoch 10, val loss: 1.9317737817764282
Epoch 20, training loss: 2.0132110118865967 = 1.927245020866394 + 0.01 * 8.596599578857422
Epoch 20, val loss: 1.9185962677001953
Epoch 30, training loss: 1.9941965341567993 = 1.9082380533218384 + 0.01 * 8.595843315124512
Epoch 30, val loss: 1.9002662897109985
Epoch 40, training loss: 1.9667109251022339 = 1.8807988166809082 + 0.01 * 8.591208457946777
Epoch 40, val loss: 1.8742384910583496
Epoch 50, training loss: 1.9290597438812256 = 1.8434770107269287 + 0.01 * 8.558267593383789
Epoch 50, val loss: 1.8409260511398315
Epoch 60, training loss: 1.8863939046859741 = 1.8023035526275635 + 0.01 * 8.40903377532959
Epoch 60, val loss: 1.8085240125656128
Epoch 70, training loss: 1.8469780683517456 = 1.7646900415420532 + 0.01 * 8.228806495666504
Epoch 70, val loss: 1.779768466949463
Epoch 80, training loss: 1.7981542348861694 = 1.7171471118927002 + 0.01 * 8.1007080078125
Epoch 80, val loss: 1.7358213663101196
Epoch 90, training loss: 1.7298567295074463 = 1.6501097679138184 + 0.01 * 7.974692344665527
Epoch 90, val loss: 1.6737182140350342
Epoch 100, training loss: 1.641331672668457 = 1.562475562095642 + 0.01 * 7.8856072425842285
Epoch 100, val loss: 1.5977551937103271
Epoch 110, training loss: 1.543294072151184 = 1.466001033782959 + 0.01 * 7.729308605194092
Epoch 110, val loss: 1.5173834562301636
Epoch 120, training loss: 1.4483435153961182 = 1.3728445768356323 + 0.01 * 7.549896717071533
Epoch 120, val loss: 1.4427577257156372
Epoch 130, training loss: 1.362419843673706 = 1.2874447107315063 + 0.01 * 7.497518539428711
Epoch 130, val loss: 1.377949833869934
Epoch 140, training loss: 1.2834726572036743 = 1.2089241743087769 + 0.01 * 7.454848766326904
Epoch 140, val loss: 1.3201849460601807
Epoch 150, training loss: 1.2082374095916748 = 1.1338844299316406 + 0.01 * 7.435299873352051
Epoch 150, val loss: 1.2657626867294312
Epoch 160, training loss: 1.1319754123687744 = 1.0578349828720093 + 0.01 * 7.4140448570251465
Epoch 160, val loss: 1.211124062538147
Epoch 170, training loss: 1.0519251823425293 = 0.9780817031860352 + 0.01 * 7.384348392486572
Epoch 170, val loss: 1.1515294313430786
Epoch 180, training loss: 0.9688889980316162 = 0.8954444527626038 + 0.01 * 7.344452381134033
Epoch 180, val loss: 1.0886634588241577
Epoch 190, training loss: 0.8865063786506653 = 0.8135246634483337 + 0.01 * 7.298173904418945
Epoch 190, val loss: 1.0251498222351074
Epoch 200, training loss: 0.8088549375534058 = 0.7362223863601685 + 0.01 * 7.263254642486572
Epoch 200, val loss: 0.9657045602798462
Epoch 210, training loss: 0.7378357648849487 = 0.6654942631721497 + 0.01 * 7.234151840209961
Epoch 210, val loss: 0.913170337677002
Epoch 220, training loss: 0.6732726693153381 = 0.6011171340942383 + 0.01 * 7.215551376342773
Epoch 220, val loss: 0.8689215183258057
Epoch 230, training loss: 0.6141806244850159 = 0.5421358942985535 + 0.01 * 7.204473495483398
Epoch 230, val loss: 0.8330603837966919
Epoch 240, training loss: 0.5600045323371887 = 0.4880470335483551 + 0.01 * 7.195749282836914
Epoch 240, val loss: 0.8053001165390015
Epoch 250, training loss: 0.5104168653488159 = 0.43854713439941406 + 0.01 * 7.186973571777344
Epoch 250, val loss: 0.7848944664001465
Epoch 260, training loss: 0.4649003744125366 = 0.39310258626937866 + 0.01 * 7.17978048324585
Epoch 260, val loss: 0.7707551121711731
Epoch 270, training loss: 0.42261645197868347 = 0.3508853614330292 + 0.01 * 7.173109531402588
Epoch 270, val loss: 0.7615950703620911
Epoch 280, training loss: 0.38289716839790344 = 0.3112558126449585 + 0.01 * 7.164135932922363
Epoch 280, val loss: 0.7561855912208557
Epoch 290, training loss: 0.34585288166999817 = 0.2742465138435364 + 0.01 * 7.160635948181152
Epoch 290, val loss: 0.7538548707962036
Epoch 300, training loss: 0.3119594156742096 = 0.24045519530773163 + 0.01 * 7.150422096252441
Epoch 300, val loss: 0.7545932531356812
Epoch 310, training loss: 0.28190115094184875 = 0.21049147844314575 + 0.01 * 7.14096736907959
Epoch 310, val loss: 0.7585198879241943
Epoch 320, training loss: 0.2558876574039459 = 0.18454235792160034 + 0.01 * 7.134529113769531
Epoch 320, val loss: 0.7655773162841797
Epoch 330, training loss: 0.23362287878990173 = 0.16232971847057343 + 0.01 * 7.1293158531188965
Epoch 330, val loss: 0.7753428816795349
Epoch 340, training loss: 0.21454989910125732 = 0.14333857595920563 + 0.01 * 7.121131896972656
Epoch 340, val loss: 0.7872682213783264
Epoch 350, training loss: 0.19820770621299744 = 0.12703877687454224 + 0.01 * 7.116893291473389
Epoch 350, val loss: 0.8008486032485962
Epoch 360, training loss: 0.18406103551387787 = 0.1129734218120575 + 0.01 * 7.108761787414551
Epoch 360, val loss: 0.8155897259712219
Epoch 370, training loss: 0.17178115248680115 = 0.10076848417520523 + 0.01 * 7.101266860961914
Epoch 370, val loss: 0.8310826420783997
Epoch 380, training loss: 0.16121906042099 = 0.09012478590011597 + 0.01 * 7.109428405761719
Epoch 380, val loss: 0.8470687866210938
Epoch 390, training loss: 0.15173780918121338 = 0.08081255108118057 + 0.01 * 7.092525959014893
Epoch 390, val loss: 0.8632732033729553
Epoch 400, training loss: 0.14349526166915894 = 0.07263178378343582 + 0.01 * 7.0863471031188965
Epoch 400, val loss: 0.8796051740646362
Epoch 410, training loss: 0.1362217217683792 = 0.06542458385229111 + 0.01 * 7.079714298248291
Epoch 410, val loss: 0.8959535956382751
Epoch 420, training loss: 0.12988150119781494 = 0.05905378237366676 + 0.01 * 7.0827717781066895
Epoch 420, val loss: 0.9122270345687866
Epoch 430, training loss: 0.12414759397506714 = 0.05341982841491699 + 0.01 * 7.072776794433594
Epoch 430, val loss: 0.9282944202423096
Epoch 440, training loss: 0.11909587681293488 = 0.04842516407370567 + 0.01 * 7.067070960998535
Epoch 440, val loss: 0.9441381096839905
Epoch 450, training loss: 0.1146053820848465 = 0.04399218037724495 + 0.01 * 7.061320781707764
Epoch 450, val loss: 0.9596602916717529
Epoch 460, training loss: 0.11066731065511703 = 0.04005460441112518 + 0.01 * 7.061270713806152
Epoch 460, val loss: 0.9748601913452148
Epoch 470, training loss: 0.10712185502052307 = 0.036554139107465744 + 0.01 * 7.056771755218506
Epoch 470, val loss: 0.9897052049636841
Epoch 480, training loss: 0.10387011617422104 = 0.03343993425369263 + 0.01 * 7.043018341064453
Epoch 480, val loss: 1.0041451454162598
Epoch 490, training loss: 0.10119626671075821 = 0.03066468983888626 + 0.01 * 7.053157806396484
Epoch 490, val loss: 1.0181851387023926
Epoch 500, training loss: 0.09856699407100677 = 0.02818860486149788 + 0.01 * 7.037838935852051
Epoch 500, val loss: 1.0318022966384888
Epoch 510, training loss: 0.09628809988498688 = 0.025975899770855904 + 0.01 * 7.031220436096191
Epoch 510, val loss: 1.0449999570846558
Epoch 520, training loss: 0.09431413561105728 = 0.023995017632842064 + 0.01 * 7.031911849975586
Epoch 520, val loss: 1.0578049421310425
Epoch 530, training loss: 0.09243717789649963 = 0.022218292579054832 + 0.01 * 7.021888256072998
Epoch 530, val loss: 1.0701968669891357
Epoch 540, training loss: 0.09080527722835541 = 0.020621374249458313 + 0.01 * 7.01839017868042
Epoch 540, val loss: 1.0822045803070068
Epoch 550, training loss: 0.08931567519903183 = 0.019183488562703133 + 0.01 * 7.013218879699707
Epoch 550, val loss: 1.0938231945037842
Epoch 560, training loss: 0.08793014287948608 = 0.017886240035295486 + 0.01 * 7.004390716552734
Epoch 560, val loss: 1.1050482988357544
Epoch 570, training loss: 0.0867551863193512 = 0.016712984070181847 + 0.01 * 7.004220485687256
Epoch 570, val loss: 1.1158894300460815
Epoch 580, training loss: 0.08564639091491699 = 0.015650302171707153 + 0.01 * 6.999608993530273
Epoch 580, val loss: 1.1263774633407593
Epoch 590, training loss: 0.08461987972259521 = 0.014685292728245258 + 0.01 * 6.9934587478637695
Epoch 590, val loss: 1.1365383863449097
Epoch 600, training loss: 0.08374089747667313 = 0.013806357979774475 + 0.01 * 6.9934539794921875
Epoch 600, val loss: 1.1463690996170044
Epoch 610, training loss: 0.08301091939210892 = 0.013002709485590458 + 0.01 * 7.000821590423584
Epoch 610, val loss: 1.1559154987335205
Epoch 620, training loss: 0.08213892579078674 = 0.012266170233488083 + 0.01 * 6.987275123596191
Epoch 620, val loss: 1.1651571989059448
Epoch 630, training loss: 0.08131479471921921 = 0.011587833985686302 + 0.01 * 6.972695827484131
Epoch 630, val loss: 1.174146056175232
Epoch 640, training loss: 0.08087610453367233 = 0.010962056927382946 + 0.01 * 6.991405487060547
Epoch 640, val loss: 1.1828914880752563
Epoch 650, training loss: 0.08002602308988571 = 0.01038417685776949 + 0.01 * 6.964184284210205
Epoch 650, val loss: 1.1913436651229858
Epoch 660, training loss: 0.07949235290288925 = 0.00984946172684431 + 0.01 * 6.964289665222168
Epoch 660, val loss: 1.1995676755905151
Epoch 670, training loss: 0.07898111641407013 = 0.009354207664728165 + 0.01 * 6.962691307067871
Epoch 670, val loss: 1.207589030265808
Epoch 680, training loss: 0.07839065790176392 = 0.008895158767700195 + 0.01 * 6.949550151824951
Epoch 680, val loss: 1.2153669595718384
Epoch 690, training loss: 0.0779634639620781 = 0.008470273576676846 + 0.01 * 6.949319362640381
Epoch 690, val loss: 1.2228811979293823
Epoch 700, training loss: 0.07758095115423203 = 0.008075423538684845 + 0.01 * 6.950552940368652
Epoch 700, val loss: 1.230132818222046
Epoch 710, training loss: 0.07712290436029434 = 0.007707839831709862 + 0.01 * 6.941506385803223
Epoch 710, val loss: 1.2371313571929932
Epoch 720, training loss: 0.07668672502040863 = 0.007366097532212734 + 0.01 * 6.932063102722168
Epoch 720, val loss: 1.2439050674438477
Epoch 730, training loss: 0.07644852995872498 = 0.007047623861581087 + 0.01 * 6.940090656280518
Epoch 730, val loss: 1.250478982925415
Epoch 740, training loss: 0.0760919526219368 = 0.006750506814569235 + 0.01 * 6.934144496917725
Epoch 740, val loss: 1.2568053007125854
Epoch 750, training loss: 0.07573381066322327 = 0.006473045330494642 + 0.01 * 6.926076889038086
Epoch 750, val loss: 1.262947916984558
Epoch 760, training loss: 0.07542087137699127 = 0.006213487591594458 + 0.01 * 6.920738697052002
Epoch 760, val loss: 1.2688913345336914
Epoch 770, training loss: 0.07511334121227264 = 0.005970786325633526 + 0.01 * 6.914255619049072
Epoch 770, val loss: 1.2746294736862183
Epoch 780, training loss: 0.07492399960756302 = 0.005743327084928751 + 0.01 * 6.918067455291748
Epoch 780, val loss: 1.2801895141601562
Epoch 790, training loss: 0.07473069429397583 = 0.005530275870114565 + 0.01 * 6.920042037963867
Epoch 790, val loss: 1.285582423210144
Epoch 800, training loss: 0.0744362473487854 = 0.005329928360879421 + 0.01 * 6.910632133483887
Epoch 800, val loss: 1.2907991409301758
Epoch 810, training loss: 0.07414466142654419 = 0.0051417276263237 + 0.01 * 6.900293350219727
Epoch 810, val loss: 1.2958743572235107
Epoch 820, training loss: 0.07391432672739029 = 0.004964331164956093 + 0.01 * 6.894999980926514
Epoch 820, val loss: 1.3007960319519043
Epoch 830, training loss: 0.07380124926567078 = 0.004796942230314016 + 0.01 * 6.900430679321289
Epoch 830, val loss: 1.3055799007415771
Epoch 840, training loss: 0.07368531078100204 = 0.004639570601284504 + 0.01 * 6.904573917388916
Epoch 840, val loss: 1.3101658821105957
Epoch 850, training loss: 0.07342223823070526 = 0.004491012543439865 + 0.01 * 6.893123149871826
Epoch 850, val loss: 1.3146615028381348
Epoch 860, training loss: 0.07311898469924927 = 0.0043504564091563225 + 0.01 * 6.8768534660339355
Epoch 860, val loss: 1.319038987159729
Epoch 870, training loss: 0.0731603130698204 = 0.004217481706291437 + 0.01 * 6.894282817840576
Epoch 870, val loss: 1.3233124017715454
Epoch 880, training loss: 0.07296577095985413 = 0.004091572016477585 + 0.01 * 6.887419700622559
Epoch 880, val loss: 1.3274269104003906
Epoch 890, training loss: 0.07260555773973465 = 0.003972148522734642 + 0.01 * 6.863340854644775
Epoch 890, val loss: 1.3314677476882935
Epoch 900, training loss: 0.072574682533741 = 0.0038584235589951277 + 0.01 * 6.871625900268555
Epoch 900, val loss: 1.335439682006836
Epoch 910, training loss: 0.07234163582324982 = 0.003750905627384782 + 0.01 * 6.859073162078857
Epoch 910, val loss: 1.3391995429992676
Epoch 920, training loss: 0.07223425805568695 = 0.0036485446617007256 + 0.01 * 6.858572006225586
Epoch 920, val loss: 1.3429406881332397
Epoch 930, training loss: 0.0721922293305397 = 0.0035513134207576513 + 0.01 * 6.864091873168945
Epoch 930, val loss: 1.3465195894241333
Epoch 940, training loss: 0.07196477800607681 = 0.0034590368159115314 + 0.01 * 6.850574493408203
Epoch 940, val loss: 1.3499789237976074
Epoch 950, training loss: 0.07220516353845596 = 0.0033709127455949783 + 0.01 * 6.883425235748291
Epoch 950, val loss: 1.3534077405929565
Epoch 960, training loss: 0.07182274758815765 = 0.0032868580892682076 + 0.01 * 6.853589057922363
Epoch 960, val loss: 1.3566869497299194
Epoch 970, training loss: 0.07158865034580231 = 0.003206761321052909 + 0.01 * 6.838189125061035
Epoch 970, val loss: 1.3599013090133667
Epoch 980, training loss: 0.0715043842792511 = 0.003130244556814432 + 0.01 * 6.837414264678955
Epoch 980, val loss: 1.36302649974823
Epoch 990, training loss: 0.07132261246442795 = 0.003056866582483053 + 0.01 * 6.826574325561523
Epoch 990, val loss: 1.3660919666290283
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.025160551071167 = 1.939192533493042 + 0.01 * 8.596809387207031
Epoch 0, val loss: 1.9390008449554443
Epoch 10, training loss: 2.014402151107788 = 1.92843496799469 + 0.01 * 8.596726417541504
Epoch 10, val loss: 1.9282945394515991
Epoch 20, training loss: 2.001145362854004 = 1.9151808023452759 + 0.01 * 8.596467018127441
Epoch 20, val loss: 1.9145931005477905
Epoch 30, training loss: 1.982595443725586 = 1.8966400623321533 + 0.01 * 8.595535278320312
Epoch 30, val loss: 1.8950508832931519
Epoch 40, training loss: 1.9559012651443481 = 1.8700053691864014 + 0.01 * 8.589591026306152
Epoch 40, val loss: 1.867048740386963
Epoch 50, training loss: 1.9203814268112183 = 1.8348631858825684 + 0.01 * 8.551823616027832
Epoch 50, val loss: 1.8320777416229248
Epoch 60, training loss: 1.8818798065185547 = 1.7981438636779785 + 0.01 * 8.373595237731934
Epoch 60, val loss: 1.8003654479980469
Epoch 70, training loss: 1.8449065685272217 = 1.7630949020385742 + 0.01 * 8.181171417236328
Epoch 70, val loss: 1.7735823392868042
Epoch 80, training loss: 1.7952197790145874 = 1.716457724571228 + 0.01 * 7.876204490661621
Epoch 80, val loss: 1.7348244190216064
Epoch 90, training loss: 1.7279143333435059 = 1.6513452529907227 + 0.01 * 7.656903266906738
Epoch 90, val loss: 1.6766271591186523
Epoch 100, training loss: 1.6406556367874146 = 1.564647912979126 + 0.01 * 7.600771903991699
Epoch 100, val loss: 1.601466178894043
Epoch 110, training loss: 1.5383225679397583 = 1.4626553058624268 + 0.01 * 7.56673002243042
Epoch 110, val loss: 1.5183764696121216
Epoch 120, training loss: 1.43234121799469 = 1.3569122552871704 + 0.01 * 7.542891979217529
Epoch 120, val loss: 1.4350093603134155
Epoch 130, training loss: 1.33026921749115 = 1.2551679611206055 + 0.01 * 7.510123252868652
Epoch 130, val loss: 1.3568105697631836
Epoch 140, training loss: 1.2346209287643433 = 1.1600548028945923 + 0.01 * 7.456616401672363
Epoch 140, val loss: 1.2827839851379395
Epoch 150, training loss: 1.144455075263977 = 1.0705952644348145 + 0.01 * 7.385984897613525
Epoch 150, val loss: 1.2122269868850708
Epoch 160, training loss: 1.0576694011688232 = 0.9843180179595947 + 0.01 * 7.335133075714111
Epoch 160, val loss: 1.1439659595489502
Epoch 170, training loss: 0.9731384515762329 = 0.9002010822296143 + 0.01 * 7.2937397956848145
Epoch 170, val loss: 1.0788872241973877
Epoch 180, training loss: 0.8923243880271912 = 0.8196824789047241 + 0.01 * 7.264192581176758
Epoch 180, val loss: 1.0194717645645142
Epoch 190, training loss: 0.8177124857902527 = 0.7453206777572632 + 0.01 * 7.239182472229004
Epoch 190, val loss: 0.9676816463470459
Epoch 200, training loss: 0.7511334419250488 = 0.6789929270744324 + 0.01 * 7.21405029296875
Epoch 200, val loss: 0.9254217147827148
Epoch 210, training loss: 0.6927292346954346 = 0.6207737922668457 + 0.01 * 7.195542335510254
Epoch 210, val loss: 0.8931563496589661
Epoch 220, training loss: 0.6410796046257019 = 0.5694281458854675 + 0.01 * 7.165144920349121
Epoch 220, val loss: 0.8699356317520142
Epoch 230, training loss: 0.5951026678085327 = 0.5236462354660034 + 0.01 * 7.145641326904297
Epoch 230, val loss: 0.8540703058242798
Epoch 240, training loss: 0.553641676902771 = 0.4823444187641144 + 0.01 * 7.129728317260742
Epoch 240, val loss: 0.8436923623085022
Epoch 250, training loss: 0.5156340003013611 = 0.44444406032562256 + 0.01 * 7.1189961433410645
Epoch 250, val loss: 0.8370900750160217
Epoch 260, training loss: 0.4800368547439575 = 0.4090268313884735 + 0.01 * 7.101001262664795
Epoch 260, val loss: 0.8334051370620728
Epoch 270, training loss: 0.44658008217811584 = 0.37568214535713196 + 0.01 * 7.089793682098389
Epoch 270, val loss: 0.8325602412223816
Epoch 280, training loss: 0.4158009886741638 = 0.34462395310401917 + 0.01 * 7.117703914642334
Epoch 280, val loss: 0.8349418640136719
Epoch 290, training loss: 0.3871321976184845 = 0.3162817656993866 + 0.01 * 7.085042476654053
Epoch 290, val loss: 0.8404420018196106
Epoch 300, training loss: 0.3613361716270447 = 0.29063886404037476 + 0.01 * 7.069731712341309
Epoch 300, val loss: 0.8484992980957031
Epoch 310, training loss: 0.3379817008972168 = 0.2673521637916565 + 0.01 * 7.062954425811768
Epoch 310, val loss: 0.8587044477462769
Epoch 320, training loss: 0.31661295890808105 = 0.245881587266922 + 0.01 * 7.073138236999512
Epoch 320, val loss: 0.8708460927009583
Epoch 330, training loss: 0.296134352684021 = 0.22558505833148956 + 0.01 * 7.054931163787842
Epoch 330, val loss: 0.88463294506073
Epoch 340, training loss: 0.27627164125442505 = 0.2057730108499527 + 0.01 * 7.049864292144775
Epoch 340, val loss: 0.8995576500892639
Epoch 350, training loss: 0.2565023899078369 = 0.1860722452402115 + 0.01 * 7.043015003204346
Epoch 350, val loss: 0.9156175255775452
Epoch 360, training loss: 0.23729616403579712 = 0.1666892021894455 + 0.01 * 7.060696601867676
Epoch 360, val loss: 0.9330732226371765
Epoch 370, training loss: 0.21872296929359436 = 0.14838148653507233 + 0.01 * 7.034149169921875
Epoch 370, val loss: 0.9522898197174072
Epoch 380, training loss: 0.20218777656555176 = 0.13186019659042358 + 0.01 * 7.032758712768555
Epoch 380, val loss: 0.9734207987785339
Epoch 390, training loss: 0.18766610324382782 = 0.1174079179763794 + 0.01 * 7.025818824768066
Epoch 390, val loss: 0.9963214993476868
Epoch 400, training loss: 0.17519593238830566 = 0.10489055514335632 + 0.01 * 7.030538558959961
Epoch 400, val loss: 1.0207542181015015
Epoch 410, training loss: 0.16415204107761383 = 0.09405644983053207 + 0.01 * 7.009559154510498
Epoch 410, val loss: 1.046238899230957
Epoch 420, training loss: 0.15469315648078918 = 0.08462750911712646 + 0.01 * 7.006564617156982
Epoch 420, val loss: 1.0723963975906372
Epoch 430, training loss: 0.14633646607398987 = 0.07638391107320786 + 0.01 * 6.995255470275879
Epoch 430, val loss: 1.0988364219665527
Epoch 440, training loss: 0.1391514092683792 = 0.06914935261011124 + 0.01 * 7.000205993652344
Epoch 440, val loss: 1.1252987384796143
Epoch 450, training loss: 0.13261672854423523 = 0.06277603656053543 + 0.01 * 6.984070301055908
Epoch 450, val loss: 1.151544213294983
Epoch 460, training loss: 0.12691234052181244 = 0.05713871493935585 + 0.01 * 6.977363109588623
Epoch 460, val loss: 1.1773942708969116
Epoch 470, training loss: 0.12178312242031097 = 0.05213717371225357 + 0.01 * 6.964594841003418
Epoch 470, val loss: 1.2027252912521362
Epoch 480, training loss: 0.11741085350513458 = 0.04769287258386612 + 0.01 * 6.971797943115234
Epoch 480, val loss: 1.2273292541503906
Epoch 490, training loss: 0.11328847706317902 = 0.043729063123464584 + 0.01 * 6.955942153930664
Epoch 490, val loss: 1.2512567043304443
Epoch 500, training loss: 0.10965730249881744 = 0.04018694534897804 + 0.01 * 6.947036266326904
Epoch 500, val loss: 1.2744274139404297
Epoch 510, training loss: 0.1064143106341362 = 0.03701449930667877 + 0.01 * 6.939980983734131
Epoch 510, val loss: 1.2968655824661255
Epoch 520, training loss: 0.10357403755187988 = 0.03416580334305763 + 0.01 * 6.940823554992676
Epoch 520, val loss: 1.318565845489502
Epoch 530, training loss: 0.10086853802204132 = 0.031604643911123276 + 0.01 * 6.926389694213867
Epoch 530, val loss: 1.3395246267318726
Epoch 540, training loss: 0.09855347126722336 = 0.029296999797225 + 0.01 * 6.925646781921387
Epoch 540, val loss: 1.3598012924194336
Epoch 550, training loss: 0.09641636908054352 = 0.0272129625082016 + 0.01 * 6.9203410148620605
Epoch 550, val loss: 1.3794422149658203
Epoch 560, training loss: 0.09439694881439209 = 0.02532866783440113 + 0.01 * 6.9068284034729
Epoch 560, val loss: 1.3983758687973022
Epoch 570, training loss: 0.09278461337089539 = 0.023621724918484688 + 0.01 * 6.916289329528809
Epoch 570, val loss: 1.4167139530181885
Epoch 580, training loss: 0.0911003053188324 = 0.022073427215218544 + 0.01 * 6.902688503265381
Epoch 580, val loss: 1.43442702293396
Epoch 590, training loss: 0.08954939246177673 = 0.020664609968662262 + 0.01 * 6.8884782791137695
Epoch 590, val loss: 1.4515827894210815
Epoch 600, training loss: 0.08822639286518097 = 0.01938064582645893 + 0.01 * 6.884574890136719
Epoch 600, val loss: 1.4682505130767822
Epoch 610, training loss: 0.08704515546560287 = 0.018209515139460564 + 0.01 * 6.883563995361328
Epoch 610, val loss: 1.48441481590271
Epoch 620, training loss: 0.08592700213193893 = 0.017139052972197533 + 0.01 * 6.878795146942139
Epoch 620, val loss: 1.5000731945037842
Epoch 630, training loss: 0.08500458300113678 = 0.01615854725241661 + 0.01 * 6.884603023529053
Epoch 630, val loss: 1.5152764320373535
Epoch 640, training loss: 0.08390769362449646 = 0.01525944285094738 + 0.01 * 6.8648247718811035
Epoch 640, val loss: 1.5299925804138184
Epoch 650, training loss: 0.08321383595466614 = 0.014432957395911217 + 0.01 * 6.878087997436523
Epoch 650, val loss: 1.5442639589309692
Epoch 660, training loss: 0.08222722262144089 = 0.013672985136508942 + 0.01 * 6.855423927307129
Epoch 660, val loss: 1.5581227540969849
Epoch 670, training loss: 0.08160100877285004 = 0.012971715070307255 + 0.01 * 6.862929344177246
Epoch 670, val loss: 1.5716029405593872
Epoch 680, training loss: 0.08074372261762619 = 0.012324812822043896 + 0.01 * 6.841891765594482
Epoch 680, val loss: 1.5846803188323975
Epoch 690, training loss: 0.08033826947212219 = 0.011726390570402145 + 0.01 * 6.861187934875488
Epoch 690, val loss: 1.5974258184432983
Epoch 700, training loss: 0.07970493286848068 = 0.011172029189765453 + 0.01 * 6.853290557861328
Epoch 700, val loss: 1.6098072528839111
Epoch 710, training loss: 0.07899800688028336 = 0.010657940991222858 + 0.01 * 6.8340067863464355
Epoch 710, val loss: 1.6218723058700562
Epoch 720, training loss: 0.07862701267004013 = 0.0101797329261899 + 0.01 * 6.844727993011475
Epoch 720, val loss: 1.6336538791656494
Epoch 730, training loss: 0.07806403934955597 = 0.00973513349890709 + 0.01 * 6.83289098739624
Epoch 730, val loss: 1.645009160041809
Epoch 740, training loss: 0.07759560644626617 = 0.009320760145783424 + 0.01 * 6.827485084533691
Epoch 740, val loss: 1.656137228012085
Epoch 750, training loss: 0.07715433835983276 = 0.00893386173993349 + 0.01 * 6.822047710418701
Epoch 750, val loss: 1.6669960021972656
Epoch 760, training loss: 0.07689687609672546 = 0.00857261661440134 + 0.01 * 6.83242654800415
Epoch 760, val loss: 1.6775201559066772
Epoch 770, training loss: 0.07629436254501343 = 0.008234702050685883 + 0.01 * 6.805965900421143
Epoch 770, val loss: 1.6877806186676025
Epoch 780, training loss: 0.07620270550251007 = 0.007917682640254498 + 0.01 * 6.828502655029297
Epoch 780, val loss: 1.6978051662445068
Epoch 790, training loss: 0.07573983073234558 = 0.0076205115765333176 + 0.01 * 6.81193208694458
Epoch 790, val loss: 1.707509160041809
Epoch 800, training loss: 0.07535296678543091 = 0.007341282442212105 + 0.01 * 6.801168918609619
Epoch 800, val loss: 1.7170552015304565
Epoch 810, training loss: 0.07517165690660477 = 0.007078500930219889 + 0.01 * 6.8093156814575195
Epoch 810, val loss: 1.7263652086257935
Epoch 820, training loss: 0.07474735379219055 = 0.00683140754699707 + 0.01 * 6.791594505310059
Epoch 820, val loss: 1.7354148626327515
Epoch 830, training loss: 0.07460962980985641 = 0.006598058622330427 + 0.01 * 6.801157474517822
Epoch 830, val loss: 1.7443431615829468
Epoch 840, training loss: 0.07441206276416779 = 0.006378128658980131 + 0.01 * 6.803393363952637
Epoch 840, val loss: 1.7530661821365356
Epoch 850, training loss: 0.07409606128931046 = 0.006170197855681181 + 0.01 * 6.792586326599121
Epoch 850, val loss: 1.7615437507629395
Epoch 860, training loss: 0.07384175807237625 = 0.005973340943455696 + 0.01 * 6.786842346191406
Epoch 860, val loss: 1.769854187965393
Epoch 870, training loss: 0.07354030013084412 = 0.005787197034806013 + 0.01 * 6.77531099319458
Epoch 870, val loss: 1.7779711484909058
Epoch 880, training loss: 0.07343394309282303 = 0.005610543768852949 + 0.01 * 6.7823405265808105
Epoch 880, val loss: 1.785921335220337
Epoch 890, training loss: 0.073226697742939 = 0.005443407688289881 + 0.01 * 6.778328895568848
Epoch 890, val loss: 1.7936506271362305
Epoch 900, training loss: 0.07300610840320587 = 0.005284327082335949 + 0.01 * 6.772178649902344
Epoch 900, val loss: 1.8012133836746216
Epoch 910, training loss: 0.07292547076940536 = 0.005133508238941431 + 0.01 * 6.779196262359619
Epoch 910, val loss: 1.808564305305481
Epoch 920, training loss: 0.07259906083345413 = 0.004990081302821636 + 0.01 * 6.760898590087891
Epoch 920, val loss: 1.8158068656921387
Epoch 930, training loss: 0.0725494995713234 = 0.004853464663028717 + 0.01 * 6.769603729248047
Epoch 930, val loss: 1.8229092359542847
Epoch 940, training loss: 0.07263489067554474 = 0.004723384510725737 + 0.01 * 6.79115104675293
Epoch 940, val loss: 1.8298454284667969
Epoch 950, training loss: 0.07235798239707947 = 0.004599588457494974 + 0.01 * 6.775839805603027
Epoch 950, val loss: 1.8365330696105957
Epoch 960, training loss: 0.07197610288858414 = 0.0044811987318098545 + 0.01 * 6.749490737915039
Epoch 960, val loss: 1.8432282209396362
Epoch 970, training loss: 0.07205557078123093 = 0.004368056543171406 + 0.01 * 6.768751621246338
Epoch 970, val loss: 1.8498058319091797
Epoch 980, training loss: 0.07183576375246048 = 0.00426013907417655 + 0.01 * 6.757562637329102
Epoch 980, val loss: 1.8561128377914429
Epoch 990, training loss: 0.07172857224941254 = 0.004157014191150665 + 0.01 * 6.757155895233154
Epoch 990, val loss: 1.8623378276824951
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8508170795993675
=== training gcn model ===
Epoch 0, training loss: 2.025228977203369 = 1.9392606019973755 + 0.01 * 8.59682846069336
Epoch 0, val loss: 1.9444584846496582
Epoch 10, training loss: 2.0156092643737793 = 1.9296417236328125 + 0.01 * 8.596760749816895
Epoch 10, val loss: 1.9352093935012817
Epoch 20, training loss: 2.0038516521453857 = 1.9178863763809204 + 0.01 * 8.59653091430664
Epoch 20, val loss: 1.9234868288040161
Epoch 30, training loss: 1.9874989986419678 = 1.9015408754348755 + 0.01 * 8.595807075500488
Epoch 30, val loss: 1.906916856765747
Epoch 40, training loss: 1.9636729955673218 = 1.8777532577514648 + 0.01 * 8.591975212097168
Epoch 40, val loss: 1.882994532585144
Epoch 50, training loss: 1.9304776191711426 = 1.844805359840393 + 0.01 * 8.567232131958008
Epoch 50, val loss: 1.8511289358139038
Epoch 60, training loss: 1.8916335105895996 = 1.807072401046753 + 0.01 * 8.456114768981934
Epoch 60, val loss: 1.8174827098846436
Epoch 70, training loss: 1.8556891679763794 = 1.7731074094772339 + 0.01 * 8.258172988891602
Epoch 70, val loss: 1.78812837600708
Epoch 80, training loss: 1.8145052194595337 = 1.7336992025375366 + 0.01 * 8.080607414245605
Epoch 80, val loss: 1.7512098550796509
Epoch 90, training loss: 1.7563903331756592 = 1.679255485534668 + 0.01 * 7.7134904861450195
Epoch 90, val loss: 1.7017289400100708
Epoch 100, training loss: 1.6801656484603882 = 1.6055703163146973 + 0.01 * 7.459532260894775
Epoch 100, val loss: 1.6379283666610718
Epoch 110, training loss: 1.586151123046875 = 1.512473702430725 + 0.01 * 7.367740631103516
Epoch 110, val loss: 1.5595686435699463
Epoch 120, training loss: 1.479882836341858 = 1.4067093133926392 + 0.01 * 7.31735372543335
Epoch 120, val loss: 1.4706413745880127
Epoch 130, training loss: 1.3698335886001587 = 1.2969638109207153 + 0.01 * 7.286972999572754
Epoch 130, val loss: 1.3791062831878662
Epoch 140, training loss: 1.2621155977249146 = 1.1895341873168945 + 0.01 * 7.2581377029418945
Epoch 140, val loss: 1.288494348526001
Epoch 150, training loss: 1.159421443939209 = 1.0870800018310547 + 0.01 * 7.234142780303955
Epoch 150, val loss: 1.201266884803772
Epoch 160, training loss: 1.062696099281311 = 0.9905869364738464 + 0.01 * 7.210911273956299
Epoch 160, val loss: 1.1191318035125732
Epoch 170, training loss: 0.9720721244812012 = 0.9002156853675842 + 0.01 * 7.185644626617432
Epoch 170, val loss: 1.0431053638458252
Epoch 180, training loss: 0.8872213959693909 = 0.8155942559242249 + 0.01 * 7.16271448135376
Epoch 180, val loss: 0.9733852744102478
Epoch 190, training loss: 0.808142364025116 = 0.7366470694541931 + 0.01 * 7.149529457092285
Epoch 190, val loss: 0.9108095169067383
Epoch 200, training loss: 0.735295295715332 = 0.6638554334640503 + 0.01 * 7.143988609313965
Epoch 200, val loss: 0.8566400408744812
Epoch 210, training loss: 0.6688754558563232 = 0.5974525809288025 + 0.01 * 7.142285346984863
Epoch 210, val loss: 0.8116598725318909
Epoch 220, training loss: 0.6084831953048706 = 0.5370733737945557 + 0.01 * 7.140981197357178
Epoch 220, val loss: 0.7750667929649353
Epoch 230, training loss: 0.5534651279449463 = 0.48207128047943115 + 0.01 * 7.139382839202881
Epoch 230, val loss: 0.7460741400718689
Epoch 240, training loss: 0.5034153461456299 = 0.4320378005504608 + 0.01 * 7.13775634765625
Epoch 240, val loss: 0.723962128162384
Epoch 250, training loss: 0.45827123522758484 = 0.38690659403800964 + 0.01 * 7.136463165283203
Epoch 250, val loss: 0.7079499363899231
Epoch 260, training loss: 0.41800299286842346 = 0.3466569781303406 + 0.01 * 7.13460111618042
Epoch 260, val loss: 0.6973118782043457
Epoch 270, training loss: 0.38234376907348633 = 0.31101444363594055 + 0.01 * 7.132933616638184
Epoch 270, val loss: 0.6912522912025452
Epoch 280, training loss: 0.3506523370742798 = 0.2793427109718323 + 0.01 * 7.130962371826172
Epoch 280, val loss: 0.688919186592102
Epoch 290, training loss: 0.32204654812812805 = 0.25075021386146545 + 0.01 * 7.129632949829102
Epoch 290, val loss: 0.6893647313117981
Epoch 300, training loss: 0.2955520451068878 = 0.22428442537784576 + 0.01 * 7.126762390136719
Epoch 300, val loss: 0.6917051672935486
Epoch 310, training loss: 0.27043893933296204 = 0.19919747114181519 + 0.01 * 7.124147891998291
Epoch 310, val loss: 0.6952341794967651
Epoch 320, training loss: 0.24636465311050415 = 0.17514127492904663 + 0.01 * 7.12233829498291
Epoch 320, val loss: 0.6995185613632202
Epoch 330, training loss: 0.22347816824913025 = 0.15230171382427216 + 0.01 * 7.11764669418335
Epoch 330, val loss: 0.704285204410553
Epoch 340, training loss: 0.2023364007472992 = 0.13118474185466766 + 0.01 * 7.115165710449219
Epoch 340, val loss: 0.7096542716026306
Epoch 350, training loss: 0.1834094524383545 = 0.11231619864702225 + 0.01 * 7.1093244552612305
Epoch 350, val loss: 0.7157760858535767
Epoch 360, training loss: 0.1670604944229126 = 0.09600924700498581 + 0.01 * 7.105125427246094
Epoch 360, val loss: 0.7229253053665161
Epoch 370, training loss: 0.15325528383255005 = 0.08226924389600754 + 0.01 * 7.098605155944824
Epoch 370, val loss: 0.7310332655906677
Epoch 380, training loss: 0.14178308844566345 = 0.07086753845214844 + 0.01 * 7.091554164886475
Epoch 380, val loss: 0.7399320006370544
Epoch 390, training loss: 0.132271870970726 = 0.06147029250860214 + 0.01 * 7.080158233642578
Epoch 390, val loss: 0.7493225336074829
Epoch 400, training loss: 0.12445735931396484 = 0.053723037242889404 + 0.01 * 7.073432445526123
Epoch 400, val loss: 0.7590921521186829
Epoch 410, training loss: 0.11790242046117783 = 0.047308094799518585 + 0.01 * 7.059432506561279
Epoch 410, val loss: 0.7689455151557922
Epoch 420, training loss: 0.11246179789304733 = 0.04195594787597656 + 0.01 * 7.05058479309082
Epoch 420, val loss: 0.7787787914276123
Epoch 430, training loss: 0.10788434743881226 = 0.03745407983660698 + 0.01 * 7.043026447296143
Epoch 430, val loss: 0.7885426878929138
Epoch 440, training loss: 0.1039869487285614 = 0.03363712131977081 + 0.01 * 7.034983158111572
Epoch 440, val loss: 0.7981833219528198
Epoch 450, training loss: 0.10066264867782593 = 0.03037327155470848 + 0.01 * 7.028938293457031
Epoch 450, val loss: 0.8076660633087158
Epoch 460, training loss: 0.09774918854236603 = 0.027561914175748825 + 0.01 * 7.018727779388428
Epoch 460, val loss: 0.8169107437133789
Epoch 470, training loss: 0.09530128538608551 = 0.02512228675186634 + 0.01 * 7.017900466918945
Epoch 470, val loss: 0.8259557485580444
Epoch 480, training loss: 0.09313364326953888 = 0.02299192175269127 + 0.01 * 7.0141730308532715
Epoch 480, val loss: 0.8347554206848145
Epoch 490, training loss: 0.09113646298646927 = 0.021122388541698456 + 0.01 * 7.001407623291016
Epoch 490, val loss: 0.843298614025116
Epoch 500, training loss: 0.08942872285842896 = 0.019471243023872375 + 0.01 * 6.995748043060303
Epoch 500, val loss: 0.8516259789466858
Epoch 510, training loss: 0.08792474865913391 = 0.018006669357419014 + 0.01 * 6.99180793762207
Epoch 510, val loss: 0.8597495555877686
Epoch 520, training loss: 0.0864882543683052 = 0.016703331843018532 + 0.01 * 6.978492259979248
Epoch 520, val loss: 0.8675798773765564
Epoch 530, training loss: 0.0853225365281105 = 0.01553738210350275 + 0.01 * 6.978515625
Epoch 530, val loss: 0.8751986026763916
Epoch 540, training loss: 0.0843319445848465 = 0.014490656554698944 + 0.01 * 6.984128952026367
Epoch 540, val loss: 0.8825817108154297
Epoch 550, training loss: 0.08319244533777237 = 0.013548932038247585 + 0.01 * 6.964351654052734
Epoch 550, val loss: 0.8897528052330017
Epoch 560, training loss: 0.08228439837694168 = 0.012697628699243069 + 0.01 * 6.958676815032959
Epoch 560, val loss: 0.8967171311378479
Epoch 570, training loss: 0.08144131302833557 = 0.011924782767891884 + 0.01 * 6.951653480529785
Epoch 570, val loss: 0.9034794569015503
Epoch 580, training loss: 0.0806988850235939 = 0.011222914792597294 + 0.01 * 6.947597026824951
Epoch 580, val loss: 0.9100890159606934
Epoch 590, training loss: 0.08007095009088516 = 0.01058312226086855 + 0.01 * 6.948782920837402
Epoch 590, val loss: 0.9164009094238281
Epoch 600, training loss: 0.07934802025556564 = 0.009997772984206676 + 0.01 * 6.935025215148926
Epoch 600, val loss: 0.9225603938102722
Epoch 610, training loss: 0.07899637520313263 = 0.00946024153381586 + 0.01 * 6.953613758087158
Epoch 610, val loss: 0.9286036491394043
Epoch 620, training loss: 0.07829858362674713 = 0.008966336958110332 + 0.01 * 6.933225154876709
Epoch 620, val loss: 0.9343727231025696
Epoch 630, training loss: 0.07773181051015854 = 0.008511053398251534 + 0.01 * 6.922076225280762
Epoch 630, val loss: 0.9400307536125183
Epoch 640, training loss: 0.07852582633495331 = 0.0080908527597785 + 0.01 * 7.043497085571289
Epoch 640, val loss: 0.9455412030220032
Epoch 650, training loss: 0.0769478976726532 = 0.007703255861997604 + 0.01 * 6.924464225769043
Epoch 650, val loss: 0.9508413672447205
Epoch 660, training loss: 0.07639671862125397 = 0.00734544126316905 + 0.01 * 6.905128479003906
Epoch 660, val loss: 0.9560211300849915
Epoch 670, training loss: 0.07603078335523605 = 0.007012301590293646 + 0.01 * 6.901848793029785
Epoch 670, val loss: 0.9610947370529175
Epoch 680, training loss: 0.07564104348421097 = 0.006703099701553583 + 0.01 * 6.893794059753418
Epoch 680, val loss: 0.9660035371780396
Epoch 690, training loss: 0.07525641471147537 = 0.00641534011811018 + 0.01 * 6.88410758972168
Epoch 690, val loss: 0.9707545638084412
Epoch 700, training loss: 0.07501951605081558 = 0.006147316191345453 + 0.01 * 6.8872199058532715
Epoch 700, val loss: 0.9754546880722046
Epoch 710, training loss: 0.07469704002141953 = 0.005896951537579298 + 0.01 * 6.880009651184082
Epoch 710, val loss: 0.9799354672431946
Epoch 720, training loss: 0.07442344725131989 = 0.005663407035171986 + 0.01 * 6.876003742218018
Epoch 720, val loss: 0.9843488931655884
Epoch 730, training loss: 0.07422909140586853 = 0.0054452912881970406 + 0.01 * 6.878379821777344
Epoch 730, val loss: 0.9886187314987183
Epoch 740, training loss: 0.07382484525442123 = 0.005240889266133308 + 0.01 * 6.858396053314209
Epoch 740, val loss: 0.9927257299423218
Epoch 750, training loss: 0.07394279539585114 = 0.005049056839197874 + 0.01 * 6.889374256134033
Epoch 750, val loss: 0.9967549443244934
Epoch 760, training loss: 0.07335327565670013 = 0.004869090858846903 + 0.01 * 6.848419189453125
Epoch 760, val loss: 1.0007202625274658
Epoch 770, training loss: 0.07328320294618607 = 0.0047008260153234005 + 0.01 * 6.858238220214844
Epoch 770, val loss: 1.0045527219772339
Epoch 780, training loss: 0.07304220646619797 = 0.0045421067625284195 + 0.01 * 6.850010395050049
Epoch 780, val loss: 1.0081552267074585
Epoch 790, training loss: 0.07269801199436188 = 0.004392348695546389 + 0.01 * 6.830566883087158
Epoch 790, val loss: 1.011745810508728
Epoch 800, training loss: 0.07259070873260498 = 0.004251257050782442 + 0.01 * 6.833945274353027
Epoch 800, val loss: 1.015312910079956
Epoch 810, training loss: 0.07236267626285553 = 0.004117732401937246 + 0.01 * 6.824494361877441
Epoch 810, val loss: 1.0186262130737305
Epoch 820, training loss: 0.07273004204034805 = 0.003991253208369017 + 0.01 * 6.8738789558410645
Epoch 820, val loss: 1.0220391750335693
Epoch 830, training loss: 0.07217568904161453 = 0.003872351488098502 + 0.01 * 6.830334186553955
Epoch 830, val loss: 1.0252288579940796
Epoch 840, training loss: 0.07187791168689728 = 0.003759615123271942 + 0.01 * 6.811830043792725
Epoch 840, val loss: 1.0283082723617554
Epoch 850, training loss: 0.07176260650157928 = 0.003652499057352543 + 0.01 * 6.81101131439209
Epoch 850, val loss: 1.0314592123031616
Epoch 860, training loss: 0.07162018120288849 = 0.0035508109722286463 + 0.01 * 6.806936740875244
Epoch 860, val loss: 1.034349799156189
Epoch 870, training loss: 0.07159353792667389 = 0.0034542561043053865 + 0.01 * 6.813928604125977
Epoch 870, val loss: 1.0372806787490845
Epoch 880, training loss: 0.07130166888237 = 0.0033622542396187782 + 0.01 * 6.793941497802734
Epoch 880, val loss: 1.040106177330017
Epoch 890, training loss: 0.07115423679351807 = 0.003274784190580249 + 0.01 * 6.787945747375488
Epoch 890, val loss: 1.0427439212799072
Epoch 900, training loss: 0.07106667011976242 = 0.003191549563780427 + 0.01 * 6.787511825561523
Epoch 900, val loss: 1.0455986261367798
Epoch 910, training loss: 0.07110545784235 = 0.003112304024398327 + 0.01 * 6.799315452575684
Epoch 910, val loss: 1.0480546951293945
Epoch 920, training loss: 0.07104547321796417 = 0.0030366512946784496 + 0.01 * 6.800882816314697
Epoch 920, val loss: 1.050626277923584
Epoch 930, training loss: 0.07082439213991165 = 0.0029645294416695833 + 0.01 * 6.78598690032959
Epoch 930, val loss: 1.053105354309082
Epoch 940, training loss: 0.07067755609750748 = 0.002895768964663148 + 0.01 * 6.7781782150268555
Epoch 940, val loss: 1.0553995370864868
Epoch 950, training loss: 0.07057976722717285 = 0.0028298096731305122 + 0.01 * 6.774995803833008
Epoch 950, val loss: 1.0578455924987793
Epoch 960, training loss: 0.07055884599685669 = 0.002766737947240472 + 0.01 * 6.779211521148682
Epoch 960, val loss: 1.0600907802581787
Epoch 970, training loss: 0.07045847177505493 = 0.0027064087335020304 + 0.01 * 6.775206565856934
Epoch 970, val loss: 1.0623462200164795
Epoch 980, training loss: 0.07030762732028961 = 0.0026488855946809053 + 0.01 * 6.76587438583374
Epoch 980, val loss: 1.0644874572753906
Epoch 990, training loss: 0.07066065818071365 = 0.0025934537407010794 + 0.01 * 6.806720733642578
Epoch 990, val loss: 1.066651463508606
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8376383763837639
The final CL Acc:0.78395, 0.01062, The final GNN Acc:0.84115, 0.00692
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10588])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.033534526824951 = 1.9475666284561157 + 0.01 * 8.59679889678955
Epoch 0, val loss: 1.9572622776031494
Epoch 10, training loss: 2.0230255126953125 = 1.937058687210083 + 0.01 * 8.596671104431152
Epoch 10, val loss: 1.9460002183914185
Epoch 20, training loss: 2.0095126628875732 = 1.9235494136810303 + 0.01 * 8.596327781677246
Epoch 20, val loss: 1.9312536716461182
Epoch 30, training loss: 1.990485668182373 = 1.9045357704162598 + 0.01 * 8.594995498657227
Epoch 30, val loss: 1.9104220867156982
Epoch 40, training loss: 1.9629329442977905 = 1.8770695924758911 + 0.01 * 8.58634090423584
Epoch 40, val loss: 1.8808635473251343
Epoch 50, training loss: 1.9261566400527954 = 1.8407503366470337 + 0.01 * 8.540627479553223
Epoch 50, val loss: 1.844041109085083
Epoch 60, training loss: 1.8878248929977417 = 1.8045587539672852 + 0.01 * 8.326611518859863
Epoch 60, val loss: 1.8117502927780151
Epoch 70, training loss: 1.8568824529647827 = 1.7750943899154663 + 0.01 * 8.17880916595459
Epoch 70, val loss: 1.786476492881775
Epoch 80, training loss: 1.8151732683181763 = 1.7353719472885132 + 0.01 * 7.980132579803467
Epoch 80, val loss: 1.7508162260055542
Epoch 90, training loss: 1.7572598457336426 = 1.6793129444122314 + 0.01 * 7.7946858406066895
Epoch 90, val loss: 1.7036617994308472
Epoch 100, training loss: 1.6789888143539429 = 1.6027319431304932 + 0.01 * 7.625690937042236
Epoch 100, val loss: 1.640982985496521
Epoch 110, training loss: 1.5868686437606812 = 1.511764407157898 + 0.01 * 7.5104217529296875
Epoch 110, val loss: 1.5663162469863892
Epoch 120, training loss: 1.491527795791626 = 1.4167299270629883 + 0.01 * 7.479786396026611
Epoch 120, val loss: 1.4901565313339233
Epoch 130, training loss: 1.3983466625213623 = 1.3239312171936035 + 0.01 * 7.441550254821777
Epoch 130, val loss: 1.419425368309021
Epoch 140, training loss: 1.3087531328201294 = 1.2346168756484985 + 0.01 * 7.413623332977295
Epoch 140, val loss: 1.3549739122390747
Epoch 150, training loss: 1.2221332788467407 = 1.148331880569458 + 0.01 * 7.380143642425537
Epoch 150, val loss: 1.2953484058380127
Epoch 160, training loss: 1.1371498107910156 = 1.0638461112976074 + 0.01 * 7.330368518829346
Epoch 160, val loss: 1.238331913948059
Epoch 170, training loss: 1.0526893138885498 = 0.9800417423248291 + 0.01 * 7.264758110046387
Epoch 170, val loss: 1.1823152303695679
Epoch 180, training loss: 0.969509482383728 = 0.897339403629303 + 0.01 * 7.217007160186768
Epoch 180, val loss: 1.126818299293518
Epoch 190, training loss: 0.8888804912567139 = 0.8170014023780823 + 0.01 * 7.187911033630371
Epoch 190, val loss: 1.0725184679031372
Epoch 200, training loss: 0.812682569026947 = 0.7409661412239075 + 0.01 * 7.171642780303955
Epoch 200, val loss: 1.0213617086410522
Epoch 210, training loss: 0.7428150177001953 = 0.6712241172790527 + 0.01 * 7.159092903137207
Epoch 210, val loss: 0.9759951233863831
Epoch 220, training loss: 0.6801266670227051 = 0.6086310744285583 + 0.01 * 7.149558067321777
Epoch 220, val loss: 0.9383504390716553
Epoch 230, training loss: 0.624019980430603 = 0.5525925755500793 + 0.01 * 7.1427388191223145
Epoch 230, val loss: 0.9091164469718933
Epoch 240, training loss: 0.5734401345252991 = 0.5020617246627808 + 0.01 * 7.137840747833252
Epoch 240, val loss: 0.8879544734954834
Epoch 250, training loss: 0.5273756980895996 = 0.4560500681400299 + 0.01 * 7.132564067840576
Epoch 250, val loss: 0.873419463634491
Epoch 260, training loss: 0.4851502478122711 = 0.4138231575489044 + 0.01 * 7.132708549499512
Epoch 260, val loss: 0.8638489842414856
Epoch 270, training loss: 0.44619858264923096 = 0.3749397397041321 + 0.01 * 7.125884056091309
Epoch 270, val loss: 0.8583109378814697
Epoch 280, training loss: 0.41016101837158203 = 0.3389523923397064 + 0.01 * 7.120861530303955
Epoch 280, val loss: 0.8559633493423462
Epoch 290, training loss: 0.3765704929828644 = 0.30540668964385986 + 0.01 * 7.116380214691162
Epoch 290, val loss: 0.8562071919441223
Epoch 300, training loss: 0.34505414962768555 = 0.2739318311214447 + 0.01 * 7.112233638763428
Epoch 300, val loss: 0.8585737347602844
Epoch 310, training loss: 0.31542670726776123 = 0.24436555802822113 + 0.01 * 7.106116771697998
Epoch 310, val loss: 0.8627772331237793
Epoch 320, training loss: 0.287901371717453 = 0.21676360070705414 + 0.01 * 7.113776683807373
Epoch 320, val loss: 0.8687189221382141
Epoch 330, training loss: 0.2622438669204712 = 0.19128401577472687 + 0.01 * 7.095986366271973
Epoch 330, val loss: 0.8762593865394592
Epoch 340, training loss: 0.23906415700912476 = 0.16813093423843384 + 0.01 * 7.093322277069092
Epoch 340, val loss: 0.8855093121528625
Epoch 350, training loss: 0.2182849943637848 = 0.14743033051490784 + 0.01 * 7.085465431213379
Epoch 350, val loss: 0.8963654041290283
Epoch 360, training loss: 0.2000301480293274 = 0.12918461859226227 + 0.01 * 7.084552764892578
Epoch 360, val loss: 0.9087243676185608
Epoch 370, training loss: 0.18404525518417358 = 0.11329205334186554 + 0.01 * 7.075319290161133
Epoch 370, val loss: 0.9223757982254028
Epoch 380, training loss: 0.17021295428276062 = 0.0995488166809082 + 0.01 * 7.066414833068848
Epoch 380, val loss: 0.9370325803756714
Epoch 390, training loss: 0.1583343744277954 = 0.08770815283060074 + 0.01 * 7.062621593475342
Epoch 390, val loss: 0.9523722529411316
Epoch 400, training loss: 0.14809077978134155 = 0.07752995938062668 + 0.01 * 7.056081295013428
Epoch 400, val loss: 0.9681976437568665
Epoch 410, training loss: 0.13928571343421936 = 0.06878390908241272 + 0.01 * 7.050180912017822
Epoch 410, val loss: 0.9843434691429138
Epoch 420, training loss: 0.13170172274112701 = 0.061256732791662216 + 0.01 * 7.044498443603516
Epoch 420, val loss: 1.000626802444458
Epoch 430, training loss: 0.12515074014663696 = 0.05476304143667221 + 0.01 * 7.038769721984863
Epoch 430, val loss: 1.0169353485107422
Epoch 440, training loss: 0.1194685697555542 = 0.049143362790346146 + 0.01 * 7.032520771026611
Epoch 440, val loss: 1.0331175327301025
Epoch 450, training loss: 0.11454840749502182 = 0.04426518827676773 + 0.01 * 7.028322219848633
Epoch 450, val loss: 1.0490726232528687
Epoch 460, training loss: 0.11034505069255829 = 0.04001612961292267 + 0.01 * 7.032892227172852
Epoch 460, val loss: 1.0646897554397583
Epoch 470, training loss: 0.10657738894224167 = 0.036304108798503876 + 0.01 * 7.0273284912109375
Epoch 470, val loss: 1.0799038410186768
Epoch 480, training loss: 0.10321705788373947 = 0.03305109590291977 + 0.01 * 7.01659631729126
Epoch 480, val loss: 1.094728946685791
Epoch 490, training loss: 0.10029195249080658 = 0.030191322788596153 + 0.01 * 7.0100626945495605
Epoch 490, val loss: 1.1090689897537231
Epoch 500, training loss: 0.09776571393013 = 0.027669105678796768 + 0.01 * 7.009661674499512
Epoch 500, val loss: 1.1229363679885864
Epoch 510, training loss: 0.09555457532405853 = 0.025437790900468826 + 0.01 * 7.011678218841553
Epoch 510, val loss: 1.1363434791564941
Epoch 520, training loss: 0.0934729054570198 = 0.023457562550902367 + 0.01 * 7.0015339851379395
Epoch 520, val loss: 1.1492407321929932
Epoch 530, training loss: 0.0916532576084137 = 0.021694527938961983 + 0.01 * 6.995872974395752
Epoch 530, val loss: 1.161705732345581
Epoch 540, training loss: 0.09003352373838425 = 0.020119110122323036 + 0.01 * 6.99144172668457
Epoch 540, val loss: 1.1737254858016968
Epoch 550, training loss: 0.08857619762420654 = 0.0187076386064291 + 0.01 * 6.986855983734131
Epoch 550, val loss: 1.1852877140045166
Epoch 560, training loss: 0.08718043565750122 = 0.017439119517803192 + 0.01 * 6.9741315841674805
Epoch 560, val loss: 1.1964197158813477
Epoch 570, training loss: 0.08655199408531189 = 0.01629510521888733 + 0.01 * 7.025689125061035
Epoch 570, val loss: 1.2071223258972168
Epoch 580, training loss: 0.08508247882127762 = 0.01526404358446598 + 0.01 * 6.981843948364258
Epoch 580, val loss: 1.2174246311187744
Epoch 590, training loss: 0.08392233401536942 = 0.014330524019896984 + 0.01 * 6.959181785583496
Epoch 590, val loss: 1.227328896522522
Epoch 600, training loss: 0.08315259963274002 = 0.013482363894581795 + 0.01 * 6.967023849487305
Epoch 600, val loss: 1.2368559837341309
Epoch 610, training loss: 0.08223910629749298 = 0.01270989514887333 + 0.01 * 6.952921390533447
Epoch 610, val loss: 1.2460583448410034
Epoch 620, training loss: 0.08144085854291916 = 0.012003662064671516 + 0.01 * 6.943719863891602
Epoch 620, val loss: 1.254946231842041
Epoch 630, training loss: 0.08095451444387436 = 0.011356147937476635 + 0.01 * 6.959836959838867
Epoch 630, val loss: 1.2635412216186523
Epoch 640, training loss: 0.08020355552434921 = 0.010762696154415607 + 0.01 * 6.944086074829102
Epoch 640, val loss: 1.2718428373336792
Epoch 650, training loss: 0.07950139045715332 = 0.010217178612947464 + 0.01 * 6.928421497344971
Epoch 650, val loss: 1.2798118591308594
Epoch 660, training loss: 0.0789877250790596 = 0.009714306332170963 + 0.01 * 6.927341938018799
Epoch 660, val loss: 1.2875266075134277
Epoch 670, training loss: 0.0787188783288002 = 0.009250151924788952 + 0.01 * 6.946873188018799
Epoch 670, val loss: 1.294971227645874
Epoch 680, training loss: 0.07800092548131943 = 0.008821548894047737 + 0.01 * 6.917938232421875
Epoch 680, val loss: 1.3021776676177979
Epoch 690, training loss: 0.07777974009513855 = 0.00842447392642498 + 0.01 * 6.935526371002197
Epoch 690, val loss: 1.3091367483139038
Epoch 700, training loss: 0.07727283984422684 = 0.008056151680648327 + 0.01 * 6.9216694831848145
Epoch 700, val loss: 1.3158458471298218
Epoch 710, training loss: 0.07681587338447571 = 0.007713770028203726 + 0.01 * 6.910210132598877
Epoch 710, val loss: 1.3223700523376465
Epoch 720, training loss: 0.07636389136314392 = 0.0073946937918663025 + 0.01 * 6.8969197273254395
Epoch 720, val loss: 1.3286370038986206
Epoch 730, training loss: 0.07593482732772827 = 0.007096830755472183 + 0.01 * 6.883800506591797
Epoch 730, val loss: 1.3347516059875488
Epoch 740, training loss: 0.07553377747535706 = 0.006818290799856186 + 0.01 * 6.871548652648926
Epoch 740, val loss: 1.3406591415405273
Epoch 750, training loss: 0.07533662766218185 = 0.006557698827236891 + 0.01 * 6.877893447875977
Epoch 750, val loss: 1.3463573455810547
Epoch 760, training loss: 0.07538066804409027 = 0.006314092315733433 + 0.01 * 6.906658172607422
Epoch 760, val loss: 1.3519083261489868
Epoch 770, training loss: 0.07478038966655731 = 0.006085717584937811 + 0.01 * 6.869467735290527
Epoch 770, val loss: 1.35722017288208
Epoch 780, training loss: 0.07445475459098816 = 0.005870916415005922 + 0.01 * 6.858383655548096
Epoch 780, val loss: 1.3624075651168823
Epoch 790, training loss: 0.07440696656703949 = 0.005668698810040951 + 0.01 * 6.87382698059082
Epoch 790, val loss: 1.3674613237380981
Epoch 800, training loss: 0.07403940707445145 = 0.005478243343532085 + 0.01 * 6.856116771697998
Epoch 800, val loss: 1.3723483085632324
Epoch 810, training loss: 0.07386911660432816 = 0.005298369098454714 + 0.01 * 6.857074737548828
Epoch 810, val loss: 1.3771244287490845
Epoch 820, training loss: 0.07355372607707977 = 0.005128573626279831 + 0.01 * 6.842514991760254
Epoch 820, val loss: 1.3817254304885864
Epoch 830, training loss: 0.07341819256544113 = 0.004968022461980581 + 0.01 * 6.845016956329346
Epoch 830, val loss: 1.386231780052185
Epoch 840, training loss: 0.07317155599594116 = 0.004816049709916115 + 0.01 * 6.8355512619018555
Epoch 840, val loss: 1.3905705213546753
Epoch 850, training loss: 0.0729338601231575 = 0.004672105424106121 + 0.01 * 6.826176166534424
Epoch 850, val loss: 1.3947967290878296
Epoch 860, training loss: 0.07278672605752945 = 0.004535549320280552 + 0.01 * 6.825118064880371
Epoch 860, val loss: 1.3989059925079346
Epoch 870, training loss: 0.07274948060512543 = 0.00440586032345891 + 0.01 * 6.834362030029297
Epoch 870, val loss: 1.4028724431991577
Epoch 880, training loss: 0.07239057123661041 = 0.004282701760530472 + 0.01 * 6.810786724090576
Epoch 880, val loss: 1.406773328781128
Epoch 890, training loss: 0.07224498689174652 = 0.004165481310337782 + 0.01 * 6.807950973510742
Epoch 890, val loss: 1.4105327129364014
Epoch 900, training loss: 0.07215003669261932 = 0.004053921904414892 + 0.01 * 6.809611797332764
Epoch 900, val loss: 1.4141813516616821
Epoch 910, training loss: 0.07206428796052933 = 0.003947705961763859 + 0.01 * 6.8116583824157715
Epoch 910, val loss: 1.4177300930023193
Epoch 920, training loss: 0.07215919345617294 = 0.0038465154357254505 + 0.01 * 6.831267833709717
Epoch 920, val loss: 1.421241283416748
Epoch 930, training loss: 0.07171805202960968 = 0.003749827155843377 + 0.01 * 6.796823024749756
Epoch 930, val loss: 1.4245946407318115
Epoch 940, training loss: 0.07164224982261658 = 0.0036573619581758976 + 0.01 * 6.798489093780518
Epoch 940, val loss: 1.4278943538665771
Epoch 950, training loss: 0.07160720229148865 = 0.003569117048755288 + 0.01 * 6.803808689117432
Epoch 950, val loss: 1.4311054944992065
Epoch 960, training loss: 0.0713803842663765 = 0.0034846037160605192 + 0.01 * 6.789577960968018
Epoch 960, val loss: 1.4342119693756104
Epoch 970, training loss: 0.07134182006120682 = 0.0034037274308502674 + 0.01 * 6.793809413909912
Epoch 970, val loss: 1.437253713607788
Epoch 980, training loss: 0.07137661427259445 = 0.0033261822536587715 + 0.01 * 6.805042743682861
Epoch 980, val loss: 1.4402618408203125
Epoch 990, training loss: 0.07100661098957062 = 0.0032519041560590267 + 0.01 * 6.775471210479736
Epoch 990, val loss: 1.4430934190750122
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.7981022667369532
=== training gcn model ===
Epoch 0, training loss: 2.048670530319214 = 1.9627022743225098 + 0.01 * 8.596834182739258
Epoch 0, val loss: 1.959538459777832
Epoch 10, training loss: 2.037750482559204 = 1.9517827033996582 + 0.01 * 8.596780776977539
Epoch 10, val loss: 1.9481221437454224
Epoch 20, training loss: 2.0244498252868652 = 1.9384838342666626 + 0.01 * 8.596606254577637
Epoch 20, val loss: 1.9341224431991577
Epoch 30, training loss: 2.0059285163879395 = 1.9199676513671875 + 0.01 * 8.596075057983398
Epoch 30, val loss: 1.914678692817688
Epoch 40, training loss: 1.9787813425064087 = 1.8928455114364624 + 0.01 * 8.593582153320312
Epoch 40, val loss: 1.886515498161316
Epoch 50, training loss: 1.9410758018493652 = 1.8553212881088257 + 0.01 * 8.575454711914062
Epoch 50, val loss: 1.8490244150161743
Epoch 60, training loss: 1.8987841606140137 = 1.8139125108718872 + 0.01 * 8.487164497375488
Epoch 60, val loss: 1.8119103908538818
Epoch 70, training loss: 1.8638051748275757 = 1.7809617519378662 + 0.01 * 8.284339904785156
Epoch 70, val loss: 1.7864270210266113
Epoch 80, training loss: 1.8258992433547974 = 1.7446277141571045 + 0.01 * 8.127153396606445
Epoch 80, val loss: 1.7553154230117798
Epoch 90, training loss: 1.7733511924743652 = 1.6949554681777954 + 0.01 * 7.839572429656982
Epoch 90, val loss: 1.7112488746643066
Epoch 100, training loss: 1.7035478353500366 = 1.6274038553237915 + 0.01 * 7.614394664764404
Epoch 100, val loss: 1.6529269218444824
Epoch 110, training loss: 1.6175987720489502 = 1.542152762413025 + 0.01 * 7.544605731964111
Epoch 110, val loss: 1.580230712890625
Epoch 120, training loss: 1.5219125747680664 = 1.446922779083252 + 0.01 * 7.498985767364502
Epoch 120, val loss: 1.5012749433517456
Epoch 130, training loss: 1.4233664274215698 = 1.3489121198654175 + 0.01 * 7.445429801940918
Epoch 130, val loss: 1.422235131263733
Epoch 140, training loss: 1.3227325677871704 = 1.248957633972168 + 0.01 * 7.377495288848877
Epoch 140, val loss: 1.3450233936309814
Epoch 150, training loss: 1.2195935249328613 = 1.1463842391967773 + 0.01 * 7.320925235748291
Epoch 150, val loss: 1.2671672105789185
Epoch 160, training loss: 1.1164753437042236 = 1.0436478853225708 + 0.01 * 7.282744407653809
Epoch 160, val loss: 1.1901321411132812
Epoch 170, training loss: 1.0174914598464966 = 0.9450106024742126 + 0.01 * 7.248083591461182
Epoch 170, val loss: 1.1172553300857544
Epoch 180, training loss: 0.9267266392707825 = 0.8545133471488953 + 0.01 * 7.2213311195373535
Epoch 180, val loss: 1.0522702932357788
Epoch 190, training loss: 0.8463907241821289 = 0.7743272185325623 + 0.01 * 7.2063493728637695
Epoch 190, val loss: 0.9971596002578735
Epoch 200, training loss: 0.7762688994407654 = 0.7043122053146362 + 0.01 * 7.195668697357178
Epoch 200, val loss: 0.951477587223053
Epoch 210, training loss: 0.7146214842796326 = 0.6427369713783264 + 0.01 * 7.188451766967773
Epoch 210, val loss: 0.9140174388885498
Epoch 220, training loss: 0.6595317125320435 = 0.587616503238678 + 0.01 * 7.19152307510376
Epoch 220, val loss: 0.883623480796814
Epoch 230, training loss: 0.609298050403595 = 0.537475049495697 + 0.01 * 7.182297706604004
Epoch 230, val loss: 0.8593279719352722
Epoch 240, training loss: 0.5630040168762207 = 0.49122893810272217 + 0.01 * 7.177505016326904
Epoch 240, val loss: 0.8402072191238403
Epoch 250, training loss: 0.5199182629585266 = 0.4481654763221741 + 0.01 * 7.1752777099609375
Epoch 250, val loss: 0.8252381682395935
Epoch 260, training loss: 0.47962328791618347 = 0.40788692235946655 + 0.01 * 7.173637390136719
Epoch 260, val loss: 0.8136338591575623
Epoch 270, training loss: 0.4419662356376648 = 0.37025186419487 + 0.01 * 7.1714372634887695
Epoch 270, val loss: 0.805045485496521
Epoch 280, training loss: 0.40685898065567017 = 0.3351558446884155 + 0.01 * 7.170314311981201
Epoch 280, val loss: 0.7992895245552063
Epoch 290, training loss: 0.3741045296192169 = 0.3024246096611023 + 0.01 * 7.16799259185791
Epoch 290, val loss: 0.7961114645004272
Epoch 300, training loss: 0.34351885318756104 = 0.27186131477355957 + 0.01 * 7.165752410888672
Epoch 300, val loss: 0.7952092289924622
Epoch 310, training loss: 0.3149883449077606 = 0.24331636726856232 + 0.01 * 7.167198181152344
Epoch 310, val loss: 0.7964581251144409
Epoch 320, training loss: 0.28839465975761414 = 0.21677228808403015 + 0.01 * 7.162237644195557
Epoch 320, val loss: 0.7995585799217224
Epoch 330, training loss: 0.26391881704330444 = 0.19232885539531708 + 0.01 * 7.158995628356934
Epoch 330, val loss: 0.8044487237930298
Epoch 340, training loss: 0.24168704450130463 = 0.17012521624565125 + 0.01 * 7.156182765960693
Epoch 340, val loss: 0.8110941052436829
Epoch 350, training loss: 0.22177959978580475 = 0.15024417638778687 + 0.01 * 7.153542518615723
Epoch 350, val loss: 0.8195329308509827
Epoch 360, training loss: 0.20417346060276031 = 0.13266602158546448 + 0.01 * 7.1507439613342285
Epoch 360, val loss: 0.82944655418396
Epoch 370, training loss: 0.1887320876121521 = 0.11726363003253937 + 0.01 * 7.146844863891602
Epoch 370, val loss: 0.8406124114990234
Epoch 380, training loss: 0.17522013187408447 = 0.10384771972894669 + 0.01 * 7.137242317199707
Epoch 380, val loss: 0.8527588248252869
Epoch 390, training loss: 0.16355572640895844 = 0.09220006316900253 + 0.01 * 7.135566234588623
Epoch 390, val loss: 0.8655992150306702
Epoch 400, training loss: 0.1534518003463745 = 0.08209642767906189 + 0.01 * 7.135537147521973
Epoch 400, val loss: 0.8789374828338623
Epoch 410, training loss: 0.14454609155654907 = 0.07331768423318863 + 0.01 * 7.12283992767334
Epoch 410, val loss: 0.8925617933273315
Epoch 420, training loss: 0.13679048418998718 = 0.06567436456680298 + 0.01 * 7.111611366271973
Epoch 420, val loss: 0.9063172340393066
Epoch 430, training loss: 0.13029587268829346 = 0.05901036784052849 + 0.01 * 7.1285505294799805
Epoch 430, val loss: 0.92008376121521
Epoch 440, training loss: 0.1241418868303299 = 0.05319608002901077 + 0.01 * 7.09458065032959
Epoch 440, val loss: 0.9336997866630554
Epoch 450, training loss: 0.11898916959762573 = 0.048103976994752884 + 0.01 * 7.088520050048828
Epoch 450, val loss: 0.9472363591194153
Epoch 460, training loss: 0.11443175375461578 = 0.04363188147544861 + 0.01 * 7.079987049102783
Epoch 460, val loss: 0.9604530334472656
Epoch 470, training loss: 0.1104860007762909 = 0.039700571447610855 + 0.01 * 7.078543663024902
Epoch 470, val loss: 0.9734303951263428
Epoch 480, training loss: 0.10686297714710236 = 0.03623001277446747 + 0.01 * 7.063296318054199
Epoch 480, val loss: 0.9860919117927551
Epoch 490, training loss: 0.10420876741409302 = 0.033155396580696106 + 0.01 * 7.105337142944336
Epoch 490, val loss: 0.9984651803970337
Epoch 500, training loss: 0.10109487175941467 = 0.03043014369904995 + 0.01 * 7.06647253036499
Epoch 500, val loss: 1.0103856325149536
Epoch 510, training loss: 0.098576121032238 = 0.028008615598082542 + 0.01 * 7.056750774383545
Epoch 510, val loss: 1.022015929222107
Epoch 520, training loss: 0.09603065252304077 = 0.025852371007204056 + 0.01 * 7.017828941345215
Epoch 520, val loss: 1.033324122428894
Epoch 530, training loss: 0.09414447098970413 = 0.02392471581697464 + 0.01 * 7.021975994110107
Epoch 530, val loss: 1.044248104095459
Epoch 540, training loss: 0.09244437515735626 = 0.02219846285879612 + 0.01 * 7.024591445922852
Epoch 540, val loss: 1.054840087890625
Epoch 550, training loss: 0.09047998487949371 = 0.02064676210284233 + 0.01 * 6.983323097229004
Epoch 550, val loss: 1.065197229385376
Epoch 560, training loss: 0.08938796818256378 = 0.01924779824912548 + 0.01 * 7.014016628265381
Epoch 560, val loss: 1.0751434564590454
Epoch 570, training loss: 0.08778156340122223 = 0.017985869199037552 + 0.01 * 6.979569911956787
Epoch 570, val loss: 1.0848145484924316
Epoch 580, training loss: 0.08660590648651123 = 0.0168430395424366 + 0.01 * 6.976286888122559
Epoch 580, val loss: 1.0942133665084839
Epoch 590, training loss: 0.08585594594478607 = 0.015804879367351532 + 0.01 * 7.005106449127197
Epoch 590, val loss: 1.1033110618591309
Epoch 600, training loss: 0.08455093204975128 = 0.014861704781651497 + 0.01 * 6.968923091888428
Epoch 600, val loss: 1.1121336221694946
Epoch 610, training loss: 0.0835980623960495 = 0.014002140611410141 + 0.01 * 6.959592342376709
Epoch 610, val loss: 1.1207075119018555
Epoch 620, training loss: 0.08309312909841537 = 0.013215765357017517 + 0.01 * 6.987736701965332
Epoch 620, val loss: 1.1288946866989136
Epoch 630, training loss: 0.08184663951396942 = 0.012496709823608398 + 0.01 * 6.934992790222168
Epoch 630, val loss: 1.136960744857788
Epoch 640, training loss: 0.08119304478168488 = 0.011836809106171131 + 0.01 * 6.9356231689453125
Epoch 640, val loss: 1.144708514213562
Epoch 650, training loss: 0.08047713339328766 = 0.011229856871068478 + 0.01 * 6.924727916717529
Epoch 650, val loss: 1.1522146463394165
Epoch 660, training loss: 0.07985517382621765 = 0.010670884512364864 + 0.01 * 6.918428897857666
Epoch 660, val loss: 1.1595466136932373
Epoch 670, training loss: 0.07914220541715622 = 0.010154632851481438 + 0.01 * 6.898757457733154
Epoch 670, val loss: 1.1665438413619995
Epoch 680, training loss: 0.07860036194324493 = 0.009676930494606495 + 0.01 * 6.892343044281006
Epoch 680, val loss: 1.1733647584915161
Epoch 690, training loss: 0.07823067158460617 = 0.009234686382114887 + 0.01 * 6.899598598480225
Epoch 690, val loss: 1.1800217628479004
Epoch 700, training loss: 0.07777033746242523 = 0.008824203163385391 + 0.01 * 6.894613265991211
Epoch 700, val loss: 1.1864778995513916
Epoch 710, training loss: 0.07729819416999817 = 0.008442233316600323 + 0.01 * 6.88559627532959
Epoch 710, val loss: 1.1926050186157227
Epoch 720, training loss: 0.07691336423158646 = 0.008086883462965488 + 0.01 * 6.88264799118042
Epoch 720, val loss: 1.1987330913543701
Epoch 730, training loss: 0.07642614096403122 = 0.007755294442176819 + 0.01 * 6.867084980010986
Epoch 730, val loss: 1.204522728919983
Epoch 740, training loss: 0.07621194422245026 = 0.007445727474987507 + 0.01 * 6.876621246337891
Epoch 740, val loss: 1.2101761102676392
Epoch 750, training loss: 0.07569941878318787 = 0.007156261708587408 + 0.01 * 6.854316234588623
Epoch 750, val loss: 1.2156904935836792
Epoch 760, training loss: 0.0755719393491745 = 0.006884793750941753 + 0.01 * 6.868714809417725
Epoch 760, val loss: 1.220994234085083
Epoch 770, training loss: 0.07508733123540878 = 0.006630164571106434 + 0.01 * 6.84571647644043
Epoch 770, val loss: 1.2262061834335327
Epoch 780, training loss: 0.07474440336227417 = 0.006390894763171673 + 0.01 * 6.83535099029541
Epoch 780, val loss: 1.2312196493148804
Epoch 790, training loss: 0.0746593102812767 = 0.006165743339806795 + 0.01 * 6.849356651306152
Epoch 790, val loss: 1.2360920906066895
Epoch 800, training loss: 0.0744839534163475 = 0.005953620187938213 + 0.01 * 6.853033542633057
Epoch 800, val loss: 1.240848183631897
Epoch 810, training loss: 0.07439631968736649 = 0.005753460340201855 + 0.01 * 6.864286422729492
Epoch 810, val loss: 1.2454701662063599
Epoch 820, training loss: 0.07392746210098267 = 0.0055642882362008095 + 0.01 * 6.836317539215088
Epoch 820, val loss: 1.2499544620513916
Epoch 830, training loss: 0.07361117005348206 = 0.0053857071325182915 + 0.01 * 6.82254695892334
Epoch 830, val loss: 1.2542873620986938
Epoch 840, training loss: 0.07338068634271622 = 0.005216641817241907 + 0.01 * 6.816404819488525
Epoch 840, val loss: 1.2584646940231323
Epoch 850, training loss: 0.07319926470518112 = 0.005056619178503752 + 0.01 * 6.81426477432251
Epoch 850, val loss: 1.262585997581482
Epoch 860, training loss: 0.0731111541390419 = 0.004904909525066614 + 0.01 * 6.820624828338623
Epoch 860, val loss: 1.2665748596191406
Epoch 870, training loss: 0.0727844312787056 = 0.0047609820030629635 + 0.01 * 6.802345275878906
Epoch 870, val loss: 1.2705210447311401
Epoch 880, training loss: 0.07273562997579575 = 0.004624022636562586 + 0.01 * 6.811161041259766
Epoch 880, val loss: 1.2741986513137817
Epoch 890, training loss: 0.07234784960746765 = 0.004494110122323036 + 0.01 * 6.785374641418457
Epoch 890, val loss: 1.277906060218811
Epoch 900, training loss: 0.07254558056592941 = 0.004370369017124176 + 0.01 * 6.817521095275879
Epoch 900, val loss: 1.2814713716506958
Epoch 910, training loss: 0.07234346121549606 = 0.004252420738339424 + 0.01 * 6.8091044425964355
Epoch 910, val loss: 1.2849714756011963
Epoch 920, training loss: 0.07209877669811249 = 0.004140075296163559 + 0.01 * 6.795870304107666
Epoch 920, val loss: 1.2883259057998657
Epoch 930, training loss: 0.07228010147809982 = 0.004032899159938097 + 0.01 * 6.8247199058532715
Epoch 930, val loss: 1.291625738143921
Epoch 940, training loss: 0.07185579836368561 = 0.003930586390197277 + 0.01 * 6.7925214767456055
Epoch 940, val loss: 1.2947907447814941
Epoch 950, training loss: 0.07143993675708771 = 0.003832904389128089 + 0.01 * 6.760703086853027
Epoch 950, val loss: 1.297943353652954
Epoch 960, training loss: 0.07154529541730881 = 0.003739293199032545 + 0.01 * 6.780600547790527
Epoch 960, val loss: 1.3009470701217651
Epoch 970, training loss: 0.07160595059394836 = 0.0036497812252491713 + 0.01 * 6.79561710357666
Epoch 970, val loss: 1.303901195526123
Epoch 980, training loss: 0.07117238640785217 = 0.003564136801287532 + 0.01 * 6.760825157165527
Epoch 980, val loss: 1.3067933320999146
Epoch 990, training loss: 0.07121462374925613 = 0.003482186235487461 + 0.01 * 6.7732439041137695
Epoch 990, val loss: 1.309582233428955
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8023194517659463
=== training gcn model ===
Epoch 0, training loss: 2.0340213775634766 = 1.9480528831481934 + 0.01 * 8.596839904785156
Epoch 0, val loss: 1.9458489418029785
Epoch 10, training loss: 2.024413824081421 = 1.9384459257125854 + 0.01 * 8.596792221069336
Epoch 10, val loss: 1.9361929893493652
Epoch 20, training loss: 2.012740135192871 = 1.9267737865447998 + 0.01 * 8.596635818481445
Epoch 20, val loss: 1.9244590997695923
Epoch 30, training loss: 1.9968103170394897 = 1.9108482599258423 + 0.01 * 8.596200942993164
Epoch 30, val loss: 1.9086545705795288
Epoch 40, training loss: 1.9739742279052734 = 1.8880330324172974 + 0.01 * 8.594117164611816
Epoch 40, val loss: 1.8865532875061035
Epoch 50, training loss: 1.9420278072357178 = 1.8562414646148682 + 0.01 * 8.578639030456543
Epoch 50, val loss: 1.8570811748504639
Epoch 60, training loss: 1.9027546644210815 = 1.8177798986434937 + 0.01 * 8.497478485107422
Epoch 60, val loss: 1.8242548704147339
Epoch 70, training loss: 1.862951397895813 = 1.7809152603149414 + 0.01 * 8.203618049621582
Epoch 70, val loss: 1.7946075201034546
Epoch 80, training loss: 1.8217413425445557 = 1.741547703742981 + 0.01 * 8.019369125366211
Epoch 80, val loss: 1.7591960430145264
Epoch 90, training loss: 1.7649344205856323 = 1.6866339445114136 + 0.01 * 7.830050468444824
Epoch 90, val loss: 1.7102642059326172
Epoch 100, training loss: 1.688300609588623 = 1.6115882396697998 + 0.01 * 7.6712422370910645
Epoch 100, val loss: 1.6480059623718262
Epoch 110, training loss: 1.5936291217803955 = 1.5176812410354614 + 0.01 * 7.594790458679199
Epoch 110, val loss: 1.5727341175079346
Epoch 120, training loss: 1.489237666130066 = 1.4136321544647217 + 0.01 * 7.560548305511475
Epoch 120, val loss: 1.4891175031661987
Epoch 130, training loss: 1.3834257125854492 = 1.308154821395874 + 0.01 * 7.527088165283203
Epoch 130, val loss: 1.40651535987854
Epoch 140, training loss: 1.2789154052734375 = 1.20408296585083 + 0.01 * 7.483243942260742
Epoch 140, val loss: 1.3265889883041382
Epoch 150, training loss: 1.1751734018325806 = 1.1009571552276611 + 0.01 * 7.421625137329102
Epoch 150, val loss: 1.248227596282959
Epoch 160, training loss: 1.0726903676986694 = 0.9990777969360352 + 0.01 * 7.361258506774902
Epoch 160, val loss: 1.1705245971679688
Epoch 170, training loss: 0.9747578501701355 = 0.9013310074806213 + 0.01 * 7.342686653137207
Epoch 170, val loss: 1.096272349357605
Epoch 180, training loss: 0.8849892020225525 = 0.8116797208786011 + 0.01 * 7.3309478759765625
Epoch 180, val loss: 1.0298093557357788
Epoch 190, training loss: 0.8054889440536499 = 0.7322438955307007 + 0.01 * 7.324505805969238
Epoch 190, val loss: 0.9742894768714905
Epoch 200, training loss: 0.7353987097740173 = 0.6622287631034851 + 0.01 * 7.316996097564697
Epoch 200, val loss: 0.9303805828094482
Epoch 210, training loss: 0.6726020574569702 = 0.5995091199874878 + 0.01 * 7.309296131134033
Epoch 210, val loss: 0.8967770338058472
Epoch 220, training loss: 0.615321159362793 = 0.5423223376274109 + 0.01 * 7.2998833656311035
Epoch 220, val loss: 0.8715060949325562
Epoch 230, training loss: 0.5625373721122742 = 0.48967117071151733 + 0.01 * 7.286620140075684
Epoch 230, val loss: 0.8524300456047058
Epoch 240, training loss: 0.5138604044914246 = 0.44114282727241516 + 0.01 * 7.271758079528809
Epoch 240, val loss: 0.8388551473617554
Epoch 250, training loss: 0.4691055119037628 = 0.39651402831077576 + 0.01 * 7.259148120880127
Epoch 250, val loss: 0.8306044936180115
Epoch 260, training loss: 0.42798006534576416 = 0.3555728793144226 + 0.01 * 7.240718841552734
Epoch 260, val loss: 0.8273603320121765
Epoch 270, training loss: 0.39037495851516724 = 0.3181115388870239 + 0.01 * 7.226343631744385
Epoch 270, val loss: 0.828478217124939
Epoch 280, training loss: 0.356137752532959 = 0.28397420048713684 + 0.01 * 7.216353893280029
Epoch 280, val loss: 0.8333829045295715
Epoch 290, training loss: 0.325111448764801 = 0.25300872325897217 + 0.01 * 7.210272312164307
Epoch 290, val loss: 0.8415481448173523
Epoch 300, training loss: 0.2970678508281708 = 0.22504137456417084 + 0.01 * 7.2026472091674805
Epoch 300, val loss: 0.8525838255882263
Epoch 310, training loss: 0.27186986804008484 = 0.19990608096122742 + 0.01 * 7.196378231048584
Epoch 310, val loss: 0.8661609888076782
Epoch 320, training loss: 0.24945007264614105 = 0.1774211823940277 + 0.01 * 7.2028889656066895
Epoch 320, val loss: 0.8817662000656128
Epoch 330, training loss: 0.2292700707912445 = 0.15741579234600067 + 0.01 * 7.185427665710449
Epoch 330, val loss: 0.8990356922149658
Epoch 340, training loss: 0.2114468365907669 = 0.13969089090824127 + 0.01 * 7.175594806671143
Epoch 340, val loss: 0.9176656007766724
Epoch 350, training loss: 0.195816308259964 = 0.12404046207666397 + 0.01 * 7.177585124969482
Epoch 350, val loss: 0.9372966289520264
Epoch 360, training loss: 0.18187746405601501 = 0.11026909947395325 + 0.01 * 7.1608357429504395
Epoch 360, val loss: 0.957633912563324
Epoch 370, training loss: 0.16969972848892212 = 0.09817273914813995 + 0.01 * 7.152698516845703
Epoch 370, val loss: 0.9784472584724426
Epoch 380, training loss: 0.15905851125717163 = 0.08755223453044891 + 0.01 * 7.150628566741943
Epoch 380, val loss: 0.9994603991508484
Epoch 390, training loss: 0.1496245563030243 = 0.0782318189740181 + 0.01 * 7.139274597167969
Epoch 390, val loss: 1.0204880237579346
Epoch 400, training loss: 0.14134055376052856 = 0.07003791630268097 + 0.01 * 7.1302642822265625
Epoch 400, val loss: 1.0412434339523315
Epoch 410, training loss: 0.13400176167488098 = 0.06283532083034515 + 0.01 * 7.116644859313965
Epoch 410, val loss: 1.0617551803588867
Epoch 420, training loss: 0.12756669521331787 = 0.05650376155972481 + 0.01 * 7.106293678283691
Epoch 420, val loss: 1.0818769931793213
Epoch 430, training loss: 0.12189847230911255 = 0.0509369820356369 + 0.01 * 7.096149444580078
Epoch 430, val loss: 1.1015264987945557
Epoch 440, training loss: 0.11727787554264069 = 0.04603619500994682 + 0.01 * 7.124168872833252
Epoch 440, val loss: 1.1206409931182861
Epoch 450, training loss: 0.11262059211730957 = 0.041726890951395035 + 0.01 * 7.089369773864746
Epoch 450, val loss: 1.1392241716384888
Epoch 460, training loss: 0.10867221653461456 = 0.037925515323877335 + 0.01 * 7.074670791625977
Epoch 460, val loss: 1.1572312116622925
Epoch 470, training loss: 0.10534034669399261 = 0.034563951194286346 + 0.01 * 7.077639579772949
Epoch 470, val loss: 1.1746524572372437
Epoch 480, training loss: 0.10224185138940811 = 0.03158643841743469 + 0.01 * 7.0655412673950195
Epoch 480, val loss: 1.1914929151535034
Epoch 490, training loss: 0.09955456107854843 = 0.028947003185749054 + 0.01 * 7.060755729675293
Epoch 490, val loss: 1.2077162265777588
Epoch 500, training loss: 0.09704026579856873 = 0.026600278913974762 + 0.01 * 7.043998718261719
Epoch 500, val loss: 1.2233604192733765
Epoch 510, training loss: 0.09492615610361099 = 0.024509543552994728 + 0.01 * 7.041661739349365
Epoch 510, val loss: 1.2384672164916992
Epoch 520, training loss: 0.09288866817951202 = 0.022641267627477646 + 0.01 * 7.024739742279053
Epoch 520, val loss: 1.253016710281372
Epoch 530, training loss: 0.09121504426002502 = 0.020967993885278702 + 0.01 * 7.024705410003662
Epoch 530, val loss: 1.2670187950134277
Epoch 540, training loss: 0.08975332230329514 = 0.019469112157821655 + 0.01 * 7.028420925140381
Epoch 540, val loss: 1.280476689338684
Epoch 550, training loss: 0.08819158375263214 = 0.01811998523771763 + 0.01 * 7.007160186767578
Epoch 550, val loss: 1.2934800386428833
Epoch 560, training loss: 0.08710412681102753 = 0.016900140792131424 + 0.01 * 7.020398139953613
Epoch 560, val loss: 1.3060357570648193
Epoch 570, training loss: 0.08584181219339371 = 0.015797598287463188 + 0.01 * 7.004421710968018
Epoch 570, val loss: 1.31812584400177
Epoch 580, training loss: 0.08471548557281494 = 0.014801467768847942 + 0.01 * 6.9914021492004395
Epoch 580, val loss: 1.3297793865203857
Epoch 590, training loss: 0.08389695733785629 = 0.013897196389734745 + 0.01 * 6.99997615814209
Epoch 590, val loss: 1.3410296440124512
Epoch 600, training loss: 0.08289431780576706 = 0.013074695132672787 + 0.01 * 6.981962203979492
Epoch 600, val loss: 1.351863145828247
Epoch 610, training loss: 0.0823434442281723 = 0.012324408628046513 + 0.01 * 7.001904010772705
Epoch 610, val loss: 1.3623086214065552
Epoch 620, training loss: 0.08153799176216125 = 0.011640342883765697 + 0.01 * 6.989765644073486
Epoch 620, val loss: 1.372365951538086
Epoch 630, training loss: 0.08071301877498627 = 0.011013545095920563 + 0.01 * 6.969947814941406
Epoch 630, val loss: 1.38209068775177
Epoch 640, training loss: 0.08004388213157654 = 0.010438485071063042 + 0.01 * 6.960539817810059
Epoch 640, val loss: 1.3914858102798462
Epoch 650, training loss: 0.0795968696475029 = 0.009909069165587425 + 0.01 * 6.968780040740967
Epoch 650, val loss: 1.4005721807479858
Epoch 660, training loss: 0.07901258766651154 = 0.009421909227967262 + 0.01 * 6.9590678215026855
Epoch 660, val loss: 1.409334659576416
Epoch 670, training loss: 0.07854865491390228 = 0.008972786366939545 + 0.01 * 6.957587242126465
Epoch 670, val loss: 1.4178346395492554
Epoch 680, training loss: 0.07806140184402466 = 0.008557658642530441 + 0.01 * 6.950374603271484
Epoch 680, val loss: 1.426031231880188
Epoch 690, training loss: 0.07763340324163437 = 0.008173041045665741 + 0.01 * 6.946036338806152
Epoch 690, val loss: 1.433977484703064
Epoch 700, training loss: 0.077262744307518 = 0.00781559944152832 + 0.01 * 6.944714546203613
Epoch 700, val loss: 1.4416624307632446
Epoch 710, training loss: 0.07683324813842773 = 0.007483535911887884 + 0.01 * 6.934971809387207
Epoch 710, val loss: 1.449110507965088
Epoch 720, training loss: 0.07649890333414078 = 0.007174118887633085 + 0.01 * 6.932478427886963
Epoch 720, val loss: 1.456317663192749
Epoch 730, training loss: 0.07618238031864166 = 0.006885427050292492 + 0.01 * 6.929696083068848
Epoch 730, val loss: 1.4633070230484009
Epoch 740, training loss: 0.07571100443601608 = 0.006615629885345697 + 0.01 * 6.909537315368652
Epoch 740, val loss: 1.4700812101364136
Epoch 750, training loss: 0.07550695538520813 = 0.006363312713801861 + 0.01 * 6.914364337921143
Epoch 750, val loss: 1.4766443967819214
Epoch 760, training loss: 0.07528967410326004 = 0.00612673070281744 + 0.01 * 6.916294097900391
Epoch 760, val loss: 1.4830321073532104
Epoch 770, training loss: 0.07518572360277176 = 0.005905004218220711 + 0.01 * 6.928071975708008
Epoch 770, val loss: 1.4892421960830688
Epoch 780, training loss: 0.07466534525156021 = 0.005696562584489584 + 0.01 * 6.896878242492676
Epoch 780, val loss: 1.4952682256698608
Epoch 790, training loss: 0.07442348450422287 = 0.00550053222104907 + 0.01 * 6.8922953605651855
Epoch 790, val loss: 1.5011248588562012
Epoch 800, training loss: 0.07446213066577911 = 0.005315832328051329 + 0.01 * 6.914629936218262
Epoch 800, val loss: 1.5068187713623047
Epoch 810, training loss: 0.07391669601202011 = 0.0051416619680821896 + 0.01 * 6.877503871917725
Epoch 810, val loss: 1.5123538970947266
Epoch 820, training loss: 0.07383066415786743 = 0.004977324046194553 + 0.01 * 6.885334014892578
Epoch 820, val loss: 1.5176936388015747
Epoch 830, training loss: 0.07367681711912155 = 0.00482240179553628 + 0.01 * 6.885441780090332
Epoch 830, val loss: 1.5228900909423828
Epoch 840, training loss: 0.0733964666724205 = 0.004675774369388819 + 0.01 * 6.872068881988525
Epoch 840, val loss: 1.5279730558395386
Epoch 850, training loss: 0.07349392771720886 = 0.00453675352036953 + 0.01 * 6.895717620849609
Epoch 850, val loss: 1.5329060554504395
Epoch 860, training loss: 0.07301008701324463 = 0.004404970910400152 + 0.01 * 6.8605122566223145
Epoch 860, val loss: 1.5377132892608643
Epoch 870, training loss: 0.0728708803653717 = 0.0042798276990652084 + 0.01 * 6.859105110168457
Epoch 870, val loss: 1.5423529148101807
Epoch 880, training loss: 0.0728856548666954 = 0.00416112644597888 + 0.01 * 6.872453212738037
Epoch 880, val loss: 1.5469059944152832
Epoch 890, training loss: 0.07262895256280899 = 0.004048100672662258 + 0.01 * 6.8580851554870605
Epoch 890, val loss: 1.5513384342193604
Epoch 900, training loss: 0.07282500714063644 = 0.00394065584987402 + 0.01 * 6.888435363769531
Epoch 900, val loss: 1.555673599243164
Epoch 910, training loss: 0.07218905538320541 = 0.0038382632192224264 + 0.01 * 6.835079193115234
Epoch 910, val loss: 1.5598458051681519
Epoch 920, training loss: 0.07231330126523972 = 0.003740758402273059 + 0.01 * 6.857254505157471
Epoch 920, val loss: 1.563915729522705
Epoch 930, training loss: 0.07216992974281311 = 0.0036477483808994293 + 0.01 * 6.8522186279296875
Epoch 930, val loss: 1.567914366722107
Epoch 940, training loss: 0.07200168818235397 = 0.0035589931067079306 + 0.01 * 6.844269752502441
Epoch 940, val loss: 1.5717473030090332
Epoch 950, training loss: 0.07219120860099792 = 0.0034741596318781376 + 0.01 * 6.871704578399658
Epoch 950, val loss: 1.57548189163208
Epoch 960, training loss: 0.07187412679195404 = 0.0033932365477085114 + 0.01 * 6.84808874130249
Epoch 960, val loss: 1.5791223049163818
Epoch 970, training loss: 0.07151521742343903 = 0.0033157705329358578 + 0.01 * 6.819945335388184
Epoch 970, val loss: 1.5826551914215088
Epoch 980, training loss: 0.07164033502340317 = 0.003241455415263772 + 0.01 * 6.839887619018555
Epoch 980, val loss: 1.5860966444015503
Epoch 990, training loss: 0.07149740308523178 = 0.0031704441644251347 + 0.01 * 6.832695960998535
Epoch 990, val loss: 1.5894696712493896
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8044280442804429
The final CL Acc:0.75679, 0.00972, The final GNN Acc:0.80162, 0.00263
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13170])
remove edge: torch.Size([2, 7904])
updated graph: torch.Size([2, 10518])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.03248929977417 = 1.946521282196045 + 0.01 * 8.596813201904297
Epoch 0, val loss: 1.9508475065231323
Epoch 10, training loss: 2.0223960876464844 = 1.9364291429519653 + 0.01 * 8.596700668334961
Epoch 10, val loss: 1.9409871101379395
Epoch 20, training loss: 2.009798049926758 = 1.923834204673767 + 0.01 * 8.596372604370117
Epoch 20, val loss: 1.9282375574111938
Epoch 30, training loss: 1.9922471046447754 = 1.9062949419021606 + 0.01 * 8.595216751098633
Epoch 30, val loss: 1.9102331399917603
Epoch 40, training loss: 1.9666328430175781 = 1.8807507753372192 + 0.01 * 8.588212966918945
Epoch 40, val loss: 1.8841335773468018
Epoch 50, training loss: 1.9310905933380127 = 1.8456064462661743 + 0.01 * 8.548415184020996
Epoch 50, val loss: 1.8496862649917603
Epoch 60, training loss: 1.8906441926956177 = 1.8068419694900513 + 0.01 * 8.380221366882324
Epoch 60, val loss: 1.8150171041488647
Epoch 70, training loss: 1.8555612564086914 = 1.773637294769287 + 0.01 * 8.192400932312012
Epoch 70, val loss: 1.7863816022872925
Epoch 80, training loss: 1.8132033348083496 = 1.733117938041687 + 0.01 * 8.008536338806152
Epoch 80, val loss: 1.7483940124511719
Epoch 90, training loss: 1.7534838914871216 = 1.6759086847305298 + 0.01 * 7.757519721984863
Epoch 90, val loss: 1.6969492435455322
Epoch 100, training loss: 1.6740561723709106 = 1.5983401536941528 + 0.01 * 7.571601867675781
Epoch 100, val loss: 1.6300277709960938
Epoch 110, training loss: 1.578519582748413 = 1.5046032667160034 + 0.01 * 7.391633033752441
Epoch 110, val loss: 1.5499980449676514
Epoch 120, training loss: 1.4806574583053589 = 1.4075261354446411 + 0.01 * 7.313133239746094
Epoch 120, val loss: 1.4697189331054688
Epoch 130, training loss: 1.3869507312774658 = 1.3143490552902222 + 0.01 * 7.260173797607422
Epoch 130, val loss: 1.3937357664108276
Epoch 140, training loss: 1.2962039709091187 = 1.2239309549331665 + 0.01 * 7.227305889129639
Epoch 140, val loss: 1.3212884664535522
Epoch 150, training loss: 1.208746314048767 = 1.1366952657699585 + 0.01 * 7.205107688903809
Epoch 150, val loss: 1.251759648323059
Epoch 160, training loss: 1.1276090145111084 = 1.055773138999939 + 0.01 * 7.18359375
Epoch 160, val loss: 1.1888998746871948
Epoch 170, training loss: 1.0545202493667603 = 0.982923686504364 + 0.01 * 7.159653186798096
Epoch 170, val loss: 1.1340452432632446
Epoch 180, training loss: 0.987919270992279 = 0.9166111350059509 + 0.01 * 7.13081169128418
Epoch 180, val loss: 1.084727168083191
Epoch 190, training loss: 0.9245830178260803 = 0.8535780310630798 + 0.01 * 7.100496292114258
Epoch 190, val loss: 1.0377979278564453
Epoch 200, training loss: 0.8613438010215759 = 0.7905910611152649 + 0.01 * 7.075275897979736
Epoch 200, val loss: 0.990483820438385
Epoch 210, training loss: 0.7965832352638245 = 0.7260558605194092 + 0.01 * 7.052735805511475
Epoch 210, val loss: 0.9423908591270447
Epoch 220, training loss: 0.7312919497489929 = 0.6609042286872864 + 0.01 * 7.038774490356445
Epoch 220, val loss: 0.8951655626296997
Epoch 230, training loss: 0.668196439743042 = 0.5979796051979065 + 0.01 * 7.021684169769287
Epoch 230, val loss: 0.8521156907081604
Epoch 240, training loss: 0.609981119632721 = 0.5397958755493164 + 0.01 * 7.018525123596191
Epoch 240, val loss: 0.816099226474762
Epoch 250, training loss: 0.5572910904884338 = 0.48727181553840637 + 0.01 * 7.001926422119141
Epoch 250, val loss: 0.7877805233001709
Epoch 260, training loss: 0.5096477270126343 = 0.43970537185668945 + 0.01 * 6.994238376617432
Epoch 260, val loss: 0.7661210298538208
Epoch 270, training loss: 0.46598494052886963 = 0.39603790640830994 + 0.01 * 6.994704723358154
Epoch 270, val loss: 0.749559223651886
Epoch 280, training loss: 0.42541807889938354 = 0.355589896440506 + 0.01 * 6.9828200340271
Epoch 280, val loss: 0.736986517906189
Epoch 290, training loss: 0.3878806531429291 = 0.31807249784469604 + 0.01 * 6.980815410614014
Epoch 290, val loss: 0.7276144027709961
Epoch 300, training loss: 0.35312968492507935 = 0.2833479940891266 + 0.01 * 6.97816801071167
Epoch 300, val loss: 0.7210286855697632
Epoch 310, training loss: 0.32101038098335266 = 0.25124600529670715 + 0.01 * 6.976438522338867
Epoch 310, val loss: 0.7168648838996887
Epoch 320, training loss: 0.29143643379211426 = 0.22167398035526276 + 0.01 * 6.9762444496154785
Epoch 320, val loss: 0.7148120999336243
Epoch 330, training loss: 0.26439231634140015 = 0.19464461505413055 + 0.01 * 6.9747700691223145
Epoch 330, val loss: 0.7145588994026184
Epoch 340, training loss: 0.239942729473114 = 0.17021428048610687 + 0.01 * 6.972845077514648
Epoch 340, val loss: 0.7157012224197388
Epoch 350, training loss: 0.21811115741729736 = 0.1483648717403412 + 0.01 * 6.9746294021606445
Epoch 350, val loss: 0.7177502512931824
Epoch 360, training loss: 0.19869890809059143 = 0.1290140151977539 + 0.01 * 6.968489646911621
Epoch 360, val loss: 0.7204546928405762
Epoch 370, training loss: 0.18165984749794006 = 0.112002894282341 + 0.01 * 6.965694904327393
Epoch 370, val loss: 0.7236802577972412
Epoch 380, training loss: 0.16680198907852173 = 0.09716954082250595 + 0.01 * 6.963245391845703
Epoch 380, val loss: 0.7273901104927063
Epoch 390, training loss: 0.15395545959472656 = 0.08434482663869858 + 0.01 * 6.961064338684082
Epoch 390, val loss: 0.7316285967826843
Epoch 400, training loss: 0.142926424741745 = 0.07333990931510925 + 0.01 * 6.958651065826416
Epoch 400, val loss: 0.7364882230758667
Epoch 410, training loss: 0.13347072899341583 = 0.06395018845796585 + 0.01 * 6.952054500579834
Epoch 410, val loss: 0.7419309020042419
Epoch 420, training loss: 0.12547607719898224 = 0.05596843734383583 + 0.01 * 6.950763702392578
Epoch 420, val loss: 0.7481089234352112
Epoch 430, training loss: 0.11870713531970978 = 0.04920127987861633 + 0.01 * 6.95058536529541
Epoch 430, val loss: 0.7548483610153198
Epoch 440, training loss: 0.11290821433067322 = 0.04346861690282822 + 0.01 * 6.943960189819336
Epoch 440, val loss: 0.7620735168457031
Epoch 450, training loss: 0.10799413174390793 = 0.03860078006982803 + 0.01 * 6.939335346221924
Epoch 450, val loss: 0.7696215510368347
Epoch 460, training loss: 0.10381104052066803 = 0.03445550426840782 + 0.01 * 6.935554027557373
Epoch 460, val loss: 0.7774643301963806
Epoch 470, training loss: 0.10038720816373825 = 0.030914271250367165 + 0.01 * 6.947294235229492
Epoch 470, val loss: 0.7854763269424438
Epoch 480, training loss: 0.09723371267318726 = 0.0278799906373024 + 0.01 * 6.935372352600098
Epoch 480, val loss: 0.7935525178909302
Epoch 490, training loss: 0.09455884248018265 = 0.02526540867984295 + 0.01 * 6.929343223571777
Epoch 490, val loss: 0.8016451597213745
Epoch 500, training loss: 0.09224563837051392 = 0.023000134155154228 + 0.01 * 6.9245500564575195
Epoch 500, val loss: 0.8097003698348999
Epoch 510, training loss: 0.09024327248334885 = 0.02102748118340969 + 0.01 * 6.921579360961914
Epoch 510, val loss: 0.8176714181900024
Epoch 520, training loss: 0.08848640322685242 = 0.01930128037929535 + 0.01 * 6.918512344360352
Epoch 520, val loss: 0.8255066871643066
Epoch 530, training loss: 0.08703424036502838 = 0.01778358966112137 + 0.01 * 6.925065040588379
Epoch 530, val loss: 0.8332242369651794
Epoch 540, training loss: 0.08560710400342941 = 0.016443448141217232 + 0.01 * 6.916366100311279
Epoch 540, val loss: 0.8407866954803467
Epoch 550, training loss: 0.08437038213014603 = 0.015253548510372639 + 0.01 * 6.911683559417725
Epoch 550, val loss: 0.8481769561767578
Epoch 560, training loss: 0.08328332006931305 = 0.014189977198839188 + 0.01 * 6.909335136413574
Epoch 560, val loss: 0.8554320335388184
Epoch 570, training loss: 0.08229034394025803 = 0.013232305645942688 + 0.01 * 6.90580415725708
Epoch 570, val loss: 0.8625260591506958
Epoch 580, training loss: 0.0814995989203453 = 0.01236577145755291 + 0.01 * 6.9133830070495605
Epoch 580, val loss: 0.8694711923599243
Epoch 590, training loss: 0.08063146471977234 = 0.011581223458051682 + 0.01 * 6.905023574829102
Epoch 590, val loss: 0.8763043284416199
Epoch 600, training loss: 0.07983871549367905 = 0.010868208482861519 + 0.01 * 6.8970513343811035
Epoch 600, val loss: 0.8829720616340637
Epoch 610, training loss: 0.07916384935379028 = 0.010219153948128223 + 0.01 * 6.894469738006592
Epoch 610, val loss: 0.889478862285614
Epoch 620, training loss: 0.07854948937892914 = 0.009627031162381172 + 0.01 * 6.892246246337891
Epoch 620, val loss: 0.8958553671836853
Epoch 630, training loss: 0.07801491767168045 = 0.009086658246815205 + 0.01 * 6.892826080322266
Epoch 630, val loss: 0.9020611643791199
Epoch 640, training loss: 0.07744932919740677 = 0.008593179285526276 + 0.01 * 6.88561487197876
Epoch 640, val loss: 0.9081292748451233
Epoch 650, training loss: 0.07699732482433319 = 0.008141178637742996 + 0.01 * 6.88561487197876
Epoch 650, val loss: 0.9140526652336121
Epoch 660, training loss: 0.07655102014541626 = 0.0077266632579267025 + 0.01 * 6.8824357986450195
Epoch 660, val loss: 0.9198136925697327
Epoch 670, training loss: 0.07615917921066284 = 0.007345394231379032 + 0.01 * 6.881378650665283
Epoch 670, val loss: 0.9254681468009949
Epoch 680, training loss: 0.07581113278865814 = 0.006993533112108707 + 0.01 * 6.881760120391846
Epoch 680, val loss: 0.9309994578361511
Epoch 690, training loss: 0.07539411634206772 = 0.0066674258559942245 + 0.01 * 6.872668743133545
Epoch 690, val loss: 0.9364292621612549
Epoch 700, training loss: 0.07512523233890533 = 0.006363034248352051 + 0.01 * 6.876220226287842
Epoch 700, val loss: 0.9418001770973206
Epoch 710, training loss: 0.0747581273317337 = 0.0060785249806940556 + 0.01 * 6.867960453033447
Epoch 710, val loss: 0.9471175670623779
Epoch 720, training loss: 0.07446282356977463 = 0.0058123329654335976 + 0.01 * 6.865049362182617
Epoch 720, val loss: 0.9523503184318542
Epoch 730, training loss: 0.07419433444738388 = 0.005562060512602329 + 0.01 * 6.863227844238281
Epoch 730, val loss: 0.9576026201248169
Epoch 740, training loss: 0.07397381216287613 = 0.0053276438266038895 + 0.01 * 6.864617347717285
Epoch 740, val loss: 0.9627997279167175
Epoch 750, training loss: 0.07371555268764496 = 0.005108957644551992 + 0.01 * 6.860659599304199
Epoch 750, val loss: 0.967849850654602
Epoch 760, training loss: 0.07346365600824356 = 0.004903777968138456 + 0.01 * 6.855988025665283
Epoch 760, val loss: 0.9728280305862427
Epoch 770, training loss: 0.0734131932258606 = 0.004711075685918331 + 0.01 * 6.870211601257324
Epoch 770, val loss: 0.9777261018753052
Epoch 780, training loss: 0.07307690382003784 = 0.004530350677669048 + 0.01 * 6.854655742645264
Epoch 780, val loss: 0.9825752377510071
Epoch 790, training loss: 0.07284810394048691 = 0.004360650200396776 + 0.01 * 6.848745346069336
Epoch 790, val loss: 0.9873096346855164
Epoch 800, training loss: 0.07268164306879044 = 0.004201082978397608 + 0.01 * 6.848055839538574
Epoch 800, val loss: 0.9919939041137695
Epoch 810, training loss: 0.07248380780220032 = 0.004050697665661573 + 0.01 * 6.843311309814453
Epoch 810, val loss: 0.9966179728507996
Epoch 820, training loss: 0.07246742397546768 = 0.003909971099346876 + 0.01 * 6.855745792388916
Epoch 820, val loss: 1.0011084079742432
Epoch 830, training loss: 0.07220590114593506 = 0.0037784965243190527 + 0.01 * 6.842741012573242
Epoch 830, val loss: 1.0053855180740356
Epoch 840, training loss: 0.07199754565954208 = 0.0036544466856867075 + 0.01 * 6.834310054779053
Epoch 840, val loss: 1.0096426010131836
Epoch 850, training loss: 0.07187075167894363 = 0.003537079319357872 + 0.01 * 6.833366870880127
Epoch 850, val loss: 1.0138320922851562
Epoch 860, training loss: 0.07212040573358536 = 0.003426174633204937 + 0.01 * 6.8694233894348145
Epoch 860, val loss: 1.0179513692855835
Epoch 870, training loss: 0.07160759717226028 = 0.003321367781609297 + 0.01 * 6.8286237716674805
Epoch 870, val loss: 1.0219258069992065
Epoch 880, training loss: 0.07150860130786896 = 0.0032221314031630754 + 0.01 * 6.828647136688232
Epoch 880, val loss: 1.0257644653320312
Epoch 890, training loss: 0.07134334743022919 = 0.0031280098482966423 + 0.01 * 6.821534156799316
Epoch 890, val loss: 1.0296380519866943
Epoch 900, training loss: 0.07173515111207962 = 0.0030386652797460556 + 0.01 * 6.869648456573486
Epoch 900, val loss: 1.0334539413452148
Epoch 910, training loss: 0.07123209536075592 = 0.0029548986349254847 + 0.01 * 6.827719688415527
Epoch 910, val loss: 1.0370618104934692
Epoch 920, training loss: 0.07101848721504211 = 0.0028755427338182926 + 0.01 * 6.814294338226318
Epoch 920, val loss: 1.0405633449554443
Epoch 930, training loss: 0.0709475502371788 = 0.00279990048147738 + 0.01 * 6.814765453338623
Epoch 930, val loss: 1.0440375804901123
Epoch 940, training loss: 0.07088600099086761 = 0.0027277858462184668 + 0.01 * 6.815822124481201
Epoch 940, val loss: 1.0474835634231567
Epoch 950, training loss: 0.07087905704975128 = 0.0026592127978801727 + 0.01 * 6.82198429107666
Epoch 950, val loss: 1.050843358039856
Epoch 960, training loss: 0.07074984908103943 = 0.0025943564251065254 + 0.01 * 6.815549373626709
Epoch 960, val loss: 1.0540097951889038
Epoch 970, training loss: 0.07060036063194275 = 0.002532293787226081 + 0.01 * 6.806806564331055
Epoch 970, val loss: 1.0571742057800293
Epoch 980, training loss: 0.07052808254957199 = 0.0024729217402637005 + 0.01 * 6.805516242980957
Epoch 980, val loss: 1.060288906097412
Epoch 990, training loss: 0.07059896737337112 = 0.0024161923211067915 + 0.01 * 6.818277835845947
Epoch 990, val loss: 1.063366174697876
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8292040063257776
=== training gcn model ===
Epoch 0, training loss: 2.0266377925872803 = 1.9406695365905762 + 0.01 * 8.59681510925293
Epoch 0, val loss: 1.933650016784668
Epoch 10, training loss: 2.0170698165893555 = 1.9311022758483887 + 0.01 * 8.596744537353516
Epoch 10, val loss: 1.9236047267913818
Epoch 20, training loss: 2.005086660385132 = 1.919121503829956 + 0.01 * 8.596510887145996
Epoch 20, val loss: 1.910757064819336
Epoch 30, training loss: 1.9881362915039062 = 1.902178168296814 + 0.01 * 8.595815658569336
Epoch 30, val loss: 1.8926011323928833
Epoch 40, training loss: 1.9631245136260986 = 1.8772006034851074 + 0.01 * 8.59239673614502
Epoch 40, val loss: 1.8662265539169312
Epoch 50, training loss: 1.928167462348938 = 1.8424720764160156 + 0.01 * 8.569536209106445
Epoch 50, val loss: 1.8311444520950317
Epoch 60, training loss: 1.88783860206604 = 1.803229808807373 + 0.01 * 8.460885047912598
Epoch 60, val loss: 1.7956598997116089
Epoch 70, training loss: 1.849348783493042 = 1.7673524618148804 + 0.01 * 8.199627876281738
Epoch 70, val loss: 1.767699122428894
Epoch 80, training loss: 1.8040072917938232 = 1.7242813110351562 + 0.01 * 7.972595691680908
Epoch 80, val loss: 1.7322664260864258
Epoch 90, training loss: 1.7413889169692993 = 1.6652530431747437 + 0.01 * 7.613588333129883
Epoch 90, val loss: 1.6791819334030151
Epoch 100, training loss: 1.6599723100662231 = 1.5859529972076416 + 0.01 * 7.401927947998047
Epoch 100, val loss: 1.6064668893814087
Epoch 110, training loss: 1.5629425048828125 = 1.489715576171875 + 0.01 * 7.322688102722168
Epoch 110, val loss: 1.5213357210159302
Epoch 120, training loss: 1.4563329219818115 = 1.3836143016815186 + 0.01 * 7.2718634605407715
Epoch 120, val loss: 1.4303741455078125
Epoch 130, training loss: 1.3458465337753296 = 1.2734986543655396 + 0.01 * 7.234790325164795
Epoch 130, val loss: 1.3399271965026855
Epoch 140, training loss: 1.2339491844177246 = 1.1619294881820679 + 0.01 * 7.201967239379883
Epoch 140, val loss: 1.2518378496170044
Epoch 150, training loss: 1.124471664428711 = 1.0527499914169312 + 0.01 * 7.1721696853637695
Epoch 150, val loss: 1.1671150922775269
Epoch 160, training loss: 1.0216096639633179 = 0.9501397013664246 + 0.01 * 7.147001266479492
Epoch 160, val loss: 1.0880249738693237
Epoch 170, training loss: 0.9268288612365723 = 0.8555326461791992 + 0.01 * 7.129624366760254
Epoch 170, val loss: 1.0158498287200928
Epoch 180, training loss: 0.8397834300994873 = 0.7685611844062805 + 0.01 * 7.1222243309021
Epoch 180, val loss: 0.9502043724060059
Epoch 190, training loss: 0.7601729035377502 = 0.6889999508857727 + 0.01 * 7.117292881011963
Epoch 190, val loss: 0.8914415836334229
Epoch 200, training loss: 0.6880197525024414 = 0.6168733239173889 + 0.01 * 7.114645481109619
Epoch 200, val loss: 0.8400762677192688
Epoch 210, training loss: 0.6229701042175293 = 0.5518442988395691 + 0.01 * 7.1125807762146
Epoch 210, val loss: 0.7970136404037476
Epoch 220, training loss: 0.5643954277038574 = 0.49328917264938354 + 0.01 * 7.11062479019165
Epoch 220, val loss: 0.7621368169784546
Epoch 230, training loss: 0.5116236209869385 = 0.4405031204223633 + 0.01 * 7.112051963806152
Epoch 230, val loss: 0.7346622943878174
Epoch 240, training loss: 0.4638652801513672 = 0.3927965462207794 + 0.01 * 7.106873512268066
Epoch 240, val loss: 0.7133262753486633
Epoch 250, training loss: 0.4205496907234192 = 0.3495030403137207 + 0.01 * 7.104665279388428
Epoch 250, val loss: 0.6970954537391663
Epoch 260, training loss: 0.381061851978302 = 0.31004565954208374 + 0.01 * 7.101620197296143
Epoch 260, val loss: 0.6853261590003967
Epoch 270, training loss: 0.3448854088783264 = 0.2739061415195465 + 0.01 * 7.097925662994385
Epoch 270, val loss: 0.6775616407394409
Epoch 280, training loss: 0.3115963637828827 = 0.24064938724040985 + 0.01 * 7.094698429107666
Epoch 280, val loss: 0.6730666160583496
Epoch 290, training loss: 0.2810099124908447 = 0.21011532843112946 + 0.01 * 7.0894598960876465
Epoch 290, val loss: 0.6714534759521484
Epoch 300, training loss: 0.2532307505607605 = 0.1824013590812683 + 0.01 * 7.082940101623535
Epoch 300, val loss: 0.6725031733512878
Epoch 310, training loss: 0.22848591208457947 = 0.1577332466840744 + 0.01 * 7.075265884399414
Epoch 310, val loss: 0.6759728789329529
Epoch 320, training loss: 0.20696595311164856 = 0.13622912764549255 + 0.01 * 7.073681831359863
Epoch 320, val loss: 0.6817275285720825
Epoch 330, training loss: 0.18841952085494995 = 0.11780159175395966 + 0.01 * 7.061792850494385
Epoch 330, val loss: 0.6893539428710938
Epoch 340, training loss: 0.17270901799201965 = 0.10216698795557022 + 0.01 * 7.054203987121582
Epoch 340, val loss: 0.6986377835273743
Epoch 350, training loss: 0.15946662425994873 = 0.08896579593420029 + 0.01 * 7.050083160400391
Epoch 350, val loss: 0.7092234492301941
Epoch 360, training loss: 0.14822465181350708 = 0.07783496379852295 + 0.01 * 7.038968563079834
Epoch 360, val loss: 0.7209064364433289
Epoch 370, training loss: 0.13877561688423157 = 0.06843870133161545 + 0.01 * 7.03369140625
Epoch 370, val loss: 0.7333223223686218
Epoch 380, training loss: 0.1308325231075287 = 0.06048832833766937 + 0.01 * 7.034419059753418
Epoch 380, val loss: 0.7462685704231262
Epoch 390, training loss: 0.12399531900882721 = 0.053737420588731766 + 0.01 * 7.025789260864258
Epoch 390, val loss: 0.7593865990638733
Epoch 400, training loss: 0.11819709837436676 = 0.04797929897904396 + 0.01 * 7.021780014038086
Epoch 400, val loss: 0.7726349830627441
Epoch 410, training loss: 0.11318399012088776 = 0.04304521158337593 + 0.01 * 7.013877868652344
Epoch 410, val loss: 0.7857797145843506
Epoch 420, training loss: 0.10895891487598419 = 0.03879758343100548 + 0.01 * 7.0161333084106445
Epoch 420, val loss: 0.7987510561943054
Epoch 430, training loss: 0.10520384460687637 = 0.03512442111968994 + 0.01 * 7.0079426765441895
Epoch 430, val loss: 0.8115148544311523
Epoch 440, training loss: 0.10194362699985504 = 0.03193048760294914 + 0.01 * 7.001314163208008
Epoch 440, val loss: 0.8239881992340088
Epoch 450, training loss: 0.09910652786493301 = 0.029139937832951546 + 0.01 * 6.996659278869629
Epoch 450, val loss: 0.8361405730247498
Epoch 460, training loss: 0.09671264886856079 = 0.0266905277967453 + 0.01 * 7.0022125244140625
Epoch 460, val loss: 0.8480096459388733
Epoch 470, training loss: 0.0944630578160286 = 0.024533957242965698 + 0.01 * 6.992909908294678
Epoch 470, val loss: 0.8594678640365601
Epoch 480, training loss: 0.09249428659677505 = 0.022625349462032318 + 0.01 * 6.986894130706787
Epoch 480, val loss: 0.8706636428833008
Epoch 490, training loss: 0.09074458479881287 = 0.02092668227851391 + 0.01 * 6.981790065765381
Epoch 490, val loss: 0.8815933465957642
Epoch 500, training loss: 0.08918455988168716 = 0.01940908282995224 + 0.01 * 6.977547645568848
Epoch 500, val loss: 0.8922111392021179
Epoch 510, training loss: 0.08782839775085449 = 0.018049515783786774 + 0.01 * 6.977888107299805
Epoch 510, val loss: 0.9025405645370483
Epoch 520, training loss: 0.08653029799461365 = 0.01682903803884983 + 0.01 * 6.970126152038574
Epoch 520, val loss: 0.9125796556472778
Epoch 530, training loss: 0.08539560437202454 = 0.01572795771062374 + 0.01 * 6.9667649269104
Epoch 530, val loss: 0.9223606586456299
Epoch 540, training loss: 0.08434408158063889 = 0.014731005765497684 + 0.01 * 6.961308002471924
Epoch 540, val loss: 0.9318637847900391
Epoch 550, training loss: 0.08349590003490448 = 0.013826113194227219 + 0.01 * 6.966978549957275
Epoch 550, val loss: 0.9411557912826538
Epoch 560, training loss: 0.08258871734142303 = 0.013003522530198097 + 0.01 * 6.95851993560791
Epoch 560, val loss: 0.9501577019691467
Epoch 570, training loss: 0.0817832499742508 = 0.012253041379153728 + 0.01 * 6.953021049499512
Epoch 570, val loss: 0.9588972330093384
Epoch 580, training loss: 0.08104395866394043 = 0.011566855013370514 + 0.01 * 6.9477105140686035
Epoch 580, val loss: 0.9674580693244934
Epoch 590, training loss: 0.08038949966430664 = 0.010937781073153019 + 0.01 * 6.945172309875488
Epoch 590, val loss: 0.9757855534553528
Epoch 600, training loss: 0.07976941764354706 = 0.010360349901020527 + 0.01 * 6.940907001495361
Epoch 600, val loss: 0.98386150598526
Epoch 610, training loss: 0.07922205328941345 = 0.009829514659941196 + 0.01 * 6.939253807067871
Epoch 610, val loss: 0.9917394518852234
Epoch 620, training loss: 0.0786830261349678 = 0.00933950487524271 + 0.01 * 6.934352397918701
Epoch 620, val loss: 0.9994341731071472
Epoch 630, training loss: 0.07821549475193024 = 0.008886593393981457 + 0.01 * 6.932890892028809
Epoch 630, val loss: 1.006898045539856
Epoch 640, training loss: 0.0777287483215332 = 0.008467589505016804 + 0.01 * 6.926115989685059
Epoch 640, val loss: 1.0141812562942505
Epoch 650, training loss: 0.07725004851818085 = 0.008079237304627895 + 0.01 * 6.917080879211426
Epoch 650, val loss: 1.0212912559509277
Epoch 660, training loss: 0.07688914984464645 = 0.007718250155448914 + 0.01 * 6.917089939117432
Epoch 660, val loss: 1.028231143951416
Epoch 670, training loss: 0.07647684216499329 = 0.007382932119071484 + 0.01 * 6.909391403198242
Epoch 670, val loss: 1.0349888801574707
Epoch 680, training loss: 0.07630493491888046 = 0.007070497144013643 + 0.01 * 6.9234442710876465
Epoch 680, val loss: 1.0416021347045898
Epoch 690, training loss: 0.07583852857351303 = 0.006779658142477274 + 0.01 * 6.905887603759766
Epoch 690, val loss: 1.0479499101638794
Epoch 700, training loss: 0.07555843144655228 = 0.006508457474410534 + 0.01 * 6.904997825622559
Epoch 700, val loss: 1.0541518926620483
Epoch 710, training loss: 0.07519466429948807 = 0.006254841573536396 + 0.01 * 6.893982410430908
Epoch 710, val loss: 1.060181975364685
Epoch 720, training loss: 0.07489567250013351 = 0.006017284467816353 + 0.01 * 6.887839317321777
Epoch 720, val loss: 1.0660792589187622
Epoch 730, training loss: 0.07470379769802094 = 0.0057942820712924 + 0.01 * 6.890952110290527
Epoch 730, val loss: 1.0718345642089844
Epoch 740, training loss: 0.0744534283876419 = 0.005584625992923975 + 0.01 * 6.886880397796631
Epoch 740, val loss: 1.0774770975112915
Epoch 750, training loss: 0.07455142587423325 = 0.005387443117797375 + 0.01 * 6.916398525238037
Epoch 750, val loss: 1.0829792022705078
Epoch 760, training loss: 0.07400614023208618 = 0.005202511325478554 + 0.01 * 6.8803629875183105
Epoch 760, val loss: 1.0882439613342285
Epoch 770, training loss: 0.07373969256877899 = 0.0050283316522836685 + 0.01 * 6.871136665344238
Epoch 770, val loss: 1.0934553146362305
Epoch 780, training loss: 0.07383204251527786 = 0.004863865207880735 + 0.01 * 6.896817684173584
Epoch 780, val loss: 1.0985678434371948
Epoch 790, training loss: 0.07339689135551453 = 0.004708640277385712 + 0.01 * 6.868825435638428
Epoch 790, val loss: 1.1034284830093384
Epoch 800, training loss: 0.07312189787626266 = 0.004561958368867636 + 0.01 * 6.855993747711182
Epoch 800, val loss: 1.1082569360733032
Epoch 810, training loss: 0.07306285202503204 = 0.004422974307090044 + 0.01 * 6.863987922668457
Epoch 810, val loss: 1.113012433052063
Epoch 820, training loss: 0.07285070419311523 = 0.0042912596836686134 + 0.01 * 6.855944633483887
Epoch 820, val loss: 1.1175824403762817
Epoch 830, training loss: 0.07261358946561813 = 0.004166392143815756 + 0.01 * 6.844719409942627
Epoch 830, val loss: 1.1220580339431763
Epoch 840, training loss: 0.07267984002828598 = 0.004047949332743883 + 0.01 * 6.863188743591309
Epoch 840, val loss: 1.1265273094177246
Epoch 850, training loss: 0.07235978543758392 = 0.003935487475246191 + 0.01 * 6.842430114746094
Epoch 850, val loss: 1.1307377815246582
Epoch 860, training loss: 0.07215207070112228 = 0.003828628221526742 + 0.01 * 6.832344055175781
Epoch 860, val loss: 1.134945034980774
Epoch 870, training loss: 0.07222937792539597 = 0.0037268870510160923 + 0.01 * 6.850249767303467
Epoch 870, val loss: 1.1390626430511475
Epoch 880, training loss: 0.07189251482486725 = 0.0036300362553447485 + 0.01 * 6.8262481689453125
Epoch 880, val loss: 1.143080472946167
Epoch 890, training loss: 0.07180732488632202 = 0.0035377012100070715 + 0.01 * 6.826962947845459
Epoch 890, val loss: 1.1470229625701904
Epoch 900, training loss: 0.07157690823078156 = 0.0034496551379561424 + 0.01 * 6.81272554397583
Epoch 900, val loss: 1.1508530378341675
Epoch 910, training loss: 0.07173164933919907 = 0.0033655171282589436 + 0.01 * 6.836613655090332
Epoch 910, val loss: 1.1546690464019775
Epoch 920, training loss: 0.071511410176754 = 0.003285320708528161 + 0.01 * 6.822608947753906
Epoch 920, val loss: 1.1582874059677124
Epoch 930, training loss: 0.07132066786289215 = 0.003208623267710209 + 0.01 * 6.811204433441162
Epoch 930, val loss: 1.1619161367416382
Epoch 940, training loss: 0.07113145291805267 = 0.003135253442451358 + 0.01 * 6.799619674682617
Epoch 940, val loss: 1.1654618978500366
Epoch 950, training loss: 0.07110036909580231 = 0.0030650196131318808 + 0.01 * 6.803534984588623
Epoch 950, val loss: 1.1689226627349854
Epoch 960, training loss: 0.07108300924301147 = 0.002997827949002385 + 0.01 * 6.808517932891846
Epoch 960, val loss: 1.1723265647888184
Epoch 970, training loss: 0.07096707820892334 = 0.0029332612175494432 + 0.01 * 6.80338191986084
Epoch 970, val loss: 1.1755902767181396
Epoch 980, training loss: 0.07087606936693192 = 0.0028713790234178305 + 0.01 * 6.800469398498535
Epoch 980, val loss: 1.1788713932037354
Epoch 990, training loss: 0.07076609879732132 = 0.0028118875343352556 + 0.01 * 6.795421600341797
Epoch 990, val loss: 1.1820182800292969
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.026494264602661 = 1.9405262470245361 + 0.01 * 8.596813201904297
Epoch 0, val loss: 1.9413594007492065
Epoch 10, training loss: 2.0165395736694336 = 1.930572271347046 + 0.01 * 8.596725463867188
Epoch 10, val loss: 1.9311878681182861
Epoch 20, training loss: 2.004082679748535 = 1.9181183576583862 + 0.01 * 8.596443176269531
Epoch 20, val loss: 1.9183881282806396
Epoch 30, training loss: 1.9864201545715332 = 1.9004664421081543 + 0.01 * 8.595372200012207
Epoch 30, val loss: 1.900293231010437
Epoch 40, training loss: 1.9603468179702759 = 1.87446129322052 + 0.01 * 8.58855152130127
Epoch 40, val loss: 1.8742763996124268
Epoch 50, training loss: 1.9244046211242676 = 1.8389146327972412 + 0.01 * 8.548998832702637
Epoch 50, val loss: 1.8405511379241943
Epoch 60, training loss: 1.8841886520385742 = 1.800512433052063 + 0.01 * 8.367627143859863
Epoch 60, val loss: 1.8077783584594727
Epoch 70, training loss: 1.8470561504364014 = 1.7655330896377563 + 0.01 * 8.152310371398926
Epoch 70, val loss: 1.7790493965148926
Epoch 80, training loss: 1.7995893955230713 = 1.7202924489974976 + 0.01 * 7.929693222045898
Epoch 80, val loss: 1.7379639148712158
Epoch 90, training loss: 1.7347321510314941 = 1.6575020551681519 + 0.01 * 7.723015308380127
Epoch 90, val loss: 1.6815698146820068
Epoch 100, training loss: 1.6501425504684448 = 1.5744997262954712 + 0.01 * 7.564284324645996
Epoch 100, val loss: 1.6115078926086426
Epoch 110, training loss: 1.554347038269043 = 1.479573369026184 + 0.01 * 7.477370262145996
Epoch 110, val loss: 1.5336785316467285
Epoch 120, training loss: 1.4563921689987183 = 1.3823015689849854 + 0.01 * 7.409057140350342
Epoch 120, val loss: 1.455062747001648
Epoch 130, training loss: 1.3594036102294922 = 1.2859482765197754 + 0.01 * 7.345536231994629
Epoch 130, val loss: 1.3794207572937012
Epoch 140, training loss: 1.2636841535568237 = 1.1906546354293823 + 0.01 * 7.302956581115723
Epoch 140, val loss: 1.3053442239761353
Epoch 150, training loss: 1.1714861392974854 = 1.0986735820770264 + 0.01 * 7.281261444091797
Epoch 150, val loss: 1.235075831413269
Epoch 160, training loss: 1.0862513780593872 = 1.01352858543396 + 0.01 * 7.27227783203125
Epoch 160, val loss: 1.1716561317443848
Epoch 170, training loss: 1.008548378944397 = 0.9358654022216797 + 0.01 * 7.268303394317627
Epoch 170, val loss: 1.11489737033844
Epoch 180, training loss: 0.9360294938087463 = 0.8633721470832825 + 0.01 * 7.265735149383545
Epoch 180, val loss: 1.0615242719650269
Epoch 190, training loss: 0.8664581775665283 = 0.7938203811645508 + 0.01 * 7.263782978057861
Epoch 190, val loss: 1.009498119354248
Epoch 200, training loss: 0.7996947765350342 = 0.727077841758728 + 0.01 * 7.261691093444824
Epoch 200, val loss: 0.9589751362800598
Epoch 210, training loss: 0.7367509603500366 = 0.6641610860824585 + 0.01 * 7.258987903594971
Epoch 210, val loss: 0.9116118550300598
Epoch 220, training loss: 0.677613377571106 = 0.6050596237182617 + 0.01 * 7.255378246307373
Epoch 220, val loss: 0.8683543801307678
Epoch 230, training loss: 0.6209027767181396 = 0.5483947992324829 + 0.01 * 7.250796794891357
Epoch 230, val loss: 0.8291624784469604
Epoch 240, training loss: 0.56546950340271 = 0.4930136799812317 + 0.01 * 7.245582103729248
Epoch 240, val loss: 0.7943024635314941
Epoch 250, training loss: 0.5115597248077393 = 0.4391675293445587 + 0.01 * 7.239218711853027
Epoch 250, val loss: 0.7646387219429016
Epoch 260, training loss: 0.4603702425956726 = 0.388068288564682 + 0.01 * 7.230195999145508
Epoch 260, val loss: 0.741080641746521
Epoch 270, training loss: 0.41318589448928833 = 0.3410073220729828 + 0.01 * 7.217857837677002
Epoch 270, val loss: 0.7239881753921509
Epoch 280, training loss: 0.37067079544067383 = 0.29864904284477234 + 0.01 * 7.202174186706543
Epoch 280, val loss: 0.7124257683753967
Epoch 290, training loss: 0.33284246921539307 = 0.26099643111228943 + 0.01 * 7.184603214263916
Epoch 290, val loss: 0.7055710554122925
Epoch 300, training loss: 0.2994358539581299 = 0.2277289479970932 + 0.01 * 7.170690536499023
Epoch 300, val loss: 0.7024402618408203
Epoch 310, training loss: 0.27005937695503235 = 0.19847694039344788 + 0.01 * 7.158242702484131
Epoch 310, val loss: 0.7022944092750549
Epoch 320, training loss: 0.2443774938583374 = 0.17287255823612213 + 0.01 * 7.150494575500488
Epoch 320, val loss: 0.704601526260376
Epoch 330, training loss: 0.22200165688991547 = 0.15059620141983032 + 0.01 * 7.14054536819458
Epoch 330, val loss: 0.7089958190917969
Epoch 340, training loss: 0.20266085863113403 = 0.13130831718444824 + 0.01 * 7.135254383087158
Epoch 340, val loss: 0.7150608897209167
Epoch 350, training loss: 0.18596898019313812 = 0.11467909812927246 + 0.01 * 7.128988265991211
Epoch 350, val loss: 0.722453773021698
Epoch 360, training loss: 0.17163756489753723 = 0.10038335621356964 + 0.01 * 7.12542200088501
Epoch 360, val loss: 0.7308917045593262
Epoch 370, training loss: 0.1593083292245865 = 0.08812149614095688 + 0.01 * 7.118683338165283
Epoch 370, val loss: 0.7400891780853271
Epoch 380, training loss: 0.1487506926059723 = 0.0776142030954361 + 0.01 * 7.113648891448975
Epoch 380, val loss: 0.7498272657394409
Epoch 390, training loss: 0.13980504870414734 = 0.06860213726758957 + 0.01 * 7.120290756225586
Epoch 390, val loss: 0.7598750591278076
Epoch 400, training loss: 0.13187019526958466 = 0.060868170112371445 + 0.01 * 7.1002020835876465
Epoch 400, val loss: 0.7700631022453308
Epoch 410, training loss: 0.12512461841106415 = 0.05421588197350502 + 0.01 * 7.090874195098877
Epoch 410, val loss: 0.7803291082382202
Epoch 420, training loss: 0.11931563913822174 = 0.048474960029125214 + 0.01 * 7.0840678215026855
Epoch 420, val loss: 0.7906203269958496
Epoch 430, training loss: 0.11427406221628189 = 0.04350491613149643 + 0.01 * 7.0769147872924805
Epoch 430, val loss: 0.8008086681365967
Epoch 440, training loss: 0.10987438261508942 = 0.03919214755296707 + 0.01 * 7.068223476409912
Epoch 440, val loss: 0.810883104801178
Epoch 450, training loss: 0.10607870668172836 = 0.03543803095817566 + 0.01 * 7.064067840576172
Epoch 450, val loss: 0.8208213448524475
Epoch 460, training loss: 0.10267315804958344 = 0.03215709701180458 + 0.01 * 7.051605701446533
Epoch 460, val loss: 0.830600917339325
Epoch 470, training loss: 0.09974586963653564 = 0.029279831796884537 + 0.01 * 7.046603679656982
Epoch 470, val loss: 0.8402194976806641
Epoch 480, training loss: 0.0972491055727005 = 0.02675117924809456 + 0.01 * 7.049792289733887
Epoch 480, val loss: 0.8496196269989014
Epoch 490, training loss: 0.09481911361217499 = 0.024523429572582245 + 0.01 * 7.029568672180176
Epoch 490, val loss: 0.8587952256202698
Epoch 500, training loss: 0.0927705243229866 = 0.022553695365786552 + 0.01 * 7.0216827392578125
Epoch 500, val loss: 0.8677657842636108
Epoch 510, training loss: 0.09098587185144424 = 0.02080644480884075 + 0.01 * 7.017942428588867
Epoch 510, val loss: 0.8764989376068115
Epoch 520, training loss: 0.08942221850156784 = 0.019251378253102303 + 0.01 * 7.01708459854126
Epoch 520, val loss: 0.8850185871124268
Epoch 530, training loss: 0.08801834285259247 = 0.01786290481686592 + 0.01 * 7.0155439376831055
Epoch 530, val loss: 0.8933143615722656
Epoch 540, training loss: 0.08666307479143143 = 0.016620049253106117 + 0.01 * 7.004302978515625
Epoch 540, val loss: 0.9013615250587463
Epoch 550, training loss: 0.08541154116392136 = 0.015503444708883762 + 0.01 * 6.990809917449951
Epoch 550, val loss: 0.9092200398445129
Epoch 560, training loss: 0.08434595912694931 = 0.014497443102300167 + 0.01 * 6.984851837158203
Epoch 560, val loss: 0.9168795347213745
Epoch 570, training loss: 0.08345916867256165 = 0.013589238747954369 + 0.01 * 6.986992835998535
Epoch 570, val loss: 0.92429119348526
Epoch 580, training loss: 0.08254457265138626 = 0.012766455300152302 + 0.01 * 6.977811336517334
Epoch 580, val loss: 0.9315185546875
Epoch 590, training loss: 0.08169713616371155 = 0.01201830804347992 + 0.01 * 6.967883110046387
Epoch 590, val loss: 0.9385490417480469
Epoch 600, training loss: 0.08111607283353806 = 0.01133638434112072 + 0.01 * 6.977968692779541
Epoch 600, val loss: 0.9454237222671509
Epoch 610, training loss: 0.08030402660369873 = 0.010714174248278141 + 0.01 * 6.958985805511475
Epoch 610, val loss: 0.9520733952522278
Epoch 620, training loss: 0.07969282567501068 = 0.010144179686903954 + 0.01 * 6.954864501953125
Epoch 620, val loss: 0.9585400819778442
Epoch 630, training loss: 0.07919584214687347 = 0.009620869532227516 + 0.01 * 6.957497596740723
Epoch 630, val loss: 0.9648606181144714
Epoch 640, training loss: 0.07857392728328705 = 0.009139688685536385 + 0.01 * 6.943424224853516
Epoch 640, val loss: 0.9709848761558533
Epoch 650, training loss: 0.07811413705348969 = 0.008695654571056366 + 0.01 * 6.9418487548828125
Epoch 650, val loss: 0.9769737124443054
Epoch 660, training loss: 0.07787914574146271 = 0.008286040276288986 + 0.01 * 6.959310054779053
Epoch 660, val loss: 0.9827862977981567
Epoch 670, training loss: 0.07722368836402893 = 0.007907135412096977 + 0.01 * 6.931654930114746
Epoch 670, val loss: 0.9884586930274963
Epoch 680, training loss: 0.07692006230354309 = 0.007554964628070593 + 0.01 * 6.93651008605957
Epoch 680, val loss: 0.9939822554588318
Epoch 690, training loss: 0.07647141814231873 = 0.0072274282574653625 + 0.01 * 6.924399375915527
Epoch 690, val loss: 0.9993693232536316
Epoch 700, training loss: 0.07616152614355087 = 0.006921934429556131 + 0.01 * 6.923959255218506
Epoch 700, val loss: 1.004645824432373
Epoch 710, training loss: 0.07571282237768173 = 0.006636800244450569 + 0.01 * 6.907602787017822
Epoch 710, val loss: 1.0097613334655762
Epoch 720, training loss: 0.07553791254758835 = 0.006370448507368565 + 0.01 * 6.916746616363525
Epoch 720, val loss: 1.0148686170578003
Epoch 730, training loss: 0.07521342486143112 = 0.006121530197560787 + 0.01 * 6.909189701080322
Epoch 730, val loss: 1.0197006464004517
Epoch 740, training loss: 0.07490961253643036 = 0.00588873540982604 + 0.01 * 6.902087688446045
Epoch 740, val loss: 1.024437427520752
Epoch 750, training loss: 0.07469847798347473 = 0.00567030580714345 + 0.01 * 6.9028167724609375
Epoch 750, val loss: 1.0290849208831787
Epoch 760, training loss: 0.07435188442468643 = 0.005464928224682808 + 0.01 * 6.888696193695068
Epoch 760, val loss: 1.0335432291030884
Epoch 770, training loss: 0.07406016439199448 = 0.0052722180262207985 + 0.01 * 6.878795146942139
Epoch 770, val loss: 1.0379962921142578
Epoch 780, training loss: 0.07382185757160187 = 0.005090671591460705 + 0.01 * 6.873118877410889
Epoch 780, val loss: 1.0422557592391968
Epoch 790, training loss: 0.07373720407485962 = 0.004919690079987049 + 0.01 * 6.881751537322998
Epoch 790, val loss: 1.0464869737625122
Epoch 800, training loss: 0.07353644073009491 = 0.004758917726576328 + 0.01 * 6.877752304077148
Epoch 800, val loss: 1.050479769706726
Epoch 810, training loss: 0.07338626682758331 = 0.0046067978255450726 + 0.01 * 6.877946853637695
Epoch 810, val loss: 1.054419994354248
Epoch 820, training loss: 0.07320018112659454 = 0.004463099408894777 + 0.01 * 6.873708248138428
Epoch 820, val loss: 1.058279275894165
Epoch 830, training loss: 0.07293134927749634 = 0.0043270341120660305 + 0.01 * 6.860431671142578
Epoch 830, val loss: 1.061982274055481
Epoch 840, training loss: 0.07280363887548447 = 0.0041984072886407375 + 0.01 * 6.860523223876953
Epoch 840, val loss: 1.0656921863555908
Epoch 850, training loss: 0.07268978655338287 = 0.004076422192156315 + 0.01 * 6.861336708068848
Epoch 850, val loss: 1.06929349899292
Epoch 860, training loss: 0.07244742661714554 = 0.0039605810306966305 + 0.01 * 6.848684787750244
Epoch 860, val loss: 1.0727465152740479
Epoch 870, training loss: 0.07223786413669586 = 0.0038506262935698032 + 0.01 * 6.838724136352539
Epoch 870, val loss: 1.0762187242507935
Epoch 880, training loss: 0.07223479449748993 = 0.003746307222172618 + 0.01 * 6.848848819732666
Epoch 880, val loss: 1.0794724225997925
Epoch 890, training loss: 0.07190535217523575 = 0.0036468689795583487 + 0.01 * 6.825848579406738
Epoch 890, val loss: 1.0826689004898071
Epoch 900, training loss: 0.07182732224464417 = 0.003552301088348031 + 0.01 * 6.827502250671387
Epoch 900, val loss: 1.085901141166687
Epoch 910, training loss: 0.07172898948192596 = 0.003461984219029546 + 0.01 * 6.826700210571289
Epoch 910, val loss: 1.088929295539856
Epoch 920, training loss: 0.07151903212070465 = 0.0033757188357412815 + 0.01 * 6.814332008361816
Epoch 920, val loss: 1.0919649600982666
Epoch 930, training loss: 0.07166476547718048 = 0.0032934059854596853 + 0.01 * 6.837136268615723
Epoch 930, val loss: 1.094926357269287
Epoch 940, training loss: 0.07132920622825623 = 0.003214943688362837 + 0.01 * 6.811426639556885
Epoch 940, val loss: 1.0977784395217896
Epoch 950, training loss: 0.07122951745986938 = 0.003140013664960861 + 0.01 * 6.808949947357178
Epoch 950, val loss: 1.1005785465240479
Epoch 960, training loss: 0.07123944908380508 = 0.003068235469982028 + 0.01 * 6.8171210289001465
Epoch 960, val loss: 1.1032754182815552
Epoch 970, training loss: 0.07096540927886963 = 0.002999559510499239 + 0.01 * 6.7965850830078125
Epoch 970, val loss: 1.1059527397155762
Epoch 980, training loss: 0.07121790945529938 = 0.002933736890554428 + 0.01 * 6.828417778015137
Epoch 980, val loss: 1.108542561531067
Epoch 990, training loss: 0.07073511183261871 = 0.0028707804158329964 + 0.01 * 6.78643274307251
Epoch 990, val loss: 1.1110504865646362
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8413284132841329
The final CL Acc:0.80741, 0.01090, The final GNN Acc:0.83535, 0.00495
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10518])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0246388912200928 = 1.9386701583862305 + 0.01 * 8.596874237060547
Epoch 0, val loss: 1.9290574789047241
Epoch 10, training loss: 2.015424966812134 = 1.9294564723968506 + 0.01 * 8.596855163574219
Epoch 10, val loss: 1.9201524257659912
Epoch 20, training loss: 2.004702091217041 = 1.9187345504760742 + 0.01 * 8.596755981445312
Epoch 20, val loss: 1.9094034433364868
Epoch 30, training loss: 1.9901723861694336 = 1.9042079448699951 + 0.01 * 8.59644889831543
Epoch 30, val loss: 1.8943815231323242
Epoch 40, training loss: 1.9691766500473022 = 1.8832260370254517 + 0.01 * 8.595057487487793
Epoch 40, val loss: 1.872442364692688
Epoch 50, training loss: 1.9395294189453125 = 1.853672742843628 + 0.01 * 8.585663795471191
Epoch 50, val loss: 1.842529058456421
Epoch 60, training loss: 1.90321683883667 = 1.817804217338562 + 0.01 * 8.541264533996582
Epoch 60, val loss: 1.8100731372833252
Epoch 70, training loss: 1.8689450025558472 = 1.785424828529358 + 0.01 * 8.352015495300293
Epoch 70, val loss: 1.787575125694275
Epoch 80, training loss: 1.8350322246551514 = 1.7523027658462524 + 0.01 * 8.272944450378418
Epoch 80, val loss: 1.7653120756149292
Epoch 90, training loss: 1.78720223903656 = 1.7059842348098755 + 0.01 * 8.121800422668457
Epoch 90, val loss: 1.7276856899261475
Epoch 100, training loss: 1.720379114151001 = 1.6416925191879272 + 0.01 * 7.868654251098633
Epoch 100, val loss: 1.673744559288025
Epoch 110, training loss: 1.6336605548858643 = 1.556904911994934 + 0.01 * 7.675560474395752
Epoch 110, val loss: 1.6051350831985474
Epoch 120, training loss: 1.5319545269012451 = 1.4562535285949707 + 0.01 * 7.570097923278809
Epoch 120, val loss: 1.5264946222305298
Epoch 130, training loss: 1.4218394756317139 = 1.3468248844146729 + 0.01 * 7.501463890075684
Epoch 130, val loss: 1.4426438808441162
Epoch 140, training loss: 1.3074802160263062 = 1.2326982021331787 + 0.01 * 7.478199481964111
Epoch 140, val loss: 1.3558191061019897
Epoch 150, training loss: 1.1915780305862427 = 1.1169086694717407 + 0.01 * 7.4669318199157715
Epoch 150, val loss: 1.2674062252044678
Epoch 160, training loss: 1.0771270990371704 = 1.0025194883346558 + 0.01 * 7.460762977600098
Epoch 160, val loss: 1.1803381443023682
Epoch 170, training loss: 0.9675538539886475 = 0.8929700255393982 + 0.01 * 7.458379745483398
Epoch 170, val loss: 1.0973988771438599
Epoch 180, training loss: 0.8658761382102966 = 0.7912883162498474 + 0.01 * 7.458783149719238
Epoch 180, val loss: 1.021639108657837
Epoch 190, training loss: 0.7739424705505371 = 0.6993415951728821 + 0.01 * 7.460089206695557
Epoch 190, val loss: 0.9542369842529297
Epoch 200, training loss: 0.692205548286438 = 0.6175952553749084 + 0.01 * 7.461032390594482
Epoch 200, val loss: 0.8953867554664612
Epoch 210, training loss: 0.6198548078536987 = 0.5452402234077454 + 0.01 * 7.461461067199707
Epoch 210, val loss: 0.8447911739349365
Epoch 220, training loss: 0.5555683374404907 = 0.4809536933898926 + 0.01 * 7.4614667892456055
Epoch 220, val loss: 0.8018236756324768
Epoch 230, training loss: 0.497932106256485 = 0.42332109808921814 + 0.01 * 7.461101055145264
Epoch 230, val loss: 0.7655145525932312
Epoch 240, training loss: 0.4457162916660309 = 0.3711114525794983 + 0.01 * 7.460483074188232
Epoch 240, val loss: 0.7349780201911926
Epoch 250, training loss: 0.39812007546424866 = 0.3235231339931488 + 0.01 * 7.459695339202881
Epoch 250, val loss: 0.7093321084976196
Epoch 260, training loss: 0.35490310192108154 = 0.2803153991699219 + 0.01 * 7.4587721824646
Epoch 260, val loss: 0.6883277893066406
Epoch 270, training loss: 0.31620723009109497 = 0.241629496216774 + 0.01 * 7.457774639129639
Epoch 270, val loss: 0.6719395518302917
Epoch 280, training loss: 0.28219205141067505 = 0.20762234926223755 + 0.01 * 7.456970691680908
Epoch 280, val loss: 0.6601136326789856
Epoch 290, training loss: 0.25280752778053284 = 0.17824919521808624 + 0.01 * 7.455832481384277
Epoch 290, val loss: 0.6525843143463135
Epoch 300, training loss: 0.22774483263492584 = 0.15320409834384918 + 0.01 * 7.454073905944824
Epoch 300, val loss: 0.6489318013191223
Epoch 310, training loss: 0.2065460979938507 = 0.13203337788581848 + 0.01 * 7.451272487640381
Epoch 310, val loss: 0.6485865712165833
Epoch 320, training loss: 0.18871384859085083 = 0.11423053592443466 + 0.01 * 7.448331356048584
Epoch 320, val loss: 0.6509280800819397
Epoch 330, training loss: 0.1737094223499298 = 0.09927881509065628 + 0.01 * 7.443060398101807
Epoch 330, val loss: 0.6554429531097412
Epoch 340, training loss: 0.16105982661247253 = 0.08670507371425629 + 0.01 * 7.435475826263428
Epoch 340, val loss: 0.6616279482841492
Epoch 350, training loss: 0.15035274624824524 = 0.07610326260328293 + 0.01 * 7.424948215484619
Epoch 350, val loss: 0.6690007448196411
Epoch 360, training loss: 0.14122013747692108 = 0.06713058054447174 + 0.01 * 7.408956050872803
Epoch 360, val loss: 0.6772236227989197
Epoch 370, training loss: 0.13340766727924347 = 0.059504490345716476 + 0.01 * 7.390317440032959
Epoch 370, val loss: 0.6859949231147766
Epoch 380, training loss: 0.12687361240386963 = 0.05299798399209976 + 0.01 * 7.387562274932861
Epoch 380, val loss: 0.6951875686645508
Epoch 390, training loss: 0.12100617587566376 = 0.04742646589875221 + 0.01 * 7.357970714569092
Epoch 390, val loss: 0.7046089172363281
Epoch 400, training loss: 0.11609819531440735 = 0.04263119027018547 + 0.01 * 7.346701145172119
Epoch 400, val loss: 0.7141120433807373
Epoch 410, training loss: 0.11172351241111755 = 0.03848778083920479 + 0.01 * 7.323573589324951
Epoch 410, val loss: 0.7235695719718933
Epoch 420, training loss: 0.10777965188026428 = 0.03489307686686516 + 0.01 * 7.288657188415527
Epoch 420, val loss: 0.7329807281494141
Epoch 430, training loss: 0.10453468561172485 = 0.031760308891534805 + 0.01 * 7.277437210083008
Epoch 430, val loss: 0.7422484755516052
Epoch 440, training loss: 0.10169661045074463 = 0.029019992798566818 + 0.01 * 7.2676615715026855
Epoch 440, val loss: 0.7513489127159119
Epoch 450, training loss: 0.09915292263031006 = 0.026612956076860428 + 0.01 * 7.2539963722229
Epoch 450, val loss: 0.7602667808532715
Epoch 460, training loss: 0.09688989818096161 = 0.024490494281053543 + 0.01 * 7.239940166473389
Epoch 460, val loss: 0.7689648270606995
Epoch 470, training loss: 0.09491758048534393 = 0.02261168882250786 + 0.01 * 7.230589866638184
Epoch 470, val loss: 0.7774330377578735
Epoch 480, training loss: 0.09308172762393951 = 0.020942125469446182 + 0.01 * 7.213960647583008
Epoch 480, val loss: 0.7857112288475037
Epoch 490, training loss: 0.09166423976421356 = 0.01945253275334835 + 0.01 * 7.221170425415039
Epoch 490, val loss: 0.7938007116317749
Epoch 500, training loss: 0.09014957398176193 = 0.018120460212230682 + 0.01 * 7.202911376953125
Epoch 500, val loss: 0.8015556931495667
Epoch 510, training loss: 0.08889761567115784 = 0.016923395916819572 + 0.01 * 7.197422027587891
Epoch 510, val loss: 0.809118926525116
Epoch 520, training loss: 0.08748623728752136 = 0.015844151377677917 + 0.01 * 7.164208889007568
Epoch 520, val loss: 0.8164322376251221
Epoch 530, training loss: 0.08670095354318619 = 0.014870398677885532 + 0.01 * 7.183055400848389
Epoch 530, val loss: 0.8235545754432678
Epoch 540, training loss: 0.08551410585641861 = 0.013989284634590149 + 0.01 * 7.152482032775879
Epoch 540, val loss: 0.830373227596283
Epoch 550, training loss: 0.08445272594690323 = 0.013188072480261326 + 0.01 * 7.126465320587158
Epoch 550, val loss: 0.8370245099067688
Epoch 560, training loss: 0.08387324213981628 = 0.012458615936338902 + 0.01 * 7.141462802886963
Epoch 560, val loss: 0.843480110168457
Epoch 570, training loss: 0.08299630135297775 = 0.011793529614806175 + 0.01 * 7.120276927947998
Epoch 570, val loss: 0.8496536612510681
Epoch 580, training loss: 0.08211396634578705 = 0.01118391938507557 + 0.01 * 7.0930047035217285
Epoch 580, val loss: 0.8556527495384216
Epoch 590, training loss: 0.08154071867465973 = 0.010624195449054241 + 0.01 * 7.0916523933410645
Epoch 590, val loss: 0.8614723682403564
Epoch 600, training loss: 0.08118462562561035 = 0.01010894775390625 + 0.01 * 7.107568264007568
Epoch 600, val loss: 0.867117166519165
Epoch 610, training loss: 0.08042120933532715 = 0.009634205140173435 + 0.01 * 7.078701019287109
Epoch 610, val loss: 0.8725236058235168
Epoch 620, training loss: 0.0796545147895813 = 0.009194856509566307 + 0.01 * 7.045966148376465
Epoch 620, val loss: 0.8778162598609924
Epoch 630, training loss: 0.07925313711166382 = 0.008788141421973705 + 0.01 * 7.046499729156494
Epoch 630, val loss: 0.882980227470398
Epoch 640, training loss: 0.07892423868179321 = 0.008411158807575703 + 0.01 * 7.051307678222656
Epoch 640, val loss: 0.8879436254501343
Epoch 650, training loss: 0.07837557792663574 = 0.008060271851718426 + 0.01 * 7.03153133392334
Epoch 650, val loss: 0.892682671546936
Epoch 660, training loss: 0.07783985137939453 = 0.007733331993222237 + 0.01 * 7.0106520652771
Epoch 660, val loss: 0.8973998427391052
Epoch 670, training loss: 0.0774216577410698 = 0.007428182289004326 + 0.01 * 6.99934720993042
Epoch 670, val loss: 0.9019590616226196
Epoch 680, training loss: 0.07732655107975006 = 0.007142626214772463 + 0.01 * 7.018393039703369
Epoch 680, val loss: 0.9064164161682129
Epoch 690, training loss: 0.07691830396652222 = 0.006875443272292614 + 0.01 * 7.004286766052246
Epoch 690, val loss: 0.9106336236000061
Epoch 700, training loss: 0.07668258994817734 = 0.006625249516218901 + 0.01 * 7.005733966827393
Epoch 700, val loss: 0.9148776531219482
Epoch 710, training loss: 0.07622495293617249 = 0.00638985401019454 + 0.01 * 6.9835100173950195
Epoch 710, val loss: 0.9188808798789978
Epoch 720, training loss: 0.07604309916496277 = 0.006168406456708908 + 0.01 * 6.98746919631958
Epoch 720, val loss: 0.9230046272277832
Epoch 730, training loss: 0.07585129886865616 = 0.005959856323897839 + 0.01 * 6.989144325256348
Epoch 730, val loss: 0.926802933216095
Epoch 740, training loss: 0.07526306062936783 = 0.005763387773185968 + 0.01 * 6.949967384338379
Epoch 740, val loss: 0.9306119084358215
Epoch 750, training loss: 0.07519882917404175 = 0.005578000098466873 + 0.01 * 6.962083339691162
Epoch 750, val loss: 0.9343559145927429
Epoch 760, training loss: 0.074835404753685 = 0.005402684677392244 + 0.01 * 6.943272113800049
Epoch 760, val loss: 0.9379612803459167
Epoch 770, training loss: 0.07472063601016998 = 0.005236817989498377 + 0.01 * 6.9483819007873535
Epoch 770, val loss: 0.9415324926376343
Epoch 780, training loss: 0.07463005185127258 = 0.00507988128811121 + 0.01 * 6.955017566680908
Epoch 780, val loss: 0.944977343082428
Epoch 790, training loss: 0.07432136684656143 = 0.004931136965751648 + 0.01 * 6.939023017883301
Epoch 790, val loss: 0.9483747482299805
Epoch 800, training loss: 0.07402104139328003 = 0.0047896141186356544 + 0.01 * 6.923142910003662
Epoch 800, val loss: 0.9515953660011292
Epoch 810, training loss: 0.07409096509218216 = 0.004655158147215843 + 0.01 * 6.943580627441406
Epoch 810, val loss: 0.9549580812454224
Epoch 820, training loss: 0.07385150343179703 = 0.004527437500655651 + 0.01 * 6.932407379150391
Epoch 820, val loss: 0.9581721425056458
Epoch 830, training loss: 0.07360055297613144 = 0.004405982326716185 + 0.01 * 6.91945743560791
Epoch 830, val loss: 0.9612615704536438
Epoch 840, training loss: 0.07348328828811646 = 0.0042902324348688126 + 0.01 * 6.919305801391602
Epoch 840, val loss: 0.9643152356147766
Epoch 850, training loss: 0.07312197238206863 = 0.004179926123470068 + 0.01 * 6.894205093383789
Epoch 850, val loss: 0.967252254486084
Epoch 860, training loss: 0.07318129390478134 = 0.00407449621707201 + 0.01 * 6.910679817199707
Epoch 860, val loss: 0.9702951908111572
Epoch 870, training loss: 0.07304131984710693 = 0.003973984159529209 + 0.01 * 6.906733989715576
Epoch 870, val loss: 0.9730769395828247
Epoch 880, training loss: 0.0727282240986824 = 0.0038780474569648504 + 0.01 * 6.8850178718566895
Epoch 880, val loss: 0.9759629964828491
Epoch 890, training loss: 0.07247325032949448 = 0.0037863729521632195 + 0.01 * 6.868688106536865
Epoch 890, val loss: 0.97866290807724
Epoch 900, training loss: 0.07287036627531052 = 0.003698445623740554 + 0.01 * 6.917191982269287
Epoch 900, val loss: 0.9814373850822449
Epoch 910, training loss: 0.0723896324634552 = 0.0036143041215837 + 0.01 * 6.877532958984375
Epoch 910, val loss: 0.9840788841247559
Epoch 920, training loss: 0.07240105420351028 = 0.003533561946824193 + 0.01 * 6.886748790740967
Epoch 920, val loss: 0.9866665601730347
Epoch 930, training loss: 0.07219061255455017 = 0.0034561387728899717 + 0.01 * 6.873447895050049
Epoch 930, val loss: 0.9893124103546143
Epoch 940, training loss: 0.07206384092569351 = 0.0033819880336523056 + 0.01 * 6.868185520172119
Epoch 940, val loss: 0.9917872548103333
Epoch 950, training loss: 0.0717371329665184 = 0.003310650819912553 + 0.01 * 6.842648506164551
Epoch 950, val loss: 0.9943720102310181
Epoch 960, training loss: 0.07171815633773804 = 0.003242303617298603 + 0.01 * 6.847585201263428
Epoch 960, val loss: 0.9967888593673706
Epoch 970, training loss: 0.07153387367725372 = 0.00317652290686965 + 0.01 * 6.835735321044922
Epoch 970, val loss: 0.999204158782959
Epoch 980, training loss: 0.07151436805725098 = 0.003113251645117998 + 0.01 * 6.840111255645752
Epoch 980, val loss: 1.0015114545822144
Epoch 990, training loss: 0.07145996391773224 = 0.0030522458255290985 + 0.01 * 6.84077262878418
Epoch 990, val loss: 1.0038915872573853
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 2.034294366836548 = 1.9483258724212646 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.9427732229232788
Epoch 10, training loss: 2.023273229598999 = 1.9373053312301636 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.932504653930664
Epoch 20, training loss: 2.009582042694092 = 1.9236162900924683 + 0.01 * 8.59657096862793
Epoch 20, val loss: 1.9193483591079712
Epoch 30, training loss: 1.9905465841293335 = 1.904587745666504 + 0.01 * 8.5958890914917
Epoch 30, val loss: 1.9008750915527344
Epoch 40, training loss: 1.963266372680664 = 1.877346396446228 + 0.01 * 8.592001914978027
Epoch 40, val loss: 1.8749253749847412
Epoch 50, training loss: 1.9271137714385986 = 1.8414403200149536 + 0.01 * 8.56734848022461
Epoch 50, val loss: 1.8429737091064453
Epoch 60, training loss: 1.8896766901016235 = 1.8050583600997925 + 0.01 * 8.461831092834473
Epoch 60, val loss: 1.814895510673523
Epoch 70, training loss: 1.8581503629684448 = 1.7751080989837646 + 0.01 * 8.304227828979492
Epoch 70, val loss: 1.7915244102478027
Epoch 80, training loss: 1.817564606666565 = 1.7364718914031982 + 0.01 * 8.109267234802246
Epoch 80, val loss: 1.7553613185882568
Epoch 90, training loss: 1.7606269121170044 = 1.6824772357940674 + 0.01 * 7.814962387084961
Epoch 90, val loss: 1.7066313028335571
Epoch 100, training loss: 1.6835565567016602 = 1.6078161001205444 + 0.01 * 7.574043273925781
Epoch 100, val loss: 1.642774224281311
Epoch 110, training loss: 1.5892744064331055 = 1.5148262977600098 + 0.01 * 7.444813251495361
Epoch 110, val loss: 1.5649898052215576
Epoch 120, training loss: 1.4868189096450806 = 1.4129048585891724 + 0.01 * 7.391409397125244
Epoch 120, val loss: 1.4818354845046997
Epoch 130, training loss: 1.3832978010177612 = 1.309604287147522 + 0.01 * 7.369353771209717
Epoch 130, val loss: 1.3996330499649048
Epoch 140, training loss: 1.2805616855621338 = 1.2070353031158447 + 0.01 * 7.352638244628906
Epoch 140, val loss: 1.3211702108383179
Epoch 150, training loss: 1.1801148653030396 = 1.1066697835922241 + 0.01 * 7.344510555267334
Epoch 150, val loss: 1.2462453842163086
Epoch 160, training loss: 1.0830469131469727 = 1.0096293687820435 + 0.01 * 7.341752052307129
Epoch 160, val loss: 1.1752538681030273
Epoch 170, training loss: 0.990478515625 = 0.9170835614204407 + 0.01 * 7.33949613571167
Epoch 170, val loss: 1.1083725690841675
Epoch 180, training loss: 0.9036967754364014 = 0.8303390741348267 + 0.01 * 7.3357672691345215
Epoch 180, val loss: 1.046367883682251
Epoch 190, training loss: 0.8242073655128479 = 0.7509176135063171 + 0.01 * 7.328973770141602
Epoch 190, val loss: 0.990989089012146
Epoch 200, training loss: 0.7530859708786011 = 0.6799173951148987 + 0.01 * 7.316857814788818
Epoch 200, val loss: 0.9443700909614563
Epoch 210, training loss: 0.6901824474334717 = 0.6171759366989136 + 0.01 * 7.3006510734558105
Epoch 210, val loss: 0.9070571064949036
Epoch 220, training loss: 0.6342264413833618 = 0.5614185929298401 + 0.01 * 7.280783176422119
Epoch 220, val loss: 0.8778495192527771
Epoch 230, training loss: 0.5835437774658203 = 0.510965883731842 + 0.01 * 7.257787227630615
Epoch 230, val loss: 0.8547390699386597
Epoch 240, training loss: 0.5367470979690552 = 0.4643406867980957 + 0.01 * 7.240638256072998
Epoch 240, val loss: 0.8360784649848938
Epoch 250, training loss: 0.4925404489040375 = 0.42028194665908813 + 0.01 * 7.225851058959961
Epoch 250, val loss: 0.8205015659332275
Epoch 260, training loss: 0.45026034116744995 = 0.37806156277656555 + 0.01 * 7.219876289367676
Epoch 260, val loss: 0.8073412775993347
Epoch 270, training loss: 0.40977051854133606 = 0.3376055955886841 + 0.01 * 7.216492176055908
Epoch 270, val loss: 0.796493411064148
Epoch 280, training loss: 0.3714039921760559 = 0.2993479371070862 + 0.01 * 7.205605983734131
Epoch 280, val loss: 0.7882447838783264
Epoch 290, training loss: 0.3359088897705078 = 0.2638869285583496 + 0.01 * 7.202195167541504
Epoch 290, val loss: 0.7826297283172607
Epoch 300, training loss: 0.30366677045822144 = 0.23170040547847748 + 0.01 * 7.196635723114014
Epoch 300, val loss: 0.7792271971702576
Epoch 310, training loss: 0.2749825119972229 = 0.20299912989139557 + 0.01 * 7.198337078094482
Epoch 310, val loss: 0.777649998664856
Epoch 320, training loss: 0.24972519278526306 = 0.1777798980474472 + 0.01 * 7.194530487060547
Epoch 320, val loss: 0.7776047587394714
Epoch 330, training loss: 0.2277049720287323 = 0.1558140516281128 + 0.01 * 7.18909215927124
Epoch 330, val loss: 0.7789284586906433
Epoch 340, training loss: 0.20862732827663422 = 0.13679763674736023 + 0.01 * 7.182969570159912
Epoch 340, val loss: 0.7815980911254883
Epoch 350, training loss: 0.1921648383140564 = 0.12037859112024307 + 0.01 * 7.178625583648682
Epoch 350, val loss: 0.7853559851646423
Epoch 360, training loss: 0.17803657054901123 = 0.106204092502594 + 0.01 * 7.183248996734619
Epoch 360, val loss: 0.790186882019043
Epoch 370, training loss: 0.16568505764007568 = 0.0939500704407692 + 0.01 * 7.17349910736084
Epoch 370, val loss: 0.7958533763885498
Epoch 380, training loss: 0.1552940309047699 = 0.08332890272140503 + 0.01 * 7.196512699127197
Epoch 380, val loss: 0.8023029565811157
Epoch 390, training loss: 0.14585164189338684 = 0.0741216391324997 + 0.01 * 7.173001289367676
Epoch 390, val loss: 0.8093406558036804
Epoch 400, training loss: 0.1377175748348236 = 0.06611310690641403 + 0.01 * 7.160447597503662
Epoch 400, val loss: 0.8169596195220947
Epoch 410, training loss: 0.1306455284357071 = 0.05913149192929268 + 0.01 * 7.151403903961182
Epoch 410, val loss: 0.824994683265686
Epoch 420, training loss: 0.12447857856750488 = 0.05303412303328514 + 0.01 * 7.144445896148682
Epoch 420, val loss: 0.8333982825279236
Epoch 430, training loss: 0.11910077929496765 = 0.04770639166235924 + 0.01 * 7.139439105987549
Epoch 430, val loss: 0.8420242667198181
Epoch 440, training loss: 0.1143382340669632 = 0.04305153712630272 + 0.01 * 7.128669738769531
Epoch 440, val loss: 0.85079425573349
Epoch 450, training loss: 0.11028481274843216 = 0.038967929780483246 + 0.01 * 7.131688594818115
Epoch 450, val loss: 0.859765350818634
Epoch 460, training loss: 0.10664010047912598 = 0.03538106754422188 + 0.01 * 7.125904083251953
Epoch 460, val loss: 0.8687520623207092
Epoch 470, training loss: 0.1033167839050293 = 0.032223302870988846 + 0.01 * 7.109348773956299
Epoch 470, val loss: 0.8777832388877869
Epoch 480, training loss: 0.10049346089363098 = 0.029433419927954674 + 0.01 * 7.106003761291504
Epoch 480, val loss: 0.8867934346199036
Epoch 490, training loss: 0.09786592423915863 = 0.026968400925397873 + 0.01 * 7.089752674102783
Epoch 490, val loss: 0.8956946134567261
Epoch 500, training loss: 0.09557389467954636 = 0.024783505126833916 + 0.01 * 7.079038619995117
Epoch 500, val loss: 0.9044769406318665
Epoch 510, training loss: 0.0936305969953537 = 0.022842492908239365 + 0.01 * 7.078810691833496
Epoch 510, val loss: 0.9131054282188416
Epoch 520, training loss: 0.09191113710403442 = 0.021114515140652657 + 0.01 * 7.079662322998047
Epoch 520, val loss: 0.9214865565299988
Epoch 530, training loss: 0.0902593582868576 = 0.019573112949728966 + 0.01 * 7.068624496459961
Epoch 530, val loss: 0.9296802878379822
Epoch 540, training loss: 0.08869858831167221 = 0.018194347620010376 + 0.01 * 7.050424098968506
Epoch 540, val loss: 0.9376700520515442
Epoch 550, training loss: 0.08747072517871857 = 0.016957208514213562 + 0.01 * 7.051351547241211
Epoch 550, val loss: 0.945448637008667
Epoch 560, training loss: 0.08625463396310806 = 0.015845559537410736 + 0.01 * 7.040907859802246
Epoch 560, val loss: 0.9529822468757629
Epoch 570, training loss: 0.08522309362888336 = 0.014842651784420013 + 0.01 * 7.038043975830078
Epoch 570, val loss: 0.9603148102760315
Epoch 580, training loss: 0.08406062424182892 = 0.013935217633843422 + 0.01 * 7.012540817260742
Epoch 580, val loss: 0.9674144387245178
Epoch 590, training loss: 0.08345946669578552 = 0.013111068867146969 + 0.01 * 7.034840106964111
Epoch 590, val loss: 0.9743368625640869
Epoch 600, training loss: 0.08249808102846146 = 0.01236211508512497 + 0.01 * 7.013596534729004
Epoch 600, val loss: 0.9809609651565552
Epoch 610, training loss: 0.0818466991186142 = 0.011679275892674923 + 0.01 * 7.016742706298828
Epoch 610, val loss: 0.9874118566513062
Epoch 620, training loss: 0.08094266802072525 = 0.01105445809662342 + 0.01 * 6.988821506500244
Epoch 620, val loss: 0.9936705231666565
Epoch 630, training loss: 0.08044255524873734 = 0.010481773875653744 + 0.01 * 6.9960784912109375
Epoch 630, val loss: 0.9997470378875732
Epoch 640, training loss: 0.07977519929409027 = 0.009956245310604572 + 0.01 * 6.981895446777344
Epoch 640, val loss: 1.00558340549469
Epoch 650, training loss: 0.0792391374707222 = 0.009472091682255268 + 0.01 * 6.9767045974731445
Epoch 650, val loss: 1.011297583580017
Epoch 660, training loss: 0.07892835140228271 = 0.009025579318404198 + 0.01 * 6.990277290344238
Epoch 660, val loss: 1.016802430152893
Epoch 670, training loss: 0.07824306935071945 = 0.008612683042883873 + 0.01 * 6.963038921356201
Epoch 670, val loss: 1.0221517086029053
Epoch 680, training loss: 0.07810711860656738 = 0.008229904808104038 + 0.01 * 6.9877214431762695
Epoch 680, val loss: 1.0273802280426025
Epoch 690, training loss: 0.07749135792255402 = 0.007874892093241215 + 0.01 * 6.961647033691406
Epoch 690, val loss: 1.0323814153671265
Epoch 700, training loss: 0.07704464346170425 = 0.0075448621064424515 + 0.01 * 6.949978828430176
Epoch 700, val loss: 1.0372529029846191
Epoch 710, training loss: 0.07716459780931473 = 0.0072370851412415504 + 0.01 * 6.992751598358154
Epoch 710, val loss: 1.042013168334961
Epoch 720, training loss: 0.0763932317495346 = 0.006950113456696272 + 0.01 * 6.944312572479248
Epoch 720, val loss: 1.0466078519821167
Epoch 730, training loss: 0.07626263797283173 = 0.006681830622255802 + 0.01 * 6.958081245422363
Epoch 730, val loss: 1.0510892868041992
Epoch 740, training loss: 0.07579009234905243 = 0.006431039422750473 + 0.01 * 6.9359049797058105
Epoch 740, val loss: 1.055433750152588
Epoch 750, training loss: 0.0755317434668541 = 0.006195986643433571 + 0.01 * 6.9335761070251465
Epoch 750, val loss: 1.0596816539764404
Epoch 760, training loss: 0.07522393763065338 = 0.0059751905500888824 + 0.01 * 6.924875259399414
Epoch 760, val loss: 1.0638011693954468
Epoch 770, training loss: 0.07501789927482605 = 0.0057678199373185635 + 0.01 * 6.9250078201293945
Epoch 770, val loss: 1.0678577423095703
Epoch 780, training loss: 0.07482481002807617 = 0.005572594236582518 + 0.01 * 6.9252214431762695
Epoch 780, val loss: 1.0717524290084839
Epoch 790, training loss: 0.07445129752159119 = 0.005388723686337471 + 0.01 * 6.906257629394531
Epoch 790, val loss: 1.075608253479004
Epoch 800, training loss: 0.07445064187049866 = 0.00521528534591198 + 0.01 * 6.9235358238220215
Epoch 800, val loss: 1.0792980194091797
Epoch 810, training loss: 0.07415592670440674 = 0.00505157932639122 + 0.01 * 6.910435199737549
Epoch 810, val loss: 1.082927942276001
Epoch 820, training loss: 0.07374204695224762 = 0.004896469414234161 + 0.01 * 6.884557723999023
Epoch 820, val loss: 1.086477279663086
Epoch 830, training loss: 0.07388357073068619 = 0.004749428015202284 + 0.01 * 6.913414478302002
Epoch 830, val loss: 1.0899804830551147
Epoch 840, training loss: 0.07352554798126221 = 0.004610547795891762 + 0.01 * 6.891500473022461
Epoch 840, val loss: 1.0933599472045898
Epoch 850, training loss: 0.07366246730089188 = 0.004478652495890856 + 0.01 * 6.918381214141846
Epoch 850, val loss: 1.0966907739639282
Epoch 860, training loss: 0.07338394969701767 = 0.004353564698249102 + 0.01 * 6.903038024902344
Epoch 860, val loss: 1.0998769998550415
Epoch 870, training loss: 0.073001928627491 = 0.004234743770211935 + 0.01 * 6.876718044281006
Epoch 870, val loss: 1.1030199527740479
Epoch 880, training loss: 0.0729542151093483 = 0.004121667705476284 + 0.01 * 6.8832550048828125
Epoch 880, val loss: 1.106123447418213
Epoch 890, training loss: 0.0727112740278244 = 0.004013782367110252 + 0.01 * 6.869749069213867
Epoch 890, val loss: 1.10912024974823
Epoch 900, training loss: 0.0724973976612091 = 0.003911146428436041 + 0.01 * 6.858625411987305
Epoch 900, val loss: 1.11210036277771
Epoch 910, training loss: 0.07249576598405838 = 0.003813063958659768 + 0.01 * 6.868269920349121
Epoch 910, val loss: 1.1149564981460571
Epoch 920, training loss: 0.07237185537815094 = 0.0037195717450231314 + 0.01 * 6.865228652954102
Epoch 920, val loss: 1.1177821159362793
Epoch 930, training loss: 0.0724579244852066 = 0.0036302655935287476 + 0.01 * 6.882766246795654
Epoch 930, val loss: 1.120552897453308
Epoch 940, training loss: 0.07200656831264496 = 0.0035450835712254047 + 0.01 * 6.846148490905762
Epoch 940, val loss: 1.1232398748397827
Epoch 950, training loss: 0.07207576185464859 = 0.0034634985495358706 + 0.01 * 6.8612260818481445
Epoch 950, val loss: 1.1259033679962158
Epoch 960, training loss: 0.07170318812131882 = 0.003385336371138692 + 0.01 * 6.831785202026367
Epoch 960, val loss: 1.1285117864608765
Epoch 970, training loss: 0.07182980328798294 = 0.0033105299808084965 + 0.01 * 6.851927280426025
Epoch 970, val loss: 1.1310750246047974
Epoch 980, training loss: 0.07168406993150711 = 0.003238717559725046 + 0.01 * 6.844535827636719
Epoch 980, val loss: 1.133542537689209
Epoch 990, training loss: 0.07136707007884979 = 0.003169967094436288 + 0.01 * 6.8197102546691895
Epoch 990, val loss: 1.1359919309616089
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 2.0447726249694824 = 1.9588043689727783 + 0.01 * 8.596831321716309
Epoch 0, val loss: 1.9564977884292603
Epoch 10, training loss: 2.0341224670410156 = 1.9481546878814697 + 0.01 * 8.596778869628906
Epoch 10, val loss: 1.9463163614273071
Epoch 20, training loss: 2.0215840339660645 = 1.9356181621551514 + 0.01 * 8.596582412719727
Epoch 20, val loss: 1.9338361024856567
Epoch 30, training loss: 2.0045433044433594 = 1.918583869934082 + 0.01 * 8.59593677520752
Epoch 30, val loss: 1.9165072441101074
Epoch 40, training loss: 1.9797173738479614 = 1.8937904834747314 + 0.01 * 8.592689514160156
Epoch 40, val loss: 1.8914095163345337
Epoch 50, training loss: 1.9446709156036377 = 1.8589591979980469 + 0.01 * 8.5711669921875
Epoch 50, val loss: 1.8576244115829468
Epoch 60, training loss: 1.9037525653839111 = 1.8190423250198364 + 0.01 * 8.471027374267578
Epoch 60, val loss: 1.8231841325759888
Epoch 70, training loss: 1.8704100847244263 = 1.787048578262329 + 0.01 * 8.336153984069824
Epoch 70, val loss: 1.7998168468475342
Epoch 80, training loss: 1.8348661661148071 = 1.752932071685791 + 0.01 * 8.19340705871582
Epoch 80, val loss: 1.7703803777694702
Epoch 90, training loss: 1.7851474285125732 = 1.7062060832977295 + 0.01 * 7.894129276275635
Epoch 90, val loss: 1.7288994789123535
Epoch 100, training loss: 1.7162569761276245 = 1.6399561166763306 + 0.01 * 7.630083084106445
Epoch 100, val loss: 1.6725038290023804
Epoch 110, training loss: 1.626818299293518 = 1.551680326461792 + 0.01 * 7.51379919052124
Epoch 110, val loss: 1.5988845825195312
Epoch 120, training loss: 1.5229588747024536 = 1.4485336542129517 + 0.01 * 7.4425249099731445
Epoch 120, val loss: 1.5163830518722534
Epoch 130, training loss: 1.4146414995193481 = 1.3408254384994507 + 0.01 * 7.3816094398498535
Epoch 130, val loss: 1.4330780506134033
Epoch 140, training loss: 1.3076212406158447 = 1.234045386314392 + 0.01 * 7.35758638381958
Epoch 140, val loss: 1.3541066646575928
Epoch 150, training loss: 1.2042598724365234 = 1.1307729482650757 + 0.01 * 7.348694801330566
Epoch 150, val loss: 1.2800382375717163
Epoch 160, training loss: 1.1059136390686035 = 1.0325162410736084 + 0.01 * 7.339738368988037
Epoch 160, val loss: 1.2107336521148682
Epoch 170, training loss: 1.0146164894104004 = 0.9413350224494934 + 0.01 * 7.328149318695068
Epoch 170, val loss: 1.1472375392913818
Epoch 180, training loss: 0.9327812790870667 = 0.8596455454826355 + 0.01 * 7.313573837280273
Epoch 180, val loss: 1.0916081666946411
Epoch 190, training loss: 0.8621266484260559 = 0.7891926765441895 + 0.01 * 7.293398380279541
Epoch 190, val loss: 1.0459598302841187
Epoch 200, training loss: 0.8026219010353088 = 0.7299172282218933 + 0.01 * 7.270467281341553
Epoch 200, val loss: 1.0107276439666748
Epoch 210, training loss: 0.7525417804718018 = 0.6800395250320435 + 0.01 * 7.2502264976501465
Epoch 210, val loss: 0.9844968318939209
Epoch 220, training loss: 0.709252119064331 = 0.6368460655212402 + 0.01 * 7.240603446960449
Epoch 220, val loss: 0.9644166231155396
Epoch 230, training loss: 0.6696773767471313 = 0.5973831415176392 + 0.01 * 7.229422092437744
Epoch 230, val loss: 0.9472587704658508
Epoch 240, training loss: 0.6310072541236877 = 0.5587663650512695 + 0.01 * 7.224087238311768
Epoch 240, val loss: 0.9300662875175476
Epoch 250, training loss: 0.5906964540481567 = 0.518501341342926 + 0.01 * 7.219508647918701
Epoch 250, val loss: 0.9109272360801697
Epoch 260, training loss: 0.5471327900886536 = 0.47498032450675964 + 0.01 * 7.215245246887207
Epoch 260, val loss: 0.8891457319259644
Epoch 270, training loss: 0.5001256465911865 = 0.4279974400997162 + 0.01 * 7.2128190994262695
Epoch 270, val loss: 0.8655461668968201
Epoch 280, training loss: 0.4510166049003601 = 0.3789319694042206 + 0.01 * 7.208465099334717
Epoch 280, val loss: 0.8424152135848999
Epoch 290, training loss: 0.40227431058883667 = 0.3302278220653534 + 0.01 * 7.204647541046143
Epoch 290, val loss: 0.8222523331642151
Epoch 300, training loss: 0.35655537247657776 = 0.28455761075019836 + 0.01 * 7.199776649475098
Epoch 300, val loss: 0.8071410059928894
Epoch 310, training loss: 0.31591320037841797 = 0.243948832154274 + 0.01 * 7.196435451507568
Epoch 310, val loss: 0.7981805801391602
Epoch 320, training loss: 0.28117427229881287 = 0.20922629535198212 + 0.01 * 7.194797992706299
Epoch 320, val loss: 0.7952120900154114
Epoch 330, training loss: 0.251970112323761 = 0.1800622045993805 + 0.01 * 7.190791606903076
Epoch 330, val loss: 0.7972949743270874
Epoch 340, training loss: 0.22751092910766602 = 0.15563015639781952 + 0.01 * 7.1880784034729
Epoch 340, val loss: 0.8033827543258667
Epoch 350, training loss: 0.20689068734645844 = 0.13507845997810364 + 0.01 * 7.181222915649414
Epoch 350, val loss: 0.8124459385871887
Epoch 360, training loss: 0.18947497010231018 = 0.11770123243331909 + 0.01 * 7.177374839782715
Epoch 360, val loss: 0.8237533569335938
Epoch 370, training loss: 0.1746530532836914 = 0.10294301062822342 + 0.01 * 7.171003818511963
Epoch 370, val loss: 0.8367419242858887
Epoch 380, training loss: 0.16200807690620422 = 0.09036724269390106 + 0.01 * 7.164083957672119
Epoch 380, val loss: 0.8509553074836731
Epoch 390, training loss: 0.15123316645622253 = 0.0796244665980339 + 0.01 * 7.16087007522583
Epoch 390, val loss: 0.8660275936126709
Epoch 400, training loss: 0.1419544517993927 = 0.07043085992336273 + 0.01 * 7.152359962463379
Epoch 400, val loss: 0.881653368473053
Epoch 410, training loss: 0.13431774079799652 = 0.06254757195711136 + 0.01 * 7.1770172119140625
Epoch 410, val loss: 0.8976436853408813
Epoch 420, training loss: 0.1271473467350006 = 0.0557841919362545 + 0.01 * 7.13631534576416
Epoch 420, val loss: 0.9135046005249023
Epoch 430, training loss: 0.12125716358423233 = 0.049963273108005524 + 0.01 * 7.12938928604126
Epoch 430, val loss: 0.9293314814567566
Epoch 440, training loss: 0.11610937118530273 = 0.044937338680028915 + 0.01 * 7.117203712463379
Epoch 440, val loss: 0.9449266791343689
Epoch 450, training loss: 0.11188749969005585 = 0.04058239981532097 + 0.01 * 7.1305108070373535
Epoch 450, val loss: 0.9602054357528687
Epoch 460, training loss: 0.1078270822763443 = 0.036800727248191833 + 0.01 * 7.102635383605957
Epoch 460, val loss: 0.975014328956604
Epoch 470, training loss: 0.10447600483894348 = 0.03350204974412918 + 0.01 * 7.097395896911621
Epoch 470, val loss: 0.9895090460777283
Epoch 480, training loss: 0.10149014741182327 = 0.03061234951019287 + 0.01 * 7.087779998779297
Epoch 480, val loss: 1.0035086870193481
Epoch 490, training loss: 0.09889288246631622 = 0.028072653338313103 + 0.01 * 7.082022666931152
Epoch 490, val loss: 1.017128825187683
Epoch 500, training loss: 0.09654595702886581 = 0.025832390412688255 + 0.01 * 7.071356773376465
Epoch 500, val loss: 1.0302237272262573
Epoch 510, training loss: 0.09467476606369019 = 0.023847529664635658 + 0.01 * 7.082724094390869
Epoch 510, val loss: 1.0429375171661377
Epoch 520, training loss: 0.09275703877210617 = 0.022081570699810982 + 0.01 * 7.06754732131958
Epoch 520, val loss: 1.0551239252090454
Epoch 530, training loss: 0.09102380275726318 = 0.020502839237451553 + 0.01 * 7.052096843719482
Epoch 530, val loss: 1.0669736862182617
Epoch 540, training loss: 0.08959035575389862 = 0.019084181636571884 + 0.01 * 7.0506181716918945
Epoch 540, val loss: 1.078400731086731
Epoch 550, training loss: 0.08824285119771957 = 0.017805494368076324 + 0.01 * 7.043735980987549
Epoch 550, val loss: 1.0894325971603394
Epoch 560, training loss: 0.08703801035881042 = 0.016647689044475555 + 0.01 * 7.039031982421875
Epoch 560, val loss: 1.1000062227249146
Epoch 570, training loss: 0.08596453815698624 = 0.01559602003544569 + 0.01 * 7.03685188293457
Epoch 570, val loss: 1.1103790998458862
Epoch 580, training loss: 0.08482176810503006 = 0.014640347100794315 + 0.01 * 7.018142223358154
Epoch 580, val loss: 1.120295763015747
Epoch 590, training loss: 0.08399835973978043 = 0.013768794946372509 + 0.01 * 7.022956848144531
Epoch 590, val loss: 1.1298701763153076
Epoch 600, training loss: 0.08304404467344284 = 0.012972770258784294 + 0.01 * 7.007127285003662
Epoch 600, val loss: 1.1392381191253662
Epoch 610, training loss: 0.08240225166082382 = 0.012244385667145252 + 0.01 * 7.015786647796631
Epoch 610, val loss: 1.148238182067871
Epoch 620, training loss: 0.08155442029237747 = 0.01157752238214016 + 0.01 * 6.997689723968506
Epoch 620, val loss: 1.1569591760635376
Epoch 630, training loss: 0.08082743734121323 = 0.010965029709041119 + 0.01 * 6.986241340637207
Epoch 630, val loss: 1.1654759645462036
Epoch 640, training loss: 0.08022917807102203 = 0.010401349514722824 + 0.01 * 6.982782363891602
Epoch 640, val loss: 1.1737241744995117
Epoch 650, training loss: 0.07977264374494553 = 0.009881897829473019 + 0.01 * 6.98907470703125
Epoch 650, val loss: 1.181581974029541
Epoch 660, training loss: 0.07919846475124359 = 0.00940261222422123 + 0.01 * 6.979585647583008
Epoch 660, val loss: 1.189317226409912
Epoch 670, training loss: 0.07850541174411774 = 0.008959462866187096 + 0.01 * 6.954594612121582
Epoch 670, val loss: 1.1967887878417969
Epoch 680, training loss: 0.07817293703556061 = 0.008548464626073837 + 0.01 * 6.962447643280029
Epoch 680, val loss: 1.2041071653366089
Epoch 690, training loss: 0.07766035199165344 = 0.008167628198862076 + 0.01 * 6.949272632598877
Epoch 690, val loss: 1.2111191749572754
Epoch 700, training loss: 0.0773852989077568 = 0.007813721895217896 + 0.01 * 6.957158088684082
Epoch 700, val loss: 1.2179057598114014
Epoch 710, training loss: 0.07695066183805466 = 0.0074844262562692165 + 0.01 * 6.946623802185059
Epoch 710, val loss: 1.2245357036590576
Epoch 720, training loss: 0.07656823098659515 = 0.007177766878157854 + 0.01 * 6.939046382904053
Epoch 720, val loss: 1.2309741973876953
Epoch 730, training loss: 0.07631118595600128 = 0.006891175638884306 + 0.01 * 6.9420013427734375
Epoch 730, val loss: 1.237267255783081
Epoch 740, training loss: 0.07590769976377487 = 0.006623334251344204 + 0.01 * 6.928436756134033
Epoch 740, val loss: 1.2431687116622925
Epoch 750, training loss: 0.07568468153476715 = 0.006372550036758184 + 0.01 * 6.93121337890625
Epoch 750, val loss: 1.2491976022720337
Epoch 760, training loss: 0.07522174715995789 = 0.006137554999440908 + 0.01 * 6.908419132232666
Epoch 760, val loss: 1.2547475099563599
Epoch 770, training loss: 0.07513753324747086 = 0.005916877184063196 + 0.01 * 6.922065258026123
Epoch 770, val loss: 1.260496973991394
Epoch 780, training loss: 0.0748061016201973 = 0.005709430668503046 + 0.01 * 6.909667015075684
Epoch 780, val loss: 1.2657170295715332
Epoch 790, training loss: 0.07441504299640656 = 0.0055144065991044044 + 0.01 * 6.890063762664795
Epoch 790, val loss: 1.271028995513916
Epoch 800, training loss: 0.0744386538863182 = 0.005330443382263184 + 0.01 * 6.910820960998535
Epoch 800, val loss: 1.2761656045913696
Epoch 810, training loss: 0.07399366796016693 = 0.0051567209884524345 + 0.01 * 6.883695125579834
Epoch 810, val loss: 1.2810513973236084
Epoch 820, training loss: 0.07389452308416367 = 0.004992663860321045 + 0.01 * 6.890186309814453
Epoch 820, val loss: 1.2859212160110474
Epoch 830, training loss: 0.07374240458011627 = 0.004837647080421448 + 0.01 * 6.890476226806641
Epoch 830, val loss: 1.2906818389892578
Epoch 840, training loss: 0.0734253078699112 = 0.0046907453797757626 + 0.01 * 6.8734564781188965
Epoch 840, val loss: 1.295163869857788
Epoch 850, training loss: 0.0733664259314537 = 0.004551619756966829 + 0.01 * 6.881481170654297
Epoch 850, val loss: 1.2996721267700195
Epoch 860, training loss: 0.07307589054107666 = 0.004419866017997265 + 0.01 * 6.865602970123291
Epoch 860, val loss: 1.3040410280227661
Epoch 870, training loss: 0.07280093431472778 = 0.00429481640458107 + 0.01 * 6.850612640380859
Epoch 870, val loss: 1.3082597255706787
Epoch 880, training loss: 0.07275121659040451 = 0.004175843205302954 + 0.01 * 6.857537269592285
Epoch 880, val loss: 1.3124067783355713
Epoch 890, training loss: 0.072590172290802 = 0.0040627820417284966 + 0.01 * 6.852739334106445
Epoch 890, val loss: 1.3164387941360474
Epoch 900, training loss: 0.07256480306386948 = 0.003955106716603041 + 0.01 * 6.8609700202941895
Epoch 900, val loss: 1.320310115814209
Epoch 910, training loss: 0.0723867192864418 = 0.003852630965411663 + 0.01 * 6.853409290313721
Epoch 910, val loss: 1.3243076801300049
Epoch 920, training loss: 0.07233720272779465 = 0.0037548886612057686 + 0.01 * 6.858231067657471
Epoch 920, val loss: 1.3279699087142944
Epoch 930, training loss: 0.071971096098423 = 0.0036616444122046232 + 0.01 * 6.830945014953613
Epoch 930, val loss: 1.3316278457641602
Epoch 940, training loss: 0.0719018504023552 = 0.0035726716741919518 + 0.01 * 6.832918167114258
Epoch 940, val loss: 1.3352932929992676
Epoch 950, training loss: 0.07196390628814697 = 0.0034877473954111338 + 0.01 * 6.847615718841553
Epoch 950, val loss: 1.338742971420288
Epoch 960, training loss: 0.07173735648393631 = 0.003406485076993704 + 0.01 * 6.833086967468262
Epoch 960, val loss: 1.3421440124511719
Epoch 970, training loss: 0.07155346125364304 = 0.003328865859657526 + 0.01 * 6.822459697723389
Epoch 970, val loss: 1.3453338146209717
Epoch 980, training loss: 0.07140947133302689 = 0.0032544529531151056 + 0.01 * 6.815502166748047
Epoch 980, val loss: 1.3487762212753296
Epoch 990, training loss: 0.07126843184232712 = 0.0031831737142056227 + 0.01 * 6.808526039123535
Epoch 990, val loss: 1.3517965078353882
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.812335266209805
The final CL Acc:0.77531, 0.01552, The final GNN Acc:0.81075, 0.00114
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13168])
remove edge: torch.Size([2, 7864])
updated graph: torch.Size([2, 10476])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.029568672180176 = 1.9436001777648926 + 0.01 * 8.596846580505371
Epoch 0, val loss: 1.9526140689849854
Epoch 10, training loss: 2.0198426246643066 = 1.9338748455047607 + 0.01 * 8.596773147583008
Epoch 10, val loss: 1.9434624910354614
Epoch 20, training loss: 2.007357597351074 = 1.9213917255401611 + 0.01 * 8.596577644348145
Epoch 20, val loss: 1.9313273429870605
Epoch 30, training loss: 1.989548921585083 = 1.9035894870758057 + 0.01 * 8.595949172973633
Epoch 30, val loss: 1.913751244544983
Epoch 40, training loss: 1.9631257057189941 = 1.8772038221359253 + 0.01 * 8.592191696166992
Epoch 40, val loss: 1.887872576713562
Epoch 50, training loss: 1.9266090393066406 = 1.8409956693649292 + 0.01 * 8.56134033203125
Epoch 50, val loss: 1.8536585569381714
Epoch 60, training loss: 1.8847993612289429 = 1.8011199235916138 + 0.01 * 8.367942810058594
Epoch 60, val loss: 1.8179855346679688
Epoch 70, training loss: 1.846356987953186 = 1.7644023895263672 + 0.01 * 8.19545841217041
Epoch 70, val loss: 1.783356785774231
Epoch 80, training loss: 1.7984117269515991 = 1.7180333137512207 + 0.01 * 8.037837982177734
Epoch 80, val loss: 1.737944483757019
Epoch 90, training loss: 1.732948899269104 = 1.6541717052459717 + 0.01 * 7.877723217010498
Epoch 90, val loss: 1.68056058883667
Epoch 100, training loss: 1.6492879390716553 = 1.572266697883606 + 0.01 * 7.702126502990723
Epoch 100, val loss: 1.6110754013061523
Epoch 110, training loss: 1.5576790571212769 = 1.4829614162445068 + 0.01 * 7.471759796142578
Epoch 110, val loss: 1.5359505414962769
Epoch 120, training loss: 1.468733310699463 = 1.3947261571884155 + 0.01 * 7.400716304779053
Epoch 120, val loss: 1.4621022939682007
Epoch 130, training loss: 1.3807075023651123 = 1.3072824478149414 + 0.01 * 7.342500686645508
Epoch 130, val loss: 1.3890243768692017
Epoch 140, training loss: 1.2916548252105713 = 1.218499779701233 + 0.01 * 7.315507411956787
Epoch 140, val loss: 1.3163461685180664
Epoch 150, training loss: 1.2026176452636719 = 1.1296690702438354 + 0.01 * 7.294852256774902
Epoch 150, val loss: 1.2460501194000244
Epoch 160, training loss: 1.116431713104248 = 1.0437005758285522 + 0.01 * 7.273116588592529
Epoch 160, val loss: 1.1805170774459839
Epoch 170, training loss: 1.0346335172653198 = 0.9621514081954956 + 0.01 * 7.2482099533081055
Epoch 170, val loss: 1.1201475858688354
Epoch 180, training loss: 0.9568747878074646 = 0.884599506855011 + 0.01 * 7.227527141571045
Epoch 180, val loss: 1.0638952255249023
Epoch 190, training loss: 0.8815120458602905 = 0.8094309568405151 + 0.01 * 7.208108425140381
Epoch 190, val loss: 1.0095934867858887
Epoch 200, training loss: 0.8068966269493103 = 0.734936535358429 + 0.01 * 7.196009159088135
Epoch 200, val loss: 0.9559856057167053
Epoch 210, training loss: 0.7324020862579346 = 0.6606168150901794 + 0.01 * 7.178528308868408
Epoch 210, val loss: 0.9034404754638672
Epoch 220, training loss: 0.6591854095458984 = 0.5875187516212463 + 0.01 * 7.166668891906738
Epoch 220, val loss: 0.8537164926528931
Epoch 230, training loss: 0.5896104574203491 = 0.5180621147155762 + 0.01 * 7.154837608337402
Epoch 230, val loss: 0.8093766570091248
Epoch 240, training loss: 0.5265082716941833 = 0.4550490081310272 + 0.01 * 7.145928859710693
Epoch 240, val loss: 0.7728689908981323
Epoch 250, training loss: 0.4713934659957886 = 0.40004661679267883 + 0.01 * 7.134684085845947
Epoch 250, val loss: 0.7451958060264587
Epoch 260, training loss: 0.42405033111572266 = 0.3528161346912384 + 0.01 * 7.1234211921691895
Epoch 260, val loss: 0.7256382703781128
Epoch 270, training loss: 0.3830398619174957 = 0.31195583939552307 + 0.01 * 7.108402729034424
Epoch 270, val loss: 0.7125710248947144
Epoch 280, training loss: 0.3470393121242523 = 0.27595770359039307 + 0.01 * 7.108160495758057
Epoch 280, val loss: 0.7043754458427429
Epoch 290, training loss: 0.3147619366645813 = 0.24380584061145782 + 0.01 * 7.095611572265625
Epoch 290, val loss: 0.6999271512031555
Epoch 300, training loss: 0.2857727110385895 = 0.21492651104927063 + 0.01 * 7.084620952606201
Epoch 300, val loss: 0.6983591914176941
Epoch 310, training loss: 0.25988438725471497 = 0.18910352885723114 + 0.01 * 7.078085422515869
Epoch 310, val loss: 0.6991084218025208
Epoch 320, training loss: 0.23701265454292297 = 0.16623623669147491 + 0.01 * 7.077641010284424
Epoch 320, val loss: 0.7020426392555237
Epoch 330, training loss: 0.21685509383678436 = 0.14617156982421875 + 0.01 * 7.068352222442627
Epoch 330, val loss: 0.7071439027786255
Epoch 340, training loss: 0.1993100792169571 = 0.12866486608982086 + 0.01 * 7.064521312713623
Epoch 340, val loss: 0.7141885161399841
Epoch 350, training loss: 0.18404921889305115 = 0.11343849450349808 + 0.01 * 7.061073303222656
Epoch 350, val loss: 0.7230509519577026
Epoch 360, training loss: 0.1707771122455597 = 0.10022250562906265 + 0.01 * 7.055459976196289
Epoch 360, val loss: 0.7334534525871277
Epoch 370, training loss: 0.1593954861164093 = 0.08877377212047577 + 0.01 * 7.0621724128723145
Epoch 370, val loss: 0.7451502084732056
Epoch 380, training loss: 0.14936164021492004 = 0.07887009531259537 + 0.01 * 7.049153804779053
Epoch 380, val loss: 0.7577454447746277
Epoch 390, training loss: 0.14071957767009735 = 0.07028666883707047 + 0.01 * 7.043291091918945
Epoch 390, val loss: 0.7710691690444946
Epoch 400, training loss: 0.13322636485099792 = 0.0628403052687645 + 0.01 * 7.038606643676758
Epoch 400, val loss: 0.7848820090293884
Epoch 410, training loss: 0.12670184671878815 = 0.05636816844344139 + 0.01 * 7.03336763381958
Epoch 410, val loss: 0.7989836931228638
Epoch 420, training loss: 0.12132968008518219 = 0.05073123052716255 + 0.01 * 7.059844970703125
Epoch 420, val loss: 0.8132137656211853
Epoch 430, training loss: 0.11617036163806915 = 0.04582078009843826 + 0.01 * 7.034958362579346
Epoch 430, val loss: 0.8273886442184448
Epoch 440, training loss: 0.11177591979503632 = 0.04152604192495346 + 0.01 * 7.024988174438477
Epoch 440, val loss: 0.8414121866226196
Epoch 450, training loss: 0.10791780054569244 = 0.03775628283619881 + 0.01 * 7.0161519050598145
Epoch 450, val loss: 0.8553079962730408
Epoch 460, training loss: 0.1045471727848053 = 0.03443673625588417 + 0.01 * 7.011044025421143
Epoch 460, val loss: 0.8689914345741272
Epoch 470, training loss: 0.10164240747690201 = 0.0315064862370491 + 0.01 * 7.01359224319458
Epoch 470, val loss: 0.8824228644371033
Epoch 480, training loss: 0.09893852472305298 = 0.028916796669363976 + 0.01 * 7.002172470092773
Epoch 480, val loss: 0.8955302238464355
Epoch 490, training loss: 0.09667147696018219 = 0.026619015261530876 + 0.01 * 7.005246162414551
Epoch 490, val loss: 0.9082950353622437
Epoch 500, training loss: 0.09454227983951569 = 0.0245742816478014 + 0.01 * 6.996800422668457
Epoch 500, val loss: 0.9207489490509033
Epoch 510, training loss: 0.09263725578784943 = 0.022748790681362152 + 0.01 * 6.988846778869629
Epoch 510, val loss: 0.9328390955924988
Epoch 520, training loss: 0.09097827225923538 = 0.021112995222210884 + 0.01 * 6.986528396606445
Epoch 520, val loss: 0.9445978403091431
Epoch 530, training loss: 0.08979077637195587 = 0.019643040373921394 + 0.01 * 7.014773845672607
Epoch 530, val loss: 0.9559961557388306
Epoch 540, training loss: 0.08815395832061768 = 0.018320785835385323 + 0.01 * 6.9833173751831055
Epoch 540, val loss: 0.9671012759208679
Epoch 550, training loss: 0.08678560703992844 = 0.017125211656093597 + 0.01 * 6.966039657592773
Epoch 550, val loss: 0.9778424501419067
Epoch 560, training loss: 0.08616217970848083 = 0.016039522364735603 + 0.01 * 7.012266159057617
Epoch 560, val loss: 0.9882796406745911
Epoch 570, training loss: 0.08460089564323425 = 0.01505555585026741 + 0.01 * 6.95453405380249
Epoch 570, val loss: 0.9984371662139893
Epoch 580, training loss: 0.0836261585354805 = 0.014159061014652252 + 0.01 * 6.946710109710693
Epoch 580, val loss: 1.0082682371139526
Epoch 590, training loss: 0.08332666754722595 = 0.01333939004689455 + 0.01 * 6.998727798461914
Epoch 590, val loss: 1.017806053161621
Epoch 600, training loss: 0.08210040628910065 = 0.012590926140546799 + 0.01 * 6.950948715209961
Epoch 600, val loss: 1.0271598100662231
Epoch 610, training loss: 0.08123943954706192 = 0.011904701590538025 + 0.01 * 6.933474063873291
Epoch 610, val loss: 1.0361686944961548
Epoch 620, training loss: 0.08068773150444031 = 0.011273473501205444 + 0.01 * 6.941425800323486
Epoch 620, val loss: 1.0449388027191162
Epoch 630, training loss: 0.07986531406641006 = 0.01069264393299818 + 0.01 * 6.917267799377441
Epoch 630, val loss: 1.0534862279891968
Epoch 640, training loss: 0.07937997579574585 = 0.010156724601984024 + 0.01 * 6.922325134277344
Epoch 640, val loss: 1.0617663860321045
Epoch 650, training loss: 0.07885116338729858 = 0.009662912227213383 + 0.01 * 6.918825626373291
Epoch 650, val loss: 1.069814920425415
Epoch 660, training loss: 0.07831583172082901 = 0.009205780923366547 + 0.01 * 6.91100549697876
Epoch 660, val loss: 1.0776089429855347
Epoch 670, training loss: 0.0778314471244812 = 0.008782562799751759 + 0.01 * 6.904888153076172
Epoch 670, val loss: 1.085261583328247
Epoch 680, training loss: 0.07731343060731888 = 0.00838936772197485 + 0.01 * 6.892406463623047
Epoch 680, val loss: 1.0926445722579956
Epoch 690, training loss: 0.07707341015338898 = 0.008023720234632492 + 0.01 * 6.904968738555908
Epoch 690, val loss: 1.0998762845993042
Epoch 700, training loss: 0.07642491161823273 = 0.007683182135224342 + 0.01 * 6.874173164367676
Epoch 700, val loss: 1.106939435005188
Epoch 710, training loss: 0.07619795203208923 = 0.007365548051893711 + 0.01 * 6.883240699768066
Epoch 710, val loss: 1.1137654781341553
Epoch 720, training loss: 0.07582619786262512 = 0.007069467566907406 + 0.01 * 6.875673294067383
Epoch 720, val loss: 1.1204478740692139
Epoch 730, training loss: 0.07570023834705353 = 0.006792402360588312 + 0.01 * 6.890783309936523
Epoch 730, val loss: 1.1269065141677856
Epoch 740, training loss: 0.07512415200471878 = 0.006533414125442505 + 0.01 * 6.859074115753174
Epoch 740, val loss: 1.1332640647888184
Epoch 750, training loss: 0.07509124279022217 = 0.0062900446355342865 + 0.01 * 6.880119800567627
Epoch 750, val loss: 1.1393826007843018
Epoch 760, training loss: 0.07457445561885834 = 0.006061527878046036 + 0.01 * 6.851292610168457
Epoch 760, val loss: 1.1454545259475708
Epoch 770, training loss: 0.07465581595897675 = 0.005846395622938871 + 0.01 * 6.880942344665527
Epoch 770, val loss: 1.1512726545333862
Epoch 780, training loss: 0.07420195639133453 = 0.005644351243972778 + 0.01 * 6.85576057434082
Epoch 780, val loss: 1.1569623947143555
Epoch 790, training loss: 0.07370773702859879 = 0.0054538678377866745 + 0.01 * 6.825387001037598
Epoch 790, val loss: 1.1625179052352905
Epoch 800, training loss: 0.07356113940477371 = 0.005273875780403614 + 0.01 * 6.828726291656494
Epoch 800, val loss: 1.1678885221481323
Epoch 810, training loss: 0.07331371307373047 = 0.005104136653244495 + 0.01 * 6.820957660675049
Epoch 810, val loss: 1.1732157468795776
Epoch 820, training loss: 0.07312551885843277 = 0.004943294916301966 + 0.01 * 6.8182220458984375
Epoch 820, val loss: 1.178331971168518
Epoch 830, training loss: 0.0730949193239212 = 0.00479107815772295 + 0.01 * 6.830383777618408
Epoch 830, val loss: 1.1833727359771729
Epoch 840, training loss: 0.07283825427293777 = 0.00464660907164216 + 0.01 * 6.819164752960205
Epoch 840, val loss: 1.1883273124694824
Epoch 850, training loss: 0.07263348996639252 = 0.004509095102548599 + 0.01 * 6.812439441680908
Epoch 850, val loss: 1.1931042671203613
Epoch 860, training loss: 0.0724107176065445 = 0.004378688987344503 + 0.01 * 6.803203105926514
Epoch 860, val loss: 1.1978378295898438
Epoch 870, training loss: 0.07244375348091125 = 0.00425468385219574 + 0.01 * 6.818906784057617
Epoch 870, val loss: 1.2023849487304688
Epoch 880, training loss: 0.07221648097038269 = 0.004136620555073023 + 0.01 * 6.807986259460449
Epoch 880, val loss: 1.2068760395050049
Epoch 890, training loss: 0.07212430983781815 = 0.004023773595690727 + 0.01 * 6.81005334854126
Epoch 890, val loss: 1.2112605571746826
Epoch 900, training loss: 0.07184509187936783 = 0.003917028196156025 + 0.01 * 6.792806148529053
Epoch 900, val loss: 1.215499758720398
Epoch 910, training loss: 0.07165661454200745 = 0.003814907046034932 + 0.01 * 6.784171104431152
Epoch 910, val loss: 1.2196547985076904
Epoch 920, training loss: 0.07174280285835266 = 0.003717356827110052 + 0.01 * 6.802544593811035
Epoch 920, val loss: 1.2237223386764526
Epoch 930, training loss: 0.0714668408036232 = 0.0036241731140762568 + 0.01 * 6.784267425537109
Epoch 930, val loss: 1.2276891469955444
Epoch 940, training loss: 0.07137230038642883 = 0.0035354590509086847 + 0.01 * 6.783684730529785
Epoch 940, val loss: 1.2315610647201538
Epoch 950, training loss: 0.07121788710355759 = 0.0034503061324357986 + 0.01 * 6.776758193969727
Epoch 950, val loss: 1.2353581190109253
Epoch 960, training loss: 0.07111921906471252 = 0.00336868385784328 + 0.01 * 6.77505350112915
Epoch 960, val loss: 1.2390717267990112
Epoch 970, training loss: 0.07092674821615219 = 0.00329097593203187 + 0.01 * 6.763577938079834
Epoch 970, val loss: 1.242705225944519
Epoch 980, training loss: 0.07104283571243286 = 0.0032159697730094194 + 0.01 * 6.782686710357666
Epoch 980, val loss: 1.2462341785430908
Epoch 990, training loss: 0.07082965224981308 = 0.0031446812208741903 + 0.01 * 6.768496990203857
Epoch 990, val loss: 1.2497133016586304
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 2.024296998977661 = 1.938328504562378 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.940079927444458
Epoch 10, training loss: 2.014039993286133 = 1.928072214126587 + 0.01 * 8.59678840637207
Epoch 10, val loss: 1.9296345710754395
Epoch 20, training loss: 2.0011165142059326 = 1.915150761604309 + 0.01 * 8.596576690673828
Epoch 20, val loss: 1.9158438444137573
Epoch 30, training loss: 1.9828453063964844 = 1.896886944770813 + 0.01 * 8.595830917358398
Epoch 30, val loss: 1.8959254026412964
Epoch 40, training loss: 1.9564522504806519 = 1.8705360889434814 + 0.01 * 8.5916166305542
Epoch 40, val loss: 1.8671375513076782
Epoch 50, training loss: 1.920931339263916 = 1.8352779150009155 + 0.01 * 8.565346717834473
Epoch 50, val loss: 1.8302240371704102
Epoch 60, training loss: 1.8822380304336548 = 1.797553539276123 + 0.01 * 8.4684476852417
Epoch 60, val loss: 1.7954788208007812
Epoch 70, training loss: 1.8439478874206543 = 1.7614312171936035 + 0.01 * 8.251669883728027
Epoch 70, val loss: 1.7669471502304077
Epoch 80, training loss: 1.7938426733016968 = 1.7136774063110352 + 0.01 * 8.016523361206055
Epoch 80, val loss: 1.7268469333648682
Epoch 90, training loss: 1.7246919870376587 = 1.647314429283142 + 0.01 * 7.737759590148926
Epoch 90, val loss: 1.666367530822754
Epoch 100, training loss: 1.6351069211959839 = 1.558800220489502 + 0.01 * 7.63067102432251
Epoch 100, val loss: 1.5868744850158691
Epoch 110, training loss: 1.5289831161499023 = 1.4530575275421143 + 0.01 * 7.592565059661865
Epoch 110, val loss: 1.4963933229446411
Epoch 120, training loss: 1.4152066707611084 = 1.3394513130187988 + 0.01 * 7.575532913208008
Epoch 120, val loss: 1.401167631149292
Epoch 130, training loss: 1.3008754253387451 = 1.225197672843933 + 0.01 * 7.567773342132568
Epoch 130, val loss: 1.3081037998199463
Epoch 140, training loss: 1.1902377605438232 = 1.114634394645691 + 0.01 * 7.560337066650391
Epoch 140, val loss: 1.217461347579956
Epoch 150, training loss: 1.0865511894226074 = 1.0110613107681274 + 0.01 * 7.54898738861084
Epoch 150, val loss: 1.1337733268737793
Epoch 160, training loss: 0.9916297793388367 = 0.916329562664032 + 0.01 * 7.530020713806152
Epoch 160, val loss: 1.0594209432601929
Epoch 170, training loss: 0.9064992070198059 = 0.8315122127532959 + 0.01 * 7.498698711395264
Epoch 170, val loss: 0.9955541491508484
Epoch 180, training loss: 0.8315073251724243 = 0.7569860219955444 + 0.01 * 7.452127933502197
Epoch 180, val loss: 0.9423710107803345
Epoch 190, training loss: 0.7659470438957214 = 0.6919506192207336 + 0.01 * 7.399643898010254
Epoch 190, val loss: 0.8991625308990479
Epoch 200, training loss: 0.7082715630531311 = 0.6347092390060425 + 0.01 * 7.356230735778809
Epoch 200, val loss: 0.8647112846374512
Epoch 210, training loss: 0.6564961671829224 = 0.5833166837692261 + 0.01 * 7.317946910858154
Epoch 210, val loss: 0.8375399708747864
Epoch 220, training loss: 0.6089115142822266 = 0.5360612869262695 + 0.01 * 7.2850260734558105
Epoch 220, val loss: 0.8163080811500549
Epoch 230, training loss: 0.5643010139465332 = 0.49172359704971313 + 0.01 * 7.257744789123535
Epoch 230, val loss: 0.7999663352966309
Epoch 240, training loss: 0.5218200087547302 = 0.44958364963531494 + 0.01 * 7.223634243011475
Epoch 240, val loss: 0.7875509262084961
Epoch 250, training loss: 0.48141422867774963 = 0.40938425064086914 + 0.01 * 7.20299768447876
Epoch 250, val loss: 0.7785341739654541
Epoch 260, training loss: 0.4430881142616272 = 0.3712092339992523 + 0.01 * 7.187887668609619
Epoch 260, val loss: 0.7725739479064941
Epoch 270, training loss: 0.4069693982601166 = 0.33528557419776917 + 0.01 * 7.1683831214904785
Epoch 270, val loss: 0.7695887684822083
Epoch 280, training loss: 0.37330782413482666 = 0.30173438787460327 + 0.01 * 7.157345294952393
Epoch 280, val loss: 0.7695644497871399
Epoch 290, training loss: 0.3421556353569031 = 0.2706106901168823 + 0.01 * 7.154494285583496
Epoch 290, val loss: 0.772490382194519
Epoch 300, training loss: 0.3133208155632019 = 0.24194955825805664 + 0.01 * 7.137126922607422
Epoch 300, val loss: 0.7781069278717041
Epoch 310, training loss: 0.2870999574661255 = 0.21577908098697662 + 0.01 * 7.132089614868164
Epoch 310, val loss: 0.7860795855522156
Epoch 320, training loss: 0.26337116956710815 = 0.1921224445104599 + 0.01 * 7.12487268447876
Epoch 320, val loss: 0.7964372038841248
Epoch 330, training loss: 0.24208413064479828 = 0.17094512283802032 + 0.01 * 7.113901138305664
Epoch 330, val loss: 0.8090783953666687
Epoch 340, training loss: 0.22323393821716309 = 0.1521347016096115 + 0.01 * 7.109922885894775
Epoch 340, val loss: 0.8238245248794556
Epoch 350, training loss: 0.20667189359664917 = 0.13550496101379395 + 0.01 * 7.11669397354126
Epoch 350, val loss: 0.8404505252838135
Epoch 360, training loss: 0.191787451505661 = 0.12085270881652832 + 0.01 * 7.093474864959717
Epoch 360, val loss: 0.8586838245391846
Epoch 370, training loss: 0.17885738611221313 = 0.10794839262962341 + 0.01 * 7.090899467468262
Epoch 370, val loss: 0.878207802772522
Epoch 380, training loss: 0.16740180552005768 = 0.09658345580101013 + 0.01 * 7.08183479309082
Epoch 380, val loss: 0.8986634016036987
Epoch 390, training loss: 0.1574816107749939 = 0.08656544983386993 + 0.01 * 7.091615200042725
Epoch 390, val loss: 0.9197428822517395
Epoch 400, training loss: 0.1484583616256714 = 0.07773229479789734 + 0.01 * 7.072606086730957
Epoch 400, val loss: 0.9411847591400146
Epoch 410, training loss: 0.1405200958251953 = 0.06993486732244492 + 0.01 * 7.058522701263428
Epoch 410, val loss: 0.9628089070320129
Epoch 420, training loss: 0.13357016444206238 = 0.06304197758436203 + 0.01 * 7.05281925201416
Epoch 420, val loss: 0.9844809174537659
Epoch 430, training loss: 0.12744608521461487 = 0.05695619061589241 + 0.01 * 7.048990249633789
Epoch 430, val loss: 1.0058979988098145
Epoch 440, training loss: 0.12199331820011139 = 0.05157969892024994 + 0.01 * 7.0413618087768555
Epoch 440, val loss: 1.0268522500991821
Epoch 450, training loss: 0.1171313226222992 = 0.04681752994656563 + 0.01 * 7.031379699707031
Epoch 450, val loss: 1.0474566221237183
Epoch 460, training loss: 0.11297794431447983 = 0.04258957505226135 + 0.01 * 7.038837432861328
Epoch 460, val loss: 1.0676606893539429
Epoch 470, training loss: 0.10904505848884583 = 0.03884168341755867 + 0.01 * 7.0203375816345215
Epoch 470, val loss: 1.0873390436172485
Epoch 480, training loss: 0.10565304011106491 = 0.03551122546195984 + 0.01 * 7.014181613922119
Epoch 480, val loss: 1.106501579284668
Epoch 490, training loss: 0.10258717089891434 = 0.032545074820518494 + 0.01 * 7.004209518432617
Epoch 490, val loss: 1.1251195669174194
Epoch 500, training loss: 0.0998682901263237 = 0.02989884279668331 + 0.01 * 6.996944904327393
Epoch 500, val loss: 1.1432011127471924
Epoch 510, training loss: 0.097444087266922 = 0.02753603644669056 + 0.01 * 6.990805625915527
Epoch 510, val loss: 1.1606922149658203
Epoch 520, training loss: 0.09529941529035568 = 0.0254208967089653 + 0.01 * 6.987852096557617
Epoch 520, val loss: 1.177578091621399
Epoch 530, training loss: 0.09334424138069153 = 0.02352464757859707 + 0.01 * 6.981959342956543
Epoch 530, val loss: 1.1938750743865967
Epoch 540, training loss: 0.09178903698921204 = 0.021821828559041023 + 0.01 * 6.996721267700195
Epoch 540, val loss: 1.2096530199050903
Epoch 550, training loss: 0.09010059386491776 = 0.020289579406380653 + 0.01 * 6.981101989746094
Epoch 550, val loss: 1.224944829940796
Epoch 560, training loss: 0.08861830830574036 = 0.01890537701547146 + 0.01 * 6.971292972564697
Epoch 560, val loss: 1.2398161888122559
Epoch 570, training loss: 0.08722658455371857 = 0.017648428678512573 + 0.01 * 6.957815647125244
Epoch 570, val loss: 1.2543275356292725
Epoch 580, training loss: 0.0860954001545906 = 0.01650247909128666 + 0.01 * 6.959292411804199
Epoch 580, val loss: 1.2685366868972778
Epoch 590, training loss: 0.08502152562141418 = 0.015456809662282467 + 0.01 * 6.9564714431762695
Epoch 590, val loss: 1.2824100255966187
Epoch 600, training loss: 0.08392926305532455 = 0.014500170946121216 + 0.01 * 6.9429097175598145
Epoch 600, val loss: 1.2960307598114014
Epoch 610, training loss: 0.08302123099565506 = 0.013623683713376522 + 0.01 * 6.939754486083984
Epoch 610, val loss: 1.309370756149292
Epoch 620, training loss: 0.08217636495828629 = 0.012819957919418812 + 0.01 * 6.935640335083008
Epoch 620, val loss: 1.3223977088928223
Epoch 630, training loss: 0.08139002323150635 = 0.01208177674561739 + 0.01 * 6.9308247566223145
Epoch 630, val loss: 1.33514404296875
Epoch 640, training loss: 0.08078717440366745 = 0.011402747593820095 + 0.01 * 6.938442707061768
Epoch 640, val loss: 1.3476011753082275
Epoch 650, training loss: 0.08008146286010742 = 0.010777857154607773 + 0.01 * 6.930360794067383
Epoch 650, val loss: 1.359771490097046
Epoch 660, training loss: 0.07940132170915604 = 0.010202874429523945 + 0.01 * 6.919844627380371
Epoch 660, val loss: 1.3716316223144531
Epoch 670, training loss: 0.07880283147096634 = 0.00967216957360506 + 0.01 * 6.913066387176514
Epoch 670, val loss: 1.383184552192688
Epoch 680, training loss: 0.07838953286409378 = 0.009181204251945019 + 0.01 * 6.920833587646484
Epoch 680, val loss: 1.3944767713546753
Epoch 690, training loss: 0.07790545374155045 = 0.008725011721253395 + 0.01 * 6.918044567108154
Epoch 690, val loss: 1.40556001663208
Epoch 700, training loss: 0.07736106961965561 = 0.008299171924591064 + 0.01 * 6.906189918518066
Epoch 700, val loss: 1.4165197610855103
Epoch 710, training loss: 0.07688663899898529 = 0.007901240140199661 + 0.01 * 6.8985395431518555
Epoch 710, val loss: 1.4273014068603516
Epoch 720, training loss: 0.07658179849386215 = 0.0075299921445548534 + 0.01 * 6.905180931091309
Epoch 720, val loss: 1.4378736019134521
Epoch 730, training loss: 0.07628900557756424 = 0.007182953413575888 + 0.01 * 6.910605430603027
Epoch 730, val loss: 1.4482542276382446
Epoch 740, training loss: 0.07584893703460693 = 0.006857705768197775 + 0.01 * 6.899123191833496
Epoch 740, val loss: 1.4585000276565552
Epoch 750, training loss: 0.07543263584375381 = 0.006552999839186668 + 0.01 * 6.88796329498291
Epoch 750, val loss: 1.468643307685852
Epoch 760, training loss: 0.07500995695590973 = 0.006266406737267971 + 0.01 * 6.874354839324951
Epoch 760, val loss: 1.4786418676376343
Epoch 770, training loss: 0.07496517896652222 = 0.005996488034725189 + 0.01 * 6.89686918258667
Epoch 770, val loss: 1.488515019416809
Epoch 780, training loss: 0.07462826371192932 = 0.005743416491895914 + 0.01 * 6.888485431671143
Epoch 780, val loss: 1.498202919960022
Epoch 790, training loss: 0.0742887482047081 = 0.0055053336545825005 + 0.01 * 6.878342151641846
Epoch 790, val loss: 1.5077495574951172
Epoch 800, training loss: 0.07399051636457443 = 0.005282489582896233 + 0.01 * 6.870802402496338
Epoch 800, val loss: 1.5170005559921265
Epoch 810, training loss: 0.07367713749408722 = 0.005074019078165293 + 0.01 * 6.860311985015869
Epoch 810, val loss: 1.5260376930236816
Epoch 820, training loss: 0.07342524826526642 = 0.0048780376091599464 + 0.01 * 6.854721546173096
Epoch 820, val loss: 1.5349239110946655
Epoch 830, training loss: 0.07328226417303085 = 0.004693126305937767 + 0.01 * 6.858913421630859
Epoch 830, val loss: 1.543681263923645
Epoch 840, training loss: 0.07310142368078232 = 0.004518736619502306 + 0.01 * 6.8582682609558105
Epoch 840, val loss: 1.5522841215133667
Epoch 850, training loss: 0.07295845448970795 = 0.004355175886303186 + 0.01 * 6.860328197479248
Epoch 850, val loss: 1.5605990886688232
Epoch 860, training loss: 0.07274943590164185 = 0.004201511386781931 + 0.01 * 6.854792594909668
Epoch 860, val loss: 1.5686309337615967
Epoch 870, training loss: 0.07248096168041229 = 0.004056772217154503 + 0.01 * 6.842418670654297
Epoch 870, val loss: 1.576564908027649
Epoch 880, training loss: 0.07230770587921143 = 0.003919819835573435 + 0.01 * 6.838788986206055
Epoch 880, val loss: 1.5843786001205444
Epoch 890, training loss: 0.07214075326919556 = 0.003790423506870866 + 0.01 * 6.835033416748047
Epoch 890, val loss: 1.591955542564392
Epoch 900, training loss: 0.07202110439538956 = 0.003668469376862049 + 0.01 * 6.835263729095459
Epoch 900, val loss: 1.5993256568908691
Epoch 910, training loss: 0.07181733101606369 = 0.0035530519671738148 + 0.01 * 6.826427936553955
Epoch 910, val loss: 1.6065654754638672
Epoch 920, training loss: 0.07171440869569778 = 0.0034439810551702976 + 0.01 * 6.827043056488037
Epoch 920, val loss: 1.613592267036438
Epoch 930, training loss: 0.07161636650562286 = 0.0033409493044018745 + 0.01 * 6.827541351318359
Epoch 930, val loss: 1.620444655418396
Epoch 940, training loss: 0.07142258435487747 = 0.0032433802261948586 + 0.01 * 6.817920684814453
Epoch 940, val loss: 1.6271170377731323
Epoch 950, training loss: 0.07140807062387466 = 0.0031507492531090975 + 0.01 * 6.825732707977295
Epoch 950, val loss: 1.6336133480072021
Epoch 960, training loss: 0.07133770734071732 = 0.0030629588291049004 + 0.01 * 6.827475547790527
Epoch 960, val loss: 1.6400102376937866
Epoch 970, training loss: 0.07124650478363037 = 0.0029795619193464518 + 0.01 * 6.826694488525391
Epoch 970, val loss: 1.6462020874023438
Epoch 980, training loss: 0.07104279845952988 = 0.0029001953080296516 + 0.01 * 6.814260482788086
Epoch 980, val loss: 1.6523127555847168
Epoch 990, training loss: 0.07096589356660843 = 0.002824644558131695 + 0.01 * 6.8141255378723145
Epoch 990, val loss: 1.6582192182540894
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8334211913547708
=== training gcn model ===
Epoch 0, training loss: 2.06269907951355 = 1.9767308235168457 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9679951667785645
Epoch 10, training loss: 2.0516855716705322 = 1.9657179117202759 + 0.01 * 8.596772193908691
Epoch 10, val loss: 1.9571702480316162
Epoch 20, training loss: 2.0385537147521973 = 1.9525879621505737 + 0.01 * 8.596578598022461
Epoch 20, val loss: 1.9437346458435059
Epoch 30, training loss: 2.020615816116333 = 1.934656023979187 + 0.01 * 8.595968246459961
Epoch 30, val loss: 1.9247983694076538
Epoch 40, training loss: 1.994101643562317 = 1.908172845840454 + 0.01 * 8.592877388000488
Epoch 40, val loss: 1.8965940475463867
Epoch 50, training loss: 1.9553521871566772 = 1.8696342706680298 + 0.01 * 8.571789741516113
Epoch 50, val loss: 1.8564937114715576
Epoch 60, training loss: 1.906193733215332 = 1.821512222290039 + 0.01 * 8.468153953552246
Epoch 60, val loss: 1.8108652830123901
Epoch 70, training loss: 1.8602195978164673 = 1.7778946161270142 + 0.01 * 8.232502937316895
Epoch 70, val loss: 1.7764980792999268
Epoch 80, training loss: 1.814628005027771 = 1.7348214387893677 + 0.01 * 7.98065185546875
Epoch 80, val loss: 1.7428239583969116
Epoch 90, training loss: 1.7539905309677124 = 1.677994728088379 + 0.01 * 7.599581241607666
Epoch 90, val loss: 1.6923596858978271
Epoch 100, training loss: 1.6755142211914062 = 1.6015808582305908 + 0.01 * 7.3933424949646
Epoch 100, val loss: 1.6229740381240845
Epoch 110, training loss: 1.5786073207855225 = 1.5053359270095825 + 0.01 * 7.327141761779785
Epoch 110, val loss: 1.5380914211273193
Epoch 120, training loss: 1.4719113111495972 = 1.3989074230194092 + 0.01 * 7.300387382507324
Epoch 120, val loss: 1.4477609395980835
Epoch 130, training loss: 1.3651888370513916 = 1.2924041748046875 + 0.01 * 7.278465270996094
Epoch 130, val loss: 1.3626712560653687
Epoch 140, training loss: 1.2612711191177368 = 1.1886894702911377 + 0.01 * 7.258167266845703
Epoch 140, val loss: 1.2824878692626953
Epoch 150, training loss: 1.1610772609710693 = 1.0886977910995483 + 0.01 * 7.237943649291992
Epoch 150, val loss: 1.2045695781707764
Epoch 160, training loss: 1.0657856464385986 = 0.9936425089836121 + 0.01 * 7.214309215545654
Epoch 160, val loss: 1.130465030670166
Epoch 170, training loss: 0.9771560430526733 = 0.9053296446800232 + 0.01 * 7.182638645172119
Epoch 170, val loss: 1.0617047548294067
Epoch 180, training loss: 0.8970538973808289 = 0.8255214691162109 + 0.01 * 7.153244495391846
Epoch 180, val loss: 0.9996458888053894
Epoch 190, training loss: 0.8264944553375244 = 0.7552992701530457 + 0.01 * 7.1195197105407715
Epoch 190, val loss: 0.9459591507911682
Epoch 200, training loss: 0.7651301026344299 = 0.6941372752189636 + 0.01 * 7.099284648895264
Epoch 200, val loss: 0.9009847044944763
Epoch 210, training loss: 0.7107856273651123 = 0.639954149723053 + 0.01 * 7.083148956298828
Epoch 210, val loss: 0.8637685775756836
Epoch 220, training loss: 0.6609989404678345 = 0.590265154838562 + 0.01 * 7.07337760925293
Epoch 220, val loss: 0.8325365781784058
Epoch 230, training loss: 0.6138712167739868 = 0.5432478785514832 + 0.01 * 7.062332630157471
Epoch 230, val loss: 0.8058982491493225
Epoch 240, training loss: 0.568571925163269 = 0.4980188310146332 + 0.01 * 7.055309295654297
Epoch 240, val loss: 0.7827199101448059
Epoch 250, training loss: 0.5248258113861084 = 0.4543454945087433 + 0.01 * 7.04802942276001
Epoch 250, val loss: 0.762471616268158
Epoch 260, training loss: 0.4825235903263092 = 0.4121018052101135 + 0.01 * 7.042178153991699
Epoch 260, val loss: 0.7449575066566467
Epoch 270, training loss: 0.44145750999450684 = 0.3710607588291168 + 0.01 * 7.039676666259766
Epoch 270, val loss: 0.7298051118850708
Epoch 280, training loss: 0.40137404203414917 = 0.3309985101222992 + 0.01 * 7.0375542640686035
Epoch 280, val loss: 0.7166285514831543
Epoch 290, training loss: 0.3621630072593689 = 0.2918581962585449 + 0.01 * 7.030482292175293
Epoch 290, val loss: 0.7048728466033936
Epoch 300, training loss: 0.32431915402412415 = 0.25406181812286377 + 0.01 * 7.0257344245910645
Epoch 300, val loss: 0.6948203444480896
Epoch 310, training loss: 0.2889212369918823 = 0.21863648295402527 + 0.01 * 7.028475761413574
Epoch 310, val loss: 0.6872299313545227
Epoch 320, training loss: 0.25702783465385437 = 0.18678009510040283 + 0.01 * 7.024774551391602
Epoch 320, val loss: 0.6829908490180969
Epoch 330, training loss: 0.22942206263542175 = 0.1591978818178177 + 0.01 * 7.022418975830078
Epoch 330, val loss: 0.6826727390289307
Epoch 340, training loss: 0.20609965920448303 = 0.13590355217456818 + 0.01 * 7.019611358642578
Epoch 340, val loss: 0.6861041188240051
Epoch 350, training loss: 0.1866290271282196 = 0.11647206544876099 + 0.01 * 7.015697002410889
Epoch 350, val loss: 0.6928051710128784
Epoch 360, training loss: 0.1704220473766327 = 0.10032186657190323 + 0.01 * 7.010019302368164
Epoch 360, val loss: 0.7021300196647644
Epoch 370, training loss: 0.15694443881511688 = 0.08688695728778839 + 0.01 * 7.005748271942139
Epoch 370, val loss: 0.7134628295898438
Epoch 380, training loss: 0.1457122564315796 = 0.07568009942770004 + 0.01 * 7.003215312957764
Epoch 380, val loss: 0.7261489033699036
Epoch 390, training loss: 0.13634371757507324 = 0.06629716604948044 + 0.01 * 7.004654884338379
Epoch 390, val loss: 0.7397327423095703
Epoch 400, training loss: 0.1284434199333191 = 0.05841316282749176 + 0.01 * 7.003026008605957
Epoch 400, val loss: 0.7538055181503296
Epoch 410, training loss: 0.12172699719667435 = 0.05176110565662384 + 0.01 * 6.996589183807373
Epoch 410, val loss: 0.7680634260177612
Epoch 420, training loss: 0.1160149797797203 = 0.04612037539482117 + 0.01 * 6.9894609451293945
Epoch 420, val loss: 0.7823410630226135
Epoch 430, training loss: 0.11137787252664566 = 0.04131358861923218 + 0.01 * 7.006428241729736
Epoch 430, val loss: 0.7964056730270386
Epoch 440, training loss: 0.10703760385513306 = 0.03720160946249962 + 0.01 * 6.983599662780762
Epoch 440, val loss: 0.8101590871810913
Epoch 450, training loss: 0.10344052314758301 = 0.033661212772130966 + 0.01 * 6.977931499481201
Epoch 450, val loss: 0.8236190676689148
Epoch 460, training loss: 0.10036228597164154 = 0.03059517964720726 + 0.01 * 6.976710796356201
Epoch 460, val loss: 0.836757481098175
Epoch 470, training loss: 0.09767047315835953 = 0.027927076444029808 + 0.01 * 6.974339485168457
Epoch 470, val loss: 0.8494990468025208
Epoch 480, training loss: 0.09526600688695908 = 0.025593996047973633 + 0.01 * 6.967201232910156
Epoch 480, val loss: 0.8618868589401245
Epoch 490, training loss: 0.09335129708051682 = 0.02354329824447632 + 0.01 * 6.980800151824951
Epoch 490, val loss: 0.8738849759101868
Epoch 500, training loss: 0.09139034897089005 = 0.021734269335865974 + 0.01 * 6.9656081199646
Epoch 500, val loss: 0.8854627013206482
Epoch 510, training loss: 0.08969259262084961 = 0.02012973092496395 + 0.01 * 6.9562859535217285
Epoch 510, val loss: 0.8966730833053589
Epoch 520, training loss: 0.0882406234741211 = 0.01870064251124859 + 0.01 * 6.95399808883667
Epoch 520, val loss: 0.9075314998626709
Epoch 530, training loss: 0.08686599135398865 = 0.01742328330874443 + 0.01 * 6.944271564483643
Epoch 530, val loss: 0.9180441498756409
Epoch 540, training loss: 0.08568690717220306 = 0.01627621240913868 + 0.01 * 6.941070079803467
Epoch 540, val loss: 0.9281936287879944
Epoch 550, training loss: 0.08511783182621002 = 0.015243034809827805 + 0.01 * 6.987479209899902
Epoch 550, val loss: 0.9380534887313843
Epoch 560, training loss: 0.08379125595092773 = 0.01431167684495449 + 0.01 * 6.947957992553711
Epoch 560, val loss: 0.9475235342979431
Epoch 570, training loss: 0.08274911344051361 = 0.013468093238770962 + 0.01 * 6.928102016448975
Epoch 570, val loss: 0.956703782081604
Epoch 580, training loss: 0.08203355222940445 = 0.01269993931055069 + 0.01 * 6.933361530303955
Epoch 580, val loss: 0.965606153011322
Epoch 590, training loss: 0.08125006407499313 = 0.011999597772955894 + 0.01 * 6.925046920776367
Epoch 590, val loss: 0.9742571711540222
Epoch 600, training loss: 0.08054238557815552 = 0.011358777992427349 + 0.01 * 6.918361186981201
Epoch 600, val loss: 0.982636034488678
Epoch 610, training loss: 0.08007634431123734 = 0.010770192369818687 + 0.01 * 6.930614948272705
Epoch 610, val loss: 0.9908053874969482
Epoch 620, training loss: 0.07928851991891861 = 0.010230360552668571 + 0.01 * 6.905815601348877
Epoch 620, val loss: 0.9986441135406494
Epoch 630, training loss: 0.07880301773548126 = 0.009733621031045914 + 0.01 * 6.90693998336792
Epoch 630, val loss: 1.0062520503997803
Epoch 640, training loss: 0.07829684019088745 = 0.009275585412979126 + 0.01 * 6.902125835418701
Epoch 640, val loss: 1.013678789138794
Epoch 650, training loss: 0.07781855016946793 = 0.008851944468915462 + 0.01 * 6.896660327911377
Epoch 650, val loss: 1.0208747386932373
Epoch 660, training loss: 0.07743121683597565 = 0.00845953170210123 + 0.01 * 6.8971686363220215
Epoch 660, val loss: 1.0278490781784058
Epoch 670, training loss: 0.07697144895792007 = 0.008094809018075466 + 0.01 * 6.887664318084717
Epoch 670, val loss: 1.0346256494522095
Epoch 680, training loss: 0.0772857666015625 = 0.0077555677853524685 + 0.01 * 6.953020095825195
Epoch 680, val loss: 1.0411914587020874
Epoch 690, training loss: 0.07631020247936249 = 0.007440407294780016 + 0.01 * 6.886979579925537
Epoch 690, val loss: 1.0475910902023315
Epoch 700, training loss: 0.07592764496803284 = 0.007146019022911787 + 0.01 * 6.8781633377075195
Epoch 700, val loss: 1.0537887811660767
Epoch 710, training loss: 0.07560048252344131 = 0.0068709971383214 + 0.01 * 6.87294864654541
Epoch 710, val loss: 1.0598446130752563
Epoch 720, training loss: 0.0753207802772522 = 0.006613244768232107 + 0.01 * 6.870753288269043
Epoch 720, val loss: 1.0657833814620972
Epoch 730, training loss: 0.0750952884554863 = 0.006371444556862116 + 0.01 * 6.872384071350098
Epoch 730, val loss: 1.0715086460113525
Epoch 740, training loss: 0.07468807697296143 = 0.006145248655229807 + 0.01 * 6.854282855987549
Epoch 740, val loss: 1.0770795345306396
Epoch 750, training loss: 0.07446371763944626 = 0.005932359490543604 + 0.01 * 6.85313606262207
Epoch 750, val loss: 1.0825161933898926
Epoch 760, training loss: 0.07424625754356384 = 0.005731669254601002 + 0.01 * 6.85145902633667
Epoch 760, val loss: 1.0878340005874634
Epoch 770, training loss: 0.07402367144823074 = 0.00554241007193923 + 0.01 * 6.848126411437988
Epoch 770, val loss: 1.0929486751556396
Epoch 780, training loss: 0.07369647175073624 = 0.005364153999835253 + 0.01 * 6.8332319259643555
Epoch 780, val loss: 1.098007082939148
Epoch 790, training loss: 0.07360424101352692 = 0.005195572506636381 + 0.01 * 6.840867042541504
Epoch 790, val loss: 1.1029102802276611
Epoch 800, training loss: 0.07337033748626709 = 0.005036395508795977 + 0.01 * 6.8333940505981445
Epoch 800, val loss: 1.1076347827911377
Epoch 810, training loss: 0.07310677319765091 = 0.0048856064677238464 + 0.01 * 6.822116851806641
Epoch 810, val loss: 1.1123249530792236
Epoch 820, training loss: 0.07290773838758469 = 0.004742470104247332 + 0.01 * 6.816526412963867
Epoch 820, val loss: 1.1169242858886719
Epoch 830, training loss: 0.07303471118211746 = 0.00460667023435235 + 0.01 * 6.842804431915283
Epoch 830, val loss: 1.1213457584381104
Epoch 840, training loss: 0.07265561819076538 = 0.004477350041270256 + 0.01 * 6.817826747894287
Epoch 840, val loss: 1.1256626844406128
Epoch 850, training loss: 0.07248471677303314 = 0.00435444014146924 + 0.01 * 6.813027858734131
Epoch 850, val loss: 1.1299397945404053
Epoch 860, training loss: 0.07232827693223953 = 0.0042373547330498695 + 0.01 * 6.8090925216674805
Epoch 860, val loss: 1.1340876817703247
Epoch 870, training loss: 0.07221102714538574 = 0.0041257478296756744 + 0.01 * 6.808527946472168
Epoch 870, val loss: 1.1381856203079224
Epoch 880, training loss: 0.07210253179073334 = 0.004019465297460556 + 0.01 * 6.808306694030762
Epoch 880, val loss: 1.1421207189559937
Epoch 890, training loss: 0.0719197690486908 = 0.003918057773262262 + 0.01 * 6.8001708984375
Epoch 890, val loss: 1.1460340023040771
Epoch 900, training loss: 0.0717008113861084 = 0.0038212668150663376 + 0.01 * 6.787954330444336
Epoch 900, val loss: 1.1497944593429565
Epoch 910, training loss: 0.0717683732509613 = 0.00372870615683496 + 0.01 * 6.803967475891113
Epoch 910, val loss: 1.1535810232162476
Epoch 920, training loss: 0.0716298520565033 = 0.0036407813895493746 + 0.01 * 6.798907279968262
Epoch 920, val loss: 1.157092571258545
Epoch 930, training loss: 0.07139132171869278 = 0.0035564168356359005 + 0.01 * 6.7834906578063965
Epoch 930, val loss: 1.1606261730194092
Epoch 940, training loss: 0.07131864130496979 = 0.0034754835069179535 + 0.01 * 6.784316539764404
Epoch 940, val loss: 1.1640748977661133
Epoch 950, training loss: 0.07122915238142014 = 0.003398058470338583 + 0.01 * 6.783109188079834
Epoch 950, val loss: 1.1673494577407837
Epoch 960, training loss: 0.0710720494389534 = 0.0033235715236514807 + 0.01 * 6.774847984313965
Epoch 960, val loss: 1.1707385778427124
Epoch 970, training loss: 0.07109730690717697 = 0.003252147464081645 + 0.01 * 6.784515857696533
Epoch 970, val loss: 1.1739845275878906
Epoch 980, training loss: 0.0711161270737648 = 0.003183639608323574 + 0.01 * 6.793248653411865
Epoch 980, val loss: 1.1771008968353271
Epoch 990, training loss: 0.07080091536045074 = 0.0031177441123872995 + 0.01 * 6.768316745758057
Epoch 990, val loss: 1.180140733718872
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8397469688982605
The final CL Acc:0.80741, 0.00907, The final GNN Acc:0.83799, 0.00326
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10486])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0325520038604736 = 1.9465835094451904 + 0.01 * 8.596851348876953
Epoch 0, val loss: 1.9434841871261597
Epoch 10, training loss: 2.022400140762329 = 1.9364320039749146 + 0.01 * 8.596807479858398
Epoch 10, val loss: 1.9331848621368408
Epoch 20, training loss: 2.0094351768493652 = 1.9234684705734253 + 0.01 * 8.596677780151367
Epoch 20, val loss: 1.919412612915039
Epoch 30, training loss: 1.9910590648651123 = 1.9050958156585693 + 0.01 * 8.596328735351562
Epoch 30, val loss: 1.8994884490966797
Epoch 40, training loss: 1.964276909828186 = 1.8783286809921265 + 0.01 * 8.594819068908691
Epoch 40, val loss: 1.8707830905914307
Epoch 50, training loss: 1.9289089441299438 = 1.8430649042129517 + 0.01 * 8.584402084350586
Epoch 50, val loss: 1.83510160446167
Epoch 60, training loss: 1.8935660123825073 = 1.8082555532455444 + 0.01 * 8.531044960021973
Epoch 60, val loss: 1.8048198223114014
Epoch 70, training loss: 1.8632112741470337 = 1.7797954082489014 + 0.01 * 8.341584205627441
Epoch 70, val loss: 1.7835158109664917
Epoch 80, training loss: 1.8249399662017822 = 1.742765188217163 + 0.01 * 8.21748161315918
Epoch 80, val loss: 1.753551721572876
Epoch 90, training loss: 1.770985722541809 = 1.6911922693252563 + 0.01 * 7.979347229003906
Epoch 90, val loss: 1.7102621793746948
Epoch 100, training loss: 1.6970562934875488 = 1.6201380491256714 + 0.01 * 7.691824436187744
Epoch 100, val loss: 1.6507611274719238
Epoch 110, training loss: 1.6099125146865845 = 1.5351271629333496 + 0.01 * 7.4785380363464355
Epoch 110, val loss: 1.5811020135879517
Epoch 120, training loss: 1.5212593078613281 = 1.4468305110931396 + 0.01 * 7.442882537841797
Epoch 120, val loss: 1.5117689371109009
Epoch 130, training loss: 1.43519926071167 = 1.3611788749694824 + 0.01 * 7.402037620544434
Epoch 130, val loss: 1.4490748643875122
Epoch 140, training loss: 1.3505613803863525 = 1.2767715454101562 + 0.01 * 7.378983974456787
Epoch 140, val loss: 1.3902771472930908
Epoch 150, training loss: 1.2654261589050293 = 1.191758632659912 + 0.01 * 7.366755485534668
Epoch 150, val loss: 1.331995964050293
Epoch 160, training loss: 1.1814308166503906 = 1.107877492904663 + 0.01 * 7.3553314208984375
Epoch 160, val loss: 1.2754926681518555
Epoch 170, training loss: 1.1015968322753906 = 1.0281894207000732 + 0.01 * 7.340735912322998
Epoch 170, val loss: 1.2227458953857422
Epoch 180, training loss: 1.0268194675445557 = 0.9536280632019043 + 0.01 * 7.319146633148193
Epoch 180, val loss: 1.1743931770324707
Epoch 190, training loss: 0.9553121328353882 = 0.8823868036270142 + 0.01 * 7.292532920837402
Epoch 190, val loss: 1.1284388303756714
Epoch 200, training loss: 0.8842878937721252 = 0.8116748929023743 + 0.01 * 7.261298656463623
Epoch 200, val loss: 1.0821348428726196
Epoch 210, training loss: 0.8121626973152161 = 0.7397990226745605 + 0.01 * 7.23636531829834
Epoch 210, val loss: 1.0345348119735718
Epoch 220, training loss: 0.7392226457595825 = 0.6671357154846191 + 0.01 * 7.208695888519287
Epoch 220, val loss: 0.9862883687019348
Epoch 230, training loss: 0.66758131980896 = 0.5956650376319885 + 0.01 * 7.191627502441406
Epoch 230, val loss: 0.9403555989265442
Epoch 240, training loss: 0.5993015170097351 = 0.5275809168815613 + 0.01 * 7.172062397003174
Epoch 240, val loss: 0.8997747898101807
Epoch 250, training loss: 0.5360198020935059 = 0.4643362760543823 + 0.01 * 7.1683502197265625
Epoch 250, val loss: 0.8664255738258362
Epoch 260, training loss: 0.47829627990722656 = 0.40667441487312317 + 0.01 * 7.162187099456787
Epoch 260, val loss: 0.8404178619384766
Epoch 270, training loss: 0.4263097643852234 = 0.3548631966114044 + 0.01 * 7.144655704498291
Epoch 270, val loss: 0.8213402628898621
Epoch 280, training loss: 0.3800436854362488 = 0.3087058961391449 + 0.01 * 7.133780479431152
Epoch 280, val loss: 0.8083104491233826
Epoch 290, training loss: 0.33918869495391846 = 0.2678537666797638 + 0.01 * 7.1334919929504395
Epoch 290, val loss: 0.8005526661872864
Epoch 300, training loss: 0.30316802859306335 = 0.23195527493953705 + 0.01 * 7.121274471282959
Epoch 300, val loss: 0.7973824143409729
Epoch 310, training loss: 0.2718426585197449 = 0.20061759650707245 + 0.01 * 7.1225080490112305
Epoch 310, val loss: 0.7982491254806519
Epoch 320, training loss: 0.24456773698329926 = 0.17347508668899536 + 0.01 * 7.109265327453613
Epoch 320, val loss: 0.8025551438331604
Epoch 330, training loss: 0.22118929028511047 = 0.15012595057487488 + 0.01 * 7.106334209442139
Epoch 330, val loss: 0.8098602890968323
Epoch 340, training loss: 0.2011595070362091 = 0.13015154004096985 + 0.01 * 7.100796222686768
Epoch 340, val loss: 0.8196471929550171
Epoch 350, training loss: 0.18400581181049347 = 0.1131168007850647 + 0.01 * 7.088901519775391
Epoch 350, val loss: 0.8315731883049011
Epoch 360, training loss: 0.16946659982204437 = 0.09861817210912704 + 0.01 * 7.084842681884766
Epoch 360, val loss: 0.8452033996582031
Epoch 370, training loss: 0.15716981887817383 = 0.0862983912229538 + 0.01 * 7.087142467498779
Epoch 370, val loss: 0.8601387739181519
Epoch 380, training loss: 0.14658281207084656 = 0.07580810785293579 + 0.01 * 7.0774712562561035
Epoch 380, val loss: 0.8760689496994019
Epoch 390, training loss: 0.1376575529575348 = 0.06685820966959 + 0.01 * 7.079934597015381
Epoch 390, val loss: 0.8927228450775146
Epoch 400, training loss: 0.12992389500141144 = 0.05921938270330429 + 0.01 * 7.070451736450195
Epoch 400, val loss: 0.909736156463623
Epoch 410, training loss: 0.12329480051994324 = 0.05268269404768944 + 0.01 * 7.0612101554870605
Epoch 410, val loss: 0.9268877506256104
Epoch 420, training loss: 0.11773587763309479 = 0.04707450047135353 + 0.01 * 7.066137790679932
Epoch 420, val loss: 0.9439415335655212
Epoch 430, training loss: 0.11282525956630707 = 0.04225387051701546 + 0.01 * 7.0571393966674805
Epoch 430, val loss: 0.9607154130935669
Epoch 440, training loss: 0.10854099690914154 = 0.03809080272912979 + 0.01 * 7.045019626617432
Epoch 440, val loss: 0.9772148728370667
Epoch 450, training loss: 0.10509984195232391 = 0.034482598304748535 + 0.01 * 7.061724662780762
Epoch 450, val loss: 0.9933314323425293
Epoch 460, training loss: 0.1017082929611206 = 0.031351346522569656 + 0.01 * 7.035694122314453
Epoch 460, val loss: 1.0089257955551147
Epoch 470, training loss: 0.09891253709793091 = 0.028616318479180336 + 0.01 * 7.0296220779418945
Epoch 470, val loss: 1.0240445137023926
Epoch 480, training loss: 0.09660413861274719 = 0.026216788217425346 + 0.01 * 7.0387349128723145
Epoch 480, val loss: 1.0386661291122437
Epoch 490, training loss: 0.09428437799215317 = 0.024104679003357887 + 0.01 * 7.017970561981201
Epoch 490, val loss: 1.0528380870819092
Epoch 500, training loss: 0.09250807762145996 = 0.02223704755306244 + 0.01 * 7.027103424072266
Epoch 500, val loss: 1.0665392875671387
Epoch 510, training loss: 0.090694859623909 = 0.02058100327849388 + 0.01 * 7.011385440826416
Epoch 510, val loss: 1.0797266960144043
Epoch 520, training loss: 0.0891229510307312 = 0.019105928018689156 + 0.01 * 7.001702785491943
Epoch 520, val loss: 1.0924595594406128
Epoch 530, training loss: 0.08784493058919907 = 0.01778733916580677 + 0.01 * 7.0057597160339355
Epoch 530, val loss: 1.104699969291687
Epoch 540, training loss: 0.0865275114774704 = 0.01660487800836563 + 0.01 * 6.992263317108154
Epoch 540, val loss: 1.1164826154708862
Epoch 550, training loss: 0.08562569320201874 = 0.015539262443780899 + 0.01 * 7.00864315032959
Epoch 550, val loss: 1.1278865337371826
Epoch 560, training loss: 0.08447344601154327 = 0.014579248614609241 + 0.01 * 6.989419460296631
Epoch 560, val loss: 1.1388044357299805
Epoch 570, training loss: 0.0834699347615242 = 0.013709495775401592 + 0.01 * 6.976044654846191
Epoch 570, val loss: 1.1493947505950928
Epoch 580, training loss: 0.0827007070183754 = 0.012918263673782349 + 0.01 * 6.978244304656982
Epoch 580, val loss: 1.1595802307128906
Epoch 590, training loss: 0.08187166601419449 = 0.012197175063192844 + 0.01 * 6.96744966506958
Epoch 590, val loss: 1.1694247722625732
Epoch 600, training loss: 0.08112849295139313 = 0.011537803336977959 + 0.01 * 6.95906925201416
Epoch 600, val loss: 1.1789298057556152
Epoch 610, training loss: 0.08060576766729355 = 0.010933781042695045 + 0.01 * 6.967199325561523
Epoch 610, val loss: 1.1880908012390137
Epoch 620, training loss: 0.07984556257724762 = 0.010379659943282604 + 0.01 * 6.946590423583984
Epoch 620, val loss: 1.1969255208969116
Epoch 630, training loss: 0.07931987941265106 = 0.009868679568171501 + 0.01 * 6.945119857788086
Epoch 630, val loss: 1.2054922580718994
Epoch 640, training loss: 0.0787193775177002 = 0.00939746294170618 + 0.01 * 6.932191371917725
Epoch 640, val loss: 1.213786005973816
Epoch 650, training loss: 0.07816062867641449 = 0.008961324580013752 + 0.01 * 6.919930458068848
Epoch 650, val loss: 1.2217963933944702
Epoch 660, training loss: 0.07797086983919144 = 0.008557664230465889 + 0.01 * 6.941320896148682
Epoch 660, val loss: 1.2295652627944946
Epoch 670, training loss: 0.07747919112443924 = 0.008182697929441929 + 0.01 * 6.929649829864502
Epoch 670, val loss: 1.2371102571487427
Epoch 680, training loss: 0.07697949558496475 = 0.00783434696495533 + 0.01 * 6.914515495300293
Epoch 680, val loss: 1.2443885803222656
Epoch 690, training loss: 0.07645561546087265 = 0.007509735878556967 + 0.01 * 6.894587516784668
Epoch 690, val loss: 1.2514607906341553
Epoch 700, training loss: 0.07613548636436462 = 0.007206867914646864 + 0.01 * 6.892862319946289
Epoch 700, val loss: 1.2583098411560059
Epoch 710, training loss: 0.07606394588947296 = 0.0069239153526723385 + 0.01 * 6.914003372192383
Epoch 710, val loss: 1.2649617195129395
Epoch 720, training loss: 0.07563532888889313 = 0.006659199483692646 + 0.01 * 6.897613048553467
Epoch 720, val loss: 1.2714262008666992
Epoch 730, training loss: 0.07517766207456589 = 0.006411063950508833 + 0.01 * 6.876660346984863
Epoch 730, val loss: 1.2776814699172974
Epoch 740, training loss: 0.07498058676719666 = 0.006177790928632021 + 0.01 * 6.880279541015625
Epoch 740, val loss: 1.2837698459625244
Epoch 750, training loss: 0.07467205077409744 = 0.005958787631243467 + 0.01 * 6.871326446533203
Epoch 750, val loss: 1.2896891832351685
Epoch 760, training loss: 0.07455281913280487 = 0.005752357188612223 + 0.01 * 6.880046367645264
Epoch 760, val loss: 1.2954559326171875
Epoch 770, training loss: 0.07412536442279816 = 0.005558086559176445 + 0.01 * 6.8567280769348145
Epoch 770, val loss: 1.301026701927185
Epoch 780, training loss: 0.07425766438245773 = 0.00537484185770154 + 0.01 * 6.888282299041748
Epoch 780, val loss: 1.306472897529602
Epoch 790, training loss: 0.07373615354299545 = 0.0052016731351614 + 0.01 * 6.853447914123535
Epoch 790, val loss: 1.311766505241394
Epoch 800, training loss: 0.07389111816883087 = 0.005037686787545681 + 0.01 * 6.885343074798584
Epoch 800, val loss: 1.3169441223144531
Epoch 810, training loss: 0.07339712232351303 = 0.004882861860096455 + 0.01 * 6.851426124572754
Epoch 810, val loss: 1.3219361305236816
Epoch 820, training loss: 0.07341600954532623 = 0.004735774826258421 + 0.01 * 6.868023872375488
Epoch 820, val loss: 1.3268351554870605
Epoch 830, training loss: 0.0731094628572464 = 0.004596158396452665 + 0.01 * 6.851330757141113
Epoch 830, val loss: 1.3316090106964111
Epoch 840, training loss: 0.0729428082704544 = 0.004463551566004753 + 0.01 * 6.847925662994385
Epoch 840, val loss: 1.3362609148025513
Epoch 850, training loss: 0.07280842959880829 = 0.004337507765740156 + 0.01 * 6.847092628479004
Epoch 850, val loss: 1.3407974243164062
Epoch 860, training loss: 0.07247939705848694 = 0.004217689856886864 + 0.01 * 6.826170444488525
Epoch 860, val loss: 1.345215082168579
Epoch 870, training loss: 0.07232154905796051 = 0.00410339143127203 + 0.01 * 6.821815490722656
Epoch 870, val loss: 1.349548101425171
Epoch 880, training loss: 0.07254072278738022 = 0.003994549624621868 + 0.01 * 6.854618072509766
Epoch 880, val loss: 1.3537613153457642
Epoch 890, training loss: 0.0720977932214737 = 0.003890800755470991 + 0.01 * 6.820699691772461
Epoch 890, val loss: 1.357890248298645
Epoch 900, training loss: 0.07192721217870712 = 0.0037918398156762123 + 0.01 * 6.81353759765625
Epoch 900, val loss: 1.3618983030319214
Epoch 910, training loss: 0.0720292329788208 = 0.0036973129026591778 + 0.01 * 6.833192348480225
Epoch 910, val loss: 1.3658300638198853
Epoch 920, training loss: 0.07174400985240936 = 0.003606881480664015 + 0.01 * 6.813713073730469
Epoch 920, val loss: 1.3696757555007935
Epoch 930, training loss: 0.0715477466583252 = 0.0035204889718443155 + 0.01 * 6.8027262687683105
Epoch 930, val loss: 1.373394250869751
Epoch 940, training loss: 0.07165832072496414 = 0.003437895094975829 + 0.01 * 6.822043418884277
Epoch 940, val loss: 1.3770413398742676
Epoch 950, training loss: 0.07143624126911163 = 0.0033585699275135994 + 0.01 * 6.807766914367676
Epoch 950, val loss: 1.3806190490722656
Epoch 960, training loss: 0.07119625806808472 = 0.003282513003796339 + 0.01 * 6.791375160217285
Epoch 960, val loss: 1.3841010332107544
Epoch 970, training loss: 0.07125550508499146 = 0.0032095788046717644 + 0.01 * 6.804593086242676
Epoch 970, val loss: 1.3875012397766113
Epoch 980, training loss: 0.07088786363601685 = 0.0031396038830280304 + 0.01 * 6.774826526641846
Epoch 980, val loss: 1.3908417224884033
Epoch 990, training loss: 0.07088539004325867 = 0.0030723006930202246 + 0.01 * 6.781309127807617
Epoch 990, val loss: 1.3941081762313843
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 2.045750856399536 = 1.9597827196121216 + 0.01 * 8.596807479858398
Epoch 0, val loss: 1.9517078399658203
Epoch 10, training loss: 2.03499174118042 = 1.9490242004394531 + 0.01 * 8.596762657165527
Epoch 10, val loss: 1.9418047666549683
Epoch 20, training loss: 2.0221211910247803 = 1.9361554384231567 + 0.01 * 8.596578598022461
Epoch 20, val loss: 1.9296385049819946
Epoch 30, training loss: 2.0044116973876953 = 1.9184517860412598 + 0.01 * 8.59598159790039
Epoch 30, val loss: 1.912642002105713
Epoch 40, training loss: 1.9784351587295532 = 1.8925082683563232 + 0.01 * 8.592689514160156
Epoch 40, val loss: 1.887743592262268
Epoch 50, training loss: 1.942104458808899 = 1.8563909530639648 + 0.01 * 8.571351051330566
Epoch 50, val loss: 1.8542890548706055
Epoch 60, training loss: 1.901789665222168 = 1.8168224096298218 + 0.01 * 8.496726989746094
Epoch 60, val loss: 1.8213688135147095
Epoch 70, training loss: 1.871623158454895 = 1.7884770631790161 + 0.01 * 8.314605712890625
Epoch 70, val loss: 1.8004177808761597
Epoch 80, training loss: 1.8407082557678223 = 1.7586301565170288 + 0.01 * 8.207816123962402
Epoch 80, val loss: 1.7731268405914307
Epoch 90, training loss: 1.79671311378479 = 1.7162806987762451 + 0.01 * 8.043245315551758
Epoch 90, val loss: 1.7352104187011719
Epoch 100, training loss: 1.7352972030639648 = 1.6565508842468262 + 0.01 * 7.874626159667969
Epoch 100, val loss: 1.6840567588806152
Epoch 110, training loss: 1.6560170650482178 = 1.5792508125305176 + 0.01 * 7.676629066467285
Epoch 110, val loss: 1.6203402280807495
Epoch 120, training loss: 1.571548342704773 = 1.4965476989746094 + 0.01 * 7.500063419342041
Epoch 120, val loss: 1.554490089416504
Epoch 130, training loss: 1.4921834468841553 = 1.418416142463684 + 0.01 * 7.376727104187012
Epoch 130, val loss: 1.494841456413269
Epoch 140, training loss: 1.417454719543457 = 1.3444355726242065 + 0.01 * 7.301911354064941
Epoch 140, val loss: 1.4413607120513916
Epoch 150, training loss: 1.3431576490402222 = 1.2705100774765015 + 0.01 * 7.2647552490234375
Epoch 150, val loss: 1.3890246152877808
Epoch 160, training loss: 1.2674448490142822 = 1.1950150728225708 + 0.01 * 7.242981433868408
Epoch 160, val loss: 1.3361214399337769
Epoch 170, training loss: 1.192287802696228 = 1.1200062036514282 + 0.01 * 7.228161334991455
Epoch 170, val loss: 1.2841564416885376
Epoch 180, training loss: 1.1211150884628296 = 1.0489537715911865 + 0.01 * 7.216134548187256
Epoch 180, val loss: 1.2367883920669556
Epoch 190, training loss: 1.0553754568099976 = 0.9833363890647888 + 0.01 * 7.203906536102295
Epoch 190, val loss: 1.1944963932037354
Epoch 200, training loss: 0.9937400817871094 = 0.921826183795929 + 0.01 * 7.191387176513672
Epoch 200, val loss: 1.1564306020736694
Epoch 210, training loss: 0.9332855343818665 = 0.8614775538444519 + 0.01 * 7.1807990074157715
Epoch 210, val loss: 1.120724081993103
Epoch 220, training loss: 0.8716103434562683 = 0.7998794317245483 + 0.01 * 7.173090934753418
Epoch 220, val loss: 1.0857268571853638
Epoch 230, training loss: 0.8078003525733948 = 0.736175537109375 + 0.01 * 7.16248083114624
Epoch 230, val loss: 1.0509992837905884
Epoch 240, training loss: 0.7431269288063049 = 0.6715710759162903 + 0.01 * 7.155584812164307
Epoch 240, val loss: 1.0174627304077148
Epoch 250, training loss: 0.6797464489936829 = 0.6082478761672974 + 0.01 * 7.149859428405762
Epoch 250, val loss: 0.9872183203697205
Epoch 260, training loss: 0.6197152733802795 = 0.5482335090637207 + 0.01 * 7.148177623748779
Epoch 260, val loss: 0.9625335931777954
Epoch 270, training loss: 0.5640001893043518 = 0.49257126450538635 + 0.01 * 7.142890930175781
Epoch 270, val loss: 0.9450823068618774
Epoch 280, training loss: 0.5127593278884888 = 0.44136181473731995 + 0.01 * 7.1397528648376465
Epoch 280, val loss: 0.9348209500312805
Epoch 290, training loss: 0.46575668454170227 = 0.39441314339637756 + 0.01 * 7.134354591369629
Epoch 290, val loss: 0.9312976002693176
Epoch 300, training loss: 0.42295071482658386 = 0.3515334129333496 + 0.01 * 7.141729831695557
Epoch 300, val loss: 0.9343728423118591
Epoch 310, training loss: 0.38402223587036133 = 0.3126635253429413 + 0.01 * 7.135871887207031
Epoch 310, val loss: 0.9436324834823608
Epoch 320, training loss: 0.3489270806312561 = 0.27762553095817566 + 0.01 * 7.130156517028809
Epoch 320, val loss: 0.9582112431526184
Epoch 330, training loss: 0.3174509108066559 = 0.24619461596012115 + 0.01 * 7.12562894821167
Epoch 330, val loss: 0.9770994782447815
Epoch 340, training loss: 0.2892760634422302 = 0.21805056929588318 + 0.01 * 7.122548580169678
Epoch 340, val loss: 0.9994086623191833
Epoch 350, training loss: 0.2640743553638458 = 0.19287940859794617 + 0.01 * 7.119495868682861
Epoch 350, val loss: 1.024366855621338
Epoch 360, training loss: 0.24157950282096863 = 0.1704125553369522 + 0.01 * 7.11669397354126
Epoch 360, val loss: 1.0512288808822632
Epoch 370, training loss: 0.22155381739139557 = 0.15041084587574005 + 0.01 * 7.114297389984131
Epoch 370, val loss: 1.0792878866195679
Epoch 380, training loss: 0.20375198125839233 = 0.1326131820678711 + 0.01 * 7.1138811111450195
Epoch 380, val loss: 1.1080585718154907
Epoch 390, training loss: 0.18786199390888214 = 0.1167619526386261 + 0.01 * 7.11000394821167
Epoch 390, val loss: 1.137328863143921
Epoch 400, training loss: 0.17372655868530273 = 0.10265766829252243 + 0.01 * 7.106889247894287
Epoch 400, val loss: 1.1667312383651733
Epoch 410, training loss: 0.16121330857276917 = 0.09016602486371994 + 0.01 * 7.104729175567627
Epoch 410, val loss: 1.1960147619247437
Epoch 420, training loss: 0.15027135610580444 = 0.07918017357587814 + 0.01 * 7.109119415283203
Epoch 420, val loss: 1.2250511646270752
Epoch 430, training loss: 0.14064309000968933 = 0.0696030855178833 + 0.01 * 7.104001522064209
Epoch 430, val loss: 1.2537171840667725
Epoch 440, training loss: 0.13229602575302124 = 0.06131983548402786 + 0.01 * 7.097618579864502
Epoch 440, val loss: 1.2820335626602173
Epoch 450, training loss: 0.12513664364814758 = 0.05419988930225372 + 0.01 * 7.0936760902404785
Epoch 450, val loss: 1.3098804950714111
Epoch 460, training loss: 0.11906784772872925 = 0.04810114949941635 + 0.01 * 7.096670150756836
Epoch 460, val loss: 1.3370676040649414
Epoch 470, training loss: 0.11380177736282349 = 0.042882341891527176 + 0.01 * 7.091943740844727
Epoch 470, val loss: 1.3635401725769043
Epoch 480, training loss: 0.10925479233264923 = 0.03840941935777664 + 0.01 * 7.084537506103516
Epoch 480, val loss: 1.3891407251358032
Epoch 490, training loss: 0.10551900416612625 = 0.03455911576747894 + 0.01 * 7.095988750457764
Epoch 490, val loss: 1.4139240980148315
Epoch 500, training loss: 0.10202701389789581 = 0.0312332883477211 + 0.01 * 7.079372406005859
Epoch 500, val loss: 1.437864065170288
Epoch 510, training loss: 0.09908048063516617 = 0.028343938291072845 + 0.01 * 7.073654651641846
Epoch 510, val loss: 1.460953712463379
Epoch 520, training loss: 0.09665142744779587 = 0.02582019567489624 + 0.01 * 7.083123207092285
Epoch 520, val loss: 1.4832102060317993
Epoch 530, training loss: 0.09434275329113007 = 0.02361052669584751 + 0.01 * 7.073222637176514
Epoch 530, val loss: 1.504692792892456
Epoch 540, training loss: 0.09229418635368347 = 0.021667351946234703 + 0.01 * 7.062683582305908
Epoch 540, val loss: 1.5254250764846802
Epoch 550, training loss: 0.09053650498390198 = 0.019949911162257195 + 0.01 * 7.058659076690674
Epoch 550, val loss: 1.545444369316101
Epoch 560, training loss: 0.08903777599334717 = 0.018426019698381424 + 0.01 * 7.06117582321167
Epoch 560, val loss: 1.5648033618927002
Epoch 570, training loss: 0.08759869635105133 = 0.017070850357413292 + 0.01 * 7.0527849197387695
Epoch 570, val loss: 1.5834487676620483
Epoch 580, training loss: 0.0865149274468422 = 0.015862392261624336 + 0.01 * 7.065253734588623
Epoch 580, val loss: 1.6014782190322876
Epoch 590, training loss: 0.08526132255792618 = 0.014781885780394077 + 0.01 * 7.047943592071533
Epoch 590, val loss: 1.618826150894165
Epoch 600, training loss: 0.08417633175849915 = 0.013810728676617146 + 0.01 * 7.036560535430908
Epoch 600, val loss: 1.6356608867645264
Epoch 610, training loss: 0.08334627747535706 = 0.01293508056551218 + 0.01 * 7.0411200523376465
Epoch 610, val loss: 1.6519334316253662
Epoch 620, training loss: 0.082495778799057 = 0.012144569307565689 + 0.01 * 7.035121440887451
Epoch 620, val loss: 1.6676490306854248
Epoch 630, training loss: 0.0817088782787323 = 0.01142818946391344 + 0.01 * 7.028068542480469
Epoch 630, val loss: 1.6827871799468994
Epoch 640, training loss: 0.08097691833972931 = 0.010776291601359844 + 0.01 * 7.020063400268555
Epoch 640, val loss: 1.697479248046875
Epoch 650, training loss: 0.08053193986415863 = 0.010181265883147717 + 0.01 * 7.035067558288574
Epoch 650, val loss: 1.71175217628479
Epoch 660, training loss: 0.07984935492277145 = 0.009637901559472084 + 0.01 * 7.021145343780518
Epoch 660, val loss: 1.725553035736084
Epoch 670, training loss: 0.07929850369691849 = 0.009140582755208015 + 0.01 * 7.015792369842529
Epoch 670, val loss: 1.7389140129089355
Epoch 680, training loss: 0.07874338328838348 = 0.008685109205543995 + 0.01 * 7.0058274269104
Epoch 680, val loss: 1.7518011331558228
Epoch 690, training loss: 0.07830924540758133 = 0.00826594140380621 + 0.01 * 7.004330158233643
Epoch 690, val loss: 1.7643505334854126
Epoch 700, training loss: 0.07782601565122604 = 0.007879747077822685 + 0.01 * 6.994626522064209
Epoch 700, val loss: 1.7765229940414429
Epoch 710, training loss: 0.07747118920087814 = 0.007523019332438707 + 0.01 * 6.994816780090332
Epoch 710, val loss: 1.7882835865020752
Epoch 720, training loss: 0.07701034098863602 = 0.007192293182015419 + 0.01 * 6.981804847717285
Epoch 720, val loss: 1.7997409105300903
Epoch 730, training loss: 0.07658779621124268 = 0.006885653827339411 + 0.01 * 6.970213890075684
Epoch 730, val loss: 1.8108713626861572
Epoch 740, training loss: 0.07632388174533844 = 0.006600569933652878 + 0.01 * 6.9723310470581055
Epoch 740, val loss: 1.8216564655303955
Epoch 750, training loss: 0.07597953826189041 = 0.006335420534014702 + 0.01 * 6.964411735534668
Epoch 750, val loss: 1.832147240638733
Epoch 760, training loss: 0.07582797110080719 = 0.006087871268391609 + 0.01 * 6.974009990692139
Epoch 760, val loss: 1.8422969579696655
Epoch 770, training loss: 0.07551911473274231 = 0.005857279058545828 + 0.01 * 6.966183662414551
Epoch 770, val loss: 1.8522067070007324
Epoch 780, training loss: 0.07524989545345306 = 0.005641572643071413 + 0.01 * 6.960832595825195
Epoch 780, val loss: 1.8617273569107056
Epoch 790, training loss: 0.074913389980793 = 0.005439561326056719 + 0.01 * 6.94738245010376
Epoch 790, val loss: 1.871141791343689
Epoch 800, training loss: 0.07457029819488525 = 0.005250091664493084 + 0.01 * 6.932021141052246
Epoch 800, val loss: 1.8802202939987183
Epoch 810, training loss: 0.0745835080742836 = 0.005071824416518211 + 0.01 * 6.951168060302734
Epoch 810, val loss: 1.8889477252960205
Epoch 820, training loss: 0.07420776784420013 = 0.004904318600893021 + 0.01 * 6.930345058441162
Epoch 820, val loss: 1.8976869583129883
Epoch 830, training loss: 0.07394534349441528 = 0.004746349062770605 + 0.01 * 6.919899940490723
Epoch 830, val loss: 1.9059712886810303
Epoch 840, training loss: 0.07381944358348846 = 0.004597578197717667 + 0.01 * 6.922187328338623
Epoch 840, val loss: 1.9140233993530273
Epoch 850, training loss: 0.07365735620260239 = 0.0044571165926754475 + 0.01 * 6.9200239181518555
Epoch 850, val loss: 1.922003149986267
Epoch 860, training loss: 0.07372304797172546 = 0.004324654582887888 + 0.01 * 6.9398393630981445
Epoch 860, val loss: 1.9297621250152588
Epoch 870, training loss: 0.07329566031694412 = 0.004199454560875893 + 0.01 * 6.909620761871338
Epoch 870, val loss: 1.9371787309646606
Epoch 880, training loss: 0.07309214770793915 = 0.004080739337950945 + 0.01 * 6.9011406898498535
Epoch 880, val loss: 1.9443110227584839
Epoch 890, training loss: 0.07330770045518875 = 0.003968277480453253 + 0.01 * 6.933942794799805
Epoch 890, val loss: 1.9513981342315674
Epoch 900, training loss: 0.0729302167892456 = 0.003861544420942664 + 0.01 * 6.906867027282715
Epoch 900, val loss: 1.9583815336227417
Epoch 910, training loss: 0.07269700616598129 = 0.0037599795032292604 + 0.01 * 6.893702983856201
Epoch 910, val loss: 1.9651154279708862
Epoch 920, training loss: 0.0724073201417923 = 0.003663225332275033 + 0.01 * 6.8744096755981445
Epoch 920, val loss: 1.9716191291809082
Epoch 930, training loss: 0.07225127518177032 = 0.003571311477571726 + 0.01 * 6.867996692657471
Epoch 930, val loss: 1.9780608415603638
Epoch 940, training loss: 0.07217886298894882 = 0.003483804175630212 + 0.01 * 6.869506359100342
Epoch 940, val loss: 1.9841995239257812
Epoch 950, training loss: 0.07221309095621109 = 0.0034004750195890665 + 0.01 * 6.881261825561523
Epoch 950, val loss: 1.990228533744812
Epoch 960, training loss: 0.07200024276971817 = 0.0033208834938704967 + 0.01 * 6.867936134338379
Epoch 960, val loss: 1.9963111877441406
Epoch 970, training loss: 0.07185972481966019 = 0.0032446684781461954 + 0.01 * 6.861505508422852
Epoch 970, val loss: 2.002018451690674
Epoch 980, training loss: 0.07172894477844238 = 0.003171851160004735 + 0.01 * 6.855709552764893
Epoch 980, val loss: 2.0076611042022705
Epoch 990, training loss: 0.07170931249856949 = 0.003102349117398262 + 0.01 * 6.860696315765381
Epoch 990, val loss: 2.0132598876953125
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 2.025491237640381 = 1.9395228624343872 + 0.01 * 8.596827507019043
Epoch 0, val loss: 1.9332587718963623
Epoch 10, training loss: 2.01545786857605 = 1.929490089416504 + 0.01 * 8.596780776977539
Epoch 10, val loss: 1.9241045713424683
Epoch 20, training loss: 2.00313401222229 = 1.9171679019927979 + 0.01 * 8.5966157913208
Epoch 20, val loss: 1.912562608718872
Epoch 30, training loss: 1.986107349395752 = 1.9001457691192627 + 0.01 * 8.59615707397461
Epoch 30, val loss: 1.8963521718978882
Epoch 40, training loss: 1.961665153503418 = 1.8757250308990479 + 0.01 * 8.594006538391113
Epoch 40, val loss: 1.8733566999435425
Epoch 50, training loss: 1.9288442134857178 = 1.843070387840271 + 0.01 * 8.577383041381836
Epoch 50, val loss: 1.8440766334533691
Epoch 60, training loss: 1.8933742046356201 = 1.808526873588562 + 0.01 * 8.484734535217285
Epoch 60, val loss: 1.8160051107406616
Epoch 70, training loss: 1.8603788614273071 = 1.778092861175537 + 0.01 * 8.22860050201416
Epoch 70, val loss: 1.790819764137268
Epoch 80, training loss: 1.8193745613098145 = 1.73871910572052 + 0.01 * 8.065540313720703
Epoch 80, val loss: 1.7539522647857666
Epoch 90, training loss: 1.7629114389419556 = 1.6839781999588013 + 0.01 * 7.893327236175537
Epoch 90, val loss: 1.7044155597686768
Epoch 100, training loss: 1.6879756450653076 = 1.6111568212509155 + 0.01 * 7.681877136230469
Epoch 100, val loss: 1.642045497894287
Epoch 110, training loss: 1.6024121046066284 = 1.527734398841858 + 0.01 * 7.467770576477051
Epoch 110, val loss: 1.571585774421692
Epoch 120, training loss: 1.518122911453247 = 1.4436171054840088 + 0.01 * 7.450575828552246
Epoch 120, val loss: 1.5039160251617432
Epoch 130, training loss: 1.4353973865509033 = 1.361146092414856 + 0.01 * 7.425125598907471
Epoch 130, val loss: 1.4399809837341309
Epoch 140, training loss: 1.352230191230774 = 1.2782772779464722 + 0.01 * 7.39528751373291
Epoch 140, val loss: 1.3779946565628052
Epoch 150, training loss: 1.2684593200683594 = 1.1947203874588013 + 0.01 * 7.373890399932861
Epoch 150, val loss: 1.3170921802520752
Epoch 160, training loss: 1.18629789352417 = 1.1126935482025146 + 0.01 * 7.360429286956787
Epoch 160, val loss: 1.260351538658142
Epoch 170, training loss: 1.107621669769287 = 1.0341435670852661 + 0.01 * 7.347809791564941
Epoch 170, val loss: 1.2085133790969849
Epoch 180, training loss: 1.0323090553283691 = 0.9590001702308655 + 0.01 * 7.330892086029053
Epoch 180, val loss: 1.1611015796661377
Epoch 190, training loss: 0.9587632417678833 = 0.8856900930404663 + 0.01 * 7.307312488555908
Epoch 190, val loss: 1.1153795719146729
Epoch 200, training loss: 0.8852576613426208 = 0.8124422430992126 + 0.01 * 7.281544208526611
Epoch 200, val loss: 1.0701349973678589
Epoch 210, training loss: 0.811408281326294 = 0.7388089299201965 + 0.01 * 7.259937763214111
Epoch 210, val loss: 1.0254589319229126
Epoch 220, training loss: 0.7390772104263306 = 0.6666876077651978 + 0.01 * 7.238958835601807
Epoch 220, val loss: 0.9832745790481567
Epoch 230, training loss: 0.6708210706710815 = 0.5985240340232849 + 0.01 * 7.229703903198242
Epoch 230, val loss: 0.9467419385910034
Epoch 240, training loss: 0.6081947684288025 = 0.5360517501831055 + 0.01 * 7.214303493499756
Epoch 240, val loss: 0.9177452921867371
Epoch 250, training loss: 0.5516468286514282 = 0.47960832715034485 + 0.01 * 7.203850269317627
Epoch 250, val loss: 0.89687579870224
Epoch 260, training loss: 0.500724732875824 = 0.4288056194782257 + 0.01 * 7.191914081573486
Epoch 260, val loss: 0.8832382559776306
Epoch 270, training loss: 0.4548722207546234 = 0.3830709457397461 + 0.01 * 7.180128574371338
Epoch 270, val loss: 0.8757571578025818
Epoch 280, training loss: 0.4136747717857361 = 0.34178099036216736 + 0.01 * 7.1893792152404785
Epoch 280, val loss: 0.8733330368995667
Epoch 290, training loss: 0.37582826614379883 = 0.30424681305885315 + 0.01 * 7.158144474029541
Epoch 290, val loss: 0.8747268319129944
Epoch 300, training loss: 0.3411610424518585 = 0.2696742117404938 + 0.01 * 7.148684024810791
Epoch 300, val loss: 0.8788294196128845
Epoch 310, training loss: 0.3090498447418213 = 0.23759612441062927 + 0.01 * 7.1453704833984375
Epoch 310, val loss: 0.8850188851356506
Epoch 320, training loss: 0.27933162450790405 = 0.20800162851810455 + 0.01 * 7.132998466491699
Epoch 320, val loss: 0.8931987881660461
Epoch 330, training loss: 0.25230708718299866 = 0.1810707151889801 + 0.01 * 7.123638153076172
Epoch 330, val loss: 0.9033339619636536
Epoch 340, training loss: 0.22815780341625214 = 0.15701383352279663 + 0.01 * 7.114397048950195
Epoch 340, val loss: 0.9154089093208313
Epoch 350, training loss: 0.20724231004714966 = 0.13586336374282837 + 0.01 * 7.137895584106445
Epoch 350, val loss: 0.9292458891868591
Epoch 360, training loss: 0.1885869801044464 = 0.11751046776771545 + 0.01 * 7.107651710510254
Epoch 360, val loss: 0.9446415901184082
Epoch 370, training loss: 0.1726137399673462 = 0.10169105976819992 + 0.01 * 7.0922675132751465
Epoch 370, val loss: 0.9611225128173828
Epoch 380, training loss: 0.15900498628616333 = 0.08811350911855698 + 0.01 * 7.08914852142334
Epoch 380, val loss: 0.9784422516822815
Epoch 390, training loss: 0.1473616659641266 = 0.0765000656247139 + 0.01 * 7.086161136627197
Epoch 390, val loss: 0.9964432120323181
Epoch 400, training loss: 0.13731378316879272 = 0.06658794730901718 + 0.01 * 7.07258415222168
Epoch 400, val loss: 1.014955759048462
Epoch 410, training loss: 0.12894307076931 = 0.05814080312848091 + 0.01 * 7.080226421356201
Epoch 410, val loss: 1.0337615013122559
Epoch 420, training loss: 0.12152385711669922 = 0.05096952244639397 + 0.01 * 7.055433750152588
Epoch 420, val loss: 1.0526161193847656
Epoch 430, training loss: 0.11539304256439209 = 0.0448886901140213 + 0.01 * 7.0504350662231445
Epoch 430, val loss: 1.0713374614715576
Epoch 440, training loss: 0.11029064655303955 = 0.03973383083939552 + 0.01 * 7.055681228637695
Epoch 440, val loss: 1.0897825956344604
Epoch 450, training loss: 0.10576453804969788 = 0.03536466136574745 + 0.01 * 7.039987087249756
Epoch 450, val loss: 1.1078523397445679
Epoch 460, training loss: 0.10192234814167023 = 0.031646471470594406 + 0.01 * 7.027587413787842
Epoch 460, val loss: 1.1254453659057617
Epoch 470, training loss: 0.09868844598531723 = 0.028470801189541817 + 0.01 * 7.021764755249023
Epoch 470, val loss: 1.1424874067306519
Epoch 480, training loss: 0.09585317969322205 = 0.02574603073298931 + 0.01 * 7.010714530944824
Epoch 480, val loss: 1.1589250564575195
Epoch 490, training loss: 0.09369628131389618 = 0.023394446820020676 + 0.01 * 7.030183792114258
Epoch 490, val loss: 1.1747077703475952
Epoch 500, training loss: 0.09136167168617249 = 0.02135683037340641 + 0.01 * 7.000484466552734
Epoch 500, val loss: 1.1899431943893433
Epoch 510, training loss: 0.08957372605800629 = 0.01958007737994194 + 0.01 * 6.999365329742432
Epoch 510, val loss: 1.2045451402664185
Epoch 520, training loss: 0.0878758653998375 = 0.018022937700152397 + 0.01 * 6.985292434692383
Epoch 520, val loss: 1.2185075283050537
Epoch 530, training loss: 0.08651912957429886 = 0.016651079058647156 + 0.01 * 6.986805438995361
Epoch 530, val loss: 1.2319577932357788
Epoch 540, training loss: 0.08518354594707489 = 0.015437564812600613 + 0.01 * 6.974598407745361
Epoch 540, val loss: 1.2448643445968628
Epoch 550, training loss: 0.08439892530441284 = 0.014357920736074448 + 0.01 * 7.004100799560547
Epoch 550, val loss: 1.2573134899139404
Epoch 560, training loss: 0.0829971432685852 = 0.013396242633461952 + 0.01 * 6.960090637207031
Epoch 560, val loss: 1.2692358493804932
Epoch 570, training loss: 0.08210258185863495 = 0.012533985078334808 + 0.01 * 6.956859588623047
Epoch 570, val loss: 1.2806845903396606
Epoch 580, training loss: 0.08127875626087189 = 0.011757285334169865 + 0.01 * 6.952147006988525
Epoch 580, val loss: 1.2917344570159912
Epoch 590, training loss: 0.0806245282292366 = 0.01105543877929449 + 0.01 * 6.9569091796875
Epoch 590, val loss: 1.3023909330368042
Epoch 600, training loss: 0.07982228696346283 = 0.010419866070151329 + 0.01 * 6.940241813659668
Epoch 600, val loss: 1.3126193284988403
Epoch 610, training loss: 0.07942978292703629 = 0.0098417392000556 + 0.01 * 6.958804607391357
Epoch 610, val loss: 1.3224680423736572
Epoch 620, training loss: 0.07864777743816376 = 0.009315427392721176 + 0.01 * 6.933235168457031
Epoch 620, val loss: 1.3320282697677612
Epoch 630, training loss: 0.07810445129871368 = 0.008833718486130238 + 0.01 * 6.9270734786987305
Epoch 630, val loss: 1.3412268161773682
Epoch 640, training loss: 0.07762369513511658 = 0.008391864597797394 + 0.01 * 6.923182964324951
Epoch 640, val loss: 1.3500902652740479
Epoch 650, training loss: 0.07714948803186417 = 0.007985791191458702 + 0.01 * 6.916369438171387
Epoch 650, val loss: 1.3586838245391846
Epoch 660, training loss: 0.07719370722770691 = 0.007610943168401718 + 0.01 * 6.958277225494385
Epoch 660, val loss: 1.366980791091919
Epoch 670, training loss: 0.07633749395608902 = 0.007265495136380196 + 0.01 * 6.907199859619141
Epoch 670, val loss: 1.375058650970459
Epoch 680, training loss: 0.07597663253545761 = 0.006945471744984388 + 0.01 * 6.903116226196289
Epoch 680, val loss: 1.3827941417694092
Epoch 690, training loss: 0.07582788914442062 = 0.006648558657616377 + 0.01 * 6.917933464050293
Epoch 690, val loss: 1.3903400897979736
Epoch 700, training loss: 0.07534011453390121 = 0.006372705101966858 + 0.01 * 6.896740913391113
Epoch 700, val loss: 1.3976603746414185
Epoch 710, training loss: 0.0750131756067276 = 0.00611552270129323 + 0.01 * 6.88976526260376
Epoch 710, val loss: 1.4046940803527832
Epoch 720, training loss: 0.07488963007926941 = 0.005875572096556425 + 0.01 * 6.9014058113098145
Epoch 720, val loss: 1.4115604162216187
Epoch 730, training loss: 0.07447052001953125 = 0.005651299376040697 + 0.01 * 6.881921768188477
Epoch 730, val loss: 1.4181989431381226
Epoch 740, training loss: 0.07429443299770355 = 0.00544141186401248 + 0.01 * 6.8853020668029785
Epoch 740, val loss: 1.4246103763580322
Epoch 750, training loss: 0.0739680677652359 = 0.005244887433946133 + 0.01 * 6.872318267822266
Epoch 750, val loss: 1.4309041500091553
Epoch 760, training loss: 0.0739026740193367 = 0.00506020849570632 + 0.01 * 6.884246349334717
Epoch 760, val loss: 1.43692147731781
Epoch 770, training loss: 0.07359889894723892 = 0.004886635113507509 + 0.01 * 6.8712263107299805
Epoch 770, val loss: 1.4428505897521973
Epoch 780, training loss: 0.07351706922054291 = 0.004723094869405031 + 0.01 * 6.879397869110107
Epoch 780, val loss: 1.4485681056976318
Epoch 790, training loss: 0.07326928526163101 = 0.004569238517433405 + 0.01 * 6.870004653930664
Epoch 790, val loss: 1.4540835618972778
Epoch 800, training loss: 0.07289545983076096 = 0.004424014128744602 + 0.01 * 6.847144603729248
Epoch 800, val loss: 1.459484577178955
Epoch 810, training loss: 0.0729539692401886 = 0.004286698065698147 + 0.01 * 6.866727352142334
Epoch 810, val loss: 1.4647810459136963
Epoch 820, training loss: 0.07277312874794006 = 0.004156896378844976 + 0.01 * 6.861623764038086
Epoch 820, val loss: 1.4698302745819092
Epoch 830, training loss: 0.07241109758615494 = 0.004034132231026888 + 0.01 * 6.8376970291137695
Epoch 830, val loss: 1.4747979640960693
Epoch 840, training loss: 0.07253065705299377 = 0.003917401190847158 + 0.01 * 6.861325263977051
Epoch 840, val loss: 1.4796558618545532
Epoch 850, training loss: 0.07217956334352493 = 0.0038069281727075577 + 0.01 * 6.837263584136963
Epoch 850, val loss: 1.4843509197235107
Epoch 860, training loss: 0.07213761657476425 = 0.003701952053233981 + 0.01 * 6.843565940856934
Epoch 860, val loss: 1.4889488220214844
Epoch 870, training loss: 0.071856789290905 = 0.0036020830739289522 + 0.01 * 6.825470924377441
Epoch 870, val loss: 1.4934159517288208
Epoch 880, training loss: 0.07174575328826904 = 0.003506912849843502 + 0.01 * 6.823884010314941
Epoch 880, val loss: 1.4977740049362183
Epoch 890, training loss: 0.07188339531421661 = 0.0034162872470915318 + 0.01 * 6.846711158752441
Epoch 890, val loss: 1.5019880533218384
Epoch 900, training loss: 0.07168982923030853 = 0.003330324310809374 + 0.01 * 6.8359503746032715
Epoch 900, val loss: 1.5061861276626587
Epoch 910, training loss: 0.07141438126564026 = 0.0032479416113346815 + 0.01 * 6.816644191741943
Epoch 910, val loss: 1.510227918624878
Epoch 920, training loss: 0.0713372528553009 = 0.0031693559139966965 + 0.01 * 6.816789627075195
Epoch 920, val loss: 1.514167070388794
Epoch 930, training loss: 0.07124874740839005 = 0.0030942466109991074 + 0.01 * 6.8154497146606445
Epoch 930, val loss: 1.5180323123931885
Epoch 940, training loss: 0.07125543802976608 = 0.003022393211722374 + 0.01 * 6.823304653167725
Epoch 940, val loss: 1.5217266082763672
Epoch 950, training loss: 0.07101413607597351 = 0.00295374752022326 + 0.01 * 6.806038856506348
Epoch 950, val loss: 1.5254443883895874
Epoch 960, training loss: 0.07104691118001938 = 0.0028880450408905745 + 0.01 * 6.815886974334717
Epoch 960, val loss: 1.5289547443389893
Epoch 970, training loss: 0.07080984860658646 = 0.002825166331604123 + 0.01 * 6.798468589782715
Epoch 970, val loss: 1.5324758291244507
Epoch 980, training loss: 0.07096105068922043 = 0.0027646797243505716 + 0.01 * 6.819637298583984
Epoch 980, val loss: 1.5359375476837158
Epoch 990, training loss: 0.0706871896982193 = 0.002706758677959442 + 0.01 * 6.798043251037598
Epoch 990, val loss: 1.5391942262649536
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8181338956246705
The final CL Acc:0.74074, 0.03024, The final GNN Acc:0.81919, 0.00188
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13188])
remove edge: torch.Size([2, 8012])
updated graph: torch.Size([2, 10644])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.030393123626709 = 1.9444248676300049 + 0.01 * 8.596835136413574
Epoch 0, val loss: 1.940627932548523
Epoch 10, training loss: 2.020278215408325 = 1.934309959411621 + 0.01 * 8.596821784973145
Epoch 10, val loss: 1.9311031103134155
Epoch 20, training loss: 2.0085606575012207 = 1.9225937128067017 + 0.01 * 8.596705436706543
Epoch 20, val loss: 1.919677972793579
Epoch 30, training loss: 1.9927582740783691 = 1.9067941904067993 + 0.01 * 8.596404075622559
Epoch 30, val loss: 1.9039114713668823
Epoch 40, training loss: 1.9699149131774902 = 1.8839623928070068 + 0.01 * 8.595256805419922
Epoch 40, val loss: 1.8810850381851196
Epoch 50, training loss: 1.9372831583023071 = 1.8514074087142944 + 0.01 * 8.58757209777832
Epoch 50, val loss: 1.8497495651245117
Epoch 60, training loss: 1.8974339962005615 = 1.8119980096817017 + 0.01 * 8.543601036071777
Epoch 60, val loss: 1.8153289556503296
Epoch 70, training loss: 1.8578388690948486 = 1.7745710611343384 + 0.01 * 8.326777458190918
Epoch 70, val loss: 1.786157250404358
Epoch 80, training loss: 1.8129075765609741 = 1.7314032316207886 + 0.01 * 8.150437355041504
Epoch 80, val loss: 1.7485311031341553
Epoch 90, training loss: 1.7494878768920898 = 1.6709961891174316 + 0.01 * 7.8491692543029785
Epoch 90, val loss: 1.6930359601974487
Epoch 100, training loss: 1.6653733253479004 = 1.588762879371643 + 0.01 * 7.66103982925415
Epoch 100, val loss: 1.6197980642318726
Epoch 110, training loss: 1.5623753070831299 = 1.4863277673721313 + 0.01 * 7.604750156402588
Epoch 110, val loss: 1.531970500946045
Epoch 120, training loss: 1.4490290880203247 = 1.3734525442123413 + 0.01 * 7.557651042938232
Epoch 120, val loss: 1.4360144138336182
Epoch 130, training loss: 1.332072138786316 = 1.257104754447937 + 0.01 * 7.496744155883789
Epoch 130, val loss: 1.3381924629211426
Epoch 140, training loss: 1.2170500755310059 = 1.1429097652435303 + 0.01 * 7.414034843444824
Epoch 140, val loss: 1.2442251443862915
Epoch 150, training loss: 1.1092753410339355 = 1.035640835762024 + 0.01 * 7.3634490966796875
Epoch 150, val loss: 1.1575517654418945
Epoch 160, training loss: 1.0116697549819946 = 0.9381356835365295 + 0.01 * 7.353410243988037
Epoch 160, val loss: 1.080600380897522
Epoch 170, training loss: 0.9249128699302673 = 0.8515500426292419 + 0.01 * 7.3362836837768555
Epoch 170, val loss: 1.0137296915054321
Epoch 180, training loss: 0.8491297960281372 = 0.7759522795677185 + 0.01 * 7.317748546600342
Epoch 180, val loss: 0.9569380879402161
Epoch 190, training loss: 0.7831563949584961 = 0.7102342247962952 + 0.01 * 7.292220115661621
Epoch 190, val loss: 0.9096998572349548
Epoch 200, training loss: 0.7245257496833801 = 0.6518274545669556 + 0.01 * 7.269828796386719
Epoch 200, val loss: 0.870201826095581
Epoch 210, training loss: 0.6701214909553528 = 0.5976587533950806 + 0.01 * 7.246275901794434
Epoch 210, val loss: 0.8360029458999634
Epoch 220, training loss: 0.6179531812667847 = 0.5456520318984985 + 0.01 * 7.230116844177246
Epoch 220, val loss: 0.8061888813972473
Epoch 230, training loss: 0.5674748420715332 = 0.49525871872901917 + 0.01 * 7.221610069274902
Epoch 230, val loss: 0.780472457408905
Epoch 240, training loss: 0.5194143652915955 = 0.4472518861293793 + 0.01 * 7.216248512268066
Epoch 240, val loss: 0.759617030620575
Epoch 250, training loss: 0.47465863823890686 = 0.4025213122367859 + 0.01 * 7.213733673095703
Epoch 250, val loss: 0.7439382672309875
Epoch 260, training loss: 0.43337252736091614 = 0.3612866699695587 + 0.01 * 7.208584785461426
Epoch 260, val loss: 0.7331796884536743
Epoch 270, training loss: 0.3951423764228821 = 0.32308632135391235 + 0.01 * 7.2056074142456055
Epoch 270, val loss: 0.7263695597648621
Epoch 280, training loss: 0.35936757922172546 = 0.2873457670211792 + 0.01 * 7.202180862426758
Epoch 280, val loss: 0.7224434018135071
Epoch 290, training loss: 0.32588475942611694 = 0.2538887858390808 + 0.01 * 7.199599266052246
Epoch 290, val loss: 0.7208178043365479
Epoch 300, training loss: 0.29504552483558655 = 0.22307364642620087 + 0.01 * 7.197187423706055
Epoch 300, val loss: 0.721562922000885
Epoch 310, training loss: 0.2674063444137573 = 0.1954437792301178 + 0.01 * 7.196258068084717
Epoch 310, val loss: 0.7248878479003906
Epoch 320, training loss: 0.24320760369300842 = 0.17128001153469086 + 0.01 * 7.192758560180664
Epoch 320, val loss: 0.7307878136634827
Epoch 330, training loss: 0.22238419950008392 = 0.15047258138656616 + 0.01 * 7.191161632537842
Epoch 330, val loss: 0.7390009760856628
Epoch 340, training loss: 0.204543799161911 = 0.13265833258628845 + 0.01 * 7.1885457038879395
Epoch 340, val loss: 0.7492163181304932
Epoch 350, training loss: 0.1892375648021698 = 0.11738435178995132 + 0.01 * 7.185321807861328
Epoch 350, val loss: 0.7610369324684143
Epoch 360, training loss: 0.17604705691337585 = 0.10423383861780167 + 0.01 * 7.1813225746154785
Epoch 360, val loss: 0.7740283012390137
Epoch 370, training loss: 0.1646420657634735 = 0.0928531289100647 + 0.01 * 7.178893089294434
Epoch 370, val loss: 0.7878320813179016
Epoch 380, training loss: 0.15470848977565765 = 0.0829576924443245 + 0.01 * 7.175079822540283
Epoch 380, val loss: 0.8021785020828247
Epoch 390, training loss: 0.14605170488357544 = 0.07431679219007492 + 0.01 * 7.173490524291992
Epoch 390, val loss: 0.8168706893920898
Epoch 400, training loss: 0.1384119987487793 = 0.06673437356948853 + 0.01 * 7.167763710021973
Epoch 400, val loss: 0.8316811323165894
Epoch 410, training loss: 0.13168597221374512 = 0.06005973741412163 + 0.01 * 7.162623405456543
Epoch 410, val loss: 0.8465540409088135
Epoch 420, training loss: 0.12580841779708862 = 0.05416879057884216 + 0.01 * 7.1639628410339355
Epoch 420, val loss: 0.8613511323928833
Epoch 430, training loss: 0.12052032351493835 = 0.04896507039666176 + 0.01 * 7.1555256843566895
Epoch 430, val loss: 0.8759840130805969
Epoch 440, training loss: 0.11586160212755203 = 0.044362351298332214 + 0.01 * 7.149925231933594
Epoch 440, val loss: 0.8904191851615906
Epoch 450, training loss: 0.11178114265203476 = 0.040287844836711884 + 0.01 * 7.149330139160156
Epoch 450, val loss: 0.9045717120170593
Epoch 460, training loss: 0.10806450247764587 = 0.03667806461453438 + 0.01 * 7.138644218444824
Epoch 460, val loss: 0.9184213876724243
Epoch 470, training loss: 0.10480629652738571 = 0.0334702730178833 + 0.01 * 7.133602142333984
Epoch 470, val loss: 0.9320040941238403
Epoch 480, training loss: 0.1019146591424942 = 0.03062216192483902 + 0.01 * 7.129249572753906
Epoch 480, val loss: 0.9452733397483826
Epoch 490, training loss: 0.09933525323867798 = 0.02808881178498268 + 0.01 * 7.1246442794799805
Epoch 490, val loss: 0.9581766724586487
Epoch 500, training loss: 0.09699366986751556 = 0.025831304490566254 + 0.01 * 7.116236686706543
Epoch 500, val loss: 0.9707738757133484
Epoch 510, training loss: 0.09484940767288208 = 0.023817528039216995 + 0.01 * 7.1031880378723145
Epoch 510, val loss: 0.9829674363136292
Epoch 520, training loss: 0.09302613139152527 = 0.02201627939939499 + 0.01 * 7.100985527038574
Epoch 520, val loss: 0.99481201171875
Epoch 530, training loss: 0.09120616316795349 = 0.020403005182743073 + 0.01 * 7.080316066741943
Epoch 530, val loss: 1.006308674812317
Epoch 540, training loss: 0.08971801400184631 = 0.018954137340188026 + 0.01 * 7.076387882232666
Epoch 540, val loss: 1.017412781715393
Epoch 550, training loss: 0.08833958208560944 = 0.01765431836247444 + 0.01 * 7.068526744842529
Epoch 550, val loss: 1.0281579494476318
Epoch 560, training loss: 0.08701564371585846 = 0.01648111082613468 + 0.01 * 7.0534539222717285
Epoch 560, val loss: 1.0385726690292358
Epoch 570, training loss: 0.08587416261434555 = 0.01541929692029953 + 0.01 * 7.0454864501953125
Epoch 570, val loss: 1.048676609992981
Epoch 580, training loss: 0.08512593060731888 = 0.014456996694207191 + 0.01 * 7.066893577575684
Epoch 580, val loss: 1.0584747791290283
Epoch 590, training loss: 0.08395132422447205 = 0.013587507419288158 + 0.01 * 7.03638219833374
Epoch 590, val loss: 1.0678105354309082
Epoch 600, training loss: 0.08293132483959198 = 0.012795623391866684 + 0.01 * 7.013570785522461
Epoch 600, val loss: 1.0769294500350952
Epoch 610, training loss: 0.08221160620450974 = 0.012072688899934292 + 0.01 * 7.013891696929932
Epoch 610, val loss: 1.0857232809066772
Epoch 620, training loss: 0.08150710165500641 = 0.011414481326937675 + 0.01 * 7.0092620849609375
Epoch 620, val loss: 1.0941492319107056
Epoch 630, training loss: 0.0806533694267273 = 0.010810510255396366 + 0.01 * 6.984286308288574
Epoch 630, val loss: 1.102370023727417
Epoch 640, training loss: 0.08016666769981384 = 0.010255170054733753 + 0.01 * 6.991149425506592
Epoch 640, val loss: 1.1102973222732544
Epoch 650, training loss: 0.07935105264186859 = 0.009745500050485134 + 0.01 * 6.960555076599121
Epoch 650, val loss: 1.117938756942749
Epoch 660, training loss: 0.07885568588972092 = 0.009275504387915134 + 0.01 * 6.9580183029174805
Epoch 660, val loss: 1.1253143548965454
Epoch 670, training loss: 0.07861954718828201 = 0.008841484785079956 + 0.01 * 6.977806568145752
Epoch 670, val loss: 1.132493019104004
Epoch 680, training loss: 0.07785185426473618 = 0.008439420722424984 + 0.01 * 6.9412431716918945
Epoch 680, val loss: 1.1394213438034058
Epoch 690, training loss: 0.07732744514942169 = 0.00806664489209652 + 0.01 * 6.926080226898193
Epoch 690, val loss: 1.1460866928100586
Epoch 700, training loss: 0.07695882767438889 = 0.0077205318957567215 + 0.01 * 6.923829555511475
Epoch 700, val loss: 1.1525821685791016
Epoch 710, training loss: 0.07674948126077652 = 0.007397993467748165 + 0.01 * 6.9351487159729
Epoch 710, val loss: 1.1588753461837769
Epoch 720, training loss: 0.07631559669971466 = 0.007097455207258463 + 0.01 * 6.921814441680908
Epoch 720, val loss: 1.1649397611618042
Epoch 730, training loss: 0.07593296468257904 = 0.006816660054028034 + 0.01 * 6.911630153656006
Epoch 730, val loss: 1.1708952188491821
Epoch 740, training loss: 0.07549063116312027 = 0.0065537323243916035 + 0.01 * 6.89369010925293
Epoch 740, val loss: 1.1766033172607422
Epoch 750, training loss: 0.07518620789051056 = 0.006307496223598719 + 0.01 * 6.887871265411377
Epoch 750, val loss: 1.18220853805542
Epoch 760, training loss: 0.07485385239124298 = 0.00607629818841815 + 0.01 * 6.877755165100098
Epoch 760, val loss: 1.1876177787780762
Epoch 770, training loss: 0.0745777115225792 = 0.005858986638486385 + 0.01 * 6.871872425079346
Epoch 770, val loss: 1.1928852796554565
Epoch 780, training loss: 0.07448125630617142 = 0.005654448643326759 + 0.01 * 6.882680892944336
Epoch 780, val loss: 1.1980502605438232
Epoch 790, training loss: 0.07415761798620224 = 0.005461387801915407 + 0.01 * 6.869623184204102
Epoch 790, val loss: 1.203001618385315
Epoch 800, training loss: 0.07389555126428604 = 0.005279762204736471 + 0.01 * 6.861578941345215
Epoch 800, val loss: 1.2078722715377808
Epoch 810, training loss: 0.07370109856128693 = 0.005108615383505821 + 0.01 * 6.859248638153076
Epoch 810, val loss: 1.2125351428985596
Epoch 820, training loss: 0.07344208657741547 = 0.004946745466440916 + 0.01 * 6.849534034729004
Epoch 820, val loss: 1.2171131372451782
Epoch 830, training loss: 0.07329858094453812 = 0.004793132189661264 + 0.01 * 6.850545406341553
Epoch 830, val loss: 1.2215808629989624
Epoch 840, training loss: 0.07308316230773926 = 0.0046476623974740505 + 0.01 * 6.843550205230713
Epoch 840, val loss: 1.225936770439148
Epoch 850, training loss: 0.07284613698720932 = 0.004509816411882639 + 0.01 * 6.833632469177246
Epoch 850, val loss: 1.2301418781280518
Epoch 860, training loss: 0.07280605286359787 = 0.004379033576697111 + 0.01 * 6.842702388763428
Epoch 860, val loss: 1.234287142753601
Epoch 870, training loss: 0.0725773349404335 = 0.004254759289324284 + 0.01 * 6.8322577476501465
Epoch 870, val loss: 1.2382817268371582
Epoch 880, training loss: 0.07231377065181732 = 0.004136477597057819 + 0.01 * 6.817729473114014
Epoch 880, val loss: 1.2422009706497192
Epoch 890, training loss: 0.07250983268022537 = 0.004023800604045391 + 0.01 * 6.84860372543335
Epoch 890, val loss: 1.2460098266601562
Epoch 900, training loss: 0.0722297802567482 = 0.003916595596820116 + 0.01 * 6.831318378448486
Epoch 900, val loss: 1.2497297525405884
Epoch 910, training loss: 0.07203956693410873 = 0.0038144602440297604 + 0.01 * 6.822510719299316
Epoch 910, val loss: 1.2532975673675537
Epoch 920, training loss: 0.07174234092235565 = 0.0037170215509831905 + 0.01 * 6.802532196044922
Epoch 920, val loss: 1.2568273544311523
Epoch 930, training loss: 0.07193097472190857 = 0.0036238536704331636 + 0.01 * 6.830712795257568
Epoch 930, val loss: 1.2602136135101318
Epoch 940, training loss: 0.07166892290115356 = 0.0035352862905710936 + 0.01 * 6.813364028930664
Epoch 940, val loss: 1.2635408639907837
Epoch 950, training loss: 0.07145173102617264 = 0.0034503380302339792 + 0.01 * 6.800139427185059
Epoch 950, val loss: 1.2667882442474365
Epoch 960, training loss: 0.07166540622711182 = 0.0033690198324620724 + 0.01 * 6.829638957977295
Epoch 960, val loss: 1.269943356513977
Epoch 970, training loss: 0.07139372080564499 = 0.0032911577727645636 + 0.01 * 6.810256004333496
Epoch 970, val loss: 1.2730238437652588
Epoch 980, training loss: 0.07123801857233047 = 0.0032165837474167347 + 0.01 * 6.802143573760986
Epoch 980, val loss: 1.2760199308395386
Epoch 990, training loss: 0.07105477899312973 = 0.003145062830299139 + 0.01 * 6.7909722328186035
Epoch 990, val loss: 1.2789493799209595
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.028043031692505 = 1.9420747756958008 + 0.01 * 8.596817016601562
Epoch 0, val loss: 1.9419255256652832
Epoch 10, training loss: 2.018371820449829 = 1.9324043989181519 + 0.01 * 8.5967435836792
Epoch 10, val loss: 1.9324146509170532
Epoch 20, training loss: 2.0063624382019043 = 1.9203975200653076 + 0.01 * 8.596494674682617
Epoch 20, val loss: 1.9200537204742432
Epoch 30, training loss: 1.989334225654602 = 1.9033774137496948 + 0.01 * 8.595678329467773
Epoch 30, val loss: 1.9021531343460083
Epoch 40, training loss: 1.9639034271240234 = 1.87799072265625 + 0.01 * 8.591267585754395
Epoch 40, val loss: 1.8754843473434448
Epoch 50, training loss: 1.9279417991638184 = 1.8423088788986206 + 0.01 * 8.5632963180542
Epoch 50, val loss: 1.8392781019210815
Epoch 60, training loss: 1.8860832452774048 = 1.8016799688339233 + 0.01 * 8.440323829650879
Epoch 60, val loss: 1.8016242980957031
Epoch 70, training loss: 1.8473689556121826 = 1.7652581930160522 + 0.01 * 8.211074829101562
Epoch 70, val loss: 1.7710673809051514
Epoch 80, training loss: 1.8004096746444702 = 1.7202527523040771 + 0.01 * 8.015694618225098
Epoch 80, val loss: 1.7310714721679688
Epoch 90, training loss: 1.7357141971588135 = 1.6578842401504517 + 0.01 * 7.782998085021973
Epoch 90, val loss: 1.6750303506851196
Epoch 100, training loss: 1.6509149074554443 = 1.575330138206482 + 0.01 * 7.558477401733398
Epoch 100, val loss: 1.6037402153015137
Epoch 110, training loss: 1.5531402826309204 = 1.4787545204162598 + 0.01 * 7.438573837280273
Epoch 110, val loss: 1.5206620693206787
Epoch 120, training loss: 1.4525073766708374 = 1.378779649734497 + 0.01 * 7.372769832611084
Epoch 120, val loss: 1.4365745782852173
Epoch 130, training loss: 1.3523483276367188 = 1.2790547609329224 + 0.01 * 7.3293609619140625
Epoch 130, val loss: 1.3551666736602783
Epoch 140, training loss: 1.2526121139526367 = 1.1796404123306274 + 0.01 * 7.2971720695495605
Epoch 140, val loss: 1.2755470275878906
Epoch 150, training loss: 1.155064582824707 = 1.0823558568954468 + 0.01 * 7.2708740234375
Epoch 150, val loss: 1.1996995210647583
Epoch 160, training loss: 1.0622719526290894 = 0.9898126721382141 + 0.01 * 7.245924949645996
Epoch 160, val loss: 1.1286816596984863
Epoch 170, training loss: 0.9754958152770996 = 0.9032968282699585 + 0.01 * 7.219898700714111
Epoch 170, val loss: 1.0626330375671387
Epoch 180, training loss: 0.8947606682777405 = 0.8227846026420593 + 0.01 * 7.197606563568115
Epoch 180, val loss: 1.001021385192871
Epoch 190, training loss: 0.8203021883964539 = 0.7484650015830994 + 0.01 * 7.183719158172607
Epoch 190, val loss: 0.9442671537399292
Epoch 200, training loss: 0.7525086998939514 = 0.6807649731636047 + 0.01 * 7.174371719360352
Epoch 200, val loss: 0.8935510516166687
Epoch 210, training loss: 0.6909132599830627 = 0.6192436218261719 + 0.01 * 7.166962623596191
Epoch 210, val loss: 0.8497409820556641
Epoch 220, training loss: 0.6337104439735413 = 0.5621108412742615 + 0.01 * 7.15995979309082
Epoch 220, val loss: 0.8123171925544739
Epoch 230, training loss: 0.5790311694145203 = 0.5075024962425232 + 0.01 * 7.152865886688232
Epoch 230, val loss: 0.7802340388298035
Epoch 240, training loss: 0.5259090662002563 = 0.45445746183395386 + 0.01 * 7.145163059234619
Epoch 240, val loss: 0.7523210644721985
Epoch 250, training loss: 0.4741998314857483 = 0.40282493829727173 + 0.01 * 7.137488842010498
Epoch 250, val loss: 0.7274228930473328
Epoch 260, training loss: 0.4243201017379761 = 0.3530338406562805 + 0.01 * 7.128626346588135
Epoch 260, val loss: 0.7053284049034119
Epoch 270, training loss: 0.37717875838279724 = 0.3059667944908142 + 0.01 * 7.1211957931518555
Epoch 270, val loss: 0.6861804127693176
Epoch 280, training loss: 0.33380720019340515 = 0.26265549659729004 + 0.01 * 7.115170955657959
Epoch 280, val loss: 0.6705087423324585
Epoch 290, training loss: 0.2950751781463623 = 0.22400693595409393 + 0.01 * 7.1068243980407715
Epoch 290, val loss: 0.6589183807373047
Epoch 300, training loss: 0.261579304933548 = 0.1905226707458496 + 0.01 * 7.105662822723389
Epoch 300, val loss: 0.6517000794410706
Epoch 310, training loss: 0.23311464488506317 = 0.16215962171554565 + 0.01 * 7.0955023765563965
Epoch 310, val loss: 0.6488410234451294
Epoch 320, training loss: 0.20928558707237244 = 0.13845449686050415 + 0.01 * 7.083108425140381
Epoch 320, val loss: 0.6500152349472046
Epoch 330, training loss: 0.18952517211437225 = 0.11876114457845688 + 0.01 * 7.07640266418457
Epoch 330, val loss: 0.6546270251274109
Epoch 340, training loss: 0.17313407361507416 = 0.10241527110338211 + 0.01 * 7.071880340576172
Epoch 340, val loss: 0.6619469523429871
Epoch 350, training loss: 0.15948686003684998 = 0.0888286754488945 + 0.01 * 7.065818786621094
Epoch 350, val loss: 0.6711657643318176
Epoch 360, training loss: 0.1480882465839386 = 0.07750295102596283 + 0.01 * 7.058530330657959
Epoch 360, val loss: 0.6816213726997375
Epoch 370, training loss: 0.13853470981121063 = 0.06802760064601898 + 0.01 * 7.050711154937744
Epoch 370, val loss: 0.6928399205207825
Epoch 380, training loss: 0.130515456199646 = 0.06006373465061188 + 0.01 * 7.04517126083374
Epoch 380, val loss: 0.7044883966445923
Epoch 390, training loss: 0.12376545369625092 = 0.05333566293120384 + 0.01 * 7.042978763580322
Epoch 390, val loss: 0.7163113355636597
Epoch 400, training loss: 0.1179739311337471 = 0.047620393335819244 + 0.01 * 7.035353660583496
Epoch 400, val loss: 0.7280645966529846
Epoch 410, training loss: 0.11298371851444244 = 0.042735643684864044 + 0.01 * 7.024807929992676
Epoch 410, val loss: 0.7396776676177979
Epoch 420, training loss: 0.10884661972522736 = 0.038539525121450424 + 0.01 * 7.030710220336914
Epoch 420, val loss: 0.7510700225830078
Epoch 430, training loss: 0.10508135706186295 = 0.03491794317960739 + 0.01 * 7.016341686248779
Epoch 430, val loss: 0.7622195482254028
Epoch 440, training loss: 0.10184149444103241 = 0.031770188361406326 + 0.01 * 7.007130146026611
Epoch 440, val loss: 0.7731122970581055
Epoch 450, training loss: 0.09901232272386551 = 0.029018910601735115 + 0.01 * 6.9993414878845215
Epoch 450, val loss: 0.7837181091308594
Epoch 460, training loss: 0.09658603370189667 = 0.02660420909523964 + 0.01 * 6.99818229675293
Epoch 460, val loss: 0.7940236330032349
Epoch 470, training loss: 0.09435398876667023 = 0.024476371705532074 + 0.01 * 6.987761974334717
Epoch 470, val loss: 0.8040298819541931
Epoch 480, training loss: 0.09241673350334167 = 0.022591713815927505 + 0.01 * 6.982501983642578
Epoch 480, val loss: 0.813779354095459
Epoch 490, training loss: 0.09080912172794342 = 0.020916394889354706 + 0.01 * 6.989272594451904
Epoch 490, val loss: 0.8232629895210266
Epoch 500, training loss: 0.08915234357118607 = 0.01942385546863079 + 0.01 * 6.972848892211914
Epoch 500, val loss: 0.8324204683303833
Epoch 510, training loss: 0.08769003301858902 = 0.018087057396769524 + 0.01 * 6.960297584533691
Epoch 510, val loss: 0.8413249850273132
Epoch 520, training loss: 0.08653683960437775 = 0.016885947436094284 + 0.01 * 6.965089321136475
Epoch 520, val loss: 0.8499842882156372
Epoch 530, training loss: 0.08530525118112564 = 0.015802763402462006 + 0.01 * 6.950248718261719
Epoch 530, val loss: 0.8583874106407166
Epoch 540, training loss: 0.08426470309495926 = 0.014821725897490978 + 0.01 * 6.944297790527344
Epoch 540, val loss: 0.8665820956230164
Epoch 550, training loss: 0.08331338316202164 = 0.013931626453995705 + 0.01 * 6.938176155090332
Epoch 550, val loss: 0.8745201230049133
Epoch 560, training loss: 0.08245029300451279 = 0.013121401891112328 + 0.01 * 6.932888984680176
Epoch 560, val loss: 0.8822909593582153
Epoch 570, training loss: 0.08172029256820679 = 0.01238178089261055 + 0.01 * 6.93385124206543
Epoch 570, val loss: 0.8898364305496216
Epoch 580, training loss: 0.08088822662830353 = 0.01170357596129179 + 0.01 * 6.918465614318848
Epoch 580, val loss: 0.8971489667892456
Epoch 590, training loss: 0.08025898039340973 = 0.011077750474214554 + 0.01 * 6.9181227684021
Epoch 590, val loss: 0.9043413996696472
Epoch 600, training loss: 0.07962018996477127 = 0.010503931902348995 + 0.01 * 6.911625862121582
Epoch 600, val loss: 0.9113448262214661
Epoch 610, training loss: 0.07905516028404236 = 0.009973239153623581 + 0.01 * 6.908191680908203
Epoch 610, val loss: 0.9181408882141113
Epoch 620, training loss: 0.07871304452419281 = 0.00948508083820343 + 0.01 * 6.922796726226807
Epoch 620, val loss: 0.9248034358024597
Epoch 630, training loss: 0.07803189754486084 = 0.009034248068928719 + 0.01 * 6.8997650146484375
Epoch 630, val loss: 0.9312357902526855
Epoch 640, training loss: 0.07755699008703232 = 0.008615134283900261 + 0.01 * 6.894185543060303
Epoch 640, val loss: 0.9375706315040588
Epoch 650, training loss: 0.07718019932508469 = 0.008221392519772053 + 0.01 * 6.895880699157715
Epoch 650, val loss: 0.9439507722854614
Epoch 660, training loss: 0.07668916136026382 = 0.00785108469426632 + 0.01 * 6.88380765914917
Epoch 660, val loss: 0.9503087997436523
Epoch 670, training loss: 0.07669488340616226 = 0.007501864805817604 + 0.01 * 6.919301986694336
Epoch 670, val loss: 0.9567537903785706
Epoch 680, training loss: 0.07594005763530731 = 0.007171911653131247 + 0.01 * 6.876814365386963
Epoch 680, val loss: 0.9630864858627319
Epoch 690, training loss: 0.07566100358963013 = 0.006860949099063873 + 0.01 * 6.880005359649658
Epoch 690, val loss: 0.9694378972053528
Epoch 700, training loss: 0.0753452256321907 = 0.006568854209035635 + 0.01 * 6.877636909484863
Epoch 700, val loss: 0.975743293762207
Epoch 710, training loss: 0.07498389482498169 = 0.006294619292020798 + 0.01 * 6.8689284324646
Epoch 710, val loss: 0.9820064902305603
Epoch 720, training loss: 0.07468178868293762 = 0.006036012899130583 + 0.01 * 6.864577770233154
Epoch 720, val loss: 0.9881977438926697
Epoch 730, training loss: 0.0744171142578125 = 0.005792806390672922 + 0.01 * 6.862431526184082
Epoch 730, val loss: 0.9943374991416931
Epoch 740, training loss: 0.0741928443312645 = 0.00556463235989213 + 0.01 * 6.862821578979492
Epoch 740, val loss: 1.0003503561019897
Epoch 750, training loss: 0.07392150163650513 = 0.005350153893232346 + 0.01 * 6.857134819030762
Epoch 750, val loss: 1.0062885284423828
Epoch 760, training loss: 0.07380464673042297 = 0.005148530937731266 + 0.01 * 6.865612030029297
Epoch 760, val loss: 1.0121312141418457
Epoch 770, training loss: 0.07349415868520737 = 0.00495841633528471 + 0.01 * 6.853574752807617
Epoch 770, val loss: 1.0177947282791138
Epoch 780, training loss: 0.07323486357927322 = 0.004779697395861149 + 0.01 * 6.845516681671143
Epoch 780, val loss: 1.0233807563781738
Epoch 790, training loss: 0.07310688495635986 = 0.004611309617757797 + 0.01 * 6.849557399749756
Epoch 790, val loss: 1.028865098953247
Epoch 800, training loss: 0.07290077209472656 = 0.004452251363545656 + 0.01 * 6.844851970672607
Epoch 800, val loss: 1.03420090675354
Epoch 810, training loss: 0.07274369150400162 = 0.0043015750125050545 + 0.01 * 6.844211578369141
Epoch 810, val loss: 1.0394020080566406
Epoch 820, training loss: 0.07253793627023697 = 0.004159925039857626 + 0.01 * 6.837800979614258
Epoch 820, val loss: 1.044540524482727
Epoch 830, training loss: 0.07232222706079483 = 0.004025586880743504 + 0.01 * 6.82966423034668
Epoch 830, val loss: 1.0495984554290771
Epoch 840, training loss: 0.0722392201423645 = 0.003898259252309799 + 0.01 * 6.834096908569336
Epoch 840, val loss: 1.0544846057891846
Epoch 850, training loss: 0.07202555984258652 = 0.0037778913974761963 + 0.01 * 6.824766635894775
Epoch 850, val loss: 1.0593762397766113
Epoch 860, training loss: 0.0719987004995346 = 0.003663751995190978 + 0.01 * 6.833495140075684
Epoch 860, val loss: 1.0641385316848755
Epoch 870, training loss: 0.07190072536468506 = 0.0035558638628572226 + 0.01 * 6.83448600769043
Epoch 870, val loss: 1.0686577558517456
Epoch 880, training loss: 0.07163173705339432 = 0.0034531618002802134 + 0.01 * 6.817857265472412
Epoch 880, val loss: 1.0732356309890747
Epoch 890, training loss: 0.07157327234745026 = 0.003355372231453657 + 0.01 * 6.8217902183532715
Epoch 890, val loss: 1.0776787996292114
Epoch 900, training loss: 0.07144302874803543 = 0.0032622930593788624 + 0.01 * 6.818073749542236
Epoch 900, val loss: 1.0819917917251587
Epoch 910, training loss: 0.07130265980958939 = 0.003173666773363948 + 0.01 * 6.812899589538574
Epoch 910, val loss: 1.0863070487976074
Epoch 920, training loss: 0.07138087600469589 = 0.0030891166534274817 + 0.01 * 6.829176425933838
Epoch 920, val loss: 1.0904483795166016
Epoch 930, training loss: 0.07117476314306259 = 0.0030085714533925056 + 0.01 * 6.816619396209717
Epoch 930, val loss: 1.0944879055023193
Epoch 940, training loss: 0.07097414135932922 = 0.002932133851572871 + 0.01 * 6.804201126098633
Epoch 940, val loss: 1.0984227657318115
Epoch 950, training loss: 0.07086874544620514 = 0.002859006868675351 + 0.01 * 6.800974369049072
Epoch 950, val loss: 1.1022692918777466
Epoch 960, training loss: 0.0707906112074852 = 0.0027890377677977085 + 0.01 * 6.8001580238342285
Epoch 960, val loss: 1.1061031818389893
Epoch 970, training loss: 0.07083185017108917 = 0.0027221287600696087 + 0.01 * 6.810972213745117
Epoch 970, val loss: 1.1098644733428955
Epoch 980, training loss: 0.07074642926454544 = 0.0026577082462608814 + 0.01 * 6.808872222900391
Epoch 980, val loss: 1.1135362386703491
Epoch 990, training loss: 0.07047373801469803 = 0.0025960144121199846 + 0.01 * 6.7877726554870605
Epoch 990, val loss: 1.1172159910202026
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.0259592533111572 = 1.9399909973144531 + 0.01 * 8.596837043762207
Epoch 0, val loss: 1.9367300271987915
Epoch 10, training loss: 2.0164055824279785 = 1.9304378032684326 + 0.01 * 8.596789360046387
Epoch 10, val loss: 1.927526593208313
Epoch 20, training loss: 2.0045928955078125 = 1.9186266660690308 + 0.01 * 8.5966215133667
Epoch 20, val loss: 1.9158052206039429
Epoch 30, training loss: 1.988059163093567 = 1.9020978212356567 + 0.01 * 8.596134185791016
Epoch 30, val loss: 1.8991215229034424
Epoch 40, training loss: 1.9637095928192139 = 1.8777719736099243 + 0.01 * 8.593765258789062
Epoch 40, val loss: 1.8745986223220825
Epoch 50, training loss: 1.929412603378296 = 1.8436477184295654 + 0.01 * 8.576486587524414
Epoch 50, val loss: 1.841278076171875
Epoch 60, training loss: 1.8889744281768799 = 1.804189920425415 + 0.01 * 8.47845458984375
Epoch 60, val loss: 1.8060787916183472
Epoch 70, training loss: 1.8494129180908203 = 1.7671347856521606 + 0.01 * 8.22780990600586
Epoch 70, val loss: 1.7757281064987183
Epoch 80, training loss: 1.8010711669921875 = 1.7207534313201904 + 0.01 * 8.031768798828125
Epoch 80, val loss: 1.7349777221679688
Epoch 90, training loss: 1.7315651178359985 = 1.6542627811431885 + 0.01 * 7.730235576629639
Epoch 90, val loss: 1.6752817630767822
Epoch 100, training loss: 1.6409177780151367 = 1.5652695894241333 + 0.01 * 7.564820289611816
Epoch 100, val loss: 1.5966053009033203
Epoch 110, training loss: 1.5364621877670288 = 1.4612752199172974 + 0.01 * 7.518699645996094
Epoch 110, val loss: 1.5080019235610962
Epoch 120, training loss: 1.4269375801086426 = 1.352091670036316 + 0.01 * 7.484585285186768
Epoch 120, val loss: 1.419213891029358
Epoch 130, training loss: 1.3157209157943726 = 1.2414119243621826 + 0.01 * 7.430902004241943
Epoch 130, val loss: 1.3324637413024902
Epoch 140, training loss: 1.204255223274231 = 1.1305370330810547 + 0.01 * 7.371816158294678
Epoch 140, val loss: 1.2478611469268799
Epoch 150, training loss: 1.0961226224899292 = 1.0228815078735352 + 0.01 * 7.324113845825195
Epoch 150, val loss: 1.1675775051116943
Epoch 160, training loss: 0.9951711297035217 = 0.9222319722175598 + 0.01 * 7.293917655944824
Epoch 160, val loss: 1.0937561988830566
Epoch 170, training loss: 0.9042755365371704 = 0.8316134214401245 + 0.01 * 7.266211986541748
Epoch 170, val loss: 1.0288079977035522
Epoch 180, training loss: 0.8253992199897766 = 0.7530385255813599 + 0.01 * 7.236067771911621
Epoch 180, val loss: 0.9743226170539856
Epoch 190, training loss: 0.7580975294113159 = 0.686000406742096 + 0.01 * 7.209715366363525
Epoch 190, val loss: 0.9304324388504028
Epoch 200, training loss: 0.6995485424995422 = 0.627638578414917 + 0.01 * 7.190995216369629
Epoch 200, val loss: 0.8951166868209839
Epoch 210, training loss: 0.6464337706565857 = 0.5746614933013916 + 0.01 * 7.177227020263672
Epoch 210, val loss: 0.8661154508590698
Epoch 220, training loss: 0.5965951681137085 = 0.5249049067497253 + 0.01 * 7.169027328491211
Epoch 220, val loss: 0.8420401215553284
Epoch 230, training loss: 0.5490292310714722 = 0.4773993492126465 + 0.01 * 7.16298770904541
Epoch 230, val loss: 0.8224448561668396
Epoch 240, training loss: 0.5034776926040649 = 0.43189936876296997 + 0.01 * 7.157833576202393
Epoch 240, val loss: 0.8073538541793823
Epoch 250, training loss: 0.4601520001888275 = 0.38861528038978577 + 0.01 * 7.153672695159912
Epoch 250, val loss: 0.796652615070343
Epoch 260, training loss: 0.4194526672363281 = 0.347968727350235 + 0.01 * 7.1483941078186035
Epoch 260, val loss: 0.7900290489196777
Epoch 270, training loss: 0.38178393244743347 = 0.31030628085136414 + 0.01 * 7.14776611328125
Epoch 270, val loss: 0.7871662974357605
Epoch 280, training loss: 0.34720203280448914 = 0.2757914960384369 + 0.01 * 7.141054630279541
Epoch 280, val loss: 0.7876967787742615
Epoch 290, training loss: 0.31581324338912964 = 0.24444591999053955 + 0.01 * 7.13673210144043
Epoch 290, val loss: 0.7912135124206543
Epoch 300, training loss: 0.28762364387512207 = 0.21629618108272552 + 0.01 * 7.132746696472168
Epoch 300, val loss: 0.7974116206169128
Epoch 310, training loss: 0.26259493827819824 = 0.191305011510849 + 0.01 * 7.128992557525635
Epoch 310, val loss: 0.8061277866363525
Epoch 320, training loss: 0.24059200286865234 = 0.1693401038646698 + 0.01 * 7.125190258026123
Epoch 320, val loss: 0.8171330094337463
Epoch 330, training loss: 0.2213696837425232 = 0.1501622200012207 + 0.01 * 7.120746612548828
Epoch 330, val loss: 0.8301653265953064
Epoch 340, training loss: 0.2046397179365158 = 0.13347773253917694 + 0.01 * 7.116199016571045
Epoch 340, val loss: 0.8448703289031982
Epoch 350, training loss: 0.1901070773601532 = 0.11897336691617966 + 0.01 * 7.113371849060059
Epoch 350, val loss: 0.8608468174934387
Epoch 360, training loss: 0.17740127444267273 = 0.10634446144104004 + 0.01 * 7.105681419372559
Epoch 360, val loss: 0.8777095675468445
Epoch 370, training loss: 0.16632968187332153 = 0.09531698375940323 + 0.01 * 7.1012701988220215
Epoch 370, val loss: 0.8951154947280884
Epoch 380, training loss: 0.15665581822395325 = 0.08566175401210785 + 0.01 * 7.099407196044922
Epoch 380, val loss: 0.9127722978591919
Epoch 390, training loss: 0.14806430041790009 = 0.07717421650886536 + 0.01 * 7.089008331298828
Epoch 390, val loss: 0.9305327534675598
Epoch 400, training loss: 0.14051008224487305 = 0.06968477368354797 + 0.01 * 7.0825300216674805
Epoch 400, val loss: 0.9482100605964661
Epoch 410, training loss: 0.13381043076515198 = 0.06306206434965134 + 0.01 * 7.0748372077941895
Epoch 410, val loss: 0.9657500982284546
Epoch 420, training loss: 0.12786780297756195 = 0.05719375237822533 + 0.01 * 7.067405700683594
Epoch 420, val loss: 0.9829956293106079
Epoch 430, training loss: 0.12266802787780762 = 0.051987625658512115 + 0.01 * 7.068040370941162
Epoch 430, val loss: 0.9999734163284302
Epoch 440, training loss: 0.11783411353826523 = 0.04736177623271942 + 0.01 * 7.047234058380127
Epoch 440, val loss: 1.0165618658065796
Epoch 450, training loss: 0.11374975740909576 = 0.04324325919151306 + 0.01 * 7.050649642944336
Epoch 450, val loss: 1.0327943563461304
Epoch 460, training loss: 0.1100730299949646 = 0.039572540670633316 + 0.01 * 7.050049781799316
Epoch 460, val loss: 1.0485732555389404
Epoch 470, training loss: 0.10663731396198273 = 0.03629766032099724 + 0.01 * 7.033965110778809
Epoch 470, val loss: 1.0639947652816772
Epoch 480, training loss: 0.10358184576034546 = 0.03336929529905319 + 0.01 * 7.021255016326904
Epoch 480, val loss: 1.0789436101913452
Epoch 490, training loss: 0.10096997767686844 = 0.030748963356018066 + 0.01 * 7.022101402282715
Epoch 490, val loss: 1.0934847593307495
Epoch 500, training loss: 0.09853167831897736 = 0.028400301933288574 + 0.01 * 7.0131378173828125
Epoch 500, val loss: 1.107586145401001
Epoch 510, training loss: 0.096290722489357 = 0.026289384812116623 + 0.01 * 7.000133991241455
Epoch 510, val loss: 1.1213221549987793
Epoch 520, training loss: 0.09437534213066101 = 0.024389227852225304 + 0.01 * 6.9986114501953125
Epoch 520, val loss: 1.13461434841156
Epoch 530, training loss: 0.09267590939998627 = 0.022678595036268234 + 0.01 * 6.999731540679932
Epoch 530, val loss: 1.147496223449707
Epoch 540, training loss: 0.09095074981451035 = 0.02113301306962967 + 0.01 * 6.981773853302002
Epoch 540, val loss: 1.1599920988082886
Epoch 550, training loss: 0.08954089879989624 = 0.019732721149921417 + 0.01 * 6.980818271636963
Epoch 550, val loss: 1.1721760034561157
Epoch 560, training loss: 0.08814868330955505 = 0.018462615087628365 + 0.01 * 6.968606948852539
Epoch 560, val loss: 1.1839675903320312
Epoch 570, training loss: 0.08715284615755081 = 0.017308548092842102 + 0.01 * 6.984429836273193
Epoch 570, val loss: 1.1954004764556885
Epoch 580, training loss: 0.0859377533197403 = 0.016258850693702698 + 0.01 * 6.96789026260376
Epoch 580, val loss: 1.2064929008483887
Epoch 590, training loss: 0.08489255607128143 = 0.01530105248093605 + 0.01 * 6.959150314331055
Epoch 590, val loss: 1.2172447443008423
Epoch 600, training loss: 0.08409284800291061 = 0.014424764551222324 + 0.01 * 6.966808795928955
Epoch 600, val loss: 1.2276560068130493
Epoch 610, training loss: 0.0832458958029747 = 0.013623407110571861 + 0.01 * 6.962248802185059
Epoch 610, val loss: 1.2377697229385376
Epoch 620, training loss: 0.08241007477045059 = 0.012888061814010143 + 0.01 * 6.952201843261719
Epoch 620, val loss: 1.2475123405456543
Epoch 630, training loss: 0.0816812589764595 = 0.012211759574711323 + 0.01 * 6.946949481964111
Epoch 630, val loss: 1.2570290565490723
Epoch 640, training loss: 0.08102305233478546 = 0.011589258909225464 + 0.01 * 6.9433794021606445
Epoch 640, val loss: 1.2662107944488525
Epoch 650, training loss: 0.08051125705242157 = 0.01101488433778286 + 0.01 * 6.949636936187744
Epoch 650, val loss: 1.2751530408859253
Epoch 660, training loss: 0.07974053919315338 = 0.010484086349606514 + 0.01 * 6.925645351409912
Epoch 660, val loss: 1.2837949991226196
Epoch 670, training loss: 0.07922884076833725 = 0.009992393665015697 + 0.01 * 6.92364501953125
Epoch 670, val loss: 1.2922561168670654
Epoch 680, training loss: 0.0787656381726265 = 0.00953610334545374 + 0.01 * 6.9229536056518555
Epoch 680, val loss: 1.3004354238510132
Epoch 690, training loss: 0.07824384421110153 = 0.009113190695643425 + 0.01 * 6.913064956665039
Epoch 690, val loss: 1.3083003759384155
Epoch 700, training loss: 0.07790039479732513 = 0.00871998816728592 + 0.01 * 6.918040752410889
Epoch 700, val loss: 1.3159689903259277
Epoch 710, training loss: 0.07746782153844833 = 0.008353342302143574 + 0.01 * 6.911448001861572
Epoch 710, val loss: 1.3234553337097168
Epoch 720, training loss: 0.07696938514709473 = 0.008011233992874622 + 0.01 * 6.895814895629883
Epoch 720, val loss: 1.3306881189346313
Epoch 730, training loss: 0.07683146744966507 = 0.007691551931202412 + 0.01 * 6.913991451263428
Epoch 730, val loss: 1.3377162218093872
Epoch 740, training loss: 0.07635142654180527 = 0.0073924255557358265 + 0.01 * 6.895900249481201
Epoch 740, val loss: 1.3446420431137085
Epoch 750, training loss: 0.07611975073814392 = 0.007111778017133474 + 0.01 * 6.900797367095947
Epoch 750, val loss: 1.3513379096984863
Epoch 760, training loss: 0.07583581656217575 = 0.006848643533885479 + 0.01 * 6.898717403411865
Epoch 760, val loss: 1.357825756072998
Epoch 770, training loss: 0.0753534585237503 = 0.006601743400096893 + 0.01 * 6.875171661376953
Epoch 770, val loss: 1.364195704460144
Epoch 780, training loss: 0.07517866790294647 = 0.006369404960423708 + 0.01 * 6.880926132202148
Epoch 780, val loss: 1.3703848123550415
Epoch 790, training loss: 0.07527973502874374 = 0.006150451023131609 + 0.01 * 6.912928581237793
Epoch 790, val loss: 1.3763877153396606
Epoch 800, training loss: 0.07458728551864624 = 0.0059441751800477505 + 0.01 * 6.864311218261719
Epoch 800, val loss: 1.3822721242904663
Epoch 810, training loss: 0.0744270533323288 = 0.005749590694904327 + 0.01 * 6.867746353149414
Epoch 810, val loss: 1.3879557847976685
Epoch 820, training loss: 0.07441692799329758 = 0.00556581374257803 + 0.01 * 6.8851118087768555
Epoch 820, val loss: 1.3935062885284424
Epoch 830, training loss: 0.07383792847394943 = 0.005392070394009352 + 0.01 * 6.844586372375488
Epoch 830, val loss: 1.3989790678024292
Epoch 840, training loss: 0.073826864361763 = 0.005227455869317055 + 0.01 * 6.859941005706787
Epoch 840, val loss: 1.404251217842102
Epoch 850, training loss: 0.07362788170576096 = 0.005071350838989019 + 0.01 * 6.855653285980225
Epoch 850, val loss: 1.4093961715698242
Epoch 860, training loss: 0.07346074283123016 = 0.004923388361930847 + 0.01 * 6.853735446929932
Epoch 860, val loss: 1.4144787788391113
Epoch 870, training loss: 0.07335780560970306 = 0.004782698582857847 + 0.01 * 6.857510566711426
Epoch 870, val loss: 1.4193388223648071
Epoch 880, training loss: 0.07323408871889114 = 0.004649257753044367 + 0.01 * 6.85848331451416
Epoch 880, val loss: 1.4241496324539185
Epoch 890, training loss: 0.07298185676336288 = 0.004522156901657581 + 0.01 * 6.8459696769714355
Epoch 890, val loss: 1.4288281202316284
Epoch 900, training loss: 0.07261885702610016 = 0.00440117297694087 + 0.01 * 6.8217692375183105
Epoch 900, val loss: 1.4333769083023071
Epoch 910, training loss: 0.07284796983003616 = 0.004285703878849745 + 0.01 * 6.856227397918701
Epoch 910, val loss: 1.437821865081787
Epoch 920, training loss: 0.07265893369913101 = 0.004175698850303888 + 0.01 * 6.848323822021484
Epoch 920, val loss: 1.442204236984253
Epoch 930, training loss: 0.07224144786596298 = 0.004070778377354145 + 0.01 * 6.817066669464111
Epoch 930, val loss: 1.4463831186294556
Epoch 940, training loss: 0.07223591208457947 = 0.003970786463469267 + 0.01 * 6.826512336730957
Epoch 940, val loss: 1.4504796266555786
Epoch 950, training loss: 0.07226303219795227 = 0.0038751899264752865 + 0.01 * 6.838784694671631
Epoch 950, val loss: 1.454542875289917
Epoch 960, training loss: 0.0718899816274643 = 0.0037837130948901176 + 0.01 * 6.810627460479736
Epoch 960, val loss: 1.4585002660751343
Epoch 970, training loss: 0.0719047486782074 = 0.0036961203441023827 + 0.01 * 6.820862770080566
Epoch 970, val loss: 1.462381362915039
Epoch 980, training loss: 0.07177890837192535 = 0.0036121548619121313 + 0.01 * 6.816675662994385
Epoch 980, val loss: 1.4661400318145752
Epoch 990, training loss: 0.07152523845434189 = 0.0035317910369485617 + 0.01 * 6.799344539642334
Epoch 990, val loss: 1.4698461294174194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8365840801265156
The final CL Acc:0.81111, 0.00302, The final GNN Acc:0.83729, 0.00179
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10538])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0190224647521973 = 1.9330542087554932 + 0.01 * 8.596814155578613
Epoch 0, val loss: 1.930540919303894
Epoch 10, training loss: 2.0095527172088623 = 1.9235849380493164 + 0.01 * 8.59677791595459
Epoch 10, val loss: 1.920938491821289
Epoch 20, training loss: 1.9981919527053833 = 1.9122258424758911 + 0.01 * 8.596606254577637
Epoch 20, val loss: 1.9092563390731812
Epoch 30, training loss: 1.9824763536453247 = 1.896514892578125 + 0.01 * 8.596141815185547
Epoch 30, val loss: 1.8931257724761963
Epoch 40, training loss: 1.9596197605133057 = 1.8736774921417236 + 0.01 * 8.594230651855469
Epoch 40, val loss: 1.8700348138809204
Epoch 50, training loss: 1.9281208515167236 = 1.842302918434143 + 0.01 * 8.581794738769531
Epoch 50, val loss: 1.8397064208984375
Epoch 60, training loss: 1.8927181959152222 = 1.8075677156448364 + 0.01 * 8.515045166015625
Epoch 60, val loss: 1.8093833923339844
Epoch 70, training loss: 1.8593772649765015 = 1.7767032384872437 + 0.01 * 8.267407417297363
Epoch 70, val loss: 1.7840681076049805
Epoch 80, training loss: 1.8181228637695312 = 1.737510085105896 + 0.01 * 8.061272621154785
Epoch 80, val loss: 1.7492599487304688
Epoch 90, training loss: 1.7598098516464233 = 1.6823031902313232 + 0.01 * 7.750661849975586
Epoch 90, val loss: 1.7009999752044678
Epoch 100, training loss: 1.6835551261901855 = 1.6073830127716064 + 0.01 * 7.617214202880859
Epoch 100, val loss: 1.6382807493209839
Epoch 110, training loss: 1.5920380353927612 = 1.516010046005249 + 0.01 * 7.602797985076904
Epoch 110, val loss: 1.5627597570419312
Epoch 120, training loss: 1.492038607597351 = 1.4161850214004517 + 0.01 * 7.5853590965271
Epoch 120, val loss: 1.4815503358840942
Epoch 130, training loss: 1.3872069120407104 = 1.3115097284317017 + 0.01 * 7.569723606109619
Epoch 130, val loss: 1.396315336227417
Epoch 140, training loss: 1.2777001857757568 = 1.2022974491119385 + 0.01 * 7.540278434753418
Epoch 140, val loss: 1.3075788021087646
Epoch 150, training loss: 1.1660646200180054 = 1.091180682182312 + 0.01 * 7.488396644592285
Epoch 150, val loss: 1.2177815437316895
Epoch 160, training loss: 1.0580989122390747 = 0.9838387966156006 + 0.01 * 7.426011085510254
Epoch 160, val loss: 1.1333969831466675
Epoch 170, training loss: 0.960354208946228 = 0.886488676071167 + 0.01 * 7.386551380157471
Epoch 170, val loss: 1.060118317604065
Epoch 180, training loss: 0.8763009309768677 = 0.8027579188346863 + 0.01 * 7.354302406311035
Epoch 180, val loss: 1.0008103847503662
Epoch 190, training loss: 0.8054730892181396 = 0.7321468591690063 + 0.01 * 7.332620143890381
Epoch 190, val loss: 0.9549515247344971
Epoch 200, training loss: 0.7445430755615234 = 0.6714843511581421 + 0.01 * 7.305874347686768
Epoch 200, val loss: 0.9197472929954529
Epoch 210, training loss: 0.690147340297699 = 0.6173932552337646 + 0.01 * 7.27540922164917
Epoch 210, val loss: 0.8923810720443726
Epoch 220, training loss: 0.6399074196815491 = 0.5673987865447998 + 0.01 * 7.250864505767822
Epoch 220, val loss: 0.8708277344703674
Epoch 230, training loss: 0.5922812819480896 = 0.5200620889663696 + 0.01 * 7.221919536590576
Epoch 230, val loss: 0.8536044359207153
Epoch 240, training loss: 0.5468810796737671 = 0.47485679388046265 + 0.01 * 7.20242977142334
Epoch 240, val loss: 0.8400137424468994
Epoch 250, training loss: 0.5036893486976624 = 0.4318530559539795 + 0.01 * 7.183629035949707
Epoch 250, val loss: 0.830115795135498
Epoch 260, training loss: 0.4630992114543915 = 0.3912963271141052 + 0.01 * 7.180288791656494
Epoch 260, val loss: 0.8242285251617432
Epoch 270, training loss: 0.4249902069568634 = 0.3534094989299774 + 0.01 * 7.158070087432861
Epoch 270, val loss: 0.8227832913398743
Epoch 280, training loss: 0.3895699977874756 = 0.3181339204311371 + 0.01 * 7.143608093261719
Epoch 280, val loss: 0.8256579637527466
Epoch 290, training loss: 0.3566696047782898 = 0.2852858006954193 + 0.01 * 7.138381004333496
Epoch 290, val loss: 0.8324337601661682
Epoch 300, training loss: 0.32599350810050964 = 0.25481173396110535 + 0.01 * 7.11817741394043
Epoch 300, val loss: 0.8426089286804199
Epoch 310, training loss: 0.2978929877281189 = 0.22670216858386993 + 0.01 * 7.119082927703857
Epoch 310, val loss: 0.8558171987533569
Epoch 320, training loss: 0.2721348702907562 = 0.20109298825263977 + 0.01 * 7.104188442230225
Epoch 320, val loss: 0.8716699481010437
Epoch 330, training loss: 0.248928502202034 = 0.1780126690864563 + 0.01 * 7.091583728790283
Epoch 330, val loss: 0.8898218274116516
Epoch 340, training loss: 0.22816091775894165 = 0.1573774367570877 + 0.01 * 7.078347682952881
Epoch 340, val loss: 0.910119891166687
Epoch 350, training loss: 0.20987586677074432 = 0.13907161355018616 + 0.01 * 7.08042573928833
Epoch 350, val loss: 0.9321069717407227
Epoch 360, training loss: 0.193757563829422 = 0.12299397587776184 + 0.01 * 7.076358318328857
Epoch 360, val loss: 0.9553483724594116
Epoch 370, training loss: 0.17950254678726196 = 0.10890897363424301 + 0.01 * 7.059356689453125
Epoch 370, val loss: 0.979624330997467
Epoch 380, training loss: 0.16707733273506165 = 0.0966082215309143 + 0.01 * 7.04691219329834
Epoch 380, val loss: 1.0046496391296387
Epoch 390, training loss: 0.1564214527606964 = 0.08588837832212448 + 0.01 * 7.053307056427002
Epoch 390, val loss: 1.0300740003585815
Epoch 400, training loss: 0.14710178971290588 = 0.07658341526985168 + 0.01 * 7.051838397979736
Epoch 400, val loss: 1.0555180311203003
Epoch 410, training loss: 0.13877737522125244 = 0.06849759817123413 + 0.01 * 7.027978420257568
Epoch 410, val loss: 1.080899715423584
Epoch 420, training loss: 0.13165205717086792 = 0.061450012028217316 + 0.01 * 7.020204067230225
Epoch 420, val loss: 1.1060675382614136
Epoch 430, training loss: 0.12563297152519226 = 0.05529235303401947 + 0.01 * 7.034061431884766
Epoch 430, val loss: 1.1309999227523804
Epoch 440, training loss: 0.12003661692142487 = 0.049913834780454636 + 0.01 * 7.012278079986572
Epoch 440, val loss: 1.1554416418075562
Epoch 450, training loss: 0.11531493067741394 = 0.04520166665315628 + 0.01 * 7.011326789855957
Epoch 450, val loss: 1.179533839225769
Epoch 460, training loss: 0.11107093095779419 = 0.04106255620718002 + 0.01 * 7.000837326049805
Epoch 460, val loss: 1.2031550407409668
Epoch 470, training loss: 0.10729361325502396 = 0.03741750866174698 + 0.01 * 6.987610340118408
Epoch 470, val loss: 1.226257562637329
Epoch 480, training loss: 0.1042126715183258 = 0.034198228269815445 + 0.01 * 7.001443862915039
Epoch 480, val loss: 1.2488394975662231
Epoch 490, training loss: 0.10120260715484619 = 0.031351253390312195 + 0.01 * 6.985135555267334
Epoch 490, val loss: 1.2709335088729858
Epoch 500, training loss: 0.0986633226275444 = 0.028824135661125183 + 0.01 * 6.983919143676758
Epoch 500, val loss: 1.2924256324768066
Epoch 510, training loss: 0.09623662382364273 = 0.026573849841952324 + 0.01 * 6.966277599334717
Epoch 510, val loss: 1.3133177757263184
Epoch 520, training loss: 0.09447203576564789 = 0.02456551231443882 + 0.01 * 6.990652561187744
Epoch 520, val loss: 1.3336237668991089
Epoch 530, training loss: 0.09226252138614655 = 0.022770199924707413 + 0.01 * 6.9492316246032715
Epoch 530, val loss: 1.3534024953842163
Epoch 540, training loss: 0.0907793715596199 = 0.0211618784815073 + 0.01 * 6.96174955368042
Epoch 540, val loss: 1.3725783824920654
Epoch 550, training loss: 0.08929035812616348 = 0.019719690084457397 + 0.01 * 6.957067012786865
Epoch 550, val loss: 1.3910739421844482
Epoch 560, training loss: 0.08784155547618866 = 0.018419312313199043 + 0.01 * 6.942224502563477
Epoch 560, val loss: 1.4089913368225098
Epoch 570, training loss: 0.08655788749456406 = 0.0172421857714653 + 0.01 * 6.931570529937744
Epoch 570, val loss: 1.426470160484314
Epoch 580, training loss: 0.08556607365608215 = 0.016175538301467896 + 0.01 * 6.939053535461426
Epoch 580, val loss: 1.44327974319458
Epoch 590, training loss: 0.08471736311912537 = 0.015206844545900822 + 0.01 * 6.951052188873291
Epoch 590, val loss: 1.4597116708755493
Epoch 600, training loss: 0.08345793932676315 = 0.014325642958283424 + 0.01 * 6.913229942321777
Epoch 600, val loss: 1.4755762815475464
Epoch 610, training loss: 0.08269147574901581 = 0.013521438464522362 + 0.01 * 6.917003631591797
Epoch 610, val loss: 1.490866780281067
Epoch 620, training loss: 0.08188982307910919 = 0.012785423547029495 + 0.01 * 6.910440444946289
Epoch 620, val loss: 1.5057519674301147
Epoch 630, training loss: 0.08114810287952423 = 0.01211005263030529 + 0.01 * 6.903804779052734
Epoch 630, val loss: 1.5201774835586548
Epoch 640, training loss: 0.08052951842546463 = 0.011489695869386196 + 0.01 * 6.903982162475586
Epoch 640, val loss: 1.5341254472732544
Epoch 650, training loss: 0.0798623114824295 = 0.010918492451310158 + 0.01 * 6.894382476806641
Epoch 650, val loss: 1.5477529764175415
Epoch 660, training loss: 0.07922561466693878 = 0.010391113348305225 + 0.01 * 6.883450031280518
Epoch 660, val loss: 1.5608645677566528
Epoch 670, training loss: 0.07888631522655487 = 0.009903375059366226 + 0.01 * 6.898294448852539
Epoch 670, val loss: 1.573570728302002
Epoch 680, training loss: 0.07822185754776001 = 0.009451142512261868 + 0.01 * 6.877071857452393
Epoch 680, val loss: 1.5860410928726196
Epoch 690, training loss: 0.07781291007995605 = 0.009031499736011028 + 0.01 * 6.878141403198242
Epoch 690, val loss: 1.5980714559555054
Epoch 700, training loss: 0.07793796807527542 = 0.00864131934940815 + 0.01 * 6.929665565490723
Epoch 700, val loss: 1.6097233295440674
Epoch 710, training loss: 0.0768817812204361 = 0.008279185742139816 + 0.01 * 6.860259056091309
Epoch 710, val loss: 1.6210650205612183
Epoch 720, training loss: 0.07664740085601807 = 0.007941469550132751 + 0.01 * 6.870593070983887
Epoch 720, val loss: 1.632049798965454
Epoch 730, training loss: 0.0761798545718193 = 0.0076257577165961266 + 0.01 * 6.855409622192383
Epoch 730, val loss: 1.642748475074768
Epoch 740, training loss: 0.07600507140159607 = 0.0073301661759614944 + 0.01 * 6.867490768432617
Epoch 740, val loss: 1.653188705444336
Epoch 750, training loss: 0.07562319934368134 = 0.0070533426478505135 + 0.01 * 6.856985569000244
Epoch 750, val loss: 1.66323983669281
Epoch 760, training loss: 0.0752604752779007 = 0.00679342495277524 + 0.01 * 6.846705436706543
Epoch 760, val loss: 1.6731315851211548
Epoch 770, training loss: 0.07498782873153687 = 0.006549424026161432 + 0.01 * 6.843840599060059
Epoch 770, val loss: 1.6827107667922974
Epoch 780, training loss: 0.07473767548799515 = 0.006319806911051273 + 0.01 * 6.841787338256836
Epoch 780, val loss: 1.6920089721679688
Epoch 790, training loss: 0.07466308772563934 = 0.006103546358644962 + 0.01 * 6.855954647064209
Epoch 790, val loss: 1.701033353805542
Epoch 800, training loss: 0.07429715991020203 = 0.005899590440094471 + 0.01 * 6.839756965637207
Epoch 800, val loss: 1.70998215675354
Epoch 810, training loss: 0.07427298277616501 = 0.005706965457648039 + 0.01 * 6.856601715087891
Epoch 810, val loss: 1.7184869050979614
Epoch 820, training loss: 0.07386945188045502 = 0.0055251880548894405 + 0.01 * 6.8344268798828125
Epoch 820, val loss: 1.727008581161499
Epoch 830, training loss: 0.07359301298856735 = 0.005353036802262068 + 0.01 * 6.823997974395752
Epoch 830, val loss: 1.7351292371749878
Epoch 840, training loss: 0.07343335449695587 = 0.0051902420818805695 + 0.01 * 6.824310779571533
Epoch 840, val loss: 1.7431480884552002
Epoch 850, training loss: 0.07357568293809891 = 0.00503568584099412 + 0.01 * 6.854000091552734
Epoch 850, val loss: 1.7509311437606812
Epoch 860, training loss: 0.07311684638261795 = 0.004889010451734066 + 0.01 * 6.822783470153809
Epoch 860, val loss: 1.7586169242858887
Epoch 870, training loss: 0.07298756390810013 = 0.004749867599457502 + 0.01 * 6.823770046234131
Epoch 870, val loss: 1.7660478353500366
Epoch 880, training loss: 0.07298949360847473 = 0.004617481492459774 + 0.01 * 6.837201118469238
Epoch 880, val loss: 1.773354411125183
Epoch 890, training loss: 0.07256117463111877 = 0.004491829313337803 + 0.01 * 6.806934833526611
Epoch 890, val loss: 1.7803847789764404
Epoch 900, training loss: 0.07272037863731384 = 0.004371926188468933 + 0.01 * 6.834845542907715
Epoch 900, val loss: 1.7873471975326538
Epoch 910, training loss: 0.07240346819162369 = 0.00425775395706296 + 0.01 * 6.814571380615234
Epoch 910, val loss: 1.794015884399414
Epoch 920, training loss: 0.07218650728464127 = 0.004148912616074085 + 0.01 * 6.8037590980529785
Epoch 920, val loss: 1.8005497455596924
Epoch 930, training loss: 0.0719698965549469 = 0.004044999368488789 + 0.01 * 6.792489528656006
Epoch 930, val loss: 1.806999683380127
Epoch 940, training loss: 0.07202528417110443 = 0.003945739008486271 + 0.01 * 6.807954788208008
Epoch 940, val loss: 1.8133093118667603
Epoch 950, training loss: 0.0717402994632721 = 0.0038509764708578587 + 0.01 * 6.7889323234558105
Epoch 950, val loss: 1.8194156885147095
Epoch 960, training loss: 0.07183121889829636 = 0.0037603466771543026 + 0.01 * 6.807087421417236
Epoch 960, val loss: 1.82535982131958
Epoch 970, training loss: 0.07162415236234665 = 0.003673554165288806 + 0.01 * 6.795060157775879
Epoch 970, val loss: 1.8313133716583252
Epoch 980, training loss: 0.0716320350766182 = 0.0035904748365283012 + 0.01 * 6.804156303405762
Epoch 980, val loss: 1.8369585275650024
Epoch 990, training loss: 0.07143129408359528 = 0.0035108928568661213 + 0.01 * 6.7920403480529785
Epoch 990, val loss: 1.842597246170044
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 2.007657051086426 = 1.9216886758804321 + 0.01 * 8.596826553344727
Epoch 0, val loss: 1.920897364616394
Epoch 10, training loss: 1.9987821578979492 = 1.9128140211105347 + 0.01 * 8.596809387207031
Epoch 10, val loss: 1.9116120338439941
Epoch 20, training loss: 1.9884192943572998 = 1.9024524688720703 + 0.01 * 8.596686363220215
Epoch 20, val loss: 1.9005253314971924
Epoch 30, training loss: 1.9744809865951538 = 1.888517141342163 + 0.01 * 8.59638786315918
Epoch 30, val loss: 1.885502815246582
Epoch 40, training loss: 1.9546103477478027 = 1.8686566352844238 + 0.01 * 8.595370292663574
Epoch 40, val loss: 1.8642725944519043
Epoch 50, training loss: 1.9274905920028687 = 1.8415974378585815 + 0.01 * 8.589320182800293
Epoch 50, val loss: 1.83638596534729
Epoch 60, training loss: 1.8968493938446045 = 1.8113497495651245 + 0.01 * 8.549960136413574
Epoch 60, val loss: 1.807983160018921
Epoch 70, training loss: 1.8668206930160522 = 1.7834770679473877 + 0.01 * 8.334359169006348
Epoch 70, val loss: 1.784930944442749
Epoch 80, training loss: 1.8286163806915283 = 1.7472217082977295 + 0.01 * 8.139469146728516
Epoch 80, val loss: 1.7546169757843018
Epoch 90, training loss: 1.7740263938903809 = 1.6946358680725098 + 0.01 * 7.939047813415527
Epoch 90, val loss: 1.7101629972457886
Epoch 100, training loss: 1.6989740133285522 = 1.6214946508407593 + 0.01 * 7.747936248779297
Epoch 100, val loss: 1.6488484144210815
Epoch 110, training loss: 1.606299638748169 = 1.5317257642745972 + 0.01 * 7.4573845863342285
Epoch 110, val loss: 1.5747696161270142
Epoch 120, training loss: 1.5086172819137573 = 1.4348899126052856 + 0.01 * 7.3727335929870605
Epoch 120, val loss: 1.4980454444885254
Epoch 130, training loss: 1.41021728515625 = 1.3370622396469116 + 0.01 * 7.315504550933838
Epoch 130, val loss: 1.4242963790893555
Epoch 140, training loss: 1.3137760162353516 = 1.2409857511520386 + 0.01 * 7.279025554656982
Epoch 140, val loss: 1.356711745262146
Epoch 150, training loss: 1.2226722240447998 = 1.1501989364624023 + 0.01 * 7.247328281402588
Epoch 150, val loss: 1.296116828918457
Epoch 160, training loss: 1.1393147706985474 = 1.0671641826629639 + 0.01 * 7.215058326721191
Epoch 160, val loss: 1.2423466444015503
Epoch 170, training loss: 1.0635985136032104 = 0.9917325973510742 + 0.01 * 7.186594486236572
Epoch 170, val loss: 1.1939246654510498
Epoch 180, training loss: 0.9938241839408875 = 0.9221332669258118 + 0.01 * 7.169093608856201
Epoch 180, val loss: 1.1491755247116089
Epoch 190, training loss: 0.9272124171257019 = 0.8556133508682251 + 0.01 * 7.159907817840576
Epoch 190, val loss: 1.1057323217391968
Epoch 200, training loss: 0.8606216311454773 = 0.7890880703926086 + 0.01 * 7.15335750579834
Epoch 200, val loss: 1.0613880157470703
Epoch 210, training loss: 0.792163610458374 = 0.7206852436065674 + 0.01 * 7.147838592529297
Epoch 210, val loss: 1.0146175622940063
Epoch 220, training loss: 0.7219761610031128 = 0.6505618095397949 + 0.01 * 7.141432762145996
Epoch 220, val loss: 0.9658334255218506
Epoch 230, training loss: 0.6519423723220825 = 0.5805989503860474 + 0.01 * 7.134339332580566
Epoch 230, val loss: 0.9168928265571594
Epoch 240, training loss: 0.5842772126197815 = 0.512980580329895 + 0.01 * 7.129664897918701
Epoch 240, val loss: 0.8710761070251465
Epoch 250, training loss: 0.5206679105758667 = 0.44946420192718506 + 0.01 * 7.120372295379639
Epoch 250, val loss: 0.8308216333389282
Epoch 260, training loss: 0.4622851610183716 = 0.3911550045013428 + 0.01 * 7.1130170822143555
Epoch 260, val loss: 0.7978372573852539
Epoch 270, training loss: 0.40970951318740845 = 0.3386421501636505 + 0.01 * 7.1067352294921875
Epoch 270, val loss: 0.7719632983207703
Epoch 280, training loss: 0.3631799817085266 = 0.2921624779701233 + 0.01 * 7.101751327514648
Epoch 280, val loss: 0.7524423003196716
Epoch 290, training loss: 0.3225322663784027 = 0.2515483498573303 + 0.01 * 7.098392009735107
Epoch 290, val loss: 0.7382379770278931
Epoch 300, training loss: 0.28737160563468933 = 0.2164257913827896 + 0.01 * 7.094581604003906
Epoch 300, val loss: 0.7284736633300781
Epoch 310, training loss: 0.2572898268699646 = 0.1863170862197876 + 0.01 * 7.0972723960876465
Epoch 310, val loss: 0.7225987911224365
Epoch 320, training loss: 0.23158201575279236 = 0.16069361567497253 + 0.01 * 7.088840961456299
Epoch 320, val loss: 0.719958484172821
Epoch 330, training loss: 0.2098250538110733 = 0.13895533978939056 + 0.01 * 7.086971759796143
Epoch 330, val loss: 0.7200596928596497
Epoch 340, training loss: 0.19136455655097961 = 0.12051182985305786 + 0.01 * 7.085273265838623
Epoch 340, val loss: 0.7224633097648621
Epoch 350, training loss: 0.1756981909275055 = 0.10485439002513885 + 0.01 * 7.084381103515625
Epoch 350, val loss: 0.7267313003540039
Epoch 360, training loss: 0.16235759854316711 = 0.09154604375362396 + 0.01 * 7.081155300140381
Epoch 360, val loss: 0.7324703931808472
Epoch 370, training loss: 0.1509825438261032 = 0.08020436763763428 + 0.01 * 7.077817440032959
Epoch 370, val loss: 0.7394223213195801
Epoch 380, training loss: 0.14126089215278625 = 0.07050775736570358 + 0.01 * 7.075313568115234
Epoch 380, val loss: 0.7473322749137878
Epoch 390, training loss: 0.13293111324310303 = 0.06219933554530144 + 0.01 * 7.073177337646484
Epoch 390, val loss: 0.7560030221939087
Epoch 400, training loss: 0.1257772147655487 = 0.05506904423236847 + 0.01 * 7.070816993713379
Epoch 400, val loss: 0.7652396559715271
Epoch 410, training loss: 0.11962848901748657 = 0.04893931373953819 + 0.01 * 7.068917274475098
Epoch 410, val loss: 0.774904727935791
Epoch 420, training loss: 0.11432914435863495 = 0.04365871101617813 + 0.01 * 7.067043781280518
Epoch 420, val loss: 0.7848621606826782
Epoch 430, training loss: 0.10975517332553864 = 0.0391041599214077 + 0.01 * 7.065101146697998
Epoch 430, val loss: 0.7949562072753906
Epoch 440, training loss: 0.10577597469091415 = 0.03516802936792374 + 0.01 * 7.060794830322266
Epoch 440, val loss: 0.8051396608352661
Epoch 450, training loss: 0.10234389454126358 = 0.03175777196884155 + 0.01 * 7.05861234664917
Epoch 450, val loss: 0.8153159618377686
Epoch 460, training loss: 0.09932886064052582 = 0.028795434162020683 + 0.01 * 7.053342342376709
Epoch 460, val loss: 0.8254460692405701
Epoch 470, training loss: 0.09672911465167999 = 0.026214322075247765 + 0.01 * 7.051479339599609
Epoch 470, val loss: 0.8354666829109192
Epoch 480, training loss: 0.0946153998374939 = 0.02396203950047493 + 0.01 * 7.065335750579834
Epoch 480, val loss: 0.8453056216239929
Epoch 490, training loss: 0.09246838092803955 = 0.0219900943338871 + 0.01 * 7.047828197479248
Epoch 490, val loss: 0.8549438118934631
Epoch 500, training loss: 0.09069237112998962 = 0.020253553986549377 + 0.01 * 7.043881893157959
Epoch 500, val loss: 0.8643442392349243
Epoch 510, training loss: 0.08910811692476273 = 0.018716992810368538 + 0.01 * 7.0391130447387695
Epoch 510, val loss: 0.8735536336898804
Epoch 520, training loss: 0.08771220594644547 = 0.017351588234305382 + 0.01 * 7.036061763763428
Epoch 520, val loss: 0.8825643062591553
Epoch 530, training loss: 0.08646515011787415 = 0.016133537515997887 + 0.01 * 7.033161163330078
Epoch 530, val loss: 0.8913657069206238
Epoch 540, training loss: 0.08538408577442169 = 0.015042909421026707 + 0.01 * 7.034117698669434
Epoch 540, val loss: 0.8999563455581665
Epoch 550, training loss: 0.08434295654296875 = 0.014063880778849125 + 0.01 * 7.027907848358154
Epoch 550, val loss: 0.908317506313324
Epoch 560, training loss: 0.08344292640686035 = 0.013182124122977257 + 0.01 * 7.026080131530762
Epoch 560, val loss: 0.9164601564407349
Epoch 570, training loss: 0.08259045332670212 = 0.012384711764752865 + 0.01 * 7.020574569702148
Epoch 570, val loss: 0.9243925213813782
Epoch 580, training loss: 0.08196389675140381 = 0.011661121621727943 + 0.01 * 7.030277729034424
Epoch 580, val loss: 0.9321224689483643
Epoch 590, training loss: 0.08122219145298004 = 0.01100326981395483 + 0.01 * 7.021892547607422
Epoch 590, val loss: 0.939656674861908
Epoch 600, training loss: 0.08057273179292679 = 0.010403324849903584 + 0.01 * 7.016940593719482
Epoch 600, val loss: 0.9469747543334961
Epoch 610, training loss: 0.07992711663246155 = 0.009854179807007313 + 0.01 * 7.007293224334717
Epoch 610, val loss: 0.9541211128234863
Epoch 620, training loss: 0.07950115203857422 = 0.009350468404591084 + 0.01 * 7.015068054199219
Epoch 620, val loss: 0.9610950350761414
Epoch 630, training loss: 0.07891854643821716 = 0.008887972682714462 + 0.01 * 7.00305700302124
Epoch 630, val loss: 0.9678564071655273
Epoch 640, training loss: 0.07844583690166473 = 0.008461621589958668 + 0.01 * 6.998421669006348
Epoch 640, val loss: 0.9744539856910706
Epoch 650, training loss: 0.0780826136469841 = 0.008067861199378967 + 0.01 * 7.0014753341674805
Epoch 650, val loss: 0.9808850288391113
Epoch 660, training loss: 0.07761959731578827 = 0.0077033755369484425 + 0.01 * 6.991621971130371
Epoch 660, val loss: 0.987170398235321
Epoch 670, training loss: 0.07721422612667084 = 0.007365529425442219 + 0.01 * 6.984869480133057
Epoch 670, val loss: 0.9932986497879028
Epoch 680, training loss: 0.07695312052965164 = 0.0070516448467969894 + 0.01 * 6.990147590637207
Epoch 680, val loss: 0.9992709755897522
Epoch 690, training loss: 0.07656347006559372 = 0.006759537383913994 + 0.01 * 6.980393409729004
Epoch 690, val loss: 1.0050989389419556
Epoch 700, training loss: 0.0762098953127861 = 0.006486943457275629 + 0.01 * 6.97229528427124
Epoch 700, val loss: 1.010796070098877
Epoch 710, training loss: 0.0760837197303772 = 0.006232816725969315 + 0.01 * 6.9850897789001465
Epoch 710, val loss: 1.0163450241088867
Epoch 720, training loss: 0.07566168904304504 = 0.005996005143970251 + 0.01 * 6.966568946838379
Epoch 720, val loss: 1.021732211112976
Epoch 730, training loss: 0.07538984715938568 = 0.005773942917585373 + 0.01 * 6.961590766906738
Epoch 730, val loss: 1.0269982814788818
Epoch 740, training loss: 0.07508502155542374 = 0.005565312225371599 + 0.01 * 6.951971054077148
Epoch 740, val loss: 1.0321407318115234
Epoch 750, training loss: 0.07522543519735336 = 0.005368947517126799 + 0.01 * 6.9856486320495605
Epoch 750, val loss: 1.037171721458435
Epoch 760, training loss: 0.07461964339017868 = 0.005184584762901068 + 0.01 * 6.943506240844727
Epoch 760, val loss: 1.0421111583709717
Epoch 770, training loss: 0.07453710585832596 = 0.005011339206248522 + 0.01 * 6.952576160430908
Epoch 770, val loss: 1.0469204187393188
Epoch 780, training loss: 0.0743560940027237 = 0.004848048090934753 + 0.01 * 6.950804710388184
Epoch 780, val loss: 1.051560878753662
Epoch 790, training loss: 0.07398110628128052 = 0.004693883471190929 + 0.01 * 6.928722381591797
Epoch 790, val loss: 1.0561177730560303
Epoch 800, training loss: 0.07377216964960098 = 0.004548339173197746 + 0.01 * 6.9223833084106445
Epoch 800, val loss: 1.0605697631835938
Epoch 810, training loss: 0.0738128051161766 = 0.004410794470459223 + 0.01 * 6.9402008056640625
Epoch 810, val loss: 1.0649453401565552
Epoch 820, training loss: 0.07358914613723755 = 0.004280742257833481 + 0.01 * 6.930840015411377
Epoch 820, val loss: 1.0691341161727905
Epoch 830, training loss: 0.07345578074455261 = 0.004157390911132097 + 0.01 * 6.92983865737915
Epoch 830, val loss: 1.0732911825180054
Epoch 840, training loss: 0.07317637652158737 = 0.004040412604808807 + 0.01 * 6.9135966300964355
Epoch 840, val loss: 1.0772809982299805
Epoch 850, training loss: 0.07317151129245758 = 0.003929398022592068 + 0.01 * 6.924211502075195
Epoch 850, val loss: 1.081243634223938
Epoch 860, training loss: 0.0729004368185997 = 0.0038239487912505865 + 0.01 * 6.907649040222168
Epoch 860, val loss: 1.0850874185562134
Epoch 870, training loss: 0.07290399074554443 = 0.00372358039021492 + 0.01 * 6.918041229248047
Epoch 870, val loss: 1.088842511177063
Epoch 880, training loss: 0.07253329455852509 = 0.0036282504443079233 + 0.01 * 6.890503883361816
Epoch 880, val loss: 1.0924409627914429
Epoch 890, training loss: 0.07230308651924133 = 0.0035373123828321695 + 0.01 * 6.876577854156494
Epoch 890, val loss: 1.0960538387298584
Epoch 900, training loss: 0.07224003225564957 = 0.0034505168441683054 + 0.01 * 6.878951549530029
Epoch 900, val loss: 1.0994986295700073
Epoch 910, training loss: 0.07205931097269058 = 0.003367590019479394 + 0.01 * 6.869172096252441
Epoch 910, val loss: 1.1029150485992432
Epoch 920, training loss: 0.07205894589424133 = 0.0032886017579585314 + 0.01 * 6.877034664154053
Epoch 920, val loss: 1.1063148975372314
Epoch 930, training loss: 0.07190794497728348 = 0.0032134249340742826 + 0.01 * 6.869451999664307
Epoch 930, val loss: 1.1094915866851807
Epoch 940, training loss: 0.07162287831306458 = 0.0031413407996296883 + 0.01 * 6.848153591156006
Epoch 940, val loss: 1.1126898527145386
Epoch 950, training loss: 0.07162440568208694 = 0.003072160528972745 + 0.01 * 6.855224609375
Epoch 950, val loss: 1.1158281564712524
Epoch 960, training loss: 0.07140740752220154 = 0.0030058957636356354 + 0.01 * 6.840150833129883
Epoch 960, val loss: 1.118855357170105
Epoch 970, training loss: 0.0714997872710228 = 0.0029423332307487726 + 0.01 * 6.855745792388916
Epoch 970, val loss: 1.1218194961547852
Epoch 980, training loss: 0.07132521271705627 = 0.0028813944663852453 + 0.01 * 6.844382286071777
Epoch 980, val loss: 1.1247589588165283
Epoch 990, training loss: 0.07115644961595535 = 0.002822871319949627 + 0.01 * 6.833358287811279
Epoch 990, val loss: 1.1276270151138306
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.025484800338745 = 1.939516544342041 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9428045749664307
Epoch 10, training loss: 2.016146421432495 = 1.9301788806915283 + 0.01 * 8.59675407409668
Epoch 10, val loss: 1.9339529275894165
Epoch 20, training loss: 2.004615306854248 = 1.9186501502990723 + 0.01 * 8.596515655517578
Epoch 20, val loss: 1.922598123550415
Epoch 30, training loss: 1.9884673357009888 = 1.9025095701217651 + 0.01 * 8.59577465057373
Epoch 30, val loss: 1.906309723854065
Epoch 40, training loss: 1.9649089574813843 = 1.8789926767349243 + 0.01 * 8.591630935668945
Epoch 40, val loss: 1.882360816001892
Epoch 50, training loss: 1.931904673576355 = 1.84628427028656 + 0.01 * 8.562042236328125
Epoch 50, val loss: 1.8498557806015015
Epoch 60, training loss: 1.8930875062942505 = 1.8090556859970093 + 0.01 * 8.40318489074707
Epoch 60, val loss: 1.8154582977294922
Epoch 70, training loss: 1.8587113618850708 = 1.77643620967865 + 0.01 * 8.227520942687988
Epoch 70, val loss: 1.7868921756744385
Epoch 80, training loss: 1.819077730178833 = 1.7386847734451294 + 0.01 * 8.039294242858887
Epoch 80, val loss: 1.7516909837722778
Epoch 90, training loss: 1.7640341520309448 = 1.6857961416244507 + 0.01 * 7.823800086975098
Epoch 90, val loss: 1.7047979831695557
Epoch 100, training loss: 1.6887308359146118 = 1.6128971576690674 + 0.01 * 7.5833659172058105
Epoch 100, val loss: 1.6434296369552612
Epoch 110, training loss: 1.5949244499206543 = 1.521203875541687 + 0.01 * 7.372056484222412
Epoch 110, val loss: 1.5666993856430054
Epoch 120, training loss: 1.4937973022460938 = 1.4205286502838135 + 0.01 * 7.326860427856445
Epoch 120, val loss: 1.4850503206253052
Epoch 130, training loss: 1.3929197788238525 = 1.3200783729553223 + 0.01 * 7.284139156341553
Epoch 130, val loss: 1.405792474746704
Epoch 140, training loss: 1.2953611612319946 = 1.2227604389190674 + 0.01 * 7.260066986083984
Epoch 140, val loss: 1.3319523334503174
Epoch 150, training loss: 1.2006250619888306 = 1.1282219886779785 + 0.01 * 7.240306377410889
Epoch 150, val loss: 1.2617944478988647
Epoch 160, training loss: 1.1072388887405396 = 1.0350366830825806 + 0.01 * 7.220216274261475
Epoch 160, val loss: 1.1938507556915283
Epoch 170, training loss: 1.0154696702957153 = 0.9434542655944824 + 0.01 * 7.201540946960449
Epoch 170, val loss: 1.1270631551742554
Epoch 180, training loss: 0.9267741441726685 = 0.854907751083374 + 0.01 * 7.186638832092285
Epoch 180, val loss: 1.063097357749939
Epoch 190, training loss: 0.8429790139198303 = 0.7711986899375916 + 0.01 * 7.1780314445495605
Epoch 190, val loss: 1.003470540046692
Epoch 200, training loss: 0.7651239037513733 = 0.6934279799461365 + 0.01 * 7.169591426849365
Epoch 200, val loss: 0.9497302174568176
Epoch 210, training loss: 0.6935595870018005 = 0.621945858001709 + 0.01 * 7.161375045776367
Epoch 210, val loss: 0.9030078053474426
Epoch 220, training loss: 0.6280069351196289 = 0.5564885139465332 + 0.01 * 7.1518402099609375
Epoch 220, val loss: 0.8633568286895752
Epoch 230, training loss: 0.567922830581665 = 0.49650445580482483 + 0.01 * 7.141839504241943
Epoch 230, val loss: 0.8303939700126648
Epoch 240, training loss: 0.5128288865089417 = 0.4415169954299927 + 0.01 * 7.131187438964844
Epoch 240, val loss: 0.8032779097557068
Epoch 250, training loss: 0.4625302255153656 = 0.39131635427474976 + 0.01 * 7.121386528015137
Epoch 250, val loss: 0.78131103515625
Epoch 260, training loss: 0.41694527864456177 = 0.34583455324172974 + 0.01 * 7.1110711097717285
Epoch 260, val loss: 0.7639153599739075
Epoch 270, training loss: 0.37589341402053833 = 0.30483871698379517 + 0.01 * 7.105471611022949
Epoch 270, val loss: 0.750744640827179
Epoch 280, training loss: 0.338851660490036 = 0.26785892248153687 + 0.01 * 7.099273204803467
Epoch 280, val loss: 0.7414162158966064
Epoch 290, training loss: 0.3053637742996216 = 0.23442672193050385 + 0.01 * 7.093703746795654
Epoch 290, val loss: 0.7355509996414185
Epoch 300, training loss: 0.2752130329608917 = 0.2042960673570633 + 0.01 * 7.0916972160339355
Epoch 300, val loss: 0.7327059507369995
Epoch 310, training loss: 0.24826565384864807 = 0.17741163074970245 + 0.01 * 7.0854034423828125
Epoch 310, val loss: 0.7327478528022766
Epoch 320, training loss: 0.22460229694843292 = 0.1537744402885437 + 0.01 * 7.082785606384277
Epoch 320, val loss: 0.7352853417396545
Epoch 330, training loss: 0.2041592299938202 = 0.1332971751689911 + 0.01 * 7.08620548248291
Epoch 330, val loss: 0.7401807308197021
Epoch 340, training loss: 0.18653368949890137 = 0.11576557159423828 + 0.01 * 7.076810836791992
Epoch 340, val loss: 0.7471620440483093
Epoch 350, training loss: 0.17156630754470825 = 0.10084876418113708 + 0.01 * 7.071755409240723
Epoch 350, val loss: 0.7558360695838928
Epoch 360, training loss: 0.1588650941848755 = 0.08819419145584106 + 0.01 * 7.067091464996338
Epoch 360, val loss: 0.765915036201477
Epoch 370, training loss: 0.14814211428165436 = 0.0774640366435051 + 0.01 * 7.067807674407959
Epoch 370, val loss: 0.7769920825958252
Epoch 380, training loss: 0.13895606994628906 = 0.06835521012544632 + 0.01 * 7.060085773468018
Epoch 380, val loss: 0.7887820601463318
Epoch 390, training loss: 0.13114994764328003 = 0.06059982255101204 + 0.01 * 7.055013179779053
Epoch 390, val loss: 0.8010677099227905
Epoch 400, training loss: 0.12447413802146912 = 0.05397748202085495 + 0.01 * 7.049665927886963
Epoch 400, val loss: 0.8136615753173828
Epoch 410, training loss: 0.11875399947166443 = 0.04830387607216835 + 0.01 * 7.045012474060059
Epoch 410, val loss: 0.8264144062995911
Epoch 420, training loss: 0.1137896180152893 = 0.04342363029718399 + 0.01 * 7.036599159240723
Epoch 420, val loss: 0.839169442653656
Epoch 430, training loss: 0.10952936857938766 = 0.039208345115184784 + 0.01 * 7.032102584838867
Epoch 430, val loss: 0.8518385887145996
Epoch 440, training loss: 0.10588899999856949 = 0.03555218130350113 + 0.01 * 7.033681869506836
Epoch 440, val loss: 0.8643702864646912
Epoch 450, training loss: 0.1025569960474968 = 0.03236664831638336 + 0.01 * 7.0190348625183105
Epoch 450, val loss: 0.8767300248146057
Epoch 460, training loss: 0.09968335926532745 = 0.02957800216972828 + 0.01 * 7.010535717010498
Epoch 460, val loss: 0.888860821723938
Epoch 470, training loss: 0.09729965031147003 = 0.027127999812364578 + 0.01 * 7.017165660858154
Epoch 470, val loss: 0.9007603526115417
Epoch 480, training loss: 0.0949949249625206 = 0.024967359378933907 + 0.01 * 7.0027570724487305
Epoch 480, val loss: 0.9123281240463257
Epoch 490, training loss: 0.09296993166208267 = 0.023052187636494637 + 0.01 * 6.991775035858154
Epoch 490, val loss: 0.9236109852790833
Epoch 500, training loss: 0.09120161086320877 = 0.02134767733514309 + 0.01 * 6.985393047332764
Epoch 500, val loss: 0.9346068501472473
Epoch 510, training loss: 0.08972819894552231 = 0.019824998453259468 + 0.01 * 6.990320682525635
Epoch 510, val loss: 0.9453185796737671
Epoch 520, training loss: 0.08823512494564056 = 0.01846175640821457 + 0.01 * 6.977336883544922
Epoch 520, val loss: 0.9557192325592041
Epoch 530, training loss: 0.08711179345846176 = 0.017238281667232513 + 0.01 * 6.987351417541504
Epoch 530, val loss: 0.9657781720161438
Epoch 540, training loss: 0.08586634695529938 = 0.016136053949594498 + 0.01 * 6.973029613494873
Epoch 540, val loss: 0.9755766987800598
Epoch 550, training loss: 0.08473681658506393 = 0.015138682909309864 + 0.01 * 6.959813594818115
Epoch 550, val loss: 0.9850719571113586
Epoch 560, training loss: 0.0837932825088501 = 0.014232928864657879 + 0.01 * 6.956035614013672
Epoch 560, val loss: 0.9943094849586487
Epoch 570, training loss: 0.08290995657444 = 0.013407878577709198 + 0.01 * 6.9502081871032715
Epoch 570, val loss: 1.0032892227172852
Epoch 580, training loss: 0.08233512938022614 = 0.012654732912778854 + 0.01 * 6.968039512634277
Epoch 580, val loss: 1.0120607614517212
Epoch 590, training loss: 0.08140609413385391 = 0.0119666438549757 + 0.01 * 6.943944931030273
Epoch 590, val loss: 1.0204987525939941
Epoch 600, training loss: 0.08077089488506317 = 0.011335689574480057 + 0.01 * 6.943521022796631
Epoch 600, val loss: 1.0287333726882935
Epoch 610, training loss: 0.08013349771499634 = 0.01075570285320282 + 0.01 * 6.937779903411865
Epoch 610, val loss: 1.0367467403411865
Epoch 620, training loss: 0.07951828092336655 = 0.01022137701511383 + 0.01 * 6.929690837860107
Epoch 620, val loss: 1.0445119142532349
Epoch 630, training loss: 0.07903430610895157 = 0.00972794834524393 + 0.01 * 6.930635929107666
Epoch 630, val loss: 1.0521055459976196
Epoch 640, training loss: 0.07848968356847763 = 0.009271838702261448 + 0.01 * 6.9217848777771
Epoch 640, val loss: 1.0594732761383057
Epoch 650, training loss: 0.07803596556186676 = 0.00884938333183527 + 0.01 * 6.91865873336792
Epoch 650, val loss: 1.0666275024414062
Epoch 660, training loss: 0.07772738486528397 = 0.008457036688923836 + 0.01 * 6.927034854888916
Epoch 660, val loss: 1.0735907554626465
Epoch 670, training loss: 0.07722897082567215 = 0.008092721924185753 + 0.01 * 6.9136247634887695
Epoch 670, val loss: 1.080398678779602
Epoch 680, training loss: 0.07693370431661606 = 0.0077534629963338375 + 0.01 * 6.918024063110352
Epoch 680, val loss: 1.0870130062103271
Epoch 690, training loss: 0.07646392285823822 = 0.007436908315867186 + 0.01 * 6.9027018547058105
Epoch 690, val loss: 1.0933994054794312
Epoch 700, training loss: 0.07627378404140472 = 0.007141026668250561 + 0.01 * 6.913276195526123
Epoch 700, val loss: 1.0996557474136353
Epoch 710, training loss: 0.07585356384515762 = 0.006864472758024931 + 0.01 * 6.898909091949463
Epoch 710, val loss: 1.1057292222976685
Epoch 720, training loss: 0.07555946707725525 = 0.006605351343750954 + 0.01 * 6.895411968231201
Epoch 720, val loss: 1.111668348312378
Epoch 730, training loss: 0.07536312937736511 = 0.0063621182925999165 + 0.01 * 6.900101184844971
Epoch 730, val loss: 1.1174269914627075
Epoch 740, training loss: 0.0750717744231224 = 0.006133636459708214 + 0.01 * 6.8938140869140625
Epoch 740, val loss: 1.1230581998825073
Epoch 750, training loss: 0.07489288598299026 = 0.005918653681874275 + 0.01 * 6.897423267364502
Epoch 750, val loss: 1.1285167932510376
Epoch 760, training loss: 0.07460247725248337 = 0.0057165357284247875 + 0.01 * 6.888594627380371
Epoch 760, val loss: 1.1338244676589966
Epoch 770, training loss: 0.07435590028762817 = 0.00552594056352973 + 0.01 * 6.882996082305908
Epoch 770, val loss: 1.1390761137008667
Epoch 780, training loss: 0.07410696148872375 = 0.005345744546502829 + 0.01 * 6.876121520996094
Epoch 780, val loss: 1.1440843343734741
Epoch 790, training loss: 0.07396440207958221 = 0.005175528582185507 + 0.01 * 6.878887176513672
Epoch 790, val loss: 1.1491249799728394
Epoch 800, training loss: 0.07366706430912018 = 0.00501473993062973 + 0.01 * 6.865232944488525
Epoch 800, val loss: 1.153895378112793
Epoch 810, training loss: 0.07358948141336441 = 0.0048622931353747845 + 0.01 * 6.8727192878723145
Epoch 810, val loss: 1.158626675605774
Epoch 820, training loss: 0.07336727529764175 = 0.004717383999377489 + 0.01 * 6.864988803863525
Epoch 820, val loss: 1.163183569908142
Epoch 830, training loss: 0.07325944304466248 = 0.004579257220029831 + 0.01 * 6.86801815032959
Epoch 830, val loss: 1.1676697731018066
Epoch 840, training loss: 0.0730283111333847 = 0.004448045045137405 + 0.01 * 6.858027458190918
Epoch 840, val loss: 1.172034502029419
Epoch 850, training loss: 0.07288987189531326 = 0.004323568660765886 + 0.01 * 6.856629848480225
Epoch 850, val loss: 1.1763651371002197
Epoch 860, training loss: 0.07279609143733978 = 0.004205196630209684 + 0.01 * 6.859089374542236
Epoch 860, val loss: 1.1804993152618408
Epoch 870, training loss: 0.07253138720989227 = 0.0040927911177277565 + 0.01 * 6.843859672546387
Epoch 870, val loss: 1.1845932006835938
Epoch 880, training loss: 0.0724041610956192 = 0.003985721617937088 + 0.01 * 6.841843605041504
Epoch 880, val loss: 1.1885191202163696
Epoch 890, training loss: 0.0722537562251091 = 0.003883394878357649 + 0.01 * 6.837036609649658
Epoch 890, val loss: 1.192368745803833
Epoch 900, training loss: 0.07226061075925827 = 0.003786076558753848 + 0.01 * 6.847453594207764
Epoch 900, val loss: 1.1961702108383179
Epoch 910, training loss: 0.07210596650838852 = 0.003693017177283764 + 0.01 * 6.84129524230957
Epoch 910, val loss: 1.199781894683838
Epoch 920, training loss: 0.07200824469327927 = 0.003604149678722024 + 0.01 * 6.840409278869629
Epoch 920, val loss: 1.203372836112976
Epoch 930, training loss: 0.07180269062519073 = 0.003519356017932296 + 0.01 * 6.828333854675293
Epoch 930, val loss: 1.2069149017333984
Epoch 940, training loss: 0.07163335382938385 = 0.0034382776357233524 + 0.01 * 6.819507598876953
Epoch 940, val loss: 1.210211157798767
Epoch 950, training loss: 0.07177039235830307 = 0.003360489849001169 + 0.01 * 6.8409905433654785
Epoch 950, val loss: 1.2136694192886353
Epoch 960, training loss: 0.07146826386451721 = 0.003286240855231881 + 0.01 * 6.818202018737793
Epoch 960, val loss: 1.2167630195617676
Epoch 970, training loss: 0.07155928760766983 = 0.003214815864339471 + 0.01 * 6.834447383880615
Epoch 970, val loss: 1.220046877861023
Epoch 980, training loss: 0.0712786614894867 = 0.0031467874068766832 + 0.01 * 6.813187122344971
Epoch 980, val loss: 1.2229881286621094
Epoch 990, training loss: 0.07143355160951614 = 0.0030809601303189993 + 0.01 * 6.835259437561035
Epoch 990, val loss: 1.2260185480117798
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8165524512387982
The final CL Acc:0.77901, 0.01145, The final GNN Acc:0.81796, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13156])
remove edge: torch.Size([2, 7892])
updated graph: torch.Size([2, 10492])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.031379461288452 = 1.9454115629196167 + 0.01 * 8.596800804138184
Epoch 0, val loss: 1.9409265518188477
Epoch 10, training loss: 2.021515130996704 = 1.9355475902557373 + 0.01 * 8.596757888793945
Epoch 10, val loss: 1.9316065311431885
Epoch 20, training loss: 2.0096912384033203 = 1.9237254858016968 + 0.01 * 8.596576690673828
Epoch 20, val loss: 1.9199647903442383
Epoch 30, training loss: 1.993383765220642 = 1.9074229001998901 + 0.01 * 8.596084594726562
Epoch 30, val loss: 1.9034842252731323
Epoch 40, training loss: 1.9695067405700684 = 1.8835699558258057 + 0.01 * 8.593676567077637
Epoch 40, val loss: 1.8793061971664429
Epoch 50, training loss: 1.9356043338775635 = 1.8498718738555908 + 0.01 * 8.573244094848633
Epoch 50, val loss: 1.8462603092193604
Epoch 60, training loss: 1.8946205377578735 = 1.8101332187652588 + 0.01 * 8.448729515075684
Epoch 60, val loss: 1.8105460405349731
Epoch 70, training loss: 1.8554344177246094 = 1.7734192609786987 + 0.01 * 8.201519012451172
Epoch 70, val loss: 1.7805721759796143
Epoch 80, training loss: 1.8101810216903687 = 1.730606198310852 + 0.01 * 7.957487106323242
Epoch 80, val loss: 1.74265718460083
Epoch 90, training loss: 1.746462106704712 = 1.6699275970458984 + 0.01 * 7.653448581695557
Epoch 90, val loss: 1.6877624988555908
Epoch 100, training loss: 1.6629165410995483 = 1.5884177684783936 + 0.01 * 7.449874401092529
Epoch 100, val loss: 1.6170064210891724
Epoch 110, training loss: 1.564937710762024 = 1.4908961057662964 + 0.01 * 7.404155254364014
Epoch 110, val loss: 1.532448410987854
Epoch 120, training loss: 1.4616906642913818 = 1.3882389068603516 + 0.01 * 7.345179557800293
Epoch 120, val loss: 1.445024847984314
Epoch 130, training loss: 1.3579740524291992 = 1.285188913345337 + 0.01 * 7.278517246246338
Epoch 130, val loss: 1.3583886623382568
Epoch 140, training loss: 1.2546346187591553 = 1.1823102235794067 + 0.01 * 7.232443332672119
Epoch 140, val loss: 1.2753479480743408
Epoch 150, training loss: 1.1518850326538086 = 1.0798561573028564 + 0.01 * 7.202888488769531
Epoch 150, val loss: 1.1944856643676758
Epoch 160, training loss: 1.0501296520233154 = 0.9783799052238464 + 0.01 * 7.174975395202637
Epoch 160, val loss: 1.1158543825149536
Epoch 170, training loss: 0.9515436291694641 = 0.8800169825553894 + 0.01 * 7.15266227722168
Epoch 170, val loss: 1.0403774976730347
Epoch 180, training loss: 0.8601807951927185 = 0.7888011932373047 + 0.01 * 7.137959003448486
Epoch 180, val loss: 0.9716716408729553
Epoch 190, training loss: 0.7796887159347534 = 0.7084106802940369 + 0.01 * 7.12780237197876
Epoch 190, val loss: 0.9136149883270264
Epoch 200, training loss: 0.7113677263259888 = 0.6401679515838623 + 0.01 * 7.119976997375488
Epoch 200, val loss: 0.8686972260475159
Epoch 210, training loss: 0.6538644433021545 = 0.5827351212501526 + 0.01 * 7.11293363571167
Epoch 210, val loss: 0.836365282535553
Epoch 220, training loss: 0.604622483253479 = 0.5335342288017273 + 0.01 * 7.108826637268066
Epoch 220, val loss: 0.814129114151001
Epoch 230, training loss: 0.5608841180801392 = 0.489912211894989 + 0.01 * 7.097193241119385
Epoch 230, val loss: 0.7986043095588684
Epoch 240, training loss: 0.5205702185630798 = 0.44969671964645386 + 0.01 * 7.087352275848389
Epoch 240, val loss: 0.7872273325920105
Epoch 250, training loss: 0.4821661710739136 = 0.41138383746147156 + 0.01 * 7.0782318115234375
Epoch 250, val loss: 0.7782849073410034
Epoch 260, training loss: 0.4449089765548706 = 0.37420740723609924 + 0.01 * 7.070156574249268
Epoch 260, val loss: 0.7710537910461426
Epoch 270, training loss: 0.4086589813232422 = 0.338050901889801 + 0.01 * 7.060809135437012
Epoch 270, val loss: 0.7652724385261536
Epoch 280, training loss: 0.37384599447250366 = 0.30327826738357544 + 0.01 * 7.056774139404297
Epoch 280, val loss: 0.7611048817634583
Epoch 290, training loss: 0.34089505672454834 = 0.2704792022705078 + 0.01 * 7.041584014892578
Epoch 290, val loss: 0.7590149641036987
Epoch 300, training loss: 0.3105529546737671 = 0.24015802145004272 + 0.01 * 7.039493560791016
Epoch 300, val loss: 0.7594218850135803
Epoch 310, training loss: 0.2830545902252197 = 0.21267074346542358 + 0.01 * 7.038384437561035
Epoch 310, val loss: 0.7623659372329712
Epoch 320, training loss: 0.25841599702835083 = 0.18819034099578857 + 0.01 * 7.022566318511963
Epoch 320, val loss: 0.7677163481712341
Epoch 330, training loss: 0.23684169352054596 = 0.16666649281978607 + 0.01 * 7.017520427703857
Epoch 330, val loss: 0.7752835750579834
Epoch 340, training loss: 0.2179979681968689 = 0.14789722859859467 + 0.01 * 7.010073661804199
Epoch 340, val loss: 0.7848789691925049
Epoch 350, training loss: 0.2017461359500885 = 0.13158859312534332 + 0.01 * 7.015753746032715
Epoch 350, val loss: 0.7961734533309937
Epoch 360, training loss: 0.187406525015831 = 0.11742064356803894 + 0.01 * 6.998588562011719
Epoch 360, val loss: 0.8088792562484741
Epoch 370, training loss: 0.17499813437461853 = 0.10506473481655121 + 0.01 * 6.9933390617370605
Epoch 370, val loss: 0.822695791721344
Epoch 380, training loss: 0.16419073939323425 = 0.09424249082803726 + 0.01 * 6.994824409484863
Epoch 380, val loss: 0.8372750282287598
Epoch 390, training loss: 0.1545814573764801 = 0.08473148196935654 + 0.01 * 6.984997272491455
Epoch 390, val loss: 0.8523537516593933
Epoch 400, training loss: 0.14613912999629974 = 0.07633524388074875 + 0.01 * 6.98038911819458
Epoch 400, val loss: 0.8677474856376648
Epoch 410, training loss: 0.13860392570495605 = 0.06889640539884567 + 0.01 * 6.9707512855529785
Epoch 410, val loss: 0.8833634257316589
Epoch 420, training loss: 0.1319233775138855 = 0.06229088082909584 + 0.01 * 6.963250160217285
Epoch 420, val loss: 0.8990703225135803
Epoch 430, training loss: 0.12607981264591217 = 0.05641879513859749 + 0.01 * 6.96610164642334
Epoch 430, val loss: 0.9147527813911438
Epoch 440, training loss: 0.12075380980968475 = 0.051196787506341934 + 0.01 * 6.955702781677246
Epoch 440, val loss: 0.930385172367096
Epoch 450, training loss: 0.11605743318796158 = 0.046544305980205536 + 0.01 * 6.951313018798828
Epoch 450, val loss: 0.94584721326828
Epoch 460, training loss: 0.11185322701931 = 0.042393025010824203 + 0.01 * 6.946020603179932
Epoch 460, val loss: 0.9611086249351501
Epoch 470, training loss: 0.10809643566608429 = 0.038699883967638016 + 0.01 * 6.93965482711792
Epoch 470, val loss: 0.975990891456604
Epoch 480, training loss: 0.10478337109088898 = 0.035412129014730453 + 0.01 * 6.937124252319336
Epoch 480, val loss: 0.9906679391860962
Epoch 490, training loss: 0.10171816498041153 = 0.03248176723718643 + 0.01 * 6.92363977432251
Epoch 490, val loss: 1.0049551725387573
Epoch 500, training loss: 0.0993039608001709 = 0.02986486256122589 + 0.01 * 6.943910121917725
Epoch 500, val loss: 1.0188835859298706
Epoch 510, training loss: 0.09670250117778778 = 0.02753068320453167 + 0.01 * 6.917181968688965
Epoch 510, val loss: 1.0323430299758911
Epoch 520, training loss: 0.09452785551548004 = 0.025442007929086685 + 0.01 * 6.9085845947265625
Epoch 520, val loss: 1.0454233884811401
Epoch 530, training loss: 0.09262803941965103 = 0.023567182943224907 + 0.01 * 6.90608549118042
Epoch 530, val loss: 1.0581778287887573
Epoch 540, training loss: 0.09091753512620926 = 0.021880527958273888 + 0.01 * 6.903700828552246
Epoch 540, val loss: 1.070578694343567
Epoch 550, training loss: 0.08929033577442169 = 0.02036050334572792 + 0.01 * 6.892983436584473
Epoch 550, val loss: 1.0825873613357544
Epoch 560, training loss: 0.0878821536898613 = 0.018986595794558525 + 0.01 * 6.88955545425415
Epoch 560, val loss: 1.0942659378051758
Epoch 570, training loss: 0.08673553913831711 = 0.0177447646856308 + 0.01 * 6.899077892303467
Epoch 570, val loss: 1.1055783033370972
Epoch 580, training loss: 0.08553366363048553 = 0.016621705144643784 + 0.01 * 6.891196250915527
Epoch 580, val loss: 1.1164727210998535
Epoch 590, training loss: 0.08442740887403488 = 0.015601815655827522 + 0.01 * 6.882559776306152
Epoch 590, val loss: 1.12699556350708
Epoch 600, training loss: 0.08340547233819962 = 0.014672414399683475 + 0.01 * 6.873305797576904
Epoch 600, val loss: 1.1372435092926025
Epoch 610, training loss: 0.08267287164926529 = 0.013823091052472591 + 0.01 * 6.884978294372559
Epoch 610, val loss: 1.1472253799438477
Epoch 620, training loss: 0.08181598782539368 = 0.013046346604824066 + 0.01 * 6.876964569091797
Epoch 620, val loss: 1.1568280458450317
Epoch 630, training loss: 0.08099278062582016 = 0.01233385968953371 + 0.01 * 6.865891933441162
Epoch 630, val loss: 1.1662107706069946
Epoch 640, training loss: 0.08033150434494019 = 0.011678715236485004 + 0.01 * 6.865278720855713
Epoch 640, val loss: 1.1753305196762085
Epoch 650, training loss: 0.07974154502153397 = 0.011075587011873722 + 0.01 * 6.86659574508667
Epoch 650, val loss: 1.1841164827346802
Epoch 660, training loss: 0.07909291237592697 = 0.01051998045295477 + 0.01 * 6.857293128967285
Epoch 660, val loss: 1.1926571130752563
Epoch 670, training loss: 0.07854092121124268 = 0.010006140917539597 + 0.01 * 6.853477478027344
Epoch 670, val loss: 1.2009705305099487
Epoch 680, training loss: 0.07798688858747482 = 0.009529909119009972 + 0.01 * 6.84569787979126
Epoch 680, val loss: 1.2090494632720947
Epoch 690, training loss: 0.07766300439834595 = 0.009088534861803055 + 0.01 * 6.857447624206543
Epoch 690, val loss: 1.2168214321136475
Epoch 700, training loss: 0.07704104483127594 = 0.008677996695041656 + 0.01 * 6.836305141448975
Epoch 700, val loss: 1.2244467735290527
Epoch 710, training loss: 0.07667037099599838 = 0.008294811472296715 + 0.01 * 6.837555885314941
Epoch 710, val loss: 1.2318737506866455
Epoch 720, training loss: 0.07640624046325684 = 0.007938262075185776 + 0.01 * 6.846798419952393
Epoch 720, val loss: 1.239047646522522
Epoch 730, training loss: 0.07594149559736252 = 0.0076063540764153 + 0.01 * 6.833514213562012
Epoch 730, val loss: 1.2460087537765503
Epoch 740, training loss: 0.07556788623332977 = 0.007295052520930767 + 0.01 * 6.82728385925293
Epoch 740, val loss: 1.2528016567230225
Epoch 750, training loss: 0.07536041736602783 = 0.007002629805356264 + 0.01 * 6.835778713226318
Epoch 750, val loss: 1.2594555616378784
Epoch 760, training loss: 0.07497747242450714 = 0.006728695705533028 + 0.01 * 6.824877738952637
Epoch 760, val loss: 1.2659363746643066
Epoch 770, training loss: 0.07464610785245895 = 0.006471018772572279 + 0.01 * 6.817509651184082
Epoch 770, val loss: 1.2722941637039185
Epoch 780, training loss: 0.07441622763872147 = 0.006228441372513771 + 0.01 * 6.818778991699219
Epoch 780, val loss: 1.2784783840179443
Epoch 790, training loss: 0.07412457466125488 = 0.006000124383717775 + 0.01 * 6.812445163726807
Epoch 790, val loss: 1.2845571041107178
Epoch 800, training loss: 0.07393604516983032 = 0.0057849157601594925 + 0.01 * 6.815113544464111
Epoch 800, val loss: 1.2904301881790161
Epoch 810, training loss: 0.07389651238918304 = 0.005582060664892197 + 0.01 * 6.831445693969727
Epoch 810, val loss: 1.2961955070495605
Epoch 820, training loss: 0.07349512726068497 = 0.005390860140323639 + 0.01 * 6.810427188873291
Epoch 820, val loss: 1.3017632961273193
Epoch 830, training loss: 0.07329659163951874 = 0.005209955852478743 + 0.01 * 6.808664321899414
Epoch 830, val loss: 1.3071908950805664
Epoch 840, training loss: 0.0730440616607666 = 0.005039310548454523 + 0.01 * 6.800475120544434
Epoch 840, val loss: 1.3124972581863403
Epoch 850, training loss: 0.0728536918759346 = 0.004877682309597731 + 0.01 * 6.797601699829102
Epoch 850, val loss: 1.3176214694976807
Epoch 860, training loss: 0.07280228286981583 = 0.004724761471152306 + 0.01 * 6.8077521324157715
Epoch 860, val loss: 1.3226970434188843
Epoch 870, training loss: 0.07242020964622498 = 0.004579823464155197 + 0.01 * 6.784039497375488
Epoch 870, val loss: 1.3275963068008423
Epoch 880, training loss: 0.07229311764240265 = 0.004442235920578241 + 0.01 * 6.785088539123535
Epoch 880, val loss: 1.3323811292648315
Epoch 890, training loss: 0.07212762534618378 = 0.004311651922762394 + 0.01 * 6.78159761428833
Epoch 890, val loss: 1.3370726108551025
Epoch 900, training loss: 0.07206512987613678 = 0.004187431652098894 + 0.01 * 6.787769794464111
Epoch 900, val loss: 1.341646671295166
Epoch 910, training loss: 0.07203172147274017 = 0.004069511312991381 + 0.01 * 6.796220779418945
Epoch 910, val loss: 1.3460298776626587
Epoch 920, training loss: 0.07175341248512268 = 0.003957660868763924 + 0.01 * 6.779575347900391
Epoch 920, val loss: 1.350329041481018
Epoch 930, training loss: 0.0716087818145752 = 0.00385082233697176 + 0.01 * 6.775795936584473
Epoch 930, val loss: 1.3545372486114502
Epoch 940, training loss: 0.07149270176887512 = 0.0037488285452127457 + 0.01 * 6.774387836456299
Epoch 940, val loss: 1.3586359024047852
Epoch 950, training loss: 0.07143403589725494 = 0.0036515253596007824 + 0.01 * 6.7782511711120605
Epoch 950, val loss: 1.3627028465270996
Epoch 960, training loss: 0.07133135199546814 = 0.003558626165613532 + 0.01 * 6.777272701263428
Epoch 960, val loss: 1.3666026592254639
Epoch 970, training loss: 0.07111930847167969 = 0.003470040624961257 + 0.01 * 6.764926910400391
Epoch 970, val loss: 1.3704403638839722
Epoch 980, training loss: 0.07097724825143814 = 0.0033851738553494215 + 0.01 * 6.7592082023620605
Epoch 980, val loss: 1.374171495437622
Epoch 990, training loss: 0.07088965177536011 = 0.0033042095601558685 + 0.01 * 6.758544445037842
Epoch 990, val loss: 1.3778061866760254
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.031233072280884 = 1.9452649354934692 + 0.01 * 8.596806526184082
Epoch 0, val loss: 1.943475365638733
Epoch 10, training loss: 2.0218372344970703 = 1.935869812965393 + 0.01 * 8.59675121307373
Epoch 10, val loss: 1.933775782585144
Epoch 20, training loss: 2.010575532913208 = 1.9246100187301636 + 0.01 * 8.596542358398438
Epoch 20, val loss: 1.922037124633789
Epoch 30, training loss: 1.9949406385421753 = 1.9089810848236084 + 0.01 * 8.595958709716797
Epoch 30, val loss: 1.9059513807296753
Epoch 40, training loss: 1.9719632863998413 = 1.88602876663208 + 0.01 * 8.593452453613281
Epoch 40, val loss: 1.8827699422836304
Epoch 50, training loss: 1.938591718673706 = 1.8528387546539307 + 0.01 * 8.575291633605957
Epoch 50, val loss: 1.8505207300186157
Epoch 60, training loss: 1.8957263231277466 = 1.8111720085144043 + 0.01 * 8.455435752868652
Epoch 60, val loss: 1.8131585121154785
Epoch 70, training loss: 1.8506649732589722 = 1.7696458101272583 + 0.01 * 8.101921081542969
Epoch 70, val loss: 1.7794033288955688
Epoch 80, training loss: 1.8014428615570068 = 1.7238144874572754 + 0.01 * 7.7628374099731445
Epoch 80, val loss: 1.7396327257156372
Epoch 90, training loss: 1.7354896068572998 = 1.6599730253219604 + 0.01 * 7.551656723022461
Epoch 90, val loss: 1.6813154220581055
Epoch 100, training loss: 1.6492851972579956 = 1.5746374130249023 + 0.01 * 7.464774131774902
Epoch 100, val loss: 1.6060715913772583
Epoch 110, training loss: 1.5441137552261353 = 1.47006356716156 + 0.01 * 7.405017852783203
Epoch 110, val loss: 1.5181771516799927
Epoch 120, training loss: 1.429428219795227 = 1.356168270111084 + 0.01 * 7.325995922088623
Epoch 120, val loss: 1.4228029251098633
Epoch 130, training loss: 1.3140192031860352 = 1.2414873838424683 + 0.01 * 7.253180503845215
Epoch 130, val loss: 1.3275115489959717
Epoch 140, training loss: 1.2028688192367554 = 1.1306895017623901 + 0.01 * 7.2179365158081055
Epoch 140, val loss: 1.2356817722320557
Epoch 150, training loss: 1.098872184753418 = 1.0269064903259277 + 0.01 * 7.196566104888916
Epoch 150, val loss: 1.1502922773361206
Epoch 160, training loss: 1.0042400360107422 = 0.9325462579727173 + 0.01 * 7.16937780380249
Epoch 160, val loss: 1.0742071866989136
Epoch 170, training loss: 0.9197972416877747 = 0.8483800292015076 + 0.01 * 7.141720294952393
Epoch 170, val loss: 1.0092040300369263
Epoch 180, training loss: 0.8453256487846375 = 0.7741245627403259 + 0.01 * 7.120107173919678
Epoch 180, val loss: 0.955937922000885
Epoch 190, training loss: 0.779593288898468 = 0.7085509300231934 + 0.01 * 7.104236602783203
Epoch 190, val loss: 0.9138888120651245
Epoch 200, training loss: 0.7204439640045166 = 0.6495005488395691 + 0.01 * 7.094341278076172
Epoch 200, val loss: 0.8810157775878906
Epoch 210, training loss: 0.6656692624092102 = 0.594780445098877 + 0.01 * 7.0888800621032715
Epoch 210, val loss: 0.854975700378418
Epoch 220, training loss: 0.613922119140625 = 0.5430975556373596 + 0.01 * 7.082454681396484
Epoch 220, val loss: 0.8342676162719727
Epoch 230, training loss: 0.5647640824317932 = 0.4939934015274048 + 0.01 * 7.077070236206055
Epoch 230, val loss: 0.818217396736145
Epoch 240, training loss: 0.5182896256446838 = 0.4475673735141754 + 0.01 * 7.072223663330078
Epoch 240, val loss: 0.8069317936897278
Epoch 250, training loss: 0.4747602343559265 = 0.4040740728378296 + 0.01 * 7.06861686706543
Epoch 250, val loss: 0.800101637840271
Epoch 260, training loss: 0.4342477321624756 = 0.3635682463645935 + 0.01 * 7.067946910858154
Epoch 260, val loss: 0.7972419857978821
Epoch 270, training loss: 0.39660847187042236 = 0.3260039985179901 + 0.01 * 7.060445785522461
Epoch 270, val loss: 0.7976297736167908
Epoch 280, training loss: 0.36187922954559326 = 0.2913469672203064 + 0.01 * 7.053225517272949
Epoch 280, val loss: 0.800797700881958
Epoch 290, training loss: 0.33014556765556335 = 0.2596304416656494 + 0.01 * 7.051513671875
Epoch 290, val loss: 0.8064163327217102
Epoch 300, training loss: 0.30129095911979675 = 0.23084861040115356 + 0.01 * 7.044234275817871
Epoch 300, val loss: 0.8140091300010681
Epoch 310, training loss: 0.27535703778266907 = 0.20489653944969177 + 0.01 * 7.04604959487915
Epoch 310, val loss: 0.823394238948822
Epoch 320, training loss: 0.2519831359386444 = 0.18160657584667206 + 0.01 * 7.037655830383301
Epoch 320, val loss: 0.8342111110687256
Epoch 330, training loss: 0.2311067432165146 = 0.1607750505208969 + 0.01 * 7.033169269561768
Epoch 330, val loss: 0.8460906147956848
Epoch 340, training loss: 0.2125169336795807 = 0.14221824705600739 + 0.01 * 7.029869556427002
Epoch 340, val loss: 0.8587425351142883
Epoch 350, training loss: 0.1960201859474182 = 0.12576517462730408 + 0.01 * 7.025501728057861
Epoch 350, val loss: 0.8719651699066162
Epoch 360, training loss: 0.181425541639328 = 0.11122585833072662 + 0.01 * 7.019969463348389
Epoch 360, val loss: 0.8855041265487671
Epoch 370, training loss: 0.16858148574829102 = 0.09840894490480423 + 0.01 * 7.01725435256958
Epoch 370, val loss: 0.8990961313247681
Epoch 380, training loss: 0.15725159645080566 = 0.0871264860033989 + 0.01 * 7.012511253356934
Epoch 380, val loss: 0.912641167640686
Epoch 390, training loss: 0.14729547500610352 = 0.07720924913883209 + 0.01 * 7.008621692657471
Epoch 390, val loss: 0.9260490536689758
Epoch 400, training loss: 0.1385936141014099 = 0.06850787997245789 + 0.01 * 7.008572578430176
Epoch 400, val loss: 0.9391628503799438
Epoch 410, training loss: 0.13092578947544098 = 0.0608830563724041 + 0.01 * 7.004273891448975
Epoch 410, val loss: 0.9519789814949036
Epoch 420, training loss: 0.1241937130689621 = 0.0542144738137722 + 0.01 * 6.997924327850342
Epoch 420, val loss: 0.9644835591316223
Epoch 430, training loss: 0.1184447854757309 = 0.04839194566011429 + 0.01 * 7.005284309387207
Epoch 430, val loss: 0.9767013192176819
Epoch 440, training loss: 0.1132284551858902 = 0.04332035034894943 + 0.01 * 6.990810394287109
Epoch 440, val loss: 0.9885328412055969
Epoch 450, training loss: 0.10875716805458069 = 0.03890282288193703 + 0.01 * 6.9854350090026855
Epoch 450, val loss: 1.0000072717666626
Epoch 460, training loss: 0.10490620136260986 = 0.0350535586476326 + 0.01 * 6.985264778137207
Epoch 460, val loss: 1.0111712217330933
Epoch 470, training loss: 0.10147225856781006 = 0.03169718012213707 + 0.01 * 6.977508068084717
Epoch 470, val loss: 1.021966576576233
Epoch 480, training loss: 0.09875567257404327 = 0.028764862567186356 + 0.01 * 6.999081611633301
Epoch 480, val loss: 1.0324739217758179
Epoch 490, training loss: 0.09594675153493881 = 0.026201829314231873 + 0.01 * 6.97449254989624
Epoch 490, val loss: 1.0425714254379272
Epoch 500, training loss: 0.09365291893482208 = 0.023953670635819435 + 0.01 * 6.969925403594971
Epoch 500, val loss: 1.0523369312286377
Epoch 510, training loss: 0.09155849367380142 = 0.021973541006445885 + 0.01 * 6.958495140075684
Epoch 510, val loss: 1.06179678440094
Epoch 520, training loss: 0.08986087888479233 = 0.02022372931241989 + 0.01 * 6.963715076446533
Epoch 520, val loss: 1.070937156677246
Epoch 530, training loss: 0.08823448419570923 = 0.018673716112971306 + 0.01 * 6.9560770988464355
Epoch 530, val loss: 1.0797711610794067
Epoch 540, training loss: 0.08679044991731644 = 0.017295680940151215 + 0.01 * 6.949476718902588
Epoch 540, val loss: 1.08824622631073
Epoch 550, training loss: 0.08566339313983917 = 0.016066256910562515 + 0.01 * 6.959713935852051
Epoch 550, val loss: 1.096523404121399
Epoch 560, training loss: 0.08429155498743057 = 0.01496686041355133 + 0.01 * 6.932469844818115
Epoch 560, val loss: 1.1044762134552002
Epoch 570, training loss: 0.0832795649766922 = 0.013979634270071983 + 0.01 * 6.92999267578125
Epoch 570, val loss: 1.1121186017990112
Epoch 580, training loss: 0.08245698362588882 = 0.013089732266962528 + 0.01 * 6.93672513961792
Epoch 580, val loss: 1.119617223739624
Epoch 590, training loss: 0.08146379888057709 = 0.012285720556974411 + 0.01 * 6.917808532714844
Epoch 590, val loss: 1.1267529726028442
Epoch 600, training loss: 0.08062580972909927 = 0.011555665172636509 + 0.01 * 6.9070143699646
Epoch 600, val loss: 1.1337344646453857
Epoch 610, training loss: 0.08006510138511658 = 0.010892804712057114 + 0.01 * 6.917229175567627
Epoch 610, val loss: 1.1404320001602173
Epoch 620, training loss: 0.07919608801603317 = 0.010288850404322147 + 0.01 * 6.890724182128906
Epoch 620, val loss: 1.1469370126724243
Epoch 630, training loss: 0.0788947120308876 = 0.009736328385770321 + 0.01 * 6.915838718414307
Epoch 630, val loss: 1.153290867805481
Epoch 640, training loss: 0.07819604128599167 = 0.009230973199009895 + 0.01 * 6.896507263183594
Epoch 640, val loss: 1.159346580505371
Epoch 650, training loss: 0.07758203148841858 = 0.00876630563288927 + 0.01 * 6.881572723388672
Epoch 650, val loss: 1.1652886867523193
Epoch 660, training loss: 0.07710887491703033 = 0.008338148705661297 + 0.01 * 6.877072811126709
Epoch 660, val loss: 1.1709932088851929
Epoch 670, training loss: 0.07654426246881485 = 0.007942999713122845 + 0.01 * 6.860126495361328
Epoch 670, val loss: 1.1766085624694824
Epoch 680, training loss: 0.07637234777212143 = 0.007578119169920683 + 0.01 * 6.879422664642334
Epoch 680, val loss: 1.1821397542953491
Epoch 690, training loss: 0.07576137036085129 = 0.007241098675876856 + 0.01 * 6.85202693939209
Epoch 690, val loss: 1.1873332262039185
Epoch 700, training loss: 0.0755843073129654 = 0.006928207352757454 + 0.01 * 6.865610599517822
Epoch 700, val loss: 1.1925246715545654
Epoch 710, training loss: 0.07504069060087204 = 0.006637266371399164 + 0.01 * 6.8403425216674805
Epoch 710, val loss: 1.1974462270736694
Epoch 720, training loss: 0.07464972883462906 = 0.006366300862282515 + 0.01 * 6.828342914581299
Epoch 720, val loss: 1.2023921012878418
Epoch 730, training loss: 0.0745549201965332 = 0.006113340612500906 + 0.01 * 6.844158172607422
Epoch 730, val loss: 1.2072193622589111
Epoch 740, training loss: 0.07419061660766602 = 0.0058772023767232895 + 0.01 * 6.831341743469238
Epoch 740, val loss: 1.2117584943771362
Epoch 750, training loss: 0.07393903285264969 = 0.005656315479427576 + 0.01 * 6.828271389007568
Epoch 750, val loss: 1.2162224054336548
Epoch 760, training loss: 0.07373425364494324 = 0.005449242424219847 + 0.01 * 6.828501224517822
Epoch 760, val loss: 1.220685362815857
Epoch 770, training loss: 0.07349778711795807 = 0.005254985764622688 + 0.01 * 6.824280738830566
Epoch 770, val loss: 1.2249470949172974
Epoch 780, training loss: 0.07332605123519897 = 0.005072423256933689 + 0.01 * 6.825362682342529
Epoch 780, val loss: 1.2290904521942139
Epoch 790, training loss: 0.07298087328672409 = 0.004900605417788029 + 0.01 * 6.8080267906188965
Epoch 790, val loss: 1.233170747756958
Epoch 800, training loss: 0.07285680621862411 = 0.004738689865916967 + 0.01 * 6.811811923980713
Epoch 800, val loss: 1.23711097240448
Epoch 810, training loss: 0.07255980372428894 = 0.0045863548293709755 + 0.01 * 6.797345161437988
Epoch 810, val loss: 1.2409594058990479
Epoch 820, training loss: 0.07234036177396774 = 0.004442267119884491 + 0.01 * 6.789809703826904
Epoch 820, val loss: 1.2447144985198975
Epoch 830, training loss: 0.07214903831481934 = 0.004306044429540634 + 0.01 * 6.784299373626709
Epoch 830, val loss: 1.2484129667282104
Epoch 840, training loss: 0.07209153473377228 = 0.00417727418243885 + 0.01 * 6.791426658630371
Epoch 840, val loss: 1.252015233039856
Epoch 850, training loss: 0.07202916592359543 = 0.004055181983858347 + 0.01 * 6.797399044036865
Epoch 850, val loss: 1.255476713180542
Epoch 860, training loss: 0.07169189304113388 = 0.003939588088542223 + 0.01 * 6.775230407714844
Epoch 860, val loss: 1.2589017152786255
Epoch 870, training loss: 0.0715102031826973 = 0.0038299146108329296 + 0.01 * 6.768029689788818
Epoch 870, val loss: 1.2622534036636353
Epoch 880, training loss: 0.07140276581048965 = 0.003725633956491947 + 0.01 * 6.76771354675293
Epoch 880, val loss: 1.265543818473816
Epoch 890, training loss: 0.07139518857002258 = 0.0036266236566007137 + 0.01 * 6.776856899261475
Epoch 890, val loss: 1.2686752080917358
Epoch 900, training loss: 0.0713336318731308 = 0.003532242961227894 + 0.01 * 6.780138969421387
Epoch 900, val loss: 1.2717998027801514
Epoch 910, training loss: 0.07112082093954086 = 0.003442582441493869 + 0.01 * 6.767823696136475
Epoch 910, val loss: 1.274722695350647
Epoch 920, training loss: 0.07098467648029327 = 0.0033571470994502306 + 0.01 * 6.762753009796143
Epoch 920, val loss: 1.2776966094970703
Epoch 930, training loss: 0.07088704407215118 = 0.003275423776358366 + 0.01 * 6.761161804199219
Epoch 930, val loss: 1.2805899381637573
Epoch 940, training loss: 0.07083455473184586 = 0.0031975633464753628 + 0.01 * 6.763699054718018
Epoch 940, val loss: 1.2833678722381592
Epoch 950, training loss: 0.07062769681215286 = 0.0031229290179908276 + 0.01 * 6.750477313995361
Epoch 950, val loss: 1.2861270904541016
Epoch 960, training loss: 0.07069241255521774 = 0.003051653504371643 + 0.01 * 6.764075756072998
Epoch 960, val loss: 1.2888660430908203
Epoch 970, training loss: 0.07038737833499908 = 0.002983459737151861 + 0.01 * 6.740392208099365
Epoch 970, val loss: 1.2914642095565796
Epoch 980, training loss: 0.070550337433815 = 0.0029181642457842827 + 0.01 * 6.763217449188232
Epoch 980, val loss: 1.2940053939819336
Epoch 990, training loss: 0.07030320167541504 = 0.002855537459254265 + 0.01 * 6.744766712188721
Epoch 990, val loss: 1.296462893486023
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.0413529872894287 = 1.9553849697113037 + 0.01 * 8.596794128417969
Epoch 0, val loss: 1.949763536453247
Epoch 10, training loss: 2.0309534072875977 = 1.9449862241744995 + 0.01 * 8.596728324890137
Epoch 10, val loss: 1.9390830993652344
Epoch 20, training loss: 2.0180153846740723 = 1.9320508241653442 + 0.01 * 8.59644889831543
Epoch 20, val loss: 1.9257394075393677
Epoch 30, training loss: 1.9994415044784546 = 1.9134862422943115 + 0.01 * 8.5955228805542
Epoch 30, val loss: 1.9064172506332397
Epoch 40, training loss: 1.9714549779891968 = 1.8855534791946411 + 0.01 * 8.590150833129883
Epoch 40, val loss: 1.8774828910827637
Epoch 50, training loss: 1.9317936897277832 = 1.8462278842926025 + 0.01 * 8.556584358215332
Epoch 50, val loss: 1.8387376070022583
Epoch 60, training loss: 1.8871201276779175 = 1.8031091690063477 + 0.01 * 8.401100158691406
Epoch 60, val loss: 1.8010419607162476
Epoch 70, training loss: 1.8499598503112793 = 1.7679316997528076 + 0.01 * 8.20280933380127
Epoch 70, val loss: 1.7741690874099731
Epoch 80, training loss: 1.8046321868896484 = 1.725414752960205 + 0.01 * 7.921749114990234
Epoch 80, val loss: 1.7376115322113037
Epoch 90, training loss: 1.7425460815429688 = 1.6664581298828125 + 0.01 * 7.608798503875732
Epoch 90, val loss: 1.6847609281539917
Epoch 100, training loss: 1.6628296375274658 = 1.5883500576019287 + 0.01 * 7.447953701019287
Epoch 100, val loss: 1.6166294813156128
Epoch 110, training loss: 1.5713385343551636 = 1.4973210096359253 + 0.01 * 7.401750087738037
Epoch 110, val loss: 1.5402979850769043
Epoch 120, training loss: 1.478842854499817 = 1.4054244756698608 + 0.01 * 7.3418426513671875
Epoch 120, val loss: 1.4659613370895386
Epoch 130, training loss: 1.3878700733184814 = 1.3150609731674194 + 0.01 * 7.280908107757568
Epoch 130, val loss: 1.3939800262451172
Epoch 140, training loss: 1.2941346168518066 = 1.2218672037124634 + 0.01 * 7.226735591888428
Epoch 140, val loss: 1.3193787336349487
Epoch 150, training loss: 1.1951783895492554 = 1.1233084201812744 + 0.01 * 7.187002182006836
Epoch 150, val loss: 1.2401379346847534
Epoch 160, training loss: 1.0932904481887817 = 1.0217161178588867 + 0.01 * 7.157431125640869
Epoch 160, val loss: 1.1589993238449097
Epoch 170, training loss: 0.9940906763076782 = 0.9227385520935059 + 0.01 * 7.135210990905762
Epoch 170, val loss: 1.0813186168670654
Epoch 180, training loss: 0.9033793210983276 = 0.8321349024772644 + 0.01 * 7.124443054199219
Epoch 180, val loss: 1.0114856958389282
Epoch 190, training loss: 0.8243818283081055 = 0.7532183527946472 + 0.01 * 7.116349220275879
Epoch 190, val loss: 0.9525184035301208
Epoch 200, training loss: 0.75684654712677 = 0.6857452392578125 + 0.01 * 7.110128402709961
Epoch 200, val loss: 0.9049411416053772
Epoch 210, training loss: 0.6980280876159668 = 0.6269724369049072 + 0.01 * 7.105567932128906
Epoch 210, val loss: 0.867215096950531
Epoch 220, training loss: 0.6450018882751465 = 0.5739864110946655 + 0.01 * 7.101548671722412
Epoch 220, val loss: 0.8370892405509949
Epoch 230, training loss: 0.5959988236427307 = 0.5250270962715149 + 0.01 * 7.097171306610107
Epoch 230, val loss: 0.8131383061408997
Epoch 240, training loss: 0.550311267375946 = 0.4793934226036072 + 0.01 * 7.091784477233887
Epoch 240, val loss: 0.7945820689201355
Epoch 250, training loss: 0.5077149868011475 = 0.4368385672569275 + 0.01 * 7.087638854980469
Epoch 250, val loss: 0.7807363867759705
Epoch 260, training loss: 0.4681161046028137 = 0.39730843901634216 + 0.01 * 7.080765247344971
Epoch 260, val loss: 0.771117627620697
Epoch 270, training loss: 0.4314339756965637 = 0.36068063974380493 + 0.01 * 7.075332164764404
Epoch 270, val loss: 0.7655896544456482
Epoch 280, training loss: 0.3973098695278168 = 0.3265860378742218 + 0.01 * 7.072384357452393
Epoch 280, val loss: 0.7635486721992493
Epoch 290, training loss: 0.365200936794281 = 0.2945259213447571 + 0.01 * 7.067502498626709
Epoch 290, val loss: 0.7643089294433594
Epoch 300, training loss: 0.3346189856529236 = 0.26402413845062256 + 0.01 * 7.059484958648682
Epoch 300, val loss: 0.7667996883392334
Epoch 310, training loss: 0.305354505777359 = 0.23481661081314087 + 0.01 * 7.0537896156311035
Epoch 310, val loss: 0.7705606818199158
Epoch 320, training loss: 0.2776927947998047 = 0.2070535123348236 + 0.01 * 7.063926696777344
Epoch 320, val loss: 0.7754502296447754
Epoch 330, training loss: 0.25173309445381165 = 0.18124617636203766 + 0.01 * 7.048692226409912
Epoch 330, val loss: 0.7812443375587463
Epoch 340, training loss: 0.22830364108085632 = 0.1579037606716156 + 0.01 * 7.039987087249756
Epoch 340, val loss: 0.7883121967315674
Epoch 350, training loss: 0.20764103531837463 = 0.13732753694057465 + 0.01 * 7.031350612640381
Epoch 350, val loss: 0.7968151569366455
Epoch 360, training loss: 0.1897839903831482 = 0.11949029564857483 + 0.01 * 7.029368877410889
Epoch 360, val loss: 0.8067286014556885
Epoch 370, training loss: 0.17436689138412476 = 0.10415700823068619 + 0.01 * 7.020989418029785
Epoch 370, val loss: 0.8177903890609741
Epoch 380, training loss: 0.1611729860305786 = 0.09098965674638748 + 0.01 * 7.018332481384277
Epoch 380, val loss: 0.8298050761222839
Epoch 390, training loss: 0.14983922243118286 = 0.0796903520822525 + 0.01 * 7.014886379241943
Epoch 390, val loss: 0.8423129320144653
Epoch 400, training loss: 0.14005421102046967 = 0.06999006867408752 + 0.01 * 7.006414413452148
Epoch 400, val loss: 0.8550202250480652
Epoch 410, training loss: 0.1316567212343216 = 0.06166144460439682 + 0.01 * 6.999527931213379
Epoch 410, val loss: 0.8677630424499512
Epoch 420, training loss: 0.12446156144142151 = 0.05451333522796631 + 0.01 * 6.994822978973389
Epoch 420, val loss: 0.8803691267967224
Epoch 430, training loss: 0.11832687258720398 = 0.04837983846664429 + 0.01 * 6.994703769683838
Epoch 430, val loss: 0.892717719078064
Epoch 440, training loss: 0.11302238702774048 = 0.04311532899737358 + 0.01 * 6.990705966949463
Epoch 440, val loss: 0.9047853946685791
Epoch 450, training loss: 0.10859297215938568 = 0.03858635202050209 + 0.01 * 7.000661849975586
Epoch 450, val loss: 0.9165153503417969
Epoch 460, training loss: 0.10457758605480194 = 0.03468684107065201 + 0.01 * 6.98907470703125
Epoch 460, val loss: 0.9279399514198303
Epoch 470, training loss: 0.1011083722114563 = 0.03131411597132683 + 0.01 * 6.97942590713501
Epoch 470, val loss: 0.939032256603241
Epoch 480, training loss: 0.09810474514961243 = 0.028381094336509705 + 0.01 * 6.972364902496338
Epoch 480, val loss: 0.9499582648277283
Epoch 490, training loss: 0.09545242786407471 = 0.0258169025182724 + 0.01 * 6.963552474975586
Epoch 490, val loss: 0.9607098698616028
Epoch 500, training loss: 0.09340814501047134 = 0.02356366068124771 + 0.01 * 6.984448432922363
Epoch 500, val loss: 0.9713114500045776
Epoch 510, training loss: 0.0912490040063858 = 0.02157973125576973 + 0.01 * 6.966928005218506
Epoch 510, val loss: 0.9817459583282471
Epoch 520, training loss: 0.08942188322544098 = 0.019824352115392685 + 0.01 * 6.95975399017334
Epoch 520, val loss: 0.9919224381446838
Epoch 530, training loss: 0.08775408565998077 = 0.018268780782818794 + 0.01 * 6.948530673980713
Epoch 530, val loss: 1.0019184350967407
Epoch 540, training loss: 0.08627749979496002 = 0.01688385382294655 + 0.01 * 6.939364910125732
Epoch 540, val loss: 1.0116561651229858
Epoch 550, training loss: 0.08522424101829529 = 0.01564871519804001 + 0.01 * 6.957552433013916
Epoch 550, val loss: 1.0211604833602905
Epoch 560, training loss: 0.08391961455345154 = 0.014545856043696404 + 0.01 * 6.937376022338867
Epoch 560, val loss: 1.0303239822387695
Epoch 570, training loss: 0.08295030891895294 = 0.01355559378862381 + 0.01 * 6.93947172164917
Epoch 570, val loss: 1.039282202720642
Epoch 580, training loss: 0.08190159499645233 = 0.0126649746671319 + 0.01 * 6.923662185668945
Epoch 580, val loss: 1.047943353652954
Epoch 590, training loss: 0.08103533834218979 = 0.01186089776456356 + 0.01 * 6.917443752288818
Epoch 590, val loss: 1.0563969612121582
Epoch 600, training loss: 0.0803193524479866 = 0.011134130880236626 + 0.01 * 6.918522357940674
Epoch 600, val loss: 1.064512014389038
Epoch 610, training loss: 0.07960512489080429 = 0.010473937727510929 + 0.01 * 6.913118839263916
Epoch 610, val loss: 1.0723968744277954
Epoch 620, training loss: 0.07907889783382416 = 0.009871921502053738 + 0.01 * 6.9206976890563965
Epoch 620, val loss: 1.0800586938858032
Epoch 630, training loss: 0.07835245877504349 = 0.009322898462414742 + 0.01 * 6.902956485748291
Epoch 630, val loss: 1.0873948335647583
Epoch 640, training loss: 0.0778997465968132 = 0.00882036704570055 + 0.01 * 6.907937526702881
Epoch 640, val loss: 1.094545602798462
Epoch 650, training loss: 0.07757915556430817 = 0.0083595160394907 + 0.01 * 6.921964168548584
Epoch 650, val loss: 1.101439118385315
Epoch 660, training loss: 0.0769452452659607 = 0.007938139140605927 + 0.01 * 6.900710582733154
Epoch 660, val loss: 1.108071208000183
Epoch 670, training loss: 0.0765567198395729 = 0.007548919878900051 + 0.01 * 6.900779724121094
Epoch 670, val loss: 1.1145306825637817
Epoch 680, training loss: 0.07598625123500824 = 0.007189970463514328 + 0.01 * 6.879628658294678
Epoch 680, val loss: 1.1208146810531616
Epoch 690, training loss: 0.07568631321191788 = 0.006857310887426138 + 0.01 * 6.882900238037109
Epoch 690, val loss: 1.1269844770431519
Epoch 700, training loss: 0.07537084817886353 = 0.006548755802214146 + 0.01 * 6.882209300994873
Epoch 700, val loss: 1.1329666376113892
Epoch 710, training loss: 0.07522528618574142 = 0.006262971553951502 + 0.01 * 6.896231651306152
Epoch 710, val loss: 1.1388732194900513
Epoch 720, training loss: 0.07455243170261383 = 0.005997430998831987 + 0.01 * 6.855499744415283
Epoch 720, val loss: 1.1445409059524536
Epoch 730, training loss: 0.07444432377815247 = 0.00574968196451664 + 0.01 * 6.869464874267578
Epoch 730, val loss: 1.1501258611679077
Epoch 740, training loss: 0.07406032085418701 = 0.005519940983504057 + 0.01 * 6.854037761688232
Epoch 740, val loss: 1.155516266822815
Epoch 750, training loss: 0.07372021675109863 = 0.005304405931383371 + 0.01 * 6.84158182144165
Epoch 750, val loss: 1.1607667207717896
Epoch 760, training loss: 0.07360813766717911 = 0.005103274714201689 + 0.01 * 6.8504862785339355
Epoch 760, val loss: 1.16583251953125
Epoch 770, training loss: 0.07354000210762024 = 0.0049151284620165825 + 0.01 * 6.86248779296875
Epoch 770, val loss: 1.1707704067230225
Epoch 780, training loss: 0.07310539484024048 = 0.004739486612379551 + 0.01 * 6.836590766906738
Epoch 780, val loss: 1.1755138635635376
Epoch 790, training loss: 0.07284153252840042 = 0.00457371911033988 + 0.01 * 6.826781749725342
Epoch 790, val loss: 1.1802403926849365
Epoch 800, training loss: 0.07268009334802628 = 0.004418219905346632 + 0.01 * 6.826187610626221
Epoch 800, val loss: 1.184715747833252
Epoch 810, training loss: 0.07268617302179337 = 0.00427160132676363 + 0.01 * 6.841456890106201
Epoch 810, val loss: 1.1891528367996216
Epoch 820, training loss: 0.07237452268600464 = 0.00413340050727129 + 0.01 * 6.8241119384765625
Epoch 820, val loss: 1.193366527557373
Epoch 830, training loss: 0.07207523286342621 = 0.004003033973276615 + 0.01 * 6.807220458984375
Epoch 830, val loss: 1.1975868940353394
Epoch 840, training loss: 0.07205749303102493 = 0.0038799468893557787 + 0.01 * 6.817754745483398
Epoch 840, val loss: 1.2016528844833374
Epoch 850, training loss: 0.07188313454389572 = 0.0037635215558111668 + 0.01 * 6.811961650848389
Epoch 850, val loss: 1.2055492401123047
Epoch 860, training loss: 0.07178474217653275 = 0.0036533584352582693 + 0.01 * 6.813138961791992
Epoch 860, val loss: 1.2094043493270874
Epoch 870, training loss: 0.07166271656751633 = 0.0035485136322677135 + 0.01 * 6.811420917510986
Epoch 870, val loss: 1.2131050825119019
Epoch 880, training loss: 0.07166474312543869 = 0.0034494774881750345 + 0.01 * 6.821527004241943
Epoch 880, val loss: 1.2167826890945435
Epoch 890, training loss: 0.07127662003040314 = 0.0033552185632288456 + 0.01 * 6.792140483856201
Epoch 890, val loss: 1.2202680110931396
Epoch 900, training loss: 0.07121960818767548 = 0.0032657040283083916 + 0.01 * 6.795390605926514
Epoch 900, val loss: 1.223833441734314
Epoch 910, training loss: 0.07103415578603745 = 0.0031804891768842936 + 0.01 * 6.785366535186768
Epoch 910, val loss: 1.2271620035171509
Epoch 920, training loss: 0.07104060053825378 = 0.0030991630628705025 + 0.01 * 6.7941436767578125
Epoch 920, val loss: 1.2305501699447632
Epoch 930, training loss: 0.0709247812628746 = 0.0030221552588045597 + 0.01 * 6.790262699127197
Epoch 930, val loss: 1.2336483001708984
Epoch 940, training loss: 0.0707242339849472 = 0.0029484929982572794 + 0.01 * 6.77757453918457
Epoch 940, val loss: 1.2368237972259521
Epoch 950, training loss: 0.07057397812604904 = 0.002878227038308978 + 0.01 * 6.769575595855713
Epoch 950, val loss: 1.2398682832717896
Epoch 960, training loss: 0.07051826268434525 = 0.002810864010825753 + 0.01 * 6.770740032196045
Epoch 960, val loss: 1.2430264949798584
Epoch 970, training loss: 0.07032579928636551 = 0.0027469524648040533 + 0.01 * 6.757884502410889
Epoch 970, val loss: 1.2458525896072388
Epoch 980, training loss: 0.07048368453979492 = 0.002685056533664465 + 0.01 * 6.779862880706787
Epoch 980, val loss: 1.2488501071929932
Epoch 990, training loss: 0.0704009011387825 = 0.0026264844927936792 + 0.01 * 6.777441501617432
Epoch 990, val loss: 1.2515771389007568
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8360569319978914
The final CL Acc:0.80000, 0.00605, The final GNN Acc:0.83606, 0.00086
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10572])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0412707328796387 = 1.9553022384643555 + 0.01 * 8.5968599319458
Epoch 0, val loss: 1.9570602178573608
Epoch 10, training loss: 2.0307114124298096 = 1.9447431564331055 + 0.01 * 8.596820831298828
Epoch 10, val loss: 1.9467811584472656
Epoch 20, training loss: 2.0178310871124268 = 1.9318642616271973 + 0.01 * 8.59667682647705
Epoch 20, val loss: 1.933763027191162
Epoch 30, training loss: 2.0000429153442383 = 1.9140803813934326 + 0.01 * 8.596259117126465
Epoch 30, val loss: 1.9154012203216553
Epoch 40, training loss: 1.9742298126220703 = 1.888287901878357 + 0.01 * 8.594197273254395
Epoch 40, val loss: 1.8888278007507324
Epoch 50, training loss: 1.938920259475708 = 1.8531296253204346 + 0.01 * 8.579061508178711
Epoch 50, val loss: 1.85410475730896
Epoch 60, training loss: 1.90012526512146 = 1.815083622932434 + 0.01 * 8.504170417785645
Epoch 60, val loss: 1.820702314376831
Epoch 70, training loss: 1.8685333728790283 = 1.7850415706634521 + 0.01 * 8.349174499511719
Epoch 70, val loss: 1.797730565071106
Epoch 80, training loss: 1.8324304819107056 = 1.7500336170196533 + 0.01 * 8.239686012268066
Epoch 80, val loss: 1.7673587799072266
Epoch 90, training loss: 1.7810933589935303 = 1.7006477117538452 + 0.01 * 8.044568061828613
Epoch 90, val loss: 1.7251131534576416
Epoch 100, training loss: 1.7093863487243652 = 1.6312389373779297 + 0.01 * 7.8147430419921875
Epoch 100, val loss: 1.6681616306304932
Epoch 110, training loss: 1.6180789470672607 = 1.5417066812515259 + 0.01 * 7.6372270584106445
Epoch 110, val loss: 1.595717191696167
Epoch 120, training loss: 1.5183404684066772 = 1.4424035549163818 + 0.01 * 7.593689441680908
Epoch 120, val loss: 1.5180892944335938
Epoch 130, training loss: 1.4178385734558105 = 1.3426005840301514 + 0.01 * 7.5238037109375
Epoch 130, val loss: 1.4430431127548218
Epoch 140, training loss: 1.3189411163330078 = 1.2444653511047363 + 0.01 * 7.447572231292725
Epoch 140, val loss: 1.3732900619506836
Epoch 150, training loss: 1.221010684967041 = 1.146918535232544 + 0.01 * 7.409209251403809
Epoch 150, val loss: 1.3058264255523682
Epoch 160, training loss: 1.1235445737838745 = 1.0495061874389648 + 0.01 * 7.403841495513916
Epoch 160, val loss: 1.2395296096801758
Epoch 170, training loss: 1.0281579494476318 = 0.9541959762573242 + 0.01 * 7.396197319030762
Epoch 170, val loss: 1.1745001077651978
Epoch 180, training loss: 0.9393285512924194 = 0.8654506206512451 + 0.01 * 7.387794017791748
Epoch 180, val loss: 1.114716649055481
Epoch 190, training loss: 0.8614071607589722 = 0.7876802086830139 + 0.01 * 7.372695446014404
Epoch 190, val loss: 1.0640883445739746
Epoch 200, training loss: 0.7959296107292175 = 0.7224588394165039 + 0.01 * 7.34707498550415
Epoch 200, val loss: 1.0248134136199951
Epoch 210, training loss: 0.7412290573120117 = 0.6680475473403931 + 0.01 * 7.318150043487549
Epoch 210, val loss: 0.9961227774620056
Epoch 220, training loss: 0.694124698638916 = 0.6213081479072571 + 0.01 * 7.281655311584473
Epoch 220, val loss: 0.9755849242210388
Epoch 230, training loss: 0.6520078182220459 = 0.5795149207115173 + 0.01 * 7.2492899894714355
Epoch 230, val loss: 0.9608497023582458
Epoch 240, training loss: 0.6133760809898376 = 0.5409974455833435 + 0.01 * 7.237862586975098
Epoch 240, val loss: 0.950118899345398
Epoch 250, training loss: 0.5770403742790222 = 0.5048736333847046 + 0.01 * 7.216674327850342
Epoch 250, val loss: 0.9425862431526184
Epoch 260, training loss: 0.542432963848114 = 0.47039249539375305 + 0.01 * 7.204048156738281
Epoch 260, val loss: 0.9376470446586609
Epoch 270, training loss: 0.5086990594863892 = 0.43671882152557373 + 0.01 * 7.19802713394165
Epoch 270, val loss: 0.934630274772644
Epoch 280, training loss: 0.47488129138946533 = 0.4030306339263916 + 0.01 * 7.185066223144531
Epoch 280, val loss: 0.9327325224876404
Epoch 290, training loss: 0.44060826301574707 = 0.36883360147476196 + 0.01 * 7.177465915679932
Epoch 290, val loss: 0.9315593242645264
Epoch 300, training loss: 0.4058568775653839 = 0.3341796398162842 + 0.01 * 7.167724132537842
Epoch 300, val loss: 0.9315245747566223
Epoch 310, training loss: 0.37120693922042847 = 0.29957348108291626 + 0.01 * 7.1633477210998535
Epoch 310, val loss: 0.9331002831459045
Epoch 320, training loss: 0.3373170495033264 = 0.26570481061935425 + 0.01 * 7.161223411560059
Epoch 320, val loss: 0.9367488622665405
Epoch 330, training loss: 0.3047798275947571 = 0.2332572340965271 + 0.01 * 7.152259826660156
Epoch 330, val loss: 0.9426673650741577
Epoch 340, training loss: 0.2743840217590332 = 0.20293711125850677 + 0.01 * 7.144690990447998
Epoch 340, val loss: 0.9509146213531494
Epoch 350, training loss: 0.2469726800918579 = 0.17538203299045563 + 0.01 * 7.1590657234191895
Epoch 350, val loss: 0.96128910779953
Epoch 360, training loss: 0.2224154770374298 = 0.15102311968803406 + 0.01 * 7.139235973358154
Epoch 360, val loss: 0.9736661314964294
Epoch 370, training loss: 0.20124298334121704 = 0.12992477416992188 + 0.01 * 7.131821155548096
Epoch 370, val loss: 0.98792964220047
Epoch 380, training loss: 0.18326014280319214 = 0.11196321994066238 + 0.01 * 7.129692077636719
Epoch 380, val loss: 1.0037587881088257
Epoch 390, training loss: 0.16813363134860992 = 0.09686850011348724 + 0.01 * 7.1265130043029785
Epoch 390, val loss: 1.0206677913665771
Epoch 400, training loss: 0.1554623246192932 = 0.08424291759729385 + 0.01 * 7.1219401359558105
Epoch 400, val loss: 1.0382165908813477
Epoch 410, training loss: 0.14485082030296326 = 0.0736876130104065 + 0.01 * 7.116320610046387
Epoch 410, val loss: 1.0561023950576782
Epoch 420, training loss: 0.1358778476715088 = 0.06482256203889847 + 0.01 * 7.105529308319092
Epoch 420, val loss: 1.074047327041626
Epoch 430, training loss: 0.12828329205513 = 0.057339269667863846 + 0.01 * 7.094401836395264
Epoch 430, val loss: 1.091788649559021
Epoch 440, training loss: 0.12185024470090866 = 0.05098588764667511 + 0.01 * 7.086435794830322
Epoch 440, val loss: 1.1092039346694946
Epoch 450, training loss: 0.11633797734975815 = 0.04555506259202957 + 0.01 * 7.078291416168213
Epoch 450, val loss: 1.126288652420044
Epoch 460, training loss: 0.11193934082984924 = 0.04088813439011574 + 0.01 * 7.1051201820373535
Epoch 460, val loss: 1.142897367477417
Epoch 470, training loss: 0.10767380148172379 = 0.03686244785785675 + 0.01 * 7.081135272979736
Epoch 470, val loss: 1.159026026725769
Epoch 480, training loss: 0.10394032299518585 = 0.03336755558848381 + 0.01 * 7.057276725769043
Epoch 480, val loss: 1.1747061014175415
Epoch 490, training loss: 0.10108266770839691 = 0.030319418758153915 + 0.01 * 7.076324462890625
Epoch 490, val loss: 1.1899739503860474
Epoch 500, training loss: 0.09820310771465302 = 0.027657603845000267 + 0.01 * 7.0545501708984375
Epoch 500, val loss: 1.204790472984314
Epoch 510, training loss: 0.09591853618621826 = 0.025321919471025467 + 0.01 * 7.059662342071533
Epoch 510, val loss: 1.2191598415374756
Epoch 520, training loss: 0.09370002150535583 = 0.023267798125743866 + 0.01 * 7.043222427368164
Epoch 520, val loss: 1.2330836057662964
Epoch 530, training loss: 0.09174074232578278 = 0.021453676745295525 + 0.01 * 7.0287065505981445
Epoch 530, val loss: 1.2466230392456055
Epoch 540, training loss: 0.09010612964630127 = 0.019844841212034225 + 0.01 * 7.026129722595215
Epoch 540, val loss: 1.2597918510437012
Epoch 550, training loss: 0.08863954991102219 = 0.018413037061691284 + 0.01 * 7.022651195526123
Epoch 550, val loss: 1.272600531578064
Epoch 560, training loss: 0.08719378709793091 = 0.0171362292021513 + 0.01 * 7.005755424499512
Epoch 560, val loss: 1.2850173711776733
Epoch 570, training loss: 0.08605942875146866 = 0.01599382609128952 + 0.01 * 7.006560325622559
Epoch 570, val loss: 1.2970761060714722
Epoch 580, training loss: 0.08499206602573395 = 0.014967580325901508 + 0.01 * 7.002448558807373
Epoch 580, val loss: 1.3087928295135498
Epoch 590, training loss: 0.08395126461982727 = 0.014042031019926071 + 0.01 * 6.990922927856445
Epoch 590, val loss: 1.3201909065246582
Epoch 600, training loss: 0.0830395296216011 = 0.013205231167376041 + 0.01 * 6.983429908752441
Epoch 600, val loss: 1.3312596082687378
Epoch 610, training loss: 0.08227179944515228 = 0.012447209097445011 + 0.01 * 6.98245906829834
Epoch 610, val loss: 1.3419665098190308
Epoch 620, training loss: 0.0815480574965477 = 0.011757115833461285 + 0.01 * 6.979094505310059
Epoch 620, val loss: 1.3524376153945923
Epoch 630, training loss: 0.08085097372531891 = 0.011126821860671043 + 0.01 * 6.972415447235107
Epoch 630, val loss: 1.3626223802566528
Epoch 640, training loss: 0.0801667720079422 = 0.010551034472882748 + 0.01 * 6.961574077606201
Epoch 640, val loss: 1.3725361824035645
Epoch 650, training loss: 0.0798008069396019 = 0.010022847913205624 + 0.01 * 6.977795600891113
Epoch 650, val loss: 1.3821449279785156
Epoch 660, training loss: 0.07913126796483994 = 0.009538047015666962 + 0.01 * 6.959321975708008
Epoch 660, val loss: 1.3914904594421387
Epoch 670, training loss: 0.0784478411078453 = 0.00909065455198288 + 0.01 * 6.935718536376953
Epoch 670, val loss: 1.4006134271621704
Epoch 680, training loss: 0.07817699015140533 = 0.00867694802582264 + 0.01 * 6.950004577636719
Epoch 680, val loss: 1.4094980955123901
Epoch 690, training loss: 0.07757175713777542 = 0.008294589817523956 + 0.01 * 6.9277167320251465
Epoch 690, val loss: 1.4181562662124634
Epoch 700, training loss: 0.07730673998594284 = 0.007939549162983894 + 0.01 * 6.9367194175720215
Epoch 700, val loss: 1.426636815071106
Epoch 710, training loss: 0.0769672617316246 = 0.007609890773892403 + 0.01 * 6.935737133026123
Epoch 710, val loss: 1.4348297119140625
Epoch 720, training loss: 0.07650909572839737 = 0.007303357124328613 + 0.01 * 6.920573711395264
Epoch 720, val loss: 1.4428644180297852
Epoch 730, training loss: 0.07638198137283325 = 0.007017037831246853 + 0.01 * 6.93649435043335
Epoch 730, val loss: 1.4507126808166504
Epoch 740, training loss: 0.07590603083372116 = 0.006749570835381746 + 0.01 * 6.915646076202393
Epoch 740, val loss: 1.458363652229309
Epoch 750, training loss: 0.07547655701637268 = 0.006499417591840029 + 0.01 * 6.897714138031006
Epoch 750, val loss: 1.4658360481262207
Epoch 760, training loss: 0.0755225345492363 = 0.00626476900652051 + 0.01 * 6.925776958465576
Epoch 760, val loss: 1.4731289148330688
Epoch 770, training loss: 0.07507971674203873 = 0.006044342648237944 + 0.01 * 6.903537750244141
Epoch 770, val loss: 1.4802600145339966
Epoch 780, training loss: 0.07475017756223679 = 0.00583729799836874 + 0.01 * 6.8912882804870605
Epoch 780, val loss: 1.4872276782989502
Epoch 790, training loss: 0.07483303546905518 = 0.005642398726195097 + 0.01 * 6.919064044952393
Epoch 790, val loss: 1.4940320253372192
Epoch 800, training loss: 0.0743996649980545 = 0.00545861478894949 + 0.01 * 6.894105434417725
Epoch 800, val loss: 1.5006471872329712
Epoch 810, training loss: 0.07427340000867844 = 0.00528535433113575 + 0.01 * 6.898805141448975
Epoch 810, val loss: 1.5071144104003906
Epoch 820, training loss: 0.07384108752012253 = 0.005121620371937752 + 0.01 * 6.871946811676025
Epoch 820, val loss: 1.5134209394454956
Epoch 830, training loss: 0.07360950112342834 = 0.004966715816408396 + 0.01 * 6.864279270172119
Epoch 830, val loss: 1.5195930004119873
Epoch 840, training loss: 0.07358899712562561 = 0.004819747991859913 + 0.01 * 6.876924991607666
Epoch 840, val loss: 1.525632381439209
Epoch 850, training loss: 0.07336576282978058 = 0.004680778831243515 + 0.01 * 6.868498802185059
Epoch 850, val loss: 1.5315048694610596
Epoch 860, training loss: 0.07320030778646469 = 0.004548937082290649 + 0.01 * 6.865137577056885
Epoch 860, val loss: 1.5372629165649414
Epoch 870, training loss: 0.07300074398517609 = 0.004423470702022314 + 0.01 * 6.857727527618408
Epoch 870, val loss: 1.5428892374038696
Epoch 880, training loss: 0.07288375496864319 = 0.004304045811295509 + 0.01 * 6.85797119140625
Epoch 880, val loss: 1.5484387874603271
Epoch 890, training loss: 0.0726928561925888 = 0.00419069267809391 + 0.01 * 6.850216388702393
Epoch 890, val loss: 1.5538278818130493
Epoch 900, training loss: 0.07253441214561462 = 0.004082393366843462 + 0.01 * 6.84520149230957
Epoch 900, val loss: 1.559131383895874
Epoch 910, training loss: 0.07257743179798126 = 0.003979193978011608 + 0.01 * 6.859824180603027
Epoch 910, val loss: 1.564296007156372
Epoch 920, training loss: 0.07244404405355453 = 0.003881108947098255 + 0.01 * 6.856293678283691
Epoch 920, val loss: 1.569305181503296
Epoch 930, training loss: 0.07223071902990341 = 0.003787281224504113 + 0.01 * 6.844344139099121
Epoch 930, val loss: 1.5742790699005127
Epoch 940, training loss: 0.07200243324041367 = 0.003697626758366823 + 0.01 * 6.830480575561523
Epoch 940, val loss: 1.5791356563568115
Epoch 950, training loss: 0.07185319811105728 = 0.0036119078285992146 + 0.01 * 6.824129104614258
Epoch 950, val loss: 1.5838816165924072
Epoch 960, training loss: 0.07189704477787018 = 0.003529799170792103 + 0.01 * 6.836724758148193
Epoch 960, val loss: 1.5885188579559326
Epoch 970, training loss: 0.07209452986717224 = 0.003451101714745164 + 0.01 * 6.86434268951416
Epoch 970, val loss: 1.5930774211883545
Epoch 980, training loss: 0.07171821594238281 = 0.003375787753611803 + 0.01 * 6.834242343902588
Epoch 980, val loss: 1.597556471824646
Epoch 990, training loss: 0.0716734230518341 = 0.0033035825472325087 + 0.01 * 6.836984634399414
Epoch 990, val loss: 1.6019502878189087
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 2.0443761348724365 = 1.9584076404571533 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9621622562408447
Epoch 10, training loss: 2.0337905883789062 = 1.9478223323822021 + 0.01 * 8.596821784973145
Epoch 10, val loss: 1.9520184993743896
Epoch 20, training loss: 2.0206310749053955 = 1.9346641302108765 + 0.01 * 8.596689224243164
Epoch 20, val loss: 1.9388831853866577
Epoch 30, training loss: 2.002139091491699 = 1.9161765575408936 + 0.01 * 8.5962553024292
Epoch 30, val loss: 1.9200280904769897
Epoch 40, training loss: 1.9749599695205688 = 1.889021396636963 + 0.01 * 8.593859672546387
Epoch 40, val loss: 1.8924682140350342
Epoch 50, training loss: 1.9376863241195679 = 1.8519343137741089 + 0.01 * 8.575201988220215
Epoch 50, val loss: 1.8566128015518188
Epoch 60, training loss: 1.8974642753601074 = 1.8125861883163452 + 0.01 * 8.487808227539062
Epoch 60, val loss: 1.8232393264770508
Epoch 70, training loss: 1.8659242391586304 = 1.7824009656906128 + 0.01 * 8.352330207824707
Epoch 70, val loss: 1.8001089096069336
Epoch 80, training loss: 1.8300355672836304 = 1.7473310232162476 + 0.01 * 8.270458221435547
Epoch 80, val loss: 1.7685216665267944
Epoch 90, training loss: 1.7800158262252808 = 1.699251413345337 + 0.01 * 8.076436996459961
Epoch 90, val loss: 1.7256278991699219
Epoch 100, training loss: 1.710264801979065 = 1.6325790882110596 + 0.01 * 7.768570899963379
Epoch 100, val loss: 1.6688334941864014
Epoch 110, training loss: 1.6240333318710327 = 1.5475530624389648 + 0.01 * 7.6480231285095215
Epoch 110, val loss: 1.5983810424804688
Epoch 120, training loss: 1.5285978317260742 = 1.4527159929275513 + 0.01 * 7.5881876945495605
Epoch 120, val loss: 1.5221515893936157
Epoch 130, training loss: 1.4310224056243896 = 1.3556269407272339 + 0.01 * 7.539545059204102
Epoch 130, val loss: 1.4472283124923706
Epoch 140, training loss: 1.3316110372543335 = 1.256445288658142 + 0.01 * 7.51657247543335
Epoch 140, val loss: 1.3720343112945557
Epoch 150, training loss: 1.2300645112991333 = 1.155150055885315 + 0.01 * 7.491443157196045
Epoch 150, val loss: 1.2967065572738647
Epoch 160, training loss: 1.1290712356567383 = 1.0545766353607178 + 0.01 * 7.449463844299316
Epoch 160, val loss: 1.2227345705032349
Epoch 170, training loss: 1.032912254333496 = 0.9589077830314636 + 0.01 * 7.400451183319092
Epoch 170, val loss: 1.1533890962600708
Epoch 180, training loss: 0.9450469017028809 = 0.8713575005531311 + 0.01 * 7.368939399719238
Epoch 180, val loss: 1.0920320749282837
Epoch 190, training loss: 0.8667224645614624 = 0.7932354807853699 + 0.01 * 7.348701000213623
Epoch 190, val loss: 1.040730595588684
Epoch 200, training loss: 0.7976498007774353 = 0.7243388891220093 + 0.01 * 7.331090450286865
Epoch 200, val loss: 1.0001200437545776
Epoch 210, training loss: 0.7367433905601501 = 0.663582980632782 + 0.01 * 7.316039562225342
Epoch 210, val loss: 0.9694889783859253
Epoch 220, training loss: 0.6824975609779358 = 0.6094540357589722 + 0.01 * 7.304354190826416
Epoch 220, val loss: 0.947505533695221
Epoch 230, training loss: 0.6336685419082642 = 0.5604637861251831 + 0.01 * 7.3204731941223145
Epoch 230, val loss: 0.9325685501098633
Epoch 240, training loss: 0.5882998108863831 = 0.515367865562439 + 0.01 * 7.29319429397583
Epoch 240, val loss: 0.9229708909988403
Epoch 250, training loss: 0.5460659861564636 = 0.47316795587539673 + 0.01 * 7.289802551269531
Epoch 250, val loss: 0.9174554944038391
Epoch 260, training loss: 0.5060074925422668 = 0.4331420660018921 + 0.01 * 7.2865424156188965
Epoch 260, val loss: 0.9153646230697632
Epoch 270, training loss: 0.4678835868835449 = 0.3950510621070862 + 0.01 * 7.2832512855529785
Epoch 270, val loss: 0.9165024161338806
Epoch 280, training loss: 0.4318224787712097 = 0.35901719331741333 + 0.01 * 7.280527114868164
Epoch 280, val loss: 0.9212163686752319
Epoch 290, training loss: 0.3981153964996338 = 0.3253351151943207 + 0.01 * 7.278027534484863
Epoch 290, val loss: 0.9297666549682617
Epoch 300, training loss: 0.36697956919670105 = 0.2942197024822235 + 0.01 * 7.2759857177734375
Epoch 300, val loss: 0.9418914318084717
Epoch 310, training loss: 0.3384772539138794 = 0.2657022178173065 + 0.01 * 7.277503490447998
Epoch 310, val loss: 0.9570268392562866
Epoch 320, training loss: 0.3123564124107361 = 0.23963810503482819 + 0.01 * 7.271829128265381
Epoch 320, val loss: 0.9747040867805481
Epoch 330, training loss: 0.28844162821769714 = 0.2157687395811081 + 0.01 * 7.2672882080078125
Epoch 330, val loss: 0.9945588111877441
Epoch 340, training loss: 0.2665036916732788 = 0.193863645195961 + 0.01 * 7.264003276824951
Epoch 340, val loss: 1.0164369344711304
Epoch 350, training loss: 0.246357262134552 = 0.17372854053974152 + 0.01 * 7.262872695922852
Epoch 350, val loss: 1.04012930393219
Epoch 360, training loss: 0.22780129313468933 = 0.1552337259054184 + 0.01 * 7.256755828857422
Epoch 360, val loss: 1.0652716159820557
Epoch 370, training loss: 0.21081335842609406 = 0.1383007913827896 + 0.01 * 7.251256942749023
Epoch 370, val loss: 1.0915435552597046
Epoch 380, training loss: 0.19538754224777222 = 0.12288948893547058 + 0.01 * 7.2498064041137695
Epoch 380, val loss: 1.118499755859375
Epoch 390, training loss: 0.18139629065990448 = 0.10897549241781235 + 0.01 * 7.242079734802246
Epoch 390, val loss: 1.1458287239074707
Epoch 400, training loss: 0.1689382791519165 = 0.09651895612478256 + 0.01 * 7.241931915283203
Epoch 400, val loss: 1.1732218265533447
Epoch 410, training loss: 0.15770772099494934 = 0.08547118306159973 + 0.01 * 7.223654270172119
Epoch 410, val loss: 1.200310230255127
Epoch 420, training loss: 0.14803701639175415 = 0.07574183493852615 + 0.01 * 7.229519367218018
Epoch 420, val loss: 1.2269399166107178
Epoch 430, training loss: 0.13937383890151978 = 0.06724003702402115 + 0.01 * 7.213380813598633
Epoch 430, val loss: 1.252806544303894
Epoch 440, training loss: 0.13187283277511597 = 0.059835389256477356 + 0.01 * 7.203744411468506
Epoch 440, val loss: 1.277794361114502
Epoch 450, training loss: 0.12532174587249756 = 0.05340876802802086 + 0.01 * 7.191298007965088
Epoch 450, val loss: 1.3017069101333618
Epoch 460, training loss: 0.11965616047382355 = 0.04783599451184273 + 0.01 * 7.182016372680664
Epoch 460, val loss: 1.3245691061019897
Epoch 470, training loss: 0.11465199291706085 = 0.04299236088991165 + 0.01 * 7.165963649749756
Epoch 470, val loss: 1.3463798761367798
Epoch 480, training loss: 0.11068050563335419 = 0.038774143904447556 + 0.01 * 7.190635681152344
Epoch 480, val loss: 1.3671478033065796
Epoch 490, training loss: 0.10661620646715164 = 0.03509891778230667 + 0.01 * 7.151729106903076
Epoch 490, val loss: 1.3869432210922241
Epoch 500, training loss: 0.10315173864364624 = 0.03188439458608627 + 0.01 * 7.126734256744385
Epoch 500, val loss: 1.405805230140686
Epoch 510, training loss: 0.10023221373558044 = 0.029064999893307686 + 0.01 * 7.1167216300964355
Epoch 510, val loss: 1.423940896987915
Epoch 520, training loss: 0.09769628196954727 = 0.02658705599606037 + 0.01 * 7.110922813415527
Epoch 520, val loss: 1.4412695169448853
Epoch 530, training loss: 0.09571968019008636 = 0.02439843863248825 + 0.01 * 7.132124423980713
Epoch 530, val loss: 1.4579083919525146
Epoch 540, training loss: 0.09350967407226562 = 0.022463660687208176 + 0.01 * 7.104601860046387
Epoch 540, val loss: 1.4739500284194946
Epoch 550, training loss: 0.09186932444572449 = 0.020747311413288116 + 0.01 * 7.11220121383667
Epoch 550, val loss: 1.489354133605957
Epoch 560, training loss: 0.08987883478403091 = 0.019221829250454903 + 0.01 * 7.065700531005859
Epoch 560, val loss: 1.5042903423309326
Epoch 570, training loss: 0.08845560252666473 = 0.017860330641269684 + 0.01 * 7.059527397155762
Epoch 570, val loss: 1.5186342000961304
Epoch 580, training loss: 0.08715322613716125 = 0.016640372574329376 + 0.01 * 7.051285743713379
Epoch 580, val loss: 1.5325695276260376
Epoch 590, training loss: 0.08592860400676727 = 0.015545640140771866 + 0.01 * 7.038297176361084
Epoch 590, val loss: 1.5460208654403687
Epoch 600, training loss: 0.08478058129549026 = 0.014559706673026085 + 0.01 * 7.022087097167969
Epoch 600, val loss: 1.559019684791565
Epoch 610, training loss: 0.08431024104356766 = 0.01366920955479145 + 0.01 * 7.064103126525879
Epoch 610, val loss: 1.5715153217315674
Epoch 620, training loss: 0.08307693153619766 = 0.0128634637221694 + 0.01 * 7.021346569061279
Epoch 620, val loss: 1.5836613178253174
Epoch 630, training loss: 0.08222001045942307 = 0.012131315656006336 + 0.01 * 7.008869647979736
Epoch 630, val loss: 1.5954291820526123
Epoch 640, training loss: 0.08152545243501663 = 0.011463158763945103 + 0.01 * 7.006229400634766
Epoch 640, val loss: 1.606757402420044
Epoch 650, training loss: 0.08074180781841278 = 0.010851990431547165 + 0.01 * 6.988982200622559
Epoch 650, val loss: 1.6177787780761719
Epoch 660, training loss: 0.08000005036592484 = 0.010291757062077522 + 0.01 * 6.970829963684082
Epoch 660, val loss: 1.628387212753296
Epoch 670, training loss: 0.07944883406162262 = 0.009776577353477478 + 0.01 * 6.967226028442383
Epoch 670, val loss: 1.6387293338775635
Epoch 680, training loss: 0.07897923141717911 = 0.009302360936999321 + 0.01 * 6.967686653137207
Epoch 680, val loss: 1.648668885231018
Epoch 690, training loss: 0.07838926464319229 = 0.008863955736160278 + 0.01 * 6.952531337738037
Epoch 690, val loss: 1.6583389043807983
Epoch 700, training loss: 0.07816866040229797 = 0.008456835523247719 + 0.01 * 6.971182823181152
Epoch 700, val loss: 1.6676982641220093
Epoch 710, training loss: 0.07758817076683044 = 0.008078847080469131 + 0.01 * 6.950932502746582
Epoch 710, val loss: 1.6768196821212769
Epoch 720, training loss: 0.07701777666807175 = 0.007726714480668306 + 0.01 * 6.929106712341309
Epoch 720, val loss: 1.6855309009552002
Epoch 730, training loss: 0.07673011720180511 = 0.007398624904453754 + 0.01 * 6.933149337768555
Epoch 730, val loss: 1.6940292119979858
Epoch 740, training loss: 0.07630522549152374 = 0.007091835141181946 + 0.01 * 6.92133903503418
Epoch 740, val loss: 1.7024047374725342
Epoch 750, training loss: 0.0760098472237587 = 0.006804504431784153 + 0.01 * 6.920534610748291
Epoch 750, val loss: 1.7103549242019653
Epoch 760, training loss: 0.07586393505334854 = 0.006535966414958239 + 0.01 * 6.932796478271484
Epoch 760, val loss: 1.7181837558746338
Epoch 770, training loss: 0.07536546885967255 = 0.0062836590223014355 + 0.01 * 6.908181190490723
Epoch 770, val loss: 1.7258344888687134
Epoch 780, training loss: 0.07499665766954422 = 0.006046491675078869 + 0.01 * 6.895016670227051
Epoch 780, val loss: 1.7331982851028442
Epoch 790, training loss: 0.07479199022054672 = 0.005823744460940361 + 0.01 * 6.896824836730957
Epoch 790, val loss: 1.7404018640518188
Epoch 800, training loss: 0.07446404546499252 = 0.005614388268440962 + 0.01 * 6.884965896606445
Epoch 800, val loss: 1.7474043369293213
Epoch 810, training loss: 0.0745379626750946 = 0.005417062435299158 + 0.01 * 6.912090301513672
Epoch 810, val loss: 1.7542542219161987
Epoch 820, training loss: 0.07408209145069122 = 0.005231116898357868 + 0.01 * 6.885097026824951
Epoch 820, val loss: 1.7608500719070435
Epoch 830, training loss: 0.07387416809797287 = 0.005055413581430912 + 0.01 * 6.881875991821289
Epoch 830, val loss: 1.7671995162963867
Epoch 840, training loss: 0.07372987270355225 = 0.004889685660600662 + 0.01 * 6.884018898010254
Epoch 840, val loss: 1.773467779159546
Epoch 850, training loss: 0.07334837317466736 = 0.004732709843665361 + 0.01 * 6.861566066741943
Epoch 850, val loss: 1.7795507907867432
Epoch 860, training loss: 0.07324880361557007 = 0.004584105219691992 + 0.01 * 6.8664703369140625
Epoch 860, val loss: 1.7853535413742065
Epoch 870, training loss: 0.0730600357055664 = 0.004444009158760309 + 0.01 * 6.861602783203125
Epoch 870, val loss: 1.7912150621414185
Epoch 880, training loss: 0.07298966497182846 = 0.004310963675379753 + 0.01 * 6.867869853973389
Epoch 880, val loss: 1.7967236042022705
Epoch 890, training loss: 0.0728234201669693 = 0.004184908699244261 + 0.01 * 6.863851070404053
Epoch 890, val loss: 1.8021785020828247
Epoch 900, training loss: 0.07260751724243164 = 0.0040651909075677395 + 0.01 * 6.854232311248779
Epoch 900, val loss: 1.8074394464492798
Epoch 910, training loss: 0.07232346385717392 = 0.0039512841030955315 + 0.01 * 6.837218284606934
Epoch 910, val loss: 1.812567949295044
Epoch 920, training loss: 0.07237248867750168 = 0.003842955455183983 + 0.01 * 6.85295295715332
Epoch 920, val loss: 1.8175650835037231
Epoch 930, training loss: 0.07208619266748428 = 0.0037397006526589394 + 0.01 * 6.834649085998535
Epoch 930, val loss: 1.8225386142730713
Epoch 940, training loss: 0.0721929520368576 = 0.0036411357577890158 + 0.01 * 6.855181694030762
Epoch 940, val loss: 1.8272253274917603
Epoch 950, training loss: 0.07202684879302979 = 0.0035473997704684734 + 0.01 * 6.847945213317871
Epoch 950, val loss: 1.831849455833435
Epoch 960, training loss: 0.07180210202932358 = 0.0034578971099108458 + 0.01 * 6.834420680999756
Epoch 960, val loss: 1.836432695388794
Epoch 970, training loss: 0.07165378332138062 = 0.0033724370878189802 + 0.01 * 6.828135013580322
Epoch 970, val loss: 1.8406533002853394
Epoch 980, training loss: 0.07157287001609802 = 0.003290755208581686 + 0.01 * 6.828211307525635
Epoch 980, val loss: 1.844926357269287
Epoch 990, training loss: 0.07145604491233826 = 0.003212722484022379 + 0.01 * 6.8243327140808105
Epoch 990, val loss: 1.8490673303604126
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 2.0398952960968018 = 1.953926920890808 + 0.01 * 8.596833229064941
Epoch 0, val loss: 1.9452166557312012
Epoch 10, training loss: 2.0294623374938965 = 1.9434947967529297 + 0.01 * 8.596763610839844
Epoch 10, val loss: 1.9353822469711304
Epoch 20, training loss: 2.0163826942443848 = 1.9304174184799194 + 0.01 * 8.596522331237793
Epoch 20, val loss: 1.9227412939071655
Epoch 30, training loss: 1.9978328943252563 = 1.911876916885376 + 0.01 * 8.59559440612793
Epoch 30, val loss: 1.9045220613479614
Epoch 40, training loss: 1.970292568206787 = 1.8843932151794434 + 0.01 * 8.589934349060059
Epoch 40, val loss: 1.8775949478149414
Epoch 50, training loss: 1.9322024583816528 = 1.8466343879699707 + 0.01 * 8.556812286376953
Epoch 50, val loss: 1.8424237966537476
Epoch 60, training loss: 1.8912432193756104 = 1.8071413040161133 + 0.01 * 8.41019058227539
Epoch 60, val loss: 1.8105002641677856
Epoch 70, training loss: 1.8600972890853882 = 1.776732087135315 + 0.01 * 8.336519241333008
Epoch 70, val loss: 1.7888942956924438
Epoch 80, training loss: 1.8220438957214355 = 1.739699125289917 + 0.01 * 8.234482765197754
Epoch 80, val loss: 1.7571303844451904
Epoch 90, training loss: 1.7682408094406128 = 1.6879889965057373 + 0.01 * 8.025176048278809
Epoch 90, val loss: 1.7114559412002563
Epoch 100, training loss: 1.6936767101287842 = 1.615893006324768 + 0.01 * 7.778372287750244
Epoch 100, val loss: 1.6504789590835571
Epoch 110, training loss: 1.6021289825439453 = 1.5253974199295044 + 0.01 * 7.673157215118408
Epoch 110, val loss: 1.5771167278289795
Epoch 120, training loss: 1.5022838115692139 = 1.4260709285736084 + 0.01 * 7.621284008026123
Epoch 120, val loss: 1.4985698461532593
Epoch 130, training loss: 1.4013094902038574 = 1.3256150484085083 + 0.01 * 7.56943941116333
Epoch 130, val loss: 1.4221915006637573
Epoch 140, training loss: 1.3022199869155884 = 1.2270621061325073 + 0.01 * 7.515784740447998
Epoch 140, val loss: 1.3496404886245728
Epoch 150, training loss: 1.2073477506637573 = 1.1328219175338745 + 0.01 * 7.452586650848389
Epoch 150, val loss: 1.2825188636779785
Epoch 160, training loss: 1.1199336051940918 = 1.0458718538284302 + 0.01 * 7.40617561340332
Epoch 160, val loss: 1.2228388786315918
Epoch 170, training loss: 1.040469765663147 = 0.9665709137916565 + 0.01 * 7.389890670776367
Epoch 170, val loss: 1.1702215671539307
Epoch 180, training loss: 0.9664689302444458 = 0.8927074074745178 + 0.01 * 7.376155376434326
Epoch 180, val loss: 1.1216365098953247
Epoch 190, training loss: 0.8955594897270203 = 0.8219713568687439 + 0.01 * 7.358815670013428
Epoch 190, val loss: 1.0747523307800293
Epoch 200, training loss: 0.8271752595901489 = 0.7537965178489685 + 0.01 * 7.33787727355957
Epoch 200, val loss: 1.0295802354812622
Epoch 210, training loss: 0.7622039318084717 = 0.6890109181404114 + 0.01 * 7.319303512573242
Epoch 210, val loss: 0.9877582788467407
Epoch 220, training loss: 0.7015964984893799 = 0.6286119818687439 + 0.01 * 7.298452377319336
Epoch 220, val loss: 0.9511298537254333
Epoch 230, training loss: 0.6456801891326904 = 0.5728079080581665 + 0.01 * 7.28722620010376
Epoch 230, val loss: 0.9206538796424866
Epoch 240, training loss: 0.5942496061325073 = 0.521434485912323 + 0.01 * 7.281512260437012
Epoch 240, val loss: 0.8967040181159973
Epoch 250, training loss: 0.5469128489494324 = 0.47416526079177856 + 0.01 * 7.274758815765381
Epoch 250, val loss: 0.8793134093284607
Epoch 260, training loss: 0.5032830834388733 = 0.43057551980018616 + 0.01 * 7.270754337310791
Epoch 260, val loss: 0.867813766002655
Epoch 270, training loss: 0.4629412293434143 = 0.39026492834091187 + 0.01 * 7.2676286697387695
Epoch 270, val loss: 0.8612333536148071
Epoch 280, training loss: 0.4253801107406616 = 0.3527146577835083 + 0.01 * 7.266546726226807
Epoch 280, val loss: 0.8584173321723938
Epoch 290, training loss: 0.3898947834968567 = 0.31727561354637146 + 0.01 * 7.261917591094971
Epoch 290, val loss: 0.8583484292030334
Epoch 300, training loss: 0.35595738887786865 = 0.28336167335510254 + 0.01 * 7.259570598602295
Epoch 300, val loss: 0.8601616621017456
Epoch 310, training loss: 0.32331499457359314 = 0.25072795152664185 + 0.01 * 7.258705139160156
Epoch 310, val loss: 0.8630988001823425
Epoch 320, training loss: 0.29215848445892334 = 0.2196163684129715 + 0.01 * 7.254213333129883
Epoch 320, val loss: 0.8667736649513245
Epoch 330, training loss: 0.2631571292877197 = 0.1906513273715973 + 0.01 * 7.250579833984375
Epoch 330, val loss: 0.8710583448410034
Epoch 340, training loss: 0.23694245517253876 = 0.16450169682502747 + 0.01 * 7.244075775146484
Epoch 340, val loss: 0.8761978149414062
Epoch 350, training loss: 0.21403102576732635 = 0.1415935456752777 + 0.01 * 7.243748188018799
Epoch 350, val loss: 0.8824288249015808
Epoch 360, training loss: 0.1942768692970276 = 0.12197881937026978 + 0.01 * 7.229804515838623
Epoch 360, val loss: 0.889924168586731
Epoch 370, training loss: 0.1776334047317505 = 0.1054079458117485 + 0.01 * 7.222545146942139
Epoch 370, val loss: 0.89862060546875
Epoch 380, training loss: 0.16368995606899261 = 0.09148793667554855 + 0.01 * 7.220202445983887
Epoch 380, val loss: 0.9082722067832947
Epoch 390, training loss: 0.15182051062583923 = 0.07978262007236481 + 0.01 * 7.203789234161377
Epoch 390, val loss: 0.918695330619812
Epoch 400, training loss: 0.1421811878681183 = 0.06990841031074524 + 0.01 * 7.227278709411621
Epoch 400, val loss: 0.9297420978546143
Epoch 410, training loss: 0.13352513313293457 = 0.06158175691962242 + 0.01 * 7.194337368011475
Epoch 410, val loss: 0.9408920407295227
Epoch 420, training loss: 0.12645143270492554 = 0.05450832471251488 + 0.01 * 7.194310665130615
Epoch 420, val loss: 0.9523285031318665
Epoch 430, training loss: 0.12020424008369446 = 0.04846082255244255 + 0.01 * 7.174341678619385
Epoch 430, val loss: 0.9638125896453857
Epoch 440, training loss: 0.11486206948757172 = 0.04326486587524414 + 0.01 * 7.159720420837402
Epoch 440, val loss: 0.975350022315979
Epoch 450, training loss: 0.11067081987857819 = 0.03878701850771904 + 0.01 * 7.188379764556885
Epoch 450, val loss: 0.9868435263633728
Epoch 460, training loss: 0.10635434091091156 = 0.03492891415953636 + 0.01 * 7.142543315887451
Epoch 460, val loss: 0.9980147480964661
Epoch 470, training loss: 0.10303138941526413 = 0.031589142978191376 + 0.01 * 7.144224643707275
Epoch 470, val loss: 1.0089973211288452
Epoch 480, training loss: 0.09997782111167908 = 0.02868509665131569 + 0.01 * 7.129271984100342
Epoch 480, val loss: 1.019823431968689
Epoch 490, training loss: 0.0973799079656601 = 0.02615165151655674 + 0.01 * 7.122826099395752
Epoch 490, val loss: 1.0302940607070923
Epoch 500, training loss: 0.09516430646181107 = 0.023933691903948784 + 0.01 * 7.123061180114746
Epoch 500, val loss: 1.0405679941177368
Epoch 510, training loss: 0.09310256689786911 = 0.02198401838541031 + 0.01 * 7.1118550300598145
Epoch 510, val loss: 1.0504364967346191
Epoch 520, training loss: 0.09134072810411453 = 0.020262010395526886 + 0.01 * 7.107872009277344
Epoch 520, val loss: 1.0600872039794922
Epoch 530, training loss: 0.08979253470897675 = 0.018734853714704514 + 0.01 * 7.105767726898193
Epoch 530, val loss: 1.0694537162780762
Epoch 540, training loss: 0.08845597505569458 = 0.017378343269228935 + 0.01 * 7.107762813568115
Epoch 540, val loss: 1.0785061120986938
Epoch 550, training loss: 0.0871102437376976 = 0.016168594360351562 + 0.01 * 7.094165325164795
Epoch 550, val loss: 1.0872575044631958
Epoch 560, training loss: 0.08597438782453537 = 0.015083986334502697 + 0.01 * 7.089040279388428
Epoch 560, val loss: 1.095798134803772
Epoch 570, training loss: 0.08484135568141937 = 0.01410837285220623 + 0.01 * 7.073298454284668
Epoch 570, val loss: 1.104014277458191
Epoch 580, training loss: 0.0840262845158577 = 0.013227648101747036 + 0.01 * 7.079863548278809
Epoch 580, val loss: 1.111973524093628
Epoch 590, training loss: 0.08308203518390656 = 0.012431222014129162 + 0.01 * 7.06508207321167
Epoch 590, val loss: 1.1197434663772583
Epoch 600, training loss: 0.08243720978498459 = 0.011708511970937252 + 0.01 * 7.072869777679443
Epoch 600, val loss: 1.12725830078125
Epoch 610, training loss: 0.08161737769842148 = 0.011051425710320473 + 0.01 * 7.056595802307129
Epoch 610, val loss: 1.1345188617706299
Epoch 620, training loss: 0.08090019971132278 = 0.010452061891555786 + 0.01 * 7.044814109802246
Epoch 620, val loss: 1.1415289640426636
Epoch 630, training loss: 0.08031287789344788 = 0.009902928955852985 + 0.01 * 7.040995121002197
Epoch 630, val loss: 1.1483862400054932
Epoch 640, training loss: 0.07981885224580765 = 0.009399203583598137 + 0.01 * 7.041964530944824
Epoch 640, val loss: 1.1550462245941162
Epoch 650, training loss: 0.07923711836338043 = 0.008936579339206219 + 0.01 * 7.030054092407227
Epoch 650, val loss: 1.1614608764648438
Epoch 660, training loss: 0.07881806045770645 = 0.008509914390742779 + 0.01 * 7.030815124511719
Epoch 660, val loss: 1.1676669120788574
Epoch 670, training loss: 0.07846146821975708 = 0.008116204291582108 + 0.01 * 7.034526348114014
Epoch 670, val loss: 1.173768401145935
Epoch 680, training loss: 0.07798635214567184 = 0.007752537727355957 + 0.01 * 7.023381233215332
Epoch 680, val loss: 1.1795638799667358
Epoch 690, training loss: 0.07753245532512665 = 0.007415218744426966 + 0.01 * 7.011723518371582
Epoch 690, val loss: 1.1852723360061646
Epoch 700, training loss: 0.0771666169166565 = 0.007101720664650202 + 0.01 * 7.0064897537231445
Epoch 700, val loss: 1.1908882856369019
Epoch 710, training loss: 0.07675644755363464 = 0.006809847429394722 + 0.01 * 6.994660377502441
Epoch 710, val loss: 1.1962014436721802
Epoch 720, training loss: 0.07676856964826584 = 0.006537730805575848 + 0.01 * 7.0230841636657715
Epoch 720, val loss: 1.2015961408615112
Epoch 730, training loss: 0.07625558972358704 = 0.006283927708864212 + 0.01 * 6.997166633605957
Epoch 730, val loss: 1.2065354585647583
Epoch 740, training loss: 0.07595954835414886 = 0.006046917289495468 + 0.01 * 6.991263389587402
Epoch 740, val loss: 1.2114988565444946
Epoch 750, training loss: 0.0758078470826149 = 0.00582499522715807 + 0.01 * 6.998284816741943
Epoch 750, val loss: 1.2164112329483032
Epoch 760, training loss: 0.07549847662448883 = 0.005616775248199701 + 0.01 * 6.988170146942139
Epoch 760, val loss: 1.2209968566894531
Epoch 770, training loss: 0.07507090270519257 = 0.005421096924692392 + 0.01 * 6.964980602264404
Epoch 770, val loss: 1.2256349325180054
Epoch 780, training loss: 0.07497105747461319 = 0.005237151402980089 + 0.01 * 6.973390102386475
Epoch 780, val loss: 1.2301112413406372
Epoch 790, training loss: 0.07474335283041 = 0.0050638518296182156 + 0.01 * 6.967949867248535
Epoch 790, val loss: 1.234447956085205
Epoch 800, training loss: 0.0746094137430191 = 0.004900451749563217 + 0.01 * 6.970896244049072
Epoch 800, val loss: 1.2387205362319946
Epoch 810, training loss: 0.0743795782327652 = 0.004746291786432266 + 0.01 * 6.963328838348389
Epoch 810, val loss: 1.2428778409957886
Epoch 820, training loss: 0.07440746575593948 = 0.004600385669618845 + 0.01 * 6.980708122253418
Epoch 820, val loss: 1.2468222379684448
Epoch 830, training loss: 0.07401926815509796 = 0.00446253502741456 + 0.01 * 6.955673694610596
Epoch 830, val loss: 1.2508054971694946
Epoch 840, training loss: 0.07379347085952759 = 0.004332119598984718 + 0.01 * 6.946135520935059
Epoch 840, val loss: 1.254653811454773
Epoch 850, training loss: 0.07348102331161499 = 0.004208107944577932 + 0.01 * 6.927291393280029
Epoch 850, val loss: 1.2585431337356567
Epoch 860, training loss: 0.07343204319477081 = 0.00409085676074028 + 0.01 * 6.934118747711182
Epoch 860, val loss: 1.262122392654419
Epoch 870, training loss: 0.07360371947288513 = 0.003979227039963007 + 0.01 * 6.962449073791504
Epoch 870, val loss: 1.2656214237213135
Epoch 880, training loss: 0.07330293953418732 = 0.0038730204105377197 + 0.01 * 6.942992210388184
Epoch 880, val loss: 1.269220232963562
Epoch 890, training loss: 0.07334287464618683 = 0.0037717365194112062 + 0.01 * 6.957114219665527
Epoch 890, val loss: 1.272687554359436
Epoch 900, training loss: 0.07276710122823715 = 0.0036753416061401367 + 0.01 * 6.909175872802734
Epoch 900, val loss: 1.2760047912597656
Epoch 910, training loss: 0.07279860228300095 = 0.0035834440495818853 + 0.01 * 6.921515464782715
Epoch 910, val loss: 1.27942955493927
Epoch 920, training loss: 0.0725400522351265 = 0.0034958445467054844 + 0.01 * 6.904420852661133
Epoch 920, val loss: 1.2825278043746948
Epoch 930, training loss: 0.07272838801145554 = 0.0034122176002711058 + 0.01 * 6.931617259979248
Epoch 930, val loss: 1.2857975959777832
Epoch 940, training loss: 0.07256847620010376 = 0.003332346910610795 + 0.01 * 6.92361307144165
Epoch 940, val loss: 1.288772463798523
Epoch 950, training loss: 0.07209229469299316 = 0.0032560627441853285 + 0.01 * 6.8836236000061035
Epoch 950, val loss: 1.2918652296066284
Epoch 960, training loss: 0.0722949281334877 = 0.0031829839572310448 + 0.01 * 6.911194801330566
Epoch 960, val loss: 1.2947919368743896
Epoch 970, training loss: 0.0719052404165268 = 0.0031130260322242975 + 0.01 * 6.8792219161987305
Epoch 970, val loss: 1.2975767850875854
Epoch 980, training loss: 0.07198933511972427 = 0.0030459496192634106 + 0.01 * 6.894339084625244
Epoch 980, val loss: 1.3005259037017822
Epoch 990, training loss: 0.07179374247789383 = 0.00298171560280025 + 0.01 * 6.881202697753906
Epoch 990, val loss: 1.3033392429351807
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8102266736953084
The final CL Acc:0.73457, 0.01062, The final GNN Acc:0.81673, 0.00464
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13228])
remove edge: torch.Size([2, 7852])
updated graph: torch.Size([2, 10524])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0235023498535156 = 1.9375338554382324 + 0.01 * 8.596860885620117
Epoch 0, val loss: 1.9427129030227661
Epoch 10, training loss: 2.0142462253570557 = 1.9282782077789307 + 0.01 * 8.596807479858398
Epoch 10, val loss: 1.9327092170715332
Epoch 20, training loss: 2.0027942657470703 = 1.916827917098999 + 0.01 * 8.596624374389648
Epoch 20, val loss: 1.9201617240905762
Epoch 30, training loss: 1.9868031740188599 = 1.9008430242538452 + 0.01 * 8.596015930175781
Epoch 30, val loss: 1.9026113748550415
Epoch 40, training loss: 1.9634222984313965 = 1.8774949312210083 + 0.01 * 8.592740058898926
Epoch 40, val loss: 1.877175211906433
Epoch 50, training loss: 1.9305381774902344 = 1.8448306322097778 + 0.01 * 8.570756912231445
Epoch 50, val loss: 1.8427557945251465
Epoch 60, training loss: 1.890982985496521 = 1.8063087463378906 + 0.01 * 8.467418670654297
Epoch 60, val loss: 1.8052406311035156
Epoch 70, training loss: 1.85307776927948 = 1.7700222730636597 + 0.01 * 8.305546760559082
Epoch 70, val loss: 1.7738678455352783
Epoch 80, training loss: 1.808063268661499 = 1.7274141311645508 + 0.01 * 8.064918518066406
Epoch 80, val loss: 1.7370857000350952
Epoch 90, training loss: 1.7451248168945312 = 1.6680221557617188 + 0.01 * 7.710269451141357
Epoch 90, val loss: 1.6837815046310425
Epoch 100, training loss: 1.6640735864639282 = 1.5889203548431396 + 0.01 * 7.515326976776123
Epoch 100, val loss: 1.6119822263717651
Epoch 110, training loss: 1.5670102834701538 = 1.4928680658340454 + 0.01 * 7.4142165184021
Epoch 110, val loss: 1.528336524963379
Epoch 120, training loss: 1.4612118005752563 = 1.387574315071106 + 0.01 * 7.3637518882751465
Epoch 120, val loss: 1.438154935836792
Epoch 130, training loss: 1.3515543937683105 = 1.2782478332519531 + 0.01 * 7.330653667449951
Epoch 130, val loss: 1.3469716310501099
Epoch 140, training loss: 1.2398407459259033 = 1.166782259941101 + 0.01 * 7.305854320526123
Epoch 140, val loss: 1.255421757698059
Epoch 150, training loss: 1.1287872791290283 = 1.0559543371200562 + 0.01 * 7.283292293548584
Epoch 150, val loss: 1.1653320789337158
Epoch 160, training loss: 1.0218528509140015 = 0.9492315053939819 + 0.01 * 7.262129783630371
Epoch 160, val loss: 1.0790438652038574
Epoch 170, training loss: 0.9227629899978638 = 0.8502602577209473 + 0.01 * 7.250275135040283
Epoch 170, val loss: 1.0005533695220947
Epoch 180, training loss: 0.8335689306259155 = 0.7611615657806396 + 0.01 * 7.240736961364746
Epoch 180, val loss: 0.931926965713501
Epoch 190, training loss: 0.7549369931221008 = 0.6825674176216125 + 0.01 * 7.236959934234619
Epoch 190, val loss: 0.8743109703063965
Epoch 200, training loss: 0.6864858269691467 = 0.6141424179077148 + 0.01 * 7.234340190887451
Epoch 200, val loss: 0.8273802995681763
Epoch 210, training loss: 0.6270315647125244 = 0.5547114014625549 + 0.01 * 7.232017993927002
Epoch 210, val loss: 0.7904261946678162
Epoch 220, training loss: 0.5749515295028687 = 0.5026503801345825 + 0.01 * 7.230116844177246
Epoch 220, val loss: 0.762090265750885
Epoch 230, training loss: 0.5284297466278076 = 0.45614269375801086 + 0.01 * 7.228703498840332
Epoch 230, val loss: 0.7408207058906555
Epoch 240, training loss: 0.48571887612342834 = 0.41344204545021057 + 0.01 * 7.227683067321777
Epoch 240, val loss: 0.7246349453926086
Epoch 250, training loss: 0.4456208646297455 = 0.3733488619327545 + 0.01 * 7.227201461791992
Epoch 250, val loss: 0.7119996547698975
Epoch 260, training loss: 0.4076669216156006 = 0.33539003133773804 + 0.01 * 7.227687358856201
Epoch 260, val loss: 0.7018235325813293
Epoch 270, training loss: 0.3719201982021332 = 0.2996405363082886 + 0.01 * 7.227967262268066
Epoch 270, val loss: 0.6936758756637573
Epoch 280, training loss: 0.33869513869285583 = 0.2664041221141815 + 0.01 * 7.229101181030273
Epoch 280, val loss: 0.6874595284461975
Epoch 290, training loss: 0.30831968784332275 = 0.23601555824279785 + 0.01 * 7.230411529541016
Epoch 290, val loss: 0.6832181811332703
Epoch 300, training loss: 0.28100481629371643 = 0.2086830884218216 + 0.01 * 7.232173442840576
Epoch 300, val loss: 0.6812021136283875
Epoch 310, training loss: 0.2567625343799591 = 0.18442565202713013 + 0.01 * 7.233687400817871
Epoch 310, val loss: 0.6815724968910217
Epoch 320, training loss: 0.23544524610042572 = 0.16309893131256104 + 0.01 * 7.234631538391113
Epoch 320, val loss: 0.6842593550682068
Epoch 330, training loss: 0.21680155396461487 = 0.1444457322359085 + 0.01 * 7.23558235168457
Epoch 330, val loss: 0.6890702247619629
Epoch 340, training loss: 0.2005370408296585 = 0.12817059457302094 + 0.01 * 7.236644744873047
Epoch 340, val loss: 0.6958436965942383
Epoch 350, training loss: 0.1863386482000351 = 0.1139690950512886 + 0.01 * 7.236955642700195
Epoch 350, val loss: 0.7043560147285461
Epoch 360, training loss: 0.173920676112175 = 0.10155468434095383 + 0.01 * 7.236598968505859
Epoch 360, val loss: 0.7143849730491638
Epoch 370, training loss: 0.163081556558609 = 0.09067784249782562 + 0.01 * 7.240371227264404
Epoch 370, val loss: 0.7256596684455872
Epoch 380, training loss: 0.15349425375461578 = 0.08112746477127075 + 0.01 * 7.2366790771484375
Epoch 380, val loss: 0.7379323244094849
Epoch 390, training loss: 0.14506857097148895 = 0.07272398471832275 + 0.01 * 7.2344584465026855
Epoch 390, val loss: 0.7509623169898987
Epoch 400, training loss: 0.13767218589782715 = 0.06531959027051926 + 0.01 * 7.235259056091309
Epoch 400, val loss: 0.764490008354187
Epoch 410, training loss: 0.13109952211380005 = 0.05878909304738045 + 0.01 * 7.231042861938477
Epoch 410, val loss: 0.7783662676811218
Epoch 420, training loss: 0.12529875338077545 = 0.05302588269114494 + 0.01 * 7.227287769317627
Epoch 420, val loss: 0.7923877835273743
Epoch 430, training loss: 0.12020336091518402 = 0.04793889448046684 + 0.01 * 7.226447105407715
Epoch 430, val loss: 0.8064728379249573
Epoch 440, training loss: 0.1156504899263382 = 0.04344834014773369 + 0.01 * 7.22021484375
Epoch 440, val loss: 0.8204864263534546
Epoch 450, training loss: 0.11168718338012695 = 0.039482515305280685 + 0.01 * 7.2204670906066895
Epoch 450, val loss: 0.8343598246574402
Epoch 460, training loss: 0.10809820145368576 = 0.035977281630039215 + 0.01 * 7.21209192276001
Epoch 460, val loss: 0.8480516076087952
Epoch 470, training loss: 0.10491149872541428 = 0.03287435322999954 + 0.01 * 7.203714847564697
Epoch 470, val loss: 0.8614264130592346
Epoch 480, training loss: 0.10205109417438507 = 0.030119463801383972 + 0.01 * 7.19316291809082
Epoch 480, val loss: 0.874539852142334
Epoch 490, training loss: 0.09946317225694656 = 0.027665330097079277 + 0.01 * 7.179783821105957
Epoch 490, val loss: 0.8873147368431091
Epoch 500, training loss: 0.09780412167310715 = 0.02547323703765869 + 0.01 * 7.233088970184326
Epoch 500, val loss: 0.8998107314109802
Epoch 510, training loss: 0.09510455280542374 = 0.023515881970524788 + 0.01 * 7.158867359161377
Epoch 510, val loss: 0.9118530750274658
Epoch 520, training loss: 0.0932764858007431 = 0.02176179736852646 + 0.01 * 7.151468753814697
Epoch 520, val loss: 0.9235672354698181
Epoch 530, training loss: 0.09197422862052917 = 0.020184721797704697 + 0.01 * 7.178950309753418
Epoch 530, val loss: 0.935015857219696
Epoch 540, training loss: 0.09008370339870453 = 0.018767140805721283 + 0.01 * 7.131656169891357
Epoch 540, val loss: 0.9460653066635132
Epoch 550, training loss: 0.0886862725019455 = 0.017489805817604065 + 0.01 * 7.119647026062012
Epoch 550, val loss: 0.956787109375
Epoch 560, training loss: 0.08724526315927505 = 0.01633922941982746 + 0.01 * 7.090603828430176
Epoch 560, val loss: 0.9670846462249756
Epoch 570, training loss: 0.0860539898276329 = 0.015299370512366295 + 0.01 * 7.0754618644714355
Epoch 570, val loss: 0.9770880937576294
Epoch 580, training loss: 0.0852561891078949 = 0.014355500228703022 + 0.01 * 7.090068817138672
Epoch 580, val loss: 0.9867660999298096
Epoch 590, training loss: 0.08418728411197662 = 0.013497145846486092 + 0.01 * 7.069014072418213
Epoch 590, val loss: 0.9960871934890747
Epoch 600, training loss: 0.08317093551158905 = 0.01271563209593296 + 0.01 * 7.045530319213867
Epoch 600, val loss: 1.005074143409729
Epoch 610, training loss: 0.08255259692668915 = 0.012001197785139084 + 0.01 * 7.055140495300293
Epoch 610, val loss: 1.0138450860977173
Epoch 620, training loss: 0.08202983438968658 = 0.011348281055688858 + 0.01 * 7.068155765533447
Epoch 620, val loss: 1.0222868919372559
Epoch 630, training loss: 0.08108481764793396 = 0.010750644840300083 + 0.01 * 7.033417701721191
Epoch 630, val loss: 1.0304467678070068
Epoch 640, training loss: 0.08050915598869324 = 0.010200350545346737 + 0.01 * 7.030880451202393
Epoch 640, val loss: 1.0383994579315186
Epoch 650, training loss: 0.07980303466320038 = 0.009693033993244171 + 0.01 * 7.011000156402588
Epoch 650, val loss: 1.0461560487747192
Epoch 660, training loss: 0.07970353215932846 = 0.00922422856092453 + 0.01 * 7.047930717468262
Epoch 660, val loss: 1.0537052154541016
Epoch 670, training loss: 0.07871489971876144 = 0.008791756816208363 + 0.01 * 6.992314338684082
Epoch 670, val loss: 1.0609145164489746
Epoch 680, training loss: 0.0783626139163971 = 0.008391277864575386 + 0.01 * 6.997134208679199
Epoch 680, val loss: 1.06801176071167
Epoch 690, training loss: 0.07803568989038467 = 0.008019507862627506 + 0.01 * 7.001618385314941
Epoch 690, val loss: 1.0748660564422607
Epoch 700, training loss: 0.07752203196287155 = 0.007674402091652155 + 0.01 * 6.984762668609619
Epoch 700, val loss: 1.0815449953079224
Epoch 710, training loss: 0.07696253806352615 = 0.007353025022894144 + 0.01 * 6.960951805114746
Epoch 710, val loss: 1.0880376100540161
Epoch 720, training loss: 0.07671945542097092 = 0.0070533850230276585 + 0.01 * 6.966607570648193
Epoch 720, val loss: 1.094354510307312
Epoch 730, training loss: 0.07640549540519714 = 0.006773308850824833 + 0.01 * 6.963219165802002
Epoch 730, val loss: 1.100468397140503
Epoch 740, training loss: 0.07611433416604996 = 0.006511779967695475 + 0.01 * 6.9602556228637695
Epoch 740, val loss: 1.1064411401748657
Epoch 750, training loss: 0.07571949064731598 = 0.006266498938202858 + 0.01 * 6.94529914855957
Epoch 750, val loss: 1.1122232675552368
Epoch 760, training loss: 0.07548403739929199 = 0.006036848295480013 + 0.01 * 6.944719314575195
Epoch 760, val loss: 1.117910385131836
Epoch 770, training loss: 0.0751415565609932 = 0.005821038968861103 + 0.01 * 6.932051658630371
Epoch 770, val loss: 1.1233400106430054
Epoch 780, training loss: 0.07472477108240128 = 0.0056184083223342896 + 0.01 * 6.9106364250183105
Epoch 780, val loss: 1.128761649131775
Epoch 790, training loss: 0.07465831935405731 = 0.00542752118781209 + 0.01 * 6.923079490661621
Epoch 790, val loss: 1.1338911056518555
Epoch 800, training loss: 0.07431432604789734 = 0.005247482098639011 + 0.01 * 6.906684875488281
Epoch 800, val loss: 1.1389803886413574
Epoch 810, training loss: 0.07396024465560913 = 0.005077498033642769 + 0.01 * 6.888275146484375
Epoch 810, val loss: 1.1439878940582275
Epoch 820, training loss: 0.07388465851545334 = 0.0049170986749231815 + 0.01 * 6.896756172180176
Epoch 820, val loss: 1.148889183998108
Epoch 830, training loss: 0.07349999994039536 = 0.004765619058161974 + 0.01 * 6.873437881469727
Epoch 830, val loss: 1.1534748077392578
Epoch 840, training loss: 0.07347802072763443 = 0.004622119478881359 + 0.01 * 6.885590553283691
Epoch 840, val loss: 1.1581534147262573
Epoch 850, training loss: 0.07318613678216934 = 0.004486149176955223 + 0.01 * 6.869998931884766
Epoch 850, val loss: 1.1625694036483765
Epoch 860, training loss: 0.07300157099962234 = 0.004357156343758106 + 0.01 * 6.864441394805908
Epoch 860, val loss: 1.1670889854431152
Epoch 870, training loss: 0.07295200973749161 = 0.004234770778566599 + 0.01 * 6.871724605560303
Epoch 870, val loss: 1.1712220907211304
Epoch 880, training loss: 0.07292310148477554 = 0.004118463955819607 + 0.01 * 6.88046407699585
Epoch 880, val loss: 1.175506353378296
Epoch 890, training loss: 0.0725177749991417 = 0.004007858689874411 + 0.01 * 6.850991249084473
Epoch 890, val loss: 1.1795319318771362
Epoch 900, training loss: 0.0724111795425415 = 0.0039025265723466873 + 0.01 * 6.850865840911865
Epoch 900, val loss: 1.1835070848464966
Epoch 910, training loss: 0.07231384515762329 = 0.0038021584041416645 + 0.01 * 6.851169109344482
Epoch 910, val loss: 1.1874758005142212
Epoch 920, training loss: 0.07215095311403275 = 0.003706292249262333 + 0.01 * 6.844466686248779
Epoch 920, val loss: 1.1911640167236328
Epoch 930, training loss: 0.07190816849470139 = 0.003614947432652116 + 0.01 * 6.829322338104248
Epoch 930, val loss: 1.1950149536132812
Epoch 940, training loss: 0.07173576205968857 = 0.0035276832059025764 + 0.01 * 6.820807933807373
Epoch 940, val loss: 1.1985242366790771
Epoch 950, training loss: 0.07183998078107834 = 0.003444202244281769 + 0.01 * 6.839578151702881
Epoch 950, val loss: 1.202157735824585
Epoch 960, training loss: 0.07162810117006302 = 0.0033644845243543386 + 0.01 * 6.826362133026123
Epoch 960, val loss: 1.205581784248352
Epoch 970, training loss: 0.07139880210161209 = 0.003288083942607045 + 0.01 * 6.811071872711182
Epoch 970, val loss: 1.2090556621551514
Epoch 980, training loss: 0.0712253600358963 = 0.0032150328624993563 + 0.01 * 6.801033020019531
Epoch 980, val loss: 1.2122727632522583
Epoch 990, training loss: 0.0711008757352829 = 0.0031447734218090773 + 0.01 * 6.795610427856445
Epoch 990, val loss: 1.2155728340148926
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 2.024233818054199 = 1.938265323638916 + 0.01 * 8.596844673156738
Epoch 0, val loss: 1.9264100790023804
Epoch 10, training loss: 2.0144262313842773 = 1.9284582138061523 + 0.01 * 8.5968017578125
Epoch 10, val loss: 1.9175139665603638
Epoch 20, training loss: 2.0023584365844727 = 1.9163919687271118 + 0.01 * 8.596651077270508
Epoch 20, val loss: 1.9063445329666138
Epoch 30, training loss: 1.9855448007583618 = 1.899582862854004 + 0.01 * 8.596199035644531
Epoch 30, val loss: 1.890624761581421
Epoch 40, training loss: 1.9608561992645264 = 1.8749161958694458 + 0.01 * 8.594006538391113
Epoch 40, val loss: 1.867606520652771
Epoch 50, training loss: 1.9262233972549438 = 1.8404388427734375 + 0.01 * 8.57845687866211
Epoch 50, val loss: 1.8367825746536255
Epoch 60, training loss: 1.886579990386963 = 1.8016008138656616 + 0.01 * 8.49791431427002
Epoch 60, val loss: 1.8056241273880005
Epoch 70, training loss: 1.8500515222549438 = 1.7669910192489624 + 0.01 * 8.306053161621094
Epoch 70, val loss: 1.7783974409103394
Epoch 80, training loss: 1.8040266036987305 = 1.7224719524383545 + 0.01 * 8.155463218688965
Epoch 80, val loss: 1.7362844944000244
Epoch 90, training loss: 1.7381868362426758 = 1.6587282419204712 + 0.01 * 7.945854663848877
Epoch 90, val loss: 1.6767942905426025
Epoch 100, training loss: 1.6525015830993652 = 1.575102686882019 + 0.01 * 7.739885330200195
Epoch 100, val loss: 1.6048442125320435
Epoch 110, training loss: 1.5552613735198975 = 1.480086088180542 + 0.01 * 7.517526149749756
Epoch 110, val loss: 1.5253496170043945
Epoch 120, training loss: 1.4584698677062988 = 1.384129524230957 + 0.01 * 7.434033393859863
Epoch 120, val loss: 1.4460372924804688
Epoch 130, training loss: 1.36419677734375 = 1.2906588315963745 + 0.01 * 7.35379695892334
Epoch 130, val loss: 1.3703463077545166
Epoch 140, training loss: 1.2718453407287598 = 1.198716163635254 + 0.01 * 7.312912940979004
Epoch 140, val loss: 1.2971729040145874
Epoch 150, training loss: 1.1826047897338867 = 1.109648585319519 + 0.01 * 7.295620441436768
Epoch 150, val loss: 1.2274593114852905
Epoch 160, training loss: 1.0988062620162964 = 1.0259463787078857 + 0.01 * 7.28598690032959
Epoch 160, val loss: 1.1637829542160034
Epoch 170, training loss: 1.021391749382019 = 0.9486227035522461 + 0.01 * 7.276905059814453
Epoch 170, val loss: 1.1064082384109497
Epoch 180, training loss: 0.9486852884292603 = 0.8760156631469727 + 0.01 * 7.266960620880127
Epoch 180, val loss: 1.0539038181304932
Epoch 190, training loss: 0.8781319260597229 = 0.8055772185325623 + 0.01 * 7.255469799041748
Epoch 190, val loss: 1.0035921335220337
Epoch 200, training loss: 0.8083133101463318 = 0.7359015345573425 + 0.01 * 7.241178512573242
Epoch 200, val loss: 0.9541672468185425
Epoch 210, training loss: 0.7397555708885193 = 0.6675058007240295 + 0.01 * 7.224975109100342
Epoch 210, val loss: 0.9059689044952393
Epoch 220, training loss: 0.6745356321334839 = 0.602419912815094 + 0.01 * 7.211575031280518
Epoch 220, val loss: 0.8610366582870483
Epoch 230, training loss: 0.6145561933517456 = 0.542560338973999 + 0.01 * 7.199586391448975
Epoch 230, val loss: 0.8214810490608215
Epoch 240, training loss: 0.5603927969932556 = 0.48848408460617065 + 0.01 * 7.190870761871338
Epoch 240, val loss: 0.788379967212677
Epoch 250, training loss: 0.5112918615341187 = 0.43943390250205994 + 0.01 * 7.1857991218566895
Epoch 250, val loss: 0.7616676092147827
Epoch 260, training loss: 0.46598052978515625 = 0.3941757380962372 + 0.01 * 7.180479049682617
Epoch 260, val loss: 0.7402780055999756
Epoch 270, training loss: 0.423561155796051 = 0.35180020332336426 + 0.01 * 7.176096439361572
Epoch 270, val loss: 0.7231019735336304
Epoch 280, training loss: 0.3837299048900604 = 0.31200936436653137 + 0.01 * 7.172053813934326
Epoch 280, val loss: 0.709409773349762
Epoch 290, training loss: 0.34663933515548706 = 0.2749536335468292 + 0.01 * 7.168571472167969
Epoch 290, val loss: 0.69880211353302
Epoch 300, training loss: 0.31267857551574707 = 0.2410447895526886 + 0.01 * 7.163380146026611
Epoch 300, val loss: 0.691279947757721
Epoch 310, training loss: 0.28227493166923523 = 0.21067753434181213 + 0.01 * 7.159740447998047
Epoch 310, val loss: 0.686701774597168
Epoch 320, training loss: 0.25556087493896484 = 0.18401573598384857 + 0.01 * 7.154512882232666
Epoch 320, val loss: 0.6849886775016785
Epoch 330, training loss: 0.23240521550178528 = 0.1609252393245697 + 0.01 * 7.1479973793029785
Epoch 330, val loss: 0.6858898401260376
Epoch 340, training loss: 0.21252334117889404 = 0.14105291664600372 + 0.01 * 7.147043704986572
Epoch 340, val loss: 0.6889158487319946
Epoch 350, training loss: 0.19535663723945618 = 0.12397900223731995 + 0.01 * 7.137763500213623
Epoch 350, val loss: 0.693656861782074
Epoch 360, training loss: 0.18055327236652374 = 0.10928589105606079 + 0.01 * 7.126738548278809
Epoch 360, val loss: 0.6996767520904541
Epoch 370, training loss: 0.1681278645992279 = 0.09661172330379486 + 0.01 * 7.151613712310791
Epoch 370, val loss: 0.7066479325294495
Epoch 380, training loss: 0.15679919719696045 = 0.08566443622112274 + 0.01 * 7.1134772300720215
Epoch 380, val loss: 0.7143205404281616
Epoch 390, training loss: 0.1472051441669464 = 0.07618134468793869 + 0.01 * 7.102379322052002
Epoch 390, val loss: 0.7225582003593445
Epoch 400, training loss: 0.13886408507823944 = 0.06794667989015579 + 0.01 * 7.091740608215332
Epoch 400, val loss: 0.7311939597129822
Epoch 410, training loss: 0.13167917728424072 = 0.06078526750206947 + 0.01 * 7.089390754699707
Epoch 410, val loss: 0.7401769757270813
Epoch 420, training loss: 0.12533718347549438 = 0.054549019783735275 + 0.01 * 7.078815937042236
Epoch 420, val loss: 0.7494015097618103
Epoch 430, training loss: 0.11975675821304321 = 0.04910748079419136 + 0.01 * 7.064927577972412
Epoch 430, val loss: 0.7588371634483337
Epoch 440, training loss: 0.11500482261180878 = 0.044348400086164474 + 0.01 * 7.065642833709717
Epoch 440, val loss: 0.7684001922607422
Epoch 450, training loss: 0.11075074970722198 = 0.0401819609105587 + 0.01 * 7.056878566741943
Epoch 450, val loss: 0.7780494689941406
Epoch 460, training loss: 0.10707758367061615 = 0.036524925380945206 + 0.01 * 7.0552659034729
Epoch 460, val loss: 0.7876855731010437
Epoch 470, training loss: 0.103744275867939 = 0.03330841660499573 + 0.01 * 7.043585777282715
Epoch 470, val loss: 0.797288179397583
Epoch 480, training loss: 0.10078734159469604 = 0.030470648780465126 + 0.01 * 7.031669616699219
Epoch 480, val loss: 0.8068349957466125
Epoch 490, training loss: 0.09822659194469452 = 0.027959587052464485 + 0.01 * 7.026700496673584
Epoch 490, val loss: 0.816243588924408
Epoch 500, training loss: 0.0958683118224144 = 0.02573212794959545 + 0.01 * 7.013618469238281
Epoch 500, val loss: 0.8255216479301453
Epoch 510, training loss: 0.09380193054676056 = 0.023748012259602547 + 0.01 * 7.005392074584961
Epoch 510, val loss: 0.8346227407455444
Epoch 520, training loss: 0.09196606278419495 = 0.021976690739393234 + 0.01 * 6.998936653137207
Epoch 520, val loss: 0.8435581922531128
Epoch 530, training loss: 0.09042386710643768 = 0.0203920416533947 + 0.01 * 7.003183364868164
Epoch 530, val loss: 0.8522529602050781
Epoch 540, training loss: 0.08891437947750092 = 0.018969925120472908 + 0.01 * 6.99444580078125
Epoch 540, val loss: 0.8607421517372131
Epoch 550, training loss: 0.08751313388347626 = 0.017689431086182594 + 0.01 * 6.982370376586914
Epoch 550, val loss: 0.868984043598175
Epoch 560, training loss: 0.08632609248161316 = 0.016531821340322495 + 0.01 * 6.979427814483643
Epoch 560, val loss: 0.8770094513893127
Epoch 570, training loss: 0.08525113761425018 = 0.015482201240956783 + 0.01 * 6.976893901824951
Epoch 570, val loss: 0.8848363161087036
Epoch 580, training loss: 0.08420910686254501 = 0.014529815874993801 + 0.01 * 6.967928886413574
Epoch 580, val loss: 0.8923972845077515
Epoch 590, training loss: 0.08324768394231796 = 0.013664393685758114 + 0.01 * 6.958329200744629
Epoch 590, val loss: 0.8997150659561157
Epoch 600, training loss: 0.08251624554395676 = 0.012874691747128963 + 0.01 * 6.964155673980713
Epoch 600, val loss: 0.9068976044654846
Epoch 610, training loss: 0.08159610629081726 = 0.012154128402471542 + 0.01 * 6.9441986083984375
Epoch 610, val loss: 0.9138540625572205
Epoch 620, training loss: 0.08090755343437195 = 0.0114942891523242 + 0.01 * 6.94132661819458
Epoch 620, val loss: 0.920602560043335
Epoch 630, training loss: 0.08025159686803818 = 0.010888947173953056 + 0.01 * 6.936264991760254
Epoch 630, val loss: 0.9271625280380249
Epoch 640, training loss: 0.07969975471496582 = 0.010332902893424034 + 0.01 * 6.936685085296631
Epoch 640, val loss: 0.9335651993751526
Epoch 650, training loss: 0.07928404211997986 = 0.009820213541388512 + 0.01 * 6.946382522583008
Epoch 650, val loss: 0.9397749304771423
Epoch 660, training loss: 0.07853150367736816 = 0.009347166866064072 + 0.01 * 6.91843318939209
Epoch 660, val loss: 0.945789098739624
Epoch 670, training loss: 0.07808949053287506 = 0.008909331634640694 + 0.01 * 6.91801643371582
Epoch 670, val loss: 0.9516353011131287
Epoch 680, training loss: 0.07758374512195587 = 0.008503415621817112 + 0.01 * 6.9080328941345215
Epoch 680, val loss: 0.9574137330055237
Epoch 690, training loss: 0.0771225243806839 = 0.008126639761030674 + 0.01 * 6.899588584899902
Epoch 690, val loss: 0.9629554152488708
Epoch 700, training loss: 0.07700024545192719 = 0.0077758594416081905 + 0.01 * 6.922439098358154
Epoch 700, val loss: 0.9684105515480042
Epoch 710, training loss: 0.07652033865451813 = 0.007449717726558447 + 0.01 * 6.90706205368042
Epoch 710, val loss: 0.9736574292182922
Epoch 720, training loss: 0.07620485126972198 = 0.007145740557461977 + 0.01 * 6.905910968780518
Epoch 720, val loss: 0.9788316488265991
Epoch 730, training loss: 0.07572660595178604 = 0.006861770059913397 + 0.01 * 6.886483669281006
Epoch 730, val loss: 0.983767569065094
Epoch 740, training loss: 0.07542109489440918 = 0.006595751270651817 + 0.01 * 6.882534027099609
Epoch 740, val loss: 0.988669753074646
Epoch 750, training loss: 0.07518240809440613 = 0.006346023175865412 + 0.01 * 6.883638858795166
Epoch 750, val loss: 0.9934477806091309
Epoch 760, training loss: 0.07496840506792068 = 0.006111637689173222 + 0.01 * 6.885677337646484
Epoch 760, val loss: 0.9981284737586975
Epoch 770, training loss: 0.07469812035560608 = 0.005891749635338783 + 0.01 * 6.880637168884277
Epoch 770, val loss: 1.0026626586914062
Epoch 780, training loss: 0.07433337718248367 = 0.005684866104274988 + 0.01 * 6.864851474761963
Epoch 780, val loss: 1.0070449113845825
Epoch 790, training loss: 0.07416406273841858 = 0.0054897950030863285 + 0.01 * 6.867426872253418
Epoch 790, val loss: 1.0114225149154663
Epoch 800, training loss: 0.07393816113471985 = 0.005306117702275515 + 0.01 * 6.863204479217529
Epoch 800, val loss: 1.0155869722366333
Epoch 810, training loss: 0.07369494438171387 = 0.005132321268320084 + 0.01 * 6.85626220703125
Epoch 810, val loss: 1.0197358131408691
Epoch 820, training loss: 0.0735311210155487 = 0.0049680666998028755 + 0.01 * 6.856305122375488
Epoch 820, val loss: 1.0237817764282227
Epoch 830, training loss: 0.07346121221780777 = 0.004812866915017366 + 0.01 * 6.864834785461426
Epoch 830, val loss: 1.0277414321899414
Epoch 840, training loss: 0.0732058435678482 = 0.0046659912914037704 + 0.01 * 6.853984832763672
Epoch 840, val loss: 1.031507134437561
Epoch 850, training loss: 0.0729355737566948 = 0.00452700350433588 + 0.01 * 6.84085750579834
Epoch 850, val loss: 1.035274863243103
Epoch 860, training loss: 0.07273981720209122 = 0.004394722171127796 + 0.01 * 6.834509372711182
Epoch 860, val loss: 1.0389732122421265
Epoch 870, training loss: 0.07281775027513504 = 0.0042694383300840855 + 0.01 * 6.854831218719482
Epoch 870, val loss: 1.0424904823303223
Epoch 880, training loss: 0.07257577031850815 = 0.00415042182430625 + 0.01 * 6.842535495758057
Epoch 880, val loss: 1.0460747480392456
Epoch 890, training loss: 0.0724683329463005 = 0.004037193953990936 + 0.01 * 6.843113899230957
Epoch 890, val loss: 1.0494256019592285
Epoch 900, training loss: 0.07221883535385132 = 0.003929306287318468 + 0.01 * 6.828952789306641
Epoch 900, val loss: 1.0528086423873901
Epoch 910, training loss: 0.07230529934167862 = 0.0038264458999037743 + 0.01 * 6.847885608673096
Epoch 910, val loss: 1.056088924407959
Epoch 920, training loss: 0.07192724943161011 = 0.003728376002982259 + 0.01 * 6.819887638092041
Epoch 920, val loss: 1.059315800666809
Epoch 930, training loss: 0.07185214012861252 = 0.003634753404185176 + 0.01 * 6.821739196777344
Epoch 930, val loss: 1.0624785423278809
Epoch 940, training loss: 0.07164617627859116 = 0.0035456044133752584 + 0.01 * 6.810057163238525
Epoch 940, val loss: 1.0654699802398682
Epoch 950, training loss: 0.07179680466651917 = 0.003460313891991973 + 0.01 * 6.833649635314941
Epoch 950, val loss: 1.0684956312179565
Epoch 960, training loss: 0.07145236432552338 = 0.0033785824198275805 + 0.01 * 6.80737829208374
Epoch 960, val loss: 1.0714014768600464
Epoch 970, training loss: 0.07145775854587555 = 0.003300508251413703 + 0.01 * 6.815725326538086
Epoch 970, val loss: 1.0743494033813477
Epoch 980, training loss: 0.07137857377529144 = 0.0032259845174849033 + 0.01 * 6.8152594566345215
Epoch 980, val loss: 1.0769901275634766
Epoch 990, training loss: 0.07123374193906784 = 0.003154344391077757 + 0.01 * 6.8079400062561035
Epoch 990, val loss: 1.079740047454834
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 2.032336473464966 = 1.9463679790496826 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.9427361488342285
Epoch 10, training loss: 2.0223755836486816 = 1.936407446861267 + 0.01 * 8.5968017578125
Epoch 10, val loss: 1.9324625730514526
Epoch 20, training loss: 2.009795904159546 = 1.923829436302185 + 0.01 * 8.596636772155762
Epoch 20, val loss: 1.9193569421768188
Epoch 30, training loss: 1.9916609525680542 = 1.9056994915008545 + 0.01 * 8.596148490905762
Epoch 30, val loss: 1.900396466255188
Epoch 40, training loss: 1.964415192604065 = 1.8784767389297485 + 0.01 * 8.593851089477539
Epoch 40, val loss: 1.872378945350647
Epoch 50, training loss: 1.9262356758117676 = 1.8404653072357178 + 0.01 * 8.577033996582031
Epoch 50, val loss: 1.835198163986206
Epoch 60, training loss: 1.8837246894836426 = 1.7988348007202148 + 0.01 * 8.488987922668457
Epoch 60, val loss: 1.7987383604049683
Epoch 70, training loss: 1.8447721004486084 = 1.7624777555465698 + 0.01 * 8.229436874389648
Epoch 70, val loss: 1.7691715955734253
Epoch 80, training loss: 1.7945141792297363 = 1.7154277563095093 + 0.01 * 7.908647060394287
Epoch 80, val loss: 1.7275049686431885
Epoch 90, training loss: 1.7260370254516602 = 1.6495752334594727 + 0.01 * 7.64617395401001
Epoch 90, val loss: 1.6690049171447754
Epoch 100, training loss: 1.6368250846862793 = 1.5611597299575806 + 0.01 * 7.566532135009766
Epoch 100, val loss: 1.5928735733032227
Epoch 110, training loss: 1.532423496246338 = 1.4571661949157715 + 0.01 * 7.5257344245910645
Epoch 110, val loss: 1.5051546096801758
Epoch 120, training loss: 1.4230854511260986 = 1.3482086658477783 + 0.01 * 7.487678050994873
Epoch 120, val loss: 1.4150298833847046
Epoch 130, training loss: 1.3120008707046509 = 1.2376797199249268 + 0.01 * 7.432117938995361
Epoch 130, val loss: 1.3258569240570068
Epoch 140, training loss: 1.2001107931137085 = 1.1264044046401978 + 0.01 * 7.370640754699707
Epoch 140, val loss: 1.2381993532180786
Epoch 150, training loss: 1.0914057493209839 = 1.0179466009140015 + 0.01 * 7.34591007232666
Epoch 150, val loss: 1.1546707153320312
Epoch 160, training loss: 0.9901067614555359 = 0.9167826771736145 + 0.01 * 7.332406044006348
Epoch 160, val loss: 1.0782538652420044
Epoch 170, training loss: 0.8996789455413818 = 0.8265020251274109 + 0.01 * 7.317689418792725
Epoch 170, val loss: 1.011414885520935
Epoch 180, training loss: 0.8214241862297058 = 0.7483910322189331 + 0.01 * 7.30331563949585
Epoch 180, val loss: 0.9550444483757019
Epoch 190, training loss: 0.7541683912277222 = 0.6812980771064758 + 0.01 * 7.287032604217529
Epoch 190, val loss: 0.9090699553489685
Epoch 200, training loss: 0.6951667070388794 = 0.6224534511566162 + 0.01 * 7.271328449249268
Epoch 200, val loss: 0.8724406361579895
Epoch 210, training loss: 0.6416974067687988 = 0.5692144632339478 + 0.01 * 7.24829626083374
Epoch 210, val loss: 0.8436213731765747
Epoch 220, training loss: 0.5921654105186462 = 0.5198725461959839 + 0.01 * 7.229288101196289
Epoch 220, val loss: 0.8209967613220215
Epoch 230, training loss: 0.5457236766815186 = 0.4735453128814697 + 0.01 * 7.217838287353516
Epoch 230, val loss: 0.8032617568969727
Epoch 240, training loss: 0.5018294453620911 = 0.4298046827316284 + 0.01 * 7.202474117279053
Epoch 240, val loss: 0.7896537780761719
Epoch 250, training loss: 0.4604405164718628 = 0.3885246515274048 + 0.01 * 7.191585540771484
Epoch 250, val loss: 0.7800700068473816
Epoch 260, training loss: 0.42166730761528015 = 0.3497447967529297 + 0.01 * 7.192251205444336
Epoch 260, val loss: 0.7745996713638306
Epoch 270, training loss: 0.38536331057548523 = 0.31355512142181396 + 0.01 * 7.180819511413574
Epoch 270, val loss: 0.7733270525932312
Epoch 280, training loss: 0.35180777311325073 = 0.280051589012146 + 0.01 * 7.17561674118042
Epoch 280, val loss: 0.7760973572731018
Epoch 290, training loss: 0.3211689591407776 = 0.24937602877616882 + 0.01 * 7.179293632507324
Epoch 290, val loss: 0.7827404737472534
Epoch 300, training loss: 0.2932673692703247 = 0.2215963453054428 + 0.01 * 7.167101860046387
Epoch 300, val loss: 0.7930667996406555
Epoch 310, training loss: 0.2683379650115967 = 0.19670508801937103 + 0.01 * 7.163286209106445
Epoch 310, val loss: 0.8066957592964172
Epoch 320, training loss: 0.24622710049152374 = 0.17460325360298157 + 0.01 * 7.162384510040283
Epoch 320, val loss: 0.8231652975082397
Epoch 330, training loss: 0.22669202089309692 = 0.15512141585350037 + 0.01 * 7.1570611000061035
Epoch 330, val loss: 0.8419556617736816
Epoch 340, training loss: 0.2096436619758606 = 0.1380399614572525 + 0.01 * 7.160370826721191
Epoch 340, val loss: 0.8625202178955078
Epoch 350, training loss: 0.19460229575634003 = 0.12310981750488281 + 0.01 * 7.149248123168945
Epoch 350, val loss: 0.8843297362327576
Epoch 360, training loss: 0.18150809407234192 = 0.11004254966974258 + 0.01 * 7.146555423736572
Epoch 360, val loss: 0.9069344997406006
Epoch 370, training loss: 0.170010507106781 = 0.09858527034521103 + 0.01 * 7.142524719238281
Epoch 370, val loss: 0.930100679397583
Epoch 380, training loss: 0.1599244624376297 = 0.08853121846914291 + 0.01 * 7.13932466506958
Epoch 380, val loss: 0.9536595344543457
Epoch 390, training loss: 0.15114471316337585 = 0.07969988137483597 + 0.01 * 7.144482612609863
Epoch 390, val loss: 0.9774225354194641
Epoch 400, training loss: 0.14324182271957397 = 0.07193703949451447 + 0.01 * 7.130478382110596
Epoch 400, val loss: 1.0011845827102661
Epoch 410, training loss: 0.13640311360359192 = 0.06509353965520859 + 0.01 * 7.130957126617432
Epoch 410, val loss: 1.0248609781265259
Epoch 420, training loss: 0.1302390992641449 = 0.05905286595225334 + 0.01 * 7.118624210357666
Epoch 420, val loss: 1.0483276844024658
Epoch 430, training loss: 0.12476396560668945 = 0.05370714142918587 + 0.01 * 7.105681896209717
Epoch 430, val loss: 1.0714908838272095
Epoch 440, training loss: 0.12016203999519348 = 0.048962950706481934 + 0.01 * 7.119909286499023
Epoch 440, val loss: 1.0942754745483398
Epoch 450, training loss: 0.11572089791297913 = 0.0447491891682148 + 0.01 * 7.097171306610107
Epoch 450, val loss: 1.1166315078735352
Epoch 460, training loss: 0.1118026152253151 = 0.04099385440349579 + 0.01 * 7.080875873565674
Epoch 460, val loss: 1.1385328769683838
Epoch 470, training loss: 0.10836565494537354 = 0.03763645887374878 + 0.01 * 7.072919845581055
Epoch 470, val loss: 1.1600286960601807
Epoch 480, training loss: 0.10564887523651123 = 0.03463629260659218 + 0.01 * 7.101258754730225
Epoch 480, val loss: 1.1809877157211304
Epoch 490, training loss: 0.10258819162845612 = 0.03195171058177948 + 0.01 * 7.063648223876953
Epoch 490, val loss: 1.201378345489502
Epoch 500, training loss: 0.10006701201200485 = 0.02953995019197464 + 0.01 * 7.052706241607666
Epoch 500, val loss: 1.2212210893630981
Epoch 510, training loss: 0.09773524105548859 = 0.02736695483326912 + 0.01 * 7.03682804107666
Epoch 510, val loss: 1.240628719329834
Epoch 520, training loss: 0.09610401839017868 = 0.025408022105693817 + 0.01 * 7.069599628448486
Epoch 520, val loss: 1.2595096826553345
Epoch 530, training loss: 0.09403340518474579 = 0.023642726242542267 + 0.01 * 7.039068222045898
Epoch 530, val loss: 1.277761459350586
Epoch 540, training loss: 0.09223689138889313 = 0.02204716019332409 + 0.01 * 7.018972873687744
Epoch 540, val loss: 1.295461893081665
Epoch 550, training loss: 0.09057766199111938 = 0.020600557327270508 + 0.01 * 6.997710704803467
Epoch 550, val loss: 1.312699794769287
Epoch 560, training loss: 0.08937649428844452 = 0.019285881891846657 + 0.01 * 7.009061336517334
Epoch 560, val loss: 1.3294448852539062
Epoch 570, training loss: 0.08828018605709076 = 0.018088940531015396 + 0.01 * 7.019124507904053
Epoch 570, val loss: 1.3456811904907227
Epoch 580, training loss: 0.08678553253412247 = 0.016997916623950005 + 0.01 * 6.978761196136475
Epoch 580, val loss: 1.3614329099655151
Epoch 590, training loss: 0.08598803728818893 = 0.016000976786017418 + 0.01 * 6.998705863952637
Epoch 590, val loss: 1.3768513202667236
Epoch 600, training loss: 0.0848207175731659 = 0.015088139101862907 + 0.01 * 6.973257541656494
Epoch 600, val loss: 1.3916418552398682
Epoch 610, training loss: 0.08383423835039139 = 0.01425100676715374 + 0.01 * 6.958323001861572
Epoch 610, val loss: 1.4061172008514404
Epoch 620, training loss: 0.0833490714430809 = 0.013481329195201397 + 0.01 * 6.986774921417236
Epoch 620, val loss: 1.420145034790039
Epoch 630, training loss: 0.08221101760864258 = 0.012774214148521423 + 0.01 * 6.943680763244629
Epoch 630, val loss: 1.433799386024475
Epoch 640, training loss: 0.08162838220596313 = 0.012121456675231457 + 0.01 * 6.950692653656006
Epoch 640, val loss: 1.447140097618103
Epoch 650, training loss: 0.08084028959274292 = 0.011519036255776882 + 0.01 * 6.932125091552734
Epoch 650, val loss: 1.4599274396896362
Epoch 660, training loss: 0.08019878715276718 = 0.010962245985865593 + 0.01 * 6.923654079437256
Epoch 660, val loss: 1.4725465774536133
Epoch 670, training loss: 0.07954327017068863 = 0.010445856489241123 + 0.01 * 6.9097418785095215
Epoch 670, val loss: 1.4847137928009033
Epoch 680, training loss: 0.07910124212503433 = 0.009965883567929268 + 0.01 * 6.913536071777344
Epoch 680, val loss: 1.496533989906311
Epoch 690, training loss: 0.07861914485692978 = 0.00951924454420805 + 0.01 * 6.909990310668945
Epoch 690, val loss: 1.508100986480713
Epoch 700, training loss: 0.07835225760936737 = 0.009102735668420792 + 0.01 * 6.924952507019043
Epoch 700, val loss: 1.5193661451339722
Epoch 710, training loss: 0.07783172279596329 = 0.008714713156223297 + 0.01 * 6.911701202392578
Epoch 710, val loss: 1.530232310295105
Epoch 720, training loss: 0.07726123183965683 = 0.00835272204130888 + 0.01 * 6.8908514976501465
Epoch 720, val loss: 1.5408161878585815
Epoch 730, training loss: 0.0768217146396637 = 0.008014427497982979 + 0.01 * 6.8807291984558105
Epoch 730, val loss: 1.5511083602905273
Epoch 740, training loss: 0.07651828229427338 = 0.007697211112827063 + 0.01 * 6.882107734680176
Epoch 740, val loss: 1.5611486434936523
Epoch 750, training loss: 0.07610194385051727 = 0.007400039583444595 + 0.01 * 6.870190620422363
Epoch 750, val loss: 1.5709011554718018
Epoch 760, training loss: 0.0760275200009346 = 0.0071214959025382996 + 0.01 * 6.8906025886535645
Epoch 760, val loss: 1.5804839134216309
Epoch 770, training loss: 0.07564876973628998 = 0.006859524641185999 + 0.01 * 6.878924369812012
Epoch 770, val loss: 1.5897047519683838
Epoch 780, training loss: 0.0751938596367836 = 0.006613713223487139 + 0.01 * 6.8580145835876465
Epoch 780, val loss: 1.5988115072250366
Epoch 790, training loss: 0.07493283599615097 = 0.0063816094771027565 + 0.01 * 6.8551225662231445
Epoch 790, val loss: 1.6077008247375488
Epoch 800, training loss: 0.0745314285159111 = 0.0061628734692931175 + 0.01 * 6.836855411529541
Epoch 800, val loss: 1.6163666248321533
Epoch 810, training loss: 0.074492447078228 = 0.0059561533853411674 + 0.01 * 6.8536295890808105
Epoch 810, val loss: 1.6249310970306396
Epoch 820, training loss: 0.07412508130073547 = 0.005761284381151199 + 0.01 * 6.836379528045654
Epoch 820, val loss: 1.633060336112976
Epoch 830, training loss: 0.07402893155813217 = 0.005576862022280693 + 0.01 * 6.8452067375183105
Epoch 830, val loss: 1.641143798828125
Epoch 840, training loss: 0.07362724095582962 = 0.005402624141424894 + 0.01 * 6.8224616050720215
Epoch 840, val loss: 1.6488306522369385
Epoch 850, training loss: 0.0734662190079689 = 0.00523725850507617 + 0.01 * 6.8228960037231445
Epoch 850, val loss: 1.6565462350845337
Epoch 860, training loss: 0.07367551326751709 = 0.0050809308886528015 + 0.01 * 6.8594584465026855
Epoch 860, val loss: 1.6638678312301636
Epoch 870, training loss: 0.0730067640542984 = 0.004932380747050047 + 0.01 * 6.807438373565674
Epoch 870, val loss: 1.6710641384124756
Epoch 880, training loss: 0.07280335575342178 = 0.004791096784174442 + 0.01 * 6.8012261390686035
Epoch 880, val loss: 1.6781190633773804
Epoch 890, training loss: 0.07271206378936768 = 0.004657094366848469 + 0.01 * 6.805497169494629
Epoch 890, val loss: 1.6850541830062866
Epoch 900, training loss: 0.07273830473423004 = 0.004529830068349838 + 0.01 * 6.820847034454346
Epoch 900, val loss: 1.6917225122451782
Epoch 910, training loss: 0.07233122736215591 = 0.004408351611346006 + 0.01 * 6.792287826538086
Epoch 910, val loss: 1.6982519626617432
Epoch 920, training loss: 0.0721612349152565 = 0.00429235165938735 + 0.01 * 6.786888599395752
Epoch 920, val loss: 1.7047476768493652
Epoch 930, training loss: 0.07239680737257004 = 0.004181728698313236 + 0.01 * 6.821508407592773
Epoch 930, val loss: 1.7110919952392578
Epoch 940, training loss: 0.07197567075490952 = 0.004076322540640831 + 0.01 * 6.789935111999512
Epoch 940, val loss: 1.7171378135681152
Epoch 950, training loss: 0.07204480469226837 = 0.0039758626371622086 + 0.01 * 6.806894302368164
Epoch 950, val loss: 1.723142385482788
Epoch 960, training loss: 0.07167834788560867 = 0.003879786003381014 + 0.01 * 6.779856204986572
Epoch 960, val loss: 1.7290215492248535
Epoch 970, training loss: 0.07147803157567978 = 0.003787814639508724 + 0.01 * 6.769021987915039
Epoch 970, val loss: 1.7347850799560547
Epoch 980, training loss: 0.07147582620382309 = 0.0036996507551521063 + 0.01 * 6.777617454528809
Epoch 980, val loss: 1.7404533624649048
Epoch 990, training loss: 0.07123779505491257 = 0.003615325316786766 + 0.01 * 6.762246608734131
Epoch 990, val loss: 1.7459444999694824
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8418555614127571
The final CL Acc:0.80000, 0.00800, The final GNN Acc:0.84010, 0.00179
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9554])
updated graph: torch.Size([2, 10628])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.049281597137451 = 1.9633134603500366 + 0.01 * 8.596819877624512
Epoch 0, val loss: 1.9677060842514038
Epoch 10, training loss: 2.0389957427978516 = 1.9530284404754639 + 0.01 * 8.596728324890137
Epoch 10, val loss: 1.9579747915267944
Epoch 20, training loss: 2.0260910987854004 = 1.9401264190673828 + 0.01 * 8.596471786499023
Epoch 20, val loss: 1.9451391696929932
Epoch 30, training loss: 2.0078225135803223 = 1.9218659400939941 + 0.01 * 8.595664024353027
Epoch 30, val loss: 1.9264435768127441
Epoch 40, training loss: 1.9804723262786865 = 1.894563913345337 + 0.01 * 8.590838432312012
Epoch 40, val loss: 1.898431658744812
Epoch 50, training loss: 1.941603660583496 = 1.8560279607772827 + 0.01 * 8.557573318481445
Epoch 50, val loss: 1.8600784540176392
Epoch 60, training loss: 1.8979699611663818 = 1.8141361474990845 + 0.01 * 8.383381843566895
Epoch 60, val loss: 1.8216640949249268
Epoch 70, training loss: 1.8660459518432617 = 1.7834086418151855 + 0.01 * 8.263729095458984
Epoch 70, val loss: 1.7944151163101196
Epoch 80, training loss: 1.8296750783920288 = 1.7495476007461548 + 0.01 * 8.0127534866333
Epoch 80, val loss: 1.7608585357666016
Epoch 90, training loss: 1.77925443649292 = 1.7031211853027344 + 0.01 * 7.613330841064453
Epoch 90, val loss: 1.717337965965271
Epoch 100, training loss: 1.711917519569397 = 1.6382213830947876 + 0.01 * 7.369616508483887
Epoch 100, val loss: 1.6587717533111572
Epoch 110, training loss: 1.6274861097335815 = 1.5547406673431396 + 0.01 * 7.274539470672607
Epoch 110, val loss: 1.5844486951828003
Epoch 120, training loss: 1.5347415208816528 = 1.462609887123108 + 0.01 * 7.213160991668701
Epoch 120, val loss: 1.5045149326324463
Epoch 130, training loss: 1.4429147243499756 = 1.3711416721343994 + 0.01 * 7.177305698394775
Epoch 130, val loss: 1.4266663789749146
Epoch 140, training loss: 1.3537770509719849 = 1.2822105884552002 + 0.01 * 7.15664529800415
Epoch 140, val loss: 1.3537412881851196
Epoch 150, training loss: 1.266099214553833 = 1.1946637630462646 + 0.01 * 7.143550395965576
Epoch 150, val loss: 1.2849204540252686
Epoch 160, training loss: 1.1803663969039917 = 1.108994722366333 + 0.01 * 7.137170314788818
Epoch 160, val loss: 1.219445824623108
Epoch 170, training loss: 1.097542643547058 = 1.0261731147766113 + 0.01 * 7.136948108673096
Epoch 170, val loss: 1.1574147939682007
Epoch 180, training loss: 1.0181763172149658 = 0.9468080401420593 + 0.01 * 7.136824607849121
Epoch 180, val loss: 1.098510980606079
Epoch 190, training loss: 0.942279577255249 = 0.8709224462509155 + 0.01 * 7.1357102394104
Epoch 190, val loss: 1.0429528951644897
Epoch 200, training loss: 0.8693128228187561 = 0.7979663610458374 + 0.01 * 7.13464879989624
Epoch 200, val loss: 0.9903647899627686
Epoch 210, training loss: 0.7991130948066711 = 0.7277756929397583 + 0.01 * 7.133739948272705
Epoch 210, val loss: 0.9417697191238403
Epoch 220, training loss: 0.7324323654174805 = 0.6610940098762512 + 0.01 * 7.13383674621582
Epoch 220, val loss: 0.899060845375061
Epoch 230, training loss: 0.6701234579086304 = 0.598792552947998 + 0.01 * 7.133090972900391
Epoch 230, val loss: 0.8639501929283142
Epoch 240, training loss: 0.6125138998031616 = 0.5411855578422546 + 0.01 * 7.13283634185791
Epoch 240, val loss: 0.836449921131134
Epoch 250, training loss: 0.5594747066497803 = 0.4881493151187897 + 0.01 * 7.132539749145508
Epoch 250, val loss: 0.8158592581748962
Epoch 260, training loss: 0.510680079460144 = 0.439357727766037 + 0.01 * 7.1322340965271
Epoch 260, val loss: 0.8007460236549377
Epoch 270, training loss: 0.46588242053985596 = 0.3945521414279938 + 0.01 * 7.133028030395508
Epoch 270, val loss: 0.789919912815094
Epoch 280, training loss: 0.4248104989528656 = 0.3534988760948181 + 0.01 * 7.131162643432617
Epoch 280, val loss: 0.7826156616210938
Epoch 290, training loss: 0.38711729645729065 = 0.3158174455165863 + 0.01 * 7.1299848556518555
Epoch 290, val loss: 0.7779505252838135
Epoch 300, training loss: 0.3522608280181885 = 0.28097641468048096 + 0.01 * 7.128440856933594
Epoch 300, val loss: 0.7752348780632019
Epoch 310, training loss: 0.3198396563529968 = 0.24854962527751923 + 0.01 * 7.129004001617432
Epoch 310, val loss: 0.7738916277885437
Epoch 320, training loss: 0.2896117866039276 = 0.21836765110492706 + 0.01 * 7.124414443969727
Epoch 320, val loss: 0.7735150456428528
Epoch 330, training loss: 0.2617529630661011 = 0.1905495524406433 + 0.01 * 7.120339393615723
Epoch 330, val loss: 0.7742499113082886
Epoch 340, training loss: 0.23657488822937012 = 0.16539175808429718 + 0.01 * 7.118312358856201
Epoch 340, val loss: 0.776088535785675
Epoch 350, training loss: 0.21423879265785217 = 0.14309795200824738 + 0.01 * 7.114083290100098
Epoch 350, val loss: 0.7792114615440369
Epoch 360, training loss: 0.1947491466999054 = 0.12368317693471909 + 0.01 * 7.10659646987915
Epoch 360, val loss: 0.7836714386940002
Epoch 370, training loss: 0.17809127271175385 = 0.10701512545347214 + 0.01 * 7.107614517211914
Epoch 370, val loss: 0.789535403251648
Epoch 380, training loss: 0.16379868984222412 = 0.0928269699215889 + 0.01 * 7.097172260284424
Epoch 380, val loss: 0.7966538071632385
Epoch 390, training loss: 0.15174344182014465 = 0.08081937581300735 + 0.01 * 7.092406749725342
Epoch 390, val loss: 0.8048921823501587
Epoch 400, training loss: 0.14149007201194763 = 0.07068523019552231 + 0.01 * 7.080483436584473
Epoch 400, val loss: 0.8138056397438049
Epoch 410, training loss: 0.1330907791852951 = 0.06213860586285591 + 0.01 * 7.095217704772949
Epoch 410, val loss: 0.8233208060264587
Epoch 420, training loss: 0.12571650743484497 = 0.05492478981614113 + 0.01 * 7.079171180725098
Epoch 420, val loss: 0.8331297636032104
Epoch 430, training loss: 0.11946311593055725 = 0.04880416393280029 + 0.01 * 7.0658955574035645
Epoch 430, val loss: 0.8431443572044373
Epoch 440, training loss: 0.11420953273773193 = 0.04358414560556412 + 0.01 * 7.0625386238098145
Epoch 440, val loss: 0.8532018661499023
Epoch 450, training loss: 0.10967260599136353 = 0.03910910338163376 + 0.01 * 7.0563507080078125
Epoch 450, val loss: 0.8632748126983643
Epoch 460, training loss: 0.10578949749469757 = 0.03525242581963539 + 0.01 * 7.053706645965576
Epoch 460, val loss: 0.8732451796531677
Epoch 470, training loss: 0.10253304988145828 = 0.03191806375980377 + 0.01 * 7.061498641967773
Epoch 470, val loss: 0.883016049861908
Epoch 480, training loss: 0.09943720698356628 = 0.029020197689533234 + 0.01 * 7.041700839996338
Epoch 480, val loss: 0.8926219940185547
Epoch 490, training loss: 0.09688670933246613 = 0.026484515517950058 + 0.01 * 7.040219306945801
Epoch 490, val loss: 0.9020417332649231
Epoch 500, training loss: 0.09463153779506683 = 0.02425280585885048 + 0.01 * 7.037872791290283
Epoch 500, val loss: 0.9112051725387573
Epoch 510, training loss: 0.09267505258321762 = 0.022280583158135414 + 0.01 * 7.03944730758667
Epoch 510, val loss: 0.9201822280883789
Epoch 520, training loss: 0.09087465703487396 = 0.020532594993710518 + 0.01 * 7.034206867218018
Epoch 520, val loss: 0.9289849400520325
Epoch 530, training loss: 0.08919231593608856 = 0.018976105377078056 + 0.01 * 7.021621227264404
Epoch 530, val loss: 0.9375767707824707
Epoch 540, training loss: 0.08776132762432098 = 0.01758713088929653 + 0.01 * 7.017419815063477
Epoch 540, val loss: 0.9459938406944275
Epoch 550, training loss: 0.08656580001115799 = 0.016345897689461708 + 0.01 * 7.021990776062012
Epoch 550, val loss: 0.9541456699371338
Epoch 560, training loss: 0.08534593135118484 = 0.01523142121732235 + 0.01 * 7.011451244354248
Epoch 560, val loss: 0.9620876908302307
Epoch 570, training loss: 0.08428284525871277 = 0.014226865023374557 + 0.01 * 7.005598068237305
Epoch 570, val loss: 0.9698346257209778
Epoch 580, training loss: 0.08337287604808807 = 0.013320304453372955 + 0.01 * 7.005257606506348
Epoch 580, val loss: 0.9773390889167786
Epoch 590, training loss: 0.08254943788051605 = 0.012501750141382217 + 0.01 * 7.0047688484191895
Epoch 590, val loss: 0.9846283197402954
Epoch 600, training loss: 0.08174444735050201 = 0.011758716776967049 + 0.01 * 6.998573303222656
Epoch 600, val loss: 0.9916297197341919
Epoch 610, training loss: 0.08103491365909576 = 0.011081867851316929 + 0.01 * 6.995304584503174
Epoch 610, val loss: 0.9984683394432068
Epoch 620, training loss: 0.08035469800233841 = 0.010464741848409176 + 0.01 * 6.9889960289001465
Epoch 620, val loss: 1.005109429359436
Epoch 630, training loss: 0.07976362109184265 = 0.009900573641061783 + 0.01 * 6.986304759979248
Epoch 630, val loss: 1.0115097761154175
Epoch 640, training loss: 0.0793369710445404 = 0.009383698925375938 + 0.01 * 6.995327472686768
Epoch 640, val loss: 1.0177397727966309
Epoch 650, training loss: 0.07870667427778244 = 0.008909232914447784 + 0.01 * 6.979743957519531
Epoch 650, val loss: 1.0237101316452026
Epoch 660, training loss: 0.07816486805677414 = 0.008472917601466179 + 0.01 * 6.9691948890686035
Epoch 660, val loss: 1.0295641422271729
Epoch 670, training loss: 0.07783599197864532 = 0.008070576004683971 + 0.01 * 6.976541996002197
Epoch 670, val loss: 1.0352603197097778
Epoch 680, training loss: 0.07727134227752686 = 0.00769897922873497 + 0.01 * 6.957235813140869
Epoch 680, val loss: 1.0407224893569946
Epoch 690, training loss: 0.07684127986431122 = 0.007355587091296911 + 0.01 * 6.948569297790527
Epoch 690, val loss: 1.0460370779037476
Epoch 700, training loss: 0.07669734954833984 = 0.00703717116266489 + 0.01 * 6.966017723083496
Epoch 700, val loss: 1.0511858463287354
Epoch 710, training loss: 0.07614389806985855 = 0.006741994526237249 + 0.01 * 6.940190315246582
Epoch 710, val loss: 1.0561671257019043
Epoch 720, training loss: 0.076027512550354 = 0.006467320024967194 + 0.01 * 6.956019401550293
Epoch 720, val loss: 1.0610166788101196
Epoch 730, training loss: 0.07561436295509338 = 0.006211172789335251 + 0.01 * 6.940318584442139
Epoch 730, val loss: 1.065679669380188
Epoch 740, training loss: 0.0753452479839325 = 0.005972208455204964 + 0.01 * 6.937304496765137
Epoch 740, val loss: 1.0702852010726929
Epoch 750, training loss: 0.0752878338098526 = 0.00574873760342598 + 0.01 * 6.953909397125244
Epoch 750, val loss: 1.0747053623199463
Epoch 760, training loss: 0.07469026744365692 = 0.005539254751056433 + 0.01 * 6.915101528167725
Epoch 760, val loss: 1.0790249109268188
Epoch 770, training loss: 0.07444696873426437 = 0.005343136377632618 + 0.01 * 6.9103827476501465
Epoch 770, val loss: 1.083164930343628
Epoch 780, training loss: 0.07469787448644638 = 0.005159358028322458 + 0.01 * 6.953851699829102
Epoch 780, val loss: 1.0872730016708374
Epoch 790, training loss: 0.07405693829059601 = 0.004986570682376623 + 0.01 * 6.907037258148193
Epoch 790, val loss: 1.091140866279602
Epoch 800, training loss: 0.07386370003223419 = 0.0048240176402032375 + 0.01 * 6.903968334197998
Epoch 800, val loss: 1.0949102640151978
Epoch 810, training loss: 0.07343191653490067 = 0.004671149421483278 + 0.01 * 6.876076698303223
Epoch 810, val loss: 1.0986337661743164
Epoch 820, training loss: 0.07321431487798691 = 0.00452682189643383 + 0.01 * 6.868749618530273
Epoch 820, val loss: 1.1021960973739624
Epoch 830, training loss: 0.07317741960287094 = 0.004390209913253784 + 0.01 * 6.878720760345459
Epoch 830, val loss: 1.1056748628616333
Epoch 840, training loss: 0.0729459896683693 = 0.0042611341923475266 + 0.01 * 6.868485450744629
Epoch 840, val loss: 1.1090582609176636
Epoch 850, training loss: 0.07272561639547348 = 0.004138953052461147 + 0.01 * 6.85866641998291
Epoch 850, val loss: 1.1123672723770142
Epoch 860, training loss: 0.07256897538900375 = 0.004022982437163591 + 0.01 * 6.8545989990234375
Epoch 860, val loss: 1.115561604499817
Epoch 870, training loss: 0.07243966311216354 = 0.0039132460951805115 + 0.01 * 6.852642059326172
Epoch 870, val loss: 1.1187248229980469
Epoch 880, training loss: 0.07233327627182007 = 0.0038087472785264254 + 0.01 * 6.852453708648682
Epoch 880, val loss: 1.1217550039291382
Epoch 890, training loss: 0.07208990305662155 = 0.003709662239998579 + 0.01 * 6.838024616241455
Epoch 890, val loss: 1.1247278451919556
Epoch 900, training loss: 0.07209756225347519 = 0.0036152645479887724 + 0.01 * 6.84822940826416
Epoch 900, val loss: 1.1275761127471924
Epoch 910, training loss: 0.0721164271235466 = 0.00352532509714365 + 0.01 * 6.859110355377197
Epoch 910, val loss: 1.1304502487182617
Epoch 920, training loss: 0.07152977585792542 = 0.003439601045101881 + 0.01 * 6.809017658233643
Epoch 920, val loss: 1.1331762075424194
Epoch 930, training loss: 0.07192163169384003 = 0.00335774221457541 + 0.01 * 6.856389045715332
Epoch 930, val loss: 1.1358158588409424
Epoch 940, training loss: 0.0713646411895752 = 0.0032795267179608345 + 0.01 * 6.808511257171631
Epoch 940, val loss: 1.1384583711624146
Epoch 950, training loss: 0.07127873599529266 = 0.003204702166840434 + 0.01 * 6.807403564453125
Epoch 950, val loss: 1.1410279273986816
Epoch 960, training loss: 0.07111629098653793 = 0.0031330417841672897 + 0.01 * 6.798325061798096
Epoch 960, val loss: 1.1435151100158691
Epoch 970, training loss: 0.07124264538288116 = 0.003064594231545925 + 0.01 * 6.817805290222168
Epoch 970, val loss: 1.1459693908691406
Epoch 980, training loss: 0.0709296390414238 = 0.0029988721944391727 + 0.01 * 6.793076992034912
Epoch 980, val loss: 1.1483501195907593
Epoch 990, training loss: 0.07110852748155594 = 0.002935925265774131 + 0.01 * 6.8172607421875
Epoch 990, val loss: 1.1506903171539307
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 2.030378818511963 = 1.9444102048873901 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9398748874664307
Epoch 10, training loss: 2.0206751823425293 = 1.9347070455551147 + 0.01 * 8.596807479858398
Epoch 10, val loss: 1.9309049844741821
Epoch 20, training loss: 2.0085582733154297 = 1.922592043876648 + 0.01 * 8.596620559692383
Epoch 20, val loss: 1.9195075035095215
Epoch 30, training loss: 1.9914424419403076 = 1.9054824113845825 + 0.01 * 8.596002578735352
Epoch 30, val loss: 1.9032355546951294
Epoch 40, training loss: 1.9664748907089233 = 1.8805519342422485 + 0.01 * 8.592300415039062
Epoch 40, val loss: 1.8798366785049438
Epoch 50, training loss: 1.9321632385253906 = 1.8465123176574707 + 0.01 * 8.565094947814941
Epoch 50, val loss: 1.8493925333023071
Epoch 60, training loss: 1.8937828540802002 = 1.8094825744628906 + 0.01 * 8.43002700805664
Epoch 60, val loss: 1.8189775943756104
Epoch 70, training loss: 1.860582709312439 = 1.777556300163269 + 0.01 * 8.30263900756836
Epoch 70, val loss: 1.7912691831588745
Epoch 80, training loss: 1.819495677947998 = 1.7380685806274414 + 0.01 * 8.14271068572998
Epoch 80, val loss: 1.751780390739441
Epoch 90, training loss: 1.7613410949707031 = 1.682878017425537 + 0.01 * 7.84630823135376
Epoch 90, val loss: 1.6998083591461182
Epoch 100, training loss: 1.6849042177200317 = 1.6087688207626343 + 0.01 * 7.613539218902588
Epoch 100, val loss: 1.6346627473831177
Epoch 110, training loss: 1.5954499244689941 = 1.5199991464614868 + 0.01 * 7.545077323913574
Epoch 110, val loss: 1.5560578107833862
Epoch 120, training loss: 1.5015603303909302 = 1.4266440868377686 + 0.01 * 7.491619110107422
Epoch 120, val loss: 1.4736943244934082
Epoch 130, training loss: 1.4083586931228638 = 1.3341169357299805 + 0.01 * 7.424172878265381
Epoch 130, val loss: 1.39310622215271
Epoch 140, training loss: 1.3149672746658325 = 1.2411863803863525 + 0.01 * 7.37809419631958
Epoch 140, val loss: 1.315374732017517
Epoch 150, training loss: 1.2198107242584229 = 1.146163821220398 + 0.01 * 7.3646955490112305
Epoch 150, val loss: 1.238270878791809
Epoch 160, training loss: 1.1228938102722168 = 1.0493433475494385 + 0.01 * 7.355045795440674
Epoch 160, val loss: 1.1621837615966797
Epoch 170, training loss: 1.026341438293457 = 0.9528813362121582 + 0.01 * 7.346013069152832
Epoch 170, val loss: 1.0884730815887451
Epoch 180, training loss: 0.9340525269508362 = 0.8607147932052612 + 0.01 * 7.333775043487549
Epoch 180, val loss: 1.0200694799423218
Epoch 190, training loss: 0.8506836891174316 = 0.777449369430542 + 0.01 * 7.323431015014648
Epoch 190, val loss: 0.9614303112030029
Epoch 200, training loss: 0.7787293791770935 = 0.7056485414505005 + 0.01 * 7.30808162689209
Epoch 200, val loss: 0.9154236316680908
Epoch 210, training loss: 0.7176720499992371 = 0.644744336605072 + 0.01 * 7.292769432067871
Epoch 210, val loss: 0.8823824524879456
Epoch 220, training loss: 0.6652354001998901 = 0.5924904942512512 + 0.01 * 7.2744927406311035
Epoch 220, val loss: 0.8605769872665405
Epoch 230, training loss: 0.6193584203720093 = 0.5467528104782104 + 0.01 * 7.260563373565674
Epoch 230, val loss: 0.8475716710090637
Epoch 240, training loss: 0.5780957937240601 = 0.5057958960533142 + 0.01 * 7.229989528656006
Epoch 240, val loss: 0.8409738540649414
Epoch 250, training loss: 0.5401450991630554 = 0.46805262565612793 + 0.01 * 7.209248065948486
Epoch 250, val loss: 0.8385891318321228
Epoch 260, training loss: 0.503870964050293 = 0.43201807141304016 + 0.01 * 7.185292720794678
Epoch 260, val loss: 0.8387688398361206
Epoch 270, training loss: 0.46801525354385376 = 0.3962538540363312 + 0.01 * 7.176141738891602
Epoch 270, val loss: 0.8401413559913635
Epoch 280, training loss: 0.4312480688095093 = 0.35964736342430115 + 0.01 * 7.160072326660156
Epoch 280, val loss: 0.8417272567749023
Epoch 290, training loss: 0.3933512568473816 = 0.32185718417167664 + 0.01 * 7.1494059562683105
Epoch 290, val loss: 0.8438142538070679
Epoch 300, training loss: 0.3549422025680542 = 0.2837049067020416 + 0.01 * 7.123730659484863
Epoch 300, val loss: 0.8471039533615112
Epoch 310, training loss: 0.3185826539993286 = 0.247116819024086 + 0.01 * 7.146583557128906
Epoch 310, val loss: 0.8529177904129028
Epoch 320, training loss: 0.28501465916633606 = 0.21413081884384155 + 0.01 * 7.088383674621582
Epoch 320, val loss: 0.8623834848403931
Epoch 330, training loss: 0.25652867555618286 = 0.18562380969524384 + 0.01 * 7.090487003326416
Epoch 330, val loss: 0.8755687475204468
Epoch 340, training loss: 0.2322203814983368 = 0.16141244769096375 + 0.01 * 7.080793380737305
Epoch 340, val loss: 0.8922364115715027
Epoch 350, training loss: 0.21169008314609528 = 0.1409335434436798 + 0.01 * 7.075654029846191
Epoch 350, val loss: 0.9118263125419617
Epoch 360, training loss: 0.19407841563224792 = 0.12356063723564148 + 0.01 * 7.0517778396606445
Epoch 360, val loss: 0.9337708353996277
Epoch 370, training loss: 0.17913907766342163 = 0.10877294838428497 + 0.01 * 7.036612510681152
Epoch 370, val loss: 0.9572679400444031
Epoch 380, training loss: 0.1664409637451172 = 0.09613373130559921 + 0.01 * 7.030724048614502
Epoch 380, val loss: 0.9817737936973572
Epoch 390, training loss: 0.1556118279695511 = 0.08529047667980194 + 0.01 * 7.032135486602783
Epoch 390, val loss: 1.0067946910858154
Epoch 400, training loss: 0.14622853696346283 = 0.0759454071521759 + 0.01 * 7.028313159942627
Epoch 400, val loss: 1.0320459604263306
Epoch 410, training loss: 0.13788026571273804 = 0.06786896288394928 + 0.01 * 7.001131534576416
Epoch 410, val loss: 1.0572733879089355
Epoch 420, training loss: 0.13083487749099731 = 0.06085583195090294 + 0.01 * 6.9979047775268555
Epoch 420, val loss: 1.0822826623916626
Epoch 430, training loss: 0.12454830855131149 = 0.05474664270877838 + 0.01 * 6.980166435241699
Epoch 430, val loss: 1.107004165649414
Epoch 440, training loss: 0.11907120048999786 = 0.04941148683428764 + 0.01 * 6.96597146987915
Epoch 440, val loss: 1.1312922239303589
Epoch 450, training loss: 0.11449629068374634 = 0.044747788459062576 + 0.01 * 6.974850177764893
Epoch 450, val loss: 1.1550674438476562
Epoch 460, training loss: 0.11016341298818588 = 0.04066171497106552 + 0.01 * 6.950170040130615
Epoch 460, val loss: 1.1782512664794922
Epoch 470, training loss: 0.10665033757686615 = 0.037073101848363876 + 0.01 * 6.957724094390869
Epoch 470, val loss: 1.2007867097854614
Epoch 480, training loss: 0.10339510440826416 = 0.033909983932971954 + 0.01 * 6.948512554168701
Epoch 480, val loss: 1.2227336168289185
Epoch 490, training loss: 0.10055304318666458 = 0.03111192025244236 + 0.01 * 6.944112300872803
Epoch 490, val loss: 1.2439284324645996
Epoch 500, training loss: 0.09790939837694168 = 0.02863168902695179 + 0.01 * 6.92777156829834
Epoch 500, val loss: 1.2645210027694702
Epoch 510, training loss: 0.09570927917957306 = 0.02642565593123436 + 0.01 * 6.928361892700195
Epoch 510, val loss: 1.284422516822815
Epoch 520, training loss: 0.0938229113817215 = 0.024455862119793892 + 0.01 * 6.936705112457275
Epoch 520, val loss: 1.303676962852478
Epoch 530, training loss: 0.09201718121767044 = 0.022692792117595673 + 0.01 * 6.932438850402832
Epoch 530, val loss: 1.322330355644226
Epoch 540, training loss: 0.09021076560020447 = 0.02111119031906128 + 0.01 * 6.9099578857421875
Epoch 540, val loss: 1.3403681516647339
Epoch 550, training loss: 0.0889865979552269 = 0.019686484709382057 + 0.01 * 6.930011749267578
Epoch 550, val loss: 1.3578941822052002
Epoch 560, training loss: 0.08726415038108826 = 0.018401360139250755 + 0.01 * 6.8862786293029785
Epoch 560, val loss: 1.3747730255126953
Epoch 570, training loss: 0.08610552549362183 = 0.017237776890397072 + 0.01 * 6.886775493621826
Epoch 570, val loss: 1.3911652565002441
Epoch 580, training loss: 0.08507195860147476 = 0.016181305050849915 + 0.01 * 6.889065742492676
Epoch 580, val loss: 1.4069955348968506
Epoch 590, training loss: 0.08432804048061371 = 0.015219323337078094 + 0.01 * 6.910871505737305
Epoch 590, val loss: 1.4222729206085205
Epoch 600, training loss: 0.0831691324710846 = 0.01434299722313881 + 0.01 * 6.882613182067871
Epoch 600, val loss: 1.437131643295288
Epoch 610, training loss: 0.0823553055524826 = 0.013541587628424168 + 0.01 * 6.881371974945068
Epoch 610, val loss: 1.451521635055542
Epoch 620, training loss: 0.0814785584807396 = 0.012806429527699947 + 0.01 * 6.867213249206543
Epoch 620, val loss: 1.46553373336792
Epoch 630, training loss: 0.08061826229095459 = 0.012130922637879848 + 0.01 * 6.848734378814697
Epoch 630, val loss: 1.4791207313537598
Epoch 640, training loss: 0.08008803427219391 = 0.011508188210427761 + 0.01 * 6.85798454284668
Epoch 640, val loss: 1.4922815561294556
Epoch 650, training loss: 0.0795743316411972 = 0.010932610370218754 + 0.01 * 6.864172458648682
Epoch 650, val loss: 1.5050636529922485
Epoch 660, training loss: 0.07892066240310669 = 0.0104003194719553 + 0.01 * 6.852034568786621
Epoch 660, val loss: 1.517564296722412
Epoch 670, training loss: 0.07841230183839798 = 0.009906245395541191 + 0.01 * 6.8506059646606445
Epoch 670, val loss: 1.5297696590423584
Epoch 680, training loss: 0.07770255953073502 = 0.009446836076676846 + 0.01 * 6.825572490692139
Epoch 680, val loss: 1.5416593551635742
Epoch 690, training loss: 0.07769188284873962 = 0.009018290787935257 + 0.01 * 6.867359161376953
Epoch 690, val loss: 1.5532333850860596
Epoch 700, training loss: 0.07703619450330734 = 0.008619758300483227 + 0.01 * 6.841643810272217
Epoch 700, val loss: 1.5645464658737183
Epoch 710, training loss: 0.07645471394062042 = 0.008248280733823776 + 0.01 * 6.820643901824951
Epoch 710, val loss: 1.575583577156067
Epoch 720, training loss: 0.07615098357200623 = 0.007900785654783249 + 0.01 * 6.825019359588623
Epoch 720, val loss: 1.5863031148910522
Epoch 730, training loss: 0.07574667781591415 = 0.007575043011456728 + 0.01 * 6.817163467407227
Epoch 730, val loss: 1.5968166589736938
Epoch 740, training loss: 0.07556496560573578 = 0.007269491907209158 + 0.01 * 6.829546928405762
Epoch 740, val loss: 1.6071518659591675
Epoch 750, training loss: 0.07523639500141144 = 0.006982683204114437 + 0.01 * 6.825371265411377
Epoch 750, val loss: 1.6172420978546143
Epoch 760, training loss: 0.07465117424726486 = 0.006713325157761574 + 0.01 * 6.793785572052002
Epoch 760, val loss: 1.627073049545288
Epoch 770, training loss: 0.07445800304412842 = 0.006459770258516073 + 0.01 * 6.79982328414917
Epoch 770, val loss: 1.636667013168335
Epoch 780, training loss: 0.07424312829971313 = 0.006220922339707613 + 0.01 * 6.802220821380615
Epoch 780, val loss: 1.6460894346237183
Epoch 790, training loss: 0.07408730685710907 = 0.005996016785502434 + 0.01 * 6.809128761291504
Epoch 790, val loss: 1.6552683115005493
Epoch 800, training loss: 0.07402361929416656 = 0.005784261506050825 + 0.01 * 6.8239359855651855
Epoch 800, val loss: 1.6643261909484863
Epoch 810, training loss: 0.07342324405908585 = 0.0055847736075520515 + 0.01 * 6.783846855163574
Epoch 810, val loss: 1.6731411218643188
Epoch 820, training loss: 0.07341243326663971 = 0.005396153777837753 + 0.01 * 6.8016276359558105
Epoch 820, val loss: 1.6816601753234863
Epoch 830, training loss: 0.0733594074845314 = 0.005217876750975847 + 0.01 * 6.814152717590332
Epoch 830, val loss: 1.6900649070739746
Epoch 840, training loss: 0.0730055719614029 = 0.005049314349889755 + 0.01 * 6.795626163482666
Epoch 840, val loss: 1.698316216468811
Epoch 850, training loss: 0.07279352843761444 = 0.004889434203505516 + 0.01 * 6.790409564971924
Epoch 850, val loss: 1.7063547372817993
Epoch 860, training loss: 0.0724899098277092 = 0.0047378214076161385 + 0.01 * 6.775208950042725
Epoch 860, val loss: 1.7142523527145386
Epoch 870, training loss: 0.07255914062261581 = 0.004593692719936371 + 0.01 * 6.796545028686523
Epoch 870, val loss: 1.721962571144104
Epoch 880, training loss: 0.07214289903640747 = 0.0044569517485797405 + 0.01 * 6.768594741821289
Epoch 880, val loss: 1.7295522689819336
Epoch 890, training loss: 0.07184356451034546 = 0.004327193833887577 + 0.01 * 6.751636981964111
Epoch 890, val loss: 1.73695969581604
Epoch 900, training loss: 0.07172750681638718 = 0.00420379126444459 + 0.01 * 6.7523722648620605
Epoch 900, val loss: 1.7441675662994385
Epoch 910, training loss: 0.07167621701955795 = 0.004086482338607311 + 0.01 * 6.758973598480225
Epoch 910, val loss: 1.751230001449585
Epoch 920, training loss: 0.07153714448213577 = 0.003974865656346083 + 0.01 * 6.756228446960449
Epoch 920, val loss: 1.7581766843795776
Epoch 930, training loss: 0.0714222863316536 = 0.003868561005219817 + 0.01 * 6.755373001098633
Epoch 930, val loss: 1.7649651765823364
Epoch 940, training loss: 0.07153534889221191 = 0.003767166519537568 + 0.01 * 6.776817798614502
Epoch 940, val loss: 1.77162766456604
Epoch 950, training loss: 0.07124155014753342 = 0.0036706833634525537 + 0.01 * 6.757086753845215
Epoch 950, val loss: 1.7781286239624023
Epoch 960, training loss: 0.0710119903087616 = 0.003578583477064967 + 0.01 * 6.743341445922852
Epoch 960, val loss: 1.7844362258911133
Epoch 970, training loss: 0.07086704671382904 = 0.003490478964522481 + 0.01 * 6.737657070159912
Epoch 970, val loss: 1.79066801071167
Epoch 980, training loss: 0.07081210613250732 = 0.0034062284976243973 + 0.01 * 6.740588188171387
Epoch 980, val loss: 1.7967848777770996
Epoch 990, training loss: 0.07080398499965668 = 0.0033257524482905865 + 0.01 * 6.747823238372803
Epoch 990, val loss: 1.8027536869049072
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 2.047187328338623 = 1.9612188339233398 + 0.01 * 8.596854209899902
Epoch 0, val loss: 1.9600318670272827
Epoch 10, training loss: 2.0371077060699463 = 1.9511398077011108 + 0.01 * 8.59678840637207
Epoch 10, val loss: 1.9503819942474365
Epoch 20, training loss: 2.024705410003662 = 1.9387396574020386 + 0.01 * 8.596576690673828
Epoch 20, val loss: 1.9380611181259155
Epoch 30, training loss: 2.007366418838501 = 1.921407699584961 + 0.01 * 8.595877647399902
Epoch 30, val loss: 1.9204286336898804
Epoch 40, training loss: 1.9817200899124146 = 1.8958022594451904 + 0.01 * 8.591782569885254
Epoch 40, val loss: 1.894408106803894
Epoch 50, training loss: 1.9452415704727173 = 1.859613060951233 + 0.01 * 8.562847137451172
Epoch 50, val loss: 1.8589582443237305
Epoch 60, training loss: 1.902917742729187 = 1.8188446760177612 + 0.01 * 8.407305717468262
Epoch 60, val loss: 1.8225817680358887
Epoch 70, training loss: 1.8700069189071655 = 1.7873271703720093 + 0.01 * 8.267975807189941
Epoch 70, val loss: 1.796460509300232
Epoch 80, training loss: 1.8352140188217163 = 1.7545945644378662 + 0.01 * 8.061941146850586
Epoch 80, val loss: 1.7648993730545044
Epoch 90, training loss: 1.7875306606292725 = 1.710572361946106 + 0.01 * 7.695827960968018
Epoch 90, val loss: 1.7229044437408447
Epoch 100, training loss: 1.7246294021606445 = 1.6496912240982056 + 0.01 * 7.493818759918213
Epoch 100, val loss: 1.6667749881744385
Epoch 110, training loss: 1.6445937156677246 = 1.5703188180923462 + 0.01 * 7.427487373352051
Epoch 110, val loss: 1.595427393913269
Epoch 120, training loss: 1.5525221824645996 = 1.4790886640548706 + 0.01 * 7.343356132507324
Epoch 120, val loss: 1.514670491218567
Epoch 130, training loss: 1.4578638076782227 = 1.3850070238113403 + 0.01 * 7.285674095153809
Epoch 130, val loss: 1.4325518608093262
Epoch 140, training loss: 1.3643429279327393 = 1.2918081283569336 + 0.01 * 7.253480434417725
Epoch 140, val loss: 1.3540525436401367
Epoch 150, training loss: 1.2729099988937378 = 1.2006160020828247 + 0.01 * 7.229402542114258
Epoch 150, val loss: 1.279367208480835
Epoch 160, training loss: 1.1850640773773193 = 1.1129822731018066 + 0.01 * 7.208179473876953
Epoch 160, val loss: 1.2096861600875854
Epoch 170, training loss: 1.1024069786071777 = 1.030476689338684 + 0.01 * 7.193023204803467
Epoch 170, val loss: 1.1462267637252808
Epoch 180, training loss: 1.0256366729736328 = 0.9537961483001709 + 0.01 * 7.184051036834717
Epoch 180, val loss: 1.089465856552124
Epoch 190, training loss: 0.9535996317863464 = 0.8818372488021851 + 0.01 * 7.17624044418335
Epoch 190, val loss: 1.0380574464797974
Epoch 200, training loss: 0.8849876523017883 = 0.813299298286438 + 0.01 * 7.168835639953613
Epoch 200, val loss: 0.9908311367034912
Epoch 210, training loss: 0.8193557262420654 = 0.747751772403717 + 0.01 * 7.160394668579102
Epoch 210, val loss: 0.947977602481842
Epoch 220, training loss: 0.7571530342102051 = 0.6856310367584229 + 0.01 * 7.152200698852539
Epoch 220, val loss: 0.9101680517196655
Epoch 230, training loss: 0.6989890933036804 = 0.6275507807731628 + 0.01 * 7.143829822540283
Epoch 230, val loss: 0.8777493238449097
Epoch 240, training loss: 0.6451908946037292 = 0.5738487243652344 + 0.01 * 7.134217262268066
Epoch 240, val loss: 0.8514199256896973
Epoch 250, training loss: 0.5954711437225342 = 0.5242137312889099 + 0.01 * 7.125740051269531
Epoch 250, val loss: 0.8310214281082153
Epoch 260, training loss: 0.5490150451660156 = 0.47782066464424133 + 0.01 * 7.11944055557251
Epoch 260, val loss: 0.8149972558021545
Epoch 270, training loss: 0.5046790242195129 = 0.4335942566394806 + 0.01 * 7.108478546142578
Epoch 270, val loss: 0.8021710515022278
Epoch 280, training loss: 0.46173667907714844 = 0.3907604217529297 + 0.01 * 7.0976243019104
Epoch 280, val loss: 0.7912079691886902
Epoch 290, training loss: 0.4202903211116791 = 0.3492521047592163 + 0.01 * 7.103822231292725
Epoch 290, val loss: 0.7816694378852844
Epoch 300, training loss: 0.3804555833339691 = 0.3095954656600952 + 0.01 * 7.0860114097595215
Epoch 300, val loss: 0.7735975980758667
Epoch 310, training loss: 0.3433876037597656 = 0.27242180705070496 + 0.01 * 7.096578598022461
Epoch 310, val loss: 0.7674524188041687
Epoch 320, training loss: 0.30898645520210266 = 0.2382076382637024 + 0.01 * 7.077880859375
Epoch 320, val loss: 0.7636431455612183
Epoch 330, training loss: 0.2777808904647827 = 0.20707949995994568 + 0.01 * 7.070137977600098
Epoch 330, val loss: 0.7624714374542236
Epoch 340, training loss: 0.24976307153701782 = 0.1790950894355774 + 0.01 * 7.066797256469727
Epoch 340, val loss: 0.7640953063964844
Epoch 350, training loss: 0.22495150566101074 = 0.15432901680469513 + 0.01 * 7.062248229980469
Epoch 350, val loss: 0.7683840394020081
Epoch 360, training loss: 0.20370590686798096 = 0.13281132280826569 + 0.01 * 7.089459419250488
Epoch 360, val loss: 0.7750794887542725
Epoch 370, training loss: 0.18507632613182068 = 0.11441677063703537 + 0.01 * 7.065954685211182
Epoch 370, val loss: 0.7838248014450073
Epoch 380, training loss: 0.1693686842918396 = 0.0988103374838829 + 0.01 * 7.055835723876953
Epoch 380, val loss: 0.7941153645515442
Epoch 390, training loss: 0.15612369775772095 = 0.08563024550676346 + 0.01 * 7.049345970153809
Epoch 390, val loss: 0.805618405342102
Epoch 400, training loss: 0.1449742615222931 = 0.07452616840600967 + 0.01 * 7.044809341430664
Epoch 400, val loss: 0.8179251551628113
Epoch 410, training loss: 0.13570868968963623 = 0.06517732888460159 + 0.01 * 7.053136825561523
Epoch 410, val loss: 0.8307387232780457
Epoch 420, training loss: 0.12766298651695251 = 0.05729938670992851 + 0.01 * 7.036359786987305
Epoch 420, val loss: 0.8438099026679993
Epoch 430, training loss: 0.121263787150383 = 0.05063993111252785 + 0.01 * 7.0623860359191895
Epoch 430, val loss: 0.8569156527519226
Epoch 440, training loss: 0.11527395993471146 = 0.04499717801809311 + 0.01 * 7.027678489685059
Epoch 440, val loss: 0.8698845505714417
Epoch 450, training loss: 0.11046184599399567 = 0.04018567129969597 + 0.01 * 7.027617454528809
Epoch 450, val loss: 0.8826336860656738
Epoch 460, training loss: 0.10625483095645905 = 0.03605501726269722 + 0.01 * 7.019981384277344
Epoch 460, val loss: 0.895096480846405
Epoch 470, training loss: 0.10265327244997025 = 0.03249011188745499 + 0.01 * 7.016315937042236
Epoch 470, val loss: 0.9072566032409668
Epoch 480, training loss: 0.09955841302871704 = 0.02939995378255844 + 0.01 * 7.015845775604248
Epoch 480, val loss: 0.9191011190414429
Epoch 490, training loss: 0.09676449000835419 = 0.02671121433377266 + 0.01 * 7.005328178405762
Epoch 490, val loss: 0.9305561780929565
Epoch 500, training loss: 0.0944494903087616 = 0.024361319839954376 + 0.01 * 7.008817195892334
Epoch 500, val loss: 0.9416783452033997
Epoch 510, training loss: 0.09228277206420898 = 0.02230096608400345 + 0.01 * 6.998180866241455
Epoch 510, val loss: 0.9524475932121277
Epoch 520, training loss: 0.09036844968795776 = 0.020485926419496536 + 0.01 * 6.98825216293335
Epoch 520, val loss: 0.9628371596336365
Epoch 530, training loss: 0.08870090544223785 = 0.018881121650338173 + 0.01 * 6.981978893280029
Epoch 530, val loss: 0.9729070663452148
Epoch 540, training loss: 0.08726834505796432 = 0.017456939443945885 + 0.01 * 6.981140613555908
Epoch 540, val loss: 0.9826523661613464
Epoch 550, training loss: 0.08587083965539932 = 0.016190236434340477 + 0.01 * 6.968060493469238
Epoch 550, val loss: 0.9919946789741516
Epoch 560, training loss: 0.08470386266708374 = 0.015058661811053753 + 0.01 * 6.964520454406738
Epoch 560, val loss: 1.0010700225830078
Epoch 570, training loss: 0.0837516039609909 = 0.014043468981981277 + 0.01 * 6.970814228057861
Epoch 570, val loss: 1.00979483127594
Epoch 580, training loss: 0.08261905610561371 = 0.013131663203239441 + 0.01 * 6.948739528656006
Epoch 580, val loss: 1.0182170867919922
Epoch 590, training loss: 0.08173244446516037 = 0.012309269048273563 + 0.01 * 6.942317962646484
Epoch 590, val loss: 1.026375412940979
Epoch 600, training loss: 0.08103536069393158 = 0.011565347202122211 + 0.01 * 6.9470014572143555
Epoch 600, val loss: 1.0342477560043335
Epoch 610, training loss: 0.08028361201286316 = 0.010892640799283981 + 0.01 * 6.9390974044799805
Epoch 610, val loss: 1.0416979789733887
Epoch 620, training loss: 0.07965396344661713 = 0.010280176065862179 + 0.01 * 6.937378883361816
Epoch 620, val loss: 1.0489697456359863
Epoch 630, training loss: 0.07892744243144989 = 0.009719714522361755 + 0.01 * 6.920773029327393
Epoch 630, val loss: 1.0559684038162231
Epoch 640, training loss: 0.07837595045566559 = 0.009205564856529236 + 0.01 * 6.917038917541504
Epoch 640, val loss: 1.0627772808074951
Epoch 650, training loss: 0.07797541469335556 = 0.008733347989618778 + 0.01 * 6.9242072105407715
Epoch 650, val loss: 1.069404125213623
Epoch 660, training loss: 0.07741347700357437 = 0.008301258087158203 + 0.01 * 6.911221981048584
Epoch 660, val loss: 1.0757108926773071
Epoch 670, training loss: 0.07699200510978699 = 0.007903525605797768 + 0.01 * 6.908848285675049
Epoch 670, val loss: 1.0818698406219482
Epoch 680, training loss: 0.07657700031995773 = 0.007535851560533047 + 0.01 * 6.904115200042725
Epoch 680, val loss: 1.0878658294677734
Epoch 690, training loss: 0.07620305567979813 = 0.007195612415671349 + 0.01 * 6.900744438171387
Epoch 690, val loss: 1.0936540365219116
Epoch 700, training loss: 0.07579855620861053 = 0.006880763918161392 + 0.01 * 6.891778945922852
Epoch 700, val loss: 1.099205493927002
Epoch 710, training loss: 0.07564371824264526 = 0.006588397081941366 + 0.01 * 6.905532360076904
Epoch 710, val loss: 1.1046375036239624
Epoch 720, training loss: 0.07514162361621857 = 0.006316389422863722 + 0.01 * 6.882524013519287
Epoch 720, val loss: 1.109864354133606
Epoch 730, training loss: 0.07484766095876694 = 0.006062954198569059 + 0.01 * 6.878470420837402
Epoch 730, val loss: 1.1149699687957764
Epoch 740, training loss: 0.0746346116065979 = 0.005826124921441078 + 0.01 * 6.880849361419678
Epoch 740, val loss: 1.1198618412017822
Epoch 750, training loss: 0.07427208125591278 = 0.005604843609035015 + 0.01 * 6.866724491119385
Epoch 750, val loss: 1.1246652603149414
Epoch 760, training loss: 0.07412140816450119 = 0.005397896748036146 + 0.01 * 6.87235164642334
Epoch 760, val loss: 1.129273772239685
Epoch 770, training loss: 0.073870949447155 = 0.0052037728019058704 + 0.01 * 6.866717338562012
Epoch 770, val loss: 1.1337692737579346
Epoch 780, training loss: 0.07355538755655289 = 0.00502153392881155 + 0.01 * 6.8533854484558105
Epoch 780, val loss: 1.138128638267517
Epoch 790, training loss: 0.0733683630824089 = 0.004850275814533234 + 0.01 * 6.851809024810791
Epoch 790, val loss: 1.1423156261444092
Epoch 800, training loss: 0.07340319454669952 = 0.004688822198659182 + 0.01 * 6.871437072753906
Epoch 800, val loss: 1.1464229822158813
Epoch 810, training loss: 0.07304441928863525 = 0.004536658059805632 + 0.01 * 6.850776195526123
Epoch 810, val loss: 1.1503534317016602
Epoch 820, training loss: 0.07287436723709106 = 0.004392921458929777 + 0.01 * 6.84814453125
Epoch 820, val loss: 1.1542608737945557
Epoch 830, training loss: 0.07262564450502396 = 0.004257244057953358 + 0.01 * 6.8368401527404785
Epoch 830, val loss: 1.1579664945602417
Epoch 840, training loss: 0.07254187762737274 = 0.004128903150558472 + 0.01 * 6.841297626495361
Epoch 840, val loss: 1.1615785360336304
Epoch 850, training loss: 0.07233268022537231 = 0.004007356241345406 + 0.01 * 6.83253288269043
Epoch 850, val loss: 1.1650924682617188
Epoch 860, training loss: 0.07237476855516434 = 0.0038920934312045574 + 0.01 * 6.848267555236816
Epoch 860, val loss: 1.168542504310608
Epoch 870, training loss: 0.0720842033624649 = 0.0037825906183570623 + 0.01 * 6.830161094665527
Epoch 870, val loss: 1.1718329191207886
Epoch 880, training loss: 0.07206009328365326 = 0.0036786003038287163 + 0.01 * 6.838149547576904
Epoch 880, val loss: 1.175119161605835
Epoch 890, training loss: 0.07173028588294983 = 0.003579914104193449 + 0.01 * 6.815037250518799
Epoch 890, val loss: 1.1782063245773315
Epoch 900, training loss: 0.07174435257911682 = 0.003485832130536437 + 0.01 * 6.825852870941162
Epoch 900, val loss: 1.1812880039215088
Epoch 910, training loss: 0.07182680070400238 = 0.003396258456632495 + 0.01 * 6.84305477142334
Epoch 910, val loss: 1.1843148469924927
Epoch 920, training loss: 0.07152712345123291 = 0.003311007283627987 + 0.01 * 6.8216118812561035
Epoch 920, val loss: 1.1871298551559448
Epoch 930, training loss: 0.07133138179779053 = 0.003229741705581546 + 0.01 * 6.810163974761963
Epoch 930, val loss: 1.1899645328521729
Epoch 940, training loss: 0.07121983915567398 = 0.0031521525233983994 + 0.01 * 6.806768894195557
Epoch 940, val loss: 1.1927462816238403
Epoch 950, training loss: 0.07120640575885773 = 0.0030781254172325134 + 0.01 * 6.812828540802002
Epoch 950, val loss: 1.1953200101852417
Epoch 960, training loss: 0.07098937779664993 = 0.003007231280207634 + 0.01 * 6.798214912414551
Epoch 960, val loss: 1.1979327201843262
Epoch 970, training loss: 0.07090640813112259 = 0.002939349040389061 + 0.01 * 6.796706199645996
Epoch 970, val loss: 1.2003980875015259
Epoch 980, training loss: 0.07075095176696777 = 0.002874301979318261 + 0.01 * 6.787665367126465
Epoch 980, val loss: 1.202921748161316
Epoch 990, training loss: 0.07086927443742752 = 0.0028120416682213545 + 0.01 * 6.805723190307617
Epoch 990, val loss: 1.2052910327911377
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.812335266209805
The final CL Acc:0.76296, 0.02619, The final GNN Acc:0.81234, 0.00172
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13240])
remove edge: torch.Size([2, 7892])
updated graph: torch.Size([2, 10576])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0345866680145264 = 1.9486182928085327 + 0.01 * 8.596831321716309
Epoch 0, val loss: 1.9505658149719238
Epoch 10, training loss: 2.0240724086761475 = 1.9381047487258911 + 0.01 * 8.596765518188477
Epoch 10, val loss: 1.9406874179840088
Epoch 20, training loss: 2.0109875202178955 = 1.9250222444534302 + 0.01 * 8.596538543701172
Epoch 20, val loss: 1.9279656410217285
Epoch 30, training loss: 1.992851734161377 = 1.9068944454193115 + 0.01 * 8.595730781555176
Epoch 30, val loss: 1.9098718166351318
Epoch 40, training loss: 1.966477394104004 = 1.8805729150772095 + 0.01 * 8.590445518493652
Epoch 40, val loss: 1.8834949731826782
Epoch 50, training loss: 1.9298874139785767 = 1.8443644046783447 + 0.01 * 8.552303314208984
Epoch 50, val loss: 1.8481687307357788
Epoch 60, training loss: 1.8875820636749268 = 1.803687572479248 + 0.01 * 8.389443397521973
Epoch 60, val loss: 1.8110384941101074
Epoch 70, training loss: 1.8493844270706177 = 1.7668824195861816 + 0.01 * 8.250205039978027
Epoch 70, val loss: 1.7787399291992188
Epoch 80, training loss: 1.80288565158844 = 1.7223742008209229 + 0.01 * 8.051143646240234
Epoch 80, val loss: 1.737537145614624
Epoch 90, training loss: 1.73822820186615 = 1.6606330871582031 + 0.01 * 7.759513854980469
Epoch 90, val loss: 1.6818890571594238
Epoch 100, training loss: 1.6550740003585815 = 1.5797317028045654 + 0.01 * 7.534228801727295
Epoch 100, val loss: 1.6122779846191406
Epoch 110, training loss: 1.561408281326294 = 1.4869600534439087 + 0.01 * 7.444828510284424
Epoch 110, val loss: 1.5336979627609253
Epoch 120, training loss: 1.4669666290283203 = 1.393398642539978 + 0.01 * 7.3568034172058105
Epoch 120, val loss: 1.457136631011963
Epoch 130, training loss: 1.3758225440979004 = 1.302841067314148 + 0.01 * 7.298152923583984
Epoch 130, val loss: 1.385286569595337
Epoch 140, training loss: 1.2866802215576172 = 1.214034914970398 + 0.01 * 7.26453161239624
Epoch 140, val loss: 1.3178038597106934
Epoch 150, training loss: 1.2000011205673218 = 1.127638816833496 + 0.01 * 7.236227035522461
Epoch 150, val loss: 1.2537890672683716
Epoch 160, training loss: 1.1174545288085938 = 1.0454010963439941 + 0.01 * 7.2053375244140625
Epoch 160, val loss: 1.194395661354065
Epoch 170, training loss: 1.0404573678970337 = 0.9686670899391174 + 0.01 * 7.179025173187256
Epoch 170, val loss: 1.1390223503112793
Epoch 180, training loss: 0.9689143896102905 = 0.8973439335823059 + 0.01 * 7.157047748565674
Epoch 180, val loss: 1.0870492458343506
Epoch 190, training loss: 0.9014975428581238 = 0.8300503492355347 + 0.01 * 7.144721508026123
Epoch 190, val loss: 1.0373321771621704
Epoch 200, training loss: 0.8366372585296631 = 0.7652555704116821 + 0.01 * 7.138167381286621
Epoch 200, val loss: 0.9889618754386902
Epoch 210, training loss: 0.7732192277908325 = 0.7018651962280273 + 0.01 * 7.135406017303467
Epoch 210, val loss: 0.9418492913246155
Epoch 220, training loss: 0.7106654644012451 = 0.6393214464187622 + 0.01 * 7.134399890899658
Epoch 220, val loss: 0.8965307474136353
Epoch 230, training loss: 0.6489607691764832 = 0.5776289105415344 + 0.01 * 7.13318395614624
Epoch 230, val loss: 0.8538573384284973
Epoch 240, training loss: 0.5886538028717041 = 0.5173282623291016 + 0.01 * 7.13255500793457
Epoch 240, val loss: 0.8152880072593689
Epoch 250, training loss: 0.5306724905967712 = 0.45934557914733887 + 0.01 * 7.1326904296875
Epoch 250, val loss: 0.7823352217674255
Epoch 260, training loss: 0.47632741928100586 = 0.4049999713897705 + 0.01 * 7.132744789123535
Epoch 260, val loss: 0.7561555504798889
Epoch 270, training loss: 0.4270334839820862 = 0.3556947708129883 + 0.01 * 7.133871078491211
Epoch 270, val loss: 0.7371672987937927
Epoch 280, training loss: 0.38344141840934753 = 0.31209275126457214 + 0.01 * 7.1348676681518555
Epoch 280, val loss: 0.7250479459762573
Epoch 290, training loss: 0.3450712263584137 = 0.2737123966217041 + 0.01 * 7.13588285446167
Epoch 290, val loss: 0.718356728553772
Epoch 300, training loss: 0.31090524792671204 = 0.23953686654567719 + 0.01 * 7.136837482452393
Epoch 300, val loss: 0.7152148485183716
Epoch 310, training loss: 0.2802882194519043 = 0.20890797674655914 + 0.01 * 7.138023853302002
Epoch 310, val loss: 0.7144870162010193
Epoch 320, training loss: 0.2530442476272583 = 0.181657612323761 + 0.01 * 7.138664722442627
Epoch 320, val loss: 0.7156503796577454
Epoch 330, training loss: 0.22916308045387268 = 0.15777741372585297 + 0.01 * 7.138567924499512
Epoch 330, val loss: 0.7184844613075256
Epoch 340, training loss: 0.2085072100162506 = 0.13712017238140106 + 0.01 * 7.138702869415283
Epoch 340, val loss: 0.7229365706443787
Epoch 350, training loss: 0.19076023995876312 = 0.11938245594501495 + 0.01 * 7.1377787590026855
Epoch 350, val loss: 0.7289403080940247
Epoch 360, training loss: 0.17555680871009827 = 0.1041882261633873 + 0.01 * 7.1368584632873535
Epoch 360, val loss: 0.7363660335540771
Epoch 370, training loss: 0.1625255048274994 = 0.09118000417947769 + 0.01 * 7.13455057144165
Epoch 370, val loss: 0.74489825963974
Epoch 380, training loss: 0.15135245025157928 = 0.08003994822502136 + 0.01 * 7.131250381469727
Epoch 380, val loss: 0.7542543411254883
Epoch 390, training loss: 0.1417645514011383 = 0.07048892229795456 + 0.01 * 7.127562999725342
Epoch 390, val loss: 0.7641150951385498
Epoch 400, training loss: 0.13352078199386597 = 0.06229003518819809 + 0.01 * 7.123075485229492
Epoch 400, val loss: 0.7743099927902222
Epoch 410, training loss: 0.12646357715129852 = 0.055243801325559616 + 0.01 * 7.12197732925415
Epoch 410, val loss: 0.7847194075584412
Epoch 420, training loss: 0.12031769007444382 = 0.04918293654918671 + 0.01 * 7.113475799560547
Epoch 420, val loss: 0.7951680421829224
Epoch 430, training loss: 0.11499865353107452 = 0.04396297410130501 + 0.01 * 7.103567600250244
Epoch 430, val loss: 0.8056427836418152
Epoch 440, training loss: 0.11056066304445267 = 0.039460137486457825 + 0.01 * 7.110053062438965
Epoch 440, val loss: 0.8159712553024292
Epoch 450, training loss: 0.10650122165679932 = 0.03557202219963074 + 0.01 * 7.092920303344727
Epoch 450, val loss: 0.8262116312980652
Epoch 460, training loss: 0.10300037264823914 = 0.03220236301422119 + 0.01 * 7.079801082611084
Epoch 460, val loss: 0.8362774848937988
Epoch 470, training loss: 0.09997739642858505 = 0.029270365834236145 + 0.01 * 7.070703506469727
Epoch 470, val loss: 0.8462428450584412
Epoch 480, training loss: 0.09737556427717209 = 0.026711516082286835 + 0.01 * 7.066405296325684
Epoch 480, val loss: 0.855941653251648
Epoch 490, training loss: 0.09506922215223312 = 0.024469511583447456 + 0.01 * 7.059971332550049
Epoch 490, val loss: 0.8654859066009521
Epoch 500, training loss: 0.09300732612609863 = 0.022496338933706284 + 0.01 * 7.051098346710205
Epoch 500, val loss: 0.8747269511222839
Epoch 510, training loss: 0.0910942330956459 = 0.020751139149069786 + 0.01 * 7.034309387207031
Epoch 510, val loss: 0.8837949633598328
Epoch 520, training loss: 0.0895293802022934 = 0.019198967143893242 + 0.01 * 7.033041477203369
Epoch 520, val loss: 0.892606794834137
Epoch 530, training loss: 0.08804035186767578 = 0.017811942845582962 + 0.01 * 7.022841453552246
Epoch 530, val loss: 0.9012834429740906
Epoch 540, training loss: 0.08664602041244507 = 0.016567209735512733 + 0.01 * 7.0078816413879395
Epoch 540, val loss: 0.9097131490707397
Epoch 550, training loss: 0.0854533240199089 = 0.015445963479578495 + 0.01 * 7.000736236572266
Epoch 550, val loss: 0.9179970026016235
Epoch 560, training loss: 0.08439638465642929 = 0.014433388598263264 + 0.01 * 6.9962992668151855
Epoch 560, val loss: 0.9260739088058472
Epoch 570, training loss: 0.08337096869945526 = 0.013513902202248573 + 0.01 * 6.985706806182861
Epoch 570, val loss: 0.9340208768844604
Epoch 580, training loss: 0.08270802348852158 = 0.01267447043210268 + 0.01 * 7.003355026245117
Epoch 580, val loss: 0.941836416721344
Epoch 590, training loss: 0.08166541159152985 = 0.011909057386219501 + 0.01 * 6.975635051727295
Epoch 590, val loss: 0.949388861656189
Epoch 600, training loss: 0.08094276487827301 = 0.011204883456230164 + 0.01 * 6.973788261413574
Epoch 600, val loss: 0.9568725824356079
Epoch 610, training loss: 0.08027356117963791 = 0.01055542565882206 + 0.01 * 6.971813678741455
Epoch 610, val loss: 0.9641916751861572
Epoch 620, training loss: 0.07967689633369446 = 0.00995648093521595 + 0.01 * 6.972041606903076
Epoch 620, val loss: 0.9712840914726257
Epoch 630, training loss: 0.07899215817451477 = 0.009404446929693222 + 0.01 * 6.958770751953125
Epoch 630, val loss: 0.9783327579498291
Epoch 640, training loss: 0.07838525623083115 = 0.0088949641212821 + 0.01 * 6.949028968811035
Epoch 640, val loss: 0.9851970672607422
Epoch 650, training loss: 0.07781340181827545 = 0.008424987085163593 + 0.01 * 6.938841342926025
Epoch 650, val loss: 0.9918176531791687
Epoch 660, training loss: 0.0774550512433052 = 0.007991449907422066 + 0.01 * 6.946360111236572
Epoch 660, val loss: 0.9984057545661926
Epoch 670, training loss: 0.07686380296945572 = 0.007590805180370808 + 0.01 * 6.927299976348877
Epoch 670, val loss: 1.004720687866211
Epoch 680, training loss: 0.07657217234373093 = 0.007219849620014429 + 0.01 * 6.935232162475586
Epoch 680, val loss: 1.0109120607376099
Epoch 690, training loss: 0.07610386610031128 = 0.006876070983707905 + 0.01 * 6.922779083251953
Epoch 690, val loss: 1.0169917345046997
Epoch 700, training loss: 0.07567711919546127 = 0.006557690445333719 + 0.01 * 6.911943435668945
Epoch 700, val loss: 1.022862434387207
Epoch 710, training loss: 0.07528959959745407 = 0.006262606475502253 + 0.01 * 6.902698993682861
Epoch 710, val loss: 1.0286377668380737
Epoch 720, training loss: 0.07501571625471115 = 0.005988225806504488 + 0.01 * 6.902749061584473
Epoch 720, val loss: 1.0341546535491943
Epoch 730, training loss: 0.07491755485534668 = 0.005732428748160601 + 0.01 * 6.918512344360352
Epoch 730, val loss: 1.0396645069122314
Epoch 740, training loss: 0.07435937970876694 = 0.005494383163750172 + 0.01 * 6.886499404907227
Epoch 740, val loss: 1.0449236631393433
Epoch 750, training loss: 0.07429735362529755 = 0.00527234748005867 + 0.01 * 6.902500152587891
Epoch 750, val loss: 1.0500452518463135
Epoch 760, training loss: 0.07393959909677505 = 0.0050651561468839645 + 0.01 * 6.887444019317627
Epoch 760, val loss: 1.055063247680664
Epoch 770, training loss: 0.07365888357162476 = 0.00487105455249548 + 0.01 * 6.878782749176025
Epoch 770, val loss: 1.0600106716156006
Epoch 780, training loss: 0.07371408492326736 = 0.004689659457653761 + 0.01 * 6.902442932128906
Epoch 780, val loss: 1.0647035837173462
Epoch 790, training loss: 0.0731784775853157 = 0.004519590176641941 + 0.01 * 6.865889072418213
Epoch 790, val loss: 1.0693620443344116
Epoch 800, training loss: 0.0729621946811676 = 0.004359789192676544 + 0.01 * 6.860240936279297
Epoch 800, val loss: 1.0738641023635864
Epoch 810, training loss: 0.07324711978435516 = 0.004209335893392563 + 0.01 * 6.903779029846191
Epoch 810, val loss: 1.0782545804977417
Epoch 820, training loss: 0.07255402207374573 = 0.004067589528858662 + 0.01 * 6.848642826080322
Epoch 820, val loss: 1.0825409889221191
Epoch 830, training loss: 0.07247429341077805 = 0.0039343880489468575 + 0.01 * 6.853990077972412
Epoch 830, val loss: 1.0867159366607666
Epoch 840, training loss: 0.0722920149564743 = 0.0038090187590569258 + 0.01 * 6.848299503326416
Epoch 840, val loss: 1.0907779932022095
Epoch 850, training loss: 0.07224098592996597 = 0.003690408542752266 + 0.01 * 6.855057716369629
Epoch 850, val loss: 1.0947548151016235
Epoch 860, training loss: 0.07185255736112595 = 0.0035781841725111008 + 0.01 * 6.827437400817871
Epoch 860, val loss: 1.0986273288726807
Epoch 870, training loss: 0.07181156426668167 = 0.00347189255990088 + 0.01 * 6.8339667320251465
Epoch 870, val loss: 1.102417230606079
Epoch 880, training loss: 0.07178080826997757 = 0.0033713660668581724 + 0.01 * 6.840944290161133
Epoch 880, val loss: 1.1060889959335327
Epoch 890, training loss: 0.0715557262301445 = 0.0032760377507656813 + 0.01 * 6.827969074249268
Epoch 890, val loss: 1.1097291707992554
Epoch 900, training loss: 0.0713384598493576 = 0.0031857669819146395 + 0.01 * 6.815269470214844
Epoch 900, val loss: 1.1132152080535889
Epoch 910, training loss: 0.07108928263187408 = 0.003099765395745635 + 0.01 * 6.798952579498291
Epoch 910, val loss: 1.1167055368423462
Epoch 920, training loss: 0.07107937335968018 = 0.0030178860761225224 + 0.01 * 6.806148529052734
Epoch 920, val loss: 1.1200406551361084
Epoch 930, training loss: 0.07111719250679016 = 0.00294006010517478 + 0.01 * 6.817713737487793
Epoch 930, val loss: 1.123326301574707
Epoch 940, training loss: 0.07089880108833313 = 0.00286612450145185 + 0.01 * 6.803267955780029
Epoch 940, val loss: 1.1265076398849487
Epoch 950, training loss: 0.07084248960018158 = 0.002795306732878089 + 0.01 * 6.804718494415283
Epoch 950, val loss: 1.1296982765197754
Epoch 960, training loss: 0.07079010456800461 = 0.002727789105847478 + 0.01 * 6.806231498718262
Epoch 960, val loss: 1.1327509880065918
Epoch 970, training loss: 0.0706464946269989 = 0.0026634149253368378 + 0.01 * 6.798308849334717
Epoch 970, val loss: 1.13576078414917
Epoch 980, training loss: 0.07051502913236618 = 0.0026018936187028885 + 0.01 * 6.791314125061035
Epoch 980, val loss: 1.1387383937835693
Epoch 990, training loss: 0.07033157348632812 = 0.0025429371744394302 + 0.01 * 6.778863906860352
Epoch 990, val loss: 1.1416441202163696
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 2.014298677444458 = 1.9283300638198853 + 0.01 * 8.59686279296875
Epoch 0, val loss: 1.923492193222046
Epoch 10, training loss: 2.004031181335449 = 1.9180630445480347 + 0.01 * 8.596807479858398
Epoch 10, val loss: 1.913675308227539
Epoch 20, training loss: 1.9912819862365723 = 1.9053161144256592 + 0.01 * 8.596592903137207
Epoch 20, val loss: 1.9010217189788818
Epoch 30, training loss: 1.9733680486679077 = 1.8874096870422363 + 0.01 * 8.59583568572998
Epoch 30, val loss: 1.8829830884933472
Epoch 40, training loss: 1.9475069046020508 = 1.8615939617156982 + 0.01 * 8.591300010681152
Epoch 40, val loss: 1.8572534322738647
Epoch 50, training loss: 1.9133198261260986 = 1.8277016878128052 + 0.01 * 8.56181812286377
Epoch 50, val loss: 1.825382113456726
Epoch 60, training loss: 1.8774629831314087 = 1.7931628227233887 + 0.01 * 8.43001651763916
Epoch 60, val loss: 1.7968249320983887
Epoch 70, training loss: 1.8422831296920776 = 1.7600349187850952 + 0.01 * 8.22482681274414
Epoch 70, val loss: 1.7692553997039795
Epoch 80, training loss: 1.7935954332351685 = 1.7130751609802246 + 0.01 * 8.05202865600586
Epoch 80, val loss: 1.7257734537124634
Epoch 90, training loss: 1.7263641357421875 = 1.647878646850586 + 0.01 * 7.848547458648682
Epoch 90, val loss: 1.6663551330566406
Epoch 100, training loss: 1.6406211853027344 = 1.564083218574524 + 0.01 * 7.6537933349609375
Epoch 100, val loss: 1.5945731401443481
Epoch 110, training loss: 1.5456839799880981 = 1.470908284187317 + 0.01 * 7.477565765380859
Epoch 110, val loss: 1.5163108110427856
Epoch 120, training loss: 1.450392723083496 = 1.3758864402770996 + 0.01 * 7.450632095336914
Epoch 120, val loss: 1.4385064840316772
Epoch 130, training loss: 1.3545972108840942 = 1.2805261611938477 + 0.01 * 7.407105445861816
Epoch 130, val loss: 1.3617078065872192
Epoch 140, training loss: 1.2591302394866943 = 1.1853758096694946 + 0.01 * 7.375443935394287
Epoch 140, val loss: 1.2849725484848022
Epoch 150, training loss: 1.1670401096343994 = 1.0935938358306885 + 0.01 * 7.344625473022461
Epoch 150, val loss: 1.2117635011672974
Epoch 160, training loss: 1.0824110507965088 = 1.0091761350631714 + 0.01 * 7.323495864868164
Epoch 160, val loss: 1.1461971998214722
Epoch 170, training loss: 1.0057417154312134 = 0.9326169490814209 + 0.01 * 7.3124823570251465
Epoch 170, val loss: 1.0881164073944092
Epoch 180, training loss: 0.9340676665306091 = 0.8610638976097107 + 0.01 * 7.3003764152526855
Epoch 180, val loss: 1.034305214881897
Epoch 190, training loss: 0.864105761051178 = 0.7912725806236267 + 0.01 * 7.283316135406494
Epoch 190, val loss: 0.982200562953949
Epoch 200, training loss: 0.7947925329208374 = 0.7221856713294983 + 0.01 * 7.260687828063965
Epoch 200, val loss: 0.9314416646957397
Epoch 210, training loss: 0.7278191447257996 = 0.6554719805717468 + 0.01 * 7.234716892242432
Epoch 210, val loss: 0.8845826983451843
Epoch 220, training loss: 0.6659507751464844 = 0.5937932729721069 + 0.01 * 7.215747356414795
Epoch 220, val loss: 0.8444186449050903
Epoch 230, training loss: 0.6105400323867798 = 0.5385147333145142 + 0.01 * 7.202528953552246
Epoch 230, val loss: 0.8127973079681396
Epoch 240, training loss: 0.5607464909553528 = 0.4888056814670563 + 0.01 * 7.194081783294678
Epoch 240, val loss: 0.789015531539917
Epoch 250, training loss: 0.5143918395042419 = 0.4424993395805359 + 0.01 * 7.1892476081848145
Epoch 250, val loss: 0.7707587480545044
Epoch 260, training loss: 0.4692992568016052 = 0.39747652411460876 + 0.01 * 7.182272911071777
Epoch 260, val loss: 0.7553224563598633
Epoch 270, training loss: 0.4244399666786194 = 0.3526895344257355 + 0.01 * 7.175043106079102
Epoch 270, val loss: 0.7409675121307373
Epoch 280, training loss: 0.3799789547920227 = 0.30829283595085144 + 0.01 * 7.168613433837891
Epoch 280, val loss: 0.7275281548500061
Epoch 290, training loss: 0.3372516930103302 = 0.26559978723526 + 0.01 * 7.165191650390625
Epoch 290, val loss: 0.715499758720398
Epoch 300, training loss: 0.2979159951210022 = 0.22631916403770447 + 0.01 * 7.159682273864746
Epoch 300, val loss: 0.7061376571655273
Epoch 310, training loss: 0.26338478922843933 = 0.19184280931949615 + 0.01 * 7.1541972160339355
Epoch 310, val loss: 0.700480043888092
Epoch 320, training loss: 0.23422634601593018 = 0.16272911429405212 + 0.01 * 7.149724006652832
Epoch 320, val loss: 0.6988903284072876
Epoch 330, training loss: 0.21015477180480957 = 0.1387036144733429 + 0.01 * 7.145116806030273
Epoch 330, val loss: 0.7011702656745911
Epoch 340, training loss: 0.1904209852218628 = 0.1190367341041565 + 0.01 * 7.138424396514893
Epoch 340, val loss: 0.7067604660987854
Epoch 350, training loss: 0.17421361804008484 = 0.10289350152015686 + 0.01 * 7.132012367248535
Epoch 350, val loss: 0.7148909568786621
Epoch 360, training loss: 0.16077935695648193 = 0.08953356742858887 + 0.01 * 7.124578475952148
Epoch 360, val loss: 0.7247807383537292
Epoch 370, training loss: 0.1495601087808609 = 0.07836870104074478 + 0.01 * 7.119141101837158
Epoch 370, val loss: 0.7359000444412231
Epoch 380, training loss: 0.14007851481437683 = 0.0689602643251419 + 0.01 * 7.111824989318848
Epoch 380, val loss: 0.7477839589118958
Epoch 390, training loss: 0.13199633359909058 = 0.06097089871764183 + 0.01 * 7.102543830871582
Epoch 390, val loss: 0.7601712346076965
Epoch 400, training loss: 0.12514793872833252 = 0.05414624884724617 + 0.01 * 7.100168704986572
Epoch 400, val loss: 0.7727677226066589
Epoch 410, training loss: 0.11923149973154068 = 0.04828900098800659 + 0.01 * 7.094249725341797
Epoch 410, val loss: 0.7855571508407593
Epoch 420, training loss: 0.11418436467647552 = 0.04324379563331604 + 0.01 * 7.094057083129883
Epoch 420, val loss: 0.7982698082923889
Epoch 430, training loss: 0.10964764654636383 = 0.03888491913676262 + 0.01 * 7.076272487640381
Epoch 430, val loss: 0.8109818696975708
Epoch 440, training loss: 0.10579340159893036 = 0.035105351358652115 + 0.01 * 7.068804740905762
Epoch 440, val loss: 0.823458194732666
Epoch 450, training loss: 0.10255682468414307 = 0.03181738778948784 + 0.01 * 7.073944568634033
Epoch 450, val loss: 0.835845947265625
Epoch 460, training loss: 0.09947709739208221 = 0.028949931263923645 + 0.01 * 7.0527167320251465
Epoch 460, val loss: 0.8479233384132385
Epoch 470, training loss: 0.09693913906812668 = 0.026438729837536812 + 0.01 * 7.050041198730469
Epoch 470, val loss: 0.859762966632843
Epoch 480, training loss: 0.09477797895669937 = 0.024231374263763428 + 0.01 * 7.054660797119141
Epoch 480, val loss: 0.8712921738624573
Epoch 490, training loss: 0.09270019829273224 = 0.022284846752882004 + 0.01 * 7.041534900665283
Epoch 490, val loss: 0.8826212882995605
Epoch 500, training loss: 0.09085497260093689 = 0.020560575649142265 + 0.01 * 7.029439449310303
Epoch 500, val loss: 0.8935673236846924
Epoch 510, training loss: 0.08925020694732666 = 0.019028479233384132 + 0.01 * 7.022172927856445
Epoch 510, val loss: 0.9042735695838928
Epoch 520, training loss: 0.08789349347352982 = 0.017663130536675453 + 0.01 * 7.023036479949951
Epoch 520, val loss: 0.9146219491958618
Epoch 530, training loss: 0.08658193796873093 = 0.016440847888588905 + 0.01 * 7.0141096115112305
Epoch 530, val loss: 0.9247658252716064
Epoch 540, training loss: 0.0853847935795784 = 0.015343151055276394 + 0.01 * 7.004164218902588
Epoch 540, val loss: 0.9345374703407288
Epoch 550, training loss: 0.08436256647109985 = 0.014354380778968334 + 0.01 * 7.000819206237793
Epoch 550, val loss: 0.9440881609916687
Epoch 560, training loss: 0.08355192840099335 = 0.01346050389111042 + 0.01 * 7.009142875671387
Epoch 560, val loss: 0.9534270167350769
Epoch 570, training loss: 0.08263912051916122 = 0.012651210650801659 + 0.01 * 6.998791217803955
Epoch 570, val loss: 0.9622840881347656
Epoch 580, training loss: 0.08177612721920013 = 0.01191611960530281 + 0.01 * 6.986001491546631
Epoch 580, val loss: 0.9709683060646057
Epoch 590, training loss: 0.08107536286115646 = 0.011245643720030785 + 0.01 * 6.982972145080566
Epoch 590, val loss: 0.9794553518295288
Epoch 600, training loss: 0.08055799454450607 = 0.010632544755935669 + 0.01 * 6.992545127868652
Epoch 600, val loss: 0.9876577854156494
Epoch 610, training loss: 0.07989269495010376 = 0.010071512311697006 + 0.01 * 6.982118606567383
Epoch 610, val loss: 0.9956945180892944
Epoch 620, training loss: 0.07921840250492096 = 0.00955620314925909 + 0.01 * 6.966219902038574
Epoch 620, val loss: 1.0033340454101562
Epoch 630, training loss: 0.07878115773200989 = 0.009081799536943436 + 0.01 * 6.969935417175293
Epoch 630, val loss: 1.0108938217163086
Epoch 640, training loss: 0.07826580107212067 = 0.008644855581223965 + 0.01 * 6.962094783782959
Epoch 640, val loss: 1.0182294845581055
Epoch 650, training loss: 0.07773784548044205 = 0.00824078731238842 + 0.01 * 6.949706077575684
Epoch 650, val loss: 1.0252292156219482
Epoch 660, training loss: 0.07730627059936523 = 0.007866456173360348 + 0.01 * 6.943981647491455
Epoch 660, val loss: 1.032196044921875
Epoch 670, training loss: 0.07699783891439438 = 0.007519298233091831 + 0.01 * 6.947854042053223
Epoch 670, val loss: 1.038816213607788
Epoch 680, training loss: 0.07658054679632187 = 0.007195785641670227 + 0.01 * 6.9384765625
Epoch 680, val loss: 1.0453248023986816
Epoch 690, training loss: 0.0763777494430542 = 0.006893933285027742 + 0.01 * 6.9483819007873535
Epoch 690, val loss: 1.0517865419387817
Epoch 700, training loss: 0.07591484487056732 = 0.006611847784370184 + 0.01 * 6.930299758911133
Epoch 700, val loss: 1.0578687191009521
Epoch 710, training loss: 0.0755796879529953 = 0.006347511429339647 + 0.01 * 6.9232177734375
Epoch 710, val loss: 1.0640006065368652
Epoch 720, training loss: 0.07535363733768463 = 0.006099988706409931 + 0.01 * 6.925364971160889
Epoch 720, val loss: 1.069878101348877
Epoch 730, training loss: 0.07510905712842941 = 0.005867567379027605 + 0.01 * 6.924149036407471
Epoch 730, val loss: 1.0755398273468018
Epoch 740, training loss: 0.07484982162714005 = 0.005649314261972904 + 0.01 * 6.920050621032715
Epoch 740, val loss: 1.081226110458374
Epoch 750, training loss: 0.07462596148252487 = 0.00544416718184948 + 0.01 * 6.918179035186768
Epoch 750, val loss: 1.0867270231246948
Epoch 760, training loss: 0.07424178719520569 = 0.005250914953649044 + 0.01 * 6.899087429046631
Epoch 760, val loss: 1.0918670892715454
Epoch 770, training loss: 0.07411211729049683 = 0.005068536382168531 + 0.01 * 6.904358863830566
Epoch 770, val loss: 1.0971081256866455
Epoch 780, training loss: 0.07385540008544922 = 0.004896577447652817 + 0.01 * 6.895882606506348
Epoch 780, val loss: 1.1021606922149658
Epoch 790, training loss: 0.07381019741296768 = 0.004733779467642307 + 0.01 * 6.907641887664795
Epoch 790, val loss: 1.1069773435592651
Epoch 800, training loss: 0.07348302006721497 = 0.004580180626362562 + 0.01 * 6.890284538269043
Epoch 800, val loss: 1.1118463277816772
Epoch 810, training loss: 0.07330825179815292 = 0.004435514099895954 + 0.01 * 6.887273788452148
Epoch 810, val loss: 1.1165378093719482
Epoch 820, training loss: 0.07303129881620407 = 0.004299125634133816 + 0.01 * 6.873217582702637
Epoch 820, val loss: 1.1210150718688965
Epoch 830, training loss: 0.0729617327451706 = 0.004169756080955267 + 0.01 * 6.879197597503662
Epoch 830, val loss: 1.1255439519882202
Epoch 840, training loss: 0.07285245507955551 = 0.0040473234839737415 + 0.01 * 6.8805131912231445
Epoch 840, val loss: 1.1298972368240356
Epoch 850, training loss: 0.07268231362104416 = 0.0039310879074037075 + 0.01 * 6.875122547149658
Epoch 850, val loss: 1.1339830160140991
Epoch 860, training loss: 0.07253878563642502 = 0.003820707555860281 + 0.01 * 6.87180757522583
Epoch 860, val loss: 1.1382910013198853
Epoch 870, training loss: 0.07238364219665527 = 0.0037159514613449574 + 0.01 * 6.866768836975098
Epoch 870, val loss: 1.1422841548919678
Epoch 880, training loss: 0.07214167714118958 = 0.0036162317264825106 + 0.01 * 6.852544784545898
Epoch 880, val loss: 1.1462706327438354
Epoch 890, training loss: 0.07221538573503494 = 0.0035211362410336733 + 0.01 * 6.869424819946289
Epoch 890, val loss: 1.1501007080078125
Epoch 900, training loss: 0.07201715558767319 = 0.0034306272864341736 + 0.01 * 6.8586530685424805
Epoch 900, val loss: 1.153841495513916
Epoch 910, training loss: 0.07181505858898163 = 0.0033443737775087357 + 0.01 * 6.847068786621094
Epoch 910, val loss: 1.157551646232605
Epoch 920, training loss: 0.07167582958936691 = 0.0032619426492601633 + 0.01 * 6.841388702392578
Epoch 920, val loss: 1.1612303256988525
Epoch 930, training loss: 0.07165601849555969 = 0.003183235414326191 + 0.01 * 6.847278118133545
Epoch 930, val loss: 1.1646039485931396
Epoch 940, training loss: 0.07150139659643173 = 0.003108000149950385 + 0.01 * 6.839339256286621
Epoch 940, val loss: 1.1679924726486206
Epoch 950, training loss: 0.07134658843278885 = 0.0030362787656486034 + 0.01 * 6.831031322479248
Epoch 950, val loss: 1.171317458152771
Epoch 960, training loss: 0.07158758491277695 = 0.002967665670439601 + 0.01 * 6.861992359161377
Epoch 960, val loss: 1.1746717691421509
Epoch 970, training loss: 0.07122498750686646 = 0.0029018581844866276 + 0.01 * 6.832313060760498
Epoch 970, val loss: 1.177762746810913
Epoch 980, training loss: 0.0710705816745758 = 0.0028389054350554943 + 0.01 * 6.8231682777404785
Epoch 980, val loss: 1.1809450387954712
Epoch 990, training loss: 0.07118719071149826 = 0.0027783631812781096 + 0.01 * 6.840882778167725
Epoch 990, val loss: 1.184097409248352
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8439641539272537
=== training gcn model ===
Epoch 0, training loss: 2.0245614051818848 = 1.938592791557312 + 0.01 * 8.596855163574219
Epoch 0, val loss: 1.9397071599960327
Epoch 10, training loss: 2.0150580406188965 = 1.9290897846221924 + 0.01 * 8.596830368041992
Epoch 10, val loss: 1.9303991794586182
Epoch 20, training loss: 2.003628730773926 = 1.9176619052886963 + 0.01 * 8.596671104431152
Epoch 20, val loss: 1.9192699193954468
Epoch 30, training loss: 1.9878790378570557 = 1.9019176959991455 + 0.01 * 8.596137046813965
Epoch 30, val loss: 1.904022455215454
Epoch 40, training loss: 1.9649720191955566 = 1.8790382146835327 + 0.01 * 8.593384742736816
Epoch 40, val loss: 1.8818382024765015
Epoch 50, training loss: 1.9323053359985352 = 1.8465709686279297 + 0.01 * 8.573433876037598
Epoch 50, val loss: 1.8512730598449707
Epoch 60, training loss: 1.8910715579986572 = 1.806397557258606 + 0.01 * 8.467398643493652
Epoch 60, val loss: 1.8154568672180176
Epoch 70, training loss: 1.8466781377792358 = 1.7651621103286743 + 0.01 * 8.151599884033203
Epoch 70, val loss: 1.7790940999984741
Epoch 80, training loss: 1.7954427003860474 = 1.716052532196045 + 0.01 * 7.939015865325928
Epoch 80, val loss: 1.731878638267517
Epoch 90, training loss: 1.7247965335845947 = 1.6465686559677124 + 0.01 * 7.822786808013916
Epoch 90, val loss: 1.6672186851501465
Epoch 100, training loss: 1.6317592859268188 = 1.55458664894104 + 0.01 * 7.717267036437988
Epoch 100, val loss: 1.5870566368103027
Epoch 110, training loss: 1.522451400756836 = 1.4466334581375122 + 0.01 * 7.581793308258057
Epoch 110, val loss: 1.4942243099212646
Epoch 120, training loss: 1.4105089902877808 = 1.335221290588379 + 0.01 * 7.528765678405762
Epoch 120, val loss: 1.3999433517456055
Epoch 130, training loss: 1.3039164543151855 = 1.2290672063827515 + 0.01 * 7.484928607940674
Epoch 130, val loss: 1.3126323223114014
Epoch 140, training loss: 1.2043895721435547 = 1.1298757791519165 + 0.01 * 7.451377868652344
Epoch 140, val loss: 1.2327560186386108
Epoch 150, training loss: 1.1088429689407349 = 1.0347118377685547 + 0.01 * 7.413118362426758
Epoch 150, val loss: 1.157729148864746
Epoch 160, training loss: 1.0157928466796875 = 0.9422073364257812 + 0.01 * 7.358555316925049
Epoch 160, val loss: 1.0851287841796875
Epoch 170, training loss: 0.9261865615844727 = 0.8531466722488403 + 0.01 * 7.30399227142334
Epoch 170, val loss: 1.0161391496658325
Epoch 180, training loss: 0.842866063117981 = 0.7700952291488647 + 0.01 * 7.2770867347717285
Epoch 180, val loss: 0.95306396484375
Epoch 190, training loss: 0.7678879499435425 = 0.6952463984489441 + 0.01 * 7.264157295227051
Epoch 190, val loss: 0.898689866065979
Epoch 200, training loss: 0.7018621563911438 = 0.6293244957923889 + 0.01 * 7.253767490386963
Epoch 200, val loss: 0.8545742034912109
Epoch 210, training loss: 0.6440262198448181 = 0.5716251134872437 + 0.01 * 7.240111827850342
Epoch 210, val loss: 0.8202906250953674
Epoch 220, training loss: 0.5930293798446655 = 0.5208036303520203 + 0.01 * 7.222572326660156
Epoch 220, val loss: 0.794380247592926
Epoch 230, training loss: 0.5473264455795288 = 0.4753011167049408 + 0.01 * 7.202536106109619
Epoch 230, val loss: 0.7748026251792908
Epoch 240, training loss: 0.5056048631668091 = 0.43378448486328125 + 0.01 * 7.182036399841309
Epoch 240, val loss: 0.7598617076873779
Epoch 250, training loss: 0.4670245349407196 = 0.39541199803352356 + 0.01 * 7.161252975463867
Epoch 250, val loss: 0.7486662864685059
Epoch 260, training loss: 0.4313594400882721 = 0.3598891794681549 + 0.01 * 7.147025108337402
Epoch 260, val loss: 0.7409098744392395
Epoch 270, training loss: 0.398452490568161 = 0.32712429761886597 + 0.01 * 7.132819652557373
Epoch 270, val loss: 0.7361786365509033
Epoch 280, training loss: 0.36796796321868896 = 0.2967522442340851 + 0.01 * 7.121571063995361
Epoch 280, val loss: 0.733662486076355
Epoch 290, training loss: 0.3391532897949219 = 0.26802513003349304 + 0.01 * 7.1128153800964355
Epoch 290, val loss: 0.7326130867004395
Epoch 300, training loss: 0.3111523389816284 = 0.24015331268310547 + 0.01 * 7.099902629852295
Epoch 300, val loss: 0.7325983047485352
Epoch 310, training loss: 0.2838294506072998 = 0.21288396418094635 + 0.01 * 7.094550609588623
Epoch 310, val loss: 0.7332402467727661
Epoch 320, training loss: 0.2576383650302887 = 0.18683911859989166 + 0.01 * 7.079925060272217
Epoch 320, val loss: 0.7349370718002319
Epoch 330, training loss: 0.23409610986709595 = 0.16308315098285675 + 0.01 * 7.101296901702881
Epoch 330, val loss: 0.7381951212882996
Epoch 340, training loss: 0.21312516927719116 = 0.14234775304794312 + 0.01 * 7.077742576599121
Epoch 340, val loss: 0.7433328032493591
Epoch 350, training loss: 0.19516989588737488 = 0.12463855743408203 + 0.01 * 7.053133010864258
Epoch 350, val loss: 0.7502002120018005
Epoch 360, training loss: 0.18002408742904663 = 0.1095731109380722 + 0.01 * 7.045097827911377
Epoch 360, val loss: 0.7586727738380432
Epoch 370, training loss: 0.1674576997756958 = 0.0967080369591713 + 0.01 * 7.074965953826904
Epoch 370, val loss: 0.7683204412460327
Epoch 380, training loss: 0.15606814622879028 = 0.08568042516708374 + 0.01 * 7.038771152496338
Epoch 380, val loss: 0.7787935137748718
Epoch 390, training loss: 0.14638185501098633 = 0.07615432888269424 + 0.01 * 7.022753715515137
Epoch 390, val loss: 0.7897006273269653
Epoch 400, training loss: 0.13807210326194763 = 0.06787411868572235 + 0.01 * 7.01979923248291
Epoch 400, val loss: 0.8009668588638306
Epoch 410, training loss: 0.13078634440898895 = 0.060642700642347336 + 0.01 * 7.014364242553711
Epoch 410, val loss: 0.8124493360519409
Epoch 420, training loss: 0.1243780106306076 = 0.054316479712724686 + 0.01 * 7.006153583526611
Epoch 420, val loss: 0.8239222764968872
Epoch 430, training loss: 0.11877952516078949 = 0.04877467826008797 + 0.01 * 7.000484466552734
Epoch 430, val loss: 0.8353563547134399
Epoch 440, training loss: 0.11388976871967316 = 0.043912000954151154 + 0.01 * 6.997776985168457
Epoch 440, val loss: 0.8466616272926331
Epoch 450, training loss: 0.1096653863787651 = 0.03964046388864517 + 0.01 * 7.002492427825928
Epoch 450, val loss: 0.8577854037284851
Epoch 460, training loss: 0.10583323240280151 = 0.035887811332941055 + 0.01 * 6.994542121887207
Epoch 460, val loss: 0.8687256574630737
Epoch 470, training loss: 0.10240998864173889 = 0.03258652612566948 + 0.01 * 6.982346534729004
Epoch 470, val loss: 0.8793287873268127
Epoch 480, training loss: 0.09950409829616547 = 0.029676441103219986 + 0.01 * 6.982765197753906
Epoch 480, val loss: 0.8897055387496948
Epoch 490, training loss: 0.0968572348356247 = 0.027111221104860306 + 0.01 * 6.974600791931152
Epoch 490, val loss: 0.8998012542724609
Epoch 500, training loss: 0.09457282721996307 = 0.024844758212566376 + 0.01 * 6.97280740737915
Epoch 500, val loss: 0.9095945954322815
Epoch 510, training loss: 0.09247059375047684 = 0.02283685840666294 + 0.01 * 6.96337366104126
Epoch 510, val loss: 0.9191407561302185
Epoch 520, training loss: 0.09074504673480988 = 0.021053563803434372 + 0.01 * 6.9691481590271
Epoch 520, val loss: 0.9284089803695679
Epoch 530, training loss: 0.08907148241996765 = 0.019466638565063477 + 0.01 * 6.960484504699707
Epoch 530, val loss: 0.9374355673789978
Epoch 540, training loss: 0.08756491541862488 = 0.01805020309984684 + 0.01 * 6.95147180557251
Epoch 540, val loss: 0.9461889266967773
Epoch 550, training loss: 0.08623684197664261 = 0.01678215153515339 + 0.01 * 6.945469379425049
Epoch 550, val loss: 0.9546782374382019
Epoch 560, training loss: 0.08504097908735275 = 0.01564379595220089 + 0.01 * 6.939718246459961
Epoch 560, val loss: 0.9629254341125488
Epoch 570, training loss: 0.08398471027612686 = 0.014619194902479649 + 0.01 * 6.936552047729492
Epoch 570, val loss: 0.9709054231643677
Epoch 580, training loss: 0.08299778401851654 = 0.01369563490152359 + 0.01 * 6.930215358734131
Epoch 580, val loss: 0.9786644577980042
Epoch 590, training loss: 0.08206252753734589 = 0.012859059497714043 + 0.01 * 6.920346736907959
Epoch 590, val loss: 0.9862119555473328
Epoch 600, training loss: 0.0813663899898529 = 0.012099407613277435 + 0.01 * 6.926698684692383
Epoch 600, val loss: 0.9935333132743835
Epoch 610, training loss: 0.08050547540187836 = 0.011407997459173203 + 0.01 * 6.909748554229736
Epoch 610, val loss: 1.0006866455078125
Epoch 620, training loss: 0.08006539195775986 = 0.010776439681649208 + 0.01 * 6.928895473480225
Epoch 620, val loss: 1.0076422691345215
Epoch 630, training loss: 0.07929539680480957 = 0.010199234820902348 + 0.01 * 6.909616470336914
Epoch 630, val loss: 1.0144416093826294
Epoch 640, training loss: 0.07861055433750153 = 0.009670281782746315 + 0.01 * 6.8940277099609375
Epoch 640, val loss: 1.021011471748352
Epoch 650, training loss: 0.07822535932064056 = 0.00918347667902708 + 0.01 * 6.904188632965088
Epoch 650, val loss: 1.0274385213851929
Epoch 660, training loss: 0.0778258740901947 = 0.008734755218029022 + 0.01 * 6.909111976623535
Epoch 660, val loss: 1.0336743593215942
Epoch 670, training loss: 0.0773024633526802 = 0.008321118541061878 + 0.01 * 6.898134231567383
Epoch 670, val loss: 1.039785385131836
Epoch 680, training loss: 0.0767398551106453 = 0.007938982918858528 + 0.01 * 6.880087375640869
Epoch 680, val loss: 1.0456818342208862
Epoch 690, training loss: 0.07644151151180267 = 0.0075844042003154755 + 0.01 * 6.8857102394104
Epoch 690, val loss: 1.0514637231826782
Epoch 700, training loss: 0.07604879885911942 = 0.007254953496158123 + 0.01 * 6.879384994506836
Epoch 700, val loss: 1.0571168661117554
Epoch 710, training loss: 0.07560281455516815 = 0.0069490037858486176 + 0.01 * 6.865380764007568
Epoch 710, val loss: 1.062622308731079
Epoch 720, training loss: 0.07534346729516983 = 0.006664259359240532 + 0.01 * 6.867920875549316
Epoch 720, val loss: 1.0679618120193481
Epoch 730, training loss: 0.07502220571041107 = 0.006398359313607216 + 0.01 * 6.862384796142578
Epoch 730, val loss: 1.0731838941574097
Epoch 740, training loss: 0.07470554858446121 = 0.006149784661829472 + 0.01 * 6.855576992034912
Epoch 740, val loss: 1.0782504081726074
Epoch 750, training loss: 0.07465912401676178 = 0.005917007103562355 + 0.01 * 6.874212265014648
Epoch 750, val loss: 1.083193063735962
Epoch 760, training loss: 0.0741865262389183 = 0.00569887924939394 + 0.01 * 6.848764896392822
Epoch 760, val loss: 1.0880707502365112
Epoch 770, training loss: 0.07408332824707031 = 0.005494263954460621 + 0.01 * 6.858906269073486
Epoch 770, val loss: 1.0927716493606567
Epoch 780, training loss: 0.0736980140209198 = 0.005302035715430975 + 0.01 * 6.839598178863525
Epoch 780, val loss: 1.0974043607711792
Epoch 790, training loss: 0.07343430072069168 = 0.005121056921780109 + 0.01 * 6.831324100494385
Epoch 790, val loss: 1.1018918752670288
Epoch 800, training loss: 0.0732608363032341 = 0.004950628615915775 + 0.01 * 6.831021308898926
Epoch 800, val loss: 1.1062933206558228
Epoch 810, training loss: 0.07313264906406403 = 0.004789839498698711 + 0.01 * 6.8342814445495605
Epoch 810, val loss: 1.1105945110321045
Epoch 820, training loss: 0.07285378873348236 = 0.0046379705891013145 + 0.01 * 6.821581840515137
Epoch 820, val loss: 1.1148178577423096
Epoch 830, training loss: 0.07290296256542206 = 0.004494257736951113 + 0.01 * 6.8408708572387695
Epoch 830, val loss: 1.1189546585083008
Epoch 840, training loss: 0.07251118123531342 = 0.004358341917395592 + 0.01 * 6.81528377532959
Epoch 840, val loss: 1.122977614402771
Epoch 850, training loss: 0.07236883044242859 = 0.004229631740599871 + 0.01 * 6.813920021057129
Epoch 850, val loss: 1.1269168853759766
Epoch 860, training loss: 0.07224548608064651 = 0.0041074384935200214 + 0.01 * 6.813805103302002
Epoch 860, val loss: 1.1307517290115356
Epoch 870, training loss: 0.07196636497974396 = 0.003991591744124889 + 0.01 * 6.797477722167969
Epoch 870, val loss: 1.134514331817627
Epoch 880, training loss: 0.0720253437757492 = 0.0038813690189272165 + 0.01 * 6.81439733505249
Epoch 880, val loss: 1.1382249593734741
Epoch 890, training loss: 0.07178094983100891 = 0.0037766185123473406 + 0.01 * 6.80043363571167
Epoch 890, val loss: 1.1418269872665405
Epoch 900, training loss: 0.07154254615306854 = 0.0036769607104361057 + 0.01 * 6.786559104919434
Epoch 900, val loss: 1.1453644037246704
Epoch 910, training loss: 0.07171288877725601 = 0.003581891767680645 + 0.01 * 6.8130998611450195
Epoch 910, val loss: 1.1488155126571655
Epoch 920, training loss: 0.07153372466564178 = 0.0034913658164441586 + 0.01 * 6.804235935211182
Epoch 920, val loss: 1.1522103548049927
Epoch 930, training loss: 0.07148165255784988 = 0.0034049481619149446 + 0.01 * 6.807670593261719
Epoch 930, val loss: 1.1555448770523071
Epoch 940, training loss: 0.07114367187023163 = 0.0033225787337869406 + 0.01 * 6.78210973739624
Epoch 940, val loss: 1.1588236093521118
Epoch 950, training loss: 0.07097098976373672 = 0.0032436468172818422 + 0.01 * 6.772734642028809
Epoch 950, val loss: 1.162025809288025
Epoch 960, training loss: 0.07119424641132355 = 0.0031681463588029146 + 0.01 * 6.802610397338867
Epoch 960, val loss: 1.1651723384857178
Epoch 970, training loss: 0.07105390727519989 = 0.003095973515883088 + 0.01 * 6.795793533325195
Epoch 970, val loss: 1.1682575941085815
Epoch 980, training loss: 0.07078027725219727 = 0.0030270095448940992 + 0.01 * 6.775327205657959
Epoch 980, val loss: 1.171279788017273
Epoch 990, training loss: 0.0707319974899292 = 0.002960807178169489 + 0.01 * 6.777119159698486
Epoch 990, val loss: 1.1742368936538696
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8355297838692674
The final CL Acc:0.80494, 0.01429, The final GNN Acc:0.83904, 0.00358
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10538])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.027998924255371 = 1.9420303106307983 + 0.01 * 8.59685230255127
Epoch 0, val loss: 1.938792109489441
Epoch 10, training loss: 2.0187337398529053 = 1.9327654838562012 + 0.01 * 8.596822738647461
Epoch 10, val loss: 1.9296841621398926
Epoch 20, training loss: 2.0078628063201904 = 1.921895980834961 + 0.01 * 8.59669303894043
Epoch 20, val loss: 1.9185090065002441
Epoch 30, training loss: 1.9931901693344116 = 1.907226800918579 + 0.01 * 8.59634017944336
Epoch 30, val loss: 1.9029351472854614
Epoch 40, training loss: 1.9720563888549805 = 1.8861069679260254 + 0.01 * 8.594943046569824
Epoch 40, val loss: 1.8802217245101929
Epoch 50, training loss: 1.9417788982391357 = 1.8559273481369019 + 0.01 * 8.585150718688965
Epoch 50, val loss: 1.8486716747283936
Epoch 60, training loss: 1.903275728225708 = 1.8180066347122192 + 0.01 * 8.526906967163086
Epoch 60, val loss: 1.8127100467681885
Epoch 70, training loss: 1.8623065948486328 = 1.780243992805481 + 0.01 * 8.206266403198242
Epoch 70, val loss: 1.7832129001617432
Epoch 80, training loss: 1.8177070617675781 = 1.7384907007217407 + 0.01 * 7.921638488769531
Epoch 80, val loss: 1.7520183324813843
Epoch 90, training loss: 1.758172631263733 = 1.6818311214447021 + 0.01 * 7.6341552734375
Epoch 90, val loss: 1.7040653228759766
Epoch 100, training loss: 1.6799424886703491 = 1.6046801805496216 + 0.01 * 7.5262322425842285
Epoch 100, val loss: 1.6357446908950806
Epoch 110, training loss: 1.5811350345611572 = 1.5064988136291504 + 0.01 * 7.463618278503418
Epoch 110, val loss: 1.5523066520690918
Epoch 120, training loss: 1.4665194749832153 = 1.3923918008804321 + 0.01 * 7.412765026092529
Epoch 120, val loss: 1.4575380086898804
Epoch 130, training loss: 1.3441977500915527 = 1.270734429359436 + 0.01 * 7.346334934234619
Epoch 130, val loss: 1.3581771850585938
Epoch 140, training loss: 1.2211557626724243 = 1.1483831405639648 + 0.01 * 7.2772626876831055
Epoch 140, val loss: 1.2603458166122437
Epoch 150, training loss: 1.103894829750061 = 1.0315301418304443 + 0.01 * 7.2364702224731445
Epoch 150, val loss: 1.1687602996826172
Epoch 160, training loss: 0.9973771572113037 = 0.9252529740333557 + 0.01 * 7.2124152183532715
Epoch 160, val loss: 1.0883160829544067
Epoch 170, training loss: 0.9046143889427185 = 0.8327023386955261 + 0.01 * 7.191206932067871
Epoch 170, val loss: 1.0223361253738403
Epoch 180, training loss: 0.8259527087211609 = 0.7541431784629822 + 0.01 * 7.180955410003662
Epoch 180, val loss: 0.9706182479858398
Epoch 190, training loss: 0.7590797543525696 = 0.6873534917831421 + 0.01 * 7.172624111175537
Epoch 190, val loss: 0.9312500953674316
Epoch 200, training loss: 0.7008244395256042 = 0.6291561126708984 + 0.01 * 7.166834831237793
Epoch 200, val loss: 0.9011842608451843
Epoch 210, training loss: 0.6483708024024963 = 0.5767480134963989 + 0.01 * 7.1622772216796875
Epoch 210, val loss: 0.8778263330459595
Epoch 220, training loss: 0.599862813949585 = 0.5282827019691467 + 0.01 * 7.158013343811035
Epoch 220, val loss: 0.8591301441192627
Epoch 230, training loss: 0.5543494820594788 = 0.48280781507492065 + 0.01 * 7.1541666984558105
Epoch 230, val loss: 0.8439210653305054
Epoch 240, training loss: 0.5114702582359314 = 0.4399723708629608 + 0.01 * 7.149791240692139
Epoch 240, val loss: 0.831983745098114
Epoch 250, training loss: 0.47125425934791565 = 0.3997988998889923 + 0.01 * 7.145536422729492
Epoch 250, val loss: 0.8235402703285217
Epoch 260, training loss: 0.4337451457977295 = 0.36234185099601746 + 0.01 * 7.1403279304504395
Epoch 260, val loss: 0.8185837268829346
Epoch 270, training loss: 0.39882728457450867 = 0.3274820148944855 + 0.01 * 7.134527683258057
Epoch 270, val loss: 0.8169829249382019
Epoch 280, training loss: 0.366155207157135 = 0.2948678433895111 + 0.01 * 7.128735065460205
Epoch 280, val loss: 0.8181387186050415
Epoch 290, training loss: 0.3353263735771179 = 0.2640722095966339 + 0.01 * 7.125415325164795
Epoch 290, val loss: 0.8214365243911743
Epoch 300, training loss: 0.30604153871536255 = 0.23490455746650696 + 0.01 * 7.1136980056762695
Epoch 300, val loss: 0.8266530632972717
Epoch 310, training loss: 0.27858734130859375 = 0.20752424001693726 + 0.01 * 7.106311798095703
Epoch 310, val loss: 0.8333800435066223
Epoch 320, training loss: 0.253364622592926 = 0.18235361576080322 + 0.01 * 7.101102352142334
Epoch 320, val loss: 0.8415412306785583
Epoch 330, training loss: 0.23073521256446838 = 0.15974903106689453 + 0.01 * 7.098618984222412
Epoch 330, val loss: 0.8511047959327698
Epoch 340, training loss: 0.21074265241622925 = 0.13987556099891663 + 0.01 * 7.086709022521973
Epoch 340, val loss: 0.8620665669441223
Epoch 350, training loss: 0.1933567076921463 = 0.12263084203004837 + 0.01 * 7.072587013244629
Epoch 350, val loss: 0.8742455840110779
Epoch 360, training loss: 0.17840975522994995 = 0.10778328776359558 + 0.01 * 7.062647819519043
Epoch 360, val loss: 0.887501060962677
Epoch 370, training loss: 0.1655515730381012 = 0.09504075348377228 + 0.01 * 7.051082611083984
Epoch 370, val loss: 0.9015900492668152
Epoch 380, training loss: 0.1545490324497223 = 0.08409127593040466 + 0.01 * 7.045775890350342
Epoch 380, val loss: 0.9162370562553406
Epoch 390, training loss: 0.14503604173660278 = 0.07465752214193344 + 0.01 * 7.0378522872924805
Epoch 390, val loss: 0.9312283396720886
Epoch 400, training loss: 0.13682159781455994 = 0.06650413572788239 + 0.01 * 7.031747341156006
Epoch 400, val loss: 0.9463603496551514
Epoch 410, training loss: 0.1296709030866623 = 0.059432558715343475 + 0.01 * 7.023834705352783
Epoch 410, val loss: 0.9615060091018677
Epoch 420, training loss: 0.12350219488143921 = 0.05327855423092842 + 0.01 * 7.022364139556885
Epoch 420, val loss: 0.97651207447052
Epoch 430, training loss: 0.1181163489818573 = 0.047907937318086624 + 0.01 * 7.020841598510742
Epoch 430, val loss: 0.9913204908370972
Epoch 440, training loss: 0.11341121792793274 = 0.04320446401834488 + 0.01 * 7.020675182342529
Epoch 440, val loss: 1.0059467554092407
Epoch 450, training loss: 0.10923711955547333 = 0.03907397389411926 + 0.01 * 7.016314506530762
Epoch 450, val loss: 1.0202265977859497
Epoch 460, training loss: 0.10552693158388138 = 0.035440221428871155 + 0.01 * 7.008671283721924
Epoch 460, val loss: 1.0341542959213257
Epoch 470, training loss: 0.10219709575176239 = 0.03223284333944321 + 0.01 * 6.996425151824951
Epoch 470, val loss: 1.047646403312683
Epoch 480, training loss: 0.09957651793956757 = 0.02939561940729618 + 0.01 * 7.01809024810791
Epoch 480, val loss: 1.0607157945632935
Epoch 490, training loss: 0.09675602614879608 = 0.026885751634836197 + 0.01 * 6.987027645111084
Epoch 490, val loss: 1.0733344554901123
Epoch 500, training loss: 0.09456086158752441 = 0.02465755119919777 + 0.01 * 6.990330696105957
Epoch 500, val loss: 1.085519552230835
Epoch 510, training loss: 0.09243875741958618 = 0.022676022723317146 + 0.01 * 6.976274013519287
Epoch 510, val loss: 1.097322702407837
Epoch 520, training loss: 0.09070686995983124 = 0.020911088213324547 + 0.01 * 6.979578495025635
Epoch 520, val loss: 1.108674168586731
Epoch 530, training loss: 0.08911123871803284 = 0.019337736070156097 + 0.01 * 6.97735071182251
Epoch 530, val loss: 1.1196424961090088
Epoch 540, training loss: 0.08767369389533997 = 0.017929142341017723 + 0.01 * 6.974454879760742
Epoch 540, val loss: 1.1302683353424072
Epoch 550, training loss: 0.08630963414907455 = 0.016664966940879822 + 0.01 * 6.964466571807861
Epoch 550, val loss: 1.1404337882995605
Epoch 560, training loss: 0.08504751324653625 = 0.015526901930570602 + 0.01 * 6.952061653137207
Epoch 560, val loss: 1.1503024101257324
Epoch 570, training loss: 0.08407025039196014 = 0.014500905759632587 + 0.01 * 6.956934452056885
Epoch 570, val loss: 1.1597565412521362
Epoch 580, training loss: 0.08303715288639069 = 0.013574955053627491 + 0.01 * 6.946219444274902
Epoch 580, val loss: 1.168927788734436
Epoch 590, training loss: 0.0823541134595871 = 0.012735608033835888 + 0.01 * 6.961850643157959
Epoch 590, val loss: 1.1777409315109253
Epoch 600, training loss: 0.08150499314069748 = 0.011974153108894825 + 0.01 * 6.953083515167236
Epoch 600, val loss: 1.1862788200378418
Epoch 610, training loss: 0.08057760447263718 = 0.011281514540314674 + 0.01 * 6.929609298706055
Epoch 610, val loss: 1.1945059299468994
Epoch 620, training loss: 0.07998012006282806 = 0.010649424977600574 + 0.01 * 6.933069229125977
Epoch 620, val loss: 1.2023403644561768
Epoch 630, training loss: 0.07934687286615372 = 0.010071118362247944 + 0.01 * 6.927575588226318
Epoch 630, val loss: 1.2100269794464111
Epoch 640, training loss: 0.07864390313625336 = 0.009540610015392303 + 0.01 * 6.910329818725586
Epoch 640, val loss: 1.2174862623214722
Epoch 650, training loss: 0.07842528820037842 = 0.009052466601133347 + 0.01 * 6.937282562255859
Epoch 650, val loss: 1.2245407104492188
Epoch 660, training loss: 0.07778622210025787 = 0.008603565394878387 + 0.01 * 6.9182658195495605
Epoch 660, val loss: 1.231499433517456
Epoch 670, training loss: 0.07716502994298935 = 0.008188984356820583 + 0.01 * 6.897604942321777
Epoch 670, val loss: 1.2382023334503174
Epoch 680, training loss: 0.07682240009307861 = 0.007804787717759609 + 0.01 * 6.901761054992676
Epoch 680, val loss: 1.2445785999298096
Epoch 690, training loss: 0.07646814733743668 = 0.0074485065415501595 + 0.01 * 6.90196418762207
Epoch 690, val loss: 1.2508906126022339
Epoch 700, training loss: 0.07591032236814499 = 0.00711705069988966 + 0.01 * 6.879327297210693
Epoch 700, val loss: 1.2569451332092285
Epoch 710, training loss: 0.07571940124034882 = 0.006808008532971144 + 0.01 * 6.891139984130859
Epoch 710, val loss: 1.262644648551941
Epoch 720, training loss: 0.07544602453708649 = 0.006520217750221491 + 0.01 * 6.892580986022949
Epoch 720, val loss: 1.2684558629989624
Epoch 730, training loss: 0.07497698813676834 = 0.006251019425690174 + 0.01 * 6.872596740722656
Epoch 730, val loss: 1.2738429307937622
Epoch 740, training loss: 0.07493837177753448 = 0.0059988247230648994 + 0.01 * 6.893954277038574
Epoch 740, val loss: 1.2790940999984741
Epoch 750, training loss: 0.07443071156740189 = 0.005762961693108082 + 0.01 * 6.866775035858154
Epoch 750, val loss: 1.2842614650726318
Epoch 760, training loss: 0.07429306954145432 = 0.005541449412703514 + 0.01 * 6.875161647796631
Epoch 760, val loss: 1.2892078161239624
Epoch 770, training loss: 0.07382173836231232 = 0.005334006156772375 + 0.01 * 6.84877347946167
Epoch 770, val loss: 1.2940213680267334
Epoch 780, training loss: 0.07363459467887878 = 0.00513896718621254 + 0.01 * 6.849563121795654
Epoch 780, val loss: 1.2986847162246704
Epoch 790, training loss: 0.07335849851369858 = 0.004956099670380354 + 0.01 * 6.840240478515625
Epoch 790, val loss: 1.303091049194336
Epoch 800, training loss: 0.0731913298368454 = 0.004784086253494024 + 0.01 * 6.840724945068359
Epoch 800, val loss: 1.3074946403503418
Epoch 810, training loss: 0.07331211119890213 = 0.004621851723641157 + 0.01 * 6.8690266609191895
Epoch 810, val loss: 1.3116931915283203
Epoch 820, training loss: 0.07290270924568176 = 0.004468789789825678 + 0.01 * 6.8433918952941895
Epoch 820, val loss: 1.3158490657806396
Epoch 830, training loss: 0.07272829115390778 = 0.004324266687035561 + 0.01 * 6.840403079986572
Epoch 830, val loss: 1.3198115825653076
Epoch 840, training loss: 0.07247046381235123 = 0.004187949933111668 + 0.01 * 6.828251838684082
Epoch 840, val loss: 1.323550820350647
Epoch 850, training loss: 0.07233819365501404 = 0.004059284925460815 + 0.01 * 6.8278913497924805
Epoch 850, val loss: 1.327343225479126
Epoch 860, training loss: 0.07218284159898758 = 0.003937694244086742 + 0.01 * 6.824514865875244
Epoch 860, val loss: 1.3309539556503296
Epoch 870, training loss: 0.07203714549541473 = 0.003822214202955365 + 0.01 * 6.821493148803711
Epoch 870, val loss: 1.3343980312347412
Epoch 880, training loss: 0.07189492881298065 = 0.003713182872161269 + 0.01 * 6.818174362182617
Epoch 880, val loss: 1.337816834449768
Epoch 890, training loss: 0.07196071743965149 = 0.0036096391268074512 + 0.01 * 6.835107803344727
Epoch 890, val loss: 1.341110348701477
Epoch 900, training loss: 0.07167746126651764 = 0.003511521266773343 + 0.01 * 6.816594123840332
Epoch 900, val loss: 1.3443323373794556
Epoch 910, training loss: 0.0715094804763794 = 0.003417873289436102 + 0.01 * 6.809161186218262
Epoch 910, val loss: 1.3474335670471191
Epoch 920, training loss: 0.0715615525841713 = 0.0033287345431745052 + 0.01 * 6.823282241821289
Epoch 920, val loss: 1.3504573106765747
Epoch 930, training loss: 0.07132125645875931 = 0.0032438007183372974 + 0.01 * 6.807745456695557
Epoch 930, val loss: 1.3532475233078003
Epoch 940, training loss: 0.07133983075618744 = 0.0031629365403205156 + 0.01 * 6.817689895629883
Epoch 940, val loss: 1.3561416864395142
Epoch 950, training loss: 0.07111598551273346 = 0.0030857061501592398 + 0.01 * 6.803028583526611
Epoch 950, val loss: 1.3588851690292358
Epoch 960, training loss: 0.07085292041301727 = 0.003011836903169751 + 0.01 * 6.784108638763428
Epoch 960, val loss: 1.3615155220031738
Epoch 970, training loss: 0.07101725041866302 = 0.002941259415820241 + 0.01 * 6.8075995445251465
Epoch 970, val loss: 1.3641475439071655
Epoch 980, training loss: 0.0707392618060112 = 0.002873619319871068 + 0.01 * 6.786563873291016
Epoch 980, val loss: 1.3667103052139282
Epoch 990, training loss: 0.07064927369356155 = 0.0028090134728699923 + 0.01 * 6.784026622772217
Epoch 990, val loss: 1.3690876960754395
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 2.016035556793213 = 1.9300674200057983 + 0.01 * 8.59681510925293
Epoch 0, val loss: 1.919877290725708
Epoch 10, training loss: 2.0066707134246826 = 1.9207030534744263 + 0.01 * 8.596761703491211
Epoch 10, val loss: 1.9111971855163574
Epoch 20, training loss: 1.994964838027954 = 1.9089992046356201 + 0.01 * 8.596561431884766
Epoch 20, val loss: 1.899983286857605
Epoch 30, training loss: 1.9784553050994873 = 1.89249587059021 + 0.01 * 8.595942497253418
Epoch 30, val loss: 1.883875846862793
Epoch 40, training loss: 1.9542683362960815 = 1.8683407306671143 + 0.01 * 8.592757225036621
Epoch 40, val loss: 1.8606750965118408
Epoch 50, training loss: 1.9214063882827759 = 1.835694432258606 + 0.01 * 8.571196556091309
Epoch 50, val loss: 1.831376314163208
Epoch 60, training loss: 1.8858845233917236 = 1.8011314868927002 + 0.01 * 8.475302696228027
Epoch 60, val loss: 1.8042229413986206
Epoch 70, training loss: 1.851338505744934 = 1.7691315412521362 + 0.01 * 8.220698356628418
Epoch 70, val loss: 1.7794920206069946
Epoch 80, training loss: 1.8063825368881226 = 1.7257437705993652 + 0.01 * 8.063881874084473
Epoch 80, val loss: 1.7405376434326172
Epoch 90, training loss: 1.743654489517212 = 1.6647863388061523 + 0.01 * 7.886815071105957
Epoch 90, val loss: 1.6860694885253906
Epoch 100, training loss: 1.6616716384887695 = 1.5847084522247314 + 0.01 * 7.696323394775391
Epoch 100, val loss: 1.619041085243225
Epoch 110, training loss: 1.569034457206726 = 1.4938400983810425 + 0.01 * 7.519441604614258
Epoch 110, val loss: 1.543055534362793
Epoch 120, training loss: 1.4749321937561035 = 1.4004313945770264 + 0.01 * 7.45007848739624
Epoch 120, val loss: 1.4668962955474854
Epoch 130, training loss: 1.3809077739715576 = 1.306904911994934 + 0.01 * 7.4002838134765625
Epoch 130, val loss: 1.3942468166351318
Epoch 140, training loss: 1.2875847816467285 = 1.2138879299163818 + 0.01 * 7.369683742523193
Epoch 140, val loss: 1.3245794773101807
Epoch 150, training loss: 1.1975312232971191 = 1.1241549253463745 + 0.01 * 7.337629318237305
Epoch 150, val loss: 1.259748935699463
Epoch 160, training loss: 1.1140023469924927 = 1.0408539772033691 + 0.01 * 7.314840316772461
Epoch 160, val loss: 1.2015382051467896
Epoch 170, training loss: 1.037907600402832 = 0.9648520946502686 + 0.01 * 7.305556297302246
Epoch 170, val loss: 1.1498740911483765
Epoch 180, training loss: 0.9677219390869141 = 0.8947176933288574 + 0.01 * 7.300424575805664
Epoch 180, val loss: 1.1034995317459106
Epoch 190, training loss: 0.9011663794517517 = 0.8282340168952942 + 0.01 * 7.293234825134277
Epoch 190, val loss: 1.060484528541565
Epoch 200, training loss: 0.8367067575454712 = 0.7638741731643677 + 0.01 * 7.283260822296143
Epoch 200, val loss: 1.0198067426681519
Epoch 210, training loss: 0.7737871408462524 = 0.7010965347290039 + 0.01 * 7.269063949584961
Epoch 210, val loss: 0.9811170697212219
Epoch 220, training loss: 0.7130634784698486 = 0.640575110912323 + 0.01 * 7.248839378356934
Epoch 220, val loss: 0.9453805685043335
Epoch 230, training loss: 0.6562341451644897 = 0.5839788317680359 + 0.01 * 7.225528240203857
Epoch 230, val loss: 0.9145427346229553
Epoch 240, training loss: 0.6045957207679749 = 0.5325704216957092 + 0.01 * 7.202531337738037
Epoch 240, val loss: 0.8897598385810852
Epoch 250, training loss: 0.5581392049789429 = 0.48624977469444275 + 0.01 * 7.188945293426514
Epoch 250, val loss: 0.8710451126098633
Epoch 260, training loss: 0.515751838684082 = 0.4439259171485901 + 0.01 * 7.182590961456299
Epoch 260, val loss: 0.8569724559783936
Epoch 270, training loss: 0.4763121008872986 = 0.4045355021953583 + 0.01 * 7.177659034729004
Epoch 270, val loss: 0.8465023040771484
Epoch 280, training loss: 0.43938568234443665 = 0.36764031648635864 + 0.01 * 7.174536228179932
Epoch 280, val loss: 0.8392689824104309
Epoch 290, training loss: 0.40503329038619995 = 0.33331751823425293 + 0.01 * 7.171576023101807
Epoch 290, val loss: 0.8352536559104919
Epoch 300, training loss: 0.373322069644928 = 0.3016310930252075 + 0.01 * 7.169098854064941
Epoch 300, val loss: 0.834014892578125
Epoch 310, training loss: 0.3439176082611084 = 0.2722444534301758 + 0.01 * 7.167314529418945
Epoch 310, val loss: 0.8348492383956909
Epoch 320, training loss: 0.3161029815673828 = 0.24445460736751556 + 0.01 * 7.1648383140563965
Epoch 320, val loss: 0.8370905518531799
Epoch 330, training loss: 0.2892683744430542 = 0.21762752532958984 + 0.01 * 7.164083480834961
Epoch 330, val loss: 0.8402191996574402
Epoch 340, training loss: 0.2632783353328705 = 0.19166028499603271 + 0.01 * 7.161804676055908
Epoch 340, val loss: 0.8437726497650146
Epoch 350, training loss: 0.23874390125274658 = 0.1671386957168579 + 0.01 * 7.160519599914551
Epoch 350, val loss: 0.8479785919189453
Epoch 360, training loss: 0.21647128462791443 = 0.14487773180007935 + 0.01 * 7.159355640411377
Epoch 360, val loss: 0.8532393574714661
Epoch 370, training loss: 0.19685883820056915 = 0.1252838522195816 + 0.01 * 7.157498836517334
Epoch 370, val loss: 0.8594939708709717
Epoch 380, training loss: 0.180012047290802 = 0.1083591878414154 + 0.01 * 7.165285587310791
Epoch 380, val loss: 0.8669959902763367
Epoch 390, training loss: 0.16539880633354187 = 0.09385570883750916 + 0.01 * 7.1543097496032715
Epoch 390, val loss: 0.8758453726768494
Epoch 400, training loss: 0.15291935205459595 = 0.0814024955034256 + 0.01 * 7.1516852378845215
Epoch 400, val loss: 0.8860947489738464
Epoch 410, training loss: 0.1421331912279129 = 0.07064314186573029 + 0.01 * 7.149004936218262
Epoch 410, val loss: 0.8975395560264587
Epoch 420, training loss: 0.13277879357337952 = 0.06129952520132065 + 0.01 * 7.1479268074035645
Epoch 420, val loss: 0.9099530577659607
Epoch 430, training loss: 0.12466003000736237 = 0.053214382380247116 + 0.01 * 7.144565582275391
Epoch 430, val loss: 0.9227920174598694
Epoch 440, training loss: 0.11769576370716095 = 0.04629109427332878 + 0.01 * 7.140466690063477
Epoch 440, val loss: 0.9359552264213562
Epoch 450, training loss: 0.11185520142316818 = 0.040427736937999725 + 0.01 * 7.142746925354004
Epoch 450, val loss: 0.9490711092948914
Epoch 460, training loss: 0.10686646401882172 = 0.03552614524960518 + 0.01 * 7.134032726287842
Epoch 460, val loss: 0.9619033336639404
Epoch 470, training loss: 0.1027449369430542 = 0.03143424540758133 + 0.01 * 7.131069183349609
Epoch 470, val loss: 0.9745252728462219
Epoch 480, training loss: 0.09926694631576538 = 0.027998993173241615 + 0.01 * 7.126795768737793
Epoch 480, val loss: 0.9869235754013062
Epoch 490, training loss: 0.09630528837442398 = 0.025097178295254707 + 0.01 * 7.1208109855651855
Epoch 490, val loss: 0.9990488886833191
Epoch 500, training loss: 0.09378257393836975 = 0.022627513855695724 + 0.01 * 7.115506649017334
Epoch 500, val loss: 1.0108304023742676
Epoch 510, training loss: 0.0915944054722786 = 0.020511416718363762 + 0.01 * 7.1082987785339355
Epoch 510, val loss: 1.0222059488296509
Epoch 520, training loss: 0.0896763876080513 = 0.01868603564798832 + 0.01 * 7.099035739898682
Epoch 520, val loss: 1.0331976413726807
Epoch 530, training loss: 0.08805691450834274 = 0.017102835699915886 + 0.01 * 7.095407485961914
Epoch 530, val loss: 1.0438594818115234
Epoch 540, training loss: 0.08658921718597412 = 0.015719275921583176 + 0.01 * 7.086994647979736
Epoch 540, val loss: 1.054167628288269
Epoch 550, training loss: 0.0852808877825737 = 0.014504184946417809 + 0.01 * 7.077670574188232
Epoch 550, val loss: 1.064081072807312
Epoch 560, training loss: 0.08439207822084427 = 0.013431315310299397 + 0.01 * 7.096076488494873
Epoch 560, val loss: 1.0736287832260132
Epoch 570, training loss: 0.08313261717557907 = 0.012481629848480225 + 0.01 * 7.065099239349365
Epoch 570, val loss: 1.0829534530639648
Epoch 580, training loss: 0.08224126696586609 = 0.011635784059762955 + 0.01 * 7.060548305511475
Epoch 580, val loss: 1.0917870998382568
Epoch 590, training loss: 0.08142584562301636 = 0.01087992638349533 + 0.01 * 7.054592132568359
Epoch 590, val loss: 1.1004743576049805
Epoch 600, training loss: 0.08065535873174667 = 0.010202326811850071 + 0.01 * 7.0453033447265625
Epoch 600, val loss: 1.1085820198059082
Epoch 610, training loss: 0.0799955502152443 = 0.009591735899448395 + 0.01 * 7.04038143157959
Epoch 610, val loss: 1.1166605949401855
Epoch 620, training loss: 0.0793013721704483 = 0.009040364064276218 + 0.01 * 7.026101112365723
Epoch 620, val loss: 1.1243321895599365
Epoch 630, training loss: 0.07863730192184448 = 0.00854043010622263 + 0.01 * 7.009687423706055
Epoch 630, val loss: 1.131758213043213
Epoch 640, training loss: 0.07803031802177429 = 0.008085944689810276 + 0.01 * 6.9944376945495605
Epoch 640, val loss: 1.1388506889343262
Epoch 650, training loss: 0.07761027663946152 = 0.007671174593269825 + 0.01 * 6.99390983581543
Epoch 650, val loss: 1.1457066535949707
Epoch 660, training loss: 0.07720073312520981 = 0.007291494403034449 + 0.01 * 6.990923881530762
Epoch 660, val loss: 1.1523877382278442
Epoch 670, training loss: 0.07680688053369522 = 0.006942854728549719 + 0.01 * 6.986402988433838
Epoch 670, val loss: 1.158812403678894
Epoch 680, training loss: 0.0762893334031105 = 0.006622204557061195 + 0.01 * 6.966712951660156
Epoch 680, val loss: 1.16500985622406
Epoch 690, training loss: 0.07588783651590347 = 0.006326232571154833 + 0.01 * 6.956160068511963
Epoch 690, val loss: 1.1709843873977661
Epoch 700, training loss: 0.07570638507604599 = 0.006052613724023104 + 0.01 * 6.965377330780029
Epoch 700, val loss: 1.1766232252120972
Epoch 710, training loss: 0.07537201046943665 = 0.005798973608762026 + 0.01 * 6.957303524017334
Epoch 710, val loss: 1.1823087930679321
Epoch 720, training loss: 0.07496567815542221 = 0.00556330569088459 + 0.01 * 6.940237522125244
Epoch 720, val loss: 1.1876806020736694
Epoch 730, training loss: 0.07466338574886322 = 0.005344055127352476 + 0.01 * 6.931933403015137
Epoch 730, val loss: 1.1929112672805786
Epoch 740, training loss: 0.07458724826574326 = 0.005139385815709829 + 0.01 * 6.944786071777344
Epoch 740, val loss: 1.1979657411575317
Epoch 750, training loss: 0.0743192806839943 = 0.004948360845446587 + 0.01 * 6.937092304229736
Epoch 750, val loss: 1.2029234170913696
Epoch 760, training loss: 0.07406769692897797 = 0.00476973969489336 + 0.01 * 6.929795742034912
Epoch 760, val loss: 1.2076631784439087
Epoch 770, training loss: 0.07368618249893188 = 0.004602727480232716 + 0.01 * 6.908345699310303
Epoch 770, val loss: 1.2122286558151245
Epoch 780, training loss: 0.07362440973520279 = 0.004445572383701801 + 0.01 * 6.917883396148682
Epoch 780, val loss: 1.216768741607666
Epoch 790, training loss: 0.07336495816707611 = 0.00429774122312665 + 0.01 * 6.906722068786621
Epoch 790, val loss: 1.2210392951965332
Epoch 800, training loss: 0.07348909229040146 = 0.004158667754381895 + 0.01 * 6.933042526245117
Epoch 800, val loss: 1.2252087593078613
Epoch 810, training loss: 0.07290826737880707 = 0.0040276264771819115 + 0.01 * 6.888063907623291
Epoch 810, val loss: 1.229406714439392
Epoch 820, training loss: 0.07308406382799149 = 0.0039040404371917248 + 0.01 * 6.918003082275391
Epoch 820, val loss: 1.2332274913787842
Epoch 830, training loss: 0.07253018021583557 = 0.0037872646935284138 + 0.01 * 6.87429141998291
Epoch 830, val loss: 1.237168788909912
Epoch 840, training loss: 0.0727199912071228 = 0.0036770286969840527 + 0.01 * 6.904296875
Epoch 840, val loss: 1.240759015083313
Epoch 850, training loss: 0.07221091538667679 = 0.0035727377980947495 + 0.01 * 6.8638176918029785
Epoch 850, val loss: 1.2445653676986694
Epoch 860, training loss: 0.07220330834388733 = 0.003473625285550952 + 0.01 * 6.8729681968688965
Epoch 860, val loss: 1.2479488849639893
Epoch 870, training loss: 0.07201912254095078 = 0.0033796278294175863 + 0.01 * 6.863949775695801
Epoch 870, val loss: 1.2514663934707642
Epoch 880, training loss: 0.07180540263652802 = 0.003290151944383979 + 0.01 * 6.851524829864502
Epoch 880, val loss: 1.2547612190246582
Epoch 890, training loss: 0.07197094708681107 = 0.0032050947193056345 + 0.01 * 6.876585006713867
Epoch 890, val loss: 1.2579714059829712
Epoch 900, training loss: 0.07171561568975449 = 0.0031242563854902983 + 0.01 * 6.859135627746582
Epoch 900, val loss: 1.2611967325210571
Epoch 910, training loss: 0.07159887254238129 = 0.0030473892111331224 + 0.01 * 6.8551483154296875
Epoch 910, val loss: 1.2641453742980957
Epoch 920, training loss: 0.07137033343315125 = 0.0029740110039711 + 0.01 * 6.839632511138916
Epoch 920, val loss: 1.2671804428100586
Epoch 930, training loss: 0.07130927592515945 = 0.002904009772464633 + 0.01 * 6.840526103973389
Epoch 930, val loss: 1.2700694799423218
Epoch 940, training loss: 0.0710805207490921 = 0.0028371396474540234 + 0.01 * 6.824338912963867
Epoch 940, val loss: 1.2728617191314697
Epoch 950, training loss: 0.07119627296924591 = 0.0027730874717235565 + 0.01 * 6.842319011688232
Epoch 950, val loss: 1.2755635976791382
Epoch 960, training loss: 0.07093434780836105 = 0.002711853478103876 + 0.01 * 6.822249412536621
Epoch 960, val loss: 1.2783204317092896
Epoch 970, training loss: 0.070924311876297 = 0.002653262810781598 + 0.01 * 6.8271050453186035
Epoch 970, val loss: 1.2808421850204468
Epoch 980, training loss: 0.07076478749513626 = 0.002597266575321555 + 0.01 * 6.8167524337768555
Epoch 980, val loss: 1.2834702730178833
Epoch 990, training loss: 0.0706404522061348 = 0.0025434643030166626 + 0.01 * 6.809699058532715
Epoch 990, val loss: 1.285807728767395
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 2.042691469192505 = 1.9567232131958008 + 0.01 * 8.596822738647461
Epoch 0, val loss: 1.9713715314865112
Epoch 10, training loss: 2.032416820526123 = 1.9464492797851562 + 0.01 * 8.596748352050781
Epoch 10, val loss: 1.9609415531158447
Epoch 20, training loss: 2.019742727279663 = 1.933777928352356 + 0.01 * 8.596489906311035
Epoch 20, val loss: 1.9479773044586182
Epoch 30, training loss: 2.0020923614501953 = 1.916136384010315 + 0.01 * 8.595602989196777
Epoch 30, val loss: 1.9299829006195068
Epoch 40, training loss: 1.976330041885376 = 1.890426754951477 + 0.01 * 8.590328216552734
Epoch 40, val loss: 1.904200792312622
Epoch 50, training loss: 1.9404590129852295 = 1.8548994064331055 + 0.01 * 8.555960655212402
Epoch 50, val loss: 1.8697266578674316
Epoch 60, training loss: 1.8987092971801758 = 1.8148753643035889 + 0.01 * 8.383393287658691
Epoch 60, val loss: 1.8330739736557007
Epoch 70, training loss: 1.8629480600357056 = 1.780961513519287 + 0.01 * 8.19865894317627
Epoch 70, val loss: 1.8015239238739014
Epoch 80, training loss: 1.8227763175964355 = 1.7426536083221436 + 0.01 * 8.012273788452148
Epoch 80, val loss: 1.7632126808166504
Epoch 90, training loss: 1.7671438455581665 = 1.6885714530944824 + 0.01 * 7.857237815856934
Epoch 90, val loss: 1.7157385349273682
Epoch 100, training loss: 1.6913808584213257 = 1.6143165826797485 + 0.01 * 7.706430435180664
Epoch 100, val loss: 1.6545400619506836
Epoch 110, training loss: 1.5979608297348022 = 1.5224660634994507 + 0.01 * 7.54947566986084
Epoch 110, val loss: 1.577283263206482
Epoch 120, training loss: 1.4974404573440552 = 1.422470211982727 + 0.01 * 7.497029781341553
Epoch 120, val loss: 1.4929918050765991
Epoch 130, training loss: 1.3961327075958252 = 1.3217730522155762 + 0.01 * 7.435970783233643
Epoch 130, val loss: 1.410724401473999
Epoch 140, training loss: 1.2968497276306152 = 1.2230967283248901 + 0.01 * 7.3752970695495605
Epoch 140, val loss: 1.332543969154358
Epoch 150, training loss: 1.1994684934616089 = 1.1264292001724243 + 0.01 * 7.303928375244141
Epoch 150, val loss: 1.2581514120101929
Epoch 160, training loss: 1.1037721633911133 = 1.031455159187317 + 0.01 * 7.23170280456543
Epoch 160, val loss: 1.1860857009887695
Epoch 170, training loss: 1.011770248413086 = 0.9399105310440063 + 0.01 * 7.185973167419434
Epoch 170, val loss: 1.1170654296875
Epoch 180, training loss: 0.9273191094398499 = 0.8556941747665405 + 0.01 * 7.16249418258667
Epoch 180, val loss: 1.0541048049926758
Epoch 190, training loss: 0.8540735244750977 = 0.7825316190719604 + 0.01 * 7.154189109802246
Epoch 190, val loss: 1.0008766651153564
Epoch 200, training loss: 0.7930662035942078 = 0.7215876579284668 + 0.01 * 7.147853374481201
Epoch 200, val loss: 0.9593833684921265
Epoch 210, training loss: 0.7422468662261963 = 0.670806884765625 + 0.01 * 7.144001483917236
Epoch 210, val loss: 0.9283953309059143
Epoch 220, training loss: 0.6978162527084351 = 0.6263993978500366 + 0.01 * 7.141684532165527
Epoch 220, val loss: 0.9046128392219543
Epoch 230, training loss: 0.6558500528335571 = 0.5844399333000183 + 0.01 * 7.141012191772461
Epoch 230, val loss: 0.8844324350357056
Epoch 240, training loss: 0.6132394075393677 = 0.541851818561554 + 0.01 * 7.138759613037109
Epoch 240, val loss: 0.8650280237197876
Epoch 250, training loss: 0.5683948993682861 = 0.49701952934265137 + 0.01 * 7.137539386749268
Epoch 250, val loss: 0.8461241722106934
Epoch 260, training loss: 0.5213915705680847 = 0.4500252306461334 + 0.01 * 7.136635780334473
Epoch 260, val loss: 0.8286694884300232
Epoch 270, training loss: 0.47391700744628906 = 0.4025494456291199 + 0.01 * 7.136756896972656
Epoch 270, val loss: 0.8146151900291443
Epoch 280, training loss: 0.42812108993530273 = 0.35676443576812744 + 0.01 * 7.135663986206055
Epoch 280, val loss: 0.8058052062988281
Epoch 290, training loss: 0.3857259750366211 = 0.3143700659275055 + 0.01 * 7.135591983795166
Epoch 290, val loss: 0.8027662634849548
Epoch 300, training loss: 0.347465842962265 = 0.2761177122592926 + 0.01 * 7.13481330871582
Epoch 300, val loss: 0.8052602410316467
Epoch 310, training loss: 0.3135371804237366 = 0.24216769635677338 + 0.01 * 7.136948585510254
Epoch 310, val loss: 0.8125011920928955
Epoch 320, training loss: 0.28372612595558167 = 0.21238958835601807 + 0.01 * 7.133653163909912
Epoch 320, val loss: 0.8236106634140015
Epoch 330, training loss: 0.25778284668922424 = 0.18645986914634705 + 0.01 * 7.132297515869141
Epoch 330, val loss: 0.8374817371368408
Epoch 340, training loss: 0.23519259691238403 = 0.16387531161308289 + 0.01 * 7.1317291259765625
Epoch 340, val loss: 0.8533119559288025
Epoch 350, training loss: 0.2154412865638733 = 0.1441420465707779 + 0.01 * 7.12992525100708
Epoch 350, val loss: 0.8707664012908936
Epoch 360, training loss: 0.19815599918365479 = 0.12688574194908142 + 0.01 * 7.127025604248047
Epoch 360, val loss: 0.8894982933998108
Epoch 370, training loss: 0.18299709260463715 = 0.1117485985159874 + 0.01 * 7.124849796295166
Epoch 370, val loss: 0.9091159105300903
Epoch 380, training loss: 0.16966530680656433 = 0.09845808148384094 + 0.01 * 7.12072229385376
Epoch 380, val loss: 0.929329514503479
Epoch 390, training loss: 0.15807440876960754 = 0.0868338793516159 + 0.01 * 7.124053478240967
Epoch 390, val loss: 0.9499093294143677
Epoch 400, training loss: 0.14780309796333313 = 0.07669123262166977 + 0.01 * 7.111186504364014
Epoch 400, val loss: 0.9705871343612671
Epoch 410, training loss: 0.1389158070087433 = 0.06785760819911957 + 0.01 * 7.1058197021484375
Epoch 410, val loss: 0.9911845922470093
Epoch 420, training loss: 0.13125982880592346 = 0.0601765476167202 + 0.01 * 7.108328342437744
Epoch 420, val loss: 1.0115339756011963
Epoch 430, training loss: 0.12446416169404984 = 0.05350280553102493 + 0.01 * 7.09613561630249
Epoch 430, val loss: 1.031503677368164
Epoch 440, training loss: 0.11857259273529053 = 0.04770294576883316 + 0.01 * 7.086965084075928
Epoch 440, val loss: 1.050958514213562
Epoch 450, training loss: 0.1135275810956955 = 0.04266447573900223 + 0.01 * 7.086310863494873
Epoch 450, val loss: 1.0698741674423218
Epoch 460, training loss: 0.10897070914506912 = 0.038284286856651306 + 0.01 * 7.0686421394348145
Epoch 460, val loss: 1.0881869792938232
Epoch 470, training loss: 0.10513101518154144 = 0.034472111612558365 + 0.01 * 7.065889835357666
Epoch 470, val loss: 1.1058770418167114
Epoch 480, training loss: 0.10176964104175568 = 0.031154178082942963 + 0.01 * 7.061546325683594
Epoch 480, val loss: 1.1229398250579834
Epoch 490, training loss: 0.098755843937397 = 0.0282581876963377 + 0.01 * 7.049765586853027
Epoch 490, val loss: 1.1393722295761108
Epoch 500, training loss: 0.09624888747930527 = 0.025726454332470894 + 0.01 * 7.052243709564209
Epoch 500, val loss: 1.155168890953064
Epoch 510, training loss: 0.09390509128570557 = 0.023507531732320786 + 0.01 * 7.0397562980651855
Epoch 510, val loss: 1.1703238487243652
Epoch 520, training loss: 0.0917809009552002 = 0.021556306630373 + 0.01 * 7.022459506988525
Epoch 520, val loss: 1.1849112510681152
Epoch 530, training loss: 0.08999884128570557 = 0.019835131242871284 + 0.01 * 7.016371250152588
Epoch 530, val loss: 1.1988950967788696
Epoch 540, training loss: 0.08850780129432678 = 0.018313992768526077 + 0.01 * 7.019381046295166
Epoch 540, val loss: 1.2122989892959595
Epoch 550, training loss: 0.08709238469600677 = 0.016962753608822823 + 0.01 * 7.01296329498291
Epoch 550, val loss: 1.225172996520996
Epoch 560, training loss: 0.08574959635734558 = 0.015758052468299866 + 0.01 * 6.999154567718506
Epoch 560, val loss: 1.2375483512878418
Epoch 570, training loss: 0.08506187051534653 = 0.014679682441055775 + 0.01 * 7.0382184982299805
Epoch 570, val loss: 1.2494189739227295
Epoch 580, training loss: 0.0837157815694809 = 0.013714591972529888 + 0.01 * 7.000119686126709
Epoch 580, val loss: 1.2608166933059692
Epoch 590, training loss: 0.08269332349300385 = 0.012845620512962341 + 0.01 * 6.98477029800415
Epoch 590, val loss: 1.2717466354370117
Epoch 600, training loss: 0.08183684945106506 = 0.012058950960636139 + 0.01 * 6.977789878845215
Epoch 600, val loss: 1.2823011875152588
Epoch 610, training loss: 0.0811537504196167 = 0.011344712227582932 + 0.01 * 6.9809041023254395
Epoch 610, val loss: 1.2924408912658691
Epoch 620, training loss: 0.08044198155403137 = 0.01069653034210205 + 0.01 * 6.974545001983643
Epoch 620, val loss: 1.3022165298461914
Epoch 630, training loss: 0.07973705977201462 = 0.010105214081704617 + 0.01 * 6.963184833526611
Epoch 630, val loss: 1.3116596937179565
Epoch 640, training loss: 0.07912733405828476 = 0.009563413448631763 + 0.01 * 6.956392288208008
Epoch 640, val loss: 1.3207464218139648
Epoch 650, training loss: 0.0786762610077858 = 0.00906585343182087 + 0.01 * 6.961041450500488
Epoch 650, val loss: 1.3295464515686035
Epoch 660, training loss: 0.07814690470695496 = 0.008609472773969173 + 0.01 * 6.9537434577941895
Epoch 660, val loss: 1.3380000591278076
Epoch 670, training loss: 0.07792901247739792 = 0.00818989984691143 + 0.01 * 6.973911285400391
Epoch 670, val loss: 1.3461843729019165
Epoch 680, training loss: 0.07713045924901962 = 0.007803180254995823 + 0.01 * 6.932728290557861
Epoch 680, val loss: 1.35407555103302
Epoch 690, training loss: 0.07687939703464508 = 0.007445266470313072 + 0.01 * 6.943413257598877
Epoch 690, val loss: 1.3616513013839722
Epoch 700, training loss: 0.07637925446033478 = 0.007114369887858629 + 0.01 * 6.926487922668457
Epoch 700, val loss: 1.3690192699432373
Epoch 710, training loss: 0.07627065479755402 = 0.006807147525250912 + 0.01 * 6.946351051330566
Epoch 710, val loss: 1.3760696649551392
Epoch 720, training loss: 0.0756293535232544 = 0.006522930692881346 + 0.01 * 6.910642147064209
Epoch 720, val loss: 1.3829587697982788
Epoch 730, training loss: 0.07552551478147507 = 0.006257981993257999 + 0.01 * 6.926753520965576
Epoch 730, val loss: 1.389532208442688
Epoch 740, training loss: 0.0752425566315651 = 0.0060111600905656815 + 0.01 * 6.923140048980713
Epoch 740, val loss: 1.3959448337554932
Epoch 750, training loss: 0.07488434761762619 = 0.005781042855232954 + 0.01 * 6.9103312492370605
Epoch 750, val loss: 1.4021109342575073
Epoch 760, training loss: 0.07461711764335632 = 0.005565237253904343 + 0.01 * 6.905188083648682
Epoch 760, val loss: 1.4080651998519897
Epoch 770, training loss: 0.07452759146690369 = 0.005363417323678732 + 0.01 * 6.916417121887207
Epoch 770, val loss: 1.4138504266738892
Epoch 780, training loss: 0.07416260987520218 = 0.005174055229872465 + 0.01 * 6.898855686187744
Epoch 780, val loss: 1.4194332361221313
Epoch 790, training loss: 0.0741383358836174 = 0.004996501840651035 + 0.01 * 6.914183616638184
Epoch 790, val loss: 1.4248493909835815
Epoch 800, training loss: 0.07364051043987274 = 0.004829340614378452 + 0.01 * 6.881117343902588
Epoch 800, val loss: 1.4300916194915771
Epoch 810, training loss: 0.07348795235157013 = 0.004671679809689522 + 0.01 * 6.881627559661865
Epoch 810, val loss: 1.4351859092712402
Epoch 820, training loss: 0.07338376343250275 = 0.004522833973169327 + 0.01 * 6.8860931396484375
Epoch 820, val loss: 1.4401495456695557
Epoch 830, training loss: 0.07311643660068512 = 0.004382457584142685 + 0.01 * 6.873398780822754
Epoch 830, val loss: 1.4449049234390259
Epoch 840, training loss: 0.072855144739151 = 0.004250300116837025 + 0.01 * 6.8604841232299805
Epoch 840, val loss: 1.4495497941970825
Epoch 850, training loss: 0.07279189676046371 = 0.0041251578368246555 + 0.01 * 6.866674423217773
Epoch 850, val loss: 1.4540271759033203
Epoch 860, training loss: 0.07256300002336502 = 0.004006343428045511 + 0.01 * 6.855666160583496
Epoch 860, val loss: 1.458375096321106
Epoch 870, training loss: 0.07270477712154388 = 0.00389377074316144 + 0.01 * 6.881100654602051
Epoch 870, val loss: 1.4626281261444092
Epoch 880, training loss: 0.07242176681756973 = 0.003786718472838402 + 0.01 * 6.863504886627197
Epoch 880, val loss: 1.4666824340820312
Epoch 890, training loss: 0.07229618728160858 = 0.0036852783523499966 + 0.01 * 6.861090660095215
Epoch 890, val loss: 1.4706593751907349
Epoch 900, training loss: 0.07208983600139618 = 0.0035886738914996386 + 0.01 * 6.85011625289917
Epoch 900, val loss: 1.4744715690612793
Epoch 910, training loss: 0.07207001000642776 = 0.0034970261622220278 + 0.01 * 6.857298374176025
Epoch 910, val loss: 1.4781965017318726
Epoch 920, training loss: 0.07183662056922913 = 0.0034095197916030884 + 0.01 * 6.842710018157959
Epoch 920, val loss: 1.4818440675735474
Epoch 930, training loss: 0.07191705703735352 = 0.0033261312637478113 + 0.01 * 6.859092712402344
Epoch 930, val loss: 1.4853320121765137
Epoch 940, training loss: 0.07166014611721039 = 0.003246449865400791 + 0.01 * 6.84136962890625
Epoch 940, val loss: 1.4887396097183228
Epoch 950, training loss: 0.07145412266254425 = 0.0031703454442322254 + 0.01 * 6.8283772468566895
Epoch 950, val loss: 1.492043137550354
Epoch 960, training loss: 0.07139328122138977 = 0.0030976554844528437 + 0.01 * 6.829563140869141
Epoch 960, val loss: 1.495172142982483
Epoch 970, training loss: 0.07145632058382034 = 0.003028210485354066 + 0.01 * 6.84281063079834
Epoch 970, val loss: 1.498297095298767
Epoch 980, training loss: 0.07113885134458542 = 0.0029619201086461544 + 0.01 * 6.817693710327148
Epoch 980, val loss: 1.5013201236724854
Epoch 990, training loss: 0.07101543247699738 = 0.0028983168303966522 + 0.01 * 6.811712265014648
Epoch 990, val loss: 1.5042064189910889
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8065366367949395
The final CL Acc:0.74691, 0.00462, The final GNN Acc:0.80865, 0.00155
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13166])
remove edge: torch.Size([2, 7858])
updated graph: torch.Size([2, 10468])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0415492057800293 = 1.9555805921554565 + 0.01 * 8.596851348876953
Epoch 0, val loss: 1.953776478767395
Epoch 10, training loss: 2.0316479206085205 = 1.945679783821106 + 0.01 * 8.596814155578613
Epoch 10, val loss: 1.9441568851470947
Epoch 20, training loss: 2.019796848297119 = 1.9338302612304688 + 0.01 * 8.596654891967773
Epoch 20, val loss: 1.932623267173767
Epoch 30, training loss: 2.0033388137817383 = 1.9173775911331177 + 0.01 * 8.5961332321167
Epoch 30, val loss: 1.916548490524292
Epoch 40, training loss: 1.9790304899215698 = 1.8930972814559937 + 0.01 * 8.593323707580566
Epoch 40, val loss: 1.8930376768112183
Epoch 50, training loss: 1.943719506263733 = 1.8579994440078735 + 0.01 * 8.572003364562988
Epoch 50, val loss: 1.8601655960083008
Epoch 60, training loss: 1.8988971710205078 = 1.8142863512039185 + 0.01 * 8.46108627319336
Epoch 60, val loss: 1.822021245956421
Epoch 70, training loss: 1.8550106287002563 = 1.7729445695877075 + 0.01 * 8.206609725952148
Epoch 70, val loss: 1.7880308628082275
Epoch 80, training loss: 1.8107798099517822 = 1.7309789657592773 + 0.01 * 7.980079174041748
Epoch 80, val loss: 1.7486441135406494
Epoch 90, training loss: 1.7496036291122437 = 1.6720608472824097 + 0.01 * 7.754283428192139
Epoch 90, val loss: 1.693542242050171
Epoch 100, training loss: 1.6677600145339966 = 1.5918599367141724 + 0.01 * 7.590011119842529
Epoch 100, val loss: 1.623698115348816
Epoch 110, training loss: 1.5670920610427856 = 1.4924252033233643 + 0.01 * 7.466686248779297
Epoch 110, val loss: 1.538832426071167
Epoch 120, training loss: 1.4604418277740479 = 1.3862642049789429 + 0.01 * 7.417760848999023
Epoch 120, val loss: 1.4499391317367554
Epoch 130, training loss: 1.3579747676849365 = 1.2842986583709717 + 0.01 * 7.367615222930908
Epoch 130, val loss: 1.3667985200881958
Epoch 140, training loss: 1.2623010873794556 = 1.1889697313308716 + 0.01 * 7.333137512207031
Epoch 140, val loss: 1.2903491258621216
Epoch 150, training loss: 1.171874761581421 = 1.098800539970398 + 0.01 * 7.307419300079346
Epoch 150, val loss: 1.2187658548355103
Epoch 160, training loss: 1.0848644971847534 = 1.0119813680648804 + 0.01 * 7.288308143615723
Epoch 160, val loss: 1.1488292217254639
Epoch 170, training loss: 1.0007014274597168 = 0.9279811978340149 + 0.01 * 7.272023677825928
Epoch 170, val loss: 1.0802980661392212
Epoch 180, training loss: 0.9202392101287842 = 0.8477464318275452 + 0.01 * 7.249276638031006
Epoch 180, val loss: 1.013651728630066
Epoch 190, training loss: 0.8450256586074829 = 0.7728504538536072 + 0.01 * 7.217522144317627
Epoch 190, val loss: 0.9513880014419556
Epoch 200, training loss: 0.7765001654624939 = 0.7046160697937012 + 0.01 * 7.188408851623535
Epoch 200, val loss: 0.8962467908859253
Epoch 210, training loss: 0.7146189212799072 = 0.6429693102836609 + 0.01 * 7.1649580001831055
Epoch 210, val loss: 0.8499768376350403
Epoch 220, training loss: 0.6578283309936523 = 0.5863184928894043 + 0.01 * 7.150987148284912
Epoch 220, val loss: 0.8120304346084595
Epoch 230, training loss: 0.6042911410331726 = 0.5328360795974731 + 0.01 * 7.145504951477051
Epoch 230, val loss: 0.780497670173645
Epoch 240, training loss: 0.5527803897857666 = 0.4813823699951172 + 0.01 * 7.13979959487915
Epoch 240, val loss: 0.7533760070800781
Epoch 250, training loss: 0.5029761791229248 = 0.4316219091415405 + 0.01 * 7.135427474975586
Epoch 250, val loss: 0.7297346591949463
Epoch 260, training loss: 0.45512211322784424 = 0.383786678314209 + 0.01 * 7.133543491363525
Epoch 260, val loss: 0.709838330745697
Epoch 270, training loss: 0.4098415672779083 = 0.3385307192802429 + 0.01 * 7.131084442138672
Epoch 270, val loss: 0.6942253112792969
Epoch 280, training loss: 0.36798095703125 = 0.29667484760284424 + 0.01 * 7.130609512329102
Epoch 280, val loss: 0.6828784942626953
Epoch 290, training loss: 0.33023545145988464 = 0.2589491307735443 + 0.01 * 7.128632068634033
Epoch 290, val loss: 0.6755691766738892
Epoch 300, training loss: 0.29700613021850586 = 0.2257164865732193 + 0.01 * 7.128965377807617
Epoch 300, val loss: 0.6719578504562378
Epoch 310, training loss: 0.26821959018707275 = 0.19694869220256805 + 0.01 * 7.127091407775879
Epoch 310, val loss: 0.6716638803482056
Epoch 320, training loss: 0.24355193972587585 = 0.17231036722660065 + 0.01 * 7.124157428741455
Epoch 320, val loss: 0.6743716597557068
Epoch 330, training loss: 0.22252602875232697 = 0.15129664540290833 + 0.01 * 7.122938632965088
Epoch 330, val loss: 0.6795799136161804
Epoch 340, training loss: 0.2045801281929016 = 0.1333448737859726 + 0.01 * 7.123525619506836
Epoch 340, val loss: 0.6867015361785889
Epoch 350, training loss: 0.18913663923740387 = 0.11795316636562347 + 0.01 * 7.118347644805908
Epoch 350, val loss: 0.6952568888664246
Epoch 360, training loss: 0.1758376955986023 = 0.10468767583370209 + 0.01 * 7.115002155303955
Epoch 360, val loss: 0.7048009634017944
Epoch 370, training loss: 0.16429930925369263 = 0.09319128841161728 + 0.01 * 7.110803127288818
Epoch 370, val loss: 0.7151507139205933
Epoch 380, training loss: 0.1542864441871643 = 0.08318206667900085 + 0.01 * 7.110437870025635
Epoch 380, val loss: 0.7260537147521973
Epoch 390, training loss: 0.14545488357543945 = 0.07443583756685257 + 0.01 * 7.101905822753906
Epoch 390, val loss: 0.7373738884925842
Epoch 400, training loss: 0.1377277374267578 = 0.06676752120256424 + 0.01 * 7.09602165222168
Epoch 400, val loss: 0.7490040063858032
Epoch 410, training loss: 0.1309586465358734 = 0.0600302554666996 + 0.01 * 7.092838764190674
Epoch 410, val loss: 0.7609095573425293
Epoch 420, training loss: 0.12499965727329254 = 0.05411200970411301 + 0.01 * 7.088764667510986
Epoch 420, val loss: 0.772951066493988
Epoch 430, training loss: 0.11968985944986343 = 0.04890408366918564 + 0.01 * 7.078577995300293
Epoch 430, val loss: 0.7851072549819946
Epoch 440, training loss: 0.11499421298503876 = 0.044313427060842514 + 0.01 * 7.068079471588135
Epoch 440, val loss: 0.7973527312278748
Epoch 450, training loss: 0.11090542376041412 = 0.040264032781124115 + 0.01 * 7.064139366149902
Epoch 450, val loss: 0.8096069097518921
Epoch 460, training loss: 0.10727265477180481 = 0.036697931587696075 + 0.01 * 7.057472229003906
Epoch 460, val loss: 0.8217503428459167
Epoch 470, training loss: 0.10396288335323334 = 0.03354601189494133 + 0.01 * 7.041687488555908
Epoch 470, val loss: 0.8337870240211487
Epoch 480, training loss: 0.10108333826065063 = 0.030751360580325127 + 0.01 * 7.033197402954102
Epoch 480, val loss: 0.8457320332527161
Epoch 490, training loss: 0.09863537549972534 = 0.028268208727240562 + 0.01 * 7.036716938018799
Epoch 490, val loss: 0.8575415015220642
Epoch 500, training loss: 0.09631569683551788 = 0.026061171665787697 + 0.01 * 7.025453090667725
Epoch 500, val loss: 0.8691098690032959
Epoch 510, training loss: 0.0942191630601883 = 0.024093909189105034 + 0.01 * 7.01252555847168
Epoch 510, val loss: 0.8805024027824402
Epoch 520, training loss: 0.09238743036985397 = 0.022335665300488472 + 0.01 * 7.005177021026611
Epoch 520, val loss: 0.8916192650794983
Epoch 530, training loss: 0.09073570370674133 = 0.020758865401148796 + 0.01 * 6.997684001922607
Epoch 530, val loss: 0.9024940729141235
Epoch 540, training loss: 0.08919385820627213 = 0.019342070445418358 + 0.01 * 6.985178470611572
Epoch 540, val loss: 0.9131126999855042
Epoch 550, training loss: 0.08772656321525574 = 0.018064815551042557 + 0.01 * 6.966175079345703
Epoch 550, val loss: 0.923466145992279
Epoch 560, training loss: 0.08653069287538528 = 0.01691022515296936 + 0.01 * 6.9620466232299805
Epoch 560, val loss: 0.9335997700691223
Epoch 570, training loss: 0.08533903956413269 = 0.01586461253464222 + 0.01 * 6.947443008422852
Epoch 570, val loss: 0.9434500336647034
Epoch 580, training loss: 0.08445659279823303 = 0.014914285391569138 + 0.01 * 6.954231262207031
Epoch 580, val loss: 0.9530406594276428
Epoch 590, training loss: 0.08353116363286972 = 0.014048066921532154 + 0.01 * 6.948309898376465
Epoch 590, val loss: 0.9624165892601013
Epoch 600, training loss: 0.08285696804523468 = 0.013256911188364029 + 0.01 * 6.960006237030029
Epoch 600, val loss: 0.9715666770935059
Epoch 610, training loss: 0.0819472223520279 = 0.012534106150269508 + 0.01 * 6.941311836242676
Epoch 610, val loss: 0.9803985357284546
Epoch 620, training loss: 0.08104675263166428 = 0.011871241964399815 + 0.01 * 6.917551040649414
Epoch 620, val loss: 0.9890425205230713
Epoch 630, training loss: 0.08042160421609879 = 0.01126184593886137 + 0.01 * 6.915976524353027
Epoch 630, val loss: 0.9974327683448792
Epoch 640, training loss: 0.08005842566490173 = 0.010700481943786144 + 0.01 * 6.935794830322266
Epoch 640, val loss: 1.0056066513061523
Epoch 650, training loss: 0.07920064777135849 = 0.010182404890656471 + 0.01 * 6.901824474334717
Epoch 650, val loss: 1.0135499238967896
Epoch 660, training loss: 0.07867974042892456 = 0.009703603573143482 + 0.01 * 6.897613525390625
Epoch 660, val loss: 1.0212836265563965
Epoch 670, training loss: 0.07839890569448471 = 0.009259969927370548 + 0.01 * 6.913893699645996
Epoch 670, val loss: 1.028833270072937
Epoch 680, training loss: 0.07769308984279633 = 0.00884836446493864 + 0.01 * 6.884472846984863
Epoch 680, val loss: 1.0361570119857788
Epoch 690, training loss: 0.07720906287431717 = 0.008465633727610111 + 0.01 * 6.874343395233154
Epoch 690, val loss: 1.0433003902435303
Epoch 700, training loss: 0.07693058252334595 = 0.008109086193144321 + 0.01 * 6.882150173187256
Epoch 700, val loss: 1.050269603729248
Epoch 710, training loss: 0.07646162807941437 = 0.00777636980637908 + 0.01 * 6.868525981903076
Epoch 710, val loss: 1.0570684671401978
Epoch 720, training loss: 0.0763688012957573 = 0.0074653783813118935 + 0.01 * 6.8903422355651855
Epoch 720, val loss: 1.0637109279632568
Epoch 730, training loss: 0.07579553872346878 = 0.007174989208579063 + 0.01 * 6.862054824829102
Epoch 730, val loss: 1.0701735019683838
Epoch 740, training loss: 0.07546139508485794 = 0.006902777589857578 + 0.01 * 6.855861663818359
Epoch 740, val loss: 1.0764312744140625
Epoch 750, training loss: 0.07524091005325317 = 0.006647261790931225 + 0.01 * 6.859364986419678
Epoch 750, val loss: 1.0826212167739868
Epoch 760, training loss: 0.07490944862365723 = 0.0064072152599692345 + 0.01 * 6.850223541259766
Epoch 760, val loss: 1.0886203050613403
Epoch 770, training loss: 0.07450704276561737 = 0.006181271281093359 + 0.01 * 6.832577228546143
Epoch 770, val loss: 1.0945050716400146
Epoch 780, training loss: 0.07436443120241165 = 0.00596889341250062 + 0.01 * 6.8395538330078125
Epoch 780, val loss: 1.1002689599990845
Epoch 790, training loss: 0.07395211607217789 = 0.005768476985394955 + 0.01 * 6.818364143371582
Epoch 790, val loss: 1.105863332748413
Epoch 800, training loss: 0.07376124709844589 = 0.005579499993473291 + 0.01 * 6.818174362182617
Epoch 800, val loss: 1.1113488674163818
Epoch 810, training loss: 0.07354497909545898 = 0.005400436464697123 + 0.01 * 6.814454555511475
Epoch 810, val loss: 1.1167231798171997
Epoch 820, training loss: 0.07368540018796921 = 0.0052311657927930355 + 0.01 * 6.845423698425293
Epoch 820, val loss: 1.122024655342102
Epoch 830, training loss: 0.07323721051216125 = 0.005070770625025034 + 0.01 * 6.816644191741943
Epoch 830, val loss: 1.1271899938583374
Epoch 840, training loss: 0.07310103625059128 = 0.004918686579912901 + 0.01 * 6.818235397338867
Epoch 840, val loss: 1.1321853399276733
Epoch 850, training loss: 0.07283932715654373 = 0.004774442408233881 + 0.01 * 6.806488513946533
Epoch 850, val loss: 1.1371922492980957
Epoch 860, training loss: 0.07257423549890518 = 0.004636318888515234 + 0.01 * 6.793792247772217
Epoch 860, val loss: 1.1420600414276123
Epoch 870, training loss: 0.07242412865161896 = 0.004505557008087635 + 0.01 * 6.7918572425842285
Epoch 870, val loss: 1.1468636989593506
Epoch 880, training loss: 0.07226164638996124 = 0.004380116704851389 + 0.01 * 6.788153648376465
Epoch 880, val loss: 1.1515626907348633
Epoch 890, training loss: 0.07209785282611847 = 0.0042610266245901585 + 0.01 * 6.783682823181152
Epoch 890, val loss: 1.1562269926071167
Epoch 900, training loss: 0.07208776473999023 = 0.004146867897361517 + 0.01 * 6.794090270996094
Epoch 900, val loss: 1.1607253551483154
Epoch 910, training loss: 0.0717686340212822 = 0.004038127139210701 + 0.01 * 6.7730512619018555
Epoch 910, val loss: 1.1652172803878784
Epoch 920, training loss: 0.07187256217002869 = 0.003934295382350683 + 0.01 * 6.793827056884766
Epoch 920, val loss: 1.1696275472640991
Epoch 930, training loss: 0.07179959863424301 = 0.0038347411900758743 + 0.01 * 6.796485900878906
Epoch 930, val loss: 1.1739366054534912
Epoch 940, training loss: 0.07153043150901794 = 0.0037402233574539423 + 0.01 * 6.779021263122559
Epoch 940, val loss: 1.1782023906707764
Epoch 950, training loss: 0.07139238715171814 = 0.003649530466645956 + 0.01 * 6.774285316467285
Epoch 950, val loss: 1.1823197603225708
Epoch 960, training loss: 0.07129119336605072 = 0.003562660189345479 + 0.01 * 6.772853374481201
Epoch 960, val loss: 1.1864237785339355
Epoch 970, training loss: 0.0710533931851387 = 0.0034793431404978037 + 0.01 * 6.757404804229736
Epoch 970, val loss: 1.1904674768447876
Epoch 980, training loss: 0.07092542946338654 = 0.0033995488192886114 + 0.01 * 6.752587795257568
Epoch 980, val loss: 1.1944148540496826
Epoch 990, training loss: 0.0708109587430954 = 0.00332304066978395 + 0.01 * 6.748791694641113
Epoch 990, val loss: 1.1982879638671875
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.0396058559417725 = 1.9536372423171997 + 0.01 * 8.596854209899902
Epoch 0, val loss: 1.9491604566574097
Epoch 10, training loss: 2.0298011302948 = 1.9438329935073853 + 0.01 * 8.596809387207031
Epoch 10, val loss: 1.9388889074325562
Epoch 20, training loss: 2.0178208351135254 = 1.931854248046875 + 0.01 * 8.596651077270508
Epoch 20, val loss: 1.926347017288208
Epoch 30, training loss: 2.0011885166168213 = 1.9152261018753052 + 0.01 * 8.596240997314453
Epoch 30, val loss: 1.9089360237121582
Epoch 40, training loss: 1.9766377210617065 = 1.8906923532485962 + 0.01 * 8.594538688659668
Epoch 40, val loss: 1.883559226989746
Epoch 50, training loss: 1.940972924232483 = 1.85514235496521 + 0.01 * 8.583062171936035
Epoch 50, val loss: 1.8480610847473145
Epoch 60, training loss: 1.8963863849639893 = 1.8111563920974731 + 0.01 * 8.523000717163086
Epoch 60, val loss: 1.8082807064056396
Epoch 70, training loss: 1.8532003164291382 = 1.7707523107528687 + 0.01 * 8.244800567626953
Epoch 70, val loss: 1.777740240097046
Epoch 80, training loss: 1.8083264827728271 = 1.728391408920288 + 0.01 * 7.993509292602539
Epoch 80, val loss: 1.7441648244857788
Epoch 90, training loss: 1.7459889650344849 = 1.6697938442230225 + 0.01 * 7.619508266448975
Epoch 90, val loss: 1.6917494535446167
Epoch 100, training loss: 1.6657099723815918 = 1.5911270380020142 + 0.01 * 7.458294868469238
Epoch 100, val loss: 1.6210922002792358
Epoch 110, training loss: 1.5688835382461548 = 1.4947770833969116 + 0.01 * 7.410643577575684
Epoch 110, val loss: 1.5402460098266602
Epoch 120, training loss: 1.463758111000061 = 1.3901125192642212 + 0.01 * 7.364560604095459
Epoch 120, val loss: 1.4537734985351562
Epoch 130, training loss: 1.3602206707000732 = 1.2868843078613281 + 0.01 * 7.3336310386657715
Epoch 130, val loss: 1.3684288263320923
Epoch 140, training loss: 1.26351797580719 = 1.1904010772705078 + 0.01 * 7.311694145202637
Epoch 140, val loss: 1.28885817527771
Epoch 150, training loss: 1.17424738407135 = 1.101338267326355 + 0.01 * 7.2909088134765625
Epoch 150, val loss: 1.2155184745788574
Epoch 160, training loss: 1.0896947383880615 = 1.016987919807434 + 0.01 * 7.270688056945801
Epoch 160, val loss: 1.1459360122680664
Epoch 170, training loss: 1.0068063735961914 = 0.9342543482780457 + 0.01 * 7.25520133972168
Epoch 170, val loss: 1.0783851146697998
Epoch 180, training loss: 0.9242690205574036 = 0.8518384695053101 + 0.01 * 7.2430572509765625
Epoch 180, val loss: 1.0121833086013794
Epoch 190, training loss: 0.8429907560348511 = 0.7707011699676514 + 0.01 * 7.228957653045654
Epoch 190, val loss: 0.9482128024101257
Epoch 200, training loss: 0.7656511068344116 = 0.6935663819313049 + 0.01 * 7.2084736824035645
Epoch 200, val loss: 0.8896552920341492
Epoch 210, training loss: 0.6952495574951172 = 0.623417317867279 + 0.01 * 7.18322229385376
Epoch 210, val loss: 0.8387698531150818
Epoch 220, training loss: 0.6329134702682495 = 0.5613309741020203 + 0.01 * 7.158252716064453
Epoch 220, val loss: 0.7972577810287476
Epoch 230, training loss: 0.5778769850730896 = 0.506487250328064 + 0.01 * 7.138976097106934
Epoch 230, val loss: 0.7650105953216553
Epoch 240, training loss: 0.5284956693649292 = 0.4572654962539673 + 0.01 * 7.123019218444824
Epoch 240, val loss: 0.7406015992164612
Epoch 250, training loss: 0.4830891788005829 = 0.4119774103164673 + 0.01 * 7.111177921295166
Epoch 250, val loss: 0.7219226360321045
Epoch 260, training loss: 0.4403872489929199 = 0.36937081813812256 + 0.01 * 7.101643085479736
Epoch 260, val loss: 0.706884503364563
Epoch 270, training loss: 0.3999650180339813 = 0.32905665040016174 + 0.01 * 7.090836048126221
Epoch 270, val loss: 0.6945468187332153
Epoch 280, training loss: 0.362191379070282 = 0.29135286808013916 + 0.01 * 7.0838494300842285
Epoch 280, val loss: 0.6851820349693298
Epoch 290, training loss: 0.3275076150894165 = 0.2567647397518158 + 0.01 * 7.074286937713623
Epoch 290, val loss: 0.6790538430213928
Epoch 300, training loss: 0.2962794005870819 = 0.22561974823474884 + 0.01 * 7.065964698791504
Epoch 300, val loss: 0.6762036085128784
Epoch 310, training loss: 0.26846808195114136 = 0.19793982803821564 + 0.01 * 7.052826404571533
Epoch 310, val loss: 0.6762411594390869
Epoch 320, training loss: 0.24407845735549927 = 0.17353671789169312 + 0.01 * 7.054174423217773
Epoch 320, val loss: 0.6790661811828613
Epoch 330, training loss: 0.22258305549621582 = 0.1521597057580948 + 0.01 * 7.0423359870910645
Epoch 330, val loss: 0.6843766570091248
Epoch 340, training loss: 0.20389103889465332 = 0.13352614641189575 + 0.01 * 7.036489486694336
Epoch 340, val loss: 0.6918870806694031
Epoch 350, training loss: 0.18775281310081482 = 0.11733172833919525 + 0.01 * 7.042108058929443
Epoch 350, val loss: 0.7011013627052307
Epoch 360, training loss: 0.17361149191856384 = 0.1032777950167656 + 0.01 * 7.033369064331055
Epoch 360, val loss: 0.7117882370948792
Epoch 370, training loss: 0.16134615242481232 = 0.0910743996500969 + 0.01 * 7.027175426483154
Epoch 370, val loss: 0.7237161993980408
Epoch 380, training loss: 0.15073050558567047 = 0.08048292994499207 + 0.01 * 7.024757385253906
Epoch 380, val loss: 0.7365193367004395
Epoch 390, training loss: 0.1415334939956665 = 0.07131030410528183 + 0.01 * 7.022319316864014
Epoch 390, val loss: 0.7500017881393433
Epoch 400, training loss: 0.13356485962867737 = 0.06337100267410278 + 0.01 * 7.01938533782959
Epoch 400, val loss: 0.7639451622962952
Epoch 410, training loss: 0.1266559511423111 = 0.056499630212783813 + 0.01 * 7.015632152557373
Epoch 410, val loss: 0.7781772613525391
Epoch 420, training loss: 0.12068042159080505 = 0.05054832622408867 + 0.01 * 7.013210296630859
Epoch 420, val loss: 0.7925055623054504
Epoch 430, training loss: 0.1154855415225029 = 0.045388445258140564 + 0.01 * 7.00970983505249
Epoch 430, val loss: 0.8067920804023743
Epoch 440, training loss: 0.11100446432828903 = 0.040905699133872986 + 0.01 * 7.009876728057861
Epoch 440, val loss: 0.8209408521652222
Epoch 450, training loss: 0.1070602759718895 = 0.03699988126754761 + 0.01 * 7.006039619445801
Epoch 450, val loss: 0.8348778486251831
Epoch 460, training loss: 0.10366611182689667 = 0.033587489277124405 + 0.01 * 7.007862091064453
Epoch 460, val loss: 0.84856116771698
Epoch 470, training loss: 0.10062018781900406 = 0.030597446486353874 + 0.01 * 7.002274036407471
Epoch 470, val loss: 0.861865222454071
Epoch 480, training loss: 0.097948357462883 = 0.027968689799308777 + 0.01 * 6.997966766357422
Epoch 480, val loss: 0.8748564124107361
Epoch 490, training loss: 0.09563593566417694 = 0.025650158524513245 + 0.01 * 6.99857759475708
Epoch 490, val loss: 0.8874254822731018
Epoch 500, training loss: 0.09350543469190598 = 0.02359904907643795 + 0.01 * 6.990638732910156
Epoch 500, val loss: 0.8996661305427551
Epoch 510, training loss: 0.09165778756141663 = 0.0217775609344244 + 0.01 * 6.988022804260254
Epoch 510, val loss: 0.9115023612976074
Epoch 520, training loss: 0.09000612795352936 = 0.020154591649770737 + 0.01 * 6.985153675079346
Epoch 520, val loss: 0.9229599237442017
Epoch 530, training loss: 0.0885719582438469 = 0.018703972920775414 + 0.01 * 6.986798286437988
Epoch 530, val loss: 0.9340997934341431
Epoch 540, training loss: 0.08719168603420258 = 0.017403915524482727 + 0.01 * 6.9787774085998535
Epoch 540, val loss: 0.9448328614234924
Epoch 550, training loss: 0.08602921664714813 = 0.01623442955315113 + 0.01 * 6.97947883605957
Epoch 550, val loss: 0.9552634954452515
Epoch 560, training loss: 0.08497720956802368 = 0.015180421993136406 + 0.01 * 6.979678630828857
Epoch 560, val loss: 0.9653615355491638
Epoch 570, training loss: 0.08392512798309326 = 0.014227522537112236 + 0.01 * 6.969760417938232
Epoch 570, val loss: 0.9751636385917664
Epoch 580, training loss: 0.08303828537464142 = 0.013362911529839039 + 0.01 * 6.9675374031066895
Epoch 580, val loss: 0.9846569895744324
Epoch 590, training loss: 0.0822921022772789 = 0.012576374225318432 + 0.01 * 6.9715728759765625
Epoch 590, val loss: 0.9938923120498657
Epoch 600, training loss: 0.0814872533082962 = 0.011859557591378689 + 0.01 * 6.962769985198975
Epoch 600, val loss: 1.0028302669525146
Epoch 610, training loss: 0.08078262954950333 = 0.011204351671040058 + 0.01 * 6.957827568054199
Epoch 610, val loss: 1.011522889137268
Epoch 620, training loss: 0.08017398416996002 = 0.010603937320411205 + 0.01 * 6.957005023956299
Epoch 620, val loss: 1.0199315547943115
Epoch 630, training loss: 0.07956898957490921 = 0.010052784346044064 + 0.01 * 6.951620578765869
Epoch 630, val loss: 1.0281485319137573
Epoch 640, training loss: 0.07910959422588348 = 0.009545700624585152 + 0.01 * 6.956389427185059
Epoch 640, val loss: 1.0361273288726807
Epoch 650, training loss: 0.07851755619049072 = 0.00907841231673956 + 0.01 * 6.943914413452148
Epoch 650, val loss: 1.043801188468933
Epoch 660, training loss: 0.07806436717510223 = 0.008646611124277115 + 0.01 * 6.941775321960449
Epoch 660, val loss: 1.0513278245925903
Epoch 670, training loss: 0.0777534544467926 = 0.008246783167123795 + 0.01 * 6.950666904449463
Epoch 670, val loss: 1.0586928129196167
Epoch 680, training loss: 0.07722270488739014 = 0.007876239717006683 + 0.01 * 6.9346466064453125
Epoch 680, val loss: 1.0656965970993042
Epoch 690, training loss: 0.07696680724620819 = 0.007531808689236641 + 0.01 * 6.94350004196167
Epoch 690, val loss: 1.0726182460784912
Epoch 700, training loss: 0.07651647925376892 = 0.007211456075310707 + 0.01 * 6.930502414703369
Epoch 700, val loss: 1.0793464183807373
Epoch 710, training loss: 0.07613783329725266 = 0.006912408396601677 + 0.01 * 6.922542572021484
Epoch 710, val loss: 1.0858820676803589
Epoch 720, training loss: 0.07592197507619858 = 0.006632475648075342 + 0.01 * 6.928950309753418
Epoch 720, val loss: 1.092305064201355
Epoch 730, training loss: 0.0755983218550682 = 0.006370870396494865 + 0.01 * 6.9227447509765625
Epoch 730, val loss: 1.0985685586929321
Epoch 740, training loss: 0.07528529316186905 = 0.006125406362116337 + 0.01 * 6.915988922119141
Epoch 740, val loss: 1.1046360731124878
Epoch 750, training loss: 0.07521021366119385 = 0.005894746631383896 + 0.01 * 6.931546688079834
Epoch 750, val loss: 1.1106165647506714
Epoch 760, training loss: 0.07473871111869812 = 0.005678179208189249 + 0.01 * 6.90605354309082
Epoch 760, val loss: 1.1164053678512573
Epoch 770, training loss: 0.07466377317905426 = 0.005474192090332508 + 0.01 * 6.9189581871032715
Epoch 770, val loss: 1.1221492290496826
Epoch 780, training loss: 0.0743011012673378 = 0.005281912628561258 + 0.01 * 6.901919364929199
Epoch 780, val loss: 1.1276781558990479
Epoch 790, training loss: 0.07410342991352081 = 0.005100101698189974 + 0.01 * 6.900332927703857
Epoch 790, val loss: 1.1331794261932373
Epoch 800, training loss: 0.07386724650859833 = 0.0049280268140137196 + 0.01 * 6.893921852111816
Epoch 800, val loss: 1.1384419202804565
Epoch 810, training loss: 0.07368119806051254 = 0.004765407647937536 + 0.01 * 6.891579627990723
Epoch 810, val loss: 1.1437044143676758
Epoch 820, training loss: 0.0734696239233017 = 0.004612007178366184 + 0.01 * 6.8857622146606445
Epoch 820, val loss: 1.148833990097046
Epoch 830, training loss: 0.07328158617019653 = 0.004466943442821503 + 0.01 * 6.88146448135376
Epoch 830, val loss: 1.1536287069320679
Epoch 840, training loss: 0.07311812043190002 = 0.004329246934503317 + 0.01 * 6.878887176513672
Epoch 840, val loss: 1.1584930419921875
Epoch 850, training loss: 0.07306109368801117 = 0.004198591690510511 + 0.01 * 6.8862504959106445
Epoch 850, val loss: 1.163280963897705
Epoch 860, training loss: 0.07284261286258698 = 0.004074635449796915 + 0.01 * 6.876798152923584
Epoch 860, val loss: 1.1678063869476318
Epoch 870, training loss: 0.07273295521736145 = 0.003956880886107683 + 0.01 * 6.877607345581055
Epoch 870, val loss: 1.172377109527588
Epoch 880, training loss: 0.07254359871149063 = 0.003845259780064225 + 0.01 * 6.8698344230651855
Epoch 880, val loss: 1.1766630411148071
Epoch 890, training loss: 0.07230016589164734 = 0.003739037550985813 + 0.01 * 6.856113433837891
Epoch 890, val loss: 1.181028127670288
Epoch 900, training loss: 0.07223569601774216 = 0.003637951100245118 + 0.01 * 6.859774589538574
Epoch 900, val loss: 1.1851251125335693
Epoch 910, training loss: 0.07204851508140564 = 0.0035417950712144375 + 0.01 * 6.850672245025635
Epoch 910, val loss: 1.189208745956421
Epoch 920, training loss: 0.07189114391803741 = 0.0034500127658247948 + 0.01 * 6.844113349914551
Epoch 920, val loss: 1.1931980848312378
Epoch 930, training loss: 0.07178527116775513 = 0.0033624994102865458 + 0.01 * 6.8422770500183105
Epoch 930, val loss: 1.1971455812454224
Epoch 940, training loss: 0.07166706025600433 = 0.0032789551187306643 + 0.01 * 6.838810920715332
Epoch 940, val loss: 1.2009121179580688
Epoch 950, training loss: 0.07161171734333038 = 0.003199095604941249 + 0.01 * 6.8412628173828125
Epoch 950, val loss: 1.2046597003936768
Epoch 960, training loss: 0.07143266499042511 = 0.003122954862192273 + 0.01 * 6.830970764160156
Epoch 960, val loss: 1.2082092761993408
Epoch 970, training loss: 0.07134062796831131 = 0.0030499512795358896 + 0.01 * 6.829067707061768
Epoch 970, val loss: 1.2119117975234985
Epoch 980, training loss: 0.0711824893951416 = 0.002979924436658621 + 0.01 * 6.820256233215332
Epoch 980, val loss: 1.2152478694915771
Epoch 990, training loss: 0.07114357501268387 = 0.002913432428613305 + 0.01 * 6.823014259338379
Epoch 990, val loss: 1.2187036275863647
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.0494048595428467 = 1.9634367227554321 + 0.01 * 8.59680461883545
Epoch 0, val loss: 1.9640803337097168
Epoch 10, training loss: 2.038499593734741 = 1.9525320529937744 + 0.01 * 8.596760749816895
Epoch 10, val loss: 1.9531302452087402
Epoch 20, training loss: 2.0254902839660645 = 1.9395248889923096 + 0.01 * 8.596529960632324
Epoch 20, val loss: 1.9399244785308838
Epoch 30, training loss: 2.00758957862854 = 1.921632170677185 + 0.01 * 8.595747947692871
Epoch 30, val loss: 1.9218329191207886
Epoch 40, training loss: 1.9814789295196533 = 1.8955680131912231 + 0.01 * 8.591097831726074
Epoch 40, val loss: 1.8958842754364014
Epoch 50, training loss: 1.9437129497528076 = 1.8581440448760986 + 0.01 * 8.556888580322266
Epoch 50, val loss: 1.8600304126739502
Epoch 60, training loss: 1.895593285560608 = 1.811828374862671 + 0.01 * 8.37648868560791
Epoch 60, val loss: 1.8191596269607544
Epoch 70, training loss: 1.8494874238967896 = 1.7676078081130981 + 0.01 * 8.187958717346191
Epoch 70, val loss: 1.7829244136810303
Epoch 80, training loss: 1.7992357015609741 = 1.7195054292678833 + 0.01 * 7.973026752471924
Epoch 80, val loss: 1.7381536960601807
Epoch 90, training loss: 1.7311931848526 = 1.6535180807113647 + 0.01 * 7.767513275146484
Epoch 90, val loss: 1.676056146621704
Epoch 100, training loss: 1.6425034999847412 = 1.566420316696167 + 0.01 * 7.60831880569458
Epoch 100, val loss: 1.6004893779754639
Epoch 110, training loss: 1.53627347946167 = 1.4623867273330688 + 0.01 * 7.388679027557373
Epoch 110, val loss: 1.5116260051727295
Epoch 120, training loss: 1.4282543659210205 = 1.3549665212631226 + 0.01 * 7.328787803649902
Epoch 120, val loss: 1.4230663776397705
Epoch 130, training loss: 1.3271856307983398 = 1.2542392015457153 + 0.01 * 7.294646739959717
Epoch 130, val loss: 1.3414158821105957
Epoch 140, training loss: 1.2346025705337524 = 1.1617166996002197 + 0.01 * 7.288582801818848
Epoch 140, val loss: 1.2682287693023682
Epoch 150, training loss: 1.1469590663909912 = 1.0741181373596191 + 0.01 * 7.284097194671631
Epoch 150, val loss: 1.1988412141799927
Epoch 160, training loss: 1.0613662004470825 = 0.9886090159416199 + 0.01 * 7.275714874267578
Epoch 160, val loss: 1.130165696144104
Epoch 170, training loss: 0.9780664443969727 = 0.9054321646690369 + 0.01 * 7.263429164886475
Epoch 170, val loss: 1.0630784034729004
Epoch 180, training loss: 0.8995473980903625 = 0.8271077871322632 + 0.01 * 7.243960380554199
Epoch 180, val loss: 0.9999562501907349
Epoch 190, training loss: 0.828294038772583 = 0.7561551332473755 + 0.01 * 7.2138895988464355
Epoch 190, val loss: 0.9436195492744446
Epoch 200, training loss: 0.7647402882575989 = 0.6929868459701538 + 0.01 * 7.175346851348877
Epoch 200, val loss: 0.8951364755630493
Epoch 210, training loss: 0.7071222066879272 = 0.6356340050697327 + 0.01 * 7.148820877075195
Epoch 210, val loss: 0.8535916805267334
Epoch 220, training loss: 0.6521117687225342 = 0.5808643698692322 + 0.01 * 7.124740123748779
Epoch 220, val loss: 0.8164853453636169
Epoch 230, training loss: 0.5969702005386353 = 0.5258728861808777 + 0.01 * 7.109732627868652
Epoch 230, val loss: 0.7815846800804138
Epoch 240, training loss: 0.5406049489974976 = 0.4695957899093628 + 0.01 * 7.100913047790527
Epoch 240, val loss: 0.7483754754066467
Epoch 250, training loss: 0.48399826884269714 = 0.413063108921051 + 0.01 * 7.093515396118164
Epoch 250, val loss: 0.717991828918457
Epoch 260, training loss: 0.4293898046016693 = 0.35854169726371765 + 0.01 * 7.084810733795166
Epoch 260, val loss: 0.6919152736663818
Epoch 270, training loss: 0.37918829917907715 = 0.3083602786064148 + 0.01 * 7.082801342010498
Epoch 270, val loss: 0.6709600687026978
Epoch 280, training loss: 0.33472585678100586 = 0.2640298008918762 + 0.01 * 7.069604873657227
Epoch 280, val loss: 0.6554986834526062
Epoch 290, training loss: 0.2966277301311493 = 0.2260182797908783 + 0.01 * 7.0609450340271
Epoch 290, val loss: 0.6454372406005859
Epoch 300, training loss: 0.2645340859889984 = 0.19399969279766083 + 0.01 * 7.053439617156982
Epoch 300, val loss: 0.6399964690208435
Epoch 310, training loss: 0.237751305103302 = 0.1672583371400833 + 0.01 * 7.049297332763672
Epoch 310, val loss: 0.6384297013282776
Epoch 320, training loss: 0.21533659100532532 = 0.1449791043996811 + 0.01 * 7.0357489585876465
Epoch 320, val loss: 0.6399011015892029
Epoch 330, training loss: 0.19664639234542847 = 0.1263648122549057 + 0.01 * 7.028158664703369
Epoch 330, val loss: 0.6437918543815613
Epoch 340, training loss: 0.180893212556839 = 0.1107226237654686 + 0.01 * 7.017058849334717
Epoch 340, val loss: 0.649555504322052
Epoch 350, training loss: 0.16800010204315186 = 0.09749307483434677 + 0.01 * 7.050703048706055
Epoch 350, val loss: 0.656766951084137
Epoch 360, training loss: 0.1563895046710968 = 0.08624723553657532 + 0.01 * 7.01422643661499
Epoch 360, val loss: 0.6652079820632935
Epoch 370, training loss: 0.14656192064285278 = 0.07661865651607513 + 0.01 * 6.994325637817383
Epoch 370, val loss: 0.6744824647903442
Epoch 380, training loss: 0.13819429278373718 = 0.06832549721002579 + 0.01 * 6.986878871917725
Epoch 380, val loss: 0.6843944191932678
Epoch 390, training loss: 0.1309536248445511 = 0.06115173548460007 + 0.01 * 6.980188846588135
Epoch 390, val loss: 0.6947501301765442
Epoch 400, training loss: 0.12466637790203094 = 0.05492755025625229 + 0.01 * 6.973883152008057
Epoch 400, val loss: 0.7053945064544678
Epoch 410, training loss: 0.11913527548313141 = 0.04950326308608055 + 0.01 * 6.963201999664307
Epoch 410, val loss: 0.7162091135978699
Epoch 420, training loss: 0.11432717740535736 = 0.044762905687093735 + 0.01 * 6.956427574157715
Epoch 420, val loss: 0.7271075248718262
Epoch 430, training loss: 0.11029301583766937 = 0.040608976036310196 + 0.01 * 6.9684038162231445
Epoch 430, val loss: 0.7379654049873352
Epoch 440, training loss: 0.10660399496555328 = 0.03696770593523979 + 0.01 * 6.963629245758057
Epoch 440, val loss: 0.7486993074417114
Epoch 450, training loss: 0.1031322330236435 = 0.033764421939849854 + 0.01 * 6.936781406402588
Epoch 450, val loss: 0.7592716813087463
Epoch 460, training loss: 0.10025157034397125 = 0.030934983864426613 + 0.01 * 6.931658744812012
Epoch 460, val loss: 0.7696923613548279
Epoch 470, training loss: 0.09786645323038101 = 0.028427930548787117 + 0.01 * 6.943852424621582
Epoch 470, val loss: 0.7799230813980103
Epoch 480, training loss: 0.09542197734117508 = 0.026203565299510956 + 0.01 * 6.921841621398926
Epoch 480, val loss: 0.7899866700172424
Epoch 490, training loss: 0.09337706118822098 = 0.02422201633453369 + 0.01 * 6.9155049324035645
Epoch 490, val loss: 0.7998026013374329
Epoch 500, training loss: 0.09157003462314606 = 0.022450430318713188 + 0.01 * 6.911960124969482
Epoch 500, val loss: 0.8093783259391785
Epoch 510, training loss: 0.08998514711856842 = 0.020861957222223282 + 0.01 * 6.912319660186768
Epoch 510, val loss: 0.8187716007232666
Epoch 520, training loss: 0.0885457694530487 = 0.019435444846749306 + 0.01 * 6.9110331535339355
Epoch 520, val loss: 0.8278840780258179
Epoch 530, training loss: 0.08716157078742981 = 0.018150223419070244 + 0.01 * 6.901134967803955
Epoch 530, val loss: 0.8367797136306763
Epoch 540, training loss: 0.08597929775714874 = 0.016987990587949753 + 0.01 * 6.8991312980651855
Epoch 540, val loss: 0.8454107642173767
Epoch 550, training loss: 0.08485379815101624 = 0.015934685245156288 + 0.01 * 6.891911506652832
Epoch 550, val loss: 0.8538661599159241
Epoch 560, training loss: 0.0838722437620163 = 0.01497778668999672 + 0.01 * 6.889446258544922
Epoch 560, val loss: 0.8620773553848267
Epoch 570, training loss: 0.08289706707000732 = 0.014105851761996746 + 0.01 * 6.879122257232666
Epoch 570, val loss: 0.8700678944587708
Epoch 580, training loss: 0.08228322863578796 = 0.013309044763445854 + 0.01 * 6.897418022155762
Epoch 580, val loss: 0.8778795003890991
Epoch 590, training loss: 0.08135037124156952 = 0.012580560520291328 + 0.01 * 6.876981735229492
Epoch 590, val loss: 0.8854503035545349
Epoch 600, training loss: 0.08062964677810669 = 0.011912206187844276 + 0.01 * 6.871744155883789
Epoch 600, val loss: 0.8928566575050354
Epoch 610, training loss: 0.07999791204929352 = 0.011297452263534069 + 0.01 * 6.870046138763428
Epoch 610, val loss: 0.9000262022018433
Epoch 620, training loss: 0.07934892922639847 = 0.010730958543717861 + 0.01 * 6.861796855926514
Epoch 620, val loss: 0.9070689678192139
Epoch 630, training loss: 0.07894276082515717 = 0.010207838378846645 + 0.01 * 6.873492240905762
Epoch 630, val loss: 0.9138836860656738
Epoch 640, training loss: 0.07832715660333633 = 0.00972449965775013 + 0.01 * 6.860265731811523
Epoch 640, val loss: 0.9205613136291504
Epoch 650, training loss: 0.07781106233596802 = 0.009276581928133965 + 0.01 * 6.853447914123535
Epoch 650, val loss: 0.9270570874214172
Epoch 660, training loss: 0.07745134830474854 = 0.008860379457473755 + 0.01 * 6.859097003936768
Epoch 660, val loss: 0.9334120154380798
Epoch 670, training loss: 0.07696393132209778 = 0.008473452180624008 + 0.01 * 6.849048137664795
Epoch 670, val loss: 0.9395918250083923
Epoch 680, training loss: 0.0765693262219429 = 0.008113136515021324 + 0.01 * 6.845618724822998
Epoch 680, val loss: 0.9456404447555542
Epoch 690, training loss: 0.07638947665691376 = 0.007776445243507624 + 0.01 * 6.861302852630615
Epoch 690, val loss: 0.9515408873558044
Epoch 700, training loss: 0.07582542300224304 = 0.007462116423994303 + 0.01 * 6.836331367492676
Epoch 700, val loss: 0.9572858810424805
Epoch 710, training loss: 0.07550559192895889 = 0.007167451083660126 + 0.01 * 6.8338141441345215
Epoch 710, val loss: 0.9628984332084656
Epoch 720, training loss: 0.07519285380840302 = 0.006890446413308382 + 0.01 * 6.8302412033081055
Epoch 720, val loss: 0.968449592590332
Epoch 730, training loss: 0.07497710734605789 = 0.006630193907767534 + 0.01 * 6.834691047668457
Epoch 730, val loss: 0.9738255143165588
Epoch 740, training loss: 0.07466162741184235 = 0.006385618355125189 + 0.01 * 6.827601432800293
Epoch 740, val loss: 0.9791314005851746
Epoch 750, training loss: 0.07437750697135925 = 0.006154741160571575 + 0.01 * 6.822277069091797
Epoch 750, val loss: 0.9843015074729919
Epoch 760, training loss: 0.07411719113588333 = 0.0059363688342273235 + 0.01 * 6.818082332611084
Epoch 760, val loss: 0.9893893599510193
Epoch 770, training loss: 0.07398057729005814 = 0.005729875527322292 + 0.01 * 6.825069904327393
Epoch 770, val loss: 0.9944143891334534
Epoch 780, training loss: 0.07371794432401657 = 0.005534903611987829 + 0.01 * 6.818304538726807
Epoch 780, val loss: 0.9992964267730713
Epoch 790, training loss: 0.0734901875257492 = 0.0053499317727983 + 0.01 * 6.814025402069092
Epoch 790, val loss: 1.0041019916534424
Epoch 800, training loss: 0.07334476709365845 = 0.005174428690224886 + 0.01 * 6.817033767700195
Epoch 800, val loss: 1.0088568925857544
Epoch 810, training loss: 0.07304226607084274 = 0.005008059088140726 + 0.01 * 6.8034210205078125
Epoch 810, val loss: 1.013487458229065
Epoch 820, training loss: 0.0730385109782219 = 0.00484991492703557 + 0.01 * 6.818859100341797
Epoch 820, val loss: 1.01802396774292
Epoch 830, training loss: 0.07274554669857025 = 0.004699561279267073 + 0.01 * 6.804598808288574
Epoch 830, val loss: 1.0225470066070557
Epoch 840, training loss: 0.07270534336566925 = 0.004556355532258749 + 0.01 * 6.814898490905762
Epoch 840, val loss: 1.026933193206787
Epoch 850, training loss: 0.0724090188741684 = 0.004420024808496237 + 0.01 * 6.798899173736572
Epoch 850, val loss: 1.0312925577163696
Epoch 860, training loss: 0.0724271610379219 = 0.0042900023981928825 + 0.01 * 6.813715934753418
Epoch 860, val loss: 1.0355185270309448
Epoch 870, training loss: 0.07214844226837158 = 0.004166193772107363 + 0.01 * 6.798224925994873
Epoch 870, val loss: 1.0397226810455322
Epoch 880, training loss: 0.07194230705499649 = 0.004048096016049385 + 0.01 * 6.789421558380127
Epoch 880, val loss: 1.0438439846038818
Epoch 890, training loss: 0.07190987467765808 = 0.0039350856095552444 + 0.01 * 6.797479152679443
Epoch 890, val loss: 1.0478546619415283
Epoch 900, training loss: 0.07170180231332779 = 0.003827342763543129 + 0.01 * 6.787446022033691
Epoch 900, val loss: 1.051869511604309
Epoch 910, training loss: 0.0717659667134285 = 0.00372486375272274 + 0.01 * 6.804110527038574
Epoch 910, val loss: 1.0557218790054321
Epoch 920, training loss: 0.0714656412601471 = 0.0036270932760089636 + 0.01 * 6.783854961395264
Epoch 920, val loss: 1.0595250129699707
Epoch 930, training loss: 0.07130800187587738 = 0.003532875794917345 + 0.01 * 6.777512550354004
Epoch 930, val loss: 1.0632771253585815
Epoch 940, training loss: 0.07128491252660751 = 0.0034429742954671383 + 0.01 * 6.784194469451904
Epoch 940, val loss: 1.0669596195220947
Epoch 950, training loss: 0.07130475342273712 = 0.0033567447680979967 + 0.01 * 6.794800758361816
Epoch 950, val loss: 1.0706411600112915
Epoch 960, training loss: 0.0710357204079628 = 0.003274839138612151 + 0.01 * 6.776088714599609
Epoch 960, val loss: 1.0741406679153442
Epoch 970, training loss: 0.07101504504680634 = 0.0031962201464921236 + 0.01 * 6.781883239746094
Epoch 970, val loss: 1.07764732837677
Epoch 980, training loss: 0.07071559131145477 = 0.003120995359495282 + 0.01 * 6.759459495544434
Epoch 980, val loss: 1.0810425281524658
Epoch 990, training loss: 0.07095764577388763 = 0.0030486774630844593 + 0.01 * 6.790896415710449
Epoch 990, val loss: 1.0843850374221802
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.8402741170268846
The final CL Acc:0.83457, 0.00873, The final GNN Acc:0.83904, 0.00090
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11674])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10598])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.04140043258667 = 1.9554316997528076 + 0.01 * 8.596866607666016
Epoch 0, val loss: 1.9519171714782715
Epoch 10, training loss: 2.0310797691345215 = 1.9451115131378174 + 0.01 * 8.596824645996094
Epoch 10, val loss: 1.9418233633041382
Epoch 20, training loss: 2.018251419067383 = 1.9322845935821533 + 0.01 * 8.596694946289062
Epoch 20, val loss: 1.9288665056228638
Epoch 30, training loss: 2.00048828125 = 1.914525032043457 + 0.01 * 8.5963134765625
Epoch 30, val loss: 1.9106377363204956
Epoch 40, training loss: 1.9746699333190918 = 1.888724446296692 + 0.01 * 8.594554901123047
Epoch 40, val loss: 1.8843045234680176
Epoch 50, training loss: 1.9387909173965454 = 1.8529616594314575 + 0.01 * 8.58292293548584
Epoch 50, val loss: 1.8493646383285522
Epoch 60, training loss: 1.8981953859329224 = 1.8128641843795776 + 0.01 * 8.533116340637207
Epoch 60, val loss: 1.814394235610962
Epoch 70, training loss: 1.8639426231384277 = 1.7800832986831665 + 0.01 * 8.385929107666016
Epoch 70, val loss: 1.790163516998291
Epoch 80, training loss: 1.825803518295288 = 1.7432360649108887 + 0.01 * 8.256741523742676
Epoch 80, val loss: 1.7597299814224243
Epoch 90, training loss: 1.7716459035873413 = 1.6916139125823975 + 0.01 * 8.003201484680176
Epoch 90, val loss: 1.7143522500991821
Epoch 100, training loss: 1.6975669860839844 = 1.6198915243148804 + 0.01 * 7.7675461769104
Epoch 100, val loss: 1.6527502536773682
Epoch 110, training loss: 1.6059503555297852 = 1.5306428670883179 + 0.01 * 7.530752182006836
Epoch 110, val loss: 1.5799976587295532
Epoch 120, training loss: 1.5101984739303589 = 1.4359691143035889 + 0.01 * 7.422937393188477
Epoch 120, val loss: 1.5052416324615479
Epoch 130, training loss: 1.4173380136489868 = 1.3438808917999268 + 0.01 * 7.3457136154174805
Epoch 130, val loss: 1.4362956285476685
Epoch 140, training loss: 1.32645845413208 = 1.2535481452941895 + 0.01 * 7.2910356521606445
Epoch 140, val loss: 1.3724392652511597
Epoch 150, training loss: 1.236236810684204 = 1.163702368736267 + 0.01 * 7.253443241119385
Epoch 150, val loss: 1.3098134994506836
Epoch 160, training loss: 1.1481777429580688 = 1.0758297443389893 + 0.01 * 7.234801292419434
Epoch 160, val loss: 1.2490941286087036
Epoch 170, training loss: 1.0640571117401123 = 0.9917762875556946 + 0.01 * 7.228085517883301
Epoch 170, val loss: 1.1907168626785278
Epoch 180, training loss: 0.9843140244483948 = 0.9120611548423767 + 0.01 * 7.225286483764648
Epoch 180, val loss: 1.1347864866256714
Epoch 190, training loss: 0.9080638289451599 = 0.8358207941055298 + 0.01 * 7.22430419921875
Epoch 190, val loss: 1.080508828163147
Epoch 200, training loss: 0.8342540264129639 = 0.7620125412940979 + 0.01 * 7.224146366119385
Epoch 200, val loss: 1.0274049043655396
Epoch 210, training loss: 0.7630943655967712 = 0.6908539533615112 + 0.01 * 7.224043846130371
Epoch 210, val loss: 0.9768453240394592
Epoch 220, training loss: 0.6962485313415527 = 0.6240091323852539 + 0.01 * 7.223941326141357
Epoch 220, val loss: 0.9317179918289185
Epoch 230, training loss: 0.6354305148124695 = 0.5631929636001587 + 0.01 * 7.223755359649658
Epoch 230, val loss: 0.894258975982666
Epoch 240, training loss: 0.5810245871543884 = 0.5087928175926208 + 0.01 * 7.223176002502441
Epoch 240, val loss: 0.865330159664154
Epoch 250, training loss: 0.5321267247200012 = 0.45990344882011414 + 0.01 * 7.2223286628723145
Epoch 250, val loss: 0.843884289264679
Epoch 260, training loss: 0.48728299140930176 = 0.41507044434547424 + 0.01 * 7.221255302429199
Epoch 260, val loss: 0.8276991248130798
Epoch 270, training loss: 0.4452335238456726 = 0.37303316593170166 + 0.01 * 7.220036029815674
Epoch 270, val loss: 0.8151757121086121
Epoch 280, training loss: 0.4052427411079407 = 0.3330537676811218 + 0.01 * 7.218896865844727
Epoch 280, val loss: 0.8055657148361206
Epoch 290, training loss: 0.36719125509262085 = 0.29501333832740784 + 0.01 * 7.217790603637695
Epoch 290, val loss: 0.798332929611206
Epoch 300, training loss: 0.3313885033130646 = 0.25921183824539185 + 0.01 * 7.217667102813721
Epoch 300, val loss: 0.7934833765029907
Epoch 310, training loss: 0.29825085401535034 = 0.2260863482952118 + 0.01 * 7.2164506912231445
Epoch 310, val loss: 0.7912331819534302
Epoch 320, training loss: 0.2681449055671692 = 0.19598253071308136 + 0.01 * 7.216238498687744
Epoch 320, val loss: 0.7917508482933044
Epoch 330, training loss: 0.24131619930267334 = 0.16913938522338867 + 0.01 * 7.217680931091309
Epoch 330, val loss: 0.7951887845993042
Epoch 340, training loss: 0.21775992214679718 = 0.14559659361839294 + 0.01 * 7.216332912445068
Epoch 340, val loss: 0.8013594150543213
Epoch 350, training loss: 0.1974097490310669 = 0.12524627149105072 + 0.01 * 7.2163472175598145
Epoch 350, val loss: 0.809980034828186
Epoch 360, training loss: 0.18000918626785278 = 0.10784462094306946 + 0.01 * 7.216456413269043
Epoch 360, val loss: 0.8206983208656311
Epoch 370, training loss: 0.16523650288581848 = 0.09307344257831573 + 0.01 * 7.216305732727051
Epoch 370, val loss: 0.8330938220024109
Epoch 380, training loss: 0.15274381637573242 = 0.08058304339647293 + 0.01 * 7.216076850891113
Epoch 380, val loss: 0.8467184901237488
Epoch 390, training loss: 0.14219620823860168 = 0.07004228234291077 + 0.01 * 7.215391635894775
Epoch 390, val loss: 0.8611705899238586
Epoch 400, training loss: 0.13331544399261475 = 0.061155978590250015 + 0.01 * 7.215947151184082
Epoch 400, val loss: 0.8761794567108154
Epoch 410, training loss: 0.12581279873847961 = 0.0536683015525341 + 0.01 * 7.214449882507324
Epoch 410, val loss: 0.891448974609375
Epoch 420, training loss: 0.11948613822460175 = 0.04735478013753891 + 0.01 * 7.213135719299316
Epoch 420, val loss: 0.9067854285240173
Epoch 430, training loss: 0.11412995308637619 = 0.042017944157123566 + 0.01 * 7.211201190948486
Epoch 430, val loss: 0.9220320582389832
Epoch 440, training loss: 0.10959360003471375 = 0.037490364164114 + 0.01 * 7.210324287414551
Epoch 440, val loss: 0.9370867609977722
Epoch 450, training loss: 0.1057039201259613 = 0.03363167867064476 + 0.01 * 7.207224369049072
Epoch 450, val loss: 0.9518697261810303
Epoch 460, training loss: 0.10237644612789154 = 0.030325714498758316 + 0.01 * 7.20507287979126
Epoch 460, val loss: 0.9662639498710632
Epoch 470, training loss: 0.09953412413597107 = 0.027478329837322235 + 0.01 * 7.20557975769043
Epoch 470, val loss: 0.9803286194801331
Epoch 480, training loss: 0.09702196717262268 = 0.025013841688632965 + 0.01 * 7.200812339782715
Epoch 480, val loss: 0.9938892722129822
Epoch 490, training loss: 0.0948234349489212 = 0.022868994623422623 + 0.01 * 7.195443630218506
Epoch 490, val loss: 1.0070934295654297
Epoch 500, training loss: 0.09298034757375717 = 0.020992686972022057 + 0.01 * 7.198765754699707
Epoch 500, val loss: 1.0199190378189087
Epoch 510, training loss: 0.09124066680669785 = 0.01934388093650341 + 0.01 * 7.18967866897583
Epoch 510, val loss: 1.0322885513305664
Epoch 520, training loss: 0.08972995728254318 = 0.017887525260448456 + 0.01 * 7.184243679046631
Epoch 520, val loss: 1.0443401336669922
Epoch 530, training loss: 0.08845560252666473 = 0.01659529097378254 + 0.01 * 7.186031341552734
Epoch 530, val loss: 1.0559828281402588
Epoch 540, training loss: 0.08719287067651749 = 0.015444249846041203 + 0.01 * 7.174862384796143
Epoch 540, val loss: 1.0672763586044312
Epoch 550, training loss: 0.08622486144304276 = 0.014414375647902489 + 0.01 * 7.18104887008667
Epoch 550, val loss: 1.0782032012939453
Epoch 560, training loss: 0.08520375937223434 = 0.013490884564816952 + 0.01 * 7.171287536621094
Epoch 560, val loss: 1.0887596607208252
Epoch 570, training loss: 0.08419903367757797 = 0.012659256346523762 + 0.01 * 7.153977394104004
Epoch 570, val loss: 1.0989943742752075
Epoch 580, training loss: 0.08339754492044449 = 0.011907116509974003 + 0.01 * 7.149043083190918
Epoch 580, val loss: 1.1088734865188599
Epoch 590, training loss: 0.08272764086723328 = 0.011224750429391861 + 0.01 * 7.150289535522461
Epoch 590, val loss: 1.1184793710708618
Epoch 600, training loss: 0.08204801380634308 = 0.01060532871633768 + 0.01 * 7.14426851272583
Epoch 600, val loss: 1.1277533769607544
Epoch 610, training loss: 0.08123937994241714 = 0.010041053406894207 + 0.01 * 7.119832992553711
Epoch 610, val loss: 1.1367392539978027
Epoch 620, training loss: 0.08051028847694397 = 0.00952538289129734 + 0.01 * 7.098491191864014
Epoch 620, val loss: 1.1453672647476196
Epoch 630, training loss: 0.080331951379776 = 0.009053308516740799 + 0.01 * 7.127863883972168
Epoch 630, val loss: 1.1538777351379395
Epoch 640, training loss: 0.07959876954555511 = 0.008620351552963257 + 0.01 * 7.097842216491699
Epoch 640, val loss: 1.1618952751159668
Epoch 650, training loss: 0.0791885033249855 = 0.00822234246879816 + 0.01 * 7.096616268157959
Epoch 650, val loss: 1.1696211099624634
Epoch 660, training loss: 0.07849553972482681 = 0.00785554014146328 + 0.01 * 7.064000129699707
Epoch 660, val loss: 1.1772277355194092
Epoch 670, training loss: 0.078142449259758 = 0.007517077960073948 + 0.01 * 7.06253719329834
Epoch 670, val loss: 1.1845769882202148
Epoch 680, training loss: 0.0775802880525589 = 0.0072038304060697556 + 0.01 * 7.0376458168029785
Epoch 680, val loss: 1.1915727853775024
Epoch 690, training loss: 0.07722023129463196 = 0.006913097575306892 + 0.01 * 7.03071403503418
Epoch 690, val loss: 1.1984320878982544
Epoch 700, training loss: 0.07677017152309418 = 0.006642569787800312 + 0.01 * 7.012760162353516
Epoch 700, val loss: 1.2049089670181274
Epoch 710, training loss: 0.07635224610567093 = 0.006390789058059454 + 0.01 * 6.996145725250244
Epoch 710, val loss: 1.2113327980041504
Epoch 720, training loss: 0.07612074911594391 = 0.0061554210260510445 + 0.01 * 6.996532917022705
Epoch 720, val loss: 1.2175310850143433
Epoch 730, training loss: 0.07553528994321823 = 0.005935187917202711 + 0.01 * 6.960010528564453
Epoch 730, val loss: 1.22343909740448
Epoch 740, training loss: 0.07577267289161682 = 0.005728581920266151 + 0.01 * 7.004409313201904
Epoch 740, val loss: 1.229317545890808
Epoch 750, training loss: 0.0751175582408905 = 0.005534889176487923 + 0.01 * 6.9582672119140625
Epoch 750, val loss: 1.2347993850708008
Epoch 760, training loss: 0.07479292154312134 = 0.005352266598492861 + 0.01 * 6.944066047668457
Epoch 760, val loss: 1.2403645515441895
Epoch 770, training loss: 0.07476552575826645 = 0.005180272273719311 + 0.01 * 6.958526134490967
Epoch 770, val loss: 1.245659351348877
Epoch 780, training loss: 0.0743103176355362 = 0.005017723422497511 + 0.01 * 6.929259777069092
Epoch 780, val loss: 1.2508515119552612
Epoch 790, training loss: 0.0742788091301918 = 0.004864437971264124 + 0.01 * 6.941437244415283
Epoch 790, val loss: 1.255859613418579
Epoch 800, training loss: 0.07377709448337555 = 0.004719228949397802 + 0.01 * 6.905786991119385
Epoch 800, val loss: 1.2607603073120117
Epoch 810, training loss: 0.07350169867277145 = 0.004581855610013008 + 0.01 * 6.891984939575195
Epoch 810, val loss: 1.2655813694000244
Epoch 820, training loss: 0.073548823595047 = 0.004451769404113293 + 0.01 * 6.909706115722656
Epoch 820, val loss: 1.2701581716537476
Epoch 830, training loss: 0.07332201302051544 = 0.004328035283833742 + 0.01 * 6.899397850036621
Epoch 830, val loss: 1.274741530418396
Epoch 840, training loss: 0.07294288277626038 = 0.004210553131997585 + 0.01 * 6.873232841491699
Epoch 840, val loss: 1.2791444063186646
Epoch 850, training loss: 0.07320580631494522 = 0.004098923876881599 + 0.01 * 6.910688400268555
Epoch 850, val loss: 1.2833997011184692
Epoch 860, training loss: 0.07253603637218475 = 0.003992518410086632 + 0.01 * 6.854351997375488
Epoch 860, val loss: 1.2876157760620117
Epoch 870, training loss: 0.07251472771167755 = 0.0038912612944841385 + 0.01 * 6.86234712600708
Epoch 870, val loss: 1.2916944026947021
Epoch 880, training loss: 0.07249533385038376 = 0.003794678021222353 + 0.01 * 6.870065689086914
Epoch 880, val loss: 1.2956523895263672
Epoch 890, training loss: 0.07217487692832947 = 0.003702383255586028 + 0.01 * 6.847249507904053
Epoch 890, val loss: 1.2996114492416382
Epoch 900, training loss: 0.07233031839132309 = 0.0036142002791166306 + 0.01 * 6.871612071990967
Epoch 900, val loss: 1.3033431768417358
Epoch 910, training loss: 0.07201647758483887 = 0.0035300028976053 + 0.01 * 6.848647594451904
Epoch 910, val loss: 1.3070476055145264
Epoch 920, training loss: 0.0719587579369545 = 0.0034494397696107626 + 0.01 * 6.8509321212768555
Epoch 920, val loss: 1.3106645345687866
Epoch 930, training loss: 0.07166101783514023 = 0.0033721537329256535 + 0.01 * 6.82888650894165
Epoch 930, val loss: 1.3141592741012573
Epoch 940, training loss: 0.07171612977981567 = 0.0032980190590023994 + 0.01 * 6.841811180114746
Epoch 940, val loss: 1.3177273273468018
Epoch 950, training loss: 0.07146541774272919 = 0.0032272101379930973 + 0.01 * 6.823821067810059
Epoch 950, val loss: 1.3209972381591797
Epoch 960, training loss: 0.07173728197813034 = 0.003158969571813941 + 0.01 * 6.8578314781188965
Epoch 960, val loss: 1.3244019746780396
Epoch 970, training loss: 0.07119999825954437 = 0.0030937297269701958 + 0.01 * 6.810627460479736
Epoch 970, val loss: 1.3275675773620605
Epoch 980, training loss: 0.07120584696531296 = 0.0030309115536510944 + 0.01 * 6.817493915557861
Epoch 980, val loss: 1.3307582139968872
Epoch 990, training loss: 0.07111700624227524 = 0.002970519009977579 + 0.01 * 6.8146491050720215
Epoch 990, val loss: 1.3339122533798218
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 2.05625057220459 = 1.9702820777893066 + 0.01 * 8.596846580505371
Epoch 0, val loss: 1.9652243852615356
Epoch 10, training loss: 2.045833110809326 = 1.9598652124404907 + 0.01 * 8.5968017578125
Epoch 10, val loss: 1.955469012260437
Epoch 20, training loss: 2.0327651500701904 = 1.9467990398406982 + 0.01 * 8.59661865234375
Epoch 20, val loss: 1.9429560899734497
Epoch 30, training loss: 2.0139834880828857 = 1.9280236959457397 + 0.01 * 8.595978736877441
Epoch 30, val loss: 1.9248641729354858
Epoch 40, training loss: 1.985946536064148 = 1.900024175643921 + 0.01 * 8.592236518859863
Epoch 40, val loss: 1.8982049226760864
Epoch 50, training loss: 1.9468683004379272 = 1.8612140417099 + 0.01 * 8.565425872802734
Epoch 50, val loss: 1.8628565073013306
Epoch 60, training loss: 1.903557538986206 = 1.8192660808563232 + 0.01 * 8.429147720336914
Epoch 60, val loss: 1.8284422159194946
Epoch 70, training loss: 1.8709133863449097 = 1.787522792816162 + 0.01 * 8.339056968688965
Epoch 70, val loss: 1.80331289768219
Epoch 80, training loss: 1.8360230922698975 = 1.754093050956726 + 0.01 * 8.192998886108398
Epoch 80, val loss: 1.771267294883728
Epoch 90, training loss: 1.7875046730041504 = 1.709003210067749 + 0.01 * 7.850141525268555
Epoch 90, val loss: 1.7308101654052734
Epoch 100, training loss: 1.720703363418579 = 1.6452605724334717 + 0.01 * 7.544276237487793
Epoch 100, val loss: 1.6772736310958862
Epoch 110, training loss: 1.6355183124542236 = 1.5617495775222778 + 0.01 * 7.376873016357422
Epoch 110, val loss: 1.6074546575546265
Epoch 120, training loss: 1.5412423610687256 = 1.468100905418396 + 0.01 * 7.314144134521484
Epoch 120, val loss: 1.5311598777770996
Epoch 130, training loss: 1.4490829706192017 = 1.3761101961135864 + 0.01 * 7.297280311584473
Epoch 130, val loss: 1.4584540128707886
Epoch 140, training loss: 1.363124132156372 = 1.2902737855911255 + 0.01 * 7.28503942489624
Epoch 140, val loss: 1.3942797183990479
Epoch 150, training loss: 1.2824450731277466 = 1.2097235918045044 + 0.01 * 7.272145748138428
Epoch 150, val loss: 1.3365508317947388
Epoch 160, training loss: 1.204204797744751 = 1.131649374961853 + 0.01 * 7.255542755126953
Epoch 160, val loss: 1.2820760011672974
Epoch 170, training loss: 1.125205397605896 = 1.0528699159622192 + 0.01 * 7.2335524559021
Epoch 170, val loss: 1.2266840934753418
Epoch 180, training loss: 1.0442745685577393 = 0.9721176624298096 + 0.01 * 7.21569299697876
Epoch 180, val loss: 1.1690737009048462
Epoch 190, training loss: 0.9632024765014648 = 0.8912194967269897 + 0.01 * 7.198297023773193
Epoch 190, val loss: 1.110546588897705
Epoch 200, training loss: 0.885662317276001 = 0.8138094544410706 + 0.01 * 7.185287952423096
Epoch 200, val loss: 1.0546324253082275
Epoch 210, training loss: 0.8146480917930603 = 0.7428755760192871 + 0.01 * 7.177253723144531
Epoch 210, val loss: 1.0047436952590942
Epoch 220, training loss: 0.7510193586349487 = 0.6793052554130554 + 0.01 * 7.171411991119385
Epoch 220, val loss: 0.9625471234321594
Epoch 230, training loss: 0.6937593817710876 = 0.6221305131912231 + 0.01 * 7.16288948059082
Epoch 230, val loss: 0.9276543855667114
Epoch 240, training loss: 0.6413213610649109 = 0.5697546005249023 + 0.01 * 7.156673908233643
Epoch 240, val loss: 0.8990459442138672
Epoch 250, training loss: 0.5925194025039673 = 0.5209786295890808 + 0.01 * 7.154080390930176
Epoch 250, val loss: 0.8760762214660645
Epoch 260, training loss: 0.5466009378433228 = 0.4751647710800171 + 0.01 * 7.143617153167725
Epoch 260, val loss: 0.8580716848373413
Epoch 270, training loss: 0.5031679272651672 = 0.4317742884159088 + 0.01 * 7.1393632888793945
Epoch 270, val loss: 0.8444936275482178
Epoch 280, training loss: 0.46164408326148987 = 0.39031314849853516 + 0.01 * 7.13309383392334
Epoch 280, val loss: 0.8346911668777466
Epoch 290, training loss: 0.42166054248809814 = 0.350411057472229 + 0.01 * 7.124948501586914
Epoch 290, val loss: 0.8279412984848022
Epoch 300, training loss: 0.38350677490234375 = 0.3122561275959015 + 0.01 * 7.125063419342041
Epoch 300, val loss: 0.8239657878875732
Epoch 310, training loss: 0.347617506980896 = 0.27646246552467346 + 0.01 * 7.115503787994385
Epoch 310, val loss: 0.8229859471321106
Epoch 320, training loss: 0.31467247009277344 = 0.24362456798553467 + 0.01 * 7.10479211807251
Epoch 320, val loss: 0.8252043128013611
Epoch 330, training loss: 0.285261869430542 = 0.21411818265914917 + 0.01 * 7.114367961883545
Epoch 330, val loss: 0.8306193947792053
Epoch 340, training loss: 0.25889894366264343 = 0.18794406950473785 + 0.01 * 7.095487117767334
Epoch 340, val loss: 0.8390232920646667
Epoch 350, training loss: 0.23573902249336243 = 0.1648050844669342 + 0.01 * 7.093392848968506
Epoch 350, val loss: 0.8501263856887817
Epoch 360, training loss: 0.2153107076883316 = 0.144445538520813 + 0.01 * 7.086517333984375
Epoch 360, val loss: 0.8635790348052979
Epoch 370, training loss: 0.19733324646949768 = 0.12659558653831482 + 0.01 * 7.073767185211182
Epoch 370, val loss: 0.8788705468177795
Epoch 380, training loss: 0.181668221950531 = 0.11097737401723862 + 0.01 * 7.069084644317627
Epoch 380, val loss: 0.8956908583641052
Epoch 390, training loss: 0.16797152161598206 = 0.09737849980592728 + 0.01 * 7.059301376342773
Epoch 390, val loss: 0.9137459397315979
Epoch 400, training loss: 0.15610608458518982 = 0.08557112514972687 + 0.01 * 7.053496360778809
Epoch 400, val loss: 0.9327077269554138
Epoch 410, training loss: 0.14574116468429565 = 0.07534147053956985 + 0.01 * 7.039969444274902
Epoch 410, val loss: 0.9523553848266602
Epoch 420, training loss: 0.13759607076644897 = 0.06649776548147202 + 0.01 * 7.1098313331604
Epoch 420, val loss: 0.9725061655044556
Epoch 430, training loss: 0.12934692203998566 = 0.0589069165289402 + 0.01 * 7.044000148773193
Epoch 430, val loss: 0.9926804304122925
Epoch 440, training loss: 0.12257222086191177 = 0.05237802863121033 + 0.01 * 7.019419193267822
Epoch 440, val loss: 1.0128533840179443
Epoch 450, training loss: 0.11687654256820679 = 0.046752698719501495 + 0.01 * 7.01238489151001
Epoch 450, val loss: 1.0329099893569946
Epoch 460, training loss: 0.11232955008745193 = 0.04189998656511307 + 0.01 * 7.042956352233887
Epoch 460, val loss: 1.0526899099349976
Epoch 470, training loss: 0.10770376026630402 = 0.03771422058343887 + 0.01 * 6.998953819274902
Epoch 470, val loss: 1.0719749927520752
Epoch 480, training loss: 0.1040678322315216 = 0.03408876433968544 + 0.01 * 6.99790620803833
Epoch 480, val loss: 1.0907883644104004
Epoch 490, training loss: 0.10081017762422562 = 0.0309374388307333 + 0.01 * 6.987273693084717
Epoch 490, val loss: 1.1091004610061646
Epoch 500, training loss: 0.09813054651021957 = 0.02818932943046093 + 0.01 * 6.994121551513672
Epoch 500, val loss: 1.1268304586410522
Epoch 510, training loss: 0.0956125259399414 = 0.025786207988858223 + 0.01 * 6.982632160186768
Epoch 510, val loss: 1.1439658403396606
Epoch 520, training loss: 0.09345422685146332 = 0.02367469295859337 + 0.01 * 6.97795295715332
Epoch 520, val loss: 1.1604934930801392
Epoch 530, training loss: 0.09145815670490265 = 0.02181335911154747 + 0.01 * 6.964479923248291
Epoch 530, val loss: 1.1764965057373047
Epoch 540, training loss: 0.09023935347795486 = 0.020166335627436638 + 0.01 * 7.0073018074035645
Epoch 540, val loss: 1.1919307708740234
Epoch 550, training loss: 0.08822551369667053 = 0.01870567724108696 + 0.01 * 6.95198392868042
Epoch 550, val loss: 1.2067248821258545
Epoch 560, training loss: 0.0869390070438385 = 0.01740287058055401 + 0.01 * 6.953613758087158
Epoch 560, val loss: 1.2210301160812378
Epoch 570, training loss: 0.0856667011976242 = 0.016235681250691414 + 0.01 * 6.9431023597717285
Epoch 570, val loss: 1.2348889112472534
Epoch 580, training loss: 0.08455051481723785 = 0.015186140313744545 + 0.01 * 6.936438083648682
Epoch 580, val loss: 1.2482969760894775
Epoch 590, training loss: 0.08381795883178711 = 0.014239861629903316 + 0.01 * 6.957809925079346
Epoch 590, val loss: 1.2613211870193481
Epoch 600, training loss: 0.08270499110221863 = 0.013385176658630371 + 0.01 * 6.931981563568115
Epoch 600, val loss: 1.2737377882003784
Epoch 610, training loss: 0.08186015486717224 = 0.012609951198101044 + 0.01 * 6.925020694732666
Epoch 610, val loss: 1.2858487367630005
Epoch 620, training loss: 0.08126192539930344 = 0.01190398633480072 + 0.01 * 6.935794353485107
Epoch 620, val loss: 1.2975784540176392
Epoch 630, training loss: 0.08050262928009033 = 0.01126088760793209 + 0.01 * 6.9241743087768555
Epoch 630, val loss: 1.3089348077774048
Epoch 640, training loss: 0.07978110015392303 = 0.010672206990420818 + 0.01 * 6.910889625549316
Epoch 640, val loss: 1.319854497909546
Epoch 650, training loss: 0.07934174686670303 = 0.010132210329174995 + 0.01 * 6.920953750610352
Epoch 650, val loss: 1.3305566310882568
Epoch 660, training loss: 0.0787377655506134 = 0.009635836817324162 + 0.01 * 6.91019344329834
Epoch 660, val loss: 1.3408530950546265
Epoch 670, training loss: 0.07816806435585022 = 0.009178299456834793 + 0.01 * 6.8989763259887695
Epoch 670, val loss: 1.3508920669555664
Epoch 680, training loss: 0.07787058502435684 = 0.00875527411699295 + 0.01 * 6.9115309715271
Epoch 680, val loss: 1.3606513738632202
Epoch 690, training loss: 0.07725024968385696 = 0.008363849483430386 + 0.01 * 6.8886399269104
Epoch 690, val loss: 1.370065689086914
Epoch 700, training loss: 0.0770568922162056 = 0.008001402951776981 + 0.01 * 6.9055495262146
Epoch 700, val loss: 1.3792922496795654
Epoch 710, training loss: 0.07655051350593567 = 0.007664746604859829 + 0.01 * 6.888576507568359
Epoch 710, val loss: 1.3880892992019653
Epoch 720, training loss: 0.076218381524086 = 0.007351037580519915 + 0.01 * 6.886734485626221
Epoch 720, val loss: 1.3967565298080444
Epoch 730, training loss: 0.07583500444889069 = 0.007058970630168915 + 0.01 * 6.877603530883789
Epoch 730, val loss: 1.405135154724121
Epoch 740, training loss: 0.07564151287078857 = 0.0067860400304198265 + 0.01 * 6.885547161102295
Epoch 740, val loss: 1.4133143424987793
Epoch 750, training loss: 0.07522162795066833 = 0.006530308164656162 + 0.01 * 6.8691325187683105
Epoch 750, val loss: 1.4212231636047363
Epoch 760, training loss: 0.0748986005783081 = 0.006290554068982601 + 0.01 * 6.860804557800293
Epoch 760, val loss: 1.4290190935134888
Epoch 770, training loss: 0.07496769726276398 = 0.006066039204597473 + 0.01 * 6.890166282653809
Epoch 770, val loss: 1.436511516571045
Epoch 780, training loss: 0.0744539350271225 = 0.005855211056768894 + 0.01 * 6.859872341156006
Epoch 780, val loss: 1.443769097328186
Epoch 790, training loss: 0.07417134940624237 = 0.005656793713569641 + 0.01 * 6.8514556884765625
Epoch 790, val loss: 1.450945496559143
Epoch 800, training loss: 0.07409103214740753 = 0.005469672381877899 + 0.01 * 6.862135887145996
Epoch 800, val loss: 1.4578405618667603
Epoch 810, training loss: 0.07397794723510742 = 0.005293236579746008 + 0.01 * 6.868471145629883
Epoch 810, val loss: 1.4646469354629517
Epoch 820, training loss: 0.0735868588089943 = 0.00512674730271101 + 0.01 * 6.846011638641357
Epoch 820, val loss: 1.471205711364746
Epoch 830, training loss: 0.07360299676656723 = 0.004969140514731407 + 0.01 * 6.8633856773376465
Epoch 830, val loss: 1.4776666164398193
Epoch 840, training loss: 0.07319316267967224 = 0.004820322152227163 + 0.01 * 6.837284088134766
Epoch 840, val loss: 1.4839096069335938
Epoch 850, training loss: 0.07337212562561035 = 0.004679226316511631 + 0.01 * 6.869290351867676
Epoch 850, val loss: 1.4900420904159546
Epoch 860, training loss: 0.07286860793828964 = 0.004545276518911123 + 0.01 * 6.832333087921143
Epoch 860, val loss: 1.4959373474121094
Epoch 870, training loss: 0.07292094081640244 = 0.004418131895363331 + 0.01 * 6.85028076171875
Epoch 870, val loss: 1.5018638372421265
Epoch 880, training loss: 0.07265525311231613 = 0.004297602456063032 + 0.01 * 6.835765838623047
Epoch 880, val loss: 1.507409691810608
Epoch 890, training loss: 0.0725623220205307 = 0.004182738251984119 + 0.01 * 6.837958335876465
Epoch 890, val loss: 1.5130456686019897
Epoch 900, training loss: 0.0722818523645401 = 0.004073337186127901 + 0.01 * 6.820851802825928
Epoch 900, val loss: 1.518363356590271
Epoch 910, training loss: 0.07210353016853333 = 0.003969196230173111 + 0.01 * 6.8134331703186035
Epoch 910, val loss: 1.5237632989883423
Epoch 920, training loss: 0.07206816971302032 = 0.0038698138669133186 + 0.01 * 6.819835662841797
Epoch 920, val loss: 1.5288481712341309
Epoch 930, training loss: 0.0719653069972992 = 0.0037750990595668554 + 0.01 * 6.819020748138428
Epoch 930, val loss: 1.533876895904541
Epoch 940, training loss: 0.07183221727609634 = 0.0036845074500888586 + 0.01 * 6.8147711753845215
Epoch 940, val loss: 1.5387779474258423
Epoch 950, training loss: 0.07188015431165695 = 0.003598134033381939 + 0.01 * 6.828202247619629
Epoch 950, val loss: 1.5436015129089355
Epoch 960, training loss: 0.07147227227687836 = 0.0035152609925717115 + 0.01 * 6.795701503753662
Epoch 960, val loss: 1.5482655763626099
Epoch 970, training loss: 0.07149722427129745 = 0.0034359360579401255 + 0.01 * 6.80612850189209
Epoch 970, val loss: 1.5529215335845947
Epoch 980, training loss: 0.07129453867673874 = 0.003360058180987835 + 0.01 * 6.793448448181152
Epoch 980, val loss: 1.5572938919067383
Epoch 990, training loss: 0.07130370289087296 = 0.003287320723757148 + 0.01 * 6.801638126373291
Epoch 990, val loss: 1.5618102550506592
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 2.0194003582000732 = 1.9334315061569214 + 0.01 * 8.596884727478027
Epoch 0, val loss: 1.9400748014450073
Epoch 10, training loss: 2.00954008102417 = 1.9235718250274658 + 0.01 * 8.59682846069336
Epoch 10, val loss: 1.9296716451644897
Epoch 20, training loss: 1.9974533319473267 = 1.9114866256713867 + 0.01 * 8.596665382385254
Epoch 20, val loss: 1.9168061017990112
Epoch 30, training loss: 1.9806647300720215 = 1.8947033882141113 + 0.01 * 8.596138954162598
Epoch 30, val loss: 1.8990225791931152
Epoch 40, training loss: 1.9563143253326416 = 1.8703796863555908 + 0.01 * 8.593459129333496
Epoch 40, val loss: 1.873834252357483
Epoch 50, training loss: 1.9236435890197754 = 1.8378952741622925 + 0.01 * 8.574837684631348
Epoch 50, val loss: 1.8419764041900635
Epoch 60, training loss: 1.8890897035598755 = 1.8042423725128174 + 0.01 * 8.48473072052002
Epoch 60, val loss: 1.8123337030410767
Epoch 70, training loss: 1.8582688570022583 = 1.774912714958191 + 0.01 * 8.33561897277832
Epoch 70, val loss: 1.7872531414031982
Epoch 80, training loss: 1.8172659873962402 = 1.7352968454360962 + 0.01 * 8.196917533874512
Epoch 80, val loss: 1.7513974905014038
Epoch 90, training loss: 1.758399248123169 = 1.6795449256896973 + 0.01 * 7.885434150695801
Epoch 90, val loss: 1.703629732131958
Epoch 100, training loss: 1.679066777229309 = 1.6028556823730469 + 0.01 * 7.6211042404174805
Epoch 100, val loss: 1.640342354774475
Epoch 110, training loss: 1.5840095281600952 = 1.5089683532714844 + 0.01 * 7.504115104675293
Epoch 110, val loss: 1.5636451244354248
Epoch 120, training loss: 1.4798718690872192 = 1.405434250831604 + 0.01 * 7.443764686584473
Epoch 120, val loss: 1.4799718856811523
Epoch 130, training loss: 1.3717056512832642 = 1.297595500946045 + 0.01 * 7.41101598739624
Epoch 130, val loss: 1.3949477672576904
Epoch 140, training loss: 1.2625291347503662 = 1.1886063814163208 + 0.01 * 7.3922810554504395
Epoch 140, val loss: 1.3115445375442505
Epoch 150, training loss: 1.155989170074463 = 1.0821259021759033 + 0.01 * 7.386323928833008
Epoch 150, val loss: 1.2326099872589111
Epoch 160, training loss: 1.0560616254806519 = 0.9822019934654236 + 0.01 * 7.385964393615723
Epoch 160, val loss: 1.160487413406372
Epoch 170, training loss: 0.9656832218170166 = 0.891830325126648 + 0.01 * 7.385287761688232
Epoch 170, val loss: 1.0967319011688232
Epoch 180, training loss: 0.8867791891098022 = 0.812949538230896 + 0.01 * 7.38296365737915
Epoch 180, val loss: 1.0424633026123047
Epoch 190, training loss: 0.8198861479759216 = 0.7460905313491821 + 0.01 * 7.379561901092529
Epoch 190, val loss: 0.9984583854675293
Epoch 200, training loss: 0.7639154195785522 = 0.6901689767837524 + 0.01 * 7.374645233154297
Epoch 200, val loss: 0.9640346765518188
Epoch 210, training loss: 0.7159796953201294 = 0.6423004865646362 + 0.01 * 7.367922782897949
Epoch 210, val loss: 0.9368368983268738
Epoch 220, training loss: 0.6723406910896301 = 0.5987496972084045 + 0.01 * 7.359101295471191
Epoch 220, val loss: 0.9136469960212708
Epoch 230, training loss: 0.629562497138977 = 0.5560712218284607 + 0.01 * 7.349128723144531
Epoch 230, val loss: 0.8916876912117004
Epoch 240, training loss: 0.5851749181747437 = 0.5118088722229004 + 0.01 * 7.336603164672852
Epoch 240, val loss: 0.8693862557411194
Epoch 250, training loss: 0.5383132696151733 = 0.4650806784629822 + 0.01 * 7.323259353637695
Epoch 250, val loss: 0.846878707408905
Epoch 260, training loss: 0.4900575280189514 = 0.41696053743362427 + 0.01 * 7.309698581695557
Epoch 260, val loss: 0.8252984881401062
Epoch 270, training loss: 0.44268208742141724 = 0.3697785437107086 + 0.01 * 7.290353298187256
Epoch 270, val loss: 0.8066473007202148
Epoch 280, training loss: 0.3986491858959198 = 0.32592180371284485 + 0.01 * 7.272739410400391
Epoch 280, val loss: 0.7925072312355042
Epoch 290, training loss: 0.3593270778656006 = 0.28670233488082886 + 0.01 * 7.262474536895752
Epoch 290, val loss: 0.7835204601287842
Epoch 300, training loss: 0.32435810565948486 = 0.25201812386512756 + 0.01 * 7.233997821807861
Epoch 300, val loss: 0.7791840434074402
Epoch 310, training loss: 0.293632447719574 = 0.2211531698703766 + 0.01 * 7.247927188873291
Epoch 310, val loss: 0.7785775661468506
Epoch 320, training loss: 0.26554203033447266 = 0.19351692497730255 + 0.01 * 7.202511787414551
Epoch 320, val loss: 0.7810496687889099
Epoch 330, training loss: 0.2406671941280365 = 0.16876661777496338 + 0.01 * 7.19005823135376
Epoch 330, val loss: 0.7862027883529663
Epoch 340, training loss: 0.21844714879989624 = 0.14680367708206177 + 0.01 * 7.164346694946289
Epoch 340, val loss: 0.7938367128372192
Epoch 350, training loss: 0.19925200939178467 = 0.12754939496517181 + 0.01 * 7.170261383056641
Epoch 350, val loss: 0.8038553595542908
Epoch 360, training loss: 0.18228736519813538 = 0.11089766025543213 + 0.01 * 7.138970375061035
Epoch 360, val loss: 0.8158285021781921
Epoch 370, training loss: 0.16792242228984833 = 0.09659646451473236 + 0.01 * 7.132596015930176
Epoch 370, val loss: 0.829575777053833
Epoch 380, training loss: 0.15585914254188538 = 0.0843878984451294 + 0.01 * 7.147124767303467
Epoch 380, val loss: 0.8447489738464355
Epoch 390, training loss: 0.14513328671455383 = 0.0740068331360817 + 0.01 * 7.112646579742432
Epoch 390, val loss: 0.8608328104019165
Epoch 400, training loss: 0.1363464593887329 = 0.06516750901937485 + 0.01 * 7.117895126342773
Epoch 400, val loss: 0.8776161670684814
Epoch 410, training loss: 0.12848058342933655 = 0.057636797428131104 + 0.01 * 7.084378242492676
Epoch 410, val loss: 0.8948184251785278
Epoch 420, training loss: 0.12193972617387772 = 0.05119287967681885 + 0.01 * 7.074685096740723
Epoch 420, val loss: 0.9121725559234619
Epoch 430, training loss: 0.11643369495868683 = 0.04565979167819023 + 0.01 * 7.077390670776367
Epoch 430, val loss: 0.929506242275238
Epoch 440, training loss: 0.11156964302062988 = 0.04089730605483055 + 0.01 * 7.067234039306641
Epoch 440, val loss: 0.9467018842697144
Epoch 450, training loss: 0.10737121850252151 = 0.036783367395401 + 0.01 * 7.058785438537598
Epoch 450, val loss: 0.963622510433197
Epoch 460, training loss: 0.10372300446033478 = 0.033219486474990845 + 0.01 * 7.050352096557617
Epoch 460, val loss: 0.9802229404449463
Epoch 470, training loss: 0.1005467027425766 = 0.0301225446164608 + 0.01 * 7.042416095733643
Epoch 470, val loss: 0.9964112639427185
Epoch 480, training loss: 0.09781377017498016 = 0.027419865131378174 + 0.01 * 7.039390563964844
Epoch 480, val loss: 1.0121809244155884
Epoch 490, training loss: 0.09536969661712646 = 0.0250542052090168 + 0.01 * 7.03154993057251
Epoch 490, val loss: 1.0274749994277954
Epoch 500, training loss: 0.09336857497692108 = 0.022975759580731392 + 0.01 * 7.039281845092773
Epoch 500, val loss: 1.04233717918396
Epoch 510, training loss: 0.09141072630882263 = 0.021145453676581383 + 0.01 * 7.026526927947998
Epoch 510, val loss: 1.0566540956497192
Epoch 520, training loss: 0.0897025316953659 = 0.019526392221450806 + 0.01 * 7.017613887786865
Epoch 520, val loss: 1.0705374479293823
Epoch 530, training loss: 0.08825349062681198 = 0.01808815449476242 + 0.01 * 7.016533851623535
Epoch 530, val loss: 1.0839612483978271
Epoch 540, training loss: 0.08679334819316864 = 0.01680653542280197 + 0.01 * 6.998681545257568
Epoch 540, val loss: 1.0969327688217163
Epoch 550, training loss: 0.08560529351234436 = 0.015659775584936142 + 0.01 * 6.994551658630371
Epoch 550, val loss: 1.109492540359497
Epoch 560, training loss: 0.08453965932130814 = 0.014630443416535854 + 0.01 * 6.990921497344971
Epoch 560, val loss: 1.121625542640686
Epoch 570, training loss: 0.08357136696577072 = 0.01370391994714737 + 0.01 * 6.9867448806762695
Epoch 570, val loss: 1.1333659887313843
Epoch 580, training loss: 0.08260215818881989 = 0.01286681741476059 + 0.01 * 6.97353458404541
Epoch 580, val loss: 1.1446925401687622
Epoch 590, training loss: 0.08211921155452728 = 0.01210741326212883 + 0.01 * 7.0011796951293945
Epoch 590, val loss: 1.1556841135025024
Epoch 600, training loss: 0.08114031702280045 = 0.011419053189456463 + 0.01 * 6.972126483917236
Epoch 600, val loss: 1.1662662029266357
Epoch 610, training loss: 0.0804656445980072 = 0.010791780427098274 + 0.01 * 6.967386722564697
Epoch 610, val loss: 1.1765518188476562
Epoch 620, training loss: 0.07981321960687637 = 0.010217812843620777 + 0.01 * 6.959540843963623
Epoch 620, val loss: 1.1865026950836182
Epoch 630, training loss: 0.07939251512289047 = 0.009692561812698841 + 0.01 * 6.969995498657227
Epoch 630, val loss: 1.1961416006088257
Epoch 640, training loss: 0.07867317646741867 = 0.009210258722305298 + 0.01 * 6.946291923522949
Epoch 640, val loss: 1.205500602722168
Epoch 650, training loss: 0.07841438800096512 = 0.008766229264438152 + 0.01 * 6.964816093444824
Epoch 650, val loss: 1.2145440578460693
Epoch 660, training loss: 0.07774852216243744 = 0.008357109501957893 + 0.01 * 6.939141273498535
Epoch 660, val loss: 1.2233988046646118
Epoch 670, training loss: 0.07731972634792328 = 0.007978834211826324 + 0.01 * 6.934089183807373
Epoch 670, val loss: 1.2319214344024658
Epoch 680, training loss: 0.0768919587135315 = 0.007628211751580238 + 0.01 * 6.926374912261963
Epoch 680, val loss: 1.240220069885254
Epoch 690, training loss: 0.076480433344841 = 0.007302418816834688 + 0.01 * 6.917801380157471
Epoch 690, val loss: 1.2483313083648682
Epoch 700, training loss: 0.07642725855112076 = 0.006999400444328785 + 0.01 * 6.942785739898682
Epoch 700, val loss: 1.2562167644500732
Epoch 710, training loss: 0.07576388865709305 = 0.0067173028364777565 + 0.01 * 6.904658794403076
Epoch 710, val loss: 1.2638050317764282
Epoch 720, training loss: 0.0755835473537445 = 0.006454147398471832 + 0.01 * 6.91294002532959
Epoch 720, val loss: 1.2711868286132812
Epoch 730, training loss: 0.075226329267025 = 0.0062084500677883625 + 0.01 * 6.901788234710693
Epoch 730, val loss: 1.278443455696106
Epoch 740, training loss: 0.07500378787517548 = 0.005978085100650787 + 0.01 * 6.9025702476501465
Epoch 740, val loss: 1.2854512929916382
Epoch 750, training loss: 0.07479608803987503 = 0.005762348882853985 + 0.01 * 6.903374195098877
Epoch 750, val loss: 1.292248249053955
Epoch 760, training loss: 0.07437583804130554 = 0.005559489596635103 + 0.01 * 6.881634712219238
Epoch 760, val loss: 1.2989492416381836
Epoch 770, training loss: 0.07447833567857742 = 0.005368811544030905 + 0.01 * 6.910952568054199
Epoch 770, val loss: 1.3054757118225098
Epoch 780, training loss: 0.07399855554103851 = 0.0051895915530622005 + 0.01 * 6.88089656829834
Epoch 780, val loss: 1.3117749691009521
Epoch 790, training loss: 0.07374955713748932 = 0.005020807962864637 + 0.01 * 6.872875690460205
Epoch 790, val loss: 1.3178831338882446
Epoch 800, training loss: 0.07362218201160431 = 0.0048611583188176155 + 0.01 * 6.876101970672607
Epoch 800, val loss: 1.323920488357544
Epoch 810, training loss: 0.07331082224845886 = 0.004710355773568153 + 0.01 * 6.860046863555908
Epoch 810, val loss: 1.329795479774475
Epoch 820, training loss: 0.07325586676597595 = 0.004567696247249842 + 0.01 * 6.868817329406738
Epoch 820, val loss: 1.335554838180542
Epoch 830, training loss: 0.07298129051923752 = 0.004432629328221083 + 0.01 * 6.8548665046691895
Epoch 830, val loss: 1.341148853302002
Epoch 840, training loss: 0.0729975476861 = 0.00430427398532629 + 0.01 * 6.869327068328857
Epoch 840, val loss: 1.346613883972168
Epoch 850, training loss: 0.07276061177253723 = 0.0041825738735497 + 0.01 * 6.857803821563721
Epoch 850, val loss: 1.3519771099090576
Epoch 860, training loss: 0.07265985757112503 = 0.004066753666847944 + 0.01 * 6.859310626983643
Epoch 860, val loss: 1.357191801071167
Epoch 870, training loss: 0.07235009223222733 = 0.003956835251301527 + 0.01 * 6.839325904846191
Epoch 870, val loss: 1.3623433113098145
Epoch 880, training loss: 0.07223107665777206 = 0.003852182300761342 + 0.01 * 6.837889671325684
Epoch 880, val loss: 1.3673466444015503
Epoch 890, training loss: 0.07218264788389206 = 0.003752396907657385 + 0.01 * 6.8430256843566895
Epoch 890, val loss: 1.3722094297409058
Epoch 900, training loss: 0.07204973697662354 = 0.003657412715256214 + 0.01 * 6.839231967926025
Epoch 900, val loss: 1.3769876956939697
Epoch 910, training loss: 0.07188157737255096 = 0.0035665645264089108 + 0.01 * 6.831501483917236
Epoch 910, val loss: 1.3817168474197388
Epoch 920, training loss: 0.07172466814517975 = 0.003479969222098589 + 0.01 * 6.824469566345215
Epoch 920, val loss: 1.3862837553024292
Epoch 930, training loss: 0.07186587154865265 = 0.003397073596715927 + 0.01 * 6.846879482269287
Epoch 930, val loss: 1.3907493352890015
Epoch 940, training loss: 0.07165895402431488 = 0.0033178552985191345 + 0.01 * 6.834109783172607
Epoch 940, val loss: 1.3952093124389648
Epoch 950, training loss: 0.07153616845607758 = 0.0032420039642602205 + 0.01 * 6.8294172286987305
Epoch 950, val loss: 1.399468183517456
Epoch 960, training loss: 0.07129348814487457 = 0.0031694681383669376 + 0.01 * 6.81240177154541
Epoch 960, val loss: 1.403709888458252
Epoch 970, training loss: 0.07124987989664078 = 0.003099696943536401 + 0.01 * 6.815018653869629
Epoch 970, val loss: 1.407814383506775
Epoch 980, training loss: 0.07109923660755157 = 0.003033023327589035 + 0.01 * 6.806621551513672
Epoch 980, val loss: 1.4118982553482056
Epoch 990, training loss: 0.0712566152215004 = 0.0029687657952308655 + 0.01 * 6.828785419464111
Epoch 990, val loss: 1.415860652923584
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8202424881391671
The final CL Acc:0.78148, 0.00800, The final GNN Acc:0.81761, 0.00188
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13164])
remove edge: torch.Size([2, 7816])
updated graph: torch.Size([2, 10424])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.029892683029175 = 1.9439241886138916 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9411139488220215
Epoch 10, training loss: 2.019930124282837 = 1.9339619874954224 + 0.01 * 8.596817970275879
Epoch 10, val loss: 1.9311410188674927
Epoch 20, training loss: 2.0082008838653564 = 1.9222346544265747 + 0.01 * 8.59661865234375
Epoch 20, val loss: 1.9192839860916138
Epoch 30, training loss: 1.9922022819519043 = 1.9062427282333374 + 0.01 * 8.595958709716797
Epoch 30, val loss: 1.9032403230667114
Epoch 40, training loss: 1.9689586162567139 = 1.8830355405807495 + 0.01 * 8.59231185913086
Epoch 40, val loss: 1.8802680969238281
Epoch 50, training loss: 1.9355798959732056 = 1.849928855895996 + 0.01 * 8.565102577209473
Epoch 50, val loss: 1.8486396074295044
Epoch 60, training loss: 1.8937408924102783 = 1.80946946144104 + 0.01 * 8.42714786529541
Epoch 60, val loss: 1.8126503229141235
Epoch 70, training loss: 1.8539131879806519 = 1.7711925506591797 + 0.01 * 8.272063255310059
Epoch 70, val loss: 1.7803937196731567
Epoch 80, training loss: 1.8089051246643066 = 1.7292133569717407 + 0.01 * 7.969172477722168
Epoch 80, val loss: 1.7401587963104248
Epoch 90, training loss: 1.7466132640838623 = 1.6703177690505981 + 0.01 * 7.629555702209473
Epoch 90, val loss: 1.6850395202636719
Epoch 100, training loss: 1.666523814201355 = 1.591789960861206 + 0.01 * 7.473381519317627
Epoch 100, val loss: 1.6167900562286377
Epoch 110, training loss: 1.5690281391143799 = 1.4950940608978271 + 0.01 * 7.393408298492432
Epoch 110, val loss: 1.532945990562439
Epoch 120, training loss: 1.4622243642807007 = 1.3888311386108398 + 0.01 * 7.3393235206604
Epoch 120, val loss: 1.441069483757019
Epoch 130, training loss: 1.3542605638504028 = 1.281234622001648 + 0.01 * 7.302589416503906
Epoch 130, val loss: 1.3495527505874634
Epoch 140, training loss: 1.2504041194915771 = 1.1775624752044678 + 0.01 * 7.284169673919678
Epoch 140, val loss: 1.2626113891601562
Epoch 150, training loss: 1.153214693069458 = 1.0804671049118042 + 0.01 * 7.274757385253906
Epoch 150, val loss: 1.1828012466430664
Epoch 160, training loss: 1.06355881690979 = 0.9909022450447083 + 0.01 * 7.265656471252441
Epoch 160, val loss: 1.1108194589614868
Epoch 170, training loss: 0.9809409976005554 = 0.9084155559539795 + 0.01 * 7.2525434494018555
Epoch 170, val loss: 1.0459009408950806
Epoch 180, training loss: 0.9039890766143799 = 0.8316634893417358 + 0.01 * 7.23255729675293
Epoch 180, val loss: 0.9868391156196594
Epoch 190, training loss: 0.8316401839256287 = 0.7595111727714539 + 0.01 * 7.2129034996032715
Epoch 190, val loss: 0.9322229623794556
Epoch 200, training loss: 0.7633427381515503 = 0.6914477348327637 + 0.01 * 7.1895036697387695
Epoch 200, val loss: 0.8821486234664917
Epoch 210, training loss: 0.6989226937294006 = 0.6272610425949097 + 0.01 * 7.16616678237915
Epoch 210, val loss: 0.8361969590187073
Epoch 220, training loss: 0.6382615566253662 = 0.5667334198951721 + 0.01 * 7.1528167724609375
Epoch 220, val loss: 0.7950666546821594
Epoch 230, training loss: 0.5811312794685364 = 0.5096523761749268 + 0.01 * 7.147890567779541
Epoch 230, val loss: 0.758845329284668
Epoch 240, training loss: 0.5272682905197144 = 0.45589128136634827 + 0.01 * 7.137701034545898
Epoch 240, val loss: 0.7275055050849915
Epoch 250, training loss: 0.4767191410064697 = 0.40537071228027344 + 0.01 * 7.134842872619629
Epoch 250, val loss: 0.7010799050331116
Epoch 260, training loss: 0.4292316436767578 = 0.3579135537147522 + 0.01 * 7.131809711456299
Epoch 260, val loss: 0.6786928772926331
Epoch 270, training loss: 0.3847731351852417 = 0.31349772214889526 + 0.01 * 7.127542972564697
Epoch 270, val loss: 0.6598021984100342
Epoch 280, training loss: 0.34366127848625183 = 0.2724219560623169 + 0.01 * 7.1239333152771
Epoch 280, val loss: 0.6440674662590027
Epoch 290, training loss: 0.3064146935939789 = 0.235194593667984 + 0.01 * 7.122009754180908
Epoch 290, val loss: 0.6317224502563477
Epoch 300, training loss: 0.27365028858184814 = 0.20224148035049438 + 0.01 * 7.140881538391113
Epoch 300, val loss: 0.6228305697441101
Epoch 310, training loss: 0.24492880702018738 = 0.17372025549411774 + 0.01 * 7.120856285095215
Epoch 310, val loss: 0.6175852417945862
Epoch 320, training loss: 0.22046278417110443 = 0.14935608208179474 + 0.01 * 7.11067008972168
Epoch 320, val loss: 0.6157762408256531
Epoch 330, training loss: 0.1999175250530243 = 0.12870195508003235 + 0.01 * 7.12155818939209
Epoch 330, val loss: 0.617121696472168
Epoch 340, training loss: 0.18227005004882812 = 0.11125665158033371 + 0.01 * 7.101340293884277
Epoch 340, val loss: 0.6212449073791504
Epoch 350, training loss: 0.16763150691986084 = 0.09653168171644211 + 0.01 * 7.109982967376709
Epoch 350, val loss: 0.6276437640190125
Epoch 360, training loss: 0.1551916003227234 = 0.08414106070995331 + 0.01 * 7.1050543785095215
Epoch 360, val loss: 0.6358678340911865
Epoch 370, training loss: 0.14451467990875244 = 0.07368437200784683 + 0.01 * 7.083030700683594
Epoch 370, val loss: 0.6454470753669739
Epoch 380, training loss: 0.13592740893363953 = 0.06482980400323868 + 0.01 * 7.1097612380981445
Epoch 380, val loss: 0.6561051607131958
Epoch 390, training loss: 0.1279531568288803 = 0.0573209784924984 + 0.01 * 7.063218116760254
Epoch 390, val loss: 0.6674883961677551
Epoch 400, training loss: 0.121517613530159 = 0.05092870444059372 + 0.01 * 7.0588908195495605
Epoch 400, val loss: 0.6792764067649841
Epoch 410, training loss: 0.11675668507814407 = 0.04546510428190231 + 0.01 * 7.129158020019531
Epoch 410, val loss: 0.691256046295166
Epoch 420, training loss: 0.11127451062202454 = 0.04079166799783707 + 0.01 * 7.04828405380249
Epoch 420, val loss: 0.7032808661460876
Epoch 430, training loss: 0.10716291517019272 = 0.03676842898130417 + 0.01 * 7.0394487380981445
Epoch 430, val loss: 0.7151364088058472
Epoch 440, training loss: 0.10352542996406555 = 0.03328513726592064 + 0.01 * 7.024028778076172
Epoch 440, val loss: 0.7268402576446533
Epoch 450, training loss: 0.10042551904916763 = 0.030259257182478905 + 0.01 * 7.016625881195068
Epoch 450, val loss: 0.7382410764694214
Epoch 460, training loss: 0.09776919335126877 = 0.027620777487754822 + 0.01 * 7.014841556549072
Epoch 460, val loss: 0.7493770718574524
Epoch 470, training loss: 0.0957193374633789 = 0.025308437645435333 + 0.01 * 7.041090488433838
Epoch 470, val loss: 0.760116457939148
Epoch 480, training loss: 0.0931612029671669 = 0.023276088759303093 + 0.01 * 6.988511085510254
Epoch 480, val loss: 0.7705361247062683
Epoch 490, training loss: 0.09139353036880493 = 0.021479671820998192 + 0.01 * 6.991385459899902
Epoch 490, val loss: 0.7806023955345154
Epoch 500, training loss: 0.0897102802991867 = 0.019885782152414322 + 0.01 * 6.982450485229492
Epoch 500, val loss: 0.7903823256492615
Epoch 510, training loss: 0.08809743821620941 = 0.018465913832187653 + 0.01 * 6.963152885437012
Epoch 510, val loss: 0.7998163104057312
Epoch 520, training loss: 0.08705981075763702 = 0.017196740955114365 + 0.01 * 6.986307621002197
Epoch 520, val loss: 0.8089373111724854
Epoch 530, training loss: 0.08564083278179169 = 0.016056377440690994 + 0.01 * 6.958446025848389
Epoch 530, val loss: 0.8177279829978943
Epoch 540, training loss: 0.08450339734554291 = 0.01502925530076027 + 0.01 * 6.947413921356201
Epoch 540, val loss: 0.8262403607368469
Epoch 550, training loss: 0.0834202840924263 = 0.014101961627602577 + 0.01 * 6.931832313537598
Epoch 550, val loss: 0.8345257639884949
Epoch 560, training loss: 0.08251994848251343 = 0.013261796906590462 + 0.01 * 6.925815582275391
Epoch 560, val loss: 0.8424907922744751
Epoch 570, training loss: 0.08173826336860657 = 0.01249749306589365 + 0.01 * 6.92407751083374
Epoch 570, val loss: 0.8502281904220581
Epoch 580, training loss: 0.08107714354991913 = 0.011801796965301037 + 0.01 * 6.927535057067871
Epoch 580, val loss: 0.8577072024345398
Epoch 590, training loss: 0.08025246113538742 = 0.011166275478899479 + 0.01 * 6.908618450164795
Epoch 590, val loss: 0.8649802207946777
Epoch 600, training loss: 0.07960551232099533 = 0.010583750903606415 + 0.01 * 6.902176380157471
Epoch 600, val loss: 0.8719759583473206
Epoch 610, training loss: 0.0790620967745781 = 0.010048890486359596 + 0.01 * 6.901320934295654
Epoch 610, val loss: 0.8788022994995117
Epoch 620, training loss: 0.07844811677932739 = 0.009556818753480911 + 0.01 * 6.889130592346191
Epoch 620, val loss: 0.8853606581687927
Epoch 630, training loss: 0.07851695269346237 = 0.009102428331971169 + 0.01 * 6.941452503204346
Epoch 630, val loss: 0.8917592167854309
Epoch 640, training loss: 0.07740388065576553 = 0.008683085441589355 + 0.01 * 6.872079372406006
Epoch 640, val loss: 0.897986888885498
Epoch 650, training loss: 0.0769123062491417 = 0.00829424150288105 + 0.01 * 6.861806869506836
Epoch 650, val loss: 0.9041078090667725
Epoch 660, training loss: 0.07682488113641739 = 0.00793253630399704 + 0.01 * 6.88923454284668
Epoch 660, val loss: 0.9100024104118347
Epoch 670, training loss: 0.07627584040164948 = 0.007596982643008232 + 0.01 * 6.867885589599609
Epoch 670, val loss: 0.9157308340072632
Epoch 680, training loss: 0.07591214030981064 = 0.007284145336598158 + 0.01 * 6.862800121307373
Epoch 680, val loss: 0.9213398098945618
Epoch 690, training loss: 0.07550334185361862 = 0.006992080714553595 + 0.01 * 6.851126194000244
Epoch 690, val loss: 0.9268240928649902
Epoch 700, training loss: 0.07531518489122391 = 0.006718829739838839 + 0.01 * 6.859635829925537
Epoch 700, val loss: 0.9320652484893799
Epoch 710, training loss: 0.07518094033002853 = 0.006463680882006884 + 0.01 * 6.871725559234619
Epoch 710, val loss: 0.9372658133506775
Epoch 720, training loss: 0.07449256628751755 = 0.006224586628377438 + 0.01 * 6.826797962188721
Epoch 720, val loss: 0.9423070549964905
Epoch 730, training loss: 0.07455434650182724 = 0.006000111345201731 + 0.01 * 6.855423927307129
Epoch 730, val loss: 0.9473088383674622
Epoch 740, training loss: 0.0741330236196518 = 0.005789120681583881 + 0.01 * 6.834390163421631
Epoch 740, val loss: 0.9519920945167542
Epoch 750, training loss: 0.07382351160049438 = 0.005590219050645828 + 0.01 * 6.823329925537109
Epoch 750, val loss: 0.9567318558692932
Epoch 760, training loss: 0.0736081674695015 = 0.005402820650488138 + 0.01 * 6.820534706115723
Epoch 760, val loss: 0.9613151550292969
Epoch 770, training loss: 0.07344042509794235 = 0.005225821398198605 + 0.01 * 6.821460723876953
Epoch 770, val loss: 0.9657484292984009
Epoch 780, training loss: 0.07335568964481354 = 0.005058605223894119 + 0.01 * 6.829708576202393
Epoch 780, val loss: 0.9701589941978455
Epoch 790, training loss: 0.07323993742465973 = 0.004900638479739428 + 0.01 * 6.833929538726807
Epoch 790, val loss: 0.9743256568908691
Epoch 800, training loss: 0.0726865604519844 = 0.004750905558466911 + 0.01 * 6.793565273284912
Epoch 800, val loss: 0.9785217046737671
Epoch 810, training loss: 0.07277212291955948 = 0.004609144292771816 + 0.01 * 6.816298007965088
Epoch 810, val loss: 0.9824920892715454
Epoch 820, training loss: 0.0726434588432312 = 0.004474705085158348 + 0.01 * 6.816874980926514
Epoch 820, val loss: 0.9863877892494202
Epoch 830, training loss: 0.07228220254182816 = 0.004347499925643206 + 0.01 * 6.79347038269043
Epoch 830, val loss: 0.9902338981628418
Epoch 840, training loss: 0.07225951552391052 = 0.004225784447044134 + 0.01 * 6.803373336791992
Epoch 840, val loss: 0.9939258694648743
Epoch 850, training loss: 0.07214957475662231 = 0.004110675770789385 + 0.01 * 6.803889751434326
Epoch 850, val loss: 0.997626543045044
Epoch 860, training loss: 0.0719255581498146 = 0.004000692628324032 + 0.01 * 6.792486667633057
Epoch 860, val loss: 1.0010802745819092
Epoch 870, training loss: 0.07163307070732117 = 0.003896270412951708 + 0.01 * 6.773679733276367
Epoch 870, val loss: 1.0045373439788818
Epoch 880, training loss: 0.07148190587759018 = 0.003796373028308153 + 0.01 * 6.768553256988525
Epoch 880, val loss: 1.0079342126846313
Epoch 890, training loss: 0.07145415246486664 = 0.003701085690408945 + 0.01 * 6.775306701660156
Epoch 890, val loss: 1.01115083694458
Epoch 900, training loss: 0.07137594372034073 = 0.0036103599704802036 + 0.01 * 6.7765583992004395
Epoch 900, val loss: 1.014474868774414
Epoch 910, training loss: 0.07141797989606857 = 0.003523188875988126 + 0.01 * 6.789479732513428
Epoch 910, val loss: 1.0175561904907227
Epoch 920, training loss: 0.07133013755083084 = 0.003440206404775381 + 0.01 * 6.7889933586120605
Epoch 920, val loss: 1.0206804275512695
Epoch 930, training loss: 0.07089302688837051 = 0.0033607010263949633 + 0.01 * 6.753232479095459
Epoch 930, val loss: 1.0237165689468384
Epoch 940, training loss: 0.07090269774198532 = 0.003284403122961521 + 0.01 * 6.761829853057861
Epoch 940, val loss: 1.0266571044921875
Epoch 950, training loss: 0.07085412740707397 = 0.0032115222420543432 + 0.01 * 6.764260768890381
Epoch 950, val loss: 1.0295194387435913
Epoch 960, training loss: 0.07056356966495514 = 0.003141585737466812 + 0.01 * 6.742198944091797
Epoch 960, val loss: 1.0323431491851807
Epoch 970, training loss: 0.07056010514497757 = 0.003074438776820898 + 0.01 * 6.748566627502441
Epoch 970, val loss: 1.0351083278656006
Epoch 980, training loss: 0.0706389918923378 = 0.0030100394506007433 + 0.01 * 6.762895107269287
Epoch 980, val loss: 1.0377776622772217
Epoch 990, training loss: 0.07033157348632812 = 0.0029480811208486557 + 0.01 * 6.738348960876465
Epoch 990, val loss: 1.0404244661331177
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.0223476886749268 = 1.936379075050354 + 0.01 * 8.596860885620117
Epoch 0, val loss: 1.9368661642074585
Epoch 10, training loss: 2.0135498046875 = 1.9275816679000854 + 0.01 * 8.596814155578613
Epoch 10, val loss: 1.9286227226257324
Epoch 20, training loss: 2.0028328895568848 = 1.9168667793273926 + 0.01 * 8.596609115600586
Epoch 20, val loss: 1.9180456399917603
Epoch 30, training loss: 1.987977147102356 = 1.9020181894302368 + 0.01 * 8.595895767211914
Epoch 30, val loss: 1.9030789136886597
Epoch 40, training loss: 1.9661937952041626 = 1.8802744150161743 + 0.01 * 8.591934204101562
Epoch 40, val loss: 1.8812073469161987
Epoch 50, training loss: 1.9345194101333618 = 1.8488836288452148 + 0.01 * 8.563579559326172
Epoch 50, val loss: 1.8503515720367432
Epoch 60, training loss: 1.8932247161865234 = 1.808990240097046 + 0.01 * 8.423443794250488
Epoch 60, val loss: 1.8135737180709839
Epoch 70, training loss: 1.8506410121917725 = 1.7687872648239136 + 0.01 * 8.185369491577148
Epoch 70, val loss: 1.7786744832992554
Epoch 80, training loss: 1.803723692893982 = 1.724162220954895 + 0.01 * 7.956151962280273
Epoch 80, val loss: 1.736918330192566
Epoch 90, training loss: 1.7385344505310059 = 1.661681890487671 + 0.01 * 7.685252666473389
Epoch 90, val loss: 1.67819344997406
Epoch 100, training loss: 1.6525911092758179 = 1.5773847103118896 + 0.01 * 7.52064323425293
Epoch 100, val loss: 1.6028802394866943
Epoch 110, training loss: 1.5486763715744019 = 1.4741178750991821 + 0.01 * 7.455845355987549
Epoch 110, val loss: 1.5137066841125488
Epoch 120, training loss: 1.4358375072479248 = 1.3618510961532593 + 0.01 * 7.3986358642578125
Epoch 120, val loss: 1.417707085609436
Epoch 130, training loss: 1.3226205110549927 = 1.24912691116333 + 0.01 * 7.3493547439575195
Epoch 130, val loss: 1.323692798614502
Epoch 140, training loss: 1.2136642932891846 = 1.1406205892562866 + 0.01 * 7.3043742179870605
Epoch 140, val loss: 1.2348649501800537
Epoch 150, training loss: 1.1122921705245972 = 1.039629340171814 + 0.01 * 7.266278266906738
Epoch 150, val loss: 1.1541858911514282
Epoch 160, training loss: 1.0200716257095337 = 0.9476828575134277 + 0.01 * 7.238882064819336
Epoch 160, val loss: 1.0821141004562378
Epoch 170, training loss: 0.9364912509918213 = 0.8642891049385071 + 0.01 * 7.220212459564209
Epoch 170, val loss: 1.0179296731948853
Epoch 180, training loss: 0.8598761558532715 = 0.7878103852272034 + 0.01 * 7.206578731536865
Epoch 180, val loss: 0.9602062106132507
Epoch 190, training loss: 0.7886079549789429 = 0.7166711091995239 + 0.01 * 7.193683624267578
Epoch 190, val loss: 0.9078373908996582
Epoch 200, training loss: 0.7213283777236938 = 0.649505078792572 + 0.01 * 7.182331562042236
Epoch 200, val loss: 0.8601278066635132
Epoch 210, training loss: 0.6573909521102905 = 0.5856959819793701 + 0.01 * 7.169497013092041
Epoch 210, val loss: 0.8163408041000366
Epoch 220, training loss: 0.5969908833503723 = 0.5254356861114502 + 0.01 * 7.155519962310791
Epoch 220, val loss: 0.7763067483901978
Epoch 230, training loss: 0.5405314564704895 = 0.4691140055656433 + 0.01 * 7.141745090484619
Epoch 230, val loss: 0.7405796051025391
Epoch 240, training loss: 0.4882327914237976 = 0.4169255495071411 + 0.01 * 7.130723476409912
Epoch 240, val loss: 0.7095369696617126
Epoch 250, training loss: 0.4398837685585022 = 0.36868926882743835 + 0.01 * 7.119451522827148
Epoch 250, val loss: 0.6831918954849243
Epoch 260, training loss: 0.39516738057136536 = 0.32402366399765015 + 0.01 * 7.114370822906494
Epoch 260, val loss: 0.6612718105316162
Epoch 270, training loss: 0.3537645936012268 = 0.2826503813266754 + 0.01 * 7.111421585083008
Epoch 270, val loss: 0.6434297561645508
Epoch 280, training loss: 0.31568244099617004 = 0.24458695948123932 + 0.01 * 7.109548091888428
Epoch 280, val loss: 0.6293661594390869
Epoch 290, training loss: 0.2812400460243225 = 0.21015849709510803 + 0.01 * 7.108154296875
Epoch 290, val loss: 0.6189955472946167
Epoch 300, training loss: 0.2508797347545624 = 0.17981307208538055 + 0.01 * 7.106666088104248
Epoch 300, val loss: 0.6124909520149231
Epoch 310, training loss: 0.22485631704330444 = 0.15379509329795837 + 0.01 * 7.106122016906738
Epoch 310, val loss: 0.6096439361572266
Epoch 320, training loss: 0.20298901200294495 = 0.13194967806339264 + 0.01 * 7.103933811187744
Epoch 320, val loss: 0.6100178360939026
Epoch 330, training loss: 0.1848178207874298 = 0.11379708349704742 + 0.01 * 7.1020731925964355
Epoch 330, val loss: 0.6132263541221619
Epoch 340, training loss: 0.16974183917045593 = 0.09874340146780014 + 0.01 * 7.0998430252075195
Epoch 340, val loss: 0.6187187433242798
Epoch 350, training loss: 0.15719863772392273 = 0.0862211063504219 + 0.01 * 7.097754001617432
Epoch 350, val loss: 0.6259264945983887
Epoch 360, training loss: 0.1467057317495346 = 0.0757472962141037 + 0.01 * 7.09584379196167
Epoch 360, val loss: 0.6344011425971985
Epoch 370, training loss: 0.13785602152347565 = 0.06693121045827866 + 0.01 * 7.0924811363220215
Epoch 370, val loss: 0.6437917351722717
Epoch 380, training loss: 0.13035908341407776 = 0.059462349861860275 + 0.01 * 7.0896735191345215
Epoch 380, val loss: 0.6538195610046387
Epoch 390, training loss: 0.12398102879524231 = 0.053098127245903015 + 0.01 * 7.088290214538574
Epoch 390, val loss: 0.6642183661460876
Epoch 400, training loss: 0.11847907304763794 = 0.04764368012547493 + 0.01 * 7.083539009094238
Epoch 400, val loss: 0.6748197674751282
Epoch 410, training loss: 0.11375740170478821 = 0.04294212907552719 + 0.01 * 7.0815277099609375
Epoch 410, val loss: 0.6855051517486572
Epoch 420, training loss: 0.10964420437812805 = 0.03886936232447624 + 0.01 * 7.077484607696533
Epoch 420, val loss: 0.6962306499481201
Epoch 430, training loss: 0.10605329275131226 = 0.03532489389181137 + 0.01 * 7.072839736938477
Epoch 430, val loss: 0.706885814666748
Epoch 440, training loss: 0.10291701555252075 = 0.03222602978348732 + 0.01 * 7.069097995758057
Epoch 440, val loss: 0.7174352407455444
Epoch 450, training loss: 0.1001659706234932 = 0.029505232349038124 + 0.01 * 7.066074371337891
Epoch 450, val loss: 0.7278300523757935
Epoch 460, training loss: 0.09772197902202606 = 0.0271062720566988 + 0.01 * 7.06157112121582
Epoch 460, val loss: 0.7380405068397522
Epoch 470, training loss: 0.09556262195110321 = 0.024982605129480362 + 0.01 * 7.058001518249512
Epoch 470, val loss: 0.7480629086494446
Epoch 480, training loss: 0.09362804889678955 = 0.0230957493185997 + 0.01 * 7.053229808807373
Epoch 480, val loss: 0.7578350901603699
Epoch 490, training loss: 0.09191066771745682 = 0.0214126855134964 + 0.01 * 7.049798011779785
Epoch 490, val loss: 0.7673798203468323
Epoch 500, training loss: 0.09036587178707123 = 0.019905950874090195 + 0.01 * 7.045991897583008
Epoch 500, val loss: 0.7767298817634583
Epoch 510, training loss: 0.08898560702800751 = 0.018552619963884354 + 0.01 * 7.043298244476318
Epoch 510, val loss: 0.7858249545097351
Epoch 520, training loss: 0.08770022541284561 = 0.017333881929516792 + 0.01 * 7.03663444519043
Epoch 520, val loss: 0.7946646213531494
Epoch 530, training loss: 0.08651513606309891 = 0.016232481226325035 + 0.01 * 7.028265476226807
Epoch 530, val loss: 0.8033177256584167
Epoch 540, training loss: 0.08551681786775589 = 0.01523426454514265 + 0.01 * 7.028255462646484
Epoch 540, val loss: 0.8117371797561646
Epoch 550, training loss: 0.08448600023984909 = 0.014327689073979855 + 0.01 * 7.015830993652344
Epoch 550, val loss: 0.8199430108070374
Epoch 560, training loss: 0.08360778540372849 = 0.013501843437552452 + 0.01 * 7.010594367980957
Epoch 560, val loss: 0.8279202580451965
Epoch 570, training loss: 0.08282528072595596 = 0.012747257947921753 + 0.01 * 7.007802486419678
Epoch 570, val loss: 0.8356907963752747
Epoch 580, training loss: 0.08198665827512741 = 0.012057030573487282 + 0.01 * 6.992962837219238
Epoch 580, val loss: 0.8432492017745972
Epoch 590, training loss: 0.08137516677379608 = 0.011423359625041485 + 0.01 * 6.995181083679199
Epoch 590, val loss: 0.850566565990448
Epoch 600, training loss: 0.08069959282875061 = 0.010841252282261848 + 0.01 * 6.985834121704102
Epoch 600, val loss: 0.8577371835708618
Epoch 610, training loss: 0.08004970848560333 = 0.01030496135354042 + 0.01 * 6.9744744300842285
Epoch 610, val loss: 0.8646951913833618
Epoch 620, training loss: 0.07946618646383286 = 0.009809575043618679 + 0.01 * 6.965661525726318
Epoch 620, val loss: 0.8714689612388611
Epoch 630, training loss: 0.07901999354362488 = 0.009351025335490704 + 0.01 * 6.966897487640381
Epoch 630, val loss: 0.8780636191368103
Epoch 640, training loss: 0.07847202569246292 = 0.008926537819206715 + 0.01 * 6.954548358917236
Epoch 640, val loss: 0.8845412731170654
Epoch 650, training loss: 0.07806804031133652 = 0.008533281274139881 + 0.01 * 6.953475475311279
Epoch 650, val loss: 0.890678882598877
Epoch 660, training loss: 0.07754877209663391 = 0.008167397230863571 + 0.01 * 6.938137531280518
Epoch 660, val loss: 0.8967193365097046
Epoch 670, training loss: 0.07714362442493439 = 0.007826019078493118 + 0.01 * 6.931760311126709
Epoch 670, val loss: 0.9026696681976318
Epoch 680, training loss: 0.07715965807437897 = 0.007507804781198502 + 0.01 * 6.96518611907959
Epoch 680, val loss: 0.9084245562553406
Epoch 690, training loss: 0.0764249786734581 = 0.007211098913103342 + 0.01 * 6.921388626098633
Epoch 690, val loss: 0.9140071868896484
Epoch 700, training loss: 0.07608533650636673 = 0.006933273747563362 + 0.01 * 6.915206432342529
Epoch 700, val loss: 0.9195011854171753
Epoch 710, training loss: 0.07573665678501129 = 0.006672384683042765 + 0.01 * 6.906427383422852
Epoch 710, val loss: 0.9248859286308289
Epoch 720, training loss: 0.0755348801612854 = 0.006427128333598375 + 0.01 * 6.9107747077941895
Epoch 720, val loss: 0.9302060008049011
Epoch 730, training loss: 0.07518865168094635 = 0.006197282113134861 + 0.01 * 6.899137496948242
Epoch 730, val loss: 0.9353259205818176
Epoch 740, training loss: 0.07499116659164429 = 0.005981320049613714 + 0.01 * 6.900984764099121
Epoch 740, val loss: 0.940270185470581
Epoch 750, training loss: 0.07467018067836761 = 0.0057777538895606995 + 0.01 * 6.889243125915527
Epoch 750, val loss: 0.9451277256011963
Epoch 760, training loss: 0.074616439640522 = 0.00558557128533721 + 0.01 * 6.9030866622924805
Epoch 760, val loss: 0.949906051158905
Epoch 770, training loss: 0.0743098258972168 = 0.005404151044785976 + 0.01 * 6.890567302703857
Epoch 770, val loss: 0.9545789957046509
Epoch 780, training loss: 0.07400734722614288 = 0.005232535768300295 + 0.01 * 6.877480983734131
Epoch 780, val loss: 0.9591473340988159
Epoch 790, training loss: 0.07381546497344971 = 0.005070094019174576 + 0.01 * 6.874537467956543
Epoch 790, val loss: 0.9636680483818054
Epoch 800, training loss: 0.07362086325883865 = 0.004916259553283453 + 0.01 * 6.870460510253906
Epoch 800, val loss: 0.968019962310791
Epoch 810, training loss: 0.0734134390950203 = 0.004770317580550909 + 0.01 * 6.864312648773193
Epoch 810, val loss: 0.9723278880119324
Epoch 820, training loss: 0.07323633134365082 = 0.0046317060478031635 + 0.01 * 6.860462665557861
Epoch 820, val loss: 0.9765631556510925
Epoch 830, training loss: 0.07307600229978561 = 0.00450008362531662 + 0.01 * 6.8575921058654785
Epoch 830, val loss: 0.9807221293449402
Epoch 840, training loss: 0.07288442552089691 = 0.0043753525242209435 + 0.01 * 6.850907325744629
Epoch 840, val loss: 0.9847163558006287
Epoch 850, training loss: 0.07280787825584412 = 0.004256618674844503 + 0.01 * 6.855125904083252
Epoch 850, val loss: 0.9886326789855957
Epoch 860, training loss: 0.07262877374887466 = 0.004143484868109226 + 0.01 * 6.84852933883667
Epoch 860, val loss: 0.9925115704536438
Epoch 870, training loss: 0.07261550426483154 = 0.004035630263388157 + 0.01 * 6.857987403869629
Epoch 870, val loss: 0.9963679313659668
Epoch 880, training loss: 0.07234620302915573 = 0.003932691644877195 + 0.01 * 6.841351509094238
Epoch 880, val loss: 1.0000407695770264
Epoch 890, training loss: 0.07240938395261765 = 0.0038343374617397785 + 0.01 * 6.857504844665527
Epoch 890, val loss: 1.0037504434585571
Epoch 900, training loss: 0.07211795449256897 = 0.0037405826151371002 + 0.01 * 6.837737560272217
Epoch 900, val loss: 1.0072803497314453
Epoch 910, training loss: 0.07195600122213364 = 0.003650953993201256 + 0.01 * 6.830504894256592
Epoch 910, val loss: 1.0107576847076416
Epoch 920, training loss: 0.07178650051355362 = 0.0035652066580951214 + 0.01 * 6.822129726409912
Epoch 920, val loss: 1.0142031908035278
Epoch 930, training loss: 0.07175515592098236 = 0.0034831217490136623 + 0.01 * 6.827203273773193
Epoch 930, val loss: 1.0175552368164062
Epoch 940, training loss: 0.0717749297618866 = 0.003404471790418029 + 0.01 * 6.8370466232299805
Epoch 940, val loss: 1.0208930969238281
Epoch 950, training loss: 0.07150602340698242 = 0.0033290954306721687 + 0.01 * 6.817692756652832
Epoch 950, val loss: 1.0241694450378418
Epoch 960, training loss: 0.07136830687522888 = 0.003256794298067689 + 0.01 * 6.811151027679443
Epoch 960, val loss: 1.0273163318634033
Epoch 970, training loss: 0.07137800753116608 = 0.003187378868460655 + 0.01 * 6.819063186645508
Epoch 970, val loss: 1.0304392576217651
Epoch 980, training loss: 0.07124275714159012 = 0.003120861481875181 + 0.01 * 6.81218957901001
Epoch 980, val loss: 1.0335509777069092
Epoch 990, training loss: 0.07107342034578323 = 0.003056945977732539 + 0.01 * 6.801648139953613
Epoch 990, val loss: 1.0365066528320312
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8465998945703743
=== training gcn model ===
Epoch 0, training loss: 2.0253121852874756 = 1.9393434524536133 + 0.01 * 8.59686279296875
Epoch 0, val loss: 1.9359186887741089
Epoch 10, training loss: 2.0149502754211426 = 1.928982138633728 + 0.01 * 8.59680461883545
Epoch 10, val loss: 1.9255088567733765
Epoch 20, training loss: 2.002089500427246 = 1.9161235094070435 + 0.01 * 8.596592903137207
Epoch 20, val loss: 1.9119542837142944
Epoch 30, training loss: 1.98418128490448 = 1.8982222080230713 + 0.01 * 8.595903396606445
Epoch 30, val loss: 1.8925392627716064
Epoch 40, training loss: 1.9583258628845215 = 1.8724035024642944 + 0.01 * 8.592233657836914
Epoch 40, val loss: 1.8644366264343262
Epoch 50, training loss: 1.9233142137527466 = 1.8376301527023315 + 0.01 * 8.568408966064453
Epoch 50, val loss: 1.8279772996902466
Epoch 60, training loss: 1.8848367929458618 = 1.8002055883407593 + 0.01 * 8.46312141418457
Epoch 60, val loss: 1.7929275035858154
Epoch 70, training loss: 1.849670171737671 = 1.7664963006973267 + 0.01 * 8.317383766174316
Epoch 70, val loss: 1.765859603881836
Epoch 80, training loss: 1.8039271831512451 = 1.7229816913604736 + 0.01 * 8.094554901123047
Epoch 80, val loss: 1.7299391031265259
Epoch 90, training loss: 1.7391257286071777 = 1.6616803407669067 + 0.01 * 7.744534492492676
Epoch 90, val loss: 1.6763631105422974
Epoch 100, training loss: 1.6556928157806396 = 1.5797868967056274 + 0.01 * 7.590590953826904
Epoch 100, val loss: 1.6036595106124878
Epoch 110, training loss: 1.5580730438232422 = 1.482600212097168 + 0.01 * 7.5472822189331055
Epoch 110, val loss: 1.5202264785766602
Epoch 120, training loss: 1.4526339769363403 = 1.3775348663330078 + 0.01 * 7.509906768798828
Epoch 120, val loss: 1.4328958988189697
Epoch 130, training loss: 1.3423917293548584 = 1.2677611112594604 + 0.01 * 7.463062286376953
Epoch 130, val loss: 1.3432343006134033
Epoch 140, training loss: 1.2298333644866943 = 1.1557821035385132 + 0.01 * 7.405123233795166
Epoch 140, val loss: 1.253234624862671
Epoch 150, training loss: 1.1188400983810425 = 1.045440435409546 + 0.01 * 7.339972019195557
Epoch 150, val loss: 1.165802240371704
Epoch 160, training loss: 1.015586495399475 = 0.9426279067993164 + 0.01 * 7.295860767364502
Epoch 160, val loss: 1.0857330560684204
Epoch 170, training loss: 0.9248946309089661 = 0.8521944284439087 + 0.01 * 7.270020008087158
Epoch 170, val loss: 1.0172358751296997
Epoch 180, training loss: 0.8478983640670776 = 0.7755156755447388 + 0.01 * 7.238270282745361
Epoch 180, val loss: 0.9619035720825195
Epoch 190, training loss: 0.7826436161994934 = 0.7106176614761353 + 0.01 * 7.202593803405762
Epoch 190, val loss: 0.917699933052063
Epoch 200, training loss: 0.725466251373291 = 0.6537250876426697 + 0.01 * 7.174115180969238
Epoch 200, val loss: 0.8814231157302856
Epoch 210, training loss: 0.6724807620048523 = 0.6009527444839478 + 0.01 * 7.152802467346191
Epoch 210, val loss: 0.8500838875770569
Epoch 220, training loss: 0.6210579872131348 = 0.5496513247489929 + 0.01 * 7.140663146972656
Epoch 220, val loss: 0.8220461010932922
Epoch 230, training loss: 0.5705968141555786 = 0.49929025769233704 + 0.01 * 7.130652904510498
Epoch 230, val loss: 0.7976656556129456
Epoch 240, training loss: 0.5218310952186584 = 0.4506045877933502 + 0.01 * 7.122652053833008
Epoch 240, val loss: 0.7776665687561035
Epoch 250, training loss: 0.475663959980011 = 0.40453678369522095 + 0.01 * 7.11271858215332
Epoch 250, val loss: 0.7624972462654114
Epoch 260, training loss: 0.4325253367424011 = 0.3614742159843445 + 0.01 * 7.1051106452941895
Epoch 260, val loss: 0.7522461414337158
Epoch 270, training loss: 0.3924742341041565 = 0.32149142026901245 + 0.01 * 7.098282337188721
Epoch 270, val loss: 0.7468329668045044
Epoch 280, training loss: 0.35566461086273193 = 0.28471609950065613 + 0.01 * 7.094851970672607
Epoch 280, val loss: 0.7459313273429871
Epoch 290, training loss: 0.3222048282623291 = 0.25133460760116577 + 0.01 * 7.087022304534912
Epoch 290, val loss: 0.7489466667175293
Epoch 300, training loss: 0.29228347539901733 = 0.22146153450012207 + 0.01 * 7.08219575881958
Epoch 300, val loss: 0.7554591298103333
Epoch 310, training loss: 0.2658030390739441 = 0.19502700865268707 + 0.01 * 7.077604293823242
Epoch 310, val loss: 0.765259325504303
Epoch 320, training loss: 0.24251577258110046 = 0.1718067228794098 + 0.01 * 7.0709052085876465
Epoch 320, val loss: 0.7779149413108826
Epoch 330, training loss: 0.22213813662528992 = 0.15147051215171814 + 0.01 * 7.066762924194336
Epoch 330, val loss: 0.7928778529167175
Epoch 340, training loss: 0.20431673526763916 = 0.13369430601596832 + 0.01 * 7.062243938446045
Epoch 340, val loss: 0.8096907734870911
Epoch 350, training loss: 0.18880194425582886 = 0.1181744635105133 + 0.01 * 7.062747478485107
Epoch 350, val loss: 0.8278020620346069
Epoch 360, training loss: 0.17523270845413208 = 0.10464008152484894 + 0.01 * 7.059263706207275
Epoch 360, val loss: 0.8468288779258728
Epoch 370, training loss: 0.16338780522346497 = 0.09284217655658722 + 0.01 * 7.054563522338867
Epoch 370, val loss: 0.8664619326591492
Epoch 380, training loss: 0.15307225286960602 = 0.0825626403093338 + 0.01 * 7.050961494445801
Epoch 380, val loss: 0.8864653706550598
Epoch 390, training loss: 0.14408057928085327 = 0.07360415160655975 + 0.01 * 7.047642230987549
Epoch 390, val loss: 0.906566321849823
Epoch 400, training loss: 0.13627830147743225 = 0.06578993052244186 + 0.01 * 7.048837184906006
Epoch 400, val loss: 0.9266268014907837
Epoch 410, training loss: 0.1293855905532837 = 0.05896993726491928 + 0.01 * 7.04156494140625
Epoch 410, val loss: 0.9465709328651428
Epoch 420, training loss: 0.12344613671302795 = 0.05300983414053917 + 0.01 * 7.043630123138428
Epoch 420, val loss: 0.9663358330726624
Epoch 430, training loss: 0.11814515292644501 = 0.04779631644487381 + 0.01 * 7.034883499145508
Epoch 430, val loss: 0.9858618974685669
Epoch 440, training loss: 0.11354158818721771 = 0.04322568327188492 + 0.01 * 7.031590461730957
Epoch 440, val loss: 1.0051188468933105
Epoch 450, training loss: 0.10954183340072632 = 0.039209991693496704 + 0.01 * 7.033184051513672
Epoch 450, val loss: 1.0240216255187988
Epoch 460, training loss: 0.10594050586223602 = 0.03567678853869438 + 0.01 * 7.026371479034424
Epoch 460, val loss: 1.0425159931182861
Epoch 470, training loss: 0.10278435051441193 = 0.03256077691912651 + 0.01 * 7.022356986999512
Epoch 470, val loss: 1.0605674982070923
Epoch 480, training loss: 0.10003164410591125 = 0.029805470257997513 + 0.01 * 7.022617816925049
Epoch 480, val loss: 1.0782102346420288
Epoch 490, training loss: 0.09752339869737625 = 0.027365246787667274 + 0.01 * 7.015815258026123
Epoch 490, val loss: 1.095373272895813
Epoch 500, training loss: 0.09530925750732422 = 0.02519775554537773 + 0.01 * 7.01115083694458
Epoch 500, val loss: 1.1120775938034058
Epoch 510, training loss: 0.09334062784910202 = 0.023266935721039772 + 0.01 * 7.007369518280029
Epoch 510, val loss: 1.1283835172653198
Epoch 520, training loss: 0.09156382083892822 = 0.021541986614465714 + 0.01 * 7.00218391418457
Epoch 520, val loss: 1.14422607421875
Epoch 530, training loss: 0.0902252197265625 = 0.01999707892537117 + 0.01 * 7.0228142738342285
Epoch 530, val loss: 1.1596640348434448
Epoch 540, training loss: 0.08863267302513123 = 0.018612375482916832 + 0.01 * 7.0020294189453125
Epoch 540, val loss: 1.1745359897613525
Epoch 550, training loss: 0.08728119730949402 = 0.01736614853143692 + 0.01 * 6.991505146026611
Epoch 550, val loss: 1.188995599746704
Epoch 560, training loss: 0.08612854778766632 = 0.01624060608446598 + 0.01 * 6.988794803619385
Epoch 560, val loss: 1.2030556201934814
Epoch 570, training loss: 0.08507969230413437 = 0.015221131965517998 + 0.01 * 6.985856056213379
Epoch 570, val loss: 1.2167527675628662
Epoch 580, training loss: 0.0841229110956192 = 0.014295510947704315 + 0.01 * 6.9827399253845215
Epoch 580, val loss: 1.2300167083740234
Epoch 590, training loss: 0.08327321708202362 = 0.013453185558319092 + 0.01 * 6.982003211975098
Epoch 590, val loss: 1.2429111003875732
Epoch 600, training loss: 0.08241727948188782 = 0.012685407884418964 + 0.01 * 6.97318696975708
Epoch 600, val loss: 1.255434274673462
Epoch 610, training loss: 0.08170850574970245 = 0.011983830481767654 + 0.01 * 6.972468376159668
Epoch 610, val loss: 1.267633080482483
Epoch 620, training loss: 0.0810587927699089 = 0.011340734548866749 + 0.01 * 6.971805572509766
Epoch 620, val loss: 1.279449701309204
Epoch 630, training loss: 0.08036709576845169 = 0.010749118402600288 + 0.01 * 6.961797714233398
Epoch 630, val loss: 1.2909938097000122
Epoch 640, training loss: 0.07980310171842575 = 0.010204201564192772 + 0.01 * 6.959889888763428
Epoch 640, val loss: 1.3022305965423584
Epoch 650, training loss: 0.07930343598127365 = 0.009700799360871315 + 0.01 * 6.960264205932617
Epoch 650, val loss: 1.3130918741226196
Epoch 660, training loss: 0.07876282930374146 = 0.009234060533344746 + 0.01 * 6.952876567840576
Epoch 660, val loss: 1.3237500190734863
Epoch 670, training loss: 0.07841002941131592 = 0.008799402043223381 + 0.01 * 6.961062908172607
Epoch 670, val loss: 1.3341704607009888
Epoch 680, training loss: 0.07790743559598923 = 0.008394296281039715 + 0.01 * 6.951314449310303
Epoch 680, val loss: 1.3443756103515625
Epoch 690, training loss: 0.07743116468191147 = 0.008013948798179626 + 0.01 * 6.941721439361572
Epoch 690, val loss: 1.3543603420257568
Epoch 700, training loss: 0.07708463817834854 = 0.00765631441026926 + 0.01 * 6.942832946777344
Epoch 700, val loss: 1.3642010688781738
Epoch 710, training loss: 0.07676306366920471 = 0.007321392651647329 + 0.01 * 6.944167137145996
Epoch 710, val loss: 1.3737738132476807
Epoch 720, training loss: 0.0763862133026123 = 0.007006906904280186 + 0.01 * 6.937930583953857
Epoch 720, val loss: 1.3831779956817627
Epoch 730, training loss: 0.07600021362304688 = 0.006711750756949186 + 0.01 * 6.92884635925293
Epoch 730, val loss: 1.3923090696334839
Epoch 740, training loss: 0.07573101669549942 = 0.006434428971260786 + 0.01 * 6.929659366607666
Epoch 740, val loss: 1.4012941122055054
Epoch 750, training loss: 0.07539692521095276 = 0.006173520814627409 + 0.01 * 6.9223408699035645
Epoch 750, val loss: 1.4101194143295288
Epoch 760, training loss: 0.07505446672439575 = 0.0059275319799780846 + 0.01 * 6.912693500518799
Epoch 760, val loss: 1.4187475442886353
Epoch 770, training loss: 0.07498496025800705 = 0.005695615895092487 + 0.01 * 6.928934574127197
Epoch 770, val loss: 1.4272334575653076
Epoch 780, training loss: 0.07471223175525665 = 0.0054777078330516815 + 0.01 * 6.923452854156494
Epoch 780, val loss: 1.4354887008666992
Epoch 790, training loss: 0.07437629252672195 = 0.005272765178233385 + 0.01 * 6.9103522300720215
Epoch 790, val loss: 1.4434328079223633
Epoch 800, training loss: 0.07408463209867477 = 0.005079546477645636 + 0.01 * 6.900508880615234
Epoch 800, val loss: 1.4513026475906372
Epoch 810, training loss: 0.07399086654186249 = 0.004896681290119886 + 0.01 * 6.909419059753418
Epoch 810, val loss: 1.4590892791748047
Epoch 820, training loss: 0.07368718832731247 = 0.0047239093109965324 + 0.01 * 6.896328449249268
Epoch 820, val loss: 1.466544508934021
Epoch 830, training loss: 0.07357436418533325 = 0.004560571163892746 + 0.01 * 6.9013800621032715
Epoch 830, val loss: 1.4739203453063965
Epoch 840, training loss: 0.07344215363264084 = 0.004405823536217213 + 0.01 * 6.903633117675781
Epoch 840, val loss: 1.4811691045761108
Epoch 850, training loss: 0.07311838865280151 = 0.004260670859366655 + 0.01 * 6.885771751403809
Epoch 850, val loss: 1.4881293773651123
Epoch 860, training loss: 0.07291099429130554 = 0.00412288773804903 + 0.01 * 6.878810405731201
Epoch 860, val loss: 1.494979977607727
Epoch 870, training loss: 0.07301869243383408 = 0.003991862293332815 + 0.01 * 6.902682781219482
Epoch 870, val loss: 1.5018082857131958
Epoch 880, training loss: 0.07270115613937378 = 0.003868084866553545 + 0.01 * 6.883306980133057
Epoch 880, val loss: 1.508341670036316
Epoch 890, training loss: 0.07251939177513123 = 0.003750832285732031 + 0.01 * 6.876856327056885
Epoch 890, val loss: 1.514647126197815
Epoch 900, training loss: 0.07234722375869751 = 0.0036397057119756937 + 0.01 * 6.870751857757568
Epoch 900, val loss: 1.5209472179412842
Epoch 910, training loss: 0.0724051371216774 = 0.0035339677706360817 + 0.01 * 6.887117385864258
Epoch 910, val loss: 1.5269831418991089
Epoch 920, training loss: 0.07223641872406006 = 0.0034337977413088083 + 0.01 * 6.8802618980407715
Epoch 920, val loss: 1.5329744815826416
Epoch 930, training loss: 0.07203725725412369 = 0.0033381045795977116 + 0.01 * 6.869915008544922
Epoch 930, val loss: 1.5387415885925293
Epoch 940, training loss: 0.07185520976781845 = 0.003246989566832781 + 0.01 * 6.8608222007751465
Epoch 940, val loss: 1.5444748401641846
Epoch 950, training loss: 0.07180286198854446 = 0.0031602007802575827 + 0.01 * 6.864266395568848
Epoch 950, val loss: 1.5500068664550781
Epoch 960, training loss: 0.07159926742315292 = 0.0030782511457800865 + 0.01 * 6.852101802825928
Epoch 960, val loss: 1.5553364753723145
Epoch 970, training loss: 0.07156043499708176 = 0.0029999567195773125 + 0.01 * 6.856048583984375
Epoch 970, val loss: 1.5605666637420654
Epoch 980, training loss: 0.07137827575206757 = 0.0029249838553369045 + 0.01 * 6.845329284667969
Epoch 980, val loss: 1.5657399892807007
Epoch 990, training loss: 0.07128481566905975 = 0.0028534268494695425 + 0.01 * 6.843139171600342
Epoch 990, val loss: 1.5707993507385254
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8397469688982605
The final CL Acc:0.82593, 0.00302, The final GNN Acc:0.84045, 0.00476
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9570])
updated graph: torch.Size([2, 10612])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0375492572784424 = 1.9515807628631592 + 0.01 * 8.596853256225586
Epoch 0, val loss: 1.9554601907730103
Epoch 10, training loss: 2.0273566246032715 = 1.9413886070251465 + 0.01 * 8.596810340881348
Epoch 10, val loss: 1.945546269416809
Epoch 20, training loss: 2.014857769012451 = 1.9288909435272217 + 0.01 * 8.596675872802734
Epoch 20, val loss: 1.9329944849014282
Epoch 30, training loss: 1.9973814487457275 = 1.9114184379577637 + 0.01 * 8.596302032470703
Epoch 30, val loss: 1.9151667356491089
Epoch 40, training loss: 1.9716789722442627 = 1.885733723640442 + 0.01 * 8.594529151916504
Epoch 40, val loss: 1.8893119096755981
Epoch 50, training loss: 1.936182975769043 = 1.8503676652908325 + 0.01 * 8.581531524658203
Epoch 50, val loss: 1.8551158905029297
Epoch 60, training loss: 1.8977041244506836 = 1.812520980834961 + 0.01 * 8.518312454223633
Epoch 60, val loss: 1.8210541009902954
Epoch 70, training loss: 1.865808367729187 = 1.7827352285385132 + 0.01 * 8.307313919067383
Epoch 70, val loss: 1.7935162782669067
Epoch 80, training loss: 1.8290257453918457 = 1.7468668222427368 + 0.01 * 8.215896606445312
Epoch 80, val loss: 1.7583268880844116
Epoch 90, training loss: 1.7768338918685913 = 1.6960203647613525 + 0.01 * 8.081351280212402
Epoch 90, val loss: 1.713478446006775
Epoch 100, training loss: 1.7046654224395752 = 1.6250123977661133 + 0.01 * 7.965302467346191
Epoch 100, val loss: 1.6536808013916016
Epoch 110, training loss: 1.6145985126495361 = 1.53633451461792 + 0.01 * 7.8263983726501465
Epoch 110, val loss: 1.5799616575241089
Epoch 120, training loss: 1.515860676765442 = 1.439630389213562 + 0.01 * 7.623024940490723
Epoch 120, val loss: 1.5011688470840454
Epoch 130, training loss: 1.4200317859649658 = 1.3448630571365356 + 0.01 * 7.5168776512146
Epoch 130, val loss: 1.426133155822754
Epoch 140, training loss: 1.329308271408081 = 1.255042314529419 + 0.01 * 7.426590442657471
Epoch 140, val loss: 1.3571420907974243
Epoch 150, training loss: 1.244006872177124 = 1.1703062057495117 + 0.01 * 7.37006139755249
Epoch 150, val loss: 1.294176697731018
Epoch 160, training loss: 1.1654448509216309 = 1.0920188426971436 + 0.01 * 7.342596530914307
Epoch 160, val loss: 1.237906575202942
Epoch 170, training loss: 1.0944368839263916 = 1.021136999130249 + 0.01 * 7.329985618591309
Epoch 170, val loss: 1.1884313821792603
Epoch 180, training loss: 1.0299984216690063 = 0.9568458795547485 + 0.01 * 7.315256595611572
Epoch 180, val loss: 1.1443471908569336
Epoch 190, training loss: 0.9697633981704712 = 0.8968037366867065 + 0.01 * 7.295963287353516
Epoch 190, val loss: 1.1037652492523193
Epoch 200, training loss: 0.9109861850738525 = 0.8382474184036255 + 0.01 * 7.273877143859863
Epoch 200, val loss: 1.0646827220916748
Epoch 210, training loss: 0.8519084453582764 = 0.7793259620666504 + 0.01 * 7.258245944976807
Epoch 210, val loss: 1.0261415243148804
Epoch 220, training loss: 0.792221188545227 = 0.7197728157043457 + 0.01 * 7.244840145111084
Epoch 220, val loss: 0.9885584115982056
Epoch 230, training loss: 0.7327523827552795 = 0.6603623628616333 + 0.01 * 7.239002227783203
Epoch 230, val loss: 0.9527848958969116
Epoch 240, training loss: 0.6745643615722656 = 0.6022184491157532 + 0.01 * 7.234589576721191
Epoch 240, val loss: 0.9194990396499634
Epoch 250, training loss: 0.618584394454956 = 0.5462619662284851 + 0.01 * 7.232241630554199
Epoch 250, val loss: 0.889388918876648
Epoch 260, training loss: 0.5654264092445374 = 0.4931238889694214 + 0.01 * 7.230250835418701
Epoch 260, val loss: 0.8630273342132568
Epoch 270, training loss: 0.515345573425293 = 0.44306015968322754 + 0.01 * 7.228538513183594
Epoch 270, val loss: 0.8406610488891602
Epoch 280, training loss: 0.46821513772010803 = 0.3959405720233917 + 0.01 * 7.227456569671631
Epoch 280, val loss: 0.8222200870513916
Epoch 290, training loss: 0.4238153100013733 = 0.3515479266643524 + 0.01 * 7.226738929748535
Epoch 290, val loss: 0.8075487613677979
Epoch 300, training loss: 0.3820107877254486 = 0.3097613453865051 + 0.01 * 7.224944114685059
Epoch 300, val loss: 0.7963790893554688
Epoch 310, training loss: 0.3429025113582611 = 0.2706589698791504 + 0.01 * 7.224353790283203
Epoch 310, val loss: 0.7885840535163879
Epoch 320, training loss: 0.30682528018951416 = 0.23459208011627197 + 0.01 * 7.2233195304870605
Epoch 320, val loss: 0.7841413617134094
Epoch 330, training loss: 0.27424752712249756 = 0.20198258757591248 + 0.01 * 7.226495265960693
Epoch 330, val loss: 0.7830895185470581
Epoch 340, training loss: 0.24532973766326904 = 0.17310144007205963 + 0.01 * 7.222830772399902
Epoch 340, val loss: 0.7854127287864685
Epoch 350, training loss: 0.22022220492362976 = 0.14800558984279633 + 0.01 * 7.221660614013672
Epoch 350, val loss: 0.7910306453704834
Epoch 360, training loss: 0.1987365186214447 = 0.1265394240617752 + 0.01 * 7.2197089195251465
Epoch 360, val loss: 0.7995172739028931
Epoch 370, training loss: 0.18056924641132355 = 0.10838068276643753 + 0.01 * 7.2188568115234375
Epoch 370, val loss: 0.8104172348976135
Epoch 380, training loss: 0.16529960930347443 = 0.09314071387052536 + 0.01 * 7.215889930725098
Epoch 380, val loss: 0.8231707215309143
Epoch 390, training loss: 0.15253493189811707 = 0.08040790259838104 + 0.01 * 7.212704181671143
Epoch 390, val loss: 0.8372353911399841
Epoch 400, training loss: 0.1418919712305069 = 0.06978705525398254 + 0.01 * 7.21049165725708
Epoch 400, val loss: 0.8522338271141052
Epoch 410, training loss: 0.1329670548439026 = 0.06092200428247452 + 0.01 * 7.204504489898682
Epoch 410, val loss: 0.8677138090133667
Epoch 420, training loss: 0.12546688318252563 = 0.05350378155708313 + 0.01 * 7.196310520172119
Epoch 420, val loss: 0.8833807706832886
Epoch 430, training loss: 0.1192927211523056 = 0.04727177694439888 + 0.01 * 7.202094078063965
Epoch 430, val loss: 0.8990420699119568
Epoch 440, training loss: 0.11383195221424103 = 0.04201198369264603 + 0.01 * 7.181996822357178
Epoch 440, val loss: 0.9145136475563049
Epoch 450, training loss: 0.10937456041574478 = 0.037539929151535034 + 0.01 * 7.1834635734558105
Epoch 450, val loss: 0.9297292232513428
Epoch 460, training loss: 0.1053709089756012 = 0.03372267261147499 + 0.01 * 7.16482400894165
Epoch 460, val loss: 0.944560170173645
Epoch 470, training loss: 0.10196129977703094 = 0.030444830656051636 + 0.01 * 7.151647090911865
Epoch 470, val loss: 0.9589287638664246
Epoch 480, training loss: 0.09902749955654144 = 0.027618039399385452 + 0.01 * 7.140946388244629
Epoch 480, val loss: 0.9728295207023621
Epoch 490, training loss: 0.09648282080888748 = 0.025166938081383705 + 0.01 * 7.131588935852051
Epoch 490, val loss: 0.9862908124923706
Epoch 500, training loss: 0.09411986172199249 = 0.023029718548059464 + 0.01 * 7.10901403427124
Epoch 500, val loss: 0.9991698265075684
Epoch 510, training loss: 0.09227684140205383 = 0.021161112934350967 + 0.01 * 7.111573219299316
Epoch 510, val loss: 1.011566162109375
Epoch 520, training loss: 0.09044815599918365 = 0.019518664106726646 + 0.01 * 7.092949390411377
Epoch 520, val loss: 1.023468255996704
Epoch 530, training loss: 0.08874839544296265 = 0.018066080287098885 + 0.01 * 7.06823205947876
Epoch 530, val loss: 1.0349200963974
Epoch 540, training loss: 0.08741054683923721 = 0.01677531562745571 + 0.01 * 7.063522815704346
Epoch 540, val loss: 1.0460244417190552
Epoch 550, training loss: 0.08622666448354721 = 0.01562679558992386 + 0.01 * 7.0599870681762695
Epoch 550, val loss: 1.056533694267273
Epoch 560, training loss: 0.0850992426276207 = 0.014597618021070957 + 0.01 * 7.0501627922058105
Epoch 560, val loss: 1.0667335987091064
Epoch 570, training loss: 0.08403432369232178 = 0.01367129571735859 + 0.01 * 7.0363030433654785
Epoch 570, val loss: 1.076612949371338
Epoch 580, training loss: 0.08314025402069092 = 0.01283505279570818 + 0.01 * 7.030519962310791
Epoch 580, val loss: 1.086105465888977
Epoch 590, training loss: 0.08265993744134903 = 0.01207747496664524 + 0.01 * 7.058246612548828
Epoch 590, val loss: 1.0952867269515991
Epoch 600, training loss: 0.08162901550531387 = 0.011391500011086464 + 0.01 * 7.023751735687256
Epoch 600, val loss: 1.1040244102478027
Epoch 610, training loss: 0.0807889997959137 = 0.010767139494419098 + 0.01 * 7.002185821533203
Epoch 610, val loss: 1.1125187873840332
Epoch 620, training loss: 0.08009617030620575 = 0.010195997543632984 + 0.01 * 6.990016937255859
Epoch 620, val loss: 1.1206837892532349
Epoch 630, training loss: 0.07951681315898895 = 0.009672201238572598 + 0.01 * 6.984461307525635
Epoch 630, val loss: 1.1286550760269165
Epoch 640, training loss: 0.07908597588539124 = 0.009191768243908882 + 0.01 * 6.9894208908081055
Epoch 640, val loss: 1.1362197399139404
Epoch 650, training loss: 0.0784483328461647 = 0.008750682696700096 + 0.01 * 6.969764709472656
Epoch 650, val loss: 1.1435580253601074
Epoch 660, training loss: 0.07799454033374786 = 0.008343750610947609 + 0.01 * 6.965079307556152
Epoch 660, val loss: 1.1506727933883667
Epoch 670, training loss: 0.07751530408859253 = 0.007967405021190643 + 0.01 * 6.954790115356445
Epoch 670, val loss: 1.1575227975845337
Epoch 680, training loss: 0.07749512791633606 = 0.007618463598191738 + 0.01 * 6.987666606903076
Epoch 680, val loss: 1.1642388105392456
Epoch 690, training loss: 0.07675672322511673 = 0.007295387331396341 + 0.01 * 6.946134090423584
Epoch 690, val loss: 1.170590877532959
Epoch 700, training loss: 0.07627401500940323 = 0.006994768511503935 + 0.01 * 6.927924633026123
Epoch 700, val loss: 1.1768709421157837
Epoch 710, training loss: 0.07601626217365265 = 0.006714387331157923 + 0.01 * 6.930187225341797
Epoch 710, val loss: 1.1828789710998535
Epoch 720, training loss: 0.07575584948062897 = 0.006452629808336496 + 0.01 * 6.930322647094727
Epoch 720, val loss: 1.1888504028320312
Epoch 730, training loss: 0.07541169971227646 = 0.006208854261785746 + 0.01 * 6.920284748077393
Epoch 730, val loss: 1.194420576095581
Epoch 740, training loss: 0.07516388595104218 = 0.00598052516579628 + 0.01 * 6.918336391448975
Epoch 740, val loss: 1.2000280618667603
Epoch 750, training loss: 0.07501085102558136 = 0.0057662660256028175 + 0.01 * 6.924458980560303
Epoch 750, val loss: 1.2053380012512207
Epoch 760, training loss: 0.07451052218675613 = 0.0055652037262916565 + 0.01 * 6.894531726837158
Epoch 760, val loss: 1.2105728387832642
Epoch 770, training loss: 0.0743110328912735 = 0.005375890992581844 + 0.01 * 6.893514156341553
Epoch 770, val loss: 1.2156448364257812
Epoch 780, training loss: 0.07427284866571426 = 0.005197557620704174 + 0.01 * 6.907529830932617
Epoch 780, val loss: 1.220510721206665
Epoch 790, training loss: 0.07385147362947464 = 0.005029481370002031 + 0.01 * 6.882199287414551
Epoch 790, val loss: 1.2253326177597046
Epoch 800, training loss: 0.07368257641792297 = 0.0048707216046750546 + 0.01 * 6.881186008453369
Epoch 800, val loss: 1.2300236225128174
Epoch 810, training loss: 0.07333739101886749 = 0.0047207786701619625 + 0.01 * 6.861661434173584
Epoch 810, val loss: 1.2344270944595337
Epoch 820, training loss: 0.07313668727874756 = 0.004578717518597841 + 0.01 * 6.855796813964844
Epoch 820, val loss: 1.2388874292373657
Epoch 830, training loss: 0.07308853417634964 = 0.0044443029910326 + 0.01 * 6.864423751831055
Epoch 830, val loss: 1.2430860996246338
Epoch 840, training loss: 0.07282672822475433 = 0.004316721111536026 + 0.01 * 6.8510003089904785
Epoch 840, val loss: 1.247365951538086
Epoch 850, training loss: 0.07290808856487274 = 0.004195998422801495 + 0.01 * 6.871209144592285
Epoch 850, val loss: 1.2512569427490234
Epoch 860, training loss: 0.0725504532456398 = 0.004081034101545811 + 0.01 * 6.846941947937012
Epoch 860, val loss: 1.2553101778030396
Epoch 870, training loss: 0.07244162261486053 = 0.003971814177930355 + 0.01 * 6.846981048583984
Epoch 870, val loss: 1.2591434717178345
Epoch 880, training loss: 0.07220924645662308 = 0.003867723047733307 + 0.01 * 6.8341522216796875
Epoch 880, val loss: 1.2628717422485352
Epoch 890, training loss: 0.07228885591030121 = 0.003768438706174493 + 0.01 * 6.852042198181152
Epoch 890, val loss: 1.2665305137634277
Epoch 900, training loss: 0.0719696581363678 = 0.0036739620845764875 + 0.01 * 6.829569339752197
Epoch 900, val loss: 1.2701047658920288
Epoch 910, training loss: 0.07222855091094971 = 0.0035835031885653734 + 0.01 * 6.864504337310791
Epoch 910, val loss: 1.2736979722976685
Epoch 920, training loss: 0.07166453450918198 = 0.003497225930914283 + 0.01 * 6.816730499267578
Epoch 920, val loss: 1.2771213054656982
Epoch 930, training loss: 0.07170595973730087 = 0.003414861159399152 + 0.01 * 6.829110145568848
Epoch 930, val loss: 1.2806198596954346
Epoch 940, training loss: 0.07153467833995819 = 0.0033363045658916235 + 0.01 * 6.81983757019043
Epoch 940, val loss: 1.2837175130844116
Epoch 950, training loss: 0.07139411568641663 = 0.0032608944457024336 + 0.01 * 6.813322067260742
Epoch 950, val loss: 1.2870539426803589
Epoch 960, training loss: 0.07145866006612778 = 0.0031886128708720207 + 0.01 * 6.827004909515381
Epoch 960, val loss: 1.290101408958435
Epoch 970, training loss: 0.07119672745466232 = 0.0031192894093692303 + 0.01 * 6.807744026184082
Epoch 970, val loss: 1.2933969497680664
Epoch 980, training loss: 0.07143436372280121 = 0.003052810439839959 + 0.01 * 6.838155746459961
Epoch 980, val loss: 1.296456217765808
Epoch 990, training loss: 0.07095158845186234 = 0.002989065134897828 + 0.01 * 6.796252250671387
Epoch 990, val loss: 1.2993742227554321
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 2.0334177017211914 = 1.9474495649337769 + 0.01 * 8.596814155578613
Epoch 0, val loss: 1.945741057395935
Epoch 10, training loss: 2.0238265991210938 = 1.9378588199615479 + 0.01 * 8.596776962280273
Epoch 10, val loss: 1.9363566637039185
Epoch 20, training loss: 2.011986494064331 = 1.926020622253418 + 0.01 * 8.596579551696777
Epoch 20, val loss: 1.9243403673171997
Epoch 30, training loss: 1.9951691627502441 = 1.909209966659546 + 0.01 * 8.595914840698242
Epoch 30, val loss: 1.9069050550460815
Epoch 40, training loss: 1.9703387022018433 = 1.884415626525879 + 0.01 * 8.592309951782227
Epoch 40, val loss: 1.881047248840332
Epoch 50, training loss: 1.9357366561889648 = 1.8500738143920898 + 0.01 * 8.566283226013184
Epoch 50, val loss: 1.846112847328186
Epoch 60, training loss: 1.896808385848999 = 1.812366008758545 + 0.01 * 8.444235801696777
Epoch 60, val loss: 1.8104640245437622
Epoch 70, training loss: 1.8649871349334717 = 1.7821687459945679 + 0.01 * 8.281841278076172
Epoch 70, val loss: 1.7843585014343262
Epoch 80, training loss: 1.826024055480957 = 1.7459853887557983 + 0.01 * 8.003865242004395
Epoch 80, val loss: 1.7530908584594727
Epoch 90, training loss: 1.771620750427246 = 1.6952238082885742 + 0.01 * 7.63969612121582
Epoch 90, val loss: 1.7105811834335327
Epoch 100, training loss: 1.699669361114502 = 1.6250320672988892 + 0.01 * 7.463733673095703
Epoch 100, val loss: 1.6525359153747559
Epoch 110, training loss: 1.6096490621566772 = 1.5358729362487793 + 0.01 * 7.3776092529296875
Epoch 110, val loss: 1.5789546966552734
Epoch 120, training loss: 1.5098276138305664 = 1.4365413188934326 + 0.01 * 7.3286261558532715
Epoch 120, val loss: 1.498767614364624
Epoch 130, training loss: 1.408420205116272 = 1.335360050201416 + 0.01 * 7.306013107299805
Epoch 130, val loss: 1.4184480905532837
Epoch 140, training loss: 1.30921471118927 = 1.2362231016159058 + 0.01 * 7.299159049987793
Epoch 140, val loss: 1.3411518335342407
Epoch 150, training loss: 1.213385820388794 = 1.1404403448104858 + 0.01 * 7.294553756713867
Epoch 150, val loss: 1.2675139904022217
Epoch 160, training loss: 1.1219757795333862 = 1.049109935760498 + 0.01 * 7.286581993103027
Epoch 160, val loss: 1.1991775035858154
Epoch 170, training loss: 1.035773754119873 = 0.963039219379425 + 0.01 * 7.273451805114746
Epoch 170, val loss: 1.1370705366134644
Epoch 180, training loss: 0.9549298286437988 = 0.8823620080947876 + 0.01 * 7.256781101226807
Epoch 180, val loss: 1.0803321599960327
Epoch 190, training loss: 0.8797186017036438 = 0.8073727488517761 + 0.01 * 7.234585285186768
Epoch 190, val loss: 1.0283713340759277
Epoch 200, training loss: 0.8106251358985901 = 0.7384801506996155 + 0.01 * 7.214496612548828
Epoch 200, val loss: 0.9816049337387085
Epoch 210, training loss: 0.7475616931915283 = 0.6756907105445862 + 0.01 * 7.187100410461426
Epoch 210, val loss: 0.9407299160957336
Epoch 220, training loss: 0.6895888447761536 = 0.6178544163703918 + 0.01 * 7.173441410064697
Epoch 220, val loss: 0.9057753086090088
Epoch 230, training loss: 0.6346818208694458 = 0.563186526298523 + 0.01 * 7.149529933929443
Epoch 230, val loss: 0.8759046196937561
Epoch 240, training loss: 0.5816637277603149 = 0.510278582572937 + 0.01 * 7.138514518737793
Epoch 240, val loss: 0.8504656553268433
Epoch 250, training loss: 0.5298972129821777 = 0.4585542678833008 + 0.01 * 7.134291648864746
Epoch 250, val loss: 0.8291126489639282
Epoch 260, training loss: 0.47937852144241333 = 0.4081818461418152 + 0.01 * 7.119668483734131
Epoch 260, val loss: 0.8118568658828735
Epoch 270, training loss: 0.43101662397384644 = 0.35991233587265015 + 0.01 * 7.110430717468262
Epoch 270, val loss: 0.7991761565208435
Epoch 280, training loss: 0.3858853578567505 = 0.3147633373737335 + 0.01 * 7.112203598022461
Epoch 280, val loss: 0.7913129329681396
Epoch 290, training loss: 0.34470587968826294 = 0.273623526096344 + 0.01 * 7.10823392868042
Epoch 290, val loss: 0.7882499694824219
Epoch 300, training loss: 0.3077842593193054 = 0.23697108030319214 + 0.01 * 7.081319332122803
Epoch 300, val loss: 0.7896109223365784
Epoch 310, training loss: 0.27561822533607483 = 0.20484715700149536 + 0.01 * 7.077107906341553
Epoch 310, val loss: 0.7947132587432861
Epoch 320, training loss: 0.2479291409254074 = 0.17705436050891876 + 0.01 * 7.087478160858154
Epoch 320, val loss: 0.80287766456604
Epoch 330, training loss: 0.22400528192520142 = 0.15326540172100067 + 0.01 * 7.07398796081543
Epoch 330, val loss: 0.8135263323783875
Epoch 340, training loss: 0.20363017916679382 = 0.13299335539340973 + 0.01 * 7.063682556152344
Epoch 340, val loss: 0.826113760471344
Epoch 350, training loss: 0.18633854389190674 = 0.1157454326748848 + 0.01 * 7.059311866760254
Epoch 350, val loss: 0.8403292894363403
Epoch 360, training loss: 0.17157843708992004 = 0.10111522674560547 + 0.01 * 7.04632043838501
Epoch 360, val loss: 0.8557736277580261
Epoch 370, training loss: 0.15914598107337952 = 0.08869154751300812 + 0.01 * 7.045442581176758
Epoch 370, val loss: 0.8720840811729431
Epoch 380, training loss: 0.14842724800109863 = 0.0781126469373703 + 0.01 * 7.031459808349609
Epoch 380, val loss: 0.8890016674995422
Epoch 390, training loss: 0.13945439457893372 = 0.06908351182937622 + 0.01 * 7.037087440490723
Epoch 390, val loss: 0.9062997102737427
Epoch 400, training loss: 0.13169072568416595 = 0.06137929856777191 + 0.01 * 7.0311431884765625
Epoch 400, val loss: 0.9236512780189514
Epoch 410, training loss: 0.12494170665740967 = 0.05478428676724434 + 0.01 * 7.01574182510376
Epoch 410, val loss: 0.9409667253494263
Epoch 420, training loss: 0.1192433089017868 = 0.04910576343536377 + 0.01 * 7.013754844665527
Epoch 420, val loss: 0.9581623077392578
Epoch 430, training loss: 0.11436930298805237 = 0.0442100428044796 + 0.01 * 7.015926361083984
Epoch 430, val loss: 0.975074827671051
Epoch 440, training loss: 0.10994589328765869 = 0.03997298330068588 + 0.01 * 6.997291088104248
Epoch 440, val loss: 0.9916510581970215
Epoch 450, training loss: 0.10627186298370361 = 0.036288224160671234 + 0.01 * 6.998363971710205
Epoch 450, val loss: 1.0078438520431519
Epoch 460, training loss: 0.1032034158706665 = 0.033073000609874725 + 0.01 * 7.0130414962768555
Epoch 460, val loss: 1.023629069328308
Epoch 470, training loss: 0.10001520067453384 = 0.030260106548666954 + 0.01 * 6.975509166717529
Epoch 470, val loss: 1.038942813873291
Epoch 480, training loss: 0.09767851233482361 = 0.027785763144493103 + 0.01 * 6.989274978637695
Epoch 480, val loss: 1.0537744760513306
Epoch 490, training loss: 0.09527764469385147 = 0.025603709742426872 + 0.01 * 6.967393398284912
Epoch 490, val loss: 1.0681096315383911
Epoch 500, training loss: 0.0932537168264389 = 0.023671772330999374 + 0.01 * 6.958194255828857
Epoch 500, val loss: 1.0819637775421143
Epoch 510, training loss: 0.09145314991474152 = 0.02195287123322487 + 0.01 * 6.950027942657471
Epoch 510, val loss: 1.0953823328018188
Epoch 520, training loss: 0.08999358862638474 = 0.020417461171746254 + 0.01 * 6.957612991333008
Epoch 520, val loss: 1.1083693504333496
Epoch 530, training loss: 0.08861105144023895 = 0.01904166303575039 + 0.01 * 6.956938743591309
Epoch 530, val loss: 1.1208893060684204
Epoch 540, training loss: 0.08724699169397354 = 0.017804687842726707 + 0.01 * 6.944230556488037
Epoch 540, val loss: 1.1329985857009888
Epoch 550, training loss: 0.08609919995069504 = 0.01668938435614109 + 0.01 * 6.940981864929199
Epoch 550, val loss: 1.144702434539795
Epoch 560, training loss: 0.08498241752386093 = 0.015680380165576935 + 0.01 * 6.930203914642334
Epoch 560, val loss: 1.1559895277023315
Epoch 570, training loss: 0.08384363353252411 = 0.014764871448278427 + 0.01 * 6.907876968383789
Epoch 570, val loss: 1.1669453382492065
Epoch 580, training loss: 0.08310214430093765 = 0.013930395245552063 + 0.01 * 6.91717529296875
Epoch 580, val loss: 1.1775259971618652
Epoch 590, training loss: 0.0820857584476471 = 0.013169283978641033 + 0.01 * 6.891647815704346
Epoch 590, val loss: 1.187773585319519
Epoch 600, training loss: 0.08155575394630432 = 0.01247277669608593 + 0.01 * 6.908298015594482
Epoch 600, val loss: 1.1976573467254639
Epoch 610, training loss: 0.08068320155143738 = 0.011833112686872482 + 0.01 * 6.885009288787842
Epoch 610, val loss: 1.2072474956512451
Epoch 620, training loss: 0.08006279170513153 = 0.01124432124197483 + 0.01 * 6.881847381591797
Epoch 620, val loss: 1.2165852785110474
Epoch 630, training loss: 0.07939413189888 = 0.010701389983296394 + 0.01 * 6.869274616241455
Epoch 630, val loss: 1.225587010383606
Epoch 640, training loss: 0.07904209941625595 = 0.010199862532317638 + 0.01 * 6.8842244148254395
Epoch 640, val loss: 1.2343250513076782
Epoch 650, training loss: 0.07838224619626999 = 0.009736103937029839 + 0.01 * 6.864614009857178
Epoch 650, val loss: 1.2428295612335205
Epoch 660, training loss: 0.07799826562404633 = 0.009305030107498169 + 0.01 * 6.86932373046875
Epoch 660, val loss: 1.2510865926742554
Epoch 670, training loss: 0.0774165615439415 = 0.008905152790248394 + 0.01 * 6.851140975952148
Epoch 670, val loss: 1.259141206741333
Epoch 680, training loss: 0.0771181732416153 = 0.008532574400305748 + 0.01 * 6.858560085296631
Epoch 680, val loss: 1.2669553756713867
Epoch 690, training loss: 0.07686710357666016 = 0.00818503275513649 + 0.01 * 6.868206977844238
Epoch 690, val loss: 1.2745600938796997
Epoch 700, training loss: 0.07638409733772278 = 0.007860898971557617 + 0.01 * 6.852319717407227
Epoch 700, val loss: 1.281964898109436
Epoch 710, training loss: 0.07595822960138321 = 0.007556946016848087 + 0.01 * 6.840128421783447
Epoch 710, val loss: 1.2891411781311035
Epoch 720, training loss: 0.07562234252691269 = 0.007272391114383936 + 0.01 * 6.834995269775391
Epoch 720, val loss: 1.296234130859375
Epoch 730, training loss: 0.07584007829427719 = 0.007005204446613789 + 0.01 * 6.883487224578857
Epoch 730, val loss: 1.3031415939331055
Epoch 740, training loss: 0.07504058629274368 = 0.006754550617188215 + 0.01 * 6.828603744506836
Epoch 740, val loss: 1.3097670078277588
Epoch 750, training loss: 0.07490308582782745 = 0.006518365815281868 + 0.01 * 6.838472366333008
Epoch 750, val loss: 1.3162962198257446
Epoch 760, training loss: 0.07463746517896652 = 0.006296406500041485 + 0.01 * 6.834105968475342
Epoch 760, val loss: 1.3226468563079834
Epoch 770, training loss: 0.0741952657699585 = 0.006086478475481272 + 0.01 * 6.810879230499268
Epoch 770, val loss: 1.3288999795913696
Epoch 780, training loss: 0.07399946451187134 = 0.005888618528842926 + 0.01 * 6.811084747314453
Epoch 780, val loss: 1.3349709510803223
Epoch 790, training loss: 0.07390708476305008 = 0.0057008941657841206 + 0.01 * 6.820619583129883
Epoch 790, val loss: 1.3409210443496704
Epoch 800, training loss: 0.07356255501508713 = 0.005524246022105217 + 0.01 * 6.803830623626709
Epoch 800, val loss: 1.3466954231262207
Epoch 810, training loss: 0.07320437580347061 = 0.005356115289032459 + 0.01 * 6.784825801849365
Epoch 810, val loss: 1.3523613214492798
Epoch 820, training loss: 0.07345426082611084 = 0.005196679383516312 + 0.01 * 6.825758457183838
Epoch 820, val loss: 1.3578757047653198
Epoch 830, training loss: 0.072950579226017 = 0.005043765529990196 + 0.01 * 6.7906813621521
Epoch 830, val loss: 1.36324942111969
Epoch 840, training loss: 0.07298614829778671 = 0.004898136015981436 + 0.01 * 6.808801174163818
Epoch 840, val loss: 1.368564486503601
Epoch 850, training loss: 0.07266758382320404 = 0.004760734736919403 + 0.01 * 6.790685176849365
Epoch 850, val loss: 1.3737518787384033
Epoch 860, training loss: 0.07247476279735565 = 0.004629848059266806 + 0.01 * 6.784491062164307
Epoch 860, val loss: 1.3788878917694092
Epoch 870, training loss: 0.07228308171033859 = 0.0045053306967020035 + 0.01 * 6.777775764465332
Epoch 870, val loss: 1.3839207887649536
Epoch 880, training loss: 0.07203246653079987 = 0.00438728416338563 + 0.01 * 6.7645182609558105
Epoch 880, val loss: 1.3888499736785889
Epoch 890, training loss: 0.0719834417104721 = 0.00427474919706583 + 0.01 * 6.770869255065918
Epoch 890, val loss: 1.3935623168945312
Epoch 900, training loss: 0.07184981554746628 = 0.00416718702763319 + 0.01 * 6.768263339996338
Epoch 900, val loss: 1.3981975317001343
Epoch 910, training loss: 0.07170268148183823 = 0.004064629320055246 + 0.01 * 6.763805866241455
Epoch 910, val loss: 1.402799367904663
Epoch 920, training loss: 0.0714878961443901 = 0.003966368269175291 + 0.01 * 6.752152919769287
Epoch 920, val loss: 1.4073152542114258
Epoch 930, training loss: 0.07162246853113174 = 0.003872585017234087 + 0.01 * 6.774988174438477
Epoch 930, val loss: 1.4117087125778198
Epoch 940, training loss: 0.07148661464452744 = 0.0037828567437827587 + 0.01 * 6.770376205444336
Epoch 940, val loss: 1.4160058498382568
Epoch 950, training loss: 0.07110118865966797 = 0.0036967310588806868 + 0.01 * 6.740445613861084
Epoch 950, val loss: 1.4202460050582886
Epoch 960, training loss: 0.07104118168354034 = 0.003614263841882348 + 0.01 * 6.742692470550537
Epoch 960, val loss: 1.4244176149368286
Epoch 970, training loss: 0.07131931930780411 = 0.0035351356491446495 + 0.01 * 6.778418064117432
Epoch 970, val loss: 1.4285269975662231
Epoch 980, training loss: 0.07080844789743423 = 0.0034592938609421253 + 0.01 * 6.734915256500244
Epoch 980, val loss: 1.4324055910110474
Epoch 990, training loss: 0.07103917747735977 = 0.003386460477486253 + 0.01 * 6.7652716636657715
Epoch 990, val loss: 1.4363715648651123
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 2.049982786178589 = 1.9640145301818848 + 0.01 * 8.596819877624512
Epoch 0, val loss: 1.9653489589691162
Epoch 10, training loss: 2.0389297008514404 = 1.952962040901184 + 0.01 * 8.596763610839844
Epoch 10, val loss: 1.9537633657455444
Epoch 20, training loss: 2.0259265899658203 = 1.9399608373641968 + 0.01 * 8.596578598022461
Epoch 20, val loss: 1.9402016401290894
Epoch 30, training loss: 2.0085012912750244 = 1.9225412607192993 + 0.01 * 8.59599494934082
Epoch 30, val loss: 1.9221720695495605
Epoch 40, training loss: 1.9835931062698364 = 1.8976662158966064 + 0.01 * 8.592694282531738
Epoch 40, val loss: 1.8965504169464111
Epoch 50, training loss: 1.948711633682251 = 1.863046407699585 + 0.01 * 8.566520690917969
Epoch 50, val loss: 1.8619189262390137
Epoch 60, training loss: 1.9071627855300903 = 1.8229135274887085 + 0.01 * 8.42492961883545
Epoch 60, val loss: 1.8248043060302734
Epoch 70, training loss: 1.8719737529754639 = 1.789077639579773 + 0.01 * 8.289610862731934
Epoch 70, val loss: 1.796342372894287
Epoch 80, training loss: 1.8355541229248047 = 1.7543400526046753 + 0.01 * 8.121411323547363
Epoch 80, val loss: 1.7650667428970337
Epoch 90, training loss: 1.7845314741134644 = 1.7063130140304565 + 0.01 * 7.821846961975098
Epoch 90, val loss: 1.722097635269165
Epoch 100, training loss: 1.7161755561828613 = 1.6404218673706055 + 0.01 * 7.575374126434326
Epoch 100, val loss: 1.6663539409637451
Epoch 110, training loss: 1.6316945552825928 = 1.5580204725265503 + 0.01 * 7.3674139976501465
Epoch 110, val loss: 1.5998458862304688
Epoch 120, training loss: 1.542927622795105 = 1.4699633121490479 + 0.01 * 7.29642915725708
Epoch 120, val loss: 1.5296238660812378
Epoch 130, training loss: 1.4578609466552734 = 1.3853328227996826 + 0.01 * 7.252810001373291
Epoch 130, val loss: 1.46303129196167
Epoch 140, training loss: 1.3762978315353394 = 1.3040298223495483 + 0.01 * 7.226806163787842
Epoch 140, val loss: 1.4004822969436646
Epoch 150, training loss: 1.2945616245269775 = 1.2225970029830933 + 0.01 * 7.196465492248535
Epoch 150, val loss: 1.3395116329193115
Epoch 160, training loss: 1.2115058898925781 = 1.1399089097976685 + 0.01 * 7.159700870513916
Epoch 160, val loss: 1.278870940208435
Epoch 170, training loss: 1.1271902322769165 = 1.055837869644165 + 0.01 * 7.135240077972412
Epoch 170, val loss: 1.218308687210083
Epoch 180, training loss: 1.04203462600708 = 0.9709436297416687 + 0.01 * 7.109099388122559
Epoch 180, val loss: 1.1569595336914062
Epoch 190, training loss: 0.9585586786270142 = 0.8876523375511169 + 0.01 * 7.090632438659668
Epoch 190, val loss: 1.0959607362747192
Epoch 200, training loss: 0.8805654048919678 = 0.8098514080047607 + 0.01 * 7.071399211883545
Epoch 200, val loss: 1.0396103858947754
Epoch 210, training loss: 0.8108726739883423 = 0.7402467727661133 + 0.01 * 7.062591552734375
Epoch 210, val loss: 0.9905650019645691
Epoch 220, training loss: 0.7493918538093567 = 0.6788434386253357 + 0.01 * 7.054842948913574
Epoch 220, val loss: 0.9493277072906494
Epoch 230, training loss: 0.6938751935958862 = 0.6233482360839844 + 0.01 * 7.052694797515869
Epoch 230, val loss: 0.9145299792289734
Epoch 240, training loss: 0.6410731077194214 = 0.570609986782074 + 0.01 * 7.04631233215332
Epoch 240, val loss: 0.8842164874076843
Epoch 250, training loss: 0.5891523361206055 = 0.518665075302124 + 0.01 * 7.048727035522461
Epoch 250, val loss: 0.8573274612426758
Epoch 260, training loss: 0.538009762763977 = 0.4675954282283783 + 0.01 * 7.041432857513428
Epoch 260, val loss: 0.8343642354011536
Epoch 270, training loss: 0.4893682897090912 = 0.41899165511131287 + 0.01 * 7.037662506103516
Epoch 270, val loss: 0.8166894316673279
Epoch 280, training loss: 0.44483283162117004 = 0.37447959184646606 + 0.01 * 7.035325050354004
Epoch 280, val loss: 0.805103063583374
Epoch 290, training loss: 0.4050063192844391 = 0.3346773684024811 + 0.01 * 7.032895565032959
Epoch 290, val loss: 0.7991576194763184
Epoch 300, training loss: 0.36945658922195435 = 0.299144983291626 + 0.01 * 7.031160354614258
Epoch 300, val loss: 0.7974795699119568
Epoch 310, training loss: 0.3374101221561432 = 0.2671148478984833 + 0.01 * 7.02952766418457
Epoch 310, val loss: 0.7990931868553162
Epoch 320, training loss: 0.30832892656326294 = 0.23800918459892273 + 0.01 * 7.031974792480469
Epoch 320, val loss: 0.8034469485282898
Epoch 330, training loss: 0.28188881278038025 = 0.21161840856075287 + 0.01 * 7.027041435241699
Epoch 330, val loss: 0.8101731538772583
Epoch 340, training loss: 0.25818976759910583 = 0.18792550265789032 + 0.01 * 7.026426792144775
Epoch 340, val loss: 0.8192879557609558
Epoch 350, training loss: 0.2371567338705063 = 0.16687601804733276 + 0.01 * 7.028071880340576
Epoch 350, val loss: 0.8304998874664307
Epoch 360, training loss: 0.21853876113891602 = 0.1483159214258194 + 0.01 * 7.022284984588623
Epoch 360, val loss: 0.8434360027313232
Epoch 370, training loss: 0.20220181345939636 = 0.13198260962963104 + 0.01 * 7.0219197273254395
Epoch 370, val loss: 0.8576966524124146
Epoch 380, training loss: 0.18779051303863525 = 0.11759784072637558 + 0.01 * 7.0192670822143555
Epoch 380, val loss: 0.8730522990226746
Epoch 390, training loss: 0.1751321256160736 = 0.10492125898599625 + 0.01 * 7.021087646484375
Epoch 390, val loss: 0.889151394367218
Epoch 400, training loss: 0.16391333937644958 = 0.09374844282865524 + 0.01 * 7.016489028930664
Epoch 400, val loss: 0.9058083891868591
Epoch 410, training loss: 0.15402406454086304 = 0.08389529585838318 + 0.01 * 7.012876033782959
Epoch 410, val loss: 0.9228386878967285
Epoch 420, training loss: 0.1452905535697937 = 0.0752047523856163 + 0.01 * 7.008579730987549
Epoch 420, val loss: 0.9401403069496155
Epoch 430, training loss: 0.13762497901916504 = 0.06754608452320099 + 0.01 * 7.0078887939453125
Epoch 430, val loss: 0.9575743675231934
Epoch 440, training loss: 0.13083285093307495 = 0.060804583132267 + 0.01 * 7.00282621383667
Epoch 440, val loss: 0.9750039577484131
Epoch 450, training loss: 0.12488281726837158 = 0.05486474931240082 + 0.01 * 7.001806735992432
Epoch 450, val loss: 0.9924440979957581
Epoch 460, training loss: 0.11961455643177032 = 0.04962785914540291 + 0.01 * 6.9986701011657715
Epoch 460, val loss: 1.009764552116394
Epoch 470, training loss: 0.11497962474822998 = 0.04500866308808327 + 0.01 * 6.997095584869385
Epoch 470, val loss: 1.0269478559494019
Epoch 480, training loss: 0.11083152890205383 = 0.040932122617959976 + 0.01 * 6.989940166473389
Epoch 480, val loss: 1.0439035892486572
Epoch 490, training loss: 0.10738015174865723 = 0.0373292900621891 + 0.01 * 7.0050859451293945
Epoch 490, val loss: 1.0605566501617432
Epoch 500, training loss: 0.10398522019386292 = 0.03414367511868477 + 0.01 * 6.984154224395752
Epoch 500, val loss: 1.07685387134552
Epoch 510, training loss: 0.10114994645118713 = 0.031318750232458115 + 0.01 * 6.983120441436768
Epoch 510, val loss: 1.0928295850753784
Epoch 520, training loss: 0.0985775962471962 = 0.028807787224650383 + 0.01 * 6.976981163024902
Epoch 520, val loss: 1.1083555221557617
Epoch 530, training loss: 0.09632755070924759 = 0.026570701971650124 + 0.01 * 6.975685119628906
Epoch 530, val loss: 1.1234997510910034
Epoch 540, training loss: 0.09429427236318588 = 0.02457273192703724 + 0.01 * 6.972154140472412
Epoch 540, val loss: 1.138235092163086
Epoch 550, training loss: 0.09243378043174744 = 0.022782061249017715 + 0.01 * 6.965171813964844
Epoch 550, val loss: 1.1525603532791138
Epoch 560, training loss: 0.09128975123167038 = 0.021171322092413902 + 0.01 * 7.011842727661133
Epoch 560, val loss: 1.1664923429489136
Epoch 570, training loss: 0.08944801986217499 = 0.019722726196050644 + 0.01 * 6.972529888153076
Epoch 570, val loss: 1.1798515319824219
Epoch 580, training loss: 0.08800294250249863 = 0.018413037061691284 + 0.01 * 6.958990573883057
Epoch 580, val loss: 1.1928101778030396
Epoch 590, training loss: 0.0867462232708931 = 0.017224451526999474 + 0.01 * 6.952177047729492
Epoch 590, val loss: 1.2053720951080322
Epoch 600, training loss: 0.08563172817230225 = 0.01614239625632763 + 0.01 * 6.948933124542236
Epoch 600, val loss: 1.2175198793411255
Epoch 610, training loss: 0.08483917266130447 = 0.015155397355556488 + 0.01 * 6.968377590179443
Epoch 610, val loss: 1.2292771339416504
Epoch 620, training loss: 0.08373124152421951 = 0.014254610054194927 + 0.01 * 6.9476637840271
Epoch 620, val loss: 1.2406365871429443
Epoch 630, training loss: 0.08285167813301086 = 0.01343021634966135 + 0.01 * 6.9421467781066895
Epoch 630, val loss: 1.2515324354171753
Epoch 640, training loss: 0.08202627301216125 = 0.012674224562942982 + 0.01 * 6.935204982757568
Epoch 640, val loss: 1.262073040008545
Epoch 650, training loss: 0.08154313266277313 = 0.011980093084275723 + 0.01 * 6.95630407333374
Epoch 650, val loss: 1.2722488641738892
Epoch 660, training loss: 0.08070358633995056 = 0.011342640966176987 + 0.01 * 6.936094284057617
Epoch 660, val loss: 1.2820836305618286
Epoch 670, training loss: 0.08006604015827179 = 0.01075539831072092 + 0.01 * 6.931064128875732
Epoch 670, val loss: 1.2915252447128296
Epoch 680, training loss: 0.07942301034927368 = 0.010213294997811317 + 0.01 * 6.920971870422363
Epoch 680, val loss: 1.3006563186645508
Epoch 690, training loss: 0.07909924536943436 = 0.00971190631389618 + 0.01 * 6.93873405456543
Epoch 690, val loss: 1.3095139265060425
Epoch 700, training loss: 0.0784723162651062 = 0.009248379617929459 + 0.01 * 6.922394275665283
Epoch 700, val loss: 1.3179526329040527
Epoch 710, training loss: 0.0779343992471695 = 0.00881890021264553 + 0.01 * 6.9115495681762695
Epoch 710, val loss: 1.3261834383010864
Epoch 720, training loss: 0.07766593992710114 = 0.008419807069003582 + 0.01 * 6.9246134757995605
Epoch 720, val loss: 1.3341946601867676
Epoch 730, training loss: 0.07723276317119598 = 0.00804906152188778 + 0.01 * 6.918369770050049
Epoch 730, val loss: 1.341812014579773
Epoch 740, training loss: 0.07680005580186844 = 0.007703735958784819 + 0.01 * 6.909631729125977
Epoch 740, val loss: 1.3493077754974365
Epoch 750, training loss: 0.07646545767784119 = 0.0073814853094518185 + 0.01 * 6.908397197723389
Epoch 750, val loss: 1.3564389944076538
Epoch 760, training loss: 0.07603085041046143 = 0.007081074174493551 + 0.01 * 6.894977569580078
Epoch 760, val loss: 1.3634058237075806
Epoch 770, training loss: 0.07577667385339737 = 0.0068002077750861645 + 0.01 * 6.897646903991699
Epoch 770, val loss: 1.3701890707015991
Epoch 780, training loss: 0.07555200159549713 = 0.006537208799272776 + 0.01 * 6.901479721069336
Epoch 780, val loss: 1.376715064048767
Epoch 790, training loss: 0.07519008219242096 = 0.006290798541158438 + 0.01 * 6.889928340911865
Epoch 790, val loss: 1.383001685142517
Epoch 800, training loss: 0.07502575218677521 = 0.0060593318194150925 + 0.01 * 6.896642208099365
Epoch 800, val loss: 1.3891634941101074
Epoch 810, training loss: 0.07473485171794891 = 0.0058416747488081455 + 0.01 * 6.889317989349365
Epoch 810, val loss: 1.3950833082199097
Epoch 820, training loss: 0.07449182122945786 = 0.0056367297656834126 + 0.01 * 6.885509014129639
Epoch 820, val loss: 1.4008618593215942
Epoch 830, training loss: 0.07425564527511597 = 0.005443630274385214 + 0.01 * 6.881201267242432
Epoch 830, val loss: 1.4064621925354004
Epoch 840, training loss: 0.07404773682355881 = 0.00526169640943408 + 0.01 * 6.878603935241699
Epoch 840, val loss: 1.4118965864181519
Epoch 850, training loss: 0.0737604945898056 = 0.005089969374239445 + 0.01 * 6.867053031921387
Epoch 850, val loss: 1.417130470275879
Epoch 860, training loss: 0.07384732365608215 = 0.0049274638295173645 + 0.01 * 6.891985893249512
Epoch 860, val loss: 1.4222795963287354
Epoch 870, training loss: 0.07351094484329224 = 0.004773829597979784 + 0.01 * 6.873711585998535
Epoch 870, val loss: 1.427190899848938
Epoch 880, training loss: 0.07331457734107971 = 0.004628265742212534 + 0.01 * 6.868631362915039
Epoch 880, val loss: 1.4320207834243774
Epoch 890, training loss: 0.07311646640300751 = 0.004490370396524668 + 0.01 * 6.86260986328125
Epoch 890, val loss: 1.436682105064392
Epoch 900, training loss: 0.07299208641052246 = 0.004359439946711063 + 0.01 * 6.863265037536621
Epoch 900, val loss: 1.441233515739441
Epoch 910, training loss: 0.0728524699807167 = 0.004235210362821817 + 0.01 * 6.8617262840271
Epoch 910, val loss: 1.4455410242080688
Epoch 920, training loss: 0.07265106588602066 = 0.004117016680538654 + 0.01 * 6.853404521942139
Epoch 920, val loss: 1.4497908353805542
Epoch 930, training loss: 0.07253571599721909 = 0.00400472991168499 + 0.01 * 6.853098392486572
Epoch 930, val loss: 1.4539263248443604
Epoch 940, training loss: 0.07243320345878601 = 0.003897997085005045 + 0.01 * 6.853521347045898
Epoch 940, val loss: 1.4579399824142456
Epoch 950, training loss: 0.07220577448606491 = 0.0037962342612445354 + 0.01 * 6.840954303741455
Epoch 950, val loss: 1.4618335962295532
Epoch 960, training loss: 0.07221299409866333 = 0.003699253313243389 + 0.01 * 6.851374626159668
Epoch 960, val loss: 1.4655981063842773
Epoch 970, training loss: 0.07198357582092285 = 0.003606730606406927 + 0.01 * 6.8376851081848145
Epoch 970, val loss: 1.4692895412445068
Epoch 980, training loss: 0.0718081146478653 = 0.0035182421561330557 + 0.01 * 6.828988075256348
Epoch 980, val loss: 1.472908616065979
Epoch 990, training loss: 0.07189464569091797 = 0.003433742094784975 + 0.01 * 6.846090316772461
Epoch 990, val loss: 1.4763596057891846
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8065366367949395
The final CL Acc:0.78519, 0.01090, The final GNN Acc:0.81075, 0.00301
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13230])
remove edge: torch.Size([2, 7940])
updated graph: torch.Size([2, 10614])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.023375988006592 = 1.9374074935913086 + 0.01 * 8.596839904785156
Epoch 0, val loss: 1.9422173500061035
Epoch 10, training loss: 2.013143539428711 = 1.927175521850586 + 0.01 * 8.596796035766602
Epoch 10, val loss: 1.9322189092636108
Epoch 20, training loss: 2.0005342960357666 = 1.9145679473876953 + 0.01 * 8.596631050109863
Epoch 20, val loss: 1.919403076171875
Epoch 30, training loss: 1.9829363822937012 = 1.8969753980636597 + 0.01 * 8.596104621887207
Epoch 30, val loss: 1.9010587930679321
Epoch 40, training loss: 1.9575831890106201 = 1.8716498613357544 + 0.01 * 8.59333324432373
Epoch 40, val loss: 1.8747751712799072
Epoch 50, training loss: 1.9231079816818237 = 1.8373733758926392 + 0.01 * 8.573461532592773
Epoch 50, val loss: 1.840703010559082
Epoch 60, training loss: 1.8838768005371094 = 1.7991060018539429 + 0.01 * 8.477083206176758
Epoch 60, val loss: 1.8057917356491089
Epoch 70, training loss: 1.8457788228988647 = 1.7631409168243408 + 0.01 * 8.263788223266602
Epoch 70, val loss: 1.7748541831970215
Epoch 80, training loss: 1.7981990575790405 = 1.7169973850250244 + 0.01 * 8.120168685913086
Epoch 80, val loss: 1.7334829568862915
Epoch 90, training loss: 1.731547474861145 = 1.6521131992340088 + 0.01 * 7.94342565536499
Epoch 90, val loss: 1.67708420753479
Epoch 100, training loss: 1.6454371213912964 = 1.567625641822815 + 0.01 * 7.781153678894043
Epoch 100, val loss: 1.6063803434371948
Epoch 110, training loss: 1.546730399131775 = 1.4707927703857422 + 0.01 * 7.5937676429748535
Epoch 110, val loss: 1.5243724584579468
Epoch 120, training loss: 1.4437432289123535 = 1.3682904243469238 + 0.01 * 7.545281410217285
Epoch 120, val loss: 1.4378904104232788
Epoch 130, training loss: 1.3377960920333862 = 1.2628978490829468 + 0.01 * 7.489819526672363
Epoch 130, val loss: 1.350095510482788
Epoch 140, training loss: 1.230773687362671 = 1.1564867496490479 + 0.01 * 7.4286932945251465
Epoch 140, val loss: 1.263664960861206
Epoch 150, training loss: 1.1265769004821777 = 1.0528159141540527 + 0.01 * 7.376101970672607
Epoch 150, val loss: 1.1809766292572021
Epoch 160, training loss: 1.0283483266830444 = 0.9549176692962646 + 0.01 * 7.343063831329346
Epoch 160, val loss: 1.1042659282684326
Epoch 170, training loss: 0.9369964599609375 = 0.8637279868125916 + 0.01 * 7.326847553253174
Epoch 170, val loss: 1.0339910984039307
Epoch 180, training loss: 0.8524648547172546 = 0.7793605327606201 + 0.01 * 7.3104329109191895
Epoch 180, val loss: 0.9695650339126587
Epoch 190, training loss: 0.7751830220222473 = 0.7022703289985657 + 0.01 * 7.291269302368164
Epoch 190, val loss: 0.9122519493103027
Epoch 200, training loss: 0.7055820822715759 = 0.6329007744789124 + 0.01 * 7.268128871917725
Epoch 200, val loss: 0.8634908199310303
Epoch 210, training loss: 0.6441385746002197 = 0.5716567635536194 + 0.01 * 7.248183727264404
Epoch 210, val loss: 0.8241063952445984
Epoch 220, training loss: 0.5905651450157166 = 0.518315851688385 + 0.01 * 7.22492790222168
Epoch 220, val loss: 0.7939527034759521
Epoch 230, training loss: 0.5438705682754517 = 0.47179949283599854 + 0.01 * 7.207106113433838
Epoch 230, val loss: 0.7719414234161377
Epoch 240, training loss: 0.5023025274276733 = 0.4303666651248932 + 0.01 * 7.1935858726501465
Epoch 240, val loss: 0.7562569379806519
Epoch 250, training loss: 0.46395063400268555 = 0.3920876979827881 + 0.01 * 7.1862945556640625
Epoch 250, val loss: 0.7451148629188538
Epoch 260, training loss: 0.4272381365299225 = 0.35541048645973206 + 0.01 * 7.182765483856201
Epoch 260, val loss: 0.7367239594459534
Epoch 270, training loss: 0.3911987543106079 = 0.31941765546798706 + 0.01 * 7.178109169006348
Epoch 270, val loss: 0.729880690574646
Epoch 280, training loss: 0.3555269241333008 = 0.2837895154953003 + 0.01 * 7.173741340637207
Epoch 280, val loss: 0.7239274382591248
Epoch 290, training loss: 0.32070425152778625 = 0.24899396300315857 + 0.01 * 7.171028137207031
Epoch 290, val loss: 0.7188047170639038
Epoch 300, training loss: 0.2879883050918579 = 0.21634738147258759 + 0.01 * 7.164091110229492
Epoch 300, val loss: 0.7151214480400085
Epoch 310, training loss: 0.25881144404411316 = 0.18721312284469604 + 0.01 * 7.159831523895264
Epoch 310, val loss: 0.7133453488349915
Epoch 320, training loss: 0.23370179533958435 = 0.16213259100914001 + 0.01 * 7.156920433044434
Epoch 320, val loss: 0.7137619256973267
Epoch 330, training loss: 0.21233870089054108 = 0.140883207321167 + 0.01 * 7.145549297332764
Epoch 330, val loss: 0.7163114547729492
Epoch 340, training loss: 0.19430534541606903 = 0.12292415648698807 + 0.01 * 7.138119220733643
Epoch 340, val loss: 0.7208157777786255
Epoch 350, training loss: 0.17904603481292725 = 0.10770585387945175 + 0.01 * 7.134018421173096
Epoch 350, val loss: 0.7269684672355652
Epoch 360, training loss: 0.16600602865219116 = 0.09474483132362366 + 0.01 * 7.126120567321777
Epoch 360, val loss: 0.7345181703567505
Epoch 370, training loss: 0.15480923652648926 = 0.08364181220531464 + 0.01 * 7.11674165725708
Epoch 370, val loss: 0.7431585192680359
Epoch 380, training loss: 0.14523105323314667 = 0.07408588379621506 + 0.01 * 7.1145172119140625
Epoch 380, val loss: 0.7526862621307373
Epoch 390, training loss: 0.1369171440601349 = 0.0658406913280487 + 0.01 * 7.107645511627197
Epoch 390, val loss: 0.7627745270729065
Epoch 400, training loss: 0.12964265048503876 = 0.05870526656508446 + 0.01 * 7.093739032745361
Epoch 400, val loss: 0.7732810974121094
Epoch 410, training loss: 0.12338942289352417 = 0.052514705806970596 + 0.01 * 7.087471961975098
Epoch 410, val loss: 0.784032940864563
Epoch 420, training loss: 0.11800375580787659 = 0.047133419662714005 + 0.01 * 7.087033748626709
Epoch 420, val loss: 0.7948974370956421
Epoch 430, training loss: 0.11321964859962463 = 0.04244894161820412 + 0.01 * 7.077070236206055
Epoch 430, val loss: 0.8057710528373718
Epoch 440, training loss: 0.10907775163650513 = 0.03836025670170784 + 0.01 * 7.071749210357666
Epoch 440, val loss: 0.8165919780731201
Epoch 450, training loss: 0.10546927154064178 = 0.03478323295712471 + 0.01 * 7.068603515625
Epoch 450, val loss: 0.8272942304611206
Epoch 460, training loss: 0.10233604907989502 = 0.031646762043237686 + 0.01 * 7.068928241729736
Epoch 460, val loss: 0.8378625512123108
Epoch 470, training loss: 0.099543496966362 = 0.028889192268252373 + 0.01 * 7.065430164337158
Epoch 470, val loss: 0.8482340574264526
Epoch 480, training loss: 0.09701858460903168 = 0.02645854279398918 + 0.01 * 7.056004524230957
Epoch 480, val loss: 0.8584071397781372
Epoch 490, training loss: 0.09478642791509628 = 0.024309245869517326 + 0.01 * 7.047718524932861
Epoch 490, val loss: 0.8683322072029114
Epoch 500, training loss: 0.09292618185281754 = 0.022402450442314148 + 0.01 * 7.05237340927124
Epoch 500, val loss: 0.8780331611633301
Epoch 510, training loss: 0.09118145704269409 = 0.02070664055645466 + 0.01 * 7.047482013702393
Epoch 510, val loss: 0.8875164985656738
Epoch 520, training loss: 0.08959154039621353 = 0.019193075597286224 + 0.01 * 7.039846897125244
Epoch 520, val loss: 0.8967386484146118
Epoch 530, training loss: 0.08813825249671936 = 0.017837822437286377 + 0.01 * 7.030043125152588
Epoch 530, val loss: 0.9057300090789795
Epoch 540, training loss: 0.08700137585401535 = 0.016620388254523277 + 0.01 * 7.0380988121032715
Epoch 540, val loss: 0.9144863486289978
Epoch 550, training loss: 0.0858277678489685 = 0.01552432868629694 + 0.01 * 7.030344009399414
Epoch 550, val loss: 0.9229838252067566
Epoch 560, training loss: 0.08473605662584305 = 0.01453397423028946 + 0.01 * 7.020208358764648
Epoch 560, val loss: 0.931248664855957
Epoch 570, training loss: 0.08374454081058502 = 0.013636728748679161 + 0.01 * 7.0107808113098145
Epoch 570, val loss: 0.9392771124839783
Epoch 580, training loss: 0.08293768763542175 = 0.0128218038007617 + 0.01 * 7.0115885734558105
Epoch 580, val loss: 0.9470705986022949
Epoch 590, training loss: 0.08220633119344711 = 0.012079567648470402 + 0.01 * 7.012676239013672
Epoch 590, val loss: 0.9546661376953125
Epoch 600, training loss: 0.0814022570848465 = 0.011401604861021042 + 0.01 * 7.000064849853516
Epoch 600, val loss: 0.9620299339294434
Epoch 610, training loss: 0.0807412788271904 = 0.01078155729919672 + 0.01 * 6.995972633361816
Epoch 610, val loss: 0.9691843390464783
Epoch 620, training loss: 0.08009055256843567 = 0.01021311990916729 + 0.01 * 6.987742900848389
Epoch 620, val loss: 0.9761341214179993
Epoch 630, training loss: 0.07961520552635193 = 0.009690091013908386 + 0.01 * 6.992511749267578
Epoch 630, val loss: 0.9828656315803528
Epoch 640, training loss: 0.07906848192214966 = 0.009208550676703453 + 0.01 * 6.985993385314941
Epoch 640, val loss: 0.9894439578056335
Epoch 650, training loss: 0.07854142785072327 = 0.008763615973293781 + 0.01 * 6.977781295776367
Epoch 650, val loss: 0.995819091796875
Epoch 660, training loss: 0.07824626564979553 = 0.008352158591151237 + 0.01 * 6.989410400390625
Epoch 660, val loss: 1.001999855041504
Epoch 670, training loss: 0.07776755839586258 = 0.007971285842359066 + 0.01 * 6.97962760925293
Epoch 670, val loss: 1.0080455541610718
Epoch 680, training loss: 0.077235646545887 = 0.007617604918777943 + 0.01 * 6.961804389953613
Epoch 680, val loss: 1.0138945579528809
Epoch 690, training loss: 0.0770074650645256 = 0.007288974244147539 + 0.01 * 6.971848964691162
Epoch 690, val loss: 1.0196056365966797
Epoch 700, training loss: 0.07648725807666779 = 0.006982684135437012 + 0.01 * 6.950457572937012
Epoch 700, val loss: 1.025162696838379
Epoch 710, training loss: 0.0764678344130516 = 0.006696647964417934 + 0.01 * 6.977118968963623
Epoch 710, val loss: 1.0305927991867065
Epoch 720, training loss: 0.07600583136081696 = 0.006429350469261408 + 0.01 * 6.957648277282715
Epoch 720, val loss: 1.0358160734176636
Epoch 730, training loss: 0.07565825432538986 = 0.006179713178426027 + 0.01 * 6.947854518890381
Epoch 730, val loss: 1.0409375429153442
Epoch 740, training loss: 0.07532064616680145 = 0.005945576820522547 + 0.01 * 6.937507152557373
Epoch 740, val loss: 1.045894742012024
Epoch 750, training loss: 0.07498808205127716 = 0.005726265721023083 + 0.01 * 6.926182270050049
Epoch 750, val loss: 1.0507303476333618
Epoch 760, training loss: 0.07478858530521393 = 0.005519985221326351 + 0.01 * 6.9268598556518555
Epoch 760, val loss: 1.0554972887039185
Epoch 770, training loss: 0.07451505213975906 = 0.005325907375663519 + 0.01 * 6.918914794921875
Epoch 770, val loss: 1.0600383281707764
Epoch 780, training loss: 0.07460735738277435 = 0.005143102258443832 + 0.01 * 6.946425914764404
Epoch 780, val loss: 1.064552903175354
Epoch 790, training loss: 0.07414213567972183 = 0.0049707284197211266 + 0.01 * 6.917140483856201
Epoch 790, val loss: 1.068845510482788
Epoch 800, training loss: 0.07430508732795715 = 0.004808071535080671 + 0.01 * 6.94970178604126
Epoch 800, val loss: 1.0731006860733032
Epoch 810, training loss: 0.07361391931772232 = 0.004654298070818186 + 0.01 * 6.895962238311768
Epoch 810, val loss: 1.0771934986114502
Epoch 820, training loss: 0.07357893884181976 = 0.0045087905600667 + 0.01 * 6.9070143699646
Epoch 820, val loss: 1.0812172889709473
Epoch 830, training loss: 0.07323358952999115 = 0.004370894283056259 + 0.01 * 6.886270046234131
Epoch 830, val loss: 1.0851174592971802
Epoch 840, training loss: 0.07324706763029099 = 0.004239827860146761 + 0.01 * 6.900723934173584
Epoch 840, val loss: 1.088884711265564
Epoch 850, training loss: 0.07291391491889954 = 0.004115786403417587 + 0.01 * 6.8798136711120605
Epoch 850, val loss: 1.0926098823547363
Epoch 860, training loss: 0.07278639078140259 = 0.003997388761490583 + 0.01 * 6.87890100479126
Epoch 860, val loss: 1.0961982011795044
Epoch 870, training loss: 0.0726761445403099 = 0.0038848579861223698 + 0.01 * 6.879128456115723
Epoch 870, val loss: 1.099716305732727
Epoch 880, training loss: 0.07233121246099472 = 0.0037776613608002663 + 0.01 * 6.855355262756348
Epoch 880, val loss: 1.1031696796417236
Epoch 890, training loss: 0.07224428653717041 = 0.0036754547618329525 + 0.01 * 6.856883525848389
Epoch 890, val loss: 1.1064943075180054
Epoch 900, training loss: 0.0722733661532402 = 0.003577829571440816 + 0.01 * 6.869553565979004
Epoch 900, val loss: 1.10977041721344
Epoch 910, training loss: 0.07200556248426437 = 0.003484800923615694 + 0.01 * 6.852076053619385
Epoch 910, val loss: 1.1129541397094727
Epoch 920, training loss: 0.0718553215265274 = 0.0033957608975470066 + 0.01 * 6.845956802368164
Epoch 920, val loss: 1.1160870790481567
Epoch 930, training loss: 0.07161499559879303 = 0.0033105844631791115 + 0.01 * 6.830441474914551
Epoch 930, val loss: 1.1191389560699463
Epoch 940, training loss: 0.07187659293413162 = 0.003228768240660429 + 0.01 * 6.864782810211182
Epoch 940, val loss: 1.122096061706543
Epoch 950, training loss: 0.07150398194789886 = 0.0031509078107774258 + 0.01 * 6.835307598114014
Epoch 950, val loss: 1.1250014305114746
Epoch 960, training loss: 0.07142477482557297 = 0.003076205961406231 + 0.01 * 6.83485746383667
Epoch 960, val loss: 1.1278420686721802
Epoch 970, training loss: 0.07123655080795288 = 0.0030045052990317345 + 0.01 * 6.823204517364502
Epoch 970, val loss: 1.130600094795227
Epoch 980, training loss: 0.07118986546993256 = 0.002935688942670822 + 0.01 * 6.825417995452881
Epoch 980, val loss: 1.1333496570587158
Epoch 990, training loss: 0.07121002674102783 = 0.0028695163782685995 + 0.01 * 6.834051132202148
Epoch 990, val loss: 1.135993242263794
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.02966046333313 = 1.9436922073364258 + 0.01 * 8.596818923950195
Epoch 0, val loss: 1.9442552328109741
Epoch 10, training loss: 2.0199811458587646 = 1.934013843536377 + 0.01 * 8.596741676330566
Epoch 10, val loss: 1.9344964027404785
Epoch 20, training loss: 2.007930040359497 = 1.921965479850769 + 0.01 * 8.59644889831543
Epoch 20, val loss: 1.9222701787948608
Epoch 30, training loss: 1.9907982349395752 = 1.9048445224761963 + 0.01 * 8.595366477966309
Epoch 30, val loss: 1.905105710029602
Epoch 40, training loss: 1.9650379419326782 = 1.8791505098342896 + 0.01 * 8.588740348815918
Epoch 40, val loss: 1.8798177242279053
Epoch 50, training loss: 1.9278390407562256 = 1.8423725366592407 + 0.01 * 8.546648025512695
Epoch 50, val loss: 1.8451606035232544
Epoch 60, training loss: 1.8837944269180298 = 1.800172209739685 + 0.01 * 8.362222671508789
Epoch 60, val loss: 1.8090884685516357
Epoch 70, training loss: 1.844538927078247 = 1.7626668214797974 + 0.01 * 8.18720531463623
Epoch 70, val loss: 1.7777717113494873
Epoch 80, training loss: 1.7952629327774048 = 1.7161705493927002 + 0.01 * 7.909237861633301
Epoch 80, val loss: 1.7337464094161987
Epoch 90, training loss: 1.7283838987350464 = 1.6516289710998535 + 0.01 * 7.675488471984863
Epoch 90, val loss: 1.6745061874389648
Epoch 100, training loss: 1.6414450407028198 = 1.5662851333618164 + 0.01 * 7.515988349914551
Epoch 100, val loss: 1.6012684106826782
Epoch 110, training loss: 1.5412276983261108 = 1.4670848846435547 + 0.01 * 7.414285182952881
Epoch 110, val loss: 1.5173665285110474
Epoch 120, training loss: 1.437770128250122 = 1.3642117977142334 + 0.01 * 7.355837821960449
Epoch 120, val loss: 1.432154655456543
Epoch 130, training loss: 1.3369358777999878 = 1.263809084892273 + 0.01 * 7.31268310546875
Epoch 130, val loss: 1.3515771627426147
Epoch 140, training loss: 1.240861177444458 = 1.1680980920791626 + 0.01 * 7.276303768157959
Epoch 140, val loss: 1.2757898569107056
Epoch 150, training loss: 1.1514253616333008 = 1.0789833068847656 + 0.01 * 7.244207382202148
Epoch 150, val loss: 1.2058266401290894
Epoch 160, training loss: 1.0700132846832275 = 0.9978487491607666 + 0.01 * 7.216458320617676
Epoch 160, val loss: 1.142674446105957
Epoch 170, training loss: 0.9960273504257202 = 0.9240759015083313 + 0.01 * 7.195146560668945
Epoch 170, val loss: 1.0853843688964844
Epoch 180, training loss: 0.9268956184387207 = 0.8550506830215454 + 0.01 * 7.184493541717529
Epoch 180, val loss: 1.031530737876892
Epoch 190, training loss: 0.8598171472549438 = 0.7880635261535645 + 0.01 * 7.175364017486572
Epoch 190, val loss: 0.9787818193435669
Epoch 200, training loss: 0.7937412858009338 = 0.7220491766929626 + 0.01 * 7.1692118644714355
Epoch 200, val loss: 0.9271267652511597
Epoch 210, training loss: 0.7298296093940735 = 0.658203125 + 0.01 * 7.162648677825928
Epoch 210, val loss: 0.8785930275917053
Epoch 220, training loss: 0.6702078580856323 = 0.5986505746841431 + 0.01 * 7.155731201171875
Epoch 220, val loss: 0.8358312249183655
Epoch 230, training loss: 0.6163924932479858 = 0.5449120402336121 + 0.01 * 7.14804220199585
Epoch 230, val loss: 0.8007700443267822
Epoch 240, training loss: 0.5686033964157104 = 0.497205913066864 + 0.01 * 7.139746189117432
Epoch 240, val loss: 0.7736489176750183
Epoch 250, training loss: 0.525917112827301 = 0.4546142816543579 + 0.01 * 7.130285739898682
Epoch 250, val loss: 0.7533885836601257
Epoch 260, training loss: 0.4871169924736023 = 0.4159298241138458 + 0.01 * 7.11871862411499
Epoch 260, val loss: 0.7383785247802734
Epoch 270, training loss: 0.4510839283466339 = 0.37992656230926514 + 0.01 * 7.115737438201904
Epoch 270, val loss: 0.7270525097846985
Epoch 280, training loss: 0.4167092740535736 = 0.34571924805641174 + 0.01 * 7.099001884460449
Epoch 280, val loss: 0.7183582782745361
Epoch 290, training loss: 0.38369494676589966 = 0.31279078125953674 + 0.01 * 7.090415000915527
Epoch 290, val loss: 0.711760938167572
Epoch 300, training loss: 0.35199058055877686 = 0.2811550796031952 + 0.01 * 7.083548545837402
Epoch 300, val loss: 0.7068729996681213
Epoch 310, training loss: 0.321941077709198 = 0.2511574625968933 + 0.01 * 7.078360557556152
Epoch 310, val loss: 0.7036300897598267
Epoch 320, training loss: 0.2939798831939697 = 0.22316575050354004 + 0.01 * 7.081412315368652
Epoch 320, val loss: 0.7021331191062927
Epoch 330, training loss: 0.26813510060310364 = 0.19738122820854187 + 0.01 * 7.075387001037598
Epoch 330, val loss: 0.7024736404418945
Epoch 340, training loss: 0.24455377459526062 = 0.17385423183441162 + 0.01 * 7.069953918457031
Epoch 340, val loss: 0.7047244906425476
Epoch 350, training loss: 0.22328799962997437 = 0.15261009335517883 + 0.01 * 7.06779146194458
Epoch 350, val loss: 0.7088710069656372
Epoch 360, training loss: 0.20428770780563354 = 0.13363367319107056 + 0.01 * 7.065403461456299
Epoch 360, val loss: 0.7148407101631165
Epoch 370, training loss: 0.1875513195991516 = 0.11687618494033813 + 0.01 * 7.067513942718506
Epoch 370, val loss: 0.7224756479263306
Epoch 380, training loss: 0.17284488677978516 = 0.10222898423671722 + 0.01 * 7.061590194702148
Epoch 380, val loss: 0.7314731478691101
Epoch 390, training loss: 0.1600988805294037 = 0.08951061218976974 + 0.01 * 7.058825969696045
Epoch 390, val loss: 0.7415822148323059
Epoch 400, training loss: 0.14906668663024902 = 0.07850895822048187 + 0.01 * 7.055773735046387
Epoch 400, val loss: 0.752577543258667
Epoch 410, training loss: 0.13954967260360718 = 0.06902295351028442 + 0.01 * 7.052672386169434
Epoch 410, val loss: 0.7641991972923279
Epoch 420, training loss: 0.13137994706630707 = 0.06086306646466255 + 0.01 * 7.051687717437744
Epoch 420, val loss: 0.7762237191200256
Epoch 430, training loss: 0.12433342635631561 = 0.053856801241636276 + 0.01 * 7.047662258148193
Epoch 430, val loss: 0.7884660959243774
Epoch 440, training loss: 0.11826842278242111 = 0.04784281551837921 + 0.01 * 7.042560577392578
Epoch 440, val loss: 0.8009137511253357
Epoch 450, training loss: 0.11314272880554199 = 0.04267656430602074 + 0.01 * 7.046616554260254
Epoch 450, val loss: 0.8133243322372437
Epoch 460, training loss: 0.10860337316989899 = 0.03823481872677803 + 0.01 * 7.036855220794678
Epoch 460, val loss: 0.825724720954895
Epoch 470, training loss: 0.1047128438949585 = 0.034405555576086044 + 0.01 * 7.030729293823242
Epoch 470, val loss: 0.8379760980606079
Epoch 480, training loss: 0.10140997171401978 = 0.03109254315495491 + 0.01 * 7.031743049621582
Epoch 480, val loss: 0.8501172661781311
Epoch 490, training loss: 0.09844726324081421 = 0.02821783348917961 + 0.01 * 7.022942543029785
Epoch 490, val loss: 0.8619772791862488
Epoch 500, training loss: 0.095905601978302 = 0.025713114067912102 + 0.01 * 7.0192484855651855
Epoch 500, val loss: 0.8736390471458435
Epoch 510, training loss: 0.09366385638713837 = 0.02352129854261875 + 0.01 * 7.014256000518799
Epoch 510, val loss: 0.885017454624176
Epoch 520, training loss: 0.09176138043403625 = 0.02159687504172325 + 0.01 * 7.016451358795166
Epoch 520, val loss: 0.8961212038993835
Epoch 530, training loss: 0.08995890617370605 = 0.019900444895029068 + 0.01 * 7.005846977233887
Epoch 530, val loss: 0.9069114327430725
Epoch 540, training loss: 0.08851895481348038 = 0.018398230895400047 + 0.01 * 7.012072563171387
Epoch 540, val loss: 0.917375922203064
Epoch 550, training loss: 0.08701175451278687 = 0.017063181847333908 + 0.01 * 6.994856834411621
Epoch 550, val loss: 0.9275926351547241
Epoch 560, training loss: 0.08574945479631424 = 0.015871919691562653 + 0.01 * 6.987753391265869
Epoch 560, val loss: 0.93748939037323
Epoch 570, training loss: 0.08478579670190811 = 0.01480476651340723 + 0.01 * 6.998103141784668
Epoch 570, val loss: 0.9471713304519653
Epoch 580, training loss: 0.08360645920038223 = 0.013846689835190773 + 0.01 * 6.975976943969727
Epoch 580, val loss: 0.9564111828804016
Epoch 590, training loss: 0.08270800113677979 = 0.012982669286429882 + 0.01 * 6.972533702850342
Epoch 590, val loss: 0.965403139591217
Epoch 600, training loss: 0.08184033632278442 = 0.012200972065329552 + 0.01 * 6.9639363288879395
Epoch 600, val loss: 0.9741756916046143
Epoch 610, training loss: 0.08115717768669128 = 0.01149233803153038 + 0.01 * 6.9664835929870605
Epoch 610, val loss: 0.9826366305351257
Epoch 620, training loss: 0.0803842768073082 = 0.010848144069314003 + 0.01 * 6.953613758087158
Epoch 620, val loss: 0.9907838106155396
Epoch 630, training loss: 0.07969482243061066 = 0.01025994773954153 + 0.01 * 6.943487644195557
Epoch 630, val loss: 0.9987729787826538
Epoch 640, training loss: 0.07912635058164597 = 0.009721361100673676 + 0.01 * 6.9404988288879395
Epoch 640, val loss: 1.006546974182129
Epoch 650, training loss: 0.07876137644052505 = 0.009228070266544819 + 0.01 * 6.9533305168151855
Epoch 650, val loss: 1.014037847518921
Epoch 660, training loss: 0.07811510562896729 = 0.008775420486927032 + 0.01 * 6.933968544006348
Epoch 660, val loss: 1.0212712287902832
Epoch 670, training loss: 0.077975332736969 = 0.008358608931303024 + 0.01 * 6.961673259735107
Epoch 670, val loss: 1.0282820463180542
Epoch 680, training loss: 0.07712399214506149 = 0.007973579689860344 + 0.01 * 6.915040969848633
Epoch 680, val loss: 1.035115122795105
Epoch 690, training loss: 0.0767848938703537 = 0.0076170265674591064 + 0.01 * 6.916787147521973
Epoch 690, val loss: 1.0418143272399902
Epoch 700, training loss: 0.07637898623943329 = 0.007286393083631992 + 0.01 * 6.909259796142578
Epoch 700, val loss: 1.0482978820800781
Epoch 710, training loss: 0.07611623406410217 = 0.006979053840041161 + 0.01 * 6.913717746734619
Epoch 710, val loss: 1.0546423196792603
Epoch 720, training loss: 0.07558824867010117 = 0.006693542003631592 + 0.01 * 6.889471054077148
Epoch 720, val loss: 1.0607367753982544
Epoch 730, training loss: 0.07528092712163925 = 0.006427164655178785 + 0.01 * 6.885376930236816
Epoch 730, val loss: 1.0666624307632446
Epoch 740, training loss: 0.07517620176076889 = 0.006178378593176603 + 0.01 * 6.899782657623291
Epoch 740, val loss: 1.0724830627441406
Epoch 750, training loss: 0.07466937601566315 = 0.005945737939327955 + 0.01 * 6.872364521026611
Epoch 750, val loss: 1.0781680345535278
Epoch 760, training loss: 0.074448361992836 = 0.005727942567318678 + 0.01 * 6.872042179107666
Epoch 760, val loss: 1.0836299657821655
Epoch 770, training loss: 0.07413557916879654 = 0.005523525178432465 + 0.01 * 6.861205577850342
Epoch 770, val loss: 1.0889651775360107
Epoch 780, training loss: 0.07399109750986099 = 0.005331231746822596 + 0.01 * 6.865986347198486
Epoch 780, val loss: 1.094247579574585
Epoch 790, training loss: 0.07365952432155609 = 0.005150670651346445 + 0.01 * 6.85088586807251
Epoch 790, val loss: 1.0993003845214844
Epoch 800, training loss: 0.07403361052274704 = 0.0049804141744971275 + 0.01 * 6.905319690704346
Epoch 800, val loss: 1.104218602180481
Epoch 810, training loss: 0.07333540916442871 = 0.004819890484213829 + 0.01 * 6.851551532745361
Epoch 810, val loss: 1.1090530157089233
Epoch 820, training loss: 0.07310419529676437 = 0.004668131470680237 + 0.01 * 6.843606472015381
Epoch 820, val loss: 1.1138008832931519
Epoch 830, training loss: 0.07292016595602036 = 0.00452467380091548 + 0.01 * 6.839550018310547
Epoch 830, val loss: 1.118391990661621
Epoch 840, training loss: 0.07271483540534973 = 0.004388892557471991 + 0.01 * 6.832594394683838
Epoch 840, val loss: 1.1229314804077148
Epoch 850, training loss: 0.0724978968501091 = 0.0042601702734827995 + 0.01 * 6.823773384094238
Epoch 850, val loss: 1.1273165941238403
Epoch 860, training loss: 0.0723947063088417 = 0.004137912765145302 + 0.01 * 6.825679779052734
Epoch 860, val loss: 1.1316334009170532
Epoch 870, training loss: 0.07241619378328323 = 0.00402201758697629 + 0.01 * 6.839417934417725
Epoch 870, val loss: 1.1358413696289062
Epoch 880, training loss: 0.07205639779567719 = 0.003911841195076704 + 0.01 * 6.814455986022949
Epoch 880, val loss: 1.139926552772522
Epoch 890, training loss: 0.07233449071645737 = 0.0038070003502070904 + 0.01 * 6.852749347686768
Epoch 890, val loss: 1.1440176963806152
Epoch 900, training loss: 0.07178264856338501 = 0.0037073995918035507 + 0.01 * 6.807524681091309
Epoch 900, val loss: 1.1479034423828125
Epoch 910, training loss: 0.07164913415908813 = 0.0036122803576290607 + 0.01 * 6.803685665130615
Epoch 910, val loss: 1.1517421007156372
Epoch 920, training loss: 0.07177634537220001 = 0.003521556966006756 + 0.01 * 6.825479507446289
Epoch 920, val loss: 1.1555618047714233
Epoch 930, training loss: 0.07153431326150894 = 0.0034352601505815983 + 0.01 * 6.809905052185059
Epoch 930, val loss: 1.159171223640442
Epoch 940, training loss: 0.07139524072408676 = 0.0033524653408676386 + 0.01 * 6.8042778968811035
Epoch 940, val loss: 1.1628029346466064
Epoch 950, training loss: 0.07124455273151398 = 0.0032733988482505083 + 0.01 * 6.797115802764893
Epoch 950, val loss: 1.1663906574249268
Epoch 960, training loss: 0.07143894582986832 = 0.003197569865733385 + 0.01 * 6.8241376876831055
Epoch 960, val loss: 1.1699155569076538
Epoch 970, training loss: 0.07105141133069992 = 0.00312524545006454 + 0.01 * 6.792617321014404
Epoch 970, val loss: 1.1732425689697266
Epoch 980, training loss: 0.0711769089102745 = 0.003055779729038477 + 0.01 * 6.812113285064697
Epoch 980, val loss: 1.1766183376312256
Epoch 990, training loss: 0.07083507627248764 = 0.002989438595250249 + 0.01 * 6.784564018249512
Epoch 990, val loss: 1.1798697710037231
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.0164172649383545 = 1.9304490089416504 + 0.01 * 8.596830368041992
Epoch 0, val loss: 1.9364694356918335
Epoch 10, training loss: 2.0067946910858154 = 1.9208271503448486 + 0.01 * 8.596756935119629
Epoch 10, val loss: 1.9269522428512573
Epoch 20, training loss: 1.9945316314697266 = 1.9085665941238403 + 0.01 * 8.596506118774414
Epoch 20, val loss: 1.9143638610839844
Epoch 30, training loss: 1.97698175907135 = 1.8910253047943115 + 0.01 * 8.59564208984375
Epoch 30, val loss: 1.8961557149887085
Epoch 40, training loss: 1.9512771368026733 = 1.8653697967529297 + 0.01 * 8.590733528137207
Epoch 40, val loss: 1.8697012662887573
Epoch 50, training loss: 1.9164111614227295 = 1.830814242362976 + 0.01 * 8.559686660766602
Epoch 50, val loss: 1.8355575799942017
Epoch 60, training loss: 1.8774467706680298 = 1.7935091257095337 + 0.01 * 8.393765449523926
Epoch 60, val loss: 1.8016479015350342
Epoch 70, training loss: 1.8385716676712036 = 1.7562586069107056 + 0.01 * 8.23130989074707
Epoch 70, val loss: 1.7678698301315308
Epoch 80, training loss: 1.7857320308685303 = 1.7056994438171387 + 0.01 * 8.00326156616211
Epoch 80, val loss: 1.7206491231918335
Epoch 90, training loss: 1.7130786180496216 = 1.6358709335327148 + 0.01 * 7.720770835876465
Epoch 90, val loss: 1.6579680442810059
Epoch 100, training loss: 1.6222702264785767 = 1.5469369888305664 + 0.01 * 7.533327579498291
Epoch 100, val loss: 1.5815271139144897
Epoch 110, training loss: 1.5214099884033203 = 1.446683406829834 + 0.01 * 7.472653865814209
Epoch 110, val loss: 1.4967504739761353
Epoch 120, training loss: 1.4175939559936523 = 1.3435269594192505 + 0.01 * 7.406693935394287
Epoch 120, val loss: 1.41209077835083
Epoch 130, training loss: 1.3138673305511475 = 1.240209937095642 + 0.01 * 7.365742206573486
Epoch 130, val loss: 1.3300883769989014
Epoch 140, training loss: 1.2117562294006348 = 1.1382603645324707 + 0.01 * 7.3495917320251465
Epoch 140, val loss: 1.2518378496170044
Epoch 150, training loss: 1.1145222187042236 = 1.0410959720611572 + 0.01 * 7.342619895935059
Epoch 150, val loss: 1.1777551174163818
Epoch 160, training loss: 1.02462899684906 = 0.9512708187103271 + 0.01 * 7.3358154296875
Epoch 160, val loss: 1.1094443798065186
Epoch 170, training loss: 0.9431356191635132 = 0.8698931932449341 + 0.01 * 7.324243068695068
Epoch 170, val loss: 1.0475051403045654
Epoch 180, training loss: 0.8703699111938477 = 0.7973271012306213 + 0.01 * 7.304282188415527
Epoch 180, val loss: 0.9924282431602478
Epoch 190, training loss: 0.8064305782318115 = 0.7337465882301331 + 0.01 * 7.268395900726318
Epoch 190, val loss: 0.9450343251228333
Epoch 200, training loss: 0.7509129047393799 = 0.6786030530929565 + 0.01 * 7.230988025665283
Epoch 200, val loss: 0.9054980278015137
Epoch 210, training loss: 0.7023934125900269 = 0.6304991245269775 + 0.01 * 7.189426898956299
Epoch 210, val loss: 0.8734520673751831
Epoch 220, training loss: 0.659389853477478 = 0.5877094864845276 + 0.01 * 7.168039798736572
Epoch 220, val loss: 0.8476738929748535
Epoch 230, training loss: 0.6200710535049438 = 0.548736572265625 + 0.01 * 7.133445739746094
Epoch 230, val loss: 0.8266592025756836
Epoch 240, training loss: 0.5832120180130005 = 0.5119944214820862 + 0.01 * 7.1217570304870605
Epoch 240, val loss: 0.8086609840393066
Epoch 250, training loss: 0.5468332171440125 = 0.4757600724697113 + 0.01 * 7.107312202453613
Epoch 250, val loss: 0.7918519973754883
Epoch 260, training loss: 0.5092113614082336 = 0.43822216987609863 + 0.01 * 7.098921298980713
Epoch 260, val loss: 0.775160551071167
Epoch 270, training loss: 0.46916264295578003 = 0.39822500944137573 + 0.01 * 7.0937628746032715
Epoch 270, val loss: 0.7587460875511169
Epoch 280, training loss: 0.4267962574958801 = 0.3558957278728485 + 0.01 * 7.090053558349609
Epoch 280, val loss: 0.7436877489089966
Epoch 290, training loss: 0.38383448123931885 = 0.3128969371318817 + 0.01 * 7.093754768371582
Epoch 290, val loss: 0.7319304943084717
Epoch 300, training loss: 0.34300497174263 = 0.27214470505714417 + 0.01 * 7.086026668548584
Epoch 300, val loss: 0.7254154682159424
Epoch 310, training loss: 0.3068819046020508 = 0.2360539585351944 + 0.01 * 7.0827956199646
Epoch 310, val loss: 0.7246859669685364
Epoch 320, training loss: 0.2761646509170532 = 0.20536722242832184 + 0.01 * 7.079742431640625
Epoch 320, val loss: 0.7295455932617188
Epoch 330, training loss: 0.25052905082702637 = 0.17953303456306458 + 0.01 * 7.0996012687683105
Epoch 330, val loss: 0.7389782071113586
Epoch 340, training loss: 0.2284201979637146 = 0.1576455980539322 + 0.01 * 7.077459812164307
Epoch 340, val loss: 0.7517091035842896
Epoch 350, training loss: 0.2095855325460434 = 0.13887327909469604 + 0.01 * 7.071225643157959
Epoch 350, val loss: 0.7667906880378723
Epoch 360, training loss: 0.19330650568008423 = 0.12262700498104095 + 0.01 * 7.067951202392578
Epoch 360, val loss: 0.7834392786026001
Epoch 370, training loss: 0.17915913462638855 = 0.10847834497690201 + 0.01 * 7.068079471588135
Epoch 370, val loss: 0.8011124134063721
Epoch 380, training loss: 0.16672903299331665 = 0.09610956162214279 + 0.01 * 7.061946868896484
Epoch 380, val loss: 0.8193575143814087
Epoch 390, training loss: 0.15583114326000214 = 0.08526863157749176 + 0.01 * 7.056251049041748
Epoch 390, val loss: 0.8380343317985535
Epoch 400, training loss: 0.14628437161445618 = 0.07575845718383789 + 0.01 * 7.052591323852539
Epoch 400, val loss: 0.856998085975647
Epoch 410, training loss: 0.13789349794387817 = 0.06741925328969955 + 0.01 * 7.04742431640625
Epoch 410, val loss: 0.8760400414466858
Epoch 420, training loss: 0.13057279586791992 = 0.060115646570920944 + 0.01 * 7.04571533203125
Epoch 420, val loss: 0.8950773477554321
Epoch 430, training loss: 0.12412066757678986 = 0.05373182147741318 + 0.01 * 7.038885116577148
Epoch 430, val loss: 0.9139470458030701
Epoch 440, training loss: 0.11856481432914734 = 0.048155736178159714 + 0.01 * 7.040907859802246
Epoch 440, val loss: 0.9325963258743286
Epoch 450, training loss: 0.1135675385594368 = 0.043284133076667786 + 0.01 * 7.0283403396606445
Epoch 450, val loss: 0.9509416222572327
Epoch 460, training loss: 0.10935643315315247 = 0.039026375859975815 + 0.01 * 7.033006191253662
Epoch 460, val loss: 0.9689521193504333
Epoch 470, training loss: 0.1055135726928711 = 0.03530415520071983 + 0.01 * 7.020941734313965
Epoch 470, val loss: 0.9864829778671265
Epoch 480, training loss: 0.10219550132751465 = 0.03204404562711716 + 0.01 * 7.015145778656006
Epoch 480, val loss: 1.0037009716033936
Epoch 490, training loss: 0.09932323545217514 = 0.02918282151222229 + 0.01 * 7.014041900634766
Epoch 490, val loss: 1.0204846858978271
Epoch 500, training loss: 0.0967540293931961 = 0.026665832847356796 + 0.01 * 7.008819103240967
Epoch 500, val loss: 1.0368092060089111
Epoch 510, training loss: 0.09448632597923279 = 0.024446163326501846 + 0.01 * 7.004016399383545
Epoch 510, val loss: 1.0525825023651123
Epoch 520, training loss: 0.09244267642498016 = 0.022481201216578484 + 0.01 * 6.996147632598877
Epoch 520, val loss: 1.0678868293762207
Epoch 530, training loss: 0.09062217175960541 = 0.020735491067171097 + 0.01 * 6.988668441772461
Epoch 530, val loss: 1.0827094316482544
Epoch 540, training loss: 0.08902975171804428 = 0.019181877374649048 + 0.01 * 6.984787464141846
Epoch 540, val loss: 1.0969640016555786
Epoch 550, training loss: 0.08758281171321869 = 0.017795059829950333 + 0.01 * 6.9787750244140625
Epoch 550, val loss: 1.1107966899871826
Epoch 560, training loss: 0.08625442534685135 = 0.016553210094571114 + 0.01 * 6.97012186050415
Epoch 560, val loss: 1.1241297721862793
Epoch 570, training loss: 0.08517111092805862 = 0.015438437461853027 + 0.01 * 6.973267555236816
Epoch 570, val loss: 1.136989712715149
Epoch 580, training loss: 0.08407946676015854 = 0.01443307101726532 + 0.01 * 6.964639663696289
Epoch 580, val loss: 1.1494258642196655
Epoch 590, training loss: 0.0830947682261467 = 0.013523653149604797 + 0.01 * 6.957111835479736
Epoch 590, val loss: 1.1615101099014282
Epoch 600, training loss: 0.08224742114543915 = 0.012699421495199203 + 0.01 * 6.954800128936768
Epoch 600, val loss: 1.173253059387207
Epoch 610, training loss: 0.0814540684223175 = 0.0119517482817173 + 0.01 * 6.95023250579834
Epoch 610, val loss: 1.1845264434814453
Epoch 620, training loss: 0.08073636889457703 = 0.011271465569734573 + 0.01 * 6.94649076461792
Epoch 620, val loss: 1.195459246635437
Epoch 630, training loss: 0.0799829438328743 = 0.010649792850017548 + 0.01 * 6.933315277099609
Epoch 630, val loss: 1.2060836553573608
Epoch 640, training loss: 0.07947245240211487 = 0.010080252774059772 + 0.01 * 6.9392194747924805
Epoch 640, val loss: 1.2164148092269897
Epoch 650, training loss: 0.07888949662446976 = 0.00955801922827959 + 0.01 * 6.93314790725708
Epoch 650, val loss: 1.2262978553771973
Epoch 660, training loss: 0.07828859239816666 = 0.009077795781195164 + 0.01 * 6.921080112457275
Epoch 660, val loss: 1.2359437942504883
Epoch 670, training loss: 0.07797906547784805 = 0.008634905330836773 + 0.01 * 6.934416770935059
Epoch 670, val loss: 1.2452927827835083
Epoch 680, training loss: 0.07737445831298828 = 0.008226591162383556 + 0.01 * 6.914787292480469
Epoch 680, val loss: 1.2543233633041382
Epoch 690, training loss: 0.07698972523212433 = 0.007848916575312614 + 0.01 * 6.914080619812012
Epoch 690, val loss: 1.263090968132019
Epoch 700, training loss: 0.0764879360795021 = 0.007499146740883589 + 0.01 * 6.898879528045654
Epoch 700, val loss: 1.2716461420059204
Epoch 710, training loss: 0.07628974318504333 = 0.007174394093453884 + 0.01 * 6.911535263061523
Epoch 710, val loss: 1.2798930406570435
Epoch 720, training loss: 0.07582378387451172 = 0.00687242578715086 + 0.01 * 6.89513635635376
Epoch 720, val loss: 1.2879323959350586
Epoch 730, training loss: 0.07546773552894592 = 0.0065910546109080315 + 0.01 * 6.887668609619141
Epoch 730, val loss: 1.2958093881607056
Epoch 740, training loss: 0.07521016895771027 = 0.006328469142317772 + 0.01 * 6.88817024230957
Epoch 740, val loss: 1.3033427000045776
Epoch 750, training loss: 0.07508338242769241 = 0.006083118729293346 + 0.01 * 6.900026798248291
Epoch 750, val loss: 1.3107855319976807
Epoch 760, training loss: 0.07465929538011551 = 0.005853706505149603 + 0.01 * 6.880558967590332
Epoch 760, val loss: 1.3178808689117432
Epoch 770, training loss: 0.07447873800992966 = 0.005638669244945049 + 0.01 * 6.884007453918457
Epoch 770, val loss: 1.324891209602356
Epoch 780, training loss: 0.07417716830968857 = 0.00543663464486599 + 0.01 * 6.874053478240967
Epoch 780, val loss: 1.3316665887832642
Epoch 790, training loss: 0.07408980280160904 = 0.005246913060545921 + 0.01 * 6.884289264678955
Epoch 790, val loss: 1.3383827209472656
Epoch 800, training loss: 0.07360728085041046 = 0.005068238824605942 + 0.01 * 6.8539042472839355
Epoch 800, val loss: 1.3448086977005005
Epoch 810, training loss: 0.07359001785516739 = 0.004899779800325632 + 0.01 * 6.869024276733398
Epoch 810, val loss: 1.3511743545532227
Epoch 820, training loss: 0.07336984574794769 = 0.004740947857499123 + 0.01 * 6.862890243530273
Epoch 820, val loss: 1.3573025465011597
Epoch 830, training loss: 0.0730506181716919 = 0.004590803291648626 + 0.01 * 6.845981597900391
Epoch 830, val loss: 1.3632714748382568
Epoch 840, training loss: 0.07296694815158844 = 0.004448848310858011 + 0.01 * 6.851810455322266
Epoch 840, val loss: 1.3691891431808472
Epoch 850, training loss: 0.0729188323020935 = 0.004314352758228779 + 0.01 * 6.860447883605957
Epoch 850, val loss: 1.374879002571106
Epoch 860, training loss: 0.07256796211004257 = 0.004187180195003748 + 0.01 * 6.838078022003174
Epoch 860, val loss: 1.380393147468567
Epoch 870, training loss: 0.0723719373345375 = 0.004066355060786009 + 0.01 * 6.8305583000183105
Epoch 870, val loss: 1.3858857154846191
Epoch 880, training loss: 0.07224307954311371 = 0.003951621241867542 + 0.01 * 6.829145431518555
Epoch 880, val loss: 1.3912335634231567
Epoch 890, training loss: 0.07213591039180756 = 0.0038426676765084267 + 0.01 * 6.829324245452881
Epoch 890, val loss: 1.396450161933899
Epoch 900, training loss: 0.07204583287239075 = 0.003739000065252185 + 0.01 * 6.830683708190918
Epoch 900, val loss: 1.401509165763855
Epoch 910, training loss: 0.07188276201486588 = 0.0036403960548341274 + 0.01 * 6.824236869812012
Epoch 910, val loss: 1.4065312147140503
Epoch 920, training loss: 0.0718328207731247 = 0.003546524792909622 + 0.01 * 6.828630447387695
Epoch 920, val loss: 1.4113658666610718
Epoch 930, training loss: 0.0715898647904396 = 0.003456912701949477 + 0.01 * 6.813295364379883
Epoch 930, val loss: 1.416100025177002
Epoch 940, training loss: 0.07145228236913681 = 0.003371461760252714 + 0.01 * 6.808082103729248
Epoch 940, val loss: 1.4207985401153564
Epoch 950, training loss: 0.07137343287467957 = 0.0032898588106036186 + 0.01 * 6.8083577156066895
Epoch 950, val loss: 1.4252681732177734
Epoch 960, training loss: 0.07150614261627197 = 0.003211733652278781 + 0.01 * 6.829441070556641
Epoch 960, val loss: 1.429813027381897
Epoch 970, training loss: 0.07121092081069946 = 0.0031371423974633217 + 0.01 * 6.80737829208374
Epoch 970, val loss: 1.4340558052062988
Epoch 980, training loss: 0.0711938813328743 = 0.0030657772440463305 + 0.01 * 6.812809944152832
Epoch 980, val loss: 1.4383679628372192
Epoch 990, training loss: 0.07102613151073456 = 0.0029973345808684826 + 0.01 * 6.802879810333252
Epoch 990, val loss: 1.4424777030944824
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8355297838692674
The final CL Acc:0.81481, 0.01210, The final GNN Acc:0.83694, 0.00163
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11682])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10588])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0463991165161133 = 1.9604305028915405 + 0.01 * 8.596855163574219
Epoch 0, val loss: 1.9615813493728638
Epoch 10, training loss: 2.035564661026001 = 1.949596643447876 + 0.01 * 8.596810340881348
Epoch 10, val loss: 1.9501789808273315
Epoch 20, training loss: 2.0218582153320312 = 1.9358913898468018 + 0.01 * 8.596674919128418
Epoch 20, val loss: 1.9355190992355347
Epoch 30, training loss: 2.0021519660949707 = 1.9161889553070068 + 0.01 * 8.596311569213867
Epoch 30, val loss: 1.9144736528396606
Epoch 40, training loss: 1.9727365970611572 = 1.8867895603179932 + 0.01 * 8.594705581665039
Epoch 40, val loss: 1.8837254047393799
Epoch 50, training loss: 1.9326051473617554 = 1.8467752933502197 + 0.01 * 8.582982063293457
Epoch 50, val loss: 1.844425082206726
Epoch 60, training loss: 1.892055869102478 = 1.8068413734436035 + 0.01 * 8.521452903747559
Epoch 60, val loss: 1.8108476400375366
Epoch 70, training loss: 1.8583924770355225 = 1.7754900455474854 + 0.01 * 8.290239334106445
Epoch 70, val loss: 1.7868983745574951
Epoch 80, training loss: 1.816282868385315 = 1.734922170639038 + 0.01 * 8.136065483093262
Epoch 80, val loss: 1.7520270347595215
Epoch 90, training loss: 1.7588402032852173 = 1.6791366338729858 + 0.01 * 7.970361232757568
Epoch 90, val loss: 1.7031822204589844
Epoch 100, training loss: 1.6822806596755981 = 1.604055643081665 + 0.01 * 7.822504043579102
Epoch 100, val loss: 1.638437271118164
Epoch 110, training loss: 1.5956438779830933 = 1.5193132162094116 + 0.01 * 7.633065700531006
Epoch 110, val loss: 1.566972255706787
Epoch 120, training loss: 1.5113109350204468 = 1.4357789754867554 + 0.01 * 7.553194522857666
Epoch 120, val loss: 1.4989275932312012
Epoch 130, training loss: 1.4302639961242676 = 1.3550604581832886 + 0.01 * 7.520350456237793
Epoch 130, val loss: 1.4369258880615234
Epoch 140, training loss: 1.3491708040237427 = 1.274453043937683 + 0.01 * 7.471771717071533
Epoch 140, val loss: 1.376842737197876
Epoch 150, training loss: 1.265781044960022 = 1.191569209098816 + 0.01 * 7.421184539794922
Epoch 150, val loss: 1.316038727760315
Epoch 160, training loss: 1.1801069974899292 = 1.1063158512115479 + 0.01 * 7.379110336303711
Epoch 160, val loss: 1.2539262771606445
Epoch 170, training loss: 1.093619704246521 = 1.0200529098510742 + 0.01 * 7.356684684753418
Epoch 170, val loss: 1.1924852132797241
Epoch 180, training loss: 1.008772373199463 = 0.9353287220001221 + 0.01 * 7.34436559677124
Epoch 180, val loss: 1.1339914798736572
Epoch 190, training loss: 0.9289743304252625 = 0.8556459546089172 + 0.01 * 7.332836151123047
Epoch 190, val loss: 1.0807173252105713
Epoch 200, training loss: 0.8573595881462097 = 0.7841717600822449 + 0.01 * 7.318780422210693
Epoch 200, val loss: 1.0356976985931396
Epoch 210, training loss: 0.7948479652404785 = 0.721837043762207 + 0.01 * 7.301089286804199
Epoch 210, val loss: 0.9993503093719482
Epoch 220, training loss: 0.7399747371673584 = 0.6671674847602844 + 0.01 * 7.280726909637451
Epoch 220, val loss: 0.9706684350967407
Epoch 230, training loss: 0.6898773908615112 = 0.6172856092453003 + 0.01 * 7.2591753005981445
Epoch 230, val loss: 0.9473797082901001
Epoch 240, training loss: 0.6419103145599365 = 0.5695247054100037 + 0.01 * 7.238559722900391
Epoch 240, val loss: 0.9274983406066895
Epoch 250, training loss: 0.5948329567909241 = 0.5225979089736938 + 0.01 * 7.2235026359558105
Epoch 250, val loss: 0.9108797311782837
Epoch 260, training loss: 0.5487271547317505 = 0.47660142183303833 + 0.01 * 7.212575912475586
Epoch 260, val loss: 0.8984951972961426
Epoch 270, training loss: 0.5043861269950867 = 0.4323683977127075 + 0.01 * 7.201774597167969
Epoch 270, val loss: 0.8915907740592957
Epoch 280, training loss: 0.46268191933631897 = 0.3907071650028229 + 0.01 * 7.197474479675293
Epoch 280, val loss: 0.8906273245811462
Epoch 290, training loss: 0.4238145351409912 = 0.3519437909126282 + 0.01 * 7.187074184417725
Epoch 290, val loss: 0.89488285779953
Epoch 300, training loss: 0.38767585158348083 = 0.31587454676628113 + 0.01 * 7.1801300048828125
Epoch 300, val loss: 0.903161883354187
Epoch 310, training loss: 0.3539141118526459 = 0.2821318507194519 + 0.01 * 7.178225517272949
Epoch 310, val loss: 0.9145514965057373
Epoch 320, training loss: 0.32229435443878174 = 0.2506270706653595 + 0.01 * 7.16672945022583
Epoch 320, val loss: 0.9284247159957886
Epoch 330, training loss: 0.2932007908821106 = 0.2215266227722168 + 0.01 * 7.167417526245117
Epoch 330, val loss: 0.9444236755371094
Epoch 340, training loss: 0.26664239168167114 = 0.19510158896446228 + 0.01 * 7.154080390930176
Epoch 340, val loss: 0.9623456001281738
Epoch 350, training loss: 0.24296995997428894 = 0.17151981592178345 + 0.01 * 7.14501428604126
Epoch 350, val loss: 0.9822407960891724
Epoch 360, training loss: 0.22244331240653992 = 0.1508050262928009 + 0.01 * 7.163829326629639
Epoch 360, val loss: 1.0039722919464111
Epoch 370, training loss: 0.20422734320163727 = 0.13280822336673737 + 0.01 * 7.141912460327148
Epoch 370, val loss: 1.0273699760437012
Epoch 380, training loss: 0.18853864073753357 = 0.11724131554365158 + 0.01 * 7.129732131958008
Epoch 380, val loss: 1.0520099401474
Epoch 390, training loss: 0.1750219464302063 = 0.10379207134246826 + 0.01 * 7.122986793518066
Epoch 390, val loss: 1.0775974988937378
Epoch 400, training loss: 0.1633298695087433 = 0.09216038882732391 + 0.01 * 7.116948127746582
Epoch 400, val loss: 1.1037482023239136
Epoch 410, training loss: 0.15319356322288513 = 0.08208268880844116 + 0.01 * 7.111087799072266
Epoch 410, val loss: 1.1302368640899658
Epoch 420, training loss: 0.14451315999031067 = 0.07332674413919449 + 0.01 * 7.118642807006836
Epoch 420, val loss: 1.1568259000778198
Epoch 430, training loss: 0.13685005903244019 = 0.06571199744939804 + 0.01 * 7.113806247711182
Epoch 430, val loss: 1.1833019256591797
Epoch 440, training loss: 0.1300416886806488 = 0.05906989797949791 + 0.01 * 7.097179889678955
Epoch 440, val loss: 1.2096024751663208
Epoch 450, training loss: 0.12419389933347702 = 0.0532577708363533 + 0.01 * 7.0936126708984375
Epoch 450, val loss: 1.235674500465393
Epoch 460, training loss: 0.11903916299343109 = 0.048158545047044754 + 0.01 * 7.088062286376953
Epoch 460, val loss: 1.2614139318466187
Epoch 470, training loss: 0.11451388895511627 = 0.04367485269904137 + 0.01 * 7.083904266357422
Epoch 470, val loss: 1.2866928577423096
Epoch 480, training loss: 0.11051246523857117 = 0.03972340375185013 + 0.01 * 7.078906536102295
Epoch 480, val loss: 1.3114855289459229
Epoch 490, training loss: 0.10735912621021271 = 0.036233801394701004 + 0.01 * 7.112532615661621
Epoch 490, val loss: 1.3357322216033936
Epoch 500, training loss: 0.10399307310581207 = 0.033152151852846146 + 0.01 * 7.084091663360596
Epoch 500, val loss: 1.359260082244873
Epoch 510, training loss: 0.10114390403032303 = 0.030420444905757904 + 0.01 * 7.072345733642578
Epoch 510, val loss: 1.3820942640304565
Epoch 520, training loss: 0.09863720834255219 = 0.027991395443677902 + 0.01 * 7.064581394195557
Epoch 520, val loss: 1.4042712450027466
Epoch 530, training loss: 0.0964202731847763 = 0.02582581527531147 + 0.01 * 7.059446334838867
Epoch 530, val loss: 1.4257780313491821
Epoch 540, training loss: 0.09447668492794037 = 0.023889776319265366 + 0.01 * 7.058691024780273
Epoch 540, val loss: 1.4465950727462769
Epoch 550, training loss: 0.09267327189445496 = 0.022155027836561203 + 0.01 * 7.05182409286499
Epoch 550, val loss: 1.466719627380371
Epoch 560, training loss: 0.09136371314525604 = 0.020596832036972046 + 0.01 * 7.076688289642334
Epoch 560, val loss: 1.4861674308776855
Epoch 570, training loss: 0.08964867889881134 = 0.01919640228152275 + 0.01 * 7.045227527618408
Epoch 570, val loss: 1.5048844814300537
Epoch 580, training loss: 0.08837156742811203 = 0.017933009192347527 + 0.01 * 7.043855667114258
Epoch 580, val loss: 1.5229719877243042
Epoch 590, training loss: 0.08715544641017914 = 0.01678932085633278 + 0.01 * 7.036612510681152
Epoch 590, val loss: 1.5404765605926514
Epoch 600, training loss: 0.08610648661851883 = 0.015751654282212257 + 0.01 * 7.035483360290527
Epoch 600, val loss: 1.557395577430725
Epoch 610, training loss: 0.08512600511312485 = 0.014807961881160736 + 0.01 * 7.03180456161499
Epoch 610, val loss: 1.57375168800354
Epoch 620, training loss: 0.08427758514881134 = 0.013947331346571445 + 0.01 * 7.033025741577148
Epoch 620, val loss: 1.5895512104034424
Epoch 630, training loss: 0.08340201526880264 = 0.013160363771021366 + 0.01 * 7.024165630340576
Epoch 630, val loss: 1.6048290729522705
Epoch 640, training loss: 0.08263163268566132 = 0.012438034638762474 + 0.01 * 7.019359588623047
Epoch 640, val loss: 1.6195847988128662
Epoch 650, training loss: 0.08199255168437958 = 0.011771615594625473 + 0.01 * 7.022093772888184
Epoch 650, val loss: 1.6339176893234253
Epoch 660, training loss: 0.08126339316368103 = 0.01115438248962164 + 0.01 * 7.01090145111084
Epoch 660, val loss: 1.647821068763733
Epoch 670, training loss: 0.08080203086137772 = 0.010581344366073608 + 0.01 * 7.022068977355957
Epoch 670, val loss: 1.6613425016403198
Epoch 680, training loss: 0.08016317337751389 = 0.01004870980978012 + 0.01 * 7.011446475982666
Epoch 680, val loss: 1.6744556427001953
Epoch 690, training loss: 0.0795564353466034 = 0.009551861323416233 + 0.01 * 7.000457763671875
Epoch 690, val loss: 1.6872780323028564
Epoch 700, training loss: 0.07916048914194107 = 0.00908797886222601 + 0.01 * 7.007251262664795
Epoch 700, val loss: 1.6997712850570679
Epoch 710, training loss: 0.07869323343038559 = 0.008655056357383728 + 0.01 * 7.003817558288574
Epoch 710, val loss: 1.711901307106018
Epoch 720, training loss: 0.07820305228233337 = 0.008250097744166851 + 0.01 * 6.995296001434326
Epoch 720, val loss: 1.7237576246261597
Epoch 730, training loss: 0.07780604064464569 = 0.00787281896919012 + 0.01 * 6.993321895599365
Epoch 730, val loss: 1.7352629899978638
Epoch 740, training loss: 0.07735484093427658 = 0.007523152511566877 + 0.01 * 6.983169078826904
Epoch 740, val loss: 1.7464401721954346
Epoch 750, training loss: 0.07701405882835388 = 0.0071965912356972694 + 0.01 * 6.981746673583984
Epoch 750, val loss: 1.7573672533035278
Epoch 760, training loss: 0.07665631175041199 = 0.006891586352139711 + 0.01 * 6.976472854614258
Epoch 760, val loss: 1.7679262161254883
Epoch 770, training loss: 0.07640088349580765 = 0.0066062286496162415 + 0.01 * 6.979465961456299
Epoch 770, val loss: 1.7782232761383057
Epoch 780, training loss: 0.07601607590913773 = 0.006339017767459154 + 0.01 * 6.967705726623535
Epoch 780, val loss: 1.7882589101791382
Epoch 790, training loss: 0.07596566528081894 = 0.006088544148951769 + 0.01 * 6.9877119064331055
Epoch 790, val loss: 1.7980173826217651
Epoch 800, training loss: 0.07551512867212296 = 0.005854054354131222 + 0.01 * 6.9661078453063965
Epoch 800, val loss: 1.8074544668197632
Epoch 810, training loss: 0.07524340599775314 = 0.0056334552355110645 + 0.01 * 6.960995197296143
Epoch 810, val loss: 1.816710352897644
Epoch 820, training loss: 0.07496299594640732 = 0.005425769370049238 + 0.01 * 6.953723430633545
Epoch 820, val loss: 1.8257133960723877
Epoch 830, training loss: 0.07488110661506653 = 0.00522961700335145 + 0.01 * 6.965149402618408
Epoch 830, val loss: 1.834547996520996
Epoch 840, training loss: 0.07450731098651886 = 0.005044109188020229 + 0.01 * 6.946320533752441
Epoch 840, val loss: 1.843229055404663
Epoch 850, training loss: 0.074400395154953 = 0.004867575131356716 + 0.01 * 6.953281879425049
Epoch 850, val loss: 1.851912021636963
Epoch 860, training loss: 0.07424372434616089 = 0.004699481651186943 + 0.01 * 6.9544243812561035
Epoch 860, val loss: 1.8605643510818481
Epoch 870, training loss: 0.0739840716123581 = 0.004540150053799152 + 0.01 * 6.944392204284668
Epoch 870, val loss: 1.8691033124923706
Epoch 880, training loss: 0.0737035945057869 = 0.0043882462196052074 + 0.01 * 6.931535243988037
Epoch 880, val loss: 1.8775631189346313
Epoch 890, training loss: 0.073571115732193 = 0.00424564303830266 + 0.01 * 6.932547092437744
Epoch 890, val loss: 1.8857530355453491
Epoch 900, training loss: 0.07341068983078003 = 0.004110895097255707 + 0.01 * 6.92997932434082
Epoch 900, val loss: 1.8937773704528809
Epoch 910, training loss: 0.07317353039979935 = 0.0039830757305026054 + 0.01 * 6.919045925140381
Epoch 910, val loss: 1.9016413688659668
Epoch 920, training loss: 0.07307123392820358 = 0.003862211247906089 + 0.01 * 6.920902729034424
Epoch 920, val loss: 1.9092023372650146
Epoch 930, training loss: 0.07289072871208191 = 0.003747121663764119 + 0.01 * 6.914361000061035
Epoch 930, val loss: 1.9167670011520386
Epoch 940, training loss: 0.07277026027441025 = 0.0036376728676259518 + 0.01 * 6.913259029388428
Epoch 940, val loss: 1.9241961240768433
Epoch 950, training loss: 0.07264119386672974 = 0.003533732146024704 + 0.01 * 6.9107465744018555
Epoch 950, val loss: 1.931422233581543
Epoch 960, training loss: 0.07252629846334457 = 0.003435061313211918 + 0.01 * 6.90912389755249
Epoch 960, val loss: 1.938502550125122
Epoch 970, training loss: 0.07227030396461487 = 0.0033415616489946842 + 0.01 * 6.892874240875244
Epoch 970, val loss: 1.9453742504119873
Epoch 980, training loss: 0.0723181739449501 = 0.0032527886796742678 + 0.01 * 6.906538486480713
Epoch 980, val loss: 1.9521321058273315
Epoch 990, training loss: 0.07210619002580643 = 0.003168470459058881 + 0.01 * 6.893772125244141
Epoch 990, val loss: 1.9585992097854614
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 2.0424580574035645 = 1.956490159034729 + 0.01 * 8.596790313720703
Epoch 0, val loss: 1.9502949714660645
Epoch 10, training loss: 2.031545877456665 = 1.9455785751342773 + 0.01 * 8.596731185913086
Epoch 10, val loss: 1.9402261972427368
Epoch 20, training loss: 2.018094062805176 = 1.93212890625 + 0.01 * 8.596508026123047
Epoch 20, val loss: 1.927488923072815
Epoch 30, training loss: 1.999072790145874 = 1.913114309310913 + 0.01 * 8.595848083496094
Epoch 30, val loss: 1.9093060493469238
Epoch 40, training loss: 1.9711555242538452 = 1.8852288722991943 + 0.01 * 8.592665672302246
Epoch 40, val loss: 1.8830009698867798
Epoch 50, training loss: 1.9328904151916504 = 1.8471795320510864 + 0.01 * 8.571084022521973
Epoch 50, val loss: 1.848726511001587
Epoch 60, training loss: 1.8913166522979736 = 1.8068937063217163 + 0.01 * 8.442293167114258
Epoch 60, val loss: 1.8160979747772217
Epoch 70, training loss: 1.8573265075683594 = 1.7755335569381714 + 0.01 * 8.179301261901855
Epoch 70, val loss: 1.7904757261276245
Epoch 80, training loss: 1.817297339439392 = 1.7377740144729614 + 0.01 * 7.952334880828857
Epoch 80, val loss: 1.7530039548873901
Epoch 90, training loss: 1.7628612518310547 = 1.6848305463790894 + 0.01 * 7.803073406219482
Epoch 90, val loss: 1.7034955024719238
Epoch 100, training loss: 1.688395380973816 = 1.6114546060562134 + 0.01 * 7.694081783294678
Epoch 100, val loss: 1.639426589012146
Epoch 110, training loss: 1.5966845750808716 = 1.5214667320251465 + 0.01 * 7.521787166595459
Epoch 110, val loss: 1.5610148906707764
Epoch 120, training loss: 1.5005629062652588 = 1.4268338680267334 + 0.01 * 7.372900485992432
Epoch 120, val loss: 1.4816511869430542
Epoch 130, training loss: 1.4095046520233154 = 1.3368712663650513 + 0.01 * 7.263337135314941
Epoch 130, val loss: 1.4083904027938843
Epoch 140, training loss: 1.3271379470825195 = 1.2550327777862549 + 0.01 * 7.210511684417725
Epoch 140, val loss: 1.3458518981933594
Epoch 150, training loss: 1.2516885995864868 = 1.1798585653305054 + 0.01 * 7.18300724029541
Epoch 150, val loss: 1.292436957359314
Epoch 160, training loss: 1.178558349609375 = 1.1068267822265625 + 0.01 * 7.173160076141357
Epoch 160, val loss: 1.2436589002609253
Epoch 170, training loss: 1.103580117225647 = 1.0318844318389893 + 0.01 * 7.169565200805664
Epoch 170, val loss: 1.1945444345474243
Epoch 180, training loss: 1.0255210399627686 = 0.9538502097129822 + 0.01 * 7.167087078094482
Epoch 180, val loss: 1.1438419818878174
Epoch 190, training loss: 0.9460124969482422 = 0.8743500113487244 + 0.01 * 7.1662468910217285
Epoch 190, val loss: 1.0924307107925415
Epoch 200, training loss: 0.867579460144043 = 0.7959210872650146 + 0.01 * 7.165835380554199
Epoch 200, val loss: 1.0424493551254272
Epoch 210, training loss: 0.7922220230102539 = 0.7205662727355957 + 0.01 * 7.16557502746582
Epoch 210, val loss: 0.995459794998169
Epoch 220, training loss: 0.7212619185447693 = 0.6496128439903259 + 0.01 * 7.164909839630127
Epoch 220, val loss: 0.9525660872459412
Epoch 230, training loss: 0.6554508209228516 = 0.5838174819946289 + 0.01 * 7.163332462310791
Epoch 230, val loss: 0.914216935634613
Epoch 240, training loss: 0.5948808789253235 = 0.5232753157615662 + 0.01 * 7.160557270050049
Epoch 240, val loss: 0.8804922699928284
Epoch 250, training loss: 0.53904128074646 = 0.4674611985683441 + 0.01 * 7.158007621765137
Epoch 250, val loss: 0.8511252999305725
Epoch 260, training loss: 0.486878901720047 = 0.41538265347480774 + 0.01 * 7.149623870849609
Epoch 260, val loss: 0.8255646824836731
Epoch 270, training loss: 0.43753254413604736 = 0.3661161959171295 + 0.01 * 7.141634464263916
Epoch 270, val loss: 0.8031138777732849
Epoch 280, training loss: 0.39060497283935547 = 0.31927958130836487 + 0.01 * 7.132538795471191
Epoch 280, val loss: 0.7833566069602966
Epoch 290, training loss: 0.34652644395828247 = 0.27529409527778625 + 0.01 * 7.123235702514648
Epoch 290, val loss: 0.7667566537857056
Epoch 300, training loss: 0.3063632845878601 = 0.23511619865894318 + 0.01 * 7.1247076988220215
Epoch 300, val loss: 0.7540678381919861
Epoch 310, training loss: 0.2707294821739197 = 0.1996752768754959 + 0.01 * 7.105419635772705
Epoch 310, val loss: 0.746191680431366
Epoch 320, training loss: 0.24034008383750916 = 0.16936548054218292 + 0.01 * 7.09745979309082
Epoch 320, val loss: 0.7435701489448547
Epoch 330, training loss: 0.21489614248275757 = 0.1439923644065857 + 0.01 * 7.090378284454346
Epoch 330, val loss: 0.7457441687583923
Epoch 340, training loss: 0.19384624063968658 = 0.12298634648323059 + 0.01 * 7.085989475250244
Epoch 340, val loss: 0.7518919706344604
Epoch 350, training loss: 0.1765037178993225 = 0.10565415769815445 + 0.01 * 7.084956645965576
Epoch 350, val loss: 0.7611245512962341
Epoch 360, training loss: 0.16208651661872864 = 0.09132812917232513 + 0.01 * 7.075839996337891
Epoch 360, val loss: 0.7724801301956177
Epoch 370, training loss: 0.15016324818134308 = 0.07943214476108551 + 0.01 * 7.073110580444336
Epoch 370, val loss: 0.7851787805557251
Epoch 380, training loss: 0.1402157098054886 = 0.06950613856315613 + 0.01 * 7.070957183837891
Epoch 380, val loss: 0.7987774014472961
Epoch 390, training loss: 0.131866917014122 = 0.061183225363492966 + 0.01 * 7.0683698654174805
Epoch 390, val loss: 0.812783420085907
Epoch 400, training loss: 0.1247689425945282 = 0.054167602211236954 + 0.01 * 7.060133457183838
Epoch 400, val loss: 0.8269084692001343
Epoch 410, training loss: 0.11879263818264008 = 0.04821991175413132 + 0.01 * 7.057272434234619
Epoch 410, val loss: 0.8409770727157593
Epoch 420, training loss: 0.11362899839878082 = 0.043150175362825394 + 0.01 * 7.047882556915283
Epoch 420, val loss: 0.8548595905303955
Epoch 430, training loss: 0.10934296250343323 = 0.038803230971097946 + 0.01 * 7.0539727210998535
Epoch 430, val loss: 0.8684327006340027
Epoch 440, training loss: 0.1054491251707077 = 0.03505803272128105 + 0.01 * 7.039109230041504
Epoch 440, val loss: 0.8816104531288147
Epoch 450, training loss: 0.1021299809217453 = 0.031811174005270004 + 0.01 * 7.031880855560303
Epoch 450, val loss: 0.8944256901741028
Epoch 460, training loss: 0.09961284697055817 = 0.02898077294230461 + 0.01 * 7.063207626342773
Epoch 460, val loss: 0.9068499207496643
Epoch 470, training loss: 0.09672580659389496 = 0.026504604145884514 + 0.01 * 7.022120475769043
Epoch 470, val loss: 0.9188558459281921
Epoch 480, training loss: 0.09444991499185562 = 0.02432665042579174 + 0.01 * 7.012326717376709
Epoch 480, val loss: 0.9304710626602173
Epoch 490, training loss: 0.0924469605088234 = 0.022400816902518272 + 0.01 * 7.004614353179932
Epoch 490, val loss: 0.9417053461074829
Epoch 500, training loss: 0.0908740684390068 = 0.02069230191409588 + 0.01 * 7.018176555633545
Epoch 500, val loss: 0.9526100158691406
Epoch 510, training loss: 0.08918467909097672 = 0.01917322538793087 + 0.01 * 7.001145362854004
Epoch 510, val loss: 0.963071346282959
Epoch 520, training loss: 0.08771581947803497 = 0.01781666837632656 + 0.01 * 6.989915370941162
Epoch 520, val loss: 0.9732041954994202
Epoch 530, training loss: 0.08654811978340149 = 0.01659967377781868 + 0.01 * 6.994844436645508
Epoch 530, val loss: 0.9829952716827393
Epoch 540, training loss: 0.08526883274316788 = 0.015505739487707615 + 0.01 * 6.976309776306152
Epoch 540, val loss: 0.9924360513687134
Epoch 550, training loss: 0.08413580805063248 = 0.014519259333610535 + 0.01 * 6.961655139923096
Epoch 550, val loss: 1.001615047454834
Epoch 560, training loss: 0.08326991647481918 = 0.013626297935843468 + 0.01 * 6.964361667633057
Epoch 560, val loss: 1.0104926824569702
Epoch 570, training loss: 0.08250509202480316 = 0.012815866619348526 + 0.01 * 6.968923091888428
Epoch 570, val loss: 1.0190564393997192
Epoch 580, training loss: 0.08162827789783478 = 0.012078857980668545 + 0.01 * 6.954942226409912
Epoch 580, val loss: 1.0273274183273315
Epoch 590, training loss: 0.08091014623641968 = 0.01140660885721445 + 0.01 * 6.950353622436523
Epoch 590, val loss: 1.0353628396987915
Epoch 600, training loss: 0.08022415637969971 = 0.01079152338206768 + 0.01 * 6.943263530731201
Epoch 600, val loss: 1.0431220531463623
Epoch 610, training loss: 0.07961035519838333 = 0.010227671824395657 + 0.01 * 6.938268184661865
Epoch 610, val loss: 1.0506633520126343
Epoch 620, training loss: 0.078968346118927 = 0.00970951933413744 + 0.01 * 6.925882816314697
Epoch 620, val loss: 1.0579731464385986
Epoch 630, training loss: 0.07855737954378128 = 0.009232105687260628 + 0.01 * 6.932527542114258
Epoch 630, val loss: 1.065062403678894
Epoch 640, training loss: 0.07814499735832214 = 0.008791928179562092 + 0.01 * 6.935306549072266
Epoch 640, val loss: 1.0718967914581299
Epoch 650, training loss: 0.07753705978393555 = 0.0083854291588068 + 0.01 * 6.915163516998291
Epoch 650, val loss: 1.078534722328186
Epoch 660, training loss: 0.07712344825267792 = 0.008008264005184174 + 0.01 * 6.911518573760986
Epoch 660, val loss: 1.0849528312683105
Epoch 670, training loss: 0.07673759013414383 = 0.007657993119210005 + 0.01 * 6.907960414886475
Epoch 670, val loss: 1.0912377834320068
Epoch 680, training loss: 0.0764116495847702 = 0.007332093548029661 + 0.01 * 6.907955646514893
Epoch 680, val loss: 1.0973117351531982
Epoch 690, training loss: 0.07601018995046616 = 0.007028475869446993 + 0.01 * 6.898171424865723
Epoch 690, val loss: 1.1032336950302124
Epoch 700, training loss: 0.07566164433956146 = 0.006744517013430595 + 0.01 * 6.891712665557861
Epoch 700, val loss: 1.1090153455734253
Epoch 710, training loss: 0.07545915246009827 = 0.006478750146925449 + 0.01 * 6.898040771484375
Epoch 710, val loss: 1.1146215200424194
Epoch 720, training loss: 0.07506419718265533 = 0.0062301279976964 + 0.01 * 6.883406639099121
Epoch 720, val loss: 1.120046854019165
Epoch 730, training loss: 0.07485595345497131 = 0.005996419116854668 + 0.01 * 6.885953426361084
Epoch 730, val loss: 1.1253916025161743
Epoch 740, training loss: 0.07462448626756668 = 0.0057774619199335575 + 0.01 * 6.884702205657959
Epoch 740, val loss: 1.1305512189865112
Epoch 750, training loss: 0.07433917373418808 = 0.00557089876383543 + 0.01 * 6.876827716827393
Epoch 750, val loss: 1.1356321573257446
Epoch 760, training loss: 0.07409808784723282 = 0.005376538727432489 + 0.01 * 6.87215518951416
Epoch 760, val loss: 1.140517234802246
Epoch 770, training loss: 0.07385209202766418 = 0.005193671211600304 + 0.01 * 6.865842342376709
Epoch 770, val loss: 1.1453182697296143
Epoch 780, training loss: 0.07375713437795639 = 0.005020535551011562 + 0.01 * 6.873659610748291
Epoch 780, val loss: 1.1500099897384644
Epoch 790, training loss: 0.07362456619739532 = 0.004857090767472982 + 0.01 * 6.8767476081848145
Epoch 790, val loss: 1.1544976234436035
Epoch 800, training loss: 0.07335948199033737 = 0.004702129401266575 + 0.01 * 6.8657355308532715
Epoch 800, val loss: 1.159006118774414
Epoch 810, training loss: 0.07324877381324768 = 0.004555367399007082 + 0.01 * 6.869340896606445
Epoch 810, val loss: 1.1633377075195312
Epoch 820, training loss: 0.07291905581951141 = 0.004416795447468758 + 0.01 * 6.850226402282715
Epoch 820, val loss: 1.1675424575805664
Epoch 830, training loss: 0.07285556197166443 = 0.004284941125661135 + 0.01 * 6.857062339782715
Epoch 830, val loss: 1.1717779636383057
Epoch 840, training loss: 0.07274746149778366 = 0.004160398151725531 + 0.01 * 6.858706951141357
Epoch 840, val loss: 1.1756588220596313
Epoch 850, training loss: 0.07245457172393799 = 0.004042149521410465 + 0.01 * 6.841242790222168
Epoch 850, val loss: 1.179612159729004
Epoch 860, training loss: 0.07242473214864731 = 0.003929636441171169 + 0.01 * 6.8495097160339355
Epoch 860, val loss: 1.1834925413131714
Epoch 870, training loss: 0.07235651463270187 = 0.0038228114135563374 + 0.01 * 6.853370666503906
Epoch 870, val loss: 1.187179684638977
Epoch 880, training loss: 0.07209371030330658 = 0.0037207501009106636 + 0.01 * 6.8372955322265625
Epoch 880, val loss: 1.1907563209533691
Epoch 890, training loss: 0.07198041677474976 = 0.0036235442385077477 + 0.01 * 6.835687637329102
Epoch 890, val loss: 1.1943914890289307
Epoch 900, training loss: 0.07194401323795319 = 0.0035309111699461937 + 0.01 * 6.841310501098633
Epoch 900, val loss: 1.1978464126586914
Epoch 910, training loss: 0.0717008113861084 = 0.0034426781348884106 + 0.01 * 6.8258137702941895
Epoch 910, val loss: 1.201196551322937
Epoch 920, training loss: 0.07165637612342834 = 0.003358228597790003 + 0.01 * 6.829814910888672
Epoch 920, val loss: 1.2046151161193848
Epoch 930, training loss: 0.07165495306253433 = 0.003277794923633337 + 0.01 * 6.837716102600098
Epoch 930, val loss: 1.207688331604004
Epoch 940, training loss: 0.0714096948504448 = 0.0032007810659706593 + 0.01 * 6.8208909034729
Epoch 940, val loss: 1.2108927965164185
Epoch 950, training loss: 0.07137342542409897 = 0.0031269399914890528 + 0.01 * 6.824648380279541
Epoch 950, val loss: 1.2140192985534668
Epoch 960, training loss: 0.07122836261987686 = 0.0030564130283892155 + 0.01 * 6.817194938659668
Epoch 960, val loss: 1.2170464992523193
Epoch 970, training loss: 0.07114619016647339 = 0.002988903783261776 + 0.01 * 6.815728664398193
Epoch 970, val loss: 1.2198772430419922
Epoch 980, training loss: 0.07090657949447632 = 0.0029240557923913 + 0.01 * 6.798252582550049
Epoch 980, val loss: 1.2227615118026733
Epoch 990, training loss: 0.07115959376096725 = 0.0028617670759558678 + 0.01 * 6.829782485961914
Epoch 990, val loss: 1.2256855964660645
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.7996837111228255
=== training gcn model ===
Epoch 0, training loss: 2.044363498687744 = 1.95839524269104 + 0.01 * 8.59682846069336
Epoch 0, val loss: 1.9542640447616577
Epoch 10, training loss: 2.0339012145996094 = 1.947933316230774 + 0.01 * 8.596782684326172
Epoch 10, val loss: 1.9445172548294067
Epoch 20, training loss: 2.0212767124176025 = 1.9353106021881104 + 0.01 * 8.5966157913208
Epoch 20, val loss: 1.9321980476379395
Epoch 30, training loss: 2.0037808418273926 = 1.917819619178772 + 0.01 * 8.59613037109375
Epoch 30, val loss: 1.9146780967712402
Epoch 40, training loss: 1.9779613018035889 = 1.8920209407806396 + 0.01 * 8.594034194946289
Epoch 40, val loss: 1.8887438774108887
Epoch 50, training loss: 1.9411368370056152 = 1.8553295135498047 + 0.01 * 8.580733299255371
Epoch 50, val loss: 1.8532246351242065
Epoch 60, training loss: 1.8980299234390259 = 1.8128368854522705 + 0.01 * 8.519307136535645
Epoch 60, val loss: 1.815731167793274
Epoch 70, training loss: 1.8589701652526855 = 1.7763177156448364 + 0.01 * 8.265247344970703
Epoch 70, val loss: 1.785634160041809
Epoch 80, training loss: 1.817387342453003 = 1.7367337942123413 + 0.01 * 8.065357208251953
Epoch 80, val loss: 1.748053789138794
Epoch 90, training loss: 1.7617900371551514 = 1.6835449934005737 + 0.01 * 7.824507236480713
Epoch 90, val loss: 1.6972118616104126
Epoch 100, training loss: 1.6875863075256348 = 1.611197590827942 + 0.01 * 7.638877868652344
Epoch 100, val loss: 1.6315420866012573
Epoch 110, training loss: 1.5944764614105225 = 1.5190539360046387 + 0.01 * 7.542253017425537
Epoch 110, val loss: 1.5482505559921265
Epoch 120, training loss: 1.4884108304977417 = 1.4134782552719116 + 0.01 * 7.493260383605957
Epoch 120, val loss: 1.456342101097107
Epoch 130, training loss: 1.3770912885665894 = 1.3026615381240845 + 0.01 * 7.442971229553223
Epoch 130, val loss: 1.3626288175582886
Epoch 140, training loss: 1.2655490636825562 = 1.1915196180343628 + 0.01 * 7.402950286865234
Epoch 140, val loss: 1.2728540897369385
Epoch 150, training loss: 1.157685399055481 = 1.0839433670043945 + 0.01 * 7.374207973480225
Epoch 150, val loss: 1.1887784004211426
Epoch 160, training loss: 1.056386113166809 = 0.9828498959541321 + 0.01 * 7.353626728057861
Epoch 160, val loss: 1.112115740776062
Epoch 170, training loss: 0.963915228843689 = 0.8905012607574463 + 0.01 * 7.341400146484375
Epoch 170, val loss: 1.044876217842102
Epoch 180, training loss: 0.8816280961036682 = 0.8082891702651978 + 0.01 * 7.333893299102783
Epoch 180, val loss: 0.9882074594497681
Epoch 190, training loss: 0.8093065619468689 = 0.7360503077507019 + 0.01 * 7.325623512268066
Epoch 190, val loss: 0.942575991153717
Epoch 200, training loss: 0.7454931735992432 = 0.6723573803901672 + 0.01 * 7.313582420349121
Epoch 200, val loss: 0.9074288606643677
Epoch 210, training loss: 0.6882516145706177 = 0.6152850985527039 + 0.01 * 7.296652793884277
Epoch 210, val loss: 0.8810994625091553
Epoch 220, training loss: 0.6358120441436768 = 0.5630501508712769 + 0.01 * 7.276192665100098
Epoch 220, val loss: 0.8615068197250366
Epoch 230, training loss: 0.5868785977363586 = 0.5143635869026184 + 0.01 * 7.251502990722656
Epoch 230, val loss: 0.8470656275749207
Epoch 240, training loss: 0.5407634377479553 = 0.46845293045043945 + 0.01 * 7.231051445007324
Epoch 240, val loss: 0.8368618488311768
Epoch 250, training loss: 0.4971521198749542 = 0.42504748702049255 + 0.01 * 7.210464000701904
Epoch 250, val loss: 0.8307098150253296
Epoch 260, training loss: 0.4560666084289551 = 0.38415488600730896 + 0.01 * 7.1911725997924805
Epoch 260, val loss: 0.8283112049102783
Epoch 270, training loss: 0.4176662266254425 = 0.3458348214626312 + 0.01 * 7.183140754699707
Epoch 270, val loss: 0.8290103077888489
Epoch 280, training loss: 0.38182690739631653 = 0.31006908416748047 + 0.01 * 7.175782680511475
Epoch 280, val loss: 0.8319254517555237
Epoch 290, training loss: 0.3484598696231842 = 0.27679309248924255 + 0.01 * 7.166677474975586
Epoch 290, val loss: 0.8364983201026917
Epoch 300, training loss: 0.31759369373321533 = 0.24599887430667877 + 0.01 * 7.159481048583984
Epoch 300, val loss: 0.8424344062805176
Epoch 310, training loss: 0.28928983211517334 = 0.21777281165122986 + 0.01 * 7.151701927185059
Epoch 310, val loss: 0.8496214747428894
Epoch 320, training loss: 0.26358407735824585 = 0.19218428432941437 + 0.01 * 7.139978408813477
Epoch 320, val loss: 0.8581179976463318
Epoch 330, training loss: 0.2405318021774292 = 0.1692466139793396 + 0.01 * 7.128518581390381
Epoch 330, val loss: 0.8678736686706543
Epoch 340, training loss: 0.22008267045021057 = 0.14886942505836487 + 0.01 * 7.12132453918457
Epoch 340, val loss: 0.8788824081420898
Epoch 350, training loss: 0.20222505927085876 = 0.13089384138584137 + 0.01 * 7.133121013641357
Epoch 350, val loss: 0.8909120559692383
Epoch 360, training loss: 0.18625003099441528 = 0.1151570975780487 + 0.01 * 7.1092939376831055
Epoch 360, val loss: 0.9036687016487122
Epoch 370, training loss: 0.17240720987319946 = 0.10142317414283752 + 0.01 * 7.098403453826904
Epoch 370, val loss: 0.9169772863388062
Epoch 380, training loss: 0.16040727496147156 = 0.08946976810693741 + 0.01 * 7.09375
Epoch 380, val loss: 0.9307249188423157
Epoch 390, training loss: 0.14998754858970642 = 0.07909554988145828 + 0.01 * 7.089200973510742
Epoch 390, val loss: 0.9446951746940613
Epoch 400, training loss: 0.14090916514396667 = 0.0701013132929802 + 0.01 * 7.080785751342773
Epoch 400, val loss: 0.9588877558708191
Epoch 410, training loss: 0.13307461142539978 = 0.06230265647172928 + 0.01 * 7.077194690704346
Epoch 410, val loss: 0.9731433987617493
Epoch 420, training loss: 0.12622956931591034 = 0.055538300424814224 + 0.01 * 7.069126605987549
Epoch 420, val loss: 0.9874836206436157
Epoch 430, training loss: 0.12027157843112946 = 0.04966322332620621 + 0.01 * 7.060835361480713
Epoch 430, val loss: 1.0017714500427246
Epoch 440, training loss: 0.11517006158828735 = 0.044553134590387344 + 0.01 * 7.061692237854004
Epoch 440, val loss: 1.0159611701965332
Epoch 450, training loss: 0.11065107583999634 = 0.04010436311364174 + 0.01 * 7.054671287536621
Epoch 450, val loss: 1.0300195217132568
Epoch 460, training loss: 0.106808140873909 = 0.03621823713183403 + 0.01 * 7.058990478515625
Epoch 460, val loss: 1.0438945293426514
Epoch 470, training loss: 0.10322628170251846 = 0.03281756490468979 + 0.01 * 7.040872097015381
Epoch 470, val loss: 1.057628870010376
Epoch 480, training loss: 0.10011875629425049 = 0.029833097010850906 + 0.01 * 7.028565406799316
Epoch 480, val loss: 1.0711281299591064
Epoch 490, training loss: 0.09742967784404755 = 0.027206923812627792 + 0.01 * 7.022275924682617
Epoch 490, val loss: 1.0843650102615356
Epoch 500, training loss: 0.09506496042013168 = 0.02489231713116169 + 0.01 * 7.0172648429870605
Epoch 500, val loss: 1.0972594022750854
Epoch 510, training loss: 0.09296570718288422 = 0.022847313433885574 + 0.01 * 7.011838912963867
Epoch 510, val loss: 1.1097785234451294
Epoch 520, training loss: 0.0910634994506836 = 0.02103368565440178 + 0.01 * 7.002982139587402
Epoch 520, val loss: 1.1220256090164185
Epoch 530, training loss: 0.0893666222691536 = 0.019420413300395012 + 0.01 * 6.9946208000183105
Epoch 530, val loss: 1.1339250802993774
Epoch 540, training loss: 0.08804100006818771 = 0.017982469871640205 + 0.01 * 7.005853652954102
Epoch 540, val loss: 1.145416498184204
Epoch 550, training loss: 0.0865514874458313 = 0.016698306426405907 + 0.01 * 6.985318183898926
Epoch 550, val loss: 1.156574010848999
Epoch 560, training loss: 0.08539079874753952 = 0.01554787065833807 + 0.01 * 6.984292507171631
Epoch 560, val loss: 1.1673822402954102
Epoch 570, training loss: 0.0842556357383728 = 0.014513762667775154 + 0.01 * 6.974187850952148
Epoch 570, val loss: 1.17784583568573
Epoch 580, training loss: 0.08320634812116623 = 0.01358172856271267 + 0.01 * 6.962461948394775
Epoch 580, val loss: 1.1879804134368896
Epoch 590, training loss: 0.08227714896202087 = 0.012738998979330063 + 0.01 * 6.953814506530762
Epoch 590, val loss: 1.1977524757385254
Epoch 600, training loss: 0.08157235383987427 = 0.01197506207972765 + 0.01 * 6.959728717803955
Epoch 600, val loss: 1.2072116136550903
Epoch 610, training loss: 0.0806892141699791 = 0.011280876584351063 + 0.01 * 6.940833568572998
Epoch 610, val loss: 1.2164032459259033
Epoch 620, training loss: 0.0802045688033104 = 0.010648315772414207 + 0.01 * 6.955625534057617
Epoch 620, val loss: 1.2252976894378662
Epoch 630, training loss: 0.07941055297851562 = 0.010070852935314178 + 0.01 * 6.933969974517822
Epoch 630, val loss: 1.2339352369308472
Epoch 640, training loss: 0.07884997129440308 = 0.009542208164930344 + 0.01 * 6.930776596069336
Epoch 640, val loss: 1.2423584461212158
Epoch 650, training loss: 0.07839415222406387 = 0.009056955575942993 + 0.01 * 6.933719635009766
Epoch 650, val loss: 1.2504401206970215
Epoch 660, training loss: 0.07791824638843536 = 0.008610941469669342 + 0.01 * 6.930730819702148
Epoch 660, val loss: 1.2583297491073608
Epoch 670, training loss: 0.0773192048072815 = 0.008199740201234818 + 0.01 * 6.9119462966918945
Epoch 670, val loss: 1.2659844160079956
Epoch 680, training loss: 0.07706601917743683 = 0.007819758728146553 + 0.01 * 6.924626350402832
Epoch 680, val loss: 1.2733697891235352
Epoch 690, training loss: 0.07660385966300964 = 0.007468184921890497 + 0.01 * 6.913567543029785
Epoch 690, val loss: 1.280593752861023
Epoch 700, training loss: 0.07618698477745056 = 0.007142193149775267 + 0.01 * 6.904479026794434
Epoch 700, val loss: 1.287558913230896
Epoch 710, training loss: 0.07584154605865479 = 0.006839405279606581 + 0.01 * 6.900214672088623
Epoch 710, val loss: 1.294394850730896
Epoch 720, training loss: 0.07555781304836273 = 0.0065574049949646 + 0.01 * 6.900040626525879
Epoch 720, val loss: 1.3010231256484985
Epoch 730, training loss: 0.07517790049314499 = 0.00629447540268302 + 0.01 * 6.88834285736084
Epoch 730, val loss: 1.3074227571487427
Epoch 740, training loss: 0.0748913362622261 = 0.006049136631190777 + 0.01 * 6.884220123291016
Epoch 740, val loss: 1.313685655593872
Epoch 750, training loss: 0.07470832020044327 = 0.0058195386081933975 + 0.01 * 6.88887882232666
Epoch 750, val loss: 1.3197764158248901
Epoch 760, training loss: 0.07451024651527405 = 0.0056044794619083405 + 0.01 * 6.89057731628418
Epoch 760, val loss: 1.3257354497909546
Epoch 770, training loss: 0.07426812499761581 = 0.005403115414083004 + 0.01 * 6.886500835418701
Epoch 770, val loss: 1.3315359354019165
Epoch 780, training loss: 0.07396989315748215 = 0.005213683005422354 + 0.01 * 6.8756208419799805
Epoch 780, val loss: 1.3371682167053223
Epoch 790, training loss: 0.07402820140123367 = 0.005035486537963152 + 0.01 * 6.899271011352539
Epoch 790, val loss: 1.3425997495651245
Epoch 800, training loss: 0.07363124936819077 = 0.004868051037192345 + 0.01 * 6.876319885253906
Epoch 800, val loss: 1.3479899168014526
Epoch 810, training loss: 0.073334701359272 = 0.004710089880973101 + 0.01 * 6.862461090087891
Epoch 810, val loss: 1.3531497716903687
Epoch 820, training loss: 0.07313928753137589 = 0.004561052192002535 + 0.01 * 6.857823371887207
Epoch 820, val loss: 1.3582509756088257
Epoch 830, training loss: 0.07319436967372894 = 0.004420159850269556 + 0.01 * 6.877420902252197
Epoch 830, val loss: 1.3631126880645752
Epoch 840, training loss: 0.07286734879016876 = 0.004286953713744879 + 0.01 * 6.858039855957031
Epoch 840, val loss: 1.3679780960083008
Epoch 850, training loss: 0.07273580878973007 = 0.004160860553383827 + 0.01 * 6.857495307922363
Epoch 850, val loss: 1.3726880550384521
Epoch 860, training loss: 0.07249844074249268 = 0.004041142761707306 + 0.01 * 6.845730304718018
Epoch 860, val loss: 1.3772857189178467
Epoch 870, training loss: 0.07240201532840729 = 0.003927590325474739 + 0.01 * 6.847442626953125
Epoch 870, val loss: 1.3817627429962158
Epoch 880, training loss: 0.0722680613398552 = 0.0038198463153094053 + 0.01 * 6.844821453094482
Epoch 880, val loss: 1.3860986232757568
Epoch 890, training loss: 0.07220029830932617 = 0.003717492101714015 + 0.01 * 6.848280906677246
Epoch 890, val loss: 1.3903045654296875
Epoch 900, training loss: 0.07200509309768677 = 0.003620084607973695 + 0.01 * 6.838500499725342
Epoch 900, val loss: 1.3945399522781372
Epoch 910, training loss: 0.07222623378038406 = 0.003527214052155614 + 0.01 * 6.86990213394165
Epoch 910, val loss: 1.3985310792922974
Epoch 920, training loss: 0.07179068773984909 = 0.0034387814812362194 + 0.01 * 6.835190773010254
Epoch 920, val loss: 1.4025732278823853
Epoch 930, training loss: 0.07164687663316727 = 0.003354380140081048 + 0.01 * 6.829249858856201
Epoch 930, val loss: 1.4064041376113892
Epoch 940, training loss: 0.07161588966846466 = 0.0032737315632402897 + 0.01 * 6.8342156410217285
Epoch 940, val loss: 1.4102287292480469
Epoch 950, training loss: 0.07154648005962372 = 0.0031967302784323692 + 0.01 * 6.834975242614746
Epoch 950, val loss: 1.4139447212219238
Epoch 960, training loss: 0.07142097502946854 = 0.0031229930464178324 + 0.01 * 6.829798698425293
Epoch 960, val loss: 1.4176068305969238
Epoch 970, training loss: 0.07123468816280365 = 0.0030525054316967726 + 0.01 * 6.81821870803833
Epoch 970, val loss: 1.4211424589157104
Epoch 980, training loss: 0.07133922725915909 = 0.002985015045851469 + 0.01 * 6.835421085357666
Epoch 980, val loss: 1.4246121644973755
Epoch 990, training loss: 0.07107352465391159 = 0.0029204830061644316 + 0.01 * 6.815304279327393
Epoch 990, val loss: 1.428076982498169
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8070637849235636
The final CL Acc:0.73827, 0.02229, The final GNN Acc:0.80742, 0.00646
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13272])
remove edge: torch.Size([2, 7884])
updated graph: torch.Size([2, 10600])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0314085483551025 = 1.945440411567688 + 0.01 * 8.596823692321777
Epoch 0, val loss: 1.9427223205566406
Epoch 10, training loss: 2.0216124057769775 = 1.9356446266174316 + 0.01 * 8.59677505493164
Epoch 10, val loss: 1.9334580898284912
Epoch 20, training loss: 2.009488821029663 = 1.9235233068466187 + 0.01 * 8.596553802490234
Epoch 20, val loss: 1.9218641519546509
Epoch 30, training loss: 1.992194414138794 = 1.9062362909317017 + 0.01 * 8.595815658569336
Epoch 30, val loss: 1.9053853750228882
Epoch 40, training loss: 1.9662094116210938 = 1.88029146194458 + 0.01 * 8.591791152954102
Epoch 40, val loss: 1.8811966180801392
Epoch 50, training loss: 1.9291905164718628 = 1.843538522720337 + 0.01 * 8.565202713012695
Epoch 50, val loss: 1.848708987236023
Epoch 60, training loss: 1.886033058166504 = 1.8016552925109863 + 0.01 * 8.437771797180176
Epoch 60, val loss: 1.8150382041931152
Epoch 70, training loss: 1.846044659614563 = 1.7640916109085083 + 0.01 * 8.19530963897705
Epoch 70, val loss: 1.7834515571594238
Epoch 80, training loss: 1.797065258026123 = 1.7165580987930298 + 0.01 * 8.050714492797852
Epoch 80, val loss: 1.7368196249008179
Epoch 90, training loss: 1.7286975383758545 = 1.6494625806808472 + 0.01 * 7.923491954803467
Epoch 90, val loss: 1.675984501838684
Epoch 100, training loss: 1.6410781145095825 = 1.5629680156707764 + 0.01 * 7.811008453369141
Epoch 100, val loss: 1.603901743888855
Epoch 110, training loss: 1.544389009475708 = 1.4681000709533691 + 0.01 * 7.628890037536621
Epoch 110, val loss: 1.5246659517288208
Epoch 120, training loss: 1.4514917135238647 = 1.376112699508667 + 0.01 * 7.537896633148193
Epoch 120, val loss: 1.4475843906402588
Epoch 130, training loss: 1.3624942302703857 = 1.2875640392303467 + 0.01 * 7.4930219650268555
Epoch 130, val loss: 1.3731017112731934
Epoch 140, training loss: 1.273573398590088 = 1.1991417407989502 + 0.01 * 7.443168640136719
Epoch 140, val loss: 1.2996996641159058
Epoch 150, training loss: 1.1825486421585083 = 1.108655571937561 + 0.01 * 7.3893022537231445
Epoch 150, val loss: 1.2259880304336548
Epoch 160, training loss: 1.089575171470642 = 1.0161333084106445 + 0.01 * 7.344188213348389
Epoch 160, val loss: 1.152185082435608
Epoch 170, training loss: 0.9966524839401245 = 0.923439085483551 + 0.01 * 7.321342468261719
Epoch 170, val loss: 1.0784574747085571
Epoch 180, training loss: 0.9070467948913574 = 0.833953320980072 + 0.01 * 7.309349536895752
Epoch 180, val loss: 1.0078988075256348
Epoch 190, training loss: 0.8244969844818115 = 0.7514913082122803 + 0.01 * 7.300567626953125
Epoch 190, val loss: 0.9438558220863342
Epoch 200, training loss: 0.7512980699539185 = 0.6783756613731384 + 0.01 * 7.292242527008057
Epoch 200, val loss: 0.8893964290618896
Epoch 210, training loss: 0.6871920824050903 = 0.6143720746040344 + 0.01 * 7.2820000648498535
Epoch 210, val loss: 0.8455361127853394
Epoch 220, training loss: 0.6303896307945251 = 0.5577019453048706 + 0.01 * 7.268767833709717
Epoch 220, val loss: 0.8114398121833801
Epoch 230, training loss: 0.578997015953064 = 0.506468653678894 + 0.01 * 7.252835750579834
Epoch 230, val loss: 0.7857823967933655
Epoch 240, training loss: 0.5316532254219055 = 0.4592980742454529 + 0.01 * 7.235513687133789
Epoch 240, val loss: 0.7668271660804749
Epoch 250, training loss: 0.48752784729003906 = 0.415337473154068 + 0.01 * 7.219036102294922
Epoch 250, val loss: 0.7528682351112366
Epoch 260, training loss: 0.44617122411727905 = 0.374129056930542 + 0.01 * 7.20421838760376
Epoch 260, val loss: 0.7425674200057983
Epoch 270, training loss: 0.4073570966720581 = 0.3353937566280365 + 0.01 * 7.196333408355713
Epoch 270, val loss: 0.7352743148803711
Epoch 280, training loss: 0.37090829014778137 = 0.2990091145038605 + 0.01 * 7.18991756439209
Epoch 280, val loss: 0.7306442260742188
Epoch 290, training loss: 0.336948424577713 = 0.2651081383228302 + 0.01 * 7.1840291023254395
Epoch 290, val loss: 0.7286085486412048
Epoch 300, training loss: 0.30582675337791443 = 0.23402869701385498 + 0.01 * 7.179805755615234
Epoch 300, val loss: 0.7291402220726013
Epoch 310, training loss: 0.2778526246547699 = 0.20608702301979065 + 0.01 * 7.176559925079346
Epoch 310, val loss: 0.7323082685470581
Epoch 320, training loss: 0.2531360685825348 = 0.181393101811409 + 0.01 * 7.174297332763672
Epoch 320, val loss: 0.7380505800247192
Epoch 330, training loss: 0.23155981302261353 = 0.159837007522583 + 0.01 * 7.172281742095947
Epoch 330, val loss: 0.7461107969284058
Epoch 340, training loss: 0.2128515988588333 = 0.14115656912326813 + 0.01 * 7.169503211975098
Epoch 340, val loss: 0.7561249136924744
Epoch 350, training loss: 0.19667932391166687 = 0.12502725422382355 + 0.01 * 7.165206432342529
Epoch 350, val loss: 0.767691433429718
Epoch 360, training loss: 0.18268951773643494 = 0.11108964681625366 + 0.01 * 7.159987449645996
Epoch 360, val loss: 0.7803438305854797
Epoch 370, training loss: 0.17058062553405762 = 0.09900882840156555 + 0.01 * 7.157180309295654
Epoch 370, val loss: 0.7937515377998352
Epoch 380, training loss: 0.16002847254276276 = 0.08849940448999405 + 0.01 * 7.152906894683838
Epoch 380, val loss: 0.8076303601264954
Epoch 390, training loss: 0.150816410779953 = 0.07931745052337646 + 0.01 * 7.149895668029785
Epoch 390, val loss: 0.8217491507530212
Epoch 400, training loss: 0.14272499084472656 = 0.07126755267381668 + 0.01 * 7.145744800567627
Epoch 400, val loss: 0.8360084891319275
Epoch 410, training loss: 0.1355808675289154 = 0.06418724358081818 + 0.01 * 7.139362812042236
Epoch 410, val loss: 0.8502427935600281
Epoch 420, training loss: 0.12930598855018616 = 0.057943686842918396 + 0.01 * 7.136229515075684
Epoch 420, val loss: 0.8643773198127747
Epoch 430, training loss: 0.12373048812150955 = 0.052433326840400696 + 0.01 * 7.129715919494629
Epoch 430, val loss: 0.8783581256866455
Epoch 440, training loss: 0.11877481639385223 = 0.047563839703798294 + 0.01 * 7.121098518371582
Epoch 440, val loss: 0.8921439051628113
Epoch 450, training loss: 0.11438141763210297 = 0.043251749128103256 + 0.01 * 7.112966537475586
Epoch 450, val loss: 0.9056979417800903
Epoch 460, training loss: 0.11050568521022797 = 0.03942810371518135 + 0.01 * 7.107758522033691
Epoch 460, val loss: 0.9189605116844177
Epoch 470, training loss: 0.10713047534227371 = 0.03603372722864151 + 0.01 * 7.10967493057251
Epoch 470, val loss: 0.9318944811820984
Epoch 480, training loss: 0.10392046719789505 = 0.033015958964824677 + 0.01 * 7.090450763702393
Epoch 480, val loss: 0.9445191621780396
Epoch 490, training loss: 0.1011127233505249 = 0.030327491462230682 + 0.01 * 7.0785231590271
Epoch 490, val loss: 0.9567782878875732
Epoch 500, training loss: 0.09872188419103622 = 0.027931245043873787 + 0.01 * 7.079063892364502
Epoch 500, val loss: 0.9686785936355591
Epoch 510, training loss: 0.0965094119310379 = 0.025792371481657028 + 0.01 * 7.071704387664795
Epoch 510, val loss: 0.9802132844924927
Epoch 520, training loss: 0.09439800679683685 = 0.023876404389739037 + 0.01 * 7.052160263061523
Epoch 520, val loss: 0.9914107918739319
Epoch 530, training loss: 0.09256577491760254 = 0.02215462550520897 + 0.01 * 7.041114807128906
Epoch 530, val loss: 1.002292513847351
Epoch 540, training loss: 0.09096620976924896 = 0.020603690296411514 + 0.01 * 7.036252498626709
Epoch 540, val loss: 1.012869119644165
Epoch 550, training loss: 0.08954145759344101 = 0.019208459183573723 + 0.01 * 7.033299446105957
Epoch 550, val loss: 1.023030161857605
Epoch 560, training loss: 0.088129423558712 = 0.017950506880879402 + 0.01 * 7.017892360687256
Epoch 560, val loss: 1.032840371131897
Epoch 570, training loss: 0.0868498831987381 = 0.016811244189739227 + 0.01 * 7.003864288330078
Epoch 570, val loss: 1.0424015522003174
Epoch 580, training loss: 0.08588071912527084 = 0.015774862840771675 + 0.01 * 7.010586261749268
Epoch 580, val loss: 1.0516833066940308
Epoch 590, training loss: 0.08487935364246368 = 0.014831562526524067 + 0.01 * 7.004778861999512
Epoch 590, val loss: 1.0606458187103271
Epoch 600, training loss: 0.08382817357778549 = 0.013970199041068554 + 0.01 * 6.985797882080078
Epoch 600, val loss: 1.0694000720977783
Epoch 610, training loss: 0.08296263217926025 = 0.013182774186134338 + 0.01 * 6.977985858917236
Epoch 610, val loss: 1.0778361558914185
Epoch 620, training loss: 0.08219286799430847 = 0.012460870668292046 + 0.01 * 6.973199844360352
Epoch 620, val loss: 1.086034893989563
Epoch 630, training loss: 0.08148543536663055 = 0.011797736398875713 + 0.01 * 6.9687700271606445
Epoch 630, val loss: 1.0940099954605103
Epoch 640, training loss: 0.08088119328022003 = 0.011187378317117691 + 0.01 * 6.969381809234619
Epoch 640, val loss: 1.1017082929611206
Epoch 650, training loss: 0.08026681840419769 = 0.010625272057950497 + 0.01 * 6.9641547203063965
Epoch 650, val loss: 1.109169602394104
Epoch 660, training loss: 0.0795293003320694 = 0.010105708613991737 + 0.01 * 6.942359447479248
Epoch 660, val loss: 1.1164742708206177
Epoch 670, training loss: 0.07895626872777939 = 0.009624785743653774 + 0.01 * 6.933148384094238
Epoch 670, val loss: 1.1234978437423706
Epoch 680, training loss: 0.07863658666610718 = 0.009178576990962029 + 0.01 * 6.945801258087158
Epoch 680, val loss: 1.1303551197052002
Epoch 690, training loss: 0.07803966850042343 = 0.008764224126935005 + 0.01 * 6.927545070648193
Epoch 690, val loss: 1.137000560760498
Epoch 700, training loss: 0.07761196792125702 = 0.00837906077504158 + 0.01 * 6.923290729522705
Epoch 700, val loss: 1.1434404850006104
Epoch 710, training loss: 0.07729262113571167 = 0.008020427078008652 + 0.01 * 6.927219390869141
Epoch 710, val loss: 1.1496663093566895
Epoch 720, training loss: 0.07673511654138565 = 0.007685874588787556 + 0.01 * 6.904924392700195
Epoch 720, val loss: 1.155791997909546
Epoch 730, training loss: 0.0764571875333786 = 0.00737346475943923 + 0.01 * 6.908372402191162
Epoch 730, val loss: 1.1617480516433716
Epoch 740, training loss: 0.07601331919431686 = 0.007081132847815752 + 0.01 * 6.893218517303467
Epoch 740, val loss: 1.1675106287002563
Epoch 750, training loss: 0.07587570697069168 = 0.006807406898587942 + 0.01 * 6.906829833984375
Epoch 750, val loss: 1.1731303930282593
Epoch 760, training loss: 0.07540585845708847 = 0.0065510873682796955 + 0.01 * 6.885477542877197
Epoch 760, val loss: 1.178574800491333
Epoch 770, training loss: 0.07516838610172272 = 0.006310549098998308 + 0.01 * 6.885783672332764
Epoch 770, val loss: 1.1838868856430054
Epoch 780, training loss: 0.07479595392942429 = 0.006084410473704338 + 0.01 * 6.871154308319092
Epoch 780, val loss: 1.1890814304351807
Epoch 790, training loss: 0.07467249035835266 = 0.00587152224034071 + 0.01 * 6.880096912384033
Epoch 790, val loss: 1.1941375732421875
Epoch 800, training loss: 0.0743522047996521 = 0.005671325605362654 + 0.01 * 6.868088245391846
Epoch 800, val loss: 1.1990164518356323
Epoch 810, training loss: 0.07431510090827942 = 0.005482156295329332 + 0.01 * 6.883294105529785
Epoch 810, val loss: 1.203782558441162
Epoch 820, training loss: 0.07390695810317993 = 0.005303701851516962 + 0.01 * 6.860325336456299
Epoch 820, val loss: 1.208464503288269
Epoch 830, training loss: 0.07370604574680328 = 0.0051351189613342285 + 0.01 * 6.85709285736084
Epoch 830, val loss: 1.2130149602890015
Epoch 840, training loss: 0.07342422753572464 = 0.004975496791303158 + 0.01 * 6.844873428344727
Epoch 840, val loss: 1.2174254655838013
Epoch 850, training loss: 0.07309584319591522 = 0.004824128467589617 + 0.01 * 6.827171325683594
Epoch 850, val loss: 1.2217483520507812
Epoch 860, training loss: 0.07307437062263489 = 0.0046805995516479015 + 0.01 * 6.8393778800964355
Epoch 860, val loss: 1.2259693145751953
Epoch 870, training loss: 0.07294189929962158 = 0.004544504918158054 + 0.01 * 6.839739799499512
Epoch 870, val loss: 1.2300636768341064
Epoch 880, training loss: 0.07260185480117798 = 0.004415289498865604 + 0.01 * 6.818656921386719
Epoch 880, val loss: 1.2340660095214844
Epoch 890, training loss: 0.07272438704967499 = 0.004292268306016922 + 0.01 * 6.843212604522705
Epoch 890, val loss: 1.2379640340805054
Epoch 900, training loss: 0.07254330813884735 = 0.004175033885985613 + 0.01 * 6.836827278137207
Epoch 900, val loss: 1.241821527481079
Epoch 910, training loss: 0.07227136194705963 = 0.004063621629029512 + 0.01 * 6.820774078369141
Epoch 910, val loss: 1.245501160621643
Epoch 920, training loss: 0.0721110850572586 = 0.003957280423492193 + 0.01 * 6.815381050109863
Epoch 920, val loss: 1.249174952507019
Epoch 930, training loss: 0.07197465747594833 = 0.003855863818898797 + 0.01 * 6.811879634857178
Epoch 930, val loss: 1.2527101039886475
Epoch 940, training loss: 0.07184016704559326 = 0.0037589073181152344 + 0.01 * 6.808126449584961
Epoch 940, val loss: 1.2562137842178345
Epoch 950, training loss: 0.07167107611894608 = 0.00366626912727952 + 0.01 * 6.800480842590332
Epoch 950, val loss: 1.259585976600647
Epoch 960, training loss: 0.07157562673091888 = 0.003577678231522441 + 0.01 * 6.799795150756836
Epoch 960, val loss: 1.262919306755066
Epoch 970, training loss: 0.07158870995044708 = 0.00349291879683733 + 0.01 * 6.809578895568848
Epoch 970, val loss: 1.2660999298095703
Epoch 980, training loss: 0.07138217240571976 = 0.0034118047915399075 + 0.01 * 6.797036647796631
Epoch 980, val loss: 1.2692846059799194
Epoch 990, training loss: 0.0711899995803833 = 0.0033340491354465485 + 0.01 * 6.785594463348389
Epoch 990, val loss: 1.2723203897476196
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8302583025830259
=== training gcn model ===
Epoch 0, training loss: 2.033548355102539 = 1.9475799798965454 + 0.01 * 8.59682846069336
Epoch 0, val loss: 1.9477554559707642
Epoch 10, training loss: 2.0229692459106445 = 1.9370018243789673 + 0.01 * 8.596748352050781
Epoch 10, val loss: 1.9377131462097168
Epoch 20, training loss: 2.0098133087158203 = 1.9238481521606445 + 0.01 * 8.596511840820312
Epoch 20, val loss: 1.9247961044311523
Epoch 30, training loss: 1.991523027420044 = 1.9055663347244263 + 0.01 * 8.595672607421875
Epoch 30, val loss: 1.9065128564834595
Epoch 40, training loss: 1.9650746583938599 = 1.8791735172271729 + 0.01 * 8.590112686157227
Epoch 40, val loss: 1.8802413940429688
Epoch 50, training loss: 1.9288344383239746 = 1.8433157205581665 + 0.01 * 8.551868438720703
Epoch 50, val loss: 1.8460208177566528
Epoch 60, training loss: 1.8873878717422485 = 1.803788185119629 + 0.01 * 8.359964370727539
Epoch 60, val loss: 1.8116430044174194
Epoch 70, training loss: 1.8517471551895142 = 1.769562005996704 + 0.01 * 8.218510627746582
Epoch 70, val loss: 1.7828078269958496
Epoch 80, training loss: 1.808019995689392 = 1.7280678749084473 + 0.01 * 7.995217323303223
Epoch 80, val loss: 1.743709921836853
Epoch 90, training loss: 1.7468225955963135 = 1.669287919998169 + 0.01 * 7.753468990325928
Epoch 90, val loss: 1.6903156042099
Epoch 100, training loss: 1.6660059690475464 = 1.5907080173492432 + 0.01 * 7.529789924621582
Epoch 100, val loss: 1.623071312904358
Epoch 110, training loss: 1.5732674598693848 = 1.4993890523910522 + 0.01 * 7.387837886810303
Epoch 110, val loss: 1.5463764667510986
Epoch 120, training loss: 1.4806361198425293 = 1.4074125289916992 + 0.01 * 7.322357177734375
Epoch 120, val loss: 1.4691917896270752
Epoch 130, training loss: 1.3921267986297607 = 1.3193494081497192 + 0.01 * 7.277740478515625
Epoch 130, val loss: 1.3972824811935425
Epoch 140, training loss: 1.3060452938079834 = 1.2336310148239136 + 0.01 * 7.241430282592773
Epoch 140, val loss: 1.329298973083496
Epoch 150, training loss: 1.2217657566070557 = 1.1496134996414185 + 0.01 * 7.2152252197265625
Epoch 150, val loss: 1.2641303539276123
Epoch 160, training loss: 1.1383404731750488 = 1.0663577318191528 + 0.01 * 7.198278903961182
Epoch 160, val loss: 1.2005575895309448
Epoch 170, training loss: 1.0545071363449097 = 0.9826467037200928 + 0.01 * 7.186039924621582
Epoch 170, val loss: 1.1361968517303467
Epoch 180, training loss: 0.9707670211791992 = 0.8989909887313843 + 0.01 * 7.177602767944336
Epoch 180, val loss: 1.0706501007080078
Epoch 190, training loss: 0.8887996673583984 = 0.817080020904541 + 0.01 * 7.171967029571533
Epoch 190, val loss: 1.0052704811096191
Epoch 200, training loss: 0.8104712963104248 = 0.7387970089912415 + 0.01 * 7.167426109313965
Epoch 200, val loss: 0.9427986145019531
Epoch 210, training loss: 0.7374430894851685 = 0.6658112406730652 + 0.01 * 7.163184642791748
Epoch 210, val loss: 0.8855178952217102
Epoch 220, training loss: 0.670745313167572 = 0.5991514325141907 + 0.01 * 7.159388542175293
Epoch 220, val loss: 0.8352724313735962
Epoch 230, training loss: 0.6103955507278442 = 0.5388419032096863 + 0.01 * 7.155365467071533
Epoch 230, val loss: 0.7924695611000061
Epoch 240, training loss: 0.5556852221488953 = 0.4841771125793457 + 0.01 * 7.150810241699219
Epoch 240, val loss: 0.7570099234580994
Epoch 250, training loss: 0.5056207776069641 = 0.4341602027416229 + 0.01 * 7.146058082580566
Epoch 250, val loss: 0.727980375289917
Epoch 260, training loss: 0.4593523144721985 = 0.38795334100723267 + 0.01 * 7.139896392822266
Epoch 260, val loss: 0.7044749855995178
Epoch 270, training loss: 0.41640380024909973 = 0.3450656831264496 + 0.01 * 7.13381290435791
Epoch 270, val loss: 0.6852984428405762
Epoch 280, training loss: 0.3766557276248932 = 0.30539578199386597 + 0.01 * 7.125994682312012
Epoch 280, val loss: 0.669486939907074
Epoch 290, training loss: 0.340160995721817 = 0.26895081996917725 + 0.01 * 7.121017932891846
Epoch 290, val loss: 0.6565268635749817
Epoch 300, training loss: 0.30679893493652344 = 0.23569075763225555 + 0.01 * 7.110817909240723
Epoch 300, val loss: 0.6461402177810669
Epoch 310, training loss: 0.27659299969673157 = 0.20551404356956482 + 0.01 * 7.1078948974609375
Epoch 310, val loss: 0.6381883025169373
Epoch 320, training loss: 0.24937009811401367 = 0.17841044068336487 + 0.01 * 7.0959649085998535
Epoch 320, val loss: 0.6326958537101746
Epoch 330, training loss: 0.22527699172496796 = 0.15441352128982544 + 0.01 * 7.0863471031188965
Epoch 330, val loss: 0.6296202540397644
Epoch 340, training loss: 0.20443537831306458 = 0.13349995017051697 + 0.01 * 7.093543529510498
Epoch 340, val loss: 0.6288056373596191
Epoch 350, training loss: 0.18622320890426636 = 0.11551524698734283 + 0.01 * 7.070796966552734
Epoch 350, val loss: 0.6300902366638184
Epoch 360, training loss: 0.17085880041122437 = 0.10016297549009323 + 0.01 * 7.069582462310791
Epoch 360, val loss: 0.6333017945289612
Epoch 370, training loss: 0.15769650042057037 = 0.08711399883031845 + 0.01 * 7.058250427246094
Epoch 370, val loss: 0.6381304264068604
Epoch 380, training loss: 0.14670491218566895 = 0.07605889439582825 + 0.01 * 7.064601421356201
Epoch 380, val loss: 0.644389271736145
Epoch 390, training loss: 0.13716556131839752 = 0.06670776754617691 + 0.01 * 7.045779228210449
Epoch 390, val loss: 0.6516767740249634
Epoch 400, training loss: 0.1291327029466629 = 0.05877847597002983 + 0.01 * 7.0354228019714355
Epoch 400, val loss: 0.6598267555236816
Epoch 410, training loss: 0.12231364846229553 = 0.052030134946107864 + 0.01 * 7.028351783752441
Epoch 410, val loss: 0.6686580777168274
Epoch 420, training loss: 0.11676785349845886 = 0.046275246888399124 + 0.01 * 7.04926061630249
Epoch 420, val loss: 0.6779316663742065
Epoch 430, training loss: 0.11152732372283936 = 0.04135434329509735 + 0.01 * 7.017298221588135
Epoch 430, val loss: 0.6874430179595947
Epoch 440, training loss: 0.10723738372325897 = 0.037116218358278275 + 0.01 * 7.012115955352783
Epoch 440, val loss: 0.6971766948699951
Epoch 450, training loss: 0.10353974997997284 = 0.033449240028858185 + 0.01 * 7.009051322937012
Epoch 450, val loss: 0.7069697976112366
Epoch 460, training loss: 0.10019572824239731 = 0.030266227200627327 + 0.01 * 6.992950439453125
Epoch 460, val loss: 0.716700553894043
Epoch 470, training loss: 0.09733772277832031 = 0.02748947963118553 + 0.01 * 6.984824180603027
Epoch 470, val loss: 0.7263606786727905
Epoch 480, training loss: 0.09510219097137451 = 0.0250625628978014 + 0.01 * 7.003962993621826
Epoch 480, val loss: 0.735893726348877
Epoch 490, training loss: 0.09277326613664627 = 0.022938678041100502 + 0.01 * 6.983458995819092
Epoch 490, val loss: 0.745150625705719
Epoch 500, training loss: 0.09082616120576859 = 0.021066470071673393 + 0.01 * 6.975968837738037
Epoch 500, val loss: 0.7542354464530945
Epoch 510, training loss: 0.089055635035038 = 0.019410230219364166 + 0.01 * 6.964540481567383
Epoch 510, val loss: 0.7630918025970459
Epoch 520, training loss: 0.08747004717588425 = 0.0179393719881773 + 0.01 * 6.953067302703857
Epoch 520, val loss: 0.7717447280883789
Epoch 530, training loss: 0.08606789261102676 = 0.016625704243779182 + 0.01 * 6.94421911239624
Epoch 530, val loss: 0.7801940441131592
Epoch 540, training loss: 0.08553352952003479 = 0.015448510646820068 + 0.01 * 7.008502006530762
Epoch 540, val loss: 0.7884054183959961
Epoch 550, training loss: 0.0837823897600174 = 0.014397215098142624 + 0.01 * 6.9385175704956055
Epoch 550, val loss: 0.7963748574256897
Epoch 560, training loss: 0.082797110080719 = 0.013450926169753075 + 0.01 * 6.93461799621582
Epoch 560, val loss: 0.8041267991065979
Epoch 570, training loss: 0.08184155821800232 = 0.012595918029546738 + 0.01 * 6.924564361572266
Epoch 570, val loss: 0.8116980195045471
Epoch 580, training loss: 0.08104879409074783 = 0.011820053681731224 + 0.01 * 6.9228739738464355
Epoch 580, val loss: 0.8190749883651733
Epoch 590, training loss: 0.08030031621456146 = 0.011116462759673595 + 0.01 * 6.9183855056762695
Epoch 590, val loss: 0.8261986374855042
Epoch 600, training loss: 0.0795731320977211 = 0.010475972667336464 + 0.01 * 6.9097161293029785
Epoch 600, val loss: 0.8331980109214783
Epoch 610, training loss: 0.0789937749505043 = 0.009890799410641193 + 0.01 * 6.910297393798828
Epoch 610, val loss: 0.8399920463562012
Epoch 620, training loss: 0.07838528603315353 = 0.009356713853776455 + 0.01 * 6.902857303619385
Epoch 620, val loss: 0.8466329574584961
Epoch 630, training loss: 0.07788300514221191 = 0.008867193013429642 + 0.01 * 6.901581764221191
Epoch 630, val loss: 0.8530563116073608
Epoch 640, training loss: 0.07727820426225662 = 0.00841759704053402 + 0.01 * 6.88606071472168
Epoch 640, val loss: 0.8592818975448608
Epoch 650, training loss: 0.07686133682727814 = 0.008003756403923035 + 0.01 * 6.885758399963379
Epoch 650, val loss: 0.8654119372367859
Epoch 660, training loss: 0.07644043117761612 = 0.00762170972302556 + 0.01 * 6.881871700286865
Epoch 660, val loss: 0.8713266253471375
Epoch 670, training loss: 0.07598631083965302 = 0.007268260698765516 + 0.01 * 6.871805667877197
Epoch 670, val loss: 0.8771044015884399
Epoch 680, training loss: 0.07568695396184921 = 0.006940686143934727 + 0.01 * 6.874627113342285
Epoch 680, val loss: 0.8827608823776245
Epoch 690, training loss: 0.07520969212055206 = 0.006636993493884802 + 0.01 * 6.857270240783691
Epoch 690, val loss: 0.8882312178611755
Epoch 700, training loss: 0.0749092847108841 = 0.006354650482535362 + 0.01 * 6.855463027954102
Epoch 700, val loss: 0.8935670256614685
Epoch 710, training loss: 0.07470301538705826 = 0.006091552320867777 + 0.01 * 6.861146450042725
Epoch 710, val loss: 0.8987632989883423
Epoch 720, training loss: 0.07436291128396988 = 0.0058459071442484856 + 0.01 * 6.851700782775879
Epoch 720, val loss: 0.9038960933685303
Epoch 730, training loss: 0.07417807728052139 = 0.005616945214569569 + 0.01 * 6.856113433837891
Epoch 730, val loss: 0.9088223576545715
Epoch 740, training loss: 0.07396961003541946 = 0.005402640905231237 + 0.01 * 6.856697082519531
Epoch 740, val loss: 0.9136412143707275
Epoch 750, training loss: 0.07349821925163269 = 0.005202261731028557 + 0.01 * 6.829596042633057
Epoch 750, val loss: 0.9183377623558044
Epoch 760, training loss: 0.0734032541513443 = 0.0050140791572630405 + 0.01 * 6.838918209075928
Epoch 760, val loss: 0.9229380488395691
Epoch 770, training loss: 0.07311619073152542 = 0.004837249871343374 + 0.01 * 6.82789421081543
Epoch 770, val loss: 0.9274206757545471
Epoch 780, training loss: 0.07298778742551804 = 0.00467113358899951 + 0.01 * 6.831665515899658
Epoch 780, val loss: 0.9317721724510193
Epoch 790, training loss: 0.07285317033529282 = 0.004514484666287899 + 0.01 * 6.833868980407715
Epoch 790, val loss: 0.9360546469688416
Epoch 800, training loss: 0.0724710077047348 = 0.004367002286016941 + 0.01 * 6.81040096282959
Epoch 800, val loss: 0.940189778804779
Epoch 810, training loss: 0.07244119048118591 = 0.004227607045322657 + 0.01 * 6.8213582038879395
Epoch 810, val loss: 0.9442175030708313
Epoch 820, training loss: 0.07224646955728531 = 0.004095953889191151 + 0.01 * 6.815051555633545
Epoch 820, val loss: 0.948193371295929
Epoch 830, training loss: 0.07208804786205292 = 0.003971131052821875 + 0.01 * 6.811692237854004
Epoch 830, val loss: 0.9520576596260071
Epoch 840, training loss: 0.07196903228759766 = 0.00385303539223969 + 0.01 * 6.8115997314453125
Epoch 840, val loss: 0.9558752179145813
Epoch 850, training loss: 0.07174471765756607 = 0.0037413935642689466 + 0.01 * 6.800333023071289
Epoch 850, val loss: 0.9595611691474915
Epoch 860, training loss: 0.07160912454128265 = 0.0036353953182697296 + 0.01 * 6.7973737716674805
Epoch 860, val loss: 0.9631646275520325
Epoch 870, training loss: 0.0715830847620964 = 0.003534492338076234 + 0.01 * 6.804859161376953
Epoch 870, val loss: 0.9666942358016968
Epoch 880, training loss: 0.07133980095386505 = 0.0034388741478323936 + 0.01 * 6.790092468261719
Epoch 880, val loss: 0.9701635241508484
Epoch 890, training loss: 0.07126673310995102 = 0.003347783349454403 + 0.01 * 6.791895389556885
Epoch 890, val loss: 0.973526656627655
Epoch 900, training loss: 0.07110999524593353 = 0.0032607533503323793 + 0.01 * 6.784924030303955
Epoch 900, val loss: 0.9768094420433044
Epoch 910, training loss: 0.07108546793460846 = 0.003178222570568323 + 0.01 * 6.790724277496338
Epoch 910, val loss: 0.9800564050674438
Epoch 920, training loss: 0.07084580510854721 = 0.0030994301196187735 + 0.01 * 6.774637699127197
Epoch 920, val loss: 0.9831969141960144
Epoch 930, training loss: 0.07085014134645462 = 0.0030242428183555603 + 0.01 * 6.782589912414551
Epoch 930, val loss: 0.9863008856773376
Epoch 940, training loss: 0.07087665796279907 = 0.002952368464320898 + 0.01 * 6.792428970336914
Epoch 940, val loss: 0.9892446994781494
Epoch 950, training loss: 0.0706123411655426 = 0.0028837919235229492 + 0.01 * 6.772854804992676
Epoch 950, val loss: 0.9922452569007874
Epoch 960, training loss: 0.0707053691148758 = 0.002818111330270767 + 0.01 * 6.788726329803467
Epoch 960, val loss: 0.995121955871582
Epoch 970, training loss: 0.07039724290370941 = 0.0027554668486118317 + 0.01 * 6.764178276062012
Epoch 970, val loss: 0.9978876113891602
Epoch 980, training loss: 0.07024889439344406 = 0.00269519817084074 + 0.01 * 6.755370140075684
Epoch 980, val loss: 1.000666618347168
Epoch 990, training loss: 0.07031865417957306 = 0.0026375632733106613 + 0.01 * 6.76810884475708
Epoch 990, val loss: 1.0034748315811157
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.0321807861328125 = 1.9462132453918457 + 0.01 * 8.596766471862793
Epoch 0, val loss: 1.944428563117981
Epoch 10, training loss: 2.0216188430786133 = 1.935652256011963 + 0.01 * 8.596650123596191
Epoch 10, val loss: 1.9334676265716553
Epoch 20, training loss: 2.008174180984497 = 1.9222114086151123 + 0.01 * 8.596285820007324
Epoch 20, val loss: 1.9195127487182617
Epoch 30, training loss: 1.9890393018722534 = 1.9030898809432983 + 0.01 * 8.59494686126709
Epoch 30, val loss: 1.8997197151184082
Epoch 40, training loss: 1.9607934951782227 = 1.8749346733093262 + 0.01 * 8.585882186889648
Epoch 40, val loss: 1.871196985244751
Epoch 50, training loss: 1.9221396446228027 = 1.8369120359420776 + 0.01 * 8.522761344909668
Epoch 50, val loss: 1.835018277168274
Epoch 60, training loss: 1.8798340559005737 = 1.7973006963729858 + 0.01 * 8.253335952758789
Epoch 60, val loss: 1.8021870851516724
Epoch 70, training loss: 1.8424975872039795 = 1.761671781539917 + 0.01 * 8.082575798034668
Epoch 70, val loss: 1.7740917205810547
Epoch 80, training loss: 1.792616844177246 = 1.7150592803955078 + 0.01 * 7.755751132965088
Epoch 80, val loss: 1.7321892976760864
Epoch 90, training loss: 1.7267343997955322 = 1.6517133712768555 + 0.01 * 7.502102851867676
Epoch 90, val loss: 1.6745423078536987
Epoch 100, training loss: 1.6434739828109741 = 1.5696041584014893 + 0.01 * 7.386982440948486
Epoch 100, val loss: 1.6025924682617188
Epoch 110, training loss: 1.5503140687942505 = 1.4770336151123047 + 0.01 * 7.32804536819458
Epoch 110, val loss: 1.524351954460144
Epoch 120, training loss: 1.4569345712661743 = 1.384088397026062 + 0.01 * 7.284616470336914
Epoch 120, val loss: 1.4499139785766602
Epoch 130, training loss: 1.365396499633789 = 1.2927868366241455 + 0.01 * 7.260970592498779
Epoch 130, val loss: 1.378266453742981
Epoch 140, training loss: 1.2743549346923828 = 1.201933741569519 + 0.01 * 7.242123126983643
Epoch 140, val loss: 1.3076627254486084
Epoch 150, training loss: 1.1846896409988403 = 1.1124171018600464 + 0.01 * 7.227258205413818
Epoch 150, val loss: 1.238785982131958
Epoch 160, training loss: 1.0990545749664307 = 1.026888370513916 + 0.01 * 7.216615676879883
Epoch 160, val loss: 1.1749624013900757
Epoch 170, training loss: 1.018678903579712 = 0.946631133556366 + 0.01 * 7.204778671264648
Epoch 170, val loss: 1.1163525581359863
Epoch 180, training loss: 0.9424532651901245 = 0.8705750703811646 + 0.01 * 7.1878228187561035
Epoch 180, val loss: 1.0613731145858765
Epoch 190, training loss: 0.8691387176513672 = 0.7974371314048767 + 0.01 * 7.170158863067627
Epoch 190, val loss: 1.0086774826049805
Epoch 200, training loss: 0.798761248588562 = 0.7272220849990845 + 0.01 * 7.15391731262207
Epoch 200, val loss: 0.9585668444633484
Epoch 210, training loss: 0.7325499057769775 = 0.6611390113830566 + 0.01 * 7.141087055206299
Epoch 210, val loss: 0.9130311012268066
Epoch 220, training loss: 0.6713729500770569 = 0.6000570058822632 + 0.01 * 7.131593227386475
Epoch 220, val loss: 0.873680830001831
Epoch 230, training loss: 0.6153004765510559 = 0.5440356731414795 + 0.01 * 7.126481056213379
Epoch 230, val loss: 0.8413446545600891
Epoch 240, training loss: 0.5638380646705627 = 0.4926663041114807 + 0.01 * 7.117175102233887
Epoch 240, val loss: 0.8158740401268005
Epoch 250, training loss: 0.5166653394699097 = 0.44555848836898804 + 0.01 * 7.110682487487793
Epoch 250, val loss: 0.7969748973846436
Epoch 260, training loss: 0.4733971059322357 = 0.4023628830909729 + 0.01 * 7.103423118591309
Epoch 260, val loss: 0.7837899327278137
Epoch 270, training loss: 0.43369966745376587 = 0.36265671253204346 + 0.01 * 7.104296684265137
Epoch 270, val loss: 0.7749595642089844
Epoch 280, training loss: 0.39689528942108154 = 0.32592153549194336 + 0.01 * 7.097373962402344
Epoch 280, val loss: 0.76932293176651
Epoch 290, training loss: 0.3624856173992157 = 0.2916511297225952 + 0.01 * 7.083448886871338
Epoch 290, val loss: 0.766164243221283
Epoch 300, training loss: 0.3303650915622711 = 0.25962212681770325 + 0.01 * 7.074296474456787
Epoch 300, val loss: 0.7654582262039185
Epoch 310, training loss: 0.30071017146110535 = 0.2299102246761322 + 0.01 * 7.079995632171631
Epoch 310, val loss: 0.7671213150024414
Epoch 320, training loss: 0.2733798623085022 = 0.20273329317569733 + 0.01 * 7.064657688140869
Epoch 320, val loss: 0.7709203362464905
Epoch 330, training loss: 0.24881288409233093 = 0.1782848834991455 + 0.01 * 7.052799701690674
Epoch 330, val loss: 0.7767612934112549
Epoch 340, training loss: 0.2274337112903595 = 0.15661309659481049 + 0.01 * 7.082061290740967
Epoch 340, val loss: 0.784486711025238
Epoch 350, training loss: 0.2082241177558899 = 0.13767968118190765 + 0.01 * 7.054442882537842
Epoch 350, val loss: 0.7937663793563843
Epoch 360, training loss: 0.19160926342010498 = 0.12122848629951477 + 0.01 * 7.038078784942627
Epoch 360, val loss: 0.8043708801269531
Epoch 370, training loss: 0.17719316482543945 = 0.1069696769118309 + 0.01 * 7.022348880767822
Epoch 370, val loss: 0.8160383701324463
Epoch 380, training loss: 0.16473478078842163 = 0.09462261199951172 + 0.01 * 7.011218070983887
Epoch 380, val loss: 0.8284270763397217
Epoch 390, training loss: 0.1542477011680603 = 0.08394387364387512 + 0.01 * 7.030383110046387
Epoch 390, val loss: 0.8412430882453918
Epoch 400, training loss: 0.14472466707229614 = 0.07471311837434769 + 0.01 * 7.001153945922852
Epoch 400, val loss: 0.8543078899383545
Epoch 410, training loss: 0.136863112449646 = 0.06671761721372604 + 0.01 * 7.0145487785339355
Epoch 410, val loss: 0.867465615272522
Epoch 420, training loss: 0.12965109944343567 = 0.05977397784590721 + 0.01 * 6.9877119064331055
Epoch 420, val loss: 0.8806234002113342
Epoch 430, training loss: 0.12397986650466919 = 0.053735192865133286 + 0.01 * 7.024467468261719
Epoch 430, val loss: 0.8936938643455505
Epoch 440, training loss: 0.1183260828256607 = 0.04848048463463783 + 0.01 * 6.984560012817383
Epoch 440, val loss: 0.906543493270874
Epoch 450, training loss: 0.11361284554004669 = 0.04388389363884926 + 0.01 * 6.972895622253418
Epoch 450, val loss: 0.9192036986351013
Epoch 460, training loss: 0.10948435962200165 = 0.039846669882535934 + 0.01 * 6.96376895904541
Epoch 460, val loss: 0.9316911697387695
Epoch 470, training loss: 0.1058347150683403 = 0.0362895205616951 + 0.01 * 6.954519748687744
Epoch 470, val loss: 0.9439042210578918
Epoch 480, training loss: 0.10279780626296997 = 0.033147718757390976 + 0.01 * 6.96500825881958
Epoch 480, val loss: 0.9558717012405396
Epoch 490, training loss: 0.09995363652706146 = 0.030370518565177917 + 0.01 * 6.958312034606934
Epoch 490, val loss: 0.9675522446632385
Epoch 500, training loss: 0.09745413810014725 = 0.02790798805654049 + 0.01 * 6.954615116119385
Epoch 500, val loss: 0.9789394736289978
Epoch 510, training loss: 0.09511514753103256 = 0.025720572099089622 + 0.01 * 6.939457416534424
Epoch 510, val loss: 0.9900507926940918
Epoch 520, training loss: 0.09304555505514145 = 0.023769577965140343 + 0.01 * 6.927597999572754
Epoch 520, val loss: 1.0008010864257812
Epoch 530, training loss: 0.09126588702201843 = 0.022024240344762802 + 0.01 * 6.924164295196533
Epoch 530, val loss: 1.0113152265548706
Epoch 540, training loss: 0.08978062868118286 = 0.020459655672311783 + 0.01 * 6.9320969581604
Epoch 540, val loss: 1.021477222442627
Epoch 550, training loss: 0.08811917901039124 = 0.019053969532251358 + 0.01 * 6.906520366668701
Epoch 550, val loss: 1.0313595533370972
Epoch 560, training loss: 0.08697617053985596 = 0.017786968499422073 + 0.01 * 6.918920040130615
Epoch 560, val loss: 1.0409528017044067
Epoch 570, training loss: 0.08583016693592072 = 0.016642050817608833 + 0.01 * 6.918811798095703
Epoch 570, val loss: 1.0502724647521973
Epoch 580, training loss: 0.08447839319705963 = 0.015604658983647823 + 0.01 * 6.887373447418213
Epoch 580, val loss: 1.0592626333236694
Epoch 590, training loss: 0.08356452733278275 = 0.014661731198430061 + 0.01 * 6.890280246734619
Epoch 590, val loss: 1.0680333375930786
Epoch 600, training loss: 0.08267077803611755 = 0.013802898116409779 + 0.01 * 6.8867878913879395
Epoch 600, val loss: 1.0765129327774048
Epoch 610, training loss: 0.08189567923545837 = 0.01301854569464922 + 0.01 * 6.887713432312012
Epoch 610, val loss: 1.0847432613372803
Epoch 620, training loss: 0.08105325698852539 = 0.012300634756684303 + 0.01 * 6.875262260437012
Epoch 620, val loss: 1.0927066802978516
Epoch 630, training loss: 0.08053140342235565 = 0.011642464436590672 + 0.01 * 6.888894081115723
Epoch 630, val loss: 1.1004233360290527
Epoch 640, training loss: 0.07971736043691635 = 0.011038239113986492 + 0.01 * 6.867912769317627
Epoch 640, val loss: 1.1078464984893799
Epoch 650, training loss: 0.07907632738351822 = 0.010481742210686207 + 0.01 * 6.8594584465026855
Epoch 650, val loss: 1.1151344776153564
Epoch 660, training loss: 0.0784076601266861 = 0.009968101046979427 + 0.01 * 6.843955993652344
Epoch 660, val loss: 1.122070550918579
Epoch 670, training loss: 0.0780203640460968 = 0.009493431076407433 + 0.01 * 6.8526930809021
Epoch 670, val loss: 1.1289100646972656
Epoch 680, training loss: 0.07744237035512924 = 0.009053281508386135 + 0.01 * 6.838908672332764
Epoch 680, val loss: 1.1354917287826538
Epoch 690, training loss: 0.07691629230976105 = 0.008645548485219479 + 0.01 * 6.8270745277404785
Epoch 690, val loss: 1.1418440341949463
Epoch 700, training loss: 0.07665442675352097 = 0.008266584947705269 + 0.01 * 6.838784217834473
Epoch 700, val loss: 1.1480118036270142
Epoch 710, training loss: 0.07621388137340546 = 0.007914028130471706 + 0.01 * 6.829985618591309
Epoch 710, val loss: 1.1540696620941162
Epoch 720, training loss: 0.07569551467895508 = 0.007584822364151478 + 0.01 * 6.811069011688232
Epoch 720, val loss: 1.1598957777023315
Epoch 730, training loss: 0.07544887810945511 = 0.007277265656739473 + 0.01 * 6.8171610832214355
Epoch 730, val loss: 1.1655299663543701
Epoch 740, training loss: 0.07507894933223724 = 0.006989776622503996 + 0.01 * 6.80891752243042
Epoch 740, val loss: 1.171063780784607
Epoch 750, training loss: 0.07491988688707352 = 0.006720399484038353 + 0.01 * 6.819948673248291
Epoch 750, val loss: 1.1763794422149658
Epoch 760, training loss: 0.07467351853847504 = 0.006467951461672783 + 0.01 * 6.820556640625
Epoch 760, val loss: 1.1815619468688965
Epoch 770, training loss: 0.07423440366983414 = 0.006230673752725124 + 0.01 * 6.800373077392578
Epoch 770, val loss: 1.1866023540496826
Epoch 780, training loss: 0.07431254535913467 = 0.006007025483995676 + 0.01 * 6.830552577972412
Epoch 780, val loss: 1.1914697885513306
Epoch 790, training loss: 0.07370547950267792 = 0.005796817131340504 + 0.01 * 6.790866374969482
Epoch 790, val loss: 1.1962944269180298
Epoch 800, training loss: 0.0736178532242775 = 0.005598442163318396 + 0.01 * 6.801941394805908
Epoch 800, val loss: 1.2008609771728516
Epoch 810, training loss: 0.07317885756492615 = 0.005411324091255665 + 0.01 * 6.7767534255981445
Epoch 810, val loss: 1.2054364681243896
Epoch 820, training loss: 0.07319983839988708 = 0.005234334617853165 + 0.01 * 6.796550273895264
Epoch 820, val loss: 1.2097705602645874
Epoch 830, training loss: 0.07287448644638062 = 0.005067033227533102 + 0.01 * 6.780745029449463
Epoch 830, val loss: 1.2140649557113647
Epoch 840, training loss: 0.07296910136938095 = 0.004908564500510693 + 0.01 * 6.80605411529541
Epoch 840, val loss: 1.2181626558303833
Epoch 850, training loss: 0.07241310179233551 = 0.004758520983159542 + 0.01 * 6.765458106994629
Epoch 850, val loss: 1.2222545146942139
Epoch 860, training loss: 0.07221267372369766 = 0.00461609335616231 + 0.01 * 6.759657859802246
Epoch 860, val loss: 1.2261543273925781
Epoch 870, training loss: 0.07227188348770142 = 0.004480850882828236 + 0.01 * 6.779102802276611
Epoch 870, val loss: 1.2299554347991943
Epoch 880, training loss: 0.07192222774028778 = 0.004352284595370293 + 0.01 * 6.756994247436523
Epoch 880, val loss: 1.2337507009506226
Epoch 890, training loss: 0.07179832458496094 = 0.004229936748743057 + 0.01 * 6.756838798522949
Epoch 890, val loss: 1.2372957468032837
Epoch 900, training loss: 0.0719505026936531 = 0.004113485105335712 + 0.01 * 6.783701419830322
Epoch 900, val loss: 1.2408311367034912
Epoch 910, training loss: 0.07154348492622375 = 0.00400264048948884 + 0.01 * 6.754085063934326
Epoch 910, val loss: 1.2443245649337769
Epoch 920, training loss: 0.07150167971849442 = 0.003896741895005107 + 0.01 * 6.760493755340576
Epoch 920, val loss: 1.2475762367248535
Epoch 930, training loss: 0.07124318927526474 = 0.0037959555629640818 + 0.01 * 6.744723320007324
Epoch 930, val loss: 1.2509006261825562
Epoch 940, training loss: 0.0711228996515274 = 0.003699557390064001 + 0.01 * 6.742334365844727
Epoch 940, val loss: 1.2539883852005005
Epoch 950, training loss: 0.07118724286556244 = 0.003607573453336954 + 0.01 * 6.757967472076416
Epoch 950, val loss: 1.257091760635376
Epoch 960, training loss: 0.07080446928739548 = 0.003519617021083832 + 0.01 * 6.728485107421875
Epoch 960, val loss: 1.2601261138916016
Epoch 970, training loss: 0.0709448903799057 = 0.0034352100919932127 + 0.01 * 6.750967979431152
Epoch 970, val loss: 1.2630093097686768
Epoch 980, training loss: 0.0706622377038002 = 0.0033546271733939648 + 0.01 * 6.730761528015137
Epoch 980, val loss: 1.265874981880188
Epoch 990, training loss: 0.07055651396512985 = 0.0032772619742900133 + 0.01 * 6.7279253005981445
Epoch 990, val loss: 1.26859712600708
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8429098576700054
The final CL Acc:0.81728, 0.01222, The final GNN Acc:0.83711, 0.00522
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10530])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0380544662475586 = 1.9520859718322754 + 0.01 * 8.596860885620117
Epoch 0, val loss: 1.9495307207107544
Epoch 10, training loss: 2.028167247772217 = 1.9421991109848022 + 0.01 * 8.59681224822998
Epoch 10, val loss: 1.9399640560150146
Epoch 20, training loss: 2.016019105911255 = 1.9300527572631836 + 0.01 * 8.596641540527344
Epoch 20, val loss: 1.9278770685195923
Epoch 30, training loss: 1.9992477893829346 = 1.9132869243621826 + 0.01 * 8.59608268737793
Epoch 30, val loss: 1.91085684299469
Epoch 40, training loss: 1.974763035774231 = 1.888831615447998 + 0.01 * 8.593141555786133
Epoch 40, val loss: 1.885960340499878
Epoch 50, training loss: 1.9403891563415527 = 1.8546637296676636 + 0.01 * 8.57254409790039
Epoch 50, val loss: 1.8524210453033447
Epoch 60, training loss: 1.9003734588623047 = 1.815685510635376 + 0.01 * 8.468791961669922
Epoch 60, val loss: 1.8178935050964355
Epoch 70, training loss: 1.867597222328186 = 1.7840237617492676 + 0.01 * 8.357341766357422
Epoch 70, val loss: 1.7931264638900757
Epoch 80, training loss: 1.8318908214569092 = 1.749404788017273 + 0.01 * 8.248598098754883
Epoch 80, val loss: 1.762657642364502
Epoch 90, training loss: 1.7802494764328003 = 1.6990760564804077 + 0.01 * 8.117347717285156
Epoch 90, val loss: 1.7189093828201294
Epoch 100, training loss: 1.7080836296081543 = 1.6279019117355347 + 0.01 * 8.018174171447754
Epoch 100, val loss: 1.6591613292694092
Epoch 110, training loss: 1.6174901723861694 = 1.5386035442352295 + 0.01 * 7.88865852355957
Epoch 110, val loss: 1.5854158401489258
Epoch 120, training loss: 1.5209637880325317 = 1.4440593719482422 + 0.01 * 7.690444469451904
Epoch 120, val loss: 1.507819652557373
Epoch 130, training loss: 1.4292247295379639 = 1.353200912475586 + 0.01 * 7.602387428283691
Epoch 130, val loss: 1.4363112449645996
Epoch 140, training loss: 1.3440483808517456 = 1.2686362266540527 + 0.01 * 7.541215896606445
Epoch 140, val loss: 1.373721957206726
Epoch 150, training loss: 1.2640154361724854 = 1.1891868114471436 + 0.01 * 7.482858657836914
Epoch 150, val loss: 1.3178002834320068
Epoch 160, training loss: 1.1856192350387573 = 1.1113739013671875 + 0.01 * 7.424532413482666
Epoch 160, val loss: 1.263048768043518
Epoch 170, training loss: 1.1046760082244873 = 1.0307469367980957 + 0.01 * 7.392909526824951
Epoch 170, val loss: 1.2046210765838623
Epoch 180, training loss: 1.0199253559112549 = 0.9462026953697205 + 0.01 * 7.372265815734863
Epoch 180, val loss: 1.1414622068405151
Epoch 190, training loss: 0.935305655002594 = 0.8617856502532959 + 0.01 * 7.352002143859863
Epoch 190, val loss: 1.0778100490570068
Epoch 200, training loss: 0.8572627305984497 = 0.783897876739502 + 0.01 * 7.33648681640625
Epoch 200, val loss: 1.0201295614242554
Epoch 210, training loss: 0.7895943522453308 = 0.7163623571395874 + 0.01 * 7.323200225830078
Epoch 210, val loss: 0.9724382162094116
Epoch 220, training loss: 0.7313821315765381 = 0.6582529544830322 + 0.01 * 7.312921047210693
Epoch 220, val loss: 0.9343035221099854
Epoch 230, training loss: 0.6790790557861328 = 0.6060159802436829 + 0.01 * 7.306310176849365
Epoch 230, val loss: 0.9028522968292236
Epoch 240, training loss: 0.6292153000831604 = 0.5561993718147278 + 0.01 * 7.3015923500061035
Epoch 240, val loss: 0.8753633499145508
Epoch 250, training loss: 0.5796625018119812 = 0.5066940784454346 + 0.01 * 7.296841621398926
Epoch 250, val loss: 0.8508902788162231
Epoch 260, training loss: 0.5301989912986755 = 0.45725515484809875 + 0.01 * 7.294382095336914
Epoch 260, val loss: 0.829903244972229
Epoch 270, training loss: 0.4820311367511749 = 0.4091101288795471 + 0.01 * 7.292100429534912
Epoch 270, val loss: 0.8138192296028137
Epoch 280, training loss: 0.436764657497406 = 0.3638807535171509 + 0.01 * 7.288390159606934
Epoch 280, val loss: 0.8040362596511841
Epoch 290, training loss: 0.39521753787994385 = 0.32237008213996887 + 0.01 * 7.284745693206787
Epoch 290, val loss: 0.8004118204116821
Epoch 300, training loss: 0.3572990596294403 = 0.2844930589199066 + 0.01 * 7.280600547790527
Epoch 300, val loss: 0.8023463487625122
Epoch 310, training loss: 0.322919636964798 = 0.2501024007797241 + 0.01 * 7.281724452972412
Epoch 310, val loss: 0.8088096976280212
Epoch 320, training loss: 0.29196953773498535 = 0.21926255524158478 + 0.01 * 7.270699501037598
Epoch 320, val loss: 0.818501353263855
Epoch 330, training loss: 0.26471126079559326 = 0.19200153648853302 + 0.01 * 7.27097225189209
Epoch 330, val loss: 0.8305298686027527
Epoch 340, training loss: 0.24073386192321777 = 0.16812333464622498 + 0.01 * 7.261053085327148
Epoch 340, val loss: 0.8442572355270386
Epoch 350, training loss: 0.21980558335781097 = 0.14727886021137238 + 0.01 * 7.2526726722717285
Epoch 350, val loss: 0.8591299057006836
Epoch 360, training loss: 0.2017078697681427 = 0.12913158535957336 + 0.01 * 7.257627487182617
Epoch 360, val loss: 0.8747704029083252
Epoch 370, training loss: 0.1857873499393463 = 0.1133844181895256 + 0.01 * 7.240292549133301
Epoch 370, val loss: 0.8910695910453796
Epoch 380, training loss: 0.1720597743988037 = 0.0997532308101654 + 0.01 * 7.230653762817383
Epoch 380, val loss: 0.9080423712730408
Epoch 390, training loss: 0.1602221429347992 = 0.08798360079526901 + 0.01 * 7.223853588104248
Epoch 390, val loss: 0.9255107045173645
Epoch 400, training loss: 0.15004616975784302 = 0.07783220708370209 + 0.01 * 7.221395492553711
Epoch 400, val loss: 0.9433022737503052
Epoch 410, training loss: 0.14139091968536377 = 0.06905330717563629 + 0.01 * 7.233760356903076
Epoch 410, val loss: 0.9612009525299072
Epoch 420, training loss: 0.13327129185199738 = 0.06145261600613594 + 0.01 * 7.1818671226501465
Epoch 420, val loss: 0.9789823293685913
Epoch 430, training loss: 0.12658411264419556 = 0.05485697090625763 + 0.01 * 7.172715187072754
Epoch 430, val loss: 0.9964287877082825
Epoch 440, training loss: 0.12081107497215271 = 0.04912998899817467 + 0.01 * 7.1681084632873535
Epoch 440, val loss: 1.013322114944458
Epoch 450, training loss: 0.11568956077098846 = 0.044150274246931076 + 0.01 * 7.153928756713867
Epoch 450, val loss: 1.0296589136123657
Epoch 460, training loss: 0.1114836260676384 = 0.03982120752334595 + 0.01 * 7.1662421226501465
Epoch 460, val loss: 1.0452524423599243
Epoch 470, training loss: 0.10740450024604797 = 0.036051154136657715 + 0.01 * 7.135334491729736
Epoch 470, val loss: 1.0602890253067017
Epoch 480, training loss: 0.10435660183429718 = 0.032754361629486084 + 0.01 * 7.160223960876465
Epoch 480, val loss: 1.0747054815292358
Epoch 490, training loss: 0.1013171449303627 = 0.029869453981518745 + 0.01 * 7.144769191741943
Epoch 490, val loss: 1.0884134769439697
Epoch 500, training loss: 0.09824709594249725 = 0.02733635902404785 + 0.01 * 7.091073989868164
Epoch 500, val loss: 1.1016361713409424
Epoch 510, training loss: 0.09605184197425842 = 0.025103725492954254 + 0.01 * 7.09481143951416
Epoch 510, val loss: 1.114251971244812
Epoch 520, training loss: 0.0938379168510437 = 0.023132629692554474 + 0.01 * 7.070528507232666
Epoch 520, val loss: 1.126304268836975
Epoch 530, training loss: 0.09220641851425171 = 0.02138303965330124 + 0.01 * 7.082338333129883
Epoch 530, val loss: 1.1378549337387085
Epoch 540, training loss: 0.09045889228582382 = 0.019826658070087433 + 0.01 * 7.063223361968994
Epoch 540, val loss: 1.1489330530166626
Epoch 550, training loss: 0.0889030173420906 = 0.018437696620821953 + 0.01 * 7.046531677246094
Epoch 550, val loss: 1.1595473289489746
Epoch 560, training loss: 0.08796519041061401 = 0.017191443592309952 + 0.01 * 7.0773749351501465
Epoch 560, val loss: 1.1697468757629395
Epoch 570, training loss: 0.08692802488803864 = 0.016072187572717667 + 0.01 * 7.085583209991455
Epoch 570, val loss: 1.179560899734497
Epoch 580, training loss: 0.08533424139022827 = 0.015063867904245853 + 0.01 * 7.027037620544434
Epoch 580, val loss: 1.188900113105774
Epoch 590, training loss: 0.08426261693239212 = 0.014151736162602901 + 0.01 * 7.0110883712768555
Epoch 590, val loss: 1.1979519128799438
Epoch 600, training loss: 0.08357793837785721 = 0.01332321111112833 + 0.01 * 7.025472640991211
Epoch 600, val loss: 1.2066318988800049
Epoch 610, training loss: 0.08305378258228302 = 0.012570079416036606 + 0.01 * 7.048370838165283
Epoch 610, val loss: 1.2150307893753052
Epoch 620, training loss: 0.08166813850402832 = 0.011882794089615345 + 0.01 * 6.97853422164917
Epoch 620, val loss: 1.2230595350265503
Epoch 630, training loss: 0.0812021940946579 = 0.011254141107201576 + 0.01 * 6.994805335998535
Epoch 630, val loss: 1.230813980102539
Epoch 640, training loss: 0.08068425953388214 = 0.010677631944417953 + 0.01 * 7.000662803649902
Epoch 640, val loss: 1.2383426427841187
Epoch 650, training loss: 0.07979631423950195 = 0.010147086344659328 + 0.01 * 6.964922904968262
Epoch 650, val loss: 1.245554804801941
Epoch 660, training loss: 0.0794920027256012 = 0.009658162482082844 + 0.01 * 6.983383655548096
Epoch 660, val loss: 1.2525670528411865
Epoch 670, training loss: 0.07866361737251282 = 0.009207316674292088 + 0.01 * 6.9456305503845215
Epoch 670, val loss: 1.2592867612838745
Epoch 680, training loss: 0.07807281613349915 = 0.008790016174316406 + 0.01 * 6.928280353546143
Epoch 680, val loss: 1.265851616859436
Epoch 690, training loss: 0.07766056060791016 = 0.00840272381901741 + 0.01 * 6.925783157348633
Epoch 690, val loss: 1.2721326351165771
Epoch 700, training loss: 0.07778213173151016 = 0.008043253794312477 + 0.01 * 6.973887920379639
Epoch 700, val loss: 1.278221607208252
Epoch 710, training loss: 0.07707848399877548 = 0.007709336467087269 + 0.01 * 6.936914443969727
Epoch 710, val loss: 1.2841284275054932
Epoch 720, training loss: 0.0765819326043129 = 0.0073975794948637486 + 0.01 * 6.918436050415039
Epoch 720, val loss: 1.28987717628479
Epoch 730, training loss: 0.07630138099193573 = 0.0071060871705412865 + 0.01 * 6.919529438018799
Epoch 730, val loss: 1.2954143285751343
Epoch 740, training loss: 0.0757923424243927 = 0.0068335761316120625 + 0.01 * 6.895876884460449
Epoch 740, val loss: 1.3007934093475342
Epoch 750, training loss: 0.07619023323059082 = 0.006578186526894569 + 0.01 * 6.961205005645752
Epoch 750, val loss: 1.3060483932495117
Epoch 760, training loss: 0.07570557296276093 = 0.006338636856526136 + 0.01 * 6.936694145202637
Epoch 760, val loss: 1.3110837936401367
Epoch 770, training loss: 0.0748872384428978 = 0.006114020012319088 + 0.01 * 6.877321720123291
Epoch 770, val loss: 1.3160134553909302
Epoch 780, training loss: 0.07512257993221283 = 0.005902547854930162 + 0.01 * 6.922003269195557
Epoch 780, val loss: 1.3208394050598145
Epoch 790, training loss: 0.07492925971746445 = 0.0057031214237213135 + 0.01 * 6.922613620758057
Epoch 790, val loss: 1.3254293203353882
Epoch 800, training loss: 0.07440906018018723 = 0.005515477154403925 + 0.01 * 6.8893585205078125
Epoch 800, val loss: 1.3299401998519897
Epoch 810, training loss: 0.0740683376789093 = 0.005338071845471859 + 0.01 * 6.873026371002197
Epoch 810, val loss: 1.3343249559402466
Epoch 820, training loss: 0.0737270936369896 = 0.005170653574168682 + 0.01 * 6.855644702911377
Epoch 820, val loss: 1.338579773902893
Epoch 830, training loss: 0.07401895523071289 = 0.005012234207242727 + 0.01 * 6.90067195892334
Epoch 830, val loss: 1.3427187204360962
Epoch 840, training loss: 0.0735473707318306 = 0.004862007685005665 + 0.01 * 6.868535995483398
Epoch 840, val loss: 1.3467791080474854
Epoch 850, training loss: 0.07335452735424042 = 0.004719561897218227 + 0.01 * 6.863496780395508
Epoch 850, val loss: 1.3507342338562012
Epoch 860, training loss: 0.0730641782283783 = 0.004584169015288353 + 0.01 * 6.848001003265381
Epoch 860, val loss: 1.3545674085617065
Epoch 870, training loss: 0.0729314386844635 = 0.004455683287233114 + 0.01 * 6.8475751876831055
Epoch 870, val loss: 1.3582861423492432
Epoch 880, training loss: 0.07295375317335129 = 0.004333327058702707 + 0.01 * 6.8620429039001465
Epoch 880, val loss: 1.3619544506072998
Epoch 890, training loss: 0.07291669398546219 = 0.0042168148793280125 + 0.01 * 6.869988441467285
Epoch 890, val loss: 1.3654998540878296
Epoch 900, training loss: 0.07253154367208481 = 0.00410604290664196 + 0.01 * 6.842549800872803
Epoch 900, val loss: 1.3689780235290527
Epoch 910, training loss: 0.07207793742418289 = 0.0040003410540521145 + 0.01 * 6.807759761810303
Epoch 910, val loss: 1.3723255395889282
Epoch 920, training loss: 0.07197709381580353 = 0.003899453906342387 + 0.01 * 6.807764053344727
Epoch 920, val loss: 1.3756109476089478
Epoch 930, training loss: 0.07237659394741058 = 0.003803066210821271 + 0.01 * 6.8573527336120605
Epoch 930, val loss: 1.378798246383667
Epoch 940, training loss: 0.07226883620023727 = 0.0037111795973032713 + 0.01 * 6.8557658195495605
Epoch 940, val loss: 1.3819458484649658
Epoch 950, training loss: 0.07170689105987549 = 0.003623205702751875 + 0.01 * 6.808368682861328
Epoch 950, val loss: 1.3849523067474365
Epoch 960, training loss: 0.07182784378528595 = 0.0035390444099903107 + 0.01 * 6.828880786895752
Epoch 960, val loss: 1.387948989868164
Epoch 970, training loss: 0.07182354480028152 = 0.003458420978859067 + 0.01 * 6.836513042449951
Epoch 970, val loss: 1.3908594846725464
Epoch 980, training loss: 0.07134176045656204 = 0.0033811205066740513 + 0.01 * 6.796064376831055
Epoch 980, val loss: 1.3936553001403809
Epoch 990, training loss: 0.07145306468009949 = 0.003306999336928129 + 0.01 * 6.814606666564941
Epoch 990, val loss: 1.396480917930603
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 2.0292372703552246 = 1.94326913356781 + 0.01 * 8.596820831298828
Epoch 0, val loss: 1.9400553703308105
Epoch 10, training loss: 2.0185225009918213 = 1.932555079460144 + 0.01 * 8.596731185913086
Epoch 10, val loss: 1.9289575815200806
Epoch 20, training loss: 2.004721164703369 = 1.9187570810317993 + 0.01 * 8.596414566040039
Epoch 20, val loss: 1.914632797241211
Epoch 30, training loss: 1.9851285219192505 = 1.899175763130188 + 0.01 * 8.595272064208984
Epoch 30, val loss: 1.894267201423645
Epoch 40, training loss: 1.9573827981948853 = 1.8714983463287354 + 0.01 * 8.58845043182373
Epoch 40, val loss: 1.86604905128479
Epoch 50, training loss: 1.9217033386230469 = 1.8361865282058716 + 0.01 * 8.551678657531738
Epoch 50, val loss: 1.8320976495742798
Epoch 60, training loss: 1.8843908309936523 = 1.8002537488937378 + 0.01 * 8.413714408874512
Epoch 60, val loss: 1.8015735149383545
Epoch 70, training loss: 1.8485838174819946 = 1.765788197517395 + 0.01 * 8.279563903808594
Epoch 70, val loss: 1.7746068239212036
Epoch 80, training loss: 1.8000264167785645 = 1.7206168174743652 + 0.01 * 7.9409589767456055
Epoch 80, val loss: 1.7367608547210693
Epoch 90, training loss: 1.7343180179595947 = 1.6574465036392212 + 0.01 * 7.687150001525879
Epoch 90, val loss: 1.6814991235733032
Epoch 100, training loss: 1.6483147144317627 = 1.5721452236175537 + 0.01 * 7.616943359375
Epoch 100, val loss: 1.6080241203308105
Epoch 110, training loss: 1.5451991558074951 = 1.469420313835144 + 0.01 * 7.577878952026367
Epoch 110, val loss: 1.5227473974227905
Epoch 120, training loss: 1.4330558776855469 = 1.3577977418899536 + 0.01 * 7.525818824768066
Epoch 120, val loss: 1.4324595928192139
Epoch 130, training loss: 1.3195185661315918 = 1.244918942451477 + 0.01 * 7.459966659545898
Epoch 130, val loss: 1.3432044982910156
Epoch 140, training loss: 1.208577036857605 = 1.134648084640503 + 0.01 * 7.392898082733154
Epoch 140, val loss: 1.2576889991760254
Epoch 150, training loss: 1.1052221059799194 = 1.0316216945648193 + 0.01 * 7.3600382804870605
Epoch 150, val loss: 1.179936170578003
Epoch 160, training loss: 1.0124355554580688 = 0.9391319751739502 + 0.01 * 7.330363750457764
Epoch 160, val loss: 1.1118531227111816
Epoch 170, training loss: 0.931641697883606 = 0.8585342168807983 + 0.01 * 7.310750961303711
Epoch 170, val loss: 1.0538474321365356
Epoch 180, training loss: 0.8627580404281616 = 0.7899066805839539 + 0.01 * 7.285137176513672
Epoch 180, val loss: 1.0067676305770874
Epoch 190, training loss: 0.8046445846557617 = 0.731989860534668 + 0.01 * 7.265473365783691
Epoch 190, val loss: 0.9704115390777588
Epoch 200, training loss: 0.7546220421791077 = 0.6822396516799927 + 0.01 * 7.238239288330078
Epoch 200, val loss: 0.9429928064346313
Epoch 210, training loss: 0.709723949432373 = 0.6376092433929443 + 0.01 * 7.211470127105713
Epoch 210, val loss: 0.9220066666603088
Epoch 220, training loss: 0.6675326824188232 = 0.5955895185470581 + 0.01 * 7.194315433502197
Epoch 220, val loss: 0.9048258066177368
Epoch 230, training loss: 0.6264708638191223 = 0.5547046661376953 + 0.01 * 7.176618576049805
Epoch 230, val loss: 0.889985203742981
Epoch 240, training loss: 0.586297333240509 = 0.5145456790924072 + 0.01 * 7.175167560577393
Epoch 240, val loss: 0.877295732498169
Epoch 250, training loss: 0.5471057295799255 = 0.4755697548389435 + 0.01 * 7.153600215911865
Epoch 250, val loss: 0.8674948811531067
Epoch 260, training loss: 0.5095823407173157 = 0.4381691515445709 + 0.01 * 7.14132022857666
Epoch 260, val loss: 0.8612410426139832
Epoch 270, training loss: 0.4736996591091156 = 0.4025016129016876 + 0.01 * 7.119805335998535
Epoch 270, val loss: 0.8590104579925537
Epoch 280, training loss: 0.4398075342178345 = 0.3686671853065491 + 0.01 * 7.114034652709961
Epoch 280, val loss: 0.860997200012207
Epoch 290, training loss: 0.4078919291496277 = 0.33686164021492004 + 0.01 * 7.103027820587158
Epoch 290, val loss: 0.8671324253082275
Epoch 300, training loss: 0.3782021999359131 = 0.30722206830978394 + 0.01 * 7.098012447357178
Epoch 300, val loss: 0.8770555853843689
Epoch 310, training loss: 0.3505037724971771 = 0.2796355187892914 + 0.01 * 7.086825847625732
Epoch 310, val loss: 0.890356183052063
Epoch 320, training loss: 0.3246030807495117 = 0.25380510091781616 + 0.01 * 7.07979679107666
Epoch 320, val loss: 0.9063023924827576
Epoch 330, training loss: 0.3002047836780548 = 0.22943997383117676 + 0.01 * 7.076481819152832
Epoch 330, val loss: 0.9245023727416992
Epoch 340, training loss: 0.2772065997123718 = 0.2064570188522339 + 0.01 * 7.074957847595215
Epoch 340, val loss: 0.9447261095046997
Epoch 350, training loss: 0.2556385397911072 = 0.1849188506603241 + 0.01 * 7.071969985961914
Epoch 350, val loss: 0.9671162962913513
Epoch 360, training loss: 0.2356613278388977 = 0.1649426966905594 + 0.01 * 7.071863174438477
Epoch 360, val loss: 0.9913480281829834
Epoch 370, training loss: 0.21732406318187714 = 0.14665618538856506 + 0.01 * 7.0667877197265625
Epoch 370, val loss: 1.0172349214553833
Epoch 380, training loss: 0.20071011781692505 = 0.13009822368621826 + 0.01 * 7.061188697814941
Epoch 380, val loss: 1.0446088314056396
Epoch 390, training loss: 0.18578365445137024 = 0.11522100120782852 + 0.01 * 7.056265354156494
Epoch 390, val loss: 1.0732771158218384
Epoch 400, training loss: 0.17264285683631897 = 0.10191986709833145 + 0.01 * 7.072298526763916
Epoch 400, val loss: 1.1030019521713257
Epoch 410, training loss: 0.1605994701385498 = 0.09011352807283401 + 0.01 * 7.0485944747924805
Epoch 410, val loss: 1.1333953142166138
Epoch 420, training loss: 0.15012522041797638 = 0.07968302071094513 + 0.01 * 7.044219970703125
Epoch 420, val loss: 1.16423761844635
Epoch 430, training loss: 0.1409199833869934 = 0.07052523642778397 + 0.01 * 7.039475440979004
Epoch 430, val loss: 1.1952086687088013
Epoch 440, training loss: 0.13290657103061676 = 0.06253287196159363 + 0.01 * 7.037370204925537
Epoch 440, val loss: 1.2260308265686035
Epoch 450, training loss: 0.12588578462600708 = 0.05558893829584122 + 0.01 * 7.0296854972839355
Epoch 450, val loss: 1.2563512325286865
Epoch 460, training loss: 0.11975622177124023 = 0.049568790942430496 + 0.01 * 7.018743515014648
Epoch 460, val loss: 1.2860838174819946
Epoch 470, training loss: 0.11459134519100189 = 0.04435645043849945 + 0.01 * 7.023489952087402
Epoch 470, val loss: 1.3148863315582275
Epoch 480, training loss: 0.11001761257648468 = 0.03983987122774124 + 0.01 * 7.0177741050720215
Epoch 480, val loss: 1.3426594734191895
Epoch 490, training loss: 0.10603991150856018 = 0.03592173010110855 + 0.01 * 7.011817932128906
Epoch 490, val loss: 1.3693774938583374
Epoch 500, training loss: 0.10258067399263382 = 0.03251321613788605 + 0.01 * 7.006746292114258
Epoch 500, val loss: 1.3949323892593384
Epoch 510, training loss: 0.09946693480014801 = 0.029536446556448936 + 0.01 * 6.993048667907715
Epoch 510, val loss: 1.419467806816101
Epoch 520, training loss: 0.0967203825712204 = 0.02693018689751625 + 0.01 * 6.979020118713379
Epoch 520, val loss: 1.442870020866394
Epoch 530, training loss: 0.09442078322172165 = 0.024640245363116264 + 0.01 * 6.978054046630859
Epoch 530, val loss: 1.4652479887008667
Epoch 540, training loss: 0.09230634570121765 = 0.02262096479535103 + 0.01 * 6.9685378074646
Epoch 540, val loss: 1.4866282939910889
Epoch 550, training loss: 0.09069135785102844 = 0.02083449624478817 + 0.01 * 6.985686302185059
Epoch 550, val loss: 1.506932258605957
Epoch 560, training loss: 0.08886419236660004 = 0.01925094798207283 + 0.01 * 6.961325168609619
Epoch 560, val loss: 1.5263350009918213
Epoch 570, training loss: 0.08743149787187576 = 0.017839854583144188 + 0.01 * 6.959164619445801
Epoch 570, val loss: 1.5449469089508057
Epoch 580, training loss: 0.08602464199066162 = 0.01657877489924431 + 0.01 * 6.944586753845215
Epoch 580, val loss: 1.5627299547195435
Epoch 590, training loss: 0.0849076583981514 = 0.01544842030853033 + 0.01 * 6.945923805236816
Epoch 590, val loss: 1.5796258449554443
Epoch 600, training loss: 0.08403582125902176 = 0.01443280279636383 + 0.01 * 6.960301876068115
Epoch 600, val loss: 1.5957728624343872
Epoch 610, training loss: 0.08294688910245895 = 0.013518483377993107 + 0.01 * 6.942840576171875
Epoch 610, val loss: 1.611322045326233
Epoch 620, training loss: 0.08192035555839539 = 0.01269157137721777 + 0.01 * 6.922878742218018
Epoch 620, val loss: 1.6261420249938965
Epoch 630, training loss: 0.08106480538845062 = 0.01194109208881855 + 0.01 * 6.912371635437012
Epoch 630, val loss: 1.6403366327285767
Epoch 640, training loss: 0.08055628091096878 = 0.011258318088948727 + 0.01 * 6.92979621887207
Epoch 640, val loss: 1.653934121131897
Epoch 650, training loss: 0.07974092662334442 = 0.010636369697749615 + 0.01 * 6.91045618057251
Epoch 650, val loss: 1.6670053005218506
Epoch 660, training loss: 0.07917412370443344 = 0.0100681958720088 + 0.01 * 6.910593032836914
Epoch 660, val loss: 1.6794722080230713
Epoch 670, training loss: 0.07856588810682297 = 0.009548403322696686 + 0.01 * 6.9017486572265625
Epoch 670, val loss: 1.691437005996704
Epoch 680, training loss: 0.07797583937644958 = 0.009071080945432186 + 0.01 * 6.890476226806641
Epoch 680, val loss: 1.7031129598617554
Epoch 690, training loss: 0.07781905680894852 = 0.008631361648440361 + 0.01 * 6.918769836425781
Epoch 690, val loss: 1.7142090797424316
Epoch 700, training loss: 0.07721737772226334 = 0.008226192556321621 + 0.01 * 6.899118423461914
Epoch 700, val loss: 1.7249795198440552
Epoch 710, training loss: 0.07667949795722961 = 0.007851701229810715 + 0.01 * 6.882779121398926
Epoch 710, val loss: 1.7352863550186157
Epoch 720, training loss: 0.07619346678256989 = 0.007504874840378761 + 0.01 * 6.86885929107666
Epoch 720, val loss: 1.7452563047409058
Epoch 730, training loss: 0.07601486891508102 = 0.007182781584560871 + 0.01 * 6.883208274841309
Epoch 730, val loss: 1.7548519372940063
Epoch 740, training loss: 0.0756920799612999 = 0.0068838088773190975 + 0.01 * 6.8808274269104
Epoch 740, val loss: 1.7640914916992188
Epoch 750, training loss: 0.0751819908618927 = 0.006605568807572126 + 0.01 * 6.857642650604248
Epoch 750, val loss: 1.7731026411056519
Epoch 760, training loss: 0.07494551688432693 = 0.006346141919493675 + 0.01 * 6.85993766784668
Epoch 760, val loss: 1.781719446182251
Epoch 770, training loss: 0.07475684583187103 = 0.006104029715061188 + 0.01 * 6.86528205871582
Epoch 770, val loss: 1.7900400161743164
Epoch 780, training loss: 0.07435155659914017 = 0.005877424497157335 + 0.01 * 6.847413063049316
Epoch 780, val loss: 1.7981550693511963
Epoch 790, training loss: 0.07414921373128891 = 0.005665135104209185 + 0.01 * 6.848407745361328
Epoch 790, val loss: 1.8060016632080078
Epoch 800, training loss: 0.07390739023685455 = 0.0054655857384204865 + 0.01 * 6.844180583953857
Epoch 800, val loss: 1.813574194908142
Epoch 810, training loss: 0.07382271438837051 = 0.005278468132019043 + 0.01 * 6.854424476623535
Epoch 810, val loss: 1.8209266662597656
Epoch 820, training loss: 0.07386356592178345 = 0.00510249612852931 + 0.01 * 6.876107692718506
Epoch 820, val loss: 1.8279364109039307
Epoch 830, training loss: 0.07322102785110474 = 0.0049369605258107185 + 0.01 * 6.82840633392334
Epoch 830, val loss: 1.834907054901123
Epoch 840, training loss: 0.07301189750432968 = 0.004780714400112629 + 0.01 * 6.823118209838867
Epoch 840, val loss: 1.8415151834487915
Epoch 850, training loss: 0.07291756570339203 = 0.004633295349776745 + 0.01 * 6.828427791595459
Epoch 850, val loss: 1.8479621410369873
Epoch 860, training loss: 0.07262212783098221 = 0.004493878688663244 + 0.01 * 6.8128252029418945
Epoch 860, val loss: 1.8542711734771729
Epoch 870, training loss: 0.07267334312200546 = 0.004361839033663273 + 0.01 * 6.831151008605957
Epoch 870, val loss: 1.8602712154388428
Epoch 880, training loss: 0.07268696278333664 = 0.0042368764989078045 + 0.01 * 6.845008373260498
Epoch 880, val loss: 1.8662340641021729
Epoch 890, training loss: 0.07228059321641922 = 0.004118287470191717 + 0.01 * 6.816230773925781
Epoch 890, val loss: 1.8719857931137085
Epoch 900, training loss: 0.07195670902729034 = 0.004005888011306524 + 0.01 * 6.7950825691223145
Epoch 900, val loss: 1.8774420022964478
Epoch 910, training loss: 0.07206016033887863 = 0.003899232717230916 + 0.01 * 6.8160929679870605
Epoch 910, val loss: 1.8829002380371094
Epoch 920, training loss: 0.07174010574817657 = 0.0037974556908011436 + 0.01 * 6.794265270233154
Epoch 920, val loss: 1.8881149291992188
Epoch 930, training loss: 0.0717439129948616 = 0.00370061956346035 + 0.01 * 6.8043293952941895
Epoch 930, val loss: 1.8932645320892334
Epoch 940, training loss: 0.07161213457584381 = 0.0036085026804357767 + 0.01 * 6.800363540649414
Epoch 940, val loss: 1.8983098268508911
Epoch 950, training loss: 0.07155314087867737 = 0.003520529717206955 + 0.01 * 6.8032612800598145
Epoch 950, val loss: 1.903029203414917
Epoch 960, training loss: 0.0715196430683136 = 0.003436596365645528 + 0.01 * 6.808305263519287
Epoch 960, val loss: 1.9077337980270386
Epoch 970, training loss: 0.07119271904230118 = 0.003356418339535594 + 0.01 * 6.783629894256592
Epoch 970, val loss: 1.9124552011489868
Epoch 980, training loss: 0.07126446068286896 = 0.0032799053005874157 + 0.01 * 6.798455715179443
Epoch 980, val loss: 1.9166632890701294
Epoch 990, training loss: 0.07095359265804291 = 0.003206691239029169 + 0.01 * 6.7746901512146
Epoch 990, val loss: 1.9211918115615845
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 2.0417652130126953 = 1.9557965993881226 + 0.01 * 8.5968599319458
Epoch 0, val loss: 1.9587366580963135
Epoch 10, training loss: 2.0312893390655518 = 1.9453213214874268 + 0.01 * 8.59680461883545
Epoch 10, val loss: 1.9488414525985718
Epoch 20, training loss: 2.0184872150421143 = 1.932521104812622 + 0.01 * 8.596606254577637
Epoch 20, val loss: 1.936283826828003
Epoch 30, training loss: 2.0007638931274414 = 1.9148043394088745 + 0.01 * 8.595956802368164
Epoch 30, val loss: 1.9184802770614624
Epoch 40, training loss: 1.9749255180358887 = 1.8889989852905273 + 0.01 * 8.592655181884766
Epoch 40, val loss: 1.8927733898162842
Epoch 50, training loss: 1.938769817352295 = 1.8530476093292236 + 0.01 * 8.572221755981445
Epoch 50, val loss: 1.8584840297698975
Epoch 60, training loss: 1.8971362113952637 = 1.8122888803482056 + 0.01 * 8.484736442565918
Epoch 60, val loss: 1.8229804039001465
Epoch 70, training loss: 1.8610920906066895 = 1.777862548828125 + 0.01 * 8.322953224182129
Epoch 70, val loss: 1.794171690940857
Epoch 80, training loss: 1.819832444190979 = 1.7384333610534668 + 0.01 * 8.139904022216797
Epoch 80, val loss: 1.7563669681549072
Epoch 90, training loss: 1.762536883354187 = 1.68255615234375 + 0.01 * 7.998078346252441
Epoch 90, val loss: 1.7063238620758057
Epoch 100, training loss: 1.684929370880127 = 1.6061131954193115 + 0.01 * 7.88162088394165
Epoch 100, val loss: 1.642115831375122
Epoch 110, training loss: 1.5923361778259277 = 1.5158270597457886 + 0.01 * 7.650914669036865
Epoch 110, val loss: 1.567673683166504
Epoch 120, training loss: 1.501507043838501 = 1.4266326427459717 + 0.01 * 7.487442970275879
Epoch 120, val loss: 1.494695782661438
Epoch 130, training loss: 1.4182136058807373 = 1.3443775177001953 + 0.01 * 7.383604049682617
Epoch 130, val loss: 1.4282500743865967
Epoch 140, training loss: 1.3407260179519653 = 1.2673906087875366 + 0.01 * 7.333542823791504
Epoch 140, val loss: 1.3684974908828735
Epoch 150, training loss: 1.266662359237671 = 1.1935458183288574 + 0.01 * 7.311648845672607
Epoch 150, val loss: 1.3128736019134521
Epoch 160, training loss: 1.1925299167633057 = 1.119561791419983 + 0.01 * 7.296810626983643
Epoch 160, val loss: 1.2576348781585693
Epoch 170, training loss: 1.1144366264343262 = 1.0416183471679688 + 0.01 * 7.281829833984375
Epoch 170, val loss: 1.1985899209976196
Epoch 180, training loss: 1.0306743383407593 = 0.9580334424972534 + 0.01 * 7.26408576965332
Epoch 180, val loss: 1.133543848991394
Epoch 190, training loss: 0.9432715177536011 = 0.8708080649375916 + 0.01 * 7.246344566345215
Epoch 190, val loss: 1.0645225048065186
Epoch 200, training loss: 0.8566875457763672 = 0.7843325138092041 + 0.01 * 7.235502243041992
Epoch 200, val loss: 0.9959129095077515
Epoch 210, training loss: 0.7753737568855286 = 0.7030916213989258 + 0.01 * 7.228215217590332
Epoch 210, val loss: 0.9326035380363464
Epoch 220, training loss: 0.70186847448349 = 0.6296409368515015 + 0.01 * 7.222754001617432
Epoch 220, val loss: 0.8785319924354553
Epoch 230, training loss: 0.6367500424385071 = 0.5645550489425659 + 0.01 * 7.2195000648498535
Epoch 230, val loss: 0.8351422548294067
Epoch 240, training loss: 0.5795964002609253 = 0.5074216723442078 + 0.01 * 7.217470169067383
Epoch 240, val loss: 0.8023015260696411
Epoch 250, training loss: 0.5297183394432068 = 0.4575302302837372 + 0.01 * 7.218809604644775
Epoch 250, val loss: 0.7785968780517578
Epoch 260, training loss: 0.4862125813961029 = 0.4140590727329254 + 0.01 * 7.215350151062012
Epoch 260, val loss: 0.7621217966079712
Epoch 270, training loss: 0.4482060372829437 = 0.37608036398887634 + 0.01 * 7.212568283081055
Epoch 270, val loss: 0.7509967088699341
Epoch 280, training loss: 0.4147898256778717 = 0.342690646648407 + 0.01 * 7.2099175453186035
Epoch 280, val loss: 0.7437644004821777
Epoch 290, training loss: 0.3851622939109802 = 0.31309816241264343 + 0.01 * 7.206414222717285
Epoch 290, val loss: 0.7397553324699402
Epoch 300, training loss: 0.3585872948169708 = 0.2865643799304962 + 0.01 * 7.202291011810303
Epoch 300, val loss: 0.7385299205780029
Epoch 310, training loss: 0.33428898453712463 = 0.26231148838996887 + 0.01 * 7.19774866104126
Epoch 310, val loss: 0.7395626306533813
Epoch 320, training loss: 0.31130802631378174 = 0.2394133061170578 + 0.01 * 7.1894731521606445
Epoch 320, val loss: 0.7422723174095154
Epoch 330, training loss: 0.28871873021125793 = 0.21689578890800476 + 0.01 * 7.182295322418213
Epoch 330, val loss: 0.7461256384849548
Epoch 340, training loss: 0.26575011014938354 = 0.19401559233665466 + 0.01 * 7.173450946807861
Epoch 340, val loss: 0.7507213354110718
Epoch 350, training loss: 0.2424318790435791 = 0.17076878249645233 + 0.01 * 7.166308879852295
Epoch 350, val loss: 0.7558156847953796
Epoch 360, training loss: 0.2196732461452484 = 0.14811159670352936 + 0.01 * 7.156164169311523
Epoch 360, val loss: 0.7618010640144348
Epoch 370, training loss: 0.1987859010696411 = 0.12724627554416656 + 0.01 * 7.153962135314941
Epoch 370, val loss: 0.7689868807792664
Epoch 380, training loss: 0.18040156364440918 = 0.10898812115192413 + 0.01 * 7.14134407043457
Epoch 380, val loss: 0.7775892019271851
Epoch 390, training loss: 0.16494810581207275 = 0.09359037131071091 + 0.01 * 7.135773658752441
Epoch 390, val loss: 0.7875141501426697
Epoch 400, training loss: 0.1521732062101364 = 0.08087554574012756 + 0.01 * 7.129766464233398
Epoch 400, val loss: 0.7986481189727783
Epoch 410, training loss: 0.14173004031181335 = 0.07044679671525955 + 0.01 * 7.128323554992676
Epoch 410, val loss: 0.8107344508171082
Epoch 420, training loss: 0.13305938243865967 = 0.061869263648986816 + 0.01 * 7.119011402130127
Epoch 420, val loss: 0.8234944343566895
Epoch 430, training loss: 0.12595093250274658 = 0.054758742451667786 + 0.01 * 7.1192193031311035
Epoch 430, val loss: 0.8365412950515747
Epoch 440, training loss: 0.11987398564815521 = 0.04880468174815178 + 0.01 * 7.106931209564209
Epoch 440, val loss: 0.8497666716575623
Epoch 450, training loss: 0.11482223868370056 = 0.04376896843314171 + 0.01 * 7.105327129364014
Epoch 450, val loss: 0.8628601431846619
Epoch 460, training loss: 0.1103895902633667 = 0.0394686721265316 + 0.01 * 7.092092514038086
Epoch 460, val loss: 0.8758566975593567
Epoch 470, training loss: 0.10656047612428665 = 0.035766348242759705 + 0.01 * 7.079412937164307
Epoch 470, val loss: 0.8886107802391052
Epoch 480, training loss: 0.10338342189788818 = 0.03255505859851837 + 0.01 * 7.082836627960205
Epoch 480, val loss: 0.9010698795318604
Epoch 490, training loss: 0.10052824020385742 = 0.029754582792520523 + 0.01 * 7.077365875244141
Epoch 490, val loss: 0.913245439529419
Epoch 500, training loss: 0.0979544147849083 = 0.027295542880892754 + 0.01 * 7.065886974334717
Epoch 500, val loss: 0.9251106381416321
Epoch 510, training loss: 0.0957658588886261 = 0.025124985724687576 + 0.01 * 7.064087390899658
Epoch 510, val loss: 0.9366180300712585
Epoch 520, training loss: 0.09380345791578293 = 0.02320300042629242 + 0.01 * 7.060046195983887
Epoch 520, val loss: 0.947765052318573
Epoch 530, training loss: 0.09197594225406647 = 0.021493231877684593 + 0.01 * 7.048271179199219
Epoch 530, val loss: 0.9586277604103088
Epoch 540, training loss: 0.09037373214960098 = 0.01996678113937378 + 0.01 * 7.0406951904296875
Epoch 540, val loss: 0.9690693020820618
Epoch 550, training loss: 0.0889650285243988 = 0.018598375841975212 + 0.01 * 7.036664962768555
Epoch 550, val loss: 0.9792184233665466
Epoch 560, training loss: 0.08782719075679779 = 0.017369160428643227 + 0.01 * 7.045803070068359
Epoch 560, val loss: 0.9889618158340454
Epoch 570, training loss: 0.08655311912298203 = 0.016260461881756783 + 0.01 * 7.029266357421875
Epoch 570, val loss: 0.9984750151634216
Epoch 580, training loss: 0.08537127822637558 = 0.015258001163601875 + 0.01 * 7.011327743530273
Epoch 580, val loss: 1.0077158212661743
Epoch 590, training loss: 0.08450658619403839 = 0.014348722994327545 + 0.01 * 7.015786647796631
Epoch 590, val loss: 1.0164940357208252
Epoch 600, training loss: 0.08349074423313141 = 0.013521155342459679 + 0.01 * 6.996959209442139
Epoch 600, val loss: 1.025164008140564
Epoch 610, training loss: 0.08277510851621628 = 0.012766779400408268 + 0.01 * 7.000833511352539
Epoch 610, val loss: 1.0333874225616455
Epoch 620, training loss: 0.08206771314144135 = 0.012077630497515202 + 0.01 * 6.999008655548096
Epoch 620, val loss: 1.0413700342178345
Epoch 630, training loss: 0.08120991289615631 = 0.011445346288383007 + 0.01 * 6.976457118988037
Epoch 630, val loss: 1.049300193786621
Epoch 640, training loss: 0.08053858578205109 = 0.010865122079849243 + 0.01 * 6.96734619140625
Epoch 640, val loss: 1.0567471981048584
Epoch 650, training loss: 0.08005816489458084 = 0.01033064629882574 + 0.01 * 6.972752094268799
Epoch 650, val loss: 1.0640008449554443
Epoch 660, training loss: 0.07965049147605896 = 0.00983766745775938 + 0.01 * 6.981282711029053
Epoch 660, val loss: 1.0711864233016968
Epoch 670, training loss: 0.07888874411582947 = 0.009381023235619068 + 0.01 * 6.950772285461426
Epoch 670, val loss: 1.0779796838760376
Epoch 680, training loss: 0.0783390924334526 = 0.008957982063293457 + 0.01 * 6.938110828399658
Epoch 680, val loss: 1.0845751762390137
Epoch 690, training loss: 0.07804667204618454 = 0.008564462885260582 + 0.01 * 6.948221206665039
Epoch 690, val loss: 1.0909743309020996
Epoch 700, training loss: 0.07756182551383972 = 0.008198468945920467 + 0.01 * 6.936335563659668
Epoch 700, val loss: 1.0972974300384521
Epoch 710, training loss: 0.07699301093816757 = 0.007857656106352806 + 0.01 * 6.913535118103027
Epoch 710, val loss: 1.1034736633300781
Epoch 720, training loss: 0.07676676660776138 = 0.0075396946631371975 + 0.01 * 6.9227070808410645
Epoch 720, val loss: 1.1091607809066772
Epoch 730, training loss: 0.07627063989639282 = 0.0072430893778800964 + 0.01 * 6.902755260467529
Epoch 730, val loss: 1.1150130033493042
Epoch 740, training loss: 0.07605196535587311 = 0.0069647799246013165 + 0.01 * 6.908718585968018
Epoch 740, val loss: 1.120617151260376
Epoch 750, training loss: 0.07577577978372574 = 0.006703612860292196 + 0.01 * 6.907216548919678
Epoch 750, val loss: 1.1259828805923462
Epoch 760, training loss: 0.0754176452755928 = 0.00645823311060667 + 0.01 * 6.895941734313965
Epoch 760, val loss: 1.1312615871429443
Epoch 770, training loss: 0.07533513754606247 = 0.006227665580809116 + 0.01 * 6.910747528076172
Epoch 770, val loss: 1.136353611946106
Epoch 780, training loss: 0.07481661438941956 = 0.006009989883750677 + 0.01 * 6.88066291809082
Epoch 780, val loss: 1.1414389610290527
Epoch 790, training loss: 0.07483038306236267 = 0.0058043464086949825 + 0.01 * 6.902604103088379
Epoch 790, val loss: 1.1462961435317993
Epoch 800, training loss: 0.07444129139184952 = 0.0056105623953044415 + 0.01 * 6.883073329925537
Epoch 800, val loss: 1.1510154008865356
Epoch 810, training loss: 0.0741892009973526 = 0.0054276385344564915 + 0.01 * 6.876156806945801
Epoch 810, val loss: 1.1555795669555664
Epoch 820, training loss: 0.07389626652002335 = 0.005254240706562996 + 0.01 * 6.864202976226807
Epoch 820, val loss: 1.1600395441055298
Epoch 830, training loss: 0.07387292385101318 = 0.0050904033705592155 + 0.01 * 6.8782525062561035
Epoch 830, val loss: 1.1644304990768433
Epoch 840, training loss: 0.07349676638841629 = 0.004935051314532757 + 0.01 * 6.856171607971191
Epoch 840, val loss: 1.1687572002410889
Epoch 850, training loss: 0.07325166463851929 = 0.004787099082022905 + 0.01 * 6.846456527709961
Epoch 850, val loss: 1.1728477478027344
Epoch 860, training loss: 0.07320786267518997 = 0.00464647077023983 + 0.01 * 6.856139183044434
Epoch 860, val loss: 1.1768395900726318
Epoch 870, training loss: 0.07305146008729935 = 0.004512459971010685 + 0.01 * 6.85390043258667
Epoch 870, val loss: 1.1809641122817993
Epoch 880, training loss: 0.0728365033864975 = 0.0043848613277077675 + 0.01 * 6.845164775848389
Epoch 880, val loss: 1.1847398281097412
Epoch 890, training loss: 0.07266907393932343 = 0.004262493457645178 + 0.01 * 6.840658664703369
Epoch 890, val loss: 1.188575267791748
Epoch 900, training loss: 0.072386734187603 = 0.004145427141338587 + 0.01 * 6.824130535125732
Epoch 900, val loss: 1.192296028137207
Epoch 910, training loss: 0.07236634939908981 = 0.004033062141388655 + 0.01 * 6.833329200744629
Epoch 910, val loss: 1.1958428621292114
Epoch 920, training loss: 0.07238761335611343 = 0.003925683442503214 + 0.01 * 6.846193313598633
Epoch 920, val loss: 1.199566125869751
Epoch 930, training loss: 0.07207342982292175 = 0.0038235948886722326 + 0.01 * 6.824984073638916
Epoch 930, val loss: 1.2030363082885742
Epoch 940, training loss: 0.07192376255989075 = 0.0037256486248224974 + 0.01 * 6.819811820983887
Epoch 940, val loss: 1.2064789533615112
Epoch 950, training loss: 0.0716884434223175 = 0.0036320872604846954 + 0.01 * 6.805636405944824
Epoch 950, val loss: 1.2099523544311523
Epoch 960, training loss: 0.07173167169094086 = 0.003542074002325535 + 0.01 * 6.818960189819336
Epoch 960, val loss: 1.2129613161087036
Epoch 970, training loss: 0.07176412642002106 = 0.0034564747475087643 + 0.01 * 6.830765247344971
Epoch 970, val loss: 1.2164359092712402
Epoch 980, training loss: 0.0714288130402565 = 0.0033741090446710587 + 0.01 * 6.8054704666137695
Epoch 980, val loss: 1.2194545269012451
Epoch 990, training loss: 0.07132672518491745 = 0.0032954623457044363 + 0.01 * 6.803126811981201
Epoch 990, val loss: 1.222625732421875
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.812335266209805
The final CL Acc:0.74321, 0.00972, The final GNN Acc:0.81269, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13100])
remove edge: torch.Size([2, 7814])
updated graph: torch.Size([2, 10358])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.015960216522217 = 1.929991602897644 + 0.01 * 8.596858978271484
Epoch 0, val loss: 1.9216846227645874
Epoch 10, training loss: 2.0061659812927246 = 1.92019784450531 + 0.01 * 8.59681510925293
Epoch 10, val loss: 1.9124339818954468
Epoch 20, training loss: 1.9939664602279663 = 1.907999873161316 + 0.01 * 8.596653938293457
Epoch 20, val loss: 1.900420069694519
Epoch 30, training loss: 1.9768589735031128 = 1.8908978700637817 + 0.01 * 8.596115112304688
Epoch 30, val loss: 1.88335120677948
Epoch 40, training loss: 1.9520952701568604 = 1.8661617040634155 + 0.01 * 8.593352317810059
Epoch 40, val loss: 1.8591276407241821
Epoch 50, training loss: 1.9187209606170654 = 1.8329763412475586 + 0.01 * 8.574457168579102
Epoch 50, val loss: 1.828690528869629
Epoch 60, training loss: 1.8821805715560913 = 1.7972685098648071 + 0.01 * 8.49120807647705
Epoch 60, val loss: 1.7999510765075684
Epoch 70, training loss: 1.8443115949630737 = 1.7615021467208862 + 0.01 * 8.280943870544434
Epoch 70, val loss: 1.7710248231887817
Epoch 80, training loss: 1.7927238941192627 = 1.7111636400222778 + 0.01 * 8.156031608581543
Epoch 80, val loss: 1.7253236770629883
Epoch 90, training loss: 1.7203973531723022 = 1.6400235891342163 + 0.01 * 8.037382125854492
Epoch 90, val loss: 1.6616411209106445
Epoch 100, training loss: 1.6286085844039917 = 1.5493791103363037 + 0.01 * 7.922946453094482
Epoch 100, val loss: 1.584889531135559
Epoch 110, training loss: 1.528090476989746 = 1.4506207704544067 + 0.01 * 7.746967792510986
Epoch 110, val loss: 1.503008246421814
Epoch 120, training loss: 1.4294987916946411 = 1.3530560731887817 + 0.01 * 7.644271373748779
Epoch 120, val loss: 1.42582106590271
Epoch 130, training loss: 1.3359770774841309 = 1.2597755193710327 + 0.01 * 7.620156764984131
Epoch 130, val loss: 1.354585886001587
Epoch 140, training loss: 1.2464197874069214 = 1.1706416606903076 + 0.01 * 7.577814102172852
Epoch 140, val loss: 1.2887808084487915
Epoch 150, training loss: 1.1599538326263428 = 1.0846590995788574 + 0.01 * 7.529471397399902
Epoch 150, val loss: 1.2262641191482544
Epoch 160, training loss: 1.0754342079162598 = 1.000779628753662 + 0.01 * 7.465452194213867
Epoch 160, val loss: 1.165114402770996
Epoch 170, training loss: 0.9928401708602905 = 0.9189074635505676 + 0.01 * 7.393270969390869
Epoch 170, val loss: 1.1041548252105713
Epoch 180, training loss: 0.9135268330574036 = 0.8400355577468872 + 0.01 * 7.349126815795898
Epoch 180, val loss: 1.0442132949829102
Epoch 190, training loss: 0.8388051986694336 = 0.765505313873291 + 0.01 * 7.329989910125732
Epoch 190, val loss: 0.9874971508979797
Epoch 200, training loss: 0.7694090604782104 = 0.6962999105453491 + 0.01 * 7.310912609100342
Epoch 200, val loss: 0.936155378818512
Epoch 210, training loss: 0.7052141427993774 = 0.6323068737983704 + 0.01 * 7.290729999542236
Epoch 210, val loss: 0.8914983868598938
Epoch 220, training loss: 0.6451566815376282 = 0.5724824666976929 + 0.01 * 7.267422676086426
Epoch 220, val loss: 0.8537464737892151
Epoch 230, training loss: 0.5885987281799316 = 0.5161409378051758 + 0.01 * 7.245778560638428
Epoch 230, val loss: 0.8228221535682678
Epoch 240, training loss: 0.5360587239265442 = 0.4637680649757385 + 0.01 * 7.229063510894775
Epoch 240, val loss: 0.7992599606513977
Epoch 250, training loss: 0.4885403513908386 = 0.41642290353775024 + 0.01 * 7.211745262145996
Epoch 250, val loss: 0.7834653258323669
Epoch 260, training loss: 0.4466758668422699 = 0.3746868073940277 + 0.01 * 7.1989054679870605
Epoch 260, val loss: 0.775081992149353
Epoch 270, training loss: 0.4101922810077667 = 0.33829033374786377 + 0.01 * 7.190194606781006
Epoch 270, val loss: 0.7731152772903442
Epoch 280, training loss: 0.3781346082687378 = 0.30634742975234985 + 0.01 * 7.178718090057373
Epoch 280, val loss: 0.7759440541267395
Epoch 290, training loss: 0.3494558036327362 = 0.2777528762817383 + 0.01 * 7.17029333114624
Epoch 290, val loss: 0.7822073101997375
Epoch 300, training loss: 0.3230643570423126 = 0.25143495202064514 + 0.01 * 7.16294002532959
Epoch 300, val loss: 0.7907954454421997
Epoch 310, training loss: 0.29808908700942993 = 0.22657200694084167 + 0.01 * 7.151707649230957
Epoch 310, val loss: 0.8008977174758911
Epoch 320, training loss: 0.2742336392402649 = 0.2027607560157776 + 0.01 * 7.147287845611572
Epoch 320, val loss: 0.8117215633392334
Epoch 330, training loss: 0.25148120522499084 = 0.18010246753692627 + 0.01 * 7.137874603271484
Epoch 330, val loss: 0.8231566548347473
Epoch 340, training loss: 0.23023144900798798 = 0.15896347165107727 + 0.01 * 7.126797676086426
Epoch 340, val loss: 0.835253119468689
Epoch 350, training loss: 0.21087735891342163 = 0.13970088958740234 + 0.01 * 7.117646217346191
Epoch 350, val loss: 0.8482319116592407
Epoch 360, training loss: 0.19363412261009216 = 0.12249090522527695 + 0.01 * 7.114322662353516
Epoch 360, val loss: 0.8621927499771118
Epoch 370, training loss: 0.17839211225509644 = 0.10732772201299667 + 0.01 * 7.106438636779785
Epoch 370, val loss: 0.8772054314613342
Epoch 380, training loss: 0.16504371166229248 = 0.09408018738031387 + 0.01 * 7.096353054046631
Epoch 380, val loss: 0.893128514289856
Epoch 390, training loss: 0.15347033739089966 = 0.08257323503494263 + 0.01 * 7.089709281921387
Epoch 390, val loss: 0.909776508808136
Epoch 400, training loss: 0.14350004494190216 = 0.07262448221445084 + 0.01 * 7.0875563621521
Epoch 400, val loss: 0.9269208312034607
Epoch 410, training loss: 0.13489791750907898 = 0.06405066698789597 + 0.01 * 7.084724426269531
Epoch 410, val loss: 0.9442651867866516
Epoch 420, training loss: 0.1274408996105194 = 0.05667407810688019 + 0.01 * 7.0766825675964355
Epoch 420, val loss: 0.9616987705230713
Epoch 430, training loss: 0.12110741436481476 = 0.050331562757492065 + 0.01 * 7.077585220336914
Epoch 430, val loss: 0.9790399670600891
Epoch 440, training loss: 0.11556842923164368 = 0.04487915337085724 + 0.01 * 7.068927764892578
Epoch 440, val loss: 0.996180534362793
Epoch 450, training loss: 0.1108434796333313 = 0.040185462683439255 + 0.01 * 7.065801620483398
Epoch 450, val loss: 1.0130192041397095
Epoch 460, training loss: 0.10675443708896637 = 0.03613739833235741 + 0.01 * 7.061703681945801
Epoch 460, val loss: 1.0294638872146606
Epoch 470, training loss: 0.10320848226547241 = 0.03263489902019501 + 0.01 * 7.057358741760254
Epoch 470, val loss: 1.045467495918274
Epoch 480, training loss: 0.10021689534187317 = 0.029592830687761307 + 0.01 * 7.062406063079834
Epoch 480, val loss: 1.0610532760620117
Epoch 490, training loss: 0.09749355167150497 = 0.026942498981952667 + 0.01 * 7.055105686187744
Epoch 490, val loss: 1.0761103630065918
Epoch 500, training loss: 0.09509331732988358 = 0.024623041972517967 + 0.01 * 7.047027587890625
Epoch 500, val loss: 1.090663194656372
Epoch 510, training loss: 0.09310640394687653 = 0.022584885358810425 + 0.01 * 7.052151679992676
Epoch 510, val loss: 1.1047337055206299
Epoch 520, training loss: 0.09122483432292938 = 0.020787470042705536 + 0.01 * 7.043736934661865
Epoch 520, val loss: 1.118253469467163
Epoch 530, training loss: 0.08954575657844543 = 0.01919640600681305 + 0.01 * 7.034935474395752
Epoch 530, val loss: 1.1312965154647827
Epoch 540, training loss: 0.08817391842603683 = 0.01778201200067997 + 0.01 * 7.039190769195557
Epoch 540, val loss: 1.1439087390899658
Epoch 550, training loss: 0.08681975305080414 = 0.01652039960026741 + 0.01 * 7.029935836791992
Epoch 550, val loss: 1.1560250520706177
Epoch 560, training loss: 0.0856277197599411 = 0.015391219407320023 + 0.01 * 7.023650646209717
Epoch 560, val loss: 1.1677017211914062
Epoch 570, training loss: 0.084658682346344 = 0.014376829378306866 + 0.01 * 7.0281853675842285
Epoch 570, val loss: 1.1789822578430176
Epoch 580, training loss: 0.08367390930652618 = 0.01346306037157774 + 0.01 * 7.021085262298584
Epoch 580, val loss: 1.1898070573806763
Epoch 590, training loss: 0.08277632296085358 = 0.012637241743505001 + 0.01 * 7.013908386230469
Epoch 590, val loss: 1.2002705335617065
Epoch 600, training loss: 0.08202606439590454 = 0.011888116598129272 + 0.01 * 7.013794898986816
Epoch 600, val loss: 1.2103824615478516
Epoch 610, training loss: 0.08124974370002747 = 0.01120720710605383 + 0.01 * 7.00425386428833
Epoch 610, val loss: 1.2200982570648193
Epoch 620, training loss: 0.08067814260721207 = 0.010586606338620186 + 0.01 * 7.009153842926025
Epoch 620, val loss: 1.2294752597808838
Epoch 630, training loss: 0.08002389222383499 = 0.010019049979746342 + 0.01 * 7.000484466552734
Epoch 630, val loss: 1.2385224103927612
Epoch 640, training loss: 0.0795070007443428 = 0.009498982690274715 + 0.01 * 7.000802040100098
Epoch 640, val loss: 1.2472813129425049
Epoch 650, training loss: 0.07897035777568817 = 0.009021583944559097 + 0.01 * 6.994877815246582
Epoch 650, val loss: 1.2557497024536133
Epoch 660, training loss: 0.078542560338974 = 0.008581884205341339 + 0.01 * 6.996067523956299
Epoch 660, val loss: 1.2639515399932861
Epoch 670, training loss: 0.07800717651844025 = 0.008176173083484173 + 0.01 * 6.983100414276123
Epoch 670, val loss: 1.2718453407287598
Epoch 680, training loss: 0.07778441905975342 = 0.007800836581736803 + 0.01 * 6.998358249664307
Epoch 680, val loss: 1.2795743942260742
Epoch 690, training loss: 0.07728271186351776 = 0.007453774567693472 + 0.01 * 6.982893466949463
Epoch 690, val loss: 1.286957859992981
Epoch 700, training loss: 0.07685586810112 = 0.007131550461053848 + 0.01 * 6.972432613372803
Epoch 700, val loss: 1.294157862663269
Epoch 710, training loss: 0.07660704851150513 = 0.006831679493188858 + 0.01 * 6.977537631988525
Epoch 710, val loss: 1.301161766052246
Epoch 720, training loss: 0.07619739323854446 = 0.006552469916641712 + 0.01 * 6.964491844177246
Epoch 720, val loss: 1.3078997135162354
Epoch 730, training loss: 0.07588857412338257 = 0.006292203441262245 + 0.01 * 6.959637641906738
Epoch 730, val loss: 1.3144173622131348
Epoch 740, training loss: 0.07563807815313339 = 0.00604888703674078 + 0.01 * 6.958919525146484
Epoch 740, val loss: 1.320786714553833
Epoch 750, training loss: 0.075327068567276 = 0.005820936989039183 + 0.01 * 6.950613498687744
Epoch 750, val loss: 1.3269240856170654
Epoch 760, training loss: 0.07527908682823181 = 0.00560721242800355 + 0.01 * 6.967187881469727
Epoch 760, val loss: 1.3329734802246094
Epoch 770, training loss: 0.07499399036169052 = 0.005406997166574001 + 0.01 * 6.9586992263793945
Epoch 770, val loss: 1.338674545288086
Epoch 780, training loss: 0.0745907798409462 = 0.0052192602306604385 + 0.01 * 6.93715238571167
Epoch 780, val loss: 1.344274640083313
Epoch 790, training loss: 0.07453572005033493 = 0.00504231546074152 + 0.01 * 6.949340343475342
Epoch 790, val loss: 1.3497302532196045
Epoch 800, training loss: 0.07424119114875793 = 0.004875480197370052 + 0.01 * 6.93657112121582
Epoch 800, val loss: 1.3549284934997559
Epoch 810, training loss: 0.07424221932888031 = 0.004718294367194176 + 0.01 * 6.952392101287842
Epoch 810, val loss: 1.3601258993148804
Epoch 820, training loss: 0.07378962635993958 = 0.004569802433252335 + 0.01 * 6.9219818115234375
Epoch 820, val loss: 1.3650137186050415
Epoch 830, training loss: 0.07367759197950363 = 0.0044292970560491085 + 0.01 * 6.924829959869385
Epoch 830, val loss: 1.369887351989746
Epoch 840, training loss: 0.0735078901052475 = 0.00429646298289299 + 0.01 * 6.921142578125
Epoch 840, val loss: 1.3745687007904053
Epoch 850, training loss: 0.07335638999938965 = 0.0041706617921590805 + 0.01 * 6.918572902679443
Epoch 850, val loss: 1.379070520401001
Epoch 860, training loss: 0.07310959696769714 = 0.004051528871059418 + 0.01 * 6.905807018280029
Epoch 860, val loss: 1.3834552764892578
Epoch 870, training loss: 0.07309360057115555 = 0.003938341047614813 + 0.01 * 6.915526390075684
Epoch 870, val loss: 1.3877841234207153
Epoch 880, training loss: 0.07298310846090317 = 0.0038307132199406624 + 0.01 * 6.915239334106445
Epoch 880, val loss: 1.3918428421020508
Epoch 890, training loss: 0.07273683696985245 = 0.00372847868129611 + 0.01 * 6.900835990905762
Epoch 890, val loss: 1.3959625959396362
Epoch 900, training loss: 0.07263567298650742 = 0.0036311426665633917 + 0.01 * 6.900453090667725
Epoch 900, val loss: 1.3999038934707642
Epoch 910, training loss: 0.07258531451225281 = 0.0035383603535592556 + 0.01 * 6.9046950340271
Epoch 910, val loss: 1.4036859273910522
Epoch 920, training loss: 0.07231113314628601 = 0.0034501447807997465 + 0.01 * 6.886098861694336
Epoch 920, val loss: 1.4073938131332397
Epoch 930, training loss: 0.0725947692990303 = 0.0033657941967248917 + 0.01 * 6.922897815704346
Epoch 930, val loss: 1.411034107208252
Epoch 940, training loss: 0.0720556229352951 = 0.0032856129109859467 + 0.01 * 6.87700080871582
Epoch 940, val loss: 1.4144060611724854
Epoch 950, training loss: 0.07206360250711441 = 0.0032087815925478935 + 0.01 * 6.885482311248779
Epoch 950, val loss: 1.4179354906082153
Epoch 960, training loss: 0.07180871814489365 = 0.0031354709062725306 + 0.01 * 6.867324352264404
Epoch 960, val loss: 1.421116828918457
Epoch 970, training loss: 0.07208698242902756 = 0.0030651772394776344 + 0.01 * 6.9021806716918945
Epoch 970, val loss: 1.4244154691696167
Epoch 980, training loss: 0.07168532907962799 = 0.0029979282990098 + 0.01 * 6.868740081787109
Epoch 980, val loss: 1.4275085926055908
Epoch 990, training loss: 0.07187160849571228 = 0.0029336644802242517 + 0.01 * 6.893794059753418
Epoch 990, val loss: 1.4306209087371826
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.0193753242492676 = 1.9334067106246948 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9350908994674683
Epoch 10, training loss: 2.00968599319458 = 1.923717975616455 + 0.01 * 8.596813201904297
Epoch 10, val loss: 1.925828456878662
Epoch 20, training loss: 1.9972057342529297 = 1.9112392663955688 + 0.01 * 8.596643447875977
Epoch 20, val loss: 1.9133484363555908
Epoch 30, training loss: 1.9788569211959839 = 1.8928955793380737 + 0.01 * 8.596138000488281
Epoch 30, val loss: 1.8949676752090454
Epoch 40, training loss: 1.9514429569244385 = 1.8655067682266235 + 0.01 * 8.59361457824707
Epoch 40, val loss: 1.868251919746399
Epoch 50, training loss: 1.9145493507385254 = 1.8288017511367798 + 0.01 * 8.574761390686035
Epoch 50, val loss: 1.8346322774887085
Epoch 60, training loss: 1.8751996755599976 = 1.7905484437942505 + 0.01 * 8.465119361877441
Epoch 60, val loss: 1.802291989326477
Epoch 70, training loss: 1.8339035511016846 = 1.751610279083252 + 0.01 * 8.229330062866211
Epoch 70, val loss: 1.7661155462265015
Epoch 80, training loss: 1.7782152891159058 = 1.6972854137420654 + 0.01 * 8.092989921569824
Epoch 80, val loss: 1.7147401571273804
Epoch 90, training loss: 1.7032729387283325 = 1.6235562562942505 + 0.01 * 7.9716691970825195
Epoch 90, val loss: 1.6515430212020874
Epoch 100, training loss: 1.6117141246795654 = 1.5334420204162598 + 0.01 * 7.82721471786499
Epoch 100, val loss: 1.577303409576416
Epoch 110, training loss: 1.514452576637268 = 1.4381471872329712 + 0.01 * 7.6305413246154785
Epoch 110, val loss: 1.4989897012710571
Epoch 120, training loss: 1.4210182428359985 = 1.345294713973999 + 0.01 * 7.572357654571533
Epoch 120, val loss: 1.4238795042037964
Epoch 130, training loss: 1.331603765487671 = 1.2566496133804321 + 0.01 * 7.495411396026611
Epoch 130, val loss: 1.354007601737976
Epoch 140, training loss: 1.2464479207992554 = 1.1721419095993042 + 0.01 * 7.430602073669434
Epoch 140, val loss: 1.2889926433563232
Epoch 150, training loss: 1.164591670036316 = 1.0907775163650513 + 0.01 * 7.381415367126465
Epoch 150, val loss: 1.228585958480835
Epoch 160, training loss: 1.085048794746399 = 1.0115145444869995 + 0.01 * 7.353430271148682
Epoch 160, val loss: 1.1710479259490967
Epoch 170, training loss: 1.0073961019515991 = 0.933986246585846 + 0.01 * 7.340989589691162
Epoch 170, val loss: 1.1148985624313354
Epoch 180, training loss: 0.9320516586303711 = 0.8587337732315063 + 0.01 * 7.331785678863525
Epoch 180, val loss: 1.0593078136444092
Epoch 190, training loss: 0.8595564961433411 = 0.7863395810127258 + 0.01 * 7.321692943572998
Epoch 190, val loss: 1.0049890279769897
Epoch 200, training loss: 0.7905607223510742 = 0.7174738049507141 + 0.01 * 7.308690071105957
Epoch 200, val loss: 0.9535855054855347
Epoch 210, training loss: 0.7256091237068176 = 0.6526837348937988 + 0.01 * 7.292537212371826
Epoch 210, val loss: 0.906075656414032
Epoch 220, training loss: 0.664836049079895 = 0.592082142829895 + 0.01 * 7.275390625
Epoch 220, val loss: 0.8630611300468445
Epoch 230, training loss: 0.6076889634132385 = 0.5351089835166931 + 0.01 * 7.257997035980225
Epoch 230, val loss: 0.824702262878418
Epoch 240, training loss: 0.553261399269104 = 0.48081302642822266 + 0.01 * 7.244840621948242
Epoch 240, val loss: 0.7907500267028809
Epoch 250, training loss: 0.5007026791572571 = 0.4283773899078369 + 0.01 * 7.232527732849121
Epoch 250, val loss: 0.7611657977104187
Epoch 260, training loss: 0.4500143229961395 = 0.3777679204940796 + 0.01 * 7.224639415740967
Epoch 260, val loss: 0.7362792491912842
Epoch 270, training loss: 0.4018910229206085 = 0.32968857884407043 + 0.01 * 7.220243453979492
Epoch 270, val loss: 0.7167115211486816
Epoch 280, training loss: 0.35743188858032227 = 0.28523650765419006 + 0.01 * 7.219539642333984
Epoch 280, val loss: 0.7033500671386719
Epoch 290, training loss: 0.31745532155036926 = 0.24530568718910217 + 0.01 * 7.214963912963867
Epoch 290, val loss: 0.6960432529449463
Epoch 300, training loss: 0.28260210156440735 = 0.2104739397764206 + 0.01 * 7.2128167152404785
Epoch 300, val loss: 0.6941929459571838
Epoch 310, training loss: 0.2529589831829071 = 0.18084587156772614 + 0.01 * 7.211312294006348
Epoch 310, val loss: 0.6969341039657593
Epoch 320, training loss: 0.22813527286052704 = 0.1560148149728775 + 0.01 * 7.212045669555664
Epoch 320, val loss: 0.7031974196434021
Epoch 330, training loss: 0.20740655064582825 = 0.13531376421451569 + 0.01 * 7.2092790603637695
Epoch 330, val loss: 0.7121288180351257
Epoch 340, training loss: 0.19007936120033264 = 0.1180097833275795 + 0.01 * 7.2069573402404785
Epoch 340, val loss: 0.7228925228118896
Epoch 350, training loss: 0.17549356818199158 = 0.10345689952373505 + 0.01 * 7.203666687011719
Epoch 350, val loss: 0.7349416613578796
Epoch 360, training loss: 0.1631622612476349 = 0.09113193303346634 + 0.01 * 7.20303201675415
Epoch 360, val loss: 0.7478326559066772
Epoch 370, training loss: 0.15263769030570984 = 0.08062896877527237 + 0.01 * 7.200871467590332
Epoch 370, val loss: 0.7611520290374756
Epoch 380, training loss: 0.14359843730926514 = 0.07162876427173615 + 0.01 * 7.1969685554504395
Epoch 380, val loss: 0.7746584415435791
Epoch 390, training loss: 0.13584552705287933 = 0.06387785077095032 + 0.01 * 7.196767807006836
Epoch 390, val loss: 0.7882106900215149
Epoch 400, training loss: 0.1291053146123886 = 0.05717647075653076 + 0.01 * 7.19288444519043
Epoch 400, val loss: 0.8016617894172668
Epoch 410, training loss: 0.1232479140162468 = 0.051362358033657074 + 0.01 * 7.188555717468262
Epoch 410, val loss: 0.815010666847229
Epoch 420, training loss: 0.11817140877246857 = 0.04630298539996147 + 0.01 * 7.18684196472168
Epoch 420, val loss: 0.8281461000442505
Epoch 430, training loss: 0.11374306678771973 = 0.04188952222466469 + 0.01 * 7.185354709625244
Epoch 430, val loss: 0.8410243988037109
Epoch 440, training loss: 0.10980938374996185 = 0.038028534501791 + 0.01 * 7.178084850311279
Epoch 440, val loss: 0.8536366820335388
Epoch 450, training loss: 0.10638082027435303 = 0.03463985398411751 + 0.01 * 7.17409610748291
Epoch 450, val loss: 0.8659610152244568
Epoch 460, training loss: 0.10340440273284912 = 0.03165626898407936 + 0.01 * 7.174813270568848
Epoch 460, val loss: 0.8780277967453003
Epoch 470, training loss: 0.10069310665130615 = 0.029022900387644768 + 0.01 * 7.167020797729492
Epoch 470, val loss: 0.889789879322052
Epoch 480, training loss: 0.09829023480415344 = 0.026690755039453506 + 0.01 * 7.159947872161865
Epoch 480, val loss: 0.9012050628662109
Epoch 490, training loss: 0.09616526961326599 = 0.024617429822683334 + 0.01 * 7.154784202575684
Epoch 490, val loss: 0.9123377799987793
Epoch 500, training loss: 0.09435135871171951 = 0.02276875264942646 + 0.01 * 7.158260822296143
Epoch 500, val loss: 0.9231415390968323
Epoch 510, training loss: 0.09252772480249405 = 0.021115779876708984 + 0.01 * 7.141194820404053
Epoch 510, val loss: 0.9336597919464111
Epoch 520, training loss: 0.09097819775342941 = 0.019630881026387215 + 0.01 * 7.134731769561768
Epoch 520, val loss: 0.9438728094100952
Epoch 530, training loss: 0.08965412527322769 = 0.018292933702468872 + 0.01 * 7.136119365692139
Epoch 530, val loss: 0.9538430571556091
Epoch 540, training loss: 0.08825792372226715 = 0.01708599552512169 + 0.01 * 7.117193222045898
Epoch 540, val loss: 0.9634822010993958
Epoch 550, training loss: 0.08708212524652481 = 0.01599317230284214 + 0.01 * 7.108895778656006
Epoch 550, val loss: 0.9729436635971069
Epoch 560, training loss: 0.08599018305540085 = 0.015002744272351265 + 0.01 * 7.098743915557861
Epoch 560, val loss: 0.9821034669876099
Epoch 570, training loss: 0.08500556647777557 = 0.01410498283803463 + 0.01 * 7.090058326721191
Epoch 570, val loss: 0.9909916520118713
Epoch 580, training loss: 0.08407612144947052 = 0.013287539593875408 + 0.01 * 7.078858852386475
Epoch 580, val loss: 0.9996659755706787
Epoch 590, training loss: 0.08334110677242279 = 0.012541946023702621 + 0.01 * 7.079916477203369
Epoch 590, val loss: 1.0080389976501465
Epoch 600, training loss: 0.08251293003559113 = 0.011860637925565243 + 0.01 * 7.0652289390563965
Epoch 600, val loss: 1.0162280797958374
Epoch 610, training loss: 0.08200175315141678 = 0.011236143298447132 + 0.01 * 7.076560974121094
Epoch 610, val loss: 1.0241647958755493
Epoch 620, training loss: 0.0811333954334259 = 0.01066310703754425 + 0.01 * 7.0470290184021
Epoch 620, val loss: 1.031825065612793
Epoch 630, training loss: 0.08082751929759979 = 0.010135512799024582 + 0.01 * 7.0692009925842285
Epoch 630, val loss: 1.0393139123916626
Epoch 640, training loss: 0.07987687736749649 = 0.009649488143622875 + 0.01 * 7.022739410400391
Epoch 640, val loss: 1.0465084314346313
Epoch 650, training loss: 0.07935847342014313 = 0.00920028518885374 + 0.01 * 7.015819549560547
Epoch 650, val loss: 1.0536279678344727
Epoch 660, training loss: 0.07892601191997528 = 0.008783920668065548 + 0.01 * 7.014209270477295
Epoch 660, val loss: 1.060464859008789
Epoch 670, training loss: 0.07842504978179932 = 0.008397623896598816 + 0.01 * 7.002742767333984
Epoch 670, val loss: 1.0671720504760742
Epoch 680, training loss: 0.078010693192482 = 0.008038723841309547 + 0.01 * 6.997197151184082
Epoch 680, val loss: 1.073691487312317
Epoch 690, training loss: 0.07765119522809982 = 0.0077044484205543995 + 0.01 * 6.994674205780029
Epoch 690, val loss: 1.080077052116394
Epoch 700, training loss: 0.07723519951105118 = 0.0073928311467170715 + 0.01 * 6.984237194061279
Epoch 700, val loss: 1.0862406492233276
Epoch 710, training loss: 0.07711268961429596 = 0.007101507857441902 + 0.01 * 7.0011186599731445
Epoch 710, val loss: 1.092291235923767
Epoch 720, training loss: 0.0767274871468544 = 0.006828998681157827 + 0.01 * 6.989849090576172
Epoch 720, val loss: 1.0981252193450928
Epoch 730, training loss: 0.0762723833322525 = 0.0065735080279409885 + 0.01 * 6.969888210296631
Epoch 730, val loss: 1.1038275957107544
Epoch 740, training loss: 0.07597386091947556 = 0.006333175115287304 + 0.01 * 6.96406888961792
Epoch 740, val loss: 1.109532117843628
Epoch 750, training loss: 0.07568882405757904 = 0.006106838118284941 + 0.01 * 6.9581990242004395
Epoch 750, val loss: 1.1149957180023193
Epoch 760, training loss: 0.07559878379106522 = 0.005894235800951719 + 0.01 * 6.970454692840576
Epoch 760, val loss: 1.1203001737594604
Epoch 770, training loss: 0.07519538700580597 = 0.005694329272955656 + 0.01 * 6.950105667114258
Epoch 770, val loss: 1.1255398988723755
Epoch 780, training loss: 0.07501528412103653 = 0.005505440291017294 + 0.01 * 6.950984001159668
Epoch 780, val loss: 1.1305911540985107
Epoch 790, training loss: 0.0748324990272522 = 0.0053271763026714325 + 0.01 * 6.950532913208008
Epoch 790, val loss: 1.135543942451477
Epoch 800, training loss: 0.07454472780227661 = 0.005158611107617617 + 0.01 * 6.93861198425293
Epoch 800, val loss: 1.1404087543487549
Epoch 810, training loss: 0.07432714849710464 = 0.004999031312763691 + 0.01 * 6.932811737060547
Epoch 810, val loss: 1.1451630592346191
Epoch 820, training loss: 0.07432706654071808 = 0.004848240874707699 + 0.01 * 6.947882652282715
Epoch 820, val loss: 1.1498078107833862
Epoch 830, training loss: 0.07395992428064346 = 0.004704967141151428 + 0.01 * 6.9254961013793945
Epoch 830, val loss: 1.1541908979415894
Epoch 840, training loss: 0.0738401859998703 = 0.004569170996546745 + 0.01 * 6.927101135253906
Epoch 840, val loss: 1.1586610078811646
Epoch 850, training loss: 0.0736745223402977 = 0.004440389573574066 + 0.01 * 6.923413276672363
Epoch 850, val loss: 1.1629899740219116
Epoch 860, training loss: 0.07340832054615021 = 0.004317814949899912 + 0.01 * 6.909050941467285
Epoch 860, val loss: 1.1671677827835083
Epoch 870, training loss: 0.07361721992492676 = 0.004201042465865612 + 0.01 * 6.941617965698242
Epoch 870, val loss: 1.171385407447815
Epoch 880, training loss: 0.07307320833206177 = 0.004090018104761839 + 0.01 * 6.898319244384766
Epoch 880, val loss: 1.1752585172653198
Epoch 890, training loss: 0.07301333546638489 = 0.003984291106462479 + 0.01 * 6.902904510498047
Epoch 890, val loss: 1.1793339252471924
Epoch 900, training loss: 0.07281765341758728 = 0.003883182071149349 + 0.01 * 6.893446922302246
Epoch 900, val loss: 1.183144211769104
Epoch 910, training loss: 0.07269182801246643 = 0.0037868348881602287 + 0.01 * 6.890500068664551
Epoch 910, val loss: 1.1868469715118408
Epoch 920, training loss: 0.0730048418045044 = 0.00369462464004755 + 0.01 * 6.9310221672058105
Epoch 920, val loss: 1.1906673908233643
Epoch 930, training loss: 0.07256169617176056 = 0.0036067375913262367 + 0.01 * 6.895496368408203
Epoch 930, val loss: 1.1940984725952148
Epoch 940, training loss: 0.07232809066772461 = 0.003522871993482113 + 0.01 * 6.88052225112915
Epoch 940, val loss: 1.1976990699768066
Epoch 950, training loss: 0.07220897823572159 = 0.0034419295843690634 + 0.01 * 6.876704692840576
Epoch 950, val loss: 1.2010880708694458
Epoch 960, training loss: 0.07208958268165588 = 0.003364663338288665 + 0.01 * 6.87249231338501
Epoch 960, val loss: 1.2045022249221802
Epoch 970, training loss: 0.0718708485364914 = 0.0032905819825828075 + 0.01 * 6.858027458190918
Epoch 970, val loss: 1.2077618837356567
Epoch 980, training loss: 0.07175671309232712 = 0.0032197190448641777 + 0.01 * 6.853700160980225
Epoch 980, val loss: 1.2111347913742065
Epoch 990, training loss: 0.0720505490899086 = 0.003151642857119441 + 0.01 * 6.889891147613525
Epoch 990, val loss: 1.2142192125320435
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8328940432261466
=== training gcn model ===
Epoch 0, training loss: 2.041688919067383 = 1.95572030544281 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.9568955898284912
Epoch 10, training loss: 2.030616044998169 = 1.9446485042572021 + 0.01 * 8.596763610839844
Epoch 10, val loss: 1.9460068941116333
Epoch 20, training loss: 2.0166549682617188 = 1.930690050125122 + 0.01 * 8.5964937210083
Epoch 20, val loss: 1.9318381547927856
Epoch 30, training loss: 1.9968723058700562 = 1.9109166860580444 + 0.01 * 8.595566749572754
Epoch 30, val loss: 1.9113408327102661
Epoch 40, training loss: 1.9677107334136963 = 1.8818070888519287 + 0.01 * 8.590361595153809
Epoch 40, val loss: 1.8812942504882812
Epoch 50, training loss: 1.9279627799987793 = 1.8423500061035156 + 0.01 * 8.561279296875
Epoch 50, val loss: 1.8424485921859741
Epoch 60, training loss: 1.886096477508545 = 1.8015739917755127 + 0.01 * 8.45224380493164
Epoch 60, val loss: 1.8068971633911133
Epoch 70, training loss: 1.851431965827942 = 1.768471598625183 + 0.01 * 8.296041488647461
Epoch 70, val loss: 1.7806298732757568
Epoch 80, training loss: 1.8077707290649414 = 1.7258576154708862 + 0.01 * 8.19131088256836
Epoch 80, val loss: 1.743566870689392
Epoch 90, training loss: 1.7460193634033203 = 1.6657650470733643 + 0.01 * 8.025425910949707
Epoch 90, val loss: 1.6910585165023804
Epoch 100, training loss: 1.6639667749404907 = 1.585371494293213 + 0.01 * 7.859533309936523
Epoch 100, val loss: 1.6229360103607178
Epoch 110, training loss: 1.574018955230713 = 1.4968020915985107 + 0.01 * 7.721686840057373
Epoch 110, val loss: 1.552247405052185
Epoch 120, training loss: 1.4910783767700195 = 1.414383888244629 + 0.01 * 7.669445514678955
Epoch 120, val loss: 1.4915894269943237
Epoch 130, training loss: 1.416570782661438 = 1.3404595851898193 + 0.01 * 7.6111159324646
Epoch 130, val loss: 1.4421206712722778
Epoch 140, training loss: 1.3458659648895264 = 1.2706999778747559 + 0.01 * 7.516595840454102
Epoch 140, val loss: 1.3971132040023804
Epoch 150, training loss: 1.2762925624847412 = 1.2021548748016357 + 0.01 * 7.413771152496338
Epoch 150, val loss: 1.3522623777389526
Epoch 160, training loss: 1.2088677883148193 = 1.135475516319275 + 0.01 * 7.339226722717285
Epoch 160, val loss: 1.3079290390014648
Epoch 170, training loss: 1.1457653045654297 = 1.073011875152588 + 0.01 * 7.275344371795654
Epoch 170, val loss: 1.2661244869232178
Epoch 180, training loss: 1.0888983011245728 = 1.0166376829147339 + 0.01 * 7.226062297821045
Epoch 180, val loss: 1.2283886671066284
Epoch 190, training loss: 1.0386117696762085 = 0.9666686058044434 + 0.01 * 7.194321155548096
Epoch 190, val loss: 1.1950042247772217
Epoch 200, training loss: 0.9933524131774902 = 0.9216139316558838 + 0.01 * 7.173847675323486
Epoch 200, val loss: 1.1641958951950073
Epoch 210, training loss: 0.9502252340316772 = 0.8785906434059143 + 0.01 * 7.163460731506348
Epoch 210, val loss: 1.1340123414993286
Epoch 220, training loss: 0.9056545495986938 = 0.8340829014778137 + 0.01 * 7.157163619995117
Epoch 220, val loss: 1.1013907194137573
Epoch 230, training loss: 0.8564548492431641 = 0.7849129438400269 + 0.01 * 7.154191493988037
Epoch 230, val loss: 1.06418776512146
Epoch 240, training loss: 0.8006768226623535 = 0.7291480302810669 + 0.01 * 7.1528801918029785
Epoch 240, val loss: 1.0215272903442383
Epoch 250, training loss: 0.7389302253723145 = 0.6674073338508606 + 0.01 * 7.152288913726807
Epoch 250, val loss: 0.9745457172393799
Epoch 260, training loss: 0.674088716506958 = 0.6025683879852295 + 0.01 * 7.1520304679870605
Epoch 260, val loss: 0.9266517758369446
Epoch 270, training loss: 0.6099473237991333 = 0.5384217500686646 + 0.01 * 7.152560710906982
Epoch 270, val loss: 0.8821361064910889
Epoch 280, training loss: 0.5495398044586182 = 0.4780113101005554 + 0.01 * 7.152848243713379
Epoch 280, val loss: 0.843945324420929
Epoch 290, training loss: 0.4945172667503357 = 0.4229829013347626 + 0.01 * 7.153436183929443
Epoch 290, val loss: 0.8132403492927551
Epoch 300, training loss: 0.44525039196014404 = 0.373702734708786 + 0.01 * 7.154767036437988
Epoch 300, val loss: 0.7898929715156555
Epoch 310, training loss: 0.4012899696826935 = 0.32973000407218933 + 0.01 * 7.155997276306152
Epoch 310, val loss: 0.7731696367263794
Epoch 320, training loss: 0.3618181049823761 = 0.290237158536911 + 0.01 * 7.1580939292907715
Epoch 320, val loss: 0.7620116472244263
Epoch 330, training loss: 0.3260764479637146 = 0.2544870376586914 + 0.01 * 7.158940315246582
Epoch 330, val loss: 0.7554817199707031
Epoch 340, training loss: 0.2936270534992218 = 0.22202888131141663 + 0.01 * 7.159817218780518
Epoch 340, val loss: 0.7529752254486084
Epoch 350, training loss: 0.264376699924469 = 0.1927780956029892 + 0.01 * 7.159861087799072
Epoch 350, val loss: 0.7541050314903259
Epoch 360, training loss: 0.2384411096572876 = 0.1668332815170288 + 0.01 * 7.160783767700195
Epoch 360, val loss: 0.7588277459144592
Epoch 370, training loss: 0.2158435881137848 = 0.14423204958438873 + 0.01 * 7.161154270172119
Epoch 370, val loss: 0.7668784260749817
Epoch 380, training loss: 0.19640475511550903 = 0.12480045109987259 + 0.01 * 7.160429954528809
Epoch 380, val loss: 0.7778679132461548
Epoch 390, training loss: 0.1798444539308548 = 0.10822843760251999 + 0.01 * 7.161601543426514
Epoch 390, val loss: 0.7914984226226807
Epoch 400, training loss: 0.16577309370040894 = 0.09418059140443802 + 0.01 * 7.159250259399414
Epoch 400, val loss: 0.8071727752685547
Epoch 410, training loss: 0.15388953685760498 = 0.082309789955616 + 0.01 * 7.157973766326904
Epoch 410, val loss: 0.8244832754135132
Epoch 420, training loss: 0.14383810758590698 = 0.07228458672761917 + 0.01 * 7.155351161956787
Epoch 420, val loss: 0.8428379893302917
Epoch 430, training loss: 0.13536585867404938 = 0.0638020783662796 + 0.01 * 7.156378269195557
Epoch 430, val loss: 0.861824631690979
Epoch 440, training loss: 0.12809807062149048 = 0.056589603424072266 + 0.01 * 7.1508469581604
Epoch 440, val loss: 0.881069540977478
Epoch 450, training loss: 0.1219467967748642 = 0.0504145585000515 + 0.01 * 7.153223514556885
Epoch 450, val loss: 0.900406539440155
Epoch 460, training loss: 0.11653643846511841 = 0.04509526491165161 + 0.01 * 7.144117832183838
Epoch 460, val loss: 0.9195355176925659
Epoch 470, training loss: 0.11185671389102936 = 0.04049405828118324 + 0.01 * 7.136265754699707
Epoch 470, val loss: 0.9383304119110107
Epoch 480, training loss: 0.10793812572956085 = 0.03649871423840523 + 0.01 * 7.1439409255981445
Epoch 480, val loss: 0.9567319750785828
Epoch 490, training loss: 0.10430248081684113 = 0.03302047401666641 + 0.01 * 7.128200531005859
Epoch 490, val loss: 0.9746556878089905
Epoch 500, training loss: 0.10119824856519699 = 0.029982155188918114 + 0.01 * 7.121609210968018
Epoch 500, val loss: 0.9920384287834167
Epoch 510, training loss: 0.09844355285167694 = 0.027320165187120438 + 0.01 * 7.112339019775391
Epoch 510, val loss: 1.008886456489563
Epoch 520, training loss: 0.09607579559087753 = 0.02498125471174717 + 0.01 * 7.109454154968262
Epoch 520, val loss: 1.0251662731170654
Epoch 530, training loss: 0.09400680661201477 = 0.0229189433157444 + 0.01 * 7.108786106109619
Epoch 530, val loss: 1.0409300327301025
Epoch 540, training loss: 0.09193690866231918 = 0.021096060052514076 + 0.01 * 7.084084987640381
Epoch 540, val loss: 1.0560516119003296
Epoch 550, training loss: 0.09020917117595673 = 0.01947857066988945 + 0.01 * 7.073060512542725
Epoch 550, val loss: 1.0706353187561035
Epoch 560, training loss: 0.08867290616035461 = 0.018040038645267487 + 0.01 * 7.063287258148193
Epoch 560, val loss: 1.0846227407455444
Epoch 570, training loss: 0.08769398927688599 = 0.016755811870098114 + 0.01 * 7.093817710876465
Epoch 570, val loss: 1.0981409549713135
Epoch 580, training loss: 0.08605095744132996 = 0.015607008710503578 + 0.01 * 7.044395446777344
Epoch 580, val loss: 1.1109987497329712
Epoch 590, training loss: 0.08494210243225098 = 0.014575131237506866 + 0.01 * 7.0366973876953125
Epoch 590, val loss: 1.1233768463134766
Epoch 600, training loss: 0.08392076194286346 = 0.013645226135849953 + 0.01 * 7.027554035186768
Epoch 600, val loss: 1.1354185342788696
Epoch 610, training loss: 0.08295196294784546 = 0.012805266305804253 + 0.01 * 7.014669895172119
Epoch 610, val loss: 1.1468788385391235
Epoch 620, training loss: 0.08213923126459122 = 0.012044255621731281 + 0.01 * 7.009497165679932
Epoch 620, val loss: 1.1580344438552856
Epoch 630, training loss: 0.08146777749061584 = 0.01135196816176176 + 0.01 * 7.0115814208984375
Epoch 630, val loss: 1.1686351299285889
Epoch 640, training loss: 0.08073151111602783 = 0.01072037871927023 + 0.01 * 7.001113414764404
Epoch 640, val loss: 1.1789302825927734
Epoch 650, training loss: 0.0802595466375351 = 0.01014365628361702 + 0.01 * 7.0115885734558105
Epoch 650, val loss: 1.1889296770095825
Epoch 660, training loss: 0.07943350076675415 = 0.009615429677069187 + 0.01 * 6.981807231903076
Epoch 660, val loss: 1.1984453201293945
Epoch 670, training loss: 0.07887695729732513 = 0.009130299091339111 + 0.01 * 6.974665641784668
Epoch 670, val loss: 1.2076365947723389
Epoch 680, training loss: 0.0782838761806488 = 0.008683502674102783 + 0.01 * 6.9600372314453125
Epoch 680, val loss: 1.216604471206665
Epoch 690, training loss: 0.07813974469900131 = 0.008271452970802784 + 0.01 * 6.9868292808532715
Epoch 690, val loss: 1.2253462076187134
Epoch 700, training loss: 0.077495276927948 = 0.007891752757132053 + 0.01 * 6.960352420806885
Epoch 700, val loss: 1.2335798740386963
Epoch 710, training loss: 0.07718411087989807 = 0.00754009373486042 + 0.01 * 6.964402198791504
Epoch 710, val loss: 1.2416582107543945
Epoch 720, training loss: 0.07662550359964371 = 0.007213910110294819 + 0.01 * 6.941159248352051
Epoch 720, val loss: 1.2494744062423706
Epoch 730, training loss: 0.07624126225709915 = 0.006910079624503851 + 0.01 * 6.93311882019043
Epoch 730, val loss: 1.2569899559020996
Epoch 740, training loss: 0.0760161429643631 = 0.006627044640481472 + 0.01 * 6.938910007476807
Epoch 740, val loss: 1.2644717693328857
Epoch 750, training loss: 0.07578477263450623 = 0.006363458000123501 + 0.01 * 6.942131519317627
Epoch 750, val loss: 1.271513819694519
Epoch 760, training loss: 0.07554155588150024 = 0.006117198150604963 + 0.01 * 6.942436218261719
Epoch 760, val loss: 1.2784233093261719
Epoch 770, training loss: 0.0749991238117218 = 0.005886775441467762 + 0.01 * 6.9112348556518555
Epoch 770, val loss: 1.2850985527038574
Epoch 780, training loss: 0.07491974532604218 = 0.005670584738254547 + 0.01 * 6.9249162673950195
Epoch 780, val loss: 1.291640281677246
Epoch 790, training loss: 0.07437626272439957 = 0.005467979237437248 + 0.01 * 6.8908281326293945
Epoch 790, val loss: 1.2979973554611206
Epoch 800, training loss: 0.0742781013250351 = 0.005277818068861961 + 0.01 * 6.900028228759766
Epoch 800, val loss: 1.3041331768035889
Epoch 810, training loss: 0.07388577610254288 = 0.005098596680909395 + 0.01 * 6.878718376159668
Epoch 810, val loss: 1.3100178241729736
Epoch 820, training loss: 0.07397586107254028 = 0.0049294717609882355 + 0.01 * 6.904638767242432
Epoch 820, val loss: 1.3160303831100464
Epoch 830, training loss: 0.07379097491502762 = 0.004770292900502682 + 0.01 * 6.902068138122559
Epoch 830, val loss: 1.321627140045166
Epoch 840, training loss: 0.07332346588373184 = 0.004619972314685583 + 0.01 * 6.870349407196045
Epoch 840, val loss: 1.3270394802093506
Epoch 850, training loss: 0.0731993019580841 = 0.0044779288582503796 + 0.01 * 6.872138023376465
Epoch 850, val loss: 1.3325163125991821
Epoch 860, training loss: 0.07309041917324066 = 0.004343640990555286 + 0.01 * 6.874677658081055
Epoch 860, val loss: 1.3376708030700684
Epoch 870, training loss: 0.07274603843688965 = 0.004215847235172987 + 0.01 * 6.8530192375183105
Epoch 870, val loss: 1.342777967453003
Epoch 880, training loss: 0.07264694571495056 = 0.004094692878425121 + 0.01 * 6.855226039886475
Epoch 880, val loss: 1.3478416204452515
Epoch 890, training loss: 0.07246191799640656 = 0.003979726228863001 + 0.01 * 6.84821891784668
Epoch 890, val loss: 1.3526870012283325
Epoch 900, training loss: 0.07239247858524323 = 0.003870234591886401 + 0.01 * 6.852224349975586
Epoch 900, val loss: 1.3573815822601318
Epoch 910, training loss: 0.07218097150325775 = 0.0037662675604224205 + 0.01 * 6.841470718383789
Epoch 910, val loss: 1.362115740776062
Epoch 920, training loss: 0.07209543138742447 = 0.003667110111564398 + 0.01 * 6.842832565307617
Epoch 920, val loss: 1.3665653467178345
Epoch 930, training loss: 0.07191742956638336 = 0.003572719404473901 + 0.01 * 6.834471702575684
Epoch 930, val loss: 1.3710922002792358
Epoch 940, training loss: 0.07180473953485489 = 0.003482712432742119 + 0.01 * 6.832202911376953
Epoch 940, val loss: 1.375342607498169
Epoch 950, training loss: 0.07170511782169342 = 0.0033968810457736254 + 0.01 * 6.83082389831543
Epoch 950, val loss: 1.3796976804733276
Epoch 960, training loss: 0.071759432554245 = 0.0033149144146591425 + 0.01 * 6.844452381134033
Epoch 960, val loss: 1.3836627006530762
Epoch 970, training loss: 0.07141319662332535 = 0.003236643271520734 + 0.01 * 6.817655086517334
Epoch 970, val loss: 1.387755036354065
Epoch 980, training loss: 0.07127318531274796 = 0.0031615777406841516 + 0.01 * 6.811161041259766
Epoch 980, val loss: 1.3917042016983032
Epoch 990, training loss: 0.071436308324337 = 0.003089686157181859 + 0.01 * 6.834662437438965
Epoch 990, val loss: 1.3954596519470215
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8386926726410122
The final CL Acc:0.79877, 0.02124, The final GNN Acc:0.83729, 0.00317
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10486])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0337233543395996 = 1.947754979133606 + 0.01 * 8.596829414367676
Epoch 0, val loss: 1.9464129209518433
Epoch 10, training loss: 2.0236949920654297 = 1.9377269744873047 + 0.01 * 8.596799850463867
Epoch 10, val loss: 1.9365166425704956
Epoch 20, training loss: 2.011491060256958 = 1.9255244731903076 + 0.01 * 8.596662521362305
Epoch 20, val loss: 1.924055576324463
Epoch 30, training loss: 1.994481086730957 = 1.9085185527801514 + 0.01 * 8.596258163452148
Epoch 30, val loss: 1.9064199924468994
Epoch 40, training loss: 1.9697484970092773 = 1.8838039636611938 + 0.01 * 8.59444808959961
Epoch 40, val loss: 1.8808757066726685
Epoch 50, training loss: 1.935360074043274 = 1.8495322465896606 + 0.01 * 8.582779884338379
Epoch 50, val loss: 1.8468772172927856
Epoch 60, training loss: 1.8964725732803345 = 1.8111810684204102 + 0.01 * 8.52914810180664
Epoch 60, val loss: 1.8130863904953003
Epoch 70, training loss: 1.8639436960220337 = 1.7802438735961914 + 0.01 * 8.369986534118652
Epoch 70, val loss: 1.7898679971694946
Epoch 80, training loss: 1.8278695344924927 = 1.7455350160598755 + 0.01 * 8.233454704284668
Epoch 80, val loss: 1.7610161304473877
Epoch 90, training loss: 1.7755038738250732 = 1.6963483095169067 + 0.01 * 7.915555477142334
Epoch 90, val loss: 1.7192937135696411
Epoch 100, training loss: 1.7047616243362427 = 1.6281276941299438 + 0.01 * 7.663398742675781
Epoch 100, val loss: 1.6621856689453125
Epoch 110, training loss: 1.6148790121078491 = 1.5394498109817505 + 0.01 * 7.54292106628418
Epoch 110, val loss: 1.5898808240890503
Epoch 120, training loss: 1.5113362073898315 = 1.4368352890014648 + 0.01 * 7.450091361999512
Epoch 120, val loss: 1.5070364475250244
Epoch 130, training loss: 1.4020860195159912 = 1.3282493352890015 + 0.01 * 7.383673667907715
Epoch 130, val loss: 1.4204922914505005
Epoch 140, training loss: 1.293083667755127 = 1.2195849418640137 + 0.01 * 7.349870204925537
Epoch 140, val loss: 1.3353734016418457
Epoch 150, training loss: 1.1892832517623901 = 1.11604642868042 + 0.01 * 7.3236870765686035
Epoch 150, val loss: 1.2569152116775513
Epoch 160, training loss: 1.0935375690460205 = 1.0206021070480347 + 0.01 * 7.29355001449585
Epoch 160, val loss: 1.187372088432312
Epoch 170, training loss: 1.005942702293396 = 0.9332796335220337 + 0.01 * 7.266305923461914
Epoch 170, val loss: 1.1261478662490845
Epoch 180, training loss: 0.9250583052635193 = 0.8525304794311523 + 0.01 * 7.2527852058410645
Epoch 180, val loss: 1.0712443590164185
Epoch 190, training loss: 0.8499628305435181 = 0.7775281667709351 + 0.01 * 7.243464469909668
Epoch 190, val loss: 1.0218309164047241
Epoch 200, training loss: 0.7812451720237732 = 0.7088625431060791 + 0.01 * 7.238265037536621
Epoch 200, val loss: 0.9787114858627319
Epoch 210, training loss: 0.7197168469429016 = 0.6473619341850281 + 0.01 * 7.2354936599731445
Epoch 210, val loss: 0.9434213042259216
Epoch 220, training loss: 0.6651940941810608 = 0.5928570628166199 + 0.01 * 7.233702659606934
Epoch 220, val loss: 0.9164470434188843
Epoch 230, training loss: 0.6165725588798523 = 0.5442452430725098 + 0.01 * 7.232729911804199
Epoch 230, val loss: 0.8968952298164368
Epoch 240, training loss: 0.5722434520721436 = 0.49992233514785767 + 0.01 * 7.232113838195801
Epoch 240, val loss: 0.8830040693283081
Epoch 250, training loss: 0.5306229591369629 = 0.4583060145378113 + 0.01 * 7.2316975593566895
Epoch 250, val loss: 0.8732003569602966
Epoch 260, training loss: 0.49067723751068115 = 0.41836369037628174 + 0.01 * 7.231354236602783
Epoch 260, val loss: 0.8661683201789856
Epoch 270, training loss: 0.45212846994400024 = 0.37980949878692627 + 0.01 * 7.231895446777344
Epoch 270, val loss: 0.8610097169876099
Epoch 280, training loss: 0.41525012254714966 = 0.342935174703598 + 0.01 * 7.2314958572387695
Epoch 280, val loss: 0.8572507500648499
Epoch 290, training loss: 0.3805050551891327 = 0.30818411707878113 + 0.01 * 7.2320942878723145
Epoch 290, val loss: 0.8547760844230652
Epoch 300, training loss: 0.34811076521873474 = 0.27578309178352356 + 0.01 * 7.232766628265381
Epoch 300, val loss: 0.8535045981407166
Epoch 310, training loss: 0.317889541387558 = 0.24555450677871704 + 0.01 * 7.233503818511963
Epoch 310, val loss: 0.8537321090698242
Epoch 320, training loss: 0.28965556621551514 = 0.21731537580490112 + 0.01 * 7.234017848968506
Epoch 320, val loss: 0.8560481667518616
Epoch 330, training loss: 0.2635187804698944 = 0.19117137789726257 + 0.01 * 7.234739780426025
Epoch 330, val loss: 0.86122065782547
Epoch 340, training loss: 0.23980054259300232 = 0.16745193302631378 + 0.01 * 7.234861850738525
Epoch 340, val loss: 0.8698021173477173
Epoch 350, training loss: 0.218776673078537 = 0.14642789959907532 + 0.01 * 7.23487663269043
Epoch 350, val loss: 0.8817066550254822
Epoch 360, training loss: 0.20045626163482666 = 0.1281106173992157 + 0.01 * 7.234565258026123
Epoch 360, val loss: 0.8965036273002625
Epoch 370, training loss: 0.18463274836540222 = 0.11228635162115097 + 0.01 * 7.234639644622803
Epoch 370, val loss: 0.913556694984436
Epoch 380, training loss: 0.17100821435451508 = 0.09867511689662933 + 0.01 * 7.233309745788574
Epoch 380, val loss: 0.9323095679283142
Epoch 390, training loss: 0.15930110216140747 = 0.08697877824306488 + 0.01 * 7.232232093811035
Epoch 390, val loss: 0.9522048234939575
Epoch 400, training loss: 0.14924326539039612 = 0.07693631947040558 + 0.01 * 7.23069429397583
Epoch 400, val loss: 0.9728802442550659
Epoch 410, training loss: 0.1406010091304779 = 0.06830719113349915 + 0.01 * 7.229382514953613
Epoch 410, val loss: 0.9939612746238708
Epoch 420, training loss: 0.13315224647521973 = 0.06087973341345787 + 0.01 * 7.227252006530762
Epoch 420, val loss: 1.0151716470718384
Epoch 430, training loss: 0.12671208381652832 = 0.05446833372116089 + 0.01 * 7.224374771118164
Epoch 430, val loss: 1.0362519025802612
Epoch 440, training loss: 0.12114754319190979 = 0.04891921207308769 + 0.01 * 7.222832679748535
Epoch 440, val loss: 1.0570566654205322
Epoch 450, training loss: 0.11630403995513916 = 0.04410076141357422 + 0.01 * 7.220327854156494
Epoch 450, val loss: 1.0774455070495605
Epoch 460, training loss: 0.11205331981182098 = 0.039900802075862885 + 0.01 * 7.215251922607422
Epoch 460, val loss: 1.0973912477493286
Epoch 470, training loss: 0.10832656174898148 = 0.036226965487003326 + 0.01 * 7.209959506988525
Epoch 470, val loss: 1.1168255805969238
Epoch 480, training loss: 0.10504371672868729 = 0.03300224244594574 + 0.01 * 7.2041473388671875
Epoch 480, val loss: 1.1357372999191284
Epoch 490, training loss: 0.1025117039680481 = 0.030161302536725998 + 0.01 * 7.235039710998535
Epoch 490, val loss: 1.1541563272476196
Epoch 500, training loss: 0.09963720291852951 = 0.027650700882077217 + 0.01 * 7.19865083694458
Epoch 500, val loss: 1.1719285249710083
Epoch 510, training loss: 0.09731100499629974 = 0.025420652702450752 + 0.01 * 7.189035892486572
Epoch 510, val loss: 1.1892025470733643
Epoch 520, training loss: 0.09522270411252975 = 0.023429444059729576 + 0.01 * 7.179326057434082
Epoch 520, val loss: 1.2059868574142456
Epoch 530, training loss: 0.0933762937784195 = 0.02164503186941147 + 0.01 * 7.173126697540283
Epoch 530, val loss: 1.2223488092422485
Epoch 540, training loss: 0.09178688377141953 = 0.020045138895511627 + 0.01 * 7.1741743087768555
Epoch 540, val loss: 1.2381778955459595
Epoch 550, training loss: 0.09012673795223236 = 0.018608633428812027 + 0.01 * 7.151810646057129
Epoch 550, val loss: 1.2535070180892944
Epoch 560, training loss: 0.08877252787351608 = 0.017315901815891266 + 0.01 * 7.145662784576416
Epoch 560, val loss: 1.2684603929519653
Epoch 570, training loss: 0.08743917942047119 = 0.0161509420722723 + 0.01 * 7.128823757171631
Epoch 570, val loss: 1.282901406288147
Epoch 580, training loss: 0.0864773839712143 = 0.01510068029165268 + 0.01 * 7.137670516967773
Epoch 580, val loss: 1.2969681024551392
Epoch 590, training loss: 0.08540881425142288 = 0.014151276089251041 + 0.01 * 7.125754356384277
Epoch 590, val loss: 1.3104939460754395
Epoch 600, training loss: 0.08441178500652313 = 0.013291265815496445 + 0.01 * 7.112051963806152
Epoch 600, val loss: 1.3236994743347168
Epoch 610, training loss: 0.08346228301525116 = 0.012510107830166817 + 0.01 * 7.095217704772949
Epoch 610, val loss: 1.3364181518554688
Epoch 620, training loss: 0.08271521329879761 = 0.011798543855547905 + 0.01 * 7.0916666984558105
Epoch 620, val loss: 1.3488143682479858
Epoch 630, training loss: 0.08196728676557541 = 0.01114837545901537 + 0.01 * 7.081891059875488
Epoch 630, val loss: 1.3607923984527588
Epoch 640, training loss: 0.0813077837228775 = 0.010552098974585533 + 0.01 * 7.075568199157715
Epoch 640, val loss: 1.3725354671478271
Epoch 650, training loss: 0.08073165267705917 = 0.010002648457884789 + 0.01 * 7.072900295257568
Epoch 650, val loss: 1.38400137424469
Epoch 660, training loss: 0.08005581051111221 = 0.009494584985077381 + 0.01 * 7.056122779846191
Epoch 660, val loss: 1.395365595817566
Epoch 670, training loss: 0.07949915528297424 = 0.009023786522448063 + 0.01 * 7.047536849975586
Epoch 670, val loss: 1.4064215421676636
Epoch 680, training loss: 0.07895966619253159 = 0.00858727004379034 + 0.01 * 7.0372395515441895
Epoch 680, val loss: 1.417370080947876
Epoch 690, training loss: 0.07847113907337189 = 0.00818084366619587 + 0.01 * 7.029029846191406
Epoch 690, val loss: 1.4281095266342163
Epoch 700, training loss: 0.0781107023358345 = 0.0078026787377893925 + 0.01 * 7.0308027267456055
Epoch 700, val loss: 1.4386553764343262
Epoch 710, training loss: 0.07764468342065811 = 0.007451944053173065 + 0.01 * 7.0192742347717285
Epoch 710, val loss: 1.4489179849624634
Epoch 720, training loss: 0.07726185768842697 = 0.007125741336494684 + 0.01 * 7.013611316680908
Epoch 720, val loss: 1.458937406539917
Epoch 730, training loss: 0.07687228173017502 = 0.00682230107486248 + 0.01 * 7.004998207092285
Epoch 730, val loss: 1.468706727027893
Epoch 740, training loss: 0.07655373960733414 = 0.006538729183375835 + 0.01 * 7.001501560211182
Epoch 740, val loss: 1.4782543182373047
Epoch 750, training loss: 0.07632830739021301 = 0.0062739052809774876 + 0.01 * 7.005440711975098
Epoch 750, val loss: 1.4876115322113037
Epoch 760, training loss: 0.07599986344575882 = 0.0060262614861130714 + 0.01 * 6.9973602294921875
Epoch 760, val loss: 1.4967204332351685
Epoch 770, training loss: 0.07585721462965012 = 0.0057945866137743 + 0.01 * 7.006263256072998
Epoch 770, val loss: 1.5055792331695557
Epoch 780, training loss: 0.07546061277389526 = 0.005577861797064543 + 0.01 * 6.988275527954102
Epoch 780, val loss: 1.514146327972412
Epoch 790, training loss: 0.07517679035663605 = 0.005373998545110226 + 0.01 * 6.980279445648193
Epoch 790, val loss: 1.5226022005081177
Epoch 800, training loss: 0.07547931373119354 = 0.0051824371330440044 + 0.01 * 7.029687881469727
Epoch 800, val loss: 1.53086256980896
Epoch 810, training loss: 0.07461132854223251 = 0.0050034793093800545 + 0.01 * 6.960784435272217
Epoch 810, val loss: 1.5386757850646973
Epoch 820, training loss: 0.07448596507310867 = 0.004834554623812437 + 0.01 * 6.9651408195495605
Epoch 820, val loss: 1.5463520288467407
Epoch 830, training loss: 0.07432501018047333 = 0.004674687050282955 + 0.01 * 6.965032577514648
Epoch 830, val loss: 1.5539820194244385
Epoch 840, training loss: 0.07397232949733734 = 0.00452461326494813 + 0.01 * 6.944772243499756
Epoch 840, val loss: 1.5613771677017212
Epoch 850, training loss: 0.07397840172052383 = 0.004382637795060873 + 0.01 * 6.959576606750488
Epoch 850, val loss: 1.5685924291610718
Epoch 860, training loss: 0.07368997484445572 = 0.004248843528330326 + 0.01 * 6.944113731384277
Epoch 860, val loss: 1.5755802392959595
Epoch 870, training loss: 0.07341400533914566 = 0.004122130107134581 + 0.01 * 6.929187774658203
Epoch 870, val loss: 1.5823413133621216
Epoch 880, training loss: 0.07329881191253662 = 0.004001924302428961 + 0.01 * 6.929688930511475
Epoch 880, val loss: 1.5890135765075684
Epoch 890, training loss: 0.07326894253492355 = 0.0038882032968103886 + 0.01 * 6.938074111938477
Epoch 890, val loss: 1.5955325365066528
Epoch 900, training loss: 0.07305334508419037 = 0.003779970109462738 + 0.01 * 6.927337646484375
Epoch 900, val loss: 1.6018239259719849
Epoch 910, training loss: 0.07290329784154892 = 0.0036774296313524246 + 0.01 * 6.9225873947143555
Epoch 910, val loss: 1.6080580949783325
Epoch 920, training loss: 0.07289403676986694 = 0.0035798815079033375 + 0.01 * 6.931415557861328
Epoch 920, val loss: 1.6141140460968018
Epoch 930, training loss: 0.0725323036313057 = 0.0034871201496571302 + 0.01 * 6.904519081115723
Epoch 930, val loss: 1.6199744939804077
Epoch 940, training loss: 0.07250531017780304 = 0.0033988570794463158 + 0.01 * 6.910645008087158
Epoch 940, val loss: 1.62574303150177
Epoch 950, training loss: 0.07233985513448715 = 0.003314678091555834 + 0.01 * 6.902517318725586
Epoch 950, val loss: 1.6314287185668945
Epoch 960, training loss: 0.07216940820217133 = 0.0032342916820198298 + 0.01 * 6.893511772155762
Epoch 960, val loss: 1.6368002891540527
Epoch 970, training loss: 0.07206768542528152 = 0.0031577867921441793 + 0.01 * 6.890989780426025
Epoch 970, val loss: 1.6422169208526611
Epoch 980, training loss: 0.0719616487622261 = 0.0030844127759337425 + 0.01 * 6.887723445892334
Epoch 980, val loss: 1.6474392414093018
Epoch 990, training loss: 0.07175632566213608 = 0.0030144436750561 + 0.01 * 6.87418794631958
Epoch 990, val loss: 1.652607798576355
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 2.0263376235961914 = 1.9403692483901978 + 0.01 * 8.596835136413574
Epoch 0, val loss: 1.9408060312271118
Epoch 10, training loss: 2.016843557357788 = 1.9308758974075317 + 0.01 * 8.596773147583008
Epoch 10, val loss: 1.9317642450332642
Epoch 20, training loss: 2.004828453063965 = 1.9188627004623413 + 0.01 * 8.596567153930664
Epoch 20, val loss: 1.919978380203247
Epoch 30, training loss: 1.9877119064331055 = 1.901753306388855 + 0.01 * 8.595863342285156
Epoch 30, val loss: 1.9030009508132935
Epoch 40, training loss: 1.9623483419418335 = 1.8764302730560303 + 0.01 * 8.591802597045898
Epoch 40, val loss: 1.878100037574768
Epoch 50, training loss: 1.9273704290390015 = 1.841713786125183 + 0.01 * 8.56566333770752
Epoch 50, val loss: 1.845634937286377
Epoch 60, training loss: 1.889358401298523 = 1.8048549890518188 + 0.01 * 8.450336456298828
Epoch 60, val loss: 1.8145915269851685
Epoch 70, training loss: 1.856271505355835 = 1.7731966972351074 + 0.01 * 8.307476043701172
Epoch 70, val loss: 1.787858009338379
Epoch 80, training loss: 1.8133280277252197 = 1.7312073707580566 + 0.01 * 8.212065696716309
Epoch 80, val loss: 1.7494701147079468
Epoch 90, training loss: 1.7528393268585205 = 1.6716328859329224 + 0.01 * 8.120649337768555
Epoch 90, val loss: 1.6980609893798828
Epoch 100, training loss: 1.6715562343597412 = 1.5908950567245483 + 0.01 * 8.066113471984863
Epoch 100, val loss: 1.6319199800491333
Epoch 110, training loss: 1.5790907144546509 = 1.499125361442566 + 0.01 * 7.996533393859863
Epoch 110, val loss: 1.5582112073898315
Epoch 120, training loss: 1.4863649606704712 = 1.4087530374526978 + 0.01 * 7.761196613311768
Epoch 120, val loss: 1.488097071647644
Epoch 130, training loss: 1.398999571800232 = 1.3229892253875732 + 0.01 * 7.601031303405762
Epoch 130, val loss: 1.423903226852417
Epoch 140, training loss: 1.316422700881958 = 1.2412902116775513 + 0.01 * 7.513254165649414
Epoch 140, val loss: 1.3663641214370728
Epoch 150, training loss: 1.2376830577850342 = 1.1630892753601074 + 0.01 * 7.459374904632568
Epoch 150, val loss: 1.3146787881851196
Epoch 160, training loss: 1.162850260734558 = 1.0884958505630493 + 0.01 * 7.4354400634765625
Epoch 160, val loss: 1.2668074369430542
Epoch 170, training loss: 1.091920018196106 = 1.0177301168441772 + 0.01 * 7.418994903564453
Epoch 170, val loss: 1.2222131490707397
Epoch 180, training loss: 1.0246987342834473 = 0.9507044553756714 + 0.01 * 7.399422645568848
Epoch 180, val loss: 1.180122971534729
Epoch 190, training loss: 0.9599747657775879 = 0.88624107837677 + 0.01 * 7.373367786407471
Epoch 190, val loss: 1.1387015581130981
Epoch 200, training loss: 0.8954570889472961 = 0.8220582604408264 + 0.01 * 7.33988094329834
Epoch 200, val loss: 1.096360445022583
Epoch 210, training loss: 0.8293144106864929 = 0.7562156915664673 + 0.01 * 7.309873104095459
Epoch 210, val loss: 1.0515542030334473
Epoch 220, training loss: 0.7613387703895569 = 0.6884863972663879 + 0.01 * 7.2852349281311035
Epoch 220, val loss: 1.0042400360107422
Epoch 230, training loss: 0.6930530667304993 = 0.6203271150588989 + 0.01 * 7.2725958824157715
Epoch 230, val loss: 0.9564512968063354
Epoch 240, training loss: 0.6262856125831604 = 0.5536325573921204 + 0.01 * 7.265307426452637
Epoch 240, val loss: 0.9107239842414856
Epoch 250, training loss: 0.5624598860740662 = 0.48985907435417175 + 0.01 * 7.260080814361572
Epoch 250, val loss: 0.8698998689651489
Epoch 260, training loss: 0.5025110840797424 = 0.42994481325149536 + 0.01 * 7.25662899017334
Epoch 260, val loss: 0.8357653021812439
Epoch 270, training loss: 0.4474562704563141 = 0.3748985528945923 + 0.01 * 7.255771160125732
Epoch 270, val loss: 0.8094543814659119
Epoch 280, training loss: 0.39792290329933167 = 0.32538214325904846 + 0.01 * 7.2540764808654785
Epoch 280, val loss: 0.7911278605461121
Epoch 290, training loss: 0.3538801968097687 = 0.28136298060417175 + 0.01 * 7.251720905303955
Epoch 290, val loss: 0.7799221277236938
Epoch 300, training loss: 0.31488341093063354 = 0.24238014221191406 + 0.01 * 7.250326633453369
Epoch 300, val loss: 0.7743517756462097
Epoch 310, training loss: 0.28057432174682617 = 0.20808832347393036 + 0.01 * 7.248598575592041
Epoch 310, val loss: 0.7729843258857727
Epoch 320, training loss: 0.2508544921875 = 0.1783571094274521 + 0.01 * 7.249736785888672
Epoch 320, val loss: 0.7748894095420837
Epoch 330, training loss: 0.2254253774881363 = 0.1529874950647354 + 0.01 * 7.243788719177246
Epoch 330, val loss: 0.7795546650886536
Epoch 340, training loss: 0.20400342345237732 = 0.1315905600786209 + 0.01 * 7.2412872314453125
Epoch 340, val loss: 0.7867971062660217
Epoch 350, training loss: 0.18602922558784485 = 0.11364174634218216 + 0.01 * 7.238749027252197
Epoch 350, val loss: 0.7963060736656189
Epoch 360, training loss: 0.17093902826309204 = 0.09859729558229446 + 0.01 * 7.234173774719238
Epoch 360, val loss: 0.8077852129936218
Epoch 370, training loss: 0.15825548768043518 = 0.08596063405275345 + 0.01 * 7.229486465454102
Epoch 370, val loss: 0.8209296464920044
Epoch 380, training loss: 0.14773723483085632 = 0.07531905919313431 + 0.01 * 7.241817951202393
Epoch 380, val loss: 0.8353292942047119
Epoch 390, training loss: 0.1385277509689331 = 0.06632837653160095 + 0.01 * 7.219938278198242
Epoch 390, val loss: 0.8506585955619812
Epoch 400, training loss: 0.1308252364397049 = 0.058702461421489716 + 0.01 * 7.212277412414551
Epoch 400, val loss: 0.8665598630905151
Epoch 410, training loss: 0.124320849776268 = 0.05220632627606392 + 0.01 * 7.211452960968018
Epoch 410, val loss: 0.8828324675559998
Epoch 420, training loss: 0.11863110959529877 = 0.04664747789502144 + 0.01 * 7.198363780975342
Epoch 420, val loss: 0.899255633354187
Epoch 430, training loss: 0.11377719044685364 = 0.04186685010790825 + 0.01 * 7.191033840179443
Epoch 430, val loss: 0.915665328502655
Epoch 440, training loss: 0.10965611785650253 = 0.03774198889732361 + 0.01 * 7.191413402557373
Epoch 440, val loss: 0.9319093227386475
Epoch 450, training loss: 0.10586924850940704 = 0.034167204052209854 + 0.01 * 7.1702046394348145
Epoch 450, val loss: 0.9478701949119568
Epoch 460, training loss: 0.10267351567745209 = 0.03105049766600132 + 0.01 * 7.162302494049072
Epoch 460, val loss: 0.9635864496231079
Epoch 470, training loss: 0.09979960322380066 = 0.028322424739599228 + 0.01 * 7.147717475891113
Epoch 470, val loss: 0.9789852499961853
Epoch 480, training loss: 0.09776958078145981 = 0.02592669427394867 + 0.01 * 7.18428897857666
Epoch 480, val loss: 0.9939689636230469
Epoch 490, training loss: 0.0951334536075592 = 0.02381829358637333 + 0.01 * 7.131516456604004
Epoch 490, val loss: 1.0085853338241577
Epoch 500, training loss: 0.0931219831109047 = 0.02195088379085064 + 0.01 * 7.117110252380371
Epoch 500, val loss: 1.0228185653686523
Epoch 510, training loss: 0.09129327535629272 = 0.02029191516339779 + 0.01 * 7.100135803222656
Epoch 510, val loss: 1.0366153717041016
Epoch 520, training loss: 0.08971769362688065 = 0.018811741843819618 + 0.01 * 7.090595245361328
Epoch 520, val loss: 1.0499986410140991
Epoch 530, training loss: 0.08838865906000137 = 0.01748546026647091 + 0.01 * 7.090320110321045
Epoch 530, val loss: 1.0628868341445923
Epoch 540, training loss: 0.0871192067861557 = 0.016297394409775734 + 0.01 * 7.082181930541992
Epoch 540, val loss: 1.075371265411377
Epoch 550, training loss: 0.08579680323600769 = 0.015224729664623737 + 0.01 * 7.0572075843811035
Epoch 550, val loss: 1.0875641107559204
Epoch 560, training loss: 0.0846840888261795 = 0.014253607951104641 + 0.01 * 7.043047904968262
Epoch 560, val loss: 1.0992497205734253
Epoch 570, training loss: 0.08372878283262253 = 0.013374624773859978 + 0.01 * 7.0354156494140625
Epoch 570, val loss: 1.1105809211730957
Epoch 580, training loss: 0.08297210931777954 = 0.012575057335197926 + 0.01 * 7.0397047996521
Epoch 580, val loss: 1.1216365098953247
Epoch 590, training loss: 0.08211038261651993 = 0.011845516040921211 + 0.01 * 7.026486873626709
Epoch 590, val loss: 1.1324424743652344
Epoch 600, training loss: 0.08149203658103943 = 0.011177695356309414 + 0.01 * 7.031434535980225
Epoch 600, val loss: 1.1428437232971191
Epoch 610, training loss: 0.08064654469490051 = 0.010566660203039646 + 0.01 * 7.007988929748535
Epoch 610, val loss: 1.1529425382614136
Epoch 620, training loss: 0.07995953410863876 = 0.010005941614508629 + 0.01 * 6.995359420776367
Epoch 620, val loss: 1.1627362966537476
Epoch 630, training loss: 0.0794435515999794 = 0.009489631280303001 + 0.01 * 6.995392322540283
Epoch 630, val loss: 1.1721891164779663
Epoch 640, training loss: 0.07876335084438324 = 0.00901365652680397 + 0.01 * 6.974969863891602
Epoch 640, val loss: 1.181538462638855
Epoch 650, training loss: 0.07821214944124222 = 0.008573547936975956 + 0.01 * 6.963860034942627
Epoch 650, val loss: 1.190653681755066
Epoch 660, training loss: 0.07767517119646072 = 0.0081654516980052 + 0.01 * 6.950972080230713
Epoch 660, val loss: 1.1993162631988525
Epoch 670, training loss: 0.07728035748004913 = 0.007787834852933884 + 0.01 * 6.949253082275391
Epoch 670, val loss: 1.2079209089279175
Epoch 680, training loss: 0.0768202617764473 = 0.007437305990606546 + 0.01 * 6.938296318054199
Epoch 680, val loss: 1.2162749767303467
Epoch 690, training loss: 0.07660131901502609 = 0.007111762650310993 + 0.01 * 6.948955535888672
Epoch 690, val loss: 1.2244012355804443
Epoch 700, training loss: 0.07607465982437134 = 0.006808743346482515 + 0.01 * 6.926591873168945
Epoch 700, val loss: 1.2322496175765991
Epoch 710, training loss: 0.07564972341060638 = 0.00652654841542244 + 0.01 * 6.912318229675293
Epoch 710, val loss: 1.2400281429290771
Epoch 720, training loss: 0.07539133727550507 = 0.006262457463890314 + 0.01 * 6.912888050079346
Epoch 720, val loss: 1.2474769353866577
Epoch 730, training loss: 0.07505719363689423 = 0.006015835329890251 + 0.01 * 6.904135704040527
Epoch 730, val loss: 1.2548776865005493
Epoch 740, training loss: 0.07499013096094131 = 0.005784385371953249 + 0.01 * 6.92057466506958
Epoch 740, val loss: 1.2620247602462769
Epoch 750, training loss: 0.07463537901639938 = 0.005567822139710188 + 0.01 * 6.906756401062012
Epoch 750, val loss: 1.2690163850784302
Epoch 760, training loss: 0.07425501942634583 = 0.0053641656413674355 + 0.01 * 6.88908576965332
Epoch 760, val loss: 1.2758283615112305
Epoch 770, training loss: 0.07438279688358307 = 0.005172587465494871 + 0.01 * 6.921020984649658
Epoch 770, val loss: 1.282523512840271
Epoch 780, training loss: 0.07390359044075012 = 0.004992630798369646 + 0.01 * 6.8910956382751465
Epoch 780, val loss: 1.2890379428863525
Epoch 790, training loss: 0.07377612590789795 = 0.004822707735002041 + 0.01 * 6.895341873168945
Epoch 790, val loss: 1.2954360246658325
Epoch 800, training loss: 0.0734633207321167 = 0.00466239545494318 + 0.01 * 6.880092620849609
Epoch 800, val loss: 1.3016899824142456
Epoch 810, training loss: 0.07347025722265244 = 0.004510730504989624 + 0.01 * 6.8959527015686035
Epoch 810, val loss: 1.3077731132507324
Epoch 820, training loss: 0.07327358424663544 = 0.0043675536289811134 + 0.01 * 6.890603065490723
Epoch 820, val loss: 1.313763976097107
Epoch 830, training loss: 0.07297109067440033 = 0.004232108127325773 + 0.01 * 6.873898506164551
Epoch 830, val loss: 1.3196066617965698
Epoch 840, training loss: 0.07263796031475067 = 0.0041037388145923615 + 0.01 * 6.853421688079834
Epoch 840, val loss: 1.3253430128097534
Epoch 850, training loss: 0.07245293259620667 = 0.003981696907430887 + 0.01 * 6.847123622894287
Epoch 850, val loss: 1.3309111595153809
Epoch 860, training loss: 0.07251542806625366 = 0.003866018960252404 + 0.01 * 6.864941596984863
Epoch 860, val loss: 1.336298942565918
Epoch 870, training loss: 0.07233551889657974 = 0.003756291698664427 + 0.01 * 6.8579230308532715
Epoch 870, val loss: 1.3417073488235474
Epoch 880, training loss: 0.07224388420581818 = 0.0036522396840155125 + 0.01 * 6.859165191650391
Epoch 880, val loss: 1.3469873666763306
Epoch 890, training loss: 0.07204113155603409 = 0.003552849870175123 + 0.01 * 6.848828315734863
Epoch 890, val loss: 1.3521305322647095
Epoch 900, training loss: 0.07178554683923721 = 0.0034581858199089766 + 0.01 * 6.832736492156982
Epoch 900, val loss: 1.357129454612732
Epoch 910, training loss: 0.07180435210466385 = 0.0033679588232189417 + 0.01 * 6.843639373779297
Epoch 910, val loss: 1.362035870552063
Epoch 920, training loss: 0.07157756388187408 = 0.00328184780664742 + 0.01 * 6.829571723937988
Epoch 920, val loss: 1.3669739961624146
Epoch 930, training loss: 0.07160604745149612 = 0.003199813887476921 + 0.01 * 6.840623378753662
Epoch 930, val loss: 1.3716496229171753
Epoch 940, training loss: 0.0713772401213646 = 0.003121409798040986 + 0.01 * 6.825582981109619
Epoch 940, val loss: 1.376325011253357
Epoch 950, training loss: 0.07120722532272339 = 0.003046355675905943 + 0.01 * 6.816086769104004
Epoch 950, val loss: 1.3808773756027222
Epoch 960, training loss: 0.0715896412730217 = 0.0029745863284915686 + 0.01 * 6.861505508422852
Epoch 960, val loss: 1.3853836059570312
Epoch 970, training loss: 0.07106325775384903 = 0.0029060875531286 + 0.01 * 6.8157172203063965
Epoch 970, val loss: 1.3897337913513184
Epoch 980, training loss: 0.07114648818969727 = 0.0028404423501342535 + 0.01 * 6.830604553222656
Epoch 980, val loss: 1.3940138816833496
Epoch 990, training loss: 0.07100628316402435 = 0.002777501242235303 + 0.01 * 6.822878360748291
Epoch 990, val loss: 1.3982703685760498
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8054823405376912
=== training gcn model ===
Epoch 0, training loss: 2.021397590637207 = 1.9354290962219238 + 0.01 * 8.596844673156738
Epoch 0, val loss: 1.9345895051956177
Epoch 10, training loss: 2.012068271636963 = 1.9261001348495483 + 0.01 * 8.596806526184082
Epoch 10, val loss: 1.9246419668197632
Epoch 20, training loss: 2.0006420612335205 = 1.9146755933761597 + 0.01 * 8.596654891967773
Epoch 20, val loss: 1.9121618270874023
Epoch 30, training loss: 1.9846631288528442 = 1.8987005949020386 + 0.01 * 8.596254348754883
Epoch 30, val loss: 1.8945380449295044
Epoch 40, training loss: 1.9612327814102173 = 1.8752881288528442 + 0.01 * 8.594466209411621
Epoch 40, val loss: 1.8690271377563477
Epoch 50, training loss: 1.929301142692566 = 1.843498945236206 + 0.01 * 8.580221176147461
Epoch 50, val loss: 1.8361546993255615
Epoch 60, training loss: 1.894907832145691 = 1.809983730316162 + 0.01 * 8.492409706115723
Epoch 60, val loss: 1.8059887886047363
Epoch 70, training loss: 1.8640762567520142 = 1.780822515487671 + 0.01 * 8.325376510620117
Epoch 70, val loss: 1.7839633226394653
Epoch 80, training loss: 1.8244681358337402 = 1.7431288957595825 + 0.01 * 8.13392162322998
Epoch 80, val loss: 1.7539961338043213
Epoch 90, training loss: 1.7680007219314575 = 1.6904125213623047 + 0.01 * 7.758814811706543
Epoch 90, val loss: 1.7097560167312622
Epoch 100, training loss: 1.6959079504013062 = 1.620568037033081 + 0.01 * 7.533993244171143
Epoch 100, val loss: 1.6507859230041504
Epoch 110, training loss: 1.6142362356185913 = 1.5397579669952393 + 0.01 * 7.447831153869629
Epoch 110, val loss: 1.5849692821502686
Epoch 120, training loss: 1.528814673423767 = 1.4549211263656616 + 0.01 * 7.389352798461914
Epoch 120, val loss: 1.5202299356460571
Epoch 130, training loss: 1.4413477182388306 = 1.367754578590393 + 0.01 * 7.359316349029541
Epoch 130, val loss: 1.4568232297897339
Epoch 140, training loss: 1.3489735126495361 = 1.275603175163269 + 0.01 * 7.337035179138184
Epoch 140, val loss: 1.3916139602661133
Epoch 150, training loss: 1.2500015497207642 = 1.1768262386322021 + 0.01 * 7.31753396987915
Epoch 150, val loss: 1.3210728168487549
Epoch 160, training loss: 1.145766019821167 = 1.0728057622909546 + 0.01 * 7.296025276184082
Epoch 160, val loss: 1.2462921142578125
Epoch 170, training loss: 1.0411319732666016 = 0.9683762192726135 + 0.01 * 7.275577068328857
Epoch 170, val loss: 1.172864556312561
Epoch 180, training loss: 0.943195641040802 = 0.8705704808235168 + 0.01 * 7.262518405914307
Epoch 180, val loss: 1.106559157371521
Epoch 190, training loss: 0.8572043180465698 = 0.7847132682800293 + 0.01 * 7.2491068840026855
Epoch 190, val loss: 1.0522875785827637
Epoch 200, training loss: 0.7843161821365356 = 0.7118403315544128 + 0.01 * 7.247586727142334
Epoch 200, val loss: 1.011141300201416
Epoch 210, training loss: 0.7218360304832458 = 0.6495233774185181 + 0.01 * 7.231265068054199
Epoch 210, val loss: 0.9810458421707153
Epoch 220, training loss: 0.6664185523986816 = 0.5942144989967346 + 0.01 * 7.220407485961914
Epoch 220, val loss: 0.9584340453147888
Epoch 230, training loss: 0.6149706840515137 = 0.5428651571273804 + 0.01 * 7.2105512619018555
Epoch 230, val loss: 0.9403197169303894
Epoch 240, training loss: 0.5655646920204163 = 0.49353399872779846 + 0.01 * 7.203067779541016
Epoch 240, val loss: 0.9249703884124756
Epoch 250, training loss: 0.5173736810684204 = 0.44545501470565796 + 0.01 * 7.191863536834717
Epoch 250, val loss: 0.9115372896194458
Epoch 260, training loss: 0.47062820196151733 = 0.39877912402153015 + 0.01 * 7.184909343719482
Epoch 260, val loss: 0.9002637267112732
Epoch 270, training loss: 0.4258699417114258 = 0.35412946343421936 + 0.01 * 7.174049377441406
Epoch 270, val loss: 0.8914308547973633
Epoch 280, training loss: 0.38380181789398193 = 0.3121671974658966 + 0.01 * 7.163460731506348
Epoch 280, val loss: 0.8853538632392883
Epoch 290, training loss: 0.34500980377197266 = 0.273387610912323 + 0.01 * 7.162217617034912
Epoch 290, val loss: 0.8823739290237427
Epoch 300, training loss: 0.3096091151237488 = 0.23812152445316315 + 0.01 * 7.148758411407471
Epoch 300, val loss: 0.882384717464447
Epoch 310, training loss: 0.2782782018184662 = 0.2065751701593399 + 0.01 * 7.170302391052246
Epoch 310, val loss: 0.8852927088737488
Epoch 320, training loss: 0.250335693359375 = 0.17883506417274475 + 0.01 * 7.150062084197998
Epoch 320, val loss: 0.8910381197929382
Epoch 330, training loss: 0.22608035802841187 = 0.1547316610813141 + 0.01 * 7.134869575500488
Epoch 330, val loss: 0.8994914889335632
Epoch 340, training loss: 0.20521482825279236 = 0.13396134972572327 + 0.01 * 7.125349044799805
Epoch 340, val loss: 0.9102550148963928
Epoch 350, training loss: 0.18741974234580994 = 0.11613799631595612 + 0.01 * 7.128174781799316
Epoch 350, val loss: 0.9228044748306274
Epoch 360, training loss: 0.17213456332683563 = 0.10086987167596817 + 0.01 * 7.126469612121582
Epoch 360, val loss: 0.9367524981498718
Epoch 370, training loss: 0.15888100862503052 = 0.0878092348575592 + 0.01 * 7.107177734375
Epoch 370, val loss: 0.9516737461090088
Epoch 380, training loss: 0.1477293223142624 = 0.07666848599910736 + 0.01 * 7.106083869934082
Epoch 380, val loss: 0.9671494364738464
Epoch 390, training loss: 0.1381245255470276 = 0.06719235330820084 + 0.01 * 7.093217372894287
Epoch 390, val loss: 0.9828200936317444
Epoch 400, training loss: 0.13010641932487488 = 0.059142109006643295 + 0.01 * 7.096430778503418
Epoch 400, val loss: 0.9985383152961731
Epoch 410, training loss: 0.1231757327914238 = 0.05231233686208725 + 0.01 * 7.086339950561523
Epoch 410, val loss: 1.0141727924346924
Epoch 420, training loss: 0.11747978627681732 = 0.04650185629725456 + 0.01 * 7.097793102264404
Epoch 420, val loss: 1.0295361280441284
Epoch 430, training loss: 0.11231380701065063 = 0.04155425354838371 + 0.01 * 7.075955390930176
Epoch 430, val loss: 1.0445826053619385
Epoch 440, training loss: 0.10795895755290985 = 0.03732149675488472 + 0.01 * 7.063745975494385
Epoch 440, val loss: 1.0592707395553589
Epoch 450, training loss: 0.1042536050081253 = 0.03368273377418518 + 0.01 * 7.057086944580078
Epoch 450, val loss: 1.0735539197921753
Epoch 460, training loss: 0.1011374443769455 = 0.03054291382431984 + 0.01 * 7.05945348739624
Epoch 460, val loss: 1.0873733758926392
Epoch 470, training loss: 0.098255455493927 = 0.027821222320199013 + 0.01 * 7.043423652648926
Epoch 470, val loss: 1.1008126735687256
Epoch 480, training loss: 0.09570246189832687 = 0.025449469685554504 + 0.01 * 7.025299072265625
Epoch 480, val loss: 1.1136940717697144
Epoch 490, training loss: 0.09352827817201614 = 0.023373527452349663 + 0.01 * 7.015474796295166
Epoch 490, val loss: 1.1261870861053467
Epoch 500, training loss: 0.09198502451181412 = 0.021545296534895897 + 0.01 * 7.043972492218018
Epoch 500, val loss: 1.138317584991455
Epoch 510, training loss: 0.09004294127225876 = 0.019931085407733917 + 0.01 * 7.011185646057129
Epoch 510, val loss: 1.1498687267303467
Epoch 520, training loss: 0.0885406956076622 = 0.018497886136174202 + 0.01 * 7.004281044006348
Epoch 520, val loss: 1.1611372232437134
Epoch 530, training loss: 0.08712751418352127 = 0.017219766974449158 + 0.01 * 6.990774631500244
Epoch 530, val loss: 1.1720378398895264
Epoch 540, training loss: 0.0858841985464096 = 0.016077164560556412 + 0.01 * 6.980703830718994
Epoch 540, val loss: 1.1824893951416016
Epoch 550, training loss: 0.08511301130056381 = 0.015050007030367851 + 0.01 * 7.006300449371338
Epoch 550, val loss: 1.1926900148391724
Epoch 560, training loss: 0.08403177559375763 = 0.014125294983386993 + 0.01 * 6.99064826965332
Epoch 560, val loss: 1.2023831605911255
Epoch 570, training loss: 0.08299046009778976 = 0.01328886765986681 + 0.01 * 6.97015905380249
Epoch 570, val loss: 1.2118675708770752
Epoch 580, training loss: 0.08208344876766205 = 0.012528943829238415 + 0.01 * 6.955451011657715
Epoch 580, val loss: 1.2210599184036255
Epoch 590, training loss: 0.0813971608877182 = 0.011836721561849117 + 0.01 * 6.956043720245361
Epoch 590, val loss: 1.2299631834030151
Epoch 600, training loss: 0.08069838583469391 = 0.011204760521650314 + 0.01 * 6.949362277984619
Epoch 600, val loss: 1.2385351657867432
Epoch 610, training loss: 0.08020545542240143 = 0.01062674168497324 + 0.01 * 6.957871913909912
Epoch 610, val loss: 1.2468293905258179
Epoch 620, training loss: 0.07954465597867966 = 0.010095399804413319 + 0.01 * 6.944925785064697
Epoch 620, val loss: 1.2549022436141968
Epoch 630, training loss: 0.07906946539878845 = 0.009606728330254555 + 0.01 * 6.9462738037109375
Epoch 630, val loss: 1.262743353843689
Epoch 640, training loss: 0.0784231498837471 = 0.009156216867268085 + 0.01 * 6.926692962646484
Epoch 640, val loss: 1.2703628540039062
Epoch 650, training loss: 0.07800610363483429 = 0.008739467710256577 + 0.01 * 6.926663875579834
Epoch 650, val loss: 1.2777583599090576
Epoch 660, training loss: 0.07763668149709702 = 0.008353524841368198 + 0.01 * 6.92831563949585
Epoch 660, val loss: 1.284963607788086
Epoch 670, training loss: 0.07710231840610504 = 0.007995433174073696 + 0.01 * 6.910688400268555
Epoch 670, val loss: 1.2920184135437012
Epoch 680, training loss: 0.07688522338867188 = 0.007662502583116293 + 0.01 * 6.922271728515625
Epoch 680, val loss: 1.2988898754119873
Epoch 690, training loss: 0.0765736997127533 = 0.007351988926529884 + 0.01 * 6.922171115875244
Epoch 690, val loss: 1.3055567741394043
Epoch 700, training loss: 0.07610920071601868 = 0.007062674034386873 + 0.01 * 6.904653072357178
Epoch 700, val loss: 1.3120006322860718
Epoch 710, training loss: 0.07578620314598083 = 0.006791647523641586 + 0.01 * 6.8994550704956055
Epoch 710, val loss: 1.3182928562164307
Epoch 720, training loss: 0.07583831250667572 = 0.006537892390042543 + 0.01 * 6.930042266845703
Epoch 720, val loss: 1.3244131803512573
Epoch 730, training loss: 0.07534712553024292 = 0.006300525274127722 + 0.01 * 6.904660224914551
Epoch 730, val loss: 1.330387830734253
Epoch 740, training loss: 0.07503668963909149 = 0.0060775126330554485 + 0.01 * 6.895917892456055
Epoch 740, val loss: 1.336148738861084
Epoch 750, training loss: 0.07487122714519501 = 0.0058673652820289135 + 0.01 * 6.900386333465576
Epoch 750, val loss: 1.3418817520141602
Epoch 760, training loss: 0.0744665265083313 = 0.005669937934726477 + 0.01 * 6.879659175872803
Epoch 760, val loss: 1.3473759889602661
Epoch 770, training loss: 0.07444774359464645 = 0.005483462009578943 + 0.01 * 6.896428108215332
Epoch 770, val loss: 1.3528070449829102
Epoch 780, training loss: 0.07404988259077072 = 0.005307565443217754 + 0.01 * 6.874232292175293
Epoch 780, val loss: 1.3580354452133179
Epoch 790, training loss: 0.07391174137592316 = 0.00514127966016531 + 0.01 * 6.87704610824585
Epoch 790, val loss: 1.3632385730743408
Epoch 800, training loss: 0.07371270656585693 = 0.004984087776392698 + 0.01 * 6.872861862182617
Epoch 800, val loss: 1.368191123008728
Epoch 810, training loss: 0.07339131087064743 = 0.004835009574890137 + 0.01 * 6.855630397796631
Epoch 810, val loss: 1.373176097869873
Epoch 820, training loss: 0.07341167330741882 = 0.004693690687417984 + 0.01 * 6.871798038482666
Epoch 820, val loss: 1.3780211210250854
Epoch 830, training loss: 0.07315899431705475 = 0.004559605848044157 + 0.01 * 6.859939098358154
Epoch 830, val loss: 1.3826580047607422
Epoch 840, training loss: 0.07288820296525955 = 0.0044322023168206215 + 0.01 * 6.845600605010986
Epoch 840, val loss: 1.3872843980789185
Epoch 850, training loss: 0.07273684442043304 = 0.004310916643589735 + 0.01 * 6.842593193054199
Epoch 850, val loss: 1.3917611837387085
Epoch 860, training loss: 0.07261446118354797 = 0.004195771645754576 + 0.01 * 6.841868877410889
Epoch 860, val loss: 1.396170735359192
Epoch 870, training loss: 0.0726126953959465 = 0.004085817839950323 + 0.01 * 6.852688312530518
Epoch 870, val loss: 1.4004642963409424
Epoch 880, training loss: 0.07257667183876038 = 0.003981304820626974 + 0.01 * 6.859536647796631
Epoch 880, val loss: 1.4046859741210938
Epoch 890, training loss: 0.072255939245224 = 0.003881236305460334 + 0.01 * 6.837470054626465
Epoch 890, val loss: 1.4088294506072998
Epoch 900, training loss: 0.07228043675422668 = 0.0037858367431908846 + 0.01 * 6.849460601806641
Epoch 900, val loss: 1.4128481149673462
Epoch 910, training loss: 0.07209741324186325 = 0.0036946400068700314 + 0.01 * 6.840277671813965
Epoch 910, val loss: 1.4167066812515259
Epoch 920, training loss: 0.07178139686584473 = 0.0036074272356927395 + 0.01 * 6.817397117614746
Epoch 920, val loss: 1.4206204414367676
Epoch 930, training loss: 0.07187709212303162 = 0.003523849416524172 + 0.01 * 6.835324287414551
Epoch 930, val loss: 1.4243611097335815
Epoch 940, training loss: 0.07154537737369537 = 0.003444116562604904 + 0.01 * 6.810126781463623
Epoch 940, val loss: 1.428002119064331
Epoch 950, training loss: 0.07165372371673584 = 0.003367507364600897 + 0.01 * 6.828621864318848
Epoch 950, val loss: 1.4315377473831177
Epoch 960, training loss: 0.07165264338254929 = 0.0032939461525529623 + 0.01 * 6.835869789123535
Epoch 960, val loss: 1.4349701404571533
Epoch 970, training loss: 0.07130506634712219 = 0.0032235272228717804 + 0.01 * 6.8081536293029785
Epoch 970, val loss: 1.4383258819580078
Epoch 980, training loss: 0.07129622250795364 = 0.0031557593028992414 + 0.01 * 6.814046382904053
Epoch 980, val loss: 1.4416675567626953
Epoch 990, training loss: 0.07121950387954712 = 0.0030907136388123035 + 0.01 * 6.8128790855407715
Epoch 990, val loss: 1.4448728561401367
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8191881918819188
The final CL Acc:0.75556, 0.00800, The final GNN Acc:0.81269, 0.00562
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13220])
remove edge: torch.Size([2, 7910])
updated graph: torch.Size([2, 10574])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0432136058807373 = 1.9572453498840332 + 0.01 * 8.596827507019043
Epoch 0, val loss: 1.9599028825759888
Epoch 10, training loss: 2.0337302684783936 = 1.9477624893188477 + 0.01 * 8.596784591674805
Epoch 10, val loss: 1.9499375820159912
Epoch 20, training loss: 2.022592067718506 = 1.9366260766983032 + 0.01 * 8.59660530090332
Epoch 20, val loss: 1.9380109310150146
Epoch 30, training loss: 2.0075814723968506 = 1.9216209650039673 + 0.01 * 8.59605598449707
Epoch 30, val loss: 1.9219369888305664
Epoch 40, training loss: 1.9858092069625854 = 1.8998768329620361 + 0.01 * 8.593241691589355
Epoch 40, val loss: 1.898850679397583
Epoch 50, training loss: 1.953855037689209 = 1.8681566715240479 + 0.01 * 8.569833755493164
Epoch 50, val loss: 1.865934133529663
Epoch 60, training loss: 1.9104602336883545 = 1.8263392448425293 + 0.01 * 8.41209888458252
Epoch 60, val loss: 1.8250110149383545
Epoch 70, training loss: 1.8646913766860962 = 1.7826430797576904 + 0.01 * 8.204833984375
Epoch 70, val loss: 1.7867224216461182
Epoch 80, training loss: 1.8199177980422974 = 1.740584135055542 + 0.01 * 7.933364391326904
Epoch 80, val loss: 1.751140832901001
Epoch 90, training loss: 1.7618279457092285 = 1.6859105825424194 + 0.01 * 7.591740608215332
Epoch 90, val loss: 1.702359914779663
Epoch 100, training loss: 1.6849749088287354 = 1.6108512878417969 + 0.01 * 7.41235876083374
Epoch 100, val loss: 1.6360629796981812
Epoch 110, training loss: 1.5895417928695679 = 1.5163323879241943 + 0.01 * 7.320944309234619
Epoch 110, val loss: 1.5546607971191406
Epoch 120, training loss: 1.4831228256225586 = 1.4103952646255493 + 0.01 * 7.272756576538086
Epoch 120, val loss: 1.465956687927246
Epoch 130, training loss: 1.3761343955993652 = 1.3037958145141602 + 0.01 * 7.233857154846191
Epoch 130, val loss: 1.3797240257263184
Epoch 140, training loss: 1.2737880945205688 = 1.201742172241211 + 0.01 * 7.20458984375
Epoch 140, val loss: 1.2987161874771118
Epoch 150, training loss: 1.179307460784912 = 1.10749089717865 + 0.01 * 7.181655406951904
Epoch 150, val loss: 1.225175380706787
Epoch 160, training loss: 1.0947413444519043 = 1.0230780839920044 + 0.01 * 7.1663312911987305
Epoch 160, val loss: 1.1601440906524658
Epoch 170, training loss: 1.0194189548492432 = 0.947847306728363 + 0.01 * 7.157168865203857
Epoch 170, val loss: 1.1031579971313477
Epoch 180, training loss: 0.9505789875984192 = 0.8790890574455261 + 0.01 * 7.148992538452148
Epoch 180, val loss: 1.0516502857208252
Epoch 190, training loss: 0.8854662775993347 = 0.8140605688095093 + 0.01 * 7.140568733215332
Epoch 190, val loss: 1.0034343004226685
Epoch 200, training loss: 0.8226966261863708 = 0.7513678073883057 + 0.01 * 7.132880210876465
Epoch 200, val loss: 0.9574806690216064
Epoch 210, training loss: 0.7620497941970825 = 0.6907846331596375 + 0.01 * 7.126514434814453
Epoch 210, val loss: 0.9144787192344666
Epoch 220, training loss: 0.7035620212554932 = 0.6323648691177368 + 0.01 * 7.119717597961426
Epoch 220, val loss: 0.8748573660850525
Epoch 230, training loss: 0.6468709111213684 = 0.5757192969322205 + 0.01 * 7.115159034729004
Epoch 230, val loss: 0.838848352432251
Epoch 240, training loss: 0.5913712978363037 = 0.5202568173408508 + 0.01 * 7.111447811126709
Epoch 240, val loss: 0.8055434823036194
Epoch 250, training loss: 0.5368633270263672 = 0.4657829701900482 + 0.01 * 7.108035564422607
Epoch 250, val loss: 0.7745917439460754
Epoch 260, training loss: 0.48368486762046814 = 0.41263410449028015 + 0.01 * 7.105076313018799
Epoch 260, val loss: 0.7458454966545105
Epoch 270, training loss: 0.432682603597641 = 0.3616572320461273 + 0.01 * 7.102536201477051
Epoch 270, val loss: 0.7195104360580444
Epoch 280, training loss: 0.38496339321136475 = 0.3139641582965851 + 0.01 * 7.099922180175781
Epoch 280, val loss: 0.6963580846786499
Epoch 290, training loss: 0.34150031208992004 = 0.2705307900905609 + 0.01 * 7.096951961517334
Epoch 290, val loss: 0.6772523522377014
Epoch 300, training loss: 0.30296167731285095 = 0.2319820523262024 + 0.01 * 7.097961902618408
Epoch 300, val loss: 0.6625807285308838
Epoch 310, training loss: 0.26944151520729065 = 0.198527529835701 + 0.01 * 7.0913987159729
Epoch 310, val loss: 0.652465283870697
Epoch 320, training loss: 0.24083629250526428 = 0.1699734926223755 + 0.01 * 7.086280822753906
Epoch 320, val loss: 0.6468632221221924
Epoch 330, training loss: 0.2167019546031952 = 0.14582650363445282 + 0.01 * 7.087545871734619
Epoch 330, val loss: 0.645063579082489
Epoch 340, training loss: 0.19629865884780884 = 0.12550675868988037 + 0.01 * 7.079189300537109
Epoch 340, val loss: 0.6464688777923584
Epoch 350, training loss: 0.17919939756393433 = 0.10842069238424301 + 0.01 * 7.0778703689575195
Epoch 350, val loss: 0.6503720879554749
Epoch 360, training loss: 0.16478297114372253 = 0.09404551237821579 + 0.01 * 7.073746681213379
Epoch 360, val loss: 0.6561660170555115
Epoch 370, training loss: 0.15268748998641968 = 0.08193470537662506 + 0.01 * 7.075279712677002
Epoch 370, val loss: 0.663237452507019
Epoch 380, training loss: 0.14238134026527405 = 0.07170382142066956 + 0.01 * 7.067752838134766
Epoch 380, val loss: 0.6712956428527832
Epoch 390, training loss: 0.1336873471736908 = 0.06303518265485764 + 0.01 * 7.065216064453125
Epoch 390, val loss: 0.6801459789276123
Epoch 400, training loss: 0.12627702951431274 = 0.05566789209842682 + 0.01 * 7.060914516448975
Epoch 400, val loss: 0.6894779801368713
Epoch 410, training loss: 0.12016510963439941 = 0.049391161650419235 + 0.01 * 7.077394962310791
Epoch 410, val loss: 0.6991091966629028
Epoch 420, training loss: 0.11458495259284973 = 0.04403359070420265 + 0.01 * 7.055136203765869
Epoch 420, val loss: 0.7089084386825562
Epoch 430, training loss: 0.10997187346220016 = 0.03943946212530136 + 0.01 * 7.05324125289917
Epoch 430, val loss: 0.7188125252723694
Epoch 440, training loss: 0.10596505552530289 = 0.03548263758420944 + 0.01 * 7.04824161529541
Epoch 440, val loss: 0.7286840081214905
Epoch 450, training loss: 0.10250088572502136 = 0.032060135155916214 + 0.01 * 7.044075012207031
Epoch 450, val loss: 0.7384567856788635
Epoch 460, training loss: 0.09950453042984009 = 0.029087508097290993 + 0.01 * 7.041702747344971
Epoch 460, val loss: 0.7480737566947937
Epoch 470, training loss: 0.09691943228244781 = 0.026497285813093185 + 0.01 * 7.042214870452881
Epoch 470, val loss: 0.7574284076690674
Epoch 480, training loss: 0.09456843137741089 = 0.024231288582086563 + 0.01 * 7.0337138175964355
Epoch 480, val loss: 0.7664965391159058
Epoch 490, training loss: 0.09252513200044632 = 0.02223871648311615 + 0.01 * 7.028641700744629
Epoch 490, val loss: 0.7754124402999878
Epoch 500, training loss: 0.09073040634393692 = 0.02047843299806118 + 0.01 * 7.0251970291137695
Epoch 500, val loss: 0.7840566039085388
Epoch 510, training loss: 0.08919063955545425 = 0.018919235095381737 + 0.01 * 7.027140140533447
Epoch 510, val loss: 0.7925102710723877
Epoch 520, training loss: 0.08768714219331741 = 0.017532959580421448 + 0.01 * 7.015418529510498
Epoch 520, val loss: 0.8006303310394287
Epoch 530, training loss: 0.08642132580280304 = 0.016294125467538834 + 0.01 * 7.012720584869385
Epoch 530, val loss: 0.808481752872467
Epoch 540, training loss: 0.08524090051651001 = 0.015182267874479294 + 0.01 * 7.005863666534424
Epoch 540, val loss: 0.8161320090293884
Epoch 550, training loss: 0.0843762457370758 = 0.014180874451994896 + 0.01 * 7.019536972045898
Epoch 550, val loss: 0.8235882520675659
Epoch 560, training loss: 0.08331682533025742 = 0.013277865014970303 + 0.01 * 7.003896236419678
Epoch 560, val loss: 0.8307814598083496
Epoch 570, training loss: 0.08239017426967621 = 0.01245957612991333 + 0.01 * 6.993060111999512
Epoch 570, val loss: 0.8377901315689087
Epoch 580, training loss: 0.08174732327461243 = 0.011716130189597607 + 0.01 * 7.003119468688965
Epoch 580, val loss: 0.8446601033210754
Epoch 590, training loss: 0.08090341836214066 = 0.01104105357080698 + 0.01 * 6.986237049102783
Epoch 590, val loss: 0.8512002825737
Epoch 600, training loss: 0.08020404726266861 = 0.01042462233453989 + 0.01 * 6.977942943572998
Epoch 600, val loss: 0.8575618863105774
Epoch 610, training loss: 0.07968128472566605 = 0.009860157035291195 + 0.01 * 6.982112884521484
Epoch 610, val loss: 0.8637630939483643
Epoch 620, training loss: 0.07906050235033035 = 0.009342165663838387 + 0.01 * 6.9718337059021
Epoch 620, val loss: 0.86974036693573
Epoch 630, training loss: 0.07857860624790192 = 0.008866183459758759 + 0.01 * 6.971242427825928
Epoch 630, val loss: 0.8755235075950623
Epoch 640, training loss: 0.07796449959278107 = 0.00842901598662138 + 0.01 * 6.953548431396484
Epoch 640, val loss: 0.8812153339385986
Epoch 650, training loss: 0.07749025523662567 = 0.008025642484426498 + 0.01 * 6.946462154388428
Epoch 650, val loss: 0.8866788148880005
Epoch 660, training loss: 0.07711651176214218 = 0.007652851287275553 + 0.01 * 6.946366786956787
Epoch 660, val loss: 0.8920130133628845
Epoch 670, training loss: 0.07682882994413376 = 0.007307585794478655 + 0.01 * 6.95212459564209
Epoch 670, val loss: 0.8971990346908569
Epoch 680, training loss: 0.07638459652662277 = 0.006987906992435455 + 0.01 * 6.939669132232666
Epoch 680, val loss: 0.9021230936050415
Epoch 690, training loss: 0.0760052278637886 = 0.006691426504403353 + 0.01 * 6.931380271911621
Epoch 690, val loss: 0.9070290327072144
Epoch 700, training loss: 0.07564204186201096 = 0.00641542999073863 + 0.01 * 6.922661781311035
Epoch 700, val loss: 0.9116689562797546
Epoch 710, training loss: 0.07524749636650085 = 0.00615834491327405 + 0.01 * 6.9089155197143555
Epoch 710, val loss: 0.9162888526916504
Epoch 720, training loss: 0.07556413114070892 = 0.0059177931398153305 + 0.01 * 6.964633941650391
Epoch 720, val loss: 0.9206892251968384
Epoch 730, training loss: 0.07488122582435608 = 0.005693991202861071 + 0.01 * 6.918723106384277
Epoch 730, val loss: 0.9249767661094666
Epoch 740, training loss: 0.07454853504896164 = 0.005484734661877155 + 0.01 * 6.9063801765441895
Epoch 740, val loss: 0.9292320609092712
Epoch 750, training loss: 0.07441188395023346 = 0.005287935957312584 + 0.01 * 6.912395000457764
Epoch 750, val loss: 0.9332249760627747
Epoch 760, training loss: 0.07422465831041336 = 0.005103279370814562 + 0.01 * 6.912137985229492
Epoch 760, val loss: 0.9371995329856873
Epoch 770, training loss: 0.07366647571325302 = 0.004930280614644289 + 0.01 * 6.87362003326416
Epoch 770, val loss: 0.9410112500190735
Epoch 780, training loss: 0.07355824112892151 = 0.004767362494021654 + 0.01 * 6.879087924957275
Epoch 780, val loss: 0.944797158241272
Epoch 790, training loss: 0.07340498268604279 = 0.004613526631146669 + 0.01 * 6.879145622253418
Epoch 790, val loss: 0.9482629895210266
Epoch 800, training loss: 0.07301047444343567 = 0.004468543455004692 + 0.01 * 6.854192733764648
Epoch 800, val loss: 0.9518275260925293
Epoch 810, training loss: 0.07291260361671448 = 0.004331638570874929 + 0.01 * 6.858096599578857
Epoch 810, val loss: 0.9552863240242004
Epoch 820, training loss: 0.07280347496271133 = 0.004201801493763924 + 0.01 * 6.860167503356934
Epoch 820, val loss: 0.9586876630783081
Epoch 830, training loss: 0.07248400151729584 = 0.004079398699104786 + 0.01 * 6.840460300445557
Epoch 830, val loss: 0.9619135856628418
Epoch 840, training loss: 0.07237131148576736 = 0.003963084425777197 + 0.01 * 6.840823173522949
Epoch 840, val loss: 0.9651559591293335
Epoch 850, training loss: 0.07233673334121704 = 0.003852569730952382 + 0.01 * 6.848416328430176
Epoch 850, val loss: 0.9682798981666565
Epoch 860, training loss: 0.07211222499608994 = 0.0037478809244930744 + 0.01 * 6.836434841156006
Epoch 860, val loss: 0.9711863398551941
Epoch 870, training loss: 0.0718555599451065 = 0.0036484352312982082 + 0.01 * 6.820712566375732
Epoch 870, val loss: 0.9742755889892578
Epoch 880, training loss: 0.07191266119480133 = 0.0035535558126866817 + 0.01 * 6.835910797119141
Epoch 880, val loss: 0.9771361351013184
Epoch 890, training loss: 0.07161805033683777 = 0.0034632429014891386 + 0.01 * 6.8154802322387695
Epoch 890, val loss: 0.9799083471298218
Epoch 900, training loss: 0.07152227312326431 = 0.0033770764712244272 + 0.01 * 6.81451940536499
Epoch 900, val loss: 0.9827757477760315
Epoch 910, training loss: 0.07146138697862625 = 0.0032947056461125612 + 0.01 * 6.816668510437012
Epoch 910, val loss: 0.9854161143302917
Epoch 920, training loss: 0.07141575962305069 = 0.0032159886322915554 + 0.01 * 6.819976806640625
Epoch 920, val loss: 0.9881287813186646
Epoch 930, training loss: 0.07114890217781067 = 0.003140972927212715 + 0.01 * 6.800793170928955
Epoch 930, val loss: 0.9906594753265381
Epoch 940, training loss: 0.07108995318412781 = 0.003069208702072501 + 0.01 * 6.802074432373047
Epoch 940, val loss: 0.9933539628982544
Epoch 950, training loss: 0.07105862349271774 = 0.0030005397275090218 + 0.01 * 6.805808067321777
Epoch 950, val loss: 0.9956754446029663
Epoch 960, training loss: 0.07100323587656021 = 0.0029346884693950415 + 0.01 * 6.806855201721191
Epoch 960, val loss: 0.9981791377067566
Epoch 970, training loss: 0.07073419541120529 = 0.0028718512039631605 + 0.01 * 6.7862348556518555
Epoch 970, val loss: 1.0004322528839111
Epoch 980, training loss: 0.07068736106157303 = 0.0028113804291933775 + 0.01 * 6.787598133087158
Epoch 980, val loss: 1.0028988122940063
Epoch 990, training loss: 0.07060728967189789 = 0.002753389999270439 + 0.01 * 6.7853899002075195
Epoch 990, val loss: 1.0050008296966553
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8328940432261466
=== training gcn model ===
Epoch 0, training loss: 2.0236880779266357 = 1.9377199411392212 + 0.01 * 8.59681510925293
Epoch 0, val loss: 1.9390324354171753
Epoch 10, training loss: 2.013540029525757 = 1.9275723695755005 + 0.01 * 8.596755981445312
Epoch 10, val loss: 1.928328037261963
Epoch 20, training loss: 2.0011918544769287 = 1.9152261018753052 + 0.01 * 8.596574783325195
Epoch 20, val loss: 1.9153858423233032
Epoch 30, training loss: 1.9839794635772705 = 1.8980191946029663 + 0.01 * 8.596024513244629
Epoch 30, val loss: 1.8975472450256348
Epoch 40, training loss: 1.9588885307312012 = 1.8729548454284668 + 0.01 * 8.593374252319336
Epoch 40, val loss: 1.8720160722732544
Epoch 50, training loss: 1.924311876296997 = 1.8385635614395142 + 0.01 * 8.574835777282715
Epoch 50, val loss: 1.8386257886886597
Epoch 60, training loss: 1.8851410150527954 = 1.8002638816833496 + 0.01 * 8.487715721130371
Epoch 60, val loss: 1.8051173686981201
Epoch 70, training loss: 1.8463383913040161 = 1.7637156248092651 + 0.01 * 8.262275695800781
Epoch 70, val loss: 1.7751060724258423
Epoch 80, training loss: 1.7971328496932983 = 1.7163275480270386 + 0.01 * 8.08053207397461
Epoch 80, val loss: 1.7332056760787964
Epoch 90, training loss: 1.7291897535324097 = 1.650702714920044 + 0.01 * 7.848706245422363
Epoch 90, val loss: 1.675504207611084
Epoch 100, training loss: 1.6413836479187012 = 1.5650004148483276 + 0.01 * 7.6383256912231445
Epoch 100, val loss: 1.6037652492523193
Epoch 110, training loss: 1.543078899383545 = 1.467667818069458 + 0.01 * 7.541114330291748
Epoch 110, val loss: 1.5237451791763306
Epoch 120, training loss: 1.4427928924560547 = 1.3677566051483154 + 0.01 * 7.503630638122559
Epoch 120, val loss: 1.4439440965652466
Epoch 130, training loss: 1.341274380683899 = 1.2668266296386719 + 0.01 * 7.444774150848389
Epoch 130, val loss: 1.3648985624313354
Epoch 140, training loss: 1.238524079322815 = 1.1647838354110718 + 0.01 * 7.374026298522949
Epoch 140, val loss: 1.2864620685577393
Epoch 150, training loss: 1.1371963024139404 = 1.0639803409576416 + 0.01 * 7.321599006652832
Epoch 150, val loss: 1.2109721899032593
Epoch 160, training loss: 1.0411696434020996 = 0.9681931138038635 + 0.01 * 7.297649383544922
Epoch 160, val loss: 1.1399286985397339
Epoch 170, training loss: 0.9523513317108154 = 0.8796104192733765 + 0.01 * 7.2740888595581055
Epoch 170, val loss: 1.0749359130859375
Epoch 180, training loss: 0.8713914155960083 = 0.7989287972450256 + 0.01 * 7.246264457702637
Epoch 180, val loss: 1.0159320831298828
Epoch 190, training loss: 0.798337996006012 = 0.7261337041854858 + 0.01 * 7.220430374145508
Epoch 190, val loss: 0.9636937975883484
Epoch 200, training loss: 0.732699453830719 = 0.6606404781341553 + 0.01 * 7.205898761749268
Epoch 200, val loss: 0.9189165234565735
Epoch 210, training loss: 0.673521876335144 = 0.6015812158584595 + 0.01 * 7.194068908691406
Epoch 210, val loss: 0.8815657496452332
Epoch 220, training loss: 0.6200228929519653 = 0.5481516718864441 + 0.01 * 7.187121868133545
Epoch 220, val loss: 0.8513004779815674
Epoch 230, training loss: 0.5714040994644165 = 0.4995601177215576 + 0.01 * 7.18440055847168
Epoch 230, val loss: 0.8275049924850464
Epoch 240, training loss: 0.52684086561203 = 0.45505455136299133 + 0.01 * 7.17863130569458
Epoch 240, val loss: 0.8095426559448242
Epoch 250, training loss: 0.485682874917984 = 0.4139236807823181 + 0.01 * 7.175919055938721
Epoch 250, val loss: 0.7966386675834656
Epoch 260, training loss: 0.44734659790992737 = 0.37562066316604614 + 0.01 * 7.172592639923096
Epoch 260, val loss: 0.7878902554512024
Epoch 270, training loss: 0.4114888906478882 = 0.3398035764694214 + 0.01 * 7.168531894683838
Epoch 270, val loss: 0.7824898362159729
Epoch 280, training loss: 0.37793368101119995 = 0.30626729130744934 + 0.01 * 7.166640281677246
Epoch 280, val loss: 0.7799401879310608
Epoch 290, training loss: 0.3465314507484436 = 0.27491530776023865 + 0.01 * 7.16161584854126
Epoch 290, val loss: 0.7799359560012817
Epoch 300, training loss: 0.3172984719276428 = 0.2457374930381775 + 0.01 * 7.156098365783691
Epoch 300, val loss: 0.7823070883750916
Epoch 310, training loss: 0.2903253138065338 = 0.21878793835639954 + 0.01 * 7.1537370681762695
Epoch 310, val loss: 0.786831259727478
Epoch 320, training loss: 0.26560842990875244 = 0.19414401054382324 + 0.01 * 7.1464409828186035
Epoch 320, val loss: 0.7933957576751709
Epoch 330, training loss: 0.2432670295238495 = 0.1718646138906479 + 0.01 * 7.140240669250488
Epoch 330, val loss: 0.8020457029342651
Epoch 340, training loss: 0.22326630353927612 = 0.15194103121757507 + 0.01 * 7.132528305053711
Epoch 340, val loss: 0.8125500082969666
Epoch 350, training loss: 0.20552678406238556 = 0.13429123163223267 + 0.01 * 7.1235551834106445
Epoch 350, val loss: 0.8247236013412476
Epoch 360, training loss: 0.18998289108276367 = 0.11875784397125244 + 0.01 * 7.122504234313965
Epoch 360, val loss: 0.8383813500404358
Epoch 370, training loss: 0.17620301246643066 = 0.10515624284744263 + 0.01 * 7.104676723480225
Epoch 370, val loss: 0.8532525300979614
Epoch 380, training loss: 0.16426420211791992 = 0.09327265620231628 + 0.01 * 7.099154949188232
Epoch 380, val loss: 0.8690469264984131
Epoch 390, training loss: 0.15391740202903748 = 0.08289892971515656 + 0.01 * 7.101848125457764
Epoch 390, val loss: 0.8854485750198364
Epoch 400, training loss: 0.14468279480934143 = 0.07384905219078064 + 0.01 * 7.0833740234375
Epoch 400, val loss: 0.9021849632263184
Epoch 410, training loss: 0.1367575228214264 = 0.0659446269273758 + 0.01 * 7.081288814544678
Epoch 410, val loss: 0.9190515875816345
Epoch 420, training loss: 0.1297943890094757 = 0.05903356149792671 + 0.01 * 7.076083183288574
Epoch 420, val loss: 0.9358044862747192
Epoch 430, training loss: 0.1236896961927414 = 0.052989762276411057 + 0.01 * 7.069993495941162
Epoch 430, val loss: 0.9523453116416931
Epoch 440, training loss: 0.11830990016460419 = 0.0476989783346653 + 0.01 * 7.061091899871826
Epoch 440, val loss: 0.9685342311859131
Epoch 450, training loss: 0.11361213773488998 = 0.04306492209434509 + 0.01 * 7.054721832275391
Epoch 450, val loss: 0.9843459725379944
Epoch 460, training loss: 0.10943454504013062 = 0.03899785876274109 + 0.01 * 7.043668746948242
Epoch 460, val loss: 0.9996688365936279
Epoch 470, training loss: 0.1057983934879303 = 0.03542224317789078 + 0.01 * 7.037614822387695
Epoch 470, val loss: 1.014503836631775
Epoch 480, training loss: 0.10259458422660828 = 0.032273150980472565 + 0.01 * 7.032143592834473
Epoch 480, val loss: 1.02881920337677
Epoch 490, training loss: 0.09979814291000366 = 0.029491644352674484 + 0.01 * 7.030649662017822
Epoch 490, val loss: 1.0426628589630127
Epoch 500, training loss: 0.09728053212165833 = 0.02703009732067585 + 0.01 * 7.025043487548828
Epoch 500, val loss: 1.0560131072998047
Epoch 510, training loss: 0.095085009932518 = 0.024844374507665634 + 0.01 * 7.024063587188721
Epoch 510, val loss: 1.0688656568527222
Epoch 520, training loss: 0.09313983470201492 = 0.022901052609086037 + 0.01 * 7.023878574371338
Epoch 520, val loss: 1.0812419652938843
Epoch 530, training loss: 0.09130494296550751 = 0.021167518571019173 + 0.01 * 7.013742446899414
Epoch 530, val loss: 1.0931283235549927
Epoch 540, training loss: 0.08967063575983047 = 0.0196168664842844 + 0.01 * 7.005377292633057
Epoch 540, val loss: 1.1046000719070435
Epoch 550, training loss: 0.08829252421855927 = 0.018225762993097305 + 0.01 * 7.006675720214844
Epoch 550, val loss: 1.1156563758850098
Epoch 560, training loss: 0.08701493591070175 = 0.016974972561001778 + 0.01 * 7.003996849060059
Epoch 560, val loss: 1.1262630224227905
Epoch 570, training loss: 0.0857509970664978 = 0.015847647562623024 + 0.01 * 6.990334987640381
Epoch 570, val loss: 1.136497974395752
Epoch 580, training loss: 0.0847645029425621 = 0.014828644692897797 + 0.01 * 6.99358606338501
Epoch 580, val loss: 1.1463730335235596
Epoch 590, training loss: 0.08385145664215088 = 0.013905281201004982 + 0.01 * 6.994617938995361
Epoch 590, val loss: 1.1558468341827393
Epoch 600, training loss: 0.08290039747953415 = 0.013067007064819336 + 0.01 * 6.983339309692383
Epoch 600, val loss: 1.1650447845458984
Epoch 610, training loss: 0.08216053247451782 = 0.012303268536925316 + 0.01 * 6.985726356506348
Epoch 610, val loss: 1.1739006042480469
Epoch 620, training loss: 0.08127658814191818 = 0.011606588028371334 + 0.01 * 6.9670000076293945
Epoch 620, val loss: 1.1824283599853516
Epoch 630, training loss: 0.08062244951725006 = 0.010969486087560654 + 0.01 * 6.965296745300293
Epoch 630, val loss: 1.1906644105911255
Epoch 640, training loss: 0.08001537621021271 = 0.010385259985923767 + 0.01 * 6.963011741638184
Epoch 640, val loss: 1.1986087560653687
Epoch 650, training loss: 0.07934913039207458 = 0.00984864216297865 + 0.01 * 6.95004940032959
Epoch 650, val loss: 1.2062557935714722
Epoch 660, training loss: 0.07880726456642151 = 0.009354965761303902 + 0.01 * 6.945230007171631
Epoch 660, val loss: 1.2136577367782593
Epoch 670, training loss: 0.07854156196117401 = 0.008899313397705555 + 0.01 * 6.964224815368652
Epoch 670, val loss: 1.220816731452942
Epoch 680, training loss: 0.07802029699087143 = 0.008479096926748753 + 0.01 * 6.95412015914917
Epoch 680, val loss: 1.2276906967163086
Epoch 690, training loss: 0.07737688720226288 = 0.008090033195912838 + 0.01 * 6.928685665130615
Epoch 690, val loss: 1.2343637943267822
Epoch 700, training loss: 0.07713262736797333 = 0.007728658616542816 + 0.01 * 6.940397262573242
Epoch 700, val loss: 1.2408522367477417
Epoch 710, training loss: 0.0766572430729866 = 0.007393296342343092 + 0.01 * 6.926395416259766
Epoch 710, val loss: 1.2470911741256714
Epoch 720, training loss: 0.0762227326631546 = 0.007081266026943922 + 0.01 * 6.91414737701416
Epoch 720, val loss: 1.2531347274780273
Epoch 730, training loss: 0.07589010894298553 = 0.00679047079756856 + 0.01 * 6.909963607788086
Epoch 730, val loss: 1.2590043544769287
Epoch 740, training loss: 0.07569139450788498 = 0.006519047077745199 + 0.01 * 6.917234420776367
Epoch 740, val loss: 1.2646572589874268
Epoch 750, training loss: 0.07541413605213165 = 0.006265577860176563 + 0.01 * 6.91485595703125
Epoch 750, val loss: 1.2700778245925903
Epoch 760, training loss: 0.07515496015548706 = 0.0060279169119894505 + 0.01 * 6.912704944610596
Epoch 760, val loss: 1.2753705978393555
Epoch 770, training loss: 0.07483771443367004 = 0.005805479362607002 + 0.01 * 6.903223514556885
Epoch 770, val loss: 1.2804888486862183
Epoch 780, training loss: 0.07466686517000198 = 0.005596273113042116 + 0.01 * 6.907059669494629
Epoch 780, val loss: 1.2854548692703247
Epoch 790, training loss: 0.0744607001543045 = 0.005400001537054777 + 0.01 * 6.906069755554199
Epoch 790, val loss: 1.2902792692184448
Epoch 800, training loss: 0.0740809440612793 = 0.005215098150074482 + 0.01 * 6.886584281921387
Epoch 800, val loss: 1.2949423789978027
Epoch 810, training loss: 0.07402487099170685 = 0.005041085183620453 + 0.01 * 6.898378372192383
Epoch 810, val loss: 1.2994756698608398
Epoch 820, training loss: 0.07368233799934387 = 0.004876666236668825 + 0.01 * 6.8805670738220215
Epoch 820, val loss: 1.3038703203201294
Epoch 830, training loss: 0.07354608178138733 = 0.004721431061625481 + 0.01 * 6.88246488571167
Epoch 830, val loss: 1.3081262111663818
Epoch 840, training loss: 0.07339582592248917 = 0.004574524704366922 + 0.01 * 6.8821306228637695
Epoch 840, val loss: 1.3122661113739014
Epoch 850, training loss: 0.07316652685403824 = 0.004435537848621607 + 0.01 * 6.873098850250244
Epoch 850, val loss: 1.3162144422531128
Epoch 860, training loss: 0.07284901291131973 = 0.004303694237023592 + 0.01 * 6.854532241821289
Epoch 860, val loss: 1.3201321363449097
Epoch 870, training loss: 0.0728038102388382 = 0.00417852820828557 + 0.01 * 6.862528324127197
Epoch 870, val loss: 1.3238816261291504
Epoch 880, training loss: 0.07265891134738922 = 0.004060122650116682 + 0.01 * 6.859879493713379
Epoch 880, val loss: 1.327501654624939
Epoch 890, training loss: 0.07243981957435608 = 0.003947555553168058 + 0.01 * 6.849226474761963
Epoch 890, val loss: 1.3309881687164307
Epoch 900, training loss: 0.07219719141721725 = 0.0038403738290071487 + 0.01 * 6.835681915283203
Epoch 900, val loss: 1.3344358205795288
Epoch 910, training loss: 0.07227284461259842 = 0.003738223109394312 + 0.01 * 6.853462219238281
Epoch 910, val loss: 1.3377611637115479
Epoch 920, training loss: 0.07189469784498215 = 0.0036409455351531506 + 0.01 * 6.825376033782959
Epoch 920, val loss: 1.3409768342971802
Epoch 930, training loss: 0.07192658632993698 = 0.003548211418092251 + 0.01 * 6.8378376960754395
Epoch 930, val loss: 1.3441311120986938
Epoch 940, training loss: 0.07177310436964035 = 0.003459773724898696 + 0.01 * 6.831333160400391
Epoch 940, val loss: 1.3471577167510986
Epoch 950, training loss: 0.07155550271272659 = 0.003375194501131773 + 0.01 * 6.818030834197998
Epoch 950, val loss: 1.3501124382019043
Epoch 960, training loss: 0.0716795101761818 = 0.0032942176330834627 + 0.01 * 6.838529586791992
Epoch 960, val loss: 1.353007435798645
Epoch 970, training loss: 0.0715809315443039 = 0.0032169162295758724 + 0.01 * 6.83640193939209
Epoch 970, val loss: 1.3557955026626587
Epoch 980, training loss: 0.07135768979787827 = 0.003142814850434661 + 0.01 * 6.821487903594971
Epoch 980, val loss: 1.3585028648376465
Epoch 990, training loss: 0.07130960375070572 = 0.0030719179194420576 + 0.01 * 6.823768615722656
Epoch 990, val loss: 1.3611325025558472
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.0309338569641113 = 1.9449654817581177 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9517019987106323
Epoch 10, training loss: 2.0197079181671143 = 1.9337403774261475 + 0.01 * 8.596750259399414
Epoch 10, val loss: 1.9396437406539917
Epoch 20, training loss: 2.006044387817383 = 1.9200793504714966 + 0.01 * 8.596505165100098
Epoch 20, val loss: 1.9246333837509155
Epoch 30, training loss: 1.9874258041381836 = 1.9014689922332764 + 0.01 * 8.595675468444824
Epoch 30, val loss: 1.9040493965148926
Epoch 40, training loss: 1.9608289003372192 = 1.8749191761016846 + 0.01 * 8.590971946716309
Epoch 40, val loss: 1.8750966787338257
Epoch 50, training loss: 1.924970269203186 = 1.8393611907958984 + 0.01 * 8.56090259552002
Epoch 50, val loss: 1.838161826133728
Epoch 60, training loss: 1.8851816654205322 = 1.8011192083358765 + 0.01 * 8.406248092651367
Epoch 60, val loss: 1.8030779361724854
Epoch 70, training loss: 1.849159836769104 = 1.766703486442566 + 0.01 * 8.24563980102539
Epoch 70, val loss: 1.7756624221801758
Epoch 80, training loss: 1.802220344543457 = 1.7223256826400757 + 0.01 * 7.989470958709717
Epoch 80, val loss: 1.7382383346557617
Epoch 90, training loss: 1.7365405559539795 = 1.659888744354248 + 0.01 * 7.665182590484619
Epoch 90, val loss: 1.6826963424682617
Epoch 100, training loss: 1.650735855102539 = 1.5751945972442627 + 0.01 * 7.5541276931762695
Epoch 100, val loss: 1.6082578897476196
Epoch 110, training loss: 1.547160267829895 = 1.4719361066818237 + 0.01 * 7.522410869598389
Epoch 110, val loss: 1.5208454132080078
Epoch 120, training loss: 1.4326361417770386 = 1.3577349185943604 + 0.01 * 7.490124702453613
Epoch 120, val loss: 1.4258533716201782
Epoch 130, training loss: 1.3131390810012817 = 1.2387360334396362 + 0.01 * 7.440307140350342
Epoch 130, val loss: 1.3277620077133179
Epoch 140, training loss: 1.1932404041290283 = 1.1193807125091553 + 0.01 * 7.385974407196045
Epoch 140, val loss: 1.2298095226287842
Epoch 150, training loss: 1.0785164833068848 = 1.0050578117370605 + 0.01 * 7.34586763381958
Epoch 150, val loss: 1.1369062662124634
Epoch 160, training loss: 0.9747723340988159 = 0.9015870094299316 + 0.01 * 7.318533897399902
Epoch 160, val loss: 1.0539366006851196
Epoch 170, training loss: 0.8854725956916809 = 0.8125036954879761 + 0.01 * 7.296889305114746
Epoch 170, val loss: 0.9841896295547485
Epoch 180, training loss: 0.8108934164047241 = 0.7381418347358704 + 0.01 * 7.275161266326904
Epoch 180, val loss: 0.9283685684204102
Epoch 190, training loss: 0.7479982972145081 = 0.675553023815155 + 0.01 * 7.244527816772461
Epoch 190, val loss: 0.8846691846847534
Epoch 200, training loss: 0.6927652955055237 = 0.6205856800079346 + 0.01 * 7.217960834503174
Epoch 200, val loss: 0.8501515984535217
Epoch 210, training loss: 0.642236053943634 = 0.5702483654022217 + 0.01 * 7.198767185211182
Epoch 210, val loss: 0.8224877119064331
Epoch 220, training loss: 0.5948205590248108 = 0.5229396224021912 + 0.01 * 7.188094139099121
Epoch 220, val loss: 0.8002820611000061
Epoch 230, training loss: 0.5497831106185913 = 0.47804439067840576 + 0.01 * 7.1738739013671875
Epoch 230, val loss: 0.7822750806808472
Epoch 240, training loss: 0.5070106387138367 = 0.43534356355667114 + 0.01 * 7.166707992553711
Epoch 240, val loss: 0.767718493938446
Epoch 250, training loss: 0.46667274832725525 = 0.39506664872169495 + 0.01 * 7.160610675811768
Epoch 250, val loss: 0.7566273808479309
Epoch 260, training loss: 0.42907437682151794 = 0.3575528860092163 + 0.01 * 7.152148723602295
Epoch 260, val loss: 0.7495194673538208
Epoch 270, training loss: 0.3942807614803314 = 0.3228089511394501 + 0.01 * 7.147181034088135
Epoch 270, val loss: 0.7465600371360779
Epoch 280, training loss: 0.3619975745677948 = 0.2905670404434204 + 0.01 * 7.14305305480957
Epoch 280, val loss: 0.7472941875457764
Epoch 290, training loss: 0.33183664083480835 = 0.2604978382587433 + 0.01 * 7.133881092071533
Epoch 290, val loss: 0.751413881778717
Epoch 300, training loss: 0.3038395345211029 = 0.23252522945404053 + 0.01 * 7.131429672241211
Epoch 300, val loss: 0.7585476040840149
Epoch 310, training loss: 0.2780112326145172 = 0.2067883312702179 + 0.01 * 7.122290134429932
Epoch 310, val loss: 0.7682066559791565
Epoch 320, training loss: 0.2547905445098877 = 0.18349525332450867 + 0.01 * 7.129530906677246
Epoch 320, val loss: 0.7803664207458496
Epoch 330, training loss: 0.23381465673446655 = 0.16277189552783966 + 0.01 * 7.104275703430176
Epoch 330, val loss: 0.7946929931640625
Epoch 340, training loss: 0.21563908457756042 = 0.14453336596488953 + 0.01 * 7.110572338104248
Epoch 340, val loss: 0.8109083771705627
Epoch 350, training loss: 0.19947701692581177 = 0.12858609855175018 + 0.01 * 7.089092254638672
Epoch 350, val loss: 0.828639030456543
Epoch 360, training loss: 0.18543487787246704 = 0.11467794328927994 + 0.01 * 7.075692653656006
Epoch 360, val loss: 0.8474173545837402
Epoch 370, training loss: 0.17334306240081787 = 0.10255646705627441 + 0.01 * 7.078658580780029
Epoch 370, val loss: 0.8668555021286011
Epoch 380, training loss: 0.16265910863876343 = 0.09198986738920212 + 0.01 * 7.066924095153809
Epoch 380, val loss: 0.8867015242576599
Epoch 390, training loss: 0.153370201587677 = 0.08275750279426575 + 0.01 * 7.061270713806152
Epoch 390, val loss: 0.9067482352256775
Epoch 400, training loss: 0.14522182941436768 = 0.0746743381023407 + 0.01 * 7.054749965667725
Epoch 400, val loss: 0.9268667697906494
Epoch 410, training loss: 0.13791656494140625 = 0.0675787627696991 + 0.01 * 7.033780574798584
Epoch 410, val loss: 0.9468637704849243
Epoch 420, training loss: 0.13161547482013702 = 0.061336297541856766 + 0.01 * 7.027918338775635
Epoch 420, val loss: 0.9667345881462097
Epoch 430, training loss: 0.12602779269218445 = 0.05582885071635246 + 0.01 * 7.019894123077393
Epoch 430, val loss: 0.9862982630729675
Epoch 440, training loss: 0.12100571393966675 = 0.05095312371850014 + 0.01 * 7.005258560180664
Epoch 440, val loss: 1.0055994987487793
Epoch 450, training loss: 0.11651433259248734 = 0.04662545025348663 + 0.01 * 6.988888263702393
Epoch 450, val loss: 1.0244977474212646
Epoch 460, training loss: 0.11273813247680664 = 0.04277456924319267 + 0.01 * 6.996356964111328
Epoch 460, val loss: 1.043025016784668
Epoch 470, training loss: 0.1091703325510025 = 0.03933634236454964 + 0.01 * 6.9833984375
Epoch 470, val loss: 1.0611501932144165
Epoch 480, training loss: 0.10599497705698013 = 0.036259256303310394 + 0.01 * 6.973572254180908
Epoch 480, val loss: 1.0787938833236694
Epoch 490, training loss: 0.1030980795621872 = 0.03349858149886131 + 0.01 * 6.959949970245361
Epoch 490, val loss: 1.0960471630096436
Epoch 500, training loss: 0.10070934891700745 = 0.031017880886793137 + 0.01 * 6.969146728515625
Epoch 500, val loss: 1.1127612590789795
Epoch 510, training loss: 0.0982680395245552 = 0.028781546279788017 + 0.01 * 6.948648929595947
Epoch 510, val loss: 1.1291053295135498
Epoch 520, training loss: 0.09635324776172638 = 0.02676069736480713 + 0.01 * 6.959255218505859
Epoch 520, val loss: 1.1450356245040894
Epoch 530, training loss: 0.0944746881723404 = 0.02493264712393284 + 0.01 * 6.954204559326172
Epoch 530, val loss: 1.1605188846588135
Epoch 540, training loss: 0.0927269384264946 = 0.02327595092356205 + 0.01 * 6.945098876953125
Epoch 540, val loss: 1.1755958795547485
Epoch 550, training loss: 0.09122681617736816 = 0.021769868209958076 + 0.01 * 6.945695400238037
Epoch 550, val loss: 1.1902673244476318
Epoch 560, training loss: 0.0896381065249443 = 0.02039974182844162 + 0.01 * 6.923836708068848
Epoch 560, val loss: 1.204494833946228
Epoch 570, training loss: 0.0885690301656723 = 0.01914946362376213 + 0.01 * 6.941956520080566
Epoch 570, val loss: 1.2183583974838257
Epoch 580, training loss: 0.0871795266866684 = 0.018008124083280563 + 0.01 * 6.917140483856201
Epoch 580, val loss: 1.2318693399429321
Epoch 590, training loss: 0.08620128035545349 = 0.016963142901659012 + 0.01 * 6.923813343048096
Epoch 590, val loss: 1.2450217008590698
Epoch 600, training loss: 0.0851086676120758 = 0.01600576564669609 + 0.01 * 6.910289764404297
Epoch 600, val loss: 1.2577612400054932
Epoch 610, training loss: 0.08420810103416443 = 0.015125930309295654 + 0.01 * 6.908217430114746
Epoch 610, val loss: 1.2702001333236694
Epoch 620, training loss: 0.08346527814865112 = 0.014316725544631481 + 0.01 * 6.91485595703125
Epoch 620, val loss: 1.2823166847229004
Epoch 630, training loss: 0.08255971223115921 = 0.01357081439346075 + 0.01 * 6.898889541625977
Epoch 630, val loss: 1.2940750122070312
Epoch 640, training loss: 0.0820295512676239 = 0.012882255017757416 + 0.01 * 6.914730072021484
Epoch 640, val loss: 1.3055768013000488
Epoch 650, training loss: 0.08121897280216217 = 0.012245758436620235 + 0.01 * 6.897321701049805
Epoch 650, val loss: 1.3166965246200562
Epoch 660, training loss: 0.08066447079181671 = 0.011656397953629494 + 0.01 * 6.9008073806762695
Epoch 660, val loss: 1.327618956565857
Epoch 670, training loss: 0.07995490729808807 = 0.011109377257525921 + 0.01 * 6.8845534324646
Epoch 670, val loss: 1.3381901979446411
Epoch 680, training loss: 0.07945888489484787 = 0.010601523332297802 + 0.01 * 6.885736465454102
Epoch 680, val loss: 1.3485511541366577
Epoch 690, training loss: 0.07898318767547607 = 0.010128771886229515 + 0.01 * 6.885441303253174
Epoch 690, val loss: 1.3585996627807617
Epoch 700, training loss: 0.07834659516811371 = 0.009688492864370346 + 0.01 * 6.865809917449951
Epoch 700, val loss: 1.368392825126648
Epoch 710, training loss: 0.07805775105953217 = 0.009277556091547012 + 0.01 * 6.8780198097229
Epoch 710, val loss: 1.3779549598693848
Epoch 720, training loss: 0.07767070829868317 = 0.008893381804227829 + 0.01 * 6.87773323059082
Epoch 720, val loss: 1.3872438669204712
Epoch 730, training loss: 0.0772394984960556 = 0.008534438908100128 + 0.01 * 6.870506286621094
Epoch 730, val loss: 1.396350622177124
Epoch 740, training loss: 0.07690463960170746 = 0.008197945542633533 + 0.01 * 6.870669364929199
Epoch 740, val loss: 1.4051711559295654
Epoch 750, training loss: 0.07638511806726456 = 0.00788251031190157 + 0.01 * 6.8502607345581055
Epoch 750, val loss: 1.4138243198394775
Epoch 760, training loss: 0.0760272815823555 = 0.007586103864014149 + 0.01 * 6.844117641448975
Epoch 760, val loss: 1.4222402572631836
Epoch 770, training loss: 0.07584815472364426 = 0.0073075066320598125 + 0.01 * 6.85406494140625
Epoch 770, val loss: 1.430433750152588
Epoch 780, training loss: 0.07556137442588806 = 0.007045413367450237 + 0.01 * 6.851596355438232
Epoch 780, val loss: 1.4384797811508179
Epoch 790, training loss: 0.07521045953035355 = 0.006798421498388052 + 0.01 * 6.841203689575195
Epoch 790, val loss: 1.446274995803833
Epoch 800, training loss: 0.07498816400766373 = 0.00656540784984827 + 0.01 * 6.842275619506836
Epoch 800, val loss: 1.4539315700531006
Epoch 810, training loss: 0.07464604824781418 = 0.006345485337078571 + 0.01 * 6.830056190490723
Epoch 810, val loss: 1.4613860845565796
Epoch 820, training loss: 0.07458202540874481 = 0.006137605290859938 + 0.01 * 6.844442844390869
Epoch 820, val loss: 1.4686543941497803
Epoch 830, training loss: 0.07435205578804016 = 0.005941114388406277 + 0.01 * 6.841094017028809
Epoch 830, val loss: 1.4757884740829468
Epoch 840, training loss: 0.07393548637628555 = 0.005754909012466669 + 0.01 * 6.818057537078857
Epoch 840, val loss: 1.4827021360397339
Epoch 850, training loss: 0.07401682436466217 = 0.00557854026556015 + 0.01 * 6.8438286781311035
Epoch 850, val loss: 1.4895355701446533
Epoch 860, training loss: 0.07356494665145874 = 0.005411001853644848 + 0.01 * 6.815394878387451
Epoch 860, val loss: 1.4961634874343872
Epoch 870, training loss: 0.07345897704362869 = 0.005252025090157986 + 0.01 * 6.820695400238037
Epoch 870, val loss: 1.5026243925094604
Epoch 880, training loss: 0.0734269767999649 = 0.00510108657181263 + 0.01 * 6.832589149475098
Epoch 880, val loss: 1.5090036392211914
Epoch 890, training loss: 0.07301633805036545 = 0.004957490600645542 + 0.01 * 6.805884838104248
Epoch 890, val loss: 1.5152097940444946
Epoch 900, training loss: 0.07298129796981812 = 0.004820783622562885 + 0.01 * 6.816051483154297
Epoch 900, val loss: 1.5212786197662354
Epoch 910, training loss: 0.07262314110994339 = 0.004690499976277351 + 0.01 * 6.793264389038086
Epoch 910, val loss: 1.527220368385315
Epoch 920, training loss: 0.07292844355106354 = 0.004566260147839785 + 0.01 * 6.83621883392334
Epoch 920, val loss: 1.5330579280853271
Epoch 930, training loss: 0.07260382920503616 = 0.004447713494300842 + 0.01 * 6.815611362457275
Epoch 930, val loss: 1.5388108491897583
Epoch 940, training loss: 0.07219909131526947 = 0.004334654659032822 + 0.01 * 6.786444187164307
Epoch 940, val loss: 1.544367790222168
Epoch 950, training loss: 0.07205449044704437 = 0.004226554185152054 + 0.01 * 6.7827935218811035
Epoch 950, val loss: 1.5498359203338623
Epoch 960, training loss: 0.07202418893575668 = 0.004123212303966284 + 0.01 * 6.790098190307617
Epoch 960, val loss: 1.555221676826477
Epoch 970, training loss: 0.07204515486955643 = 0.004024161957204342 + 0.01 * 6.802099704742432
Epoch 970, val loss: 1.5604604482650757
Epoch 980, training loss: 0.07181666791439056 = 0.00392947718501091 + 0.01 * 6.788719177246094
Epoch 980, val loss: 1.5656328201293945
Epoch 990, training loss: 0.07159112393856049 = 0.0038387731183320284 + 0.01 * 6.775235652923584
Epoch 990, val loss: 1.5706557035446167
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8365840801265156
The final CL Acc:0.80741, 0.01600, The final GNN Acc:0.83518, 0.00163
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9384])
updated graph: torch.Size([2, 10478])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0522003173828125 = 1.9662325382232666 + 0.01 * 8.596783638000488
Epoch 0, val loss: 1.9655741453170776
Epoch 10, training loss: 2.041144847869873 = 1.955177664756775 + 0.01 * 8.59671688079834
Epoch 10, val loss: 1.9543795585632324
Epoch 20, training loss: 2.027559280395508 = 1.9415944814682007 + 0.01 * 8.596477508544922
Epoch 20, val loss: 1.9406006336212158
Epoch 30, training loss: 2.008462905883789 = 1.9225056171417236 + 0.01 * 8.595717430114746
Epoch 30, val loss: 1.921487808227539
Epoch 40, training loss: 1.9802579879760742 = 1.894347071647644 + 0.01 * 8.591094017028809
Epoch 40, val loss: 1.8939790725708008
Epoch 50, training loss: 1.9408361911773682 = 1.8552446365356445 + 0.01 * 8.559158325195312
Epoch 50, val loss: 1.8580739498138428
Epoch 60, training loss: 1.8973941802978516 = 1.8134467601776123 + 0.01 * 8.394745826721191
Epoch 60, val loss: 1.824578046798706
Epoch 70, training loss: 1.864786982536316 = 1.781943440437317 + 0.01 * 8.28435230255127
Epoch 70, val loss: 1.8019863367080688
Epoch 80, training loss: 1.8264539241790771 = 1.7458405494689941 + 0.01 * 8.061336517333984
Epoch 80, val loss: 1.7706048488616943
Epoch 90, training loss: 1.773213267326355 = 1.6958425045013428 + 0.01 * 7.737081050872803
Epoch 90, val loss: 1.7274391651153564
Epoch 100, training loss: 1.701353669166565 = 1.6258997917175293 + 0.01 * 7.545384883880615
Epoch 100, val loss: 1.6694685220718384
Epoch 110, training loss: 1.6130475997924805 = 1.5385857820510864 + 0.01 * 7.4461798667907715
Epoch 110, val loss: 1.5994774103164673
Epoch 120, training loss: 1.520247220993042 = 1.4468199014663696 + 0.01 * 7.342728614807129
Epoch 120, val loss: 1.5289549827575684
Epoch 130, training loss: 1.432652473449707 = 1.360036849975586 + 0.01 * 7.261565685272217
Epoch 130, val loss: 1.4634071588516235
Epoch 140, training loss: 1.3492392301559448 = 1.2771031856536865 + 0.01 * 7.2136101722717285
Epoch 140, val loss: 1.4031497240066528
Epoch 150, training loss: 1.26698637008667 = 1.1950585842132568 + 0.01 * 7.192774295806885
Epoch 150, val loss: 1.3432538509368896
Epoch 160, training loss: 1.1859991550445557 = 1.1142961978912354 + 0.01 * 7.1703009605407715
Epoch 160, val loss: 1.2846873998641968
Epoch 170, training loss: 1.1086018085479736 = 1.0370732545852661 + 0.01 * 7.152860164642334
Epoch 170, val loss: 1.2300477027893066
Epoch 180, training loss: 1.035862922668457 = 0.9644424319267273 + 0.01 * 7.142051696777344
Epoch 180, val loss: 1.1807866096496582
Epoch 190, training loss: 0.9666541218757629 = 0.8953494429588318 + 0.01 * 7.130466938018799
Epoch 190, val loss: 1.1358413696289062
Epoch 200, training loss: 0.8994250893592834 = 0.8281955718994141 + 0.01 * 7.122951507568359
Epoch 200, val loss: 1.0941531658172607
Epoch 210, training loss: 0.8340694904327393 = 0.762857973575592 + 0.01 * 7.121148586273193
Epoch 210, val loss: 1.056363821029663
Epoch 220, training loss: 0.7720236778259277 = 0.7009296417236328 + 0.01 * 7.109403133392334
Epoch 220, val loss: 1.0242952108383179
Epoch 230, training loss: 0.7150630950927734 = 0.6440386176109314 + 0.01 * 7.102445125579834
Epoch 230, val loss: 0.9997875094413757
Epoch 240, training loss: 0.6640902757644653 = 0.593131422996521 + 0.01 * 7.095883369445801
Epoch 240, val loss: 0.9840856194496155
Epoch 250, training loss: 0.6192381978034973 = 0.548372745513916 + 0.01 * 7.086543083190918
Epoch 250, val loss: 0.9768111705780029
Epoch 260, training loss: 0.5800060033798218 = 0.509253978729248 + 0.01 * 7.075201034545898
Epoch 260, val loss: 0.9766923785209656
Epoch 270, training loss: 0.5455566048622131 = 0.4748170077800751 + 0.01 * 7.073960304260254
Epoch 270, val loss: 0.9820107817649841
Epoch 280, training loss: 0.5144454836845398 = 0.4438484311103821 + 0.01 * 7.05970573425293
Epoch 280, val loss: 0.9912952780723572
Epoch 290, training loss: 0.4855429530143738 = 0.41496819257736206 + 0.01 * 7.057476997375488
Epoch 290, val loss: 1.0034483671188354
Epoch 300, training loss: 0.45727306604385376 = 0.3867727518081665 + 0.01 * 7.050032615661621
Epoch 300, val loss: 1.0173883438110352
Epoch 310, training loss: 0.42854925990104675 = 0.35812628269195557 + 0.01 * 7.042297840118408
Epoch 310, val loss: 1.0324158668518066
Epoch 320, training loss: 0.39908772706985474 = 0.3286808729171753 + 0.01 * 7.04068660736084
Epoch 320, val loss: 1.0484375953674316
Epoch 330, training loss: 0.36934715509414673 = 0.2989770174026489 + 0.01 * 7.037014961242676
Epoch 330, val loss: 1.065540075302124
Epoch 340, training loss: 0.3404015600681305 = 0.27010053396224976 + 0.01 * 7.030102252960205
Epoch 340, val loss: 1.0843756198883057
Epoch 350, training loss: 0.31299731135368347 = 0.2427891045808792 + 0.01 * 7.020820140838623
Epoch 350, val loss: 1.105351209640503
Epoch 360, training loss: 0.2871359884738922 = 0.21691477298736572 + 0.01 * 7.022122383117676
Epoch 360, val loss: 1.128614902496338
Epoch 370, training loss: 0.26195061206817627 = 0.19173918664455414 + 0.01 * 7.021143913269043
Epoch 370, val loss: 1.1541351079940796
Epoch 380, training loss: 0.23707586526870728 = 0.16691961884498596 + 0.01 * 7.015625
Epoch 380, val loss: 1.1823142766952515
Epoch 390, training loss: 0.21348325908184052 = 0.1433163285255432 + 0.01 * 7.016693115234375
Epoch 390, val loss: 1.213553547859192
Epoch 400, training loss: 0.192491814494133 = 0.12235134840011597 + 0.01 * 7.014046669006348
Epoch 400, val loss: 1.247488021850586
Epoch 410, training loss: 0.17475703358650208 = 0.10465043038129807 + 0.01 * 7.0106611251831055
Epoch 410, val loss: 1.2832425832748413
Epoch 420, training loss: 0.15995541214942932 = 0.08999036997556686 + 0.01 * 6.996505260467529
Epoch 420, val loss: 1.3203551769256592
Epoch 430, training loss: 0.14785006642341614 = 0.07786180824041367 + 0.01 * 6.998825550079346
Epoch 430, val loss: 1.3586490154266357
Epoch 440, training loss: 0.1377609819173813 = 0.06779401004314423 + 0.01 * 6.996697425842285
Epoch 440, val loss: 1.3975777626037598
Epoch 450, training loss: 0.1293533444404602 = 0.059415340423583984 + 0.01 * 6.993801593780518
Epoch 450, val loss: 1.4362177848815918
Epoch 460, training loss: 0.12226304411888123 = 0.05243605375289917 + 0.01 * 6.982698917388916
Epoch 460, val loss: 1.4739327430725098
Epoch 470, training loss: 0.11637629568576813 = 0.046607133001089096 + 0.01 * 6.976916313171387
Epoch 470, val loss: 1.5102134943008423
Epoch 480, training loss: 0.1114095002412796 = 0.04170846566557884 + 0.01 * 6.970104217529297
Epoch 480, val loss: 1.5448287725448608
Epoch 490, training loss: 0.10718581080436707 = 0.03754916414618492 + 0.01 * 6.963664531707764
Epoch 490, val loss: 1.577857255935669
Epoch 500, training loss: 0.10363833606243134 = 0.03398432955145836 + 0.01 * 6.965400218963623
Epoch 500, val loss: 1.60930335521698
Epoch 510, training loss: 0.10041199624538422 = 0.030904851853847504 + 0.01 * 6.950714588165283
Epoch 510, val loss: 1.6392370462417603
Epoch 520, training loss: 0.09773802757263184 = 0.02822357416152954 + 0.01 * 6.951445579528809
Epoch 520, val loss: 1.6678271293640137
Epoch 530, training loss: 0.0952737033367157 = 0.025874856859445572 + 0.01 * 6.939884185791016
Epoch 530, val loss: 1.6951651573181152
Epoch 540, training loss: 0.0933656319975853 = 0.023805156350135803 + 0.01 * 6.956048011779785
Epoch 540, val loss: 1.7213413715362549
Epoch 550, training loss: 0.09137605130672455 = 0.021975481882691383 + 0.01 * 6.940057277679443
Epoch 550, val loss: 1.7464090585708618
Epoch 560, training loss: 0.0900576263666153 = 0.020349673926830292 + 0.01 * 6.970795154571533
Epoch 560, val loss: 1.7704496383666992
Epoch 570, training loss: 0.0882372185587883 = 0.018900839611887932 + 0.01 * 6.933638095855713
Epoch 570, val loss: 1.7934677600860596
Epoch 580, training loss: 0.08701552450656891 = 0.017603740096092224 + 0.01 * 6.941178321838379
Epoch 580, val loss: 1.8156681060791016
Epoch 590, training loss: 0.08564957231283188 = 0.01643819361925125 + 0.01 * 6.921137809753418
Epoch 590, val loss: 1.83701491355896
Epoch 600, training loss: 0.08448046445846558 = 0.015387993305921555 + 0.01 * 6.909247398376465
Epoch 600, val loss: 1.857621669769287
Epoch 610, training loss: 0.08364561200141907 = 0.014438420534133911 + 0.01 * 6.920719623565674
Epoch 610, val loss: 1.8774789571762085
Epoch 620, training loss: 0.08290661871433258 = 0.013578108511865139 + 0.01 * 6.9328508377075195
Epoch 620, val loss: 1.8965734243392944
Epoch 630, training loss: 0.08177726715803146 = 0.012797136791050434 + 0.01 * 6.898013114929199
Epoch 630, val loss: 1.9150598049163818
Epoch 640, training loss: 0.08108542114496231 = 0.012085611931979656 + 0.01 * 6.899981498718262
Epoch 640, val loss: 1.9328233003616333
Epoch 650, training loss: 0.08031654357910156 = 0.011435954831540585 + 0.01 * 6.888058662414551
Epoch 650, val loss: 1.9500101804733276
Epoch 660, training loss: 0.07974502444267273 = 0.010840695351362228 + 0.01 * 6.890432834625244
Epoch 660, val loss: 1.966691493988037
Epoch 670, training loss: 0.07930522412061691 = 0.010294166393578053 + 0.01 * 6.901105880737305
Epoch 670, val loss: 1.9827817678451538
Epoch 680, training loss: 0.07872819900512695 = 0.009791533462703228 + 0.01 * 6.893667221069336
Epoch 680, val loss: 1.9983595609664917
Epoch 690, training loss: 0.07812076061964035 = 0.009328363463282585 + 0.01 * 6.879239559173584
Epoch 690, val loss: 2.013462781906128
Epoch 700, training loss: 0.07756167650222778 = 0.008900115266442299 + 0.01 * 6.866156101226807
Epoch 700, val loss: 2.028045177459717
Epoch 710, training loss: 0.07759591937065125 = 0.008503347635269165 + 0.01 * 6.909257411956787
Epoch 710, val loss: 2.0422263145446777
Epoch 720, training loss: 0.0768558457493782 = 0.008136025629937649 + 0.01 * 6.871982097625732
Epoch 720, val loss: 2.055934429168701
Epoch 730, training loss: 0.07659608870744705 = 0.0077946241945028305 + 0.01 * 6.880146503448486
Epoch 730, val loss: 2.0691986083984375
Epoch 740, training loss: 0.07622691988945007 = 0.00747730303555727 + 0.01 * 6.874961853027344
Epoch 740, val loss: 2.0821545124053955
Epoch 750, training loss: 0.0757002979516983 = 0.0071812644600868225 + 0.01 * 6.851903438568115
Epoch 750, val loss: 2.0947136878967285
Epoch 760, training loss: 0.07552998512983322 = 0.006904534995555878 + 0.01 * 6.862545490264893
Epoch 760, val loss: 2.1068978309631348
Epoch 770, training loss: 0.07511435449123383 = 0.006645700428634882 + 0.01 * 6.846865653991699
Epoch 770, val loss: 2.1188716888427734
Epoch 780, training loss: 0.07481053471565247 = 0.00640285387635231 + 0.01 * 6.840768814086914
Epoch 780, val loss: 2.130444288253784
Epoch 790, training loss: 0.07471901178359985 = 0.00617518974468112 + 0.01 * 6.854382514953613
Epoch 790, val loss: 2.1416385173797607
Epoch 800, training loss: 0.07427258044481277 = 0.0059614041820168495 + 0.01 * 6.831118106842041
Epoch 800, val loss: 2.1526119709014893
Epoch 810, training loss: 0.07421866059303284 = 0.005760011728852987 + 0.01 * 6.845864772796631
Epoch 810, val loss: 2.1632277965545654
Epoch 820, training loss: 0.07407934963703156 = 0.005570576060563326 + 0.01 * 6.85087776184082
Epoch 820, val loss: 2.173612117767334
Epoch 830, training loss: 0.07369133830070496 = 0.005391933023929596 + 0.01 * 6.8299407958984375
Epoch 830, val loss: 2.1837165355682373
Epoch 840, training loss: 0.07362308353185654 = 0.005222989711910486 + 0.01 * 6.840009689331055
Epoch 840, val loss: 2.193472146987915
Epoch 850, training loss: 0.07325628399848938 = 0.0050635733641684055 + 0.01 * 6.819271087646484
Epoch 850, val loss: 2.2031090259552
Epoch 860, training loss: 0.07310160994529724 = 0.0049123940989375114 + 0.01 * 6.81892204284668
Epoch 860, val loss: 2.212453603744507
Epoch 870, training loss: 0.07305106520652771 = 0.0047692772932350636 + 0.01 * 6.828178405761719
Epoch 870, val loss: 2.221501588821411
Epoch 880, training loss: 0.07284228503704071 = 0.004633912816643715 + 0.01 * 6.820837497711182
Epoch 880, val loss: 2.2303991317749023
Epoch 890, training loss: 0.0728718638420105 = 0.00450507365167141 + 0.01 * 6.836678981781006
Epoch 890, val loss: 2.239042043685913
Epoch 900, training loss: 0.07246332615613937 = 0.004383021499961615 + 0.01 * 6.80803108215332
Epoch 900, val loss: 2.2475528717041016
Epoch 910, training loss: 0.07222998887300491 = 0.0042665996588766575 + 0.01 * 6.796339511871338
Epoch 910, val loss: 2.255734920501709
Epoch 920, training loss: 0.07213521003723145 = 0.004156073555350304 + 0.01 * 6.797913551330566
Epoch 920, val loss: 2.2637805938720703
Epoch 930, training loss: 0.07206019014120102 = 0.004050718620419502 + 0.01 * 6.800947189331055
Epoch 930, val loss: 2.271684408187866
Epoch 940, training loss: 0.07215387374162674 = 0.003950267564505339 + 0.01 * 6.8203606605529785
Epoch 940, val loss: 2.2793943881988525
Epoch 950, training loss: 0.07183420658111572 = 0.003854393260553479 + 0.01 * 6.797981262207031
Epoch 950, val loss: 2.2869040966033936
Epoch 960, training loss: 0.07148924469947815 = 0.0037628300487995148 + 0.01 * 6.772641181945801
Epoch 960, val loss: 2.294180154800415
Epoch 970, training loss: 0.0718693882226944 = 0.0036753437016159296 + 0.01 * 6.8194050788879395
Epoch 970, val loss: 2.3013052940368652
Epoch 980, training loss: 0.0714852586388588 = 0.003591792657971382 + 0.01 * 6.789346694946289
Epoch 980, val loss: 2.308356285095215
Epoch 990, training loss: 0.07125017791986465 = 0.0035116064827889204 + 0.01 * 6.773857116699219
Epoch 990, val loss: 2.315030336380005
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6703703703703704
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 2.034888982772827 = 1.9489210844039917 + 0.01 * 8.596799850463867
Epoch 0, val loss: 1.9544254541397095
Epoch 10, training loss: 2.024949073791504 = 1.938981533050537 + 0.01 * 8.596745491027832
Epoch 10, val loss: 1.9441367387771606
Epoch 20, training loss: 2.0126917362213135 = 1.9267264604568481 + 0.01 * 8.596537590026855
Epoch 20, val loss: 1.9313760995864868
Epoch 30, training loss: 1.9953233003616333 = 1.9093639850616455 + 0.01 * 8.595931053161621
Epoch 30, val loss: 1.913502812385559
Epoch 40, training loss: 1.9696354866027832 = 1.883705973625183 + 0.01 * 8.592954635620117
Epoch 40, val loss: 1.8877276182174683
Epoch 50, training loss: 1.9334771633148193 = 1.8477652072906494 + 0.01 * 8.57119083404541
Epoch 50, val loss: 1.853505253791809
Epoch 60, training loss: 1.8923826217651367 = 1.807819128036499 + 0.01 * 8.456351280212402
Epoch 60, val loss: 1.819273829460144
Epoch 70, training loss: 1.8569416999816895 = 1.774822473526001 + 0.01 * 8.211917877197266
Epoch 70, val loss: 1.7927347421646118
Epoch 80, training loss: 1.8149380683898926 = 1.7350242137908936 + 0.01 * 7.991385459899902
Epoch 80, val loss: 1.7577588558197021
Epoch 90, training loss: 1.7566434144973755 = 1.6791086196899414 + 0.01 * 7.753481864929199
Epoch 90, val loss: 1.7113134860992432
Epoch 100, training loss: 1.6783087253570557 = 1.6028172969818115 + 0.01 * 7.549149036407471
Epoch 100, val loss: 1.6506459712982178
Epoch 110, training loss: 1.5843443870544434 = 1.5097399950027466 + 0.01 * 7.460445404052734
Epoch 110, val loss: 1.5766773223876953
Epoch 120, training loss: 1.484643578529358 = 1.410378098487854 + 0.01 * 7.4265456199646
Epoch 120, val loss: 1.4981472492218018
Epoch 130, training loss: 1.3850468397140503 = 1.3109833002090454 + 0.01 * 7.4063520431518555
Epoch 130, val loss: 1.4208014011383057
Epoch 140, training loss: 1.2865314483642578 = 1.2126808166503906 + 0.01 * 7.385061264038086
Epoch 140, val loss: 1.3462494611740112
Epoch 150, training loss: 1.1903555393218994 = 1.1166738271713257 + 0.01 * 7.368168830871582
Epoch 150, val loss: 1.274821400642395
Epoch 160, training loss: 1.0990530252456665 = 1.025516390800476 + 0.01 * 7.353666305541992
Epoch 160, val loss: 1.2082579135894775
Epoch 170, training loss: 1.0139975547790527 = 0.940657913684845 + 0.01 * 7.333958148956299
Epoch 170, val loss: 1.1469340324401855
Epoch 180, training loss: 0.9355672597885132 = 0.8625542521476746 + 0.01 * 7.301299571990967
Epoch 180, val loss: 1.0911142826080322
Epoch 190, training loss: 0.8642249703407288 = 0.7916346788406372 + 0.01 * 7.259029865264893
Epoch 190, val loss: 1.0417331457138062
Epoch 200, training loss: 0.8007384538650513 = 0.7285804748535156 + 0.01 * 7.215795516967773
Epoch 200, val loss: 0.9999523162841797
Epoch 210, training loss: 0.74506014585495 = 0.673205554485321 + 0.01 * 7.185459613800049
Epoch 210, val loss: 0.9669964909553528
Epoch 220, training loss: 0.695265531539917 = 0.6236515641212463 + 0.01 * 7.161395072937012
Epoch 220, val loss: 0.9416245222091675
Epoch 230, training loss: 0.6487652063369751 = 0.5773387551307678 + 0.01 * 7.142642021179199
Epoch 230, val loss: 0.922095537185669
Epoch 240, training loss: 0.6033170819282532 = 0.5320402383804321 + 0.01 * 7.127682685852051
Epoch 240, val loss: 0.9062032103538513
Epoch 250, training loss: 0.5575177073478699 = 0.48633065819740295 + 0.01 * 7.118703365325928
Epoch 250, val loss: 0.8925995826721191
Epoch 260, training loss: 0.5108155608177185 = 0.43969687819480896 + 0.01 * 7.111867427825928
Epoch 260, val loss: 0.8813029527664185
Epoch 270, training loss: 0.4635026156902313 = 0.392473042011261 + 0.01 * 7.102957725524902
Epoch 270, val loss: 0.8723861575126648
Epoch 280, training loss: 0.41661134362220764 = 0.34557604789733887 + 0.01 * 7.103529930114746
Epoch 280, val loss: 0.866141140460968
Epoch 290, training loss: 0.37126559019088745 = 0.30033716559410095 + 0.01 * 7.0928425788879395
Epoch 290, val loss: 0.8623110055923462
Epoch 300, training loss: 0.3289562165737152 = 0.25807100534439087 + 0.01 * 7.0885210037231445
Epoch 300, val loss: 0.8608960509300232
Epoch 310, training loss: 0.2907256782054901 = 0.2198866754770279 + 0.01 * 7.083901405334473
Epoch 310, val loss: 0.861881673336029
Epoch 320, training loss: 0.25727325677871704 = 0.18647423386573792 + 0.01 * 7.079900741577148
Epoch 320, val loss: 0.8652584552764893
Epoch 330, training loss: 0.22880378365516663 = 0.15797263383865356 + 0.01 * 7.083116054534912
Epoch 330, val loss: 0.8710753917694092
Epoch 340, training loss: 0.20482328534126282 = 0.1340806931257248 + 0.01 * 7.0742597579956055
Epoch 340, val loss: 0.8793254494667053
Epoch 350, training loss: 0.18490925431251526 = 0.11422238498926163 + 0.01 * 7.068687438964844
Epoch 350, val loss: 0.8898127675056458
Epoch 360, training loss: 0.16840893030166626 = 0.09777547419071198 + 0.01 * 7.0633463859558105
Epoch 360, val loss: 0.9021373391151428
Epoch 370, training loss: 0.1547771394252777 = 0.0841706395149231 + 0.01 * 7.060649871826172
Epoch 370, val loss: 0.9158702492713928
Epoch 380, training loss: 0.14344993233680725 = 0.07290872186422348 + 0.01 * 7.054121971130371
Epoch 380, val loss: 0.9304990768432617
Epoch 390, training loss: 0.1340436041355133 = 0.06355178356170654 + 0.01 * 7.049181938171387
Epoch 390, val loss: 0.9458019733428955
Epoch 400, training loss: 0.1263473927974701 = 0.055737823247909546 + 0.01 * 7.0609564781188965
Epoch 400, val loss: 0.9614865779876709
Epoch 410, training loss: 0.11957502365112305 = 0.04917856678366661 + 0.01 * 7.039645671844482
Epoch 410, val loss: 0.9773370623588562
Epoch 420, training loss: 0.11393874138593674 = 0.04362925887107849 + 0.01 * 7.030948162078857
Epoch 420, val loss: 0.9932109713554382
Epoch 430, training loss: 0.10927117615938187 = 0.03890106827020645 + 0.01 * 7.03701114654541
Epoch 430, val loss: 1.0090144872665405
Epoch 440, training loss: 0.1051192507147789 = 0.03485707938671112 + 0.01 * 7.026216983795166
Epoch 440, val loss: 1.0246092081069946
Epoch 450, training loss: 0.10152500867843628 = 0.031379904597997665 + 0.01 * 7.014510631561279
Epoch 450, val loss: 1.0399516820907593
Epoch 460, training loss: 0.0985410213470459 = 0.02837672084569931 + 0.01 * 7.016429901123047
Epoch 460, val loss: 1.0549981594085693
Epoch 470, training loss: 0.09582005441188812 = 0.02577587030827999 + 0.01 * 7.00441837310791
Epoch 470, val loss: 1.0696207284927368
Epoch 480, training loss: 0.09347766637802124 = 0.023513324558734894 + 0.01 * 6.996434211730957
Epoch 480, val loss: 1.083832859992981
Epoch 490, training loss: 0.09143338352441788 = 0.021536288782954216 + 0.01 * 6.989709377288818
Epoch 490, val loss: 1.0975861549377441
Epoch 500, training loss: 0.08961041271686554 = 0.019801268354058266 + 0.01 * 6.980915069580078
Epoch 500, val loss: 1.110886812210083
Epoch 510, training loss: 0.08830274641513824 = 0.01827133819460869 + 0.01 * 7.003141403198242
Epoch 510, val loss: 1.1237905025482178
Epoch 520, training loss: 0.08670953661203384 = 0.01691853068768978 + 0.01 * 6.979100227355957
Epoch 520, val loss: 1.1361894607543945
Epoch 530, training loss: 0.08544395118951797 = 0.015716062858700752 + 0.01 * 6.9727888107299805
Epoch 530, val loss: 1.1482189893722534
Epoch 540, training loss: 0.08418528735637665 = 0.014641852118074894 + 0.01 * 6.954343795776367
Epoch 540, val loss: 1.1598633527755737
Epoch 550, training loss: 0.08344672620296478 = 0.01367923617362976 + 0.01 * 6.976748943328857
Epoch 550, val loss: 1.1711214780807495
Epoch 560, training loss: 0.0823335126042366 = 0.012813069857656956 + 0.01 * 6.952044486999512
Epoch 560, val loss: 1.1820318698883057
Epoch 570, training loss: 0.08144757151603699 = 0.012030287645757198 + 0.01 * 6.941728591918945
Epoch 570, val loss: 1.1926288604736328
Epoch 580, training loss: 0.08077038079500198 = 0.011321621015667915 + 0.01 * 6.944876670837402
Epoch 580, val loss: 1.2028535604476929
Epoch 590, training loss: 0.08006519824266434 = 0.010679316706955433 + 0.01 * 6.9385881423950195
Epoch 590, val loss: 1.2127962112426758
Epoch 600, training loss: 0.07931166142225266 = 0.01009343471378088 + 0.01 * 6.921823024749756
Epoch 600, val loss: 1.2224458456039429
Epoch 610, training loss: 0.07873815298080444 = 0.009559234604239464 + 0.01 * 6.917891979217529
Epoch 610, val loss: 1.2317472696304321
Epoch 620, training loss: 0.07824000716209412 = 0.009071079082787037 + 0.01 * 6.916893005371094
Epoch 620, val loss: 1.2407476902008057
Epoch 630, training loss: 0.07790409028530121 = 0.008623261004686356 + 0.01 * 6.928082466125488
Epoch 630, val loss: 1.2494767904281616
Epoch 640, training loss: 0.07741773128509521 = 0.008211548440158367 + 0.01 * 6.920618534088135
Epoch 640, val loss: 1.257936954498291
Epoch 650, training loss: 0.07681714743375778 = 0.00783132016658783 + 0.01 * 6.898582935333252
Epoch 650, val loss: 1.2661035060882568
Epoch 660, training loss: 0.07641355693340302 = 0.007479614578187466 + 0.01 * 6.893394470214844
Epoch 660, val loss: 1.2741000652313232
Epoch 670, training loss: 0.0760856419801712 = 0.007153350859880447 + 0.01 * 6.893229961395264
Epoch 670, val loss: 1.2818492650985718
Epoch 680, training loss: 0.07561813294887543 = 0.0068507008254528046 + 0.01 * 6.876743316650391
Epoch 680, val loss: 1.2893939018249512
Epoch 690, training loss: 0.07550158351659775 = 0.006569534540176392 + 0.01 * 6.893204689025879
Epoch 690, val loss: 1.2966464757919312
Epoch 700, training loss: 0.07503511011600494 = 0.006307484582066536 + 0.01 * 6.872762680053711
Epoch 700, val loss: 1.3037532567977905
Epoch 710, training loss: 0.0746990516781807 = 0.006062537897378206 + 0.01 * 6.863651752471924
Epoch 710, val loss: 1.3106504678726196
Epoch 720, training loss: 0.07446036487817764 = 0.0058335051871836185 + 0.01 * 6.8626861572265625
Epoch 720, val loss: 1.3173613548278809
Epoch 730, training loss: 0.074156254529953 = 0.005619415082037449 + 0.01 * 6.853684425354004
Epoch 730, val loss: 1.323965072631836
Epoch 740, training loss: 0.07405997812747955 = 0.005418790969997644 + 0.01 * 6.864119052886963
Epoch 740, val loss: 1.3303141593933105
Epoch 750, training loss: 0.073961041867733 = 0.005229849833995104 + 0.01 * 6.873119354248047
Epoch 750, val loss: 1.3365126848220825
Epoch 760, training loss: 0.07346077263355255 = 0.005052624270319939 + 0.01 * 6.840814590454102
Epoch 760, val loss: 1.342549443244934
Epoch 770, training loss: 0.07318128645420074 = 0.004885602276772261 + 0.01 * 6.829568862915039
Epoch 770, val loss: 1.348443627357483
Epoch 780, training loss: 0.07312124222517014 = 0.0047280569560825825 + 0.01 * 6.839318752288818
Epoch 780, val loss: 1.354172706604004
Epoch 790, training loss: 0.07278644293546677 = 0.004579410422593355 + 0.01 * 6.820703506469727
Epoch 790, val loss: 1.3597832918167114
Epoch 800, training loss: 0.07273010909557343 = 0.004438605159521103 + 0.01 * 6.829150676727295
Epoch 800, val loss: 1.3652962446212769
Epoch 810, training loss: 0.07263492047786713 = 0.00430570961907506 + 0.01 * 6.832921028137207
Epoch 810, val loss: 1.3705774545669556
Epoch 820, training loss: 0.07231845706701279 = 0.004179447423666716 + 0.01 * 6.813901424407959
Epoch 820, val loss: 1.3758403062820435
Epoch 830, training loss: 0.07232356816530228 = 0.004060014151036739 + 0.01 * 6.826355457305908
Epoch 830, val loss: 1.3809140920639038
Epoch 840, training loss: 0.07218274474143982 = 0.003946566954255104 + 0.01 * 6.823618412017822
Epoch 840, val loss: 1.3858556747436523
Epoch 850, training loss: 0.07202431559562683 = 0.0038387144450098276 + 0.01 * 6.818559646606445
Epoch 850, val loss: 1.3907675743103027
Epoch 860, training loss: 0.07184629142284393 = 0.003736201673746109 + 0.01 * 6.811008930206299
Epoch 860, val loss: 1.3954567909240723
Epoch 870, training loss: 0.07161273062229156 = 0.003638508031144738 + 0.01 * 6.797422409057617
Epoch 870, val loss: 1.4000937938690186
Epoch 880, training loss: 0.07154501974582672 = 0.0035455410834401846 + 0.01 * 6.799948692321777
Epoch 880, val loss: 1.4046411514282227
Epoch 890, training loss: 0.07149842381477356 = 0.003456783713772893 + 0.01 * 6.804163932800293
Epoch 890, val loss: 1.4090577363967896
Epoch 900, training loss: 0.07141323387622833 = 0.0033722687512636185 + 0.01 * 6.8040971755981445
Epoch 900, val loss: 1.4133926630020142
Epoch 910, training loss: 0.07132580876350403 = 0.003291411092504859 + 0.01 * 6.803439617156982
Epoch 910, val loss: 1.4176101684570312
Epoch 920, training loss: 0.07105349749326706 = 0.003214200958609581 + 0.01 * 6.783929824829102
Epoch 920, val loss: 1.4217023849487305
Epoch 930, training loss: 0.0709640309214592 = 0.0031401952728629112 + 0.01 * 6.782383441925049
Epoch 930, val loss: 1.4258424043655396
Epoch 940, training loss: 0.07097946107387543 = 0.0030696236062794924 + 0.01 * 6.7909836769104
Epoch 940, val loss: 1.4297223091125488
Epoch 950, training loss: 0.07058466225862503 = 0.0030020042322576046 + 0.01 * 6.758265972137451
Epoch 950, val loss: 1.4336169958114624
Epoch 960, training loss: 0.07060438394546509 = 0.00293719326145947 + 0.01 * 6.766719818115234
Epoch 960, val loss: 1.4374165534973145
Epoch 970, training loss: 0.07054471969604492 = 0.0028749671764671803 + 0.01 * 6.766974925994873
Epoch 970, val loss: 1.4410545825958252
Epoch 980, training loss: 0.07058928161859512 = 0.002815109444782138 + 0.01 * 6.777417182922363
Epoch 980, val loss: 1.4447696208953857
Epoch 990, training loss: 0.07056619971990585 = 0.0027579732704907656 + 0.01 * 6.78082275390625
Epoch 990, val loss: 1.448309302330017
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.7996837111228255
=== training gcn model ===
Epoch 0, training loss: 2.0217444896698 = 1.9357763528823853 + 0.01 * 8.596809387207031
Epoch 0, val loss: 1.9410983324050903
Epoch 10, training loss: 2.012328624725342 = 1.926361083984375 + 0.01 * 8.596745491027832
Epoch 10, val loss: 1.930749773979187
Epoch 20, training loss: 2.000469923019409 = 1.9145047664642334 + 0.01 * 8.596524238586426
Epoch 20, val loss: 1.9174002408981323
Epoch 30, training loss: 1.9836002588272095 = 1.8976414203643799 + 0.01 * 8.595880508422852
Epoch 30, val loss: 1.8983073234558105
Epoch 40, training loss: 1.9588176012039185 = 1.8728899955749512 + 0.01 * 8.592761993408203
Epoch 40, val loss: 1.8706053495407104
Epoch 50, training loss: 1.9253976345062256 = 1.8396658897399902 + 0.01 * 8.573175430297852
Epoch 50, val loss: 1.8352473974227905
Epoch 60, training loss: 1.890511393547058 = 1.8057411909103394 + 0.01 * 8.477019309997559
Epoch 60, val loss: 1.80373215675354
Epoch 70, training loss: 1.8600424528121948 = 1.7768887281417847 + 0.01 * 8.315377235412598
Epoch 70, val loss: 1.7821346521377563
Epoch 80, training loss: 1.8195546865463257 = 1.7388033866882324 + 0.01 * 8.0751314163208
Epoch 80, val loss: 1.7540456056594849
Epoch 90, training loss: 1.7623333930969238 = 1.6851352453231812 + 0.01 * 7.71981143951416
Epoch 90, val loss: 1.7113475799560547
Epoch 100, training loss: 1.6858808994293213 = 1.6106210947036743 + 0.01 * 7.525980472564697
Epoch 100, val loss: 1.6501977443695068
Epoch 110, training loss: 1.5927908420562744 = 1.5184053182601929 + 0.01 * 7.438558101654053
Epoch 110, val loss: 1.5757958889007568
Epoch 120, training loss: 1.490522027015686 = 1.4169944524765015 + 0.01 * 7.352753162384033
Epoch 120, val loss: 1.4958281517028809
Epoch 130, training loss: 1.3849833011627197 = 1.3119657039642334 + 0.01 * 7.301753997802734
Epoch 130, val loss: 1.4160963296890259
Epoch 140, training loss: 1.2765164375305176 = 1.2038239240646362 + 0.01 * 7.269256114959717
Epoch 140, val loss: 1.336350917816162
Epoch 150, training loss: 1.1674753427505493 = 1.095200538635254 + 0.01 * 7.227475643157959
Epoch 150, val loss: 1.2579108476638794
Epoch 160, training loss: 1.06282639503479 = 0.9909392595291138 + 0.01 * 7.188709259033203
Epoch 160, val loss: 1.184994101524353
Epoch 170, training loss: 0.9674995541572571 = 0.8958351612091064 + 0.01 * 7.166440486907959
Epoch 170, val loss: 1.1208018064498901
Epoch 180, training loss: 0.8845555186271667 = 0.813057541847229 + 0.01 * 7.14979887008667
Epoch 180, val loss: 1.0678220987319946
Epoch 190, training loss: 0.8152645230293274 = 0.7439185976982117 + 0.01 * 7.134591579437256
Epoch 190, val loss: 1.0272032022476196
Epoch 200, training loss: 0.75808185338974 = 0.686860978603363 + 0.01 * 7.122089862823486
Epoch 200, val loss: 0.9983909130096436
Epoch 210, training loss: 0.7093198895454407 = 0.6381279826164246 + 0.01 * 7.119191646575928
Epoch 210, val loss: 0.9783233404159546
Epoch 220, training loss: 0.6648519039154053 = 0.5937813520431519 + 0.01 * 7.107053756713867
Epoch 220, val loss: 0.9633628726005554
Epoch 230, training loss: 0.6216691732406616 = 0.5506696701049805 + 0.01 * 7.099952697753906
Epoch 230, val loss: 0.9508846998214722
Epoch 240, training loss: 0.57778400182724 = 0.5068322420120239 + 0.01 * 7.095175743103027
Epoch 240, val loss: 0.9392334818840027
Epoch 250, training loss: 0.53240966796875 = 0.46149274706840515 + 0.01 * 7.0916948318481445
Epoch 250, val loss: 0.9276697635650635
Epoch 260, training loss: 0.4857694208621979 = 0.4148825407028198 + 0.01 * 7.088688373565674
Epoch 260, val loss: 0.9168511033058167
Epoch 270, training loss: 0.43868106603622437 = 0.36785078048706055 + 0.01 * 7.0830302238464355
Epoch 270, val loss: 0.9071295857429504
Epoch 280, training loss: 0.3924499452114105 = 0.32162293791770935 + 0.01 * 7.082700252532959
Epoch 280, val loss: 0.8992056846618652
Epoch 290, training loss: 0.34853219985961914 = 0.27775442600250244 + 0.01 * 7.077775955200195
Epoch 290, val loss: 0.8941437005996704
Epoch 300, training loss: 0.3083745837211609 = 0.2376648336648941 + 0.01 * 7.070974349975586
Epoch 300, val loss: 0.8930162191390991
Epoch 310, training loss: 0.2732084393501282 = 0.20224981009960175 + 0.01 * 7.095861434936523
Epoch 310, val loss: 0.8963691592216492
Epoch 320, training loss: 0.24246805906295776 = 0.17180518805980682 + 0.01 * 7.066287994384766
Epoch 320, val loss: 0.9038032293319702
Epoch 330, training loss: 0.21664243936538696 = 0.14603286981582642 + 0.01 * 7.0609564781188965
Epoch 330, val loss: 0.9150674343109131
Epoch 340, training loss: 0.19497966766357422 = 0.12442536652088165 + 0.01 * 7.055429458618164
Epoch 340, val loss: 0.9295036792755127
Epoch 350, training loss: 0.17695336043834686 = 0.10643630474805832 + 0.01 * 7.051705837249756
Epoch 350, val loss: 0.9461983442306519
Epoch 360, training loss: 0.16199126839637756 = 0.09151826053857803 + 0.01 * 7.047299861907959
Epoch 360, val loss: 0.9644777774810791
Epoch 370, training loss: 0.14973163604736328 = 0.07916292548179626 + 0.01 * 7.0568718910217285
Epoch 370, val loss: 0.9836485981941223
Epoch 380, training loss: 0.13937202095985413 = 0.06892822682857513 + 0.01 * 7.0443806648254395
Epoch 380, val loss: 1.0031237602233887
Epoch 390, training loss: 0.13078054785728455 = 0.06040741503238678 + 0.01 * 7.037313938140869
Epoch 390, val loss: 1.022641658782959
Epoch 400, training loss: 0.12358184158802032 = 0.05327156186103821 + 0.01 * 7.031027793884277
Epoch 400, val loss: 1.041912317276001
Epoch 410, training loss: 0.11755381524562836 = 0.04725838080048561 + 0.01 * 7.029543399810791
Epoch 410, val loss: 1.060854434967041
Epoch 420, training loss: 0.11244465410709381 = 0.042165081948041916 + 0.01 * 7.027957916259766
Epoch 420, val loss: 1.0792933702468872
Epoch 430, training loss: 0.1080159991979599 = 0.03782486170530319 + 0.01 * 7.019113540649414
Epoch 430, val loss: 1.0972410440444946
Epoch 440, training loss: 0.10423393547534943 = 0.034102048724889755 + 0.01 * 7.013188362121582
Epoch 440, val loss: 1.1146641969680786
Epoch 450, training loss: 0.10117997974157333 = 0.030890261754393578 + 0.01 * 7.0289716720581055
Epoch 450, val loss: 1.1315600872039795
Epoch 460, training loss: 0.09814632683992386 = 0.02810833789408207 + 0.01 * 7.0037994384765625
Epoch 460, val loss: 1.1478235721588135
Epoch 470, training loss: 0.09568675607442856 = 0.025683166459202766 + 0.01 * 7.000359535217285
Epoch 470, val loss: 1.1635841131210327
Epoch 480, training loss: 0.09351461380720139 = 0.023557009175419807 + 0.01 * 6.995760440826416
Epoch 480, val loss: 1.178848147392273
Epoch 490, training loss: 0.09169711172580719 = 0.02168440818786621 + 0.01 * 7.001270771026611
Epoch 490, val loss: 1.1935895681381226
Epoch 500, training loss: 0.08993960916996002 = 0.020029157400131226 + 0.01 * 6.9910454750061035
Epoch 500, val loss: 1.2078200578689575
Epoch 510, training loss: 0.08835920691490173 = 0.0185594130307436 + 0.01 * 6.979979515075684
Epoch 510, val loss: 1.2215923070907593
Epoch 520, training loss: 0.08709650486707687 = 0.01724863052368164 + 0.01 * 6.984787464141846
Epoch 520, val loss: 1.2349560260772705
Epoch 530, training loss: 0.08584465831518173 = 0.01607584021985531 + 0.01 * 6.976881980895996
Epoch 530, val loss: 1.2477983236312866
Epoch 540, training loss: 0.0847209170460701 = 0.015022299252450466 + 0.01 * 6.96986198425293
Epoch 540, val loss: 1.2602380514144897
Epoch 550, training loss: 0.08375819772481918 = 0.014072156511247158 + 0.01 * 6.96860408782959
Epoch 550, val loss: 1.2723041772842407
Epoch 560, training loss: 0.08280590921640396 = 0.01321353018283844 + 0.01 * 6.959238052368164
Epoch 560, val loss: 1.2839314937591553
Epoch 570, training loss: 0.0819951519370079 = 0.012435122393071651 + 0.01 * 6.956003189086914
Epoch 570, val loss: 1.2952275276184082
Epoch 580, training loss: 0.0812636986374855 = 0.011726675555109978 + 0.01 * 6.953701972961426
Epoch 580, val loss: 1.3061405420303345
Epoch 590, training loss: 0.08055523037910461 = 0.011080573312938213 + 0.01 * 6.947465896606445
Epoch 590, val loss: 1.3167232275009155
Epoch 600, training loss: 0.07995988428592682 = 0.010489700362086296 + 0.01 * 6.947018623352051
Epoch 600, val loss: 1.3269726037979126
Epoch 610, training loss: 0.07931755483150482 = 0.009947697632014751 + 0.01 * 6.936986446380615
Epoch 610, val loss: 1.3369457721710205
Epoch 620, training loss: 0.07899630814790726 = 0.009449323639273643 + 0.01 * 6.95469856262207
Epoch 620, val loss: 1.346739649772644
Epoch 630, training loss: 0.07827345281839371 = 0.008990928530693054 + 0.01 * 6.928252220153809
Epoch 630, val loss: 1.3560093641281128
Epoch 640, training loss: 0.07784189283847809 = 0.00856767874211073 + 0.01 * 6.927422046661377
Epoch 640, val loss: 1.3651068210601807
Epoch 650, training loss: 0.0774599239230156 = 0.008175716735422611 + 0.01 * 6.9284210205078125
Epoch 650, val loss: 1.3739618062973022
Epoch 660, training loss: 0.07698597759008408 = 0.007812471594661474 + 0.01 * 6.917351245880127
Epoch 660, val loss: 1.3825926780700684
Epoch 670, training loss: 0.07684142887592316 = 0.007474967278540134 + 0.01 * 6.936646461486816
Epoch 670, val loss: 1.3909814357757568
Epoch 680, training loss: 0.0763494148850441 = 0.007161537650972605 + 0.01 * 6.918787956237793
Epoch 680, val loss: 1.3990336656570435
Epoch 690, training loss: 0.07590389251708984 = 0.006869306322187185 + 0.01 * 6.903458595275879
Epoch 690, val loss: 1.4069647789001465
Epoch 700, training loss: 0.07560747116804123 = 0.006596340332180262 + 0.01 * 6.901113033294678
Epoch 700, val loss: 1.4147446155548096
Epoch 710, training loss: 0.07560154050588608 = 0.006341248285025358 + 0.01 * 6.926029205322266
Epoch 710, val loss: 1.4223179817199707
Epoch 720, training loss: 0.07502822577953339 = 0.006102635059505701 + 0.01 * 6.892559051513672
Epoch 720, val loss: 1.429439902305603
Epoch 730, training loss: 0.07476188987493515 = 0.005879085045307875 + 0.01 * 6.888280868530273
Epoch 730, val loss: 1.4366109371185303
Epoch 740, training loss: 0.0745282769203186 = 0.005669067148119211 + 0.01 * 6.885921478271484
Epoch 740, val loss: 1.4436044692993164
Epoch 750, training loss: 0.07432374358177185 = 0.005471165291965008 + 0.01 * 6.885258197784424
Epoch 750, val loss: 1.4503904581069946
Epoch 760, training loss: 0.07403725385665894 = 0.005285104736685753 + 0.01 * 6.875214576721191
Epoch 760, val loss: 1.4569629430770874
Epoch 770, training loss: 0.0738176554441452 = 0.005109804682433605 + 0.01 * 6.870785713195801
Epoch 770, val loss: 1.4633742570877075
Epoch 780, training loss: 0.07364890724420547 = 0.004944357089698315 + 0.01 * 6.870455265045166
Epoch 780, val loss: 1.4697082042694092
Epoch 790, training loss: 0.07352741062641144 = 0.004788069054484367 + 0.01 * 6.873933792114258
Epoch 790, val loss: 1.475767731666565
Epoch 800, training loss: 0.07327940315008163 = 0.004640125669538975 + 0.01 * 6.863927841186523
Epoch 800, val loss: 1.4817131757736206
Epoch 810, training loss: 0.07312989234924316 = 0.004500170703977346 + 0.01 * 6.862972259521484
Epoch 810, val loss: 1.4875198602676392
Epoch 820, training loss: 0.07303445041179657 = 0.004367347806692123 + 0.01 * 6.8667097091674805
Epoch 820, val loss: 1.4931384325027466
Epoch 830, training loss: 0.07279559224843979 = 0.00424156803637743 + 0.01 * 6.85540246963501
Epoch 830, val loss: 1.4986919164657593
Epoch 840, training loss: 0.07271257787942886 = 0.004122002050280571 + 0.01 * 6.859057426452637
Epoch 840, val loss: 1.5040724277496338
Epoch 850, training loss: 0.07246073335409164 = 0.0040085529908537865 + 0.01 * 6.845218181610107
Epoch 850, val loss: 1.5092657804489136
Epoch 860, training loss: 0.07231033593416214 = 0.003900418756529689 + 0.01 * 6.840992450714111
Epoch 860, val loss: 1.514423131942749
Epoch 870, training loss: 0.07219294458627701 = 0.0037976729217916727 + 0.01 * 6.839527606964111
Epoch 870, val loss: 1.5194555521011353
Epoch 880, training loss: 0.07216627895832062 = 0.0036995543632656336 + 0.01 * 6.846672534942627
Epoch 880, val loss: 1.5241830348968506
Epoch 890, training loss: 0.072050541639328 = 0.0036061240825802088 + 0.01 * 6.8444414138793945
Epoch 890, val loss: 1.5290946960449219
Epoch 900, training loss: 0.07186846435070038 = 0.003516924334689975 + 0.01 * 6.835154056549072
Epoch 900, val loss: 1.533539891242981
Epoch 910, training loss: 0.07178441435098648 = 0.0034315483644604683 + 0.01 * 6.8352861404418945
Epoch 910, val loss: 1.5382559299468994
Epoch 920, training loss: 0.07154092192649841 = 0.00335004017688334 + 0.01 * 6.819088459014893
Epoch 920, val loss: 1.54252028465271
Epoch 930, training loss: 0.07158355414867401 = 0.0032719674054533243 + 0.01 * 6.831158638000488
Epoch 930, val loss: 1.5470765829086304
Epoch 940, training loss: 0.07134542614221573 = 0.0031972925644367933 + 0.01 * 6.814813613891602
Epoch 940, val loss: 1.5510748624801636
Epoch 950, training loss: 0.07143833488225937 = 0.0031257758382707834 + 0.01 * 6.831255912780762
Epoch 950, val loss: 1.5555049180984497
Epoch 960, training loss: 0.07127169519662857 = 0.0030572505202144384 + 0.01 * 6.821444511413574
Epoch 960, val loss: 1.5592522621154785
Epoch 970, training loss: 0.07109422981739044 = 0.0029916006606072187 + 0.01 * 6.810263156890869
Epoch 970, val loss: 1.5631636381149292
Epoch 980, training loss: 0.07091879844665527 = 0.0029284085612744093 + 0.01 * 6.799039363861084
Epoch 980, val loss: 1.5670673847198486
Epoch 990, training loss: 0.07102806121110916 = 0.0028677512891590595 + 0.01 * 6.816030979156494
Epoch 990, val loss: 1.5708670616149902
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.79388508170796
The final CL Acc:0.71358, 0.03148, The final GNN Acc:0.79898, 0.00391
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13240])
remove edge: torch.Size([2, 7868])
updated graph: torch.Size([2, 10552])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.034327268600464 = 1.9483590126037598 + 0.01 * 8.596817016601562
Epoch 0, val loss: 1.9433369636535645
Epoch 10, training loss: 2.0238869190216064 = 1.9379194974899292 + 0.01 * 8.596748352050781
Epoch 10, val loss: 1.933018445968628
Epoch 20, training loss: 2.011019468307495 = 1.9250540733337402 + 0.01 * 8.596532821655273
Epoch 20, val loss: 1.9200669527053833
Epoch 30, training loss: 1.99297297000885 = 1.9070147275924683 + 0.01 * 8.595829010009766
Epoch 30, val loss: 1.9017754793167114
Epoch 40, training loss: 1.9664735794067383 = 1.8805534839630127 + 0.01 * 8.592007637023926
Epoch 40, val loss: 1.8751928806304932
Epoch 50, training loss: 1.929977536201477 = 1.844317078590393 + 0.01 * 8.56604290008545
Epoch 50, val loss: 1.8404706716537476
Epoch 60, training loss: 1.890060305595398 = 1.8055156469345093 + 0.01 * 8.454471588134766
Epoch 60, val loss: 1.8077681064605713
Epoch 70, training loss: 1.8557788133621216 = 1.7730646133422852 + 0.01 * 8.271415710449219
Epoch 70, val loss: 1.7834769487380981
Epoch 80, training loss: 1.8143733739852905 = 1.732996940612793 + 0.01 * 8.137638092041016
Epoch 80, val loss: 1.7489620447158813
Epoch 90, training loss: 1.756314754486084 = 1.6765929460525513 + 0.01 * 7.972184181213379
Epoch 90, val loss: 1.6982773542404175
Epoch 100, training loss: 1.6795270442962646 = 1.6014127731323242 + 0.01 * 7.811430931091309
Epoch 100, val loss: 1.6333180665969849
Epoch 110, training loss: 1.590709924697876 = 1.5150933265686035 + 0.01 * 7.561666011810303
Epoch 110, val loss: 1.5628961324691772
Epoch 120, training loss: 1.5030349493026733 = 1.429067850112915 + 0.01 * 7.3967084884643555
Epoch 120, val loss: 1.4954034090042114
Epoch 130, training loss: 1.4198315143585205 = 1.3462952375411987 + 0.01 * 7.353631019592285
Epoch 130, val loss: 1.4342714548110962
Epoch 140, training loss: 1.3377220630645752 = 1.264447569847107 + 0.01 * 7.327444076538086
Epoch 140, val loss: 1.3752321004867554
Epoch 150, training loss: 1.255601167678833 = 1.1825706958770752 + 0.01 * 7.303041458129883
Epoch 150, val loss: 1.3158373832702637
Epoch 160, training loss: 1.1757019758224487 = 1.1028245687484741 + 0.01 * 7.287735462188721
Epoch 160, val loss: 1.258707880973816
Epoch 170, training loss: 1.1009321212768555 = 1.02814519405365 + 0.01 * 7.278696537017822
Epoch 170, val loss: 1.205654501914978
Epoch 180, training loss: 1.0320570468902588 = 0.9593518376350403 + 0.01 * 7.270514965057373
Epoch 180, val loss: 1.1568270921707153
Epoch 190, training loss: 0.9667607545852661 = 0.894152045249939 + 0.01 * 7.260869026184082
Epoch 190, val loss: 1.11017644405365
Epoch 200, training loss: 0.9018330574035645 = 0.8293539881706238 + 0.01 * 7.2479047775268555
Epoch 200, val loss: 1.0635026693344116
Epoch 210, training loss: 0.8349547386169434 = 0.7626286149024963 + 0.01 * 7.232614994049072
Epoch 210, val loss: 1.0151116847991943
Epoch 220, training loss: 0.7664837837219238 = 0.6943483352661133 + 0.01 * 7.21354341506958
Epoch 220, val loss: 0.9659179449081421
Epoch 230, training loss: 0.6992913484573364 = 0.6273571252822876 + 0.01 * 7.193424701690674
Epoch 230, val loss: 0.9191126227378845
Epoch 240, training loss: 0.6366981267929077 = 0.5649523735046387 + 0.01 * 7.174576759338379
Epoch 240, val loss: 0.8784056305885315
Epoch 250, training loss: 0.5802510976791382 = 0.5086067318916321 + 0.01 * 7.164438247680664
Epoch 250, val loss: 0.8460114002227783
Epoch 260, training loss: 0.5294721126556396 = 0.45789700746536255 + 0.01 * 7.157510757446289
Epoch 260, val loss: 0.8215010166168213
Epoch 270, training loss: 0.48329848051071167 = 0.41180363297462463 + 0.01 * 7.1494832038879395
Epoch 270, val loss: 0.8032846450805664
Epoch 280, training loss: 0.44111302495002747 = 0.3696804642677307 + 0.01 * 7.143255710601807
Epoch 280, val loss: 0.7899208068847656
Epoch 290, training loss: 0.40275201201438904 = 0.3313708007335663 + 0.01 * 7.138120651245117
Epoch 290, val loss: 0.780610203742981
Epoch 300, training loss: 0.36822643876075745 = 0.2969154715538025 + 0.01 * 7.131095886230469
Epoch 300, val loss: 0.7750278115272522
Epoch 310, training loss: 0.33730649948120117 = 0.2660483419895172 + 0.01 * 7.125816345214844
Epoch 310, val loss: 0.7730140686035156
Epoch 320, training loss: 0.30934518575668335 = 0.23819951713085175 + 0.01 * 7.114567756652832
Epoch 320, val loss: 0.774333119392395
Epoch 330, training loss: 0.2837006151676178 = 0.21265684068202972 + 0.01 * 7.104376792907715
Epoch 330, val loss: 0.7785532474517822
Epoch 340, training loss: 0.2597945034503937 = 0.18882837891578674 + 0.01 * 7.096611976623535
Epoch 340, val loss: 0.7850391864776611
Epoch 350, training loss: 0.23714309930801392 = 0.16632410883903503 + 0.01 * 7.081898212432861
Epoch 350, val loss: 0.7931263446807861
Epoch 360, training loss: 0.2159671187400818 = 0.14522363245487213 + 0.01 * 7.074349880218506
Epoch 360, val loss: 0.8024213314056396
Epoch 370, training loss: 0.19654065370559692 = 0.1258927434682846 + 0.01 * 7.064792156219482
Epoch 370, val loss: 0.8128150105476379
Epoch 380, training loss: 0.1792328655719757 = 0.10868269950151443 + 0.01 * 7.055016040802002
Epoch 380, val loss: 0.8244105577468872
Epoch 390, training loss: 0.164223775267601 = 0.09375271946191788 + 0.01 * 7.04710578918457
Epoch 390, val loss: 0.8372582197189331
Epoch 400, training loss: 0.1514648199081421 = 0.08104068040847778 + 0.01 * 7.042413234710693
Epoch 400, val loss: 0.851250946521759
Epoch 410, training loss: 0.14072784781455994 = 0.07033561170101166 + 0.01 * 7.039224147796631
Epoch 410, val loss: 0.8661755919456482
Epoch 420, training loss: 0.13175000250339508 = 0.06134962663054466 + 0.01 * 7.040037631988525
Epoch 420, val loss: 0.8818135857582092
Epoch 430, training loss: 0.12411094456911087 = 0.053796932101249695 + 0.01 * 7.03140115737915
Epoch 430, val loss: 0.8978697061538696
Epoch 440, training loss: 0.11761745065450668 = 0.047423429787158966 + 0.01 * 7.019402027130127
Epoch 440, val loss: 0.9142085909843445
Epoch 450, training loss: 0.11215373128652573 = 0.04201476275920868 + 0.01 * 7.013896942138672
Epoch 450, val loss: 0.9305883049964905
Epoch 460, training loss: 0.10749240219593048 = 0.03739992156624794 + 0.01 * 7.009247779846191
Epoch 460, val loss: 0.9468792676925659
Epoch 470, training loss: 0.10359998047351837 = 0.03343995660543442 + 0.01 * 7.016002655029297
Epoch 470, val loss: 0.9629380702972412
Epoch 480, training loss: 0.1000853031873703 = 0.030033383518457413 + 0.01 * 7.005192756652832
Epoch 480, val loss: 0.9786302447319031
Epoch 490, training loss: 0.09701453149318695 = 0.027093835175037384 + 0.01 * 6.992069721221924
Epoch 490, val loss: 0.9939788579940796
Epoch 500, training loss: 0.09471665322780609 = 0.02454826608300209 + 0.01 * 7.016839504241943
Epoch 500, val loss: 1.008900761604309
Epoch 510, training loss: 0.09217497706413269 = 0.022341208532452583 + 0.01 * 6.983377456665039
Epoch 510, val loss: 1.0232692956924438
Epoch 520, training loss: 0.09019660204648972 = 0.02041763998568058 + 0.01 * 6.977896690368652
Epoch 520, val loss: 1.0371646881103516
Epoch 530, training loss: 0.08858603239059448 = 0.018733832985162735 + 0.01 * 6.985219955444336
Epoch 530, val loss: 1.050558090209961
Epoch 540, training loss: 0.08698499947786331 = 0.01725527085363865 + 0.01 * 6.972973346710205
Epoch 540, val loss: 1.063498854637146
Epoch 550, training loss: 0.08557255566120148 = 0.015950344502925873 + 0.01 * 6.962221145629883
Epoch 550, val loss: 1.0759423971176147
Epoch 560, training loss: 0.08447157591581345 = 0.014792835339903831 + 0.01 * 6.967874050140381
Epoch 560, val loss: 1.087950587272644
Epoch 570, training loss: 0.08323568105697632 = 0.013763617724180222 + 0.01 * 6.947206497192383
Epoch 570, val loss: 1.099495530128479
Epoch 580, training loss: 0.08249098062515259 = 0.01284400001168251 + 0.01 * 6.964698791503906
Epoch 580, val loss: 1.110696792602539
Epoch 590, training loss: 0.08137626200914383 = 0.012020647525787354 + 0.01 * 6.935561656951904
Epoch 590, val loss: 1.1213891506195068
Epoch 600, training loss: 0.08057256042957306 = 0.0112793343141675 + 0.01 * 6.929322242736816
Epoch 600, val loss: 1.1317591667175293
Epoch 610, training loss: 0.07997686415910721 = 0.0106090372428298 + 0.01 * 6.9367828369140625
Epoch 610, val loss: 1.1417759656906128
Epoch 620, training loss: 0.07919719815254211 = 0.01000170223414898 + 0.01 * 6.919549465179443
Epoch 620, val loss: 1.1514633893966675
Epoch 630, training loss: 0.07858823239803314 = 0.00944975670427084 + 0.01 * 6.91384744644165
Epoch 630, val loss: 1.1608240604400635
Epoch 640, training loss: 0.07816461473703384 = 0.008946621790528297 + 0.01 * 6.921799659729004
Epoch 640, val loss: 1.1698057651519775
Epoch 650, training loss: 0.07748142629861832 = 0.008486850187182426 + 0.01 * 6.899457931518555
Epoch 650, val loss: 1.1785558462142944
Epoch 660, training loss: 0.07706183940172195 = 0.008065464906394482 + 0.01 * 6.8996381759643555
Epoch 660, val loss: 1.1870065927505493
Epoch 670, training loss: 0.07671674340963364 = 0.0076779816299676895 + 0.01 * 6.903876781463623
Epoch 670, val loss: 1.1951217651367188
Epoch 680, training loss: 0.07629004865884781 = 0.007321466691792011 + 0.01 * 6.896857738494873
Epoch 680, val loss: 1.203094244003296
Epoch 690, training loss: 0.07590978592634201 = 0.0069920835085213184 + 0.01 * 6.891770362854004
Epoch 690, val loss: 1.210689902305603
Epoch 700, training loss: 0.07546180486679077 = 0.006687448360025883 + 0.01 * 6.87743616104126
Epoch 700, val loss: 1.2181752920150757
Epoch 710, training loss: 0.07515218108892441 = 0.006404607091099024 + 0.01 * 6.874757289886475
Epoch 710, val loss: 1.2254024744033813
Epoch 720, training loss: 0.07485711574554443 = 0.006141755264252424 + 0.01 * 6.8715362548828125
Epoch 720, val loss: 1.2324024438858032
Epoch 730, training loss: 0.07466080784797668 = 0.005896906368434429 + 0.01 * 6.87639045715332
Epoch 730, val loss: 1.2391616106033325
Epoch 740, training loss: 0.07422348856925964 = 0.005668951198458672 + 0.01 * 6.855453968048096
Epoch 740, val loss: 1.2458374500274658
Epoch 750, training loss: 0.07404063642024994 = 0.00545587670058012 + 0.01 * 6.858476161956787
Epoch 750, val loss: 1.2521777153015137
Epoch 760, training loss: 0.07374146580696106 = 0.005256428848952055 + 0.01 * 6.848504066467285
Epoch 760, val loss: 1.258431315422058
Epoch 770, training loss: 0.07364939153194427 = 0.005069360602647066 + 0.01 * 6.858003616333008
Epoch 770, val loss: 1.2645810842514038
Epoch 780, training loss: 0.07330310344696045 = 0.004893814213573933 + 0.01 * 6.84092903137207
Epoch 780, val loss: 1.2704083919525146
Epoch 790, training loss: 0.07305295020341873 = 0.004728748928755522 + 0.01 * 6.832420349121094
Epoch 790, val loss: 1.2762436866760254
Epoch 800, training loss: 0.07302392274141312 = 0.004573243670165539 + 0.01 * 6.845068454742432
Epoch 800, val loss: 1.2818883657455444
Epoch 810, training loss: 0.07271718233823776 = 0.004426645580679178 + 0.01 * 6.82905387878418
Epoch 810, val loss: 1.2873220443725586
Epoch 820, training loss: 0.07247523218393326 = 0.004288299009203911 + 0.01 * 6.818693161010742
Epoch 820, val loss: 1.2927346229553223
Epoch 830, training loss: 0.07262499630451202 = 0.0041575077921152115 + 0.01 * 6.846749305725098
Epoch 830, val loss: 1.2979695796966553
Epoch 840, training loss: 0.07221204042434692 = 0.004033845849335194 + 0.01 * 6.817819595336914
Epoch 840, val loss: 1.3030264377593994
Epoch 850, training loss: 0.07215848565101624 = 0.003916763700544834 + 0.01 * 6.824172496795654
Epoch 850, val loss: 1.3080224990844727
Epoch 860, training loss: 0.07189042121171951 = 0.003805543063208461 + 0.01 * 6.808488368988037
Epoch 860, val loss: 1.312845230102539
Epoch 870, training loss: 0.07179100066423416 = 0.0037000421434640884 + 0.01 * 6.809096336364746
Epoch 870, val loss: 1.3176158666610718
Epoch 880, training loss: 0.07181338220834732 = 0.003599821589887142 + 0.01 * 6.821355819702148
Epoch 880, val loss: 1.3221712112426758
Epoch 890, training loss: 0.07157128304243088 = 0.00350458943285048 + 0.01 * 6.806669235229492
Epoch 890, val loss: 1.3267254829406738
Epoch 900, training loss: 0.07131484150886536 = 0.0034139156341552734 + 0.01 * 6.790092945098877
Epoch 900, val loss: 1.331120491027832
Epoch 910, training loss: 0.07125881314277649 = 0.0033275093883275986 + 0.01 * 6.793130397796631
Epoch 910, val loss: 1.3354675769805908
Epoch 920, training loss: 0.07138891518115997 = 0.0032450761646032333 + 0.01 * 6.8143839836120605
Epoch 920, val loss: 1.3397119045257568
Epoch 930, training loss: 0.07101516425609589 = 0.0031664432026445866 + 0.01 * 6.784872055053711
Epoch 930, val loss: 1.3438652753829956
Epoch 940, training loss: 0.07095924764871597 = 0.003091421676799655 + 0.01 * 6.786782741546631
Epoch 940, val loss: 1.347914218902588
Epoch 950, training loss: 0.0709424689412117 = 0.00301955034956336 + 0.01 * 6.79229211807251
Epoch 950, val loss: 1.3518729209899902
Epoch 960, training loss: 0.0706777349114418 = 0.002950953785330057 + 0.01 * 6.772678852081299
Epoch 960, val loss: 1.3557876348495483
Epoch 970, training loss: 0.07070113718509674 = 0.002885210793465376 + 0.01 * 6.781593322753906
Epoch 970, val loss: 1.3596045970916748
Epoch 980, training loss: 0.07064608484506607 = 0.002822078764438629 + 0.01 * 6.782401084899902
Epoch 980, val loss: 1.3633215427398682
Epoch 990, training loss: 0.07054763287305832 = 0.0027618424501270056 + 0.01 * 6.778579235076904
Epoch 990, val loss: 1.3670899868011475
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.0399398803710938 = 1.9539716243743896 + 0.01 * 8.596817016601562
Epoch 0, val loss: 1.9583712816238403
Epoch 10, training loss: 2.0286591053009033 = 1.942691683769226 + 0.01 * 8.596732139587402
Epoch 10, val loss: 1.946959376335144
Epoch 20, training loss: 2.013984441757202 = 1.9280197620391846 + 0.01 * 8.596476554870605
Epoch 20, val loss: 1.9316431283950806
Epoch 30, training loss: 1.9928243160247803 = 1.9068679809570312 + 0.01 * 8.595632553100586
Epoch 30, val loss: 1.9093037843704224
Epoch 40, training loss: 1.9618356227874756 = 1.8759303092956543 + 0.01 * 8.590534210205078
Epoch 40, val loss: 1.87734055519104
Epoch 50, training loss: 1.9209520816802979 = 1.8353866338729858 + 0.01 * 8.55654525756836
Epoch 50, val loss: 1.8381481170654297
Epoch 60, training loss: 1.8790106773376465 = 1.7952288389205933 + 0.01 * 8.378188133239746
Epoch 60, val loss: 1.8037445545196533
Epoch 70, training loss: 1.841422200202942 = 1.759434700012207 + 0.01 * 8.19875431060791
Epoch 70, val loss: 1.7729268074035645
Epoch 80, training loss: 1.789264440536499 = 1.7091591358184814 + 0.01 * 8.010525703430176
Epoch 80, val loss: 1.7264161109924316
Epoch 90, training loss: 1.718217134475708 = 1.639862298965454 + 0.01 * 7.835480690002441
Epoch 90, val loss: 1.6653504371643066
Epoch 100, training loss: 1.6305818557739258 = 1.5540740489959717 + 0.01 * 7.65078592300415
Epoch 100, val loss: 1.5944362878799438
Epoch 110, training loss: 1.5426585674285889 = 1.4671173095703125 + 0.01 * 7.5541205406188965
Epoch 110, val loss: 1.5217913389205933
Epoch 120, training loss: 1.4617021083831787 = 1.3865162134170532 + 0.01 * 7.518590450286865
Epoch 120, val loss: 1.4559381008148193
Epoch 130, training loss: 1.384042739868164 = 1.3093024492263794 + 0.01 * 7.474028587341309
Epoch 130, val loss: 1.3941774368286133
Epoch 140, training loss: 1.305403709411621 = 1.231196403503418 + 0.01 * 7.420732498168945
Epoch 140, val loss: 1.3321139812469482
Epoch 150, training loss: 1.2236608266830444 = 1.1500513553619385 + 0.01 * 7.360947132110596
Epoch 150, val loss: 1.2690712213516235
Epoch 160, training loss: 1.1383700370788574 = 1.0652130842208862 + 0.01 * 7.315700054168701
Epoch 160, val loss: 1.2040091753005981
Epoch 170, training loss: 1.0506174564361572 = 0.977626383304596 + 0.01 * 7.299106597900391
Epoch 170, val loss: 1.1367524862289429
Epoch 180, training loss: 0.9632664322853088 = 0.8904364109039307 + 0.01 * 7.283003330230713
Epoch 180, val loss: 1.0698916912078857
Epoch 190, training loss: 0.8806781768798828 = 0.8080649375915527 + 0.01 * 7.261322021484375
Epoch 190, val loss: 1.0071234703063965
Epoch 200, training loss: 0.8063782453536987 = 0.7340555191040039 + 0.01 * 7.232271671295166
Epoch 200, val loss: 0.9522557258605957
Epoch 210, training loss: 0.7415216565132141 = 0.6694701313972473 + 0.01 * 7.205151557922363
Epoch 210, val loss: 0.9070557951927185
Epoch 220, training loss: 0.684868574142456 = 0.6130571961402893 + 0.01 * 7.1811347007751465
Epoch 220, val loss: 0.8711064457893372
Epoch 230, training loss: 0.6344917416572571 = 0.562829852104187 + 0.01 * 7.166187286376953
Epoch 230, val loss: 0.843291699886322
Epoch 240, training loss: 0.5885469913482666 = 0.5170013308525085 + 0.01 * 7.1545634269714355
Epoch 240, val loss: 0.8223837614059448
Epoch 250, training loss: 0.5455114841461182 = 0.47404369711875916 + 0.01 * 7.146777629852295
Epoch 250, val loss: 0.8071195483207703
Epoch 260, training loss: 0.5040018558502197 = 0.43258848786354065 + 0.01 * 7.14133882522583
Epoch 260, val loss: 0.7958011031150818
Epoch 270, training loss: 0.46311527490615845 = 0.39175257086753845 + 0.01 * 7.136270523071289
Epoch 270, val loss: 0.7868989706039429
Epoch 280, training loss: 0.42270708084106445 = 0.3513985574245453 + 0.01 * 7.130853652954102
Epoch 280, val loss: 0.7796359658241272
Epoch 290, training loss: 0.3834072947502136 = 0.31214800477027893 + 0.01 * 7.125927448272705
Epoch 290, val loss: 0.7743718028068542
Epoch 300, training loss: 0.3463403582572937 = 0.2751248776912689 + 0.01 * 7.121546745300293
Epoch 300, val loss: 0.771847128868103
Epoch 310, training loss: 0.3126125931739807 = 0.24144026637077332 + 0.01 * 7.117233753204346
Epoch 310, val loss: 0.7729430794715881
Epoch 320, training loss: 0.28284645080566406 = 0.21171170473098755 + 0.01 * 7.113474369049072
Epoch 320, val loss: 0.7780562043190002
Epoch 330, training loss: 0.25704139471054077 = 0.18593907356262207 + 0.01 * 7.110232353210449
Epoch 330, val loss: 0.7869400382041931
Epoch 340, training loss: 0.2348427027463913 = 0.1637449711561203 + 0.01 * 7.109773635864258
Epoch 340, val loss: 0.7989643812179565
Epoch 350, training loss: 0.2157176285982132 = 0.1446438878774643 + 0.01 * 7.10737419128418
Epoch 350, val loss: 0.8133474588394165
Epoch 360, training loss: 0.19922509789466858 = 0.12818998098373413 + 0.01 * 7.103511333465576
Epoch 360, val loss: 0.8294856548309326
Epoch 370, training loss: 0.18497426807880402 = 0.11398329585790634 + 0.01 * 7.09909725189209
Epoch 370, val loss: 0.8470088839530945
Epoch 380, training loss: 0.17266419529914856 = 0.10166588425636292 + 0.01 * 7.099832057952881
Epoch 380, val loss: 0.8654742240905762
Epoch 390, training loss: 0.16188248991966248 = 0.09093616157770157 + 0.01 * 7.094634056091309
Epoch 390, val loss: 0.8845025300979614
Epoch 400, training loss: 0.1524306684732437 = 0.08154662698507309 + 0.01 * 7.088404178619385
Epoch 400, val loss: 0.9039407968521118
Epoch 410, training loss: 0.14415696263313293 = 0.07330530881881714 + 0.01 * 7.085165500640869
Epoch 410, val loss: 0.9235666394233704
Epoch 420, training loss: 0.13686269521713257 = 0.06605623662471771 + 0.01 * 7.0806450843811035
Epoch 420, val loss: 0.9432814717292786
Epoch 430, training loss: 0.1304616630077362 = 0.05966329574584961 + 0.01 * 7.079837799072266
Epoch 430, val loss: 0.9629952907562256
Epoch 440, training loss: 0.12474595010280609 = 0.054019708186388016 + 0.01 * 7.0726237297058105
Epoch 440, val loss: 0.9826344847679138
Epoch 450, training loss: 0.11968587338924408 = 0.04903096333146095 + 0.01 * 7.06549072265625
Epoch 450, val loss: 1.0020548105239868
Epoch 460, training loss: 0.11517676711082458 = 0.04461592063307762 + 0.01 * 7.056085109710693
Epoch 460, val loss: 1.0212080478668213
Epoch 470, training loss: 0.1111491397023201 = 0.04070259630680084 + 0.01 * 7.044654369354248
Epoch 470, val loss: 1.040013074874878
Epoch 480, training loss: 0.10780896246433258 = 0.037227220833301544 + 0.01 * 7.0581746101379395
Epoch 480, val loss: 1.0584856271743774
Epoch 490, training loss: 0.10448534786701202 = 0.03413759917020798 + 0.01 * 7.034775257110596
Epoch 490, val loss: 1.0765016078948975
Epoch 500, training loss: 0.10163892060518265 = 0.03138493001461029 + 0.01 * 7.025399208068848
Epoch 500, val loss: 1.0941252708435059
Epoch 510, training loss: 0.09914996474981308 = 0.028929801657795906 + 0.01 * 7.022017002105713
Epoch 510, val loss: 1.1112885475158691
Epoch 520, training loss: 0.09684136509895325 = 0.0267326682806015 + 0.01 * 7.01086950302124
Epoch 520, val loss: 1.1281039714813232
Epoch 530, training loss: 0.09494231641292572 = 0.024761036038398743 + 0.01 * 7.018128395080566
Epoch 530, val loss: 1.1444807052612305
Epoch 540, training loss: 0.09301869571208954 = 0.02299146167933941 + 0.01 * 7.002723217010498
Epoch 540, val loss: 1.1604503393173218
Epoch 550, training loss: 0.09126988053321838 = 0.021398652344942093 + 0.01 * 6.987123489379883
Epoch 550, val loss: 1.1760139465332031
Epoch 560, training loss: 0.08993291109800339 = 0.01996077597141266 + 0.01 * 6.997213840484619
Epoch 560, val loss: 1.1911617517471313
Epoch 570, training loss: 0.08854757994413376 = 0.018660753965377808 + 0.01 * 6.988682746887207
Epoch 570, val loss: 1.2059351205825806
Epoch 580, training loss: 0.087323859333992 = 0.017482487484812737 + 0.01 * 6.984137058258057
Epoch 580, val loss: 1.220334529876709
Epoch 590, training loss: 0.0861792266368866 = 0.01641262322664261 + 0.01 * 6.976660251617432
Epoch 590, val loss: 1.2342487573623657
Epoch 600, training loss: 0.08508133888244629 = 0.015439074486494064 + 0.01 * 6.964226245880127
Epoch 600, val loss: 1.2479015588760376
Epoch 610, training loss: 0.08415322005748749 = 0.014550413005053997 + 0.01 * 6.960280895233154
Epoch 610, val loss: 1.2611247301101685
Epoch 620, training loss: 0.08334047347307205 = 0.013737876899540424 + 0.01 * 6.960259437561035
Epoch 620, val loss: 1.2740252017974854
Epoch 630, training loss: 0.08240640163421631 = 0.01299354899674654 + 0.01 * 6.941285133361816
Epoch 630, val loss: 1.2865185737609863
Epoch 640, training loss: 0.08172110468149185 = 0.012310070917010307 + 0.01 * 6.941102981567383
Epoch 640, val loss: 1.2987018823623657
Epoch 650, training loss: 0.08131381869316101 = 0.011682260781526566 + 0.01 * 6.963155746459961
Epoch 650, val loss: 1.3105629682540894
Epoch 660, training loss: 0.08043704181909561 = 0.01110338605940342 + 0.01 * 6.933365821838379
Epoch 660, val loss: 1.3220678567886353
Epoch 670, training loss: 0.07996279746294022 = 0.010568364523351192 + 0.01 * 6.939443111419678
Epoch 670, val loss: 1.3332915306091309
Epoch 680, training loss: 0.07937554270029068 = 0.010073700919747353 + 0.01 * 6.930184364318848
Epoch 680, val loss: 1.3442846536636353
Epoch 690, training loss: 0.07885082066059113 = 0.009614690206944942 + 0.01 * 6.92361307144165
Epoch 690, val loss: 1.354891300201416
Epoch 700, training loss: 0.07830281555652618 = 0.009188234806060791 + 0.01 * 6.911458492279053
Epoch 700, val loss: 1.365362286567688
Epoch 710, training loss: 0.07797388732433319 = 0.008790797553956509 + 0.01 * 6.918309211730957
Epoch 710, val loss: 1.3754687309265137
Epoch 720, training loss: 0.0778563991189003 = 0.008420849218964577 + 0.01 * 6.9435553550720215
Epoch 720, val loss: 1.3853800296783447
Epoch 730, training loss: 0.07716018706560135 = 0.008076480589807034 + 0.01 * 6.908370494842529
Epoch 730, val loss: 1.39503014087677
Epoch 740, training loss: 0.07671438157558441 = 0.00775394681841135 + 0.01 * 6.89604377746582
Epoch 740, val loss: 1.4044134616851807
Epoch 750, training loss: 0.07643567770719528 = 0.007452104706317186 + 0.01 * 6.898357391357422
Epoch 750, val loss: 1.4136567115783691
Epoch 760, training loss: 0.07597441971302032 = 0.007169017568230629 + 0.01 * 6.880540370941162
Epoch 760, val loss: 1.422682762145996
Epoch 770, training loss: 0.07578890770673752 = 0.006903129164129496 + 0.01 * 6.888577938079834
Epoch 770, val loss: 1.4314531087875366
Epoch 780, training loss: 0.07565944641828537 = 0.006654048804193735 + 0.01 * 6.900540351867676
Epoch 780, val loss: 1.4400774240493774
Epoch 790, training loss: 0.07519662380218506 = 0.006419670302420855 + 0.01 * 6.877695083618164
Epoch 790, val loss: 1.4483840465545654
Epoch 800, training loss: 0.07483182102441788 = 0.006199090741574764 + 0.01 * 6.8632731437683105
Epoch 800, val loss: 1.4565422534942627
Epoch 810, training loss: 0.07472644001245499 = 0.00599099462851882 + 0.01 * 6.873544216156006
Epoch 810, val loss: 1.464511513710022
Epoch 820, training loss: 0.07468394190073013 = 0.005794620141386986 + 0.01 * 6.888932228088379
Epoch 820, val loss: 1.4723109006881714
Epoch 830, training loss: 0.07413855195045471 = 0.005608984734863043 + 0.01 * 6.852957248687744
Epoch 830, val loss: 1.4799562692642212
Epoch 840, training loss: 0.0741124302148819 = 0.005433391313999891 + 0.01 * 6.867904186248779
Epoch 840, val loss: 1.4873894453048706
Epoch 850, training loss: 0.0736892968416214 = 0.0052671488374471664 + 0.01 * 6.842215061187744
Epoch 850, val loss: 1.4946620464324951
Epoch 860, training loss: 0.07352308928966522 = 0.00510949594900012 + 0.01 * 6.841359615325928
Epoch 860, val loss: 1.501793622970581
Epoch 870, training loss: 0.07352515310049057 = 0.004960054997354746 + 0.01 * 6.856510162353516
Epoch 870, val loss: 1.5087817907333374
Epoch 880, training loss: 0.0731486901640892 = 0.004818306304514408 + 0.01 * 6.833038806915283
Epoch 880, val loss: 1.5156004428863525
Epoch 890, training loss: 0.07346448302268982 = 0.0046833716332912445 + 0.01 * 6.878111362457275
Epoch 890, val loss: 1.5222687721252441
Epoch 900, training loss: 0.07283338904380798 = 0.004555392079055309 + 0.01 * 6.8277997970581055
Epoch 900, val loss: 1.5288283824920654
Epoch 910, training loss: 0.07306355983018875 = 0.004433336667716503 + 0.01 * 6.863022804260254
Epoch 910, val loss: 1.535194993019104
Epoch 920, training loss: 0.07285657525062561 = 0.004317162558436394 + 0.01 * 6.853940963745117
Epoch 920, val loss: 1.5414308309555054
Epoch 930, training loss: 0.07255125790834427 = 0.004206836689263582 + 0.01 * 6.834442138671875
Epoch 930, val loss: 1.5475462675094604
Epoch 940, training loss: 0.07230208814144135 = 0.004101206548511982 + 0.01 * 6.820087909698486
Epoch 940, val loss: 1.5535635948181152
Epoch 950, training loss: 0.07219576090574265 = 0.0040004015900194645 + 0.01 * 6.819536209106445
Epoch 950, val loss: 1.5594427585601807
Epoch 960, training loss: 0.07192636281251907 = 0.003904017386958003 + 0.01 * 6.802235126495361
Epoch 960, val loss: 1.5651917457580566
Epoch 970, training loss: 0.0718572810292244 = 0.003811994567513466 + 0.01 * 6.804529190063477
Epoch 970, val loss: 1.570812702178955
Epoch 980, training loss: 0.07180286943912506 = 0.003724092384800315 + 0.01 * 6.807878017425537
Epoch 980, val loss: 1.5763726234436035
Epoch 990, training loss: 0.07164577394723892 = 0.0036397785879671574 + 0.01 * 6.800599575042725
Epoch 990, val loss: 1.581727147102356
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.0316433906555176 = 1.9456753730773926 + 0.01 * 8.59679126739502
Epoch 0, val loss: 1.9444658756256104
Epoch 10, training loss: 2.021967649459839 = 1.9360004663467407 + 0.01 * 8.596711158752441
Epoch 10, val loss: 1.9356530904769897
Epoch 20, training loss: 2.009859085083008 = 1.9238951206207275 + 0.01 * 8.596402168273926
Epoch 20, val loss: 1.9242061376571655
Epoch 30, training loss: 1.9924836158752441 = 1.906530737876892 + 0.01 * 8.595293045043945
Epoch 30, val loss: 1.9075219631195068
Epoch 40, training loss: 1.966366171836853 = 1.8804770708084106 + 0.01 * 8.58891487121582
Epoch 40, val loss: 1.882728934288025
Epoch 50, training loss: 1.9290132522583008 = 1.8435014486312866 + 0.01 * 8.551183700561523
Epoch 50, val loss: 1.8491342067718506
Epoch 60, training loss: 1.8853658437728882 = 1.8015044927597046 + 0.01 * 8.386139869689941
Epoch 60, val loss: 1.814396858215332
Epoch 70, training loss: 1.846475601196289 = 1.7646256685256958 + 0.01 * 8.184992790222168
Epoch 70, val loss: 1.7835254669189453
Epoch 80, training loss: 1.7993171215057373 = 1.7193270921707153 + 0.01 * 7.998998641967773
Epoch 80, val loss: 1.7387382984161377
Epoch 90, training loss: 1.7352790832519531 = 1.6569623947143555 + 0.01 * 7.8316731452941895
Epoch 90, val loss: 1.680769443511963
Epoch 100, training loss: 1.6514619588851929 = 1.574727177619934 + 0.01 * 7.673483371734619
Epoch 100, val loss: 1.6098400354385376
Epoch 110, training loss: 1.5551671981811523 = 1.4807747602462769 + 0.01 * 7.439248085021973
Epoch 110, val loss: 1.5303184986114502
Epoch 120, training loss: 1.460184097290039 = 1.3867429494857788 + 0.01 * 7.34411096572876
Epoch 120, val loss: 1.453431248664856
Epoch 130, training loss: 1.3708621263504028 = 1.297836184501648 + 0.01 * 7.3025970458984375
Epoch 130, val loss: 1.383005142211914
Epoch 140, training loss: 1.2861794233322144 = 1.2133445739746094 + 0.01 * 7.283482074737549
Epoch 140, val loss: 1.317596435546875
Epoch 150, training loss: 1.2044013738632202 = 1.1317411661148071 + 0.01 * 7.266017436981201
Epoch 150, val loss: 1.25557541847229
Epoch 160, training loss: 1.124283790588379 = 1.0517817735671997 + 0.01 * 7.250201225280762
Epoch 160, val loss: 1.1952635049819946
Epoch 170, training loss: 1.0454621315002441 = 0.9730902910232544 + 0.01 * 7.237189769744873
Epoch 170, val loss: 1.1358275413513184
Epoch 180, training loss: 0.9681559801101685 = 0.8959411978721619 + 0.01 * 7.2214789390563965
Epoch 180, val loss: 1.0770785808563232
Epoch 190, training loss: 0.892605721950531 = 0.8205960988998413 + 0.01 * 7.200962543487549
Epoch 190, val loss: 1.019364356994629
Epoch 200, training loss: 0.8191871643066406 = 0.7474197745323181 + 0.01 * 7.176739692687988
Epoch 200, val loss: 0.9634461402893066
Epoch 210, training loss: 0.7489163875579834 = 0.6773763298988342 + 0.01 * 7.154004096984863
Epoch 210, val loss: 0.9105673432350159
Epoch 220, training loss: 0.6832741498947144 = 0.6118517518043518 + 0.01 * 7.142240524291992
Epoch 220, val loss: 0.8625684380531311
Epoch 230, training loss: 0.6232318878173828 = 0.5518977046012878 + 0.01 * 7.133415222167969
Epoch 230, val loss: 0.8211128115653992
Epoch 240, training loss: 0.5688907504081726 = 0.4976198077201843 + 0.01 * 7.127094268798828
Epoch 240, val loss: 0.7865737080574036
Epoch 250, training loss: 0.5193930864334106 = 0.44818314909935 + 0.01 * 7.120996952056885
Epoch 250, val loss: 0.7582325339317322
Epoch 260, training loss: 0.4735325574874878 = 0.40239033102989197 + 0.01 * 7.114224433898926
Epoch 260, val loss: 0.7347294688224792
Epoch 270, training loss: 0.43033987283706665 = 0.35927850008010864 + 0.01 * 7.106136322021484
Epoch 270, val loss: 0.7151474356651306
Epoch 280, training loss: 0.3894641101360321 = 0.3184778392314911 + 0.01 * 7.098627090454102
Epoch 280, val loss: 0.698918342590332
Epoch 290, training loss: 0.35116398334503174 = 0.2802797257900238 + 0.01 * 7.08842658996582
Epoch 290, val loss: 0.6859495043754578
Epoch 300, training loss: 0.31615346670150757 = 0.2453516572713852 + 0.01 * 7.080182075500488
Epoch 300, val loss: 0.6763628721237183
Epoch 310, training loss: 0.28496041893959045 = 0.21422693133354187 + 0.01 * 7.073349475860596
Epoch 310, val loss: 0.6702627539634705
Epoch 320, training loss: 0.25768694281578064 = 0.18704886734485626 + 0.01 * 7.063807487487793
Epoch 320, val loss: 0.6675910949707031
Epoch 330, training loss: 0.234176903963089 = 0.16358907520771027 + 0.01 * 7.058782577514648
Epoch 330, val loss: 0.6680940389633179
Epoch 340, training loss: 0.2139989733695984 = 0.14344441890716553 + 0.01 * 7.055454730987549
Epoch 340, val loss: 0.6713590621948242
Epoch 350, training loss: 0.19669052958488464 = 0.12616375088691711 + 0.01 * 7.05267858505249
Epoch 350, val loss: 0.6768993735313416
Epoch 360, training loss: 0.18180881440639496 = 0.1113104596734047 + 0.01 * 7.049835681915283
Epoch 360, val loss: 0.6842638850212097
Epoch 370, training loss: 0.16896629333496094 = 0.09850052744150162 + 0.01 * 7.046576976776123
Epoch 370, val loss: 0.6930153965950012
Epoch 380, training loss: 0.15784001350402832 = 0.08740684390068054 + 0.01 * 7.043317794799805
Epoch 380, val loss: 0.7028366923332214
Epoch 390, training loss: 0.14816877245903015 = 0.07776083052158356 + 0.01 * 7.04079532623291
Epoch 390, val loss: 0.7134268283843994
Epoch 400, training loss: 0.13975660502910614 = 0.06934711337089539 + 0.01 * 7.04094934463501
Epoch 400, val loss: 0.724595308303833
Epoch 410, training loss: 0.13237732648849487 = 0.06198984012007713 + 0.01 * 7.0387492179870605
Epoch 410, val loss: 0.7361884117126465
Epoch 420, training loss: 0.12589071691036224 = 0.05554406717419624 + 0.01 * 7.034665107727051
Epoch 420, val loss: 0.7480781674385071
Epoch 430, training loss: 0.1202203631401062 = 0.0498911589384079 + 0.01 * 7.0329203605651855
Epoch 430, val loss: 0.7601296901702881
Epoch 440, training loss: 0.11522699147462845 = 0.04492894560098648 + 0.01 * 7.029804706573486
Epoch 440, val loss: 0.7722352743148804
Epoch 450, training loss: 0.11086029559373856 = 0.04056890308856964 + 0.01 * 7.029139518737793
Epoch 450, val loss: 0.7843527793884277
Epoch 460, training loss: 0.10698720067739487 = 0.036736153066158295 + 0.01 * 7.025104999542236
Epoch 460, val loss: 0.7964052557945251
Epoch 470, training loss: 0.10360727459192276 = 0.03336367756128311 + 0.01 * 7.024359703063965
Epoch 470, val loss: 0.8083192706108093
Epoch 480, training loss: 0.10059799253940582 = 0.030393268913030624 + 0.01 * 7.020473003387451
Epoch 480, val loss: 0.8200623393058777
Epoch 490, training loss: 0.0979493111371994 = 0.02777061238884926 + 0.01 * 7.017869472503662
Epoch 490, val loss: 0.8316121101379395
Epoch 500, training loss: 0.09560089558362961 = 0.025448640808463097 + 0.01 * 7.015225410461426
Epoch 500, val loss: 0.8429281115531921
Epoch 510, training loss: 0.0935087502002716 = 0.02338717319071293 + 0.01 * 7.012158393859863
Epoch 510, val loss: 0.854013204574585
Epoch 520, training loss: 0.09167454391717911 = 0.021551920101046562 + 0.01 * 7.012262344360352
Epoch 520, val loss: 0.8648387789726257
Epoch 530, training loss: 0.09000439196825027 = 0.019915832206606865 + 0.01 * 7.008855819702148
Epoch 530, val loss: 0.8753818869590759
Epoch 540, training loss: 0.0885159894824028 = 0.018453272059559822 + 0.01 * 7.006271839141846
Epoch 540, val loss: 0.8856402635574341
Epoch 550, training loss: 0.08715718984603882 = 0.017141761258244514 + 0.01 * 7.0015435218811035
Epoch 550, val loss: 0.895686149597168
Epoch 560, training loss: 0.08594674617052078 = 0.01596294529736042 + 0.01 * 6.998380661010742
Epoch 560, val loss: 0.9054163098335266
Epoch 570, training loss: 0.08486805111169815 = 0.01490064151585102 + 0.01 * 6.99674129486084
Epoch 570, val loss: 0.9148810505867004
Epoch 580, training loss: 0.08387952297925949 = 0.013940716162323952 + 0.01 * 6.993880748748779
Epoch 580, val loss: 0.9241012930870056
Epoch 590, training loss: 0.08296464383602142 = 0.013070615008473396 + 0.01 * 6.989403247833252
Epoch 590, val loss: 0.9330705404281616
Epoch 600, training loss: 0.08215200901031494 = 0.012281116098165512 + 0.01 * 6.98708963394165
Epoch 600, val loss: 0.941778838634491
Epoch 610, training loss: 0.08138903975486755 = 0.011562131345272064 + 0.01 * 6.982690811157227
Epoch 610, val loss: 0.9502503275871277
Epoch 620, training loss: 0.08067365735769272 = 0.010905827395617962 + 0.01 * 6.976783275604248
Epoch 620, val loss: 0.9585253596305847
Epoch 630, training loss: 0.08019410073757172 = 0.010304098017513752 + 0.01 * 6.98900032043457
Epoch 630, val loss: 0.9665526151657104
Epoch 640, training loss: 0.07944167405366898 = 0.009752863086760044 + 0.01 * 6.968881130218506
Epoch 640, val loss: 0.9743415117263794
Epoch 650, training loss: 0.07891808450222015 = 0.009246188215911388 + 0.01 * 6.967189788818359
Epoch 650, val loss: 0.9819462299346924
Epoch 660, training loss: 0.07847006618976593 = 0.008779739961028099 + 0.01 * 6.9690327644348145
Epoch 660, val loss: 0.9893777370452881
Epoch 670, training loss: 0.07790501415729523 = 0.008349336683750153 + 0.01 * 6.955567836761475
Epoch 670, val loss: 0.9965424537658691
Epoch 680, training loss: 0.0775003433227539 = 0.007951309904456139 + 0.01 * 6.954903602600098
Epoch 680, val loss: 1.0035253763198853
Epoch 690, training loss: 0.07714953273534775 = 0.00758278788998723 + 0.01 * 6.956674098968506
Epoch 690, val loss: 1.010367512702942
Epoch 700, training loss: 0.07664893567562103 = 0.007241410668939352 + 0.01 * 6.9407525062561035
Epoch 700, val loss: 1.0169999599456787
Epoch 710, training loss: 0.07637162506580353 = 0.006924312561750412 + 0.01 * 6.94473123550415
Epoch 710, val loss: 1.0234304666519165
Epoch 720, training loss: 0.07598060369491577 = 0.006629857700318098 + 0.01 * 6.935074806213379
Epoch 720, val loss: 1.0297068357467651
Epoch 730, training loss: 0.0756436362862587 = 0.006355200428515673 + 0.01 * 6.9288434982299805
Epoch 730, val loss: 1.035828948020935
Epoch 740, training loss: 0.0752822607755661 = 0.006099050398916006 + 0.01 * 6.918320655822754
Epoch 740, val loss: 1.0417442321777344
Epoch 750, training loss: 0.07515285164117813 = 0.005860273726284504 + 0.01 * 6.929257392883301
Epoch 750, val loss: 1.0474865436553955
Epoch 760, training loss: 0.07478891313076019 = 0.005636694375425577 + 0.01 * 6.91522216796875
Epoch 760, val loss: 1.0531078577041626
Epoch 770, training loss: 0.07450829446315765 = 0.005426909774541855 + 0.01 * 6.908138275146484
Epoch 770, val loss: 1.058639407157898
Epoch 780, training loss: 0.07428432255983353 = 0.005230149254202843 + 0.01 * 6.905417442321777
Epoch 780, val loss: 1.0639888048171997
Epoch 790, training loss: 0.07410434633493423 = 0.00504530081525445 + 0.01 * 6.905904293060303
Epoch 790, val loss: 1.069145917892456
Epoch 800, training loss: 0.0737680122256279 = 0.004871939774602652 + 0.01 * 6.889606952667236
Epoch 800, val loss: 1.0742180347442627
Epoch 810, training loss: 0.0737600326538086 = 0.004708380904048681 + 0.01 * 6.905165195465088
Epoch 810, val loss: 1.079079270362854
Epoch 820, training loss: 0.0733284056186676 = 0.0045547327026724815 + 0.01 * 6.8773674964904785
Epoch 820, val loss: 1.0839428901672363
Epoch 830, training loss: 0.07319764792919159 = 0.0044095804914832115 + 0.01 * 6.8788065910339355
Epoch 830, val loss: 1.0885813236236572
Epoch 840, training loss: 0.07303141057491302 = 0.0042722634971141815 + 0.01 * 6.875915050506592
Epoch 840, val loss: 1.0931754112243652
Epoch 850, training loss: 0.07280449569225311 = 0.004142310470342636 + 0.01 * 6.866218090057373
Epoch 850, val loss: 1.0976227521896362
Epoch 860, training loss: 0.0725756585597992 = 0.004019386135041714 + 0.01 * 6.855627059936523
Epoch 860, val loss: 1.1019574403762817
Epoch 870, training loss: 0.072571761906147 = 0.0039029072504490614 + 0.01 * 6.866885662078857
Epoch 870, val loss: 1.1062259674072266
Epoch 880, training loss: 0.07236365973949432 = 0.0037923366762697697 + 0.01 * 6.8571319580078125
Epoch 880, val loss: 1.1103390455245972
Epoch 890, training loss: 0.0721941888332367 = 0.003687405725941062 + 0.01 * 6.85067892074585
Epoch 890, val loss: 1.114415168762207
Epoch 900, training loss: 0.07214286178350449 = 0.003587850835174322 + 0.01 * 6.855501651763916
Epoch 900, val loss: 1.1183745861053467
Epoch 910, training loss: 0.07191762328147888 = 0.0034930272959172726 + 0.01 * 6.8424601554870605
Epoch 910, val loss: 1.1222172975540161
Epoch 920, training loss: 0.0718635693192482 = 0.0034028184600174427 + 0.01 * 6.846075057983398
Epoch 920, val loss: 1.1260353326797485
Epoch 930, training loss: 0.0717826709151268 = 0.0033169593662023544 + 0.01 * 6.846571445465088
Epoch 930, val loss: 1.1297926902770996
Epoch 940, training loss: 0.07155570387840271 = 0.00323498691432178 + 0.01 * 6.832071304321289
Epoch 940, val loss: 1.1333812475204468
Epoch 950, training loss: 0.07141364365816116 = 0.0031567756086587906 + 0.01 * 6.825686931610107
Epoch 950, val loss: 1.136977195739746
Epoch 960, training loss: 0.07128531485795975 = 0.003082062117755413 + 0.01 * 6.82032585144043
Epoch 960, val loss: 1.140459656715393
Epoch 970, training loss: 0.07121177017688751 = 0.0030109945219010115 + 0.01 * 6.820077419281006
Epoch 970, val loss: 1.143843412399292
Epoch 980, training loss: 0.07112974673509598 = 0.002942913444712758 + 0.01 * 6.818683624267578
Epoch 980, val loss: 1.1471598148345947
Epoch 990, training loss: 0.07093235105276108 = 0.002877800725400448 + 0.01 * 6.805454730987549
Epoch 990, val loss: 1.1504579782485962
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8487084870848709
The final CL Acc:0.80494, 0.02426, The final GNN Acc:0.83957, 0.00646
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11664])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10636])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0319583415985107 = 1.9459898471832275 + 0.01 * 8.596858024597168
Epoch 0, val loss: 1.9393812417984009
Epoch 10, training loss: 2.0225892066955566 = 1.9366209506988525 + 0.01 * 8.596822738647461
Epoch 10, val loss: 1.93048095703125
Epoch 20, training loss: 2.011096954345703 = 1.9251298904418945 + 0.01 * 8.596694946289062
Epoch 20, val loss: 1.9192737340927124
Epoch 30, training loss: 1.9953100681304932 = 1.9093464612960815 + 0.01 * 8.596363067626953
Epoch 30, val loss: 1.9034799337387085
Epoch 40, training loss: 1.9723225831985474 = 1.8863718509674072 + 0.01 * 8.595072746276855
Epoch 40, val loss: 1.8803558349609375
Epoch 50, training loss: 1.939943552017212 = 1.8540750741958618 + 0.01 * 8.586844444274902
Epoch 50, val loss: 1.8486943244934082
Epoch 60, training loss: 1.902060627937317 = 1.8166422843933105 + 0.01 * 8.541830062866211
Epoch 60, val loss: 1.8143364191055298
Epoch 70, training loss: 1.869038462638855 = 1.7852927446365356 + 0.01 * 8.374567031860352
Epoch 70, val loss: 1.7876588106155396
Epoch 80, training loss: 1.8333537578582764 = 1.7510663270950317 + 0.01 * 8.228737831115723
Epoch 80, val loss: 1.7570993900299072
Epoch 90, training loss: 1.781409740447998 = 1.7020549774169922 + 0.01 * 7.935478687286377
Epoch 90, val loss: 1.7135965824127197
Epoch 100, training loss: 1.711184024810791 = 1.6353272199630737 + 0.01 * 7.585684776306152
Epoch 100, val loss: 1.6559513807296753
Epoch 110, training loss: 1.6263132095336914 = 1.5520458221435547 + 0.01 * 7.4267401695251465
Epoch 110, val loss: 1.5864605903625488
Epoch 120, training loss: 1.5336273908615112 = 1.4596611261367798 + 0.01 * 7.396622657775879
Epoch 120, val loss: 1.509090781211853
Epoch 130, training loss: 1.4380645751953125 = 1.3642336130142212 + 0.01 * 7.383092403411865
Epoch 130, val loss: 1.4317883253097534
Epoch 140, training loss: 1.3405382633209229 = 1.2668267488479614 + 0.01 * 7.371148586273193
Epoch 140, val loss: 1.3547035455703735
Epoch 150, training loss: 1.2406470775604248 = 1.1669886112213135 + 0.01 * 7.365843772888184
Epoch 150, val loss: 1.2788630723953247
Epoch 160, training loss: 1.1405526399612427 = 1.0669481754302979 + 0.01 * 7.360449314117432
Epoch 160, val loss: 1.2059293985366821
Epoch 170, training loss: 1.0427513122558594 = 0.9692328572273254 + 0.01 * 7.351846694946289
Epoch 170, val loss: 1.1372216939926147
Epoch 180, training loss: 0.9487018585205078 = 0.8753302097320557 + 0.01 * 7.337162017822266
Epoch 180, val loss: 1.0726046562194824
Epoch 190, training loss: 0.859778881072998 = 0.7866182923316956 + 0.01 * 7.316056251525879
Epoch 190, val loss: 1.012145757675171
Epoch 200, training loss: 0.7782700657844543 = 0.705291211605072 + 0.01 * 7.297885894775391
Epoch 200, val loss: 0.9573329091072083
Epoch 210, training loss: 0.7060293555259705 = 0.6331859230995178 + 0.01 * 7.284345626831055
Epoch 210, val loss: 0.9106796979904175
Epoch 220, training loss: 0.6435041427612305 = 0.5707875490188599 + 0.01 * 7.271660804748535
Epoch 220, val loss: 0.8735299110412598
Epoch 230, training loss: 0.5898956060409546 = 0.5172622799873352 + 0.01 * 7.263334274291992
Epoch 230, val loss: 0.8457172513008118
Epoch 240, training loss: 0.5434874296188354 = 0.4709039330482483 + 0.01 * 7.25834846496582
Epoch 240, val loss: 0.8257492780685425
Epoch 250, training loss: 0.5023016929626465 = 0.42974814772605896 + 0.01 * 7.255352973937988
Epoch 250, val loss: 0.8115997910499573
Epoch 260, training loss: 0.4646373391151428 = 0.39212602376937866 + 0.01 * 7.251133441925049
Epoch 260, val loss: 0.8015820980072021
Epoch 270, training loss: 0.4291357100009918 = 0.35668933391571045 + 0.01 * 7.244637966156006
Epoch 270, val loss: 0.7943615913391113
Epoch 280, training loss: 0.3948245644569397 = 0.32245394587516785 + 0.01 * 7.237061500549316
Epoch 280, val loss: 0.7890594005584717
Epoch 290, training loss: 0.3611311912536621 = 0.28885456919670105 + 0.01 * 7.227663516998291
Epoch 290, val loss: 0.785413920879364
Epoch 300, training loss: 0.328281044960022 = 0.2560722529888153 + 0.01 * 7.220879077911377
Epoch 300, val loss: 0.7839263677597046
Epoch 310, training loss: 0.29704898595809937 = 0.22494766116142273 + 0.01 * 7.2101311683654785
Epoch 310, val loss: 0.7849984765052795
Epoch 320, training loss: 0.2684740722179413 = 0.1964716911315918 + 0.01 * 7.20023775100708
Epoch 320, val loss: 0.7888160943984985
Epoch 330, training loss: 0.24332085251808167 = 0.17135848104953766 + 0.01 * 7.196237564086914
Epoch 330, val loss: 0.7953963279724121
Epoch 340, training loss: 0.2216373085975647 = 0.14979323744773865 + 0.01 * 7.184406280517578
Epoch 340, val loss: 0.8042893409729004
Epoch 350, training loss: 0.2034081220626831 = 0.1314164102077484 + 0.01 * 7.19917106628418
Epoch 350, val loss: 0.8149281144142151
Epoch 360, training loss: 0.18754377961158752 = 0.11576506495475769 + 0.01 * 7.177871227264404
Epoch 360, val loss: 0.8268336057662964
Epoch 370, training loss: 0.17394764721393585 = 0.10233178734779358 + 0.01 * 7.161585807800293
Epoch 370, val loss: 0.8396763205528259
Epoch 380, training loss: 0.1622239649295807 = 0.09070421755313873 + 0.01 * 7.151974201202393
Epoch 380, val loss: 0.8531563878059387
Epoch 390, training loss: 0.15205830335617065 = 0.0805891752243042 + 0.01 * 7.146913528442383
Epoch 390, val loss: 0.8671783208847046
Epoch 400, training loss: 0.1431446522474289 = 0.07175645232200623 + 0.01 * 7.138820171356201
Epoch 400, val loss: 0.8815351128578186
Epoch 410, training loss: 0.13537028431892395 = 0.06402081996202469 + 0.01 * 7.134946346282959
Epoch 410, val loss: 0.8961008191108704
Epoch 420, training loss: 0.12851910293102264 = 0.057231079787015915 + 0.01 * 7.128802299499512
Epoch 420, val loss: 0.9109446406364441
Epoch 430, training loss: 0.12250180542469025 = 0.05127435550093651 + 0.01 * 7.122744560241699
Epoch 430, val loss: 0.9258248805999756
Epoch 440, training loss: 0.11727306246757507 = 0.046050045639276505 + 0.01 * 7.1223015785217285
Epoch 440, val loss: 0.9407895803451538
Epoch 450, training loss: 0.11276103556156158 = 0.04146506264805794 + 0.01 * 7.1295976638793945
Epoch 450, val loss: 0.9557152390480042
Epoch 460, training loss: 0.10853293538093567 = 0.037446536123752594 + 0.01 * 7.108639717102051
Epoch 460, val loss: 0.9705273509025574
Epoch 470, training loss: 0.10498537123203278 = 0.033917687833309174 + 0.01 * 7.1067681312561035
Epoch 470, val loss: 0.9852296113967896
Epoch 480, training loss: 0.10198698937892914 = 0.030814535915851593 + 0.01 * 7.117245674133301
Epoch 480, val loss: 0.9996606707572937
Epoch 490, training loss: 0.09904053807258606 = 0.028088700026273727 + 0.01 * 7.095184326171875
Epoch 490, val loss: 1.0138145685195923
Epoch 500, training loss: 0.09660178422927856 = 0.025685882195830345 + 0.01 * 7.091590404510498
Epoch 500, val loss: 1.0277297496795654
Epoch 510, training loss: 0.09451283514499664 = 0.023562729358673096 + 0.01 * 7.095010757446289
Epoch 510, val loss: 1.0413120985031128
Epoch 520, training loss: 0.09255386888980865 = 0.0216844342648983 + 0.01 * 7.086944103240967
Epoch 520, val loss: 1.0544923543930054
Epoch 530, training loss: 0.09079806506633759 = 0.0200177151709795 + 0.01 * 7.078035354614258
Epoch 530, val loss: 1.0673861503601074
Epoch 540, training loss: 0.08928769826889038 = 0.01853374019265175 + 0.01 * 7.075395584106445
Epoch 540, val loss: 1.079838514328003
Epoch 550, training loss: 0.08789335936307907 = 0.017209233716130257 + 0.01 * 7.068412780761719
Epoch 550, val loss: 1.0919746160507202
Epoch 560, training loss: 0.08669823408126831 = 0.0160219743847847 + 0.01 * 7.067626476287842
Epoch 560, val loss: 1.1037635803222656
Epoch 570, training loss: 0.08557580411434174 = 0.014956752769649029 + 0.01 * 7.061905384063721
Epoch 570, val loss: 1.1151831150054932
Epoch 580, training loss: 0.08451613783836365 = 0.01399737223982811 + 0.01 * 7.051876068115234
Epoch 580, val loss: 1.1261547803878784
Epoch 590, training loss: 0.08368312567472458 = 0.01312987320125103 + 0.01 * 7.055325031280518
Epoch 590, val loss: 1.1368637084960938
Epoch 600, training loss: 0.08283055573701859 = 0.012345504947006702 + 0.01 * 7.048504829406738
Epoch 600, val loss: 1.1472276449203491
Epoch 610, training loss: 0.08201119303703308 = 0.011632527224719524 + 0.01 * 7.037867069244385
Epoch 610, val loss: 1.1572521924972534
Epoch 620, training loss: 0.08130954205989838 = 0.0109823914244771 + 0.01 * 7.03271484375
Epoch 620, val loss: 1.1669402122497559
Epoch 630, training loss: 0.08070290088653564 = 0.010389383882284164 + 0.01 * 7.0313520431518555
Epoch 630, val loss: 1.1763209104537964
Epoch 640, training loss: 0.08006230741739273 = 0.009846578352153301 + 0.01 * 7.021573543548584
Epoch 640, val loss: 1.1853959560394287
Epoch 650, training loss: 0.07956989109516144 = 0.009347970597445965 + 0.01 * 7.022192478179932
Epoch 650, val loss: 1.1942555904388428
Epoch 660, training loss: 0.07900428771972656 = 0.0088892737403512 + 0.01 * 7.011501312255859
Epoch 660, val loss: 1.2027897834777832
Epoch 670, training loss: 0.07853680849075317 = 0.00846617016941309 + 0.01 * 7.007063865661621
Epoch 670, val loss: 1.2111237049102783
Epoch 680, training loss: 0.07817784696817398 = 0.008075515739619732 + 0.01 * 7.010233402252197
Epoch 680, val loss: 1.2191810607910156
Epoch 690, training loss: 0.07775835692882538 = 0.007714618928730488 + 0.01 * 7.004373550415039
Epoch 690, val loss: 1.2269591093063354
Epoch 700, training loss: 0.07727565616369247 = 0.007379512302577496 + 0.01 * 6.989614486694336
Epoch 700, val loss: 1.2345472574234009
Epoch 710, training loss: 0.07689093798398972 = 0.00706799142062664 + 0.01 * 6.98229455947876
Epoch 710, val loss: 1.2419519424438477
Epoch 720, training loss: 0.07674054801464081 = 0.0067776888608932495 + 0.01 * 6.996286392211914
Epoch 720, val loss: 1.249171495437622
Epoch 730, training loss: 0.07624500244855881 = 0.006507673766463995 + 0.01 * 6.973732948303223
Epoch 730, val loss: 1.256096601486206
Epoch 740, training loss: 0.07619500905275345 = 0.006255312357097864 + 0.01 * 6.993969440460205
Epoch 740, val loss: 1.2628251314163208
Epoch 750, training loss: 0.07572041451931 = 0.006019230000674725 + 0.01 * 6.970118999481201
Epoch 750, val loss: 1.2694586515426636
Epoch 760, training loss: 0.07564058899879456 = 0.005798495840281248 + 0.01 * 6.9842095375061035
Epoch 760, val loss: 1.275823950767517
Epoch 770, training loss: 0.07521041482686996 = 0.005591786932200193 + 0.01 * 6.961862564086914
Epoch 770, val loss: 1.2819868326187134
Epoch 780, training loss: 0.0749737098813057 = 0.005397231318056583 + 0.01 * 6.957648277282715
Epoch 780, val loss: 1.2880653142929077
Epoch 790, training loss: 0.07471750676631927 = 0.005214234814047813 + 0.01 * 6.950327396392822
Epoch 790, val loss: 1.2939623594284058
Epoch 800, training loss: 0.07450351864099503 = 0.0050416444428265095 + 0.01 * 6.946187973022461
Epoch 800, val loss: 1.2997386455535889
Epoch 810, training loss: 0.07425808161497116 = 0.004879156593233347 + 0.01 * 6.937892913818359
Epoch 810, val loss: 1.3052849769592285
Epoch 820, training loss: 0.07405994832515717 = 0.004725569859147072 + 0.01 * 6.933437824249268
Epoch 820, val loss: 1.310775637626648
Epoch 830, training loss: 0.07397773861885071 = 0.004580486565828323 + 0.01 * 6.939725399017334
Epoch 830, val loss: 1.3160496950149536
Epoch 840, training loss: 0.07377943396568298 = 0.004442896693944931 + 0.01 * 6.933654308319092
Epoch 840, val loss: 1.3212366104125977
Epoch 850, training loss: 0.07343694567680359 = 0.004312761127948761 + 0.01 * 6.912418842315674
Epoch 850, val loss: 1.326299786567688
Epoch 860, training loss: 0.07379491627216339 = 0.004188884049654007 + 0.01 * 6.960602760314941
Epoch 860, val loss: 1.3313015699386597
Epoch 870, training loss: 0.07336025685071945 = 0.004071532282978296 + 0.01 * 6.928872585296631
Epoch 870, val loss: 1.3360650539398193
Epoch 880, training loss: 0.07315923273563385 = 0.0039605023339390755 + 0.01 * 6.919873237609863
Epoch 880, val loss: 1.3407262563705444
Epoch 890, training loss: 0.0730019062757492 = 0.003853923873975873 + 0.01 * 6.914798259735107
Epoch 890, val loss: 1.345340609550476
Epoch 900, training loss: 0.07274896651506424 = 0.0037535037845373154 + 0.01 * 6.899546146392822
Epoch 900, val loss: 1.3498262166976929
Epoch 910, training loss: 0.07265977561473846 = 0.0036569361109286547 + 0.01 * 6.9002838134765625
Epoch 910, val loss: 1.354207992553711
Epoch 920, training loss: 0.07248345017433167 = 0.003565377788618207 + 0.01 * 6.891807556152344
Epoch 920, val loss: 1.3585253953933716
Epoch 930, training loss: 0.072609543800354 = 0.0034771726932376623 + 0.01 * 6.91323709487915
Epoch 930, val loss: 1.3627519607543945
Epoch 940, training loss: 0.0722283273935318 = 0.0033941210713237524 + 0.01 * 6.883420944213867
Epoch 940, val loss: 1.3668204545974731
Epoch 950, training loss: 0.07256145775318146 = 0.0033137560822069645 + 0.01 * 6.924769878387451
Epoch 950, val loss: 1.370855450630188
Epoch 960, training loss: 0.07192183285951614 = 0.0032373634167015553 + 0.01 * 6.8684468269348145
Epoch 960, val loss: 1.3747469186782837
Epoch 970, training loss: 0.0719260722398758 = 0.003164136316627264 + 0.01 * 6.876194000244141
Epoch 970, val loss: 1.3785760402679443
Epoch 980, training loss: 0.07204806804656982 = 0.003094061277806759 + 0.01 * 6.8954010009765625
Epoch 980, val loss: 1.38229238986969
Epoch 990, training loss: 0.07180412113666534 = 0.0030269944109022617 + 0.01 * 6.877713203430176
Epoch 990, val loss: 1.3859734535217285
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8239325250395362
=== training gcn model ===
Epoch 0, training loss: 2.0369558334350586 = 1.9509875774383545 + 0.01 * 8.596817016601562
Epoch 0, val loss: 1.9472862482070923
Epoch 10, training loss: 2.026679039001465 = 1.940711259841919 + 0.01 * 8.596778869628906
Epoch 10, val loss: 1.9375133514404297
Epoch 20, training loss: 2.014350414276123 = 1.9283844232559204 + 0.01 * 8.596601486206055
Epoch 20, val loss: 1.925309419631958
Epoch 30, training loss: 1.997353196144104 = 1.911392331123352 + 0.01 * 8.596088409423828
Epoch 30, val loss: 1.9079899787902832
Epoch 40, training loss: 1.9727909564971924 = 1.886855125427246 + 0.01 * 8.593578338623047
Epoch 40, val loss: 1.882636547088623
Epoch 50, training loss: 1.9389119148254395 = 1.853179931640625 + 0.01 * 8.573192596435547
Epoch 50, val loss: 1.8486077785491943
Epoch 60, training loss: 1.9009346961975098 = 1.8164316415786743 + 0.01 * 8.45030403137207
Epoch 60, val loss: 1.8139864206314087
Epoch 70, training loss: 1.870645523071289 = 1.7875267267227173 + 0.01 * 8.311874389648438
Epoch 70, val loss: 1.789219856262207
Epoch 80, training loss: 1.8359946012496948 = 1.7552244663238525 + 0.01 * 8.077008247375488
Epoch 80, val loss: 1.7613921165466309
Epoch 90, training loss: 1.7868943214416504 = 1.7101337909698486 + 0.01 * 7.676050662994385
Epoch 90, val loss: 1.7223587036132812
Epoch 100, training loss: 1.7219903469085693 = 1.646696925163269 + 0.01 * 7.5293474197387695
Epoch 100, val loss: 1.6670769453048706
Epoch 110, training loss: 1.6393260955810547 = 1.564767837524414 + 0.01 * 7.455822467803955
Epoch 110, val loss: 1.5975228548049927
Epoch 120, training loss: 1.5461492538452148 = 1.472408652305603 + 0.01 * 7.374057292938232
Epoch 120, val loss: 1.5221199989318848
Epoch 130, training loss: 1.4515584707260132 = 1.3782932758331299 + 0.01 * 7.3265180587768555
Epoch 130, val loss: 1.4493438005447388
Epoch 140, training loss: 1.3574104309082031 = 1.284424901008606 + 0.01 * 7.298548698425293
Epoch 140, val loss: 1.3800691366195679
Epoch 150, training loss: 1.2648847103118896 = 1.1922436952590942 + 0.01 * 7.26410436630249
Epoch 150, val loss: 1.3152458667755127
Epoch 160, training loss: 1.177486538887024 = 1.1051369905471802 + 0.01 * 7.234952449798584
Epoch 160, val loss: 1.2567867040634155
Epoch 170, training loss: 1.098114252090454 = 1.025991678237915 + 0.01 * 7.212255001068115
Epoch 170, val loss: 1.206902265548706
Epoch 180, training loss: 1.026636004447937 = 0.9546409249305725 + 0.01 * 7.1995134353637695
Epoch 180, val loss: 1.1643091440200806
Epoch 190, training loss: 0.960131049156189 = 0.888289213180542 + 0.01 * 7.184182167053223
Epoch 190, val loss: 1.125831961631775
Epoch 200, training loss: 0.895584762096405 = 0.8238438367843628 + 0.01 * 7.1740946769714355
Epoch 200, val loss: 1.0884133577346802
Epoch 210, training loss: 0.831743597984314 = 0.7600994110107422 + 0.01 * 7.164419174194336
Epoch 210, val loss: 1.0513237714767456
Epoch 220, training loss: 0.7698207497596741 = 0.6982712745666504 + 0.01 * 7.15494966506958
Epoch 220, val loss: 1.0164867639541626
Epoch 230, training loss: 0.7118750810623169 = 0.6403972506523132 + 0.01 * 7.147783279418945
Epoch 230, val loss: 0.9868271350860596
Epoch 240, training loss: 0.6590744853019714 = 0.587714672088623 + 0.01 * 7.135982990264893
Epoch 240, val loss: 0.9640925526618958
Epoch 250, training loss: 0.6118213534355164 = 0.5402290225028992 + 0.01 * 7.159231662750244
Epoch 250, val loss: 0.9485734701156616
Epoch 260, training loss: 0.5684143900871277 = 0.49718594551086426 + 0.01 * 7.122844219207764
Epoch 260, val loss: 0.9394089579582214
Epoch 270, training loss: 0.528409481048584 = 0.4572071433067322 + 0.01 * 7.120232582092285
Epoch 270, val loss: 0.9348076581954956
Epoch 280, training loss: 0.49002230167388916 = 0.4188871681690216 + 0.01 * 7.11351203918457
Epoch 280, val loss: 0.9334859251976013
Epoch 290, training loss: 0.45238831639289856 = 0.38128697872161865 + 0.01 * 7.110133647918701
Epoch 290, val loss: 0.9341970086097717
Epoch 300, training loss: 0.4150833487510681 = 0.3440258502960205 + 0.01 * 7.105748653411865
Epoch 300, val loss: 0.936265230178833
Epoch 310, training loss: 0.37841081619262695 = 0.3073670268058777 + 0.01 * 7.1043782234191895
Epoch 310, val loss: 0.9397889375686646
Epoch 320, training loss: 0.3430372476577759 = 0.27204644680023193 + 0.01 * 7.099080562591553
Epoch 320, val loss: 0.9449750185012817
Epoch 330, training loss: 0.31024566292762756 = 0.23890742659568787 + 0.01 * 7.133824825286865
Epoch 330, val loss: 0.9522694945335388
Epoch 340, training loss: 0.27967512607574463 = 0.20871034264564514 + 0.01 * 7.096477508544922
Epoch 340, val loss: 0.9619251489639282
Epoch 350, training loss: 0.252641499042511 = 0.1817636787891388 + 0.01 * 7.0877814292907715
Epoch 350, val loss: 0.9738369584083557
Epoch 360, training loss: 0.22898247838020325 = 0.15810716152191162 + 0.01 * 7.087531089782715
Epoch 360, val loss: 0.9877716302871704
Epoch 370, training loss: 0.2084721028804779 = 0.13760443031787872 + 0.01 * 7.086766719818115
Epoch 370, val loss: 1.0034123659133911
Epoch 380, training loss: 0.19088178873062134 = 0.11998721957206726 + 0.01 * 7.089457035064697
Epoch 380, val loss: 1.0203956365585327
Epoch 390, training loss: 0.17573198676109314 = 0.10490275919437408 + 0.01 * 7.082923412322998
Epoch 390, val loss: 1.0385125875473022
Epoch 400, training loss: 0.1627284586429596 = 0.0919836014509201 + 0.01 * 7.074484825134277
Epoch 400, val loss: 1.0573312044143677
Epoch 410, training loss: 0.15170256793498993 = 0.08090060204267502 + 0.01 * 7.080196380615234
Epoch 410, val loss: 1.0767427682876587
Epoch 420, training loss: 0.14209434390068054 = 0.07138128578662872 + 0.01 * 7.071306228637695
Epoch 420, val loss: 1.0962998867034912
Epoch 430, training loss: 0.1338525116443634 = 0.06317555159330368 + 0.01 * 7.067697048187256
Epoch 430, val loss: 1.1160223484039307
Epoch 440, training loss: 0.12669771909713745 = 0.05609045550227165 + 0.01 * 7.060726165771484
Epoch 440, val loss: 1.135744571685791
Epoch 450, training loss: 0.12076373398303986 = 0.049961384385824203 + 0.01 * 7.080234527587891
Epoch 450, val loss: 1.155363917350769
Epoch 460, training loss: 0.11514036357402802 = 0.04466037079691887 + 0.01 * 7.047999382019043
Epoch 460, val loss: 1.1747204065322876
Epoch 470, training loss: 0.11050066351890564 = 0.04006677865982056 + 0.01 * 7.043388366699219
Epoch 470, val loss: 1.1938046216964722
Epoch 480, training loss: 0.10651323199272156 = 0.036083467304706573 + 0.01 * 7.042976379394531
Epoch 480, val loss: 1.2125500440597534
Epoch 490, training loss: 0.10307895392179489 = 0.03262607753276825 + 0.01 * 7.0452880859375
Epoch 490, val loss: 1.2306948900222778
Epoch 500, training loss: 0.0999872237443924 = 0.029614904895424843 + 0.01 * 7.037231922149658
Epoch 500, val loss: 1.2484333515167236
Epoch 510, training loss: 0.09723770618438721 = 0.026987379416823387 + 0.01 * 7.0250325202941895
Epoch 510, val loss: 1.2654997110366821
Epoch 520, training loss: 0.09502582997083664 = 0.02469060570001602 + 0.01 * 7.033522605895996
Epoch 520, val loss: 1.2820459604263306
Epoch 530, training loss: 0.09284761548042297 = 0.02267611026763916 + 0.01 * 7.01715087890625
Epoch 530, val loss: 1.2978936433792114
Epoch 540, training loss: 0.09103194624185562 = 0.020901193842291832 + 0.01 * 7.013075351715088
Epoch 540, val loss: 1.313193440437317
Epoch 550, training loss: 0.08937761932611465 = 0.01933075301349163 + 0.01 * 7.00468635559082
Epoch 550, val loss: 1.3280177116394043
Epoch 560, training loss: 0.08801014721393585 = 0.017936047166585922 + 0.01 * 7.007410526275635
Epoch 560, val loss: 1.3421660661697388
Epoch 570, training loss: 0.08671761304140091 = 0.016693832352757454 + 0.01 * 7.002378463745117
Epoch 570, val loss: 1.3558927774429321
Epoch 580, training loss: 0.08564276248216629 = 0.015582042746245861 + 0.01 * 7.006072521209717
Epoch 580, val loss: 1.3690458536148071
Epoch 590, training loss: 0.08454301953315735 = 0.014585231430828571 + 0.01 * 6.995779037475586
Epoch 590, val loss: 1.381665825843811
Epoch 600, training loss: 0.08346247673034668 = 0.01368668582290411 + 0.01 * 6.977579116821289
Epoch 600, val loss: 1.393904447555542
Epoch 610, training loss: 0.08308138698339462 = 0.012873723171651363 + 0.01 * 7.020766735076904
Epoch 610, val loss: 1.4056885242462158
Epoch 620, training loss: 0.08188333362340927 = 0.012136266566812992 + 0.01 * 6.974707126617432
Epoch 620, val loss: 1.416921615600586
Epoch 630, training loss: 0.0811733603477478 = 0.011465342715382576 + 0.01 * 6.970801830291748
Epoch 630, val loss: 1.4278281927108765
Epoch 640, training loss: 0.08043599128723145 = 0.010853910818696022 + 0.01 * 6.958208084106445
Epoch 640, val loss: 1.438363790512085
Epoch 650, training loss: 0.08011937886476517 = 0.010294427163898945 + 0.01 * 6.982495307922363
Epoch 650, val loss: 1.4485366344451904
Epoch 660, training loss: 0.07931248843669891 = 0.009780613705515862 + 0.01 * 6.953187465667725
Epoch 660, val loss: 1.458236575126648
Epoch 670, training loss: 0.07863450795412064 = 0.009308435022830963 + 0.01 * 6.932607173919678
Epoch 670, val loss: 1.4677276611328125
Epoch 680, training loss: 0.0782204121351242 = 0.00887296162545681 + 0.01 * 6.9347453117370605
Epoch 680, val loss: 1.476909875869751
Epoch 690, training loss: 0.07764966040849686 = 0.008470441214740276 + 0.01 * 6.917922496795654
Epoch 690, val loss: 1.485711693763733
Epoch 700, training loss: 0.07747483253479004 = 0.008097702637314796 + 0.01 * 6.937713146209717
Epoch 700, val loss: 1.4943292140960693
Epoch 710, training loss: 0.07695724070072174 = 0.0077520087361335754 + 0.01 * 6.920523166656494
Epoch 710, val loss: 1.5025495290756226
Epoch 720, training loss: 0.07669862359762192 = 0.007430523633956909 + 0.01 * 6.926810264587402
Epoch 720, val loss: 1.5105223655700684
Epoch 730, training loss: 0.07626292109489441 = 0.007131886202841997 + 0.01 * 6.913103103637695
Epoch 730, val loss: 1.5184156894683838
Epoch 740, training loss: 0.07597418874502182 = 0.006852680817246437 + 0.01 * 6.912150859832764
Epoch 740, val loss: 1.5259047746658325
Epoch 750, training loss: 0.0755021870136261 = 0.006591949611902237 + 0.01 * 6.8910231590271
Epoch 750, val loss: 1.5332744121551514
Epoch 760, training loss: 0.07518844306468964 = 0.006347657646983862 + 0.01 * 6.8840789794921875
Epoch 760, val loss: 1.540308952331543
Epoch 770, training loss: 0.07494159042835236 = 0.006118562072515488 + 0.01 * 6.882302284240723
Epoch 770, val loss: 1.5472886562347412
Epoch 780, training loss: 0.07462619245052338 = 0.005903659388422966 + 0.01 * 6.87225341796875
Epoch 780, val loss: 1.5539714097976685
Epoch 790, training loss: 0.07443060725927353 = 0.00570174353197217 + 0.01 * 6.872886657714844
Epoch 790, val loss: 1.5604523420333862
Epoch 800, training loss: 0.07441974431276321 = 0.00551138399168849 + 0.01 * 6.890836238861084
Epoch 800, val loss: 1.5667606592178345
Epoch 810, training loss: 0.07399260252714157 = 0.0053322408348321915 + 0.01 * 6.866036415100098
Epoch 810, val loss: 1.572932243347168
Epoch 820, training loss: 0.07386545836925507 = 0.005162848625332117 + 0.01 * 6.870261192321777
Epoch 820, val loss: 1.5788729190826416
Epoch 830, training loss: 0.07344640046358109 = 0.005003047175705433 + 0.01 * 6.844335556030273
Epoch 830, val loss: 1.5847036838531494
Epoch 840, training loss: 0.07349619269371033 = 0.004851792939007282 + 0.01 * 6.864439487457275
Epoch 840, val loss: 1.5903676748275757
Epoch 850, training loss: 0.07317911833524704 = 0.0047084917314350605 + 0.01 * 6.847062587738037
Epoch 850, val loss: 1.5958753824234009
Epoch 860, training loss: 0.07302159816026688 = 0.004572675097733736 + 0.01 * 6.844892501831055
Epoch 860, val loss: 1.601223111152649
Epoch 870, training loss: 0.07280422002077103 = 0.004443719517439604 + 0.01 * 6.836050033569336
Epoch 870, val loss: 1.6064248085021973
Epoch 880, training loss: 0.07263022661209106 = 0.004321004264056683 + 0.01 * 6.830922603607178
Epoch 880, val loss: 1.6114627122879028
Epoch 890, training loss: 0.07241611182689667 = 0.0042045642621815205 + 0.01 * 6.821154594421387
Epoch 890, val loss: 1.6164430379867554
Epoch 900, training loss: 0.07232481241226196 = 0.004093680530786514 + 0.01 * 6.823113441467285
Epoch 900, val loss: 1.6212457418441772
Epoch 910, training loss: 0.07229823619127274 = 0.003988064359873533 + 0.01 * 6.831017017364502
Epoch 910, val loss: 1.625905156135559
Epoch 920, training loss: 0.07209598273038864 = 0.0038875150494277477 + 0.01 * 6.820847511291504
Epoch 920, val loss: 1.6304618120193481
Epoch 930, training loss: 0.07213855534791946 = 0.0037915795110166073 + 0.01 * 6.834697723388672
Epoch 930, val loss: 1.6349083185195923
Epoch 940, training loss: 0.07177944481372833 = 0.003699958324432373 + 0.01 * 6.807948589324951
Epoch 940, val loss: 1.6392345428466797
Epoch 950, training loss: 0.07164175808429718 = 0.003612498752772808 + 0.01 * 6.802926063537598
Epoch 950, val loss: 1.6434533596038818
Epoch 960, training loss: 0.07170981168746948 = 0.0035287528298795223 + 0.01 * 6.818106174468994
Epoch 960, val loss: 1.647567868232727
Epoch 970, training loss: 0.07144559174776077 = 0.003448720322921872 + 0.01 * 6.79968786239624
Epoch 970, val loss: 1.6515445709228516
Epoch 980, training loss: 0.07144179940223694 = 0.003371975151821971 + 0.01 * 6.806982517242432
Epoch 980, val loss: 1.6554399728775024
Epoch 990, training loss: 0.07125779241323471 = 0.003298562252894044 + 0.01 * 6.795922756195068
Epoch 990, val loss: 1.6593506336212158
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.819715340010543
=== training gcn model ===
Epoch 0, training loss: 2.0400335788726807 = 1.954065203666687 + 0.01 * 8.596845626831055
Epoch 0, val loss: 1.9525411128997803
Epoch 10, training loss: 2.030177593231201 = 1.9442095756530762 + 0.01 * 8.59681224822998
Epoch 10, val loss: 1.9435827732086182
Epoch 20, training loss: 2.018059730529785 = 1.9320930242538452 + 0.01 * 8.596665382385254
Epoch 20, val loss: 1.9322925806045532
Epoch 30, training loss: 2.0011653900146484 = 1.91520357131958 + 0.01 * 8.596181869506836
Epoch 30, val loss: 1.9163818359375
Epoch 40, training loss: 1.9763002395629883 = 1.8903617858886719 + 0.01 * 8.593840599060059
Epoch 40, val loss: 1.8931779861450195
Epoch 50, training loss: 1.9416346549987793 = 1.8558465242385864 + 0.01 * 8.578818321228027
Epoch 50, val loss: 1.8621587753295898
Epoch 60, training loss: 1.9031033515930176 = 1.8179041147232056 + 0.01 * 8.519927978515625
Epoch 60, val loss: 1.830091953277588
Epoch 70, training loss: 1.8718883991241455 = 1.7878284454345703 + 0.01 * 8.405991554260254
Epoch 70, val loss: 1.8017945289611816
Epoch 80, training loss: 1.8370780944824219 = 1.7541145086288452 + 0.01 * 8.296356201171875
Epoch 80, val loss: 1.7655407190322876
Epoch 90, training loss: 1.7878153324127197 = 1.706708550453186 + 0.01 * 8.110678672790527
Epoch 90, val loss: 1.7224973440170288
Epoch 100, training loss: 1.7198976278305054 = 1.6404484510421753 + 0.01 * 7.944923400878906
Epoch 100, val loss: 1.6671416759490967
Epoch 110, training loss: 1.6363064050674438 = 1.5591681003570557 + 0.01 * 7.713832378387451
Epoch 110, val loss: 1.6006304025650024
Epoch 120, training loss: 1.5500702857971191 = 1.474058747291565 + 0.01 * 7.6011576652526855
Epoch 120, val loss: 1.5321171283721924
Epoch 130, training loss: 1.468051552772522 = 1.392752766609192 + 0.01 * 7.529876708984375
Epoch 130, val loss: 1.469774603843689
Epoch 140, training loss: 1.389967918395996 = 1.3152997493743896 + 0.01 * 7.466821193695068
Epoch 140, val loss: 1.4130522012710571
Epoch 150, training loss: 1.3124173879623413 = 1.2383911609649658 + 0.01 * 7.40262508392334
Epoch 150, val loss: 1.3586370944976807
Epoch 160, training loss: 1.2356016635894775 = 1.1620233058929443 + 0.01 * 7.357841491699219
Epoch 160, val loss: 1.3078689575195312
Epoch 170, training loss: 1.1610915660858154 = 1.0877677202224731 + 0.01 * 7.332388877868652
Epoch 170, val loss: 1.261981725692749
Epoch 180, training loss: 1.0899962186813354 = 1.0169528722763062 + 0.01 * 7.304330348968506
Epoch 180, val loss: 1.2209826707839966
Epoch 190, training loss: 1.0230154991149902 = 0.9502918720245361 + 0.01 * 7.272365570068359
Epoch 190, val loss: 1.1846768856048584
Epoch 200, training loss: 0.959824800491333 = 0.8873816132545471 + 0.01 * 7.2443156242370605
Epoch 200, val loss: 1.1516363620758057
Epoch 210, training loss: 0.8988003134727478 = 0.8265963196754456 + 0.01 * 7.220396995544434
Epoch 210, val loss: 1.1202542781829834
Epoch 220, training loss: 0.838106095790863 = 0.7660685181617737 + 0.01 * 7.203759670257568
Epoch 220, val loss: 1.0896058082580566
Epoch 230, training loss: 0.7764856219291687 = 0.7045484185218811 + 0.01 * 7.193719863891602
Epoch 230, val loss: 1.059606671333313
Epoch 240, training loss: 0.7131258845329285 = 0.6412306427955627 + 0.01 * 7.189526557922363
Epoch 240, val loss: 1.0307120084762573
Epoch 250, training loss: 0.6484524607658386 = 0.5766512155532837 + 0.01 * 7.180123805999756
Epoch 250, val loss: 1.0035730600357056
Epoch 260, training loss: 0.5845403075218201 = 0.5127761960029602 + 0.01 * 7.1764116287231445
Epoch 260, val loss: 0.9799636006355286
Epoch 270, training loss: 0.5237736701965332 = 0.45206961035728455 + 0.01 * 7.170403003692627
Epoch 270, val loss: 0.9617642760276794
Epoch 280, training loss: 0.46788129210472107 = 0.39620524644851685 + 0.01 * 7.167604923248291
Epoch 280, val loss: 0.9502754211425781
Epoch 290, training loss: 0.41712215542793274 = 0.345504492521286 + 0.01 * 7.161766052246094
Epoch 290, val loss: 0.9458025693893433
Epoch 300, training loss: 0.3714737892150879 = 0.29986679553985596 + 0.01 * 7.160698413848877
Epoch 300, val loss: 0.9477043151855469
Epoch 310, training loss: 0.3307955265045166 = 0.2592073976993561 + 0.01 * 7.158812999725342
Epoch 310, val loss: 0.955161988735199
Epoch 320, training loss: 0.295073002576828 = 0.22351036965847015 + 0.01 * 7.156264305114746
Epoch 320, val loss: 0.9671412706375122
Epoch 330, training loss: 0.2641541659832001 = 0.19261935353279114 + 0.01 * 7.153481960296631
Epoch 330, val loss: 0.9825783967971802
Epoch 340, training loss: 0.23772895336151123 = 0.16613025963306427 + 0.01 * 7.159869194030762
Epoch 340, val loss: 1.0006788969039917
Epoch 350, training loss: 0.21498963236808777 = 0.14349187910556793 + 0.01 * 7.149775981903076
Epoch 350, val loss: 1.0206667184829712
Epoch 360, training loss: 0.19560447335243225 = 0.12412887811660767 + 0.01 * 7.147560119628906
Epoch 360, val loss: 1.042104959487915
Epoch 370, training loss: 0.17900711297988892 = 0.10756047070026398 + 0.01 * 7.1446638107299805
Epoch 370, val loss: 1.0646309852600098
Epoch 380, training loss: 0.164840966463089 = 0.09341877698898315 + 0.01 * 7.1422200202941895
Epoch 380, val loss: 1.0879740715026855
Epoch 390, training loss: 0.15281599760055542 = 0.08142741769552231 + 0.01 * 7.138857841491699
Epoch 390, val loss: 1.111838698387146
Epoch 400, training loss: 0.14276933670043945 = 0.07132389396429062 + 0.01 * 7.1445441246032715
Epoch 400, val loss: 1.136009931564331
Epoch 410, training loss: 0.13414987921714783 = 0.06282363086938858 + 0.01 * 7.13262414932251
Epoch 410, val loss: 1.1603913307189941
Epoch 420, training loss: 0.12694358825683594 = 0.055639803409576416 + 0.01 * 7.130378246307373
Epoch 420, val loss: 1.1847152709960938
Epoch 430, training loss: 0.12079526484012604 = 0.04953150823712349 + 0.01 * 7.126375198364258
Epoch 430, val loss: 1.2086780071258545
Epoch 440, training loss: 0.11557552218437195 = 0.044309768825769424 + 0.01 * 7.126574993133545
Epoch 440, val loss: 1.2322040796279907
Epoch 450, training loss: 0.11103709042072296 = 0.0398254469037056 + 0.01 * 7.121164798736572
Epoch 450, val loss: 1.255057692527771
Epoch 460, training loss: 0.10709740221500397 = 0.03595278039574623 + 0.01 * 7.114462375640869
Epoch 460, val loss: 1.2773631811141968
Epoch 470, training loss: 0.10370743274688721 = 0.03259200602769852 + 0.01 * 7.111542701721191
Epoch 470, val loss: 1.298926830291748
Epoch 480, training loss: 0.10070325434207916 = 0.029662596061825752 + 0.01 * 7.104065895080566
Epoch 480, val loss: 1.3197647333145142
Epoch 490, training loss: 0.09817121922969818 = 0.027096526697278023 + 0.01 * 7.10746955871582
Epoch 490, val loss: 1.3399208784103394
Epoch 500, training loss: 0.09583431482315063 = 0.024843772873282433 + 0.01 * 7.099053859710693
Epoch 500, val loss: 1.3592615127563477
Epoch 510, training loss: 0.09380146861076355 = 0.02285701222717762 + 0.01 * 7.094445705413818
Epoch 510, val loss: 1.3778669834136963
Epoch 520, training loss: 0.09198214113712311 = 0.021096570417284966 + 0.01 * 7.08855676651001
Epoch 520, val loss: 1.3958150148391724
Epoch 530, training loss: 0.0903654396533966 = 0.019530195742845535 + 0.01 * 7.083524227142334
Epoch 530, val loss: 1.4131613969802856
Epoch 540, training loss: 0.08902839571237564 = 0.018131792545318604 + 0.01 * 7.08966064453125
Epoch 540, val loss: 1.429774284362793
Epoch 550, training loss: 0.08764971047639847 = 0.016880204901099205 + 0.01 * 7.076950550079346
Epoch 550, val loss: 1.4458338022232056
Epoch 560, training loss: 0.08654629439115524 = 0.015755772590637207 + 0.01 * 7.079052448272705
Epoch 560, val loss: 1.4612842798233032
Epoch 570, training loss: 0.08545379340648651 = 0.014742432162165642 + 0.01 * 7.071135997772217
Epoch 570, val loss: 1.476271629333496
Epoch 580, training loss: 0.0844520702958107 = 0.013826650567352772 + 0.01 * 7.062541484832764
Epoch 580, val loss: 1.4906235933303833
Epoch 590, training loss: 0.08359779417514801 = 0.012996007688343525 + 0.01 * 7.060178279876709
Epoch 590, val loss: 1.5044481754302979
Epoch 600, training loss: 0.08283057808876038 = 0.012240561656653881 + 0.01 * 7.059001922607422
Epoch 600, val loss: 1.5178961753845215
Epoch 610, training loss: 0.08204364031553268 = 0.011552180163562298 + 0.01 * 7.04914665222168
Epoch 610, val loss: 1.530836582183838
Epoch 620, training loss: 0.08140488713979721 = 0.010922687128186226 + 0.01 * 7.048220634460449
Epoch 620, val loss: 1.5432707071304321
Epoch 630, training loss: 0.08079174160957336 = 0.010345976799726486 + 0.01 * 7.044577121734619
Epoch 630, val loss: 1.5554633140563965
Epoch 640, training loss: 0.0801953449845314 = 0.009817165322601795 + 0.01 * 7.037817478179932
Epoch 640, val loss: 1.5670371055603027
Epoch 650, training loss: 0.07965651154518127 = 0.00932995043694973 + 0.01 * 7.032655715942383
Epoch 650, val loss: 1.5784105062484741
Epoch 660, training loss: 0.07924079149961472 = 0.00888004805892706 + 0.01 * 7.036074161529541
Epoch 660, val loss: 1.5893840789794922
Epoch 670, training loss: 0.07874910533428192 = 0.008464816957712173 + 0.01 * 7.02842903137207
Epoch 670, val loss: 1.5999644994735718
Epoch 680, training loss: 0.07831799983978271 = 0.008080018684267998 + 0.01 * 7.023797988891602
Epoch 680, val loss: 1.6102806329727173
Epoch 690, training loss: 0.07802584767341614 = 0.007723377086222172 + 0.01 * 7.030247211456299
Epoch 690, val loss: 1.6200954914093018
Epoch 700, training loss: 0.07754769921302795 = 0.007391872350126505 + 0.01 * 7.015583038330078
Epoch 700, val loss: 1.6297651529312134
Epoch 710, training loss: 0.07716897130012512 = 0.007083008531481028 + 0.01 * 7.008596897125244
Epoch 710, val loss: 1.6391630172729492
Epoch 720, training loss: 0.07681899517774582 = 0.006794652435928583 + 0.01 * 7.002434253692627
Epoch 720, val loss: 1.6482429504394531
Epoch 730, training loss: 0.07657310366630554 = 0.006525138858705759 + 0.01 * 7.004796981811523
Epoch 730, val loss: 1.6570713520050049
Epoch 740, training loss: 0.07625297456979752 = 0.006272858940064907 + 0.01 * 6.998011589050293
Epoch 740, val loss: 1.665787935256958
Epoch 750, training loss: 0.07602525502443314 = 0.006036883220076561 + 0.01 * 6.998836994171143
Epoch 750, val loss: 1.673879861831665
Epoch 760, training loss: 0.07573103159666061 = 0.005815672222524881 + 0.01 * 6.991536617279053
Epoch 760, val loss: 1.6820874214172363
Epoch 770, training loss: 0.07544528692960739 = 0.005607972852885723 + 0.01 * 6.983731746673584
Epoch 770, val loss: 1.690007209777832
Epoch 780, training loss: 0.07520046085119247 = 0.005412264261394739 + 0.01 * 6.978819847106934
Epoch 780, val loss: 1.6975922584533691
Epoch 790, training loss: 0.07507829368114471 = 0.005227887537330389 + 0.01 * 6.98504114151001
Epoch 790, val loss: 1.7049204111099243
Epoch 800, training loss: 0.07472241669893265 = 0.005054058972746134 + 0.01 * 6.966836452484131
Epoch 800, val loss: 1.7123380899429321
Epoch 810, training loss: 0.07484526932239532 = 0.00489000091329217 + 0.01 * 6.9955267906188965
Epoch 810, val loss: 1.7192026376724243
Epoch 820, training loss: 0.07435574382543564 = 0.004734973423182964 + 0.01 * 6.962077617645264
Epoch 820, val loss: 1.726258397102356
Epoch 830, training loss: 0.0743088498711586 = 0.00458834832534194 + 0.01 * 6.972050189971924
Epoch 830, val loss: 1.7327346801757812
Epoch 840, training loss: 0.07405349612236023 = 0.004449790343642235 + 0.01 * 6.9603705406188965
Epoch 840, val loss: 1.739390254020691
Epoch 850, training loss: 0.07376034557819366 = 0.00431817676872015 + 0.01 * 6.944216728210449
Epoch 850, val loss: 1.7456753253936768
Epoch 860, training loss: 0.07367254048585892 = 0.004193334840238094 + 0.01 * 6.947920322418213
Epoch 860, val loss: 1.7517286539077759
Epoch 870, training loss: 0.07347843796014786 = 0.0040748571045696735 + 0.01 * 6.9403581619262695
Epoch 870, val loss: 1.7579573392868042
Epoch 880, training loss: 0.07338255643844604 = 0.0039621759206056595 + 0.01 * 6.942038059234619
Epoch 880, val loss: 1.763584852218628
Epoch 890, training loss: 0.073388010263443 = 0.00385488523170352 + 0.01 * 6.953312873840332
Epoch 890, val loss: 1.7694315910339355
Epoch 900, training loss: 0.07318506389856339 = 0.0037529286928474903 + 0.01 * 6.943212985992432
Epoch 900, val loss: 1.7750918865203857
Epoch 910, training loss: 0.07287373393774033 = 0.003655938198789954 + 0.01 * 6.921779155731201
Epoch 910, val loss: 1.7803117036819458
Epoch 920, training loss: 0.07271867245435715 = 0.0035633817315101624 + 0.01 * 6.915529251098633
Epoch 920, val loss: 1.7856309413909912
Epoch 930, training loss: 0.07276072353124619 = 0.0034748914185911417 + 0.01 * 6.928583145141602
Epoch 930, val loss: 1.7907812595367432
Epoch 940, training loss: 0.072535939514637 = 0.003390466095879674 + 0.01 * 6.914546966552734
Epoch 940, val loss: 1.7959868907928467
Epoch 950, training loss: 0.0723140686750412 = 0.0033096307888627052 + 0.01 * 6.900444030761719
Epoch 950, val loss: 1.8007844686508179
Epoch 960, training loss: 0.0725262239575386 = 0.003232453251257539 + 0.01 * 6.929376602172852
Epoch 960, val loss: 1.805457592010498
Epoch 970, training loss: 0.07221297919750214 = 0.003158596809953451 + 0.01 * 6.90543794631958
Epoch 970, val loss: 1.8104515075683594
Epoch 980, training loss: 0.07195831090211868 = 0.0030879436526447535 + 0.01 * 6.8870368003845215
Epoch 980, val loss: 1.8149324655532837
Epoch 990, training loss: 0.07205614447593689 = 0.0030201077461242676 + 0.01 * 6.903603553771973
Epoch 990, val loss: 1.8192640542984009
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8218239325250396
The final CL Acc:0.74444, 0.00800, The final GNN Acc:0.82182, 0.00172
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13232])
remove edge: torch.Size([2, 7940])
updated graph: torch.Size([2, 10616])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.03756046295166 = 1.951592206954956 + 0.01 * 8.596834182739258
Epoch 0, val loss: 1.9507148265838623
Epoch 10, training loss: 2.027055263519287 = 1.9410876035690308 + 0.01 * 8.596756935119629
Epoch 10, val loss: 1.9401825666427612
Epoch 20, training loss: 2.0135655403137207 = 1.927600383758545 + 0.01 * 8.596510887145996
Epoch 20, val loss: 1.9268972873687744
Epoch 30, training loss: 1.9940778017044067 = 1.9081202745437622 + 0.01 * 8.595747947692871
Epoch 30, val loss: 1.9081183671951294
Epoch 40, training loss: 1.9648375511169434 = 1.8789185285568237 + 0.01 * 8.591906547546387
Epoch 40, val loss: 1.8807625770568848
Epoch 50, training loss: 1.9244163036346436 = 1.838731050491333 + 0.01 * 8.568525314331055
Epoch 50, val loss: 1.8454982042312622
Epoch 60, training loss: 1.8810890913009644 = 1.7964991331100464 + 0.01 * 8.458992958068848
Epoch 60, val loss: 1.8120026588439941
Epoch 70, training loss: 1.8424590826034546 = 1.7596384286880493 + 0.01 * 8.28207015991211
Epoch 70, val loss: 1.7793790102005005
Epoch 80, training loss: 1.7923598289489746 = 1.710646152496338 + 0.01 * 8.171369552612305
Epoch 80, val loss: 1.7301217317581177
Epoch 90, training loss: 1.724702000617981 = 1.6442054510116577 + 0.01 * 8.049656867980957
Epoch 90, val loss: 1.669074535369873
Epoch 100, training loss: 1.6398531198501587 = 1.560689926147461 + 0.01 * 7.916322708129883
Epoch 100, val loss: 1.5992058515548706
Epoch 110, training loss: 1.5503400564193726 = 1.4736961126327515 + 0.01 * 7.664400100708008
Epoch 110, val loss: 1.5282297134399414
Epoch 120, training loss: 1.4695261716842651 = 1.3939943313598633 + 0.01 * 7.553184986114502
Epoch 120, val loss: 1.4651339054107666
Epoch 130, training loss: 1.3954657316207886 = 1.3206313848495483 + 0.01 * 7.4834370613098145
Epoch 130, val loss: 1.4077551364898682
Epoch 140, training loss: 1.3221485614776611 = 1.2478641271591187 + 0.01 * 7.428439617156982
Epoch 140, val loss: 1.3503305912017822
Epoch 150, training loss: 1.2467643022537231 = 1.1729185581207275 + 0.01 * 7.3845744132995605
Epoch 150, val loss: 1.2908978462219238
Epoch 160, training loss: 1.1696242094039917 = 1.0959852933883667 + 0.01 * 7.363889694213867
Epoch 160, val loss: 1.2299144268035889
Epoch 170, training loss: 1.0926367044448853 = 1.0190330743789673 + 0.01 * 7.36036491394043
Epoch 170, val loss: 1.1693452596664429
Epoch 180, training loss: 1.0176746845245361 = 0.9441084265708923 + 0.01 * 7.356624126434326
Epoch 180, val loss: 1.1106150150299072
Epoch 190, training loss: 0.9458620548248291 = 0.8723630309104919 + 0.01 * 7.349900722503662
Epoch 190, val loss: 1.0546873807907104
Epoch 200, training loss: 0.8775220513343811 = 0.8041326403617859 + 0.01 * 7.338939189910889
Epoch 200, val loss: 1.00177001953125
Epoch 210, training loss: 0.8131438493728638 = 0.7399494051933289 + 0.01 * 7.3194427490234375
Epoch 210, val loss: 0.9530178308486938
Epoch 220, training loss: 0.7532017230987549 = 0.6803416609764099 + 0.01 * 7.286007404327393
Epoch 220, val loss: 0.9095458388328552
Epoch 230, training loss: 0.6974287033081055 = 0.6249721050262451 + 0.01 * 7.245658874511719
Epoch 230, val loss: 0.8719885945320129
Epoch 240, training loss: 0.6449458599090576 = 0.5727601051330566 + 0.01 * 7.2185773849487305
Epoch 240, val loss: 0.840134859085083
Epoch 250, training loss: 0.5949772000312805 = 0.5231166481971741 + 0.01 * 7.186056137084961
Epoch 250, val loss: 0.8138471841812134
Epoch 260, training loss: 0.5478581786155701 = 0.47619205713272095 + 0.01 * 7.166611194610596
Epoch 260, val loss: 0.7932814359664917
Epoch 270, training loss: 0.5038586258888245 = 0.4322512447834015 + 0.01 * 7.160740852355957
Epoch 270, val loss: 0.7782642245292664
Epoch 280, training loss: 0.46275800466537476 = 0.3913020193576813 + 0.01 * 7.145596981048584
Epoch 280, val loss: 0.7679243087768555
Epoch 290, training loss: 0.42431119084358215 = 0.3529645800590515 + 0.01 * 7.134661674499512
Epoch 290, val loss: 0.7610969543457031
Epoch 300, training loss: 0.38814014196395874 = 0.3168976306915283 + 0.01 * 7.124250411987305
Epoch 300, val loss: 0.7572029829025269
Epoch 310, training loss: 0.35414284467697144 = 0.2829884886741638 + 0.01 * 7.1154375076293945
Epoch 310, val loss: 0.7560231685638428
Epoch 320, training loss: 0.32289132475852966 = 0.25144466757774353 + 0.01 * 7.14466667175293
Epoch 320, val loss: 0.7575340270996094
Epoch 330, training loss: 0.2937977910041809 = 0.22266295552253723 + 0.01 * 7.11348295211792
Epoch 330, val loss: 0.7618670463562012
Epoch 340, training loss: 0.26786336302757263 = 0.19687744975090027 + 0.01 * 7.098592281341553
Epoch 340, val loss: 0.7691296339035034
Epoch 350, training loss: 0.2450135350227356 = 0.17411887645721436 + 0.01 * 7.089465141296387
Epoch 350, val loss: 0.7791792750358582
Epoch 360, training loss: 0.22517603635787964 = 0.15422235429286957 + 0.01 * 7.0953688621521
Epoch 360, val loss: 0.7915543913841248
Epoch 370, training loss: 0.20770323276519775 = 0.13690803945064545 + 0.01 * 7.079518795013428
Epoch 370, val loss: 0.8057622909545898
Epoch 380, training loss: 0.19255489110946655 = 0.12182015180587769 + 0.01 * 7.07347297668457
Epoch 380, val loss: 0.8214165568351746
Epoch 390, training loss: 0.17932960391044617 = 0.10864106565713882 + 0.01 * 7.068853378295898
Epoch 390, val loss: 0.8381837606430054
Epoch 400, training loss: 0.1676599085330963 = 0.09706832468509674 + 0.01 * 7.059159278869629
Epoch 400, val loss: 0.855755090713501
Epoch 410, training loss: 0.15740078687667847 = 0.08686190843582153 + 0.01 * 7.053886890411377
Epoch 410, val loss: 0.8739196062088013
Epoch 420, training loss: 0.14837369322776794 = 0.07783693820238113 + 0.01 * 7.053674697875977
Epoch 420, val loss: 0.8924906253814697
Epoch 430, training loss: 0.14030581712722778 = 0.0698486715555191 + 0.01 * 7.04571533203125
Epoch 430, val loss: 0.9112667441368103
Epoch 440, training loss: 0.1332387924194336 = 0.0627761036157608 + 0.01 * 7.046268463134766
Epoch 440, val loss: 0.9301698803901672
Epoch 450, training loss: 0.12687626481056213 = 0.056519199162721634 + 0.01 * 7.035706996917725
Epoch 450, val loss: 0.9490737318992615
Epoch 460, training loss: 0.12128356099128723 = 0.050987888127565384 + 0.01 * 7.029567718505859
Epoch 460, val loss: 0.9678570628166199
Epoch 470, training loss: 0.11638587713241577 = 0.0461011677980423 + 0.01 * 7.028470993041992
Epoch 470, val loss: 0.9865002632141113
Epoch 480, training loss: 0.11199679970741272 = 0.04178023338317871 + 0.01 * 7.021656513214111
Epoch 480, val loss: 1.0048794746398926
Epoch 490, training loss: 0.10810645669698715 = 0.037958547472953796 + 0.01 * 7.014791011810303
Epoch 490, val loss: 1.022924542427063
Epoch 500, training loss: 0.10474607348442078 = 0.034579433500766754 + 0.01 * 7.016664028167725
Epoch 500, val loss: 1.0405457019805908
Epoch 510, training loss: 0.10162980109453201 = 0.03159128129482269 + 0.01 * 7.003851890563965
Epoch 510, val loss: 1.0577365159988403
Epoch 520, training loss: 0.09895647317171097 = 0.02894047647714615 + 0.01 * 7.0015997886657715
Epoch 520, val loss: 1.0745190382003784
Epoch 530, training loss: 0.09653501212596893 = 0.026583826169371605 + 0.01 * 6.995118618011475
Epoch 530, val loss: 1.0908647775650024
Epoch 540, training loss: 0.09449803084135056 = 0.024484416469931602 + 0.01 * 7.00136137008667
Epoch 540, val loss: 1.1067543029785156
Epoch 550, training loss: 0.09254418313503265 = 0.022613240405917168 + 0.01 * 6.993094444274902
Epoch 550, val loss: 1.1221728324890137
Epoch 560, training loss: 0.09072855114936829 = 0.020940372720360756 + 0.01 * 6.978818416595459
Epoch 560, val loss: 1.1371142864227295
Epoch 570, training loss: 0.08926233649253845 = 0.019441017881035805 + 0.01 * 6.982132434844971
Epoch 570, val loss: 1.151628851890564
Epoch 580, training loss: 0.08777585625648499 = 0.01809505559504032 + 0.01 * 6.968080043792725
Epoch 580, val loss: 1.1657352447509766
Epoch 590, training loss: 0.08654095977544785 = 0.016883132979273796 + 0.01 * 6.965782642364502
Epoch 590, val loss: 1.1794085502624512
Epoch 600, training loss: 0.08541196584701538 = 0.015789620578289032 + 0.01 * 6.962234973907471
Epoch 600, val loss: 1.1926348209381104
Epoch 610, training loss: 0.08439852297306061 = 0.014800209552049637 + 0.01 * 6.9598307609558105
Epoch 610, val loss: 1.2054572105407715
Epoch 620, training loss: 0.08351777493953705 = 0.013903134502470493 + 0.01 * 6.961463928222656
Epoch 620, val loss: 1.2178117036819458
Epoch 630, training loss: 0.0825190395116806 = 0.013088077306747437 + 0.01 * 6.94309663772583
Epoch 630, val loss: 1.2297991514205933
Epoch 640, training loss: 0.08173404633998871 = 0.012344857677817345 + 0.01 * 6.938918590545654
Epoch 640, val loss: 1.2413936853408813
Epoch 650, training loss: 0.08105722814798355 = 0.011666074395179749 + 0.01 * 6.939115524291992
Epoch 650, val loss: 1.252564787864685
Epoch 660, training loss: 0.08032500743865967 = 0.011044776067137718 + 0.01 * 6.928022861480713
Epoch 660, val loss: 1.2633897066116333
Epoch 670, training loss: 0.0796675980091095 = 0.010474738664925098 + 0.01 * 6.919285774230957
Epoch 670, val loss: 1.2738648653030396
Epoch 680, training loss: 0.07911907136440277 = 0.009950462728738785 + 0.01 * 6.916860580444336
Epoch 680, val loss: 1.2840123176574707
Epoch 690, training loss: 0.0786646232008934 = 0.009468314237892628 + 0.01 * 6.919630527496338
Epoch 690, val loss: 1.293760061264038
Epoch 700, training loss: 0.07807190716266632 = 0.009023178368806839 + 0.01 * 6.904872417449951
Epoch 700, val loss: 1.3032137155532837
Epoch 710, training loss: 0.07774438709020615 = 0.008611368015408516 + 0.01 * 6.913301944732666
Epoch 710, val loss: 1.312377691268921
Epoch 720, training loss: 0.07730244100093842 = 0.008229820057749748 + 0.01 * 6.907262325286865
Epoch 720, val loss: 1.321218729019165
Epoch 730, training loss: 0.07698210328817368 = 0.007875953800976276 + 0.01 * 6.910614967346191
Epoch 730, val loss: 1.329888939857483
Epoch 740, training loss: 0.07637309283018112 = 0.007546260021626949 + 0.01 * 6.882683277130127
Epoch 740, val loss: 1.338239312171936
Epoch 750, training loss: 0.07620064169168472 = 0.007239288650453091 + 0.01 * 6.896135330200195
Epoch 750, val loss: 1.346411943435669
Epoch 760, training loss: 0.07576227933168411 = 0.006952857133001089 + 0.01 * 6.880942344665527
Epoch 760, val loss: 1.3543275594711304
Epoch 770, training loss: 0.07539919763803482 = 0.006684956606477499 + 0.01 * 6.871423721313477
Epoch 770, val loss: 1.3619892597198486
Epoch 780, training loss: 0.07522743195295334 = 0.006434118375182152 + 0.01 * 6.879331588745117
Epoch 780, val loss: 1.3694660663604736
Epoch 790, training loss: 0.07489699125289917 = 0.006199150811880827 + 0.01 * 6.869784355163574
Epoch 790, val loss: 1.3766965866088867
Epoch 800, training loss: 0.07456376403570175 = 0.005978371016681194 + 0.01 * 6.858539581298828
Epoch 800, val loss: 1.3837255239486694
Epoch 810, training loss: 0.07447611540555954 = 0.00577076431363821 + 0.01 * 6.870535373687744
Epoch 810, val loss: 1.3906170129776
Epoch 820, training loss: 0.07409441471099854 = 0.005575541872531176 + 0.01 * 6.851886749267578
Epoch 820, val loss: 1.3972728252410889
Epoch 830, training loss: 0.07381991297006607 = 0.0053914692252874374 + 0.01 * 6.842844009399414
Epoch 830, val loss: 1.4037514925003052
Epoch 840, training loss: 0.07388331741094589 = 0.005217558238655329 + 0.01 * 6.866575717926025
Epoch 840, val loss: 1.41004478931427
Epoch 850, training loss: 0.07356713712215424 = 0.00505379494279623 + 0.01 * 6.851334571838379
Epoch 850, val loss: 1.4162561893463135
Epoch 860, training loss: 0.0732966884970665 = 0.004898502957075834 + 0.01 * 6.839818477630615
Epoch 860, val loss: 1.4222780466079712
Epoch 870, training loss: 0.07316423952579498 = 0.004751833621412516 + 0.01 * 6.841240882873535
Epoch 870, val loss: 1.4281706809997559
Epoch 880, training loss: 0.07293623685836792 = 0.0046125720255076885 + 0.01 * 6.832366466522217
Epoch 880, val loss: 1.4338651895523071
Epoch 890, training loss: 0.07274343818426132 = 0.004480454139411449 + 0.01 * 6.826298713684082
Epoch 890, val loss: 1.4394582509994507
Epoch 900, training loss: 0.07263436168432236 = 0.004355048760771751 + 0.01 * 6.827930927276611
Epoch 900, val loss: 1.4448591470718384
Epoch 910, training loss: 0.07248908281326294 = 0.004235563334077597 + 0.01 * 6.825351715087891
Epoch 910, val loss: 1.4501566886901855
Epoch 920, training loss: 0.07230271399021149 = 0.004122227896004915 + 0.01 * 6.818048477172852
Epoch 920, val loss: 1.4553742408752441
Epoch 930, training loss: 0.07222326099872589 = 0.00401428434997797 + 0.01 * 6.820898056030273
Epoch 930, val loss: 1.4604041576385498
Epoch 940, training loss: 0.07198472321033478 = 0.00391132477670908 + 0.01 * 6.807340145111084
Epoch 940, val loss: 1.4653321504592896
Epoch 950, training loss: 0.07193518429994583 = 0.0038130313623696566 + 0.01 * 6.812215805053711
Epoch 950, val loss: 1.4701907634735107
Epoch 960, training loss: 0.07173463702201843 = 0.0037194108590483665 + 0.01 * 6.801522731781006
Epoch 960, val loss: 1.4748954772949219
Epoch 970, training loss: 0.0716666728258133 = 0.0036298325285315514 + 0.01 * 6.803684234619141
Epoch 970, val loss: 1.4794895648956299
Epoch 980, training loss: 0.07149133086204529 = 0.0035442833323031664 + 0.01 * 6.794704437255859
Epoch 980, val loss: 1.4839894771575928
Epoch 990, training loss: 0.07147441059350967 = 0.003462423337623477 + 0.01 * 6.801198482513428
Epoch 990, val loss: 1.488391399383545
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.0381884574890137 = 1.9522199630737305 + 0.01 * 8.59685230255127
Epoch 0, val loss: 1.9505054950714111
Epoch 10, training loss: 2.0282626152038574 = 1.9422943592071533 + 0.01 * 8.596817970275879
Epoch 10, val loss: 1.9410370588302612
Epoch 20, training loss: 2.0161592960357666 = 1.930192470550537 + 0.01 * 8.596684455871582
Epoch 20, val loss: 1.9289714097976685
Epoch 30, training loss: 1.9991434812545776 = 1.9131807088851929 + 0.01 * 8.596277236938477
Epoch 30, val loss: 1.91172194480896
Epoch 40, training loss: 1.9738868474960327 = 1.8879435062408447 + 0.01 * 8.59433650970459
Epoch 40, val loss: 1.8861618041992188
Epoch 50, training loss: 1.9377638101577759 = 1.8519481420516968 + 0.01 * 8.581564903259277
Epoch 50, val loss: 1.8509687185287476
Epoch 60, training loss: 1.894705057144165 = 1.8094053268432617 + 0.01 * 8.529972076416016
Epoch 60, val loss: 1.812727928161621
Epoch 70, training loss: 1.8552788496017456 = 1.7719908952713013 + 0.01 * 8.328798294067383
Epoch 70, val loss: 1.7821593284606934
Epoch 80, training loss: 1.8126246929168701 = 1.730319857597351 + 0.01 * 8.230488777160645
Epoch 80, val loss: 1.7447389364242554
Epoch 90, training loss: 1.7520445585250854 = 1.6714540719985962 + 0.01 * 8.059046745300293
Epoch 90, val loss: 1.691004991531372
Epoch 100, training loss: 1.6693812608718872 = 1.5907825231552124 + 0.01 * 7.859870910644531
Epoch 100, val loss: 1.6195576190948486
Epoch 110, training loss: 1.5679171085357666 = 1.490530252456665 + 0.01 * 7.738687992095947
Epoch 110, val loss: 1.5338865518569946
Epoch 120, training loss: 1.45832097530365 = 1.381353735923767 + 0.01 * 7.69672966003418
Epoch 120, val loss: 1.442612886428833
Epoch 130, training loss: 1.3492566347122192 = 1.2725822925567627 + 0.01 * 7.667428970336914
Epoch 130, val loss: 1.3553184270858765
Epoch 140, training loss: 1.2467135190963745 = 1.170300006866455 + 0.01 * 7.641348361968994
Epoch 140, val loss: 1.2770106792449951
Epoch 150, training loss: 1.1549357175827026 = 1.078851580619812 + 0.01 * 7.608408451080322
Epoch 150, val loss: 1.20929753780365
Epoch 160, training loss: 1.0748792886734009 = 0.9992194175720215 + 0.01 * 7.5659918785095215
Epoch 160, val loss: 1.1516494750976562
Epoch 170, training loss: 1.0034289360046387 = 0.9282861351966858 + 0.01 * 7.514283180236816
Epoch 170, val loss: 1.0996975898742676
Epoch 180, training loss: 0.9362818598747253 = 0.8617203831672668 + 0.01 * 7.456149578094482
Epoch 180, val loss: 1.0491371154785156
Epoch 190, training loss: 0.8704065680503845 = 0.7962847352027893 + 0.01 * 7.412184238433838
Epoch 190, val loss: 0.9979138374328613
Epoch 200, training loss: 0.8048809766769409 = 0.7309959530830383 + 0.01 * 7.3885016441345215
Epoch 200, val loss: 0.9468261003494263
Epoch 210, training loss: 0.7404757142066956 = 0.6667410731315613 + 0.01 * 7.3734636306762695
Epoch 210, val loss: 0.8984572291374207
Epoch 220, training loss: 0.6781228184700012 = 0.6045381426811218 + 0.01 * 7.358469009399414
Epoch 220, val loss: 0.8556477427482605
Epoch 230, training loss: 0.6183993220329285 = 0.5449637174606323 + 0.01 * 7.343560695648193
Epoch 230, val loss: 0.819595992565155
Epoch 240, training loss: 0.5618821978569031 = 0.48860424757003784 + 0.01 * 7.327793121337891
Epoch 240, val loss: 0.7906234860420227
Epoch 250, training loss: 0.5093304514884949 = 0.43622738122940063 + 0.01 * 7.31030797958374
Epoch 250, val loss: 0.7688665390014648
Epoch 260, training loss: 0.46120700240135193 = 0.38832247257232666 + 0.01 * 7.2884521484375
Epoch 260, val loss: 0.754136323928833
Epoch 270, training loss: 0.4176737070083618 = 0.3449706733226776 + 0.01 * 7.270304203033447
Epoch 270, val loss: 0.7456138730049133
Epoch 280, training loss: 0.3784998059272766 = 0.3060815632343292 + 0.01 * 7.241825103759766
Epoch 280, val loss: 0.7422884702682495
Epoch 290, training loss: 0.34359416365623474 = 0.2714640200138092 + 0.01 * 7.213015079498291
Epoch 290, val loss: 0.74314284324646
Epoch 300, training loss: 0.3128330707550049 = 0.24092215299606323 + 0.01 * 7.191091537475586
Epoch 300, val loss: 0.7473743557929993
Epoch 310, training loss: 0.2858932614326477 = 0.2141435593366623 + 0.01 * 7.174970626831055
Epoch 310, val loss: 0.7544712424278259
Epoch 320, training loss: 0.26222774386405945 = 0.19068565964698792 + 0.01 * 7.154208183288574
Epoch 320, val loss: 0.7639907002449036
Epoch 330, training loss: 0.24150443077087402 = 0.17011761665344238 + 0.01 * 7.138680934906006
Epoch 330, val loss: 0.7754234075546265
Epoch 340, training loss: 0.2234010249376297 = 0.1520538628101349 + 0.01 * 7.134716510772705
Epoch 340, val loss: 0.7883225083351135
Epoch 350, training loss: 0.20735840499401093 = 0.13615383207798004 + 0.01 * 7.120457649230957
Epoch 350, val loss: 0.8023146986961365
Epoch 360, training loss: 0.1932174563407898 = 0.12207874655723572 + 0.01 * 7.1138715744018555
Epoch 360, val loss: 0.817157506942749
Epoch 370, training loss: 0.18063440918922424 = 0.10958170145750046 + 0.01 * 7.105270862579346
Epoch 370, val loss: 0.8325327634811401
Epoch 380, training loss: 0.1694442331790924 = 0.09846172481775284 + 0.01 * 7.098251819610596
Epoch 380, val loss: 0.8482449054718018
Epoch 390, training loss: 0.15948238968849182 = 0.08855970948934555 + 0.01 * 7.092268466949463
Epoch 390, val loss: 0.8642459511756897
Epoch 400, training loss: 0.15061382949352264 = 0.0797451063990593 + 0.01 * 7.086872100830078
Epoch 400, val loss: 0.8803356885910034
Epoch 410, training loss: 0.1427241861820221 = 0.0719066634774208 + 0.01 * 7.081752777099609
Epoch 410, val loss: 0.8964849710464478
Epoch 420, training loss: 0.13586854934692383 = 0.06494376808404922 + 0.01 * 7.092477798461914
Epoch 420, val loss: 0.9125071167945862
Epoch 430, training loss: 0.12954187393188477 = 0.05878639221191406 + 0.01 * 7.075547218322754
Epoch 430, val loss: 0.9282875061035156
Epoch 440, training loss: 0.12402524799108505 = 0.05332614481449127 + 0.01 * 7.069910526275635
Epoch 440, val loss: 0.9438670873641968
Epoch 450, training loss: 0.11912459135055542 = 0.04847230389714241 + 0.01 * 7.0652289390563965
Epoch 450, val loss: 0.9591131210327148
Epoch 460, training loss: 0.11475854367017746 = 0.044150374829769135 + 0.01 * 7.060817241668701
Epoch 460, val loss: 0.9740634560585022
Epoch 470, training loss: 0.11085817217826843 = 0.040289174765348434 + 0.01 * 7.056899547576904
Epoch 470, val loss: 0.9887063503265381
Epoch 480, training loss: 0.10748960077762604 = 0.0368291400372982 + 0.01 * 7.066046714782715
Epoch 480, val loss: 1.002992033958435
Epoch 490, training loss: 0.10424210876226425 = 0.03373616188764572 + 0.01 * 7.050594806671143
Epoch 490, val loss: 1.0168416500091553
Epoch 500, training loss: 0.10143787413835526 = 0.030959317460656166 + 0.01 * 7.047855854034424
Epoch 500, val loss: 1.0302878618240356
Epoch 510, training loss: 0.09888046234846115 = 0.028460396453738213 + 0.01 * 7.042006492614746
Epoch 510, val loss: 1.0433506965637207
Epoch 520, training loss: 0.09659335017204285 = 0.02620868571102619 + 0.01 * 7.038466453552246
Epoch 520, val loss: 1.0559812784194946
Epoch 530, training loss: 0.09465137124061584 = 0.024176333099603653 + 0.01 * 7.04750394821167
Epoch 530, val loss: 1.06821608543396
Epoch 540, training loss: 0.09266790002584457 = 0.022343792021274567 + 0.01 * 7.032411098480225
Epoch 540, val loss: 1.079972743988037
Epoch 550, training loss: 0.09096449613571167 = 0.020687371492385864 + 0.01 * 7.027712345123291
Epoch 550, val loss: 1.091381549835205
Epoch 560, training loss: 0.08942659199237823 = 0.019187938421964645 + 0.01 * 7.023865699768066
Epoch 560, val loss: 1.1023459434509277
Epoch 570, training loss: 0.0883224681019783 = 0.017829185351729393 + 0.01 * 7.049328327178955
Epoch 570, val loss: 1.1129554510116577
Epoch 580, training loss: 0.08682893216609955 = 0.016604475677013397 + 0.01 * 7.0224456787109375
Epoch 580, val loss: 1.1230480670928955
Epoch 590, training loss: 0.08566607534885406 = 0.015495060943067074 + 0.01 * 7.017101764678955
Epoch 590, val loss: 1.1327934265136719
Epoch 600, training loss: 0.08459368348121643 = 0.01448663230985403 + 0.01 * 7.010705471038818
Epoch 600, val loss: 1.142183542251587
Epoch 610, training loss: 0.0836203545331955 = 0.013567881658673286 + 0.01 * 7.005247592926025
Epoch 610, val loss: 1.151248574256897
Epoch 620, training loss: 0.08278777450323105 = 0.012729349546134472 + 0.01 * 7.005843162536621
Epoch 620, val loss: 1.1599713563919067
Epoch 630, training loss: 0.0819862112402916 = 0.01196356862783432 + 0.01 * 7.002264499664307
Epoch 630, val loss: 1.1683166027069092
Epoch 640, training loss: 0.0812130868434906 = 0.011263693682849407 + 0.01 * 6.994939804077148
Epoch 640, val loss: 1.176404595375061
Epoch 650, training loss: 0.0805760994553566 = 0.010622508823871613 + 0.01 * 6.995359420776367
Epoch 650, val loss: 1.184179425239563
Epoch 660, training loss: 0.07995324581861496 = 0.010034019127488136 + 0.01 * 6.9919233322143555
Epoch 660, val loss: 1.191652536392212
Epoch 670, training loss: 0.07944547384977341 = 0.009493740275502205 + 0.01 * 6.995173931121826
Epoch 670, val loss: 1.1988331079483032
Epoch 680, training loss: 0.07883551716804504 = 0.008996164426207542 + 0.01 * 6.9839348793029785
Epoch 680, val loss: 1.2057632207870483
Epoch 690, training loss: 0.07837481796741486 = 0.008537663146853447 + 0.01 * 6.983715534210205
Epoch 690, val loss: 1.2124674320220947
Epoch 700, training loss: 0.07787869870662689 = 0.008114666678011417 + 0.01 * 6.976403713226318
Epoch 700, val loss: 1.2188448905944824
Epoch 710, training loss: 0.07742436975240707 = 0.007723318412899971 + 0.01 * 6.9701056480407715
Epoch 710, val loss: 1.225135326385498
Epoch 720, training loss: 0.07698992639780045 = 0.007360413204878569 + 0.01 * 6.962951183319092
Epoch 720, val loss: 1.2311644554138184
Epoch 730, training loss: 0.07671850174665451 = 0.00702398968860507 + 0.01 * 6.969451427459717
Epoch 730, val loss: 1.2370249032974243
Epoch 740, training loss: 0.07625012844800949 = 0.006710557732731104 + 0.01 * 6.9539570808410645
Epoch 740, val loss: 1.2426164150238037
Epoch 750, training loss: 0.07594330608844757 = 0.006418529432266951 + 0.01 * 6.952477931976318
Epoch 750, val loss: 1.2480850219726562
Epoch 760, training loss: 0.07567531615495682 = 0.0061445580795407295 + 0.01 * 6.953075885772705
Epoch 760, val loss: 1.2534343004226685
Epoch 770, training loss: 0.0753016248345375 = 0.005888863932341337 + 0.01 * 6.9412760734558105
Epoch 770, val loss: 1.2584298849105835
Epoch 780, training loss: 0.07509506493806839 = 0.005648748483508825 + 0.01 * 6.944632053375244
Epoch 780, val loss: 1.2634035348892212
Epoch 790, training loss: 0.07476744800806046 = 0.005423771683126688 + 0.01 * 6.934367656707764
Epoch 790, val loss: 1.268086552619934
Epoch 800, training loss: 0.07451198995113373 = 0.005212634336203337 + 0.01 * 6.929935455322266
Epoch 800, val loss: 1.272696852684021
Epoch 810, training loss: 0.07439859211444855 = 0.005014065653085709 + 0.01 * 6.93845272064209
Epoch 810, val loss: 1.277159571647644
Epoch 820, training loss: 0.07405032217502594 = 0.004828029312193394 + 0.01 * 6.922229766845703
Epoch 820, val loss: 1.2813318967819214
Epoch 830, training loss: 0.07380849123001099 = 0.004652653355151415 + 0.01 * 6.915583610534668
Epoch 830, val loss: 1.285523533821106
Epoch 840, training loss: 0.0736180767416954 = 0.004487129859626293 + 0.01 * 6.913094997406006
Epoch 840, val loss: 1.2895479202270508
Epoch 850, training loss: 0.07340279966592789 = 0.004331772681325674 + 0.01 * 6.907102584838867
Epoch 850, val loss: 1.2932891845703125
Epoch 860, training loss: 0.07338231056928635 = 0.00418580649420619 + 0.01 * 6.919650554656982
Epoch 860, val loss: 1.2970795631408691
Epoch 870, training loss: 0.07314493507146835 = 0.004048326518386602 + 0.01 * 6.909661293029785
Epoch 870, val loss: 1.300724744796753
Epoch 880, training loss: 0.07293692231178284 = 0.003918695263564587 + 0.01 * 6.901823043823242
Epoch 880, val loss: 1.3042101860046387
Epoch 890, training loss: 0.07300033420324326 = 0.0037958913017064333 + 0.01 * 6.920444011688232
Epoch 890, val loss: 1.3076633214950562
Epoch 900, training loss: 0.0726846233010292 = 0.003679916262626648 + 0.01 * 6.900471210479736
Epoch 900, val loss: 1.3109080791473389
Epoch 910, training loss: 0.07248653471469879 = 0.0035699813161045313 + 0.01 * 6.891655445098877
Epoch 910, val loss: 1.3141591548919678
Epoch 920, training loss: 0.07223901897668839 = 0.003465892281383276 + 0.01 * 6.877312660217285
Epoch 920, val loss: 1.317261815071106
Epoch 930, training loss: 0.07210275530815125 = 0.003366959746927023 + 0.01 * 6.873579502105713
Epoch 930, val loss: 1.3203074932098389
Epoch 940, training loss: 0.07195006310939789 = 0.003273330396041274 + 0.01 * 6.867673397064209
Epoch 940, val loss: 1.323229193687439
Epoch 950, training loss: 0.07208605110645294 = 0.00318430014885962 + 0.01 * 6.890174865722656
Epoch 950, val loss: 1.3260924816131592
Epoch 960, training loss: 0.07190046459436417 = 0.003099682740867138 + 0.01 * 6.8800787925720215
Epoch 960, val loss: 1.3288105726242065
Epoch 970, training loss: 0.07180754095315933 = 0.00301922345533967 + 0.01 * 6.8788323402404785
Epoch 970, val loss: 1.3314499855041504
Epoch 980, training loss: 0.07157838344573975 = 0.0029425679240375757 + 0.01 * 6.86358118057251
Epoch 980, val loss: 1.3341127634048462
Epoch 990, training loss: 0.07144278287887573 = 0.0028695445507764816 + 0.01 * 6.85732364654541
Epoch 990, val loss: 1.3366143703460693
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.0396993160247803 = 1.953730821609497 + 0.01 * 8.596860885620117
Epoch 0, val loss: 1.9645600318908691
Epoch 10, training loss: 2.029353380203247 = 1.943385124206543 + 0.01 * 8.59682559967041
Epoch 10, val loss: 1.9534803628921509
Epoch 20, training loss: 2.016958475112915 = 1.9309916496276855 + 0.01 * 8.5966796875
Epoch 20, val loss: 1.9398891925811768
Epoch 30, training loss: 1.999974012374878 = 1.9140114784240723 + 0.01 * 8.596251487731934
Epoch 30, val loss: 1.9211410284042358
Epoch 40, training loss: 1.9750813245773315 = 1.8891383409500122 + 0.01 * 8.594304084777832
Epoch 40, val loss: 1.8939905166625977
Epoch 50, training loss: 1.939521074295044 = 1.853712558746338 + 0.01 * 8.580848693847656
Epoch 50, val loss: 1.856600284576416
Epoch 60, training loss: 1.8964507579803467 = 1.8112766742706299 + 0.01 * 8.517406463623047
Epoch 60, val loss: 1.81490957736969
Epoch 70, training loss: 1.8567390441894531 = 1.7732223272323608 + 0.01 * 8.351675987243652
Epoch 70, val loss: 1.7807691097259521
Epoch 80, training loss: 1.813485026359558 = 1.7315645217895508 + 0.01 * 8.192054748535156
Epoch 80, val loss: 1.7422784566879272
Epoch 90, training loss: 1.751499056816101 = 1.672778606414795 + 0.01 * 7.872050762176514
Epoch 90, val loss: 1.6881122589111328
Epoch 100, training loss: 1.669958233833313 = 1.5933805704116821 + 0.01 * 7.657765865325928
Epoch 100, val loss: 1.617609977722168
Epoch 110, training loss: 1.5706617832183838 = 1.495771050453186 + 0.01 * 7.489074230194092
Epoch 110, val loss: 1.5329595804214478
Epoch 120, training loss: 1.4649174213409424 = 1.3906195163726807 + 0.01 * 7.4297943115234375
Epoch 120, val loss: 1.443126916885376
Epoch 130, training loss: 1.3593071699142456 = 1.285433053970337 + 0.01 * 7.3874125480651855
Epoch 130, val loss: 1.355993628501892
Epoch 140, training loss: 1.2570875883102417 = 1.18350350856781 + 0.01 * 7.358407497406006
Epoch 140, val loss: 1.2745957374572754
Epoch 150, training loss: 1.1615628004074097 = 1.0881747007369995 + 0.01 * 7.338806629180908
Epoch 150, val loss: 1.200032114982605
Epoch 160, training loss: 1.0750744342803955 = 1.0018830299377441 + 0.01 * 7.31913948059082
Epoch 160, val loss: 1.1337695121765137
Epoch 170, training loss: 0.9968646168708801 = 0.9239228367805481 + 0.01 * 7.29417610168457
Epoch 170, val loss: 1.074544906616211
Epoch 180, training loss: 0.92386794090271 = 0.8512033820152283 + 0.01 * 7.266454219818115
Epoch 180, val loss: 1.0192276239395142
Epoch 190, training loss: 0.8534645438194275 = 0.7809659242630005 + 0.01 * 7.249859809875488
Epoch 190, val loss: 0.965739369392395
Epoch 200, training loss: 0.7849442362785339 = 0.7125850319862366 + 0.01 * 7.235921382904053
Epoch 200, val loss: 0.9143314957618713
Epoch 210, training loss: 0.7204422354698181 = 0.6481655240058899 + 0.01 * 7.227670192718506
Epoch 210, val loss: 0.8678313493728638
Epoch 220, training loss: 0.662706732749939 = 0.5905108451843262 + 0.01 * 7.219586372375488
Epoch 220, val loss: 0.829365074634552
Epoch 230, training loss: 0.6131173968315125 = 0.5409967303276062 + 0.01 * 7.2120680809021
Epoch 230, val loss: 0.800715446472168
Epoch 240, training loss: 0.570996105670929 = 0.4989684522151947 + 0.01 * 7.2027668952941895
Epoch 240, val loss: 0.7809382677078247
Epoch 250, training loss: 0.5345023274421692 = 0.4625251591205597 + 0.01 * 7.197715759277344
Epoch 250, val loss: 0.7678768038749695
Epoch 260, training loss: 0.5012082457542419 = 0.42934709787368774 + 0.01 * 7.186115264892578
Epoch 260, val loss: 0.7586769461631775
Epoch 270, training loss: 0.46904122829437256 = 0.3972562253475189 + 0.01 * 7.1784987449646
Epoch 270, val loss: 0.7510385513305664
Epoch 280, training loss: 0.4363326132297516 = 0.3646043837070465 + 0.01 * 7.17282247543335
Epoch 280, val loss: 0.7435763478279114
Epoch 290, training loss: 0.40222567319869995 = 0.33057594299316406 + 0.01 * 7.16497278213501
Epoch 290, val loss: 0.7357618808746338
Epoch 300, training loss: 0.3673328757286072 = 0.29557743668556213 + 0.01 * 7.175544738769531
Epoch 300, val loss: 0.7276355624198914
Epoch 310, training loss: 0.33261117339134216 = 0.2609889507293701 + 0.01 * 7.1622233390808105
Epoch 310, val loss: 0.7204407453536987
Epoch 320, training loss: 0.29986751079559326 = 0.22832845151424408 + 0.01 * 7.153907775878906
Epoch 320, val loss: 0.7152271270751953
Epoch 330, training loss: 0.27020263671875 = 0.19869399070739746 + 0.01 * 7.15086555480957
Epoch 330, val loss: 0.7126307487487793
Epoch 340, training loss: 0.24407243728637695 = 0.17258699238300323 + 0.01 * 7.148545742034912
Epoch 340, val loss: 0.7127665281295776
Epoch 350, training loss: 0.221413716673851 = 0.1499556005001068 + 0.01 * 7.145812034606934
Epoch 350, val loss: 0.7152242064476013
Epoch 360, training loss: 0.201912522315979 = 0.1304544061422348 + 0.01 * 7.145811080932617
Epoch 360, val loss: 0.7196151614189148
Epoch 370, training loss: 0.18509453535079956 = 0.11366372555494308 + 0.01 * 7.143081188201904
Epoch 370, val loss: 0.7253469824790955
Epoch 380, training loss: 0.17060433328151703 = 0.09920069575309753 + 0.01 * 7.140363693237305
Epoch 380, val loss: 0.7323331832885742
Epoch 390, training loss: 0.1581103801727295 = 0.08674667030572891 + 0.01 * 7.1363701820373535
Epoch 390, val loss: 0.740329921245575
Epoch 400, training loss: 0.14740845561027527 = 0.07604040205478668 + 0.01 * 7.136806488037109
Epoch 400, val loss: 0.7492479085922241
Epoch 410, training loss: 0.1381712257862091 = 0.06685827672481537 + 0.01 * 7.131296157836914
Epoch 410, val loss: 0.7589724659919739
Epoch 420, training loss: 0.13029390573501587 = 0.05899655818939209 + 0.01 * 7.129735469818115
Epoch 420, val loss: 0.7693248391151428
Epoch 430, training loss: 0.12354163825511932 = 0.05226961150765419 + 0.01 * 7.127203464508057
Epoch 430, val loss: 0.780272364616394
Epoch 440, training loss: 0.11774539947509766 = 0.04651082307100296 + 0.01 * 7.123457908630371
Epoch 440, val loss: 0.7915869355201721
Epoch 450, training loss: 0.11274456977844238 = 0.041570667177438736 + 0.01 * 7.117390155792236
Epoch 450, val loss: 0.8030838370323181
Epoch 460, training loss: 0.10845355689525604 = 0.037318479269742966 + 0.01 * 7.113508224487305
Epoch 460, val loss: 0.8147884607315063
Epoch 470, training loss: 0.10473811626434326 = 0.03364555165171623 + 0.01 * 7.109256744384766
Epoch 470, val loss: 0.8265231251716614
Epoch 480, training loss: 0.10151100158691406 = 0.030462127178907394 + 0.01 * 7.104887962341309
Epoch 480, val loss: 0.8380991220474243
Epoch 490, training loss: 0.09869387000799179 = 0.027690289542078972 + 0.01 * 7.100358009338379
Epoch 490, val loss: 0.8495741486549377
Epoch 500, training loss: 0.09622283279895782 = 0.025265704840421677 + 0.01 * 7.095713138580322
Epoch 500, val loss: 0.8608385920524597
Epoch 510, training loss: 0.09404225647449493 = 0.02313641831278801 + 0.01 * 7.090583801269531
Epoch 510, val loss: 0.8719269633293152
Epoch 520, training loss: 0.09214597940444946 = 0.021259676665067673 + 0.01 * 7.088630676269531
Epoch 520, val loss: 0.8827596306800842
Epoch 530, training loss: 0.09042882919311523 = 0.01960039883852005 + 0.01 * 7.08284330368042
Epoch 530, val loss: 0.8933535814285278
Epoch 540, training loss: 0.08889567852020264 = 0.018126530572772026 + 0.01 * 7.0769147872924805
Epoch 540, val loss: 0.9036766886711121
Epoch 550, training loss: 0.087694451212883 = 0.01681245118379593 + 0.01 * 7.088200092315674
Epoch 550, val loss: 0.9137548208236694
Epoch 560, training loss: 0.08629973232746124 = 0.015637820586562157 + 0.01 * 7.06619119644165
Epoch 560, val loss: 0.9234846830368042
Epoch 570, training loss: 0.08520917594432831 = 0.014583717100322247 + 0.01 * 7.062546253204346
Epoch 570, val loss: 0.9329699873924255
Epoch 580, training loss: 0.08419468998908997 = 0.013634140603244305 + 0.01 * 7.056055545806885
Epoch 580, val loss: 0.9422441124916077
Epoch 590, training loss: 0.08340030908584595 = 0.012776223011314869 + 0.01 * 7.062408447265625
Epoch 590, val loss: 0.9512038826942444
Epoch 600, training loss: 0.08249222487211227 = 0.012000317685306072 + 0.01 * 7.049191474914551
Epoch 600, val loss: 0.9599614143371582
Epoch 610, training loss: 0.0817137137055397 = 0.011295723728835583 + 0.01 * 7.041799545288086
Epoch 610, val loss: 0.968447208404541
Epoch 620, training loss: 0.08114863932132721 = 0.010653771460056305 + 0.01 * 7.0494866371154785
Epoch 620, val loss: 0.9766519069671631
Epoch 630, training loss: 0.08033791184425354 = 0.010068230330944061 + 0.01 * 7.026968479156494
Epoch 630, val loss: 0.9846163988113403
Epoch 640, training loss: 0.07979768514633179 = 0.00953221507370472 + 0.01 * 7.026547431945801
Epoch 640, val loss: 0.9924177527427673
Epoch 650, training loss: 0.07922016829252243 = 0.009040215983986855 + 0.01 * 7.017995357513428
Epoch 650, val loss: 0.9999849796295166
Epoch 660, training loss: 0.07871273159980774 = 0.008587483316659927 + 0.01 * 7.01252555847168
Epoch 660, val loss: 1.0073472261428833
Epoch 670, training loss: 0.07826921343803406 = 0.008170688524842262 + 0.01 * 7.009852886199951
Epoch 670, val loss: 1.014473557472229
Epoch 680, training loss: 0.0778057724237442 = 0.007785614579916 + 0.01 * 7.002016544342041
Epoch 680, val loss: 1.0213556289672852
Epoch 690, training loss: 0.07762189209461212 = 0.007427998352795839 + 0.01 * 7.019389629364014
Epoch 690, val loss: 1.028132677078247
Epoch 700, training loss: 0.07700245082378387 = 0.007096836343407631 + 0.01 * 6.990561485290527
Epoch 700, val loss: 1.034765601158142
Epoch 710, training loss: 0.0765429362654686 = 0.006788608618080616 + 0.01 * 6.975432872772217
Epoch 710, val loss: 1.041229486465454
Epoch 720, training loss: 0.07624861598014832 = 0.006501285824924707 + 0.01 * 6.974733352661133
Epoch 720, val loss: 1.0475231409072876
Epoch 730, training loss: 0.07594238221645355 = 0.0062331208027899265 + 0.01 * 6.970926761627197
Epoch 730, val loss: 1.0535483360290527
Epoch 740, training loss: 0.07564634084701538 = 0.005981978494673967 + 0.01 * 6.96643590927124
Epoch 740, val loss: 1.0596065521240234
Epoch 750, training loss: 0.07577753067016602 = 0.005747415591031313 + 0.01 * 7.003011703491211
Epoch 750, val loss: 1.0653597116470337
Epoch 760, training loss: 0.07518241554498672 = 0.005528753623366356 + 0.01 * 6.965365886688232
Epoch 760, val loss: 1.0709139108657837
Epoch 770, training loss: 0.07481566071510315 = 0.005323701072484255 + 0.01 * 6.9491963386535645
Epoch 770, val loss: 1.0765324831008911
Epoch 780, training loss: 0.0745110884308815 = 0.005130582954734564 + 0.01 * 6.938050746917725
Epoch 780, val loss: 1.0818198919296265
Epoch 790, training loss: 0.07445694506168365 = 0.004948840010911226 + 0.01 * 6.950810432434082
Epoch 790, val loss: 1.087151050567627
Epoch 800, training loss: 0.0740385353565216 = 0.004777888301759958 + 0.01 * 6.926065444946289
Epoch 800, val loss: 1.0921746492385864
Epoch 810, training loss: 0.0740639939904213 = 0.004617215599864721 + 0.01 * 6.94467830657959
Epoch 810, val loss: 1.0970995426177979
Epoch 820, training loss: 0.07369618117809296 = 0.004465506412088871 + 0.01 * 6.923067569732666
Epoch 820, val loss: 1.101800560951233
Epoch 830, training loss: 0.07342410832643509 = 0.004322065506130457 + 0.01 * 6.91020393371582
Epoch 830, val loss: 1.1065682172775269
Epoch 840, training loss: 0.07331664860248566 = 0.004186282400041819 + 0.01 * 6.913036823272705
Epoch 840, val loss: 1.1111211776733398
Epoch 850, training loss: 0.0731077790260315 = 0.004058264195919037 + 0.01 * 6.904951572418213
Epoch 850, val loss: 1.1154499053955078
Epoch 860, training loss: 0.07292375713586807 = 0.0039369333535432816 + 0.01 * 6.898682594299316
Epoch 860, val loss: 1.1198828220367432
Epoch 870, training loss: 0.07276437431573868 = 0.0038217115215957165 + 0.01 * 6.8942670822143555
Epoch 870, val loss: 1.1239396333694458
Epoch 880, training loss: 0.07260826975107193 = 0.0037128934636712074 + 0.01 * 6.889537811279297
Epoch 880, val loss: 1.1281352043151855
Epoch 890, training loss: 0.07241316139698029 = 0.003609555074945092 + 0.01 * 6.880361080169678
Epoch 890, val loss: 1.1319949626922607
Epoch 900, training loss: 0.07274491339921951 = 0.0035106458235532045 + 0.01 * 6.923427104949951
Epoch 900, val loss: 1.1359707117080688
Epoch 910, training loss: 0.07217493653297424 = 0.0034178264904767275 + 0.01 * 6.875710964202881
Epoch 910, val loss: 1.1396105289459229
Epoch 920, training loss: 0.07206030935049057 = 0.003328748745843768 + 0.01 * 6.873156547546387
Epoch 920, val loss: 1.1433570384979248
Epoch 930, training loss: 0.07184727489948273 = 0.003243807703256607 + 0.01 * 6.860347270965576
Epoch 930, val loss: 1.1469581127166748
Epoch 940, training loss: 0.07181760668754578 = 0.003162669949233532 + 0.01 * 6.8654937744140625
Epoch 940, val loss: 1.1505721807479858
Epoch 950, training loss: 0.07158995419740677 = 0.0030854884535074234 + 0.01 * 6.850447177886963
Epoch 950, val loss: 1.15394926071167
Epoch 960, training loss: 0.07177386432886124 = 0.003011003602296114 + 0.01 * 6.876286029815674
Epoch 960, val loss: 1.1573481559753418
Epoch 970, training loss: 0.0713602751493454 = 0.002940993756055832 + 0.01 * 6.841928005218506
Epoch 970, val loss: 1.1607184410095215
Epoch 980, training loss: 0.07169848680496216 = 0.002873307792469859 + 0.01 * 6.882518291473389
Epoch 980, val loss: 1.1639317274093628
Epoch 990, training loss: 0.07107548415660858 = 0.0028086972888559103 + 0.01 * 6.82667875289917
Epoch 990, val loss: 1.1670500040054321
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8386926726410122
The final CL Acc:0.81358, 0.00972, The final GNN Acc:0.83922, 0.00075
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9458])
updated graph: torch.Size([2, 10532])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0245730876922607 = 1.9386049509048462 + 0.01 * 8.596810340881348
Epoch 0, val loss: 1.9400571584701538
Epoch 10, training loss: 2.014847993850708 = 1.928880214691162 + 0.01 * 8.596769332885742
Epoch 10, val loss: 1.9309475421905518
Epoch 20, training loss: 2.0032267570495605 = 1.9172606468200684 + 0.01 * 8.596606254577637
Epoch 20, val loss: 1.9196571111679077
Epoch 30, training loss: 1.9873888492584229 = 1.9014267921447754 + 0.01 * 8.596209526062012
Epoch 30, val loss: 1.903915286064148
Epoch 40, training loss: 1.9645003080368042 = 1.8785514831542969 + 0.01 * 8.594879150390625
Epoch 40, val loss: 1.8811341524124146
Epoch 50, training loss: 1.9328815937042236 = 1.8470157384872437 + 0.01 * 8.586580276489258
Epoch 50, val loss: 1.850917935371399
Epoch 60, training loss: 1.8965649604797363 = 1.811331033706665 + 0.01 * 8.523393630981445
Epoch 60, val loss: 1.8194350004196167
Epoch 70, training loss: 1.859764575958252 = 1.778334140777588 + 0.01 * 8.143049240112305
Epoch 70, val loss: 1.790632724761963
Epoch 80, training loss: 1.8167986869812012 = 1.7375684976577759 + 0.01 * 7.923023223876953
Epoch 80, val loss: 1.751366376876831
Epoch 90, training loss: 1.7587376832962036 = 1.6806488037109375 + 0.01 * 7.8088908195495605
Epoch 90, val loss: 1.6987563371658325
Epoch 100, training loss: 1.6800994873046875 = 1.6031914949417114 + 0.01 * 7.690801620483398
Epoch 100, val loss: 1.6304762363433838
Epoch 110, training loss: 1.5843762159347534 = 1.5090585947036743 + 0.01 * 7.531759738922119
Epoch 110, val loss: 1.5481055974960327
Epoch 120, training loss: 1.4822521209716797 = 1.4081733226776123 + 0.01 * 7.407883644104004
Epoch 120, val loss: 1.4633082151412964
Epoch 130, training loss: 1.3839691877365112 = 1.3105061054229736 + 0.01 * 7.346312999725342
Epoch 130, val loss: 1.3856043815612793
Epoch 140, training loss: 1.293524146080017 = 1.2202774286270142 + 0.01 * 7.324669361114502
Epoch 140, val loss: 1.3194408416748047
Epoch 150, training loss: 1.208871841430664 = 1.1357935667037964 + 0.01 * 7.30782413482666
Epoch 150, val loss: 1.2612429857254028
Epoch 160, training loss: 1.1256972551345825 = 1.0527918338775635 + 0.01 * 7.290543556213379
Epoch 160, val loss: 1.2057558298110962
Epoch 170, training loss: 1.0413163900375366 = 0.9685637354850769 + 0.01 * 7.27526330947876
Epoch 170, val loss: 1.1496526002883911
Epoch 180, training loss: 0.9556986689567566 = 0.8830516338348389 + 0.01 * 7.264706134796143
Epoch 180, val loss: 1.09310781955719
Epoch 190, training loss: 0.8709420561790466 = 0.7983102798461914 + 0.01 * 7.263176441192627
Epoch 190, val loss: 1.0375750064849854
Epoch 200, training loss: 0.7898082733154297 = 0.7172184586524963 + 0.01 * 7.258983612060547
Epoch 200, val loss: 0.9855789542198181
Epoch 210, training loss: 0.7148466110229492 = 0.6422791481018066 + 0.01 * 7.256743431091309
Epoch 210, val loss: 0.9396790862083435
Epoch 220, training loss: 0.6474714875221252 = 0.5749183893203735 + 0.01 * 7.255311965942383
Epoch 220, val loss: 0.9010962843894958
Epoch 230, training loss: 0.5878291130065918 = 0.5152955055236816 + 0.01 * 7.253359317779541
Epoch 230, val loss: 0.8700257539749146
Epoch 240, training loss: 0.5353067517280579 = 0.46279701590538025 + 0.01 * 7.250972270965576
Epoch 240, val loss: 0.8456324338912964
Epoch 250, training loss: 0.4889596998691559 = 0.4164828658103943 + 0.01 * 7.247684478759766
Epoch 250, val loss: 0.827051043510437
Epoch 260, training loss: 0.4477306604385376 = 0.3752630650997162 + 0.01 * 7.246758460998535
Epoch 260, val loss: 0.8128727674484253
Epoch 270, training loss: 0.4105048179626465 = 0.33810633420944214 + 0.01 * 7.239847660064697
Epoch 270, val loss: 0.8019936084747314
Epoch 280, training loss: 0.37649405002593994 = 0.30413782596588135 + 0.01 * 7.235623359680176
Epoch 280, val loss: 0.7934825420379639
Epoch 290, training loss: 0.34492796659469604 = 0.2726297378540039 + 0.01 * 7.229824542999268
Epoch 290, val loss: 0.7866712212562561
Epoch 300, training loss: 0.3155289888381958 = 0.2432386428117752 + 0.01 * 7.229033470153809
Epoch 300, val loss: 0.78146892786026
Epoch 310, training loss: 0.2882283926010132 = 0.21602869033813477 + 0.01 * 7.219970226287842
Epoch 310, val loss: 0.7780362367630005
Epoch 320, training loss: 0.2634051740169525 = 0.19129244983196259 + 0.01 * 7.211273670196533
Epoch 320, val loss: 0.7767754197120667
Epoch 330, training loss: 0.2412968873977661 = 0.1692761778831482 + 0.01 * 7.202070236206055
Epoch 330, val loss: 0.7778446674346924
Epoch 340, training loss: 0.22194804251194 = 0.14998267590999603 + 0.01 * 7.196536540985107
Epoch 340, val loss: 0.7813597321510315
Epoch 350, training loss: 0.2049938440322876 = 0.13319170475006104 + 0.01 * 7.1802144050598145
Epoch 350, val loss: 0.7871877551078796
Epoch 360, training loss: 0.1903020590543747 = 0.1185799166560173 + 0.01 * 7.172214031219482
Epoch 360, val loss: 0.7950805425643921
Epoch 370, training loss: 0.17743070423603058 = 0.10582739859819412 + 0.01 * 7.160330772399902
Epoch 370, val loss: 0.8046445846557617
Epoch 380, training loss: 0.1661645770072937 = 0.09464585036039352 + 0.01 * 7.151873588562012
Epoch 380, val loss: 0.8155627250671387
Epoch 390, training loss: 0.15628822147846222 = 0.08479992300271988 + 0.01 * 7.148829936981201
Epoch 390, val loss: 0.8275482058525085
Epoch 400, training loss: 0.14759904146194458 = 0.07610877603292465 + 0.01 * 7.149026393890381
Epoch 400, val loss: 0.8403120636940002
Epoch 410, training loss: 0.13977712392807007 = 0.0684061273932457 + 0.01 * 7.1371002197265625
Epoch 410, val loss: 0.8537923693656921
Epoch 420, training loss: 0.13287653028964996 = 0.06156546249985695 + 0.01 * 7.131107330322266
Epoch 420, val loss: 0.8678869009017944
Epoch 430, training loss: 0.1267571747303009 = 0.055491503328084946 + 0.01 * 7.126567840576172
Epoch 430, val loss: 0.8825348019599915
Epoch 440, training loss: 0.12131944298744202 = 0.050109587609767914 + 0.01 * 7.120985984802246
Epoch 440, val loss: 0.8975818157196045
Epoch 450, training loss: 0.11662371456623077 = 0.04535076394677162 + 0.01 * 7.127295017242432
Epoch 450, val loss: 0.9128813743591309
Epoch 460, training loss: 0.11229632794857025 = 0.04115300998091698 + 0.01 * 7.114331245422363
Epoch 460, val loss: 0.9283488392829895
Epoch 470, training loss: 0.10855463147163391 = 0.03745037689805031 + 0.01 * 7.110424995422363
Epoch 470, val loss: 0.9437519311904907
Epoch 480, training loss: 0.1053442656993866 = 0.03418254852294922 + 0.01 * 7.116171836853027
Epoch 480, val loss: 0.9590110778808594
Epoch 490, training loss: 0.1023491770029068 = 0.03129817172884941 + 0.01 * 7.105100631713867
Epoch 490, val loss: 0.9740474224090576
Epoch 500, training loss: 0.09970681369304657 = 0.028742948547005653 + 0.01 * 7.096386909484863
Epoch 500, val loss: 0.9887580871582031
Epoch 510, training loss: 0.09739121049642563 = 0.026470914483070374 + 0.01 * 7.092029571533203
Epoch 510, val loss: 1.0030988454818726
Epoch 520, training loss: 0.09531538188457489 = 0.02444540709257126 + 0.01 * 7.0869975090026855
Epoch 520, val loss: 1.0171663761138916
Epoch 530, training loss: 0.09350593388080597 = 0.02263481356203556 + 0.01 * 7.0871124267578125
Epoch 530, val loss: 1.0307214260101318
Epoch 540, training loss: 0.09176614880561829 = 0.021013882011175156 + 0.01 * 7.075226306915283
Epoch 540, val loss: 1.0439094305038452
Epoch 550, training loss: 0.09025691449642181 = 0.01955605112016201 + 0.01 * 7.0700860023498535
Epoch 550, val loss: 1.0568013191223145
Epoch 560, training loss: 0.0890209823846817 = 0.018240878358483315 + 0.01 * 7.0780110359191895
Epoch 560, val loss: 1.0691572427749634
Epoch 570, training loss: 0.08766493201255798 = 0.017055131494998932 + 0.01 * 7.060980319976807
Epoch 570, val loss: 1.0812791585922241
Epoch 580, training loss: 0.08653109520673752 = 0.0159789826720953 + 0.01 * 7.055211544036865
Epoch 580, val loss: 1.0930612087249756
Epoch 590, training loss: 0.08555092662572861 = 0.015003948472440243 + 0.01 * 7.054697513580322
Epoch 590, val loss: 1.1044120788574219
Epoch 600, training loss: 0.08453722298145294 = 0.014117910526692867 + 0.01 * 7.041931629180908
Epoch 600, val loss: 1.11543869972229
Epoch 610, training loss: 0.08381333947181702 = 0.01331010926514864 + 0.01 * 7.050323009490967
Epoch 610, val loss: 1.1260946989059448
Epoch 620, training loss: 0.08286178857088089 = 0.012572995387017727 + 0.01 * 7.028879642486572
Epoch 620, val loss: 1.1364738941192627
Epoch 630, training loss: 0.08222100138664246 = 0.011898202821612358 + 0.01 * 7.032279968261719
Epoch 630, val loss: 1.1465680599212646
Epoch 640, training loss: 0.08160089701414108 = 0.011279439553618431 + 0.01 * 7.0321455001831055
Epoch 640, val loss: 1.1563959121704102
Epoch 650, training loss: 0.08081769943237305 = 0.01071156281977892 + 0.01 * 7.010613441467285
Epoch 650, val loss: 1.1657674312591553
Epoch 660, training loss: 0.08026030659675598 = 0.010188781656324863 + 0.01 * 7.007153034210205
Epoch 660, val loss: 1.1749234199523926
Epoch 670, training loss: 0.07992604374885559 = 0.009705448523163795 + 0.01 * 7.022059917449951
Epoch 670, val loss: 1.183750033378601
Epoch 680, training loss: 0.07918704301118851 = 0.009260041639208794 + 0.01 * 6.992700099945068
Epoch 680, val loss: 1.1924575567245483
Epoch 690, training loss: 0.07881689071655273 = 0.00884751882404089 + 0.01 * 6.996937274932861
Epoch 690, val loss: 1.2008273601531982
Epoch 700, training loss: 0.07831559330224991 = 0.008464212529361248 + 0.01 * 6.985138416290283
Epoch 700, val loss: 1.2090823650360107
Epoch 710, training loss: 0.0778014063835144 = 0.008107720874249935 + 0.01 * 6.9693684577941895
Epoch 710, val loss: 1.217081904411316
Epoch 720, training loss: 0.0776980072259903 = 0.007775682955980301 + 0.01 * 6.992231845855713
Epoch 720, val loss: 1.224729299545288
Epoch 730, training loss: 0.07706994563341141 = 0.00746638048440218 + 0.01 * 6.96035623550415
Epoch 730, val loss: 1.2322715520858765
Epoch 740, training loss: 0.07678189873695374 = 0.007177620194852352 + 0.01 * 6.960428237915039
Epoch 740, val loss: 1.2394760847091675
Epoch 750, training loss: 0.07634850591421127 = 0.0069075217470526695 + 0.01 * 6.944098472595215
Epoch 750, val loss: 1.2465124130249023
Epoch 760, training loss: 0.07600024342536926 = 0.006654301658272743 + 0.01 * 6.934594631195068
Epoch 760, val loss: 1.2534892559051514
Epoch 770, training loss: 0.0757453516125679 = 0.006416484713554382 + 0.01 * 6.932886600494385
Epoch 770, val loss: 1.2603083848953247
Epoch 780, training loss: 0.07547452300786972 = 0.006193066015839577 + 0.01 * 6.928145885467529
Epoch 780, val loss: 1.2667951583862305
Epoch 790, training loss: 0.07514899224042892 = 0.005982805509120226 + 0.01 * 6.916618347167969
Epoch 790, val loss: 1.2731655836105347
Epoch 800, training loss: 0.0749933049082756 = 0.005784363951534033 + 0.01 * 6.920894622802734
Epoch 800, val loss: 1.2794005870819092
Epoch 810, training loss: 0.07467903941869736 = 0.005597373470664024 + 0.01 * 6.908166408538818
Epoch 810, val loss: 1.2855511903762817
Epoch 820, training loss: 0.07456447929143906 = 0.005420679692178965 + 0.01 * 6.9143805503845215
Epoch 820, val loss: 1.2915254831314087
Epoch 830, training loss: 0.07448205351829529 = 0.005253932438790798 + 0.01 * 6.922811985015869
Epoch 830, val loss: 1.2972999811172485
Epoch 840, training loss: 0.07425230741500854 = 0.005095680244266987 + 0.01 * 6.91566276550293
Epoch 840, val loss: 1.302936315536499
Epoch 850, training loss: 0.07388731092214584 = 0.004945995751768351 + 0.01 * 6.894132137298584
Epoch 850, val loss: 1.3084396123886108
Epoch 860, training loss: 0.0736493170261383 = 0.004804044961929321 + 0.01 * 6.884527683258057
Epoch 860, val loss: 1.3138298988342285
Epoch 870, training loss: 0.07362174987792969 = 0.004668715409934521 + 0.01 * 6.895304203033447
Epoch 870, val loss: 1.3189513683319092
Epoch 880, training loss: 0.07329258322715759 = 0.004540512803941965 + 0.01 * 6.875207424163818
Epoch 880, val loss: 1.3242121934890747
Epoch 890, training loss: 0.07353272289037704 = 0.004418553784489632 + 0.01 * 6.911417007446289
Epoch 890, val loss: 1.3291391134262085
Epoch 900, training loss: 0.07311777025461197 = 0.00430218456313014 + 0.01 * 6.881559371948242
Epoch 900, val loss: 1.334006667137146
Epoch 910, training loss: 0.0728505477309227 = 0.00419141910970211 + 0.01 * 6.865912914276123
Epoch 910, val loss: 1.3387926816940308
Epoch 920, training loss: 0.07308591157197952 = 0.004085647873580456 + 0.01 * 6.900026798248291
Epoch 920, val loss: 1.3434677124023438
Epoch 930, training loss: 0.07259747385978699 = 0.003984919283539057 + 0.01 * 6.861255645751953
Epoch 930, val loss: 1.347878336906433
Epoch 940, training loss: 0.07247830927371979 = 0.0038887783885002136 + 0.01 * 6.858953475952148
Epoch 940, val loss: 1.3523309230804443
Epoch 950, training loss: 0.07248502969741821 = 0.003796743229031563 + 0.01 * 6.868828773498535
Epoch 950, val loss: 1.3566503524780273
Epoch 960, training loss: 0.07234593480825424 = 0.00370859750546515 + 0.01 * 6.863733291625977
Epoch 960, val loss: 1.360808253288269
Epoch 970, training loss: 0.07201629877090454 = 0.0036242848727852106 + 0.01 * 6.839201927185059
Epoch 970, val loss: 1.3650943040847778
Epoch 980, training loss: 0.07187020033597946 = 0.003543396247550845 + 0.01 * 6.8326802253723145
Epoch 980, val loss: 1.3690619468688965
Epoch 990, training loss: 0.07198067754507065 = 0.0034660121891647577 + 0.01 * 6.851467132568359
Epoch 990, val loss: 1.3729780912399292
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 2.0380027294158936 = 1.9520342350006104 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.9637529850006104
Epoch 10, training loss: 2.028352737426758 = 1.9423850774765015 + 0.01 * 8.596776008605957
Epoch 10, val loss: 1.953568696975708
Epoch 20, training loss: 2.0166096687316895 = 1.9306435585021973 + 0.01 * 8.596604347229004
Epoch 20, val loss: 1.9410266876220703
Epoch 30, training loss: 2.000544548034668 = 1.914583444595337 + 0.01 * 8.59611988067627
Epoch 30, val loss: 1.9237333536148071
Epoch 40, training loss: 1.9771087169647217 = 1.8911694288253784 + 0.01 * 8.593924522399902
Epoch 40, val loss: 1.89853036403656
Epoch 50, training loss: 1.9437565803527832 = 1.857962965965271 + 0.01 * 8.579357147216797
Epoch 50, val loss: 1.8636794090270996
Epoch 60, training loss: 1.9038758277893066 = 1.8187663555145264 + 0.01 * 8.510948181152344
Epoch 60, val loss: 1.825595736503601
Epoch 70, training loss: 1.8695809841156006 = 1.78603196144104 + 0.01 * 8.354898452758789
Epoch 70, val loss: 1.7977598905563354
Epoch 80, training loss: 1.8344370126724243 = 1.7528029680252075 + 0.01 * 8.163399696350098
Epoch 80, val loss: 1.7693078517913818
Epoch 90, training loss: 1.7852636575698853 = 1.706938624382019 + 0.01 * 7.832501411437988
Epoch 90, val loss: 1.7291380167007446
Epoch 100, training loss: 1.720417857170105 = 1.644101619720459 + 0.01 * 7.6316237449646
Epoch 100, val loss: 1.6759785413742065
Epoch 110, training loss: 1.6364272832870483 = 1.5617308616638184 + 0.01 * 7.469644069671631
Epoch 110, val loss: 1.6082533597946167
Epoch 120, training loss: 1.5378618240356445 = 1.464377760887146 + 0.01 * 7.348404884338379
Epoch 120, val loss: 1.5284894704818726
Epoch 130, training loss: 1.4337538480758667 = 1.3608779907226562 + 0.01 * 7.287585258483887
Epoch 130, val loss: 1.4452654123306274
Epoch 140, training loss: 1.3321226835250854 = 1.259505033493042 + 0.01 * 7.2617692947387695
Epoch 140, val loss: 1.364855170249939
Epoch 150, training loss: 1.2365137338638306 = 1.164039134979248 + 0.01 * 7.24746036529541
Epoch 150, val loss: 1.2904548645019531
Epoch 160, training loss: 1.147641897201538 = 1.0752696990966797 + 0.01 * 7.237216472625732
Epoch 160, val loss: 1.2226028442382812
Epoch 170, training loss: 1.0647772550582886 = 0.9924880266189575 + 0.01 * 7.228920936584473
Epoch 170, val loss: 1.1617766618728638
Epoch 180, training loss: 0.9860029816627502 = 0.9137957096099854 + 0.01 * 7.220728874206543
Epoch 180, val loss: 1.1064189672470093
Epoch 190, training loss: 0.9100023508071899 = 0.8378700017929077 + 0.01 * 7.213233947753906
Epoch 190, val loss: 1.054764747619629
Epoch 200, training loss: 0.8366504311561584 = 0.7645864486694336 + 0.01 * 7.206399440765381
Epoch 200, val loss: 1.0061123371124268
Epoch 210, training loss: 0.7673556804656982 = 0.6953601837158203 + 0.01 * 7.199547290802002
Epoch 210, val loss: 0.9614977836608887
Epoch 220, training loss: 0.7035245895385742 = 0.6315929889678955 + 0.01 * 7.193157196044922
Epoch 220, val loss: 0.9225305318832397
Epoch 230, training loss: 0.6454242467880249 = 0.5735616683959961 + 0.01 * 7.186257839202881
Epoch 230, val loss: 0.8908451199531555
Epoch 240, training loss: 0.5924007892608643 = 0.5206093788146973 + 0.01 * 7.179141998291016
Epoch 240, val loss: 0.8665196895599365
Epoch 250, training loss: 0.5436559915542603 = 0.47195276618003845 + 0.01 * 7.170323371887207
Epoch 250, val loss: 0.8486365079879761
Epoch 260, training loss: 0.49842727184295654 = 0.42682746052742004 + 0.01 * 7.159981727600098
Epoch 260, val loss: 0.8354759216308594
Epoch 270, training loss: 0.4562680125236511 = 0.38469985127449036 + 0.01 * 7.1568169593811035
Epoch 270, val loss: 0.8258141279220581
Epoch 280, training loss: 0.4167114496231079 = 0.3452984094619751 + 0.01 * 7.1413044929504395
Epoch 280, val loss: 0.8192790150642395
Epoch 290, training loss: 0.3799046277999878 = 0.3085602819919586 + 0.01 * 7.134435653686523
Epoch 290, val loss: 0.8156815767288208
Epoch 300, training loss: 0.3458525240421295 = 0.27459296584129333 + 0.01 * 7.125956058502197
Epoch 300, val loss: 0.8149566054344177
Epoch 310, training loss: 0.314716100692749 = 0.24353384971618652 + 0.01 * 7.118224620819092
Epoch 310, val loss: 0.817113995552063
Epoch 320, training loss: 0.28657233715057373 = 0.21545447409152985 + 0.01 * 7.111785411834717
Epoch 320, val loss: 0.822020947933197
Epoch 330, training loss: 0.26145678758621216 = 0.19035948812961578 + 0.01 * 7.109730243682861
Epoch 330, val loss: 0.8293837904930115
Epoch 340, training loss: 0.23919665813446045 = 0.16814760863780975 + 0.01 * 7.104904651641846
Epoch 340, val loss: 0.8389434814453125
Epoch 350, training loss: 0.2195514738559723 = 0.14861875772476196 + 0.01 * 7.093270778656006
Epoch 350, val loss: 0.850212812423706
Epoch 360, training loss: 0.20249731838703156 = 0.13153746724128723 + 0.01 * 7.095984935760498
Epoch 360, val loss: 0.8629127144813538
Epoch 370, training loss: 0.18751715123653412 = 0.11663531512022018 + 0.01 * 7.088183403015137
Epoch 370, val loss: 0.8767309188842773
Epoch 380, training loss: 0.17443716526031494 = 0.1036200299859047 + 0.01 * 7.081713676452637
Epoch 380, val loss: 0.891247034072876
Epoch 390, training loss: 0.16295427083969116 = 0.09224642068147659 + 0.01 * 7.070784091949463
Epoch 390, val loss: 0.9062291979789734
Epoch 400, training loss: 0.15297576785087585 = 0.08229789137840271 + 0.01 * 7.067787170410156
Epoch 400, val loss: 0.9215845465660095
Epoch 410, training loss: 0.14433975517749786 = 0.07358860224485397 + 0.01 * 7.07511568069458
Epoch 410, val loss: 0.9370835423469543
Epoch 420, training loss: 0.13648691773414612 = 0.06596209108829498 + 0.01 * 7.052483081817627
Epoch 420, val loss: 0.9526904225349426
Epoch 430, training loss: 0.12985430657863617 = 0.059273894876241684 + 0.01 * 7.058042049407959
Epoch 430, val loss: 0.9682720899581909
Epoch 440, training loss: 0.12386888265609741 = 0.05340349301695824 + 0.01 * 7.046538829803467
Epoch 440, val loss: 0.9837552905082703
Epoch 450, training loss: 0.11863299459218979 = 0.04825030267238617 + 0.01 * 7.038269519805908
Epoch 450, val loss: 0.9989954233169556
Epoch 460, training loss: 0.11412551254034042 = 0.04372821003198624 + 0.01 * 7.039730072021484
Epoch 460, val loss: 1.013969898223877
Epoch 470, training loss: 0.11003030836582184 = 0.03974653407931328 + 0.01 * 7.028378009796143
Epoch 470, val loss: 1.028610110282898
Epoch 480, training loss: 0.10644160211086273 = 0.036231234669685364 + 0.01 * 7.0210371017456055
Epoch 480, val loss: 1.0429480075836182
Epoch 490, training loss: 0.10336507856845856 = 0.03312555328011513 + 0.01 * 7.023952484130859
Epoch 490, val loss: 1.05693781375885
Epoch 500, training loss: 0.10053014755249023 = 0.030378369614481926 + 0.01 * 7.0151777267456055
Epoch 500, val loss: 1.0704008340835571
Epoch 510, training loss: 0.09797961264848709 = 0.027938760817050934 + 0.01 * 7.004085063934326
Epoch 510, val loss: 1.083514928817749
Epoch 520, training loss: 0.09592588990926743 = 0.025765487924218178 + 0.01 * 7.016040325164795
Epoch 520, val loss: 1.0962755680084229
Epoch 530, training loss: 0.09379430115222931 = 0.02382718026638031 + 0.01 * 6.9967122077941895
Epoch 530, val loss: 1.1086069345474243
Epoch 540, training loss: 0.09211420267820358 = 0.022092515602707863 + 0.01 * 7.002169132232666
Epoch 540, val loss: 1.1205124855041504
Epoch 550, training loss: 0.0904020220041275 = 0.02053680829703808 + 0.01 * 6.986521244049072
Epoch 550, val loss: 1.131959319114685
Epoch 560, training loss: 0.08910370618104935 = 0.019136155024170876 + 0.01 * 6.996755599975586
Epoch 560, val loss: 1.143075704574585
Epoch 570, training loss: 0.08779047429561615 = 0.017874913290143013 + 0.01 * 6.991556167602539
Epoch 570, val loss: 1.1538617610931396
Epoch 580, training loss: 0.08643198758363724 = 0.016735272482037544 + 0.01 * 6.969671726226807
Epoch 580, val loss: 1.1641943454742432
Epoch 590, training loss: 0.08536551892757416 = 0.01570155844092369 + 0.01 * 6.966395854949951
Epoch 590, val loss: 1.1741690635681152
Epoch 600, training loss: 0.08438446372747421 = 0.01476206723600626 + 0.01 * 6.962239742279053
Epoch 600, val loss: 1.1838197708129883
Epoch 610, training loss: 0.08357049524784088 = 0.01390562579035759 + 0.01 * 6.96648645401001
Epoch 610, val loss: 1.1931272745132446
Epoch 620, training loss: 0.08266747742891312 = 0.013123917393386364 + 0.01 * 6.954356670379639
Epoch 620, val loss: 1.2021563053131104
Epoch 630, training loss: 0.0819474384188652 = 0.012408377602696419 + 0.01 * 6.953906536102295
Epoch 630, val loss: 1.2107874155044556
Epoch 640, training loss: 0.08131285756826401 = 0.011751577258110046 + 0.01 * 6.956128120422363
Epoch 640, val loss: 1.2192068099975586
Epoch 650, training loss: 0.08054201304912567 = 0.011148478835821152 + 0.01 * 6.939353942871094
Epoch 650, val loss: 1.2273173332214355
Epoch 660, training loss: 0.07994277030229568 = 0.010592645965516567 + 0.01 * 6.9350128173828125
Epoch 660, val loss: 1.2351630926132202
Epoch 670, training loss: 0.0794914960861206 = 0.010079573839902878 + 0.01 * 6.941192626953125
Epoch 670, val loss: 1.2427117824554443
Epoch 680, training loss: 0.079043447971344 = 0.009606018662452698 + 0.01 * 6.943742752075195
Epoch 680, val loss: 1.250035285949707
Epoch 690, training loss: 0.07832151651382446 = 0.009167810901999474 + 0.01 * 6.915370464324951
Epoch 690, val loss: 1.2570937871932983
Epoch 700, training loss: 0.07793465256690979 = 0.008760927245020866 + 0.01 * 6.917372703552246
Epoch 700, val loss: 1.2640074491500854
Epoch 710, training loss: 0.07751120626926422 = 0.008382354862987995 + 0.01 * 6.9128851890563965
Epoch 710, val loss: 1.2707058191299438
Epoch 720, training loss: 0.0770951509475708 = 0.008029770106077194 + 0.01 * 6.906538486480713
Epoch 720, val loss: 1.2771254777908325
Epoch 730, training loss: 0.07665415853261948 = 0.007700601126998663 + 0.01 * 6.895356178283691
Epoch 730, val loss: 1.283348560333252
Epoch 740, training loss: 0.07636462152004242 = 0.0073932986706495285 + 0.01 * 6.89713191986084
Epoch 740, val loss: 1.2894368171691895
Epoch 750, training loss: 0.07616651058197021 = 0.007105399388819933 + 0.01 * 6.906111240386963
Epoch 750, val loss: 1.29529869556427
Epoch 760, training loss: 0.07573224604129791 = 0.006835169158875942 + 0.01 * 6.889708042144775
Epoch 760, val loss: 1.3011571168899536
Epoch 770, training loss: 0.07541488856077194 = 0.006580644752830267 + 0.01 * 6.883424758911133
Epoch 770, val loss: 1.3065801858901978
Epoch 780, training loss: 0.07511844485998154 = 0.006339970510452986 + 0.01 * 6.877848148345947
Epoch 780, val loss: 1.3120588064193726
Epoch 790, training loss: 0.07507436722517014 = 0.006111992057412863 + 0.01 * 6.896237373352051
Epoch 790, val loss: 1.3172383308410645
Epoch 800, training loss: 0.07469889521598816 = 0.005896404851227999 + 0.01 * 6.8802490234375
Epoch 800, val loss: 1.3225111961364746
Epoch 810, training loss: 0.0744919404387474 = 0.005692014470696449 + 0.01 * 6.879992961883545
Epoch 810, val loss: 1.3274043798446655
Epoch 820, training loss: 0.07408098131418228 = 0.005498514045029879 + 0.01 * 6.858246803283691
Epoch 820, val loss: 1.3322312831878662
Epoch 830, training loss: 0.07382985949516296 = 0.005314666777849197 + 0.01 * 6.851519584655762
Epoch 830, val loss: 1.33694326877594
Epoch 840, training loss: 0.07367780804634094 = 0.005139864981174469 + 0.01 * 6.853794574737549
Epoch 840, val loss: 1.3414649963378906
Epoch 850, training loss: 0.07338099181652069 = 0.00497380131855607 + 0.01 * 6.840719223022461
Epoch 850, val loss: 1.3459216356277466
Epoch 860, training loss: 0.07328659296035767 = 0.0048161642625927925 + 0.01 * 6.847043037414551
Epoch 860, val loss: 1.35032057762146
Epoch 870, training loss: 0.073200523853302 = 0.004666351713240147 + 0.01 * 6.85341739654541
Epoch 870, val loss: 1.3544360399246216
Epoch 880, training loss: 0.07290652394294739 = 0.004523451905697584 + 0.01 * 6.838307857513428
Epoch 880, val loss: 1.3586379289627075
Epoch 890, training loss: 0.07278478890657425 = 0.004387864843010902 + 0.01 * 6.839693069458008
Epoch 890, val loss: 1.3624744415283203
Epoch 900, training loss: 0.0726594403386116 = 0.004258537665009499 + 0.01 * 6.840090274810791
Epoch 900, val loss: 1.366363525390625
Epoch 910, training loss: 0.07236373424530029 = 0.004135575145483017 + 0.01 * 6.822815895080566
Epoch 910, val loss: 1.370139479637146
Epoch 920, training loss: 0.072394460439682 = 0.004018755629658699 + 0.01 * 6.837571144104004
Epoch 920, val loss: 1.3736844062805176
Epoch 930, training loss: 0.07202126085758209 = 0.003907413221895695 + 0.01 * 6.811384677886963
Epoch 930, val loss: 1.37735116481781
Epoch 940, training loss: 0.0719054564833641 = 0.0038013113662600517 + 0.01 * 6.810414791107178
Epoch 940, val loss: 1.3806180953979492
Epoch 950, training loss: 0.07173153012990952 = 0.0036999310832470655 + 0.01 * 6.803160667419434
Epoch 950, val loss: 1.3840473890304565
Epoch 960, training loss: 0.07160520553588867 = 0.0036031354684382677 + 0.01 * 6.800207138061523
Epoch 960, val loss: 1.387255311012268
Epoch 970, training loss: 0.0715852901339531 = 0.003510694485157728 + 0.01 * 6.807460308074951
Epoch 970, val loss: 1.390433669090271
Epoch 980, training loss: 0.07135219871997833 = 0.003422426525503397 + 0.01 * 6.792977809906006
Epoch 980, val loss: 1.3935608863830566
Epoch 990, training loss: 0.07129140198230743 = 0.0033379325177520514 + 0.01 * 6.795347213745117
Epoch 990, val loss: 1.3965001106262207
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 2.045501232147217 = 1.9595330953598022 + 0.01 * 8.596803665161133
Epoch 0, val loss: 1.9575140476226807
Epoch 10, training loss: 2.0355122089385986 = 1.9495445489883423 + 0.01 * 8.596755981445312
Epoch 10, val loss: 1.9472383260726929
Epoch 20, training loss: 2.0237176418304443 = 1.9377520084381104 + 0.01 * 8.596561431884766
Epoch 20, val loss: 1.9351130723953247
Epoch 30, training loss: 2.0075793266296387 = 1.9216195344924927 + 0.01 * 8.595967292785645
Epoch 30, val loss: 1.9187430143356323
Epoch 40, training loss: 1.9838457107543945 = 1.8979153633117676 + 0.01 * 8.593031883239746
Epoch 40, val loss: 1.894995093345642
Epoch 50, training loss: 1.9492840766906738 = 1.8635423183441162 + 0.01 * 8.574179649353027
Epoch 50, val loss: 1.8617395162582397
Epoch 60, training loss: 1.906632661819458 = 1.8216485977172852 + 0.01 * 8.498406410217285
Epoch 60, val loss: 1.824596881866455
Epoch 70, training loss: 1.8703349828720093 = 1.7870298624038696 + 0.01 * 8.330509185791016
Epoch 70, val loss: 1.7980067729949951
Epoch 80, training loss: 1.8354250192642212 = 1.7544920444488525 + 0.01 * 8.093300819396973
Epoch 80, val loss: 1.7700704336166382
Epoch 90, training loss: 1.786911129951477 = 1.709374189376831 + 0.01 * 7.7536940574646
Epoch 90, val loss: 1.7301485538482666
Epoch 100, training loss: 1.7236367464065552 = 1.6483570337295532 + 0.01 * 7.5279669761657715
Epoch 100, val loss: 1.6785613298416138
Epoch 110, training loss: 1.6416130065917969 = 1.567400574684143 + 0.01 * 7.421238899230957
Epoch 110, val loss: 1.6111972332000732
Epoch 120, training loss: 1.5452033281326294 = 1.4715269804000854 + 0.01 * 7.367631435394287
Epoch 120, val loss: 1.5311028957366943
Epoch 130, training loss: 1.4418981075286865 = 1.3686113357543945 + 0.01 * 7.328682899475098
Epoch 130, val loss: 1.4458032846450806
Epoch 140, training loss: 1.3363301753997803 = 1.2634203433990479 + 0.01 * 7.29098653793335
Epoch 140, val loss: 1.3589919805526733
Epoch 150, training loss: 1.2303247451782227 = 1.1577409505844116 + 0.01 * 7.258380889892578
Epoch 150, val loss: 1.272588849067688
Epoch 160, training loss: 1.1262445449829102 = 1.05386221408844 + 0.01 * 7.23823356628418
Epoch 160, val loss: 1.1898893117904663
Epoch 170, training loss: 1.0269346237182617 = 0.954634428024292 + 0.01 * 7.230014324188232
Epoch 170, val loss: 1.1128724813461304
Epoch 180, training loss: 0.9344555735588074 = 0.862175703048706 + 0.01 * 7.227989673614502
Epoch 180, val loss: 1.0427831411361694
Epoch 190, training loss: 0.8497393131256104 = 0.7774685025215149 + 0.01 * 7.227081775665283
Epoch 190, val loss: 0.980358898639679
Epoch 200, training loss: 0.7731640338897705 = 0.7009009122848511 + 0.01 * 7.226309299468994
Epoch 200, val loss: 0.9259005784988403
Epoch 210, training loss: 0.7041504979133606 = 0.6318945288658142 + 0.01 * 7.22559928894043
Epoch 210, val loss: 0.8792055249214172
Epoch 220, training loss: 0.6415336728096008 = 0.569283664226532 + 0.01 * 7.22499942779541
Epoch 220, val loss: 0.8397806882858276
Epoch 230, training loss: 0.5841917395591736 = 0.511949360370636 + 0.01 * 7.224236488342285
Epoch 230, val loss: 0.8070788979530334
Epoch 240, training loss: 0.5313568115234375 = 0.4591250419616699 + 0.01 * 7.223175525665283
Epoch 240, val loss: 0.7806339859962463
Epoch 250, training loss: 0.4824732542037964 = 0.4102597236633301 + 0.01 * 7.221353530883789
Epoch 250, val loss: 0.7599181532859802
Epoch 260, training loss: 0.43703263998031616 = 0.364841103553772 + 0.01 * 7.219155311584473
Epoch 260, val loss: 0.7438669204711914
Epoch 270, training loss: 0.394675612449646 = 0.3225210905075073 + 0.01 * 7.215450763702393
Epoch 270, val loss: 0.7317333817481995
Epoch 280, training loss: 0.3552963435649872 = 0.28317514061927795 + 0.01 * 7.212121486663818
Epoch 280, val loss: 0.7232165932655334
Epoch 290, training loss: 0.31901925802230835 = 0.24696271121501923 + 0.01 * 7.2056565284729
Epoch 290, val loss: 0.7182487845420837
Epoch 300, training loss: 0.2862333059310913 = 0.21418653428554535 + 0.01 * 7.204675674438477
Epoch 300, val loss: 0.716819167137146
Epoch 310, training loss: 0.2570028305053711 = 0.1850907951593399 + 0.01 * 7.1912055015563965
Epoch 310, val loss: 0.7188668251037598
Epoch 320, training loss: 0.23155882954597473 = 0.159713014960289 + 0.01 * 7.184582233428955
Epoch 320, val loss: 0.7241969704627991
Epoch 330, training loss: 0.2096906453371048 = 0.13789002597332 + 0.01 * 7.1800618171691895
Epoch 330, val loss: 0.7324102520942688
Epoch 340, training loss: 0.19100546836853027 = 0.1193094477057457 + 0.01 * 7.1696014404296875
Epoch 340, val loss: 0.7430480718612671
Epoch 350, training loss: 0.17516188323497772 = 0.10358044505119324 + 0.01 * 7.158143997192383
Epoch 350, val loss: 0.7556191682815552
Epoch 360, training loss: 0.1618177443742752 = 0.09030064195394516 + 0.01 * 7.151710510253906
Epoch 360, val loss: 0.7695491909980774
Epoch 370, training loss: 0.1505693644285202 = 0.07908984273672104 + 0.01 * 7.147952556610107
Epoch 370, val loss: 0.7844014763832092
Epoch 380, training loss: 0.14104411005973816 = 0.06960909068584442 + 0.01 * 7.143501281738281
Epoch 380, val loss: 0.7998119592666626
Epoch 390, training loss: 0.13294826447963715 = 0.06157080829143524 + 0.01 * 7.1377458572387695
Epoch 390, val loss: 0.815520703792572
Epoch 400, training loss: 0.12604622542858124 = 0.054731130599975586 + 0.01 * 7.131509780883789
Epoch 400, val loss: 0.8312896490097046
Epoch 410, training loss: 0.12017623335123062 = 0.04888971894979477 + 0.01 * 7.1286516189575195
Epoch 410, val loss: 0.8469724059104919
Epoch 420, training loss: 0.11511777341365814 = 0.04388052597641945 + 0.01 * 7.123724460601807
Epoch 420, val loss: 0.862446129322052
Epoch 430, training loss: 0.11085303127765656 = 0.03956407308578491 + 0.01 * 7.128896236419678
Epoch 430, val loss: 0.8776566386222839
Epoch 440, training loss: 0.10703925788402557 = 0.03582989051938057 + 0.01 * 7.120936393737793
Epoch 440, val loss: 0.8925357460975647
Epoch 450, training loss: 0.10370685905218124 = 0.032583728432655334 + 0.01 * 7.112313270568848
Epoch 450, val loss: 0.9070706367492676
Epoch 460, training loss: 0.1008153110742569 = 0.029749533161520958 + 0.01 * 7.1065778732299805
Epoch 460, val loss: 0.9212259650230408
Epoch 470, training loss: 0.09828974306583405 = 0.027264337986707687 + 0.01 * 7.102540969848633
Epoch 470, val loss: 0.9349801540374756
Epoch 480, training loss: 0.09606819599866867 = 0.02507472224533558 + 0.01 * 7.099348068237305
Epoch 480, val loss: 0.9483747482299805
Epoch 490, training loss: 0.09410019963979721 = 0.023137113079428673 + 0.01 * 7.096308708190918
Epoch 490, val loss: 0.9613847732543945
Epoch 500, training loss: 0.09236431121826172 = 0.021417293697595596 + 0.01 * 7.094701290130615
Epoch 500, val loss: 0.9740023016929626
Epoch 510, training loss: 0.09076599776744843 = 0.019885266199707985 + 0.01 * 7.08807373046875
Epoch 510, val loss: 0.9862349629402161
Epoch 520, training loss: 0.08933500200510025 = 0.018513886258006096 + 0.01 * 7.082111358642578
Epoch 520, val loss: 0.9981133341789246
Epoch 530, training loss: 0.08804767578840256 = 0.017281562089920044 + 0.01 * 7.076611518859863
Epoch 530, val loss: 1.0096708536148071
Epoch 540, training loss: 0.08693070709705353 = 0.01617109589278698 + 0.01 * 7.075961112976074
Epoch 540, val loss: 1.0208488702774048
Epoch 550, training loss: 0.08591089397668839 = 0.015168683603405952 + 0.01 * 7.074221611022949
Epoch 550, val loss: 1.031714677810669
Epoch 560, training loss: 0.08490768074989319 = 0.014259734191000462 + 0.01 * 7.064795017242432
Epoch 560, val loss: 1.042239785194397
Epoch 570, training loss: 0.08404259383678436 = 0.013432769104838371 + 0.01 * 7.0609822273254395
Epoch 570, val loss: 1.052495002746582
Epoch 580, training loss: 0.08341651409864426 = 0.012678236700594425 + 0.01 * 7.073827743530273
Epoch 580, val loss: 1.0624361038208008
Epoch 590, training loss: 0.08261290937662125 = 0.011989479884505272 + 0.01 * 7.062342643737793
Epoch 590, val loss: 1.0720961093902588
Epoch 600, training loss: 0.08185497671365738 = 0.011358444578945637 + 0.01 * 7.049653053283691
Epoch 600, val loss: 1.081470012664795
Epoch 610, training loss: 0.08118569850921631 = 0.010778574272990227 + 0.01 * 7.040712356567383
Epoch 610, val loss: 1.0906113386154175
Epoch 620, training loss: 0.08086580783128738 = 0.010244420729577541 + 0.01 * 7.062138557434082
Epoch 620, val loss: 1.0994685888290405
Epoch 630, training loss: 0.08010651916265488 = 0.009752319194376469 + 0.01 * 7.035419940948486
Epoch 630, val loss: 1.108106017112732
Epoch 640, training loss: 0.07961038500070572 = 0.009297532960772514 + 0.01 * 7.031285285949707
Epoch 640, val loss: 1.1164730787277222
Epoch 650, training loss: 0.07910633087158203 = 0.008876127190887928 + 0.01 * 7.023020267486572
Epoch 650, val loss: 1.1246417760849
Epoch 660, training loss: 0.07869185507297516 = 0.008484762161970139 + 0.01 * 7.02070951461792
Epoch 660, val loss: 1.1326121091842651
Epoch 670, training loss: 0.07872284948825836 = 0.008120899088680744 + 0.01 * 7.060194969177246
Epoch 670, val loss: 1.1403197050094604
Epoch 680, training loss: 0.07802959531545639 = 0.0077831195667386055 + 0.01 * 7.0246477127075195
Epoch 680, val loss: 1.1478688716888428
Epoch 690, training loss: 0.07756102085113525 = 0.007468277122825384 + 0.01 * 7.009274959564209
Epoch 690, val loss: 1.155150055885315
Epoch 700, training loss: 0.0771796703338623 = 0.007173959165811539 + 0.01 * 7.000571250915527
Epoch 700, val loss: 1.162337064743042
Epoch 710, training loss: 0.07695863395929337 = 0.006898324470967054 + 0.01 * 7.006031513214111
Epoch 710, val loss: 1.1693052053451538
Epoch 720, training loss: 0.07657156139612198 = 0.006640201900154352 + 0.01 * 6.993135929107666
Epoch 720, val loss: 1.1761270761489868
Epoch 730, training loss: 0.07645440846681595 = 0.006397953722625971 + 0.01 * 7.005645751953125
Epoch 730, val loss: 1.182733178138733
Epoch 740, training loss: 0.07611677050590515 = 0.006170901469886303 + 0.01 * 6.994587421417236
Epoch 740, val loss: 1.1892154216766357
Epoch 750, training loss: 0.07569923996925354 = 0.0059572369791567326 + 0.01 * 6.974200248718262
Epoch 750, val loss: 1.1955112218856812
Epoch 760, training loss: 0.07559151947498322 = 0.005755932070314884 + 0.01 * 6.9835591316223145
Epoch 760, val loss: 1.2016808986663818
Epoch 770, training loss: 0.07532582432031631 = 0.005566426087170839 + 0.01 * 6.975939750671387
Epoch 770, val loss: 1.2077083587646484
Epoch 780, training loss: 0.07531524449586868 = 0.005387514363974333 + 0.01 * 6.992773056030273
Epoch 780, val loss: 1.21355140209198
Epoch 790, training loss: 0.07487688958644867 = 0.005218627862632275 + 0.01 * 6.965826511383057
Epoch 790, val loss: 1.2193127870559692
Epoch 800, training loss: 0.07501215487718582 = 0.005058602429926395 + 0.01 * 6.995355129241943
Epoch 800, val loss: 1.2248982191085815
Epoch 810, training loss: 0.07458126544952393 = 0.004907424096018076 + 0.01 * 6.967384338378906
Epoch 810, val loss: 1.2304117679595947
Epoch 820, training loss: 0.07429811358451843 = 0.004764136858284473 + 0.01 * 6.953397274017334
Epoch 820, val loss: 1.2357832193374634
Epoch 830, training loss: 0.07423034310340881 = 0.004627978894859552 + 0.01 * 6.960236549377441
Epoch 830, val loss: 1.2410016059875488
Epoch 840, training loss: 0.07399215549230576 = 0.0044989134185016155 + 0.01 * 6.949324131011963
Epoch 840, val loss: 1.2461785078048706
Epoch 850, training loss: 0.07370103150606155 = 0.004376084543764591 + 0.01 * 6.932494640350342
Epoch 850, val loss: 1.251160740852356
Epoch 860, training loss: 0.07351326942443848 = 0.00425944896414876 + 0.01 * 6.925382137298584
Epoch 860, val loss: 1.256101369857788
Epoch 870, training loss: 0.07344625890254974 = 0.004148320760577917 + 0.01 * 6.9297943115234375
Epoch 870, val loss: 1.2608979940414429
Epoch 880, training loss: 0.07337284088134766 = 0.004042458720505238 + 0.01 * 6.933038234710693
Epoch 880, val loss: 1.2656315565109253
Epoch 890, training loss: 0.07315877825021744 = 0.0039415741339325905 + 0.01 * 6.921720504760742
Epoch 890, val loss: 1.2702664136886597
Epoch 900, training loss: 0.07312656939029694 = 0.0038450879510492086 + 0.01 * 6.92814826965332
Epoch 900, val loss: 1.274768352508545
Epoch 910, training loss: 0.07291226834058762 = 0.0037531668785959482 + 0.01 * 6.915910243988037
Epoch 910, val loss: 1.279234766960144
Epoch 920, training loss: 0.0726412907242775 = 0.0036652314011007547 + 0.01 * 6.897605895996094
Epoch 920, val loss: 1.2835335731506348
Epoch 930, training loss: 0.07262763381004333 = 0.003581189550459385 + 0.01 * 6.90464448928833
Epoch 930, val loss: 1.2878129482269287
Epoch 940, training loss: 0.0725238025188446 = 0.0035007623955607414 + 0.01 * 6.902304649353027
Epoch 940, val loss: 1.2919673919677734
Epoch 950, training loss: 0.07261340320110321 = 0.003423821646720171 + 0.01 * 6.9189581871032715
Epoch 950, val loss: 1.2960902452468872
Epoch 960, training loss: 0.0724261924624443 = 0.0033499887213110924 + 0.01 * 6.907620429992676
Epoch 960, val loss: 1.3000763654708862
Epoch 970, training loss: 0.07211396843194962 = 0.003279342781752348 + 0.01 * 6.883462905883789
Epoch 970, val loss: 1.3040575981140137
Epoch 980, training loss: 0.07238459587097168 = 0.003211389994248748 + 0.01 * 6.917320728302002
Epoch 980, val loss: 1.3079025745391846
Epoch 990, training loss: 0.07201671600341797 = 0.0031461927574127913 + 0.01 * 6.887052536010742
Epoch 990, val loss: 1.3117146492004395
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8054823405376912
The final CL Acc:0.77407, 0.01684, The final GNN Acc:0.80724, 0.00407
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13166])
remove edge: torch.Size([2, 7912])
updated graph: torch.Size([2, 10522])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0207998752593994 = 1.9348313808441162 + 0.01 * 8.596839904785156
Epoch 0, val loss: 1.9411426782608032
Epoch 10, training loss: 2.0109293460845947 = 1.9249614477157593 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.930506706237793
Epoch 20, training loss: 1.998475193977356 = 1.9125089645385742 + 0.01 * 8.5966215133667
Epoch 20, val loss: 1.9169191122055054
Epoch 30, training loss: 1.9808274507522583 = 1.89486563205719 + 0.01 * 8.596182823181152
Epoch 30, val loss: 1.8977595567703247
Epoch 40, training loss: 1.9547852277755737 = 1.8688417673110962 + 0.01 * 8.59434986114502
Epoch 40, val loss: 1.8700069189071655
Epoch 50, training loss: 1.9193328619003296 = 1.8335191011428833 + 0.01 * 8.581378936767578
Epoch 50, val loss: 1.8341503143310547
Epoch 60, training loss: 1.8806610107421875 = 1.7956175804138184 + 0.01 * 8.504339218139648
Epoch 60, val loss: 1.799119472503662
Epoch 70, training loss: 1.843746542930603 = 1.7606009244918823 + 0.01 * 8.314563751220703
Epoch 70, val loss: 1.7686933279037476
Epoch 80, training loss: 1.7945419549942017 = 1.7138409614562988 + 0.01 * 8.070103645324707
Epoch 80, val loss: 1.7269624471664429
Epoch 90, training loss: 1.726057767868042 = 1.6486244201660156 + 0.01 * 7.743335723876953
Epoch 90, val loss: 1.6685913801193237
Epoch 100, training loss: 1.6390092372894287 = 1.563198208808899 + 0.01 * 7.581098556518555
Epoch 100, val loss: 1.5929545164108276
Epoch 110, training loss: 1.540663719177246 = 1.4655405282974243 + 0.01 * 7.512314319610596
Epoch 110, val loss: 1.5087988376617432
Epoch 120, training loss: 1.437727689743042 = 1.3633458614349365 + 0.01 * 7.438183784484863
Epoch 120, val loss: 1.4225434064865112
Epoch 130, training loss: 1.3329226970672607 = 1.2591960430145264 + 0.01 * 7.372664451599121
Epoch 130, val loss: 1.3367701768875122
Epoch 140, training loss: 1.2281103134155273 = 1.1547603607177734 + 0.01 * 7.33499813079834
Epoch 140, val loss: 1.2532930374145508
Epoch 150, training loss: 1.127134084701538 = 1.0540016889572144 + 0.01 * 7.313236236572266
Epoch 150, val loss: 1.1739414930343628
Epoch 160, training loss: 1.0331882238388062 = 0.9602736830711365 + 0.01 * 7.291453838348389
Epoch 160, val loss: 1.1009451150894165
Epoch 170, training loss: 0.9473278522491455 = 0.8746301531791687 + 0.01 * 7.269769191741943
Epoch 170, val loss: 1.0347154140472412
Epoch 180, training loss: 0.8695459365844727 = 0.797034502029419 + 0.01 * 7.25114107131958
Epoch 180, val loss: 0.9753587245941162
Epoch 190, training loss: 0.7996641993522644 = 0.727283775806427 + 0.01 * 7.238041877746582
Epoch 190, val loss: 0.9230546355247498
Epoch 200, training loss: 0.737195611000061 = 0.6649104356765747 + 0.01 * 7.228518962860107
Epoch 200, val loss: 0.878112256526947
Epoch 210, training loss: 0.6812394261360168 = 0.6090196967124939 + 0.01 * 7.221972465515137
Epoch 210, val loss: 0.8402992486953735
Epoch 220, training loss: 0.630799412727356 = 0.5586466193199158 + 0.01 * 7.215281963348389
Epoch 220, val loss: 0.8095904588699341
Epoch 230, training loss: 0.5850934982299805 = 0.512993574142456 + 0.01 * 7.209990501403809
Epoch 230, val loss: 0.7855347394943237
Epoch 240, training loss: 0.543150007724762 = 0.4711248576641083 + 0.01 * 7.202512741088867
Epoch 240, val loss: 0.7670032978057861
Epoch 250, training loss: 0.5039188861846924 = 0.4319632351398468 + 0.01 * 7.195563316345215
Epoch 250, val loss: 0.7525568008422852
Epoch 260, training loss: 0.46651601791381836 = 0.3946402668952942 + 0.01 * 7.1875739097595215
Epoch 260, val loss: 0.7408159375190735
Epoch 270, training loss: 0.4305720329284668 = 0.3588014841079712 + 0.01 * 7.1770548820495605
Epoch 270, val loss: 0.7310751676559448
Epoch 280, training loss: 0.3962400555610657 = 0.3245599567890167 + 0.01 * 7.1680097579956055
Epoch 280, val loss: 0.7233685255050659
Epoch 290, training loss: 0.3636808395385742 = 0.29210761189460754 + 0.01 * 7.157324314117432
Epoch 290, val loss: 0.7177988290786743
Epoch 300, training loss: 0.33294522762298584 = 0.26149141788482666 + 0.01 * 7.145382881164551
Epoch 300, val loss: 0.714367151260376
Epoch 310, training loss: 0.30400073528289795 = 0.2326701283454895 + 0.01 * 7.133059024810791
Epoch 310, val loss: 0.7128836512565613
Epoch 320, training loss: 0.27708402276039124 = 0.20583751797676086 + 0.01 * 7.124650955200195
Epoch 320, val loss: 0.713358998298645
Epoch 330, training loss: 0.2524788975715637 = 0.1813666969537735 + 0.01 * 7.1112213134765625
Epoch 330, val loss: 0.7158836722373962
Epoch 340, training loss: 0.23071353137493134 = 0.1595505326986313 + 0.01 * 7.116300106048584
Epoch 340, val loss: 0.7205002307891846
Epoch 350, training loss: 0.21146175265312195 = 0.14049208164215088 + 0.01 * 7.0969672203063965
Epoch 350, val loss: 0.7271304726600647
Epoch 360, training loss: 0.19473615288734436 = 0.12402082979679108 + 0.01 * 7.071532726287842
Epoch 360, val loss: 0.7356777191162109
Epoch 370, training loss: 0.1807420700788498 = 0.1098737120628357 + 0.01 * 7.086835861206055
Epoch 370, val loss: 0.7458192706108093
Epoch 380, training loss: 0.16834139823913574 = 0.09772238880395889 + 0.01 * 7.061902046203613
Epoch 380, val loss: 0.7572688460350037
Epoch 390, training loss: 0.15770959854125977 = 0.08721646666526794 + 0.01 * 7.049314022064209
Epoch 390, val loss: 0.7696990966796875
Epoch 400, training loss: 0.1487741321325302 = 0.07808437198400497 + 0.01 * 7.068976402282715
Epoch 400, val loss: 0.7828523516654968
Epoch 410, training loss: 0.1405421793460846 = 0.07013750076293945 + 0.01 * 7.040467262268066
Epoch 410, val loss: 0.7963864207267761
Epoch 420, training loss: 0.13349272310733795 = 0.06318160146474838 + 0.01 * 7.0311126708984375
Epoch 420, val loss: 0.8101751804351807
Epoch 430, training loss: 0.1271834373474121 = 0.057074353098869324 + 0.01 * 7.010909557342529
Epoch 430, val loss: 0.8240153193473816
Epoch 440, training loss: 0.1218702495098114 = 0.05170221999287605 + 0.01 * 7.016802787780762
Epoch 440, val loss: 0.8378446102142334
Epoch 450, training loss: 0.1169833242893219 = 0.04696640744805336 + 0.01 * 7.001691818237305
Epoch 450, val loss: 0.85148024559021
Epoch 460, training loss: 0.11291968077421188 = 0.04277898371219635 + 0.01 * 7.0140700340271
Epoch 460, val loss: 0.8649523258209229
Epoch 470, training loss: 0.10898615419864655 = 0.039072442799806595 + 0.01 * 6.9913716316223145
Epoch 470, val loss: 0.8781558871269226
Epoch 480, training loss: 0.10548765957355499 = 0.03578469529747963 + 0.01 * 6.970296382904053
Epoch 480, val loss: 0.8910234570503235
Epoch 490, training loss: 0.10249102115631104 = 0.03286221995949745 + 0.01 * 6.962880611419678
Epoch 490, val loss: 0.903511643409729
Epoch 500, training loss: 0.09997914731502533 = 0.03025885298848152 + 0.01 * 6.972029209136963
Epoch 500, val loss: 0.9156096577644348
Epoch 510, training loss: 0.09758598357439041 = 0.027934091165661812 + 0.01 * 6.9651899337768555
Epoch 510, val loss: 0.9273256659507751
Epoch 520, training loss: 0.09549183398485184 = 0.02585235796868801 + 0.01 * 6.963947772979736
Epoch 520, val loss: 0.9386962056159973
Epoch 530, training loss: 0.09341181814670563 = 0.023987101390957832 + 0.01 * 6.942471981048584
Epoch 530, val loss: 0.9497069120407104
Epoch 540, training loss: 0.09152746945619583 = 0.022307997569441795 + 0.01 * 6.921947479248047
Epoch 540, val loss: 0.9603754281997681
Epoch 550, training loss: 0.09018723666667938 = 0.02079235017299652 + 0.01 * 6.939488887786865
Epoch 550, val loss: 0.9707241654396057
Epoch 560, training loss: 0.0885268896818161 = 0.019423408433794975 + 0.01 * 6.910347938537598
Epoch 560, val loss: 0.9807307720184326
Epoch 570, training loss: 0.08730853348970413 = 0.018181471154093742 + 0.01 * 6.91270637512207
Epoch 570, val loss: 0.9903883337974548
Epoch 580, training loss: 0.08615407347679138 = 0.01705138012766838 + 0.01 * 6.9102702140808105
Epoch 580, val loss: 0.9998152852058411
Epoch 590, training loss: 0.08514486998319626 = 0.016023658215999603 + 0.01 * 6.912121295928955
Epoch 590, val loss: 1.0089575052261353
Epoch 600, training loss: 0.08421747386455536 = 0.015087021514773369 + 0.01 * 6.913045406341553
Epoch 600, val loss: 1.0177359580993652
Epoch 610, training loss: 0.08315205574035645 = 0.014232463203370571 + 0.01 * 6.891959190368652
Epoch 610, val loss: 1.0262573957443237
Epoch 620, training loss: 0.08244684338569641 = 0.01345024909824133 + 0.01 * 6.899659156799316
Epoch 620, val loss: 1.0345712900161743
Epoch 630, training loss: 0.08150546997785568 = 0.012732536531984806 + 0.01 * 6.877293586730957
Epoch 630, val loss: 1.0425691604614258
Epoch 640, training loss: 0.08110518753528595 = 0.012071655131876469 + 0.01 * 6.903353214263916
Epoch 640, val loss: 1.0504289865493774
Epoch 650, training loss: 0.08010587096214294 = 0.011463548056781292 + 0.01 * 6.864232540130615
Epoch 650, val loss: 1.0579534769058228
Epoch 660, training loss: 0.07986701279878616 = 0.010902107693254948 + 0.01 * 6.896490573883057
Epoch 660, val loss: 1.0652602910995483
Epoch 670, training loss: 0.07912306487560272 = 0.01038333773612976 + 0.01 * 6.8739728927612305
Epoch 670, val loss: 1.0723460912704468
Epoch 680, training loss: 0.07837849110364914 = 0.009902420453727245 + 0.01 * 6.847607612609863
Epoch 680, val loss: 1.0792360305786133
Epoch 690, training loss: 0.07792365550994873 = 0.009454915300011635 + 0.01 * 6.846874237060547
Epoch 690, val loss: 1.085951805114746
Epoch 700, training loss: 0.0776141881942749 = 0.009038512594997883 + 0.01 * 6.857567310333252
Epoch 700, val loss: 1.092477560043335
Epoch 710, training loss: 0.07705042511224747 = 0.008650536648929119 + 0.01 * 6.839989185333252
Epoch 710, val loss: 1.0988026857376099
Epoch 720, training loss: 0.0767003744840622 = 0.00828730408102274 + 0.01 * 6.841307640075684
Epoch 720, val loss: 1.104976773262024
Epoch 730, training loss: 0.07637447863817215 = 0.007948780432343483 + 0.01 * 6.842569828033447
Epoch 730, val loss: 1.1109741926193237
Epoch 740, training loss: 0.07588016241788864 = 0.007632075808942318 + 0.01 * 6.824808597564697
Epoch 740, val loss: 1.1167950630187988
Epoch 750, training loss: 0.07588905096054077 = 0.00733458437025547 + 0.01 * 6.855446815490723
Epoch 750, val loss: 1.1224629878997803
Epoch 760, training loss: 0.07538667321205139 = 0.007055678404867649 + 0.01 * 6.833099842071533
Epoch 760, val loss: 1.1279579401016235
Epoch 770, training loss: 0.07503671944141388 = 0.006793426815420389 + 0.01 * 6.824329853057861
Epoch 770, val loss: 1.1333457231521606
Epoch 780, training loss: 0.07510390877723694 = 0.006546590477228165 + 0.01 * 6.855732440948486
Epoch 780, val loss: 1.138587474822998
Epoch 790, training loss: 0.07455719262361526 = 0.006313838995993137 + 0.01 * 6.82433557510376
Epoch 790, val loss: 1.1436423063278198
Epoch 800, training loss: 0.07428038120269775 = 0.006094474345445633 + 0.01 * 6.8185906410217285
Epoch 800, val loss: 1.148650050163269
Epoch 810, training loss: 0.07403997331857681 = 0.005887380801141262 + 0.01 * 6.8152594566345215
Epoch 810, val loss: 1.1535040140151978
Epoch 820, training loss: 0.07410332560539246 = 0.005692050326615572 + 0.01 * 6.841127395629883
Epoch 820, val loss: 1.158241868019104
Epoch 830, training loss: 0.0735192522406578 = 0.0055077257566154 + 0.01 * 6.801153182983398
Epoch 830, val loss: 1.162822961807251
Epoch 840, training loss: 0.07331559807062149 = 0.005333160515874624 + 0.01 * 6.798243999481201
Epoch 840, val loss: 1.1673551797866821
Epoch 850, training loss: 0.07318341732025146 = 0.005167709197849035 + 0.01 * 6.801571369171143
Epoch 850, val loss: 1.1717137098312378
Epoch 860, training loss: 0.07293408364057541 = 0.005010784603655338 + 0.01 * 6.792329788208008
Epoch 860, val loss: 1.1760361194610596
Epoch 870, training loss: 0.07301027327775955 = 0.004861968569457531 + 0.01 * 6.814830780029297
Epoch 870, val loss: 1.1802510023117065
Epoch 880, training loss: 0.0726809948682785 = 0.0047206878662109375 + 0.01 * 6.796030521392822
Epoch 880, val loss: 1.18430757522583
Epoch 890, training loss: 0.0725623145699501 = 0.004586369264870882 + 0.01 * 6.7975945472717285
Epoch 890, val loss: 1.18832266330719
Epoch 900, training loss: 0.07275289297103882 = 0.004458457697182894 + 0.01 * 6.829443454742432
Epoch 900, val loss: 1.1922279596328735
Epoch 910, training loss: 0.0722060427069664 = 0.004336721729487181 + 0.01 * 6.786932468414307
Epoch 910, val loss: 1.1960501670837402
Epoch 920, training loss: 0.07190856337547302 = 0.00422073807567358 + 0.01 * 6.768782615661621
Epoch 920, val loss: 1.1997737884521484
Epoch 930, training loss: 0.07203280925750732 = 0.004109949804842472 + 0.01 * 6.792286396026611
Epoch 930, val loss: 1.2034457921981812
Epoch 940, training loss: 0.07173271477222443 = 0.004004230722784996 + 0.01 * 6.772848129272461
Epoch 940, val loss: 1.2070049047470093
Epoch 950, training loss: 0.0716334879398346 = 0.003903241828083992 + 0.01 * 6.773025035858154
Epoch 950, val loss: 1.2104932069778442
Epoch 960, training loss: 0.07166621834039688 = 0.003806884167715907 + 0.01 * 6.785933494567871
Epoch 960, val loss: 1.2138923406600952
Epoch 970, training loss: 0.07138702273368835 = 0.003714747494086623 + 0.01 * 6.767227649688721
Epoch 970, val loss: 1.217219591140747
Epoch 980, training loss: 0.07118098437786102 = 0.003626550082117319 + 0.01 * 6.755443572998047
Epoch 980, val loss: 1.2204644680023193
Epoch 990, training loss: 0.0711105614900589 = 0.0035420237109065056 + 0.01 * 6.756854057312012
Epoch 990, val loss: 1.223650574684143
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 2.036257266998291 = 1.9502891302108765 + 0.01 * 8.596817970275879
Epoch 0, val loss: 1.9460928440093994
Epoch 10, training loss: 2.026215076446533 = 1.9402474164962769 + 0.01 * 8.596756935119629
Epoch 10, val loss: 1.9357887506484985
Epoch 20, training loss: 2.014080762863159 = 1.9281152486801147 + 0.01 * 8.596545219421387
Epoch 20, val loss: 1.9231928586959839
Epoch 30, training loss: 1.9976441860198975 = 1.9116847515106201 + 0.01 * 8.595946311950684
Epoch 30, val loss: 1.9060381650924683
Epoch 40, training loss: 1.9739669561386108 = 1.8880342245101929 + 0.01 * 8.593276977539062
Epoch 40, val loss: 1.8814574480056763
Epoch 50, training loss: 1.9401819705963135 = 1.8544176816940308 + 0.01 * 8.576427459716797
Epoch 50, val loss: 1.8475241661071777
Epoch 60, training loss: 1.8979721069335938 = 1.8130624294281006 + 0.01 * 8.490966796875
Epoch 60, val loss: 1.8089157342910767
Epoch 70, training loss: 1.856335997581482 = 1.7736505270004272 + 0.01 * 8.268543243408203
Epoch 70, val loss: 1.776437759399414
Epoch 80, training loss: 1.8115431070327759 = 1.7317190170288086 + 0.01 * 7.9824042320251465
Epoch 80, val loss: 1.7410207986831665
Epoch 90, training loss: 1.7507292032241821 = 1.674768090248108 + 0.01 * 7.596108436584473
Epoch 90, val loss: 1.6895641088485718
Epoch 100, training loss: 1.6732380390167236 = 1.5990222692489624 + 0.01 * 7.421582221984863
Epoch 100, val loss: 1.6211057901382446
Epoch 110, training loss: 1.5797441005706787 = 1.5063304901123047 + 0.01 * 7.341365337371826
Epoch 110, val loss: 1.539572834968567
Epoch 120, training loss: 1.4780848026275635 = 1.4051575660705566 + 0.01 * 7.292727470397949
Epoch 120, val loss: 1.4535460472106934
Epoch 130, training loss: 1.3735103607177734 = 1.300893783569336 + 0.01 * 7.261657238006592
Epoch 130, val loss: 1.368621587753296
Epoch 140, training loss: 1.2680562734603882 = 1.1956928968429565 + 0.01 * 7.236342906951904
Epoch 140, val loss: 1.2851864099502563
Epoch 150, training loss: 1.1638920307159424 = 1.091768503189087 + 0.01 * 7.2123565673828125
Epoch 150, val loss: 1.2037978172302246
Epoch 160, training loss: 1.063962697982788 = 0.9920315742492676 + 0.01 * 7.193108081817627
Epoch 160, val loss: 1.1257672309875488
Epoch 170, training loss: 0.969935417175293 = 0.898125410079956 + 0.01 * 7.181003093719482
Epoch 170, val loss: 1.05270254611969
Epoch 180, training loss: 0.8822537064552307 = 0.8104885220527649 + 0.01 * 7.176517486572266
Epoch 180, val loss: 0.9845482707023621
Epoch 190, training loss: 0.8008877635002136 = 0.7291792035102844 + 0.01 * 7.170857906341553
Epoch 190, val loss: 0.921768069267273
Epoch 200, training loss: 0.7262063026428223 = 0.6545324325561523 + 0.01 * 7.167385101318359
Epoch 200, val loss: 0.8652837872505188
Epoch 210, training loss: 0.6580495834350586 = 0.5864151120185852 + 0.01 * 7.163446426391602
Epoch 210, val loss: 0.8157104849815369
Epoch 220, training loss: 0.5958951711654663 = 0.5243039131164551 + 0.01 * 7.159124851226807
Epoch 220, val loss: 0.7734239101409912
Epoch 230, training loss: 0.5392110347747803 = 0.4676688611507416 + 0.01 * 7.154215335845947
Epoch 230, val loss: 0.7380343675613403
Epoch 240, training loss: 0.487589955329895 = 0.4160975515842438 + 0.01 * 7.149240970611572
Epoch 240, val loss: 0.708921492099762
Epoch 250, training loss: 0.4406898617744446 = 0.3692428469657898 + 0.01 * 7.144700527191162
Epoch 250, val loss: 0.6852279305458069
Epoch 260, training loss: 0.3981361985206604 = 0.32675841450691223 + 0.01 * 7.13778018951416
Epoch 260, val loss: 0.6658403873443604
Epoch 270, training loss: 0.35942187905311584 = 0.28812748193740845 + 0.01 * 7.129440784454346
Epoch 270, val loss: 0.6499427556991577
Epoch 280, training loss: 0.32419636845588684 = 0.2529744803905487 + 0.01 * 7.122189998626709
Epoch 280, val loss: 0.6370623707771301
Epoch 290, training loss: 0.29241490364074707 = 0.22112734615802765 + 0.01 * 7.128754615783691
Epoch 290, val loss: 0.627038836479187
Epoch 300, training loss: 0.2637641429901123 = 0.19268417358398438 + 0.01 * 7.107995986938477
Epoch 300, val loss: 0.619670569896698
Epoch 310, training loss: 0.23873570561408997 = 0.16771526634693146 + 0.01 * 7.102043628692627
Epoch 310, val loss: 0.6149164438247681
Epoch 320, training loss: 0.2170858383178711 = 0.14613983035087585 + 0.01 * 7.094601154327393
Epoch 320, val loss: 0.6126288771629333
Epoch 330, training loss: 0.198616623878479 = 0.12770676612854004 + 0.01 * 7.090986728668213
Epoch 330, val loss: 0.6126109957695007
Epoch 340, training loss: 0.18289366364479065 = 0.1120440661907196 + 0.01 * 7.084959506988525
Epoch 340, val loss: 0.6146138906478882
Epoch 350, training loss: 0.16949322819709778 = 0.0987250879406929 + 0.01 * 7.076815128326416
Epoch 350, val loss: 0.6182897686958313
Epoch 360, training loss: 0.15822114050388336 = 0.08736644685268402 + 0.01 * 7.0854692459106445
Epoch 360, val loss: 0.6232402324676514
Epoch 370, training loss: 0.1483152210712433 = 0.07764892280101776 + 0.01 * 7.0666303634643555
Epoch 370, val loss: 0.6292001008987427
Epoch 380, training loss: 0.13987141847610474 = 0.06929143518209457 + 0.01 * 7.057997703552246
Epoch 380, val loss: 0.635926365852356
Epoch 390, training loss: 0.13274405896663666 = 0.06206219270825386 + 0.01 * 7.068186283111572
Epoch 390, val loss: 0.6432137489318848
Epoch 400, training loss: 0.12629903852939606 = 0.05579173564910889 + 0.01 * 7.0507307052612305
Epoch 400, val loss: 0.6509297490119934
Epoch 410, training loss: 0.12070686370134354 = 0.0503239780664444 + 0.01 * 7.0382890701293945
Epoch 410, val loss: 0.658864438533783
Epoch 420, training loss: 0.11590567231178284 = 0.04553580284118652 + 0.01 * 7.0369873046875
Epoch 420, val loss: 0.667022168636322
Epoch 430, training loss: 0.11160662770271301 = 0.04132961854338646 + 0.01 * 7.027700424194336
Epoch 430, val loss: 0.6752885580062866
Epoch 440, training loss: 0.10779240727424622 = 0.03762345016002655 + 0.01 * 7.016895771026611
Epoch 440, val loss: 0.6836401224136353
Epoch 450, training loss: 0.10443311929702759 = 0.03434913232922554 + 0.01 * 7.008399486541748
Epoch 450, val loss: 0.6919816732406616
Epoch 460, training loss: 0.10152184963226318 = 0.031451981514692307 + 0.01 * 7.00698709487915
Epoch 460, val loss: 0.7003136873245239
Epoch 470, training loss: 0.09883839637041092 = 0.028882639482617378 + 0.01 * 6.995575904846191
Epoch 470, val loss: 0.7085460424423218
Epoch 480, training loss: 0.09653022140264511 = 0.026595309376716614 + 0.01 * 6.993491172790527
Epoch 480, val loss: 0.7166659832000732
Epoch 490, training loss: 0.0945376604795456 = 0.024554800242185593 + 0.01 * 6.998286247253418
Epoch 490, val loss: 0.7246972918510437
Epoch 500, training loss: 0.09256448596715927 = 0.022732092067599297 + 0.01 * 6.983239650726318
Epoch 500, val loss: 0.7325397729873657
Epoch 510, training loss: 0.09079207479953766 = 0.02109784260392189 + 0.01 * 6.969423294067383
Epoch 510, val loss: 0.7402589917182922
Epoch 520, training loss: 0.08918195962905884 = 0.019628984853625298 + 0.01 * 6.955297470092773
Epoch 520, val loss: 0.7477704882621765
Epoch 530, training loss: 0.08781267702579498 = 0.018305402249097824 + 0.01 * 6.950727939605713
Epoch 530, val loss: 0.7551309466362
Epoch 540, training loss: 0.08664335310459137 = 0.017108803614974022 + 0.01 * 6.953454971313477
Epoch 540, val loss: 0.7623357176780701
Epoch 550, training loss: 0.085474394261837 = 0.016025429591536522 + 0.01 * 6.944896697998047
Epoch 550, val loss: 0.7693265080451965
Epoch 560, training loss: 0.08438156545162201 = 0.015042445622384548 + 0.01 * 6.9339118003845215
Epoch 560, val loss: 0.7761481404304504
Epoch 570, training loss: 0.08338016271591187 = 0.014146555215120316 + 0.01 * 6.923360824584961
Epoch 570, val loss: 0.7827883958816528
Epoch 580, training loss: 0.08255872130393982 = 0.013328786939382553 + 0.01 * 6.9229936599731445
Epoch 580, val loss: 0.7892785668373108
Epoch 590, training loss: 0.08174162358045578 = 0.012580682523548603 + 0.01 * 6.9160943031311035
Epoch 590, val loss: 0.7955669164657593
Epoch 600, training loss: 0.08106014132499695 = 0.011894438415765762 + 0.01 * 6.916570663452148
Epoch 600, val loss: 0.8017320036888123
Epoch 610, training loss: 0.08036772161722183 = 0.011265178211033344 + 0.01 * 6.910254001617432
Epoch 610, val loss: 0.8076950311660767
Epoch 620, training loss: 0.07975573092699051 = 0.010685546323657036 + 0.01 * 6.907018661499023
Epoch 620, val loss: 0.8135306239128113
Epoch 630, training loss: 0.07914699614048004 = 0.010151473805308342 + 0.01 * 6.899552345275879
Epoch 630, val loss: 0.8191947340965271
Epoch 640, training loss: 0.0788988545536995 = 0.009657418355345726 + 0.01 * 6.9241437911987305
Epoch 640, val loss: 0.8247348070144653
Epoch 650, training loss: 0.07804502546787262 = 0.009201598353683949 + 0.01 * 6.884342670440674
Epoch 650, val loss: 0.8301051259040833
Epoch 660, training loss: 0.07757686078548431 = 0.008778447285294533 + 0.01 * 6.879841327667236
Epoch 660, val loss: 0.8353659510612488
Epoch 670, training loss: 0.0772307887673378 = 0.008384985849261284 + 0.01 * 6.884580612182617
Epoch 670, val loss: 0.8404737710952759
Epoch 680, training loss: 0.07683198899030685 = 0.00802004337310791 + 0.01 * 6.881194591522217
Epoch 680, val loss: 0.8454590439796448
Epoch 690, training loss: 0.07650594413280487 = 0.0076797278597950935 + 0.01 * 6.882621765136719
Epoch 690, val loss: 0.8503113985061646
Epoch 700, training loss: 0.07610038667917252 = 0.007362233009189367 + 0.01 * 6.873815536499023
Epoch 700, val loss: 0.8550381064414978
Epoch 710, training loss: 0.07584788650274277 = 0.007065675687044859 + 0.01 * 6.878221035003662
Epoch 710, val loss: 0.8596718311309814
Epoch 720, training loss: 0.07537917792797089 = 0.006788160186260939 + 0.01 * 6.859102249145508
Epoch 720, val loss: 0.8641490340232849
Epoch 730, training loss: 0.07520690560340881 = 0.006528174038976431 + 0.01 * 6.867872714996338
Epoch 730, val loss: 0.8685458898544312
Epoch 740, training loss: 0.07496128231287003 = 0.006283964496105909 + 0.01 * 6.867732524871826
Epoch 740, val loss: 0.8728183507919312
Epoch 750, training loss: 0.07455532252788544 = 0.006055261008441448 + 0.01 * 6.850006580352783
Epoch 750, val loss: 0.8770049214363098
Epoch 760, training loss: 0.07446768879890442 = 0.0058397892862558365 + 0.01 * 6.862790107727051
Epoch 760, val loss: 0.8810946941375732
Epoch 770, training loss: 0.07410380989313126 = 0.0056371227838099 + 0.01 * 6.846668720245361
Epoch 770, val loss: 0.88505619764328
Epoch 780, training loss: 0.07416219264268875 = 0.005445980932563543 + 0.01 * 6.871621608734131
Epoch 780, val loss: 0.8889725208282471
Epoch 790, training loss: 0.0737164169549942 = 0.005265632178634405 + 0.01 * 6.845078468322754
Epoch 790, val loss: 0.8927304744720459
Epoch 800, training loss: 0.07342304289340973 = 0.005095231346786022 + 0.01 * 6.8327813148498535
Epoch 800, val loss: 0.8964489698410034
Epoch 810, training loss: 0.07327744364738464 = 0.004933982156217098 + 0.01 * 6.834346294403076
Epoch 810, val loss: 0.900088369846344
Epoch 820, training loss: 0.07301642000675201 = 0.004781555384397507 + 0.01 * 6.823486328125
Epoch 820, val loss: 0.9035999774932861
Epoch 830, training loss: 0.07293731719255447 = 0.004636770114302635 + 0.01 * 6.830054759979248
Epoch 830, val loss: 0.9070548415184021
Epoch 840, training loss: 0.07281436026096344 = 0.004500115755945444 + 0.01 * 6.831425189971924
Epoch 840, val loss: 0.9104242920875549
Epoch 850, training loss: 0.07259509712457657 = 0.004369825124740601 + 0.01 * 6.8225274085998535
Epoch 850, val loss: 0.9137334227561951
Epoch 860, training loss: 0.07271190732717514 = 0.00424632104113698 + 0.01 * 6.846558570861816
Epoch 860, val loss: 0.9169355630874634
Epoch 870, training loss: 0.07225115597248077 = 0.004128769040107727 + 0.01 * 6.812238693237305
Epoch 870, val loss: 0.9200891852378845
Epoch 880, training loss: 0.07244167476892471 = 0.00401691347360611 + 0.01 * 6.8424763679504395
Epoch 880, val loss: 0.9232082366943359
Epoch 890, training loss: 0.0719863772392273 = 0.003910294733941555 + 0.01 * 6.807608604431152
Epoch 890, val loss: 0.9261963367462158
Epoch 900, training loss: 0.07182859629392624 = 0.0038086173590272665 + 0.01 * 6.801997661590576
Epoch 900, val loss: 0.9291602373123169
Epoch 910, training loss: 0.07171596586704254 = 0.003711820812895894 + 0.01 * 6.800414562225342
Epoch 910, val loss: 0.9320255517959595
Epoch 920, training loss: 0.07165666669607162 = 0.0036190063692629337 + 0.01 * 6.803765773773193
Epoch 920, val loss: 0.9349055290222168
Epoch 930, training loss: 0.07153386622667313 = 0.0035305400379002094 + 0.01 * 6.800333023071289
Epoch 930, val loss: 0.9376733303070068
Epoch 940, training loss: 0.07159802317619324 = 0.003445893991738558 + 0.01 * 6.815213203430176
Epoch 940, val loss: 0.9403737187385559
Epoch 950, training loss: 0.07135798037052155 = 0.0033650819677859545 + 0.01 * 6.799289703369141
Epoch 950, val loss: 0.943045437335968
Epoch 960, training loss: 0.07140171527862549 = 0.0032874702010303736 + 0.01 * 6.811424255371094
Epoch 960, val loss: 0.9456425309181213
Epoch 970, training loss: 0.07114756852388382 = 0.0032134440261870623 + 0.01 * 6.793412685394287
Epoch 970, val loss: 0.9482150673866272
Epoch 980, training loss: 0.07097998261451721 = 0.003142328467220068 + 0.01 * 6.78376579284668
Epoch 980, val loss: 0.950706422328949
Epoch 990, training loss: 0.0710524171590805 = 0.0030740397050976753 + 0.01 * 6.797837734222412
Epoch 990, val loss: 0.9531815052032471
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.041945695877075 = 1.9559779167175293 + 0.01 * 8.596783638000488
Epoch 0, val loss: 1.9549981355667114
Epoch 10, training loss: 2.031627893447876 = 1.9456607103347778 + 0.01 * 8.596720695495605
Epoch 10, val loss: 1.9452368021011353
Epoch 20, training loss: 2.019327402114868 = 1.9333624839782715 + 0.01 * 8.5964994430542
Epoch 20, val loss: 1.9332666397094727
Epoch 30, training loss: 2.0028250217437744 = 1.9168665409088135 + 0.01 * 8.59584903717041
Epoch 30, val loss: 1.9168319702148438
Epoch 40, training loss: 1.9790537357330322 = 1.8931277990341187 + 0.01 * 8.592597007751465
Epoch 40, val loss: 1.892948031425476
Epoch 50, training loss: 1.9448386430740356 = 1.8591344356536865 + 0.01 * 8.570420265197754
Epoch 50, val loss: 1.8595367670059204
Epoch 60, training loss: 1.901561975479126 = 1.8169374465942383 + 0.01 * 8.462458610534668
Epoch 60, val loss: 1.8208072185516357
Epoch 70, training loss: 1.8597230911254883 = 1.7766790390014648 + 0.01 * 8.30440902709961
Epoch 70, val loss: 1.7876940965652466
Epoch 80, training loss: 1.81588613986969 = 1.7356547117233276 + 0.01 * 8.02314281463623
Epoch 80, val loss: 1.7520685195922852
Epoch 90, training loss: 1.7562493085861206 = 1.6804072856903076 + 0.01 * 7.584201812744141
Epoch 90, val loss: 1.7010068893432617
Epoch 100, training loss: 1.6804826259613037 = 1.6065459251403809 + 0.01 * 7.3936686515808105
Epoch 100, val loss: 1.635583519935608
Epoch 110, training loss: 1.5877101421356201 = 1.514399766921997 + 0.01 * 7.3310394287109375
Epoch 110, val loss: 1.5581518411636353
Epoch 120, training loss: 1.485968828201294 = 1.4130746126174927 + 0.01 * 7.289418697357178
Epoch 120, val loss: 1.4744199514389038
Epoch 130, training loss: 1.3822680711746216 = 1.3096771240234375 + 0.01 * 7.259100437164307
Epoch 130, val loss: 1.3899157047271729
Epoch 140, training loss: 1.2787327766418457 = 1.2063966989517212 + 0.01 * 7.233606815338135
Epoch 140, val loss: 1.305773138999939
Epoch 150, training loss: 1.1757378578186035 = 1.1036444902420044 + 0.01 * 7.209333896636963
Epoch 150, val loss: 1.2223821878433228
Epoch 160, training loss: 1.0748867988586426 = 1.0029911994934082 + 0.01 * 7.189563274383545
Epoch 160, val loss: 1.1407204866409302
Epoch 170, training loss: 0.9781140089035034 = 0.9063251614570618 + 0.01 * 7.178885459899902
Epoch 170, val loss: 1.0624675750732422
Epoch 180, training loss: 0.8870606422424316 = 0.8153905272483826 + 0.01 * 7.1670122146606445
Epoch 180, val loss: 0.9893583059310913
Epoch 190, training loss: 0.8033086061477661 = 0.7317217588424683 + 0.01 * 7.158684730529785
Epoch 190, val loss: 0.9239644408226013
Epoch 200, training loss: 0.7275998592376709 = 0.6560847759246826 + 0.01 * 7.151505470275879
Epoch 200, val loss: 0.8681346774101257
Epoch 210, training loss: 0.6593260169029236 = 0.5879075527191162 + 0.01 * 7.141846656799316
Epoch 210, val loss: 0.8220457434654236
Epoch 220, training loss: 0.5971892476081848 = 0.5258651971817017 + 0.01 * 7.132405757904053
Epoch 220, val loss: 0.7844090461730957
Epoch 230, training loss: 0.5400760173797607 = 0.46880999207496643 + 0.01 * 7.126604080200195
Epoch 230, val loss: 0.753600537776947
Epoch 240, training loss: 0.48711973428726196 = 0.41591742634773254 + 0.01 * 7.120229721069336
Epoch 240, val loss: 0.7281191945075989
Epoch 250, training loss: 0.43787339329719543 = 0.3667466342449188 + 0.01 * 7.112675189971924
Epoch 250, val loss: 0.7070558667182922
Epoch 260, training loss: 0.3923742175102234 = 0.3213004767894745 + 0.01 * 7.107373237609863
Epoch 260, val loss: 0.6901416182518005
Epoch 270, training loss: 0.3509398102760315 = 0.2798476219177246 + 0.01 * 7.109219074249268
Epoch 270, val loss: 0.6772756576538086
Epoch 280, training loss: 0.3136216402053833 = 0.24259807169437408 + 0.01 * 7.102358818054199
Epoch 280, val loss: 0.6685123443603516
Epoch 290, training loss: 0.28047171235084534 = 0.2095222771167755 + 0.01 * 7.094944000244141
Epoch 290, val loss: 0.6638003587722778
Epoch 300, training loss: 0.25147587060928345 = 0.1805621087551117 + 0.01 * 7.09137487411499
Epoch 300, val loss: 0.6628351211547852
Epoch 310, training loss: 0.2265331894159317 = 0.1555580049753189 + 0.01 * 7.0975189208984375
Epoch 310, val loss: 0.6653470993041992
Epoch 320, training loss: 0.20508408546447754 = 0.13424202799797058 + 0.01 * 7.084205150604248
Epoch 320, val loss: 0.6708736419677734
Epoch 330, training loss: 0.18705376982688904 = 0.11622143536806107 + 0.01 * 7.08323335647583
Epoch 330, val loss: 0.6788755655288696
Epoch 340, training loss: 0.17180337011814117 = 0.10101664811372757 + 0.01 * 7.078672409057617
Epoch 340, val loss: 0.6889315843582153
Epoch 350, training loss: 0.15900683403015137 = 0.08817692846059799 + 0.01 * 7.082991600036621
Epoch 350, val loss: 0.7005620002746582
Epoch 360, training loss: 0.14805850386619568 = 0.07731369882822037 + 0.01 * 7.074481010437012
Epoch 360, val loss: 0.7132348418235779
Epoch 370, training loss: 0.1387435793876648 = 0.06807912886142731 + 0.01 * 7.066446304321289
Epoch 370, val loss: 0.7267132997512817
Epoch 380, training loss: 0.1308026909828186 = 0.06020282581448555 + 0.01 * 7.059986114501953
Epoch 380, val loss: 0.7407105565071106
Epoch 390, training loss: 0.12408130615949631 = 0.053466618061065674 + 0.01 * 7.061469078063965
Epoch 390, val loss: 0.7550897598266602
Epoch 400, training loss: 0.11824771761894226 = 0.04769119247794151 + 0.01 * 7.055652141571045
Epoch 400, val loss: 0.7695353627204895
Epoch 410, training loss: 0.11316601932048798 = 0.042723268270492554 + 0.01 * 7.044275283813477
Epoch 410, val loss: 0.7839962244033813
Epoch 420, training loss: 0.10892251133918762 = 0.03843841701745987 + 0.01 * 7.048409461975098
Epoch 420, val loss: 0.7983034253120422
Epoch 430, training loss: 0.10504946112632751 = 0.034731484949588776 + 0.01 * 7.031797885894775
Epoch 430, val loss: 0.812378466129303
Epoch 440, training loss: 0.10176288336515427 = 0.03151002526283264 + 0.01 * 7.025285720825195
Epoch 440, val loss: 0.8261251449584961
Epoch 450, training loss: 0.09888862818479538 = 0.028701001778244972 + 0.01 * 7.018763065338135
Epoch 450, val loss: 0.8395691514015198
Epoch 460, training loss: 0.09651176631450653 = 0.026243124157190323 + 0.01 * 7.026864528656006
Epoch 460, val loss: 0.8526148200035095
Epoch 470, training loss: 0.09418915212154388 = 0.024083752185106277 + 0.01 * 7.010540008544922
Epoch 470, val loss: 0.8652760982513428
Epoch 480, training loss: 0.09230267256498337 = 0.022176727652549744 + 0.01 * 7.012594699859619
Epoch 480, val loss: 0.8775519728660583
Epoch 490, training loss: 0.09042339026927948 = 0.020487556234002113 + 0.01 * 6.993583679199219
Epoch 490, val loss: 0.8894816637039185
Epoch 500, training loss: 0.08888547867536545 = 0.018984578549861908 + 0.01 * 6.9900898933410645
Epoch 500, val loss: 0.9010274410247803
Epoch 510, training loss: 0.08752819895744324 = 0.017642199993133545 + 0.01 * 6.98859977722168
Epoch 510, val loss: 0.9122015833854675
Epoch 520, training loss: 0.08613758534193039 = 0.01643972657620907 + 0.01 * 6.969785690307617
Epoch 520, val loss: 0.9230652451515198
Epoch 530, training loss: 0.08497253060340881 = 0.015358787029981613 + 0.01 * 6.961374282836914
Epoch 530, val loss: 0.9335727095603943
Epoch 540, training loss: 0.08406952768564224 = 0.014383593574166298 + 0.01 * 6.968594074249268
Epoch 540, val loss: 0.9437159299850464
Epoch 550, training loss: 0.08303359895944595 = 0.01350226067006588 + 0.01 * 6.953133583068848
Epoch 550, val loss: 0.9535945057868958
Epoch 560, training loss: 0.08210405707359314 = 0.012702555395662785 + 0.01 * 6.940150260925293
Epoch 560, val loss: 0.9631338715553284
Epoch 570, training loss: 0.08159294724464417 = 0.011974675580859184 + 0.01 * 6.961827278137207
Epoch 570, val loss: 0.97240149974823
Epoch 580, training loss: 0.08058412373065948 = 0.01131152082234621 + 0.01 * 6.927260398864746
Epoch 580, val loss: 0.9812977910041809
Epoch 590, training loss: 0.07989544421434402 = 0.010704620741307735 + 0.01 * 6.919082164764404
Epoch 590, val loss: 0.9900192618370056
Epoch 600, training loss: 0.07943219691514969 = 0.01014753244817257 + 0.01 * 6.928466320037842
Epoch 600, val loss: 0.9984655380249023
Epoch 610, training loss: 0.07877545058727264 = 0.009635361842811108 + 0.01 * 6.914009094238281
Epoch 610, val loss: 1.0066457986831665
Epoch 620, training loss: 0.07838533073663712 = 0.00916347000747919 + 0.01 * 6.922186374664307
Epoch 620, val loss: 1.0146455764770508
Epoch 630, training loss: 0.0777381882071495 = 0.008728357031941414 + 0.01 * 6.9009833335876465
Epoch 630, val loss: 1.0223335027694702
Epoch 640, training loss: 0.07726768404245377 = 0.008325980044901371 + 0.01 * 6.89417028427124
Epoch 640, val loss: 1.0298539400100708
Epoch 650, training loss: 0.07695572078227997 = 0.007952681742608547 + 0.01 * 6.900303840637207
Epoch 650, val loss: 1.037201166152954
Epoch 660, training loss: 0.07646515220403671 = 0.007606193423271179 + 0.01 * 6.8858962059021
Epoch 660, val loss: 1.0442912578582764
Epoch 670, training loss: 0.07617326825857162 = 0.007284139283001423 + 0.01 * 6.888912677764893
Epoch 670, val loss: 1.0511786937713623
Epoch 680, training loss: 0.07577211409807205 = 0.006983849685639143 + 0.01 * 6.87882661819458
Epoch 680, val loss: 1.0578995943069458
Epoch 690, training loss: 0.07545354217290878 = 0.006703660357743502 + 0.01 * 6.874988555908203
Epoch 690, val loss: 1.064488410949707
Epoch 700, training loss: 0.0750892162322998 = 0.006441709119826555 + 0.01 * 6.864750862121582
Epoch 700, val loss: 1.0708163976669312
Epoch 710, training loss: 0.07499229162931442 = 0.006196556147187948 + 0.01 * 6.879573822021484
Epoch 710, val loss: 1.0770901441574097
Epoch 720, training loss: 0.07463433593511581 = 0.00596685241907835 + 0.01 * 6.866748809814453
Epoch 720, val loss: 1.0831058025360107
Epoch 730, training loss: 0.07432456314563751 = 0.005751186516135931 + 0.01 * 6.857337951660156
Epoch 730, val loss: 1.0890700817108154
Epoch 740, training loss: 0.07419656217098236 = 0.005548328161239624 + 0.01 * 6.864823341369629
Epoch 740, val loss: 1.0947827100753784
Epoch 750, training loss: 0.07388495653867722 = 0.005357418674975634 + 0.01 * 6.852753639221191
Epoch 750, val loss: 1.1004432439804077
Epoch 760, training loss: 0.07360543310642242 = 0.005177443381398916 + 0.01 * 6.842799186706543
Epoch 760, val loss: 1.1059858798980713
Epoch 770, training loss: 0.07347072660923004 = 0.0050076269544661045 + 0.01 * 6.846310138702393
Epoch 770, val loss: 1.1113548278808594
Epoch 780, training loss: 0.07330738008022308 = 0.00484752282500267 + 0.01 * 6.845986366271973
Epoch 780, val loss: 1.1165635585784912
Epoch 790, training loss: 0.07330179214477539 = 0.004695888143032789 + 0.01 * 6.860590934753418
Epoch 790, val loss: 1.1217292547225952
Epoch 800, training loss: 0.07291659712791443 = 0.004552554339170456 + 0.01 * 6.836403846740723
Epoch 800, val loss: 1.1267329454421997
Epoch 810, training loss: 0.07264814525842667 = 0.004416653420776129 + 0.01 * 6.823149681091309
Epoch 810, val loss: 1.131603479385376
Epoch 820, training loss: 0.07260550558567047 = 0.004287635441869497 + 0.01 * 6.831787109375
Epoch 820, val loss: 1.1364248991012573
Epoch 830, training loss: 0.07234948873519897 = 0.004165082238614559 + 0.01 * 6.818440914154053
Epoch 830, val loss: 1.1411058902740479
Epoch 840, training loss: 0.0724768415093422 = 0.004048518370836973 + 0.01 * 6.842832565307617
Epoch 840, val loss: 1.1457480192184448
Epoch 850, training loss: 0.07202225178480148 = 0.0039379894733428955 + 0.01 * 6.808426380157471
Epoch 850, val loss: 1.1501845121383667
Epoch 860, training loss: 0.07191949337720871 = 0.0038326645735651255 + 0.01 * 6.808682441711426
Epoch 860, val loss: 1.1545432806015015
Epoch 870, training loss: 0.071769118309021 = 0.0037322829011827707 + 0.01 * 6.803683280944824
Epoch 870, val loss: 1.1588895320892334
Epoch 880, training loss: 0.07168690860271454 = 0.0036368188448250294 + 0.01 * 6.805008888244629
Epoch 880, val loss: 1.1629281044006348
Epoch 890, training loss: 0.07177986204624176 = 0.0035454537719488144 + 0.01 * 6.8234405517578125
Epoch 890, val loss: 1.1670171022415161
Epoch 900, training loss: 0.07147926092147827 = 0.0034583033993840218 + 0.01 * 6.802095413208008
Epoch 900, val loss: 1.1710383892059326
Epoch 910, training loss: 0.07150253653526306 = 0.003374934894964099 + 0.01 * 6.812760353088379
Epoch 910, val loss: 1.1750248670578003
Epoch 920, training loss: 0.07126062363386154 = 0.0032954213675111532 + 0.01 * 6.796520233154297
Epoch 920, val loss: 1.1788344383239746
Epoch 930, training loss: 0.07110070437192917 = 0.0032192219514399767 + 0.01 * 6.788147926330566
Epoch 930, val loss: 1.182652473449707
Epoch 940, training loss: 0.07106252014636993 = 0.0031462351325899363 + 0.01 * 6.791628360748291
Epoch 940, val loss: 1.1863514184951782
Epoch 950, training loss: 0.07109120488166809 = 0.0030762820970267057 + 0.01 * 6.801492214202881
Epoch 950, val loss: 1.1900181770324707
Epoch 960, training loss: 0.07075750082731247 = 0.0030092813540250063 + 0.01 * 6.774822235107422
Epoch 960, val loss: 1.1935756206512451
Epoch 970, training loss: 0.07088294625282288 = 0.0029449970461428165 + 0.01 * 6.793795108795166
Epoch 970, val loss: 1.1970356702804565
Epoch 980, training loss: 0.07068981230258942 = 0.002883277600631118 + 0.01 * 6.780653953552246
Epoch 980, val loss: 1.2004578113555908
Epoch 990, training loss: 0.07063842564821243 = 0.002824070630595088 + 0.01 * 6.781435966491699
Epoch 990, val loss: 1.2037887573242188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8392198207696363
The final CL Acc:0.82716, 0.00924, The final GNN Acc:0.83904, 0.00194
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10534])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0374412536621094 = 1.9514731168746948 + 0.01 * 8.596819877624512
Epoch 0, val loss: 1.9540737867355347
Epoch 10, training loss: 2.028104782104492 = 1.9421371221542358 + 0.01 * 8.596763610839844
Epoch 10, val loss: 1.9449397325515747
Epoch 20, training loss: 2.016862392425537 = 1.930896520614624 + 0.01 * 8.596583366394043
Epoch 20, val loss: 1.9335424900054932
Epoch 30, training loss: 2.0014398097991943 = 1.9154785871505737 + 0.01 * 8.5961275100708
Epoch 30, val loss: 1.9174010753631592
Epoch 40, training loss: 1.9787439107894897 = 1.89280104637146 + 0.01 * 8.594290733337402
Epoch 40, val loss: 1.8935002088546753
Epoch 50, training loss: 1.9461519718170166 = 1.8603322505950928 + 0.01 * 8.58197021484375
Epoch 50, val loss: 1.8602238893508911
Epoch 60, training loss: 1.9066028594970703 = 1.8214695453643799 + 0.01 * 8.513335227966309
Epoch 60, val loss: 1.8235423564910889
Epoch 70, training loss: 1.8725336790084839 = 1.789207935333252 + 0.01 * 8.332576751708984
Epoch 70, val loss: 1.796433925628662
Epoch 80, training loss: 1.837813377380371 = 1.757209062576294 + 0.01 * 8.060432434082031
Epoch 80, val loss: 1.7681325674057007
Epoch 90, training loss: 1.7892400026321411 = 1.712326169013977 + 0.01 * 7.691380023956299
Epoch 90, val loss: 1.7294820547103882
Epoch 100, training loss: 1.7248218059539795 = 1.6499488353729248 + 0.01 * 7.487292289733887
Epoch 100, val loss: 1.677698016166687
Epoch 110, training loss: 1.6420868635177612 = 1.5681347846984863 + 0.01 * 7.395208835601807
Epoch 110, val loss: 1.611108422279358
Epoch 120, training loss: 1.5476151704788208 = 1.474181056022644 + 0.01 * 7.343414306640625
Epoch 120, val loss: 1.5348008871078491
Epoch 130, training loss: 1.4492151737213135 = 1.3760915994644165 + 0.01 * 7.312352180480957
Epoch 130, val loss: 1.4564604759216309
Epoch 140, training loss: 1.3508117198944092 = 1.2779076099395752 + 0.01 * 7.290413856506348
Epoch 140, val loss: 1.3801316022872925
Epoch 150, training loss: 1.2527921199798584 = 1.1801012754440308 + 0.01 * 7.2690839767456055
Epoch 150, val loss: 1.3055055141448975
Epoch 160, training loss: 1.1572831869125366 = 1.0847887992858887 + 0.01 * 7.249443054199219
Epoch 160, val loss: 1.235098958015442
Epoch 170, training loss: 1.0670037269592285 = 0.9946085810661316 + 0.01 * 7.239518642425537
Epoch 170, val loss: 1.1704065799713135
Epoch 180, training loss: 0.983113706111908 = 0.9107997417449951 + 0.01 * 7.231399059295654
Epoch 180, val loss: 1.111869215965271
Epoch 190, training loss: 0.9055920839309692 = 0.8333717584609985 + 0.01 * 7.222034931182861
Epoch 190, val loss: 1.0590739250183105
Epoch 200, training loss: 0.834777295589447 = 0.7626301050186157 + 0.01 * 7.214718341827393
Epoch 200, val loss: 1.0124285221099854
Epoch 210, training loss: 0.7708257436752319 = 0.6987578272819519 + 0.01 * 7.206793785095215
Epoch 210, val loss: 0.9730034470558167
Epoch 220, training loss: 0.7129225134849548 = 0.6409408450126648 + 0.01 * 7.198165416717529
Epoch 220, val loss: 0.9407224655151367
Epoch 230, training loss: 0.6594566106796265 = 0.5875617861747742 + 0.01 * 7.189480304718018
Epoch 230, val loss: 0.9147158861160278
Epoch 240, training loss: 0.609169602394104 = 0.5373431444168091 + 0.01 * 7.182648658752441
Epoch 240, val loss: 0.8937298059463501
Epoch 250, training loss: 0.5615807175636292 = 0.4898272752761841 + 0.01 * 7.175342559814453
Epoch 250, val loss: 0.8766477108001709
Epoch 260, training loss: 0.5168200135231018 = 0.4451633393764496 + 0.01 * 7.165665149688721
Epoch 260, val loss: 0.86322420835495
Epoch 270, training loss: 0.47541701793670654 = 0.40380948781967163 + 0.01 * 7.160754680633545
Epoch 270, val loss: 0.853807806968689
Epoch 280, training loss: 0.4374502897262573 = 0.36591336131095886 + 0.01 * 7.153692245483398
Epoch 280, val loss: 0.8490158319473267
Epoch 290, training loss: 0.40236932039260864 = 0.33097657561302185 + 0.01 * 7.139276027679443
Epoch 290, val loss: 0.8484262824058533
Epoch 300, training loss: 0.3697894215583801 = 0.2983258366584778 + 0.01 * 7.146359443664551
Epoch 300, val loss: 0.8515178561210632
Epoch 310, training loss: 0.3387036621570587 = 0.26739701628685 + 0.01 * 7.130664348602295
Epoch 310, val loss: 0.857700526714325
Epoch 320, training loss: 0.30911752581596375 = 0.23793146014213562 + 0.01 * 7.1186065673828125
Epoch 320, val loss: 0.8665865063667297
Epoch 330, training loss: 0.28130343556404114 = 0.21011535823345184 + 0.01 * 7.118807315826416
Epoch 330, val loss: 0.8776835203170776
Epoch 340, training loss: 0.25548484921455383 = 0.18441347777843475 + 0.01 * 7.107137680053711
Epoch 340, val loss: 0.8909386396408081
Epoch 350, training loss: 0.23212388157844543 = 0.16120067238807678 + 0.01 * 7.092321872711182
Epoch 350, val loss: 0.9058574438095093
Epoch 360, training loss: 0.2115994393825531 = 0.14064963161945343 + 0.01 * 7.0949811935424805
Epoch 360, val loss: 0.9222688674926758
Epoch 370, training loss: 0.19355155527591705 = 0.12275137007236481 + 0.01 * 7.080018997192383
Epoch 370, val loss: 0.9398754239082336
Epoch 380, training loss: 0.17807507514953613 = 0.10728137940168381 + 0.01 * 7.07936954498291
Epoch 380, val loss: 0.9582582712173462
Epoch 390, training loss: 0.16461360454559326 = 0.09396708756685257 + 0.01 * 7.064652919769287
Epoch 390, val loss: 0.9772576093673706
Epoch 400, training loss: 0.15308578312397003 = 0.08253742754459381 + 0.01 * 7.054835796356201
Epoch 400, val loss: 0.99668949842453
Epoch 410, training loss: 0.14335618913173676 = 0.07274802774190903 + 0.01 * 7.060816287994385
Epoch 410, val loss: 1.0161857604980469
Epoch 420, training loss: 0.13480761647224426 = 0.0643656998872757 + 0.01 * 7.044191837310791
Epoch 420, val loss: 1.0356851816177368
Epoch 430, training loss: 0.1275816261768341 = 0.05717053264379501 + 0.01 * 7.04110860824585
Epoch 430, val loss: 1.0549389123916626
Epoch 440, training loss: 0.12126535177230835 = 0.050988856703042984 + 0.01 * 7.027649402618408
Epoch 440, val loss: 1.0739002227783203
Epoch 450, training loss: 0.11577780544757843 = 0.045660898089408875 + 0.01 * 7.011691093444824
Epoch 450, val loss: 1.0924451351165771
Epoch 460, training loss: 0.11138194799423218 = 0.041057221591472626 + 0.01 * 7.032473087310791
Epoch 460, val loss: 1.1103352308273315
Epoch 470, training loss: 0.10719475895166397 = 0.03707347810268402 + 0.01 * 7.0121283531188965
Epoch 470, val loss: 1.127658486366272
Epoch 480, training loss: 0.10360521823167801 = 0.033609822392463684 + 0.01 * 6.999539852142334
Epoch 480, val loss: 1.1443204879760742
Epoch 490, training loss: 0.10039716958999634 = 0.03058702126145363 + 0.01 * 6.981015205383301
Epoch 490, val loss: 1.1602827310562134
Epoch 500, training loss: 0.09802751988172531 = 0.027935758233070374 + 0.01 * 7.009176254272461
Epoch 500, val loss: 1.175631046295166
Epoch 510, training loss: 0.09540235996246338 = 0.025608371943235397 + 0.01 * 6.979398250579834
Epoch 510, val loss: 1.190285086631775
Epoch 520, training loss: 0.09318947792053223 = 0.02355475351214409 + 0.01 * 6.963473320007324
Epoch 520, val loss: 1.2042776346206665
Epoch 530, training loss: 0.09149282425642014 = 0.021733103320002556 + 0.01 * 6.975971698760986
Epoch 530, val loss: 1.2177079916000366
Epoch 540, training loss: 0.08977462351322174 = 0.02011282555758953 + 0.01 * 6.966179847717285
Epoch 540, val loss: 1.2305221557617188
Epoch 550, training loss: 0.08816681802272797 = 0.018663208931684494 + 0.01 * 6.950361251831055
Epoch 550, val loss: 1.2428257465362549
Epoch 560, training loss: 0.0868421420454979 = 0.01735958456993103 + 0.01 * 6.94825553894043
Epoch 560, val loss: 1.2546247243881226
Epoch 570, training loss: 0.08548463881015778 = 0.01618608459830284 + 0.01 * 6.929856300354004
Epoch 570, val loss: 1.2659831047058105
Epoch 580, training loss: 0.08460477739572525 = 0.015127074904739857 + 0.01 * 6.947770118713379
Epoch 580, val loss: 1.2769619226455688
Epoch 590, training loss: 0.08338413387537003 = 0.014166059903800488 + 0.01 * 6.921807765960693
Epoch 590, val loss: 1.2875707149505615
Epoch 600, training loss: 0.08267021924257278 = 0.013290769420564175 + 0.01 * 6.937945365905762
Epoch 600, val loss: 1.297809362411499
Epoch 610, training loss: 0.08174033463001251 = 0.012492259033024311 + 0.01 * 6.924807548522949
Epoch 610, val loss: 1.3077641725540161
Epoch 620, training loss: 0.08096245676279068 = 0.011763080954551697 + 0.01 * 6.919937610626221
Epoch 620, val loss: 1.3174717426300049
Epoch 630, training loss: 0.08003830909729004 = 0.011094365268945694 + 0.01 * 6.8943939208984375
Epoch 630, val loss: 1.3268133401870728
Epoch 640, training loss: 0.07957800477743149 = 0.010478436946868896 + 0.01 * 6.909956932067871
Epoch 640, val loss: 1.335997462272644
Epoch 650, training loss: 0.07901819795370102 = 0.009913190267980099 + 0.01 * 6.910501003265381
Epoch 650, val loss: 1.3448164463043213
Epoch 660, training loss: 0.07819405198097229 = 0.009393229149281979 + 0.01 * 6.880082607269287
Epoch 660, val loss: 1.353386640548706
Epoch 670, training loss: 0.07798445224761963 = 0.008913247846066952 + 0.01 * 6.907120227813721
Epoch 670, val loss: 1.3618147373199463
Epoch 680, training loss: 0.07725052535533905 = 0.008470929227769375 + 0.01 * 6.877959728240967
Epoch 680, val loss: 1.3699114322662354
Epoch 690, training loss: 0.07675624638795853 = 0.00806171540170908 + 0.01 * 6.869453430175781
Epoch 690, val loss: 1.3778637647628784
Epoch 700, training loss: 0.07637526094913483 = 0.007682178169488907 + 0.01 * 6.8693084716796875
Epoch 700, val loss: 1.3855210542678833
Epoch 710, training loss: 0.0759650319814682 = 0.007330259773880243 + 0.01 * 6.8634772300720215
Epoch 710, val loss: 1.3930234909057617
Epoch 720, training loss: 0.07574347406625748 = 0.007002760656177998 + 0.01 * 6.87407112121582
Epoch 720, val loss: 1.4003276824951172
Epoch 730, training loss: 0.07539618760347366 = 0.006697900593280792 + 0.01 * 6.869828701019287
Epoch 730, val loss: 1.407384991645813
Epoch 740, training loss: 0.07531043887138367 = 0.006414027418941259 + 0.01 * 6.889641284942627
Epoch 740, val loss: 1.4143126010894775
Epoch 750, training loss: 0.07465748488903046 = 0.006148926913738251 + 0.01 * 6.850856304168701
Epoch 750, val loss: 1.4210368394851685
Epoch 760, training loss: 0.0744096040725708 = 0.005901563912630081 + 0.01 * 6.850803852081299
Epoch 760, val loss: 1.4276624917984009
Epoch 770, training loss: 0.07437413930892944 = 0.00567026948556304 + 0.01 * 6.870386600494385
Epoch 770, val loss: 1.434035062789917
Epoch 780, training loss: 0.07390055060386658 = 0.005453804973512888 + 0.01 * 6.844674587249756
Epoch 780, val loss: 1.4401586055755615
Epoch 790, training loss: 0.07353425025939941 = 0.005251155234873295 + 0.01 * 6.828309535980225
Epoch 790, val loss: 1.4462898969650269
Epoch 800, training loss: 0.07355969399213791 = 0.005060993600636721 + 0.01 * 6.849870204925537
Epoch 800, val loss: 1.4521888494491577
Epoch 810, training loss: 0.07366965711116791 = 0.004882193636149168 + 0.01 * 6.878746509552002
Epoch 810, val loss: 1.4578638076782227
Epoch 820, training loss: 0.07327765971422195 = 0.004714409355074167 + 0.01 * 6.856325626373291
Epoch 820, val loss: 1.4632773399353027
Epoch 830, training loss: 0.07272177189588547 = 0.004556372761726379 + 0.01 * 6.816539764404297
Epoch 830, val loss: 1.4686776399612427
Epoch 840, training loss: 0.0724862813949585 = 0.004406999330967665 + 0.01 * 6.807928562164307
Epoch 840, val loss: 1.4739441871643066
Epoch 850, training loss: 0.07229803502559662 = 0.0042661260813474655 + 0.01 * 6.8031907081604
Epoch 850, val loss: 1.4790457487106323
Epoch 860, training loss: 0.07216347754001617 = 0.004132801666855812 + 0.01 * 6.803068161010742
Epoch 860, val loss: 1.4840497970581055
Epoch 870, training loss: 0.07221774011850357 = 0.004006814677268267 + 0.01 * 6.821092128753662
Epoch 870, val loss: 1.4888896942138672
Epoch 880, training loss: 0.07217561453580856 = 0.003887154394760728 + 0.01 * 6.828845977783203
Epoch 880, val loss: 1.4935276508331299
Epoch 890, training loss: 0.0717850923538208 = 0.0037741479463875294 + 0.01 * 6.801094055175781
Epoch 890, val loss: 1.4981184005737305
Epoch 900, training loss: 0.07168254256248474 = 0.003666781121864915 + 0.01 * 6.801576137542725
Epoch 900, val loss: 1.5025007724761963
Epoch 910, training loss: 0.07165957242250443 = 0.0035647950135171413 + 0.01 * 6.809477806091309
Epoch 910, val loss: 1.5068795680999756
Epoch 920, training loss: 0.07157184183597565 = 0.003468101378530264 + 0.01 * 6.810373783111572
Epoch 920, val loss: 1.5110535621643066
Epoch 930, training loss: 0.0712629035115242 = 0.00337613164447248 + 0.01 * 6.788677215576172
Epoch 930, val loss: 1.5151461362838745
Epoch 940, training loss: 0.07114431262016296 = 0.0032884886022657156 + 0.01 * 6.785582542419434
Epoch 940, val loss: 1.519047737121582
Epoch 950, training loss: 0.07138856500387192 = 0.0032054465264081955 + 0.01 * 6.81831169128418
Epoch 950, val loss: 1.5229487419128418
Epoch 960, training loss: 0.07114844769239426 = 0.0031259821262210608 + 0.01 * 6.802246570587158
Epoch 960, val loss: 1.5266538858413696
Epoch 970, training loss: 0.07074517011642456 = 0.0030502532608807087 + 0.01 * 6.769492149353027
Epoch 970, val loss: 1.5304126739501953
Epoch 980, training loss: 0.07081460952758789 = 0.0029777202289551497 + 0.01 * 6.783689022064209
Epoch 980, val loss: 1.5340275764465332
Epoch 990, training loss: 0.07052241265773773 = 0.002908559050410986 + 0.01 * 6.761385440826416
Epoch 990, val loss: 1.5375056266784668
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 2.0416038036346436 = 1.9556363821029663 + 0.01 * 8.596742630004883
Epoch 0, val loss: 1.9609347581863403
Epoch 10, training loss: 2.031128168106079 = 1.9451615810394287 + 0.01 * 8.596662521362305
Epoch 10, val loss: 1.9500569105148315
Epoch 20, training loss: 2.0180959701538086 = 1.9321324825286865 + 0.01 * 8.596359252929688
Epoch 20, val loss: 1.936334490776062
Epoch 30, training loss: 1.9996548891067505 = 1.9137016534805298 + 0.01 * 8.595327377319336
Epoch 30, val loss: 1.9169894456863403
Epoch 40, training loss: 1.972551703453064 = 1.8866589069366455 + 0.01 * 8.589277267456055
Epoch 40, val loss: 1.8892639875411987
Epoch 50, training loss: 1.9356838464736938 = 1.8501423597335815 + 0.01 * 8.554144859313965
Epoch 50, val loss: 1.8537391424179077
Epoch 60, training loss: 1.89610755443573 = 1.812333345413208 + 0.01 * 8.37741756439209
Epoch 60, val loss: 1.8205031156539917
Epoch 70, training loss: 1.8640780448913574 = 1.7819479703903198 + 0.01 * 8.213007926940918
Epoch 70, val loss: 1.79403555393219
Epoch 80, training loss: 1.8235150575637817 = 1.7446764707565308 + 0.01 * 7.883860111236572
Epoch 80, val loss: 1.7588627338409424
Epoch 90, training loss: 1.7694469690322876 = 1.6931686401367188 + 0.01 * 7.627828598022461
Epoch 90, val loss: 1.7144176959991455
Epoch 100, training loss: 1.6968318223953247 = 1.6225850582122803 + 0.01 * 7.424680709838867
Epoch 100, val loss: 1.6563838720321655
Epoch 110, training loss: 1.609689474105835 = 1.536229133605957 + 0.01 * 7.346030235290527
Epoch 110, val loss: 1.5853016376495361
Epoch 120, training loss: 1.517731785774231 = 1.444760799407959 + 0.01 * 7.297095775604248
Epoch 120, val loss: 1.512087345123291
Epoch 130, training loss: 1.4285712242126465 = 1.3559085130691528 + 0.01 * 7.266270160675049
Epoch 130, val loss: 1.441767930984497
Epoch 140, training loss: 1.341826319694519 = 1.2694313526153564 + 0.01 * 7.239499092102051
Epoch 140, val loss: 1.3755862712860107
Epoch 150, training loss: 1.2557765245437622 = 1.1836729049682617 + 0.01 * 7.210360527038574
Epoch 150, val loss: 1.313223958015442
Epoch 160, training loss: 1.1712452173233032 = 1.099325180053711 + 0.01 * 7.1919989585876465
Epoch 160, val loss: 1.2549875974655151
Epoch 170, training loss: 1.0893840789794922 = 1.0176234245300293 + 0.01 * 7.176070213317871
Epoch 170, val loss: 1.2000000476837158
Epoch 180, training loss: 1.0108213424682617 = 0.9391675591468811 + 0.01 * 7.165380001068115
Epoch 180, val loss: 1.147985816001892
Epoch 190, training loss: 0.9355576038360596 = 0.8640105128288269 + 0.01 * 7.154709339141846
Epoch 190, val loss: 1.0986703634262085
Epoch 200, training loss: 0.8637813925743103 = 0.7923198342323303 + 0.01 * 7.146155834197998
Epoch 200, val loss: 1.0519545078277588
Epoch 210, training loss: 0.7960567474365234 = 0.7247074246406555 + 0.01 * 7.134932994842529
Epoch 210, val loss: 1.0090986490249634
Epoch 220, training loss: 0.7328436374664307 = 0.6616286039352417 + 0.01 * 7.121501922607422
Epoch 220, val loss: 0.9712757468223572
Epoch 230, training loss: 0.6737125515937805 = 0.6025488972663879 + 0.01 * 7.116363048553467
Epoch 230, val loss: 0.9382883906364441
Epoch 240, training loss: 0.6174386739730835 = 0.5463864803314209 + 0.01 * 7.105217456817627
Epoch 240, val loss: 0.909210741519928
Epoch 250, training loss: 0.5631823539733887 = 0.4922764003276825 + 0.01 * 7.090592384338379
Epoch 250, val loss: 0.8831217885017395
Epoch 260, training loss: 0.5112777352333069 = 0.4403042197227478 + 0.01 * 7.097352504730225
Epoch 260, val loss: 0.8603989481925964
Epoch 270, training loss: 0.4622252583503723 = 0.3914136588573456 + 0.01 * 7.0811591148376465
Epoch 270, val loss: 0.8420478105545044
Epoch 280, training loss: 0.4173201322555542 = 0.34662938117980957 + 0.01 * 7.069074630737305
Epoch 280, val loss: 0.8288511037826538
Epoch 290, training loss: 0.37733983993530273 = 0.3066162168979645 + 0.01 * 7.072362899780273
Epoch 290, val loss: 0.8207672238349915
Epoch 300, training loss: 0.3418766260147095 = 0.2712489068508148 + 0.01 * 7.062770366668701
Epoch 300, val loss: 0.8170515894889832
Epoch 310, training loss: 0.31022894382476807 = 0.23963363468647003 + 0.01 * 7.059532642364502
Epoch 310, val loss: 0.8164849281311035
Epoch 320, training loss: 0.2815183401107788 = 0.21070954203605652 + 0.01 * 7.080880165100098
Epoch 320, val loss: 0.817984938621521
Epoch 330, training loss: 0.25434255599975586 = 0.18378493189811707 + 0.01 * 7.055761337280273
Epoch 330, val loss: 0.8206238746643066
Epoch 340, training loss: 0.2291131466627121 = 0.15865179896354675 + 0.01 * 7.046134948730469
Epoch 340, val loss: 0.8239972591400146
Epoch 350, training loss: 0.20603135228157043 = 0.1356026828289032 + 0.01 * 7.042866230010986
Epoch 350, val loss: 0.8281601071357727
Epoch 360, training loss: 0.18550226092338562 = 0.11510027199983597 + 0.01 * 7.0401997566223145
Epoch 360, val loss: 0.8334734439849854
Epoch 370, training loss: 0.16778230667114258 = 0.09743459522724152 + 0.01 * 7.034770965576172
Epoch 370, val loss: 0.8402571082115173
Epoch 380, training loss: 0.15291716158390045 = 0.08257517963647842 + 0.01 * 7.03419828414917
Epoch 380, val loss: 0.8487761616706848
Epoch 390, training loss: 0.14072543382644653 = 0.07028605043888092 + 0.01 * 7.043938159942627
Epoch 390, val loss: 0.8588259220123291
Epoch 400, training loss: 0.13052305579185486 = 0.06022666022181511 + 0.01 * 7.02963924407959
Epoch 400, val loss: 0.8701541423797607
Epoch 410, training loss: 0.12233760952949524 = 0.052009452134370804 + 0.01 * 7.032816410064697
Epoch 410, val loss: 0.8823863863945007
Epoch 420, training loss: 0.11547153443098068 = 0.04529520124197006 + 0.01 * 7.017633438110352
Epoch 420, val loss: 0.8951784372329712
Epoch 430, training loss: 0.1098627969622612 = 0.039774566888809204 + 0.01 * 7.008823394775391
Epoch 430, val loss: 0.9082374572753906
Epoch 440, training loss: 0.10532413423061371 = 0.03520462289452553 + 0.01 * 7.011950969696045
Epoch 440, val loss: 0.9212667942047119
Epoch 450, training loss: 0.10145553201436996 = 0.03139472007751465 + 0.01 * 7.006081581115723
Epoch 450, val loss: 0.9341403841972351
Epoch 460, training loss: 0.0982508435845375 = 0.028188375756144524 + 0.01 * 7.006246566772461
Epoch 460, val loss: 0.9467208981513977
Epoch 470, training loss: 0.09548984467983246 = 0.025470903143286705 + 0.01 * 7.001893997192383
Epoch 470, val loss: 0.9589617848396301
Epoch 480, training loss: 0.09299562871456146 = 0.02314784564077854 + 0.01 * 6.984778881072998
Epoch 480, val loss: 0.9708104729652405
Epoch 490, training loss: 0.09101168066263199 = 0.021146325394511223 + 0.01 * 6.986535549163818
Epoch 490, val loss: 0.9822599291801453
Epoch 500, training loss: 0.08915422856807709 = 0.019411465153098106 + 0.01 * 6.974276065826416
Epoch 500, val loss: 0.9933291077613831
Epoch 510, training loss: 0.08754569292068481 = 0.017895342782139778 + 0.01 * 6.965035438537598
Epoch 510, val loss: 1.004031777381897
Epoch 520, training loss: 0.08615369349718094 = 0.016562988981604576 + 0.01 * 6.959070682525635
Epoch 520, val loss: 1.0143382549285889
Epoch 530, training loss: 0.08494386076927185 = 0.01538657583296299 + 0.01 * 6.955728530883789
Epoch 530, val loss: 1.024314522743225
Epoch 540, training loss: 0.08384200185537338 = 0.014341985806822777 + 0.01 * 6.9500017166137695
Epoch 540, val loss: 1.0339164733886719
Epoch 550, training loss: 0.08309763669967651 = 0.01340896263718605 + 0.01 * 6.96886682510376
Epoch 550, val loss: 1.0432084798812866
Epoch 560, training loss: 0.08189855515956879 = 0.012574189342558384 + 0.01 * 6.932436943054199
Epoch 560, val loss: 1.0521963834762573
Epoch 570, training loss: 0.08123338222503662 = 0.011822233907878399 + 0.01 * 6.941115379333496
Epoch 570, val loss: 1.0608251094818115
Epoch 580, training loss: 0.08038794249296188 = 0.01114238053560257 + 0.01 * 6.924556255340576
Epoch 580, val loss: 1.0691956281661987
Epoch 590, training loss: 0.07970978319644928 = 0.010525700636208057 + 0.01 * 6.918408393859863
Epoch 590, val loss: 1.0773285627365112
Epoch 600, training loss: 0.0791013240814209 = 0.009964325465261936 + 0.01 * 6.913700103759766
Epoch 600, val loss: 1.0851430892944336
Epoch 610, training loss: 0.07873491942882538 = 0.009452014230191708 + 0.01 * 6.928290843963623
Epoch 610, val loss: 1.092761754989624
Epoch 620, training loss: 0.07797273993492126 = 0.008982917293906212 + 0.01 * 6.898982524871826
Epoch 620, val loss: 1.1001391410827637
Epoch 630, training loss: 0.07762202620506287 = 0.008551928214728832 + 0.01 * 6.907010555267334
Epoch 630, val loss: 1.1072646379470825
Epoch 640, training loss: 0.0771501213312149 = 0.008155403658747673 + 0.01 * 6.899471759796143
Epoch 640, val loss: 1.1142250299453735
Epoch 650, training loss: 0.07661224901676178 = 0.007788818329572678 + 0.01 * 6.88234281539917
Epoch 650, val loss: 1.1209567785263062
Epoch 660, training loss: 0.07629726082086563 = 0.007449814584106207 + 0.01 * 6.884745121002197
Epoch 660, val loss: 1.1274991035461426
Epoch 670, training loss: 0.0759107768535614 = 0.007134975865483284 + 0.01 * 6.877580642700195
Epoch 670, val loss: 1.133866548538208
Epoch 680, training loss: 0.07548747956752777 = 0.00684276781976223 + 0.01 * 6.864470958709717
Epoch 680, val loss: 1.1400551795959473
Epoch 690, training loss: 0.07536999881267548 = 0.00656919926404953 + 0.01 * 6.880080223083496
Epoch 690, val loss: 1.1460626125335693
Epoch 700, training loss: 0.07507666200399399 = 0.006315484177321196 + 0.01 * 6.876117706298828
Epoch 700, val loss: 1.1519275903701782
Epoch 710, training loss: 0.07480752468109131 = 0.006077604368329048 + 0.01 * 6.872992515563965
Epoch 710, val loss: 1.1576062440872192
Epoch 720, training loss: 0.07442781329154968 = 0.005853988695889711 + 0.01 * 6.857382774353027
Epoch 720, val loss: 1.1631810665130615
Epoch 730, training loss: 0.07428404688835144 = 0.0056447796523571014 + 0.01 * 6.863927364349365
Epoch 730, val loss: 1.1685930490493774
Epoch 740, training loss: 0.07388842105865479 = 0.005448296200484037 + 0.01 * 6.844012260437012
Epoch 740, val loss: 1.1739304065704346
Epoch 750, training loss: 0.073590949177742 = 0.005264125764369965 + 0.01 * 6.8326826095581055
Epoch 750, val loss: 1.179042100906372
Epoch 760, training loss: 0.07368620485067368 = 0.005090207327157259 + 0.01 * 6.859600067138672
Epoch 760, val loss: 1.184117078781128
Epoch 770, training loss: 0.07336485385894775 = 0.004926178604364395 + 0.01 * 6.843868255615234
Epoch 770, val loss: 1.1890432834625244
Epoch 780, training loss: 0.07313022762537003 = 0.00477144680917263 + 0.01 * 6.835878372192383
Epoch 780, val loss: 1.1938326358795166
Epoch 790, training loss: 0.07287377119064331 = 0.004625081550329924 + 0.01 * 6.824869155883789
Epoch 790, val loss: 1.1985009908676147
Epoch 800, training loss: 0.07268340140581131 = 0.00448712520301342 + 0.01 * 6.81962776184082
Epoch 800, val loss: 1.2030731439590454
Epoch 810, training loss: 0.07252991199493408 = 0.004355926997959614 + 0.01 * 6.817398548126221
Epoch 810, val loss: 1.2075591087341309
Epoch 820, training loss: 0.07235628366470337 = 0.004231972619891167 + 0.01 * 6.812431335449219
Epoch 820, val loss: 1.2118535041809082
Epoch 830, training loss: 0.07217996567487717 = 0.004114298149943352 + 0.01 * 6.8065667152404785
Epoch 830, val loss: 1.2161870002746582
Epoch 840, training loss: 0.07204201072454453 = 0.004002130124717951 + 0.01 * 6.803987979888916
Epoch 840, val loss: 1.220288872718811
Epoch 850, training loss: 0.07184723764657974 = 0.0038959693629294634 + 0.01 * 6.7951273918151855
Epoch 850, val loss: 1.2243934869766235
Epoch 860, training loss: 0.07173698395490646 = 0.0037943862844258547 + 0.01 * 6.794259548187256
Epoch 860, val loss: 1.2283408641815186
Epoch 870, training loss: 0.07169417291879654 = 0.003698234912008047 + 0.01 * 6.799593925476074
Epoch 870, val loss: 1.2322754859924316
Epoch 880, training loss: 0.07162056863307953 = 0.003606129204854369 + 0.01 * 6.8014445304870605
Epoch 880, val loss: 1.236095905303955
Epoch 890, training loss: 0.07132139056921005 = 0.003518147859722376 + 0.01 * 6.7803239822387695
Epoch 890, val loss: 1.2397977113723755
Epoch 900, training loss: 0.07116139680147171 = 0.0034343244042247534 + 0.01 * 6.772707462310791
Epoch 900, val loss: 1.2434366941452026
Epoch 910, training loss: 0.07120373845100403 = 0.0033541424199938774 + 0.01 * 6.78495979309082
Epoch 910, val loss: 1.2470704317092896
Epoch 920, training loss: 0.0715041309595108 = 0.003277260810136795 + 0.01 * 6.822686672210693
Epoch 920, val loss: 1.2505244016647339
Epoch 930, training loss: 0.07100487500429153 = 0.003203794825822115 + 0.01 * 6.7801079750061035
Epoch 930, val loss: 1.2539857625961304
Epoch 940, training loss: 0.0707935020327568 = 0.0031338566914200783 + 0.01 * 6.765964508056641
Epoch 940, val loss: 1.2573156356811523
Epoch 950, training loss: 0.07069086283445358 = 0.0030664526857435703 + 0.01 * 6.762441158294678
Epoch 950, val loss: 1.2606219053268433
Epoch 960, training loss: 0.07083240151405334 = 0.0030018917750567198 + 0.01 * 6.783051490783691
Epoch 960, val loss: 1.2638485431671143
Epoch 970, training loss: 0.07054069638252258 = 0.0029397865291684866 + 0.01 * 6.760091304779053
Epoch 970, val loss: 1.2670460939407349
Epoch 980, training loss: 0.07048854231834412 = 0.0028803073801100254 + 0.01 * 6.760824203491211
Epoch 980, val loss: 1.2701681852340698
Epoch 990, training loss: 0.07052119821310043 = 0.0028231116011738777 + 0.01 * 6.769808769226074
Epoch 990, val loss: 1.2732206583023071
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.0290865898132324 = 1.9431188106536865 + 0.01 * 8.596789360046387
Epoch 0, val loss: 1.9432733058929443
Epoch 10, training loss: 2.019425630569458 = 1.9334585666656494 + 0.01 * 8.596707344055176
Epoch 10, val loss: 1.933263897895813
Epoch 20, training loss: 2.007227897644043 = 1.921263575553894 + 0.01 * 8.596444129943848
Epoch 20, val loss: 1.9206255674362183
Epoch 30, training loss: 1.989854097366333 = 1.9038975238800049 + 0.01 * 8.595662117004395
Epoch 30, val loss: 1.9025245904922485
Epoch 40, training loss: 1.9642654657363892 = 1.8783506155014038 + 0.01 * 8.59148120880127
Epoch 40, val loss: 1.8760329484939575
Epoch 50, training loss: 1.9292975664138794 = 1.8436568975448608 + 0.01 * 8.56406307220459
Epoch 50, val loss: 1.8415734767913818
Epoch 60, training loss: 1.891827940940857 = 1.8075507879257202 + 0.01 * 8.427713394165039
Epoch 60, val loss: 1.8092448711395264
Epoch 70, training loss: 1.8609625101089478 = 1.7782281637191772 + 0.01 * 8.273435592651367
Epoch 70, val loss: 1.7843984365463257
Epoch 80, training loss: 1.8202853202819824 = 1.7406933307647705 + 0.01 * 7.959205150604248
Epoch 80, val loss: 1.7507067918777466
Epoch 90, training loss: 1.76386296749115 = 1.6876940727233887 + 0.01 * 7.616894721984863
Epoch 90, val loss: 1.7043969631195068
Epoch 100, training loss: 1.689043641090393 = 1.6141761541366577 + 0.01 * 7.486752033233643
Epoch 100, val loss: 1.6411634683609009
Epoch 110, training loss: 1.5967713594436646 = 1.5227484703063965 + 0.01 * 7.402288913726807
Epoch 110, val loss: 1.564109444618225
Epoch 120, training loss: 1.4951108694076538 = 1.4219802618026733 + 0.01 * 7.313060760498047
Epoch 120, val loss: 1.4813607931137085
Epoch 130, training loss: 1.3913276195526123 = 1.3188755512237549 + 0.01 * 7.245204448699951
Epoch 130, val loss: 1.3986092805862427
Epoch 140, training loss: 1.2876476049423218 = 1.2156919240951538 + 0.01 * 7.195573329925537
Epoch 140, val loss: 1.3181644678115845
Epoch 150, training loss: 1.1861108541488647 = 1.1144449710845947 + 0.01 * 7.166592597961426
Epoch 150, val loss: 1.242431879043579
Epoch 160, training loss: 1.090575933456421 = 1.0191231966018677 + 0.01 * 7.14527702331543
Epoch 160, val loss: 1.1737949848175049
Epoch 170, training loss: 1.0034555196762085 = 0.9321659803390503 + 0.01 * 7.128950595855713
Epoch 170, val loss: 1.1139789819717407
Epoch 180, training loss: 0.9251669645309448 = 0.8539330363273621 + 0.01 * 7.1233930587768555
Epoch 180, val loss: 1.0631221532821655
Epoch 190, training loss: 0.8549386262893677 = 0.7838594913482666 + 0.01 * 7.107914447784424
Epoch 190, val loss: 1.0212860107421875
Epoch 200, training loss: 0.7918063998222351 = 0.7208355069160461 + 0.01 * 7.0970869064331055
Epoch 200, val loss: 0.9877340793609619
Epoch 210, training loss: 0.7343699932098389 = 0.6634086966514587 + 0.01 * 7.096132755279541
Epoch 210, val loss: 0.9613513946533203
Epoch 220, training loss: 0.6808980703353882 = 0.6100444197654724 + 0.01 * 7.085361957550049
Epoch 220, val loss: 0.940147876739502
Epoch 230, training loss: 0.6303320527076721 = 0.5594816207885742 + 0.01 * 7.085042953491211
Epoch 230, val loss: 0.9225093722343445
Epoch 240, training loss: 0.5818347930908203 = 0.5110520720481873 + 0.01 * 7.078269004821777
Epoch 240, val loss: 0.9073982834815979
Epoch 250, training loss: 0.5353280901908875 = 0.4645744562149048 + 0.01 * 7.075362682342529
Epoch 250, val loss: 0.8951188921928406
Epoch 260, training loss: 0.49101758003234863 = 0.42028263211250305 + 0.01 * 7.0734944343566895
Epoch 260, val loss: 0.8866674900054932
Epoch 270, training loss: 0.4492022395133972 = 0.3784632682800293 + 0.01 * 7.0738983154296875
Epoch 270, val loss: 0.8828045725822449
Epoch 280, training loss: 0.4100985825061798 = 0.3393436670303345 + 0.01 * 7.075491428375244
Epoch 280, val loss: 0.8835774064064026
Epoch 290, training loss: 0.3737122118473053 = 0.30300018191337585 + 0.01 * 7.071202278137207
Epoch 290, val loss: 0.8886851072311401
Epoch 300, training loss: 0.3401162624359131 = 0.2694053649902344 + 0.01 * 7.071091175079346
Epoch 300, val loss: 0.8975123763084412
Epoch 310, training loss: 0.30924123525619507 = 0.23850570619106293 + 0.01 * 7.073553085327148
Epoch 310, val loss: 0.9096152782440186
Epoch 320, training loss: 0.28095752000808716 = 0.21026065945625305 + 0.01 * 7.069687843322754
Epoch 320, val loss: 0.9244821071624756
Epoch 330, training loss: 0.2553444802761078 = 0.18463733792304993 + 0.01 * 7.070715427398682
Epoch 330, val loss: 0.9414681792259216
Epoch 340, training loss: 0.23226973414421082 = 0.16161011159420013 + 0.01 * 7.065961837768555
Epoch 340, val loss: 0.9601238369941711
Epoch 350, training loss: 0.21176469326019287 = 0.14109747111797333 + 0.01 * 7.066723346710205
Epoch 350, val loss: 0.9799680113792419
Epoch 360, training loss: 0.19365708529949188 = 0.122990183532238 + 0.01 * 7.066690444946289
Epoch 360, val loss: 1.0006910562515259
Epoch 370, training loss: 0.17770512402057648 = 0.10710892826318741 + 0.01 * 7.059619903564453
Epoch 370, val loss: 1.0219266414642334
Epoch 380, training loss: 0.16382813453674316 = 0.0932621881365776 + 0.01 * 7.056595802307129
Epoch 380, val loss: 1.0434702634811401
Epoch 390, training loss: 0.15187068283557892 = 0.08126416057348251 + 0.01 * 7.060652256011963
Epoch 390, val loss: 1.0651477575302124
Epoch 400, training loss: 0.14147333800792694 = 0.07093679159879684 + 0.01 * 7.05365514755249
Epoch 400, val loss: 1.0867549180984497
Epoch 410, training loss: 0.13256190717220306 = 0.06208498030900955 + 0.01 * 7.047692775726318
Epoch 410, val loss: 1.108221173286438
Epoch 420, training loss: 0.12494026124477386 = 0.05452004447579384 + 0.01 * 7.042022228240967
Epoch 420, val loss: 1.1293890476226807
Epoch 430, training loss: 0.11847127974033356 = 0.048069555312395096 + 0.01 * 7.040173053741455
Epoch 430, val loss: 1.150073528289795
Epoch 440, training loss: 0.11290550976991653 = 0.0425773486495018 + 0.01 * 7.032816410064697
Epoch 440, val loss: 1.1701551675796509
Epoch 450, training loss: 0.10817958414554596 = 0.03788997605443001 + 0.01 * 7.028960704803467
Epoch 450, val loss: 1.1896320581436157
Epoch 460, training loss: 0.10414820909500122 = 0.03387950733304024 + 0.01 * 7.026870250701904
Epoch 460, val loss: 1.2084145545959473
Epoch 470, training loss: 0.1006091982126236 = 0.030439376831054688 + 0.01 * 7.016982078552246
Epoch 470, val loss: 1.2264519929885864
Epoch 480, training loss: 0.09767469018697739 = 0.027477247640490532 + 0.01 * 7.019744396209717
Epoch 480, val loss: 1.243804931640625
Epoch 490, training loss: 0.09501028805971146 = 0.024917559698224068 + 0.01 * 7.009273529052734
Epoch 490, val loss: 1.2604155540466309
Epoch 500, training loss: 0.09287342429161072 = 0.02269437350332737 + 0.01 * 7.017905235290527
Epoch 500, val loss: 1.2762689590454102
Epoch 510, training loss: 0.09078049659729004 = 0.02075767144560814 + 0.01 * 7.002283096313477
Epoch 510, val loss: 1.2914985418319702
Epoch 520, training loss: 0.08892005681991577 = 0.01906166598200798 + 0.01 * 6.985838890075684
Epoch 520, val loss: 1.306087851524353
Epoch 530, training loss: 0.08739057183265686 = 0.017568407580256462 + 0.01 * 6.9822163581848145
Epoch 530, val loss: 1.319993257522583
Epoch 540, training loss: 0.08609341830015182 = 0.016248052939772606 + 0.01 * 6.984536647796631
Epoch 540, val loss: 1.3333492279052734
Epoch 550, training loss: 0.08479037135839462 = 0.01507708989083767 + 0.01 * 6.971327781677246
Epoch 550, val loss: 1.3460735082626343
Epoch 560, training loss: 0.08370593190193176 = 0.01403321698307991 + 0.01 * 6.967271327972412
Epoch 560, val loss: 1.3583409786224365
Epoch 570, training loss: 0.08267505466938019 = 0.013099252246320248 + 0.01 * 6.957580089569092
Epoch 570, val loss: 1.3700807094573975
Epoch 580, training loss: 0.08179934322834015 = 0.012260155752301216 + 0.01 * 6.953918933868408
Epoch 580, val loss: 1.381395697593689
Epoch 590, training loss: 0.08112231642007828 = 0.011505341157317162 + 0.01 * 6.961697578430176
Epoch 590, val loss: 1.3921451568603516
Epoch 600, training loss: 0.08030646294355392 = 0.01082370150834322 + 0.01 * 6.948276042938232
Epoch 600, val loss: 1.402490496635437
Epoch 610, training loss: 0.07953670620918274 = 0.010204959660768509 + 0.01 * 6.933175086975098
Epoch 610, val loss: 1.4125289916992188
Epoch 620, training loss: 0.07909953594207764 = 0.009641292504966259 + 0.01 * 6.945825099945068
Epoch 620, val loss: 1.422196865081787
Epoch 630, training loss: 0.07846145331859589 = 0.00912726204842329 + 0.01 * 6.933419704437256
Epoch 630, val loss: 1.431449294090271
Epoch 640, training loss: 0.07795954495668411 = 0.008656086400151253 + 0.01 * 6.930346488952637
Epoch 640, val loss: 1.4404199123382568
Epoch 650, training loss: 0.07747751474380493 = 0.008224175311625004 + 0.01 * 6.9253339767456055
Epoch 650, val loss: 1.449043869972229
Epoch 660, training loss: 0.07706236839294434 = 0.007826309651136398 + 0.01 * 6.923605918884277
Epoch 660, val loss: 1.4574180841445923
Epoch 670, training loss: 0.07663387060165405 = 0.007459764834493399 + 0.01 * 6.917410373687744
Epoch 670, val loss: 1.4654649496078491
Epoch 680, training loss: 0.07620525360107422 = 0.007120728958398104 + 0.01 * 6.90845251083374
Epoch 680, val loss: 1.473343014717102
Epoch 690, training loss: 0.07592736184597015 = 0.006807939615100622 + 0.01 * 6.911942481994629
Epoch 690, val loss: 1.4808242321014404
Epoch 700, training loss: 0.07561463862657547 = 0.006517206784337759 + 0.01 * 6.909742832183838
Epoch 700, val loss: 1.4881449937820435
Epoch 710, training loss: 0.07521504908800125 = 0.006247109267860651 + 0.01 * 6.896793842315674
Epoch 710, val loss: 1.4952714443206787
Epoch 720, training loss: 0.07503747940063477 = 0.005995606537908316 + 0.01 * 6.904187202453613
Epoch 720, val loss: 1.5021448135375977
Epoch 730, training loss: 0.07472436875104904 = 0.005761527922004461 + 0.01 * 6.896284580230713
Epoch 730, val loss: 1.508791208267212
Epoch 740, training loss: 0.07455181330442429 = 0.005541956517845392 + 0.01 * 6.9009857177734375
Epoch 740, val loss: 1.5153237581253052
Epoch 750, training loss: 0.0741739347577095 = 0.005337771959602833 + 0.01 * 6.883615970611572
Epoch 750, val loss: 1.5215590000152588
Epoch 760, training loss: 0.0741419866681099 = 0.0051454356871545315 + 0.01 * 6.899655342102051
Epoch 760, val loss: 1.527685284614563
Epoch 770, training loss: 0.07374447584152222 = 0.004965778440237045 + 0.01 * 6.877870559692383
Epoch 770, val loss: 1.5336297750473022
Epoch 780, training loss: 0.07351167500019073 = 0.004796288441866636 + 0.01 * 6.871539115905762
Epoch 780, val loss: 1.5394253730773926
Epoch 790, training loss: 0.07332437485456467 = 0.0046373652294278145 + 0.01 * 6.8687005043029785
Epoch 790, val loss: 1.5450153350830078
Epoch 800, training loss: 0.07313226163387299 = 0.004487161058932543 + 0.01 * 6.864509582519531
Epoch 800, val loss: 1.5504506826400757
Epoch 810, training loss: 0.07306988537311554 = 0.0043459003791213036 + 0.01 * 6.872398376464844
Epoch 810, val loss: 1.5557680130004883
Epoch 820, training loss: 0.07280310243368149 = 0.004212308209389448 + 0.01 * 6.859079837799072
Epoch 820, val loss: 1.560994029045105
Epoch 830, training loss: 0.072689950466156 = 0.004085912834852934 + 0.01 * 6.860403537750244
Epoch 830, val loss: 1.5660152435302734
Epoch 840, training loss: 0.07253611832857132 = 0.00396657595410943 + 0.01 * 6.856954574584961
Epoch 840, val loss: 1.5709083080291748
Epoch 850, training loss: 0.07234799861907959 = 0.0038536766078323126 + 0.01 * 6.849431991577148
Epoch 850, val loss: 1.5756019353866577
Epoch 860, training loss: 0.07215718924999237 = 0.0037459651939570904 + 0.01 * 6.841122150421143
Epoch 860, val loss: 1.580336093902588
Epoch 870, training loss: 0.07206936180591583 = 0.003644021227955818 + 0.01 * 6.842534065246582
Epoch 870, val loss: 1.5848639011383057
Epoch 880, training loss: 0.07192100584506989 = 0.003547669155523181 + 0.01 * 6.837333679199219
Epoch 880, val loss: 1.5892473459243774
Epoch 890, training loss: 0.0720784068107605 = 0.0034553681034594774 + 0.01 * 6.862303733825684
Epoch 890, val loss: 1.5935925245285034
Epoch 900, training loss: 0.07182629406452179 = 0.0033678142353892326 + 0.01 * 6.845848560333252
Epoch 900, val loss: 1.5978394746780396
Epoch 910, training loss: 0.07159526646137238 = 0.0032842103391885757 + 0.01 * 6.831105709075928
Epoch 910, val loss: 1.601912498474121
Epoch 920, training loss: 0.07148618996143341 = 0.0032046933192759752 + 0.01 * 6.828149318695068
Epoch 920, val loss: 1.6059504747390747
Epoch 930, training loss: 0.07138112187385559 = 0.0031286957673728466 + 0.01 * 6.82524299621582
Epoch 930, val loss: 1.6099328994750977
Epoch 940, training loss: 0.07131003588438034 = 0.003056077752262354 + 0.01 * 6.825395584106445
Epoch 940, val loss: 1.6137750148773193
Epoch 950, training loss: 0.07115061581134796 = 0.0029866404365748167 + 0.01 * 6.816397666931152
Epoch 950, val loss: 1.617521047592163
Epoch 960, training loss: 0.07110702991485596 = 0.0029207593761384487 + 0.01 * 6.818626880645752
Epoch 960, val loss: 1.6211552619934082
Epoch 970, training loss: 0.07125258445739746 = 0.0028571896255016327 + 0.01 * 6.839539051055908
Epoch 970, val loss: 1.6247037649154663
Epoch 980, training loss: 0.0710720419883728 = 0.002796548418700695 + 0.01 * 6.827549457550049
Epoch 980, val loss: 1.6282055377960205
Epoch 990, training loss: 0.07084432244300842 = 0.002738409908488393 + 0.01 * 6.810591220855713
Epoch 990, val loss: 1.6315537691116333
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8033737480231946
The final CL Acc:0.75432, 0.00972, The final GNN Acc:0.81357, 0.00722
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13086])
remove edge: torch.Size([2, 7906])
updated graph: torch.Size([2, 10436])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.019956111907959 = 1.9339885711669922 + 0.01 * 8.596760749816895
Epoch 0, val loss: 1.9374839067459106
Epoch 10, training loss: 2.0100908279418945 = 1.9241238832473755 + 0.01 * 8.596685409545898
Epoch 10, val loss: 1.9273898601531982
Epoch 20, training loss: 1.99826979637146 = 1.9123059511184692 + 0.01 * 8.596381187438965
Epoch 20, val loss: 1.9151278734207153
Epoch 30, training loss: 1.9821470975875854 = 1.8961924314498901 + 0.01 * 8.595471382141113
Epoch 30, val loss: 1.8984646797180176
Epoch 40, training loss: 1.9590996503829956 = 1.8731882572174072 + 0.01 * 8.591140747070312
Epoch 40, val loss: 1.87497878074646
Epoch 50, training loss: 1.9269309043884277 = 1.8413017988204956 + 0.01 * 8.562911987304688
Epoch 50, val loss: 1.8433548212051392
Epoch 60, training loss: 1.8862583637237549 = 1.8024462461471558 + 0.01 * 8.381211280822754
Epoch 60, val loss: 1.8070735931396484
Epoch 70, training loss: 1.842092752456665 = 1.762614369392395 + 0.01 * 7.947841167449951
Epoch 70, val loss: 1.7717994451522827
Epoch 80, training loss: 1.7913872003555298 = 1.715049386024475 + 0.01 * 7.633780479431152
Epoch 80, val loss: 1.7275811433792114
Epoch 90, training loss: 1.7246595621109009 = 1.6500624418258667 + 0.01 * 7.459716320037842
Epoch 90, val loss: 1.6668298244476318
Epoch 100, training loss: 1.6375374794006348 = 1.563852071762085 + 0.01 * 7.368537425994873
Epoch 100, val loss: 1.5883492231369019
Epoch 110, training loss: 1.530361533164978 = 1.4572895765304565 + 0.01 * 7.307197093963623
Epoch 110, val loss: 1.4931434392929077
Epoch 120, training loss: 1.4098148345947266 = 1.3371961116790771 + 0.01 * 7.261875629425049
Epoch 120, val loss: 1.388805627822876
Epoch 130, training loss: 1.2840148210525513 = 1.2116483449935913 + 0.01 * 7.2366509437561035
Epoch 130, val loss: 1.2814993858337402
Epoch 140, training loss: 1.160471796989441 = 1.088368535041809 + 0.01 * 7.2103271484375
Epoch 140, val loss: 1.1767685413360596
Epoch 150, training loss: 1.0457236766815186 = 0.9738755822181702 + 0.01 * 7.184813976287842
Epoch 150, val loss: 1.0801215171813965
Epoch 160, training loss: 0.9442874789237976 = 0.8726451992988586 + 0.01 * 7.1642255783081055
Epoch 160, val loss: 0.9965264201164246
Epoch 170, training loss: 0.8572989702224731 = 0.7857958674430847 + 0.01 * 7.1503095626831055
Epoch 170, val loss: 0.9280524849891663
Epoch 180, training loss: 0.7824529409408569 = 0.7110267281532288 + 0.01 * 7.142622470855713
Epoch 180, val loss: 0.8737660646438599
Epoch 190, training loss: 0.7160563468933105 = 0.6447003483772278 + 0.01 * 7.1356000900268555
Epoch 190, val loss: 0.8307361006736755
Epoch 200, training loss: 0.6549615263938904 = 0.5836846828460693 + 0.01 * 7.127686977386475
Epoch 200, val loss: 0.7958577871322632
Epoch 210, training loss: 0.5973960161209106 = 0.5262147188186646 + 0.01 * 7.118129253387451
Epoch 210, val loss: 0.7666481137275696
Epoch 220, training loss: 0.5431274771690369 = 0.472046822309494 + 0.01 * 7.108066558837891
Epoch 220, val loss: 0.7425337433815002
Epoch 230, training loss: 0.4927287697792053 = 0.4217703342437744 + 0.01 * 7.09584379196167
Epoch 230, val loss: 0.7238526940345764
Epoch 240, training loss: 0.4467978775501251 = 0.37594568729400635 + 0.01 * 7.085218906402588
Epoch 240, val loss: 0.710828423500061
Epoch 250, training loss: 0.40545108914375305 = 0.3346908688545227 + 0.01 * 7.076022624969482
Epoch 250, val loss: 0.7030960917472839
Epoch 260, training loss: 0.3683520257472992 = 0.2977086007595062 + 0.01 * 7.064342498779297
Epoch 260, val loss: 0.6999180912971497
Epoch 270, training loss: 0.33509114384651184 = 0.26454004645347595 + 0.01 * 7.055109977722168
Epoch 270, val loss: 0.7004077434539795
Epoch 280, training loss: 0.3052867352962494 = 0.23474271595478058 + 0.01 * 7.054401874542236
Epoch 280, val loss: 0.7038685083389282
Epoch 290, training loss: 0.2785092890262604 = 0.20801998674869537 + 0.01 * 7.048929691314697
Epoch 290, val loss: 0.7097861170768738
Epoch 300, training loss: 0.25451669096946716 = 0.18411631882190704 + 0.01 * 7.040036678314209
Epoch 300, val loss: 0.7177190184593201
Epoch 310, training loss: 0.23321622610092163 = 0.16284212470054626 + 0.01 * 7.03740930557251
Epoch 310, val loss: 0.727324903011322
Epoch 320, training loss: 0.21433138847351074 = 0.14399512112140656 + 0.01 * 7.033626556396484
Epoch 320, val loss: 0.7383722066879272
Epoch 330, training loss: 0.19769105315208435 = 0.1273733377456665 + 0.01 * 7.031770706176758
Epoch 330, val loss: 0.7506037950515747
Epoch 340, training loss: 0.18308600783348083 = 0.11277003586292267 + 0.01 * 7.03159761428833
Epoch 340, val loss: 0.7637743353843689
Epoch 350, training loss: 0.17025434970855713 = 0.0999796912074089 + 0.01 * 7.027466297149658
Epoch 350, val loss: 0.7776576280593872
Epoch 360, training loss: 0.15904128551483154 = 0.08879441767930984 + 0.01 * 7.024685859680176
Epoch 360, val loss: 0.7921366095542908
Epoch 370, training loss: 0.1492312252521515 = 0.07902801781892776 + 0.01 * 7.020319938659668
Epoch 370, val loss: 0.8070431351661682
Epoch 380, training loss: 0.1406824290752411 = 0.07050440460443497 + 0.01 * 7.017803192138672
Epoch 380, val loss: 0.8222007155418396
Epoch 390, training loss: 0.13321930170059204 = 0.06306253373622894 + 0.01 * 7.015676021575928
Epoch 390, val loss: 0.8375238180160522
Epoch 400, training loss: 0.12666654586791992 = 0.05656103789806366 + 0.01 * 7.010550022125244
Epoch 400, val loss: 0.8529574871063232
Epoch 410, training loss: 0.12097594141960144 = 0.05087563022971153 + 0.01 * 7.010031700134277
Epoch 410, val loss: 0.8683642148971558
Epoch 420, training loss: 0.11593745648860931 = 0.04589701443910599 + 0.01 * 7.004044055938721
Epoch 420, val loss: 0.8837231397628784
Epoch 430, training loss: 0.11155582964420319 = 0.041531141847372055 + 0.01 * 7.002469539642334
Epoch 430, val loss: 0.8989279270172119
Epoch 440, training loss: 0.10766845941543579 = 0.03769616037607193 + 0.01 * 6.997230052947998
Epoch 440, val loss: 0.9139853119850159
Epoch 450, training loss: 0.1042747050523758 = 0.034319791942834854 + 0.01 * 6.995491981506348
Epoch 450, val loss: 0.9288303852081299
Epoch 460, training loss: 0.10125662386417389 = 0.03134194388985634 + 0.01 * 6.991467475891113
Epoch 460, val loss: 0.9433943033218384
Epoch 470, training loss: 0.09857514500617981 = 0.028708992525935173 + 0.01 * 6.986615180969238
Epoch 470, val loss: 0.9576809406280518
Epoch 480, training loss: 0.09620776772499084 = 0.02637387625873089 + 0.01 * 6.983388900756836
Epoch 480, val loss: 0.9716458916664124
Epoch 490, training loss: 0.09410570561885834 = 0.024297788739204407 + 0.01 * 6.9807915687561035
Epoch 490, val loss: 0.985308825969696
Epoch 500, training loss: 0.09219510853290558 = 0.02244740165770054 + 0.01 * 6.974771022796631
Epoch 500, val loss: 0.9986482858657837
Epoch 510, training loss: 0.09050048887729645 = 0.02079225704073906 + 0.01 * 6.970822811126709
Epoch 510, val loss: 1.011639952659607
Epoch 520, training loss: 0.08898427337408066 = 0.01930573396384716 + 0.01 * 6.967854022979736
Epoch 520, val loss: 1.024327278137207
Epoch 530, training loss: 0.08766154944896698 = 0.0179680697619915 + 0.01 * 6.969348430633545
Epoch 530, val loss: 1.0367064476013184
Epoch 540, training loss: 0.08634597063064575 = 0.016761159524321556 + 0.01 * 6.9584808349609375
Epoch 540, val loss: 1.0487197637557983
Epoch 550, training loss: 0.08524109423160553 = 0.015669770538806915 + 0.01 * 6.957132816314697
Epoch 550, val loss: 1.06045663356781
Epoch 560, training loss: 0.08415625989437103 = 0.014681676402688026 + 0.01 * 6.947458744049072
Epoch 560, val loss: 1.0718567371368408
Epoch 570, training loss: 0.08323121070861816 = 0.0137823186814785 + 0.01 * 6.944889545440674
Epoch 570, val loss: 1.0829609632492065
Epoch 580, training loss: 0.08252116292715073 = 0.012961211614310741 + 0.01 * 6.955995559692383
Epoch 580, val loss: 1.0938395261764526
Epoch 590, training loss: 0.0815831869840622 = 0.012210334651172161 + 0.01 * 6.937285423278809
Epoch 590, val loss: 1.104356050491333
Epoch 600, training loss: 0.08092308044433594 = 0.011522441171109676 + 0.01 * 6.940063953399658
Epoch 600, val loss: 1.114621877670288
Epoch 610, training loss: 0.08015745878219604 = 0.010892338119447231 + 0.01 * 6.926511764526367
Epoch 610, val loss: 1.124604344367981
Epoch 620, training loss: 0.07956243306398392 = 0.010313780978322029 + 0.01 * 6.924865245819092
Epoch 620, val loss: 1.1343286037445068
Epoch 630, training loss: 0.07893985509872437 = 0.00978202186524868 + 0.01 * 6.915783405303955
Epoch 630, val loss: 1.1437592506408691
Epoch 640, training loss: 0.07856826484203339 = 0.009291703812777996 + 0.01 * 6.927656173706055
Epoch 640, val loss: 1.1529370546340942
Epoch 650, training loss: 0.07804549485445023 = 0.00883937906473875 + 0.01 * 6.92061185836792
Epoch 650, val loss: 1.161845088005066
Epoch 660, training loss: 0.07753981649875641 = 0.008422041311860085 + 0.01 * 6.911777973175049
Epoch 660, val loss: 1.1705007553100586
Epoch 670, training loss: 0.0770546942949295 = 0.008035281673073769 + 0.01 * 6.901941299438477
Epoch 670, val loss: 1.1789367198944092
Epoch 680, training loss: 0.07668180763721466 = 0.007677096873521805 + 0.01 * 6.900470733642578
Epoch 680, val loss: 1.1871148347854614
Epoch 690, training loss: 0.07629410922527313 = 0.0073440950363874435 + 0.01 * 6.895001411437988
Epoch 690, val loss: 1.1950585842132568
Epoch 700, training loss: 0.0759352520108223 = 0.007034196052700281 + 0.01 * 6.890105724334717
Epoch 700, val loss: 1.2028138637542725
Epoch 710, training loss: 0.07574190199375153 = 0.006745440419763327 + 0.01 * 6.899646282196045
Epoch 710, val loss: 1.2103654146194458
Epoch 720, training loss: 0.07540770620107651 = 0.006476006470620632 + 0.01 * 6.893170356750488
Epoch 720, val loss: 1.2176775932312012
Epoch 730, training loss: 0.07516901195049286 = 0.006223929114639759 + 0.01 * 6.894507884979248
Epoch 730, val loss: 1.2248213291168213
Epoch 740, training loss: 0.07479774951934814 = 0.005988236516714096 + 0.01 * 6.880951404571533
Epoch 740, val loss: 1.2317750453948975
Epoch 750, training loss: 0.0745304599404335 = 0.005767306312918663 + 0.01 * 6.876315593719482
Epoch 750, val loss: 1.238525629043579
Epoch 760, training loss: 0.07432510703802109 = 0.005559979006648064 + 0.01 * 6.8765130043029785
Epoch 760, val loss: 1.245147466659546
Epoch 770, training loss: 0.07401059567928314 = 0.005365183111280203 + 0.01 * 6.864541053771973
Epoch 770, val loss: 1.2515885829925537
Epoch 780, training loss: 0.07380549609661102 = 0.005181673914194107 + 0.01 * 6.862382411956787
Epoch 780, val loss: 1.257806658744812
Epoch 790, training loss: 0.07368919253349304 = 0.005009389016777277 + 0.01 * 6.867980003356934
Epoch 790, val loss: 1.2639615535736084
Epoch 800, training loss: 0.07339566200971603 = 0.004846679046750069 + 0.01 * 6.854898929595947
Epoch 800, val loss: 1.2698558568954468
Epoch 810, training loss: 0.07324139773845673 = 0.004692832473665476 + 0.01 * 6.854856967926025
Epoch 810, val loss: 1.275695562362671
Epoch 820, training loss: 0.0729866623878479 = 0.004547966178506613 + 0.01 * 6.843869686126709
Epoch 820, val loss: 1.2813541889190674
Epoch 830, training loss: 0.07279399037361145 = 0.004410082474350929 + 0.01 * 6.838390827178955
Epoch 830, val loss: 1.2869126796722412
Epoch 840, training loss: 0.0727783814072609 = 0.004280361346900463 + 0.01 * 6.849802494049072
Epoch 840, val loss: 1.2922848463058472
Epoch 850, training loss: 0.07248298823833466 = 0.004157183691859245 + 0.01 * 6.83258056640625
Epoch 850, val loss: 1.297501564025879
Epoch 860, training loss: 0.07248904556035995 = 0.004039806313812733 + 0.01 * 6.844923496246338
Epoch 860, val loss: 1.3026877641677856
Epoch 870, training loss: 0.07224228233098984 = 0.003928891383111477 + 0.01 * 6.831338882446289
Epoch 870, val loss: 1.3077067136764526
Epoch 880, training loss: 0.07199764996767044 = 0.0038234787061810493 + 0.01 * 6.817417144775391
Epoch 880, val loss: 1.3125903606414795
Epoch 890, training loss: 0.07185656577348709 = 0.0037229955196380615 + 0.01 * 6.813356876373291
Epoch 890, val loss: 1.3174477815628052
Epoch 900, training loss: 0.0717368870973587 = 0.003627519588917494 + 0.01 * 6.81093692779541
Epoch 900, val loss: 1.3221334218978882
Epoch 910, training loss: 0.07221114635467529 = 0.0035364541690796614 + 0.01 * 6.867469310760498
Epoch 910, val loss: 1.3267585039138794
Epoch 920, training loss: 0.07168784737586975 = 0.003450017888098955 + 0.01 * 6.823782920837402
Epoch 920, val loss: 1.3311830759048462
Epoch 930, training loss: 0.07141223549842834 = 0.0033671914134174585 + 0.01 * 6.80450439453125
Epoch 930, val loss: 1.3354727029800415
Epoch 940, training loss: 0.07138510048389435 = 0.0032883004751056433 + 0.01 * 6.809679985046387
Epoch 940, val loss: 1.3397749662399292
Epoch 950, training loss: 0.07125063240528107 = 0.003212581155821681 + 0.01 * 6.803805828094482
Epoch 950, val loss: 1.3439503908157349
Epoch 960, training loss: 0.07127509266138077 = 0.003140372224152088 + 0.01 * 6.813472270965576
Epoch 960, val loss: 1.3480678796768188
Epoch 970, training loss: 0.07116967439651489 = 0.0030712075531482697 + 0.01 * 6.809847354888916
Epoch 970, val loss: 1.3520511388778687
Epoch 980, training loss: 0.0708857923746109 = 0.003004890400916338 + 0.01 * 6.788090229034424
Epoch 980, val loss: 1.355955958366394
Epoch 990, training loss: 0.0708283931016922 = 0.002941213548183441 + 0.01 * 6.788718223571777
Epoch 990, val loss: 1.3597427606582642
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.0191211700439453 = 1.9331530332565308 + 0.01 * 8.59682559967041
Epoch 0, val loss: 1.9264347553253174
Epoch 10, training loss: 2.0092689990997314 = 1.9233009815216064 + 0.01 * 8.596796035766602
Epoch 10, val loss: 1.9165419340133667
Epoch 20, training loss: 1.9974379539489746 = 1.9114714860916138 + 0.01 * 8.596650123596191
Epoch 20, val loss: 1.9044615030288696
Epoch 30, training loss: 1.9813339710235596 = 1.8953713178634644 + 0.01 * 8.596264839172363
Epoch 30, val loss: 1.887909173965454
Epoch 40, training loss: 1.9584081172943115 = 1.872459888458252 + 0.01 * 8.594826698303223
Epoch 40, val loss: 1.8646875619888306
Epoch 50, training loss: 1.927228331565857 = 1.8413673639297485 + 0.01 * 8.586097717285156
Epoch 50, val loss: 1.8346573114395142
Epoch 60, training loss: 1.89093816280365 = 1.8055775165557861 + 0.01 * 8.536064147949219
Epoch 60, val loss: 1.803472876548767
Epoch 70, training loss: 1.8531702756881714 = 1.7703704833984375 + 0.01 * 8.279974937438965
Epoch 70, val loss: 1.775732159614563
Epoch 80, training loss: 1.8069666624069214 = 1.7261102199554443 + 0.01 * 8.08564281463623
Epoch 80, val loss: 1.7382558584213257
Epoch 90, training loss: 1.7421741485595703 = 1.6641708612442017 + 0.01 * 7.800325393676758
Epoch 90, val loss: 1.6835907697677612
Epoch 100, training loss: 1.6580936908721924 = 1.582174301147461 + 0.01 * 7.591940879821777
Epoch 100, val loss: 1.6124267578125
Epoch 110, training loss: 1.5600277185440063 = 1.485365390777588 + 0.01 * 7.466235637664795
Epoch 110, val loss: 1.5316760540008545
Epoch 120, training loss: 1.4560999870300293 = 1.3820589780807495 + 0.01 * 7.404097557067871
Epoch 120, val loss: 1.4488160610198975
Epoch 130, training loss: 1.3503808975219727 = 1.2768288850784302 + 0.01 * 7.355202674865723
Epoch 130, val loss: 1.3659989833831787
Epoch 140, training loss: 1.2463278770446777 = 1.1730707883834839 + 0.01 * 7.325706481933594
Epoch 140, val loss: 1.285741925239563
Epoch 150, training loss: 1.1480178833007812 = 1.0749826431274414 + 0.01 * 7.303526878356934
Epoch 150, val loss: 1.2105991840362549
Epoch 160, training loss: 1.0582941770553589 = 0.9854478240013123 + 0.01 * 7.284631729125977
Epoch 160, val loss: 1.1426297426223755
Epoch 170, training loss: 0.9770033955574036 = 0.9043933749198914 + 0.01 * 7.261003017425537
Epoch 170, val loss: 1.082104206085205
Epoch 180, training loss: 0.9024043679237366 = 0.8301849961280823 + 0.01 * 7.221934795379639
Epoch 180, val loss: 1.0278103351593018
Epoch 190, training loss: 0.8337749242782593 = 0.7620243430137634 + 0.01 * 7.175058364868164
Epoch 190, val loss: 0.9786818027496338
Epoch 200, training loss: 0.7709043622016907 = 0.6995682120323181 + 0.01 * 7.133615970611572
Epoch 200, val loss: 0.9350152015686035
Epoch 210, training loss: 0.7128696441650391 = 0.6418477892875671 + 0.01 * 7.1021857261657715
Epoch 210, val loss: 0.896569550037384
Epoch 220, training loss: 0.6585531234741211 = 0.5877394676208496 + 0.01 * 7.081364631652832
Epoch 220, val loss: 0.8626126050949097
Epoch 230, training loss: 0.607162594795227 = 0.536476194858551 + 0.01 * 7.068639278411865
Epoch 230, val loss: 0.8328095078468323
Epoch 240, training loss: 0.5585200190544128 = 0.4879052937030792 + 0.01 * 7.061474323272705
Epoch 240, val loss: 0.8072072863578796
Epoch 250, training loss: 0.5128299593925476 = 0.4422758221626282 + 0.01 * 7.055415153503418
Epoch 250, val loss: 0.7862100005149841
Epoch 260, training loss: 0.4701499044895172 = 0.3996594250202179 + 0.01 * 7.049047470092773
Epoch 260, val loss: 0.7698116898536682
Epoch 270, training loss: 0.43029820919036865 = 0.35985279083251953 + 0.01 * 7.04454231262207
Epoch 270, val loss: 0.7576696872711182
Epoch 280, training loss: 0.39308539032936096 = 0.32269009947776794 + 0.01 * 7.039530277252197
Epoch 280, val loss: 0.7495732307434082
Epoch 290, training loss: 0.35846877098083496 = 0.28812289237976074 + 0.01 * 7.034586429595947
Epoch 290, val loss: 0.7452269792556763
Epoch 300, training loss: 0.32651180028915405 = 0.2561635673046112 + 0.01 * 7.034823417663574
Epoch 300, val loss: 0.7441993951797485
Epoch 310, training loss: 0.2971229553222656 = 0.22684140503406525 + 0.01 * 7.028154373168945
Epoch 310, val loss: 0.7458299398422241
Epoch 320, training loss: 0.2704302668571472 = 0.20019623637199402 + 0.01 * 7.023403644561768
Epoch 320, val loss: 0.7497729659080505
Epoch 330, training loss: 0.24647480249404907 = 0.17627014219760895 + 0.01 * 7.020465850830078
Epoch 330, val loss: 0.7557052969932556
Epoch 340, training loss: 0.2252148687839508 = 0.15505088865756989 + 0.01 * 7.016397953033447
Epoch 340, val loss: 0.7634162902832031
Epoch 350, training loss: 0.2065732181072235 = 0.1364038586616516 + 0.01 * 7.016936779022217
Epoch 350, val loss: 0.7727855443954468
Epoch 360, training loss: 0.19021488726139069 = 0.12010452151298523 + 0.01 * 7.011036396026611
Epoch 360, val loss: 0.7835085391998291
Epoch 370, training loss: 0.17597118020057678 = 0.10587085038423538 + 0.01 * 7.01003360748291
Epoch 370, val loss: 0.7952911853790283
Epoch 380, training loss: 0.16351217031478882 = 0.09343837201595306 + 0.01 * 7.007380962371826
Epoch 380, val loss: 0.807971715927124
Epoch 390, training loss: 0.15264514088630676 = 0.08258593827486038 + 0.01 * 7.005919933319092
Epoch 390, val loss: 0.8212701678276062
Epoch 400, training loss: 0.1431494653224945 = 0.07312487065792084 + 0.01 * 7.00246000289917
Epoch 400, val loss: 0.8349999189376831
Epoch 410, training loss: 0.13488994538784027 = 0.06489408016204834 + 0.01 * 6.999586582183838
Epoch 410, val loss: 0.8489771485328674
Epoch 420, training loss: 0.12780526280403137 = 0.05774898827075958 + 0.01 * 7.005627155303955
Epoch 420, val loss: 0.8630445003509521
Epoch 430, training loss: 0.1215135008096695 = 0.05155595764517784 + 0.01 * 6.995754718780518
Epoch 430, val loss: 0.877088725566864
Epoch 440, training loss: 0.11610814183950424 = 0.04618600755929947 + 0.01 * 6.992213249206543
Epoch 440, val loss: 0.8910915851593018
Epoch 450, training loss: 0.1114170178771019 = 0.04152607172727585 + 0.01 * 6.9890947341918945
Epoch 450, val loss: 0.9049761295318604
Epoch 460, training loss: 0.10735765099525452 = 0.03747405484318733 + 0.01 * 6.9883599281311035
Epoch 460, val loss: 0.9186960458755493
Epoch 470, training loss: 0.10378528386354446 = 0.03394375741481781 + 0.01 * 6.984152793884277
Epoch 470, val loss: 0.9322018027305603
Epoch 480, training loss: 0.10069093108177185 = 0.030858714133501053 + 0.01 * 6.983222484588623
Epoch 480, val loss: 0.945496141910553
Epoch 490, training loss: 0.09793917089700699 = 0.028153853490948677 + 0.01 * 6.978531837463379
Epoch 490, val loss: 0.9584780335426331
Epoch 500, training loss: 0.09558075666427612 = 0.02577437087893486 + 0.01 * 6.9806389808654785
Epoch 500, val loss: 0.9711607694625854
Epoch 510, training loss: 0.09340669214725494 = 0.023674773052334785 + 0.01 * 6.97319221496582
Epoch 510, val loss: 0.9834637641906738
Epoch 520, training loss: 0.09151316434144974 = 0.021815180778503418 + 0.01 * 6.969798564910889
Epoch 520, val loss: 0.9954689145088196
Epoch 530, training loss: 0.0898342952132225 = 0.020160948857665062 + 0.01 * 6.967334747314453
Epoch 530, val loss: 1.0071356296539307
Epoch 540, training loss: 0.08832570910453796 = 0.01868496462702751 + 0.01 * 6.96407413482666
Epoch 540, val loss: 1.0184568166732788
Epoch 550, training loss: 0.08699320256710052 = 0.01736385002732277 + 0.01 * 6.962934970855713
Epoch 550, val loss: 1.0294140577316284
Epoch 560, training loss: 0.08574748039245605 = 0.016175441443920135 + 0.01 * 6.957204341888428
Epoch 560, val loss: 1.04008150100708
Epoch 570, training loss: 0.08474650233983994 = 0.015101568773388863 + 0.01 * 6.964493751525879
Epoch 570, val loss: 1.0504664182662964
Epoch 580, training loss: 0.08363401144742966 = 0.014128186739981174 + 0.01 * 6.950582504272461
Epoch 580, val loss: 1.0606071949005127
Epoch 590, training loss: 0.0827377438545227 = 0.013243161141872406 + 0.01 * 6.949458599090576
Epoch 590, val loss: 1.070486307144165
Epoch 600, training loss: 0.08199815452098846 = 0.012436240911483765 + 0.01 * 6.956191539764404
Epoch 600, val loss: 1.0800787210464478
Epoch 610, training loss: 0.08113802969455719 = 0.011699846014380455 + 0.01 * 6.943818092346191
Epoch 610, val loss: 1.0893871784210205
Epoch 620, training loss: 0.08042948693037033 = 0.011025791987776756 + 0.01 * 6.940369129180908
Epoch 620, val loss: 1.0984169244766235
Epoch 630, training loss: 0.07976588606834412 = 0.01040648017078638 + 0.01 * 6.935940742492676
Epoch 630, val loss: 1.1072345972061157
Epoch 640, training loss: 0.07933565229177475 = 0.00983655359596014 + 0.01 * 6.949909687042236
Epoch 640, val loss: 1.1157891750335693
Epoch 650, training loss: 0.07861479371786118 = 0.009313380345702171 + 0.01 * 6.930140972137451
Epoch 650, val loss: 1.1240471601486206
Epoch 660, training loss: 0.07809661328792572 = 0.008830652572214603 + 0.01 * 6.926596164703369
Epoch 660, val loss: 1.1320712566375732
Epoch 670, training loss: 0.07762663066387177 = 0.00838408898562193 + 0.01 * 6.924254894256592
Epoch 670, val loss: 1.1399030685424805
Epoch 680, training loss: 0.07718589901924133 = 0.007971187122166157 + 0.01 * 6.921471118927002
Epoch 680, val loss: 1.1474905014038086
Epoch 690, training loss: 0.07677367329597473 = 0.007588879205286503 + 0.01 * 6.9184794425964355
Epoch 690, val loss: 1.154837965965271
Epoch 700, training loss: 0.07643311470746994 = 0.007234180346131325 + 0.01 * 6.919893741607666
Epoch 700, val loss: 1.1620146036148071
Epoch 710, training loss: 0.07602259516716003 = 0.00690452242270112 + 0.01 * 6.911807537078857
Epoch 710, val loss: 1.1689268350601196
Epoch 720, training loss: 0.0756671205163002 = 0.006597431376576424 + 0.01 * 6.9069695472717285
Epoch 720, val loss: 1.175689458847046
Epoch 730, training loss: 0.07545610517263412 = 0.006310788914561272 + 0.01 * 6.914531230926514
Epoch 730, val loss: 1.1822960376739502
Epoch 740, training loss: 0.07515187561511993 = 0.006044449284672737 + 0.01 * 6.910743236541748
Epoch 740, val loss: 1.1886018514633179
Epoch 750, training loss: 0.07478411495685577 = 0.005796578247100115 + 0.01 * 6.8987531661987305
Epoch 750, val loss: 1.194718599319458
Epoch 760, training loss: 0.07451725751161575 = 0.005564883816987276 + 0.01 * 6.895237922668457
Epoch 760, val loss: 1.20068359375
Epoch 770, training loss: 0.0742751806974411 = 0.005347506143152714 + 0.01 * 6.892767906188965
Epoch 770, val loss: 1.2065142393112183
Epoch 780, training loss: 0.07414861023426056 = 0.005143349524587393 + 0.01 * 6.90052604675293
Epoch 780, val loss: 1.2121816873550415
Epoch 790, training loss: 0.07384704053401947 = 0.004952102433890104 + 0.01 * 6.889493942260742
Epoch 790, val loss: 1.2176371812820435
Epoch 800, training loss: 0.07362493127584457 = 0.0047721560113132 + 0.01 * 6.88527774810791
Epoch 800, val loss: 1.2230169773101807
Epoch 810, training loss: 0.07348082959651947 = 0.004602900240570307 + 0.01 * 6.887792587280273
Epoch 810, val loss: 1.228203535079956
Epoch 820, training loss: 0.07320261746644974 = 0.004443501587957144 + 0.01 * 6.875911712646484
Epoch 820, val loss: 1.2332569360733032
Epoch 830, training loss: 0.07316397875547409 = 0.004293193109333515 + 0.01 * 6.887078285217285
Epoch 830, val loss: 1.2381670475006104
Epoch 840, training loss: 0.07289087027311325 = 0.004151776898652315 + 0.01 * 6.8739094734191895
Epoch 840, val loss: 1.2429002523422241
Epoch 850, training loss: 0.07267482578754425 = 0.004018019884824753 + 0.01 * 6.865681171417236
Epoch 850, val loss: 1.2475287914276123
Epoch 860, training loss: 0.0725560337305069 = 0.0038913688622415066 + 0.01 * 6.866466045379639
Epoch 860, val loss: 1.252061128616333
Epoch 870, training loss: 0.07244425266981125 = 0.003771540941670537 + 0.01 * 6.8672709465026855
Epoch 870, val loss: 1.2564163208007812
Epoch 880, training loss: 0.07226520031690598 = 0.003658198518678546 + 0.01 * 6.860700607299805
Epoch 880, val loss: 1.260671854019165
Epoch 890, training loss: 0.07221220433712006 = 0.00355091062374413 + 0.01 * 6.8661298751831055
Epoch 890, val loss: 1.2648252248764038
Epoch 900, training loss: 0.07207728177309036 = 0.0034493375569581985 + 0.01 * 6.862794876098633
Epoch 900, val loss: 1.2687970399856567
Epoch 910, training loss: 0.0719839334487915 = 0.0033529119100421667 + 0.01 * 6.863102436065674
Epoch 910, val loss: 1.2727121114730835
Epoch 920, training loss: 0.07172548025846481 = 0.0032615663949400187 + 0.01 * 6.846391677856445
Epoch 920, val loss: 1.2764710187911987
Epoch 930, training loss: 0.07161641865968704 = 0.0031744965817779303 + 0.01 * 6.8441925048828125
Epoch 930, val loss: 1.2801138162612915
Epoch 940, training loss: 0.0714765191078186 = 0.0030914070084691048 + 0.01 * 6.838510990142822
Epoch 940, val loss: 1.2837331295013428
Epoch 950, training loss: 0.07181289047002792 = 0.0030122469179332256 + 0.01 * 6.880064487457275
Epoch 950, val loss: 1.2873042821884155
Epoch 960, training loss: 0.07141081243753433 = 0.0029370684642344713 + 0.01 * 6.847373962402344
Epoch 960, val loss: 1.2906241416931152
Epoch 970, training loss: 0.07118092477321625 = 0.0028654616326093674 + 0.01 * 6.831546306610107
Epoch 970, val loss: 1.293887972831726
Epoch 980, training loss: 0.07122635841369629 = 0.0027970613446086645 + 0.01 * 6.842930316925049
Epoch 980, val loss: 1.2971190214157104
Epoch 990, training loss: 0.07109222561120987 = 0.0027316694613546133 + 0.01 * 6.836055755615234
Epoch 990, val loss: 1.3002221584320068
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.0466690063476562 = 1.9607009887695312 + 0.01 * 8.5968017578125
Epoch 0, val loss: 1.9667882919311523
Epoch 10, training loss: 2.035890579223633 = 1.9499233961105347 + 0.01 * 8.596724510192871
Epoch 10, val loss: 1.9559794664382935
Epoch 20, training loss: 2.0225276947021484 = 1.936562418937683 + 0.01 * 8.596539497375488
Epoch 20, val loss: 1.9419734477996826
Epoch 30, training loss: 2.0039846897125244 = 1.9180243015289307 + 0.01 * 8.596030235290527
Epoch 30, val loss: 1.922023057937622
Epoch 40, training loss: 1.9765961170196533 = 1.8906594514846802 + 0.01 * 8.593667984008789
Epoch 40, val loss: 1.8926355838775635
Epoch 50, training loss: 1.9381327629089355 = 1.852356195449829 + 0.01 * 8.577658653259277
Epoch 50, val loss: 1.8532129526138306
Epoch 60, training loss: 1.8956859111785889 = 1.8106216192245483 + 0.01 * 8.506431579589844
Epoch 60, val loss: 1.8145524263381958
Epoch 70, training loss: 1.86137056350708 = 1.778070092201233 + 0.01 * 8.330052375793457
Epoch 70, val loss: 1.787630558013916
Epoch 80, training loss: 1.8212122917175293 = 1.7393046617507935 + 0.01 * 8.190767288208008
Epoch 80, val loss: 1.7533055543899536
Epoch 90, training loss: 1.7636003494262695 = 1.6843043565750122 + 0.01 * 7.929603099822998
Epoch 90, val loss: 1.7045373916625977
Epoch 100, training loss: 1.6853283643722534 = 1.6085395812988281 + 0.01 * 7.678873062133789
Epoch 100, val loss: 1.6386661529541016
Epoch 110, training loss: 1.5954668521881104 = 1.5197535753250122 + 0.01 * 7.571332931518555
Epoch 110, val loss: 1.5638177394866943
Epoch 120, training loss: 1.508936882019043 = 1.4335328340530396 + 0.01 * 7.540400981903076
Epoch 120, val loss: 1.4928314685821533
Epoch 130, training loss: 1.4274778366088867 = 1.352465271949768 + 0.01 * 7.501251697540283
Epoch 130, val loss: 1.4278563261032104
Epoch 140, training loss: 1.3461562395095825 = 1.2716034650802612 + 0.01 * 7.455278396606445
Epoch 140, val loss: 1.3648232221603394
Epoch 150, training loss: 1.2612042427062988 = 1.187207818031311 + 0.01 * 7.399646759033203
Epoch 150, val loss: 1.2993007898330688
Epoch 160, training loss: 1.172183632850647 = 1.0989480018615723 + 0.01 * 7.323562145233154
Epoch 160, val loss: 1.2312986850738525
Epoch 170, training loss: 1.0812519788742065 = 1.0089277029037476 + 0.01 * 7.232429027557373
Epoch 170, val loss: 1.162028431892395
Epoch 180, training loss: 0.9922139644622803 = 0.9203272461891174 + 0.01 * 7.188669681549072
Epoch 180, val loss: 1.0931968688964844
Epoch 190, training loss: 0.9078432321548462 = 0.8362391591072083 + 0.01 * 7.160405158996582
Epoch 190, val loss: 1.0273637771606445
Epoch 200, training loss: 0.8302024602890015 = 0.7588084936141968 + 0.01 * 7.139400005340576
Epoch 200, val loss: 0.967501699924469
Epoch 210, training loss: 0.7598429918289185 = 0.6885876655578613 + 0.01 * 7.12553071975708
Epoch 210, val loss: 0.915879487991333
Epoch 220, training loss: 0.6957486867904663 = 0.6246789693832397 + 0.01 * 7.106968879699707
Epoch 220, val loss: 0.8734388947486877
Epoch 230, training loss: 0.6369490623474121 = 0.5660097002983093 + 0.01 * 7.093934535980225
Epoch 230, val loss: 0.8401232957839966
Epoch 240, training loss: 0.5828865170478821 = 0.5120208859443665 + 0.01 * 7.086563587188721
Epoch 240, val loss: 0.8149814009666443
Epoch 250, training loss: 0.5330494046211243 = 0.46232613921165466 + 0.01 * 7.072327613830566
Epoch 250, val loss: 0.7969067692756653
Epoch 260, training loss: 0.4871335029602051 = 0.41649699211120605 + 0.01 * 7.063650131225586
Epoch 260, val loss: 0.7849281430244446
Epoch 270, training loss: 0.4448527693748474 = 0.3743148148059845 + 0.01 * 7.05379581451416
Epoch 270, val loss: 0.7785972356796265
Epoch 280, training loss: 0.4060039222240448 = 0.3355148732662201 + 0.01 * 7.0489044189453125
Epoch 280, val loss: 0.7772477865219116
Epoch 290, training loss: 0.3701121211051941 = 0.29973965883255005 + 0.01 * 7.037246227264404
Epoch 290, val loss: 0.7799113988876343
Epoch 300, training loss: 0.33693796396255493 = 0.26672109961509705 + 0.01 * 7.0216875076293945
Epoch 300, val loss: 0.78563392162323
Epoch 310, training loss: 0.3067185878753662 = 0.2363937646150589 + 0.01 * 7.032481670379639
Epoch 310, val loss: 0.7941473722457886
Epoch 320, training loss: 0.2789841890335083 = 0.2089131623506546 + 0.01 * 7.0071024894714355
Epoch 320, val loss: 0.8051654100418091
Epoch 330, training loss: 0.25430381298065186 = 0.18429841101169586 + 0.01 * 7.000540733337402
Epoch 330, val loss: 0.8184227347373962
Epoch 340, training loss: 0.23244130611419678 = 0.16248738765716553 + 0.01 * 6.995390892028809
Epoch 340, val loss: 0.8336763978004456
Epoch 350, training loss: 0.21312890946865082 = 0.14334440231323242 + 0.01 * 6.978450775146484
Epoch 350, val loss: 0.8506685495376587
Epoch 360, training loss: 0.1963396817445755 = 0.12662151455879211 + 0.01 * 6.971816539764404
Epoch 360, val loss: 0.8691361546516418
Epoch 370, training loss: 0.18187980353832245 = 0.11203259229660034 + 0.01 * 6.9847211837768555
Epoch 370, val loss: 0.8886628746986389
Epoch 380, training loss: 0.16895198822021484 = 0.09931890666484833 + 0.01 * 6.963309288024902
Epoch 380, val loss: 0.9088878631591797
Epoch 390, training loss: 0.15800979733467102 = 0.08823542296886444 + 0.01 * 6.977436542510986
Epoch 390, val loss: 0.9296371936798096
Epoch 400, training loss: 0.14810959994792938 = 0.07858549803495407 + 0.01 * 6.952410697937012
Epoch 400, val loss: 0.9505147337913513
Epoch 410, training loss: 0.1396576315164566 = 0.0701640322804451 + 0.01 * 6.949359893798828
Epoch 410, val loss: 0.9714778065681458
Epoch 420, training loss: 0.13222911953926086 = 0.06281225383281708 + 0.01 * 6.941686153411865
Epoch 420, val loss: 0.9923152327537537
Epoch 430, training loss: 0.1257094442844391 = 0.056379787623882294 + 0.01 * 6.932965278625488
Epoch 430, val loss: 1.0129215717315674
Epoch 440, training loss: 0.1199953705072403 = 0.05074332654476166 + 0.01 * 6.925204277038574
Epoch 440, val loss: 1.0332016944885254
Epoch 450, training loss: 0.11505524814128876 = 0.04579871520400047 + 0.01 * 6.92565393447876
Epoch 450, val loss: 1.0531082153320312
Epoch 460, training loss: 0.11061876267194748 = 0.04145558923482895 + 0.01 * 6.916317462921143
Epoch 460, val loss: 1.0725972652435303
Epoch 470, training loss: 0.10692831873893738 = 0.037634458392858505 + 0.01 * 6.929385662078857
Epoch 470, val loss: 1.091530203819275
Epoch 480, training loss: 0.10338221490383148 = 0.03427133336663246 + 0.01 * 6.911087989807129
Epoch 480, val loss: 1.1098817586898804
Epoch 490, training loss: 0.10025229305028915 = 0.03130365163087845 + 0.01 * 6.894864559173584
Epoch 490, val loss: 1.127695083618164
Epoch 500, training loss: 0.09760289639234543 = 0.02867666631937027 + 0.01 * 6.892623424530029
Epoch 500, val loss: 1.1449353694915771
Epoch 510, training loss: 0.0952865481376648 = 0.026346223428845406 + 0.01 * 6.894032955169678
Epoch 510, val loss: 1.1616119146347046
Epoch 520, training loss: 0.09319687634706497 = 0.02427562139928341 + 0.01 * 6.892125129699707
Epoch 520, val loss: 1.1777454614639282
Epoch 530, training loss: 0.09129311889410019 = 0.022430509328842163 + 0.01 * 6.886261463165283
Epoch 530, val loss: 1.1933008432388306
Epoch 540, training loss: 0.08958713710308075 = 0.020781919360160828 + 0.01 * 6.88052225112915
Epoch 540, val loss: 1.2082632780075073
Epoch 550, training loss: 0.08806112408638 = 0.01930549368262291 + 0.01 * 6.87556266784668
Epoch 550, val loss: 1.2226773500442505
Epoch 560, training loss: 0.08676986396312714 = 0.017979850992560387 + 0.01 * 6.879001140594482
Epoch 560, val loss: 1.2365672588348389
Epoch 570, training loss: 0.08553445339202881 = 0.016785884276032448 + 0.01 * 6.874856948852539
Epoch 570, val loss: 1.2498908042907715
Epoch 580, training loss: 0.08434823155403137 = 0.015707407146692276 + 0.01 * 6.864083290100098
Epoch 580, val loss: 1.26279878616333
Epoch 590, training loss: 0.08348412066698074 = 0.014730172231793404 + 0.01 * 6.87539529800415
Epoch 590, val loss: 1.275260090827942
Epoch 600, training loss: 0.08238761872053146 = 0.013843946158885956 + 0.01 * 6.854367256164551
Epoch 600, val loss: 1.2872494459152222
Epoch 610, training loss: 0.08157765865325928 = 0.013037602417171001 + 0.01 * 6.854005813598633
Epoch 610, val loss: 1.2988442182540894
Epoch 620, training loss: 0.08093321323394775 = 0.012302170507609844 + 0.01 * 6.863104343414307
Epoch 620, val loss: 1.3099610805511475
Epoch 630, training loss: 0.08012247830629349 = 0.011630249209702015 + 0.01 * 6.8492231369018555
Epoch 630, val loss: 1.3207017183303833
Epoch 640, training loss: 0.07943563908338547 = 0.011014482006430626 + 0.01 * 6.84211540222168
Epoch 640, val loss: 1.3310950994491577
Epoch 650, training loss: 0.07879959791898727 = 0.010448803193867207 + 0.01 * 6.835079193115234
Epoch 650, val loss: 1.3411184549331665
Epoch 660, training loss: 0.0782846063375473 = 0.009928145445883274 + 0.01 * 6.835646152496338
Epoch 660, val loss: 1.3508204221725464
Epoch 670, training loss: 0.07778364419937134 = 0.009447786957025528 + 0.01 * 6.8335862159729
Epoch 670, val loss: 1.3601175546646118
Epoch 680, training loss: 0.07736817747354507 = 0.009003924205899239 + 0.01 * 6.83642578125
Epoch 680, val loss: 1.369217038154602
Epoch 690, training loss: 0.07694458216428757 = 0.008593335747718811 + 0.01 * 6.835124492645264
Epoch 690, val loss: 1.377990484237671
Epoch 700, training loss: 0.07639563828706741 = 0.008212854154407978 + 0.01 * 6.8182783126831055
Epoch 700, val loss: 1.3863708972930908
Epoch 710, training loss: 0.07605177164077759 = 0.007858982309699059 + 0.01 * 6.819278717041016
Epoch 710, val loss: 1.3946171998977661
Epoch 720, training loss: 0.07585464417934418 = 0.00752924894914031 + 0.01 * 6.832539081573486
Epoch 720, val loss: 1.4026464223861694
Epoch 730, training loss: 0.07550810277462006 = 0.007222189102321863 + 0.01 * 6.828591346740723
Epoch 730, val loss: 1.4104115962982178
Epoch 740, training loss: 0.0751415267586708 = 0.0069352793507277966 + 0.01 * 6.820625305175781
Epoch 740, val loss: 1.4179444313049316
Epoch 750, training loss: 0.0747281163930893 = 0.006667094770818949 + 0.01 * 6.806102275848389
Epoch 750, val loss: 1.4253252744674683
Epoch 760, training loss: 0.07459025084972382 = 0.006415861658751965 + 0.01 * 6.817439079284668
Epoch 760, val loss: 1.4324928522109985
Epoch 770, training loss: 0.07425666600465775 = 0.006180280353873968 + 0.01 * 6.807638645172119
Epoch 770, val loss: 1.4393407106399536
Epoch 780, training loss: 0.07405136525630951 = 0.0059589785523712635 + 0.01 * 6.809238433837891
Epoch 780, val loss: 1.4461766481399536
Epoch 790, training loss: 0.07370313256978989 = 0.005750966724008322 + 0.01 * 6.7952165603637695
Epoch 790, val loss: 1.4527956247329712
Epoch 800, training loss: 0.0734708309173584 = 0.005555217619985342 + 0.01 * 6.791561603546143
Epoch 800, val loss: 1.4592379331588745
Epoch 810, training loss: 0.07323773950338364 = 0.0053706965409219265 + 0.01 * 6.786704063415527
Epoch 810, val loss: 1.4654438495635986
Epoch 820, training loss: 0.07313313335180283 = 0.00519630778580904 + 0.01 * 6.793682098388672
Epoch 820, val loss: 1.4716001749038696
Epoch 830, training loss: 0.07285545766353607 = 0.005031582899391651 + 0.01 * 6.7823872566223145
Epoch 830, val loss: 1.4776328802108765
Epoch 840, training loss: 0.07273602485656738 = 0.004875781014561653 + 0.01 * 6.786024570465088
Epoch 840, val loss: 1.4835166931152344
Epoch 850, training loss: 0.07273321598768234 = 0.004728436935693026 + 0.01 * 6.800477981567383
Epoch 850, val loss: 1.4892878532409668
Epoch 860, training loss: 0.07241304963827133 = 0.004588673356920481 + 0.01 * 6.782438278198242
Epoch 860, val loss: 1.4948835372924805
Epoch 870, training loss: 0.07220092415809631 = 0.004456222523003817 + 0.01 * 6.774470329284668
Epoch 870, val loss: 1.5003384351730347
Epoch 880, training loss: 0.0720650926232338 = 0.0043304977007210255 + 0.01 * 6.773458957672119
Epoch 880, val loss: 1.505725383758545
Epoch 890, training loss: 0.07198185473680496 = 0.0042107985354959965 + 0.01 * 6.777105808258057
Epoch 890, val loss: 1.510891079902649
Epoch 900, training loss: 0.07194909453392029 = 0.00409697275608778 + 0.01 * 6.785212516784668
Epoch 900, val loss: 1.5160688161849976
Epoch 910, training loss: 0.07179735600948334 = 0.003988698590546846 + 0.01 * 6.7808661460876465
Epoch 910, val loss: 1.5210047960281372
Epoch 920, training loss: 0.07159630209207535 = 0.0038855974562466145 + 0.01 * 6.771070957183838
Epoch 920, val loss: 1.525789499282837
Epoch 930, training loss: 0.07139826565980911 = 0.0037872258108109236 + 0.01 * 6.761104106903076
Epoch 930, val loss: 1.5307116508483887
Epoch 940, training loss: 0.07143993675708771 = 0.0036932802759110928 + 0.01 * 6.774665832519531
Epoch 940, val loss: 1.5354734659194946
Epoch 950, training loss: 0.07122902572154999 = 0.0036036234814673662 + 0.01 * 6.762540817260742
Epoch 950, val loss: 1.5400817394256592
Epoch 960, training loss: 0.07108914107084274 = 0.003517881501466036 + 0.01 * 6.757126331329346
Epoch 960, val loss: 1.5447089672088623
Epoch 970, training loss: 0.07089539617300034 = 0.003435954451560974 + 0.01 * 6.745944499969482
Epoch 970, val loss: 1.5492552518844604
Epoch 980, training loss: 0.07078753411769867 = 0.003357470966875553 + 0.01 * 6.743006706237793
Epoch 980, val loss: 1.553729772567749
Epoch 990, training loss: 0.07067619264125824 = 0.003282300429418683 + 0.01 * 6.739388942718506
Epoch 990, val loss: 1.5582088232040405
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8386926726410122
The final CL Acc:0.80617, 0.01772, The final GNN Acc:0.83764, 0.00114
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10466])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.038080930709839 = 1.9521125555038452 + 0.01 * 8.596826553344727
Epoch 0, val loss: 1.9445157051086426
Epoch 10, training loss: 2.028654098510742 = 1.9426860809326172 + 0.01 * 8.596803665161133
Epoch 10, val loss: 1.9356664419174194
Epoch 20, training loss: 2.017230987548828 = 1.9312642812728882 + 0.01 * 8.59666919708252
Epoch 20, val loss: 1.9245004653930664
Epoch 30, training loss: 2.001239061355591 = 1.9152761697769165 + 0.01 * 8.596295356750488
Epoch 30, val loss: 1.9083852767944336
Epoch 40, training loss: 1.9775025844573975 = 1.8915544748306274 + 0.01 * 8.594817161560059
Epoch 40, val loss: 1.8843003511428833
Epoch 50, training loss: 1.9435603618621826 = 1.857709288597107 + 0.01 * 8.585112571716309
Epoch 50, val loss: 1.850707769393921
Epoch 60, training loss: 1.9034463167190552 = 1.8181402683258057 + 0.01 * 8.530603408813477
Epoch 60, val loss: 1.8146915435791016
Epoch 70, training loss: 1.8687388896942139 = 1.785267949104309 + 0.01 * 8.347094535827637
Epoch 70, val loss: 1.788390874862671
Epoch 80, training loss: 1.8318535089492798 = 1.7509249448776245 + 0.01 * 8.092855453491211
Epoch 80, val loss: 1.7587823867797852
Epoch 90, training loss: 1.7805075645446777 = 1.7027170658111572 + 0.01 * 7.779049873352051
Epoch 90, val loss: 1.7168278694152832
Epoch 100, training loss: 1.712347388267517 = 1.6362242698669434 + 0.01 * 7.612310886383057
Epoch 100, val loss: 1.6605557203292847
Epoch 110, training loss: 1.6230918169021606 = 1.548084020614624 + 0.01 * 7.500774383544922
Epoch 110, val loss: 1.5890604257583618
Epoch 120, training loss: 1.519165277481079 = 1.4445202350616455 + 0.01 * 7.464501857757568
Epoch 120, val loss: 1.5062414407730103
Epoch 130, training loss: 1.4106841087341309 = 1.3363124132156372 + 0.01 * 7.437173843383789
Epoch 130, val loss: 1.4218767881393433
Epoch 140, training loss: 1.3060429096221924 = 1.231930136680603 + 0.01 * 7.411276817321777
Epoch 140, val loss: 1.3414065837860107
Epoch 150, training loss: 1.20892333984375 = 1.1350736618041992 + 0.01 * 7.384969711303711
Epoch 150, val loss: 1.2683228254318237
Epoch 160, training loss: 1.1195636987686157 = 1.0460163354873657 + 0.01 * 7.354738712310791
Epoch 160, val loss: 1.2028354406356812
Epoch 170, training loss: 1.0372941493988037 = 0.9641286730766296 + 0.01 * 7.316551685333252
Epoch 170, val loss: 1.1446508169174194
Epoch 180, training loss: 0.9617828726768494 = 0.8889994025230408 + 0.01 * 7.278345584869385
Epoch 180, val loss: 1.0930458307266235
Epoch 190, training loss: 0.8929275274276733 = 0.8204708695411682 + 0.01 * 7.245663642883301
Epoch 190, val loss: 1.0470548868179321
Epoch 200, training loss: 0.8314415216445923 = 0.7591793537139893 + 0.01 * 7.226219177246094
Epoch 200, val loss: 1.0073221921920776
Epoch 210, training loss: 0.777723491191864 = 0.7056593298912048 + 0.01 * 7.206418037414551
Epoch 210, val loss: 0.9744817614555359
Epoch 220, training loss: 0.7311276197433472 = 0.6591960191726685 + 0.01 * 7.1931610107421875
Epoch 220, val loss: 0.9481936097145081
Epoch 230, training loss: 0.6895917654037476 = 0.6178094744682312 + 0.01 * 7.178228855133057
Epoch 230, val loss: 0.9272481799125671
Epoch 240, training loss: 0.6503348350524902 = 0.5786712765693665 + 0.01 * 7.166357517242432
Epoch 240, val loss: 0.9092923998832703
Epoch 250, training loss: 0.6110185980796814 = 0.5393716096878052 + 0.01 * 7.164697647094727
Epoch 250, val loss: 0.8923784494400024
Epoch 260, training loss: 0.5700134038925171 = 0.49845901131629944 + 0.01 * 7.155440807342529
Epoch 260, val loss: 0.8756392598152161
Epoch 270, training loss: 0.5269331932067871 = 0.45545148849487305 + 0.01 * 7.148169040679932
Epoch 270, val loss: 0.8589309453964233
Epoch 280, training loss: 0.4821179211139679 = 0.41066741943359375 + 0.01 * 7.145050525665283
Epoch 280, val loss: 0.8422237038612366
Epoch 290, training loss: 0.43639081716537476 = 0.36498206853866577 + 0.01 * 7.14087438583374
Epoch 290, val loss: 0.8257620334625244
Epoch 300, training loss: 0.3910687565803528 = 0.31969454884529114 + 0.01 * 7.137422561645508
Epoch 300, val loss: 0.8101754784584045
Epoch 310, training loss: 0.34756210446357727 = 0.27623623609542847 + 0.01 * 7.132587432861328
Epoch 310, val loss: 0.7965524792671204
Epoch 320, training loss: 0.30717983841896057 = 0.23584681749343872 + 0.01 * 7.133302211761475
Epoch 320, val loss: 0.7856322526931763
Epoch 330, training loss: 0.27069324254989624 = 0.19944287836551666 + 0.01 * 7.125035762786865
Epoch 330, val loss: 0.7782111763954163
Epoch 340, training loss: 0.23878976702690125 = 0.16766002774238586 + 0.01 * 7.112973213195801
Epoch 340, val loss: 0.774764358997345
Epoch 350, training loss: 0.21205052733421326 = 0.1408166140317917 + 0.01 * 7.1233906745910645
Epoch 350, val loss: 0.775397002696991
Epoch 360, training loss: 0.18976753950119019 = 0.11876089125871658 + 0.01 * 7.100665092468262
Epoch 360, val loss: 0.7795202732086182
Epoch 370, training loss: 0.17197564244270325 = 0.10086040198802948 + 0.01 * 7.1115241050720215
Epoch 370, val loss: 0.7863907217979431
Epoch 380, training loss: 0.15722572803497314 = 0.08635985851287842 + 0.01 * 7.086587905883789
Epoch 380, val loss: 0.7951596975326538
Epoch 390, training loss: 0.1455676257610321 = 0.07453729957342148 + 0.01 * 7.103032112121582
Epoch 390, val loss: 0.8051440119743347
Epoch 400, training loss: 0.13559472560882568 = 0.0648355558514595 + 0.01 * 7.075916290283203
Epoch 400, val loss: 0.81578129529953
Epoch 410, training loss: 0.12770907580852509 = 0.056796129792928696 + 0.01 * 7.091294765472412
Epoch 410, val loss: 0.8267967700958252
Epoch 420, training loss: 0.12077111005783081 = 0.050092607736587524 + 0.01 * 7.067850112915039
Epoch 420, val loss: 0.8379865884780884
Epoch 430, training loss: 0.1149878203868866 = 0.04445313662290573 + 0.01 * 7.053468227386475
Epoch 430, val loss: 0.8492250442504883
Epoch 440, training loss: 0.11014501750469208 = 0.03967277333140373 + 0.01 * 7.047224044799805
Epoch 440, val loss: 0.8603690266609192
Epoch 450, training loss: 0.10619333386421204 = 0.03560006245970726 + 0.01 * 7.059327602386475
Epoch 450, val loss: 0.8713411688804626
Epoch 460, training loss: 0.10254150629043579 = 0.03211313858628273 + 0.01 * 7.042837142944336
Epoch 460, val loss: 0.8821111917495728
Epoch 470, training loss: 0.09936913102865219 = 0.029104383662343025 + 0.01 * 7.026475429534912
Epoch 470, val loss: 0.8927047252655029
Epoch 480, training loss: 0.09691167622804642 = 0.026492014527320862 + 0.01 * 7.041965961456299
Epoch 480, val loss: 0.9030365347862244
Epoch 490, training loss: 0.09436263144016266 = 0.024217650294303894 + 0.01 * 7.014498233795166
Epoch 490, val loss: 0.9131067991256714
Epoch 500, training loss: 0.0923679769039154 = 0.022225642576813698 + 0.01 * 7.0142340660095215
Epoch 500, val loss: 0.9229200482368469
Epoch 510, training loss: 0.09064643085002899 = 0.02047046646475792 + 0.01 * 7.017596244812012
Epoch 510, val loss: 0.9324687123298645
Epoch 520, training loss: 0.08898916095495224 = 0.018919410184025764 + 0.01 * 7.0069756507873535
Epoch 520, val loss: 0.9417153596878052
Epoch 530, training loss: 0.08751798421144485 = 0.017540939152240753 + 0.01 * 6.997704982757568
Epoch 530, val loss: 0.9506822228431702
Epoch 540, training loss: 0.08625852316617966 = 0.01630675606429577 + 0.01 * 6.995176315307617
Epoch 540, val loss: 0.9594698548316956
Epoch 550, training loss: 0.08506602048873901 = 0.015199007466435432 + 0.01 * 6.986701488494873
Epoch 550, val loss: 0.9679400324821472
Epoch 560, training loss: 0.0840429738163948 = 0.014199809171259403 + 0.01 * 6.984316349029541
Epoch 560, val loss: 0.9762570858001709
Epoch 570, training loss: 0.08320388942956924 = 0.013296735472977161 + 0.01 * 6.990715503692627
Epoch 570, val loss: 0.9842645525932312
Epoch 580, training loss: 0.08223750442266464 = 0.012477517127990723 + 0.01 * 6.975998878479004
Epoch 580, val loss: 0.992114245891571
Epoch 590, training loss: 0.08145292848348618 = 0.011732361279428005 + 0.01 * 6.972056865692139
Epoch 590, val loss: 0.9997701048851013
Epoch 600, training loss: 0.08060487359762192 = 0.0110556585714221 + 0.01 * 6.954922199249268
Epoch 600, val loss: 1.0071804523468018
Epoch 610, training loss: 0.08011949807405472 = 0.010438144207000732 + 0.01 * 6.968135356903076
Epoch 610, val loss: 1.014373779296875
Epoch 620, training loss: 0.07944685965776443 = 0.009872674942016602 + 0.01 * 6.957418918609619
Epoch 620, val loss: 1.0214362144470215
Epoch 630, training loss: 0.07893634587526321 = 0.009354925714433193 + 0.01 * 6.958141803741455
Epoch 630, val loss: 1.02823007106781
Epoch 640, training loss: 0.07832230627536774 = 0.008878747932612896 + 0.01 * 6.9443559646606445
Epoch 640, val loss: 1.0349245071411133
Epoch 650, training loss: 0.07797059416770935 = 0.008440176025032997 + 0.01 * 6.953042507171631
Epoch 650, val loss: 1.0413610935211182
Epoch 660, training loss: 0.07738593220710754 = 0.008035183884203434 + 0.01 * 6.935074806213379
Epoch 660, val loss: 1.0476771593093872
Epoch 670, training loss: 0.0769464299082756 = 0.007660781033337116 + 0.01 * 6.92856502532959
Epoch 670, val loss: 1.0538220405578613
Epoch 680, training loss: 0.07656417787075043 = 0.007314345333725214 + 0.01 * 6.924983501434326
Epoch 680, val loss: 1.0598007440567017
Epoch 690, training loss: 0.07646539062261581 = 0.006992687936872244 + 0.01 * 6.947269916534424
Epoch 690, val loss: 1.06564462184906
Epoch 700, training loss: 0.07587841898202896 = 0.006694071926176548 + 0.01 * 6.9184346199035645
Epoch 700, val loss: 1.0713069438934326
Epoch 710, training loss: 0.07565680891275406 = 0.006415718700736761 + 0.01 * 6.92410945892334
Epoch 710, val loss: 1.076844334602356
Epoch 720, training loss: 0.0754011794924736 = 0.006156166549772024 + 0.01 * 6.924501419067383
Epoch 720, val loss: 1.0822492837905884
Epoch 730, training loss: 0.07493758946657181 = 0.00591372512280941 + 0.01 * 6.902386665344238
Epoch 730, val loss: 1.0874987840652466
Epoch 740, training loss: 0.07474936544895172 = 0.005686864722520113 + 0.01 * 6.90625
Epoch 740, val loss: 1.092665672302246
Epoch 750, training loss: 0.07448191940784454 = 0.005474972538650036 + 0.01 * 6.900694847106934
Epoch 750, val loss: 1.0975614786148071
Epoch 760, training loss: 0.07432039082050323 = 0.005276036914438009 + 0.01 * 6.904435157775879
Epoch 760, val loss: 1.1024693250656128
Epoch 770, training loss: 0.07399892807006836 = 0.005089121405035257 + 0.01 * 6.890981197357178
Epoch 770, val loss: 1.1072430610656738
Epoch 780, training loss: 0.0740126371383667 = 0.004913142882287502 + 0.01 * 6.909949779510498
Epoch 780, val loss: 1.111923098564148
Epoch 790, training loss: 0.07355815172195435 = 0.004747728351503611 + 0.01 * 6.881042957305908
Epoch 790, val loss: 1.1164164543151855
Epoch 800, training loss: 0.07345601171255112 = 0.004591846372932196 + 0.01 * 6.886416912078857
Epoch 800, val loss: 1.1208099126815796
Epoch 810, training loss: 0.07326410710811615 = 0.004444606136530638 + 0.01 * 6.8819499015808105
Epoch 810, val loss: 1.1251907348632812
Epoch 820, training loss: 0.07311262935400009 = 0.004305543377995491 + 0.01 * 6.880709171295166
Epoch 820, val loss: 1.1294031143188477
Epoch 830, training loss: 0.07298466563224792 = 0.004174038767814636 + 0.01 * 6.8810625076293945
Epoch 830, val loss: 1.1335045099258423
Epoch 840, training loss: 0.07271142303943634 = 0.004049467854201794 + 0.01 * 6.866196155548096
Epoch 840, val loss: 1.1375714540481567
Epoch 850, training loss: 0.0726613998413086 = 0.003931224346160889 + 0.01 * 6.87301778793335
Epoch 850, val loss: 1.14155113697052
Epoch 860, training loss: 0.07241957634687424 = 0.0038191676139831543 + 0.01 * 6.86004114151001
Epoch 860, val loss: 1.1453498601913452
Epoch 870, training loss: 0.07251575589179993 = 0.003712722333148122 + 0.01 * 6.880303859710693
Epoch 870, val loss: 1.1492445468902588
Epoch 880, training loss: 0.07211193442344666 = 0.003611865919083357 + 0.01 * 6.8500075340271
Epoch 880, val loss: 1.1527955532073975
Epoch 890, training loss: 0.0719342902302742 = 0.003515679156407714 + 0.01 * 6.841861724853516
Epoch 890, val loss: 1.1564631462097168
Epoch 900, training loss: 0.07184185087680817 = 0.003423952730372548 + 0.01 * 6.841789722442627
Epoch 900, val loss: 1.159914493560791
Epoch 910, training loss: 0.07204243540763855 = 0.00333644961938262 + 0.01 * 6.870598793029785
Epoch 910, val loss: 1.1635075807571411
Epoch 920, training loss: 0.07174887508153915 = 0.00325327692553401 + 0.01 * 6.849560260772705
Epoch 920, val loss: 1.1668552160263062
Epoch 930, training loss: 0.07174216955900192 = 0.003173821372911334 + 0.01 * 6.85683536529541
Epoch 930, val loss: 1.17010498046875
Epoch 940, training loss: 0.07141613960266113 = 0.0030980848241597414 + 0.01 * 6.831805229187012
Epoch 940, val loss: 1.1733332872390747
Epoch 950, training loss: 0.07145780324935913 = 0.003025731770321727 + 0.01 * 6.843207359313965
Epoch 950, val loss: 1.1764304637908936
Epoch 960, training loss: 0.07121090590953827 = 0.0029563058633357286 + 0.01 * 6.825459957122803
Epoch 960, val loss: 1.179573655128479
Epoch 970, training loss: 0.0712025836110115 = 0.002889819210395217 + 0.01 * 6.831276893615723
Epoch 970, val loss: 1.1825870275497437
Epoch 980, training loss: 0.07115015387535095 = 0.0028261414263397455 + 0.01 * 6.832401752471924
Epoch 980, val loss: 1.1855549812316895
Epoch 990, training loss: 0.07097078114748001 = 0.002765054116025567 + 0.01 * 6.820573329925537
Epoch 990, val loss: 1.188485860824585
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8007380073800738
=== training gcn model ===
Epoch 0, training loss: 2.038024663925171 = 1.9520561695098877 + 0.01 * 8.5968599319458
Epoch 0, val loss: 1.95203697681427
Epoch 10, training loss: 2.0270395278930664 = 1.9410713911056519 + 0.01 * 8.596810340881348
Epoch 10, val loss: 1.9408786296844482
Epoch 20, training loss: 2.013047933578491 = 1.9270813465118408 + 0.01 * 8.596668243408203
Epoch 20, val loss: 1.9267596006393433
Epoch 30, training loss: 1.9931538105010986 = 1.9071911573410034 + 0.01 * 8.596261024475098
Epoch 30, val loss: 1.90691077709198
Epoch 40, training loss: 1.964175820350647 = 1.8782328367233276 + 0.01 * 8.5942964553833
Epoch 40, val loss: 1.8786026239395142
Epoch 50, training loss: 1.9263287782669067 = 1.8405307531356812 + 0.01 * 8.579797744750977
Epoch 50, val loss: 1.8436037302017212
Epoch 60, training loss: 1.8892402648925781 = 1.8042361736297607 + 0.01 * 8.50040340423584
Epoch 60, val loss: 1.8127180337905884
Epoch 70, training loss: 1.857522964477539 = 1.7737854719161987 + 0.01 * 8.373749732971191
Epoch 70, val loss: 1.7850260734558105
Epoch 80, training loss: 1.813914179801941 = 1.7317746877670288 + 0.01 * 8.213949203491211
Epoch 80, val loss: 1.746190071105957
Epoch 90, training loss: 1.7522510290145874 = 1.6714537143707275 + 0.01 * 8.079727172851562
Epoch 90, val loss: 1.6946789026260376
Epoch 100, training loss: 1.6724247932434082 = 1.5922892093658447 + 0.01 * 8.013557434082031
Epoch 100, val loss: 1.6323074102401733
Epoch 110, training loss: 1.5862338542938232 = 1.5077491998672485 + 0.01 * 7.848465919494629
Epoch 110, val loss: 1.5681424140930176
Epoch 120, training loss: 1.5064456462860107 = 1.4295188188552856 + 0.01 * 7.692683219909668
Epoch 120, val loss: 1.5116324424743652
Epoch 130, training loss: 1.4347623586654663 = 1.3584460020065308 + 0.01 * 7.631640911102295
Epoch 130, val loss: 1.4624515771865845
Epoch 140, training loss: 1.3671966791152954 = 1.2913216352462769 + 0.01 * 7.587509632110596
Epoch 140, val loss: 1.418201208114624
Epoch 150, training loss: 1.3026679754257202 = 1.2272582054138184 + 0.01 * 7.540973663330078
Epoch 150, val loss: 1.3785237073898315
Epoch 160, training loss: 1.2399251461029053 = 1.165254831314087 + 0.01 * 7.467036724090576
Epoch 160, val loss: 1.342341423034668
Epoch 170, training loss: 1.1761716604232788 = 1.1023691892623901 + 0.01 * 7.380243301391602
Epoch 170, val loss: 1.3051841259002686
Epoch 180, training loss: 1.1094282865524292 = 1.0360645055770874 + 0.01 * 7.336373805999756
Epoch 180, val loss: 1.2640252113342285
Epoch 190, training loss: 1.0392091274261475 = 0.9661208391189575 + 0.01 * 7.308828830718994
Epoch 190, val loss: 1.2182443141937256
Epoch 200, training loss: 0.9671244621276855 = 0.8941881656646729 + 0.01 * 7.293630123138428
Epoch 200, val loss: 1.1701747179031372
Epoch 210, training loss: 0.895514726638794 = 0.8227237462997437 + 0.01 * 7.279097557067871
Epoch 210, val loss: 1.122170329093933
Epoch 220, training loss: 0.8270524144172668 = 0.7543855905532837 + 0.01 * 7.266680717468262
Epoch 220, val loss: 1.0769273042678833
Epoch 230, training loss: 0.7639722228050232 = 0.691405177116394 + 0.01 * 7.256707191467285
Epoch 230, val loss: 1.0369256734848022
Epoch 240, training loss: 0.7069773077964783 = 0.6345093250274658 + 0.01 * 7.246797561645508
Epoch 240, val loss: 1.0033047199249268
Epoch 250, training loss: 0.6552712917327881 = 0.5828493237495422 + 0.01 * 7.242199420928955
Epoch 250, val loss: 0.9762747287750244
Epoch 260, training loss: 0.6076236963272095 = 0.5352814197540283 + 0.01 * 7.2342305183410645
Epoch 260, val loss: 0.9554993510246277
Epoch 270, training loss: 0.5634241700172424 = 0.49115172028541565 + 0.01 * 7.227243900299072
Epoch 270, val loss: 0.9403003454208374
Epoch 280, training loss: 0.5223666429519653 = 0.4501544237136841 + 0.01 * 7.221222400665283
Epoch 280, val loss: 0.9301363825798035
Epoch 290, training loss: 0.4841574728488922 = 0.41200822591781616 + 0.01 * 7.214923858642578
Epoch 290, val loss: 0.9245696663856506
Epoch 300, training loss: 0.4481233060359955 = 0.3760162591934204 + 0.01 * 7.210705757141113
Epoch 300, val loss: 0.9226975440979004
Epoch 310, training loss: 0.4134165942668915 = 0.3413747549057007 + 0.01 * 7.204183578491211
Epoch 310, val loss: 0.923734188079834
Epoch 320, training loss: 0.3796241581439972 = 0.3075602054595947 + 0.01 * 7.206395149230957
Epoch 320, val loss: 0.9268678426742554
Epoch 330, training loss: 0.3466322422027588 = 0.2746393084526062 + 0.01 * 7.199292182922363
Epoch 330, val loss: 0.9315300583839417
Epoch 340, training loss: 0.31512925028800964 = 0.24319139122962952 + 0.01 * 7.193786144256592
Epoch 340, val loss: 0.937721848487854
Epoch 350, training loss: 0.2858515977859497 = 0.21396195888519287 + 0.01 * 7.188962459564209
Epoch 350, val loss: 0.9455604553222656
Epoch 360, training loss: 0.2593116760253906 = 0.18742510676383972 + 0.01 * 7.188658237457275
Epoch 360, val loss: 0.9549797773361206
Epoch 370, training loss: 0.23560616374015808 = 0.16372238099575043 + 0.01 * 7.188377380371094
Epoch 370, val loss: 0.9658156633377075
Epoch 380, training loss: 0.21463312208652496 = 0.1428147256374359 + 0.01 * 7.181839466094971
Epoch 380, val loss: 0.9778236746788025
Epoch 390, training loss: 0.1962900459766388 = 0.12452609091997147 + 0.01 * 7.176394939422607
Epoch 390, val loss: 0.990900993347168
Epoch 400, training loss: 0.1804097443819046 = 0.10860241204500198 + 0.01 * 7.1807332038879395
Epoch 400, val loss: 1.0046939849853516
Epoch 410, training loss: 0.16652165353298187 = 0.0947749987244606 + 0.01 * 7.174665927886963
Epoch 410, val loss: 1.0190105438232422
Epoch 420, training loss: 0.15447898209095 = 0.08278500288724899 + 0.01 * 7.169398307800293
Epoch 420, val loss: 1.0338048934936523
Epoch 430, training loss: 0.1440494954586029 = 0.07241185754537582 + 0.01 * 7.163764476776123
Epoch 430, val loss: 1.048884391784668
Epoch 440, training loss: 0.1350460946559906 = 0.0634630024433136 + 0.01 * 7.158308982849121
Epoch 440, val loss: 1.0641218423843384
Epoch 450, training loss: 0.12735308706760406 = 0.05577126517891884 + 0.01 * 7.158182621002197
Epoch 450, val loss: 1.079477071762085
Epoch 460, training loss: 0.12069228291511536 = 0.04916809871792793 + 0.01 * 7.152418613433838
Epoch 460, val loss: 1.0947386026382446
Epoch 470, training loss: 0.11497533321380615 = 0.043503712862730026 + 0.01 * 7.147162914276123
Epoch 470, val loss: 1.109898328781128
Epoch 480, training loss: 0.11013080179691315 = 0.03864699974656105 + 0.01 * 7.148379802703857
Epoch 480, val loss: 1.1247771978378296
Epoch 490, training loss: 0.10585677623748779 = 0.0344812273979187 + 0.01 * 7.137555122375488
Epoch 490, val loss: 1.1394100189208984
Epoch 500, training loss: 0.10225500166416168 = 0.030897069722414017 + 0.01 * 7.135793209075928
Epoch 500, val loss: 1.153673529624939
Epoch 510, training loss: 0.09911182522773743 = 0.027806654572486877 + 0.01 * 7.130517482757568
Epoch 510, val loss: 1.1675745248794556
Epoch 520, training loss: 0.09634233266115189 = 0.025131167843937874 + 0.01 * 7.121116638183594
Epoch 520, val loss: 1.1809911727905273
Epoch 530, training loss: 0.09398441761732101 = 0.022804824635386467 + 0.01 * 7.117959022521973
Epoch 530, val loss: 1.1940340995788574
Epoch 540, training loss: 0.09190885722637177 = 0.02077614702284336 + 0.01 * 7.113271713256836
Epoch 540, val loss: 1.206613302230835
Epoch 550, training loss: 0.09010693430900574 = 0.019000403583049774 + 0.01 * 7.110653400421143
Epoch 550, val loss: 1.218714952468872
Epoch 560, training loss: 0.08842982351779938 = 0.017438258975744247 + 0.01 * 7.099156856536865
Epoch 560, val loss: 1.2303279638290405
Epoch 570, training loss: 0.08733844012022018 = 0.01605849899351597 + 0.01 * 7.127994537353516
Epoch 570, val loss: 1.2415504455566406
Epoch 580, training loss: 0.08569637686014175 = 0.014840813353657722 + 0.01 * 7.085556507110596
Epoch 580, val loss: 1.2523707151412964
Epoch 590, training loss: 0.08464587479829788 = 0.013760251924395561 + 0.01 * 7.088562488555908
Epoch 590, val loss: 1.2627174854278564
Epoch 600, training loss: 0.08376477658748627 = 0.01279616728425026 + 0.01 * 7.096861362457275
Epoch 600, val loss: 1.2728673219680786
Epoch 610, training loss: 0.08267517387866974 = 0.011933728121221066 + 0.01 * 7.07414436340332
Epoch 610, val loss: 1.2825204133987427
Epoch 620, training loss: 0.08178894221782684 = 0.011159746907651424 + 0.01 * 7.062919616699219
Epoch 620, val loss: 1.291906476020813
Epoch 630, training loss: 0.08125929534435272 = 0.01046218816190958 + 0.01 * 7.079710960388184
Epoch 630, val loss: 1.3010202646255493
Epoch 640, training loss: 0.08038625121116638 = 0.009832505136728287 + 0.01 * 7.055374622344971
Epoch 640, val loss: 1.3097201585769653
Epoch 650, training loss: 0.07977220416069031 = 0.009261764585971832 + 0.01 * 7.051044464111328
Epoch 650, val loss: 1.3181884288787842
Epoch 660, training loss: 0.07917604595422745 = 0.00874323584139347 + 0.01 * 7.043281555175781
Epoch 660, val loss: 1.3263468742370605
Epoch 670, training loss: 0.07863948494195938 = 0.008270515128970146 + 0.01 * 7.0368971824646
Epoch 670, val loss: 1.3342399597167969
Epoch 680, training loss: 0.07816759496927261 = 0.007838545367121696 + 0.01 * 7.032904624938965
Epoch 680, val loss: 1.3418835401535034
Epoch 690, training loss: 0.07778223603963852 = 0.00744348531588912 + 0.01 * 7.033874988555908
Epoch 690, val loss: 1.3491538763046265
Epoch 700, training loss: 0.07739076763391495 = 0.0070805782452225685 + 0.01 * 7.03101921081543
Epoch 700, val loss: 1.3562901020050049
Epoch 710, training loss: 0.07687535881996155 = 0.006746490951627493 + 0.01 * 7.012887001037598
Epoch 710, val loss: 1.363175630569458
Epoch 720, training loss: 0.07687166333198547 = 0.006437982432544231 + 0.01 * 7.043368339538574
Epoch 720, val loss: 1.3698675632476807
Epoch 730, training loss: 0.0762069821357727 = 0.006153019145131111 + 0.01 * 7.005396366119385
Epoch 730, val loss: 1.3763006925582886
Epoch 740, training loss: 0.07616216689348221 = 0.005889131687581539 + 0.01 * 7.027303218841553
Epoch 740, val loss: 1.3825490474700928
Epoch 750, training loss: 0.07564259320497513 = 0.005644521676003933 + 0.01 * 6.999807357788086
Epoch 750, val loss: 1.3885352611541748
Epoch 760, training loss: 0.07553910464048386 = 0.00541672995314002 + 0.01 * 7.012238025665283
Epoch 760, val loss: 1.3944857120513916
Epoch 770, training loss: 0.0751066580414772 = 0.00520456675440073 + 0.01 * 6.990209579467773
Epoch 770, val loss: 1.4001301527023315
Epoch 780, training loss: 0.07477667927742004 = 0.005006515420973301 + 0.01 * 6.977016925811768
Epoch 780, val loss: 1.4057344198226929
Epoch 790, training loss: 0.0745997428894043 = 0.004821364767849445 + 0.01 * 6.977838039398193
Epoch 790, val loss: 1.4110418558120728
Epoch 800, training loss: 0.07436425983905792 = 0.004647854715585709 + 0.01 * 6.971640586853027
Epoch 800, val loss: 1.4162704944610596
Epoch 810, training loss: 0.07426843047142029 = 0.00448534544557333 + 0.01 * 6.97830867767334
Epoch 810, val loss: 1.421360969543457
Epoch 820, training loss: 0.07390902191400528 = 0.0043326690793037415 + 0.01 * 6.957635402679443
Epoch 820, val loss: 1.4263256788253784
Epoch 830, training loss: 0.07377637922763824 = 0.004189292900264263 + 0.01 * 6.958708763122559
Epoch 830, val loss: 1.4310160875320435
Epoch 840, training loss: 0.07352526485919952 = 0.004054142162203789 + 0.01 * 6.947112083435059
Epoch 840, val loss: 1.4357073307037354
Epoch 850, training loss: 0.07344869524240494 = 0.003926571924239397 + 0.01 * 6.952212333679199
Epoch 850, val loss: 1.4402527809143066
Epoch 860, training loss: 0.07322559505701065 = 0.0038061675149947405 + 0.01 * 6.9419426918029785
Epoch 860, val loss: 1.4446280002593994
Epoch 870, training loss: 0.07310835272073746 = 0.0036922337021678686 + 0.01 * 6.941612243652344
Epoch 870, val loss: 1.44895601272583
Epoch 880, training loss: 0.07302729040384293 = 0.0035843674559146166 + 0.01 * 6.9442925453186035
Epoch 880, val loss: 1.4531828165054321
Epoch 890, training loss: 0.07326097041368484 = 0.003482423722743988 + 0.01 * 6.9778547286987305
Epoch 890, val loss: 1.4572367668151855
Epoch 900, training loss: 0.07267235964536667 = 0.0033857792150229216 + 0.01 * 6.9286580085754395
Epoch 900, val loss: 1.4611693620681763
Epoch 910, training loss: 0.07251214236021042 = 0.0032939836382865906 + 0.01 * 6.921815872192383
Epoch 910, val loss: 1.4650664329528809
Epoch 920, training loss: 0.07237054407596588 = 0.0032067690044641495 + 0.01 * 6.916377544403076
Epoch 920, val loss: 1.468788981437683
Epoch 930, training loss: 0.07245736569166183 = 0.0031235511414706707 + 0.01 * 6.9333815574646
Epoch 930, val loss: 1.4724701642990112
Epoch 940, training loss: 0.07215951383113861 = 0.0030444003641605377 + 0.01 * 6.9115118980407715
Epoch 940, val loss: 1.4761261940002441
Epoch 950, training loss: 0.07245833426713943 = 0.0029692004900425673 + 0.01 * 6.94891357421875
Epoch 950, val loss: 1.4796245098114014
Epoch 960, training loss: 0.07211030274629593 = 0.0028975801542401314 + 0.01 * 6.921271800994873
Epoch 960, val loss: 1.4830152988433838
Epoch 970, training loss: 0.07179707288742065 = 0.0028291349299252033 + 0.01 * 6.896793842315674
Epoch 970, val loss: 1.4862743616104126
Epoch 980, training loss: 0.07165557146072388 = 0.0027637025341391563 + 0.01 * 6.889186859130859
Epoch 980, val loss: 1.4895727634429932
Epoch 990, training loss: 0.07174617797136307 = 0.002701143966987729 + 0.01 * 6.90450382232666
Epoch 990, val loss: 1.4927183389663696
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.7981022667369532
=== training gcn model ===
Epoch 0, training loss: 2.034184217453003 = 1.948216199874878 + 0.01 * 8.596795082092285
Epoch 0, val loss: 1.9461674690246582
Epoch 10, training loss: 2.023646831512451 = 1.937679648399353 + 0.01 * 8.596710205078125
Epoch 10, val loss: 1.9355264902114868
Epoch 20, training loss: 2.010679006576538 = 1.9247139692306519 + 0.01 * 8.596501350402832
Epoch 20, val loss: 1.922326683998108
Epoch 30, training loss: 1.9927181005477905 = 1.9067583084106445 + 0.01 * 8.595975875854492
Epoch 30, val loss: 1.9039939641952515
Epoch 40, training loss: 1.9667377471923828 = 1.880802035331726 + 0.01 * 8.593576431274414
Epoch 40, val loss: 1.8778380155563354
Epoch 50, training loss: 1.9315228462219238 = 1.8457555770874023 + 0.01 * 8.576723098754883
Epoch 50, val loss: 1.8439905643463135
Epoch 60, training loss: 1.8940128087997437 = 1.8091833591461182 + 0.01 * 8.482949256896973
Epoch 60, val loss: 1.8119813203811646
Epoch 70, training loss: 1.8635975122451782 = 1.7803597450256348 + 0.01 * 8.32377815246582
Epoch 70, val loss: 1.7874326705932617
Epoch 80, training loss: 1.824870228767395 = 1.7443770170211792 + 0.01 * 8.049323081970215
Epoch 80, val loss: 1.7525980472564697
Epoch 90, training loss: 1.7716962099075317 = 1.694199562072754 + 0.01 * 7.749669551849365
Epoch 90, val loss: 1.7066879272460938
Epoch 100, training loss: 1.7006165981292725 = 1.6258641481399536 + 0.01 * 7.475241661071777
Epoch 100, val loss: 1.6489181518554688
Epoch 110, training loss: 1.6170270442962646 = 1.543066143989563 + 0.01 * 7.396093845367432
Epoch 110, val loss: 1.5796531438827515
Epoch 120, training loss: 1.5291354656219482 = 1.4555346965789795 + 0.01 * 7.360075950622559
Epoch 120, val loss: 1.5078858137130737
Epoch 130, training loss: 1.4414645433425903 = 1.368080973625183 + 0.01 * 7.338359832763672
Epoch 130, val loss: 1.4372252225875854
Epoch 140, training loss: 1.3520632982254028 = 1.2788408994674683 + 0.01 * 7.32223653793335
Epoch 140, val loss: 1.3675715923309326
Epoch 150, training loss: 1.2590371370315552 = 1.1859689950942993 + 0.01 * 7.306819438934326
Epoch 150, val loss: 1.296908974647522
Epoch 160, training loss: 1.162574291229248 = 1.089713215827942 + 0.01 * 7.28610372543335
Epoch 160, val loss: 1.2250648736953735
Epoch 170, training loss: 1.0651676654815674 = 0.9925147891044617 + 0.01 * 7.265283107757568
Epoch 170, val loss: 1.1539138555526733
Epoch 180, training loss: 0.9715051651000977 = 0.8990339636802673 + 0.01 * 7.247119426727295
Epoch 180, val loss: 1.087719440460205
Epoch 190, training loss: 0.886273980140686 = 0.814007580280304 + 0.01 * 7.2266411781311035
Epoch 190, val loss: 1.0303940773010254
Epoch 200, training loss: 0.8120744228363037 = 0.7399640083312988 + 0.01 * 7.211042881011963
Epoch 200, val loss: 0.9846153259277344
Epoch 210, training loss: 0.7483228445053101 = 0.676400899887085 + 0.01 * 7.19219446182251
Epoch 210, val loss: 0.9499613642692566
Epoch 220, training loss: 0.6928411722183228 = 0.6210969090461731 + 0.01 * 7.174423694610596
Epoch 220, val loss: 0.9242300987243652
Epoch 230, training loss: 0.6433826088905334 = 0.5717949271202087 + 0.01 * 7.1587677001953125
Epoch 230, val loss: 0.9054247140884399
Epoch 240, training loss: 0.5981991291046143 = 0.5266976952552795 + 0.01 * 7.150143623352051
Epoch 240, val loss: 0.8917497992515564
Epoch 250, training loss: 0.5561783909797668 = 0.4847686290740967 + 0.01 * 7.140975475311279
Epoch 250, val loss: 0.8823179006576538
Epoch 260, training loss: 0.5166133642196655 = 0.4453575015068054 + 0.01 * 7.125584125518799
Epoch 260, val loss: 0.8762198686599731
Epoch 270, training loss: 0.4792049825191498 = 0.4079781770706177 + 0.01 * 7.122681617736816
Epoch 270, val loss: 0.8728987574577332
Epoch 280, training loss: 0.4436299204826355 = 0.3724334239959717 + 0.01 * 7.119650363922119
Epoch 280, val loss: 0.8718340396881104
Epoch 290, training loss: 0.4097675085067749 = 0.33859676122665405 + 0.01 * 7.117076396942139
Epoch 290, val loss: 0.8729206323623657
Epoch 300, training loss: 0.37790796160697937 = 0.30662375688552856 + 0.01 * 7.128419876098633
Epoch 300, val loss: 0.8762400150299072
Epoch 310, training loss: 0.34770646691322327 = 0.27674102783203125 + 0.01 * 7.096543788909912
Epoch 310, val loss: 0.8821967244148254
Epoch 320, training loss: 0.3200022876262665 = 0.24909166991710663 + 0.01 * 7.091062545776367
Epoch 320, val loss: 0.8910015225410461
Epoch 330, training loss: 0.29465484619140625 = 0.2237335592508316 + 0.01 * 7.092128276824951
Epoch 330, val loss: 0.9024040102958679
Epoch 340, training loss: 0.27164626121520996 = 0.20065931975841522 + 0.01 * 7.09869384765625
Epoch 340, val loss: 0.9162327647209167
Epoch 350, training loss: 0.25079643726348877 = 0.17978884279727936 + 0.01 * 7.100760459899902
Epoch 350, val loss: 0.9321866035461426
Epoch 360, training loss: 0.23188084363937378 = 0.16102659702301025 + 0.01 * 7.085423946380615
Epoch 360, val loss: 0.9502055048942566
Epoch 370, training loss: 0.21497531235218048 = 0.14423616230487823 + 0.01 * 7.073915481567383
Epoch 370, val loss: 0.9699088931083679
Epoch 380, training loss: 0.19992998242378235 = 0.12926076352596283 + 0.01 * 7.066922664642334
Epoch 380, val loss: 0.991036057472229
Epoch 390, training loss: 0.18713825941085815 = 0.1159358099102974 + 0.01 * 7.120245933532715
Epoch 390, val loss: 1.0132423639297485
Epoch 400, training loss: 0.17471495270729065 = 0.10413101315498352 + 0.01 * 7.0583930015563965
Epoch 400, val loss: 1.0360727310180664
Epoch 410, training loss: 0.1642182469367981 = 0.09366784989833832 + 0.01 * 7.0550408363342285
Epoch 410, val loss: 1.0594120025634766
Epoch 420, training loss: 0.1549651324748993 = 0.08439680933952332 + 0.01 * 7.056832313537598
Epoch 420, val loss: 1.0829989910125732
Epoch 430, training loss: 0.14675572514533997 = 0.07618498802185059 + 0.01 * 7.057074069976807
Epoch 430, val loss: 1.1066782474517822
Epoch 440, training loss: 0.13939020037651062 = 0.06891241669654846 + 0.01 * 7.047779083251953
Epoch 440, val loss: 1.1301608085632324
Epoch 450, training loss: 0.13294968008995056 = 0.062469325959682465 + 0.01 * 7.048036575317383
Epoch 450, val loss: 1.1534011363983154
Epoch 460, training loss: 0.12747161090373993 = 0.05675780028104782 + 0.01 * 7.071381092071533
Epoch 460, val loss: 1.176228404045105
Epoch 470, training loss: 0.12223070859909058 = 0.05169406533241272 + 0.01 * 7.053664684295654
Epoch 470, val loss: 1.1986308097839355
Epoch 480, training loss: 0.11757861077785492 = 0.04719637706875801 + 0.01 * 7.038222789764404
Epoch 480, val loss: 1.2204266786575317
Epoch 490, training loss: 0.11352623999118805 = 0.04319602996110916 + 0.01 * 7.033021450042725
Epoch 490, val loss: 1.2417497634887695
Epoch 500, training loss: 0.11023926734924316 = 0.03963546082377434 + 0.01 * 7.060380935668945
Epoch 500, val loss: 1.2625714540481567
Epoch 510, training loss: 0.10677371919155121 = 0.03646104410290718 + 0.01 * 7.031268119812012
Epoch 510, val loss: 1.2826746702194214
Epoch 520, training loss: 0.10381878167390823 = 0.03362308442592621 + 0.01 * 7.0195698738098145
Epoch 520, val loss: 1.3023265600204468
Epoch 530, training loss: 0.1012982428073883 = 0.031078536063432693 + 0.01 * 7.021970272064209
Epoch 530, val loss: 1.321393370628357
Epoch 540, training loss: 0.09912029653787613 = 0.028793474659323692 + 0.01 * 7.032682418823242
Epoch 540, val loss: 1.3398258686065674
Epoch 550, training loss: 0.09677279740571976 = 0.02673814445734024 + 0.01 * 7.00346565246582
Epoch 550, val loss: 1.3578343391418457
Epoch 560, training loss: 0.09468487650156021 = 0.024883555248379707 + 0.01 * 6.980132579803467
Epoch 560, val loss: 1.3752764463424683
Epoch 570, training loss: 0.09309244155883789 = 0.02320638671517372 + 0.01 * 6.988605499267578
Epoch 570, val loss: 1.3922340869903564
Epoch 580, training loss: 0.09174083918333054 = 0.021690577268600464 + 0.01 * 7.005026340484619
Epoch 580, val loss: 1.4086782932281494
Epoch 590, training loss: 0.08991460502147675 = 0.020316369831562042 + 0.01 * 6.9598236083984375
Epoch 590, val loss: 1.424540638923645
Epoch 600, training loss: 0.088705874979496 = 0.019066201522946358 + 0.01 * 6.963967323303223
Epoch 600, val loss: 1.4400153160095215
Epoch 610, training loss: 0.0874258279800415 = 0.017928574234247208 + 0.01 * 6.949725151062012
Epoch 610, val loss: 1.4549795389175415
Epoch 620, training loss: 0.08662929385900497 = 0.016889551654458046 + 0.01 * 6.973974704742432
Epoch 620, val loss: 1.4695097208023071
Epoch 630, training loss: 0.08543156087398529 = 0.01593935303390026 + 0.01 * 6.949220657348633
Epoch 630, val loss: 1.483655571937561
Epoch 640, training loss: 0.0843818262219429 = 0.015068253502249718 + 0.01 * 6.931356906890869
Epoch 640, val loss: 1.4973177909851074
Epoch 650, training loss: 0.08374233543872833 = 0.014268371276557446 + 0.01 * 6.947396755218506
Epoch 650, val loss: 1.5106481313705444
Epoch 660, training loss: 0.082772396504879 = 0.013532325625419617 + 0.01 * 6.924006938934326
Epoch 660, val loss: 1.5235918760299683
Epoch 670, training loss: 0.08213939517736435 = 0.012853674590587616 + 0.01 * 6.928572177886963
Epoch 670, val loss: 1.5362176895141602
Epoch 680, training loss: 0.08143572509288788 = 0.012226815335452557 + 0.01 * 6.920891284942627
Epoch 680, val loss: 1.5484014749526978
Epoch 690, training loss: 0.08092198520898819 = 0.01164814829826355 + 0.01 * 6.927383899688721
Epoch 690, val loss: 1.5602792501449585
Epoch 700, training loss: 0.08022904396057129 = 0.011111185885965824 + 0.01 * 6.911786079406738
Epoch 700, val loss: 1.5717947483062744
Epoch 710, training loss: 0.079571433365345 = 0.010613813064992428 + 0.01 * 6.8957624435424805
Epoch 710, val loss: 1.5830227136611938
Epoch 720, training loss: 0.07915192097425461 = 0.010151547379791737 + 0.01 * 6.9000372886657715
Epoch 720, val loss: 1.5939282178878784
Epoch 730, training loss: 0.07878689467906952 = 0.009720399975776672 + 0.01 * 6.906649589538574
Epoch 730, val loss: 1.6044275760650635
Epoch 740, training loss: 0.07837654650211334 = 0.00931878387928009 + 0.01 * 6.905776500701904
Epoch 740, val loss: 1.614730954170227
Epoch 750, training loss: 0.07775647193193436 = 0.008943455293774605 + 0.01 * 6.8813018798828125
Epoch 750, val loss: 1.6247318983078003
Epoch 760, training loss: 0.0773773342370987 = 0.008592675440013409 + 0.01 * 6.8784661293029785
Epoch 760, val loss: 1.6344032287597656
Epoch 770, training loss: 0.07723495364189148 = 0.0082639055326581 + 0.01 * 6.897104740142822
Epoch 770, val loss: 1.643849492073059
Epoch 780, training loss: 0.07666031271219254 = 0.00795619748532772 + 0.01 * 6.870411396026611
Epoch 780, val loss: 1.6529570817947388
Epoch 790, training loss: 0.07698815315961838 = 0.00766694825142622 + 0.01 * 6.9321208000183105
Epoch 790, val loss: 1.6620129346847534
Epoch 800, training loss: 0.07623451948165894 = 0.007395403925329447 + 0.01 * 6.883911609649658
Epoch 800, val loss: 1.6704990863800049
Epoch 810, training loss: 0.07570161670446396 = 0.007139756344258785 + 0.01 * 6.856186389923096
Epoch 810, val loss: 1.6790636777877808
Epoch 820, training loss: 0.07562361657619476 = 0.006898325402289629 + 0.01 * 6.872529029846191
Epoch 820, val loss: 1.687357783317566
Epoch 830, training loss: 0.07513830810785294 = 0.006670682691037655 + 0.01 * 6.846762657165527
Epoch 830, val loss: 1.6953964233398438
Epoch 840, training loss: 0.07482043653726578 = 0.006455457769334316 + 0.01 * 6.836498260498047
Epoch 840, val loss: 1.703346610069275
Epoch 850, training loss: 0.07466793060302734 = 0.006251940969377756 + 0.01 * 6.841598987579346
Epoch 850, val loss: 1.710993766784668
Epoch 860, training loss: 0.07474645972251892 = 0.00605942914262414 + 0.01 * 6.868703365325928
Epoch 860, val loss: 1.7184712886810303
Epoch 870, training loss: 0.07411903142929077 = 0.005877024028450251 + 0.01 * 6.8242011070251465
Epoch 870, val loss: 1.7257139682769775
Epoch 880, training loss: 0.07416483014822006 = 0.005703778471797705 + 0.01 * 6.846105575561523
Epoch 880, val loss: 1.7328739166259766
Epoch 890, training loss: 0.0737975686788559 = 0.00553931575268507 + 0.01 * 6.8258256912231445
Epoch 890, val loss: 1.7396512031555176
Epoch 900, training loss: 0.07346990704536438 = 0.005383096635341644 + 0.01 * 6.808681488037109
Epoch 900, val loss: 1.7465091943740845
Epoch 910, training loss: 0.07335200905799866 = 0.005234571173787117 + 0.01 * 6.811744213104248
Epoch 910, val loss: 1.753064751625061
Epoch 920, training loss: 0.07333066314458847 = 0.005093164276331663 + 0.01 * 6.823750019073486
Epoch 920, val loss: 1.7594636678695679
Epoch 930, training loss: 0.07295002043247223 = 0.004958647768944502 + 0.01 * 6.799137592315674
Epoch 930, val loss: 1.7656749486923218
Epoch 940, training loss: 0.07323804497718811 = 0.004830334335565567 + 0.01 * 6.840771675109863
Epoch 940, val loss: 1.7719581127166748
Epoch 950, training loss: 0.07295843213796616 = 0.004707871936261654 + 0.01 * 6.825056076049805
Epoch 950, val loss: 1.7777186632156372
Epoch 960, training loss: 0.07248447835445404 = 0.0045911166816949844 + 0.01 * 6.789336204528809
Epoch 960, val loss: 1.783707618713379
Epoch 970, training loss: 0.0729004293680191 = 0.004479389637708664 + 0.01 * 6.842103958129883
Epoch 970, val loss: 1.7894597053527832
Epoch 980, training loss: 0.0725974291563034 = 0.004372767638415098 + 0.01 * 6.822465896606445
Epoch 980, val loss: 1.7950265407562256
Epoch 990, training loss: 0.07206229865550995 = 0.004270540550351143 + 0.01 * 6.779176235198975
Epoch 990, val loss: 1.8005211353302002
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.804955192409067
The final CL Acc:0.75432, 0.01145, The final GNN Acc:0.80127, 0.00282
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13198])
remove edge: torch.Size([2, 7916])
updated graph: torch.Size([2, 10558])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0452234745025635 = 1.9592548608779907 + 0.01 * 8.596866607666016
Epoch 0, val loss: 1.9577399492263794
Epoch 10, training loss: 2.034564256668091 = 1.9485960006713867 + 0.01 * 8.59682846069336
Epoch 10, val loss: 1.9474737644195557
Epoch 20, training loss: 2.021449565887451 = 1.9354826211929321 + 0.01 * 8.596684455871582
Epoch 20, val loss: 1.9344260692596436
Epoch 30, training loss: 2.0030360221862793 = 1.9170732498168945 + 0.01 * 8.596278190612793
Epoch 30, val loss: 1.9157006740570068
Epoch 40, training loss: 1.9759836196899414 = 1.8900381326675415 + 0.01 * 8.594552993774414
Epoch 40, val loss: 1.8879928588867188
Epoch 50, training loss: 1.9380508661270142 = 1.8522154092788696 + 0.01 * 8.58354377746582
Epoch 50, val loss: 1.8502901792526245
Epoch 60, training loss: 1.8948512077331543 = 1.809530258178711 + 0.01 * 8.532096862792969
Epoch 60, val loss: 1.8112754821777344
Epoch 70, training loss: 1.8575148582458496 = 1.7735172510147095 + 0.01 * 8.399761199951172
Epoch 70, val loss: 1.7814925909042358
Epoch 80, training loss: 1.8146493434906006 = 1.7322248220443726 + 0.01 * 8.242452621459961
Epoch 80, val loss: 1.7444798946380615
Epoch 90, training loss: 1.7560356855392456 = 1.6753848791122437 + 0.01 * 8.06507682800293
Epoch 90, val loss: 1.6920373439788818
Epoch 100, training loss: 1.679876446723938 = 1.6000458002090454 + 0.01 * 7.983065605163574
Epoch 100, val loss: 1.624857783317566
Epoch 110, training loss: 1.5906604528427124 = 1.5119202136993408 + 0.01 * 7.874022006988525
Epoch 110, val loss: 1.5493383407592773
Epoch 120, training loss: 1.4996308088302612 = 1.4224843978881836 + 0.01 * 7.7146453857421875
Epoch 120, val loss: 1.4765264987945557
Epoch 130, training loss: 1.4121897220611572 = 1.3362783193588257 + 0.01 * 7.591141700744629
Epoch 130, val loss: 1.410064935684204
Epoch 140, training loss: 1.3262783288955688 = 1.2513502836227417 + 0.01 * 7.492804527282715
Epoch 140, val loss: 1.3472076654434204
Epoch 150, training loss: 1.2403384447097778 = 1.165955662727356 + 0.01 * 7.43828010559082
Epoch 150, val loss: 1.2844918966293335
Epoch 160, training loss: 1.1543574333190918 = 1.0801372528076172 + 0.01 * 7.4220147132873535
Epoch 160, val loss: 1.222095251083374
Epoch 170, training loss: 1.0690600872039795 = 0.9949561953544617 + 0.01 * 7.4103899002075195
Epoch 170, val loss: 1.1606277227401733
Epoch 180, training loss: 0.9864956140518188 = 0.9125146865844727 + 0.01 * 7.3980913162231445
Epoch 180, val loss: 1.1018853187561035
Epoch 190, training loss: 0.9088304042816162 = 0.8349653482437134 + 0.01 * 7.386508941650391
Epoch 190, val loss: 1.0476562976837158
Epoch 200, training loss: 0.8370645642280579 = 0.7633419036865234 + 0.01 * 7.3722662925720215
Epoch 200, val loss: 0.9989887475967407
Epoch 210, training loss: 0.7711021304130554 = 0.6975886225700378 + 0.01 * 7.351351737976074
Epoch 210, val loss: 0.9562756419181824
Epoch 220, training loss: 0.7104290127754211 = 0.6372007727622986 + 0.01 * 7.322821617126465
Epoch 220, val loss: 0.9193810820579529
Epoch 230, training loss: 0.6547020077705383 = 0.5817696452140808 + 0.01 * 7.293233871459961
Epoch 230, val loss: 0.8878712058067322
Epoch 240, training loss: 0.6035053730010986 = 0.5308799147605896 + 0.01 * 7.262547969818115
Epoch 240, val loss: 0.8612259030342102
Epoch 250, training loss: 0.5563464760780334 = 0.48398521542549133 + 0.01 * 7.236128807067871
Epoch 250, val loss: 0.8388308882713318
Epoch 260, training loss: 0.5123750567436218 = 0.4402931332588196 + 0.01 * 7.208192348480225
Epoch 260, val loss: 0.8201155066490173
Epoch 270, training loss: 0.47099441289901733 = 0.3990626037120819 + 0.01 * 7.193180561065674
Epoch 270, val loss: 0.8042446970939636
Epoch 280, training loss: 0.4318721294403076 = 0.35996511578559875 + 0.01 * 7.19070291519165
Epoch 280, val loss: 0.7909716963768005
Epoch 290, training loss: 0.3948374390602112 = 0.32313844561576843 + 0.01 * 7.169899940490723
Epoch 290, val loss: 0.7804226279258728
Epoch 300, training loss: 0.3604130148887634 = 0.2888267934322357 + 0.01 * 7.158621311187744
Epoch 300, val loss: 0.7728354930877686
Epoch 310, training loss: 0.32865047454833984 = 0.2570514380931854 + 0.01 * 7.159902095794678
Epoch 310, val loss: 0.7677944898605347
Epoch 320, training loss: 0.2991747260093689 = 0.22772835195064545 + 0.01 * 7.144636631011963
Epoch 320, val loss: 0.7652325630187988
Epoch 330, training loss: 0.2721044421195984 = 0.2006969004869461 + 0.01 * 7.140756130218506
Epoch 330, val loss: 0.7647742033004761
Epoch 340, training loss: 0.24722519516944885 = 0.1759212762117386 + 0.01 * 7.130392074584961
Epoch 340, val loss: 0.7659983038902283
Epoch 350, training loss: 0.2247193455696106 = 0.15350046753883362 + 0.01 * 7.121888637542725
Epoch 350, val loss: 0.768667995929718
Epoch 360, training loss: 0.20476216077804565 = 0.1335681974887848 + 0.01 * 7.119395732879639
Epoch 360, val loss: 0.7727576494216919
Epoch 370, training loss: 0.18728044629096985 = 0.11613921821117401 + 0.01 * 7.114123344421387
Epoch 370, val loss: 0.7781199216842651
Epoch 380, training loss: 0.1722051203250885 = 0.10112033784389496 + 0.01 * 7.108477592468262
Epoch 380, val loss: 0.7847378849983215
Epoch 390, training loss: 0.15925391018390656 = 0.08830303698778152 + 0.01 * 7.09508752822876
Epoch 390, val loss: 0.7925686836242676
Epoch 400, training loss: 0.14824338257312775 = 0.07741300761699677 + 0.01 * 7.083037376403809
Epoch 400, val loss: 0.80137699842453
Epoch 410, training loss: 0.1389956921339035 = 0.06815681606531143 + 0.01 * 7.083887577056885
Epoch 410, val loss: 0.8109572529792786
Epoch 420, training loss: 0.13099710643291473 = 0.060283053666353226 + 0.01 * 7.07140588760376
Epoch 420, val loss: 0.8210404515266418
Epoch 430, training loss: 0.12418868392705917 = 0.053559310734272 + 0.01 * 7.0629377365112305
Epoch 430, val loss: 0.8314712047576904
Epoch 440, training loss: 0.11836575716733932 = 0.04779744893312454 + 0.01 * 7.056830883026123
Epoch 440, val loss: 0.8421004414558411
Epoch 450, training loss: 0.11351753771305084 = 0.04283897951245308 + 0.01 * 7.0678558349609375
Epoch 450, val loss: 0.8527377843856812
Epoch 460, training loss: 0.1090623289346695 = 0.03855704143643379 + 0.01 * 7.050528526306152
Epoch 460, val loss: 0.863239049911499
Epoch 470, training loss: 0.10520270466804504 = 0.03483918681740761 + 0.01 * 7.036352157592773
Epoch 470, val loss: 0.8735631108283997
Epoch 480, training loss: 0.10207308828830719 = 0.03159636631608009 + 0.01 * 7.047672748565674
Epoch 480, val loss: 0.8836992383003235
Epoch 490, training loss: 0.09900286793708801 = 0.028761718422174454 + 0.01 * 7.024114608764648
Epoch 490, val loss: 0.8934987187385559
Epoch 500, training loss: 0.09644298255443573 = 0.026271702721714973 + 0.01 * 7.017127990722656
Epoch 500, val loss: 0.9029767513275146
Epoch 510, training loss: 0.09425222873687744 = 0.02407398447394371 + 0.01 * 7.017824649810791
Epoch 510, val loss: 0.9122105836868286
Epoch 520, training loss: 0.09222423285245895 = 0.022133514285087585 + 0.01 * 7.009072303771973
Epoch 520, val loss: 0.9210734963417053
Epoch 530, training loss: 0.09047144651412964 = 0.02041405253112316 + 0.01 * 7.005739688873291
Epoch 530, val loss: 0.9296681880950928
Epoch 540, training loss: 0.08918842673301697 = 0.018881436437368393 + 0.01 * 7.030698776245117
Epoch 540, val loss: 0.9379539489746094
Epoch 550, training loss: 0.08740051090717316 = 0.01751243695616722 + 0.01 * 6.988807678222656
Epoch 550, val loss: 0.945835292339325
Epoch 560, training loss: 0.08616918325424194 = 0.016282854601740837 + 0.01 * 6.988633155822754
Epoch 560, val loss: 0.9535198211669922
Epoch 570, training loss: 0.08498426526784897 = 0.01517788041383028 + 0.01 * 6.980638027191162
Epoch 570, val loss: 0.9609368443489075
Epoch 580, training loss: 0.08403952419757843 = 0.014180619269609451 + 0.01 * 6.9858903884887695
Epoch 580, val loss: 0.9681088924407959
Epoch 590, training loss: 0.08299100399017334 = 0.01328036654740572 + 0.01 * 6.971063613891602
Epoch 590, val loss: 0.9750345945358276
Epoch 600, training loss: 0.08220896869897842 = 0.012463088147342205 + 0.01 * 6.974588394165039
Epoch 600, val loss: 0.9817140698432922
Epoch 610, training loss: 0.08139829337596893 = 0.011720158159732819 + 0.01 * 6.967813491821289
Epoch 610, val loss: 0.9881680011749268
Epoch 620, training loss: 0.08095244318246841 = 0.011042329482734203 + 0.01 * 6.991011619567871
Epoch 620, val loss: 0.9944836497306824
Epoch 630, training loss: 0.07983677834272385 = 0.010423930361866951 + 0.01 * 6.941285133361816
Epoch 630, val loss: 1.0004726648330688
Epoch 640, training loss: 0.07943086326122284 = 0.009856363758444786 + 0.01 * 6.957449913024902
Epoch 640, val loss: 1.0063307285308838
Epoch 650, training loss: 0.07866383343935013 = 0.00933591928333044 + 0.01 * 6.932791709899902
Epoch 650, val loss: 1.0119421482086182
Epoch 660, training loss: 0.07835908234119415 = 0.00885708723217249 + 0.01 * 6.950200080871582
Epoch 660, val loss: 1.0175132751464844
Epoch 670, training loss: 0.07770353555679321 = 0.008415807038545609 + 0.01 * 6.928772926330566
Epoch 670, val loss: 1.0227763652801514
Epoch 680, training loss: 0.0772411972284317 = 0.008006545715034008 + 0.01 * 6.923465728759766
Epoch 680, val loss: 1.0279529094696045
Epoch 690, training loss: 0.07696739584207535 = 0.007627151440829039 + 0.01 * 6.934024333953857
Epoch 690, val loss: 1.0329970121383667
Epoch 700, training loss: 0.076462022960186 = 0.007276430260390043 + 0.01 * 6.9185590744018555
Epoch 700, val loss: 1.0377782583236694
Epoch 710, training loss: 0.07614424079656601 = 0.006951302755624056 + 0.01 * 6.919294357299805
Epoch 710, val loss: 1.0424885749816895
Epoch 720, training loss: 0.07585293799638748 = 0.006649409420788288 + 0.01 * 6.920352935791016
Epoch 720, val loss: 1.0470556020736694
Epoch 730, training loss: 0.07541035115718842 = 0.006368625443428755 + 0.01 * 6.904173374176025
Epoch 730, val loss: 1.0515193939208984
Epoch 740, training loss: 0.07519824057817459 = 0.006106005050241947 + 0.01 * 6.909224033355713
Epoch 740, val loss: 1.0557818412780762
Epoch 750, training loss: 0.07482630014419556 = 0.005861557554453611 + 0.01 * 6.896474361419678
Epoch 750, val loss: 1.059991717338562
Epoch 760, training loss: 0.07453171908855438 = 0.005632970482110977 + 0.01 * 6.8898749351501465
Epoch 760, val loss: 1.0639913082122803
Epoch 770, training loss: 0.07431007921695709 = 0.005419256165623665 + 0.01 * 6.889082908630371
Epoch 770, val loss: 1.0679932832717896
Epoch 780, training loss: 0.07399828732013702 = 0.0052195447497069836 + 0.01 * 6.877874851226807
Epoch 780, val loss: 1.0717288255691528
Epoch 790, training loss: 0.07401751726865768 = 0.0050317225977778435 + 0.01 * 6.898580074310303
Epoch 790, val loss: 1.075445294380188
Epoch 800, training loss: 0.07373300939798355 = 0.004855410195887089 + 0.01 * 6.887760162353516
Epoch 800, val loss: 1.078992486000061
Epoch 810, training loss: 0.07339003682136536 = 0.0046893153339624405 + 0.01 * 6.870072841644287
Epoch 810, val loss: 1.0825095176696777
Epoch 820, training loss: 0.07313607633113861 = 0.004533141851425171 + 0.01 * 6.860293865203857
Epoch 820, val loss: 1.0858871936798096
Epoch 830, training loss: 0.07304220646619797 = 0.004385537467896938 + 0.01 * 6.86566686630249
Epoch 830, val loss: 1.0891315937042236
Epoch 840, training loss: 0.07280654460191727 = 0.004246086347848177 + 0.01 * 6.856046199798584
Epoch 840, val loss: 1.092354416847229
Epoch 850, training loss: 0.0726916715502739 = 0.004114022944122553 + 0.01 * 6.857765197753906
Epoch 850, val loss: 1.0954774618148804
Epoch 860, training loss: 0.07255837321281433 = 0.003989438991993666 + 0.01 * 6.856894016265869
Epoch 860, val loss: 1.0985034704208374
Epoch 870, training loss: 0.07279279828071594 = 0.003871101187542081 + 0.01 * 6.892169952392578
Epoch 870, val loss: 1.1014695167541504
Epoch 880, training loss: 0.07225462049245834 = 0.0037591762375086546 + 0.01 * 6.849544525146484
Epoch 880, val loss: 1.1042839288711548
Epoch 890, training loss: 0.07206661254167557 = 0.003653018968179822 + 0.01 * 6.841359615325928
Epoch 890, val loss: 1.1070736646652222
Epoch 900, training loss: 0.07198509573936462 = 0.0035522100515663624 + 0.01 * 6.843288421630859
Epoch 900, val loss: 1.109802007675171
Epoch 910, training loss: 0.07194985449314117 = 0.0034560945350676775 + 0.01 * 6.849376201629639
Epoch 910, val loss: 1.1124621629714966
Epoch 920, training loss: 0.07180377095937729 = 0.003364593256264925 + 0.01 * 6.8439178466796875
Epoch 920, val loss: 1.1149741411209106
Epoch 930, training loss: 0.07183999568223953 = 0.003277538111433387 + 0.01 * 6.856245517730713
Epoch 930, val loss: 1.1175527572631836
Epoch 940, training loss: 0.07152054458856583 = 0.003194826887920499 + 0.01 * 6.832571983337402
Epoch 940, val loss: 1.1199917793273926
Epoch 950, training loss: 0.07146742939949036 = 0.003115681931376457 + 0.01 * 6.835175037384033
Epoch 950, val loss: 1.1223121881484985
Epoch 960, training loss: 0.0713195651769638 = 0.003040179843083024 + 0.01 * 6.827938556671143
Epoch 960, val loss: 1.1246299743652344
Epoch 970, training loss: 0.07126364856958389 = 0.002968324813991785 + 0.01 * 6.829532623291016
Epoch 970, val loss: 1.1268982887268066
Epoch 980, training loss: 0.07106941938400269 = 0.0028994299937039614 + 0.01 * 6.8169989585876465
Epoch 980, val loss: 1.129040002822876
Epoch 990, training loss: 0.07103807479143143 = 0.002833265345543623 + 0.01 * 6.820481300354004
Epoch 990, val loss: 1.1312142610549927
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 2.0419881343841553 = 1.9560197591781616 + 0.01 * 8.596837997436523
Epoch 0, val loss: 1.9632692337036133
Epoch 10, training loss: 2.031447410583496 = 1.9454792737960815 + 0.01 * 8.596803665161133
Epoch 10, val loss: 1.9517438411712646
Epoch 20, training loss: 2.018205165863037 = 1.9322385787963867 + 0.01 * 8.596656799316406
Epoch 20, val loss: 1.9367928504943848
Epoch 30, training loss: 1.9989904165267944 = 1.913028597831726 + 0.01 * 8.596179962158203
Epoch 30, val loss: 1.9148640632629395
Epoch 40, training loss: 1.9700684547424316 = 1.884130597114563 + 0.01 * 8.593782424926758
Epoch 40, val loss: 1.882090449333191
Epoch 50, training loss: 1.9301917552947998 = 1.8444098234176636 + 0.01 * 8.578189849853516
Epoch 50, val loss: 1.8391993045806885
Epoch 60, training loss: 1.8883601427078247 = 1.803203821182251 + 0.01 * 8.51563549041748
Epoch 60, val loss: 1.8001623153686523
Epoch 70, training loss: 1.8543894290924072 = 1.7704715728759766 + 0.01 * 8.391782760620117
Epoch 70, val loss: 1.7742974758148193
Epoch 80, training loss: 1.809546709060669 = 1.7275828123092651 + 0.01 * 8.196395874023438
Epoch 80, val loss: 1.7386316061019897
Epoch 90, training loss: 1.746620535850525 = 1.6683717966079712 + 0.01 * 7.824869155883789
Epoch 90, val loss: 1.686883807182312
Epoch 100, training loss: 1.665318489074707 = 1.5889912843704224 + 0.01 * 7.632722854614258
Epoch 100, val loss: 1.6173635721206665
Epoch 110, training loss: 1.5728294849395752 = 1.4969731569290161 + 0.01 * 7.585638523101807
Epoch 110, val loss: 1.5390443801879883
Epoch 120, training loss: 1.4797362089157104 = 1.4041248559951782 + 0.01 * 7.561130046844482
Epoch 120, val loss: 1.4637353420257568
Epoch 130, training loss: 1.3867645263671875 = 1.3114107847213745 + 0.01 * 7.535375118255615
Epoch 130, val loss: 1.3908841609954834
Epoch 140, training loss: 1.2893257141113281 = 1.2143934965133667 + 0.01 * 7.493223190307617
Epoch 140, val loss: 1.3151166439056396
Epoch 150, training loss: 1.1867479085922241 = 1.1124025583267212 + 0.01 * 7.434535503387451
Epoch 150, val loss: 1.2363231182098389
Epoch 160, training loss: 1.0838136672973633 = 1.0099906921386719 + 0.01 * 7.382298946380615
Epoch 160, val loss: 1.157723307609558
Epoch 170, training loss: 0.9872065782546997 = 0.9136247038841248 + 0.01 * 7.358187675476074
Epoch 170, val loss: 1.085666537284851
Epoch 180, training loss: 0.9011129140853882 = 0.827693521976471 + 0.01 * 7.341939449310303
Epoch 180, val loss: 1.0226900577545166
Epoch 190, training loss: 0.8263905644416809 = 0.7531803250312805 + 0.01 * 7.321025848388672
Epoch 190, val loss: 0.96956866979599
Epoch 200, training loss: 0.7612577676773071 = 0.6882832050323486 + 0.01 * 7.297454833984375
Epoch 200, val loss: 0.9248965382575989
Epoch 210, training loss: 0.702999472618103 = 0.6302513480186462 + 0.01 * 7.274810791015625
Epoch 210, val loss: 0.8872023820877075
Epoch 220, training loss: 0.6490929126739502 = 0.5766237378120422 + 0.01 * 7.246914386749268
Epoch 220, val loss: 0.8554509878158569
Epoch 230, training loss: 0.5982520580291748 = 0.5259994268417358 + 0.01 * 7.2252607345581055
Epoch 230, val loss: 0.8292688727378845
Epoch 240, training loss: 0.5502210855484009 = 0.47816047072410583 + 0.01 * 7.206060409545898
Epoch 240, val loss: 0.8084946870803833
Epoch 250, training loss: 0.505355179309845 = 0.43346327543258667 + 0.01 * 7.18919038772583
Epoch 250, val loss: 0.7928780317306519
Epoch 260, training loss: 0.46403080224990845 = 0.39223331212997437 + 0.01 * 7.17974853515625
Epoch 260, val loss: 0.7817547917366028
Epoch 270, training loss: 0.42621445655822754 = 0.3545576333999634 + 0.01 * 7.165680885314941
Epoch 270, val loss: 0.774478018283844
Epoch 280, training loss: 0.39187905192375183 = 0.32036012411117554 + 0.01 * 7.15189266204834
Epoch 280, val loss: 0.770679771900177
Epoch 290, training loss: 0.3608579635620117 = 0.2893788814544678 + 0.01 * 7.147909641265869
Epoch 290, val loss: 0.7697623372077942
Epoch 300, training loss: 0.332625150680542 = 0.26122456789016724 + 0.01 * 7.140059947967529
Epoch 300, val loss: 0.7712554335594177
Epoch 310, training loss: 0.30671125650405884 = 0.23532703518867493 + 0.01 * 7.138422012329102
Epoch 310, val loss: 0.7744690179824829
Epoch 320, training loss: 0.2825155556201935 = 0.2110864222049713 + 0.01 * 7.142913341522217
Epoch 320, val loss: 0.7788066267967224
Epoch 330, training loss: 0.2593592405319214 = 0.18802373111248016 + 0.01 * 7.133551597595215
Epoch 330, val loss: 0.7837465405464172
Epoch 340, training loss: 0.2373262643814087 = 0.16601479053497314 + 0.01 * 7.131146430969238
Epoch 340, val loss: 0.7893401980400085
Epoch 350, training loss: 0.21667590737342834 = 0.14536155760288239 + 0.01 * 7.131434917449951
Epoch 350, val loss: 0.7957834601402283
Epoch 360, training loss: 0.19782429933547974 = 0.1265668272972107 + 0.01 * 7.125747203826904
Epoch 360, val loss: 0.8036546111106873
Epoch 370, training loss: 0.1811685711145401 = 0.10997581481933594 + 0.01 * 7.1192755699157715
Epoch 370, val loss: 0.8131964206695557
Epoch 380, training loss: 0.16679058969020844 = 0.0956353172659874 + 0.01 * 7.115527153015137
Epoch 380, val loss: 0.8244213461875916
Epoch 390, training loss: 0.15444274246692657 = 0.08336640149354935 + 0.01 * 7.107634544372559
Epoch 390, val loss: 0.8369765877723694
Epoch 400, training loss: 0.14389893412590027 = 0.07290468364953995 + 0.01 * 7.09942626953125
Epoch 400, val loss: 0.8505538702011108
Epoch 410, training loss: 0.13489356637001038 = 0.06397934257984161 + 0.01 * 7.091423034667969
Epoch 410, val loss: 0.8648850321769714
Epoch 420, training loss: 0.12730400264263153 = 0.05634966120123863 + 0.01 * 7.095434665679932
Epoch 420, val loss: 0.8797113299369812
Epoch 430, training loss: 0.12067510932683945 = 0.04982795566320419 + 0.01 * 7.084715366363525
Epoch 430, val loss: 0.8948044776916504
Epoch 440, training loss: 0.11493191123008728 = 0.04424319043755531 + 0.01 * 7.068872928619385
Epoch 440, val loss: 0.909959614276886
Epoch 450, training loss: 0.1101612001657486 = 0.03945998474955559 + 0.01 * 7.070121765136719
Epoch 450, val loss: 0.9249995350837708
Epoch 460, training loss: 0.10593900084495544 = 0.035360921174287796 + 0.01 * 7.057807445526123
Epoch 460, val loss: 0.9397776126861572
Epoch 470, training loss: 0.10230234265327454 = 0.03183513879776001 + 0.01 * 7.046720504760742
Epoch 470, val loss: 0.9542676210403442
Epoch 480, training loss: 0.09924137592315674 = 0.02879193052649498 + 0.01 * 7.044944763183594
Epoch 480, val loss: 0.9683802723884583
Epoch 490, training loss: 0.09643430262804031 = 0.026156730949878693 + 0.01 * 7.02775764465332
Epoch 490, val loss: 0.9820453524589539
Epoch 500, training loss: 0.09409330785274506 = 0.023864973336458206 + 0.01 * 7.022834300994873
Epoch 500, val loss: 0.9952448010444641
Epoch 510, training loss: 0.0920712873339653 = 0.02186422049999237 + 0.01 * 7.020706653594971
Epoch 510, val loss: 1.0079511404037476
Epoch 520, training loss: 0.09023172408342361 = 0.020108483731746674 + 0.01 * 7.012324333190918
Epoch 520, val loss: 1.0202548503875732
Epoch 530, training loss: 0.08879079669713974 = 0.018559785559773445 + 0.01 * 7.023101329803467
Epoch 530, val loss: 1.0321414470672607
Epoch 540, training loss: 0.08716484904289246 = 0.017189403995871544 + 0.01 * 6.997544765472412
Epoch 540, val loss: 1.0436021089553833
Epoch 550, training loss: 0.08581332862377167 = 0.015971384942531586 + 0.01 * 6.984194755554199
Epoch 550, val loss: 1.054660677909851
Epoch 560, training loss: 0.08491400629281998 = 0.014883511699736118 + 0.01 * 7.003049850463867
Epoch 560, val loss: 1.0653706789016724
Epoch 570, training loss: 0.083792544901371 = 0.01390893291682005 + 0.01 * 6.988361358642578
Epoch 570, val loss: 1.0756810903549194
Epoch 580, training loss: 0.08280619233846664 = 0.013032651506364346 + 0.01 * 6.977354049682617
Epoch 580, val loss: 1.08563232421875
Epoch 590, training loss: 0.08178745955228806 = 0.012241102755069733 + 0.01 * 6.954636096954346
Epoch 590, val loss: 1.0953049659729004
Epoch 600, training loss: 0.08122257143259048 = 0.011524156667292118 + 0.01 * 6.969841957092285
Epoch 600, val loss: 1.104674220085144
Epoch 610, training loss: 0.0804818645119667 = 0.010874047875404358 + 0.01 * 6.960781574249268
Epoch 610, val loss: 1.1136789321899414
Epoch 620, training loss: 0.07978492230176926 = 0.010281977243721485 + 0.01 * 6.950294494628906
Epoch 620, val loss: 1.1223987340927124
Epoch 630, training loss: 0.0792495459318161 = 0.009740927256643772 + 0.01 * 6.950862407684326
Epoch 630, val loss: 1.1308883428573608
Epoch 640, training loss: 0.07849244773387909 = 0.00924525037407875 + 0.01 * 6.924719333648682
Epoch 640, val loss: 1.1390944719314575
Epoch 650, training loss: 0.07823621481657028 = 0.00878929067403078 + 0.01 * 6.944692134857178
Epoch 650, val loss: 1.147099256515503
Epoch 660, training loss: 0.07792455703020096 = 0.008369565941393375 + 0.01 * 6.955499172210693
Epoch 660, val loss: 1.154820442199707
Epoch 670, training loss: 0.07716454565525055 = 0.007983262650668621 + 0.01 * 6.918128490447998
Epoch 670, val loss: 1.1622594594955444
Epoch 680, training loss: 0.07669749110937119 = 0.007626304402947426 + 0.01 * 6.907118797302246
Epoch 680, val loss: 1.1695022583007812
Epoch 690, training loss: 0.07642335444688797 = 0.007295760326087475 + 0.01 * 6.912759780883789
Epoch 690, val loss: 1.176529884338379
Epoch 700, training loss: 0.07605476677417755 = 0.006988620385527611 + 0.01 * 6.906614780426025
Epoch 700, val loss: 1.1833752393722534
Epoch 710, training loss: 0.0757429450750351 = 0.006702893413603306 + 0.01 * 6.904005527496338
Epoch 710, val loss: 1.1900300979614258
Epoch 720, training loss: 0.07566098123788834 = 0.006436442490667105 + 0.01 * 6.922453880310059
Epoch 720, val loss: 1.1965314149856567
Epoch 730, training loss: 0.07513337582349777 = 0.006188154686242342 + 0.01 * 6.894522666931152
Epoch 730, val loss: 1.2028075456619263
Epoch 740, training loss: 0.07499797642230988 = 0.005955779459327459 + 0.01 * 6.904220104217529
Epoch 740, val loss: 1.2089428901672363
Epoch 750, training loss: 0.07452920079231262 = 0.005738409701734781 + 0.01 * 6.879079818725586
Epoch 750, val loss: 1.2149112224578857
Epoch 760, training loss: 0.07423189282417297 = 0.0055344440042972565 + 0.01 * 6.869744777679443
Epoch 760, val loss: 1.220757007598877
Epoch 770, training loss: 0.07404062151908875 = 0.0053427559323608875 + 0.01 * 6.869786739349365
Epoch 770, val loss: 1.2264518737792969
Epoch 780, training loss: 0.07408168166875839 = 0.00516236899420619 + 0.01 * 6.891931533813477
Epoch 780, val loss: 1.2319986820220947
Epoch 790, training loss: 0.07385541498661041 = 0.004992990754544735 + 0.01 * 6.886242389678955
Epoch 790, val loss: 1.237358808517456
Epoch 800, training loss: 0.07357434928417206 = 0.00483316695317626 + 0.01 * 6.874118328094482
Epoch 800, val loss: 1.2425758838653564
Epoch 810, training loss: 0.07336515188217163 = 0.004682459868490696 + 0.01 * 6.868269443511963
Epoch 810, val loss: 1.2477190494537354
Epoch 820, training loss: 0.07325001060962677 = 0.00453980965539813 + 0.01 * 6.8710198402404785
Epoch 820, val loss: 1.252709984779358
Epoch 830, training loss: 0.07301349937915802 = 0.004404913168400526 + 0.01 * 6.860859394073486
Epoch 830, val loss: 1.2575799226760864
Epoch 840, training loss: 0.07288475334644318 = 0.004276982508599758 + 0.01 * 6.860776901245117
Epoch 840, val loss: 1.262372374534607
Epoch 850, training loss: 0.07256390899419785 = 0.004155807197093964 + 0.01 * 6.840810298919678
Epoch 850, val loss: 1.2670055627822876
Epoch 860, training loss: 0.07246571779251099 = 0.004040692001581192 + 0.01 * 6.842503070831299
Epoch 860, val loss: 1.2715591192245483
Epoch 870, training loss: 0.07230417430400848 = 0.003931376151740551 + 0.01 * 6.8372802734375
Epoch 870, val loss: 1.2759902477264404
Epoch 880, training loss: 0.07246404141187668 = 0.003827370936051011 + 0.01 * 6.8636674880981445
Epoch 880, val loss: 1.2803481817245483
Epoch 890, training loss: 0.07219498604536057 = 0.0037284938152879477 + 0.01 * 6.846649169921875
Epoch 890, val loss: 1.284563660621643
Epoch 900, training loss: 0.07213164120912552 = 0.0036341226659715176 + 0.01 * 6.8497514724731445
Epoch 900, val loss: 1.2886695861816406
Epoch 910, training loss: 0.07185688614845276 = 0.003544287756085396 + 0.01 * 6.8312602043151855
Epoch 910, val loss: 1.2926777601242065
Epoch 920, training loss: 0.07170329988002777 = 0.003458465915173292 + 0.01 * 6.824483871459961
Epoch 920, val loss: 1.2966017723083496
Epoch 930, training loss: 0.07159636914730072 = 0.003376586362719536 + 0.01 * 6.821978569030762
Epoch 930, val loss: 1.3004335165023804
Epoch 940, training loss: 0.07141988724470139 = 0.0032983082346618176 + 0.01 * 6.812158107757568
Epoch 940, val loss: 1.3041726350784302
Epoch 950, training loss: 0.07140755653381348 = 0.0032233342062681913 + 0.01 * 6.818421840667725
Epoch 950, val loss: 1.3078349828720093
Epoch 960, training loss: 0.07134874165058136 = 0.0031516081653535366 + 0.01 * 6.819713592529297
Epoch 960, val loss: 1.3114198446273804
Epoch 970, training loss: 0.07113394886255264 = 0.0030829922761768103 + 0.01 * 6.805095672607422
Epoch 970, val loss: 1.3148964643478394
Epoch 980, training loss: 0.07102903723716736 = 0.0030170753598213196 + 0.01 * 6.801196575164795
Epoch 980, val loss: 1.3183878660202026
Epoch 990, training loss: 0.07092764973640442 = 0.002953983610495925 + 0.01 * 6.797366619110107
Epoch 990, val loss: 1.3216420412063599
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.0372228622436523 = 1.9512547254562378 + 0.01 * 8.596803665161133
Epoch 0, val loss: 1.9498759508132935
Epoch 10, training loss: 2.0267510414123535 = 1.9407835006713867 + 0.01 * 8.596744537353516
Epoch 10, val loss: 1.9392390251159668
Epoch 20, training loss: 2.013932943344116 = 1.92796790599823 + 0.01 * 8.596494674682617
Epoch 20, val loss: 1.9260450601577759
Epoch 30, training loss: 1.9959580898284912 = 1.9100018739700317 + 0.01 * 8.595623970031738
Epoch 30, val loss: 1.9074714183807373
Epoch 40, training loss: 1.9696578979492188 = 1.883754014968872 + 0.01 * 8.590389251708984
Epoch 40, val loss: 1.8804070949554443
Epoch 50, training loss: 1.9329044818878174 = 1.8473378419876099 + 0.01 * 8.556658744812012
Epoch 50, val loss: 1.8443831205368042
Epoch 60, training loss: 1.8899400234222412 = 1.8060976266860962 + 0.01 * 8.38424301147461
Epoch 60, val loss: 1.8077138662338257
Epoch 70, training loss: 1.8508775234222412 = 1.7686694860458374 + 0.01 * 8.220807075500488
Epoch 70, val loss: 1.7780617475509644
Epoch 80, training loss: 1.8025577068328857 = 1.7233573198318481 + 0.01 * 7.920044422149658
Epoch 80, val loss: 1.7386844158172607
Epoch 90, training loss: 1.7370870113372803 = 1.6608673334121704 + 0.01 * 7.621973514556885
Epoch 90, val loss: 1.681654691696167
Epoch 100, training loss: 1.651710867881775 = 1.5766217708587646 + 0.01 * 7.508914470672607
Epoch 100, val loss: 1.6063562631607056
Epoch 110, training loss: 1.549649953842163 = 1.4752517938613892 + 0.01 * 7.4398193359375
Epoch 110, val loss: 1.5191166400909424
Epoch 120, training loss: 1.4401885271072388 = 1.3663678169250488 + 0.01 * 7.3820695877075195
Epoch 120, val loss: 1.4271235466003418
Epoch 130, training loss: 1.3284988403320312 = 1.254958987236023 + 0.01 * 7.3539814949035645
Epoch 130, val loss: 1.3345259428024292
Epoch 140, training loss: 1.2165696620941162 = 1.143136739730835 + 0.01 * 7.343289852142334
Epoch 140, val loss: 1.2444010972976685
Epoch 150, training loss: 1.1076689958572388 = 1.034300446510315 + 0.01 * 7.336850643157959
Epoch 150, val loss: 1.1578574180603027
Epoch 160, training loss: 1.0056183338165283 = 0.9323155283927917 + 0.01 * 7.330275535583496
Epoch 160, val loss: 1.0780572891235352
Epoch 170, training loss: 0.9134786128997803 = 0.8402675986289978 + 0.01 * 7.321098327636719
Epoch 170, val loss: 1.006951928138733
Epoch 180, training loss: 0.8328526020050049 = 0.7597384452819824 + 0.01 * 7.311417102813721
Epoch 180, val loss: 0.9460123181343079
Epoch 190, training loss: 0.7630119323730469 = 0.6900508999824524 + 0.01 * 7.296105861663818
Epoch 190, val loss: 0.8952649235725403
Epoch 200, training loss: 0.7012646198272705 = 0.6284775137901306 + 0.01 * 7.278709411621094
Epoch 200, val loss: 0.8530939221382141
Epoch 210, training loss: 0.644511878490448 = 0.5719433426856995 + 0.01 * 7.256853103637695
Epoch 210, val loss: 0.8173865079879761
Epoch 220, training loss: 0.59093177318573 = 0.5185109972953796 + 0.01 * 7.242077827453613
Epoch 220, val loss: 0.7868099212646484
Epoch 230, training loss: 0.5397200584411621 = 0.46767863631248474 + 0.01 * 7.204141139984131
Epoch 230, val loss: 0.7610636949539185
Epoch 240, training loss: 0.4913809299468994 = 0.4194931089878082 + 0.01 * 7.188782215118408
Epoch 240, val loss: 0.7400856018066406
Epoch 250, training loss: 0.44586002826690674 = 0.3741699159145355 + 0.01 * 7.169013023376465
Epoch 250, val loss: 0.7238180637359619
Epoch 260, training loss: 0.4035075902938843 = 0.3319643437862396 + 0.01 * 7.154325485229492
Epoch 260, val loss: 0.7120833992958069
Epoch 270, training loss: 0.36452507972717285 = 0.293089359998703 + 0.01 * 7.143572807312012
Epoch 270, val loss: 0.7046987414360046
Epoch 280, training loss: 0.3291931450366974 = 0.2578487992286682 + 0.01 * 7.134435653686523
Epoch 280, val loss: 0.7013132572174072
Epoch 290, training loss: 0.29770761728286743 = 0.22648200392723083 + 0.01 * 7.122560024261475
Epoch 290, val loss: 0.7016834020614624
Epoch 300, training loss: 0.2701624631881714 = 0.1990371197462082 + 0.01 * 7.11253547668457
Epoch 300, val loss: 0.7054122686386108
Epoch 310, training loss: 0.2465546578168869 = 0.17533479630947113 + 0.01 * 7.121986389160156
Epoch 310, val loss: 0.7121392488479614
Epoch 320, training loss: 0.22598306834697723 = 0.15498864650726318 + 0.01 * 7.099442005157471
Epoch 320, val loss: 0.7212681770324707
Epoch 330, training loss: 0.208267480134964 = 0.13747701048851013 + 0.01 * 7.079046726226807
Epoch 330, val loss: 0.7323450446128845
Epoch 340, training loss: 0.19315797090530396 = 0.12234225124120712 + 0.01 * 7.081571102142334
Epoch 340, val loss: 0.7449025511741638
Epoch 350, training loss: 0.179913729429245 = 0.1092069149017334 + 0.01 * 7.070681571960449
Epoch 350, val loss: 0.7585276365280151
Epoch 360, training loss: 0.16846230626106262 = 0.09775828570127487 + 0.01 * 7.070403099060059
Epoch 360, val loss: 0.7728630900382996
Epoch 370, training loss: 0.15823370218276978 = 0.08773165196180344 + 0.01 * 7.050204753875732
Epoch 370, val loss: 0.7876399159431458
Epoch 380, training loss: 0.14926297962665558 = 0.07890502363443375 + 0.01 * 7.03579568862915
Epoch 380, val loss: 0.8027428984642029
Epoch 390, training loss: 0.14165350794792175 = 0.07110773026943207 + 0.01 * 7.054577827453613
Epoch 390, val loss: 0.8180416226387024
Epoch 400, training loss: 0.13463418185710907 = 0.06422226876020432 + 0.01 * 7.041191577911377
Epoch 400, val loss: 0.8333441615104675
Epoch 410, training loss: 0.12834399938583374 = 0.05812874808907509 + 0.01 * 7.0215253829956055
Epoch 410, val loss: 0.8485690355300903
Epoch 420, training loss: 0.12283720076084137 = 0.05273442715406418 + 0.01 * 7.01027774810791
Epoch 420, val loss: 0.8636012077331543
Epoch 430, training loss: 0.11795245110988617 = 0.047953348606824875 + 0.01 * 6.999910354614258
Epoch 430, val loss: 0.8784244060516357
Epoch 440, training loss: 0.11366896331310272 = 0.043710581958293915 + 0.01 * 6.995838165283203
Epoch 440, val loss: 0.8929541707038879
Epoch 450, training loss: 0.10986734926700592 = 0.03994261845946312 + 0.01 * 6.992473125457764
Epoch 450, val loss: 0.9071704149246216
Epoch 460, training loss: 0.10636913776397705 = 0.036591786891222 + 0.01 * 6.9777350425720215
Epoch 460, val loss: 0.9210275411605835
Epoch 470, training loss: 0.10338692367076874 = 0.033608436584472656 + 0.01 * 6.977849006652832
Epoch 470, val loss: 0.9345330595970154
Epoch 480, training loss: 0.10086705535650253 = 0.030945518985390663 + 0.01 * 6.992153644561768
Epoch 480, val loss: 0.9476748108863831
Epoch 490, training loss: 0.09836424887180328 = 0.02856532670557499 + 0.01 * 6.979892253875732
Epoch 490, val loss: 0.9604563117027283
Epoch 500, training loss: 0.09601802378892899 = 0.02643400989472866 + 0.01 * 6.958401679992676
Epoch 500, val loss: 0.9729039072990417
Epoch 510, training loss: 0.09414631128311157 = 0.02451988123357296 + 0.01 * 6.962643146514893
Epoch 510, val loss: 0.9849947094917297
Epoch 520, training loss: 0.09248895198106766 = 0.022797228768467903 + 0.01 * 6.969172477722168
Epoch 520, val loss: 0.9967702031135559
Epoch 530, training loss: 0.09057663381099701 = 0.021245066076517105 + 0.01 * 6.933157444000244
Epoch 530, val loss: 1.008131980895996
Epoch 540, training loss: 0.0892084464430809 = 0.019843831658363342 + 0.01 * 6.936461448669434
Epoch 540, val loss: 1.0191515684127808
Epoch 550, training loss: 0.08814007043838501 = 0.018574921414256096 + 0.01 * 6.956514835357666
Epoch 550, val loss: 1.0298484563827515
Epoch 560, training loss: 0.08680297434329987 = 0.01742486096918583 + 0.01 * 6.937811374664307
Epoch 560, val loss: 1.0402026176452637
Epoch 570, training loss: 0.08572462946176529 = 0.016378192231059074 + 0.01 * 6.934643745422363
Epoch 570, val loss: 1.0502731800079346
Epoch 580, training loss: 0.08450133353471756 = 0.015423963777720928 + 0.01 * 6.9077372550964355
Epoch 580, val loss: 1.0600171089172363
Epoch 590, training loss: 0.08384929597377777 = 0.014551674947142601 + 0.01 * 6.929762363433838
Epoch 590, val loss: 1.0695271492004395
Epoch 600, training loss: 0.08300044387578964 = 0.013753799721598625 + 0.01 * 6.924664497375488
Epoch 600, val loss: 1.0786917209625244
Epoch 610, training loss: 0.08187919110059738 = 0.013021901249885559 + 0.01 * 6.88572883605957
Epoch 610, val loss: 1.0875803232192993
Epoch 620, training loss: 0.08151206374168396 = 0.012348182499408722 + 0.01 * 6.916388511657715
Epoch 620, val loss: 1.0962754487991333
Epoch 630, training loss: 0.0808158591389656 = 0.011728130280971527 + 0.01 * 6.908772945404053
Epoch 630, val loss: 1.1046311855316162
Epoch 640, training loss: 0.08009475469589233 = 0.011156176216900349 + 0.01 * 6.893857479095459
Epoch 640, val loss: 1.1127763986587524
Epoch 650, training loss: 0.07933702319860458 = 0.01062687672674656 + 0.01 * 6.8710150718688965
Epoch 650, val loss: 1.120684027671814
Epoch 660, training loss: 0.0794273167848587 = 0.010136203840374947 + 0.01 * 6.929111957550049
Epoch 660, val loss: 1.128404974937439
Epoch 670, training loss: 0.0784451812505722 = 0.009681117720901966 + 0.01 * 6.876406192779541
Epoch 670, val loss: 1.1358586549758911
Epoch 680, training loss: 0.0780194103717804 = 0.009258432313799858 + 0.01 * 6.876097679138184
Epoch 680, val loss: 1.1431262493133545
Epoch 690, training loss: 0.07752315700054169 = 0.00886478926986456 + 0.01 * 6.865837097167969
Epoch 690, val loss: 1.1501643657684326
Epoch 700, training loss: 0.07752130925655365 = 0.008497465401887894 + 0.01 * 6.902385234832764
Epoch 700, val loss: 1.1570512056350708
Epoch 710, training loss: 0.07670263946056366 = 0.008155040442943573 + 0.01 * 6.85476016998291
Epoch 710, val loss: 1.1636942625045776
Epoch 720, training loss: 0.07614731043577194 = 0.007834581658244133 + 0.01 * 6.831272602081299
Epoch 720, val loss: 1.1701949834823608
Epoch 730, training loss: 0.07586555182933807 = 0.00753425108268857 + 0.01 * 6.833130359649658
Epoch 730, val loss: 1.1765278577804565
Epoch 740, training loss: 0.07570764422416687 = 0.007252438925206661 + 0.01 * 6.845520496368408
Epoch 740, val loss: 1.1826858520507812
Epoch 750, training loss: 0.07530084997415543 = 0.0069877286441624165 + 0.01 * 6.83131217956543
Epoch 750, val loss: 1.1886719465255737
Epoch 760, training loss: 0.07509700953960419 = 0.006738798227161169 + 0.01 * 6.835821151733398
Epoch 760, val loss: 1.194525122642517
Epoch 770, training loss: 0.0747867152094841 = 0.006504348013550043 + 0.01 * 6.8282365798950195
Epoch 770, val loss: 1.200230360031128
Epoch 780, training loss: 0.07469110935926437 = 0.006283358670771122 + 0.01 * 6.840775489807129
Epoch 780, val loss: 1.2058048248291016
Epoch 790, training loss: 0.07417130470275879 = 0.006074696313589811 + 0.01 * 6.809660911560059
Epoch 790, val loss: 1.2112343311309814
Epoch 800, training loss: 0.07408764958381653 = 0.005877424031496048 + 0.01 * 6.821022033691406
Epoch 800, val loss: 1.2165731191635132
Epoch 810, training loss: 0.07412105798721313 = 0.00569106824696064 + 0.01 * 6.84299898147583
Epoch 810, val loss: 1.2217286825180054
Epoch 820, training loss: 0.07375246286392212 = 0.005515004973858595 + 0.01 * 6.823746204376221
Epoch 820, val loss: 1.2267564535140991
Epoch 830, training loss: 0.0733526274561882 = 0.0053476607427001 + 0.01 * 6.800497055053711
Epoch 830, val loss: 1.231705904006958
Epoch 840, training loss: 0.07309448719024658 = 0.005188905168324709 + 0.01 * 6.790558338165283
Epoch 840, val loss: 1.2365500926971436
Epoch 850, training loss: 0.07312149554491043 = 0.005037950351834297 + 0.01 * 6.80835485458374
Epoch 850, val loss: 1.2412989139556885
Epoch 860, training loss: 0.07295453548431396 = 0.004894758574664593 + 0.01 * 6.805978298187256
Epoch 860, val loss: 1.24588143825531
Epoch 870, training loss: 0.07284187525510788 = 0.0047583067789673805 + 0.01 * 6.808357238769531
Epoch 870, val loss: 1.2504057884216309
Epoch 880, training loss: 0.07260049879550934 = 0.0046284813433885574 + 0.01 * 6.797202110290527
Epoch 880, val loss: 1.2548120021820068
Epoch 890, training loss: 0.07240771502256393 = 0.00450474489480257 + 0.01 * 6.790297031402588
Epoch 890, val loss: 1.259144902229309
Epoch 900, training loss: 0.0722898319363594 = 0.004386721178889275 + 0.01 * 6.790311336517334
Epoch 900, val loss: 1.263363242149353
Epoch 910, training loss: 0.07208413630723953 = 0.004273849073797464 + 0.01 * 6.781029224395752
Epoch 910, val loss: 1.2675237655639648
Epoch 920, training loss: 0.07190553098917007 = 0.004166270140558481 + 0.01 * 6.773926258087158
Epoch 920, val loss: 1.2715826034545898
Epoch 930, training loss: 0.071845144033432 = 0.0040635704062879086 + 0.01 * 6.778157711029053
Epoch 930, val loss: 1.2755563259124756
Epoch 940, training loss: 0.07153619825839996 = 0.003965176176279783 + 0.01 * 6.7571024894714355
Epoch 940, val loss: 1.2794666290283203
Epoch 950, training loss: 0.07173997163772583 = 0.003870955668389797 + 0.01 * 6.786901473999023
Epoch 950, val loss: 1.283294439315796
Epoch 960, training loss: 0.07162213325500488 = 0.003781066508963704 + 0.01 * 6.784107208251953
Epoch 960, val loss: 1.2869892120361328
Epoch 970, training loss: 0.0713396966457367 = 0.0036948281340301037 + 0.01 * 6.764487266540527
Epoch 970, val loss: 1.2906445264816284
Epoch 980, training loss: 0.07128328084945679 = 0.003612056141719222 + 0.01 * 6.767122268676758
Epoch 980, val loss: 1.2942167520523071
Epoch 990, training loss: 0.07112996280193329 = 0.003532714443281293 + 0.01 * 6.759725093841553
Epoch 990, val loss: 1.297711968421936
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8434370057986295
The final CL Acc:0.81852, 0.00302, The final GNN Acc:0.84150, 0.00203
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0433738231658936 = 1.9574058055877686 + 0.01 * 8.59681224822998
Epoch 0, val loss: 1.954778790473938
Epoch 10, training loss: 2.0328593254089355 = 1.9468920230865479 + 0.01 * 8.596733093261719
Epoch 10, val loss: 1.9437319040298462
Epoch 20, training loss: 2.019826650619507 = 1.9338626861572266 + 0.01 * 8.596402168273926
Epoch 20, val loss: 1.9300254583358765
Epoch 30, training loss: 2.001516819000244 = 1.9155668020248413 + 0.01 * 8.594990730285645
Epoch 30, val loss: 1.911001205444336
Epoch 40, training loss: 1.9745595455169678 = 1.8887168169021606 + 0.01 * 8.58427906036377
Epoch 40, val loss: 1.8837109804153442
Epoch 50, training loss: 1.9369549751281738 = 1.8517892360687256 + 0.01 * 8.516575813293457
Epoch 50, val loss: 1.8485773801803589
Epoch 60, training loss: 1.8944613933563232 = 1.8126943111419678 + 0.01 * 8.176708221435547
Epoch 60, val loss: 1.8173370361328125
Epoch 70, training loss: 1.8632622957229614 = 1.7825859785079956 + 0.01 * 8.067635536193848
Epoch 70, val loss: 1.7982068061828613
Epoch 80, training loss: 1.8265464305877686 = 1.748084306716919 + 0.01 * 7.846214294433594
Epoch 80, val loss: 1.7700912952423096
Epoch 90, training loss: 1.7761191129684448 = 1.7010841369628906 + 0.01 * 7.503499507904053
Epoch 90, val loss: 1.7284144163131714
Epoch 100, training loss: 1.7081916332244873 = 1.6352373361587524 + 0.01 * 7.295428276062012
Epoch 100, val loss: 1.6715811491012573
Epoch 110, training loss: 1.6231489181518555 = 1.5509629249572754 + 0.01 * 7.218594074249268
Epoch 110, val loss: 1.6010916233062744
Epoch 120, training loss: 1.5282117128372192 = 1.4566303491592407 + 0.01 * 7.158135414123535
Epoch 120, val loss: 1.5257606506347656
Epoch 130, training loss: 1.4316532611846924 = 1.3604989051818848 + 0.01 * 7.115434169769287
Epoch 130, val loss: 1.4535685777664185
Epoch 140, training loss: 1.3354384899139404 = 1.2646256685256958 + 0.01 * 7.081277370452881
Epoch 140, val loss: 1.3831251859664917
Epoch 150, training loss: 1.2383010387420654 = 1.1677381992340088 + 0.01 * 7.0562872886657715
Epoch 150, val loss: 1.3125901222229004
Epoch 160, training loss: 1.1413469314575195 = 1.0709022283554077 + 0.01 * 7.044470310211182
Epoch 160, val loss: 1.2421315908432007
Epoch 170, training loss: 1.0472917556762695 = 0.9769123792648315 + 0.01 * 7.037940979003906
Epoch 170, val loss: 1.1753742694854736
Epoch 180, training loss: 0.9590955376625061 = 0.8887587189674377 + 0.01 * 7.0336833000183105
Epoch 180, val loss: 1.1150720119476318
Epoch 190, training loss: 0.8789898157119751 = 0.8086844086647034 + 0.01 * 7.030538558959961
Epoch 190, val loss: 1.0630950927734375
Epoch 200, training loss: 0.8079673647880554 = 0.7376957535743713 + 0.01 * 7.027162075042725
Epoch 200, val loss: 1.0198310613632202
Epoch 210, training loss: 0.7453109622001648 = 0.6750535368919373 + 0.01 * 7.0257439613342285
Epoch 210, val loss: 0.9853499531745911
Epoch 220, training loss: 0.6891052722930908 = 0.6188620924949646 + 0.01 * 7.024320125579834
Epoch 220, val loss: 0.9581242203712463
Epoch 230, training loss: 0.6375970244407654 = 0.5673562288284302 + 0.01 * 7.024080753326416
Epoch 230, val loss: 0.9371921420097351
Epoch 240, training loss: 0.5897420644760132 = 0.5195020437240601 + 0.01 * 7.024001598358154
Epoch 240, val loss: 0.9216655492782593
Epoch 250, training loss: 0.5451282262802124 = 0.4748925566673279 + 0.01 * 7.0235676765441895
Epoch 250, val loss: 0.9111650586128235
Epoch 260, training loss: 0.503661036491394 = 0.43341705203056335 + 0.01 * 7.0243964195251465
Epoch 260, val loss: 0.9056442975997925
Epoch 270, training loss: 0.4652261734008789 = 0.3949848711490631 + 0.01 * 7.024130344390869
Epoch 270, val loss: 0.9048404693603516
Epoch 280, training loss: 0.4295867383480072 = 0.3593534827232361 + 0.01 * 7.023326396942139
Epoch 280, val loss: 0.9083176255226135
Epoch 290, training loss: 0.39639467000961304 = 0.32616424560546875 + 0.01 * 7.023040771484375
Epoch 290, val loss: 0.9155102968215942
Epoch 300, training loss: 0.3652300536632538 = 0.2949903905391693 + 0.01 * 7.023965835571289
Epoch 300, val loss: 0.9257814884185791
Epoch 310, training loss: 0.33576247096061707 = 0.2655392289161682 + 0.01 * 7.022325038909912
Epoch 310, val loss: 0.9384937286376953
Epoch 320, training loss: 0.3079521656036377 = 0.23772166669368744 + 0.01 * 7.023050785064697
Epoch 320, val loss: 0.9533208608627319
Epoch 330, training loss: 0.28185975551605225 = 0.2116551399230957 + 0.01 * 7.0204620361328125
Epoch 330, val loss: 0.9700676798820496
Epoch 340, training loss: 0.25774189829826355 = 0.18754570186138153 + 0.01 * 7.019620418548584
Epoch 340, val loss: 0.9886526465415955
Epoch 350, training loss: 0.235767662525177 = 0.16558721661567688 + 0.01 * 7.018045425415039
Epoch 350, val loss: 1.0089285373687744
Epoch 360, training loss: 0.21604451537132263 = 0.1458594650030136 + 0.01 * 7.018506050109863
Epoch 360, val loss: 1.03066885471344
Epoch 370, training loss: 0.19848915934562683 = 0.12834832072257996 + 0.01 * 7.014082908630371
Epoch 370, val loss: 1.0536839962005615
Epoch 380, training loss: 0.18308475613594055 = 0.11296568810939789 + 0.01 * 7.011906623840332
Epoch 380, val loss: 1.0776447057724
Epoch 390, training loss: 0.16968199610710144 = 0.09956611692905426 + 0.01 * 7.0115885734558105
Epoch 390, val loss: 1.1022157669067383
Epoch 400, training loss: 0.1580319106578827 = 0.08795950561761856 + 0.01 * 7.007241725921631
Epoch 400, val loss: 1.1270169019699097
Epoch 410, training loss: 0.14799124002456665 = 0.07794015109539032 + 0.01 * 7.0051093101501465
Epoch 410, val loss: 1.1518629789352417
Epoch 420, training loss: 0.13926967978477478 = 0.06929951161146164 + 0.01 * 6.997016906738281
Epoch 420, val loss: 1.1765412092208862
Epoch 430, training loss: 0.13177265226840973 = 0.061836667358875275 + 0.01 * 6.993598937988281
Epoch 430, val loss: 1.2009179592132568
Epoch 440, training loss: 0.12523609399795532 = 0.05538000166416168 + 0.01 * 6.985609531402588
Epoch 440, val loss: 1.2248250246047974
Epoch 450, training loss: 0.11958344280719757 = 0.049786973744630814 + 0.01 * 6.979647159576416
Epoch 450, val loss: 1.2481595277786255
Epoch 460, training loss: 0.11468212306499481 = 0.044927727431058884 + 0.01 * 6.97544002532959
Epoch 460, val loss: 1.2709187269210815
Epoch 470, training loss: 0.1104806512594223 = 0.04069705680012703 + 0.01 * 6.978359699249268
Epoch 470, val loss: 1.2929997444152832
Epoch 480, training loss: 0.10681062936782837 = 0.03699779510498047 + 0.01 * 6.981283664703369
Epoch 480, val loss: 1.3143486976623535
Epoch 490, training loss: 0.10348257422447205 = 0.0337546207010746 + 0.01 * 6.9727959632873535
Epoch 490, val loss: 1.3350660800933838
Epoch 500, training loss: 0.10064578801393509 = 0.03090016543865204 + 0.01 * 6.974562168121338
Epoch 500, val loss: 1.355069637298584
Epoch 510, training loss: 0.09790496528148651 = 0.028382625430822372 + 0.01 * 6.952234268188477
Epoch 510, val loss: 1.3743922710418701
Epoch 520, training loss: 0.09558509290218353 = 0.02615208551287651 + 0.01 * 6.943300247192383
Epoch 520, val loss: 1.393039345741272
Epoch 530, training loss: 0.09377319365739822 = 0.024168193340301514 + 0.01 * 6.960500240325928
Epoch 530, val loss: 1.4110476970672607
Epoch 540, training loss: 0.09182873368263245 = 0.022399531677365303 + 0.01 * 6.942920684814453
Epoch 540, val loss: 1.428410291671753
Epoch 550, training loss: 0.09009624272584915 = 0.020816192030906677 + 0.01 * 6.928005218505859
Epoch 550, val loss: 1.4451959133148193
Epoch 560, training loss: 0.0885944738984108 = 0.01939302682876587 + 0.01 * 6.920144557952881
Epoch 560, val loss: 1.4613783359527588
Epoch 570, training loss: 0.08733934164047241 = 0.018108125776052475 + 0.01 * 6.923121929168701
Epoch 570, val loss: 1.4769726991653442
Epoch 580, training loss: 0.08622880280017853 = 0.01694309711456299 + 0.01 * 6.928570747375488
Epoch 580, val loss: 1.492163896560669
Epoch 590, training loss: 0.08496067672967911 = 0.01588231511414051 + 0.01 * 6.907836437225342
Epoch 590, val loss: 1.5069552659988403
Epoch 600, training loss: 0.08395948261022568 = 0.01491297036409378 + 0.01 * 6.904651641845703
Epoch 600, val loss: 1.521260142326355
Epoch 610, training loss: 0.08312629163265228 = 0.014026922173798084 + 0.01 * 6.909936904907227
Epoch 610, val loss: 1.5352177619934082
Epoch 620, training loss: 0.08216503262519836 = 0.013214834965765476 + 0.01 * 6.895020008087158
Epoch 620, val loss: 1.548843502998352
Epoch 630, training loss: 0.08174854516983032 = 0.012468033470213413 + 0.01 * 6.928051471710205
Epoch 630, val loss: 1.5620638132095337
Epoch 640, training loss: 0.08073629438877106 = 0.011782731860876083 + 0.01 * 6.895356178283691
Epoch 640, val loss: 1.574857234954834
Epoch 650, training loss: 0.07997950166463852 = 0.011153021827340126 + 0.01 * 6.88264799118042
Epoch 650, val loss: 1.5872637033462524
Epoch 660, training loss: 0.07933401316404343 = 0.010572528466582298 + 0.01 * 6.876148700714111
Epoch 660, val loss: 1.5993884801864624
Epoch 670, training loss: 0.07920005172491074 = 0.010035970248281956 + 0.01 * 6.916408061981201
Epoch 670, val loss: 1.6111109256744385
Epoch 680, training loss: 0.07837257534265518 = 0.009540634229779243 + 0.01 * 6.883194446563721
Epoch 680, val loss: 1.6225117444992065
Epoch 690, training loss: 0.0777672752737999 = 0.00908228475600481 + 0.01 * 6.868498802185059
Epoch 690, val loss: 1.633591890335083
Epoch 700, training loss: 0.07748767733573914 = 0.008657186292111874 + 0.01 * 6.883049488067627
Epoch 700, val loss: 1.6442972421646118
Epoch 710, training loss: 0.07700635492801666 = 0.008262498304247856 + 0.01 * 6.874385356903076
Epoch 710, val loss: 1.654756784439087
Epoch 720, training loss: 0.07657027989625931 = 0.007895645685493946 + 0.01 * 6.867463111877441
Epoch 720, val loss: 1.6648554801940918
Epoch 730, training loss: 0.07611563056707382 = 0.007554061245173216 + 0.01 * 6.856156826019287
Epoch 730, val loss: 1.6746296882629395
Epoch 740, training loss: 0.07584051787853241 = 0.007235411554574966 + 0.01 * 6.860511302947998
Epoch 740, val loss: 1.684185266494751
Epoch 750, training loss: 0.07543589919805527 = 0.006937830243259668 + 0.01 * 6.849806785583496
Epoch 750, val loss: 1.6934232711791992
Epoch 760, training loss: 0.07515345513820648 = 0.006659730803221464 + 0.01 * 6.849372863769531
Epoch 760, val loss: 1.7023510932922363
Epoch 770, training loss: 0.0748114064335823 = 0.006400334183126688 + 0.01 * 6.841107368469238
Epoch 770, val loss: 1.7110118865966797
Epoch 780, training loss: 0.0745619684457779 = 0.0061574368737638 + 0.01 * 6.840453147888184
Epoch 780, val loss: 1.7194560766220093
Epoch 790, training loss: 0.07421360909938812 = 0.005929645150899887 + 0.01 * 6.828397274017334
Epoch 790, val loss: 1.7274962663650513
Epoch 800, training loss: 0.07414618134498596 = 0.005715691018849611 + 0.01 * 6.843049049377441
Epoch 800, val loss: 1.7353876829147339
Epoch 810, training loss: 0.07376985996961594 = 0.005514792166650295 + 0.01 * 6.825506687164307
Epoch 810, val loss: 1.743096113204956
Epoch 820, training loss: 0.07369472831487656 = 0.005325415171682835 + 0.01 * 6.836931228637695
Epoch 820, val loss: 1.7504210472106934
Epoch 830, training loss: 0.07352212071418762 = 0.005147555842995644 + 0.01 * 6.837457180023193
Epoch 830, val loss: 1.7576390504837036
Epoch 840, training loss: 0.07310722768306732 = 0.004979770630598068 + 0.01 * 6.812745571136475
Epoch 840, val loss: 1.7644994258880615
Epoch 850, training loss: 0.07335702329874039 = 0.004821088630706072 + 0.01 * 6.853593349456787
Epoch 850, val loss: 1.7712812423706055
Epoch 860, training loss: 0.07276130467653275 = 0.004671459086239338 + 0.01 * 6.808984756469727
Epoch 860, val loss: 1.777849555015564
Epoch 870, training loss: 0.07281410694122314 = 0.004529672674834728 + 0.01 * 6.82844352722168
Epoch 870, val loss: 1.784036636352539
Epoch 880, training loss: 0.07251349091529846 = 0.004395369440317154 + 0.01 * 6.811811923980713
Epoch 880, val loss: 1.7902135848999023
Epoch 890, training loss: 0.07221168279647827 = 0.004268178250640631 + 0.01 * 6.794350624084473
Epoch 890, val loss: 1.7960790395736694
Epoch 900, training loss: 0.07222148776054382 = 0.0041474695317447186 + 0.01 * 6.80740213394165
Epoch 900, val loss: 1.801749348640442
Epoch 910, training loss: 0.07214812189340591 = 0.004032892640680075 + 0.01 * 6.811522483825684
Epoch 910, val loss: 1.8074239492416382
Epoch 920, training loss: 0.07200201600790024 = 0.003924215212464333 + 0.01 * 6.807780742645264
Epoch 920, val loss: 1.8126884698867798
Epoch 930, training loss: 0.07166925072669983 = 0.00382079160772264 + 0.01 * 6.78484582901001
Epoch 930, val loss: 1.8178449869155884
Epoch 940, training loss: 0.07179902493953705 = 0.003722467226907611 + 0.01 * 6.8076558113098145
Epoch 940, val loss: 1.8227392435073853
Epoch 950, training loss: 0.0714687779545784 = 0.003628836479038 + 0.01 * 6.783994674682617
Epoch 950, val loss: 1.8277133703231812
Epoch 960, training loss: 0.07126962393522263 = 0.003539430210366845 + 0.01 * 6.773019790649414
Epoch 960, val loss: 1.8322638273239136
Epoch 970, training loss: 0.0711050033569336 = 0.003454380901530385 + 0.01 * 6.76506233215332
Epoch 970, val loss: 1.8368852138519287
Epoch 980, training loss: 0.07101369649171829 = 0.0033730114810168743 + 0.01 * 6.764068603515625
Epoch 980, val loss: 1.8412998914718628
Epoch 990, training loss: 0.0708584412932396 = 0.003295290982350707 + 0.01 * 6.756315231323242
Epoch 990, val loss: 1.8453106880187988
Epoch 1000, training loss: 0.07086310535669327 = 0.0032212375663220882 + 0.01 * 6.764186859130859
Epoch 1000, val loss: 1.8496065139770508
Epoch 1010, training loss: 0.07075928151607513 = 0.003150534350425005 + 0.01 * 6.760874271392822
Epoch 1010, val loss: 1.8533951044082642
Epoch 1020, training loss: 0.07123324275016785 = 0.0030827417504042387 + 0.01 * 6.81505012512207
Epoch 1020, val loss: 1.857305884361267
Epoch 1030, training loss: 0.07051538676023483 = 0.0030177803710103035 + 0.01 * 6.749760627746582
Epoch 1030, val loss: 1.8609888553619385
Epoch 1040, training loss: 0.0704672783613205 = 0.002955531934276223 + 0.01 * 6.7511749267578125
Epoch 1040, val loss: 1.8645051717758179
Epoch 1050, training loss: 0.07013918459415436 = 0.002895750803872943 + 0.01 * 6.724343776702881
Epoch 1050, val loss: 1.8681124448776245
Epoch 1060, training loss: 0.07033680379390717 = 0.0028382381424307823 + 0.01 * 6.749856472015381
Epoch 1060, val loss: 1.8713099956512451
Epoch 1070, training loss: 0.0703316256403923 = 0.002783167874440551 + 0.01 * 6.754846096038818
Epoch 1070, val loss: 1.8747038841247559
Epoch 1080, training loss: 0.07012204825878143 = 0.002730243606492877 + 0.01 * 6.739181041717529
Epoch 1080, val loss: 1.877583384513855
Epoch 1090, training loss: 0.06980597972869873 = 0.002679530531167984 + 0.01 * 6.712645053863525
Epoch 1090, val loss: 1.8808470964431763
Epoch 1100, training loss: 0.06994020938873291 = 0.0026305902283638716 + 0.01 * 6.730961799621582
Epoch 1100, val loss: 1.8835430145263672
Epoch 1110, training loss: 0.07026169449090958 = 0.0025834040716290474 + 0.01 * 6.767828941345215
Epoch 1110, val loss: 1.886460781097412
Epoch 1120, training loss: 0.06978031247854233 = 0.002538131084293127 + 0.01 * 6.724218368530273
Epoch 1120, val loss: 1.889097809791565
Epoch 1130, training loss: 0.06958310306072235 = 0.0024944432079792023 + 0.01 * 6.708866119384766
Epoch 1130, val loss: 1.8917255401611328
Epoch 1140, training loss: 0.06964634358882904 = 0.002452289219945669 + 0.01 * 6.719405651092529
Epoch 1140, val loss: 1.8943612575531006
Epoch 1150, training loss: 0.06957460194826126 = 0.0024114467669278383 + 0.01 * 6.716315746307373
Epoch 1150, val loss: 1.8967777490615845
Epoch 1160, training loss: 0.06949356198310852 = 0.0023721596226096153 + 0.01 * 6.7121405601501465
Epoch 1160, val loss: 1.8991235494613647
Epoch 1170, training loss: 0.06956805288791656 = 0.0023341323249042034 + 0.01 * 6.723392009735107
Epoch 1170, val loss: 1.9014745950698853
Epoch 1180, training loss: 0.06936722248792648 = 0.002297355327755213 + 0.01 * 6.706986427307129
Epoch 1180, val loss: 1.9037595987319946
Epoch 1190, training loss: 0.06911550462245941 = 0.0022618090733885765 + 0.01 * 6.685369968414307
Epoch 1190, val loss: 1.905979871749878
Epoch 1200, training loss: 0.06930939853191376 = 0.0022274276707321405 + 0.01 * 6.708197593688965
Epoch 1200, val loss: 1.908050298690796
Epoch 1210, training loss: 0.06908276677131653 = 0.0021942888852208853 + 0.01 * 6.688848495483398
Epoch 1210, val loss: 1.910160779953003
Epoch 1220, training loss: 0.06912495940923691 = 0.0021620700135827065 + 0.01 * 6.6962890625
Epoch 1220, val loss: 1.9119830131530762
Epoch 1230, training loss: 0.06929563730955124 = 0.0021310122683644295 + 0.01 * 6.7164626121521
Epoch 1230, val loss: 1.914068341255188
Epoch 1240, training loss: 0.06882324814796448 = 0.0021007955074310303 + 0.01 * 6.672245502471924
Epoch 1240, val loss: 1.9158357381820679
Epoch 1250, training loss: 0.06895081698894501 = 0.0020715398713946342 + 0.01 * 6.687928199768066
Epoch 1250, val loss: 1.9176254272460938
Epoch 1260, training loss: 0.06874943524599075 = 0.002043274464085698 + 0.01 * 6.6706156730651855
Epoch 1260, val loss: 1.9194207191467285
Epoch 1270, training loss: 0.06856905668973923 = 0.002015776000916958 + 0.01 * 6.655328273773193
Epoch 1270, val loss: 1.921021819114685
Epoch 1280, training loss: 0.06873061507940292 = 0.001989157870411873 + 0.01 * 6.6741461753845215
Epoch 1280, val loss: 1.9227219820022583
Epoch 1290, training loss: 0.0685000792145729 = 0.00196330389007926 + 0.01 * 6.653677940368652
Epoch 1290, val loss: 1.9242680072784424
Epoch 1300, training loss: 0.06842609494924545 = 0.0019382031168788671 + 0.01 * 6.648789405822754
Epoch 1300, val loss: 1.9258127212524414
Epoch 1310, training loss: 0.06842123717069626 = 0.0019138152711093426 + 0.01 * 6.650742530822754
Epoch 1310, val loss: 1.927246332168579
Epoch 1320, training loss: 0.06841810792684555 = 0.0018901298753917217 + 0.01 * 6.652798175811768
Epoch 1320, val loss: 1.9287384748458862
Epoch 1330, training loss: 0.06830745935440063 = 0.0018671005964279175 + 0.01 * 6.644036293029785
Epoch 1330, val loss: 1.9301122426986694
Epoch 1340, training loss: 0.06829581409692764 = 0.001844705198891461 + 0.01 * 6.645111083984375
Epoch 1340, val loss: 1.931452989578247
Epoch 1350, training loss: 0.0684548020362854 = 0.0018228950211778283 + 0.01 * 6.6631903648376465
Epoch 1350, val loss: 1.9326777458190918
Epoch 1360, training loss: 0.0681598037481308 = 0.0018017636612057686 + 0.01 * 6.635804176330566
Epoch 1360, val loss: 1.9339289665222168
Epoch 1370, training loss: 0.06820566207170486 = 0.0017811970319598913 + 0.01 * 6.642446517944336
Epoch 1370, val loss: 1.935023546218872
Epoch 1380, training loss: 0.0680837407708168 = 0.0017611744115129113 + 0.01 * 6.632256984710693
Epoch 1380, val loss: 1.936181664466858
Epoch 1390, training loss: 0.06800352036952972 = 0.0017416252521798015 + 0.01 * 6.626189708709717
Epoch 1390, val loss: 1.9371792078018188
Epoch 1400, training loss: 0.06818237155675888 = 0.001722615910694003 + 0.01 * 6.645975112915039
Epoch 1400, val loss: 1.9382455348968506
Epoch 1410, training loss: 0.06799628585577011 = 0.0017041392857208848 + 0.01 * 6.629214763641357
Epoch 1410, val loss: 1.939213514328003
Epoch 1420, training loss: 0.0677923783659935 = 0.0016861115582287312 + 0.01 * 6.610627174377441
Epoch 1420, val loss: 1.9401822090148926
Epoch 1430, training loss: 0.0682951882481575 = 0.001668572542257607 + 0.01 * 6.662661552429199
Epoch 1430, val loss: 1.9412155151367188
Epoch 1440, training loss: 0.06787483394145966 = 0.0016513480804860592 + 0.01 * 6.622348785400391
Epoch 1440, val loss: 1.9420140981674194
Epoch 1450, training loss: 0.06783582270145416 = 0.001634617568925023 + 0.01 * 6.620120048522949
Epoch 1450, val loss: 1.9429271221160889
Epoch 1460, training loss: 0.06760866940021515 = 0.0016182733234018087 + 0.01 * 6.599039554595947
Epoch 1460, val loss: 1.9437615871429443
Epoch 1470, training loss: 0.06781875342130661 = 0.001602306729182601 + 0.01 * 6.621644496917725
Epoch 1470, val loss: 1.9443997144699097
Epoch 1480, training loss: 0.06767953932285309 = 0.0015868290793150663 + 0.01 * 6.609271049499512
Epoch 1480, val loss: 1.945311188697815
Epoch 1490, training loss: 0.06740792095661163 = 0.0015716649359092116 + 0.01 * 6.583625793457031
Epoch 1490, val loss: 1.9460290670394897
Epoch 1500, training loss: 0.06788135319948196 = 0.0015568527160212398 + 0.01 * 6.632450103759766
Epoch 1500, val loss: 1.9466973543167114
Epoch 1510, training loss: 0.06764692813158035 = 0.0015423984732478857 + 0.01 * 6.610453128814697
Epoch 1510, val loss: 1.9473251104354858
Epoch 1520, training loss: 0.06741888076066971 = 0.0015282727545127273 + 0.01 * 6.589061260223389
Epoch 1520, val loss: 1.9479808807373047
Epoch 1530, training loss: 0.0679003894329071 = 0.0015145250363275409 + 0.01 * 6.638586521148682
Epoch 1530, val loss: 1.9486205577850342
Epoch 1540, training loss: 0.06729468703269958 = 0.0015010220231488347 + 0.01 * 6.579366683959961
Epoch 1540, val loss: 1.9491889476776123
Epoch 1550, training loss: 0.06747230887413025 = 0.0014878425281494856 + 0.01 * 6.598446369171143
Epoch 1550, val loss: 1.949736475944519
Epoch 1560, training loss: 0.0673932209610939 = 0.0014749678084626794 + 0.01 * 6.591825485229492
Epoch 1560, val loss: 1.9502689838409424
Epoch 1570, training loss: 0.0674314945936203 = 0.0014623475726693869 + 0.01 * 6.596914768218994
Epoch 1570, val loss: 1.9508000612258911
Epoch 1580, training loss: 0.0676041916012764 = 0.0014500041725113988 + 0.01 * 6.615418910980225
Epoch 1580, val loss: 1.9513864517211914
Epoch 1590, training loss: 0.06729859858751297 = 0.0014378897612914443 + 0.01 * 6.586071014404297
Epoch 1590, val loss: 1.9517098665237427
Epoch 1600, training loss: 0.06740111112594604 = 0.0014260446187108755 + 0.01 * 6.597507476806641
Epoch 1600, val loss: 1.9522165060043335
Epoch 1610, training loss: 0.06725625693798065 = 0.0014144275337457657 + 0.01 * 6.584183216094971
Epoch 1610, val loss: 1.9526152610778809
Epoch 1620, training loss: 0.06721276789903641 = 0.0014030798338353634 + 0.01 * 6.580968856811523
Epoch 1620, val loss: 1.953064203262329
Epoch 1630, training loss: 0.06720411777496338 = 0.001391988480463624 + 0.01 * 6.581212997436523
Epoch 1630, val loss: 1.9534778594970703
Epoch 1640, training loss: 0.06702245771884918 = 0.001381064997985959 + 0.01 * 6.5641398429870605
Epoch 1640, val loss: 1.9539058208465576
Epoch 1650, training loss: 0.0669398158788681 = 0.0013703700387850404 + 0.01 * 6.556944847106934
Epoch 1650, val loss: 1.9542425870895386
Epoch 1660, training loss: 0.06718587130308151 = 0.00135991966817528 + 0.01 * 6.582595348358154
Epoch 1660, val loss: 1.954666256904602
Epoch 1670, training loss: 0.06695220619440079 = 0.0013496802421286702 + 0.01 * 6.560252666473389
Epoch 1670, val loss: 1.9549684524536133
Epoch 1680, training loss: 0.06703737378120422 = 0.0013396532740443945 + 0.01 * 6.569772720336914
Epoch 1680, val loss: 1.955291986465454
Epoch 1690, training loss: 0.06721534579992294 = 0.0013298019766807556 + 0.01 * 6.588554859161377
Epoch 1690, val loss: 1.9556050300598145
Epoch 1700, training loss: 0.06705795228481293 = 0.0013201229739934206 + 0.01 * 6.573782920837402
Epoch 1700, val loss: 1.955897331237793
Epoch 1710, training loss: 0.0672135055065155 = 0.0013106490951031446 + 0.01 * 6.5902862548828125
Epoch 1710, val loss: 1.9560691118240356
Epoch 1720, training loss: 0.06694118678569794 = 0.001301337848417461 + 0.01 * 6.563984394073486
Epoch 1720, val loss: 1.95638108253479
Epoch 1730, training loss: 0.06690675020217896 = 0.0012922282330691814 + 0.01 * 6.561452388763428
Epoch 1730, val loss: 1.9565744400024414
Epoch 1740, training loss: 0.06690968573093414 = 0.0012832505162805319 + 0.01 * 6.562643527984619
Epoch 1740, val loss: 1.9567923545837402
Epoch 1750, training loss: 0.06691904366016388 = 0.0012744561536237597 + 0.01 * 6.564458847045898
Epoch 1750, val loss: 1.9569799900054932
Epoch 1760, training loss: 0.06700796633958817 = 0.0012658617924898863 + 0.01 * 6.5742106437683105
Epoch 1760, val loss: 1.9572796821594238
Epoch 1770, training loss: 0.06700575351715088 = 0.0012573895510286093 + 0.01 * 6.574836730957031
Epoch 1770, val loss: 1.9573326110839844
Epoch 1780, training loss: 0.06687520444393158 = 0.0012490173103287816 + 0.01 * 6.562618732452393
Epoch 1780, val loss: 1.9574449062347412
Epoch 1790, training loss: 0.06686971336603165 = 0.0012408343609422445 + 0.01 * 6.562887668609619
Epoch 1790, val loss: 1.957611322402954
Epoch 1800, training loss: 0.06674844026565552 = 0.001232821960002184 + 0.01 * 6.55156135559082
Epoch 1800, val loss: 1.957830548286438
Epoch 1810, training loss: 0.06655608862638474 = 0.0012249158462509513 + 0.01 * 6.533117771148682
Epoch 1810, val loss: 1.9579325914382935
Epoch 1820, training loss: 0.06689485162496567 = 0.001217157463543117 + 0.01 * 6.567770004272461
Epoch 1820, val loss: 1.9580658674240112
Epoch 1830, training loss: 0.06687289476394653 = 0.0012095140991732478 + 0.01 * 6.566338539123535
Epoch 1830, val loss: 1.958165168762207
Epoch 1840, training loss: 0.06675691902637482 = 0.0012019810965284705 + 0.01 * 6.55549430847168
Epoch 1840, val loss: 1.9582408666610718
Epoch 1850, training loss: 0.06687941402196884 = 0.0011946260929107666 + 0.01 * 6.568479061126709
Epoch 1850, val loss: 1.9584221839904785
Epoch 1860, training loss: 0.06650853157043457 = 0.0011873348848894238 + 0.01 * 6.5321197509765625
Epoch 1860, val loss: 1.9584859609603882
Epoch 1870, training loss: 0.0666954293847084 = 0.0011801904765889049 + 0.01 * 6.5515241622924805
Epoch 1870, val loss: 1.9585044384002686
Epoch 1880, training loss: 0.06661606580018997 = 0.0011731713311746716 + 0.01 * 6.544290065765381
Epoch 1880, val loss: 1.9586389064788818
Epoch 1890, training loss: 0.06667397916316986 = 0.0011662555625662208 + 0.01 * 6.550772666931152
Epoch 1890, val loss: 1.9585198163986206
Epoch 1900, training loss: 0.06669865548610687 = 0.0011594294337555766 + 0.01 * 6.553922653198242
Epoch 1900, val loss: 1.9586468935012817
Epoch 1910, training loss: 0.06661111861467361 = 0.0011527558090165257 + 0.01 * 6.545836448669434
Epoch 1910, val loss: 1.9587018489837646
Epoch 1920, training loss: 0.06661668419837952 = 0.0011461592512205243 + 0.01 * 6.547052383422852
Epoch 1920, val loss: 1.9587316513061523
Epoch 1930, training loss: 0.06651068478822708 = 0.0011396880727261305 + 0.01 * 6.537099838256836
Epoch 1930, val loss: 1.958863615989685
Epoch 1940, training loss: 0.06648173183202744 = 0.0011332338908687234 + 0.01 * 6.534849643707275
Epoch 1940, val loss: 1.9588254690170288
Epoch 1950, training loss: 0.06684797257184982 = 0.001126940012909472 + 0.01 * 6.572103977203369
Epoch 1950, val loss: 1.958888053894043
Epoch 1960, training loss: 0.06652624905109406 = 0.001120711793191731 + 0.01 * 6.540554523468018
Epoch 1960, val loss: 1.958871603012085
Epoch 1970, training loss: 0.06657413393259048 = 0.0011145919561386108 + 0.01 * 6.545954704284668
Epoch 1970, val loss: 1.9588623046875
Epoch 1980, training loss: 0.06648971140384674 = 0.0011085437145084143 + 0.01 * 6.538116931915283
Epoch 1980, val loss: 1.9589064121246338
Epoch 1990, training loss: 0.06641210615634918 = 0.0011025865096598864 + 0.01 * 6.530951499938965
Epoch 1990, val loss: 1.9588236808776855
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.827095413811281
=== training gcn model ===
Epoch 0, training loss: 2.053898334503174 = 1.9679298400878906 + 0.01 * 8.596848487854004
Epoch 0, val loss: 1.9713475704193115
Epoch 10, training loss: 2.043412923812866 = 1.9574450254440308 + 0.01 * 8.596789360046387
Epoch 10, val loss: 1.9612853527069092
Epoch 20, training loss: 2.0303127765655518 = 1.9443467855453491 + 0.01 * 8.596590042114258
Epoch 20, val loss: 1.9482910633087158
Epoch 30, training loss: 2.0117664337158203 = 1.925807237625122 + 0.01 * 8.595930099487305
Epoch 30, val loss: 1.9293667078018188
Epoch 40, training loss: 1.984035849571228 = 1.8981159925460815 + 0.01 * 8.591983795166016
Epoch 40, val loss: 1.9009227752685547
Epoch 50, training loss: 1.9445972442626953 = 1.8589624166488647 + 0.01 * 8.563488960266113
Epoch 50, val loss: 1.8622705936431885
Epoch 60, training loss: 1.899579644203186 = 1.815140962600708 + 0.01 * 8.443868637084961
Epoch 60, val loss: 1.8238065242767334
Epoch 70, training loss: 1.8640024662017822 = 1.7820857763290405 + 0.01 * 8.191667556762695
Epoch 70, val loss: 1.7998191118240356
Epoch 80, training loss: 1.8272695541381836 = 1.7467701435089111 + 0.01 * 8.049936294555664
Epoch 80, val loss: 1.7696782350540161
Epoch 90, training loss: 1.7752091884613037 = 1.6976357698440552 + 0.01 * 7.757345199584961
Epoch 90, val loss: 1.7250168323516846
Epoch 100, training loss: 1.703430414199829 = 1.628391146659851 + 0.01 * 7.503931999206543
Epoch 100, val loss: 1.6643978357315063
Epoch 110, training loss: 1.6109858751296997 = 1.5369417667388916 + 0.01 * 7.404416561126709
Epoch 110, val loss: 1.5870481729507446
Epoch 120, training loss: 1.5034143924713135 = 1.4297435283660889 + 0.01 * 7.367090702056885
Epoch 120, val loss: 1.4983201026916504
Epoch 130, training loss: 1.3915669918060303 = 1.3184677362442017 + 0.01 * 7.3099212646484375
Epoch 130, val loss: 1.4106014966964722
Epoch 140, training loss: 1.2829415798187256 = 1.2103962898254395 + 0.01 * 7.254528045654297
Epoch 140, val loss: 1.3287465572357178
Epoch 150, training loss: 1.1816364526748657 = 1.1095637083053589 + 0.01 * 7.207275390625
Epoch 150, val loss: 1.2553941011428833
Epoch 160, training loss: 1.0904384851455688 = 1.0188121795654297 + 0.01 * 7.162627220153809
Epoch 160, val loss: 1.1923267841339111
Epoch 170, training loss: 1.0093128681182861 = 0.9380565285682678 + 0.01 * 7.1256303787231445
Epoch 170, val loss: 1.1380610466003418
Epoch 180, training loss: 0.9365864396095276 = 0.8656395077705383 + 0.01 * 7.094691276550293
Epoch 180, val loss: 1.091295838356018
Epoch 190, training loss: 0.8712748289108276 = 0.8004962801933289 + 0.01 * 7.077853679656982
Epoch 190, val loss: 1.0509685277938843
Epoch 200, training loss: 0.8129576444625854 = 0.7424018979072571 + 0.01 * 7.055577278137207
Epoch 200, val loss: 1.0175299644470215
Epoch 210, training loss: 0.7611624002456665 = 0.690761148929596 + 0.01 * 7.04012393951416
Epoch 210, val loss: 0.9910390377044678
Epoch 220, training loss: 0.7143269777297974 = 0.6440568566322327 + 0.01 * 7.02701473236084
Epoch 220, val loss: 0.9701802730560303
Epoch 230, training loss: 0.6705256104469299 = 0.6003211736679077 + 0.01 * 7.020445823669434
Epoch 230, val loss: 0.9534441828727722
Epoch 240, training loss: 0.6280627250671387 = 0.557974636554718 + 0.01 * 7.008811950683594
Epoch 240, val loss: 0.9391067028045654
Epoch 250, training loss: 0.5862166881561279 = 0.5161929130554199 + 0.01 * 7.002379894256592
Epoch 250, val loss: 0.9265317320823669
Epoch 260, training loss: 0.5449475646018982 = 0.4749932885169983 + 0.01 * 6.995425701141357
Epoch 260, val loss: 0.9162660837173462
Epoch 270, training loss: 0.5047339797019958 = 0.4348166882991791 + 0.01 * 6.991731643676758
Epoch 270, val loss: 0.9093486666679382
Epoch 280, training loss: 0.4661394953727722 = 0.39626556634902954 + 0.01 * 6.987393379211426
Epoch 280, val loss: 0.906589686870575
Epoch 290, training loss: 0.42965179681777954 = 0.3598141074180603 + 0.01 * 6.983768463134766
Epoch 290, val loss: 0.908122718334198
Epoch 300, training loss: 0.39552026987075806 = 0.3257248103618622 + 0.01 * 6.979544639587402
Epoch 300, val loss: 0.913651704788208
Epoch 310, training loss: 0.3637502193450928 = 0.29399651288986206 + 0.01 * 6.975369930267334
Epoch 310, val loss: 0.9223911166191101
Epoch 320, training loss: 0.3342058062553406 = 0.2644958198070526 + 0.01 * 6.970997333526611
Epoch 320, val loss: 0.9334551692008972
Epoch 330, training loss: 0.306684285402298 = 0.23701609671115875 + 0.01 * 6.966819763183594
Epoch 330, val loss: 0.9463059902191162
Epoch 340, training loss: 0.28106963634490967 = 0.21141859889030457 + 0.01 * 6.965104579925537
Epoch 340, val loss: 0.9604019522666931
Epoch 350, training loss: 0.2573036551475525 = 0.18764008581638336 + 0.01 * 6.966355323791504
Epoch 350, val loss: 0.9753260612487793
Epoch 360, training loss: 0.2353402078151703 = 0.16572076082229614 + 0.01 * 6.961945533752441
Epoch 360, val loss: 0.9908452033996582
Epoch 370, training loss: 0.215312659740448 = 0.14573009312152863 + 0.01 * 6.95825719833374
Epoch 370, val loss: 1.0067862272262573
Epoch 380, training loss: 0.19734185934066772 = 0.12772466242313385 + 0.01 * 6.961719036102295
Epoch 380, val loss: 1.023188829421997
Epoch 390, training loss: 0.18128696084022522 = 0.11172999441623688 + 0.01 * 6.9556965827941895
Epoch 390, val loss: 1.0402120351791382
Epoch 400, training loss: 0.16721516847610474 = 0.0976925641298294 + 0.01 * 6.952259540557861
Epoch 400, val loss: 1.0576878786087036
Epoch 410, training loss: 0.1549782156944275 = 0.08548861742019653 + 0.01 * 6.948958873748779
Epoch 410, val loss: 1.0757626295089722
Epoch 420, training loss: 0.1444636881351471 = 0.07494840025901794 + 0.01 * 6.951528072357178
Epoch 420, val loss: 1.0940980911254883
Epoch 430, training loss: 0.135308176279068 = 0.0658932775259018 + 0.01 * 6.94149112701416
Epoch 430, val loss: 1.1126784086227417
Epoch 440, training loss: 0.12750883400440216 = 0.058132268488407135 + 0.01 * 6.937656402587891
Epoch 440, val loss: 1.1312216520309448
Epoch 450, training loss: 0.1208127960562706 = 0.051486365497112274 + 0.01 * 6.932642936706543
Epoch 450, val loss: 1.1495275497436523
Epoch 460, training loss: 0.11508582532405853 = 0.045788925141096115 + 0.01 * 6.929690361022949
Epoch 460, val loss: 1.167441725730896
Epoch 470, training loss: 0.1102101057767868 = 0.04089828208088875 + 0.01 * 6.931182384490967
Epoch 470, val loss: 1.1848185062408447
Epoch 480, training loss: 0.10589858889579773 = 0.036688778549432755 + 0.01 * 6.920980453491211
Epoch 480, val loss: 1.2018027305603027
Epoch 490, training loss: 0.10222366452217102 = 0.03305325657129288 + 0.01 * 6.917041301727295
Epoch 490, val loss: 1.2181024551391602
Epoch 500, training loss: 0.09908079355955124 = 0.029902080073952675 + 0.01 * 6.917871952056885
Epoch 500, val loss: 1.2339553833007812
Epoch 510, training loss: 0.09620756655931473 = 0.027160780504345894 + 0.01 * 6.904678821563721
Epoch 510, val loss: 1.2490222454071045
Epoch 520, training loss: 0.09379260241985321 = 0.02476876974105835 + 0.01 * 6.902383327484131
Epoch 520, val loss: 1.263649821281433
Epoch 530, training loss: 0.09165646880865097 = 0.022672809660434723 + 0.01 * 6.8983659744262695
Epoch 530, val loss: 1.2776764631271362
Epoch 540, training loss: 0.08985111117362976 = 0.0208280049264431 + 0.01 * 6.902310371398926
Epoch 540, val loss: 1.2912389039993286
Epoch 550, training loss: 0.08814413100481033 = 0.019198572263121605 + 0.01 * 6.894555568695068
Epoch 550, val loss: 1.304296612739563
Epoch 560, training loss: 0.0865638330578804 = 0.01775313727557659 + 0.01 * 6.881069183349609
Epoch 560, val loss: 1.316849708557129
Epoch 570, training loss: 0.08527515083551407 = 0.01646609790623188 + 0.01 * 6.880905628204346
Epoch 570, val loss: 1.3289424180984497
Epoch 580, training loss: 0.08403227478265762 = 0.015317135490477085 + 0.01 * 6.871514320373535
Epoch 580, val loss: 1.3406389951705933
Epoch 590, training loss: 0.08297854661941528 = 0.014286532998085022 + 0.01 * 6.869201183319092
Epoch 590, val loss: 1.351901888847351
Epoch 600, training loss: 0.08198589086532593 = 0.013357577845454216 + 0.01 * 6.862831115722656
Epoch 600, val loss: 1.3627747297286987
Epoch 610, training loss: 0.08151624351739883 = 0.01251702569425106 + 0.01 * 6.899921894073486
Epoch 610, val loss: 1.3733776807785034
Epoch 620, training loss: 0.08044016361236572 = 0.011758057400584221 + 0.01 * 6.868210315704346
Epoch 620, val loss: 1.3834705352783203
Epoch 630, training loss: 0.07964734733104706 = 0.01106934156268835 + 0.01 * 6.857800483703613
Epoch 630, val loss: 1.3933335542678833
Epoch 640, training loss: 0.07893785834312439 = 0.010441171005368233 + 0.01 * 6.849668979644775
Epoch 640, val loss: 1.4028711318969727
Epoch 650, training loss: 0.07831710577011108 = 0.009866212494671345 + 0.01 * 6.845089435577393
Epoch 650, val loss: 1.4121558666229248
Epoch 660, training loss: 0.07775917649269104 = 0.009338300675153732 + 0.01 * 6.842087268829346
Epoch 660, val loss: 1.421224594116211
Epoch 670, training loss: 0.07727158069610596 = 0.008853045292198658 + 0.01 * 6.841853618621826
Epoch 670, val loss: 1.4299982786178589
Epoch 680, training loss: 0.07674748450517654 = 0.008406554348766804 + 0.01 * 6.834092617034912
Epoch 680, val loss: 1.4385654926300049
Epoch 690, training loss: 0.07633540034294128 = 0.007994270883500576 + 0.01 * 6.834112644195557
Epoch 690, val loss: 1.4469501972198486
Epoch 700, training loss: 0.0758965015411377 = 0.007612638175487518 + 0.01 * 6.8283867835998535
Epoch 700, val loss: 1.4550851583480835
Epoch 710, training loss: 0.07579407840967178 = 0.007258655969053507 + 0.01 * 6.853542804718018
Epoch 710, val loss: 1.4629825353622437
Epoch 720, training loss: 0.0751757100224495 = 0.0069306944496929646 + 0.01 * 6.8245015144348145
Epoch 720, val loss: 1.4706634283065796
Epoch 730, training loss: 0.07482986897230148 = 0.006625728216022253 + 0.01 * 6.8204145431518555
Epoch 730, val loss: 1.4782334566116333
Epoch 740, training loss: 0.07458902150392532 = 0.006341586355119944 + 0.01 * 6.82474422454834
Epoch 740, val loss: 1.4855469465255737
Epoch 750, training loss: 0.0742606520652771 = 0.0060765850357711315 + 0.01 * 6.81840705871582
Epoch 750, val loss: 1.4927501678466797
Epoch 760, training loss: 0.07395254075527191 = 0.005828781519085169 + 0.01 * 6.812376022338867
Epoch 760, val loss: 1.4997823238372803
Epoch 770, training loss: 0.0739462822675705 = 0.005596827249974012 + 0.01 * 6.834946155548096
Epoch 770, val loss: 1.5066574811935425
Epoch 780, training loss: 0.07353120297193527 = 0.005379821173846722 + 0.01 * 6.815138339996338
Epoch 780, val loss: 1.5132883787155151
Epoch 790, training loss: 0.07327982783317566 = 0.005176439881324768 + 0.01 * 6.810338973999023
Epoch 790, val loss: 1.519869089126587
Epoch 800, training loss: 0.07309513539075851 = 0.004985330626368523 + 0.01 * 6.810980796813965
Epoch 800, val loss: 1.52627694606781
Epoch 810, training loss: 0.07281305640935898 = 0.004805612377822399 + 0.01 * 6.800744533538818
Epoch 810, val loss: 1.5324889421463013
Epoch 820, training loss: 0.07260109484195709 = 0.004636548925191164 + 0.01 * 6.796454429626465
Epoch 820, val loss: 1.5386276245117188
Epoch 830, training loss: 0.07255299389362335 = 0.004476931411772966 + 0.01 * 6.807606220245361
Epoch 830, val loss: 1.544668436050415
Epoch 840, training loss: 0.07225296646356583 = 0.004326669033616781 + 0.01 * 6.792629718780518
Epoch 840, val loss: 1.5504322052001953
Epoch 850, training loss: 0.07207184284925461 = 0.004184524528682232 + 0.01 * 6.788732051849365
Epoch 850, val loss: 1.5562325716018677
Epoch 860, training loss: 0.071933813393116 = 0.004050715826451778 + 0.01 * 6.7883100509643555
Epoch 860, val loss: 1.5617098808288574
Epoch 870, training loss: 0.0718488022685051 = 0.003923952579498291 + 0.01 * 6.792485237121582
Epoch 870, val loss: 1.5671803951263428
Epoch 880, training loss: 0.07160618156194687 = 0.0038039737846702337 + 0.01 * 6.7802205085754395
Epoch 880, val loss: 1.572593092918396
Epoch 890, training loss: 0.07149732857942581 = 0.0036902427673339844 + 0.01 * 6.7807087898254395
Epoch 890, val loss: 1.5778621435165405
Epoch 900, training loss: 0.0714082419872284 = 0.0035822433419525623 + 0.01 * 6.782599925994873
Epoch 900, val loss: 1.582962989807129
Epoch 910, training loss: 0.07130055874586105 = 0.0034797904081642628 + 0.01 * 6.782076358795166
Epoch 910, val loss: 1.5880060195922852
Epoch 920, training loss: 0.07110817730426788 = 0.0033823971170932055 + 0.01 * 6.77257776260376
Epoch 920, val loss: 1.5930033922195435
Epoch 930, training loss: 0.07112088054418564 = 0.003289722139015794 + 0.01 * 6.783115863800049
Epoch 930, val loss: 1.5978538990020752
Epoch 940, training loss: 0.07085910439491272 = 0.003201595041900873 + 0.01 * 6.765750885009766
Epoch 940, val loss: 1.602541208267212
Epoch 950, training loss: 0.07095380872488022 = 0.003117684042081237 + 0.01 * 6.7836127281188965
Epoch 950, val loss: 1.6072794198989868
Epoch 960, training loss: 0.07078239321708679 = 0.003037851769477129 + 0.01 * 6.774454116821289
Epoch 960, val loss: 1.6117397546768188
Epoch 970, training loss: 0.07062134146690369 = 0.002961530117318034 + 0.01 * 6.765981197357178
Epoch 970, val loss: 1.616263508796692
Epoch 980, training loss: 0.07064639776945114 = 0.002888622460886836 + 0.01 * 6.775778293609619
Epoch 980, val loss: 1.620685338973999
Epoch 990, training loss: 0.07051023840904236 = 0.002819314831867814 + 0.01 * 6.769092559814453
Epoch 990, val loss: 1.6248257160186768
Epoch 1000, training loss: 0.07035032659769058 = 0.002752972301095724 + 0.01 * 6.759735107421875
Epoch 1000, val loss: 1.6290613412857056
Epoch 1010, training loss: 0.07024040073156357 = 0.0026893524918705225 + 0.01 * 6.755105018615723
Epoch 1010, val loss: 1.6332075595855713
Epoch 1020, training loss: 0.07033118605613708 = 0.0026285352651029825 + 0.01 * 6.770265102386475
Epoch 1020, val loss: 1.6372572183609009
Epoch 1030, training loss: 0.07017635554075241 = 0.0025704645086079836 + 0.01 * 6.760589122772217
Epoch 1030, val loss: 1.6411573886871338
Epoch 1040, training loss: 0.07002757489681244 = 0.0025145895779132843 + 0.01 * 6.7512993812561035
Epoch 1040, val loss: 1.6450601816177368
Epoch 1050, training loss: 0.07001638412475586 = 0.0024611970875412226 + 0.01 * 6.755518913269043
Epoch 1050, val loss: 1.6488746404647827
Epoch 1060, training loss: 0.0698704868555069 = 0.0024099256843328476 + 0.01 * 6.746056079864502
Epoch 1060, val loss: 1.6525723934173584
Epoch 1070, training loss: 0.069814532995224 = 0.0023606589529663324 + 0.01 * 6.745388031005859
Epoch 1070, val loss: 1.6562325954437256
Epoch 1080, training loss: 0.06973391771316528 = 0.002313476987183094 + 0.01 * 6.742043972015381
Epoch 1080, val loss: 1.6598623991012573
Epoch 1090, training loss: 0.06970363855361938 = 0.00226787431165576 + 0.01 * 6.743576526641846
Epoch 1090, val loss: 1.6633437871932983
Epoch 1100, training loss: 0.06968557834625244 = 0.0022243382409214973 + 0.01 * 6.746123790740967
Epoch 1100, val loss: 1.666759729385376
Epoch 1110, training loss: 0.06954718381166458 = 0.0021823926363140345 + 0.01 * 6.73647928237915
Epoch 1110, val loss: 1.6700925827026367
Epoch 1120, training loss: 0.06944791972637177 = 0.002142021432518959 + 0.01 * 6.730589866638184
Epoch 1120, val loss: 1.6734049320220947
Epoch 1130, training loss: 0.06961463391780853 = 0.002103040460497141 + 0.01 * 6.75115966796875
Epoch 1130, val loss: 1.676679253578186
Epoch 1140, training loss: 0.06943975389003754 = 0.002065730281174183 + 0.01 * 6.737402439117432
Epoch 1140, val loss: 1.6797187328338623
Epoch 1150, training loss: 0.06947343051433563 = 0.002029634080827236 + 0.01 * 6.744379997253418
Epoch 1150, val loss: 1.682895302772522
Epoch 1160, training loss: 0.06919548660516739 = 0.0019949653651565313 + 0.01 * 6.720052242279053
Epoch 1160, val loss: 1.6858727931976318
Epoch 1170, training loss: 0.06934487819671631 = 0.0019613541662693024 + 0.01 * 6.738351821899414
Epoch 1170, val loss: 1.6889066696166992
Epoch 1180, training loss: 0.069179967045784 = 0.0019291588105261326 + 0.01 * 6.725081443786621
Epoch 1180, val loss: 1.691758155822754
Epoch 1190, training loss: 0.0691271722316742 = 0.0018978059524670243 + 0.01 * 6.722936630249023
Epoch 1190, val loss: 1.6946591138839722
Epoch 1200, training loss: 0.06913920491933823 = 0.00186758185736835 + 0.01 * 6.7271623611450195
Epoch 1200, val loss: 1.697502613067627
Epoch 1210, training loss: 0.06900207698345184 = 0.0018384258728474379 + 0.01 * 6.716365337371826
Epoch 1210, val loss: 1.7003065347671509
Epoch 1220, training loss: 0.06919905543327332 = 0.0018102352041751146 + 0.01 * 6.738882064819336
Epoch 1220, val loss: 1.7030342817306519
Epoch 1230, training loss: 0.06886015832424164 = 0.0017829538555815816 + 0.01 * 6.707720756530762
Epoch 1230, val loss: 1.705627679824829
Epoch 1240, training loss: 0.06905800104141235 = 0.0017565557500347495 + 0.01 * 6.73014497756958
Epoch 1240, val loss: 1.708313226699829
Epoch 1250, training loss: 0.06885816901922226 = 0.0017311281990259886 + 0.01 * 6.712704181671143
Epoch 1250, val loss: 1.7107774019241333
Epoch 1260, training loss: 0.0688132792711258 = 0.001706469338387251 + 0.01 * 6.710681438446045
Epoch 1260, val loss: 1.7132948637008667
Epoch 1270, training loss: 0.06882021576166153 = 0.0016827286453917623 + 0.01 * 6.713748931884766
Epoch 1270, val loss: 1.7157409191131592
Epoch 1280, training loss: 0.06873267889022827 = 0.0016594891203567386 + 0.01 * 6.7073187828063965
Epoch 1280, val loss: 1.7182625532150269
Epoch 1290, training loss: 0.06861048936843872 = 0.0016370757948607206 + 0.01 * 6.697341442108154
Epoch 1290, val loss: 1.720595359802246
Epoch 1300, training loss: 0.06884778290987015 = 0.0016153304604813457 + 0.01 * 6.723245143890381
Epoch 1300, val loss: 1.7229962348937988
Epoch 1310, training loss: 0.0686006098985672 = 0.0015942767495289445 + 0.01 * 6.700633525848389
Epoch 1310, val loss: 1.7252137660980225
Epoch 1320, training loss: 0.06856304407119751 = 0.0015739090740680695 + 0.01 * 6.69891357421875
Epoch 1320, val loss: 1.727489709854126
Epoch 1330, training loss: 0.06869886815547943 = 0.0015541405882686377 + 0.01 * 6.714472770690918
Epoch 1330, val loss: 1.7295984029769897
Epoch 1340, training loss: 0.0684909075498581 = 0.001534844283014536 + 0.01 * 6.695606708526611
Epoch 1340, val loss: 1.7318799495697021
Epoch 1350, training loss: 0.06862682849168777 = 0.0015161443734541535 + 0.01 * 6.711068630218506
Epoch 1350, val loss: 1.734019160270691
Epoch 1360, training loss: 0.06852935254573822 = 0.0014979547122493386 + 0.01 * 6.703139781951904
Epoch 1360, val loss: 1.7360588312149048
Epoch 1370, training loss: 0.0684274286031723 = 0.0014803636586293578 + 0.01 * 6.694706439971924
Epoch 1370, val loss: 1.738147497177124
Epoch 1380, training loss: 0.06839422881603241 = 0.0014631710946559906 + 0.01 * 6.693105220794678
Epoch 1380, val loss: 1.7401517629623413
Epoch 1390, training loss: 0.06831078976392746 = 0.0014465262647718191 + 0.01 * 6.686426639556885
Epoch 1390, val loss: 1.7421629428863525
Epoch 1400, training loss: 0.06847559660673141 = 0.0014303644420579076 + 0.01 * 6.70452356338501
Epoch 1400, val loss: 1.7440569400787354
Epoch 1410, training loss: 0.06825117021799088 = 0.0014145794557407498 + 0.01 * 6.683658599853516
Epoch 1410, val loss: 1.7459218502044678
Epoch 1420, training loss: 0.06835475564002991 = 0.0013993075117468834 + 0.01 * 6.695544719696045
Epoch 1420, val loss: 1.7477973699569702
Epoch 1430, training loss: 0.0682586058974266 = 0.001384441857226193 + 0.01 * 6.687416076660156
Epoch 1430, val loss: 1.7496050596237183
Epoch 1440, training loss: 0.06832822412252426 = 0.0013698962284252048 + 0.01 * 6.695833206176758
Epoch 1440, val loss: 1.7514170408248901
Epoch 1450, training loss: 0.0681285411119461 = 0.0013558478094637394 + 0.01 * 6.677268981933594
Epoch 1450, val loss: 1.7531152963638306
Epoch 1460, training loss: 0.06827008724212646 = 0.0013420643517747521 + 0.01 * 6.6928019523620605
Epoch 1460, val loss: 1.7549376487731934
Epoch 1470, training loss: 0.06808389723300934 = 0.0013286492321640253 + 0.01 * 6.675525188446045
Epoch 1470, val loss: 1.7565964460372925
Epoch 1480, training loss: 0.06813391298055649 = 0.001315623172558844 + 0.01 * 6.681829452514648
Epoch 1480, val loss: 1.7582612037658691
Epoch 1490, training loss: 0.06819053739309311 = 0.0013028583489358425 + 0.01 * 6.688767910003662
Epoch 1490, val loss: 1.7599061727523804
Epoch 1500, training loss: 0.06817026436328888 = 0.0012904810719192028 + 0.01 * 6.687978744506836
Epoch 1500, val loss: 1.7614223957061768
Epoch 1510, training loss: 0.06803620606660843 = 0.0012783799320459366 + 0.01 * 6.675782680511475
Epoch 1510, val loss: 1.7630445957183838
Epoch 1520, training loss: 0.06814628094434738 = 0.0012666197726503015 + 0.01 * 6.687966346740723
Epoch 1520, val loss: 1.7645916938781738
Epoch 1530, training loss: 0.0679859071969986 = 0.001255118171684444 + 0.01 * 6.673079490661621
Epoch 1530, val loss: 1.766019582748413
Epoch 1540, training loss: 0.06791090220212936 = 0.0012439279817044735 + 0.01 * 6.666697978973389
Epoch 1540, val loss: 1.7674317359924316
Epoch 1550, training loss: 0.06794758141040802 = 0.0012330343015491962 + 0.01 * 6.671454906463623
Epoch 1550, val loss: 1.768971562385559
Epoch 1560, training loss: 0.06792664527893066 = 0.0012223627418279648 + 0.01 * 6.670428276062012
Epoch 1560, val loss: 1.7703415155410767
Epoch 1570, training loss: 0.06780155003070831 = 0.0012119065504521132 + 0.01 * 6.65896463394165
Epoch 1570, val loss: 1.7718194723129272
Epoch 1580, training loss: 0.06784214079380035 = 0.0012016845867037773 + 0.01 * 6.664046287536621
Epoch 1580, val loss: 1.7731207609176636
Epoch 1590, training loss: 0.06774154305458069 = 0.0011916512157768011 + 0.01 * 6.654989242553711
Epoch 1590, val loss: 1.7746555805206299
Epoch 1600, training loss: 0.06785264611244202 = 0.0011819344945251942 + 0.01 * 6.667071342468262
Epoch 1600, val loss: 1.7759101390838623
Epoch 1610, training loss: 0.06780901551246643 = 0.0011723976349458098 + 0.01 * 6.663661479949951
Epoch 1610, val loss: 1.7772549390792847
Epoch 1620, training loss: 0.06774032115936279 = 0.0011631249217316508 + 0.01 * 6.657719612121582
Epoch 1620, val loss: 1.7785524129867554
Epoch 1630, training loss: 0.06772428750991821 = 0.0011540044797584414 + 0.01 * 6.657028675079346
Epoch 1630, val loss: 1.7798773050308228
Epoch 1640, training loss: 0.06790517270565033 = 0.0011450921883806586 + 0.01 * 6.6760077476501465
Epoch 1640, val loss: 1.7810219526290894
Epoch 1650, training loss: 0.06766095757484436 = 0.0011363994562998414 + 0.01 * 6.652456283569336
Epoch 1650, val loss: 1.7822937965393066
Epoch 1660, training loss: 0.06775177270174026 = 0.0011279912432655692 + 0.01 * 6.662378787994385
Epoch 1660, val loss: 1.7834714651107788
Epoch 1670, training loss: 0.06764603406190872 = 0.0011196518316864967 + 0.01 * 6.6526384353637695
Epoch 1670, val loss: 1.7848060131072998
Epoch 1680, training loss: 0.06777269393205643 = 0.001111502991989255 + 0.01 * 6.666119575500488
Epoch 1680, val loss: 1.7858777046203613
Epoch 1690, training loss: 0.067594014108181 = 0.0011035602074116468 + 0.01 * 6.649045467376709
Epoch 1690, val loss: 1.786997675895691
Epoch 1700, training loss: 0.06759273260831833 = 0.0010958056664094329 + 0.01 * 6.649692535400391
Epoch 1700, val loss: 1.7880773544311523
Epoch 1710, training loss: 0.06755152344703674 = 0.001088157296180725 + 0.01 * 6.646336555480957
Epoch 1710, val loss: 1.7892732620239258
Epoch 1720, training loss: 0.0676371231675148 = 0.0010807422222569585 + 0.01 * 6.655638217926025
Epoch 1720, val loss: 1.7903481721878052
Epoch 1730, training loss: 0.06766048073768616 = 0.0010734007228165865 + 0.01 * 6.658708095550537
Epoch 1730, val loss: 1.7914615869522095
Epoch 1740, training loss: 0.06745273619890213 = 0.0010662088170647621 + 0.01 * 6.638652801513672
Epoch 1740, val loss: 1.7925256490707397
Epoch 1750, training loss: 0.06746108829975128 = 0.0010591821046546102 + 0.01 * 6.640190601348877
Epoch 1750, val loss: 1.7935409545898438
Epoch 1760, training loss: 0.06745272874832153 = 0.0010523052187636495 + 0.01 * 6.640042304992676
Epoch 1760, val loss: 1.7946810722351074
Epoch 1770, training loss: 0.06763774156570435 = 0.0010455993469804525 + 0.01 * 6.659214019775391
Epoch 1770, val loss: 1.7954853773117065
Epoch 1780, training loss: 0.0673755407333374 = 0.001039062044583261 + 0.01 * 6.633647441864014
Epoch 1780, val loss: 1.7964273691177368
Epoch 1790, training loss: 0.06753884255886078 = 0.0010325617622584105 + 0.01 * 6.650628566741943
Epoch 1790, val loss: 1.7974512577056885
Epoch 1800, training loss: 0.06737300753593445 = 0.0010262540308758616 + 0.01 * 6.634675979614258
Epoch 1800, val loss: 1.7982442378997803
Epoch 1810, training loss: 0.06755632162094116 = 0.00102001812774688 + 0.01 * 6.653630256652832
Epoch 1810, val loss: 1.7991863489151
Epoch 1820, training loss: 0.06736081838607788 = 0.0010139233199879527 + 0.01 * 6.634690284729004
Epoch 1820, val loss: 1.8001610040664673
Epoch 1830, training loss: 0.0673685073852539 = 0.0010079015046358109 + 0.01 * 6.63606071472168
Epoch 1830, val loss: 1.8009140491485596
Epoch 1840, training loss: 0.06729947030544281 = 0.0010020656045526266 + 0.01 * 6.6297407150268555
Epoch 1840, val loss: 1.8018085956573486
Epoch 1850, training loss: 0.06726902723312378 = 0.0009962768526747823 + 0.01 * 6.627275466918945
Epoch 1850, val loss: 1.8026210069656372
Epoch 1860, training loss: 0.06736503541469574 = 0.0009906416526064277 + 0.01 * 6.637439250946045
Epoch 1860, val loss: 1.8034358024597168
Epoch 1870, training loss: 0.0672677606344223 = 0.0009850746719166636 + 0.01 * 6.628269195556641
Epoch 1870, val loss: 1.8042150735855103
Epoch 1880, training loss: 0.06748131662607193 = 0.000979632488451898 + 0.01 * 6.6501688957214355
Epoch 1880, val loss: 1.8048895597457886
Epoch 1890, training loss: 0.06719108670949936 = 0.0009742737165652215 + 0.01 * 6.621681213378906
Epoch 1890, val loss: 1.8057256937026978
Epoch 1900, training loss: 0.06713246554136276 = 0.0009690154111012816 + 0.01 * 6.616344928741455
Epoch 1900, val loss: 1.8065857887268066
Epoch 1910, training loss: 0.06719788163900375 = 0.0009638096671551466 + 0.01 * 6.623407363891602
Epoch 1910, val loss: 1.8072891235351562
Epoch 1920, training loss: 0.06722012162208557 = 0.0009587506065145135 + 0.01 * 6.6261372566223145
Epoch 1920, val loss: 1.8079736232757568
Epoch 1930, training loss: 0.06718499958515167 = 0.0009537552832625806 + 0.01 * 6.623124599456787
Epoch 1930, val loss: 1.8087856769561768
Epoch 1940, training loss: 0.0671696737408638 = 0.0009488979121670127 + 0.01 * 6.622077465057373
Epoch 1940, val loss: 1.8094048500061035
Epoch 1950, training loss: 0.06711000949144363 = 0.0009440716821700335 + 0.01 * 6.616594314575195
Epoch 1950, val loss: 1.8101286888122559
Epoch 1960, training loss: 0.06705652922391891 = 0.0009393813088536263 + 0.01 * 6.6117143630981445
Epoch 1960, val loss: 1.8107789754867554
Epoch 1970, training loss: 0.06698081642389297 = 0.0009346974547952414 + 0.01 * 6.604611873626709
Epoch 1970, val loss: 1.8115482330322266
Epoch 1980, training loss: 0.06700200587511063 = 0.0009301240788772702 + 0.01 * 6.6071882247924805
Epoch 1980, val loss: 1.8121954202651978
Epoch 1990, training loss: 0.06698392331600189 = 0.0009255901677533984 + 0.01 * 6.605833530426025
Epoch 1990, val loss: 1.8128825426101685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 2.024873733520508 = 1.9389054775238037 + 0.01 * 8.59683609008789
Epoch 0, val loss: 1.9419406652450562
Epoch 10, training loss: 2.015350341796875 = 1.9293824434280396 + 0.01 * 8.596784591674805
Epoch 10, val loss: 1.932224988937378
Epoch 20, training loss: 2.0039772987365723 = 1.9180119037628174 + 0.01 * 8.59653377532959
Epoch 20, val loss: 1.9204790592193604
Epoch 30, training loss: 1.9882466793060303 = 1.9022914171218872 + 0.01 * 8.595532417297363
Epoch 30, val loss: 1.9043238162994385
Epoch 40, training loss: 1.9649760723114014 = 1.8790839910507202 + 0.01 * 8.589207649230957
Epoch 40, val loss: 1.8811485767364502
Epoch 50, training loss: 1.931926965713501 = 1.8464336395263672 + 0.01 * 8.549337387084961
Epoch 50, val loss: 1.8503843545913696
Epoch 60, training loss: 1.8934566974639893 = 1.8097577095031738 + 0.01 * 8.369894981384277
Epoch 60, val loss: 1.819865345954895
Epoch 70, training loss: 1.8603113889694214 = 1.7798014879226685 + 0.01 * 8.050989151000977
Epoch 70, val loss: 1.7970117330551147
Epoch 80, training loss: 1.8219242095947266 = 1.7433679103851318 + 0.01 * 7.855630874633789
Epoch 80, val loss: 1.763800024986267
Epoch 90, training loss: 1.768181324005127 = 1.6916009187698364 + 0.01 * 7.658045768737793
Epoch 90, val loss: 1.719335913658142
Epoch 100, training loss: 1.6954307556152344 = 1.6201982498168945 + 0.01 * 7.523246765136719
Epoch 100, val loss: 1.6615350246429443
Epoch 110, training loss: 1.6029939651489258 = 1.5286791324615479 + 0.01 * 7.431488990783691
Epoch 110, val loss: 1.586796760559082
Epoch 120, training loss: 1.4986271858215332 = 1.4249906539916992 + 0.01 * 7.363653182983398
Epoch 120, val loss: 1.5038529634475708
Epoch 130, training loss: 1.3924591541290283 = 1.3193871974945068 + 0.01 * 7.307191371917725
Epoch 130, val loss: 1.4201396703720093
Epoch 140, training loss: 1.2913591861724854 = 1.2187408208847046 + 0.01 * 7.261836051940918
Epoch 140, val loss: 1.3435537815093994
Epoch 150, training loss: 1.1996594667434692 = 1.127410888671875 + 0.01 * 7.224858283996582
Epoch 150, val loss: 1.2774603366851807
Epoch 160, training loss: 1.1191993951797485 = 1.0471841096878052 + 0.01 * 7.201532363891602
Epoch 160, val loss: 1.223034143447876
Epoch 170, training loss: 1.0484392642974854 = 0.9765288829803467 + 0.01 * 7.191036701202393
Epoch 170, val loss: 1.1774888038635254
Epoch 180, training loss: 0.9834516644477844 = 0.9115816354751587 + 0.01 * 7.187000751495361
Epoch 180, val loss: 1.1364339590072632
Epoch 190, training loss: 0.9199796915054321 = 0.848150908946991 + 0.01 * 7.182879447937012
Epoch 190, val loss: 1.096335768699646
Epoch 200, training loss: 0.8550640344619751 = 0.7832944989204407 + 0.01 * 7.176955223083496
Epoch 200, val loss: 1.0552674531936646
Epoch 210, training loss: 0.7881197929382324 = 0.7164403200149536 + 0.01 * 7.167946815490723
Epoch 210, val loss: 1.0132659673690796
Epoch 220, training loss: 0.721008837223053 = 0.6494687795639038 + 0.01 * 7.154004096984863
Epoch 220, val loss: 0.9734059572219849
Epoch 230, training loss: 0.6564966440200806 = 0.5851437449455261 + 0.01 * 7.135289192199707
Epoch 230, val loss: 0.9385157227516174
Epoch 240, training loss: 0.5966232419013977 = 0.5254938006401062 + 0.01 * 7.112946510314941
Epoch 240, val loss: 0.9105713963508606
Epoch 250, training loss: 0.5416257381439209 = 0.47068262100219727 + 0.01 * 7.094310760498047
Epoch 250, val loss: 0.8897502422332764
Epoch 260, training loss: 0.4901834726333618 = 0.41940975189208984 + 0.01 * 7.077372074127197
Epoch 260, val loss: 0.874724805355072
Epoch 270, training loss: 0.4409523606300354 = 0.3702458441257477 + 0.01 * 7.070650577545166
Epoch 270, val loss: 0.864319920539856
Epoch 280, training loss: 0.39341920614242554 = 0.32277607917785645 + 0.01 * 7.0643110275268555
Epoch 280, val loss: 0.8579076528549194
Epoch 290, training loss: 0.3484790325164795 = 0.2778815031051636 + 0.01 * 7.059754848480225
Epoch 290, val loss: 0.8557907938957214
Epoch 300, training loss: 0.3076130151748657 = 0.23703831434249878 + 0.01 * 7.057468414306641
Epoch 300, val loss: 0.8583348393440247
Epoch 310, training loss: 0.27196502685546875 = 0.20140954852104187 + 0.01 * 7.055548667907715
Epoch 310, val loss: 0.8657733201980591
Epoch 320, training loss: 0.24186080694198608 = 0.17132878303527832 + 0.01 * 7.053201675415039
Epoch 320, val loss: 0.8780716061592102
Epoch 330, training loss: 0.2168836146593094 = 0.14640294015407562 + 0.01 * 7.048067569732666
Epoch 330, val loss: 0.8945073485374451
Epoch 340, training loss: 0.1963336169719696 = 0.12588292360305786 + 0.01 * 7.045068740844727
Epoch 340, val loss: 0.9140110015869141
Epoch 350, training loss: 0.1793755739927292 = 0.10897339880466461 + 0.01 * 7.040217876434326
Epoch 350, val loss: 0.9356356263160706
Epoch 360, training loss: 0.16532963514328003 = 0.09497332572937012 + 0.01 * 7.035630702972412
Epoch 360, val loss: 0.9586181640625
Epoch 370, training loss: 0.1536104679107666 = 0.08331034332513809 + 0.01 * 7.030013561248779
Epoch 370, val loss: 0.9823421835899353
Epoch 380, training loss: 0.14377325773239136 = 0.07352948933839798 + 0.01 * 7.024376392364502
Epoch 380, val loss: 1.0063518285751343
Epoch 390, training loss: 0.13553160429000854 = 0.06527391821146011 + 0.01 * 7.025768280029297
Epoch 390, val loss: 1.0303010940551758
Epoch 400, training loss: 0.1283421665430069 = 0.05825772136449814 + 0.01 * 7.008444309234619
Epoch 400, val loss: 1.0538880825042725
Epoch 410, training loss: 0.12227658927440643 = 0.052250608801841736 + 0.01 * 7.002598285675049
Epoch 410, val loss: 1.077053427696228
Epoch 420, training loss: 0.11728867888450623 = 0.04707356542348862 + 0.01 * 7.021511554718018
Epoch 420, val loss: 1.0996513366699219
Epoch 430, training loss: 0.11250516772270203 = 0.04259074106812477 + 0.01 * 6.991443157196045
Epoch 430, val loss: 1.1215776205062866
Epoch 440, training loss: 0.10843777656555176 = 0.038683321326971054 + 0.01 * 6.975445747375488
Epoch 440, val loss: 1.14287269115448
Epoch 450, training loss: 0.10494866222143173 = 0.03525827080011368 + 0.01 * 6.969038963317871
Epoch 450, val loss: 1.1635637283325195
Epoch 460, training loss: 0.1018700823187828 = 0.0322415828704834 + 0.01 * 6.962850093841553
Epoch 460, val loss: 1.183576226234436
Epoch 470, training loss: 0.099040187895298 = 0.029567977413535118 + 0.01 * 6.947220802307129
Epoch 470, val loss: 1.2029916048049927
Epoch 480, training loss: 0.09662242978811264 = 0.027200622484087944 + 0.01 * 6.942180633544922
Epoch 480, val loss: 1.221714973449707
Epoch 490, training loss: 0.09453368186950684 = 0.025095487013459206 + 0.01 * 6.943819522857666
Epoch 490, val loss: 1.2397907972335815
Epoch 500, training loss: 0.09251485764980316 = 0.023213421925902367 + 0.01 * 6.930143356323242
Epoch 500, val loss: 1.2573623657226562
Epoch 510, training loss: 0.0906853973865509 = 0.021524706855416298 + 0.01 * 6.916069030761719
Epoch 510, val loss: 1.2743741273880005
Epoch 520, training loss: 0.08926988393068314 = 0.020005779340863228 + 0.01 * 6.926410675048828
Epoch 520, val loss: 1.2908668518066406
Epoch 530, training loss: 0.08772257715463638 = 0.01863711141049862 + 0.01 * 6.908546447753906
Epoch 530, val loss: 1.3068219423294067
Epoch 540, training loss: 0.08640360087156296 = 0.01739678345620632 + 0.01 * 6.900681972503662
Epoch 540, val loss: 1.3223061561584473
Epoch 550, training loss: 0.08519594371318817 = 0.016270842403173447 + 0.01 * 6.892510414123535
Epoch 550, val loss: 1.3373277187347412
Epoch 560, training loss: 0.08427104353904724 = 0.015251840464770794 + 0.01 * 6.901920795440674
Epoch 560, val loss: 1.3518335819244385
Epoch 570, training loss: 0.08319615572690964 = 0.014323133043944836 + 0.01 * 6.887302875518799
Epoch 570, val loss: 1.3659732341766357
Epoch 580, training loss: 0.08223751187324524 = 0.01347334310412407 + 0.01 * 6.87641716003418
Epoch 580, val loss: 1.3797478675842285
Epoch 590, training loss: 0.08139438927173615 = 0.012694663368165493 + 0.01 * 6.8699727058410645
Epoch 590, val loss: 1.3931626081466675
Epoch 600, training loss: 0.08071586489677429 = 0.011980913579463959 + 0.01 * 6.873495578765869
Epoch 600, val loss: 1.4061905145645142
Epoch 610, training loss: 0.08024519681930542 = 0.011324580758810043 + 0.01 * 6.892062187194824
Epoch 610, val loss: 1.4189106225967407
Epoch 620, training loss: 0.07939919829368591 = 0.010722635313868523 + 0.01 * 6.867656230926514
Epoch 620, val loss: 1.4312160015106201
Epoch 630, training loss: 0.07879449427127838 = 0.010167904198169708 + 0.01 * 6.862659454345703
Epoch 630, val loss: 1.4432088136672974
Epoch 640, training loss: 0.07818114012479782 = 0.009656545706093311 + 0.01 * 6.852458953857422
Epoch 640, val loss: 1.4548224210739136
Epoch 650, training loss: 0.07751185446977615 = 0.009183897636830807 + 0.01 * 6.8327956199646
Epoch 650, val loss: 1.4661327600479126
Epoch 660, training loss: 0.0773792713880539 = 0.008746023289859295 + 0.01 * 6.863325595855713
Epoch 660, val loss: 1.4771186113357544
Epoch 670, training loss: 0.07664994150400162 = 0.008341378532350063 + 0.01 * 6.8308563232421875
Epoch 670, val loss: 1.4877275228500366
Epoch 680, training loss: 0.07615423947572708 = 0.007965469732880592 + 0.01 * 6.818877220153809
Epoch 680, val loss: 1.4981000423431396
Epoch 690, training loss: 0.07583852112293243 = 0.007615651004016399 + 0.01 * 6.822287082672119
Epoch 690, val loss: 1.5081913471221924
Epoch 700, training loss: 0.07540533691644669 = 0.007290086708962917 + 0.01 * 6.811525344848633
Epoch 700, val loss: 1.517984390258789
Epoch 710, training loss: 0.07522363215684891 = 0.006985682062804699 + 0.01 * 6.823795795440674
Epoch 710, val loss: 1.5275626182556152
Epoch 720, training loss: 0.07479377090930939 = 0.006702091544866562 + 0.01 * 6.809168338775635
Epoch 720, val loss: 1.5368024110794067
Epoch 730, training loss: 0.07451345026493073 = 0.006437538657337427 + 0.01 * 6.807590961456299
Epoch 730, val loss: 1.545753836631775
Epoch 740, training loss: 0.0742969810962677 = 0.00618939520791173 + 0.01 * 6.810758590698242
Epoch 740, val loss: 1.5544793605804443
Epoch 750, training loss: 0.0740378275513649 = 0.005957452114671469 + 0.01 * 6.808037757873535
Epoch 750, val loss: 1.5629609823226929
Epoch 760, training loss: 0.07369419932365417 = 0.005739645566791296 + 0.01 * 6.795455455780029
Epoch 760, val loss: 1.5711528062820435
Epoch 770, training loss: 0.07345317304134369 = 0.00553490687161684 + 0.01 * 6.7918267250061035
Epoch 770, val loss: 1.5791746377944946
Epoch 780, training loss: 0.07308074086904526 = 0.0053419810719788074 + 0.01 * 6.773875713348389
Epoch 780, val loss: 1.5870130062103271
Epoch 790, training loss: 0.07297556102275848 = 0.005160202272236347 + 0.01 * 6.781535625457764
Epoch 790, val loss: 1.594569444656372
Epoch 800, training loss: 0.07282601296901703 = 0.004988814238458872 + 0.01 * 6.783720016479492
Epoch 800, val loss: 1.601967215538025
Epoch 810, training loss: 0.07253310084342957 = 0.004826991353183985 + 0.01 * 6.770610809326172
Epoch 810, val loss: 1.6091747283935547
Epoch 820, training loss: 0.07230072468519211 = 0.00467406352981925 + 0.01 * 6.76266622543335
Epoch 820, val loss: 1.6161903142929077
Epoch 830, training loss: 0.07225927710533142 = 0.004528969060629606 + 0.01 * 6.773030757904053
Epoch 830, val loss: 1.623016119003296
Epoch 840, training loss: 0.07209184765815735 = 0.0043918900191783905 + 0.01 * 6.76999568939209
Epoch 840, val loss: 1.629654884338379
Epoch 850, training loss: 0.07180176675319672 = 0.00426193792372942 + 0.01 * 6.7539825439453125
Epoch 850, val loss: 1.6361711025238037
Epoch 860, training loss: 0.07164743542671204 = 0.00413845619186759 + 0.01 * 6.7508978843688965
Epoch 860, val loss: 1.6424884796142578
Epoch 870, training loss: 0.07150747627019882 = 0.004021471831947565 + 0.01 * 6.748600482940674
Epoch 870, val loss: 1.6486259698867798
Epoch 880, training loss: 0.07133816927671432 = 0.003910051193088293 + 0.01 * 6.742812156677246
Epoch 880, val loss: 1.654699683189392
Epoch 890, training loss: 0.07141999900341034 = 0.0038040662184357643 + 0.01 * 6.761593341827393
Epoch 890, val loss: 1.6605336666107178
Epoch 900, training loss: 0.07127486914396286 = 0.0037032803520560265 + 0.01 * 6.7571587562561035
Epoch 900, val loss: 1.6662869453430176
Epoch 910, training loss: 0.07102398574352264 = 0.003607002319768071 + 0.01 * 6.741698265075684
Epoch 910, val loss: 1.671897292137146
Epoch 920, training loss: 0.07104870676994324 = 0.003515172516927123 + 0.01 * 6.753353595733643
Epoch 920, val loss: 1.6773499250411987
Epoch 930, training loss: 0.07086657732725143 = 0.003427571151405573 + 0.01 * 6.743900299072266
Epoch 930, val loss: 1.6826984882354736
Epoch 940, training loss: 0.07063499838113785 = 0.0033439204562455416 + 0.01 * 6.7291083335876465
Epoch 940, val loss: 1.6878684759140015
Epoch 950, training loss: 0.07071688026189804 = 0.003264141734689474 + 0.01 * 6.745273590087891
Epoch 950, val loss: 1.69296133518219
Epoch 960, training loss: 0.07051118463277817 = 0.003187761176377535 + 0.01 * 6.73234224319458
Epoch 960, val loss: 1.6979424953460693
Epoch 970, training loss: 0.07030017673969269 = 0.0031144535169005394 + 0.01 * 6.718572616577148
Epoch 970, val loss: 1.702793002128601
Epoch 980, training loss: 0.07037871330976486 = 0.003044332144781947 + 0.01 * 6.733438491821289
Epoch 980, val loss: 1.7075486183166504
Epoch 990, training loss: 0.07029782980680466 = 0.0029771560803055763 + 0.01 * 6.732067584991455
Epoch 990, val loss: 1.7121737003326416
Epoch 1000, training loss: 0.07018241286277771 = 0.002912801457569003 + 0.01 * 6.7269606590271
Epoch 1000, val loss: 1.7167139053344727
Epoch 1010, training loss: 0.07000567018985748 = 0.002851019613444805 + 0.01 * 6.715465545654297
Epoch 1010, val loss: 1.721134901046753
Epoch 1020, training loss: 0.07011444866657257 = 0.0027917518746107817 + 0.01 * 6.732269763946533
Epoch 1020, val loss: 1.725496768951416
Epoch 1030, training loss: 0.06988292187452316 = 0.002734786132350564 + 0.01 * 6.714813232421875
Epoch 1030, val loss: 1.7297385931015015
Epoch 1040, training loss: 0.06982344388961792 = 0.0026799715124070644 + 0.01 * 6.7143473625183105
Epoch 1040, val loss: 1.7338721752166748
Epoch 1050, training loss: 0.06961507350206375 = 0.002627317328006029 + 0.01 * 6.698775768280029
Epoch 1050, val loss: 1.7379229068756104
Epoch 1060, training loss: 0.06971853971481323 = 0.0025766706094145775 + 0.01 * 6.714187145233154
Epoch 1060, val loss: 1.7418808937072754
Epoch 1070, training loss: 0.06961360573768616 = 0.002527989447116852 + 0.01 * 6.708561420440674
Epoch 1070, val loss: 1.7458032369613647
Epoch 1080, training loss: 0.06939263641834259 = 0.0024810133036226034 + 0.01 * 6.691162109375
Epoch 1080, val loss: 1.7495888471603394
Epoch 1090, training loss: 0.06939367204904556 = 0.002435708185657859 + 0.01 * 6.695796966552734
Epoch 1090, val loss: 1.7533046007156372
Epoch 1100, training loss: 0.06990709155797958 = 0.0023921921383589506 + 0.01 * 6.751490116119385
Epoch 1100, val loss: 1.7568926811218262
Epoch 1110, training loss: 0.06931331008672714 = 0.002350234892219305 + 0.01 * 6.69630765914917
Epoch 1110, val loss: 1.760463833808899
Epoch 1120, training loss: 0.06925210356712341 = 0.002309804782271385 + 0.01 * 6.694229602813721
Epoch 1120, val loss: 1.7638859748840332
Epoch 1130, training loss: 0.06923488527536392 = 0.0022707567550241947 + 0.01 * 6.696413040161133
Epoch 1130, val loss: 1.7673002481460571
Epoch 1140, training loss: 0.06903128325939178 = 0.00223292363807559 + 0.01 * 6.679835796356201
Epoch 1140, val loss: 1.770613670349121
Epoch 1150, training loss: 0.06909898668527603 = 0.002196406712755561 + 0.01 * 6.690258026123047
Epoch 1150, val loss: 1.7738404273986816
Epoch 1160, training loss: 0.06894326955080032 = 0.0021612721029669046 + 0.01 * 6.6782002449035645
Epoch 1160, val loss: 1.7770110368728638
Epoch 1170, training loss: 0.06901506334543228 = 0.0021271565929055214 + 0.01 * 6.688790798187256
Epoch 1170, val loss: 1.7800978422164917
Epoch 1180, training loss: 0.06884320825338364 = 0.002094254596158862 + 0.01 * 6.674895286560059
Epoch 1180, val loss: 1.7830784320831299
Epoch 1190, training loss: 0.06931447237730026 = 0.002062475308775902 + 0.01 * 6.7251996994018555
Epoch 1190, val loss: 1.7859917879104614
Epoch 1200, training loss: 0.06872838735580444 = 0.0020316678564995527 + 0.01 * 6.669672012329102
Epoch 1200, val loss: 1.7889152765274048
Epoch 1210, training loss: 0.06871823966503143 = 0.0020018459763377905 + 0.01 * 6.671639919281006
Epoch 1210, val loss: 1.7916580438613892
Epoch 1220, training loss: 0.06886027008295059 = 0.0019730497151613235 + 0.01 * 6.688722133636475
Epoch 1220, val loss: 1.7944424152374268
Epoch 1230, training loss: 0.06859824806451797 = 0.001945010619238019 + 0.01 * 6.665323734283447
Epoch 1230, val loss: 1.797119379043579
Epoch 1240, training loss: 0.0686301663517952 = 0.0019179466180503368 + 0.01 * 6.67122220993042
Epoch 1240, val loss: 1.79983651638031
Epoch 1250, training loss: 0.06843395531177521 = 0.0018915781984105706 + 0.01 * 6.654237747192383
Epoch 1250, val loss: 1.8023525476455688
Epoch 1260, training loss: 0.06848364323377609 = 0.0018661320209503174 + 0.01 * 6.6617512702941895
Epoch 1260, val loss: 1.8049216270446777
Epoch 1270, training loss: 0.06860382854938507 = 0.0018414725782349706 + 0.01 * 6.676236152648926
Epoch 1270, val loss: 1.8074533939361572
Epoch 1280, training loss: 0.0685608759522438 = 0.001817457377910614 + 0.01 * 6.674341678619385
Epoch 1280, val loss: 1.8098376989364624
Epoch 1290, training loss: 0.06843231618404388 = 0.0017942623235285282 + 0.01 * 6.6638054847717285
Epoch 1290, val loss: 1.812239646911621
Epoch 1300, training loss: 0.06836050748825073 = 0.0017716501606628299 + 0.01 * 6.658885478973389
Epoch 1300, val loss: 1.8146004676818848
Epoch 1310, training loss: 0.06836114078760147 = 0.0017498243832960725 + 0.01 * 6.661131858825684
Epoch 1310, val loss: 1.816811203956604
Epoch 1320, training loss: 0.06820231676101685 = 0.0017285477370023727 + 0.01 * 6.647377014160156
Epoch 1320, val loss: 1.819040298461914
Epoch 1330, training loss: 0.06816431879997253 = 0.0017078566597774625 + 0.01 * 6.645646095275879
Epoch 1330, val loss: 1.8212639093399048
Epoch 1340, training loss: 0.06848442554473877 = 0.0016878582537174225 + 0.01 * 6.679656982421875
Epoch 1340, val loss: 1.823349118232727
Epoch 1350, training loss: 0.06798616796731949 = 0.0016682895366102457 + 0.01 * 6.63178825378418
Epoch 1350, val loss: 1.8254472017288208
Epoch 1360, training loss: 0.06794828176498413 = 0.0016492806607857347 + 0.01 * 6.629899978637695
Epoch 1360, val loss: 1.827474594116211
Epoch 1370, training loss: 0.06821762770414352 = 0.0016309351194649935 + 0.01 * 6.658669471740723
Epoch 1370, val loss: 1.8294875621795654
Epoch 1380, training loss: 0.06804423779249191 = 0.0016129091382026672 + 0.01 * 6.643133163452148
Epoch 1380, val loss: 1.8314028978347778
Epoch 1390, training loss: 0.06790797412395477 = 0.001595539040863514 + 0.01 * 6.631243705749512
Epoch 1390, val loss: 1.833369255065918
Epoch 1400, training loss: 0.06811419874429703 = 0.0015785336727276444 + 0.01 * 6.653566837310791
Epoch 1400, val loss: 1.8351620435714722
Epoch 1410, training loss: 0.06781939417123795 = 0.001562021323479712 + 0.01 * 6.625737190246582
Epoch 1410, val loss: 1.8370534181594849
Epoch 1420, training loss: 0.06781217455863953 = 0.00154590024612844 + 0.01 * 6.6266279220581055
Epoch 1420, val loss: 1.8388371467590332
Epoch 1430, training loss: 0.06789706647396088 = 0.0015302803367376328 + 0.01 * 6.636678218841553
Epoch 1430, val loss: 1.840668797492981
Epoch 1440, training loss: 0.06782013177871704 = 0.0015149270184338093 + 0.01 * 6.630520343780518
Epoch 1440, val loss: 1.8424017429351807
Epoch 1450, training loss: 0.06766434013843536 = 0.0015000331914052367 + 0.01 * 6.616430759429932
Epoch 1450, val loss: 1.8441128730773926
Epoch 1460, training loss: 0.0678502693772316 = 0.001485514105297625 + 0.01 * 6.636476039886475
Epoch 1460, val loss: 1.8458399772644043
Epoch 1470, training loss: 0.06767761707305908 = 0.0014713453128933907 + 0.01 * 6.620626926422119
Epoch 1470, val loss: 1.8474668264389038
Epoch 1480, training loss: 0.06746715307235718 = 0.001457547303289175 + 0.01 * 6.600960731506348
Epoch 1480, val loss: 1.8491337299346924
Epoch 1490, training loss: 0.06755444407463074 = 0.0014439893420785666 + 0.01 * 6.611045837402344
Epoch 1490, val loss: 1.850765585899353
Epoch 1500, training loss: 0.06801712512969971 = 0.0014308822574093938 + 0.01 * 6.658624172210693
Epoch 1500, val loss: 1.8523368835449219
Epoch 1510, training loss: 0.06746279448270798 = 0.0014180648140609264 + 0.01 * 6.604473114013672
Epoch 1510, val loss: 1.853823184967041
Epoch 1520, training loss: 0.06735023856163025 = 0.0014054944040253758 + 0.01 * 6.594474792480469
Epoch 1520, val loss: 1.8554240465164185
Epoch 1530, training loss: 0.06743133068084717 = 0.0013932859292253852 + 0.01 * 6.603804111480713
Epoch 1530, val loss: 1.8569586277008057
Epoch 1540, training loss: 0.06746739894151688 = 0.001381311914883554 + 0.01 * 6.608608245849609
Epoch 1540, val loss: 1.8583701848983765
Epoch 1550, training loss: 0.0674670860171318 = 0.001369619625620544 + 0.01 * 6.609746932983398
Epoch 1550, val loss: 1.859943151473999
Epoch 1560, training loss: 0.06720663607120514 = 0.0013582126703113317 + 0.01 * 6.584842681884766
Epoch 1560, val loss: 1.8613022565841675
Epoch 1570, training loss: 0.06718200445175171 = 0.0013470720732584596 + 0.01 * 6.583493709564209
Epoch 1570, val loss: 1.862757682800293
Epoch 1580, training loss: 0.06766650825738907 = 0.001336167100816965 + 0.01 * 6.633034706115723
Epoch 1580, val loss: 1.8641341924667358
Epoch 1590, training loss: 0.06727399677038193 = 0.0013254927471280098 + 0.01 * 6.594850540161133
Epoch 1590, val loss: 1.865519642829895
Epoch 1600, training loss: 0.06705142557621002 = 0.0013150157174095511 + 0.01 * 6.573641777038574
Epoch 1600, val loss: 1.8668614625930786
Epoch 1610, training loss: 0.06721667945384979 = 0.0013048427645117044 + 0.01 * 6.591183662414551
Epoch 1610, val loss: 1.8682143688201904
Epoch 1620, training loss: 0.06706322729587555 = 0.0012948042713105679 + 0.01 * 6.576842308044434
Epoch 1620, val loss: 1.8694630861282349
Epoch 1630, training loss: 0.06732834875583649 = 0.0012850480852648616 + 0.01 * 6.604330539703369
Epoch 1630, val loss: 1.8708574771881104
Epoch 1640, training loss: 0.06694035977125168 = 0.0012754325289279222 + 0.01 * 6.566492557525635
Epoch 1640, val loss: 1.8721410036087036
Epoch 1650, training loss: 0.0669226124882698 = 0.0012660407228395343 + 0.01 * 6.565657615661621
Epoch 1650, val loss: 1.8733668327331543
Epoch 1660, training loss: 0.06711976230144501 = 0.0012568692909553647 + 0.01 * 6.586289882659912
Epoch 1660, val loss: 1.8746190071105957
Epoch 1670, training loss: 0.06696420162916183 = 0.0012477829586714506 + 0.01 * 6.571641445159912
Epoch 1670, val loss: 1.8758702278137207
Epoch 1680, training loss: 0.06703861057758331 = 0.0012389719486236572 + 0.01 * 6.5799641609191895
Epoch 1680, val loss: 1.877151370048523
Epoch 1690, training loss: 0.06690498441457748 = 0.001230323687195778 + 0.01 * 6.5674662590026855
Epoch 1690, val loss: 1.8783040046691895
Epoch 1700, training loss: 0.0668816864490509 = 0.001221735030412674 + 0.01 * 6.565995216369629
Epoch 1700, val loss: 1.8794816732406616
Epoch 1710, training loss: 0.06697419285774231 = 0.0012134313583374023 + 0.01 * 6.576076030731201
Epoch 1710, val loss: 1.880675196647644
Epoch 1720, training loss: 0.06682327389717102 = 0.001205266802571714 + 0.01 * 6.561800956726074
Epoch 1720, val loss: 1.8818154335021973
Epoch 1730, training loss: 0.06671790033578873 = 0.0011972136562690139 + 0.01 * 6.552069187164307
Epoch 1730, val loss: 1.8828985691070557
Epoch 1740, training loss: 0.06687774509191513 = 0.0011894076596945524 + 0.01 * 6.568833827972412
Epoch 1740, val loss: 1.884047031402588
Epoch 1750, training loss: 0.06668631732463837 = 0.0011816367041319609 + 0.01 * 6.5504679679870605
Epoch 1750, val loss: 1.8851263523101807
Epoch 1760, training loss: 0.06674089282751083 = 0.00117406714707613 + 0.01 * 6.556682109832764
Epoch 1760, val loss: 1.8862022161483765
Epoch 1770, training loss: 0.06670861691236496 = 0.0011666591744869947 + 0.01 * 6.554196357727051
Epoch 1770, val loss: 1.88723623752594
Epoch 1780, training loss: 0.06677387654781342 = 0.0011593502713367343 + 0.01 * 6.561453342437744
Epoch 1780, val loss: 1.8882975578308105
Epoch 1790, training loss: 0.06686801463365555 = 0.0011521729175001383 + 0.01 * 6.571584701538086
Epoch 1790, val loss: 1.889358401298523
Epoch 1800, training loss: 0.06666502356529236 = 0.0011451472528278828 + 0.01 * 6.551987648010254
Epoch 1800, val loss: 1.8903173208236694
Epoch 1810, training loss: 0.06668961048126221 = 0.0011382353259250522 + 0.01 * 6.555138111114502
Epoch 1810, val loss: 1.8913588523864746
Epoch 1820, training loss: 0.06664954125881195 = 0.0011314292205497622 + 0.01 * 6.551811218261719
Epoch 1820, val loss: 1.8924063444137573
Epoch 1830, training loss: 0.0666179284453392 = 0.0011246873764321208 + 0.01 * 6.549324035644531
Epoch 1830, val loss: 1.893357515335083
Epoch 1840, training loss: 0.06651965528726578 = 0.0011181371519342065 + 0.01 * 6.540151596069336
Epoch 1840, val loss: 1.8943482637405396
Epoch 1850, training loss: 0.06639093905687332 = 0.0011116700479760766 + 0.01 * 6.527926921844482
Epoch 1850, val loss: 1.8953323364257812
Epoch 1860, training loss: 0.06658852845430374 = 0.0011052746558561921 + 0.01 * 6.548326015472412
Epoch 1860, val loss: 1.8962457180023193
Epoch 1870, training loss: 0.06651497632265091 = 0.0010990682058036327 + 0.01 * 6.541591644287109
Epoch 1870, val loss: 1.8971978425979614
Epoch 1880, training loss: 0.06642112135887146 = 0.001092883525416255 + 0.01 * 6.532824516296387
Epoch 1880, val loss: 1.898192048072815
Epoch 1890, training loss: 0.06634833663702011 = 0.0010868218960240483 + 0.01 * 6.526151180267334
Epoch 1890, val loss: 1.8990586996078491
Epoch 1900, training loss: 0.06664049625396729 = 0.001080853515304625 + 0.01 * 6.55596399307251
Epoch 1900, val loss: 1.9000930786132812
Epoch 1910, training loss: 0.06630071252584457 = 0.001074989908374846 + 0.01 * 6.5225725173950195
Epoch 1910, val loss: 1.9009250402450562
Epoch 1920, training loss: 0.06630957126617432 = 0.001069213729351759 + 0.01 * 6.524035930633545
Epoch 1920, val loss: 1.9018774032592773
Epoch 1930, training loss: 0.06642698496580124 = 0.0010635273065418005 + 0.01 * 6.536345958709717
Epoch 1930, val loss: 1.9027938842773438
Epoch 1940, training loss: 0.0663742646574974 = 0.0010578965302556753 + 0.01 * 6.531637191772461
Epoch 1940, val loss: 1.9036365747451782
Epoch 1950, training loss: 0.06640185415744781 = 0.0010523980017751455 + 0.01 * 6.534946441650391
Epoch 1950, val loss: 1.9045794010162354
Epoch 1960, training loss: 0.06611914932727814 = 0.0010469203116372228 + 0.01 * 6.507223606109619
Epoch 1960, val loss: 1.9054512977600098
Epoch 1970, training loss: 0.0664052963256836 = 0.0010415763827040792 + 0.01 * 6.536372184753418
Epoch 1970, val loss: 1.9063806533813477
Epoch 1980, training loss: 0.06634140014648438 = 0.0010362955508753657 + 0.01 * 6.530510425567627
Epoch 1980, val loss: 1.9071564674377441
Epoch 1990, training loss: 0.06619904190301895 = 0.0010310437064617872 + 0.01 * 6.516800403594971
Epoch 1990, val loss: 1.9080727100372314
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8223510806536637
The final CL Acc:0.75062, 0.01823, The final GNN Acc:0.82130, 0.00522
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13190])
remove edge: torch.Size([2, 7886])
updated graph: torch.Size([2, 10520])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0108466148376465 = 1.9248782396316528 + 0.01 * 8.59684944152832
Epoch 0, val loss: 1.917463779449463
Epoch 10, training loss: 2.001418352127075 = 1.9154505729675293 + 0.01 * 8.596786499023438
Epoch 10, val loss: 1.908909559249878
Epoch 20, training loss: 1.9898792505264282 = 1.9039137363433838 + 0.01 * 8.596553802490234
Epoch 20, val loss: 1.8982402086257935
Epoch 30, training loss: 1.974135160446167 = 1.8881773948669434 + 0.01 * 8.595771789550781
Epoch 30, val loss: 1.8833720684051514
Epoch 40, training loss: 1.9516288042068481 = 1.8657140731811523 + 0.01 * 8.591473579406738
Epoch 40, val loss: 1.8621478080749512
Epoch 50, training loss: 1.9206117391586304 = 1.834994912147522 + 0.01 * 8.561684608459473
Epoch 50, val loss: 1.8343783617019653
Epoch 60, training loss: 1.8831219673156738 = 1.7989983558654785 + 0.01 * 8.412365913391113
Epoch 60, val loss: 1.8044815063476562
Epoch 70, training loss: 1.8412747383117676 = 1.7616037130355835 + 0.01 * 7.96709680557251
Epoch 70, val loss: 1.7735998630523682
Epoch 80, training loss: 1.7906343936920166 = 1.7123335599899292 + 0.01 * 7.830081462860107
Epoch 80, val loss: 1.7278228998184204
Epoch 90, training loss: 1.7196898460388184 = 1.6429409980773926 + 0.01 * 7.674882411956787
Epoch 90, val loss: 1.6635971069335938
Epoch 100, training loss: 1.6277602910995483 = 1.5522607564926147 + 0.01 * 7.5499587059021
Epoch 100, val loss: 1.585050344467163
Epoch 110, training loss: 1.522245168685913 = 1.4475116729736328 + 0.01 * 7.473345756530762
Epoch 110, val loss: 1.495521068572998
Epoch 120, training loss: 1.4115562438964844 = 1.3371477127075195 + 0.01 * 7.440859317779541
Epoch 120, val loss: 1.4036575555801392
Epoch 130, training loss: 1.3003250360488892 = 1.2263144254684448 + 0.01 * 7.401061058044434
Epoch 130, val loss: 1.3128604888916016
Epoch 140, training loss: 1.192499041557312 = 1.1190110445022583 + 0.01 * 7.3488030433654785
Epoch 140, val loss: 1.2272651195526123
Epoch 150, training loss: 1.091561198234558 = 1.0188066959381104 + 0.01 * 7.275455474853516
Epoch 150, val loss: 1.1496775150299072
Epoch 160, training loss: 0.9991685748100281 = 0.9272809624671936 + 0.01 * 7.188761234283447
Epoch 160, val loss: 1.0804466009140015
Epoch 170, training loss: 0.915296196937561 = 0.8439944982528687 + 0.01 * 7.130167484283447
Epoch 170, val loss: 1.0180095434188843
Epoch 180, training loss: 0.8397392630577087 = 0.7687689065933228 + 0.01 * 7.097036361694336
Epoch 180, val loss: 0.9623944163322449
Epoch 190, training loss: 0.7725204229354858 = 0.7017620205879211 + 0.01 * 7.075838565826416
Epoch 190, val loss: 0.914337694644928
Epoch 200, training loss: 0.7128061056137085 = 0.6421584486961365 + 0.01 * 7.064762592315674
Epoch 200, val loss: 0.8739448189735413
Epoch 210, training loss: 0.6586788892745972 = 0.5880807042121887 + 0.01 * 7.05981969833374
Epoch 210, val loss: 0.8403444290161133
Epoch 220, training loss: 0.608373761177063 = 0.5377911329269409 + 0.01 * 7.058262348175049
Epoch 220, val loss: 0.8122646808624268
Epoch 230, training loss: 0.5607912540435791 = 0.4902377128601074 + 0.01 * 7.055352210998535
Epoch 230, val loss: 0.7888422012329102
Epoch 240, training loss: 0.5154665112495422 = 0.44493624567985535 + 0.01 * 7.05302619934082
Epoch 240, val loss: 0.7698602676391602
Epoch 250, training loss: 0.47224920988082886 = 0.4017461836338043 + 0.01 * 7.0503034591674805
Epoch 250, val loss: 0.7553179860115051
Epoch 260, training loss: 0.4313560426235199 = 0.36084404587745667 + 0.01 * 7.051200866699219
Epoch 260, val loss: 0.7448766827583313
Epoch 270, training loss: 0.3929111361503601 = 0.32246825098991394 + 0.01 * 7.0442891120910645
Epoch 270, val loss: 0.7380405068397522
Epoch 280, training loss: 0.35718029737472534 = 0.28676342964172363 + 0.01 * 7.041687488555908
Epoch 280, val loss: 0.7344505190849304
Epoch 290, training loss: 0.324199914932251 = 0.2538207471370697 + 0.01 * 7.03791618347168
Epoch 290, val loss: 0.733788788318634
Epoch 300, training loss: 0.29408347606658936 = 0.22373948991298676 + 0.01 * 7.034399032592773
Epoch 300, val loss: 0.7356862425804138
Epoch 310, training loss: 0.26692789793014526 = 0.19660863280296326 + 0.01 * 7.031925678253174
Epoch 310, val loss: 0.739948034286499
Epoch 320, training loss: 0.24272660911083221 = 0.17245346307754517 + 0.01 * 7.02731466293335
Epoch 320, val loss: 0.7463822364807129
Epoch 330, training loss: 0.2214515060186386 = 0.15119333565235138 + 0.01 * 7.025816917419434
Epoch 330, val loss: 0.7546401619911194
Epoch 340, training loss: 0.20284616947174072 = 0.13264641165733337 + 0.01 * 7.0199761390686035
Epoch 340, val loss: 0.7645257115364075
Epoch 350, training loss: 0.18671299517154694 = 0.11657016724348068 + 0.01 * 7.014282703399658
Epoch 350, val loss: 0.7758330702781677
Epoch 360, training loss: 0.17279037833213806 = 0.10269104689359665 + 0.01 * 7.009932994842529
Epoch 360, val loss: 0.7883970737457275
Epoch 370, training loss: 0.1607738733291626 = 0.09073233604431152 + 0.01 * 7.004154205322266
Epoch 370, val loss: 0.8020094037055969
Epoch 380, training loss: 0.15040387213230133 = 0.0804285854101181 + 0.01 * 6.997529029846191
Epoch 380, val loss: 0.8164359331130981
Epoch 390, training loss: 0.1414988934993744 = 0.07153932005167007 + 0.01 * 6.995957374572754
Epoch 390, val loss: 0.8314374089241028
Epoch 400, training loss: 0.13370206952095032 = 0.06385616213083267 + 0.01 * 6.984591960906982
Epoch 400, val loss: 0.8468408584594727
Epoch 410, training loss: 0.12694142758846283 = 0.05719536170363426 + 0.01 * 6.974606990814209
Epoch 410, val loss: 0.8624916076660156
Epoch 420, training loss: 0.12118680775165558 = 0.05140981823205948 + 0.01 * 6.977698802947998
Epoch 420, val loss: 0.8781858086585999
Epoch 430, training loss: 0.11597307026386261 = 0.04637611657381058 + 0.01 * 6.959695339202881
Epoch 430, val loss: 0.8938132524490356
Epoch 440, training loss: 0.11150781810283661 = 0.04198027774691582 + 0.01 * 6.95275354385376
Epoch 440, val loss: 0.909317672252655
Epoch 450, training loss: 0.10765420645475388 = 0.03812717646360397 + 0.01 * 6.952703475952148
Epoch 450, val loss: 0.92461758852005
Epoch 460, training loss: 0.10418493300676346 = 0.03473877161741257 + 0.01 * 6.944616317749023
Epoch 460, val loss: 0.9396439790725708
Epoch 470, training loss: 0.10110482573509216 = 0.031748250126838684 + 0.01 * 6.935657978057861
Epoch 470, val loss: 0.9544025659561157
Epoch 480, training loss: 0.09837685525417328 = 0.0290981475263834 + 0.01 * 6.927871227264404
Epoch 480, val loss: 0.9688312411308289
Epoch 490, training loss: 0.09596376866102219 = 0.02674185484647751 + 0.01 * 6.922191619873047
Epoch 490, val loss: 0.9829447269439697
Epoch 500, training loss: 0.09385620057582855 = 0.02464054338634014 + 0.01 * 6.921565532684326
Epoch 500, val loss: 0.9967396855354309
Epoch 510, training loss: 0.09197379648685455 = 0.022760888561606407 + 0.01 * 6.921290874481201
Epoch 510, val loss: 1.0101876258850098
Epoch 520, training loss: 0.09020111709833145 = 0.02107597328722477 + 0.01 * 6.912514686584473
Epoch 520, val loss: 1.023303508758545
Epoch 530, training loss: 0.08864440768957138 = 0.019561277702450752 + 0.01 * 6.908313274383545
Epoch 530, val loss: 1.0360668897628784
Epoch 540, training loss: 0.08728395402431488 = 0.018196560442447662 + 0.01 * 6.9087395668029785
Epoch 540, val loss: 1.0484825372695923
Epoch 550, training loss: 0.0859956368803978 = 0.01696574129164219 + 0.01 * 6.902989864349365
Epoch 550, val loss: 1.0605653524398804
Epoch 560, training loss: 0.08484379947185516 = 0.01585279032588005 + 0.01 * 6.8991007804870605
Epoch 560, val loss: 1.072289228439331
Epoch 570, training loss: 0.08387164026498795 = 0.014843042008578777 + 0.01 * 6.902860164642334
Epoch 570, val loss: 1.0837059020996094
Epoch 580, training loss: 0.08291357010602951 = 0.013926154002547264 + 0.01 * 6.898741722106934
Epoch 580, val loss: 1.0948148965835571
Epoch 590, training loss: 0.08200842887163162 = 0.013091126456856728 + 0.01 * 6.891730308532715
Epoch 590, val loss: 1.1055892705917358
Epoch 600, training loss: 0.08120143413543701 = 0.012329031713306904 + 0.01 * 6.887239933013916
Epoch 600, val loss: 1.1160986423492432
Epoch 610, training loss: 0.08051862567663193 = 0.011631779372692108 + 0.01 * 6.8886847496032715
Epoch 610, val loss: 1.1263139247894287
Epoch 620, training loss: 0.07981842011213303 = 0.010992893949151039 + 0.01 * 6.882552623748779
Epoch 620, val loss: 1.1362321376800537
Epoch 630, training loss: 0.07923620939254761 = 0.010406526736915112 + 0.01 * 6.882968902587891
Epoch 630, val loss: 1.1458972692489624
Epoch 640, training loss: 0.07862166315317154 = 0.00986775103956461 + 0.01 * 6.875391483306885
Epoch 640, val loss: 1.1552958488464355
Epoch 650, training loss: 0.07813189178705215 = 0.009370693005621433 + 0.01 * 6.876120090484619
Epoch 650, val loss: 1.1644606590270996
Epoch 660, training loss: 0.07759507745504379 = 0.008910959586501122 + 0.01 * 6.868412017822266
Epoch 660, val loss: 1.1733511686325073
Epoch 670, training loss: 0.07718358188867569 = 0.008483991958200932 + 0.01 * 6.869958877563477
Epoch 670, val loss: 1.1820318698883057
Epoch 680, training loss: 0.0767570436000824 = 0.008087233640253544 + 0.01 * 6.866981029510498
Epoch 680, val loss: 1.1905038356781006
Epoch 690, training loss: 0.07636559009552002 = 0.007716790772974491 + 0.01 * 6.864880084991455
Epoch 690, val loss: 1.1987537145614624
Epoch 700, training loss: 0.07600755989551544 = 0.007373802829533815 + 0.01 * 6.863375663757324
Epoch 700, val loss: 1.2067843675613403
Epoch 710, training loss: 0.07559821754693985 = 0.007055029738694429 + 0.01 * 6.854319095611572
Epoch 710, val loss: 1.2146116495132446
Epoch 720, training loss: 0.07524506002664566 = 0.006757323630154133 + 0.01 * 6.84877347946167
Epoch 720, val loss: 1.2222539186477661
Epoch 730, training loss: 0.07502230256795883 = 0.006478952709585428 + 0.01 * 6.854335308074951
Epoch 730, val loss: 1.2296968698501587
Epoch 740, training loss: 0.07466352730989456 = 0.006218566093593836 + 0.01 * 6.844496250152588
Epoch 740, val loss: 1.236943006515503
Epoch 750, training loss: 0.07449973374605179 = 0.005974790547043085 + 0.01 * 6.852494239807129
Epoch 750, val loss: 1.2439935207366943
Epoch 760, training loss: 0.07414835691452026 = 0.005746783688664436 + 0.01 * 6.840157985687256
Epoch 760, val loss: 1.250906229019165
Epoch 770, training loss: 0.07390649616718292 = 0.005532626993954182 + 0.01 * 6.8373870849609375
Epoch 770, val loss: 1.2576029300689697
Epoch 780, training loss: 0.0737103819847107 = 0.0053320107981562614 + 0.01 * 6.837837219238281
Epoch 780, val loss: 1.2641394138336182
Epoch 790, training loss: 0.07339759171009064 = 0.005143382120877504 + 0.01 * 6.82542085647583
Epoch 790, val loss: 1.2704862356185913
Epoch 800, training loss: 0.07319363206624985 = 0.0049659195356070995 + 0.01 * 6.822772026062012
Epoch 800, val loss: 1.2766886949539185
Epoch 810, training loss: 0.07305851578712463 = 0.004798579961061478 + 0.01 * 6.825993537902832
Epoch 810, val loss: 1.2827732563018799
Epoch 820, training loss: 0.07285791635513306 = 0.004641590639948845 + 0.01 * 6.8216328620910645
Epoch 820, val loss: 1.2886435985565186
Epoch 830, training loss: 0.07261098176240921 = 0.004493182059377432 + 0.01 * 6.811779975891113
Epoch 830, val loss: 1.2944049835205078
Epoch 840, training loss: 0.07260482013225555 = 0.004352899733930826 + 0.01 * 6.825192451477051
Epoch 840, val loss: 1.3000386953353882
Epoch 850, training loss: 0.07227747142314911 = 0.004220604430884123 + 0.01 * 6.805686950683594
Epoch 850, val loss: 1.3054981231689453
Epoch 860, training loss: 0.07206668704748154 = 0.004095294512808323 + 0.01 * 6.797139644622803
Epoch 860, val loss: 1.3108628988265991
Epoch 870, training loss: 0.07200432568788528 = 0.0039763618260622025 + 0.01 * 6.802796363830566
Epoch 870, val loss: 1.3160946369171143
Epoch 880, training loss: 0.07186508178710938 = 0.0038639940321445465 + 0.01 * 6.800108909606934
Epoch 880, val loss: 1.321151852607727
Epoch 890, training loss: 0.07166624814271927 = 0.003757030703127384 + 0.01 * 6.790921688079834
Epoch 890, val loss: 1.326112151145935
Epoch 900, training loss: 0.07148764282464981 = 0.0036554280668497086 + 0.01 * 6.78322172164917
Epoch 900, val loss: 1.3309932947158813
Epoch 910, training loss: 0.07142890244722366 = 0.0035589749459177256 + 0.01 * 6.78699254989624
Epoch 910, val loss: 1.3357280492782593
Epoch 920, training loss: 0.07125931978225708 = 0.0034669123124331236 + 0.01 * 6.77924108505249
Epoch 920, val loss: 1.3403397798538208
Epoch 930, training loss: 0.07127340883016586 = 0.003379496978595853 + 0.01 * 6.789391994476318
Epoch 930, val loss: 1.3448799848556519
Epoch 940, training loss: 0.07104244083166122 = 0.0032962134573608637 + 0.01 * 6.774622917175293
Epoch 940, val loss: 1.349249243736267
Epoch 950, training loss: 0.07091906666755676 = 0.0032167905010282993 + 0.01 * 6.770227909088135
Epoch 950, val loss: 1.3535947799682617
Epoch 960, training loss: 0.07082274556159973 = 0.003141017397865653 + 0.01 * 6.7681732177734375
Epoch 960, val loss: 1.3577998876571655
Epoch 970, training loss: 0.07066477090120316 = 0.0030684475786983967 + 0.01 * 6.759632587432861
Epoch 970, val loss: 1.3619410991668701
Epoch 980, training loss: 0.07064662873744965 = 0.0029989152681082487 + 0.01 * 6.764770984649658
Epoch 980, val loss: 1.3660223484039307
Epoch 990, training loss: 0.07059875130653381 = 0.0029326791409403086 + 0.01 * 6.766607284545898
Epoch 990, val loss: 1.3699538707733154
Epoch 1000, training loss: 0.0704241469502449 = 0.0028692875057458878 + 0.01 * 6.755486488342285
Epoch 1000, val loss: 1.3738576173782349
Epoch 1010, training loss: 0.07032522559165955 = 0.002808450488373637 + 0.01 * 6.751677513122559
Epoch 1010, val loss: 1.3776823282241821
Epoch 1020, training loss: 0.07025721669197083 = 0.002749969018623233 + 0.01 * 6.750724792480469
Epoch 1020, val loss: 1.3814008235931396
Epoch 1030, training loss: 0.07017187774181366 = 0.002694075694307685 + 0.01 * 6.747779846191406
Epoch 1030, val loss: 1.385087251663208
Epoch 1040, training loss: 0.07007716596126556 = 0.0026403330266475677 + 0.01 * 6.743683815002441
Epoch 1040, val loss: 1.3886164426803589
Epoch 1050, training loss: 0.07000453770160675 = 0.0025887133087962866 + 0.01 * 6.741582870483398
Epoch 1050, val loss: 1.3921990394592285
Epoch 1060, training loss: 0.07000333815813065 = 0.002539090346544981 + 0.01 * 6.746425151824951
Epoch 1060, val loss: 1.3955838680267334
Epoch 1070, training loss: 0.06996370106935501 = 0.002491430612280965 + 0.01 * 6.747227191925049
Epoch 1070, val loss: 1.3989956378936768
Epoch 1080, training loss: 0.0697585865855217 = 0.0024454633239656687 + 0.01 * 6.731312274932861
Epoch 1080, val loss: 1.4022902250289917
Epoch 1090, training loss: 0.06971907615661621 = 0.0024012760259211063 + 0.01 * 6.731780052185059
Epoch 1090, val loss: 1.4055513143539429
Epoch 1100, training loss: 0.06964816898107529 = 0.002358704572543502 + 0.01 * 6.728946685791016
Epoch 1100, val loss: 1.4087449312210083
Epoch 1110, training loss: 0.06996457278728485 = 0.0023176984395831823 + 0.01 * 6.764688014984131
Epoch 1110, val loss: 1.411850929260254
Epoch 1120, training loss: 0.06951382756233215 = 0.0022781456355005503 + 0.01 * 6.723567962646484
Epoch 1120, val loss: 1.4148750305175781
Epoch 1130, training loss: 0.06947121024131775 = 0.002240030327811837 + 0.01 * 6.723118305206299
Epoch 1130, val loss: 1.4178954362869263
Epoch 1140, training loss: 0.06945548951625824 = 0.0022031397093087435 + 0.01 * 6.725235462188721
Epoch 1140, val loss: 1.4208271503448486
Epoch 1150, training loss: 0.06942224502563477 = 0.0021674709860235453 + 0.01 * 6.725477695465088
Epoch 1150, val loss: 1.4237323999404907
Epoch 1160, training loss: 0.06958508491516113 = 0.0021331419702619314 + 0.01 * 6.745194911956787
Epoch 1160, val loss: 1.426546335220337
Epoch 1170, training loss: 0.06935868412256241 = 0.0020999563857913017 + 0.01 * 6.725872993469238
Epoch 1170, val loss: 1.4292842149734497
Epoch 1180, training loss: 0.06919711083173752 = 0.002067926339805126 + 0.01 * 6.712919235229492
Epoch 1180, val loss: 1.4320110082626343
Epoch 1190, training loss: 0.06909730285406113 = 0.0020368576515465975 + 0.01 * 6.706044673919678
Epoch 1190, val loss: 1.434695839881897
Epoch 1200, training loss: 0.06919004768133163 = 0.00200672191567719 + 0.01 * 6.718332767486572
Epoch 1200, val loss: 1.437315821647644
Epoch 1210, training loss: 0.06908875703811646 = 0.001977859530597925 + 0.01 * 6.711090087890625
Epoch 1210, val loss: 1.4398771524429321
Epoch 1220, training loss: 0.06899543851613998 = 0.001949670840986073 + 0.01 * 6.704577445983887
Epoch 1220, val loss: 1.4423596858978271
Epoch 1230, training loss: 0.06896720826625824 = 0.0019223635317757726 + 0.01 * 6.704484939575195
Epoch 1230, val loss: 1.444849967956543
Epoch 1240, training loss: 0.06900384277105331 = 0.0018959373701363802 + 0.01 * 6.710791110992432
Epoch 1240, val loss: 1.4473276138305664
Epoch 1250, training loss: 0.06889233738183975 = 0.001870331005193293 + 0.01 * 6.702200412750244
Epoch 1250, val loss: 1.4496819972991943
Epoch 1260, training loss: 0.06884431838989258 = 0.001845518359914422 + 0.01 * 6.699880123138428
Epoch 1260, val loss: 1.4520293474197388
Epoch 1270, training loss: 0.06904309242963791 = 0.0018213996663689613 + 0.01 * 6.722169876098633
Epoch 1270, val loss: 1.454375147819519
Epoch 1280, training loss: 0.06876645237207413 = 0.0017980835400521755 + 0.01 * 6.696836471557617
Epoch 1280, val loss: 1.4566435813903809
Epoch 1290, training loss: 0.06876498460769653 = 0.0017754529835656285 + 0.01 * 6.698953151702881
Epoch 1290, val loss: 1.4588521718978882
Epoch 1300, training loss: 0.06863920390605927 = 0.0017534507205709815 + 0.01 * 6.688575267791748
Epoch 1300, val loss: 1.4610801935195923
Epoch 1310, training loss: 0.06871110945940018 = 0.0017321145860478282 + 0.01 * 6.697900295257568
Epoch 1310, val loss: 1.4632676839828491
Epoch 1320, training loss: 0.06862916052341461 = 0.0017113267676904798 + 0.01 * 6.691783905029297
Epoch 1320, val loss: 1.465376853942871
Epoch 1330, training loss: 0.06861233711242676 = 0.0016911946004256606 + 0.01 * 6.692114353179932
Epoch 1330, val loss: 1.4674561023712158
Epoch 1340, training loss: 0.0685378909111023 = 0.001671601552516222 + 0.01 * 6.686628818511963
Epoch 1340, val loss: 1.469490885734558
Epoch 1350, training loss: 0.06845439225435257 = 0.001652551582083106 + 0.01 * 6.680184364318848
Epoch 1350, val loss: 1.471535563468933
Epoch 1360, training loss: 0.06855899095535278 = 0.0016339184949174523 + 0.01 * 6.692507266998291
Epoch 1360, val loss: 1.4735225439071655
Epoch 1370, training loss: 0.06846670806407928 = 0.0016159422229975462 + 0.01 * 6.685076713562012
Epoch 1370, val loss: 1.4754875898361206
Epoch 1380, training loss: 0.06843985617160797 = 0.0015984007623046637 + 0.01 * 6.684145450592041
Epoch 1380, val loss: 1.4773703813552856
Epoch 1390, training loss: 0.06833892315626144 = 0.0015814191428944468 + 0.01 * 6.675750732421875
Epoch 1390, val loss: 1.4792500734329224
Epoch 1400, training loss: 0.06842021644115448 = 0.0015647899126634002 + 0.01 * 6.685542583465576
Epoch 1400, val loss: 1.4810980558395386
Epoch 1410, training loss: 0.06842762976884842 = 0.0015486639458686113 + 0.01 * 6.687896728515625
Epoch 1410, val loss: 1.4828633069992065
Epoch 1420, training loss: 0.06841152161359787 = 0.0015329529996961355 + 0.01 * 6.687857627868652
Epoch 1420, val loss: 1.4846248626708984
Epoch 1430, training loss: 0.0682184249162674 = 0.0015176950255408883 + 0.01 * 6.67007303237915
Epoch 1430, val loss: 1.486382007598877
Epoch 1440, training loss: 0.06845688074827194 = 0.0015028308844193816 + 0.01 * 6.695404529571533
Epoch 1440, val loss: 1.4880669116973877
Epoch 1450, training loss: 0.06824270635843277 = 0.0014881838578730822 + 0.01 * 6.67545223236084
Epoch 1450, val loss: 1.4897346496582031
Epoch 1460, training loss: 0.06832053512334824 = 0.0014740437036380172 + 0.01 * 6.684649467468262
Epoch 1460, val loss: 1.491412878036499
Epoch 1470, training loss: 0.0681878924369812 = 0.0014602042501792312 + 0.01 * 6.672769069671631
Epoch 1470, val loss: 1.4930001497268677
Epoch 1480, training loss: 0.06809300184249878 = 0.0014467278961092234 + 0.01 * 6.6646270751953125
Epoch 1480, val loss: 1.494602918624878
Epoch 1490, training loss: 0.06829410791397095 = 0.0014335890300571918 + 0.01 * 6.686051845550537
Epoch 1490, val loss: 1.4961276054382324
Epoch 1500, training loss: 0.06805293262004852 = 0.001420786022208631 + 0.01 * 6.663214683532715
Epoch 1500, val loss: 1.4976645708084106
Epoch 1510, training loss: 0.06831324845552444 = 0.0014082634588703513 + 0.01 * 6.6904988288879395
Epoch 1510, val loss: 1.4991891384124756
Epoch 1520, training loss: 0.0679670125246048 = 0.0013959152856841683 + 0.01 * 6.65710973739624
Epoch 1520, val loss: 1.5006526708602905
Epoch 1530, training loss: 0.06784450262784958 = 0.0013840461615473032 + 0.01 * 6.646045684814453
Epoch 1530, val loss: 1.5021024942398071
Epoch 1540, training loss: 0.06811477988958359 = 0.0013723892625421286 + 0.01 * 6.674239158630371
Epoch 1540, val loss: 1.5035239458084106
Epoch 1550, training loss: 0.0678330659866333 = 0.001360928057692945 + 0.01 * 6.647213935852051
Epoch 1550, val loss: 1.504934310913086
Epoch 1560, training loss: 0.06780007481575012 = 0.0013498661573976278 + 0.01 * 6.645020961761475
Epoch 1560, val loss: 1.5062966346740723
Epoch 1570, training loss: 0.06799615174531937 = 0.0013390332460403442 + 0.01 * 6.665711879730225
Epoch 1570, val loss: 1.5075918436050415
Epoch 1580, training loss: 0.06782101839780807 = 0.0013284072047099471 + 0.01 * 6.649260997772217
Epoch 1580, val loss: 1.5089524984359741
Epoch 1590, training loss: 0.06785421818494797 = 0.0013180176028981805 + 0.01 * 6.65362024307251
Epoch 1590, val loss: 1.510241150856018
Epoch 1600, training loss: 0.0680101066827774 = 0.0013078151969239116 + 0.01 * 6.670228958129883
Epoch 1600, val loss: 1.511532187461853
Epoch 1610, training loss: 0.06767579168081284 = 0.0012979090679436922 + 0.01 * 6.637788772583008
Epoch 1610, val loss: 1.5128462314605713
Epoch 1620, training loss: 0.0678689181804657 = 0.001288184430450201 + 0.01 * 6.658073902130127
Epoch 1620, val loss: 1.5140613317489624
Epoch 1630, training loss: 0.06768856197595596 = 0.0012786946026608348 + 0.01 * 6.640986919403076
Epoch 1630, val loss: 1.5153071880340576
Epoch 1640, training loss: 0.06773575395345688 = 0.0012694427277892828 + 0.01 * 6.646631240844727
Epoch 1640, val loss: 1.5165005922317505
Epoch 1650, training loss: 0.06766857951879501 = 0.0012603610521182418 + 0.01 * 6.640821933746338
Epoch 1650, val loss: 1.5176368951797485
Epoch 1660, training loss: 0.06779275834560394 = 0.001251478330232203 + 0.01 * 6.654128074645996
Epoch 1660, val loss: 1.5188308954238892
Epoch 1670, training loss: 0.06750936061143875 = 0.001242774073034525 + 0.01 * 6.626659393310547
Epoch 1670, val loss: 1.5199594497680664
Epoch 1680, training loss: 0.06757255643606186 = 0.0012342846021056175 + 0.01 * 6.633827209472656
Epoch 1680, val loss: 1.5211033821105957
Epoch 1690, training loss: 0.06770006567239761 = 0.0012259159702807665 + 0.01 * 6.647414684295654
Epoch 1690, val loss: 1.5221971273422241
Epoch 1700, training loss: 0.06740591675043106 = 0.0012177458265796304 + 0.01 * 6.618817329406738
Epoch 1700, val loss: 1.5232762098312378
Epoch 1710, training loss: 0.06770849227905273 = 0.0012097822036594152 + 0.01 * 6.649871349334717
Epoch 1710, val loss: 1.5243343114852905
Epoch 1720, training loss: 0.0672842264175415 = 0.0012019218411296606 + 0.01 * 6.608231067657471
Epoch 1720, val loss: 1.525356411933899
Epoch 1730, training loss: 0.06734807044267654 = 0.0011942517012357712 + 0.01 * 6.615382194519043
Epoch 1730, val loss: 1.5263994932174683
Epoch 1740, training loss: 0.06754051148891449 = 0.0011867268476635218 + 0.01 * 6.635378360748291
Epoch 1740, val loss: 1.527383804321289
Epoch 1750, training loss: 0.06737212091684341 = 0.0011793222511187196 + 0.01 * 6.619279861450195
Epoch 1750, val loss: 1.528396487236023
Epoch 1760, training loss: 0.06744807958602905 = 0.0011720777256414294 + 0.01 * 6.627599716186523
Epoch 1760, val loss: 1.529336929321289
Epoch 1770, training loss: 0.06736086308956146 = 0.0011649444932118058 + 0.01 * 6.619592189788818
Epoch 1770, val loss: 1.530353307723999
Epoch 1780, training loss: 0.06749078631401062 = 0.001157960738055408 + 0.01 * 6.633282661437988
Epoch 1780, val loss: 1.531326413154602
Epoch 1790, training loss: 0.06718910485506058 = 0.0011510802432894707 + 0.01 * 6.603802680969238
Epoch 1790, val loss: 1.5322377681732178
Epoch 1800, training loss: 0.06728587299585342 = 0.0011443354887887836 + 0.01 * 6.614153861999512
Epoch 1800, val loss: 1.533252477645874
Epoch 1810, training loss: 0.06737155467271805 = 0.0011377386981621385 + 0.01 * 6.623381614685059
Epoch 1810, val loss: 1.5341382026672363
Epoch 1820, training loss: 0.06710973381996155 = 0.001131243770942092 + 0.01 * 6.597848892211914
Epoch 1820, val loss: 1.5350481271743774
Epoch 1830, training loss: 0.06738396733999252 = 0.0011249182280153036 + 0.01 * 6.6259050369262695
Epoch 1830, val loss: 1.5359361171722412
Epoch 1840, training loss: 0.06726318597793579 = 0.001118592917919159 + 0.01 * 6.61445951461792
Epoch 1840, val loss: 1.536872386932373
Epoch 1850, training loss: 0.06727832555770874 = 0.0011124422308057547 + 0.01 * 6.616588592529297
Epoch 1850, val loss: 1.5377687215805054
Epoch 1860, training loss: 0.06712165474891663 = 0.0011064212303608656 + 0.01 * 6.601523399353027
Epoch 1860, val loss: 1.5386422872543335
Epoch 1870, training loss: 0.06705174595117569 = 0.0011004098923876882 + 0.01 * 6.595133304595947
Epoch 1870, val loss: 1.5395764112472534
Epoch 1880, training loss: 0.06732168793678284 = 0.0010945770191028714 + 0.01 * 6.622710704803467
Epoch 1880, val loss: 1.5404657125473022
Epoch 1890, training loss: 0.06708073616027832 = 0.0010888162069022655 + 0.01 * 6.599192142486572
Epoch 1890, val loss: 1.5412875413894653
Epoch 1900, training loss: 0.06695719063282013 = 0.0010831670369952917 + 0.01 * 6.587402820587158
Epoch 1900, val loss: 1.5421836376190186
Epoch 1910, training loss: 0.06731043010950089 = 0.0010775853879749775 + 0.01 * 6.623284816741943
Epoch 1910, val loss: 1.5430465936660767
Epoch 1920, training loss: 0.0669429674744606 = 0.0010720595018938184 + 0.01 * 6.587091445922852
Epoch 1920, val loss: 1.543897271156311
Epoch 1930, training loss: 0.06714121997356415 = 0.001066650846041739 + 0.01 * 6.607457637786865
Epoch 1930, val loss: 1.544793725013733
Epoch 1940, training loss: 0.06688284873962402 = 0.0010613161139190197 + 0.01 * 6.582153797149658
Epoch 1940, val loss: 1.5456146001815796
Epoch 1950, training loss: 0.06704109907150269 = 0.0010560713708400726 + 0.01 * 6.5985026359558105
Epoch 1950, val loss: 1.5464999675750732
Epoch 1960, training loss: 0.0669424831867218 = 0.0010509424610063434 + 0.01 * 6.589154243469238
Epoch 1960, val loss: 1.5472924709320068
Epoch 1970, training loss: 0.0668623074889183 = 0.0010458219330757856 + 0.01 * 6.581648826599121
Epoch 1970, val loss: 1.548174500465393
Epoch 1980, training loss: 0.06706702709197998 = 0.0010408272501081228 + 0.01 * 6.602620601654053
Epoch 1980, val loss: 1.5489623546600342
Epoch 1990, training loss: 0.06679831445217133 = 0.0010358322178944945 + 0.01 * 6.5762481689453125
Epoch 1990, val loss: 1.549856185913086
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.0216803550720215 = 1.9357120990753174 + 0.01 * 8.596829414367676
Epoch 0, val loss: 1.9392123222351074
Epoch 10, training loss: 2.012284755706787 = 1.9263168573379517 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.9299899339675903
Epoch 20, training loss: 2.0010342597961426 = 1.9150686264038086 + 0.01 * 8.596562385559082
Epoch 20, val loss: 1.9188655614852905
Epoch 30, training loss: 1.9852839708328247 = 1.8993263244628906 + 0.01 * 8.595760345458984
Epoch 30, val loss: 1.9031720161437988
Epoch 40, training loss: 1.9620999097824097 = 1.8761922121047974 + 0.01 * 8.590764999389648
Epoch 40, val loss: 1.8801382780075073
Epoch 50, training loss: 1.9289205074310303 = 1.8434034585952759 + 0.01 * 8.551701545715332
Epoch 50, val loss: 1.8486953973770142
Epoch 60, training loss: 1.887263536453247 = 1.8041715621948242 + 0.01 * 8.309191703796387
Epoch 60, val loss: 1.813492774963379
Epoch 70, training loss: 1.8456114530563354 = 1.76581871509552 + 0.01 * 7.979268550872803
Epoch 70, val loss: 1.7792727947235107
Epoch 80, training loss: 1.7959836721420288 = 1.71868097782135 + 0.01 * 7.730270862579346
Epoch 80, val loss: 1.7333952188491821
Epoch 90, training loss: 1.7269837856292725 = 1.6521373987197876 + 0.01 * 7.484637260437012
Epoch 90, val loss: 1.6712175607681274
Epoch 100, training loss: 1.6383470296859741 = 1.565474271774292 + 0.01 * 7.287275314331055
Epoch 100, val loss: 1.5953761339187622
Epoch 110, training loss: 1.5382800102233887 = 1.4663734436035156 + 0.01 * 7.190659046173096
Epoch 110, val loss: 1.5093574523925781
Epoch 120, training loss: 1.4355580806732178 = 1.364128828048706 + 0.01 * 7.1429219245910645
Epoch 120, val loss: 1.4235875606536865
Epoch 130, training loss: 1.3338271379470825 = 1.262689471244812 + 0.01 * 7.1137614250183105
Epoch 130, val loss: 1.3404890298843384
Epoch 140, training loss: 1.234136939048767 = 1.1630260944366455 + 0.01 * 7.111083507537842
Epoch 140, val loss: 1.2608768939971924
Epoch 150, training loss: 1.1364487409591675 = 1.0653572082519531 + 0.01 * 7.109152793884277
Epoch 150, val loss: 1.1848684549331665
Epoch 160, training loss: 1.0400500297546387 = 0.9689822793006897 + 0.01 * 7.106780052185059
Epoch 160, val loss: 1.1105669736862183
Epoch 170, training loss: 0.9456149339675903 = 0.8745816349983215 + 0.01 * 7.103329181671143
Epoch 170, val loss: 1.037505030632019
Epoch 180, training loss: 0.855326771736145 = 0.7843537926673889 + 0.01 * 7.0973005294799805
Epoch 180, val loss: 0.967531681060791
Epoch 190, training loss: 0.7714310884475708 = 0.7005471587181091 + 0.01 * 7.08839225769043
Epoch 190, val loss: 0.9029535055160522
Epoch 200, training loss: 0.6948733329772949 = 0.6241050958633423 + 0.01 * 7.076825141906738
Epoch 200, val loss: 0.8454061150550842
Epoch 210, training loss: 0.6254429817199707 = 0.5548067688941956 + 0.01 * 7.063620567321777
Epoch 210, val loss: 0.7956695556640625
Epoch 220, training loss: 0.5621861815452576 = 0.4916747808456421 + 0.01 * 7.051141738891602
Epoch 220, val loss: 0.7533854842185974
Epoch 230, training loss: 0.5041446685791016 = 0.43374329805374146 + 0.01 * 7.040134906768799
Epoch 230, val loss: 0.7179325222969055
Epoch 240, training loss: 0.45083174109458923 = 0.3805074393749237 + 0.01 * 7.032430648803711
Epoch 240, val loss: 0.6887707710266113
Epoch 250, training loss: 0.40229469537734985 = 0.3320131003856659 + 0.01 * 7.028160095214844
Epoch 250, val loss: 0.6655843257904053
Epoch 260, training loss: 0.358762264251709 = 0.2885120213031769 + 0.01 * 7.025024890899658
Epoch 260, val loss: 0.6482369303703308
Epoch 270, training loss: 0.3203788101673126 = 0.2501523792743683 + 0.01 * 7.022643089294434
Epoch 270, val loss: 0.6361634135246277
Epoch 280, training loss: 0.28704026341438293 = 0.216825932264328 + 0.01 * 7.0214338302612305
Epoch 280, val loss: 0.6286255717277527
Epoch 290, training loss: 0.2583587169647217 = 0.18815235793590546 + 0.01 * 7.020637512207031
Epoch 290, val loss: 0.6248891353607178
Epoch 300, training loss: 0.23381933569908142 = 0.1635909229516983 + 0.01 * 7.022841453552246
Epoch 300, val loss: 0.6242247819900513
Epoch 310, training loss: 0.2127867043018341 = 0.14256343245506287 + 0.01 * 7.022327899932861
Epoch 310, val loss: 0.6259912252426147
Epoch 320, training loss: 0.19473311305046082 = 0.12453359365463257 + 0.01 * 7.019951820373535
Epoch 320, val loss: 0.6296750903129578
Epoch 330, training loss: 0.1792430728673935 = 0.10905421525239944 + 0.01 * 7.018886089324951
Epoch 330, val loss: 0.6348775625228882
Epoch 340, training loss: 0.16594375669956207 = 0.09576089680194855 + 0.01 * 7.018286228179932
Epoch 340, val loss: 0.6413441300392151
Epoch 350, training loss: 0.15452595055103302 = 0.08435013145208359 + 0.01 * 7.017582416534424
Epoch 350, val loss: 0.6488326191902161
Epoch 360, training loss: 0.1447276473045349 = 0.07456135749816895 + 0.01 * 7.016628265380859
Epoch 360, val loss: 0.6572319865226746
Epoch 370, training loss: 0.13631734251976013 = 0.06616038084030151 + 0.01 * 7.015695571899414
Epoch 370, val loss: 0.6663684248924255
Epoch 380, training loss: 0.12908285856246948 = 0.05892907828092575 + 0.01 * 7.015377998352051
Epoch 380, val loss: 0.676038920879364
Epoch 390, training loss: 0.12282201647758484 = 0.05268510803580284 + 0.01 * 7.01369047164917
Epoch 390, val loss: 0.6861415505409241
Epoch 400, training loss: 0.11738196015357971 = 0.047268468886613846 + 0.01 * 7.011349201202393
Epoch 400, val loss: 0.6965283751487732
Epoch 410, training loss: 0.11263987421989441 = 0.04255111888051033 + 0.01 * 7.008875370025635
Epoch 410, val loss: 0.7071204781532288
Epoch 420, training loss: 0.1085919439792633 = 0.038429513573646545 + 0.01 * 7.0162434577941895
Epoch 420, val loss: 0.7178476452827454
Epoch 430, training loss: 0.10489164292812347 = 0.034820783883333206 + 0.01 * 7.007086753845215
Epoch 430, val loss: 0.7286000847816467
Epoch 440, training loss: 0.1016617864370346 = 0.03165057674050331 + 0.01 * 7.001121520996094
Epoch 440, val loss: 0.7392659783363342
Epoch 450, training loss: 0.09883656352758408 = 0.028857558965682983 + 0.01 * 6.997900485992432
Epoch 450, val loss: 0.7498191595077515
Epoch 460, training loss: 0.09634922444820404 = 0.02639128640294075 + 0.01 * 6.995793342590332
Epoch 460, val loss: 0.7602010369300842
Epoch 470, training loss: 0.09411601722240448 = 0.024206407368183136 + 0.01 * 6.990961074829102
Epoch 470, val loss: 0.770362377166748
Epoch 480, training loss: 0.09228558093309402 = 0.022265559062361717 + 0.01 * 7.002002239227295
Epoch 480, val loss: 0.7803043127059937
Epoch 490, training loss: 0.09039425849914551 = 0.02054000087082386 + 0.01 * 6.985426425933838
Epoch 490, val loss: 0.7899927496910095
Epoch 500, training loss: 0.08877812325954437 = 0.01900000125169754 + 0.01 * 6.97781229019165
Epoch 500, val loss: 0.7993886470794678
Epoch 510, training loss: 0.08733906596899033 = 0.017620958387851715 + 0.01 * 6.971810817718506
Epoch 510, val loss: 0.8085664510726929
Epoch 520, training loss: 0.08619512617588043 = 0.016382766887545586 + 0.01 * 6.981235980987549
Epoch 520, val loss: 0.8174635767936707
Epoch 530, training loss: 0.08486480265855789 = 0.015270864591002464 + 0.01 * 6.9593939781188965
Epoch 530, val loss: 0.8261222839355469
Epoch 540, training loss: 0.08380572497844696 = 0.014268087223172188 + 0.01 * 6.95376443862915
Epoch 540, val loss: 0.8345118761062622
Epoch 550, training loss: 0.08303016424179077 = 0.013361976481974125 + 0.01 * 6.9668192863464355
Epoch 550, val loss: 0.8426409363746643
Epoch 560, training loss: 0.08195390552282333 = 0.01254300493746996 + 0.01 * 6.941090106964111
Epoch 560, val loss: 0.8505335450172424
Epoch 570, training loss: 0.08116548508405685 = 0.011798455379903316 + 0.01 * 6.936702728271484
Epoch 570, val loss: 0.8581475019454956
Epoch 580, training loss: 0.08033417165279388 = 0.011119693517684937 + 0.01 * 6.92144775390625
Epoch 580, val loss: 0.8656168580055237
Epoch 590, training loss: 0.08006656914949417 = 0.010500629432499409 + 0.01 * 6.956593990325928
Epoch 590, val loss: 0.8728200793266296
Epoch 600, training loss: 0.07912391424179077 = 0.009937379509210587 + 0.01 * 6.9186530113220215
Epoch 600, val loss: 0.8797355890274048
Epoch 610, training loss: 0.07844897359609604 = 0.009420359507203102 + 0.01 * 6.902861595153809
Epoch 610, val loss: 0.8864883780479431
Epoch 620, training loss: 0.07796922326087952 = 0.008944205939769745 + 0.01 * 6.902502059936523
Epoch 620, val loss: 0.8930650949478149
Epoch 630, training loss: 0.07747027277946472 = 0.008505678735673428 + 0.01 * 6.896459102630615
Epoch 630, val loss: 0.8993715643882751
Epoch 640, training loss: 0.07698030769824982 = 0.008101491257548332 + 0.01 * 6.887881278991699
Epoch 640, val loss: 0.9055999517440796
Epoch 650, training loss: 0.07655500620603561 = 0.007727712858468294 + 0.01 * 6.882729530334473
Epoch 650, val loss: 0.911538302898407
Epoch 660, training loss: 0.07615150511264801 = 0.0073817274533212185 + 0.01 * 6.876977443695068
Epoch 660, val loss: 0.9174066185951233
Epoch 670, training loss: 0.07581806182861328 = 0.007060306146740913 + 0.01 * 6.8757758140563965
Epoch 670, val loss: 0.9230124354362488
Epoch 680, training loss: 0.07545755803585052 = 0.006761685945093632 + 0.01 * 6.869587421417236
Epoch 680, val loss: 0.9285553693771362
Epoch 690, training loss: 0.07506853342056274 = 0.006483102682977915 + 0.01 * 6.858543395996094
Epoch 690, val loss: 0.9338745474815369
Epoch 700, training loss: 0.0748288482427597 = 0.006223177537322044 + 0.01 * 6.860567092895508
Epoch 700, val loss: 0.9391149282455444
Epoch 710, training loss: 0.07443416118621826 = 0.005980148445814848 + 0.01 * 6.845401287078857
Epoch 710, val loss: 0.9441859126091003
Epoch 720, training loss: 0.074480801820755 = 0.0057525415904819965 + 0.01 * 6.872826099395752
Epoch 720, val loss: 0.9491943120956421
Epoch 730, training loss: 0.07394663989543915 = 0.005539302714169025 + 0.01 * 6.840733528137207
Epoch 730, val loss: 0.9539450407028198
Epoch 740, training loss: 0.07375843822956085 = 0.00533922528848052 + 0.01 * 6.841921329498291
Epoch 740, val loss: 0.9586822390556335
Epoch 750, training loss: 0.07348381727933884 = 0.005150826647877693 + 0.01 * 6.83329963684082
Epoch 750, val loss: 0.9632336497306824
Epoch 760, training loss: 0.07337343692779541 = 0.00497373566031456 + 0.01 * 6.839969635009766
Epoch 760, val loss: 0.9677629470825195
Epoch 770, training loss: 0.07308520376682281 = 0.004806554410606623 + 0.01 * 6.827865123748779
Epoch 770, val loss: 0.9720304012298584
Epoch 780, training loss: 0.07281500101089478 = 0.004649176727980375 + 0.01 * 6.816582202911377
Epoch 780, val loss: 0.9764033555984497
Epoch 790, training loss: 0.07264527678489685 = 0.0045005446299910545 + 0.01 * 6.8144731521606445
Epoch 790, val loss: 0.9804799556732178
Epoch 800, training loss: 0.07251904904842377 = 0.004360035061836243 + 0.01 * 6.815901279449463
Epoch 800, val loss: 0.9845975637435913
Epoch 810, training loss: 0.07236581295728683 = 0.004226956050843 + 0.01 * 6.8138861656188965
Epoch 810, val loss: 0.9885601997375488
Epoch 820, training loss: 0.0721384659409523 = 0.00410094391554594 + 0.01 * 6.803752422332764
Epoch 820, val loss: 0.992475152015686
Epoch 830, training loss: 0.07223891466856003 = 0.003981368150562048 + 0.01 * 6.825754642486572
Epoch 830, val loss: 0.9961641430854797
Epoch 840, training loss: 0.07183535397052765 = 0.0038681598380208015 + 0.01 * 6.796719074249268
Epoch 840, val loss: 0.9999843835830688
Epoch 850, training loss: 0.07170310616493225 = 0.0037602924276143312 + 0.01 * 6.794281482696533
Epoch 850, val loss: 1.0036240816116333
Epoch 860, training loss: 0.07171178609132767 = 0.003657741704955697 + 0.01 * 6.805404186248779
Epoch 860, val loss: 1.0071964263916016
Epoch 870, training loss: 0.07159046083688736 = 0.0035603190772235394 + 0.01 * 6.803014278411865
Epoch 870, val loss: 1.010575532913208
Epoch 880, training loss: 0.07128334045410156 = 0.0034674610942602158 + 0.01 * 6.781587600708008
Epoch 880, val loss: 1.0140314102172852
Epoch 890, training loss: 0.07110560685396194 = 0.003378926543518901 + 0.01 * 6.77266788482666
Epoch 890, val loss: 1.0173685550689697
Epoch 900, training loss: 0.07104995846748352 = 0.003294035093858838 + 0.01 * 6.77559232711792
Epoch 900, val loss: 1.0206983089447021
Epoch 910, training loss: 0.07096142321825027 = 0.0032128968741744757 + 0.01 * 6.774852275848389
Epoch 910, val loss: 1.0240020751953125
Epoch 920, training loss: 0.07102309167385101 = 0.0031356916297227144 + 0.01 * 6.788740158081055
Epoch 920, val loss: 1.027123212814331
Epoch 930, training loss: 0.07072633504867554 = 0.0030620533507317305 + 0.01 * 6.766428470611572
Epoch 930, val loss: 1.0301973819732666
Epoch 940, training loss: 0.0705718919634819 = 0.0029917177744209766 + 0.01 * 6.758017063140869
Epoch 940, val loss: 1.0331693887710571
Epoch 950, training loss: 0.07065397500991821 = 0.002924354048445821 + 0.01 * 6.7729620933532715
Epoch 950, val loss: 1.036062479019165
Epoch 960, training loss: 0.07041102647781372 = 0.0028599859215319157 + 0.01 * 6.755104064941406
Epoch 960, val loss: 1.039135217666626
Epoch 970, training loss: 0.07019247114658356 = 0.00279838964343071 + 0.01 * 6.739408493041992
Epoch 970, val loss: 1.0418075323104858
Epoch 980, training loss: 0.07023105770349503 = 0.0027392616029828787 + 0.01 * 6.749180316925049
Epoch 980, val loss: 1.0446268320083618
Epoch 990, training loss: 0.0699455589056015 = 0.0026827752590179443 + 0.01 * 6.726278305053711
Epoch 990, val loss: 1.0472583770751953
Epoch 1000, training loss: 0.06987953931093216 = 0.0026283885817974806 + 0.01 * 6.725115776062012
Epoch 1000, val loss: 1.0499475002288818
Epoch 1010, training loss: 0.06993148475885391 = 0.0025760955177247524 + 0.01 * 6.735538959503174
Epoch 1010, val loss: 1.052456021308899
Epoch 1020, training loss: 0.06993522495031357 = 0.002525934251025319 + 0.01 * 6.74092960357666
Epoch 1020, val loss: 1.0551501512527466
Epoch 1030, training loss: 0.06963273882865906 = 0.002477695234119892 + 0.01 * 6.715504169464111
Epoch 1030, val loss: 1.0575710535049438
Epoch 1040, training loss: 0.06979524344205856 = 0.0024313372559845448 + 0.01 * 6.736390113830566
Epoch 1040, val loss: 1.0600024461746216
Epoch 1050, training loss: 0.06968320906162262 = 0.0023866670671850443 + 0.01 * 6.729654312133789
Epoch 1050, val loss: 1.062496542930603
Epoch 1060, training loss: 0.06976640224456787 = 0.0023436553310602903 + 0.01 * 6.742274761199951
Epoch 1060, val loss: 1.064698576927185
Epoch 1070, training loss: 0.06935808062553406 = 0.002302194479852915 + 0.01 * 6.7055888175964355
Epoch 1070, val loss: 1.0671453475952148
Epoch 1080, training loss: 0.06922120600938797 = 0.002262286376208067 + 0.01 * 6.695892333984375
Epoch 1080, val loss: 1.069322109222412
Epoch 1090, training loss: 0.0692162737250328 = 0.0022236828226596117 + 0.01 * 6.699258804321289
Epoch 1090, val loss: 1.0715452432632446
Epoch 1100, training loss: 0.06929866969585419 = 0.0021864385344088078 + 0.01 * 6.711223125457764
Epoch 1100, val loss: 1.073873519897461
Epoch 1110, training loss: 0.06907375901937485 = 0.002150533488020301 + 0.01 * 6.692322254180908
Epoch 1110, val loss: 1.075893759727478
Epoch 1120, training loss: 0.06896762549877167 = 0.002115777228027582 + 0.01 * 6.685184478759766
Epoch 1120, val loss: 1.0780601501464844
Epoch 1130, training loss: 0.06888018548488617 = 0.0020822256337851286 + 0.01 * 6.679795742034912
Epoch 1130, val loss: 1.0800960063934326
Epoch 1140, training loss: 0.06895681470632553 = 0.0020497089717537165 + 0.01 * 6.69071102142334
Epoch 1140, val loss: 1.082142949104309
Epoch 1150, training loss: 0.06884929537773132 = 0.002018377650529146 + 0.01 * 6.68309211730957
Epoch 1150, val loss: 1.084084391593933
Epoch 1160, training loss: 0.06878065317869186 = 0.001987908501178026 + 0.01 * 6.679274082183838
Epoch 1160, val loss: 1.086018681526184
Epoch 1170, training loss: 0.06900011748075485 = 0.0019584798719733953 + 0.01 * 6.704163551330566
Epoch 1170, val loss: 1.0879719257354736
Epoch 1180, training loss: 0.06877107918262482 = 0.0019299734849482775 + 0.01 * 6.684110641479492
Epoch 1180, val loss: 1.0898984670639038
Epoch 1190, training loss: 0.06896031647920609 = 0.001902404590509832 + 0.01 * 6.70579195022583
Epoch 1190, val loss: 1.0916553735733032
Epoch 1200, training loss: 0.06853196769952774 = 0.0018755896016955376 + 0.01 * 6.665638446807861
Epoch 1200, val loss: 1.0935187339782715
Epoch 1210, training loss: 0.06849055737257004 = 0.0018496335251256824 + 0.01 * 6.664092540740967
Epoch 1210, val loss: 1.0953553915023804
Epoch 1220, training loss: 0.06867040693759918 = 0.0018245178507640958 + 0.01 * 6.68458890914917
Epoch 1220, val loss: 1.097066879272461
Epoch 1230, training loss: 0.06848381459712982 = 0.0018000699346885085 + 0.01 * 6.668374538421631
Epoch 1230, val loss: 1.0987951755523682
Epoch 1240, training loss: 0.06846528500318527 = 0.001776409219019115 + 0.01 * 6.668887615203857
Epoch 1240, val loss: 1.1004407405853271
Epoch 1250, training loss: 0.0683336853981018 = 0.0017533863428980112 + 0.01 * 6.658030033111572
Epoch 1250, val loss: 1.1021478176116943
Epoch 1260, training loss: 0.06868654489517212 = 0.00173103844281286 + 0.01 * 6.695550918579102
Epoch 1260, val loss: 1.103704571723938
Epoch 1270, training loss: 0.06818995624780655 = 0.0017094059148803353 + 0.01 * 6.648055076599121
Epoch 1270, val loss: 1.1053617000579834
Epoch 1280, training loss: 0.06861044466495514 = 0.0016883611679077148 + 0.01 * 6.692208766937256
Epoch 1280, val loss: 1.1068576574325562
Epoch 1290, training loss: 0.06818696111440659 = 0.0016679177060723305 + 0.01 * 6.651904582977295
Epoch 1290, val loss: 1.108498454093933
Epoch 1300, training loss: 0.06808578222990036 = 0.0016480365302413702 + 0.01 * 6.643774509429932
Epoch 1300, val loss: 1.1099597215652466
Epoch 1310, training loss: 0.06812408566474915 = 0.0016286623431369662 + 0.01 * 6.649542808532715
Epoch 1310, val loss: 1.1114259958267212
Epoch 1320, training loss: 0.0681537389755249 = 0.0016098255291581154 + 0.01 * 6.654391765594482
Epoch 1320, val loss: 1.1128795146942139
Epoch 1330, training loss: 0.06816470623016357 = 0.001591523177921772 + 0.01 * 6.657318115234375
Epoch 1330, val loss: 1.1143590211868286
Epoch 1340, training loss: 0.06796547025442123 = 0.0015737205976620317 + 0.01 * 6.639174938201904
Epoch 1340, val loss: 1.115777611732483
Epoch 1350, training loss: 0.06805195659399033 = 0.0015563768101856112 + 0.01 * 6.649558067321777
Epoch 1350, val loss: 1.1171635389328003
Epoch 1360, training loss: 0.06786107271909714 = 0.0015394740039482713 + 0.01 * 6.63215970993042
Epoch 1360, val loss: 1.1184537410736084
Epoch 1370, training loss: 0.06775298714637756 = 0.0015230677090585232 + 0.01 * 6.622992038726807
Epoch 1370, val loss: 1.1198902130126953
Epoch 1380, training loss: 0.0679587796330452 = 0.0015070503577589989 + 0.01 * 6.645173072814941
Epoch 1380, val loss: 1.1210429668426514
Epoch 1390, training loss: 0.06773553043603897 = 0.0014914508210495114 + 0.01 * 6.62440824508667
Epoch 1390, val loss: 1.122416615486145
Epoch 1400, training loss: 0.06796789914369583 = 0.0014762741047888994 + 0.01 * 6.649162769317627
Epoch 1400, val loss: 1.123699426651001
Epoch 1410, training loss: 0.06770926713943481 = 0.0014613979728892446 + 0.01 * 6.624786376953125
Epoch 1410, val loss: 1.1248353719711304
Epoch 1420, training loss: 0.06765980273485184 = 0.0014469536254182458 + 0.01 * 6.6212849617004395
Epoch 1420, val loss: 1.1261816024780273
Epoch 1430, training loss: 0.06754794716835022 = 0.0014328787801787257 + 0.01 * 6.611506938934326
Epoch 1430, val loss: 1.1272934675216675
Epoch 1440, training loss: 0.0675467699766159 = 0.0014191343216225505 + 0.01 * 6.612763404846191
Epoch 1440, val loss: 1.12848699092865
Epoch 1450, training loss: 0.06753700971603394 = 0.0014056350337341428 + 0.01 * 6.613137722015381
Epoch 1450, val loss: 1.1296782493591309
Epoch 1460, training loss: 0.06755794584751129 = 0.0013925365637987852 + 0.01 * 6.616540908813477
Epoch 1460, val loss: 1.1307661533355713
Epoch 1470, training loss: 0.06738150119781494 = 0.0013797011924907565 + 0.01 * 6.600180625915527
Epoch 1470, val loss: 1.1319118738174438
Epoch 1480, training loss: 0.06727524846792221 = 0.0013671600027009845 + 0.01 * 6.590809345245361
Epoch 1480, val loss: 1.1330499649047852
Epoch 1490, training loss: 0.06751104444265366 = 0.001354952692054212 + 0.01 * 6.615609169006348
Epoch 1490, val loss: 1.1341191530227661
Epoch 1500, training loss: 0.06746353209018707 = 0.0013429791433736682 + 0.01 * 6.612054824829102
Epoch 1500, val loss: 1.1351944208145142
Epoch 1510, training loss: 0.06746960431337357 = 0.0013313080416992307 + 0.01 * 6.613830089569092
Epoch 1510, val loss: 1.136242389678955
Epoch 1520, training loss: 0.06725449115037918 = 0.0013198164524510503 + 0.01 * 6.593467712402344
Epoch 1520, val loss: 1.1371750831604004
Epoch 1530, training loss: 0.06734683364629745 = 0.0013086509425193071 + 0.01 * 6.603818416595459
Epoch 1530, val loss: 1.1382864713668823
Epoch 1540, training loss: 0.06728176772594452 = 0.00129771139472723 + 0.01 * 6.598405361175537
Epoch 1540, val loss: 1.1392358541488647
Epoch 1550, training loss: 0.06716027110815048 = 0.001287004561163485 + 0.01 * 6.587327003479004
Epoch 1550, val loss: 1.140252947807312
Epoch 1560, training loss: 0.06709995120763779 = 0.0012765148421749473 + 0.01 * 6.582343578338623
Epoch 1560, val loss: 1.1412222385406494
Epoch 1570, training loss: 0.06717733293771744 = 0.00126626156270504 + 0.01 * 6.591107368469238
Epoch 1570, val loss: 1.1422057151794434
Epoch 1580, training loss: 0.06718167662620544 = 0.0012562376214191318 + 0.01 * 6.592544078826904
Epoch 1580, val loss: 1.1431437730789185
Epoch 1590, training loss: 0.06705480813980103 = 0.001246416475623846 + 0.01 * 6.58083963394165
Epoch 1590, val loss: 1.1440438032150269
Epoch 1600, training loss: 0.06703263521194458 = 0.0012367712333798409 + 0.01 * 6.579586029052734
Epoch 1600, val loss: 1.1449977159500122
Epoch 1610, training loss: 0.06768791377544403 = 0.0012272895546630025 + 0.01 * 6.64606237411499
Epoch 1610, val loss: 1.1458195447921753
Epoch 1620, training loss: 0.06696068495512009 = 0.0012180700432509184 + 0.01 * 6.574261665344238
Epoch 1620, val loss: 1.1467597484588623
Epoch 1630, training loss: 0.06733021140098572 = 0.0012090224772691727 + 0.01 * 6.612119197845459
Epoch 1630, val loss: 1.1476430892944336
Epoch 1640, training loss: 0.06688649952411652 = 0.0012001260183751583 + 0.01 * 6.568637847900391
Epoch 1640, val loss: 1.1484791040420532
Epoch 1650, training loss: 0.06715525686740875 = 0.0011913953348994255 + 0.01 * 6.596386909484863
Epoch 1650, val loss: 1.1492769718170166
Epoch 1660, training loss: 0.06688307225704193 = 0.001182875712402165 + 0.01 * 6.570019721984863
Epoch 1660, val loss: 1.15015709400177
Epoch 1670, training loss: 0.06669846922159195 = 0.0011744721559807658 + 0.01 * 6.552399635314941
Epoch 1670, val loss: 1.1510030031204224
Epoch 1680, training loss: 0.06693943589925766 = 0.001166264759376645 + 0.01 * 6.577317237854004
Epoch 1680, val loss: 1.151794672012329
Epoch 1690, training loss: 0.06657294183969498 = 0.0011582087026908994 + 0.01 * 6.541473388671875
Epoch 1690, val loss: 1.152622103691101
Epoch 1700, training loss: 0.0666721984744072 = 0.0011502811685204506 + 0.01 * 6.552191734313965
Epoch 1700, val loss: 1.1533544063568115
Epoch 1710, training loss: 0.06681268662214279 = 0.0011425093980506063 + 0.01 * 6.567017555236816
Epoch 1710, val loss: 1.1541355848312378
Epoch 1720, training loss: 0.06656468659639359 = 0.001134872087277472 + 0.01 * 6.5429816246032715
Epoch 1720, val loss: 1.1549584865570068
Epoch 1730, training loss: 0.0667823776602745 = 0.0011273823911324143 + 0.01 * 6.565499305725098
Epoch 1730, val loss: 1.1557313203811646
Epoch 1740, training loss: 0.06646446138620377 = 0.001120015513151884 + 0.01 * 6.534444808959961
Epoch 1740, val loss: 1.156503438949585
Epoch 1750, training loss: 0.06662184745073318 = 0.001112782396376133 + 0.01 * 6.550906658172607
Epoch 1750, val loss: 1.15721595287323
Epoch 1760, training loss: 0.06665188819169998 = 0.0011056778021156788 + 0.01 * 6.55462121963501
Epoch 1760, val loss: 1.1579170227050781
Epoch 1770, training loss: 0.06663904339075089 = 0.0010986900888383389 + 0.01 * 6.554035186767578
Epoch 1770, val loss: 1.1586676836013794
Epoch 1780, training loss: 0.06652682274580002 = 0.001091823331080377 + 0.01 * 6.543499946594238
Epoch 1780, val loss: 1.1593847274780273
Epoch 1790, training loss: 0.06684722751379013 = 0.0010850710095837712 + 0.01 * 6.576215744018555
Epoch 1790, val loss: 1.1600978374481201
Epoch 1800, training loss: 0.06639154255390167 = 0.0010784500045701861 + 0.01 * 6.531309127807617
Epoch 1800, val loss: 1.1607822179794312
Epoch 1810, training loss: 0.06640367209911346 = 0.001071934006176889 + 0.01 * 6.533173561096191
Epoch 1810, val loss: 1.1614789962768555
Epoch 1820, training loss: 0.06658145785331726 = 0.001065518124960363 + 0.01 * 6.5515947341918945
Epoch 1820, val loss: 1.162171483039856
Epoch 1830, training loss: 0.06648879498243332 = 0.0010592126054689288 + 0.01 * 6.542958736419678
Epoch 1830, val loss: 1.1628680229187012
Epoch 1840, training loss: 0.0667782723903656 = 0.0010529947467148304 + 0.01 * 6.5725274085998535
Epoch 1840, val loss: 1.1635059118270874
Epoch 1850, training loss: 0.06634873151779175 = 0.0010468660620972514 + 0.01 * 6.530186653137207
Epoch 1850, val loss: 1.1641932725906372
Epoch 1860, training loss: 0.06638623774051666 = 0.0010408556554466486 + 0.01 * 6.534538745880127
Epoch 1860, val loss: 1.1648390293121338
Epoch 1870, training loss: 0.06628762185573578 = 0.0010349350050091743 + 0.01 * 6.5252685546875
Epoch 1870, val loss: 1.1655356884002686
Epoch 1880, training loss: 0.06647688895463943 = 0.0010291322832927108 + 0.01 * 6.54477596282959
Epoch 1880, val loss: 1.1661720275878906
Epoch 1890, training loss: 0.06639904528856277 = 0.0010233946377411485 + 0.01 * 6.537565231323242
Epoch 1890, val loss: 1.1668224334716797
Epoch 1900, training loss: 0.06650494039058685 = 0.001017760019749403 + 0.01 * 6.548718452453613
Epoch 1900, val loss: 1.1674509048461914
Epoch 1910, training loss: 0.06632669270038605 = 0.0010121774394065142 + 0.01 * 6.53145170211792
Epoch 1910, val loss: 1.1680378913879395
Epoch 1920, training loss: 0.06630917638540268 = 0.0010067210532724857 + 0.01 * 6.530245780944824
Epoch 1920, val loss: 1.1686644554138184
Epoch 1930, training loss: 0.0662570521235466 = 0.001001315307803452 + 0.01 * 6.52557373046875
Epoch 1930, val loss: 1.1692943572998047
Epoch 1940, training loss: 0.06639201939105988 = 0.000996014685370028 + 0.01 * 6.539600372314453
Epoch 1940, val loss: 1.1699072122573853
Epoch 1950, training loss: 0.06626489013433456 = 0.0009907216299325228 + 0.01 * 6.527417182922363
Epoch 1950, val loss: 1.1705061197280884
Epoch 1960, training loss: 0.06645851582288742 = 0.0009855771204456687 + 0.01 * 6.5472941398620605
Epoch 1960, val loss: 1.1711475849151611
Epoch 1970, training loss: 0.06613431125879288 = 0.000980500248260796 + 0.01 * 6.515380859375
Epoch 1970, val loss: 1.1717239618301392
Epoch 1980, training loss: 0.06625693291425705 = 0.0009754744241945446 + 0.01 * 6.528145790100098
Epoch 1980, val loss: 1.1723121404647827
Epoch 1990, training loss: 0.06604965031147003 = 0.000970532710198313 + 0.01 * 6.507911682128906
Epoch 1990, val loss: 1.1729321479797363
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.024214744567871 = 1.9382463693618774 + 0.01 * 8.596826553344727
Epoch 0, val loss: 1.942001223564148
Epoch 10, training loss: 2.0143020153045654 = 1.9283345937728882 + 0.01 * 8.596733093261719
Epoch 10, val loss: 1.9325168132781982
Epoch 20, training loss: 2.0019428730010986 = 1.915979027748108 + 0.01 * 8.596393585205078
Epoch 20, val loss: 1.9202919006347656
Epoch 30, training loss: 1.9847478866577148 = 1.898797869682312 + 0.01 * 8.595003128051758
Epoch 30, val loss: 1.903010606765747
Epoch 40, training loss: 1.9598957300186157 = 1.8740346431732178 + 0.01 * 8.586112022399902
Epoch 40, val loss: 1.8782542943954468
Epoch 50, training loss: 1.925656795501709 = 1.8402771949768066 + 0.01 * 8.537965774536133
Epoch 50, val loss: 1.84591543674469
Epoch 60, training loss: 1.8859038352966309 = 1.8024253845214844 + 0.01 * 8.347845077514648
Epoch 60, val loss: 1.81229567527771
Epoch 70, training loss: 1.8486509323120117 = 1.7676070928573608 + 0.01 * 8.104387283325195
Epoch 70, val loss: 1.7820336818695068
Epoch 80, training loss: 1.8044602870941162 = 1.7244987487792969 + 0.01 * 7.996158599853516
Epoch 80, val loss: 1.742263913154602
Epoch 90, training loss: 1.7414296865463257 = 1.6632533073425293 + 0.01 * 7.817638874053955
Epoch 90, val loss: 1.6884751319885254
Epoch 100, training loss: 1.657793641090393 = 1.581443428993225 + 0.01 * 7.6350178718566895
Epoch 100, val loss: 1.6198276281356812
Epoch 110, training loss: 1.558701753616333 = 1.4838095903396606 + 0.01 * 7.489217281341553
Epoch 110, val loss: 1.5392295122146606
Epoch 120, training loss: 1.4548416137695312 = 1.3806513547897339 + 0.01 * 7.419024467468262
Epoch 120, val loss: 1.4533106088638306
Epoch 130, training loss: 1.3513637781143188 = 1.277937412261963 + 0.01 * 7.3426361083984375
Epoch 130, val loss: 1.3692128658294678
Epoch 140, training loss: 1.2503411769866943 = 1.1777786016464233 + 0.01 * 7.256261825561523
Epoch 140, val loss: 1.288169026374817
Epoch 150, training loss: 1.1550155878067017 = 1.0829635858535767 + 0.01 * 7.205197811126709
Epoch 150, val loss: 1.213114619255066
Epoch 160, training loss: 1.0681003332138062 = 0.9962625503540039 + 0.01 * 7.183773994445801
Epoch 160, val loss: 1.146301031112671
Epoch 170, training loss: 0.9889374375343323 = 0.9171804189682007 + 0.01 * 7.17570161819458
Epoch 170, val loss: 1.0860283374786377
Epoch 180, training loss: 0.914753258228302 = 0.8430670499801636 + 0.01 * 7.168622016906738
Epoch 180, val loss: 1.029175043106079
Epoch 190, training loss: 0.8435276746749878 = 0.7719205617904663 + 0.01 * 7.160712242126465
Epoch 190, val loss: 0.9743354916572571
Epoch 200, training loss: 0.7754497528076172 = 0.7039307355880737 + 0.01 * 7.151900768280029
Epoch 200, val loss: 0.9218435287475586
Epoch 210, training loss: 0.7115744948387146 = 0.6401568055152893 + 0.01 * 7.141770839691162
Epoch 210, val loss: 0.8736248016357422
Epoch 220, training loss: 0.6518014669418335 = 0.5804975032806396 + 0.01 * 7.130398273468018
Epoch 220, val loss: 0.8307178020477295
Epoch 230, training loss: 0.5948497653007507 = 0.52367103099823 + 0.01 * 7.117875576019287
Epoch 230, val loss: 0.7927080392837524
Epoch 240, training loss: 0.5395601391792297 = 0.4685235917568207 + 0.01 * 7.103653907775879
Epoch 240, val loss: 0.7586021423339844
Epoch 250, training loss: 0.4855883717536926 = 0.4147186279296875 + 0.01 * 7.08697509765625
Epoch 250, val loss: 0.7280974984169006
Epoch 260, training loss: 0.4334946572780609 = 0.3627745807170868 + 0.01 * 7.0720086097717285
Epoch 260, val loss: 0.701300859451294
Epoch 270, training loss: 0.3842925727367401 = 0.31376299262046814 + 0.01 * 7.0529584884643555
Epoch 270, val loss: 0.6787591576576233
Epoch 280, training loss: 0.33920642733573914 = 0.2688691318035126 + 0.01 * 7.033729553222656
Epoch 280, val loss: 0.6609151363372803
Epoch 290, training loss: 0.299211323261261 = 0.2289377897977829 + 0.01 * 7.027352333068848
Epoch 290, val loss: 0.6482282280921936
Epoch 300, training loss: 0.26439452171325684 = 0.19430555403232574 + 0.01 * 7.008898735046387
Epoch 300, val loss: 0.6408414840698242
Epoch 310, training loss: 0.23487168550491333 = 0.164877787232399 + 0.01 * 6.999391078948975
Epoch 310, val loss: 0.6385065913200378
Epoch 320, training loss: 0.21016965806484222 = 0.14022812247276306 + 0.01 * 6.99415397644043
Epoch 320, val loss: 0.6405783891677856
Epoch 330, training loss: 0.18962901830673218 = 0.11977170407772064 + 0.01 * 6.985731601715088
Epoch 330, val loss: 0.646268367767334
Epoch 340, training loss: 0.17270803451538086 = 0.10287664830684662 + 0.01 * 6.983138561248779
Epoch 340, val loss: 0.6547374725341797
Epoch 350, training loss: 0.15870395302772522 = 0.08893004804849625 + 0.01 * 6.977389812469482
Epoch 350, val loss: 0.6651517748832703
Epoch 360, training loss: 0.14707474410533905 = 0.07737302035093307 + 0.01 * 6.970172882080078
Epoch 360, val loss: 0.6768567562103271
Epoch 370, training loss: 0.13739484548568726 = 0.0677531436085701 + 0.01 * 6.964171409606934
Epoch 370, val loss: 0.6893730163574219
Epoch 380, training loss: 0.12927967309951782 = 0.05970440432429314 + 0.01 * 6.957527160644531
Epoch 380, val loss: 0.7022711038589478
Epoch 390, training loss: 0.12251423299312592 = 0.05292518809437752 + 0.01 * 6.958904266357422
Epoch 390, val loss: 0.7153304219245911
Epoch 400, training loss: 0.11677631735801697 = 0.04718531295657158 + 0.01 * 6.95910120010376
Epoch 400, val loss: 0.7282870411872864
Epoch 410, training loss: 0.11170526593923569 = 0.04229806363582611 + 0.01 * 6.940720081329346
Epoch 410, val loss: 0.7410333752632141
Epoch 420, training loss: 0.10738575458526611 = 0.038107212632894516 + 0.01 * 6.927854061126709
Epoch 420, val loss: 0.7535558342933655
Epoch 430, training loss: 0.1037713885307312 = 0.03449185565114021 + 0.01 * 6.927953720092773
Epoch 430, val loss: 0.7658299207687378
Epoch 440, training loss: 0.1005282998085022 = 0.03136184439063072 + 0.01 * 6.9166460037231445
Epoch 440, val loss: 0.7777470350265503
Epoch 450, training loss: 0.09772509336471558 = 0.028632186353206635 + 0.01 * 6.909290790557861
Epoch 450, val loss: 0.7893217206001282
Epoch 460, training loss: 0.09533846378326416 = 0.026238422840833664 + 0.01 * 6.910004615783691
Epoch 460, val loss: 0.8005675077438354
Epoch 470, training loss: 0.09315566718578339 = 0.024130377918481827 + 0.01 * 6.902529716491699
Epoch 470, val loss: 0.81147301197052
Epoch 480, training loss: 0.09117865562438965 = 0.02226504310965538 + 0.01 * 6.891360759735107
Epoch 480, val loss: 0.8220149278640747
Epoch 490, training loss: 0.0895809531211853 = 0.02060732990503311 + 0.01 * 6.89736270904541
Epoch 490, val loss: 0.8322460055351257
Epoch 500, training loss: 0.08793406188488007 = 0.01913030631840229 + 0.01 * 6.880375862121582
Epoch 500, val loss: 0.8421558141708374
Epoch 510, training loss: 0.0865824818611145 = 0.017808962613344193 + 0.01 * 6.877351760864258
Epoch 510, val loss: 0.8517638444900513
Epoch 520, training loss: 0.08533592522144318 = 0.01662290282547474 + 0.01 * 6.871302604675293
Epoch 520, val loss: 0.8610518574714661
Epoch 530, training loss: 0.08414521813392639 = 0.015552788972854614 + 0.01 * 6.859243392944336
Epoch 530, val loss: 0.8700281381607056
Epoch 540, training loss: 0.08326275646686554 = 0.014584601856768131 + 0.01 * 6.8678154945373535
Epoch 540, val loss: 0.8787952065467834
Epoch 550, training loss: 0.08227571099996567 = 0.01370752789080143 + 0.01 * 6.856818675994873
Epoch 550, val loss: 0.8872268795967102
Epoch 560, training loss: 0.08136158436536789 = 0.012909328565001488 + 0.01 * 6.8452253341674805
Epoch 560, val loss: 0.89544677734375
Epoch 570, training loss: 0.08067981153726578 = 0.01218149159103632 + 0.01 * 6.849832057952881
Epoch 570, val loss: 0.9034336805343628
Epoch 580, training loss: 0.07987336814403534 = 0.011517303064465523 + 0.01 * 6.835607051849365
Epoch 580, val loss: 0.9111541509628296
Epoch 590, training loss: 0.07932378351688385 = 0.010907810181379318 + 0.01 * 6.841597080230713
Epoch 590, val loss: 0.918636679649353
Epoch 600, training loss: 0.07860589772462845 = 0.010348229669034481 + 0.01 * 6.825766563415527
Epoch 600, val loss: 0.9259330630302429
Epoch 610, training loss: 0.0781347006559372 = 0.009832820855081081 + 0.01 * 6.830187797546387
Epoch 610, val loss: 0.9330129027366638
Epoch 620, training loss: 0.07748532295227051 = 0.009357740171253681 + 0.01 * 6.812757968902588
Epoch 620, val loss: 0.9398744702339172
Epoch 630, training loss: 0.07691323012113571 = 0.008918152190744877 + 0.01 * 6.799508094787598
Epoch 630, val loss: 0.9465754628181458
Epoch 640, training loss: 0.07650802284479141 = 0.008510405197739601 + 0.01 * 6.799761772155762
Epoch 640, val loss: 0.9530894160270691
Epoch 650, training loss: 0.07631849497556686 = 0.008132501505315304 + 0.01 * 6.818599224090576
Epoch 650, val loss: 0.9594413638114929
Epoch 660, training loss: 0.07567569613456726 = 0.007781898602843285 + 0.01 * 6.789379596710205
Epoch 660, val loss: 0.965573787689209
Epoch 670, training loss: 0.07522298395633698 = 0.007455693557858467 + 0.01 * 6.776729106903076
Epoch 670, val loss: 0.9715601205825806
Epoch 680, training loss: 0.07484040409326553 = 0.007151254452764988 + 0.01 * 6.768915176391602
Epoch 680, val loss: 0.9774009585380554
Epoch 690, training loss: 0.07492393255233765 = 0.006866481155157089 + 0.01 * 6.805744647979736
Epoch 690, val loss: 0.983128547668457
Epoch 700, training loss: 0.07423009723424911 = 0.006601194851100445 + 0.01 * 6.762890815734863
Epoch 700, val loss: 0.9885822534561157
Epoch 710, training loss: 0.07416599988937378 = 0.006352321244776249 + 0.01 * 6.781368255615234
Epoch 710, val loss: 0.9940040707588196
Epoch 720, training loss: 0.07373970746994019 = 0.006119103170931339 + 0.01 * 6.762060642242432
Epoch 720, val loss: 0.9991920590400696
Epoch 730, training loss: 0.07339581102132797 = 0.005899667739868164 + 0.01 * 6.749614715576172
Epoch 730, val loss: 1.0043386220932007
Epoch 740, training loss: 0.07309988886117935 = 0.005693124141544104 + 0.01 * 6.7406768798828125
Epoch 740, val loss: 1.0093624591827393
Epoch 750, training loss: 0.07312911748886108 = 0.005498316138982773 + 0.01 * 6.76308012008667
Epoch 750, val loss: 1.014269232749939
Epoch 760, training loss: 0.07265400886535645 = 0.005314474459737539 + 0.01 * 6.733953475952148
Epoch 760, val loss: 1.0190231800079346
Epoch 770, training loss: 0.07279478013515472 = 0.0051408372819423676 + 0.01 * 6.76539421081543
Epoch 770, val loss: 1.0237460136413574
Epoch 780, training loss: 0.07228358834981918 = 0.004977161064743996 + 0.01 * 6.730643272399902
Epoch 780, val loss: 1.02824866771698
Epoch 790, training loss: 0.07207183539867401 = 0.0048220050521194935 + 0.01 * 6.724982738494873
Epoch 790, val loss: 1.0327306985855103
Epoch 800, training loss: 0.07186360657215118 = 0.004674934782087803 + 0.01 * 6.718867301940918
Epoch 800, val loss: 1.0370286703109741
Epoch 810, training loss: 0.07198686897754669 = 0.004535179119557142 + 0.01 * 6.745169639587402
Epoch 810, val loss: 1.0413426160812378
Epoch 820, training loss: 0.07172214984893799 = 0.004402804654091597 + 0.01 * 6.731935024261475
Epoch 820, val loss: 1.0454881191253662
Epoch 830, training loss: 0.07146605849266052 = 0.004277034197002649 + 0.01 * 6.718902587890625
Epoch 830, val loss: 1.0495409965515137
Epoch 840, training loss: 0.07129951566457748 = 0.004157422110438347 + 0.01 * 6.714209079742432
Epoch 840, val loss: 1.053531289100647
Epoch 850, training loss: 0.0712810680270195 = 0.004043417051434517 + 0.01 * 6.7237653732299805
Epoch 850, val loss: 1.057401418685913
Epoch 860, training loss: 0.07110238075256348 = 0.003934809006750584 + 0.01 * 6.716757774353027
Epoch 860, val loss: 1.0612850189208984
Epoch 870, training loss: 0.07092534005641937 = 0.0038315774872899055 + 0.01 * 6.709375858306885
Epoch 870, val loss: 1.0649534463882446
Epoch 880, training loss: 0.07076086848974228 = 0.00373275438323617 + 0.01 * 6.702811241149902
Epoch 880, val loss: 1.0685961246490479
Epoch 890, training loss: 0.07068943232297897 = 0.003638411173596978 + 0.01 * 6.705102443695068
Epoch 890, val loss: 1.0722479820251465
Epoch 900, training loss: 0.07068577408790588 = 0.003548357868567109 + 0.01 * 6.713741779327393
Epoch 900, val loss: 1.0757148265838623
Epoch 910, training loss: 0.07050951570272446 = 0.0034622130915522575 + 0.01 * 6.70473051071167
Epoch 910, val loss: 1.0791670083999634
Epoch 920, training loss: 0.07032564282417297 = 0.0033797495998442173 + 0.01 * 6.694589614868164
Epoch 920, val loss: 1.0825318098068237
Epoch 930, training loss: 0.07017938047647476 = 0.003300759708508849 + 0.01 * 6.687862873077393
Epoch 930, val loss: 1.0858720541000366
Epoch 940, training loss: 0.07021155208349228 = 0.003225176827982068 + 0.01 * 6.69863748550415
Epoch 940, val loss: 1.0890978574752808
Epoch 950, training loss: 0.07011376321315765 = 0.003152688266709447 + 0.01 * 6.696107387542725
Epoch 950, val loss: 1.0923035144805908
Epoch 960, training loss: 0.06996678560972214 = 0.0030832288321107626 + 0.01 * 6.688355445861816
Epoch 960, val loss: 1.095401644706726
Epoch 970, training loss: 0.06985166668891907 = 0.0030165172647684813 + 0.01 * 6.6835150718688965
Epoch 970, val loss: 1.0984585285186768
Epoch 980, training loss: 0.06974063813686371 = 0.0029522711411118507 + 0.01 * 6.678836822509766
Epoch 980, val loss: 1.1015130281448364
Epoch 990, training loss: 0.06991702318191528 = 0.002890622476115823 + 0.01 * 6.702640056610107
Epoch 990, val loss: 1.1044431924819946
Epoch 1000, training loss: 0.06968065351247787 = 0.0028315528761595488 + 0.01 * 6.684909820556641
Epoch 1000, val loss: 1.1073347330093384
Epoch 1010, training loss: 0.06976766884326935 = 0.002774536609649658 + 0.01 * 6.699313163757324
Epoch 1010, val loss: 1.1102020740509033
Epoch 1020, training loss: 0.06954324245452881 = 0.0027195727452635765 + 0.01 * 6.682367324829102
Epoch 1020, val loss: 1.1129499673843384
Epoch 1030, training loss: 0.06931839138269424 = 0.0026667180936783552 + 0.01 * 6.665167331695557
Epoch 1030, val loss: 1.1156853437423706
Epoch 1040, training loss: 0.0696682557463646 = 0.002615708624944091 + 0.01 * 6.705255508422852
Epoch 1040, val loss: 1.1184725761413574
Epoch 1050, training loss: 0.0693371370434761 = 0.00256665563210845 + 0.01 * 6.6770477294921875
Epoch 1050, val loss: 1.1210371255874634
Epoch 1060, training loss: 0.06918835639953613 = 0.0025193647015839815 + 0.01 * 6.666899681091309
Epoch 1060, val loss: 1.1236560344696045
Epoch 1070, training loss: 0.06921441853046417 = 0.0024737862404435873 + 0.01 * 6.674063205718994
Epoch 1070, val loss: 1.1261985301971436
Epoch 1080, training loss: 0.06929555535316467 = 0.0024297793861478567 + 0.01 * 6.686578273773193
Epoch 1080, val loss: 1.1287647485733032
Epoch 1090, training loss: 0.06923818588256836 = 0.0023874989710748196 + 0.01 * 6.685068607330322
Epoch 1090, val loss: 1.1311472654342651
Epoch 1100, training loss: 0.06889372318983078 = 0.0023465228732675314 + 0.01 * 6.654720306396484
Epoch 1100, val loss: 1.1335591077804565
Epoch 1110, training loss: 0.0691821500658989 = 0.0023068806622177362 + 0.01 * 6.687527179718018
Epoch 1110, val loss: 1.1358933448791504
Epoch 1120, training loss: 0.06894005835056305 = 0.0022687388118356466 + 0.01 * 6.6671319007873535
Epoch 1120, val loss: 1.1382293701171875
Epoch 1130, training loss: 0.06889943033456802 = 0.0022317005787044764 + 0.01 * 6.666772842407227
Epoch 1130, val loss: 1.1404850482940674
Epoch 1140, training loss: 0.06873410940170288 = 0.0021958097349852324 + 0.01 * 6.653829574584961
Epoch 1140, val loss: 1.1427922248840332
Epoch 1150, training loss: 0.06885643303394318 = 0.00216110423207283 + 0.01 * 6.669532775878906
Epoch 1150, val loss: 1.144956350326538
Epoch 1160, training loss: 0.06878841668367386 = 0.0021275507751852274 + 0.01 * 6.666086673736572
Epoch 1160, val loss: 1.1471569538116455
Epoch 1170, training loss: 0.06856392323970795 = 0.0020950823090970516 + 0.01 * 6.646884441375732
Epoch 1170, val loss: 1.1493117809295654
Epoch 1180, training loss: 0.06869353353977203 = 0.0020637004636228085 + 0.01 * 6.662983417510986
Epoch 1180, val loss: 1.151379108428955
Epoch 1190, training loss: 0.06861327588558197 = 0.002033151686191559 + 0.01 * 6.658012390136719
Epoch 1190, val loss: 1.1534503698349
Epoch 1200, training loss: 0.06849664449691772 = 0.0020036031492054462 + 0.01 * 6.649304389953613
Epoch 1200, val loss: 1.1554635763168335
Epoch 1210, training loss: 0.06855543702840805 = 0.001974924234673381 + 0.01 * 6.658051490783691
Epoch 1210, val loss: 1.1574640274047852
Epoch 1220, training loss: 0.06840122491121292 = 0.0019471215782687068 + 0.01 * 6.645411014556885
Epoch 1220, val loss: 1.1593736410140991
Epoch 1230, training loss: 0.06871648877859116 = 0.001920076902024448 + 0.01 * 6.6796417236328125
Epoch 1230, val loss: 1.1613306999206543
Epoch 1240, training loss: 0.06840818375349045 = 0.0018939577275887132 + 0.01 * 6.651422500610352
Epoch 1240, val loss: 1.1632179021835327
Epoch 1250, training loss: 0.06870701909065247 = 0.0018685328541323543 + 0.01 * 6.683848857879639
Epoch 1250, val loss: 1.165091633796692
Epoch 1260, training loss: 0.06823942810297012 = 0.0018438722472637892 + 0.01 * 6.639555931091309
Epoch 1260, val loss: 1.1669613122940063
Epoch 1270, training loss: 0.06822843849658966 = 0.001819995348341763 + 0.01 * 6.640844345092773
Epoch 1270, val loss: 1.168715000152588
Epoch 1280, training loss: 0.06833190470933914 = 0.001796787022612989 + 0.01 * 6.6535115242004395
Epoch 1280, val loss: 1.1704580783843994
Epoch 1290, training loss: 0.06807635724544525 = 0.0017741556512191892 + 0.01 * 6.630220413208008
Epoch 1290, val loss: 1.17222261428833
Epoch 1300, training loss: 0.06834788620471954 = 0.0017521196277812123 + 0.01 * 6.659576416015625
Epoch 1300, val loss: 1.1738996505737305
Epoch 1310, training loss: 0.06803888827562332 = 0.0017308014212176204 + 0.01 * 6.6308088302612305
Epoch 1310, val loss: 1.1755927801132202
Epoch 1320, training loss: 0.06813958287239075 = 0.001709964475594461 + 0.01 * 6.642961502075195
Epoch 1320, val loss: 1.1773478984832764
Epoch 1330, training loss: 0.06806495040655136 = 0.0016897819004952908 + 0.01 * 6.637516498565674
Epoch 1330, val loss: 1.1788551807403564
Epoch 1340, training loss: 0.06794294714927673 = 0.0016701300628483295 + 0.01 * 6.627281665802002
Epoch 1340, val loss: 1.1805013418197632
Epoch 1350, training loss: 0.0679037943482399 = 0.0016510537825524807 + 0.01 * 6.625273704528809
Epoch 1350, val loss: 1.1820032596588135
Epoch 1360, training loss: 0.06788670271635056 = 0.001632380299270153 + 0.01 * 6.62543249130249
Epoch 1360, val loss: 1.1836227178573608
Epoch 1370, training loss: 0.06791047751903534 = 0.0016142339445650578 + 0.01 * 6.629624366760254
Epoch 1370, val loss: 1.1850937604904175
Epoch 1380, training loss: 0.06764782965183258 = 0.001596534508280456 + 0.01 * 6.605129718780518
Epoch 1380, val loss: 1.186627984046936
Epoch 1390, training loss: 0.06772937625646591 = 0.001579341827891767 + 0.01 * 6.61500358581543
Epoch 1390, val loss: 1.1881144046783447
Epoch 1400, training loss: 0.06758390367031097 = 0.0015626136446371675 + 0.01 * 6.6021294593811035
Epoch 1400, val loss: 1.1894856691360474
Epoch 1410, training loss: 0.0675344243645668 = 0.0015462505398318172 + 0.01 * 6.598817825317383
Epoch 1410, val loss: 1.1910377740859985
Epoch 1420, training loss: 0.0675974041223526 = 0.0015303402906283736 + 0.01 * 6.606706142425537
Epoch 1420, val loss: 1.1922967433929443
Epoch 1430, training loss: 0.06755483150482178 = 0.0015147748636081815 + 0.01 * 6.604005813598633
Epoch 1430, val loss: 1.193719506263733
Epoch 1440, training loss: 0.06795762479305267 = 0.0014996057143434882 + 0.01 * 6.645802021026611
Epoch 1440, val loss: 1.195117712020874
Epoch 1450, training loss: 0.06757977604866028 = 0.0014847753336653113 + 0.01 * 6.609500408172607
Epoch 1450, val loss: 1.1964460611343384
Epoch 1460, training loss: 0.06790543347597122 = 0.0014703497290611267 + 0.01 * 6.643508434295654
Epoch 1460, val loss: 1.1977100372314453
Epoch 1470, training loss: 0.06728972494602203 = 0.0014562162104994059 + 0.01 * 6.583351135253906
Epoch 1470, val loss: 1.1991444826126099
Epoch 1480, training loss: 0.06789814680814743 = 0.0014424333348870277 + 0.01 * 6.645571231842041
Epoch 1480, val loss: 1.2003185749053955
Epoch 1490, training loss: 0.06743813306093216 = 0.001429053139872849 + 0.01 * 6.600908279418945
Epoch 1490, val loss: 1.2016665935516357
Epoch 1500, training loss: 0.06750564277172089 = 0.001415913226082921 + 0.01 * 6.608972549438477
Epoch 1500, val loss: 1.2028543949127197
Epoch 1510, training loss: 0.06734426319599152 = 0.001403097645379603 + 0.01 * 6.5941162109375
Epoch 1510, val loss: 1.2041316032409668
Epoch 1520, training loss: 0.06716181337833405 = 0.0013905420200899243 + 0.01 * 6.577127933502197
Epoch 1520, val loss: 1.2053329944610596
Epoch 1530, training loss: 0.06701625883579254 = 0.001378234475851059 + 0.01 * 6.563802242279053
Epoch 1530, val loss: 1.2065633535385132
Epoch 1540, training loss: 0.06739414483308792 = 0.0013662026030942798 + 0.01 * 6.602794170379639
Epoch 1540, val loss: 1.2077233791351318
Epoch 1550, training loss: 0.06720076501369476 = 0.001354449661448598 + 0.01 * 6.58463191986084
Epoch 1550, val loss: 1.2090020179748535
Epoch 1560, training loss: 0.06722097843885422 = 0.00134302640799433 + 0.01 * 6.587795257568359
Epoch 1560, val loss: 1.2101187705993652
Epoch 1570, training loss: 0.06708497554063797 = 0.0013318018754944205 + 0.01 * 6.575317859649658
Epoch 1570, val loss: 1.2112880945205688
Epoch 1580, training loss: 0.06721299141645432 = 0.0013208416057750583 + 0.01 * 6.589215278625488
Epoch 1580, val loss: 1.2123754024505615
Epoch 1590, training loss: 0.0670187771320343 = 0.001310060964897275 + 0.01 * 6.570871829986572
Epoch 1590, val loss: 1.213582992553711
Epoch 1600, training loss: 0.0670403316617012 = 0.0012995420256629586 + 0.01 * 6.574078559875488
Epoch 1600, val loss: 1.2146755456924438
Epoch 1610, training loss: 0.06699137389659882 = 0.0012892350787296891 + 0.01 * 6.57021427154541
Epoch 1610, val loss: 1.215821385383606
Epoch 1620, training loss: 0.06702002882957458 = 0.001279155840165913 + 0.01 * 6.574088096618652
Epoch 1620, val loss: 1.2169244289398193
Epoch 1630, training loss: 0.06683282554149628 = 0.0012693203752860427 + 0.01 * 6.556351184844971
Epoch 1630, val loss: 1.2178846597671509
Epoch 1640, training loss: 0.06696785241365433 = 0.0012595580192282796 + 0.01 * 6.570829391479492
Epoch 1640, val loss: 1.2191294431686401
Epoch 1650, training loss: 0.06709570437669754 = 0.0012501007877290249 + 0.01 * 6.584559917449951
Epoch 1650, val loss: 1.2201119661331177
Epoch 1660, training loss: 0.06679609417915344 = 0.0012407845351845026 + 0.01 * 6.555531024932861
Epoch 1660, val loss: 1.2212560176849365
Epoch 1670, training loss: 0.06732768565416336 = 0.0012317142682150006 + 0.01 * 6.609597206115723
Epoch 1670, val loss: 1.2222694158554077
Epoch 1680, training loss: 0.06676929444074631 = 0.0012227133847773075 + 0.01 * 6.55465841293335
Epoch 1680, val loss: 1.2232939004898071
Epoch 1690, training loss: 0.06709626317024231 = 0.0012139378814026713 + 0.01 * 6.58823299407959
Epoch 1690, val loss: 1.2243762016296387
Epoch 1700, training loss: 0.06661659479141235 = 0.0012053374666720629 + 0.01 * 6.541125774383545
Epoch 1700, val loss: 1.2253895998001099
Epoch 1710, training loss: 0.06661555916070938 = 0.0011968762846663594 + 0.01 * 6.541868209838867
Epoch 1710, val loss: 1.2264208793640137
Epoch 1720, training loss: 0.06662166118621826 = 0.0011885851854458451 + 0.01 * 6.543307781219482
Epoch 1720, val loss: 1.227353572845459
Epoch 1730, training loss: 0.06660941243171692 = 0.0011803576489910483 + 0.01 * 6.542905330657959
Epoch 1730, val loss: 1.2284358739852905
Epoch 1740, training loss: 0.06694480031728745 = 0.0011723545612767339 + 0.01 * 6.577244758605957
Epoch 1740, val loss: 1.2293626070022583
Epoch 1750, training loss: 0.06660337001085281 = 0.0011644621845334768 + 0.01 * 6.543890953063965
Epoch 1750, val loss: 1.2304704189300537
Epoch 1760, training loss: 0.06679587811231613 = 0.0011567467590793967 + 0.01 * 6.563913345336914
Epoch 1760, val loss: 1.2313417196273804
Epoch 1770, training loss: 0.06649810075759888 = 0.0011490964097902179 + 0.01 * 6.534900665283203
Epoch 1770, val loss: 1.2324806451797485
Epoch 1780, training loss: 0.0667019858956337 = 0.0011416118359193206 + 0.01 * 6.556036949157715
Epoch 1780, val loss: 1.2333738803863525
Epoch 1790, training loss: 0.06663645803928375 = 0.0011343007208779454 + 0.01 * 6.550215721130371
Epoch 1790, val loss: 1.2342913150787354
Epoch 1800, training loss: 0.06688638031482697 = 0.0011269981041550636 + 0.01 * 6.5759382247924805
Epoch 1800, val loss: 1.2353659868240356
Epoch 1810, training loss: 0.06647242605686188 = 0.0011199393775314093 + 0.01 * 6.535248279571533
Epoch 1810, val loss: 1.2362771034240723
Epoch 1820, training loss: 0.066383957862854 = 0.0011129614431411028 + 0.01 * 6.527099609375
Epoch 1820, val loss: 1.2372303009033203
Epoch 1830, training loss: 0.06622957438230515 = 0.0011061037657782435 + 0.01 * 6.5123467445373535
Epoch 1830, val loss: 1.2381179332733154
Epoch 1840, training loss: 0.06651698797941208 = 0.0010993201285600662 + 0.01 * 6.541767120361328
Epoch 1840, val loss: 1.2390851974487305
Epoch 1850, training loss: 0.06630818545818329 = 0.0010926362592726946 + 0.01 * 6.521554946899414
Epoch 1850, val loss: 1.2400437593460083
Epoch 1860, training loss: 0.0662972554564476 = 0.0010860721813514829 + 0.01 * 6.521118640899658
Epoch 1860, val loss: 1.2410075664520264
Epoch 1870, training loss: 0.06640878319740295 = 0.001079625217244029 + 0.01 * 6.532916069030762
Epoch 1870, val loss: 1.2418797016143799
Epoch 1880, training loss: 0.06625009328126907 = 0.0010732529917731881 + 0.01 * 6.517683506011963
Epoch 1880, val loss: 1.2429062128067017
Epoch 1890, training loss: 0.06651182472705841 = 0.0010670310584828258 + 0.01 * 6.544479846954346
Epoch 1890, val loss: 1.2437328100204468
Epoch 1900, training loss: 0.06634683907032013 = 0.00106080062687397 + 0.01 * 6.528604507446289
Epoch 1900, val loss: 1.2447443008422852
Epoch 1910, training loss: 0.06647004932165146 = 0.0010547564597800374 + 0.01 * 6.541529178619385
Epoch 1910, val loss: 1.2455945014953613
Epoch 1920, training loss: 0.06633303314447403 = 0.001048734993673861 + 0.01 * 6.528429985046387
Epoch 1920, val loss: 1.2465250492095947
Epoch 1930, training loss: 0.06616998463869095 = 0.0010428333189338446 + 0.01 * 6.5127153396606445
Epoch 1930, val loss: 1.2474439144134521
Epoch 1940, training loss: 0.06625493615865707 = 0.0010370220988988876 + 0.01 * 6.521791458129883
Epoch 1940, val loss: 1.2482731342315674
Epoch 1950, training loss: 0.06620049476623535 = 0.0010312822414562106 + 0.01 * 6.516921520233154
Epoch 1950, val loss: 1.2492402791976929
Epoch 1960, training loss: 0.06616826355457306 = 0.0010256037348881364 + 0.01 * 6.514266014099121
Epoch 1960, val loss: 1.250120997428894
Epoch 1970, training loss: 0.0660010501742363 = 0.0010200246470049024 + 0.01 * 6.498103141784668
Epoch 1970, val loss: 1.2509478330612183
Epoch 1980, training loss: 0.0662887766957283 = 0.0010145102860406041 + 0.01 * 6.527426719665527
Epoch 1980, val loss: 1.2518750429153442
Epoch 1990, training loss: 0.06617874652147293 = 0.0010090970899909735 + 0.01 * 6.516964912414551
Epoch 1990, val loss: 1.2527574300765991
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.838165524512388
The final CL Acc:0.82099, 0.01062, The final GNN Acc:0.83975, 0.00129
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11568])
remove edge: torch.Size([2, 9410])
updated graph: torch.Size([2, 10422])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0547852516174316 = 1.9688169956207275 + 0.01 * 8.596816062927246
Epoch 0, val loss: 1.9769517183303833
Epoch 10, training loss: 2.0436689853668213 = 1.9577020406723022 + 0.01 * 8.59670352935791
Epoch 10, val loss: 1.9652884006500244
Epoch 20, training loss: 2.0296623706817627 = 1.9436990022659302 + 0.01 * 8.59633731842041
Epoch 20, val loss: 1.950465440750122
Epoch 30, training loss: 2.0095672607421875 = 1.9236180782318115 + 0.01 * 8.594921112060547
Epoch 30, val loss: 1.929169774055481
Epoch 40, training loss: 1.979349970817566 = 1.8935000896453857 + 0.01 * 8.584988594055176
Epoch 40, val loss: 1.89754319190979
Epoch 50, training loss: 1.937256097793579 = 1.8519971370697021 + 0.01 * 8.525900840759277
Epoch 50, val loss: 1.8557910919189453
Epoch 60, training loss: 1.8921250104904175 = 1.8097455501556396 + 0.01 * 8.23794174194336
Epoch 60, val loss: 1.817987084388733
Epoch 70, training loss: 1.8632935285568237 = 1.7822368144989014 + 0.01 * 8.105666160583496
Epoch 70, val loss: 1.7964924573898315
Epoch 80, training loss: 1.830358862876892 = 1.7506470680236816 + 0.01 * 7.971177577972412
Epoch 80, val loss: 1.7685229778289795
Epoch 90, training loss: 1.7843453884124756 = 1.7072944641113281 + 0.01 * 7.7050909996032715
Epoch 90, val loss: 1.731895089149475
Epoch 100, training loss: 1.7210396528244019 = 1.646956205368042 + 0.01 * 7.408348083496094
Epoch 100, val loss: 1.6825093030929565
Epoch 110, training loss: 1.6404012441635132 = 1.5685685873031616 + 0.01 * 7.183263301849365
Epoch 110, val loss: 1.6197158098220825
Epoch 120, training loss: 1.5506263971328735 = 1.4791399240493774 + 0.01 * 7.14864444732666
Epoch 120, val loss: 1.5505244731903076
Epoch 130, training loss: 1.4594295024871826 = 1.3882462978363037 + 0.01 * 7.118326187133789
Epoch 130, val loss: 1.4807382822036743
Epoch 140, training loss: 1.369985580444336 = 1.2990134954452515 + 0.01 * 7.097213268280029
Epoch 140, val loss: 1.4123553037643433
Epoch 150, training loss: 1.28061842918396 = 1.209776759147644 + 0.01 * 7.084166049957275
Epoch 150, val loss: 1.344208836555481
Epoch 160, training loss: 1.192247986793518 = 1.1215157508850098 + 0.01 * 7.073222637176514
Epoch 160, val loss: 1.2775746583938599
Epoch 170, training loss: 1.1083929538726807 = 1.0377936363220215 + 0.01 * 7.059935092926025
Epoch 170, val loss: 1.2165563106536865
Epoch 180, training loss: 1.031341791152954 = 0.9609057903289795 + 0.01 * 7.043603897094727
Epoch 180, val loss: 1.162514090538025
Epoch 190, training loss: 0.9602630734443665 = 0.8900062441825867 + 0.01 * 7.0256853103637695
Epoch 190, val loss: 1.1141515970230103
Epoch 200, training loss: 0.8934383988380432 = 0.8233191967010498 + 0.01 * 7.011919021606445
Epoch 200, val loss: 1.069058895111084
Epoch 210, training loss: 0.8300358057022095 = 0.7600690722465515 + 0.01 * 6.9966721534729
Epoch 210, val loss: 1.0261940956115723
Epoch 220, training loss: 0.7705228924751282 = 0.7006903886795044 + 0.01 * 6.983249664306641
Epoch 220, val loss: 0.9860076904296875
Epoch 230, training loss: 0.7149469256401062 = 0.6452023983001709 + 0.01 * 6.974454879760742
Epoch 230, val loss: 0.9491022229194641
Epoch 240, training loss: 0.6620143055915833 = 0.5923952460289001 + 0.01 * 6.961904048919678
Epoch 240, val loss: 0.9151479005813599
Epoch 250, training loss: 0.6099764108657837 = 0.5404548048973083 + 0.01 * 6.952158451080322
Epoch 250, val loss: 0.883974552154541
Epoch 260, training loss: 0.5578985214233398 = 0.4884532392024994 + 0.01 * 6.944529056549072
Epoch 260, val loss: 0.8559462428092957
Epoch 270, training loss: 0.506095826625824 = 0.43669837713241577 + 0.01 * 6.939743518829346
Epoch 270, val loss: 0.832392692565918
Epoch 280, training loss: 0.4555768668651581 = 0.38622790575027466 + 0.01 * 6.9348955154418945
Epoch 280, val loss: 0.8136167526245117
Epoch 290, training loss: 0.40755513310432434 = 0.3382681608200073 + 0.01 * 6.92869758605957
Epoch 290, val loss: 0.7994317412376404
Epoch 300, training loss: 0.3633333444595337 = 0.2939465641975403 + 0.01 * 6.9386796951293945
Epoch 300, val loss: 0.789540708065033
Epoch 310, training loss: 0.32328417897224426 = 0.25403186678886414 + 0.01 * 6.925231456756592
Epoch 310, val loss: 0.783216655254364
Epoch 320, training loss: 0.2879161536693573 = 0.21872392296791077 + 0.01 * 6.919224262237549
Epoch 320, val loss: 0.7801581025123596
Epoch 330, training loss: 0.25710687041282654 = 0.1879347711801529 + 0.01 * 6.917210578918457
Epoch 330, val loss: 0.7798331379890442
Epoch 340, training loss: 0.23059144616127014 = 0.16143037378787994 + 0.01 * 6.916107177734375
Epoch 340, val loss: 0.7822117209434509
Epoch 350, training loss: 0.20800629258155823 = 0.13884449005126953 + 0.01 * 6.91618013381958
Epoch 350, val loss: 0.7870412468910217
Epoch 360, training loss: 0.18884597718715668 = 0.11973083764314651 + 0.01 * 6.9115142822265625
Epoch 360, val loss: 0.7939433455467224
Epoch 370, training loss: 0.17269344627857208 = 0.103602834045887 + 0.01 * 6.909061431884766
Epoch 370, val loss: 0.8025969862937927
Epoch 380, training loss: 0.15910184383392334 = 0.09000445157289505 + 0.01 * 6.909739971160889
Epoch 380, val loss: 0.812747061252594
Epoch 390, training loss: 0.14761808514595032 = 0.07852879166603088 + 0.01 * 6.908928394317627
Epoch 390, val loss: 0.8240993618965149
Epoch 400, training loss: 0.13787361979484558 = 0.06882823258638382 + 0.01 * 6.904539108276367
Epoch 400, val loss: 0.8363888263702393
Epoch 410, training loss: 0.1296379566192627 = 0.0606074184179306 + 0.01 * 6.903054714202881
Epoch 410, val loss: 0.8493483662605286
Epoch 420, training loss: 0.12265726923942566 = 0.053620871156454086 + 0.01 * 6.903640270233154
Epoch 420, val loss: 0.8627340197563171
Epoch 430, training loss: 0.11666423827409744 = 0.04766552895307541 + 0.01 * 6.899871349334717
Epoch 430, val loss: 0.8762869238853455
Epoch 440, training loss: 0.1115446612238884 = 0.042569197714328766 + 0.01 * 6.897546768188477
Epoch 440, val loss: 0.8898943662643433
Epoch 450, training loss: 0.10715330392122269 = 0.03818999230861664 + 0.01 * 6.896331310272217
Epoch 450, val loss: 0.9034386277198792
Epoch 460, training loss: 0.10335107892751694 = 0.034411609172821045 + 0.01 * 6.893947124481201
Epoch 460, val loss: 0.916802704334259
Epoch 470, training loss: 0.10008896887302399 = 0.03113815374672413 + 0.01 * 6.895081996917725
Epoch 470, val loss: 0.929932713508606
Epoch 480, training loss: 0.09718840569257736 = 0.028289049863815308 + 0.01 * 6.889935493469238
Epoch 480, val loss: 0.9427950382232666
Epoch 490, training loss: 0.09467650949954987 = 0.025798078626394272 + 0.01 * 6.887843132019043
Epoch 490, val loss: 0.9553694128990173
Epoch 500, training loss: 0.09247064590454102 = 0.0236075259745121 + 0.01 * 6.886312484741211
Epoch 500, val loss: 0.9675890803337097
Epoch 510, training loss: 0.0905337929725647 = 0.021678535267710686 + 0.01 * 6.885525703430176
Epoch 510, val loss: 0.9794650673866272
Epoch 520, training loss: 0.08880080282688141 = 0.019973402842879295 + 0.01 * 6.882740020751953
Epoch 520, val loss: 0.9909648895263672
Epoch 530, training loss: 0.0872693806886673 = 0.018460063263773918 + 0.01 * 6.880931854248047
Epoch 530, val loss: 1.002179741859436
Epoch 540, training loss: 0.08589179068803787 = 0.017110874876379967 + 0.01 * 6.878091335296631
Epoch 540, val loss: 1.0130209922790527
Epoch 550, training loss: 0.08465350419282913 = 0.015903720632195473 + 0.01 * 6.874979019165039
Epoch 550, val loss: 1.0235860347747803
Epoch 560, training loss: 0.08358767628669739 = 0.014819908887147903 + 0.01 * 6.876777172088623
Epoch 560, val loss: 1.0338250398635864
Epoch 570, training loss: 0.08255214244127274 = 0.013844513334333897 + 0.01 * 6.870763301849365
Epoch 570, val loss: 1.0437225103378296
Epoch 580, training loss: 0.08169594407081604 = 0.012963883578777313 + 0.01 * 6.87320613861084
Epoch 580, val loss: 1.0533537864685059
Epoch 590, training loss: 0.08083748072385788 = 0.01216717530041933 + 0.01 * 6.867030143737793
Epoch 590, val loss: 1.062650442123413
Epoch 600, training loss: 0.08009938150644302 = 0.011443705298006535 + 0.01 * 6.865567684173584
Epoch 600, val loss: 1.071644902229309
Epoch 610, training loss: 0.07943971455097198 = 0.010784689337015152 + 0.01 * 6.86550235748291
Epoch 610, val loss: 1.08039128780365
Epoch 620, training loss: 0.07877997308969498 = 0.010182474739849567 + 0.01 * 6.859749794006348
Epoch 620, val loss: 1.0888710021972656
Epoch 630, training loss: 0.07824189215898514 = 0.009631027467548847 + 0.01 * 6.861086368560791
Epoch 630, val loss: 1.0970957279205322
Epoch 640, training loss: 0.07778660953044891 = 0.0091250604018569 + 0.01 * 6.866154670715332
Epoch 640, val loss: 1.105070948600769
Epoch 650, training loss: 0.07722499966621399 = 0.00866005290299654 + 0.01 * 6.856494903564453
Epoch 650, val loss: 1.1128647327423096
Epoch 660, training loss: 0.07676607370376587 = 0.008231239393353462 + 0.01 * 6.8534836769104
Epoch 660, val loss: 1.1203832626342773
Epoch 670, training loss: 0.07631927728652954 = 0.007834944874048233 + 0.01 * 6.848433494567871
Epoch 670, val loss: 1.1276910305023193
Epoch 680, training loss: 0.07593150436878204 = 0.00746785756200552 + 0.01 * 6.846364974975586
Epoch 680, val loss: 1.1348226070404053
Epoch 690, training loss: 0.07570075243711472 = 0.007127690594643354 + 0.01 * 6.857306480407715
Epoch 690, val loss: 1.1417317390441895
Epoch 700, training loss: 0.07529861479997635 = 0.006813261192291975 + 0.01 * 6.848535537719727
Epoch 700, val loss: 1.148380994796753
Epoch 710, training loss: 0.07495841383934021 = 0.006521232891827822 + 0.01 * 6.8437180519104
Epoch 710, val loss: 1.1548656225204468
Epoch 720, training loss: 0.07463773339986801 = 0.006248893216252327 + 0.01 * 6.838884353637695
Epoch 720, val loss: 1.161142110824585
Epoch 730, training loss: 0.07434780150651932 = 0.005994597915560007 + 0.01 * 6.835320472717285
Epoch 730, val loss: 1.1672731637954712
Epoch 740, training loss: 0.07412249594926834 = 0.005756818223744631 + 0.01 * 6.8365678787231445
Epoch 740, val loss: 1.173243761062622
Epoch 750, training loss: 0.07393934577703476 = 0.005534464027732611 + 0.01 * 6.840488433837891
Epoch 750, val loss: 1.1790372133255005
Epoch 760, training loss: 0.07363966852426529 = 0.005326179787516594 + 0.01 * 6.831348896026611
Epoch 760, val loss: 1.1846706867218018
Epoch 770, training loss: 0.07344196736812592 = 0.00513082928955555 + 0.01 * 6.831113815307617
Epoch 770, val loss: 1.1901814937591553
Epoch 780, training loss: 0.07318650931119919 = 0.004947301931679249 + 0.01 * 6.823920249938965
Epoch 780, val loss: 1.1954773664474487
Epoch 790, training loss: 0.07300778478384018 = 0.004774673376232386 + 0.01 * 6.8233113288879395
Epoch 790, val loss: 1.200661063194275
Epoch 800, training loss: 0.07288611680269241 = 0.004611981101334095 + 0.01 * 6.827413558959961
Epoch 800, val loss: 1.2057441473007202
Epoch 810, training loss: 0.07264410704374313 = 0.004458527080714703 + 0.01 * 6.818558216094971
Epoch 810, val loss: 1.210637092590332
Epoch 820, training loss: 0.07251454144716263 = 0.004313909448683262 + 0.01 * 6.820063591003418
Epoch 820, val loss: 1.2154000997543335
Epoch 830, training loss: 0.07229563593864441 = 0.004177317954599857 + 0.01 * 6.811831951141357
Epoch 830, val loss: 1.2200393676757812
Epoch 840, training loss: 0.07227115333080292 = 0.004047992639243603 + 0.01 * 6.822316646575928
Epoch 840, val loss: 1.2245490550994873
Epoch 850, training loss: 0.07199978828430176 = 0.003925794269889593 + 0.01 * 6.807399749755859
Epoch 850, val loss: 1.2289373874664307
Epoch 860, training loss: 0.0718642994761467 = 0.0038099391385912895 + 0.01 * 6.805436134338379
Epoch 860, val loss: 1.2332128286361694
Epoch 870, training loss: 0.07179981470108032 = 0.003700016066431999 + 0.01 * 6.8099799156188965
Epoch 870, val loss: 1.2373380661010742
Epoch 880, training loss: 0.07168948650360107 = 0.0035955735947936773 + 0.01 * 6.809391498565674
Epoch 880, val loss: 1.2413408756256104
Epoch 890, training loss: 0.07151275128126144 = 0.003496505320072174 + 0.01 * 6.801624774932861
Epoch 890, val loss: 1.245284080505371
Epoch 900, training loss: 0.07138565182685852 = 0.0034021937754005194 + 0.01 * 6.798346042633057
Epoch 900, val loss: 1.2490904331207275
Epoch 910, training loss: 0.07124097645282745 = 0.0033124382607638836 + 0.01 * 6.792853832244873
Epoch 910, val loss: 1.2527896165847778
Epoch 920, training loss: 0.07112859934568405 = 0.003226930508390069 + 0.01 * 6.790167331695557
Epoch 920, val loss: 1.256417155265808
Epoch 930, training loss: 0.07137599587440491 = 0.0031455927528440952 + 0.01 * 6.823040962219238
Epoch 930, val loss: 1.2599778175354004
Epoch 940, training loss: 0.07099512219429016 = 0.003068247577175498 + 0.01 * 6.79268741607666
Epoch 940, val loss: 1.2633639574050903
Epoch 950, training loss: 0.07087691128253937 = 0.0029943762347102165 + 0.01 * 6.7882537841796875
Epoch 950, val loss: 1.2666983604431152
Epoch 960, training loss: 0.07083523273468018 = 0.0029236956033855677 + 0.01 * 6.791153907775879
Epoch 960, val loss: 1.2699044942855835
Epoch 970, training loss: 0.07065331190824509 = 0.0028561954386532307 + 0.01 * 6.779712200164795
Epoch 970, val loss: 1.2730990648269653
Epoch 980, training loss: 0.07070661336183548 = 0.0027915413957089186 + 0.01 * 6.791507244110107
Epoch 980, val loss: 1.2761554718017578
Epoch 990, training loss: 0.07044391334056854 = 0.0027298275381326675 + 0.01 * 6.771409034729004
Epoch 990, val loss: 1.2791048288345337
Epoch 1000, training loss: 0.07036883383989334 = 0.002670746762305498 + 0.01 * 6.769808292388916
Epoch 1000, val loss: 1.282002329826355
Epoch 1010, training loss: 0.0702911987900734 = 0.002614114200696349 + 0.01 * 6.767709255218506
Epoch 1010, val loss: 1.284775972366333
Epoch 1020, training loss: 0.07023118436336517 = 0.002559957094490528 + 0.01 * 6.767122268676758
Epoch 1020, val loss: 1.2875488996505737
Epoch 1030, training loss: 0.07064604014158249 = 0.0025079422630369663 + 0.01 * 6.813809871673584
Epoch 1030, val loss: 1.2902010679244995
Epoch 1040, training loss: 0.07017312198877335 = 0.002458222210407257 + 0.01 * 6.771490097045898
Epoch 1040, val loss: 1.2927112579345703
Epoch 1050, training loss: 0.06996677070856094 = 0.002410367364063859 + 0.01 * 6.755640506744385
Epoch 1050, val loss: 1.2952704429626465
Epoch 1060, training loss: 0.06991545855998993 = 0.0023644829634577036 + 0.01 * 6.755097389221191
Epoch 1060, val loss: 1.2975889444351196
Epoch 1070, training loss: 0.06993921101093292 = 0.002320332685485482 + 0.01 * 6.761888027191162
Epoch 1070, val loss: 1.3000094890594482
Epoch 1080, training loss: 0.06976157426834106 = 0.0022778790444135666 + 0.01 * 6.7483696937561035
Epoch 1080, val loss: 1.3022187948226929
Epoch 1090, training loss: 0.06957559287548065 = 0.002237029606476426 + 0.01 * 6.733856201171875
Epoch 1090, val loss: 1.3044524192810059
Epoch 1100, training loss: 0.06992331147193909 = 0.0021976171992719173 + 0.01 * 6.77256965637207
Epoch 1100, val loss: 1.306610345840454
Epoch 1110, training loss: 0.06956436485052109 = 0.0021599463652819395 + 0.01 * 6.740441799163818
Epoch 1110, val loss: 1.3086382150650024
Epoch 1120, training loss: 0.06931455433368683 = 0.0021235935855656862 + 0.01 * 6.7190961837768555
Epoch 1120, val loss: 1.3107209205627441
Epoch 1130, training loss: 0.06954235583543777 = 0.00208852905780077 + 0.01 * 6.745382308959961
Epoch 1130, val loss: 1.312727451324463
Epoch 1140, training loss: 0.06922903656959534 = 0.002054893411695957 + 0.01 * 6.717414855957031
Epoch 1140, val loss: 1.3146010637283325
Epoch 1150, training loss: 0.06928455084562302 = 0.002022272441536188 + 0.01 * 6.726227760314941
Epoch 1150, val loss: 1.316437840461731
Epoch 1160, training loss: 0.06915763765573502 = 0.0019909061957150698 + 0.01 * 6.716672897338867
Epoch 1160, val loss: 1.318214774131775
Epoch 1170, training loss: 0.0691070482134819 = 0.001960648922249675 + 0.01 * 6.714640140533447
Epoch 1170, val loss: 1.3199212551116943
Epoch 1180, training loss: 0.06899367272853851 = 0.0019313518423587084 + 0.01 * 6.706232070922852
Epoch 1180, val loss: 1.3216474056243896
Epoch 1190, training loss: 0.06908239424228668 = 0.0019030445255339146 + 0.01 * 6.717935085296631
Epoch 1190, val loss: 1.3232935667037964
Epoch 1200, training loss: 0.06884469836950302 = 0.0018757302314043045 + 0.01 * 6.696897506713867
Epoch 1200, val loss: 1.3248481750488281
Epoch 1210, training loss: 0.06903021782636642 = 0.001849300810135901 + 0.01 * 6.7180914878845215
Epoch 1210, val loss: 1.326354742050171
Epoch 1220, training loss: 0.06883647292852402 = 0.0018237550975754857 + 0.01 * 6.701272487640381
Epoch 1220, val loss: 1.3279283046722412
Epoch 1230, training loss: 0.06881224364042282 = 0.0017990474589169025 + 0.01 * 6.701320171356201
Epoch 1230, val loss: 1.3294254541397095
Epoch 1240, training loss: 0.06881043314933777 = 0.001774997217580676 + 0.01 * 6.7035441398620605
Epoch 1240, val loss: 1.3308054208755493
Epoch 1250, training loss: 0.06852541118860245 = 0.0017518283566460013 + 0.01 * 6.677358627319336
Epoch 1250, val loss: 1.3322426080703735
Epoch 1260, training loss: 0.06901516765356064 = 0.0017293434357270598 + 0.01 * 6.728582382202148
Epoch 1260, val loss: 1.333703875541687
Epoch 1270, training loss: 0.06850668787956238 = 0.0017075247596949339 + 0.01 * 6.679916858673096
Epoch 1270, val loss: 1.334990382194519
Epoch 1280, training loss: 0.06861599534749985 = 0.0016863849014043808 + 0.01 * 6.6929612159729
Epoch 1280, val loss: 1.3363795280456543
Epoch 1290, training loss: 0.0685519278049469 = 0.0016659105895087123 + 0.01 * 6.688601970672607
Epoch 1290, val loss: 1.3377375602722168
Epoch 1300, training loss: 0.06825253367424011 = 0.0016459048492833972 + 0.01 * 6.660662651062012
Epoch 1300, val loss: 1.3389748334884644
Epoch 1310, training loss: 0.06870698183774948 = 0.00162658526096493 + 0.01 * 6.708039283752441
Epoch 1310, val loss: 1.3403538465499878
Epoch 1320, training loss: 0.06818731129169464 = 0.0016078506596386433 + 0.01 * 6.6579461097717285
Epoch 1320, val loss: 1.3414791822433472
Epoch 1330, training loss: 0.06813643872737885 = 0.0015896111726760864 + 0.01 * 6.6546831130981445
Epoch 1330, val loss: 1.3427351713180542
Epoch 1340, training loss: 0.06819646805524826 = 0.0015718755312263966 + 0.01 * 6.662458896636963
Epoch 1340, val loss: 1.343950867652893
Epoch 1350, training loss: 0.06801596283912659 = 0.001554651651531458 + 0.01 * 6.6461310386657715
Epoch 1350, val loss: 1.3450275659561157
Epoch 1360, training loss: 0.06805650144815445 = 0.001537841628305614 + 0.01 * 6.6518659591674805
Epoch 1360, val loss: 1.346238374710083
Epoch 1370, training loss: 0.06799837201833725 = 0.001521601341664791 + 0.01 * 6.647676944732666
Epoch 1370, val loss: 1.3472975492477417
Epoch 1380, training loss: 0.0681668072938919 = 0.001505673280917108 + 0.01 * 6.666113376617432
Epoch 1380, val loss: 1.348458170890808
Epoch 1390, training loss: 0.068024642765522 = 0.001490198541432619 + 0.01 * 6.653444290161133
Epoch 1390, val loss: 1.3494707345962524
Epoch 1400, training loss: 0.0678657740354538 = 0.0014750855043530464 + 0.01 * 6.639068603515625
Epoch 1400, val loss: 1.350468635559082
Epoch 1410, training loss: 0.06772727519273758 = 0.001460454543121159 + 0.01 * 6.626681804656982
Epoch 1410, val loss: 1.351557731628418
Epoch 1420, training loss: 0.06804417818784714 = 0.0014461433747783303 + 0.01 * 6.65980339050293
Epoch 1420, val loss: 1.3525751829147339
Epoch 1430, training loss: 0.06792659312486649 = 0.0014323147479444742 + 0.01 * 6.649428367614746
Epoch 1430, val loss: 1.3535690307617188
Epoch 1440, training loss: 0.0678669661283493 = 0.0014186680782586336 + 0.01 * 6.644830226898193
Epoch 1440, val loss: 1.354557752609253
Epoch 1450, training loss: 0.06763812154531479 = 0.0014054703060537577 + 0.01 * 6.623265266418457
Epoch 1450, val loss: 1.3555353879928589
Epoch 1460, training loss: 0.06762342154979706 = 0.0013925540260970592 + 0.01 * 6.623086929321289
Epoch 1460, val loss: 1.3564808368682861
Epoch 1470, training loss: 0.06756152957677841 = 0.0013799418229609728 + 0.01 * 6.61815881729126
Epoch 1470, val loss: 1.3573881387710571
Epoch 1480, training loss: 0.06772224605083466 = 0.001367703778669238 + 0.01 * 6.635454177856445
Epoch 1480, val loss: 1.3583265542984009
Epoch 1490, training loss: 0.06747601181268692 = 0.0013557517668232322 + 0.01 * 6.612026214599609
Epoch 1490, val loss: 1.3591817617416382
Epoch 1500, training loss: 0.06753639131784439 = 0.0013440106995403767 + 0.01 * 6.619238376617432
Epoch 1500, val loss: 1.360126256942749
Epoch 1510, training loss: 0.06764314323663712 = 0.001332624931819737 + 0.01 * 6.631052017211914
Epoch 1510, val loss: 1.3608847856521606
Epoch 1520, training loss: 0.06746078282594681 = 0.0013215368380770087 + 0.01 * 6.613924503326416
Epoch 1520, val loss: 1.3618309497833252
Epoch 1530, training loss: 0.06746552884578705 = 0.0013106903061270714 + 0.01 * 6.61548376083374
Epoch 1530, val loss: 1.3627132177352905
Epoch 1540, training loss: 0.06790497153997421 = 0.0013000376056879759 + 0.01 * 6.660493850708008
Epoch 1540, val loss: 1.3635281324386597
Epoch 1550, training loss: 0.06722145527601242 = 0.0012896932894364 + 0.01 * 6.593175888061523
Epoch 1550, val loss: 1.3643338680267334
Epoch 1560, training loss: 0.06732399761676788 = 0.0012795088114216924 + 0.01 * 6.6044487953186035
Epoch 1560, val loss: 1.3652156591415405
Epoch 1570, training loss: 0.06724775582551956 = 0.0012696260819211602 + 0.01 * 6.597813129425049
Epoch 1570, val loss: 1.3659638166427612
Epoch 1580, training loss: 0.06724238395690918 = 0.0012598829343914986 + 0.01 * 6.598249912261963
Epoch 1580, val loss: 1.366869330406189
Epoch 1590, training loss: 0.06727876514196396 = 0.0012504576006904244 + 0.01 * 6.60283088684082
Epoch 1590, val loss: 1.3675912618637085
Epoch 1600, training loss: 0.06723936647176743 = 0.0012410955969244242 + 0.01 * 6.599827289581299
Epoch 1600, val loss: 1.3683111667633057
Epoch 1610, training loss: 0.0673673003911972 = 0.0012320305686444044 + 0.01 * 6.613527297973633
Epoch 1610, val loss: 1.3690985441207886
Epoch 1620, training loss: 0.06728892773389816 = 0.0012231436558067799 + 0.01 * 6.6065778732299805
Epoch 1620, val loss: 1.3697916269302368
Epoch 1630, training loss: 0.06735160946846008 = 0.0012144290376454592 + 0.01 * 6.613718032836914
Epoch 1630, val loss: 1.3705905675888062
Epoch 1640, training loss: 0.06723204255104065 = 0.0012059176806360483 + 0.01 * 6.602612495422363
Epoch 1640, val loss: 1.3712053298950195
Epoch 1650, training loss: 0.06702249497175217 = 0.0011975556844845414 + 0.01 * 6.582494258880615
Epoch 1650, val loss: 1.3719308376312256
Epoch 1660, training loss: 0.06697061657905579 = 0.0011893840273842216 + 0.01 * 6.578123569488525
Epoch 1660, val loss: 1.37264883518219
Epoch 1670, training loss: 0.067295141518116 = 0.0011814204044640064 + 0.01 * 6.611371994018555
Epoch 1670, val loss: 1.3732966184616089
Epoch 1680, training loss: 0.0668838694691658 = 0.001173495315015316 + 0.01 * 6.571037769317627
Epoch 1680, val loss: 1.3738698959350586
Epoch 1690, training loss: 0.06723003834486008 = 0.0011658229632303119 + 0.01 * 6.606421947479248
Epoch 1690, val loss: 1.3745485544204712
Epoch 1700, training loss: 0.06683467328548431 = 0.001158278901129961 + 0.01 * 6.567639350891113
Epoch 1700, val loss: 1.3751634359359741
Epoch 1710, training loss: 0.06704780459403992 = 0.001150852651335299 + 0.01 * 6.589694976806641
Epoch 1710, val loss: 1.3757519721984863
Epoch 1720, training loss: 0.06689303368330002 = 0.0011436290806159377 + 0.01 * 6.574941158294678
Epoch 1720, val loss: 1.3763707876205444
Epoch 1730, training loss: 0.06674472242593765 = 0.001136479782871902 + 0.01 * 6.560823917388916
Epoch 1730, val loss: 1.3770208358764648
Epoch 1740, training loss: 0.06675419956445694 = 0.0011295205913484097 + 0.01 * 6.5624680519104
Epoch 1740, val loss: 1.3775205612182617
Epoch 1750, training loss: 0.06671096384525299 = 0.0011226461501792073 + 0.01 * 6.558831691741943
Epoch 1750, val loss: 1.378174066543579
Epoch 1760, training loss: 0.0668022483587265 = 0.0011159246787428856 + 0.01 * 6.56863260269165
Epoch 1760, val loss: 1.3787057399749756
Epoch 1770, training loss: 0.06671851128339767 = 0.0011092625791206956 + 0.01 * 6.560924530029297
Epoch 1770, val loss: 1.3792210817337036
Epoch 1780, training loss: 0.06679022312164307 = 0.001102743437513709 + 0.01 * 6.5687479972839355
Epoch 1780, val loss: 1.3798296451568604
Epoch 1790, training loss: 0.06681942194700241 = 0.0010963057866320014 + 0.01 * 6.572311878204346
Epoch 1790, val loss: 1.3803517818450928
Epoch 1800, training loss: 0.0666409581899643 = 0.00108999980147928 + 0.01 * 6.55509614944458
Epoch 1800, val loss: 1.3808557987213135
Epoch 1810, training loss: 0.06664884090423584 = 0.0010838087182492018 + 0.01 * 6.5565032958984375
Epoch 1810, val loss: 1.3813916444778442
Epoch 1820, training loss: 0.06648425757884979 = 0.0010776815470308065 + 0.01 * 6.5406575202941895
Epoch 1820, val loss: 1.3818609714508057
Epoch 1830, training loss: 0.06666141003370285 = 0.0010716700926423073 + 0.01 * 6.558974266052246
Epoch 1830, val loss: 1.3823692798614502
Epoch 1840, training loss: 0.06657222658395767 = 0.001065848278813064 + 0.01 * 6.550637722015381
Epoch 1840, val loss: 1.382814645767212
Epoch 1850, training loss: 0.06658148020505905 = 0.0010599870001897216 + 0.01 * 6.552149295806885
Epoch 1850, val loss: 1.3832955360412598
Epoch 1860, training loss: 0.06675569713115692 = 0.0010543296812102199 + 0.01 * 6.570137023925781
Epoch 1860, val loss: 1.3837242126464844
Epoch 1870, training loss: 0.06647342443466187 = 0.0010486289393156767 + 0.01 * 6.542479038238525
Epoch 1870, val loss: 1.3841992616653442
Epoch 1880, training loss: 0.06675922870635986 = 0.0010431068949401379 + 0.01 * 6.57161283493042
Epoch 1880, val loss: 1.3846261501312256
Epoch 1890, training loss: 0.06646490097045898 = 0.0010376342106610537 + 0.01 * 6.542726993560791
Epoch 1890, val loss: 1.384997010231018
Epoch 1900, training loss: 0.0665990337729454 = 0.0010322738671675324 + 0.01 * 6.556676387786865
Epoch 1900, val loss: 1.385467767715454
Epoch 1910, training loss: 0.06625667214393616 = 0.0010269141057506204 + 0.01 * 6.522975921630859
Epoch 1910, val loss: 1.385831356048584
Epoch 1920, training loss: 0.066378653049469 = 0.0010216865921393037 + 0.01 * 6.535696983337402
Epoch 1920, val loss: 1.3862849473953247
Epoch 1930, training loss: 0.0664701983332634 = 0.00101657141931355 + 0.01 * 6.54536247253418
Epoch 1930, val loss: 1.3867229223251343
Epoch 1940, training loss: 0.06628084182739258 = 0.0010114432079717517 + 0.01 * 6.526939868927002
Epoch 1940, val loss: 1.3870378732681274
Epoch 1950, training loss: 0.0663452073931694 = 0.0010064347879961133 + 0.01 * 6.533877849578857
Epoch 1950, val loss: 1.387516975402832
Epoch 1960, training loss: 0.06625624746084213 = 0.0010014716535806656 + 0.01 * 6.525477886199951
Epoch 1960, val loss: 1.3878774642944336
Epoch 1970, training loss: 0.06625033915042877 = 0.000996571034193039 + 0.01 * 6.52537727355957
Epoch 1970, val loss: 1.388370394706726
Epoch 1980, training loss: 0.0662265345454216 = 0.0009917643619701266 + 0.01 * 6.523477554321289
Epoch 1980, val loss: 1.3887078762054443
Epoch 1990, training loss: 0.06620531529188156 = 0.0009869859786704183 + 0.01 * 6.521833419799805
Epoch 1990, val loss: 1.389145851135254
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 2.0467894077301025 = 1.9608210325241089 + 0.01 * 8.596845626831055
Epoch 0, val loss: 1.9575719833374023
Epoch 10, training loss: 2.036428213119507 = 1.950460433959961 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.9471533298492432
Epoch 20, training loss: 2.0238864421844482 = 1.937920331954956 + 0.01 * 8.596609115600586
Epoch 20, val loss: 1.9342436790466309
Epoch 30, training loss: 2.0065791606903076 = 1.9206181764602661 + 0.01 * 8.596088409423828
Epoch 30, val loss: 1.9160993099212646
Epoch 40, training loss: 1.981289029121399 = 1.8953595161437988 + 0.01 * 8.592954635620117
Epoch 40, val loss: 1.889513373374939
Epoch 50, training loss: 1.9453891515731812 = 1.859755277633667 + 0.01 * 8.563387870788574
Epoch 50, val loss: 1.8531177043914795
Epoch 60, training loss: 1.9018739461898804 = 1.817811131477356 + 0.01 * 8.406280517578125
Epoch 60, val loss: 1.8145313262939453
Epoch 70, training loss: 1.8629969358444214 = 1.78183114528656 + 0.01 * 8.116576194763184
Epoch 70, val loss: 1.7880489826202393
Epoch 80, training loss: 1.8244695663452148 = 1.7452213764190674 + 0.01 * 7.924816131591797
Epoch 80, val loss: 1.7614246606826782
Epoch 90, training loss: 1.769879698753357 = 1.6940921545028687 + 0.01 * 7.57875394821167
Epoch 90, val loss: 1.719672441482544
Epoch 100, training loss: 1.6970645189285278 = 1.623357892036438 + 0.01 * 7.370666027069092
Epoch 100, val loss: 1.6602028608322144
Epoch 110, training loss: 1.6041855812072754 = 1.5310986042022705 + 0.01 * 7.30869722366333
Epoch 110, val loss: 1.5852075815200806
Epoch 120, training loss: 1.4967641830444336 = 1.4242607355117798 + 0.01 * 7.250345230102539
Epoch 120, val loss: 1.5008875131607056
Epoch 130, training loss: 1.3840781450271606 = 1.3122625350952148 + 0.01 * 7.181556224822998
Epoch 130, val loss: 1.416025996208191
Epoch 140, training loss: 1.2714049816131592 = 1.1999844312667847 + 0.01 * 7.142057418823242
Epoch 140, val loss: 1.3327817916870117
Epoch 150, training loss: 1.1622520685195923 = 1.0909323692321777 + 0.01 * 7.131965637207031
Epoch 150, val loss: 1.2548730373382568
Epoch 160, training loss: 1.0603933334350586 = 0.9891073107719421 + 0.01 * 7.128605365753174
Epoch 160, val loss: 1.183986783027649
Epoch 170, training loss: 0.9680300951004028 = 0.896785318851471 + 0.01 * 7.124476432800293
Epoch 170, val loss: 1.1217175722122192
Epoch 180, training loss: 0.8859530687332153 = 0.8147835731506348 + 0.01 * 7.116950988769531
Epoch 180, val loss: 1.0681068897247314
Epoch 190, training loss: 0.8142471313476562 = 0.7432000637054443 + 0.01 * 7.104706764221191
Epoch 190, val loss: 1.0227997303009033
Epoch 200, training loss: 0.7522265315055847 = 0.6813323497772217 + 0.01 * 7.089419364929199
Epoch 200, val loss: 0.9855321645736694
Epoch 210, training loss: 0.6982863545417786 = 0.6275806427001953 + 0.01 * 7.0705718994140625
Epoch 210, val loss: 0.9554077982902527
Epoch 220, training loss: 0.650351881980896 = 0.5798200964927673 + 0.01 * 7.053177833557129
Epoch 220, val loss: 0.9311331510543823
Epoch 230, training loss: 0.6060886383056641 = 0.5356985926628113 + 0.01 * 7.039007663726807
Epoch 230, val loss: 0.9106578230857849
Epoch 240, training loss: 0.5634126663208008 = 0.4931764304637909 + 0.01 * 7.02362060546875
Epoch 240, val loss: 0.8922775983810425
Epoch 250, training loss: 0.5211153030395508 = 0.4509541392326355 + 0.01 * 7.016119003295898
Epoch 250, val loss: 0.8749435544013977
Epoch 260, training loss: 0.47890499234199524 = 0.4087895154953003 + 0.01 * 7.0115485191345215
Epoch 260, val loss: 0.8588275909423828
Epoch 270, training loss: 0.4373871386051178 = 0.3673039376735687 + 0.01 * 7.0083208084106445
Epoch 270, val loss: 0.8448917865753174
Epoch 280, training loss: 0.3975261449813843 = 0.3274795711040497 + 0.01 * 7.0046563148498535
Epoch 280, val loss: 0.833703339099884
Epoch 290, training loss: 0.3601025640964508 = 0.2900128960609436 + 0.01 * 7.008967876434326
Epoch 290, val loss: 0.825190544128418
Epoch 300, training loss: 0.3252435028553009 = 0.25523868203163147 + 0.01 * 7.000481605529785
Epoch 300, val loss: 0.8188597559928894
Epoch 310, training loss: 0.2932206690311432 = 0.22327134013175964 + 0.01 * 6.99493408203125
Epoch 310, val loss: 0.8144726157188416
Epoch 320, training loss: 0.26415616273880005 = 0.1942499876022339 + 0.01 * 6.990617752075195
Epoch 320, val loss: 0.8119950890541077
Epoch 330, training loss: 0.23826442658901215 = 0.16833776235580444 + 0.01 * 6.992666244506836
Epoch 330, val loss: 0.8114739656448364
Epoch 340, training loss: 0.21544547379016876 = 0.1455918699502945 + 0.01 * 6.985360622406006
Epoch 340, val loss: 0.8131430745124817
Epoch 350, training loss: 0.1956658661365509 = 0.1258852183818817 + 0.01 * 6.978064060211182
Epoch 350, val loss: 0.8169463276863098
Epoch 360, training loss: 0.17889553308486938 = 0.10896136611700058 + 0.01 * 6.9934163093566895
Epoch 360, val loss: 0.8227736949920654
Epoch 370, training loss: 0.16427981853485107 = 0.09452830255031586 + 0.01 * 6.975152492523193
Epoch 370, val loss: 0.8304116725921631
Epoch 380, training loss: 0.1518901288509369 = 0.08223911374807358 + 0.01 * 6.96510124206543
Epoch 380, val loss: 0.8394824862480164
Epoch 390, training loss: 0.14140208065509796 = 0.07178778201341629 + 0.01 * 6.961430072784424
Epoch 390, val loss: 0.8497165441513062
Epoch 400, training loss: 0.13242629170417786 = 0.06291069090366364 + 0.01 * 6.951559543609619
Epoch 400, val loss: 0.8608261942863464
Epoch 410, training loss: 0.12485675513744354 = 0.05537368729710579 + 0.01 * 6.948307514190674
Epoch 410, val loss: 0.8725679516792297
Epoch 420, training loss: 0.11856252700090408 = 0.04898203909397125 + 0.01 * 6.958049297332764
Epoch 420, val loss: 0.8846328854560852
Epoch 430, training loss: 0.11295194923877716 = 0.04353563115000725 + 0.01 * 6.941631317138672
Epoch 430, val loss: 0.8968775272369385
Epoch 440, training loss: 0.10825324803590775 = 0.03887665271759033 + 0.01 * 6.937659740447998
Epoch 440, val loss: 0.9092060327529907
Epoch 450, training loss: 0.10421699285507202 = 0.03487579524517059 + 0.01 * 6.934120178222656
Epoch 450, val loss: 0.9215087890625
Epoch 460, training loss: 0.1007399931550026 = 0.0314268097281456 + 0.01 * 6.931318759918213
Epoch 460, val loss: 0.9336591958999634
Epoch 470, training loss: 0.09787067770957947 = 0.028443047776818275 + 0.01 * 6.942763328552246
Epoch 470, val loss: 0.9456018805503845
Epoch 480, training loss: 0.09509402513504028 = 0.02585395611822605 + 0.01 * 6.924006938934326
Epoch 480, val loss: 0.9572743773460388
Epoch 490, training loss: 0.09280785918235779 = 0.023594532161951065 + 0.01 * 6.921333312988281
Epoch 490, val loss: 0.9686577916145325
Epoch 500, training loss: 0.09086005389690399 = 0.02161313220858574 + 0.01 * 6.924692153930664
Epoch 500, val loss: 0.9797396063804626
Epoch 510, training loss: 0.08900941163301468 = 0.01986881159245968 + 0.01 * 6.914059638977051
Epoch 510, val loss: 0.9905378818511963
Epoch 520, training loss: 0.08775974810123444 = 0.01832701452076435 + 0.01 * 6.943274021148682
Epoch 520, val loss: 1.0010684728622437
Epoch 530, training loss: 0.08604316413402557 = 0.016961928457021713 + 0.01 * 6.90812349319458
Epoch 530, val loss: 1.011239767074585
Epoch 540, training loss: 0.0848393365740776 = 0.015746114775538445 + 0.01 * 6.909322261810303
Epoch 540, val loss: 1.0211269855499268
Epoch 550, training loss: 0.0837818831205368 = 0.014657620340585709 + 0.01 * 6.912426471710205
Epoch 550, val loss: 1.0307523012161255
Epoch 560, training loss: 0.08265341073274612 = 0.013680403120815754 + 0.01 * 6.897300720214844
Epoch 560, val loss: 1.040108323097229
Epoch 570, training loss: 0.08188056945800781 = 0.01279926672577858 + 0.01 * 6.908130168914795
Epoch 570, val loss: 1.0492501258850098
Epoch 580, training loss: 0.08096544444561005 = 0.012002889066934586 + 0.01 * 6.896255016326904
Epoch 580, val loss: 1.0581134557724
Epoch 590, training loss: 0.08014176785945892 = 0.011279995553195477 + 0.01 * 6.8861775398254395
Epoch 590, val loss: 1.066801905632019
Epoch 600, training loss: 0.07957644760608673 = 0.010621575638651848 + 0.01 * 6.895487308502197
Epoch 600, val loss: 1.0752993822097778
Epoch 610, training loss: 0.07895330339670181 = 0.010021738708019257 + 0.01 * 6.8931565284729
Epoch 610, val loss: 1.083514928817749
Epoch 620, training loss: 0.07826340198516846 = 0.009473166428506374 + 0.01 * 6.879023551940918
Epoch 620, val loss: 1.0915688276290894
Epoch 630, training loss: 0.07771549373865128 = 0.008970164693892002 + 0.01 * 6.874533176422119
Epoch 630, val loss: 1.0994149446487427
Epoch 640, training loss: 0.07736017554998398 = 0.008507722988724709 + 0.01 * 6.8852458000183105
Epoch 640, val loss: 1.1070667505264282
Epoch 650, training loss: 0.07683386653661728 = 0.008082800544798374 + 0.01 * 6.8751068115234375
Epoch 650, val loss: 1.1144964694976807
Epoch 660, training loss: 0.07657424360513687 = 0.007691182196140289 + 0.01 * 6.888306140899658
Epoch 660, val loss: 1.1217681169509888
Epoch 670, training loss: 0.07590554654598236 = 0.007330589462071657 + 0.01 * 6.85749626159668
Epoch 670, val loss: 1.1288113594055176
Epoch 680, training loss: 0.0756160244345665 = 0.006996816489845514 + 0.01 * 6.8619208335876465
Epoch 680, val loss: 1.1356762647628784
Epoch 690, training loss: 0.0752677246928215 = 0.006687437184154987 + 0.01 * 6.858028888702393
Epoch 690, val loss: 1.1423671245574951
Epoch 700, training loss: 0.07485494017601013 = 0.0063993241637945175 + 0.01 * 6.845561504364014
Epoch 700, val loss: 1.1489216089248657
Epoch 710, training loss: 0.07450470328330994 = 0.006130776833742857 + 0.01 * 6.837392807006836
Epoch 710, val loss: 1.155320405960083
Epoch 720, training loss: 0.07446034252643585 = 0.005879661533981562 + 0.01 * 6.858067989349365
Epoch 720, val loss: 1.1616253852844238
Epoch 730, training loss: 0.07401812821626663 = 0.00564523646607995 + 0.01 * 6.837288856506348
Epoch 730, val loss: 1.1676783561706543
Epoch 740, training loss: 0.07370273023843765 = 0.005425433162599802 + 0.01 * 6.827730178833008
Epoch 740, val loss: 1.1737053394317627
Epoch 750, training loss: 0.07354088127613068 = 0.005219319369643927 + 0.01 * 6.832156658172607
Epoch 750, val loss: 1.1795315742492676
Epoch 760, training loss: 0.07330441474914551 = 0.005026106256991625 + 0.01 * 6.827830791473389
Epoch 760, val loss: 1.1852836608886719
Epoch 770, training loss: 0.07292303442955017 = 0.004844348877668381 + 0.01 * 6.807868003845215
Epoch 770, val loss: 1.190879464149475
Epoch 780, training loss: 0.07286032289266586 = 0.0046736146323382854 + 0.01 * 6.818670749664307
Epoch 780, val loss: 1.1963040828704834
Epoch 790, training loss: 0.0729961171746254 = 0.00451243482530117 + 0.01 * 6.848368167877197
Epoch 790, val loss: 1.2016416788101196
Epoch 800, training loss: 0.07238689064979553 = 0.004361276049166918 + 0.01 * 6.802561283111572
Epoch 800, val loss: 1.2068513631820679
Epoch 810, training loss: 0.07222606241703033 = 0.0042183976620435715 + 0.01 * 6.800766468048096
Epoch 810, val loss: 1.2119325399398804
Epoch 820, training loss: 0.07198918610811234 = 0.004083650652319193 + 0.01 * 6.790554046630859
Epoch 820, val loss: 1.216917634010315
Epoch 830, training loss: 0.0717713013291359 = 0.003956065978854895 + 0.01 * 6.781523704528809
Epoch 830, val loss: 1.2217587232589722
Epoch 840, training loss: 0.07182430475950241 = 0.0038351910188794136 + 0.01 * 6.7989115715026855
Epoch 840, val loss: 1.2265676259994507
Epoch 850, training loss: 0.07163119316101074 = 0.0037212322931736708 + 0.01 * 6.790996551513672
Epoch 850, val loss: 1.231140375137329
Epoch 860, training loss: 0.07141154259443283 = 0.003613218665122986 + 0.01 * 6.77983283996582
Epoch 860, val loss: 1.23572838306427
Epoch 870, training loss: 0.0714927464723587 = 0.0035108220763504505 + 0.01 * 6.798192024230957
Epoch 870, val loss: 1.2401098012924194
Epoch 880, training loss: 0.07120286673307419 = 0.003413706785067916 + 0.01 * 6.778915882110596
Epoch 880, val loss: 1.2444627285003662
Epoch 890, training loss: 0.0710238516330719 = 0.00332121760584414 + 0.01 * 6.770263671875
Epoch 890, val loss: 1.2487082481384277
Epoch 900, training loss: 0.07085619866847992 = 0.003233381314203143 + 0.01 * 6.762281894683838
Epoch 900, val loss: 1.252853274345398
Epoch 910, training loss: 0.07069429010152817 = 0.0031498216558247805 + 0.01 * 6.754446983337402
Epoch 910, val loss: 1.2568647861480713
Epoch 920, training loss: 0.07073383778333664 = 0.003069876227527857 + 0.01 * 6.766396522521973
Epoch 920, val loss: 1.2608871459960938
Epoch 930, training loss: 0.07061288505792618 = 0.0029941254761070013 + 0.01 * 6.761875629425049
Epoch 930, val loss: 1.2647324800491333
Epoch 940, training loss: 0.07044844329357147 = 0.0029218674171715975 + 0.01 * 6.752658367156982
Epoch 940, val loss: 1.2685158252716064
Epoch 950, training loss: 0.0705568939447403 = 0.0028527032118290663 + 0.01 * 6.770419120788574
Epoch 950, val loss: 1.2721824645996094
Epoch 960, training loss: 0.07035059481859207 = 0.0027867581229656935 + 0.01 * 6.756384372711182
Epoch 960, val loss: 1.2758285999298096
Epoch 970, training loss: 0.07061631977558136 = 0.002723485231399536 + 0.01 * 6.789283275604248
Epoch 970, val loss: 1.279435634613037
Epoch 980, training loss: 0.07013890147209167 = 0.002663208404555917 + 0.01 * 6.7475690841674805
Epoch 980, val loss: 1.2828493118286133
Epoch 990, training loss: 0.07002962380647659 = 0.0026053718756884336 + 0.01 * 6.742425441741943
Epoch 990, val loss: 1.2862403392791748
Epoch 1000, training loss: 0.07005103677511215 = 0.002549975411966443 + 0.01 * 6.750105857849121
Epoch 1000, val loss: 1.2895727157592773
Epoch 1010, training loss: 0.06995886564254761 = 0.00249694031663239 + 0.01 * 6.746192455291748
Epoch 1010, val loss: 1.2928510904312134
Epoch 1020, training loss: 0.06992609798908234 = 0.0024460211861878633 + 0.01 * 6.748007774353027
Epoch 1020, val loss: 1.2960063219070435
Epoch 1030, training loss: 0.0697636678814888 = 0.0023973442148417234 + 0.01 * 6.736632347106934
Epoch 1030, val loss: 1.2990871667861938
Epoch 1040, training loss: 0.06968700140714645 = 0.0023503857664763927 + 0.01 * 6.733662128448486
Epoch 1040, val loss: 1.3021414279937744
Epoch 1050, training loss: 0.06973467767238617 = 0.00230535794980824 + 0.01 * 6.742931842803955
Epoch 1050, val loss: 1.3051056861877441
Epoch 1060, training loss: 0.06957347691059113 = 0.0022621965035796165 + 0.01 * 6.731127738952637
Epoch 1060, val loss: 1.3079065084457397
Epoch 1070, training loss: 0.06949251890182495 = 0.0022205922286957502 + 0.01 * 6.7271928787231445
Epoch 1070, val loss: 1.3108211755752563
Epoch 1080, training loss: 0.0692737028002739 = 0.002180456882342696 + 0.01 * 6.709324836730957
Epoch 1080, val loss: 1.3135251998901367
Epoch 1090, training loss: 0.06940940022468567 = 0.0021418046671897173 + 0.01 * 6.726759910583496
Epoch 1090, val loss: 1.3163549900054932
Epoch 1100, training loss: 0.06947793811559677 = 0.0021047398913651705 + 0.01 * 6.737320423126221
Epoch 1100, val loss: 1.3189504146575928
Epoch 1110, training loss: 0.06915164738893509 = 0.0020691577810794115 + 0.01 * 6.708248615264893
Epoch 1110, val loss: 1.3214941024780273
Epoch 1120, training loss: 0.06908448785543442 = 0.0020347326062619686 + 0.01 * 6.704975605010986
Epoch 1120, val loss: 1.3240889310836792
Epoch 1130, training loss: 0.06951913237571716 = 0.002001648535951972 + 0.01 * 6.751748085021973
Epoch 1130, val loss: 1.3265440464019775
Epoch 1140, training loss: 0.06906253099441528 = 0.001969704870134592 + 0.01 * 6.709282875061035
Epoch 1140, val loss: 1.3288960456848145
Epoch 1150, training loss: 0.0688963383436203 = 0.001938865054398775 + 0.01 * 6.6957478523254395
Epoch 1150, val loss: 1.331297755241394
Epoch 1160, training loss: 0.06920664012432098 = 0.0019089981215074658 + 0.01 * 6.729764461517334
Epoch 1160, val loss: 1.3337091207504272
Epoch 1170, training loss: 0.06905258446931839 = 0.0018801431870087981 + 0.01 * 6.717243671417236
Epoch 1170, val loss: 1.33590829372406
Epoch 1180, training loss: 0.0691126361489296 = 0.001852255198173225 + 0.01 * 6.726038932800293
Epoch 1180, val loss: 1.3382607698440552
Epoch 1190, training loss: 0.068671815097332 = 0.0018253352027386427 + 0.01 * 6.684648036956787
Epoch 1190, val loss: 1.340423822402954
Epoch 1200, training loss: 0.06900091469287872 = 0.001799195189960301 + 0.01 * 6.720171928405762
Epoch 1200, val loss: 1.3426648378372192
Epoch 1210, training loss: 0.06882209330797195 = 0.0017739394679665565 + 0.01 * 6.70481538772583
Epoch 1210, val loss: 1.3447734117507935
Epoch 1220, training loss: 0.06853944808244705 = 0.0017495077336207032 + 0.01 * 6.678994178771973
Epoch 1220, val loss: 1.346895694732666
Epoch 1230, training loss: 0.06846373528242111 = 0.0017257756553590298 + 0.01 * 6.6737961769104
Epoch 1230, val loss: 1.3490283489227295
Epoch 1240, training loss: 0.06861039251089096 = 0.001702930429019034 + 0.01 * 6.690746784210205
Epoch 1240, val loss: 1.3509663343429565
Epoch 1250, training loss: 0.068657286465168 = 0.0016806231578812003 + 0.01 * 6.697667121887207
Epoch 1250, val loss: 1.3529877662658691
Epoch 1260, training loss: 0.06844462454319 = 0.0016591615276411176 + 0.01 * 6.67854642868042
Epoch 1260, val loss: 1.3549652099609375
Epoch 1270, training loss: 0.0689060166478157 = 0.001638389891013503 + 0.01 * 6.726762771606445
Epoch 1270, val loss: 1.3568156957626343
Epoch 1280, training loss: 0.06846470385789871 = 0.001618121168576181 + 0.01 * 6.684658050537109
Epoch 1280, val loss: 1.3586928844451904
Epoch 1290, training loss: 0.06862198561429977 = 0.0015985287027433515 + 0.01 * 6.702345848083496
Epoch 1290, val loss: 1.3604786396026611
Epoch 1300, training loss: 0.06814087927341461 = 0.0015795388026162982 + 0.01 * 6.656134605407715
Epoch 1300, val loss: 1.3623020648956299
Epoch 1310, training loss: 0.06844740360975266 = 0.0015610759146511555 + 0.01 * 6.688632965087891
Epoch 1310, val loss: 1.364158272743225
Epoch 1320, training loss: 0.06835798919200897 = 0.0015431555220857263 + 0.01 * 6.681483745574951
Epoch 1320, val loss: 1.3657946586608887
Epoch 1330, training loss: 0.06854123622179031 = 0.0015257850755006075 + 0.01 * 6.701545238494873
Epoch 1330, val loss: 1.3675376176834106
Epoch 1340, training loss: 0.06812509149312973 = 0.0015088740037754178 + 0.01 * 6.661621570587158
Epoch 1340, val loss: 1.3692289590835571
Epoch 1350, training loss: 0.0682520791888237 = 0.001492462120950222 + 0.01 * 6.675961494445801
Epoch 1350, val loss: 1.3707656860351562
Epoch 1360, training loss: 0.0679144561290741 = 0.0014764914521947503 + 0.01 * 6.643796920776367
Epoch 1360, val loss: 1.3724499940872192
Epoch 1370, training loss: 0.06797881424427032 = 0.0014609714271500707 + 0.01 * 6.651784420013428
Epoch 1370, val loss: 1.3740156888961792
Epoch 1380, training loss: 0.06796818971633911 = 0.0014459037920460105 + 0.01 * 6.652228355407715
Epoch 1380, val loss: 1.3755258321762085
Epoch 1390, training loss: 0.06807015836238861 = 0.0014311812119558454 + 0.01 * 6.66389799118042
Epoch 1390, val loss: 1.377139925956726
Epoch 1400, training loss: 0.06790544092655182 = 0.0014168714405968785 + 0.01 * 6.648857116699219
Epoch 1400, val loss: 1.3785673379898071
Epoch 1410, training loss: 0.06803453713655472 = 0.00140292732976377 + 0.01 * 6.663160800933838
Epoch 1410, val loss: 1.3799811601638794
Epoch 1420, training loss: 0.06760427355766296 = 0.001389307202771306 + 0.01 * 6.621496200561523
Epoch 1420, val loss: 1.381500482559204
Epoch 1430, training loss: 0.0681145191192627 = 0.001376081258058548 + 0.01 * 6.673843860626221
Epoch 1430, val loss: 1.3829288482666016
Epoch 1440, training loss: 0.06802476942539215 = 0.0013632623013108969 + 0.01 * 6.6661505699157715
Epoch 1440, val loss: 1.3842952251434326
Epoch 1450, training loss: 0.06765750795602798 = 0.0013507979456335306 + 0.01 * 6.630671501159668
Epoch 1450, val loss: 1.385573148727417
Epoch 1460, training loss: 0.06763048470020294 = 0.0013385552447289228 + 0.01 * 6.629193305969238
Epoch 1460, val loss: 1.3870410919189453
Epoch 1470, training loss: 0.06774662435054779 = 0.001326690660789609 + 0.01 * 6.641993999481201
Epoch 1470, val loss: 1.3882216215133667
Epoch 1480, training loss: 0.06757982820272446 = 0.0013150374870747328 + 0.01 * 6.626479625701904
Epoch 1480, val loss: 1.3895437717437744
Epoch 1490, training loss: 0.06766551733016968 = 0.00130375602748245 + 0.01 * 6.636176109313965
Epoch 1490, val loss: 1.390823483467102
Epoch 1500, training loss: 0.06739639490842819 = 0.0012926454655826092 + 0.01 * 6.61037540435791
Epoch 1500, val loss: 1.3919678926467896
Epoch 1510, training loss: 0.06749562174081802 = 0.001281830365769565 + 0.01 * 6.621379852294922
Epoch 1510, val loss: 1.393365502357483
Epoch 1520, training loss: 0.06771878898143768 = 0.0012712988536804914 + 0.01 * 6.644749164581299
Epoch 1520, val loss: 1.394465446472168
Epoch 1530, training loss: 0.06731100380420685 = 0.001261018100194633 + 0.01 * 6.604998588562012
Epoch 1530, val loss: 1.3955740928649902
Epoch 1540, training loss: 0.06743039935827255 = 0.0012508875224739313 + 0.01 * 6.6179518699646
Epoch 1540, val loss: 1.3968708515167236
Epoch 1550, training loss: 0.06751511245965958 = 0.0012410709168761969 + 0.01 * 6.627403736114502
Epoch 1550, val loss: 1.397826075553894
Epoch 1560, training loss: 0.0673123449087143 = 0.0012314378982409835 + 0.01 * 6.608090877532959
Epoch 1560, val loss: 1.3989900350570679
Epoch 1570, training loss: 0.06716164201498032 = 0.0012219497002661228 + 0.01 * 6.593969345092773
Epoch 1570, val loss: 1.4001749753952026
Epoch 1580, training loss: 0.06720878928899765 = 0.0012127467198297381 + 0.01 * 6.59960412979126
Epoch 1580, val loss: 1.4011561870574951
Epoch 1590, training loss: 0.06779016554355621 = 0.0012036910047754645 + 0.01 * 6.658647060394287
Epoch 1590, val loss: 1.4022529125213623
Epoch 1600, training loss: 0.06737703830003738 = 0.0011948078172281384 + 0.01 * 6.618223190307617
Epoch 1600, val loss: 1.4033970832824707
Epoch 1610, training loss: 0.0672241598367691 = 0.0011862301034852862 + 0.01 * 6.603793144226074
Epoch 1610, val loss: 1.404349684715271
Epoch 1620, training loss: 0.06709302216768265 = 0.0011777532054111362 + 0.01 * 6.591526985168457
Epoch 1620, val loss: 1.4053332805633545
Epoch 1630, training loss: 0.06738486140966415 = 0.0011694717686623335 + 0.01 * 6.621539115905762
Epoch 1630, val loss: 1.4062806367874146
Epoch 1640, training loss: 0.06706929951906204 = 0.0011613763635978103 + 0.01 * 6.590792655944824
Epoch 1640, val loss: 1.4072450399398804
Epoch 1650, training loss: 0.06734684109687805 = 0.0011534456862136722 + 0.01 * 6.619339466094971
Epoch 1650, val loss: 1.408191442489624
Epoch 1660, training loss: 0.06707195937633514 = 0.0011456479551270604 + 0.01 * 6.592631816864014
Epoch 1660, val loss: 1.4089845418930054
Epoch 1670, training loss: 0.06693518906831741 = 0.0011380533687770367 + 0.01 * 6.579713821411133
Epoch 1670, val loss: 1.4099534749984741
Epoch 1680, training loss: 0.06699392944574356 = 0.00113061279989779 + 0.01 * 6.586331844329834
Epoch 1680, val loss: 1.4107005596160889
Epoch 1690, training loss: 0.06690096855163574 = 0.0011233168188482523 + 0.01 * 6.577765464782715
Epoch 1690, val loss: 1.4116750955581665
Epoch 1700, training loss: 0.06713064759969711 = 0.0011161805596202612 + 0.01 * 6.601447105407715
Epoch 1700, val loss: 1.4123647212982178
Epoch 1710, training loss: 0.06712067127227783 = 0.0011091716587543488 + 0.01 * 6.601150035858154
Epoch 1710, val loss: 1.4132779836654663
Epoch 1720, training loss: 0.0670165866613388 = 0.0011023308616131544 + 0.01 * 6.591425895690918
Epoch 1720, val loss: 1.4140697717666626
Epoch 1730, training loss: 0.0669848695397377 = 0.001095563406124711 + 0.01 * 6.588930606842041
Epoch 1730, val loss: 1.4148448705673218
Epoch 1740, training loss: 0.06718513369560242 = 0.0010889299446716905 + 0.01 * 6.609620094299316
Epoch 1740, val loss: 1.4157065153121948
Epoch 1750, training loss: 0.06666634976863861 = 0.0010824298951774836 + 0.01 * 6.558392524719238
Epoch 1750, val loss: 1.416505217552185
Epoch 1760, training loss: 0.06670696288347244 = 0.0010760447476059198 + 0.01 * 6.563092231750488
Epoch 1760, val loss: 1.4172627925872803
Epoch 1770, training loss: 0.06717143952846527 = 0.0010698214173316956 + 0.01 * 6.610162258148193
Epoch 1770, val loss: 1.4179695844650269
Epoch 1780, training loss: 0.06654489785432816 = 0.0010636678198352456 + 0.01 * 6.548123359680176
Epoch 1780, val loss: 1.4187605381011963
Epoch 1790, training loss: 0.06675083190202713 = 0.0010576420463621616 + 0.01 * 6.569319725036621
Epoch 1790, val loss: 1.4194191694259644
Epoch 1800, training loss: 0.06650911271572113 = 0.0010517163900658488 + 0.01 * 6.545739650726318
Epoch 1800, val loss: 1.4202470779418945
Epoch 1810, training loss: 0.06655407696962357 = 0.0010459041222929955 + 0.01 * 6.550817012786865
Epoch 1810, val loss: 1.420962929725647
Epoch 1820, training loss: 0.06663034856319427 = 0.001040188828483224 + 0.01 * 6.559016227722168
Epoch 1820, val loss: 1.421608805656433
Epoch 1830, training loss: 0.06651755422353745 = 0.0010345812188461423 + 0.01 * 6.548297882080078
Epoch 1830, val loss: 1.422365427017212
Epoch 1840, training loss: 0.0672367662191391 = 0.001029048697091639 + 0.01 * 6.620771408081055
Epoch 1840, val loss: 1.423057198524475
Epoch 1850, training loss: 0.06688936799764633 = 0.0010236401576548815 + 0.01 * 6.586572647094727
Epoch 1850, val loss: 1.423745036125183
Epoch 1860, training loss: 0.06657438725233078 = 0.001018306938931346 + 0.01 * 6.555607795715332
Epoch 1860, val loss: 1.4243526458740234
Epoch 1870, training loss: 0.06630093604326248 = 0.001013038563542068 + 0.01 * 6.528789520263672
Epoch 1870, val loss: 1.4251070022583008
Epoch 1880, training loss: 0.06639419496059418 = 0.0010078290943056345 + 0.01 * 6.538636207580566
Epoch 1880, val loss: 1.4258419275283813
Epoch 1890, training loss: 0.06659089773893356 = 0.0010027254465967417 + 0.01 * 6.558817386627197
Epoch 1890, val loss: 1.426535725593567
Epoch 1900, training loss: 0.06650654226541519 = 0.0009976672008633614 + 0.01 * 6.550887107849121
Epoch 1900, val loss: 1.4272468090057373
Epoch 1910, training loss: 0.06666850298643112 = 0.0009927407372742891 + 0.01 * 6.567576885223389
Epoch 1910, val loss: 1.4278720617294312
Epoch 1920, training loss: 0.06630287319421768 = 0.0009878863347694278 + 0.01 * 6.531498908996582
Epoch 1920, val loss: 1.4285598993301392
Epoch 1930, training loss: 0.06647792458534241 = 0.0009830825729295611 + 0.01 * 6.5494842529296875
Epoch 1930, val loss: 1.4292148351669312
Epoch 1940, training loss: 0.06638895720243454 = 0.0009783483110368252 + 0.01 * 6.541060447692871
Epoch 1940, val loss: 1.4298630952835083
Epoch 1950, training loss: 0.06638059765100479 = 0.0009736818610690534 + 0.01 * 6.540692329406738
Epoch 1950, val loss: 1.430512547492981
Epoch 1960, training loss: 0.06616401672363281 = 0.000969066983088851 + 0.01 * 6.519495010375977
Epoch 1960, val loss: 1.4311269521713257
Epoch 1970, training loss: 0.06660416722297668 = 0.0009645006502978504 + 0.01 * 6.563966751098633
Epoch 1970, val loss: 1.4318771362304688
Epoch 1980, training loss: 0.06633924692869186 = 0.0009600511402823031 + 0.01 * 6.537919998168945
Epoch 1980, val loss: 1.4324206113815308
Epoch 1990, training loss: 0.06614401191473007 = 0.0009556502918712795 + 0.01 * 6.518836498260498
Epoch 1990, val loss: 1.4330956935882568
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 2.024996280670166 = 1.9390277862548828 + 0.01 * 8.596845626831055
Epoch 0, val loss: 1.9350382089614868
Epoch 10, training loss: 2.0147202014923096 = 1.9287526607513428 + 0.01 * 8.596757888793945
Epoch 10, val loss: 1.9249376058578491
Epoch 20, training loss: 2.0016160011291504 = 1.915650725364685 + 0.01 * 8.59653377532959
Epoch 20, val loss: 1.9115431308746338
Epoch 30, training loss: 1.9828238487243652 = 1.8968665599822998 + 0.01 * 8.595733642578125
Epoch 30, val loss: 1.8916927576065063
Epoch 40, training loss: 1.9553717374801636 = 1.8694666624069214 + 0.01 * 8.590507507324219
Epoch 40, val loss: 1.862802505493164
Epoch 50, training loss: 1.9200987815856934 = 1.834542155265808 + 0.01 * 8.555665016174316
Epoch 50, val loss: 1.828077793121338
Epoch 60, training loss: 1.8860645294189453 = 1.8022394180297852 + 0.01 * 8.382515907287598
Epoch 60, val loss: 1.8006534576416016
Epoch 70, training loss: 1.853975534439087 = 1.7731165885925293 + 0.01 * 8.085894584655762
Epoch 70, val loss: 1.778551697731018
Epoch 80, training loss: 1.8130658864974976 = 1.7335259914398193 + 0.01 * 7.9539875984191895
Epoch 80, val loss: 1.7460359334945679
Epoch 90, training loss: 1.755211591720581 = 1.6773442029953003 + 0.01 * 7.786735534667969
Epoch 90, val loss: 1.6983095407485962
Epoch 100, training loss: 1.6784785985946655 = 1.6022549867630005 + 0.01 * 7.622355937957764
Epoch 100, val loss: 1.636346459388733
Epoch 110, training loss: 1.591356873512268 = 1.516521692276001 + 0.01 * 7.483518600463867
Epoch 110, val loss: 1.5679805278778076
Epoch 120, training loss: 1.5036661624908447 = 1.4293104410171509 + 0.01 * 7.435569763183594
Epoch 120, val loss: 1.501153826713562
Epoch 130, training loss: 1.4162940979003906 = 1.342331886291504 + 0.01 * 7.396223068237305
Epoch 130, val loss: 1.4382808208465576
Epoch 140, training loss: 1.326176404953003 = 1.252886414527893 + 0.01 * 7.328997611999512
Epoch 140, val loss: 1.3756296634674072
Epoch 150, training loss: 1.2324590682983398 = 1.1598535776138306 + 0.01 * 7.260555267333984
Epoch 150, val loss: 1.3123048543930054
Epoch 160, training loss: 1.1382360458374023 = 1.0660622119903564 + 0.01 * 7.217379570007324
Epoch 160, val loss: 1.250348687171936
Epoch 170, training loss: 1.0472289323806763 = 0.9752143025398254 + 0.01 * 7.201458930969238
Epoch 170, val loss: 1.1925718784332275
Epoch 180, training loss: 0.961622953414917 = 0.8896557688713074 + 0.01 * 7.196719646453857
Epoch 180, val loss: 1.1399445533752441
Epoch 190, training loss: 0.8824664354324341 = 0.8105168342590332 + 0.01 * 7.194962501525879
Epoch 190, val loss: 1.094018816947937
Epoch 200, training loss: 0.810154914855957 = 0.7382286787033081 + 0.01 * 7.192625045776367
Epoch 200, val loss: 1.0558512210845947
Epoch 210, training loss: 0.7448360323905945 = 0.6729483604431152 + 0.01 * 7.188769340515137
Epoch 210, val loss: 1.0257227420806885
Epoch 220, training loss: 0.6863510608673096 = 0.6145004630088806 + 0.01 * 7.185059070587158
Epoch 220, val loss: 1.0034217834472656
Epoch 230, training loss: 0.6339854598045349 = 0.5622097849845886 + 0.01 * 7.177566051483154
Epoch 230, val loss: 0.9878881573677063
Epoch 240, training loss: 0.5865387916564941 = 0.5148478150367737 + 0.01 * 7.169094562530518
Epoch 240, val loss: 0.9777997136116028
Epoch 250, training loss: 0.5426281690597534 = 0.47102656960487366 + 0.01 * 7.160157680511475
Epoch 250, val loss: 0.9718931913375854
Epoch 260, training loss: 0.5010501146316528 = 0.4295720160007477 + 0.01 * 7.147807598114014
Epoch 260, val loss: 0.9691604971885681
Epoch 270, training loss: 0.4610739052295685 = 0.38966313004493713 + 0.01 * 7.141077995300293
Epoch 270, val loss: 0.9686530232429504
Epoch 280, training loss: 0.4223425090312958 = 0.35107120871543884 + 0.01 * 7.127129554748535
Epoch 280, val loss: 0.9699806571006775
Epoch 290, training loss: 0.3851666748523712 = 0.3140648901462555 + 0.01 * 7.1101789474487305
Epoch 290, val loss: 0.9734111428260803
Epoch 300, training loss: 0.3501233756542206 = 0.2791569232940674 + 0.01 * 7.096644878387451
Epoch 300, val loss: 0.9790850877761841
Epoch 310, training loss: 0.31764763593673706 = 0.24681596457958221 + 0.01 * 7.083167552947998
Epoch 310, val loss: 0.9871585369110107
Epoch 320, training loss: 0.28815799951553345 = 0.21736182272434235 + 0.01 * 7.079618453979492
Epoch 320, val loss: 0.9980505108833313
Epoch 330, training loss: 0.2616400718688965 = 0.1909252405166626 + 0.01 * 7.071482181549072
Epoch 330, val loss: 1.011735439300537
Epoch 340, training loss: 0.23801875114440918 = 0.167485311627388 + 0.01 * 7.053343772888184
Epoch 340, val loss: 1.0281188488006592
Epoch 350, training loss: 0.21732154488563538 = 0.14685848355293274 + 0.01 * 7.046305179595947
Epoch 350, val loss: 1.046793818473816
Epoch 360, training loss: 0.19920766353607178 = 0.12880641222000122 + 0.01 * 7.040125370025635
Epoch 360, val loss: 1.0673168897628784
Epoch 370, training loss: 0.18342605233192444 = 0.11306845396757126 + 0.01 * 7.035760402679443
Epoch 370, val loss: 1.0890415906906128
Epoch 380, training loss: 0.16969919204711914 = 0.09938647598028183 + 0.01 * 7.031270980834961
Epoch 380, val loss: 1.111451268196106
Epoch 390, training loss: 0.15773864090442657 = 0.0875234380364418 + 0.01 * 7.021520137786865
Epoch 390, val loss: 1.1341935396194458
Epoch 400, training loss: 0.14734742045402527 = 0.07725510746240616 + 0.01 * 7.0092315673828125
Epoch 400, val loss: 1.1569328308105469
Epoch 410, training loss: 0.1383971869945526 = 0.0683671087026596 + 0.01 * 7.0030083656311035
Epoch 410, val loss: 1.179360032081604
Epoch 420, training loss: 0.13068479299545288 = 0.06068428233265877 + 0.01 * 7.000051021575928
Epoch 420, val loss: 1.2013429403305054
Epoch 430, training loss: 0.1239214763045311 = 0.05403990298509598 + 0.01 * 6.988157272338867
Epoch 430, val loss: 1.222815990447998
Epoch 440, training loss: 0.1181379184126854 = 0.04829927533864975 + 0.01 * 6.983864784240723
Epoch 440, val loss: 1.2436575889587402
Epoch 450, training loss: 0.11307042837142944 = 0.043338071554899216 + 0.01 * 6.973236083984375
Epoch 450, val loss: 1.26385498046875
Epoch 460, training loss: 0.10875479876995087 = 0.03903663530945778 + 0.01 * 6.971816539764404
Epoch 460, val loss: 1.2834692001342773
Epoch 470, training loss: 0.10495267808437347 = 0.035298969596624374 + 0.01 * 6.965371608734131
Epoch 470, val loss: 1.3024475574493408
Epoch 480, training loss: 0.10159596800804138 = 0.03203989937901497 + 0.01 * 6.9556074142456055
Epoch 480, val loss: 1.3208168745040894
Epoch 490, training loss: 0.09883295744657516 = 0.02919006161391735 + 0.01 * 6.964289665222168
Epoch 490, val loss: 1.3386176824569702
Epoch 500, training loss: 0.09615666419267654 = 0.026692697778344154 + 0.01 * 6.946397304534912
Epoch 500, val loss: 1.355794906616211
Epoch 510, training loss: 0.09392111003398895 = 0.024494515731930733 + 0.01 * 6.942659854888916
Epoch 510, val loss: 1.3724377155303955
Epoch 520, training loss: 0.09188570082187653 = 0.02255202829837799 + 0.01 * 6.9333672523498535
Epoch 520, val loss: 1.3885647058486938
Epoch 530, training loss: 0.0901527851819992 = 0.020827824249863625 + 0.01 * 6.932496070861816
Epoch 530, val loss: 1.4042466878890991
Epoch 540, training loss: 0.08857668191194534 = 0.019292691722512245 + 0.01 * 6.928399085998535
Epoch 540, val loss: 1.4193273782730103
Epoch 550, training loss: 0.08721857517957687 = 0.017921196296811104 + 0.01 * 6.9297380447387695
Epoch 550, val loss: 1.4339622259140015
Epoch 560, training loss: 0.08577905595302582 = 0.01669291779398918 + 0.01 * 6.908614158630371
Epoch 560, val loss: 1.4481476545333862
Epoch 570, training loss: 0.08467051386833191 = 0.015588361769914627 + 0.01 * 6.908215045928955
Epoch 570, val loss: 1.461818814277649
Epoch 580, training loss: 0.08363401144742966 = 0.01459228340536356 + 0.01 * 6.904172897338867
Epoch 580, val loss: 1.4751259088516235
Epoch 590, training loss: 0.08264115452766418 = 0.013691525906324387 + 0.01 * 6.89496374130249
Epoch 590, val loss: 1.4879406690597534
Epoch 600, training loss: 0.08179251849651337 = 0.012873955070972443 + 0.01 * 6.891856670379639
Epoch 600, val loss: 1.5004074573516846
Epoch 610, training loss: 0.08102252334356308 = 0.012130826711654663 + 0.01 * 6.889170169830322
Epoch 610, val loss: 1.5124214887619019
Epoch 620, training loss: 0.08031696826219559 = 0.011453171260654926 + 0.01 * 6.886379718780518
Epoch 620, val loss: 1.5240837335586548
Epoch 630, training loss: 0.07956624031066895 = 0.01083415374159813 + 0.01 * 6.873208522796631
Epoch 630, val loss: 1.5353423357009888
Epoch 640, training loss: 0.07901336997747421 = 0.010266841389238834 + 0.01 * 6.874652862548828
Epoch 640, val loss: 1.5462795495986938
Epoch 650, training loss: 0.07871516048908234 = 0.009745561517775059 + 0.01 * 6.896960258483887
Epoch 650, val loss: 1.5569645166397095
Epoch 660, training loss: 0.07787308841943741 = 0.009266198612749577 + 0.01 * 6.860689163208008
Epoch 660, val loss: 1.5671855211257935
Epoch 670, training loss: 0.07748357206583023 = 0.008823652751743793 + 0.01 * 6.865991592407227
Epoch 670, val loss: 1.5770716667175293
Epoch 680, training loss: 0.07699406892061234 = 0.008414684794843197 + 0.01 * 6.857938766479492
Epoch 680, val loss: 1.5868247747421265
Epoch 690, training loss: 0.07669300585985184 = 0.008035489358007908 + 0.01 * 6.865752220153809
Epoch 690, val loss: 1.5961496829986572
Epoch 700, training loss: 0.076300248503685 = 0.007683577481657267 + 0.01 * 6.861667633056641
Epoch 700, val loss: 1.60532546043396
Epoch 710, training loss: 0.0758695900440216 = 0.007356593851000071 + 0.01 * 6.85129976272583
Epoch 710, val loss: 1.6141186952590942
Epoch 720, training loss: 0.07555841654539108 = 0.007051872555166483 + 0.01 * 6.8506550788879395
Epoch 720, val loss: 1.6227086782455444
Epoch 730, training loss: 0.07530529797077179 = 0.006767808459699154 + 0.01 * 6.8537492752075195
Epoch 730, val loss: 1.6311761140823364
Epoch 740, training loss: 0.07491452246904373 = 0.006502364296466112 + 0.01 * 6.84121561050415
Epoch 740, val loss: 1.6391832828521729
Epoch 750, training loss: 0.0745336040854454 = 0.006254118867218494 + 0.01 * 6.827948093414307
Epoch 750, val loss: 1.6471796035766602
Epoch 760, training loss: 0.07436448335647583 = 0.006021495442837477 + 0.01 * 6.834299564361572
Epoch 760, val loss: 1.654716968536377
Epoch 770, training loss: 0.07408131659030914 = 0.005803460255265236 + 0.01 * 6.827785968780518
Epoch 770, val loss: 1.6622852087020874
Epoch 780, training loss: 0.07368441671133041 = 0.005598553456366062 + 0.01 * 6.808586597442627
Epoch 780, val loss: 1.6695374250411987
Epoch 790, training loss: 0.07358608394861221 = 0.005405539181083441 + 0.01 * 6.818054676055908
Epoch 790, val loss: 1.6765203475952148
Epoch 800, training loss: 0.07355079054832458 = 0.00522396108135581 + 0.01 * 6.832683086395264
Epoch 800, val loss: 1.6835383176803589
Epoch 810, training loss: 0.0733814612030983 = 0.0050525483675301075 + 0.01 * 6.83289098739624
Epoch 810, val loss: 1.6900510787963867
Epoch 820, training loss: 0.07284552603960037 = 0.004891170654445887 + 0.01 * 6.795435428619385
Epoch 820, val loss: 1.696540117263794
Epoch 830, training loss: 0.07275311648845673 = 0.004738571122288704 + 0.01 * 6.801454544067383
Epoch 830, val loss: 1.7029255628585815
Epoch 840, training loss: 0.07266312837600708 = 0.004594102036207914 + 0.01 * 6.806902885437012
Epoch 840, val loss: 1.7089998722076416
Epoch 850, training loss: 0.07243572175502777 = 0.004457444883882999 + 0.01 * 6.797828197479248
Epoch 850, val loss: 1.7149627208709717
Epoch 860, training loss: 0.07248611748218536 = 0.004327517002820969 + 0.01 * 6.815860748291016
Epoch 860, val loss: 1.720740556716919
Epoch 870, training loss: 0.07205389440059662 = 0.004204464145004749 + 0.01 * 6.784942626953125
Epoch 870, val loss: 1.7264271974563599
Epoch 880, training loss: 0.071934774518013 = 0.004087558016180992 + 0.01 * 6.784721851348877
Epoch 880, val loss: 1.7320237159729004
Epoch 890, training loss: 0.07164285331964493 = 0.003976582549512386 + 0.01 * 6.766626834869385
Epoch 890, val loss: 1.7373462915420532
Epoch 900, training loss: 0.07152429968118668 = 0.0038708180654793978 + 0.01 * 6.765348434448242
Epoch 900, val loss: 1.7427266836166382
Epoch 910, training loss: 0.07153886556625366 = 0.003770089242607355 + 0.01 * 6.7768778800964355
Epoch 910, val loss: 1.7477864027023315
Epoch 920, training loss: 0.07158536463975906 = 0.0036738759372383356 + 0.01 * 6.791149139404297
Epoch 920, val loss: 1.7527602910995483
Epoch 930, training loss: 0.07107178866863251 = 0.0035823839716613293 + 0.01 * 6.748940944671631
Epoch 930, val loss: 1.7576709985733032
Epoch 940, training loss: 0.07120583206415176 = 0.003494922537356615 + 0.01 * 6.771090507507324
Epoch 940, val loss: 1.762518286705017
Epoch 950, training loss: 0.07079038769006729 = 0.003411375917494297 + 0.01 * 6.737901210784912
Epoch 950, val loss: 1.7670469284057617
Epoch 960, training loss: 0.07068266719579697 = 0.003331381594762206 + 0.01 * 6.735129356384277
Epoch 960, val loss: 1.7717704772949219
Epoch 970, training loss: 0.07097321003675461 = 0.0032548957969993353 + 0.01 * 6.771831035614014
Epoch 970, val loss: 1.776068925857544
Epoch 980, training loss: 0.07076520472764969 = 0.0031815890688449144 + 0.01 * 6.75836181640625
Epoch 980, val loss: 1.7803442478179932
Epoch 990, training loss: 0.07038772851228714 = 0.003111673519015312 + 0.01 * 6.72760534286499
Epoch 990, val loss: 1.7846739292144775
Epoch 1000, training loss: 0.07055448740720749 = 0.0030445053707808256 + 0.01 * 6.750998020172119
Epoch 1000, val loss: 1.788853406906128
Epoch 1010, training loss: 0.07062263041734695 = 0.0029800343327224255 + 0.01 * 6.764260292053223
Epoch 1010, val loss: 1.7928909063339233
Epoch 1020, training loss: 0.07031069695949554 = 0.002918113023042679 + 0.01 * 6.739259243011475
Epoch 1020, val loss: 1.7967543601989746
Epoch 1030, training loss: 0.07037019729614258 = 0.002858662512153387 + 0.01 * 6.751153945922852
Epoch 1030, val loss: 1.8007498979568481
Epoch 1040, training loss: 0.07016819715499878 = 0.0028015291318297386 + 0.01 * 6.736667156219482
Epoch 1040, val loss: 1.8043537139892578
Epoch 1050, training loss: 0.06992395222187042 = 0.002746605547145009 + 0.01 * 6.717735290527344
Epoch 1050, val loss: 1.808201551437378
Epoch 1060, training loss: 0.07002339512109756 = 0.002693679416552186 + 0.01 * 6.73297119140625
Epoch 1060, val loss: 1.811769723892212
Epoch 1070, training loss: 0.06996647268533707 = 0.0026427418924868107 + 0.01 * 6.732373237609863
Epoch 1070, val loss: 1.8153804540634155
Epoch 1080, training loss: 0.06976579874753952 = 0.002593717770650983 + 0.01 * 6.717207908630371
Epoch 1080, val loss: 1.8187131881713867
Epoch 1090, training loss: 0.069579117000103 = 0.002546511124819517 + 0.01 * 6.70326042175293
Epoch 1090, val loss: 1.8221380710601807
Epoch 1100, training loss: 0.06978157162666321 = 0.0025009752716869116 + 0.01 * 6.728059768676758
Epoch 1100, val loss: 1.8255198001861572
Epoch 1110, training loss: 0.06968165934085846 = 0.002456994028761983 + 0.01 * 6.722466468811035
Epoch 1110, val loss: 1.8285934925079346
Epoch 1120, training loss: 0.06934458017349243 = 0.0024144845083355904 + 0.01 * 6.693009376525879
Epoch 1120, val loss: 1.8318289518356323
Epoch 1130, training loss: 0.06936755776405334 = 0.002373589901253581 + 0.01 * 6.69939661026001
Epoch 1130, val loss: 1.8349635601043701
Epoch 1140, training loss: 0.0695255920290947 = 0.0023340019397437572 + 0.01 * 6.7191596031188965
Epoch 1140, val loss: 1.8378915786743164
Epoch 1150, training loss: 0.06914298236370087 = 0.0022958028130233288 + 0.01 * 6.684718608856201
Epoch 1150, val loss: 1.84079909324646
Epoch 1160, training loss: 0.06929616630077362 = 0.0022588996216654778 + 0.01 * 6.7037272453308105
Epoch 1160, val loss: 1.8437155485153198
Epoch 1170, training loss: 0.06922652572393417 = 0.0022231400944292545 + 0.01 * 6.700338840484619
Epoch 1170, val loss: 1.8463846445083618
Epoch 1180, training loss: 0.06911160051822662 = 0.0021886040922254324 + 0.01 * 6.692300319671631
Epoch 1180, val loss: 1.8491895198822021
Epoch 1190, training loss: 0.06897131353616714 = 0.0021550562232732773 + 0.01 * 6.681625843048096
Epoch 1190, val loss: 1.8517788648605347
Epoch 1200, training loss: 0.06906473636627197 = 0.0021227055694907904 + 0.01 * 6.694202899932861
Epoch 1200, val loss: 1.8543437719345093
Epoch 1210, training loss: 0.06886322051286697 = 0.0020912811160087585 + 0.01 * 6.677194118499756
Epoch 1210, val loss: 1.8568838834762573
Epoch 1220, training loss: 0.06908771395683289 = 0.0020608799532055855 + 0.01 * 6.702682971954346
Epoch 1220, val loss: 1.859468936920166
Epoch 1230, training loss: 0.06877555698156357 = 0.0020314110442996025 + 0.01 * 6.674415111541748
Epoch 1230, val loss: 1.8617401123046875
Epoch 1240, training loss: 0.06867406517267227 = 0.0020028208382427692 + 0.01 * 6.667124271392822
Epoch 1240, val loss: 1.8642234802246094
Epoch 1250, training loss: 0.06884796172380447 = 0.001975124469026923 + 0.01 * 6.687283992767334
Epoch 1250, val loss: 1.866471290588379
Epoch 1260, training loss: 0.06854912638664246 = 0.0019482431234791875 + 0.01 * 6.660088539123535
Epoch 1260, val loss: 1.868752121925354
Epoch 1270, training loss: 0.06839151680469513 = 0.00192215945571661 + 0.01 * 6.646935939788818
Epoch 1270, val loss: 1.8709412813186646
Epoch 1280, training loss: 0.06876545399427414 = 0.0018968115327879786 + 0.01 * 6.686864376068115
Epoch 1280, val loss: 1.873137354850769
Epoch 1290, training loss: 0.06840641051530838 = 0.0018722697859629989 + 0.01 * 6.653413772583008
Epoch 1290, val loss: 1.8751176595687866
Epoch 1300, training loss: 0.0684126764535904 = 0.0018483513267710805 + 0.01 * 6.656432151794434
Epoch 1300, val loss: 1.8773157596588135
Epoch 1310, training loss: 0.06871218979358673 = 0.001825199113227427 + 0.01 * 6.688699245452881
Epoch 1310, val loss: 1.8792070150375366
Epoch 1320, training loss: 0.06825660169124603 = 0.0018026453908532858 + 0.01 * 6.645395755767822
Epoch 1320, val loss: 1.8812488317489624
Epoch 1330, training loss: 0.06826022267341614 = 0.001780630787834525 + 0.01 * 6.6479597091674805
Epoch 1330, val loss: 1.8831297159194946
Epoch 1340, training loss: 0.06830815970897675 = 0.0017593613592907786 + 0.01 * 6.654880046844482
Epoch 1340, val loss: 1.8850617408752441
Epoch 1350, training loss: 0.06818485260009766 = 0.00173852255102247 + 0.01 * 6.644632816314697
Epoch 1350, val loss: 1.886920690536499
Epoch 1360, training loss: 0.06836175173521042 = 0.0017182988813146949 + 0.01 * 6.664345741271973
Epoch 1360, val loss: 1.8887653350830078
Epoch 1370, training loss: 0.06836157292127609 = 0.0016986496048048139 + 0.01 * 6.666292667388916
Epoch 1370, val loss: 1.8904300928115845
Epoch 1380, training loss: 0.06804562360048294 = 0.0016794676193967462 + 0.01 * 6.636616230010986
Epoch 1380, val loss: 1.8922332525253296
Epoch 1390, training loss: 0.0680457204580307 = 0.0016607906436547637 + 0.01 * 6.638493061065674
Epoch 1390, val loss: 1.8938848972320557
Epoch 1400, training loss: 0.06805963814258575 = 0.0016426570946350694 + 0.01 * 6.641697883605957
Epoch 1400, val loss: 1.895463228225708
Epoch 1410, training loss: 0.06800469011068344 = 0.001624926459044218 + 0.01 * 6.63797664642334
Epoch 1410, val loss: 1.89717698097229
Epoch 1420, training loss: 0.06799420714378357 = 0.001607708865776658 + 0.01 * 6.638649940490723
Epoch 1420, val loss: 1.8986124992370605
Epoch 1430, training loss: 0.0677666887640953 = 0.0015908447094261646 + 0.01 * 6.617584228515625
Epoch 1430, val loss: 1.900283932685852
Epoch 1440, training loss: 0.06775355339050293 = 0.0015744237462058663 + 0.01 * 6.617912769317627
Epoch 1440, val loss: 1.9017266035079956
Epoch 1450, training loss: 0.06775274127721786 = 0.0015584197826683521 + 0.01 * 6.619431972503662
Epoch 1450, val loss: 1.903158187866211
Epoch 1460, training loss: 0.06751442700624466 = 0.0015427953330799937 + 0.01 * 6.597163677215576
Epoch 1460, val loss: 1.9045977592468262
Epoch 1470, training loss: 0.06775403022766113 = 0.001527617801912129 + 0.01 * 6.622641563415527
Epoch 1470, val loss: 1.905991792678833
Epoch 1480, training loss: 0.06744834035634995 = 0.0015127083752304316 + 0.01 * 6.593563556671143
Epoch 1480, val loss: 1.9073450565338135
Epoch 1490, training loss: 0.06760060042142868 = 0.0014981144340708852 + 0.01 * 6.61024808883667
Epoch 1490, val loss: 1.9087038040161133
Epoch 1500, training loss: 0.0676947832107544 = 0.001483943429775536 + 0.01 * 6.621083736419678
Epoch 1500, val loss: 1.9099562168121338
Epoch 1510, training loss: 0.06765532493591309 = 0.0014700780156999826 + 0.01 * 6.618524551391602
Epoch 1510, val loss: 1.9112451076507568
Epoch 1520, training loss: 0.06745831668376923 = 0.0014565426390618086 + 0.01 * 6.600177764892578
Epoch 1520, val loss: 1.912469506263733
Epoch 1530, training loss: 0.06736873090267181 = 0.001443271292373538 + 0.01 * 6.592545986175537
Epoch 1530, val loss: 1.9137918949127197
Epoch 1540, training loss: 0.06747797876596451 = 0.0014303834177553654 + 0.01 * 6.604759693145752
Epoch 1540, val loss: 1.9149402379989624
Epoch 1550, training loss: 0.06737099587917328 = 0.001417643390595913 + 0.01 * 6.595335006713867
Epoch 1550, val loss: 1.9160847663879395
Epoch 1560, training loss: 0.06751766055822372 = 0.0014052329352125525 + 0.01 * 6.611242771148682
Epoch 1560, val loss: 1.917169451713562
Epoch 1570, training loss: 0.06753861904144287 = 0.0013931053690612316 + 0.01 * 6.614552021026611
Epoch 1570, val loss: 1.9183752536773682
Epoch 1580, training loss: 0.06728308647871017 = 0.0013811413664370775 + 0.01 * 6.5901947021484375
Epoch 1580, val loss: 1.9193702936172485
Epoch 1590, training loss: 0.06722943484783173 = 0.0013695693342015147 + 0.01 * 6.585987091064453
Epoch 1590, val loss: 1.9203011989593506
Epoch 1600, training loss: 0.0671754851937294 = 0.0013582028914242983 + 0.01 * 6.581727981567383
Epoch 1600, val loss: 1.9215060472488403
Epoch 1610, training loss: 0.06743531674146652 = 0.0013470498379319906 + 0.01 * 6.608826637268066
Epoch 1610, val loss: 1.9224457740783691
Epoch 1620, training loss: 0.06735892593860626 = 0.0013361104065552354 + 0.01 * 6.60228157043457
Epoch 1620, val loss: 1.9233059883117676
Epoch 1630, training loss: 0.06720441579818726 = 0.001325329882092774 + 0.01 * 6.587908744812012
Epoch 1630, val loss: 1.9243597984313965
Epoch 1640, training loss: 0.06708778440952301 = 0.0013148473808541894 + 0.01 * 6.577293395996094
Epoch 1640, val loss: 1.9252064228057861
Epoch 1650, training loss: 0.06713070720434189 = 0.0013045399682596326 + 0.01 * 6.582617282867432
Epoch 1650, val loss: 1.9261447191238403
Epoch 1660, training loss: 0.06722313165664673 = 0.0012944756308570504 + 0.01 * 6.592865467071533
Epoch 1660, val loss: 1.9270163774490356
Epoch 1670, training loss: 0.06702820956707001 = 0.0012845729943364859 + 0.01 * 6.574364185333252
Epoch 1670, val loss: 1.9277936220169067
Epoch 1680, training loss: 0.06698088347911835 = 0.0012748149456456304 + 0.01 * 6.5706071853637695
Epoch 1680, val loss: 1.928735375404358
Epoch 1690, training loss: 0.06737810373306274 = 0.0012653570156544447 + 0.01 * 6.611274719238281
Epoch 1690, val loss: 1.9294718503952026
Epoch 1700, training loss: 0.06709400564432144 = 0.0012559550814330578 + 0.01 * 6.583805561065674
Epoch 1700, val loss: 1.9302903413772583
Epoch 1710, training loss: 0.06708676367998123 = 0.001246797968633473 + 0.01 * 6.5839972496032715
Epoch 1710, val loss: 1.9310705661773682
Epoch 1720, training loss: 0.06719328463077545 = 0.001237907214090228 + 0.01 * 6.5955376625061035
Epoch 1720, val loss: 1.9317231178283691
Epoch 1730, training loss: 0.06687810271978378 = 0.0012290807208046317 + 0.01 * 6.564902305603027
Epoch 1730, val loss: 1.9325127601623535
Epoch 1740, training loss: 0.06704805046319962 = 0.0012204247759655118 + 0.01 * 6.582762241363525
Epoch 1740, val loss: 1.9331393241882324
Epoch 1750, training loss: 0.0672144740819931 = 0.0012119343737140298 + 0.01 * 6.600254058837891
Epoch 1750, val loss: 1.9338808059692383
Epoch 1760, training loss: 0.06692660599946976 = 0.0012036303523927927 + 0.01 * 6.572297096252441
Epoch 1760, val loss: 1.9344292879104614
Epoch 1770, training loss: 0.06694210320711136 = 0.0011953877983614802 + 0.01 * 6.574671745300293
Epoch 1770, val loss: 1.9352295398712158
Epoch 1780, training loss: 0.06685184687376022 = 0.0011874294141307473 + 0.01 * 6.566442012786865
Epoch 1780, val loss: 1.9356043338775635
Epoch 1790, training loss: 0.06687655299901962 = 0.0011794763850048184 + 0.01 * 6.569707870483398
Epoch 1790, val loss: 1.936437964439392
Epoch 1800, training loss: 0.0667898878455162 = 0.0011716706212610006 + 0.01 * 6.561821937561035
Epoch 1800, val loss: 1.9369183778762817
Epoch 1810, training loss: 0.0667385458946228 = 0.0011640459997579455 + 0.01 * 6.557449817657471
Epoch 1810, val loss: 1.9373652935028076
Epoch 1820, training loss: 0.06672099232673645 = 0.0011565376771613955 + 0.01 * 6.556445598602295
Epoch 1820, val loss: 1.9380089044570923
Epoch 1830, training loss: 0.06686728447675705 = 0.001149135292507708 + 0.01 * 6.571815490722656
Epoch 1830, val loss: 1.9383673667907715
Epoch 1840, training loss: 0.06664719432592392 = 0.0011418869253247976 + 0.01 * 6.550531387329102
Epoch 1840, val loss: 1.938847541809082
Epoch 1850, training loss: 0.06656137853860855 = 0.001134730177000165 + 0.01 * 6.542665004730225
Epoch 1850, val loss: 1.93935227394104
Epoch 1860, training loss: 0.06686101853847504 = 0.0011277039302513003 + 0.01 * 6.573331356048584
Epoch 1860, val loss: 1.93976891040802
Epoch 1870, training loss: 0.06677401065826416 = 0.001120791770517826 + 0.01 * 6.565321922302246
Epoch 1870, val loss: 1.9401534795761108
Epoch 1880, training loss: 0.06655243784189224 = 0.0011139502748847008 + 0.01 * 6.543848514556885
Epoch 1880, val loss: 1.9405626058578491
Epoch 1890, training loss: 0.06684010475873947 = 0.001107308897189796 + 0.01 * 6.573280334472656
Epoch 1890, val loss: 1.940908432006836
Epoch 1900, training loss: 0.06663405150175095 = 0.0011006823042407632 + 0.01 * 6.553337097167969
Epoch 1900, val loss: 1.9412875175476074
Epoch 1910, training loss: 0.06648784130811691 = 0.0010941922664642334 + 0.01 * 6.539365291595459
Epoch 1910, val loss: 1.9415502548217773
Epoch 1920, training loss: 0.06657682359218597 = 0.0010878366883844137 + 0.01 * 6.548898696899414
Epoch 1920, val loss: 1.9418816566467285
Epoch 1930, training loss: 0.06663436442613602 = 0.0010815324494615197 + 0.01 * 6.555283069610596
Epoch 1930, val loss: 1.9422506093978882
Epoch 1940, training loss: 0.06647691875696182 = 0.001075336360372603 + 0.01 * 6.540158748626709
Epoch 1940, val loss: 1.9425190687179565
Epoch 1950, training loss: 0.06638842076063156 = 0.0010692458599805832 + 0.01 * 6.531917095184326
Epoch 1950, val loss: 1.942795991897583
Epoch 1960, training loss: 0.06661183387041092 = 0.0010632651392370462 + 0.01 * 6.554856777191162
Epoch 1960, val loss: 1.9430508613586426
Epoch 1970, training loss: 0.06644682586193085 = 0.001057279878295958 + 0.01 * 6.538954257965088
Epoch 1970, val loss: 1.943368911743164
Epoch 1980, training loss: 0.06634612381458282 = 0.0010514101013541222 + 0.01 * 6.529471397399902
Epoch 1980, val loss: 1.9435890913009644
Epoch 1990, training loss: 0.06665174663066864 = 0.00104572216514498 + 0.01 * 6.560603141784668
Epoch 1990, val loss: 1.9437999725341797
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8212967843964154
The final CL Acc:0.77160, 0.03492, The final GNN Acc:0.81884, 0.00217
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13258])
remove edge: torch.Size([2, 7956])
updated graph: torch.Size([2, 10658])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0260417461395264 = 1.9400736093521118 + 0.01 * 8.596822738647461
Epoch 0, val loss: 1.9236332178115845
Epoch 10, training loss: 2.0155439376831055 = 1.9295769929885864 + 0.01 * 8.596701622009277
Epoch 10, val loss: 1.913483738899231
Epoch 20, training loss: 2.0023138523101807 = 1.9163511991500854 + 0.01 * 8.596272468566895
Epoch 20, val loss: 1.9003883600234985
Epoch 30, training loss: 1.9837521314620972 = 1.8978081941604614 + 0.01 * 8.594393730163574
Epoch 30, val loss: 1.881748080253601
Epoch 40, training loss: 1.9564725160598755 = 1.8706700801849365 + 0.01 * 8.580242156982422
Epoch 40, val loss: 1.8547061681747437
Epoch 50, training loss: 1.9188798666000366 = 1.8338959217071533 + 0.01 * 8.498398780822754
Epoch 50, val loss: 1.8201147317886353
Epoch 60, training loss: 1.8765877485275269 = 1.795257806777954 + 0.01 * 8.132996559143066
Epoch 60, val loss: 1.7885817289352417
Epoch 70, training loss: 1.838878870010376 = 1.759377360343933 + 0.01 * 7.950156211853027
Epoch 70, val loss: 1.7610695362091064
Epoch 80, training loss: 1.7891807556152344 = 1.7116986513137817 + 0.01 * 7.748215675354004
Epoch 80, val loss: 1.7187358140945435
Epoch 90, training loss: 1.7213876247406006 = 1.646085500717163 + 0.01 * 7.53021764755249
Epoch 90, val loss: 1.6585127115249634
Epoch 100, training loss: 1.635872721672058 = 1.5627031326293945 + 0.01 * 7.316953659057617
Epoch 100, val loss: 1.585923194885254
Epoch 110, training loss: 1.5455509424209595 = 1.473351001739502 + 0.01 * 7.219998836517334
Epoch 110, val loss: 1.5110586881637573
Epoch 120, training loss: 1.458748698234558 = 1.3868898153305054 + 0.01 * 7.185893535614014
Epoch 120, val loss: 1.4418389797210693
Epoch 130, training loss: 1.3745089769363403 = 1.302969217300415 + 0.01 * 7.153972148895264
Epoch 130, val loss: 1.3759678602218628
Epoch 140, training loss: 1.289463758468628 = 1.2180272340774536 + 0.01 * 7.143653869628906
Epoch 140, val loss: 1.3089416027069092
Epoch 150, training loss: 1.2015166282653809 = 1.1300967931747437 + 0.01 * 7.141979217529297
Epoch 150, val loss: 1.239322304725647
Epoch 160, training loss: 1.1110066175460815 = 1.039583444595337 + 0.01 * 7.14231538772583
Epoch 160, val loss: 1.1670024394989014
Epoch 170, training loss: 1.019949197769165 = 0.948523998260498 + 0.01 * 7.142516613006592
Epoch 170, val loss: 1.0936912298202515
Epoch 180, training loss: 0.9319775700569153 = 0.8605774641036987 + 0.01 * 7.14000940322876
Epoch 180, val loss: 1.0226037502288818
Epoch 190, training loss: 0.8508427143096924 = 0.779506266117096 + 0.01 * 7.133641719818115
Epoch 190, val loss: 0.9578242301940918
Epoch 200, training loss: 0.7783190011978149 = 0.7070882320404053 + 0.01 * 7.123079776763916
Epoch 200, val loss: 0.902252733707428
Epoch 210, training loss: 0.7137699723243713 = 0.6426824331283569 + 0.01 * 7.108756065368652
Epoch 210, val loss: 0.8565164804458618
Epoch 220, training loss: 0.6553056240081787 = 0.5844140648841858 + 0.01 * 7.08915901184082
Epoch 220, val loss: 0.8192110061645508
Epoch 230, training loss: 0.6012142300605774 = 0.5305739045143127 + 0.01 * 7.064032554626465
Epoch 230, val loss: 0.7882641553878784
Epoch 240, training loss: 0.550385057926178 = 0.48003387451171875 + 0.01 * 7.035120010375977
Epoch 240, val loss: 0.7620748281478882
Epoch 250, training loss: 0.5022158026695251 = 0.43206146359443665 + 0.01 * 7.015435695648193
Epoch 250, val loss: 0.7395187020301819
Epoch 260, training loss: 0.45595434308052063 = 0.38615649938583374 + 0.01 * 6.979783535003662
Epoch 260, val loss: 0.7199787497520447
Epoch 270, training loss: 0.4117417633533478 = 0.34209969639778137 + 0.01 * 6.964206218719482
Epoch 270, val loss: 0.7034576535224915
Epoch 280, training loss: 0.369587242603302 = 0.30010509490966797 + 0.01 * 6.94821310043335
Epoch 280, val loss: 0.6895450353622437
Epoch 290, training loss: 0.3299024701118469 = 0.26059490442276 + 0.01 * 6.930755615234375
Epoch 290, val loss: 0.6780000329017639
Epoch 300, training loss: 0.29350417852401733 = 0.22434639930725098 + 0.01 * 6.915776252746582
Epoch 300, val loss: 0.6689357757568359
Epoch 310, training loss: 0.2611443102359772 = 0.19209399819374084 + 0.01 * 6.905030250549316
Epoch 310, val loss: 0.6626143455505371
Epoch 320, training loss: 0.23333433270454407 = 0.1641913652420044 + 0.01 * 6.914296627044678
Epoch 320, val loss: 0.6594630479812622
Epoch 330, training loss: 0.20956450700759888 = 0.14060531556606293 + 0.01 * 6.895918846130371
Epoch 330, val loss: 0.6594601273536682
Epoch 340, training loss: 0.1897818148136139 = 0.12086737900972366 + 0.01 * 6.891444206237793
Epoch 340, val loss: 0.6625012159347534
Epoch 350, training loss: 0.17323316633701324 = 0.10441271960735321 + 0.01 * 6.882044792175293
Epoch 350, val loss: 0.6680931448936462
Epoch 360, training loss: 0.15941813588142395 = 0.09066309034824371 + 0.01 * 6.875505447387695
Epoch 360, val loss: 0.6758304834365845
Epoch 370, training loss: 0.1478380709886551 = 0.0791291818022728 + 0.01 * 6.870888710021973
Epoch 370, val loss: 0.6852887272834778
Epoch 380, training loss: 0.13806065917015076 = 0.06940992921590805 + 0.01 * 6.865072727203369
Epoch 380, val loss: 0.6959265470504761
Epoch 390, training loss: 0.1297607272863388 = 0.06117889657616615 + 0.01 * 6.85818338394165
Epoch 390, val loss: 0.7074753046035767
Epoch 400, training loss: 0.12265091389417648 = 0.05417512357234955 + 0.01 * 6.847579002380371
Epoch 400, val loss: 0.7195694446563721
Epoch 410, training loss: 0.11698369681835175 = 0.04818900302052498 + 0.01 * 6.87946891784668
Epoch 410, val loss: 0.7320061922073364
Epoch 420, training loss: 0.11154371500015259 = 0.043068889528512955 + 0.01 * 6.847483158111572
Epoch 420, val loss: 0.744478702545166
Epoch 430, training loss: 0.10698553919792175 = 0.03866179287433624 + 0.01 * 6.8323750495910645
Epoch 430, val loss: 0.7569246888160706
Epoch 440, training loss: 0.10331708937883377 = 0.03485056757926941 + 0.01 * 6.846652030944824
Epoch 440, val loss: 0.7692648768424988
Epoch 450, training loss: 0.09973694384098053 = 0.03154832497239113 + 0.01 * 6.818862438201904
Epoch 450, val loss: 0.7813273668289185
Epoch 460, training loss: 0.09682588279247284 = 0.028672179207205772 + 0.01 * 6.815370559692383
Epoch 460, val loss: 0.7931777834892273
Epoch 470, training loss: 0.09424187988042831 = 0.02615802176296711 + 0.01 * 6.808385848999023
Epoch 470, val loss: 0.8047279715538025
Epoch 480, training loss: 0.09213060140609741 = 0.023951206356287003 + 0.01 * 6.817939281463623
Epoch 480, val loss: 0.8159499168395996
Epoch 490, training loss: 0.09011849761009216 = 0.022010434418916702 + 0.01 * 6.810805797576904
Epoch 490, val loss: 0.8268593549728394
Epoch 500, training loss: 0.08829805999994278 = 0.020294619724154472 + 0.01 * 6.800343990325928
Epoch 500, val loss: 0.8373956680297852
Epoch 510, training loss: 0.08666376769542694 = 0.018770435824990273 + 0.01 * 6.789333343505859
Epoch 510, val loss: 0.8476611375808716
Epoch 520, training loss: 0.08536902070045471 = 0.01741069182753563 + 0.01 * 6.795833587646484
Epoch 520, val loss: 0.8576267957687378
Epoch 530, training loss: 0.08404890447854996 = 0.016195064410567284 + 0.01 * 6.785384178161621
Epoch 530, val loss: 0.8672627210617065
Epoch 540, training loss: 0.08314816653728485 = 0.015106442384421825 + 0.01 * 6.804172515869141
Epoch 540, val loss: 0.8765681385993958
Epoch 550, training loss: 0.08197803050279617 = 0.014129254035651684 + 0.01 * 6.784878253936768
Epoch 550, val loss: 0.8855375647544861
Epoch 560, training loss: 0.08090299367904663 = 0.013246067799627781 + 0.01 * 6.765692710876465
Epoch 560, val loss: 0.8942477107048035
Epoch 570, training loss: 0.08012676239013672 = 0.012444693595170975 + 0.01 * 6.76820707321167
Epoch 570, val loss: 0.9026947617530823
Epoch 580, training loss: 0.07938110828399658 = 0.011716265231370926 + 0.01 * 6.76648473739624
Epoch 580, val loss: 0.9108933806419373
Epoch 590, training loss: 0.07875408232212067 = 0.01105277705937624 + 0.01 * 6.770130634307861
Epoch 590, val loss: 0.9188619256019592
Epoch 600, training loss: 0.078087218105793 = 0.010446564294397831 + 0.01 * 6.764065742492676
Epoch 600, val loss: 0.9265161156654358
Epoch 610, training loss: 0.07742737233638763 = 0.009891548193991184 + 0.01 * 6.753582954406738
Epoch 610, val loss: 0.9340181350708008
Epoch 620, training loss: 0.07693228870630264 = 0.009381497278809547 + 0.01 * 6.755079746246338
Epoch 620, val loss: 0.941256046295166
Epoch 630, training loss: 0.07645444571971893 = 0.008911916986107826 + 0.01 * 6.754252910614014
Epoch 630, val loss: 0.9482861757278442
Epoch 640, training loss: 0.07595517486333847 = 0.008479775860905647 + 0.01 * 6.747540473937988
Epoch 640, val loss: 0.9551202654838562
Epoch 650, training loss: 0.0755673348903656 = 0.008081046864390373 + 0.01 * 6.748628616333008
Epoch 650, val loss: 0.9616735577583313
Epoch 660, training loss: 0.07517407089471817 = 0.007711428217589855 + 0.01 * 6.746264457702637
Epoch 660, val loss: 0.9681291580200195
Epoch 670, training loss: 0.07471542805433273 = 0.0073682754300534725 + 0.01 * 6.734715461730957
Epoch 670, val loss: 0.9743589162826538
Epoch 680, training loss: 0.07441640645265579 = 0.00704911258071661 + 0.01 * 6.736729145050049
Epoch 680, val loss: 0.9804443120956421
Epoch 690, training loss: 0.07426443696022034 = 0.0067521375603973866 + 0.01 * 6.751229763031006
Epoch 690, val loss: 0.9863826036453247
Epoch 700, training loss: 0.07376348972320557 = 0.006475739646703005 + 0.01 * 6.7287750244140625
Epoch 700, val loss: 0.9921071529388428
Epoch 710, training loss: 0.07349394261837006 = 0.006217566784471273 + 0.01 * 6.727637767791748
Epoch 710, val loss: 0.9976783990859985
Epoch 720, training loss: 0.07325746864080429 = 0.005976011976599693 + 0.01 * 6.728145599365234
Epoch 720, val loss: 1.0031332969665527
Epoch 730, training loss: 0.07304945588111877 = 0.005749819800257683 + 0.01 * 6.729963779449463
Epoch 730, val loss: 1.008460521697998
Epoch 740, training loss: 0.0727926641702652 = 0.005537643097341061 + 0.01 * 6.725502014160156
Epoch 740, val loss: 1.0135771036148071
Epoch 750, training loss: 0.07261821627616882 = 0.0053384071215987206 + 0.01 * 6.727981090545654
Epoch 750, val loss: 1.0186084508895874
Epoch 760, training loss: 0.07227860391139984 = 0.005150952842086554 + 0.01 * 6.712765693664551
Epoch 760, val loss: 1.0235105752944946
Epoch 770, training loss: 0.07252299040555954 = 0.004974656272679567 + 0.01 * 6.754833698272705
Epoch 770, val loss: 1.0282667875289917
Epoch 780, training loss: 0.07195574790239334 = 0.00480887945741415 + 0.01 * 6.714686870574951
Epoch 780, val loss: 1.0329381227493286
Epoch 790, training loss: 0.07174015045166016 = 0.004652346484363079 + 0.01 * 6.708780765533447
Epoch 790, val loss: 1.037428855895996
Epoch 800, training loss: 0.07150834053754807 = 0.004504302982240915 + 0.01 * 6.700404167175293
Epoch 800, val loss: 1.0418498516082764
Epoch 810, training loss: 0.07153689116239548 = 0.004364193417131901 + 0.01 * 6.7172698974609375
Epoch 810, val loss: 1.0461711883544922
Epoch 820, training loss: 0.07127410173416138 = 0.004231578670442104 + 0.01 * 6.70425271987915
Epoch 820, val loss: 1.0503754615783691
Epoch 830, training loss: 0.07118745148181915 = 0.004106004256755114 + 0.01 * 6.708144664764404
Epoch 830, val loss: 1.0544846057891846
Epoch 840, training loss: 0.07100772112607956 = 0.003986875060945749 + 0.01 * 6.702085018157959
Epoch 840, val loss: 1.0585265159606934
Epoch 850, training loss: 0.07092083245515823 = 0.0038737524300813675 + 0.01 * 6.704708576202393
Epoch 850, val loss: 1.062443494796753
Epoch 860, training loss: 0.0708322748541832 = 0.0037663334514945745 + 0.01 * 6.706593990325928
Epoch 860, val loss: 1.0662693977355957
Epoch 870, training loss: 0.07056617736816406 = 0.0036642581690102816 + 0.01 * 6.690192222595215
Epoch 870, val loss: 1.069974660873413
Epoch 880, training loss: 0.07062481343746185 = 0.0035669791977852583 + 0.01 * 6.705783367156982
Epoch 880, val loss: 1.0736291408538818
Epoch 890, training loss: 0.07029269635677338 = 0.0034744590520858765 + 0.01 * 6.68182373046875
Epoch 890, val loss: 1.0771989822387695
Epoch 900, training loss: 0.07042904943227768 = 0.003386054653674364 + 0.01 * 6.704298973083496
Epoch 900, val loss: 1.080661416053772
Epoch 910, training loss: 0.07012852281332016 = 0.0033019252587109804 + 0.01 * 6.682660102844238
Epoch 910, val loss: 1.0841366052627563
Epoch 920, training loss: 0.06998587399721146 = 0.0032214263919740915 + 0.01 * 6.6764445304870605
Epoch 920, val loss: 1.0873870849609375
Epoch 930, training loss: 0.06988227367401123 = 0.0031445056665688753 + 0.01 * 6.6737775802612305
Epoch 930, val loss: 1.0906940698623657
Epoch 940, training loss: 0.06984372437000275 = 0.003071071347221732 + 0.01 * 6.677265644073486
Epoch 940, val loss: 1.0938557386398315
Epoch 950, training loss: 0.06991183012723923 = 0.003000544151291251 + 0.01 * 6.691129207611084
Epoch 950, val loss: 1.0969780683517456
Epoch 960, training loss: 0.06969349086284637 = 0.002933299634605646 + 0.01 * 6.676019191741943
Epoch 960, val loss: 1.1000280380249023
Epoch 970, training loss: 0.06968549638986588 = 0.0028686756268143654 + 0.01 * 6.681682109832764
Epoch 970, val loss: 1.1030131578445435
Epoch 980, training loss: 0.0695287436246872 = 0.002807024633511901 + 0.01 * 6.6721720695495605
Epoch 980, val loss: 1.105965495109558
Epoch 990, training loss: 0.06965462118387222 = 0.0027476937975734472 + 0.01 * 6.690692901611328
Epoch 990, val loss: 1.1087520122528076
Epoch 1000, training loss: 0.06930551677942276 = 0.002690844004973769 + 0.01 * 6.661467552185059
Epoch 1000, val loss: 1.1116067171096802
Epoch 1010, training loss: 0.06919293850660324 = 0.002636257791891694 + 0.01 * 6.655667781829834
Epoch 1010, val loss: 1.1143232583999634
Epoch 1020, training loss: 0.06915757805109024 = 0.0025837605353444815 + 0.01 * 6.657382011413574
Epoch 1020, val loss: 1.1170228719711304
Epoch 1030, training loss: 0.06910189986228943 = 0.0025332034565508366 + 0.01 * 6.656869411468506
Epoch 1030, val loss: 1.11968994140625
Epoch 1040, training loss: 0.06905021518468857 = 0.0024845267180353403 + 0.01 * 6.65656852722168
Epoch 1040, val loss: 1.1221988201141357
Epoch 1050, training loss: 0.06924155354499817 = 0.002437833696603775 + 0.01 * 6.6803717613220215
Epoch 1050, val loss: 1.124807357788086
Epoch 1060, training loss: 0.06878209859132767 = 0.0023927842266857624 + 0.01 * 6.6389312744140625
Epoch 1060, val loss: 1.1272506713867188
Epoch 1070, training loss: 0.0692267045378685 = 0.0023494332563132048 + 0.01 * 6.687726974487305
Epoch 1070, val loss: 1.1296719312667847
Epoch 1080, training loss: 0.06866104155778885 = 0.0023077421355992556 + 0.01 * 6.635329723358154
Epoch 1080, val loss: 1.132039189338684
Epoch 1090, training loss: 0.06867969781160355 = 0.002267551841214299 + 0.01 * 6.641214847564697
Epoch 1090, val loss: 1.1343014240264893
Epoch 1100, training loss: 0.06857303529977798 = 0.002228727098554373 + 0.01 * 6.634430885314941
Epoch 1100, val loss: 1.136641502380371
Epoch 1110, training loss: 0.0684753879904747 = 0.0021911656949669123 + 0.01 * 6.628422737121582
Epoch 1110, val loss: 1.138867735862732
Epoch 1120, training loss: 0.06879226118326187 = 0.0021549935918301344 + 0.01 * 6.663726806640625
Epoch 1120, val loss: 1.1410928964614868
Epoch 1130, training loss: 0.06851950287818909 = 0.0021199367474764585 + 0.01 * 6.639956951141357
Epoch 1130, val loss: 1.143211841583252
Epoch 1140, training loss: 0.06838429719209671 = 0.002086114604026079 + 0.01 * 6.629817962646484
Epoch 1140, val loss: 1.1453289985656738
Epoch 1150, training loss: 0.0684933140873909 = 0.002053419826552272 + 0.01 * 6.6439900398254395
Epoch 1150, val loss: 1.1474323272705078
Epoch 1160, training loss: 0.06814517080783844 = 0.0020218126010149717 + 0.01 * 6.612335681915283
Epoch 1160, val loss: 1.1494677066802979
Epoch 1170, training loss: 0.06842562556266785 = 0.0019911485724151134 + 0.01 * 6.643448352813721
Epoch 1170, val loss: 1.1514254808425903
Epoch 1180, training loss: 0.06820668280124664 = 0.0019615229684859514 + 0.01 * 6.624516487121582
Epoch 1180, val loss: 1.153399109840393
Epoch 1190, training loss: 0.06813816726207733 = 0.0019328438211232424 + 0.01 * 6.620532512664795
Epoch 1190, val loss: 1.1553497314453125
Epoch 1200, training loss: 0.0684802457690239 = 0.001905127544887364 + 0.01 * 6.657512187957764
Epoch 1200, val loss: 1.1572015285491943
Epoch 1210, training loss: 0.0680994838476181 = 0.0018783228006213903 + 0.01 * 6.622116565704346
Epoch 1210, val loss: 1.1590797901153564
Epoch 1220, training loss: 0.0680311843752861 = 0.00185235810931772 + 0.01 * 6.617883205413818
Epoch 1220, val loss: 1.1608784198760986
Epoch 1230, training loss: 0.06804457306861877 = 0.0018270405707880855 + 0.01 * 6.621753692626953
Epoch 1230, val loss: 1.16262686252594
Epoch 1240, training loss: 0.06790560483932495 = 0.001802508719265461 + 0.01 * 6.610309600830078
Epoch 1240, val loss: 1.1644428968429565
Epoch 1250, training loss: 0.06764321029186249 = 0.0017786999233067036 + 0.01 * 6.586451053619385
Epoch 1250, val loss: 1.1661639213562012
Epoch 1260, training loss: 0.06784746050834656 = 0.001755514065735042 + 0.01 * 6.609194755554199
Epoch 1260, val loss: 1.1678565740585327
Epoch 1270, training loss: 0.06779148429632187 = 0.0017331107519567013 + 0.01 * 6.605837821960449
Epoch 1270, val loss: 1.1695488691329956
Epoch 1280, training loss: 0.06765510141849518 = 0.0017112575005739927 + 0.01 * 6.594384670257568
Epoch 1280, val loss: 1.171201467514038
Epoch 1290, training loss: 0.068050317466259 = 0.0016901345225051045 + 0.01 * 6.6360182762146
Epoch 1290, val loss: 1.1727657318115234
Epoch 1300, training loss: 0.06758245080709457 = 0.0016695949016138911 + 0.01 * 6.5912861824035645
Epoch 1300, val loss: 1.1744043827056885
Epoch 1310, training loss: 0.06766030192375183 = 0.0016496399184688926 + 0.01 * 6.6010661125183105
Epoch 1310, val loss: 1.175923228263855
Epoch 1320, training loss: 0.06740296632051468 = 0.001630187383852899 + 0.01 * 6.577278137207031
Epoch 1320, val loss: 1.1774520874023438
Epoch 1330, training loss: 0.06736171990633011 = 0.0016112974844872952 + 0.01 * 6.575042247772217
Epoch 1330, val loss: 1.1789519786834717
Epoch 1340, training loss: 0.06736648082733154 = 0.0015929387882351875 + 0.01 * 6.577354431152344
Epoch 1340, val loss: 1.1804428100585938
Epoch 1350, training loss: 0.06739737093448639 = 0.0015749692684039474 + 0.01 * 6.582240104675293
Epoch 1350, val loss: 1.1818937063217163
Epoch 1360, training loss: 0.06751451641321182 = 0.0015575505094602704 + 0.01 * 6.595696449279785
Epoch 1360, val loss: 1.1833338737487793
Epoch 1370, training loss: 0.06732470542192459 = 0.001540555851534009 + 0.01 * 6.578415393829346
Epoch 1370, val loss: 1.1847659349441528
Epoch 1380, training loss: 0.06732400506734848 = 0.0015240113716572523 + 0.01 * 6.579999923706055
Epoch 1380, val loss: 1.1861447095870972
Epoch 1390, training loss: 0.067510686814785 = 0.0015079049626365304 + 0.01 * 6.600277900695801
Epoch 1390, val loss: 1.1874916553497314
Epoch 1400, training loss: 0.06719465553760529 = 0.001492191106081009 + 0.01 * 6.57024621963501
Epoch 1400, val loss: 1.1888459920883179
Epoch 1410, training loss: 0.06726190447807312 = 0.0014769198605790734 + 0.01 * 6.578498363494873
Epoch 1410, val loss: 1.1901423931121826
Epoch 1420, training loss: 0.06712856143712997 = 0.0014619353460147977 + 0.01 * 6.566662788391113
Epoch 1420, val loss: 1.1914764642715454
Epoch 1430, training loss: 0.06749427318572998 = 0.001447365852072835 + 0.01 * 6.604691028594971
Epoch 1430, val loss: 1.1927539110183716
Epoch 1440, training loss: 0.0671948790550232 = 0.0014331755228340626 + 0.01 * 6.576170444488525
Epoch 1440, val loss: 1.1940268278121948
Epoch 1450, training loss: 0.06714644283056259 = 0.0014193503884598613 + 0.01 * 6.572709560394287
Epoch 1450, val loss: 1.1952451467514038
Epoch 1460, training loss: 0.06714848428964615 = 0.0014058746164664626 + 0.01 * 6.574261665344238
Epoch 1460, val loss: 1.1964303255081177
Epoch 1470, training loss: 0.06710357964038849 = 0.0013926733518019319 + 0.01 * 6.571091175079346
Epoch 1470, val loss: 1.197663426399231
Epoch 1480, training loss: 0.06710950285196304 = 0.0013797933934256434 + 0.01 * 6.572970867156982
Epoch 1480, val loss: 1.1988105773925781
Epoch 1490, training loss: 0.06678569316864014 = 0.0013672177447006106 + 0.01 * 6.5418477058410645
Epoch 1490, val loss: 1.2000136375427246
Epoch 1500, training loss: 0.0666838139295578 = 0.0013548944843932986 + 0.01 * 6.532892227172852
Epoch 1500, val loss: 1.2011077404022217
Epoch 1510, training loss: 0.06680858880281448 = 0.0013428945094347 + 0.01 * 6.54656982421875
Epoch 1510, val loss: 1.2022466659545898
Epoch 1520, training loss: 0.06666938960552216 = 0.0013311009388417006 + 0.01 * 6.533829212188721
Epoch 1520, val loss: 1.2033666372299194
Epoch 1530, training loss: 0.06677732616662979 = 0.0013196627842262387 + 0.01 * 6.545766830444336
Epoch 1530, val loss: 1.204451084136963
Epoch 1540, training loss: 0.06657382845878601 = 0.001308394013904035 + 0.01 * 6.526543617248535
Epoch 1540, val loss: 1.2054914236068726
Epoch 1550, training loss: 0.06666024029254913 = 0.0012974586570635438 + 0.01 * 6.536278247833252
Epoch 1550, val loss: 1.2065606117248535
Epoch 1560, training loss: 0.06677437573671341 = 0.0012867137556895614 + 0.01 * 6.548766136169434
Epoch 1560, val loss: 1.2076094150543213
Epoch 1570, training loss: 0.06702673435211182 = 0.0012761889956891537 + 0.01 * 6.575055122375488
Epoch 1570, val loss: 1.208611011505127
Epoch 1580, training loss: 0.06656406074762344 = 0.0012658645864576101 + 0.01 * 6.529820442199707
Epoch 1580, val loss: 1.209687352180481
Epoch 1590, training loss: 0.06664633750915527 = 0.001255761831998825 + 0.01 * 6.53905725479126
Epoch 1590, val loss: 1.2106668949127197
Epoch 1600, training loss: 0.06652768701314926 = 0.0012459029676392674 + 0.01 * 6.528178691864014
Epoch 1600, val loss: 1.2117085456848145
Epoch 1610, training loss: 0.06649693101644516 = 0.001236242474988103 + 0.01 * 6.526068687438965
Epoch 1610, val loss: 1.212624192237854
Epoch 1620, training loss: 0.06671075522899628 = 0.0012268200516700745 + 0.01 * 6.548393726348877
Epoch 1620, val loss: 1.2135869264602661
Epoch 1630, training loss: 0.06641492247581482 = 0.001217536279000342 + 0.01 * 6.519738674163818
Epoch 1630, val loss: 1.2145694494247437
Epoch 1640, training loss: 0.06647432595491409 = 0.0012084435438737273 + 0.01 * 6.526587963104248
Epoch 1640, val loss: 1.2154796123504639
Epoch 1650, training loss: 0.0665118619799614 = 0.0011995305540040135 + 0.01 * 6.531233310699463
Epoch 1650, val loss: 1.2164579629898071
Epoch 1660, training loss: 0.06643261760473251 = 0.0011907690204679966 + 0.01 * 6.5241851806640625
Epoch 1660, val loss: 1.2173843383789062
Epoch 1670, training loss: 0.06670019030570984 = 0.0011822296073660254 + 0.01 * 6.5517964363098145
Epoch 1670, val loss: 1.2182890176773071
Epoch 1680, training loss: 0.06622497737407684 = 0.0011738203465938568 + 0.01 * 6.5051164627075195
Epoch 1680, val loss: 1.219200611114502
Epoch 1690, training loss: 0.06658344715833664 = 0.0011656066635623574 + 0.01 * 6.541784286499023
Epoch 1690, val loss: 1.2200547456741333
Epoch 1700, training loss: 0.06637175381183624 = 0.001157485879957676 + 0.01 * 6.521426677703857
Epoch 1700, val loss: 1.2210246324539185
Epoch 1710, training loss: 0.06671750545501709 = 0.001149555086158216 + 0.01 * 6.5567946434021
Epoch 1710, val loss: 1.2218506336212158
Epoch 1720, training loss: 0.06634844839572906 = 0.0011417901841923594 + 0.01 * 6.520666122436523
Epoch 1720, val loss: 1.2227962017059326
Epoch 1730, training loss: 0.06615892052650452 = 0.0011341266799718142 + 0.01 * 6.502479553222656
Epoch 1730, val loss: 1.2235764265060425
Epoch 1740, training loss: 0.06656969338655472 = 0.0011266556102782488 + 0.01 * 6.544304370880127
Epoch 1740, val loss: 1.224455714225769
Epoch 1750, training loss: 0.0660291537642479 = 0.0011192691745236516 + 0.01 * 6.490988731384277
Epoch 1750, val loss: 1.2252925634384155
Epoch 1760, training loss: 0.06612428277730942 = 0.001112012192606926 + 0.01 * 6.501227378845215
Epoch 1760, val loss: 1.2261089086532593
Epoch 1770, training loss: 0.06611032038927078 = 0.0011049143504351377 + 0.01 * 6.500540733337402
Epoch 1770, val loss: 1.2269439697265625
Epoch 1780, training loss: 0.06601431965827942 = 0.00109787133987993 + 0.01 * 6.491644859313965
Epoch 1780, val loss: 1.2278014421463013
Epoch 1790, training loss: 0.06628569215536118 = 0.0010910052806138992 + 0.01 * 6.519469261169434
Epoch 1790, val loss: 1.228603720664978
Epoch 1800, training loss: 0.06601158529520035 = 0.001084231655113399 + 0.01 * 6.492735385894775
Epoch 1800, val loss: 1.2294062376022339
Epoch 1810, training loss: 0.0661131888628006 = 0.001077565480954945 + 0.01 * 6.503562927246094
Epoch 1810, val loss: 1.2302337884902954
Epoch 1820, training loss: 0.06629127264022827 = 0.00107098079752177 + 0.01 * 6.522029399871826
Epoch 1820, val loss: 1.2310456037521362
Epoch 1830, training loss: 0.06584294885396957 = 0.0010645529255270958 + 0.01 * 6.477839469909668
Epoch 1830, val loss: 1.2318294048309326
Epoch 1840, training loss: 0.06620709598064423 = 0.0010582172544673085 + 0.01 * 6.514887809753418
Epoch 1840, val loss: 1.2325928211212158
Epoch 1850, training loss: 0.06598564237356186 = 0.0010519567877054214 + 0.01 * 6.493368625640869
Epoch 1850, val loss: 1.2333431243896484
Epoch 1860, training loss: 0.06579096615314484 = 0.0010458043543621898 + 0.01 * 6.474516868591309
Epoch 1860, val loss: 1.2341220378875732
Epoch 1870, training loss: 0.06583523005247116 = 0.0010397734586149454 + 0.01 * 6.479546070098877
Epoch 1870, val loss: 1.234869122505188
Epoch 1880, training loss: 0.0660586729645729 = 0.0010338516440242529 + 0.01 * 6.5024824142456055
Epoch 1880, val loss: 1.2355737686157227
Epoch 1890, training loss: 0.06582308560609818 = 0.001027959631755948 + 0.01 * 6.479513168334961
Epoch 1890, val loss: 1.2363598346710205
Epoch 1900, training loss: 0.0661565512418747 = 0.0010222104610875249 + 0.01 * 6.513433933258057
Epoch 1900, val loss: 1.23710036277771
Epoch 1910, training loss: 0.06591582298278809 = 0.0010165059939026833 + 0.01 * 6.489932060241699
Epoch 1910, val loss: 1.2377898693084717
Epoch 1920, training loss: 0.06581582874059677 = 0.001010914915241301 + 0.01 * 6.4804911613464355
Epoch 1920, val loss: 1.238532304763794
Epoch 1930, training loss: 0.06593496352434158 = 0.0010054168524220586 + 0.01 * 6.492954730987549
Epoch 1930, val loss: 1.239214539527893
Epoch 1940, training loss: 0.06564105302095413 = 0.0009999728063121438 + 0.01 * 6.464107990264893
Epoch 1940, val loss: 1.2399652004241943
Epoch 1950, training loss: 0.06582837551832199 = 0.0009946165373548865 + 0.01 * 6.483376502990723
Epoch 1950, val loss: 1.2406609058380127
Epoch 1960, training loss: 0.06561583280563354 = 0.0009893323294818401 + 0.01 * 6.462649822235107
Epoch 1960, val loss: 1.2413920164108276
Epoch 1970, training loss: 0.0656391829252243 = 0.0009841191349551082 + 0.01 * 6.465506553649902
Epoch 1970, val loss: 1.2420624494552612
Epoch 1980, training loss: 0.06583479046821594 = 0.0009790102485567331 + 0.01 * 6.4855780601501465
Epoch 1980, val loss: 1.2427701950073242
Epoch 1990, training loss: 0.06575451046228409 = 0.0009739374509081244 + 0.01 * 6.478056907653809
Epoch 1990, val loss: 1.2434215545654297
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.0354886054992676 = 1.949520230293274 + 0.01 * 8.596829414367676
Epoch 0, val loss: 1.9498891830444336
Epoch 10, training loss: 2.024958848953247 = 1.9389911890029907 + 0.01 * 8.59676456451416
Epoch 10, val loss: 1.9398972988128662
Epoch 20, training loss: 2.0122716426849365 = 1.9263063669204712 + 0.01 * 8.596529960632324
Epoch 20, val loss: 1.9274945259094238
Epoch 30, training loss: 1.9949368238449097 = 1.9089797735214233 + 0.01 * 8.595709800720215
Epoch 30, val loss: 1.91021728515625
Epoch 40, training loss: 1.9697448015213013 = 1.883842945098877 + 0.01 * 8.590190887451172
Epoch 40, val loss: 1.8853027820587158
Epoch 50, training loss: 1.9341168403625488 = 1.8486151695251465 + 0.01 * 8.550168991088867
Epoch 50, val loss: 1.8520288467407227
Epoch 60, training loss: 1.8914107084274292 = 1.8076798915863037 + 0.01 * 8.373078346252441
Epoch 60, val loss: 1.816715955734253
Epoch 70, training loss: 1.8496242761611938 = 1.7694740295410156 + 0.01 * 8.015028953552246
Epoch 70, val loss: 1.7849088907241821
Epoch 80, training loss: 1.8024041652679443 = 1.7235369682312012 + 0.01 * 7.886720657348633
Epoch 80, val loss: 1.7421048879623413
Epoch 90, training loss: 1.736686110496521 = 1.6592686176300049 + 0.01 * 7.741752624511719
Epoch 90, val loss: 1.6838550567626953
Epoch 100, training loss: 1.650526523590088 = 1.5746684074401855 + 0.01 * 7.585809707641602
Epoch 100, val loss: 1.6118489503860474
Epoch 110, training loss: 1.5536067485809326 = 1.479354977607727 + 0.01 * 7.42517614364624
Epoch 110, val loss: 1.5325512886047363
Epoch 120, training loss: 1.4593042135238647 = 1.3855812549591064 + 0.01 * 7.372292995452881
Epoch 120, val loss: 1.4566885232925415
Epoch 130, training loss: 1.3701828718185425 = 1.296636700630188 + 0.01 * 7.354619026184082
Epoch 130, val loss: 1.3865395784378052
Epoch 140, training loss: 1.2836846113204956 = 1.2104443311691284 + 0.01 * 7.324028015136719
Epoch 140, val loss: 1.3206509351730347
Epoch 150, training loss: 1.1979671716690063 = 1.1249834299087524 + 0.01 * 7.298377990722656
Epoch 150, val loss: 1.257280945777893
Epoch 160, training loss: 1.1109998226165771 = 1.038225769996643 + 0.01 * 7.277411460876465
Epoch 160, val loss: 1.1939104795455933
Epoch 170, training loss: 1.0222150087356567 = 0.9496832489967346 + 0.01 * 7.25317907333374
Epoch 170, val loss: 1.128649353981018
Epoch 180, training loss: 0.9340833425521851 = 0.8618666529655457 + 0.01 * 7.22166633605957
Epoch 180, val loss: 1.0636403560638428
Epoch 190, training loss: 0.8507038354873657 = 0.7788739204406738 + 0.01 * 7.182989120483398
Epoch 190, val loss: 1.0026975870132446
Epoch 200, training loss: 0.7749928832054138 = 0.7035442590713501 + 0.01 * 7.144862651824951
Epoch 200, val loss: 0.9487444758415222
Epoch 210, training loss: 0.7073314785957336 = 0.6361591219902039 + 0.01 * 7.11723518371582
Epoch 210, val loss: 0.9031213521957397
Epoch 220, training loss: 0.6462581753730774 = 0.5752681493759155 + 0.01 * 7.099000453948975
Epoch 220, val loss: 0.8655379414558411
Epoch 230, training loss: 0.5900747776031494 = 0.5192636251449585 + 0.01 * 7.081117153167725
Epoch 230, val loss: 0.8350798487663269
Epoch 240, training loss: 0.5378596782684326 = 0.4671950340270996 + 0.01 * 7.066465377807617
Epoch 240, val loss: 0.811198353767395
Epoch 250, training loss: 0.48930370807647705 = 0.41883185505867004 + 0.01 * 7.047184944152832
Epoch 250, val loss: 0.7936733961105347
Epoch 260, training loss: 0.44462233781814575 = 0.3742988407611847 + 0.01 * 7.032349109649658
Epoch 260, val loss: 0.7821578979492188
Epoch 270, training loss: 0.40403011441230774 = 0.33378684520721436 + 0.01 * 7.024327278137207
Epoch 270, val loss: 0.7763278484344482
Epoch 280, training loss: 0.3674395680427551 = 0.2972976863384247 + 0.01 * 7.014188289642334
Epoch 280, val loss: 0.7752608060836792
Epoch 290, training loss: 0.3347097635269165 = 0.2646315097808838 + 0.01 * 7.007823944091797
Epoch 290, val loss: 0.7781423330307007
Epoch 300, training loss: 0.30555838346481323 = 0.23551933467388153 + 0.01 * 7.003903388977051
Epoch 300, val loss: 0.7846090197563171
Epoch 310, training loss: 0.27963680028915405 = 0.20964667201042175 + 0.01 * 6.999011993408203
Epoch 310, val loss: 0.7940799593925476
Epoch 320, training loss: 0.2566932141780853 = 0.18670831620693207 + 0.01 * 6.9984893798828125
Epoch 320, val loss: 0.8060085773468018
Epoch 330, training loss: 0.23628821969032288 = 0.1663835346698761 + 0.01 * 6.990469455718994
Epoch 330, val loss: 0.8198632001876831
Epoch 340, training loss: 0.21825481951236725 = 0.1483801156282425 + 0.01 * 6.987470626831055
Epoch 340, val loss: 0.8352384567260742
Epoch 350, training loss: 0.20226018130779266 = 0.13243341445922852 + 0.01 * 6.982676982879639
Epoch 350, val loss: 0.8516388535499573
Epoch 360, training loss: 0.1881362497806549 = 0.11830820143222809 + 0.01 * 6.982804775238037
Epoch 360, val loss: 0.8687340617179871
Epoch 370, training loss: 0.17558637261390686 = 0.10580644011497498 + 0.01 * 6.977993965148926
Epoch 370, val loss: 0.8862483501434326
Epoch 380, training loss: 0.164480522274971 = 0.09475019574165344 + 0.01 * 6.973032474517822
Epoch 380, val loss: 0.904000461101532
Epoch 390, training loss: 0.15465247631072998 = 0.08497998863458633 + 0.01 * 6.967248916625977
Epoch 390, val loss: 0.9218385219573975
Epoch 400, training loss: 0.14595505595207214 = 0.07635107636451721 + 0.01 * 6.960397720336914
Epoch 400, val loss: 0.9396418333053589
Epoch 410, training loss: 0.13829603791236877 = 0.0687335729598999 + 0.01 * 6.956246376037598
Epoch 410, val loss: 0.9572312831878662
Epoch 420, training loss: 0.13150957226753235 = 0.062007658183574677 + 0.01 * 6.950191020965576
Epoch 420, val loss: 0.9745559096336365
Epoch 430, training loss: 0.1255502551794052 = 0.05606192722916603 + 0.01 * 6.9488325119018555
Epoch 430, val loss: 0.9915448427200317
Epoch 440, training loss: 0.12017954140901566 = 0.05079301446676254 + 0.01 * 6.938652515411377
Epoch 440, val loss: 1.0081716775894165
Epoch 450, training loss: 0.11540012061595917 = 0.04610808566212654 + 0.01 * 6.929203510284424
Epoch 450, val loss: 1.0243568420410156
Epoch 460, training loss: 0.1112041175365448 = 0.04193618521094322 + 0.01 * 6.926793098449707
Epoch 460, val loss: 1.0400640964508057
Epoch 470, training loss: 0.10739248245954514 = 0.03821959346532822 + 0.01 * 6.9172892570495605
Epoch 470, val loss: 1.0553054809570312
Epoch 480, training loss: 0.10399360209703445 = 0.03490316867828369 + 0.01 * 6.909043788909912
Epoch 480, val loss: 1.070037841796875
Epoch 490, training loss: 0.10131482779979706 = 0.03193970024585724 + 0.01 * 6.9375128746032715
Epoch 490, val loss: 1.0842959880828857
Epoch 500, training loss: 0.09838860481977463 = 0.02929433435201645 + 0.01 * 6.909427165985107
Epoch 500, val loss: 1.098083257675171
Epoch 510, training loss: 0.09585678577423096 = 0.026926912367343903 + 0.01 * 6.8929877281188965
Epoch 510, val loss: 1.1113477945327759
Epoch 520, training loss: 0.09369899332523346 = 0.02480391412973404 + 0.01 * 6.889508247375488
Epoch 520, val loss: 1.1242221593856812
Epoch 530, training loss: 0.09192762523889542 = 0.022896816954016685 + 0.01 * 6.903080940246582
Epoch 530, val loss: 1.1366256475448608
Epoch 540, training loss: 0.09000709652900696 = 0.02118305303156376 + 0.01 * 6.882404804229736
Epoch 540, val loss: 1.148573398590088
Epoch 550, training loss: 0.08839276432991028 = 0.01963803917169571 + 0.01 * 6.8754730224609375
Epoch 550, val loss: 1.160089135169983
Epoch 560, training loss: 0.08694138377904892 = 0.01824341155588627 + 0.01 * 6.869797706604004
Epoch 560, val loss: 1.171228289604187
Epoch 570, training loss: 0.08565372973680496 = 0.016982302069664 + 0.01 * 6.867142677307129
Epoch 570, val loss: 1.182020664215088
Epoch 580, training loss: 0.08452870696783066 = 0.015842154622077942 + 0.01 * 6.868655681610107
Epoch 580, val loss: 1.192396640777588
Epoch 590, training loss: 0.08339114487171173 = 0.014807406812906265 + 0.01 * 6.858373641967773
Epoch 590, val loss: 1.2024613618850708
Epoch 600, training loss: 0.08242865651845932 = 0.01386649813503027 + 0.01 * 6.856215953826904
Epoch 600, val loss: 1.2121697664260864
Epoch 610, training loss: 0.08154652267694473 = 0.013009579852223396 + 0.01 * 6.853694438934326
Epoch 610, val loss: 1.2215863466262817
Epoch 620, training loss: 0.08066055178642273 = 0.012228469364345074 + 0.01 * 6.8432087898254395
Epoch 620, val loss: 1.230612874031067
Epoch 630, training loss: 0.07997620850801468 = 0.01151488907635212 + 0.01 * 6.846132278442383
Epoch 630, val loss: 1.2393418550491333
Epoch 640, training loss: 0.07921624928712845 = 0.01086226012557745 + 0.01 * 6.8353986740112305
Epoch 640, val loss: 1.2478277683258057
Epoch 650, training loss: 0.07858604192733765 = 0.010263797827064991 + 0.01 * 6.832224369049072
Epoch 650, val loss: 1.2559789419174194
Epoch 660, training loss: 0.07803231477737427 = 0.009714226238429546 + 0.01 * 6.8318095207214355
Epoch 660, val loss: 1.2639230489730835
Epoch 670, training loss: 0.07765339314937592 = 0.009208686649799347 + 0.01 * 6.844470977783203
Epoch 670, val loss: 1.271539330482483
Epoch 680, training loss: 0.07702320069074631 = 0.0087431650608778 + 0.01 * 6.828003883361816
Epoch 680, val loss: 1.278918743133545
Epoch 690, training loss: 0.07652270048856735 = 0.00831358227878809 + 0.01 * 6.820911884307861
Epoch 690, val loss: 1.2860596179962158
Epoch 700, training loss: 0.07621189951896667 = 0.007916656322777271 + 0.01 * 6.829524040222168
Epoch 700, val loss: 1.292898416519165
Epoch 710, training loss: 0.07565781474113464 = 0.007549053989350796 + 0.01 * 6.810876369476318
Epoch 710, val loss: 1.2995365858078003
Epoch 720, training loss: 0.07531379908323288 = 0.007207947317510843 + 0.01 * 6.8105854988098145
Epoch 720, val loss: 1.305981159210205
Epoch 730, training loss: 0.07509234547615051 = 0.006890348624438047 + 0.01 * 6.820199966430664
Epoch 730, val loss: 1.3122347593307495
Epoch 740, training loss: 0.07462258636951447 = 0.00659584766253829 + 0.01 * 6.802673816680908
Epoch 740, val loss: 1.318321943283081
Epoch 750, training loss: 0.07440844178199768 = 0.006320590153336525 + 0.01 * 6.808785438537598
Epoch 750, val loss: 1.3241854906082153
Epoch 760, training loss: 0.0740068331360817 = 0.006063695065677166 + 0.01 * 6.794314384460449
Epoch 760, val loss: 1.3298470973968506
Epoch 770, training loss: 0.0740116760134697 = 0.00582287460565567 + 0.01 * 6.818880081176758
Epoch 770, val loss: 1.3353592157363892
Epoch 780, training loss: 0.07344237715005875 = 0.005598271731287241 + 0.01 * 6.7844109535217285
Epoch 780, val loss: 1.3408235311508179
Epoch 790, training loss: 0.0731428936123848 = 0.005386787932366133 + 0.01 * 6.775611400604248
Epoch 790, val loss: 1.3460336923599243
Epoch 800, training loss: 0.07301541417837143 = 0.005188581068068743 + 0.01 * 6.7826828956604
Epoch 800, val loss: 1.351109504699707
Epoch 810, training loss: 0.07267449796199799 = 0.005002528894692659 + 0.01 * 6.767197132110596
Epoch 810, val loss: 1.3560627698898315
Epoch 820, training loss: 0.07255924493074417 = 0.0048275114968419075 + 0.01 * 6.7731733322143555
Epoch 820, val loss: 1.3608176708221436
Epoch 830, training loss: 0.07228659838438034 = 0.004662798717617989 + 0.01 * 6.762380599975586
Epoch 830, val loss: 1.3654720783233643
Epoch 840, training loss: 0.0721907988190651 = 0.004507300443947315 + 0.01 * 6.768349647521973
Epoch 840, val loss: 1.3699951171875
Epoch 850, training loss: 0.0720529779791832 = 0.0043603419326245785 + 0.01 * 6.769263744354248
Epoch 850, val loss: 1.3743610382080078
Epoch 860, training loss: 0.07188498228788376 = 0.004221566021442413 + 0.01 * 6.766341686248779
Epoch 860, val loss: 1.3787420988082886
Epoch 870, training loss: 0.07162000983953476 = 0.0040903156623244286 + 0.01 * 6.752969741821289
Epoch 870, val loss: 1.3828210830688477
Epoch 880, training loss: 0.07164859771728516 = 0.003965935669839382 + 0.01 * 6.768266201019287
Epoch 880, val loss: 1.386885404586792
Epoch 890, training loss: 0.07143605500459671 = 0.0038482388481497765 + 0.01 * 6.758781909942627
Epoch 890, val loss: 1.390902042388916
Epoch 900, training loss: 0.07117576897144318 = 0.0037365180905908346 + 0.01 * 6.74392557144165
Epoch 900, val loss: 1.3946986198425293
Epoch 910, training loss: 0.07110970467329025 = 0.003630226943641901 + 0.01 * 6.747947692871094
Epoch 910, val loss: 1.3984514474868774
Epoch 920, training loss: 0.07099426537752151 = 0.0035292322281748056 + 0.01 * 6.7465033531188965
Epoch 920, val loss: 1.402130365371704
Epoch 930, training loss: 0.07077307999134064 = 0.003432792378589511 + 0.01 * 6.7340288162231445
Epoch 930, val loss: 1.4057737588882446
Epoch 940, training loss: 0.0707399845123291 = 0.0033408324234187603 + 0.01 * 6.739915370941162
Epoch 940, val loss: 1.4092494249343872
Epoch 950, training loss: 0.07060705125331879 = 0.003253113478422165 + 0.01 * 6.735394477844238
Epoch 950, val loss: 1.4127618074417114
Epoch 960, training loss: 0.07050512731075287 = 0.0031693833880126476 + 0.01 * 6.733574867248535
Epoch 960, val loss: 1.4160455465316772
Epoch 970, training loss: 0.07035816460847855 = 0.0030891632195562124 + 0.01 * 6.726900577545166
Epoch 970, val loss: 1.4193792343139648
Epoch 980, training loss: 0.07031916081905365 = 0.00301247532479465 + 0.01 * 6.730668544769287
Epoch 980, val loss: 1.4226049184799194
Epoch 990, training loss: 0.0701722651720047 = 0.0029390458948910236 + 0.01 * 6.72332239151001
Epoch 990, val loss: 1.425780177116394
Epoch 1000, training loss: 0.07031232118606567 = 0.002868377137929201 + 0.01 * 6.744394302368164
Epoch 1000, val loss: 1.4288362264633179
Epoch 1010, training loss: 0.07012533396482468 = 0.002800858346745372 + 0.01 * 6.732447624206543
Epoch 1010, val loss: 1.4319497346878052
Epoch 1020, training loss: 0.06991507112979889 = 0.0027357349172234535 + 0.01 * 6.7179341316223145
Epoch 1020, val loss: 1.4349043369293213
Epoch 1030, training loss: 0.06990649551153183 = 0.002673956099897623 + 0.01 * 6.723254680633545
Epoch 1030, val loss: 1.4378125667572021
Epoch 1040, training loss: 0.06988883018493652 = 0.0026146157179027796 + 0.01 * 6.727421283721924
Epoch 1040, val loss: 1.4405407905578613
Epoch 1050, training loss: 0.06970718502998352 = 0.0025576837360858917 + 0.01 * 6.7149505615234375
Epoch 1050, val loss: 1.4433575868606567
Epoch 1060, training loss: 0.06962911784648895 = 0.002503235125914216 + 0.01 * 6.712588310241699
Epoch 1060, val loss: 1.4460266828536987
Epoch 1070, training loss: 0.06964654475450516 = 0.0024509415961802006 + 0.01 * 6.719560623168945
Epoch 1070, val loss: 1.4485838413238525
Epoch 1080, training loss: 0.06951653212308884 = 0.0024005973245948553 + 0.01 * 6.7115936279296875
Epoch 1080, val loss: 1.451248049736023
Epoch 1090, training loss: 0.06945312023162842 = 0.0023524765856564045 + 0.01 * 6.710064888000488
Epoch 1090, val loss: 1.4536809921264648
Epoch 1100, training loss: 0.06940606981515884 = 0.0023061931133270264 + 0.01 * 6.709988117218018
Epoch 1100, val loss: 1.4561439752578735
Epoch 1110, training loss: 0.06937402486801147 = 0.0022615280468016863 + 0.01 * 6.711250305175781
Epoch 1110, val loss: 1.458489179611206
Epoch 1120, training loss: 0.06918615847826004 = 0.002218797570094466 + 0.01 * 6.696735858917236
Epoch 1120, val loss: 1.4608956575393677
Epoch 1130, training loss: 0.06919252872467041 = 0.0021774093620479107 + 0.01 * 6.701512336730957
Epoch 1130, val loss: 1.4631023406982422
Epoch 1140, training loss: 0.06913886219263077 = 0.0021376698277890682 + 0.01 * 6.700119495391846
Epoch 1140, val loss: 1.4653809070587158
Epoch 1150, training loss: 0.06920134276151657 = 0.002099576173350215 + 0.01 * 6.710176944732666
Epoch 1150, val loss: 1.4675779342651367
Epoch 1160, training loss: 0.06904473900794983 = 0.0020627279300242662 + 0.01 * 6.698200702667236
Epoch 1160, val loss: 1.4695438146591187
Epoch 1170, training loss: 0.06891725212335587 = 0.00202737282961607 + 0.01 * 6.68898868560791
Epoch 1170, val loss: 1.471653699874878
Epoch 1180, training loss: 0.06892765313386917 = 0.0019930428825318813 + 0.01 * 6.693460941314697
Epoch 1180, val loss: 1.473702073097229
Epoch 1190, training loss: 0.06882239133119583 = 0.0019600559026002884 + 0.01 * 6.6862335205078125
Epoch 1190, val loss: 1.4755979776382446
Epoch 1200, training loss: 0.06886708736419678 = 0.001928119221702218 + 0.01 * 6.693896770477295
Epoch 1200, val loss: 1.4775443077087402
Epoch 1210, training loss: 0.06880717724561691 = 0.0018972790567204356 + 0.01 * 6.690990447998047
Epoch 1210, val loss: 1.4795101881027222
Epoch 1220, training loss: 0.0686955377459526 = 0.0018675331957638264 + 0.01 * 6.68280029296875
Epoch 1220, val loss: 1.4812942743301392
Epoch 1230, training loss: 0.06866646558046341 = 0.0018388130702078342 + 0.01 * 6.682765483856201
Epoch 1230, val loss: 1.4830420017242432
Epoch 1240, training loss: 0.06877544522285461 = 0.0018111133249476552 + 0.01 * 6.696433067321777
Epoch 1240, val loss: 1.484761357307434
Epoch 1250, training loss: 0.06852908432483673 = 0.001784181804396212 + 0.01 * 6.674489974975586
Epoch 1250, val loss: 1.4865611791610718
Epoch 1260, training loss: 0.06887275725603104 = 0.0017581433057785034 + 0.01 * 6.711461544036865
Epoch 1260, val loss: 1.4881993532180786
Epoch 1270, training loss: 0.06860679388046265 = 0.001733063138090074 + 0.01 * 6.687373161315918
Epoch 1270, val loss: 1.4897912740707397
Epoch 1280, training loss: 0.06844384968280792 = 0.0017090286128222942 + 0.01 * 6.6734819412231445
Epoch 1280, val loss: 1.4913169145584106
Epoch 1290, training loss: 0.06845780462026596 = 0.001685354276560247 + 0.01 * 6.677245140075684
Epoch 1290, val loss: 1.492828130722046
Epoch 1300, training loss: 0.0683988407254219 = 0.001662669819779694 + 0.01 * 6.673616886138916
Epoch 1300, val loss: 1.4943562746047974
Epoch 1310, training loss: 0.06836936622858047 = 0.0016404404304921627 + 0.01 * 6.672893047332764
Epoch 1310, val loss: 1.49578857421875
Epoch 1320, training loss: 0.06832616776227951 = 0.0016191109316423535 + 0.01 * 6.670706272125244
Epoch 1320, val loss: 1.4971933364868164
Epoch 1330, training loss: 0.06832363456487656 = 0.001598271308466792 + 0.01 * 6.672536849975586
Epoch 1330, val loss: 1.4986332654953003
Epoch 1340, training loss: 0.06816697865724564 = 0.0015780259855091572 + 0.01 * 6.658895492553711
Epoch 1340, val loss: 1.4999507665634155
Epoch 1350, training loss: 0.06813551485538483 = 0.001558487070724368 + 0.01 * 6.657702445983887
Epoch 1350, val loss: 1.501367449760437
Epoch 1360, training loss: 0.06822358071804047 = 0.0015394960064440966 + 0.01 * 6.668408393859863
Epoch 1360, val loss: 1.5026593208312988
Epoch 1370, training loss: 0.06804072856903076 = 0.0015210674609988928 + 0.01 * 6.651966571807861
Epoch 1370, val loss: 1.5038784742355347
Epoch 1380, training loss: 0.06812967360019684 = 0.0015031778020784259 + 0.01 * 6.662650108337402
Epoch 1380, val loss: 1.505058765411377
Epoch 1390, training loss: 0.06817316263914108 = 0.0014857696369290352 + 0.01 * 6.668738842010498
Epoch 1390, val loss: 1.5063846111297607
Epoch 1400, training loss: 0.06802190840244293 = 0.0014689074596390128 + 0.01 * 6.655300617218018
Epoch 1400, val loss: 1.5075209140777588
Epoch 1410, training loss: 0.06822001934051514 = 0.0014525987207889557 + 0.01 * 6.676741600036621
Epoch 1410, val loss: 1.508634328842163
Epoch 1420, training loss: 0.0679347813129425 = 0.0014366167597472668 + 0.01 * 6.649816989898682
Epoch 1420, val loss: 1.509765386581421
Epoch 1430, training loss: 0.06792382150888443 = 0.0014210602967068553 + 0.01 * 6.6502766609191895
Epoch 1430, val loss: 1.510791540145874
Epoch 1440, training loss: 0.06799760460853577 = 0.0014060868415981531 + 0.01 * 6.659152030944824
Epoch 1440, val loss: 1.5118556022644043
Epoch 1450, training loss: 0.06779026240110397 = 0.001391315134242177 + 0.01 * 6.639894962310791
Epoch 1450, val loss: 1.5129948854446411
Epoch 1460, training loss: 0.06796994060277939 = 0.001377015607431531 + 0.01 * 6.659292221069336
Epoch 1460, val loss: 1.5139169692993164
Epoch 1470, training loss: 0.06786374002695084 = 0.0013631006004288793 + 0.01 * 6.650063991546631
Epoch 1470, val loss: 1.5150015354156494
Epoch 1480, training loss: 0.06800082325935364 = 0.0013496583560481668 + 0.01 * 6.665116786956787
Epoch 1480, val loss: 1.5159531831741333
Epoch 1490, training loss: 0.06772635132074356 = 0.0013363781617954373 + 0.01 * 6.638997554779053
Epoch 1490, val loss: 1.516858696937561
Epoch 1500, training loss: 0.06765155494213104 = 0.001323562697507441 + 0.01 * 6.63279914855957
Epoch 1500, val loss: 1.5178076028823853
Epoch 1510, training loss: 0.06769774854183197 = 0.0013110545696690679 + 0.01 * 6.638669013977051
Epoch 1510, val loss: 1.5187309980392456
Epoch 1520, training loss: 0.06777743250131607 = 0.0012989344540983438 + 0.01 * 6.647850036621094
Epoch 1520, val loss: 1.5196099281311035
Epoch 1530, training loss: 0.06753960251808167 = 0.0012869357597082853 + 0.01 * 6.6252665519714355
Epoch 1530, val loss: 1.520527720451355
Epoch 1540, training loss: 0.0675700306892395 = 0.0012753441696986556 + 0.01 * 6.62946891784668
Epoch 1540, val loss: 1.521391749382019
Epoch 1550, training loss: 0.06776292622089386 = 0.001264010090380907 + 0.01 * 6.649891376495361
Epoch 1550, val loss: 1.5221939086914062
Epoch 1560, training loss: 0.06743943691253662 = 0.0012530150124803185 + 0.01 * 6.618642330169678
Epoch 1560, val loss: 1.5229829549789429
Epoch 1570, training loss: 0.06745826452970505 = 0.0012422293657436967 + 0.01 * 6.621603488922119
Epoch 1570, val loss: 1.523775577545166
Epoch 1580, training loss: 0.0674416646361351 = 0.001231710659340024 + 0.01 * 6.62099552154541
Epoch 1580, val loss: 1.5245370864868164
Epoch 1590, training loss: 0.0677773505449295 = 0.0012215367751196027 + 0.01 * 6.655581474304199
Epoch 1590, val loss: 1.525317907333374
Epoch 1600, training loss: 0.06728173792362213 = 0.0012114396085962653 + 0.01 * 6.607029438018799
Epoch 1600, val loss: 1.5259867906570435
Epoch 1610, training loss: 0.06770284473896027 = 0.0012016913387924433 + 0.01 * 6.650115489959717
Epoch 1610, val loss: 1.5267027616500854
Epoch 1620, training loss: 0.06744591146707535 = 0.0011922562262043357 + 0.01 * 6.625365734100342
Epoch 1620, val loss: 1.5273879766464233
Epoch 1630, training loss: 0.06728122383356094 = 0.0011828787392005324 + 0.01 * 6.609835147857666
Epoch 1630, val loss: 1.5280449390411377
Epoch 1640, training loss: 0.06740282475948334 = 0.0011738028842955828 + 0.01 * 6.6229023933410645
Epoch 1640, val loss: 1.5286707878112793
Epoch 1650, training loss: 0.06731028109788895 = 0.0011649762745946646 + 0.01 * 6.61453104019165
Epoch 1650, val loss: 1.5292619466781616
Epoch 1660, training loss: 0.06733819842338562 = 0.001156232669018209 + 0.01 * 6.618196964263916
Epoch 1660, val loss: 1.5298885107040405
Epoch 1670, training loss: 0.06714072078466415 = 0.0011476974468678236 + 0.01 * 6.599302768707275
Epoch 1670, val loss: 1.530470848083496
Epoch 1680, training loss: 0.06730717420578003 = 0.001139333937317133 + 0.01 * 6.61678409576416
Epoch 1680, val loss: 1.5310554504394531
Epoch 1690, training loss: 0.06706581264734268 = 0.0011312148999422789 + 0.01 * 6.5934600830078125
Epoch 1690, val loss: 1.5315508842468262
Epoch 1700, training loss: 0.0671025738120079 = 0.0011231842217966914 + 0.01 * 6.597939491271973
Epoch 1700, val loss: 1.532199740409851
Epoch 1710, training loss: 0.06725030392408371 = 0.0011154490057379007 + 0.01 * 6.613485336303711
Epoch 1710, val loss: 1.5326911211013794
Epoch 1720, training loss: 0.06707987189292908 = 0.0011077793315052986 + 0.01 * 6.5972089767456055
Epoch 1720, val loss: 1.5332261323928833
Epoch 1730, training loss: 0.06692884117364883 = 0.00110031315125525 + 0.01 * 6.582852840423584
Epoch 1730, val loss: 1.5336741209030151
Epoch 1740, training loss: 0.06695890426635742 = 0.0010929693235084414 + 0.01 * 6.5865936279296875
Epoch 1740, val loss: 1.534228801727295
Epoch 1750, training loss: 0.06692972779273987 = 0.0010857348097488284 + 0.01 * 6.584399700164795
Epoch 1750, val loss: 1.5346227884292603
Epoch 1760, training loss: 0.06706833839416504 = 0.001078713801689446 + 0.01 * 6.598962306976318
Epoch 1760, val loss: 1.5351167917251587
Epoch 1770, training loss: 0.06680498272180557 = 0.0010718867415562272 + 0.01 * 6.573309898376465
Epoch 1770, val loss: 1.5356221199035645
Epoch 1780, training loss: 0.06683152914047241 = 0.0010651310440152884 + 0.01 * 6.576640605926514
Epoch 1780, val loss: 1.5359910726547241
Epoch 1790, training loss: 0.06695748120546341 = 0.001058475230820477 + 0.01 * 6.589900493621826
Epoch 1790, val loss: 1.5363820791244507
Epoch 1800, training loss: 0.06688393652439117 = 0.0010519407223910093 + 0.01 * 6.583199501037598
Epoch 1800, val loss: 1.536808729171753
Epoch 1810, training loss: 0.06689754873514175 = 0.0010455052834004164 + 0.01 * 6.585204601287842
Epoch 1810, val loss: 1.5372503995895386
Epoch 1820, training loss: 0.06675419211387634 = 0.0010391895193606615 + 0.01 * 6.571500301361084
Epoch 1820, val loss: 1.5375949144363403
Epoch 1830, training loss: 0.06696337461471558 = 0.0010330541990697384 + 0.01 * 6.593032360076904
Epoch 1830, val loss: 1.537979006767273
Epoch 1840, training loss: 0.06673990935087204 = 0.0010269525228068233 + 0.01 * 6.571296215057373
Epoch 1840, val loss: 1.5384039878845215
Epoch 1850, training loss: 0.06683699786663055 = 0.0010210017208009958 + 0.01 * 6.581599712371826
Epoch 1850, val loss: 1.5387083292007446
Epoch 1860, training loss: 0.0667068287730217 = 0.0010151420719921589 + 0.01 * 6.569168567657471
Epoch 1860, val loss: 1.5390554666519165
Epoch 1870, training loss: 0.06675171852111816 = 0.001009373925626278 + 0.01 * 6.574234485626221
Epoch 1870, val loss: 1.5394057035446167
Epoch 1880, training loss: 0.0667550340294838 = 0.0010036815656349063 + 0.01 * 6.575135231018066
Epoch 1880, val loss: 1.5398330688476562
Epoch 1890, training loss: 0.06662201881408691 = 0.0009980907198041677 + 0.01 * 6.5623931884765625
Epoch 1890, val loss: 1.5400720834732056
Epoch 1900, training loss: 0.06669323891401291 = 0.0009926824131980538 + 0.01 * 6.5700554847717285
Epoch 1900, val loss: 1.5404210090637207
Epoch 1910, training loss: 0.06669001281261444 = 0.000987253850325942 + 0.01 * 6.570276260375977
Epoch 1910, val loss: 1.5407209396362305
Epoch 1920, training loss: 0.06658194959163666 = 0.000981997698545456 + 0.01 * 6.559995651245117
Epoch 1920, val loss: 1.5410126447677612
Epoch 1930, training loss: 0.0664975643157959 = 0.0009766781004145741 + 0.01 * 6.552088737487793
Epoch 1930, val loss: 1.5413625240325928
Epoch 1940, training loss: 0.06672929972410202 = 0.000971516128629446 + 0.01 * 6.575778484344482
Epoch 1940, val loss: 1.541636347770691
Epoch 1950, training loss: 0.06646128743886948 = 0.0009664234239608049 + 0.01 * 6.5494866371154785
Epoch 1950, val loss: 1.5419503450393677
Epoch 1960, training loss: 0.06647928804159164 = 0.000961386424023658 + 0.01 * 6.551790714263916
Epoch 1960, val loss: 1.5422106981277466
Epoch 1970, training loss: 0.06656412035226822 = 0.0009564616484567523 + 0.01 * 6.560766220092773
Epoch 1970, val loss: 1.542531967163086
Epoch 1980, training loss: 0.06637587398290634 = 0.0009516336722299457 + 0.01 * 6.542424201965332
Epoch 1980, val loss: 1.542816400527954
Epoch 1990, training loss: 0.06663796305656433 = 0.0009468293865211308 + 0.01 * 6.569113731384277
Epoch 1990, val loss: 1.5430772304534912
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.046191692352295 = 1.9602237939834595 + 0.01 * 8.596797943115234
Epoch 0, val loss: 1.9591152667999268
Epoch 10, training loss: 2.035745620727539 = 1.9497785568237305 + 0.01 * 8.596702575683594
Epoch 10, val loss: 1.9491807222366333
Epoch 20, training loss: 2.022906541824341 = 1.9369431734085083 + 0.01 * 8.596333503723145
Epoch 20, val loss: 1.936575174331665
Epoch 30, training loss: 2.004793167114258 = 1.9188467264175415 + 0.01 * 8.594648361206055
Epoch 30, val loss: 1.9185160398483276
Epoch 40, training loss: 1.9777064323425293 = 1.891893982887268 + 0.01 * 8.581239700317383
Epoch 40, val loss: 1.8916687965393066
Epoch 50, training loss: 1.9383373260498047 = 1.8533746004104614 + 0.01 * 8.496274948120117
Epoch 50, val loss: 1.8547204732894897
Epoch 60, training loss: 1.8896600008010864 = 1.808434247970581 + 0.01 * 8.122580528259277
Epoch 60, val loss: 1.815363883972168
Epoch 70, training loss: 1.8495898246765137 = 1.769510269165039 + 0.01 * 8.00795841217041
Epoch 70, val loss: 1.7837107181549072
Epoch 80, training loss: 1.8035221099853516 = 1.7254996299743652 + 0.01 * 7.802244663238525
Epoch 80, val loss: 1.742460012435913
Epoch 90, training loss: 1.740166425704956 = 1.6640955209732056 + 0.01 * 7.607089996337891
Epoch 90, val loss: 1.6852102279663086
Epoch 100, training loss: 1.6570074558258057 = 1.5829263925552368 + 0.01 * 7.408102035522461
Epoch 100, val loss: 1.6148773431777954
Epoch 110, training loss: 1.5610220432281494 = 1.4885327816009521 + 0.01 * 7.248932361602783
Epoch 110, val loss: 1.5367515087127686
Epoch 120, training loss: 1.4671008586883545 = 1.3953649997711182 + 0.01 * 7.173583984375
Epoch 120, val loss: 1.4612061977386475
Epoch 130, training loss: 1.3783900737762451 = 1.3074146509170532 + 0.01 * 7.097541332244873
Epoch 130, val loss: 1.391005516052246
Epoch 140, training loss: 1.2919423580169678 = 1.2214393615722656 + 0.01 * 7.050297260284424
Epoch 140, val loss: 1.3237937688827515
Epoch 150, training loss: 1.2060211896896362 = 1.1357624530792236 + 0.01 * 7.025875091552734
Epoch 150, val loss: 1.257736086845398
Epoch 160, training loss: 1.1214959621429443 = 1.0514194965362549 + 0.01 * 7.0076422691345215
Epoch 160, val loss: 1.1935471296310425
Epoch 170, training loss: 1.0395551919937134 = 0.9695959091186523 + 0.01 * 6.9959235191345215
Epoch 170, val loss: 1.1317576169967651
Epoch 180, training loss: 0.9601009488105774 = 0.8902043104171753 + 0.01 * 6.989661693572998
Epoch 180, val loss: 1.071312665939331
Epoch 190, training loss: 0.8824059367179871 = 0.8125388622283936 + 0.01 * 6.986705780029297
Epoch 190, val loss: 1.0116212368011475
Epoch 200, training loss: 0.8067877292633057 = 0.7369509935379028 + 0.01 * 6.983673572540283
Epoch 200, val loss: 0.9537666440010071
Epoch 210, training loss: 0.73499596118927 = 0.6651765704154968 + 0.01 * 6.981939792633057
Epoch 210, val loss: 0.899992048740387
Epoch 220, training loss: 0.6688694953918457 = 0.599064826965332 + 0.01 * 6.980467319488525
Epoch 220, val loss: 0.8528327941894531
Epoch 230, training loss: 0.6090762615203857 = 0.5392791032791138 + 0.01 * 6.97971773147583
Epoch 230, val loss: 0.8139637112617493
Epoch 240, training loss: 0.5551047325134277 = 0.4853185713291168 + 0.01 * 6.978618621826172
Epoch 240, val loss: 0.7836663126945496
Epoch 250, training loss: 0.5059952735900879 = 0.4362162947654724 + 0.01 * 6.977898120880127
Epoch 250, val loss: 0.7610679268836975
Epoch 260, training loss: 0.4606827199459076 = 0.3909219801425934 + 0.01 * 6.976073265075684
Epoch 260, val loss: 0.7446127533912659
Epoch 270, training loss: 0.41840460896492004 = 0.3486620783805847 + 0.01 * 6.974253177642822
Epoch 270, val loss: 0.7324104309082031
Epoch 280, training loss: 0.3789195716381073 = 0.3091870844364166 + 0.01 * 6.973249912261963
Epoch 280, val loss: 0.7235883474349976
Epoch 290, training loss: 0.342518150806427 = 0.27280712127685547 + 0.01 * 6.9711012840271
Epoch 290, val loss: 0.7180618643760681
Epoch 300, training loss: 0.30974245071411133 = 0.24003291130065918 + 0.01 * 6.9709553718566895
Epoch 300, val loss: 0.7160515785217285
Epoch 310, training loss: 0.2808254063129425 = 0.21115046739578247 + 0.01 * 6.967494964599609
Epoch 310, val loss: 0.717549204826355
Epoch 320, training loss: 0.2556992769241333 = 0.18604205548763275 + 0.01 * 6.965723514556885
Epoch 320, val loss: 0.7223770022392273
Epoch 330, training loss: 0.233953595161438 = 0.16432300209999084 + 0.01 * 6.963059425354004
Epoch 330, val loss: 0.7300645709037781
Epoch 340, training loss: 0.2152765542268753 = 0.14553062617778778 + 0.01 * 6.974592685699463
Epoch 340, val loss: 0.7399995923042297
Epoch 350, training loss: 0.19885627925395966 = 0.12923124432563782 + 0.01 * 6.962503910064697
Epoch 350, val loss: 0.7517493963241577
Epoch 360, training loss: 0.18462640047073364 = 0.1150381937623024 + 0.01 * 6.9588212966918945
Epoch 360, val loss: 0.7648128271102905
Epoch 370, training loss: 0.17216771841049194 = 0.10262692719697952 + 0.01 * 6.954078197479248
Epoch 370, val loss: 0.7789129614830017
Epoch 380, training loss: 0.16124659776687622 = 0.09173094481229782 + 0.01 * 6.951565742492676
Epoch 380, val loss: 0.7937082648277283
Epoch 390, training loss: 0.15166878700256348 = 0.08213568478822708 + 0.01 * 6.953309535980225
Epoch 390, val loss: 0.8089988827705383
Epoch 400, training loss: 0.14311625063419342 = 0.07366525381803513 + 0.01 * 6.945099830627441
Epoch 400, val loss: 0.8246106505393982
Epoch 410, training loss: 0.13556867837905884 = 0.06617265939712524 + 0.01 * 6.939601421356201
Epoch 410, val loss: 0.8403981924057007
Epoch 420, training loss: 0.12902091443538666 = 0.05953659489750862 + 0.01 * 6.948432445526123
Epoch 420, val loss: 0.8562670946121216
Epoch 430, training loss: 0.12308147549629211 = 0.05366482958197594 + 0.01 * 6.941664218902588
Epoch 430, val loss: 0.8720219731330872
Epoch 440, training loss: 0.11777447164058685 = 0.048469264060258865 + 0.01 * 6.930521011352539
Epoch 440, val loss: 0.8876551985740662
Epoch 450, training loss: 0.1130940318107605 = 0.04387051984667778 + 0.01 * 6.922350883483887
Epoch 450, val loss: 0.9031170606613159
Epoch 460, training loss: 0.10896085202693939 = 0.03979955613613129 + 0.01 * 6.9161295890808105
Epoch 460, val loss: 0.9183521270751953
Epoch 470, training loss: 0.10532249510288239 = 0.03619660809636116 + 0.01 * 6.912589073181152
Epoch 470, val loss: 0.9332854151725769
Epoch 480, training loss: 0.10206881165504456 = 0.033007364720106125 + 0.01 * 6.906144618988037
Epoch 480, val loss: 0.9478780627250671
Epoch 490, training loss: 0.09910425543785095 = 0.030176106840372086 + 0.01 * 6.892815113067627
Epoch 490, val loss: 0.9621043801307678
Epoch 500, training loss: 0.09739486128091812 = 0.02765444479882717 + 0.01 * 6.97404146194458
Epoch 500, val loss: 0.9759944081306458
Epoch 510, training loss: 0.09422437846660614 = 0.02540811523795128 + 0.01 * 6.881626129150391
Epoch 510, val loss: 0.9894218444824219
Epoch 520, training loss: 0.09213390201330185 = 0.023396773263812065 + 0.01 * 6.87371301651001
Epoch 520, val loss: 1.0024629831314087
Epoch 530, training loss: 0.09025782346725464 = 0.021590102463960648 + 0.01 * 6.866771697998047
Epoch 530, val loss: 1.0151503086090088
Epoch 540, training loss: 0.08892187476158142 = 0.019961465150117874 + 0.01 * 6.896041393280029
Epoch 540, val loss: 1.0274593830108643
Epoch 550, training loss: 0.08706884831190109 = 0.01849403977394104 + 0.01 * 6.857481002807617
Epoch 550, val loss: 1.0392910242080688
Epoch 560, training loss: 0.08581612259149551 = 0.01716991327702999 + 0.01 * 6.864621162414551
Epoch 560, val loss: 1.050739049911499
Epoch 570, training loss: 0.08460734784603119 = 0.015976659953594208 + 0.01 * 6.863068580627441
Epoch 570, val loss: 1.0616878271102905
Epoch 580, training loss: 0.08348158746957779 = 0.01489613763988018 + 0.01 * 6.858544826507568
Epoch 580, val loss: 1.072314977645874
Epoch 590, training loss: 0.0823238417506218 = 0.013919352553784847 + 0.01 * 6.84044885635376
Epoch 590, val loss: 1.0824475288391113
Epoch 600, training loss: 0.08132315427064896 = 0.013031582348048687 + 0.01 * 6.829157829284668
Epoch 600, val loss: 1.0922510623931885
Epoch 610, training loss: 0.08066178113222122 = 0.012225126847624779 + 0.01 * 6.843665599822998
Epoch 610, val loss: 1.1017347574234009
Epoch 620, training loss: 0.07964573800563812 = 0.011490490287542343 + 0.01 * 6.815524578094482
Epoch 620, val loss: 1.1108348369598389
Epoch 630, training loss: 0.07899221032857895 = 0.010822199285030365 + 0.01 * 6.8170013427734375
Epoch 630, val loss: 1.1195423603057861
Epoch 640, training loss: 0.07837100327014923 = 0.010211805813014507 + 0.01 * 6.815919876098633
Epoch 640, val loss: 1.128065586090088
Epoch 650, training loss: 0.07763136178255081 = 0.009654117748141289 + 0.01 * 6.797724723815918
Epoch 650, val loss: 1.1362301111221313
Epoch 660, training loss: 0.07704955339431763 = 0.009144101291894913 + 0.01 * 6.790545463562012
Epoch 660, val loss: 1.144027829170227
Epoch 670, training loss: 0.07658602297306061 = 0.008675523102283478 + 0.01 * 6.791049957275391
Epoch 670, val loss: 1.1515991687774658
Epoch 680, training loss: 0.07653899490833282 = 0.008243401534855366 + 0.01 * 6.829558849334717
Epoch 680, val loss: 1.1589868068695068
Epoch 690, training loss: 0.07568362355232239 = 0.007845076732337475 + 0.01 * 6.783854961395264
Epoch 690, val loss: 1.166025161743164
Epoch 700, training loss: 0.07525643706321716 = 0.007477547042071819 + 0.01 * 6.777889728546143
Epoch 700, val loss: 1.1728523969650269
Epoch 710, training loss: 0.07496946305036545 = 0.007136710919439793 + 0.01 * 6.783275604248047
Epoch 710, val loss: 1.1795506477355957
Epoch 720, training loss: 0.07467278838157654 = 0.006821790710091591 + 0.01 * 6.785099983215332
Epoch 720, val loss: 1.1859045028686523
Epoch 730, training loss: 0.07431171089410782 = 0.006529621779918671 + 0.01 * 6.778209209442139
Epoch 730, val loss: 1.1921226978302002
Epoch 740, training loss: 0.07391414046287537 = 0.0062573812901973724 + 0.01 * 6.765676021575928
Epoch 740, val loss: 1.198038101196289
Epoch 750, training loss: 0.07355697453022003 = 0.006003337912261486 + 0.01 * 6.755363941192627
Epoch 750, val loss: 1.2039167881011963
Epoch 760, training loss: 0.07332812249660492 = 0.005765900947153568 + 0.01 * 6.756222724914551
Epoch 760, val loss: 1.2095521688461304
Epoch 770, training loss: 0.07297798246145248 = 0.005543916020542383 + 0.01 * 6.743406295776367
Epoch 770, val loss: 1.2150461673736572
Epoch 780, training loss: 0.07299448549747467 = 0.005335676483809948 + 0.01 * 6.765881061553955
Epoch 780, val loss: 1.2204169034957886
Epoch 790, training loss: 0.07269316166639328 = 0.005140542984008789 + 0.01 * 6.7552618980407715
Epoch 790, val loss: 1.225478172302246
Epoch 800, training loss: 0.07228433340787888 = 0.0049576060846447945 + 0.01 * 6.732672691345215
Epoch 800, val loss: 1.2305511236190796
Epoch 810, training loss: 0.07210756838321686 = 0.004786073695868254 + 0.01 * 6.732149600982666
Epoch 810, val loss: 1.2353148460388184
Epoch 820, training loss: 0.07189951092004776 = 0.004624407272785902 + 0.01 * 6.727510929107666
Epoch 820, val loss: 1.2400436401367188
Epoch 830, training loss: 0.07190961390733719 = 0.004471799358725548 + 0.01 * 6.743781089782715
Epoch 830, val loss: 1.244646668434143
Epoch 840, training loss: 0.07161879539489746 = 0.004327932372689247 + 0.01 * 6.729086875915527
Epoch 840, val loss: 1.2490406036376953
Epoch 850, training loss: 0.07146113365888596 = 0.0041919006034731865 + 0.01 * 6.726923942565918
Epoch 850, val loss: 1.2534151077270508
Epoch 860, training loss: 0.07136458158493042 = 0.004063132219016552 + 0.01 * 6.730145454406738
Epoch 860, val loss: 1.2575974464416504
Epoch 870, training loss: 0.07107855379581451 = 0.003941111266613007 + 0.01 * 6.713744640350342
Epoch 870, val loss: 1.2616956233978271
Epoch 880, training loss: 0.07117177546024323 = 0.003825386054813862 + 0.01 * 6.734638690948486
Epoch 880, val loss: 1.265679121017456
Epoch 890, training loss: 0.07103439420461655 = 0.0037156622856855392 + 0.01 * 6.731873512268066
Epoch 890, val loss: 1.2695136070251465
Epoch 900, training loss: 0.0706862360239029 = 0.0036113904789090157 + 0.01 * 6.707484245300293
Epoch 900, val loss: 1.2732936143875122
Epoch 910, training loss: 0.07088964432477951 = 0.0035123606212437153 + 0.01 * 6.737728595733643
Epoch 910, val loss: 1.2769596576690674
Epoch 920, training loss: 0.07059332728385925 = 0.0034184681717306376 + 0.01 * 6.717486381530762
Epoch 920, val loss: 1.2804852724075317
Epoch 930, training loss: 0.07033570855855942 = 0.0033290681894868612 + 0.01 * 6.700664043426514
Epoch 930, val loss: 1.2839338779449463
Epoch 940, training loss: 0.07028432935476303 = 0.003243657760322094 + 0.01 * 6.704067707061768
Epoch 940, val loss: 1.2873436212539673
Epoch 950, training loss: 0.07050345093011856 = 0.0031621092930436134 + 0.01 * 6.734134197235107
Epoch 950, val loss: 1.29062020778656
Epoch 960, training loss: 0.07002213597297668 = 0.0030844577122479677 + 0.01 * 6.69376802444458
Epoch 960, val loss: 1.293802261352539
Epoch 970, training loss: 0.06999388337135315 = 0.003010326065123081 + 0.01 * 6.6983561515808105
Epoch 970, val loss: 1.2969176769256592
Epoch 980, training loss: 0.07007147371768951 = 0.002939479658380151 + 0.01 * 6.713200092315674
Epoch 980, val loss: 1.2999387979507446
Epoch 990, training loss: 0.06982950866222382 = 0.0028717806562781334 + 0.01 * 6.695772647857666
Epoch 990, val loss: 1.3028993606567383
Epoch 1000, training loss: 0.07002854347229004 = 0.002806976670399308 + 0.01 * 6.722157001495361
Epoch 1000, val loss: 1.305794596672058
Epoch 1010, training loss: 0.06983668357133865 = 0.0027449005283415318 + 0.01 * 6.709178924560547
Epoch 1010, val loss: 1.3085484504699707
Epoch 1020, training loss: 0.06956888735294342 = 0.00268541369587183 + 0.01 * 6.688347816467285
Epoch 1020, val loss: 1.311285376548767
Epoch 1030, training loss: 0.06943129003047943 = 0.002628287533298135 + 0.01 * 6.680300235748291
Epoch 1030, val loss: 1.3139710426330566
Epoch 1040, training loss: 0.0693550705909729 = 0.002573520177975297 + 0.01 * 6.678155422210693
Epoch 1040, val loss: 1.3165507316589355
Epoch 1050, training loss: 0.06921233981847763 = 0.002521032001823187 + 0.01 * 6.669131278991699
Epoch 1050, val loss: 1.3190937042236328
Epoch 1060, training loss: 0.0693833976984024 = 0.002470611361786723 + 0.01 * 6.691278457641602
Epoch 1060, val loss: 1.3215343952178955
Epoch 1070, training loss: 0.06920026987791061 = 0.002422160701826215 + 0.01 * 6.6778106689453125
Epoch 1070, val loss: 1.3239561319351196
Epoch 1080, training loss: 0.06905362010002136 = 0.0023756036534905434 + 0.01 * 6.667801856994629
Epoch 1080, val loss: 1.3262579441070557
Epoch 1090, training loss: 0.0692179799079895 = 0.002330845920369029 + 0.01 * 6.688713550567627
Epoch 1090, val loss: 1.3285536766052246
Epoch 1100, training loss: 0.06907260417938232 = 0.0022877994924783707 + 0.01 * 6.678480625152588
Epoch 1100, val loss: 1.3307135105133057
Epoch 1110, training loss: 0.0688934400677681 = 0.002246358199045062 + 0.01 * 6.664708614349365
Epoch 1110, val loss: 1.3328709602355957
Epoch 1120, training loss: 0.06879910081624985 = 0.00220644474029541 + 0.01 * 6.659265995025635
Epoch 1120, val loss: 1.3349580764770508
Epoch 1130, training loss: 0.0687948688864708 = 0.0021678993944078684 + 0.01 * 6.662696838378906
Epoch 1130, val loss: 1.336964726448059
Epoch 1140, training loss: 0.06861848384141922 = 0.002130720531567931 + 0.01 * 6.648776531219482
Epoch 1140, val loss: 1.3389545679092407
Epoch 1150, training loss: 0.06876541674137115 = 0.0020948515739291906 + 0.01 * 6.667057037353516
Epoch 1150, val loss: 1.3408453464508057
Epoch 1160, training loss: 0.06875908374786377 = 0.0020603155717253685 + 0.01 * 6.669877052307129
Epoch 1160, val loss: 1.3427125215530396
Epoch 1170, training loss: 0.06869295984506607 = 0.0020270065870136023 + 0.01 * 6.666595458984375
Epoch 1170, val loss: 1.344515323638916
Epoch 1180, training loss: 0.06868324428796768 = 0.0019948009867221117 + 0.01 * 6.668844223022461
Epoch 1180, val loss: 1.3462930917739868
Epoch 1190, training loss: 0.06850891560316086 = 0.001963664311915636 + 0.01 * 6.654524803161621
Epoch 1190, val loss: 1.3480384349822998
Epoch 1200, training loss: 0.06833312660455704 = 0.0019336059922352433 + 0.01 * 6.639952182769775
Epoch 1200, val loss: 1.3496818542480469
Epoch 1210, training loss: 0.06822384148836136 = 0.0019045453518629074 + 0.01 * 6.631929397583008
Epoch 1210, val loss: 1.3512816429138184
Epoch 1220, training loss: 0.06854256242513657 = 0.001876356196589768 + 0.01 * 6.666621208190918
Epoch 1220, val loss: 1.3529102802276611
Epoch 1230, training loss: 0.06850437074899673 = 0.0018491055816411972 + 0.01 * 6.665526390075684
Epoch 1230, val loss: 1.3544358015060425
Epoch 1240, training loss: 0.06825309991836548 = 0.001822778140194714 + 0.01 * 6.643032550811768
Epoch 1240, val loss: 1.3559448719024658
Epoch 1250, training loss: 0.06820352375507355 = 0.0017972763162106276 + 0.01 * 6.640625476837158
Epoch 1250, val loss: 1.3573720455169678
Epoch 1260, training loss: 0.06813220679759979 = 0.0017725194338709116 + 0.01 * 6.6359686851501465
Epoch 1260, val loss: 1.3587777614593506
Epoch 1270, training loss: 0.06796877831220627 = 0.0017484790878370404 + 0.01 * 6.622029781341553
Epoch 1270, val loss: 1.3601667881011963
Epoch 1280, training loss: 0.06789889931678772 = 0.001725170761346817 + 0.01 * 6.617372512817383
Epoch 1280, val loss: 1.3614927530288696
Epoch 1290, training loss: 0.06814663857221603 = 0.001702569774352014 + 0.01 * 6.644407272338867
Epoch 1290, val loss: 1.3628695011138916
Epoch 1300, training loss: 0.06806455552577972 = 0.0016807235078886151 + 0.01 * 6.638383388519287
Epoch 1300, val loss: 1.3640062808990479
Epoch 1310, training loss: 0.06776602566242218 = 0.0016594954067841172 + 0.01 * 6.610652923583984
Epoch 1310, val loss: 1.3652609586715698
Epoch 1320, training loss: 0.06791200488805771 = 0.0016388450749218464 + 0.01 * 6.627316474914551
Epoch 1320, val loss: 1.3665064573287964
Epoch 1330, training loss: 0.06793032586574554 = 0.0016188155859708786 + 0.01 * 6.631150722503662
Epoch 1330, val loss: 1.3675180673599243
Epoch 1340, training loss: 0.06771184504032135 = 0.0015993166016414762 + 0.01 * 6.611252784729004
Epoch 1340, val loss: 1.3687725067138672
Epoch 1350, training loss: 0.0679408386349678 = 0.0015803668648004532 + 0.01 * 6.63604736328125
Epoch 1350, val loss: 1.369852066040039
Epoch 1360, training loss: 0.06753847002983093 = 0.0015619888436049223 + 0.01 * 6.597647666931152
Epoch 1360, val loss: 1.3708765506744385
Epoch 1370, training loss: 0.06755106896162033 = 0.0015441387658938766 + 0.01 * 6.600693225860596
Epoch 1370, val loss: 1.3719556331634521
Epoch 1380, training loss: 0.0676838755607605 = 0.0015268035931512713 + 0.01 * 6.6157073974609375
Epoch 1380, val loss: 1.3728960752487183
Epoch 1390, training loss: 0.06751388311386108 = 0.0015099230222404003 + 0.01 * 6.600396633148193
Epoch 1390, val loss: 1.3738083839416504
Epoch 1400, training loss: 0.06735444068908691 = 0.0014934113714843988 + 0.01 * 6.586103439331055
Epoch 1400, val loss: 1.3748215436935425
Epoch 1410, training loss: 0.06742469221353531 = 0.001477356650866568 + 0.01 * 6.594734191894531
Epoch 1410, val loss: 1.375711441040039
Epoch 1420, training loss: 0.06728033721446991 = 0.001461719861254096 + 0.01 * 6.58186149597168
Epoch 1420, val loss: 1.3765342235565186
Epoch 1430, training loss: 0.06729668378829956 = 0.0014464787673205137 + 0.01 * 6.585020542144775
Epoch 1430, val loss: 1.3774523735046387
Epoch 1440, training loss: 0.0674455389380455 = 0.0014316713204607368 + 0.01 * 6.601387023925781
Epoch 1440, val loss: 1.378158450126648
Epoch 1450, training loss: 0.06739544868469238 = 0.0014172327937558293 + 0.01 * 6.597821235656738
Epoch 1450, val loss: 1.3790092468261719
Epoch 1460, training loss: 0.06727040559053421 = 0.0014031489845365286 + 0.01 * 6.586726188659668
Epoch 1460, val loss: 1.379713535308838
Epoch 1470, training loss: 0.0672122985124588 = 0.0013893920695409179 + 0.01 * 6.5822906494140625
Epoch 1470, val loss: 1.3805509805679321
Epoch 1480, training loss: 0.06722341477870941 = 0.0013760010479018092 + 0.01 * 6.584741592407227
Epoch 1480, val loss: 1.3812463283538818
Epoch 1490, training loss: 0.06716325879096985 = 0.0013629287714138627 + 0.01 * 6.580033302307129
Epoch 1490, val loss: 1.3819332122802734
Epoch 1500, training loss: 0.06706482172012329 = 0.0013501450885087252 + 0.01 * 6.571467876434326
Epoch 1500, val loss: 1.3826048374176025
Epoch 1510, training loss: 0.06730382889509201 = 0.0013376560527831316 + 0.01 * 6.596617698669434
Epoch 1510, val loss: 1.3832918405532837
Epoch 1520, training loss: 0.06709607690572739 = 0.0013254511868581176 + 0.01 * 6.577062606811523
Epoch 1520, val loss: 1.383934497833252
Epoch 1530, training loss: 0.06691668182611465 = 0.0013135556364431977 + 0.01 * 6.5603132247924805
Epoch 1530, val loss: 1.384629249572754
Epoch 1540, training loss: 0.06684505194425583 = 0.0013019333127886057 + 0.01 * 6.554311752319336
Epoch 1540, val loss: 1.3851765394210815
Epoch 1550, training loss: 0.06718089431524277 = 0.0012905426556244493 + 0.01 * 6.5890350341796875
Epoch 1550, val loss: 1.385781168937683
Epoch 1560, training loss: 0.06682644039392471 = 0.001279422314837575 + 0.01 * 6.554701805114746
Epoch 1560, val loss: 1.3863965272903442
Epoch 1570, training loss: 0.06695157289505005 = 0.001268557389266789 + 0.01 * 6.568301677703857
Epoch 1570, val loss: 1.3869367837905884
Epoch 1580, training loss: 0.06699439883232117 = 0.0012579523026943207 + 0.01 * 6.573644638061523
Epoch 1580, val loss: 1.3874993324279785
Epoch 1590, training loss: 0.06678418815135956 = 0.0012475718976929784 + 0.01 * 6.553661823272705
Epoch 1590, val loss: 1.3880518674850464
Epoch 1600, training loss: 0.0668901801109314 = 0.0012374166399240494 + 0.01 * 6.565276622772217
Epoch 1600, val loss: 1.3884786367416382
Epoch 1610, training loss: 0.06701187044382095 = 0.0012274218024685979 + 0.01 * 6.578444957733154
Epoch 1610, val loss: 1.3890893459320068
Epoch 1620, training loss: 0.06686566770076752 = 0.0012176623567938805 + 0.01 * 6.56480073928833
Epoch 1620, val loss: 1.3895454406738281
Epoch 1630, training loss: 0.06682021915912628 = 0.001208148431032896 + 0.01 * 6.561207294464111
Epoch 1630, val loss: 1.3899800777435303
Epoch 1640, training loss: 0.06675124913454056 = 0.0011987840989604592 + 0.01 * 6.555246829986572
Epoch 1640, val loss: 1.3904861211776733
Epoch 1650, training loss: 0.06676315516233444 = 0.0011896209325641394 + 0.01 * 6.557353973388672
Epoch 1650, val loss: 1.3909602165222168
Epoch 1660, training loss: 0.06678763777017593 = 0.0011806344846263528 + 0.01 * 6.560700416564941
Epoch 1660, val loss: 1.3912980556488037
Epoch 1670, training loss: 0.06657880544662476 = 0.0011717976303771138 + 0.01 * 6.540700912475586
Epoch 1670, val loss: 1.3918200731277466
Epoch 1680, training loss: 0.06670060753822327 = 0.001163178007118404 + 0.01 * 6.553743362426758
Epoch 1680, val loss: 1.392166256904602
Epoch 1690, training loss: 0.066878542304039 = 0.001154727186076343 + 0.01 * 6.572381496429443
Epoch 1690, val loss: 1.3925628662109375
Epoch 1700, training loss: 0.06671932339668274 = 0.0011464401613920927 + 0.01 * 6.55728816986084
Epoch 1700, val loss: 1.3929414749145508
Epoch 1710, training loss: 0.06652116775512695 = 0.0011383037781342864 + 0.01 * 6.538286209106445
Epoch 1710, val loss: 1.3933454751968384
Epoch 1720, training loss: 0.0665707141160965 = 0.0011303568026050925 + 0.01 * 6.544035911560059
Epoch 1720, val loss: 1.3936847448349
Epoch 1730, training loss: 0.06643953919410706 = 0.0011225085472688079 + 0.01 * 6.531702995300293
Epoch 1730, val loss: 1.3940179347991943
Epoch 1740, training loss: 0.06670014560222626 = 0.0011148229241371155 + 0.01 * 6.558532238006592
Epoch 1740, val loss: 1.3944591283798218
Epoch 1750, training loss: 0.06651759147644043 = 0.001107282703742385 + 0.01 * 6.541031360626221
Epoch 1750, val loss: 1.3946986198425293
Epoch 1760, training loss: 0.06640738248825073 = 0.0010998897487297654 + 0.01 * 6.530749320983887
Epoch 1760, val loss: 1.395031452178955
Epoch 1770, training loss: 0.0665922537446022 = 0.0010926249669864774 + 0.01 * 6.549962997436523
Epoch 1770, val loss: 1.3953773975372314
Epoch 1780, training loss: 0.06639638543128967 = 0.0010854883585125208 + 0.01 * 6.531089782714844
Epoch 1780, val loss: 1.3956916332244873
Epoch 1790, training loss: 0.06649697571992874 = 0.001078495872206986 + 0.01 * 6.541848659515381
Epoch 1790, val loss: 1.3959311246871948
Epoch 1800, training loss: 0.06627277284860611 = 0.001071605714969337 + 0.01 * 6.520117282867432
Epoch 1800, val loss: 1.3962891101837158
Epoch 1810, training loss: 0.06628953665494919 = 0.0010648472234606743 + 0.01 * 6.522469520568848
Epoch 1810, val loss: 1.3965857028961182
Epoch 1820, training loss: 0.06646133959293365 = 0.001058215624652803 + 0.01 * 6.540312767028809
Epoch 1820, val loss: 1.3968031406402588
Epoch 1830, training loss: 0.06621477007865906 = 0.0010516948532313108 + 0.01 * 6.516307830810547
Epoch 1830, val loss: 1.3971036672592163
Epoch 1840, training loss: 0.06623481214046478 = 0.0010452686110511422 + 0.01 * 6.518954277038574
Epoch 1840, val loss: 1.3973748683929443
Epoch 1850, training loss: 0.0664326548576355 = 0.0010389507515355945 + 0.01 * 6.539370536804199
Epoch 1850, val loss: 1.3975718021392822
Epoch 1860, training loss: 0.06617218255996704 = 0.001032741623930633 + 0.01 * 6.513944149017334
Epoch 1860, val loss: 1.397819995880127
Epoch 1870, training loss: 0.06632775068283081 = 0.001026623067446053 + 0.01 * 6.530113220214844
Epoch 1870, val loss: 1.3980635404586792
Epoch 1880, training loss: 0.06636730581521988 = 0.0010206432780250907 + 0.01 * 6.534667015075684
Epoch 1880, val loss: 1.3983045816421509
Epoch 1890, training loss: 0.06613674759864807 = 0.0010147198336198926 + 0.01 * 6.512203216552734
Epoch 1890, val loss: 1.3984488248825073
Epoch 1900, training loss: 0.06611276417970657 = 0.001008888822980225 + 0.01 * 6.510387420654297
Epoch 1900, val loss: 1.3987739086151123
Epoch 1910, training loss: 0.0661783441901207 = 0.0010032013524323702 + 0.01 * 6.517514705657959
Epoch 1910, val loss: 1.3988947868347168
Epoch 1920, training loss: 0.06652042269706726 = 0.000997604220174253 + 0.01 * 6.552281856536865
Epoch 1920, val loss: 1.3990904092788696
Epoch 1930, training loss: 0.06605669856071472 = 0.0009920524898916483 + 0.01 * 6.50646448135376
Epoch 1930, val loss: 1.3993473052978516
Epoch 1940, training loss: 0.06594642251729965 = 0.0009866119362413883 + 0.01 * 6.495981216430664
Epoch 1940, val loss: 1.3995357751846313
Epoch 1950, training loss: 0.06634197384119034 = 0.0009812634671106935 + 0.01 * 6.536071300506592
Epoch 1950, val loss: 1.3996931314468384
Epoch 1960, training loss: 0.06603813171386719 = 0.0009759487002156675 + 0.01 * 6.506218433380127
Epoch 1960, val loss: 1.3998719453811646
Epoch 1970, training loss: 0.06588144600391388 = 0.0009707006393000484 + 0.01 * 6.491075038909912
Epoch 1970, val loss: 1.4001212120056152
Epoch 1980, training loss: 0.06624428182840347 = 0.0009655540343374014 + 0.01 * 6.5278730392456055
Epoch 1980, val loss: 1.4002752304077148
Epoch 1990, training loss: 0.06599856168031693 = 0.0009604539372958243 + 0.01 * 6.503810882568359
Epoch 1990, val loss: 1.400454044342041
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8392198207696363
The final CL Acc:0.81358, 0.01222, The final GNN Acc:0.83676, 0.00179
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11562])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10482])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0534727573394775 = 1.9675040245056152 + 0.01 * 8.5968656539917
Epoch 0, val loss: 1.9666637182235718
Epoch 10, training loss: 2.0421950817108154 = 1.9562265872955322 + 0.01 * 8.596846580505371
Epoch 10, val loss: 1.9552092552185059
Epoch 20, training loss: 2.0289721488952637 = 1.943005084991455 + 0.01 * 8.596718788146973
Epoch 20, val loss: 1.941681146621704
Epoch 30, training loss: 2.010932683944702 = 1.9249696731567383 + 0.01 * 8.59630298614502
Epoch 30, val loss: 1.9232263565063477
Epoch 40, training loss: 1.9846761226654053 = 1.898735523223877 + 0.01 * 8.594062805175781
Epoch 40, val loss: 1.896580696105957
Epoch 50, training loss: 1.9479560852050781 = 1.8621933460235596 + 0.01 * 8.576276779174805
Epoch 50, val loss: 1.8607784509658813
Epoch 60, training loss: 1.905893325805664 = 1.820997714996338 + 0.01 * 8.4895601272583
Epoch 60, val loss: 1.824444055557251
Epoch 70, training loss: 1.8696198463439941 = 1.7882198095321655 + 0.01 * 8.140003204345703
Epoch 70, val loss: 1.799728274345398
Epoch 80, training loss: 1.8346054553985596 = 1.7540100812911987 + 0.01 * 8.059541702270508
Epoch 80, val loss: 1.7715786695480347
Epoch 90, training loss: 1.7874212265014648 = 1.7079544067382812 + 0.01 * 7.946678161621094
Epoch 90, val loss: 1.7324562072753906
Epoch 100, training loss: 1.7216274738311768 = 1.644030213356018 + 0.01 * 7.759725093841553
Epoch 100, val loss: 1.6789169311523438
Epoch 110, training loss: 1.6382025480270386 = 1.5626331567764282 + 0.01 * 7.556944847106934
Epoch 110, val loss: 1.6117150783538818
Epoch 120, training loss: 1.5480550527572632 = 1.4739242792129517 + 0.01 * 7.413073539733887
Epoch 120, val loss: 1.5414745807647705
Epoch 130, training loss: 1.4593642950057983 = 1.385902762413025 + 0.01 * 7.346153259277344
Epoch 130, val loss: 1.4755662679672241
Epoch 140, training loss: 1.3724039793014526 = 1.2995213270187378 + 0.01 * 7.28826904296875
Epoch 140, val loss: 1.4143372774124146
Epoch 150, training loss: 1.2867761850357056 = 1.2143150568008423 + 0.01 * 7.246118068695068
Epoch 150, val loss: 1.355293869972229
Epoch 160, training loss: 1.2036468982696533 = 1.1313453912734985 + 0.01 * 7.230151176452637
Epoch 160, val loss: 1.2982404232025146
Epoch 170, training loss: 1.124230146408081 = 1.0519790649414062 + 0.01 * 7.225109100341797
Epoch 170, val loss: 1.2435237169265747
Epoch 180, training loss: 1.0490409135818481 = 0.9768108129501343 + 0.01 * 7.223015308380127
Epoch 180, val loss: 1.1924406290054321
Epoch 190, training loss: 0.9770610928535461 = 0.9048264026641846 + 0.01 * 7.2234697341918945
Epoch 190, val loss: 1.1443537473678589
Epoch 200, training loss: 0.9064491391181946 = 0.8342087268829346 + 0.01 * 7.224040985107422
Epoch 200, val loss: 1.097313404083252
Epoch 210, training loss: 0.8361445665359497 = 0.7639046311378479 + 0.01 * 7.2239909172058105
Epoch 210, val loss: 1.0502108335494995
Epoch 220, training loss: 0.7664048075675964 = 0.6941797137260437 + 0.01 * 7.222507476806641
Epoch 220, val loss: 1.0032411813735962
Epoch 230, training loss: 0.6982515454292297 = 0.6260594725608826 + 0.01 * 7.219207286834717
Epoch 230, val loss: 0.9574568867683411
Epoch 240, training loss: 0.6326613426208496 = 0.5605181455612183 + 0.01 * 7.214323043823242
Epoch 240, val loss: 0.9153875708580017
Epoch 250, training loss: 0.5706499814987183 = 0.4985617399215698 + 0.01 * 7.208822727203369
Epoch 250, val loss: 0.8793313503265381
Epoch 260, training loss: 0.5133002400398254 = 0.4412839114665985 + 0.01 * 7.201630592346191
Epoch 260, val loss: 0.8508692383766174
Epoch 270, training loss: 0.4617554843425751 = 0.3897661864757538 + 0.01 * 7.1989288330078125
Epoch 270, val loss: 0.8303760886192322
Epoch 280, training loss: 0.4163963496685028 = 0.3445380926132202 + 0.01 * 7.185825347900391
Epoch 280, val loss: 0.8172968029975891
Epoch 290, training loss: 0.3771499991416931 = 0.3054051995277405 + 0.01 * 7.174481391906738
Epoch 290, val loss: 0.8104432225227356
Epoch 300, training loss: 0.34327179193496704 = 0.2716653347015381 + 0.01 * 7.160645961761475
Epoch 300, val loss: 0.8085238933563232
Epoch 310, training loss: 0.3138562738895416 = 0.24239641427993774 + 0.01 * 7.145986557006836
Epoch 310, val loss: 0.8103861808776855
Epoch 320, training loss: 0.28802239894866943 = 0.21674098074436188 + 0.01 * 7.128141403198242
Epoch 320, val loss: 0.8150515556335449
Epoch 330, training loss: 0.2651093304157257 = 0.1940142959356308 + 0.01 * 7.109503746032715
Epoch 330, val loss: 0.8218618631362915
Epoch 340, training loss: 0.2446797788143158 = 0.17374278604984283 + 0.01 * 7.0936994552612305
Epoch 340, val loss: 0.8303188681602478
Epoch 350, training loss: 0.22645437717437744 = 0.15561369061470032 + 0.01 * 7.08406925201416
Epoch 350, val loss: 0.8400379419326782
Epoch 360, training loss: 0.21041101217269897 = 0.13938386738300323 + 0.01 * 7.102715492248535
Epoch 360, val loss: 0.8507276773452759
Epoch 370, training loss: 0.19554653763771057 = 0.12487616389989853 + 0.01 * 7.0670366287231445
Epoch 370, val loss: 0.8621837496757507
Epoch 380, training loss: 0.1824159324169159 = 0.11189726740121841 + 0.01 * 7.051867485046387
Epoch 380, val loss: 0.8742490410804749
Epoch 390, training loss: 0.1706877052783966 = 0.10027819126844406 + 0.01 * 7.040951251983643
Epoch 390, val loss: 0.8867639899253845
Epoch 400, training loss: 0.16023442149162292 = 0.0898810401558876 + 0.01 * 7.035337924957275
Epoch 400, val loss: 0.8996118903160095
Epoch 410, training loss: 0.15088513493537903 = 0.08058734983205795 + 0.01 * 7.029778480529785
Epoch 410, val loss: 0.9126437902450562
Epoch 420, training loss: 0.1424986720085144 = 0.07228390872478485 + 0.01 * 7.021475791931152
Epoch 420, val loss: 0.9258180856704712
Epoch 430, training loss: 0.13513928651809692 = 0.06488250941038132 + 0.01 * 7.025677680969238
Epoch 430, val loss: 0.9391568899154663
Epoch 440, training loss: 0.12836499512195587 = 0.05830628052353859 + 0.01 * 7.0058722496032715
Epoch 440, val loss: 0.9524808526039124
Epoch 450, training loss: 0.12248613685369492 = 0.052476778626441956 + 0.01 * 7.000936031341553
Epoch 450, val loss: 0.9657543301582336
Epoch 460, training loss: 0.11726963520050049 = 0.04732266813516617 + 0.01 * 6.994697093963623
Epoch 460, val loss: 0.9789335131645203
Epoch 470, training loss: 0.11264880001544952 = 0.042770955711603165 + 0.01 * 6.9877848625183105
Epoch 470, val loss: 0.9919382929801941
Epoch 480, training loss: 0.1086680144071579 = 0.038757238537073135 + 0.01 * 6.991077899932861
Epoch 480, val loss: 1.0047303438186646
Epoch 490, training loss: 0.10506105422973633 = 0.03522128984332085 + 0.01 * 6.9839768409729
Epoch 490, val loss: 1.017198920249939
Epoch 500, training loss: 0.10182460397481918 = 0.03209850192070007 + 0.01 * 6.9726104736328125
Epoch 500, val loss: 1.0293790102005005
Epoch 510, training loss: 0.09899258613586426 = 0.029335083439946175 + 0.01 * 6.965750694274902
Epoch 510, val loss: 1.0412914752960205
Epoch 520, training loss: 0.09649114310741425 = 0.02688409574329853 + 0.01 * 6.960705280303955
Epoch 520, val loss: 1.0529117584228516
Epoch 530, training loss: 0.09432399272918701 = 0.02470528893172741 + 0.01 * 6.9618706703186035
Epoch 530, val loss: 1.0642404556274414
Epoch 540, training loss: 0.0923222154378891 = 0.022765465080738068 + 0.01 * 6.95567512512207
Epoch 540, val loss: 1.0751655101776123
Epoch 550, training loss: 0.0905560776591301 = 0.02103469707071781 + 0.01 * 6.952138423919678
Epoch 550, val loss: 1.0858417749404907
Epoch 560, training loss: 0.08890166878700256 = 0.019485846161842346 + 0.01 * 6.941582202911377
Epoch 560, val loss: 1.0961246490478516
Epoch 570, training loss: 0.08748079091310501 = 0.01809576340019703 + 0.01 * 6.938503265380859
Epoch 570, val loss: 1.1060880422592163
Epoch 580, training loss: 0.08614690601825714 = 0.016846204176545143 + 0.01 * 6.930070877075195
Epoch 580, val loss: 1.1157734394073486
Epoch 590, training loss: 0.08503427356481552 = 0.015719903632998466 + 0.01 * 6.931437015533447
Epoch 590, val loss: 1.1250909566879272
Epoch 600, training loss: 0.08394193649291992 = 0.014701597392559052 + 0.01 * 6.924034118652344
Epoch 600, val loss: 1.1341081857681274
Epoch 610, training loss: 0.0830397680401802 = 0.013778804801404476 + 0.01 * 6.926096439361572
Epoch 610, val loss: 1.142879605293274
Epoch 620, training loss: 0.08211660385131836 = 0.012941606342792511 + 0.01 * 6.917500019073486
Epoch 620, val loss: 1.1513607501983643
Epoch 630, training loss: 0.08128286153078079 = 0.012179567478597164 + 0.01 * 6.910329341888428
Epoch 630, val loss: 1.1595377922058105
Epoch 640, training loss: 0.08070719987154007 = 0.011484600603580475 + 0.01 * 6.92225980758667
Epoch 640, val loss: 1.1674840450286865
Epoch 650, training loss: 0.07986477762460709 = 0.010850059799849987 + 0.01 * 6.901472568511963
Epoch 650, val loss: 1.1751757860183716
Epoch 660, training loss: 0.0791880190372467 = 0.0102689852938056 + 0.01 * 6.891903400421143
Epoch 660, val loss: 1.1825737953186035
Epoch 670, training loss: 0.07873187214136124 = 0.009735616855323315 + 0.01 * 6.899625778198242
Epoch 670, val loss: 1.1897822618484497
Epoch 680, training loss: 0.07816644757986069 = 0.009245174005627632 + 0.01 * 6.892127990722656
Epoch 680, val loss: 1.1967445611953735
Epoch 690, training loss: 0.07764673978090286 = 0.008793310262262821 + 0.01 * 6.885343074798584
Epoch 690, val loss: 1.2034856081008911
Epoch 700, training loss: 0.07718690484762192 = 0.008376206271350384 + 0.01 * 6.881070613861084
Epoch 700, val loss: 1.2100099325180054
Epoch 710, training loss: 0.0770120769739151 = 0.007990038022398949 + 0.01 * 6.902203559875488
Epoch 710, val loss: 1.2164033651351929
Epoch 720, training loss: 0.07633721828460693 = 0.0076325153931975365 + 0.01 * 6.8704705238342285
Epoch 720, val loss: 1.2225861549377441
Epoch 730, training loss: 0.07597563415765762 = 0.0073008909821510315 + 0.01 * 6.867474555969238
Epoch 730, val loss: 1.2285798788070679
Epoch 740, training loss: 0.07571634650230408 = 0.0069927312433719635 + 0.01 * 6.872361660003662
Epoch 740, val loss: 1.23442804813385
Epoch 750, training loss: 0.0753054991364479 = 0.006705579813569784 + 0.01 * 6.859992027282715
Epoch 750, val loss: 1.2400416135787964
Epoch 760, training loss: 0.0749560073018074 = 0.006436978466808796 + 0.01 * 6.851902961730957
Epoch 760, val loss: 1.2455534934997559
Epoch 770, training loss: 0.07475955784320831 = 0.006186658516526222 + 0.01 * 6.857290267944336
Epoch 770, val loss: 1.2509058713912964
Epoch 780, training loss: 0.07436937838792801 = 0.005952195264399052 + 0.01 * 6.8417181968688965
Epoch 780, val loss: 1.256137490272522
Epoch 790, training loss: 0.07412070035934448 = 0.00573193421587348 + 0.01 * 6.838877201080322
Epoch 790, val loss: 1.2611777782440186
Epoch 800, training loss: 0.07406757026910782 = 0.0055252606980502605 + 0.01 * 6.854230880737305
Epoch 800, val loss: 1.2660729885101318
Epoch 810, training loss: 0.07371823489665985 = 0.005331339314579964 + 0.01 * 6.838689804077148
Epoch 810, val loss: 1.2708817720413208
Epoch 820, training loss: 0.07346973568201065 = 0.005149271804839373 + 0.01 * 6.832046031951904
Epoch 820, val loss: 1.2754963636398315
Epoch 830, training loss: 0.07323174178600311 = 0.004977278411388397 + 0.01 * 6.825446128845215
Epoch 830, val loss: 1.2800893783569336
Epoch 840, training loss: 0.07309971004724503 = 0.004815360996872187 + 0.01 * 6.828434944152832
Epoch 840, val loss: 1.2844529151916504
Epoch 850, training loss: 0.07283186912536621 = 0.004662432707846165 + 0.01 * 6.816944122314453
Epoch 850, val loss: 1.2887989282608032
Epoch 860, training loss: 0.07262678444385529 = 0.00451810983940959 + 0.01 * 6.810867786407471
Epoch 860, val loss: 1.2929755449295044
Epoch 870, training loss: 0.07263994216918945 = 0.004381643608212471 + 0.01 * 6.825829982757568
Epoch 870, val loss: 1.2970244884490967
Epoch 880, training loss: 0.07232967019081116 = 0.004252077545970678 + 0.01 * 6.807759761810303
Epoch 880, val loss: 1.301068663597107
Epoch 890, training loss: 0.07227826863527298 = 0.004129477776587009 + 0.01 * 6.814879417419434
Epoch 890, val loss: 1.3049384355545044
Epoch 900, training loss: 0.07205352932214737 = 0.004013446159660816 + 0.01 * 6.8040080070495605
Epoch 900, val loss: 1.3087143898010254
Epoch 910, training loss: 0.07194137573242188 = 0.003903013886883855 + 0.01 * 6.803836345672607
Epoch 910, val loss: 1.312398076057434
Epoch 920, training loss: 0.07178764790296555 = 0.0037981749046593904 + 0.01 * 6.798947811126709
Epoch 920, val loss: 1.3159606456756592
Epoch 930, training loss: 0.07165534049272537 = 0.003698401851579547 + 0.01 * 6.795694351196289
Epoch 930, val loss: 1.319498062133789
Epoch 940, training loss: 0.07142043858766556 = 0.003603540826588869 + 0.01 * 6.781689643859863
Epoch 940, val loss: 1.3229507207870483
Epoch 950, training loss: 0.07134635746479034 = 0.0035130870528519154 + 0.01 * 6.783327102661133
Epoch 950, val loss: 1.3262056112289429
Epoch 960, training loss: 0.07111887633800507 = 0.0034266405273228884 + 0.01 * 6.769223690032959
Epoch 960, val loss: 1.329550862312317
Epoch 970, training loss: 0.07113233953714371 = 0.0033441483974456787 + 0.01 * 6.7788190841674805
Epoch 970, val loss: 1.3327528238296509
Epoch 980, training loss: 0.07094756513834 = 0.003265505190938711 + 0.01 * 6.7682061195373535
Epoch 980, val loss: 1.3359198570251465
Epoch 990, training loss: 0.07107507437467575 = 0.0031900026369839907 + 0.01 * 6.788506984710693
Epoch 990, val loss: 1.3388947248458862
Epoch 1000, training loss: 0.0708654522895813 = 0.0031179659999907017 + 0.01 * 6.774748802185059
Epoch 1000, val loss: 1.3419010639190674
Epoch 1010, training loss: 0.07095688581466675 = 0.0030491496436297894 + 0.01 * 6.790773868560791
Epoch 1010, val loss: 1.3448164463043213
Epoch 1020, training loss: 0.07064151763916016 = 0.0029831824358552694 + 0.01 * 6.765833854675293
Epoch 1020, val loss: 1.3476228713989258
Epoch 1030, training loss: 0.07047685235738754 = 0.0029200005810707808 + 0.01 * 6.755684852600098
Epoch 1030, val loss: 1.3503997325897217
Epoch 1040, training loss: 0.07042569667100906 = 0.00285946112126112 + 0.01 * 6.7566237449646
Epoch 1040, val loss: 1.3530683517456055
Epoch 1050, training loss: 0.07033517211675644 = 0.002801149385049939 + 0.01 * 6.7534027099609375
Epoch 1050, val loss: 1.3557299375534058
Epoch 1060, training loss: 0.07018683850765228 = 0.002745348261669278 + 0.01 * 6.744149208068848
Epoch 1060, val loss: 1.3583389520645142
Epoch 1070, training loss: 0.07032894343137741 = 0.002691631903871894 + 0.01 * 6.763731002807617
Epoch 1070, val loss: 1.3608653545379639
Epoch 1080, training loss: 0.07017271965742111 = 0.0026400666683912277 + 0.01 * 6.753264904022217
Epoch 1080, val loss: 1.3633326292037964
Epoch 1090, training loss: 0.06989400088787079 = 0.002590280259028077 + 0.01 * 6.730371952056885
Epoch 1090, val loss: 1.3657368421554565
Epoch 1100, training loss: 0.06977424025535583 = 0.0025425024796277285 + 0.01 * 6.723174095153809
Epoch 1100, val loss: 1.3681044578552246
Epoch 1110, training loss: 0.06994282454252243 = 0.0024964064359664917 + 0.01 * 6.74464225769043
Epoch 1110, val loss: 1.3704283237457275
Epoch 1120, training loss: 0.06977465003728867 = 0.0024521166924387217 + 0.01 * 6.732253074645996
Epoch 1120, val loss: 1.3726876974105835
Epoch 1130, training loss: 0.06957148760557175 = 0.0024093957617878914 + 0.01 * 6.716209411621094
Epoch 1130, val loss: 1.3748751878738403
Epoch 1140, training loss: 0.06966360658407211 = 0.0023680534213781357 + 0.01 * 6.729555606842041
Epoch 1140, val loss: 1.3770396709442139
Epoch 1150, training loss: 0.06954196840524673 = 0.0023283937480300665 + 0.01 * 6.721357345581055
Epoch 1150, val loss: 1.3791460990905762
Epoch 1160, training loss: 0.06942840665578842 = 0.002289912663400173 + 0.01 * 6.7138495445251465
Epoch 1160, val loss: 1.3812587261199951
Epoch 1170, training loss: 0.06969141215085983 = 0.0022528315894305706 + 0.01 * 6.7438578605651855
Epoch 1170, val loss: 1.383202075958252
Epoch 1180, training loss: 0.0693015530705452 = 0.00221701106056571 + 0.01 * 6.708454608917236
Epoch 1180, val loss: 1.3852424621582031
Epoch 1190, training loss: 0.0693071186542511 = 0.0021824182476848364 + 0.01 * 6.712470531463623
Epoch 1190, val loss: 1.3871573209762573
Epoch 1200, training loss: 0.06928113847970963 = 0.0021489181090146303 + 0.01 * 6.713222026824951
Epoch 1200, val loss: 1.3889802694320679
Epoch 1210, training loss: 0.06924497336149216 = 0.002116420306265354 + 0.01 * 6.712855339050293
Epoch 1210, val loss: 1.3908746242523193
Epoch 1220, training loss: 0.06911955028772354 = 0.002085104351863265 + 0.01 * 6.703444957733154
Epoch 1220, val loss: 1.3927361965179443
Epoch 1230, training loss: 0.06909658014774323 = 0.0020546945743262768 + 0.01 * 6.704188823699951
Epoch 1230, val loss: 1.3944348096847534
Epoch 1240, training loss: 0.0690070390701294 = 0.0020251532550901175 + 0.01 * 6.698188781738281
Epoch 1240, val loss: 1.3962222337722778
Epoch 1250, training loss: 0.0691390261054039 = 0.001996710430830717 + 0.01 * 6.714231491088867
Epoch 1250, val loss: 1.3978956937789917
Epoch 1260, training loss: 0.06904053688049316 = 0.00196900125592947 + 0.01 * 6.707153797149658
Epoch 1260, val loss: 1.3995774984359741
Epoch 1270, training loss: 0.06885518878698349 = 0.001942151808179915 + 0.01 * 6.691303730010986
Epoch 1270, val loss: 1.4012197256088257
Epoch 1280, training loss: 0.06897839903831482 = 0.0019161212258040905 + 0.01 * 6.706228256225586
Epoch 1280, val loss: 1.4027799367904663
Epoch 1290, training loss: 0.06892041862010956 = 0.0018909915816038847 + 0.01 * 6.702942371368408
Epoch 1290, val loss: 1.4042754173278809
Epoch 1300, training loss: 0.06881202757358551 = 0.0018663767259567976 + 0.01 * 6.694565296173096
Epoch 1300, val loss: 1.4058953523635864
Epoch 1310, training loss: 0.06889432668685913 = 0.0018426760798320174 + 0.01 * 6.705165386199951
Epoch 1310, val loss: 1.4074020385742188
Epoch 1320, training loss: 0.06874383985996246 = 0.0018194884760305285 + 0.01 * 6.692434787750244
Epoch 1320, val loss: 1.4088506698608398
Epoch 1330, training loss: 0.06877215206623077 = 0.0017970611806958914 + 0.01 * 6.697509765625
Epoch 1330, val loss: 1.410314679145813
Epoch 1340, training loss: 0.06867501884698868 = 0.0017751826671883464 + 0.01 * 6.68998384475708
Epoch 1340, val loss: 1.4116637706756592
Epoch 1350, training loss: 0.06860215216875076 = 0.0017539099790155888 + 0.01 * 6.684823989868164
Epoch 1350, val loss: 1.4130746126174927
Epoch 1360, training loss: 0.06869185715913773 = 0.0017333346186205745 + 0.01 * 6.695852756500244
Epoch 1360, val loss: 1.414363145828247
Epoch 1370, training loss: 0.06860041618347168 = 0.0017131486674770713 + 0.01 * 6.688726902008057
Epoch 1370, val loss: 1.4156932830810547
Epoch 1380, training loss: 0.06860805302858353 = 0.001693636761046946 + 0.01 * 6.691441535949707
Epoch 1380, val loss: 1.4169739484786987
Epoch 1390, training loss: 0.06842930614948273 = 0.001674627885222435 + 0.01 * 6.6754679679870605
Epoch 1390, val loss: 1.4182164669036865
Epoch 1400, training loss: 0.06828732043504715 = 0.0016560530057176948 + 0.01 * 6.6631269454956055
Epoch 1400, val loss: 1.4194440841674805
Epoch 1410, training loss: 0.06825902312994003 = 0.00163804623298347 + 0.01 * 6.662097930908203
Epoch 1410, val loss: 1.420614242553711
Epoch 1420, training loss: 0.06832592934370041 = 0.0016204466810449958 + 0.01 * 6.670548439025879
Epoch 1420, val loss: 1.4217824935913086
Epoch 1430, training loss: 0.06842412799596786 = 0.0016034198924899101 + 0.01 * 6.682071208953857
Epoch 1430, val loss: 1.42290198802948
Epoch 1440, training loss: 0.06829485297203064 = 0.0015866656322032213 + 0.01 * 6.67081880569458
Epoch 1440, val loss: 1.4240378141403198
Epoch 1450, training loss: 0.06814190745353699 = 0.0015703815734013915 + 0.01 * 6.6571526527404785
Epoch 1450, val loss: 1.4251266717910767
Epoch 1460, training loss: 0.0685369223356247 = 0.0015545889036729932 + 0.01 * 6.6982340812683105
Epoch 1460, val loss: 1.426132321357727
Epoch 1470, training loss: 0.06815691292285919 = 0.0015390628250315785 + 0.01 * 6.661784648895264
Epoch 1470, val loss: 1.4272195100784302
Epoch 1480, training loss: 0.06831434369087219 = 0.0015240523498505354 + 0.01 * 6.67902946472168
Epoch 1480, val loss: 1.428230881690979
Epoch 1490, training loss: 0.0681302398443222 = 0.0015092195244506001 + 0.01 * 6.662102222442627
Epoch 1490, val loss: 1.4292505979537964
Epoch 1500, training loss: 0.06857351213693619 = 0.001494823256507516 + 0.01 * 6.707869052886963
Epoch 1500, val loss: 1.4302047491073608
Epoch 1510, training loss: 0.06805357336997986 = 0.0014808127889409661 + 0.01 * 6.657276153564453
Epoch 1510, val loss: 1.4312347173690796
Epoch 1520, training loss: 0.06822657585144043 = 0.0014671058161184192 + 0.01 * 6.675947189331055
Epoch 1520, val loss: 1.4321331977844238
Epoch 1530, training loss: 0.06798949092626572 = 0.0014536944217979908 + 0.01 * 6.653579235076904
Epoch 1530, val loss: 1.433092474937439
Epoch 1540, training loss: 0.06840444356203079 = 0.0014406535774469376 + 0.01 * 6.696379661560059
Epoch 1540, val loss: 1.4339231252670288
Epoch 1550, training loss: 0.06791520118713379 = 0.0014278313610702753 + 0.01 * 6.64873743057251
Epoch 1550, val loss: 1.4349148273468018
Epoch 1560, training loss: 0.06807401031255722 = 0.0014153558295220137 + 0.01 * 6.665865421295166
Epoch 1560, val loss: 1.4357541799545288
Epoch 1570, training loss: 0.0678672268986702 = 0.0014031389728188515 + 0.01 * 6.646409034729004
Epoch 1570, val loss: 1.4365800619125366
Epoch 1580, training loss: 0.06800170242786407 = 0.0013911890564486384 + 0.01 * 6.6610517501831055
Epoch 1580, val loss: 1.4374361038208008
Epoch 1590, training loss: 0.06787373125553131 = 0.0013795164413750172 + 0.01 * 6.649421215057373
Epoch 1590, val loss: 1.4382284879684448
Epoch 1600, training loss: 0.06810956448316574 = 0.0013680951669812202 + 0.01 * 6.67414665222168
Epoch 1600, val loss: 1.439029574394226
Epoch 1610, training loss: 0.06784294545650482 = 0.0013568740105256438 + 0.01 * 6.648606777191162
Epoch 1610, val loss: 1.4398552179336548
Epoch 1620, training loss: 0.06773743033409119 = 0.001345951110124588 + 0.01 * 6.639147758483887
Epoch 1620, val loss: 1.4405786991119385
Epoch 1630, training loss: 0.0678076446056366 = 0.0013352270470932126 + 0.01 * 6.647242069244385
Epoch 1630, val loss: 1.4413199424743652
Epoch 1640, training loss: 0.06803444027900696 = 0.0013248706236481667 + 0.01 * 6.670957565307617
Epoch 1640, val loss: 1.4420406818389893
Epoch 1650, training loss: 0.0676976814866066 = 0.0013145426055416465 + 0.01 * 6.6383137702941895
Epoch 1650, val loss: 1.4427309036254883
Epoch 1660, training loss: 0.06780689209699631 = 0.0013045326340943575 + 0.01 * 6.650236129760742
Epoch 1660, val loss: 1.4434683322906494
Epoch 1670, training loss: 0.06773348897695541 = 0.001294687855988741 + 0.01 * 6.643880367279053
Epoch 1670, val loss: 1.4441059827804565
Epoch 1680, training loss: 0.06761668622493744 = 0.0012850507628172636 + 0.01 * 6.6331634521484375
Epoch 1680, val loss: 1.4447542428970337
Epoch 1690, training loss: 0.06769859045743942 = 0.0012756244977936149 + 0.01 * 6.64229679107666
Epoch 1690, val loss: 1.445417046546936
Epoch 1700, training loss: 0.06750264018774033 = 0.0012663512025028467 + 0.01 * 6.623628616333008
Epoch 1700, val loss: 1.4460761547088623
Epoch 1710, training loss: 0.06746084243059158 = 0.0012572623090818524 + 0.01 * 6.620357990264893
Epoch 1710, val loss: 1.446690320968628
Epoch 1720, training loss: 0.06758739054203033 = 0.001248332904651761 + 0.01 * 6.633906364440918
Epoch 1720, val loss: 1.4473429918289185
Epoch 1730, training loss: 0.06755323708057404 = 0.0012395592639222741 + 0.01 * 6.6313676834106445
Epoch 1730, val loss: 1.4479683637619019
Epoch 1740, training loss: 0.06752731651067734 = 0.001231042086146772 + 0.01 * 6.629627227783203
Epoch 1740, val loss: 1.4485520124435425
Epoch 1750, training loss: 0.06754283607006073 = 0.0012226349208503962 + 0.01 * 6.632020473480225
Epoch 1750, val loss: 1.4491512775421143
Epoch 1760, training loss: 0.06761328876018524 = 0.0012144374195486307 + 0.01 * 6.639884948730469
Epoch 1760, val loss: 1.4497127532958984
Epoch 1770, training loss: 0.06745864450931549 = 0.001206415705382824 + 0.01 * 6.625223636627197
Epoch 1770, val loss: 1.4502092599868774
Epoch 1780, training loss: 0.06767904758453369 = 0.0011985322926193476 + 0.01 * 6.648052215576172
Epoch 1780, val loss: 1.4506815671920776
Epoch 1790, training loss: 0.06736776232719421 = 0.0011907549342140555 + 0.01 * 6.617701530456543
Epoch 1790, val loss: 1.4513194561004639
Epoch 1800, training loss: 0.06739827990531921 = 0.0011831872398033738 + 0.01 * 6.621509075164795
Epoch 1800, val loss: 1.4517942667007446
Epoch 1810, training loss: 0.067369744181633 = 0.0011756600579246879 + 0.01 * 6.61940860748291
Epoch 1810, val loss: 1.4523091316223145
Epoch 1820, training loss: 0.0673835426568985 = 0.0011683847988024354 + 0.01 * 6.62151575088501
Epoch 1820, val loss: 1.4528038501739502
Epoch 1830, training loss: 0.0674113854765892 = 0.0011611400404945016 + 0.01 * 6.625024318695068
Epoch 1830, val loss: 1.4532911777496338
Epoch 1840, training loss: 0.06735414266586304 = 0.0011540375417098403 + 0.01 * 6.620010852813721
Epoch 1840, val loss: 1.4536664485931396
Epoch 1850, training loss: 0.06724663078784943 = 0.0011470072204247117 + 0.01 * 6.609961986541748
Epoch 1850, val loss: 1.4542735815048218
Epoch 1860, training loss: 0.06733252853155136 = 0.0011401755036786199 + 0.01 * 6.619235515594482
Epoch 1860, val loss: 1.4546949863433838
Epoch 1870, training loss: 0.06713441759347916 = 0.0011333697475492954 + 0.01 * 6.600104808807373
Epoch 1870, val loss: 1.4551467895507812
Epoch 1880, training loss: 0.06742754578590393 = 0.0011267426889389753 + 0.01 * 6.630080223083496
Epoch 1880, val loss: 1.455578327178955
Epoch 1890, training loss: 0.06718273460865021 = 0.0011201758170500398 + 0.01 * 6.606256008148193
Epoch 1890, val loss: 1.456033706665039
Epoch 1900, training loss: 0.06734683364629745 = 0.0011137609835714102 + 0.01 * 6.623307704925537
Epoch 1900, val loss: 1.4564834833145142
Epoch 1910, training loss: 0.06704377382993698 = 0.0011074033100157976 + 0.01 * 6.593637466430664
Epoch 1910, val loss: 1.4569194316864014
Epoch 1920, training loss: 0.06707705557346344 = 0.0011011784663423896 + 0.01 * 6.597587585449219
Epoch 1920, val loss: 1.4573568105697632
Epoch 1930, training loss: 0.06695612519979477 = 0.0010950503638014197 + 0.01 * 6.5861077308654785
Epoch 1930, val loss: 1.4576897621154785
Epoch 1940, training loss: 0.06740208715200424 = 0.0010890444973483682 + 0.01 * 6.6313042640686035
Epoch 1940, val loss: 1.45809006690979
Epoch 1950, training loss: 0.06718473881483078 = 0.0010830711107701063 + 0.01 * 6.610167026519775
Epoch 1950, val loss: 1.4585306644439697
Epoch 1960, training loss: 0.06722070276737213 = 0.0010772502282634377 + 0.01 * 6.614345550537109
Epoch 1960, val loss: 1.4589662551879883
Epoch 1970, training loss: 0.06697586178779602 = 0.0010714730015024543 + 0.01 * 6.5904388427734375
Epoch 1970, val loss: 1.4592801332473755
Epoch 1980, training loss: 0.06703519821166992 = 0.0010657868115231395 + 0.01 * 6.596941947937012
Epoch 1980, val loss: 1.4596768617630005
Epoch 1990, training loss: 0.06712634116411209 = 0.00106023158878088 + 0.01 * 6.606611251831055
Epoch 1990, val loss: 1.460028052330017
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 2.052798271179199 = 1.966829538345337 + 0.01 * 8.5968656539917
Epoch 0, val loss: 1.9591163396835327
Epoch 10, training loss: 2.0421698093414307 = 1.9562013149261475 + 0.01 * 8.596851348876953
Epoch 10, val loss: 1.9492298364639282
Epoch 20, training loss: 2.0299689769744873 = 1.94400155544281 + 0.01 * 8.59675121307373
Epoch 20, val loss: 1.9373191595077515
Epoch 30, training loss: 2.013498067855835 = 1.927533745765686 + 0.01 * 8.596434593200684
Epoch 30, val loss: 1.920594334602356
Epoch 40, training loss: 1.9892468452453613 = 1.9032975435256958 + 0.01 * 8.594931602478027
Epoch 40, val loss: 1.8957138061523438
Epoch 50, training loss: 1.9540534019470215 = 1.8682180643081665 + 0.01 * 8.583529472351074
Epoch 50, val loss: 1.8605376482009888
Epoch 60, training loss: 1.9106976985931396 = 1.8254425525665283 + 0.01 * 8.525510787963867
Epoch 60, val loss: 1.8212225437164307
Epoch 70, training loss: 1.8699532747268677 = 1.7876472473144531 + 0.01 * 8.230602264404297
Epoch 70, val loss: 1.7923450469970703
Epoch 80, training loss: 1.831861138343811 = 1.7509902715682983 + 0.01 * 8.08708381652832
Epoch 80, val loss: 1.7642439603805542
Epoch 90, training loss: 1.7812047004699707 = 1.7019565105438232 + 0.01 * 7.924820423126221
Epoch 90, val loss: 1.7220020294189453
Epoch 100, training loss: 1.711793065071106 = 1.634692907333374 + 0.01 * 7.710018634796143
Epoch 100, val loss: 1.6639087200164795
Epoch 110, training loss: 1.6245555877685547 = 1.5490506887435913 + 0.01 * 7.550488471984863
Epoch 110, val loss: 1.5939667224884033
Epoch 120, training loss: 1.5264381170272827 = 1.4515671730041504 + 0.01 * 7.487099647521973
Epoch 120, val loss: 1.51621413230896
Epoch 130, training loss: 1.4256621599197388 = 1.351234793663025 + 0.01 * 7.442734718322754
Epoch 130, val loss: 1.439436912536621
Epoch 140, training loss: 1.325823426246643 = 1.251969814300537 + 0.01 * 7.3853583335876465
Epoch 140, val loss: 1.3653253316879272
Epoch 150, training loss: 1.2296671867370605 = 1.1563266515731812 + 0.01 * 7.334059238433838
Epoch 150, val loss: 1.2956396341323853
Epoch 160, training loss: 1.1411787271499634 = 1.0683614015579224 + 0.01 * 7.281735897064209
Epoch 160, val loss: 1.2335020303726196
Epoch 170, training loss: 1.0626623630523682 = 0.9902079701423645 + 0.01 * 7.245445251464844
Epoch 170, val loss: 1.1809521913528442
Epoch 180, training loss: 0.9923534989356995 = 0.9200424551963806 + 0.01 * 7.231103420257568
Epoch 180, val loss: 1.1363587379455566
Epoch 190, training loss: 0.9265137910842896 = 0.8542965650558472 + 0.01 * 7.2217206954956055
Epoch 190, val loss: 1.0966094732284546
Epoch 200, training loss: 0.862012505531311 = 0.7899107336997986 + 0.01 * 7.210177898406982
Epoch 200, val loss: 1.059012770652771
Epoch 210, training loss: 0.7973289489746094 = 0.7253769636154175 + 0.01 * 7.195201396942139
Epoch 210, val loss: 1.0221810340881348
Epoch 220, training loss: 0.7326253652572632 = 0.6608307361602783 + 0.01 * 7.179461479187012
Epoch 220, val loss: 0.9868287444114685
Epoch 230, training loss: 0.6691684722900391 = 0.597526490688324 + 0.01 * 7.164196014404297
Epoch 230, val loss: 0.9547529220581055
Epoch 240, training loss: 0.608370840549469 = 0.5368831157684326 + 0.01 * 7.148772239685059
Epoch 240, val loss: 0.9272863864898682
Epoch 250, training loss: 0.5513134002685547 = 0.47996050119400024 + 0.01 * 7.13529109954834
Epoch 250, val loss: 0.9049195647239685
Epoch 260, training loss: 0.4984821081161499 = 0.4272681474685669 + 0.01 * 7.121395587921143
Epoch 260, val loss: 0.8875056505203247
Epoch 270, training loss: 0.4499680995941162 = 0.3788081109523773 + 0.01 * 7.1159987449646
Epoch 270, val loss: 0.8741586208343506
Epoch 280, training loss: 0.40544360876083374 = 0.3343437910079956 + 0.01 * 7.109982013702393
Epoch 280, val loss: 0.8641353845596313
Epoch 290, training loss: 0.36460039019584656 = 0.2935432493686676 + 0.01 * 7.105713844299316
Epoch 290, val loss: 0.8572205901145935
Epoch 300, training loss: 0.32712361216545105 = 0.25610867142677307 + 0.01 * 7.101494789123535
Epoch 300, val loss: 0.8530774712562561
Epoch 310, training loss: 0.2928869128227234 = 0.22191718220710754 + 0.01 * 7.096973896026611
Epoch 310, val loss: 0.8518510460853577
Epoch 320, training loss: 0.26191744208335876 = 0.19099678099155426 + 0.01 * 7.092066287994385
Epoch 320, val loss: 0.853507936000824
Epoch 330, training loss: 0.23436257243156433 = 0.16348622739315033 + 0.01 * 7.0876336097717285
Epoch 330, val loss: 0.8581181764602661
Epoch 340, training loss: 0.21033740043640137 = 0.13948582112789154 + 0.01 * 7.085158348083496
Epoch 340, val loss: 0.8658224940299988
Epoch 350, training loss: 0.18983671069145203 = 0.11896113306283951 + 0.01 * 7.087558746337891
Epoch 350, val loss: 0.8765041828155518
Epoch 360, training loss: 0.1724701225757599 = 0.10168443620204926 + 0.01 * 7.078568935394287
Epoch 360, val loss: 0.8898842334747314
Epoch 370, training loss: 0.15803048014640808 = 0.08727674186229706 + 0.01 * 7.075374126434326
Epoch 370, val loss: 0.9053328633308411
Epoch 380, training loss: 0.14602920413017273 = 0.07531321048736572 + 0.01 * 7.071599960327148
Epoch 380, val loss: 0.9221848845481873
Epoch 390, training loss: 0.13608089089393616 = 0.06538573652505875 + 0.01 * 7.069516658782959
Epoch 390, val loss: 0.939879298210144
Epoch 400, training loss: 0.1277521550655365 = 0.05713236704468727 + 0.01 * 7.061979293823242
Epoch 400, val loss: 0.9578954577445984
Epoch 410, training loss: 0.12081623822450638 = 0.050239741802215576 + 0.01 * 7.057649612426758
Epoch 410, val loss: 0.9758985638618469
Epoch 420, training loss: 0.11505842208862305 = 0.04445528984069824 + 0.01 * 7.0603132247924805
Epoch 420, val loss: 0.9936636686325073
Epoch 430, training loss: 0.11007790267467499 = 0.03958011791110039 + 0.01 * 7.049777984619141
Epoch 430, val loss: 1.0110337734222412
Epoch 440, training loss: 0.10586461424827576 = 0.03544469550251961 + 0.01 * 7.0419921875
Epoch 440, val loss: 1.0278851985931396
Epoch 450, training loss: 0.10235226154327393 = 0.03191760554909706 + 0.01 * 7.043466091156006
Epoch 450, val loss: 1.044248342514038
Epoch 460, training loss: 0.09917165338993073 = 0.028891773894429207 + 0.01 * 7.027988433837891
Epoch 460, val loss: 1.0599441528320312
Epoch 470, training loss: 0.0964297354221344 = 0.026278506964445114 + 0.01 * 7.015122890472412
Epoch 470, val loss: 1.0750749111175537
Epoch 480, training loss: 0.09406810253858566 = 0.024008581414818764 + 0.01 * 7.00595235824585
Epoch 480, val loss: 1.0896402597427368
Epoch 490, training loss: 0.09218102693557739 = 0.022030651569366455 + 0.01 * 7.015037536621094
Epoch 490, val loss: 1.1035473346710205
Epoch 500, training loss: 0.09024523943662643 = 0.02029387094080448 + 0.01 * 6.9951372146606445
Epoch 500, val loss: 1.116956114768982
Epoch 510, training loss: 0.08864901214838028 = 0.018758468329906464 + 0.01 * 6.989054203033447
Epoch 510, val loss: 1.129826307296753
Epoch 520, training loss: 0.08728651702404022 = 0.01739524118602276 + 0.01 * 6.9891276359558105
Epoch 520, val loss: 1.142211675643921
Epoch 530, training loss: 0.08602650463581085 = 0.016181547194719315 + 0.01 * 6.984496593475342
Epoch 530, val loss: 1.1542216539382935
Epoch 540, training loss: 0.08487799018621445 = 0.015095179900527 + 0.01 * 6.978281021118164
Epoch 540, val loss: 1.1655726432800293
Epoch 550, training loss: 0.08381491154432297 = 0.01411986630409956 + 0.01 * 6.9695048332214355
Epoch 550, val loss: 1.1767175197601318
Epoch 560, training loss: 0.08284825086593628 = 0.013239802792668343 + 0.01 * 6.960844993591309
Epoch 560, val loss: 1.187198281288147
Epoch 570, training loss: 0.0818893238902092 = 0.012442239560186863 + 0.01 * 6.944708824157715
Epoch 570, val loss: 1.1976054906845093
Epoch 580, training loss: 0.08120537549257278 = 0.011717653833329678 + 0.01 * 6.948771953582764
Epoch 580, val loss: 1.2074124813079834
Epoch 590, training loss: 0.08045212179422379 = 0.011057584546506405 + 0.01 * 6.939453601837158
Epoch 590, val loss: 1.2170552015304565
Epoch 600, training loss: 0.0796632468700409 = 0.010453622788190842 + 0.01 * 6.920963287353516
Epoch 600, val loss: 1.2263222932815552
Epoch 610, training loss: 0.07904326915740967 = 0.009899923577904701 + 0.01 * 6.914334774017334
Epoch 610, val loss: 1.235334038734436
Epoch 620, training loss: 0.0785127580165863 = 0.009391386061906815 + 0.01 * 6.912137985229492
Epoch 620, val loss: 1.2439947128295898
Epoch 630, training loss: 0.07794176787137985 = 0.008922656066715717 + 0.01 * 6.901910781860352
Epoch 630, val loss: 1.2524768114089966
Epoch 640, training loss: 0.07764746248722076 = 0.008489904925227165 + 0.01 * 6.915755748748779
Epoch 640, val loss: 1.260757327079773
Epoch 650, training loss: 0.07708092778921127 = 0.008089540526270866 + 0.01 * 6.899138450622559
Epoch 650, val loss: 1.2686758041381836
Epoch 660, training loss: 0.07665348798036575 = 0.007718962151557207 + 0.01 * 6.893452167510986
Epoch 660, val loss: 1.2764816284179688
Epoch 670, training loss: 0.07633699476718903 = 0.007375223096460104 + 0.01 * 6.896176815032959
Epoch 670, val loss: 1.283969759941101
Epoch 680, training loss: 0.07592157274484634 = 0.007055747322738171 + 0.01 * 6.886582851409912
Epoch 680, val loss: 1.2912538051605225
Epoch 690, training loss: 0.07561052590608597 = 0.006758126895874739 + 0.01 * 6.885240077972412
Epoch 690, val loss: 1.2982646226882935
Epoch 700, training loss: 0.07514209300279617 = 0.00648079626262188 + 0.01 * 6.8661298751831055
Epoch 700, val loss: 1.3052006959915161
Epoch 710, training loss: 0.07519201934337616 = 0.006221435032784939 + 0.01 * 6.897058486938477
Epoch 710, val loss: 1.3119785785675049
Epoch 720, training loss: 0.07474419474601746 = 0.005979207810014486 + 0.01 * 6.876498699188232
Epoch 720, val loss: 1.3183326721191406
Epoch 730, training loss: 0.0744658038020134 = 0.005752657540142536 + 0.01 * 6.871314525604248
Epoch 730, val loss: 1.3247146606445312
Epoch 740, training loss: 0.07438632100820541 = 0.005539874546229839 + 0.01 * 6.884644985198975
Epoch 740, val loss: 1.3307644128799438
Epoch 750, training loss: 0.07380329072475433 = 0.0053403861820697784 + 0.01 * 6.846290111541748
Epoch 750, val loss: 1.3366938829421997
Epoch 760, training loss: 0.07391215860843658 = 0.005152862519025803 + 0.01 * 6.875930309295654
Epoch 760, val loss: 1.3424582481384277
Epoch 770, training loss: 0.07350397855043411 = 0.004976388532668352 + 0.01 * 6.852758884429932
Epoch 770, val loss: 1.3482884168624878
Epoch 780, training loss: 0.0734589695930481 = 0.004810267593711615 + 0.01 * 6.864870548248291
Epoch 780, val loss: 1.3535767793655396
Epoch 790, training loss: 0.07296489179134369 = 0.004653827287256718 + 0.01 * 6.831106662750244
Epoch 790, val loss: 1.3589856624603271
Epoch 800, training loss: 0.07285597175359726 = 0.004505829885601997 + 0.01 * 6.835014343261719
Epoch 800, val loss: 1.3643147945404053
Epoch 810, training loss: 0.07270269095897675 = 0.004365966189652681 + 0.01 * 6.833672523498535
Epoch 810, val loss: 1.3691617250442505
Epoch 820, training loss: 0.07251955568790436 = 0.004233789164572954 + 0.01 * 6.828576564788818
Epoch 820, val loss: 1.3743066787719727
Epoch 830, training loss: 0.07223818451166153 = 0.004108386114239693 + 0.01 * 6.8129801750183105
Epoch 830, val loss: 1.3790143728256226
Epoch 840, training loss: 0.07205957919359207 = 0.003989502787590027 + 0.01 * 6.807007789611816
Epoch 840, val loss: 1.3837848901748657
Epoch 850, training loss: 0.07225348800420761 = 0.003876711241900921 + 0.01 * 6.8376784324646
Epoch 850, val loss: 1.3883819580078125
Epoch 860, training loss: 0.07178653031587601 = 0.0037695784121751785 + 0.01 * 6.801695823669434
Epoch 860, val loss: 1.3927396535873413
Epoch 870, training loss: 0.07168656587600708 = 0.003667999291792512 + 0.01 * 6.801856517791748
Epoch 870, val loss: 1.3971410989761353
Epoch 880, training loss: 0.07180072367191315 = 0.0035712302196770906 + 0.01 * 6.8229498863220215
Epoch 880, val loss: 1.401275873184204
Epoch 890, training loss: 0.07130876183509827 = 0.003479174105450511 + 0.01 * 6.782958984375
Epoch 890, val loss: 1.4055379629135132
Epoch 900, training loss: 0.07130221277475357 = 0.0033915520180016756 + 0.01 * 6.7910661697387695
Epoch 900, val loss: 1.4096232652664185
Epoch 910, training loss: 0.07152535766363144 = 0.0033079676795750856 + 0.01 * 6.821739196777344
Epoch 910, val loss: 1.4133864641189575
Epoch 920, training loss: 0.07123272866010666 = 0.0032284122426062822 + 0.01 * 6.800432205200195
Epoch 920, val loss: 1.417273759841919
Epoch 930, training loss: 0.0708339512348175 = 0.0031526461243629456 + 0.01 * 6.768130302429199
Epoch 930, val loss: 1.421030044555664
Epoch 940, training loss: 0.07084131240844727 = 0.003079839749261737 + 0.01 * 6.776147842407227
Epoch 940, val loss: 1.4246543645858765
Epoch 950, training loss: 0.07089045643806458 = 0.0030103567987680435 + 0.01 * 6.788010120391846
Epoch 950, val loss: 1.4283735752105713
Epoch 960, training loss: 0.07052388787269592 = 0.002943652216345072 + 0.01 * 6.758024215698242
Epoch 960, val loss: 1.431766152381897
Epoch 970, training loss: 0.07058829814195633 = 0.002880162326619029 + 0.01 * 6.770813465118408
Epoch 970, val loss: 1.435185194015503
Epoch 980, training loss: 0.07049150764942169 = 0.0028191134333610535 + 0.01 * 6.767239570617676
Epoch 980, val loss: 1.4384419918060303
Epoch 990, training loss: 0.0702793151140213 = 0.0027605569921433926 + 0.01 * 6.751875877380371
Epoch 990, val loss: 1.4417928457260132
Epoch 1000, training loss: 0.07020311057567596 = 0.0027044005692005157 + 0.01 * 6.749871253967285
Epoch 1000, val loss: 1.4449859857559204
Epoch 1010, training loss: 0.07019238919019699 = 0.0026504378765821457 + 0.01 * 6.754194736480713
Epoch 1010, val loss: 1.4480040073394775
Epoch 1020, training loss: 0.07019035518169403 = 0.0025985180400311947 + 0.01 * 6.75918436050415
Epoch 1020, val loss: 1.4511648416519165
Epoch 1030, training loss: 0.06998905539512634 = 0.002548566088080406 + 0.01 * 6.744049072265625
Epoch 1030, val loss: 1.4539906978607178
Epoch 1040, training loss: 0.06990821659564972 = 0.002500625792890787 + 0.01 * 6.740759372711182
Epoch 1040, val loss: 1.457146406173706
Epoch 1050, training loss: 0.06975674629211426 = 0.002454327652230859 + 0.01 * 6.7302422523498535
Epoch 1050, val loss: 1.459754467010498
Epoch 1060, training loss: 0.06993637979030609 = 0.002410066546872258 + 0.01 * 6.752631187438965
Epoch 1060, val loss: 1.462563157081604
Epoch 1070, training loss: 0.06966681778430939 = 0.0023671097587794065 + 0.01 * 6.729970932006836
Epoch 1070, val loss: 1.4653353691101074
Epoch 1080, training loss: 0.069607675075531 = 0.0023258430883288383 + 0.01 * 6.728183746337891
Epoch 1080, val loss: 1.4680153131484985
Epoch 1090, training loss: 0.06957217305898666 = 0.00228590564802289 + 0.01 * 6.728626728057861
Epoch 1090, val loss: 1.4705950021743774
Epoch 1100, training loss: 0.06938257813453674 = 0.002247373340651393 + 0.01 * 6.7135210037231445
Epoch 1100, val loss: 1.4730935096740723
Epoch 1110, training loss: 0.06948023289442062 = 0.0022101541981101036 + 0.01 * 6.727007865905762
Epoch 1110, val loss: 1.4755979776382446
Epoch 1120, training loss: 0.06932990998029709 = 0.002174291992560029 + 0.01 * 6.715561866760254
Epoch 1120, val loss: 1.4779584407806396
Epoch 1130, training loss: 0.0696592852473259 = 0.002139708958566189 + 0.01 * 6.751957416534424
Epoch 1130, val loss: 1.4804613590240479
Epoch 1140, training loss: 0.06930485367774963 = 0.0021061915904283524 + 0.01 * 6.7198662757873535
Epoch 1140, val loss: 1.4824786186218262
Epoch 1150, training loss: 0.06933886557817459 = 0.0020739338360726833 + 0.01 * 6.7264933586120605
Epoch 1150, val loss: 1.484874963760376
Epoch 1160, training loss: 0.06912354379892349 = 0.0020425301045179367 + 0.01 * 6.708101272583008
Epoch 1160, val loss: 1.48702871799469
Epoch 1170, training loss: 0.06926517188549042 = 0.0020121692214161158 + 0.01 * 6.725300312042236
Epoch 1170, val loss: 1.4893704652786255
Epoch 1180, training loss: 0.06903789937496185 = 0.001982729649171233 + 0.01 * 6.705517768859863
Epoch 1180, val loss: 1.4911836385726929
Epoch 1190, training loss: 0.06897708028554916 = 0.00195433315820992 + 0.01 * 6.702275276184082
Epoch 1190, val loss: 1.4933797121047974
Epoch 1200, training loss: 0.06893254071474075 = 0.0019266565795987844 + 0.01 * 6.700589179992676
Epoch 1200, val loss: 1.495344877243042
Epoch 1210, training loss: 0.06877022236585617 = 0.0018999772146344185 + 0.01 * 6.6870245933532715
Epoch 1210, val loss: 1.4973257780075073
Epoch 1220, training loss: 0.06902319192886353 = 0.0018740943633019924 + 0.01 * 6.71491003036499
Epoch 1220, val loss: 1.4992910623550415
Epoch 1230, training loss: 0.06885497272014618 = 0.001848788233473897 + 0.01 * 6.700618743896484
Epoch 1230, val loss: 1.50118887424469
Epoch 1240, training loss: 0.06895168870687485 = 0.001824500854127109 + 0.01 * 6.712719440460205
Epoch 1240, val loss: 1.5031782388687134
Epoch 1250, training loss: 0.06881484389305115 = 0.001800772501155734 + 0.01 * 6.701406955718994
Epoch 1250, val loss: 1.504833459854126
Epoch 1260, training loss: 0.06873008608818054 = 0.0017778492765501142 + 0.01 * 6.695223808288574
Epoch 1260, val loss: 1.5067917108535767
Epoch 1270, training loss: 0.06873476505279541 = 0.0017554644728079438 + 0.01 * 6.697930335998535
Epoch 1270, val loss: 1.5084798336029053
Epoch 1280, training loss: 0.06873831152915955 = 0.0017338695470243692 + 0.01 * 6.70044469833374
Epoch 1280, val loss: 1.5101897716522217
Epoch 1290, training loss: 0.06856589764356613 = 0.0017126967431977391 + 0.01 * 6.6853203773498535
Epoch 1290, val loss: 1.5119478702545166
Epoch 1300, training loss: 0.06864769756793976 = 0.0016922765644267201 + 0.01 * 6.695541858673096
Epoch 1300, val loss: 1.513527750968933
Epoch 1310, training loss: 0.06854436546564102 = 0.001672257436439395 + 0.01 * 6.687210559844971
Epoch 1310, val loss: 1.5151233673095703
Epoch 1320, training loss: 0.06846898049116135 = 0.0016529804561287165 + 0.01 * 6.681600093841553
Epoch 1320, val loss: 1.5168206691741943
Epoch 1330, training loss: 0.06847415119409561 = 0.0016339649446308613 + 0.01 * 6.684019088745117
Epoch 1330, val loss: 1.518356442451477
Epoch 1340, training loss: 0.0689222663640976 = 0.0016157038044184446 + 0.01 * 6.730656147003174
Epoch 1340, val loss: 1.5199207067489624
Epoch 1350, training loss: 0.0684744119644165 = 0.0015977692091837525 + 0.01 * 6.68766450881958
Epoch 1350, val loss: 1.5214712619781494
Epoch 1360, training loss: 0.06836141645908356 = 0.0015805315924808383 + 0.01 * 6.678089141845703
Epoch 1360, val loss: 1.523005723953247
Epoch 1370, training loss: 0.06828009337186813 = 0.0015636058524250984 + 0.01 * 6.671648979187012
Epoch 1370, val loss: 1.5243953466415405
Epoch 1380, training loss: 0.06839814782142639 = 0.0015471270307898521 + 0.01 * 6.685102462768555
Epoch 1380, val loss: 1.5257923603057861
Epoch 1390, training loss: 0.06828886270523071 = 0.001530998619273305 + 0.01 * 6.675786972045898
Epoch 1390, val loss: 1.5272659063339233
Epoch 1400, training loss: 0.06813842803239822 = 0.001515424926765263 + 0.01 * 6.6623005867004395
Epoch 1400, val loss: 1.5287375450134277
Epoch 1410, training loss: 0.0681578740477562 = 0.0015001604333519936 + 0.01 * 6.665771484375
Epoch 1410, val loss: 1.5299965143203735
Epoch 1420, training loss: 0.06823495030403137 = 0.0014852911699563265 + 0.01 * 6.674966335296631
Epoch 1420, val loss: 1.531531572341919
Epoch 1430, training loss: 0.06807137280702591 = 0.0014707510126754642 + 0.01 * 6.660062313079834
Epoch 1430, val loss: 1.532686471939087
Epoch 1440, training loss: 0.06813247501850128 = 0.001456643920391798 + 0.01 * 6.667583465576172
Epoch 1440, val loss: 1.534063696861267
Epoch 1450, training loss: 0.06823741644620895 = 0.0014428917784243822 + 0.01 * 6.679452419281006
Epoch 1450, val loss: 1.5353504419326782
Epoch 1460, training loss: 0.06793522089719772 = 0.001429376075975597 + 0.01 * 6.650584697723389
Epoch 1460, val loss: 1.5366233587265015
Epoch 1470, training loss: 0.06816671788692474 = 0.0014162660809233785 + 0.01 * 6.675045490264893
Epoch 1470, val loss: 1.5379823446273804
Epoch 1480, training loss: 0.06803620606660843 = 0.001403394271619618 + 0.01 * 6.6632819175720215
Epoch 1480, val loss: 1.5391331911087036
Epoch 1490, training loss: 0.06806444376707077 = 0.0013908320106565952 + 0.01 * 6.667361259460449
Epoch 1490, val loss: 1.5403454303741455
Epoch 1500, training loss: 0.06813319772481918 = 0.0013786435592919588 + 0.01 * 6.6754560470581055
Epoch 1500, val loss: 1.5414576530456543
Epoch 1510, training loss: 0.06785278022289276 = 0.001366608077660203 + 0.01 * 6.648617744445801
Epoch 1510, val loss: 1.5426841974258423
Epoch 1520, training loss: 0.0680951476097107 = 0.0013550551375374198 + 0.01 * 6.674009799957275
Epoch 1520, val loss: 1.5438811779022217
Epoch 1530, training loss: 0.06791757792234421 = 0.0013436004519462585 + 0.01 * 6.657398223876953
Epoch 1530, val loss: 1.5449169874191284
Epoch 1540, training loss: 0.06782357394695282 = 0.00133238616399467 + 0.01 * 6.649118900299072
Epoch 1540, val loss: 1.5461219549179077
Epoch 1550, training loss: 0.0679643526673317 = 0.0013214802602306008 + 0.01 * 6.664287567138672
Epoch 1550, val loss: 1.5472139120101929
Epoch 1560, training loss: 0.06771180778741837 = 0.0013108376879245043 + 0.01 * 6.640097141265869
Epoch 1560, val loss: 1.548200011253357
Epoch 1570, training loss: 0.0677589476108551 = 0.0013003976782783866 + 0.01 * 6.64585542678833
Epoch 1570, val loss: 1.54924476146698
Epoch 1580, training loss: 0.06768849492073059 = 0.0012901825830340385 + 0.01 * 6.63983154296875
Epoch 1580, val loss: 1.5502549409866333
Epoch 1590, training loss: 0.06779681891202927 = 0.0012801509583368897 + 0.01 * 6.65166711807251
Epoch 1590, val loss: 1.5513473749160767
Epoch 1600, training loss: 0.06810690462589264 = 0.0012703597312793136 + 0.01 * 6.683654308319092
Epoch 1600, val loss: 1.5523715019226074
Epoch 1610, training loss: 0.06776305288076401 = 0.0012609056429937482 + 0.01 * 6.650214672088623
Epoch 1610, val loss: 1.553305983543396
Epoch 1620, training loss: 0.06771040707826614 = 0.001251604058779776 + 0.01 * 6.645880699157715
Epoch 1620, val loss: 1.5542609691619873
Epoch 1630, training loss: 0.06761998683214188 = 0.0012424373999238014 + 0.01 * 6.637754917144775
Epoch 1630, val loss: 1.5553615093231201
Epoch 1640, training loss: 0.06765073537826538 = 0.0012334040366113186 + 0.01 * 6.641732692718506
Epoch 1640, val loss: 1.5563355684280396
Epoch 1650, training loss: 0.06771077960729599 = 0.00122463284060359 + 0.01 * 6.648614883422852
Epoch 1650, val loss: 1.5572139024734497
Epoch 1660, training loss: 0.06747110933065414 = 0.0012160263722762465 + 0.01 * 6.6255083084106445
Epoch 1660, val loss: 1.5581952333450317
Epoch 1670, training loss: 0.06780247390270233 = 0.0012075527338311076 + 0.01 * 6.659492492675781
Epoch 1670, val loss: 1.5593202114105225
Epoch 1680, training loss: 0.0674845278263092 = 0.0011992418440058827 + 0.01 * 6.628528118133545
Epoch 1680, val loss: 1.5601177215576172
Epoch 1690, training loss: 0.06752093881368637 = 0.0011911682086065412 + 0.01 * 6.63297700881958
Epoch 1690, val loss: 1.56107759475708
Epoch 1700, training loss: 0.06776314973831177 = 0.0011831936426460743 + 0.01 * 6.657995223999023
Epoch 1700, val loss: 1.56206214427948
Epoch 1710, training loss: 0.06745485216379166 = 0.001175392302684486 + 0.01 * 6.627946376800537
Epoch 1710, val loss: 1.5630122423171997
Epoch 1720, training loss: 0.06737670302391052 = 0.001167760230600834 + 0.01 * 6.620894432067871
Epoch 1720, val loss: 1.5638563632965088
Epoch 1730, training loss: 0.06773781031370163 = 0.0011603205930441618 + 0.01 * 6.657748699188232
Epoch 1730, val loss: 1.5647170543670654
Epoch 1740, training loss: 0.06749061495065689 = 0.001152912387624383 + 0.01 * 6.633770942687988
Epoch 1740, val loss: 1.565790057182312
Epoch 1750, training loss: 0.06758418679237366 = 0.0011456996435299516 + 0.01 * 6.643848896026611
Epoch 1750, val loss: 1.5666216611862183
Epoch 1760, training loss: 0.06745303422212601 = 0.0011386434780433774 + 0.01 * 6.631439208984375
Epoch 1760, val loss: 1.5675287246704102
Epoch 1770, training loss: 0.0674540102481842 = 0.0011316543677821755 + 0.01 * 6.632236003875732
Epoch 1770, val loss: 1.5684292316436768
Epoch 1780, training loss: 0.06718727946281433 = 0.0011248484952375293 + 0.01 * 6.606243133544922
Epoch 1780, val loss: 1.5694444179534912
Epoch 1790, training loss: 0.06719766557216644 = 0.0011181514710187912 + 0.01 * 6.6079511642456055
Epoch 1790, val loss: 1.5703134536743164
Epoch 1800, training loss: 0.06736878305673599 = 0.0011115589877590537 + 0.01 * 6.625722408294678
Epoch 1800, val loss: 1.5712158679962158
Epoch 1810, training loss: 0.06732632964849472 = 0.0011050889734178782 + 0.01 * 6.622123718261719
Epoch 1810, val loss: 1.5720487833023071
Epoch 1820, training loss: 0.06715574860572815 = 0.0010987380519509315 + 0.01 * 6.605700969696045
Epoch 1820, val loss: 1.5729409456253052
Epoch 1830, training loss: 0.06707065552473068 = 0.0010924921371042728 + 0.01 * 6.597816467285156
Epoch 1830, val loss: 1.5739285945892334
Epoch 1840, training loss: 0.06715960055589676 = 0.0010863026836887002 + 0.01 * 6.607330322265625
Epoch 1840, val loss: 1.574635624885559
Epoch 1850, training loss: 0.06752055883407593 = 0.0010802849428728223 + 0.01 * 6.6440277099609375
Epoch 1850, val loss: 1.5756022930145264
Epoch 1860, training loss: 0.06722240895032883 = 0.0010743241291493177 + 0.01 * 6.614808559417725
Epoch 1860, val loss: 1.5765502452850342
Epoch 1870, training loss: 0.06733587384223938 = 0.0010685516754165292 + 0.01 * 6.626732349395752
Epoch 1870, val loss: 1.577289342880249
Epoch 1880, training loss: 0.06710988283157349 = 0.001062779570929706 + 0.01 * 6.604710102081299
Epoch 1880, val loss: 1.578252911567688
Epoch 1890, training loss: 0.06712418794631958 = 0.00105714809615165 + 0.01 * 6.606704235076904
Epoch 1890, val loss: 1.5790632963180542
Epoch 1900, training loss: 0.06702614575624466 = 0.0010515560861676931 + 0.01 * 6.59745979309082
Epoch 1900, val loss: 1.5799325704574585
Epoch 1910, training loss: 0.06715452671051025 = 0.0010460707126185298 + 0.01 * 6.610846042633057
Epoch 1910, val loss: 1.5807666778564453
Epoch 1920, training loss: 0.06701865792274475 = 0.0010406739311292768 + 0.01 * 6.597798824310303
Epoch 1920, val loss: 1.581748604774475
Epoch 1930, training loss: 0.06694933772087097 = 0.0010353542165830731 + 0.01 * 6.5913987159729
Epoch 1930, val loss: 1.5825507640838623
Epoch 1940, training loss: 0.06711646914482117 = 0.0010301468428224325 + 0.01 * 6.6086320877075195
Epoch 1940, val loss: 1.5833337306976318
Epoch 1950, training loss: 0.06708523631095886 = 0.0010249507613480091 + 0.01 * 6.606028079986572
Epoch 1950, val loss: 1.5842839479446411
Epoch 1960, training loss: 0.0671420618891716 = 0.0010199074167758226 + 0.01 * 6.612215042114258
Epoch 1960, val loss: 1.5851571559906006
Epoch 1970, training loss: 0.0670330822467804 = 0.0010148974834010005 + 0.01 * 6.601819038391113
Epoch 1970, val loss: 1.5859266519546509
Epoch 1980, training loss: 0.06720219552516937 = 0.001009971252642572 + 0.01 * 6.619222640991211
Epoch 1980, val loss: 1.5867817401885986
Epoch 1990, training loss: 0.06675585359334946 = 0.0010050974087789655 + 0.01 * 6.575075626373291
Epoch 1990, val loss: 1.5876635313034058
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.801265155508698
=== training gcn model ===
Epoch 0, training loss: 2.037888526916504 = 1.951919674873352 + 0.01 * 8.59687614440918
Epoch 0, val loss: 1.9529063701629639
Epoch 10, training loss: 2.0281126499176025 = 1.9421442747116089 + 0.01 * 8.596837997436523
Epoch 10, val loss: 1.9432255029678345
Epoch 20, training loss: 2.0157501697540283 = 1.9297832250595093 + 0.01 * 8.596702575683594
Epoch 20, val loss: 1.9303679466247559
Epoch 30, training loss: 1.9980590343475342 = 1.9120967388153076 + 0.01 * 8.596232414245605
Epoch 30, val loss: 1.9113131761550903
Epoch 40, training loss: 1.9717833995819092 = 1.8858493566513062 + 0.01 * 8.593404769897461
Epoch 40, val loss: 1.8827682733535767
Epoch 50, training loss: 1.9353675842285156 = 1.8496466875076294 + 0.01 * 8.572089195251465
Epoch 50, val loss: 1.844451904296875
Epoch 60, training loss: 1.895748496055603 = 1.8108676671981812 + 0.01 * 8.488079071044922
Epoch 60, val loss: 1.8074926137924194
Epoch 70, training loss: 1.862213373184204 = 1.77976393699646 + 0.01 * 8.244945526123047
Epoch 70, val loss: 1.782248616218567
Epoch 80, training loss: 1.8235931396484375 = 1.742555856704712 + 0.01 * 8.103726387023926
Epoch 80, val loss: 1.751634955406189
Epoch 90, training loss: 1.7701232433319092 = 1.6916561126708984 + 0.01 * 7.846714019775391
Epoch 90, val loss: 1.7077618837356567
Epoch 100, training loss: 1.6975135803222656 = 1.621887445449829 + 0.01 * 7.562608242034912
Epoch 100, val loss: 1.6471757888793945
Epoch 110, training loss: 1.6090439558029175 = 1.5353740453720093 + 0.01 * 7.36699104309082
Epoch 110, val loss: 1.5736236572265625
Epoch 120, training loss: 1.5137132406234741 = 1.440743327140808 + 0.01 * 7.296996116638184
Epoch 120, val loss: 1.4960824251174927
Epoch 130, training loss: 1.4179458618164062 = 1.3453960418701172 + 0.01 * 7.254988193511963
Epoch 130, val loss: 1.4219404458999634
Epoch 140, training loss: 1.3225536346435547 = 1.2502310276031494 + 0.01 * 7.2322564125061035
Epoch 140, val loss: 1.350982666015625
Epoch 150, training loss: 1.2257499694824219 = 1.1535264253616333 + 0.01 * 7.222352981567383
Epoch 150, val loss: 1.2802233695983887
Epoch 160, training loss: 1.126888394355774 = 1.0547540187835693 + 0.01 * 7.213435649871826
Epoch 160, val loss: 1.2066363096237183
Epoch 170, training loss: 1.0279476642608643 = 0.9559471607208252 + 0.01 * 7.2000532150268555
Epoch 170, val loss: 1.1323293447494507
Epoch 180, training loss: 0.9332388043403625 = 0.861414909362793 + 0.01 * 7.182392120361328
Epoch 180, val loss: 1.061071515083313
Epoch 190, training loss: 0.846429705619812 = 0.7748029232025146 + 0.01 * 7.162680625915527
Epoch 190, val loss: 0.997215986251831
Epoch 200, training loss: 0.7688342928886414 = 0.6973914504051208 + 0.01 * 7.144282817840576
Epoch 200, val loss: 0.943577229976654
Epoch 210, training loss: 0.6994705200195312 = 0.6281869411468506 + 0.01 * 7.128355979919434
Epoch 210, val loss: 0.9001821279525757
Epoch 220, training loss: 0.6367472410202026 = 0.5655785799026489 + 0.01 * 7.11686372756958
Epoch 220, val loss: 0.8649174571037292
Epoch 230, training loss: 0.5795153379440308 = 0.5084009170532227 + 0.01 * 7.111439228057861
Epoch 230, val loss: 0.8358590006828308
Epoch 240, training loss: 0.5267643928527832 = 0.45568838715553284 + 0.01 * 7.107602596282959
Epoch 240, val loss: 0.8115019202232361
Epoch 250, training loss: 0.4776991307735443 = 0.40666815638542175 + 0.01 * 7.103098392486572
Epoch 250, val loss: 0.7913208603858948
Epoch 260, training loss: 0.4320729672908783 = 0.3610641658306122 + 0.01 * 7.100880146026611
Epoch 260, val loss: 0.7749562859535217
Epoch 270, training loss: 0.38994231820106506 = 0.31895798444747925 + 0.01 * 7.098433494567871
Epoch 270, val loss: 0.7622467875480652
Epoch 280, training loss: 0.3515332341194153 = 0.2805430293083191 + 0.01 * 7.099021911621094
Epoch 280, val loss: 0.7529926896095276
Epoch 290, training loss: 0.3168606758117676 = 0.24590107798576355 + 0.01 * 7.095959663391113
Epoch 290, val loss: 0.7467527985572815
Epoch 300, training loss: 0.28585684299468994 = 0.21492266654968262 + 0.01 * 7.093419551849365
Epoch 300, val loss: 0.743313729763031
Epoch 310, training loss: 0.2583589255809784 = 0.18743622303009033 + 0.01 * 7.092270374298096
Epoch 310, val loss: 0.7424641847610474
Epoch 320, training loss: 0.23418956995010376 = 0.1632845252752304 + 0.01 * 7.090505599975586
Epoch 320, val loss: 0.744064211845398
Epoch 330, training loss: 0.21318845450878143 = 0.14230741560459137 + 0.01 * 7.088103771209717
Epoch 330, val loss: 0.747658371925354
Epoch 340, training loss: 0.19513759016990662 = 0.1242750957608223 + 0.01 * 7.086248874664307
Epoch 340, val loss: 0.7528953552246094
Epoch 350, training loss: 0.1797136664390564 = 0.1088731437921524 + 0.01 * 7.0840535163879395
Epoch 350, val loss: 0.7594878673553467
Epoch 360, training loss: 0.16656064987182617 = 0.09574370831251144 + 0.01 * 7.081695079803467
Epoch 360, val loss: 0.7671610116958618
Epoch 370, training loss: 0.15530270338058472 = 0.08452989906072617 + 0.01 * 7.0772809982299805
Epoch 370, val loss: 0.7757751941680908
Epoch 380, training loss: 0.1456420123577118 = 0.07490934431552887 + 0.01 * 7.073266506195068
Epoch 380, val loss: 0.7850631475448608
Epoch 390, training loss: 0.13732776045799255 = 0.06661579012870789 + 0.01 * 7.071196556091309
Epoch 390, val loss: 0.7950285077095032
Epoch 400, training loss: 0.13010184466838837 = 0.05943817272782326 + 0.01 * 7.066366672515869
Epoch 400, val loss: 0.805404782295227
Epoch 410, training loss: 0.12380184233188629 = 0.053204674273729324 + 0.01 * 7.059717178344727
Epoch 410, val loss: 0.8162895441055298
Epoch 420, training loss: 0.118411585688591 = 0.04778057709336281 + 0.01 * 7.063101291656494
Epoch 420, val loss: 0.8274610638618469
Epoch 430, training loss: 0.1135789304971695 = 0.04305640608072281 + 0.01 * 7.052252292633057
Epoch 430, val loss: 0.8388748168945312
Epoch 440, training loss: 0.10936380177736282 = 0.038933165371418 + 0.01 * 7.043063640594482
Epoch 440, val loss: 0.8504423499107361
Epoch 450, training loss: 0.10568652302026749 = 0.03532780706882477 + 0.01 * 7.035871505737305
Epoch 450, val loss: 0.8620772361755371
Epoch 460, training loss: 0.10245972871780396 = 0.03216995671391487 + 0.01 * 7.028977394104004
Epoch 460, val loss: 0.8737011551856995
Epoch 470, training loss: 0.09972043335437775 = 0.029400499537587166 + 0.01 * 7.031993389129639
Epoch 470, val loss: 0.8851956725120544
Epoch 480, training loss: 0.0971040278673172 = 0.026961280032992363 + 0.01 * 7.014274597167969
Epoch 480, val loss: 0.8966248035430908
Epoch 490, training loss: 0.09487184137105942 = 0.02480498142540455 + 0.01 * 7.006686687469482
Epoch 490, val loss: 0.9078534245491028
Epoch 500, training loss: 0.09289631247520447 = 0.022892780601978302 + 0.01 * 7.0003533363342285
Epoch 500, val loss: 0.9189386367797852
Epoch 510, training loss: 0.09109219163656235 = 0.021192142739892006 + 0.01 * 6.9900054931640625
Epoch 510, val loss: 0.9297472834587097
Epoch 520, training loss: 0.08990950137376785 = 0.019673721864819527 + 0.01 * 7.02357816696167
Epoch 520, val loss: 0.9404106736183167
Epoch 530, training loss: 0.08807210624217987 = 0.018315237015485764 + 0.01 * 6.975686550140381
Epoch 530, val loss: 0.9507615566253662
Epoch 540, training loss: 0.08681544661521912 = 0.017094681039452553 + 0.01 * 6.972076892852783
Epoch 540, val loss: 0.9609110355377197
Epoch 550, training loss: 0.0856490507721901 = 0.015994543209671974 + 0.01 * 6.965450763702393
Epoch 550, val loss: 0.970858097076416
Epoch 560, training loss: 0.08476568758487701 = 0.014999918639659882 + 0.01 * 6.976577281951904
Epoch 560, val loss: 0.9805092215538025
Epoch 570, training loss: 0.08364696055650711 = 0.014098350889980793 + 0.01 * 6.954860687255859
Epoch 570, val loss: 0.989920973777771
Epoch 580, training loss: 0.08270774036645889 = 0.01327879261225462 + 0.01 * 6.94289493560791
Epoch 580, val loss: 0.9991464018821716
Epoch 590, training loss: 0.08187544345855713 = 0.012531750835478306 + 0.01 * 6.9343695640563965
Epoch 590, val loss: 1.008072853088379
Epoch 600, training loss: 0.0815432220697403 = 0.011849421076476574 + 0.01 * 6.969380855560303
Epoch 600, val loss: 1.0168308019638062
Epoch 610, training loss: 0.08054163306951523 = 0.01122524868696928 + 0.01 * 6.931639194488525
Epoch 610, val loss: 1.0253679752349854
Epoch 620, training loss: 0.07982488721609116 = 0.010651911608874798 + 0.01 * 6.917297840118408
Epoch 620, val loss: 1.0336018800735474
Epoch 630, training loss: 0.0794023796916008 = 0.010123802348971367 + 0.01 * 6.927857398986816
Epoch 630, val loss: 1.0417312383651733
Epoch 640, training loss: 0.07866427302360535 = 0.009636735543608665 + 0.01 * 6.9027533531188965
Epoch 640, val loss: 1.049628734588623
Epoch 650, training loss: 0.07810814678668976 = 0.00918690674006939 + 0.01 * 6.892123699188232
Epoch 650, val loss: 1.0572727918624878
Epoch 660, training loss: 0.07794994115829468 = 0.008769799023866653 + 0.01 * 6.918015003204346
Epoch 660, val loss: 1.0648384094238281
Epoch 670, training loss: 0.0773291364312172 = 0.008383511565625668 + 0.01 * 6.894562721252441
Epoch 670, val loss: 1.0721051692962646
Epoch 680, training loss: 0.0770425871014595 = 0.008024328388273716 + 0.01 * 6.901825904846191
Epoch 680, val loss: 1.0792922973632812
Epoch 690, training loss: 0.0763871893286705 = 0.007689578924328089 + 0.01 * 6.869760990142822
Epoch 690, val loss: 1.0862257480621338
Epoch 700, training loss: 0.07627429068088531 = 0.007377363741397858 + 0.01 * 6.889692783355713
Epoch 700, val loss: 1.0930356979370117
Epoch 710, training loss: 0.07571353018283844 = 0.007086167577654123 + 0.01 * 6.862736701965332
Epoch 710, val loss: 1.0997164249420166
Epoch 720, training loss: 0.0754057914018631 = 0.006813697516918182 + 0.01 * 6.8592095375061035
Epoch 720, val loss: 1.1061874628067017
Epoch 730, training loss: 0.07495962083339691 = 0.006558314431458712 + 0.01 * 6.840130805969238
Epoch 730, val loss: 1.1124992370605469
Epoch 740, training loss: 0.07506462186574936 = 0.006318651605397463 + 0.01 * 6.874597549438477
Epoch 740, val loss: 1.118654489517212
Epoch 750, training loss: 0.07445875555276871 = 0.006094214040786028 + 0.01 * 6.836453914642334
Epoch 750, val loss: 1.1246980428695679
Epoch 760, training loss: 0.0743994265794754 = 0.005882464814931154 + 0.01 * 6.851696014404297
Epoch 760, val loss: 1.1305391788482666
Epoch 770, training loss: 0.0740240290760994 = 0.005683575756847858 + 0.01 * 6.834044933319092
Epoch 770, val loss: 1.1363677978515625
Epoch 780, training loss: 0.07379566878080368 = 0.0054954239167273045 + 0.01 * 6.8300251960754395
Epoch 780, val loss: 1.1419533491134644
Epoch 790, training loss: 0.0735112652182579 = 0.005317805800586939 + 0.01 * 6.819345951080322
Epoch 790, val loss: 1.1475145816802979
Epoch 800, training loss: 0.07319983839988708 = 0.005149906035512686 + 0.01 * 6.804993152618408
Epoch 800, val loss: 1.152896523475647
Epoch 810, training loss: 0.07306976616382599 = 0.004991074092686176 + 0.01 * 6.8078694343566895
Epoch 810, val loss: 1.1580991744995117
Epoch 820, training loss: 0.07296553254127502 = 0.004840750712901354 + 0.01 * 6.812478065490723
Epoch 820, val loss: 1.1632441282272339
Epoch 830, training loss: 0.0726340189576149 = 0.004698094911873341 + 0.01 * 6.7935919761657715
Epoch 830, val loss: 1.1683388948440552
Epoch 840, training loss: 0.0725669339299202 = 0.0045626284554600716 + 0.01 * 6.800430774688721
Epoch 840, val loss: 1.1732977628707886
Epoch 850, training loss: 0.07223071157932281 = 0.0044338698498904705 + 0.01 * 6.779684066772461
Epoch 850, val loss: 1.1780794858932495
Epoch 860, training loss: 0.07236005365848541 = 0.0043111699633300304 + 0.01 * 6.80488920211792
Epoch 860, val loss: 1.1827796697616577
Epoch 870, training loss: 0.07206074893474579 = 0.004194754175841808 + 0.01 * 6.786600112915039
Epoch 870, val loss: 1.1875120401382446
Epoch 880, training loss: 0.07172194868326187 = 0.004083399195224047 + 0.01 * 6.76385498046875
Epoch 880, val loss: 1.1920055150985718
Epoch 890, training loss: 0.07182847708463669 = 0.003977430984377861 + 0.01 * 6.785104274749756
Epoch 890, val loss: 1.1965200901031494
Epoch 900, training loss: 0.07155946642160416 = 0.003876312868669629 + 0.01 * 6.768315315246582
Epoch 900, val loss: 1.200798749923706
Epoch 910, training loss: 0.0716506764292717 = 0.0037796779070049524 + 0.01 * 6.787099838256836
Epoch 910, val loss: 1.2050834894180298
Epoch 920, training loss: 0.07129031419754028 = 0.0036875600926578045 + 0.01 * 6.760275363922119
Epoch 920, val loss: 1.2093167304992676
Epoch 930, training loss: 0.0710788294672966 = 0.003599189454689622 + 0.01 * 6.747963905334473
Epoch 930, val loss: 1.2134156227111816
Epoch 940, training loss: 0.07119838893413544 = 0.003514555748552084 + 0.01 * 6.768383026123047
Epoch 940, val loss: 1.2174304723739624
Epoch 950, training loss: 0.07093396782875061 = 0.00343361496925354 + 0.01 * 6.750035285949707
Epoch 950, val loss: 1.2214679718017578
Epoch 960, training loss: 0.0710243284702301 = 0.0033560050651431084 + 0.01 * 6.7668328285217285
Epoch 960, val loss: 1.2253451347351074
Epoch 970, training loss: 0.07073043286800385 = 0.003281598212197423 + 0.01 * 6.7448835372924805
Epoch 970, val loss: 1.2291488647460938
Epoch 980, training loss: 0.07085293531417847 = 0.0032101511023938656 + 0.01 * 6.764278888702393
Epoch 980, val loss: 1.232939600944519
Epoch 990, training loss: 0.07054353505373001 = 0.0031416122801601887 + 0.01 * 6.740192413330078
Epoch 990, val loss: 1.2365899085998535
Epoch 1000, training loss: 0.0703926831483841 = 0.003075886983424425 + 0.01 * 6.731679439544678
Epoch 1000, val loss: 1.2401927709579468
Epoch 1010, training loss: 0.07062030583620071 = 0.0030125626362860203 + 0.01 * 6.760774612426758
Epoch 1010, val loss: 1.2437406778335571
Epoch 1020, training loss: 0.07019421458244324 = 0.0029517170041799545 + 0.01 * 6.724249839782715
Epoch 1020, val loss: 1.247269630432129
Epoch 1030, training loss: 0.07048721611499786 = 0.0028930343687534332 + 0.01 * 6.759418487548828
Epoch 1030, val loss: 1.250671625137329
Epoch 1040, training loss: 0.0702405646443367 = 0.002836809726431966 + 0.01 * 6.740375518798828
Epoch 1040, val loss: 1.2541117668151855
Epoch 1050, training loss: 0.07019930332899094 = 0.0027825694996863604 + 0.01 * 6.741673946380615
Epoch 1050, val loss: 1.2574526071548462
Epoch 1060, training loss: 0.06994359195232391 = 0.002730185864493251 + 0.01 * 6.721340656280518
Epoch 1060, val loss: 1.2606614828109741
Epoch 1070, training loss: 0.06987877190113068 = 0.0026798027101904154 + 0.01 * 6.7198967933654785
Epoch 1070, val loss: 1.2638297080993652
Epoch 1080, training loss: 0.06978818029165268 = 0.0026311196852475405 + 0.01 * 6.7157063484191895
Epoch 1080, val loss: 1.2670344114303589
Epoch 1090, training loss: 0.06980834156274796 = 0.0025842459872365 + 0.01 * 6.722409248352051
Epoch 1090, val loss: 1.2701581716537476
Epoch 1100, training loss: 0.06962696462869644 = 0.002538914093747735 + 0.01 * 6.708805561065674
Epoch 1100, val loss: 1.273184895515442
Epoch 1110, training loss: 0.06968327611684799 = 0.0024951575323939323 + 0.01 * 6.718811988830566
Epoch 1110, val loss: 1.2762117385864258
Epoch 1120, training loss: 0.06957106292247772 = 0.002453017979860306 + 0.01 * 6.711804389953613
Epoch 1120, val loss: 1.2792048454284668
Epoch 1130, training loss: 0.06947501748800278 = 0.0024121117312461138 + 0.01 * 6.706291198730469
Epoch 1130, val loss: 1.28208589553833
Epoch 1140, training loss: 0.06970024108886719 = 0.002372595015913248 + 0.01 * 6.732765197753906
Epoch 1140, val loss: 1.284911870956421
Epoch 1150, training loss: 0.06940434873104095 = 0.0023344524670392275 + 0.01 * 6.706989765167236
Epoch 1150, val loss: 1.2877835035324097
Epoch 1160, training loss: 0.06920485198497772 = 0.002297450555488467 + 0.01 * 6.690739631652832
Epoch 1160, val loss: 1.2905528545379639
Epoch 1170, training loss: 0.06913916021585464 = 0.0022617410868406296 + 0.01 * 6.687742233276367
Epoch 1170, val loss: 1.2932894229888916
Epoch 1180, training loss: 0.06969785690307617 = 0.0022270716726779938 + 0.01 * 6.747078895568848
Epoch 1180, val loss: 1.2959940433502197
Epoch 1190, training loss: 0.06904866546392441 = 0.002193409251049161 + 0.01 * 6.685525894165039
Epoch 1190, val loss: 1.298637866973877
Epoch 1200, training loss: 0.06912006437778473 = 0.002160899806767702 + 0.01 * 6.695916652679443
Epoch 1200, val loss: 1.3012385368347168
Epoch 1210, training loss: 0.06904052942991257 = 0.002129363128915429 + 0.01 * 6.691116809844971
Epoch 1210, val loss: 1.3038486242294312
Epoch 1220, training loss: 0.06914538890123367 = 0.002098781056702137 + 0.01 * 6.704660892486572
Epoch 1220, val loss: 1.3063263893127441
Epoch 1230, training loss: 0.06906960904598236 = 0.0020692411344498396 + 0.01 * 6.700036525726318
Epoch 1230, val loss: 1.3088487386703491
Epoch 1240, training loss: 0.06889522075653076 = 0.002040478400886059 + 0.01 * 6.685474395751953
Epoch 1240, val loss: 1.3112516403198242
Epoch 1250, training loss: 0.06890847533941269 = 0.002012621145695448 + 0.01 * 6.6895856857299805
Epoch 1250, val loss: 1.3137141466140747
Epoch 1260, training loss: 0.06892206519842148 = 0.0019854784477502108 + 0.01 * 6.69365930557251
Epoch 1260, val loss: 1.3159981966018677
Epoch 1270, training loss: 0.06875395774841309 = 0.001959164859727025 + 0.01 * 6.679479122161865
Epoch 1270, val loss: 1.318377137184143
Epoch 1280, training loss: 0.0688716471195221 = 0.001933566527441144 + 0.01 * 6.693808078765869
Epoch 1280, val loss: 1.3206838369369507
Epoch 1290, training loss: 0.06856133043766022 = 0.0019088519038632512 + 0.01 * 6.665247917175293
Epoch 1290, val loss: 1.322943925857544
Epoch 1300, training loss: 0.06850653886795044 = 0.0018847277387976646 + 0.01 * 6.662181377410889
Epoch 1300, val loss: 1.325203776359558
Epoch 1310, training loss: 0.06869129836559296 = 0.0018612953135743737 + 0.01 * 6.683000087738037
Epoch 1310, val loss: 1.3272943496704102
Epoch 1320, training loss: 0.0685778558254242 = 0.0018385328585281968 + 0.01 * 6.673932075500488
Epoch 1320, val loss: 1.3295072317123413
Epoch 1330, training loss: 0.06881681084632874 = 0.0018163800705224276 + 0.01 * 6.700043201446533
Epoch 1330, val loss: 1.3315926790237427
Epoch 1340, training loss: 0.0685439184308052 = 0.0017948225140571594 + 0.01 * 6.674910068511963
Epoch 1340, val loss: 1.3337297439575195
Epoch 1350, training loss: 0.06876190751791 = 0.0017738669412210584 + 0.01 * 6.69880485534668
Epoch 1350, val loss: 1.335773229598999
Epoch 1360, training loss: 0.06839385628700256 = 0.0017534255748614669 + 0.01 * 6.664043426513672
Epoch 1360, val loss: 1.3377830982208252
Epoch 1370, training loss: 0.06863691657781601 = 0.001733590615913272 + 0.01 * 6.690332412719727
Epoch 1370, val loss: 1.3397902250289917
Epoch 1380, training loss: 0.06828602403402328 = 0.0017142902361229062 + 0.01 * 6.6571736335754395
Epoch 1380, val loss: 1.3417787551879883
Epoch 1390, training loss: 0.06827154755592346 = 0.001695498125627637 + 0.01 * 6.657605171203613
Epoch 1390, val loss: 1.3437330722808838
Epoch 1400, training loss: 0.0683138445019722 = 0.0016771291848272085 + 0.01 * 6.663671970367432
Epoch 1400, val loss: 1.345629096031189
Epoch 1410, training loss: 0.06813187152147293 = 0.0016592323081567883 + 0.01 * 6.64726448059082
Epoch 1410, val loss: 1.3475326299667358
Epoch 1420, training loss: 0.06840931624174118 = 0.0016418277518823743 + 0.01 * 6.676749229431152
Epoch 1420, val loss: 1.3493876457214355
Epoch 1430, training loss: 0.06804468482732773 = 0.001624863944016397 + 0.01 * 6.641982078552246
Epoch 1430, val loss: 1.3512765169143677
Epoch 1440, training loss: 0.06816092878580093 = 0.0016083403024822474 + 0.01 * 6.655258655548096
Epoch 1440, val loss: 1.3530763387680054
Epoch 1450, training loss: 0.06815335899591446 = 0.001592147396877408 + 0.01 * 6.656121253967285
Epoch 1450, val loss: 1.354799509048462
Epoch 1460, training loss: 0.06800143420696259 = 0.0015764726558700204 + 0.01 * 6.642495632171631
Epoch 1460, val loss: 1.356653094291687
Epoch 1470, training loss: 0.06834761798381805 = 0.001561170443892479 + 0.01 * 6.67864465713501
Epoch 1470, val loss: 1.3583881855010986
Epoch 1480, training loss: 0.06794609129428864 = 0.0015461756847798824 + 0.01 * 6.639991760253906
Epoch 1480, val loss: 1.360123872756958
Epoch 1490, training loss: 0.068130724132061 = 0.0015315517084673047 + 0.01 * 6.65991735458374
Epoch 1490, val loss: 1.3617743253707886
Epoch 1500, training loss: 0.06818042695522308 = 0.0015172894345596433 + 0.01 * 6.666314125061035
Epoch 1500, val loss: 1.3635032176971436
Epoch 1510, training loss: 0.0678931400179863 = 0.0015033698873594403 + 0.01 * 6.63897705078125
Epoch 1510, val loss: 1.3651522397994995
Epoch 1520, training loss: 0.06787477433681488 = 0.0014897469663992524 + 0.01 * 6.638503074645996
Epoch 1520, val loss: 1.3667848110198975
Epoch 1530, training loss: 0.06813595443964005 = 0.0014764555962756276 + 0.01 * 6.66594934463501
Epoch 1530, val loss: 1.3683063983917236
Epoch 1540, training loss: 0.06773348152637482 = 0.001463562250137329 + 0.01 * 6.6269917488098145
Epoch 1540, val loss: 1.369982123374939
Epoch 1550, training loss: 0.06774535775184631 = 0.0014508995227515697 + 0.01 * 6.629446506500244
Epoch 1550, val loss: 1.3715393543243408
Epoch 1560, training loss: 0.0675630122423172 = 0.00143849803134799 + 0.01 * 6.612451553344727
Epoch 1560, val loss: 1.37307870388031
Epoch 1570, training loss: 0.06772932410240173 = 0.001426382572390139 + 0.01 * 6.630293846130371
Epoch 1570, val loss: 1.3745781183242798
Epoch 1580, training loss: 0.06799844652414322 = 0.001414578058756888 + 0.01 * 6.658387660980225
Epoch 1580, val loss: 1.3760994672775269
Epoch 1590, training loss: 0.06756957620382309 = 0.0014029738958925009 + 0.01 * 6.616660118103027
Epoch 1590, val loss: 1.3776050806045532
Epoch 1600, training loss: 0.06779775023460388 = 0.0013917051255702972 + 0.01 * 6.640604496002197
Epoch 1600, val loss: 1.3790991306304932
Epoch 1610, training loss: 0.06776030361652374 = 0.0013805931666865945 + 0.01 * 6.637970924377441
Epoch 1610, val loss: 1.3805967569351196
Epoch 1620, training loss: 0.06751833856105804 = 0.0013697375543415546 + 0.01 * 6.6148600578308105
Epoch 1620, val loss: 1.382048487663269
Epoch 1630, training loss: 0.06758800148963928 = 0.0013591080205515027 + 0.01 * 6.622889518737793
Epoch 1630, val loss: 1.3834823369979858
Epoch 1640, training loss: 0.06780186295509338 = 0.0013487624237313867 + 0.01 * 6.645309925079346
Epoch 1640, val loss: 1.384934902191162
Epoch 1650, training loss: 0.06734053790569305 = 0.0013385185739025474 + 0.01 * 6.6002020835876465
Epoch 1650, val loss: 1.3863251209259033
Epoch 1660, training loss: 0.06751998513936996 = 0.0013285487657412887 + 0.01 * 6.619143962860107
Epoch 1660, val loss: 1.3877331018447876
Epoch 1670, training loss: 0.06750611215829849 = 0.0013188045704737306 + 0.01 * 6.6187310218811035
Epoch 1670, val loss: 1.3891816139221191
Epoch 1680, training loss: 0.06744575500488281 = 0.0013091908767819405 + 0.01 * 6.613656044006348
Epoch 1680, val loss: 1.3905445337295532
Epoch 1690, training loss: 0.06744694709777832 = 0.0012998100137338042 + 0.01 * 6.614713668823242
Epoch 1690, val loss: 1.3919129371643066
Epoch 1700, training loss: 0.06757408380508423 = 0.001290582469664514 + 0.01 * 6.628350257873535
Epoch 1700, val loss: 1.3932416439056396
Epoch 1710, training loss: 0.0673486739397049 = 0.0012815453810617328 + 0.01 * 6.606712818145752
Epoch 1710, val loss: 1.394662618637085
Epoch 1720, training loss: 0.0675489604473114 = 0.0012727375142276287 + 0.01 * 6.627622127532959
Epoch 1720, val loss: 1.3960051536560059
Epoch 1730, training loss: 0.06726644188165665 = 0.0012640270870178938 + 0.01 * 6.600241661071777
Epoch 1730, val loss: 1.3973466157913208
Epoch 1740, training loss: 0.06754324585199356 = 0.0012555154971778393 + 0.01 * 6.628773212432861
Epoch 1740, val loss: 1.3986124992370605
Epoch 1750, training loss: 0.06714870035648346 = 0.0012472044909372926 + 0.01 * 6.590149402618408
Epoch 1750, val loss: 1.4000059366226196
Epoch 1760, training loss: 0.06741061806678772 = 0.0012390254996716976 + 0.01 * 6.617159843444824
Epoch 1760, val loss: 1.4012703895568848
Epoch 1770, training loss: 0.06721042841672897 = 0.001230984227731824 + 0.01 * 6.597944736480713
Epoch 1770, val loss: 1.4025020599365234
Epoch 1780, training loss: 0.06702124327421188 = 0.0012231168802827597 + 0.01 * 6.579812526702881
Epoch 1780, val loss: 1.403805136680603
Epoch 1790, training loss: 0.06713327765464783 = 0.0012154169380664825 + 0.01 * 6.591786861419678
Epoch 1790, val loss: 1.405077576637268
Epoch 1800, training loss: 0.0670766532421112 = 0.0012077941792085767 + 0.01 * 6.586886405944824
Epoch 1800, val loss: 1.406295895576477
Epoch 1810, training loss: 0.0670863687992096 = 0.0012003438314422965 + 0.01 * 6.5886030197143555
Epoch 1810, val loss: 1.4075742959976196
Epoch 1820, training loss: 0.06741278618574142 = 0.0011930776527151465 + 0.01 * 6.621971130371094
Epoch 1820, val loss: 1.4087964296340942
Epoch 1830, training loss: 0.06714814901351929 = 0.0011858097277581692 + 0.01 * 6.59623384475708
Epoch 1830, val loss: 1.40999436378479
Epoch 1840, training loss: 0.06703568994998932 = 0.0011787416879087687 + 0.01 * 6.585695266723633
Epoch 1840, val loss: 1.411210298538208
Epoch 1850, training loss: 0.06702054291963577 = 0.0011717916931957006 + 0.01 * 6.584875583648682
Epoch 1850, val loss: 1.412354826927185
Epoch 1860, training loss: 0.06702850013971329 = 0.0011649623047560453 + 0.01 * 6.586353302001953
Epoch 1860, val loss: 1.413565993309021
Epoch 1870, training loss: 0.06700550019741058 = 0.001158247352577746 + 0.01 * 6.584725856781006
Epoch 1870, val loss: 1.4146794080734253
Epoch 1880, training loss: 0.06668170541524887 = 0.0011516594095155597 + 0.01 * 6.553004741668701
Epoch 1880, val loss: 1.41586434841156
Epoch 1890, training loss: 0.0671936422586441 = 0.001145173329859972 + 0.01 * 6.604846954345703
Epoch 1890, val loss: 1.4170050621032715
Epoch 1900, training loss: 0.06681826710700989 = 0.001138780266046524 + 0.01 * 6.567948341369629
Epoch 1900, val loss: 1.4181225299835205
Epoch 1910, training loss: 0.0670003816485405 = 0.001132513745687902 + 0.01 * 6.586787223815918
Epoch 1910, val loss: 1.4192678928375244
Epoch 1920, training loss: 0.06687366962432861 = 0.00112633325625211 + 0.01 * 6.574733734130859
Epoch 1920, val loss: 1.4203771352767944
Epoch 1930, training loss: 0.06697818636894226 = 0.0011202520690858364 + 0.01 * 6.5857930183410645
Epoch 1930, val loss: 1.4214956760406494
Epoch 1940, training loss: 0.06687093526124954 = 0.0011143054580315948 + 0.01 * 6.575663089752197
Epoch 1940, val loss: 1.4226011037826538
Epoch 1950, training loss: 0.06678774207830429 = 0.001108394586481154 + 0.01 * 6.567934989929199
Epoch 1950, val loss: 1.4236737489700317
Epoch 1960, training loss: 0.0667501762509346 = 0.0011026227148249745 + 0.01 * 6.564754962921143
Epoch 1960, val loss: 1.4247665405273438
Epoch 1970, training loss: 0.06671140342950821 = 0.0010968949645757675 + 0.01 * 6.561450958251953
Epoch 1970, val loss: 1.4257997274398804
Epoch 1980, training loss: 0.06663849949836731 = 0.0010912538273259997 + 0.01 * 6.55472469329834
Epoch 1980, val loss: 1.426904559135437
Epoch 1990, training loss: 0.06660323590040207 = 0.0010856996523216367 + 0.01 * 6.551753520965576
Epoch 1990, val loss: 1.4279893636703491
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8060094886663153
The final CL Acc:0.77037, 0.01386, The final GNN Acc:0.80373, 0.00194
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13200])
remove edge: torch.Size([2, 7902])
updated graph: torch.Size([2, 10546])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.03644061088562 = 1.950472116470337 + 0.01 * 8.596855163574219
Epoch 0, val loss: 1.959945797920227
Epoch 10, training loss: 2.026486396789551 = 1.9405186176300049 + 0.01 * 8.596783638000488
Epoch 10, val loss: 1.9496012926101685
Epoch 20, training loss: 2.0142533779144287 = 1.9282883405685425 + 0.01 * 8.596503257751465
Epoch 20, val loss: 1.9369522333145142
Epoch 30, training loss: 1.9968808889389038 = 1.910927653312683 + 0.01 * 8.595318794250488
Epoch 30, val loss: 1.919111967086792
Epoch 40, training loss: 1.9706960916519165 = 1.8848289251327515 + 0.01 * 8.58671760559082
Epoch 40, val loss: 1.8927251100540161
Epoch 50, training loss: 1.932836651802063 = 1.8475394248962402 + 0.01 * 8.5297212600708
Epoch 50, val loss: 1.8563426733016968
Epoch 60, training loss: 1.8865374326705933 = 1.8036706447601318 + 0.01 * 8.286675453186035
Epoch 60, val loss: 1.8163448572158813
Epoch 70, training loss: 1.8456920385360718 = 1.764037013053894 + 0.01 * 8.165504455566406
Epoch 70, val loss: 1.7809839248657227
Epoch 80, training loss: 1.79879629611969 = 1.718392014503479 + 0.01 * 8.040431022644043
Epoch 80, val loss: 1.7370734214782715
Epoch 90, training loss: 1.7330923080444336 = 1.6546013355255127 + 0.01 * 7.8490986824035645
Epoch 90, val loss: 1.6792998313903809
Epoch 100, training loss: 1.645845651626587 = 1.569667100906372 + 0.01 * 7.617855072021484
Epoch 100, val loss: 1.6059715747833252
Epoch 110, training loss: 1.5441911220550537 = 1.4706249237060547 + 0.01 * 7.356619834899902
Epoch 110, val loss: 1.5221660137176514
Epoch 120, training loss: 1.4411205053329468 = 1.3682641983032227 + 0.01 * 7.285632133483887
Epoch 120, val loss: 1.435068964958191
Epoch 130, training loss: 1.3392540216445923 = 1.2669795751571655 + 0.01 * 7.227441310882568
Epoch 130, val loss: 1.3496707677841187
Epoch 140, training loss: 1.2383722066879272 = 1.166220784187317 + 0.01 * 7.215139389038086
Epoch 140, val loss: 1.266327977180481
Epoch 150, training loss: 1.1395174264907837 = 1.0674359798431396 + 0.01 * 7.208144664764404
Epoch 150, val loss: 1.185828685760498
Epoch 160, training loss: 1.04495108127594 = 0.9729284644126892 + 0.01 * 7.20226526260376
Epoch 160, val loss: 1.1103806495666504
Epoch 170, training loss: 0.9557593464851379 = 0.8837883472442627 + 0.01 * 7.197098731994629
Epoch 170, val loss: 1.0400182008743286
Epoch 180, training loss: 0.8724002838134766 = 0.8005082011222839 + 0.01 * 7.189211368560791
Epoch 180, val loss: 0.9753391742706299
Epoch 190, training loss: 0.7960548996925354 = 0.7242760062217712 + 0.01 * 7.1778883934021
Epoch 190, val loss: 0.9173702001571655
Epoch 200, training loss: 0.7282851934432983 = 0.6566582918167114 + 0.01 * 7.162692070007324
Epoch 200, val loss: 0.8683114647865295
Epoch 210, training loss: 0.6696878671646118 = 0.5982118844985962 + 0.01 * 7.147599220275879
Epoch 210, val loss: 0.8293786644935608
Epoch 220, training loss: 0.619553804397583 = 0.5482584834098816 + 0.01 * 7.129533767700195
Epoch 220, val loss: 0.800204336643219
Epoch 230, training loss: 0.5765968561172485 = 0.5054556727409363 + 0.01 * 7.1141157150268555
Epoch 230, val loss: 0.779231071472168
Epoch 240, training loss: 0.5392853021621704 = 0.46819621324539185 + 0.01 * 7.1089067459106445
Epoch 240, val loss: 0.764153778553009
Epoch 250, training loss: 0.5058066248893738 = 0.4348318576812744 + 0.01 * 7.097476005554199
Epoch 250, val loss: 0.7528361678123474
Epoch 260, training loss: 0.4748142957687378 = 0.40388423204421997 + 0.01 * 7.0930070877075195
Epoch 260, val loss: 0.7437646985054016
Epoch 270, training loss: 0.4450322389602661 = 0.3741441071033478 + 0.01 * 7.088813781738281
Epoch 270, val loss: 0.7359362840652466
Epoch 280, training loss: 0.41562920808792114 = 0.34476858377456665 + 0.01 * 7.086061477661133
Epoch 280, val loss: 0.7286257147789001
Epoch 290, training loss: 0.3862840533256531 = 0.3154464066028595 + 0.01 * 7.083766460418701
Epoch 290, val loss: 0.7219160795211792
Epoch 300, training loss: 0.35724759101867676 = 0.2864293158054352 + 0.01 * 7.081827640533447
Epoch 300, val loss: 0.715932309627533
Epoch 310, training loss: 0.32907259464263916 = 0.2582712173461914 + 0.01 * 7.080138683319092
Epoch 310, val loss: 0.7110401391983032
Epoch 320, training loss: 0.3022312521934509 = 0.23144464194774628 + 0.01 * 7.078660011291504
Epoch 320, val loss: 0.7071880102157593
Epoch 330, training loss: 0.2769220769405365 = 0.20614923536777496 + 0.01 * 7.0772833824157715
Epoch 330, val loss: 0.704326868057251
Epoch 340, training loss: 0.2532253861427307 = 0.18244130909442902 + 0.01 * 7.078409671783447
Epoch 340, val loss: 0.7022263407707214
Epoch 350, training loss: 0.23118853569030762 = 0.16043096780776978 + 0.01 * 7.0757575035095215
Epoch 350, val loss: 0.7008486986160278
Epoch 360, training loss: 0.2110096514225006 = 0.1402818113565445 + 0.01 * 7.072783946990967
Epoch 360, val loss: 0.7001485228538513
Epoch 370, training loss: 0.1928444802761078 = 0.12214075028896332 + 0.01 * 7.070373058319092
Epoch 370, val loss: 0.7003529071807861
Epoch 380, training loss: 0.17674286663532257 = 0.10607627034187317 + 0.01 * 7.066659450531006
Epoch 380, val loss: 0.7016337513923645
Epoch 390, training loss: 0.16271081566810608 = 0.09205420315265656 + 0.01 * 7.065661907196045
Epoch 390, val loss: 0.7040069103240967
Epoch 400, training loss: 0.15053290128707886 = 0.07995694130659103 + 0.01 * 7.057596683502197
Epoch 400, val loss: 0.707508385181427
Epoch 410, training loss: 0.14013123512268066 = 0.06961459666490555 + 0.01 * 7.051663875579834
Epoch 410, val loss: 0.7121084332466125
Epoch 420, training loss: 0.13127580285072327 = 0.06083126366138458 + 0.01 * 7.044454574584961
Epoch 420, val loss: 0.7174688577651978
Epoch 430, training loss: 0.12379173934459686 = 0.05340432748198509 + 0.01 * 7.038740634918213
Epoch 430, val loss: 0.7236310839653015
Epoch 440, training loss: 0.11758281290531158 = 0.04712648317217827 + 0.01 * 7.045633792877197
Epoch 440, val loss: 0.7301887273788452
Epoch 450, training loss: 0.11202098429203033 = 0.04181605577468872 + 0.01 * 7.020493030548096
Epoch 450, val loss: 0.7371091246604919
Epoch 460, training loss: 0.10741670429706573 = 0.03730802610516548 + 0.01 * 7.010867595672607
Epoch 460, val loss: 0.7442065477371216
Epoch 470, training loss: 0.1035071462392807 = 0.03346356377005577 + 0.01 * 7.004357814788818
Epoch 470, val loss: 0.7513943910598755
Epoch 480, training loss: 0.10004949569702148 = 0.030168386176228523 + 0.01 * 6.9881110191345215
Epoch 480, val loss: 0.7586058974266052
Epoch 490, training loss: 0.09717493504285812 = 0.0273289754986763 + 0.01 * 6.984595775604248
Epoch 490, val loss: 0.7657560110092163
Epoch 500, training loss: 0.09459951519966125 = 0.024869343265891075 + 0.01 * 6.97301721572876
Epoch 500, val loss: 0.7728652954101562
Epoch 510, training loss: 0.09229587018489838 = 0.022727493196725845 + 0.01 * 6.9568376541137695
Epoch 510, val loss: 0.7798007726669312
Epoch 520, training loss: 0.09055715799331665 = 0.02085231803357601 + 0.01 * 6.970484256744385
Epoch 520, val loss: 0.7866520881652832
Epoch 530, training loss: 0.08864083141088486 = 0.019205106422305107 + 0.01 * 6.943572521209717
Epoch 530, val loss: 0.7933161854743958
Epoch 540, training loss: 0.08708525449037552 = 0.01774916611611843 + 0.01 * 6.933608531951904
Epoch 540, val loss: 0.7998005747795105
Epoch 550, training loss: 0.085820771753788 = 0.01645687222480774 + 0.01 * 6.936390399932861
Epoch 550, val loss: 0.8061708211898804
Epoch 560, training loss: 0.08447633683681488 = 0.015305637381970882 + 0.01 * 6.917069911956787
Epoch 560, val loss: 0.8122978210449219
Epoch 570, training loss: 0.08356136083602905 = 0.014275701716542244 + 0.01 * 6.9285664558410645
Epoch 570, val loss: 0.8182723522186279
Epoch 580, training loss: 0.08239170163869858 = 0.013351562432944775 + 0.01 * 6.9040141105651855
Epoch 580, val loss: 0.8240625858306885
Epoch 590, training loss: 0.08148421347141266 = 0.01251932978630066 + 0.01 * 6.896488666534424
Epoch 590, val loss: 0.8297513723373413
Epoch 600, training loss: 0.08065443485975266 = 0.011767445132136345 + 0.01 * 6.888699054718018
Epoch 600, val loss: 0.8352165222167969
Epoch 610, training loss: 0.07984456419944763 = 0.011085770092904568 + 0.01 * 6.875879287719727
Epoch 610, val loss: 0.8405836820602417
Epoch 620, training loss: 0.07914752513170242 = 0.010465884581208229 + 0.01 * 6.8681640625
Epoch 620, val loss: 0.8457726240158081
Epoch 630, training loss: 0.07869499921798706 = 0.009900275617837906 + 0.01 * 6.8794732093811035
Epoch 630, val loss: 0.8508563041687012
Epoch 640, training loss: 0.07800858467817307 = 0.009383306838572025 + 0.01 * 6.862528324127197
Epoch 640, val loss: 0.8557363152503967
Epoch 650, training loss: 0.07734832167625427 = 0.008909007534384727 + 0.01 * 6.843931674957275
Epoch 650, val loss: 0.8605217337608337
Epoch 660, training loss: 0.07740951329469681 = 0.008472487330436707 + 0.01 * 6.893702983856201
Epoch 660, val loss: 0.8652331233024597
Epoch 670, training loss: 0.07640603184700012 = 0.008070860058069229 + 0.01 * 6.833517074584961
Epoch 670, val loss: 0.8697413206100464
Epoch 680, training loss: 0.07650281488895416 = 0.0076999785378575325 + 0.01 * 6.880283832550049
Epoch 680, val loss: 0.8741497993469238
Epoch 690, training loss: 0.07570882886648178 = 0.007357446011155844 + 0.01 * 6.835138320922852
Epoch 690, val loss: 0.8783974647521973
Epoch 700, training loss: 0.07531628012657166 = 0.007039758842438459 + 0.01 * 6.827652931213379
Epoch 700, val loss: 0.8825439810752869
Epoch 710, training loss: 0.07490367442369461 = 0.0067441631108522415 + 0.01 * 6.815951824188232
Epoch 710, val loss: 0.8866276144981384
Epoch 720, training loss: 0.07470417022705078 = 0.0064687528647482395 + 0.01 * 6.82354211807251
Epoch 720, val loss: 0.8906649947166443
Epoch 730, training loss: 0.07430418580770493 = 0.006211893633008003 + 0.01 * 6.809229850769043
Epoch 730, val loss: 0.8944904804229736
Epoch 740, training loss: 0.074017234146595 = 0.005971906241029501 + 0.01 * 6.804532527923584
Epoch 740, val loss: 0.8982935547828674
Epoch 750, training loss: 0.07391582429409027 = 0.005747262388467789 + 0.01 * 6.816856384277344
Epoch 750, val loss: 0.9020162224769592
Epoch 760, training loss: 0.07355202734470367 = 0.005536924581974745 + 0.01 * 6.801510810852051
Epoch 760, val loss: 0.9055984616279602
Epoch 770, training loss: 0.07371735572814941 = 0.005339448340237141 + 0.01 * 6.8377909660339355
Epoch 770, val loss: 0.9091280102729797
Epoch 780, training loss: 0.07307135313749313 = 0.005154162645339966 + 0.01 * 6.791719436645508
Epoch 780, val loss: 0.9125277400016785
Epoch 790, training loss: 0.07282354682683945 = 0.004979782737791538 + 0.01 * 6.784376621246338
Epoch 790, val loss: 0.9158626198768616
Epoch 800, training loss: 0.0726374015212059 = 0.00481523759663105 + 0.01 * 6.782216548919678
Epoch 800, val loss: 0.9191434383392334
Epoch 810, training loss: 0.07238893955945969 = 0.004659968428313732 + 0.01 * 6.772897720336914
Epoch 810, val loss: 0.9223778247833252
Epoch 820, training loss: 0.07235386222600937 = 0.0045133065432310104 + 0.01 * 6.784055709838867
Epoch 820, val loss: 0.9255191087722778
Epoch 830, training loss: 0.07213196158409119 = 0.004374622832983732 + 0.01 * 6.775733947753906
Epoch 830, val loss: 0.9285239577293396
Epoch 840, training loss: 0.07204698771238327 = 0.0042433543130755424 + 0.01 * 6.780364036560059
Epoch 840, val loss: 0.9315364360809326
Epoch 850, training loss: 0.07173135876655579 = 0.004119024146348238 + 0.01 * 6.761233806610107
Epoch 850, val loss: 0.9344252943992615
Epoch 860, training loss: 0.07167553901672363 = 0.004000997170805931 + 0.01 * 6.767454147338867
Epoch 860, val loss: 0.9373292922973633
Epoch 870, training loss: 0.07155860215425491 = 0.003889077343046665 + 0.01 * 6.7669525146484375
Epoch 870, val loss: 0.940052330493927
Epoch 880, training loss: 0.07129441946744919 = 0.0037826825864613056 + 0.01 * 6.751173973083496
Epoch 880, val loss: 0.9428097605705261
Epoch 890, training loss: 0.07115906476974487 = 0.0036815183702856302 + 0.01 * 6.7477545738220215
Epoch 890, val loss: 0.9454318284988403
Epoch 900, training loss: 0.07128675282001495 = 0.0035850275307893753 + 0.01 * 6.770173072814941
Epoch 900, val loss: 0.9480706453323364
Epoch 910, training loss: 0.07099533826112747 = 0.0034931565169245005 + 0.01 * 6.750217914581299
Epoch 910, val loss: 0.9506447315216064
Epoch 920, training loss: 0.07084841281175613 = 0.0034055139403790236 + 0.01 * 6.744290351867676
Epoch 920, val loss: 0.9531329274177551
Epoch 930, training loss: 0.07063218951225281 = 0.003321818308904767 + 0.01 * 6.731037139892578
Epoch 930, val loss: 0.9555522799491882
Epoch 940, training loss: 0.07091444730758667 = 0.003241846803575754 + 0.01 * 6.767260551452637
Epoch 940, val loss: 0.9580492973327637
Epoch 950, training loss: 0.07060641050338745 = 0.0031656234059482813 + 0.01 * 6.744078636169434
Epoch 950, val loss: 0.9602951407432556
Epoch 960, training loss: 0.07043831050395966 = 0.0030926750041544437 + 0.01 * 6.734563827514648
Epoch 960, val loss: 0.9625669121742249
Epoch 970, training loss: 0.07041038572788239 = 0.0030227568931877613 + 0.01 * 6.738762378692627
Epoch 970, val loss: 0.9648114442825317
Epoch 980, training loss: 0.07023825496435165 = 0.002955799922347069 + 0.01 * 6.728245735168457
Epoch 980, val loss: 0.9670840501785278
Epoch 990, training loss: 0.070164754986763 = 0.002891583601012826 + 0.01 * 6.7273173332214355
Epoch 990, val loss: 0.9691699147224426
Epoch 1000, training loss: 0.0699768140912056 = 0.0028299661353230476 + 0.01 * 6.714684963226318
Epoch 1000, val loss: 0.9713326692581177
Epoch 1010, training loss: 0.07013501971960068 = 0.0027708110865205526 + 0.01 * 6.73642110824585
Epoch 1010, val loss: 0.9734677076339722
Epoch 1020, training loss: 0.06987596303224564 = 0.002714045811444521 + 0.01 * 6.716192245483398
Epoch 1020, val loss: 0.97547847032547
Epoch 1030, training loss: 0.07004526257514954 = 0.0026595653034746647 + 0.01 * 6.738569736480713
Epoch 1030, val loss: 0.9775015115737915
Epoch 1040, training loss: 0.06980156898498535 = 0.0026071269530802965 + 0.01 * 6.7194437980651855
Epoch 1040, val loss: 0.9794570207595825
Epoch 1050, training loss: 0.06969115883111954 = 0.002556676510721445 + 0.01 * 6.713448524475098
Epoch 1050, val loss: 0.9814072251319885
Epoch 1060, training loss: 0.06987563520669937 = 0.0025081164203584194 + 0.01 * 6.736752033233643
Epoch 1060, val loss: 0.9833081364631653
Epoch 1070, training loss: 0.06960997730493546 = 0.002461368218064308 + 0.01 * 6.714860916137695
Epoch 1070, val loss: 0.9851370453834534
Epoch 1080, training loss: 0.06948806345462799 = 0.002416300820186734 + 0.01 * 6.707176685333252
Epoch 1080, val loss: 0.9869384765625
Epoch 1090, training loss: 0.06965889781713486 = 0.0023728683590888977 + 0.01 * 6.728603363037109
Epoch 1090, val loss: 0.9888036847114563
Epoch 1100, training loss: 0.06944379955530167 = 0.0023310312535613775 + 0.01 * 6.711276531219482
Epoch 1100, val loss: 0.9905686974525452
Epoch 1110, training loss: 0.06924659013748169 = 0.002290667500346899 + 0.01 * 6.695592403411865
Epoch 1110, val loss: 0.9922611713409424
Epoch 1120, training loss: 0.06942339241504669 = 0.0022517035249620676 + 0.01 * 6.717168807983398
Epoch 1120, val loss: 0.9940373301506042
Epoch 1130, training loss: 0.06921108812093735 = 0.0022141439840197563 + 0.01 * 6.699694633483887
Epoch 1130, val loss: 0.9956349730491638
Epoch 1140, training loss: 0.06909197568893433 = 0.0021778394002467394 + 0.01 * 6.691413402557373
Epoch 1140, val loss: 0.9972893595695496
Epoch 1150, training loss: 0.06921142339706421 = 0.0021426586899906397 + 0.01 * 6.706876754760742
Epoch 1150, val loss: 0.9989228248596191
Epoch 1160, training loss: 0.06901638209819794 = 0.0021086670458316803 + 0.01 * 6.69077205657959
Epoch 1160, val loss: 1.0004922151565552
Epoch 1170, training loss: 0.06915847212076187 = 0.002075813477858901 + 0.01 * 6.708265781402588
Epoch 1170, val loss: 1.0021296739578247
Epoch 1180, training loss: 0.06888610869646072 = 0.002044085878878832 + 0.01 * 6.684202194213867
Epoch 1180, val loss: 1.003557562828064
Epoch 1190, training loss: 0.06885191053152084 = 0.0020133894868195057 + 0.01 * 6.683851718902588
Epoch 1190, val loss: 1.0050699710845947
Epoch 1200, training loss: 0.06884631514549255 = 0.001983586698770523 + 0.01 * 6.686273097991943
Epoch 1200, val loss: 1.006562352180481
Epoch 1210, training loss: 0.06879613548517227 = 0.001954692183062434 + 0.01 * 6.684144496917725
Epoch 1210, val loss: 1.008002519607544
Epoch 1220, training loss: 0.06872706115245819 = 0.0019266799790784717 + 0.01 * 6.680037975311279
Epoch 1220, val loss: 1.0094414949417114
Epoch 1230, training loss: 0.06882138550281525 = 0.00189947837498039 + 0.01 * 6.692190647125244
Epoch 1230, val loss: 1.010817289352417
Epoch 1240, training loss: 0.06863342970609665 = 0.0018731503514572978 + 0.01 * 6.676027774810791
Epoch 1240, val loss: 1.0122345685958862
Epoch 1250, training loss: 0.06852961331605911 = 0.0018476209370419383 + 0.01 * 6.66819953918457
Epoch 1250, val loss: 1.0136454105377197
Epoch 1260, training loss: 0.06881191581487656 = 0.0018229682464152575 + 0.01 * 6.69889497756958
Epoch 1260, val loss: 1.014907956123352
Epoch 1270, training loss: 0.06853005290031433 = 0.001799004734493792 + 0.01 * 6.673105239868164
Epoch 1270, val loss: 1.0161805152893066
Epoch 1280, training loss: 0.06859739124774933 = 0.0017756640445441008 + 0.01 * 6.682172775268555
Epoch 1280, val loss: 1.0175062417984009
Epoch 1290, training loss: 0.06847082823514938 = 0.0017529692267999053 + 0.01 * 6.671785354614258
Epoch 1290, val loss: 1.018785834312439
Epoch 1300, training loss: 0.06861352175474167 = 0.0017309606773778796 + 0.01 * 6.68825626373291
Epoch 1300, val loss: 1.0201095342636108
Epoch 1310, training loss: 0.06835586577653885 = 0.0017095580697059631 + 0.01 * 6.664630889892578
Epoch 1310, val loss: 1.0212743282318115
Epoch 1320, training loss: 0.06879392266273499 = 0.0016887878300622106 + 0.01 * 6.710514068603516
Epoch 1320, val loss: 1.0224528312683105
Epoch 1330, training loss: 0.06829903274774551 = 0.0016685795271769166 + 0.01 * 6.663045406341553
Epoch 1330, val loss: 1.0237404108047485
Epoch 1340, training loss: 0.0683455839753151 = 0.00164897833019495 + 0.01 * 6.669660568237305
Epoch 1340, val loss: 1.0247966051101685
Epoch 1350, training loss: 0.06821735948324203 = 0.001629867241717875 + 0.01 * 6.658749103546143
Epoch 1350, val loss: 1.0259760618209839
Epoch 1360, training loss: 0.06822258979082108 = 0.0016112502198666334 + 0.01 * 6.661134243011475
Epoch 1360, val loss: 1.0271334648132324
Epoch 1370, training loss: 0.06818554550409317 = 0.001593099907040596 + 0.01 * 6.659244537353516
Epoch 1370, val loss: 1.0282593965530396
Epoch 1380, training loss: 0.06808684021234512 = 0.001575416186824441 + 0.01 * 6.651142120361328
Epoch 1380, val loss: 1.0293831825256348
Epoch 1390, training loss: 0.06823140382766724 = 0.0015582283958792686 + 0.01 * 6.667317867279053
Epoch 1390, val loss: 1.0304820537567139
Epoch 1400, training loss: 0.06829375773668289 = 0.0015414515510201454 + 0.01 * 6.675230979919434
Epoch 1400, val loss: 1.0315450429916382
Epoch 1410, training loss: 0.06801005452871323 = 0.001525121508166194 + 0.01 * 6.648493766784668
Epoch 1410, val loss: 1.0325936079025269
Epoch 1420, training loss: 0.06832714378833771 = 0.0015091662062332034 + 0.01 * 6.681797504425049
Epoch 1420, val loss: 1.0336027145385742
Epoch 1430, training loss: 0.06797818839550018 = 0.0014936568913981318 + 0.01 * 6.648453235626221
Epoch 1430, val loss: 1.0346732139587402
Epoch 1440, training loss: 0.06800553947687149 = 0.0014785070670768619 + 0.01 * 6.652703285217285
Epoch 1440, val loss: 1.0356782674789429
Epoch 1450, training loss: 0.06827075034379959 = 0.0014637408312410116 + 0.01 * 6.68070125579834
Epoch 1450, val loss: 1.0366567373275757
Epoch 1460, training loss: 0.06795667111873627 = 0.0014493617927655578 + 0.01 * 6.650731086730957
Epoch 1460, val loss: 1.0376731157302856
Epoch 1470, training loss: 0.06809030473232269 = 0.0014352879952639341 + 0.01 * 6.665502071380615
Epoch 1470, val loss: 1.0385276079177856
Epoch 1480, training loss: 0.06783264875411987 = 0.001421601977199316 + 0.01 * 6.6411051750183105
Epoch 1480, val loss: 1.0396020412445068
Epoch 1490, training loss: 0.0681723952293396 = 0.0014082911657169461 + 0.01 * 6.67641019821167
Epoch 1490, val loss: 1.0404601097106934
Epoch 1500, training loss: 0.067731574177742 = 0.00139525905251503 + 0.01 * 6.633631706237793
Epoch 1500, val loss: 1.0413854122161865
Epoch 1510, training loss: 0.06782118231058121 = 0.0013825218193233013 + 0.01 * 6.643866062164307
Epoch 1510, val loss: 1.0423142910003662
Epoch 1520, training loss: 0.06785213202238083 = 0.0013700949493795633 + 0.01 * 6.6482038497924805
Epoch 1520, val loss: 1.0430867671966553
Epoch 1530, training loss: 0.067729651927948 = 0.0013579468941316009 + 0.01 * 6.637170314788818
Epoch 1530, val loss: 1.044085144996643
Epoch 1540, training loss: 0.06779450923204422 = 0.001346078235656023 + 0.01 * 6.644842624664307
Epoch 1540, val loss: 1.0448968410491943
Epoch 1550, training loss: 0.06793300807476044 = 0.0013344214530661702 + 0.01 * 6.6598591804504395
Epoch 1550, val loss: 1.0457656383514404
Epoch 1560, training loss: 0.06769508123397827 = 0.0013230660697445273 + 0.01 * 6.637201309204102
Epoch 1560, val loss: 1.046590805053711
Epoch 1570, training loss: 0.06769450753927231 = 0.001311983447521925 + 0.01 * 6.6382527351379395
Epoch 1570, val loss: 1.0474668741226196
Epoch 1580, training loss: 0.06765753030776978 = 0.0013011457631364465 + 0.01 * 6.63563871383667
Epoch 1580, val loss: 1.048238754272461
Epoch 1590, training loss: 0.06758344173431396 = 0.001290518674068153 + 0.01 * 6.6292924880981445
Epoch 1590, val loss: 1.0490715503692627
Epoch 1600, training loss: 0.06795810908079147 = 0.0012801522389054298 + 0.01 * 6.6677961349487305
Epoch 1600, val loss: 1.0499016046524048
Epoch 1610, training loss: 0.06747763603925705 = 0.0012700128136202693 + 0.01 * 6.620762348175049
Epoch 1610, val loss: 1.0506089925765991
Epoch 1620, training loss: 0.06746301054954529 = 0.001260116696357727 + 0.01 * 6.620289325714111
Epoch 1620, val loss: 1.0513553619384766
Epoch 1630, training loss: 0.06746861338615417 = 0.0012503634206950665 + 0.01 * 6.621824741363525
Epoch 1630, val loss: 1.0520602464675903
Epoch 1640, training loss: 0.06774874776601791 = 0.0012408321490511298 + 0.01 * 6.650791645050049
Epoch 1640, val loss: 1.0529718399047852
Epoch 1650, training loss: 0.06753119081258774 = 0.0012314995983615518 + 0.01 * 6.629969120025635
Epoch 1650, val loss: 1.0536445379257202
Epoch 1660, training loss: 0.0673053190112114 = 0.0012223984813317657 + 0.01 * 6.608292102813721
Epoch 1660, val loss: 1.054387092590332
Epoch 1670, training loss: 0.0674533098936081 = 0.0012135128490626812 + 0.01 * 6.6239800453186035
Epoch 1670, val loss: 1.0550216436386108
Epoch 1680, training loss: 0.06736183911561966 = 0.0012047798372805119 + 0.01 * 6.615705966949463
Epoch 1680, val loss: 1.055708408355713
Epoch 1690, training loss: 0.06736331433057785 = 0.0011961942072957754 + 0.01 * 6.61671257019043
Epoch 1690, val loss: 1.056442141532898
Epoch 1700, training loss: 0.06715278327465057 = 0.0011878040386363864 + 0.01 * 6.596498012542725
Epoch 1700, val loss: 1.0570842027664185
Epoch 1710, training loss: 0.06716421246528625 = 0.0011795873288065195 + 0.01 * 6.5984625816345215
Epoch 1710, val loss: 1.0578057765960693
Epoch 1720, training loss: 0.06726612150669098 = 0.0011715227738022804 + 0.01 * 6.60945987701416
Epoch 1720, val loss: 1.0583609342575073
Epoch 1730, training loss: 0.06730601191520691 = 0.0011636224808171391 + 0.01 * 6.614238739013672
Epoch 1730, val loss: 1.059140682220459
Epoch 1740, training loss: 0.06733538210391998 = 0.0011558656115084887 + 0.01 * 6.6179518699646
Epoch 1740, val loss: 1.0597267150878906
Epoch 1750, training loss: 0.06721372902393341 = 0.0011482988484203815 + 0.01 * 6.606543064117432
Epoch 1750, val loss: 1.0603454113006592
Epoch 1760, training loss: 0.06732381880283356 = 0.0011408714344725013 + 0.01 * 6.618295192718506
Epoch 1760, val loss: 1.0610466003417969
Epoch 1770, training loss: 0.0672433003783226 = 0.001133571146056056 + 0.01 * 6.610973358154297
Epoch 1770, val loss: 1.0615615844726562
Epoch 1780, training loss: 0.06722234189510345 = 0.0011264132335782051 + 0.01 * 6.609592914581299
Epoch 1780, val loss: 1.0622210502624512
Epoch 1790, training loss: 0.06699560582637787 = 0.0011193609097972512 + 0.01 * 6.587625026702881
Epoch 1790, val loss: 1.0627816915512085
Epoch 1800, training loss: 0.06722822785377502 = 0.0011124275624752045 + 0.01 * 6.6115803718566895
Epoch 1800, val loss: 1.0634294748306274
Epoch 1810, training loss: 0.06718447804450989 = 0.001105644041672349 + 0.01 * 6.607883453369141
Epoch 1810, val loss: 1.0640003681182861
Epoch 1820, training loss: 0.06700687110424042 = 0.0010990028968080878 + 0.01 * 6.590786457061768
Epoch 1820, val loss: 1.0645347833633423
Epoch 1830, training loss: 0.06706254929304123 = 0.0010924533708021045 + 0.01 * 6.597009658813477
Epoch 1830, val loss: 1.0650449991226196
Epoch 1840, training loss: 0.06701896339654922 = 0.001086024334654212 + 0.01 * 6.593294143676758
Epoch 1840, val loss: 1.0656651258468628
Epoch 1850, training loss: 0.06690141558647156 = 0.001079680398106575 + 0.01 * 6.582173824310303
Epoch 1850, val loss: 1.0661895275115967
Epoch 1860, training loss: 0.06696098297834396 = 0.0010734517127275467 + 0.01 * 6.5887532234191895
Epoch 1860, val loss: 1.0666358470916748
Epoch 1870, training loss: 0.0670657828450203 = 0.0010673522483557463 + 0.01 * 6.5998430252075195
Epoch 1870, val loss: 1.0672355890274048
Epoch 1880, training loss: 0.06674975156784058 = 0.0010613068006932735 + 0.01 * 6.568844795227051
Epoch 1880, val loss: 1.0677701234817505
Epoch 1890, training loss: 0.06722231209278107 = 0.001055386383086443 + 0.01 * 6.616692543029785
Epoch 1890, val loss: 1.068287968635559
Epoch 1900, training loss: 0.06682438403367996 = 0.0010495769092813134 + 0.01 * 6.577480792999268
Epoch 1900, val loss: 1.0687224864959717
Epoch 1910, training loss: 0.06680330634117126 = 0.0010438590543344617 + 0.01 * 6.5759453773498535
Epoch 1910, val loss: 1.0692965984344482
Epoch 1920, training loss: 0.06669872999191284 = 0.0010381989413872361 + 0.01 * 6.56605339050293
Epoch 1920, val loss: 1.0697486400604248
Epoch 1930, training loss: 0.06668310612440109 = 0.0010326342890039086 + 0.01 * 6.565047264099121
Epoch 1930, val loss: 1.0703383684158325
Epoch 1940, training loss: 0.0666457861661911 = 0.0010271526407450438 + 0.01 * 6.561863899230957
Epoch 1940, val loss: 1.070731282234192
Epoch 1950, training loss: 0.06668256968259811 = 0.0010217605158686638 + 0.01 * 6.5660810470581055
Epoch 1950, val loss: 1.0713165998458862
Epoch 1960, training loss: 0.06666889786720276 = 0.0010164696723222733 + 0.01 * 6.565243244171143
Epoch 1960, val loss: 1.0716755390167236
Epoch 1970, training loss: 0.06654169410467148 = 0.0010112067684531212 + 0.01 * 6.553048610687256
Epoch 1970, val loss: 1.0722604990005493
Epoch 1980, training loss: 0.06686609238386154 = 0.0010060566710308194 + 0.01 * 6.58600378036499
Epoch 1980, val loss: 1.0726197957992554
Epoch 1990, training loss: 0.0666486918926239 = 0.0010009754914790392 + 0.01 * 6.56477165222168
Epoch 1990, val loss: 1.0731157064437866
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.030891180038452 = 1.9449225664138794 + 0.01 * 8.596858978271484
Epoch 0, val loss: 1.9425593614578247
Epoch 10, training loss: 2.021345615386963 = 1.9353774785995483 + 0.01 * 8.596817016601562
Epoch 10, val loss: 1.9326391220092773
Epoch 20, training loss: 2.0096609592437744 = 1.9236944913864136 + 0.01 * 8.596640586853027
Epoch 20, val loss: 1.9205726385116577
Epoch 30, training loss: 1.993238925933838 = 1.9072784185409546 + 0.01 * 8.596055030822754
Epoch 30, val loss: 1.9037129878997803
Epoch 40, training loss: 1.9689422845840454 = 1.8830150365829468 + 0.01 * 8.592729568481445
Epoch 40, val loss: 1.879105567932129
Epoch 50, training loss: 1.9344171285629272 = 1.8487521409988403 + 0.01 * 8.566496849060059
Epoch 50, val loss: 1.845788836479187
Epoch 60, training loss: 1.893659234046936 = 1.8092997074127197 + 0.01 * 8.435952186584473
Epoch 60, val loss: 1.8112961053848267
Epoch 70, training loss: 1.857244610786438 = 1.7755465507507324 + 0.01 * 8.16980266571045
Epoch 70, val loss: 1.785461664199829
Epoch 80, training loss: 1.8181324005126953 = 1.7376021146774292 + 0.01 * 8.053033828735352
Epoch 80, val loss: 1.7529361248016357
Epoch 90, training loss: 1.7623370885849 = 1.6842997074127197 + 0.01 * 7.803739547729492
Epoch 90, val loss: 1.7049314975738525
Epoch 100, training loss: 1.6861286163330078 = 1.6105964183807373 + 0.01 * 7.553223133087158
Epoch 100, val loss: 1.6401206254959106
Epoch 110, training loss: 1.5909171104431152 = 1.5170001983642578 + 0.01 * 7.391696453094482
Epoch 110, val loss: 1.5615841150283813
Epoch 120, training loss: 1.487059473991394 = 1.4140340089797974 + 0.01 * 7.30255126953125
Epoch 120, val loss: 1.4759345054626465
Epoch 130, training loss: 1.382401704788208 = 1.3099993467330933 + 0.01 * 7.240231513977051
Epoch 130, val loss: 1.3897219896316528
Epoch 140, training loss: 1.280116081237793 = 1.2081266641616821 + 0.01 * 7.19893741607666
Epoch 140, val loss: 1.3062291145324707
Epoch 150, training loss: 1.18185293674469 = 1.1100966930389404 + 0.01 * 7.175622463226318
Epoch 150, val loss: 1.225915551185608
Epoch 160, training loss: 1.089576005935669 = 1.0179425477981567 + 0.01 * 7.163344860076904
Epoch 160, val loss: 1.151727318763733
Epoch 170, training loss: 1.0037438869476318 = 0.9322604537010193 + 0.01 * 7.1483378410339355
Epoch 170, val loss: 1.0839307308197021
Epoch 180, training loss: 0.9232401847839355 = 0.8519139289855957 + 0.01 * 7.132627487182617
Epoch 180, val loss: 1.021080493927002
Epoch 190, training loss: 0.8472927212715149 = 0.7761187553405762 + 0.01 * 7.117397785186768
Epoch 190, val loss: 0.9620447158813477
Epoch 200, training loss: 0.7762936949729919 = 0.7052848935127258 + 0.01 * 7.1008806228637695
Epoch 200, val loss: 0.9075385928153992
Epoch 210, training loss: 0.7110733389854431 = 0.6401838660240173 + 0.01 * 7.088949203491211
Epoch 210, val loss: 0.8592485785484314
Epoch 220, training loss: 0.6517646312713623 = 0.580998420715332 + 0.01 * 7.076622486114502
Epoch 220, val loss: 0.8181370496749878
Epoch 230, training loss: 0.5979268550872803 = 0.5272340774536133 + 0.01 * 7.069278240203857
Epoch 230, val loss: 0.7841688394546509
Epoch 240, training loss: 0.5485902428627014 = 0.4779956340789795 + 0.01 * 7.059462070465088
Epoch 240, val loss: 0.7563338279724121
Epoch 250, training loss: 0.5026859045028687 = 0.43214353919029236 + 0.01 * 7.054237365722656
Epoch 250, val loss: 0.7331688404083252
Epoch 260, training loss: 0.4591609835624695 = 0.38866740465164185 + 0.01 * 7.04935884475708
Epoch 260, val loss: 0.7132943272590637
Epoch 270, training loss: 0.417436420917511 = 0.3469988703727722 + 0.01 * 7.043755531311035
Epoch 270, val loss: 0.6958264112472534
Epoch 280, training loss: 0.3774138391017914 = 0.30703067779541016 + 0.01 * 7.038315296173096
Epoch 280, val loss: 0.6804465651512146
Epoch 290, training loss: 0.33928143978118896 = 0.2689436376094818 + 0.01 * 7.033778667449951
Epoch 290, val loss: 0.6672841310501099
Epoch 300, training loss: 0.3033961057662964 = 0.23311936855316162 + 0.01 * 7.027672290802002
Epoch 300, val loss: 0.6565483212471008
Epoch 310, training loss: 0.2701941132545471 = 0.20002421736717224 + 0.01 * 7.016988754272461
Epoch 310, val loss: 0.6485769152641296
Epoch 320, training loss: 0.24039606750011444 = 0.1702064424753189 + 0.01 * 7.018962860107422
Epoch 320, val loss: 0.6436634063720703
Epoch 330, training loss: 0.2141602486371994 = 0.1441144049167633 + 0.01 * 7.004584312438965
Epoch 330, val loss: 0.6419002413749695
Epoch 340, training loss: 0.19193299114704132 = 0.12194056808948517 + 0.01 * 6.999242782592773
Epoch 340, val loss: 0.643216609954834
Epoch 350, training loss: 0.17349623143672943 = 0.10355624556541443 + 0.01 * 6.993999004364014
Epoch 350, val loss: 0.647152304649353
Epoch 360, training loss: 0.1583796590566635 = 0.0885055735707283 + 0.01 * 6.987408638000488
Epoch 360, val loss: 0.6532214879989624
Epoch 370, training loss: 0.1460590660572052 = 0.07623837888240814 + 0.01 * 6.982069969177246
Epoch 370, val loss: 0.6609537601470947
Epoch 380, training loss: 0.1359405517578125 = 0.0662054494023323 + 0.01 * 6.973510265350342
Epoch 380, val loss: 0.6698347926139832
Epoch 390, training loss: 0.12762048840522766 = 0.05793657898902893 + 0.01 * 6.9683918952941895
Epoch 390, val loss: 0.6794607639312744
Epoch 400, training loss: 0.12065358459949493 = 0.05106198415160179 + 0.01 * 6.959160327911377
Epoch 400, val loss: 0.689516544342041
Epoch 410, training loss: 0.11518511176109314 = 0.045297130942344666 + 0.01 * 6.988798141479492
Epoch 410, val loss: 0.6997758150100708
Epoch 420, training loss: 0.10996776819229126 = 0.04043031483888626 + 0.01 * 6.953745365142822
Epoch 420, val loss: 0.7100388407707214
Epoch 430, training loss: 0.10574039071798325 = 0.03627943992614746 + 0.01 * 6.9460954666137695
Epoch 430, val loss: 0.720258355140686
Epoch 440, training loss: 0.10212115943431854 = 0.03270694240927696 + 0.01 * 6.941421985626221
Epoch 440, val loss: 0.7303057909011841
Epoch 450, training loss: 0.09899391978979111 = 0.029608815908432007 + 0.01 * 6.938510894775391
Epoch 450, val loss: 0.7402212619781494
Epoch 460, training loss: 0.0964176207780838 = 0.02690761350095272 + 0.01 * 6.951000690460205
Epoch 460, val loss: 0.7499812245368958
Epoch 470, training loss: 0.09385944902896881 = 0.024548135697841644 + 0.01 * 6.931131839752197
Epoch 470, val loss: 0.7594618201255798
Epoch 480, training loss: 0.09173882752656937 = 0.022477196529507637 + 0.01 * 6.926163196563721
Epoch 480, val loss: 0.7687406539916992
Epoch 490, training loss: 0.08991923928260803 = 0.02065105363726616 + 0.01 * 6.926818370819092
Epoch 490, val loss: 0.7777732610702515
Epoch 500, training loss: 0.08824273943901062 = 0.019035374745726585 + 0.01 * 6.920736789703369
Epoch 500, val loss: 0.7866026163101196
Epoch 510, training loss: 0.08686299622058868 = 0.017598945647478104 + 0.01 * 6.9264044761657715
Epoch 510, val loss: 0.7952679395675659
Epoch 520, training loss: 0.08548533916473389 = 0.016315996646881104 + 0.01 * 6.916934490203857
Epoch 520, val loss: 0.8037384152412415
Epoch 530, training loss: 0.08424639701843262 = 0.015159781090915203 + 0.01 * 6.908661842346191
Epoch 530, val loss: 0.8122456073760986
Epoch 540, training loss: 0.0832533910870552 = 0.014113311655819416 + 0.01 * 6.914007663726807
Epoch 540, val loss: 0.8206838369369507
Epoch 550, training loss: 0.08222772181034088 = 0.013166707009077072 + 0.01 * 6.906101226806641
Epoch 550, val loss: 0.8289865851402283
Epoch 560, training loss: 0.08147623389959335 = 0.012306662276387215 + 0.01 * 6.916957378387451
Epoch 560, val loss: 0.8372429013252258
Epoch 570, training loss: 0.08056752383708954 = 0.011525829322636127 + 0.01 * 6.90416955947876
Epoch 570, val loss: 0.8453599214553833
Epoch 580, training loss: 0.07979050278663635 = 0.010814476758241653 + 0.01 * 6.897602081298828
Epoch 580, val loss: 0.8532792329788208
Epoch 590, training loss: 0.07906799763441086 = 0.010165284387767315 + 0.01 * 6.8902716636657715
Epoch 590, val loss: 0.8610982894897461
Epoch 600, training loss: 0.07852619886398315 = 0.009571598842740059 + 0.01 * 6.89546012878418
Epoch 600, val loss: 0.8687782287597656
Epoch 610, training loss: 0.07788914442062378 = 0.009028377011418343 + 0.01 * 6.886076927185059
Epoch 610, val loss: 0.8762263059616089
Epoch 620, training loss: 0.07734550535678864 = 0.008531134575605392 + 0.01 * 6.8814377784729
Epoch 620, val loss: 0.8835805654525757
Epoch 630, training loss: 0.07686815410852432 = 0.008074670098721981 + 0.01 * 6.879348278045654
Epoch 630, val loss: 0.8906920552253723
Epoch 640, training loss: 0.07638975232839584 = 0.007654508110135794 + 0.01 * 6.873524188995361
Epoch 640, val loss: 0.8976820707321167
Epoch 650, training loss: 0.07600749284029007 = 0.007267876993864775 + 0.01 * 6.873961448669434
Epoch 650, val loss: 0.9044689536094666
Epoch 660, training loss: 0.07561276853084564 = 0.006911108270287514 + 0.01 * 6.870166301727295
Epoch 660, val loss: 0.9110348224639893
Epoch 670, training loss: 0.07532815635204315 = 0.006580535788089037 + 0.01 * 6.874762058258057
Epoch 670, val loss: 0.917508602142334
Epoch 680, training loss: 0.07487978786230087 = 0.006274079438298941 + 0.01 * 6.860570907592773
Epoch 680, val loss: 0.9237254858016968
Epoch 690, training loss: 0.07465185970067978 = 0.00598918879404664 + 0.01 * 6.866267681121826
Epoch 690, val loss: 0.9299250245094299
Epoch 700, training loss: 0.07431600242853165 = 0.00572605337947607 + 0.01 * 6.858994960784912
Epoch 700, val loss: 0.9358277320861816
Epoch 710, training loss: 0.07398486882448196 = 0.005480740685015917 + 0.01 * 6.850412845611572
Epoch 710, val loss: 0.9416182637214661
Epoch 720, training loss: 0.07375721633434296 = 0.005251734051853418 + 0.01 * 6.850548267364502
Epoch 720, val loss: 0.9472570419311523
Epoch 730, training loss: 0.07359760254621506 = 0.0050374469719827175 + 0.01 * 6.856015205383301
Epoch 730, val loss: 0.9528778195381165
Epoch 740, training loss: 0.07325883209705353 = 0.004837260581552982 + 0.01 * 6.842157363891602
Epoch 740, val loss: 0.9581557512283325
Epoch 750, training loss: 0.07302805036306381 = 0.0046488563530147076 + 0.01 * 6.837919235229492
Epoch 750, val loss: 0.9634137153625488
Epoch 760, training loss: 0.07296957075595856 = 0.004471042193472385 + 0.01 * 6.849853038787842
Epoch 760, val loss: 0.9686036109924316
Epoch 770, training loss: 0.0727095901966095 = 0.004303891211748123 + 0.01 * 6.840570449829102
Epoch 770, val loss: 0.9734320640563965
Epoch 780, training loss: 0.07248223572969437 = 0.004145234823226929 + 0.01 * 6.833700180053711
Epoch 780, val loss: 0.9783366918563843
Epoch 790, training loss: 0.07254821062088013 = 0.003994372207671404 + 0.01 * 6.855383396148682
Epoch 790, val loss: 0.9831594824790955
Epoch 800, training loss: 0.07220236212015152 = 0.003851486835628748 + 0.01 * 6.835087776184082
Epoch 800, val loss: 0.9879039525985718
Epoch 810, training loss: 0.07208570092916489 = 0.0037156217731535435 + 0.01 * 6.837008476257324
Epoch 810, val loss: 0.9925495386123657
Epoch 820, training loss: 0.071864053606987 = 0.0035870373249053955 + 0.01 * 6.827701568603516
Epoch 820, val loss: 0.9971615076065063
Epoch 830, training loss: 0.07177196443080902 = 0.003464854322373867 + 0.01 * 6.830711364746094
Epoch 830, val loss: 1.0016151666641235
Epoch 840, training loss: 0.07148870080709457 = 0.0033494187518954277 + 0.01 * 6.813928604125977
Epoch 840, val loss: 1.0061991214752197
Epoch 850, training loss: 0.07140228897333145 = 0.0032401601783931255 + 0.01 * 6.8162126541137695
Epoch 850, val loss: 1.010379672050476
Epoch 860, training loss: 0.07137036323547363 = 0.0031363768503069878 + 0.01 * 6.823399066925049
Epoch 860, val loss: 1.0147072076797485
Epoch 870, training loss: 0.07115073502063751 = 0.0030377365183085203 + 0.01 * 6.811300277709961
Epoch 870, val loss: 1.0189199447631836
Epoch 880, training loss: 0.0709865391254425 = 0.002944071078673005 + 0.01 * 6.80424690246582
Epoch 880, val loss: 1.0229825973510742
Epoch 890, training loss: 0.07095550000667572 = 0.0028553882148116827 + 0.01 * 6.810011386871338
Epoch 890, val loss: 1.0270668268203735
Epoch 900, training loss: 0.07084208726882935 = 0.002771303290501237 + 0.01 * 6.807078838348389
Epoch 900, val loss: 1.030976414680481
Epoch 910, training loss: 0.07060588896274567 = 0.0026914074551314116 + 0.01 * 6.791447639465332
Epoch 910, val loss: 1.0348142385482788
Epoch 920, training loss: 0.07067876309156418 = 0.002615519566461444 + 0.01 * 6.806324481964111
Epoch 920, val loss: 1.0386661291122437
Epoch 930, training loss: 0.07048078626394272 = 0.0025439613964408636 + 0.01 * 6.793682098388672
Epoch 930, val loss: 1.0421258211135864
Epoch 940, training loss: 0.07057802379131317 = 0.0024757939390838146 + 0.01 * 6.81022310256958
Epoch 940, val loss: 1.0458154678344727
Epoch 950, training loss: 0.070321224629879 = 0.002411343390122056 + 0.01 * 6.790987968444824
Epoch 950, val loss: 1.0491914749145508
Epoch 960, training loss: 0.0702076107263565 = 0.0023497187066823244 + 0.01 * 6.785789489746094
Epoch 960, val loss: 1.0526801347732544
Epoch 970, training loss: 0.070189468562603 = 0.0022909692488610744 + 0.01 * 6.789849758148193
Epoch 970, val loss: 1.0559388399124146
Epoch 980, training loss: 0.07010814547538757 = 0.0022350752260535955 + 0.01 * 6.787306785583496
Epoch 980, val loss: 1.0591813325881958
Epoch 990, training loss: 0.06994126737117767 = 0.0021819386165589094 + 0.01 * 6.775932788848877
Epoch 990, val loss: 1.0622766017913818
Epoch 1000, training loss: 0.06986343115568161 = 0.002131085144355893 + 0.01 * 6.773235321044922
Epoch 1000, val loss: 1.0652155876159668
Epoch 1010, training loss: 0.06988838315010071 = 0.002082876628264785 + 0.01 * 6.780550479888916
Epoch 1010, val loss: 1.0683128833770752
Epoch 1020, training loss: 0.06986575573682785 = 0.0020369284320622683 + 0.01 * 6.7828826904296875
Epoch 1020, val loss: 1.0710645914077759
Epoch 1030, training loss: 0.06960853189229965 = 0.001992890378460288 + 0.01 * 6.761564254760742
Epoch 1030, val loss: 1.0739200115203857
Epoch 1040, training loss: 0.06970414519309998 = 0.0019505979726091027 + 0.01 * 6.775354385375977
Epoch 1040, val loss: 1.0765469074249268
Epoch 1050, training loss: 0.06955768167972565 = 0.0019101607613265514 + 0.01 * 6.764752388000488
Epoch 1050, val loss: 1.0792304277420044
Epoch 1060, training loss: 0.06955500692129135 = 0.0018715329933911562 + 0.01 * 6.76834774017334
Epoch 1060, val loss: 1.081940770149231
Epoch 1070, training loss: 0.06952168047428131 = 0.0018346485449001193 + 0.01 * 6.768703937530518
Epoch 1070, val loss: 1.0841609239578247
Epoch 1080, training loss: 0.06934156268835068 = 0.001799313467927277 + 0.01 * 6.75422477722168
Epoch 1080, val loss: 1.0867812633514404
Epoch 1090, training loss: 0.06927385181188583 = 0.0017652950482442975 + 0.01 * 6.750855445861816
Epoch 1090, val loss: 1.0889774560928345
Epoch 1100, training loss: 0.06933899223804474 = 0.001732592354528606 + 0.01 * 6.760639667510986
Epoch 1100, val loss: 1.0913504362106323
Epoch 1110, training loss: 0.06915388256311417 = 0.001701368484646082 + 0.01 * 6.745251178741455
Epoch 1110, val loss: 1.0936017036437988
Epoch 1120, training loss: 0.06906519830226898 = 0.0016714417142793536 + 0.01 * 6.739376068115234
Epoch 1120, val loss: 1.0957801342010498
Epoch 1130, training loss: 0.06907327473163605 = 0.0016425653593614697 + 0.01 * 6.743071556091309
Epoch 1130, val loss: 1.097834825515747
Epoch 1140, training loss: 0.0691148117184639 = 0.001614908454939723 + 0.01 * 6.749990463256836
Epoch 1140, val loss: 1.10007643699646
Epoch 1150, training loss: 0.06917814165353775 = 0.0015883893938735127 + 0.01 * 6.758975028991699
Epoch 1150, val loss: 1.1019785404205322
Epoch 1160, training loss: 0.06881450116634369 = 0.0015629315748810768 + 0.01 * 6.725157260894775
Epoch 1160, val loss: 1.1038974523544312
Epoch 1170, training loss: 0.06885512173175812 = 0.0015382921556010842 + 0.01 * 6.731683254241943
Epoch 1170, val loss: 1.1058677434921265
Epoch 1180, training loss: 0.06882672011852264 = 0.0015145824290812016 + 0.01 * 6.731213569641113
Epoch 1180, val loss: 1.1076185703277588
Epoch 1190, training loss: 0.06876078248023987 = 0.0014918226515874267 + 0.01 * 6.726895809173584
Epoch 1190, val loss: 1.1095292568206787
Epoch 1200, training loss: 0.06867093592882156 = 0.0014699118910357356 + 0.01 * 6.720102787017822
Epoch 1200, val loss: 1.1110466718673706
Epoch 1210, training loss: 0.06868214905261993 = 0.0014487388543784618 + 0.01 * 6.723341464996338
Epoch 1210, val loss: 1.112995982170105
Epoch 1220, training loss: 0.06915206462144852 = 0.001428242539986968 + 0.01 * 6.772382736206055
Epoch 1220, val loss: 1.1144543886184692
Epoch 1230, training loss: 0.06857143342494965 = 0.001408595941029489 + 0.01 * 6.716283321380615
Epoch 1230, val loss: 1.116180658340454
Epoch 1240, training loss: 0.06889431178569794 = 0.0013895852025598288 + 0.01 * 6.7504730224609375
Epoch 1240, val loss: 1.117784857749939
Epoch 1250, training loss: 0.06838464736938477 = 0.0013712601503357291 + 0.01 * 6.701339244842529
Epoch 1250, val loss: 1.1191604137420654
Epoch 1260, training loss: 0.06863053888082504 = 0.0013535043690353632 + 0.01 * 6.727704048156738
Epoch 1260, val loss: 1.120620846748352
Epoch 1270, training loss: 0.06842082738876343 = 0.0013363421894609928 + 0.01 * 6.708448886871338
Epoch 1270, val loss: 1.1220289468765259
Epoch 1280, training loss: 0.06833510100841522 = 0.0013196705840528011 + 0.01 * 6.70154333114624
Epoch 1280, val loss: 1.1233735084533691
Epoch 1290, training loss: 0.06840094178915024 = 0.0013035505544394255 + 0.01 * 6.709738731384277
Epoch 1290, val loss: 1.1245930194854736
Epoch 1300, training loss: 0.06828533113002777 = 0.0012880233116447926 + 0.01 * 6.69973087310791
Epoch 1300, val loss: 1.1261866092681885
Epoch 1310, training loss: 0.06844819337129593 = 0.0012730280868709087 + 0.01 * 6.7175164222717285
Epoch 1310, val loss: 1.1270129680633545
Epoch 1320, training loss: 0.06813301146030426 = 0.001258473377674818 + 0.01 * 6.6874542236328125
Epoch 1320, val loss: 1.1286238431930542
Epoch 1330, training loss: 0.0682322233915329 = 0.0012443073792383075 + 0.01 * 6.69879150390625
Epoch 1330, val loss: 1.1296677589416504
Epoch 1340, training loss: 0.06804308295249939 = 0.0012306065764278173 + 0.01 * 6.681248188018799
Epoch 1340, val loss: 1.1309977769851685
Epoch 1350, training loss: 0.06806427240371704 = 0.0012173003051429987 + 0.01 * 6.68469762802124
Epoch 1350, val loss: 1.1319478750228882
Epoch 1360, training loss: 0.06792344897985458 = 0.0012043701717630029 + 0.01 * 6.671907901763916
Epoch 1360, val loss: 1.1332632303237915
Epoch 1370, training loss: 0.06798317283391953 = 0.0011918541276827455 + 0.01 * 6.679131984710693
Epoch 1370, val loss: 1.1344068050384521
Epoch 1380, training loss: 0.06794760376214981 = 0.0011797372717410326 + 0.01 * 6.676786422729492
Epoch 1380, val loss: 1.1353801488876343
Epoch 1390, training loss: 0.06781048327684402 = 0.001167947775684297 + 0.01 * 6.664254188537598
Epoch 1390, val loss: 1.136631727218628
Epoch 1400, training loss: 0.0679011270403862 = 0.0011564852902665734 + 0.01 * 6.674464225769043
Epoch 1400, val loss: 1.1375906467437744
Epoch 1410, training loss: 0.0677020400762558 = 0.0011453664628788829 + 0.01 * 6.655667781829834
Epoch 1410, val loss: 1.1386269330978394
Epoch 1420, training loss: 0.06781111657619476 = 0.0011345200473442674 + 0.01 * 6.667659759521484
Epoch 1420, val loss: 1.1396666765213013
Epoch 1430, training loss: 0.06774760782718658 = 0.001123962108977139 + 0.01 * 6.662364482879639
Epoch 1430, val loss: 1.1405692100524902
Epoch 1440, training loss: 0.06757867336273193 = 0.0011137258261442184 + 0.01 * 6.6464948654174805
Epoch 1440, val loss: 1.1415208578109741
Epoch 1450, training loss: 0.06775956600904465 = 0.001103789429180324 + 0.01 * 6.6655778884887695
Epoch 1450, val loss: 1.1423611640930176
Epoch 1460, training loss: 0.06774862855672836 = 0.001094095059670508 + 0.01 * 6.6654534339904785
Epoch 1460, val loss: 1.1433881521224976
Epoch 1470, training loss: 0.06760018318891525 = 0.001084678340703249 + 0.01 * 6.651550769805908
Epoch 1470, val loss: 1.1442501544952393
Epoch 1480, training loss: 0.06763724237680435 = 0.0010754260001704097 + 0.01 * 6.656182289123535
Epoch 1480, val loss: 1.14517080783844
Epoch 1490, training loss: 0.06772966682910919 = 0.0010664289584383368 + 0.01 * 6.666323661804199
Epoch 1490, val loss: 1.145982027053833
Epoch 1500, training loss: 0.06754659861326218 = 0.001057738671079278 + 0.01 * 6.648886203765869
Epoch 1500, val loss: 1.1468549966812134
Epoch 1510, training loss: 0.06789427995681763 = 0.0010491886641830206 + 0.01 * 6.684508800506592
Epoch 1510, val loss: 1.1475378274917603
Epoch 1520, training loss: 0.06749900430440903 = 0.0010408967500552535 + 0.01 * 6.645810604095459
Epoch 1520, val loss: 1.1485011577606201
Epoch 1530, training loss: 0.06752096116542816 = 0.0010328330099582672 + 0.01 * 6.648813247680664
Epoch 1530, val loss: 1.1492124795913696
Epoch 1540, training loss: 0.06742213666439056 = 0.0010249317856505513 + 0.01 * 6.639720916748047
Epoch 1540, val loss: 1.1499178409576416
Epoch 1550, training loss: 0.06730427592992783 = 0.0010171940084546804 + 0.01 * 6.628708362579346
Epoch 1550, val loss: 1.150773525238037
Epoch 1560, training loss: 0.0673297867178917 = 0.001009677886031568 + 0.01 * 6.632010459899902
Epoch 1560, val loss: 1.1514506340026855
Epoch 1570, training loss: 0.06729911267757416 = 0.0010023320792242885 + 0.01 * 6.629677772521973
Epoch 1570, val loss: 1.1521192789077759
Epoch 1580, training loss: 0.06725545227527618 = 0.000995192094705999 + 0.01 * 6.626026153564453
Epoch 1580, val loss: 1.1528162956237793
Epoch 1590, training loss: 0.06727853417396545 = 0.0009881913429126143 + 0.01 * 6.629034519195557
Epoch 1590, val loss: 1.1535154581069946
Epoch 1600, training loss: 0.06721888482570648 = 0.00098132505081594 + 0.01 * 6.623756408691406
Epoch 1600, val loss: 1.1541868448257446
Epoch 1610, training loss: 0.06716760247945786 = 0.000974616501480341 + 0.01 * 6.619298934936523
Epoch 1610, val loss: 1.1549263000488281
Epoch 1620, training loss: 0.0673164427280426 = 0.0009681160445325077 + 0.01 * 6.634833335876465
Epoch 1620, val loss: 1.1555430889129639
Epoch 1630, training loss: 0.06706583499908447 = 0.0009617404430173337 + 0.01 * 6.610409259796143
Epoch 1630, val loss: 1.1561130285263062
Epoch 1640, training loss: 0.06709073483943939 = 0.0009555297438055277 + 0.01 * 6.613520622253418
Epoch 1640, val loss: 1.156869888305664
Epoch 1650, training loss: 0.06714649498462677 = 0.0009494491969235241 + 0.01 * 6.619704723358154
Epoch 1650, val loss: 1.1573976278305054
Epoch 1660, training loss: 0.06724786758422852 = 0.0009434505482204258 + 0.01 * 6.630441665649414
Epoch 1660, val loss: 1.1580272912979126
Epoch 1670, training loss: 0.06708413362503052 = 0.0009376031230203807 + 0.01 * 6.61465311050415
Epoch 1670, val loss: 1.1585649251937866
Epoch 1680, training loss: 0.06703019887208939 = 0.0009318641969002783 + 0.01 * 6.609833717346191
Epoch 1680, val loss: 1.1591789722442627
Epoch 1690, training loss: 0.06713967025279999 = 0.0009262295207008719 + 0.01 * 6.621344566345215
Epoch 1690, val loss: 1.159712314605713
Epoch 1700, training loss: 0.06708671897649765 = 0.0009207188268192112 + 0.01 * 6.616600036621094
Epoch 1700, val loss: 1.1603654623031616
Epoch 1710, training loss: 0.06697516143321991 = 0.0009153432329185307 + 0.01 * 6.605982303619385
Epoch 1710, val loss: 1.160745620727539
Epoch 1720, training loss: 0.06696294248104095 = 0.0009100626921281219 + 0.01 * 6.605287551879883
Epoch 1720, val loss: 1.161296010017395
Epoch 1730, training loss: 0.0669565424323082 = 0.0009048892534337938 + 0.01 * 6.605165481567383
Epoch 1730, val loss: 1.1619296073913574
Epoch 1740, training loss: 0.06683418899774551 = 0.0008998393313959241 + 0.01 * 6.593435287475586
Epoch 1740, val loss: 1.1623677015304565
Epoch 1750, training loss: 0.0670509785413742 = 0.0008948598988354206 + 0.01 * 6.615612030029297
Epoch 1750, val loss: 1.1629911661148071
Epoch 1760, training loss: 0.06672858446836472 = 0.0008900118409655988 + 0.01 * 6.583857536315918
Epoch 1760, val loss: 1.1633820533752441
Epoch 1770, training loss: 0.06689650565385818 = 0.0008852389873936772 + 0.01 * 6.601126670837402
Epoch 1770, val loss: 1.1639268398284912
Epoch 1780, training loss: 0.06676667928695679 = 0.0008805882534943521 + 0.01 * 6.588608741760254
Epoch 1780, val loss: 1.1644166707992554
Epoch 1790, training loss: 0.06691858917474747 = 0.0008760137134231627 + 0.01 * 6.604257106781006
Epoch 1790, val loss: 1.1647471189498901
Epoch 1800, training loss: 0.06682547926902771 = 0.0008715100702829659 + 0.01 * 6.595396995544434
Epoch 1800, val loss: 1.1654112339019775
Epoch 1810, training loss: 0.06683960556983948 = 0.0008670938550494611 + 0.01 * 6.5972514152526855
Epoch 1810, val loss: 1.1657298803329468
Epoch 1820, training loss: 0.06666667759418488 = 0.0008627656497992575 + 0.01 * 6.580391883850098
Epoch 1820, val loss: 1.1662577390670776
Epoch 1830, training loss: 0.06668078154325485 = 0.0008585018804296851 + 0.01 * 6.582228183746338
Epoch 1830, val loss: 1.1666673421859741
Epoch 1840, training loss: 0.06653377413749695 = 0.000854309240821749 + 0.01 * 6.567946910858154
Epoch 1840, val loss: 1.1670912504196167
Epoch 1850, training loss: 0.06677348911762238 = 0.0008501735283061862 + 0.01 * 6.592331886291504
Epoch 1850, val loss: 1.167668342590332
Epoch 1860, training loss: 0.0665355697274208 = 0.0008461748948320746 + 0.01 * 6.568939208984375
Epoch 1860, val loss: 1.1680207252502441
Epoch 1870, training loss: 0.06672698259353638 = 0.0008422281825914979 + 0.01 * 6.588475227355957
Epoch 1870, val loss: 1.1684232950210571
Epoch 1880, training loss: 0.066749207675457 = 0.0008383552194572985 + 0.01 * 6.591085433959961
Epoch 1880, val loss: 1.1688724756240845
Epoch 1890, training loss: 0.06669630110263824 = 0.0008345466339960694 + 0.01 * 6.586175441741943
Epoch 1890, val loss: 1.1693116426467896
Epoch 1900, training loss: 0.06672996282577515 = 0.000830807548481971 + 0.01 * 6.589915752410889
Epoch 1900, val loss: 1.1695059537887573
Epoch 1910, training loss: 0.06636866927146912 = 0.0008270885446108878 + 0.01 * 6.5541582107543945
Epoch 1910, val loss: 1.170090913772583
Epoch 1920, training loss: 0.06687735766172409 = 0.0008234219858422875 + 0.01 * 6.605393409729004
Epoch 1920, val loss: 1.1705026626586914
Epoch 1930, training loss: 0.06636624783277512 = 0.0008198603754863143 + 0.01 * 6.554638385772705
Epoch 1930, val loss: 1.1708428859710693
Epoch 1940, training loss: 0.06674134731292725 = 0.000816316285636276 + 0.01 * 6.592503547668457
Epoch 1940, val loss: 1.171244502067566
Epoch 1950, training loss: 0.06637237221002579 = 0.0008128439076244831 + 0.01 * 6.555953025817871
Epoch 1950, val loss: 1.1716347932815552
Epoch 1960, training loss: 0.06665560603141785 = 0.0008094337536022067 + 0.01 * 6.584617614746094
Epoch 1960, val loss: 1.171995997428894
Epoch 1970, training loss: 0.06658700853586197 = 0.0008060926920734346 + 0.01 * 6.578091621398926
Epoch 1970, val loss: 1.1725034713745117
Epoch 1980, training loss: 0.066526398062706 = 0.0008028351003304124 + 0.01 * 6.572356224060059
Epoch 1980, val loss: 1.1727262735366821
Epoch 1990, training loss: 0.06635180115699768 = 0.0007995891501195729 + 0.01 * 6.555221080780029
Epoch 1990, val loss: 1.173064112663269
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.045074224472046 = 1.9591054916381836 + 0.01 * 8.596863746643066
Epoch 0, val loss: 1.9533565044403076
Epoch 10, training loss: 2.0342185497283936 = 1.9482505321502686 + 0.01 * 8.596805572509766
Epoch 10, val loss: 1.9428654909133911
Epoch 20, training loss: 2.0206902027130127 = 1.9347244501113892 + 0.01 * 8.59657096862793
Epoch 20, val loss: 1.9291483163833618
Epoch 30, training loss: 2.0015337467193604 = 1.9155774116516113 + 0.01 * 8.595636367797852
Epoch 30, val loss: 1.9094281196594238
Epoch 40, training loss: 1.973239541053772 = 1.887342929840088 + 0.01 * 8.589661598205566
Epoch 40, val loss: 1.8804996013641357
Epoch 50, training loss: 1.9335252046585083 = 1.8480279445648193 + 0.01 * 8.549731254577637
Epoch 50, val loss: 1.8420202732086182
Epoch 60, training loss: 1.8882837295532227 = 1.8045539855957031 + 0.01 * 8.372973442077637
Epoch 60, val loss: 1.8041456937789917
Epoch 70, training loss: 1.8511192798614502 = 1.7692866325378418 + 0.01 * 8.183262825012207
Epoch 70, val loss: 1.7774063348770142
Epoch 80, training loss: 1.8096081018447876 = 1.7289936542510986 + 0.01 * 8.061448097229004
Epoch 80, val loss: 1.7433679103851318
Epoch 90, training loss: 1.7511978149414062 = 1.6725664138793945 + 0.01 * 7.8631439208984375
Epoch 90, val loss: 1.6931506395339966
Epoch 100, training loss: 1.6721127033233643 = 1.595729947090149 + 0.01 * 7.638278484344482
Epoch 100, val loss: 1.625138282775879
Epoch 110, training loss: 1.5770829916000366 = 1.5021042823791504 + 0.01 * 7.497872352600098
Epoch 110, val loss: 1.545671820640564
Epoch 120, training loss: 1.4771672487258911 = 1.4029467105865479 + 0.01 * 7.422050952911377
Epoch 120, val loss: 1.4629358053207397
Epoch 130, training loss: 1.378298044204712 = 1.3049176931381226 + 0.01 * 7.338036060333252
Epoch 130, val loss: 1.3829741477966309
Epoch 140, training loss: 1.2825140953063965 = 1.2099756002426147 + 0.01 * 7.2538533210754395
Epoch 140, val loss: 1.3062514066696167
Epoch 150, training loss: 1.1915618181228638 = 1.119604468345642 + 0.01 * 7.195735931396484
Epoch 150, val loss: 1.2339030504226685
Epoch 160, training loss: 1.1068394184112549 = 1.0351862907409668 + 0.01 * 7.16531229019165
Epoch 160, val loss: 1.1672006845474243
Epoch 170, training loss: 1.0278139114379883 = 0.9563748836517334 + 0.01 * 7.14390230178833
Epoch 170, val loss: 1.1065096855163574
Epoch 180, training loss: 0.9521143436431885 = 0.8809114694595337 + 0.01 * 7.120290279388428
Epoch 180, val loss: 1.0493260622024536
Epoch 190, training loss: 0.8776029348373413 = 0.8066071271896362 + 0.01 * 7.099578857421875
Epoch 190, val loss: 0.9933817386627197
Epoch 200, training loss: 0.8043788075447083 = 0.7335408329963684 + 0.01 * 7.083797931671143
Epoch 200, val loss: 0.938796877861023
Epoch 210, training loss: 0.7346039414405823 = 0.6638726592063904 + 0.01 * 7.073129653930664
Epoch 210, val loss: 0.8878868818283081
Epoch 220, training loss: 0.6706393957138062 = 0.599976658821106 + 0.01 * 7.0662760734558105
Epoch 220, val loss: 0.8432852625846863
Epoch 230, training loss: 0.6135319471359253 = 0.5429107546806335 + 0.01 * 7.06212043762207
Epoch 230, val loss: 0.8068483471870422
Epoch 240, training loss: 0.5627933740615845 = 0.4922071397304535 + 0.01 * 7.058626174926758
Epoch 240, val loss: 0.7784545421600342
Epoch 250, training loss: 0.5170189738273621 = 0.4464751183986664 + 0.01 * 7.054385662078857
Epoch 250, val loss: 0.7569072246551514
Epoch 260, training loss: 0.4748139977455139 = 0.40431642532348633 + 0.01 * 7.04975700378418
Epoch 260, val loss: 0.7403219938278198
Epoch 270, training loss: 0.4353085160255432 = 0.36486387252807617 + 0.01 * 7.044466018676758
Epoch 270, val loss: 0.7271415591239929
Epoch 280, training loss: 0.3981686532497406 = 0.3277507722377777 + 0.01 * 7.041788578033447
Epoch 280, val loss: 0.7162039875984192
Epoch 290, training loss: 0.3632526695728302 = 0.29290372133255005 + 0.01 * 7.034895896911621
Epoch 290, val loss: 0.7070167660713196
Epoch 300, training loss: 0.33061331510543823 = 0.2603203058242798 + 0.01 * 7.029301166534424
Epoch 300, val loss: 0.6996267437934875
Epoch 310, training loss: 0.3004304766654968 = 0.23020097613334656 + 0.01 * 7.022950649261475
Epoch 310, val loss: 0.6943318247795105
Epoch 320, training loss: 0.2731715440750122 = 0.2029501348733902 + 0.01 * 7.0221405029296875
Epoch 320, val loss: 0.6916257739067078
Epoch 330, training loss: 0.24907773733139038 = 0.1789301633834839 + 0.01 * 7.014758110046387
Epoch 330, val loss: 0.6917965412139893
Epoch 340, training loss: 0.22819624841213226 = 0.15811437368392944 + 0.01 * 7.008187294006348
Epoch 340, val loss: 0.694681704044342
Epoch 350, training loss: 0.2103099375963211 = 0.14016243815422058 + 0.01 * 7.014750003814697
Epoch 350, val loss: 0.6999627351760864
Epoch 360, training loss: 0.19469067454338074 = 0.12464312463998795 + 0.01 * 7.004754066467285
Epoch 360, val loss: 0.707071840763092
Epoch 370, training loss: 0.18110135197639465 = 0.11113537102937698 + 0.01 * 6.996598243713379
Epoch 370, val loss: 0.7154912948608398
Epoch 380, training loss: 0.16923187673091888 = 0.09929680079221725 + 0.01 * 6.993507385253906
Epoch 380, val loss: 0.7248947024345398
Epoch 390, training loss: 0.15877455472946167 = 0.08886488527059555 + 0.01 * 6.990967750549316
Epoch 390, val loss: 0.7350562810897827
Epoch 400, training loss: 0.14950519800186157 = 0.07963258773088455 + 0.01 * 6.9872612953186035
Epoch 400, val loss: 0.7457650303840637
Epoch 410, training loss: 0.14155137538909912 = 0.07144999504089355 + 0.01 * 7.010137557983398
Epoch 410, val loss: 0.7568721771240234
Epoch 420, training loss: 0.13404281437397003 = 0.0642056092619896 + 0.01 * 6.983720779418945
Epoch 420, val loss: 0.7683531045913696
Epoch 430, training loss: 0.12759831547737122 = 0.05778966099023819 + 0.01 * 6.980865955352783
Epoch 430, val loss: 0.7799643874168396
Epoch 440, training loss: 0.12188331037759781 = 0.05210772156715393 + 0.01 * 6.9775590896606445
Epoch 440, val loss: 0.7916783690452576
Epoch 450, training loss: 0.11681793630123138 = 0.04707872122526169 + 0.01 * 6.973921298980713
Epoch 450, val loss: 0.803327739238739
Epoch 460, training loss: 0.11233871430158615 = 0.04262963682413101 + 0.01 * 6.970907688140869
Epoch 460, val loss: 0.8148277997970581
Epoch 470, training loss: 0.10837265104055405 = 0.03869355469942093 + 0.01 * 6.967909812927246
Epoch 470, val loss: 0.8261594772338867
Epoch 480, training loss: 0.10485948622226715 = 0.03521091863512993 + 0.01 * 6.9648566246032715
Epoch 480, val loss: 0.8372559547424316
Epoch 490, training loss: 0.10189150273799896 = 0.03212722763419151 + 0.01 * 6.9764275550842285
Epoch 490, val loss: 0.8480720520019531
Epoch 500, training loss: 0.0989961326122284 = 0.029395734891295433 + 0.01 * 6.960040092468262
Epoch 500, val loss: 0.8586112260818481
Epoch 510, training loss: 0.0965394452214241 = 0.026970522478222847 + 0.01 * 6.956892490386963
Epoch 510, val loss: 0.8688361644744873
Epoch 520, training loss: 0.09434372186660767 = 0.02481278032064438 + 0.01 * 6.953094005584717
Epoch 520, val loss: 0.8787527680397034
Epoch 530, training loss: 0.09239456057548523 = 0.0228886641561985 + 0.01 * 6.950589179992676
Epoch 530, val loss: 0.888395369052887
Epoch 540, training loss: 0.09068788588047028 = 0.021169304847717285 + 0.01 * 6.9518585205078125
Epoch 540, val loss: 0.8977615237236023
Epoch 550, training loss: 0.08909884840250015 = 0.019629893824458122 + 0.01 * 6.946895599365234
Epoch 550, val loss: 0.9067795276641846
Epoch 560, training loss: 0.08766527473926544 = 0.01824774593114853 + 0.01 * 6.941752910614014
Epoch 560, val loss: 0.9155572652816772
Epoch 570, training loss: 0.08637701719999313 = 0.017003601416945457 + 0.01 * 6.937341213226318
Epoch 570, val loss: 0.9240394830703735
Epoch 580, training loss: 0.08530367165803909 = 0.01588091067969799 + 0.01 * 6.9422760009765625
Epoch 580, val loss: 0.9322342872619629
Epoch 590, training loss: 0.08418232947587967 = 0.014865768142044544 + 0.01 * 6.931656360626221
Epoch 590, val loss: 0.9402071833610535
Epoch 600, training loss: 0.08326643705368042 = 0.01394528429955244 + 0.01 * 6.93211555480957
Epoch 600, val loss: 0.947920560836792
Epoch 610, training loss: 0.08237449824810028 = 0.013108505867421627 + 0.01 * 6.926599025726318
Epoch 610, val loss: 0.9553356170654297
Epoch 620, training loss: 0.08156176656484604 = 0.012345707044005394 + 0.01 * 6.921606063842773
Epoch 620, val loss: 0.9625444412231445
Epoch 630, training loss: 0.08093735575675964 = 0.01164861861616373 + 0.01 * 6.9288740158081055
Epoch 630, val loss: 0.9695178866386414
Epoch 640, training loss: 0.08021427690982819 = 0.011010964401066303 + 0.01 * 6.9203314781188965
Epoch 640, val loss: 0.9763363599777222
Epoch 650, training loss: 0.0795479491353035 = 0.01042602863162756 + 0.01 * 6.912192344665527
Epoch 650, val loss: 0.9829047322273254
Epoch 660, training loss: 0.07905234396457672 = 0.009888175874948502 + 0.01 * 6.916416645050049
Epoch 660, val loss: 0.9892478585243225
Epoch 670, training loss: 0.07848306745290756 = 0.0093932393938303 + 0.01 * 6.90898323059082
Epoch 670, val loss: 0.995435357093811
Epoch 680, training loss: 0.07796281576156616 = 0.008936513215303421 + 0.01 * 6.902629852294922
Epoch 680, val loss: 1.001412272453308
Epoch 690, training loss: 0.07751616835594177 = 0.008513919077813625 + 0.01 * 6.900224685668945
Epoch 690, val loss: 1.0072424411773682
Epoch 700, training loss: 0.07722707837820053 = 0.00812220573425293 + 0.01 * 6.910487174987793
Epoch 700, val loss: 1.0129286050796509
Epoch 710, training loss: 0.07674878090620041 = 0.00775872403755784 + 0.01 * 6.899005889892578
Epoch 710, val loss: 1.0183656215667725
Epoch 720, training loss: 0.07640096545219421 = 0.007420700043439865 + 0.01 * 6.898026466369629
Epoch 720, val loss: 1.0237540006637573
Epoch 730, training loss: 0.07598941028118134 = 0.0071057965978980064 + 0.01 * 6.88836145401001
Epoch 730, val loss: 1.028955340385437
Epoch 740, training loss: 0.07568448781967163 = 0.006811994593590498 + 0.01 * 6.88724946975708
Epoch 740, val loss: 1.0340304374694824
Epoch 750, training loss: 0.07537603378295898 = 0.006537036504596472 + 0.01 * 6.883900165557861
Epoch 750, val loss: 1.0389764308929443
Epoch 760, training loss: 0.0751112774014473 = 0.006279734428972006 + 0.01 * 6.883154392242432
Epoch 760, val loss: 1.0438169240951538
Epoch 770, training loss: 0.07483962178230286 = 0.006037724670022726 + 0.01 * 6.880189895629883
Epoch 770, val loss: 1.048590064048767
Epoch 780, training loss: 0.07452382147312164 = 0.00581044377759099 + 0.01 * 6.871337413787842
Epoch 780, val loss: 1.0531808137893677
Epoch 790, training loss: 0.07433325797319412 = 0.005597135983407497 + 0.01 * 6.873612403869629
Epoch 790, val loss: 1.05768883228302
Epoch 800, training loss: 0.07404212653636932 = 0.005395956803113222 + 0.01 * 6.864616870880127
Epoch 800, val loss: 1.0620980262756348
Epoch 810, training loss: 0.07392166554927826 = 0.005205920897424221 + 0.01 * 6.871574878692627
Epoch 810, val loss: 1.0664159059524536
Epoch 820, training loss: 0.0736355409026146 = 0.005026691593229771 + 0.01 * 6.860885143280029
Epoch 820, val loss: 1.0706074237823486
Epoch 830, training loss: 0.07351842522621155 = 0.004857642110437155 + 0.01 * 6.866077899932861
Epoch 830, val loss: 1.0746896266937256
Epoch 840, training loss: 0.0732237845659256 = 0.0046976488083601 + 0.01 * 6.85261344909668
Epoch 840, val loss: 1.0787135362625122
Epoch 850, training loss: 0.07320462167263031 = 0.004546383861452341 + 0.01 * 6.865824222564697
Epoch 850, val loss: 1.0826600790023804
Epoch 860, training loss: 0.07285510748624802 = 0.0044032796286046505 + 0.01 * 6.8451828956604
Epoch 860, val loss: 1.0864684581756592
Epoch 870, training loss: 0.07268490642309189 = 0.004267748910933733 + 0.01 * 6.8417158126831055
Epoch 870, val loss: 1.0902223587036133
Epoch 880, training loss: 0.07264910638332367 = 0.004138925578445196 + 0.01 * 6.85101842880249
Epoch 880, val loss: 1.0939315557479858
Epoch 890, training loss: 0.07254748046398163 = 0.004016741178929806 + 0.01 * 6.853074073791504
Epoch 890, val loss: 1.0975040197372437
Epoch 900, training loss: 0.07222079485654831 = 0.003900726791471243 + 0.01 * 6.832006454467773
Epoch 900, val loss: 1.1009833812713623
Epoch 910, training loss: 0.07221492379903793 = 0.003790159011259675 + 0.01 * 6.8424763679504395
Epoch 910, val loss: 1.1043928861618042
Epoch 920, training loss: 0.07209595292806625 = 0.0036853584460914135 + 0.01 * 6.841060161590576
Epoch 920, val loss: 1.1077717542648315
Epoch 930, training loss: 0.071939617395401 = 0.00358560960739851 + 0.01 * 6.83540153503418
Epoch 930, val loss: 1.111016035079956
Epoch 940, training loss: 0.07172366976737976 = 0.003490208415314555 + 0.01 * 6.823346138000488
Epoch 940, val loss: 1.1142135858535767
Epoch 950, training loss: 0.07160872220993042 = 0.003399125998839736 + 0.01 * 6.82096004486084
Epoch 950, val loss: 1.1174185276031494
Epoch 960, training loss: 0.07145161926746368 = 0.0033121025189757347 + 0.01 * 6.81395149230957
Epoch 960, val loss: 1.1204628944396973
Epoch 970, training loss: 0.0713440552353859 = 0.00322893843986094 + 0.01 * 6.811511993408203
Epoch 970, val loss: 1.1234997510910034
Epoch 980, training loss: 0.07127390056848526 = 0.0031492726411670446 + 0.01 * 6.812463283538818
Epoch 980, val loss: 1.1264885663986206
Epoch 990, training loss: 0.07123883068561554 = 0.0030734287574887276 + 0.01 * 6.816540241241455
Epoch 990, val loss: 1.1292794942855835
Epoch 1000, training loss: 0.07113803178071976 = 0.0030009031761437654 + 0.01 * 6.813713073730469
Epoch 1000, val loss: 1.1321191787719727
Epoch 1010, training loss: 0.07102880626916885 = 0.0029312316328287125 + 0.01 * 6.809757709503174
Epoch 1010, val loss: 1.1348365545272827
Epoch 1020, training loss: 0.07079768925905228 = 0.0028647060971707106 + 0.01 * 6.793298721313477
Epoch 1020, val loss: 1.1375322341918945
Epoch 1030, training loss: 0.07109665125608444 = 0.0028008485678583384 + 0.01 * 6.829579830169678
Epoch 1030, val loss: 1.1401407718658447
Epoch 1040, training loss: 0.07076181471347809 = 0.002739707939326763 + 0.01 * 6.802210807800293
Epoch 1040, val loss: 1.1427007913589478
Epoch 1050, training loss: 0.07059009373188019 = 0.002681109821423888 + 0.01 * 6.79089879989624
Epoch 1050, val loss: 1.1452282667160034
Epoch 1060, training loss: 0.07057136297225952 = 0.0026248812209814787 + 0.01 * 6.794647693634033
Epoch 1060, val loss: 1.1475640535354614
Epoch 1070, training loss: 0.07042491436004639 = 0.0025709050241857767 + 0.01 * 6.785401344299316
Epoch 1070, val loss: 1.1499298810958862
Epoch 1080, training loss: 0.0705159455537796 = 0.002518872730433941 + 0.01 * 6.799707889556885
Epoch 1080, val loss: 1.1522791385650635
Epoch 1090, training loss: 0.07031819969415665 = 0.0024691687431186438 + 0.01 * 6.784903526306152
Epoch 1090, val loss: 1.1545295715332031
Epoch 1100, training loss: 0.07031887024641037 = 0.002421314362436533 + 0.01 * 6.7897562980651855
Epoch 1100, val loss: 1.156747579574585
Epoch 1110, training loss: 0.07021241635084152 = 0.002375204348936677 + 0.01 * 6.783720970153809
Epoch 1110, val loss: 1.158848524093628
Epoch 1120, training loss: 0.07012101262807846 = 0.002330916468054056 + 0.01 * 6.779009819030762
Epoch 1120, val loss: 1.1609904766082764
Epoch 1130, training loss: 0.06989818066358566 = 0.002288361545652151 + 0.01 * 6.760981559753418
Epoch 1130, val loss: 1.163024663925171
Epoch 1140, training loss: 0.06995890289545059 = 0.002247374737635255 + 0.01 * 6.771152973175049
Epoch 1140, val loss: 1.1650625467300415
Epoch 1150, training loss: 0.06969985365867615 = 0.002207791432738304 + 0.01 * 6.74920654296875
Epoch 1150, val loss: 1.1670255661010742
Epoch 1160, training loss: 0.06961419433355331 = 0.0021696481853723526 + 0.01 * 6.744454860687256
Epoch 1160, val loss: 1.1689902544021606
Epoch 1170, training loss: 0.06962921470403671 = 0.0021328723523765802 + 0.01 * 6.749634742736816
Epoch 1170, val loss: 1.1708778142929077
Epoch 1180, training loss: 0.06948048621416092 = 0.0020975039806216955 + 0.01 * 6.738298416137695
Epoch 1180, val loss: 1.1726332902908325
Epoch 1190, training loss: 0.06982421875 = 0.002063390100374818 + 0.01 * 6.776082992553711
Epoch 1190, val loss: 1.174446702003479
Epoch 1200, training loss: 0.06958205997943878 = 0.0020303677301853895 + 0.01 * 6.755169868469238
Epoch 1200, val loss: 1.1761467456817627
Epoch 1210, training loss: 0.0693490281701088 = 0.0019986038096249104 + 0.01 * 6.735042572021484
Epoch 1210, val loss: 1.1778360605239868
Epoch 1220, training loss: 0.06964338570833206 = 0.0019676752854138613 + 0.01 * 6.767570972442627
Epoch 1220, val loss: 1.179513931274414
Epoch 1230, training loss: 0.06927136331796646 = 0.0019378192955628037 + 0.01 * 6.733354091644287
Epoch 1230, val loss: 1.1810901165008545
Epoch 1240, training loss: 0.06911918520927429 = 0.0019090550485998392 + 0.01 * 6.721013069152832
Epoch 1240, val loss: 1.1827012300491333
Epoch 1250, training loss: 0.06924702972173691 = 0.0018810461042448878 + 0.01 * 6.736598491668701
Epoch 1250, val loss: 1.1842089891433716
Epoch 1260, training loss: 0.06912178546190262 = 0.001854010741226375 + 0.01 * 6.726778030395508
Epoch 1260, val loss: 1.1857448816299438
Epoch 1270, training loss: 0.06924602389335632 = 0.0018278098432347178 + 0.01 * 6.741822242736816
Epoch 1270, val loss: 1.1872243881225586
Epoch 1280, training loss: 0.0690418928861618 = 0.001802462269552052 + 0.01 * 6.723943710327148
Epoch 1280, val loss: 1.1886446475982666
Epoch 1290, training loss: 0.06884420663118362 = 0.0017779621994122863 + 0.01 * 6.706624507904053
Epoch 1290, val loss: 1.1900379657745361
Epoch 1300, training loss: 0.06887377798557281 = 0.0017540857661515474 + 0.01 * 6.711969375610352
Epoch 1300, val loss: 1.1914302110671997
Epoch 1310, training loss: 0.06912414729595184 = 0.001730879070237279 + 0.01 * 6.7393269538879395
Epoch 1310, val loss: 1.1927658319473267
Epoch 1320, training loss: 0.06879891455173492 = 0.0017084411811083555 + 0.01 * 6.709047794342041
Epoch 1320, val loss: 1.1940704584121704
Epoch 1330, training loss: 0.06863483041524887 = 0.0016867188969627023 + 0.01 * 6.69481086730957
Epoch 1330, val loss: 1.195325255393982
Epoch 1340, training loss: 0.06880217045545578 = 0.0016655351500958204 + 0.01 * 6.713663578033447
Epoch 1340, val loss: 1.196555495262146
Epoch 1350, training loss: 0.06876254826784134 = 0.0016450447728857398 + 0.01 * 6.7117509841918945
Epoch 1350, val loss: 1.197831392288208
Epoch 1360, training loss: 0.06880252808332443 = 0.0016251157503575087 + 0.01 * 6.7177414894104
Epoch 1360, val loss: 1.1989786624908447
Epoch 1370, training loss: 0.06865925341844559 = 0.0016058087348937988 + 0.01 * 6.7053446769714355
Epoch 1370, val loss: 1.2001080513000488
Epoch 1380, training loss: 0.06854966282844543 = 0.0015870165079832077 + 0.01 * 6.696264266967773
Epoch 1380, val loss: 1.2011961936950684
Epoch 1390, training loss: 0.06850118935108185 = 0.00156866991892457 + 0.01 * 6.6932525634765625
Epoch 1390, val loss: 1.2022837400436401
Epoch 1400, training loss: 0.06856878101825714 = 0.0015508008655160666 + 0.01 * 6.701797962188721
Epoch 1400, val loss: 1.2034008502960205
Epoch 1410, training loss: 0.06843888014554977 = 0.0015334744239225984 + 0.01 * 6.690540790557861
Epoch 1410, val loss: 1.2044070959091187
Epoch 1420, training loss: 0.06829521805047989 = 0.0015166174853220582 + 0.01 * 6.677860260009766
Epoch 1420, val loss: 1.2054157257080078
Epoch 1430, training loss: 0.06832669675350189 = 0.0015001327265053988 + 0.01 * 6.682656764984131
Epoch 1430, val loss: 1.206373929977417
Epoch 1440, training loss: 0.06829603016376495 = 0.0014841626398265362 + 0.01 * 6.681186676025391
Epoch 1440, val loss: 1.2073785066604614
Epoch 1450, training loss: 0.06829111278057098 = 0.0014684895286336541 + 0.01 * 6.682262420654297
Epoch 1450, val loss: 1.2083065509796143
Epoch 1460, training loss: 0.06838483363389969 = 0.0014533541398122907 + 0.01 * 6.693148136138916
Epoch 1460, val loss: 1.2092167139053345
Epoch 1470, training loss: 0.06824904680252075 = 0.0014384863898158073 + 0.01 * 6.681056499481201
Epoch 1470, val loss: 1.2101198434829712
Epoch 1480, training loss: 0.0682460144162178 = 0.001424037734977901 + 0.01 * 6.6821980476379395
Epoch 1480, val loss: 1.2109678983688354
Epoch 1490, training loss: 0.06822526454925537 = 0.001409966847859323 + 0.01 * 6.681529521942139
Epoch 1490, val loss: 1.2118412256240845
Epoch 1500, training loss: 0.0681772530078888 = 0.0013962994562461972 + 0.01 * 6.678095817565918
Epoch 1500, val loss: 1.2126511335372925
Epoch 1510, training loss: 0.06814292818307877 = 0.001382864429615438 + 0.01 * 6.67600679397583
Epoch 1510, val loss: 1.2134425640106201
Epoch 1520, training loss: 0.06815224885940552 = 0.001369861769489944 + 0.01 * 6.678238391876221
Epoch 1520, val loss: 1.214247226715088
Epoch 1530, training loss: 0.06794331967830658 = 0.0013570648152381182 + 0.01 * 6.658625602722168
Epoch 1530, val loss: 1.2149709463119507
Epoch 1540, training loss: 0.06830217689275742 = 0.0013446699595078826 + 0.01 * 6.695750713348389
Epoch 1540, val loss: 1.215711236000061
Epoch 1550, training loss: 0.06804295629262924 = 0.001332470797933638 + 0.01 * 6.671049118041992
Epoch 1550, val loss: 1.2164781093597412
Epoch 1560, training loss: 0.0682598128914833 = 0.0013206282164901495 + 0.01 * 6.693918704986572
Epoch 1560, val loss: 1.2171604633331299
Epoch 1570, training loss: 0.06789867579936981 = 0.0013090423308312893 + 0.01 * 6.658963203430176
Epoch 1570, val loss: 1.217832088470459
Epoch 1580, training loss: 0.06780924648046494 = 0.0012977734440937638 + 0.01 * 6.651147842407227
Epoch 1580, val loss: 1.2185299396514893
Epoch 1590, training loss: 0.06801079958677292 = 0.0012866266770288348 + 0.01 * 6.672417163848877
Epoch 1590, val loss: 1.2191931009292603
Epoch 1600, training loss: 0.06793040037155151 = 0.0012758825905621052 + 0.01 * 6.665452003479004
Epoch 1600, val loss: 1.2198312282562256
Epoch 1610, training loss: 0.06789998710155487 = 0.0012652224395424128 + 0.01 * 6.663476943969727
Epoch 1610, val loss: 1.220497727394104
Epoch 1620, training loss: 0.06781818717718124 = 0.0012548886006698012 + 0.01 * 6.656330585479736
Epoch 1620, val loss: 1.2211060523986816
Epoch 1630, training loss: 0.06778198480606079 = 0.0012448260094970465 + 0.01 * 6.653716087341309
Epoch 1630, val loss: 1.2217262983322144
Epoch 1640, training loss: 0.06799343228340149 = 0.0012348174350336194 + 0.01 * 6.675861835479736
Epoch 1640, val loss: 1.2222726345062256
Epoch 1650, training loss: 0.06766016036272049 = 0.0012251597363501787 + 0.01 * 6.643500328063965
Epoch 1650, val loss: 1.2228343486785889
Epoch 1660, training loss: 0.0678413063287735 = 0.001215714612044394 + 0.01 * 6.662559509277344
Epoch 1660, val loss: 1.2233850955963135
Epoch 1670, training loss: 0.06766371428966522 = 0.0012063600588589907 + 0.01 * 6.645735740661621
Epoch 1670, val loss: 1.2239277362823486
Epoch 1680, training loss: 0.06754282861948013 = 0.0011973291402682662 + 0.01 * 6.634549617767334
Epoch 1680, val loss: 1.2244551181793213
Epoch 1690, training loss: 0.06792040169239044 = 0.0011883849510923028 + 0.01 * 6.673202037811279
Epoch 1690, val loss: 1.2249071598052979
Epoch 1700, training loss: 0.06770481914281845 = 0.0011796961771324277 + 0.01 * 6.652512550354004
Epoch 1700, val loss: 1.225403070449829
Epoch 1710, training loss: 0.06754729896783829 = 0.0011712193954735994 + 0.01 * 6.637608528137207
Epoch 1710, val loss: 1.2258886098861694
Epoch 1720, training loss: 0.06767035275697708 = 0.00116282410454005 + 0.01 * 6.650752544403076
Epoch 1720, val loss: 1.226240873336792
Epoch 1730, training loss: 0.06741658598184586 = 0.0011546628084033728 + 0.01 * 6.626192569732666
Epoch 1730, val loss: 1.2266912460327148
Epoch 1740, training loss: 0.06760095059871674 = 0.0011465963907539845 + 0.01 * 6.645435333251953
Epoch 1740, val loss: 1.2271417379379272
Epoch 1750, training loss: 0.06758817285299301 = 0.0011387442937120795 + 0.01 * 6.644942760467529
Epoch 1750, val loss: 1.227529764175415
Epoch 1760, training loss: 0.06739731878042221 = 0.0011310632107779384 + 0.01 * 6.6266255378723145
Epoch 1760, val loss: 1.2278978824615479
Epoch 1770, training loss: 0.06750953197479248 = 0.001123462920077145 + 0.01 * 6.638607025146484
Epoch 1770, val loss: 1.2282485961914062
Epoch 1780, training loss: 0.0675647184252739 = 0.0011160305002704263 + 0.01 * 6.644868850708008
Epoch 1780, val loss: 1.2286431789398193
Epoch 1790, training loss: 0.0676601454615593 = 0.0011087257880717516 + 0.01 * 6.655141830444336
Epoch 1790, val loss: 1.2290138006210327
Epoch 1800, training loss: 0.06753552705049515 = 0.001101610716432333 + 0.01 * 6.6433916091918945
Epoch 1800, val loss: 1.229317307472229
Epoch 1810, training loss: 0.06733532249927521 = 0.0010946518741548061 + 0.01 * 6.624067306518555
Epoch 1810, val loss: 1.2296268939971924
Epoch 1820, training loss: 0.06728672236204147 = 0.0010877493768930435 + 0.01 * 6.619897842407227
Epoch 1820, val loss: 1.2299247980117798
Epoch 1830, training loss: 0.06715594977140427 = 0.0010810182429850101 + 0.01 * 6.607492923736572
Epoch 1830, val loss: 1.2302519083023071
Epoch 1840, training loss: 0.06744898110628128 = 0.0010743652237579226 + 0.01 * 6.6374616622924805
Epoch 1840, val loss: 1.2304562330245972
Epoch 1850, training loss: 0.06731969118118286 = 0.0010677911341190338 + 0.01 * 6.625189781188965
Epoch 1850, val loss: 1.230757474899292
Epoch 1860, training loss: 0.06716350466012955 = 0.001061424263752997 + 0.01 * 6.610208511352539
Epoch 1860, val loss: 1.2310065031051636
Epoch 1870, training loss: 0.06701184064149857 = 0.001055112574249506 + 0.01 * 6.595672607421875
Epoch 1870, val loss: 1.2312037944793701
Epoch 1880, training loss: 0.06707127392292023 = 0.0010489597916603088 + 0.01 * 6.602231502532959
Epoch 1880, val loss: 1.2314741611480713
Epoch 1890, training loss: 0.06706861406564713 = 0.0010428990935906768 + 0.01 * 6.602571487426758
Epoch 1890, val loss: 1.231649398803711
Epoch 1900, training loss: 0.0674242153763771 = 0.001036892645061016 + 0.01 * 6.63873291015625
Epoch 1900, val loss: 1.2318599224090576
Epoch 1910, training loss: 0.0670812726020813 = 0.0010310054058209062 + 0.01 * 6.605026721954346
Epoch 1910, val loss: 1.232003927230835
Epoch 1920, training loss: 0.06707006692886353 = 0.0010252154897898436 + 0.01 * 6.604485034942627
Epoch 1920, val loss: 1.23220694065094
Epoch 1930, training loss: 0.0674557313323021 = 0.0010195671347901225 + 0.01 * 6.643616676330566
Epoch 1930, val loss: 1.2323156595230103
Epoch 1940, training loss: 0.06710486114025116 = 0.0010139007354155183 + 0.01 * 6.609096527099609
Epoch 1940, val loss: 1.2325074672698975
Epoch 1950, training loss: 0.06721559911966324 = 0.0010084311943501234 + 0.01 * 6.6207170486450195
Epoch 1950, val loss: 1.2326157093048096
Epoch 1960, training loss: 0.06683870404958725 = 0.0010029250988736749 + 0.01 * 6.583578109741211
Epoch 1960, val loss: 1.2327604293823242
Epoch 1970, training loss: 0.06683744490146637 = 0.0009975886205211282 + 0.01 * 6.583985805511475
Epoch 1970, val loss: 1.23288893699646
Epoch 1980, training loss: 0.06705263257026672 = 0.000992300221696496 + 0.01 * 6.6060333251953125
Epoch 1980, val loss: 1.2330148220062256
Epoch 1990, training loss: 0.06689433008432388 = 0.0009870370849967003 + 0.01 * 6.590729236602783
Epoch 1990, val loss: 1.2330985069274902
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8339483394833949
The final CL Acc:0.82099, 0.01062, The final GNN Acc:0.83658, 0.00197
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11580])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10492])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.024714231491089 = 1.9387457370758057 + 0.01 * 8.59683895111084
Epoch 0, val loss: 1.9348185062408447
Epoch 10, training loss: 2.0146186351776123 = 1.9286507368087769 + 0.01 * 8.59679889678955
Epoch 10, val loss: 1.9245320558547974
Epoch 20, training loss: 2.0023856163024902 = 1.916419506072998 + 0.01 * 8.596617698669434
Epoch 20, val loss: 1.9118586778640747
Epoch 30, training loss: 1.9855791330337524 = 1.8996193408966064 + 0.01 * 8.595976829528809
Epoch 30, val loss: 1.8945590257644653
Epoch 40, training loss: 1.961586594581604 = 1.8756632804870605 + 0.01 * 8.59233570098877
Epoch 40, val loss: 1.8704544305801392
Epoch 50, training loss: 1.9293932914733887 = 1.8437151908874512 + 0.01 * 8.567815780639648
Epoch 50, val loss: 1.8400392532348633
Epoch 60, training loss: 1.8948214054107666 = 1.8100330829620361 + 0.01 * 8.47883415222168
Epoch 60, val loss: 1.8121181726455688
Epoch 70, training loss: 1.8637441396713257 = 1.781638741493225 + 0.01 * 8.210535049438477
Epoch 70, val loss: 1.791210412979126
Epoch 80, training loss: 1.8266533613204956 = 1.7454317808151245 + 0.01 * 8.122157096862793
Epoch 80, val loss: 1.7602856159210205
Epoch 90, training loss: 1.7739155292510986 = 1.6938302516937256 + 0.01 * 8.008532524108887
Epoch 90, val loss: 1.714964747428894
Epoch 100, training loss: 1.702118158340454 = 1.622862696647644 + 0.01 * 7.925551891326904
Epoch 100, val loss: 1.65554940700531
Epoch 110, training loss: 1.6170986890792847 = 1.538874626159668 + 0.01 * 7.822408199310303
Epoch 110, val loss: 1.589469075202942
Epoch 120, training loss: 1.5296159982681274 = 1.4533568620681763 + 0.01 * 7.62591028213501
Epoch 120, val loss: 1.5255836248397827
Epoch 130, training loss: 1.4439423084259033 = 1.3699697256088257 + 0.01 * 7.397254467010498
Epoch 130, val loss: 1.464547872543335
Epoch 140, training loss: 1.359184741973877 = 1.2856855392456055 + 0.01 * 7.349923133850098
Epoch 140, val loss: 1.4028007984161377
Epoch 150, training loss: 1.2724995613098145 = 1.19950270652771 + 0.01 * 7.299684047698975
Epoch 150, val loss: 1.3406648635864258
Epoch 160, training loss: 1.186697006225586 = 1.1139793395996094 + 0.01 * 7.2717719078063965
Epoch 160, val loss: 1.2794549465179443
Epoch 170, training loss: 1.1047723293304443 = 1.0321215391159058 + 0.01 * 7.2650837898254395
Epoch 170, val loss: 1.220953345298767
Epoch 180, training loss: 1.0271176099777222 = 0.9544799327850342 + 0.01 * 7.263762474060059
Epoch 180, val loss: 1.1653673648834229
Epoch 190, training loss: 0.9521594643592834 = 0.8795154094696045 + 0.01 * 7.264403820037842
Epoch 190, val loss: 1.1121149063110352
Epoch 200, training loss: 0.8780540227890015 = 0.8054065704345703 + 0.01 * 7.264744758605957
Epoch 200, val loss: 1.059361219406128
Epoch 210, training loss: 0.804499089717865 = 0.7318518161773682 + 0.01 * 7.264727592468262
Epoch 210, val loss: 1.0072358846664429
Epoch 220, training loss: 0.7334401607513428 = 0.6608015894889832 + 0.01 * 7.263857841491699
Epoch 220, val loss: 0.9582350850105286
Epoch 230, training loss: 0.6672371029853821 = 0.5946168899536133 + 0.01 * 7.262021541595459
Epoch 230, val loss: 0.9155037999153137
Epoch 240, training loss: 0.6073012351989746 = 0.5347071886062622 + 0.01 * 7.259406089782715
Epoch 240, val loss: 0.8812916874885559
Epoch 250, training loss: 0.5537723898887634 = 0.4812126159667969 + 0.01 * 7.255979061126709
Epoch 250, val loss: 0.8558446764945984
Epoch 260, training loss: 0.5059041976928711 = 0.43339258432388306 + 0.01 * 7.251158237457275
Epoch 260, val loss: 0.8377783894538879
Epoch 270, training loss: 0.4627472162246704 = 0.3902965188026428 + 0.01 * 7.245068550109863
Epoch 270, val loss: 0.8258365392684937
Epoch 280, training loss: 0.42353901267051697 = 0.35119104385375977 + 0.01 * 7.234797477722168
Epoch 280, val loss: 0.8186712265014648
Epoch 290, training loss: 0.38786110281944275 = 0.3156430423259735 + 0.01 * 7.221806526184082
Epoch 290, val loss: 0.8154684901237488
Epoch 300, training loss: 0.355359822511673 = 0.28329476714134216 + 0.01 * 7.206506252288818
Epoch 300, val loss: 0.8158777952194214
Epoch 310, training loss: 0.3256431818008423 = 0.25373634696006775 + 0.01 * 7.1906819343566895
Epoch 310, val loss: 0.8192498683929443
Epoch 320, training loss: 0.29819241166114807 = 0.2264517992734909 + 0.01 * 7.1740617752075195
Epoch 320, val loss: 0.8251152634620667
Epoch 330, training loss: 0.27270379662513733 = 0.20106017589569092 + 0.01 * 7.164361476898193
Epoch 330, val loss: 0.8333832621574402
Epoch 340, training loss: 0.24907243251800537 = 0.17749129235744476 + 0.01 * 7.158114910125732
Epoch 340, val loss: 0.8438301086425781
Epoch 350, training loss: 0.22744746506214142 = 0.15591077506542206 + 0.01 * 7.153669357299805
Epoch 350, val loss: 0.8565571308135986
Epoch 360, training loss: 0.20802687108516693 = 0.13652180135250092 + 0.01 * 7.15050745010376
Epoch 360, val loss: 0.8715238571166992
Epoch 370, training loss: 0.19092294573783875 = 0.11943601071834564 + 0.01 * 7.148694038391113
Epoch 370, val loss: 0.8885780572891235
Epoch 380, training loss: 0.17604967951774597 = 0.10458332300186157 + 0.01 * 7.146636486053467
Epoch 380, val loss: 0.9074191451072693
Epoch 390, training loss: 0.16320937871932983 = 0.09176624566316605 + 0.01 * 7.144313335418701
Epoch 390, val loss: 0.92762291431427
Epoch 400, training loss: 0.1521635204553604 = 0.08073335886001587 + 0.01 * 7.143016338348389
Epoch 400, val loss: 0.948628306388855
Epoch 410, training loss: 0.1426433026790619 = 0.07123434543609619 + 0.01 * 7.140896797180176
Epoch 410, val loss: 0.9700072407722473
Epoch 420, training loss: 0.1344251185655594 = 0.0630473867058754 + 0.01 * 7.137773036956787
Epoch 420, val loss: 0.991571843624115
Epoch 430, training loss: 0.12735117971897125 = 0.05598337575793266 + 0.01 * 7.136780738830566
Epoch 430, val loss: 1.0129694938659668
Epoch 440, training loss: 0.12120722234249115 = 0.04988129064440727 + 0.01 * 7.132593154907227
Epoch 440, val loss: 1.0340018272399902
Epoch 450, training loss: 0.11589944362640381 = 0.04459872841835022 + 0.01 * 7.130071640014648
Epoch 450, val loss: 1.0545692443847656
Epoch 460, training loss: 0.11131875216960907 = 0.04001832380890846 + 0.01 * 7.130042552947998
Epoch 460, val loss: 1.0745558738708496
Epoch 470, training loss: 0.10727371275424957 = 0.036033716052770615 + 0.01 * 7.124000072479248
Epoch 470, val loss: 1.0940358638763428
Epoch 480, training loss: 0.10376269370317459 = 0.032552897930145264 + 0.01 * 7.1209797859191895
Epoch 480, val loss: 1.1129531860351562
Epoch 490, training loss: 0.10069945454597473 = 0.029501503333449364 + 0.01 * 7.119795322418213
Epoch 490, val loss: 1.1312254667282104
Epoch 500, training loss: 0.09796245396137238 = 0.02682143822312355 + 0.01 * 7.114101886749268
Epoch 500, val loss: 1.1488087177276611
Epoch 510, training loss: 0.09556476026773453 = 0.024459674954414368 + 0.01 * 7.110508441925049
Epoch 510, val loss: 1.1658111810684204
Epoch 520, training loss: 0.09343867748975754 = 0.022372761741280556 + 0.01 * 7.106592178344727
Epoch 520, val loss: 1.1821951866149902
Epoch 530, training loss: 0.09160885214805603 = 0.02053167298436165 + 0.01 * 7.107717990875244
Epoch 530, val loss: 1.1977977752685547
Epoch 540, training loss: 0.08989594131708145 = 0.01890215277671814 + 0.01 * 7.099379062652588
Epoch 540, val loss: 1.2128016948699951
Epoch 550, training loss: 0.08839040249586105 = 0.017451390624046326 + 0.01 * 7.09390115737915
Epoch 550, val loss: 1.2272599935531616
Epoch 560, training loss: 0.08703629672527313 = 0.01615561917424202 + 0.01 * 7.088068008422852
Epoch 560, val loss: 1.2411571741104126
Epoch 570, training loss: 0.08583474159240723 = 0.01499543059617281 + 0.01 * 7.083930969238281
Epoch 570, val loss: 1.2545701265335083
Epoch 580, training loss: 0.08483589440584183 = 0.013956205919384956 + 0.01 * 7.0879693031311035
Epoch 580, val loss: 1.2672938108444214
Epoch 590, training loss: 0.08373794704675674 = 0.01302147563546896 + 0.01 * 7.0716471672058105
Epoch 590, val loss: 1.2795826196670532
Epoch 600, training loss: 0.08282746374607086 = 0.012177571654319763 + 0.01 * 7.06498908996582
Epoch 600, val loss: 1.2914562225341797
Epoch 610, training loss: 0.08214345574378967 = 0.011414580047130585 + 0.01 * 7.072887420654297
Epoch 610, val loss: 1.3028509616851807
Epoch 620, training loss: 0.08129283040761948 = 0.010723891668021679 + 0.01 * 7.056894302368164
Epoch 620, val loss: 1.3137646913528442
Epoch 630, training loss: 0.08055328577756882 = 0.010095454752445221 + 0.01 * 7.045783042907715
Epoch 630, val loss: 1.3243234157562256
Epoch 640, training loss: 0.07999478280544281 = 0.009522557258605957 + 0.01 * 7.04722261428833
Epoch 640, val loss: 1.3345248699188232
Epoch 650, training loss: 0.07929649949073792 = 0.009000249207019806 + 0.01 * 7.029624938964844
Epoch 650, val loss: 1.3443111181259155
Epoch 660, training loss: 0.07869057357311249 = 0.00852288119494915 + 0.01 * 7.016768932342529
Epoch 660, val loss: 1.35359525680542
Epoch 670, training loss: 0.07817239314317703 = 0.008085495792329311 + 0.01 * 7.008689880371094
Epoch 670, val loss: 1.3626691102981567
Epoch 680, training loss: 0.07767056673765182 = 0.007684105541557074 + 0.01 * 6.9986467361450195
Epoch 680, val loss: 1.3712430000305176
Epoch 690, training loss: 0.07730387151241302 = 0.007314892951399088 + 0.01 * 6.998898029327393
Epoch 690, val loss: 1.3795502185821533
Epoch 700, training loss: 0.0766601413488388 = 0.0069748396053910255 + 0.01 * 6.968530654907227
Epoch 700, val loss: 1.3876055479049683
Epoch 710, training loss: 0.0763145238161087 = 0.00666069658473134 + 0.01 * 6.9653825759887695
Epoch 710, val loss: 1.3952921628952026
Epoch 720, training loss: 0.07638390362262726 = 0.006369845941662788 + 0.01 * 7.001406192779541
Epoch 720, val loss: 1.4026731252670288
Epoch 730, training loss: 0.07565687596797943 = 0.0061002932488918304 + 0.01 * 6.955657958984375
Epoch 730, val loss: 1.4097989797592163
Epoch 740, training loss: 0.07548579573631287 = 0.005849363747984171 + 0.01 * 6.963643550872803
Epoch 740, val loss: 1.4166131019592285
Epoch 750, training loss: 0.07500096410512924 = 0.0056160325184464455 + 0.01 * 6.938493728637695
Epoch 750, val loss: 1.423288106918335
Epoch 760, training loss: 0.07472867518663406 = 0.0053979638032615185 + 0.01 * 6.933071613311768
Epoch 760, val loss: 1.4296925067901611
Epoch 770, training loss: 0.07438164204359055 = 0.005193510092794895 + 0.01 * 6.918813705444336
Epoch 770, val loss: 1.4358408451080322
Epoch 780, training loss: 0.07425064593553543 = 0.005002909805625677 + 0.01 * 6.924774169921875
Epoch 780, val loss: 1.4419203996658325
Epoch 790, training loss: 0.07389398664236069 = 0.0048242127522826195 + 0.01 * 6.906977653503418
Epoch 790, val loss: 1.4475589990615845
Epoch 800, training loss: 0.07369735091924667 = 0.00465628644451499 + 0.01 * 6.904107093811035
Epoch 800, val loss: 1.4532748460769653
Epoch 810, training loss: 0.07339619100093842 = 0.004498607479035854 + 0.01 * 6.889758110046387
Epoch 810, val loss: 1.4585448503494263
Epoch 820, training loss: 0.07326187193393707 = 0.004349699709564447 + 0.01 * 6.891217231750488
Epoch 820, val loss: 1.463857650756836
Epoch 830, training loss: 0.07312938570976257 = 0.0042091491632163525 + 0.01 * 6.892024040222168
Epoch 830, val loss: 1.468962550163269
Epoch 840, training loss: 0.07277851551771164 = 0.004076091106981039 + 0.01 * 6.870242595672607
Epoch 840, val loss: 1.4738985300064087
Epoch 850, training loss: 0.07322767376899719 = 0.003950279206037521 + 0.01 * 6.927740097045898
Epoch 850, val loss: 1.4789117574691772
Epoch 860, training loss: 0.0726747065782547 = 0.0038320154417306185 + 0.01 * 6.8842692375183105
Epoch 860, val loss: 1.483418583869934
Epoch 870, training loss: 0.07256599515676498 = 0.0037197903729975224 + 0.01 * 6.884620666503906
Epoch 870, val loss: 1.487943172454834
Epoch 880, training loss: 0.07222916930913925 = 0.003613800276070833 + 0.01 * 6.861536979675293
Epoch 880, val loss: 1.4921739101409912
Epoch 890, training loss: 0.072002112865448 = 0.003512729424983263 + 0.01 * 6.848938465118408
Epoch 890, val loss: 1.4964255094528198
Epoch 900, training loss: 0.07196682691574097 = 0.0034168600104749203 + 0.01 * 6.854996681213379
Epoch 900, val loss: 1.5006352663040161
Epoch 910, training loss: 0.07173387706279755 = 0.00332577433437109 + 0.01 * 6.840810298919678
Epoch 910, val loss: 1.5044962167739868
Epoch 920, training loss: 0.0718475729227066 = 0.003238977398723364 + 0.01 * 6.8608598709106445
Epoch 920, val loss: 1.508535385131836
Epoch 930, training loss: 0.07154691964387894 = 0.0031565872486680746 + 0.01 * 6.839033126831055
Epoch 930, val loss: 1.5121666193008423
Epoch 940, training loss: 0.07161056250333786 = 0.0030776045750826597 + 0.01 * 6.853296279907227
Epoch 940, val loss: 1.5159509181976318
Epoch 950, training loss: 0.0712277963757515 = 0.0030030179768800735 + 0.01 * 6.822478294372559
Epoch 950, val loss: 1.5194578170776367
Epoch 960, training loss: 0.07119492441415787 = 0.0029310123063623905 + 0.01 * 6.826391220092773
Epoch 960, val loss: 1.5228663682937622
Epoch 970, training loss: 0.07135413587093353 = 0.0028625428676605225 + 0.01 * 6.8491597175598145
Epoch 970, val loss: 1.5263203382492065
Epoch 980, training loss: 0.07099544256925583 = 0.0027971251402050257 + 0.01 * 6.819831848144531
Epoch 980, val loss: 1.529575228691101
Epoch 990, training loss: 0.0710865929722786 = 0.002734441077336669 + 0.01 * 6.835216045379639
Epoch 990, val loss: 1.532837152481079
Epoch 1000, training loss: 0.07080283761024475 = 0.0026747265364974737 + 0.01 * 6.812811374664307
Epoch 1000, val loss: 1.5358284711837769
Epoch 1010, training loss: 0.07088667899370193 = 0.0026171053759753704 + 0.01 * 6.8269572257995605
Epoch 1010, val loss: 1.5388630628585815
Epoch 1020, training loss: 0.07061246782541275 = 0.0025623987894505262 + 0.01 * 6.805006980895996
Epoch 1020, val loss: 1.5418486595153809
Epoch 1030, training loss: 0.07083339244127274 = 0.0025094873271882534 + 0.01 * 6.832390785217285
Epoch 1030, val loss: 1.5446511507034302
Epoch 1040, training loss: 0.0704655721783638 = 0.002458929782733321 + 0.01 * 6.80066442489624
Epoch 1040, val loss: 1.547480821609497
Epoch 1050, training loss: 0.07052949070930481 = 0.0024102574680000544 + 0.01 * 6.811923503875732
Epoch 1050, val loss: 1.550309658050537
Epoch 1060, training loss: 0.07029172033071518 = 0.002363670151680708 + 0.01 * 6.7928056716918945
Epoch 1060, val loss: 1.5528130531311035
Epoch 1070, training loss: 0.07016419619321823 = 0.0023186185862869024 + 0.01 * 6.784557342529297
Epoch 1070, val loss: 1.5554190874099731
Epoch 1080, training loss: 0.07005108147859573 = 0.0022755831014364958 + 0.01 * 6.777549743652344
Epoch 1080, val loss: 1.5580726861953735
Epoch 1090, training loss: 0.07007794827222824 = 0.0022338887210935354 + 0.01 * 6.7844061851501465
Epoch 1090, val loss: 1.5604655742645264
Epoch 1100, training loss: 0.07014718651771545 = 0.0021938637364655733 + 0.01 * 6.795332908630371
Epoch 1100, val loss: 1.562943935394287
Epoch 1110, training loss: 0.06996683776378632 = 0.00215516472235322 + 0.01 * 6.781167507171631
Epoch 1110, val loss: 1.5652613639831543
Epoch 1120, training loss: 0.0698082447052002 = 0.0021179725881665945 + 0.01 * 6.7690277099609375
Epoch 1120, val loss: 1.567615270614624
Epoch 1130, training loss: 0.06964021921157837 = 0.0020820496138185263 + 0.01 * 6.755817413330078
Epoch 1130, val loss: 1.5698403120040894
Epoch 1140, training loss: 0.06996437162160873 = 0.0020472314208745956 + 0.01 * 6.791714191436768
Epoch 1140, val loss: 1.5720659494400024
Epoch 1150, training loss: 0.06966716051101685 = 0.0020138565450906754 + 0.01 * 6.765330791473389
Epoch 1150, val loss: 1.5742650032043457
Epoch 1160, training loss: 0.06952241063117981 = 0.0019815941341221333 + 0.01 * 6.754082202911377
Epoch 1160, val loss: 1.5763862133026123
Epoch 1170, training loss: 0.0694861188530922 = 0.0019504318479448557 + 0.01 * 6.753568649291992
Epoch 1170, val loss: 1.5784120559692383
Epoch 1180, training loss: 0.06959109008312225 = 0.0019201156683266163 + 0.01 * 6.767097473144531
Epoch 1180, val loss: 1.5804978609085083
Epoch 1190, training loss: 0.06938912719488144 = 0.001891093677841127 + 0.01 * 6.74980354309082
Epoch 1190, val loss: 1.5823578834533691
Epoch 1200, training loss: 0.06956673413515091 = 0.0018627665704116225 + 0.01 * 6.770397186279297
Epoch 1200, val loss: 1.5843554735183716
Epoch 1210, training loss: 0.0692809596657753 = 0.0018354495987296104 + 0.01 * 6.744550704956055
Epoch 1210, val loss: 1.5862091779708862
Epoch 1220, training loss: 0.06926921010017395 = 0.0018089114455506206 + 0.01 * 6.746030330657959
Epoch 1220, val loss: 1.588019609451294
Epoch 1230, training loss: 0.06935746967792511 = 0.001783372019417584 + 0.01 * 6.757410049438477
Epoch 1230, val loss: 1.5899337530136108
Epoch 1240, training loss: 0.06910720467567444 = 0.001758525031618774 + 0.01 * 6.734868049621582
Epoch 1240, val loss: 1.5916779041290283
Epoch 1250, training loss: 0.0692843347787857 = 0.0017343537183478475 + 0.01 * 6.754998207092285
Epoch 1250, val loss: 1.5934391021728516
Epoch 1260, training loss: 0.06901660561561584 = 0.0017111164052039385 + 0.01 * 6.730549335479736
Epoch 1260, val loss: 1.595229983329773
Epoch 1270, training loss: 0.06892310827970505 = 0.001688399352133274 + 0.01 * 6.723471164703369
Epoch 1270, val loss: 1.5967888832092285
Epoch 1280, training loss: 0.06915119290351868 = 0.0016666640294715762 + 0.01 * 6.748452663421631
Epoch 1280, val loss: 1.5986511707305908
Epoch 1290, training loss: 0.06893935799598694 = 0.0016452772542834282 + 0.01 * 6.729408264160156
Epoch 1290, val loss: 1.6000304222106934
Epoch 1300, training loss: 0.06894604861736298 = 0.00162472203373909 + 0.01 * 6.732133388519287
Epoch 1300, val loss: 1.6016770601272583
Epoch 1310, training loss: 0.06874745339155197 = 0.0016047428362071514 + 0.01 * 6.714271068572998
Epoch 1310, val loss: 1.6031092405319214
Epoch 1320, training loss: 0.06895332783460617 = 0.0015852240612730384 + 0.01 * 6.736810684204102
Epoch 1320, val loss: 1.6047075986862183
Epoch 1330, training loss: 0.06871544569730759 = 0.001566416583955288 + 0.01 * 6.714902877807617
Epoch 1330, val loss: 1.6061033010482788
Epoch 1340, training loss: 0.06876759976148605 = 0.0015480185393244028 + 0.01 * 6.721958160400391
Epoch 1340, val loss: 1.6075034141540527
Epoch 1350, training loss: 0.06869585067033768 = 0.001530192093923688 + 0.01 * 6.71656608581543
Epoch 1350, val loss: 1.6088961362838745
Epoch 1360, training loss: 0.06872821599245071 = 0.0015127439983189106 + 0.01 * 6.721547603607178
Epoch 1360, val loss: 1.610253930091858
Epoch 1370, training loss: 0.06864985078573227 = 0.0014958709944039583 + 0.01 * 6.715398788452148
Epoch 1370, val loss: 1.6116968393325806
Epoch 1380, training loss: 0.06847868859767914 = 0.0014794259332120419 + 0.01 * 6.699926376342773
Epoch 1380, val loss: 1.6129506826400757
Epoch 1390, training loss: 0.06870750337839127 = 0.001463346416130662 + 0.01 * 6.7244157791137695
Epoch 1390, val loss: 1.6143064498901367
Epoch 1400, training loss: 0.06851264089345932 = 0.0014478268567472696 + 0.01 * 6.706481456756592
Epoch 1400, val loss: 1.615583896636963
Epoch 1410, training loss: 0.06854046136140823 = 0.0014326367527246475 + 0.01 * 6.710782527923584
Epoch 1410, val loss: 1.6168632507324219
Epoch 1420, training loss: 0.0684468001127243 = 0.001417803461663425 + 0.01 * 6.702900409698486
Epoch 1420, val loss: 1.6179814338684082
Epoch 1430, training loss: 0.06869664043188095 = 0.001403453410603106 + 0.01 * 6.729319095611572
Epoch 1430, val loss: 1.6192729473114014
Epoch 1440, training loss: 0.06832205504179001 = 0.0013894594740122557 + 0.01 * 6.6932597160339355
Epoch 1440, val loss: 1.6203663349151611
Epoch 1450, training loss: 0.06845905631780624 = 0.0013757774140685797 + 0.01 * 6.708327770233154
Epoch 1450, val loss: 1.6214734315872192
Epoch 1460, training loss: 0.06824593991041183 = 0.0013624622952193022 + 0.01 * 6.688347816467285
Epoch 1460, val loss: 1.6225528717041016
Epoch 1470, training loss: 0.06827820837497711 = 0.0013494439190253615 + 0.01 * 6.692876815795898
Epoch 1470, val loss: 1.6236042976379395
Epoch 1480, training loss: 0.06807845085859299 = 0.0013368496438488364 + 0.01 * 6.674160480499268
Epoch 1480, val loss: 1.6247429847717285
Epoch 1490, training loss: 0.06819059699773788 = 0.0013244623551145196 + 0.01 * 6.686614036560059
Epoch 1490, val loss: 1.6257598400115967
Epoch 1500, training loss: 0.06818611174821854 = 0.0013125489931553602 + 0.01 * 6.687356948852539
Epoch 1500, val loss: 1.6267926692962646
Epoch 1510, training loss: 0.06829193979501724 = 0.0013007252709940076 + 0.01 * 6.699121952056885
Epoch 1510, val loss: 1.6277117729187012
Epoch 1520, training loss: 0.06802677363157272 = 0.0012892433442175388 + 0.01 * 6.67375373840332
Epoch 1520, val loss: 1.628717064857483
Epoch 1530, training loss: 0.06828378885984421 = 0.001277989475056529 + 0.01 * 6.700579643249512
Epoch 1530, val loss: 1.6296770572662354
Epoch 1540, training loss: 0.0680442824959755 = 0.0012671397998929024 + 0.01 * 6.677713871002197
Epoch 1540, val loss: 1.6306357383728027
Epoch 1550, training loss: 0.06801402568817139 = 0.0012563770869746804 + 0.01 * 6.675765037536621
Epoch 1550, val loss: 1.6313570737838745
Epoch 1560, training loss: 0.06796501576900482 = 0.0012459821300581098 + 0.01 * 6.671903133392334
Epoch 1560, val loss: 1.6323775053024292
Epoch 1570, training loss: 0.06800352036952972 = 0.001235715113580227 + 0.01 * 6.676780700683594
Epoch 1570, val loss: 1.633129358291626
Epoch 1580, training loss: 0.06822039186954498 = 0.0012257382040843368 + 0.01 * 6.699465274810791
Epoch 1580, val loss: 1.6339902877807617
Epoch 1590, training loss: 0.06784740835428238 = 0.0012159188045188785 + 0.01 * 6.663149356842041
Epoch 1590, val loss: 1.6347522735595703
Epoch 1600, training loss: 0.06801455467939377 = 0.0012063671601936221 + 0.01 * 6.680819034576416
Epoch 1600, val loss: 1.6355564594268799
Epoch 1610, training loss: 0.06785739958286285 = 0.0011970206396654248 + 0.01 * 6.666038513183594
Epoch 1610, val loss: 1.6362313032150269
Epoch 1620, training loss: 0.06774180382490158 = 0.0011878714431077242 + 0.01 * 6.655393600463867
Epoch 1620, val loss: 1.6370097398757935
Epoch 1630, training loss: 0.06803996860980988 = 0.0011788976844400167 + 0.01 * 6.686107158660889
Epoch 1630, val loss: 1.6377451419830322
Epoch 1640, training loss: 0.06774168461561203 = 0.0011701697949320078 + 0.01 * 6.657151222229004
Epoch 1640, val loss: 1.638435959815979
Epoch 1650, training loss: 0.0677201971411705 = 0.0011616019764915109 + 0.01 * 6.655858993530273
Epoch 1650, val loss: 1.6391663551330566
Epoch 1660, training loss: 0.06788180768489838 = 0.0011531702475622296 + 0.01 * 6.672863483428955
Epoch 1660, val loss: 1.6397382020950317
Epoch 1670, training loss: 0.06777460128068924 = 0.0011449364246800542 + 0.01 * 6.662966728210449
Epoch 1670, val loss: 1.640516996383667
Epoch 1680, training loss: 0.06786682456731796 = 0.0011368542909622192 + 0.01 * 6.672996997833252
Epoch 1680, val loss: 1.6411586999893188
Epoch 1690, training loss: 0.06780664622783661 = 0.0011288943933323026 + 0.01 * 6.6677751541137695
Epoch 1690, val loss: 1.6417360305786133
Epoch 1700, training loss: 0.06759960204362869 = 0.001121179899200797 + 0.01 * 6.6478424072265625
Epoch 1700, val loss: 1.6423789262771606
Epoch 1710, training loss: 0.06757067888975143 = 0.0011135345557704568 + 0.01 * 6.64571475982666
Epoch 1710, val loss: 1.6430968046188354
Epoch 1720, training loss: 0.06767022609710693 = 0.0011060840915888548 + 0.01 * 6.65641450881958
Epoch 1720, val loss: 1.6436110734939575
Epoch 1730, training loss: 0.06746724247932434 = 0.0010987627319991589 + 0.01 * 6.636848449707031
Epoch 1730, val loss: 1.6441946029663086
Epoch 1740, training loss: 0.0678652673959732 = 0.0010915438178926706 + 0.01 * 6.677371978759766
Epoch 1740, val loss: 1.6446717977523804
Epoch 1750, training loss: 0.06763333827257156 = 0.0010844991775229573 + 0.01 * 6.654884338378906
Epoch 1750, val loss: 1.6453897953033447
Epoch 1760, training loss: 0.06776334345340729 = 0.0010775678092613816 + 0.01 * 6.668577194213867
Epoch 1760, val loss: 1.6458330154418945
Epoch 1770, training loss: 0.06759396940469742 = 0.0010708436602726579 + 0.01 * 6.652313232421875
Epoch 1770, val loss: 1.6464810371398926
Epoch 1780, training loss: 0.06749583780765533 = 0.0010641603730618954 + 0.01 * 6.643167972564697
Epoch 1780, val loss: 1.6468877792358398
Epoch 1790, training loss: 0.06765575706958771 = 0.001057606190443039 + 0.01 * 6.659814834594727
Epoch 1790, val loss: 1.6474157571792603
Epoch 1800, training loss: 0.06753167510032654 = 0.0010511643486097455 + 0.01 * 6.648050785064697
Epoch 1800, val loss: 1.6479209661483765
Epoch 1810, training loss: 0.06737574934959412 = 0.0010448212269693613 + 0.01 * 6.633092403411865
Epoch 1810, val loss: 1.648488998413086
Epoch 1820, training loss: 0.06740143150091171 = 0.0010385937057435513 + 0.01 * 6.636283874511719
Epoch 1820, val loss: 1.6488577127456665
Epoch 1830, training loss: 0.06749976426362991 = 0.0010325066978111863 + 0.01 * 6.646725654602051
Epoch 1830, val loss: 1.649338960647583
Epoch 1840, training loss: 0.06721694022417068 = 0.0010264961747452617 + 0.01 * 6.619044780731201
Epoch 1840, val loss: 1.6498106718063354
Epoch 1850, training loss: 0.0673426166176796 = 0.0010205808794125915 + 0.01 * 6.632203578948975
Epoch 1850, val loss: 1.6502506732940674
Epoch 1860, training loss: 0.06740368157625198 = 0.0010147864231839776 + 0.01 * 6.638889789581299
Epoch 1860, val loss: 1.650619626045227
Epoch 1870, training loss: 0.06746076792478561 = 0.0010090533178299665 + 0.01 * 6.645171642303467
Epoch 1870, val loss: 1.6511003971099854
Epoch 1880, training loss: 0.06748097389936447 = 0.0010034226579591632 + 0.01 * 6.647755146026611
Epoch 1880, val loss: 1.6515543460845947
Epoch 1890, training loss: 0.06757287681102753 = 0.000997863127849996 + 0.01 * 6.657501220703125
Epoch 1890, val loss: 1.6520018577575684
Epoch 1900, training loss: 0.06717060506343842 = 0.0009924715850502253 + 0.01 * 6.617813587188721
Epoch 1900, val loss: 1.6524988412857056
Epoch 1910, training loss: 0.06719006597995758 = 0.0009871174115687609 + 0.01 * 6.620294570922852
Epoch 1910, val loss: 1.652848243713379
Epoch 1920, training loss: 0.06724222749471664 = 0.000981803284958005 + 0.01 * 6.62604284286499
Epoch 1920, val loss: 1.6533300876617432
Epoch 1930, training loss: 0.06738059222698212 = 0.0009765902068465948 + 0.01 * 6.640399932861328
Epoch 1930, val loss: 1.6537554264068604
Epoch 1940, training loss: 0.06720517575740814 = 0.0009715278283692896 + 0.01 * 6.6233649253845215
Epoch 1940, val loss: 1.6542197465896606
Epoch 1950, training loss: 0.06750062108039856 = 0.0009664990939199924 + 0.01 * 6.653412342071533
Epoch 1950, val loss: 1.6545301675796509
Epoch 1960, training loss: 0.06704913824796677 = 0.0009615320013836026 + 0.01 * 6.608760356903076
Epoch 1960, val loss: 1.6550599336624146
Epoch 1970, training loss: 0.06714099645614624 = 0.0009566146763972938 + 0.01 * 6.618437767028809
Epoch 1970, val loss: 1.6553955078125
Epoch 1980, training loss: 0.06697160005569458 = 0.0009518143488094211 + 0.01 * 6.6019792556762695
Epoch 1980, val loss: 1.6558740139007568
Epoch 1990, training loss: 0.06719009578227997 = 0.0009470306686125696 + 0.01 * 6.624306678771973
Epoch 1990, val loss: 1.6561555862426758
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8202424881391671
=== training gcn model ===
Epoch 0, training loss: 2.042349100112915 = 1.9563807249069214 + 0.01 * 8.596832275390625
Epoch 0, val loss: 1.956782579421997
Epoch 10, training loss: 2.0316319465637207 = 1.9456642866134644 + 0.01 * 8.596763610839844
Epoch 10, val loss: 1.9463824033737183
Epoch 20, training loss: 2.0184390544891357 = 1.932474136352539 + 0.01 * 8.59649658203125
Epoch 20, val loss: 1.9330860376358032
Epoch 30, training loss: 1.9998995065689087 = 1.9139442443847656 + 0.01 * 8.595525741577148
Epoch 30, val loss: 1.9140290021896362
Epoch 40, training loss: 1.9728175401687622 = 1.8869198560714722 + 0.01 * 8.589764595031738
Epoch 40, val loss: 1.8863269090652466
Epoch 50, training loss: 1.9355812072753906 = 1.8500443696975708 + 0.01 * 8.553688049316406
Epoch 50, val loss: 1.8500256538391113
Epoch 60, training loss: 1.8946999311447144 = 1.8108973503112793 + 0.01 * 8.380260467529297
Epoch 60, val loss: 1.8156335353851318
Epoch 70, training loss: 1.862576961517334 = 1.7810477018356323 + 0.01 * 8.152929306030273
Epoch 70, val loss: 1.792834997177124
Epoch 80, training loss: 1.826035499572754 = 1.7467342615127563 + 0.01 * 7.9301252365112305
Epoch 80, val loss: 1.763620138168335
Epoch 90, training loss: 1.7756807804107666 = 1.6993253231048584 + 0.01 * 7.635547161102295
Epoch 90, val loss: 1.7223807573318481
Epoch 100, training loss: 1.7075846195220947 = 1.6334989070892334 + 0.01 * 7.408568859100342
Epoch 100, val loss: 1.666739821434021
Epoch 110, training loss: 1.6210569143295288 = 1.547850489616394 + 0.01 * 7.320638179779053
Epoch 110, val loss: 1.5956345796585083
Epoch 120, training loss: 1.5232301950454712 = 1.4503124952316284 + 0.01 * 7.291772842407227
Epoch 120, val loss: 1.5171042680740356
Epoch 130, training loss: 1.4222103357315063 = 1.3494304418563843 + 0.01 * 7.27799129486084
Epoch 130, val loss: 1.438372254371643
Epoch 140, training loss: 1.3209466934204102 = 1.2482948303222656 + 0.01 * 7.265192031860352
Epoch 140, val loss: 1.3604865074157715
Epoch 150, training loss: 1.2203971147537231 = 1.1478418111801147 + 0.01 * 7.255536079406738
Epoch 150, val loss: 1.2839027643203735
Epoch 160, training loss: 1.1236834526062012 = 1.0511927604675293 + 0.01 * 7.249070167541504
Epoch 160, val loss: 1.2112995386123657
Epoch 170, training loss: 1.0334134101867676 = 0.9609516859054565 + 0.01 * 7.246173858642578
Epoch 170, val loss: 1.1453458070755005
Epoch 180, training loss: 0.9495481252670288 = 0.877099335193634 + 0.01 * 7.244879722595215
Epoch 180, val loss: 1.0859930515289307
Epoch 190, training loss: 0.8706929683685303 = 0.7982537150382996 + 0.01 * 7.243924140930176
Epoch 190, val loss: 1.0309083461761475
Epoch 200, training loss: 0.7967069149017334 = 0.7242804169654846 + 0.01 * 7.242647647857666
Epoch 200, val loss: 0.9794411063194275
Epoch 210, training loss: 0.728568971157074 = 0.6561619639396667 + 0.01 * 7.2407002449035645
Epoch 210, val loss: 0.9328552484512329
Epoch 220, training loss: 0.6668883562088013 = 0.5945090055465698 + 0.01 * 7.237934112548828
Epoch 220, val loss: 0.8926799297332764
Epoch 230, training loss: 0.611282467842102 = 0.5389376878738403 + 0.01 * 7.2344770431518555
Epoch 230, val loss: 0.8596704006195068
Epoch 240, training loss: 0.5606865882873535 = 0.48837748169898987 + 0.01 * 7.230912208557129
Epoch 240, val loss: 0.8334807753562927
Epoch 250, training loss: 0.5137160420417786 = 0.44145068526268005 + 0.01 * 7.226537704467773
Epoch 250, val loss: 0.8129470348358154
Epoch 260, training loss: 0.46901050209999084 = 0.3967922329902649 + 0.01 * 7.221827983856201
Epoch 260, val loss: 0.7968235015869141
Epoch 270, training loss: 0.4256473481655121 = 0.35349035263061523 + 0.01 * 7.215699672698975
Epoch 270, val loss: 0.7842518091201782
Epoch 280, training loss: 0.3835528492927551 = 0.31144705414772034 + 0.01 * 7.2105793952941895
Epoch 280, val loss: 0.7752203941345215
Epoch 290, training loss: 0.34353160858154297 = 0.27152717113494873 + 0.01 * 7.200443267822266
Epoch 290, val loss: 0.7704295516014099
Epoch 300, training loss: 0.3070093095302582 = 0.2350417822599411 + 0.01 * 7.196752548217773
Epoch 300, val loss: 0.7703705430030823
Epoch 310, training loss: 0.274696946144104 = 0.20292741060256958 + 0.01 * 7.1769537925720215
Epoch 310, val loss: 0.7750238180160522
Epoch 320, training loss: 0.24701708555221558 = 0.1753564476966858 + 0.01 * 7.166064739227295
Epoch 320, val loss: 0.7839372158050537
Epoch 330, training loss: 0.22333788871765137 = 0.151930570602417 + 0.01 * 7.1407318115234375
Epoch 330, val loss: 0.7963388562202454
Epoch 340, training loss: 0.20353442430496216 = 0.13206854462623596 + 0.01 * 7.1465888023376465
Epoch 340, val loss: 0.8114430904388428
Epoch 350, training loss: 0.18629767000675201 = 0.11521219462156296 + 0.01 * 7.108547687530518
Epoch 350, val loss: 0.8284540176391602
Epoch 360, training loss: 0.17191484570503235 = 0.10085289180278778 + 0.01 * 7.106195449829102
Epoch 360, val loss: 0.8469154834747314
Epoch 370, training loss: 0.15931814908981323 = 0.08862190693616867 + 0.01 * 7.0696234703063965
Epoch 370, val loss: 0.8662555813789368
Epoch 380, training loss: 0.1487807333469391 = 0.07818598300218582 + 0.01 * 7.059475898742676
Epoch 380, val loss: 0.8862496018409729
Epoch 390, training loss: 0.14000871777534485 = 0.06927761435508728 + 0.01 * 7.073110580444336
Epoch 390, val loss: 0.9065166115760803
Epoch 400, training loss: 0.13201916217803955 = 0.061654865741729736 + 0.01 * 7.036428928375244
Epoch 400, val loss: 0.9268876910209656
Epoch 410, training loss: 0.12544573843479156 = 0.055100686848163605 + 0.01 * 7.034505367279053
Epoch 410, val loss: 0.94718337059021
Epoch 420, training loss: 0.11964080482721329 = 0.049458108842372894 + 0.01 * 7.0182695388793945
Epoch 420, val loss: 0.9672471880912781
Epoch 430, training loss: 0.11470776796340942 = 0.04458089545369148 + 0.01 * 7.012686729431152
Epoch 430, val loss: 0.9871005415916443
Epoch 440, training loss: 0.11033545434474945 = 0.04033976420760155 + 0.01 * 6.999569892883301
Epoch 440, val loss: 1.0065544843673706
Epoch 450, training loss: 0.10655608773231506 = 0.03663923591375351 + 0.01 * 6.991685390472412
Epoch 450, val loss: 1.0256246328353882
Epoch 460, training loss: 0.10335192084312439 = 0.033408354967832565 + 0.01 * 6.994356155395508
Epoch 460, val loss: 1.044220209121704
Epoch 470, training loss: 0.10032478719949722 = 0.03056854009628296 + 0.01 * 6.9756245613098145
Epoch 470, val loss: 1.062334418296814
Epoch 480, training loss: 0.09774161130189896 = 0.028060071170330048 + 0.01 * 6.968153953552246
Epoch 480, val loss: 1.0799897909164429
Epoch 490, training loss: 0.09558993577957153 = 0.025835158303380013 + 0.01 * 6.975478172302246
Epoch 490, val loss: 1.0971589088439941
Epoch 500, training loss: 0.093526691198349 = 0.023861464112997055 + 0.01 * 6.966522693634033
Epoch 500, val loss: 1.1138536930084229
Epoch 510, training loss: 0.09165345132350922 = 0.02210155874490738 + 0.01 * 6.955189228057861
Epoch 510, val loss: 1.1300022602081299
Epoch 520, training loss: 0.09005418419837952 = 0.020529400557279587 + 0.01 * 6.952478408813477
Epoch 520, val loss: 1.145645022392273
Epoch 530, training loss: 0.08844005316495895 = 0.019117427989840508 + 0.01 * 6.932262420654297
Epoch 530, val loss: 1.1608867645263672
Epoch 540, training loss: 0.08723176270723343 = 0.01784578710794449 + 0.01 * 6.938597679138184
Epoch 540, val loss: 1.1757125854492188
Epoch 550, training loss: 0.08587749302387238 = 0.016699085012078285 + 0.01 * 6.917840957641602
Epoch 550, val loss: 1.1899958848953247
Epoch 560, training loss: 0.08491716533899307 = 0.015660030767321587 + 0.01 * 6.925713539123535
Epoch 560, val loss: 1.2039307355880737
Epoch 570, training loss: 0.08374539017677307 = 0.01471780240535736 + 0.01 * 6.902759075164795
Epoch 570, val loss: 1.2173582315444946
Epoch 580, training loss: 0.08289249241352081 = 0.013861121609807014 + 0.01 * 6.90313720703125
Epoch 580, val loss: 1.2304354906082153
Epoch 590, training loss: 0.08232122659683228 = 0.013078921474516392 + 0.01 * 6.924230575561523
Epoch 590, val loss: 1.2431262731552124
Epoch 600, training loss: 0.08120027929544449 = 0.012364487163722515 + 0.01 * 6.883579730987549
Epoch 600, val loss: 1.2553759813308716
Epoch 610, training loss: 0.08058463037014008 = 0.011709139682352543 + 0.01 * 6.88754940032959
Epoch 610, val loss: 1.2672935724258423
Epoch 620, training loss: 0.08001960068941116 = 0.011107705533504486 + 0.01 * 6.8911895751953125
Epoch 620, val loss: 1.2789154052734375
Epoch 630, training loss: 0.07922328263521194 = 0.010554182343184948 + 0.01 * 6.866910457611084
Epoch 630, val loss: 1.2901121377944946
Epoch 640, training loss: 0.07880403101444244 = 0.010043212212622166 + 0.01 * 6.876082420349121
Epoch 640, val loss: 1.3010165691375732
Epoch 650, training loss: 0.07824509590864182 = 0.009571696631610394 + 0.01 * 6.867340087890625
Epoch 650, val loss: 1.311630129814148
Epoch 660, training loss: 0.07793280482292175 = 0.00913497805595398 + 0.01 * 6.8797831535339355
Epoch 660, val loss: 1.3219115734100342
Epoch 670, training loss: 0.07720736414194107 = 0.008729926310479641 + 0.01 * 6.847743988037109
Epoch 670, val loss: 1.331984043121338
Epoch 680, training loss: 0.07723072171211243 = 0.008353781886398792 + 0.01 * 6.887693881988525
Epoch 680, val loss: 1.3416818380355835
Epoch 690, training loss: 0.0763544887304306 = 0.008004129864275455 + 0.01 * 6.835035800933838
Epoch 690, val loss: 1.35115385055542
Epoch 700, training loss: 0.07586740702390671 = 0.0076781888492405415 + 0.01 * 6.81892204284668
Epoch 700, val loss: 1.3603274822235107
Epoch 710, training loss: 0.0758056491613388 = 0.007373551372438669 + 0.01 * 6.843210220336914
Epoch 710, val loss: 1.3692688941955566
Epoch 720, training loss: 0.07516191154718399 = 0.007088914513587952 + 0.01 * 6.807299613952637
Epoch 720, val loss: 1.3779979944229126
Epoch 730, training loss: 0.07527153939008713 = 0.00682221120223403 + 0.01 * 6.844932556152344
Epoch 730, val loss: 1.3865221738815308
Epoch 740, training loss: 0.07476308196783066 = 0.006572534795850515 + 0.01 * 6.81905460357666
Epoch 740, val loss: 1.3947944641113281
Epoch 750, training loss: 0.07436428219079971 = 0.006337630096822977 + 0.01 * 6.802665710449219
Epoch 750, val loss: 1.4029245376586914
Epoch 760, training loss: 0.07433663308620453 = 0.006116283591836691 + 0.01 * 6.82203483581543
Epoch 760, val loss: 1.4107602834701538
Epoch 770, training loss: 0.07384247332811356 = 0.005908206105232239 + 0.01 * 6.793426513671875
Epoch 770, val loss: 1.4184635877609253
Epoch 780, training loss: 0.07378014922142029 = 0.005711630918085575 + 0.01 * 6.806852340698242
Epoch 780, val loss: 1.4259580373764038
Epoch 790, training loss: 0.07354860007762909 = 0.005526699125766754 + 0.01 * 6.80219030380249
Epoch 790, val loss: 1.433332085609436
Epoch 800, training loss: 0.07313846051692963 = 0.005352032370865345 + 0.01 * 6.778642654418945
Epoch 800, val loss: 1.4404178857803345
Epoch 810, training loss: 0.072966068983078 = 0.0051865666173398495 + 0.01 * 6.777950763702393
Epoch 810, val loss: 1.4474008083343506
Epoch 820, training loss: 0.07262343168258667 = 0.005029637832194567 + 0.01 * 6.759378910064697
Epoch 820, val loss: 1.4542757272720337
Epoch 830, training loss: 0.07284634560346603 = 0.004880945663899183 + 0.01 * 6.796540260314941
Epoch 830, val loss: 1.4609564542770386
Epoch 840, training loss: 0.07222092896699905 = 0.00474033085629344 + 0.01 * 6.74806022644043
Epoch 840, val loss: 1.467422366142273
Epoch 850, training loss: 0.07204937934875488 = 0.00460639176890254 + 0.01 * 6.744298934936523
Epoch 850, val loss: 1.4738301038742065
Epoch 860, training loss: 0.07188206166028976 = 0.004478903952986002 + 0.01 * 6.740316390991211
Epoch 860, val loss: 1.480061650276184
Epoch 870, training loss: 0.07176481932401657 = 0.004357978701591492 + 0.01 * 6.740684509277344
Epoch 870, val loss: 1.4861575365066528
Epoch 880, training loss: 0.07173573970794678 = 0.004242624156177044 + 0.01 * 6.749311923980713
Epoch 880, val loss: 1.492202639579773
Epoch 890, training loss: 0.07147348672151566 = 0.0041326964274048805 + 0.01 * 6.734079360961914
Epoch 890, val loss: 1.4979804754257202
Epoch 900, training loss: 0.07150231301784515 = 0.00402796221897006 + 0.01 * 6.747435092926025
Epoch 900, val loss: 1.5037412643432617
Epoch 910, training loss: 0.07111472636461258 = 0.0039280494675040245 + 0.01 * 6.718667507171631
Epoch 910, val loss: 1.5093578100204468
Epoch 920, training loss: 0.07099559158086777 = 0.003832268761470914 + 0.01 * 6.716332912445068
Epoch 920, val loss: 1.5149078369140625
Epoch 930, training loss: 0.07077954709529877 = 0.0037410357035696507 + 0.01 * 6.703851699829102
Epoch 930, val loss: 1.520264983177185
Epoch 940, training loss: 0.07061727344989777 = 0.0036533139646053314 + 0.01 * 6.696396350860596
Epoch 940, val loss: 1.5256494283676147
Epoch 950, training loss: 0.0707135796546936 = 0.003569371532648802 + 0.01 * 6.714420795440674
Epoch 950, val loss: 1.5307374000549316
Epoch 960, training loss: 0.07050047069787979 = 0.003489327384158969 + 0.01 * 6.701114177703857
Epoch 960, val loss: 1.5358935594558716
Epoch 970, training loss: 0.07030075043439865 = 0.0034125237725675106 + 0.01 * 6.6888227462768555
Epoch 970, val loss: 1.5408852100372314
Epoch 980, training loss: 0.07022066414356232 = 0.003338450798764825 + 0.01 * 6.6882219314575195
Epoch 980, val loss: 1.5457735061645508
Epoch 990, training loss: 0.07003281265497208 = 0.0032679815776646137 + 0.01 * 6.676483631134033
Epoch 990, val loss: 1.550559163093567
Epoch 1000, training loss: 0.07008422911167145 = 0.0031996944453567266 + 0.01 * 6.688453197479248
Epoch 1000, val loss: 1.5553879737854004
Epoch 1010, training loss: 0.06990037858486176 = 0.0031343651935458183 + 0.01 * 6.676601886749268
Epoch 1010, val loss: 1.5599170923233032
Epoch 1020, training loss: 0.07004596292972565 = 0.0030716254841536283 + 0.01 * 6.6974334716796875
Epoch 1020, val loss: 1.5645098686218262
Epoch 1030, training loss: 0.0696888118982315 = 0.0030112802051007748 + 0.01 * 6.66775369644165
Epoch 1030, val loss: 1.568929672241211
Epoch 1040, training loss: 0.06957913190126419 = 0.002953127259388566 + 0.01 * 6.662600517272949
Epoch 1040, val loss: 1.5732911825180054
Epoch 1050, training loss: 0.0695929154753685 = 0.002896996447816491 + 0.01 * 6.669591903686523
Epoch 1050, val loss: 1.5775907039642334
Epoch 1060, training loss: 0.06972000002861023 = 0.002842928050085902 + 0.01 * 6.687707424163818
Epoch 1060, val loss: 1.5818194150924683
Epoch 1070, training loss: 0.06939571350812912 = 0.002790900645777583 + 0.01 * 6.660481929779053
Epoch 1070, val loss: 1.5859546661376953
Epoch 1080, training loss: 0.0694202184677124 = 0.0027405356522649527 + 0.01 * 6.66796875
Epoch 1080, val loss: 1.5900499820709229
Epoch 1090, training loss: 0.06919459253549576 = 0.002692214213311672 + 0.01 * 6.650237560272217
Epoch 1090, val loss: 1.5940366983413696
Epoch 1100, training loss: 0.06919603049755096 = 0.0026452396996319294 + 0.01 * 6.655079364776611
Epoch 1100, val loss: 1.5980257987976074
Epoch 1110, training loss: 0.06906677037477493 = 0.002600029343739152 + 0.01 * 6.646674156188965
Epoch 1110, val loss: 1.6019095182418823
Epoch 1120, training loss: 0.06911405175924301 = 0.002556215738877654 + 0.01 * 6.6557841300964355
Epoch 1120, val loss: 1.6057218313217163
Epoch 1130, training loss: 0.06900199502706528 = 0.002514394000172615 + 0.01 * 6.6487603187561035
Epoch 1130, val loss: 1.6093969345092773
Epoch 1140, training loss: 0.06888025254011154 = 0.002473431872203946 + 0.01 * 6.640682697296143
Epoch 1140, val loss: 1.6131192445755005
Epoch 1150, training loss: 0.06884065270423889 = 0.002434227615594864 + 0.01 * 6.640643119812012
Epoch 1150, val loss: 1.6166982650756836
Epoch 1160, training loss: 0.06895424425601959 = 0.0023960298858582973 + 0.01 * 6.655821323394775
Epoch 1160, val loss: 1.6202797889709473
Epoch 1170, training loss: 0.06882138550281525 = 0.002359043573960662 + 0.01 * 6.646234512329102
Epoch 1170, val loss: 1.623765230178833
Epoch 1180, training loss: 0.06866321712732315 = 0.002323367167264223 + 0.01 * 6.6339850425720215
Epoch 1180, val loss: 1.6271530389785767
Epoch 1190, training loss: 0.0687289834022522 = 0.00228888145647943 + 0.01 * 6.6440110206604
Epoch 1190, val loss: 1.6305770874023438
Epoch 1200, training loss: 0.0684932991862297 = 0.0022551175206899643 + 0.01 * 6.6238179206848145
Epoch 1200, val loss: 1.633927345275879
Epoch 1210, training loss: 0.06839648634195328 = 0.002222537063062191 + 0.01 * 6.617394924163818
Epoch 1210, val loss: 1.6372023820877075
Epoch 1220, training loss: 0.06868690252304077 = 0.0021911929361522198 + 0.01 * 6.649570941925049
Epoch 1220, val loss: 1.6404403448104858
Epoch 1230, training loss: 0.06833009421825409 = 0.0021605398505926132 + 0.01 * 6.616955757141113
Epoch 1230, val loss: 1.6436212062835693
Epoch 1240, training loss: 0.06827466934919357 = 0.0021308523137122393 + 0.01 * 6.614381790161133
Epoch 1240, val loss: 1.6467336416244507
Epoch 1250, training loss: 0.06831454485654831 = 0.00210225535556674 + 0.01 * 6.62122917175293
Epoch 1250, val loss: 1.6498059034347534
Epoch 1260, training loss: 0.06822127103805542 = 0.0020741845946758986 + 0.01 * 6.614709377288818
Epoch 1260, val loss: 1.652891755104065
Epoch 1270, training loss: 0.06813651323318481 = 0.002046965528279543 + 0.01 * 6.608954906463623
Epoch 1270, val loss: 1.6558834314346313
Epoch 1280, training loss: 0.06840889155864716 = 0.0020205501932650805 + 0.01 * 6.638833999633789
Epoch 1280, val loss: 1.6588541269302368
Epoch 1290, training loss: 0.06789663434028625 = 0.0019948494154959917 + 0.01 * 6.590178489685059
Epoch 1290, val loss: 1.661781907081604
Epoch 1300, training loss: 0.06807626783847809 = 0.0019699870608747005 + 0.01 * 6.610628604888916
Epoch 1300, val loss: 1.6647313833236694
Epoch 1310, training loss: 0.06799192726612091 = 0.0019456747686490417 + 0.01 * 6.604625701904297
Epoch 1310, val loss: 1.6674925088882446
Epoch 1320, training loss: 0.06796497851610184 = 0.0019221532857045531 + 0.01 * 6.604282855987549
Epoch 1320, val loss: 1.6703733205795288
Epoch 1330, training loss: 0.067949578166008 = 0.0018991678953170776 + 0.01 * 6.60504150390625
Epoch 1330, val loss: 1.6731330156326294
Epoch 1340, training loss: 0.06782571226358414 = 0.0018768343143165112 + 0.01 * 6.594888210296631
Epoch 1340, val loss: 1.67583429813385
Epoch 1350, training loss: 0.06782852858304977 = 0.0018552282126620412 + 0.01 * 6.597329616546631
Epoch 1350, val loss: 1.678573489189148
Epoch 1360, training loss: 0.06756620854139328 = 0.0018340146634727716 + 0.01 * 6.5732197761535645
Epoch 1360, val loss: 1.6812574863433838
Epoch 1370, training loss: 0.06774058938026428 = 0.0018132892437279224 + 0.01 * 6.5927300453186035
Epoch 1370, val loss: 1.6839059591293335
Epoch 1380, training loss: 0.06777503341436386 = 0.0017934186616912484 + 0.01 * 6.598161220550537
Epoch 1380, val loss: 1.6864345073699951
Epoch 1390, training loss: 0.06787438690662384 = 0.0017737469170242548 + 0.01 * 6.610064506530762
Epoch 1390, val loss: 1.6890596151351929
Epoch 1400, training loss: 0.06747358292341232 = 0.0017548352479934692 + 0.01 * 6.571875095367432
Epoch 1400, val loss: 1.6915138959884644
Epoch 1410, training loss: 0.06746743619441986 = 0.001736220670863986 + 0.01 * 6.573121547698975
Epoch 1410, val loss: 1.6940538883209229
Epoch 1420, training loss: 0.06754174828529358 = 0.0017180773429572582 + 0.01 * 6.582367420196533
Epoch 1420, val loss: 1.6964776515960693
Epoch 1430, training loss: 0.06755442172288895 = 0.0017004559049382806 + 0.01 * 6.585397243499756
Epoch 1430, val loss: 1.6989259719848633
Epoch 1440, training loss: 0.0675939992070198 = 0.0016832721885293722 + 0.01 * 6.5910725593566895
Epoch 1440, val loss: 1.7012661695480347
Epoch 1450, training loss: 0.06726144999265671 = 0.0016663993010297418 + 0.01 * 6.559505462646484
Epoch 1450, val loss: 1.7036731243133545
Epoch 1460, training loss: 0.06726007908582687 = 0.001650048652663827 + 0.01 * 6.561002731323242
Epoch 1460, val loss: 1.7059746980667114
Epoch 1470, training loss: 0.06755932420492172 = 0.0016339734429493546 + 0.01 * 6.592535495758057
Epoch 1470, val loss: 1.7082891464233398
Epoch 1480, training loss: 0.06717642396688461 = 0.001618355861864984 + 0.01 * 6.555806636810303
Epoch 1480, val loss: 1.7105658054351807
Epoch 1490, training loss: 0.06714047491550446 = 0.00160297064576298 + 0.01 * 6.553750991821289
Epoch 1490, val loss: 1.7127883434295654
Epoch 1500, training loss: 0.06712625175714493 = 0.001588105340488255 + 0.01 * 6.553814888000488
Epoch 1500, val loss: 1.7149990797042847
Epoch 1510, training loss: 0.06716302782297134 = 0.0015734218759462237 + 0.01 * 6.558960914611816
Epoch 1510, val loss: 1.7172166109085083
Epoch 1520, training loss: 0.06733965128660202 = 0.001559196854941547 + 0.01 * 6.57804536819458
Epoch 1520, val loss: 1.7193924188613892
Epoch 1530, training loss: 0.06722392141819 = 0.001545184408314526 + 0.01 * 6.567873954772949
Epoch 1530, val loss: 1.721453309059143
Epoch 1540, training loss: 0.06737618893384933 = 0.0015315612545236945 + 0.01 * 6.584462642669678
Epoch 1540, val loss: 1.723548412322998
Epoch 1550, training loss: 0.06706847250461578 = 0.001518344972282648 + 0.01 * 6.5550127029418945
Epoch 1550, val loss: 1.725691556930542
Epoch 1560, training loss: 0.0670894905924797 = 0.0015051597729325294 + 0.01 * 6.558433532714844
Epoch 1560, val loss: 1.7277483940124512
Epoch 1570, training loss: 0.06728960573673248 = 0.0014925244031473994 + 0.01 * 6.579708099365234
Epoch 1570, val loss: 1.7297136783599854
Epoch 1580, training loss: 0.066956527531147 = 0.00147997064050287 + 0.01 * 6.547656059265137
Epoch 1580, val loss: 1.7317670583724976
Epoch 1590, training loss: 0.06690403074026108 = 0.0014676684513688087 + 0.01 * 6.543636322021484
Epoch 1590, val loss: 1.733734130859375
Epoch 1600, training loss: 0.06695669144392014 = 0.0014557564863935113 + 0.01 * 6.550093650817871
Epoch 1600, val loss: 1.7357434034347534
Epoch 1610, training loss: 0.06708668172359467 = 0.0014440187951549888 + 0.01 * 6.564266681671143
Epoch 1610, val loss: 1.7376947402954102
Epoch 1620, training loss: 0.06682410091161728 = 0.0014324869262054563 + 0.01 * 6.539161682128906
Epoch 1620, val loss: 1.7395907640457153
Epoch 1630, training loss: 0.06703124195337296 = 0.0014212648384273052 + 0.01 * 6.560997486114502
Epoch 1630, val loss: 1.7415002584457397
Epoch 1640, training loss: 0.06678692996501923 = 0.0014101759297773242 + 0.01 * 6.537674903869629
Epoch 1640, val loss: 1.7433528900146484
Epoch 1650, training loss: 0.06689395010471344 = 0.0013993768952786922 + 0.01 * 6.549457550048828
Epoch 1650, val loss: 1.7452565431594849
Epoch 1660, training loss: 0.06684496998786926 = 0.0013887359527871013 + 0.01 * 6.545623302459717
Epoch 1660, val loss: 1.7471199035644531
Epoch 1670, training loss: 0.06681537628173828 = 0.001378364977426827 + 0.01 * 6.543701171875
Epoch 1670, val loss: 1.7488676309585571
Epoch 1680, training loss: 0.06678181886672974 = 0.0013682303251698613 + 0.01 * 6.541358947753906
Epoch 1680, val loss: 1.7506983280181885
Epoch 1690, training loss: 0.06679587066173553 = 0.0013581925304606557 + 0.01 * 6.543767929077148
Epoch 1690, val loss: 1.7524585723876953
Epoch 1700, training loss: 0.06675920635461807 = 0.001348427264019847 + 0.01 * 6.541077613830566
Epoch 1700, val loss: 1.7542424201965332
Epoch 1710, training loss: 0.06681158393621445 = 0.0013387835351750255 + 0.01 * 6.547280311584473
Epoch 1710, val loss: 1.7559396028518677
Epoch 1720, training loss: 0.06680761277675629 = 0.0013293765950948 + 0.01 * 6.547823905944824
Epoch 1720, val loss: 1.7576403617858887
Epoch 1730, training loss: 0.06645778566598892 = 0.0013200904941186309 + 0.01 * 6.513769626617432
Epoch 1730, val loss: 1.7593724727630615
Epoch 1740, training loss: 0.06653766334056854 = 0.0013110112631693482 + 0.01 * 6.522665500640869
Epoch 1740, val loss: 1.7610448598861694
Epoch 1750, training loss: 0.06655314564704895 = 0.0013020886108279228 + 0.01 * 6.525106430053711
Epoch 1750, val loss: 1.7626529932022095
Epoch 1760, training loss: 0.06668583303689957 = 0.0012932824902236462 + 0.01 * 6.539255619049072
Epoch 1760, val loss: 1.7643487453460693
Epoch 1770, training loss: 0.06655985862016678 = 0.001284690690226853 + 0.01 * 6.527517318725586
Epoch 1770, val loss: 1.765942096710205
Epoch 1780, training loss: 0.06643249839544296 = 0.001276262803003192 + 0.01 * 6.515623569488525
Epoch 1780, val loss: 1.7675213813781738
Epoch 1790, training loss: 0.06645381450653076 = 0.00126800243742764 + 0.01 * 6.518581390380859
Epoch 1790, val loss: 1.7691205739974976
Epoch 1800, training loss: 0.06652466207742691 = 0.0012598076136782765 + 0.01 * 6.526485443115234
Epoch 1800, val loss: 1.770662784576416
Epoch 1810, training loss: 0.06650175899267197 = 0.0012517516734078526 + 0.01 * 6.525001049041748
Epoch 1810, val loss: 1.7722117900848389
Epoch 1820, training loss: 0.06639504432678223 = 0.0012438632547855377 + 0.01 * 6.515118598937988
Epoch 1820, val loss: 1.7737027406692505
Epoch 1830, training loss: 0.06633783131837845 = 0.001236159703694284 + 0.01 * 6.510166645050049
Epoch 1830, val loss: 1.775245189666748
Epoch 1840, training loss: 0.06641561537981033 = 0.0012284684926271439 + 0.01 * 6.5187153816223145
Epoch 1840, val loss: 1.776767611503601
Epoch 1850, training loss: 0.06649436056613922 = 0.0012209386331960559 + 0.01 * 6.527342319488525
Epoch 1850, val loss: 1.7782293558120728
Epoch 1860, training loss: 0.06656856834888458 = 0.0012135409051552415 + 0.01 * 6.5355024337768555
Epoch 1860, val loss: 1.7796871662139893
Epoch 1870, training loss: 0.06646399199962616 = 0.0012063563335686922 + 0.01 * 6.525763988494873
Epoch 1870, val loss: 1.7811518907546997
Epoch 1880, training loss: 0.06645181775093079 = 0.0011991816572844982 + 0.01 * 6.525263786315918
Epoch 1880, val loss: 1.7825918197631836
Epoch 1890, training loss: 0.0663425400853157 = 0.0011921650730073452 + 0.01 * 6.5150370597839355
Epoch 1890, val loss: 1.7840662002563477
Epoch 1900, training loss: 0.06619657576084137 = 0.001185138477012515 + 0.01 * 6.501143932342529
Epoch 1900, val loss: 1.7855333089828491
Epoch 1910, training loss: 0.06637609004974365 = 0.0011783530935645103 + 0.01 * 6.519773960113525
Epoch 1910, val loss: 1.786970615386963
Epoch 1920, training loss: 0.06631778925657272 = 0.0011716879671439528 + 0.01 * 6.514610290527344
Epoch 1920, val loss: 1.7883621454238892
Epoch 1930, training loss: 0.06623318791389465 = 0.0011650491505861282 + 0.01 * 6.506814002990723
Epoch 1930, val loss: 1.7897708415985107
Epoch 1940, training loss: 0.06636778265237808 = 0.0011585899628698826 + 0.01 * 6.520919322967529
Epoch 1940, val loss: 1.7911655902862549
Epoch 1950, training loss: 0.06628536432981491 = 0.0011521533597260714 + 0.01 * 6.513321876525879
Epoch 1950, val loss: 1.7925968170166016
Epoch 1960, training loss: 0.06605583429336548 = 0.00114585948176682 + 0.01 * 6.490997314453125
Epoch 1960, val loss: 1.793954849243164
Epoch 1970, training loss: 0.06633645296096802 = 0.0011396027402952313 + 0.01 * 6.519684791564941
Epoch 1970, val loss: 1.795339822769165
Epoch 1980, training loss: 0.06625889986753464 = 0.001133476267568767 + 0.01 * 6.512542247772217
Epoch 1980, val loss: 1.7966488599777222
Epoch 1990, training loss: 0.06629621982574463 = 0.001127408118918538 + 0.01 * 6.516881465911865
Epoch 1990, val loss: 1.7980132102966309
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 2.026641607284546 = 1.9406733512878418 + 0.01 * 8.596822738647461
Epoch 0, val loss: 1.9333500862121582
Epoch 10, training loss: 2.0162670612335205 = 1.9302994012832642 + 0.01 * 8.59676456451416
Epoch 10, val loss: 1.9228180646896362
Epoch 20, training loss: 2.0037713050842285 = 1.9178062677383423 + 0.01 * 8.5964937210083
Epoch 20, val loss: 1.9102624654769897
Epoch 30, training loss: 1.9865976572036743 = 1.9006446599960327 + 0.01 * 8.595301628112793
Epoch 30, val loss: 1.8932468891143799
Epoch 40, training loss: 1.962071180343628 = 1.876198649406433 + 0.01 * 8.587249755859375
Epoch 40, val loss: 1.8694802522659302
Epoch 50, training loss: 1.9289911985397339 = 1.8435882329940796 + 0.01 * 8.54029655456543
Epoch 50, val loss: 1.839839220046997
Epoch 60, training loss: 1.893507480621338 = 1.809956669807434 + 0.01 * 8.355077743530273
Epoch 60, val loss: 1.8137387037277222
Epoch 70, training loss: 1.865779995918274 = 1.78368079662323 + 0.01 * 8.209924697875977
Epoch 70, val loss: 1.7953698635101318
Epoch 80, training loss: 1.8311257362365723 = 1.7502223253250122 + 0.01 * 8.090347290039062
Epoch 80, val loss: 1.7655023336410522
Epoch 90, training loss: 1.781812071800232 = 1.7025995254516602 + 0.01 * 7.921260356903076
Epoch 90, val loss: 1.7230188846588135
Epoch 100, training loss: 1.7140529155731201 = 1.6362121105194092 + 0.01 * 7.78408145904541
Epoch 100, val loss: 1.6666840314865112
Epoch 110, training loss: 1.6314862966537476 = 1.555040717124939 + 0.01 * 7.644559860229492
Epoch 110, val loss: 1.6010853052139282
Epoch 120, training loss: 1.5457723140716553 = 1.471021294593811 + 0.01 * 7.4751081466674805
Epoch 120, val loss: 1.5362112522125244
Epoch 130, training loss: 1.4649585485458374 = 1.391228437423706 + 0.01 * 7.373012065887451
Epoch 130, val loss: 1.475867748260498
Epoch 140, training loss: 1.3866404294967651 = 1.313531517982483 + 0.01 * 7.310896396636963
Epoch 140, val loss: 1.4173529148101807
Epoch 150, training loss: 1.3073265552520752 = 1.2344547510147095 + 0.01 * 7.2871856689453125
Epoch 150, val loss: 1.358399510383606
Epoch 160, training loss: 1.2254210710525513 = 1.1526654958724976 + 0.01 * 7.2755608558654785
Epoch 160, val loss: 1.2988219261169434
Epoch 170, training loss: 1.1407068967819214 = 1.067984700202942 + 0.01 * 7.272224426269531
Epoch 170, val loss: 1.2380528450012207
Epoch 180, training loss: 1.0538065433502197 = 0.9810888171195984 + 0.01 * 7.271778583526611
Epoch 180, val loss: 1.1747194528579712
Epoch 190, training loss: 0.9673667550086975 = 0.8946541547775269 + 0.01 * 7.271262168884277
Epoch 190, val loss: 1.110977292060852
Epoch 200, training loss: 0.8849872946739197 = 0.8122809529304504 + 0.01 * 7.270636081695557
Epoch 200, val loss: 1.0491464138031006
Epoch 210, training loss: 0.8094304800033569 = 0.7367343902587891 + 0.01 * 7.269611358642578
Epoch 210, val loss: 0.9929868578910828
Epoch 220, training loss: 0.7412523627281189 = 0.6685724854469299 + 0.01 * 7.267986297607422
Epoch 220, val loss: 0.9437934160232544
Epoch 230, training loss: 0.6795153021812439 = 0.6068556904792786 + 0.01 * 7.265958786010742
Epoch 230, val loss: 0.9023623466491699
Epoch 240, training loss: 0.6227821111679077 = 0.5501453280448914 + 0.01 * 7.263680458068848
Epoch 240, val loss: 0.8683560490608215
Epoch 250, training loss: 0.5701364874839783 = 0.49752914905548096 + 0.01 * 7.260735511779785
Epoch 250, val loss: 0.8415601849555969
Epoch 260, training loss: 0.5209673047065735 = 0.4484056830406189 + 0.01 * 7.256159782409668
Epoch 260, val loss: 0.8212012648582458
Epoch 270, training loss: 0.47485384345054626 = 0.4023691713809967 + 0.01 * 7.248466491699219
Epoch 270, val loss: 0.8063424825668335
Epoch 280, training loss: 0.43133544921875 = 0.3589818775653839 + 0.01 * 7.235355854034424
Epoch 280, val loss: 0.7956655025482178
Epoch 290, training loss: 0.3901790976524353 = 0.31792718172073364 + 0.01 * 7.225192070007324
Epoch 290, val loss: 0.7877118587493896
Epoch 300, training loss: 0.3511291742324829 = 0.2791089415550232 + 0.01 * 7.202023506164551
Epoch 300, val loss: 0.781559407711029
Epoch 310, training loss: 0.31458592414855957 = 0.2427775114774704 + 0.01 * 7.18084192276001
Epoch 310, val loss: 0.7771265506744385
Epoch 320, training loss: 0.2813131809234619 = 0.2094753235578537 + 0.01 * 7.183786869049072
Epoch 320, val loss: 0.7748739123344421
Epoch 330, training loss: 0.25141751766204834 = 0.1797753870487213 + 0.01 * 7.16421365737915
Epoch 330, val loss: 0.7751951217651367
Epoch 340, training loss: 0.2254735231399536 = 0.15399400889873505 + 0.01 * 7.147951126098633
Epoch 340, val loss: 0.778116762638092
Epoch 350, training loss: 0.20352593064308167 = 0.13211318850517273 + 0.01 * 7.141274929046631
Epoch 350, val loss: 0.7835795879364014
Epoch 360, training loss: 0.185177281498909 = 0.1138068214058876 + 0.01 * 7.137045860290527
Epoch 360, val loss: 0.7913808226585388
Epoch 370, training loss: 0.169906347990036 = 0.09858094900846481 + 0.01 * 7.132540225982666
Epoch 370, val loss: 0.8011594414710999
Epoch 380, training loss: 0.1571444869041443 = 0.0858907476067543 + 0.01 * 7.125374794006348
Epoch 380, val loss: 0.812438428401947
Epoch 390, training loss: 0.14643852412700653 = 0.07525550574064255 + 0.01 * 7.118302345275879
Epoch 390, val loss: 0.8248263001441956
Epoch 400, training loss: 0.13741371035575867 = 0.06628444045782089 + 0.01 * 7.112926959991455
Epoch 400, val loss: 0.8379876613616943
Epoch 410, training loss: 0.12990103662014008 = 0.05866347253322601 + 0.01 * 7.123756408691406
Epoch 410, val loss: 0.8516804575920105
Epoch 420, training loss: 0.12318969517946243 = 0.05216575413942337 + 0.01 * 7.1023945808410645
Epoch 420, val loss: 0.8657501339912415
Epoch 430, training loss: 0.11759229004383087 = 0.046595752239227295 + 0.01 * 7.099653720855713
Epoch 430, val loss: 0.8799988627433777
Epoch 440, training loss: 0.1127476915717125 = 0.041797734797000885 + 0.01 * 7.094995975494385
Epoch 440, val loss: 0.8942331075668335
Epoch 450, training loss: 0.10855766385793686 = 0.03764846920967102 + 0.01 * 7.090919494628906
Epoch 450, val loss: 0.9084437489509583
Epoch 460, training loss: 0.104903943836689 = 0.03405162692070007 + 0.01 * 7.085231781005859
Epoch 460, val loss: 0.9224552512168884
Epoch 470, training loss: 0.10166773200035095 = 0.030921654775738716 + 0.01 * 7.074607849121094
Epoch 470, val loss: 0.9362608790397644
Epoch 480, training loss: 0.09883937984704971 = 0.0281855296343565 + 0.01 * 7.065385341644287
Epoch 480, val loss: 0.949709951877594
Epoch 490, training loss: 0.09641498327255249 = 0.025784440338611603 + 0.01 * 7.06305456161499
Epoch 490, val loss: 0.9628649950027466
Epoch 500, training loss: 0.09428153932094574 = 0.023672688752412796 + 0.01 * 7.060884475708008
Epoch 500, val loss: 0.975728452205658
Epoch 510, training loss: 0.09231838583946228 = 0.021807227283716202 + 0.01 * 7.051115989685059
Epoch 510, val loss: 0.98818439245224
Epoch 520, training loss: 0.09063684940338135 = 0.020154699683189392 + 0.01 * 7.048214912414551
Epoch 520, val loss: 1.0002646446228027
Epoch 530, training loss: 0.08902626484632492 = 0.01868482120335102 + 0.01 * 7.034144878387451
Epoch 530, val loss: 1.012001395225525
Epoch 540, training loss: 0.08759478479623795 = 0.017371753230690956 + 0.01 * 7.022303581237793
Epoch 540, val loss: 1.0233581066131592
Epoch 550, training loss: 0.08643945306539536 = 0.016195518895983696 + 0.01 * 7.024393081665039
Epoch 550, val loss: 1.0343912839889526
Epoch 560, training loss: 0.08540938049554825 = 0.01513782050460577 + 0.01 * 7.027155876159668
Epoch 560, val loss: 1.0450940132141113
Epoch 570, training loss: 0.08423463255167007 = 0.014184719882905483 + 0.01 * 7.00499153137207
Epoch 570, val loss: 1.0554264783859253
Epoch 580, training loss: 0.08356577903032303 = 0.013322509825229645 + 0.01 * 7.024326801300049
Epoch 580, val loss: 1.0654429197311401
Epoch 590, training loss: 0.08255580812692642 = 0.012541880831122398 + 0.01 * 7.001392364501953
Epoch 590, val loss: 1.0752079486846924
Epoch 600, training loss: 0.08181369304656982 = 0.011831853538751602 + 0.01 * 6.998184680938721
Epoch 600, val loss: 1.0845969915390015
Epoch 610, training loss: 0.08089479058980942 = 0.011185242794454098 + 0.01 * 6.970954895019531
Epoch 610, val loss: 1.0937111377716064
Epoch 620, training loss: 0.08020269125699997 = 0.010593838058412075 + 0.01 * 6.960885524749756
Epoch 620, val loss: 1.1024994850158691
Epoch 630, training loss: 0.07990779727697372 = 0.010051530785858631 + 0.01 * 6.985626697540283
Epoch 630, val loss: 1.1110540628433228
Epoch 640, training loss: 0.0790315717458725 = 0.009553829208016396 + 0.01 * 6.947774887084961
Epoch 640, val loss: 1.119370698928833
Epoch 650, training loss: 0.07841718941926956 = 0.009095458313822746 + 0.01 * 6.932173252105713
Epoch 650, val loss: 1.1274021863937378
Epoch 660, training loss: 0.07797199487686157 = 0.008672451600432396 + 0.01 * 6.9299540519714355
Epoch 660, val loss: 1.1351606845855713
Epoch 670, training loss: 0.07804012298583984 = 0.008281063288450241 + 0.01 * 6.975905895233154
Epoch 670, val loss: 1.1426447629928589
Epoch 680, training loss: 0.07721453905105591 = 0.007919035851955414 + 0.01 * 6.9295501708984375
Epoch 680, val loss: 1.1500238180160522
Epoch 690, training loss: 0.07676379382610321 = 0.007582563906908035 + 0.01 * 6.918123722076416
Epoch 690, val loss: 1.1571204662322998
Epoch 700, training loss: 0.07636385411024094 = 0.00726926326751709 + 0.01 * 6.909459590911865
Epoch 700, val loss: 1.164070963859558
Epoch 710, training loss: 0.07608731836080551 = 0.00697726896032691 + 0.01 * 6.911005020141602
Epoch 710, val loss: 1.1708099842071533
Epoch 720, training loss: 0.07584478706121445 = 0.0067046633921563625 + 0.01 * 6.9140119552612305
Epoch 720, val loss: 1.177242398262024
Epoch 730, training loss: 0.07522666454315186 = 0.006449725944548845 + 0.01 * 6.8776936531066895
Epoch 730, val loss: 1.1836477518081665
Epoch 740, training loss: 0.07534792274236679 = 0.006210410036146641 + 0.01 * 6.913751602172852
Epoch 740, val loss: 1.1898456811904907
Epoch 750, training loss: 0.0748058557510376 = 0.005985839758068323 + 0.01 * 6.882001876831055
Epoch 750, val loss: 1.1958447694778442
Epoch 760, training loss: 0.07451800256967545 = 0.005775034427642822 + 0.01 * 6.874297142028809
Epoch 760, val loss: 1.20171320438385
Epoch 770, training loss: 0.074205681681633 = 0.0055764829739928246 + 0.01 * 6.862919807434082
Epoch 770, val loss: 1.2074342966079712
Epoch 780, training loss: 0.07419990748167038 = 0.0053893462754786015 + 0.01 * 6.881056308746338
Epoch 780, val loss: 1.2129186391830444
Epoch 790, training loss: 0.07388924062252045 = 0.005213250871747732 + 0.01 * 6.867598533630371
Epoch 790, val loss: 1.2184171676635742
Epoch 800, training loss: 0.07360342144966125 = 0.00504676578566432 + 0.01 * 6.855666160583496
Epoch 800, val loss: 1.2236442565917969
Epoch 810, training loss: 0.0734456405043602 = 0.00488952687010169 + 0.01 * 6.855611324310303
Epoch 810, val loss: 1.2287312746047974
Epoch 820, training loss: 0.07330308854579926 = 0.004740545991808176 + 0.01 * 6.856254577636719
Epoch 820, val loss: 1.2337534427642822
Epoch 830, training loss: 0.07297046482563019 = 0.004599798936396837 + 0.01 * 6.837066650390625
Epoch 830, val loss: 1.2386250495910645
Epoch 840, training loss: 0.07298021763563156 = 0.004465934820473194 + 0.01 * 6.851428031921387
Epoch 840, val loss: 1.2433884143829346
Epoch 850, training loss: 0.07262307405471802 = 0.004338701255619526 + 0.01 * 6.828437328338623
Epoch 850, val loss: 1.2480331659317017
Epoch 860, training loss: 0.07242532074451447 = 0.004217911511659622 + 0.01 * 6.820740699768066
Epoch 860, val loss: 1.252555012702942
Epoch 870, training loss: 0.07230224460363388 = 0.004102930426597595 + 0.01 * 6.819931507110596
Epoch 870, val loss: 1.2570104598999023
Epoch 880, training loss: 0.07211728394031525 = 0.003993511199951172 + 0.01 * 6.812377452850342
Epoch 880, val loss: 1.261300802230835
Epoch 890, training loss: 0.07214947789907455 = 0.0038891539443284273 + 0.01 * 6.826032638549805
Epoch 890, val loss: 1.2655750513076782
Epoch 900, training loss: 0.07194163650274277 = 0.0037897254806011915 + 0.01 * 6.815191745758057
Epoch 900, val loss: 1.2696348428726196
Epoch 910, training loss: 0.07209683954715729 = 0.0036948102060705423 + 0.01 * 6.840203285217285
Epoch 910, val loss: 1.273673415184021
Epoch 920, training loss: 0.07177381962537766 = 0.003604474477469921 + 0.01 * 6.816934585571289
Epoch 920, val loss: 1.277685284614563
Epoch 930, training loss: 0.07156975567340851 = 0.003517949255183339 + 0.01 * 6.805180549621582
Epoch 930, val loss: 1.2814873456954956
Epoch 940, training loss: 0.07156525552272797 = 0.0034353407099843025 + 0.01 * 6.812991619110107
Epoch 940, val loss: 1.2852184772491455
Epoch 950, training loss: 0.07152722030878067 = 0.003356321481987834 + 0.01 * 6.817090034484863
Epoch 950, val loss: 1.2889217138290405
Epoch 960, training loss: 0.0714542493224144 = 0.003280486213043332 + 0.01 * 6.817377090454102
Epoch 960, val loss: 1.2924882173538208
Epoch 970, training loss: 0.0711762085556984 = 0.003208023263141513 + 0.01 * 6.796818733215332
Epoch 970, val loss: 1.2960975170135498
Epoch 980, training loss: 0.07100299000740051 = 0.0031384092289954424 + 0.01 * 6.786458492279053
Epoch 980, val loss: 1.2995071411132812
Epoch 990, training loss: 0.07100533694028854 = 0.00307180592790246 + 0.01 * 6.793353080749512
Epoch 990, val loss: 1.3028241395950317
Epoch 1000, training loss: 0.07080131769180298 = 0.0030077039264142513 + 0.01 * 6.779361724853516
Epoch 1000, val loss: 1.3061150312423706
Epoch 1010, training loss: 0.07073835283517838 = 0.0029462624806910753 + 0.01 * 6.779208660125732
Epoch 1010, val loss: 1.3093280792236328
Epoch 1020, training loss: 0.07056617736816406 = 0.0028871309477835894 + 0.01 * 6.767904758453369
Epoch 1020, val loss: 1.312551498413086
Epoch 1030, training loss: 0.07048413902521133 = 0.0028303666040301323 + 0.01 * 6.765377998352051
Epoch 1030, val loss: 1.3155341148376465
Epoch 1040, training loss: 0.070791095495224 = 0.002775667468085885 + 0.01 * 6.80154275894165
Epoch 1040, val loss: 1.3185689449310303
Epoch 1050, training loss: 0.07056336849927902 = 0.0027232838328927755 + 0.01 * 6.784008502960205
Epoch 1050, val loss: 1.3215916156768799
Epoch 1060, training loss: 0.07030302286148071 = 0.002672583796083927 + 0.01 * 6.763044357299805
Epoch 1060, val loss: 1.3244813680648804
Epoch 1070, training loss: 0.07039150595664978 = 0.002623956650495529 + 0.01 * 6.776754379272461
Epoch 1070, val loss: 1.3272265195846558
Epoch 1080, training loss: 0.07044525444507599 = 0.002576833590865135 + 0.01 * 6.786842346191406
Epoch 1080, val loss: 1.329994797706604
Epoch 1090, training loss: 0.07000435888767242 = 0.0025315172970294952 + 0.01 * 6.747284889221191
Epoch 1090, val loss: 1.3327468633651733
Epoch 1100, training loss: 0.07008568197488785 = 0.002487646881490946 + 0.01 * 6.7598042488098145
Epoch 1100, val loss: 1.3354207277297974
Epoch 1110, training loss: 0.07020352780818939 = 0.002445413963869214 + 0.01 * 6.775811195373535
Epoch 1110, val loss: 1.338067650794983
Epoch 1120, training loss: 0.06981834769248962 = 0.0024046332109719515 + 0.01 * 6.7413716316223145
Epoch 1120, val loss: 1.3405420780181885
Epoch 1130, training loss: 0.06983989477157593 = 0.00236518238671124 + 0.01 * 6.747471332550049
Epoch 1130, val loss: 1.3430765867233276
Epoch 1140, training loss: 0.06973222643136978 = 0.0023270880337804556 + 0.01 * 6.740514278411865
Epoch 1140, val loss: 1.3455010652542114
Epoch 1150, training loss: 0.0695175901055336 = 0.00229029543697834 + 0.01 * 6.722729682922363
Epoch 1150, val loss: 1.3479695320129395
Epoch 1160, training loss: 0.06961596012115479 = 0.0022546686232089996 + 0.01 * 6.7361297607421875
Epoch 1160, val loss: 1.3502583503723145
Epoch 1170, training loss: 0.06973768770694733 = 0.002220200840383768 + 0.01 * 6.751749038696289
Epoch 1170, val loss: 1.3525947332382202
Epoch 1180, training loss: 0.06964525580406189 = 0.0021868692710995674 + 0.01 * 6.745838165283203
Epoch 1180, val loss: 1.3548638820648193
Epoch 1190, training loss: 0.06964938342571259 = 0.0021546673960983753 + 0.01 * 6.749471187591553
Epoch 1190, val loss: 1.3570696115493774
Epoch 1200, training loss: 0.06930118799209595 = 0.0021233565639704466 + 0.01 * 6.717783451080322
Epoch 1200, val loss: 1.3592193126678467
Epoch 1210, training loss: 0.06926217675209045 = 0.002093092305585742 + 0.01 * 6.7169084548950195
Epoch 1210, val loss: 1.3612722158432007
Epoch 1220, training loss: 0.06919114291667938 = 0.002063702791929245 + 0.01 * 6.71274471282959
Epoch 1220, val loss: 1.3633477687835693
Epoch 1230, training loss: 0.06932702660560608 = 0.002035253681242466 + 0.01 * 6.729177474975586
Epoch 1230, val loss: 1.3654794692993164
Epoch 1240, training loss: 0.06894996762275696 = 0.0020076315850019455 + 0.01 * 6.6942338943481445
Epoch 1240, val loss: 1.3673651218414307
Epoch 1250, training loss: 0.06881257891654968 = 0.0019808015786111355 + 0.01 * 6.683177471160889
Epoch 1250, val loss: 1.3693227767944336
Epoch 1260, training loss: 0.0688331350684166 = 0.00195474736392498 + 0.01 * 6.687839031219482
Epoch 1260, val loss: 1.3712042570114136
Epoch 1270, training loss: 0.0691327229142189 = 0.0019294944358989596 + 0.01 * 6.72032356262207
Epoch 1270, val loss: 1.3730798959732056
Epoch 1280, training loss: 0.06904301047325134 = 0.001905015087686479 + 0.01 * 6.713799476623535
Epoch 1280, val loss: 1.3750083446502686
Epoch 1290, training loss: 0.06874330341815948 = 0.001881177886389196 + 0.01 * 6.68621301651001
Epoch 1290, val loss: 1.376743197441101
Epoch 1300, training loss: 0.06866675615310669 = 0.001858046860434115 + 0.01 * 6.68087100982666
Epoch 1300, val loss: 1.3784093856811523
Epoch 1310, training loss: 0.0686635673046112 = 0.0018356393557041883 + 0.01 * 6.682793140411377
Epoch 1310, val loss: 1.380219578742981
Epoch 1320, training loss: 0.06854213029146194 = 0.00181383709423244 + 0.01 * 6.672829627990723
Epoch 1320, val loss: 1.3818837404251099
Epoch 1330, training loss: 0.06892398744821548 = 0.0017925204010680318 + 0.01 * 6.713146686553955
Epoch 1330, val loss: 1.3835227489471436
Epoch 1340, training loss: 0.06850507110357285 = 0.001771837123669684 + 0.01 * 6.673323154449463
Epoch 1340, val loss: 1.3851416110992432
Epoch 1350, training loss: 0.068476103246212 = 0.0017516835359856486 + 0.01 * 6.6724419593811035
Epoch 1350, val loss: 1.3866876363754272
Epoch 1360, training loss: 0.0683053582906723 = 0.0017320348415523767 + 0.01 * 6.657332420349121
Epoch 1360, val loss: 1.3882805109024048
Epoch 1370, training loss: 0.06852339208126068 = 0.0017129031475633383 + 0.01 * 6.68104887008667
Epoch 1370, val loss: 1.3897098302841187
Epoch 1380, training loss: 0.06833198666572571 = 0.0016942898510023952 + 0.01 * 6.663769721984863
Epoch 1380, val loss: 1.391178011894226
Epoch 1390, training loss: 0.06825518608093262 = 0.0016761664301156998 + 0.01 * 6.657902240753174
Epoch 1390, val loss: 1.3926161527633667
Epoch 1400, training loss: 0.06802375614643097 = 0.00165843206923455 + 0.01 * 6.636532306671143
Epoch 1400, val loss: 1.3940743207931519
Epoch 1410, training loss: 0.06809812039136887 = 0.0016411957331001759 + 0.01 * 6.645692348480225
Epoch 1410, val loss: 1.3954628705978394
Epoch 1420, training loss: 0.0682874396443367 = 0.0016243585851043463 + 0.01 * 6.666308403015137
Epoch 1420, val loss: 1.3967875242233276
Epoch 1430, training loss: 0.06805000454187393 = 0.0016079258639365435 + 0.01 * 6.644207954406738
Epoch 1430, val loss: 1.3981260061264038
Epoch 1440, training loss: 0.06835835427045822 = 0.0015918759163469076 + 0.01 * 6.676648139953613
Epoch 1440, val loss: 1.3995327949523926
Epoch 1450, training loss: 0.06792224943637848 = 0.0015761653194203973 + 0.01 * 6.634608745574951
Epoch 1450, val loss: 1.4007055759429932
Epoch 1460, training loss: 0.0678948312997818 = 0.0015608402900397778 + 0.01 * 6.633399486541748
Epoch 1460, val loss: 1.401951789855957
Epoch 1470, training loss: 0.06801126152276993 = 0.001545869279652834 + 0.01 * 6.646539211273193
Epoch 1470, val loss: 1.4031765460968018
Epoch 1480, training loss: 0.06796059012413025 = 0.0015313607873395085 + 0.01 * 6.642922878265381
Epoch 1480, val loss: 1.4044679403305054
Epoch 1490, training loss: 0.06783000379800797 = 0.0015170653350651264 + 0.01 * 6.631294250488281
Epoch 1490, val loss: 1.4055688381195068
Epoch 1500, training loss: 0.06806828081607819 = 0.0015031489310786128 + 0.01 * 6.656513690948486
Epoch 1500, val loss: 1.4068197011947632
Epoch 1510, training loss: 0.06776005774736404 = 0.0014895333442837 + 0.01 * 6.627052307128906
Epoch 1510, val loss: 1.4078816175460815
Epoch 1520, training loss: 0.06797806918621063 = 0.0014762494247406721 + 0.01 * 6.650182247161865
Epoch 1520, val loss: 1.409075140953064
Epoch 1530, training loss: 0.06755576282739639 = 0.0014632114907726645 + 0.01 * 6.609255313873291
Epoch 1530, val loss: 1.41010582447052
Epoch 1540, training loss: 0.06754808127880096 = 0.001450474839657545 + 0.01 * 6.6097612380981445
Epoch 1540, val loss: 1.4112157821655273
Epoch 1550, training loss: 0.06765494495630264 = 0.001438003615476191 + 0.01 * 6.6216936111450195
Epoch 1550, val loss: 1.4122337102890015
Epoch 1560, training loss: 0.06792593002319336 = 0.0014258079463616014 + 0.01 * 6.650012493133545
Epoch 1560, val loss: 1.4133059978485107
Epoch 1570, training loss: 0.06774948537349701 = 0.0014138608239591122 + 0.01 * 6.6335625648498535
Epoch 1570, val loss: 1.4143402576446533
Epoch 1580, training loss: 0.06749129295349121 = 0.0014021382667124271 + 0.01 * 6.608915328979492
Epoch 1580, val loss: 1.4152384996414185
Epoch 1590, training loss: 0.06741566956043243 = 0.0013906884705647826 + 0.01 * 6.602498531341553
Epoch 1590, val loss: 1.4162911176681519
Epoch 1600, training loss: 0.06735827773809433 = 0.001379454624839127 + 0.01 * 6.5978827476501465
Epoch 1600, val loss: 1.4172312021255493
Epoch 1610, training loss: 0.06755870580673218 = 0.0013684930745512247 + 0.01 * 6.619021415710449
Epoch 1610, val loss: 1.4182136058807373
Epoch 1620, training loss: 0.06743044406175613 = 0.0013577283825725317 + 0.01 * 6.607271671295166
Epoch 1620, val loss: 1.4191367626190186
Epoch 1630, training loss: 0.06728161126375198 = 0.0013472121208906174 + 0.01 * 6.59343957901001
Epoch 1630, val loss: 1.4200252294540405
Epoch 1640, training loss: 0.06742896139621735 = 0.0013368766522035003 + 0.01 * 6.609208106994629
Epoch 1640, val loss: 1.4209405183792114
Epoch 1650, training loss: 0.06748607009649277 = 0.0013267851900309324 + 0.01 * 6.615928649902344
Epoch 1650, val loss: 1.4219121932983398
Epoch 1660, training loss: 0.06740611046552658 = 0.0013168345903977752 + 0.01 * 6.6089277267456055
Epoch 1660, val loss: 1.4227261543273926
Epoch 1670, training loss: 0.06729009747505188 = 0.0013070740969851613 + 0.01 * 6.598302364349365
Epoch 1670, val loss: 1.4235895872116089
Epoch 1680, training loss: 0.06748972833156586 = 0.0012975141871720552 + 0.01 * 6.619222164154053
Epoch 1680, val loss: 1.4244697093963623
Epoch 1690, training loss: 0.06721241772174835 = 0.0012881343718618155 + 0.01 * 6.592428684234619
Epoch 1690, val loss: 1.4252797365188599
Epoch 1700, training loss: 0.06728620082139969 = 0.0012788960011675954 + 0.01 * 6.600729942321777
Epoch 1700, val loss: 1.4260808229446411
Epoch 1710, training loss: 0.0673183724284172 = 0.001269862288609147 + 0.01 * 6.604851245880127
Epoch 1710, val loss: 1.4269413948059082
Epoch 1720, training loss: 0.0672178789973259 = 0.0012609835248440504 + 0.01 * 6.59568977355957
Epoch 1720, val loss: 1.427669644355774
Epoch 1730, training loss: 0.06700940430164337 = 0.0012522708857432008 + 0.01 * 6.575713157653809
Epoch 1730, val loss: 1.428478479385376
Epoch 1740, training loss: 0.06713390350341797 = 0.0012437395052984357 + 0.01 * 6.589016437530518
Epoch 1740, val loss: 1.4292781352996826
Epoch 1750, training loss: 0.06702613830566406 = 0.0012352976482361555 + 0.01 * 6.5790839195251465
Epoch 1750, val loss: 1.4300224781036377
Epoch 1760, training loss: 0.06693276762962341 = 0.0012270368169993162 + 0.01 * 6.570573329925537
Epoch 1760, val loss: 1.4307825565338135
Epoch 1770, training loss: 0.06696652621030807 = 0.001218890305608511 + 0.01 * 6.574763774871826
Epoch 1770, val loss: 1.4315550327301025
Epoch 1780, training loss: 0.06706887483596802 = 0.001210907008498907 + 0.01 * 6.585797309875488
Epoch 1780, val loss: 1.432257056236267
Epoch 1790, training loss: 0.06690831482410431 = 0.0012030138168483973 + 0.01 * 6.570530414581299
Epoch 1790, val loss: 1.4329370260238647
Epoch 1800, training loss: 0.06702802330255508 = 0.0011952900094911456 + 0.01 * 6.583273887634277
Epoch 1800, val loss: 1.433640956878662
Epoch 1810, training loss: 0.06690877676010132 = 0.0011876646894961596 + 0.01 * 6.572111129760742
Epoch 1810, val loss: 1.4342896938323975
Epoch 1820, training loss: 0.06689289957284927 = 0.0011801745276898146 + 0.01 * 6.571272850036621
Epoch 1820, val loss: 1.4350061416625977
Epoch 1830, training loss: 0.06702276319265366 = 0.001172836171463132 + 0.01 * 6.584993362426758
Epoch 1830, val loss: 1.4356818199157715
Epoch 1840, training loss: 0.06665913015604019 = 0.0011656122514978051 + 0.01 * 6.549351692199707
Epoch 1840, val loss: 1.436376690864563
Epoch 1850, training loss: 0.06683927029371262 = 0.001158474711701274 + 0.01 * 6.568079471588135
Epoch 1850, val loss: 1.4370241165161133
Epoch 1860, training loss: 0.06718820333480835 = 0.0011514592915773392 + 0.01 * 6.60367488861084
Epoch 1860, val loss: 1.4376260042190552
Epoch 1870, training loss: 0.06673654913902283 = 0.0011445702984929085 + 0.01 * 6.559197425842285
Epoch 1870, val loss: 1.4383198022842407
Epoch 1880, training loss: 0.06674428284168243 = 0.0011377399787306786 + 0.01 * 6.560654163360596
Epoch 1880, val loss: 1.4388927221298218
Epoch 1890, training loss: 0.06664950400590897 = 0.001131068216636777 + 0.01 * 6.551844120025635
Epoch 1890, val loss: 1.4395776987075806
Epoch 1900, training loss: 0.06649599969387054 = 0.0011244758497923613 + 0.01 * 6.537152290344238
Epoch 1900, val loss: 1.440156102180481
Epoch 1910, training loss: 0.06660056859254837 = 0.0011180061846971512 + 0.01 * 6.548256874084473
Epoch 1910, val loss: 1.440796136856079
Epoch 1920, training loss: 0.06675354391336441 = 0.0011116091627627611 + 0.01 * 6.564193248748779
Epoch 1920, val loss: 1.44138765335083
Epoch 1930, training loss: 0.0665566474199295 = 0.0011053176131099463 + 0.01 * 6.545133590698242
Epoch 1930, val loss: 1.4419488906860352
Epoch 1940, training loss: 0.0665765032172203 = 0.001099080196581781 + 0.01 * 6.5477423667907715
Epoch 1940, val loss: 1.4425512552261353
Epoch 1950, training loss: 0.06689327955245972 = 0.0010930183343589306 + 0.01 * 6.580026626586914
Epoch 1950, val loss: 1.4432165622711182
Epoch 1960, training loss: 0.06669998914003372 = 0.0010869365651160479 + 0.01 * 6.561305522918701
Epoch 1960, val loss: 1.4436908960342407
Epoch 1970, training loss: 0.06636958569288254 = 0.001081006834283471 + 0.01 * 6.528858184814453
Epoch 1970, val loss: 1.4442497491836548
Epoch 1980, training loss: 0.0666009932756424 = 0.0010751689551398158 + 0.01 * 6.552582740783691
Epoch 1980, val loss: 1.4448210000991821
Epoch 1990, training loss: 0.0664965882897377 = 0.001069375779479742 + 0.01 * 6.542721748352051
Epoch 1990, val loss: 1.4454154968261719
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8139167105956774
The final CL Acc:0.75802, 0.00462, The final GNN Acc:0.81585, 0.00311
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13124])
remove edge: torch.Size([2, 7986])
updated graph: torch.Size([2, 10554])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0459914207458496 = 1.9600231647491455 + 0.01 * 8.596831321716309
Epoch 0, val loss: 1.9613929986953735
Epoch 10, training loss: 2.035784959793091 = 1.9498175382614136 + 0.01 * 8.596739768981934
Epoch 10, val loss: 1.9515703916549683
Epoch 20, training loss: 2.023073434829712 = 1.9371094703674316 + 0.01 * 8.59640121459961
Epoch 20, val loss: 1.9388089179992676
Epoch 30, training loss: 2.00528621673584 = 1.9193357229232788 + 0.01 * 8.595057487487793
Epoch 30, val loss: 1.920441746711731
Epoch 40, training loss: 1.9789701700210571 = 1.8931076526641846 + 0.01 * 8.58625602722168
Epoch 40, val loss: 1.8934024572372437
Epoch 50, training loss: 1.9412444829940796 = 1.8558512926101685 + 0.01 * 8.539318084716797
Epoch 50, val loss: 1.8561190366744995
Epoch 60, training loss: 1.8948925733566284 = 1.8113961219787598 + 0.01 * 8.349639892578125
Epoch 60, val loss: 1.814806342124939
Epoch 70, training loss: 1.8522932529449463 = 1.7707210779190063 + 0.01 * 8.15721321105957
Epoch 70, val loss: 1.780479907989502
Epoch 80, training loss: 1.808310866355896 = 1.728214979171753 + 0.01 * 8.009587287902832
Epoch 80, val loss: 1.7432481050491333
Epoch 90, training loss: 1.74862802028656 = 1.671275019645691 + 0.01 * 7.7353010177612305
Epoch 90, val loss: 1.6910476684570312
Epoch 100, training loss: 1.6705701351165771 = 1.5956437587738037 + 0.01 * 7.492632865905762
Epoch 100, val loss: 1.6230902671813965
Epoch 110, training loss: 1.5759650468826294 = 1.5022341012954712 + 0.01 * 7.373097896575928
Epoch 110, val loss: 1.5415977239608765
Epoch 120, training loss: 1.4708906412124634 = 1.3975392580032349 + 0.01 * 7.33514404296875
Epoch 120, val loss: 1.4529733657836914
Epoch 130, training loss: 1.3638018369674683 = 1.290714144706726 + 0.01 * 7.308772563934326
Epoch 130, val loss: 1.365973949432373
Epoch 140, training loss: 1.2602708339691162 = 1.1874189376831055 + 0.01 * 7.2851948738098145
Epoch 140, val loss: 1.283294439315796
Epoch 150, training loss: 1.163170337677002 = 1.0906068086624146 + 0.01 * 7.256350994110107
Epoch 150, val loss: 1.2064920663833618
Epoch 160, training loss: 1.0737863779067993 = 1.0015790462493896 + 0.01 * 7.220738887786865
Epoch 160, val loss: 1.1362990140914917
Epoch 170, training loss: 0.9912552237510681 = 0.9194279313087463 + 0.01 * 7.182727336883545
Epoch 170, val loss: 1.0722354650497437
Epoch 180, training loss: 0.9140663146972656 = 0.8425971865653992 + 0.01 * 7.146914005279541
Epoch 180, val loss: 1.013320803642273
Epoch 190, training loss: 0.8415477275848389 = 0.7703571319580078 + 0.01 * 7.11906099319458
Epoch 190, val loss: 0.9589600563049316
Epoch 200, training loss: 0.773891031742096 = 0.7028182744979858 + 0.01 * 7.107275485992432
Epoch 200, val loss: 0.9094244241714478
Epoch 210, training loss: 0.7111949324607849 = 0.6402302980422974 + 0.01 * 7.09646463394165
Epoch 210, val loss: 0.8648265600204468
Epoch 220, training loss: 0.6534872651100159 = 0.5825908780097961 + 0.01 * 7.089640140533447
Epoch 220, val loss: 0.8256353735923767
Epoch 230, training loss: 0.600549042224884 = 0.5297027826309204 + 0.01 * 7.084626197814941
Epoch 230, val loss: 0.7920793890953064
Epoch 240, training loss: 0.5518577694892883 = 0.48105642199516296 + 0.01 * 7.080134391784668
Epoch 240, val loss: 0.7638982534408569
Epoch 250, training loss: 0.5067851543426514 = 0.436028391122818 + 0.01 * 7.075673580169678
Epoch 250, val loss: 0.7404875159263611
Epoch 260, training loss: 0.464678019285202 = 0.39396241307258606 + 0.01 * 7.071561336517334
Epoch 260, val loss: 0.720989465713501
Epoch 270, training loss: 0.42507296800613403 = 0.35439300537109375 + 0.01 * 7.067995548248291
Epoch 270, val loss: 0.7047243118286133
Epoch 280, training loss: 0.3875862658023834 = 0.3169481158256531 + 0.01 * 7.063815593719482
Epoch 280, val loss: 0.6911613345146179
Epoch 290, training loss: 0.35209208726882935 = 0.2814837694168091 + 0.01 * 7.060831069946289
Epoch 290, val loss: 0.6799845695495605
Epoch 300, training loss: 0.31872403621673584 = 0.24813202023506165 + 0.01 * 7.059203147888184
Epoch 300, val loss: 0.6711851358413696
Epoch 310, training loss: 0.28786832094192505 = 0.21727347373962402 + 0.01 * 7.059486389160156
Epoch 310, val loss: 0.6649866700172424
Epoch 320, training loss: 0.25989624857902527 = 0.1893562376499176 + 0.01 * 7.0540008544921875
Epoch 320, val loss: 0.6615661382675171
Epoch 330, training loss: 0.23513442277908325 = 0.16463690996170044 + 0.01 * 7.049751281738281
Epoch 330, val loss: 0.6609657406806946
Epoch 340, training loss: 0.21359798312187195 = 0.14312204718589783 + 0.01 * 7.04759407043457
Epoch 340, val loss: 0.6629970073699951
Epoch 350, training loss: 0.19508512318134308 = 0.12461630254983902 + 0.01 * 7.046882152557373
Epoch 350, val loss: 0.6674017906188965
Epoch 360, training loss: 0.17918506264686584 = 0.1087854877114296 + 0.01 * 7.039956569671631
Epoch 360, val loss: 0.6738558411598206
Epoch 370, training loss: 0.16563071310520172 = 0.09525766223669052 + 0.01 * 7.0373053550720215
Epoch 370, val loss: 0.6820528507232666
Epoch 380, training loss: 0.1539880782365799 = 0.08368433266878128 + 0.01 * 7.030375003814697
Epoch 380, val loss: 0.6915605664253235
Epoch 390, training loss: 0.14404025673866272 = 0.07375713437795639 + 0.01 * 7.0283122062683105
Epoch 390, val loss: 0.7020335793495178
Epoch 400, training loss: 0.13544178009033203 = 0.06522399932146072 + 0.01 * 7.021778583526611
Epoch 400, val loss: 0.7132176160812378
Epoch 410, training loss: 0.128043070435524 = 0.05786929652094841 + 0.01 * 7.0173773765563965
Epoch 410, val loss: 0.7248486280441284
Epoch 420, training loss: 0.12159962952136993 = 0.051510777324438095 + 0.01 * 7.008885383605957
Epoch 420, val loss: 0.7368159890174866
Epoch 430, training loss: 0.11603021621704102 = 0.04600159451365471 + 0.01 * 7.002861976623535
Epoch 430, val loss: 0.7490249276161194
Epoch 440, training loss: 0.11125437915325165 = 0.04122288152575493 + 0.01 * 7.003150463104248
Epoch 440, val loss: 0.761407732963562
Epoch 450, training loss: 0.10708481073379517 = 0.037080198526382446 + 0.01 * 7.000461578369141
Epoch 450, val loss: 0.7738078832626343
Epoch 460, training loss: 0.10340046137571335 = 0.03347703069448471 + 0.01 * 6.992342948913574
Epoch 460, val loss: 0.7862161993980408
Epoch 470, training loss: 0.10015131533145905 = 0.0303330160677433 + 0.01 * 6.98183012008667
Epoch 470, val loss: 0.7985711097717285
Epoch 480, training loss: 0.09747850894927979 = 0.027581067755818367 + 0.01 * 6.989744663238525
Epoch 480, val loss: 0.8108305931091309
Epoch 490, training loss: 0.09497779607772827 = 0.025170383974909782 + 0.01 * 6.980741024017334
Epoch 490, val loss: 0.8228551745414734
Epoch 500, training loss: 0.09279916435480118 = 0.023049382492899895 + 0.01 * 6.974977970123291
Epoch 500, val loss: 0.8346261382102966
Epoch 510, training loss: 0.0908462256193161 = 0.021177425980567932 + 0.01 * 6.9668803215026855
Epoch 510, val loss: 0.8461207151412964
Epoch 520, training loss: 0.08908698707818985 = 0.019518710672855377 + 0.01 * 6.956827640533447
Epoch 520, val loss: 0.8573107123374939
Epoch 530, training loss: 0.08757410943508148 = 0.01804395765066147 + 0.01 * 6.953015327453613
Epoch 530, val loss: 0.8682311177253723
Epoch 540, training loss: 0.08620136231184006 = 0.016729356721043587 + 0.01 * 6.947201251983643
Epoch 540, val loss: 0.8787674307823181
Epoch 550, training loss: 0.08498933166265488 = 0.015552779659628868 + 0.01 * 6.943655014038086
Epoch 550, val loss: 0.8890148997306824
Epoch 560, training loss: 0.08409153670072556 = 0.0144961541518569 + 0.01 * 6.959537982940674
Epoch 560, val loss: 0.8989629149436951
Epoch 570, training loss: 0.08289849758148193 = 0.013545393012464046 + 0.01 * 6.9353108406066895
Epoch 570, val loss: 0.9085526466369629
Epoch 580, training loss: 0.08205518126487732 = 0.012687142938375473 + 0.01 * 6.936803340911865
Epoch 580, val loss: 0.917853057384491
Epoch 590, training loss: 0.08115227520465851 = 0.0119102094322443 + 0.01 * 6.9242072105407715
Epoch 590, val loss: 0.926851749420166
Epoch 600, training loss: 0.08048983663320541 = 0.011204772628843784 + 0.01 * 6.928506851196289
Epoch 600, val loss: 0.935586154460907
Epoch 610, training loss: 0.07970952987670898 = 0.01056304108351469 + 0.01 * 6.914649486541748
Epoch 610, val loss: 0.9439868927001953
Epoch 620, training loss: 0.0792338103055954 = 0.009977104142308235 + 0.01 * 6.925670623779297
Epoch 620, val loss: 0.9521398544311523
Epoch 630, training loss: 0.07854035496711731 = 0.009441195987164974 + 0.01 * 6.909915924072266
Epoch 630, val loss: 0.9600357413291931
Epoch 640, training loss: 0.07802777737379074 = 0.008949726819992065 + 0.01 * 6.9078049659729
Epoch 640, val loss: 0.967681348323822
Epoch 650, training loss: 0.07748932391405106 = 0.00849821139127016 + 0.01 * 6.899110794067383
Epoch 650, val loss: 0.9750783443450928
Epoch 660, training loss: 0.07727442681789398 = 0.008082251995801926 + 0.01 * 6.919217586517334
Epoch 660, val loss: 0.9822714328765869
Epoch 670, training loss: 0.07663200050592422 = 0.007698734290897846 + 0.01 * 6.893326759338379
Epoch 670, val loss: 0.9892347455024719
Epoch 680, training loss: 0.07632387429475784 = 0.0073439194820821285 + 0.01 * 6.897995948791504
Epoch 680, val loss: 0.9960463643074036
Epoch 690, training loss: 0.0759756937623024 = 0.007015436887741089 + 0.01 * 6.896025657653809
Epoch 690, val loss: 1.0026198625564575
Epoch 700, training loss: 0.0755668506026268 = 0.006711309775710106 + 0.01 * 6.885554313659668
Epoch 700, val loss: 1.0089415311813354
Epoch 710, training loss: 0.07515716552734375 = 0.006428304128348827 + 0.01 * 6.872886657714844
Epoch 710, val loss: 1.0150877237319946
Epoch 720, training loss: 0.07480034232139587 = 0.006164249032735825 + 0.01 * 6.863609790802002
Epoch 720, val loss: 1.021085262298584
Epoch 730, training loss: 0.07451475411653519 = 0.005918232258409262 + 0.01 * 6.859652042388916
Epoch 730, val loss: 1.0269253253936768
Epoch 740, training loss: 0.07442130148410797 = 0.0056879096664488316 + 0.01 * 6.873339653015137
Epoch 740, val loss: 1.0325582027435303
Epoch 750, training loss: 0.07392598688602448 = 0.005472993478178978 + 0.01 * 6.845299243927002
Epoch 750, val loss: 1.038059115409851
Epoch 760, training loss: 0.07379691302776337 = 0.005271507892757654 + 0.01 * 6.852540493011475
Epoch 760, val loss: 1.0434324741363525
Epoch 770, training loss: 0.0735221803188324 = 0.005082030780613422 + 0.01 * 6.844015121459961
Epoch 770, val loss: 1.048609733581543
Epoch 780, training loss: 0.07340165227651596 = 0.004903928842395544 + 0.01 * 6.8497724533081055
Epoch 780, val loss: 1.0537385940551758
Epoch 790, training loss: 0.07316410541534424 = 0.004736247006803751 + 0.01 * 6.8427863121032715
Epoch 790, val loss: 1.058632493019104
Epoch 800, training loss: 0.07280346751213074 = 0.004577945917844772 + 0.01 * 6.822552680969238
Epoch 800, val loss: 1.0634459257125854
Epoch 810, training loss: 0.0725817158818245 = 0.004428258165717125 + 0.01 * 6.815345764160156
Epoch 810, val loss: 1.0681405067443848
Epoch 820, training loss: 0.07274676859378815 = 0.004286523908376694 + 0.01 * 6.846024513244629
Epoch 820, val loss: 1.0727473497390747
Epoch 830, training loss: 0.0722271054983139 = 0.004152537323534489 + 0.01 * 6.807456970214844
Epoch 830, val loss: 1.077217698097229
Epoch 840, training loss: 0.07222321629524231 = 0.004025803878903389 + 0.01 * 6.819741249084473
Epoch 840, val loss: 1.0815527439117432
Epoch 850, training loss: 0.07188914716243744 = 0.003905673511326313 + 0.01 * 6.798347473144531
Epoch 850, val loss: 1.0858086347579956
Epoch 860, training loss: 0.07176126539707184 = 0.0037914991844445467 + 0.01 * 6.7969770431518555
Epoch 860, val loss: 1.090001106262207
Epoch 870, training loss: 0.07157517224550247 = 0.003682973561808467 + 0.01 * 6.789220333099365
Epoch 870, val loss: 1.0940431356430054
Epoch 880, training loss: 0.07171633839607239 = 0.0035797462332993746 + 0.01 * 6.81365966796875
Epoch 880, val loss: 1.0980045795440674
Epoch 890, training loss: 0.07129012048244476 = 0.0034814963582903147 + 0.01 * 6.780861854553223
Epoch 890, val loss: 1.1018322706222534
Epoch 900, training loss: 0.07177035510540009 = 0.003387796925380826 + 0.01 * 6.838256359100342
Epoch 900, val loss: 1.1057041883468628
Epoch 910, training loss: 0.07113055139780045 = 0.003298766678199172 + 0.01 * 6.783178329467773
Epoch 910, val loss: 1.1093506813049316
Epoch 920, training loss: 0.07118125259876251 = 0.0032136505469679832 + 0.01 * 6.796760082244873
Epoch 920, val loss: 1.1129118204116821
Epoch 930, training loss: 0.0708332359790802 = 0.0031324943993240595 + 0.01 * 6.770074844360352
Epoch 930, val loss: 1.1164287328720093
Epoch 940, training loss: 0.0706646665930748 = 0.0030548276845365763 + 0.01 * 6.760984420776367
Epoch 940, val loss: 1.1198818683624268
Epoch 950, training loss: 0.07081517577171326 = 0.0029804392252117395 + 0.01 * 6.783473968505859
Epoch 950, val loss: 1.1232600212097168
Epoch 960, training loss: 0.07046997547149658 = 0.0029092926997691393 + 0.01 * 6.756068229675293
Epoch 960, val loss: 1.1265254020690918
Epoch 970, training loss: 0.07069498300552368 = 0.002841196721419692 + 0.01 * 6.785378932952881
Epoch 970, val loss: 1.1297895908355713
Epoch 980, training loss: 0.07027655839920044 = 0.0027760090306401253 + 0.01 * 6.750054836273193
Epoch 980, val loss: 1.132900595664978
Epoch 990, training loss: 0.07062461227178574 = 0.0027135901618748903 + 0.01 * 6.791101932525635
Epoch 990, val loss: 1.1359624862670898
Epoch 1000, training loss: 0.07015136629343033 = 0.002653948962688446 + 0.01 * 6.749741554260254
Epoch 1000, val loss: 1.1389085054397583
Epoch 1010, training loss: 0.06998191028833389 = 0.002596538048237562 + 0.01 * 6.738537788391113
Epoch 1010, val loss: 1.1418366432189941
Epoch 1020, training loss: 0.06990452110767365 = 0.002541296649724245 + 0.01 * 6.736322402954102
Epoch 1020, val loss: 1.144741415977478
Epoch 1030, training loss: 0.07019632309675217 = 0.00248826015740633 + 0.01 * 6.770806312561035
Epoch 1030, val loss: 1.1475732326507568
Epoch 1040, training loss: 0.06985601782798767 = 0.002437405753880739 + 0.01 * 6.741861343383789
Epoch 1040, val loss: 1.1503055095672607
Epoch 1050, training loss: 0.06968449056148529 = 0.0023885173723101616 + 0.01 * 6.729597568511963
Epoch 1050, val loss: 1.1529484987258911
Epoch 1060, training loss: 0.06978493928909302 = 0.0023414480965584517 + 0.01 * 6.744349479675293
Epoch 1060, val loss: 1.155606985092163
Epoch 1070, training loss: 0.06965431571006775 = 0.002296082442626357 + 0.01 * 6.735823631286621
Epoch 1070, val loss: 1.158136248588562
Epoch 1080, training loss: 0.06945858895778656 = 0.0022523589432239532 + 0.01 * 6.72062349319458
Epoch 1080, val loss: 1.1606972217559814
Epoch 1090, training loss: 0.06951098889112473 = 0.002210212405771017 + 0.01 * 6.730077743530273
Epoch 1090, val loss: 1.163162350654602
Epoch 1100, training loss: 0.0694226622581482 = 0.0021696737967431545 + 0.01 * 6.725298881530762
Epoch 1100, val loss: 1.1655367612838745
Epoch 1110, training loss: 0.06967397034168243 = 0.0021305533591657877 + 0.01 * 6.7543416023254395
Epoch 1110, val loss: 1.1679219007492065
Epoch 1120, training loss: 0.069363072514534 = 0.0020928571466356516 + 0.01 * 6.727021217346191
Epoch 1120, val loss: 1.170224905014038
Epoch 1130, training loss: 0.06931916624307632 = 0.0020564566366374493 + 0.01 * 6.726271152496338
Epoch 1130, val loss: 1.1724697351455688
Epoch 1140, training loss: 0.06916188448667526 = 0.002021225867792964 + 0.01 * 6.714066028594971
Epoch 1140, val loss: 1.1745997667312622
Epoch 1150, training loss: 0.06926380842924118 = 0.0019872456323355436 + 0.01 * 6.72765588760376
Epoch 1150, val loss: 1.1768137216567993
Epoch 1160, training loss: 0.06920121610164642 = 0.0019544465467333794 + 0.01 * 6.724677085876465
Epoch 1160, val loss: 1.1788731813430786
Epoch 1170, training loss: 0.06899085640907288 = 0.0019227074226364493 + 0.01 * 6.706815242767334
Epoch 1170, val loss: 1.1809325218200684
Epoch 1180, training loss: 0.06889476627111435 = 0.0018920049769803882 + 0.01 * 6.700275897979736
Epoch 1180, val loss: 1.1829595565795898
Epoch 1190, training loss: 0.06880081444978714 = 0.0018623287323862314 + 0.01 * 6.693848133087158
Epoch 1190, val loss: 1.1848877668380737
Epoch 1200, training loss: 0.06905949115753174 = 0.0018335540080443025 + 0.01 * 6.722594261169434
Epoch 1200, val loss: 1.1868209838867188
Epoch 1210, training loss: 0.068822480738163 = 0.001805899664759636 + 0.01 * 6.701657772064209
Epoch 1210, val loss: 1.1886838674545288
Epoch 1220, training loss: 0.06866287440061569 = 0.001779097248800099 + 0.01 * 6.68837833404541
Epoch 1220, val loss: 1.1905406713485718
Epoch 1230, training loss: 0.06871531158685684 = 0.0017531345365568995 + 0.01 * 6.6962175369262695
Epoch 1230, val loss: 1.1923351287841797
Epoch 1240, training loss: 0.06858743727207184 = 0.0017279840540140867 + 0.01 * 6.685945510864258
Epoch 1240, val loss: 1.194077730178833
Epoch 1250, training loss: 0.06859386712312698 = 0.00170355464797467 + 0.01 * 6.689031600952148
Epoch 1250, val loss: 1.1958459615707397
Epoch 1260, training loss: 0.06856120377779007 = 0.0016798735596239567 + 0.01 * 6.688133239746094
Epoch 1260, val loss: 1.1975016593933105
Epoch 1270, training loss: 0.06850416958332062 = 0.0016569728031754494 + 0.01 * 6.684719562530518
Epoch 1270, val loss: 1.1991935968399048
Epoch 1280, training loss: 0.06872402876615524 = 0.0016347746131941676 + 0.01 * 6.708925247192383
Epoch 1280, val loss: 1.2007569074630737
Epoch 1290, training loss: 0.0683685764670372 = 0.0016132296295836568 + 0.01 * 6.675535202026367
Epoch 1290, val loss: 1.2023868560791016
Epoch 1300, training loss: 0.06843474507331848 = 0.0015923617174848914 + 0.01 * 6.684238910675049
Epoch 1300, val loss: 1.2039521932601929
Epoch 1310, training loss: 0.0685817301273346 = 0.0015721172094345093 + 0.01 * 6.700961112976074
Epoch 1310, val loss: 1.2054144144058228
Epoch 1320, training loss: 0.06833516061306 = 0.0015524927293881774 + 0.01 * 6.678267002105713
Epoch 1320, val loss: 1.2069575786590576
Epoch 1330, training loss: 0.0683058649301529 = 0.001533409464173019 + 0.01 * 6.677245140075684
Epoch 1330, val loss: 1.2083641290664673
Epoch 1340, training loss: 0.06816086173057556 = 0.0015148725360631943 + 0.01 * 6.6645989418029785
Epoch 1340, val loss: 1.2098102569580078
Epoch 1350, training loss: 0.0684422180056572 = 0.0014968912582844496 + 0.01 * 6.694532871246338
Epoch 1350, val loss: 1.2111963033676147
Epoch 1360, training loss: 0.06809084117412567 = 0.0014793934533372521 + 0.01 * 6.661144733428955
Epoch 1360, val loss: 1.212565541267395
Epoch 1370, training loss: 0.06810025125741959 = 0.0014623847091570497 + 0.01 * 6.663786888122559
Epoch 1370, val loss: 1.21389901638031
Epoch 1380, training loss: 0.06823477149009705 = 0.0014458720106631517 + 0.01 * 6.678889751434326
Epoch 1380, val loss: 1.2152299880981445
Epoch 1390, training loss: 0.06826110184192657 = 0.0014298107707872987 + 0.01 * 6.68312931060791
Epoch 1390, val loss: 1.216470718383789
Epoch 1400, training loss: 0.06798270344734192 = 0.0014142111176624894 + 0.01 * 6.656849384307861
Epoch 1400, val loss: 1.21769380569458
Epoch 1410, training loss: 0.06786004453897476 = 0.0013990286970511079 + 0.01 * 6.646101951599121
Epoch 1410, val loss: 1.2189726829528809
Epoch 1420, training loss: 0.06795528531074524 = 0.0013842445332556963 + 0.01 * 6.657104015350342
Epoch 1420, val loss: 1.220137596130371
Epoch 1430, training loss: 0.06775396317243576 = 0.001369839534163475 + 0.01 * 6.638411998748779
Epoch 1430, val loss: 1.2213842868804932
Epoch 1440, training loss: 0.06806553900241852 = 0.001355818472802639 + 0.01 * 6.670971870422363
Epoch 1440, val loss: 1.2225520610809326
Epoch 1450, training loss: 0.06798559427261353 = 0.0013421736657619476 + 0.01 * 6.664342880249023
Epoch 1450, val loss: 1.2236753702163696
Epoch 1460, training loss: 0.06773465126752853 = 0.0013288905611261725 + 0.01 * 6.640575885772705
Epoch 1460, val loss: 1.2248095273971558
Epoch 1470, training loss: 0.06781595200300217 = 0.0013159462250769138 + 0.01 * 6.650001049041748
Epoch 1470, val loss: 1.2259008884429932
Epoch 1480, training loss: 0.06769700348377228 = 0.0013033398427069187 + 0.01 * 6.63936710357666
Epoch 1480, val loss: 1.2268956899642944
Epoch 1490, training loss: 0.06764303892850876 = 0.0012910929508507252 + 0.01 * 6.635194778442383
Epoch 1490, val loss: 1.228036642074585
Epoch 1500, training loss: 0.0678674727678299 = 0.0012791475746780634 + 0.01 * 6.658832550048828
Epoch 1500, val loss: 1.2290183305740356
Epoch 1510, training loss: 0.0676579475402832 = 0.0012674557510763407 + 0.01 * 6.639049530029297
Epoch 1510, val loss: 1.229992151260376
Epoch 1520, training loss: 0.06764617562294006 = 0.0012560747563838959 + 0.01 * 6.639010906219482
Epoch 1520, val loss: 1.2310656309127808
Epoch 1530, training loss: 0.0675259605050087 = 0.00124495686031878 + 0.01 * 6.628100395202637
Epoch 1530, val loss: 1.2320202589035034
Epoch 1540, training loss: 0.06752823293209076 = 0.0012341059045866132 + 0.01 * 6.629412651062012
Epoch 1540, val loss: 1.2329157590866089
Epoch 1550, training loss: 0.06740251928567886 = 0.0012235495960339904 + 0.01 * 6.617897033691406
Epoch 1550, val loss: 1.2339203357696533
Epoch 1560, training loss: 0.06736276298761368 = 0.0012131965486332774 + 0.01 * 6.614957332611084
Epoch 1560, val loss: 1.2347875833511353
Epoch 1570, training loss: 0.06762193143367767 = 0.0012030873913317919 + 0.01 * 6.6418843269348145
Epoch 1570, val loss: 1.2357473373413086
Epoch 1580, training loss: 0.06736582517623901 = 0.0011932140914723277 + 0.01 * 6.61726188659668
Epoch 1580, val loss: 1.2365493774414062
Epoch 1590, training loss: 0.06780964881181717 = 0.0011835957411676645 + 0.01 * 6.662604808807373
Epoch 1590, val loss: 1.2374216318130493
Epoch 1600, training loss: 0.06751706451177597 = 0.0011742326896637678 + 0.01 * 6.634283542633057
Epoch 1600, val loss: 1.2382652759552002
Epoch 1610, training loss: 0.06727802753448486 = 0.0011650696396827698 + 0.01 * 6.6112961769104
Epoch 1610, val loss: 1.2390824556350708
Epoch 1620, training loss: 0.06773442029953003 = 0.0011560841230675578 + 0.01 * 6.657833576202393
Epoch 1620, val loss: 1.239890456199646
Epoch 1630, training loss: 0.0671769306063652 = 0.001147323870100081 + 0.01 * 6.60296106338501
Epoch 1630, val loss: 1.2407118082046509
Epoch 1640, training loss: 0.06731683760881424 = 0.001138731138780713 + 0.01 * 6.6178107261657715
Epoch 1640, val loss: 1.2414535284042358
Epoch 1650, training loss: 0.06722696125507355 = 0.0011303649516776204 + 0.01 * 6.609659671783447
Epoch 1650, val loss: 1.242234230041504
Epoch 1660, training loss: 0.06706607341766357 = 0.001122155343182385 + 0.01 * 6.594391822814941
Epoch 1660, val loss: 1.2430788278579712
Epoch 1670, training loss: 0.06716349720954895 = 0.0011141389841213822 + 0.01 * 6.604936122894287
Epoch 1670, val loss: 1.2437679767608643
Epoch 1680, training loss: 0.06717823445796967 = 0.001106283743865788 + 0.01 * 6.6071953773498535
Epoch 1680, val loss: 1.2445356845855713
Epoch 1690, training loss: 0.06718207895755768 = 0.0010986299021169543 + 0.01 * 6.608345031738281
Epoch 1690, val loss: 1.2452489137649536
Epoch 1700, training loss: 0.06708710640668869 = 0.0010910824639722705 + 0.01 * 6.599602222442627
Epoch 1700, val loss: 1.2459161281585693
Epoch 1710, training loss: 0.06752356886863708 = 0.0010836711153388023 + 0.01 * 6.643989562988281
Epoch 1710, val loss: 1.2466979026794434
Epoch 1720, training loss: 0.06716495752334595 = 0.0010764579055830836 + 0.01 * 6.608850002288818
Epoch 1720, val loss: 1.24737548828125
Epoch 1730, training loss: 0.06694433093070984 = 0.0010693671647459269 + 0.01 * 6.587496757507324
Epoch 1730, val loss: 1.2480554580688477
Epoch 1740, training loss: 0.0671476200222969 = 0.0010623963316902518 + 0.01 * 6.608522891998291
Epoch 1740, val loss: 1.2487176656723022
Epoch 1750, training loss: 0.06706459075212479 = 0.0010555584449321032 + 0.01 * 6.600903034210205
Epoch 1750, val loss: 1.2493536472320557
Epoch 1760, training loss: 0.06714487820863724 = 0.0010488812113180757 + 0.01 * 6.609600067138672
Epoch 1760, val loss: 1.2500402927398682
Epoch 1770, training loss: 0.0668695718050003 = 0.0010423037456348538 + 0.01 * 6.58272647857666
Epoch 1770, val loss: 1.2506849765777588
Epoch 1780, training loss: 0.06712102144956589 = 0.001035883673466742 + 0.01 * 6.608513832092285
Epoch 1780, val loss: 1.2512989044189453
Epoch 1790, training loss: 0.0670187771320343 = 0.0010295568499714136 + 0.01 * 6.598921775817871
Epoch 1790, val loss: 1.2519052028656006
Epoch 1800, training loss: 0.06693251430988312 = 0.0010233543580397964 + 0.01 * 6.590916633605957
Epoch 1800, val loss: 1.2525264024734497
Epoch 1810, training loss: 0.06695766001939774 = 0.0010172477923333645 + 0.01 * 6.594041347503662
Epoch 1810, val loss: 1.2531274557113647
Epoch 1820, training loss: 0.06682819873094559 = 0.0010112636955454946 + 0.01 * 6.581693649291992
Epoch 1820, val loss: 1.253688097000122
Epoch 1830, training loss: 0.06707610934972763 = 0.001005382277071476 + 0.01 * 6.607072830200195
Epoch 1830, val loss: 1.2542866468429565
Epoch 1840, training loss: 0.06683173775672913 = 0.000999573734588921 + 0.01 * 6.583216190338135
Epoch 1840, val loss: 1.2548658847808838
Epoch 1850, training loss: 0.06680185347795486 = 0.0009939140873029828 + 0.01 * 6.580794334411621
Epoch 1850, val loss: 1.255454421043396
Epoch 1860, training loss: 0.06706583499908447 = 0.0009883457096293569 + 0.01 * 6.607748985290527
Epoch 1860, val loss: 1.2559630870819092
Epoch 1870, training loss: 0.06664066016674042 = 0.0009828463662415743 + 0.01 * 6.565782070159912
Epoch 1870, val loss: 1.2565252780914307
Epoch 1880, training loss: 0.06679001450538635 = 0.000977504882030189 + 0.01 * 6.581251621246338
Epoch 1880, val loss: 1.256989598274231
Epoch 1890, training loss: 0.06667152047157288 = 0.0009722307440824807 + 0.01 * 6.5699286460876465
Epoch 1890, val loss: 1.2575751543045044
Epoch 1900, training loss: 0.06678859889507294 = 0.0009670719155110419 + 0.01 * 6.582152843475342
Epoch 1900, val loss: 1.2581130266189575
Epoch 1910, training loss: 0.06669865548610687 = 0.000961966987233609 + 0.01 * 6.573668956756592
Epoch 1910, val loss: 1.2585307359695435
Epoch 1920, training loss: 0.06673093140125275 = 0.0009569647954776883 + 0.01 * 6.577396392822266
Epoch 1920, val loss: 1.259071707725525
Epoch 1930, training loss: 0.06653817743062973 = 0.0009520120220258832 + 0.01 * 6.558616638183594
Epoch 1930, val loss: 1.2595852613449097
Epoch 1940, training loss: 0.06657245755195618 = 0.0009471580269746482 + 0.01 * 6.562530040740967
Epoch 1940, val loss: 1.2600382566452026
Epoch 1950, training loss: 0.06642898917198181 = 0.0009423578158020973 + 0.01 * 6.54866361618042
Epoch 1950, val loss: 1.260550618171692
Epoch 1960, training loss: 0.0664752945303917 = 0.0009376378147862852 + 0.01 * 6.553765773773193
Epoch 1960, val loss: 1.2610234022140503
Epoch 1970, training loss: 0.06672962009906769 = 0.0009329983149655163 + 0.01 * 6.579662322998047
Epoch 1970, val loss: 1.2614668607711792
Epoch 1980, training loss: 0.06666155159473419 = 0.0009283808176405728 + 0.01 * 6.573317050933838
Epoch 1980, val loss: 1.2619181871414185
Epoch 1990, training loss: 0.06638702750205994 = 0.0009238794445991516 + 0.01 * 6.5463151931762695
Epoch 1990, val loss: 1.262447714805603
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 2.0388343334198 = 1.9528659582138062 + 0.01 * 8.596844673156738
Epoch 0, val loss: 1.9487446546554565
Epoch 10, training loss: 2.0282957553863525 = 1.9423279762268066 + 0.01 * 8.596780776977539
Epoch 10, val loss: 1.9383537769317627
Epoch 20, training loss: 2.014705181121826 = 1.928739309310913 + 0.01 * 8.59659481048584
Epoch 20, val loss: 1.9244260787963867
Epoch 30, training loss: 1.9951093196868896 = 1.909149408340454 + 0.01 * 8.595993041992188
Epoch 30, val loss: 1.903947353363037
Epoch 40, training loss: 1.9660117626190186 = 1.8800851106643677 + 0.01 * 8.592667579650879
Epoch 40, val loss: 1.8738675117492676
Epoch 50, training loss: 1.926343560218811 = 1.8406431674957275 + 0.01 * 8.57003402709961
Epoch 50, val loss: 1.8349915742874146
Epoch 60, training loss: 1.8846673965454102 = 1.7999091148376465 + 0.01 * 8.475833892822266
Epoch 60, val loss: 1.7994476556777954
Epoch 70, training loss: 1.8488576412200928 = 1.7669733762741089 + 0.01 * 8.188431739807129
Epoch 70, val loss: 1.7730220556259155
Epoch 80, training loss: 1.8056724071502686 = 1.7249693870544434 + 0.01 * 8.070306777954102
Epoch 80, val loss: 1.7360697984695435
Epoch 90, training loss: 1.745283842086792 = 1.6666538715362549 + 0.01 * 7.86299991607666
Epoch 90, val loss: 1.6844429969787598
Epoch 100, training loss: 1.6646995544433594 = 1.5881013870239258 + 0.01 * 7.659816265106201
Epoch 100, val loss: 1.616332769393921
Epoch 110, training loss: 1.5699669122695923 = 1.4956010580062866 + 0.01 * 7.436583042144775
Epoch 110, val loss: 1.5388379096984863
Epoch 120, training loss: 1.4731253385543823 = 1.3999080657958984 + 0.01 * 7.321725845336914
Epoch 120, val loss: 1.4596080780029297
Epoch 130, training loss: 1.3767778873443604 = 1.3039408922195435 + 0.01 * 7.283699035644531
Epoch 130, val loss: 1.3831651210784912
Epoch 140, training loss: 1.279525876045227 = 1.2070643901824951 + 0.01 * 7.246154308319092
Epoch 140, val loss: 1.3073889017105103
Epoch 150, training loss: 1.1821601390838623 = 1.1099742650985718 + 0.01 * 7.218588829040527
Epoch 150, val loss: 1.2318717241287231
Epoch 160, training loss: 1.0885006189346313 = 1.016446828842163 + 0.01 * 7.205379486083984
Epoch 160, val loss: 1.1609114408493042
Epoch 170, training loss: 1.0016542673110962 = 0.9296910166740417 + 0.01 * 7.196323394775391
Epoch 170, val loss: 1.0962218046188354
Epoch 180, training loss: 0.9219501614570618 = 0.8500934839248657 + 0.01 * 7.185669422149658
Epoch 180, val loss: 1.0369782447814941
Epoch 190, training loss: 0.8489773869514465 = 0.7772694826126099 + 0.01 * 7.170793056488037
Epoch 190, val loss: 0.9826048612594604
Epoch 200, training loss: 0.7827950716018677 = 0.711287796497345 + 0.01 * 7.150726795196533
Epoch 200, val loss: 0.9331915378570557
Epoch 210, training loss: 0.7230629324913025 = 0.651793897151947 + 0.01 * 7.126904487609863
Epoch 210, val loss: 0.8890831470489502
Epoch 220, training loss: 0.6682621240615845 = 0.5971929430961609 + 0.01 * 7.106919765472412
Epoch 220, val loss: 0.8499059677124023
Epoch 230, training loss: 0.6162804961204529 = 0.5453577637672424 + 0.01 * 7.092271327972412
Epoch 230, val loss: 0.814558744430542
Epoch 240, training loss: 0.5657656192779541 = 0.49492865800857544 + 0.01 * 7.083698749542236
Epoch 240, val loss: 0.7829998135566711
Epoch 250, training loss: 0.5166277289390564 = 0.44582775235176086 + 0.01 * 7.07999849319458
Epoch 250, val loss: 0.7557631134986877
Epoch 260, training loss: 0.4694458842277527 = 0.39866912364959717 + 0.01 * 7.0776777267456055
Epoch 260, val loss: 0.7331357002258301
Epoch 270, training loss: 0.42489728331565857 = 0.3541361093521118 + 0.01 * 7.076118469238281
Epoch 270, val loss: 0.7147048711776733
Epoch 280, training loss: 0.3834839165210724 = 0.31273674964904785 + 0.01 * 7.074717044830322
Epoch 280, val loss: 0.7001916766166687
Epoch 290, training loss: 0.3454896807670593 = 0.2747552692890167 + 0.01 * 7.073439598083496
Epoch 290, val loss: 0.6892654299736023
Epoch 300, training loss: 0.31106412410736084 = 0.24032719433307648 + 0.01 * 7.073692798614502
Epoch 300, val loss: 0.6817248463630676
Epoch 310, training loss: 0.28031519055366516 = 0.20959143340587616 + 0.01 * 7.072375297546387
Epoch 310, val loss: 0.6772997379302979
Epoch 320, training loss: 0.25332826375961304 = 0.1826157122850418 + 0.01 * 7.071253776550293
Epoch 320, val loss: 0.6757737994194031
Epoch 330, training loss: 0.22999712824821472 = 0.15928928554058075 + 0.01 * 7.070784568786621
Epoch 330, val loss: 0.6769040822982788
Epoch 340, training loss: 0.21001945436000824 = 0.13931547105312347 + 0.01 * 7.070398330688477
Epoch 340, val loss: 0.6803634166717529
Epoch 350, training loss: 0.1929897964000702 = 0.12229049205780029 + 0.01 * 7.069931507110596
Epoch 350, val loss: 0.6858408451080322
Epoch 360, training loss: 0.1784762740135193 = 0.10778181999921799 + 0.01 * 7.069446086883545
Epoch 360, val loss: 0.6929759979248047
Epoch 370, training loss: 0.16606909036636353 = 0.0953812226653099 + 0.01 * 7.068787574768066
Epoch 370, val loss: 0.7013748288154602
Epoch 380, training loss: 0.15541279315948486 = 0.08473236858844757 + 0.01 * 7.068042755126953
Epoch 380, val loss: 0.7107189893722534
Epoch 390, training loss: 0.14621388912200928 = 0.07554224133491516 + 0.01 * 7.067164897918701
Epoch 390, val loss: 0.7207552790641785
Epoch 400, training loss: 0.13823692500591278 = 0.06757533550262451 + 0.01 * 7.066159248352051
Epoch 400, val loss: 0.7312924861907959
Epoch 410, training loss: 0.13129675388336182 = 0.060642581433057785 + 0.01 * 7.065417766571045
Epoch 410, val loss: 0.7421696186065674
Epoch 420, training loss: 0.12522447109222412 = 0.05458966642618179 + 0.01 * 7.063479900360107
Epoch 420, val loss: 0.7532454133033752
Epoch 430, training loss: 0.1199067234992981 = 0.049283284693956375 + 0.01 * 7.062344074249268
Epoch 430, val loss: 0.7644691467285156
Epoch 440, training loss: 0.11520525813102722 = 0.04460683837532997 + 0.01 * 7.059842109680176
Epoch 440, val loss: 0.7758399844169617
Epoch 450, training loss: 0.11104737222194672 = 0.04046450927853584 + 0.01 * 7.058286190032959
Epoch 450, val loss: 0.7873369455337524
Epoch 460, training loss: 0.1073443815112114 = 0.03678528964519501 + 0.01 * 7.055909156799316
Epoch 460, val loss: 0.7989676594734192
Epoch 470, training loss: 0.10404927283525467 = 0.03351537138223648 + 0.01 * 7.0533905029296875
Epoch 470, val loss: 0.8106695413589478
Epoch 480, training loss: 0.10111195594072342 = 0.030609169974923134 + 0.01 * 7.050279140472412
Epoch 480, val loss: 0.8223742246627808
Epoch 490, training loss: 0.09855596721172333 = 0.028025418519973755 + 0.01 * 7.053055286407471
Epoch 490, val loss: 0.8340242505073547
Epoch 500, training loss: 0.09617134928703308 = 0.025727279484272003 + 0.01 * 7.044406890869141
Epoch 500, val loss: 0.8455881476402283
Epoch 510, training loss: 0.09407079964876175 = 0.023679791018366814 + 0.01 * 7.0391011238098145
Epoch 510, val loss: 0.8570190072059631
Epoch 520, training loss: 0.09223370254039764 = 0.021852267906069756 + 0.01 * 7.038143634796143
Epoch 520, val loss: 0.8682687282562256
Epoch 530, training loss: 0.0906171202659607 = 0.020218197256326675 + 0.01 * 7.039892673492432
Epoch 530, val loss: 0.8793365955352783
Epoch 540, training loss: 0.08900237083435059 = 0.01875443384051323 + 0.01 * 7.0247931480407715
Epoch 540, val loss: 0.8901652693748474
Epoch 550, training loss: 0.08763933181762695 = 0.01743970811367035 + 0.01 * 7.019962310791016
Epoch 550, val loss: 0.900726318359375
Epoch 560, training loss: 0.08658415079116821 = 0.016255535185337067 + 0.01 * 7.032861709594727
Epoch 560, val loss: 0.9110729694366455
Epoch 570, training loss: 0.08530932664871216 = 0.01518774963915348 + 0.01 * 7.012158393859863
Epoch 570, val loss: 0.9211536049842834
Epoch 580, training loss: 0.0842323899269104 = 0.01422115322202444 + 0.01 * 7.001123428344727
Epoch 580, val loss: 0.9309495687484741
Epoch 590, training loss: 0.08365070074796677 = 0.01334339939057827 + 0.01 * 7.0307297706604
Epoch 590, val loss: 0.9405033588409424
Epoch 600, training loss: 0.08251641690731049 = 0.01254591066390276 + 0.01 * 6.997050762176514
Epoch 600, val loss: 0.949806272983551
Epoch 610, training loss: 0.08155830204486847 = 0.011818292550742626 + 0.01 * 6.974001407623291
Epoch 610, val loss: 0.958783745765686
Epoch 620, training loss: 0.0808483138680458 = 0.011152321472764015 + 0.01 * 6.96959924697876
Epoch 620, val loss: 0.9675595760345459
Epoch 630, training loss: 0.080370232462883 = 0.010541416704654694 + 0.01 * 6.982882022857666
Epoch 630, val loss: 0.9760615825653076
Epoch 640, training loss: 0.07950463891029358 = 0.009981409646570683 + 0.01 * 6.952322959899902
Epoch 640, val loss: 0.984369695186615
Epoch 650, training loss: 0.07892721891403198 = 0.009465285576879978 + 0.01 * 6.946193695068359
Epoch 650, val loss: 0.9924110770225525
Epoch 660, training loss: 0.07837774604558945 = 0.008989046327769756 + 0.01 * 6.938870429992676
Epoch 660, val loss: 1.0002211332321167
Epoch 670, training loss: 0.0778670534491539 = 0.008548798970878124 + 0.01 * 6.931825637817383
Epoch 670, val loss: 1.007838487625122
Epoch 680, training loss: 0.07741174846887589 = 0.008141147904098034 + 0.01 * 6.927060604095459
Epoch 680, val loss: 1.0152180194854736
Epoch 690, training loss: 0.07697233557701111 = 0.007763183210045099 + 0.01 * 6.920915603637695
Epoch 690, val loss: 1.0224639177322388
Epoch 700, training loss: 0.0766378790140152 = 0.00741163594648242 + 0.01 * 6.922624111175537
Epoch 700, val loss: 1.0294854640960693
Epoch 710, training loss: 0.07633534073829651 = 0.007084371522068977 + 0.01 * 6.925097465515137
Epoch 710, val loss: 1.0363500118255615
Epoch 720, training loss: 0.07588781416416168 = 0.006779452785849571 + 0.01 * 6.910836219787598
Epoch 720, val loss: 1.0430116653442383
Epoch 730, training loss: 0.07572881877422333 = 0.006494616158306599 + 0.01 * 6.923420429229736
Epoch 730, val loss: 1.0494567155838013
Epoch 740, training loss: 0.07531527429819107 = 0.006228600163012743 + 0.01 * 6.90866756439209
Epoch 740, val loss: 1.0558191537857056
Epoch 750, training loss: 0.07492014765739441 = 0.005979675799608231 + 0.01 * 6.894047737121582
Epoch 750, val loss: 1.0619503259658813
Epoch 760, training loss: 0.07495048642158508 = 0.005746124312281609 + 0.01 * 6.920435905456543
Epoch 760, val loss: 1.0679197311401367
Epoch 770, training loss: 0.07445534318685532 = 0.005527452565729618 + 0.01 * 6.892789363861084
Epoch 770, val loss: 1.0737767219543457
Epoch 780, training loss: 0.07421500980854034 = 0.005321972072124481 + 0.01 * 6.889303684234619
Epoch 780, val loss: 1.0793850421905518
Epoch 790, training loss: 0.07394549250602722 = 0.005128463264554739 + 0.01 * 6.881702899932861
Epoch 790, val loss: 1.0849641561508179
Epoch 800, training loss: 0.07368991523981094 = 0.004946455359458923 + 0.01 * 6.8743462562561035
Epoch 800, val loss: 1.0902528762817383
Epoch 810, training loss: 0.07346946001052856 = 0.004775225650519133 + 0.01 * 6.8694233894348145
Epoch 810, val loss: 1.0955342054367065
Epoch 820, training loss: 0.07325457781553268 = 0.004613532684743404 + 0.01 * 6.864104747772217
Epoch 820, val loss: 1.1005841493606567
Epoch 830, training loss: 0.07351043820381165 = 0.004460589494556189 + 0.01 * 6.904984951019287
Epoch 830, val loss: 1.1055474281311035
Epoch 840, training loss: 0.07313293218612671 = 0.004316564183682203 + 0.01 * 6.8816375732421875
Epoch 840, val loss: 1.1103330850601196
Epoch 850, training loss: 0.07276978343725204 = 0.004180517978966236 + 0.01 * 6.858926773071289
Epoch 850, val loss: 1.1150295734405518
Epoch 860, training loss: 0.07258409261703491 = 0.0040515330620110035 + 0.01 * 6.853255748748779
Epoch 860, val loss: 1.119554042816162
Epoch 870, training loss: 0.07243691384792328 = 0.003929118160158396 + 0.01 * 6.850780010223389
Epoch 870, val loss: 1.1240726709365845
Epoch 880, training loss: 0.0721583217382431 = 0.0038129640743136406 + 0.01 * 6.834536075592041
Epoch 880, val loss: 1.1283342838287354
Epoch 890, training loss: 0.072127565741539 = 0.003702629590407014 + 0.01 * 6.842493534088135
Epoch 890, val loss: 1.1325979232788086
Epoch 900, training loss: 0.07201619446277618 = 0.0035979750100523233 + 0.01 * 6.841822147369385
Epoch 900, val loss: 1.1366653442382812
Epoch 910, training loss: 0.07191778719425201 = 0.0034983642399311066 + 0.01 * 6.841942310333252
Epoch 910, val loss: 1.1407233476638794
Epoch 920, training loss: 0.07167652994394302 = 0.003403622191399336 + 0.01 * 6.827291011810303
Epoch 920, val loss: 1.1445993185043335
Epoch 930, training loss: 0.07190441340208054 = 0.0033132105600088835 + 0.01 * 6.8591203689575195
Epoch 930, val loss: 1.1484663486480713
Epoch 940, training loss: 0.07138928025960922 = 0.0032273537945002317 + 0.01 * 6.816192626953125
Epoch 940, val loss: 1.1521868705749512
Epoch 950, training loss: 0.07149487733840942 = 0.0031453364063054323 + 0.01 * 6.834954261779785
Epoch 950, val loss: 1.1557787656784058
Epoch 960, training loss: 0.07118343561887741 = 0.003067238489165902 + 0.01 * 6.811619758605957
Epoch 960, val loss: 1.15938138961792
Epoch 970, training loss: 0.0710945576429367 = 0.002992540830746293 + 0.01 * 6.810202121734619
Epoch 970, val loss: 1.1627295017242432
Epoch 980, training loss: 0.07100743055343628 = 0.0029210997745394707 + 0.01 * 6.808633804321289
Epoch 980, val loss: 1.1661763191223145
Epoch 990, training loss: 0.0708426982164383 = 0.0028527257964015007 + 0.01 * 6.798997402191162
Epoch 990, val loss: 1.1693897247314453
Epoch 1000, training loss: 0.07069949805736542 = 0.0027872200589627028 + 0.01 * 6.791228294372559
Epoch 1000, val loss: 1.1726990938186646
Epoch 1010, training loss: 0.07076989114284515 = 0.002724563702940941 + 0.01 * 6.804533004760742
Epoch 1010, val loss: 1.1757179498672485
Epoch 1020, training loss: 0.07038222253322601 = 0.002664469415321946 + 0.01 * 6.771775722503662
Epoch 1020, val loss: 1.1788908243179321
Epoch 1030, training loss: 0.07056807726621628 = 0.0026067669969052076 + 0.01 * 6.796131134033203
Epoch 1030, val loss: 1.1818134784698486
Epoch 1040, training loss: 0.07031237334012985 = 0.0025515302550047636 + 0.01 * 6.7760844230651855
Epoch 1040, val loss: 1.1848328113555908
Epoch 1050, training loss: 0.07032628357410431 = 0.002498386427760124 + 0.01 * 6.782789707183838
Epoch 1050, val loss: 1.1876407861709595
Epoch 1060, training loss: 0.07011465728282928 = 0.002447449369356036 + 0.01 * 6.766721248626709
Epoch 1060, val loss: 1.1904501914978027
Epoch 1070, training loss: 0.07004230469465256 = 0.002398422919213772 + 0.01 * 6.764388561248779
Epoch 1070, val loss: 1.1932111978530884
Epoch 1080, training loss: 0.07009366154670715 = 0.002351215574890375 + 0.01 * 6.77424430847168
Epoch 1080, val loss: 1.1958580017089844
Epoch 1090, training loss: 0.06997901946306229 = 0.002305833389982581 + 0.01 * 6.767318248748779
Epoch 1090, val loss: 1.1985560655593872
Epoch 1100, training loss: 0.0698118582367897 = 0.002262040041387081 + 0.01 * 6.754981517791748
Epoch 1100, val loss: 1.2010668516159058
Epoch 1110, training loss: 0.06989490240812302 = 0.0022199456579983234 + 0.01 * 6.767495632171631
Epoch 1110, val loss: 1.2035772800445557
Epoch 1120, training loss: 0.06981724500656128 = 0.0021793972700834274 + 0.01 * 6.763784408569336
Epoch 1120, val loss: 1.2060824632644653
Epoch 1130, training loss: 0.06965923309326172 = 0.002140285214409232 + 0.01 * 6.751895427703857
Epoch 1130, val loss: 1.2084250450134277
Epoch 1140, training loss: 0.06981988251209259 = 0.0021025186870247126 + 0.01 * 6.7717366218566895
Epoch 1140, val loss: 1.2108525037765503
Epoch 1150, training loss: 0.06950902938842773 = 0.0020662059541791677 + 0.01 * 6.744283199310303
Epoch 1150, val loss: 1.213109016418457
Epoch 1160, training loss: 0.06970902532339096 = 0.002031143521890044 + 0.01 * 6.767788410186768
Epoch 1160, val loss: 1.2152776718139648
Epoch 1170, training loss: 0.06951854377985 = 0.0019973639864474535 + 0.01 * 6.752118110656738
Epoch 1170, val loss: 1.2175565958023071
Epoch 1180, training loss: 0.0693911761045456 = 0.0019646016880869865 + 0.01 * 6.7426581382751465
Epoch 1180, val loss: 1.2196499109268188
Epoch 1190, training loss: 0.0694042295217514 = 0.001932949060574174 + 0.01 * 6.747128486633301
Epoch 1190, val loss: 1.2217669486999512
Epoch 1200, training loss: 0.06928395479917526 = 0.0019024460343644023 + 0.01 * 6.7381510734558105
Epoch 1200, val loss: 1.2238078117370605
Epoch 1210, training loss: 0.06927809119224548 = 0.0018728550057858229 + 0.01 * 6.7405242919921875
Epoch 1210, val loss: 1.2258589267730713
Epoch 1220, training loss: 0.06917501240968704 = 0.001844262471422553 + 0.01 * 6.733075141906738
Epoch 1220, val loss: 1.2277418375015259
Epoch 1230, training loss: 0.06927713006734848 = 0.0018166045192629099 + 0.01 * 6.746052265167236
Epoch 1230, val loss: 1.2296910285949707
Epoch 1240, training loss: 0.06908977031707764 = 0.0017897745128720999 + 0.01 * 6.730000019073486
Epoch 1240, val loss: 1.2315672636032104
Epoch 1250, training loss: 0.0693773627281189 = 0.001763794687576592 + 0.01 * 6.761356353759766
Epoch 1250, val loss: 1.2333767414093018
Epoch 1260, training loss: 0.0690513476729393 = 0.0017386855324730277 + 0.01 * 6.731266498565674
Epoch 1260, val loss: 1.2351773977279663
Epoch 1270, training loss: 0.06908514350652695 = 0.0017143214354291558 + 0.01 * 6.737082481384277
Epoch 1270, val loss: 1.2368749380111694
Epoch 1280, training loss: 0.06908940523862839 = 0.0016907753888517618 + 0.01 * 6.739863395690918
Epoch 1280, val loss: 1.2386271953582764
Epoch 1290, training loss: 0.06887611001729965 = 0.0016678840620443225 + 0.01 * 6.720822811126709
Epoch 1290, val loss: 1.2402867078781128
Epoch 1300, training loss: 0.06895916163921356 = 0.0016456464072689414 + 0.01 * 6.731351375579834
Epoch 1300, val loss: 1.2419337034225464
Epoch 1310, training loss: 0.06874869763851166 = 0.0016241067787632346 + 0.01 * 6.712459087371826
Epoch 1310, val loss: 1.2435003519058228
Epoch 1320, training loss: 0.06884459406137466 = 0.0016031707637012005 + 0.01 * 6.724142074584961
Epoch 1320, val loss: 1.2450915575027466
Epoch 1330, training loss: 0.06871277093887329 = 0.0015828245086595416 + 0.01 * 6.712994575500488
Epoch 1330, val loss: 1.2466093301773071
Epoch 1340, training loss: 0.06889248639345169 = 0.001563160796649754 + 0.01 * 6.7329325675964355
Epoch 1340, val loss: 1.2480719089508057
Epoch 1350, training loss: 0.06870558857917786 = 0.001544095342978835 + 0.01 * 6.716149806976318
Epoch 1350, val loss: 1.2495583295822144
Epoch 1360, training loss: 0.06869398802518845 = 0.0015255373436957598 + 0.01 * 6.7168450355529785
Epoch 1360, val loss: 1.2509812116622925
Epoch 1370, training loss: 0.06855762749910355 = 0.0015075275441631675 + 0.01 * 6.705010414123535
Epoch 1370, val loss: 1.2523105144500732
Epoch 1380, training loss: 0.0685848519206047 = 0.001490010879933834 + 0.01 * 6.709484100341797
Epoch 1380, val loss: 1.2537059783935547
Epoch 1390, training loss: 0.06854198127985 = 0.0014729119138792157 + 0.01 * 6.706907272338867
Epoch 1390, val loss: 1.2549947500228882
Epoch 1400, training loss: 0.06837928295135498 = 0.001456314348615706 + 0.01 * 6.692297458648682
Epoch 1400, val loss: 1.2563247680664062
Epoch 1410, training loss: 0.0686352401971817 = 0.0014401869848370552 + 0.01 * 6.7195048332214355
Epoch 1410, val loss: 1.2574999332427979
Epoch 1420, training loss: 0.06832963228225708 = 0.0014246053760871291 + 0.01 * 6.690503120422363
Epoch 1420, val loss: 1.2588027715682983
Epoch 1430, training loss: 0.06861771643161774 = 0.0014093826757743955 + 0.01 * 6.7208333015441895
Epoch 1430, val loss: 1.2599263191223145
Epoch 1440, training loss: 0.06841757148504257 = 0.0013946805847808719 + 0.01 * 6.70228910446167
Epoch 1440, val loss: 1.2611254453659058
Epoch 1450, training loss: 0.06843201071023941 = 0.001380292815156281 + 0.01 * 6.705171585083008
Epoch 1450, val loss: 1.262233853340149
Epoch 1460, training loss: 0.06819016486406326 = 0.0013663549907505512 + 0.01 * 6.6823811531066895
Epoch 1460, val loss: 1.2633264064788818
Epoch 1470, training loss: 0.06833784282207489 = 0.0013527731643989682 + 0.01 * 6.698507308959961
Epoch 1470, val loss: 1.26435387134552
Epoch 1480, training loss: 0.06836120784282684 = 0.0013395866844803095 + 0.01 * 6.702162742614746
Epoch 1480, val loss: 1.265402913093567
Epoch 1490, training loss: 0.06809293478727341 = 0.0013267087051644921 + 0.01 * 6.6766228675842285
Epoch 1490, val loss: 1.2663828134536743
Epoch 1500, training loss: 0.0684979185461998 = 0.0013141166418790817 + 0.01 * 6.718380451202393
Epoch 1500, val loss: 1.267427682876587
Epoch 1510, training loss: 0.06819906085729599 = 0.001301865908317268 + 0.01 * 6.6897196769714355
Epoch 1510, val loss: 1.2683125734329224
Epoch 1520, training loss: 0.06825464218854904 = 0.0012898973654955626 + 0.01 * 6.696475028991699
Epoch 1520, val loss: 1.2692971229553223
Epoch 1530, training loss: 0.06806609034538269 = 0.001278233015909791 + 0.01 * 6.678785800933838
Epoch 1530, val loss: 1.2702299356460571
Epoch 1540, training loss: 0.06815357506275177 = 0.0012668335111811757 + 0.01 * 6.688673973083496
Epoch 1540, val loss: 1.2710906267166138
Epoch 1550, training loss: 0.06790368258953094 = 0.0012557378504425287 + 0.01 * 6.664794921875
Epoch 1550, val loss: 1.2720036506652832
Epoch 1560, training loss: 0.06828901916742325 = 0.0012448779307305813 + 0.01 * 6.704414367675781
Epoch 1560, val loss: 1.2728146314620972
Epoch 1570, training loss: 0.0678396224975586 = 0.0012343466514721513 + 0.01 * 6.660528182983398
Epoch 1570, val loss: 1.2736531496047974
Epoch 1580, training loss: 0.06823417544364929 = 0.0012240464566275477 + 0.01 * 6.701013088226318
Epoch 1580, val loss: 1.274434208869934
Epoch 1590, training loss: 0.06786511093378067 = 0.00121403019875288 + 0.01 * 6.665108680725098
Epoch 1590, val loss: 1.2752399444580078
Epoch 1600, training loss: 0.06786122173070908 = 0.0012042134767398238 + 0.01 * 6.665700912475586
Epoch 1600, val loss: 1.2759888172149658
Epoch 1610, training loss: 0.06808986514806747 = 0.0011946436716243625 + 0.01 * 6.6895222663879395
Epoch 1610, val loss: 1.2767590284347534
Epoch 1620, training loss: 0.06787596642971039 = 0.0011852543102577329 + 0.01 * 6.669071197509766
Epoch 1620, val loss: 1.2775094509124756
Epoch 1630, training loss: 0.06785563379526138 = 0.0011760834604501724 + 0.01 * 6.66795539855957
Epoch 1630, val loss: 1.2781801223754883
Epoch 1640, training loss: 0.0679941326379776 = 0.0011670832755044103 + 0.01 * 6.682704925537109
Epoch 1640, val loss: 1.2789678573608398
Epoch 1650, training loss: 0.06763356924057007 = 0.0011583090526983142 + 0.01 * 6.647525787353516
Epoch 1650, val loss: 1.2795604467391968
Epoch 1660, training loss: 0.06790568679571152 = 0.0011497663799673319 + 0.01 * 6.675591945648193
Epoch 1660, val loss: 1.2802722454071045
Epoch 1670, training loss: 0.06760475784540176 = 0.001141403685323894 + 0.01 * 6.646335601806641
Epoch 1670, val loss: 1.280903935432434
Epoch 1680, training loss: 0.06786158680915833 = 0.001133214682340622 + 0.01 * 6.672836780548096
Epoch 1680, val loss: 1.2815895080566406
Epoch 1690, training loss: 0.06787898391485214 = 0.001125185051932931 + 0.01 * 6.675380229949951
Epoch 1690, val loss: 1.2821828126907349
Epoch 1700, training loss: 0.06749293953180313 = 0.00111736252438277 + 0.01 * 6.6375579833984375
Epoch 1700, val loss: 1.2827978134155273
Epoch 1710, training loss: 0.06787178665399551 = 0.0011096700327470899 + 0.01 * 6.676212310791016
Epoch 1710, val loss: 1.2834826707839966
Epoch 1720, training loss: 0.06738168746232986 = 0.001102129346691072 + 0.01 * 6.627956390380859
Epoch 1720, val loss: 1.283966302871704
Epoch 1730, training loss: 0.06765202432870865 = 0.0010947576956823468 + 0.01 * 6.655726909637451
Epoch 1730, val loss: 1.2846122980117798
Epoch 1740, training loss: 0.06764698028564453 = 0.001087547978386283 + 0.01 * 6.655943870544434
Epoch 1740, val loss: 1.2851532697677612
Epoch 1750, training loss: 0.06748804450035095 = 0.0010804841294884682 + 0.01 * 6.640756130218506
Epoch 1750, val loss: 1.285683512687683
Epoch 1760, training loss: 0.06731529533863068 = 0.0010735521791502833 + 0.01 * 6.624174118041992
Epoch 1760, val loss: 1.2862772941589355
Epoch 1770, training loss: 0.0672885850071907 = 0.0010667433962225914 + 0.01 * 6.6221842765808105
Epoch 1770, val loss: 1.286760926246643
Epoch 1780, training loss: 0.06751511991024017 = 0.0010600608075037599 + 0.01 * 6.645506381988525
Epoch 1780, val loss: 1.2872730493545532
Epoch 1790, training loss: 0.06733950227499008 = 0.0010535013861954212 + 0.01 * 6.628600120544434
Epoch 1790, val loss: 1.2878668308258057
Epoch 1800, training loss: 0.06732170283794403 = 0.0010470389388501644 + 0.01 * 6.627466678619385
Epoch 1800, val loss: 1.2882641553878784
Epoch 1810, training loss: 0.0672951191663742 = 0.0010407412191852927 + 0.01 * 6.625438213348389
Epoch 1810, val loss: 1.2888234853744507
Epoch 1820, training loss: 0.06732158362865448 = 0.0010345146292820573 + 0.01 * 6.628706932067871
Epoch 1820, val loss: 1.2893054485321045
Epoch 1830, training loss: 0.06759649515151978 = 0.001028402359224856 + 0.01 * 6.656809329986572
Epoch 1830, val loss: 1.2896860837936401
Epoch 1840, training loss: 0.06738322228193283 = 0.0010224514408037066 + 0.01 * 6.636077404022217
Epoch 1840, val loss: 1.2902097702026367
Epoch 1850, training loss: 0.06724874675273895 = 0.0010165689745917916 + 0.01 * 6.623217582702637
Epoch 1850, val loss: 1.2906526327133179
Epoch 1860, training loss: 0.06723631173372269 = 0.0010107847629114985 + 0.01 * 6.62255334854126
Epoch 1860, val loss: 1.2910776138305664
Epoch 1870, training loss: 0.06742062419652939 = 0.0010050969431176782 + 0.01 * 6.641552448272705
Epoch 1870, val loss: 1.2915478944778442
Epoch 1880, training loss: 0.06714603304862976 = 0.0009994907304644585 + 0.01 * 6.614655017852783
Epoch 1880, val loss: 1.2919273376464844
Epoch 1890, training loss: 0.06700566411018372 = 0.0009939986048266292 + 0.01 * 6.60116720199585
Epoch 1890, val loss: 1.2924116849899292
Epoch 1900, training loss: 0.06716552376747131 = 0.0009885943727567792 + 0.01 * 6.617692947387695
Epoch 1900, val loss: 1.2927390336990356
Epoch 1910, training loss: 0.06730974465608597 = 0.0009833149379119277 + 0.01 * 6.632643699645996
Epoch 1910, val loss: 1.2932261228561401
Epoch 1920, training loss: 0.06699629873037338 = 0.0009781008120626211 + 0.01 * 6.601820468902588
Epoch 1920, val loss: 1.2935848236083984
Epoch 1930, training loss: 0.06724219024181366 = 0.0009729756275191903 + 0.01 * 6.6269211769104
Epoch 1930, val loss: 1.2939528226852417
Epoch 1940, training loss: 0.06709058582782745 = 0.0009679356007836759 + 0.01 * 6.612265586853027
Epoch 1940, val loss: 1.2943693399429321
Epoch 1950, training loss: 0.06698542833328247 = 0.0009629486012272537 + 0.01 * 6.602247714996338
Epoch 1950, val loss: 1.2947108745574951
Epoch 1960, training loss: 0.06704899668693542 = 0.0009580624755471945 + 0.01 * 6.60909366607666
Epoch 1960, val loss: 1.2951581478118896
Epoch 1970, training loss: 0.06695180386304855 = 0.0009531861287541687 + 0.01 * 6.599862575531006
Epoch 1970, val loss: 1.295499324798584
Epoch 1980, training loss: 0.06694886833429337 = 0.0009484171168878675 + 0.01 * 6.600045680999756
Epoch 1980, val loss: 1.2959632873535156
Epoch 1990, training loss: 0.06697799265384674 = 0.0009436918189749122 + 0.01 * 6.603430271148682
Epoch 1990, val loss: 1.2963591814041138
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 2.03031063079834 = 1.9443422555923462 + 0.01 * 8.596846580505371
Epoch 0, val loss: 1.9438632726669312
Epoch 10, training loss: 2.0200719833374023 = 1.934104084968567 + 0.01 * 8.596781730651855
Epoch 10, val loss: 1.933985710144043
Epoch 20, training loss: 2.007483720779419 = 1.921518087387085 + 0.01 * 8.596555709838867
Epoch 20, val loss: 1.921280026435852
Epoch 30, training loss: 1.9899600744247437 = 1.9040024280548096 + 0.01 * 8.595766067504883
Epoch 30, val loss: 1.9032154083251953
Epoch 40, training loss: 1.9640308618545532 = 1.8781250715255737 + 0.01 * 8.590575218200684
Epoch 40, val loss: 1.8766071796417236
Epoch 50, training loss: 1.9273531436920166 = 1.8418428897857666 + 0.01 * 8.551027297973633
Epoch 50, val loss: 1.8407093286514282
Epoch 60, training loss: 1.884203314781189 = 1.8008276224136353 + 0.01 * 8.337569236755371
Epoch 60, val loss: 1.8038145303726196
Epoch 70, training loss: 1.8453545570373535 = 1.7643003463745117 + 0.01 * 8.105424880981445
Epoch 70, val loss: 1.772640585899353
Epoch 80, training loss: 1.797325849533081 = 1.7182012796401978 + 0.01 * 7.912454128265381
Epoch 80, val loss: 1.7303577661514282
Epoch 90, training loss: 1.731138825416565 = 1.6537307500839233 + 0.01 * 7.740802764892578
Epoch 90, val loss: 1.6730804443359375
Epoch 100, training loss: 1.6456173658370972 = 1.5694942474365234 + 0.01 * 7.612310409545898
Epoch 100, val loss: 1.6020326614379883
Epoch 110, training loss: 1.5495965480804443 = 1.4751183986663818 + 0.01 * 7.447812080383301
Epoch 110, val loss: 1.5229419469833374
Epoch 120, training loss: 1.4551990032196045 = 1.3813449144363403 + 0.01 * 7.385407447814941
Epoch 120, val loss: 1.4471089839935303
Epoch 130, training loss: 1.365496277809143 = 1.2922194004058838 + 0.01 * 7.327693462371826
Epoch 130, val loss: 1.3781031370162964
Epoch 140, training loss: 1.2807868719100952 = 1.2079088687896729 + 0.01 * 7.287797451019287
Epoch 140, val loss: 1.3154903650283813
Epoch 150, training loss: 1.1996266841888428 = 1.1270887851715088 + 0.01 * 7.253791332244873
Epoch 150, val loss: 1.257188081741333
Epoch 160, training loss: 1.1196637153625488 = 1.047351598739624 + 0.01 * 7.231215953826904
Epoch 160, val loss: 1.1999272108078003
Epoch 170, training loss: 1.0395532846450806 = 0.967369794845581 + 0.01 * 7.218348979949951
Epoch 170, val loss: 1.1406363248825073
Epoch 180, training loss: 0.9599359631538391 = 0.8878968358039856 + 0.01 * 7.203910827636719
Epoch 180, val loss: 1.0798976421356201
Epoch 190, training loss: 0.8827192783355713 = 0.8108876347541809 + 0.01 * 7.183165073394775
Epoch 190, val loss: 1.019500732421875
Epoch 200, training loss: 0.8099371194839478 = 0.7383427023887634 + 0.01 * 7.159444808959961
Epoch 200, val loss: 0.9620541930198669
Epoch 210, training loss: 0.7428154945373535 = 0.6714133024215698 + 0.01 * 7.140218257904053
Epoch 210, val loss: 0.9095702171325684
Epoch 220, training loss: 0.6807058453559875 = 0.6094949245452881 + 0.01 * 7.121092319488525
Epoch 220, val loss: 0.8628489375114441
Epoch 230, training loss: 0.6221420764923096 = 0.5510437488555908 + 0.01 * 7.109835147857666
Epoch 230, val loss: 0.8219723701477051
Epoch 240, training loss: 0.5657179951667786 = 0.49474015831947327 + 0.01 * 7.097784519195557
Epoch 240, val loss: 0.786664605140686
Epoch 250, training loss: 0.5110887289047241 = 0.440167635679245 + 0.01 * 7.092111110687256
Epoch 250, val loss: 0.756127119064331
Epoch 260, training loss: 0.45887821912765503 = 0.38801631331443787 + 0.01 * 7.0861921310424805
Epoch 260, val loss: 0.7307579517364502
Epoch 270, training loss: 0.4102879762649536 = 0.33947044610977173 + 0.01 * 7.081751823425293
Epoch 270, val loss: 0.7108086347579956
Epoch 280, training loss: 0.3662033975124359 = 0.29542168974876404 + 0.01 * 7.078171730041504
Epoch 280, val loss: 0.6963054537773132
Epoch 290, training loss: 0.32704371213912964 = 0.2563067078590393 + 0.01 * 7.073699951171875
Epoch 290, val loss: 0.6865226030349731
Epoch 300, training loss: 0.2929081618785858 = 0.22225433588027954 + 0.01 * 7.065383434295654
Epoch 300, val loss: 0.6807869076728821
Epoch 310, training loss: 0.26369741559028625 = 0.19306369125843048 + 0.01 * 7.063372611999512
Epoch 310, val loss: 0.678161084651947
Epoch 320, training loss: 0.2388007938861847 = 0.16825206577777863 + 0.01 * 7.054872035980225
Epoch 320, val loss: 0.6780052185058594
Epoch 330, training loss: 0.2176712453365326 = 0.14718669652938843 + 0.01 * 7.048454284667969
Epoch 330, val loss: 0.6799517273902893
Epoch 340, training loss: 0.1997472643852234 = 0.1292649209499359 + 0.01 * 7.048234939575195
Epoch 340, val loss: 0.6836181879043579
Epoch 350, training loss: 0.18433165550231934 = 0.11395680904388428 + 0.01 * 7.0374836921691895
Epoch 350, val loss: 0.6886922121047974
Epoch 360, training loss: 0.17112389206886292 = 0.10082005709409714 + 0.01 * 7.030384540557861
Epoch 360, val loss: 0.6949776411056519
Epoch 370, training loss: 0.1597113013267517 = 0.08949902653694153 + 0.01 * 7.0212273597717285
Epoch 370, val loss: 0.7022323608398438
Epoch 380, training loss: 0.15001173317432404 = 0.07970672100782394 + 0.01 * 7.030501365661621
Epoch 380, val loss: 0.7102372050285339
Epoch 390, training loss: 0.14131379127502441 = 0.07121797651052475 + 0.01 * 7.009580612182617
Epoch 390, val loss: 0.7187635898590088
Epoch 400, training loss: 0.1338510513305664 = 0.06383218616247177 + 0.01 * 7.001885890960693
Epoch 400, val loss: 0.7277379631996155
Epoch 410, training loss: 0.12733222544193268 = 0.05738811567425728 + 0.01 * 6.994411468505859
Epoch 410, val loss: 0.7370730638504028
Epoch 420, training loss: 0.12178221344947815 = 0.05175323784351349 + 0.01 * 7.0028977394104
Epoch 420, val loss: 0.7466550469398499
Epoch 430, training loss: 0.11666092276573181 = 0.04681916534900665 + 0.01 * 6.984175682067871
Epoch 430, val loss: 0.7564122676849365
Epoch 440, training loss: 0.11226744949817657 = 0.04248708486557007 + 0.01 * 6.978036403656006
Epoch 440, val loss: 0.7662310004234314
Epoch 450, training loss: 0.10837617516517639 = 0.03867395222187042 + 0.01 * 6.970222473144531
Epoch 450, val loss: 0.7760686874389648
Epoch 460, training loss: 0.10502837598323822 = 0.03530944883823395 + 0.01 * 6.971892833709717
Epoch 460, val loss: 0.7858462333679199
Epoch 470, training loss: 0.10196320712566376 = 0.03233487531542778 + 0.01 * 6.962833404541016
Epoch 470, val loss: 0.7955568432807922
Epoch 480, training loss: 0.09926756471395493 = 0.02969861775636673 + 0.01 * 6.956894874572754
Epoch 480, val loss: 0.8051267266273499
Epoch 490, training loss: 0.09684374928474426 = 0.02735930308699608 + 0.01 * 6.9484453201293945
Epoch 490, val loss: 0.8145480155944824
Epoch 500, training loss: 0.09471333771944046 = 0.025273920968174934 + 0.01 * 6.943942070007324
Epoch 500, val loss: 0.8238019347190857
Epoch 510, training loss: 0.09273830056190491 = 0.023406624794006348 + 0.01 * 6.933167457580566
Epoch 510, val loss: 0.8329075574874878
Epoch 520, training loss: 0.09114450216293335 = 0.02173025719821453 + 0.01 * 6.941424369812012
Epoch 520, val loss: 0.8418117165565491
Epoch 530, training loss: 0.0894656777381897 = 0.020224515348672867 + 0.01 * 6.9241156578063965
Epoch 530, val loss: 0.8504886627197266
Epoch 540, training loss: 0.08807356655597687 = 0.01886928454041481 + 0.01 * 6.92042875289917
Epoch 540, val loss: 0.8589779734611511
Epoch 550, training loss: 0.08670349419116974 = 0.017644604668021202 + 0.01 * 6.90588903427124
Epoch 550, val loss: 0.8672666549682617
Epoch 560, training loss: 0.0856422483921051 = 0.016535446047782898 + 0.01 * 6.910680294036865
Epoch 560, val loss: 0.8753534555435181
Epoch 570, training loss: 0.084811732172966 = 0.015528969466686249 + 0.01 * 6.928276062011719
Epoch 570, val loss: 0.8831847310066223
Epoch 580, training loss: 0.0835021510720253 = 0.014615150168538094 + 0.01 * 6.888700485229492
Epoch 580, val loss: 0.8908847570419312
Epoch 590, training loss: 0.08261162042617798 = 0.013781790621578693 + 0.01 * 6.882983207702637
Epoch 590, val loss: 0.8983554840087891
Epoch 600, training loss: 0.0820252001285553 = 0.013019292615354061 + 0.01 * 6.900590896606445
Epoch 600, val loss: 0.905582070350647
Epoch 610, training loss: 0.08115273714065552 = 0.01232090499252081 + 0.01 * 6.883183002471924
Epoch 610, val loss: 0.9126866459846497
Epoch 620, training loss: 0.08044085651636124 = 0.011678500100970268 + 0.01 * 6.8762359619140625
Epoch 620, val loss: 0.9196308851242065
Epoch 630, training loss: 0.07975055277347565 = 0.01108735054731369 + 0.01 * 6.866320610046387
Epoch 630, val loss: 0.9263666868209839
Epoch 640, training loss: 0.0791267529129982 = 0.010541687719523907 + 0.01 * 6.858506679534912
Epoch 640, val loss: 0.9329097270965576
Epoch 650, training loss: 0.07853788882493973 = 0.010037724860012531 + 0.01 * 6.850016117095947
Epoch 650, val loss: 0.9392916560173035
Epoch 660, training loss: 0.07804203033447266 = 0.009570940397679806 + 0.01 * 6.847108840942383
Epoch 660, val loss: 0.9455462694168091
Epoch 670, training loss: 0.07774953544139862 = 0.009137460961937904 + 0.01 * 6.861207485198975
Epoch 670, val loss: 0.9515370726585388
Epoch 680, training loss: 0.07730475068092346 = 0.008736121468245983 + 0.01 * 6.856863498687744
Epoch 680, val loss: 0.9575268626213074
Epoch 690, training loss: 0.07675215601921082 = 0.008362713269889355 + 0.01 * 6.838944435119629
Epoch 690, val loss: 0.9632899761199951
Epoch 700, training loss: 0.07639026641845703 = 0.008014154620468616 + 0.01 * 6.837610721588135
Epoch 700, val loss: 0.9688904881477356
Epoch 710, training loss: 0.07609004527330399 = 0.0076882545836269855 + 0.01 * 6.840178966522217
Epoch 710, val loss: 0.9743754267692566
Epoch 720, training loss: 0.07582809031009674 = 0.007383512798696756 + 0.01 * 6.844458103179932
Epoch 720, val loss: 0.9797559976577759
Epoch 730, training loss: 0.07537248730659485 = 0.00709842424839735 + 0.01 * 6.827406406402588
Epoch 730, val loss: 0.9850183725357056
Epoch 740, training loss: 0.07498534768819809 = 0.006830668076872826 + 0.01 * 6.815467834472656
Epoch 740, val loss: 0.9901025891304016
Epoch 750, training loss: 0.07478979974985123 = 0.006579552311450243 + 0.01 * 6.8210248947143555
Epoch 750, val loss: 0.9950425028800964
Epoch 760, training loss: 0.07442046701908112 = 0.0063437228091061115 + 0.01 * 6.807674884796143
Epoch 760, val loss: 0.9999575018882751
Epoch 770, training loss: 0.0741671696305275 = 0.006121431477367878 + 0.01 * 6.8045735359191895
Epoch 770, val loss: 1.0047358274459839
Epoch 780, training loss: 0.07449938356876373 = 0.00591205433011055 + 0.01 * 6.858733177185059
Epoch 780, val loss: 1.0093048810958862
Epoch 790, training loss: 0.07385274767875671 = 0.005714669823646545 + 0.01 * 6.813807964324951
Epoch 790, val loss: 1.013925552368164
Epoch 800, training loss: 0.07347804307937622 = 0.005528480280190706 + 0.01 * 6.794956207275391
Epoch 800, val loss: 1.0183285474777222
Epoch 810, training loss: 0.07341369241476059 = 0.005352289415895939 + 0.01 * 6.806140899658203
Epoch 810, val loss: 1.0226719379425049
Epoch 820, training loss: 0.07327855378389359 = 0.005185409914702177 + 0.01 * 6.809314727783203
Epoch 820, val loss: 1.0268809795379639
Epoch 830, training loss: 0.07302520424127579 = 0.0050277793779969215 + 0.01 * 6.799742698669434
Epoch 830, val loss: 1.031021237373352
Epoch 840, training loss: 0.07279374450445175 = 0.0048777940683066845 + 0.01 * 6.791595458984375
Epoch 840, val loss: 1.0351210832595825
Epoch 850, training loss: 0.07253192365169525 = 0.004735576454550028 + 0.01 * 6.779634952545166
Epoch 850, val loss: 1.0390610694885254
Epoch 860, training loss: 0.07244592159986496 = 0.004600067622959614 + 0.01 * 6.784585475921631
Epoch 860, val loss: 1.0429471731185913
Epoch 870, training loss: 0.07232747972011566 = 0.004471552558243275 + 0.01 * 6.785592555999756
Epoch 870, val loss: 1.0466867685317993
Epoch 880, training loss: 0.07217960804700851 = 0.004349320195615292 + 0.01 * 6.783028602600098
Epoch 880, val loss: 1.050441026687622
Epoch 890, training loss: 0.07194088399410248 = 0.004232986830174923 + 0.01 * 6.770789623260498
Epoch 890, val loss: 1.0540978908538818
Epoch 900, training loss: 0.07182882726192474 = 0.004121863283216953 + 0.01 * 6.770696640014648
Epoch 900, val loss: 1.05758798122406
Epoch 910, training loss: 0.07171095162630081 = 0.004015909507870674 + 0.01 * 6.769504547119141
Epoch 910, val loss: 1.061096429824829
Epoch 920, training loss: 0.07158845663070679 = 0.003914981614798307 + 0.01 * 6.76734733581543
Epoch 920, val loss: 1.0644969940185547
Epoch 930, training loss: 0.07170607149600983 = 0.003818244906142354 + 0.01 * 6.788782596588135
Epoch 930, val loss: 1.0678635835647583
Epoch 940, training loss: 0.07149955630302429 = 0.003725755261257291 + 0.01 * 6.777380466461182
Epoch 940, val loss: 1.0710639953613281
Epoch 950, training loss: 0.07124707102775574 = 0.003637472167611122 + 0.01 * 6.760960102081299
Epoch 950, val loss: 1.0742664337158203
Epoch 960, training loss: 0.07106029987335205 = 0.0035529322922229767 + 0.01 * 6.750737190246582
Epoch 960, val loss: 1.0774226188659668
Epoch 970, training loss: 0.07108917832374573 = 0.0034717584494501352 + 0.01 * 6.761741638183594
Epoch 970, val loss: 1.0804290771484375
Epoch 980, training loss: 0.07098953425884247 = 0.0033942689187824726 + 0.01 * 6.75952672958374
Epoch 980, val loss: 1.0834572315216064
Epoch 990, training loss: 0.07085414975881577 = 0.003319650888442993 + 0.01 * 6.753450393676758
Epoch 990, val loss: 1.086384654045105
Epoch 1000, training loss: 0.07085920125246048 = 0.0032481420785188675 + 0.01 * 6.761106491088867
Epoch 1000, val loss: 1.0892798900604248
Epoch 1010, training loss: 0.07056857645511627 = 0.0031794768292456865 + 0.01 * 6.73891019821167
Epoch 1010, val loss: 1.0920941829681396
Epoch 1020, training loss: 0.07043410837650299 = 0.0031133927404880524 + 0.01 * 6.732071399688721
Epoch 1020, val loss: 1.094832181930542
Epoch 1030, training loss: 0.07052324712276459 = 0.0030499363783746958 + 0.01 * 6.747330665588379
Epoch 1030, val loss: 1.097565770149231
Epoch 1040, training loss: 0.07041645795106888 = 0.002988958265632391 + 0.01 * 6.74275016784668
Epoch 1040, val loss: 1.1001906394958496
Epoch 1050, training loss: 0.0702572762966156 = 0.0029302092734724283 + 0.01 * 6.732706546783447
Epoch 1050, val loss: 1.1028128862380981
Epoch 1060, training loss: 0.07027355581521988 = 0.002873585792258382 + 0.01 * 6.739996910095215
Epoch 1060, val loss: 1.1053234338760376
Epoch 1070, training loss: 0.07022350281476974 = 0.0028191593009978533 + 0.01 * 6.740434646606445
Epoch 1070, val loss: 1.1078498363494873
Epoch 1080, training loss: 0.07011166214942932 = 0.002766391960904002 + 0.01 * 6.734527111053467
Epoch 1080, val loss: 1.1102465391159058
Epoch 1090, training loss: 0.07037857919931412 = 0.002715847222134471 + 0.01 * 6.766273021697998
Epoch 1090, val loss: 1.1126796007156372
Epoch 1100, training loss: 0.0700407549738884 = 0.002667155582457781 + 0.01 * 6.737359523773193
Epoch 1100, val loss: 1.115045428276062
Epoch 1110, training loss: 0.06991622596979141 = 0.0026200469583272934 + 0.01 * 6.729617595672607
Epoch 1110, val loss: 1.1173739433288574
Epoch 1120, training loss: 0.06974083185195923 = 0.002574679907411337 + 0.01 * 6.716615200042725
Epoch 1120, val loss: 1.1196168661117554
Epoch 1130, training loss: 0.06979444622993469 = 0.002530787605792284 + 0.01 * 6.72636604309082
Epoch 1130, val loss: 1.1218258142471313
Epoch 1140, training loss: 0.06989564001560211 = 0.0024883649311959743 + 0.01 * 6.740727424621582
Epoch 1140, val loss: 1.124052882194519
Epoch 1150, training loss: 0.0697043240070343 = 0.0024473483208566904 + 0.01 * 6.725697040557861
Epoch 1150, val loss: 1.1261625289916992
Epoch 1160, training loss: 0.06959504634141922 = 0.002407616702839732 + 0.01 * 6.718742847442627
Epoch 1160, val loss: 1.1282923221588135
Epoch 1170, training loss: 0.06958445906639099 = 0.0023693679831922054 + 0.01 * 6.7215094566345215
Epoch 1170, val loss: 1.1302882432937622
Epoch 1180, training loss: 0.06952444463968277 = 0.0023321015760302544 + 0.01 * 6.719234466552734
Epoch 1180, val loss: 1.132340908050537
Epoch 1190, training loss: 0.06948250532150269 = 0.002296254737302661 + 0.01 * 6.718625068664551
Epoch 1190, val loss: 1.1343010663986206
Epoch 1200, training loss: 0.06947066634893417 = 0.002261445624753833 + 0.01 * 6.720922470092773
Epoch 1200, val loss: 1.1362121105194092
Epoch 1210, training loss: 0.06934400647878647 = 0.002227708464488387 + 0.01 * 6.711629867553711
Epoch 1210, val loss: 1.1381713151931763
Epoch 1220, training loss: 0.06917313486337662 = 0.002195078646764159 + 0.01 * 6.697806358337402
Epoch 1220, val loss: 1.1400028467178345
Epoch 1230, training loss: 0.0693095475435257 = 0.0021634181030094624 + 0.01 * 6.714613437652588
Epoch 1230, val loss: 1.1418203115463257
Epoch 1240, training loss: 0.06907271593809128 = 0.0021326749119907618 + 0.01 * 6.694004058837891
Epoch 1240, val loss: 1.1436669826507568
Epoch 1250, training loss: 0.06906163692474365 = 0.0021027682814747095 + 0.01 * 6.695886611938477
Epoch 1250, val loss: 1.1453760862350464
Epoch 1260, training loss: 0.06902705132961273 = 0.002073930809274316 + 0.01 * 6.6953125
Epoch 1260, val loss: 1.1470929384231567
Epoch 1270, training loss: 0.06900762021541595 = 0.0020458269864320755 + 0.01 * 6.696179389953613
Epoch 1270, val loss: 1.1488643884658813
Epoch 1280, training loss: 0.06902717053890228 = 0.0020186291076242924 + 0.01 * 6.7008538246154785
Epoch 1280, val loss: 1.1504724025726318
Epoch 1290, training loss: 0.06894601136445999 = 0.0019921371713280678 + 0.01 * 6.695387363433838
Epoch 1290, val loss: 1.152153491973877
Epoch 1300, training loss: 0.06884554028511047 = 0.0019663164857774973 + 0.01 * 6.687922477722168
Epoch 1300, val loss: 1.1537411212921143
Epoch 1310, training loss: 0.06882485002279282 = 0.0019412873080000281 + 0.01 * 6.688356876373291
Epoch 1310, val loss: 1.1552859544754028
Epoch 1320, training loss: 0.06867755949497223 = 0.001917030313052237 + 0.01 * 6.676053524017334
Epoch 1320, val loss: 1.1568552255630493
Epoch 1330, training loss: 0.06860742717981339 = 0.001893223961815238 + 0.01 * 6.671420574188232
Epoch 1330, val loss: 1.1583298444747925
Epoch 1340, training loss: 0.06868278235197067 = 0.0018702357774600387 + 0.01 * 6.6812543869018555
Epoch 1340, val loss: 1.1598252058029175
Epoch 1350, training loss: 0.06846267729997635 = 0.0018478456186130643 + 0.01 * 6.661483287811279
Epoch 1350, val loss: 1.1612951755523682
Epoch 1360, training loss: 0.06855440884828568 = 0.0018261027289554477 + 0.01 * 6.672831058502197
Epoch 1360, val loss: 1.1626977920532227
Epoch 1370, training loss: 0.06884969025850296 = 0.0018048554193228483 + 0.01 * 6.704483509063721
Epoch 1370, val loss: 1.1641666889190674
Epoch 1380, training loss: 0.06860684603452682 = 0.001784337218850851 + 0.01 * 6.682250499725342
Epoch 1380, val loss: 1.1654834747314453
Epoch 1390, training loss: 0.0684998407959938 = 0.001764292479492724 + 0.01 * 6.673555374145508
Epoch 1390, val loss: 1.1668916940689087
Epoch 1400, training loss: 0.06849713623523712 = 0.0017446056008338928 + 0.01 * 6.675253391265869
Epoch 1400, val loss: 1.1681469678878784
Epoch 1410, training loss: 0.0684572234749794 = 0.0017256081337109208 + 0.01 * 6.673161506652832
Epoch 1410, val loss: 1.1694589853286743
Epoch 1420, training loss: 0.06828710436820984 = 0.0017070819158107042 + 0.01 * 6.6580023765563965
Epoch 1420, val loss: 1.1708062887191772
Epoch 1430, training loss: 0.06845405697822571 = 0.0016890260158106685 + 0.01 * 6.676503658294678
Epoch 1430, val loss: 1.1719869375228882
Epoch 1440, training loss: 0.06854117661714554 = 0.001671393634751439 + 0.01 * 6.686977863311768
Epoch 1440, val loss: 1.1732654571533203
Epoch 1450, training loss: 0.06809642910957336 = 0.0016541488002985716 + 0.01 * 6.644227981567383
Epoch 1450, val loss: 1.174475073814392
Epoch 1460, training loss: 0.06823816895484924 = 0.0016374060651287436 + 0.01 * 6.66007661819458
Epoch 1460, val loss: 1.1755815744400024
Epoch 1470, training loss: 0.06849382072687149 = 0.001621002214960754 + 0.01 * 6.687282085418701
Epoch 1470, val loss: 1.1768006086349487
Epoch 1480, training loss: 0.06812097877264023 = 0.0016051101265475154 + 0.01 * 6.651587009429932
Epoch 1480, val loss: 1.177947998046875
Epoch 1490, training loss: 0.06798912584781647 = 0.0015894657699391246 + 0.01 * 6.6399664878845215
Epoch 1490, val loss: 1.1790621280670166
Epoch 1500, training loss: 0.06799251586198807 = 0.0015743167605251074 + 0.01 * 6.641819953918457
Epoch 1500, val loss: 1.180179238319397
Epoch 1510, training loss: 0.06802420318126678 = 0.0015594980213791132 + 0.01 * 6.64647102355957
Epoch 1510, val loss: 1.1813340187072754
Epoch 1520, training loss: 0.06803324818611145 = 0.0015448095509782434 + 0.01 * 6.648844242095947
Epoch 1520, val loss: 1.182297706604004
Epoch 1530, training loss: 0.06788192689418793 = 0.001530635985545814 + 0.01 * 6.635129928588867
Epoch 1530, val loss: 1.1833792924880981
Epoch 1540, training loss: 0.06792658567428589 = 0.001516898162662983 + 0.01 * 6.640968322753906
Epoch 1540, val loss: 1.1844446659088135
Epoch 1550, training loss: 0.06782946735620499 = 0.001503249746747315 + 0.01 * 6.632621765136719
Epoch 1550, val loss: 1.1853971481323242
Epoch 1560, training loss: 0.06778962165117264 = 0.0014899865491315722 + 0.01 * 6.629963397979736
Epoch 1560, val loss: 1.1864832639694214
Epoch 1570, training loss: 0.06787571310997009 = 0.001477059326134622 + 0.01 * 6.639865398406982
Epoch 1570, val loss: 1.1874096393585205
Epoch 1580, training loss: 0.06810015439987183 = 0.0014644080074504018 + 0.01 * 6.663574695587158
Epoch 1580, val loss: 1.1883649826049805
Epoch 1590, training loss: 0.06759309023618698 = 0.0014519658870995045 + 0.01 * 6.61411190032959
Epoch 1590, val loss: 1.1894383430480957
Epoch 1600, training loss: 0.06762877106666565 = 0.0014398284256458282 + 0.01 * 6.618894577026367
Epoch 1600, val loss: 1.1902889013290405
Epoch 1610, training loss: 0.06774988770484924 = 0.0014279430033639073 + 0.01 * 6.632194995880127
Epoch 1610, val loss: 1.191296935081482
Epoch 1620, training loss: 0.06743087619543076 = 0.001416293904185295 + 0.01 * 6.601458549499512
Epoch 1620, val loss: 1.19219172000885
Epoch 1630, training loss: 0.06748811900615692 = 0.001404920476488769 + 0.01 * 6.608320236206055
Epoch 1630, val loss: 1.1930850744247437
Epoch 1640, training loss: 0.06782906502485275 = 0.001393762999214232 + 0.01 * 6.643529891967773
Epoch 1640, val loss: 1.193989634513855
Epoch 1650, training loss: 0.06744377315044403 = 0.001382828108035028 + 0.01 * 6.606094837188721
Epoch 1650, val loss: 1.1948769092559814
Epoch 1660, training loss: 0.06753793358802795 = 0.0013720790157094598 + 0.01 * 6.616585731506348
Epoch 1660, val loss: 1.1957966089248657
Epoch 1670, training loss: 0.06732367724180222 = 0.0013616217765957117 + 0.01 * 6.596205711364746
Epoch 1670, val loss: 1.1966521739959717
Epoch 1680, training loss: 0.06749175488948822 = 0.0013512917794287205 + 0.01 * 6.614046573638916
Epoch 1680, val loss: 1.1975277662277222
Epoch 1690, training loss: 0.06758110225200653 = 0.00134115235414356 + 0.01 * 6.623995780944824
Epoch 1690, val loss: 1.1983449459075928
Epoch 1700, training loss: 0.06733360141515732 = 0.0013312168885022402 + 0.01 * 6.600238800048828
Epoch 1700, val loss: 1.1991674900054932
Epoch 1710, training loss: 0.06712523847818375 = 0.0013214232167229056 + 0.01 * 6.580382347106934
Epoch 1710, val loss: 1.1999398469924927
Epoch 1720, training loss: 0.06729397177696228 = 0.001311921514570713 + 0.01 * 6.59820556640625
Epoch 1720, val loss: 1.2008297443389893
Epoch 1730, training loss: 0.0673377513885498 = 0.001302547985687852 + 0.01 * 6.603520393371582
Epoch 1730, val loss: 1.201559066772461
Epoch 1740, training loss: 0.06733433902263641 = 0.001293349196203053 + 0.01 * 6.604099750518799
Epoch 1740, val loss: 1.2023345232009888
Epoch 1750, training loss: 0.0670703873038292 = 0.0012843797449022532 + 0.01 * 6.578600883483887
Epoch 1750, val loss: 1.2031923532485962
Epoch 1760, training loss: 0.06714986264705658 = 0.001275459653697908 + 0.01 * 6.587440490722656
Epoch 1760, val loss: 1.2038860321044922
Epoch 1770, training loss: 0.06718412041664124 = 0.001266751321963966 + 0.01 * 6.591737747192383
Epoch 1770, val loss: 1.204641342163086
Epoch 1780, training loss: 0.06706754118204117 = 0.0012581258779391646 + 0.01 * 6.580941200256348
Epoch 1780, val loss: 1.2054712772369385
Epoch 1790, training loss: 0.06698589771986008 = 0.0012497073039412498 + 0.01 * 6.573619365692139
Epoch 1790, val loss: 1.2060692310333252
Epoch 1800, training loss: 0.06730911135673523 = 0.0012414498487487435 + 0.01 * 6.606766223907471
Epoch 1800, val loss: 1.2069123983383179
Epoch 1810, training loss: 0.06725780665874481 = 0.00123336142860353 + 0.01 * 6.602444648742676
Epoch 1810, val loss: 1.20755136013031
Epoch 1820, training loss: 0.06692516803741455 = 0.0012253366876393557 + 0.01 * 6.56998348236084
Epoch 1820, val loss: 1.2082315683364868
Epoch 1830, training loss: 0.06732665002346039 = 0.001217529526911676 + 0.01 * 6.610912322998047
Epoch 1830, val loss: 1.2090091705322266
Epoch 1840, training loss: 0.06708446145057678 = 0.0012098351726308465 + 0.01 * 6.587462902069092
Epoch 1840, val loss: 1.2096328735351562
Epoch 1850, training loss: 0.06697861850261688 = 0.0012022139271721244 + 0.01 * 6.577641010284424
Epoch 1850, val loss: 1.2103773355484009
Epoch 1860, training loss: 0.06734735518693924 = 0.0011947896564379334 + 0.01 * 6.6152567863464355
Epoch 1860, val loss: 1.2110034227371216
Epoch 1870, training loss: 0.06696347147226334 = 0.001187366433441639 + 0.01 * 6.577610015869141
Epoch 1870, val loss: 1.2116737365722656
Epoch 1880, training loss: 0.06687493622303009 = 0.0011801361106336117 + 0.01 * 6.569479942321777
Epoch 1880, val loss: 1.2123056650161743
Epoch 1890, training loss: 0.06709502637386322 = 0.001172971329651773 + 0.01 * 6.592205047607422
Epoch 1890, val loss: 1.2129266262054443
Epoch 1900, training loss: 0.06676694005727768 = 0.0011659858282655478 + 0.01 * 6.560095310211182
Epoch 1900, val loss: 1.2136149406433105
Epoch 1910, training loss: 0.06670988351106644 = 0.0011590723879635334 + 0.01 * 6.555081367492676
Epoch 1910, val loss: 1.2141934633255005
Epoch 1920, training loss: 0.06692569702863693 = 0.0011522668646648526 + 0.01 * 6.577342987060547
Epoch 1920, val loss: 1.2148531675338745
Epoch 1930, training loss: 0.06663532555103302 = 0.0011455317726358771 + 0.01 * 6.54897928237915
Epoch 1930, val loss: 1.215422511100769
Epoch 1940, training loss: 0.06661317497491837 = 0.0011389179853722453 + 0.01 * 6.547425746917725
Epoch 1940, val loss: 1.2159889936447144
Epoch 1950, training loss: 0.06695305556058884 = 0.0011324610095471144 + 0.01 * 6.582059860229492
Epoch 1950, val loss: 1.2166366577148438
Epoch 1960, training loss: 0.06673876196146011 = 0.0011259913444519043 + 0.01 * 6.561277389526367
Epoch 1960, val loss: 1.2171943187713623
Epoch 1970, training loss: 0.06667690724134445 = 0.0011196293635293841 + 0.01 * 6.555727958679199
Epoch 1970, val loss: 1.2177358865737915
Epoch 1980, training loss: 0.06680533289909363 = 0.001113394508138299 + 0.01 * 6.569194316864014
Epoch 1980, val loss: 1.218330979347229
Epoch 1990, training loss: 0.066555917263031 = 0.0011072165798395872 + 0.01 * 6.544870376586914
Epoch 1990, val loss: 1.2188544273376465
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8344754876120191
The final CL Acc:0.82716, 0.00698, The final GNN Acc:0.83869, 0.00298
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11566])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10510])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.025393009185791 = 1.9394245147705078 + 0.01 * 8.596846580505371
Epoch 0, val loss: 1.9312931299209595
Epoch 10, training loss: 2.0149965286254883 = 1.9290283918380737 + 0.01 * 8.5968017578125
Epoch 10, val loss: 1.9217441082000732
Epoch 20, training loss: 2.0025532245635986 = 1.916586995124817 + 0.01 * 8.596611976623535
Epoch 20, val loss: 1.9099938869476318
Epoch 30, training loss: 1.9855388402938843 = 1.899579405784607 + 0.01 * 8.595947265625
Epoch 30, val loss: 1.8937753438949585
Epoch 40, training loss: 1.9612324237823486 = 1.8753101825714111 + 0.01 * 8.592229843139648
Epoch 40, val loss: 1.8710181713104248
Epoch 50, training loss: 1.9285798072814941 = 1.8429057598114014 + 0.01 * 8.567408561706543
Epoch 50, val loss: 1.8425270318984985
Epoch 60, training loss: 1.8934381008148193 = 1.8089662790298462 + 0.01 * 8.447186470031738
Epoch 60, val loss: 1.8166885375976562
Epoch 70, training loss: 1.8605340719223022 = 1.7786650657653809 + 0.01 * 8.186897277832031
Epoch 70, val loss: 1.7932137250900269
Epoch 80, training loss: 1.8199048042297363 = 1.7389575242996216 + 0.01 * 8.0947265625
Epoch 80, val loss: 1.7568178176879883
Epoch 90, training loss: 1.7630914449691772 = 1.6829997301101685 + 0.01 * 8.00916862487793
Epoch 90, val loss: 1.7069755792617798
Epoch 100, training loss: 1.686252474784851 = 1.6067076921463013 + 0.01 * 7.95447301864624
Epoch 100, val loss: 1.6420167684555054
Epoch 110, training loss: 1.5957200527191162 = 1.5167373418807983 + 0.01 * 7.898265838623047
Epoch 110, val loss: 1.5681592226028442
Epoch 120, training loss: 1.5028409957885742 = 1.4253944158554077 + 0.01 * 7.744660377502441
Epoch 120, val loss: 1.4973783493041992
Epoch 130, training loss: 1.4144710302352905 = 1.3399145603179932 + 0.01 * 7.455645561218262
Epoch 130, val loss: 1.4342155456542969
Epoch 140, training loss: 1.3359720706939697 = 1.26161789894104 + 0.01 * 7.435413837432861
Epoch 140, val loss: 1.3798068761825562
Epoch 150, training loss: 1.2624021768569946 = 1.188448190689087 + 0.01 * 7.395402431488037
Epoch 150, val loss: 1.3317970037460327
Epoch 160, training loss: 1.1897419691085815 = 1.1162012815475464 + 0.01 * 7.3540730476379395
Epoch 160, val loss: 1.2843564748764038
Epoch 170, training loss: 1.1142593622207642 = 1.0411655902862549 + 0.01 * 7.309375286102295
Epoch 170, val loss: 1.2338573932647705
Epoch 180, training loss: 1.0354636907577515 = 0.9626615047454834 + 0.01 * 7.280221462249756
Epoch 180, val loss: 1.1793361902236938
Epoch 190, training loss: 0.9559773802757263 = 0.8832656145095825 + 0.01 * 7.271175384521484
Epoch 190, val loss: 1.1236295700073242
Epoch 200, training loss: 0.8792945146560669 = 0.8066616058349609 + 0.01 * 7.263293743133545
Epoch 200, val loss: 1.0713231563568115
Epoch 210, training loss: 0.8077565431594849 = 0.7352440357208252 + 0.01 * 7.251251697540283
Epoch 210, val loss: 1.025976300239563
Epoch 220, training loss: 0.7418389916419983 = 0.6695162057876587 + 0.01 * 7.232278823852539
Epoch 220, val loss: 0.9891945123672485
Epoch 230, training loss: 0.6806949377059937 = 0.6086164116859436 + 0.01 * 7.207850456237793
Epoch 230, val loss: 0.9607070088386536
Epoch 240, training loss: 0.6233000159263611 = 0.5513775944709778 + 0.01 * 7.192239761352539
Epoch 240, val loss: 0.9390968084335327
Epoch 250, training loss: 0.5687013864517212 = 0.49699366092681885 + 0.01 * 7.170773029327393
Epoch 250, val loss: 0.9236348867416382
Epoch 260, training loss: 0.5169527530670166 = 0.4453435242176056 + 0.01 * 7.160922050476074
Epoch 260, val loss: 0.9147199988365173
Epoch 270, training loss: 0.46858835220336914 = 0.39705079793930054 + 0.01 * 7.153754234313965
Epoch 270, val loss: 0.9129071831703186
Epoch 280, training loss: 0.4244205355644226 = 0.3529271185398102 + 0.01 * 7.149341583251953
Epoch 280, val loss: 0.9186538457870483
Epoch 290, training loss: 0.38487428426742554 = 0.3134106993675232 + 0.01 * 7.146360397338867
Epoch 290, val loss: 0.9314631819725037
Epoch 300, training loss: 0.3497946262359619 = 0.27835941314697266 + 0.01 * 7.143519878387451
Epoch 300, val loss: 0.9503231644630432
Epoch 310, training loss: 0.3188064694404602 = 0.24739599227905273 + 0.01 * 7.141048908233643
Epoch 310, val loss: 0.973925769329071
Epoch 320, training loss: 0.2913474440574646 = 0.21997253596782684 + 0.01 * 7.137492656707764
Epoch 320, val loss: 1.0010454654693604
Epoch 330, training loss: 0.26692911982536316 = 0.19558575749397278 + 0.01 * 7.134336471557617
Epoch 330, val loss: 1.030985951423645
Epoch 340, training loss: 0.24510931968688965 = 0.17380717396736145 + 0.01 * 7.130214214324951
Epoch 340, val loss: 1.0630699396133423
Epoch 350, training loss: 0.22556520998477936 = 0.1543097198009491 + 0.01 * 7.125548839569092
Epoch 350, val loss: 1.096833348274231
Epoch 360, training loss: 0.20804986357688904 = 0.13684673607349396 + 0.01 * 7.1203131675720215
Epoch 360, val loss: 1.1318550109863281
Epoch 370, training loss: 0.19238601624965668 = 0.1212446466088295 + 0.01 * 7.114137172698975
Epoch 370, val loss: 1.1677367687225342
Epoch 380, training loss: 0.17844533920288086 = 0.10736966878175735 + 0.01 * 7.107566833496094
Epoch 380, val loss: 1.2041648626327515
Epoch 390, training loss: 0.16610005497932434 = 0.09509571641683578 + 0.01 * 7.100434303283691
Epoch 390, val loss: 1.2407934665679932
Epoch 400, training loss: 0.15521979331970215 = 0.08429714292287827 + 0.01 * 7.092265605926514
Epoch 400, val loss: 1.2773864269256592
Epoch 410, training loss: 0.14571282267570496 = 0.07484032213687897 + 0.01 * 7.087250232696533
Epoch 410, val loss: 1.3135783672332764
Epoch 420, training loss: 0.13739542663097382 = 0.06658448278903961 + 0.01 * 7.081094741821289
Epoch 420, val loss: 1.3492131233215332
Epoch 430, training loss: 0.130144402384758 = 0.05939188972115517 + 0.01 * 7.075251579284668
Epoch 430, val loss: 1.3840349912643433
Epoch 440, training loss: 0.12388825416564941 = 0.05313320830464363 + 0.01 * 7.075504779815674
Epoch 440, val loss: 1.4177769422531128
Epoch 450, training loss: 0.11837403476238251 = 0.04768863692879677 + 0.01 * 7.068539619445801
Epoch 450, val loss: 1.4505091905593872
Epoch 460, training loss: 0.11356723308563232 = 0.04294603690505028 + 0.01 * 7.062119960784912
Epoch 460, val loss: 1.4819865226745605
Epoch 470, training loss: 0.10938586294651031 = 0.0388069786131382 + 0.01 * 7.057888031005859
Epoch 470, val loss: 1.5122793912887573
Epoch 480, training loss: 0.10572616755962372 = 0.03518681228160858 + 0.01 * 7.053936004638672
Epoch 480, val loss: 1.5414330959320068
Epoch 490, training loss: 0.10250651091337204 = 0.032012663781642914 + 0.01 * 7.049384593963623
Epoch 490, val loss: 1.5695267915725708
Epoch 500, training loss: 0.09969393908977509 = 0.029222017154097557 + 0.01 * 7.047192096710205
Epoch 500, val loss: 1.5965157747268677
Epoch 510, training loss: 0.0972147285938263 = 0.026763219386339188 + 0.01 * 7.045151233673096
Epoch 510, val loss: 1.6223440170288086
Epoch 520, training loss: 0.09496666491031647 = 0.02459005080163479 + 0.01 * 7.037661552429199
Epoch 520, val loss: 1.6471612453460693
Epoch 530, training loss: 0.09298936277627945 = 0.022662607952952385 + 0.01 * 7.032675266265869
Epoch 530, val loss: 1.6709675788879395
Epoch 540, training loss: 0.09122049808502197 = 0.020947322249412537 + 0.01 * 7.027317523956299
Epoch 540, val loss: 1.6938809156417847
Epoch 550, training loss: 0.08964209258556366 = 0.019415803253650665 + 0.01 * 7.0226287841796875
Epoch 550, val loss: 1.7159000635147095
Epoch 560, training loss: 0.08823102712631226 = 0.018043242394924164 + 0.01 * 7.018778324127197
Epoch 560, val loss: 1.7371288537979126
Epoch 570, training loss: 0.08699995279312134 = 0.01680801808834076 + 0.01 * 7.019193649291992
Epoch 570, val loss: 1.757529377937317
Epoch 580, training loss: 0.08580247312784195 = 0.01568983681499958 + 0.01 * 7.011264324188232
Epoch 580, val loss: 1.7773470878601074
Epoch 590, training loss: 0.0847313404083252 = 0.014673169702291489 + 0.01 * 7.005817890167236
Epoch 590, val loss: 1.7966288328170776
Epoch 600, training loss: 0.08372707664966583 = 0.013746538199484348 + 0.01 * 6.998054027557373
Epoch 600, val loss: 1.8153119087219238
Epoch 610, training loss: 0.08287311345338821 = 0.012900078669190407 + 0.01 * 6.9973039627075195
Epoch 610, val loss: 1.8335407972335815
Epoch 620, training loss: 0.0819886326789856 = 0.012126619927585125 + 0.01 * 6.986201763153076
Epoch 620, val loss: 1.851244330406189
Epoch 630, training loss: 0.08128497749567032 = 0.011418514885008335 + 0.01 * 6.98664665222168
Epoch 630, val loss: 1.8685040473937988
Epoch 640, training loss: 0.08056896179914474 = 0.010770373977720737 + 0.01 * 6.979859352111816
Epoch 640, val loss: 1.8851838111877441
Epoch 650, training loss: 0.07990149408578873 = 0.010176180861890316 + 0.01 * 6.972531318664551
Epoch 650, val loss: 1.9014118909835815
Epoch 660, training loss: 0.0792684406042099 = 0.009630476124584675 + 0.01 * 6.963796138763428
Epoch 660, val loss: 1.917100191116333
Epoch 670, training loss: 0.0787428542971611 = 0.00912830326706171 + 0.01 * 6.961455345153809
Epoch 670, val loss: 1.932286262512207
Epoch 680, training loss: 0.07822523266077042 = 0.008665899746119976 + 0.01 * 6.955933570861816
Epoch 680, val loss: 1.9470252990722656
Epoch 690, training loss: 0.07777601480484009 = 0.008239850401878357 + 0.01 * 6.953616619110107
Epoch 690, val loss: 1.96133553981781
Epoch 700, training loss: 0.07734285295009613 = 0.007846800610423088 + 0.01 * 6.9496049880981445
Epoch 700, val loss: 1.9750943183898926
Epoch 710, training loss: 0.0769043043255806 = 0.007482823915779591 + 0.01 * 6.942147731781006
Epoch 710, val loss: 1.9885306358337402
Epoch 720, training loss: 0.07644287496805191 = 0.007145330775529146 + 0.01 * 6.929754257202148
Epoch 720, val loss: 2.0014452934265137
Epoch 730, training loss: 0.07613692432641983 = 0.00683191092684865 + 0.01 * 6.930501461029053
Epoch 730, val loss: 2.0140085220336914
Epoch 740, training loss: 0.07580794394016266 = 0.006540974602103233 + 0.01 * 6.926697254180908
Epoch 740, val loss: 2.0261080265045166
Epoch 750, training loss: 0.07544396072626114 = 0.006269996054470539 + 0.01 * 6.917396068572998
Epoch 750, val loss: 2.0379223823547363
Epoch 760, training loss: 0.07512468099594116 = 0.006017018109560013 + 0.01 * 6.910766124725342
Epoch 760, val loss: 2.049365758895874
Epoch 770, training loss: 0.07499117404222488 = 0.005780578125268221 + 0.01 * 6.921060085296631
Epoch 770, val loss: 2.0605292320251465
Epoch 780, training loss: 0.07469959557056427 = 0.005559728480875492 + 0.01 * 6.913987159729004
Epoch 780, val loss: 2.0711557865142822
Epoch 790, training loss: 0.074364073574543 = 0.005352912470698357 + 0.01 * 6.901115894317627
Epoch 790, val loss: 2.081634283065796
Epoch 800, training loss: 0.07409985363483429 = 0.005158913787454367 + 0.01 * 6.894093990325928
Epoch 800, val loss: 2.0918095111846924
Epoch 810, training loss: 0.07399706542491913 = 0.004976552911102772 + 0.01 * 6.902050971984863
Epoch 810, val loss: 2.1017370223999023
Epoch 820, training loss: 0.07377764582633972 = 0.004805197007954121 + 0.01 * 6.897244453430176
Epoch 820, val loss: 2.1111841201782227
Epoch 830, training loss: 0.07347821444272995 = 0.00464385561645031 + 0.01 * 6.8834357261657715
Epoch 830, val loss: 2.1205403804779053
Epoch 840, training loss: 0.07328140735626221 = 0.004491670988500118 + 0.01 * 6.878973960876465
Epoch 840, val loss: 2.129620313644409
Epoch 850, training loss: 0.073054738342762 = 0.0043479460291564465 + 0.01 * 6.87067985534668
Epoch 850, val loss: 2.138493299484253
Epoch 860, training loss: 0.07328325510025024 = 0.004212071653455496 + 0.01 * 6.907118320465088
Epoch 860, val loss: 2.1471214294433594
Epoch 870, training loss: 0.07272173464298248 = 0.004083843901753426 + 0.01 * 6.863789081573486
Epoch 870, val loss: 2.155320405960083
Epoch 880, training loss: 0.07256783545017242 = 0.003962516784667969 + 0.01 * 6.860531806945801
Epoch 880, val loss: 2.163381576538086
Epoch 890, training loss: 0.07240249961614609 = 0.0038473503664135933 + 0.01 * 6.855515003204346
Epoch 890, val loss: 2.1713781356811523
Epoch 900, training loss: 0.07232646644115448 = 0.0037380242720246315 + 0.01 * 6.858844757080078
Epoch 900, val loss: 2.1790406703948975
Epoch 910, training loss: 0.07222889363765717 = 0.0036343419924378395 + 0.01 * 6.859455585479736
Epoch 910, val loss: 2.1865527629852295
Epoch 920, training loss: 0.07204307615756989 = 0.0035358071327209473 + 0.01 * 6.850727081298828
Epoch 920, val loss: 2.1938719749450684
Epoch 930, training loss: 0.07193417102098465 = 0.0034420667216181755 + 0.01 * 6.8492112159729
Epoch 930, val loss: 2.2009990215301514
Epoch 940, training loss: 0.07190541177988052 = 0.0033529167994856834 + 0.01 * 6.855249404907227
Epoch 940, val loss: 2.207904100418091
Epoch 950, training loss: 0.07169977575540543 = 0.0032679603900760412 + 0.01 * 6.84318208694458
Epoch 950, val loss: 2.214759588241577
Epoch 960, training loss: 0.07165753096342087 = 0.0031869583763182163 + 0.01 * 6.847057342529297
Epoch 960, val loss: 2.221407890319824
Epoch 970, training loss: 0.07141510397195816 = 0.0031096215825527906 + 0.01 * 6.830548286437988
Epoch 970, val loss: 2.227837562561035
Epoch 980, training loss: 0.07135901600122452 = 0.0030358333606272936 + 0.01 * 6.832318305969238
Epoch 980, val loss: 2.2341675758361816
Epoch 990, training loss: 0.07133133709430695 = 0.0029653047677129507 + 0.01 * 6.83660364151001
Epoch 990, val loss: 2.2404351234436035
Epoch 1000, training loss: 0.07109617441892624 = 0.0028978849295526743 + 0.01 * 6.819828510284424
Epoch 1000, val loss: 2.2463431358337402
Epoch 1010, training loss: 0.07100555300712585 = 0.0028333254158496857 + 0.01 * 6.817222595214844
Epoch 1010, val loss: 2.2522125244140625
Epoch 1020, training loss: 0.07097689062356949 = 0.0027714946772903204 + 0.01 * 6.820539474487305
Epoch 1020, val loss: 2.2579901218414307
Epoch 1030, training loss: 0.07110556215047836 = 0.0027122879400849342 + 0.01 * 6.839327335357666
Epoch 1030, val loss: 2.2635390758514404
Epoch 1040, training loss: 0.07076563686132431 = 0.002655534539371729 + 0.01 * 6.811009883880615
Epoch 1040, val loss: 2.2688815593719482
Epoch 1050, training loss: 0.07067083567380905 = 0.0026010952424257994 + 0.01 * 6.806973934173584
Epoch 1050, val loss: 2.2741925716400146
Epoch 1060, training loss: 0.07069472223520279 = 0.00254883524030447 + 0.01 * 6.81458854675293
Epoch 1060, val loss: 2.279346466064453
Epoch 1070, training loss: 0.07059946656227112 = 0.0024986283387988806 + 0.01 * 6.810083866119385
Epoch 1070, val loss: 2.2842822074890137
Epoch 1080, training loss: 0.07050114125013351 = 0.0024503893218934536 + 0.01 * 6.805075168609619
Epoch 1080, val loss: 2.2892017364501953
Epoch 1090, training loss: 0.07036161422729492 = 0.0024039053823798895 + 0.01 * 6.795770645141602
Epoch 1090, val loss: 2.293951988220215
Epoch 1100, training loss: 0.07043681293725967 = 0.0023592179641127586 + 0.01 * 6.807759761810303
Epoch 1100, val loss: 2.298569440841675
Epoch 1110, training loss: 0.07027026265859604 = 0.002316215541213751 + 0.01 * 6.79540491104126
Epoch 1110, val loss: 2.3029749393463135
Epoch 1120, training loss: 0.0701325461268425 = 0.002274898812174797 + 0.01 * 6.785764694213867
Epoch 1120, val loss: 2.307358741760254
Epoch 1130, training loss: 0.07021214067935944 = 0.002235050080344081 + 0.01 * 6.7977094650268555
Epoch 1130, val loss: 2.3117282390594482
Epoch 1140, training loss: 0.07001092284917831 = 0.0021965811029076576 + 0.01 * 6.781434535980225
Epoch 1140, val loss: 2.3158421516418457
Epoch 1150, training loss: 0.07018192112445831 = 0.0021595105063170195 + 0.01 * 6.80224084854126
Epoch 1150, val loss: 2.319842576980591
Epoch 1160, training loss: 0.0700213834643364 = 0.002123776590451598 + 0.01 * 6.789761066436768
Epoch 1160, val loss: 2.323909282684326
Epoch 1170, training loss: 0.0699818804860115 = 0.0020892624743282795 + 0.01 * 6.789262294769287
Epoch 1170, val loss: 2.3276562690734863
Epoch 1180, training loss: 0.06995277851819992 = 0.0020559867843985558 + 0.01 * 6.789679527282715
Epoch 1180, val loss: 2.3314571380615234
Epoch 1190, training loss: 0.06988037377595901 = 0.0020238508004695177 + 0.01 * 6.785652160644531
Epoch 1190, val loss: 2.335115909576416
Epoch 1200, training loss: 0.06968025863170624 = 0.0019928112160414457 + 0.01 * 6.768744468688965
Epoch 1200, val loss: 2.338630199432373
Epoch 1210, training loss: 0.06962331384420395 = 0.001962750218808651 + 0.01 * 6.766056060791016
Epoch 1210, val loss: 2.3421943187713623
Epoch 1220, training loss: 0.06957079470157623 = 0.0019336960976943374 + 0.01 * 6.7637104988098145
Epoch 1220, val loss: 2.345578193664551
Epoch 1230, training loss: 0.06950133293867111 = 0.0019055950688198209 + 0.01 * 6.759573459625244
Epoch 1230, val loss: 2.3489105701446533
Epoch 1240, training loss: 0.06955272704362869 = 0.0018783521372824907 + 0.01 * 6.767437934875488
Epoch 1240, val loss: 2.352114200592041
Epoch 1250, training loss: 0.06944870203733444 = 0.0018519432051107287 + 0.01 * 6.7596755027771
Epoch 1250, val loss: 2.3553144931793213
Epoch 1260, training loss: 0.06943457573652267 = 0.0018264339305460453 + 0.01 * 6.760814189910889
Epoch 1260, val loss: 2.358426809310913
Epoch 1270, training loss: 0.06931769847869873 = 0.0018016139511018991 + 0.01 * 6.751608371734619
Epoch 1270, val loss: 2.361515998840332
Epoch 1280, training loss: 0.06935344636440277 = 0.00177759921643883 + 0.01 * 6.757585048675537
Epoch 1280, val loss: 2.3645107746124268
Epoch 1290, training loss: 0.06930235773324966 = 0.001754286466166377 + 0.01 * 6.754807472229004
Epoch 1290, val loss: 2.367286205291748
Epoch 1300, training loss: 0.06916514039039612 = 0.0017317807069048285 + 0.01 * 6.743336200714111
Epoch 1300, val loss: 2.3701562881469727
Epoch 1310, training loss: 0.06932491064071655 = 0.0017099290853366256 + 0.01 * 6.76149845123291
Epoch 1310, val loss: 2.372990846633911
Epoch 1320, training loss: 0.06916686147451401 = 0.0016886017983779311 + 0.01 * 6.747826099395752
Epoch 1320, val loss: 2.375689744949341
Epoch 1330, training loss: 0.06916362792253494 = 0.001667977194301784 + 0.01 * 6.749565124511719
Epoch 1330, val loss: 2.378317356109619
Epoch 1340, training loss: 0.0690549910068512 = 0.001647931756451726 + 0.01 * 6.740705966949463
Epoch 1340, val loss: 2.3808023929595947
Epoch 1350, training loss: 0.0691094771027565 = 0.001628437777981162 + 0.01 * 6.748104572296143
Epoch 1350, val loss: 2.3833677768707275
Epoch 1360, training loss: 0.06916223466396332 = 0.0016095314640551805 + 0.01 * 6.755270481109619
Epoch 1360, val loss: 2.3858518600463867
Epoch 1370, training loss: 0.0690140500664711 = 0.0015910773072391748 + 0.01 * 6.742297172546387
Epoch 1370, val loss: 2.3881664276123047
Epoch 1380, training loss: 0.06913065910339355 = 0.0015731744933873415 + 0.01 * 6.755748748779297
Epoch 1380, val loss: 2.3904943466186523
Epoch 1390, training loss: 0.06895650178194046 = 0.0015556751750409603 + 0.01 * 6.740082740783691
Epoch 1390, val loss: 2.392662525177002
Epoch 1400, training loss: 0.06886066496372223 = 0.0015387667808681726 + 0.01 * 6.732189655303955
Epoch 1400, val loss: 2.3947949409484863
Epoch 1410, training loss: 0.06895419210195541 = 0.001522308331914246 + 0.01 * 6.743188381195068
Epoch 1410, val loss: 2.396986246109009
Epoch 1420, training loss: 0.06898084282875061 = 0.0015062990132719278 + 0.01 * 6.747454643249512
Epoch 1420, val loss: 2.399043083190918
Epoch 1430, training loss: 0.06888262927532196 = 0.001490653259679675 + 0.01 * 6.739197731018066
Epoch 1430, val loss: 2.4010045528411865
Epoch 1440, training loss: 0.0689314752817154 = 0.0014754313742741942 + 0.01 * 6.745604515075684
Epoch 1440, val loss: 2.4029836654663086
Epoch 1450, training loss: 0.06867875158786774 = 0.0014605758478865027 + 0.01 * 6.721817970275879
Epoch 1450, val loss: 2.4049601554870605
Epoch 1460, training loss: 0.0686224102973938 = 0.0014461372047662735 + 0.01 * 6.71762752532959
Epoch 1460, val loss: 2.4068026542663574
Epoch 1470, training loss: 0.0686669796705246 = 0.0014320674818009138 + 0.01 * 6.723491191864014
Epoch 1470, val loss: 2.4086873531341553
Epoch 1480, training loss: 0.06871436536312103 = 0.0014183582970872521 + 0.01 * 6.729600429534912
Epoch 1480, val loss: 2.410480260848999
Epoch 1490, training loss: 0.06866785138845444 = 0.001404913142323494 + 0.01 * 6.726294040679932
Epoch 1490, val loss: 2.4122092723846436
Epoch 1500, training loss: 0.06852077692747116 = 0.0013918753247708082 + 0.01 * 6.712890625
Epoch 1500, val loss: 2.41395902633667
Epoch 1510, training loss: 0.06870275735855103 = 0.0013791248202323914 + 0.01 * 6.732363224029541
Epoch 1510, val loss: 2.4157207012176514
Epoch 1520, training loss: 0.06866287440061569 = 0.0013666779268532991 + 0.01 * 6.729620456695557
Epoch 1520, val loss: 2.417166233062744
Epoch 1530, training loss: 0.06851290911436081 = 0.001354520907625556 + 0.01 * 6.71583890914917
Epoch 1530, val loss: 2.4187512397766113
Epoch 1540, training loss: 0.06854476034641266 = 0.0013426932273432612 + 0.01 * 6.7202067375183105
Epoch 1540, val loss: 2.420290231704712
Epoch 1550, training loss: 0.0684066191315651 = 0.0013310817303135991 + 0.01 * 6.707553863525391
Epoch 1550, val loss: 2.421905279159546
Epoch 1560, training loss: 0.0683256983757019 = 0.0013197934022173285 + 0.01 * 6.70059061050415
Epoch 1560, val loss: 2.4232797622680664
Epoch 1570, training loss: 0.06836605817079544 = 0.0013087759725749493 + 0.01 * 6.705728530883789
Epoch 1570, val loss: 2.4247934818267822
Epoch 1580, training loss: 0.068328358232975 = 0.001298007438890636 + 0.01 * 6.7030348777771
Epoch 1580, val loss: 2.4261538982391357
Epoch 1590, training loss: 0.06839845329523087 = 0.0012874407693743706 + 0.01 * 6.71110200881958
Epoch 1590, val loss: 2.4275410175323486
Epoch 1600, training loss: 0.06832712143659592 = 0.0012771720066666603 + 0.01 * 6.704995155334473
Epoch 1600, val loss: 2.4288156032562256
Epoch 1610, training loss: 0.06838515400886536 = 0.0012670630821958184 + 0.01 * 6.711809158325195
Epoch 1610, val loss: 2.4300789833068848
Epoch 1620, training loss: 0.06830926239490509 = 0.0012572554405778646 + 0.01 * 6.705200672149658
Epoch 1620, val loss: 2.4313111305236816
Epoch 1630, training loss: 0.068427175283432 = 0.0012476220726966858 + 0.01 * 6.717955589294434
Epoch 1630, val loss: 2.432610511779785
Epoch 1640, training loss: 0.06826110929250717 = 0.0012381462147459388 + 0.01 * 6.702296733856201
Epoch 1640, val loss: 2.433816432952881
Epoch 1650, training loss: 0.06818681210279465 = 0.0012289106380194426 + 0.01 * 6.695789813995361
Epoch 1650, val loss: 2.4348323345184326
Epoch 1660, training loss: 0.06816793233156204 = 0.0012198772747069597 + 0.01 * 6.694806098937988
Epoch 1660, val loss: 2.4359819889068604
Epoch 1670, training loss: 0.06822822988033295 = 0.001211070572026074 + 0.01 * 6.701716423034668
Epoch 1670, val loss: 2.4371285438537598
Epoch 1680, training loss: 0.06815093010663986 = 0.0012023849412798882 + 0.01 * 6.694855213165283
Epoch 1680, val loss: 2.4380717277526855
Epoch 1690, training loss: 0.06820934265851974 = 0.0011938628740608692 + 0.01 * 6.701548099517822
Epoch 1690, val loss: 2.439188241958618
Epoch 1700, training loss: 0.06819625943899155 = 0.0011855547782033682 + 0.01 * 6.701070785522461
Epoch 1700, val loss: 2.440133810043335
Epoch 1710, training loss: 0.06818784028291702 = 0.0011774246813729405 + 0.01 * 6.7010416984558105
Epoch 1710, val loss: 2.4409756660461426
Epoch 1720, training loss: 0.06820797175168991 = 0.0011694912100210786 + 0.01 * 6.703848361968994
Epoch 1720, val loss: 2.442004919052124
Epoch 1730, training loss: 0.06794574111700058 = 0.0011616830015555024 + 0.01 * 6.678406238555908
Epoch 1730, val loss: 2.442824602127075
Epoch 1740, training loss: 0.06786898523569107 = 0.0011540474370121956 + 0.01 * 6.671494007110596
Epoch 1740, val loss: 2.443742275238037
Epoch 1750, training loss: 0.06791836023330688 = 0.0011465661227703094 + 0.01 * 6.677178859710693
Epoch 1750, val loss: 2.4444854259490967
Epoch 1760, training loss: 0.067995585501194 = 0.0011392348678782582 + 0.01 * 6.685634613037109
Epoch 1760, val loss: 2.445371627807617
Epoch 1770, training loss: 0.06792410463094711 = 0.0011320459889248013 + 0.01 * 6.679205417633057
Epoch 1770, val loss: 2.4459800720214844
Epoch 1780, training loss: 0.06799495220184326 = 0.0011249848175793886 + 0.01 * 6.686996936798096
Epoch 1780, val loss: 2.4467275142669678
Epoch 1790, training loss: 0.06788250058889389 = 0.001118035172112286 + 0.01 * 6.676446914672852
Epoch 1790, val loss: 2.4473931789398193
Epoch 1800, training loss: 0.0678415447473526 = 0.0011112078791484237 + 0.01 * 6.673033714294434
Epoch 1800, val loss: 2.448073625564575
Epoch 1810, training loss: 0.06780620664358139 = 0.0011045231949537992 + 0.01 * 6.670168399810791
Epoch 1810, val loss: 2.4486193656921387
Epoch 1820, training loss: 0.06784241646528244 = 0.0010980007937178016 + 0.01 * 6.674441814422607
Epoch 1820, val loss: 2.4492077827453613
Epoch 1830, training loss: 0.06764369457960129 = 0.0010915710590779781 + 0.01 * 6.655212879180908
Epoch 1830, val loss: 2.4497411251068115
Epoch 1840, training loss: 0.06757918000221252 = 0.0010852303821593523 + 0.01 * 6.649394989013672
Epoch 1840, val loss: 2.4503731727600098
Epoch 1850, training loss: 0.0675581693649292 = 0.001079011824913323 + 0.01 * 6.647915363311768
Epoch 1850, val loss: 2.4508559703826904
Epoch 1860, training loss: 0.06783564388751984 = 0.0010729434434324503 + 0.01 * 6.676270008087158
Epoch 1860, val loss: 2.4513962268829346
Epoch 1870, training loss: 0.06785091757774353 = 0.0010669177863746881 + 0.01 * 6.67840051651001
Epoch 1870, val loss: 2.4518308639526367
Epoch 1880, training loss: 0.06767548620700836 = 0.0010609813034534454 + 0.01 * 6.66145133972168
Epoch 1880, val loss: 2.4521944522857666
Epoch 1890, training loss: 0.067566879093647 = 0.0010552157182246447 + 0.01 * 6.651165962219238
Epoch 1890, val loss: 2.4526455402374268
Epoch 1900, training loss: 0.06792304664850235 = 0.0010495607275515795 + 0.01 * 6.687348365783691
Epoch 1900, val loss: 2.453061103820801
Epoch 1910, training loss: 0.06777695566415787 = 0.0010439087636768818 + 0.01 * 6.673304557800293
Epoch 1910, val loss: 2.453511953353882
Epoch 1920, training loss: 0.06747566908597946 = 0.0010383495828136802 + 0.01 * 6.643732070922852
Epoch 1920, val loss: 2.453695774078369
Epoch 1930, training loss: 0.06736406683921814 = 0.0010329042561352253 + 0.01 * 6.633116722106934
Epoch 1930, val loss: 2.454085350036621
Epoch 1940, training loss: 0.06764637678861618 = 0.0010275882668793201 + 0.01 * 6.66187858581543
Epoch 1940, val loss: 2.4543728828430176
Epoch 1950, training loss: 0.06746614724397659 = 0.001022312673740089 + 0.01 * 6.644383907318115
Epoch 1950, val loss: 2.4546213150024414
Epoch 1960, training loss: 0.06743212044239044 = 0.001017157919704914 + 0.01 * 6.641496181488037
Epoch 1960, val loss: 2.454913854598999
Epoch 1970, training loss: 0.0676598846912384 = 0.0010120583465322852 + 0.01 * 6.664783000946045
Epoch 1970, val loss: 2.4552040100097656
Epoch 1980, training loss: 0.0674825981259346 = 0.0010070085991173983 + 0.01 * 6.647559642791748
Epoch 1980, val loss: 2.4552862644195557
Epoch 1990, training loss: 0.06749209016561508 = 0.0010020576883107424 + 0.01 * 6.649003028869629
Epoch 1990, val loss: 2.455589771270752
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 2.0581789016723633 = 1.97221040725708 + 0.01 * 8.59685230255127
Epoch 0, val loss: 1.9759577512741089
Epoch 10, training loss: 2.0472071170806885 = 1.961239218711853 + 0.01 * 8.596785545349121
Epoch 10, val loss: 1.964553952217102
Epoch 20, training loss: 2.0331716537475586 = 1.9472057819366455 + 0.01 * 8.596589088439941
Epoch 20, val loss: 1.9496954679489136
Epoch 30, training loss: 2.0131895542144775 = 1.9272304773330688 + 0.01 * 8.595908164978027
Epoch 30, val loss: 1.928634524345398
Epoch 40, training loss: 1.983351707458496 = 1.8974311351776123 + 0.01 * 8.592061996459961
Epoch 40, val loss: 1.8976861238479614
Epoch 50, training loss: 1.9414870738983154 = 1.8558213710784912 + 0.01 * 8.566568374633789
Epoch 50, val loss: 1.856477975845337
Epoch 60, training loss: 1.8958935737609863 = 1.8112741708755493 + 0.01 * 8.46194076538086
Epoch 60, val loss: 1.8167005777359009
Epoch 70, training loss: 1.8618383407592773 = 1.7793967723846436 + 0.01 * 8.244157791137695
Epoch 70, val loss: 1.7902238368988037
Epoch 80, training loss: 1.8253662586212158 = 1.7440615892410278 + 0.01 * 8.130467414855957
Epoch 80, val loss: 1.7573471069335938
Epoch 90, training loss: 1.7754435539245605 = 1.6962206363677979 + 0.01 * 7.922287940979004
Epoch 90, val loss: 1.715570330619812
Epoch 100, training loss: 1.7072728872299194 = 1.630189061164856 + 0.01 * 7.708381175994873
Epoch 100, val loss: 1.6598396301269531
Epoch 110, training loss: 1.6203354597091675 = 1.5449599027633667 + 0.01 * 7.537560939788818
Epoch 110, val loss: 1.5881601572036743
Epoch 120, training loss: 1.5239429473876953 = 1.449341058731079 + 0.01 * 7.460193634033203
Epoch 120, val loss: 1.5092315673828125
Epoch 130, training loss: 1.4271206855773926 = 1.353056788444519 + 0.01 * 7.406391143798828
Epoch 130, val loss: 1.4292895793914795
Epoch 140, training loss: 1.332503318786621 = 1.2587906122207642 + 0.01 * 7.371268272399902
Epoch 140, val loss: 1.3520835638046265
Epoch 150, training loss: 1.2404663562774658 = 1.1670089960098267 + 0.01 * 7.345737457275391
Epoch 150, val loss: 1.2770048379898071
Epoch 160, training loss: 1.152660846710205 = 1.0794793367385864 + 0.01 * 7.3181471824646
Epoch 160, val loss: 1.20676589012146
Epoch 170, training loss: 1.0703647136688232 = 0.9974132776260376 + 0.01 * 7.295139312744141
Epoch 170, val loss: 1.1422221660614014
Epoch 180, training loss: 0.9935750961303711 = 0.9207630157470703 + 0.01 * 7.281207084655762
Epoch 180, val loss: 1.083237648010254
Epoch 190, training loss: 0.9215421080589294 = 0.8488059043884277 + 0.01 * 7.273622035980225
Epoch 190, val loss: 1.029024362564087
Epoch 200, training loss: 0.8533294796943665 = 0.7806550860404968 + 0.01 * 7.267438888549805
Epoch 200, val loss: 0.9791353344917297
Epoch 210, training loss: 0.7887955904006958 = 0.7161974310874939 + 0.01 * 7.259818077087402
Epoch 210, val loss: 0.933661699295044
Epoch 220, training loss: 0.7278254628181458 = 0.655339777469635 + 0.01 * 7.248569011688232
Epoch 220, val loss: 0.8929926156997681
Epoch 230, training loss: 0.6702613234519958 = 0.597935676574707 + 0.01 * 7.2325639724731445
Epoch 230, val loss: 0.8573647737503052
Epoch 240, training loss: 0.6157945990562439 = 0.5436896681785583 + 0.01 * 7.21049165725708
Epoch 240, val loss: 0.8269171714782715
Epoch 250, training loss: 0.5640666484832764 = 0.49221304059028625 + 0.01 * 7.185362815856934
Epoch 250, val loss: 0.8017870187759399
Epoch 260, training loss: 0.5149423480033875 = 0.44330430030822754 + 0.01 * 7.163806915283203
Epoch 260, val loss: 0.7818357944488525
Epoch 270, training loss: 0.468462198972702 = 0.39696627855300903 + 0.01 * 7.14959192276001
Epoch 270, val loss: 0.7662016749382019
Epoch 280, training loss: 0.42485496401786804 = 0.35342100262641907 + 0.01 * 7.143395900726318
Epoch 280, val loss: 0.7540726065635681
Epoch 290, training loss: 0.38437265157699585 = 0.31299126148223877 + 0.01 * 7.1381378173828125
Epoch 290, val loss: 0.7445947527885437
Epoch 300, training loss: 0.3472517132759094 = 0.275917649269104 + 0.01 * 7.1334052085876465
Epoch 300, val loss: 0.7371577620506287
Epoch 310, training loss: 0.3135322034358978 = 0.24222923815250397 + 0.01 * 7.13029670715332
Epoch 310, val loss: 0.7315584421157837
Epoch 320, training loss: 0.2831125855445862 = 0.21184998750686646 + 0.01 * 7.126260757446289
Epoch 320, val loss: 0.7275336384773254
Epoch 330, training loss: 0.2558664083480835 = 0.18463227152824402 + 0.01 * 7.123414516448975
Epoch 330, val loss: 0.7251530289649963
Epoch 340, training loss: 0.23164813220500946 = 0.16046123206615448 + 0.01 * 7.118690490722656
Epoch 340, val loss: 0.7248100638389587
Epoch 350, training loss: 0.21034547686576843 = 0.13918772339820862 + 0.01 * 7.115776062011719
Epoch 350, val loss: 0.726337730884552
Epoch 360, training loss: 0.19179107248783112 = 0.12064666301012039 + 0.01 * 7.11444091796875
Epoch 360, val loss: 0.7298374772071838
Epoch 370, training loss: 0.17572864890098572 = 0.10463006794452667 + 0.01 * 7.10985803604126
Epoch 370, val loss: 0.7350556254386902
Epoch 380, training loss: 0.16194987297058105 = 0.09088525176048279 + 0.01 * 7.106462478637695
Epoch 380, val loss: 0.7419624328613281
Epoch 390, training loss: 0.1501888632774353 = 0.07916785776615143 + 0.01 * 7.102101802825928
Epoch 390, val loss: 0.7501834034919739
Epoch 400, training loss: 0.14019301533699036 = 0.06921335309743881 + 0.01 * 7.097966194152832
Epoch 400, val loss: 0.7594392895698547
Epoch 410, training loss: 0.13173066079616547 = 0.06075483560562134 + 0.01 * 7.0975823402404785
Epoch 410, val loss: 0.7694587707519531
Epoch 420, training loss: 0.12449415028095245 = 0.053560417145490646 + 0.01 * 7.0933732986450195
Epoch 420, val loss: 0.7799564599990845
Epoch 430, training loss: 0.11839545518159866 = 0.04743599146604538 + 0.01 * 7.095946788787842
Epoch 430, val loss: 0.7907945513725281
Epoch 440, training loss: 0.11305603384971619 = 0.04220707714557648 + 0.01 * 7.084896087646484
Epoch 440, val loss: 0.8017167448997498
Epoch 450, training loss: 0.10851828008890152 = 0.03772387653589249 + 0.01 * 7.079440593719482
Epoch 450, val loss: 0.812613308429718
Epoch 460, training loss: 0.1046827882528305 = 0.033869680017232895 + 0.01 * 7.081311225891113
Epoch 460, val loss: 0.8234430551528931
Epoch 470, training loss: 0.10132454335689545 = 0.03054785169661045 + 0.01 * 7.077669143676758
Epoch 470, val loss: 0.83407062292099
Epoch 480, training loss: 0.09834330528974533 = 0.027675047516822815 + 0.01 * 7.066825866699219
Epoch 480, val loss: 0.8444652557373047
Epoch 490, training loss: 0.0958564504981041 = 0.025179656222462654 + 0.01 * 7.0676798820495605
Epoch 490, val loss: 0.854641318321228
Epoch 500, training loss: 0.09359443932771683 = 0.023003438487648964 + 0.01 * 7.05910062789917
Epoch 500, val loss: 0.8645834922790527
Epoch 510, training loss: 0.09170642495155334 = 0.02109784446656704 + 0.01 * 7.060858249664307
Epoch 510, val loss: 0.8743101358413696
Epoch 520, training loss: 0.08989785611629486 = 0.01942359283566475 + 0.01 * 7.047427177429199
Epoch 520, val loss: 0.8836957216262817
Epoch 530, training loss: 0.0883534848690033 = 0.0179449450224638 + 0.01 * 7.040853977203369
Epoch 530, val loss: 0.8928385376930237
Epoch 540, training loss: 0.08698859065771103 = 0.016634447500109673 + 0.01 * 7.035414218902588
Epoch 540, val loss: 0.9017331600189209
Epoch 550, training loss: 0.08580377697944641 = 0.015468650497496128 + 0.01 * 7.033513069152832
Epoch 550, val loss: 0.910408616065979
Epoch 560, training loss: 0.08463059365749359 = 0.014427818357944489 + 0.01 * 7.020277500152588
Epoch 560, val loss: 0.9187819957733154
Epoch 570, training loss: 0.08364666998386383 = 0.01349458284676075 + 0.01 * 7.015209197998047
Epoch 570, val loss: 0.9270239472389221
Epoch 580, training loss: 0.08274223655462265 = 0.012655396945774555 + 0.01 * 7.008684158325195
Epoch 580, val loss: 0.9348978400230408
Epoch 590, training loss: 0.08209987729787827 = 0.011897402815520763 + 0.01 * 7.020247936248779
Epoch 590, val loss: 0.9426367282867432
Epoch 600, training loss: 0.08121364563703537 = 0.011212505400180817 + 0.01 * 7.0001139640808105
Epoch 600, val loss: 0.9500461220741272
Epoch 610, training loss: 0.08098036795854568 = 0.010590687394142151 + 0.01 * 7.038968086242676
Epoch 610, val loss: 0.9573466181755066
Epoch 620, training loss: 0.0799628347158432 = 0.010025082156062126 + 0.01 * 6.993774890899658
Epoch 620, val loss: 0.9643154740333557
Epoch 630, training loss: 0.07921155542135239 = 0.009508291259407997 + 0.01 * 6.9703264236450195
Epoch 630, val loss: 0.9711127281188965
Epoch 640, training loss: 0.07867483794689178 = 0.009033760987222195 + 0.01 * 6.964107990264893
Epoch 640, val loss: 0.9777593612670898
Epoch 650, training loss: 0.07802360504865646 = 0.008598105981945992 + 0.01 * 6.942550182342529
Epoch 650, val loss: 0.98429936170578
Epoch 660, training loss: 0.07775377482175827 = 0.008197732269763947 + 0.01 * 6.955604553222656
Epoch 660, val loss: 0.9905071258544922
Epoch 670, training loss: 0.077464759349823 = 0.007828177884221077 + 0.01 * 6.963658809661865
Epoch 670, val loss: 0.9965612292289734
Epoch 680, training loss: 0.07670771330595016 = 0.00748650636523962 + 0.01 * 6.922120571136475
Epoch 680, val loss: 1.0025192499160767
Epoch 690, training loss: 0.07632885873317719 = 0.007169872522354126 + 0.01 * 6.91589879989624
Epoch 690, val loss: 1.0083776712417603
Epoch 700, training loss: 0.0762072503566742 = 0.006875764112919569 + 0.01 * 6.933149337768555
Epoch 700, val loss: 1.0139096975326538
Epoch 710, training loss: 0.07575387507677078 = 0.006602047942578793 + 0.01 * 6.915183067321777
Epoch 710, val loss: 1.0194458961486816
Epoch 720, training loss: 0.07530245184898376 = 0.006346996407955885 + 0.01 * 6.895545959472656
Epoch 720, val loss: 1.024830937385559
Epoch 730, training loss: 0.07514990866184235 = 0.0061085037887096405 + 0.01 * 6.904139995574951
Epoch 730, val loss: 1.0300829410552979
Epoch 740, training loss: 0.0748242512345314 = 0.005885259713977575 + 0.01 * 6.893899440765381
Epoch 740, val loss: 1.035165548324585
Epoch 750, training loss: 0.0745009034872055 = 0.005675897002220154 + 0.01 * 6.882500648498535
Epoch 750, val loss: 1.0402514934539795
Epoch 760, training loss: 0.07424749433994293 = 0.005479475017637014 + 0.01 * 6.876801490783691
Epoch 760, val loss: 1.0450503826141357
Epoch 770, training loss: 0.07450920343399048 = 0.005294754635542631 + 0.01 * 6.921444892883301
Epoch 770, val loss: 1.0499272346496582
Epoch 780, training loss: 0.07377485930919647 = 0.005121121648699045 + 0.01 * 6.8653740882873535
Epoch 780, val loss: 1.0544683933258057
Epoch 790, training loss: 0.07348798960447311 = 0.0049572838470339775 + 0.01 * 6.853071212768555
Epoch 790, val loss: 1.0590405464172363
Epoch 800, training loss: 0.07343308627605438 = 0.004802568815648556 + 0.01 * 6.863051891326904
Epoch 800, val loss: 1.063478708267212
Epoch 810, training loss: 0.07335871458053589 = 0.004656224511563778 + 0.01 * 6.870249271392822
Epoch 810, val loss: 1.0679173469543457
Epoch 820, training loss: 0.07328221201896667 = 0.004518237896263599 + 0.01 * 6.876397609710693
Epoch 820, val loss: 1.0721098184585571
Epoch 830, training loss: 0.07297876477241516 = 0.00438737915828824 + 0.01 * 6.859138488769531
Epoch 830, val loss: 1.076238751411438
Epoch 840, training loss: 0.07281958311796188 = 0.004263619892299175 + 0.01 * 6.855597019195557
Epoch 840, val loss: 1.0804568529129028
Epoch 850, training loss: 0.07263580709695816 = 0.004145776387304068 + 0.01 * 6.849003314971924
Epoch 850, val loss: 1.0842376947402954
Epoch 860, training loss: 0.07218936085700989 = 0.004033978562802076 + 0.01 * 6.81553840637207
Epoch 860, val loss: 1.088131070137024
Epoch 870, training loss: 0.07226769626140594 = 0.003927640616893768 + 0.01 * 6.834005355834961
Epoch 870, val loss: 1.0921858549118042
Epoch 880, training loss: 0.07239165157079697 = 0.0038261960726231337 + 0.01 * 6.856545925140381
Epoch 880, val loss: 1.095707893371582
Epoch 890, training loss: 0.07190315425395966 = 0.0037297895178198814 + 0.01 * 6.817336559295654
Epoch 890, val loss: 1.0994203090667725
Epoch 900, training loss: 0.07206759601831436 = 0.003638297552242875 + 0.01 * 6.842930316925049
Epoch 900, val loss: 1.1030843257904053
Epoch 910, training loss: 0.07182495296001434 = 0.0035501178354024887 + 0.01 * 6.827483654022217
Epoch 910, val loss: 1.1064560413360596
Epoch 920, training loss: 0.07140669971704483 = 0.003466626862064004 + 0.01 * 6.794007301330566
Epoch 920, val loss: 1.109900951385498
Epoch 930, training loss: 0.07133392244577408 = 0.003386410418897867 + 0.01 * 6.7947516441345215
Epoch 930, val loss: 1.1134027242660522
Epoch 940, training loss: 0.07172171771526337 = 0.003309770952910185 + 0.01 * 6.841195106506348
Epoch 940, val loss: 1.116804838180542
Epoch 950, training loss: 0.07125900685787201 = 0.003236458171159029 + 0.01 * 6.802255153656006
Epoch 950, val loss: 1.119982361793518
Epoch 960, training loss: 0.07104458659887314 = 0.00316622550599277 + 0.01 * 6.787836074829102
Epoch 960, val loss: 1.1231088638305664
Epoch 970, training loss: 0.07092567533254623 = 0.003099005203694105 + 0.01 * 6.782667636871338
Epoch 970, val loss: 1.1263296604156494
Epoch 980, training loss: 0.07133439928293228 = 0.003034257562831044 + 0.01 * 6.830014705657959
Epoch 980, val loss: 1.129387617111206
Epoch 990, training loss: 0.0709201842546463 = 0.0029724095948040485 + 0.01 * 6.7947773933410645
Epoch 990, val loss: 1.132378101348877
Epoch 1000, training loss: 0.07055255025625229 = 0.002913133706897497 + 0.01 * 6.763942241668701
Epoch 1000, val loss: 1.1353189945220947
Epoch 1010, training loss: 0.07072095572948456 = 0.002855811035260558 + 0.01 * 6.786514759063721
Epoch 1010, val loss: 1.1381545066833496
Epoch 1020, training loss: 0.07036043703556061 = 0.002800618764013052 + 0.01 * 6.755981922149658
Epoch 1020, val loss: 1.141068935394287
Epoch 1030, training loss: 0.07059808820486069 = 0.002747779479250312 + 0.01 * 6.785030841827393
Epoch 1030, val loss: 1.1440536975860596
Epoch 1040, training loss: 0.07060985267162323 = 0.002696830313652754 + 0.01 * 6.791302680969238
Epoch 1040, val loss: 1.1466015577316284
Epoch 1050, training loss: 0.07021816074848175 = 0.002647841814905405 + 0.01 * 6.7570319175720215
Epoch 1050, val loss: 1.1492104530334473
Epoch 1060, training loss: 0.07029853761196136 = 0.0026007432024925947 + 0.01 * 6.769779205322266
Epoch 1060, val loss: 1.1519627571105957
Epoch 1070, training loss: 0.07015708088874817 = 0.0025549111887812614 + 0.01 * 6.760217189788818
Epoch 1070, val loss: 1.1545149087905884
Epoch 1080, training loss: 0.07020004093647003 = 0.0025110337883234024 + 0.01 * 6.7689008712768555
Epoch 1080, val loss: 1.1572444438934326
Epoch 1090, training loss: 0.07003740221261978 = 0.0024685983080416918 + 0.01 * 6.756880283355713
Epoch 1090, val loss: 1.1596806049346924
Epoch 1100, training loss: 0.06978441774845123 = 0.0024275528267025948 + 0.01 * 6.735686302185059
Epoch 1100, val loss: 1.162036657333374
Epoch 1110, training loss: 0.06969606131315231 = 0.002388064283877611 + 0.01 * 6.730799674987793
Epoch 1110, val loss: 1.164629578590393
Epoch 1120, training loss: 0.06980850547552109 = 0.0023494502529501915 + 0.01 * 6.745905876159668
Epoch 1120, val loss: 1.1669520139694214
Epoch 1130, training loss: 0.06975220888853073 = 0.0023125791922211647 + 0.01 * 6.743963241577148
Epoch 1130, val loss: 1.1693689823150635
Epoch 1140, training loss: 0.06962204724550247 = 0.002276573795825243 + 0.01 * 6.734547138214111
Epoch 1140, val loss: 1.1718207597732544
Epoch 1150, training loss: 0.06968831270933151 = 0.002241880865767598 + 0.01 * 6.744643688201904
Epoch 1150, val loss: 1.1740062236785889
Epoch 1160, training loss: 0.06946156919002533 = 0.0022082417272031307 + 0.01 * 6.725333213806152
Epoch 1160, val loss: 1.176315426826477
Epoch 1170, training loss: 0.06946305185556412 = 0.002175735542550683 + 0.01 * 6.728731632232666
Epoch 1170, val loss: 1.178589940071106
Epoch 1180, training loss: 0.06921350210905075 = 0.00214418675750494 + 0.01 * 6.7069315910339355
Epoch 1180, val loss: 1.1808199882507324
Epoch 1190, training loss: 0.06956148892641068 = 0.00211375136859715 + 0.01 * 6.744773864746094
Epoch 1190, val loss: 1.1829947233200073
Epoch 1200, training loss: 0.0690859705209732 = 0.002084125764667988 + 0.01 * 6.700184345245361
Epoch 1200, val loss: 1.185071587562561
Epoch 1210, training loss: 0.06913509964942932 = 0.0020556494127959013 + 0.01 * 6.707944869995117
Epoch 1210, val loss: 1.18715500831604
Epoch 1220, training loss: 0.06920423358678818 = 0.002027657814323902 + 0.01 * 6.717657089233398
Epoch 1220, val loss: 1.1893101930618286
Epoch 1230, training loss: 0.06906510889530182 = 0.002000913955271244 + 0.01 * 6.706419467926025
Epoch 1230, val loss: 1.191196084022522
Epoch 1240, training loss: 0.0690213218331337 = 0.0019744455348700285 + 0.01 * 6.704687595367432
Epoch 1240, val loss: 1.1932792663574219
Epoch 1250, training loss: 0.06923326104879379 = 0.0019490835256874561 + 0.01 * 6.72841739654541
Epoch 1250, val loss: 1.1953479051589966
Epoch 1260, training loss: 0.06913352012634277 = 0.0019241789123043418 + 0.01 * 6.7209343910217285
Epoch 1260, val loss: 1.197170615196228
Epoch 1270, training loss: 0.06885580718517303 = 0.0019003474153578281 + 0.01 * 6.695546627044678
Epoch 1270, val loss: 1.1991424560546875
Epoch 1280, training loss: 0.06904983520507812 = 0.001876852591522038 + 0.01 * 6.71729850769043
Epoch 1280, val loss: 1.2010577917099
Epoch 1290, training loss: 0.06901431828737259 = 0.0018541420577093959 + 0.01 * 6.716017723083496
Epoch 1290, val loss: 1.2029430866241455
Epoch 1300, training loss: 0.0686488002538681 = 0.001832153764553368 + 0.01 * 6.68166446685791
Epoch 1300, val loss: 1.2048002481460571
Epoch 1310, training loss: 0.06865975260734558 = 0.0018105903873220086 + 0.01 * 6.684916973114014
Epoch 1310, val loss: 1.2066289186477661
Epoch 1320, training loss: 0.0686686635017395 = 0.0017897103680297732 + 0.01 * 6.68789529800415
Epoch 1320, val loss: 1.2082730531692505
Epoch 1330, training loss: 0.06858856230974197 = 0.0017694394337013364 + 0.01 * 6.681912899017334
Epoch 1330, val loss: 1.210192322731018
Epoch 1340, training loss: 0.06860899925231934 = 0.0017496587242931128 + 0.01 * 6.685934543609619
Epoch 1340, val loss: 1.21186363697052
Epoch 1350, training loss: 0.06880529224872589 = 0.0017303423956036568 + 0.01 * 6.707494735717773
Epoch 1350, val loss: 1.2136458158493042
Epoch 1360, training loss: 0.06878441572189331 = 0.0017116549424827099 + 0.01 * 6.707276344299316
Epoch 1360, val loss: 1.215139389038086
Epoch 1370, training loss: 0.06836234033107758 = 0.0016933344304561615 + 0.01 * 6.666900634765625
Epoch 1370, val loss: 1.21687912940979
Epoch 1380, training loss: 0.06849385797977448 = 0.0016755741089582443 + 0.01 * 6.681828022003174
Epoch 1380, val loss: 1.2185879945755005
Epoch 1390, training loss: 0.06856779009103775 = 0.0016581404488533735 + 0.01 * 6.690964698791504
Epoch 1390, val loss: 1.2200899124145508
Epoch 1400, training loss: 0.0685698539018631 = 0.0016411739634349942 + 0.01 * 6.692868232727051
Epoch 1400, val loss: 1.2217068672180176
Epoch 1410, training loss: 0.06825197488069534 = 0.0016246680170297623 + 0.01 * 6.662731170654297
Epoch 1410, val loss: 1.2233339548110962
Epoch 1420, training loss: 0.06832297891378403 = 0.0016085185343399644 + 0.01 * 6.671446323394775
Epoch 1420, val loss: 1.224868655204773
Epoch 1430, training loss: 0.06819020956754684 = 0.0015927694039419293 + 0.01 * 6.659743785858154
Epoch 1430, val loss: 1.226348876953125
Epoch 1440, training loss: 0.06812869012355804 = 0.0015773687046021223 + 0.01 * 6.655131816864014
Epoch 1440, val loss: 1.2279256582260132
Epoch 1450, training loss: 0.06848159432411194 = 0.0015624273801222444 + 0.01 * 6.6919169425964355
Epoch 1450, val loss: 1.229377031326294
Epoch 1460, training loss: 0.06807555258274078 = 0.0015477538108825684 + 0.01 * 6.652780055999756
Epoch 1460, val loss: 1.2309767007827759
Epoch 1470, training loss: 0.06815086305141449 = 0.0015335219213739038 + 0.01 * 6.661734104156494
Epoch 1470, val loss: 1.2325092554092407
Epoch 1480, training loss: 0.0684518814086914 = 0.0015194613952189684 + 0.01 * 6.693241596221924
Epoch 1480, val loss: 1.23373281955719
Epoch 1490, training loss: 0.06800036877393723 = 0.0015058465069159865 + 0.01 * 6.6494526863098145
Epoch 1490, val loss: 1.2353249788284302
Epoch 1500, training loss: 0.0683252140879631 = 0.0014926291769370437 + 0.01 * 6.683259010314941
Epoch 1500, val loss: 1.2364511489868164
Epoch 1510, training loss: 0.06776218861341476 = 0.0014795311726629734 + 0.01 * 6.628265857696533
Epoch 1510, val loss: 1.2380337715148926
Epoch 1520, training loss: 0.06807772070169449 = 0.0014668850926682353 + 0.01 * 6.661083698272705
Epoch 1520, val loss: 1.2394766807556152
Epoch 1530, training loss: 0.06816402822732925 = 0.0014543378492817283 + 0.01 * 6.670969486236572
Epoch 1530, val loss: 1.2406878471374512
Epoch 1540, training loss: 0.06790493428707123 = 0.0014423619722947478 + 0.01 * 6.646256923675537
Epoch 1540, val loss: 1.2423574924468994
Epoch 1550, training loss: 0.0681958869099617 = 0.0014304127544164658 + 0.01 * 6.676547050476074
Epoch 1550, val loss: 1.2436097860336304
Epoch 1560, training loss: 0.06785692274570465 = 0.0014187042834237218 + 0.01 * 6.643822193145752
Epoch 1560, val loss: 1.2448302507400513
Epoch 1570, training loss: 0.06766925752162933 = 0.001407420844770968 + 0.01 * 6.626183986663818
Epoch 1570, val loss: 1.2461687326431274
Epoch 1580, training loss: 0.06792493909597397 = 0.0013961782678961754 + 0.01 * 6.652875900268555
Epoch 1580, val loss: 1.247410535812378
Epoch 1590, training loss: 0.06769300997257233 = 0.0013853305717930198 + 0.01 * 6.630768299102783
Epoch 1590, val loss: 1.2488559484481812
Epoch 1600, training loss: 0.06764048337936401 = 0.0013745781034231186 + 0.01 * 6.626590728759766
Epoch 1600, val loss: 1.2501863241195679
Epoch 1610, training loss: 0.06778274476528168 = 0.0013639931567013264 + 0.01 * 6.641875267028809
Epoch 1610, val loss: 1.2515965700149536
Epoch 1620, training loss: 0.06758687645196915 = 0.0013537730555981398 + 0.01 * 6.623310565948486
Epoch 1620, val loss: 1.2528033256530762
Epoch 1630, training loss: 0.06782162189483643 = 0.0013436834560707211 + 0.01 * 6.647794246673584
Epoch 1630, val loss: 1.2540723085403442
Epoch 1640, training loss: 0.06767386943101883 = 0.0013337846612557769 + 0.01 * 6.634008407592773
Epoch 1640, val loss: 1.2554912567138672
Epoch 1650, training loss: 0.06746233999729156 = 0.0013240016996860504 + 0.01 * 6.613833427429199
Epoch 1650, val loss: 1.256874918937683
Epoch 1660, training loss: 0.06753060966730118 = 0.0013145196717232466 + 0.01 * 6.621608734130859
Epoch 1660, val loss: 1.2579487562179565
Epoch 1670, training loss: 0.06746823340654373 = 0.0013052711728960276 + 0.01 * 6.61629581451416
Epoch 1670, val loss: 1.2592573165893555
Epoch 1680, training loss: 0.06735590100288391 = 0.0012961078900843859 + 0.01 * 6.6059794425964355
Epoch 1680, val loss: 1.2606321573257446
Epoch 1690, training loss: 0.06755296140909195 = 0.0012870283098891377 + 0.01 * 6.626593589782715
Epoch 1690, val loss: 1.261901617050171
Epoch 1700, training loss: 0.06743142753839493 = 0.001278224983252585 + 0.01 * 6.615320682525635
Epoch 1700, val loss: 1.2629539966583252
Epoch 1710, training loss: 0.06731138378381729 = 0.0012696441262960434 + 0.01 * 6.6041741371154785
Epoch 1710, val loss: 1.2640862464904785
Epoch 1720, training loss: 0.06724625825881958 = 0.0012610881822183728 + 0.01 * 6.598517417907715
Epoch 1720, val loss: 1.2655279636383057
Epoch 1730, training loss: 0.06732591986656189 = 0.0012527686776593328 + 0.01 * 6.6073150634765625
Epoch 1730, val loss: 1.2666586637496948
Epoch 1740, training loss: 0.06750794500112534 = 0.0012447134358808398 + 0.01 * 6.6263227462768555
Epoch 1740, val loss: 1.2677571773529053
Epoch 1750, training loss: 0.06702646613121033 = 0.0012365382863208652 + 0.01 * 6.578993320465088
Epoch 1750, val loss: 1.2689332962036133
Epoch 1760, training loss: 0.06711957603693008 = 0.0012286528944969177 + 0.01 * 6.589092254638672
Epoch 1760, val loss: 1.2700787782669067
Epoch 1770, training loss: 0.06730229407548904 = 0.0012208514381200075 + 0.01 * 6.608144283294678
Epoch 1770, val loss: 1.271032452583313
Epoch 1780, training loss: 0.06701931357383728 = 0.00121321901679039 + 0.01 * 6.580610275268555
Epoch 1780, val loss: 1.2723398208618164
Epoch 1790, training loss: 0.06732556223869324 = 0.0012056907871738076 + 0.01 * 6.611987113952637
Epoch 1790, val loss: 1.2734014987945557
Epoch 1800, training loss: 0.06725508719682693 = 0.0011982796713709831 + 0.01 * 6.605681419372559
Epoch 1800, val loss: 1.2746325731277466
Epoch 1810, training loss: 0.06691279262304306 = 0.0011910211760550737 + 0.01 * 6.572177410125732
Epoch 1810, val loss: 1.2756972312927246
Epoch 1820, training loss: 0.06700433045625687 = 0.0011838085483759642 + 0.01 * 6.582052707672119
Epoch 1820, val loss: 1.2766319513320923
Epoch 1830, training loss: 0.06702272593975067 = 0.0011767989490181208 + 0.01 * 6.584592819213867
Epoch 1830, val loss: 1.2777230739593506
Epoch 1840, training loss: 0.06690388172864914 = 0.0011698619928210974 + 0.01 * 6.573402404785156
Epoch 1840, val loss: 1.2789348363876343
Epoch 1850, training loss: 0.06698422133922577 = 0.0011629584478214383 + 0.01 * 6.582126140594482
Epoch 1850, val loss: 1.2798206806182861
Epoch 1860, training loss: 0.06695099920034409 = 0.0011562408180907369 + 0.01 * 6.579476356506348
Epoch 1860, val loss: 1.2810463905334473
Epoch 1870, training loss: 0.06676200777292252 = 0.001149588031694293 + 0.01 * 6.56124210357666
Epoch 1870, val loss: 1.2821142673492432
Epoch 1880, training loss: 0.06694637984037399 = 0.0011430415324866772 + 0.01 * 6.580334663391113
Epoch 1880, val loss: 1.283141016960144
Epoch 1890, training loss: 0.06694907695055008 = 0.0011366301914677024 + 0.01 * 6.581244945526123
Epoch 1890, val loss: 1.284209966659546
Epoch 1900, training loss: 0.06681951880455017 = 0.0011302540078759193 + 0.01 * 6.568926811218262
Epoch 1900, val loss: 1.2851760387420654
Epoch 1910, training loss: 0.06679724901914597 = 0.0011240127496421337 + 0.01 * 6.567324161529541
Epoch 1910, val loss: 1.2862690687179565
Epoch 1920, training loss: 0.06685490161180496 = 0.0011178436689078808 + 0.01 * 6.57370662689209
Epoch 1920, val loss: 1.287313461303711
Epoch 1930, training loss: 0.06678219139575958 = 0.001111786114051938 + 0.01 * 6.567040920257568
Epoch 1930, val loss: 1.2883776426315308
Epoch 1940, training loss: 0.0666884034872055 = 0.0011057173833251 + 0.01 * 6.5582685470581055
Epoch 1940, val loss: 1.2894189357757568
Epoch 1950, training loss: 0.0668058767914772 = 0.0010998372454196215 + 0.01 * 6.570603847503662
Epoch 1950, val loss: 1.2904279232025146
Epoch 1960, training loss: 0.06658784300088882 = 0.0010939986677840352 + 0.01 * 6.549384593963623
Epoch 1960, val loss: 1.2915157079696655
Epoch 1970, training loss: 0.06694076210260391 = 0.0010882779024541378 + 0.01 * 6.5852484703063965
Epoch 1970, val loss: 1.2926294803619385
Epoch 1980, training loss: 0.06693670898675919 = 0.0010826012585312128 + 0.01 * 6.585411071777344
Epoch 1980, val loss: 1.2936434745788574
Epoch 1990, training loss: 0.06647351384162903 = 0.0010769887594506145 + 0.01 * 6.539652347564697
Epoch 1990, val loss: 1.2946836948394775
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.7928307854507117
=== training gcn model ===
Epoch 0, training loss: 2.043731927871704 = 1.957763671875 + 0.01 * 8.596833229064941
Epoch 0, val loss: 1.9577091932296753
Epoch 10, training loss: 2.0337228775024414 = 1.947754979133606 + 0.01 * 8.596780776977539
Epoch 10, val loss: 1.9485206604003906
Epoch 20, training loss: 2.0219054222106934 = 1.9359394311904907 + 0.01 * 8.596590042114258
Epoch 20, val loss: 1.9372823238372803
Epoch 30, training loss: 2.0060272216796875 = 1.9200674295425415 + 0.01 * 8.59598159790039
Epoch 30, val loss: 1.9217509031295776
Epoch 40, training loss: 1.9831949472427368 = 1.8972660303115845 + 0.01 * 8.592887878417969
Epoch 40, val loss: 1.8992542028427124
Epoch 50, training loss: 1.9505751132965088 = 1.8648483753204346 + 0.01 * 8.57266902923584
Epoch 50, val loss: 1.8680999279022217
Epoch 60, training loss: 1.9098894596099854 = 1.825011134147644 + 0.01 * 8.487832069396973
Epoch 60, val loss: 1.832281231880188
Epoch 70, training loss: 1.870611310005188 = 1.7886981964111328 + 0.01 * 8.191315650939941
Epoch 70, val loss: 1.8019367456436157
Epoch 80, training loss: 1.8344025611877441 = 1.7539894580841064 + 0.01 * 8.041311264038086
Epoch 80, val loss: 1.7695342302322388
Epoch 90, training loss: 1.7855089902877808 = 1.707582712173462 + 0.01 * 7.792625427246094
Epoch 90, val loss: 1.7253385782241821
Epoch 100, training loss: 1.7193559408187866 = 1.6437376737594604 + 0.01 * 7.561830520629883
Epoch 100, val loss: 1.6692403554916382
Epoch 110, training loss: 1.6343779563903809 = 1.5601853132247925 + 0.01 * 7.419259548187256
Epoch 110, val loss: 1.598298192024231
Epoch 120, training loss: 1.5344966650009155 = 1.460831642150879 + 0.01 * 7.366497039794922
Epoch 120, val loss: 1.5149307250976562
Epoch 130, training loss: 1.4278175830841064 = 1.354485273361206 + 0.01 * 7.333227157592773
Epoch 130, val loss: 1.4278846979141235
Epoch 140, training loss: 1.3217228651046753 = 1.2486975193023682 + 0.01 * 7.302531719207764
Epoch 140, val loss: 1.3432602882385254
Epoch 150, training loss: 1.2217940092086792 = 1.1490635871887207 + 0.01 * 7.273045063018799
Epoch 150, val loss: 1.2659118175506592
Epoch 160, training loss: 1.1311053037643433 = 1.0586309432983398 + 0.01 * 7.247436046600342
Epoch 160, val loss: 1.199041485786438
Epoch 170, training loss: 1.0497260093688965 = 0.9774370789527893 + 0.01 * 7.228894233703613
Epoch 170, val loss: 1.1424757242202759
Epoch 180, training loss: 0.975501537322998 = 0.9033353328704834 + 0.01 * 7.216620922088623
Epoch 180, val loss: 1.094021201133728
Epoch 190, training loss: 0.9057605862617493 = 0.8337075710296631 + 0.01 * 7.205301761627197
Epoch 190, val loss: 1.0507742166519165
Epoch 200, training loss: 0.8383588790893555 = 0.7664356827735901 + 0.01 * 7.192322731018066
Epoch 200, val loss: 1.0106163024902344
Epoch 210, training loss: 0.7722026109695435 = 0.7004413604736328 + 0.01 * 7.176123142242432
Epoch 210, val loss: 0.9734026789665222
Epoch 220, training loss: 0.7070349454879761 = 0.635474681854248 + 0.01 * 7.156023979187012
Epoch 220, val loss: 0.9390007853507996
Epoch 230, training loss: 0.6431369185447693 = 0.571776270866394 + 0.01 * 7.136063575744629
Epoch 230, val loss: 0.9074575901031494
Epoch 240, training loss: 0.5810481905937195 = 0.5098413825035095 + 0.01 * 7.120682716369629
Epoch 240, val loss: 0.8789830803871155
Epoch 250, training loss: 0.5214536190032959 = 0.45035555958747864 + 0.01 * 7.10980749130249
Epoch 250, val loss: 0.8544507622718811
Epoch 260, training loss: 0.46532437205314636 = 0.3942752480506897 + 0.01 * 7.104913234710693
Epoch 260, val loss: 0.8349061012268066
Epoch 270, training loss: 0.4136960804462433 = 0.34269288182258606 + 0.01 * 7.100320339202881
Epoch 270, val loss: 0.8210214376449585
Epoch 280, training loss: 0.36745429039001465 = 0.29646676778793335 + 0.01 * 7.098751068115234
Epoch 280, val loss: 0.8130242824554443
Epoch 290, training loss: 0.3269011378288269 = 0.2559290826320648 + 0.01 * 7.097204685211182
Epoch 290, val loss: 0.8107290267944336
Epoch 300, training loss: 0.29185470938682556 = 0.22088785469532013 + 0.01 * 7.096685886383057
Epoch 300, val loss: 0.8135769963264465
Epoch 310, training loss: 0.26180312037467957 = 0.19084760546684265 + 0.01 * 7.095551490783691
Epoch 310, val loss: 0.8206799030303955
Epoch 320, training loss: 0.23615504801273346 = 0.16520723700523376 + 0.01 * 7.094781398773193
Epoch 320, val loss: 0.8311598300933838
Epoch 330, training loss: 0.21428707242012024 = 0.14335750043392181 + 0.01 * 7.092957019805908
Epoch 330, val loss: 0.8444342613220215
Epoch 340, training loss: 0.19566422700881958 = 0.12474707514047623 + 0.01 * 7.091715335845947
Epoch 340, val loss: 0.8598257303237915
Epoch 350, training loss: 0.17979782819747925 = 0.10889443010091782 + 0.01 * 7.090339660644531
Epoch 350, val loss: 0.8767319321632385
Epoch 360, training loss: 0.1662842333316803 = 0.09537994116544724 + 0.01 * 7.090429782867432
Epoch 360, val loss: 0.8945510983467102
Epoch 370, training loss: 0.15469805896282196 = 0.08384513854980469 + 0.01 * 7.085292339324951
Epoch 370, val loss: 0.9129285216331482
Epoch 380, training loss: 0.14480331540107727 = 0.07398176193237305 + 0.01 * 7.082155227661133
Epoch 380, val loss: 0.9316530227661133
Epoch 390, training loss: 0.13631370663642883 = 0.0655280128121376 + 0.01 * 7.078568935394287
Epoch 390, val loss: 0.950463056564331
Epoch 400, training loss: 0.1290222704410553 = 0.058265022933483124 + 0.01 * 7.0757246017456055
Epoch 400, val loss: 0.9691933989524841
Epoch 410, training loss: 0.1227094829082489 = 0.052008938044309616 + 0.01 * 7.07005500793457
Epoch 410, val loss: 0.9877195954322815
Epoch 420, training loss: 0.11724037677049637 = 0.04660401493310928 + 0.01 * 7.063636302947998
Epoch 420, val loss: 1.0059252977371216
Epoch 430, training loss: 0.11255744844675064 = 0.04192206263542175 + 0.01 * 7.063539028167725
Epoch 430, val loss: 1.0238162279129028
Epoch 440, training loss: 0.10839097201824188 = 0.037855904549360275 + 0.01 * 7.053507328033447
Epoch 440, val loss: 1.0411739349365234
Epoch 450, training loss: 0.10474566370248795 = 0.0343111976981163 + 0.01 * 7.043447017669678
Epoch 450, val loss: 1.058120608329773
Epoch 460, training loss: 0.1015959233045578 = 0.031211603432893753 + 0.01 * 7.0384321212768555
Epoch 460, val loss: 1.074566125869751
Epoch 470, training loss: 0.09879161417484283 = 0.028494488447904587 + 0.01 * 7.029712677001953
Epoch 470, val loss: 1.090509295463562
Epoch 480, training loss: 0.09635089337825775 = 0.026103220880031586 + 0.01 * 7.0247673988342285
Epoch 480, val loss: 1.106002688407898
Epoch 490, training loss: 0.09428659826517105 = 0.023991264402866364 + 0.01 * 7.029533863067627
Epoch 490, val loss: 1.1209828853607178
Epoch 500, training loss: 0.09217311441898346 = 0.022122012451291084 + 0.01 * 7.005110740661621
Epoch 500, val loss: 1.1354228258132935
Epoch 510, training loss: 0.09050782769918442 = 0.020460747182369232 + 0.01 * 7.004708290100098
Epoch 510, val loss: 1.1493794918060303
Epoch 520, training loss: 0.08891448378562927 = 0.018978925421833992 + 0.01 * 6.993556499481201
Epoch 520, val loss: 1.1629163026809692
Epoch 530, training loss: 0.08754753321409225 = 0.017653340473771095 + 0.01 * 6.989419937133789
Epoch 530, val loss: 1.1759206056594849
Epoch 540, training loss: 0.08626356720924377 = 0.016464687883853912 + 0.01 * 6.979887962341309
Epoch 540, val loss: 1.1885230541229248
Epoch 550, training loss: 0.08520236611366272 = 0.01539380569010973 + 0.01 * 6.980855941772461
Epoch 550, val loss: 1.2006946802139282
Epoch 560, training loss: 0.08416491001844406 = 0.014426371082663536 + 0.01 * 6.973853588104248
Epoch 560, val loss: 1.2125113010406494
Epoch 570, training loss: 0.08318374305963516 = 0.013549355790019035 + 0.01 * 6.963438510894775
Epoch 570, val loss: 1.2239100933074951
Epoch 580, training loss: 0.08234325796365738 = 0.012751499190926552 + 0.01 * 6.959176063537598
Epoch 580, val loss: 1.234998106956482
Epoch 590, training loss: 0.08158904314041138 = 0.012023882940411568 + 0.01 * 6.956516265869141
Epoch 590, val loss: 1.2457588911056519
Epoch 600, training loss: 0.08092936128377914 = 0.01135936938226223 + 0.01 * 6.956998825073242
Epoch 600, val loss: 1.256165623664856
Epoch 610, training loss: 0.08020215481519699 = 0.010750859044492245 + 0.01 * 6.94512939453125
Epoch 610, val loss: 1.266211986541748
Epoch 620, training loss: 0.07957589626312256 = 0.01019216701388359 + 0.01 * 6.938372611999512
Epoch 620, val loss: 1.2760632038116455
Epoch 630, training loss: 0.0789870172739029 = 0.00967805553227663 + 0.01 * 6.930896282196045
Epoch 630, val loss: 1.2855693101882935
Epoch 640, training loss: 0.07845194637775421 = 0.009203669615089893 + 0.01 * 6.924827575683594
Epoch 640, val loss: 1.2948182821273804
Epoch 650, training loss: 0.0779477208852768 = 0.008765215054154396 + 0.01 * 6.918250560760498
Epoch 650, val loss: 1.30384361743927
Epoch 660, training loss: 0.077442467212677 = 0.008359210565686226 + 0.01 * 6.908325672149658
Epoch 660, val loss: 1.3126548528671265
Epoch 670, training loss: 0.07713332772254944 = 0.007982665672898293 + 0.01 * 6.915066242218018
Epoch 670, val loss: 1.3211370706558228
Epoch 680, training loss: 0.07674115151166916 = 0.0076337396167218685 + 0.01 * 6.910741806030273
Epoch 680, val loss: 1.3294250965118408
Epoch 690, training loss: 0.07626288384199142 = 0.007309066131711006 + 0.01 * 6.895382404327393
Epoch 690, val loss: 1.3374760150909424
Epoch 700, training loss: 0.07602542638778687 = 0.0070063285529613495 + 0.01 * 6.901909351348877
Epoch 700, val loss: 1.3453813791275024
Epoch 710, training loss: 0.0755913108587265 = 0.006723688915371895 + 0.01 * 6.886762619018555
Epoch 710, val loss: 1.3530097007751465
Epoch 720, training loss: 0.07527828961610794 = 0.006459368392825127 + 0.01 * 6.881892204284668
Epoch 720, val loss: 1.3604930639266968
Epoch 730, training loss: 0.07509022951126099 = 0.006212064530700445 + 0.01 * 6.887816429138184
Epoch 730, val loss: 1.3677341938018799
Epoch 740, training loss: 0.07473041117191315 = 0.005980278830975294 + 0.01 * 6.87501335144043
Epoch 740, val loss: 1.3748821020126343
Epoch 750, training loss: 0.074503093957901 = 0.005762646906077862 + 0.01 * 6.874044895172119
Epoch 750, val loss: 1.3817853927612305
Epoch 760, training loss: 0.0741397961974144 = 0.0055582961067557335 + 0.01 * 6.858150005340576
Epoch 760, val loss: 1.3885530233383179
Epoch 770, training loss: 0.07398038357496262 = 0.005365879274904728 + 0.01 * 6.861450672149658
Epoch 770, val loss: 1.395156979560852
Epoch 780, training loss: 0.0738367810845375 = 0.005184534005820751 + 0.01 * 6.865224838256836
Epoch 780, val loss: 1.4016084671020508
Epoch 790, training loss: 0.0736561119556427 = 0.0050132363103330135 + 0.01 * 6.864287853240967
Epoch 790, val loss: 1.407905101776123
Epoch 800, training loss: 0.07335084676742554 = 0.0048514544032514095 + 0.01 * 6.849939346313477
Epoch 800, val loss: 1.4139854907989502
Epoch 810, training loss: 0.07332275807857513 = 0.004698542412370443 + 0.01 * 6.86242151260376
Epoch 810, val loss: 1.4199684858322144
Epoch 820, training loss: 0.07297688722610474 = 0.004553598817437887 + 0.01 * 6.842329025268555
Epoch 820, val loss: 1.4258325099945068
Epoch 830, training loss: 0.07284693419933319 = 0.004416089504957199 + 0.01 * 6.843084812164307
Epoch 830, val loss: 1.4315249919891357
Epoch 840, training loss: 0.07263398915529251 = 0.004286156967282295 + 0.01 * 6.834783554077148
Epoch 840, val loss: 1.4370759725570679
Epoch 850, training loss: 0.07253269851207733 = 0.0041627730242908 + 0.01 * 6.8369927406311035
Epoch 850, val loss: 1.4424769878387451
Epoch 860, training loss: 0.07236112654209137 = 0.004045413341373205 + 0.01 * 6.831571578979492
Epoch 860, val loss: 1.4477858543395996
Epoch 870, training loss: 0.07212129235267639 = 0.0039338222704827785 + 0.01 * 6.818747043609619
Epoch 870, val loss: 1.453018307685852
Epoch 880, training loss: 0.07211720943450928 = 0.003827520413324237 + 0.01 * 6.828969478607178
Epoch 880, val loss: 1.4580966234207153
Epoch 890, training loss: 0.07193781435489655 = 0.0037263776175677776 + 0.01 * 6.821144104003906
Epoch 890, val loss: 1.4630645513534546
Epoch 900, training loss: 0.07188668102025986 = 0.003629930317401886 + 0.01 * 6.8256754875183105
Epoch 900, val loss: 1.4679210186004639
Epoch 910, training loss: 0.07161836326122284 = 0.0035382509231567383 + 0.01 * 6.808011531829834
Epoch 910, val loss: 1.4726107120513916
Epoch 920, training loss: 0.07159694284200668 = 0.0034506379161030054 + 0.01 * 6.814630508422852
Epoch 920, val loss: 1.477198600769043
Epoch 930, training loss: 0.07142000645399094 = 0.0033668752294033766 + 0.01 * 6.805313587188721
Epoch 930, val loss: 1.4817839860916138
Epoch 940, training loss: 0.07136255502700806 = 0.0032868036068975925 + 0.01 * 6.807575225830078
Epoch 940, val loss: 1.486203908920288
Epoch 950, training loss: 0.07124608755111694 = 0.0032103639096021652 + 0.01 * 6.803572177886963
Epoch 950, val loss: 1.4905307292938232
Epoch 960, training loss: 0.07113655656576157 = 0.0031370166689157486 + 0.01 * 6.799954414367676
Epoch 960, val loss: 1.4947437047958374
Epoch 970, training loss: 0.07101660221815109 = 0.0030668992549180984 + 0.01 * 6.794970989227295
Epoch 970, val loss: 1.4989210367202759
Epoch 980, training loss: 0.0710236132144928 = 0.0029996635857969522 + 0.01 * 6.802395343780518
Epoch 980, val loss: 1.503005862236023
Epoch 990, training loss: 0.07097286731004715 = 0.0029351243283599615 + 0.01 * 6.803774356842041
Epoch 990, val loss: 1.5069562196731567
Epoch 1000, training loss: 0.07075843214988708 = 0.002873393939808011 + 0.01 * 6.788504123687744
Epoch 1000, val loss: 1.5108510255813599
Epoch 1010, training loss: 0.07064232975244522 = 0.0028139951173216105 + 0.01 * 6.782833576202393
Epoch 1010, val loss: 1.5146368741989136
Epoch 1020, training loss: 0.07081782817840576 = 0.0027569688390940428 + 0.01 * 6.80608606338501
Epoch 1020, val loss: 1.5183501243591309
Epoch 1030, training loss: 0.07062465697526932 = 0.002702136989682913 + 0.01 * 6.7922515869140625
Epoch 1030, val loss: 1.5220404863357544
Epoch 1040, training loss: 0.0704837292432785 = 0.002649699803441763 + 0.01 * 6.783402919769287
Epoch 1040, val loss: 1.5255463123321533
Epoch 1050, training loss: 0.07037342339754105 = 0.0025989795103669167 + 0.01 * 6.777444362640381
Epoch 1050, val loss: 1.5290573835372925
Epoch 1060, training loss: 0.0702492818236351 = 0.0025502063799649477 + 0.01 * 6.769907474517822
Epoch 1060, val loss: 1.5324958562850952
Epoch 1070, training loss: 0.07022450864315033 = 0.0025031641125679016 + 0.01 * 6.772134304046631
Epoch 1070, val loss: 1.5358483791351318
Epoch 1080, training loss: 0.07030508667230606 = 0.002458015689626336 + 0.01 * 6.784707546234131
Epoch 1080, val loss: 1.5391433238983154
Epoch 1090, training loss: 0.07016871869564056 = 0.0024144204799085855 + 0.01 * 6.775430202484131
Epoch 1090, val loss: 1.5423396825790405
Epoch 1100, training loss: 0.07012785971164703 = 0.0023725347127765417 + 0.01 * 6.7755327224731445
Epoch 1100, val loss: 1.5454480648040771
Epoch 1110, training loss: 0.06989853084087372 = 0.002332002390176058 + 0.01 * 6.75665283203125
Epoch 1110, val loss: 1.5485550165176392
Epoch 1120, training loss: 0.07011719793081284 = 0.0022928318940103054 + 0.01 * 6.782436370849609
Epoch 1120, val loss: 1.5515943765640259
Epoch 1130, training loss: 0.06988710165023804 = 0.0022552593145519495 + 0.01 * 6.763184070587158
Epoch 1130, val loss: 1.554559588432312
Epoch 1140, training loss: 0.06974413245916367 = 0.002218886511400342 + 0.01 * 6.752524375915527
Epoch 1140, val loss: 1.5574026107788086
Epoch 1150, training loss: 0.06977315992116928 = 0.0021836874075233936 + 0.01 * 6.758947372436523
Epoch 1150, val loss: 1.5602307319641113
Epoch 1160, training loss: 0.06986682862043381 = 0.0021496322005987167 + 0.01 * 6.771719932556152
Epoch 1160, val loss: 1.5630704164505005
Epoch 1170, training loss: 0.06958360970020294 = 0.0021168740931898355 + 0.01 * 6.746673583984375
Epoch 1170, val loss: 1.5657806396484375
Epoch 1180, training loss: 0.06952055543661118 = 0.002085027750581503 + 0.01 * 6.7435526847839355
Epoch 1180, val loss: 1.5684537887573242
Epoch 1190, training loss: 0.06966915726661682 = 0.0020542943384498358 + 0.01 * 6.761486530303955
Epoch 1190, val loss: 1.5711047649383545
Epoch 1200, training loss: 0.06946519762277603 = 0.0020244221668690443 + 0.01 * 6.744077682495117
Epoch 1200, val loss: 1.5736417770385742
Epoch 1210, training loss: 0.06953967362642288 = 0.0019955611787736416 + 0.01 * 6.754411220550537
Epoch 1210, val loss: 1.5761445760726929
Epoch 1220, training loss: 0.06959915161132812 = 0.001967557007446885 + 0.01 * 6.76315975189209
Epoch 1220, val loss: 1.5786902904510498
Epoch 1230, training loss: 0.06938280910253525 = 0.0019406563369557261 + 0.01 * 6.744215488433838
Epoch 1230, val loss: 1.5810810327529907
Epoch 1240, training loss: 0.06933705508708954 = 0.0019143607933074236 + 0.01 * 6.742269515991211
Epoch 1240, val loss: 1.5834640264511108
Epoch 1250, training loss: 0.06967717409133911 = 0.0018889443017542362 + 0.01 * 6.778822898864746
Epoch 1250, val loss: 1.5857983827590942
Epoch 1260, training loss: 0.06915167719125748 = 0.0018642242066562176 + 0.01 * 6.728745460510254
Epoch 1260, val loss: 1.5880972146987915
Epoch 1270, training loss: 0.06921307742595673 = 0.001840276992879808 + 0.01 * 6.737279891967773
Epoch 1270, val loss: 1.5903620719909668
Epoch 1280, training loss: 0.06905588507652283 = 0.001817133161239326 + 0.01 * 6.723875522613525
Epoch 1280, val loss: 1.5925754308700562
Epoch 1290, training loss: 0.06916922330856323 = 0.0017946000443771482 + 0.01 * 6.737462520599365
Epoch 1290, val loss: 1.5947248935699463
Epoch 1300, training loss: 0.06911037862300873 = 0.0017726363148540258 + 0.01 * 6.733774662017822
Epoch 1300, val loss: 1.596950888633728
Epoch 1310, training loss: 0.06904616951942444 = 0.0017514133360236883 + 0.01 * 6.729475498199463
Epoch 1310, val loss: 1.5990153551101685
Epoch 1320, training loss: 0.06909342110157013 = 0.001730753923766315 + 0.01 * 6.736266613006592
Epoch 1320, val loss: 1.6011040210723877
Epoch 1330, training loss: 0.06898559629917145 = 0.0017106964951381087 + 0.01 * 6.727489948272705
Epoch 1330, val loss: 1.6030986309051514
Epoch 1340, training loss: 0.06897205114364624 = 0.0016910989070311189 + 0.01 * 6.728095531463623
Epoch 1340, val loss: 1.6051766872406006
Epoch 1350, training loss: 0.06892627477645874 = 0.001672136364504695 + 0.01 * 6.725414276123047
Epoch 1350, val loss: 1.607116460800171
Epoch 1360, training loss: 0.06905833631753922 = 0.0016537593910470605 + 0.01 * 6.740458011627197
Epoch 1360, val loss: 1.6090749502182007
Epoch 1370, training loss: 0.06877592206001282 = 0.0016357017448171973 + 0.01 * 6.714022636413574
Epoch 1370, val loss: 1.6109459400177002
Epoch 1380, training loss: 0.06906379759311676 = 0.0016182385152205825 + 0.01 * 6.744555950164795
Epoch 1380, val loss: 1.6127570867538452
Epoch 1390, training loss: 0.06865309178829193 = 0.001601228374056518 + 0.01 * 6.705186367034912
Epoch 1390, val loss: 1.6145715713500977
Epoch 1400, training loss: 0.06866051256656647 = 0.0015846648020669818 + 0.01 * 6.707584857940674
Epoch 1400, val loss: 1.6163718700408936
Epoch 1410, training loss: 0.06868093460798264 = 0.0015685224207118154 + 0.01 * 6.711241245269775
Epoch 1410, val loss: 1.6181640625
Epoch 1420, training loss: 0.06906573474407196 = 0.001552715664729476 + 0.01 * 6.751302242279053
Epoch 1420, val loss: 1.6199405193328857
Epoch 1430, training loss: 0.06854935735464096 = 0.0015375850489363074 + 0.01 * 6.701177597045898
Epoch 1430, val loss: 1.6216317415237427
Epoch 1440, training loss: 0.06857079267501831 = 0.0015227266121655703 + 0.01 * 6.704806804656982
Epoch 1440, val loss: 1.623246192932129
Epoch 1450, training loss: 0.0684547871351242 = 0.0015081750461831689 + 0.01 * 6.6946611404418945
Epoch 1450, val loss: 1.624889850616455
Epoch 1460, training loss: 0.06850455701351166 = 0.0014940050896257162 + 0.01 * 6.70105504989624
Epoch 1460, val loss: 1.626507043838501
Epoch 1470, training loss: 0.06864966452121735 = 0.001480136183090508 + 0.01 * 6.716952323913574
Epoch 1470, val loss: 1.628177523612976
Epoch 1480, training loss: 0.06871416419744492 = 0.0014667294453829527 + 0.01 * 6.724743366241455
Epoch 1480, val loss: 1.6297396421432495
Epoch 1490, training loss: 0.06852325052022934 = 0.001453503267839551 + 0.01 * 6.706974983215332
Epoch 1490, val loss: 1.6312358379364014
Epoch 1500, training loss: 0.06856287270784378 = 0.0014408366987481713 + 0.01 * 6.7122039794921875
Epoch 1500, val loss: 1.6327781677246094
Epoch 1510, training loss: 0.06834837794303894 = 0.0014282353222370148 + 0.01 * 6.692014217376709
Epoch 1510, val loss: 1.6342179775238037
Epoch 1520, training loss: 0.06845860183238983 = 0.0014160637510940433 + 0.01 * 6.704254150390625
Epoch 1520, val loss: 1.6356717348098755
Epoch 1530, training loss: 0.06835751235485077 = 0.0014041127869859338 + 0.01 * 6.695339679718018
Epoch 1530, val loss: 1.6371424198150635
Epoch 1540, training loss: 0.06837952136993408 = 0.0013924406375735998 + 0.01 * 6.6987080574035645
Epoch 1540, val loss: 1.6385351419448853
Epoch 1550, training loss: 0.06824513524770737 = 0.0013810915406793356 + 0.01 * 6.686404228210449
Epoch 1550, val loss: 1.639932632446289
Epoch 1560, training loss: 0.06847582757472992 = 0.0013700049603357911 + 0.01 * 6.710582733154297
Epoch 1560, val loss: 1.6412502527236938
Epoch 1570, training loss: 0.06811080127954483 = 0.0013590866001322865 + 0.01 * 6.6751708984375
Epoch 1570, val loss: 1.6426335573196411
Epoch 1580, training loss: 0.0680823028087616 = 0.0013484638184309006 + 0.01 * 6.673383712768555
Epoch 1580, val loss: 1.6439534425735474
Epoch 1590, training loss: 0.06836661696434021 = 0.0013381834141910076 + 0.01 * 6.70284366607666
Epoch 1590, val loss: 1.6452499628067017
Epoch 1600, training loss: 0.0680335983633995 = 0.001327931648120284 + 0.01 * 6.670566558837891
Epoch 1600, val loss: 1.6465988159179688
Epoch 1610, training loss: 0.06801804155111313 = 0.001318060327321291 + 0.01 * 6.6699981689453125
Epoch 1610, val loss: 1.6477974653244019
Epoch 1620, training loss: 0.06823692470788956 = 0.0013083807425573468 + 0.01 * 6.692854404449463
Epoch 1620, val loss: 1.649078607559204
Epoch 1630, training loss: 0.06797954440116882 = 0.0012988667003810406 + 0.01 * 6.668067932128906
Epoch 1630, val loss: 1.6502882242202759
Epoch 1640, training loss: 0.06800341606140137 = 0.0012895890977233648 + 0.01 * 6.671382904052734
Epoch 1640, val loss: 1.651472806930542
Epoch 1650, training loss: 0.06792597472667694 = 0.0012805761070922017 + 0.01 * 6.664539813995361
Epoch 1650, val loss: 1.652662992477417
Epoch 1660, training loss: 0.0682721883058548 = 0.0012717022327706218 + 0.01 * 6.700048923492432
Epoch 1660, val loss: 1.6538400650024414
Epoch 1670, training loss: 0.0680563896894455 = 0.0012629656121134758 + 0.01 * 6.679342746734619
Epoch 1670, val loss: 1.6548840999603271
Epoch 1680, training loss: 0.06802627444267273 = 0.0012544734636321664 + 0.01 * 6.67717981338501
Epoch 1680, val loss: 1.655990719795227
Epoch 1690, training loss: 0.06769043952226639 = 0.0012461119331419468 + 0.01 * 6.64443302154541
Epoch 1690, val loss: 1.6570930480957031
Epoch 1700, training loss: 0.0677005872130394 = 0.0012379527324810624 + 0.01 * 6.646263599395752
Epoch 1700, val loss: 1.6581363677978516
Epoch 1710, training loss: 0.06803064793348312 = 0.0012300128582865 + 0.01 * 6.680063247680664
Epoch 1710, val loss: 1.6592448949813843
Epoch 1720, training loss: 0.06778642535209656 = 0.0012220887001603842 + 0.01 * 6.656433582305908
Epoch 1720, val loss: 1.6602816581726074
Epoch 1730, training loss: 0.06766530126333237 = 0.0012144384672865272 + 0.01 * 6.645086288452148
Epoch 1730, val loss: 1.661307454109192
Epoch 1740, training loss: 0.06765396893024445 = 0.0012068648356944323 + 0.01 * 6.644710063934326
Epoch 1740, val loss: 1.662240982055664
Epoch 1750, training loss: 0.06748289614915848 = 0.001199489226564765 + 0.01 * 6.628340721130371
Epoch 1750, val loss: 1.6633201837539673
Epoch 1760, training loss: 0.06767264008522034 = 0.0011921928962692618 + 0.01 * 6.648045063018799
Epoch 1760, val loss: 1.664252519607544
Epoch 1770, training loss: 0.0675540342926979 = 0.0011851016897708178 + 0.01 * 6.636893272399902
Epoch 1770, val loss: 1.665225863456726
Epoch 1780, training loss: 0.06760969012975693 = 0.0011781122302636504 + 0.01 * 6.643157958984375
Epoch 1780, val loss: 1.6662143468856812
Epoch 1790, training loss: 0.06746743619441986 = 0.001171209616586566 + 0.01 * 6.629622936248779
Epoch 1790, val loss: 1.6671684980392456
Epoch 1800, training loss: 0.06776323914527893 = 0.0011643825564533472 + 0.01 * 6.659885883331299
Epoch 1800, val loss: 1.6680837869644165
Epoch 1810, training loss: 0.06752428412437439 = 0.0011578785488381982 + 0.01 * 6.636640548706055
Epoch 1810, val loss: 1.6690292358398438
Epoch 1820, training loss: 0.06740707904100418 = 0.001151326228864491 + 0.01 * 6.625576019287109
Epoch 1820, val loss: 1.669901967048645
Epoch 1830, training loss: 0.06747139245271683 = 0.0011448938166722655 + 0.01 * 6.632649898529053
Epoch 1830, val loss: 1.6708180904388428
Epoch 1840, training loss: 0.06734795868396759 = 0.0011386522091925144 + 0.01 * 6.6209306716918945
Epoch 1840, val loss: 1.6716618537902832
Epoch 1850, training loss: 0.06737588346004486 = 0.0011323525104671717 + 0.01 * 6.624353408813477
Epoch 1850, val loss: 1.6726127862930298
Epoch 1860, training loss: 0.0675651803612709 = 0.0011263283668085933 + 0.01 * 6.643885612487793
Epoch 1860, val loss: 1.673491358757019
Epoch 1870, training loss: 0.06728924065828323 = 0.0011202921159565449 + 0.01 * 6.616894721984863
Epoch 1870, val loss: 1.6743295192718506
Epoch 1880, training loss: 0.0672069638967514 = 0.0011143959127366543 + 0.01 * 6.609256744384766
Epoch 1880, val loss: 1.6751971244812012
Epoch 1890, training loss: 0.06746096909046173 = 0.0011086027370765805 + 0.01 * 6.635237216949463
Epoch 1890, val loss: 1.6760395765304565
Epoch 1900, training loss: 0.06717762351036072 = 0.0011029167799279094 + 0.01 * 6.607470989227295
Epoch 1900, val loss: 1.6768112182617188
Epoch 1910, training loss: 0.06758485734462738 = 0.001097227679565549 + 0.01 * 6.648763179779053
Epoch 1910, val loss: 1.6776798963546753
Epoch 1920, training loss: 0.06707867234945297 = 0.0010917283361777663 + 0.01 * 6.598694801330566
Epoch 1920, val loss: 1.6785533428192139
Epoch 1930, training loss: 0.06698711216449738 = 0.0010862289927899837 + 0.01 * 6.590088844299316
Epoch 1930, val loss: 1.6793274879455566
Epoch 1940, training loss: 0.06720609217882156 = 0.0010808510705828667 + 0.01 * 6.612524509429932
Epoch 1940, val loss: 1.6801550388336182
Epoch 1950, training loss: 0.06695506721735 = 0.0010755956172943115 + 0.01 * 6.587947368621826
Epoch 1950, val loss: 1.680978536605835
Epoch 1960, training loss: 0.06686228513717651 = 0.0010703528532758355 + 0.01 * 6.579193592071533
Epoch 1960, val loss: 1.6817457675933838
Epoch 1970, training loss: 0.06714785844087601 = 0.0010652310447767377 + 0.01 * 6.608262538909912
Epoch 1970, val loss: 1.6825352907180786
Epoch 1980, training loss: 0.0669364482164383 = 0.001060066162608564 + 0.01 * 6.587637901306152
Epoch 1980, val loss: 1.683337688446045
Epoch 1990, training loss: 0.06708960235118866 = 0.0010550826555117965 + 0.01 * 6.603452205657959
Epoch 1990, val loss: 1.684147834777832
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8017923036373221
The final CL Acc:0.76420, 0.02983, The final GNN Acc:0.80109, 0.00648
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13134])
remove edge: torch.Size([2, 7862])
updated graph: torch.Size([2, 10440])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.0451316833496094 = 1.9591635465621948 + 0.01 * 8.596809387207031
Epoch 0, val loss: 1.9615130424499512
Epoch 10, training loss: 2.034557819366455 = 1.9485903978347778 + 0.01 * 8.596736907958984
Epoch 10, val loss: 1.9507020711898804
Epoch 20, training loss: 2.0215823650360107 = 1.9356176853179932 + 0.01 * 8.596466064453125
Epoch 20, val loss: 1.9372899532318115
Epoch 30, training loss: 2.003505229949951 = 1.9175504446029663 + 0.01 * 8.59548282623291
Epoch 30, val loss: 1.918701410293579
Epoch 40, training loss: 1.9769335985183716 = 1.8910408020019531 + 0.01 * 8.589280128479004
Epoch 40, val loss: 1.8917139768600464
Epoch 50, training loss: 1.9390559196472168 = 1.8535761833190918 + 0.01 * 8.547972679138184
Epoch 50, val loss: 1.8549829721450806
Epoch 60, training loss: 1.8930588960647583 = 1.809510350227356 + 0.01 * 8.354853630065918
Epoch 60, val loss: 1.8154629468917847
Epoch 70, training loss: 1.852112054824829 = 1.770781397819519 + 0.01 * 8.133071899414062
Epoch 70, val loss: 1.7842013835906982
Epoch 80, training loss: 1.8083772659301758 = 1.7288625240325928 + 0.01 * 7.951470375061035
Epoch 80, val loss: 1.746962547302246
Epoch 90, training loss: 1.7474079132080078 = 1.6703184843063354 + 0.01 * 7.708946704864502
Epoch 90, val loss: 1.6935036182403564
Epoch 100, training loss: 1.6656816005706787 = 1.5908960103988647 + 0.01 * 7.478560924530029
Epoch 100, val loss: 1.6233984231948853
Epoch 110, training loss: 1.5668394565582275 = 1.4936639070510864 + 0.01 * 7.317559242248535
Epoch 110, val loss: 1.5417814254760742
Epoch 120, training loss: 1.4613685607910156 = 1.388631820678711 + 0.01 * 7.273671627044678
Epoch 120, val loss: 1.455018162727356
Epoch 130, training loss: 1.3565491437911987 = 1.2842130661010742 + 0.01 * 7.233608722686768
Epoch 130, val loss: 1.3704758882522583
Epoch 140, training loss: 1.2544585466384888 = 1.1823796033859253 + 0.01 * 7.207898139953613
Epoch 140, val loss: 1.2889432907104492
Epoch 150, training loss: 1.1568762063980103 = 1.08490788936615 + 0.01 * 7.196834087371826
Epoch 150, val loss: 1.211490511894226
Epoch 160, training loss: 1.065925121307373 = 0.9940253496170044 + 0.01 * 7.18997859954834
Epoch 160, val loss: 1.1398051977157593
Epoch 170, training loss: 0.9827417731285095 = 0.9109308123588562 + 0.01 * 7.181093692779541
Epoch 170, val loss: 1.074450969696045
Epoch 180, training loss: 0.9066951274871826 = 0.8350297212600708 + 0.01 * 7.166538238525391
Epoch 180, val loss: 1.014414668083191
Epoch 190, training loss: 0.8371379971504211 = 0.7657068371772766 + 0.01 * 7.143115520477295
Epoch 190, val loss: 0.9593619108200073
Epoch 200, training loss: 0.7739647030830383 = 0.7027791738510132 + 0.01 * 7.118551731109619
Epoch 200, val loss: 0.9099747538566589
Epoch 210, training loss: 0.716568648815155 = 0.645691454410553 + 0.01 * 7.087721824645996
Epoch 210, val loss: 0.8669552803039551
Epoch 220, training loss: 0.6638198494911194 = 0.5931293368339539 + 0.01 * 7.069051742553711
Epoch 220, val loss: 0.8299980163574219
Epoch 230, training loss: 0.6142448782920837 = 0.5436606407165527 + 0.01 * 7.05842399597168
Epoch 230, val loss: 0.7978298664093018
Epoch 240, training loss: 0.5667580366134644 = 0.4962292015552521 + 0.01 * 7.052881240844727
Epoch 240, val loss: 0.769440233707428
Epoch 250, training loss: 0.520699143409729 = 0.4501957595348358 + 0.01 * 7.0503411293029785
Epoch 250, val loss: 0.7445589303970337
Epoch 260, training loss: 0.47579655051231384 = 0.4053160846233368 + 0.01 * 7.0480475425720215
Epoch 260, val loss: 0.7234365940093994
Epoch 270, training loss: 0.43221619725227356 = 0.3617556393146515 + 0.01 * 7.046054840087891
Epoch 270, val loss: 0.7063261270523071
Epoch 280, training loss: 0.3906189799308777 = 0.32017719745635986 + 0.01 * 7.044177532196045
Epoch 280, val loss: 0.6935428380966187
Epoch 290, training loss: 0.3519333302974701 = 0.2814978063106537 + 0.01 * 7.043551445007324
Epoch 290, val loss: 0.6853117942810059
Epoch 300, training loss: 0.3168376386165619 = 0.24642303586006165 + 0.01 * 7.041459560394287
Epoch 300, val loss: 0.6816920638084412
Epoch 310, training loss: 0.28561341762542725 = 0.21521815657615662 + 0.01 * 7.03952693939209
Epoch 310, val loss: 0.6823194622993469
Epoch 320, training loss: 0.2582101821899414 = 0.1878337264060974 + 0.01 * 7.0376458168029785
Epoch 320, val loss: 0.6867509484291077
Epoch 330, training loss: 0.23440295457839966 = 0.16403669118881226 + 0.01 * 7.036627292633057
Epoch 330, val loss: 0.6944470405578613
Epoch 340, training loss: 0.2138492912054062 = 0.14349444210529327 + 0.01 * 7.035484790802002
Epoch 340, val loss: 0.7048162221908569
Epoch 350, training loss: 0.1962178349494934 = 0.12584513425827026 + 0.01 * 7.037271022796631
Epoch 350, val loss: 0.7172520160675049
Epoch 360, training loss: 0.18101970851421356 = 0.11068488657474518 + 0.01 * 7.033482074737549
Epoch 360, val loss: 0.7311712503433228
Epoch 370, training loss: 0.1679348647594452 = 0.09762760251760483 + 0.01 * 7.030726909637451
Epoch 370, val loss: 0.7462061643600464
Epoch 380, training loss: 0.15669956803321838 = 0.08635982125997543 + 0.01 * 7.0339741706848145
Epoch 380, val loss: 0.762069582939148
Epoch 390, training loss: 0.14690038561820984 = 0.07662604004144669 + 0.01 * 7.027435779571533
Epoch 390, val loss: 0.7784935832023621
Epoch 400, training loss: 0.13844731450080872 = 0.06820796430110931 + 0.01 * 7.023934841156006
Epoch 400, val loss: 0.7952444553375244
Epoch 410, training loss: 0.13114215433597565 = 0.060919102281332016 + 0.01 * 7.022305965423584
Epoch 410, val loss: 0.8121939301490784
Epoch 420, training loss: 0.12477566301822662 = 0.05459662154316902 + 0.01 * 7.017904758453369
Epoch 420, val loss: 0.8291385769844055
Epoch 430, training loss: 0.1192573755979538 = 0.049100663512945175 + 0.01 * 7.015671253204346
Epoch 430, val loss: 0.845990002155304
Epoch 440, training loss: 0.11447401344776154 = 0.04431253671646118 + 0.01 * 7.016147613525391
Epoch 440, val loss: 0.86265629529953
Epoch 450, training loss: 0.11023744195699692 = 0.04013139754533768 + 0.01 * 7.0106048583984375
Epoch 450, val loss: 0.8789756298065186
Epoch 460, training loss: 0.10649493336677551 = 0.03647039830684662 + 0.01 * 7.002453327178955
Epoch 460, val loss: 0.8949545621871948
Epoch 470, training loss: 0.10321331024169922 = 0.03325356915593147 + 0.01 * 6.995974063873291
Epoch 470, val loss: 0.910615861415863
Epoch 480, training loss: 0.10064885020256042 = 0.030414912849664688 + 0.01 * 7.023394584655762
Epoch 480, val loss: 0.925961434841156
Epoch 490, training loss: 0.0977381020784378 = 0.027906548231840134 + 0.01 * 6.983155727386475
Epoch 490, val loss: 0.940854012966156
Epoch 500, training loss: 0.09548972547054291 = 0.02567952685058117 + 0.01 * 6.981019496917725
Epoch 500, val loss: 0.955420732498169
Epoch 510, training loss: 0.09340667724609375 = 0.02369343861937523 + 0.01 * 6.971323490142822
Epoch 510, val loss: 0.9696797728538513
Epoch 520, training loss: 0.09164286404848099 = 0.02192019671201706 + 0.01 * 6.972267150878906
Epoch 520, val loss: 0.983555793762207
Epoch 530, training loss: 0.08993307501077652 = 0.020334886386990547 + 0.01 * 6.959819316864014
Epoch 530, val loss: 0.9969685673713684
Epoch 540, training loss: 0.0884605422616005 = 0.01891184411942959 + 0.01 * 6.954869747161865
Epoch 540, val loss: 1.0100784301757812
Epoch 550, training loss: 0.08700653910636902 = 0.01763235218822956 + 0.01 * 6.9374189376831055
Epoch 550, val loss: 1.0227930545806885
Epoch 560, training loss: 0.08574635535478592 = 0.016479169949889183 + 0.01 * 6.926718711853027
Epoch 560, val loss: 1.0351910591125488
Epoch 570, training loss: 0.08470817655324936 = 0.015438808128237724 + 0.01 * 6.926936626434326
Epoch 570, val loss: 1.0470572710037231
Epoch 580, training loss: 0.08373323827981949 = 0.01449529267847538 + 0.01 * 6.923794746398926
Epoch 580, val loss: 1.0586830377578735
Epoch 590, training loss: 0.08270815014839172 = 0.013638189062476158 + 0.01 * 6.90699577331543
Epoch 590, val loss: 1.0699399709701538
Epoch 600, training loss: 0.08185935765504837 = 0.0128576485440135 + 0.01 * 6.900171279907227
Epoch 600, val loss: 1.080823540687561
Epoch 610, training loss: 0.08109133690595627 = 0.012143753468990326 + 0.01 * 6.894758224487305
Epoch 610, val loss: 1.0914462804794312
Epoch 620, training loss: 0.08068256825208664 = 0.01149089727550745 + 0.01 * 6.919167518615723
Epoch 620, val loss: 1.1017204523086548
Epoch 630, training loss: 0.07984034717082977 = 0.010892987251281738 + 0.01 * 6.894735813140869
Epoch 630, val loss: 1.1116522550582886
Epoch 640, training loss: 0.07912429422140121 = 0.010342461988329887 + 0.01 * 6.878183364868164
Epoch 640, val loss: 1.1213217973709106
Epoch 650, training loss: 0.07867240160703659 = 0.009834212251007557 + 0.01 * 6.883819103240967
Epoch 650, val loss: 1.130782127380371
Epoch 660, training loss: 0.0781610980629921 = 0.009365065954625607 + 0.01 * 6.879603385925293
Epoch 660, val loss: 1.140009880065918
Epoch 670, training loss: 0.07768122851848602 = 0.008930938318371773 + 0.01 * 6.875029563903809
Epoch 670, val loss: 1.1489603519439697
Epoch 680, training loss: 0.07720690220594406 = 0.0085287569090724 + 0.01 * 6.867815017700195
Epoch 680, val loss: 1.157522201538086
Epoch 690, training loss: 0.07666006684303284 = 0.008155444636940956 + 0.01 * 6.850462436676025
Epoch 690, val loss: 1.1660202741622925
Epoch 700, training loss: 0.07623862475156784 = 0.007808092050254345 + 0.01 * 6.843053817749023
Epoch 700, val loss: 1.1742148399353027
Epoch 710, training loss: 0.07597082108259201 = 0.007484035100787878 + 0.01 * 6.848679065704346
Epoch 710, val loss: 1.182206153869629
Epoch 720, training loss: 0.075483538210392 = 0.007181642111390829 + 0.01 * 6.830189228057861
Epoch 720, val loss: 1.1900252103805542
Epoch 730, training loss: 0.07524596154689789 = 0.006898876745253801 + 0.01 * 6.834708213806152
Epoch 730, val loss: 1.197660207748413
Epoch 740, training loss: 0.07493298500776291 = 0.006634717807173729 + 0.01 * 6.829826831817627
Epoch 740, val loss: 1.2050918340682983
Epoch 750, training loss: 0.07455356419086456 = 0.006387344561517239 + 0.01 * 6.816621780395508
Epoch 750, val loss: 1.2121973037719727
Epoch 760, training loss: 0.07455356419086456 = 0.00615489948540926 + 0.01 * 6.8398661613464355
Epoch 760, val loss: 1.2192991971969604
Epoch 770, training loss: 0.07414934039115906 = 0.005936914589256048 + 0.01 * 6.821242809295654
Epoch 770, val loss: 1.226112723350525
Epoch 780, training loss: 0.07397428154945374 = 0.00573163665831089 + 0.01 * 6.824265003204346
Epoch 780, val loss: 1.2327964305877686
Epoch 790, training loss: 0.07362432777881622 = 0.005538244266062975 + 0.01 * 6.808608055114746
Epoch 790, val loss: 1.2393076419830322
Epoch 800, training loss: 0.0732506588101387 = 0.005355506669729948 + 0.01 * 6.789515495300293
Epoch 800, val loss: 1.2457162141799927
Epoch 810, training loss: 0.0731002688407898 = 0.005182969383895397 + 0.01 * 6.7917304039001465
Epoch 810, val loss: 1.2520405054092407
Epoch 820, training loss: 0.0730070024728775 = 0.005019946955144405 + 0.01 * 6.798705577850342
Epoch 820, val loss: 1.2580389976501465
Epoch 830, training loss: 0.07295557856559753 = 0.0048656584694981575 + 0.01 * 6.8089919090271
Epoch 830, val loss: 1.2639997005462646
Epoch 840, training loss: 0.07260052859783173 = 0.004719493445008993 + 0.01 * 6.788103103637695
Epoch 840, val loss: 1.2697961330413818
Epoch 850, training loss: 0.07243635505437851 = 0.0045809317380189896 + 0.01 * 6.7855424880981445
Epoch 850, val loss: 1.2753465175628662
Epoch 860, training loss: 0.07209309935569763 = 0.004449326079338789 + 0.01 * 6.764378070831299
Epoch 860, val loss: 1.2807979583740234
Epoch 870, training loss: 0.07210326194763184 = 0.0043243952095508575 + 0.01 * 6.777887344360352
Epoch 870, val loss: 1.2861857414245605
Epoch 880, training loss: 0.07184217870235443 = 0.004205454606562853 + 0.01 * 6.763672351837158
Epoch 880, val loss: 1.2914057970046997
Epoch 890, training loss: 0.07173304259777069 = 0.004092443268746138 + 0.01 * 6.7640604972839355
Epoch 890, val loss: 1.296538233757019
Epoch 900, training loss: 0.07177500426769257 = 0.003984719980508089 + 0.01 * 6.779028415679932
Epoch 900, val loss: 1.3015069961547852
Epoch 910, training loss: 0.07139220833778381 = 0.0038820903282612562 + 0.01 * 6.751012325286865
Epoch 910, val loss: 1.306335687637329
Epoch 920, training loss: 0.0714588537812233 = 0.003784000873565674 + 0.01 * 6.767485618591309
Epoch 920, val loss: 1.311303734779358
Epoch 930, training loss: 0.07125604152679443 = 0.003690241603180766 + 0.01 * 6.756579875946045
Epoch 930, val loss: 1.315836787223816
Epoch 940, training loss: 0.07106766104698181 = 0.0036009366158396006 + 0.01 * 6.7466721534729
Epoch 940, val loss: 1.3204772472381592
Epoch 950, training loss: 0.0709996297955513 = 0.0035152507480233908 + 0.01 * 6.748438358306885
Epoch 950, val loss: 1.3250221014022827
Epoch 960, training loss: 0.07114943116903305 = 0.0034333604853600264 + 0.01 * 6.771607398986816
Epoch 960, val loss: 1.3293732404708862
Epoch 970, training loss: 0.07081965357065201 = 0.0033550274092704058 + 0.01 * 6.746462821960449
Epoch 970, val loss: 1.3337126970291138
Epoch 980, training loss: 0.0706164762377739 = 0.003279963508248329 + 0.01 * 6.733651638031006
Epoch 980, val loss: 1.3379179239273071
Epoch 990, training loss: 0.07083018124103546 = 0.003207903355360031 + 0.01 * 6.762228012084961
Epoch 990, val loss: 1.3421196937561035
Epoch 1000, training loss: 0.07052961736917496 = 0.0031387752387672663 + 0.01 * 6.739084720611572
Epoch 1000, val loss: 1.3461633920669556
Epoch 1010, training loss: 0.07054757326841354 = 0.003072519088163972 + 0.01 * 6.747505187988281
Epoch 1010, val loss: 1.3501291275024414
Epoch 1020, training loss: 0.07025925070047379 = 0.003008980071172118 + 0.01 * 6.725027561187744
Epoch 1020, val loss: 1.3539258241653442
Epoch 1030, training loss: 0.07030367851257324 = 0.0029478559736162424 + 0.01 * 6.73558235168457
Epoch 1030, val loss: 1.3577519655227661
Epoch 1040, training loss: 0.0702192634344101 = 0.002888903021812439 + 0.01 * 6.733036041259766
Epoch 1040, val loss: 1.3615056276321411
Epoch 1050, training loss: 0.06997242569923401 = 0.0028325810562819242 + 0.01 * 6.71398401260376
Epoch 1050, val loss: 1.365338921546936
Epoch 1060, training loss: 0.06996003538370132 = 0.002778005553409457 + 0.01 * 6.718202590942383
Epoch 1060, val loss: 1.3687397241592407
Epoch 1070, training loss: 0.06985502690076828 = 0.0027256908360868692 + 0.01 * 6.712933540344238
Epoch 1070, val loss: 1.3722378015518188
Epoch 1080, training loss: 0.06971121579408646 = 0.002675372641533613 + 0.01 * 6.703585147857666
Epoch 1080, val loss: 1.3758447170257568
Epoch 1090, training loss: 0.06974861025810242 = 0.002626552013680339 + 0.01 * 6.71220588684082
Epoch 1090, val loss: 1.3790677785873413
Epoch 1100, training loss: 0.06956391781568527 = 0.002579750958830118 + 0.01 * 6.6984171867370605
Epoch 1100, val loss: 1.3825011253356934
Epoch 1110, training loss: 0.06961707770824432 = 0.0025343862362205982 + 0.01 * 6.708268642425537
Epoch 1110, val loss: 1.3857128620147705
Epoch 1120, training loss: 0.06952908635139465 = 0.0024907896295189857 + 0.01 * 6.703830242156982
Epoch 1120, val loss: 1.3889257907867432
Epoch 1130, training loss: 0.0694737508893013 = 0.0024485704489052296 + 0.01 * 6.702517509460449
Epoch 1130, val loss: 1.3919943571090698
Epoch 1140, training loss: 0.06964433938264847 = 0.002407867694273591 + 0.01 * 6.723647117614746
Epoch 1140, val loss: 1.3951175212860107
Epoch 1150, training loss: 0.06934865564107895 = 0.002368430607020855 + 0.01 * 6.698022842407227
Epoch 1150, val loss: 1.3981096744537354
Epoch 1160, training loss: 0.06926896423101425 = 0.002330508315935731 + 0.01 * 6.693845272064209
Epoch 1160, val loss: 1.401111364364624
Epoch 1170, training loss: 0.06916017830371857 = 0.002293708734214306 + 0.01 * 6.686647415161133
Epoch 1170, val loss: 1.403897762298584
Epoch 1180, training loss: 0.06923428177833557 = 0.002258233493193984 + 0.01 * 6.697604656219482
Epoch 1180, val loss: 1.4068046808242798
Epoch 1190, training loss: 0.06935412436723709 = 0.002223723568022251 + 0.01 * 6.713040351867676
Epoch 1190, val loss: 1.409546971321106
Epoch 1200, training loss: 0.06898384541273117 = 0.0021904308814555407 + 0.01 * 6.6793413162231445
Epoch 1200, val loss: 1.4122282266616821
Epoch 1210, training loss: 0.06889353692531586 = 0.0021582019980996847 + 0.01 * 6.673533916473389
Epoch 1210, val loss: 1.4150272607803345
Epoch 1220, training loss: 0.06892033666372299 = 0.0021268127020448446 + 0.01 * 6.679352760314941
Epoch 1220, val loss: 1.4175469875335693
Epoch 1230, training loss: 0.06909596174955368 = 0.0020965232979506254 + 0.01 * 6.699943542480469
Epoch 1230, val loss: 1.4202196598052979
Epoch 1240, training loss: 0.06904777139425278 = 0.0020670678932219744 + 0.01 * 6.698070526123047
Epoch 1240, val loss: 1.4227179288864136
Epoch 1250, training loss: 0.0686836689710617 = 0.0020386523101478815 + 0.01 * 6.664502143859863
Epoch 1250, val loss: 1.4251351356506348
Epoch 1260, training loss: 0.0687512755393982 = 0.002011107048019767 + 0.01 * 6.674016952514648
Epoch 1260, val loss: 1.4275740385055542
Epoch 1270, training loss: 0.06883569061756134 = 0.0019842172041535378 + 0.01 * 6.685147285461426
Epoch 1270, val loss: 1.4300330877304077
Epoch 1280, training loss: 0.06896509230136871 = 0.001958281733095646 + 0.01 * 6.700681209564209
Epoch 1280, val loss: 1.432235836982727
Epoch 1290, training loss: 0.06865495443344116 = 0.001933091669343412 + 0.01 * 6.672186374664307
Epoch 1290, val loss: 1.4346882104873657
Epoch 1300, training loss: 0.06845329701900482 = 0.0019085274543613195 + 0.01 * 6.654477119445801
Epoch 1300, val loss: 1.4368010759353638
Epoch 1310, training loss: 0.06855463981628418 = 0.0018847361207008362 + 0.01 * 6.666990756988525
Epoch 1310, val loss: 1.4391090869903564
Epoch 1320, training loss: 0.06859087944030762 = 0.0018615399021655321 + 0.01 * 6.672933578491211
Epoch 1320, val loss: 1.4411507844924927
Epoch 1330, training loss: 0.06837835907936096 = 0.0018390368204563856 + 0.01 * 6.653932571411133
Epoch 1330, val loss: 1.4435075521469116
Epoch 1340, training loss: 0.06838636845350266 = 0.0018171390984207392 + 0.01 * 6.656923294067383
Epoch 1340, val loss: 1.445557951927185
Epoch 1350, training loss: 0.06825675070285797 = 0.0017957650125026703 + 0.01 * 6.646098613739014
Epoch 1350, val loss: 1.4476033449172974
Epoch 1360, training loss: 0.06847542524337769 = 0.0017750341212376952 + 0.01 * 6.670039176940918
Epoch 1360, val loss: 1.449826955795288
Epoch 1370, training loss: 0.06832136958837509 = 0.0017547904280945659 + 0.01 * 6.656658172607422
Epoch 1370, val loss: 1.4516749382019043
Epoch 1380, training loss: 0.06807457655668259 = 0.0017351830611005425 + 0.01 * 6.633939743041992
Epoch 1380, val loss: 1.4536402225494385
Epoch 1390, training loss: 0.06819958984851837 = 0.0017160288989543915 + 0.01 * 6.6483564376831055
Epoch 1390, val loss: 1.4555943012237549
Epoch 1400, training loss: 0.06809575855731964 = 0.001697358675301075 + 0.01 * 6.6398396492004395
Epoch 1400, val loss: 1.4574731588363647
Epoch 1410, training loss: 0.06812810152769089 = 0.0016791990492492914 + 0.01 * 6.644890785217285
Epoch 1410, val loss: 1.4594635963439941
Epoch 1420, training loss: 0.06818650662899017 = 0.0016614951891824603 + 0.01 * 6.652501583099365
Epoch 1420, val loss: 1.4612927436828613
Epoch 1430, training loss: 0.06793764233589172 = 0.001644293311983347 + 0.01 * 6.629335403442383
Epoch 1430, val loss: 1.463141679763794
Epoch 1440, training loss: 0.06798967719078064 = 0.0016274931840598583 + 0.01 * 6.636219024658203
Epoch 1440, val loss: 1.4648295640945435
Epoch 1450, training loss: 0.06791180372238159 = 0.0016110407887026668 + 0.01 * 6.630076885223389
Epoch 1450, val loss: 1.466600775718689
Epoch 1460, training loss: 0.06789516657590866 = 0.0015950517263263464 + 0.01 * 6.630011558532715
Epoch 1460, val loss: 1.4684590101242065
Epoch 1470, training loss: 0.06808951497077942 = 0.0015793894417583942 + 0.01 * 6.651012420654297
Epoch 1470, val loss: 1.4701303243637085
Epoch 1480, training loss: 0.06766856461763382 = 0.0015641512582078576 + 0.01 * 6.6104416847229
Epoch 1480, val loss: 1.4719245433807373
Epoch 1490, training loss: 0.06797698885202408 = 0.001549266162328422 + 0.01 * 6.642772197723389
Epoch 1490, val loss: 1.4734195470809937
Epoch 1500, training loss: 0.06781431287527084 = 0.0015347084263339639 + 0.01 * 6.627960681915283
Epoch 1500, val loss: 1.4750791788101196
Epoch 1510, training loss: 0.06782066076993942 = 0.0015205983072519302 + 0.01 * 6.630006790161133
Epoch 1510, val loss: 1.4767721891403198
Epoch 1520, training loss: 0.06754186749458313 = 0.0015066866762936115 + 0.01 * 6.603518486022949
Epoch 1520, val loss: 1.4782013893127441
Epoch 1530, training loss: 0.06771127134561539 = 0.0014932149788364768 + 0.01 * 6.621805667877197
Epoch 1530, val loss: 1.479813814163208
Epoch 1540, training loss: 0.06750170886516571 = 0.0014798424672335386 + 0.01 * 6.602187156677246
Epoch 1540, val loss: 1.4813061952590942
Epoch 1550, training loss: 0.06766468286514282 = 0.0014669287484139204 + 0.01 * 6.619775295257568
Epoch 1550, val loss: 1.4828866720199585
Epoch 1560, training loss: 0.06753120571374893 = 0.0014541730051860213 + 0.01 * 6.60770320892334
Epoch 1560, val loss: 1.4843389987945557
Epoch 1570, training loss: 0.06742505729198456 = 0.0014418542850762606 + 0.01 * 6.598320484161377
Epoch 1570, val loss: 1.485924482345581
Epoch 1580, training loss: 0.06765478104352951 = 0.0014296642038971186 + 0.01 * 6.622512340545654
Epoch 1580, val loss: 1.4873058795928955
Epoch 1590, training loss: 0.06723843514919281 = 0.0014177549164742231 + 0.01 * 6.582067966461182
Epoch 1590, val loss: 1.4887032508850098
Epoch 1600, training loss: 0.06759621202945709 = 0.0014060967369005084 + 0.01 * 6.619011402130127
Epoch 1600, val loss: 1.4900189638137817
Epoch 1610, training loss: 0.0673917829990387 = 0.001394696650095284 + 0.01 * 6.599708557128906
Epoch 1610, val loss: 1.491502046585083
Epoch 1620, training loss: 0.06745858490467072 = 0.0013835470890626311 + 0.01 * 6.607503414154053
Epoch 1620, val loss: 1.49289870262146
Epoch 1630, training loss: 0.06773616373538971 = 0.001372597529552877 + 0.01 * 6.636356353759766
Epoch 1630, val loss: 1.494202733039856
Epoch 1640, training loss: 0.06733488291501999 = 0.0013619089731946588 + 0.01 * 6.5972981452941895
Epoch 1640, val loss: 1.4956409931182861
Epoch 1650, training loss: 0.06752721220254898 = 0.0013514190213754773 + 0.01 * 6.617579936981201
Epoch 1650, val loss: 1.496854305267334
Epoch 1660, training loss: 0.06717225909233093 = 0.001341158989816904 + 0.01 * 6.583110332489014
Epoch 1660, val loss: 1.4981530904769897
Epoch 1670, training loss: 0.06709285825490952 = 0.0013311131624504924 + 0.01 * 6.576174736022949
Epoch 1670, val loss: 1.4993810653686523
Epoch 1680, training loss: 0.06725209951400757 = 0.0013212261255830526 + 0.01 * 6.593087673187256
Epoch 1680, val loss: 1.5006307363510132
Epoch 1690, training loss: 0.0671062171459198 = 0.0013115719193592668 + 0.01 * 6.579464912414551
Epoch 1690, val loss: 1.5020164251327515
Epoch 1700, training loss: 0.06726593524217606 = 0.001302081043832004 + 0.01 * 6.596385955810547
Epoch 1700, val loss: 1.5031403303146362
Epoch 1710, training loss: 0.06710866093635559 = 0.00129276467487216 + 0.01 * 6.581589698791504
Epoch 1710, val loss: 1.5042213201522827
Epoch 1720, training loss: 0.06711359322071075 = 0.0012836521491408348 + 0.01 * 6.58299446105957
Epoch 1720, val loss: 1.5056931972503662
Epoch 1730, training loss: 0.0675342008471489 = 0.0012746556894853711 + 0.01 * 6.625954627990723
Epoch 1730, val loss: 1.5068308115005493
Epoch 1740, training loss: 0.06704523414373398 = 0.0012658738996833563 + 0.01 * 6.577935695648193
Epoch 1740, val loss: 1.507886528968811
Epoch 1750, training loss: 0.06690943241119385 = 0.0012572882696986198 + 0.01 * 6.56521463394165
Epoch 1750, val loss: 1.5091056823730469
Epoch 1760, training loss: 0.06730781495571136 = 0.0012487733038142323 + 0.01 * 6.6059041023254395
Epoch 1760, val loss: 1.5103100538253784
Epoch 1770, training loss: 0.06699766218662262 = 0.0012404622975736856 + 0.01 * 6.575720310211182
Epoch 1770, val loss: 1.511304259300232
Epoch 1780, training loss: 0.06690338999032974 = 0.0012322869151830673 + 0.01 * 6.567110538482666
Epoch 1780, val loss: 1.5124764442443848
Epoch 1790, training loss: 0.06690312176942825 = 0.0012242448283359408 + 0.01 * 6.567887783050537
Epoch 1790, val loss: 1.513585090637207
Epoch 1800, training loss: 0.06743474304676056 = 0.0012163517531007528 + 0.01 * 6.62183952331543
Epoch 1800, val loss: 1.5146182775497437
Epoch 1810, training loss: 0.06689147651195526 = 0.0012086223578080535 + 0.01 * 6.5682854652404785
Epoch 1810, val loss: 1.5157506465911865
Epoch 1820, training loss: 0.06685175746679306 = 0.0012010270729660988 + 0.01 * 6.565073490142822
Epoch 1820, val loss: 1.5167160034179688
Epoch 1830, training loss: 0.06685912609100342 = 0.0011935096699744463 + 0.01 * 6.566561698913574
Epoch 1830, val loss: 1.5177454948425293
Epoch 1840, training loss: 0.06667622923851013 = 0.0011861976236104965 + 0.01 * 6.5490031242370605
Epoch 1840, val loss: 1.5188698768615723
Epoch 1850, training loss: 0.06695380806922913 = 0.0011789356358349323 + 0.01 * 6.577487468719482
Epoch 1850, val loss: 1.5196962356567383
Epoch 1860, training loss: 0.06693172454833984 = 0.001171813695691526 + 0.01 * 6.575991630554199
Epoch 1860, val loss: 1.5208284854888916
Epoch 1870, training loss: 0.06683176010847092 = 0.0011648061918094754 + 0.01 * 6.566695213317871
Epoch 1870, val loss: 1.5219403505325317
Epoch 1880, training loss: 0.06665448099374771 = 0.0011579025303944945 + 0.01 * 6.549658298492432
Epoch 1880, val loss: 1.522727131843567
Epoch 1890, training loss: 0.0666472464799881 = 0.0011511826887726784 + 0.01 * 6.549606800079346
Epoch 1890, val loss: 1.5238178968429565
Epoch 1900, training loss: 0.06697442382574081 = 0.0011444460833445191 + 0.01 * 6.582997798919678
Epoch 1900, val loss: 1.5246164798736572
Epoch 1910, training loss: 0.06674821674823761 = 0.0011379066854715347 + 0.01 * 6.561031341552734
Epoch 1910, val loss: 1.5256798267364502
Epoch 1920, training loss: 0.06689706444740295 = 0.0011314222356304526 + 0.01 * 6.576564311981201
Epoch 1920, val loss: 1.526474118232727
Epoch 1930, training loss: 0.06668024510145187 = 0.001125069335103035 + 0.01 * 6.55551815032959
Epoch 1930, val loss: 1.5274702310562134
Epoch 1940, training loss: 0.06647216528654099 = 0.0011188104981556535 + 0.01 * 6.535335540771484
Epoch 1940, val loss: 1.5283617973327637
Epoch 1950, training loss: 0.06645382195711136 = 0.0011126056779175997 + 0.01 * 6.534121990203857
Epoch 1950, val loss: 1.529203176498413
Epoch 1960, training loss: 0.06661475449800491 = 0.0011065311264246702 + 0.01 * 6.550822734832764
Epoch 1960, val loss: 1.5301543474197388
Epoch 1970, training loss: 0.0667809545993805 = 0.0011005208361893892 + 0.01 * 6.568044185638428
Epoch 1970, val loss: 1.5310920476913452
Epoch 1980, training loss: 0.06635966897010803 = 0.0010946154361590743 + 0.01 * 6.526504993438721
Epoch 1980, val loss: 1.5319101810455322
Epoch 1990, training loss: 0.06664110720157623 = 0.0010888274991884828 + 0.01 * 6.555228233337402
Epoch 1990, val loss: 1.5326151847839355
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 2.026801347732544 = 1.9408330917358398 + 0.01 * 8.596826553344727
Epoch 0, val loss: 1.9409174919128418
Epoch 10, training loss: 2.0164172649383545 = 1.9304497241973877 + 0.01 * 8.596755981445312
Epoch 10, val loss: 1.9308698177337646
Epoch 20, training loss: 2.0036392211914062 = 1.9176743030548096 + 0.01 * 8.596494674682617
Epoch 20, val loss: 1.9181599617004395
Epoch 30, training loss: 1.985944390296936 = 1.8999892473220825 + 0.01 * 8.595515251159668
Epoch 30, val loss: 1.9003639221191406
Epoch 40, training loss: 1.9602282047271729 = 1.874336838722229 + 0.01 * 8.589132308959961
Epoch 40, val loss: 1.8749055862426758
Epoch 50, training loss: 1.9248896837234497 = 1.8394049406051636 + 0.01 * 8.548476219177246
Epoch 50, val loss: 1.8418269157409668
Epoch 60, training loss: 1.8844590187072754 = 1.8007272481918335 + 0.01 * 8.37317943572998
Epoch 60, val loss: 1.8081600666046143
Epoch 70, training loss: 1.846663475036621 = 1.765661358833313 + 0.01 * 8.10021686553955
Epoch 70, val loss: 1.77792227268219
Epoch 80, training loss: 1.8007605075836182 = 1.7214341163635254 + 0.01 * 7.9326372146606445
Epoch 80, val loss: 1.7356871366500854
Epoch 90, training loss: 1.73715078830719 = 1.6595003604888916 + 0.01 * 7.765038967132568
Epoch 90, val loss: 1.6793184280395508
Epoch 100, training loss: 1.6540006399154663 = 1.5777971744537354 + 0.01 * 7.620346546173096
Epoch 100, val loss: 1.6093207597732544
Epoch 110, training loss: 1.55669367313385 = 1.4818819761276245 + 0.01 * 7.481169700622559
Epoch 110, val loss: 1.5273411273956299
Epoch 120, training loss: 1.4566905498504639 = 1.3824551105499268 + 0.01 * 7.42354679107666
Epoch 120, val loss: 1.4439759254455566
Epoch 130, training loss: 1.3596657514572144 = 1.2860994338989258 + 0.01 * 7.356632709503174
Epoch 130, val loss: 1.364528775215149
Epoch 140, training loss: 1.266937494277954 = 1.1939809322357178 + 0.01 * 7.295660972595215
Epoch 140, val loss: 1.2904061079025269
Epoch 150, training loss: 1.1784032583236694 = 1.105822205543518 + 0.01 * 7.258103847503662
Epoch 150, val loss: 1.2197773456573486
Epoch 160, training loss: 1.094111680984497 = 1.0218485593795776 + 0.01 * 7.226316452026367
Epoch 160, val loss: 1.152444839477539
Epoch 170, training loss: 1.0146992206573486 = 0.942767858505249 + 0.01 * 7.193141460418701
Epoch 170, val loss: 1.0887774229049683
Epoch 180, training loss: 0.9404500722885132 = 0.8688371181488037 + 0.01 * 7.161296367645264
Epoch 180, val loss: 1.0290263891220093
Epoch 190, training loss: 0.8709836006164551 = 0.7996492981910706 + 0.01 * 7.133429527282715
Epoch 190, val loss: 0.973464846611023
Epoch 200, training loss: 0.8061025738716125 = 0.7349840998649597 + 0.01 * 7.111847400665283
Epoch 200, val loss: 0.9225346446037292
Epoch 210, training loss: 0.7462109327316284 = 0.675192654132843 + 0.01 * 7.1018290519714355
Epoch 210, val loss: 0.8772340416908264
Epoch 220, training loss: 0.6905236840248108 = 0.6195883750915527 + 0.01 * 7.093529224395752
Epoch 220, val loss: 0.8381305932998657
Epoch 230, training loss: 0.637153148651123 = 0.5662989020347595 + 0.01 * 7.085422515869141
Epoch 230, val loss: 0.8044307231903076
Epoch 240, training loss: 0.5843303799629211 = 0.513522744178772 + 0.01 * 7.080763816833496
Epoch 240, val loss: 0.7750095725059509
Epoch 250, training loss: 0.5316439867019653 = 0.4608679413795471 + 0.01 * 7.077607154846191
Epoch 250, val loss: 0.74921715259552
Epoch 260, training loss: 0.48009419441223145 = 0.40934130549430847 + 0.01 * 7.075290203094482
Epoch 260, val loss: 0.7275204658508301
Epoch 270, training loss: 0.43145644664764404 = 0.3606814742088318 + 0.01 * 7.077498912811279
Epoch 270, val loss: 0.7110563516616821
Epoch 280, training loss: 0.3869422376155853 = 0.31619152426719666 + 0.01 * 7.075071334838867
Epoch 280, val loss: 0.7002411484718323
Epoch 290, training loss: 0.3470679521560669 = 0.27633795142173767 + 0.01 * 7.073001384735107
Epoch 290, val loss: 0.6945584416389465
Epoch 300, training loss: 0.3117122948169708 = 0.24099130928516388 + 0.01 * 7.072098731994629
Epoch 300, val loss: 0.6931323409080505
Epoch 310, training loss: 0.2805325984954834 = 0.20981520414352417 + 0.01 * 7.071739196777344
Epoch 310, val loss: 0.6952522397041321
Epoch 320, training loss: 0.2531925141811371 = 0.18248112499713898 + 0.01 * 7.071138858795166
Epoch 320, val loss: 0.7001885771751404
Epoch 330, training loss: 0.22939252853393555 = 0.15866300463676453 + 0.01 * 7.0729522705078125
Epoch 330, val loss: 0.707429051399231
Epoch 340, training loss: 0.20873218774795532 = 0.13803032040596008 + 0.01 * 7.070187091827393
Epoch 340, val loss: 0.7163801193237305
Epoch 350, training loss: 0.1909130960702896 = 0.12022137641906738 + 0.01 * 7.069171905517578
Epoch 350, val loss: 0.7266661524772644
Epoch 360, training loss: 0.17556816339492798 = 0.10487475991249084 + 0.01 * 7.069340705871582
Epoch 360, val loss: 0.7379632592201233
Epoch 370, training loss: 0.16232672333717346 = 0.09165671467781067 + 0.01 * 7.067001819610596
Epoch 370, val loss: 0.7500022649765015
Epoch 380, training loss: 0.1509954333305359 = 0.08027574419975281 + 0.01 * 7.071969032287598
Epoch 380, val loss: 0.762522280216217
Epoch 390, training loss: 0.14113590121269226 = 0.07048428058624268 + 0.01 * 7.065162181854248
Epoch 390, val loss: 0.7753493189811707
Epoch 400, training loss: 0.13268917798995972 = 0.062063686549663544 + 0.01 * 7.062549114227295
Epoch 400, val loss: 0.788353443145752
Epoch 410, training loss: 0.1254233568906784 = 0.05482255667448044 + 0.01 * 7.060080051422119
Epoch 410, val loss: 0.8014253973960876
Epoch 420, training loss: 0.11917237192392349 = 0.04859531670808792 + 0.01 * 7.057705879211426
Epoch 420, val loss: 0.814504861831665
Epoch 430, training loss: 0.1137867420911789 = 0.04323768615722656 + 0.01 * 7.054905891418457
Epoch 430, val loss: 0.8274929523468018
Epoch 440, training loss: 0.10919961333274841 = 0.03862319886684418 + 0.01 * 7.057641506195068
Epoch 440, val loss: 0.8403283357620239
Epoch 450, training loss: 0.10514691472053528 = 0.03464386984705925 + 0.01 * 7.050304889678955
Epoch 450, val loss: 0.8529025316238403
Epoch 460, training loss: 0.10166069865226746 = 0.031203709542751312 + 0.01 * 7.045699119567871
Epoch 460, val loss: 0.8651424050331116
Epoch 470, training loss: 0.09863295406103134 = 0.028221771121025085 + 0.01 * 7.041118621826172
Epoch 470, val loss: 0.8770514726638794
Epoch 480, training loss: 0.09606046974658966 = 0.02562924660742283 + 0.01 * 7.0431227684021
Epoch 480, val loss: 0.8886556029319763
Epoch 490, training loss: 0.09370563924312592 = 0.02336762845516205 + 0.01 * 7.033801078796387
Epoch 490, val loss: 0.8998964428901672
Epoch 500, training loss: 0.09173677861690521 = 0.021387513726949692 + 0.01 * 7.0349273681640625
Epoch 500, val loss: 0.9107384085655212
Epoch 510, training loss: 0.08989664912223816 = 0.019648006185889244 + 0.01 * 7.024864196777344
Epoch 510, val loss: 0.9212302565574646
Epoch 520, training loss: 0.08838243782520294 = 0.018113519996404648 + 0.01 * 7.02689266204834
Epoch 520, val loss: 0.9314016699790955
Epoch 530, training loss: 0.08688262104988098 = 0.016754627227783203 + 0.01 * 7.0127997398376465
Epoch 530, val loss: 0.9411812424659729
Epoch 540, training loss: 0.0856218934059143 = 0.015546520240604877 + 0.01 * 7.007536888122559
Epoch 540, val loss: 0.9506354331970215
Epoch 550, training loss: 0.08451242744922638 = 0.014468658715486526 + 0.01 * 7.004376411437988
Epoch 550, val loss: 0.9598070979118347
Epoch 560, training loss: 0.08347838371992111 = 0.013503484427928925 + 0.01 * 6.997489929199219
Epoch 560, val loss: 0.9686221480369568
Epoch 570, training loss: 0.08268754184246063 = 0.012635947205126286 + 0.01 * 7.005159854888916
Epoch 570, val loss: 0.9771559834480286
Epoch 580, training loss: 0.0816897302865982 = 0.011854337528347969 + 0.01 * 6.983539581298828
Epoch 580, val loss: 0.9854129552841187
Epoch 590, training loss: 0.08093401044607162 = 0.01114725787192583 + 0.01 * 6.97867488861084
Epoch 590, val loss: 0.9933808445930481
Epoch 600, training loss: 0.08020390570163727 = 0.01050566229969263 + 0.01 * 6.969824314117432
Epoch 600, val loss: 1.001090168952942
Epoch 610, training loss: 0.07974256575107574 = 0.009922351688146591 + 0.01 * 6.982022285461426
Epoch 610, val loss: 1.0084933042526245
Epoch 620, training loss: 0.07896322011947632 = 0.009391049854457378 + 0.01 * 6.957217216491699
Epoch 620, val loss: 1.0156673192977905
Epoch 630, training loss: 0.07838202267885208 = 0.008904795162379742 + 0.01 * 6.947722911834717
Epoch 630, val loss: 1.0225646495819092
Epoch 640, training loss: 0.07790213823318481 = 0.008458302356302738 + 0.01 * 6.94438362121582
Epoch 640, val loss: 1.0293004512786865
Epoch 650, training loss: 0.07734228670597076 = 0.00804773811250925 + 0.01 * 6.929455280303955
Epoch 650, val loss: 1.0357943773269653
Epoch 660, training loss: 0.07693686336278915 = 0.0076690856367349625 + 0.01 * 6.9267778396606445
Epoch 660, val loss: 1.0421019792556763
Epoch 670, training loss: 0.07759128510951996 = 0.007319007068872452 + 0.01 * 7.027228355407715
Epoch 670, val loss: 1.0481845140457153
Epoch 680, training loss: 0.07617612928152084 = 0.006996635813266039 + 0.01 * 6.917949199676514
Epoch 680, val loss: 1.05408775806427
Epoch 690, training loss: 0.07581695169210434 = 0.006698062177747488 + 0.01 * 6.911888599395752
Epoch 690, val loss: 1.0598552227020264
Epoch 700, training loss: 0.0754379853606224 = 0.006420313846319914 + 0.01 * 6.901767253875732
Epoch 700, val loss: 1.0653458833694458
Epoch 710, training loss: 0.07504449039697647 = 0.006160850170999765 + 0.01 * 6.888364315032959
Epoch 710, val loss: 1.0707083940505981
Epoch 720, training loss: 0.07492218166589737 = 0.005918241571635008 + 0.01 * 6.900393962860107
Epoch 720, val loss: 1.0760235786437988
Epoch 730, training loss: 0.07468966394662857 = 0.005692112259566784 + 0.01 * 6.899755954742432
Epoch 730, val loss: 1.081074833869934
Epoch 740, training loss: 0.0742606371641159 = 0.005480712745338678 + 0.01 * 6.877992153167725
Epoch 740, val loss: 1.0860008001327515
Epoch 750, training loss: 0.07399473339319229 = 0.005282806698232889 + 0.01 * 6.871192932128906
Epoch 750, val loss: 1.0907797813415527
Epoch 760, training loss: 0.07378043234348297 = 0.005096211563795805 + 0.01 * 6.868422031402588
Epoch 760, val loss: 1.0954978466033936
Epoch 770, training loss: 0.07369983196258545 = 0.004921391140669584 + 0.01 * 6.877843856811523
Epoch 770, val loss: 1.099980354309082
Epoch 780, training loss: 0.0732555240392685 = 0.004756745416671038 + 0.01 * 6.84987735748291
Epoch 780, val loss: 1.1043976545333862
Epoch 790, training loss: 0.0731976181268692 = 0.004601838532835245 + 0.01 * 6.8595781326293945
Epoch 790, val loss: 1.1086914539337158
Epoch 800, training loss: 0.07294657826423645 = 0.0044553931802511215 + 0.01 * 6.849118709564209
Epoch 800, val loss: 1.1127996444702148
Epoch 810, training loss: 0.0726422443985939 = 0.004317222163081169 + 0.01 * 6.8325018882751465
Epoch 810, val loss: 1.116873025894165
Epoch 820, training loss: 0.07248435914516449 = 0.0041864365339279175 + 0.01 * 6.829792499542236
Epoch 820, val loss: 1.1207952499389648
Epoch 830, training loss: 0.07224772870540619 = 0.0040626018308103085 + 0.01 * 6.818512916564941
Epoch 830, val loss: 1.1245977878570557
Epoch 840, training loss: 0.07231932878494263 = 0.003944865427911282 + 0.01 * 6.837446689605713
Epoch 840, val loss: 1.128361701965332
Epoch 850, training loss: 0.07220720499753952 = 0.0038333754055202007 + 0.01 * 6.837382793426514
Epoch 850, val loss: 1.1319721937179565
Epoch 860, training loss: 0.07197388261556625 = 0.0037277135998010635 + 0.01 * 6.824617385864258
Epoch 860, val loss: 1.1353871822357178
Epoch 870, training loss: 0.0716506764292717 = 0.0036271277349442244 + 0.01 * 6.80235481262207
Epoch 870, val loss: 1.138835072517395
Epoch 880, training loss: 0.07166565954685211 = 0.0035313740372657776 + 0.01 * 6.81342887878418
Epoch 880, val loss: 1.1421924829483032
Epoch 890, training loss: 0.0714215636253357 = 0.0034400876611471176 + 0.01 * 6.798147201538086
Epoch 890, val loss: 1.145437240600586
Epoch 900, training loss: 0.07134584337472916 = 0.0033530984073877335 + 0.01 * 6.799274921417236
Epoch 900, val loss: 1.148606538772583
Epoch 910, training loss: 0.07126562297344208 = 0.0032701462041586637 + 0.01 * 6.7995476722717285
Epoch 910, val loss: 1.151707649230957
Epoch 920, training loss: 0.07115045934915543 = 0.0031908955425024033 + 0.01 * 6.795956134796143
Epoch 920, val loss: 1.154744267463684
Epoch 930, training loss: 0.07090958952903748 = 0.003115211147814989 + 0.01 * 6.779438018798828
Epoch 930, val loss: 1.1577547788619995
Epoch 940, training loss: 0.07095932960510254 = 0.0030427840538322926 + 0.01 * 6.791654586791992
Epoch 940, val loss: 1.1606577634811401
Epoch 950, training loss: 0.07102324068546295 = 0.00297350506298244 + 0.01 * 6.804973602294922
Epoch 950, val loss: 1.1634151935577393
Epoch 960, training loss: 0.0707317665219307 = 0.0029072260949760675 + 0.01 * 6.782454490661621
Epoch 960, val loss: 1.166216254234314
Epoch 970, training loss: 0.07052220404148102 = 0.002843653317540884 + 0.01 * 6.767855167388916
Epoch 970, val loss: 1.168891191482544
Epoch 980, training loss: 0.07055144011974335 = 0.0027827962767332792 + 0.01 * 6.776864528656006
Epoch 980, val loss: 1.171533465385437
Epoch 990, training loss: 0.07057645171880722 = 0.0027242647483944893 + 0.01 * 6.785218238830566
Epoch 990, val loss: 1.1741042137145996
Epoch 1000, training loss: 0.07041628658771515 = 0.0026680519804358482 + 0.01 * 6.7748236656188965
Epoch 1000, val loss: 1.1766180992126465
Epoch 1010, training loss: 0.07018159329891205 = 0.0026141367852687836 + 0.01 * 6.756746292114258
Epoch 1010, val loss: 1.1790776252746582
Epoch 1020, training loss: 0.07033101469278336 = 0.002562178298830986 + 0.01 * 6.776883602142334
Epoch 1020, val loss: 1.1815099716186523
Epoch 1030, training loss: 0.0702633336186409 = 0.002512147184461355 + 0.01 * 6.775118350982666
Epoch 1030, val loss: 1.1838315725326538
Epoch 1040, training loss: 0.0699935108423233 = 0.0024643195793032646 + 0.01 * 6.752919673919678
Epoch 1040, val loss: 1.186227798461914
Epoch 1050, training loss: 0.06989492475986481 = 0.002418183023110032 + 0.01 * 6.747674465179443
Epoch 1050, val loss: 1.1884363889694214
Epoch 1060, training loss: 0.07009010016918182 = 0.0023737396113574505 + 0.01 * 6.771636486053467
Epoch 1060, val loss: 1.1906121969223022
Epoch 1070, training loss: 0.06981205940246582 = 0.00233088294044137 + 0.01 * 6.748117446899414
Epoch 1070, val loss: 1.1928186416625977
Epoch 1080, training loss: 0.06971099227666855 = 0.002289451891556382 + 0.01 * 6.742154121398926
Epoch 1080, val loss: 1.1949735879898071
Epoch 1090, training loss: 0.06969353556632996 = 0.002249568235129118 + 0.01 * 6.744396686553955
Epoch 1090, val loss: 1.197049617767334
Epoch 1100, training loss: 0.06958746910095215 = 0.0022109863348305225 + 0.01 * 6.737648010253906
Epoch 1100, val loss: 1.199065923690796
Epoch 1110, training loss: 0.06989436596632004 = 0.0021739108487963676 + 0.01 * 6.772046089172363
Epoch 1110, val loss: 1.2010812759399414
Epoch 1120, training loss: 0.06960387527942657 = 0.002137916162610054 + 0.01 * 6.746596336364746
Epoch 1120, val loss: 1.2030431032180786
Epoch 1130, training loss: 0.0696091577410698 = 0.002103332197293639 + 0.01 * 6.750583171844482
Epoch 1130, val loss: 1.2049996852874756
Epoch 1140, training loss: 0.06934834271669388 = 0.0020697196014225483 + 0.01 * 6.72786283493042
Epoch 1140, val loss: 1.2068285942077637
Epoch 1150, training loss: 0.06950157880783081 = 0.002037211786955595 + 0.01 * 6.746437072753906
Epoch 1150, val loss: 1.2087159156799316
Epoch 1160, training loss: 0.06935945153236389 = 0.002005912596359849 + 0.01 * 6.735354423522949
Epoch 1160, val loss: 1.2105391025543213
Epoch 1170, training loss: 0.06926150619983673 = 0.0019754383247345686 + 0.01 * 6.728607177734375
Epoch 1170, val loss: 1.2122900485992432
Epoch 1180, training loss: 0.06916575133800507 = 0.0019460845505818725 + 0.01 * 6.7219672203063965
Epoch 1180, val loss: 1.2140800952911377
Epoch 1190, training loss: 0.06907258927822113 = 0.0019175567431375384 + 0.01 * 6.715503692626953
Epoch 1190, val loss: 1.2157707214355469
Epoch 1200, training loss: 0.06933251023292542 = 0.001889907754957676 + 0.01 * 6.744260311126709
Epoch 1200, val loss: 1.2174619436264038
Epoch 1210, training loss: 0.06905577331781387 = 0.0018631552811712027 + 0.01 * 6.719262599945068
Epoch 1210, val loss: 1.219177484512329
Epoch 1220, training loss: 0.06914135068655014 = 0.001837296993471682 + 0.01 * 6.730405330657959
Epoch 1220, val loss: 1.2207987308502197
Epoch 1230, training loss: 0.0689309760928154 = 0.0018120541935786605 + 0.01 * 6.711892127990723
Epoch 1230, val loss: 1.2223155498504639
Epoch 1240, training loss: 0.06882228702306747 = 0.0017876491183415055 + 0.01 * 6.703464031219482
Epoch 1240, val loss: 1.2239172458648682
Epoch 1250, training loss: 0.06900864839553833 = 0.0017639579018577933 + 0.01 * 6.724469184875488
Epoch 1250, val loss: 1.2254688739776611
Epoch 1260, training loss: 0.06901656091213226 = 0.0017408898565918207 + 0.01 * 6.727567195892334
Epoch 1260, val loss: 1.2269139289855957
Epoch 1270, training loss: 0.06871552020311356 = 0.0017185482429340482 + 0.01 * 6.699697017669678
Epoch 1270, val loss: 1.228387713432312
Epoch 1280, training loss: 0.06911000609397888 = 0.0016967161791399121 + 0.01 * 6.741329193115234
Epoch 1280, val loss: 1.229864239692688
Epoch 1290, training loss: 0.06868594139814377 = 0.001675762701779604 + 0.01 * 6.701018333435059
Epoch 1290, val loss: 1.231213927268982
Epoch 1300, training loss: 0.06873565912246704 = 0.0016552358865737915 + 0.01 * 6.708042144775391
Epoch 1300, val loss: 1.2326149940490723
Epoch 1310, training loss: 0.06865326315164566 = 0.0016353282844647765 + 0.01 * 6.701793670654297
Epoch 1310, val loss: 1.2340160608291626
Epoch 1320, training loss: 0.06859030574560165 = 0.001615947694517672 + 0.01 * 6.6974358558654785
Epoch 1320, val loss: 1.2353501319885254
Epoch 1330, training loss: 0.06857915967702866 = 0.0015970742097124457 + 0.01 * 6.698208808898926
Epoch 1330, val loss: 1.236605167388916
Epoch 1340, training loss: 0.06861970573663712 = 0.0015786720905452967 + 0.01 * 6.704102993011475
Epoch 1340, val loss: 1.237979531288147
Epoch 1350, training loss: 0.06841907650232315 = 0.0015608902322128415 + 0.01 * 6.685818672180176
Epoch 1350, val loss: 1.2391895055770874
Epoch 1360, training loss: 0.06832557171583176 = 0.0015434458618983626 + 0.01 * 6.678212642669678
Epoch 1360, val loss: 1.240455150604248
Epoch 1370, training loss: 0.06847641617059708 = 0.0015265580732375383 + 0.01 * 6.694986343383789
Epoch 1370, val loss: 1.2416845560073853
Epoch 1380, training loss: 0.06822840869426727 = 0.0015101326862350106 + 0.01 * 6.67182731628418
Epoch 1380, val loss: 1.242794156074524
Epoch 1390, training loss: 0.0684693232178688 = 0.001494096708483994 + 0.01 * 6.69752311706543
Epoch 1390, val loss: 1.2439481019973755
Epoch 1400, training loss: 0.06821641325950623 = 0.0014784716768190265 + 0.01 * 6.673794746398926
Epoch 1400, val loss: 1.245107889175415
Epoch 1410, training loss: 0.0684223622083664 = 0.0014632083475589752 + 0.01 * 6.695915222167969
Epoch 1410, val loss: 1.2462035417556763
Epoch 1420, training loss: 0.06815054267644882 = 0.0014483743580058217 + 0.01 * 6.6702165603637695
Epoch 1420, val loss: 1.2472792863845825
Epoch 1430, training loss: 0.06813482940196991 = 0.0014338523615151644 + 0.01 * 6.670097827911377
Epoch 1430, val loss: 1.2482895851135254
Epoch 1440, training loss: 0.0680716335773468 = 0.001419696258381009 + 0.01 * 6.665194511413574
Epoch 1440, val loss: 1.249403953552246
Epoch 1450, training loss: 0.0680338516831398 = 0.0014059473760426044 + 0.01 * 6.662790298461914
Epoch 1450, val loss: 1.2503387928009033
Epoch 1460, training loss: 0.06813039630651474 = 0.0013924851082265377 + 0.01 * 6.673791408538818
Epoch 1460, val loss: 1.2513960599899292
Epoch 1470, training loss: 0.06816300749778748 = 0.0013793411199003458 + 0.01 * 6.678366661071777
Epoch 1470, val loss: 1.2522770166397095
Epoch 1480, training loss: 0.06792982667684555 = 0.0013665406731888652 + 0.01 * 6.6563286781311035
Epoch 1480, val loss: 1.2532612085342407
Epoch 1490, training loss: 0.06791439652442932 = 0.0013540114741772413 + 0.01 * 6.656039237976074
Epoch 1490, val loss: 1.2541946172714233
Epoch 1500, training loss: 0.0680743157863617 = 0.0013418893795460463 + 0.01 * 6.673243045806885
Epoch 1500, val loss: 1.2550731897354126
Epoch 1510, training loss: 0.06797225028276443 = 0.0013299370184540749 + 0.01 * 6.664231300354004
Epoch 1510, val loss: 1.2559220790863037
Epoch 1520, training loss: 0.06781179457902908 = 0.0013182604452595115 + 0.01 * 6.649353504180908
Epoch 1520, val loss: 1.256830096244812
Epoch 1530, training loss: 0.06799960881471634 = 0.001306820777244866 + 0.01 * 6.669278621673584
Epoch 1530, val loss: 1.2576086521148682
Epoch 1540, training loss: 0.06776443868875504 = 0.0012956383870914578 + 0.01 * 6.64687967300415
Epoch 1540, val loss: 1.2583855390548706
Epoch 1550, training loss: 0.06765419989824295 = 0.001284781377762556 + 0.01 * 6.636942386627197
Epoch 1550, val loss: 1.259247899055481
Epoch 1560, training loss: 0.06774080544710159 = 0.001274063135497272 + 0.01 * 6.646674633026123
Epoch 1560, val loss: 1.2600152492523193
Epoch 1570, training loss: 0.06774324178695679 = 0.0012636615429073572 + 0.01 * 6.647958278656006
Epoch 1570, val loss: 1.2607213258743286
Epoch 1580, training loss: 0.06758499890565872 = 0.0012533951085060835 + 0.01 * 6.633160591125488
Epoch 1580, val loss: 1.2614781856536865
Epoch 1590, training loss: 0.06780368089675903 = 0.0012433946831151843 + 0.01 * 6.6560282707214355
Epoch 1590, val loss: 1.26219642162323
Epoch 1600, training loss: 0.06751871109008789 = 0.001233618357218802 + 0.01 * 6.628509998321533
Epoch 1600, val loss: 1.262912392616272
Epoch 1610, training loss: 0.06757893413305283 = 0.001223981031216681 + 0.01 * 6.635495185852051
Epoch 1610, val loss: 1.2636178731918335
Epoch 1620, training loss: 0.06761902570724487 = 0.0012145655928179622 + 0.01 * 6.640446662902832
Epoch 1620, val loss: 1.264246940612793
Epoch 1630, training loss: 0.06744074821472168 = 0.0012053638929501176 + 0.01 * 6.623538494110107
Epoch 1630, val loss: 1.2649142742156982
Epoch 1640, training loss: 0.06768296658992767 = 0.0011962655698880553 + 0.01 * 6.648670196533203
Epoch 1640, val loss: 1.2655991315841675
Epoch 1650, training loss: 0.06746622174978256 = 0.0011874454794451594 + 0.01 * 6.627877712249756
Epoch 1650, val loss: 1.2662087678909302
Epoch 1660, training loss: 0.06751546263694763 = 0.0011787149123847485 + 0.01 * 6.6336750984191895
Epoch 1660, val loss: 1.2668938636779785
Epoch 1670, training loss: 0.06727724522352219 = 0.0011702112387865782 + 0.01 * 6.610703468322754
Epoch 1670, val loss: 1.2674663066864014
Epoch 1680, training loss: 0.06758913397789001 = 0.0011618917342275381 + 0.01 * 6.642724514007568
Epoch 1680, val loss: 1.268109917640686
Epoch 1690, training loss: 0.067501500248909 = 0.0011536345118656754 + 0.01 * 6.634787082672119
Epoch 1690, val loss: 1.2686153650283813
Epoch 1700, training loss: 0.06745181977748871 = 0.0011456613428890705 + 0.01 * 6.630616188049316
Epoch 1700, val loss: 1.2692313194274902
Epoch 1710, training loss: 0.06728391349315643 = 0.0011377328773960471 + 0.01 * 6.614618301391602
Epoch 1710, val loss: 1.269742727279663
Epoch 1720, training loss: 0.06717030704021454 = 0.0011299866018816829 + 0.01 * 6.604032516479492
Epoch 1720, val loss: 1.2703551054000854
Epoch 1730, training loss: 0.06728801876306534 = 0.0011223765322938561 + 0.01 * 6.6165642738342285
Epoch 1730, val loss: 1.2708892822265625
Epoch 1740, training loss: 0.06737538427114487 = 0.001114857499487698 + 0.01 * 6.6260528564453125
Epoch 1740, val loss: 1.271418809890747
Epoch 1750, training loss: 0.06744072586297989 = 0.0011074902722612023 + 0.01 * 6.633323669433594
Epoch 1750, val loss: 1.2719637155532837
Epoch 1760, training loss: 0.0670965164899826 = 0.0011002791579812765 + 0.01 * 6.599623680114746
Epoch 1760, val loss: 1.2724714279174805
Epoch 1770, training loss: 0.06744623929262161 = 0.001093179569579661 + 0.01 * 6.635306358337402
Epoch 1770, val loss: 1.2729806900024414
Epoch 1780, training loss: 0.06705877929925919 = 0.0010861668270081282 + 0.01 * 6.59726095199585
Epoch 1780, val loss: 1.2735323905944824
Epoch 1790, training loss: 0.06718984991312027 = 0.0010793074034154415 + 0.01 * 6.611054420471191
Epoch 1790, val loss: 1.2740199565887451
Epoch 1800, training loss: 0.06719520688056946 = 0.001072539365850389 + 0.01 * 6.612267017364502
Epoch 1800, val loss: 1.2745492458343506
Epoch 1810, training loss: 0.0671292319893837 = 0.0010659501422196627 + 0.01 * 6.60632848739624
Epoch 1810, val loss: 1.2749736309051514
Epoch 1820, training loss: 0.06698358058929443 = 0.0010594144696369767 + 0.01 * 6.592417240142822
Epoch 1820, val loss: 1.275508165359497
Epoch 1830, training loss: 0.0670403242111206 = 0.001053036074154079 + 0.01 * 6.598729133605957
Epoch 1830, val loss: 1.2759544849395752
Epoch 1840, training loss: 0.06729399412870407 = 0.0010467392858117819 + 0.01 * 6.624725818634033
Epoch 1840, val loss: 1.2763612270355225
Epoch 1850, training loss: 0.06699313223361969 = 0.0010405052453279495 + 0.01 * 6.5952630043029785
Epoch 1850, val loss: 1.2768728733062744
Epoch 1860, training loss: 0.06692333519458771 = 0.0010343779576942325 + 0.01 * 6.588895797729492
Epoch 1860, val loss: 1.2773051261901855
Epoch 1870, training loss: 0.06693405658006668 = 0.0010283145820721984 + 0.01 * 6.590574264526367
Epoch 1870, val loss: 1.2778016328811646
Epoch 1880, training loss: 0.06712808459997177 = 0.0010224616853520274 + 0.01 * 6.610562801361084
Epoch 1880, val loss: 1.2781926393508911
Epoch 1890, training loss: 0.0672244057059288 = 0.0010165756102651358 + 0.01 * 6.62078332901001
Epoch 1890, val loss: 1.278611421585083
Epoch 1900, training loss: 0.06685081124305725 = 0.001010874635539949 + 0.01 * 6.583994388580322
Epoch 1900, val loss: 1.2789900302886963
Epoch 1910, training loss: 0.06680555641651154 = 0.0010051855351775885 + 0.01 * 6.5800371170043945
Epoch 1910, val loss: 1.2794159650802612
Epoch 1920, training loss: 0.06690499186515808 = 0.0009996079606935382 + 0.01 * 6.59053897857666
Epoch 1920, val loss: 1.279815912246704
Epoch 1930, training loss: 0.06718321144580841 = 0.0009941013995558023 + 0.01 * 6.618911266326904
Epoch 1930, val loss: 1.2801989316940308
Epoch 1940, training loss: 0.06695002317428589 = 0.0009886971674859524 + 0.01 * 6.596132755279541
Epoch 1940, val loss: 1.2805659770965576
Epoch 1950, training loss: 0.06675215810537338 = 0.0009833346121013165 + 0.01 * 6.576882362365723
Epoch 1950, val loss: 1.280954122543335
Epoch 1960, training loss: 0.06676636636257172 = 0.000978070660494268 + 0.01 * 6.578830242156982
Epoch 1960, val loss: 1.2813364267349243
Epoch 1970, training loss: 0.06716644018888474 = 0.0009729098528623581 + 0.01 * 6.619353294372559
Epoch 1970, val loss: 1.2816932201385498
Epoch 1980, training loss: 0.06675305962562561 = 0.0009677252382971346 + 0.01 * 6.578533172607422
Epoch 1980, val loss: 1.2820624113082886
Epoch 1990, training loss: 0.06693103164434433 = 0.0009626697865314782 + 0.01 * 6.596836566925049
Epoch 1990, val loss: 1.282429814338684
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8323668950975225
=== training gcn model ===
Epoch 0, training loss: 2.0150203704833984 = 1.9290523529052734 + 0.01 * 8.596795082092285
Epoch 0, val loss: 1.9339452981948853
Epoch 10, training loss: 2.0056474208831787 = 1.9196804761886597 + 0.01 * 8.596704483032227
Epoch 10, val loss: 1.9239370822906494
Epoch 20, training loss: 1.9938398599624634 = 1.907875895500183 + 0.01 * 8.596397399902344
Epoch 20, val loss: 1.9112029075622559
Epoch 30, training loss: 1.97693932056427 = 1.8909860849380493 + 0.01 * 8.595328330993652
Epoch 30, val loss: 1.8929814100265503
Epoch 40, training loss: 1.9520920515060425 = 1.8662083148956299 + 0.01 * 8.588374137878418
Epoch 40, val loss: 1.866572380065918
Epoch 50, training loss: 1.9182968139648438 = 1.8328895568847656 + 0.01 * 8.540719985961914
Epoch 50, val loss: 1.83280348777771
Epoch 60, training loss: 1.8814587593078613 = 1.798264741897583 + 0.01 * 8.319404602050781
Epoch 60, val loss: 1.8012785911560059
Epoch 70, training loss: 1.8488227128982544 = 1.7677578926086426 + 0.01 * 8.10647964477539
Epoch 70, val loss: 1.7752641439437866
Epoch 80, training loss: 1.8046104907989502 = 1.7261532545089722 + 0.01 * 7.845720291137695
Epoch 80, val loss: 1.7384613752365112
Epoch 90, training loss: 1.7433481216430664 = 1.6674572229385376 + 0.01 * 7.589086055755615
Epoch 90, val loss: 1.6875908374786377
Epoch 100, training loss: 1.6623666286468506 = 1.5883378982543945 + 0.01 * 7.402875900268555
Epoch 100, val loss: 1.620869517326355
Epoch 110, training loss: 1.5679155588150024 = 1.4948965311050415 + 0.01 * 7.301898956298828
Epoch 110, val loss: 1.5426234006881714
Epoch 120, training loss: 1.4698526859283447 = 1.397323489189148 + 0.01 * 7.25292444229126
Epoch 120, val loss: 1.463173270225525
Epoch 130, training loss: 1.371453046798706 = 1.2993003129959106 + 0.01 * 7.215274333953857
Epoch 130, val loss: 1.3839490413665771
Epoch 140, training loss: 1.2728770971298218 = 1.201006531715393 + 0.01 * 7.187060832977295
Epoch 140, val loss: 1.306152582168579
Epoch 150, training loss: 1.1766841411590576 = 1.1050018072128296 + 0.01 * 7.168231010437012
Epoch 150, val loss: 1.2319918870925903
Epoch 160, training loss: 1.086705207824707 = 1.015179991722107 + 0.01 * 7.152515888214111
Epoch 160, val loss: 1.164832592010498
Epoch 170, training loss: 1.0037832260131836 = 0.9324378967285156 + 0.01 * 7.13453483581543
Epoch 170, val loss: 1.1049859523773193
Epoch 180, training loss: 0.9257048964500427 = 0.8546028137207031 + 0.01 * 7.110208511352539
Epoch 180, val loss: 1.0490623712539673
Epoch 190, training loss: 0.8502614498138428 = 0.779430091381073 + 0.01 * 7.08313512802124
Epoch 190, val loss: 0.9951073527336121
Epoch 200, training loss: 0.777435839176178 = 0.7068995833396912 + 0.01 * 7.053627014160156
Epoch 200, val loss: 0.9432811141014099
Epoch 210, training loss: 0.7094528675079346 = 0.6391129493713379 + 0.01 * 7.033993244171143
Epoch 210, val loss: 0.8958632349967957
Epoch 220, training loss: 0.6485622525215149 = 0.5784329175949097 + 0.01 * 7.012935161590576
Epoch 220, val loss: 0.8557204604148865
Epoch 230, training loss: 0.5955412983894348 = 0.5255683064460754 + 0.01 * 6.9972968101501465
Epoch 230, val loss: 0.824069082736969
Epoch 240, training loss: 0.5491958856582642 = 0.4794202744960785 + 0.01 * 6.977558612823486
Epoch 240, val loss: 0.8004093766212463
Epoch 250, training loss: 0.5075895190238953 = 0.4379521310329437 + 0.01 * 6.9637370109558105
Epoch 250, val loss: 0.7828546166419983
Epoch 260, training loss: 0.4685732126235962 = 0.3989839255809784 + 0.01 * 6.958928108215332
Epoch 260, val loss: 0.7691672444343567
Epoch 270, training loss: 0.4302055835723877 = 0.36079519987106323 + 0.01 * 6.941038131713867
Epoch 270, val loss: 0.7575024366378784
Epoch 280, training loss: 0.3915817141532898 = 0.32227015495300293 + 0.01 * 6.931157112121582
Epoch 280, val loss: 0.7470815777778625
Epoch 290, training loss: 0.3526324927806854 = 0.28333964943885803 + 0.01 * 6.929285049438477
Epoch 290, val loss: 0.7381352186203003
Epoch 300, training loss: 0.31444624066352844 = 0.24519392848014832 + 0.01 * 6.925232410430908
Epoch 300, val loss: 0.7318465113639832
Epoch 310, training loss: 0.2792183756828308 = 0.20998473465442657 + 0.01 * 6.923365116119385
Epoch 310, val loss: 0.729382336139679
Epoch 320, training loss: 0.24874038994312286 = 0.1794925481081009 + 0.01 * 6.924784183502197
Epoch 320, val loss: 0.7314311265945435
Epoch 330, training loss: 0.2233315408229828 = 0.15415848791599274 + 0.01 * 6.917306423187256
Epoch 330, val loss: 0.7373458743095398
Epoch 340, training loss: 0.20255304872989655 = 0.13339272141456604 + 0.01 * 6.916032791137695
Epoch 340, val loss: 0.7461586594581604
Epoch 350, training loss: 0.18539832532405853 = 0.11628302931785583 + 0.01 * 6.911530017852783
Epoch 350, val loss: 0.7569175958633423
Epoch 360, training loss: 0.17110300064086914 = 0.10200274735689163 + 0.01 * 6.9100260734558105
Epoch 360, val loss: 0.7689347267150879
Epoch 370, training loss: 0.15898454189300537 = 0.08992782235145569 + 0.01 * 6.905673027038574
Epoch 370, val loss: 0.7816904187202454
Epoch 380, training loss: 0.1486215889453888 = 0.07959761470556259 + 0.01 * 6.902397632598877
Epoch 380, val loss: 0.7949854135513306
Epoch 390, training loss: 0.13969379663467407 = 0.07068689912557602 + 0.01 * 6.900689125061035
Epoch 390, val loss: 0.8086411356925964
Epoch 400, training loss: 0.1319378763437271 = 0.06296585500240326 + 0.01 * 6.897202491760254
Epoch 400, val loss: 0.8225479125976562
Epoch 410, training loss: 0.12516888976097107 = 0.056255292147397995 + 0.01 * 6.891359806060791
Epoch 410, val loss: 0.836546003818512
Epoch 420, training loss: 0.11946780979633331 = 0.050410617142915726 + 0.01 * 6.905718803405762
Epoch 420, val loss: 0.8505866527557373
Epoch 430, training loss: 0.1141960397362709 = 0.045315466821193695 + 0.01 * 6.888057231903076
Epoch 430, val loss: 0.8645834922790527
Epoch 440, training loss: 0.10965108126401901 = 0.040863998234272 + 0.01 * 6.878708362579346
Epoch 440, val loss: 0.8784700036048889
Epoch 450, training loss: 0.10582420974969864 = 0.03696214407682419 + 0.01 * 6.88620662689209
Epoch 450, val loss: 0.8922910094261169
Epoch 460, training loss: 0.10223552584648132 = 0.033543527126312256 + 0.01 * 6.869199752807617
Epoch 460, val loss: 0.9058665037155151
Epoch 470, training loss: 0.09920764714479446 = 0.030540285632014275 + 0.01 * 6.866735935211182
Epoch 470, val loss: 0.9192095994949341
Epoch 480, training loss: 0.096488356590271 = 0.027894578874111176 + 0.01 * 6.859377861022949
Epoch 480, val loss: 0.9323297739028931
Epoch 490, training loss: 0.09427253901958466 = 0.02555861882865429 + 0.01 * 6.871392250061035
Epoch 490, val loss: 0.9451383352279663
Epoch 500, training loss: 0.09208931773900986 = 0.023493146523833275 + 0.01 * 6.859617233276367
Epoch 500, val loss: 0.9576622843742371
Epoch 510, training loss: 0.09014548361301422 = 0.02165817841887474 + 0.01 * 6.848730564117432
Epoch 510, val loss: 0.9698801636695862
Epoch 520, training loss: 0.08847001194953918 = 0.020021462813019753 + 0.01 * 6.844855308532715
Epoch 520, val loss: 0.9818218350410461
Epoch 530, training loss: 0.08696044981479645 = 0.018557224422693253 + 0.01 * 6.840322494506836
Epoch 530, val loss: 0.9935083389282227
Epoch 540, training loss: 0.08567468076944351 = 0.017243819311261177 + 0.01 * 6.843086242675781
Epoch 540, val loss: 1.0049189329147339
Epoch 550, training loss: 0.084515780210495 = 0.016067085787653923 + 0.01 * 6.844869136810303
Epoch 550, val loss: 1.016061544418335
Epoch 560, training loss: 0.08339697122573853 = 0.015008695423603058 + 0.01 * 6.838828086853027
Epoch 560, val loss: 1.0267820358276367
Epoch 570, training loss: 0.08234168589115143 = 0.01405333075672388 + 0.01 * 6.828835964202881
Epoch 570, val loss: 1.0372192859649658
Epoch 580, training loss: 0.08144910633563995 = 0.013187948614358902 + 0.01 * 6.82611608505249
Epoch 580, val loss: 1.0473664999008179
Epoch 590, training loss: 0.08066275715827942 = 0.012401494197547436 + 0.01 * 6.826127052307129
Epoch 590, val loss: 1.0571999549865723
Epoch 600, training loss: 0.07987630367279053 = 0.011685255914926529 + 0.01 * 6.81910514831543
Epoch 600, val loss: 1.0668092966079712
Epoch 610, training loss: 0.07921549677848816 = 0.011031311936676502 + 0.01 * 6.818418502807617
Epoch 610, val loss: 1.0761209726333618
Epoch 620, training loss: 0.0785687044262886 = 0.010433272458612919 + 0.01 * 6.813543319702148
Epoch 620, val loss: 1.0851713418960571
Epoch 630, training loss: 0.07798159122467041 = 0.009884890168905258 + 0.01 * 6.809670925140381
Epoch 630, val loss: 1.0940132141113281
Epoch 640, training loss: 0.0774836540222168 = 0.009380652569234371 + 0.01 * 6.810299873352051
Epoch 640, val loss: 1.1025846004486084
Epoch 650, training loss: 0.07698598504066467 = 0.008916005492210388 + 0.01 * 6.806998252868652
Epoch 650, val loss: 1.1109602451324463
Epoch 660, training loss: 0.07650359719991684 = 0.008487259969115257 + 0.01 * 6.801633834838867
Epoch 660, val loss: 1.1191000938415527
Epoch 670, training loss: 0.07616352289915085 = 0.008090867660939693 + 0.01 * 6.807265758514404
Epoch 670, val loss: 1.1270262002944946
Epoch 680, training loss: 0.07574773579835892 = 0.007724530063569546 + 0.01 * 6.80232048034668
Epoch 680, val loss: 1.1347311735153198
Epoch 690, training loss: 0.07532651722431183 = 0.0073844268918037415 + 0.01 * 6.794209003448486
Epoch 690, val loss: 1.1422450542449951
Epoch 700, training loss: 0.07510720193386078 = 0.007068127393722534 + 0.01 * 6.803907871246338
Epoch 700, val loss: 1.149552345275879
Epoch 710, training loss: 0.07468123733997345 = 0.006773692090064287 + 0.01 * 6.790754318237305
Epoch 710, val loss: 1.1566414833068848
Epoch 720, training loss: 0.07438942790031433 = 0.006498821545392275 + 0.01 * 6.789061069488525
Epoch 720, val loss: 1.163609266281128
Epoch 730, training loss: 0.07410505414009094 = 0.00624213507398963 + 0.01 * 6.786292552947998
Epoch 730, val loss: 1.1703836917877197
Epoch 740, training loss: 0.0738258808851242 = 0.00600225618109107 + 0.01 * 6.782362937927246
Epoch 740, val loss: 1.1769554615020752
Epoch 750, training loss: 0.07356000691652298 = 0.00577731104567647 + 0.01 * 6.7782697677612305
Epoch 750, val loss: 1.183410406112671
Epoch 760, training loss: 0.07334091514348984 = 0.005566192790865898 + 0.01 * 6.777472019195557
Epoch 760, val loss: 1.1896589994430542
Epoch 770, training loss: 0.07313842326402664 = 0.005367941223084927 + 0.01 * 6.777048587799072
Epoch 770, val loss: 1.1958370208740234
Epoch 780, training loss: 0.07296711206436157 = 0.00518126180395484 + 0.01 * 6.778585433959961
Epoch 780, val loss: 1.201851725578308
Epoch 790, training loss: 0.07268976420164108 = 0.005005612038075924 + 0.01 * 6.768415451049805
Epoch 790, val loss: 1.2076455354690552
Epoch 800, training loss: 0.07249432057142258 = 0.004839831963181496 + 0.01 * 6.765449523925781
Epoch 800, val loss: 1.213409185409546
Epoch 810, training loss: 0.07235322147607803 = 0.004683240316808224 + 0.01 * 6.766998291015625
Epoch 810, val loss: 1.2189955711364746
Epoch 820, training loss: 0.0721348375082016 = 0.004535391461104155 + 0.01 * 6.759944438934326
Epoch 820, val loss: 1.224455714225769
Epoch 830, training loss: 0.07202979177236557 = 0.004395437892526388 + 0.01 * 6.763435363769531
Epoch 830, val loss: 1.2297992706298828
Epoch 840, training loss: 0.0718170702457428 = 0.0042630587704479694 + 0.01 * 6.755401134490967
Epoch 840, val loss: 1.235014796257019
Epoch 850, training loss: 0.07175088673830032 = 0.004137554205954075 + 0.01 * 6.761333465576172
Epoch 850, val loss: 1.2401093244552612
Epoch 860, training loss: 0.07151326537132263 = 0.004018742591142654 + 0.01 * 6.749453067779541
Epoch 860, val loss: 1.245068907737732
Epoch 870, training loss: 0.07143597304821014 = 0.0039057163521647453 + 0.01 * 6.753026008605957
Epoch 870, val loss: 1.249915361404419
Epoch 880, training loss: 0.07130703330039978 = 0.0037983041256666183 + 0.01 * 6.75087308883667
Epoch 880, val loss: 1.254698395729065
Epoch 890, training loss: 0.07115697115659714 = 0.003696325235068798 + 0.01 * 6.74606466293335
Epoch 890, val loss: 1.259278416633606
Epoch 900, training loss: 0.0710482969880104 = 0.003599017160013318 + 0.01 * 6.744927883148193
Epoch 900, val loss: 1.2638318538665771
Epoch 910, training loss: 0.0708959624171257 = 0.0035062162205576897 + 0.01 * 6.7389750480651855
Epoch 910, val loss: 1.2682873010635376
Epoch 920, training loss: 0.07081539928913116 = 0.0034176926128566265 + 0.01 * 6.739770412445068
Epoch 920, val loss: 1.2726582288742065
Epoch 930, training loss: 0.07084555178880692 = 0.0033329047728329897 + 0.01 * 6.751265048980713
Epoch 930, val loss: 1.2769702672958374
Epoch 940, training loss: 0.0706108808517456 = 0.003251890419051051 + 0.01 * 6.735899448394775
Epoch 940, val loss: 1.2810968160629272
Epoch 950, training loss: 0.07051794230937958 = 0.0031738344114273787 + 0.01 * 6.734410762786865
Epoch 950, val loss: 1.2852771282196045
Epoch 960, training loss: 0.07047354429960251 = 0.0030992503743618727 + 0.01 * 6.737430095672607
Epoch 960, val loss: 1.2893469333648682
Epoch 970, training loss: 0.0703171044588089 = 0.0030279236380010843 + 0.01 * 6.728918075561523
Epoch 970, val loss: 1.2932442426681519
Epoch 980, training loss: 0.07019728422164917 = 0.002959241159260273 + 0.01 * 6.723804473876953
Epoch 980, val loss: 1.2971736192703247
Epoch 990, training loss: 0.07027729600667953 = 0.0028937733732163906 + 0.01 * 6.738352298736572
Epoch 990, val loss: 1.3010140657424927
Epoch 1000, training loss: 0.07012660056352615 = 0.0028305284213274717 + 0.01 * 6.729607582092285
Epoch 1000, val loss: 1.304705023765564
Epoch 1010, training loss: 0.07013897597789764 = 0.002770233666524291 + 0.01 * 6.736874580383301
Epoch 1010, val loss: 1.3084133863449097
Epoch 1020, training loss: 0.06994493305683136 = 0.0027122909668833017 + 0.01 * 6.723264217376709
Epoch 1020, val loss: 1.311934471130371
Epoch 1030, training loss: 0.06978341937065125 = 0.002656603464856744 + 0.01 * 6.712682247161865
Epoch 1030, val loss: 1.315473198890686
Epoch 1040, training loss: 0.0697110965847969 = 0.0026029942091554403 + 0.01 * 6.710810661315918
Epoch 1040, val loss: 1.3189080953598022
Epoch 1050, training loss: 0.06971676647663116 = 0.0025516641326248646 + 0.01 * 6.716509819030762
Epoch 1050, val loss: 1.3223000764846802
Epoch 1060, training loss: 0.06976258754730225 = 0.002502051880583167 + 0.01 * 6.726053714752197
Epoch 1060, val loss: 1.325579285621643
Epoch 1070, training loss: 0.06954831629991531 = 0.0024543670006096363 + 0.01 * 6.709394454956055
Epoch 1070, val loss: 1.3288416862487793
Epoch 1080, training loss: 0.06946149468421936 = 0.002408359432592988 + 0.01 * 6.705313205718994
Epoch 1080, val loss: 1.3320469856262207
Epoch 1090, training loss: 0.06951411068439484 = 0.0023642098531126976 + 0.01 * 6.714990139007568
Epoch 1090, val loss: 1.3351759910583496
Epoch 1100, training loss: 0.06929807364940643 = 0.0023214551620185375 + 0.01 * 6.697662353515625
Epoch 1100, val loss: 1.3382266759872437
Epoch 1110, training loss: 0.06934910267591476 = 0.0022803773172199726 + 0.01 * 6.706872940063477
Epoch 1110, val loss: 1.341257929801941
Epoch 1120, training loss: 0.06928727775812149 = 0.002240567235276103 + 0.01 * 6.704671382904053
Epoch 1120, val loss: 1.344207763671875
Epoch 1130, training loss: 0.06926833093166351 = 0.0022024314384907484 + 0.01 * 6.70659065246582
Epoch 1130, val loss: 1.3471189737319946
Epoch 1140, training loss: 0.06922107189893723 = 0.0021654414013028145 + 0.01 * 6.705562591552734
Epoch 1140, val loss: 1.3499221801757812
Epoch 1150, training loss: 0.06906818598508835 = 0.002129972679540515 + 0.01 * 6.693821430206299
Epoch 1150, val loss: 1.3527253866195679
Epoch 1160, training loss: 0.06923754513263702 = 0.0020955821964889765 + 0.01 * 6.714196681976318
Epoch 1160, val loss: 1.355404257774353
Epoch 1170, training loss: 0.06905149668455124 = 0.0020624438766390085 + 0.01 * 6.698904991149902
Epoch 1170, val loss: 1.3581064939498901
Epoch 1180, training loss: 0.06885535269975662 = 0.002030263189226389 + 0.01 * 6.682509422302246
Epoch 1180, val loss: 1.3607341051101685
Epoch 1190, training loss: 0.06886886060237885 = 0.0019991600420325994 + 0.01 * 6.686969757080078
Epoch 1190, val loss: 1.3633452653884888
Epoch 1200, training loss: 0.06900987029075623 = 0.0019691858906298876 + 0.01 * 6.704068183898926
Epoch 1200, val loss: 1.3658807277679443
Epoch 1210, training loss: 0.06879092752933502 = 0.0019399778684601188 + 0.01 * 6.685095310211182
Epoch 1210, val loss: 1.3683559894561768
Epoch 1220, training loss: 0.06864380091428757 = 0.0019119901116937399 + 0.01 * 6.673181533813477
Epoch 1220, val loss: 1.3707891702651978
Epoch 1230, training loss: 0.068910151720047 = 0.0018848032923415303 + 0.01 * 6.7025346755981445
Epoch 1230, val loss: 1.3731286525726318
Epoch 1240, training loss: 0.06875172257423401 = 0.0018584412755444646 + 0.01 * 6.689328193664551
Epoch 1240, val loss: 1.3754962682724
Epoch 1250, training loss: 0.06860718876123428 = 0.0018329248996451497 + 0.01 * 6.677426338195801
Epoch 1250, val loss: 1.37776780128479
Epoch 1260, training loss: 0.06858132034540176 = 0.0018082450842484832 + 0.01 * 6.67730712890625
Epoch 1260, val loss: 1.3800644874572754
Epoch 1270, training loss: 0.06859934329986572 = 0.001784093794412911 + 0.01 * 6.681525230407715
Epoch 1270, val loss: 1.3823009729385376
Epoch 1280, training loss: 0.06854520738124847 = 0.0017609120113775134 + 0.01 * 6.67842960357666
Epoch 1280, val loss: 1.3844300508499146
Epoch 1290, training loss: 0.0683288499712944 = 0.0017381854122504592 + 0.01 * 6.659067153930664
Epoch 1290, val loss: 1.3866363763809204
Epoch 1300, training loss: 0.0684099793434143 = 0.0017162958392873406 + 0.01 * 6.669368743896484
Epoch 1300, val loss: 1.38874351978302
Epoch 1310, training loss: 0.06846573203802109 = 0.0016950125573202968 + 0.01 * 6.677072048187256
Epoch 1310, val loss: 1.3908238410949707
Epoch 1320, training loss: 0.0684061273932457 = 0.0016743987798690796 + 0.01 * 6.673172950744629
Epoch 1320, val loss: 1.3927652835845947
Epoch 1330, training loss: 0.06820644438266754 = 0.001654425635933876 + 0.01 * 6.655202388763428
Epoch 1330, val loss: 1.394757866859436
Epoch 1340, training loss: 0.0682024359703064 = 0.0016349502839148045 + 0.01 * 6.6567487716674805
Epoch 1340, val loss: 1.3967430591583252
Epoch 1350, training loss: 0.06843823194503784 = 0.0016160677187144756 + 0.01 * 6.682216644287109
Epoch 1350, val loss: 1.3986163139343262
Epoch 1360, training loss: 0.0681297555565834 = 0.0015975787537172437 + 0.01 * 6.6532182693481445
Epoch 1360, val loss: 1.4005296230316162
Epoch 1370, training loss: 0.06816058605909348 = 0.0015797665109857917 + 0.01 * 6.658082008361816
Epoch 1370, val loss: 1.4024392366409302
Epoch 1380, training loss: 0.06799780577421188 = 0.0015623719664290547 + 0.01 * 6.643543243408203
Epoch 1380, val loss: 1.4042282104492188
Epoch 1390, training loss: 0.06809142976999283 = 0.0015455634566023946 + 0.01 * 6.6545867919921875
Epoch 1390, val loss: 1.4059393405914307
Epoch 1400, training loss: 0.06811532378196716 = 0.0015289988368749619 + 0.01 * 6.658632755279541
Epoch 1400, val loss: 1.4077630043029785
Epoch 1410, training loss: 0.06801685690879822 = 0.0015130452811717987 + 0.01 * 6.650380611419678
Epoch 1410, val loss: 1.409448504447937
Epoch 1420, training loss: 0.06786026805639267 = 0.0014974751975387335 + 0.01 * 6.636279582977295
Epoch 1420, val loss: 1.4112112522125244
Epoch 1430, training loss: 0.06791756302118301 = 0.0014823401579633355 + 0.01 * 6.6435227394104
Epoch 1430, val loss: 1.4128096103668213
Epoch 1440, training loss: 0.06788736581802368 = 0.0014676004648208618 + 0.01 * 6.641976356506348
Epoch 1440, val loss: 1.4144222736358643
Epoch 1450, training loss: 0.06779349595308304 = 0.001453100354410708 + 0.01 * 6.634039878845215
Epoch 1450, val loss: 1.4161128997802734
Epoch 1460, training loss: 0.06799862533807755 = 0.0014390627620741725 + 0.01 * 6.655956268310547
Epoch 1460, val loss: 1.417695164680481
Epoch 1470, training loss: 0.0676378682255745 = 0.0014254387933760881 + 0.01 * 6.621243476867676
Epoch 1470, val loss: 1.41915762424469
Epoch 1480, training loss: 0.06785022467374802 = 0.0014121284475550056 + 0.01 * 6.6438093185424805
Epoch 1480, val loss: 1.4207446575164795
Epoch 1490, training loss: 0.06773249804973602 = 0.001399202854372561 + 0.01 * 6.633329391479492
Epoch 1490, val loss: 1.4222102165222168
Epoch 1500, training loss: 0.06790141761302948 = 0.001386447111144662 + 0.01 * 6.651496887207031
Epoch 1500, val loss: 1.4236778020858765
Epoch 1510, training loss: 0.06753568351268768 = 0.0013741122093051672 + 0.01 * 6.616157531738281
Epoch 1510, val loss: 1.4250894784927368
Epoch 1520, training loss: 0.06778723001480103 = 0.0013619750970974565 + 0.01 * 6.642526149749756
Epoch 1520, val loss: 1.4265509843826294
Epoch 1530, training loss: 0.0676112174987793 = 0.0013502686051651835 + 0.01 * 6.626094818115234
Epoch 1530, val loss: 1.4279478788375854
Epoch 1540, training loss: 0.0674792155623436 = 0.001338775735348463 + 0.01 * 6.614044189453125
Epoch 1540, val loss: 1.429263710975647
Epoch 1550, training loss: 0.06756246089935303 = 0.0013275723904371262 + 0.01 * 6.623488903045654
Epoch 1550, val loss: 1.430617094039917
Epoch 1560, training loss: 0.06764310598373413 = 0.0013164269039407372 + 0.01 * 6.6326680183410645
Epoch 1560, val loss: 1.4320186376571655
Epoch 1570, training loss: 0.06753964722156525 = 0.0013057747855782509 + 0.01 * 6.623387336730957
Epoch 1570, val loss: 1.4332796335220337
Epoch 1580, training loss: 0.0675535798072815 = 0.0012950798263773322 + 0.01 * 6.625849723815918
Epoch 1580, val loss: 1.43459153175354
Epoch 1590, training loss: 0.06748337298631668 = 0.0012849014019593596 + 0.01 * 6.619846820831299
Epoch 1590, val loss: 1.4358325004577637
Epoch 1600, training loss: 0.06725268065929413 = 0.0012748031876981258 + 0.01 * 6.597788333892822
Epoch 1600, val loss: 1.4370821714401245
Epoch 1610, training loss: 0.0674598440527916 = 0.0012651571305468678 + 0.01 * 6.619468688964844
Epoch 1610, val loss: 1.4383383989334106
Epoch 1620, training loss: 0.06732268631458282 = 0.0012554111890494823 + 0.01 * 6.6067280769348145
Epoch 1620, val loss: 1.4394779205322266
Epoch 1630, training loss: 0.06735102832317352 = 0.0012460022699087858 + 0.01 * 6.61050271987915
Epoch 1630, val loss: 1.4407511949539185
Epoch 1640, training loss: 0.06738182157278061 = 0.0012367669260129333 + 0.01 * 6.614505290985107
Epoch 1640, val loss: 1.4419277906417847
Epoch 1650, training loss: 0.06705169379711151 = 0.0012278355425223708 + 0.01 * 6.582386016845703
Epoch 1650, val loss: 1.4430758953094482
Epoch 1660, training loss: 0.06738732755184174 = 0.0012191106798127294 + 0.01 * 6.616822242736816
Epoch 1660, val loss: 1.444288730621338
Epoch 1670, training loss: 0.06721405684947968 = 0.001210390473715961 + 0.01 * 6.600366592407227
Epoch 1670, val loss: 1.4453303813934326
Epoch 1680, training loss: 0.06706348061561584 = 0.0012019037967547774 + 0.01 * 6.586157321929932
Epoch 1680, val loss: 1.4464994668960571
Epoch 1690, training loss: 0.0673506110906601 = 0.001193766132928431 + 0.01 * 6.615684509277344
Epoch 1690, val loss: 1.4475351572036743
Epoch 1700, training loss: 0.06694254279136658 = 0.0011855429038405418 + 0.01 * 6.575700283050537
Epoch 1700, val loss: 1.448652744293213
Epoch 1710, training loss: 0.06700266152620316 = 0.0011775322491303086 + 0.01 * 6.582512855529785
Epoch 1710, val loss: 1.4497489929199219
Epoch 1720, training loss: 0.06721699237823486 = 0.0011698389425873756 + 0.01 * 6.604715824127197
Epoch 1720, val loss: 1.4507030248641968
Epoch 1730, training loss: 0.06689807772636414 = 0.001162115135230124 + 0.01 * 6.573596954345703
Epoch 1730, val loss: 1.451812505722046
Epoch 1740, training loss: 0.06728587299585342 = 0.0011547026224434376 + 0.01 * 6.61311674118042
Epoch 1740, val loss: 1.452841877937317
Epoch 1750, training loss: 0.06698678433895111 = 0.0011472324840724468 + 0.01 * 6.58395528793335
Epoch 1750, val loss: 1.4538532495498657
Epoch 1760, training loss: 0.06700737029314041 = 0.0011400480289012194 + 0.01 * 6.586732387542725
Epoch 1760, val loss: 1.4548759460449219
Epoch 1770, training loss: 0.0669553130865097 = 0.0011329447152093053 + 0.01 * 6.582237243652344
Epoch 1770, val loss: 1.4557993412017822
Epoch 1780, training loss: 0.06669764220714569 = 0.0011259521124884486 + 0.01 * 6.557168960571289
Epoch 1780, val loss: 1.4568368196487427
Epoch 1790, training loss: 0.06678038090467453 = 0.0011192118981853127 + 0.01 * 6.566116809844971
Epoch 1790, val loss: 1.4578185081481934
Epoch 1800, training loss: 0.06682733446359634 = 0.001112399622797966 + 0.01 * 6.571493148803711
Epoch 1800, val loss: 1.4587651491165161
Epoch 1810, training loss: 0.06660862267017365 = 0.001105811563320458 + 0.01 * 6.550280570983887
Epoch 1810, val loss: 1.4597753286361694
Epoch 1820, training loss: 0.06671242415904999 = 0.0010992964962497354 + 0.01 * 6.561312675476074
Epoch 1820, val loss: 1.4607268571853638
Epoch 1830, training loss: 0.06698949635028839 = 0.001092987135052681 + 0.01 * 6.589651584625244
Epoch 1830, val loss: 1.4616798162460327
Epoch 1840, training loss: 0.06665515154600143 = 0.0010866600787267089 + 0.01 * 6.556849479675293
Epoch 1840, val loss: 1.4626215696334839
Epoch 1850, training loss: 0.06658805906772614 = 0.0010804834309965372 + 0.01 * 6.550757884979248
Epoch 1850, val loss: 1.4635676145553589
Epoch 1860, training loss: 0.06689459085464478 = 0.0010743685998022556 + 0.01 * 6.582022666931152
Epoch 1860, val loss: 1.4645057916641235
Epoch 1870, training loss: 0.06659144163131714 = 0.0010685144225135446 + 0.01 * 6.552292823791504
Epoch 1870, val loss: 1.4653853178024292
Epoch 1880, training loss: 0.06648328900337219 = 0.0010626203147694468 + 0.01 * 6.542067050933838
Epoch 1880, val loss: 1.4663090705871582
Epoch 1890, training loss: 0.06658685207366943 = 0.0010569202713668346 + 0.01 * 6.5529937744140625
Epoch 1890, val loss: 1.4672144651412964
Epoch 1900, training loss: 0.06652642041444778 = 0.0010511779692023993 + 0.01 * 6.5475239753723145
Epoch 1900, val loss: 1.4681017398834229
Epoch 1910, training loss: 0.06673146039247513 = 0.0010456452146172523 + 0.01 * 6.568581581115723
Epoch 1910, val loss: 1.468978762626648
Epoch 1920, training loss: 0.0663725733757019 = 0.0010401455219835043 + 0.01 * 6.533243179321289
Epoch 1920, val loss: 1.469875693321228
Epoch 1930, training loss: 0.06669007241725922 = 0.001034654094837606 + 0.01 * 6.565542221069336
Epoch 1930, val loss: 1.470754861831665
Epoch 1940, training loss: 0.06659271568059921 = 0.0010294292587786913 + 0.01 * 6.556328773498535
Epoch 1940, val loss: 1.4716227054595947
Epoch 1950, training loss: 0.0663800910115242 = 0.001024029334075749 + 0.01 * 6.535606384277344
Epoch 1950, val loss: 1.472493052482605
Epoch 1960, training loss: 0.06650872528553009 = 0.0010189964668825269 + 0.01 * 6.548973560333252
Epoch 1960, val loss: 1.473310112953186
Epoch 1970, training loss: 0.06632623821496964 = 0.0010138351935893297 + 0.01 * 6.531239986419678
Epoch 1970, val loss: 1.4742008447647095
Epoch 1980, training loss: 0.06642936170101166 = 0.001008807448670268 + 0.01 * 6.542055606842041
Epoch 1980, val loss: 1.4750680923461914
Epoch 1990, training loss: 0.06627680361270905 = 0.0010038751643151045 + 0.01 * 6.5272932052612305
Epoch 1990, val loss: 1.4759010076522827
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8397469688982605
The final CL Acc:0.80123, 0.01720, The final GNN Acc:0.83623, 0.00302
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9438])
updated graph: torch.Size([2, 10504])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.04402232170105 = 1.9580538272857666 + 0.01 * 8.59685230255127
Epoch 0, val loss: 1.9602715969085693
Epoch 10, training loss: 2.033712863922119 = 1.9477450847625732 + 0.01 * 8.596786499023438
Epoch 10, val loss: 1.9506553411483765
Epoch 20, training loss: 2.0211873054504395 = 1.9352219104766846 + 0.01 * 8.59654712677002
Epoch 20, val loss: 1.9385509490966797
Epoch 30, training loss: 2.003664255142212 = 1.9177076816558838 + 0.01 * 8.595659255981445
Epoch 30, val loss: 1.921217441558838
Epoch 40, training loss: 1.9775770902633667 = 1.8916752338409424 + 0.01 * 8.590188980102539
Epoch 40, val loss: 1.895560622215271
Epoch 50, training loss: 1.9404618740081787 = 1.8549003601074219 + 0.01 * 8.556146621704102
Epoch 50, val loss: 1.8609912395477295
Epoch 60, training loss: 1.8980040550231934 = 1.8139293193817139 + 0.01 * 8.407475471496582
Epoch 60, val loss: 1.826641321182251
Epoch 70, training loss: 1.8648078441619873 = 1.7822256088256836 + 0.01 * 8.25822925567627
Epoch 70, val loss: 1.80208158493042
Epoch 80, training loss: 1.8287794589996338 = 1.747238278388977 + 0.01 * 8.154120445251465
Epoch 80, val loss: 1.7692257165908813
Epoch 90, training loss: 1.7794580459594727 = 1.6994014978408813 + 0.01 * 8.005651473999023
Epoch 90, val loss: 1.7273881435394287
Epoch 100, training loss: 1.7120301723480225 = 1.633910059928894 + 0.01 * 7.812015056610107
Epoch 100, val loss: 1.675317406654358
Epoch 110, training loss: 1.6282309293746948 = 1.5522942543029785 + 0.01 * 7.593671798706055
Epoch 110, val loss: 1.6118892431259155
Epoch 120, training loss: 1.5372779369354248 = 1.4627443552017212 + 0.01 * 7.453361988067627
Epoch 120, val loss: 1.5442479848861694
Epoch 130, training loss: 1.4468971490859985 = 1.3734705448150635 + 0.01 * 7.342665672302246
Epoch 130, val loss: 1.4780117273330688
Epoch 140, training loss: 1.3585293292999268 = 1.2858664989471436 + 0.01 * 7.266282558441162
Epoch 140, val loss: 1.413869857788086
Epoch 150, training loss: 1.269946813583374 = 1.197774052619934 + 0.01 * 7.217281341552734
Epoch 150, val loss: 1.3506736755371094
Epoch 160, training loss: 1.1803102493286133 = 1.1084439754486084 + 0.01 * 7.186622142791748
Epoch 160, val loss: 1.286881923675537
Epoch 170, training loss: 1.0905985832214355 = 1.0188913345336914 + 0.01 * 7.170731067657471
Epoch 170, val loss: 1.2221044301986694
Epoch 180, training loss: 1.0034900903701782 = 0.9318516254425049 + 0.01 * 7.163851737976074
Epoch 180, val loss: 1.1586593389511108
Epoch 190, training loss: 0.9220096468925476 = 0.850423276424408 + 0.01 * 7.158636569976807
Epoch 190, val loss: 1.099585771560669
Epoch 200, training loss: 0.8487011790275574 = 0.7771549820899963 + 0.01 * 7.154617786407471
Epoch 200, val loss: 1.0480424165725708
Epoch 210, training loss: 0.7840797901153564 = 0.7125496864318848 + 0.01 * 7.153012752532959
Epoch 210, val loss: 1.0057897567749023
Epoch 220, training loss: 0.726544201374054 = 0.6550184488296509 + 0.01 * 7.15257453918457
Epoch 220, val loss: 0.9726831316947937
Epoch 230, training loss: 0.6734585165977478 = 0.6019315719604492 + 0.01 * 7.1526970863342285
Epoch 230, val loss: 0.9468868970870972
Epoch 240, training loss: 0.6224907040596008 = 0.5509606599807739 + 0.01 * 7.153005123138428
Epoch 240, val loss: 0.9257722496986389
Epoch 250, training loss: 0.5725694894790649 = 0.5010331273078918 + 0.01 * 7.153634548187256
Epoch 250, val loss: 0.907835066318512
Epoch 260, training loss: 0.523896336555481 = 0.45234593749046326 + 0.01 * 7.155040264129639
Epoch 260, val loss: 0.8933002948760986
Epoch 270, training loss: 0.47734445333480835 = 0.40579545497894287 + 0.01 * 7.154899597167969
Epoch 270, val loss: 0.8832331895828247
Epoch 280, training loss: 0.43377864360809326 = 0.3622293174266815 + 0.01 * 7.154931545257568
Epoch 280, val loss: 0.878537654876709
Epoch 290, training loss: 0.3936362564563751 = 0.3220854103565216 + 0.01 * 7.155084133148193
Epoch 290, val loss: 0.8792963624000549
Epoch 300, training loss: 0.3569852411746979 = 0.28543466329574585 + 0.01 * 7.155056953430176
Epoch 300, val loss: 0.8849150538444519
Epoch 310, training loss: 0.32371786236763 = 0.2521688640117645 + 0.01 * 7.154901027679443
Epoch 310, val loss: 0.894592821598053
Epoch 320, training loss: 0.29365235567092896 = 0.22209720313549042 + 0.01 * 7.155517101287842
Epoch 320, val loss: 0.9076143503189087
Epoch 330, training loss: 0.26657748222351074 = 0.1950315237045288 + 0.01 * 7.154595851898193
Epoch 330, val loss: 0.9233197569847107
Epoch 340, training loss: 0.2423408329486847 = 0.17080165445804596 + 0.01 * 7.153918743133545
Epoch 340, val loss: 0.9412626028060913
Epoch 350, training loss: 0.22078928351402283 = 0.14925949275493622 + 0.01 * 7.152979850769043
Epoch 350, val loss: 0.9611740708351135
Epoch 360, training loss: 0.20177620649337769 = 0.1302456259727478 + 0.01 * 7.153059005737305
Epoch 360, val loss: 0.9827023148536682
Epoch 370, training loss: 0.1851176619529724 = 0.11358147114515305 + 0.01 * 7.153619289398193
Epoch 370, val loss: 1.0053654909133911
Epoch 380, training loss: 0.17059600353240967 = 0.09909505397081375 + 0.01 * 7.150094985961914
Epoch 380, val loss: 1.0288200378417969
Epoch 390, training loss: 0.15806913375854492 = 0.08658367395401001 + 0.01 * 7.148545742034912
Epoch 390, val loss: 1.0528991222381592
Epoch 400, training loss: 0.1472925841808319 = 0.07583059370517731 + 0.01 * 7.146200180053711
Epoch 400, val loss: 1.0772721767425537
Epoch 410, training loss: 0.1380736529827118 = 0.06662081927061081 + 0.01 * 7.145283222198486
Epoch 410, val loss: 1.101731777191162
Epoch 420, training loss: 0.13019132614135742 = 0.05875476077198982 + 0.01 * 7.1436567306518555
Epoch 420, val loss: 1.1260625123977661
Epoch 430, training loss: 0.12343738973140717 = 0.0520368255674839 + 0.01 * 7.14005708694458
Epoch 430, val loss: 1.1501044034957886
Epoch 440, training loss: 0.11764377355575562 = 0.04629100486636162 + 0.01 * 7.13527774810791
Epoch 440, val loss: 1.1736860275268555
Epoch 450, training loss: 0.1128469705581665 = 0.04136732965707779 + 0.01 * 7.147964000701904
Epoch 450, val loss: 1.1966809034347534
Epoch 460, training loss: 0.10844291746616364 = 0.03713801130652428 + 0.01 * 7.130491256713867
Epoch 460, val loss: 1.2190172672271729
Epoch 470, training loss: 0.10474468767642975 = 0.033491674810647964 + 0.01 * 7.125301361083984
Epoch 470, val loss: 1.2406319379806519
Epoch 480, training loss: 0.10152607411146164 = 0.03033408522605896 + 0.01 * 7.119198799133301
Epoch 480, val loss: 1.2615407705307007
Epoch 490, training loss: 0.09888417273759842 = 0.027588827535510063 + 0.01 * 7.1295342445373535
Epoch 490, val loss: 1.2816808223724365
Epoch 500, training loss: 0.09630683809518814 = 0.02519480139017105 + 0.01 * 7.111204147338867
Epoch 500, val loss: 1.3010283708572388
Epoch 510, training loss: 0.0941423773765564 = 0.023096688091754913 + 0.01 * 7.104569435119629
Epoch 510, val loss: 1.319626808166504
Epoch 520, training loss: 0.09236128628253937 = 0.02124953828752041 + 0.01 * 7.111175060272217
Epoch 520, val loss: 1.3375097513198853
Epoch 530, training loss: 0.09053771942853928 = 0.019617853686213493 + 0.01 * 7.091986656188965
Epoch 530, val loss: 1.354708194732666
Epoch 540, training loss: 0.08906412124633789 = 0.018170153722167015 + 0.01 * 7.0893964767456055
Epoch 540, val loss: 1.3712598085403442
Epoch 550, training loss: 0.087745301425457 = 0.01688181608915329 + 0.01 * 7.086348533630371
Epoch 550, val loss: 1.3871244192123413
Epoch 560, training loss: 0.0865401104092598 = 0.01573111116886139 + 0.01 * 7.080900192260742
Epoch 560, val loss: 1.4023958444595337
Epoch 570, training loss: 0.08535565435886383 = 0.014699743129312992 + 0.01 * 7.065591335296631
Epoch 570, val loss: 1.4170558452606201
Epoch 580, training loss: 0.08422422409057617 = 0.013771658763289452 + 0.01 * 7.045256614685059
Epoch 580, val loss: 1.4311765432357788
Epoch 590, training loss: 0.08364951610565186 = 0.012934398837387562 + 0.01 * 7.071511268615723
Epoch 590, val loss: 1.4447953701019287
Epoch 600, training loss: 0.08266578614711761 = 0.012178471311926842 + 0.01 * 7.048731327056885
Epoch 600, val loss: 1.4578181505203247
Epoch 610, training loss: 0.08174256235361099 = 0.011491455137729645 + 0.01 * 7.025111198425293
Epoch 610, val loss: 1.4703903198242188
Epoch 620, training loss: 0.08107030391693115 = 0.010865588672459126 + 0.01 * 7.020471572875977
Epoch 620, val loss: 1.4825142621994019
Epoch 630, training loss: 0.08051192760467529 = 0.010294247418642044 + 0.01 * 7.021768569946289
Epoch 630, val loss: 1.4941514730453491
Epoch 640, training loss: 0.07993029803037643 = 0.009770979173481464 + 0.01 * 7.015931606292725
Epoch 640, val loss: 1.5054675340652466
Epoch 650, training loss: 0.07912181317806244 = 0.009290475398302078 + 0.01 * 6.9831342697143555
Epoch 650, val loss: 1.516325831413269
Epoch 660, training loss: 0.07899637520313263 = 0.008848395198583603 + 0.01 * 7.014798164367676
Epoch 660, val loss: 1.526862382888794
Epoch 670, training loss: 0.07811450958251953 = 0.008441692218184471 + 0.01 * 6.967281341552734
Epoch 670, val loss: 1.53695547580719
Epoch 680, training loss: 0.07766884565353394 = 0.008065872825682163 + 0.01 * 6.960297584533691
Epoch 680, val loss: 1.5468021631240845
Epoch 690, training loss: 0.07750452309846878 = 0.00771741010248661 + 0.01 * 6.9787116050720215
Epoch 690, val loss: 1.5563743114471436
Epoch 700, training loss: 0.07707170397043228 = 0.007393883541226387 + 0.01 * 6.967782020568848
Epoch 700, val loss: 1.5655033588409424
Epoch 710, training loss: 0.07660292088985443 = 0.0070928046479821205 + 0.01 * 6.951012134552002
Epoch 710, val loss: 1.5745142698287964
Epoch 720, training loss: 0.0761425793170929 = 0.006812187377363443 + 0.01 * 6.933039665222168
Epoch 720, val loss: 1.5831822156906128
Epoch 730, training loss: 0.07605040073394775 = 0.006549835205078125 + 0.01 * 6.950056552886963
Epoch 730, val loss: 1.5916543006896973
Epoch 740, training loss: 0.07549963146448135 = 0.006304784677922726 + 0.01 * 6.919484615325928
Epoch 740, val loss: 1.599800944328308
Epoch 750, training loss: 0.07555781304836273 = 0.006075385957956314 + 0.01 * 6.9482421875
Epoch 750, val loss: 1.6077903509140015
Epoch 760, training loss: 0.07502131164073944 = 0.005860059522092342 + 0.01 * 6.916125297546387
Epoch 760, val loss: 1.6154471635818481
Epoch 770, training loss: 0.07469373941421509 = 0.00565785588696599 + 0.01 * 6.903587818145752
Epoch 770, val loss: 1.6229780912399292
Epoch 780, training loss: 0.07455537468194962 = 0.005467319395393133 + 0.01 * 6.908805847167969
Epoch 780, val loss: 1.6303261518478394
Epoch 790, training loss: 0.07437407970428467 = 0.005287939682602882 + 0.01 * 6.908614158630371
Epoch 790, val loss: 1.6374566555023193
Epoch 800, training loss: 0.07425200939178467 = 0.005118728615343571 + 0.01 * 6.913328647613525
Epoch 800, val loss: 1.6443907022476196
Epoch 810, training loss: 0.07389267534017563 = 0.004958896432071924 + 0.01 * 6.893377780914307
Epoch 810, val loss: 1.651154637336731
Epoch 820, training loss: 0.07377037405967712 = 0.004807754419744015 + 0.01 * 6.896262168884277
Epoch 820, val loss: 1.6577428579330444
Epoch 830, training loss: 0.07348205894231796 = 0.004665030166506767 + 0.01 * 6.881702899932861
Epoch 830, val loss: 1.6641416549682617
Epoch 840, training loss: 0.07334832847118378 = 0.004529454745352268 + 0.01 * 6.881887912750244
Epoch 840, val loss: 1.6704192161560059
Epoch 850, training loss: 0.07325667887926102 = 0.00440116785466671 + 0.01 * 6.8855509757995605
Epoch 850, val loss: 1.676478385925293
Epoch 860, training loss: 0.07302356511354446 = 0.00427898159250617 + 0.01 * 6.8744587898254395
Epoch 860, val loss: 1.6824556589126587
Epoch 870, training loss: 0.07282852381467819 = 0.004162993747740984 + 0.01 * 6.866552829742432
Epoch 870, val loss: 1.688259243965149
Epoch 880, training loss: 0.07266127318143845 = 0.004052519798278809 + 0.01 * 6.860875129699707
Epoch 880, val loss: 1.693966269493103
Epoch 890, training loss: 0.07239879667758942 = 0.003947739023715258 + 0.01 * 6.8451056480407715
Epoch 890, val loss: 1.6994786262512207
Epoch 900, training loss: 0.07251089066267014 = 0.003847430692985654 + 0.01 * 6.86634635925293
Epoch 900, val loss: 1.7049075365066528
Epoch 910, training loss: 0.07232971489429474 = 0.0037518367171287537 + 0.01 * 6.857787609100342
Epoch 910, val loss: 1.7102046012878418
Epoch 920, training loss: 0.07213281840085983 = 0.0036607948131859303 + 0.01 * 6.847201824188232
Epoch 920, val loss: 1.7153371572494507
Epoch 930, training loss: 0.07213455438613892 = 0.0035740546882152557 + 0.01 * 6.85605001449585
Epoch 930, val loss: 1.7203762531280518
Epoch 940, training loss: 0.07192187011241913 = 0.003490590490400791 + 0.01 * 6.843128204345703
Epoch 940, val loss: 1.7252949476242065
Epoch 950, training loss: 0.07200782746076584 = 0.0034109088592231274 + 0.01 * 6.859692096710205
Epoch 950, val loss: 1.7302409410476685
Epoch 960, training loss: 0.0716141015291214 = 0.003334945300593972 + 0.01 * 6.827915668487549
Epoch 960, val loss: 1.7349069118499756
Epoch 970, training loss: 0.07154079526662827 = 0.003262059763073921 + 0.01 * 6.827873706817627
Epoch 970, val loss: 1.739489197731018
Epoch 980, training loss: 0.07134908437728882 = 0.0031921330373734236 + 0.01 * 6.815694808959961
Epoch 980, val loss: 1.7440840005874634
Epoch 990, training loss: 0.07141051441431046 = 0.0031252012122422457 + 0.01 * 6.828531742095947
Epoch 990, val loss: 1.7484052181243896
Epoch 1000, training loss: 0.07117635756731033 = 0.0030606656800955534 + 0.01 * 6.811568737030029
Epoch 1000, val loss: 1.7528380155563354
Epoch 1010, training loss: 0.07119959592819214 = 0.002998633775860071 + 0.01 * 6.820096492767334
Epoch 1010, val loss: 1.757102608680725
Epoch 1020, training loss: 0.0711674839258194 = 0.0029392384458333254 + 0.01 * 6.822824478149414
Epoch 1020, val loss: 1.761231541633606
Epoch 1030, training loss: 0.07086958736181259 = 0.00288211228325963 + 0.01 * 6.7987470626831055
Epoch 1030, val loss: 1.765297770500183
Epoch 1040, training loss: 0.07105103135108948 = 0.002827371470630169 + 0.01 * 6.822366237640381
Epoch 1040, val loss: 1.7693208456039429
Epoch 1050, training loss: 0.07096337527036667 = 0.0027742162346839905 + 0.01 * 6.818915843963623
Epoch 1050, val loss: 1.7732053995132446
Epoch 1060, training loss: 0.07062084227800369 = 0.0027235273737460375 + 0.01 * 6.789731025695801
Epoch 1060, val loss: 1.7770309448242188
Epoch 1070, training loss: 0.07060611248016357 = 0.0026741912588477135 + 0.01 * 6.793192386627197
Epoch 1070, val loss: 1.7808494567871094
Epoch 1080, training loss: 0.07049473375082016 = 0.0026266302447766066 + 0.01 * 6.786810874938965
Epoch 1080, val loss: 1.7844700813293457
Epoch 1090, training loss: 0.07046910375356674 = 0.0025810066144913435 + 0.01 * 6.788809776306152
Epoch 1090, val loss: 1.7881546020507812
Epoch 1100, training loss: 0.07055214792490005 = 0.0025367988273501396 + 0.01 * 6.801535606384277
Epoch 1100, val loss: 1.7916160821914673
Epoch 1110, training loss: 0.07022756338119507 = 0.0024945097975432873 + 0.01 * 6.773305892944336
Epoch 1110, val loss: 1.7950739860534668
Epoch 1120, training loss: 0.0704311728477478 = 0.0024533038958907127 + 0.01 * 6.797786712646484
Epoch 1120, val loss: 1.7984498739242554
Epoch 1130, training loss: 0.07015828043222427 = 0.0024134537670761347 + 0.01 * 6.7744832038879395
Epoch 1130, val loss: 1.8017938137054443
Epoch 1140, training loss: 0.07004491984844208 = 0.002375128911808133 + 0.01 * 6.766979217529297
Epoch 1140, val loss: 1.805054783821106
Epoch 1150, training loss: 0.07016293704509735 = 0.002337871352210641 + 0.01 * 6.782506465911865
Epoch 1150, val loss: 1.808295726776123
Epoch 1160, training loss: 0.07024237513542175 = 0.0023018254432827234 + 0.01 * 6.794054985046387
Epoch 1160, val loss: 1.8115026950836182
Epoch 1170, training loss: 0.07003709673881531 = 0.0022669476456940174 + 0.01 * 6.77701473236084
Epoch 1170, val loss: 1.8145509958267212
Epoch 1180, training loss: 0.06996849179267883 = 0.0022330263163894415 + 0.01 * 6.77354621887207
Epoch 1180, val loss: 1.8176500797271729
Epoch 1190, training loss: 0.0697273463010788 = 0.002200076123699546 + 0.01 * 6.752727031707764
Epoch 1190, val loss: 1.8206034898757935
Epoch 1200, training loss: 0.06984111666679382 = 0.002168242586776614 + 0.01 * 6.767287254333496
Epoch 1200, val loss: 1.8236215114593506
Epoch 1210, training loss: 0.06959816068410873 = 0.0021374779753386974 + 0.01 * 6.746068954467773
Epoch 1210, val loss: 1.8265055418014526
Epoch 1220, training loss: 0.06950544565916061 = 0.002107658190652728 + 0.01 * 6.739778995513916
Epoch 1220, val loss: 1.829319953918457
Epoch 1230, training loss: 0.06987237930297852 = 0.002078649355098605 + 0.01 * 6.779372692108154
Epoch 1230, val loss: 1.832140564918518
Epoch 1240, training loss: 0.0696694478392601 = 0.0020505678839981556 + 0.01 * 6.761887550354004
Epoch 1240, val loss: 1.8348792791366577
Epoch 1250, training loss: 0.06950851529836655 = 0.002023429609835148 + 0.01 * 6.748508930206299
Epoch 1250, val loss: 1.837475299835205
Epoch 1260, training loss: 0.06933298707008362 = 0.001996872015297413 + 0.01 * 6.733611583709717
Epoch 1260, val loss: 1.840254783630371
Epoch 1270, training loss: 0.06950546056032181 = 0.0019712341018021107 + 0.01 * 6.753422737121582
Epoch 1270, val loss: 1.8427313566207886
Epoch 1280, training loss: 0.06940826028585434 = 0.0019462654599919915 + 0.01 * 6.746199607849121
Epoch 1280, val loss: 1.8453222513198853
Epoch 1290, training loss: 0.06939959526062012 = 0.0019220091635361314 + 0.01 * 6.747758388519287
Epoch 1290, val loss: 1.847790241241455
Epoch 1300, training loss: 0.06930588185787201 = 0.0018983640475198627 + 0.01 * 6.740752220153809
Epoch 1300, val loss: 1.8502191305160522
Epoch 1310, training loss: 0.06934484839439392 = 0.0018754353513941169 + 0.01 * 6.746941089630127
Epoch 1310, val loss: 1.8527363538742065
Epoch 1320, training loss: 0.06931215524673462 = 0.0018531634705141187 + 0.01 * 6.745899677276611
Epoch 1320, val loss: 1.8549401760101318
Epoch 1330, training loss: 0.06919456273317337 = 0.0018314196495339274 + 0.01 * 6.736314296722412
Epoch 1330, val loss: 1.8573654890060425
Epoch 1340, training loss: 0.0693630576133728 = 0.0018103061011061072 + 0.01 * 6.755275249481201
Epoch 1340, val loss: 1.859573245048523
Epoch 1350, training loss: 0.06889115273952484 = 0.0017897264333441854 + 0.01 * 6.710142612457275
Epoch 1350, val loss: 1.8618870973587036
Epoch 1360, training loss: 0.06892584264278412 = 0.0017696644645184278 + 0.01 * 6.715618133544922
Epoch 1360, val loss: 1.8641102313995361
Epoch 1370, training loss: 0.06939250230789185 = 0.00175014219712466 + 0.01 * 6.764235973358154
Epoch 1370, val loss: 1.8662781715393066
Epoch 1380, training loss: 0.06896595656871796 = 0.0017311807023361325 + 0.01 * 6.723477840423584
Epoch 1380, val loss: 1.8684149980545044
Epoch 1390, training loss: 0.06886520236730576 = 0.0017126576276496053 + 0.01 * 6.715254783630371
Epoch 1390, val loss: 1.870532512664795
Epoch 1400, training loss: 0.06892917305231094 = 0.0016946090618148446 + 0.01 * 6.723455905914307
Epoch 1400, val loss: 1.8726708889007568
Epoch 1410, training loss: 0.06889436393976212 = 0.0016771041555330157 + 0.01 * 6.721725940704346
Epoch 1410, val loss: 1.8745704889297485
Epoch 1420, training loss: 0.06879612058401108 = 0.001659839996136725 + 0.01 * 6.713627815246582
Epoch 1420, val loss: 1.876697063446045
Epoch 1430, training loss: 0.06879014521837234 = 0.0016431453404948115 + 0.01 * 6.714700222015381
Epoch 1430, val loss: 1.8786139488220215
Epoch 1440, training loss: 0.06853855401277542 = 0.001626742770895362 + 0.01 * 6.691181182861328
Epoch 1440, val loss: 1.8806638717651367
Epoch 1450, training loss: 0.06881154328584671 = 0.0016108294948935509 + 0.01 * 6.720071315765381
Epoch 1450, val loss: 1.882485270500183
Epoch 1460, training loss: 0.06861311197280884 = 0.0015952649991959333 + 0.01 * 6.701784610748291
Epoch 1460, val loss: 1.884403944015503
Epoch 1470, training loss: 0.0686912015080452 = 0.0015800916589796543 + 0.01 * 6.711111068725586
Epoch 1470, val loss: 1.8862605094909668
Epoch 1480, training loss: 0.06887344270944595 = 0.0015653107548132539 + 0.01 * 6.730813503265381
Epoch 1480, val loss: 1.8879910707473755
Epoch 1490, training loss: 0.06837069988250732 = 0.0015507973730564117 + 0.01 * 6.681989669799805
Epoch 1490, val loss: 1.8898497819900513
Epoch 1500, training loss: 0.06872735917568207 = 0.0015366827137768269 + 0.01 * 6.7190680503845215
Epoch 1500, val loss: 1.8915680646896362
Epoch 1510, training loss: 0.0682622417807579 = 0.0015228776028379798 + 0.01 * 6.673936367034912
Epoch 1510, val loss: 1.8933227062225342
Epoch 1520, training loss: 0.06837528944015503 = 0.0015094063710421324 + 0.01 * 6.686588764190674
Epoch 1520, val loss: 1.894992709159851
Epoch 1530, training loss: 0.06853204220533371 = 0.0014961773995310068 + 0.01 * 6.703586578369141
Epoch 1530, val loss: 1.8966407775878906
Epoch 1540, training loss: 0.06821557134389877 = 0.0014832265442237258 + 0.01 * 6.673234939575195
Epoch 1540, val loss: 1.8983474969863892
Epoch 1550, training loss: 0.06824023276567459 = 0.001470669754780829 + 0.01 * 6.676956653594971
Epoch 1550, val loss: 1.8999963998794556
Epoch 1560, training loss: 0.06865588575601578 = 0.0014583576703444123 + 0.01 * 6.719753265380859
Epoch 1560, val loss: 1.9014723300933838
Epoch 1570, training loss: 0.06824212521314621 = 0.0014462833059951663 + 0.01 * 6.67958402633667
Epoch 1570, val loss: 1.9030976295471191
Epoch 1580, training loss: 0.06798209995031357 = 0.0014344468945637345 + 0.01 * 6.654765605926514
Epoch 1580, val loss: 1.9046063423156738
Epoch 1590, training loss: 0.0686064213514328 = 0.0014228979125618935 + 0.01 * 6.718352317810059
Epoch 1590, val loss: 1.906153678894043
Epoch 1600, training loss: 0.06821732223033905 = 0.00141158199403435 + 0.01 * 6.680574417114258
Epoch 1600, val loss: 1.9076176881790161
Epoch 1610, training loss: 0.0681588277220726 = 0.0014004840049892664 + 0.01 * 6.675834655761719
Epoch 1610, val loss: 1.9091846942901611
Epoch 1620, training loss: 0.06799880415201187 = 0.0013896181480959058 + 0.01 * 6.660919189453125
Epoch 1620, val loss: 1.9105968475341797
Epoch 1630, training loss: 0.06787323951721191 = 0.0013789860531687737 + 0.01 * 6.649425506591797
Epoch 1630, val loss: 1.9120713472366333
Epoch 1640, training loss: 0.06864362955093384 = 0.0013686370803043246 + 0.01 * 6.727499961853027
Epoch 1640, val loss: 1.9133777618408203
Epoch 1650, training loss: 0.06780001521110535 = 0.0013583634281530976 + 0.01 * 6.644165515899658
Epoch 1650, val loss: 1.9149253368377686
Epoch 1660, training loss: 0.06816926598548889 = 0.0013483762741088867 + 0.01 * 6.682089328765869
Epoch 1660, val loss: 1.916212797164917
Epoch 1670, training loss: 0.06798505783081055 = 0.0013386165956035256 + 0.01 * 6.664644241333008
Epoch 1670, val loss: 1.917572259902954
Epoch 1680, training loss: 0.06775198131799698 = 0.001328983693383634 + 0.01 * 6.642300128936768
Epoch 1680, val loss: 1.9188766479492188
Epoch 1690, training loss: 0.06776325404644012 = 0.0013195299543440342 + 0.01 * 6.644372940063477
Epoch 1690, val loss: 1.92017662525177
Epoch 1700, training loss: 0.06768113374710083 = 0.0013102731900289655 + 0.01 * 6.637086391448975
Epoch 1700, val loss: 1.921553134918213
Epoch 1710, training loss: 0.06769296526908875 = 0.001301184413023293 + 0.01 * 6.639178276062012
Epoch 1710, val loss: 1.9227254390716553
Epoch 1720, training loss: 0.0678059533238411 = 0.0012922472087666392 + 0.01 * 6.651371479034424
Epoch 1720, val loss: 1.9241842031478882
Epoch 1730, training loss: 0.06759250909090042 = 0.0012835327070206404 + 0.01 * 6.6308979988098145
Epoch 1730, val loss: 1.925252079963684
Epoch 1740, training loss: 0.06757807731628418 = 0.001274928799830377 + 0.01 * 6.63031530380249
Epoch 1740, val loss: 1.9265971183776855
Epoch 1750, training loss: 0.0678631067276001 = 0.001266503008082509 + 0.01 * 6.659660816192627
Epoch 1750, val loss: 1.92770254611969
Epoch 1760, training loss: 0.06749318540096283 = 0.001258241361938417 + 0.01 * 6.623494625091553
Epoch 1760, val loss: 1.9288551807403564
Epoch 1770, training loss: 0.06783013790845871 = 0.0012500900775194168 + 0.01 * 6.658005237579346
Epoch 1770, val loss: 1.9300342798233032
Epoch 1780, training loss: 0.06746876984834671 = 0.001242084545083344 + 0.01 * 6.622668743133545
Epoch 1780, val loss: 1.9311765432357788
Epoch 1790, training loss: 0.06750010699033737 = 0.0012341588735580444 + 0.01 * 6.6265950202941895
Epoch 1790, val loss: 1.9324496984481812
Epoch 1800, training loss: 0.06743336468935013 = 0.0012264758115634322 + 0.01 * 6.6206889152526855
Epoch 1800, val loss: 1.9334676265716553
Epoch 1810, training loss: 0.0673530325293541 = 0.0012188262771815062 + 0.01 * 6.613420486450195
Epoch 1810, val loss: 1.9346394538879395
Epoch 1820, training loss: 0.0674276053905487 = 0.0012113872217014432 + 0.01 * 6.621621608734131
Epoch 1820, val loss: 1.9356858730316162
Epoch 1830, training loss: 0.06724710017442703 = 0.0012039991561323404 + 0.01 * 6.604310512542725
Epoch 1830, val loss: 1.9368098974227905
Epoch 1840, training loss: 0.06750428676605225 = 0.001196791068650782 + 0.01 * 6.630749702453613
Epoch 1840, val loss: 1.9378339052200317
Epoch 1850, training loss: 0.06722117215394974 = 0.0011896404903382063 + 0.01 * 6.603153228759766
Epoch 1850, val loss: 1.9389512538909912
Epoch 1860, training loss: 0.06719167530536652 = 0.001182627398520708 + 0.01 * 6.60090446472168
Epoch 1860, val loss: 1.9399971961975098
Epoch 1870, training loss: 0.06723184138536453 = 0.0011756974272429943 + 0.01 * 6.605615139007568
Epoch 1870, val loss: 1.941056489944458
Epoch 1880, training loss: 0.06705312430858612 = 0.0011688830563798547 + 0.01 * 6.588424205780029
Epoch 1880, val loss: 1.9421063661575317
Epoch 1890, training loss: 0.06714756041765213 = 0.0011622215388342738 + 0.01 * 6.598533630371094
Epoch 1890, val loss: 1.943033218383789
Epoch 1900, training loss: 0.06710226833820343 = 0.0011556256795302033 + 0.01 * 6.594664573669434
Epoch 1900, val loss: 1.9440133571624756
Epoch 1910, training loss: 0.06706858426332474 = 0.0011491128243505955 + 0.01 * 6.591947555541992
Epoch 1910, val loss: 1.9450668096542358
Epoch 1920, training loss: 0.06703893095254898 = 0.0011427089339122176 + 0.01 * 6.5896220207214355
Epoch 1920, val loss: 1.9459553956985474
Epoch 1930, training loss: 0.06713021546602249 = 0.001136397710070014 + 0.01 * 6.599381446838379
Epoch 1930, val loss: 1.94693124294281
Epoch 1940, training loss: 0.06713782995939255 = 0.0011301689082756639 + 0.01 * 6.600766181945801
Epoch 1940, val loss: 1.9479331970214844
Epoch 1950, training loss: 0.06691095232963562 = 0.0011240163585171103 + 0.01 * 6.578693389892578
Epoch 1950, val loss: 1.9488184452056885
Epoch 1960, training loss: 0.067196324467659 = 0.0011179550783708692 + 0.01 * 6.607837677001953
Epoch 1960, val loss: 1.9497305154800415
Epoch 1970, training loss: 0.06685441732406616 = 0.001111961668357253 + 0.01 * 6.574245929718018
Epoch 1970, val loss: 1.9507466554641724
Epoch 1980, training loss: 0.06703206896781921 = 0.0011060864198952913 + 0.01 * 6.592598915100098
Epoch 1980, val loss: 1.9514942169189453
Epoch 1990, training loss: 0.06683748960494995 = 0.0011002760147675872 + 0.01 * 6.573720932006836
Epoch 1990, val loss: 1.9524568319320679
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.7986294148655773
=== training gcn model ===
Epoch 0, training loss: 2.0455682277679443 = 1.9595999717712402 + 0.01 * 8.596834182739258
Epoch 0, val loss: 1.9621647596359253
Epoch 10, training loss: 2.0347952842712402 = 1.9488277435302734 + 0.01 * 8.596742630004883
Epoch 10, val loss: 1.9518922567367554
Epoch 20, training loss: 2.020855188369751 = 1.9348913431167603 + 0.01 * 8.596390724182129
Epoch 20, val loss: 1.938520908355713
Epoch 30, training loss: 2.000553607940674 = 1.9146074056625366 + 0.01 * 8.594620704650879
Epoch 30, val loss: 1.9191781282424927
Epoch 40, training loss: 1.9705175161361694 = 1.8847055435180664 + 0.01 * 8.581192016601562
Epoch 40, val loss: 1.8915635347366333
Epoch 50, training loss: 1.9301586151123047 = 1.8449921607971191 + 0.01 * 8.516639709472656
Epoch 50, val loss: 1.857365369796753
Epoch 60, training loss: 1.8888994455337524 = 1.8058803081512451 + 0.01 * 8.301908493041992
Epoch 60, val loss: 1.827446699142456
Epoch 70, training loss: 1.8577525615692139 = 1.775320053100586 + 0.01 * 8.24325180053711
Epoch 70, val loss: 1.8018865585327148
Epoch 80, training loss: 1.8184399604797363 = 1.7371500730514526 + 0.01 * 8.128988265991211
Epoch 80, val loss: 1.766183614730835
Epoch 90, training loss: 1.7638527154922485 = 1.684570550918579 + 0.01 * 7.928221702575684
Epoch 90, val loss: 1.7221887111663818
Epoch 100, training loss: 1.6886470317840576 = 1.6117790937423706 + 0.01 * 7.686793804168701
Epoch 100, val loss: 1.6648796796798706
Epoch 110, training loss: 1.5982203483581543 = 1.523041009902954 + 0.01 * 7.517934322357178
Epoch 110, val loss: 1.5972299575805664
Epoch 120, training loss: 1.5037044286727905 = 1.4295793771743774 + 0.01 * 7.412508964538574
Epoch 120, val loss: 1.5277706384658813
Epoch 130, training loss: 1.4107115268707275 = 1.337310552597046 + 0.01 * 7.340094566345215
Epoch 130, val loss: 1.45993971824646
Epoch 140, training loss: 1.3193957805633545 = 1.2462153434753418 + 0.01 * 7.318038463592529
Epoch 140, val loss: 1.3931790590286255
Epoch 150, training loss: 1.23043692111969 = 1.1573206186294556 + 0.01 * 7.3116350173950195
Epoch 150, val loss: 1.3285247087478638
Epoch 160, training loss: 1.1466692686080933 = 1.0736052989959717 + 0.01 * 7.306396007537842
Epoch 160, val loss: 1.269167184829712
Epoch 170, training loss: 1.070578694343567 = 0.9975576996803284 + 0.01 * 7.302094459533691
Epoch 170, val loss: 1.217360496520996
Epoch 180, training loss: 1.0011398792266846 = 0.9281601905822754 + 0.01 * 7.297971248626709
Epoch 180, val loss: 1.1716862916946411
Epoch 190, training loss: 0.9354680180549622 = 0.8625307083129883 + 0.01 * 7.293732166290283
Epoch 190, val loss: 1.1287779808044434
Epoch 200, training loss: 0.8717597723007202 = 0.7988793253898621 + 0.01 * 7.288043975830078
Epoch 200, val loss: 1.0870459079742432
Epoch 210, training loss: 0.8104349374771118 = 0.7376331686973572 + 0.01 * 7.280178070068359
Epoch 210, val loss: 1.0472345352172852
Epoch 220, training loss: 0.7530075907707214 = 0.6803123950958252 + 0.01 * 7.269517421722412
Epoch 220, val loss: 1.011849045753479
Epoch 230, training loss: 0.7000651955604553 = 0.6274924874305725 + 0.01 * 7.257272720336914
Epoch 230, val loss: 0.9824635982513428
Epoch 240, training loss: 0.650797963142395 = 0.5783869624137878 + 0.01 * 7.2410969734191895
Epoch 240, val loss: 0.9591543674468994
Epoch 250, training loss: 0.6044813990592957 = 0.5322345495223999 + 0.01 * 7.224684238433838
Epoch 250, val loss: 0.9414266347885132
Epoch 260, training loss: 0.5607879757881165 = 0.4887278974056244 + 0.01 * 7.206008434295654
Epoch 260, val loss: 0.929425060749054
Epoch 270, training loss: 0.5195444822311401 = 0.44762638211250305 + 0.01 * 7.191812038421631
Epoch 270, val loss: 0.9226361513137817
Epoch 280, training loss: 0.4804401993751526 = 0.40865036845207214 + 0.01 * 7.178984642028809
Epoch 280, val loss: 0.9206385612487793
Epoch 290, training loss: 0.4433063864707947 = 0.37159550189971924 + 0.01 * 7.1710896492004395
Epoch 290, val loss: 0.9224255681037903
Epoch 300, training loss: 0.40797755122184753 = 0.3364679515361786 + 0.01 * 7.150960445404053
Epoch 300, val loss: 0.9269155859947205
Epoch 310, training loss: 0.3745407462120056 = 0.3031019866466522 + 0.01 * 7.1438775062561035
Epoch 310, val loss: 0.9328967332839966
Epoch 320, training loss: 0.34245023131370544 = 0.27109941840171814 + 0.01 * 7.135080814361572
Epoch 320, val loss: 0.9394196271896362
Epoch 330, training loss: 0.3111906945705414 = 0.2399492859840393 + 0.01 * 7.124140739440918
Epoch 330, val loss: 0.9460508227348328
Epoch 340, training loss: 0.2808421552181244 = 0.20972958207130432 + 0.01 * 7.111258506774902
Epoch 340, val loss: 0.9526687264442444
Epoch 350, training loss: 0.2522447407245636 = 0.18124572932720184 + 0.01 * 7.099902153015137
Epoch 350, val loss: 0.9599736928939819
Epoch 360, training loss: 0.2266622930765152 = 0.15559834241867065 + 0.01 * 7.106395244598389
Epoch 360, val loss: 0.9685755968093872
Epoch 370, training loss: 0.20428751409053802 = 0.13345664739608765 + 0.01 * 7.0830864906311035
Epoch 370, val loss: 0.9789601564407349
Epoch 380, training loss: 0.1857854574918747 = 0.11477936804294586 + 0.01 * 7.100608825683594
Epoch 380, val loss: 0.9909336566925049
Epoch 390, training loss: 0.16979527473449707 = 0.09918539226055145 + 0.01 * 7.060987949371338
Epoch 390, val loss: 1.0043638944625854
Epoch 400, training loss: 0.1566825956106186 = 0.08614443242549896 + 0.01 * 7.053816795349121
Epoch 400, val loss: 1.0189405679702759
Epoch 410, training loss: 0.14558853209018707 = 0.07520076632499695 + 0.01 * 7.038776874542236
Epoch 410, val loss: 1.0343036651611328
Epoch 420, training loss: 0.13632461428642273 = 0.06598212569952011 + 0.01 * 7.0342488288879395
Epoch 420, val loss: 1.050199031829834
Epoch 430, training loss: 0.12857812643051147 = 0.05819159373641014 + 0.01 * 7.0386528968811035
Epoch 430, val loss: 1.0662767887115479
Epoch 440, training loss: 0.12171438336372375 = 0.05159228295087814 + 0.01 * 7.012210369110107
Epoch 440, val loss: 1.0823390483856201
Epoch 450, training loss: 0.11594399064779282 = 0.04596947133541107 + 0.01 * 6.997452259063721
Epoch 450, val loss: 1.0982463359832764
Epoch 460, training loss: 0.11133792996406555 = 0.04115576297044754 + 0.01 * 7.018217086791992
Epoch 460, val loss: 1.1138240098953247
Epoch 470, training loss: 0.1068917065858841 = 0.03702584654092789 + 0.01 * 6.986586093902588
Epoch 470, val loss: 1.1289921998977661
Epoch 480, training loss: 0.10326626151800156 = 0.03346157819032669 + 0.01 * 6.98046875
Epoch 480, val loss: 1.1436598300933838
Epoch 490, training loss: 0.1000041738152504 = 0.030371608212590218 + 0.01 * 6.9632568359375
Epoch 490, val loss: 1.157893180847168
Epoch 500, training loss: 0.09731759130954742 = 0.027681929990649223 + 0.01 * 6.963566303253174
Epoch 500, val loss: 1.171669602394104
Epoch 510, training loss: 0.09490644931793213 = 0.025328323245048523 + 0.01 * 6.957812786102295
Epoch 510, val loss: 1.1849664449691772
Epoch 520, training loss: 0.0927114263176918 = 0.023258542641997337 + 0.01 * 6.94528865814209
Epoch 520, val loss: 1.197831392288208
Epoch 530, training loss: 0.09103716909885406 = 0.02142765186727047 + 0.01 * 6.960951805114746
Epoch 530, val loss: 1.2102761268615723
Epoch 540, training loss: 0.08924011886119843 = 0.019800713285803795 + 0.01 * 6.94394063949585
Epoch 540, val loss: 1.2222906351089478
Epoch 550, training loss: 0.0878191888332367 = 0.018346646800637245 + 0.01 * 6.947254180908203
Epoch 550, val loss: 1.2339509725570679
Epoch 560, training loss: 0.08630207180976868 = 0.017044752836227417 + 0.01 * 6.925732135772705
Epoch 560, val loss: 1.2452057600021362
Epoch 570, training loss: 0.08503876626491547 = 0.015873201191425323 + 0.01 * 6.916556358337402
Epoch 570, val loss: 1.2560744285583496
Epoch 580, training loss: 0.08406376838684082 = 0.014814927242696285 + 0.01 * 6.924883842468262
Epoch 580, val loss: 1.2666015625
Epoch 590, training loss: 0.08301810175180435 = 0.01385961938649416 + 0.01 * 6.915848255157471
Epoch 590, val loss: 1.276703953742981
Epoch 600, training loss: 0.08203084021806717 = 0.012993703596293926 + 0.01 * 6.903714179992676
Epoch 600, val loss: 1.2865188121795654
Epoch 610, training loss: 0.08132988959550858 = 0.012205586768686771 + 0.01 * 6.912430286407471
Epoch 610, val loss: 1.2960129976272583
Epoch 620, training loss: 0.08041566610336304 = 0.011487423442304134 + 0.01 * 6.892824649810791
Epoch 620, val loss: 1.3052325248718262
Epoch 630, training loss: 0.07963414490222931 = 0.010830394923686981 + 0.01 * 6.880375385284424
Epoch 630, val loss: 1.3141440153121948
Epoch 640, training loss: 0.0790766030550003 = 0.01022912934422493 + 0.01 * 6.8847479820251465
Epoch 640, val loss: 1.3227615356445312
Epoch 650, training loss: 0.07852792739868164 = 0.009677890688180923 + 0.01 * 6.885003566741943
Epoch 650, val loss: 1.3311530351638794
Epoch 660, training loss: 0.07812037318944931 = 0.009170680306851864 + 0.01 * 6.894969463348389
Epoch 660, val loss: 1.3392984867095947
Epoch 670, training loss: 0.07734715938568115 = 0.008702986873686314 + 0.01 * 6.864417552947998
Epoch 670, val loss: 1.3472017049789429
Epoch 680, training loss: 0.07690179347991943 = 0.00827089138329029 + 0.01 * 6.863090515136719
Epoch 680, val loss: 1.3548154830932617
Epoch 690, training loss: 0.0765368863940239 = 0.007871058769524097 + 0.01 * 6.866582870483398
Epoch 690, val loss: 1.3622913360595703
Epoch 700, training loss: 0.07605769485235214 = 0.0074999635107815266 + 0.01 * 6.855772972106934
Epoch 700, val loss: 1.369532585144043
Epoch 710, training loss: 0.07571239024400711 = 0.007155666593462229 + 0.01 * 6.855672836303711
Epoch 710, val loss: 1.3765555620193481
Epoch 720, training loss: 0.0754539743065834 = 0.006835751701146364 + 0.01 * 6.861822128295898
Epoch 720, val loss: 1.383359432220459
Epoch 730, training loss: 0.07489722967147827 = 0.006537930574268103 + 0.01 * 6.835930347442627
Epoch 730, val loss: 1.3900163173675537
Epoch 740, training loss: 0.07472092658281326 = 0.0062603577971458435 + 0.01 * 6.846056938171387
Epoch 740, val loss: 1.3964413404464722
Epoch 750, training loss: 0.07430188357830048 = 0.006000991445034742 + 0.01 * 6.830089569091797
Epoch 750, val loss: 1.4027438163757324
Epoch 760, training loss: 0.07407765090465546 = 0.005758384708315134 + 0.01 * 6.831927299499512
Epoch 760, val loss: 1.4087899923324585
Epoch 770, training loss: 0.07396435737609863 = 0.005531752482056618 + 0.01 * 6.843260765075684
Epoch 770, val loss: 1.4148097038269043
Epoch 780, training loss: 0.07363642007112503 = 0.0053191883489489555 + 0.01 * 6.831723690032959
Epoch 780, val loss: 1.4205344915390015
Epoch 790, training loss: 0.07349248230457306 = 0.005119744688272476 + 0.01 * 6.837274551391602
Epoch 790, val loss: 1.4261630773544312
Epoch 800, training loss: 0.07310904562473297 = 0.004932228475809097 + 0.01 * 6.817681312561035
Epoch 800, val loss: 1.4316908121109009
Epoch 810, training loss: 0.07294362038373947 = 0.004755609668791294 + 0.01 * 6.818801403045654
Epoch 810, val loss: 1.4370369911193848
Epoch 820, training loss: 0.07277330756187439 = 0.00458910409361124 + 0.01 * 6.81842041015625
Epoch 820, val loss: 1.442245364189148
Epoch 830, training loss: 0.07248333841562271 = 0.00443216972053051 + 0.01 * 6.805116653442383
Epoch 830, val loss: 1.447296142578125
Epoch 840, training loss: 0.0723784938454628 = 0.004284351132810116 + 0.01 * 6.809414386749268
Epoch 840, val loss: 1.4522795677185059
Epoch 850, training loss: 0.07215122133493423 = 0.004144506994634867 + 0.01 * 6.800671577453613
Epoch 850, val loss: 1.4570742845535278
Epoch 860, training loss: 0.07218438386917114 = 0.00401227455586195 + 0.01 * 6.817211151123047
Epoch 860, val loss: 1.4617738723754883
Epoch 870, training loss: 0.07185757160186768 = 0.00388719467446208 + 0.01 * 6.7970380783081055
Epoch 870, val loss: 1.4664074182510376
Epoch 880, training loss: 0.07197988033294678 = 0.003768588649109006 + 0.01 * 6.821129322052002
Epoch 880, val loss: 1.4708096981048584
Epoch 890, training loss: 0.07151106745004654 = 0.0036563947796821594 + 0.01 * 6.785467147827148
Epoch 890, val loss: 1.4751837253570557
Epoch 900, training loss: 0.07146237045526505 = 0.003549854503944516 + 0.01 * 6.791252136230469
Epoch 900, val loss: 1.4794083833694458
Epoch 910, training loss: 0.07129061967134476 = 0.003448714502155781 + 0.01 * 6.7841901779174805
Epoch 910, val loss: 1.4835957288742065
