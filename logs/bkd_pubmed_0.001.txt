Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1207075119018555 = 1.1103484630584717 + 0.001 * 10.359018325805664
Epoch 0, val loss: 1.1088546514511108
Epoch 10, training loss: 1.1075390577316284 = 1.0971802473068237 + 0.001 * 10.358848571777344
Epoch 10, val loss: 1.0956101417541504
Epoch 20, training loss: 1.0876438617706299 = 1.0772857666015625 + 0.001 * 10.358122825622559
Epoch 20, val loss: 1.0753223896026611
Epoch 30, training loss: 1.0562074184417725 = 1.0458520650863647 + 0.001 * 10.355379104614258
Epoch 30, val loss: 1.0439635515213013
Epoch 40, training loss: 1.0179635286331177 = 1.007622241973877 + 0.001 * 10.341255187988281
Epoch 40, val loss: 1.0066410303115845
Epoch 50, training loss: 0.9656041264533997 = 0.9553695321083069 + 0.001 * 10.234597206115723
Epoch 50, val loss: 0.9528117775917053
Epoch 60, training loss: 0.8830702304840088 = 0.8732578158378601 + 0.001 * 9.812405586242676
Epoch 60, val loss: 0.871274471282959
Epoch 70, training loss: 0.7767239212989807 = 0.7670681476593018 + 0.001 * 9.655768394470215
Epoch 70, val loss: 0.7668431401252747
Epoch 80, training loss: 0.6569632887840271 = 0.6474440097808838 + 0.001 * 9.519279479980469
Epoch 80, val loss: 0.6529704332351685
Epoch 90, training loss: 0.5415241718292236 = 0.5320270657539368 + 0.001 * 9.4971284866333
Epoch 90, val loss: 0.5500403046607971
Epoch 100, training loss: 0.45520102977752686 = 0.44570988416671753 + 0.001 * 9.49113655090332
Epoch 100, val loss: 0.4811362028121948
Epoch 110, training loss: 0.40309497714042664 = 0.3936122953891754 + 0.001 * 9.482695579528809
Epoch 110, val loss: 0.4458407163619995
Epoch 120, training loss: 0.3723504841327667 = 0.3628680408000946 + 0.001 * 9.482438087463379
Epoch 120, val loss: 0.42923492193222046
Epoch 130, training loss: 0.351046621799469 = 0.3415639400482178 + 0.001 * 9.482688903808594
Epoch 130, val loss: 0.4195145070552826
Epoch 140, training loss: 0.3343254625797272 = 0.3248412311077118 + 0.001 * 9.484222412109375
Epoch 140, val loss: 0.4124819338321686
Epoch 150, training loss: 0.32046690583229065 = 0.3109808564186096 + 0.001 * 9.486042976379395
Epoch 150, val loss: 0.407243937253952
Epoch 160, training loss: 0.308453232049942 = 0.2989655137062073 + 0.001 * 9.487724304199219
Epoch 160, val loss: 0.4038679003715515
Epoch 170, training loss: 0.29769808053970337 = 0.28820863366127014 + 0.001 * 9.489445686340332
Epoch 170, val loss: 0.4023028314113617
Epoch 180, training loss: 0.28788986802101135 = 0.2783986032009125 + 0.001 * 9.491275787353516
Epoch 180, val loss: 0.4021552503108978
Epoch 190, training loss: 0.2788134217262268 = 0.2693198025226593 + 0.001 * 9.493613243103027
Epoch 190, val loss: 0.40310221910476685
Epoch 200, training loss: 0.27030423283576965 = 0.26080816984176636 + 0.001 * 9.496071815490723
Epoch 200, val loss: 0.40506696701049805
Epoch 210, training loss: 0.26222720742225647 = 0.2527293264865875 + 0.001 * 9.497871398925781
Epoch 210, val loss: 0.4080192744731903
Epoch 220, training loss: 0.25447988510131836 = 0.24498118460178375 + 0.001 * 9.49870491027832
Epoch 220, val loss: 0.41187784075737
Epoch 230, training loss: 0.2470117062330246 = 0.23751366138458252 + 0.001 * 9.498039245605469
Epoch 230, val loss: 0.41667214035987854
Epoch 240, training loss: 0.23978260159492493 = 0.23028779029846191 + 0.001 * 9.494813919067383
Epoch 240, val loss: 0.4224303662776947
Epoch 250, training loss: 0.23277589678764343 = 0.22328020632266998 + 0.001 * 9.495688438415527
Epoch 250, val loss: 0.42913275957107544
Epoch 260, training loss: 0.22597481310367584 = 0.21648825705051422 + 0.001 * 9.486550331115723
Epoch 260, val loss: 0.43673259019851685
Epoch 270, training loss: 0.2193966507911682 = 0.20991043746471405 + 0.001 * 9.486210823059082
Epoch 270, val loss: 0.44561129808425903
Epoch 280, training loss: 0.2130420058965683 = 0.2035517543554306 + 0.001 * 9.490246772766113
Epoch 280, val loss: 0.4547487199306488
Epoch 290, training loss: 0.206952303647995 = 0.19746161997318268 + 0.001 * 9.490676879882812
Epoch 290, val loss: 0.465251624584198
Epoch 300, training loss: 0.20105595886707306 = 0.19156582653522491 + 0.001 * 9.490131378173828
Epoch 300, val loss: 0.4758109152317047
Epoch 310, training loss: 0.1954267919063568 = 0.18594639003276825 + 0.001 * 9.480406761169434
Epoch 310, val loss: 0.4880615472793579
Epoch 320, training loss: 0.18993660807609558 = 0.18044666945934296 + 0.001 * 9.4899320602417
Epoch 320, val loss: 0.49757376313209534
Epoch 330, training loss: 0.18467022478580475 = 0.17514632642269135 + 0.001 * 9.523900032043457
Epoch 330, val loss: 0.5108075141906738
Epoch 340, training loss: 0.179449662566185 = 0.16997231543064117 + 0.001 * 9.477344512939453
Epoch 340, val loss: 0.5211881995201111
Epoch 350, training loss: 0.17440152168273926 = 0.1649303138256073 + 0.001 * 9.47121524810791
Epoch 350, val loss: 0.5365243554115295
Epoch 360, training loss: 0.16937299072742462 = 0.1599220633506775 + 0.001 * 9.45093059539795
Epoch 360, val loss: 0.5474579334259033
Epoch 370, training loss: 0.16450992226600647 = 0.15503188967704773 + 0.001 * 9.478025436401367
Epoch 370, val loss: 0.5637616515159607
Epoch 380, training loss: 0.15962845087051392 = 0.15017563104629517 + 0.001 * 9.452816009521484
Epoch 380, val loss: 0.5747948884963989
Epoch 390, training loss: 0.1548731029033661 = 0.1453731209039688 + 0.001 * 9.499982833862305
Epoch 390, val loss: 0.592104434967041
Epoch 400, training loss: 0.15014109015464783 = 0.14067886769771576 + 0.001 * 9.46222972869873
Epoch 400, val loss: 0.606134295463562
Epoch 410, training loss: 0.1455194056034088 = 0.136098250746727 + 0.001 * 9.42115592956543
Epoch 410, val loss: 0.6211124658584595
Epoch 420, training loss: 0.14109471440315247 = 0.1316690891981125 + 0.001 * 9.425618171691895
Epoch 420, val loss: 0.6397916674613953
Epoch 430, training loss: 0.13658581674098969 = 0.1271660327911377 + 0.001 * 9.41978931427002
Epoch 430, val loss: 0.65234375
Epoch 440, training loss: 0.1321372091770172 = 0.12270955741405487 + 0.001 * 9.427656173706055
Epoch 440, val loss: 0.6741496920585632
Epoch 450, training loss: 0.12777432799339294 = 0.11835857480764389 + 0.001 * 9.415759086608887
Epoch 450, val loss: 0.6867357492446899
Epoch 460, training loss: 0.12344375997781754 = 0.11405572295188904 + 0.001 * 9.388038635253906
Epoch 460, val loss: 0.7091456651687622
Epoch 470, training loss: 0.11915072798728943 = 0.1097530722618103 + 0.001 * 9.39765739440918
Epoch 470, val loss: 0.7236141562461853
Epoch 480, training loss: 0.11484095454216003 = 0.10546121746301651 + 0.001 * 9.379740715026855
Epoch 480, val loss: 0.7467876076698303
Epoch 490, training loss: 0.11068105697631836 = 0.10128063708543777 + 0.001 * 9.400421142578125
Epoch 490, val loss: 0.7639584541320801
Epoch 500, training loss: 0.10657566785812378 = 0.09719575196504593 + 0.001 * 9.379912376403809
Epoch 500, val loss: 0.7851020097732544
Epoch 510, training loss: 0.10268690437078476 = 0.09331057220697403 + 0.001 * 9.37632942199707
Epoch 510, val loss: 0.8086762428283691
Epoch 520, training loss: 0.09874163568019867 = 0.08937712013721466 + 0.001 * 9.364516258239746
Epoch 520, val loss: 0.8249414563179016
Epoch 530, training loss: 0.09481319040060043 = 0.08542727679014206 + 0.001 * 9.385915756225586
Epoch 530, val loss: 0.8534783124923706
Epoch 540, training loss: 0.09086765348911285 = 0.08149144053459167 + 0.001 * 9.376213073730469
Epoch 540, val loss: 0.8695315718650818
Epoch 550, training loss: 0.08692838996648788 = 0.07756649702787399 + 0.001 * 9.36189079284668
Epoch 550, val loss: 0.8976209759712219
Epoch 560, training loss: 0.08313582837581635 = 0.07377889007329941 + 0.001 * 9.356934547424316
Epoch 560, val loss: 0.9183060526847839
Epoch 570, training loss: 0.07953180372714996 = 0.07016485184431076 + 0.001 * 9.366949081420898
Epoch 570, val loss: 0.9419660568237305
Epoch 580, training loss: 0.07616912573575974 = 0.06679673492908478 + 0.001 * 9.372390747070312
Epoch 580, val loss: 0.9702571034431458
Epoch 590, training loss: 0.07267030328512192 = 0.06329724937677383 + 0.001 * 9.373055458068848
Epoch 590, val loss: 0.9881641864776611
Epoch 600, training loss: 0.06924885511398315 = 0.05989432707428932 + 0.001 * 9.354528427124023
Epoch 600, val loss: 1.0190783739089966
Epoch 610, training loss: 0.06594344228506088 = 0.056596532464027405 + 0.001 * 9.34691047668457
Epoch 610, val loss: 1.0397974252700806
Epoch 620, training loss: 0.06289882212877274 = 0.05355362966656685 + 0.001 * 9.345192909240723
Epoch 620, val loss: 1.0664572715759277
Epoch 630, training loss: 0.060127511620521545 = 0.0507819764316082 + 0.001 * 9.345534324645996
Epoch 630, val loss: 1.0959506034851074
Epoch 640, training loss: 0.057283930480480194 = 0.04793644696474075 + 0.001 * 9.347484588623047
Epoch 640, val loss: 1.1157562732696533
Epoch 650, training loss: 0.05449112132191658 = 0.04513880982995033 + 0.001 * 9.352310180664062
Epoch 650, val loss: 1.1467722654342651
Epoch 660, training loss: 0.05189446359872818 = 0.04256030544638634 + 0.001 * 9.334159851074219
Epoch 660, val loss: 1.1706244945526123
Epoch 670, training loss: 0.04952823370695114 = 0.04015600308775902 + 0.001 * 9.372232437133789
Epoch 670, val loss: 1.1962320804595947
Epoch 680, training loss: 0.047291941940784454 = 0.037920888513326645 + 0.001 * 9.371052742004395
Epoch 680, val loss: 1.2260863780975342
Epoch 690, training loss: 0.045039232820272446 = 0.03569132089614868 + 0.001 * 9.34791088104248
Epoch 690, val loss: 1.247016191482544
Epoch 700, training loss: 0.04289946332573891 = 0.03355919569730759 + 0.001 * 9.340267181396484
Epoch 700, val loss: 1.2770123481750488
Epoch 710, training loss: 0.040974389761686325 = 0.0316065214574337 + 0.001 * 9.367868423461914
Epoch 710, val loss: 1.3021506071090698
Epoch 720, training loss: 0.03914973512291908 = 0.029777325689792633 + 0.001 * 9.372408866882324
Epoch 720, val loss: 1.3266912698745728
Epoch 730, training loss: 0.03741048648953438 = 0.028065666556358337 + 0.001 * 9.344818115234375
Epoch 730, val loss: 1.355719804763794
Epoch 740, training loss: 0.0357351154088974 = 0.026397615671157837 + 0.001 * 9.337501525878906
Epoch 740, val loss: 1.3775101900100708
Epoch 750, training loss: 0.03420247882604599 = 0.02484814263880253 + 0.001 * 9.354336738586426
Epoch 750, val loss: 1.4055668115615845
Epoch 760, training loss: 0.032777369022369385 = 0.023408567532896996 + 0.001 * 9.368803024291992
Epoch 760, val loss: 1.4285235404968262
Epoch 770, training loss: 0.03138401359319687 = 0.022059714421629906 + 0.001 * 9.324297904968262
Epoch 770, val loss: 1.4539536237716675
Epoch 780, training loss: 0.030131136998534203 = 0.020801814272999763 + 0.001 * 9.32932186126709
Epoch 780, val loss: 1.4778043031692505
Epoch 790, training loss: 0.028947729617357254 = 0.019616713747382164 + 0.001 * 9.331015586853027
Epoch 790, val loss: 1.5004194974899292
Epoch 800, training loss: 0.027896318584680557 = 0.018503135070204735 + 0.001 * 9.393182754516602
Epoch 800, val loss: 1.525322675704956
Epoch 810, training loss: 0.026791689917445183 = 0.017452042549848557 + 0.001 * 9.33964729309082
Epoch 810, val loss: 1.546449899673462
Epoch 820, training loss: 0.02578151598572731 = 0.01645861193537712 + 0.001 * 9.322903633117676
Epoch 820, val loss: 1.5701220035552979
Epoch 830, training loss: 0.024837028235197067 = 0.015524976886808872 + 0.001 * 9.312049865722656
Epoch 830, val loss: 1.591307282447815
Epoch 840, training loss: 0.02395998314023018 = 0.01464962400496006 + 0.001 * 9.310359001159668
Epoch 840, val loss: 1.6133873462677002
Epoch 850, training loss: 0.0231851264834404 = 0.013826685026288033 + 0.001 * 9.358442306518555
Epoch 850, val loss: 1.6332818269729614
Epoch 860, training loss: 0.022360719740390778 = 0.01304494310170412 + 0.001 * 9.315776824951172
Epoch 860, val loss: 1.654648780822754
Epoch 870, training loss: 0.021622110158205032 = 0.01230147946625948 + 0.001 * 9.32063102722168
Epoch 870, val loss: 1.6750792264938354
Epoch 880, training loss: 0.0209170151501894 = 0.011589158326387405 + 0.001 * 9.327856063842773
Epoch 880, val loss: 1.6952967643737793
Epoch 890, training loss: 0.020243540406227112 = 0.010917242616415024 + 0.001 * 9.326298713684082
Epoch 890, val loss: 1.7155985832214355
Epoch 900, training loss: 0.019592970609664917 = 0.01028451044112444 + 0.001 * 9.308459281921387
Epoch 900, val loss: 1.7355012893676758
Epoch 910, training loss: 0.019011985510587692 = 0.009694620966911316 + 0.001 * 9.317363739013672
Epoch 910, val loss: 1.7545055150985718
Epoch 920, training loss: 0.018453368917107582 = 0.009150007739663124 + 0.001 * 9.303360939025879
Epoch 920, val loss: 1.7727839946746826
Epoch 930, training loss: 0.017963282763957977 = 0.008646644651889801 + 0.001 * 9.31663703918457
Epoch 930, val loss: 1.7904514074325562
Epoch 940, training loss: 0.017512278631329536 = 0.008179822936654091 + 0.001 * 9.3324556350708
Epoch 940, val loss: 1.8076688051223755
Epoch 950, training loss: 0.017032861709594727 = 0.007746551185846329 + 0.001 * 9.286311149597168
Epoch 950, val loss: 1.8248028755187988
Epoch 960, training loss: 0.01665211096405983 = 0.007347173523157835 + 0.001 * 9.304936408996582
Epoch 960, val loss: 1.8418296575546265
Epoch 970, training loss: 0.01626479998230934 = 0.006975314114242792 + 0.001 * 9.289485931396484
Epoch 970, val loss: 1.8577841520309448
Epoch 980, training loss: 0.015951571986079216 = 0.006629166658967733 + 0.001 * 9.322405815124512
Epoch 980, val loss: 1.8736546039581299
Epoch 990, training loss: 0.015597809106111526 = 0.006307642441242933 + 0.001 * 9.290166854858398
Epoch 990, val loss: 1.8890427350997925
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8417
Overall ASR: 0.6445
Flip ASR: 0.5579/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1047323942184448 = 1.094373345375061 + 0.001 * 10.359034538269043
Epoch 0, val loss: 1.0928311347961426
Epoch 10, training loss: 1.093383550643921 = 1.0830247402191162 + 0.001 * 10.35885238647461
Epoch 10, val loss: 1.081318974494934
Epoch 20, training loss: 1.075534701347351 = 1.0651766061782837 + 0.001 * 10.35806941986084
Epoch 20, val loss: 1.0629615783691406
Epoch 30, training loss: 1.0460724830627441 = 1.0357173681259155 + 0.001 * 10.355062484741211
Epoch 30, val loss: 1.033348560333252
Epoch 40, training loss: 1.0054972171783447 = 0.9951595664024353 + 0.001 * 10.337652206420898
Epoch 40, val loss: 0.9928237199783325
Epoch 50, training loss: 0.938526451587677 = 0.9283689260482788 + 0.001 * 10.157499313354492
Epoch 50, val loss: 0.9244688153266907
Epoch 60, training loss: 0.8399244546890259 = 0.8301464319229126 + 0.001 * 9.778007507324219
Epoch 60, val loss: 0.8262823820114136
Epoch 70, training loss: 0.7222663760185242 = 0.7125471830368042 + 0.001 * 9.7191743850708
Epoch 70, val loss: 0.713129997253418
Epoch 80, training loss: 0.6073799133300781 = 0.5977753400802612 + 0.001 * 9.604546546936035
Epoch 80, val loss: 0.6074562668800354
Epoch 90, training loss: 0.5121628046035767 = 0.50267094373703 + 0.001 * 9.491844177246094
Epoch 90, val loss: 0.5246375203132629
Epoch 100, training loss: 0.4417355954647064 = 0.43223851919174194 + 0.001 * 9.497062683105469
Epoch 100, val loss: 0.4685671329498291
Epoch 110, training loss: 0.39629459381103516 = 0.3868069350719452 + 0.001 * 9.48766803741455
Epoch 110, val loss: 0.43812230229377747
Epoch 120, training loss: 0.3682267963886261 = 0.3587457239627838 + 0.001 * 9.481064796447754
Epoch 120, val loss: 0.42410311102867126
Epoch 130, training loss: 0.3479763865470886 = 0.33849310874938965 + 0.001 * 9.483262062072754
Epoch 130, val loss: 0.4158099591732025
Epoch 140, training loss: 0.3314971625804901 = 0.3220122754573822 + 0.001 * 9.48488998413086
Epoch 140, val loss: 0.4095346927642822
Epoch 150, training loss: 0.31762397289276123 = 0.3081360161304474 + 0.001 * 9.48796272277832
Epoch 150, val loss: 0.4049215316772461
Epoch 160, training loss: 0.3055517375469208 = 0.29606083035469055 + 0.001 * 9.49091911315918
Epoch 160, val loss: 0.4020657539367676
Epoch 170, training loss: 0.29470139741897583 = 0.28520750999450684 + 0.001 * 9.493884086608887
Epoch 170, val loss: 0.40079328417778015
Epoch 180, training loss: 0.2847323417663574 = 0.2752355933189392 + 0.001 * 9.496746063232422
Epoch 180, val loss: 0.4009014070034027
Epoch 190, training loss: 0.2754615247249603 = 0.2659618854522705 + 0.001 * 9.499628067016602
Epoch 190, val loss: 0.40213221311569214
Epoch 200, training loss: 0.2667357325553894 = 0.25723347067832947 + 0.001 * 9.502276420593262
Epoch 200, val loss: 0.4043519198894501
Epoch 210, training loss: 0.2584436535835266 = 0.24893920123577118 + 0.001 * 9.504451751708984
Epoch 210, val loss: 0.40747201442718506
Epoch 220, training loss: 0.25050148367881775 = 0.24099533259868622 + 0.001 * 9.506144523620605
Epoch 220, val loss: 0.4116058051586151
Epoch 230, training loss: 0.24285347759723663 = 0.23334677517414093 + 0.001 * 9.50670051574707
Epoch 230, val loss: 0.4166571795940399
Epoch 240, training loss: 0.23545882105827332 = 0.22595404088497162 + 0.001 * 9.504778861999512
Epoch 240, val loss: 0.422608882188797
Epoch 250, training loss: 0.22829315066337585 = 0.218795046210289 + 0.001 * 9.498096466064453
Epoch 250, val loss: 0.42954137921333313
Epoch 260, training loss: 0.22137674689292908 = 0.2118832767009735 + 0.001 * 9.493474006652832
Epoch 260, val loss: 0.4377608895301819
Epoch 270, training loss: 0.2147415578365326 = 0.20524895191192627 + 0.001 * 9.492600440979004
Epoch 270, val loss: 0.4470304846763611
Epoch 280, training loss: 0.2084258794784546 = 0.19893741607666016 + 0.001 * 9.48846435546875
Epoch 280, val loss: 0.4563329219818115
Epoch 290, training loss: 0.20235860347747803 = 0.192839115858078 + 0.001 * 9.519490242004395
Epoch 290, val loss: 0.4675590395927429
Epoch 300, training loss: 0.19651973247528076 = 0.18703322112560272 + 0.001 * 9.486506462097168
Epoch 300, val loss: 0.47968509793281555
Epoch 310, training loss: 0.19088608026504517 = 0.18140719830989838 + 0.001 * 9.478885650634766
Epoch 310, val loss: 0.49074581265449524
Epoch 320, training loss: 0.1853785216808319 = 0.17591708898544312 + 0.001 * 9.461429595947266
Epoch 320, val loss: 0.5029330849647522
Epoch 330, training loss: 0.18024788796901703 = 0.17076794803142548 + 0.001 * 9.479937553405762
Epoch 330, val loss: 0.5178563594818115
Epoch 340, training loss: 0.17511597275733948 = 0.16563549637794495 + 0.001 * 9.48046875
Epoch 340, val loss: 0.5298173427581787
Epoch 350, training loss: 0.17017406225204468 = 0.16071273386478424 + 0.001 * 9.461333274841309
Epoch 350, val loss: 0.5434439778327942
Epoch 360, training loss: 0.16531388461589813 = 0.15585915744304657 + 0.001 * 9.454731941223145
Epoch 360, val loss: 0.5566900968551636
Epoch 370, training loss: 0.16068261861801147 = 0.15120714902877808 + 0.001 * 9.475471496582031
Epoch 370, val loss: 0.5737305283546448
Epoch 380, training loss: 0.15597286820411682 = 0.14652614295482635 + 0.001 * 9.446732521057129
Epoch 380, val loss: 0.5857297778129578
Epoch 390, training loss: 0.15141257643699646 = 0.1419704407453537 + 0.001 * 9.442137718200684
Epoch 390, val loss: 0.5999887585639954
Epoch 400, training loss: 0.14685866236686707 = 0.13740870356559753 + 0.001 * 9.449954986572266
Epoch 400, val loss: 0.6173915266990662
Epoch 410, training loss: 0.14238956570625305 = 0.13295382261276245 + 0.001 * 9.435745239257812
Epoch 410, val loss: 0.630725622177124
Epoch 420, training loss: 0.13814093172550201 = 0.12870818376541138 + 0.001 * 9.432750701904297
Epoch 420, val loss: 0.6510545611381531
Epoch 430, training loss: 0.1337137520313263 = 0.1242889016866684 + 0.001 * 9.424848556518555
Epoch 430, val loss: 0.6679086685180664
Epoch 440, training loss: 0.12931397557258606 = 0.11989536136388779 + 0.001 * 9.418621063232422
Epoch 440, val loss: 0.6844214200973511
Epoch 450, training loss: 0.12505470216274261 = 0.1156095340847969 + 0.001 * 9.445168495178223
Epoch 450, val loss: 0.7012901306152344
Epoch 460, training loss: 0.12104744464159012 = 0.11161880195140839 + 0.001 * 9.42863941192627
Epoch 460, val loss: 0.7178418040275574
Epoch 470, training loss: 0.11659357696771622 = 0.10718154162168503 + 0.001 * 9.412033081054688
Epoch 470, val loss: 0.7391648888587952
Epoch 480, training loss: 0.11247176676988602 = 0.10304579138755798 + 0.001 * 9.42597770690918
Epoch 480, val loss: 0.7605664134025574
Epoch 490, training loss: 0.10837841778993607 = 0.09896835684776306 + 0.001 * 9.410063743591309
Epoch 490, val loss: 0.7803774476051331
Epoch 500, training loss: 0.10463474690914154 = 0.09523029625415802 + 0.001 * 9.404446601867676
Epoch 500, val loss: 0.7978461980819702
Epoch 510, training loss: 0.1004623994231224 = 0.0910438597202301 + 0.001 * 9.418538093566895
Epoch 510, val loss: 0.8241484761238098
Epoch 520, training loss: 0.09657224267721176 = 0.087149478495121 + 0.001 * 9.422762870788574
Epoch 520, val loss: 0.8434141874313354
Epoch 530, training loss: 0.09288819134235382 = 0.08346410095691681 + 0.001 * 9.424086570739746
Epoch 530, val loss: 0.8723331689834595
Epoch 540, training loss: 0.08906236290931702 = 0.07967425137758255 + 0.001 * 9.38810920715332
Epoch 540, val loss: 0.8879859447479248
Epoch 550, training loss: 0.08537428826093674 = 0.07598574459552765 + 0.001 * 9.388541221618652
Epoch 550, val loss: 0.9176068902015686
Epoch 560, training loss: 0.08179433643817902 = 0.0724102109670639 + 0.001 * 9.384123802185059
Epoch 560, val loss: 0.9356622099876404
Epoch 570, training loss: 0.07821418344974518 = 0.06882710754871368 + 0.001 * 9.387073516845703
Epoch 570, val loss: 0.9658809304237366
Epoch 580, training loss: 0.07478349655866623 = 0.06539400666952133 + 0.001 * 9.389488220214844
Epoch 580, val loss: 0.9877955913543701
Epoch 590, training loss: 0.07168888300657272 = 0.062268707901239395 + 0.001 * 9.420177459716797
Epoch 590, val loss: 1.0111891031265259
Epoch 600, training loss: 0.06855647265911102 = 0.05915240943431854 + 0.001 * 9.404061317443848
Epoch 600, val loss: 1.0438860654830933
Epoch 610, training loss: 0.06538467109203339 = 0.056000709533691406 + 0.001 * 9.383964538574219
Epoch 610, val loss: 1.0623854398727417
Epoch 620, training loss: 0.06236656755208969 = 0.05298616737127304 + 0.001 * 9.380400657653809
Epoch 620, val loss: 1.0946966409683228
Epoch 630, training loss: 0.05949706584215164 = 0.050096552819013596 + 0.001 * 9.400514602661133
Epoch 630, val loss: 1.1181474924087524
Epoch 640, training loss: 0.05691995471715927 = 0.04751173406839371 + 0.001 * 9.408219337463379
Epoch 640, val loss: 1.1425395011901855
Epoch 650, training loss: 0.054262854158878326 = 0.04489361122250557 + 0.001 * 9.369240760803223
Epoch 650, val loss: 1.1749714612960815
Epoch 660, training loss: 0.05177838355302811 = 0.042345430701971054 + 0.001 * 9.432950973510742
Epoch 660, val loss: 1.1958773136138916
Epoch 670, training loss: 0.04928987845778465 = 0.03989311307668686 + 0.001 * 9.396763801574707
Epoch 670, val loss: 1.2264893054962158
Epoch 680, training loss: 0.0470348484814167 = 0.03766048327088356 + 0.001 * 9.374364852905273
Epoch 680, val loss: 1.2535548210144043
Epoch 690, training loss: 0.04492729529738426 = 0.035544056445360184 + 0.001 * 9.38323974609375
Epoch 690, val loss: 1.2764205932617188
Epoch 700, training loss: 0.04281669110059738 = 0.033434417098760605 + 0.001 * 9.382272720336914
Epoch 700, val loss: 1.3075380325317383
Epoch 710, training loss: 0.040875136852264404 = 0.03148248419165611 + 0.001 * 9.392650604248047
Epoch 710, val loss: 1.331945538520813
Epoch 720, training loss: 0.03903929889202118 = 0.029671108350157738 + 0.001 * 9.368191719055176
Epoch 720, val loss: 1.357372522354126
Epoch 730, training loss: 0.03731169551610947 = 0.027954721823334694 + 0.001 * 9.356971740722656
Epoch 730, val loss: 1.386683464050293
Epoch 740, training loss: 0.03568434715270996 = 0.02627537213265896 + 0.001 * 9.408976554870605
Epoch 740, val loss: 1.4088993072509766
Epoch 750, training loss: 0.03409072384238243 = 0.024710966274142265 + 0.001 * 9.37975788116455
Epoch 750, val loss: 1.4368714094161987
Epoch 760, training loss: 0.03260911628603935 = 0.023238569498062134 + 0.001 * 9.370546340942383
Epoch 760, val loss: 1.4601880311965942
Epoch 770, training loss: 0.031201709061861038 = 0.021849144250154495 + 0.001 * 9.352563858032227
Epoch 770, val loss: 1.4857606887817383
Epoch 780, training loss: 0.029885346069931984 = 0.02055259421467781 + 0.001 * 9.332751274108887
Epoch 780, val loss: 1.5104159116744995
Epoch 790, training loss: 0.02867962419986725 = 0.019309837371110916 + 0.001 * 9.369786262512207
Epoch 790, val loss: 1.5333466529846191
Epoch 800, training loss: 0.027507152408361435 = 0.01812894456088543 + 0.001 * 9.378207206726074
Epoch 800, val loss: 1.5571388006210327
Epoch 810, training loss: 0.026409517973661423 = 0.017026767134666443 + 0.001 * 9.382750511169434
Epoch 810, val loss: 1.5811481475830078
Epoch 820, training loss: 0.025344768539071083 = 0.015992669388651848 + 0.001 * 9.35209846496582
Epoch 820, val loss: 1.6039561033248901
Epoch 830, training loss: 0.024384910240769386 = 0.015017957426607609 + 0.001 * 9.366951942443848
Epoch 830, val loss: 1.626578688621521
Epoch 840, training loss: 0.023436684161424637 = 0.014105826616287231 + 0.001 * 9.330857276916504
Epoch 840, val loss: 1.64857017993927
Epoch 850, training loss: 0.022603340446949005 = 0.013258293271064758 + 0.001 * 9.345047950744629
Epoch 850, val loss: 1.6711483001708984
Epoch 860, training loss: 0.021807339042425156 = 0.012469126842916012 + 0.001 * 9.338211059570312
Epoch 860, val loss: 1.692354679107666
Epoch 870, training loss: 0.02107587456703186 = 0.011736606247723103 + 0.001 * 9.33926773071289
Epoch 870, val loss: 1.7138104438781738
Epoch 880, training loss: 0.020396843552589417 = 0.011057335883378983 + 0.001 * 9.339507102966309
Epoch 880, val loss: 1.7347294092178345
Epoch 890, training loss: 0.019757531583309174 = 0.010426796972751617 + 0.001 * 9.330733299255371
Epoch 890, val loss: 1.7550854682922363
Epoch 900, training loss: 0.01918729767203331 = 0.009842765517532825 + 0.001 * 9.344531059265137
Epoch 900, val loss: 1.7751610279083252
Epoch 910, training loss: 0.01863006502389908 = 0.00930226780474186 + 0.001 * 9.32779598236084
Epoch 910, val loss: 1.7949637174606323
Epoch 920, training loss: 0.01817340776324272 = 0.008798796683549881 + 0.001 * 9.374611854553223
Epoch 920, val loss: 1.8135448694229126
Epoch 930, training loss: 0.017702750861644745 = 0.008332861587405205 + 0.001 * 9.369889259338379
Epoch 930, val loss: 1.8325955867767334
Epoch 940, training loss: 0.017237680032849312 = 0.007899025455117226 + 0.001 * 9.338654518127441
Epoch 940, val loss: 1.850584864616394
Epoch 950, training loss: 0.01682552695274353 = 0.00749730970710516 + 0.001 * 9.328216552734375
Epoch 950, val loss: 1.8685308694839478
Epoch 960, training loss: 0.016448523849248886 = 0.0071227881126105785 + 0.001 * 9.325735092163086
Epoch 960, val loss: 1.885888695716858
Epoch 970, training loss: 0.016081301495432854 = 0.006774733308702707 + 0.001 * 9.306567192077637
Epoch 970, val loss: 1.9029334783554077
Epoch 980, training loss: 0.01578702963888645 = 0.006450365297496319 + 0.001 * 9.336664199829102
Epoch 980, val loss: 1.9193072319030762
Epoch 990, training loss: 0.015469729900360107 = 0.006148453336209059 + 0.001 * 9.32127571105957
Epoch 990, val loss: 1.9354690313339233
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.6912
Flip ASR: 0.6165/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.102263331413269 = 1.0919042825698853 + 0.001 * 10.3590087890625
Epoch 0, val loss: 1.090820550918579
Epoch 10, training loss: 1.0920724868774414 = 1.0817135572433472 + 0.001 * 10.358901023864746
Epoch 10, val loss: 1.0805877447128296
Epoch 20, training loss: 1.0769526958465576 = 1.0665943622589111 + 0.001 * 10.358370780944824
Epoch 20, val loss: 1.0650655031204224
Epoch 30, training loss: 1.051426649093628 = 1.0410699844360352 + 0.001 * 10.356650352478027
Epoch 30, val loss: 1.0391050577163696
Epoch 40, training loss: 1.0119050741195679 = 1.0015556812286377 + 0.001 * 10.349381446838379
Epoch 40, val loss: 0.9993933439254761
Epoch 50, training loss: 0.9478991031646729 = 0.9376041889190674 + 0.001 * 10.294912338256836
Epoch 50, val loss: 0.9338973760604858
Epoch 60, training loss: 0.8476317524909973 = 0.8378374576568604 + 0.001 * 9.79427433013916
Epoch 60, val loss: 0.834240198135376
Epoch 70, training loss: 0.7203242778778076 = 0.7107037901878357 + 0.001 * 9.620516777038574
Epoch 70, val loss: 0.7112676501274109
Epoch 80, training loss: 0.5912185907363892 = 0.5817150473594666 + 0.001 * 9.503522872924805
Epoch 80, val loss: 0.5929616689682007
Epoch 90, training loss: 0.48959478735923767 = 0.4801051616668701 + 0.001 * 9.489631652832031
Epoch 90, val loss: 0.5070898532867432
Epoch 100, training loss: 0.4243961572647095 = 0.4149094820022583 + 0.001 * 9.486689567565918
Epoch 100, val loss: 0.4584594964981079
Epoch 110, training loss: 0.3865112066268921 = 0.37703177332878113 + 0.001 * 9.479422569274902
Epoch 110, val loss: 0.4357720911502838
Epoch 120, training loss: 0.3621809184551239 = 0.35270145535469055 + 0.001 * 9.479470252990723
Epoch 120, val loss: 0.4244506061077118
Epoch 130, training loss: 0.34352442622184753 = 0.3340437710285187 + 0.001 * 9.480660438537598
Epoch 130, val loss: 0.4165268540382385
Epoch 140, training loss: 0.3282330334186554 = 0.31875020265579224 + 0.001 * 9.482816696166992
Epoch 140, val loss: 0.4103378653526306
Epoch 150, training loss: 0.3152569830417633 = 0.3057715594768524 + 0.001 * 9.485420227050781
Epoch 150, val loss: 0.40606725215911865
Epoch 160, training loss: 0.30387213826179504 = 0.2943846583366394 + 0.001 * 9.487471580505371
Epoch 160, val loss: 0.40372976660728455
Epoch 170, training loss: 0.2936176657676697 = 0.2841286063194275 + 0.001 * 9.489062309265137
Epoch 170, val loss: 0.4030587077140808
Epoch 180, training loss: 0.2841976583003998 = 0.27470695972442627 + 0.001 * 9.49071216583252
Epoch 180, val loss: 0.4036640524864197
Epoch 190, training loss: 0.27541813254356384 = 0.26592475175857544 + 0.001 * 9.493385314941406
Epoch 190, val loss: 0.4052712321281433
Epoch 200, training loss: 0.2671175003051758 = 0.257621169090271 + 0.001 * 9.496338844299316
Epoch 200, val loss: 0.4078320562839508
Epoch 210, training loss: 0.25919005274772644 = 0.24969147145748138 + 0.001 * 9.498583793640137
Epoch 210, val loss: 0.4112913906574249
Epoch 220, training loss: 0.25158196687698364 = 0.242081880569458 + 0.001 * 9.500090599060059
Epoch 220, val loss: 0.41558992862701416
Epoch 230, training loss: 0.24425432085990906 = 0.2347542643547058 + 0.001 * 9.500054359436035
Epoch 230, val loss: 0.42078840732574463
Epoch 240, training loss: 0.23716570436954498 = 0.22766846418380737 + 0.001 * 9.497233390808105
Epoch 240, val loss: 0.42690515518188477
Epoch 250, training loss: 0.23029737174510956 = 0.22080308198928833 + 0.001 * 9.494284629821777
Epoch 250, val loss: 0.433898389339447
Epoch 260, training loss: 0.22367654740810394 = 0.21417291462421417 + 0.001 * 9.503637313842773
Epoch 260, val loss: 0.44206199049949646
Epoch 270, training loss: 0.217316672205925 = 0.20782528817653656 + 0.001 * 9.491386413574219
Epoch 270, val loss: 0.45131468772888184
Epoch 280, training loss: 0.21123181283473969 = 0.20173850655555725 + 0.001 * 9.493306159973145
Epoch 280, val loss: 0.4614097476005554
Epoch 290, training loss: 0.20540979504585266 = 0.19591592252254486 + 0.001 * 9.493870735168457
Epoch 290, val loss: 0.47077372670173645
Epoch 300, training loss: 0.1998579055070877 = 0.1903279572725296 + 0.001 * 9.529947280883789
Epoch 300, val loss: 0.4826928973197937
Epoch 310, training loss: 0.19452151656150818 = 0.18503491580486298 + 0.001 * 9.486601829528809
Epoch 310, val loss: 0.4922199547290802
Epoch 320, training loss: 0.18936742842197418 = 0.17985890805721283 + 0.001 * 9.508522987365723
Epoch 320, val loss: 0.5037103295326233
Epoch 330, training loss: 0.1842990666627884 = 0.1748281717300415 + 0.001 * 9.470897674560547
Epoch 330, val loss: 0.5155770778656006
Epoch 340, training loss: 0.17937114834785461 = 0.1699187159538269 + 0.001 * 9.452435493469238
Epoch 340, val loss: 0.5289270877838135
Epoch 350, training loss: 0.17455172538757324 = 0.16508615016937256 + 0.001 * 9.46557903289795
Epoch 350, val loss: 0.5406498908996582
Epoch 360, training loss: 0.1698123663663864 = 0.16035202145576477 + 0.001 * 9.460348129272461
Epoch 360, val loss: 0.5548548698425293
Epoch 370, training loss: 0.16512569785118103 = 0.15565721690654755 + 0.001 * 9.46848201751709
Epoch 370, val loss: 0.5671860575675964
Epoch 380, training loss: 0.160756915807724 = 0.15130917727947235 + 0.001 * 9.447742462158203
Epoch 380, val loss: 0.5794363617897034
Epoch 390, training loss: 0.15609656274318695 = 0.14666908979415894 + 0.001 * 9.427474975585938
Epoch 390, val loss: 0.5937436819076538
Epoch 400, training loss: 0.1515931934118271 = 0.14216063916683197 + 0.001 * 9.432557106018066
Epoch 400, val loss: 0.6089401245117188
Epoch 410, training loss: 0.14721903204917908 = 0.13780738413333893 + 0.001 * 9.411654472351074
Epoch 410, val loss: 0.6262681484222412
Epoch 420, training loss: 0.14282165467739105 = 0.13340836763381958 + 0.001 * 9.413281440734863
Epoch 420, val loss: 0.6408885717391968
Epoch 430, training loss: 0.13908667862415314 = 0.12963849306106567 + 0.001 * 9.448180198669434
Epoch 430, val loss: 0.6615495681762695
Epoch 440, training loss: 0.13449271023273468 = 0.12509626150131226 + 0.001 * 9.396452903747559
Epoch 440, val loss: 0.6760090589523315
Epoch 450, training loss: 0.13029484450817108 = 0.1209157258272171 + 0.001 * 9.379111289978027
Epoch 450, val loss: 0.6921349167823792
Epoch 460, training loss: 0.12616999447345734 = 0.11673629283905029 + 0.001 * 9.433700561523438
Epoch 460, val loss: 0.7089335322380066
Epoch 470, training loss: 0.12205052375793457 = 0.11266863346099854 + 0.001 * 9.381891250610352
Epoch 470, val loss: 0.7245081067085266
Epoch 480, training loss: 0.1179451122879982 = 0.10855420678853989 + 0.001 * 9.390904426574707
Epoch 480, val loss: 0.7439576983451843
Epoch 490, training loss: 0.11427134275436401 = 0.10487646609544754 + 0.001 * 9.394880294799805
Epoch 490, val loss: 0.7608429789543152
Epoch 500, training loss: 0.10983962565660477 = 0.10045822709798813 + 0.001 * 9.38139820098877
Epoch 500, val loss: 0.7828909158706665
Epoch 510, training loss: 0.10596397519111633 = 0.09657479077577591 + 0.001 * 9.389183044433594
Epoch 510, val loss: 0.8066496253013611
Epoch 520, training loss: 0.1019870787858963 = 0.09261375665664673 + 0.001 * 9.373318672180176
Epoch 520, val loss: 0.8235100507736206
Epoch 530, training loss: 0.09801679104566574 = 0.08864811062812805 + 0.001 * 9.368682861328125
Epoch 530, val loss: 0.847837507724762
Epoch 540, training loss: 0.09427300095558167 = 0.08490949869155884 + 0.001 * 9.363499641418457
Epoch 540, val loss: 0.8711960911750793
Epoch 550, training loss: 0.09059998393058777 = 0.0812118873000145 + 0.001 * 9.388092994689941
Epoch 550, val loss: 0.8884871006011963
Epoch 560, training loss: 0.08686621487140656 = 0.07749602943658829 + 0.001 * 9.370182037353516
Epoch 560, val loss: 0.9191229343414307
Epoch 570, training loss: 0.08321680128574371 = 0.07384917885065079 + 0.001 * 9.367624282836914
Epoch 570, val loss: 0.936698853969574
Epoch 580, training loss: 0.07968769967556 = 0.07033280283212662 + 0.001 * 9.354897499084473
Epoch 580, val loss: 0.9671030640602112
Epoch 590, training loss: 0.07615971565246582 = 0.06679268181324005 + 0.001 * 9.367030143737793
Epoch 590, val loss: 0.9860621094703674
Epoch 600, training loss: 0.07281546294689178 = 0.06342963874340057 + 0.001 * 9.385821342468262
Epoch 600, val loss: 1.0134068727493286
Epoch 610, training loss: 0.06985640525817871 = 0.06047582998871803 + 0.001 * 9.380576133728027
Epoch 610, val loss: 1.0429147481918335
Epoch 620, training loss: 0.06661656498908997 = 0.057285796850919724 + 0.001 * 9.330763816833496
Epoch 620, val loss: 1.061442255973816
Epoch 630, training loss: 0.06360547989606857 = 0.054267365485429764 + 0.001 * 9.338111877441406
Epoch 630, val loss: 1.0941877365112305
Epoch 640, training loss: 0.060643523931503296 = 0.05129314213991165 + 0.001 * 9.350380897521973
Epoch 640, val loss: 1.1150755882263184
Epoch 650, training loss: 0.057870879769325256 = 0.04854689911007881 + 0.001 * 9.323980331420898
Epoch 650, val loss: 1.142430067062378
Epoch 660, training loss: 0.05527637153863907 = 0.04595939815044403 + 0.001 * 9.316973686218262
Epoch 660, val loss: 1.1710370779037476
Epoch 670, training loss: 0.05278170853853226 = 0.04342561587691307 + 0.001 * 9.356094360351562
Epoch 670, val loss: 1.1921985149383545
Epoch 680, training loss: 0.05030433088541031 = 0.04097237065434456 + 0.001 * 9.331957817077637
Epoch 680, val loss: 1.2232075929641724
Epoch 690, training loss: 0.04800493270158768 = 0.03869539871811867 + 0.001 * 9.30953311920166
Epoch 690, val loss: 1.2451192140579224
Epoch 700, training loss: 0.04582665115594864 = 0.03651731088757515 + 0.001 * 9.309340476989746
Epoch 700, val loss: 1.2742335796356201
Epoch 710, training loss: 0.043785206973552704 = 0.03446415811777115 + 0.001 * 9.321050643920898
Epoch 710, val loss: 1.298943042755127
Epoch 720, training loss: 0.04187685251235962 = 0.032567135989665985 + 0.001 * 9.309714317321777
Epoch 720, val loss: 1.3225950002670288
Epoch 730, training loss: 0.0399983748793602 = 0.030667755752801895 + 0.001 * 9.330618858337402
Epoch 730, val loss: 1.3531984090805054
Epoch 740, training loss: 0.038190729916095734 = 0.02887474186718464 + 0.001 * 9.315987586975098
Epoch 740, val loss: 1.3753730058670044
Epoch 750, training loss: 0.036526888608932495 = 0.027228085324168205 + 0.001 * 9.298802375793457
Epoch 750, val loss: 1.4015710353851318
Epoch 760, training loss: 0.035000018775463104 = 0.025693034753203392 + 0.001 * 9.306984901428223
Epoch 760, val loss: 1.4280290603637695
Epoch 770, training loss: 0.03352081775665283 = 0.024204423651099205 + 0.001 * 9.316394805908203
Epoch 770, val loss: 1.4504739046096802
Epoch 780, training loss: 0.03209434449672699 = 0.022800559177994728 + 0.001 * 9.293787002563477
Epoch 780, val loss: 1.476070523262024
Epoch 790, training loss: 0.03079628199338913 = 0.021501457318663597 + 0.001 * 9.294824600219727
Epoch 790, val loss: 1.4997575283050537
Epoch 800, training loss: 0.029570017009973526 = 0.020270349457859993 + 0.001 * 9.299668312072754
Epoch 800, val loss: 1.5224031209945679
Epoch 810, training loss: 0.028406422585248947 = 0.019111299887299538 + 0.001 * 9.295123100280762
Epoch 810, val loss: 1.5464469194412231
Epoch 820, training loss: 0.02730736881494522 = 0.01800738275051117 + 0.001 * 9.299986839294434
Epoch 820, val loss: 1.5683567523956299
Epoch 830, training loss: 0.026274312287569046 = 0.01697169803082943 + 0.001 * 9.302614212036133
Epoch 830, val loss: 1.5902000665664673
Epoch 840, training loss: 0.0252880547195673 = 0.015995558351278305 + 0.001 * 9.292495727539062
Epoch 840, val loss: 1.6136614084243774
Epoch 850, training loss: 0.024358894675970078 = 0.015063847415149212 + 0.001 * 9.295045852661133
Epoch 850, val loss: 1.6348696947097778
Epoch 860, training loss: 0.02350153587758541 = 0.014187264256179333 + 0.001 * 9.314270973205566
Epoch 860, val loss: 1.6576478481292725
Epoch 870, training loss: 0.022647880017757416 = 0.013357941061258316 + 0.001 * 9.289939880371094
Epoch 870, val loss: 1.6783645153045654
Epoch 880, training loss: 0.02188238501548767 = 0.012580855749547482 + 0.001 * 9.301527976989746
Epoch 880, val loss: 1.6992467641830444
Epoch 890, training loss: 0.021166767925024033 = 0.011853272095322609 + 0.001 * 9.313495635986328
Epoch 890, val loss: 1.7191451787948608
Epoch 900, training loss: 0.020461510866880417 = 0.011171205900609493 + 0.001 * 9.290304183959961
Epoch 900, val loss: 1.7399824857711792
Epoch 910, training loss: 0.019839808344841003 = 0.010534605011343956 + 0.001 * 9.305203437805176
Epoch 910, val loss: 1.760084867477417
Epoch 920, training loss: 0.01923004910349846 = 0.009946071542799473 + 0.001 * 9.283976554870605
Epoch 920, val loss: 1.7787641286849976
Epoch 930, training loss: 0.018710575997829437 = 0.009397773072123528 + 0.001 * 9.3128023147583
Epoch 930, val loss: 1.7975645065307617
Epoch 940, training loss: 0.01820545457303524 = 0.00889147724956274 + 0.001 * 9.313977241516113
Epoch 940, val loss: 1.8164942264556885
Epoch 950, training loss: 0.017720896750688553 = 0.008424747735261917 + 0.001 * 9.296147346496582
Epoch 950, val loss: 1.8348057270050049
Epoch 960, training loss: 0.017300114035606384 = 0.007985846139490604 + 0.001 * 9.3142671585083
Epoch 960, val loss: 1.8523962497711182
Epoch 970, training loss: 0.016874009743332863 = 0.007583076599985361 + 0.001 * 9.290932655334473
Epoch 970, val loss: 1.8698533773422241
Epoch 980, training loss: 0.016480455175042152 = 0.007203476503491402 + 0.001 * 9.276978492736816
Epoch 980, val loss: 1.8864120244979858
Epoch 990, training loss: 0.01615072973072529 = 0.006852428894490004 + 0.001 * 9.298300743103027
Epoch 990, val loss: 1.9029481410980225
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8422
Overall ASR: 0.6729
Flip ASR: 0.5927/1554 nodes
The final ASR:0.66954, 0.01920, Accuracy:0.84154, 0.00063
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97498])
remove edge: torch.Size([2, 79878])
updated graph: torch.Size([2, 88728])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.7003
Flip ASR: 0.6293/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7388
Flip ASR: 0.6763/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8493
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72143, 0.01595, Accuracy:0.85050, 0.00104
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.106103777885437 = 1.0957448482513428 + 0.001 * 10.358981132507324
Epoch 0, val loss: 1.0942052602767944
Epoch 10, training loss: 1.0940864086151123 = 1.0837277173995972 + 0.001 * 10.358733177185059
Epoch 10, val loss: 1.0820555686950684
Epoch 20, training loss: 1.0755573511123657 = 1.0651994943618774 + 0.001 * 10.357873916625977
Epoch 20, val loss: 1.0631842613220215
Epoch 30, training loss: 1.045953631401062 = 1.0355987548828125 + 0.001 * 10.35488510131836
Epoch 30, val loss: 1.0336772203445435
Epoch 40, training loss: 1.0055469274520874 = 0.9952071905136108 + 0.001 * 10.33971881866455
Epoch 40, val loss: 0.993613064289093
Epoch 50, training loss: 0.9405534863471985 = 0.9303635358810425 + 0.001 * 10.189974784851074
Epoch 50, val loss: 0.9273719787597656
Epoch 60, training loss: 0.8424455523490906 = 0.8327473998069763 + 0.001 * 9.698172569274902
Epoch 60, val loss: 0.8301948308944702
Epoch 70, training loss: 0.7199715375900269 = 0.7104073762893677 + 0.001 * 9.564189910888672
Epoch 70, val loss: 0.7114933729171753
Epoch 80, training loss: 0.5934468507766724 = 0.5839496850967407 + 0.001 * 9.497180938720703
Epoch 80, val loss: 0.5947737693786621
Epoch 90, training loss: 0.48961204290390015 = 0.4801173210144043 + 0.001 * 9.494721412658691
Epoch 90, val loss: 0.5065621733665466
Epoch 100, training loss: 0.4222308099269867 = 0.4127415120601654 + 0.001 * 9.489310264587402
Epoch 100, val loss: 0.4560467302799225
Epoch 110, training loss: 0.3838326334953308 = 0.3743498921394348 + 0.001 * 9.482732772827148
Epoch 110, val loss: 0.433254599571228
Epoch 120, training loss: 0.3595227599143982 = 0.35003983974456787 + 0.001 * 9.482919692993164
Epoch 120, val loss: 0.42263174057006836
Epoch 130, training loss: 0.34109804034233093 = 0.33161500096321106 + 0.001 * 9.48302936553955
Epoch 130, val loss: 0.4155612289905548
Epoch 140, training loss: 0.3260433077812195 = 0.3165595233440399 + 0.001 * 9.483796119689941
Epoch 140, val loss: 0.41013118624687195
Epoch 150, training loss: 0.31322088837623596 = 0.30373671650886536 + 0.001 * 9.484161376953125
Epoch 150, val loss: 0.4064522981643677
Epoch 160, training loss: 0.3018932044506073 = 0.2924097776412964 + 0.001 * 9.48343276977539
Epoch 160, val loss: 0.40451326966285706
Epoch 170, training loss: 0.2916150689125061 = 0.28213343024253845 + 0.001 * 9.481647491455078
Epoch 170, val loss: 0.403908371925354
Epoch 180, training loss: 0.28211545944213867 = 0.2726362645626068 + 0.001 * 9.479180335998535
Epoch 180, val loss: 0.4045109152793884
Epoch 190, training loss: 0.2731952369213104 = 0.2637175917625427 + 0.001 * 9.477649688720703
Epoch 190, val loss: 0.40610331296920776
Epoch 200, training loss: 0.2647349238395691 = 0.2552562355995178 + 0.001 * 9.478672981262207
Epoch 200, val loss: 0.40873241424560547
Epoch 210, training loss: 0.2566319704055786 = 0.24715211987495422 + 0.001 * 9.479852676391602
Epoch 210, val loss: 0.4122495651245117
Epoch 220, training loss: 0.24883578717708588 = 0.23935577273368835 + 0.001 * 9.480011940002441
Epoch 220, val loss: 0.416668564081192
Epoch 230, training loss: 0.2413126528263092 = 0.23183339834213257 + 0.001 * 9.479260444641113
Epoch 230, val loss: 0.42198318243026733
Epoch 240, training loss: 0.23404666781425476 = 0.22457048296928406 + 0.001 * 9.476179122924805
Epoch 240, val loss: 0.4283212125301361
Epoch 250, training loss: 0.22702723741531372 = 0.21755766868591309 + 0.001 * 9.469568252563477
Epoch 250, val loss: 0.43559715151786804
Epoch 260, training loss: 0.22024378180503845 = 0.21077150106430054 + 0.001 * 9.472272872924805
Epoch 260, val loss: 0.44372111558914185
Epoch 270, training loss: 0.21371959149837494 = 0.20425350964069366 + 0.001 * 9.466076850891113
Epoch 270, val loss: 0.4538383483886719
Epoch 280, training loss: 0.20744679868221283 = 0.19798392057418823 + 0.001 * 9.462882041931152
Epoch 280, val loss: 0.46307581663131714
Epoch 290, training loss: 0.2014252096414566 = 0.19195792078971863 + 0.001 * 9.467288970947266
Epoch 290, val loss: 0.4723300337791443
Epoch 300, training loss: 0.1956089735031128 = 0.18614153563976288 + 0.001 * 9.46743392944336
Epoch 300, val loss: 0.48467302322387695
Epoch 310, training loss: 0.1899964064359665 = 0.18052588403224945 + 0.001 * 9.470520973205566
Epoch 310, val loss: 0.49347764253616333
Epoch 320, training loss: 0.18449844419956207 = 0.17503571510314941 + 0.001 * 9.462733268737793
Epoch 320, val loss: 0.5066676735877991
Epoch 330, training loss: 0.17917469143867493 = 0.16971434652805328 + 0.001 * 9.460338592529297
Epoch 330, val loss: 0.516872763633728
Epoch 340, training loss: 0.17393846809864044 = 0.16445142030715942 + 0.001 * 9.487048149108887
Epoch 340, val loss: 0.5321598052978516
Epoch 350, training loss: 0.1687246710062027 = 0.15928302705287933 + 0.001 * 9.441642761230469
Epoch 350, val loss: 0.5427300333976746
Epoch 360, training loss: 0.1636441946029663 = 0.1542024463415146 + 0.001 * 9.441740989685059
Epoch 360, val loss: 0.5580974221229553
Epoch 370, training loss: 0.158720001578331 = 0.14926840364933014 + 0.001 * 9.451594352722168
Epoch 370, val loss: 0.5714879631996155
Epoch 380, training loss: 0.15384173393249512 = 0.14441369473934174 + 0.001 * 9.428040504455566
Epoch 380, val loss: 0.5870134830474854
Epoch 390, training loss: 0.14908699691295624 = 0.13963808119297028 + 0.001 * 9.448922157287598
Epoch 390, val loss: 0.6004972457885742
Epoch 400, training loss: 0.14446815848350525 = 0.13499389588832855 + 0.001 * 9.474255561828613
Epoch 400, val loss: 0.6162286400794983
Epoch 410, training loss: 0.13975708186626434 = 0.13033343851566315 + 0.001 * 9.423636436462402
Epoch 410, val loss: 0.6313934326171875
Epoch 420, training loss: 0.1352480947971344 = 0.1257847249507904 + 0.001 * 9.463370323181152
Epoch 420, val loss: 0.6500964760780334
Epoch 430, training loss: 0.13065105676651 = 0.12127067893743515 + 0.001 * 9.38037395477295
Epoch 430, val loss: 0.6657406091690063
Epoch 440, training loss: 0.1264069378376007 = 0.11701787263154984 + 0.001 * 9.389058113098145
Epoch 440, val loss: 0.6809883117675781
Epoch 450, training loss: 0.12184131145477295 = 0.11244729906320572 + 0.001 * 9.394011497497559
Epoch 450, val loss: 0.7017707824707031
Epoch 460, training loss: 0.11751577258110046 = 0.10813261568546295 + 0.001 * 9.383160591125488
Epoch 460, val loss: 0.7189418077468872
Epoch 470, training loss: 0.11331503838300705 = 0.10396724939346313 + 0.001 * 9.347790718078613
Epoch 470, val loss: 0.743321418762207
Epoch 480, training loss: 0.10907071828842163 = 0.09970616549253464 + 0.001 * 9.364554405212402
Epoch 480, val loss: 0.7562729120254517
Epoch 490, training loss: 0.10487145185470581 = 0.09551596641540527 + 0.001 * 9.35548210144043
Epoch 490, val loss: 0.7821630835533142
Epoch 500, training loss: 0.10075079649686813 = 0.09139988571405411 + 0.001 * 9.350907325744629
Epoch 500, val loss: 0.7975248694419861
Epoch 510, training loss: 0.09659223258495331 = 0.0872572734951973 + 0.001 * 9.33495807647705
Epoch 510, val loss: 0.8248654007911682
Epoch 520, training loss: 0.0925101712346077 = 0.08316811919212341 + 0.001 * 9.34205436706543
Epoch 520, val loss: 0.8434624075889587
Epoch 530, training loss: 0.08874140679836273 = 0.07938619703054428 + 0.001 * 9.355205535888672
Epoch 530, val loss: 0.8649808168411255
Epoch 540, training loss: 0.08488479256629944 = 0.07555798441171646 + 0.001 * 9.32680892944336
Epoch 540, val loss: 0.8917669057846069
Epoch 550, training loss: 0.08124902099370956 = 0.07192761451005936 + 0.001 * 9.321407318115234
Epoch 550, val loss: 0.9101378321647644
Epoch 560, training loss: 0.07759284973144531 = 0.06826791912317276 + 0.001 * 9.324926376342773
Epoch 560, val loss: 0.9411723613739014
Epoch 570, training loss: 0.07404778152704239 = 0.06470692157745361 + 0.001 * 9.34085750579834
Epoch 570, val loss: 0.9596356749534607
Epoch 580, training loss: 0.07065653055906296 = 0.061329927295446396 + 0.001 * 9.326605796813965
Epoch 580, val loss: 0.9873679876327515
Epoch 590, training loss: 0.06757098436355591 = 0.05825630947947502 + 0.001 * 9.314672470092773
Epoch 590, val loss: 1.0144951343536377
Epoch 600, training loss: 0.06446752697229385 = 0.055144116282463074 + 0.001 * 9.323410034179688
Epoch 600, val loss: 1.0342973470687866
Epoch 610, training loss: 0.061472464352846146 = 0.05215376988053322 + 0.001 * 9.318695068359375
Epoch 610, val loss: 1.0661646127700806
Epoch 620, training loss: 0.05851585045456886 = 0.0491984561085701 + 0.001 * 9.31739330291748
Epoch 620, val loss: 1.0864201784133911
Epoch 630, training loss: 0.05578554421663284 = 0.046477802097797394 + 0.001 * 9.307741165161133
Epoch 630, val loss: 1.1132230758666992
Epoch 640, training loss: 0.05328759923577309 = 0.04396883025765419 + 0.001 * 9.318767547607422
Epoch 640, val loss: 1.1436469554901123
Epoch 650, training loss: 0.05070502683520317 = 0.04140716791152954 + 0.001 * 9.297858238220215
Epoch 650, val loss: 1.163482427597046
Epoch 660, training loss: 0.04830892011523247 = 0.03898676857352257 + 0.001 * 9.322152137756348
Epoch 660, val loss: 1.1930491924285889
Epoch 670, training loss: 0.04609810188412666 = 0.03678414970636368 + 0.001 * 9.31395149230957
Epoch 670, val loss: 1.2197359800338745
Epoch 680, training loss: 0.04397372901439667 = 0.034668680280447006 + 0.001 * 9.305047035217285
Epoch 680, val loss: 1.2414474487304688
Epoch 690, training loss: 0.04189932346343994 = 0.0326019711792469 + 0.001 * 9.297350883483887
Epoch 690, val loss: 1.271365761756897
Epoch 700, training loss: 0.03998933732509613 = 0.030678320676088333 + 0.001 * 9.311016082763672
Epoch 700, val loss: 1.2947977781295776
Epoch 710, training loss: 0.03821093589067459 = 0.028908584266901016 + 0.001 * 9.302349090576172
Epoch 710, val loss: 1.318724274635315
Epoch 720, training loss: 0.03649928793311119 = 0.027203261852264404 + 0.001 * 9.296025276184082
Epoch 720, val loss: 1.3475865125656128
Epoch 730, training loss: 0.034866005182266235 = 0.025573354214429855 + 0.001 * 9.292649269104004
Epoch 730, val loss: 1.3700159788131714
Epoch 740, training loss: 0.03336739540100098 = 0.024053730070590973 + 0.001 * 9.313665390014648
Epoch 740, val loss: 1.3962398767471313
Epoch 750, training loss: 0.0319664292037487 = 0.022664470598101616 + 0.001 * 9.301957130432129
Epoch 750, val loss: 1.423065423965454
Epoch 760, training loss: 0.030622567981481552 = 0.021302929148077965 + 0.001 * 9.3196382522583
Epoch 760, val loss: 1.443968415260315
Epoch 770, training loss: 0.029341036453843117 = 0.02003629133105278 + 0.001 * 9.304744720458984
Epoch 770, val loss: 1.4707704782485962
Epoch 780, training loss: 0.028132103383541107 = 0.018849510699510574 + 0.001 * 9.282590866088867
Epoch 780, val loss: 1.4917235374450684
Epoch 790, training loss: 0.027033699676394463 = 0.0177325326949358 + 0.001 * 9.301166534423828
Epoch 790, val loss: 1.5175014734268188
Epoch 800, training loss: 0.02597581222653389 = 0.016673533245921135 + 0.001 * 9.302279472351074
Epoch 800, val loss: 1.5389515161514282
Epoch 810, training loss: 0.024972522631287575 = 0.01568147912621498 + 0.001 * 9.291043281555176
Epoch 810, val loss: 1.5624912977218628
Epoch 820, training loss: 0.024041082710027695 = 0.014753935858607292 + 0.001 * 9.287147521972656
Epoch 820, val loss: 1.5833443403244019
Epoch 830, training loss: 0.02317698299884796 = 0.013878400437533855 + 0.001 * 9.298583030700684
Epoch 830, val loss: 1.6068463325500488
Epoch 840, training loss: 0.022336900234222412 = 0.013056082651019096 + 0.001 * 9.280817031860352
Epoch 840, val loss: 1.6270489692687988
Epoch 850, training loss: 0.021555578336119652 = 0.01228653360158205 + 0.001 * 9.269043922424316
Epoch 850, val loss: 1.6494661569595337
Epoch 860, training loss: 0.02086779847741127 = 0.011567064560949802 + 0.001 * 9.300732612609863
Epoch 860, val loss: 1.6705162525177002
Epoch 870, training loss: 0.020200200378894806 = 0.01090607326477766 + 0.001 * 9.2941255569458
Epoch 870, val loss: 1.689635992050171
Epoch 880, training loss: 0.019566990435123444 = 0.010286634787917137 + 0.001 * 9.280356407165527
Epoch 880, val loss: 1.7102538347244263
Epoch 890, training loss: 0.01900830864906311 = 0.009714607149362564 + 0.001 * 9.293701171875
Epoch 890, val loss: 1.7302451133728027
Epoch 900, training loss: 0.01846352592110634 = 0.009182902052998543 + 0.001 * 9.280622482299805
Epoch 900, val loss: 1.7493888139724731
Epoch 910, training loss: 0.017963368445634842 = 0.008690441027283669 + 0.001 * 9.272927284240723
Epoch 910, val loss: 1.7677010297775269
Epoch 920, training loss: 0.017499681562185287 = 0.008233131840825081 + 0.001 * 9.266548156738281
Epoch 920, val loss: 1.785927176475525
Epoch 930, training loss: 0.017080355435609818 = 0.007810236886143684 + 0.001 * 9.270118713378906
Epoch 930, val loss: 1.8040809631347656
Epoch 940, training loss: 0.01669096201658249 = 0.0074161626398563385 + 0.001 * 9.274799346923828
Epoch 940, val loss: 1.821654200553894
Epoch 950, training loss: 0.01631329208612442 = 0.007047680672258139 + 0.001 * 9.265610694885254
Epoch 950, val loss: 1.8384264707565308
Epoch 960, training loss: 0.015972629189491272 = 0.006705468986183405 + 0.001 * 9.267160415649414
Epoch 960, val loss: 1.8547977209091187
Epoch 970, training loss: 0.015663977712392807 = 0.006387013476341963 + 0.001 * 9.27696418762207
Epoch 970, val loss: 1.8709617853164673
Epoch 980, training loss: 0.015355756506323814 = 0.0060903397388756275 + 0.001 * 9.265417098999023
Epoch 980, val loss: 1.886866807937622
Epoch 990, training loss: 0.015116212889552116 = 0.00581330107524991 + 0.001 * 9.302910804748535
Epoch 990, val loss: 1.9018418788909912
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8412
Overall ASR: 0.6572
Flip ASR: 0.5746/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1288751363754272 = 1.1185160875320435 + 0.001 * 10.359010696411133
Epoch 0, val loss: 1.1169434785842896
Epoch 10, training loss: 1.1135008335113525 = 1.1031420230865479 + 0.001 * 10.358848571777344
Epoch 10, val loss: 1.1013031005859375
Epoch 20, training loss: 1.0906014442443848 = 1.0802432298660278 + 0.001 * 10.35818099975586
Epoch 20, val loss: 1.0776326656341553
Epoch 30, training loss: 1.055513858795166 = 1.0451581478118896 + 0.001 * 10.355724334716797
Epoch 30, val loss: 1.0422556400299072
Epoch 40, training loss: 1.0163730382919312 = 1.006030559539795 + 0.001 * 10.342516899108887
Epoch 40, val loss: 1.0042897462844849
Epoch 50, training loss: 0.963388204574585 = 0.953154981136322 + 0.001 * 10.233199119567871
Epoch 50, val loss: 0.9500643610954285
Epoch 60, training loss: 0.8810607194900513 = 0.8712533712387085 + 0.001 * 9.807353019714355
Epoch 60, val loss: 0.8678277134895325
Epoch 70, training loss: 0.7773476839065552 = 0.7676233649253845 + 0.001 * 9.724313735961914
Epoch 70, val loss: 0.7662798762321472
Epoch 80, training loss: 0.664106547832489 = 0.6545050740242004 + 0.001 * 9.601449966430664
Epoch 80, val loss: 0.6589035391807556
Epoch 90, training loss: 0.5552037358283997 = 0.5457018613815308 + 0.001 * 9.501885414123535
Epoch 90, val loss: 0.5610433220863342
Epoch 100, training loss: 0.46919721364974976 = 0.4596922695636749 + 0.001 * 9.504953384399414
Epoch 100, val loss: 0.48896196484565735
Epoch 110, training loss: 0.4129822254180908 = 0.40349042415618896 + 0.001 * 9.49178695678711
Epoch 110, val loss: 0.4473016560077667
Epoch 120, training loss: 0.3794814944267273 = 0.3699985444545746 + 0.001 * 9.482964515686035
Epoch 120, val loss: 0.4276287853717804
Epoch 130, training loss: 0.357047438621521 = 0.34756720066070557 + 0.001 * 9.480247497558594
Epoch 130, val loss: 0.41721630096435547
Epoch 140, training loss: 0.3395693600177765 = 0.33009234070777893 + 0.001 * 9.477011680603027
Epoch 140, val loss: 0.4099139869213104
Epoch 150, training loss: 0.32515859603881836 = 0.315683513879776 + 0.001 * 9.475075721740723
Epoch 150, val loss: 0.40446096658706665
Epoch 160, training loss: 0.31281691789627075 = 0.3033440411090851 + 0.001 * 9.472885131835938
Epoch 160, val loss: 0.4009351432323456
Epoch 170, training loss: 0.3018580675125122 = 0.2923871874809265 + 0.001 * 9.470876693725586
Epoch 170, val loss: 0.3992471694946289
Epoch 180, training loss: 0.291871577501297 = 0.2824019491672516 + 0.001 * 9.469618797302246
Epoch 180, val loss: 0.39889031648635864
Epoch 190, training loss: 0.28261616826057434 = 0.2731461822986603 + 0.001 * 9.469980239868164
Epoch 190, val loss: 0.3996208906173706
Epoch 200, training loss: 0.27391383051872253 = 0.2644430696964264 + 0.001 * 9.470759391784668
Epoch 200, val loss: 0.4013524651527405
Epoch 210, training loss: 0.26564326882362366 = 0.2561723291873932 + 0.001 * 9.470951080322266
Epoch 210, val loss: 0.4040675163269043
Epoch 220, training loss: 0.2577058970928192 = 0.24823515117168427 + 0.001 * 9.470747947692871
Epoch 220, val loss: 0.40772294998168945
Epoch 230, training loss: 0.250053346157074 = 0.24058367311954498 + 0.001 * 9.4696626663208
Epoch 230, val loss: 0.41224414110183716
Epoch 240, training loss: 0.24264594912528992 = 0.23317919671535492 + 0.001 * 9.466756820678711
Epoch 240, val loss: 0.4176464080810547
Epoch 250, training loss: 0.23544776439666748 = 0.22598733007907867 + 0.001 * 9.460441589355469
Epoch 250, val loss: 0.42388930916786194
Epoch 260, training loss: 0.22847238183021545 = 0.21899397671222687 + 0.001 * 9.478409767150879
Epoch 260, val loss: 0.43095117807388306
Epoch 270, training loss: 0.22169509530067444 = 0.21223580837249756 + 0.001 * 9.459294319152832
Epoch 270, val loss: 0.43917974829673767
Epoch 280, training loss: 0.21521712839603424 = 0.20576177537441254 + 0.001 * 9.455351829528809
Epoch 280, val loss: 0.4484953284263611
Epoch 290, training loss: 0.2090291529893875 = 0.19957728683948517 + 0.001 * 9.451862335205078
Epoch 290, val loss: 0.45869606733322144
Epoch 300, training loss: 0.20309698581695557 = 0.1936444789171219 + 0.001 * 9.45250129699707
Epoch 300, val loss: 0.46938464045524597
Epoch 310, training loss: 0.19754666090011597 = 0.18809416890144348 + 0.001 * 9.452488899230957
Epoch 310, val loss: 0.48186880350112915
Epoch 320, training loss: 0.19205623865127563 = 0.1826099008321762 + 0.001 * 9.446330070495605
Epoch 320, val loss: 0.4915812015533447
Epoch 330, training loss: 0.1868511438369751 = 0.17741741240024567 + 0.001 * 9.433737754821777
Epoch 330, val loss: 0.5038827657699585
Epoch 340, training loss: 0.181739941239357 = 0.17230679094791412 + 0.001 * 9.433155059814453
Epoch 340, val loss: 0.516595184803009
Epoch 350, training loss: 0.17669296264648438 = 0.16727493703365326 + 0.001 * 9.418027877807617
Epoch 350, val loss: 0.5302948951721191
Epoch 360, training loss: 0.17172203958034515 = 0.1623106300830841 + 0.001 * 9.411402702331543
Epoch 360, val loss: 0.5430421829223633
Epoch 370, training loss: 0.16709661483764648 = 0.1576656550168991 + 0.001 * 9.430964469909668
Epoch 370, val loss: 0.5596737861633301
Epoch 380, training loss: 0.16227850317955017 = 0.1528659164905548 + 0.001 * 9.412586212158203
Epoch 380, val loss: 0.5715081095695496
Epoch 390, training loss: 0.1576809287071228 = 0.14827802777290344 + 0.001 * 9.40289306640625
Epoch 390, val loss: 0.5855448842048645
Epoch 400, training loss: 0.15317976474761963 = 0.14377327263355255 + 0.001 * 9.406490325927734
Epoch 400, val loss: 0.601141095161438
Epoch 410, training loss: 0.14872772991657257 = 0.1392894834280014 + 0.001 * 9.438251495361328
Epoch 410, val loss: 0.6148569583892822
Epoch 420, training loss: 0.14422738552093506 = 0.13482120633125305 + 0.001 * 9.406173706054688
Epoch 420, val loss: 0.6311105489730835
Epoch 430, training loss: 0.14001065492630005 = 0.1306096911430359 + 0.001 * 9.400956153869629
Epoch 430, val loss: 0.6491286158561707
Epoch 440, training loss: 0.13549049198627472 = 0.1261131763458252 + 0.001 * 9.37731647491455
Epoch 440, val loss: 0.6628245711326599
Epoch 450, training loss: 0.13116982579231262 = 0.12177985906600952 + 0.001 * 9.389958381652832
Epoch 450, val loss: 0.6803950071334839
Epoch 460, training loss: 0.12700466811656952 = 0.11763880401849747 + 0.001 * 9.36586856842041
Epoch 460, val loss: 0.6954230666160583
Epoch 470, training loss: 0.12274180352687836 = 0.11336615681648254 + 0.001 * 9.375650405883789
Epoch 470, val loss: 0.7174665331840515
Epoch 480, training loss: 0.11843353509902954 = 0.1090645119547844 + 0.001 * 9.369023323059082
Epoch 480, val loss: 0.7314850687980652
Epoch 490, training loss: 0.11419357359409332 = 0.10481903702020645 + 0.001 * 9.374533653259277
Epoch 490, val loss: 0.7535650134086609
Epoch 500, training loss: 0.10999645292758942 = 0.10064123570919037 + 0.001 * 9.35521411895752
Epoch 500, val loss: 0.7716665863990784
Epoch 510, training loss: 0.10610304772853851 = 0.09674257040023804 + 0.001 * 9.360480308532715
Epoch 510, val loss: 0.7895654439926147
Epoch 520, training loss: 0.10190044343471527 = 0.09255664795637131 + 0.001 * 9.343796730041504
Epoch 520, val loss: 0.8138436079025269
Epoch 530, training loss: 0.09791553765535355 = 0.08857810497283936 + 0.001 * 9.337434768676758
Epoch 530, val loss: 0.8316605091094971
Epoch 540, training loss: 0.09398474544286728 = 0.08463016897439957 + 0.001 * 9.354573249816895
Epoch 540, val loss: 0.8580470085144043
Epoch 550, training loss: 0.09003741294145584 = 0.08068685978651047 + 0.001 * 9.35055160522461
Epoch 550, val loss: 0.8754960298538208
Epoch 560, training loss: 0.08627358824014664 = 0.0768982544541359 + 0.001 * 9.375334739685059
Epoch 560, val loss: 0.9014303684234619
Epoch 570, training loss: 0.08260302245616913 = 0.0732414498925209 + 0.001 * 9.361568450927734
Epoch 570, val loss: 0.9237411022186279
Epoch 580, training loss: 0.07917779684066772 = 0.06983989477157593 + 0.001 * 9.337900161743164
Epoch 580, val loss: 0.9439936876296997
Epoch 590, training loss: 0.07563354074954987 = 0.0663105919957161 + 0.001 * 9.32295036315918
Epoch 590, val loss: 0.9740078449249268
Epoch 600, training loss: 0.07212541997432709 = 0.06278903782367706 + 0.001 * 9.336383819580078
Epoch 600, val loss: 0.9936680197715759
Epoch 610, training loss: 0.06885986775159836 = 0.059531353414058685 + 0.001 * 9.328514099121094
Epoch 610, val loss: 1.0193777084350586
Epoch 620, training loss: 0.06570141017436981 = 0.056376345455646515 + 0.001 * 9.325061798095703
Epoch 620, val loss: 1.0460647344589233
Epoch 630, training loss: 0.06273828446865082 = 0.05341967195272446 + 0.001 * 9.31861400604248
Epoch 630, val loss: 1.0678415298461914
Epoch 640, training loss: 0.05971841886639595 = 0.05038391798734665 + 0.001 * 9.334501266479492
Epoch 640, val loss: 1.098564863204956
Epoch 650, training loss: 0.05682751536369324 = 0.047512236982584 + 0.001 * 9.315276145935059
Epoch 650, val loss: 1.1219449043273926
Epoch 660, training loss: 0.05421482399106026 = 0.04488610848784447 + 0.001 * 9.328715324401855
Epoch 660, val loss: 1.1463367938995361
Epoch 670, training loss: 0.05166875198483467 = 0.0423300638794899 + 0.001 * 9.338688850402832
Epoch 670, val loss: 1.1769965887069702
Epoch 680, training loss: 0.04912310838699341 = 0.03980015218257904 + 0.001 * 9.322954177856445
Epoch 680, val loss: 1.1985993385314941
Epoch 690, training loss: 0.046829868108034134 = 0.037479788064956665 + 0.001 * 9.350079536437988
Epoch 690, val loss: 1.2259371280670166
Epoch 700, training loss: 0.04467836022377014 = 0.03535626828670502 + 0.001 * 9.32209300994873
Epoch 700, val loss: 1.2549775838851929
Epoch 710, training loss: 0.0425291582942009 = 0.033221110701560974 + 0.001 * 9.308045387268066
Epoch 710, val loss: 1.275498390197754
Epoch 720, training loss: 0.04053153470158577 = 0.03122149035334587 + 0.001 * 9.31004524230957
Epoch 720, val loss: 1.303915023803711
Epoch 730, training loss: 0.038675934076309204 = 0.02936708554625511 + 0.001 * 9.308846473693848
Epoch 730, val loss: 1.3285505771636963
Epoch 740, training loss: 0.03691866248846054 = 0.02761920541524887 + 0.001 * 9.299456596374512
Epoch 740, val loss: 1.3533087968826294
Epoch 750, training loss: 0.03529181331396103 = 0.025985732674598694 + 0.001 * 9.306081771850586
Epoch 750, val loss: 1.3799803256988525
Epoch 760, training loss: 0.033729247748851776 = 0.02440851740539074 + 0.001 * 9.320732116699219
Epoch 760, val loss: 1.4019393920898438
Epoch 770, training loss: 0.03223566710948944 = 0.022933118045330048 + 0.001 * 9.302547454833984
Epoch 770, val loss: 1.4266507625579834
Epoch 780, training loss: 0.030865585431456566 = 0.021559463813900948 + 0.001 * 9.306120872497559
Epoch 780, val loss: 1.451961874961853
Epoch 790, training loss: 0.029541121795773506 = 0.020238211378455162 + 0.001 * 9.302909851074219
Epoch 790, val loss: 1.474416732788086
Epoch 800, training loss: 0.028323456645011902 = 0.019006717950105667 + 0.001 * 9.316739082336426
Epoch 800, val loss: 1.4979515075683594
Epoch 810, training loss: 0.027144689112901688 = 0.01785174384713173 + 0.001 * 9.29294490814209
Epoch 810, val loss: 1.5217046737670898
Epoch 820, training loss: 0.026048103347420692 = 0.016755029559135437 + 0.001 * 9.293073654174805
Epoch 820, val loss: 1.542549967765808
Epoch 830, training loss: 0.02501392923295498 = 0.015715349465608597 + 0.001 * 9.298579216003418
Epoch 830, val loss: 1.5658055543899536
Epoch 840, training loss: 0.0240501519292593 = 0.01474481076002121 + 0.001 * 9.305340766906738
Epoch 840, val loss: 1.587570309638977
Epoch 850, training loss: 0.023147102445364 = 0.01384052075445652 + 0.001 * 9.306580543518066
Epoch 850, val loss: 1.6089967489242554
Epoch 860, training loss: 0.022279836237430573 = 0.012989278882741928 + 0.001 * 9.290555953979492
Epoch 860, val loss: 1.6308825016021729
Epoch 870, training loss: 0.021493326872587204 = 0.0121989194303751 + 0.001 * 9.29440689086914
Epoch 870, val loss: 1.6514872312545776
Epoch 880, training loss: 0.020770810544490814 = 0.011468217708170414 + 0.001 * 9.302591323852539
Epoch 880, val loss: 1.6723538637161255
Epoch 890, training loss: 0.02013709954917431 = 0.010791059583425522 + 0.001 * 9.346039772033691
Epoch 890, val loss: 1.6916593313217163
Epoch 900, training loss: 0.019473455846309662 = 0.010169136337935925 + 0.001 * 9.30431842803955
Epoch 900, val loss: 1.7112468481063843
Epoch 910, training loss: 0.018891848623752594 = 0.009593005292117596 + 0.001 * 9.298842430114746
Epoch 910, val loss: 1.7303200960159302
Epoch 920, training loss: 0.01834113709628582 = 0.009060814045369625 + 0.001 * 9.280323028564453
Epoch 920, val loss: 1.749145269393921
Epoch 930, training loss: 0.017860375344753265 = 0.008567102253437042 + 0.001 * 9.293272018432617
Epoch 930, val loss: 1.7675213813781738
Epoch 940, training loss: 0.017393777146935463 = 0.008109907619655132 + 0.001 * 9.283868789672852
Epoch 940, val loss: 1.7854814529418945
Epoch 950, training loss: 0.016981080174446106 = 0.007684850599616766 + 0.001 * 9.296228408813477
Epoch 950, val loss: 1.8032459020614624
Epoch 960, training loss: 0.016588041558861732 = 0.00729161174967885 + 0.001 * 9.296429634094238
Epoch 960, val loss: 1.8204280138015747
Epoch 970, training loss: 0.01624133624136448 = 0.006925666239112616 + 0.001 * 9.315670013427734
Epoch 970, val loss: 1.836984634399414
Epoch 980, training loss: 0.01587444171309471 = 0.00658554257825017 + 0.001 * 9.288898468017578
Epoch 980, val loss: 1.8535622358322144
Epoch 990, training loss: 0.015577257610857487 = 0.006269127130508423 + 0.001 * 9.308130264282227
Epoch 990, val loss: 1.869181752204895
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8427
Overall ASR: 0.6729
Flip ASR: 0.5940/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.0974361896514893 = 1.087077260017395 + 0.001 * 10.35898494720459
Epoch 0, val loss: 1.0860471725463867
Epoch 10, training loss: 1.0872318744659424 = 1.0768730640411377 + 0.001 * 10.358810424804688
Epoch 10, val loss: 1.0757673978805542
Epoch 20, training loss: 1.0718355178833008 = 1.0614774227142334 + 0.001 * 10.358085632324219
Epoch 20, val loss: 1.0599690675735474
Epoch 30, training loss: 1.0464658737182617 = 1.0361104011535645 + 0.001 * 10.355509757995605
Epoch 30, val loss: 1.0341671705245972
Epoch 40, training loss: 1.0080550909042358 = 0.9977126121520996 + 0.001 * 10.34249210357666
Epoch 40, val loss: 0.9950257539749146
Epoch 50, training loss: 0.9426351189613342 = 0.9324212074279785 + 0.001 * 10.213896751403809
Epoch 50, val loss: 0.9279733896255493
Epoch 60, training loss: 0.8449890613555908 = 0.8352248668670654 + 0.001 * 9.764191627502441
Epoch 60, val loss: 0.8306314945220947
Epoch 70, training loss: 0.7258782982826233 = 0.7161805033683777 + 0.001 * 9.697803497314453
Epoch 70, val loss: 0.7158626317977905
Epoch 80, training loss: 0.6073942184448242 = 0.5978119373321533 + 0.001 * 9.582260131835938
Epoch 80, val loss: 0.6071286797523499
Epoch 90, training loss: 0.5100347399711609 = 0.500548243522644 + 0.001 * 9.486506462097168
Epoch 90, val loss: 0.5228040814399719
Epoch 100, training loss: 0.4401148557662964 = 0.4306216239929199 + 0.001 * 9.493230819702148
Epoch 100, val loss: 0.4672818183898926
Epoch 110, training loss: 0.3960922658443451 = 0.38661205768585205 + 0.001 * 9.480204582214355
Epoch 110, val loss: 0.43801578879356384
Epoch 120, training loss: 0.3685779273509979 = 0.35910511016845703 + 0.001 * 9.47282886505127
Epoch 120, val loss: 0.42442786693573
Epoch 130, training loss: 0.3485024869441986 = 0.3390309810638428 + 0.001 * 9.471515655517578
Epoch 130, val loss: 0.41625267267227173
Epoch 140, training loss: 0.3322974443435669 = 0.32282736897468567 + 0.001 * 9.470075607299805
Epoch 140, val loss: 0.40990889072418213
Epoch 150, training loss: 0.3186757564544678 = 0.3092057704925537 + 0.001 * 9.469998359680176
Epoch 150, val loss: 0.4052030146121979
Epoch 160, training loss: 0.30675581097602844 = 0.29728642106056213 + 0.001 * 9.469396591186523
Epoch 160, val loss: 0.40220507979393005
Epoch 170, training loss: 0.2959863543510437 = 0.286517471075058 + 0.001 * 9.468887329101562
Epoch 170, val loss: 0.40072259306907654
Epoch 180, training loss: 0.28606390953063965 = 0.2765943109989166 + 0.001 * 9.469583511352539
Epoch 180, val loss: 0.400501549243927
Epoch 190, training loss: 0.2768087685108185 = 0.2673366963863373 + 0.001 * 9.47205924987793
Epoch 190, val loss: 0.4014025032520294
Epoch 200, training loss: 0.2680773437023163 = 0.25860241055488586 + 0.001 * 9.474944114685059
Epoch 200, val loss: 0.40336355566978455
Epoch 210, training loss: 0.2597820460796356 = 0.25030457973480225 + 0.001 * 9.477476119995117
Epoch 210, val loss: 0.40638768672943115
Epoch 220, training loss: 0.2518565356731415 = 0.24237678945064545 + 0.001 * 9.479759216308594
Epoch 220, val loss: 0.4104016423225403
Epoch 230, training loss: 0.2442493438720703 = 0.23476794362068176 + 0.001 * 9.481393814086914
Epoch 230, val loss: 0.41537684202194214
Epoch 240, training loss: 0.2369338870048523 = 0.2274520993232727 + 0.001 * 9.481789588928223
Epoch 240, val loss: 0.4213254451751709
Epoch 250, training loss: 0.2299032062292099 = 0.22042374312877655 + 0.001 * 9.47945785522461
Epoch 250, val loss: 0.42832037806510925
Epoch 260, training loss: 0.22318890690803528 = 0.2136916220188141 + 0.001 * 9.497286796569824
Epoch 260, val loss: 0.4364016652107239
Epoch 270, training loss: 0.21673250198364258 = 0.207257941365242 + 0.001 * 9.474566459655762
Epoch 270, val loss: 0.44515642523765564
Epoch 280, training loss: 0.21058231592178345 = 0.2011089026927948 + 0.001 * 9.473407745361328
Epoch 280, val loss: 0.45581865310668945
Epoch 290, training loss: 0.20468908548355103 = 0.19521750509738922 + 0.001 * 9.471575736999512
Epoch 290, val loss: 0.4651303291320801
Epoch 300, training loss: 0.1990150362253189 = 0.1895427405834198 + 0.001 * 9.472294807434082
Epoch 300, val loss: 0.47680777311325073
Epoch 310, training loss: 0.19362442195415497 = 0.18415693938732147 + 0.001 * 9.467487335205078
Epoch 310, val loss: 0.48804381489753723
Epoch 320, training loss: 0.18837681412696838 = 0.17890812456607819 + 0.001 * 9.46868896484375
Epoch 320, val loss: 0.4990485906600952
Epoch 330, training loss: 0.1833447962999344 = 0.17387953400611877 + 0.001 * 9.465255737304688
Epoch 330, val loss: 0.5111726522445679
Epoch 340, training loss: 0.17845910787582397 = 0.16896706819534302 + 0.001 * 9.492035865783691
Epoch 340, val loss: 0.5245000720024109
Epoch 350, training loss: 0.1736060380935669 = 0.1641440987586975 + 0.001 * 9.461938858032227
Epoch 350, val loss: 0.536244809627533
Epoch 360, training loss: 0.16881507635116577 = 0.15936939418315887 + 0.001 * 9.445686340332031
Epoch 360, val loss: 0.5508500933647156
Epoch 370, training loss: 0.1641150265932083 = 0.15468212962150574 + 0.001 * 9.432891845703125
Epoch 370, val loss: 0.5636814832687378
Epoch 380, training loss: 0.159446120262146 = 0.1500188261270523 + 0.001 * 9.427298545837402
Epoch 380, val loss: 0.5783265233039856
Epoch 390, training loss: 0.1549653857946396 = 0.14554274082183838 + 0.001 * 9.42264175415039
Epoch 390, val loss: 0.595424473285675
Epoch 400, training loss: 0.15023666620254517 = 0.14080728590488434 + 0.001 * 9.429381370544434
Epoch 400, val loss: 0.6083887815475464
Epoch 410, training loss: 0.14568769931793213 = 0.13628090918064117 + 0.001 * 9.406794548034668
Epoch 410, val loss: 0.6245701909065247
Epoch 420, training loss: 0.14142504334449768 = 0.1320231556892395 + 0.001 * 9.401893615722656
Epoch 420, val loss: 0.6378753185272217
Epoch 430, training loss: 0.13684754073619843 = 0.12746571004390717 + 0.001 * 9.38182544708252
Epoch 430, val loss: 0.657082736492157
Epoch 440, training loss: 0.13261908292770386 = 0.12321539968252182 + 0.001 * 9.403679847717285
Epoch 440, val loss: 0.675229012966156
Epoch 450, training loss: 0.12832850217819214 = 0.1189480796456337 + 0.001 * 9.380415916442871
Epoch 450, val loss: 0.6938529014587402
Epoch 460, training loss: 0.12406284362077713 = 0.11469241231679916 + 0.001 * 9.370429039001465
Epoch 460, val loss: 0.7096009850502014
Epoch 470, training loss: 0.1199112981557846 = 0.11055122315883636 + 0.001 * 9.360074996948242
Epoch 470, val loss: 0.7283630967140198
Epoch 480, training loss: 0.11566689610481262 = 0.106304831802845 + 0.001 * 9.36206340789795
Epoch 480, val loss: 0.7480806708335876
Epoch 490, training loss: 0.11154073476791382 = 0.10217969864606857 + 0.001 * 9.361031532287598
Epoch 490, val loss: 0.7676635384559631
Epoch 500, training loss: 0.10767508298158646 = 0.09831789135932922 + 0.001 * 9.357192993164062
Epoch 500, val loss: 0.7928140163421631
Epoch 510, training loss: 0.10351212322711945 = 0.09414225071668625 + 0.001 * 9.36987018585205
Epoch 510, val loss: 0.8076809048652649
Epoch 520, training loss: 0.0994759127497673 = 0.09011407941579819 + 0.001 * 9.36182975769043
Epoch 520, val loss: 0.8335798978805542
Epoch 530, training loss: 0.095547154545784 = 0.08619954437017441 + 0.001 * 9.34760570526123
Epoch 530, val loss: 0.8503967523574829
Epoch 540, training loss: 0.09153082966804504 = 0.08219154924154282 + 0.001 * 9.339276313781738
Epoch 540, val loss: 0.8769177794456482
Epoch 550, training loss: 0.08778654038906097 = 0.07845465838909149 + 0.001 * 9.331878662109375
Epoch 550, val loss: 0.8995652794837952
Epoch 560, training loss: 0.08400174975395203 = 0.07466023415327072 + 0.001 * 9.341513633728027
Epoch 560, val loss: 0.9206885099411011
Epoch 570, training loss: 0.0804222971200943 = 0.07108408212661743 + 0.001 * 9.338218688964844
Epoch 570, val loss: 0.9473998546600342
Epoch 580, training loss: 0.07677917182445526 = 0.06743655353784561 + 0.001 * 9.34261703491211
Epoch 580, val loss: 0.9663335084915161
Epoch 590, training loss: 0.07322166860103607 = 0.06390268355607986 + 0.001 * 9.318985939025879
Epoch 590, val loss: 0.9958347678184509
Epoch 600, training loss: 0.06987949460744858 = 0.060546256601810455 + 0.001 * 9.333237648010254
Epoch 600, val loss: 1.0156763792037964
Epoch 610, training loss: 0.06658173352479935 = 0.0572376549243927 + 0.001 * 9.344077110290527
Epoch 610, val loss: 1.044219732284546
Epoch 620, training loss: 0.06355231255292892 = 0.05423078313469887 + 0.001 * 9.32153034210205
Epoch 620, val loss: 1.0725839138031006
Epoch 630, training loss: 0.06046813726425171 = 0.051139865070581436 + 0.001 * 9.328271865844727
Epoch 630, val loss: 1.092254638671875
Epoch 640, training loss: 0.057434190064668655 = 0.048118144273757935 + 0.001 * 9.316044807434082
Epoch 640, val loss: 1.1220043897628784
Epoch 650, training loss: 0.05481192469596863 = 0.045498963445425034 + 0.001 * 9.31296157836914
Epoch 650, val loss: 1.1504894495010376
Epoch 660, training loss: 0.052056536078453064 = 0.042741890996694565 + 0.001 * 9.314644813537598
Epoch 660, val loss: 1.1724534034729004
Epoch 670, training loss: 0.04952625185251236 = 0.040215812623500824 + 0.001 * 9.310439109802246
Epoch 670, val loss: 1.2033284902572632
Epoch 680, training loss: 0.047118812799453735 = 0.03779451549053192 + 0.001 * 9.324298858642578
Epoch 680, val loss: 1.2251763343811035
Epoch 690, training loss: 0.044760577380657196 = 0.035443924367427826 + 0.001 * 9.316650390625
Epoch 690, val loss: 1.25642991065979
Epoch 700, training loss: 0.04256550595164299 = 0.03323633596301079 + 0.001 * 9.329168319702148
Epoch 700, val loss: 1.2804312705993652
Epoch 710, training loss: 0.04053691774606705 = 0.031225526705384254 + 0.001 * 9.311391830444336
Epoch 710, val loss: 1.3047209978103638
Epoch 720, training loss: 0.03856685757637024 = 0.02925032563507557 + 0.001 * 9.316530227661133
Epoch 720, val loss: 1.3343558311462402
Epoch 730, training loss: 0.03671880066394806 = 0.02740720845758915 + 0.001 * 9.311593055725098
Epoch 730, val loss: 1.358670949935913
Epoch 740, training loss: 0.035011474043130875 = 0.025699231773614883 + 0.001 * 9.31224250793457
Epoch 740, val loss: 1.3834097385406494
Epoch 750, training loss: 0.0333782434463501 = 0.024089252576231956 + 0.001 * 9.28898811340332
Epoch 750, val loss: 1.4114867448806763
Epoch 760, training loss: 0.03184051066637039 = 0.022536111995577812 + 0.001 * 9.304398536682129
Epoch 760, val loss: 1.4330898523330688
Epoch 770, training loss: 0.03039909526705742 = 0.021085374057292938 + 0.001 * 9.313720703125
Epoch 770, val loss: 1.4597516059875488
Epoch 780, training loss: 0.029040904715657234 = 0.01973007246851921 + 0.001 * 9.310832023620605
Epoch 780, val loss: 1.483184576034546
Epoch 790, training loss: 0.027743935585021973 = 0.018461018800735474 + 0.001 * 9.282916069030762
Epoch 790, val loss: 1.507560133934021
Epoch 800, training loss: 0.026549965143203735 = 0.01727386936545372 + 0.001 * 9.276095390319824
Epoch 800, val loss: 1.530903935432434
Epoch 810, training loss: 0.02547462284564972 = 0.0161551795899868 + 0.001 * 9.319441795349121
Epoch 810, val loss: 1.5540064573287964
Epoch 820, training loss: 0.024416562169790268 = 0.015121961943805218 + 0.001 * 9.294600486755371
Epoch 820, val loss: 1.5778706073760986
Epoch 830, training loss: 0.023454684764146805 = 0.014164775609970093 + 0.001 * 9.289909362792969
Epoch 830, val loss: 1.5997860431671143
Epoch 840, training loss: 0.022541839629411697 = 0.013269522227346897 + 0.001 * 9.272316932678223
Epoch 840, val loss: 1.621852159500122
Epoch 850, training loss: 0.021744966506958008 = 0.012454135343432426 + 0.001 * 9.290830612182617
Epoch 850, val loss: 1.6446980237960815
Epoch 860, training loss: 0.021003756672143936 = 0.011701857671141624 + 0.001 * 9.301899909973145
Epoch 860, val loss: 1.6653770208358765
Epoch 870, training loss: 0.020305801182985306 = 0.011003169231116772 + 0.001 * 9.302632331848145
Epoch 870, val loss: 1.6864664554595947
Epoch 880, training loss: 0.019649429246783257 = 0.010358145460486412 + 0.001 * 9.29128360748291
Epoch 880, val loss: 1.7069023847579956
Epoch 890, training loss: 0.01903793215751648 = 0.009764797054231167 + 0.001 * 9.2731351852417
Epoch 890, val loss: 1.7269232273101807
Epoch 900, training loss: 0.01850566826760769 = 0.009216741658747196 + 0.001 * 9.288926124572754
Epoch 900, val loss: 1.7468607425689697
Epoch 910, training loss: 0.01797502115368843 = 0.008709023706614971 + 0.001 * 9.265997886657715
Epoch 910, val loss: 1.7658578157424927
Epoch 920, training loss: 0.017521347850561142 = 0.008237715810537338 + 0.001 * 9.28363037109375
Epoch 920, val loss: 1.7843661308288574
Epoch 930, training loss: 0.017091520130634308 = 0.007801954634487629 + 0.001 * 9.289565086364746
Epoch 930, val loss: 1.802623987197876
Epoch 940, training loss: 0.016691718250513077 = 0.007396642584353685 + 0.001 * 9.295075416564941
Epoch 940, val loss: 1.8210893869400024
Epoch 950, training loss: 0.01630755513906479 = 0.0070195612497627735 + 0.001 * 9.287993431091309
Epoch 950, val loss: 1.8387064933776855
Epoch 960, training loss: 0.015942702069878578 = 0.006671766750514507 + 0.001 * 9.27093505859375
Epoch 960, val loss: 1.8557813167572021
Epoch 970, training loss: 0.01563369855284691 = 0.006347142159938812 + 0.001 * 9.286555290222168
Epoch 970, val loss: 1.8723063468933105
Epoch 980, training loss: 0.015311559662222862 = 0.006046272814273834 + 0.001 * 9.265286445617676
Epoch 980, val loss: 1.8882814645767212
Epoch 990, training loss: 0.015035659074783325 = 0.005765980575233698 + 0.001 * 9.269678115844727
Epoch 990, val loss: 1.9040467739105225
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8392
Overall ASR: 0.6790
Flip ASR: 0.6010/1554 nodes
The final ASR:0.66971, 0.00919, Accuracy:0.84103, 0.00145
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97504])
remove edge: torch.Size([2, 79736])
updated graph: torch.Size([2, 88592])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6993
Flip ASR: 0.6281/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7419
Flip ASR: 0.6802/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72211, 0.01752, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1068686246871948 = 1.0965094566345215 + 0.001 * 10.359109878540039
Epoch 0, val loss: 1.0956569910049438
Epoch 10, training loss: 1.094650387763977 = 1.0842914581298828 + 0.001 * 10.358962059020996
Epoch 10, val loss: 1.0832103490829468
Epoch 20, training loss: 1.076034665107727 = 1.065676212310791 + 0.001 * 10.358449935913086
Epoch 20, val loss: 1.063984990119934
Epoch 30, training loss: 1.0460625886917114 = 1.035705804824829 + 0.001 * 10.35672664642334
Epoch 30, val loss: 1.0335752964019775
Epoch 40, training loss: 1.0051671266555786 = 0.9948174357414246 + 0.001 * 10.349686622619629
Epoch 40, val loss: 0.992752194404602
Epoch 50, training loss: 0.9405696988105774 = 0.9302645325660706 + 0.001 * 10.305191993713379
Epoch 50, val loss: 0.9271183609962463
Epoch 60, training loss: 0.8438135981559753 = 0.8338944315910339 + 0.001 * 9.919188499450684
Epoch 60, val loss: 0.8312340974807739
Epoch 70, training loss: 0.7251918911933899 = 0.7155410051345825 + 0.001 * 9.650895118713379
Epoch 70, val loss: 0.7172734141349792
Epoch 80, training loss: 0.602751612663269 = 0.5932132601737976 + 0.001 * 9.53833293914795
Epoch 80, val loss: 0.6041544079780579
Epoch 90, training loss: 0.49551206827163696 = 0.48599010705947876 + 0.001 * 9.521950721740723
Epoch 90, val loss: 0.5125641822814941
Epoch 100, training loss: 0.4228701591491699 = 0.41335755586624146 + 0.001 * 9.512598037719727
Epoch 100, val loss: 0.4580193758010864
Epoch 110, training loss: 0.3822377920150757 = 0.3727278411388397 + 0.001 * 9.509963035583496
Epoch 110, val loss: 0.43323254585266113
Epoch 120, training loss: 0.3575980067253113 = 0.3480859398841858 + 0.001 * 9.512079238891602
Epoch 120, val loss: 0.4215880036354065
Epoch 130, training loss: 0.33956071734428406 = 0.33004605770111084 + 0.001 * 9.514666557312012
Epoch 130, val loss: 0.41421088576316833
Epoch 140, training loss: 0.32490673661231995 = 0.31538811326026917 + 0.001 * 9.518609046936035
Epoch 140, val loss: 0.40884262323379517
Epoch 150, training loss: 0.31241628527641296 = 0.30289342999458313 + 0.001 * 9.522852897644043
Epoch 150, val loss: 0.405354768037796
Epoch 160, training loss: 0.3013877272605896 = 0.2918606996536255 + 0.001 * 9.527012825012207
Epoch 160, val loss: 0.40360498428344727
Epoch 170, training loss: 0.2913829982280731 = 0.28185203671455383 + 0.001 * 9.5309476852417
Epoch 170, val loss: 0.40318840742111206
Epoch 180, training loss: 0.28212758898735046 = 0.2725927233695984 + 0.001 * 9.534862518310547
Epoch 180, val loss: 0.40383994579315186
Epoch 190, training loss: 0.27343884110450745 = 0.26389989256858826 + 0.001 * 9.538947105407715
Epoch 190, val loss: 0.4055823087692261
Epoch 200, training loss: 0.2651843726634979 = 0.2556409537792206 + 0.001 * 9.543417930603027
Epoch 200, val loss: 0.40841004252433777
Epoch 210, training loss: 0.25728267431259155 = 0.24773457646369934 + 0.001 * 9.548091888427734
Epoch 210, val loss: 0.4121370017528534
Epoch 220, training loss: 0.24967889487743378 = 0.24012602865695953 + 0.001 * 9.55286979675293
Epoch 220, val loss: 0.41670292615890503
Epoch 230, training loss: 0.24233628809452057 = 0.23277881741523743 + 0.001 * 9.557469367980957
Epoch 230, val loss: 0.4221763610839844
Epoch 240, training loss: 0.2352360486984253 = 0.225674569606781 + 0.001 * 9.561478614807129
Epoch 240, val loss: 0.4285987317562103
Epoch 250, training loss: 0.2283833920955658 = 0.2188194990158081 + 0.001 * 9.563889503479004
Epoch 250, val loss: 0.4359446167945862
Epoch 260, training loss: 0.2217922806739807 = 0.21222753822803497 + 0.001 * 9.564749717712402
Epoch 260, val loss: 0.4444030821323395
Epoch 270, training loss: 0.21550925076007843 = 0.2059461772441864 + 0.001 * 9.56307315826416
Epoch 270, val loss: 0.45477837324142456
Epoch 280, training loss: 0.20944511890411377 = 0.19987216591835022 + 0.001 * 9.572956085205078
Epoch 280, val loss: 0.4640441834926605
Epoch 290, training loss: 0.20369593799114227 = 0.19412797689437866 + 0.001 * 9.567954063415527
Epoch 290, val loss: 0.4736957252025604
Epoch 300, training loss: 0.19817563891410828 = 0.18860343098640442 + 0.001 * 9.5722074508667
Epoch 300, val loss: 0.48452267050743103
Epoch 310, training loss: 0.1929086148738861 = 0.18336160480976105 + 0.001 * 9.547016143798828
Epoch 310, val loss: 0.4939243197441101
Epoch 320, training loss: 0.18772751092910767 = 0.1781800240278244 + 0.001 * 9.5474853515625
Epoch 320, val loss: 0.5061924457550049
Epoch 330, training loss: 0.18284274637699127 = 0.173258975148201 + 0.001 * 9.583767890930176
Epoch 330, val loss: 0.5183118581771851
Epoch 340, training loss: 0.1780020147562027 = 0.16845346987247467 + 0.001 * 9.548539161682129
Epoch 340, val loss: 0.5311726927757263
Epoch 350, training loss: 0.17323648929595947 = 0.16369444131851196 + 0.001 * 9.542041778564453
Epoch 350, val loss: 0.5423699021339417
Epoch 360, training loss: 0.16856420040130615 = 0.15906164050102234 + 0.001 * 9.5025634765625
Epoch 360, val loss: 0.5561858415603638
Epoch 370, training loss: 0.16415567696094513 = 0.15456531941890717 + 0.001 * 9.590362548828125
Epoch 370, val loss: 0.5686113834381104
Epoch 380, training loss: 0.15963958203792572 = 0.1501084417104721 + 0.001 * 9.531134605407715
Epoch 380, val loss: 0.5815460681915283
Epoch 390, training loss: 0.15517733991146088 = 0.14567728340625763 + 0.001 * 9.500049591064453
Epoch 390, val loss: 0.5977057814598083
Epoch 400, training loss: 0.15074793994426727 = 0.14124365150928497 + 0.001 * 9.504289627075195
Epoch 400, val loss: 0.6108651757240295
Epoch 410, training loss: 0.14651232957839966 = 0.136984184384346 + 0.001 * 9.528138160705566
Epoch 410, val loss: 0.6271252036094666
Epoch 420, training loss: 0.14222858846187592 = 0.13275545835494995 + 0.001 * 9.473130226135254
Epoch 420, val loss: 0.643181562423706
Epoch 430, training loss: 0.13816522061824799 = 0.12867726385593414 + 0.001 * 9.487957000732422
Epoch 430, val loss: 0.6581273078918457
Epoch 440, training loss: 0.13400353491306305 = 0.12454782426357269 + 0.001 * 9.455714225769043
Epoch 440, val loss: 0.6730368733406067
Epoch 450, training loss: 0.12992973625659943 = 0.12047748267650604 + 0.001 * 9.452253341674805
Epoch 450, val loss: 0.6880837082862854
Epoch 460, training loss: 0.1259230077266693 = 0.11646388471126556 + 0.001 * 9.459121704101562
Epoch 460, val loss: 0.7045196294784546
Epoch 470, training loss: 0.12184222787618637 = 0.11237853020429611 + 0.001 * 9.463699340820312
Epoch 470, val loss: 0.7222267985343933
Epoch 480, training loss: 0.11785051971673965 = 0.10838722437620163 + 0.001 * 9.463295936584473
Epoch 480, val loss: 0.7445635199546814
Epoch 490, training loss: 0.11399804055690765 = 0.10453267395496368 + 0.001 * 9.46536636352539
Epoch 490, val loss: 0.7605668902397156
Epoch 500, training loss: 0.11022575199604034 = 0.10075356811285019 + 0.001 * 9.472187042236328
Epoch 500, val loss: 0.7774362564086914
Epoch 510, training loss: 0.10638810694217682 = 0.09692858159542084 + 0.001 * 9.459527015686035
Epoch 510, val loss: 0.800701916217804
Epoch 520, training loss: 0.10247667878866196 = 0.09305445849895477 + 0.001 * 9.422218322753906
Epoch 520, val loss: 0.8189191818237305
Epoch 530, training loss: 0.0991363674402237 = 0.08969567716121674 + 0.001 * 9.440691947937012
Epoch 530, val loss: 0.8341802954673767
Epoch 540, training loss: 0.09516289085149765 = 0.08574095368385315 + 0.001 * 9.421935081481934
Epoch 540, val loss: 0.8599340319633484
Epoch 550, training loss: 0.09163254499435425 = 0.08220842480659485 + 0.001 * 9.42411994934082
Epoch 550, val loss: 0.8827865123748779
Epoch 560, training loss: 0.0880553349852562 = 0.07860276103019714 + 0.001 * 9.452576637268066
Epoch 560, val loss: 0.904778778553009
Epoch 570, training loss: 0.08456137031316757 = 0.07513678818941116 + 0.001 * 9.424579620361328
Epoch 570, val loss: 0.9231815338134766
Epoch 580, training loss: 0.08111023902893066 = 0.07168623805046082 + 0.001 * 9.42399787902832
Epoch 580, val loss: 0.9466723799705505
Epoch 590, training loss: 0.07778583467006683 = 0.06834971159696579 + 0.001 * 9.43612289428711
Epoch 590, val loss: 0.9676311612129211
Epoch 600, training loss: 0.07460568100214005 = 0.06519918143749237 + 0.001 * 9.406501770019531
Epoch 600, val loss: 0.9990299940109253
Epoch 610, training loss: 0.07134610414505005 = 0.06191984564065933 + 0.001 * 9.426259994506836
Epoch 610, val loss: 1.0182870626449585
Epoch 620, training loss: 0.06841786950826645 = 0.05898599699139595 + 0.001 * 9.43187427520752
Epoch 620, val loss: 1.0387544631958008
Epoch 630, training loss: 0.06537455320358276 = 0.05598065257072449 + 0.001 * 9.393902778625488
Epoch 630, val loss: 1.0660951137542725
Epoch 640, training loss: 0.06254391372203827 = 0.05315077677369118 + 0.001 * 9.393138885498047
Epoch 640, val loss: 1.0878562927246094
Epoch 650, training loss: 0.05973650515079498 = 0.050340499728918076 + 0.001 * 9.39600658416748
Epoch 650, val loss: 1.1141507625579834
Epoch 660, training loss: 0.0573095828294754 = 0.0479060597717762 + 0.001 * 9.403522491455078
Epoch 660, val loss: 1.1355527639389038
Epoch 670, training loss: 0.05465991795063019 = 0.04525088891386986 + 0.001 * 9.409027099609375
Epoch 670, val loss: 1.1692777872085571
Epoch 680, training loss: 0.05210705101490021 = 0.042721353471279144 + 0.001 * 9.385696411132812
Epoch 680, val loss: 1.1879996061325073
Epoch 690, training loss: 0.049685124307870865 = 0.040301110595464706 + 0.001 * 9.384012222290039
Epoch 690, val loss: 1.2174392938613892
Epoch 700, training loss: 0.047516584396362305 = 0.03811774030327797 + 0.001 * 9.398842811584473
Epoch 700, val loss: 1.2433208227157593
Epoch 710, training loss: 0.04542311280965805 = 0.03598707541823387 + 0.001 * 9.436036109924316
Epoch 710, val loss: 1.2637662887573242
Epoch 720, training loss: 0.043275538831949234 = 0.03388484939932823 + 0.001 * 9.390689849853516
Epoch 720, val loss: 1.2926453351974487
Epoch 730, training loss: 0.04143518581986427 = 0.03201201185584068 + 0.001 * 9.423174858093262
Epoch 730, val loss: 1.3188695907592773
Epoch 740, training loss: 0.03955622389912605 = 0.03017968125641346 + 0.001 * 9.376543045043945
Epoch 740, val loss: 1.3390891551971436
Epoch 750, training loss: 0.037798844277858734 = 0.02839994803071022 + 0.001 * 9.398896217346191
Epoch 750, val loss: 1.3676269054412842
Epoch 760, training loss: 0.03614737465977669 = 0.026764996349811554 + 0.001 * 9.382376670837402
Epoch 760, val loss: 1.391207218170166
Epoch 770, training loss: 0.034646060317754745 = 0.025252414867281914 + 0.001 * 9.393646240234375
Epoch 770, val loss: 1.4125760793685913
Epoch 780, training loss: 0.033138081431388855 = 0.023767046630382538 + 0.001 * 9.371033668518066
Epoch 780, val loss: 1.440256953239441
Epoch 790, training loss: 0.03176110237836838 = 0.02236337587237358 + 0.001 * 9.397727966308594
Epoch 790, val loss: 1.4608649015426636
Epoch 800, training loss: 0.030428379774093628 = 0.0210508331656456 + 0.001 * 9.377546310424805
Epoch 800, val loss: 1.4855905771255493
Epoch 810, training loss: 0.029183465987443924 = 0.019820546731352806 + 0.001 * 9.362918853759766
Epoch 810, val loss: 1.5086185932159424
Epoch 820, training loss: 0.02806411311030388 = 0.018666177988052368 + 0.001 * 9.397934913635254
Epoch 820, val loss: 1.529728651046753
Epoch 830, training loss: 0.026912109926342964 = 0.017557866871356964 + 0.001 * 9.354242324829102
Epoch 830, val loss: 1.5540680885314941
Epoch 840, training loss: 0.02587496116757393 = 0.016514282673597336 + 0.001 * 9.360676765441895
Epoch 840, val loss: 1.5739468336105347
Epoch 850, training loss: 0.024893868714571 = 0.015527216717600822 + 0.001 * 9.366652488708496
Epoch 850, val loss: 1.5967267751693726
Epoch 860, training loss: 0.023942820727825165 = 0.014596804976463318 + 0.001 * 9.346015930175781
Epoch 860, val loss: 1.6176871061325073
Epoch 870, training loss: 0.02308322861790657 = 0.013726312667131424 + 0.001 * 9.356914520263672
Epoch 870, val loss: 1.6385554075241089
Epoch 880, training loss: 0.02226325124502182 = 0.012908860109746456 + 0.001 * 9.354390144348145
Epoch 880, val loss: 1.65823233127594
Epoch 890, training loss: 0.021520789712667465 = 0.012147999368607998 + 0.001 * 9.372790336608887
Epoch 890, val loss: 1.679459810256958
Epoch 900, training loss: 0.020778514444828033 = 0.011437387205660343 + 0.001 * 9.341127395629883
Epoch 900, val loss: 1.6984723806381226
Epoch 910, training loss: 0.020126203075051308 = 0.010781770572066307 + 0.001 * 9.34443187713623
Epoch 910, val loss: 1.7184727191925049
Epoch 920, training loss: 0.019524041563272476 = 0.010172559879720211 + 0.001 * 9.351481437683105
Epoch 920, val loss: 1.7372132539749146
Epoch 930, training loss: 0.018980197608470917 = 0.00960883591324091 + 0.001 * 9.371360778808594
Epoch 930, val loss: 1.7555994987487793
Epoch 940, training loss: 0.018427595496177673 = 0.009085770696401596 + 0.001 * 9.341824531555176
Epoch 940, val loss: 1.7744837999343872
Epoch 950, training loss: 0.017931926995515823 = 0.008600723929703236 + 0.001 * 9.331202507019043
Epoch 950, val loss: 1.792449712753296
Epoch 960, training loss: 0.017487041652202606 = 0.008152578957378864 + 0.001 * 9.334461212158203
Epoch 960, val loss: 1.809775471687317
Epoch 970, training loss: 0.017092078924179077 = 0.00773347495123744 + 0.001 * 9.358604431152344
Epoch 970, val loss: 1.8271913528442383
Epoch 980, training loss: 0.01669660396873951 = 0.007345645688474178 + 0.001 * 9.350957870483398
Epoch 980, val loss: 1.843898057937622
Epoch 990, training loss: 0.016331398859620094 = 0.006984546314924955 + 0.001 * 9.34685230255127
Epoch 990, val loss: 1.860314965248108
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8412
Overall ASR: 0.6476
Flip ASR: 0.5618/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1241447925567627 = 1.113785743713379 + 0.001 * 10.359062194824219
Epoch 0, val loss: 1.111924648284912
Epoch 10, training loss: 1.1107220649719238 = 1.1003631353378296 + 0.001 * 10.358933448791504
Epoch 10, val loss: 1.0984643697738647
Epoch 20, training loss: 1.091220498085022 = 1.0808621644973755 + 0.001 * 10.358360290527344
Epoch 20, val loss: 1.0785553455352783
Epoch 30, training loss: 1.0599875450134277 = 1.0496312379837036 + 0.001 * 10.356327056884766
Epoch 30, val loss: 1.0471609830856323
Epoch 40, training loss: 1.0194672346115112 = 1.0091207027435303 + 0.001 * 10.346556663513184
Epoch 40, val loss: 1.0076433420181274
Epoch 50, training loss: 0.9642682671546936 = 0.9540000557899475 + 0.001 * 10.26822566986084
Epoch 50, val loss: 0.9509807825088501
Epoch 60, training loss: 0.8780011534690857 = 0.8681410551071167 + 0.001 * 9.860101699829102
Epoch 60, val loss: 0.8651399612426758
Epoch 70, training loss: 0.7705712914466858 = 0.7608152627944946 + 0.001 * 9.756003379821777
Epoch 70, val loss: 0.7601906657218933
Epoch 80, training loss: 0.6578879952430725 = 0.6482766270637512 + 0.001 * 9.611342430114746
Epoch 80, val loss: 0.6543789505958557
Epoch 90, training loss: 0.5564525127410889 = 0.5469468832015991 + 0.001 * 9.505653381347656
Epoch 90, val loss: 0.5634273886680603
Epoch 100, training loss: 0.4753662645816803 = 0.4658479690551758 + 0.001 * 9.518304824829102
Epoch 100, val loss: 0.4951576590538025
Epoch 110, training loss: 0.41806793212890625 = 0.4085586965084076 + 0.001 * 9.509225845336914
Epoch 110, val loss: 0.4529101848602295
Epoch 120, training loss: 0.3825134038925171 = 0.3730042576789856 + 0.001 * 9.509138107299805
Epoch 120, val loss: 0.4321037232875824
Epoch 130, training loss: 0.3593088388442993 = 0.3497960567474365 + 0.001 * 9.512787818908691
Epoch 130, val loss: 0.42169520258903503
Epoch 140, training loss: 0.3412262499332428 = 0.3317103683948517 + 0.001 * 9.515872955322266
Epoch 140, val loss: 0.41417956352233887
Epoch 150, training loss: 0.3261484205722809 = 0.31662803888320923 + 0.001 * 9.520383834838867
Epoch 150, val loss: 0.4080621004104614
Epoch 160, training loss: 0.31325727701187134 = 0.3037319481372833 + 0.001 * 9.525341033935547
Epoch 160, val loss: 0.4036361277103424
Epoch 170, training loss: 0.30189889669418335 = 0.2923681437969208 + 0.001 * 9.530756950378418
Epoch 170, val loss: 0.40100035071372986
Epoch 180, training loss: 0.2916215658187866 = 0.28208568692207336 + 0.001 * 9.535865783691406
Epoch 180, val loss: 0.3999745845794678
Epoch 190, training loss: 0.28215762972831726 = 0.2726168632507324 + 0.001 * 9.540767669677734
Epoch 190, val loss: 0.4003092348575592
Epoch 200, training loss: 0.27333885431289673 = 0.26379361748695374 + 0.001 * 9.545225143432617
Epoch 200, val loss: 0.40174606442451477
Epoch 210, training loss: 0.26503095030784607 = 0.25548219680786133 + 0.001 * 9.548766136169434
Epoch 210, val loss: 0.40407297015190125
Epoch 220, training loss: 0.2571231722831726 = 0.24757225811481476 + 0.001 * 9.550918579101562
Epoch 220, val loss: 0.4074110984802246
Epoch 230, training loss: 0.2495463341474533 = 0.23999546468257904 + 0.001 * 9.550865173339844
Epoch 230, val loss: 0.41163820028305054
Epoch 240, training loss: 0.24225060641765594 = 0.23270446062088013 + 0.001 * 9.546146392822266
Epoch 240, val loss: 0.41668394207954407
Epoch 250, training loss: 0.23520232737064362 = 0.22566407918930054 + 0.001 * 9.538250923156738
Epoch 250, val loss: 0.422607958316803
Epoch 260, training loss: 0.228412926197052 = 0.2188737988471985 + 0.001 * 9.539130210876465
Epoch 260, val loss: 0.42949891090393066
Epoch 270, training loss: 0.22189265489578247 = 0.2123483270406723 + 0.001 * 9.544326782226562
Epoch 270, val loss: 0.43758100271224976
Epoch 280, training loss: 0.21563202142715454 = 0.20609518885612488 + 0.001 * 9.536824226379395
Epoch 280, val loss: 0.44671323895454407
Epoch 290, training loss: 0.20963796973228455 = 0.20010654628276825 + 0.001 * 9.531429290771484
Epoch 290, val loss: 0.45657798647880554
Epoch 300, training loss: 0.20394743978977203 = 0.19441893696784973 + 0.001 * 9.5285005569458
Epoch 300, val loss: 0.466082364320755
Epoch 310, training loss: 0.19847236573696136 = 0.18890877068042755 + 0.001 * 9.5635986328125
Epoch 310, val loss: 0.4774588942527771
Epoch 320, training loss: 0.1931696981191635 = 0.18364158272743225 + 0.001 * 9.528116226196289
Epoch 320, val loss: 0.48918259143829346
Epoch 330, training loss: 0.18801981210708618 = 0.1785111129283905 + 0.001 * 9.508703231811523
Epoch 330, val loss: 0.5001992583274841
Epoch 340, training loss: 0.18302679061889648 = 0.17351116240024567 + 0.001 * 9.515628814697266
Epoch 340, val loss: 0.5128743052482605
Epoch 350, training loss: 0.17822259664535522 = 0.16870951652526855 + 0.001 * 9.51307487487793
Epoch 350, val loss: 0.5248916149139404
Epoch 360, training loss: 0.17353767156600952 = 0.16404402256011963 + 0.001 * 9.493646621704102
Epoch 360, val loss: 0.537211537361145
Epoch 370, training loss: 0.16888175904750824 = 0.15937262773513794 + 0.001 * 9.509130477905273
Epoch 370, val loss: 0.5488423109054565
Epoch 380, training loss: 0.16427426040172577 = 0.15479789674282074 + 0.001 * 9.476363182067871
Epoch 380, val loss: 0.5625298023223877
Epoch 390, training loss: 0.15972203016281128 = 0.150262251496315 + 0.001 * 9.459779739379883
Epoch 390, val loss: 0.5773176550865173
Epoch 400, training loss: 0.15525031089782715 = 0.14579637348651886 + 0.001 * 9.453944206237793
Epoch 400, val loss: 0.593041181564331
Epoch 410, training loss: 0.15089526772499084 = 0.14142419397830963 + 0.001 * 9.471076965332031
Epoch 410, val loss: 0.6050242185592651
Epoch 420, training loss: 0.14657875895500183 = 0.1371171623468399 + 0.001 * 9.461596488952637
Epoch 420, val loss: 0.6231095790863037
Epoch 430, training loss: 0.14226998388767242 = 0.1328280121088028 + 0.001 * 9.441975593566895
Epoch 430, val loss: 0.6375566720962524
Epoch 440, training loss: 0.13814300298690796 = 0.1286807507276535 + 0.001 * 9.46224594116211
Epoch 440, val loss: 0.6578609347343445
Epoch 450, training loss: 0.13397294282913208 = 0.12451224029064178 + 0.001 * 9.46070671081543
Epoch 450, val loss: 0.6701380014419556
Epoch 460, training loss: 0.12974220514297485 = 0.12030515819787979 + 0.001 * 9.437044143676758
Epoch 460, val loss: 0.6871887445449829
Epoch 470, training loss: 0.12566980719566345 = 0.11623408645391464 + 0.001 * 9.435721397399902
Epoch 470, val loss: 0.7086102366447449
Epoch 480, training loss: 0.12150595337152481 = 0.11208231002092361 + 0.001 * 9.423645973205566
Epoch 480, val loss: 0.7246526479721069
Epoch 490, training loss: 0.1176358014345169 = 0.10820994526147842 + 0.001 * 9.425853729248047
Epoch 490, val loss: 0.7432015538215637
Epoch 500, training loss: 0.11370091885328293 = 0.10427761822938919 + 0.001 * 9.423299789428711
Epoch 500, val loss: 0.7610180377960205
Epoch 510, training loss: 0.10972187668085098 = 0.10029156506061554 + 0.001 * 9.43031120300293
Epoch 510, val loss: 0.7803748846054077
Epoch 520, training loss: 0.10586047917604446 = 0.09644808620214462 + 0.001 * 9.412389755249023
Epoch 520, val loss: 0.8040734529495239
Epoch 530, training loss: 0.10206896811723709 = 0.09264007955789566 + 0.001 * 9.428885459899902
Epoch 530, val loss: 0.8207104802131653
Epoch 540, training loss: 0.09817856550216675 = 0.08875013887882233 + 0.001 * 9.428424835205078
Epoch 540, val loss: 0.8460886478424072
Epoch 550, training loss: 0.09442983567714691 = 0.0850159227848053 + 0.001 * 9.413912773132324
Epoch 550, val loss: 0.8671546578407288
Epoch 560, training loss: 0.09092331677675247 = 0.0814942866563797 + 0.001 * 9.42902660369873
Epoch 560, val loss: 0.8862339854240417
Epoch 570, training loss: 0.08725941181182861 = 0.07785830646753311 + 0.001 * 9.401106834411621
Epoch 570, val loss: 0.9154050350189209
Epoch 580, training loss: 0.08364168554544449 = 0.07420578598976135 + 0.001 * 9.43589973449707
Epoch 580, val loss: 0.9322438836097717
Epoch 590, training loss: 0.08005379140377045 = 0.070642851293087 + 0.001 * 9.410937309265137
Epoch 590, val loss: 0.9609153866767883
Epoch 600, training loss: 0.07667995989322662 = 0.06727901101112366 + 0.001 * 9.400952339172363
Epoch 600, val loss: 0.9834781289100647
Epoch 610, training loss: 0.07353575527667999 = 0.06412087380886078 + 0.001 * 9.41487979888916
Epoch 610, val loss: 1.006142497062683
Epoch 620, training loss: 0.07044242322444916 = 0.061007868498563766 + 0.001 * 9.434553146362305
Epoch 620, val loss: 1.0357129573822021
Epoch 630, training loss: 0.06742795556783676 = 0.058001965284347534 + 0.001 * 9.425992012023926
Epoch 630, val loss: 1.0543346405029297
Epoch 640, training loss: 0.06435275077819824 = 0.05496162176132202 + 0.001 * 9.39112663269043
Epoch 640, val loss: 1.0866776704788208
Epoch 650, training loss: 0.06144823133945465 = 0.05205162614583969 + 0.001 * 9.396604537963867
Epoch 650, val loss: 1.1073626279830933
Epoch 660, training loss: 0.05874887853860855 = 0.04935082793235779 + 0.001 * 9.398052215576172
Epoch 660, val loss: 1.1344059705734253
Epoch 670, training loss: 0.05612901970744133 = 0.04673970490694046 + 0.001 * 9.389314651489258
Epoch 670, val loss: 1.1614001989364624
Epoch 680, training loss: 0.05371120944619179 = 0.0443185493350029 + 0.001 * 9.392660140991211
Epoch 680, val loss: 1.183751106262207
Epoch 690, training loss: 0.051258236169815063 = 0.04186217114329338 + 0.001 * 9.396065711975098
Epoch 690, val loss: 1.2151916027069092
Epoch 700, training loss: 0.04891837388277054 = 0.03949325904250145 + 0.001 * 9.425116539001465
Epoch 700, val loss: 1.237738847732544
Epoch 710, training loss: 0.04681454598903656 = 0.03740782290697098 + 0.001 * 9.406723022460938
Epoch 710, val loss: 1.2615230083465576
Epoch 720, training loss: 0.044671379029750824 = 0.03528763726353645 + 0.001 * 9.383743286132812
Epoch 720, val loss: 1.2925434112548828
Epoch 730, training loss: 0.04267704859375954 = 0.033256255090236664 + 0.001 * 9.420793533325195
Epoch 730, val loss: 1.312853217124939
Epoch 740, training loss: 0.04069837927818298 = 0.031318485736846924 + 0.001 * 9.379891395568848
Epoch 740, val loss: 1.3414239883422852
Epoch 750, training loss: 0.038967106491327286 = 0.029551442712545395 + 0.001 * 9.41566276550293
Epoch 750, val loss: 1.367022156715393
Epoch 760, training loss: 0.03729259967803955 = 0.027877673506736755 + 0.001 * 9.414925575256348
Epoch 760, val loss: 1.389123797416687
Epoch 770, training loss: 0.035581059753894806 = 0.026226133108139038 + 0.001 * 9.354927062988281
Epoch 770, val loss: 1.4172320365905762
Epoch 780, training loss: 0.03412173315882683 = 0.02470800280570984 + 0.001 * 9.41373062133789
Epoch 780, val loss: 1.4408713579177856
Epoch 790, training loss: 0.03261777386069298 = 0.02327027916908264 + 0.001 * 9.347494125366211
Epoch 790, val loss: 1.4661260843276978
Epoch 800, training loss: 0.03130575641989708 = 0.02191554754972458 + 0.001 * 9.390207290649414
Epoch 800, val loss: 1.4904206991195679
Epoch 810, training loss: 0.03004341945052147 = 0.02063492126762867 + 0.001 * 9.408496856689453
Epoch 810, val loss: 1.5141210556030273
Epoch 820, training loss: 0.02879016287624836 = 0.019423101097345352 + 0.001 * 9.367061614990234
Epoch 820, val loss: 1.5386323928833008
Epoch 830, training loss: 0.02770140953361988 = 0.01827068254351616 + 0.001 * 9.430727005004883
Epoch 830, val loss: 1.5624535083770752
Epoch 840, training loss: 0.026557549834251404 = 0.017185373231768608 + 0.001 * 9.372177124023438
Epoch 840, val loss: 1.5849385261535645
Epoch 850, training loss: 0.02552204206585884 = 0.016162270680069923 + 0.001 * 9.359771728515625
Epoch 850, val loss: 1.6078377962112427
Epoch 860, training loss: 0.024537207558751106 = 0.015195873565971851 + 0.001 * 9.341333389282227
Epoch 860, val loss: 1.6305869817733765
Epoch 870, training loss: 0.02365751937031746 = 0.014293304644525051 + 0.001 * 9.364212989807129
Epoch 870, val loss: 1.6518826484680176
Epoch 880, training loss: 0.022810952737927437 = 0.013436274603009224 + 0.001 * 9.374677658081055
Epoch 880, val loss: 1.6737492084503174
Epoch 890, training loss: 0.02197796106338501 = 0.012645874172449112 + 0.001 * 9.332086563110352
Epoch 890, val loss: 1.696282982826233
Epoch 900, training loss: 0.021240688860416412 = 0.011900374665856361 + 0.001 * 9.340313911437988
Epoch 900, val loss: 1.7175275087356567
Epoch 910, training loss: 0.020567884668707848 = 0.011210959404706955 + 0.001 * 9.356925010681152
Epoch 910, val loss: 1.7379740476608276
Epoch 920, training loss: 0.019946761429309845 = 0.010574867948889732 + 0.001 * 9.371891975402832
Epoch 920, val loss: 1.7581310272216797
Epoch 930, training loss: 0.01930166780948639 = 0.009982009418308735 + 0.001 * 9.319658279418945
Epoch 930, val loss: 1.7778478860855103
Epoch 940, training loss: 0.01882489025592804 = 0.009433390572667122 + 0.001 * 9.391500473022461
Epoch 940, val loss: 1.7985163927078247
Epoch 950, training loss: 0.01824348419904709 = 0.008923507295548916 + 0.001 * 9.319975852966309
Epoch 950, val loss: 1.8173397779464722
Epoch 960, training loss: 0.017767535522580147 = 0.008451273664832115 + 0.001 * 9.316261291503906
Epoch 960, val loss: 1.8364986181259155
Epoch 970, training loss: 0.01736142486333847 = 0.00801180675625801 + 0.001 * 9.349617958068848
Epoch 970, val loss: 1.8551630973815918
Epoch 980, training loss: 0.016908317804336548 = 0.007603725418448448 + 0.001 * 9.304591178894043
Epoch 980, val loss: 1.8729586601257324
Epoch 990, training loss: 0.016532719135284424 = 0.0072243851609528065 + 0.001 * 9.308333396911621
Epoch 990, val loss: 1.8905590772628784
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8412
Overall ASR: 0.6922
Flip ASR: 0.6171/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1034140586853027 = 1.093055009841919 + 0.001 * 10.35905933380127
Epoch 0, val loss: 1.0918740034103394
Epoch 10, training loss: 1.0926909446716309 = 1.0823320150375366 + 0.001 * 10.358896255493164
Epoch 10, val loss: 1.0810314416885376
Epoch 20, training loss: 1.0761851072311401 = 1.0658268928527832 + 0.001 * 10.358258247375488
Epoch 20, val loss: 1.0640860795974731
Epoch 30, training loss: 1.0488555431365967 = 1.0384994745254517 + 0.001 * 10.356046676635742
Epoch 30, val loss: 1.0363805294036865
Epoch 40, training loss: 1.0089315176010132 = 0.9985856413841248 + 0.001 * 10.345818519592285
Epoch 40, val loss: 0.996238112449646
Epoch 50, training loss: 0.9446523785591125 = 0.9343864321708679 + 0.001 * 10.26594066619873
Epoch 50, val loss: 0.930391788482666
Epoch 60, training loss: 0.8464791774749756 = 0.8366813659667969 + 0.001 * 9.797815322875977
Epoch 60, val loss: 0.8327922821044922
Epoch 70, training loss: 0.721666693687439 = 0.7120232582092285 + 0.001 * 9.643440246582031
Epoch 70, val loss: 0.7119145393371582
Epoch 80, training loss: 0.5923818945884705 = 0.5828616619110107 + 0.001 * 9.520256996154785
Epoch 80, val loss: 0.5930901765823364
Epoch 90, training loss: 0.4888686239719391 = 0.47935959696769714 + 0.001 * 9.509018898010254
Epoch 90, val loss: 0.5055347681045532
Epoch 100, training loss: 0.4228314757347107 = 0.41332924365997314 + 0.001 * 9.502223014831543
Epoch 100, val loss: 0.4563464820384979
Epoch 110, training loss: 0.3850322961807251 = 0.37553665041923523 + 0.001 * 9.495645523071289
Epoch 110, val loss: 0.433945894241333
Epoch 120, training loss: 0.3609221875667572 = 0.3514261543750763 + 0.001 * 9.496021270751953
Epoch 120, val loss: 0.4229483902454376
Epoch 130, training loss: 0.3424336612224579 = 0.332937091588974 + 0.001 * 9.496575355529785
Epoch 130, val loss: 0.4153063893318176
Epoch 140, training loss: 0.32725271582603455 = 0.3177535831928253 + 0.001 * 9.499144554138184
Epoch 140, val loss: 0.4094633162021637
Epoch 150, training loss: 0.3142751455307007 = 0.30477288365364075 + 0.001 * 9.502270698547363
Epoch 150, val loss: 0.4054851830005646
Epoch 160, training loss: 0.3027893602848053 = 0.2932835817337036 + 0.001 * 9.505777359008789
Epoch 160, val loss: 0.40330860018730164
Epoch 170, training loss: 0.29236671328544617 = 0.2828574776649475 + 0.001 * 9.509243965148926
Epoch 170, val loss: 0.40258532762527466
Epoch 180, training loss: 0.28276610374450684 = 0.2732529640197754 + 0.001 * 9.513154029846191
Epoch 180, val loss: 0.4030219316482544
Epoch 190, training loss: 0.27379110455513 = 0.2642737329006195 + 0.001 * 9.517366409301758
Epoch 190, val loss: 0.40454384684562683
Epoch 200, training loss: 0.2652936577796936 = 0.25577208399772644 + 0.001 * 9.521573066711426
Epoch 200, val loss: 0.4071168601512909
Epoch 210, training loss: 0.25718560814857483 = 0.2476600706577301 + 0.001 * 9.525524139404297
Epoch 210, val loss: 0.4106638729572296
Epoch 220, training loss: 0.2493850588798523 = 0.23985616862773895 + 0.001 * 9.52889633178711
Epoch 220, val loss: 0.41517144441604614
Epoch 230, training loss: 0.24184846878051758 = 0.2323172390460968 + 0.001 * 9.53122329711914
Epoch 230, val loss: 0.42062821984291077
Epoch 240, training loss: 0.23455365002155304 = 0.22502191364765167 + 0.001 * 9.531731605529785
Epoch 240, val loss: 0.42695534229278564
Epoch 250, training loss: 0.2274893969297409 = 0.21796026825904846 + 0.001 * 9.529125213623047
Epoch 250, val loss: 0.43432459235191345
Epoch 260, training loss: 0.22071635723114014 = 0.21118728816509247 + 0.001 * 9.529061317443848
Epoch 260, val loss: 0.4422633647918701
Epoch 270, training loss: 0.21417684853076935 = 0.204647034406662 + 0.001 * 9.529809951782227
Epoch 270, val loss: 0.4515618681907654
Epoch 280, training loss: 0.20792360603809357 = 0.19839337468147278 + 0.001 * 9.530227661132812
Epoch 280, val loss: 0.4624280631542206
Epoch 290, training loss: 0.20193982124328613 = 0.19241516292095184 + 0.001 * 9.524651527404785
Epoch 290, val loss: 0.4717232882976532
Epoch 300, training loss: 0.1961687207221985 = 0.18664905428886414 + 0.001 * 9.519659042358398
Epoch 300, val loss: 0.48430025577545166
Epoch 310, training loss: 0.19059564173221588 = 0.1810794472694397 + 0.001 * 9.516190528869629
Epoch 310, val loss: 0.49403172731399536
Epoch 320, training loss: 0.1851806342601776 = 0.17566367983818054 + 0.001 * 9.516953468322754
Epoch 320, val loss: 0.5081591010093689
Epoch 330, training loss: 0.1798989325761795 = 0.17039315402507782 + 0.001 * 9.50578498840332
Epoch 330, val loss: 0.5179442763328552
Epoch 340, training loss: 0.17470920085906982 = 0.1652083843946457 + 0.001 * 9.50082015991211
Epoch 340, val loss: 0.5328643918037415
Epoch 350, training loss: 0.16959910094738007 = 0.16010244190692902 + 0.001 * 9.496654510498047
Epoch 350, val loss: 0.5439280271530151
Epoch 360, training loss: 0.16463403403759003 = 0.15514202415943146 + 0.001 * 9.492002487182617
Epoch 360, val loss: 0.5609461665153503
Epoch 370, training loss: 0.15968725085258484 = 0.1501639485359192 + 0.001 * 9.523306846618652
Epoch 370, val loss: 0.5730587840080261
Epoch 380, training loss: 0.15480688214302063 = 0.14532151818275452 + 0.001 * 9.485360145568848
Epoch 380, val loss: 0.5889372229576111
Epoch 390, training loss: 0.1501520723104477 = 0.14066007733345032 + 0.001 * 9.491998672485352
Epoch 390, val loss: 0.6011138558387756
Epoch 400, training loss: 0.14541584253311157 = 0.13594816625118256 + 0.001 * 9.467668533325195
Epoch 400, val loss: 0.6204121112823486
Epoch 410, training loss: 0.14070168137550354 = 0.1312038153409958 + 0.001 * 9.497870445251465
Epoch 410, val loss: 0.6349719166755676
Epoch 420, training loss: 0.13606305420398712 = 0.12663419544696808 + 0.001 * 9.428857803344727
Epoch 420, val loss: 0.6516733765602112
Epoch 430, training loss: 0.1315510869026184 = 0.12207210063934326 + 0.001 * 9.47899341583252
Epoch 430, val loss: 0.6667031049728394
Epoch 440, training loss: 0.12739169597625732 = 0.1179497092962265 + 0.001 * 9.44199275970459
Epoch 440, val loss: 0.6900370717048645
Epoch 450, training loss: 0.12289179861545563 = 0.11347141116857529 + 0.001 * 9.420385360717773
Epoch 450, val loss: 0.7042313814163208
Epoch 460, training loss: 0.11856363713741302 = 0.10912627726793289 + 0.001 * 9.437355995178223
Epoch 460, val loss: 0.7217563986778259
Epoch 470, training loss: 0.1143483966588974 = 0.10493592917919159 + 0.001 * 9.412464141845703
Epoch 470, val loss: 0.7419772148132324
Epoch 480, training loss: 0.11018405109643936 = 0.10075515508651733 + 0.001 * 9.428898811340332
Epoch 480, val loss: 0.7585911154747009
Epoch 490, training loss: 0.1059313416481018 = 0.09652503579854965 + 0.001 * 9.406307220458984
Epoch 490, val loss: 0.7807745933532715
Epoch 500, training loss: 0.10209345072507858 = 0.09269256144762039 + 0.001 * 9.40088939666748
Epoch 500, val loss: 0.8030285835266113
Epoch 510, training loss: 0.0978674665093422 = 0.08847285062074661 + 0.001 * 9.394615173339844
Epoch 510, val loss: 0.8249277472496033
Epoch 520, training loss: 0.09392791986465454 = 0.08452894538640976 + 0.001 * 9.398974418640137
Epoch 520, val loss: 0.8421460390090942
Epoch 530, training loss: 0.09006036072969437 = 0.08065041154623032 + 0.001 * 9.409951210021973
Epoch 530, val loss: 0.8689673542976379
Epoch 540, training loss: 0.08611360192298889 = 0.07670655846595764 + 0.001 * 9.407042503356934
Epoch 540, val loss: 0.8885422945022583
Epoch 550, training loss: 0.08251439779996872 = 0.0731201171875 + 0.001 * 9.394279479980469
Epoch 550, val loss: 0.9121109247207642
Epoch 560, training loss: 0.07887506484985352 = 0.06948324292898178 + 0.001 * 9.391823768615723
Epoch 560, val loss: 0.9341201782226562
Epoch 570, training loss: 0.07542916387319565 = 0.06601918488740921 + 0.001 * 9.409976959228516
Epoch 570, val loss: 0.9642556309700012
Epoch 580, training loss: 0.07201249152421951 = 0.06258565187454224 + 0.001 * 9.426836013793945
Epoch 580, val loss: 0.9845555424690247
Epoch 590, training loss: 0.06872498244047165 = 0.05932826176285744 + 0.001 * 9.396718978881836
Epoch 590, val loss: 1.011592149734497
Epoch 600, training loss: 0.06567233055830002 = 0.056259457021951675 + 0.001 * 9.412873268127441
Epoch 600, val loss: 1.0384862422943115
Epoch 610, training loss: 0.06274952739477158 = 0.0533648356795311 + 0.001 * 9.384692192077637
Epoch 610, val loss: 1.0593349933624268
Epoch 620, training loss: 0.059794094413518906 = 0.05039827153086662 + 0.001 * 9.395822525024414
Epoch 620, val loss: 1.0915197134017944
Epoch 630, training loss: 0.05698421597480774 = 0.04758228734135628 + 0.001 * 9.40192985534668
Epoch 630, val loss: 1.113351821899414
Epoch 640, training loss: 0.05437503755092621 = 0.04496689513325691 + 0.001 * 9.408143997192383
Epoch 640, val loss: 1.1400080919265747
Epoch 650, training loss: 0.05191051959991455 = 0.04253389313817024 + 0.001 * 9.376626968383789
Epoch 650, val loss: 1.1695729494094849
Epoch 660, training loss: 0.049504563212394714 = 0.04010733962059021 + 0.001 * 9.397222518920898
Epoch 660, val loss: 1.1897693872451782
Epoch 670, training loss: 0.047154750674963 = 0.03776698559522629 + 0.001 * 9.387764930725098
Epoch 670, val loss: 1.2201907634735107
Epoch 680, training loss: 0.044977251440286636 = 0.035607028752565384 + 0.001 * 9.370222091674805
Epoch 680, val loss: 1.2454609870910645
Epoch 690, training loss: 0.04293238744139671 = 0.03355235606431961 + 0.001 * 9.38003158569336
Epoch 690, val loss: 1.2721531391143799
Epoch 700, training loss: 0.04101211577653885 = 0.03161093592643738 + 0.001 * 9.401178359985352
Epoch 700, val loss: 1.29558527469635
Epoch 710, training loss: 0.039193712174892426 = 0.029783915728330612 + 0.001 * 9.409795761108398
Epoch 710, val loss: 1.3214086294174194
Epoch 720, training loss: 0.03747814893722534 = 0.02810726873576641 + 0.001 * 9.370879173278809
Epoch 720, val loss: 1.3483279943466187
Epoch 730, training loss: 0.03588554635643959 = 0.026484781876206398 + 0.001 * 9.400765419006348
Epoch 730, val loss: 1.3694300651550293
Epoch 740, training loss: 0.034325115382671356 = 0.02493748627603054 + 0.001 * 9.387630462646484
Epoch 740, val loss: 1.3976668119430542
Epoch 750, training loss: 0.03283359482884407 = 0.023477641865611076 + 0.001 * 9.355952262878418
Epoch 750, val loss: 1.4195038080215454
Epoch 760, training loss: 0.03148765116930008 = 0.02212354727089405 + 0.001 * 9.364105224609375
Epoch 760, val loss: 1.444103479385376
Epoch 770, training loss: 0.0302375927567482 = 0.020862944424152374 + 0.001 * 9.37464714050293
Epoch 770, val loss: 1.4690546989440918
Epoch 780, training loss: 0.029022488743066788 = 0.01967042312026024 + 0.001 * 9.352066040039062
Epoch 780, val loss: 1.4897722005844116
Epoch 790, training loss: 0.027885276824235916 = 0.018530670553445816 + 0.001 * 9.354604721069336
Epoch 790, val loss: 1.5155776739120483
Epoch 800, training loss: 0.026808395981788635 = 0.017445538192987442 + 0.001 * 9.3628568649292
Epoch 800, val loss: 1.5371602773666382
Epoch 810, training loss: 0.025762667879462242 = 0.016418129205703735 + 0.001 * 9.344538688659668
Epoch 810, val loss: 1.5604408979415894
Epoch 820, training loss: 0.024813303723931313 = 0.01546342670917511 + 0.001 * 9.349876403808594
Epoch 820, val loss: 1.5824544429779053
Epoch 830, training loss: 0.023895427584648132 = 0.014551771804690361 + 0.001 * 9.343655586242676
Epoch 830, val loss: 1.6057838201522827
Epoch 840, training loss: 0.023027870804071426 = 0.013692071661353111 + 0.001 * 9.335797309875488
Epoch 840, val loss: 1.6265305280685425
Epoch 850, training loss: 0.022232018411159515 = 0.012885197065770626 + 0.001 * 9.346820831298828
Epoch 850, val loss: 1.6473637819290161
Epoch 860, training loss: 0.021503331139683723 = 0.0121292220428586 + 0.001 * 9.37410831451416
Epoch 860, val loss: 1.668214201927185
Epoch 870, training loss: 0.02078331634402275 = 0.011420284397900105 + 0.001 * 9.363030433654785
Epoch 870, val loss: 1.6886824369430542
Epoch 880, training loss: 0.02012540027499199 = 0.01075830589979887 + 0.001 * 9.367094039916992
Epoch 880, val loss: 1.708570122718811
Epoch 890, training loss: 0.019488967955112457 = 0.010142027400434017 + 0.001 * 9.346940994262695
Epoch 890, val loss: 1.7279324531555176
Epoch 900, training loss: 0.018893102183938026 = 0.009567581117153168 + 0.001 * 9.325520515441895
Epoch 900, val loss: 1.7479569911956787
Epoch 910, training loss: 0.018403295427560806 = 0.009032758884131908 + 0.001 * 9.370536804199219
Epoch 910, val loss: 1.767029881477356
Epoch 920, training loss: 0.017870794981718063 = 0.008541359566152096 + 0.001 * 9.329434394836426
Epoch 920, val loss: 1.7855278253555298
Epoch 930, training loss: 0.017400700598955154 = 0.008085034787654877 + 0.001 * 9.315664291381836
Epoch 930, val loss: 1.8034511804580688
Epoch 940, training loss: 0.01697503961622715 = 0.007662507239729166 + 0.001 * 9.312531471252441
Epoch 940, val loss: 1.821179747581482
Epoch 950, training loss: 0.016596490517258644 = 0.007269901689141989 + 0.001 * 9.32658863067627
Epoch 950, val loss: 1.8379716873168945
Epoch 960, training loss: 0.016273610293865204 = 0.006906104274094105 + 0.001 * 9.367505073547363
Epoch 960, val loss: 1.8543933629989624
Epoch 970, training loss: 0.015917107462882996 = 0.00656865956261754 + 0.001 * 9.348447799682617
Epoch 970, val loss: 1.8704720735549927
Epoch 980, training loss: 0.015582306310534477 = 0.0062520550563931465 + 0.001 * 9.33025074005127
Epoch 980, val loss: 1.886473298072815
Epoch 990, training loss: 0.015262935310602188 = 0.005958006251603365 + 0.001 * 9.304927825927734
Epoch 990, val loss: 1.9019068479537964
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8392
Overall ASR: 0.6628
Flip ASR: 0.5811/1554 nodes
The final ASR:0.66751, 0.01852, Accuracy:0.84052, 0.00096
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97480])
remove edge: torch.Size([2, 79920])
updated graph: torch.Size([2, 88752])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7414
Flip ASR: 0.6795/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72211, 0.01711, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1146361827850342 = 1.1042771339416504 + 0.001 * 10.359076499938965
Epoch 0, val loss: 1.1022781133651733
Epoch 10, training loss: 1.1022588014602661 = 1.0918998718261719 + 0.001 * 10.358952522277832
Epoch 10, val loss: 1.0898940563201904
Epoch 20, training loss: 1.0835829973220825 = 1.073224663734436 + 0.001 * 10.358376502990723
Epoch 20, val loss: 1.0708636045455933
Epoch 30, training loss: 1.0532128810882568 = 1.0428568124771118 + 0.001 * 10.356108665466309
Epoch 30, val loss: 1.0404953956604004
Epoch 40, training loss: 1.0132241249084473 = 1.0028800964355469 + 0.001 * 10.344076156616211
Epoch 40, val loss: 1.0013320446014404
Epoch 50, training loss: 0.9544677138328552 = 0.9442266225814819 + 0.001 * 10.241108894348145
Epoch 50, val loss: 0.9410409927368164
Epoch 60, training loss: 0.8628550171852112 = 0.8530990481376648 + 0.001 * 9.755979537963867
Epoch 60, val loss: 0.850180983543396
Epoch 70, training loss: 0.7449830174446106 = 0.7353509068489075 + 0.001 * 9.632136344909668
Epoch 70, val loss: 0.7351459264755249
Epoch 80, training loss: 0.618218719959259 = 0.608689546585083 + 0.001 * 9.52917766571045
Epoch 80, val loss: 0.6170435547828674
Epoch 90, training loss: 0.5097377896308899 = 0.5002223253250122 + 0.001 * 9.515457153320312
Epoch 90, val loss: 0.5233848690986633
Epoch 100, training loss: 0.43664485216140747 = 0.42713648080825806 + 0.001 * 9.508370399475098
Epoch 100, val loss: 0.4668439030647278
Epoch 110, training loss: 0.39376625418663025 = 0.3842700719833374 + 0.001 * 9.496170997619629
Epoch 110, val loss: 0.4391232132911682
Epoch 120, training loss: 0.3672735393047333 = 0.35778704285621643 + 0.001 * 9.486496925354004
Epoch 120, val loss: 0.42543715238571167
Epoch 130, training loss: 0.34771180152893066 = 0.33823537826538086 + 0.001 * 9.47642993927002
Epoch 130, val loss: 0.4166887104511261
Epoch 140, training loss: 0.33193182945251465 = 0.32245320081710815 + 0.001 * 9.478629112243652
Epoch 140, val loss: 0.4101285934448242
Epoch 150, training loss: 0.3186984360218048 = 0.3092120885848999 + 0.001 * 9.486344337463379
Epoch 150, val loss: 0.405307412147522
Epoch 160, training loss: 0.3071468472480774 = 0.2976546883583069 + 0.001 * 9.492168426513672
Epoch 160, val loss: 0.4021979570388794
Epoch 170, training loss: 0.29676303267478943 = 0.2872643768787384 + 0.001 * 9.49866008758545
Epoch 170, val loss: 0.40072527527809143
Epoch 180, training loss: 0.28723132610321045 = 0.2777264714241028 + 0.001 * 9.504854202270508
Epoch 180, val loss: 0.40062952041625977
Epoch 190, training loss: 0.27836206555366516 = 0.2688509523868561 + 0.001 * 9.511126518249512
Epoch 190, val loss: 0.4016697406768799
Epoch 200, training loss: 0.2700107991695404 = 0.26049336791038513 + 0.001 * 9.517444610595703
Epoch 200, val loss: 0.40371137857437134
Epoch 210, training loss: 0.26207292079925537 = 0.25254932045936584 + 0.001 * 9.523599624633789
Epoch 210, val loss: 0.40672174096107483
Epoch 220, training loss: 0.25444385409355164 = 0.24491462111473083 + 0.001 * 9.529218673706055
Epoch 220, val loss: 0.4105819761753082
Epoch 230, training loss: 0.24706895649433136 = 0.23753541707992554 + 0.001 * 9.53354263305664
Epoch 230, val loss: 0.41527438163757324
Epoch 240, training loss: 0.23991011083126068 = 0.2303750067949295 + 0.001 * 9.535099983215332
Epoch 240, val loss: 0.42082473635673523
Epoch 250, training loss: 0.2329530417919159 = 0.22341299057006836 + 0.001 * 9.540051460266113
Epoch 250, val loss: 0.42713260650634766
Epoch 260, training loss: 0.2261887788772583 = 0.21665440499782562 + 0.001 * 9.534374237060547
Epoch 260, val loss: 0.4344828724861145
Epoch 270, training loss: 0.21965740621089935 = 0.2101288139820099 + 0.001 * 9.528589248657227
Epoch 270, val loss: 0.4428056478500366
Epoch 280, training loss: 0.2133815884590149 = 0.20385229587554932 + 0.001 * 9.529288291931152
Epoch 280, val loss: 0.4519311785697937
Epoch 290, training loss: 0.20734836161136627 = 0.19781890511512756 + 0.001 * 9.529452323913574
Epoch 290, val loss: 0.46292659640312195
Epoch 300, training loss: 0.2015276849269867 = 0.19200193881988525 + 0.001 * 9.525753021240234
Epoch 300, val loss: 0.471985787153244
Epoch 310, training loss: 0.19589249789714813 = 0.18637515604496002 + 0.001 * 9.517341613769531
Epoch 310, val loss: 0.483601838350296
Epoch 320, training loss: 0.1904670000076294 = 0.18093417584896088 + 0.001 * 9.532815933227539
Epoch 320, val loss: 0.49431630969047546
Epoch 330, training loss: 0.1851344108581543 = 0.17562884092330933 + 0.001 * 9.50556755065918
Epoch 330, val loss: 0.5066559314727783
Epoch 340, training loss: 0.17993012070655823 = 0.17040683329105377 + 0.001 * 9.523287773132324
Epoch 340, val loss: 0.5179991722106934
Epoch 350, training loss: 0.1748478263616562 = 0.16534501314163208 + 0.001 * 9.502809524536133
Epoch 350, val loss: 0.5302882790565491
Epoch 360, training loss: 0.1698322296142578 = 0.16034053266048431 + 0.001 * 9.491700172424316
Epoch 360, val loss: 0.5450059175491333
Epoch 370, training loss: 0.16499894857406616 = 0.15547533333301544 + 0.001 * 9.523611068725586
Epoch 370, val loss: 0.5566263794898987
Epoch 380, training loss: 0.16010090708732605 = 0.15061718225479126 + 0.001 * 9.483717918395996
Epoch 380, val loss: 0.5738692879676819
Epoch 390, training loss: 0.15535320341587067 = 0.1458614617586136 + 0.001 * 9.491734504699707
Epoch 390, val loss: 0.5853670835494995
Epoch 400, training loss: 0.15065497159957886 = 0.14118395745754242 + 0.001 * 9.471010208129883
Epoch 400, val loss: 0.6029966473579407
Epoch 410, training loss: 0.1460275650024414 = 0.1365513801574707 + 0.001 * 9.476183891296387
Epoch 410, val loss: 0.6162446141242981
Epoch 420, training loss: 0.1414853036403656 = 0.13201534748077393 + 0.001 * 9.469956398010254
Epoch 420, val loss: 0.6324092745780945
Epoch 430, training loss: 0.13700661063194275 = 0.12754562497138977 + 0.001 * 9.460992813110352
Epoch 430, val loss: 0.6495298147201538
Epoch 440, training loss: 0.13264642655849457 = 0.12318016588687897 + 0.001 * 9.466254234313965
Epoch 440, val loss: 0.6640962362289429
Epoch 450, training loss: 0.12831924855709076 = 0.11885329335927963 + 0.001 * 9.465950012207031
Epoch 450, val loss: 0.6859666705131531
Epoch 460, training loss: 0.12396713346242905 = 0.11445975303649902 + 0.001 * 9.507380485534668
Epoch 460, val loss: 0.6985495090484619
Epoch 470, training loss: 0.11955949664115906 = 0.11010335385799408 + 0.001 * 9.456140518188477
Epoch 470, val loss: 0.7213007211685181
Epoch 480, training loss: 0.11527115106582642 = 0.10580657422542572 + 0.001 * 9.46457290649414
Epoch 480, val loss: 0.7365089654922485
Epoch 490, training loss: 0.11109089106321335 = 0.10160153359174728 + 0.001 * 9.489359855651855
Epoch 490, val loss: 0.7581075429916382
Epoch 500, training loss: 0.10696417093276978 = 0.09749110043048859 + 0.001 * 9.473071098327637
Epoch 500, val loss: 0.7779178023338318
Epoch 510, training loss: 0.10305066406726837 = 0.0935993418097496 + 0.001 * 9.451324462890625
Epoch 510, val loss: 0.7945596575737
Epoch 520, training loss: 0.09896665811538696 = 0.08950679004192352 + 0.001 * 9.459871292114258
Epoch 520, val loss: 0.8215034604072571
Epoch 530, training loss: 0.09496846050024033 = 0.08552078157663345 + 0.001 * 9.447677612304688
Epoch 530, val loss: 0.837024450302124
Epoch 540, training loss: 0.09098591655492783 = 0.08153658360242844 + 0.001 * 9.449331283569336
Epoch 540, val loss: 0.8652429580688477
Epoch 550, training loss: 0.08705954998731613 = 0.07763756811618805 + 0.001 * 9.421981811523438
Epoch 550, val loss: 0.884213924407959
Epoch 560, training loss: 0.08338385075330734 = 0.07396826893091202 + 0.001 * 9.415578842163086
Epoch 560, val loss: 0.9077598452568054
Epoch 570, training loss: 0.0798865333199501 = 0.07043375074863434 + 0.001 * 9.45278263092041
Epoch 570, val loss: 0.9352008700370789
Epoch 580, training loss: 0.0763511061668396 = 0.06693415343761444 + 0.001 * 9.416950225830078
Epoch 580, val loss: 0.9535542130470276
Epoch 590, training loss: 0.07282775640487671 = 0.06340070068836212 + 0.001 * 9.427058219909668
Epoch 590, val loss: 0.9842401742935181
Epoch 600, training loss: 0.06950893253087997 = 0.06007703021168709 + 0.001 * 9.431903839111328
Epoch 600, val loss: 1.0063445568084717
Epoch 610, training loss: 0.06643874198198318 = 0.05701078101992607 + 0.001 * 9.427960395812988
Epoch 610, val loss: 1.0307508707046509
Epoch 620, training loss: 0.06348039954900742 = 0.05406884104013443 + 0.001 * 9.4115571975708
Epoch 620, val loss: 1.061683177947998
Epoch 630, training loss: 0.060493409633636475 = 0.05106198787689209 + 0.001 * 9.431422233581543
Epoch 630, val loss: 1.081286072731018
Epoch 640, training loss: 0.05768324434757233 = 0.048215534538030624 + 0.001 * 9.467711448669434
Epoch 640, val loss: 1.1109321117401123
Epoch 650, training loss: 0.0550137534737587 = 0.04560672119259834 + 0.001 * 9.40703296661377
Epoch 650, val loss: 1.1374011039733887
Epoch 660, training loss: 0.05256086587905884 = 0.043164148926734924 + 0.001 * 9.396716117858887
Epoch 660, val loss: 1.1588034629821777
Epoch 670, training loss: 0.05007413029670715 = 0.04065036028623581 + 0.001 * 9.423770904541016
Epoch 670, val loss: 1.1909047365188599
Epoch 680, training loss: 0.04771067574620247 = 0.038303691893815994 + 0.001 * 9.406982421875
Epoch 680, val loss: 1.21371591091156
Epoch 690, training loss: 0.04559885710477829 = 0.03618896007537842 + 0.001 * 9.409895896911621
Epoch 690, val loss: 1.2385469675064087
Epoch 700, training loss: 0.043517302721738815 = 0.03412318974733353 + 0.001 * 9.394111633300781
Epoch 700, val loss: 1.2686963081359863
Epoch 710, training loss: 0.04150243476033211 = 0.03211751952767372 + 0.001 * 9.384915351867676
Epoch 710, val loss: 1.2900700569152832
Epoch 720, training loss: 0.03963496908545494 = 0.030247073620557785 + 0.001 * 9.387893676757812
Epoch 720, val loss: 1.3180011510849
Epoch 730, training loss: 0.037893328815698624 = 0.02850816212594509 + 0.001 * 9.38516616821289
Epoch 730, val loss: 1.3432034254074097
Epoch 740, training loss: 0.03630877286195755 = 0.02689557522535324 + 0.001 * 9.413195610046387
Epoch 740, val loss: 1.365976333618164
Epoch 750, training loss: 0.03470676392316818 = 0.025329995900392532 + 0.001 * 9.376769065856934
Epoch 750, val loss: 1.3942477703094482
Epoch 760, training loss: 0.03322421759366989 = 0.023846924304962158 + 0.001 * 9.37729263305664
Epoch 760, val loss: 1.4159082174301147
Epoch 770, training loss: 0.03185373544692993 = 0.022467363625764847 + 0.001 * 9.386372566223145
Epoch 770, val loss: 1.4418054819107056
Epoch 780, training loss: 0.030553068965673447 = 0.021175244823098183 + 0.001 * 9.377822875976562
Epoch 780, val loss: 1.4650589227676392
Epoch 790, training loss: 0.029339272528886795 = 0.01995955966413021 + 0.001 * 9.379712104797363
Epoch 790, val loss: 1.4887702465057373
Epoch 800, training loss: 0.028180040419101715 = 0.018810486420989037 + 0.001 * 9.36955451965332
Epoch 800, val loss: 1.5121902227401733
Epoch 810, training loss: 0.027103355154395103 = 0.017726894468069077 + 0.001 * 9.376460075378418
Epoch 810, val loss: 1.5352967977523804
Epoch 820, training loss: 0.02609175629913807 = 0.0167057067155838 + 0.001 * 9.386049270629883
Epoch 820, val loss: 1.558202862739563
Epoch 830, training loss: 0.025088509544730186 = 0.015741607174277306 + 0.001 * 9.346901893615723
Epoch 830, val loss: 1.5803351402282715
Epoch 840, training loss: 0.02420852705836296 = 0.014833717606961727 + 0.001 * 9.374809265136719
Epoch 840, val loss: 1.6029322147369385
Epoch 850, training loss: 0.02331632934510708 = 0.01397344097495079 + 0.001 * 9.342887878417969
Epoch 850, val loss: 1.6244287490844727
Epoch 860, training loss: 0.022498708218336105 = 0.013166457414627075 + 0.001 * 9.332249641418457
Epoch 860, val loss: 1.6453561782836914
Epoch 870, training loss: 0.021773243322968483 = 0.012408539652824402 + 0.001 * 9.364703178405762
Epoch 870, val loss: 1.6667333841323853
Epoch 880, training loss: 0.021089773625135422 = 0.0117015540599823 + 0.001 * 9.388219833374023
Epoch 880, val loss: 1.6867804527282715
Epoch 890, training loss: 0.02039918676018715 = 0.011041953228414059 + 0.001 * 9.357234001159668
Epoch 890, val loss: 1.7080775499343872
Epoch 900, training loss: 0.019754324108362198 = 0.010425830259919167 + 0.001 * 9.328492164611816
Epoch 900, val loss: 1.7273811101913452
Epoch 910, training loss: 0.01917806826531887 = 0.009855560958385468 + 0.001 * 9.32250690460205
Epoch 910, val loss: 1.746241807937622
Epoch 920, training loss: 0.01866970583796501 = 0.009324660524725914 + 0.001 * 9.345046043395996
Epoch 920, val loss: 1.7658147811889648
Epoch 930, training loss: 0.018166720867156982 = 0.008832584135234356 + 0.001 * 9.334136009216309
Epoch 930, val loss: 1.7838953733444214
Epoch 940, training loss: 0.017711160704493523 = 0.008375660516321659 + 0.001 * 9.33549976348877
Epoch 940, val loss: 1.802219033241272
Epoch 950, training loss: 0.017290152609348297 = 0.007950877770781517 + 0.001 * 9.339275360107422
Epoch 950, val loss: 1.820026159286499
Epoch 960, training loss: 0.016881026327610016 = 0.007556232623755932 + 0.001 * 9.324792861938477
Epoch 960, val loss: 1.8372985124588013
Epoch 970, training loss: 0.016523147001862526 = 0.007188745308667421 + 0.001 * 9.33440113067627
Epoch 970, val loss: 1.8540376424789429
Epoch 980, training loss: 0.01616206020116806 = 0.006845707073807716 + 0.001 * 9.316351890563965
Epoch 980, val loss: 1.870294213294983
Epoch 990, training loss: 0.015857134014368057 = 0.006526162847876549 + 0.001 * 9.330970764160156
Epoch 990, val loss: 1.8861430883407593
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8412
Overall ASR: 0.6481
Flip ASR: 0.5644/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.106716513633728 = 1.0963573455810547 + 0.001 * 10.359128952026367
Epoch 0, val loss: 1.0953048467636108
Epoch 10, training loss: 1.0953686237335205 = 1.0850095748901367 + 0.001 * 10.359001159667969
Epoch 10, val loss: 1.0838085412979126
Epoch 20, training loss: 1.0777381658554077 = 1.0673797130584717 + 0.001 * 10.358479499816895
Epoch 20, val loss: 1.0657274723052979
Epoch 30, training loss: 1.048712134361267 = 1.0383557081222534 + 0.001 * 10.356465339660645
Epoch 30, val loss: 1.0363571643829346
Epoch 40, training loss: 1.0067793130874634 = 0.996432900428772 + 0.001 * 10.34644889831543
Epoch 40, val loss: 0.9942288398742676
Epoch 50, training loss: 0.9409408569335938 = 0.9306702017784119 + 0.001 * 10.270630836486816
Epoch 50, val loss: 0.9269295334815979
Epoch 60, training loss: 0.8386982679367065 = 0.8289963006973267 + 0.001 * 9.701972007751465
Epoch 60, val loss: 0.8261463046073914
Epoch 70, training loss: 0.7096901535987854 = 0.7001261115074158 + 0.001 * 9.564066886901855
Epoch 70, val loss: 0.7013298273086548
Epoch 80, training loss: 0.5782617330551147 = 0.5687500238418579 + 0.001 * 9.51168441772461
Epoch 80, val loss: 0.5811663866043091
Epoch 90, training loss: 0.4751518964767456 = 0.4656433165073395 + 0.001 * 9.50858211517334
Epoch 90, val loss: 0.4953775107860565
Epoch 100, training loss: 0.41284605860710144 = 0.40334436297416687 + 0.001 * 9.501686096191406
Epoch 100, val loss: 0.45056065917015076
Epoch 110, training loss: 0.37824565172195435 = 0.3687519133090973 + 0.001 * 9.49372386932373
Epoch 110, val loss: 0.4309992790222168
Epoch 120, training loss: 0.355556845664978 = 0.3460729718208313 + 0.001 * 9.48388385772705
Epoch 120, val loss: 0.42068442702293396
Epoch 130, training loss: 0.33804598450660706 = 0.3285748064517975 + 0.001 * 9.471172332763672
Epoch 130, val loss: 0.41344645619392395
Epoch 140, training loss: 0.32362520694732666 = 0.31415945291519165 + 0.001 * 9.465746879577637
Epoch 140, val loss: 0.408153772354126
Epoch 150, training loss: 0.3112311363220215 = 0.30176034569740295 + 0.001 * 9.470800399780273
Epoch 150, val loss: 0.4048396944999695
Epoch 160, training loss: 0.3002166748046875 = 0.29074203968048096 + 0.001 * 9.474644660949707
Epoch 160, val loss: 0.40325483679771423
Epoch 170, training loss: 0.2901790738105774 = 0.28070059418678284 + 0.001 * 9.478477478027344
Epoch 170, val loss: 0.40307000279426575
Epoch 180, training loss: 0.28088411688804626 = 0.27140146493911743 + 0.001 * 9.482638359069824
Epoch 180, val loss: 0.4039970636367798
Epoch 190, training loss: 0.2721365988254547 = 0.2626497745513916 + 0.001 * 9.486818313598633
Epoch 190, val loss: 0.4060416519641876
Epoch 200, training loss: 0.26381829380989075 = 0.2543272376060486 + 0.001 * 9.491066932678223
Epoch 200, val loss: 0.4091123640537262
Epoch 210, training loss: 0.2558591961860657 = 0.24636398255825043 + 0.001 * 9.495213508605957
Epoch 210, val loss: 0.4131937026977539
Epoch 220, training loss: 0.24820852279663086 = 0.23870940506458282 + 0.001 * 9.499112129211426
Epoch 220, val loss: 0.41823846101760864
Epoch 230, training loss: 0.24084262549877167 = 0.23133999109268188 + 0.001 * 9.502633094787598
Epoch 230, val loss: 0.42427438497543335
Epoch 240, training loss: 0.23375636339187622 = 0.22425074875354767 + 0.001 * 9.505606651306152
Epoch 240, val loss: 0.4314868152141571
Epoch 250, training loss: 0.2269834727048874 = 0.2174755185842514 + 0.001 * 9.507956504821777
Epoch 250, val loss: 0.4395166039466858
Epoch 260, training loss: 0.22050164639949799 = 0.21099218726158142 + 0.001 * 9.509459495544434
Epoch 260, val loss: 0.44844645261764526
Epoch 270, training loss: 0.2143050730228424 = 0.2047945261001587 + 0.001 * 9.510547637939453
Epoch 270, val loss: 0.4573221504688263
Epoch 280, training loss: 0.20844905078411102 = 0.19893571734428406 + 0.001 * 9.513337135314941
Epoch 280, val loss: 0.4664864242076874
Epoch 290, training loss: 0.20276428759098053 = 0.1932538002729416 + 0.001 * 9.510490417480469
Epoch 290, val loss: 0.4764763116836548
Epoch 300, training loss: 0.19731444120407104 = 0.18780645728111267 + 0.001 * 9.507976531982422
Epoch 300, val loss: 0.4881401062011719
Epoch 310, training loss: 0.19208984076976776 = 0.18258216977119446 + 0.001 * 9.507667541503906
Epoch 310, val loss: 0.4979390501976013
Epoch 320, training loss: 0.1869598627090454 = 0.17745555937290192 + 0.001 * 9.50430679321289
Epoch 320, val loss: 0.5114691257476807
Epoch 330, training loss: 0.1819899082183838 = 0.17249354720115662 + 0.001 * 9.49636173248291
Epoch 330, val loss: 0.5206128358840942
Epoch 340, training loss: 0.17717278003692627 = 0.16765393316745758 + 0.001 * 9.518838882446289
Epoch 340, val loss: 0.535428524017334
Epoch 350, training loss: 0.17235586047172546 = 0.1628597527742386 + 0.001 * 9.496114730834961
Epoch 350, val loss: 0.5452473759651184
Epoch 360, training loss: 0.16765795648097992 = 0.15816475450992584 + 0.001 * 9.493195533752441
Epoch 360, val loss: 0.5613100528717041
Epoch 370, training loss: 0.16303299367427826 = 0.1535637527704239 + 0.001 * 9.469246864318848
Epoch 370, val loss: 0.5714338421821594
Epoch 380, training loss: 0.1584792286157608 = 0.14895300567150116 + 0.001 * 9.52621841430664
Epoch 380, val loss: 0.5873157978057861
Epoch 390, training loss: 0.15399165451526642 = 0.1445198655128479 + 0.001 * 9.47178840637207
Epoch 390, val loss: 0.6010252237319946
Epoch 400, training loss: 0.14965680241584778 = 0.14019638299942017 + 0.001 * 9.460413932800293
Epoch 400, val loss: 0.6176848411560059
Epoch 410, training loss: 0.14529933035373688 = 0.13579121232032776 + 0.001 * 9.508122444152832
Epoch 410, val loss: 0.6299018859863281
Epoch 420, training loss: 0.14102448523044586 = 0.13156980276107788 + 0.001 * 9.454682350158691
Epoch 420, val loss: 0.6485978364944458
Epoch 430, training loss: 0.1366758495569229 = 0.12722906470298767 + 0.001 * 9.446781158447266
Epoch 430, val loss: 0.6602455377578735
Epoch 440, training loss: 0.13257981836795807 = 0.1230032816529274 + 0.001 * 9.576539039611816
Epoch 440, val loss: 0.6815692782402039
Epoch 450, training loss: 0.12838231027126312 = 0.11892273277044296 + 0.001 * 9.45958137512207
Epoch 450, val loss: 0.6937206387519836
Epoch 460, training loss: 0.12424379587173462 = 0.11479393392801285 + 0.001 * 9.449862480163574
Epoch 460, val loss: 0.7164610028266907
Epoch 470, training loss: 0.12002235651016235 = 0.11056627333164215 + 0.001 * 9.456080436706543
Epoch 470, val loss: 0.7302892208099365
Epoch 480, training loss: 0.11596626788377762 = 0.10651736706495285 + 0.001 * 9.44890308380127
Epoch 480, val loss: 0.7505016922950745
Epoch 490, training loss: 0.11194796115159988 = 0.10250687599182129 + 0.001 * 9.44108772277832
Epoch 490, val loss: 0.7671864628791809
Epoch 500, training loss: 0.10794846713542938 = 0.09852533787488937 + 0.001 * 9.423126220703125
Epoch 500, val loss: 0.7914974689483643
Epoch 510, training loss: 0.1039794310927391 = 0.09454455226659775 + 0.001 * 9.43487548828125
Epoch 510, val loss: 0.8090588450431824
Epoch 520, training loss: 0.10028092563152313 = 0.09086089581251144 + 0.001 * 9.42003059387207
Epoch 520, val loss: 0.8277063965797424
Epoch 530, training loss: 0.09642848372459412 = 0.08699874579906464 + 0.001 * 9.42973804473877
Epoch 530, val loss: 0.8552641868591309
Epoch 540, training loss: 0.0926976278424263 = 0.08328838646411896 + 0.001 * 9.4092435836792
Epoch 540, val loss: 0.8708001375198364
Epoch 550, training loss: 0.08893024921417236 = 0.07949069142341614 + 0.001 * 9.439560890197754
Epoch 550, val loss: 0.9017423987388611
Epoch 560, training loss: 0.08519989997148514 = 0.07577580213546753 + 0.001 * 9.424098014831543
Epoch 560, val loss: 0.9197825193405151
Epoch 570, training loss: 0.08169833570718765 = 0.07228648662567139 + 0.001 * 9.411847114562988
Epoch 570, val loss: 0.9450576901435852
Epoch 580, training loss: 0.0784420594573021 = 0.06902937591075897 + 0.001 * 9.41268539428711
Epoch 580, val loss: 0.9733191728591919
Epoch 590, training loss: 0.07518403232097626 = 0.06574992090463638 + 0.001 * 9.43410873413086
Epoch 590, val loss: 0.9902457594871521
Epoch 600, training loss: 0.07183357328176498 = 0.06243513897061348 + 0.001 * 9.398433685302734
Epoch 600, val loss: 1.023226022720337
Epoch 610, training loss: 0.06869427114725113 = 0.059303976595401764 + 0.001 * 9.390296936035156
Epoch 610, val loss: 1.043919563293457
Epoch 620, training loss: 0.06577886641025543 = 0.05637924745678902 + 0.001 * 9.399619102478027
Epoch 620, val loss: 1.0702537298202515
Epoch 630, training loss: 0.06308993697166443 = 0.053695641458034515 + 0.001 * 9.394291877746582
Epoch 630, val loss: 1.1017370223999023
Epoch 640, training loss: 0.06029635667800903 = 0.0508727952837944 + 0.001 * 9.423562049865723
Epoch 640, val loss: 1.119364857673645
Epoch 650, training loss: 0.057561419904232025 = 0.04814695194363594 + 0.001 * 9.414468765258789
Epoch 650, val loss: 1.1518661975860596
Epoch 660, training loss: 0.055048618465662 = 0.0456213504076004 + 0.001 * 9.427268028259277
Epoch 660, val loss: 1.1762633323669434
Epoch 670, training loss: 0.052699699997901917 = 0.04330260679125786 + 0.001 * 9.39709186553955
Epoch 670, val loss: 1.1983623504638672
Epoch 680, training loss: 0.05033811554312706 = 0.04093316197395325 + 0.001 * 9.404953002929688
Epoch 680, val loss: 1.2313164472579956
Epoch 690, training loss: 0.04805967956781387 = 0.038676660507917404 + 0.001 * 9.383016586303711
Epoch 690, val loss: 1.2541972398757935
Epoch 700, training loss: 0.04604946821928024 = 0.03668264299631119 + 0.001 * 9.366827011108398
Epoch 700, val loss: 1.2767621278762817
Epoch 710, training loss: 0.04402826726436615 = 0.03465984761714935 + 0.001 * 9.36841869354248
Epoch 710, val loss: 1.309310793876648
Epoch 720, training loss: 0.04210572689771652 = 0.03271465376019478 + 0.001 * 9.391071319580078
Epoch 720, val loss: 1.3295891284942627
Epoch 730, training loss: 0.04030768200755119 = 0.030925603583455086 + 0.001 * 9.382079124450684
Epoch 730, val loss: 1.3561354875564575
Epoch 740, training loss: 0.03865329176187515 = 0.02927834913134575 + 0.001 * 9.374939918518066
Epoch 740, val loss: 1.3846871852874756
Epoch 750, training loss: 0.037012238055467606 = 0.027642423287034035 + 0.001 * 9.369812965393066
Epoch 750, val loss: 1.4048603773117065
Epoch 760, training loss: 0.03547053039073944 = 0.026106376200914383 + 0.001 * 9.364151954650879
Epoch 760, val loss: 1.4320505857467651
Epoch 770, training loss: 0.034051068127155304 = 0.02468855306506157 + 0.001 * 9.362515449523926
Epoch 770, val loss: 1.4569385051727295
Epoch 780, training loss: 0.03271752968430519 = 0.023346658796072006 + 0.001 * 9.370869636535645
Epoch 780, val loss: 1.4782027006149292
Epoch 790, training loss: 0.03142913430929184 = 0.02205190248787403 + 0.001 * 9.377230644226074
Epoch 790, val loss: 1.5051604509353638
Epoch 800, training loss: 0.03021916374564171 = 0.020840205252170563 + 0.001 * 9.378958702087402
Epoch 800, val loss: 1.5267643928527832
Epoch 810, training loss: 0.029069984331727028 = 0.019703391939401627 + 0.001 * 9.366592407226562
Epoch 810, val loss: 1.5508689880371094
Epoch 820, training loss: 0.028001142665743828 = 0.018639754503965378 + 0.001 * 9.361387252807617
Epoch 820, val loss: 1.5748581886291504
Epoch 830, training loss: 0.026986312121152878 = 0.017623169347643852 + 0.001 * 9.363142967224121
Epoch 830, val loss: 1.5959818363189697
Epoch 840, training loss: 0.02600890025496483 = 0.01666371524333954 + 0.001 * 9.345184326171875
Epoch 840, val loss: 1.6200741529464722
Epoch 850, training loss: 0.02514082007110119 = 0.015754086896777153 + 0.001 * 9.386733055114746
Epoch 850, val loss: 1.6410771608352661
Epoch 860, training loss: 0.024262461811304092 = 0.014894841238856316 + 0.001 * 9.367620468139648
Epoch 860, val loss: 1.663190245628357
Epoch 870, training loss: 0.023418091237545013 = 0.014082744717597961 + 0.001 * 9.335346221923828
Epoch 870, val loss: 1.6856627464294434
Epoch 880, training loss: 0.02265068329870701 = 0.013308917172253132 + 0.001 * 9.341765403747559
Epoch 880, val loss: 1.7061783075332642
Epoch 890, training loss: 0.021942991763353348 = 0.012574402615427971 + 0.001 * 9.368587493896484
Epoch 890, val loss: 1.7289912700653076
Epoch 900, training loss: 0.021227478981018066 = 0.011882437393069267 + 0.001 * 9.345040321350098
Epoch 900, val loss: 1.7486250400543213
Epoch 910, training loss: 0.02063092030584812 = 0.011230205185711384 + 0.001 * 9.400714874267578
Epoch 910, val loss: 1.7694538831710815
Epoch 920, training loss: 0.01996535435318947 = 0.010621030814945698 + 0.001 * 9.344322204589844
Epoch 920, val loss: 1.7887519598007202
Epoch 930, training loss: 0.01939161866903305 = 0.010053442791104317 + 0.001 * 9.338175773620605
Epoch 930, val loss: 1.8081533908843994
Epoch 940, training loss: 0.018863659352064133 = 0.009525041095912457 + 0.001 * 9.338618278503418
Epoch 940, val loss: 1.8270095586776733
Epoch 950, training loss: 0.018363378942012787 = 0.009032969363033772 + 0.001 * 9.330409049987793
Epoch 950, val loss: 1.8461540937423706
Epoch 960, training loss: 0.017921164631843567 = 0.008574463427066803 + 0.001 * 9.346701622009277
Epoch 960, val loss: 1.8641176223754883
Epoch 970, training loss: 0.017510458827018738 = 0.008147026412189007 + 0.001 * 9.363430976867676
Epoch 970, val loss: 1.8826544284820557
Epoch 980, training loss: 0.01707294024527073 = 0.007748934905976057 + 0.001 * 9.324004173278809
Epoch 980, val loss: 1.8996999263763428
Epoch 990, training loss: 0.016718532890081406 = 0.007377345580607653 + 0.001 * 9.3411865234375
Epoch 990, val loss: 1.9170106649398804
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8397
Overall ASR: 0.6587
Flip ASR: 0.5766/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1177277565002441 = 1.1073687076568604 + 0.001 * 10.359044075012207
Epoch 0, val loss: 1.106652021408081
Epoch 10, training loss: 1.1046122312545776 = 1.0942533016204834 + 0.001 * 10.358890533447266
Epoch 10, val loss: 1.0932717323303223
Epoch 20, training loss: 1.0847752094268799 = 1.074416995048523 + 0.001 * 10.358175277709961
Epoch 20, val loss: 1.0726500749588013
Epoch 30, training loss: 1.0531214475631714 = 1.0427662134170532 + 0.001 * 10.355268478393555
Epoch 30, val loss: 1.040264368057251
Epoch 40, training loss: 1.0117660760879517 = 1.0014268159866333 + 0.001 * 10.339277267456055
Epoch 40, val loss: 0.9990567564964294
Epoch 50, training loss: 0.9522209763526917 = 0.9420261979103088 + 0.001 * 10.194796562194824
Epoch 50, val loss: 0.9383998513221741
Epoch 60, training loss: 0.8617074489593506 = 0.8519895672798157 + 0.001 * 9.717884063720703
Epoch 60, val loss: 0.8483647108078003
Epoch 70, training loss: 0.7475340366363525 = 0.7379093170166016 + 0.001 * 9.624715805053711
Epoch 70, val loss: 0.7372195720672607
Epoch 80, training loss: 0.6235378384590149 = 0.6140161156654358 + 0.001 * 9.52171516418457
Epoch 80, val loss: 0.6213399767875671
Epoch 90, training loss: 0.5132166743278503 = 0.5037017464637756 + 0.001 * 9.514941215515137
Epoch 90, val loss: 0.5255166888237
Epoch 100, training loss: 0.43702366948127747 = 0.42751365900039673 + 0.001 * 9.510003089904785
Epoch 100, val loss: 0.46598783135414124
Epoch 110, training loss: 0.3927346169948578 = 0.3832361400127411 + 0.001 * 9.4984712600708
Epoch 110, val loss: 0.43738535046577454
Epoch 120, training loss: 0.3662019371986389 = 0.3567085862159729 + 0.001 * 9.493335723876953
Epoch 120, val loss: 0.42438873648643494
Epoch 130, training loss: 0.3468910753726959 = 0.33740749955177307 + 0.001 * 9.483579635620117
Epoch 130, val loss: 0.4163385331630707
Epoch 140, training loss: 0.3312244415283203 = 0.321746826171875 + 0.001 * 9.477609634399414
Epoch 140, val loss: 0.41011160612106323
Epoch 150, training loss: 0.3179725110530853 = 0.30849090218544006 + 0.001 * 9.481595039367676
Epoch 150, val loss: 0.405565470457077
Epoch 160, training loss: 0.3063754439353943 = 0.2968887686729431 + 0.001 * 9.486675262451172
Epoch 160, val loss: 0.4028988182544708
Epoch 170, training loss: 0.29592928290367126 = 0.28643813729286194 + 0.001 * 9.4911470413208
Epoch 170, val loss: 0.401795357465744
Epoch 180, training loss: 0.2863391041755676 = 0.2768430709838867 + 0.001 * 9.49602222442627
Epoch 180, val loss: 0.4018419682979584
Epoch 190, training loss: 0.2774025797843933 = 0.2679023742675781 + 0.001 * 9.500216484069824
Epoch 190, val loss: 0.4029691517353058
Epoch 200, training loss: 0.2689630389213562 = 0.25945979356765747 + 0.001 * 9.503230094909668
Epoch 200, val loss: 0.4050407111644745
Epoch 210, training loss: 0.26091572642326355 = 0.2514120638370514 + 0.001 * 9.503655433654785
Epoch 210, val loss: 0.4080835282802582
Epoch 220, training loss: 0.2531965970993042 = 0.24369679391384125 + 0.001 * 9.49980640411377
Epoch 220, val loss: 0.41200903058052063
Epoch 230, training loss: 0.24577152729034424 = 0.23627372086048126 + 0.001 * 9.49781322479248
Epoch 230, val loss: 0.4166742265224457
Epoch 240, training loss: 0.23860742151737213 = 0.22911378741264343 + 0.001 * 9.493639945983887
Epoch 240, val loss: 0.42232179641723633
Epoch 250, training loss: 0.23169347643852234 = 0.22220133244991302 + 0.001 * 9.492149353027344
Epoch 250, val loss: 0.42891016602516174
Epoch 260, training loss: 0.22503897547721863 = 0.21555206179618835 + 0.001 * 9.486907005310059
Epoch 260, val loss: 0.4363739490509033
Epoch 270, training loss: 0.21867164969444275 = 0.2091844081878662 + 0.001 * 9.487239837646484
Epoch 270, val loss: 0.4449152946472168
Epoch 280, training loss: 0.21258243918418884 = 0.20310412347316742 + 0.001 * 9.47831916809082
Epoch 280, val loss: 0.45438051223754883
Epoch 290, training loss: 0.2067950814962387 = 0.1973174512386322 + 0.001 * 9.477624893188477
Epoch 290, val loss: 0.46530431509017944
Epoch 300, training loss: 0.2012459635734558 = 0.1917605698108673 + 0.001 * 9.48538875579834
Epoch 300, val loss: 0.47432607412338257
Epoch 310, training loss: 0.1959073394536972 = 0.18642327189445496 + 0.001 * 9.484073638916016
Epoch 310, val loss: 0.4846578538417816
Epoch 320, training loss: 0.19071875512599945 = 0.18124951422214508 + 0.001 * 9.46924114227295
Epoch 320, val loss: 0.4969973564147949
Epoch 330, training loss: 0.18571536242961884 = 0.176235631108284 + 0.001 * 9.479726791381836
Epoch 330, val loss: 0.5080196857452393
Epoch 340, training loss: 0.18090160191059113 = 0.17141979932785034 + 0.001 * 9.481801986694336
Epoch 340, val loss: 0.5215319991111755
Epoch 350, training loss: 0.1761227399110794 = 0.16664153337478638 + 0.001 * 9.481206893920898
Epoch 350, val loss: 0.5323584675788879
Epoch 360, training loss: 0.17139475047588348 = 0.1619420349597931 + 0.001 * 9.452715873718262
Epoch 360, val loss: 0.5474992394447327
Epoch 370, training loss: 0.16685788333415985 = 0.157403826713562 + 0.001 * 9.454058647155762
Epoch 370, val loss: 0.5582605004310608
Epoch 380, training loss: 0.16227617859840393 = 0.15281842648983002 + 0.001 * 9.457756996154785
Epoch 380, val loss: 0.5735576748847961
Epoch 390, training loss: 0.15784361958503723 = 0.14839205145835876 + 0.001 * 9.451566696166992
Epoch 390, val loss: 0.5884328484535217
Epoch 400, training loss: 0.1536276787519455 = 0.14419864118099213 + 0.001 * 9.429031372070312
Epoch 400, val loss: 0.6048044562339783
Epoch 410, training loss: 0.14928264915943146 = 0.1398511528968811 + 0.001 * 9.431489944458008
Epoch 410, val loss: 0.6170114874839783
Epoch 420, training loss: 0.14515575766563416 = 0.13573625683784485 + 0.001 * 9.419495582580566
Epoch 420, val loss: 0.6353654861450195
Epoch 430, training loss: 0.14098557829856873 = 0.13157612085342407 + 0.001 * 9.409455299377441
Epoch 430, val loss: 0.6474757194519043
Epoch 440, training loss: 0.1369055062532425 = 0.12749361991882324 + 0.001 * 9.411881446838379
Epoch 440, val loss: 0.6671651005744934
Epoch 450, training loss: 0.1329788863658905 = 0.12351605296134949 + 0.001 * 9.462825775146484
Epoch 450, val loss: 0.6803968548774719
Epoch 460, training loss: 0.12898702919483185 = 0.11955607682466507 + 0.001 * 9.430954933166504
Epoch 460, val loss: 0.6998133659362793
Epoch 470, training loss: 0.12515485286712646 = 0.11574562638998032 + 0.001 * 9.409223556518555
Epoch 470, val loss: 0.7173900604248047
Epoch 480, training loss: 0.12134838849306107 = 0.11191710829734802 + 0.001 * 9.43127727508545
Epoch 480, val loss: 0.7324166893959045
Epoch 490, training loss: 0.11758319288492203 = 0.10816379636526108 + 0.001 * 9.419393539428711
Epoch 490, val loss: 0.755743145942688
Epoch 500, training loss: 0.11371196061372757 = 0.10431664437055588 + 0.001 * 9.395318031311035
Epoch 500, val loss: 0.7688562273979187
Epoch 510, training loss: 0.10993503034114838 = 0.10053969919681549 + 0.001 * 9.395332336425781
Epoch 510, val loss: 0.7931446433067322
Epoch 520, training loss: 0.10615547746419907 = 0.09676995873451233 + 0.001 * 9.385520935058594
Epoch 520, val loss: 0.8105466365814209
Epoch 530, training loss: 0.10284896194934845 = 0.09346882998943329 + 0.001 * 9.380135536193848
Epoch 530, val loss: 0.8278557658195496
Epoch 540, training loss: 0.09895674139261246 = 0.08955515176057816 + 0.001 * 9.401590347290039
Epoch 540, val loss: 0.8528174161911011
Epoch 550, training loss: 0.09536996483802795 = 0.08599281311035156 + 0.001 * 9.377154350280762
Epoch 550, val loss: 0.8727445602416992
Epoch 560, training loss: 0.09207262098789215 = 0.08269956707954407 + 0.001 * 9.373052597045898
Epoch 560, val loss: 0.8991451263427734
Epoch 570, training loss: 0.08852002769708633 = 0.07912939786911011 + 0.001 * 9.390626907348633
Epoch 570, val loss: 0.9142502546310425
Epoch 580, training loss: 0.08501281589269638 = 0.07563789188861847 + 0.001 * 9.37492561340332
Epoch 580, val loss: 0.9414576292037964
Epoch 590, training loss: 0.08182572573423386 = 0.07244224846363068 + 0.001 * 9.383476257324219
Epoch 590, val loss: 0.965164303779602
Epoch 600, training loss: 0.07864400744438171 = 0.06927280873060226 + 0.001 * 9.371200561523438
Epoch 600, val loss: 0.9829635620117188
Epoch 610, training loss: 0.07542100548744202 = 0.06603744626045227 + 0.001 * 9.383560180664062
Epoch 610, val loss: 1.0140694379806519
Epoch 620, training loss: 0.07223108410835266 = 0.06285061687231064 + 0.001 * 9.380468368530273
Epoch 620, val loss: 1.0342445373535156
Epoch 630, training loss: 0.06949494034051895 = 0.06011812016367912 + 0.001 * 9.37682056427002
Epoch 630, val loss: 1.055719256401062
Epoch 640, training loss: 0.06655927747488022 = 0.05718434602022171 + 0.001 * 9.37492847442627
Epoch 640, val loss: 1.0884631872177124
Epoch 650, training loss: 0.06374790519475937 = 0.05434197559952736 + 0.001 * 9.405926704406738
Epoch 650, val loss: 1.10640287399292
Epoch 660, training loss: 0.06091994047164917 = 0.05156277120113373 + 0.001 * 9.357169151306152
Epoch 660, val loss: 1.1376352310180664
Epoch 670, training loss: 0.05834365636110306 = 0.048981405794620514 + 0.001 * 9.362250328063965
Epoch 670, val loss: 1.1621556282043457
Epoch 680, training loss: 0.0559735968708992 = 0.04660046473145485 + 0.001 * 9.373130798339844
Epoch 680, val loss: 1.1833261251449585
Epoch 690, training loss: 0.053503140807151794 = 0.044131189584732056 + 0.001 * 9.37195110321045
Epoch 690, val loss: 1.2165195941925049
Epoch 700, training loss: 0.05110929161310196 = 0.04174372926354408 + 0.001 * 9.365562438964844
Epoch 700, val loss: 1.2376430034637451
Epoch 710, training loss: 0.048907581716775894 = 0.039551183581352234 + 0.001 * 9.35639762878418
Epoch 710, val loss: 1.2640939950942993
Epoch 720, training loss: 0.04687279835343361 = 0.037509575486183167 + 0.001 * 9.363222122192383
Epoch 720, val loss: 1.2937943935394287
Epoch 730, training loss: 0.04479394480586052 = 0.03543990105390549 + 0.001 * 9.354043960571289
Epoch 730, val loss: 1.3140820264816284
Epoch 740, training loss: 0.04285760968923569 = 0.033485788851976395 + 0.001 * 9.371818542480469
Epoch 740, val loss: 1.341551423072815
Epoch 750, training loss: 0.04109974205493927 = 0.03170165419578552 + 0.001 * 9.398085594177246
Epoch 750, val loss: 1.3679561614990234
Epoch 760, training loss: 0.03934232518076897 = 0.02997341938316822 + 0.001 * 9.368904113769531
Epoch 760, val loss: 1.3887296915054321
Epoch 770, training loss: 0.03764820471405983 = 0.028295470401644707 + 0.001 * 9.352734565734863
Epoch 770, val loss: 1.4159470796585083
Epoch 780, training loss: 0.03611819073557854 = 0.026754291728138924 + 0.001 * 9.363897323608398
Epoch 780, val loss: 1.441079020500183
Epoch 790, training loss: 0.03463558107614517 = 0.0252931397408247 + 0.001 * 9.34244155883789
Epoch 790, val loss: 1.4617494344711304
Epoch 800, training loss: 0.0332188718020916 = 0.02386852167546749 + 0.001 * 9.350349426269531
Epoch 800, val loss: 1.4894291162490845
Epoch 810, training loss: 0.03193522244691849 = 0.0225269366055727 + 0.001 * 9.408287048339844
Epoch 810, val loss: 1.5109201669692993
Epoch 820, training loss: 0.030635790899395943 = 0.0212700292468071 + 0.001 * 9.365760803222656
Epoch 820, val loss: 1.5343669652938843
Epoch 830, training loss: 0.029439808800816536 = 0.02008465863764286 + 0.001 * 9.355149269104004
Epoch 830, val loss: 1.5590507984161377
Epoch 840, training loss: 0.028290117159485817 = 0.01893804781138897 + 0.001 * 9.352068901062012
Epoch 840, val loss: 1.5793923139572144
Epoch 850, training loss: 0.027220837771892548 = 0.017856841906905174 + 0.001 * 9.363995552062988
Epoch 850, val loss: 1.6031670570373535
Epoch 860, training loss: 0.02616024948656559 = 0.0168323814868927 + 0.001 * 9.32786750793457
Epoch 860, val loss: 1.624652624130249
Epoch 870, training loss: 0.02519873157143593 = 0.01586414873600006 + 0.001 * 9.334583282470703
Epoch 870, val loss: 1.6466472148895264
Epoch 880, training loss: 0.024274393916130066 = 0.014948176220059395 + 0.001 * 9.326218605041504
Epoch 880, val loss: 1.6679346561431885
Epoch 890, training loss: 0.023433765396475792 = 0.014082306064665318 + 0.001 * 9.351458549499512
Epoch 890, val loss: 1.6892340183258057
Epoch 900, training loss: 0.02259248122572899 = 0.013270679861307144 + 0.001 * 9.321800231933594
Epoch 900, val loss: 1.710432767868042
Epoch 910, training loss: 0.0218462273478508 = 0.01251235045492649 + 0.001 * 9.333877563476562
Epoch 910, val loss: 1.7303447723388672
Epoch 920, training loss: 0.02112402766942978 = 0.011805790476500988 + 0.001 * 9.3182373046875
Epoch 920, val loss: 1.750914216041565
Epoch 930, training loss: 0.020479895174503326 = 0.01114847231656313 + 0.001 * 9.33142375946045
Epoch 930, val loss: 1.7702593803405762
Epoch 940, training loss: 0.01987048238515854 = 0.010537678375840187 + 0.001 * 9.332804679870605
Epoch 940, val loss: 1.7900868654251099
Epoch 950, training loss: 0.019290495663881302 = 0.009971234016120434 + 0.001 * 9.31926155090332
Epoch 950, val loss: 1.8091706037521362
Epoch 960, training loss: 0.018769728019833565 = 0.00944542232900858 + 0.001 * 9.324305534362793
Epoch 960, val loss: 1.8278826475143433
Epoch 970, training loss: 0.018282655626535416 = 0.008954554796218872 + 0.001 * 9.328100204467773
Epoch 970, val loss: 1.8464446067810059
Epoch 980, training loss: 0.01782812550663948 = 0.00849936343729496 + 0.001 * 9.328761100769043
Epoch 980, val loss: 1.8649367094039917
Epoch 990, training loss: 0.01738346368074417 = 0.008074698969721794 + 0.001 * 9.308764457702637
Epoch 990, val loss: 1.882787823677063
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8422
Overall ASR: 0.6719
Flip ASR: 0.5927/1554 nodes
The final ASR:0.65957, 0.00975, Accuracy:0.84103, 0.00104
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97542])
remove edge: torch.Size([2, 79770])
updated graph: torch.Size([2, 88664])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8508
Overall ASR: 0.7622
Flip ASR: 0.7040/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72904, 0.02561, Accuracy:0.85033, 0.00041
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1061664819717407 = 1.095807433128357 + 0.001 * 10.359076499938965
Epoch 0, val loss: 1.0942909717559814
Epoch 10, training loss: 1.0949046611785889 = 1.0845457315444946 + 0.001 * 10.358928680419922
Epoch 10, val loss: 1.0829405784606934
Epoch 20, training loss: 1.0775617361068726 = 1.067203402519226 + 0.001 * 10.358322143554688
Epoch 20, val loss: 1.0652018785476685
Epoch 30, training loss: 1.0485939979553223 = 1.0382378101348877 + 0.001 * 10.35618782043457
Epoch 30, val loss: 1.0360406637191772
Epoch 40, training loss: 1.0061335563659668 = 0.9957873225212097 + 0.001 * 10.346230506896973
Epoch 40, val loss: 0.9936093091964722
Epoch 50, training loss: 0.9388677477836609 = 0.9285984635353088 + 0.001 * 10.269306182861328
Epoch 50, val loss: 0.9248243570327759
Epoch 60, training loss: 0.836829423904419 = 0.8270831108093262 + 0.001 * 9.746285438537598
Epoch 60, val loss: 0.8237210512161255
Epoch 70, training loss: 0.7107160687446594 = 0.7010842561721802 + 0.001 * 9.63178539276123
Epoch 70, val loss: 0.7023374438285828
Epoch 80, training loss: 0.5842651724815369 = 0.5747297406196594 + 0.001 * 9.535406112670898
Epoch 80, val loss: 0.586972713470459
Epoch 90, training loss: 0.4844347834587097 = 0.47492387890815735 + 0.001 * 9.510915756225586
Epoch 90, val loss: 0.503022313117981
Epoch 100, training loss: 0.4206881523132324 = 0.4111849367618561 + 0.001 * 9.50320816040039
Epoch 100, val loss: 0.45556172728538513
Epoch 110, training loss: 0.3838854730129242 = 0.37440061569213867 + 0.001 * 9.484864234924316
Epoch 110, val loss: 0.4338054656982422
Epoch 120, training loss: 0.3601379692554474 = 0.35066792368888855 + 0.001 * 9.470049858093262
Epoch 120, val loss: 0.42326003313064575
Epoch 130, training loss: 0.3418792188167572 = 0.3324221968650818 + 0.001 * 9.457026481628418
Epoch 130, val loss: 0.41598865389823914
Epoch 140, training loss: 0.3268613815307617 = 0.3174056112766266 + 0.001 * 9.455769538879395
Epoch 140, val loss: 0.41025665402412415
Epoch 150, training loss: 0.3140627443790436 = 0.30460330843925476 + 0.001 * 9.45943832397461
Epoch 150, val loss: 0.406179279088974
Epoch 160, training loss: 0.3027629852294922 = 0.29330167174339294 + 0.001 * 9.461320877075195
Epoch 160, val loss: 0.4038139879703522
Epoch 170, training loss: 0.29251062870025635 = 0.2830467224121094 + 0.001 * 9.463907241821289
Epoch 170, val loss: 0.40291741490364075
Epoch 180, training loss: 0.2830527424812317 = 0.2735861539840698 + 0.001 * 9.46657657623291
Epoch 180, val loss: 0.4032631516456604
Epoch 190, training loss: 0.27419808506965637 = 0.26472848653793335 + 0.001 * 9.4696044921875
Epoch 190, val loss: 0.40482816100120544
Epoch 200, training loss: 0.26581403613090515 = 0.2563410997390747 + 0.001 * 9.472949981689453
Epoch 200, val loss: 0.40747156739234924
Epoch 210, training loss: 0.25780189037323 = 0.2483254224061966 + 0.001 * 9.476454734802246
Epoch 210, val loss: 0.4110817015171051
Epoch 220, training loss: 0.2501206398010254 = 0.24064059555530548 + 0.001 * 9.480046272277832
Epoch 220, val loss: 0.4156033396720886
Epoch 230, training loss: 0.24273933470249176 = 0.23325571417808533 + 0.001 * 9.483624458312988
Epoch 230, val loss: 0.4209766089916229
Epoch 240, training loss: 0.23564068973064423 = 0.22615370154380798 + 0.001 * 9.486982345581055
Epoch 240, val loss: 0.4272531569004059
Epoch 250, training loss: 0.2288031280040741 = 0.21931324899196625 + 0.001 * 9.489882469177246
Epoch 250, val loss: 0.4345766007900238
Epoch 260, training loss: 0.22223661839962006 = 0.21274414658546448 + 0.001 * 9.492465019226074
Epoch 260, val loss: 0.44330108165740967
Epoch 270, training loss: 0.21596470475196838 = 0.20646828413009644 + 0.001 * 9.496427536010742
Epoch 270, val loss: 0.45164385437965393
Epoch 280, training loss: 0.20999440550804138 = 0.20049937069416046 + 0.001 * 9.495034217834473
Epoch 280, val loss: 0.462202787399292
Epoch 290, training loss: 0.20429986715316772 = 0.19480566680431366 + 0.001 * 9.4942045211792
Epoch 290, val loss: 0.47315865755081177
Epoch 300, training loss: 0.1988658607006073 = 0.1893695890903473 + 0.001 * 9.496278762817383
Epoch 300, val loss: 0.48414409160614014
Epoch 310, training loss: 0.19364570081233978 = 0.18415690958499908 + 0.001 * 9.488797187805176
Epoch 310, val loss: 0.49391624331474304
Epoch 320, training loss: 0.18858478963375092 = 0.17908774316310883 + 0.001 * 9.497042655944824
Epoch 320, val loss: 0.5075429677963257
Epoch 330, training loss: 0.18367230892181396 = 0.174189031124115 + 0.001 * 9.483274459838867
Epoch 330, val loss: 0.5173091292381287
Epoch 340, training loss: 0.17890425026416779 = 0.16942404210567474 + 0.001 * 9.480205535888672
Epoch 340, val loss: 0.5324301719665527
Epoch 350, training loss: 0.17422862350940704 = 0.16475459933280945 + 0.001 * 9.474027633666992
Epoch 350, val loss: 0.5425946712493896
Epoch 360, training loss: 0.1696493774652481 = 0.16015464067459106 + 0.001 * 9.494732856750488
Epoch 360, val loss: 0.5588210225105286
Epoch 370, training loss: 0.16507306694984436 = 0.15560634434223175 + 0.001 * 9.466718673706055
Epoch 370, val loss: 0.5691261887550354
Epoch 380, training loss: 0.1605587750673294 = 0.15109717845916748 + 0.001 * 9.461601257324219
Epoch 380, val loss: 0.5855191946029663
Epoch 390, training loss: 0.15616664290428162 = 0.14666682481765747 + 0.001 * 9.499811172485352
Epoch 390, val loss: 0.5982571840286255
Epoch 400, training loss: 0.15177291631698608 = 0.14231912791728973 + 0.001 * 9.453794479370117
Epoch 400, val loss: 0.6141213178634644
Epoch 410, training loss: 0.14748665690422058 = 0.13804586231708527 + 0.001 * 9.44079875946045
Epoch 410, val loss: 0.6289805173873901
Epoch 420, training loss: 0.1433885544538498 = 0.13392792642116547 + 0.001 * 9.460627555847168
Epoch 420, val loss: 0.6423513889312744
Epoch 430, training loss: 0.13924367725849152 = 0.12978093326091766 + 0.001 * 9.46274185180664
Epoch 430, val loss: 0.6633099913597107
Epoch 440, training loss: 0.13506056368350983 = 0.12560763955116272 + 0.001 * 9.452924728393555
Epoch 440, val loss: 0.6749280095100403
Epoch 450, training loss: 0.1308974027633667 = 0.12145639210939407 + 0.001 * 9.441003799438477
Epoch 450, val loss: 0.6963038444519043
Epoch 460, training loss: 0.12681198120117188 = 0.11737939715385437 + 0.001 * 9.432584762573242
Epoch 460, val loss: 0.7114586234092712
Epoch 470, training loss: 0.12279272824525833 = 0.11336629092693329 + 0.001 * 9.426437377929688
Epoch 470, val loss: 0.7312725782394409
Epoch 480, training loss: 0.11884354054927826 = 0.10940000414848328 + 0.001 * 9.443535804748535
Epoch 480, val loss: 0.7493942379951477
Epoch 490, training loss: 0.11505185067653656 = 0.10561083257198334 + 0.001 * 9.441014289855957
Epoch 490, val loss: 0.7655321359634399
Epoch 500, training loss: 0.11113428324460983 = 0.1017024889588356 + 0.001 * 9.431796073913574
Epoch 500, val loss: 0.7913596034049988
Epoch 510, training loss: 0.10717073082923889 = 0.09774331748485565 + 0.001 * 9.427410125732422
Epoch 510, val loss: 0.8049554228782654
Epoch 520, training loss: 0.10325179249048233 = 0.09383125603199005 + 0.001 * 9.420536041259766
Epoch 520, val loss: 0.8308606743812561
Epoch 530, training loss: 0.09944935142993927 = 0.09001576900482178 + 0.001 * 9.43358325958252
Epoch 530, val loss: 0.8486863970756531
Epoch 540, training loss: 0.09574117511510849 = 0.08632717281579971 + 0.001 * 9.4140043258667
Epoch 540, val loss: 0.870887815952301
Epoch 550, training loss: 0.0922713652253151 = 0.08282532542943954 + 0.001 * 9.446039199829102
Epoch 550, val loss: 0.8967165350914001
Epoch 560, training loss: 0.08864180743694305 = 0.07922080159187317 + 0.001 * 9.421003341674805
Epoch 560, val loss: 0.9126321077346802
Epoch 570, training loss: 0.08504798263311386 = 0.07563645392656326 + 0.001 * 9.411526679992676
Epoch 570, val loss: 0.9429603219032288
Epoch 580, training loss: 0.08149180561304092 = 0.07207798957824707 + 0.001 * 9.413816452026367
Epoch 580, val loss: 0.9607535004615784
Epoch 590, training loss: 0.07808171957731247 = 0.06867031753063202 + 0.001 * 9.411402702331543
Epoch 590, val loss: 0.9875205159187317
Epoch 600, training loss: 0.07492746412754059 = 0.06552372872829437 + 0.001 * 9.40373420715332
Epoch 600, val loss: 1.0141171216964722
Epoch 610, training loss: 0.07178007811307907 = 0.06238451972603798 + 0.001 * 9.395560264587402
Epoch 610, val loss: 1.03264582157135
Epoch 620, training loss: 0.06856656819581985 = 0.059170398861169815 + 0.001 * 9.396170616149902
Epoch 620, val loss: 1.0657916069030762
Epoch 630, training loss: 0.0654563307762146 = 0.056057579815387726 + 0.001 * 9.398754119873047
Epoch 630, val loss: 1.087099313735962
Epoch 640, training loss: 0.06261782348155975 = 0.05322577804327011 + 0.001 * 9.392045974731445
Epoch 640, val loss: 1.112738847732544
Epoch 650, training loss: 0.059897445142269135 = 0.05048505216836929 + 0.001 * 9.412393569946289
Epoch 650, val loss: 1.1463006734848022
Epoch 660, training loss: 0.05706074833869934 = 0.04766657203435898 + 0.001 * 9.394177436828613
Epoch 660, val loss: 1.1660441160202026
Epoch 670, training loss: 0.05442066863179207 = 0.045018598437309265 + 0.001 * 9.402070999145508
Epoch 670, val loss: 1.196755051612854
Epoch 680, training loss: 0.052021462470293045 = 0.04264090955257416 + 0.001 * 9.380552291870117
Epoch 680, val loss: 1.2252626419067383
Epoch 690, training loss: 0.04966969043016434 = 0.04028719291090965 + 0.001 * 9.382494926452637
Epoch 690, val loss: 1.2461066246032715
Epoch 700, training loss: 0.04734282195568085 = 0.03794841095805168 + 0.001 * 9.39441204071045
Epoch 700, val loss: 1.277501106262207
Epoch 710, training loss: 0.04522009193897247 = 0.035830557346343994 + 0.001 * 9.389533996582031
Epoch 710, val loss: 1.30307936668396
Epoch 720, training loss: 0.04325811192393303 = 0.03386419638991356 + 0.001 * 9.393914222717285
Epoch 720, val loss: 1.3252252340316772
Epoch 730, training loss: 0.04128347337245941 = 0.03190193325281143 + 0.001 * 9.381537437438965
Epoch 730, val loss: 1.3559209108352661
Epoch 740, training loss: 0.03945503011345863 = 0.030066730454564095 + 0.001 * 9.388298988342285
Epoch 740, val loss: 1.3788480758666992
Epoch 750, training loss: 0.03776540607213974 = 0.02838374301791191 + 0.001 * 9.381662368774414
Epoch 750, val loss: 1.4031261205673218
Epoch 760, training loss: 0.03614722937345505 = 0.026778560131788254 + 0.001 * 9.368668556213379
Epoch 760, val loss: 1.4314783811569214
Epoch 770, training loss: 0.03466152399778366 = 0.025228090584278107 + 0.001 * 9.433430671691895
Epoch 770, val loss: 1.4520624876022339
Epoch 780, training loss: 0.033166930079460144 = 0.023770838975906372 + 0.001 * 9.396090507507324
Epoch 780, val loss: 1.4792128801345825
Epoch 790, training loss: 0.03177672624588013 = 0.022405557334423065 + 0.001 * 9.371167182922363
Epoch 790, val loss: 1.50210702419281
Epoch 800, training loss: 0.030501972883939743 = 0.021135514602065086 + 0.001 * 9.36645793914795
Epoch 800, val loss: 1.5249691009521484
Epoch 810, training loss: 0.029284266754984856 = 0.01992225833237171 + 0.001 * 9.362008094787598
Epoch 810, val loss: 1.5503132343292236
Epoch 820, training loss: 0.028155772015452385 = 0.01878301426768303 + 0.001 * 9.372756958007812
Epoch 820, val loss: 1.571026086807251
Epoch 830, training loss: 0.027059540152549744 = 0.01769549399614334 + 0.001 * 9.364045143127441
Epoch 830, val loss: 1.5963728427886963
Epoch 840, training loss: 0.026024263352155685 = 0.01667235791683197 + 0.001 * 9.351905822753906
Epoch 840, val loss: 1.6173075437545776
Epoch 850, training loss: 0.025098571553826332 = 0.01570999249815941 + 0.001 * 9.388578414916992
Epoch 850, val loss: 1.6399788856506348
Epoch 860, training loss: 0.024182360619306564 = 0.014806031249463558 + 0.001 * 9.376328468322754
Epoch 860, val loss: 1.66176438331604
Epoch 870, training loss: 0.02335912361741066 = 0.013957154005765915 + 0.001 * 9.401968955993652
Epoch 870, val loss: 1.6824321746826172
Epoch 880, training loss: 0.022525697946548462 = 0.013151927851140499 + 0.001 * 9.373770713806152
Epoch 880, val loss: 1.7046523094177246
Epoch 890, training loss: 0.02175780199468136 = 0.012399071827530861 + 0.001 * 9.358729362487793
Epoch 890, val loss: 1.7244725227355957
Epoch 900, training loss: 0.02107211761176586 = 0.011695122346282005 + 0.001 * 9.376995086669922
Epoch 900, val loss: 1.7458713054656982
Epoch 910, training loss: 0.020390702411532402 = 0.011041049845516682 + 0.001 * 9.349652290344238
Epoch 910, val loss: 1.7659111022949219
Epoch 920, training loss: 0.019795281812548637 = 0.01043093390762806 + 0.001 * 9.364347457885742
Epoch 920, val loss: 1.7846065759658813
Epoch 930, training loss: 0.019199445843696594 = 0.009863724932074547 + 0.001 * 9.335719108581543
Epoch 930, val loss: 1.8045811653137207
Epoch 940, training loss: 0.01867036521434784 = 0.009335285983979702 + 0.001 * 9.335079193115234
Epoch 940, val loss: 1.8232333660125732
Epoch 950, training loss: 0.018179569393396378 = 0.008845691569149494 + 0.001 * 9.333876609802246
Epoch 950, val loss: 1.8414280414581299
Epoch 960, training loss: 0.017765583470463753 = 0.008388456888496876 + 0.001 * 9.37712574005127
Epoch 960, val loss: 1.859958529472351
Epoch 970, training loss: 0.01730961538851261 = 0.00796546507626772 + 0.001 * 9.344149589538574
Epoch 970, val loss: 1.8783533573150635
Epoch 980, training loss: 0.016956068575382233 = 0.007569135166704655 + 0.001 * 9.386932373046875
Epoch 980, val loss: 1.8955045938491821
Epoch 990, training loss: 0.016530916094779968 = 0.007200122345238924 + 0.001 * 9.330792427062988
Epoch 990, val loss: 1.9122300148010254
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8387
Overall ASR: 0.6699
Flip ASR: 0.5901/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.0986937284469604 = 1.0883346796035767 + 0.001 * 10.359086036682129
Epoch 0, val loss: 1.0879768133163452
Epoch 10, training loss: 1.0887160301208496 = 1.0783569812774658 + 0.001 * 10.35899543762207
Epoch 10, val loss: 1.077959656715393
Epoch 20, training loss: 1.0737814903259277 = 1.0634229183197021 + 0.001 * 10.358539581298828
Epoch 20, val loss: 1.0625956058502197
Epoch 30, training loss: 1.0476030111312866 = 1.0372459888458252 + 0.001 * 10.356982231140137
Epoch 30, val loss: 1.0357457399368286
Epoch 40, training loss: 1.0036840438842773 = 0.9933335781097412 + 0.001 * 10.35051155090332
Epoch 40, val loss: 0.9908498525619507
Epoch 50, training loss: 0.9313851594924927 = 0.9210838079452515 + 0.001 * 10.30135440826416
Epoch 50, val loss: 0.9172177314758301
Epoch 60, training loss: 0.8215379118919373 = 0.8118318915367126 + 0.001 * 9.705991744995117
Epoch 60, val loss: 0.8093422651290894
Epoch 70, training loss: 0.687157392501831 = 0.6776117086410522 + 0.001 * 9.545672416687012
Epoch 70, val loss: 0.6807861328125
Epoch 80, training loss: 0.5572942495346069 = 0.5477882623672485 + 0.001 * 9.505983352661133
Epoch 80, val loss: 0.5645290017127991
Epoch 90, training loss: 0.4622780382633209 = 0.45277708768844604 + 0.001 * 9.500960350036621
Epoch 90, val loss: 0.487773060798645
Epoch 100, training loss: 0.4066625237464905 = 0.3971674144268036 + 0.001 * 9.49512004852295
Epoch 100, val loss: 0.449313759803772
Epoch 110, training loss: 0.3744036555290222 = 0.3649143874645233 + 0.001 * 9.489274024963379
Epoch 110, val loss: 0.43109196424484253
Epoch 120, training loss: 0.3523108959197998 = 0.3428298830986023 + 0.001 * 9.481003761291504
Epoch 120, val loss: 0.42048922181129456
Epoch 130, training loss: 0.3351173996925354 = 0.32564783096313477 + 0.001 * 9.469582557678223
Epoch 130, val loss: 0.41301852464675903
Epoch 140, training loss: 0.32092908024787903 = 0.311465322971344 + 0.001 * 9.463746070861816
Epoch 140, val loss: 0.40776941180229187
Epoch 150, training loss: 0.30879873037338257 = 0.2993289530277252 + 0.001 * 9.469782829284668
Epoch 150, val loss: 0.40457528829574585
Epoch 160, training loss: 0.29807233810424805 = 0.2885974049568176 + 0.001 * 9.47494125366211
Epoch 160, val loss: 0.40329480171203613
Epoch 170, training loss: 0.28833189606666565 = 0.27885133028030396 + 0.001 * 9.480558395385742
Epoch 170, val loss: 0.4034671485424042
Epoch 180, training loss: 0.27933019399642944 = 0.26984336972236633 + 0.001 * 9.48681354522705
Epoch 180, val loss: 0.4046841561794281
Epoch 190, training loss: 0.2708883285522461 = 0.2613951861858368 + 0.001 * 9.493148803710938
Epoch 190, val loss: 0.4068935215473175
Epoch 200, training loss: 0.2628788948059082 = 0.2533792555332184 + 0.001 * 9.499628067016602
Epoch 200, val loss: 0.41009238362312317
Epoch 210, training loss: 0.2552337646484375 = 0.24572785198688507 + 0.001 * 9.505922317504883
Epoch 210, val loss: 0.4142616093158722
Epoch 220, training loss: 0.2478896975517273 = 0.2383778691291809 + 0.001 * 9.511821746826172
Epoch 220, val loss: 0.4193057715892792
Epoch 230, training loss: 0.24080286920070648 = 0.23128587007522583 + 0.001 * 9.516998291015625
Epoch 230, val loss: 0.42530012130737305
Epoch 240, training loss: 0.23404526710510254 = 0.22452381253242493 + 0.001 * 9.52144718170166
Epoch 240, val loss: 0.43301263451576233
Epoch 250, training loss: 0.2274809628725052 = 0.2179567813873291 + 0.001 * 9.524174690246582
Epoch 250, val loss: 0.4398776590824127
Epoch 260, training loss: 0.22124503552913666 = 0.2117193639278412 + 0.001 * 9.525677680969238
Epoch 260, val loss: 0.4483794867992401
Epoch 270, training loss: 0.21524393558502197 = 0.20572133362293243 + 0.001 * 9.522594451904297
Epoch 270, val loss: 0.45760905742645264
Epoch 280, training loss: 0.20949503779411316 = 0.1999792903661728 + 0.001 * 9.515753746032715
Epoch 280, val loss: 0.4680202305316925
Epoch 290, training loss: 0.20392900705337524 = 0.19442187249660492 + 0.001 * 9.50714111328125
Epoch 290, val loss: 0.4775969088077545
Epoch 300, training loss: 0.19865672290325165 = 0.18915709853172302 + 0.001 * 9.499622344970703
Epoch 300, val loss: 0.48631566762924194
Epoch 310, training loss: 0.19358296692371368 = 0.18407262861728668 + 0.001 * 9.510335922241211
Epoch 310, val loss: 0.49738451838493347
Epoch 320, training loss: 0.1887301802635193 = 0.17922504246234894 + 0.001 * 9.505143165588379
Epoch 320, val loss: 0.5101320743560791
Epoch 330, training loss: 0.18394893407821655 = 0.17446525394916534 + 0.001 * 9.483672142028809
Epoch 330, val loss: 0.5222740769386292
Epoch 340, training loss: 0.17931827902793884 = 0.16983574628829956 + 0.001 * 9.482525825500488
Epoch 340, val loss: 0.5334362387657166
Epoch 350, training loss: 0.1747356355190277 = 0.16526605188846588 + 0.001 * 9.469584465026855
Epoch 350, val loss: 0.5461149215698242
Epoch 360, training loss: 0.1702316701412201 = 0.1607615351676941 + 0.001 * 9.470129013061523
Epoch 360, val loss: 0.5597888231277466
Epoch 370, training loss: 0.16615070402622223 = 0.15669289231300354 + 0.001 * 9.457818031311035
Epoch 370, val loss: 0.5695157051086426
Epoch 380, training loss: 0.16162756085395813 = 0.15217256546020508 + 0.001 * 9.454989433288574
Epoch 380, val loss: 0.5859870314598083
Epoch 390, training loss: 0.15752622485160828 = 0.14807431399822235 + 0.001 * 9.451912879943848
Epoch 390, val loss: 0.5993017554283142
Epoch 400, training loss: 0.1533811241388321 = 0.14392779767513275 + 0.001 * 9.453329086303711
Epoch 400, val loss: 0.6127177476882935
Epoch 410, training loss: 0.14929039776325226 = 0.13983851671218872 + 0.001 * 9.451879501342773
Epoch 410, val loss: 0.6251952648162842
Epoch 420, training loss: 0.14516833424568176 = 0.13571548461914062 + 0.001 * 9.452853202819824
Epoch 420, val loss: 0.6405133008956909
Epoch 430, training loss: 0.1415948122739792 = 0.13216017186641693 + 0.001 * 9.434637069702148
Epoch 430, val loss: 0.6536726355552673
Epoch 440, training loss: 0.13736498355865479 = 0.1279110163450241 + 0.001 * 9.45396614074707
Epoch 440, val loss: 0.6705960631370544
Epoch 450, training loss: 0.1333853155374527 = 0.12395806610584259 + 0.001 * 9.42724609375
Epoch 450, val loss: 0.6868776679039001
Epoch 460, training loss: 0.12946031987667084 = 0.12001911550760269 + 0.001 * 9.441197395324707
Epoch 460, val loss: 0.7039014101028442
Epoch 470, training loss: 0.12559765577316284 = 0.11616570502519608 + 0.001 * 9.431946754455566
Epoch 470, val loss: 0.7224285006523132
Epoch 480, training loss: 0.12204714119434357 = 0.11263210326433182 + 0.001 * 9.415040969848633
Epoch 480, val loss: 0.7433533072471619
Epoch 490, training loss: 0.1180916428565979 = 0.10867726802825928 + 0.001 * 9.4143705368042
Epoch 490, val loss: 0.7557960748672485
Epoch 500, training loss: 0.11433225125074387 = 0.10492531955242157 + 0.001 * 9.406933784484863
Epoch 500, val loss: 0.7794835567474365
Epoch 510, training loss: 0.11051066964864731 = 0.10109017789363861 + 0.001 * 9.420490264892578
Epoch 510, val loss: 0.7932305335998535
Epoch 520, training loss: 0.10677999258041382 = 0.09736068546772003 + 0.001 * 9.419303894042969
Epoch 520, val loss: 0.81935715675354
Epoch 530, training loss: 0.1032375693321228 = 0.09383601695299149 + 0.001 * 9.4015531539917
Epoch 530, val loss: 0.8332844972610474
Epoch 540, training loss: 0.09962914884090424 = 0.09022451937198639 + 0.001 * 9.404631614685059
Epoch 540, val loss: 0.8612927198410034
Epoch 550, training loss: 0.09592476487159729 = 0.0865337923169136 + 0.001 * 9.390971183776855
Epoch 550, val loss: 0.8770433664321899
Epoch 560, training loss: 0.09247792512178421 = 0.0830274224281311 + 0.001 * 9.45050048828125
Epoch 560, val loss: 0.9016509652137756
Epoch 570, training loss: 0.08900455385446548 = 0.07960174232721329 + 0.001 * 9.402811050415039
Epoch 570, val loss: 0.9232779145240784
Epoch 580, training loss: 0.08574499189853668 = 0.07633543759584427 + 0.001 * 9.409550666809082
Epoch 580, val loss: 0.9451502561569214
Epoch 590, training loss: 0.08266468346118927 = 0.07326921820640564 + 0.001 * 9.395464897155762
Epoch 590, val loss: 0.9757367968559265
Epoch 600, training loss: 0.07933139055967331 = 0.06994028389453888 + 0.001 * 9.391104698181152
Epoch 600, val loss: 0.9901052117347717
Epoch 610, training loss: 0.07613105326890945 = 0.06673943996429443 + 0.001 * 9.391615867614746
Epoch 610, val loss: 1.0211127996444702
Epoch 620, training loss: 0.07303951680660248 = 0.06366144865751266 + 0.001 * 9.378069877624512
Epoch 620, val loss: 1.0423663854599
Epoch 630, training loss: 0.07031068205833435 = 0.06091465428471565 + 0.001 * 9.396027565002441
Epoch 630, val loss: 1.0629335641860962
Epoch 640, training loss: 0.06736688315868378 = 0.0579989068210125 + 0.001 * 9.367975234985352
Epoch 640, val loss: 1.096448302268982
Epoch 650, training loss: 0.06452625244855881 = 0.05511636659502983 + 0.001 * 9.409886360168457
Epoch 650, val loss: 1.1142127513885498
Epoch 660, training loss: 0.06180569529533386 = 0.05240339785814285 + 0.001 * 9.40229606628418
Epoch 660, val loss: 1.142954707145691
Epoch 670, training loss: 0.05934508144855499 = 0.04996795207262039 + 0.001 * 9.377130508422852
Epoch 670, val loss: 1.171561360359192
Epoch 680, training loss: 0.05691007897257805 = 0.04752495139837265 + 0.001 * 9.385127067565918
Epoch 680, val loss: 1.1896945238113403
Epoch 690, training loss: 0.05444476008415222 = 0.045063260942697525 + 0.001 * 9.381498336791992
Epoch 690, val loss: 1.2223049402236938
Epoch 700, training loss: 0.05218014121055603 = 0.042780883610248566 + 0.001 * 9.399258613586426
Epoch 700, val loss: 1.2448878288269043
Epoch 710, training loss: 0.050164759159088135 = 0.040754806250333786 + 0.001 * 9.409951210021973
Epoch 710, val loss: 1.2666683197021484
Epoch 720, training loss: 0.048022545874118805 = 0.038655780255794525 + 0.001 * 9.366765975952148
Epoch 720, val loss: 1.2989705801010132
Epoch 730, training loss: 0.04599102959036827 = 0.036619897931814194 + 0.001 * 9.371131896972656
Epoch 730, val loss: 1.3188927173614502
Epoch 740, training loss: 0.04410455375909805 = 0.034752052277326584 + 0.001 * 9.352499008178711
Epoch 740, val loss: 1.3452130556106567
Epoch 750, training loss: 0.04234282672405243 = 0.03299523890018463 + 0.001 * 9.347585678100586
Epoch 750, val loss: 1.372051477432251
Epoch 760, training loss: 0.0406542606651783 = 0.03128552809357643 + 0.001 * 9.368732452392578
Epoch 760, val loss: 1.3923907279968262
Epoch 770, training loss: 0.039000704884529114 = 0.029654528945684433 + 0.001 * 9.346177101135254
Epoch 770, val loss: 1.4204328060150146
Epoch 780, training loss: 0.037499748170375824 = 0.02812274359166622 + 0.001 * 9.37700366973877
Epoch 780, val loss: 1.442025065422058
Epoch 790, training loss: 0.03603111207485199 = 0.02667105384171009 + 0.001 * 9.36005687713623
Epoch 790, val loss: 1.4668989181518555
Epoch 800, training loss: 0.03468400612473488 = 0.025317665189504623 + 0.001 * 9.366340637207031
Epoch 800, val loss: 1.4921135902404785
Epoch 810, training loss: 0.03334064409136772 = 0.024010008201003075 + 0.001 * 9.330635070800781
Epoch 810, val loss: 1.5127452611923218
Epoch 820, training loss: 0.0321313850581646 = 0.022767258808016777 + 0.001 * 9.364126205444336
Epoch 820, val loss: 1.5393325090408325
Epoch 830, training loss: 0.030901946127414703 = 0.02157272771000862 + 0.001 * 9.329216957092285
Epoch 830, val loss: 1.5593360662460327
Epoch 840, training loss: 0.029792945832014084 = 0.020451176911592484 + 0.001 * 9.341768264770508
Epoch 840, val loss: 1.5841031074523926
Epoch 850, training loss: 0.0287475623190403 = 0.0193951353430748 + 0.001 * 9.35242748260498
Epoch 850, val loss: 1.60611093044281
Epoch 860, training loss: 0.027737822383642197 = 0.018397236242890358 + 0.001 * 9.340584754943848
Epoch 860, val loss: 1.6279886960983276
Epoch 870, training loss: 0.026831677183508873 = 0.017452722415328026 + 0.001 * 9.37895393371582
Epoch 870, val loss: 1.6518635749816895
Epoch 880, training loss: 0.025867069140076637 = 0.016546867787837982 + 0.001 * 9.32020092010498
Epoch 880, val loss: 1.6711316108703613
Epoch 890, training loss: 0.025012727826833725 = 0.015685811638832092 + 0.001 * 9.32691478729248
Epoch 890, val loss: 1.694935917854309
Epoch 900, training loss: 0.024198384955525398 = 0.014864581637084484 + 0.001 * 9.333803176879883
Epoch 900, val loss: 1.7144955396652222
Epoch 910, training loss: 0.023440523073077202 = 0.014082083478569984 + 0.001 * 9.358439445495605
Epoch 910, val loss: 1.736498475074768
Epoch 920, training loss: 0.02266436442732811 = 0.013343250378966331 + 0.001 * 9.321114540100098
Epoch 920, val loss: 1.7562135457992554
Epoch 930, training loss: 0.02198069915175438 = 0.012639191932976246 + 0.001 * 9.341507911682129
Epoch 930, val loss: 1.7776906490325928
Epoch 940, training loss: 0.021294083446264267 = 0.011973836459219456 + 0.001 * 9.320245742797852
Epoch 940, val loss: 1.7967233657836914
Epoch 950, training loss: 0.02068811096251011 = 0.01134210079908371 + 0.001 * 9.346009254455566
Epoch 950, val loss: 1.8175522089004517
Epoch 960, training loss: 0.020061645656824112 = 0.010749461129307747 + 0.001 * 9.31218433380127
Epoch 960, val loss: 1.8364652395248413
Epoch 970, training loss: 0.019510339945554733 = 0.010191383771598339 + 0.001 * 9.31895637512207
Epoch 970, val loss: 1.8564481735229492
Epoch 980, training loss: 0.018990259617567062 = 0.009669724851846695 + 0.001 * 9.320533752441406
Epoch 980, val loss: 1.874554991722107
Epoch 990, training loss: 0.0185216274112463 = 0.00918080098927021 + 0.001 * 9.340826034545898
Epoch 990, val loss: 1.8936936855316162
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.6734
Flip ASR: 0.5933/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1072638034820557 = 1.0969047546386719 + 0.001 * 10.35904598236084
Epoch 0, val loss: 1.0953800678253174
Epoch 10, training loss: 1.096147894859314 = 1.0857889652252197 + 0.001 * 10.358915328979492
Epoch 10, val loss: 1.0842500925064087
Epoch 20, training loss: 1.0794172286987305 = 1.069058895111084 + 0.001 * 10.358309745788574
Epoch 20, val loss: 1.0671101808547974
Epoch 30, training loss: 1.0513578653335571 = 1.041001796722412 + 0.001 * 10.356094360351562
Epoch 30, val loss: 1.0387611389160156
Epoch 40, training loss: 1.0101906061172485 = 0.9998456239700317 + 0.001 * 10.344993591308594
Epoch 40, val loss: 0.997755229473114
Epoch 50, training loss: 0.9461899995803833 = 0.9359595775604248 + 0.001 * 10.23042106628418
Epoch 50, val loss: 0.9324133992195129
Epoch 60, training loss: 0.8458298444747925 = 0.8361509442329407 + 0.001 * 9.678893089294434
Epoch 60, val loss: 0.8328017592430115
Epoch 70, training loss: 0.7195421457290649 = 0.7099558711051941 + 0.001 * 9.586257934570312
Epoch 70, val loss: 0.7108736038208008
Epoch 80, training loss: 0.59253990650177 = 0.5830308794975281 + 0.001 * 9.509037971496582
Epoch 80, val loss: 0.5945706963539124
Epoch 90, training loss: 0.4905296266078949 = 0.4810384511947632 + 0.001 * 9.491166114807129
Epoch 90, val loss: 0.5075749158859253
Epoch 100, training loss: 0.42382264137268066 = 0.4143444299697876 + 0.001 * 9.478214263916016
Epoch 100, val loss: 0.4566402733325958
Epoch 110, training loss: 0.3855237364768982 = 0.3760600686073303 + 0.001 * 9.463677406311035
Epoch 110, val loss: 0.43323200941085815
Epoch 120, training loss: 0.3613271117210388 = 0.35187360644340515 + 0.001 * 9.453519821166992
Epoch 120, val loss: 0.4222552180290222
Epoch 130, training loss: 0.3429830074310303 = 0.3335396647453308 + 0.001 * 9.443344116210938
Epoch 130, val loss: 0.4149499535560608
Epoch 140, training loss: 0.3279998302459717 = 0.31855908036231995 + 0.001 * 9.4407377243042
Epoch 140, val loss: 0.4093407690525055
Epoch 150, training loss: 0.31526216864585876 = 0.3058141767978668 + 0.001 * 9.447978019714355
Epoch 150, val loss: 0.4055494964122772
Epoch 160, training loss: 0.3040274381637573 = 0.2945731282234192 + 0.001 * 9.454317092895508
Epoch 160, val loss: 0.4034874439239502
Epoch 170, training loss: 0.29384541511535645 = 0.2843845784664154 + 0.001 * 9.460840225219727
Epoch 170, val loss: 0.4027140140533447
Epoch 180, training loss: 0.2844410240650177 = 0.27497321367263794 + 0.001 * 9.46780776977539
Epoch 180, val loss: 0.40303829312324524
Epoch 190, training loss: 0.27561089396476746 = 0.2661360502243042 + 0.001 * 9.474834442138672
Epoch 190, val loss: 0.4043985605239868
Epoch 200, training loss: 0.26722994446754456 = 0.2577480673789978 + 0.001 * 9.4818754196167
Epoch 200, val loss: 0.4067508578300476
Epoch 210, training loss: 0.259211927652359 = 0.2497233748435974 + 0.001 * 9.488551139831543
Epoch 210, val loss: 0.4100198745727539
Epoch 220, training loss: 0.25150585174560547 = 0.24201129376888275 + 0.001 * 9.49454402923584
Epoch 220, val loss: 0.4141617715358734
Epoch 230, training loss: 0.2440820038318634 = 0.23458266258239746 + 0.001 * 9.499344825744629
Epoch 230, val loss: 0.4191146194934845
Epoch 240, training loss: 0.23691509664058685 = 0.22741299867630005 + 0.001 * 9.502096176147461
Epoch 240, val loss: 0.42497557401657104
Epoch 250, training loss: 0.23000605404376984 = 0.22050374746322632 + 0.001 * 9.502310752868652
Epoch 250, val loss: 0.43181654810905457
Epoch 260, training loss: 0.22336122393608093 = 0.2138567566871643 + 0.001 * 9.504459381103516
Epoch 260, val loss: 0.4393201172351837
Epoch 270, training loss: 0.21698693931102753 = 0.20748527348041534 + 0.001 * 9.501664161682129
Epoch 270, val loss: 0.44730374217033386
Epoch 280, training loss: 0.21082429587841034 = 0.20133419334888458 + 0.001 * 9.490097045898438
Epoch 280, val loss: 0.457710325717926
Epoch 290, training loss: 0.204904705286026 = 0.19541092216968536 + 0.001 * 9.493789672851562
Epoch 290, val loss: 0.4676346778869629
Epoch 300, training loss: 0.1992158740758896 = 0.18972909450531006 + 0.001 * 9.486774444580078
Epoch 300, val loss: 0.47871163487434387
Epoch 310, training loss: 0.19369244575500488 = 0.1842036098241806 + 0.001 * 9.488838195800781
Epoch 310, val loss: 0.4878253936767578
Epoch 320, training loss: 0.18830768764019012 = 0.17882932722568512 + 0.001 * 9.478354454040527
Epoch 320, val loss: 0.5017048716545105
Epoch 330, training loss: 0.1830138862133026 = 0.17355598509311676 + 0.001 * 9.457901954650879
Epoch 330, val loss: 0.5121631622314453
Epoch 340, training loss: 0.1778438836336136 = 0.16838368773460388 + 0.001 * 9.460195541381836
Epoch 340, val loss: 0.5261725187301636
Epoch 350, training loss: 0.17279241979122162 = 0.16333656013011932 + 0.001 * 9.455865859985352
Epoch 350, val loss: 0.5380813479423523
Epoch 360, training loss: 0.16777873039245605 = 0.15834484994411469 + 0.001 * 9.43388557434082
Epoch 360, val loss: 0.5522018671035767
Epoch 370, training loss: 0.16287533938884735 = 0.15340431034564972 + 0.001 * 9.471027374267578
Epoch 370, val loss: 0.5678941607475281
Epoch 380, training loss: 0.15837515890598297 = 0.14894865453243256 + 0.001 * 9.426506042480469
Epoch 380, val loss: 0.5798462629318237
Epoch 390, training loss: 0.15348860621452332 = 0.14406512677669525 + 0.001 * 9.42347526550293
Epoch 390, val loss: 0.5984916687011719
Epoch 400, training loss: 0.14890700578689575 = 0.13947482407093048 + 0.001 * 9.432178497314453
Epoch 400, val loss: 0.6114842295646667
Epoch 410, training loss: 0.144344761967659 = 0.13493143022060394 + 0.001 * 9.413336753845215
Epoch 410, val loss: 0.6268996000289917
Epoch 420, training loss: 0.13982217013835907 = 0.13041353225708008 + 0.001 * 9.4086332321167
Epoch 420, val loss: 0.6440250277519226
Epoch 430, training loss: 0.13533346354961395 = 0.12590159475803375 + 0.001 * 9.431861877441406
Epoch 430, val loss: 0.6588131785392761
Epoch 440, training loss: 0.13080143928527832 = 0.12138574570417404 + 0.001 * 9.415690422058105
Epoch 440, val loss: 0.6768887639045715
Epoch 450, training loss: 0.12663252651691437 = 0.11723031103610992 + 0.001 * 9.402216911315918
Epoch 450, val loss: 0.6997381448745728
Epoch 460, training loss: 0.12198104709386826 = 0.11259069293737411 + 0.001 * 9.390353202819824
Epoch 460, val loss: 0.7133623957633972
Epoch 470, training loss: 0.11768864840269089 = 0.1082906723022461 + 0.001 * 9.397974967956543
Epoch 470, val loss: 0.7304816246032715
Epoch 480, training loss: 0.11343828588724136 = 0.10404330492019653 + 0.001 * 9.394977569580078
Epoch 480, val loss: 0.7544760704040527
Epoch 490, training loss: 0.10915307700634003 = 0.0997343435883522 + 0.001 * 9.418732643127441
Epoch 490, val loss: 0.771375298500061
Epoch 500, training loss: 0.1050439104437828 = 0.09564917534589767 + 0.001 * 9.394736289978027
Epoch 500, val loss: 0.7925065159797668
Epoch 510, training loss: 0.100867860019207 = 0.09148326516151428 + 0.001 * 9.384592056274414
Epoch 510, val loss: 0.8127890825271606
Epoch 520, training loss: 0.09685487300157547 = 0.08746103197336197 + 0.001 * 9.393843650817871
Epoch 520, val loss: 0.8386149406433105
Epoch 530, training loss: 0.09285557270050049 = 0.08347240090370178 + 0.001 * 9.383170127868652
Epoch 530, val loss: 0.8591011166572571
Epoch 540, training loss: 0.0892125740647316 = 0.079774871468544 + 0.001 * 9.437700271606445
Epoch 540, val loss: 0.8798841834068298
Epoch 550, training loss: 0.08532197028398514 = 0.07591889798641205 + 0.001 * 9.403074264526367
Epoch 550, val loss: 0.909041702747345
Epoch 560, training loss: 0.08159840852022171 = 0.07221918553113937 + 0.001 * 9.379225730895996
Epoch 560, val loss: 0.9273003935813904
Epoch 570, training loss: 0.07789941877126694 = 0.0685112252831459 + 0.001 * 9.388190269470215
Epoch 570, val loss: 0.9578646421432495
Epoch 580, training loss: 0.07432414591312408 = 0.06493642926216125 + 0.001 * 9.387714385986328
Epoch 580, val loss: 0.9784397482872009
Epoch 590, training loss: 0.07096578180789948 = 0.06157772243022919 + 0.001 * 9.388057708740234
Epoch 590, val loss: 1.0050917863845825
Epoch 600, training loss: 0.06782630831003189 = 0.058432020246982574 + 0.001 * 9.394288063049316
Epoch 600, val loss: 1.0327380895614624
Epoch 610, training loss: 0.06474455446004868 = 0.055366311222314835 + 0.001 * 9.378243446350098
Epoch 610, val loss: 1.0529440641403198
Epoch 620, training loss: 0.06168906390666962 = 0.052291546016931534 + 0.001 * 9.397516250610352
Epoch 620, val loss: 1.0851985216140747
Epoch 630, training loss: 0.05875878408551216 = 0.049384523183107376 + 0.001 * 9.374260902404785
Epoch 630, val loss: 1.1083227396011353
Epoch 640, training loss: 0.0561399832367897 = 0.046728409826755524 + 0.001 * 9.411574363708496
Epoch 640, val loss: 1.1330431699752808
Epoch 650, training loss: 0.05352265387773514 = 0.04413742199540138 + 0.001 * 9.385232925415039
Epoch 650, val loss: 1.1647229194641113
Epoch 660, training loss: 0.05095280706882477 = 0.04155554249882698 + 0.001 * 9.397263526916504
Epoch 660, val loss: 1.186073660850525
Epoch 670, training loss: 0.04854079335927963 = 0.03915955498814583 + 0.001 * 9.381237030029297
Epoch 670, val loss: 1.214394450187683
Epoch 680, training loss: 0.046305734664201736 = 0.03693870082497597 + 0.001 * 9.367033004760742
Epoch 680, val loss: 1.2424412965774536
Epoch 690, training loss: 0.044190410524606705 = 0.034815430641174316 + 0.001 * 9.374979972839355
Epoch 690, val loss: 1.2643104791641235
Epoch 700, training loss: 0.042163655161857605 = 0.032734621316194534 + 0.001 * 9.429031372070312
Epoch 700, val loss: 1.2943278551101685
Epoch 710, training loss: 0.040212444961071014 = 0.030828755348920822 + 0.001 * 9.383687973022461
Epoch 710, val loss: 1.3197637796401978
Epoch 720, training loss: 0.03842582181096077 = 0.029060276225209236 + 0.001 * 9.365544319152832
Epoch 720, val loss: 1.3419241905212402
Epoch 730, training loss: 0.036689162254333496 = 0.02733783610165119 + 0.001 * 9.351324081420898
Epoch 730, val loss: 1.371341347694397
Epoch 740, training loss: 0.035080891102552414 = 0.02573501504957676 + 0.001 * 9.345874786376953
Epoch 740, val loss: 1.3928775787353516
Epoch 750, training loss: 0.033582963049411774 = 0.024223215878009796 + 0.001 * 9.359748840332031
Epoch 750, val loss: 1.419753074645996
Epoch 760, training loss: 0.03217285871505737 = 0.022816866636276245 + 0.001 * 9.35599136352539
Epoch 760, val loss: 1.443483829498291
Epoch 770, training loss: 0.03085910528898239 = 0.02150643989443779 + 0.001 * 9.352664947509766
Epoch 770, val loss: 1.4661803245544434
Epoch 780, training loss: 0.02960864081978798 = 0.020257312804460526 + 0.001 * 9.351327896118164
Epoch 780, val loss: 1.4915772676467896
Epoch 790, training loss: 0.028434153646230698 = 0.019086496904492378 + 0.001 * 9.34765625
Epoch 790, val loss: 1.5126771926879883
Epoch 800, training loss: 0.027380269020795822 = 0.01798078790307045 + 0.001 * 9.399480819702148
Epoch 800, val loss: 1.5372055768966675
Epoch 810, training loss: 0.02629781700670719 = 0.016934772953391075 + 0.001 * 9.363043785095215
Epoch 810, val loss: 1.559446096420288
Epoch 820, training loss: 0.025322403758764267 = 0.01595527119934559 + 0.001 * 9.367132186889648
Epoch 820, val loss: 1.581006646156311
Epoch 830, training loss: 0.02439296990633011 = 0.015028477646410465 + 0.001 * 9.36449146270752
Epoch 830, val loss: 1.6034563779830933
Epoch 840, training loss: 0.023493753746151924 = 0.014156978577375412 + 0.001 * 9.336774826049805
Epoch 840, val loss: 1.6256948709487915
Epoch 850, training loss: 0.022671503946185112 = 0.01333443634212017 + 0.001 * 9.337067604064941
Epoch 850, val loss: 1.646468997001648
Epoch 860, training loss: 0.021915530785918236 = 0.012562048621475697 + 0.001 * 9.35348129272461
Epoch 860, val loss: 1.6667611598968506
Epoch 870, training loss: 0.02114962786436081 = 0.01182393729686737 + 0.001 * 9.325690269470215
Epoch 870, val loss: 1.6873828172683716
Epoch 880, training loss: 0.020455453544855118 = 0.011127855628728867 + 0.001 * 9.327596664428711
Epoch 880, val loss: 1.7078365087509155
Epoch 890, training loss: 0.019800253212451935 = 0.010472379624843597 + 0.001 * 9.327872276306152
Epoch 890, val loss: 1.7278587818145752
Epoch 900, training loss: 0.019166307523846626 = 0.009861579164862633 + 0.001 * 9.304727554321289
Epoch 900, val loss: 1.7466994524002075
Epoch 910, training loss: 0.01861029490828514 = 0.009298413060605526 + 0.001 * 9.311880111694336
Epoch 910, val loss: 1.7652469873428345
Epoch 920, training loss: 0.018094640225172043 = 0.008778839372098446 + 0.001 * 9.315801620483398
Epoch 920, val loss: 1.7840455770492554
Epoch 930, training loss: 0.017623860388994217 = 0.00829888042062521 + 0.001 * 9.324978828430176
Epoch 930, val loss: 1.8016234636306763
Epoch 940, training loss: 0.01714908331632614 = 0.007855204865336418 + 0.001 * 9.293877601623535
Epoch 940, val loss: 1.81919264793396
Epoch 950, training loss: 0.016770213842391968 = 0.007445124443620443 + 0.001 * 9.325088500976562
Epoch 950, val loss: 1.836443305015564
Epoch 960, training loss: 0.01636045053601265 = 0.007064794655889273 + 0.001 * 9.295656204223633
Epoch 960, val loss: 1.8531999588012695
Epoch 970, training loss: 0.01601041853427887 = 0.006711961235851049 + 0.001 * 9.298457145690918
Epoch 970, val loss: 1.8694812059402466
Epoch 980, training loss: 0.015671920031309128 = 0.006384186912328005 + 0.001 * 9.287732124328613
Epoch 980, val loss: 1.8854666948318481
Epoch 990, training loss: 0.015408642590045929 = 0.00607768539339304 + 0.001 * 9.33095645904541
Epoch 990, val loss: 1.9012271165847778
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8402
Overall ASR: 0.6739
Flip ASR: 0.5952/1554 nodes
The final ASR:0.67241, 0.00180, Accuracy:0.83984, 0.00086
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97572])
remove edge: torch.Size([2, 79842])
updated graph: torch.Size([2, 88766])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8508
Overall ASR: 0.7622
Flip ASR: 0.7040/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72904, 0.02561, Accuracy:0.85033, 0.00041
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.1179530620574951 = 1.1075938940048218 + 0.001 * 10.359153747558594
Epoch 0, val loss: 1.1064729690551758
Epoch 10, training loss: 1.1047409772872925 = 1.0943819284439087 + 0.001 * 10.359061241149902
Epoch 10, val loss: 1.0930660963058472
Epoch 20, training loss: 1.0845139026641846 = 1.0741552114486694 + 0.001 * 10.358670234680176
Epoch 20, val loss: 1.0722466707229614
Epoch 30, training loss: 1.0520251989364624 = 1.0416679382324219 + 0.001 * 10.35725212097168
Epoch 30, val loss: 1.0393704175949097
Epoch 40, training loss: 1.0090832710266113 = 0.9987321496009827 + 0.001 * 10.35117244720459
Epoch 40, val loss: 0.9966646432876587
Epoch 50, training loss: 0.9470062851905823 = 0.9366894960403442 + 0.001 * 10.3167724609375
Epoch 50, val loss: 0.932819128036499
Epoch 60, training loss: 0.8510889410972595 = 0.8411600589752197 + 0.001 * 9.928868293762207
Epoch 60, val loss: 0.837664008140564
Epoch 70, training loss: 0.7307434678077698 = 0.7210533022880554 + 0.001 * 9.690162658691406
Epoch 70, val loss: 0.7209725975990295
Epoch 80, training loss: 0.6065059900283813 = 0.5968936085700989 + 0.001 * 9.612408638000488
Epoch 80, val loss: 0.6059709191322327
Epoch 90, training loss: 0.5000424981117249 = 0.49044620990753174 + 0.001 * 9.596284866333008
Epoch 90, val loss: 0.5143380165100098
Epoch 100, training loss: 0.4276738464832306 = 0.41809117794036865 + 0.001 * 9.582659721374512
Epoch 100, val loss: 0.4583146870136261
Epoch 110, training loss: 0.3861594498157501 = 0.3765872120857239 + 0.001 * 9.57224178314209
Epoch 110, val loss: 0.4320494532585144
Epoch 120, training loss: 0.36086753010749817 = 0.35131025314331055 + 0.001 * 9.557271957397461
Epoch 120, val loss: 0.42053744196891785
Epoch 130, training loss: 0.34227824211120605 = 0.3327365815639496 + 0.001 * 9.541661262512207
Epoch 130, val loss: 0.41363590955734253
Epoch 140, training loss: 0.3271936774253845 = 0.31766098737716675 + 0.001 * 9.532687187194824
Epoch 140, val loss: 0.40838930010795593
Epoch 150, training loss: 0.3143678605556488 = 0.3048342168331146 + 0.001 * 9.533635139465332
Epoch 150, val loss: 0.4047619104385376
Epoch 160, training loss: 0.3030684292316437 = 0.29353418946266174 + 0.001 * 9.534238815307617
Epoch 160, val loss: 0.40281471610069275
Epoch 170, training loss: 0.2928420305252075 = 0.2833080291748047 + 0.001 * 9.534005165100098
Epoch 170, val loss: 0.40219414234161377
Epoch 180, training loss: 0.2834193706512451 = 0.27388596534729004 + 0.001 * 9.533417701721191
Epoch 180, val loss: 0.4026145935058594
Epoch 190, training loss: 0.27459049224853516 = 0.26505881547927856 + 0.001 * 9.531691551208496
Epoch 190, val loss: 0.40405184030532837
Epoch 200, training loss: 0.26621684432029724 = 0.25668805837631226 + 0.001 * 9.528776168823242
Epoch 200, val loss: 0.4065479040145874
Epoch 210, training loss: 0.2582226097583771 = 0.24869345128536224 + 0.001 * 9.529160499572754
Epoch 210, val loss: 0.40996551513671875
Epoch 220, training loss: 0.25055697560310364 = 0.2410224825143814 + 0.001 * 9.534505844116211
Epoch 220, val loss: 0.41421931982040405
Epoch 230, training loss: 0.24318033456802368 = 0.23364587128162384 + 0.001 * 9.534469604492188
Epoch 230, val loss: 0.41932135820388794
Epoch 240, training loss: 0.2360866814851761 = 0.22654974460601807 + 0.001 * 9.536933898925781
Epoch 240, val loss: 0.42518043518066406
Epoch 250, training loss: 0.22928540408611298 = 0.2197408527135849 + 0.001 * 9.544548988342285
Epoch 250, val loss: 0.43193531036376953
Epoch 260, training loss: 0.22274331748485565 = 0.21319955587387085 + 0.001 * 9.543766975402832
Epoch 260, val loss: 0.4395536184310913
Epoch 270, training loss: 0.21646754443645477 = 0.20691600441932678 + 0.001 * 9.551533699035645
Epoch 270, val loss: 0.4478902518749237
Epoch 280, training loss: 0.21044382452964783 = 0.2008902132511139 + 0.001 * 9.553605079650879
Epoch 280, val loss: 0.45793604850769043
Epoch 290, training loss: 0.20466366410255432 = 0.19511684775352478 + 0.001 * 9.546808242797852
Epoch 290, val loss: 0.467100590467453
Epoch 300, training loss: 0.19911783933639526 = 0.1895630806684494 + 0.001 * 9.554757118225098
Epoch 300, val loss: 0.476559579372406
Epoch 310, training loss: 0.1937040388584137 = 0.1841578483581543 + 0.001 * 9.546197891235352
Epoch 310, val loss: 0.48819223046302795
Epoch 320, training loss: 0.18850870430469513 = 0.17894268035888672 + 0.001 * 9.56602954864502
Epoch 320, val loss: 0.49796390533447266
Epoch 330, training loss: 0.18343901634216309 = 0.1738913357257843 + 0.001 * 9.547673225402832
Epoch 330, val loss: 0.5099225640296936
Epoch 340, training loss: 0.17852461338043213 = 0.16895292699337006 + 0.001 * 9.57168960571289
Epoch 340, val loss: 0.5228514075279236
Epoch 350, training loss: 0.17356403172016144 = 0.16405336558818817 + 0.001 * 9.510659217834473
Epoch 350, val loss: 0.5350990295410156
Epoch 360, training loss: 0.1689235121011734 = 0.15937234461307526 + 0.001 * 9.551165580749512
Epoch 360, val loss: 0.550541341304779
Epoch 370, training loss: 0.16400155425071716 = 0.15448597073554993 + 0.001 * 9.515583992004395
Epoch 370, val loss: 0.5614407658576965
Epoch 380, training loss: 0.15936324000358582 = 0.14980830252170563 + 0.001 * 9.55493450164795
Epoch 380, val loss: 0.5740712285041809
Epoch 390, training loss: 0.15467600524425507 = 0.14516136050224304 + 0.001 * 9.514647483825684
Epoch 390, val loss: 0.5905834436416626
Epoch 400, training loss: 0.15016119182109833 = 0.1406114548444748 + 0.001 * 9.549734115600586
Epoch 400, val loss: 0.605072557926178
Epoch 410, training loss: 0.1456342190504074 = 0.1361027956008911 + 0.001 * 9.531425476074219
Epoch 410, val loss: 0.6192190051078796
Epoch 420, training loss: 0.14115725457668304 = 0.13169153034687042 + 0.001 * 9.465727806091309
Epoch 420, val loss: 0.6362393498420715
Epoch 430, training loss: 0.1369057595729828 = 0.12744133174419403 + 0.001 * 9.464431762695312
Epoch 430, val loss: 0.6488699913024902
Epoch 440, training loss: 0.13257786631584167 = 0.1231207475066185 + 0.001 * 9.457119941711426
Epoch 440, val loss: 0.6708438396453857
Epoch 450, training loss: 0.1282975673675537 = 0.11884143948554993 + 0.001 * 9.456122398376465
Epoch 450, val loss: 0.6828963160514832
Epoch 460, training loss: 0.12404421716928482 = 0.11459077894687653 + 0.001 * 9.453437805175781
Epoch 460, val loss: 0.7049788236618042
Epoch 470, training loss: 0.11991187930107117 = 0.11041536927223206 + 0.001 * 9.496513366699219
Epoch 470, val loss: 0.720404326915741
Epoch 480, training loss: 0.1157979816198349 = 0.10634998232126236 + 0.001 * 9.447996139526367
Epoch 480, val loss: 0.7397803068161011
Epoch 490, training loss: 0.1118801012635231 = 0.10242914408445358 + 0.001 * 9.450959205627441
Epoch 490, val loss: 0.761451780796051
Epoch 500, training loss: 0.1079547181725502 = 0.09849783033132553 + 0.001 * 9.456887245178223
Epoch 500, val loss: 0.7762465476989746
Epoch 510, training loss: 0.10393665730953217 = 0.0944925844669342 + 0.001 * 9.444073677062988
Epoch 510, val loss: 0.8031185269355774
Epoch 520, training loss: 0.10003060102462769 = 0.09057604521512985 + 0.001 * 9.454556465148926
Epoch 520, val loss: 0.8183460831642151
Epoch 530, training loss: 0.09622503072023392 = 0.08678504079580307 + 0.001 * 9.439986228942871
Epoch 530, val loss: 0.8445603251457214
Epoch 540, training loss: 0.09247750788927078 = 0.08302661776542664 + 0.001 * 9.45089054107666
Epoch 540, val loss: 0.8630333542823792
Epoch 550, training loss: 0.08886779844760895 = 0.07941769808530807 + 0.001 * 9.450103759765625
Epoch 550, val loss: 0.8859002590179443
Epoch 560, training loss: 0.08537980914115906 = 0.07594775408506393 + 0.001 * 9.432058334350586
Epoch 560, val loss: 0.9124114513397217
Epoch 570, training loss: 0.08195221424102783 = 0.07250503450632095 + 0.001 * 9.44717788696289
Epoch 570, val loss: 0.9287998676300049
Epoch 580, training loss: 0.07843584567308426 = 0.0690029188990593 + 0.001 * 9.432926177978516
Epoch 580, val loss: 0.9608175754547119
Epoch 590, training loss: 0.07502338290214539 = 0.0655752494931221 + 0.001 * 9.448134422302246
Epoch 590, val loss: 0.9796078205108643
Epoch 600, training loss: 0.07174984365701675 = 0.062327418476343155 + 0.001 * 9.422426223754883
Epoch 600, val loss: 1.0065895318984985
Epoch 610, training loss: 0.06865434348583221 = 0.05921533331274986 + 0.001 * 9.439006805419922
Epoch 610, val loss: 1.0334155559539795
Epoch 620, training loss: 0.0657317116856575 = 0.05630210414528847 + 0.001 * 9.429607391357422
Epoch 620, val loss: 1.0538653135299683
Epoch 630, training loss: 0.06271419674158096 = 0.05327482149004936 + 0.001 * 9.439375877380371
Epoch 630, val loss: 1.0877585411071777
Epoch 640, training loss: 0.0597737655043602 = 0.050352007150650024 + 0.001 * 9.421757698059082
Epoch 640, val loss: 1.1088120937347412
Epoch 650, training loss: 0.05706458166241646 = 0.047647520899772644 + 0.001 * 9.417059898376465
Epoch 650, val loss: 1.136683464050293
Epoch 660, training loss: 0.05450289323925972 = 0.0450713224709034 + 0.001 * 9.431571006774902
Epoch 660, val loss: 1.1653271913528442
Epoch 670, training loss: 0.052085913717746735 = 0.04267588257789612 + 0.001 * 9.410028457641602
Epoch 670, val loss: 1.187463402748108
Epoch 680, training loss: 0.049797795712947845 = 0.04025272652506828 + 0.001 * 9.545066833496094
Epoch 680, val loss: 1.221234917640686
Epoch 690, training loss: 0.047398220747709274 = 0.03794519603252411 + 0.001 * 9.453024864196777
Epoch 690, val loss: 1.2436472177505493
Epoch 700, training loss: 0.04527028650045395 = 0.03583435341715813 + 0.001 * 9.435934066772461
Epoch 700, val loss: 1.2706420421600342
Epoch 710, training loss: 0.04323643445968628 = 0.03382450342178345 + 0.001 * 9.4119291305542
Epoch 710, val loss: 1.299920678138733
Epoch 720, training loss: 0.041333168745040894 = 0.031930938363075256 + 0.001 * 9.40223217010498
Epoch 720, val loss: 1.3214881420135498
Epoch 730, training loss: 0.03947844356298447 = 0.030078012496232986 + 0.001 * 9.400431632995605
Epoch 730, val loss: 1.3526437282562256
Epoch 740, training loss: 0.03774384409189224 = 0.028344301506876945 + 0.001 * 9.399543762207031
Epoch 740, val loss: 1.3752087354660034
Epoch 750, training loss: 0.036135170608758926 = 0.02673032134771347 + 0.001 * 9.404850006103516
Epoch 750, val loss: 1.4016637802124023
Epoch 760, training loss: 0.034641992300748825 = 0.025218108668923378 + 0.001 * 9.423882484436035
Epoch 760, val loss: 1.4283567667007446
Epoch 770, training loss: 0.03321859985589981 = 0.02377633936703205 + 0.001 * 9.44225788116455
Epoch 770, val loss: 1.4508566856384277
Epoch 780, training loss: 0.03182743862271309 = 0.022417720407247543 + 0.001 * 9.40971851348877
Epoch 780, val loss: 1.4780808687210083
Epoch 790, training loss: 0.030521012842655182 = 0.02113623544573784 + 0.001 * 9.384777069091797
Epoch 790, val loss: 1.5009047985076904
Epoch 800, training loss: 0.02931506559252739 = 0.01993286795914173 + 0.001 * 9.382196426391602
Epoch 800, val loss: 1.5253052711486816
Epoch 810, training loss: 0.028201626613736153 = 0.01879728212952614 + 0.001 * 9.404343605041504
Epoch 810, val loss: 1.5496011972427368
Epoch 820, training loss: 0.02710558846592903 = 0.017721068114042282 + 0.001 * 9.384519577026367
Epoch 820, val loss: 1.5724081993103027
Epoch 830, training loss: 0.026099713519215584 = 0.01670461893081665 + 0.001 * 9.39509391784668
Epoch 830, val loss: 1.59673273563385
Epoch 840, training loss: 0.025130536407232285 = 0.015739623457193375 + 0.001 * 9.390912055969238
Epoch 840, val loss: 1.6180680990219116
Epoch 850, training loss: 0.024215243756771088 = 0.01483483612537384 + 0.001 * 9.380406379699707
Epoch 850, val loss: 1.6423544883728027
Epoch 860, training loss: 0.023381056264042854 = 0.013974307104945183 + 0.001 * 9.40674877166748
Epoch 860, val loss: 1.6627345085144043
Epoch 870, training loss: 0.02262130007147789 = 0.013166994787752628 + 0.001 * 9.454303741455078
Epoch 870, val loss: 1.6857950687408447
Epoch 880, training loss: 0.021798867732286453 = 0.012408736161887646 + 0.001 * 9.390130043029785
Epoch 880, val loss: 1.7060550451278687
Epoch 890, training loss: 0.021120566874742508 = 0.01170311402529478 + 0.001 * 9.417452812194824
Epoch 890, val loss: 1.7283384799957275
Epoch 900, training loss: 0.020413782447576523 = 0.011044641956686974 + 0.001 * 9.369138717651367
Epoch 900, val loss: 1.7477431297302246
Epoch 910, training loss: 0.019812187179923058 = 0.01043161004781723 + 0.001 * 9.380577087402344
Epoch 910, val loss: 1.7689224481582642
Epoch 920, training loss: 0.019241880625486374 = 0.009863796643912792 + 0.001 * 9.378084182739258
Epoch 920, val loss: 1.7882810831069946
Epoch 930, training loss: 0.01870514452457428 = 0.009335537441074848 + 0.001 * 9.369607925415039
Epoch 930, val loss: 1.807471752166748
Epoch 940, training loss: 0.018223214894533157 = 0.008847769349813461 + 0.001 * 9.375444412231445
Epoch 940, val loss: 1.827130675315857
Epoch 950, training loss: 0.01781436800956726 = 0.008391255512833595 + 0.001 * 9.423112869262695
Epoch 950, val loss: 1.845271110534668
Epoch 960, training loss: 0.017330031841993332 = 0.007967768236994743 + 0.001 * 9.362262725830078
Epoch 960, val loss: 1.8633344173431396
Epoch 970, training loss: 0.016943994909524918 = 0.0075737484730780125 + 0.001 * 9.370245933532715
Epoch 970, val loss: 1.8808759450912476
Epoch 980, training loss: 0.016564220190048218 = 0.007206915412098169 + 0.001 * 9.357304573059082
Epoch 980, val loss: 1.8977584838867188
Epoch 990, training loss: 0.01626603864133358 = 0.006864883471280336 + 0.001 * 9.401154518127441
Epoch 990, val loss: 1.9148023128509521
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8427
Overall ASR: 0.6557
Flip ASR: 0.5746/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.114396095275879 = 1.1040369272232056 + 0.001 * 10.359113693237305
Epoch 0, val loss: 1.1031949520111084
Epoch 10, training loss: 1.1014999151229858 = 1.091140866279602 + 0.001 * 10.359006881713867
Epoch 10, val loss: 1.0900487899780273
