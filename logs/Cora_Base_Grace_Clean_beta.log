nohup: ignoring input
run_robust_acc.py:14: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0, add_edge_rate_2=0, attack='none', base_model='GCN', cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0005, cl_num_epochs=200, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, clf_weight=1, config='config.yaml', cont_batch_size=0, cont_weight=1, cuda=True, dataset='Cora', debug=True, device_id=0, drop_edge_rate_1=0.2, drop_edge_rate_2=0, drop_feat_rate_1=0.3, drop_feat_rate_2=0.2, dropout=0.5, encoder_model='Grace', hidden=128, if_smoothed=False, inv_weight=1, no_cuda=False, noisy_level=0.3, num_hidden=128, num_proj_hidden=128, prob=0.8, seed=10, select_target_ratio=0.1, tau=0.1, test_model='GCN', train_lr=0.01, weight_decay=0.0005)
beta 0.5
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596590042114258
Epoch 10, training loss: 8.57994556427002
Epoch 20, training loss: 8.460957527160645
Epoch 30, training loss: 8.388433456420898
Epoch 40, training loss: 8.20606803894043
Epoch 50, training loss: 7.814011573791504
Epoch 60, training loss: 7.466342449188232
Epoch 70, training loss: 7.124662399291992
Epoch 80, training loss: 7.170851230621338
Epoch 90, training loss: 6.818734169006348
Epoch 100, training loss: 6.61647891998291
Epoch 110, training loss: 6.637344837188721
Epoch 120, training loss: 6.45511531829834
Epoch 130, training loss: 6.504480838775635
Epoch 140, training loss: 6.215512752532959
Epoch 150, training loss: 6.192091941833496
Epoch 160, training loss: 6.1611456871032715
Epoch 170, training loss: 6.102746486663818
Epoch 180, training loss: 6.080826759338379
Epoch 190, training loss: 5.976360321044922
Epoch 200, training loss: 5.827854633331299
Epoch 210, training loss: 5.914714813232422
Epoch 220, training loss: 5.840671062469482
Epoch 230, training loss: 6.035642623901367
Epoch 240, training loss: 5.777406692504883
Epoch 250, training loss: 5.66325044631958
Epoch 260, training loss: 5.741137981414795
Epoch 270, training loss: 5.6841864585876465
Epoch 280, training loss: 5.598658084869385
Epoch 290, training loss: 5.534592628479004
Epoch 300, training loss: 5.551583766937256
Epoch 310, training loss: 5.553131103515625
Epoch 320, training loss: 5.438969612121582
Epoch 330, training loss: 5.542585849761963
Epoch 340, training loss: 5.382659435272217
Epoch 350, training loss: 5.496912479400635
Epoch 360, training loss: 5.345024108886719
Epoch 370, training loss: 5.422786712646484
Epoch 380, training loss: 5.372560977935791
Epoch 390, training loss: 5.428958415985107
Epoch 400, training loss: 5.337429523468018
Epoch 410, training loss: 5.347701549530029
Epoch 420, training loss: 5.343392848968506
Epoch 430, training loss: 5.295217990875244
Epoch 440, training loss: 5.195281982421875
Epoch 450, training loss: 5.278816223144531
Epoch 460, training loss: 5.260769844055176
Epoch 470, training loss: 5.184961318969727
Epoch 480, training loss: 5.1746344566345215
Epoch 490, training loss: 5.233500003814697
none
Accuracy: 0.742
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.5963716506958
Epoch 10, training loss: 8.571284294128418
Epoch 20, training loss: 8.447871208190918
Epoch 30, training loss: 8.281460762023926
Epoch 40, training loss: 7.747941970825195
Epoch 50, training loss: 7.8009352684021
Epoch 60, training loss: 7.1691765785217285
Epoch 70, training loss: 7.151339530944824
Epoch 80, training loss: 6.868888854980469
Epoch 90, training loss: 6.763852119445801
Epoch 100, training loss: 6.551255702972412
Epoch 110, training loss: 6.596958160400391
Epoch 120, training loss: 6.463494300842285
Epoch 130, training loss: 6.421138286590576
Epoch 140, training loss: 6.359165191650391
Epoch 150, training loss: 6.285684108734131
Epoch 160, training loss: 6.154792785644531
Epoch 170, training loss: 6.151769638061523
Epoch 180, training loss: 6.04384183883667
Epoch 190, training loss: 6.00739049911499
Epoch 200, training loss: 6.004042625427246
Epoch 210, training loss: 5.857310771942139
Epoch 220, training loss: 5.833719730377197
Epoch 230, training loss: 5.844393253326416
Epoch 240, training loss: 5.885279178619385
Epoch 250, training loss: 5.77617073059082
Epoch 260, training loss: 5.741848468780518
Epoch 270, training loss: 5.764952182769775
Epoch 280, training loss: 5.7122297286987305
Epoch 290, training loss: 5.6185784339904785
Epoch 300, training loss: 5.622708320617676
Epoch 310, training loss: 5.615054130554199
Epoch 320, training loss: 5.517184734344482
Epoch 330, training loss: 5.585809707641602
Epoch 340, training loss: 5.5673017501831055
Epoch 350, training loss: 5.49985408782959
Epoch 360, training loss: 5.506134033203125
Epoch 370, training loss: 5.57312536239624
Epoch 380, training loss: 5.461400508880615
Epoch 390, training loss: 5.4432196617126465
Epoch 400, training loss: 5.411093235015869
Epoch 410, training loss: 5.4878153800964355
Epoch 420, training loss: 5.390004634857178
Epoch 430, training loss: 5.407580375671387
Epoch 440, training loss: 5.392039775848389
Epoch 450, training loss: 5.34094762802124
Epoch 460, training loss: 5.364091873168945
Epoch 470, training loss: 5.334799766540527
Epoch 480, training loss: 5.293849468231201
Epoch 490, training loss: 5.274646282196045
none
Accuracy: 0.749
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.59648609161377
Epoch 10, training loss: 8.5730562210083
Epoch 20, training loss: 8.482414245605469
Epoch 30, training loss: 8.30551528930664
Epoch 40, training loss: 7.865212440490723
Epoch 50, training loss: 7.454425811767578
Epoch 60, training loss: 7.27922248840332
Epoch 70, training loss: 7.0219573974609375
Epoch 80, training loss: 6.866586685180664
Epoch 90, training loss: 6.766001224517822
Epoch 100, training loss: 6.5420684814453125
Epoch 110, training loss: 6.559813022613525
Epoch 120, training loss: 6.399006366729736
Epoch 130, training loss: 6.246801376342773
Epoch 140, training loss: 6.229757308959961
Epoch 150, training loss: 6.259176254272461
Epoch 160, training loss: 6.112763404846191
Epoch 170, training loss: 5.990635871887207
Epoch 180, training loss: 5.980282306671143
Epoch 190, training loss: 5.8805317878723145
Epoch 200, training loss: 5.870918273925781
Epoch 210, training loss: 5.841344356536865
Epoch 220, training loss: 5.778104782104492
Epoch 230, training loss: 5.726505279541016
Epoch 240, training loss: 5.748395919799805
Epoch 250, training loss: 5.561866760253906
Epoch 260, training loss: 5.657362461090088
Epoch 270, training loss: 5.612392425537109
Epoch 280, training loss: 5.5868072509765625
Epoch 290, training loss: 5.47133207321167
Epoch 300, training loss: 5.488766193389893
Epoch 310, training loss: 5.539803504943848
Epoch 320, training loss: 5.41906213760376
Epoch 330, training loss: 5.428523540496826
Epoch 340, training loss: 5.404443264007568
Epoch 350, training loss: 5.459979057312012
Epoch 360, training loss: 5.395775318145752
Epoch 370, training loss: 5.370628833770752
Epoch 380, training loss: 5.317294597625732
Epoch 390, training loss: 5.328614711761475
Epoch 400, training loss: 5.437726974487305
Epoch 410, training loss: 5.224395751953125
Epoch 420, training loss: 5.323474407196045
Epoch 430, training loss: 5.320260524749756
Epoch 440, training loss: 5.2514214515686035
Epoch 450, training loss: 5.181895732879639
Epoch 460, training loss: 5.235132694244385
Epoch 470, training loss: 5.174954414367676
Epoch 480, training loss: 5.186148643493652
Epoch 490, training loss: 5.155482769012451
none
Accuracy: 0.776
beta 0.6
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.59634780883789
Epoch 10, training loss: 8.573080062866211
Epoch 20, training loss: 8.43250846862793
Epoch 30, training loss: 8.167598724365234
Epoch 40, training loss: 7.5235514640808105
Epoch 50, training loss: 7.306521415710449
Epoch 60, training loss: 6.953023910522461
Epoch 70, training loss: 6.7478132247924805
Epoch 80, training loss: 6.645499229431152
Epoch 90, training loss: 6.550108909606934
Epoch 100, training loss: 6.347035884857178
Epoch 110, training loss: 6.515596866607666
Epoch 120, training loss: 6.270130634307861
Epoch 130, training loss: 6.093658447265625
Epoch 140, training loss: 6.140698432922363
Epoch 150, training loss: 5.970289707183838
Epoch 160, training loss: 5.91635799407959
Epoch 170, training loss: 5.883665084838867
Epoch 180, training loss: 5.897274017333984
Epoch 190, training loss: 5.8172197341918945
Epoch 200, training loss: 5.714946269989014
Epoch 210, training loss: 5.732644081115723
Epoch 220, training loss: 5.777001857757568
Epoch 230, training loss: 5.645519256591797
Epoch 240, training loss: 5.606316089630127
Epoch 250, training loss: 5.607206344604492
Epoch 260, training loss: 5.575056076049805
Epoch 270, training loss: 5.510590076446533
Epoch 280, training loss: 5.436585903167725
Epoch 290, training loss: 5.469221591949463
Epoch 300, training loss: 5.418797492980957
Epoch 310, training loss: 5.496243476867676
Epoch 320, training loss: 5.366225242614746
Epoch 330, training loss: 5.4371514320373535
Epoch 340, training loss: 5.432385444641113
Epoch 350, training loss: 5.257663249969482
Epoch 360, training loss: 5.210023403167725
Epoch 370, training loss: 5.324177265167236
Epoch 380, training loss: 5.207735538482666
Epoch 390, training loss: 5.338324546813965
Epoch 400, training loss: 5.309627056121826
Epoch 410, training loss: 5.273375511169434
Epoch 420, training loss: 5.245610237121582
Epoch 430, training loss: 5.225461483001709
Epoch 440, training loss: 5.216444492340088
Epoch 450, training loss: 5.2039899826049805
Epoch 460, training loss: 5.104336261749268
Epoch 470, training loss: 5.128679275512695
Epoch 480, training loss: 5.117246627807617
Epoch 490, training loss: 5.133357524871826
none
Accuracy: 0.729
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596473693847656
Epoch 10, training loss: 8.578410148620605
Epoch 20, training loss: 8.455202102661133
Epoch 30, training loss: 8.381482124328613
Epoch 40, training loss: 8.187747955322266
Epoch 50, training loss: 7.727833271026611
Epoch 60, training loss: 7.410834789276123
Epoch 70, training loss: 7.1808881759643555
Epoch 80, training loss: 6.906824111938477
Epoch 90, training loss: 6.866170406341553
Epoch 100, training loss: 6.60511589050293
Epoch 110, training loss: 6.522054672241211
Epoch 120, training loss: 6.584027290344238
Epoch 130, training loss: 6.469081878662109
Epoch 140, training loss: 6.356942176818848
Epoch 150, training loss: 6.347721576690674
Epoch 160, training loss: 6.168044567108154
Epoch 170, training loss: 6.097580909729004
Epoch 180, training loss: 6.021017551422119
Epoch 190, training loss: 5.942508697509766
Epoch 200, training loss: 5.927978992462158
Epoch 210, training loss: 5.8658976554870605
Epoch 220, training loss: 5.842704772949219
Epoch 230, training loss: 5.80378532409668
Epoch 240, training loss: 5.629369258880615
Epoch 250, training loss: 5.657975196838379
Epoch 260, training loss: 5.65883207321167
Epoch 270, training loss: 5.582525253295898
Epoch 280, training loss: 5.672642230987549
Epoch 290, training loss: 5.562511444091797
Epoch 300, training loss: 5.529324054718018
Epoch 310, training loss: 5.5213494300842285
Epoch 320, training loss: 5.480665683746338
Epoch 330, training loss: 5.489365100860596
Epoch 340, training loss: 5.3987250328063965
Epoch 350, training loss: 5.420036315917969
Epoch 360, training loss: 5.344165802001953
Epoch 370, training loss: 5.439200401306152
Epoch 380, training loss: 5.416418552398682
Epoch 390, training loss: 5.4270339012146
Epoch 400, training loss: 5.279601573944092
Epoch 410, training loss: 5.310153007507324
Epoch 420, training loss: 5.302760124206543
Epoch 430, training loss: 5.342284202575684
Epoch 440, training loss: 5.2813897132873535
Epoch 450, training loss: 5.246084690093994
Epoch 460, training loss: 5.180231094360352
Epoch 470, training loss: 5.175591468811035
Epoch 480, training loss: 5.1922712326049805
Epoch 490, training loss: 5.1415934562683105
none
Accuracy: 0.784
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596302032470703
Epoch 10, training loss: 8.571667671203613
Epoch 20, training loss: 8.459891319274902
Epoch 30, training loss: 8.269866943359375
Epoch 40, training loss: 8.035257339477539
Epoch 50, training loss: 7.48836612701416
Epoch 60, training loss: 7.0869903564453125
Epoch 70, training loss: 6.906652450561523
Epoch 80, training loss: 6.819148063659668
Epoch 90, training loss: 6.648889064788818
Epoch 100, training loss: 6.632816791534424
Epoch 110, training loss: 6.567575931549072
Epoch 120, training loss: 6.495115756988525
Epoch 130, training loss: 6.454954624176025
Epoch 140, training loss: 6.227365970611572
Epoch 150, training loss: 6.255542755126953
Epoch 160, training loss: 6.1450676918029785
Epoch 170, training loss: 6.03222131729126
Epoch 180, training loss: 6.081316947937012
Epoch 190, training loss: 5.988847255706787
Epoch 200, training loss: 5.847876071929932
Epoch 210, training loss: 5.83588171005249
Epoch 220, training loss: 5.773212909698486
Epoch 230, training loss: 5.814366340637207
Epoch 240, training loss: 5.755658149719238
Epoch 250, training loss: 5.738720417022705
Epoch 260, training loss: 5.797857284545898
Epoch 270, training loss: 5.6105828285217285
Epoch 280, training loss: 5.59620475769043
Epoch 290, training loss: 5.548773765563965
Epoch 300, training loss: 5.575068950653076
Epoch 310, training loss: 5.584974765777588
Epoch 320, training loss: 5.502992630004883
Epoch 330, training loss: 5.467910289764404
Epoch 340, training loss: 5.537961959838867
Epoch 350, training loss: 5.45132303237915
Epoch 360, training loss: 5.370317459106445
Epoch 370, training loss: 5.3424177169799805
Epoch 380, training loss: 5.344064712524414
Epoch 390, training loss: 5.298985004425049
Epoch 400, training loss: 5.375967025756836
Epoch 410, training loss: 5.352633953094482
Epoch 420, training loss: 5.236692428588867
Epoch 430, training loss: 5.315898418426514
Epoch 440, training loss: 5.17723274230957
Epoch 450, training loss: 5.286897659301758
Epoch 460, training loss: 5.214284896850586
Epoch 470, training loss: 5.250966548919678
Epoch 480, training loss: 5.179625511169434
Epoch 490, training loss: 5.1830549240112305
none
Accuracy: 0.754
beta 0.7
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.59661865234375
Epoch 10, training loss: 8.577083587646484
Epoch 20, training loss: 8.477276802062988
Epoch 30, training loss: 8.366113662719727
Epoch 40, training loss: 8.18541145324707
Epoch 50, training loss: 7.91306209564209
Epoch 60, training loss: 7.495251178741455
Epoch 70, training loss: 7.085109233856201
Epoch 80, training loss: 6.845300674438477
Epoch 90, training loss: 6.748281955718994
Epoch 100, training loss: 6.701729774475098
Epoch 110, training loss: 6.518903732299805
Epoch 120, training loss: 6.47648286819458
Epoch 130, training loss: 6.353715896606445
Epoch 140, training loss: 6.299551486968994
Epoch 150, training loss: 6.189093589782715
Epoch 160, training loss: 6.0599164962768555
Epoch 170, training loss: 6.177109718322754
Epoch 180, training loss: 6.0453643798828125
Epoch 190, training loss: 5.878885746002197
Epoch 200, training loss: 5.933870792388916
Epoch 210, training loss: 5.791543960571289
Epoch 220, training loss: 5.798806190490723
Epoch 230, training loss: 5.692671775817871
Epoch 240, training loss: 5.636434555053711
Epoch 250, training loss: 5.738241195678711
Epoch 260, training loss: 5.706564903259277
Epoch 270, training loss: 5.630797386169434
Epoch 280, training loss: 5.596196174621582
Epoch 290, training loss: 5.549062728881836
Epoch 300, training loss: 5.579918384552002
Epoch 310, training loss: 5.459221363067627
Epoch 320, training loss: 5.462460041046143
Epoch 330, training loss: 5.4337687492370605
Epoch 340, training loss: 5.381947994232178
Epoch 350, training loss: 5.444272518157959
Epoch 360, training loss: 5.329179763793945
Epoch 370, training loss: 5.306785583496094
Epoch 380, training loss: 5.309415817260742
Epoch 390, training loss: 5.297695159912109
Epoch 400, training loss: 5.3688435554504395
Epoch 410, training loss: 5.273440361022949
Epoch 420, training loss: 5.271706581115723
Epoch 430, training loss: 5.233097076416016
Epoch 440, training loss: 5.214487552642822
Epoch 450, training loss: 5.24752950668335
Epoch 460, training loss: 5.1501851081848145
Epoch 470, training loss: 5.239044189453125
Epoch 480, training loss: 5.146372318267822
Epoch 490, training loss: 5.172131061553955
none
Accuracy: 0.788
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596598625183105
Epoch 10, training loss: 8.575411796569824
Epoch 20, training loss: 8.456130981445312
Epoch 30, training loss: 8.32411003112793
Epoch 40, training loss: 8.111075401306152
Epoch 50, training loss: 7.626136302947998
Epoch 60, training loss: 7.405442237854004
Epoch 70, training loss: 7.0031843185424805
Epoch 80, training loss: 6.830894947052002
Epoch 90, training loss: 6.648422718048096
Epoch 100, training loss: 6.5272746086120605
Epoch 110, training loss: 6.484216213226318
Epoch 120, training loss: 6.412464618682861
Epoch 130, training loss: 6.240115165710449
Epoch 140, training loss: 6.337469100952148
Epoch 150, training loss: 6.135244369506836
Epoch 160, training loss: 6.156704902648926
Epoch 170, training loss: 5.9505486488342285
Epoch 180, training loss: 6.001763820648193
Epoch 190, training loss: 5.968331336975098
Epoch 200, training loss: 5.872113227844238
Epoch 210, training loss: 5.827308654785156
Epoch 220, training loss: 5.722390174865723
Epoch 230, training loss: 5.773559093475342
Epoch 240, training loss: 5.777310371398926
Epoch 250, training loss: 5.787009239196777
Epoch 260, training loss: 5.677844524383545
Epoch 270, training loss: 5.674424171447754
Epoch 280, training loss: 5.531968116760254
Epoch 290, training loss: 5.631415843963623
Epoch 300, training loss: 5.619288444519043
Epoch 310, training loss: 5.511019229888916
Epoch 320, training loss: 5.464393615722656
Epoch 330, training loss: 5.51876974105835
Epoch 340, training loss: 5.389700412750244
Epoch 350, training loss: 5.512922286987305
Epoch 360, training loss: 5.414707183837891
Epoch 370, training loss: 5.4853196144104
Epoch 380, training loss: 5.3932013511657715
Epoch 390, training loss: 5.482117176055908
Epoch 400, training loss: 5.344792366027832
Epoch 410, training loss: 5.351902961730957
Epoch 420, training loss: 5.293150901794434
Epoch 430, training loss: 5.320906162261963
Epoch 440, training loss: 5.211922645568848
Epoch 450, training loss: 5.275816917419434
Epoch 460, training loss: 5.211619853973389
Epoch 470, training loss: 5.205684661865234
Epoch 480, training loss: 5.267208576202393
Epoch 490, training loss: 5.223930358886719
none
Accuracy: 0.782
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596572875976562
Epoch 10, training loss: 8.576898574829102
Epoch 20, training loss: 8.482650756835938
Epoch 30, training loss: 8.353633880615234
Epoch 40, training loss: 8.092537879943848
Epoch 50, training loss: 7.680095672607422
Epoch 60, training loss: 7.290703773498535
Epoch 70, training loss: 7.011475563049316
Epoch 80, training loss: 6.951623916625977
Epoch 90, training loss: 6.868002414703369
Epoch 100, training loss: 6.746845245361328
Epoch 110, training loss: 6.574629783630371
Epoch 120, training loss: 6.369633674621582
Epoch 130, training loss: 6.484580993652344
Epoch 140, training loss: 6.376428127288818
Epoch 150, training loss: 6.276419162750244
Epoch 160, training loss: 6.185450077056885
Epoch 170, training loss: 6.265732288360596
Epoch 180, training loss: 6.181344985961914
Epoch 190, training loss: 6.023848533630371
Epoch 200, training loss: 5.998588562011719
Epoch 210, training loss: 5.999025821685791
Epoch 220, training loss: 5.977986812591553
Epoch 230, training loss: 5.812559604644775
Epoch 240, training loss: 5.8109450340271
Epoch 250, training loss: 5.785511493682861
Epoch 260, training loss: 5.7368927001953125
Epoch 270, training loss: 5.736702919006348
Epoch 280, training loss: 5.774230003356934
Epoch 290, training loss: 5.683584213256836
Epoch 300, training loss: 5.532188415527344
Epoch 310, training loss: 5.562435150146484
Epoch 320, training loss: 5.515168190002441
Epoch 330, training loss: 5.490315914154053
Epoch 340, training loss: 5.540761470794678
Epoch 350, training loss: 5.5255866050720215
Epoch 360, training loss: 5.494457721710205
Epoch 370, training loss: 5.429621696472168
Epoch 380, training loss: 5.495567321777344
Epoch 390, training loss: 5.418497085571289
Epoch 400, training loss: 5.515309810638428
Epoch 410, training loss: 5.368349552154541
Epoch 420, training loss: 5.371738433837891
Epoch 430, training loss: 5.339101314544678
Epoch 440, training loss: 5.3503546714782715
Epoch 450, training loss: 5.33320951461792
Epoch 460, training loss: 5.2996320724487305
Epoch 470, training loss: 5.332437992095947
Epoch 480, training loss: 5.243372440338135
Epoch 490, training loss: 5.2355194091796875
none
Accuracy: 0.77
beta 0.9
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596203804016113
Epoch 10, training loss: 8.563660621643066
Epoch 20, training loss: 8.436126708984375
Epoch 30, training loss: 8.129581451416016
Epoch 40, training loss: 7.660060405731201
Epoch 50, training loss: 7.449915409088135
Epoch 60, training loss: 7.130740165710449
Epoch 70, training loss: 7.011030673980713
Epoch 80, training loss: 6.908579349517822
Epoch 90, training loss: 6.729260444641113
Epoch 100, training loss: 6.708781719207764
Epoch 110, training loss: 6.434330940246582
Epoch 120, training loss: 6.547682762145996
Epoch 130, training loss: 6.517332077026367
Epoch 140, training loss: 6.294944763183594
Epoch 150, training loss: 6.22418212890625
Epoch 160, training loss: 6.160858631134033
Epoch 170, training loss: 6.107048988342285
Epoch 180, training loss: 6.070413112640381
Epoch 190, training loss: 6.113814830780029
Epoch 200, training loss: 5.888820171356201
Epoch 210, training loss: 5.9039411544799805
Epoch 220, training loss: 5.891341209411621
Epoch 230, training loss: 5.786754608154297
Epoch 240, training loss: 5.756371974945068
Epoch 250, training loss: 5.717257499694824
Epoch 260, training loss: 5.673151969909668
Epoch 270, training loss: 5.719761848449707
Epoch 280, training loss: 5.647082328796387
Epoch 290, training loss: 5.584383010864258
Epoch 300, training loss: 5.532925128936768
Epoch 310, training loss: 5.564021587371826
Epoch 320, training loss: 5.378629684448242
Epoch 330, training loss: 5.489758014678955
Epoch 340, training loss: 5.4801506996154785
Epoch 350, training loss: 5.433364391326904
Epoch 360, training loss: 5.302371501922607
Epoch 370, training loss: 5.42908239364624
Epoch 380, training loss: 5.303012371063232
Epoch 390, training loss: 5.351095676422119
Epoch 400, training loss: 5.235947132110596
Epoch 410, training loss: 5.3397016525268555
Epoch 420, training loss: 5.347507476806641
Epoch 430, training loss: 5.2232160568237305
Epoch 440, training loss: 5.217966079711914
Epoch 450, training loss: 5.207634925842285
Epoch 460, training loss: 5.350279808044434
Epoch 470, training loss: 5.220654487609863
Epoch 480, training loss: 5.152092456817627
Epoch 490, training loss: 5.193192481994629
none
Accuracy: 0.768
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596442222595215
Epoch 10, training loss: 8.5720796585083
Epoch 20, training loss: 8.43402099609375
Epoch 30, training loss: 8.190417289733887
Epoch 40, training loss: 7.868353366851807
Epoch 50, training loss: 7.529393196105957
Epoch 60, training loss: 7.316649913787842
Epoch 70, training loss: 7.2364420890808105
Epoch 80, training loss: 6.862325668334961
Epoch 90, training loss: 6.803439617156982
Epoch 100, training loss: 6.665294170379639
Epoch 110, training loss: 6.573750019073486
Epoch 120, training loss: 6.389791011810303
Epoch 130, training loss: 6.3599534034729
Epoch 140, training loss: 6.178894519805908
Epoch 150, training loss: 6.052197456359863
Epoch 160, training loss: 6.12180757522583
Epoch 170, training loss: 5.981321811676025
Epoch 180, training loss: 6.011843204498291
Epoch 190, training loss: 6.026534557342529
Epoch 200, training loss: 5.943294525146484
Epoch 210, training loss: 5.775291919708252
Epoch 220, training loss: 5.866082668304443
Epoch 230, training loss: 5.7254133224487305
Epoch 240, training loss: 5.759958267211914
Epoch 250, training loss: 5.681732654571533
Epoch 260, training loss: 5.6939802169799805
Epoch 270, training loss: 5.7801513671875
Epoch 280, training loss: 5.598513126373291
Epoch 290, training loss: 5.604931354522705
Epoch 300, training loss: 5.530367374420166
Epoch 310, training loss: 5.549044609069824
Epoch 320, training loss: 5.517724514007568
Epoch 330, training loss: 5.471274375915527
Epoch 340, training loss: 5.502327919006348
Epoch 350, training loss: 5.4220075607299805
Epoch 360, training loss: 5.400282859802246
Epoch 370, training loss: 5.442697525024414
Epoch 380, training loss: 5.426164150238037
Epoch 390, training loss: 5.386145114898682
Epoch 400, training loss: 5.35100793838501
Epoch 410, training loss: 5.314669609069824
Epoch 420, training loss: 5.339443206787109
Epoch 430, training loss: 5.3584370613098145
Epoch 440, training loss: 5.237083911895752
Epoch 450, training loss: 5.276783466339111
Epoch 460, training loss: 5.289580821990967
Epoch 470, training loss: 5.2263288497924805
Epoch 480, training loss: 5.272487640380859
Epoch 490, training loss: 5.259822368621826
none
Accuracy: 0.77
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.596570014953613
Epoch 10, training loss: 8.573456764221191
Epoch 20, training loss: 8.45581340789795
Epoch 30, training loss: 8.335161209106445
Epoch 40, training loss: 7.959509372711182
Epoch 50, training loss: 7.459063529968262
Epoch 60, training loss: 7.322323799133301
Epoch 70, training loss: 7.225112438201904
Epoch 80, training loss: 7.018045425415039
Epoch 90, training loss: 6.833581924438477
Epoch 100, training loss: 6.7387471199035645
Epoch 110, training loss: 6.649311542510986
Epoch 120, training loss: 6.509133815765381
Epoch 130, training loss: 6.491059303283691
Epoch 140, training loss: 6.399107456207275
Epoch 150, training loss: 6.183675765991211
Epoch 160, training loss: 6.217788219451904
Epoch 170, training loss: 6.118474006652832
Epoch 180, training loss: 5.978211879730225
Epoch 190, training loss: 5.973845481872559
Epoch 200, training loss: 5.90219783782959
Epoch 210, training loss: 5.844329833984375
Epoch 220, training loss: 5.962766647338867
Epoch 230, training loss: 5.874039173126221
Epoch 240, training loss: 5.695815086364746
Epoch 250, training loss: 5.684009075164795
Epoch 260, training loss: 5.717794895172119
Epoch 270, training loss: 5.696211814880371
Epoch 280, training loss: 5.632377624511719
Epoch 290, training loss: 5.60434627532959
Epoch 300, training loss: 5.650983810424805
Epoch 310, training loss: 5.4315900802612305
Epoch 320, training loss: 5.55540657043457
Epoch 330, training loss: 5.498535633087158
Epoch 340, training loss: 5.40829610824585
Epoch 350, training loss: 5.498817443847656
Epoch 360, training loss: 5.404029369354248
Epoch 370, training loss: 5.309940814971924
Epoch 380, training loss: 5.384787082672119
Epoch 390, training loss: 5.39319372177124
Epoch 400, training loss: 5.345234394073486
Epoch 410, training loss: 5.319072723388672
Epoch 420, training loss: 5.298291206359863
Epoch 430, training loss: 5.264473915100098
Epoch 440, training loss: 5.257170677185059
Epoch 450, training loss: 5.265900611877441
Epoch 460, training loss: 5.2828755378723145
Epoch 470, training loss: 5.216554164886475
Epoch 480, training loss: 5.287765026092529
Epoch 490, training loss: 5.1705641746521
none
Accuracy: 0.752
