Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10536])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8059446811676025 = 1.946260690689087 + 0.1 * 8.596839904785156
Epoch 0, val loss: 1.955057978630066
Epoch 10, training loss: 2.7965328693389893 = 1.936861276626587 + 0.1 * 8.596714973449707
Epoch 10, val loss: 1.9457311630249023
Epoch 20, training loss: 2.7844882011413574 = 1.9249111413955688 + 0.1 * 8.595770835876465
Epoch 20, val loss: 1.9334321022033691
Epoch 30, training loss: 2.766356945037842 = 1.9076534509658813 + 0.1 * 8.587035179138184
Epoch 30, val loss: 1.9152075052261353
Epoch 40, training loss: 2.7354252338409424 = 1.8819137811660767 + 0.1 * 8.535114288330078
Epoch 40, val loss: 1.8878588676452637
Epoch 50, training loss: 2.677072048187256 = 1.8480812311172485 + 0.1 * 8.289909362792969
Epoch 50, val loss: 1.8531608581542969
Epoch 60, training loss: 2.614093780517578 = 1.8119769096374512 + 0.1 * 8.021167755126953
Epoch 60, val loss: 1.817631721496582
Epoch 70, training loss: 2.564770221710205 = 1.7803138494491577 + 0.1 * 7.8445634841918945
Epoch 70, val loss: 1.786865472793579
Epoch 80, training loss: 2.500314474105835 = 1.7465330362319946 + 0.1 * 7.537814617156982
Epoch 80, val loss: 1.7540568113327026
Epoch 90, training loss: 2.429588556289673 = 1.7046923637390137 + 0.1 * 7.248961448669434
Epoch 90, val loss: 1.7180888652801514
Epoch 100, training loss: 2.3575358390808105 = 1.64919114112854 + 0.1 * 7.083446979522705
Epoch 100, val loss: 1.671671748161316
Epoch 110, training loss: 2.2806308269500732 = 1.5760468244552612 + 0.1 * 7.045840740203857
Epoch 110, val loss: 1.608842372894287
Epoch 120, training loss: 2.1933059692382812 = 1.4912372827529907 + 0.1 * 7.020687103271484
Epoch 120, val loss: 1.5381197929382324
Epoch 130, training loss: 2.102482795715332 = 1.4022738933563232 + 0.1 * 7.00208854675293
Epoch 130, val loss: 1.4659086465835571
Epoch 140, training loss: 2.0105769634246826 = 1.3120864629745483 + 0.1 * 6.9849042892456055
Epoch 140, val loss: 1.3925334215164185
Epoch 150, training loss: 1.9181205034255981 = 1.22195565700531 + 0.1 * 6.961648464202881
Epoch 150, val loss: 1.3207532167434692
Epoch 160, training loss: 1.8275939226150513 = 1.1344783306121826 + 0.1 * 6.931155681610107
Epoch 160, val loss: 1.2524328231811523
Epoch 170, training loss: 1.743399739265442 = 1.0530896186828613 + 0.1 * 6.903100967407227
Epoch 170, val loss: 1.1916583776474
Epoch 180, training loss: 1.6665786504745483 = 0.9781615734100342 + 0.1 * 6.8841705322265625
Epoch 180, val loss: 1.1377546787261963
Epoch 190, training loss: 1.5951930284500122 = 0.9080768823623657 + 0.1 * 6.871161460876465
Epoch 190, val loss: 1.0883653163909912
Epoch 200, training loss: 1.5270545482635498 = 0.8409061431884766 + 0.1 * 6.861483573913574
Epoch 200, val loss: 1.0418273210525513
Epoch 210, training loss: 1.4608936309814453 = 0.7765297293663025 + 0.1 * 6.843638896942139
Epoch 210, val loss: 0.9978287220001221
Epoch 220, training loss: 1.397420883178711 = 0.7150254845619202 + 0.1 * 6.823953151702881
Epoch 220, val loss: 0.9567705988883972
Epoch 230, training loss: 1.3388586044311523 = 0.6577858328819275 + 0.1 * 6.810727119445801
Epoch 230, val loss: 0.9204085469245911
Epoch 240, training loss: 1.2843003273010254 = 0.6056362986564636 + 0.1 * 6.786640644073486
Epoch 240, val loss: 0.8902601003646851
Epoch 250, training loss: 1.235250473022461 = 0.5576702356338501 + 0.1 * 6.775801658630371
Epoch 250, val loss: 0.8661634922027588
Epoch 260, training loss: 1.1892180442810059 = 0.5134456753730774 + 0.1 * 6.757722854614258
Epoch 260, val loss: 0.8476279377937317
Epoch 270, training loss: 1.146828532218933 = 0.4726073443889618 + 0.1 * 6.742211818695068
Epoch 270, val loss: 0.8337159156799316
Epoch 280, training loss: 1.1077752113342285 = 0.43488267064094543 + 0.1 * 6.7289252281188965
Epoch 280, val loss: 0.8236384391784668
Epoch 290, training loss: 1.0719196796417236 = 0.3999417722225189 + 0.1 * 6.719778537750244
Epoch 290, val loss: 0.8169236779212952
Epoch 300, training loss: 1.0393164157867432 = 0.36732217669487 + 0.1 * 6.719942569732666
Epoch 300, val loss: 0.8131617903709412
Epoch 310, training loss: 1.00770103931427 = 0.33682453632354736 + 0.1 * 6.708765029907227
Epoch 310, val loss: 0.8117141723632812
Epoch 320, training loss: 0.977878212928772 = 0.30797964334487915 + 0.1 * 6.698985576629639
Epoch 320, val loss: 0.8121811747550964
Epoch 330, training loss: 0.9502422213554382 = 0.2807362675666809 + 0.1 * 6.695059299468994
Epoch 330, val loss: 0.8143541216850281
Epoch 340, training loss: 0.9240446090698242 = 0.25513723492622375 + 0.1 * 6.689073085784912
Epoch 340, val loss: 0.8183001279830933
Epoch 350, training loss: 0.9004618525505066 = 0.23130632936954498 + 0.1 * 6.691555500030518
Epoch 350, val loss: 0.823925793170929
Epoch 360, training loss: 0.8772229552268982 = 0.20936942100524902 + 0.1 * 6.678534984588623
Epoch 360, val loss: 0.8310716152191162
Epoch 370, training loss: 0.8563458919525146 = 0.18921618163585663 + 0.1 * 6.6712965965271
Epoch 370, val loss: 0.8397881388664246
Epoch 380, training loss: 0.8384751081466675 = 0.1708410084247589 + 0.1 * 6.676340579986572
Epoch 380, val loss: 0.8498426675796509
Epoch 390, training loss: 0.8271620273590088 = 0.1542554795742035 + 0.1 * 6.72906494140625
Epoch 390, val loss: 0.861176609992981
Epoch 400, training loss: 0.8055728673934937 = 0.13953813910484314 + 0.1 * 6.6603474617004395
Epoch 400, val loss: 0.8734679222106934
Epoch 410, training loss: 0.791863203048706 = 0.12637785077095032 + 0.1 * 6.654853343963623
Epoch 410, val loss: 0.8865312933921814
Epoch 420, training loss: 0.7792956233024597 = 0.11458147317171097 + 0.1 * 6.647141456604004
Epoch 420, val loss: 0.9002983570098877
Epoch 430, training loss: 0.7704985737800598 = 0.10401569306850433 + 0.1 * 6.664828300476074
Epoch 430, val loss: 0.9147629141807556
Epoch 440, training loss: 0.7584105730056763 = 0.09463182091712952 + 0.1 * 6.637787342071533
Epoch 440, val loss: 0.9295752048492432
Epoch 450, training loss: 0.7497014999389648 = 0.08625846356153488 + 0.1 * 6.634429931640625
Epoch 450, val loss: 0.944682776927948
Epoch 460, training loss: 0.7418991327285767 = 0.0787794217467308 + 0.1 * 6.631196975708008
Epoch 460, val loss: 0.9599627256393433
Epoch 470, training loss: 0.7345787286758423 = 0.07210446894168854 + 0.1 * 6.62474250793457
Epoch 470, val loss: 0.9753496050834656
Epoch 480, training loss: 0.7288894653320312 = 0.06612662225961685 + 0.1 * 6.627627849578857
Epoch 480, val loss: 0.9907486438751221
Epoch 490, training loss: 0.7229037284851074 = 0.06078736484050751 + 0.1 * 6.621163368225098
Epoch 490, val loss: 1.0059394836425781
Epoch 500, training loss: 0.717043936252594 = 0.056003130972385406 + 0.1 * 6.610407829284668
Epoch 500, val loss: 1.0209238529205322
Epoch 510, training loss: 0.7134487628936768 = 0.05170071870088577 + 0.1 * 6.617480278015137
Epoch 510, val loss: 1.0357038974761963
Epoch 520, training loss: 0.708727240562439 = 0.04783332347869873 + 0.1 * 6.608939170837402
Epoch 520, val loss: 1.0502363443374634
Epoch 530, training loss: 0.7047483325004578 = 0.04434745013713837 + 0.1 * 6.604008674621582
Epoch 530, val loss: 1.0644516944885254
Epoch 540, training loss: 0.7008459568023682 = 0.04120643436908722 + 0.1 * 6.596395492553711
Epoch 540, val loss: 1.0782959461212158
Epoch 550, training loss: 0.6972781419754028 = 0.038366906344890594 + 0.1 * 6.589112281799316
Epoch 550, val loss: 1.0918292999267578
Epoch 560, training loss: 0.6965992450714111 = 0.03579166904091835 + 0.1 * 6.608075141906738
Epoch 560, val loss: 1.1050509214401245
Epoch 570, training loss: 0.692258894443512 = 0.0334610678255558 + 0.1 * 6.587977886199951
Epoch 570, val loss: 1.1179022789001465
Epoch 580, training loss: 0.689590334892273 = 0.03134278953075409 + 0.1 * 6.582475185394287
Epoch 580, val loss: 1.1303805112838745
Epoch 590, training loss: 0.6867822408676147 = 0.029414458200335503 + 0.1 * 6.5736775398254395
Epoch 590, val loss: 1.1425094604492188
Epoch 600, training loss: 0.6852467656135559 = 0.027654433622956276 + 0.1 * 6.575923442840576
Epoch 600, val loss: 1.1542913913726807
Epoch 610, training loss: 0.6837397217750549 = 0.02604442834854126 + 0.1 * 6.576952934265137
Epoch 610, val loss: 1.1657522916793823
Epoch 620, training loss: 0.6814351081848145 = 0.02456984668970108 + 0.1 * 6.568652153015137
Epoch 620, val loss: 1.176905870437622
Epoch 630, training loss: 0.6792502403259277 = 0.023216307163238525 + 0.1 * 6.560338973999023
Epoch 630, val loss: 1.1876318454742432
Epoch 640, training loss: 0.6780493855476379 = 0.021974848583340645 + 0.1 * 6.5607452392578125
Epoch 640, val loss: 1.198244571685791
Epoch 650, training loss: 0.6766189336776733 = 0.020827753469347954 + 0.1 * 6.557911396026611
Epoch 650, val loss: 1.208436369895935
Epoch 660, training loss: 0.6769565343856812 = 0.019770434126257896 + 0.1 * 6.5718607902526855
Epoch 660, val loss: 1.2183259725570679
Epoch 670, training loss: 0.672706663608551 = 0.01879447139799595 + 0.1 * 6.539121627807617
Epoch 670, val loss: 1.228029727935791
Epoch 680, training loss: 0.6728599667549133 = 0.017888646572828293 + 0.1 * 6.549713134765625
Epoch 680, val loss: 1.2373883724212646
Epoch 690, training loss: 0.6708532571792603 = 0.017050202935934067 + 0.1 * 6.53803014755249
Epoch 690, val loss: 1.2464759349822998
Epoch 700, training loss: 0.6696817278862 = 0.016271615400910378 + 0.1 * 6.534101486206055
Epoch 700, val loss: 1.2554372549057007
Epoch 710, training loss: 0.6695089340209961 = 0.015544482506811619 + 0.1 * 6.539644241333008
Epoch 710, val loss: 1.2640984058380127
Epoch 720, training loss: 0.6690435409545898 = 0.014868602156639099 + 0.1 * 6.541749000549316
Epoch 720, val loss: 1.2724214792251587
Epoch 730, training loss: 0.6675639152526855 = 0.01423733402043581 + 0.1 * 6.533265590667725
Epoch 730, val loss: 1.2807632684707642
Epoch 740, training loss: 0.6656936407089233 = 0.013647827319800854 + 0.1 * 6.520457744598389
Epoch 740, val loss: 1.2886384725570679
Epoch 750, training loss: 0.6656255722045898 = 0.013097296468913555 + 0.1 * 6.525282859802246
Epoch 750, val loss: 1.296587586402893
Epoch 760, training loss: 0.6647732853889465 = 0.012579696252942085 + 0.1 * 6.521935939788818
Epoch 760, val loss: 1.3040797710418701
Epoch 770, training loss: 0.663436233997345 = 0.01209577452391386 + 0.1 * 6.513404846191406
Epoch 770, val loss: 1.3115397691726685
Epoch 780, training loss: 0.6634794473648071 = 0.011640937998890877 + 0.1 * 6.51838493347168
Epoch 780, val loss: 1.3188142776489258
Epoch 790, training loss: 0.661576509475708 = 0.011212383396923542 + 0.1 * 6.503640651702881
Epoch 790, val loss: 1.3258050680160522
Epoch 800, training loss: 0.6612341403961182 = 0.010809578001499176 + 0.1 * 6.504245281219482
Epoch 800, val loss: 1.332771897315979
Epoch 810, training loss: 0.661026120185852 = 0.01042971946299076 + 0.1 * 6.5059638023376465
Epoch 810, val loss: 1.3394938707351685
Epoch 820, training loss: 0.6601706147193909 = 0.010070813819766045 + 0.1 * 6.500998020172119
Epoch 820, val loss: 1.3460814952850342
Epoch 830, training loss: 0.6596872806549072 = 0.009731840342283249 + 0.1 * 6.49955415725708
Epoch 830, val loss: 1.3523796796798706
Epoch 840, training loss: 0.6587861776351929 = 0.00941227562725544 + 0.1 * 6.493738651275635
Epoch 840, val loss: 1.358787178993225
Epoch 850, training loss: 0.6577745079994202 = 0.00910937413573265 + 0.1 * 6.4866509437561035
Epoch 850, val loss: 1.3648977279663086
Epoch 860, training loss: 0.6575458645820618 = 0.00882320012897253 + 0.1 * 6.487226486206055
Epoch 860, val loss: 1.3710501194000244
Epoch 870, training loss: 0.656528115272522 = 0.008549750782549381 + 0.1 * 6.479783535003662
Epoch 870, val loss: 1.3767949342727661
Epoch 880, training loss: 0.6557319760322571 = 0.008291260339319706 + 0.1 * 6.474407196044922
Epoch 880, val loss: 1.3826926946640015
Epoch 890, training loss: 0.6580390930175781 = 0.008045235648751259 + 0.1 * 6.499938011169434
Epoch 890, val loss: 1.3883434534072876
Epoch 900, training loss: 0.6553332209587097 = 0.007810392417013645 + 0.1 * 6.4752278327941895
Epoch 900, val loss: 1.3938337564468384
Epoch 910, training loss: 0.6551514267921448 = 0.00758780725300312 + 0.1 * 6.4756364822387695
Epoch 910, val loss: 1.3992403745651245
Epoch 920, training loss: 0.6554011702537537 = 0.007375975139439106 + 0.1 * 6.480252265930176
Epoch 920, val loss: 1.4045785665512085
Epoch 930, training loss: 0.6539276242256165 = 0.007173466961830854 + 0.1 * 6.467541217803955
Epoch 930, val loss: 1.4098999500274658
Epoch 940, training loss: 0.6540917158126831 = 0.006979587022215128 + 0.1 * 6.471121311187744
Epoch 940, val loss: 1.4150241613388062
Epoch 950, training loss: 0.6537166237831116 = 0.006793851498514414 + 0.1 * 6.469227313995361
Epoch 950, val loss: 1.4199141263961792
Epoch 960, training loss: 0.6541096568107605 = 0.006616989616304636 + 0.1 * 6.474926471710205
Epoch 960, val loss: 1.4248541593551636
Epoch 970, training loss: 0.6522973775863647 = 0.00644805608317256 + 0.1 * 6.458493232727051
Epoch 970, val loss: 1.4297399520874023
Epoch 980, training loss: 0.6520757079124451 = 0.006285910960286856 + 0.1 * 6.457898139953613
Epoch 980, val loss: 1.4344507455825806
Epoch 990, training loss: 0.6525168418884277 = 0.006130822002887726 + 0.1 * 6.463860034942627
Epoch 990, val loss: 1.4390747547149658
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 2.814138174057007 = 1.954451560974121 + 0.1 * 8.5968656539917
Epoch 0, val loss: 1.9532983303070068
Epoch 10, training loss: 2.802532196044922 = 1.9428534507751465 + 0.1 * 8.596787452697754
Epoch 10, val loss: 1.941853642463684
Epoch 20, training loss: 2.7875466346740723 = 1.9279083013534546 + 0.1 * 8.596384048461914
Epoch 20, val loss: 1.9264702796936035
Epoch 30, training loss: 2.765871047973633 = 1.9066177606582642 + 0.1 * 8.592533111572266
Epoch 30, val loss: 1.904057502746582
Epoch 40, training loss: 2.732050657272339 = 1.8757672309875488 + 0.1 * 8.562834739685059
Epoch 40, val loss: 1.8717567920684814
Epoch 50, training loss: 2.6796438694000244 = 1.8370800018310547 + 0.1 * 8.425638198852539
Epoch 50, val loss: 1.8334630727767944
Epoch 60, training loss: 2.6126537322998047 = 1.8028565645217896 + 0.1 * 8.097970962524414
Epoch 60, val loss: 1.8032952547073364
Epoch 70, training loss: 2.572370767593384 = 1.7741459608078003 + 0.1 * 7.982247352600098
Epoch 70, val loss: 1.777876853942871
Epoch 80, training loss: 2.507450819015503 = 1.7367607355117798 + 0.1 * 7.7069010734558105
Epoch 80, val loss: 1.743485450744629
Epoch 90, training loss: 2.4259183406829834 = 1.6892706155776978 + 0.1 * 7.3664774894714355
Epoch 90, val loss: 1.701709270477295
Epoch 100, training loss: 2.3423540592193604 = 1.6279401779174805 + 0.1 * 7.144138813018799
Epoch 100, val loss: 1.6498196125030518
Epoch 110, training loss: 2.2577672004699707 = 1.5514352321624756 + 0.1 * 7.063318252563477
Epoch 110, val loss: 1.582889437675476
Epoch 120, training loss: 2.1697487831115723 = 1.4683363437652588 + 0.1 * 7.014123439788818
Epoch 120, val loss: 1.5131456851959229
Epoch 130, training loss: 2.085005760192871 = 1.386635184288025 + 0.1 * 6.983706951141357
Epoch 130, val loss: 1.447519302368164
Epoch 140, training loss: 2.000755548477173 = 1.3052421808242798 + 0.1 * 6.955132961273193
Epoch 140, val loss: 1.3834238052368164
Epoch 150, training loss: 1.9151418209075928 = 1.2224749326705933 + 0.1 * 6.926669597625732
Epoch 150, val loss: 1.319556474685669
Epoch 160, training loss: 1.8299565315246582 = 1.1393425464630127 + 0.1 * 6.906138896942139
Epoch 160, val loss: 1.2578561305999756
Epoch 170, training loss: 1.7458322048187256 = 1.0567364692687988 + 0.1 * 6.890956401824951
Epoch 170, val loss: 1.1978670358657837
Epoch 180, training loss: 1.6628549098968506 = 0.9749586582183838 + 0.1 * 6.878962516784668
Epoch 180, val loss: 1.1389946937561035
Epoch 190, training loss: 1.5833121538162231 = 0.896044135093689 + 0.1 * 6.872680187225342
Epoch 190, val loss: 1.0826990604400635
Epoch 200, training loss: 1.5079302787780762 = 0.8222939372062683 + 0.1 * 6.856362819671631
Epoch 200, val loss: 1.0305323600769043
Epoch 210, training loss: 1.4389005899429321 = 0.7544392943382263 + 0.1 * 6.844613075256348
Epoch 210, val loss: 0.9839037656784058
Epoch 220, training loss: 1.3766919374465942 = 0.6936614513397217 + 0.1 * 6.8303046226501465
Epoch 220, val loss: 0.9441227912902832
Epoch 230, training loss: 1.32061767578125 = 0.638900101184845 + 0.1 * 6.817174911499023
Epoch 230, val loss: 0.910757303237915
Epoch 240, training loss: 1.2700339555740356 = 0.5885414481163025 + 0.1 * 6.814924716949463
Epoch 240, val loss: 0.8830417990684509
Epoch 250, training loss: 1.2217471599578857 = 0.5419064164161682 + 0.1 * 6.798407554626465
Epoch 250, val loss: 0.8602114319801331
Epoch 260, training loss: 1.1770612001419067 = 0.4979337155818939 + 0.1 * 6.791274547576904
Epoch 260, val loss: 0.8410338759422302
Epoch 270, training loss: 1.1364468336105347 = 0.45669543743133545 + 0.1 * 6.797513961791992
Epoch 270, val loss: 0.8252801299095154
Epoch 280, training loss: 1.0968679189682007 = 0.4186682105064392 + 0.1 * 6.781997203826904
Epoch 280, val loss: 0.813054084777832
Epoch 290, training loss: 1.0613821744918823 = 0.38387036323547363 + 0.1 * 6.775117874145508
Epoch 290, val loss: 0.8047049641609192
Epoch 300, training loss: 1.0312151908874512 = 0.3525499403476715 + 0.1 * 6.786652088165283
Epoch 300, val loss: 0.8004490733146667
Epoch 310, training loss: 1.001457929611206 = 0.3249296545982361 + 0.1 * 6.765283107757568
Epoch 310, val loss: 0.8002285361289978
Epoch 320, training loss: 0.9763312935829163 = 0.3004692792892456 + 0.1 * 6.758620262145996
Epoch 320, val loss: 0.8033751249313354
Epoch 330, training loss: 0.9558306932449341 = 0.27887269854545593 + 0.1 * 6.7695794105529785
Epoch 330, val loss: 0.8090711236000061
Epoch 340, training loss: 0.9345743656158447 = 0.25983932614326477 + 0.1 * 6.747350215911865
Epoch 340, val loss: 0.8167257308959961
Epoch 350, training loss: 0.9167335033416748 = 0.24270564317703247 + 0.1 * 6.740278244018555
Epoch 350, val loss: 0.8259021043777466
Epoch 360, training loss: 0.9024582505226135 = 0.22700561583042145 + 0.1 * 6.754526615142822
Epoch 360, val loss: 0.8361548781394958
Epoch 370, training loss: 0.885787308216095 = 0.21242405474185944 + 0.1 * 6.733632564544678
Epoch 370, val loss: 0.8473391532897949
Epoch 380, training loss: 0.8703572750091553 = 0.1984054148197174 + 0.1 * 6.719518661499023
Epoch 380, val loss: 0.8592969179153442
Epoch 390, training loss: 0.8553553819656372 = 0.18459419906139374 + 0.1 * 6.707611560821533
Epoch 390, val loss: 0.8717827796936035
Epoch 400, training loss: 0.8423243761062622 = 0.17091242969036102 + 0.1 * 6.7141194343566895
Epoch 400, val loss: 0.8846389651298523
Epoch 410, training loss: 0.8276728391647339 = 0.15745772421360016 + 0.1 * 6.702151298522949
Epoch 410, val loss: 0.8974996209144592
Epoch 420, training loss: 0.8145554065704346 = 0.14431720972061157 + 0.1 * 6.7023820877075195
Epoch 420, val loss: 0.9103476405143738
Epoch 430, training loss: 0.8009123802185059 = 0.13173235952854156 + 0.1 * 6.691800117492676
Epoch 430, val loss: 0.9230639338493347
Epoch 440, training loss: 0.7876232266426086 = 0.11984990537166595 + 0.1 * 6.677732944488525
Epoch 440, val loss: 0.9357114434242249
Epoch 450, training loss: 0.7758057713508606 = 0.10876581817865372 + 0.1 * 6.670399188995361
Epoch 450, val loss: 0.9482790231704712
Epoch 460, training loss: 0.7649351358413696 = 0.09855660051107407 + 0.1 * 6.663785457611084
Epoch 460, val loss: 0.9609150290489197
Epoch 470, training loss: 0.7551615238189697 = 0.08921389281749725 + 0.1 * 6.659475803375244
Epoch 470, val loss: 0.9736288785934448
Epoch 480, training loss: 0.7462793588638306 = 0.0807412639260292 + 0.1 * 6.655380725860596
Epoch 480, val loss: 0.9864335656166077
Epoch 490, training loss: 0.7394005060195923 = 0.07311102747917175 + 0.1 * 6.662895202636719
Epoch 490, val loss: 0.999295175075531
Epoch 500, training loss: 0.7309172749519348 = 0.06630963087081909 + 0.1 * 6.646076202392578
Epoch 500, val loss: 1.012216329574585
Epoch 510, training loss: 0.7262181043624878 = 0.06025008484721184 + 0.1 * 6.659679889678955
Epoch 510, val loss: 1.0251476764678955
Epoch 520, training loss: 0.7189030647277832 = 0.05489521846175194 + 0.1 * 6.640078067779541
Epoch 520, val loss: 1.0378413200378418
Epoch 530, training loss: 0.713073194026947 = 0.05015026777982712 + 0.1 * 6.6292290687561035
Epoch 530, val loss: 1.0503042936325073
Epoch 540, training loss: 0.7084290385246277 = 0.045940101146698 + 0.1 * 6.624889373779297
Epoch 540, val loss: 1.0626235008239746
Epoch 550, training loss: 0.7041775584220886 = 0.04220607131719589 + 0.1 * 6.619714736938477
Epoch 550, val loss: 1.0746735334396362
Epoch 560, training loss: 0.6997833251953125 = 0.03888804093003273 + 0.1 * 6.60895299911499
Epoch 560, val loss: 1.0865334272384644
Epoch 570, training loss: 0.697259783744812 = 0.035931363701820374 + 0.1 * 6.613284111022949
Epoch 570, val loss: 1.0980165004730225
Epoch 580, training loss: 0.6942552924156189 = 0.033296000212430954 + 0.1 * 6.609592914581299
Epoch 580, val loss: 1.1092915534973145
Epoch 590, training loss: 0.6904934644699097 = 0.03094368241727352 + 0.1 * 6.5954976081848145
Epoch 590, val loss: 1.1201269626617432
Epoch 600, training loss: 0.6868448853492737 = 0.028828050941228867 + 0.1 * 6.580168724060059
Epoch 600, val loss: 1.1307052373886108
Epoch 610, training loss: 0.6865552067756653 = 0.026919184252619743 + 0.1 * 6.596360206604004
Epoch 610, val loss: 1.1410244703292847
Epoch 620, training loss: 0.6836985945701599 = 0.02519908919930458 + 0.1 * 6.584994792938232
Epoch 620, val loss: 1.1509838104248047
Epoch 630, training loss: 0.6811538338661194 = 0.02364536188542843 + 0.1 * 6.575084686279297
Epoch 630, val loss: 1.1606926918029785
Epoch 640, training loss: 0.679133415222168 = 0.0222302433103323 + 0.1 * 6.569031238555908
Epoch 640, val loss: 1.1701151132583618
Epoch 650, training loss: 0.6780486106872559 = 0.02094416134059429 + 0.1 * 6.571044445037842
Epoch 650, val loss: 1.179179310798645
Epoch 660, training loss: 0.6767165660858154 = 0.01977461203932762 + 0.1 * 6.5694193840026855
Epoch 660, val loss: 1.187984585762024
Epoch 670, training loss: 0.674788236618042 = 0.018704740330576897 + 0.1 * 6.560835361480713
Epoch 670, val loss: 1.1965981721878052
Epoch 680, training loss: 0.6721906065940857 = 0.017722513526678085 + 0.1 * 6.544681072235107
Epoch 680, val loss: 1.2049221992492676
Epoch 690, training loss: 0.6735761761665344 = 0.016818739473819733 + 0.1 * 6.5675740242004395
Epoch 690, val loss: 1.2129837274551392
Epoch 700, training loss: 0.6709433794021606 = 0.01598992384970188 + 0.1 * 6.549534797668457
Epoch 700, val loss: 1.2208796739578247
Epoch 710, training loss: 0.6696773767471313 = 0.015223834663629532 + 0.1 * 6.544535160064697
Epoch 710, val loss: 1.2285616397857666
Epoch 720, training loss: 0.6673072576522827 = 0.014515748247504234 + 0.1 * 6.527915000915527
Epoch 720, val loss: 1.2358616590499878
Epoch 730, training loss: 0.6674468517303467 = 0.013862467370927334 + 0.1 * 6.535843372344971
Epoch 730, val loss: 1.243189811706543
Epoch 740, training loss: 0.6657044291496277 = 0.01325286366045475 + 0.1 * 6.524515628814697
Epoch 740, val loss: 1.2502429485321045
Epoch 750, training loss: 0.6651820540428162 = 0.012685016728937626 + 0.1 * 6.524970531463623
Epoch 750, val loss: 1.257171869277954
Epoch 760, training loss: 0.665093183517456 = 0.012155049480497837 + 0.1 * 6.529381275177002
Epoch 760, val loss: 1.263925552368164
Epoch 770, training loss: 0.6630131602287292 = 0.011661123484373093 + 0.1 * 6.513520240783691
Epoch 770, val loss: 1.2704418897628784
Epoch 780, training loss: 0.6634991765022278 = 0.011199736967682838 + 0.1 * 6.522994518280029
Epoch 780, val loss: 1.276753306388855
Epoch 790, training loss: 0.6609072089195251 = 0.010770360007882118 + 0.1 * 6.501368045806885
Epoch 790, val loss: 1.282976746559143
Epoch 800, training loss: 0.6612095236778259 = 0.010366425849497318 + 0.1 * 6.5084309577941895
Epoch 800, val loss: 1.2890942096710205
Epoch 810, training loss: 0.6594828367233276 = 0.00998669397085905 + 0.1 * 6.494961261749268
Epoch 810, val loss: 1.2949268817901611
Epoch 820, training loss: 0.6592304706573486 = 0.009629633277654648 + 0.1 * 6.496008396148682
Epoch 820, val loss: 1.3007808923721313
Epoch 830, training loss: 0.6593267917633057 = 0.00929240882396698 + 0.1 * 6.500343322753906
Epoch 830, val loss: 1.3063888549804688
Epoch 840, training loss: 0.6591724753379822 = 0.00897504948079586 + 0.1 * 6.501974105834961
Epoch 840, val loss: 1.3119301795959473
Epoch 850, training loss: 0.6569701433181763 = 0.008676311001181602 + 0.1 * 6.482938289642334
Epoch 850, val loss: 1.3174259662628174
Epoch 860, training loss: 0.6590908765792847 = 0.008392841555178165 + 0.1 * 6.506979942321777
Epoch 860, val loss: 1.3227458000183105
Epoch 870, training loss: 0.6576368808746338 = 0.008124721236526966 + 0.1 * 6.495121479034424
Epoch 870, val loss: 1.3278799057006836
Epoch 880, training loss: 0.6558640599250793 = 0.007871533744037151 + 0.1 * 6.479925632476807
Epoch 880, val loss: 1.3330013751983643
Epoch 890, training loss: 0.6551587581634521 = 0.007631387561559677 + 0.1 * 6.475274085998535
Epoch 890, val loss: 1.3380274772644043
Epoch 900, training loss: 0.6544994711875916 = 0.0074023776687681675 + 0.1 * 6.47097110748291
Epoch 900, val loss: 1.3428629636764526
Epoch 910, training loss: 0.6548110246658325 = 0.0071854256093502045 + 0.1 * 6.476256370544434
Epoch 910, val loss: 1.3476643562316895
Epoch 920, training loss: 0.653988242149353 = 0.006978941615670919 + 0.1 * 6.4700927734375
Epoch 920, val loss: 1.3523567914962769
Epoch 930, training loss: 0.6541097164154053 = 0.006782561540603638 + 0.1 * 6.47327184677124
Epoch 930, val loss: 1.356995701789856
Epoch 940, training loss: 0.6532646417617798 = 0.006594787817448378 + 0.1 * 6.46669864654541
Epoch 940, val loss: 1.3615007400512695
Epoch 950, training loss: 0.6522142291069031 = 0.006415810436010361 + 0.1 * 6.457984447479248
Epoch 950, val loss: 1.3659287691116333
Epoch 960, training loss: 0.6546590328216553 = 0.006244899705052376 + 0.1 * 6.4841413497924805
Epoch 960, val loss: 1.3702441453933716
Epoch 970, training loss: 0.6524202227592468 = 0.006082557607442141 + 0.1 * 6.463376522064209
Epoch 970, val loss: 1.3744478225708008
Epoch 980, training loss: 0.6515437960624695 = 0.005926788784563541 + 0.1 * 6.456170082092285
Epoch 980, val loss: 1.378682255744934
Epoch 990, training loss: 0.6527617573738098 = 0.005777700338512659 + 0.1 * 6.4698405265808105
Epoch 990, val loss: 1.382746696472168
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 2.7942957878112793 = 1.9346117973327637 + 0.1 * 8.596840858459473
Epoch 0, val loss: 1.9360485076904297
Epoch 10, training loss: 2.784254550933838 = 1.924577236175537 + 0.1 * 8.596772193908691
Epoch 10, val loss: 1.9263192415237427
Epoch 20, training loss: 2.7719991207122803 = 1.9123650789260864 + 0.1 * 8.59634017944336
Epoch 20, val loss: 1.913939356803894
Epoch 30, training loss: 2.7545273303985596 = 1.8952656984329224 + 0.1 * 8.59261703491211
Epoch 30, val loss: 1.8957690000534058
Epoch 40, training loss: 2.726656436920166 = 1.8701159954071045 + 0.1 * 8.565403938293457
Epoch 40, val loss: 1.8685226440429688
Epoch 50, training loss: 2.679811954498291 = 1.8367027044296265 + 0.1 * 8.43109130859375
Epoch 50, val loss: 1.8335402011871338
Epoch 60, training loss: 2.609523296356201 = 1.8025051355361938 + 0.1 * 8.070182800292969
Epoch 60, val loss: 1.8006303310394287
Epoch 70, training loss: 2.561154842376709 = 1.7702558040618896 + 0.1 * 7.908990383148193
Epoch 70, val loss: 1.7707562446594238
Epoch 80, training loss: 2.487712860107422 = 1.7300095558166504 + 0.1 * 7.577033042907715
Epoch 80, val loss: 1.7338745594024658
Epoch 90, training loss: 2.407205104827881 = 1.6785410642623901 + 0.1 * 7.286639213562012
Epoch 90, val loss: 1.6878854036331177
Epoch 100, training loss: 2.3275840282440186 = 1.611189842224121 + 0.1 * 7.163941860198975
Epoch 100, val loss: 1.628777265548706
Epoch 110, training loss: 2.238186836242676 = 1.5294506549835205 + 0.1 * 7.087361812591553
Epoch 110, val loss: 1.5569626092910767
Epoch 120, training loss: 2.1427764892578125 = 1.4389779567718506 + 0.1 * 7.0379862785339355
Epoch 120, val loss: 1.481389045715332
Epoch 130, training loss: 2.045970916748047 = 1.3445825576782227 + 0.1 * 7.013884544372559
Epoch 130, val loss: 1.404542088508606
Epoch 140, training loss: 1.9483730792999268 = 1.2486886978149414 + 0.1 * 6.9968438148498535
Epoch 140, val loss: 1.3289519548416138
Epoch 150, training loss: 1.8512423038482666 = 1.1535251140594482 + 0.1 * 6.977170944213867
Epoch 150, val loss: 1.2557209730148315
Epoch 160, training loss: 1.7585053443908691 = 1.0631409883499146 + 0.1 * 6.953642845153809
Epoch 160, val loss: 1.1879857778549194
Epoch 170, training loss: 1.6740946769714355 = 0.9815783500671387 + 0.1 * 6.925162315368652
Epoch 170, val loss: 1.1285148859024048
Epoch 180, training loss: 1.5986759662628174 = 0.9080743789672852 + 0.1 * 6.906015396118164
Epoch 180, val loss: 1.0764862298965454
Epoch 190, training loss: 1.5294965505599976 = 0.841518759727478 + 0.1 * 6.879777908325195
Epoch 190, val loss: 1.0305060148239136
Epoch 200, training loss: 1.4658558368682861 = 0.7797696590423584 + 0.1 * 6.860860824584961
Epoch 200, val loss: 0.9888231158256531
Epoch 210, training loss: 1.407665491104126 = 0.7231140732765198 + 0.1 * 6.845513820648193
Epoch 210, val loss: 0.9522497057914734
Epoch 220, training loss: 1.3540985584259033 = 0.6713199019432068 + 0.1 * 6.827785968780518
Epoch 220, val loss: 0.9213122129440308
Epoch 230, training loss: 1.3066588640213013 = 0.6235097646713257 + 0.1 * 6.831490993499756
Epoch 230, val loss: 0.896111786365509
Epoch 240, training loss: 1.2606679201126099 = 0.5798415541648865 + 0.1 * 6.808263301849365
Epoch 240, val loss: 0.8768898844718933
Epoch 250, training loss: 1.2186939716339111 = 0.5388553142547607 + 0.1 * 6.798386573791504
Epoch 250, val loss: 0.8622708916664124
Epoch 260, training loss: 1.1787958145141602 = 0.5000671148300171 + 0.1 * 6.787286758422852
Epoch 260, val loss: 0.8524318337440491
Epoch 270, training loss: 1.1410876512527466 = 0.46308937668800354 + 0.1 * 6.779982089996338
Epoch 270, val loss: 0.8468759059906006
Epoch 280, training loss: 1.1060198545455933 = 0.4280144274234772 + 0.1 * 6.780054092407227
Epoch 280, val loss: 0.8447537422180176
Epoch 290, training loss: 1.0725681781768799 = 0.3951605260372162 + 0.1 * 6.774076461791992
Epoch 290, val loss: 0.8450895547866821
Epoch 300, training loss: 1.040395975112915 = 0.36411553621292114 + 0.1 * 6.762803554534912
Epoch 300, val loss: 0.8469813466072083
Epoch 310, training loss: 1.0131165981292725 = 0.3345586955547333 + 0.1 * 6.785578727722168
Epoch 310, val loss: 0.8501109480857849
Epoch 320, training loss: 0.9824062585830688 = 0.30644410848617554 + 0.1 * 6.7596211433410645
Epoch 320, val loss: 0.8541052937507629
Epoch 330, training loss: 0.9542921781539917 = 0.2793981730937958 + 0.1 * 6.748939514160156
Epoch 330, val loss: 0.8587970733642578
Epoch 340, training loss: 0.9275214672088623 = 0.25334587693214417 + 0.1 * 6.741755962371826
Epoch 340, val loss: 0.8640991449356079
Epoch 350, training loss: 0.9021593332290649 = 0.22846448421478271 + 0.1 * 6.736948490142822
Epoch 350, val loss: 0.8700661659240723
Epoch 360, training loss: 0.8790374398231506 = 0.20498643815517426 + 0.1 * 6.740509986877441
Epoch 360, val loss: 0.8767945170402527
Epoch 370, training loss: 0.8560908436775208 = 0.1833544820547104 + 0.1 * 6.727363586425781
Epoch 370, val loss: 0.884232223033905
Epoch 380, training loss: 0.8350661993026733 = 0.1635836809873581 + 0.1 * 6.714824676513672
Epoch 380, val loss: 0.8924311995506287
Epoch 390, training loss: 0.8178056478500366 = 0.14570294320583344 + 0.1 * 6.72102689743042
Epoch 390, val loss: 0.9013053774833679
Epoch 400, training loss: 0.8000035285949707 = 0.12973619997501373 + 0.1 * 6.702673435211182
Epoch 400, val loss: 0.910791277885437
Epoch 410, training loss: 0.7845597267150879 = 0.1155259907245636 + 0.1 * 6.690337181091309
Epoch 410, val loss: 0.9206820130348206
Epoch 420, training loss: 0.771608829498291 = 0.10298731923103333 + 0.1 * 6.686214447021484
Epoch 420, val loss: 0.9308222532272339
Epoch 430, training loss: 0.7590704560279846 = 0.09197366237640381 + 0.1 * 6.6709675788879395
Epoch 430, val loss: 0.94130939245224
Epoch 440, training loss: 0.7495753765106201 = 0.08229198306798935 + 0.1 * 6.6728339195251465
Epoch 440, val loss: 0.9519788026809692
Epoch 450, training loss: 0.7407518625259399 = 0.0738193690776825 + 0.1 * 6.6693243980407715
Epoch 450, val loss: 0.9626931548118591
Epoch 460, training loss: 0.7307834029197693 = 0.06641004234552383 + 0.1 * 6.643733501434326
Epoch 460, val loss: 0.9734194278717041
Epoch 470, training loss: 0.7233870625495911 = 0.05992942303419113 + 0.1 * 6.634576320648193
Epoch 470, val loss: 0.984105110168457
Epoch 480, training loss: 0.7167386412620544 = 0.05425949767231941 + 0.1 * 6.624791622161865
Epoch 480, val loss: 0.9946527481079102
Epoch 490, training loss: 0.7113312482833862 = 0.049278102815151215 + 0.1 * 6.6205315589904785
Epoch 490, val loss: 1.0050395727157593
Epoch 500, training loss: 0.7074289321899414 = 0.04490336775779724 + 0.1 * 6.625256061553955
Epoch 500, val loss: 1.0153132677078247
Epoch 510, training loss: 0.7018159031867981 = 0.041068185120821 + 0.1 * 6.607476711273193
Epoch 510, val loss: 1.025242805480957
Epoch 520, training loss: 0.6992852091789246 = 0.037681207060813904 + 0.1 * 6.616039752960205
Epoch 520, val loss: 1.0350141525268555
Epoch 530, training loss: 0.6950117945671082 = 0.03468754515051842 + 0.1 * 6.603242874145508
Epoch 530, val loss: 1.0444093942642212
Epoch 540, training loss: 0.6916363835334778 = 0.03203292563557625 + 0.1 * 6.596034526824951
Epoch 540, val loss: 1.0536469221115112
Epoch 550, training loss: 0.6893165111541748 = 0.029668349772691727 + 0.1 * 6.596481800079346
Epoch 550, val loss: 1.0626674890518188
Epoch 560, training loss: 0.6848264932632446 = 0.027561821043491364 + 0.1 * 6.572646141052246
Epoch 560, val loss: 1.071471095085144
Epoch 570, training loss: 0.6828904151916504 = 0.02567344903945923 + 0.1 * 6.572169780731201
Epoch 570, val loss: 1.080053448677063
Epoch 580, training loss: 0.6815019249916077 = 0.02397952973842621 + 0.1 * 6.575223445892334
Epoch 580, val loss: 1.0883090496063232
Epoch 590, training loss: 0.6785712838172913 = 0.022462796419858932 + 0.1 * 6.561084747314453
Epoch 590, val loss: 1.0963740348815918
Epoch 600, training loss: 0.6780946850776672 = 0.021090781316161156 + 0.1 * 6.570038795471191
Epoch 600, val loss: 1.104158639907837
Epoch 610, training loss: 0.6746588945388794 = 0.019850093871355057 + 0.1 * 6.548088073730469
Epoch 610, val loss: 1.1117544174194336
Epoch 620, training loss: 0.6740095019340515 = 0.0187219250947237 + 0.1 * 6.552875518798828
Epoch 620, val loss: 1.1191726922988892
Epoch 630, training loss: 0.6728157997131348 = 0.01769442856311798 + 0.1 * 6.55121374130249
Epoch 630, val loss: 1.1262826919555664
Epoch 640, training loss: 0.6713729500770569 = 0.01675785891711712 + 0.1 * 6.5461506843566895
Epoch 640, val loss: 1.1333212852478027
Epoch 650, training loss: 0.6689431071281433 = 0.015899691730737686 + 0.1 * 6.530433654785156
Epoch 650, val loss: 1.140063762664795
Epoch 660, training loss: 0.6697045564651489 = 0.015109328553080559 + 0.1 * 6.545952320098877
Epoch 660, val loss: 1.146686315536499
Epoch 670, training loss: 0.6673458814620972 = 0.014384929090738297 + 0.1 * 6.529609680175781
Epoch 670, val loss: 1.1531312465667725
Epoch 680, training loss: 0.6658577919006348 = 0.013718381524085999 + 0.1 * 6.5213942527771
Epoch 680, val loss: 1.1593914031982422
Epoch 690, training loss: 0.6640846729278564 = 0.013100944459438324 + 0.1 * 6.509837627410889
Epoch 690, val loss: 1.1654284000396729
Epoch 700, training loss: 0.6626278758049011 = 0.012527641840279102 + 0.1 * 6.501002311706543
Epoch 700, val loss: 1.1714024543762207
Epoch 710, training loss: 0.6632080078125 = 0.011995291337370872 + 0.1 * 6.512126922607422
Epoch 710, val loss: 1.1772080659866333
Epoch 720, training loss: 0.6633768081665039 = 0.011499716900289059 + 0.1 * 6.518770694732666
Epoch 720, val loss: 1.1829062700271606
Epoch 730, training loss: 0.66129070520401 = 0.011040718294680119 + 0.1 * 6.502499580383301
Epoch 730, val loss: 1.1884710788726807
Epoch 740, training loss: 0.6593863368034363 = 0.010610727593302727 + 0.1 * 6.487756252288818
Epoch 740, val loss: 1.1938354969024658
Epoch 750, training loss: 0.6622605323791504 = 0.010207793675363064 + 0.1 * 6.520527362823486
Epoch 750, val loss: 1.199101209640503
Epoch 760, training loss: 0.6586588621139526 = 0.00982959195971489 + 0.1 * 6.488292694091797
Epoch 760, val loss: 1.204369068145752
Epoch 770, training loss: 0.6610507965087891 = 0.009476211853325367 + 0.1 * 6.515746116638184
Epoch 770, val loss: 1.2094569206237793
Epoch 780, training loss: 0.6571703553199768 = 0.009143142960965633 + 0.1 * 6.48027229309082
Epoch 780, val loss: 1.2143791913986206
Epoch 790, training loss: 0.6566163897514343 = 0.00883046817034483 + 0.1 * 6.477859020233154
Epoch 790, val loss: 1.2192647457122803
Epoch 800, training loss: 0.6568605899810791 = 0.008535228669643402 + 0.1 * 6.483253479003906
Epoch 800, val loss: 1.2240862846374512
Epoch 810, training loss: 0.6565828323364258 = 0.00825615506619215 + 0.1 * 6.483266830444336
Epoch 810, val loss: 1.2287509441375732
Epoch 820, training loss: 0.656196117401123 = 0.007993154227733612 + 0.1 * 6.482029438018799
Epoch 820, val loss: 1.2333344221115112
Epoch 830, training loss: 0.6554805636405945 = 0.007744569797068834 + 0.1 * 6.477360248565674
Epoch 830, val loss: 1.2378917932510376
Epoch 840, training loss: 0.6552624106407166 = 0.007508734241127968 + 0.1 * 6.477536201477051
Epoch 840, val loss: 1.2423624992370605
Epoch 850, training loss: 0.6532514095306396 = 0.007285003550350666 + 0.1 * 6.4596638679504395
Epoch 850, val loss: 1.2466620206832886
Epoch 860, training loss: 0.6550658345222473 = 0.0070728096179664135 + 0.1 * 6.4799299240112305
Epoch 860, val loss: 1.2509404420852661
Epoch 870, training loss: 0.6534144878387451 = 0.0068709971383214 + 0.1 * 6.465435028076172
Epoch 870, val loss: 1.2552123069763184
Epoch 880, training loss: 0.6531496047973633 = 0.0066794115118682384 + 0.1 * 6.4647016525268555
Epoch 880, val loss: 1.2593368291854858
Epoch 890, training loss: 0.6528922319412231 = 0.006497296504676342 + 0.1 * 6.463949203491211
Epoch 890, val loss: 1.2634451389312744
Epoch 900, training loss: 0.6514340043067932 = 0.006323541514575481 + 0.1 * 6.451104640960693
Epoch 900, val loss: 1.2673962116241455
Epoch 910, training loss: 0.650582492351532 = 0.006157896481454372 + 0.1 * 6.4442458152771
Epoch 910, val loss: 1.2712956666946411
Epoch 920, training loss: 0.6498710513114929 = 0.005999396555125713 + 0.1 * 6.438716411590576
Epoch 920, val loss: 1.2751848697662354
Epoch 930, training loss: 0.6510703563690186 = 0.005848430097103119 + 0.1 * 6.452219486236572
Epoch 930, val loss: 1.278949499130249
Epoch 940, training loss: 0.6493290066719055 = 0.005703813396394253 + 0.1 * 6.436252117156982
Epoch 940, val loss: 1.2827529907226562
Epoch 950, training loss: 0.6490790247917175 = 0.005566158331930637 + 0.1 * 6.435128211975098
Epoch 950, val loss: 1.2863883972167969
Epoch 960, training loss: 0.6491895318031311 = 0.005433730315417051 + 0.1 * 6.437557697296143
Epoch 960, val loss: 1.2899994850158691
Epoch 970, training loss: 0.6498715877532959 = 0.0053068241104483604 + 0.1 * 6.445647239685059
Epoch 970, val loss: 1.2935341596603394
Epoch 980, training loss: 0.6480227708816528 = 0.005185310263186693 + 0.1 * 6.428374767303467
Epoch 980, val loss: 1.2970346212387085
Epoch 990, training loss: 0.6481297612190247 = 0.005068638361990452 + 0.1 * 6.430610656738281
Epoch 990, val loss: 1.300469994544983
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8102266736953084
The final CL Acc:0.74568, 0.00462, The final GNN Acc:0.80689, 0.00245
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13208])
remove edge: torch.Size([2, 7880])
updated graph: torch.Size([2, 10532])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8167967796325684 = 1.9571115970611572 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.954528570175171
Epoch 10, training loss: 2.806018590927124 = 1.946342945098877 + 0.1 * 8.596755981445312
Epoch 10, val loss: 1.9443750381469727
Epoch 20, training loss: 2.792802095413208 = 1.9331984519958496 + 0.1 * 8.596035957336426
Epoch 20, val loss: 1.931570291519165
Epoch 30, training loss: 2.773879289627075 = 1.914975643157959 + 0.1 * 8.58903694152832
Epoch 30, val loss: 1.9135862588882446
Epoch 40, training loss: 2.7421746253967285 = 1.8881679773330688 + 0.1 * 8.540067672729492
Epoch 40, val loss: 1.8873852491378784
Epoch 50, training loss: 2.67903995513916 = 1.8511881828308105 + 0.1 * 8.278517723083496
Epoch 50, val loss: 1.8531054258346558
Epoch 60, training loss: 2.6125428676605225 = 1.8088831901550293 + 0.1 * 8.03659725189209
Epoch 60, val loss: 1.8162589073181152
Epoch 70, training loss: 2.5556323528289795 = 1.7688337564468384 + 0.1 * 7.867986679077148
Epoch 70, val loss: 1.7824655771255493
Epoch 80, training loss: 2.482698917388916 = 1.7277793884277344 + 0.1 * 7.549195766448975
Epoch 80, val loss: 1.7442752122879028
Epoch 90, training loss: 2.3980777263641357 = 1.6754001379013062 + 0.1 * 7.226775646209717
Epoch 90, val loss: 1.6940675973892212
Epoch 100, training loss: 2.31447172164917 = 1.6061652898788452 + 0.1 * 7.083065509796143
Epoch 100, val loss: 1.632015585899353
Epoch 110, training loss: 2.222092628479004 = 1.5209864377975464 + 0.1 * 7.011061668395996
Epoch 110, val loss: 1.5581976175308228
Epoch 120, training loss: 2.12965989112854 = 1.4321086406707764 + 0.1 * 6.97551155090332
Epoch 120, val loss: 1.4826469421386719
Epoch 130, training loss: 2.0405311584472656 = 1.3462163209915161 + 0.1 * 6.943148136138916
Epoch 130, val loss: 1.4123625755310059
Epoch 140, training loss: 1.9519712924957275 = 1.260953426361084 + 0.1 * 6.910177707672119
Epoch 140, val loss: 1.3438197374343872
Epoch 150, training loss: 1.863128900527954 = 1.1747446060180664 + 0.1 * 6.883842468261719
Epoch 150, val loss: 1.2742432355880737
Epoch 160, training loss: 1.7745568752288818 = 1.0880515575408936 + 0.1 * 6.865052700042725
Epoch 160, val loss: 1.2048358917236328
Epoch 170, training loss: 1.6840391159057617 = 0.9996230602264404 + 0.1 * 6.844161033630371
Epoch 170, val loss: 1.133755087852478
Epoch 180, training loss: 1.5939035415649414 = 0.9109992384910583 + 0.1 * 6.829042434692383
Epoch 180, val loss: 1.0619994401931763
Epoch 190, training loss: 1.5068130493164062 = 0.8255536556243896 + 0.1 * 6.812593460083008
Epoch 190, val loss: 0.9930485486984253
Epoch 200, training loss: 1.4255852699279785 = 0.7455980777740479 + 0.1 * 6.79987096786499
Epoch 200, val loss: 0.9293797612190247
Epoch 210, training loss: 1.3527352809906006 = 0.6740158200263977 + 0.1 * 6.787193775177002
Epoch 210, val loss: 0.8752084970474243
Epoch 220, training loss: 1.2875826358795166 = 0.6098112463951111 + 0.1 * 6.777713775634766
Epoch 220, val loss: 0.8302506804466248
Epoch 230, training loss: 1.2278900146484375 = 0.5513888001441956 + 0.1 * 6.765011787414551
Epoch 230, val loss: 0.793739914894104
Epoch 240, training loss: 1.1749539375305176 = 0.49830150604248047 + 0.1 * 6.766524314880371
Epoch 240, val loss: 0.7647744417190552
Epoch 250, training loss: 1.1259069442749023 = 0.4510219991207123 + 0.1 * 6.748848915100098
Epoch 250, val loss: 0.7429370284080505
Epoch 260, training loss: 1.0827696323394775 = 0.40862154960632324 + 0.1 * 6.741480350494385
Epoch 260, val loss: 0.7269499897956848
Epoch 270, training loss: 1.0440325736999512 = 0.3705461621284485 + 0.1 * 6.734863758087158
Epoch 270, val loss: 0.7162025570869446
Epoch 280, training loss: 1.0090222358703613 = 0.336253821849823 + 0.1 * 6.727684020996094
Epoch 280, val loss: 0.7098745703697205
Epoch 290, training loss: 0.9780418872833252 = 0.3051990568637848 + 0.1 * 6.728428363800049
Epoch 290, val loss: 0.7072082161903381
Epoch 300, training loss: 0.9489370584487915 = 0.27708104252815247 + 0.1 * 6.718560218811035
Epoch 300, val loss: 0.7073327898979187
Epoch 310, training loss: 0.9238812923431396 = 0.25141674280166626 + 0.1 * 6.724645137786865
Epoch 310, val loss: 0.7099080085754395
Epoch 320, training loss: 0.899106502532959 = 0.22802817821502686 + 0.1 * 6.710783004760742
Epoch 320, val loss: 0.7143264412879944
Epoch 330, training loss: 0.8777048587799072 = 0.20662300288677216 + 0.1 * 6.710818290710449
Epoch 330, val loss: 0.7204108834266663
Epoch 340, training loss: 0.857811689376831 = 0.18710508942604065 + 0.1 * 6.707065582275391
Epoch 340, val loss: 0.7278217077255249
Epoch 350, training loss: 0.8393584489822388 = 0.16934166848659515 + 0.1 * 6.700167655944824
Epoch 350, val loss: 0.7363115549087524
Epoch 360, training loss: 0.8237372040748596 = 0.1532185822725296 + 0.1 * 6.705186367034912
Epoch 360, val loss: 0.7458007335662842
Epoch 370, training loss: 0.8078325986862183 = 0.13868233561515808 + 0.1 * 6.691502571105957
Epoch 370, val loss: 0.7559387683868408
Epoch 380, training loss: 0.794445812702179 = 0.1255847066640854 + 0.1 * 6.688610553741455
Epoch 380, val loss: 0.766733705997467
Epoch 390, training loss: 0.7828733921051025 = 0.11386924982070923 + 0.1 * 6.690041542053223
Epoch 390, val loss: 0.7780114412307739
Epoch 400, training loss: 0.7711426019668579 = 0.10341456532478333 + 0.1 * 6.677280426025391
Epoch 400, val loss: 0.7895153760910034
Epoch 410, training loss: 0.7616575360298157 = 0.09404744952917099 + 0.1 * 6.676101207733154
Epoch 410, val loss: 0.801418125629425
Epoch 420, training loss: 0.7530238032341003 = 0.08567807823419571 + 0.1 * 6.673457145690918
Epoch 420, val loss: 0.8135244846343994
Epoch 430, training loss: 0.7450955510139465 = 0.0782049372792244 + 0.1 * 6.668906211853027
Epoch 430, val loss: 0.8258609771728516
Epoch 440, training loss: 0.7380893230438232 = 0.07152585685253143 + 0.1 * 6.665634632110596
Epoch 440, val loss: 0.8382871150970459
Epoch 450, training loss: 0.7311710715293884 = 0.06557537615299225 + 0.1 * 6.655957221984863
Epoch 450, val loss: 0.8506016731262207
Epoch 460, training loss: 0.7256728410720825 = 0.060247790068387985 + 0.1 * 6.654250621795654
Epoch 460, val loss: 0.8631686568260193
Epoch 470, training loss: 0.7200930714607239 = 0.05547138303518295 + 0.1 * 6.646216869354248
Epoch 470, val loss: 0.8756102323532104
Epoch 480, training loss: 0.717732310295105 = 0.051202915608882904 + 0.1 * 6.665294170379639
Epoch 480, val loss: 0.8882250785827637
Epoch 490, training loss: 0.710902988910675 = 0.04739682748913765 + 0.1 * 6.635061264038086
Epoch 490, val loss: 0.9000587463378906
Epoch 500, training loss: 0.707168698310852 = 0.04397156462073326 + 0.1 * 6.63197135925293
Epoch 500, val loss: 0.9122061729431152
Epoch 510, training loss: 0.7032138109207153 = 0.04087406396865845 + 0.1 * 6.623397350311279
Epoch 510, val loss: 0.9242072105407715
Epoch 520, training loss: 0.6997852921485901 = 0.03807143494486809 + 0.1 * 6.617138385772705
Epoch 520, val loss: 0.9361667633056641
Epoch 530, training loss: 0.6969849467277527 = 0.03554517775774002 + 0.1 * 6.6143975257873535
Epoch 530, val loss: 0.9475110769271851
Epoch 540, training loss: 0.6959420442581177 = 0.033251646906137466 + 0.1 * 6.626904010772705
Epoch 540, val loss: 0.9589527249336243
Epoch 550, training loss: 0.692127525806427 = 0.031169118359684944 + 0.1 * 6.609583854675293
Epoch 550, val loss: 0.9700567126274109
Epoch 560, training loss: 0.689314603805542 = 0.029267936944961548 + 0.1 * 6.600466251373291
Epoch 560, val loss: 0.9809098839759827
Epoch 570, training loss: 0.6865029335021973 = 0.027529507875442505 + 0.1 * 6.589734077453613
Epoch 570, val loss: 0.9916300177574158
Epoch 580, training loss: 0.6888877153396606 = 0.0259380042552948 + 0.1 * 6.629497528076172
Epoch 580, val loss: 1.0023356676101685
Epoch 590, training loss: 0.6838600635528564 = 0.024490978568792343 + 0.1 * 6.593690872192383
Epoch 590, val loss: 1.0122796297073364
Epoch 600, training loss: 0.6808255314826965 = 0.02316316030919552 + 0.1 * 6.576623916625977
Epoch 600, val loss: 1.0221132040023804
Epoch 610, training loss: 0.6799954771995544 = 0.021938811987638474 + 0.1 * 6.580566883087158
Epoch 610, val loss: 1.0317564010620117
Epoch 620, training loss: 0.6794289946556091 = 0.020808983594179153 + 0.1 * 6.586199760437012
Epoch 620, val loss: 1.0413100719451904
Epoch 630, training loss: 0.6762665510177612 = 0.0197695791721344 + 0.1 * 6.564969539642334
Epoch 630, val loss: 1.0502638816833496
Epoch 640, training loss: 0.6746897101402283 = 0.018804961815476418 + 0.1 * 6.558847427368164
Epoch 640, val loss: 1.0593315362930298
Epoch 650, training loss: 0.6760412454605103 = 0.0179092176258564 + 0.1 * 6.581319808959961
Epoch 650, val loss: 1.0682319402694702
Epoch 660, training loss: 0.6726714968681335 = 0.017081042751669884 + 0.1 * 6.555904865264893
Epoch 660, val loss: 1.0767089128494263
Epoch 670, training loss: 0.672252357006073 = 0.016309747472405434 + 0.1 * 6.5594258308410645
Epoch 670, val loss: 1.0849244594573975
Epoch 680, training loss: 0.6707961559295654 = 0.015594295226037502 + 0.1 * 6.552018165588379
Epoch 680, val loss: 1.093263030052185
Epoch 690, training loss: 0.6689306497573853 = 0.014927003532648087 + 0.1 * 6.540036201477051
Epoch 690, val loss: 1.1009234189987183
Epoch 700, training loss: 0.6689711809158325 = 0.01430260669440031 + 0.1 * 6.546685695648193
Epoch 700, val loss: 1.1087722778320312
Epoch 710, training loss: 0.6680312156677246 = 0.013720953837037086 + 0.1 * 6.543102741241455
Epoch 710, val loss: 1.1162288188934326
Epoch 720, training loss: 0.6676818132400513 = 0.013174216262996197 + 0.1 * 6.5450758934021
Epoch 720, val loss: 1.1235517263412476
Epoch 730, training loss: 0.6658926606178284 = 0.012662222608923912 + 0.1 * 6.532304286956787
Epoch 730, val loss: 1.1310118436813354
Epoch 740, training loss: 0.6646146774291992 = 0.012182386592030525 + 0.1 * 6.524322509765625
Epoch 740, val loss: 1.1377203464508057
Epoch 750, training loss: 0.6665474772453308 = 0.011730901896953583 + 0.1 * 6.548165798187256
Epoch 750, val loss: 1.144740104675293
Epoch 760, training loss: 0.6643679738044739 = 0.011306527070701122 + 0.1 * 6.530614376068115
Epoch 760, val loss: 1.1516304016113281
Epoch 770, training loss: 0.6622016429901123 = 0.010908400639891624 + 0.1 * 6.512931823730469
Epoch 770, val loss: 1.1578201055526733
Epoch 780, training loss: 0.6627859473228455 = 0.01053163968026638 + 0.1 * 6.522542953491211
Epoch 780, val loss: 1.1643391847610474
Epoch 790, training loss: 0.6616417765617371 = 0.010175736621022224 + 0.1 * 6.514659881591797
Epoch 790, val loss: 1.1706668138504028
Epoch 800, training loss: 0.6599907875061035 = 0.009839585050940514 + 0.1 * 6.501511573791504
Epoch 800, val loss: 1.1766366958618164
Epoch 810, training loss: 0.6607258915901184 = 0.009520657360553741 + 0.1 * 6.512052059173584
Epoch 810, val loss: 1.1827870607376099
Epoch 820, training loss: 0.6590309143066406 = 0.009218438528478146 + 0.1 * 6.498124599456787
Epoch 820, val loss: 1.1885510683059692
Epoch 830, training loss: 0.6594861745834351 = 0.008931966498494148 + 0.1 * 6.505541801452637
Epoch 830, val loss: 1.1943962574005127
Epoch 840, training loss: 0.6573567390441895 = 0.008661029860377312 + 0.1 * 6.48695707321167
Epoch 840, val loss: 1.1998794078826904
Epoch 850, training loss: 0.6582531332969666 = 0.00840336736291647 + 0.1 * 6.498497486114502
Epoch 850, val loss: 1.2053929567337036
Epoch 860, training loss: 0.6571999192237854 = 0.00815744698047638 + 0.1 * 6.490424633026123
Epoch 860, val loss: 1.2107889652252197
Epoch 870, training loss: 0.6559018492698669 = 0.007923908531665802 + 0.1 * 6.479779243469238
Epoch 870, val loss: 1.2161000967025757
Epoch 880, training loss: 0.6565355062484741 = 0.007701339665800333 + 0.1 * 6.488341808319092
Epoch 880, val loss: 1.2212235927581787
Epoch 890, training loss: 0.6553006172180176 = 0.007488870527595282 + 0.1 * 6.4781174659729
Epoch 890, val loss: 1.2263679504394531
Epoch 900, training loss: 0.6554403305053711 = 0.00728672044351697 + 0.1 * 6.481535911560059
Epoch 900, val loss: 1.2312361001968384
Epoch 910, training loss: 0.6553577780723572 = 0.007092639803886414 + 0.1 * 6.482651233673096
Epoch 910, val loss: 1.2362418174743652
Epoch 920, training loss: 0.6543596386909485 = 0.006908134091645479 + 0.1 * 6.474514961242676
Epoch 920, val loss: 1.2409449815750122
Epoch 930, training loss: 0.6541266441345215 = 0.006730930879712105 + 0.1 * 6.473957061767578
Epoch 930, val loss: 1.2456525564193726
Epoch 940, training loss: 0.6534386277198792 = 0.0065620653331279755 + 0.1 * 6.468765735626221
Epoch 940, val loss: 1.2502763271331787
Epoch 950, training loss: 0.6525482535362244 = 0.006400308106094599 + 0.1 * 6.461479663848877
Epoch 950, val loss: 1.2546347379684448
Epoch 960, training loss: 0.6549088954925537 = 0.006244676653295755 + 0.1 * 6.486642360687256
Epoch 960, val loss: 1.2590141296386719
Epoch 970, training loss: 0.6536213755607605 = 0.00609540706500411 + 0.1 * 6.475259304046631
Epoch 970, val loss: 1.2636042833328247
Epoch 980, training loss: 0.6519873142242432 = 0.005952607374638319 + 0.1 * 6.4603471755981445
Epoch 980, val loss: 1.2677208185195923
Epoch 990, training loss: 0.6513800621032715 = 0.005816024728119373 + 0.1 * 6.455639839172363
Epoch 990, val loss: 1.271733045578003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 2.801323890686035 = 1.941641092300415 + 0.1 * 8.59682846069336
Epoch 0, val loss: 1.942983865737915
Epoch 10, training loss: 2.79093337059021 = 1.9312647581100464 + 0.1 * 8.596685409545898
Epoch 10, val loss: 1.9322519302368164
Epoch 20, training loss: 2.777803421020508 = 1.9182376861572266 + 0.1 * 8.595658302307129
Epoch 20, val loss: 1.918726921081543
Epoch 30, training loss: 2.7584590911865234 = 1.8998949527740479 + 0.1 * 8.585639953613281
Epoch 30, val loss: 1.8997843265533447
Epoch 40, training loss: 2.7250373363494873 = 1.873101830482483 + 0.1 * 8.519355773925781
Epoch 40, val loss: 1.8726112842559814
Epoch 50, training loss: 2.6562857627868652 = 1.8381513357162476 + 0.1 * 8.181343078613281
Epoch 50, val loss: 1.838895559310913
Epoch 60, training loss: 2.5937862396240234 = 1.7987849712371826 + 0.1 * 7.950011253356934
Epoch 60, val loss: 1.8029122352600098
Epoch 70, training loss: 2.5282206535339355 = 1.7595373392105103 + 0.1 * 7.686833381652832
Epoch 70, val loss: 1.7682523727416992
Epoch 80, training loss: 2.4570119380950928 = 1.7162953615188599 + 0.1 * 7.407166004180908
Epoch 80, val loss: 1.7287148237228394
Epoch 90, training loss: 2.3798890113830566 = 1.6625134944915771 + 0.1 * 7.173755645751953
Epoch 90, val loss: 1.6809660196304321
Epoch 100, training loss: 2.2968645095825195 = 1.5934123992919922 + 0.1 * 7.03452205657959
Epoch 100, val loss: 1.6211421489715576
Epoch 110, training loss: 2.208289861679077 = 1.5098094940185547 + 0.1 * 6.984802722930908
Epoch 110, val loss: 1.5495762825012207
Epoch 120, training loss: 2.1161046028137207 = 1.4211981296539307 + 0.1 * 6.949065685272217
Epoch 120, val loss: 1.4767552614212036
Epoch 130, training loss: 2.0252199172973633 = 1.3339639902114868 + 0.1 * 6.912558078765869
Epoch 130, val loss: 1.4062066078186035
Epoch 140, training loss: 1.936647891998291 = 1.2486556768417358 + 0.1 * 6.879922866821289
Epoch 140, val loss: 1.3382002115249634
Epoch 150, training loss: 1.851550817489624 = 1.1649326086044312 + 0.1 * 6.866182804107666
Epoch 150, val loss: 1.272925853729248
Epoch 160, training loss: 1.7707035541534424 = 1.0864125490188599 + 0.1 * 6.842909812927246
Epoch 160, val loss: 1.2131601572036743
Epoch 170, training loss: 1.694862723350525 = 1.011922836303711 + 0.1 * 6.8293986320495605
Epoch 170, val loss: 1.1571918725967407
Epoch 180, training loss: 1.6228432655334473 = 0.9415780901908875 + 0.1 * 6.812652111053467
Epoch 180, val loss: 1.1047769784927368
Epoch 190, training loss: 1.5557528734207153 = 0.875575602054596 + 0.1 * 6.801772594451904
Epoch 190, val loss: 1.0556775331497192
Epoch 200, training loss: 1.4914555549621582 = 0.8146005272865295 + 0.1 * 6.768550395965576
Epoch 200, val loss: 1.0104005336761475
Epoch 210, training loss: 1.4321057796478271 = 0.7572117447853088 + 0.1 * 6.748939514160156
Epoch 210, val loss: 0.9679927229881287
Epoch 220, training loss: 1.375517725944519 = 0.7030553221702576 + 0.1 * 6.724623680114746
Epoch 220, val loss: 0.9287049174308777
Epoch 230, training loss: 1.322075605392456 = 0.6509914398193359 + 0.1 * 6.710841655731201
Epoch 230, val loss: 0.8921900391578674
Epoch 240, training loss: 1.2724571228027344 = 0.6009495258331299 + 0.1 * 6.715076446533203
Epoch 240, val loss: 0.8589329719543457
Epoch 250, training loss: 1.2211697101593018 = 0.552416205406189 + 0.1 * 6.687535762786865
Epoch 250, val loss: 0.8291353583335876
Epoch 260, training loss: 1.171581506729126 = 0.5043258666992188 + 0.1 * 6.672556400299072
Epoch 260, val loss: 0.8018710613250732
Epoch 270, training loss: 1.1242486238479614 = 0.4567663371562958 + 0.1 * 6.67482328414917
Epoch 270, val loss: 0.777742862701416
Epoch 280, training loss: 1.0765985250473022 = 0.4108761250972748 + 0.1 * 6.657223701477051
Epoch 280, val loss: 0.7569552659988403
Epoch 290, training loss: 1.0317052602767944 = 0.3667890727519989 + 0.1 * 6.649161338806152
Epoch 290, val loss: 0.7394292950630188
Epoch 300, training loss: 0.9894693493843079 = 0.3251912593841553 + 0.1 * 6.642780780792236
Epoch 300, val loss: 0.7257564067840576
Epoch 310, training loss: 0.9497120380401611 = 0.2867027223110199 + 0.1 * 6.630092620849609
Epoch 310, val loss: 0.7159067988395691
Epoch 320, training loss: 0.9147739410400391 = 0.2519439458847046 + 0.1 * 6.628299713134766
Epoch 320, val loss: 0.7102369666099548
Epoch 330, training loss: 0.8829099535942078 = 0.22113853693008423 + 0.1 * 6.617713928222656
Epoch 330, val loss: 0.7083262801170349
Epoch 340, training loss: 0.8573118448257446 = 0.19414612650871277 + 0.1 * 6.631657600402832
Epoch 340, val loss: 0.7098675966262817
Epoch 350, training loss: 0.8311132192611694 = 0.17097674310207367 + 0.1 * 6.601364612579346
Epoch 350, val loss: 0.7142972946166992
Epoch 360, training loss: 0.8106346726417542 = 0.15114986896514893 + 0.1 * 6.594847679138184
Epoch 360, val loss: 0.7208451628684998
Epoch 370, training loss: 0.7934146523475647 = 0.13425801694393158 + 0.1 * 6.59156608581543
Epoch 370, val loss: 0.7290247082710266
Epoch 380, training loss: 0.7770770788192749 = 0.119838185608387 + 0.1 * 6.572388648986816
Epoch 380, val loss: 0.7384821176528931
Epoch 390, training loss: 0.7667056918144226 = 0.10743264853954315 + 0.1 * 6.5927300453186035
Epoch 390, val loss: 0.7488301396369934
Epoch 400, training loss: 0.7545229196548462 = 0.09679142385721207 + 0.1 * 6.577314376831055
Epoch 400, val loss: 0.7598422765731812
Epoch 410, training loss: 0.7434282302856445 = 0.08761122822761536 + 0.1 * 6.558169364929199
Epoch 410, val loss: 0.771043062210083
Epoch 420, training loss: 0.7341889142990112 = 0.07962866872549057 + 0.1 * 6.545602798461914
Epoch 420, val loss: 0.7826609015464783
Epoch 430, training loss: 0.7271653413772583 = 0.07263878732919693 + 0.1 * 6.545265197753906
Epoch 430, val loss: 0.7942019701004028
Epoch 440, training loss: 0.7203364372253418 = 0.06651239097118378 + 0.1 * 6.5382399559021
Epoch 440, val loss: 0.8059400916099548
Epoch 450, training loss: 0.7165069580078125 = 0.06110432744026184 + 0.1 * 6.5540266036987305
Epoch 450, val loss: 0.8174548745155334
Epoch 460, training loss: 0.7088909149169922 = 0.05633057653903961 + 0.1 * 6.525603294372559
Epoch 460, val loss: 0.8289086818695068
Epoch 470, training loss: 0.704393208026886 = 0.05208580568432808 + 0.1 * 6.523073673248291
Epoch 470, val loss: 0.8402811288833618
Epoch 480, training loss: 0.7012861967086792 = 0.048285678029060364 + 0.1 * 6.530004978179932
Epoch 480, val loss: 0.8513818979263306
Epoch 490, training loss: 0.6964823603630066 = 0.04487933591008186 + 0.1 * 6.516030311584473
Epoch 490, val loss: 0.8623844385147095
Epoch 500, training loss: 0.6927047371864319 = 0.041810132563114166 + 0.1 * 6.508945941925049
Epoch 500, val loss: 0.8732476234436035
Epoch 510, training loss: 0.6892843246459961 = 0.03903861343860626 + 0.1 * 6.502457141876221
Epoch 510, val loss: 0.8838484287261963
Epoch 520, training loss: 0.6898654103279114 = 0.036527059972286224 + 0.1 * 6.533383369445801
Epoch 520, val loss: 0.8943430185317993
Epoch 530, training loss: 0.6836564540863037 = 0.03425851836800575 + 0.1 * 6.493978977203369
Epoch 530, val loss: 0.9045862555503845
Epoch 540, training loss: 0.6820375323295593 = 0.032189708203077316 + 0.1 * 6.498477935791016
Epoch 540, val loss: 0.9147638082504272
Epoch 550, training loss: 0.6788557171821594 = 0.030299993231892586 + 0.1 * 6.4855570793151855
Epoch 550, val loss: 0.9244949221611023
Epoch 560, training loss: 0.6784178614616394 = 0.028566448017954826 + 0.1 * 6.498514175415039
Epoch 560, val loss: 0.9342429041862488
Epoch 570, training loss: 0.6749603152275085 = 0.026976607739925385 + 0.1 * 6.479836940765381
Epoch 570, val loss: 0.9436976313591003
Epoch 580, training loss: 0.6756408214569092 = 0.025515029206871986 + 0.1 * 6.50125789642334
Epoch 580, val loss: 0.9530628323554993
Epoch 590, training loss: 0.6716209650039673 = 0.024173637852072716 + 0.1 * 6.474473476409912
Epoch 590, val loss: 0.9621149301528931
Epoch 600, training loss: 0.6706228256225586 = 0.02293369174003601 + 0.1 * 6.476891040802002
Epoch 600, val loss: 0.971184492111206
Epoch 610, training loss: 0.6687527298927307 = 0.02178703434765339 + 0.1 * 6.469656467437744
Epoch 610, val loss: 0.9798203110694885
Epoch 620, training loss: 0.6679693460464478 = 0.02072618342936039 + 0.1 * 6.472431182861328
Epoch 620, val loss: 0.9885676503181458
Epoch 630, training loss: 0.6666448712348938 = 0.019741395488381386 + 0.1 * 6.469034194946289
Epoch 630, val loss: 0.9968065023422241
Epoch 640, training loss: 0.665209174156189 = 0.01882747933268547 + 0.1 * 6.4638166427612305
Epoch 640, val loss: 1.0052342414855957
Epoch 650, training loss: 0.6647483110427856 = 0.017977647483348846 + 0.1 * 6.467706203460693
Epoch 650, val loss: 1.0132001638412476
Epoch 660, training loss: 0.663337767124176 = 0.01718573458492756 + 0.1 * 6.461520195007324
Epoch 660, val loss: 1.0212500095367432
Epoch 670, training loss: 0.6632577776908875 = 0.016446106135845184 + 0.1 * 6.468116760253906
Epoch 670, val loss: 1.0289876461029053
Epoch 680, training loss: 0.6613611578941345 = 0.015754738822579384 + 0.1 * 6.456063747406006
Epoch 680, val loss: 1.0365474224090576
Epoch 690, training loss: 0.6611901521682739 = 0.015107628889381886 + 0.1 * 6.460824966430664
Epoch 690, val loss: 1.0440539121627808
Epoch 700, training loss: 0.6596755385398865 = 0.014501871541142464 + 0.1 * 6.4517364501953125
Epoch 700, val loss: 1.0512295961380005
Epoch 710, training loss: 0.659162163734436 = 0.013934298418462276 + 0.1 * 6.4522786140441895
Epoch 710, val loss: 1.0585299730300903
Epoch 720, training loss: 0.6592673063278198 = 0.01340052392333746 + 0.1 * 6.458668231964111
Epoch 720, val loss: 1.0653690099716187
Epoch 730, training loss: 0.6568914651870728 = 0.012898306362330914 + 0.1 * 6.439931392669678
Epoch 730, val loss: 1.0723670721054077
Epoch 740, training loss: 0.6575517058372498 = 0.01242541428655386 + 0.1 * 6.451262474060059
Epoch 740, val loss: 1.0790965557098389
Epoch 750, training loss: 0.6561318039894104 = 0.011980170384049416 + 0.1 * 6.441516399383545
Epoch 750, val loss: 1.085645079612732
Epoch 760, training loss: 0.6560984253883362 = 0.01155993714928627 + 0.1 * 6.445384502410889
Epoch 760, val loss: 1.092220425605774
Epoch 770, training loss: 0.6555732488632202 = 0.011162705719470978 + 0.1 * 6.44410514831543
Epoch 770, val loss: 1.0985393524169922
Epoch 780, training loss: 0.6550151109695435 = 0.010787923820316792 + 0.1 * 6.442272186279297
Epoch 780, val loss: 1.1048084497451782
Epoch 790, training loss: 0.6533433794975281 = 0.01043270155787468 + 0.1 * 6.429106712341309
Epoch 790, val loss: 1.1108944416046143
Epoch 800, training loss: 0.6534726023674011 = 0.010096087120473385 + 0.1 * 6.433765411376953
Epoch 800, val loss: 1.1170004606246948
Epoch 810, training loss: 0.6539377570152283 = 0.009776622988283634 + 0.1 * 6.441610813140869
Epoch 810, val loss: 1.1228384971618652
Epoch 820, training loss: 0.651961088180542 = 0.00947270356118679 + 0.1 * 6.4248833656311035
Epoch 820, val loss: 1.128663420677185
Epoch 830, training loss: 0.6547479629516602 = 0.009184364229440689 + 0.1 * 6.455636024475098
Epoch 830, val loss: 1.1343586444854736
Epoch 840, training loss: 0.6512091755867004 = 0.008910518139600754 + 0.1 * 6.4229865074157715
Epoch 840, val loss: 1.1397567987442017
Epoch 850, training loss: 0.6500673294067383 = 0.008650636300444603 + 0.1 * 6.414166450500488
Epoch 850, val loss: 1.1455512046813965
Epoch 860, training loss: 0.6504074931144714 = 0.008402404375374317 + 0.1 * 6.420050621032715
Epoch 860, val loss: 1.1509361267089844
Epoch 870, training loss: 0.6504388451576233 = 0.008165042847394943 + 0.1 * 6.4227375984191895
Epoch 870, val loss: 1.1559666395187378
Epoch 880, training loss: 0.6488903760910034 = 0.00793879572302103 + 0.1 * 6.409515857696533
Epoch 880, val loss: 1.1612883806228638
Epoch 890, training loss: 0.6501637101173401 = 0.007723329588770866 + 0.1 * 6.424403667449951
Epoch 890, val loss: 1.1666204929351807
Epoch 900, training loss: 0.6494080424308777 = 0.007516864687204361 + 0.1 * 6.418911457061768
Epoch 900, val loss: 1.1713844537734985
Epoch 910, training loss: 0.6495537161827087 = 0.007319449447095394 + 0.1 * 6.422342300415039
Epoch 910, val loss: 1.1764596700668335
Epoch 920, training loss: 0.6477716565132141 = 0.007130573969334364 + 0.1 * 6.4064106941223145
Epoch 920, val loss: 1.1812974214553833
Epoch 930, training loss: 0.64693683385849 = 0.0069501688703894615 + 0.1 * 6.399867057800293
Epoch 930, val loss: 1.186253309249878
Epoch 940, training loss: 0.6483063697814941 = 0.006776814814656973 + 0.1 * 6.415295600891113
Epoch 940, val loss: 1.1908612251281738
Epoch 950, training loss: 0.6472599506378174 = 0.006610569544136524 + 0.1 * 6.406493663787842
Epoch 950, val loss: 1.1955238580703735
Epoch 960, training loss: 0.6475526690483093 = 0.006451602093875408 + 0.1 * 6.411010265350342
Epoch 960, val loss: 1.200109601020813
Epoch 970, training loss: 0.645824134349823 = 0.006298809312283993 + 0.1 * 6.3952531814575195
Epoch 970, val loss: 1.2046663761138916
Epoch 980, training loss: 0.6470144987106323 = 0.0061518121510744095 + 0.1 * 6.408627033233643
Epoch 980, val loss: 1.2091835737228394
Epoch 990, training loss: 0.6450347900390625 = 0.006010634358972311 + 0.1 * 6.3902411460876465
Epoch 990, val loss: 1.213383436203003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.8253073692321777 = 1.9656237363815308 + 0.1 * 8.59683609008789
Epoch 0, val loss: 1.9652303457260132
Epoch 10, training loss: 2.8143763542175293 = 1.9547042846679688 + 0.1 * 8.596721649169922
Epoch 10, val loss: 1.9547903537750244
Epoch 20, training loss: 2.8007426261901855 = 1.9411671161651611 + 0.1 * 8.59575366973877
Epoch 20, val loss: 1.941366195678711
Epoch 30, training loss: 2.7806556224823 = 1.9220420122146606 + 0.1 * 8.586135864257812
Epoch 30, val loss: 1.9220515489578247
Epoch 40, training loss: 2.7466163635253906 = 1.8935014009475708 + 0.1 * 8.531149864196777
Epoch 40, val loss: 1.8932723999023438
Epoch 50, training loss: 2.6824898719787598 = 1.854011058807373 + 0.1 * 8.284788131713867
Epoch 50, val loss: 1.8553522825241089
Epoch 60, training loss: 2.6170241832733154 = 1.808847188949585 + 0.1 * 8.081768989562988
Epoch 60, val loss: 1.8160420656204224
Epoch 70, training loss: 2.5647993087768555 = 1.7701438665390015 + 0.1 * 7.9465556144714355
Epoch 70, val loss: 1.784825325012207
Epoch 80, training loss: 2.49619460105896 = 1.7304010391235352 + 0.1 * 7.657936096191406
Epoch 80, val loss: 1.7481356859207153
Epoch 90, training loss: 2.4124789237976074 = 1.680598497390747 + 0.1 * 7.318803310394287
Epoch 90, val loss: 1.7012280225753784
Epoch 100, training loss: 2.33095645904541 = 1.616963267326355 + 0.1 * 7.139933109283447
Epoch 100, val loss: 1.644444465637207
Epoch 110, training loss: 2.243351936340332 = 1.5386912822723389 + 0.1 * 7.046606540679932
Epoch 110, val loss: 1.578633189201355
Epoch 120, training loss: 2.151761293411255 = 1.456438660621643 + 0.1 * 6.953225612640381
Epoch 120, val loss: 1.5110031366348267
Epoch 130, training loss: 2.0670132637023926 = 1.3769221305847168 + 0.1 * 6.900909900665283
Epoch 130, val loss: 1.4467253684997559
Epoch 140, training loss: 1.986583948135376 = 1.298963189125061 + 0.1 * 6.8762078285217285
Epoch 140, val loss: 1.3850237131118774
Epoch 150, training loss: 1.9055060148239136 = 1.219398021697998 + 0.1 * 6.861079692840576
Epoch 150, val loss: 1.3235445022583008
Epoch 160, training loss: 1.8223371505737305 = 1.1375632286071777 + 0.1 * 6.847738265991211
Epoch 160, val loss: 1.2603005170822144
Epoch 170, training loss: 1.737541675567627 = 1.0538456439971924 + 0.1 * 6.836960792541504
Epoch 170, val loss: 1.1952263116836548
Epoch 180, training loss: 1.6533143520355225 = 0.970447838306427 + 0.1 * 6.828664302825928
Epoch 180, val loss: 1.1301679611206055
Epoch 190, training loss: 1.5715467929840088 = 0.8894670009613037 + 0.1 * 6.820796966552734
Epoch 190, val loss: 1.0660884380340576
Epoch 200, training loss: 1.4943978786468506 = 0.8126872777938843 + 0.1 * 6.817105770111084
Epoch 200, val loss: 1.0043975114822388
Epoch 210, training loss: 1.4232492446899414 = 0.7423308491706848 + 0.1 * 6.809183597564697
Epoch 210, val loss: 0.9482350945472717
Epoch 220, training loss: 1.3588318824768066 = 0.6786491870880127 + 0.1 * 6.801826000213623
Epoch 220, val loss: 0.8994027376174927
Epoch 230, training loss: 1.3008360862731934 = 0.6212737560272217 + 0.1 * 6.795622825622559
Epoch 230, val loss: 0.8584632873535156
Epoch 240, training loss: 1.2494221925735474 = 0.5702187418937683 + 0.1 * 6.79203462600708
Epoch 240, val loss: 0.8257874846458435
Epoch 250, training loss: 1.203464388847351 = 0.5248107314109802 + 0.1 * 6.786536693572998
Epoch 250, val loss: 0.8005238175392151
Epoch 260, training loss: 1.162027359008789 = 0.4838627576828003 + 0.1 * 6.781645774841309
Epoch 260, val loss: 0.7813093662261963
Epoch 270, training loss: 1.124828577041626 = 0.4462372958660126 + 0.1 * 6.78591251373291
Epoch 270, val loss: 0.766900897026062
Epoch 280, training loss: 1.0886353254318237 = 0.4112374484539032 + 0.1 * 6.773978233337402
Epoch 280, val loss: 0.7560263872146606
Epoch 290, training loss: 1.0550395250320435 = 0.3780154585838318 + 0.1 * 6.770240783691406
Epoch 290, val loss: 0.7474719882011414
Epoch 300, training loss: 1.0232021808624268 = 0.34628117084503174 + 0.1 * 6.7692108154296875
Epoch 300, val loss: 0.7408870458602905
Epoch 310, training loss: 0.9927343130111694 = 0.31638818979263306 + 0.1 * 6.763461112976074
Epoch 310, val loss: 0.7362231016159058
Epoch 320, training loss: 0.9642452597618103 = 0.28850317001342773 + 0.1 * 6.757420539855957
Epoch 320, val loss: 0.7335059642791748
Epoch 330, training loss: 0.937971830368042 = 0.26264795660972595 + 0.1 * 6.753238201141357
Epoch 330, val loss: 0.7326999306678772
Epoch 340, training loss: 0.9133245944976807 = 0.23878724873065948 + 0.1 * 6.745373725891113
Epoch 340, val loss: 0.7338207960128784
Epoch 350, training loss: 0.891726553440094 = 0.21682746708393097 + 0.1 * 6.748991012573242
Epoch 350, val loss: 0.7367259860038757
Epoch 360, training loss: 0.8702738285064697 = 0.19674524664878845 + 0.1 * 6.73528528213501
Epoch 360, val loss: 0.7414601445198059
Epoch 370, training loss: 0.8510246872901917 = 0.1782493144273758 + 0.1 * 6.727753639221191
Epoch 370, val loss: 0.7478154897689819
Epoch 380, training loss: 0.8353539109230042 = 0.1612551361322403 + 0.1 * 6.740987777709961
Epoch 380, val loss: 0.7557098865509033
Epoch 390, training loss: 0.8182999491691589 = 0.14588706195354462 + 0.1 * 6.724128723144531
Epoch 390, val loss: 0.764719545841217
Epoch 400, training loss: 0.8030545115470886 = 0.13197042047977448 + 0.1 * 6.710840702056885
Epoch 400, val loss: 0.7745730876922607
Epoch 410, training loss: 0.7896730899810791 = 0.11936431378126144 + 0.1 * 6.703087329864502
Epoch 410, val loss: 0.7851594090461731
Epoch 420, training loss: 0.7775978446006775 = 0.10797242820262909 + 0.1 * 6.696253776550293
Epoch 420, val loss: 0.7962986826896667
Epoch 430, training loss: 0.7667791843414307 = 0.09775123000144958 + 0.1 * 6.690279483795166
Epoch 430, val loss: 0.8077946305274963
Epoch 440, training loss: 0.7583389282226562 = 0.08867843449115753 + 0.1 * 6.6966047286987305
Epoch 440, val loss: 0.8191738724708557
Epoch 450, training loss: 0.7487107515335083 = 0.08058610558509827 + 0.1 * 6.681246757507324
Epoch 450, val loss: 0.8306436538696289
Epoch 460, training loss: 0.7411820292472839 = 0.07335805147886276 + 0.1 * 6.678239345550537
Epoch 460, val loss: 0.8422726392745972
Epoch 470, training loss: 0.7344266176223755 = 0.0669165551662445 + 0.1 * 6.675100326538086
Epoch 470, val loss: 0.8538877964019775
Epoch 480, training loss: 0.7283872961997986 = 0.06118657439947128 + 0.1 * 6.6720075607299805
Epoch 480, val loss: 0.8652604818344116
Epoch 490, training loss: 0.7241148352622986 = 0.05607496574521065 + 0.1 * 6.680398464202881
Epoch 490, val loss: 0.8764954209327698
Epoch 500, training loss: 0.7178739309310913 = 0.05152643099427223 + 0.1 * 6.663475036621094
Epoch 500, val loss: 0.8874285221099854
Epoch 510, training loss: 0.713325023651123 = 0.04745440557599068 + 0.1 * 6.658706188201904
Epoch 510, val loss: 0.8981917500495911
Epoch 520, training loss: 0.709572434425354 = 0.04379602149128914 + 0.1 * 6.657763957977295
Epoch 520, val loss: 0.9087845683097839
Epoch 530, training loss: 0.7062456607818604 = 0.04050742834806442 + 0.1 * 6.657382011413574
Epoch 530, val loss: 0.9191576242446899
Epoch 540, training loss: 0.7027899026870728 = 0.03755493834614754 + 0.1 * 6.652349948883057
Epoch 540, val loss: 0.9291849136352539
Epoch 550, training loss: 0.698996901512146 = 0.03488890081644058 + 0.1 * 6.641079902648926
Epoch 550, val loss: 0.9389526844024658
Epoch 560, training loss: 0.6971862316131592 = 0.03247758746147156 + 0.1 * 6.6470866203308105
Epoch 560, val loss: 0.9485149383544922
Epoch 570, training loss: 0.6941508650779724 = 0.030302297323942184 + 0.1 * 6.638485431671143
Epoch 570, val loss: 0.9578045010566711
Epoch 580, training loss: 0.6912811994552612 = 0.028332402929663658 + 0.1 * 6.629487991333008
Epoch 580, val loss: 0.9667046666145325
Epoch 590, training loss: 0.6904180645942688 = 0.026539303362369537 + 0.1 * 6.638787269592285
Epoch 590, val loss: 0.975463330745697
Epoch 600, training loss: 0.6888732314109802 = 0.024911193177103996 + 0.1 * 6.639620780944824
Epoch 600, val loss: 0.9839658141136169
Epoch 610, training loss: 0.6850403547286987 = 0.023430675268173218 + 0.1 * 6.6160969734191895
Epoch 610, val loss: 0.992184579372406
Epoch 620, training loss: 0.6824986934661865 = 0.022078054025769234 + 0.1 * 6.604206562042236
Epoch 620, val loss: 1.0001271963119507
Epoch 630, training loss: 0.6836140751838684 = 0.020837903022766113 + 0.1 * 6.627761363983154
Epoch 630, val loss: 1.007933497428894
Epoch 640, training loss: 0.679638683795929 = 0.019703315570950508 + 0.1 * 6.599353313446045
Epoch 640, val loss: 1.0154892206192017
Epoch 650, training loss: 0.6790722012519836 = 0.018660379573702812 + 0.1 * 6.604118347167969
Epoch 650, val loss: 1.0228909254074097
Epoch 660, training loss: 0.6762953400611877 = 0.01770184002816677 + 0.1 * 6.585934638977051
Epoch 660, val loss: 1.0300754308700562
Epoch 670, training loss: 0.6755598783493042 = 0.016818981617689133 + 0.1 * 6.587408542633057
Epoch 670, val loss: 1.037004828453064
Epoch 680, training loss: 0.6742033958435059 = 0.016001548618078232 + 0.1 * 6.582018852233887
Epoch 680, val loss: 1.0439199209213257
Epoch 690, training loss: 0.6723817586898804 = 0.015246315859258175 + 0.1 * 6.571354389190674
Epoch 690, val loss: 1.050523281097412
Epoch 700, training loss: 0.6719269752502441 = 0.014546447433531284 + 0.1 * 6.57380485534668
Epoch 700, val loss: 1.0570493936538696
Epoch 710, training loss: 0.6698055267333984 = 0.013898436911404133 + 0.1 * 6.559071063995361
Epoch 710, val loss: 1.0632566213607788
Epoch 720, training loss: 0.6698230504989624 = 0.013295088894665241 + 0.1 * 6.565279483795166
Epoch 720, val loss: 1.069489598274231
Epoch 730, training loss: 0.6682083010673523 = 0.012732520699501038 + 0.1 * 6.554758071899414
Epoch 730, val loss: 1.0756038427352905
Epoch 740, training loss: 0.6670356392860413 = 0.012209158390760422 + 0.1 * 6.548264503479004
Epoch 740, val loss: 1.0813119411468506
Epoch 750, training loss: 0.6656193137168884 = 0.01171900425106287 + 0.1 * 6.539003372192383
Epoch 750, val loss: 1.0871319770812988
Epoch 760, training loss: 0.6645190119743347 = 0.011259141378104687 + 0.1 * 6.532598495483398
Epoch 760, val loss: 1.0929183959960938
Epoch 770, training loss: 0.664036214351654 = 0.010828510858118534 + 0.1 * 6.532077312469482
Epoch 770, val loss: 1.0983396768569946
Epoch 780, training loss: 0.664017915725708 = 0.010424276813864708 + 0.1 * 6.53593635559082
Epoch 780, val loss: 1.1038910150527954
Epoch 790, training loss: 0.6623823046684265 = 0.01004436518996954 + 0.1 * 6.523379802703857
Epoch 790, val loss: 1.1091982126235962
Epoch 800, training loss: 0.6622023582458496 = 0.009686178527772427 + 0.1 * 6.5251617431640625
Epoch 800, val loss: 1.114556074142456
Epoch 810, training loss: 0.6606693267822266 = 0.009348681196570396 + 0.1 * 6.5132060050964355
Epoch 810, val loss: 1.1196937561035156
Epoch 820, training loss: 0.6614407300949097 = 0.009030256420373917 + 0.1 * 6.524104595184326
Epoch 820, val loss: 1.124807596206665
Epoch 830, training loss: 0.6599489450454712 = 0.008729218505322933 + 0.1 * 6.512197017669678
Epoch 830, val loss: 1.129705786705017
Epoch 840, training loss: 0.6602870225906372 = 0.008444547653198242 + 0.1 * 6.5184245109558105
Epoch 840, val loss: 1.1346803903579712
Epoch 850, training loss: 0.6584296822547913 = 0.008175762370228767 + 0.1 * 6.502538681030273
Epoch 850, val loss: 1.1394705772399902
Epoch 860, training loss: 0.6589977145195007 = 0.007921934127807617 + 0.1 * 6.510757923126221
Epoch 860, val loss: 1.1440560817718506
Epoch 870, training loss: 0.6571959853172302 = 0.007679886184632778 + 0.1 * 6.495161056518555
Epoch 870, val loss: 1.1486395597457886
Epoch 880, training loss: 0.6568245887756348 = 0.007449647877365351 + 0.1 * 6.493749618530273
Epoch 880, val loss: 1.1532115936279297
Epoch 890, training loss: 0.6571868658065796 = 0.0072308932431042194 + 0.1 * 6.4995598793029785
Epoch 890, val loss: 1.1576417684555054
Epoch 900, training loss: 0.6553624868392944 = 0.007023239973932505 + 0.1 * 6.483392715454102
Epoch 900, val loss: 1.1620118618011475
Epoch 910, training loss: 0.6555662751197815 = 0.006825268734246492 + 0.1 * 6.487410068511963
Epoch 910, val loss: 1.1664003133773804
Epoch 920, training loss: 0.6557614207267761 = 0.006637736223638058 + 0.1 * 6.491236686706543
Epoch 920, val loss: 1.1705329418182373
Epoch 930, training loss: 0.6541012525558472 = 0.006458999589085579 + 0.1 * 6.476422309875488
Epoch 930, val loss: 1.1745644807815552
Epoch 940, training loss: 0.6557725071907043 = 0.006287350784987211 + 0.1 * 6.494851589202881
Epoch 940, val loss: 1.1786469221115112
Epoch 950, training loss: 0.6545299887657166 = 0.006122807040810585 + 0.1 * 6.484071731567383
Epoch 950, val loss: 1.182633876800537
Epoch 960, training loss: 0.6536785364151001 = 0.0059664687141776085 + 0.1 * 6.477120399475098
Epoch 960, val loss: 1.1864949464797974
Epoch 970, training loss: 0.6533958315849304 = 0.005816198419779539 + 0.1 * 6.475796222686768
Epoch 970, val loss: 1.190346121788025
Epoch 980, training loss: 0.6532870531082153 = 0.0056725372560322285 + 0.1 * 6.476145267486572
Epoch 980, val loss: 1.1940944194793701
Epoch 990, training loss: 0.6514555215835571 = 0.0055351522751152515 + 0.1 * 6.459203720092773
Epoch 990, val loss: 1.197773814201355
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8334211913547708
The final CL Acc:0.80988, 0.00761, The final GNN Acc:0.83676, 0.00280
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10532])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8067848682403564 = 1.9471025466918945 + 0.1 * 8.596822738647461
Epoch 0, val loss: 1.9482089281082153
Epoch 10, training loss: 2.7965080738067627 = 1.936837077140808 + 0.1 * 8.596710205078125
Epoch 10, val loss: 1.9379103183746338
Epoch 20, training loss: 2.7836291790008545 = 1.9240361452102661 + 0.1 * 8.595930099487305
Epoch 20, val loss: 1.924850583076477
Epoch 30, training loss: 2.764896869659424 = 1.9060248136520386 + 0.1 * 8.58872127532959
Epoch 30, val loss: 1.9062265157699585
Epoch 40, training loss: 2.7338688373565674 = 1.8798216581344604 + 0.1 * 8.540472030639648
Epoch 40, val loss: 1.879504680633545
Epoch 50, training loss: 2.6733806133270264 = 1.8454781770706177 + 0.1 * 8.279024124145508
Epoch 50, val loss: 1.8462390899658203
Epoch 60, training loss: 2.6124892234802246 = 1.8092858791351318 + 0.1 * 8.032033920288086
Epoch 60, val loss: 1.8136781454086304
Epoch 70, training loss: 2.55157470703125 = 1.778565526008606 + 0.1 * 7.730090618133545
Epoch 70, val loss: 1.7877000570297241
Epoch 80, training loss: 2.4808669090270996 = 1.7480390071868896 + 0.1 * 7.328278541564941
Epoch 80, val loss: 1.7594815492630005
Epoch 90, training loss: 2.417283058166504 = 1.7099432945251465 + 0.1 * 7.073397159576416
Epoch 90, val loss: 1.7248940467834473
Epoch 100, training loss: 2.3550567626953125 = 1.6579134464263916 + 0.1 * 6.971432685852051
Epoch 100, val loss: 1.6798876523971558
Epoch 110, training loss: 2.2809481620788574 = 1.5885064601898193 + 0.1 * 6.924417972564697
Epoch 110, val loss: 1.6196976900100708
Epoch 120, training loss: 2.1947453022003174 = 1.5061372518539429 + 0.1 * 6.886080265045166
Epoch 120, val loss: 1.5490691661834717
Epoch 130, training loss: 2.102814197540283 = 1.4178012609481812 + 0.1 * 6.850128650665283
Epoch 130, val loss: 1.4752357006072998
Epoch 140, training loss: 2.0108935832977295 = 1.3283822536468506 + 0.1 * 6.825113773345947
Epoch 140, val loss: 1.4027817249298096
Epoch 150, training loss: 1.9229254722595215 = 1.2425580024719238 + 0.1 * 6.803674697875977
Epoch 150, val loss: 1.3357698917388916
Epoch 160, training loss: 1.8387445211410522 = 1.1598021984100342 + 0.1 * 6.789422988891602
Epoch 160, val loss: 1.2739133834838867
Epoch 170, training loss: 1.757305383682251 = 1.0793955326080322 + 0.1 * 6.779097557067871
Epoch 170, val loss: 1.2159779071807861
Epoch 180, training loss: 1.6775579452514648 = 1.0003865957260132 + 0.1 * 6.771714210510254
Epoch 180, val loss: 1.159857153892517
Epoch 190, training loss: 1.5989372730255127 = 0.9223995804786682 + 0.1 * 6.765376091003418
Epoch 190, val loss: 1.1042479276657104
Epoch 200, training loss: 1.5233304500579834 = 0.8473950624465942 + 0.1 * 6.759354591369629
Epoch 200, val loss: 1.0506528615951538
Epoch 210, training loss: 1.4511675834655762 = 0.7762136459350586 + 0.1 * 6.749539852142334
Epoch 210, val loss: 0.9996012449264526
Epoch 220, training loss: 1.383347511291504 = 0.7091761827468872 + 0.1 * 6.741714000701904
Epoch 220, val loss: 0.9521486759185791
Epoch 230, training loss: 1.3224228620529175 = 0.6476187109947205 + 0.1 * 6.748041152954102
Epoch 230, val loss: 0.9099776148796082
Epoch 240, training loss: 1.2644641399383545 = 0.5914627909660339 + 0.1 * 6.730012893676758
Epoch 240, val loss: 0.8733530640602112
Epoch 250, training loss: 1.2118220329284668 = 0.5392745733261108 + 0.1 * 6.725474834442139
Epoch 250, val loss: 0.8418425917625427
Epoch 260, training loss: 1.1632802486419678 = 0.49016815423965454 + 0.1 * 6.731120586395264
Epoch 260, val loss: 0.8148022294044495
Epoch 270, training loss: 1.1156396865844727 = 0.4438416063785553 + 0.1 * 6.717980861663818
Epoch 270, val loss: 0.79184889793396
Epoch 280, training loss: 1.071428894996643 = 0.3995573818683624 + 0.1 * 6.718714714050293
Epoch 280, val loss: 0.772290825843811
Epoch 290, training loss: 1.028043270111084 = 0.3575144112110138 + 0.1 * 6.705288410186768
Epoch 290, val loss: 0.7557841539382935
Epoch 300, training loss: 0.9880713224411011 = 0.31783249974250793 + 0.1 * 6.702387809753418
Epoch 300, val loss: 0.7420247793197632
Epoch 310, training loss: 0.9504519104957581 = 0.2810738682746887 + 0.1 * 6.693780422210693
Epoch 310, val loss: 0.7310279607772827
Epoch 320, training loss: 0.9172115325927734 = 0.24770578742027283 + 0.1 * 6.695056915283203
Epoch 320, val loss: 0.7227624654769897
Epoch 330, training loss: 0.8875336647033691 = 0.2181713879108429 + 0.1 * 6.693622589111328
Epoch 330, val loss: 0.7171016931533813
Epoch 340, training loss: 0.8607447147369385 = 0.19236336648464203 + 0.1 * 6.683813095092773
Epoch 340, val loss: 0.7137855887413025
Epoch 350, training loss: 0.8370060324668884 = 0.1698252409696579 + 0.1 * 6.671807765960693
Epoch 350, val loss: 0.7126249074935913
Epoch 360, training loss: 0.8169956207275391 = 0.15027371048927307 + 0.1 * 6.6672186851501465
Epoch 360, val loss: 0.7133917212486267
Epoch 370, training loss: 0.7996286749839783 = 0.13345612585544586 + 0.1 * 6.6617255210876465
Epoch 370, val loss: 0.715485155582428
Epoch 380, training loss: 0.7849088907241821 = 0.11891216784715652 + 0.1 * 6.659966945648193
Epoch 380, val loss: 0.7189932465553284
Epoch 390, training loss: 0.7717155814170837 = 0.10637999325990677 + 0.1 * 6.653356075286865
Epoch 390, val loss: 0.7234266996383667
Epoch 400, training loss: 0.7595200538635254 = 0.09553271532058716 + 0.1 * 6.639873504638672
Epoch 400, val loss: 0.7286940217018127
Epoch 410, training loss: 0.7497918009757996 = 0.08608951419591904 + 0.1 * 6.637022972106934
Epoch 410, val loss: 0.7346503734588623
Epoch 420, training loss: 0.7413537502288818 = 0.07788752019405365 + 0.1 * 6.63466215133667
Epoch 420, val loss: 0.7411028742790222
Epoch 430, training loss: 0.7330828905105591 = 0.07074370235204697 + 0.1 * 6.623392105102539
Epoch 430, val loss: 0.7477279305458069
Epoch 440, training loss: 0.7264111042022705 = 0.06445605307817459 + 0.1 * 6.619550704956055
Epoch 440, val loss: 0.7547264099121094
Epoch 450, training loss: 0.7212387919425964 = 0.05892239883542061 + 0.1 * 6.62316370010376
Epoch 450, val loss: 0.7618657946586609
Epoch 460, training loss: 0.7144381403923035 = 0.05403232201933861 + 0.1 * 6.604057788848877
Epoch 460, val loss: 0.7690857648849487
Epoch 470, training loss: 0.7110433578491211 = 0.049685340374708176 + 0.1 * 6.613579750061035
Epoch 470, val loss: 0.7765071392059326
Epoch 480, training loss: 0.7053465843200684 = 0.045818865299224854 + 0.1 * 6.595277309417725
Epoch 480, val loss: 0.7837477922439575
Epoch 490, training loss: 0.7016128897666931 = 0.04235479608178139 + 0.1 * 6.592580795288086
Epoch 490, val loss: 0.7911381125450134
Epoch 500, training loss: 0.6982588171958923 = 0.039243970066308975 + 0.1 * 6.590147972106934
Epoch 500, val loss: 0.7983904480934143
Epoch 510, training loss: 0.6952091455459595 = 0.03645314276218414 + 0.1 * 6.587560176849365
Epoch 510, val loss: 0.8057374954223633
Epoch 520, training loss: 0.6918202042579651 = 0.03395221382379532 + 0.1 * 6.57867956161499
Epoch 520, val loss: 0.8126143217086792
Epoch 530, training loss: 0.6884103417396545 = 0.03168858215212822 + 0.1 * 6.5672173500061035
Epoch 530, val loss: 0.81966233253479
Epoch 540, training loss: 0.6883829832077026 = 0.029632018879055977 + 0.1 * 6.587509632110596
Epoch 540, val loss: 0.8265940546989441
Epoch 550, training loss: 0.6844336986541748 = 0.027772700414061546 + 0.1 * 6.5666093826293945
Epoch 550, val loss: 0.8334012031555176
Epoch 560, training loss: 0.681958794593811 = 0.026082664728164673 + 0.1 * 6.558761119842529
Epoch 560, val loss: 0.8399409055709839
Epoch 570, training loss: 0.680221438407898 = 0.02454039268195629 + 0.1 * 6.5568108558654785
Epoch 570, val loss: 0.8466030955314636
Epoch 580, training loss: 0.6773932576179504 = 0.023134609684348106 + 0.1 * 6.542586326599121
Epoch 580, val loss: 0.8528720140457153
Epoch 590, training loss: 0.6774806976318359 = 0.02184474840760231 + 0.1 * 6.55635929107666
Epoch 590, val loss: 0.8592286705970764
Epoch 600, training loss: 0.6747730374336243 = 0.02066500298678875 + 0.1 * 6.541080474853516
Epoch 600, val loss: 0.8654137849807739
Epoch 610, training loss: 0.6731502413749695 = 0.019581403583288193 + 0.1 * 6.535688400268555
Epoch 610, val loss: 0.8713492155075073
Epoch 620, training loss: 0.6723485589027405 = 0.01858101785182953 + 0.1 * 6.537675380706787
Epoch 620, val loss: 0.8773406147956848
Epoch 630, training loss: 0.6698406338691711 = 0.017659053206443787 + 0.1 * 6.521815299987793
Epoch 630, val loss: 0.8831031322479248
Epoch 640, training loss: 0.6688579320907593 = 0.016808241605758667 + 0.1 * 6.520496368408203
Epoch 640, val loss: 0.8889057636260986
Epoch 650, training loss: 0.6684526205062866 = 0.016022473573684692 + 0.1 * 6.524301528930664
Epoch 650, val loss: 0.8943092226982117
Epoch 660, training loss: 0.6680507659912109 = 0.015292404219508171 + 0.1 * 6.527583599090576
Epoch 660, val loss: 0.89984130859375
Epoch 670, training loss: 0.6660408973693848 = 0.014614874497056007 + 0.1 * 6.514260292053223
Epoch 670, val loss: 0.9051957726478577
Epoch 680, training loss: 0.6651285886764526 = 0.013984122313559055 + 0.1 * 6.511444568634033
Epoch 680, val loss: 0.9103653430938721
Epoch 690, training loss: 0.6636698246002197 = 0.013395285233855247 + 0.1 * 6.502745151519775
Epoch 690, val loss: 0.9156085848808289
Epoch 700, training loss: 0.664512574672699 = 0.012845630757510662 + 0.1 * 6.516669273376465
Epoch 700, val loss: 0.9206392765045166
Epoch 710, training loss: 0.6617943644523621 = 0.012332256883382797 + 0.1 * 6.4946208000183105
Epoch 710, val loss: 0.9255908131599426
Epoch 720, training loss: 0.662539541721344 = 0.011850564740598202 + 0.1 * 6.506889820098877
Epoch 720, val loss: 0.9304237365722656
Epoch 730, training loss: 0.6609532237052917 = 0.011399137787520885 + 0.1 * 6.495540618896484
Epoch 730, val loss: 0.935309648513794
Epoch 740, training loss: 0.6602813601493835 = 0.01097689475864172 + 0.1 * 6.493044376373291
Epoch 740, val loss: 0.9399632811546326
Epoch 750, training loss: 0.6593254804611206 = 0.010580237954854965 + 0.1 * 6.487452507019043
Epoch 750, val loss: 0.9443634748458862
Epoch 760, training loss: 0.6589534282684326 = 0.010205228812992573 + 0.1 * 6.487482070922852
Epoch 760, val loss: 0.9488736987113953
Epoch 770, training loss: 0.6574088931083679 = 0.00985140260308981 + 0.1 * 6.475574970245361
Epoch 770, val loss: 0.9533053636550903
Epoch 780, training loss: 0.6590859889984131 = 0.009517238475382328 + 0.1 * 6.495687007904053
Epoch 780, val loss: 0.9576669335365295
Epoch 790, training loss: 0.6559703946113586 = 0.009202363900840282 + 0.1 * 6.46768045425415
Epoch 790, val loss: 0.9619047045707703
Epoch 800, training loss: 0.6560643911361694 = 0.00890437327325344 + 0.1 * 6.47160005569458
Epoch 800, val loss: 0.9659708738327026
Epoch 810, training loss: 0.6572871804237366 = 0.008621527813374996 + 0.1 * 6.486656665802002
Epoch 810, val loss: 0.9701312184333801
Epoch 820, training loss: 0.6549698114395142 = 0.008353999815881252 + 0.1 * 6.466157913208008
Epoch 820, val loss: 0.9741955995559692
Epoch 830, training loss: 0.657565176486969 = 0.00810012686997652 + 0.1 * 6.494649887084961
Epoch 830, val loss: 0.9780568480491638
Epoch 840, training loss: 0.6542648077011108 = 0.007859410718083382 + 0.1 * 6.464053630828857
Epoch 840, val loss: 0.9820575714111328
Epoch 850, training loss: 0.6535123586654663 = 0.007630784530192614 + 0.1 * 6.458815574645996
Epoch 850, val loss: 0.9856871366500854
Epoch 860, training loss: 0.6531771421432495 = 0.007412625011056662 + 0.1 * 6.457644939422607
Epoch 860, val loss: 0.9894059896469116
Epoch 870, training loss: 0.6527295708656311 = 0.007204587105661631 + 0.1 * 6.455249786376953
Epoch 870, val loss: 0.993109941482544
Epoch 880, training loss: 0.651792585849762 = 0.007006172090768814 + 0.1 * 6.447864055633545
Epoch 880, val loss: 0.9967547655105591
Epoch 890, training loss: 0.6524603962898254 = 0.006816857028752565 + 0.1 * 6.456435203552246
Epoch 890, val loss: 1.0002765655517578
Epoch 900, training loss: 0.6521246433258057 = 0.0066356840543448925 + 0.1 * 6.45488977432251
Epoch 900, val loss: 1.0039002895355225
Epoch 910, training loss: 0.6529898047447205 = 0.006463289726525545 + 0.1 * 6.465264797210693
Epoch 910, val loss: 1.0074620246887207
Epoch 920, training loss: 0.6508470773696899 = 0.006299225613474846 + 0.1 * 6.445478439331055
Epoch 920, val loss: 1.0107836723327637
Epoch 930, training loss: 0.6501256227493286 = 0.006141951307654381 + 0.1 * 6.439836502075195
Epoch 930, val loss: 1.013993740081787
Epoch 940, training loss: 0.6505042314529419 = 0.005990705918520689 + 0.1 * 6.445135593414307
Epoch 940, val loss: 1.0173699855804443
Epoch 950, training loss: 0.64968341588974 = 0.005846088752150536 + 0.1 * 6.43837308883667
Epoch 950, val loss: 1.0206431150436401
Epoch 960, training loss: 0.6502522230148315 = 0.005707471165806055 + 0.1 * 6.44544792175293
Epoch 960, val loss: 1.0237783193588257
Epoch 970, training loss: 0.6490544676780701 = 0.00557421101257205 + 0.1 * 6.434802532196045
Epoch 970, val loss: 1.0269594192504883
Epoch 980, training loss: 0.6497154235839844 = 0.005446313414722681 + 0.1 * 6.442690849304199
Epoch 980, val loss: 1.0299946069717407
Epoch 990, training loss: 0.6486429572105408 = 0.005323249846696854 + 0.1 * 6.433197021484375
Epoch 990, val loss: 1.033176064491272
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 2.7931883335113525 = 1.9335048198699951 + 0.1 * 8.596835136413574
Epoch 0, val loss: 1.9274367094039917
Epoch 10, training loss: 2.7839956283569336 = 1.9243173599243164 + 0.1 * 8.596783638000488
Epoch 10, val loss: 1.9181314706802368
Epoch 20, training loss: 2.7732152938842773 = 1.9135807752609253 + 0.1 * 8.596344947814941
Epoch 20, val loss: 1.9072363376617432
Epoch 30, training loss: 2.7581920623779297 = 1.8989328145980835 + 0.1 * 8.592592239379883
Epoch 30, val loss: 1.8925021886825562
Epoch 40, training loss: 2.733867645263672 = 1.8774135112762451 + 0.1 * 8.564539909362793
Epoch 40, val loss: 1.8711750507354736
Epoch 50, training loss: 2.688441276550293 = 1.847139596939087 + 0.1 * 8.413016319274902
Epoch 50, val loss: 1.8424241542816162
Epoch 60, training loss: 2.6194677352905273 = 1.8129825592041016 + 0.1 * 8.064851760864258
Epoch 60, val loss: 1.8122695684432983
Epoch 70, training loss: 2.5701632499694824 = 1.7814656496047974 + 0.1 * 7.88697624206543
Epoch 70, val loss: 1.7855249643325806
Epoch 80, training loss: 2.509906053543091 = 1.7496681213378906 + 0.1 * 7.602378845214844
Epoch 80, val loss: 1.7563247680664062
Epoch 90, training loss: 2.4427008628845215 = 1.7095979452133179 + 0.1 * 7.331028461456299
Epoch 90, val loss: 1.7175122499465942
Epoch 100, training loss: 2.370290517807007 = 1.6553510427474976 + 0.1 * 7.149394989013672
Epoch 100, val loss: 1.6664180755615234
Epoch 110, training loss: 2.286618709564209 = 1.5815460681915283 + 0.1 * 7.050726413726807
Epoch 110, val loss: 1.599837303161621
Epoch 120, training loss: 2.188821792602539 = 1.490816354751587 + 0.1 * 6.980055332183838
Epoch 120, val loss: 1.5183658599853516
Epoch 130, training loss: 2.08540940284729 = 1.391814112663269 + 0.1 * 6.935953617095947
Epoch 130, val loss: 1.4318315982818604
Epoch 140, training loss: 1.9809527397155762 = 1.2905365228652954 + 0.1 * 6.9041619300842285
Epoch 140, val loss: 1.3449487686157227
Epoch 150, training loss: 1.8790156841278076 = 1.1909153461456299 + 0.1 * 6.881002902984619
Epoch 150, val loss: 1.2618329524993896
Epoch 160, training loss: 1.7838196754455566 = 1.0968923568725586 + 0.1 * 6.8692731857299805
Epoch 160, val loss: 1.1855987310409546
Epoch 170, training loss: 1.6949090957641602 = 1.009357213973999 + 0.1 * 6.8555192947387695
Epoch 170, val loss: 1.1163285970687866
Epoch 180, training loss: 1.6109609603881836 = 0.9267091155052185 + 0.1 * 6.8425188064575195
Epoch 180, val loss: 1.0529875755310059
Epoch 190, training loss: 1.5304436683654785 = 0.847213089466095 + 0.1 * 6.832304954528809
Epoch 190, val loss: 0.9950918555259705
Epoch 200, training loss: 1.4531402587890625 = 0.7709783315658569 + 0.1 * 6.821619987487793
Epoch 200, val loss: 0.9431942105293274
Epoch 210, training loss: 1.3803858757019043 = 0.6990008354187012 + 0.1 * 6.813849449157715
Epoch 210, val loss: 0.8984616994857788
Epoch 220, training loss: 1.3129477500915527 = 0.6320610642433167 + 0.1 * 6.808867454528809
Epoch 220, val loss: 0.8608769178390503
Epoch 230, training loss: 1.2515102624893188 = 0.571441113948822 + 0.1 * 6.800691604614258
Epoch 230, val loss: 0.8308398723602295
Epoch 240, training loss: 1.1963043212890625 = 0.5170326232910156 + 0.1 * 6.792716026306152
Epoch 240, val loss: 0.807246744632721
Epoch 250, training loss: 1.1469846963882446 = 0.46799808740615845 + 0.1 * 6.789865970611572
Epoch 250, val loss: 0.789131224155426
Epoch 260, training loss: 1.102229118347168 = 0.4240163266658783 + 0.1 * 6.78212833404541
Epoch 260, val loss: 0.7758029699325562
Epoch 270, training loss: 1.0616035461425781 = 0.3843846917152405 + 0.1 * 6.772188186645508
Epoch 270, val loss: 0.7664671540260315
Epoch 280, training loss: 1.0255285501480103 = 0.34863629937171936 + 0.1 * 6.768922328948975
Epoch 280, val loss: 0.7604168057441711
Epoch 290, training loss: 0.9914886951446533 = 0.3162575960159302 + 0.1 * 6.752310752868652
Epoch 290, val loss: 0.7572375535964966
Epoch 300, training loss: 0.9618426561355591 = 0.28674712777137756 + 0.1 * 6.750955581665039
Epoch 300, val loss: 0.7565678954124451
Epoch 310, training loss: 0.933587908744812 = 0.2599477171897888 + 0.1 * 6.736401557922363
Epoch 310, val loss: 0.7579442262649536
Epoch 320, training loss: 0.9078856706619263 = 0.23545372486114502 + 0.1 * 6.7243194580078125
Epoch 320, val loss: 0.7612111568450928
Epoch 330, training loss: 0.8859295845031738 = 0.2131803333759308 + 0.1 * 6.727492809295654
Epoch 330, val loss: 0.7662226557731628
Epoch 340, training loss: 0.8640891313552856 = 0.19317391514778137 + 0.1 * 6.709151744842529
Epoch 340, val loss: 0.7725388407707214
Epoch 350, training loss: 0.8455660343170166 = 0.17516516149044037 + 0.1 * 6.70400857925415
Epoch 350, val loss: 0.7800643444061279
Epoch 360, training loss: 0.8286353349685669 = 0.1589566022157669 + 0.1 * 6.696787357330322
Epoch 360, val loss: 0.7886574864387512
Epoch 370, training loss: 0.8147017955780029 = 0.14450326561927795 + 0.1 * 6.7019853591918945
Epoch 370, val loss: 0.7980708479881287
Epoch 380, training loss: 0.8004635572433472 = 0.13166898488998413 + 0.1 * 6.687945365905762
Epoch 380, val loss: 0.808150053024292
Epoch 390, training loss: 0.7885937094688416 = 0.12021013349294662 + 0.1 * 6.683835983276367
Epoch 390, val loss: 0.8188109397888184
Epoch 400, training loss: 0.7789227962493896 = 0.11001671105623245 + 0.1 * 6.689060688018799
Epoch 400, val loss: 0.8298455476760864
Epoch 410, training loss: 0.7679834365844727 = 0.10093146562576294 + 0.1 * 6.670519828796387
Epoch 410, val loss: 0.8412063717842102
Epoch 420, training loss: 0.759125828742981 = 0.09277363866567612 + 0.1 * 6.663522243499756
Epoch 420, val loss: 0.8528745770454407
Epoch 430, training loss: 0.7517290711402893 = 0.08542881160974503 + 0.1 * 6.663002014160156
Epoch 430, val loss: 0.8647421002388
Epoch 440, training loss: 0.7442207336425781 = 0.07883322983980179 + 0.1 * 6.65387487411499
Epoch 440, val loss: 0.8766732811927795
Epoch 450, training loss: 0.7381857633590698 = 0.07291064411401749 + 0.1 * 6.6527509689331055
Epoch 450, val loss: 0.8885601758956909
Epoch 460, training loss: 0.732609748840332 = 0.06755381077528 + 0.1 * 6.650559425354004
Epoch 460, val loss: 0.900486171245575
Epoch 470, training loss: 0.7270497679710388 = 0.06270989030599594 + 0.1 * 6.643398761749268
Epoch 470, val loss: 0.9123038649559021
Epoch 480, training loss: 0.721712589263916 = 0.05832555890083313 + 0.1 * 6.633870601654053
Epoch 480, val loss: 0.9240291714668274
Epoch 490, training loss: 0.7169010639190674 = 0.05434294417500496 + 0.1 * 6.62558126449585
Epoch 490, val loss: 0.9356220364570618
Epoch 500, training loss: 0.7123873233795166 = 0.050718359649181366 + 0.1 * 6.616689205169678
Epoch 500, val loss: 0.9471365809440613
Epoch 510, training loss: 0.709125280380249 = 0.04742656648159027 + 0.1 * 6.6169867515563965
Epoch 510, val loss: 0.9583801031112671
Epoch 520, training loss: 0.7061086893081665 = 0.04441801831126213 + 0.1 * 6.61690616607666
Epoch 520, val loss: 0.9694890975952148
Epoch 530, training loss: 0.7015794515609741 = 0.04167097806930542 + 0.1 * 6.599084377288818
Epoch 530, val loss: 0.9804474711418152
Epoch 540, training loss: 0.7000322937965393 = 0.03915005549788475 + 0.1 * 6.608822345733643
Epoch 540, val loss: 0.9912586212158203
Epoch 550, training loss: 0.6965295672416687 = 0.03684522584080696 + 0.1 * 6.596843242645264
Epoch 550, val loss: 1.0018107891082764
Epoch 560, training loss: 0.6921147108078003 = 0.03472955524921417 + 0.1 * 6.573851585388184
Epoch 560, val loss: 1.0121607780456543
Epoch 570, training loss: 0.6916930675506592 = 0.032781172543764114 + 0.1 * 6.589118957519531
Epoch 570, val loss: 1.0223156213760376
Epoch 580, training loss: 0.6875422596931458 = 0.030990367755293846 + 0.1 * 6.565518856048584
Epoch 580, val loss: 1.0322613716125488
Epoch 590, training loss: 0.6861659288406372 = 0.029335957020521164 + 0.1 * 6.568299293518066
Epoch 590, val loss: 1.04200279712677
Epoch 600, training loss: 0.6859116554260254 = 0.02780837006866932 + 0.1 * 6.581032752990723
Epoch 600, val loss: 1.051499366760254
Epoch 610, training loss: 0.6816912293434143 = 0.026401903480291367 + 0.1 * 6.55289363861084
Epoch 610, val loss: 1.0606783628463745
Epoch 620, training loss: 0.6795820593833923 = 0.025096161291003227 + 0.1 * 6.544858455657959
Epoch 620, val loss: 1.0697472095489502
Epoch 630, training loss: 0.6789743304252625 = 0.023881945759058 + 0.1 * 6.5509233474731445
Epoch 630, val loss: 1.0786590576171875
Epoch 640, training loss: 0.6768624782562256 = 0.022753022611141205 + 0.1 * 6.541094779968262
Epoch 640, val loss: 1.087442398071289
Epoch 650, training loss: 0.6757002472877502 = 0.021704180166125298 + 0.1 * 6.5399603843688965
Epoch 650, val loss: 1.0959501266479492
Epoch 660, training loss: 0.674737811088562 = 0.020725922659039497 + 0.1 * 6.540119171142578
Epoch 660, val loss: 1.1043274402618408
Epoch 670, training loss: 0.6728780269622803 = 0.019814634695649147 + 0.1 * 6.530633926391602
Epoch 670, val loss: 1.1124844551086426
Epoch 680, training loss: 0.673125147819519 = 0.018961474299430847 + 0.1 * 6.5416364669799805
Epoch 680, val loss: 1.120519995689392
Epoch 690, training loss: 0.6701275706291199 = 0.01816481538116932 + 0.1 * 6.519627571105957
Epoch 690, val loss: 1.1283149719238281
Epoch 700, training loss: 0.6695603728294373 = 0.01741885580122471 + 0.1 * 6.5214152336120605
Epoch 700, val loss: 1.1359634399414062
Epoch 710, training loss: 0.6685958504676819 = 0.016721777617931366 + 0.1 * 6.518740653991699
Epoch 710, val loss: 1.1433905363082886
Epoch 720, training loss: 0.6666170358657837 = 0.016065523028373718 + 0.1 * 6.505514621734619
Epoch 720, val loss: 1.150696873664856
Epoch 730, training loss: 0.6680727601051331 = 0.015447272919118404 + 0.1 * 6.526255130767822
Epoch 730, val loss: 1.1578724384307861
Epoch 740, training loss: 0.6657813787460327 = 0.014866957440972328 + 0.1 * 6.509144306182861
Epoch 740, val loss: 1.1649022102355957
Epoch 750, training loss: 0.6645780801773071 = 0.014319006353616714 + 0.1 * 6.502590656280518
Epoch 750, val loss: 1.1717714071273804
Epoch 760, training loss: 0.6633440852165222 = 0.013804198242723942 + 0.1 * 6.495398998260498
Epoch 760, val loss: 1.1784353256225586
Epoch 770, training loss: 0.6619153022766113 = 0.013317341916263103 + 0.1 * 6.4859795570373535
Epoch 770, val loss: 1.1849992275238037
Epoch 780, training loss: 0.664219319820404 = 0.012856586836278439 + 0.1 * 6.513627052307129
Epoch 780, val loss: 1.1914693117141724
Epoch 790, training loss: 0.6613631248474121 = 0.01241987757384777 + 0.1 * 6.489432334899902
Epoch 790, val loss: 1.1977437734603882
Epoch 800, training loss: 0.6604047417640686 = 0.012008748948574066 + 0.1 * 6.483960151672363
Epoch 800, val loss: 1.2039234638214111
Epoch 810, training loss: 0.661772608757019 = 0.011617043055593967 + 0.1 * 6.501555442810059
Epoch 810, val loss: 1.2099583148956299
Epoch 820, training loss: 0.6593233346939087 = 0.011248155497014523 + 0.1 * 6.4807515144348145
Epoch 820, val loss: 1.2158255577087402
Epoch 830, training loss: 0.6582541465759277 = 0.010897312313318253 + 0.1 * 6.473567962646484
Epoch 830, val loss: 1.221630573272705
Epoch 840, training loss: 0.6587718725204468 = 0.010562828741967678 + 0.1 * 6.482089996337891
Epoch 840, val loss: 1.227325439453125
Epoch 850, training loss: 0.6573007702827454 = 0.010244899429380894 + 0.1 * 6.4705586433410645
Epoch 850, val loss: 1.2329331636428833
Epoch 860, training loss: 0.6578186750411987 = 0.009942954406142235 + 0.1 * 6.478756904602051
Epoch 860, val loss: 1.2383853197097778
Epoch 870, training loss: 0.6560349464416504 = 0.009655022993683815 + 0.1 * 6.463799476623535
Epoch 870, val loss: 1.2437748908996582
Epoch 880, training loss: 0.6574435234069824 = 0.009380851872265339 + 0.1 * 6.480626583099365
Epoch 880, val loss: 1.2490589618682861
Epoch 890, training loss: 0.6548139452934265 = 0.009119005873799324 + 0.1 * 6.456948757171631
Epoch 890, val loss: 1.2542228698730469
Epoch 900, training loss: 0.6561635136604309 = 0.008869272656738758 + 0.1 * 6.472941875457764
Epoch 900, val loss: 1.259316086769104
Epoch 910, training loss: 0.654439389705658 = 0.008630283176898956 + 0.1 * 6.4580912590026855
Epoch 910, val loss: 1.264290690422058
Epoch 920, training loss: 0.6537420153617859 = 0.008401804603636265 + 0.1 * 6.453402042388916
Epoch 920, val loss: 1.2692598104476929
Epoch 930, training loss: 0.6544726490974426 = 0.008182982914149761 + 0.1 * 6.462896347045898
Epoch 930, val loss: 1.2740955352783203
Epoch 940, training loss: 0.6528864502906799 = 0.00797340739518404 + 0.1 * 6.449130058288574
Epoch 940, val loss: 1.2788300514221191
Epoch 950, training loss: 0.6538645625114441 = 0.007773310411721468 + 0.1 * 6.460912227630615
Epoch 950, val loss: 1.2835019826889038
Epoch 960, training loss: 0.6525296568870544 = 0.007581158075481653 + 0.1 * 6.449484825134277
Epoch 960, val loss: 1.2880966663360596
Epoch 970, training loss: 0.6537308692932129 = 0.007396867964416742 + 0.1 * 6.463339805603027
Epoch 970, val loss: 1.2926212549209595
Epoch 980, training loss: 0.6519589424133301 = 0.007219988852739334 + 0.1 * 6.447389602661133
Epoch 980, val loss: 1.297028660774231
Epoch 990, training loss: 0.651340663433075 = 0.007050430867820978 + 0.1 * 6.442902088165283
Epoch 990, val loss: 1.301452398300171
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 2.818199396133423 = 1.958518147468567 + 0.1 * 8.596813201904297
Epoch 0, val loss: 1.9640109539031982
Epoch 10, training loss: 2.8076610565185547 = 1.9479910135269165 + 0.1 * 8.596700668334961
Epoch 10, val loss: 1.9536044597625732
Epoch 20, training loss: 2.7948336601257324 = 1.9352481365203857 + 0.1 * 8.595854759216309
Epoch 20, val loss: 1.9403326511383057
Epoch 30, training loss: 2.7764618396759033 = 1.917641520500183 + 0.1 * 8.588202476501465
Epoch 30, val loss: 1.9213758707046509
Epoch 40, training loss: 2.7456626892089844 = 1.891705870628357 + 0.1 * 8.539568901062012
Epoch 40, val loss: 1.8931851387023926
Epoch 50, training loss: 2.6853232383728027 = 1.8565058708190918 + 0.1 * 8.288172721862793
Epoch 50, val loss: 1.856501579284668
Epoch 60, training loss: 2.6183834075927734 = 1.8177907466888428 + 0.1 * 8.005927085876465
Epoch 60, val loss: 1.8195055723190308
Epoch 70, training loss: 2.560981273651123 = 1.7847791910171509 + 0.1 * 7.762020587921143
Epoch 70, val loss: 1.7910130023956299
Epoch 80, training loss: 2.4915783405303955 = 1.7540541887283325 + 0.1 * 7.375241279602051
Epoch 80, val loss: 1.7641682624816895
Epoch 90, training loss: 2.431307792663574 = 1.7167826890945435 + 0.1 * 7.1452507972717285
Epoch 90, val loss: 1.7304272651672363
Epoch 100, training loss: 2.369011878967285 = 1.6664026975631714 + 0.1 * 7.026093006134033
Epoch 100, val loss: 1.6851727962493896
Epoch 110, training loss: 2.294062852859497 = 1.5989725589752197 + 0.1 * 6.950903415679932
Epoch 110, val loss: 1.6263259649276733
Epoch 120, training loss: 2.20896053314209 = 1.5176767110824585 + 0.1 * 6.912837505340576
Epoch 120, val loss: 1.557955026626587
Epoch 130, training loss: 2.119112968444824 = 1.430450439453125 + 0.1 * 6.886624813079834
Epoch 130, val loss: 1.487097144126892
Epoch 140, training loss: 2.028963565826416 = 1.342435598373413 + 0.1 * 6.865280628204346
Epoch 140, val loss: 1.418183445930481
Epoch 150, training loss: 1.9396758079528809 = 1.255164623260498 + 0.1 * 6.84511137008667
Epoch 150, val loss: 1.353548288345337
Epoch 160, training loss: 1.8540105819702148 = 1.171181559562683 + 0.1 * 6.82828950881958
Epoch 160, val loss: 1.2927842140197754
Epoch 170, training loss: 1.775869369506836 = 1.0948597192764282 + 0.1 * 6.810096740722656
Epoch 170, val loss: 1.2399400472640991
Epoch 180, training loss: 1.7069147825241089 = 1.0275260210037231 + 0.1 * 6.793887615203857
Epoch 180, val loss: 1.19527268409729
Epoch 190, training loss: 1.6463309526443481 = 0.9679320454597473 + 0.1 * 6.783988952636719
Epoch 190, val loss: 1.1574550867080688
Epoch 200, training loss: 1.5916824340820312 = 0.9137080311775208 + 0.1 * 6.779744625091553
Epoch 200, val loss: 1.1245405673980713
Epoch 210, training loss: 1.539499282836914 = 0.8623118996620178 + 0.1 * 6.771873474121094
Epoch 210, val loss: 1.094751000404358
Epoch 220, training loss: 1.4883586168289185 = 0.8115969300270081 + 0.1 * 6.7676167488098145
Epoch 220, val loss: 1.0662250518798828
Epoch 230, training loss: 1.4376020431518555 = 0.7611286044120789 + 0.1 * 6.764734268188477
Epoch 230, val loss: 1.0391429662704468
Epoch 240, training loss: 1.3866453170776367 = 0.7108240723609924 + 0.1 * 6.758212566375732
Epoch 240, val loss: 1.0133788585662842
Epoch 250, training loss: 1.3365304470062256 = 0.6609019637107849 + 0.1 * 6.756284236907959
Epoch 250, val loss: 0.988616406917572
Epoch 260, training loss: 1.287097454071045 = 0.6123220920562744 + 0.1 * 6.747753143310547
Epoch 260, val loss: 0.9657648205757141
Epoch 270, training loss: 1.2400038242340088 = 0.565644383430481 + 0.1 * 6.743595123291016
Epoch 270, val loss: 0.9454728960990906
Epoch 280, training loss: 1.1955461502075195 = 0.5214657783508301 + 0.1 * 6.740804195404053
Epoch 280, val loss: 0.928647518157959
Epoch 290, training loss: 1.1553964614868164 = 0.4801256060600281 + 0.1 * 6.7527079582214355
Epoch 290, val loss: 0.9157137870788574
Epoch 300, training loss: 1.1150201559066772 = 0.4416887164115906 + 0.1 * 6.733314037322998
Epoch 300, val loss: 0.9061939120292664
Epoch 310, training loss: 1.0777429342269897 = 0.40504762530326843 + 0.1 * 6.72695255279541
Epoch 310, val loss: 0.8990223407745361
Epoch 320, training loss: 1.0418822765350342 = 0.36941492557525635 + 0.1 * 6.724672794342041
Epoch 320, val loss: 0.893459141254425
Epoch 330, training loss: 1.0070865154266357 = 0.33466920256614685 + 0.1 * 6.724173545837402
Epoch 330, val loss: 0.8890384435653687
Epoch 340, training loss: 0.9732296466827393 = 0.3008635342121124 + 0.1 * 6.723660945892334
Epoch 340, val loss: 0.8853363394737244
Epoch 350, training loss: 0.939994215965271 = 0.2681383192539215 + 0.1 * 6.718559265136719
Epoch 350, val loss: 0.882287323474884
Epoch 360, training loss: 0.9087412357330322 = 0.23714880645275116 + 0.1 * 6.715924263000488
Epoch 360, val loss: 0.8802116513252258
Epoch 370, training loss: 0.879875123500824 = 0.20853187143802643 + 0.1 * 6.713432788848877
Epoch 370, val loss: 0.8795746564865112
Epoch 380, training loss: 0.8538338541984558 = 0.18271732330322266 + 0.1 * 6.711165428161621
Epoch 380, val loss: 0.8806576728820801
Epoch 390, training loss: 0.8331714868545532 = 0.1598786860704422 + 0.1 * 6.732928276062012
Epoch 390, val loss: 0.8836276531219482
Epoch 400, training loss: 0.8115420937538147 = 0.14012181758880615 + 0.1 * 6.714202404022217
Epoch 400, val loss: 0.8881392478942871
Epoch 410, training loss: 0.7936186790466309 = 0.12304681539535522 + 0.1 * 6.705718517303467
Epoch 410, val loss: 0.8941673040390015
Epoch 420, training loss: 0.7783716917037964 = 0.10830023139715195 + 0.1 * 6.700714588165283
Epoch 420, val loss: 0.9014963507652283
Epoch 430, training loss: 0.7656092643737793 = 0.09556911140680313 + 0.1 * 6.700401306152344
Epoch 430, val loss: 0.9098199009895325
Epoch 440, training loss: 0.7549721002578735 = 0.08465451747179031 + 0.1 * 6.703176021575928
Epoch 440, val loss: 0.9187831878662109
Epoch 450, training loss: 0.7446608543395996 = 0.0752829909324646 + 0.1 * 6.6937785148620605
Epoch 450, val loss: 0.9281142950057983
Epoch 460, training loss: 0.7362101078033447 = 0.06718000769615173 + 0.1 * 6.690300464630127
Epoch 460, val loss: 0.9379078149795532
Epoch 470, training loss: 0.7301443219184875 = 0.06016009673476219 + 0.1 * 6.6998419761657715
Epoch 470, val loss: 0.9479601979255676
Epoch 480, training loss: 0.7223865985870361 = 0.05409446358680725 + 0.1 * 6.682920932769775
Epoch 480, val loss: 0.9580441117286682
Epoch 490, training loss: 0.716706395149231 = 0.048824243247509 + 0.1 * 6.678821086883545
Epoch 490, val loss: 0.9681988954544067
Epoch 500, training loss: 0.7130959630012512 = 0.04422594979405403 + 0.1 * 6.688700199127197
Epoch 500, val loss: 0.9783383011817932
Epoch 510, training loss: 0.7077294588088989 = 0.040214747190475464 + 0.1 * 6.67514705657959
Epoch 510, val loss: 0.9883798360824585
Epoch 520, training loss: 0.7033981680870056 = 0.0367094911634922 + 0.1 * 6.666886806488037
Epoch 520, val loss: 0.998163640499115
Epoch 530, training loss: 0.7000061273574829 = 0.033639129251241684 + 0.1 * 6.663670063018799
Epoch 530, val loss: 1.0078229904174805
Epoch 540, training loss: 0.6977161765098572 = 0.030931254848837852 + 0.1 * 6.667849063873291
Epoch 540, val loss: 1.0173664093017578
Epoch 550, training loss: 0.6939851641654968 = 0.028536437079310417 + 0.1 * 6.654487133026123
Epoch 550, val loss: 1.0266515016555786
Epoch 560, training loss: 0.6918466091156006 = 0.026410678401589394 + 0.1 * 6.654358863830566
Epoch 560, val loss: 1.0357115268707275
Epoch 570, training loss: 0.6893154978752136 = 0.02452152408659458 + 0.1 * 6.647939682006836
Epoch 570, val loss: 1.04459547996521
Epoch 580, training loss: 0.6888102889060974 = 0.02283131144940853 + 0.1 * 6.659789562225342
Epoch 580, val loss: 1.0531651973724365
Epoch 590, training loss: 0.6853418350219727 = 0.021324483677744865 + 0.1 * 6.640173435211182
Epoch 590, val loss: 1.0615116357803345
Epoch 600, training loss: 0.6857027411460876 = 0.019968416541814804 + 0.1 * 6.65734338760376
Epoch 600, val loss: 1.0696157217025757
Epoch 610, training loss: 0.6827927827835083 = 0.018748100847005844 + 0.1 * 6.640446662902832
Epoch 610, val loss: 1.0774204730987549
Epoch 620, training loss: 0.6798582077026367 = 0.01764635555446148 + 0.1 * 6.62211799621582
Epoch 620, val loss: 1.0850062370300293
Epoch 630, training loss: 0.6798481941223145 = 0.01664300635457039 + 0.1 * 6.632051944732666
Epoch 630, val loss: 1.0924724340438843
Epoch 640, training loss: 0.6773582696914673 = 0.015730781480669975 + 0.1 * 6.616274833679199
Epoch 640, val loss: 1.099590539932251
Epoch 650, training loss: 0.6761013269424438 = 0.014900166541337967 + 0.1 * 6.612011909484863
Epoch 650, val loss: 1.1065987348556519
Epoch 660, training loss: 0.6752802133560181 = 0.01413919497281313 + 0.1 * 6.611410140991211
Epoch 660, val loss: 1.1133235692977905
Epoch 670, training loss: 0.6741710305213928 = 0.013441373594105244 + 0.1 * 6.607296466827393
Epoch 670, val loss: 1.11996328830719
Epoch 680, training loss: 0.6737245321273804 = 0.012799150310456753 + 0.1 * 6.609253883361816
Epoch 680, val loss: 1.1261178255081177
Epoch 690, training loss: 0.670771062374115 = 0.012212648056447506 + 0.1 * 6.5855841636657715
Epoch 690, val loss: 1.1322979927062988
Epoch 700, training loss: 0.6698008179664612 = 0.011667344719171524 + 0.1 * 6.581334590911865
Epoch 700, val loss: 1.138251781463623
Epoch 710, training loss: 0.668640673160553 = 0.0111624114215374 + 0.1 * 6.574782848358154
Epoch 710, val loss: 1.1440750360488892
Epoch 720, training loss: 0.6681790351867676 = 0.010693255811929703 + 0.1 * 6.574857711791992
Epoch 720, val loss: 1.1496648788452148
Epoch 730, training loss: 0.6667160987854004 = 0.010260093957185745 + 0.1 * 6.5645599365234375
Epoch 730, val loss: 1.1552047729492188
Epoch 740, training loss: 0.6659091114997864 = 0.009854570031166077 + 0.1 * 6.560545444488525
Epoch 740, val loss: 1.1605719327926636
Epoch 750, training loss: 0.6686899662017822 = 0.009474804624915123 + 0.1 * 6.592151641845703
Epoch 750, val loss: 1.1657981872558594
Epoch 760, training loss: 0.6635945439338684 = 0.009121761657297611 + 0.1 * 6.544727802276611
Epoch 760, val loss: 1.1708987951278687
Epoch 770, training loss: 0.6633201241493225 = 0.00879150815308094 + 0.1 * 6.545286178588867
Epoch 770, val loss: 1.1759865283966064
Epoch 780, training loss: 0.6639759540557861 = 0.008480130694806576 + 0.1 * 6.554958343505859
Epoch 780, val loss: 1.1809135675430298
Epoch 790, training loss: 0.6617957949638367 = 0.008187135681509972 + 0.1 * 6.536086559295654
Epoch 790, val loss: 1.1856311559677124
Epoch 800, training loss: 0.6606866717338562 = 0.007912722416222095 + 0.1 * 6.527739524841309
Epoch 800, val loss: 1.1904507875442505
Epoch 810, training loss: 0.6650285720825195 = 0.007652308326214552 + 0.1 * 6.573762893676758
Epoch 810, val loss: 1.1949653625488281
Epoch 820, training loss: 0.6604265570640564 = 0.007408143952488899 + 0.1 * 6.530183792114258
Epoch 820, val loss: 1.1993826627731323
Epoch 830, training loss: 0.659062385559082 = 0.007179811131209135 + 0.1 * 6.518825531005859
Epoch 830, val loss: 1.2039153575897217
Epoch 840, training loss: 0.6584067344665527 = 0.006961182691156864 + 0.1 * 6.514455795288086
Epoch 840, val loss: 1.2082363367080688
Epoch 850, training loss: 0.6583866477012634 = 0.006753318943083286 + 0.1 * 6.516333103179932
Epoch 850, val loss: 1.2124096155166626
Epoch 860, training loss: 0.6575584411621094 = 0.006557549349963665 + 0.1 * 6.510009288787842
Epoch 860, val loss: 1.2166434526443481
Epoch 870, training loss: 0.6568984985351562 = 0.006371621508151293 + 0.1 * 6.505268573760986
Epoch 870, val loss: 1.220752239227295
Epoch 880, training loss: 0.6583325862884521 = 0.006194625981152058 + 0.1 * 6.521378993988037
Epoch 880, val loss: 1.224751591682434
Epoch 890, training loss: 0.6557902097702026 = 0.006025445181876421 + 0.1 * 6.497647762298584
Epoch 890, val loss: 1.2285181283950806
Epoch 900, training loss: 0.6554266214370728 = 0.005866330582648516 + 0.1 * 6.495602607727051
Epoch 900, val loss: 1.2324466705322266
Epoch 910, training loss: 0.6543674468994141 = 0.005714050494134426 + 0.1 * 6.4865336418151855
Epoch 910, val loss: 1.236225962638855
Epoch 920, training loss: 0.6552256941795349 = 0.005567027255892754 + 0.1 * 6.496586799621582
Epoch 920, val loss: 1.2398322820663452
Epoch 930, training loss: 0.6539705395698547 = 0.005427987314760685 + 0.1 * 6.485424995422363
Epoch 930, val loss: 1.2434470653533936
Epoch 940, training loss: 0.6532309055328369 = 0.005295372102409601 + 0.1 * 6.4793548583984375
Epoch 940, val loss: 1.2471011877059937
Epoch 950, training loss: 0.6553513407707214 = 0.00516763748601079 + 0.1 * 6.501836776733398
Epoch 950, val loss: 1.250519871711731
Epoch 960, training loss: 0.6529473066329956 = 0.00504560861736536 + 0.1 * 6.4790167808532715
Epoch 960, val loss: 1.2539399862289429
Epoch 970, training loss: 0.6547706723213196 = 0.00492892786860466 + 0.1 * 6.498417377471924
Epoch 970, val loss: 1.257354736328125
Epoch 980, training loss: 0.6522032022476196 = 0.004816843196749687 + 0.1 * 6.47386360168457
Epoch 980, val loss: 1.260563611984253
Epoch 990, training loss: 0.6516048908233643 = 0.00470955204218626 + 0.1 * 6.4689531326293945
Epoch 990, val loss: 1.263850212097168
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8060094886663153
The final CL Acc:0.76296, 0.01889, The final GNN Acc:0.80794, 0.00138
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13162])
remove edge: torch.Size([2, 8034])
updated graph: torch.Size([2, 10640])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8008792400360107 = 1.9411944150924683 + 0.1 * 8.596848487854004
Epoch 0, val loss: 1.946363925933838
Epoch 10, training loss: 2.7905325889587402 = 1.9308586120605469 + 0.1 * 8.596739768981934
Epoch 10, val loss: 1.9357166290283203
Epoch 20, training loss: 2.7777249813079834 = 1.9181113243103027 + 0.1 * 8.596137046813965
Epoch 20, val loss: 1.9224883317947388
Epoch 30, training loss: 2.7595810890197754 = 1.9004456996917725 + 0.1 * 8.591354370117188
Epoch 30, val loss: 1.9041624069213867
Epoch 40, training loss: 2.7309205532073975 = 1.874681830406189 + 0.1 * 8.562387466430664
Epoch 40, val loss: 1.878139853477478
Epoch 50, training loss: 2.6844263076782227 = 1.8400707244873047 + 0.1 * 8.44355583190918
Epoch 50, val loss: 1.8453630208969116
Epoch 60, training loss: 2.6089134216308594 = 1.8033453226089478 + 0.1 * 8.055682182312012
Epoch 60, val loss: 1.8136563301086426
Epoch 70, training loss: 2.562155246734619 = 1.7700175046920776 + 0.1 * 7.921377182006836
Epoch 70, val loss: 1.7854710817337036
Epoch 80, training loss: 2.5034141540527344 = 1.7295290231704712 + 0.1 * 7.738852500915527
Epoch 80, val loss: 1.7480560541152954
Epoch 90, training loss: 2.4316513538360596 = 1.6770598888397217 + 0.1 * 7.545914173126221
Epoch 90, val loss: 1.7015751600265503
Epoch 100, training loss: 2.340399742126465 = 1.6111210584640503 + 0.1 * 7.292787075042725
Epoch 100, val loss: 1.6466482877731323
Epoch 110, training loss: 2.2462570667266846 = 1.5302006006240845 + 0.1 * 7.16056489944458
Epoch 110, val loss: 1.578135371208191
Epoch 120, training loss: 2.15054988861084 = 1.4408655166625977 + 0.1 * 7.09684419631958
Epoch 120, val loss: 1.5035911798477173
Epoch 130, training loss: 2.0558528900146484 = 1.353324055671692 + 0.1 * 7.025289535522461
Epoch 130, val loss: 1.4331636428833008
Epoch 140, training loss: 1.9634406566619873 = 1.2673228979110718 + 0.1 * 6.961178302764893
Epoch 140, val loss: 1.3636276721954346
Epoch 150, training loss: 1.8728885650634766 = 1.1812901496887207 + 0.1 * 6.915983200073242
Epoch 150, val loss: 1.2936055660247803
Epoch 160, training loss: 1.7855541706085205 = 1.0967954397201538 + 0.1 * 6.887587070465088
Epoch 160, val loss: 1.225947618484497
Epoch 170, training loss: 1.704477310180664 = 1.0172393321990967 + 0.1 * 6.872380256652832
Epoch 170, val loss: 1.164754867553711
Epoch 180, training loss: 1.6294755935668945 = 0.9438541531562805 + 0.1 * 6.856215000152588
Epoch 180, val loss: 1.1102596521377563
Epoch 190, training loss: 1.5586702823638916 = 0.8745594024658203 + 0.1 * 6.841108798980713
Epoch 190, val loss: 1.05979323387146
Epoch 200, training loss: 1.4912621974945068 = 0.8082907795906067 + 0.1 * 6.829713821411133
Epoch 200, val loss: 1.0114332437515259
Epoch 210, training loss: 1.4262558221817017 = 0.7451711297035217 + 0.1 * 6.81084680557251
Epoch 210, val loss: 0.9656656980514526
Epoch 220, training loss: 1.365950584411621 = 0.6860591173171997 + 0.1 * 6.798914432525635
Epoch 220, val loss: 0.9238753318786621
Epoch 230, training loss: 1.3111352920532227 = 0.6322283148765564 + 0.1 * 6.789070129394531
Epoch 230, val loss: 0.888599157333374
Epoch 240, training loss: 1.2607381343841553 = 0.5830020308494568 + 0.1 * 6.777360916137695
Epoch 240, val loss: 0.8597279191017151
Epoch 250, training loss: 1.2146778106689453 = 0.5378139615058899 + 0.1 * 6.768637657165527
Epoch 250, val loss: 0.837114691734314
Epoch 260, training loss: 1.1728298664093018 = 0.4964240789413452 + 0.1 * 6.7640581130981445
Epoch 260, val loss: 0.8202840089797974
Epoch 270, training loss: 1.1347730159759521 = 0.45826342701911926 + 0.1 * 6.765096187591553
Epoch 270, val loss: 0.8086344003677368
Epoch 280, training loss: 1.0981544256210327 = 0.42288920283317566 + 0.1 * 6.752652645111084
Epoch 280, val loss: 0.8012927174568176
Epoch 290, training loss: 1.0637394189834595 = 0.3893682658672333 + 0.1 * 6.743711948394775
Epoch 290, val loss: 0.797211229801178
Epoch 300, training loss: 1.0308263301849365 = 0.35732853412628174 + 0.1 * 6.734977722167969
Epoch 300, val loss: 0.7959898710250854
Epoch 310, training loss: 1.0005024671554565 = 0.3268841803073883 + 0.1 * 6.73618221282959
Epoch 310, val loss: 0.7973267436027527
Epoch 320, training loss: 0.9712262153625488 = 0.29836010932922363 + 0.1 * 6.728661060333252
Epoch 320, val loss: 0.8011980056762695
Epoch 330, training loss: 0.9432185888290405 = 0.2715452015399933 + 0.1 * 6.716733455657959
Epoch 330, val loss: 0.8074036836624146
Epoch 340, training loss: 0.9173149466514587 = 0.24612152576446533 + 0.1 * 6.7119340896606445
Epoch 340, val loss: 0.8155967593193054
Epoch 350, training loss: 0.8959426879882812 = 0.22200311720371246 + 0.1 * 6.739395618438721
Epoch 350, val loss: 0.8255980610847473
Epoch 360, training loss: 0.870211124420166 = 0.1996004432439804 + 0.1 * 6.706106662750244
Epoch 360, val loss: 0.8371853232383728
Epoch 370, training loss: 0.8489736914634705 = 0.1789541393518448 + 0.1 * 6.700195789337158
Epoch 370, val loss: 0.8503821492195129
Epoch 380, training loss: 0.8297942280769348 = 0.16019189357757568 + 0.1 * 6.696023464202881
Epoch 380, val loss: 0.8653734922409058
Epoch 390, training loss: 0.8126387596130371 = 0.14336158335208893 + 0.1 * 6.692771911621094
Epoch 390, val loss: 0.8820708990097046
Epoch 400, training loss: 0.7973410487174988 = 0.12838101387023926 + 0.1 * 6.689599990844727
Epoch 400, val loss: 0.9002559781074524
Epoch 410, training loss: 0.7850649356842041 = 0.1151157021522522 + 0.1 * 6.699492454528809
Epoch 410, val loss: 0.9195383787155151
Epoch 420, training loss: 0.7723777294158936 = 0.10345713794231415 + 0.1 * 6.689205646514893
Epoch 420, val loss: 0.9392640590667725
Epoch 430, training loss: 0.761391818523407 = 0.09315580129623413 + 0.1 * 6.6823601722717285
Epoch 430, val loss: 0.9595705270767212
Epoch 440, training loss: 0.7519341707229614 = 0.084039106965065 + 0.1 * 6.678950786590576
Epoch 440, val loss: 0.9799596667289734
Epoch 450, training loss: 0.7435991764068604 = 0.07597140967845917 + 0.1 * 6.676277160644531
Epoch 450, val loss: 1.0004903078079224
Epoch 460, training loss: 0.738868772983551 = 0.06883667409420013 + 0.1 * 6.700320720672607
Epoch 460, val loss: 1.0208935737609863
Epoch 470, training loss: 0.7303179502487183 = 0.06259097903966904 + 0.1 * 6.67726993560791
Epoch 470, val loss: 1.0405288934707642
Epoch 480, training loss: 0.7243080735206604 = 0.05708325281739235 + 0.1 * 6.672248363494873
Epoch 480, val loss: 1.0598812103271484
Epoch 490, training loss: 0.7188144326210022 = 0.05219734460115433 + 0.1 * 6.666171073913574
Epoch 490, val loss: 1.0787508487701416
Epoch 500, training loss: 0.7142043709754944 = 0.04784805327653885 + 0.1 * 6.663563251495361
Epoch 500, val loss: 1.0972272157669067
Epoch 510, training loss: 0.7100133299827576 = 0.04396631196141243 + 0.1 * 6.660470485687256
Epoch 510, val loss: 1.1152749061584473
Epoch 520, training loss: 0.7063227295875549 = 0.04049384221434593 + 0.1 * 6.658288955688477
Epoch 520, val loss: 1.1328743696212769
Epoch 530, training loss: 0.7030656933784485 = 0.03738933429121971 + 0.1 * 6.656763553619385
Epoch 530, val loss: 1.1500061750411987
Epoch 540, training loss: 0.7003412842750549 = 0.03461717814207077 + 0.1 * 6.657240867614746
Epoch 540, val loss: 1.1664372682571411
Epoch 550, training loss: 0.6972391605377197 = 0.032123930752277374 + 0.1 * 6.65115213394165
Epoch 550, val loss: 1.1824747323989868
Epoch 560, training loss: 0.6946648359298706 = 0.029873860999941826 + 0.1 * 6.647909641265869
Epoch 560, val loss: 1.1981682777404785
Epoch 570, training loss: 0.6938439011573792 = 0.02783874049782753 + 0.1 * 6.660051345825195
Epoch 570, val loss: 1.2134392261505127
Epoch 580, training loss: 0.6906260848045349 = 0.026006443426012993 + 0.1 * 6.646196365356445
Epoch 580, val loss: 1.228054404258728
Epoch 590, training loss: 0.6885349750518799 = 0.02434653230011463 + 0.1 * 6.6418843269348145
Epoch 590, val loss: 1.2423460483551025
Epoch 600, training loss: 0.6866940259933472 = 0.022836601361632347 + 0.1 * 6.63857364654541
Epoch 600, val loss: 1.2562658786773682
Epoch 610, training loss: 0.6854925751686096 = 0.021459870040416718 + 0.1 * 6.640326976776123
Epoch 610, val loss: 1.2698036432266235
Epoch 620, training loss: 0.6835415959358215 = 0.020205998793244362 + 0.1 * 6.633356094360352
Epoch 620, val loss: 1.2829676866531372
Epoch 630, training loss: 0.682237982749939 = 0.019061792641878128 + 0.1 * 6.63176155090332
Epoch 630, val loss: 1.2956653833389282
Epoch 640, training loss: 0.6812271475791931 = 0.01801259070634842 + 0.1 * 6.632145404815674
Epoch 640, val loss: 1.3080981969833374
Epoch 650, training loss: 0.6804861426353455 = 0.017053034156560898 + 0.1 * 6.634331226348877
Epoch 650, val loss: 1.3200291395187378
Epoch 660, training loss: 0.6786941289901733 = 0.016171881929039955 + 0.1 * 6.625222682952881
Epoch 660, val loss: 1.331653118133545
Epoch 670, training loss: 0.677730143070221 = 0.015359043143689632 + 0.1 * 6.623711109161377
Epoch 670, val loss: 1.3429597616195679
Epoch 680, training loss: 0.676978588104248 = 0.014610330574214458 + 0.1 * 6.623682022094727
Epoch 680, val loss: 1.3539998531341553
Epoch 690, training loss: 0.6761869192123413 = 0.013920600526034832 + 0.1 * 6.6226630210876465
Epoch 690, val loss: 1.3644745349884033
Epoch 700, training loss: 0.674924373626709 = 0.013280889950692654 + 0.1 * 6.6164350509643555
Epoch 700, val loss: 1.374802589416504
Epoch 710, training loss: 0.6745371222496033 = 0.012686165980994701 + 0.1 * 6.618509292602539
Epoch 710, val loss: 1.3848828077316284
Epoch 720, training loss: 0.6733294725418091 = 0.01213498879224062 + 0.1 * 6.611945152282715
Epoch 720, val loss: 1.3945485353469849
Epoch 730, training loss: 0.672658383846283 = 0.011622927151620388 + 0.1 * 6.610354900360107
Epoch 730, val loss: 1.40401291847229
Epoch 740, training loss: 0.6724126935005188 = 0.011145330034196377 + 0.1 * 6.612673282623291
Epoch 740, val loss: 1.4131134748458862
Epoch 750, training loss: 0.6709156036376953 = 0.010700499638915062 + 0.1 * 6.602150917053223
Epoch 750, val loss: 1.422028660774231
Epoch 760, training loss: 0.670549750328064 = 0.010285301133990288 + 0.1 * 6.602644443511963
Epoch 760, val loss: 1.4305757284164429
Epoch 770, training loss: 0.6695477366447449 = 0.009896139614284039 + 0.1 * 6.596515655517578
Epoch 770, val loss: 1.4388724565505981
Epoch 780, training loss: 0.6693086624145508 = 0.009532625786960125 + 0.1 * 6.597760200500488
Epoch 780, val loss: 1.4470303058624268
Epoch 790, training loss: 0.6688412427902222 = 0.00919233076274395 + 0.1 * 6.596488952636719
Epoch 790, val loss: 1.4546481370925903
Epoch 800, training loss: 0.6672781705856323 = 0.00887126661837101 + 0.1 * 6.58406925201416
Epoch 800, val loss: 1.4622544050216675
Epoch 810, training loss: 0.6671255826950073 = 0.00856925267726183 + 0.1 * 6.5855631828308105
Epoch 810, val loss: 1.469731330871582
Epoch 820, training loss: 0.6692866086959839 = 0.00828451570123434 + 0.1 * 6.610021114349365
Epoch 820, val loss: 1.4770203828811646
Epoch 830, training loss: 0.6652477979660034 = 0.008017669431865215 + 0.1 * 6.572301387786865
Epoch 830, val loss: 1.4837981462478638
Epoch 840, training loss: 0.6651326417922974 = 0.007764847949147224 + 0.1 * 6.5736775398254395
Epoch 840, val loss: 1.4905376434326172
Epoch 850, training loss: 0.6652223467826843 = 0.00752566521987319 + 0.1 * 6.576966762542725
Epoch 850, val loss: 1.4970148801803589
Epoch 860, training loss: 0.6640892028808594 = 0.007299498654901981 + 0.1 * 6.567896842956543
Epoch 860, val loss: 1.5034109354019165
Epoch 870, training loss: 0.6636306047439575 = 0.007085016462951899 + 0.1 * 6.565455436706543
Epoch 870, val loss: 1.5096760988235474
Epoch 880, training loss: 0.6636692881584167 = 0.0068827420473098755 + 0.1 * 6.567865371704102
Epoch 880, val loss: 1.5155253410339355
Epoch 890, training loss: 0.6630152463912964 = 0.006689503323286772 + 0.1 * 6.563257694244385
Epoch 890, val loss: 1.521435260772705
Epoch 900, training loss: 0.6622839570045471 = 0.006506375037133694 + 0.1 * 6.557775497436523
Epoch 900, val loss: 1.5271856784820557
Epoch 910, training loss: 0.6610670685768127 = 0.006331362761557102 + 0.1 * 6.547356605529785
Epoch 910, val loss: 1.5328292846679688
Epoch 920, training loss: 0.6607274413108826 = 0.006164302583783865 + 0.1 * 6.545631408691406
Epoch 920, val loss: 1.5383622646331787
Epoch 930, training loss: 0.6610918641090393 = 0.0060045975260436535 + 0.1 * 6.550872802734375
Epoch 930, val loss: 1.543897032737732
Epoch 940, training loss: 0.6614986062049866 = 0.00585304107517004 + 0.1 * 6.556455135345459
Epoch 940, val loss: 1.5489702224731445
Epoch 950, training loss: 0.6604481339454651 = 0.005707700736820698 + 0.1 * 6.5474042892456055
Epoch 950, val loss: 1.5543138980865479
Epoch 960, training loss: 0.6590901613235474 = 0.005569733213633299 + 0.1 * 6.53520393371582
Epoch 960, val loss: 1.5590788125991821
Epoch 970, training loss: 0.6591711640357971 = 0.0054367478005588055 + 0.1 * 6.537344455718994
Epoch 970, val loss: 1.5640584230422974
Epoch 980, training loss: 0.6574877500534058 = 0.005309435538947582 + 0.1 * 6.521782875061035
Epoch 980, val loss: 1.5689539909362793
Epoch 990, training loss: 0.6583356857299805 = 0.005187351256608963 + 0.1 * 6.5314836502075195
Epoch 990, val loss: 1.5737744569778442
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.7994766235351562 = 1.9397927522659302 + 0.1 * 8.596839904785156
Epoch 0, val loss: 1.9447039365768433
Epoch 10, training loss: 2.7891674041748047 = 1.929491639137268 + 0.1 * 8.596758842468262
Epoch 10, val loss: 1.9338921308517456
Epoch 20, training loss: 2.7766849994659424 = 1.9170711040496826 + 0.1 * 8.596138954162598
Epoch 20, val loss: 1.920538067817688
Epoch 30, training loss: 2.7589385509490967 = 1.8998881578445435 + 0.1 * 8.59050464630127
Epoch 30, val loss: 1.9020297527313232
Epoch 40, training loss: 2.729998826980591 = 1.874899983406067 + 0.1 * 8.550989151000977
Epoch 40, val loss: 1.8755443096160889
Epoch 50, training loss: 2.675177574157715 = 1.8415378332138062 + 0.1 * 8.336397171020508
Epoch 50, val loss: 1.8420889377593994
Epoch 60, training loss: 2.6043176651000977 = 1.8047235012054443 + 0.1 * 7.995942115783691
Epoch 60, val loss: 1.8078848123550415
Epoch 70, training loss: 2.547935962677002 = 1.7685902118682861 + 0.1 * 7.793457508087158
Epoch 70, val loss: 1.7760125398635864
Epoch 80, training loss: 2.4823107719421387 = 1.7278931140899658 + 0.1 * 7.5441765785217285
Epoch 80, val loss: 1.7400124073028564
Epoch 90, training loss: 2.4070708751678467 = 1.677147626876831 + 0.1 * 7.299232482910156
Epoch 90, val loss: 1.695212960243225
Epoch 100, training loss: 2.3190128803253174 = 1.608938217163086 + 0.1 * 7.100745677947998
Epoch 100, val loss: 1.6352746486663818
Epoch 110, training loss: 2.224560499191284 = 1.5230847597122192 + 0.1 * 7.014758110046387
Epoch 110, val loss: 1.5613776445388794
Epoch 120, training loss: 2.12610125541687 = 1.4285773038864136 + 0.1 * 6.9752397537231445
Epoch 120, val loss: 1.4821969270706177
Epoch 130, training loss: 2.0307602882385254 = 1.3356432914733887 + 0.1 * 6.951170444488525
Epoch 130, val loss: 1.4071680307388306
Epoch 140, training loss: 1.939605474472046 = 1.246959924697876 + 0.1 * 6.926455497741699
Epoch 140, val loss: 1.3389866352081299
Epoch 150, training loss: 1.8518033027648926 = 1.1623040437698364 + 0.1 * 6.894992828369141
Epoch 150, val loss: 1.275576114654541
Epoch 160, training loss: 1.766459584236145 = 1.0799834728240967 + 0.1 * 6.864760875701904
Epoch 160, val loss: 1.2142695188522339
Epoch 170, training loss: 1.6820991039276123 = 0.9975420236587524 + 0.1 * 6.845571517944336
Epoch 170, val loss: 1.1532148122787476
Epoch 180, training loss: 1.5969221591949463 = 0.9140281081199646 + 0.1 * 6.828940391540527
Epoch 180, val loss: 1.0909626483917236
Epoch 190, training loss: 1.5156121253967285 = 0.8340497612953186 + 0.1 * 6.815622806549072
Epoch 190, val loss: 1.0321182012557983
Epoch 200, training loss: 1.442065715789795 = 0.7620834112167358 + 0.1 * 6.79982328414917
Epoch 200, val loss: 0.980900764465332
Epoch 210, training loss: 1.3786280155181885 = 0.6999174952507019 + 0.1 * 6.787105560302734
Epoch 210, val loss: 0.9393816590309143
Epoch 220, training loss: 1.3240869045257568 = 0.6465648412704468 + 0.1 * 6.7752203941345215
Epoch 220, val loss: 0.9069026708602905
Epoch 230, training loss: 1.2759292125701904 = 0.5985558032989502 + 0.1 * 6.7737345695495605
Epoch 230, val loss: 0.8800845146179199
Epoch 240, training loss: 1.2297508716583252 = 0.5536695122718811 + 0.1 * 6.760812759399414
Epoch 240, val loss: 0.8569349646568298
Epoch 250, training loss: 1.184901475906372 = 0.5101343989372253 + 0.1 * 6.7476701736450195
Epoch 250, val loss: 0.8361359238624573
Epoch 260, training loss: 1.1425217390060425 = 0.4676315486431122 + 0.1 * 6.748902320861816
Epoch 260, val loss: 0.8180009126663208
Epoch 270, training loss: 1.100284218788147 = 0.4270860254764557 + 0.1 * 6.73198127746582
Epoch 270, val loss: 0.8037508130073547
Epoch 280, training loss: 1.0615384578704834 = 0.38874322175979614 + 0.1 * 6.727952003479004
Epoch 280, val loss: 0.7939047813415527
Epoch 290, training loss: 1.026032567024231 = 0.3532765209674835 + 0.1 * 6.727560520172119
Epoch 290, val loss: 0.788486897945404
Epoch 300, training loss: 0.9912828207015991 = 0.3205212950706482 + 0.1 * 6.707615375518799
Epoch 300, val loss: 0.7866989374160767
Epoch 310, training loss: 0.9602494239807129 = 0.29018452763557434 + 0.1 * 6.700648307800293
Epoch 310, val loss: 0.7877674698829651
Epoch 320, training loss: 0.9355453252792358 = 0.26230737566947937 + 0.1 * 6.732379913330078
Epoch 320, val loss: 0.7912725806236267
Epoch 330, training loss: 0.9062641263008118 = 0.23732493817806244 + 0.1 * 6.689391613006592
Epoch 330, val loss: 0.796963632106781
Epoch 340, training loss: 0.8829094171524048 = 0.21487972140312195 + 0.1 * 6.680297374725342
Epoch 340, val loss: 0.804379940032959
Epoch 350, training loss: 0.8628329038619995 = 0.194722980260849 + 0.1 * 6.6810994148254395
Epoch 350, val loss: 0.8137422800064087
Epoch 360, training loss: 0.8437948226928711 = 0.17673975229263306 + 0.1 * 6.67055082321167
Epoch 360, val loss: 0.8246235251426697
Epoch 370, training loss: 0.8269721865653992 = 0.1606607586145401 + 0.1 * 6.663114547729492
Epoch 370, val loss: 0.8368877172470093
Epoch 380, training loss: 0.8121397495269775 = 0.14625413715839386 + 0.1 * 6.658856391906738
Epoch 380, val loss: 0.850362241268158
Epoch 390, training loss: 0.798474907875061 = 0.13338036835193634 + 0.1 * 6.650945663452148
Epoch 390, val loss: 0.8646599650382996
Epoch 400, training loss: 0.7862928509712219 = 0.12178526073694229 + 0.1 * 6.645075798034668
Epoch 400, val loss: 0.8798240423202515
Epoch 410, training loss: 0.7756085991859436 = 0.1113055944442749 + 0.1 * 6.643029689788818
Epoch 410, val loss: 0.8956851959228516
Epoch 420, training loss: 0.7662644386291504 = 0.10187232494354248 + 0.1 * 6.6439208984375
Epoch 420, val loss: 0.9119504690170288
Epoch 430, training loss: 0.7564743757247925 = 0.09337513893842697 + 0.1 * 6.6309919357299805
Epoch 430, val loss: 0.9285038709640503
Epoch 440, training loss: 0.7480077147483826 = 0.08569114655256271 + 0.1 * 6.623165607452393
Epoch 440, val loss: 0.9453577995300293
Epoch 450, training loss: 0.7423692345619202 = 0.07874898612499237 + 0.1 * 6.636202335357666
Epoch 450, val loss: 0.962287962436676
Epoch 460, training loss: 0.7345341444015503 = 0.07251165807247162 + 0.1 * 6.620224475860596
Epoch 460, val loss: 0.9790797233581543
Epoch 470, training loss: 0.7284348607063293 = 0.06688044965267181 + 0.1 * 6.615543842315674
Epoch 470, val loss: 0.9958943724632263
Epoch 480, training loss: 0.7227701544761658 = 0.06180465221405029 + 0.1 * 6.609654903411865
Epoch 480, val loss: 1.0123263597488403
Epoch 490, training loss: 0.7170990109443665 = 0.057219378650188446 + 0.1 * 6.5987958908081055
Epoch 490, val loss: 1.0287295579910278
Epoch 500, training loss: 0.7142468094825745 = 0.05306830257177353 + 0.1 * 6.611785411834717
Epoch 500, val loss: 1.0448689460754395
Epoch 510, training loss: 0.7087010741233826 = 0.04932176694273949 + 0.1 * 6.593792915344238
Epoch 510, val loss: 1.0605806112289429
Epoch 520, training loss: 0.7047221064567566 = 0.045917902141809464 + 0.1 * 6.58804178237915
Epoch 520, val loss: 1.0760608911514282
Epoch 530, training loss: 0.7030991315841675 = 0.04281872883439064 + 0.1 * 6.602804183959961
Epoch 530, val loss: 1.0913664102554321
Epoch 540, training loss: 0.6979292035102844 = 0.04000735655426979 + 0.1 * 6.57921838760376
Epoch 540, val loss: 1.1061334609985352
Epoch 550, training loss: 0.6953582167625427 = 0.03744447976350784 + 0.1 * 6.579137325286865
Epoch 550, val loss: 1.1206674575805664
Epoch 560, training loss: 0.6925737261772156 = 0.035103388130664825 + 0.1 * 6.574703216552734
Epoch 560, val loss: 1.1349977254867554
Epoch 570, training loss: 0.6909828186035156 = 0.03296429663896561 + 0.1 * 6.5801849365234375
Epoch 570, val loss: 1.1490075588226318
Epoch 580, training loss: 0.6874848008155823 = 0.031005246564745903 + 0.1 * 6.564795017242432
Epoch 580, val loss: 1.1626092195510864
Epoch 590, training loss: 0.6865528225898743 = 0.02921047993004322 + 0.1 * 6.573422908782959
Epoch 590, val loss: 1.175946593284607
Epoch 600, training loss: 0.6832935810089111 = 0.027565183117985725 + 0.1 * 6.557283878326416
Epoch 600, val loss: 1.188846468925476
Epoch 610, training loss: 0.6815844774246216 = 0.026050277054309845 + 0.1 * 6.555342197418213
Epoch 610, val loss: 1.2015708684921265
Epoch 620, training loss: 0.6795811653137207 = 0.02465449459850788 + 0.1 * 6.549266338348389
Epoch 620, val loss: 1.2140111923217773
Epoch 630, training loss: 0.677723228931427 = 0.02336721308529377 + 0.1 * 6.543560028076172
Epoch 630, val loss: 1.2260960340499878
Epoch 640, training loss: 0.676994264125824 = 0.022175105288624763 + 0.1 * 6.548191070556641
Epoch 640, val loss: 1.2379251718521118
Epoch 650, training loss: 0.6754948496818542 = 0.021073834970593452 + 0.1 * 6.544209957122803
Epoch 650, val loss: 1.2495408058166504
Epoch 660, training loss: 0.6727578043937683 = 0.02005266584455967 + 0.1 * 6.527050971984863
Epoch 660, val loss: 1.2607848644256592
Epoch 670, training loss: 0.672021746635437 = 0.019103696569800377 + 0.1 * 6.529180526733398
Epoch 670, val loss: 1.2718418836593628
Epoch 680, training loss: 0.6712479591369629 = 0.018222691491246223 + 0.1 * 6.530252456665039
Epoch 680, val loss: 1.2826207876205444
Epoch 690, training loss: 0.669584333896637 = 0.0174034982919693 + 0.1 * 6.521808624267578
Epoch 690, val loss: 1.2930554151535034
Epoch 700, training loss: 0.6676268577575684 = 0.016640257090330124 + 0.1 * 6.509866237640381
Epoch 700, val loss: 1.3033205270767212
Epoch 710, training loss: 0.6666154265403748 = 0.015926042571663857 + 0.1 * 6.506893634796143
Epoch 710, val loss: 1.313416600227356
Epoch 720, training loss: 0.6671063899993896 = 0.015257807448506355 + 0.1 * 6.5184855461120605
Epoch 720, val loss: 1.3232184648513794
Epoch 730, training loss: 0.6653393507003784 = 0.014633377082645893 + 0.1 * 6.507059574127197
Epoch 730, val loss: 1.3328410387039185
Epoch 740, training loss: 0.6665271520614624 = 0.014047683216631413 + 0.1 * 6.524794578552246
Epoch 740, val loss: 1.3422167301177979
Epoch 750, training loss: 0.663314163684845 = 0.013498484157025814 + 0.1 * 6.498156547546387
Epoch 750, val loss: 1.3513319492340088
Epoch 760, training loss: 0.6637579202651978 = 0.012983045540750027 + 0.1 * 6.507749080657959
Epoch 760, val loss: 1.3603318929672241
Epoch 770, training loss: 0.661804735660553 = 0.012499152682721615 + 0.1 * 6.493055820465088
Epoch 770, val loss: 1.3690071105957031
Epoch 780, training loss: 0.6608031988143921 = 0.012042471207678318 + 0.1 * 6.487607479095459
Epoch 780, val loss: 1.3775882720947266
Epoch 790, training loss: 0.66077721118927 = 0.011611638590693474 + 0.1 * 6.4916558265686035
Epoch 790, val loss: 1.3859758377075195
Epoch 800, training loss: 0.6604758501052856 = 0.011205339804291725 + 0.1 * 6.492705345153809
Epoch 800, val loss: 1.3940858840942383
Epoch 810, training loss: 0.6600585579872131 = 0.010821184143424034 + 0.1 * 6.492373943328857
Epoch 810, val loss: 1.4021486043930054
Epoch 820, training loss: 0.6585965156555176 = 0.010458157397806644 + 0.1 * 6.481383323669434
Epoch 820, val loss: 1.4099316596984863
Epoch 830, training loss: 0.6584754586219788 = 0.010115017183125019 + 0.1 * 6.483604431152344
Epoch 830, val loss: 1.4176044464111328
Epoch 840, training loss: 0.6571821570396423 = 0.00979015976190567 + 0.1 * 6.473919868469238
Epoch 840, val loss: 1.4250648021697998
Epoch 850, training loss: 0.6570094227790833 = 0.00948178768157959 + 0.1 * 6.475276470184326
Epoch 850, val loss: 1.4324506521224976
Epoch 860, training loss: 0.6574602723121643 = 0.009188936091959476 + 0.1 * 6.482713222503662
Epoch 860, val loss: 1.4395831823349
Epoch 870, training loss: 0.6553872227668762 = 0.00891126599162817 + 0.1 * 6.464759349822998
Epoch 870, val loss: 1.4466086626052856
Epoch 880, training loss: 0.6553388237953186 = 0.00864686444401741 + 0.1 * 6.466919422149658
Epoch 880, val loss: 1.4534870386123657
Epoch 890, training loss: 0.654927134513855 = 0.008395234122872353 + 0.1 * 6.46531867980957
Epoch 890, val loss: 1.460222601890564
Epoch 900, training loss: 0.6546400785446167 = 0.008155770599842072 + 0.1 * 6.464842796325684
Epoch 900, val loss: 1.4667353630065918
Epoch 910, training loss: 0.6530905961990356 = 0.007927668280899525 + 0.1 * 6.451629161834717
Epoch 910, val loss: 1.4731892347335815
Epoch 920, training loss: 0.6525545120239258 = 0.007710393518209457 + 0.1 * 6.448441028594971
Epoch 920, val loss: 1.479567050933838
Epoch 930, training loss: 0.6513141989707947 = 0.007502489723265171 + 0.1 * 6.438117027282715
Epoch 930, val loss: 1.485783338546753
Epoch 940, training loss: 0.6546620726585388 = 0.007303515914827585 + 0.1 * 6.47358512878418
Epoch 940, val loss: 1.4919148683547974
Epoch 950, training loss: 0.652036190032959 = 0.0071133957244455814 + 0.1 * 6.449227809906006
Epoch 950, val loss: 1.4978413581848145
Epoch 960, training loss: 0.6521743535995483 = 0.006931595038622618 + 0.1 * 6.452427387237549
Epoch 960, val loss: 1.5037155151367188
Epoch 970, training loss: 0.6507781147956848 = 0.0067576272413134575 + 0.1 * 6.440204620361328
Epoch 970, val loss: 1.5094720125198364
Epoch 980, training loss: 0.6529390811920166 = 0.006590928416699171 + 0.1 * 6.463481903076172
Epoch 980, val loss: 1.5152305364608765
Epoch 990, training loss: 0.6503708958625793 = 0.0064310613088309765 + 0.1 * 6.439398288726807
Epoch 990, val loss: 1.520649790763855
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 2.8082871437072754 = 1.9486048221588135 + 0.1 * 8.596823692321777
Epoch 0, val loss: 1.9484881162643433
Epoch 10, training loss: 2.797987222671509 = 1.938318133354187 + 0.1 * 8.59669017791748
Epoch 10, val loss: 1.9386564493179321
Epoch 20, training loss: 2.785156726837158 = 1.925594449043274 + 0.1 * 8.595623970031738
Epoch 20, val loss: 1.9260679483413696
Epoch 30, training loss: 2.766169786453247 = 1.9077024459838867 + 0.1 * 8.584673881530762
Epoch 30, val loss: 1.9080449342727661
Epoch 40, training loss: 2.7313921451568604 = 1.881494164466858 + 0.1 * 8.498979568481445
Epoch 40, val loss: 1.8818573951721191
Epoch 50, training loss: 2.655730724334717 = 1.846419095993042 + 0.1 * 8.093116760253906
Epoch 50, val loss: 1.8484466075897217
Epoch 60, training loss: 2.6013057231903076 = 1.8069521188735962 + 0.1 * 7.943535327911377
Epoch 60, val loss: 1.8128635883331299
Epoch 70, training loss: 2.5327770709991455 = 1.7686911821365356 + 0.1 * 7.6408586502075195
Epoch 70, val loss: 1.7802010774612427
Epoch 80, training loss: 2.4543559551239014 = 1.7303886413574219 + 0.1 * 7.239673137664795
Epoch 80, val loss: 1.7468172311782837
Epoch 90, training loss: 2.3869221210479736 = 1.6808122396469116 + 0.1 * 7.061099052429199
Epoch 90, val loss: 1.7026561498641968
Epoch 100, training loss: 2.312453031539917 = 1.6137899160385132 + 0.1 * 6.986630916595459
Epoch 100, val loss: 1.6462153196334839
Epoch 110, training loss: 2.2223918437957764 = 1.5291014909744263 + 0.1 * 6.932903289794922
Epoch 110, val loss: 1.5769938230514526
Epoch 120, training loss: 2.1239120960235596 = 1.4344104528427124 + 0.1 * 6.895015716552734
Epoch 120, val loss: 1.4989486932754517
Epoch 130, training loss: 2.0239198207855225 = 1.3372284173965454 + 0.1 * 6.866913318634033
Epoch 130, val loss: 1.4208353757858276
Epoch 140, training loss: 1.9264602661132812 = 1.241329550743103 + 0.1 * 6.851306438446045
Epoch 140, val loss: 1.3459227085113525
Epoch 150, training loss: 1.8318350315093994 = 1.148485779762268 + 0.1 * 6.833492279052734
Epoch 150, val loss: 1.2743923664093018
Epoch 160, training loss: 1.7412171363830566 = 1.0589993000030518 + 0.1 * 6.822178840637207
Epoch 160, val loss: 1.206149935722351
Epoch 170, training loss: 1.6551933288574219 = 0.9749392867088318 + 0.1 * 6.8025407791137695
Epoch 170, val loss: 1.1420904397964478
Epoch 180, training loss: 1.5746265649795532 = 0.8955329656600952 + 0.1 * 6.79093599319458
Epoch 180, val loss: 1.0815258026123047
Epoch 190, training loss: 1.49904465675354 = 0.8214908838272095 + 0.1 * 6.775537490844727
Epoch 190, val loss: 1.0254782438278198
Epoch 200, training loss: 1.4285533428192139 = 0.7526580095291138 + 0.1 * 6.758954048156738
Epoch 200, val loss: 0.9742687940597534
Epoch 210, training loss: 1.3647925853729248 = 0.6897575259208679 + 0.1 * 6.750350475311279
Epoch 210, val loss: 0.9293436408042908
Epoch 220, training loss: 1.3066067695617676 = 0.6333077549934387 + 0.1 * 6.732990741729736
Epoch 220, val loss: 0.8914669752120972
Epoch 230, training loss: 1.2553601264953613 = 0.5825631618499756 + 0.1 * 6.727969646453857
Epoch 230, val loss: 0.8600850105285645
Epoch 240, training loss: 1.208603858947754 = 0.5370851755142212 + 0.1 * 6.715187072753906
Epoch 240, val loss: 0.8342569470405579
Epoch 250, training loss: 1.165294885635376 = 0.4949718713760376 + 0.1 * 6.703230381011963
Epoch 250, val loss: 0.8121598958969116
Epoch 260, training loss: 1.1245551109313965 = 0.4548758268356323 + 0.1 * 6.696792125701904
Epoch 260, val loss: 0.7925890684127808
Epoch 270, training loss: 1.0851645469665527 = 0.41625383496284485 + 0.1 * 6.6891069412231445
Epoch 270, val loss: 0.7748557329177856
Epoch 280, training loss: 1.047278881072998 = 0.37862080335617065 + 0.1 * 6.686581134796143
Epoch 280, val loss: 0.7585670351982117
Epoch 290, training loss: 1.0096427202224731 = 0.3417390286922455 + 0.1 * 6.679036617279053
Epoch 290, val loss: 0.7438182234764099
Epoch 300, training loss: 0.9741191267967224 = 0.30598902702331543 + 0.1 * 6.681300640106201
Epoch 300, val loss: 0.7310250997543335
Epoch 310, training loss: 0.939877450466156 = 0.27225446701049805 + 0.1 * 6.676229953765869
Epoch 310, val loss: 0.720642626285553
Epoch 320, training loss: 0.9076049327850342 = 0.24090082943439484 + 0.1 * 6.667041301727295
Epoch 320, val loss: 0.7132166028022766
Epoch 330, training loss: 0.8785157799720764 = 0.21237291395664215 + 0.1 * 6.661428451538086
Epoch 330, val loss: 0.7089244723320007
Epoch 340, training loss: 0.8526202440261841 = 0.18703486025333405 + 0.1 * 6.655853748321533
Epoch 340, val loss: 0.7078444957733154
Epoch 350, training loss: 0.8302167654037476 = 0.16504153609275818 + 0.1 * 6.65175199508667
Epoch 350, val loss: 0.7096938490867615
Epoch 360, training loss: 0.8106358051300049 = 0.14606830477714539 + 0.1 * 6.645675182342529
Epoch 360, val loss: 0.7141090631484985
Epoch 370, training loss: 0.794036328792572 = 0.12973785400390625 + 0.1 * 6.642984867095947
Epoch 370, val loss: 0.7206192016601562
Epoch 380, training loss: 0.7792002558708191 = 0.11570291221141815 + 0.1 * 6.634973049163818
Epoch 380, val loss: 0.7287250757217407
Epoch 390, training loss: 0.7664433717727661 = 0.10363134741783142 + 0.1 * 6.628119945526123
Epoch 390, val loss: 0.737985372543335
Epoch 400, training loss: 0.7547541260719299 = 0.09322399646043777 + 0.1 * 6.615301132202148
Epoch 400, val loss: 0.7479468584060669
Epoch 410, training loss: 0.7449992895126343 = 0.08415674418210983 + 0.1 * 6.608425140380859
Epoch 410, val loss: 0.7585655450820923
Epoch 420, training loss: 0.7392221093177795 = 0.07623440772294998 + 0.1 * 6.629877090454102
Epoch 420, val loss: 0.7696143388748169
Epoch 430, training loss: 0.7295747995376587 = 0.06933421641588211 + 0.1 * 6.602406024932861
Epoch 430, val loss: 0.7805867791175842
Epoch 440, training loss: 0.7220402359962463 = 0.06328855454921722 + 0.1 * 6.587516784667969
Epoch 440, val loss: 0.7916218042373657
Epoch 450, training loss: 0.7164140343666077 = 0.05795514956116676 + 0.1 * 6.584588527679443
Epoch 450, val loss: 0.8027129769325256
Epoch 460, training loss: 0.7108451724052429 = 0.053230274468660355 + 0.1 * 6.576148509979248
Epoch 460, val loss: 0.8137197494506836
Epoch 470, training loss: 0.7066245079040527 = 0.0490303598344326 + 0.1 * 6.575941562652588
Epoch 470, val loss: 0.824606716632843
Epoch 480, training loss: 0.7018662691116333 = 0.04528834670782089 + 0.1 * 6.565779209136963
Epoch 480, val loss: 0.8352577686309814
Epoch 490, training loss: 0.6981849074363708 = 0.04195045679807663 + 0.1 * 6.562344074249268
Epoch 490, val loss: 0.8457046151161194
Epoch 500, training loss: 0.6938795447349548 = 0.03896872699260712 + 0.1 * 6.549108028411865
Epoch 500, val loss: 0.8557674288749695
Epoch 510, training loss: 0.6905823349952698 = 0.0362795814871788 + 0.1 * 6.543027877807617
Epoch 510, val loss: 0.8657251596450806
Epoch 520, training loss: 0.6877313256263733 = 0.03385164961218834 + 0.1 * 6.538796901702881
Epoch 520, val loss: 0.875457227230072
Epoch 530, training loss: 0.6847341656684875 = 0.031655315309762955 + 0.1 * 6.530788421630859
Epoch 530, val loss: 0.8848884701728821
Epoch 540, training loss: 0.6827097535133362 = 0.029661742970347404 + 0.1 * 6.53048038482666
Epoch 540, val loss: 0.8941141366958618
Epoch 550, training loss: 0.6806781888008118 = 0.027850348502397537 + 0.1 * 6.528278350830078
Epoch 550, val loss: 0.9030537605285645
Epoch 560, training loss: 0.678665816783905 = 0.02620280347764492 + 0.1 * 6.524630069732666
Epoch 560, val loss: 0.911791205406189
Epoch 570, training loss: 0.6760536432266235 = 0.02469845861196518 + 0.1 * 6.513552188873291
Epoch 570, val loss: 0.9202345609664917
Epoch 580, training loss: 0.6745677590370178 = 0.023320037871599197 + 0.1 * 6.512477397918701
Epoch 580, val loss: 0.92854905128479
Epoch 590, training loss: 0.6746562719345093 = 0.022058889269828796 + 0.1 * 6.525973796844482
Epoch 590, val loss: 0.9365565776824951
Epoch 600, training loss: 0.6714098453521729 = 0.020901994779706 + 0.1 * 6.505078315734863
Epoch 600, val loss: 0.9443026185035706
Epoch 610, training loss: 0.6695325970649719 = 0.019835514947772026 + 0.1 * 6.4969706535339355
Epoch 610, val loss: 0.9518886804580688
Epoch 620, training loss: 0.668621838092804 = 0.018850209191441536 + 0.1 * 6.497716426849365
Epoch 620, val loss: 0.9594041705131531
Epoch 630, training loss: 0.6670789122581482 = 0.017941460013389587 + 0.1 * 6.4913740158081055
Epoch 630, val loss: 0.9665623307228088
Epoch 640, training loss: 0.6665378212928772 = 0.017098354175686836 + 0.1 * 6.494394302368164
Epoch 640, val loss: 0.9736769199371338
Epoch 650, training loss: 0.6660293340682983 = 0.016316378489136696 + 0.1 * 6.497129917144775
Epoch 650, val loss: 0.9805915951728821
Epoch 660, training loss: 0.6645204424858093 = 0.015592116862535477 + 0.1 * 6.489283561706543
Epoch 660, val loss: 0.9873619079589844
Epoch 670, training loss: 0.6628305315971375 = 0.01491878367960453 + 0.1 * 6.479116916656494
Epoch 670, val loss: 0.9938169121742249
Epoch 680, training loss: 0.6622586846351624 = 0.01428900845348835 + 0.1 * 6.479696750640869
Epoch 680, val loss: 1.000253677368164
Epoch 690, training loss: 0.662106990814209 = 0.013701120391488075 + 0.1 * 6.484058380126953
Epoch 690, val loss: 1.0066094398498535
Epoch 700, training loss: 0.6617377996444702 = 0.013152103871107101 + 0.1 * 6.485856533050537
Epoch 700, val loss: 1.0126830339431763
Epoch 710, training loss: 0.6590776443481445 = 0.01263925526291132 + 0.1 * 6.464383602142334
Epoch 710, val loss: 1.0186271667480469
Epoch 720, training loss: 0.6582865118980408 = 0.012157149612903595 + 0.1 * 6.461293697357178
Epoch 720, val loss: 1.0244625806808472
Epoch 730, training loss: 0.657730758190155 = 0.011703947558999062 + 0.1 * 6.460267543792725
Epoch 730, val loss: 1.030234694480896
Epoch 740, training loss: 0.6574682593345642 = 0.011278478428721428 + 0.1 * 6.461897850036621
Epoch 740, val loss: 1.0357714891433716
Epoch 750, training loss: 0.658362090587616 = 0.010877386666834354 + 0.1 * 6.474847316741943
Epoch 750, val loss: 1.0412169694900513
Epoch 760, training loss: 0.6565542221069336 = 0.01049942709505558 + 0.1 * 6.460547924041748
Epoch 760, val loss: 1.0466322898864746
Epoch 770, training loss: 0.6565638184547424 = 0.010143565014004707 + 0.1 * 6.464202404022217
Epoch 770, val loss: 1.0518465042114258
Epoch 780, training loss: 0.6551502346992493 = 0.00980770867317915 + 0.1 * 6.45342493057251
Epoch 780, val loss: 1.0568656921386719
Epoch 790, training loss: 0.6533322334289551 = 0.009489906020462513 + 0.1 * 6.438423156738281
Epoch 790, val loss: 1.0618414878845215
Epoch 800, training loss: 0.6561737060546875 = 0.009187780320644379 + 0.1 * 6.4698591232299805
Epoch 800, val loss: 1.0667989253997803
Epoch 810, training loss: 0.6527242660522461 = 0.008901444263756275 + 0.1 * 6.438228130340576
Epoch 810, val loss: 1.071685791015625
Epoch 820, training loss: 0.6534543633460999 = 0.008630170486867428 + 0.1 * 6.4482421875
Epoch 820, val loss: 1.076372504234314
Epoch 830, training loss: 0.6523122191429138 = 0.008372941054403782 + 0.1 * 6.439392566680908
Epoch 830, val loss: 1.0809321403503418
Epoch 840, training loss: 0.652018666267395 = 0.008128033019602299 + 0.1 * 6.438906192779541
Epoch 840, val loss: 1.0854849815368652
Epoch 850, training loss: 0.6510664224624634 = 0.007894650101661682 + 0.1 * 6.431717872619629
Epoch 850, val loss: 1.0899434089660645
Epoch 860, training loss: 0.6533083915710449 = 0.00767219252884388 + 0.1 * 6.456361770629883
Epoch 860, val loss: 1.094352126121521
Epoch 870, training loss: 0.6503743529319763 = 0.007460621651262045 + 0.1 * 6.429137229919434
Epoch 870, val loss: 1.098618507385254
Epoch 880, training loss: 0.6509069800376892 = 0.00725895632058382 + 0.1 * 6.4364800453186035
Epoch 880, val loss: 1.1027623414993286
Epoch 890, training loss: 0.6495335102081299 = 0.007066556718200445 + 0.1 * 6.4246697425842285
Epoch 890, val loss: 1.1068947315216064
Epoch 900, training loss: 0.6492160558700562 = 0.006882091984152794 + 0.1 * 6.42333984375
Epoch 900, val loss: 1.110953450202942
Epoch 910, training loss: 0.6482234597206116 = 0.0067057497799396515 + 0.1 * 6.415176868438721
Epoch 910, val loss: 1.1149213314056396
Epoch 920, training loss: 0.6491838693618774 = 0.006537041161209345 + 0.1 * 6.426468372344971
Epoch 920, val loss: 1.1188186407089233
Epoch 930, training loss: 0.647739589214325 = 0.006374888587743044 + 0.1 * 6.413646697998047
Epoch 930, val loss: 1.1227041482925415
Epoch 940, training loss: 0.6489226818084717 = 0.006219940260052681 + 0.1 * 6.427027225494385
Epoch 940, val loss: 1.126482367515564
Epoch 950, training loss: 0.6469613909721375 = 0.006071289069950581 + 0.1 * 6.408901214599609
Epoch 950, val loss: 1.1302326917648315
Epoch 960, training loss: 0.6465387940406799 = 0.005928712897002697 + 0.1 * 6.406100749969482
Epoch 960, val loss: 1.1338180303573608
Epoch 970, training loss: 0.6475620269775391 = 0.005791435018181801 + 0.1 * 6.41770601272583
Epoch 970, val loss: 1.137459635734558
Epoch 980, training loss: 0.6470152735710144 = 0.005659908521920443 + 0.1 * 6.413553237915039
Epoch 980, val loss: 1.1410075426101685
Epoch 990, training loss: 0.6453443169593811 = 0.005533588584512472 + 0.1 * 6.398107051849365
Epoch 990, val loss: 1.1444644927978516
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8413284132841329
The final CL Acc:0.79259, 0.01048, The final GNN Acc:0.83869, 0.00197
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11662])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10614])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.804142475128174 = 1.9444546699523926 + 0.1 * 8.596878051757812
Epoch 0, val loss: 1.9446381330490112
Epoch 10, training loss: 2.7940001487731934 = 1.934314489364624 + 0.1 * 8.596855163574219
Epoch 10, val loss: 1.9339632987976074
Epoch 20, training loss: 2.781850576400757 = 1.9221830368041992 + 0.1 * 8.596675872802734
Epoch 20, val loss: 1.9208850860595703
Epoch 30, training loss: 2.765136241912842 = 1.9056206941604614 + 0.1 * 8.5951566696167
Epoch 30, val loss: 1.9028403759002686
Epoch 40, training loss: 2.7397491931915283 = 1.8817317485809326 + 0.1 * 8.58017349243164
Epoch 40, val loss: 1.8770196437835693
Epoch 50, training loss: 2.6986262798309326 = 1.8489725589752197 + 0.1 * 8.496537208557129
Epoch 50, val loss: 1.8430891036987305
Epoch 60, training loss: 2.629603147506714 = 1.813833236694336 + 0.1 * 8.157699584960938
Epoch 60, val loss: 1.810531735420227
Epoch 70, training loss: 2.590435743331909 = 1.784623384475708 + 0.1 * 8.058123588562012
Epoch 70, val loss: 1.7877174615859985
Epoch 80, training loss: 2.531649589538574 = 1.7522575855255127 + 0.1 * 7.793920516967773
Epoch 80, val loss: 1.7614357471466064
Epoch 90, training loss: 2.4495325088500977 = 1.709842562675476 + 0.1 * 7.39689826965332
Epoch 90, val loss: 1.724033236503601
Epoch 100, training loss: 2.372102737426758 = 1.652692198753357 + 0.1 * 7.194106578826904
Epoch 100, val loss: 1.6744592189788818
Epoch 110, training loss: 2.2877771854400635 = 1.5787047147750854 + 0.1 * 7.090724945068359
Epoch 110, val loss: 1.6111650466918945
Epoch 120, training loss: 2.197059154510498 = 1.4927831888198853 + 0.1 * 7.042760848999023
Epoch 120, val loss: 1.5375275611877441
Epoch 130, training loss: 2.1035842895507812 = 1.4016249179840088 + 0.1 * 7.019593238830566
Epoch 130, val loss: 1.4647612571716309
Epoch 140, training loss: 2.0085198879241943 = 1.3081318140029907 + 0.1 * 7.003880023956299
Epoch 140, val loss: 1.392929196357727
Epoch 150, training loss: 1.9095494747161865 = 1.210579514503479 + 0.1 * 6.989698886871338
Epoch 150, val loss: 1.319828987121582
Epoch 160, training loss: 1.8067412376403809 = 1.1093758344650269 + 0.1 * 6.973653793334961
Epoch 160, val loss: 1.2455004453659058
Epoch 170, training loss: 1.7051119804382324 = 1.0093778371810913 + 0.1 * 6.957341194152832
Epoch 170, val loss: 1.1721497774124146
Epoch 180, training loss: 1.6084110736846924 = 0.914474368095398 + 0.1 * 6.939366340637207
Epoch 180, val loss: 1.1025511026382446
Epoch 190, training loss: 1.5200345516204834 = 0.8281932473182678 + 0.1 * 6.9184136390686035
Epoch 190, val loss: 1.039448618888855
Epoch 200, training loss: 1.4427821636199951 = 0.7531246542930603 + 0.1 * 6.8965744972229
Epoch 200, val loss: 0.9860669374465942
Epoch 210, training loss: 1.374886393547058 = 0.6873428821563721 + 0.1 * 6.875434875488281
Epoch 210, val loss: 0.9414048194885254
Epoch 220, training loss: 1.3160169124603271 = 0.6284812092781067 + 0.1 * 6.875357627868652
Epoch 220, val loss: 0.9043241739273071
Epoch 230, training loss: 1.2621533870697021 = 0.5761075615882874 + 0.1 * 6.860457420349121
Epoch 230, val loss: 0.8742136359214783
Epoch 240, training loss: 1.2114472389221191 = 0.5277568101882935 + 0.1 * 6.836904048919678
Epoch 240, val loss: 0.8486671447753906
Epoch 250, training loss: 1.164567470550537 = 0.48215460777282715 + 0.1 * 6.824129104614258
Epoch 250, val loss: 0.8262209892272949
Epoch 260, training loss: 1.1208243370056152 = 0.4390829801559448 + 0.1 * 6.817412853240967
Epoch 260, val loss: 0.8066367506980896
Epoch 270, training loss: 1.0802395343780518 = 0.3987855315208435 + 0.1 * 6.814539432525635
Epoch 270, val loss: 0.7896308898925781
Epoch 280, training loss: 1.0419400930404663 = 0.36116451025009155 + 0.1 * 6.807755470275879
Epoch 280, val loss: 0.7751594185829163
Epoch 290, training loss: 1.006804347038269 = 0.32641854882240295 + 0.1 * 6.803857326507568
Epoch 290, val loss: 0.7631176710128784
Epoch 300, training loss: 0.9747039079666138 = 0.2948063910007477 + 0.1 * 6.798974990844727
Epoch 300, val loss: 0.7534401416778564
Epoch 310, training loss: 0.9479434490203857 = 0.2662755846977234 + 0.1 * 6.816678524017334
Epoch 310, val loss: 0.7459508180618286
Epoch 320, training loss: 0.9201163053512573 = 0.24047482013702393 + 0.1 * 6.796414852142334
Epoch 320, val loss: 0.7402740716934204
Epoch 330, training loss: 0.8957513570785522 = 0.21676217019557953 + 0.1 * 6.789891719818115
Epoch 330, val loss: 0.7362282872200012
Epoch 340, training loss: 0.8753306865692139 = 0.19477902352809906 + 0.1 * 6.805516242980957
Epoch 340, val loss: 0.733470618724823
Epoch 350, training loss: 0.8530241250991821 = 0.17461147904396057 + 0.1 * 6.784126281738281
Epoch 350, val loss: 0.7318212985992432
Epoch 360, training loss: 0.8340306282043457 = 0.15616779029369354 + 0.1 * 6.778628349304199
Epoch 360, val loss: 0.7314676642417908
Epoch 370, training loss: 0.8175009489059448 = 0.13949498534202576 + 0.1 * 6.780059814453125
Epoch 370, val loss: 0.7323283553123474
Epoch 380, training loss: 0.8010748624801636 = 0.12466708570718765 + 0.1 * 6.764077186584473
Epoch 380, val loss: 0.7343539595603943
Epoch 390, training loss: 0.7878803014755249 = 0.11160023510456085 + 0.1 * 6.762800693511963
Epoch 390, val loss: 0.7374264001846313
Epoch 400, training loss: 0.7762008309364319 = 0.10022365301847458 + 0.1 * 6.759771347045898
Epoch 400, val loss: 0.7414761185646057
Epoch 410, training loss: 0.7651819586753845 = 0.09026167541742325 + 0.1 * 6.749202728271484
Epoch 410, val loss: 0.7462261915206909
Epoch 420, training loss: 0.7556621432304382 = 0.08153987675905228 + 0.1 * 6.741222381591797
Epoch 420, val loss: 0.7516188621520996
Epoch 430, training loss: 0.7474307417869568 = 0.07391088455915451 + 0.1 * 6.735198497772217
Epoch 430, val loss: 0.7575997710227966
Epoch 440, training loss: 0.7414602637290955 = 0.06720183044672012 + 0.1 * 6.742584228515625
Epoch 440, val loss: 0.7639463543891907
Epoch 450, training loss: 0.7338621020317078 = 0.06130969524383545 + 0.1 * 6.725523948669434
Epoch 450, val loss: 0.7706617712974548
Epoch 460, training loss: 0.7278696298599243 = 0.05612531676888466 + 0.1 * 6.717442989349365
Epoch 460, val loss: 0.7775928378105164
Epoch 470, training loss: 0.721159815788269 = 0.05153748765587807 + 0.1 * 6.69622278213501
Epoch 470, val loss: 0.7847747802734375
Epoch 480, training loss: 0.7169981002807617 = 0.04747333750128746 + 0.1 * 6.695247650146484
Epoch 480, val loss: 0.792011022567749
Epoch 490, training loss: 0.7121650576591492 = 0.04386661946773529 + 0.1 * 6.682984352111816
Epoch 490, val loss: 0.7994508147239685
Epoch 500, training loss: 0.7094897627830505 = 0.04064847528934479 + 0.1 * 6.688413143157959
Epoch 500, val loss: 0.8068468570709229
Epoch 510, training loss: 0.7052003145217896 = 0.037775829434394836 + 0.1 * 6.6742448806762695
Epoch 510, val loss: 0.8141953349113464
Epoch 520, training loss: 0.7032750248908997 = 0.03521008417010307 + 0.1 * 6.680649280548096
Epoch 520, val loss: 0.8215239644050598
Epoch 530, training loss: 0.6977367997169495 = 0.03290242329239845 + 0.1 * 6.648343563079834
Epoch 530, val loss: 0.8288640379905701
Epoch 540, training loss: 0.6966844201087952 = 0.030812542885541916 + 0.1 * 6.658718585968018
Epoch 540, val loss: 0.8360598087310791
Epoch 550, training loss: 0.6926382780075073 = 0.028926542028784752 + 0.1 * 6.637117385864258
Epoch 550, val loss: 0.8430812358856201
Epoch 560, training loss: 0.6904041171073914 = 0.02721736580133438 + 0.1 * 6.631866931915283
Epoch 560, val loss: 0.8501468896865845
Epoch 570, training loss: 0.6898304224014282 = 0.025664906948804855 + 0.1 * 6.641654968261719
Epoch 570, val loss: 0.8568646907806396
Epoch 580, training loss: 0.6863900423049927 = 0.024248719215393066 + 0.1 * 6.621413230895996
Epoch 580, val loss: 0.8636530637741089
Epoch 590, training loss: 0.6848582625389099 = 0.0229492150247097 + 0.1 * 6.6190900802612305
Epoch 590, val loss: 0.8703051805496216
Epoch 600, training loss: 0.6835299134254456 = 0.02175908535718918 + 0.1 * 6.617708206176758
Epoch 600, val loss: 0.8766486644744873
Epoch 610, training loss: 0.6801615357398987 = 0.020667962729930878 + 0.1 * 6.594935417175293
Epoch 610, val loss: 0.8830472230911255
Epoch 620, training loss: 0.6802131533622742 = 0.019659051671624184 + 0.1 * 6.605540752410889
Epoch 620, val loss: 0.8893152475357056
Epoch 630, training loss: 0.6792406439781189 = 0.018727345392107964 + 0.1 * 6.605132579803467
Epoch 630, val loss: 0.8953065872192383
Epoch 640, training loss: 0.6768518686294556 = 0.017864858731627464 + 0.1 * 6.589869499206543
Epoch 640, val loss: 0.9012895822525024
Epoch 650, training loss: 0.6753670573234558 = 0.017063947394490242 + 0.1 * 6.583030700683594
Epoch 650, val loss: 0.9072086215019226
Epoch 660, training loss: 0.6757726073265076 = 0.01631975546479225 + 0.1 * 6.594528675079346
Epoch 660, val loss: 0.912837564945221
Epoch 670, training loss: 0.6732835173606873 = 0.015629595145583153 + 0.1 * 6.576539039611816
Epoch 670, val loss: 0.9184476137161255
Epoch 680, training loss: 0.674390971660614 = 0.014984136447310448 + 0.1 * 6.5940680503845215
Epoch 680, val loss: 0.9240146279335022
Epoch 690, training loss: 0.670458972454071 = 0.014381642453372478 + 0.1 * 6.5607733726501465
Epoch 690, val loss: 0.9293437004089355
Epoch 700, training loss: 0.6702126860618591 = 0.0138170775026083 + 0.1 * 6.563956260681152
Epoch 700, val loss: 0.9347065687179565
Epoch 710, training loss: 0.6679666638374329 = 0.01328694075345993 + 0.1 * 6.546796798706055
Epoch 710, val loss: 0.9398220181465149
Epoch 720, training loss: 0.6680890321731567 = 0.012790772132575512 + 0.1 * 6.552982330322266
Epoch 720, val loss: 0.9449579119682312
Epoch 730, training loss: 0.670036256313324 = 0.01232389360666275 + 0.1 * 6.577123165130615
Epoch 730, val loss: 0.9499856233596802
Epoch 740, training loss: 0.6658951044082642 = 0.011884157545864582 + 0.1 * 6.540109634399414
Epoch 740, val loss: 0.9548786282539368
Epoch 750, training loss: 0.6648299694061279 = 0.011469660326838493 + 0.1 * 6.533602714538574
Epoch 750, val loss: 0.9597710371017456
Epoch 760, training loss: 0.6663883328437805 = 0.011077494360506535 + 0.1 * 6.553107738494873
Epoch 760, val loss: 0.964510977268219
Epoch 770, training loss: 0.6637023687362671 = 0.010709048248827457 + 0.1 * 6.529932975769043
Epoch 770, val loss: 0.969060480594635
Epoch 780, training loss: 0.6636314392089844 = 0.01036059856414795 + 0.1 * 6.532708168029785
Epoch 780, val loss: 0.973716139793396
Epoch 790, training loss: 0.6640000939369202 = 0.010029383935034275 + 0.1 * 6.539707183837891
Epoch 790, val loss: 0.9781973361968994
Epoch 800, training loss: 0.6627013683319092 = 0.009715857915580273 + 0.1 * 6.529854774475098
Epoch 800, val loss: 0.9826238751411438
Epoch 810, training loss: 0.6607668995857239 = 0.009417588822543621 + 0.1 * 6.513493061065674
Epoch 810, val loss: 0.9870314002037048
Epoch 820, training loss: 0.6634640693664551 = 0.00913329143077135 + 0.1 * 6.543307781219482
Epoch 820, val loss: 0.9913139343261719
Epoch 830, training loss: 0.6595340371131897 = 0.008864903822541237 + 0.1 * 6.5066914558410645
Epoch 830, val loss: 0.9954532384872437
Epoch 840, training loss: 0.65888911485672 = 0.00860938522964716 + 0.1 * 6.502796649932861
Epoch 840, val loss: 0.9996833801269531
Epoch 850, training loss: 0.6606640219688416 = 0.008364833891391754 + 0.1 * 6.52299165725708
Epoch 850, val loss: 1.003740668296814
Epoch 860, training loss: 0.6576204299926758 = 0.008133389055728912 + 0.1 * 6.494870185852051
Epoch 860, val loss: 1.0077329874038696
Epoch 870, training loss: 0.6579204797744751 = 0.00791201926767826 + 0.1 * 6.500084400177002
Epoch 870, val loss: 1.0117617845535278
Epoch 880, training loss: 0.6581518054008484 = 0.007699756883084774 + 0.1 * 6.504520416259766
Epoch 880, val loss: 1.0156669616699219
Epoch 890, training loss: 0.659679651260376 = 0.007497355807572603 + 0.1 * 6.521822452545166
Epoch 890, val loss: 1.0195128917694092
Epoch 900, training loss: 0.6568189859390259 = 0.007303653750568628 + 0.1 * 6.495152950286865
Epoch 900, val loss: 1.0233303308486938
Epoch 910, training loss: 0.6573781967163086 = 0.00711800716817379 + 0.1 * 6.5026021003723145
Epoch 910, val loss: 1.0270880460739136
Epoch 920, training loss: 0.6574814319610596 = 0.006940697785466909 + 0.1 * 6.505406856536865
Epoch 920, val loss: 1.0307379961013794
Epoch 930, training loss: 0.6545429229736328 = 0.006770673673599958 + 0.1 * 6.477722644805908
Epoch 930, val loss: 1.034377932548523
Epoch 940, training loss: 0.6558527946472168 = 0.006607207003980875 + 0.1 * 6.49245548248291
Epoch 940, val loss: 1.0379878282546997
Epoch 950, training loss: 0.6558406352996826 = 0.006450742483139038 + 0.1 * 6.493898391723633
Epoch 950, val loss: 1.0414315462112427
Epoch 960, training loss: 0.6540644764900208 = 0.0063003492541611195 + 0.1 * 6.477641582489014
Epoch 960, val loss: 1.0449211597442627
Epoch 970, training loss: 0.6534340381622314 = 0.006155947223305702 + 0.1 * 6.472780704498291
Epoch 970, val loss: 1.0483484268188477
Epoch 980, training loss: 0.6556626558303833 = 0.0060166846960783005 + 0.1 * 6.4964599609375
Epoch 980, val loss: 1.0516964197158813
Epoch 990, training loss: 0.6524848937988281 = 0.005883406847715378 + 0.1 * 6.466014862060547
Epoch 990, val loss: 1.0549964904785156
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 2.801743507385254 = 1.9420565366744995 + 0.1 * 8.596870422363281
Epoch 0, val loss: 1.940210223197937
Epoch 10, training loss: 2.792255401611328 = 1.93257474899292 + 0.1 * 8.596805572509766
Epoch 10, val loss: 1.9310688972473145
Epoch 20, training loss: 2.7802581787109375 = 1.920625925064087 + 0.1 * 8.596322059631348
Epoch 20, val loss: 1.9190423488616943
Epoch 30, training loss: 2.7627415657043457 = 1.9036059379577637 + 0.1 * 8.591355323791504
Epoch 30, val loss: 1.901565432548523
Epoch 40, training loss: 2.733966112136841 = 1.8784443140029907 + 0.1 * 8.555217742919922
Epoch 40, val loss: 1.8758141994476318
Epoch 50, training loss: 2.6814677715301514 = 1.8444464206695557 + 0.1 * 8.37021255493164
Epoch 50, val loss: 1.84247887134552
Epoch 60, training loss: 2.622501850128174 = 1.8081756830215454 + 0.1 * 8.14326286315918
Epoch 60, val loss: 1.8098863363265991
Epoch 70, training loss: 2.5803661346435547 = 1.777024507522583 + 0.1 * 8.033416748046875
Epoch 70, val loss: 1.784794569015503
Epoch 80, training loss: 2.5216548442840576 = 1.7420791387557983 + 0.1 * 7.7957563400268555
Epoch 80, val loss: 1.7537814378738403
Epoch 90, training loss: 2.445025682449341 = 1.697577953338623 + 0.1 * 7.474477767944336
Epoch 90, val loss: 1.7132431268692017
Epoch 100, training loss: 2.3608527183532715 = 1.6396645307540894 + 0.1 * 7.2118821144104
Epoch 100, val loss: 1.663096308708191
Epoch 110, training loss: 2.274526596069336 = 1.5654305219650269 + 0.1 * 7.090961456298828
Epoch 110, val loss: 1.5999033451080322
Epoch 120, training loss: 2.1834189891815186 = 1.478136420249939 + 0.1 * 7.052824974060059
Epoch 120, val loss: 1.5251961946487427
Epoch 130, training loss: 2.0884041786193848 = 1.3856011629104614 + 0.1 * 7.028028964996338
Epoch 130, val loss: 1.450197458267212
Epoch 140, training loss: 1.992910623550415 = 1.2918531894683838 + 0.1 * 7.010573387145996
Epoch 140, val loss: 1.3781977891921997
Epoch 150, training loss: 1.898026704788208 = 1.198736548423767 + 0.1 * 6.992900848388672
Epoch 150, val loss: 1.3077714443206787
Epoch 160, training loss: 1.8043224811553955 = 1.1069645881652832 + 0.1 * 6.973577976226807
Epoch 160, val loss: 1.239897608757019
Epoch 170, training loss: 1.714121699333191 = 1.0178025960922241 + 0.1 * 6.963191032409668
Epoch 170, val loss: 1.174686312675476
Epoch 180, training loss: 1.62786865234375 = 0.933125913143158 + 0.1 * 6.947427749633789
Epoch 180, val loss: 1.113074779510498
Epoch 190, training loss: 1.546393632888794 = 0.8527784943580627 + 0.1 * 6.936150550842285
Epoch 190, val loss: 1.054858684539795
Epoch 200, training loss: 1.4701030254364014 = 0.7774633765220642 + 0.1 * 6.92639684677124
Epoch 200, val loss: 1.0015698671340942
Epoch 210, training loss: 1.3999474048614502 = 0.7079827189445496 + 0.1 * 6.919646739959717
Epoch 210, val loss: 0.9544622302055359
Epoch 220, training loss: 1.3355729579925537 = 0.6449118256568909 + 0.1 * 6.906610488891602
Epoch 220, val loss: 0.9147753715515137
Epoch 230, training loss: 1.2775540351867676 = 0.587673008441925 + 0.1 * 6.898810863494873
Epoch 230, val loss: 0.8822863698005676
Epoch 240, training loss: 1.2243146896362305 = 0.5358792543411255 + 0.1 * 6.8843536376953125
Epoch 240, val loss: 0.8562919497489929
Epoch 250, training loss: 1.1759343147277832 = 0.4886067807674408 + 0.1 * 6.873274803161621
Epoch 250, val loss: 0.8358871340751648
Epoch 260, training loss: 1.1319758892059326 = 0.445371150970459 + 0.1 * 6.8660478591918945
Epoch 260, val loss: 0.8199759721755981
Epoch 270, training loss: 1.0903631448745728 = 0.405416339635849 + 0.1 * 6.849467754364014
Epoch 270, val loss: 0.8074864745140076
Epoch 280, training loss: 1.0534440279006958 = 0.3677769899368286 + 0.1 * 6.856670379638672
Epoch 280, val loss: 0.797728955745697
Epoch 290, training loss: 1.015802025794983 = 0.33251258730888367 + 0.1 * 6.8328938484191895
Epoch 290, val loss: 0.789951741695404
Epoch 300, training loss: 0.9813780784606934 = 0.2991716265678406 + 0.1 * 6.822064399719238
Epoch 300, val loss: 0.7837551236152649
Epoch 310, training loss: 0.9492810964584351 = 0.2678191363811493 + 0.1 * 6.814619064331055
Epoch 310, val loss: 0.7789417505264282
Epoch 320, training loss: 0.9216874241828918 = 0.23884117603302002 + 0.1 * 6.828462600708008
Epoch 320, val loss: 0.7755590081214905
Epoch 330, training loss: 0.893155574798584 = 0.21281157433986664 + 0.1 * 6.803439617156982
Epoch 330, val loss: 0.7737451791763306
Epoch 340, training loss: 0.8700602054595947 = 0.18957000970840454 + 0.1 * 6.804902076721191
Epoch 340, val loss: 0.7737351655960083
Epoch 350, training loss: 0.8484220504760742 = 0.16898413002490997 + 0.1 * 6.794379234313965
Epoch 350, val loss: 0.7756876349449158
Epoch 360, training loss: 0.8296389579772949 = 0.1507609784603119 + 0.1 * 6.7887797355651855
Epoch 360, val loss: 0.7793459892272949
Epoch 370, training loss: 0.8147227168083191 = 0.13464315235614777 + 0.1 * 6.800795555114746
Epoch 370, val loss: 0.784567654132843
Epoch 380, training loss: 0.7984572649002075 = 0.12052006274461746 + 0.1 * 6.779371738433838
Epoch 380, val loss: 0.7912988066673279
Epoch 390, training loss: 0.7857515215873718 = 0.1080884337425232 + 0.1 * 6.776630878448486
Epoch 390, val loss: 0.7991039752960205
Epoch 400, training loss: 0.7755130529403687 = 0.0971807911992073 + 0.1 * 6.783322811126709
Epoch 400, val loss: 0.8076933026313782
Epoch 410, training loss: 0.7643724679946899 = 0.0876220315694809 + 0.1 * 6.7675042152404785
Epoch 410, val loss: 0.8170455694198608
Epoch 420, training loss: 0.7549232244491577 = 0.07918252795934677 + 0.1 * 6.757407188415527
Epoch 420, val loss: 0.8269051313400269
Epoch 430, training loss: 0.7492296099662781 = 0.07171081751585007 + 0.1 * 6.775187969207764
Epoch 430, val loss: 0.8371791243553162
Epoch 440, training loss: 0.7417029142379761 = 0.0651656910777092 + 0.1 * 6.765372276306152
Epoch 440, val loss: 0.8475453853607178
Epoch 450, training loss: 0.7335691452026367 = 0.0593891516327858 + 0.1 * 6.741799354553223
Epoch 450, val loss: 0.8580502867698669
Epoch 460, training loss: 0.7278928756713867 = 0.05424521863460541 + 0.1 * 6.736476421356201
Epoch 460, val loss: 0.8685585856437683
Epoch 470, training loss: 0.7237376570701599 = 0.04965133219957352 + 0.1 * 6.74086332321167
Epoch 470, val loss: 0.8790563941001892
Epoch 480, training loss: 0.7182482481002808 = 0.04557385295629501 + 0.1 * 6.726744174957275
Epoch 480, val loss: 0.8893886208534241
Epoch 490, training loss: 0.7136008143424988 = 0.04194331169128418 + 0.1 * 6.716574668884277
Epoch 490, val loss: 0.8997767567634583
Epoch 500, training loss: 0.7114467620849609 = 0.0386977456510067 + 0.1 * 6.727490425109863
Epoch 500, val loss: 0.9096798896789551
Epoch 510, training loss: 0.7065239548683167 = 0.035808686167001724 + 0.1 * 6.707152366638184
Epoch 510, val loss: 0.9196803569793701
Epoch 520, training loss: 0.7029792070388794 = 0.033215004950761795 + 0.1 * 6.697641849517822
Epoch 520, val loss: 0.9293803572654724
Epoch 530, training loss: 0.6997618675231934 = 0.030877886340022087 + 0.1 * 6.688839912414551
Epoch 530, val loss: 0.9389654994010925
Epoch 540, training loss: 0.6994714736938477 = 0.028776036575436592 + 0.1 * 6.706954002380371
Epoch 540, val loss: 0.9480809569358826
Epoch 550, training loss: 0.6951434016227722 = 0.02689005434513092 + 0.1 * 6.682533264160156
Epoch 550, val loss: 0.9575071930885315
Epoch 560, training loss: 0.6930720806121826 = 0.02518334984779358 + 0.1 * 6.678887367248535
Epoch 560, val loss: 0.9664846062660217
Epoch 570, training loss: 0.6904639005661011 = 0.0236371997743845 + 0.1 * 6.668267250061035
Epoch 570, val loss: 0.9751217365264893
Epoch 580, training loss: 0.6883999109268188 = 0.022231992334127426 + 0.1 * 6.661679267883301
Epoch 580, val loss: 0.9839236736297607
Epoch 590, training loss: 0.6871511340141296 = 0.02095254883170128 + 0.1 * 6.661985874176025
Epoch 590, val loss: 0.9921700954437256
Epoch 600, training loss: 0.6850119233131409 = 0.019786490127444267 + 0.1 * 6.652254104614258
Epoch 600, val loss: 1.000566840171814
Epoch 610, training loss: 0.6828435063362122 = 0.018718644976615906 + 0.1 * 6.6412482261657715
Epoch 610, val loss: 1.008499264717102
Epoch 620, training loss: 0.6820313930511475 = 0.017738645896315575 + 0.1 * 6.642927169799805
Epoch 620, val loss: 1.0164631605148315
Epoch 630, training loss: 0.6798508763313293 = 0.01683650352060795 + 0.1 * 6.630143165588379
Epoch 630, val loss: 1.0242459774017334
Epoch 640, training loss: 0.6819822192192078 = 0.01600280962884426 + 0.1 * 6.659793853759766
Epoch 640, val loss: 1.0318472385406494
Epoch 650, training loss: 0.6779788136482239 = 0.01523617934435606 + 0.1 * 6.6274261474609375
Epoch 650, val loss: 1.0390543937683105
Epoch 660, training loss: 0.6766660213470459 = 0.014527375809848309 + 0.1 * 6.621386528015137
Epoch 660, val loss: 1.046459674835205
Epoch 670, training loss: 0.6758800148963928 = 0.013869383372366428 + 0.1 * 6.620105743408203
Epoch 670, val loss: 1.0533398389816284
Epoch 680, training loss: 0.675247848033905 = 0.013258034363389015 + 0.1 * 6.619897842407227
Epoch 680, val loss: 1.060212254524231
Epoch 690, training loss: 0.6740929484367371 = 0.012688415125012398 + 0.1 * 6.6140456199646
Epoch 690, val loss: 1.067046046257019
Epoch 700, training loss: 0.6727701425552368 = 0.012156867422163486 + 0.1 * 6.606132984161377
Epoch 700, val loss: 1.0735130310058594
Epoch 710, training loss: 0.672343373298645 = 0.011660831049084663 + 0.1 * 6.606825351715088
Epoch 710, val loss: 1.0800299644470215
Epoch 720, training loss: 0.6720901727676392 = 0.011196675710380077 + 0.1 * 6.6089348793029785
Epoch 720, val loss: 1.086333155632019
Epoch 730, training loss: 0.6709423065185547 = 0.010762518271803856 + 0.1 * 6.601798057556152
Epoch 730, val loss: 1.0924842357635498
Epoch 740, training loss: 0.6706673502922058 = 0.010354570113122463 + 0.1 * 6.603127479553223
Epoch 740, val loss: 1.098609209060669
Epoch 750, training loss: 0.6692140102386475 = 0.00997124332934618 + 0.1 * 6.5924272537231445
Epoch 750, val loss: 1.1044344902038574
Epoch 760, training loss: 0.669348418712616 = 0.009610399603843689 + 0.1 * 6.597379684448242
Epoch 760, val loss: 1.110268235206604
Epoch 770, training loss: 0.6686420440673828 = 0.009271789342164993 + 0.1 * 6.59370231628418
Epoch 770, val loss: 1.1157755851745605
Epoch 780, training loss: 0.6679915189743042 = 0.008953031152486801 + 0.1 * 6.590384483337402
Epoch 780, val loss: 1.121418833732605
Epoch 790, training loss: 0.6669785976409912 = 0.008651591837406158 + 0.1 * 6.5832695960998535
Epoch 790, val loss: 1.1270052194595337
Epoch 800, training loss: 0.6672078967094421 = 0.008365636691451073 + 0.1 * 6.588422775268555
Epoch 800, val loss: 1.1323193311691284
Epoch 810, training loss: 0.6669328212738037 = 0.008095133118331432 + 0.1 * 6.588376522064209
Epoch 810, val loss: 1.1374958753585815
Epoch 820, training loss: 0.6661405563354492 = 0.007839472033083439 + 0.1 * 6.583010673522949
Epoch 820, val loss: 1.1427687406539917
Epoch 830, training loss: 0.6662119626998901 = 0.007597141433507204 + 0.1 * 6.586147785186768
Epoch 830, val loss: 1.1476103067398071
Epoch 840, training loss: 0.6642631888389587 = 0.0073682451620697975 + 0.1 * 6.568949222564697
Epoch 840, val loss: 1.1526386737823486
Epoch 850, training loss: 0.6648533940315247 = 0.007150257471948862 + 0.1 * 6.57703161239624
Epoch 850, val loss: 1.1576741933822632
Epoch 860, training loss: 0.664121150970459 = 0.006942653097212315 + 0.1 * 6.571784973144531
Epoch 860, val loss: 1.1621843576431274
Epoch 870, training loss: 0.6637867093086243 = 0.006745696533471346 + 0.1 * 6.570410251617432
Epoch 870, val loss: 1.1668962240219116
Epoch 880, training loss: 0.6632941961288452 = 0.006558097433298826 + 0.1 * 6.567360877990723
Epoch 880, val loss: 1.1716326475143433
Epoch 890, training loss: 0.662477970123291 = 0.006378527265042067 + 0.1 * 6.5609941482543945
Epoch 890, val loss: 1.1761629581451416
Epoch 900, training loss: 0.6628676652908325 = 0.006207046564668417 + 0.1 * 6.566606044769287
Epoch 900, val loss: 1.1803803443908691
Epoch 910, training loss: 0.6616458296775818 = 0.0060445102863013744 + 0.1 * 6.556013107299805
Epoch 910, val loss: 1.1847038269042969
Epoch 920, training loss: 0.6620297431945801 = 0.005888776853680611 + 0.1 * 6.5614094734191895
Epoch 920, val loss: 1.1890686750411987
Epoch 930, training loss: 0.6606032252311707 = 0.005739922635257244 + 0.1 * 6.548633098602295
Epoch 930, val loss: 1.1932415962219238
Epoch 940, training loss: 0.6652912497520447 = 0.005597186740487814 + 0.1 * 6.596940517425537
Epoch 940, val loss: 1.197378158569336
Epoch 950, training loss: 0.6615689396858215 = 0.005461004097014666 + 0.1 * 6.561079502105713
Epoch 950, val loss: 1.2009848356246948
Epoch 960, training loss: 0.6599982976913452 = 0.005331162828952074 + 0.1 * 6.546671390533447
Epoch 960, val loss: 1.2050857543945312
Epoch 970, training loss: 0.6596081852912903 = 0.005206460133194923 + 0.1 * 6.544017314910889
Epoch 970, val loss: 1.2092088460922241
Epoch 980, training loss: 0.6603682637214661 = 0.005085884593427181 + 0.1 * 6.552823543548584
Epoch 980, val loss: 1.2128551006317139
Epoch 990, training loss: 0.6587539911270142 = 0.004970661364495754 + 0.1 * 6.537833213806152
Epoch 990, val loss: 1.2163783311843872
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 2.8035998344421387 = 1.9439117908477783 + 0.1 * 8.596879005432129
Epoch 0, val loss: 1.9363113641738892
Epoch 10, training loss: 2.7940378189086914 = 1.9343562126159668 + 0.1 * 8.59681510925293
Epoch 10, val loss: 1.9270272254943848
Epoch 20, training loss: 2.7819664478302 = 1.92232084274292 + 0.1 * 8.596455574035645
Epoch 20, val loss: 1.9148882627487183
Epoch 30, training loss: 2.7646665573120117 = 1.9053391218185425 + 0.1 * 8.59327507019043
Epoch 30, val loss: 1.8973573446273804
Epoch 40, training loss: 2.7373127937316895 = 1.8803133964538574 + 0.1 * 8.569994926452637
Epoch 40, val loss: 1.8715649843215942
Epoch 50, training loss: 2.6928701400756836 = 1.846014380455017 + 0.1 * 8.468558311462402
Epoch 50, val loss: 1.8380842208862305
Epoch 60, training loss: 2.621875047683716 = 1.8091903924942017 + 0.1 * 8.126846313476562
Epoch 60, val loss: 1.8069038391113281
Epoch 70, training loss: 2.5799577236175537 = 1.7781320810317993 + 0.1 * 8.018256187438965
Epoch 70, val loss: 1.784484624862671
Epoch 80, training loss: 2.5204927921295166 = 1.7432512044906616 + 0.1 * 7.772416114807129
Epoch 80, val loss: 1.7561626434326172
Epoch 90, training loss: 2.447161912918091 = 1.6985056400299072 + 0.1 * 7.4865617752075195
Epoch 90, val loss: 1.717423439025879
Epoch 100, training loss: 2.3678348064422607 = 1.6388051509857178 + 0.1 * 7.290297031402588
Epoch 100, val loss: 1.6667535305023193
Epoch 110, training loss: 2.27939510345459 = 1.5610727071762085 + 0.1 * 7.183224201202393
Epoch 110, val loss: 1.600663661956787
Epoch 120, training loss: 2.1796326637268066 = 1.4692070484161377 + 0.1 * 7.1042561531066895
Epoch 120, val loss: 1.5247975587844849
Epoch 130, training loss: 2.0767483711242676 = 1.3703726530075073 + 0.1 * 7.063756465911865
Epoch 130, val loss: 1.4463443756103516
Epoch 140, training loss: 1.9729664325714111 = 1.268271803855896 + 0.1 * 7.046945571899414
Epoch 140, val loss: 1.367288589477539
Epoch 150, training loss: 1.8690851926803589 = 1.165557861328125 + 0.1 * 7.03527307510376
Epoch 150, val loss: 1.2907744646072388
Epoch 160, training loss: 1.7680847644805908 = 1.0656858682632446 + 0.1 * 7.023989200592041
Epoch 160, val loss: 1.2179054021835327
Epoch 170, training loss: 1.6724791526794434 = 0.9716575741767883 + 0.1 * 7.008214950561523
Epoch 170, val loss: 1.1509038209915161
Epoch 180, training loss: 1.5838508605957031 = 0.8855090141296387 + 0.1 * 6.9834184646606445
Epoch 180, val loss: 1.0918312072753906
Epoch 190, training loss: 1.5034778118133545 = 0.8074246048927307 + 0.1 * 6.960532188415527
Epoch 190, val loss: 1.0403412580490112
Epoch 200, training loss: 1.430584192276001 = 0.7379986643791199 + 0.1 * 6.925854682922363
Epoch 200, val loss: 0.9982050061225891
Epoch 210, training loss: 1.3673934936523438 = 0.676288366317749 + 0.1 * 6.911050319671631
Epoch 210, val loss: 0.9646933078765869
Epoch 220, training loss: 1.3100972175598145 = 0.621865451335907 + 0.1 * 6.882317066192627
Epoch 220, val loss: 0.9400408864021301
Epoch 230, training loss: 1.2595311403274536 = 0.5728830099105835 + 0.1 * 6.866481304168701
Epoch 230, val loss: 0.9221177101135254
Epoch 240, training loss: 1.2135863304138184 = 0.5282239317893982 + 0.1 * 6.853623867034912
Epoch 240, val loss: 0.9095320701599121
Epoch 250, training loss: 1.1710622310638428 = 0.48701632022857666 + 0.1 * 6.84045934677124
Epoch 250, val loss: 0.9007276892662048
Epoch 260, training loss: 1.1313058137893677 = 0.4483383595943451 + 0.1 * 6.829674243927002
Epoch 260, val loss: 0.8943309783935547
Epoch 270, training loss: 1.09395170211792 = 0.4117220640182495 + 0.1 * 6.822297096252441
Epoch 270, val loss: 0.8894824981689453
Epoch 280, training loss: 1.0579081773757935 = 0.37700361013412476 + 0.1 * 6.809045314788818
Epoch 280, val loss: 0.8858129978179932
Epoch 290, training loss: 1.0258173942565918 = 0.34441739320755005 + 0.1 * 6.813999176025391
Epoch 290, val loss: 0.8831623196601868
Epoch 300, training loss: 0.9944326877593994 = 0.31439438462257385 + 0.1 * 6.8003830909729
Epoch 300, val loss: 0.8815934658050537
Epoch 310, training loss: 0.9658745527267456 = 0.2866500914096832 + 0.1 * 6.792243957519531
Epoch 310, val loss: 0.88102126121521
Epoch 320, training loss: 0.9394741654396057 = 0.26092809438705444 + 0.1 * 6.785460472106934
Epoch 320, val loss: 0.88153475522995
Epoch 330, training loss: 0.9172264337539673 = 0.2369559109210968 + 0.1 * 6.802704811096191
Epoch 330, val loss: 0.8830878138542175
Epoch 340, training loss: 0.8925755023956299 = 0.21482093632221222 + 0.1 * 6.77754545211792
Epoch 340, val loss: 0.8856181502342224
Epoch 350, training loss: 0.8714919090270996 = 0.19430549442768097 + 0.1 * 6.771864414215088
Epoch 350, val loss: 0.8890560269355774
Epoch 360, training loss: 0.8521102070808411 = 0.17543542385101318 + 0.1 * 6.766747951507568
Epoch 360, val loss: 0.89371657371521
Epoch 370, training loss: 0.8347687721252441 = 0.1583842635154724 + 0.1 * 6.763844966888428
Epoch 370, val loss: 0.8994576334953308
Epoch 380, training loss: 0.8186557292938232 = 0.14314398169517517 + 0.1 * 6.755117893218994
Epoch 380, val loss: 0.9062485098838806
Epoch 390, training loss: 0.8048118352890015 = 0.12954865396022797 + 0.1 * 6.752631664276123
Epoch 390, val loss: 0.914323091506958
Epoch 400, training loss: 0.7918547987937927 = 0.11750055849552155 + 0.1 * 6.743542671203613
Epoch 400, val loss: 0.9235848188400269
Epoch 410, training loss: 0.7803199291229248 = 0.10677936673164368 + 0.1 * 6.735404968261719
Epoch 410, val loss: 0.9338177442550659
Epoch 420, training loss: 0.7713637351989746 = 0.09722765535116196 + 0.1 * 6.741360664367676
Epoch 420, val loss: 0.9449753165245056
Epoch 430, training loss: 0.7604795098304749 = 0.08871020376682281 + 0.1 * 6.717693328857422
Epoch 430, val loss: 0.956839919090271
Epoch 440, training loss: 0.7521921992301941 = 0.08106847107410431 + 0.1 * 6.71123743057251
Epoch 440, val loss: 0.9695383906364441
Epoch 450, training loss: 0.7446050047874451 = 0.07420826703310013 + 0.1 * 6.703967094421387
Epoch 450, val loss: 0.9827633500099182
Epoch 460, training loss: 0.7387920618057251 = 0.06804882735013962 + 0.1 * 6.707432270050049
Epoch 460, val loss: 0.9963200092315674
Epoch 470, training loss: 0.731352686882019 = 0.0625377744436264 + 0.1 * 6.6881489753723145
Epoch 470, val loss: 1.010192632675171
Epoch 480, training loss: 0.7255728840827942 = 0.0575929693877697 + 0.1 * 6.6797990798950195
Epoch 480, val loss: 1.0241066217422485
Epoch 490, training loss: 0.7226820588111877 = 0.0531427301466465 + 0.1 * 6.695393085479736
Epoch 490, val loss: 1.038175106048584
Epoch 500, training loss: 0.7162474989891052 = 0.04915553703904152 + 0.1 * 6.670919418334961
Epoch 500, val loss: 1.0521260499954224
Epoch 510, training loss: 0.7122317552566528 = 0.04556848853826523 + 0.1 * 6.666632652282715
Epoch 510, val loss: 1.065872073173523
Epoch 520, training loss: 0.7088476419448853 = 0.04233866184949875 + 0.1 * 6.6650896072387695
Epoch 520, val loss: 1.0793635845184326
Epoch 530, training loss: 0.7049030065536499 = 0.03942570462822914 + 0.1 * 6.654772758483887
Epoch 530, val loss: 1.0927022695541382
Epoch 540, training loss: 0.7024679780006409 = 0.0367862768471241 + 0.1 * 6.6568169593811035
Epoch 540, val loss: 1.105919361114502
Epoch 550, training loss: 0.6999490261077881 = 0.034392427653074265 + 0.1 * 6.655566215515137
Epoch 550, val loss: 1.1188080310821533
Epoch 560, training loss: 0.6964904069900513 = 0.03222033008933067 + 0.1 * 6.642701148986816
Epoch 560, val loss: 1.1313570737838745
Epoch 570, training loss: 0.6943243145942688 = 0.030245399102568626 + 0.1 * 6.640788555145264
Epoch 570, val loss: 1.1437740325927734
Epoch 580, training loss: 0.6922537088394165 = 0.028440039604902267 + 0.1 * 6.638136386871338
Epoch 580, val loss: 1.1559007167816162
Epoch 590, training loss: 0.6899712681770325 = 0.026787366718053818 + 0.1 * 6.631839275360107
Epoch 590, val loss: 1.1677495241165161
Epoch 600, training loss: 0.6887245178222656 = 0.025273354724049568 + 0.1 * 6.634511470794678
Epoch 600, val loss: 1.1792832612991333
Epoch 610, training loss: 0.686034083366394 = 0.023885197937488556 + 0.1 * 6.62148904800415
Epoch 610, val loss: 1.190661072731018
Epoch 620, training loss: 0.6855112910270691 = 0.022608324885368347 + 0.1 * 6.629029273986816
Epoch 620, val loss: 1.2017619609832764
Epoch 630, training loss: 0.6832002997398376 = 0.021431561559438705 + 0.1 * 6.617687225341797
Epoch 630, val loss: 1.2126742601394653
Epoch 640, training loss: 0.6832098364830017 = 0.020345719531178474 + 0.1 * 6.628640651702881
Epoch 640, val loss: 1.2230640649795532
Epoch 650, training loss: 0.6800951957702637 = 0.019345803186297417 + 0.1 * 6.607493877410889
Epoch 650, val loss: 1.2335405349731445
Epoch 660, training loss: 0.6791557669639587 = 0.01842021942138672 + 0.1 * 6.60735559463501
Epoch 660, val loss: 1.243505835533142
Epoch 670, training loss: 0.6772392988204956 = 0.01756049133837223 + 0.1 * 6.596787929534912
Epoch 670, val loss: 1.2532193660736084
Epoch 680, training loss: 0.6757895350456238 = 0.01676304265856743 + 0.1 * 6.590264797210693
Epoch 680, val loss: 1.2629550695419312
Epoch 690, training loss: 0.6749575138092041 = 0.016019929200410843 + 0.1 * 6.5893754959106445
Epoch 690, val loss: 1.2723946571350098
Epoch 700, training loss: 0.6756200790405273 = 0.015326527878642082 + 0.1 * 6.602935791015625
Epoch 700, val loss: 1.2811506986618042
Epoch 710, training loss: 0.6734049320220947 = 0.014683304354548454 + 0.1 * 6.587215900421143
Epoch 710, val loss: 1.290357232093811
Epoch 720, training loss: 0.671363115310669 = 0.014081507921218872 + 0.1 * 6.572816371917725
Epoch 720, val loss: 1.2989537715911865
Epoch 730, training loss: 0.6719147562980652 = 0.01351670827716589 + 0.1 * 6.583980083465576
Epoch 730, val loss: 1.3074995279312134
Epoch 740, training loss: 0.6693922281265259 = 0.012986624613404274 + 0.1 * 6.564056396484375
Epoch 740, val loss: 1.3158994913101196
Epoch 750, training loss: 0.6698632836341858 = 0.01248871348798275 + 0.1 * 6.573745250701904
Epoch 750, val loss: 1.323899745941162
Epoch 760, training loss: 0.6690827012062073 = 0.01202130876481533 + 0.1 * 6.570613861083984
Epoch 760, val loss: 1.3319319486618042
Epoch 770, training loss: 0.6684226393699646 = 0.01158226653933525 + 0.1 * 6.568403720855713
Epoch 770, val loss: 1.3395119905471802
Epoch 780, training loss: 0.6659671068191528 = 0.011169361881911755 + 0.1 * 6.547977447509766
Epoch 780, val loss: 1.3472403287887573
Epoch 790, training loss: 0.6657482981681824 = 0.010778370313346386 + 0.1 * 6.549699306488037
Epoch 790, val loss: 1.3546675443649292
Epoch 800, training loss: 0.665003776550293 = 0.010408476926386356 + 0.1 * 6.545953273773193
Epoch 800, val loss: 1.3618031740188599
Epoch 810, training loss: 0.664874792098999 = 0.010059460997581482 + 0.1 * 6.548152923583984
Epoch 810, val loss: 1.3689988851547241
Epoch 820, training loss: 0.6635183095932007 = 0.009728914126753807 + 0.1 * 6.537893772125244
Epoch 820, val loss: 1.3758410215377808
Epoch 830, training loss: 0.6642785668373108 = 0.009416118264198303 + 0.1 * 6.548624515533447
Epoch 830, val loss: 1.3826886415481567
Epoch 840, training loss: 0.6634842753410339 = 0.009118695743381977 + 0.1 * 6.543655872344971
Epoch 840, val loss: 1.389268398284912
Epoch 850, training loss: 0.6626406311988831 = 0.008837043307721615 + 0.1 * 6.538035869598389
Epoch 850, val loss: 1.3959499597549438
Epoch 860, training loss: 0.6616560220718384 = 0.008568547666072845 + 0.1 * 6.530874729156494
Epoch 860, val loss: 1.4021518230438232
Epoch 870, training loss: 0.6617109775543213 = 0.00831374991685152 + 0.1 * 6.533971786499023
Epoch 870, val loss: 1.4085251092910767
Epoch 880, training loss: 0.6616753935813904 = 0.008071104064583778 + 0.1 * 6.5360426902771
Epoch 880, val loss: 1.414560079574585
Epoch 890, training loss: 0.6600807309150696 = 0.007840529084205627 + 0.1 * 6.522401809692383
Epoch 890, val loss: 1.420659065246582
Epoch 900, training loss: 0.6605419516563416 = 0.007620377000421286 + 0.1 * 6.529215335845947
Epoch 900, val loss: 1.4265786409378052
Epoch 910, training loss: 0.658848762512207 = 0.007410170044749975 + 0.1 * 6.51438570022583
Epoch 910, val loss: 1.4323015213012695
Epoch 920, training loss: 0.6606209874153137 = 0.0072093564085662365 + 0.1 * 6.534116268157959
Epoch 920, val loss: 1.437943458557129
Epoch 930, training loss: 0.6587504148483276 = 0.007018051575869322 + 0.1 * 6.517323970794678
Epoch 930, val loss: 1.4434394836425781
Epoch 940, training loss: 0.6589027047157288 = 0.006835304666310549 + 0.1 * 6.520674228668213
Epoch 940, val loss: 1.449034571647644
Epoch 950, training loss: 0.6573436260223389 = 0.00665988028049469 + 0.1 * 6.506837368011475
Epoch 950, val loss: 1.4544179439544678
Epoch 960, training loss: 0.6592448949813843 = 0.00649179657921195 + 0.1 * 6.527531147003174
Epoch 960, val loss: 1.4596638679504395
Epoch 970, training loss: 0.6571509838104248 = 0.006330941338092089 + 0.1 * 6.508200168609619
Epoch 970, val loss: 1.4649497270584106
Epoch 980, training loss: 0.6575949788093567 = 0.006176547612994909 + 0.1 * 6.514184474945068
Epoch 980, val loss: 1.4700889587402344
Epoch 990, training loss: 0.6568174958229065 = 0.006028461270034313 + 0.1 * 6.507890224456787
Epoch 990, val loss: 1.474860429763794
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8133895624670533
The final CL Acc:0.77037, 0.01571, The final GNN Acc:0.81163, 0.00151
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13082])
remove edge: torch.Size([2, 7866])
updated graph: torch.Size([2, 10392])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.808720111846924 = 1.9490342140197754 + 0.1 * 8.596858978271484
Epoch 0, val loss: 1.9504286050796509
Epoch 10, training loss: 2.7976467609405518 = 1.9379706382751465 + 0.1 * 8.596760749816895
Epoch 10, val loss: 1.939067006111145
Epoch 20, training loss: 2.78364896774292 = 1.9240368604660034 + 0.1 * 8.59611988067627
Epoch 20, val loss: 1.9247486591339111
Epoch 30, training loss: 2.7632009983062744 = 1.9041885137557983 + 0.1 * 8.590124130249023
Epoch 30, val loss: 1.9044320583343506
Epoch 40, training loss: 2.7301342487335205 = 1.8750215768814087 + 0.1 * 8.551127433776855
Epoch 40, val loss: 1.875052571296692
Epoch 50, training loss: 2.673429489135742 = 1.8370901346206665 + 0.1 * 8.363393783569336
Epoch 50, val loss: 1.8394166231155396
Epoch 60, training loss: 2.6078851222991943 = 1.7997432947158813 + 0.1 * 8.08141803741455
Epoch 60, val loss: 1.8080109357833862
Epoch 70, training loss: 2.5578367710113525 = 1.7675564289093018 + 0.1 * 7.902802467346191
Epoch 70, val loss: 1.7812974452972412
Epoch 80, training loss: 2.483318328857422 = 1.7290897369384766 + 0.1 * 7.5422868728637695
Epoch 80, val loss: 1.746026873588562
Epoch 90, training loss: 2.4075918197631836 = 1.678712010383606 + 0.1 * 7.2887983322143555
Epoch 90, val loss: 1.6998575925827026
Epoch 100, training loss: 2.33077073097229 = 1.6117539405822754 + 0.1 * 7.190168380737305
Epoch 100, val loss: 1.6418218612670898
Epoch 110, training loss: 2.2443103790283203 = 1.5313780307769775 + 0.1 * 7.129323482513428
Epoch 110, val loss: 1.5740206241607666
Epoch 120, training loss: 2.156285524368286 = 1.4492154121398926 + 0.1 * 7.0707011222839355
Epoch 120, val loss: 1.5057460069656372
Epoch 130, training loss: 2.071951389312744 = 1.3719414472579956 + 0.1 * 7.000099182128906
Epoch 130, val loss: 1.4437047243118286
Epoch 140, training loss: 1.9923322200775146 = 1.298007845878601 + 0.1 * 6.943243980407715
Epoch 140, val loss: 1.3846286535263062
Epoch 150, training loss: 1.9143215417861938 = 1.2230830192565918 + 0.1 * 6.912384986877441
Epoch 150, val loss: 1.3241386413574219
Epoch 160, training loss: 1.8351225852966309 = 1.1454071998596191 + 0.1 * 6.897154331207275
Epoch 160, val loss: 1.2616393566131592
Epoch 170, training loss: 1.753328800201416 = 1.0655345916748047 + 0.1 * 6.877941131591797
Epoch 170, val loss: 1.1980265378952026
Epoch 180, training loss: 1.6685843467712402 = 0.9818600416183472 + 0.1 * 6.867242336273193
Epoch 180, val loss: 1.130547285079956
Epoch 190, training loss: 1.5815417766571045 = 0.8959519863128662 + 0.1 * 6.855896949768066
Epoch 190, val loss: 1.060947060585022
Epoch 200, training loss: 1.4965133666992188 = 0.8119063973426819 + 0.1 * 6.846068859100342
Epoch 200, val loss: 0.9936783313751221
Epoch 210, training loss: 1.4184372425079346 = 0.7354013919830322 + 0.1 * 6.830358028411865
Epoch 210, val loss: 0.9350530505180359
Epoch 220, training loss: 1.3498308658599854 = 0.6681614518165588 + 0.1 * 6.816693305969238
Epoch 220, val loss: 0.8871977925300598
Epoch 230, training loss: 1.2915328741073608 = 0.6106004118919373 + 0.1 * 6.809324741363525
Epoch 230, val loss: 0.8511848449707031
Epoch 240, training loss: 1.2408902645111084 = 0.5615530610084534 + 0.1 * 6.793372631072998
Epoch 240, val loss: 0.8253043293952942
Epoch 250, training loss: 1.1975419521331787 = 0.5190706253051758 + 0.1 * 6.784713268280029
Epoch 250, val loss: 0.8072395324707031
Epoch 260, training loss: 1.1597139835357666 = 0.48164206743240356 + 0.1 * 6.7807183265686035
Epoch 260, val loss: 0.7948810458183289
Epoch 270, training loss: 1.125045895576477 = 0.448106974363327 + 0.1 * 6.769389629364014
Epoch 270, val loss: 0.7864145040512085
Epoch 280, training loss: 1.0933963060379028 = 0.4170185923576355 + 0.1 * 6.763777256011963
Epoch 280, val loss: 0.7802221775054932
Epoch 290, training loss: 1.0629080533981323 = 0.3872983157634735 + 0.1 * 6.756097793579102
Epoch 290, val loss: 0.7753884792327881
Epoch 300, training loss: 1.0361697673797607 = 0.3584463596343994 + 0.1 * 6.777234077453613
Epoch 300, val loss: 0.7715482711791992
Epoch 310, training loss: 1.005037546157837 = 0.3306869864463806 + 0.1 * 6.743505954742432
Epoch 310, val loss: 0.7684236764907837
Epoch 320, training loss: 0.9768424034118652 = 0.3038542866706848 + 0.1 * 6.7298808097839355
Epoch 320, val loss: 0.7662017345428467
Epoch 330, training loss: 0.9513623714447021 = 0.27794450521469116 + 0.1 * 6.73417854309082
Epoch 330, val loss: 0.7647066116333008
Epoch 340, training loss: 0.9249205589294434 = 0.2532765865325928 + 0.1 * 6.716439723968506
Epoch 340, val loss: 0.763987123966217
Epoch 350, training loss: 0.8995936512947083 = 0.22971360385417938 + 0.1 * 6.698800086975098
Epoch 350, val loss: 0.7638508081436157
Epoch 360, training loss: 0.8811489343643188 = 0.2071940302848816 + 0.1 * 6.739549160003662
Epoch 360, val loss: 0.7642872333526611
Epoch 370, training loss: 0.855074942111969 = 0.18610288202762604 + 0.1 * 6.689720630645752
Epoch 370, val loss: 0.7651983499526978
Epoch 380, training loss: 0.8339312672615051 = 0.16649217903614044 + 0.1 * 6.67439079284668
Epoch 380, val loss: 0.7669353485107422
Epoch 390, training loss: 0.8143541812896729 = 0.1485050618648529 + 0.1 * 6.658491611480713
Epoch 390, val loss: 0.7696231007575989
Epoch 400, training loss: 0.798751950263977 = 0.13233891129493713 + 0.1 * 6.664130210876465
Epoch 400, val loss: 0.7735862731933594
Epoch 410, training loss: 0.7829334139823914 = 0.1180642619729042 + 0.1 * 6.648691177368164
Epoch 410, val loss: 0.7783554792404175
Epoch 420, training loss: 0.7688980102539062 = 0.10552127659320831 + 0.1 * 6.633767127990723
Epoch 420, val loss: 0.7843049168586731
Epoch 430, training loss: 0.7608224749565125 = 0.09457921981811523 + 0.1 * 6.6624321937561035
Epoch 430, val loss: 0.7911331653594971
Epoch 440, training loss: 0.747786283493042 = 0.08506402373313904 + 0.1 * 6.627222537994385
Epoch 440, val loss: 0.7987052798271179
Epoch 450, training loss: 0.738395094871521 = 0.07674066722393036 + 0.1 * 6.616544246673584
Epoch 450, val loss: 0.8069127202033997
Epoch 460, training loss: 0.7318414449691772 = 0.06944125145673752 + 0.1 * 6.624001502990723
Epoch 460, val loss: 0.8156073689460754
Epoch 470, training loss: 0.724501371383667 = 0.06305456906557083 + 0.1 * 6.614468097686768
Epoch 470, val loss: 0.8246848583221436
Epoch 480, training loss: 0.7176277041435242 = 0.057434145361185074 + 0.1 * 6.601935386657715
Epoch 480, val loss: 0.8338950276374817
Epoch 490, training loss: 0.7125447988510132 = 0.052463795989751816 + 0.1 * 6.6008100509643555
Epoch 490, val loss: 0.8434333205223083
Epoch 500, training loss: 0.7075982689857483 = 0.048052385449409485 + 0.1 * 6.595458984375
Epoch 500, val loss: 0.8530395030975342
Epoch 510, training loss: 0.7053685188293457 = 0.044134173542261124 + 0.1 * 6.6123433113098145
Epoch 510, val loss: 0.8626489043235779
Epoch 520, training loss: 0.6993979215621948 = 0.040650997310876846 + 0.1 * 6.58746862411499
Epoch 520, val loss: 0.8719878196716309
Epoch 530, training loss: 0.6955326795578003 = 0.03752555325627327 + 0.1 * 6.580071449279785
Epoch 530, val loss: 0.8813136219978333
Epoch 540, training loss: 0.6955205202102661 = 0.034709617495536804 + 0.1 * 6.608108997344971
Epoch 540, val loss: 0.8905537724494934
Epoch 550, training loss: 0.6893531680107117 = 0.032187674194574356 + 0.1 * 6.571654796600342
Epoch 550, val loss: 0.8996130228042603
Epoch 560, training loss: 0.6867043972015381 = 0.029912356287240982 + 0.1 * 6.567920207977295
Epoch 560, val loss: 0.9083696603775024
Epoch 570, training loss: 0.6850129961967468 = 0.0278554018586874 + 0.1 * 6.571576118469238
Epoch 570, val loss: 0.9170955419540405
Epoch 580, training loss: 0.6818680763244629 = 0.025998957455158234 + 0.1 * 6.558691024780273
Epoch 580, val loss: 0.9254176020622253
Epoch 590, training loss: 0.680401623249054 = 0.024314967915415764 + 0.1 * 6.560866832733154
Epoch 590, val loss: 0.9336677193641663
Epoch 600, training loss: 0.679043710231781 = 0.022788889706134796 + 0.1 * 6.5625481605529785
Epoch 600, val loss: 0.9417697191238403
Epoch 610, training loss: 0.6762121915817261 = 0.021405400708317757 + 0.1 * 6.548067569732666
Epoch 610, val loss: 0.9494989514350891
Epoch 620, training loss: 0.675697386264801 = 0.02014468051493168 + 0.1 * 6.5555267333984375
Epoch 620, val loss: 0.9570794105529785
Epoch 630, training loss: 0.6737021803855896 = 0.018994519487023354 + 0.1 * 6.54707670211792
Epoch 630, val loss: 0.9645171165466309
Epoch 640, training loss: 0.6717075705528259 = 0.017940524965524673 + 0.1 * 6.537670135498047
Epoch 640, val loss: 0.9716886281967163
Epoch 650, training loss: 0.6705271005630493 = 0.016975801438093185 + 0.1 * 6.535512447357178
Epoch 650, val loss: 0.9787804484367371
Epoch 660, training loss: 0.6691620349884033 = 0.01608908362686634 + 0.1 * 6.530729293823242
Epoch 660, val loss: 0.9855876564979553
Epoch 670, training loss: 0.6683533191680908 = 0.015273753553628922 + 0.1 * 6.530795574188232
Epoch 670, val loss: 0.9923149943351746
Epoch 680, training loss: 0.6677610278129578 = 0.01452447846531868 + 0.1 * 6.532365798950195
Epoch 680, val loss: 0.9988855719566345
Epoch 690, training loss: 0.6663134098052979 = 0.013833791948854923 + 0.1 * 6.524796009063721
Epoch 690, val loss: 1.0050426721572876
Epoch 700, training loss: 0.6650286912918091 = 0.013192125596106052 + 0.1 * 6.518365383148193
Epoch 700, val loss: 1.0111173391342163
Epoch 710, training loss: 0.6644926071166992 = 0.01259624119848013 + 0.1 * 6.51896333694458
Epoch 710, val loss: 1.0172111988067627
Epoch 720, training loss: 0.6642806529998779 = 0.012043789960443974 + 0.1 * 6.522368431091309
Epoch 720, val loss: 1.0230685472488403
Epoch 730, training loss: 0.6625158190727234 = 0.011530888266861439 + 0.1 * 6.5098490715026855
Epoch 730, val loss: 1.028739333152771
Epoch 740, training loss: 0.6621280908584595 = 0.011052003130316734 + 0.1 * 6.51076078414917
Epoch 740, val loss: 1.0341604948043823
Epoch 750, training loss: 0.661887526512146 = 0.010604334063827991 + 0.1 * 6.512832164764404
Epoch 750, val loss: 1.039607286453247
Epoch 760, training loss: 0.6613946557044983 = 0.010186710394918919 + 0.1 * 6.512079238891602
Epoch 760, val loss: 1.0449730157852173
Epoch 770, training loss: 0.659599781036377 = 0.00979670975357294 + 0.1 * 6.498030185699463
Epoch 770, val loss: 1.050016164779663
Epoch 780, training loss: 0.6591851115226746 = 0.00942945759743452 + 0.1 * 6.497556209564209
Epoch 780, val loss: 1.054948091506958
Epoch 790, training loss: 0.6587824821472168 = 0.009084442630410194 + 0.1 * 6.4969801902771
Epoch 790, val loss: 1.0599452257156372
Epoch 800, training loss: 0.6585049033164978 = 0.008761630393564701 + 0.1 * 6.497432231903076
Epoch 800, val loss: 1.0647473335266113
Epoch 810, training loss: 0.6574914455413818 = 0.008456460200250149 + 0.1 * 6.490349769592285
Epoch 810, val loss: 1.0692226886749268
Epoch 820, training loss: 0.65850830078125 = 0.008167562074959278 + 0.1 * 6.503407001495361
Epoch 820, val loss: 1.0738712549209595
Epoch 830, training loss: 0.6561534404754639 = 0.00789621565490961 + 0.1 * 6.482571601867676
Epoch 830, val loss: 1.0785666704177856
Epoch 840, training loss: 0.6556664109230042 = 0.007640918716788292 + 0.1 * 6.480255126953125
Epoch 840, val loss: 1.0827819108963013
Epoch 850, training loss: 0.6573624610900879 = 0.007397866807878017 + 0.1 * 6.499645709991455
Epoch 850, val loss: 1.0869089365005493
Epoch 860, training loss: 0.6552528738975525 = 0.007168249227106571 + 0.1 * 6.480846405029297
Epoch 860, val loss: 1.0911222696304321
Epoch 870, training loss: 0.654607355594635 = 0.006949794944375753 + 0.1 * 6.47657585144043
Epoch 870, val loss: 1.0951393842697144
Epoch 880, training loss: 0.6538146734237671 = 0.006742756813764572 + 0.1 * 6.470718860626221
Epoch 880, val loss: 1.0992803573608398
Epoch 890, training loss: 0.6541024446487427 = 0.006546297110617161 + 0.1 * 6.475561141967773
Epoch 890, val loss: 1.103041648864746
Epoch 900, training loss: 0.6538200974464417 = 0.006358753424137831 + 0.1 * 6.474613666534424
Epoch 900, val loss: 1.1069647073745728
Epoch 910, training loss: 0.653474748134613 = 0.0061807939782738686 + 0.1 * 6.4729390144348145
Epoch 910, val loss: 1.1106784343719482
Epoch 920, training loss: 0.6524242162704468 = 0.006011433899402618 + 0.1 * 6.464128017425537
Epoch 920, val loss: 1.114310622215271
Epoch 930, training loss: 0.6520823240280151 = 0.005849784705787897 + 0.1 * 6.462325572967529
Epoch 930, val loss: 1.1177879571914673
Epoch 940, training loss: 0.6527466177940369 = 0.005694979801774025 + 0.1 * 6.470516204833984
Epoch 940, val loss: 1.1212420463562012
Epoch 950, training loss: 0.6539701819419861 = 0.005547782871872187 + 0.1 * 6.484224319458008
Epoch 950, val loss: 1.1247228384017944
Epoch 960, training loss: 0.650895357131958 = 0.005407317075878382 + 0.1 * 6.454880237579346
Epoch 960, val loss: 1.1282589435577393
Epoch 970, training loss: 0.6509221792221069 = 0.005273232702165842 + 0.1 * 6.456489562988281
Epoch 970, val loss: 1.1313223838806152
Epoch 980, training loss: 0.6499313116073608 = 0.005144165828824043 + 0.1 * 6.447871208190918
Epoch 980, val loss: 1.1345314979553223
Epoch 990, training loss: 0.6498422026634216 = 0.00502031808719039 + 0.1 * 6.448218822479248
Epoch 990, val loss: 1.1375936269760132
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.8027076721191406 = 1.9430207014083862 + 0.1 * 8.596868515014648
Epoch 0, val loss: 1.9491389989852905
Epoch 10, training loss: 2.7926881313323975 = 1.9330097436904907 + 0.1 * 8.596783638000488
Epoch 10, val loss: 1.938689112663269
Epoch 20, training loss: 2.780055284500122 = 1.9204363822937012 + 0.1 * 8.596189498901367
Epoch 20, val loss: 1.9255495071411133
Epoch 30, training loss: 2.76166033744812 = 1.9025989770889282 + 0.1 * 8.590614318847656
Epoch 30, val loss: 1.9070276021957397
Epoch 40, training loss: 2.731870651245117 = 1.8763659000396729 + 0.1 * 8.555047035217285
Epoch 40, val loss: 1.880081057548523
Epoch 50, training loss: 2.6807258129119873 = 1.8408615589141846 + 0.1 * 8.398641586303711
Epoch 50, val loss: 1.8452461957931519
Epoch 60, training loss: 2.614379644393921 = 1.8021174669265747 + 0.1 * 8.1226224899292
Epoch 60, val loss: 1.8101489543914795
Epoch 70, training loss: 2.5702242851257324 = 1.7680526971817017 + 0.1 * 8.02171516418457
Epoch 70, val loss: 1.780598759651184
Epoch 80, training loss: 2.5052616596221924 = 1.7285168170928955 + 0.1 * 7.7674479484558105
Epoch 80, val loss: 1.742775559425354
Epoch 90, training loss: 2.4209604263305664 = 1.6758973598480225 + 0.1 * 7.450629234313965
Epoch 90, val loss: 1.6930207014083862
Epoch 100, training loss: 2.3323311805725098 = 1.6070162057876587 + 0.1 * 7.253150463104248
Epoch 100, val loss: 1.632262110710144
Epoch 110, training loss: 2.2372047901153564 = 1.5198594331741333 + 0.1 * 7.173454284667969
Epoch 110, val loss: 1.5559660196304321
Epoch 120, training loss: 2.133146286010742 = 1.4224821329116821 + 0.1 * 7.106642723083496
Epoch 120, val loss: 1.4713703393936157
Epoch 130, training loss: 2.0296988487243652 = 1.3245824575424194 + 0.1 * 7.051163196563721
Epoch 130, val loss: 1.389862298965454
Epoch 140, training loss: 1.9315345287322998 = 1.2289828062057495 + 0.1 * 7.025516510009766
Epoch 140, val loss: 1.3115333318710327
Epoch 150, training loss: 1.8378376960754395 = 1.1367436647415161 + 0.1 * 7.010939598083496
Epoch 150, val loss: 1.2369537353515625
Epoch 160, training loss: 1.748365879058838 = 1.0487364530563354 + 0.1 * 6.996294021606445
Epoch 160, val loss: 1.1669856309890747
Epoch 170, training loss: 1.6638376712799072 = 0.9658860564231873 + 0.1 * 6.979516506195068
Epoch 170, val loss: 1.1015008687973022
Epoch 180, training loss: 1.5848110914230347 = 0.8886387348175049 + 0.1 * 6.961723327636719
Epoch 180, val loss: 1.039804458618164
Epoch 190, training loss: 1.5100947618484497 = 0.8160245418548584 + 0.1 * 6.940701961517334
Epoch 190, val loss: 0.9816997647285461
Epoch 200, training loss: 1.441591501235962 = 0.7491301894187927 + 0.1 * 6.924612522125244
Epoch 200, val loss: 0.92903733253479
Epoch 210, training loss: 1.378525972366333 = 0.6881015300750732 + 0.1 * 6.904243469238281
Epoch 210, val loss: 0.8832699656486511
Epoch 220, training loss: 1.321812629699707 = 0.6322990655899048 + 0.1 * 6.895135402679443
Epoch 220, val loss: 0.8453014492988586
Epoch 230, training loss: 1.2691528797149658 = 0.5818118453025818 + 0.1 * 6.873410224914551
Epoch 230, val loss: 0.8154621720314026
Epoch 240, training loss: 1.2213146686553955 = 0.5356475114822388 + 0.1 * 6.856672286987305
Epoch 240, val loss: 0.792304515838623
Epoch 250, training loss: 1.1779475212097168 = 0.4936258792877197 + 0.1 * 6.8432159423828125
Epoch 250, val loss: 0.7750087976455688
Epoch 260, training loss: 1.1383447647094727 = 0.4551219046115875 + 0.1 * 6.832228183746338
Epoch 260, val loss: 0.7625174522399902
Epoch 270, training loss: 1.1008795499801636 = 0.41887444257736206 + 0.1 * 6.820051193237305
Epoch 270, val loss: 0.7534617185592651
Epoch 280, training loss: 1.0655860900878906 = 0.3846357762813568 + 0.1 * 6.809503078460693
Epoch 280, val loss: 0.7474035620689392
Epoch 290, training loss: 1.032402515411377 = 0.35247454047203064 + 0.1 * 6.799279689788818
Epoch 290, val loss: 0.744199812412262
Epoch 300, training loss: 1.001624584197998 = 0.3226032853126526 + 0.1 * 6.790212154388428
Epoch 300, val loss: 0.7439289093017578
Epoch 310, training loss: 0.9747774600982666 = 0.2951904833316803 + 0.1 * 6.79586935043335
Epoch 310, val loss: 0.7465649843215942
Epoch 320, training loss: 0.9485892057418823 = 0.27022847533226013 + 0.1 * 6.783607482910156
Epoch 320, val loss: 0.7517764568328857
Epoch 330, training loss: 0.9254159927368164 = 0.24729877710342407 + 0.1 * 6.781171798706055
Epoch 330, val loss: 0.7591078281402588
Epoch 340, training loss: 0.903622031211853 = 0.2261437028646469 + 0.1 * 6.774783134460449
Epoch 340, val loss: 0.7683066725730896
Epoch 350, training loss: 0.8830646276473999 = 0.20648232102394104 + 0.1 * 6.765822410583496
Epoch 350, val loss: 0.7788832783699036
Epoch 360, training loss: 0.8646052479743958 = 0.18816925585269928 + 0.1 * 6.764359951019287
Epoch 360, val loss: 0.7907458543777466
Epoch 370, training loss: 0.8474922180175781 = 0.17124952375888824 + 0.1 * 6.762426853179932
Epoch 370, val loss: 0.8033738732337952
Epoch 380, training loss: 0.8313995003700256 = 0.15569673478603363 + 0.1 * 6.757027626037598
Epoch 380, val loss: 0.8166313767433167
Epoch 390, training loss: 0.8164156079292297 = 0.14137493073940277 + 0.1 * 6.750406742095947
Epoch 390, val loss: 0.8304906487464905
Epoch 400, training loss: 0.8027650117874146 = 0.1282183825969696 + 0.1 * 6.745466709136963
Epoch 400, val loss: 0.8449233174324036
Epoch 410, training loss: 0.7901756763458252 = 0.11620602756738663 + 0.1 * 6.739696025848389
Epoch 410, val loss: 0.859628438949585
Epoch 420, training loss: 0.7791868448257446 = 0.10530821233987808 + 0.1 * 6.738786220550537
Epoch 420, val loss: 0.8742455244064331
Epoch 430, training loss: 0.7684624195098877 = 0.09541873633861542 + 0.1 * 6.7304368019104
Epoch 430, val loss: 0.8889331817626953
Epoch 440, training loss: 0.7597244381904602 = 0.08650119602680206 + 0.1 * 6.732232570648193
Epoch 440, val loss: 0.9037237167358398
Epoch 450, training loss: 0.7503219246864319 = 0.07850390672683716 + 0.1 * 6.718180179595947
Epoch 450, val loss: 0.9182408452033997
Epoch 460, training loss: 0.7436517477035522 = 0.07133332639932632 + 0.1 * 6.723184108734131
Epoch 460, val loss: 0.9326273798942566
Epoch 470, training loss: 0.735880970954895 = 0.06493369489908218 + 0.1 * 6.70947265625
Epoch 470, val loss: 0.946876049041748
Epoch 480, training loss: 0.7297738790512085 = 0.059228405356407166 + 0.1 * 6.705454349517822
Epoch 480, val loss: 0.9609888195991516
Epoch 490, training loss: 0.7242035865783691 = 0.05414755642414093 + 0.1 * 6.700560092926025
Epoch 490, val loss: 0.9748777151107788
Epoch 500, training loss: 0.7182556390762329 = 0.04963773861527443 + 0.1 * 6.686178684234619
Epoch 500, val loss: 0.9884006977081299
Epoch 510, training loss: 0.7139464020729065 = 0.04562102258205414 + 0.1 * 6.683253765106201
Epoch 510, val loss: 1.0016664266586304
Epoch 520, training loss: 0.7095023989677429 = 0.04203595593571663 + 0.1 * 6.674664497375488
Epoch 520, val loss: 1.014773964881897
Epoch 530, training loss: 0.7065029144287109 = 0.038835134357213974 + 0.1 * 6.67667818069458
Epoch 530, val loss: 1.0274147987365723
Epoch 540, training loss: 0.702255368232727 = 0.035968005657196045 + 0.1 * 6.662873268127441
Epoch 540, val loss: 1.0396946668624878
Epoch 550, training loss: 0.7005386352539062 = 0.03338763862848282 + 0.1 * 6.671509742736816
Epoch 550, val loss: 1.0519475936889648
Epoch 560, training loss: 0.6965305209159851 = 0.031069891527295113 + 0.1 * 6.654606342315674
Epoch 560, val loss: 1.0635207891464233
Epoch 570, training loss: 0.6938532590866089 = 0.02897249534726143 + 0.1 * 6.648807525634766
Epoch 570, val loss: 1.0751128196716309
Epoch 580, training loss: 0.6913641691207886 = 0.027071407064795494 + 0.1 * 6.642927169799805
Epoch 580, val loss: 1.0862561464309692
Epoch 590, training loss: 0.6905568838119507 = 0.025344539433717728 + 0.1 * 6.652122974395752
Epoch 590, val loss: 1.0975321531295776
Epoch 600, training loss: 0.6886818408966064 = 0.02378230169415474 + 0.1 * 6.648995399475098
Epoch 600, val loss: 1.1077781915664673
Epoch 610, training loss: 0.6860920190811157 = 0.022358618676662445 + 0.1 * 6.63733434677124
Epoch 610, val loss: 1.1180827617645264
Epoch 620, training loss: 0.6837363243103027 = 0.02105991542339325 + 0.1 * 6.626763820648193
Epoch 620, val loss: 1.1278084516525269
Epoch 630, training loss: 0.6840291619300842 = 0.019869495183229446 + 0.1 * 6.64159631729126
Epoch 630, val loss: 1.1376254558563232
Epoch 640, training loss: 0.681300163269043 = 0.01878165826201439 + 0.1 * 6.625185012817383
Epoch 640, val loss: 1.1469464302062988
Epoch 650, training loss: 0.6801667213439941 = 0.01778472401201725 + 0.1 * 6.623819828033447
Epoch 650, val loss: 1.156099796295166
Epoch 660, training loss: 0.6788287162780762 = 0.016871273517608643 + 0.1 * 6.619574069976807
Epoch 660, val loss: 1.1646027565002441
Epoch 670, training loss: 0.6769660711288452 = 0.016027875244617462 + 0.1 * 6.609382152557373
Epoch 670, val loss: 1.1731256246566772
Epoch 680, training loss: 0.6802092790603638 = 0.015246827155351639 + 0.1 * 6.649624347686768
Epoch 680, val loss: 1.1815181970596313
Epoch 690, training loss: 0.6756261587142944 = 0.014528005383908749 + 0.1 * 6.6109819412231445
Epoch 690, val loss: 1.1894018650054932
Epoch 700, training loss: 0.6732527613639832 = 0.013861516490578651 + 0.1 * 6.593912124633789
Epoch 700, val loss: 1.197134256362915
Epoch 710, training loss: 0.673039972782135 = 0.013241558335721493 + 0.1 * 6.597984313964844
Epoch 710, val loss: 1.2046494483947754
Epoch 720, training loss: 0.6734541654586792 = 0.012664960697293282 + 0.1 * 6.607892036437988
Epoch 720, val loss: 1.2121479511260986
Epoch 730, training loss: 0.6706228852272034 = 0.012129568494856358 + 0.1 * 6.584933280944824
Epoch 730, val loss: 1.219077467918396
Epoch 740, training loss: 0.6705471277236938 = 0.0116294389590621 + 0.1 * 6.589176654815674
Epoch 740, val loss: 1.2261072397232056
Epoch 750, training loss: 0.6695291996002197 = 0.011162498965859413 + 0.1 * 6.583666801452637
Epoch 750, val loss: 1.232679009437561
Epoch 760, training loss: 0.6677595376968384 = 0.010725338943302631 + 0.1 * 6.570342063903809
Epoch 760, val loss: 1.2393969297409058
Epoch 770, training loss: 0.6684752702713013 = 0.010315122082829475 + 0.1 * 6.581601142883301
Epoch 770, val loss: 1.2457449436187744
Epoch 780, training loss: 0.6664333343505859 = 0.009930218569934368 + 0.1 * 6.5650315284729
Epoch 780, val loss: 1.2522255182266235
Epoch 790, training loss: 0.6658344268798828 = 0.009569692425429821 + 0.1 * 6.562646865844727
Epoch 790, val loss: 1.2580853700637817
Epoch 800, training loss: 0.6684311032295227 = 0.009229770861566067 + 0.1 * 6.592012882232666
Epoch 800, val loss: 1.2641879320144653
Epoch 810, training loss: 0.6638271808624268 = 0.008910403586924076 + 0.1 * 6.549167633056641
Epoch 810, val loss: 1.2698580026626587
Epoch 820, training loss: 0.6635233163833618 = 0.008608918637037277 + 0.1 * 6.5491437911987305
Epoch 820, val loss: 1.2754294872283936
Epoch 830, training loss: 0.6632434129714966 = 0.00832357257604599 + 0.1 * 6.549198627471924
Epoch 830, val loss: 1.2811052799224854
Epoch 840, training loss: 0.6624773144721985 = 0.00805418286472559 + 0.1 * 6.544230937957764
Epoch 840, val loss: 1.286512017250061
Epoch 850, training loss: 0.6623440980911255 = 0.007799522485584021 + 0.1 * 6.545445919036865
Epoch 850, val loss: 1.2915228605270386
Epoch 860, training loss: 0.6607891321182251 = 0.007557720877230167 + 0.1 * 6.532313823699951
Epoch 860, val loss: 1.2966967821121216
Epoch 870, training loss: 0.6600981950759888 = 0.0073281279765069485 + 0.1 * 6.527700424194336
Epoch 870, val loss: 1.3018019199371338
Epoch 880, training loss: 0.6613627076148987 = 0.007110683713108301 + 0.1 * 6.542520523071289
Epoch 880, val loss: 1.3066569566726685
Epoch 890, training loss: 0.6598072648048401 = 0.0069038281217217445 + 0.1 * 6.52903413772583
Epoch 890, val loss: 1.3113354444503784
Epoch 900, training loss: 0.6588224768638611 = 0.00670720636844635 + 0.1 * 6.521152973175049
Epoch 900, val loss: 1.3159806728363037
Epoch 910, training loss: 0.6596052050590515 = 0.006519823335111141 + 0.1 * 6.530853748321533
Epoch 910, val loss: 1.3205276727676392
Epoch 920, training loss: 0.6582701206207275 = 0.006340827327221632 + 0.1 * 6.519293308258057
Epoch 920, val loss: 1.325102686882019
Epoch 930, training loss: 0.6578059792518616 = 0.006170948036015034 + 0.1 * 6.516350269317627
Epoch 930, val loss: 1.3291234970092773
Epoch 940, training loss: 0.6582776308059692 = 0.006008101627230644 + 0.1 * 6.522695064544678
Epoch 940, val loss: 1.3335645198822021
Epoch 950, training loss: 0.6567887663841248 = 0.005853111390024424 + 0.1 * 6.509356498718262
Epoch 950, val loss: 1.3375898599624634
Epoch 960, training loss: 0.6562052369117737 = 0.005704703740775585 + 0.1 * 6.505005359649658
Epoch 960, val loss: 1.3415101766586304
Epoch 970, training loss: 0.6555150747299194 = 0.005562203470617533 + 0.1 * 6.499528884887695
Epoch 970, val loss: 1.3454705476760864
Epoch 980, training loss: 0.6567557454109192 = 0.005425566807389259 + 0.1 * 6.513301849365234
Epoch 980, val loss: 1.3495166301727295
Epoch 990, training loss: 0.6554628014564514 = 0.005294863134622574 + 0.1 * 6.501679420471191
Epoch 990, val loss: 1.3533825874328613
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.811941623687744 = 1.9522558450698853 + 0.1 * 8.596857070922852
Epoch 0, val loss: 1.9421815872192383
Epoch 10, training loss: 2.8013105392456055 = 1.9416334629058838 + 0.1 * 8.596770286560059
Epoch 10, val loss: 1.9322125911712646
Epoch 20, training loss: 2.788048267364502 = 1.928420066833496 + 0.1 * 8.596281051635742
Epoch 20, val loss: 1.9194186925888062
Epoch 30, training loss: 2.768765687942505 = 1.909599781036377 + 0.1 * 8.591659545898438
Epoch 30, val loss: 1.9008922576904297
Epoch 40, training loss: 2.737184524536133 = 1.8816349506378174 + 0.1 * 8.555495262145996
Epoch 40, val loss: 1.8736343383789062
Epoch 50, training loss: 2.677495002746582 = 1.843901515007019 + 0.1 * 8.33593463897705
Epoch 50, val loss: 1.8384859561920166
Epoch 60, training loss: 2.607358694076538 = 1.80344820022583 + 0.1 * 8.039104461669922
Epoch 60, val loss: 1.8037686347961426
Epoch 70, training loss: 2.553330421447754 = 1.7679588794708252 + 0.1 * 7.853715896606445
Epoch 70, val loss: 1.77449369430542
Epoch 80, training loss: 2.480973958969116 = 1.7296147346496582 + 0.1 * 7.513591766357422
Epoch 80, val loss: 1.7398384809494019
Epoch 90, training loss: 2.405491590499878 = 1.6820271015167236 + 0.1 * 7.234644412994385
Epoch 90, val loss: 1.6965893507003784
Epoch 100, training loss: 2.330819606781006 = 1.6195919513702393 + 0.1 * 7.11227560043335
Epoch 100, val loss: 1.6406927108764648
Epoch 110, training loss: 2.244539737701416 = 1.5407706499099731 + 0.1 * 7.037692070007324
Epoch 110, val loss: 1.5692795515060425
Epoch 120, training loss: 2.150404691696167 = 1.4528332948684692 + 0.1 * 6.97571325302124
Epoch 120, val loss: 1.4928085803985596
Epoch 130, training loss: 2.0555920600891113 = 1.3624547719955444 + 0.1 * 6.931372165679932
Epoch 130, val loss: 1.4169872999191284
Epoch 140, training loss: 1.9620118141174316 = 1.2717252969741821 + 0.1 * 6.902865886688232
Epoch 140, val loss: 1.3435481786727905
Epoch 150, training loss: 1.8703391551971436 = 1.1813328266143799 + 0.1 * 6.890063762664795
Epoch 150, val loss: 1.2728055715560913
Epoch 160, training loss: 1.7810337543487549 = 1.0930603742599487 + 0.1 * 6.879734039306641
Epoch 160, val loss: 1.2051455974578857
Epoch 170, training loss: 1.6942846775054932 = 1.0070538520812988 + 0.1 * 6.872308254241943
Epoch 170, val loss: 1.1411480903625488
Epoch 180, training loss: 1.6107118129730225 = 0.9241337180137634 + 0.1 * 6.865780830383301
Epoch 180, val loss: 1.079629898071289
Epoch 190, training loss: 1.531395673751831 = 0.8456969261169434 + 0.1 * 6.856987476348877
Epoch 190, val loss: 1.021992802619934
Epoch 200, training loss: 1.4567337036132812 = 0.7718886137008667 + 0.1 * 6.848451137542725
Epoch 200, val loss: 0.9682251214981079
Epoch 210, training loss: 1.3865125179290771 = 0.702735185623169 + 0.1 * 6.837772369384766
Epoch 210, val loss: 0.9189431667327881
Epoch 220, training loss: 1.3215413093566895 = 0.6388461589813232 + 0.1 * 6.826950550079346
Epoch 220, val loss: 0.8753064870834351
Epoch 230, training loss: 1.2616238594055176 = 0.5803231596946716 + 0.1 * 6.813006401062012
Epoch 230, val loss: 0.8377243876457214
Epoch 240, training loss: 1.20697021484375 = 0.5269228219985962 + 0.1 * 6.800474643707275
Epoch 240, val loss: 0.8063071966171265
Epoch 250, training loss: 1.157702922821045 = 0.4785827398300171 + 0.1 * 6.791201591491699
Epoch 250, val loss: 0.7808763980865479
Epoch 260, training loss: 1.1136198043823242 = 0.43449175357818604 + 0.1 * 6.791280269622803
Epoch 260, val loss: 0.7605194449424744
Epoch 270, training loss: 1.072465181350708 = 0.39450857043266296 + 0.1 * 6.779566287994385
Epoch 270, val loss: 0.7449290752410889
Epoch 280, training loss: 1.0346345901489258 = 0.3578549921512604 + 0.1 * 6.767796039581299
Epoch 280, val loss: 0.7332687973976135
Epoch 290, training loss: 1.000258207321167 = 0.32416653633117676 + 0.1 * 6.760915756225586
Epoch 290, val loss: 0.7251109480857849
Epoch 300, training loss: 0.9694691896438599 = 0.2934427261352539 + 0.1 * 6.7602643966674805
Epoch 300, val loss: 0.7200180888175964
Epoch 310, training loss: 0.9405548572540283 = 0.26565712690353394 + 0.1 * 6.748977184295654
Epoch 310, val loss: 0.7175463438034058
Epoch 320, training loss: 0.914141058921814 = 0.24038708209991455 + 0.1 * 6.737539768218994
Epoch 320, val loss: 0.7174670696258545
Epoch 330, training loss: 0.8921747803688049 = 0.21743285655975342 + 0.1 * 6.747419357299805
Epoch 330, val loss: 0.7193666100502014
Epoch 340, training loss: 0.8698959350585938 = 0.1968841254711151 + 0.1 * 6.730118274688721
Epoch 340, val loss: 0.7228434681892395
Epoch 350, training loss: 0.8498634099960327 = 0.1783667802810669 + 0.1 * 6.714966297149658
Epoch 350, val loss: 0.7274905443191528
Epoch 360, training loss: 0.8331870436668396 = 0.16167496144771576 + 0.1 * 6.715120792388916
Epoch 360, val loss: 0.7332211136817932
Epoch 370, training loss: 0.816626787185669 = 0.14665105938911438 + 0.1 * 6.699756622314453
Epoch 370, val loss: 0.7397651076316833
Epoch 380, training loss: 0.8046939373016357 = 0.13305608928203583 + 0.1 * 6.716378211975098
Epoch 380, val loss: 0.7470075488090515
Epoch 390, training loss: 0.789606511592865 = 0.12086327373981476 + 0.1 * 6.687432289123535
Epoch 390, val loss: 0.754793107509613
Epoch 400, training loss: 0.7779668569564819 = 0.10988283157348633 + 0.1 * 6.680840015411377
Epoch 400, val loss: 0.7629978060722351
Epoch 410, training loss: 0.7678185701370239 = 0.09998457133769989 + 0.1 * 6.67833948135376
Epoch 410, val loss: 0.7716881632804871
Epoch 420, training loss: 0.7583075761795044 = 0.0911388248205185 + 0.1 * 6.671687126159668
Epoch 420, val loss: 0.7806942462921143
Epoch 430, training loss: 0.7494662404060364 = 0.08321202546358109 + 0.1 * 6.66254186630249
Epoch 430, val loss: 0.7898918986320496
Epoch 440, training loss: 0.7418080568313599 = 0.07608436048030853 + 0.1 * 6.657236576080322
Epoch 440, val loss: 0.7993347644805908
Epoch 450, training loss: 0.736204206943512 = 0.0697181448340416 + 0.1 * 6.664860725402832
Epoch 450, val loss: 0.8088685274124146
Epoch 460, training loss: 0.7287046313285828 = 0.0640362948179245 + 0.1 * 6.646683216094971
Epoch 460, val loss: 0.8183637857437134
Epoch 470, training loss: 0.7242170572280884 = 0.058935340493917465 + 0.1 * 6.6528167724609375
Epoch 470, val loss: 0.8278642296791077
Epoch 480, training loss: 0.7178882956504822 = 0.054375384002923965 + 0.1 * 6.635128974914551
Epoch 480, val loss: 0.8373192548751831
Epoch 490, training loss: 0.7131460905075073 = 0.05027735233306885 + 0.1 * 6.628687381744385
Epoch 490, val loss: 0.8466736674308777
Epoch 500, training loss: 0.7083411812782288 = 0.04657638445496559 + 0.1 * 6.617647647857666
Epoch 500, val loss: 0.8559780716896057
Epoch 510, training loss: 0.7062473893165588 = 0.04322431981563568 + 0.1 * 6.63023042678833
Epoch 510, val loss: 0.8651911020278931
Epoch 520, training loss: 0.7030131816864014 = 0.04021552577614784 + 0.1 * 6.627976417541504
Epoch 520, val loss: 0.8741449117660522
Epoch 530, training loss: 0.6982793807983398 = 0.03750666230916977 + 0.1 * 6.607727527618408
Epoch 530, val loss: 0.882982075214386
Epoch 540, training loss: 0.6963108777999878 = 0.0350465290248394 + 0.1 * 6.612643718719482
Epoch 540, val loss: 0.8916136622428894
Epoch 550, training loss: 0.6932119727134705 = 0.03281254693865776 + 0.1 * 6.603994369506836
Epoch 550, val loss: 0.9001709818840027
Epoch 560, training loss: 0.6904059052467346 = 0.030773507431149483 + 0.1 * 6.596323490142822
Epoch 560, val loss: 0.9085571765899658
Epoch 570, training loss: 0.6896795034408569 = 0.02890917658805847 + 0.1 * 6.607703685760498
Epoch 570, val loss: 0.9167829155921936
Epoch 580, training loss: 0.6857319474220276 = 0.027216332033276558 + 0.1 * 6.585156440734863
Epoch 580, val loss: 0.9247134327888489
Epoch 590, training loss: 0.6836353540420532 = 0.025669099763035774 + 0.1 * 6.579662322998047
Epoch 590, val loss: 0.9325466752052307
Epoch 600, training loss: 0.681945264339447 = 0.024245457723736763 + 0.1 * 6.576998233795166
Epoch 600, val loss: 0.9401661157608032
Epoch 610, training loss: 0.6821497678756714 = 0.022933878004550934 + 0.1 * 6.592158317565918
Epoch 610, val loss: 0.9476171135902405
Epoch 620, training loss: 0.6788830161094666 = 0.021729471161961555 + 0.1 * 6.571535587310791
Epoch 620, val loss: 0.9550065994262695
Epoch 630, training loss: 0.6773554086685181 = 0.020620768889784813 + 0.1 * 6.567346096038818
Epoch 630, val loss: 0.9620683193206787
Epoch 640, training loss: 0.67570561170578 = 0.019594484940171242 + 0.1 * 6.561110973358154
Epoch 640, val loss: 0.9691260457038879
Epoch 650, training loss: 0.6754119396209717 = 0.018642066046595573 + 0.1 * 6.5676984786987305
Epoch 650, val loss: 0.9759449362754822
Epoch 660, training loss: 0.6733470559120178 = 0.017759786918759346 + 0.1 * 6.555872440338135
Epoch 660, val loss: 0.9826083779335022
Epoch 670, training loss: 0.6727718710899353 = 0.016938650980591774 + 0.1 * 6.558332443237305
Epoch 670, val loss: 0.9892271757125854
Epoch 680, training loss: 0.6714778542518616 = 0.0161760114133358 + 0.1 * 6.553018569946289
Epoch 680, val loss: 0.9955750703811646
Epoch 690, training loss: 0.6707035899162292 = 0.015466069802641869 + 0.1 * 6.552374839782715
Epoch 690, val loss: 1.0019097328186035
Epoch 700, training loss: 0.6697530150413513 = 0.01480413693934679 + 0.1 * 6.549488544464111
Epoch 700, val loss: 1.0078468322753906
Epoch 710, training loss: 0.6675234436988831 = 0.014186195097863674 + 0.1 * 6.533372402191162
Epoch 710, val loss: 1.0138778686523438
Epoch 720, training loss: 0.667497992515564 = 0.013607418164610863 + 0.1 * 6.538905143737793
Epoch 720, val loss: 1.0196492671966553
Epoch 730, training loss: 0.6668691635131836 = 0.013064603321254253 + 0.1 * 6.538045406341553
Epoch 730, val loss: 1.0253194570541382
Epoch 740, training loss: 0.6654857397079468 = 0.012555926106870174 + 0.1 * 6.529298305511475
Epoch 740, val loss: 1.0308846235275269
Epoch 750, training loss: 0.6639974117279053 = 0.012078573927283287 + 0.1 * 6.519188404083252
Epoch 750, val loss: 1.0363266468048096
Epoch 760, training loss: 0.6640053987503052 = 0.01162906363606453 + 0.1 * 6.523763179779053
Epoch 760, val loss: 1.0415593385696411
Epoch 770, training loss: 0.6621014475822449 = 0.011205598711967468 + 0.1 * 6.508958339691162
Epoch 770, val loss: 1.0467957258224487
Epoch 780, training loss: 0.6646572351455688 = 0.010805661790072918 + 0.1 * 6.538515567779541
Epoch 780, val loss: 1.051836371421814
Epoch 790, training loss: 0.6613875031471252 = 0.010429583489894867 + 0.1 * 6.509579181671143
Epoch 790, val loss: 1.0567301511764526
Epoch 800, training loss: 0.6601023077964783 = 0.010074113495647907 + 0.1 * 6.50028133392334
Epoch 800, val loss: 1.0616415739059448
Epoch 810, training loss: 0.6597495079040527 = 0.009737552143633366 + 0.1 * 6.500119686126709
Epoch 810, val loss: 1.0662472248077393
Epoch 820, training loss: 0.6597331166267395 = 0.009419135749340057 + 0.1 * 6.503139495849609
Epoch 820, val loss: 1.0709036588668823
Epoch 830, training loss: 0.6598111391067505 = 0.00911681354045868 + 0.1 * 6.506943225860596
Epoch 830, val loss: 1.0753728151321411
Epoch 840, training loss: 0.658141016960144 = 0.008830464445054531 + 0.1 * 6.493104934692383
Epoch 840, val loss: 1.0798066854476929
Epoch 850, training loss: 0.6582586169242859 = 0.008557559922337532 + 0.1 * 6.497010231018066
Epoch 850, val loss: 1.0841811895370483
Epoch 860, training loss: 0.6566537618637085 = 0.008299024775624275 + 0.1 * 6.483546733856201
Epoch 860, val loss: 1.088392972946167
Epoch 870, training loss: 0.6568809151649475 = 0.00805232860147953 + 0.1 * 6.488285541534424
Epoch 870, val loss: 1.092589259147644
Epoch 880, training loss: 0.6593526601791382 = 0.00781719945371151 + 0.1 * 6.515354633331299
Epoch 880, val loss: 1.0966472625732422
Epoch 890, training loss: 0.6561073660850525 = 0.007594551891088486 + 0.1 * 6.485128402709961
Epoch 890, val loss: 1.1005948781967163
Epoch 900, training loss: 0.6550641655921936 = 0.007382125593721867 + 0.1 * 6.47681999206543
Epoch 900, val loss: 1.1045838594436646
Epoch 910, training loss: 0.6562071442604065 = 0.007178294472396374 + 0.1 * 6.490288257598877
Epoch 910, val loss: 1.1083672046661377
Epoch 920, training loss: 0.6539932489395142 = 0.006984767038375139 + 0.1 * 6.4700846672058105
Epoch 920, val loss: 1.1120818853378296
Epoch 930, training loss: 0.6542283892631531 = 0.006799482740461826 + 0.1 * 6.4742889404296875
Epoch 930, val loss: 1.1158605813980103
Epoch 940, training loss: 0.6533557176589966 = 0.006622058339416981 + 0.1 * 6.467336177825928
Epoch 940, val loss: 1.1194618940353394
Epoch 950, training loss: 0.6523911356925964 = 0.006452012341469526 + 0.1 * 6.459391117095947
Epoch 950, val loss: 1.122983455657959
Epoch 960, training loss: 0.6521968841552734 = 0.006289840675890446 + 0.1 * 6.459070205688477
Epoch 960, val loss: 1.1265108585357666
Epoch 970, training loss: 0.6547048687934875 = 0.006133695598691702 + 0.1 * 6.485712051391602
Epoch 970, val loss: 1.1299420595169067
Epoch 980, training loss: 0.6533426642417908 = 0.005984526593238115 + 0.1 * 6.473580837249756
Epoch 980, val loss: 1.1332534551620483
Epoch 990, training loss: 0.6516854763031006 = 0.005841725971549749 + 0.1 * 6.458437442779541
Epoch 990, val loss: 1.1365646123886108
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8418555614127571
The final CL Acc:0.79259, 0.01386, The final GNN Acc:0.83869, 0.00282
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11654])
remove edge: torch.Size([2, 9486])
updated graph: torch.Size([2, 10584])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.808539867401123 = 1.9488556385040283 + 0.1 * 8.596842765808105
Epoch 0, val loss: 1.9513280391693115
Epoch 10, training loss: 2.7982707023620605 = 1.9385952949523926 + 0.1 * 8.596753120422363
Epoch 10, val loss: 1.9406980276107788
Epoch 20, training loss: 2.785928726196289 = 1.9263137578964233 + 0.1 * 8.596149444580078
Epoch 20, val loss: 1.9278191328048706
Epoch 30, training loss: 2.7686314582824707 = 1.9095661640167236 + 0.1 * 8.590652465820312
Epoch 30, val loss: 1.9102331399917603
Epoch 40, training loss: 2.740906238555908 = 1.885275959968567 + 0.1 * 8.556303977966309
Epoch 40, val loss: 1.8850187063217163
Epoch 50, training loss: 2.69279146194458 = 1.8523017168045044 + 0.1 * 8.404898643493652
Epoch 50, val loss: 1.852126955986023
Epoch 60, training loss: 2.634812355041504 = 1.8152070045471191 + 0.1 * 8.196053504943848
Epoch 60, val loss: 1.8174867630004883
Epoch 70, training loss: 2.5863072872161865 = 1.7817589044570923 + 0.1 * 8.04548454284668
Epoch 70, val loss: 1.7880151271820068
Epoch 80, training loss: 2.521805763244629 = 1.7471153736114502 + 0.1 * 7.746902942657471
Epoch 80, val loss: 1.7563865184783936
Epoch 90, training loss: 2.447924852371216 = 1.7039135694503784 + 0.1 * 7.440113544464111
Epoch 90, val loss: 1.7186585664749146
Epoch 100, training loss: 2.37378191947937 = 1.6483254432678223 + 0.1 * 7.254563808441162
Epoch 100, val loss: 1.6720894575119019
Epoch 110, training loss: 2.292325973510742 = 1.5770255327224731 + 0.1 * 7.153004169464111
Epoch 110, val loss: 1.6106247901916504
Epoch 120, training loss: 2.2010998725891113 = 1.4944242238998413 + 0.1 * 7.066756725311279
Epoch 120, val loss: 1.5419865846633911
Epoch 130, training loss: 2.107093334197998 = 1.4058966636657715 + 0.1 * 7.011966705322266
Epoch 130, val loss: 1.4717483520507812
Epoch 140, training loss: 2.011685848236084 = 1.3143236637115479 + 0.1 * 6.973621368408203
Epoch 140, val loss: 1.399919033050537
Epoch 150, training loss: 1.9161229133605957 = 1.22069251537323 + 0.1 * 6.95430326461792
Epoch 150, val loss: 1.3278052806854248
Epoch 160, training loss: 1.8223159313201904 = 1.1284778118133545 + 0.1 * 6.938381195068359
Epoch 160, val loss: 1.2567815780639648
Epoch 170, training loss: 1.7324254512786865 = 1.0400773286819458 + 0.1 * 6.923480987548828
Epoch 170, val loss: 1.1904425621032715
Epoch 180, training loss: 1.6477190256118774 = 0.9568555951118469 + 0.1 * 6.908634185791016
Epoch 180, val loss: 1.1288118362426758
Epoch 190, training loss: 1.5693252086639404 = 0.8795126080513 + 0.1 * 6.898125171661377
Epoch 190, val loss: 1.0719070434570312
Epoch 200, training loss: 1.4958988428115845 = 0.8077752590179443 + 0.1 * 6.881235599517822
Epoch 200, val loss: 1.0192822217941284
Epoch 210, training loss: 1.428607702255249 = 0.7409696578979492 + 0.1 * 6.876379489898682
Epoch 210, val loss: 0.9707366824150085
Epoch 220, training loss: 1.36613130569458 = 0.6802589893341064 + 0.1 * 6.8587236404418945
Epoch 220, val loss: 0.9281460046768188
Epoch 230, training loss: 1.3108214139938354 = 0.6251314282417297 + 0.1 * 6.856899738311768
Epoch 230, val loss: 0.8918002247810364
Epoch 240, training loss: 1.2591708898544312 = 0.5752610564231873 + 0.1 * 6.83909797668457
Epoch 240, val loss: 0.862034261226654
Epoch 250, training loss: 1.2120945453643799 = 0.5292792916297913 + 0.1 * 6.828152179718018
Epoch 250, val loss: 0.8377398252487183
Epoch 260, training loss: 1.1696879863739014 = 0.48627209663391113 + 0.1 * 6.834158897399902
Epoch 260, val loss: 0.8182868361473083
Epoch 270, training loss: 1.1275951862335205 = 0.445993036031723 + 0.1 * 6.816020965576172
Epoch 270, val loss: 0.8029589056968689
Epoch 280, training loss: 1.0882856845855713 = 0.40772274136543274 + 0.1 * 6.805629253387451
Epoch 280, val loss: 0.7904974818229675
Epoch 290, training loss: 1.053317666053772 = 0.371217280626297 + 0.1 * 6.821003437042236
Epoch 290, val loss: 0.7802830934524536
Epoch 300, training loss: 1.0160300731658936 = 0.3370008170604706 + 0.1 * 6.790292739868164
Epoch 300, val loss: 0.7724647521972656
Epoch 310, training loss: 0.9834033846855164 = 0.3049120306968689 + 0.1 * 6.784913539886475
Epoch 310, val loss: 0.7665160298347473
Epoch 320, training loss: 0.9525512456893921 = 0.2751530706882477 + 0.1 * 6.773981094360352
Epoch 320, val loss: 0.762742280960083
Epoch 330, training loss: 0.9244114756584167 = 0.24797983467578888 + 0.1 * 6.764316558837891
Epoch 330, val loss: 0.7612760066986084
Epoch 340, training loss: 0.8992248177528381 = 0.22325195372104645 + 0.1 * 6.759728908538818
Epoch 340, val loss: 0.7620211839675903
Epoch 350, training loss: 0.8780720233917236 = 0.20102223753929138 + 0.1 * 6.770498275756836
Epoch 350, val loss: 0.76483553647995
Epoch 360, training loss: 0.855808436870575 = 0.18120424449443817 + 0.1 * 6.746041774749756
Epoch 360, val loss: 0.7695909142494202
Epoch 370, training loss: 0.8372528553009033 = 0.16342435777187347 + 0.1 * 6.738284587860107
Epoch 370, val loss: 0.7761759161949158
Epoch 380, training loss: 0.8206373453140259 = 0.14752593636512756 + 0.1 * 6.731113433837891
Epoch 380, val loss: 0.7842895984649658
Epoch 390, training loss: 0.8058668375015259 = 0.13331861793994904 + 0.1 * 6.725481986999512
Epoch 390, val loss: 0.7938797473907471
Epoch 400, training loss: 0.7931990623474121 = 0.12061484903097153 + 0.1 * 6.725842475891113
Epoch 400, val loss: 0.8047804236412048
Epoch 410, training loss: 0.7804907560348511 = 0.10930128395557404 + 0.1 * 6.711894989013672
Epoch 410, val loss: 0.8169224262237549
Epoch 420, training loss: 0.7696852684020996 = 0.09916098415851593 + 0.1 * 6.70524263381958
Epoch 420, val loss: 0.8299609422683716
Epoch 430, training loss: 0.7602154016494751 = 0.09008144587278366 + 0.1 * 6.701339244842529
Epoch 430, val loss: 0.8437964916229248
Epoch 440, training loss: 0.7508801817893982 = 0.0819690003991127 + 0.1 * 6.689111232757568
Epoch 440, val loss: 0.8581601977348328
Epoch 450, training loss: 0.7443708181381226 = 0.07470932602882385 + 0.1 * 6.696614742279053
Epoch 450, val loss: 0.8727403283119202
Epoch 460, training loss: 0.7363953590393066 = 0.06824476271867752 + 0.1 * 6.681506156921387
Epoch 460, val loss: 0.8873990774154663
Epoch 470, training loss: 0.729713499546051 = 0.062453534454107285 + 0.1 * 6.672599792480469
Epoch 470, val loss: 0.9020313024520874
Epoch 480, training loss: 0.7234411239624023 = 0.057269178330898285 + 0.1 * 6.661719799041748
Epoch 480, val loss: 0.9167362451553345
Epoch 490, training loss: 0.7184002995491028 = 0.05262749642133713 + 0.1 * 6.6577277183532715
Epoch 490, val loss: 0.9311761260032654
Epoch 500, training loss: 0.7143787145614624 = 0.048467498272657394 + 0.1 * 6.659111976623535
Epoch 500, val loss: 0.9454710483551025
Epoch 510, training loss: 0.7100130319595337 = 0.04474648833274841 + 0.1 * 6.652665138244629
Epoch 510, val loss: 0.9593477845191956
Epoch 520, training loss: 0.7072270512580872 = 0.0414065346121788 + 0.1 * 6.658205032348633
Epoch 520, val loss: 0.972994863986969
Epoch 530, training loss: 0.7019020915031433 = 0.03841184452176094 + 0.1 * 6.634902477264404
Epoch 530, val loss: 0.9862581491470337
Epoch 540, training loss: 0.6978442668914795 = 0.035719696432352066 + 0.1 * 6.621245861053467
Epoch 540, val loss: 0.9990959763526917
Epoch 550, training loss: 0.6971665024757385 = 0.03329255059361458 + 0.1 * 6.638739585876465
Epoch 550, val loss: 1.011668086051941
Epoch 560, training loss: 0.6931601762771606 = 0.031102534383535385 + 0.1 * 6.62057638168335
Epoch 560, val loss: 1.023850917816162
Epoch 570, training loss: 0.6903460621833801 = 0.029118867591023445 + 0.1 * 6.612271308898926
Epoch 570, val loss: 1.035707712173462
Epoch 580, training loss: 0.6878821849822998 = 0.027317579835653305 + 0.1 * 6.605646133422852
Epoch 580, val loss: 1.0471501350402832
Epoch 590, training loss: 0.6872052550315857 = 0.025680676102638245 + 0.1 * 6.615245819091797
Epoch 590, val loss: 1.058296799659729
Epoch 600, training loss: 0.6829794645309448 = 0.024189399555325508 + 0.1 * 6.587900638580322
Epoch 600, val loss: 1.0690053701400757
Epoch 610, training loss: 0.6826290488243103 = 0.022824829444289207 + 0.1 * 6.598042011260986
Epoch 610, val loss: 1.0794740915298462
Epoch 620, training loss: 0.6821384429931641 = 0.021573903039097786 + 0.1 * 6.605645179748535
Epoch 620, val loss: 1.0896705389022827
Epoch 630, training loss: 0.6790450811386108 = 0.020428676158189774 + 0.1 * 6.5861639976501465
Epoch 630, val loss: 1.099465250968933
Epoch 640, training loss: 0.6776804327964783 = 0.019380100071430206 + 0.1 * 6.583003520965576
Epoch 640, val loss: 1.1090106964111328
Epoch 650, training loss: 0.6760680079460144 = 0.01841086335480213 + 0.1 * 6.576571464538574
Epoch 650, val loss: 1.1182217597961426
Epoch 660, training loss: 0.6745208501815796 = 0.0175181832164526 + 0.1 * 6.570026874542236
Epoch 660, val loss: 1.127224326133728
Epoch 670, training loss: 0.67238849401474 = 0.016689959913492203 + 0.1 * 6.556985378265381
Epoch 670, val loss: 1.1359236240386963
Epoch 680, training loss: 0.673325777053833 = 0.01592232659459114 + 0.1 * 6.574034690856934
Epoch 680, val loss: 1.1444029808044434
Epoch 690, training loss: 0.6710485219955444 = 0.015210554003715515 + 0.1 * 6.558379173278809
Epoch 690, val loss: 1.152601957321167
Epoch 700, training loss: 0.6709839105606079 = 0.014549238607287407 + 0.1 * 6.5643463134765625
Epoch 700, val loss: 1.1606276035308838
Epoch 710, training loss: 0.6684919595718384 = 0.013931510969996452 + 0.1 * 6.545604228973389
Epoch 710, val loss: 1.1684602499008179
Epoch 720, training loss: 0.6692633032798767 = 0.013355609029531479 + 0.1 * 6.55907678604126
Epoch 720, val loss: 1.1761165857315063
Epoch 730, training loss: 0.6683419942855835 = 0.012816243804991245 + 0.1 * 6.555257320404053
Epoch 730, val loss: 1.1834416389465332
Epoch 740, training loss: 0.6674110293388367 = 0.012312080711126328 + 0.1 * 6.550989627838135
Epoch 740, val loss: 1.1906843185424805
Epoch 750, training loss: 0.6652724146842957 = 0.011839400976896286 + 0.1 * 6.534330368041992
Epoch 750, val loss: 1.1976443529129028
Epoch 760, training loss: 0.6646285057067871 = 0.01139783300459385 + 0.1 * 6.532306671142578
Epoch 760, val loss: 1.204603910446167
Epoch 770, training loss: 0.6651583909988403 = 0.010981009341776371 + 0.1 * 6.541773319244385
Epoch 770, val loss: 1.2112555503845215
Epoch 780, training loss: 0.6645491123199463 = 0.010589088313281536 + 0.1 * 6.539600372314453
Epoch 780, val loss: 1.2177056074142456
Epoch 790, training loss: 0.6627230644226074 = 0.010220708325505257 + 0.1 * 6.525023460388184
Epoch 790, val loss: 1.224132776260376
Epoch 800, training loss: 0.6629593968391418 = 0.009871900081634521 + 0.1 * 6.530874729156494
Epoch 800, val loss: 1.230333685874939
Epoch 810, training loss: 0.661291241645813 = 0.009542880579829216 + 0.1 * 6.517483711242676
Epoch 810, val loss: 1.2363176345825195
Epoch 820, training loss: 0.6627523899078369 = 0.00923236459493637 + 0.1 * 6.535200119018555
Epoch 820, val loss: 1.242246389389038
Epoch 830, training loss: 0.6599538326263428 = 0.008937958627939224 + 0.1 * 6.510158538818359
Epoch 830, val loss: 1.2480875253677368
Epoch 840, training loss: 0.6588960886001587 = 0.008659390732645988 + 0.1 * 6.50236701965332
Epoch 840, val loss: 1.2537509202957153
Epoch 850, training loss: 0.6590141654014587 = 0.008394427597522736 + 0.1 * 6.506197452545166
Epoch 850, val loss: 1.2592594623565674
Epoch 860, training loss: 0.658414900302887 = 0.008143553510308266 + 0.1 * 6.502713203430176
Epoch 860, val loss: 1.264588475227356
Epoch 870, training loss: 0.6575038433074951 = 0.007905520498752594 + 0.1 * 6.495982646942139
Epoch 870, val loss: 1.2700027227401733
Epoch 880, training loss: 0.6575877070426941 = 0.007678277790546417 + 0.1 * 6.499094009399414
Epoch 880, val loss: 1.2751152515411377
Epoch 890, training loss: 0.6571826338768005 = 0.007462239824235439 + 0.1 * 6.497203826904297
Epoch 890, val loss: 1.2801802158355713
Epoch 900, training loss: 0.6563423871994019 = 0.00725603336468339 + 0.1 * 6.49086332321167
Epoch 900, val loss: 1.285152554512024
Epoch 910, training loss: 0.6561240553855896 = 0.007059927564114332 + 0.1 * 6.490641117095947
Epoch 910, val loss: 1.2900335788726807
Epoch 920, training loss: 0.6561726927757263 = 0.006871990393847227 + 0.1 * 6.493006706237793
Epoch 920, val loss: 1.294708490371704
Epoch 930, training loss: 0.6547331213951111 = 0.006693496368825436 + 0.1 * 6.480396270751953
Epoch 930, val loss: 1.2993634939193726
Epoch 940, training loss: 0.6554918885231018 = 0.006522892042994499 + 0.1 * 6.489689826965332
Epoch 940, val loss: 1.3040002584457397
Epoch 950, training loss: 0.6544253826141357 = 0.006359429564327002 + 0.1 * 6.480659008026123
Epoch 950, val loss: 1.3084516525268555
Epoch 960, training loss: 0.6536535620689392 = 0.0062024518847465515 + 0.1 * 6.47451114654541
Epoch 960, val loss: 1.3127692937850952
Epoch 970, training loss: 0.6532170176506042 = 0.006052547600120306 + 0.1 * 6.471644401550293
Epoch 970, val loss: 1.3170675039291382
Epoch 980, training loss: 0.6536893248558044 = 0.005908461753278971 + 0.1 * 6.477807998657227
Epoch 980, val loss: 1.3212714195251465
Epoch 990, training loss: 0.6526650786399841 = 0.00577051006257534 + 0.1 * 6.468945503234863
Epoch 990, val loss: 1.3254427909851074
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 2.7920422554016113 = 1.932357907295227 + 0.1 * 8.596842765808105
Epoch 0, val loss: 1.9259066581726074
Epoch 10, training loss: 2.7821414470672607 = 1.9224671125411987 + 0.1 * 8.596742630004883
Epoch 10, val loss: 1.9160953760147095
Epoch 20, training loss: 2.7698638439178467 = 1.9102492332458496 + 0.1 * 8.596146583557129
Epoch 20, val loss: 1.9037120342254639
Epoch 30, training loss: 2.752453565597534 = 1.8933358192443848 + 0.1 * 8.591177940368652
Epoch 30, val loss: 1.8866342306137085
Epoch 40, training loss: 2.724602699279785 = 1.8689725399017334 + 0.1 * 8.55630111694336
Epoch 40, val loss: 1.8625417947769165
Epoch 50, training loss: 2.671802520751953 = 1.8370182514190674 + 0.1 * 8.347843170166016
Epoch 50, val loss: 1.832595705986023
Epoch 60, training loss: 2.6020383834838867 = 1.804032802581787 + 0.1 * 7.9800543785095215
Epoch 60, val loss: 1.8033967018127441
Epoch 70, training loss: 2.540254592895508 = 1.773516058921814 + 0.1 * 7.667386054992676
Epoch 70, val loss: 1.7756532430648804
Epoch 80, training loss: 2.4848175048828125 = 1.7384942770004272 + 0.1 * 7.463231563568115
Epoch 80, val loss: 1.744016408920288
Epoch 90, training loss: 2.419236898422241 = 1.6927179098129272 + 0.1 * 7.265189170837402
Epoch 90, val loss: 1.7046735286712646
Epoch 100, training loss: 2.3433544635772705 = 1.6320058107376099 + 0.1 * 7.113487243652344
Epoch 100, val loss: 1.652328372001648
Epoch 110, training loss: 2.255979537963867 = 1.5530296564102173 + 0.1 * 7.0295000076293945
Epoch 110, val loss: 1.5827676057815552
Epoch 120, training loss: 2.1584558486938477 = 1.4606802463531494 + 0.1 * 6.977754592895508
Epoch 120, val loss: 1.5056283473968506
Epoch 130, training loss: 2.058074951171875 = 1.363101601600647 + 0.1 * 6.949732780456543
Epoch 130, val loss: 1.4289605617523193
Epoch 140, training loss: 1.9609148502349854 = 1.267401933670044 + 0.1 * 6.935129642486572
Epoch 140, val loss: 1.3554167747497559
Epoch 150, training loss: 1.869537591934204 = 1.176803708076477 + 0.1 * 6.92733907699585
Epoch 150, val loss: 1.2881263494491577
Epoch 160, training loss: 1.7833993434906006 = 1.0917251110076904 + 0.1 * 6.916742324829102
Epoch 160, val loss: 1.2256996631622314
Epoch 170, training loss: 1.7011420726776123 = 1.0105829238891602 + 0.1 * 6.90559196472168
Epoch 170, val loss: 1.1660425662994385
Epoch 180, training loss: 1.6225788593292236 = 0.9333409070968628 + 0.1 * 6.892378807067871
Epoch 180, val loss: 1.1082412004470825
Epoch 190, training loss: 1.548464059829712 = 0.8606851696968079 + 0.1 * 6.877788066864014
Epoch 190, val loss: 1.0534905195236206
Epoch 200, training loss: 1.4793086051940918 = 0.7930389046669006 + 0.1 * 6.862697124481201
Epoch 200, val loss: 1.0027663707733154
Epoch 210, training loss: 1.4157636165618896 = 0.7313920259475708 + 0.1 * 6.843716621398926
Epoch 210, val loss: 0.9578906297683716
Epoch 220, training loss: 1.3582754135131836 = 0.6754512786865234 + 0.1 * 6.82824182510376
Epoch 220, val loss: 0.9195833206176758
Epoch 230, training loss: 1.306153416633606 = 0.6251082420349121 + 0.1 * 6.810451507568359
Epoch 230, val loss: 0.8880831599235535
Epoch 240, training loss: 1.2591288089752197 = 0.5790849924087524 + 0.1 * 6.800438404083252
Epoch 240, val loss: 0.8625090718269348
Epoch 250, training loss: 1.2150940895080566 = 0.5360040068626404 + 0.1 * 6.790900707244873
Epoch 250, val loss: 0.8408286571502686
Epoch 260, training loss: 1.1729925870895386 = 0.4947194457054138 + 0.1 * 6.782731533050537
Epoch 260, val loss: 0.8223317265510559
Epoch 270, training loss: 1.1321841478347778 = 0.45426586270332336 + 0.1 * 6.7791829109191895
Epoch 270, val loss: 0.8058014512062073
Epoch 280, training loss: 1.091396689414978 = 0.41417500376701355 + 0.1 * 6.772216796875
Epoch 280, val loss: 0.7907883524894714
Epoch 290, training loss: 1.0517135858535767 = 0.3745882213115692 + 0.1 * 6.77125358581543
Epoch 290, val loss: 0.7774805426597595
Epoch 300, training loss: 1.0125973224639893 = 0.33596283197402954 + 0.1 * 6.766345500946045
Epoch 300, val loss: 0.7660045027732849
Epoch 310, training loss: 0.9746853113174438 = 0.298727422952652 + 0.1 * 6.759578704833984
Epoch 310, val loss: 0.7567323446273804
Epoch 320, training loss: 0.9413809776306152 = 0.2635518014431 + 0.1 * 6.778292179107666
Epoch 320, val loss: 0.7500936388969421
Epoch 330, training loss: 0.9072562456130981 = 0.2314687967300415 + 0.1 * 6.757874488830566
Epoch 330, val loss: 0.7462922930717468
Epoch 340, training loss: 0.8779087662696838 = 0.20281775295734406 + 0.1 * 6.75091028213501
Epoch 340, val loss: 0.7452055215835571
Epoch 350, training loss: 0.8523431420326233 = 0.17775064706802368 + 0.1 * 6.745924949645996
Epoch 350, val loss: 0.7465970516204834
Epoch 360, training loss: 0.8313201069831848 = 0.1561555415391922 + 0.1 * 6.751645565032959
Epoch 360, val loss: 0.750188946723938
Epoch 370, training loss: 0.8118409514427185 = 0.13774503767490387 + 0.1 * 6.740959167480469
Epoch 370, val loss: 0.7555859684944153
Epoch 380, training loss: 0.795100212097168 = 0.12198242545127869 + 0.1 * 6.731177806854248
Epoch 380, val loss: 0.7623794078826904
Epoch 390, training loss: 0.7810508012771606 = 0.10846854001283646 + 0.1 * 6.725822448730469
Epoch 390, val loss: 0.7703720927238464
Epoch 400, training loss: 0.7689374685287476 = 0.09685414284467697 + 0.1 * 6.7208333015441895
Epoch 400, val loss: 0.7791259288787842
Epoch 410, training loss: 0.7582496404647827 = 0.08681102842092514 + 0.1 * 6.714386463165283
Epoch 410, val loss: 0.7885987758636475
Epoch 420, training loss: 0.7492027878761292 = 0.07811655849218369 + 0.1 * 6.710862159729004
Epoch 420, val loss: 0.7984318733215332
Epoch 430, training loss: 0.7419703602790833 = 0.07057701796293259 + 0.1 * 6.71393346786499
Epoch 430, val loss: 0.8084585666656494
Epoch 440, training loss: 0.7338618040084839 = 0.0640195682644844 + 0.1 * 6.698422431945801
Epoch 440, val loss: 0.8185218572616577
Epoch 450, training loss: 0.7269319295883179 = 0.058279138058423996 + 0.1 * 6.686527729034424
Epoch 450, val loss: 0.8285764455795288
Epoch 460, training loss: 0.7215685844421387 = 0.053217463195323944 + 0.1 * 6.683511257171631
Epoch 460, val loss: 0.8386619091033936
Epoch 470, training loss: 0.7170253396034241 = 0.0487520806491375 + 0.1 * 6.682732582092285
Epoch 470, val loss: 0.8485559225082397
Epoch 480, training loss: 0.7116049528121948 = 0.044815465807914734 + 0.1 * 6.66789436340332
Epoch 480, val loss: 0.8582046627998352
Epoch 490, training loss: 0.7073962688446045 = 0.04131857678294182 + 0.1 * 6.660776615142822
Epoch 490, val loss: 0.8677232265472412
Epoch 500, training loss: 0.7054576873779297 = 0.03819643333554268 + 0.1 * 6.672612190246582
Epoch 500, val loss: 0.8770059943199158
Epoch 510, training loss: 0.7012813091278076 = 0.03542478755116463 + 0.1 * 6.658565044403076
Epoch 510, val loss: 0.8859855532646179
Epoch 520, training loss: 0.6971608400344849 = 0.03294987604022026 + 0.1 * 6.6421098709106445
Epoch 520, val loss: 0.8946906328201294
Epoch 530, training loss: 0.6940647959709167 = 0.030717937275767326 + 0.1 * 6.6334686279296875
Epoch 530, val loss: 0.9032911062240601
Epoch 540, training loss: 0.6921351552009583 = 0.028700249269604683 + 0.1 * 6.634349346160889
Epoch 540, val loss: 0.9116681814193726
Epoch 550, training loss: 0.6898562908172607 = 0.026875382289290428 + 0.1 * 6.6298089027404785
Epoch 550, val loss: 0.9198358654975891
Epoch 560, training loss: 0.6872783303260803 = 0.025215832516551018 + 0.1 * 6.620625019073486
Epoch 560, val loss: 0.927788496017456
Epoch 570, training loss: 0.6867961287498474 = 0.023707007989287376 + 0.1 * 6.630890846252441
Epoch 570, val loss: 0.9355106949806213
Epoch 580, training loss: 0.68302983045578 = 0.022338533774018288 + 0.1 * 6.606913089752197
Epoch 580, val loss: 0.9429273009300232
Epoch 590, training loss: 0.6818528175354004 = 0.021085990592837334 + 0.1 * 6.607668399810791
Epoch 590, val loss: 0.9502068161964417
Epoch 600, training loss: 0.6806210279464722 = 0.0199346374720335 + 0.1 * 6.606863498687744
Epoch 600, val loss: 0.9573290944099426
Epoch 610, training loss: 0.6802101135253906 = 0.018878258764743805 + 0.1 * 6.61331844329834
Epoch 610, val loss: 0.9642590284347534
Epoch 620, training loss: 0.6774148344993591 = 0.017907356843352318 + 0.1 * 6.59507417678833
Epoch 620, val loss: 0.9709408283233643
Epoch 630, training loss: 0.6767116785049438 = 0.01701042614877224 + 0.1 * 6.597012042999268
Epoch 630, val loss: 0.9774895310401917
Epoch 640, training loss: 0.6767145395278931 = 0.01618189737200737 + 0.1 * 6.6053266525268555
Epoch 640, val loss: 0.9838626384735107
Epoch 650, training loss: 0.6739145517349243 = 0.015416014939546585 + 0.1 * 6.584985256195068
Epoch 650, val loss: 0.9901119470596313
Epoch 660, training loss: 0.6738640069961548 = 0.014703691937029362 + 0.1 * 6.5916032791137695
Epoch 660, val loss: 0.9962325692176819
Epoch 670, training loss: 0.6731387972831726 = 0.014042082242667675 + 0.1 * 6.590967178344727
Epoch 670, val loss: 1.0021861791610718
Epoch 680, training loss: 0.671606719493866 = 0.01342863030731678 + 0.1 * 6.581780910491943
Epoch 680, val loss: 1.0079476833343506
Epoch 690, training loss: 0.6702806949615479 = 0.012857711873948574 + 0.1 * 6.5742292404174805
Epoch 690, val loss: 1.013622760772705
Epoch 700, training loss: 0.6697289943695068 = 0.012323278933763504 + 0.1 * 6.574057102203369
Epoch 700, val loss: 1.0192290544509888
Epoch 710, training loss: 0.6696699857711792 = 0.011822432279586792 + 0.1 * 6.578475475311279
Epoch 710, val loss: 1.024687647819519
Epoch 720, training loss: 0.6689738035202026 = 0.011354218237102032 + 0.1 * 6.57619571685791
Epoch 720, val loss: 1.0300294160842896
Epoch 730, training loss: 0.6678351759910583 = 0.010916566476225853 + 0.1 * 6.569185733795166
Epoch 730, val loss: 1.0352532863616943
Epoch 740, training loss: 0.6675359010696411 = 0.010504370555281639 + 0.1 * 6.570315361022949
Epoch 740, val loss: 1.0403720140457153
Epoch 750, training loss: 0.6666029691696167 = 0.01011737622320652 + 0.1 * 6.564856052398682
Epoch 750, val loss: 1.045391321182251
Epoch 760, training loss: 0.6664243936538696 = 0.009753460995852947 + 0.1 * 6.566709041595459
Epoch 760, val loss: 1.0502936840057373
Epoch 770, training loss: 0.6662431359291077 = 0.009410001337528229 + 0.1 * 6.568331241607666
Epoch 770, val loss: 1.055150032043457
Epoch 780, training loss: 0.666630208492279 = 0.009086660109460354 + 0.1 * 6.575435161590576
Epoch 780, val loss: 1.0598245859146118
Epoch 790, training loss: 0.6642933487892151 = 0.008782091550529003 + 0.1 * 6.555112361907959
Epoch 790, val loss: 1.0644220113754272
Epoch 800, training loss: 0.6641625761985779 = 0.008493308909237385 + 0.1 * 6.556692600250244
Epoch 800, val loss: 1.0689507722854614
Epoch 810, training loss: 0.6632384657859802 = 0.008220380172133446 + 0.1 * 6.550180912017822
Epoch 810, val loss: 1.0734480619430542
Epoch 820, training loss: 0.6630833148956299 = 0.007960823364555836 + 0.1 * 6.551225185394287
Epoch 820, val loss: 1.0777878761291504
Epoch 830, training loss: 0.6624245643615723 = 0.007716270629316568 + 0.1 * 6.547082424163818
Epoch 830, val loss: 1.082055687904358
Epoch 840, training loss: 0.6623055934906006 = 0.007484066765755415 + 0.1 * 6.548214912414551
Epoch 840, val loss: 1.0862623453140259
Epoch 850, training loss: 0.660888671875 = 0.007262694649398327 + 0.1 * 6.536259651184082
Epoch 850, val loss: 1.090354561805725
Epoch 860, training loss: 0.6606906652450562 = 0.007052973378449678 + 0.1 * 6.536376953125
Epoch 860, val loss: 1.0943406820297241
Epoch 870, training loss: 0.6624733805656433 = 0.006853118538856506 + 0.1 * 6.5562028884887695
Epoch 870, val loss: 1.0983397960662842
Epoch 880, training loss: 0.6607786417007446 = 0.006662637460976839 + 0.1 * 6.541159629821777
Epoch 880, val loss: 1.1021463871002197
Epoch 890, training loss: 0.6604400873184204 = 0.0064815678633749485 + 0.1 * 6.539584636688232
Epoch 890, val loss: 1.1059266328811646
Epoch 900, training loss: 0.6587123274803162 = 0.006308437790721655 + 0.1 * 6.524038791656494
Epoch 900, val loss: 1.109660267829895
Epoch 910, training loss: 0.6591725945472717 = 0.006143142003566027 + 0.1 * 6.530294418334961
Epoch 910, val loss: 1.1133230924606323
Epoch 920, training loss: 0.6583623886108398 = 0.005984572693705559 + 0.1 * 6.523778438568115
Epoch 920, val loss: 1.1168783903121948
Epoch 930, training loss: 0.6592151522636414 = 0.005833733361214399 + 0.1 * 6.533813953399658
Epoch 930, val loss: 1.1203937530517578
Epoch 940, training loss: 0.6575847268104553 = 0.005689005833119154 + 0.1 * 6.518957138061523
Epoch 940, val loss: 1.1238465309143066
Epoch 950, training loss: 0.6575761437416077 = 0.005550645757466555 + 0.1 * 6.520255088806152
Epoch 950, val loss: 1.127199649810791
Epoch 960, training loss: 0.6564324498176575 = 0.005417702719569206 + 0.1 * 6.510147571563721
Epoch 960, val loss: 1.130521297454834
Epoch 970, training loss: 0.6585972905158997 = 0.0052903881296515465 + 0.1 * 6.533068656921387
Epoch 970, val loss: 1.1337461471557617
Epoch 980, training loss: 0.655633807182312 = 0.0051684253849089146 + 0.1 * 6.504653453826904
Epoch 980, val loss: 1.1369659900665283
Epoch 990, training loss: 0.6563557386398315 = 0.005051536485552788 + 0.1 * 6.513041973114014
Epoch 990, val loss: 1.1401017904281616
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 2.802354574203491 = 1.942673683166504 + 0.1 * 8.596808433532715
Epoch 0, val loss: 1.9419828653335571
Epoch 10, training loss: 2.7925734519958496 = 1.9329075813293457 + 0.1 * 8.596659660339355
Epoch 10, val loss: 1.9323631525039673
Epoch 20, training loss: 2.7801272869110107 = 1.9205584526062012 + 0.1 * 8.595688819885254
Epoch 20, val loss: 1.9196308851242065
Epoch 30, training loss: 2.761714458465576 = 1.9029297828674316 + 0.1 * 8.587845802307129
Epoch 30, val loss: 1.901163101196289
Epoch 40, training loss: 2.731353998184204 = 1.8772000074386597 + 0.1 * 8.541540145874023
Epoch 40, val loss: 1.8746365308761597
Epoch 50, training loss: 2.674591302871704 = 1.8438416719436646 + 0.1 * 8.307497024536133
Epoch 50, val loss: 1.8426388502120972
Epoch 60, training loss: 2.6162497997283936 = 1.8084388971328735 + 0.1 * 8.078108787536621
Epoch 60, val loss: 1.8116594552993774
Epoch 70, training loss: 2.5419771671295166 = 1.7751182317733765 + 0.1 * 7.668590068817139
Epoch 70, val loss: 1.783807396888733
Epoch 80, training loss: 2.464111804962158 = 1.7394084930419922 + 0.1 * 7.247032165527344
Epoch 80, val loss: 1.752618670463562
Epoch 90, training loss: 2.400969982147217 = 1.6941916942596436 + 0.1 * 7.067781448364258
Epoch 90, val loss: 1.7128188610076904
Epoch 100, training loss: 2.3338255882263184 = 1.6336348056793213 + 0.1 * 7.001906871795654
Epoch 100, val loss: 1.6596686840057373
Epoch 110, training loss: 2.252225875854492 = 1.5567950010299683 + 0.1 * 6.954308986663818
Epoch 110, val loss: 1.5939397811889648
Epoch 120, training loss: 2.160403251647949 = 1.4684710502624512 + 0.1 * 6.919321537017822
Epoch 120, val loss: 1.5216819047927856
Epoch 130, training loss: 2.0635340213775635 = 1.3741108179092407 + 0.1 * 6.894232749938965
Epoch 130, val loss: 1.4464153051376343
Epoch 140, training loss: 1.963061809539795 = 1.2753812074661255 + 0.1 * 6.876805782318115
Epoch 140, val loss: 1.3689191341400146
Epoch 150, training loss: 1.8624292612075806 = 1.1756463050842285 + 0.1 * 6.867829322814941
Epoch 150, val loss: 1.2933677434921265
Epoch 160, training loss: 1.766629695892334 = 1.0805238485336304 + 0.1 * 6.861058235168457
Epoch 160, val loss: 1.2232797145843506
Epoch 170, training loss: 1.676742672920227 = 0.9913187026977539 + 0.1 * 6.854239463806152
Epoch 170, val loss: 1.1591482162475586
Epoch 180, training loss: 1.593685269355774 = 0.9087458252906799 + 0.1 * 6.84939432144165
Epoch 180, val loss: 1.1017889976501465
Epoch 190, training loss: 1.5172810554504395 = 0.8324935436248779 + 0.1 * 6.847874641418457
Epoch 190, val loss: 1.0502185821533203
Epoch 200, training loss: 1.44577956199646 = 0.7614225149154663 + 0.1 * 6.843569755554199
Epoch 200, val loss: 1.0034087896347046
Epoch 210, training loss: 1.3797905445098877 = 0.695827066898346 + 0.1 * 6.839634418487549
Epoch 210, val loss: 0.9616187214851379
Epoch 220, training loss: 1.3197708129882812 = 0.6361520886421204 + 0.1 * 6.836186408996582
Epoch 220, val loss: 0.9256600737571716
Epoch 230, training loss: 1.2649216651916504 = 0.5816174745559692 + 0.1 * 6.833042144775391
Epoch 230, val loss: 0.8951945900917053
Epoch 240, training loss: 1.2157771587371826 = 0.5316161513328552 + 0.1 * 6.841610431671143
Epoch 240, val loss: 0.8698229193687439
Epoch 250, training loss: 1.1687458753585815 = 0.4861402213573456 + 0.1 * 6.826056480407715
Epoch 250, val loss: 0.8494912385940552
Epoch 260, training loss: 1.1266340017318726 = 0.44429922103881836 + 0.1 * 6.823347568511963
Epoch 260, val loss: 0.8332236409187317
Epoch 270, training loss: 1.0875420570373535 = 0.40553298592567444 + 0.1 * 6.820090293884277
Epoch 270, val loss: 0.8203526735305786
Epoch 280, training loss: 1.0506064891815186 = 0.36938780546188354 + 0.1 * 6.8121867179870605
Epoch 280, val loss: 0.8105137348175049
Epoch 290, training loss: 1.0170480012893677 = 0.3353274464607239 + 0.1 * 6.817205429077148
Epoch 290, val loss: 0.8031700253486633
Epoch 300, training loss: 0.9834011197090149 = 0.30331259965896606 + 0.1 * 6.800885200500488
Epoch 300, val loss: 0.7980433106422424
Epoch 310, training loss: 0.9529135227203369 = 0.2732386887073517 + 0.1 * 6.796748638153076
Epoch 310, val loss: 0.7948582768440247
Epoch 320, training loss: 0.9240083694458008 = 0.2453107237815857 + 0.1 * 6.786976337432861
Epoch 320, val loss: 0.7936626076698303
Epoch 330, training loss: 0.897981584072113 = 0.21954645216464996 + 0.1 * 6.784351348876953
Epoch 330, val loss: 0.7944456934928894
Epoch 340, training loss: 0.8741052746772766 = 0.19624923169612885 + 0.1 * 6.778560161590576
Epoch 340, val loss: 0.7972782850265503
Epoch 350, training loss: 0.8517931699752808 = 0.17538519203662872 + 0.1 * 6.764080047607422
Epoch 350, val loss: 0.801855206489563
Epoch 360, training loss: 0.8326123356819153 = 0.15678201615810394 + 0.1 * 6.758302688598633
Epoch 360, val loss: 0.808164656162262
Epoch 370, training loss: 0.8153544664382935 = 0.14032529294490814 + 0.1 * 6.750291347503662
Epoch 370, val loss: 0.8160474300384521
Epoch 380, training loss: 0.8010885119438171 = 0.1258111149072647 + 0.1 * 6.752773761749268
Epoch 380, val loss: 0.825172483921051
Epoch 390, training loss: 0.786522626876831 = 0.11306668817996979 + 0.1 * 6.734559535980225
Epoch 390, val loss: 0.8354109525680542
Epoch 400, training loss: 0.7757107615470886 = 0.10183942317962646 + 0.1 * 6.738713264465332
Epoch 400, val loss: 0.8465356230735779
Epoch 410, training loss: 0.7643634676933289 = 0.09198211878538132 + 0.1 * 6.723813056945801
Epoch 410, val loss: 0.858389139175415
Epoch 420, training loss: 0.7551858425140381 = 0.08329112827777863 + 0.1 * 6.718946933746338
Epoch 420, val loss: 0.8707035183906555
Epoch 430, training loss: 0.7459608912467957 = 0.07563818991184235 + 0.1 * 6.7032270431518555
Epoch 430, val loss: 0.8833869695663452
Epoch 440, training loss: 0.7384511828422546 = 0.06888961046934128 + 0.1 * 6.695615291595459
Epoch 440, val loss: 0.8962600231170654
Epoch 450, training loss: 0.7340660095214844 = 0.06291846930980682 + 0.1 * 6.711474895477295
Epoch 450, val loss: 0.9092490077018738
Epoch 460, training loss: 0.7256354689598083 = 0.05765431374311447 + 0.1 * 6.679811477661133
Epoch 460, val loss: 0.922227144241333
Epoch 470, training loss: 0.7202391028404236 = 0.05297398194670677 + 0.1 * 6.6726508140563965
Epoch 470, val loss: 0.9350550770759583
Epoch 480, training loss: 0.7159276604652405 = 0.04883188754320145 + 0.1 * 6.670957565307617
Epoch 480, val loss: 0.9477874040603638
Epoch 490, training loss: 0.7100247740745544 = 0.045153893530368805 + 0.1 * 6.648708343505859
Epoch 490, val loss: 0.9601705074310303
Epoch 500, training loss: 0.7058893442153931 = 0.04185411334037781 + 0.1 * 6.640352249145508
Epoch 500, val loss: 0.9723741412162781
Epoch 510, training loss: 0.7029165625572205 = 0.03888848051428795 + 0.1 * 6.640280723571777
Epoch 510, val loss: 0.9843539595603943
Epoch 520, training loss: 0.6999541521072388 = 0.03622933477163315 + 0.1 * 6.637248516082764
Epoch 520, val loss: 0.9961360096931458
Epoch 530, training loss: 0.6960510611534119 = 0.03382769972085953 + 0.1 * 6.6222333908081055
Epoch 530, val loss: 1.0074714422225952
Epoch 540, training loss: 0.6940304040908813 = 0.03165407106280327 + 0.1 * 6.623763084411621
Epoch 540, val loss: 1.0186903476715088
Epoch 550, training loss: 0.6915376782417297 = 0.029685350134968758 + 0.1 * 6.618523120880127
Epoch 550, val loss: 1.0295636653900146
Epoch 560, training loss: 0.6901620030403137 = 0.02789352647960186 + 0.1 * 6.622684478759766
Epoch 560, val loss: 1.0401816368103027
Epoch 570, training loss: 0.6863332390785217 = 0.02626447193324566 + 0.1 * 6.600687503814697
Epoch 570, val loss: 1.0505069494247437
Epoch 580, training loss: 0.6838481426239014 = 0.024777796119451523 + 0.1 * 6.59070348739624
Epoch 580, val loss: 1.0605530738830566
Epoch 590, training loss: 0.6856696009635925 = 0.02341439761221409 + 0.1 * 6.622551918029785
Epoch 590, val loss: 1.0702857971191406
Epoch 600, training loss: 0.6803944110870361 = 0.022168222814798355 + 0.1 * 6.58226203918457
Epoch 600, val loss: 1.0798399448394775
Epoch 610, training loss: 0.6796737313270569 = 0.02102026902139187 + 0.1 * 6.58653450012207
Epoch 610, val loss: 1.0889815092086792
Epoch 620, training loss: 0.6772817969322205 = 0.019963517785072327 + 0.1 * 6.573183059692383
Epoch 620, val loss: 1.0979893207550049
Epoch 630, training loss: 0.6759569644927979 = 0.018987737596035004 + 0.1 * 6.569692134857178
Epoch 630, val loss: 1.1068331003189087
Epoch 640, training loss: 0.674565315246582 = 0.018081484362483025 + 0.1 * 6.56483793258667
Epoch 640, val loss: 1.1153488159179688
Epoch 650, training loss: 0.673836886882782 = 0.017241930589079857 + 0.1 * 6.565949440002441
Epoch 650, val loss: 1.123770833015442
Epoch 660, training loss: 0.6748136878013611 = 0.016463767737150192 + 0.1 * 6.583499431610107
Epoch 660, val loss: 1.131917119026184
Epoch 670, training loss: 0.6708088517189026 = 0.015739575028419495 + 0.1 * 6.550693035125732
Epoch 670, val loss: 1.1399418115615845
Epoch 680, training loss: 0.6728715300559998 = 0.015065004117786884 + 0.1 * 6.578064918518066
Epoch 680, val loss: 1.1477305889129639
Epoch 690, training loss: 0.6691676378250122 = 0.014436894096434116 + 0.1 * 6.54730749130249
Epoch 690, val loss: 1.1552703380584717
Epoch 700, training loss: 0.6678948402404785 = 0.01385123934596777 + 0.1 * 6.540435791015625
Epoch 700, val loss: 1.1627131700515747
Epoch 710, training loss: 0.6723132729530334 = 0.01329887192696333 + 0.1 * 6.590144157409668
Epoch 710, val loss: 1.1699044704437256
Epoch 720, training loss: 0.665782630443573 = 0.012782622128725052 + 0.1 * 6.529999732971191
Epoch 720, val loss: 1.1770546436309814
Epoch 730, training loss: 0.6654313802719116 = 0.012301137670874596 + 0.1 * 6.531301975250244
Epoch 730, val loss: 1.184032678604126
Epoch 740, training loss: 0.6684020161628723 = 0.011845041066408157 + 0.1 * 6.5655694007873535
Epoch 740, val loss: 1.1906933784484863
Epoch 750, training loss: 0.6640464067459106 = 0.01141922827810049 + 0.1 * 6.526271820068359
Epoch 750, val loss: 1.197341799736023
Epoch 760, training loss: 0.6629632711410522 = 0.011018103919923306 + 0.1 * 6.51945161819458
Epoch 760, val loss: 1.2039459943771362
Epoch 770, training loss: 0.662768542766571 = 0.010636691935360432 + 0.1 * 6.521318435668945
Epoch 770, val loss: 1.2102360725402832
Epoch 780, training loss: 0.6619639992713928 = 0.01027736533433199 + 0.1 * 6.516866207122803
Epoch 780, val loss: 1.2164994478225708
Epoch 790, training loss: 0.6613375544548035 = 0.00993894413113594 + 0.1 * 6.513985633850098
Epoch 790, val loss: 1.2225786447525024
Epoch 800, training loss: 0.6607118248939514 = 0.009618323296308517 + 0.1 * 6.510934829711914
Epoch 800, val loss: 1.2286180257797241
Epoch 810, training loss: 0.660228431224823 = 0.009313568472862244 + 0.1 * 6.509148597717285
Epoch 810, val loss: 1.2344714403152466
Epoch 820, training loss: 0.6591973900794983 = 0.00902416929602623 + 0.1 * 6.501732349395752
Epoch 820, val loss: 1.2402236461639404
Epoch 830, training loss: 0.6590834259986877 = 0.008751153014600277 + 0.1 * 6.503322601318359
Epoch 830, val loss: 1.245888590812683
Epoch 840, training loss: 0.6600977182388306 = 0.008490964770317078 + 0.1 * 6.5160675048828125
Epoch 840, val loss: 1.2514058351516724
Epoch 850, training loss: 0.6576840281486511 = 0.008243100717663765 + 0.1 * 6.494409561157227
Epoch 850, val loss: 1.2569069862365723
Epoch 860, training loss: 0.6576496362686157 = 0.008007247000932693 + 0.1 * 6.496423721313477
Epoch 860, val loss: 1.2623100280761719
Epoch 870, training loss: 0.6587169170379639 = 0.007781126536428928 + 0.1 * 6.509357452392578
Epoch 870, val loss: 1.2675937414169312
Epoch 880, training loss: 0.6565669178962708 = 0.00756607111543417 + 0.1 * 6.490008354187012
Epoch 880, val loss: 1.2727880477905273
Epoch 890, training loss: 0.6555629372596741 = 0.007361527532339096 + 0.1 * 6.482014179229736
Epoch 890, val loss: 1.2779659032821655
Epoch 900, training loss: 0.6564182639122009 = 0.00716575188562274 + 0.1 * 6.492525100708008
Epoch 900, val loss: 1.2829790115356445
Epoch 910, training loss: 0.6556969285011292 = 0.006979084108024836 + 0.1 * 6.487177848815918
Epoch 910, val loss: 1.2878299951553345
Epoch 920, training loss: 0.6547478437423706 = 0.006800844334065914 + 0.1 * 6.479470252990723
Epoch 920, val loss: 1.2926965951919556
Epoch 930, training loss: 0.6538392305374146 = 0.006630227901041508 + 0.1 * 6.472089767456055
Epoch 930, val loss: 1.2975047826766968
Epoch 940, training loss: 0.6551936864852905 = 0.0064660971984267235 + 0.1 * 6.48727560043335
Epoch 940, val loss: 1.3021819591522217
Epoch 950, training loss: 0.6540339589118958 = 0.006308448500931263 + 0.1 * 6.477254867553711
Epoch 950, val loss: 1.3067080974578857
Epoch 960, training loss: 0.652997612953186 = 0.006158745847642422 + 0.1 * 6.468388557434082
Epoch 960, val loss: 1.3113579750061035
Epoch 970, training loss: 0.6556586027145386 = 0.006013302132487297 + 0.1 * 6.496453285217285
Epoch 970, val loss: 1.315805196762085
Epoch 980, training loss: 0.6532787084579468 = 0.005874602124094963 + 0.1 * 6.474040985107422
Epoch 980, val loss: 1.320141315460205
Epoch 990, training loss: 0.653773307800293 = 0.00574191939085722 + 0.1 * 6.480314254760742
Epoch 990, val loss: 1.3245010375976562
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8070637849235636
The final CL Acc:0.77407, 0.01318, The final GNN Acc:0.81023, 0.00224
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13138])
remove edge: torch.Size([2, 7894])
updated graph: torch.Size([2, 10476])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.813255786895752 = 1.9535731077194214 + 0.1 * 8.59682559967041
Epoch 0, val loss: 1.9472994804382324
Epoch 10, training loss: 2.8034303188323975 = 1.943756103515625 + 0.1 * 8.596742630004883
Epoch 10, val loss: 1.938140869140625
Epoch 20, training loss: 2.7916812896728516 = 1.9320682287216187 + 0.1 * 8.596129417419434
Epoch 20, val loss: 1.926824688911438
Epoch 30, training loss: 2.775031566619873 = 1.916015863418579 + 0.1 * 8.590156555175781
Epoch 30, val loss: 1.9108798503875732
Epoch 40, training loss: 2.7465977668762207 = 1.8922747373580933 + 0.1 * 8.543229103088379
Epoch 40, val loss: 1.8871655464172363
Epoch 50, training loss: 2.6847968101501465 = 1.85914945602417 + 0.1 * 8.25647258758545
Epoch 50, val loss: 1.8554044961929321
Epoch 60, training loss: 2.6208062171936035 = 1.8199129104614258 + 0.1 * 8.008933067321777
Epoch 60, val loss: 1.820349931716919
Epoch 70, training loss: 2.5425612926483154 = 1.781912088394165 + 0.1 * 7.606492042541504
Epoch 70, val loss: 1.7881089448928833
Epoch 80, training loss: 2.465761423110962 = 1.7447741031646729 + 0.1 * 7.209873676300049
Epoch 80, val loss: 1.7562810182571411
Epoch 90, training loss: 2.4051923751831055 = 1.6998001337051392 + 0.1 * 7.053922653198242
Epoch 90, val loss: 1.7160634994506836
Epoch 100, training loss: 2.339057683944702 = 1.637844204902649 + 0.1 * 7.0121355056762695
Epoch 100, val loss: 1.6596992015838623
Epoch 110, training loss: 2.2533035278320312 = 1.5559978485107422 + 0.1 * 6.973057270050049
Epoch 110, val loss: 1.588219165802002
Epoch 120, training loss: 2.152717351913452 = 1.459448218345642 + 0.1 * 6.932692050933838
Epoch 120, val loss: 1.5073456764221191
Epoch 130, training loss: 2.046781063079834 = 1.3577522039413452 + 0.1 * 6.890288829803467
Epoch 130, val loss: 1.4239109754562378
Epoch 140, training loss: 1.9429889917373657 = 1.2578784227371216 + 0.1 * 6.851105690002441
Epoch 140, val loss: 1.3444416522979736
Epoch 150, training loss: 1.8462903499603271 = 1.1635794639587402 + 0.1 * 6.827108860015869
Epoch 150, val loss: 1.2717252969741821
Epoch 160, training loss: 1.7541136741638184 = 1.0729544162750244 + 0.1 * 6.811591625213623
Epoch 160, val loss: 1.2019939422607422
Epoch 170, training loss: 1.6639429330825806 = 0.9841381907463074 + 0.1 * 6.7980475425720215
Epoch 170, val loss: 1.1339244842529297
Epoch 180, training loss: 1.5767663717269897 = 0.8979814052581787 + 0.1 * 6.787849426269531
Epoch 180, val loss: 1.0673872232437134
Epoch 190, training loss: 1.496037244796753 = 0.8181557059288025 + 0.1 * 6.778814792633057
Epoch 190, val loss: 1.0068962574005127
Epoch 200, training loss: 1.4246726036071777 = 0.7479690909385681 + 0.1 * 6.767035484313965
Epoch 200, val loss: 0.9553926587104797
Epoch 210, training loss: 1.3628795146942139 = 0.6872745156288147 + 0.1 * 6.756049156188965
Epoch 210, val loss: 0.9132080674171448
Epoch 220, training loss: 1.3105173110961914 = 0.6340215802192688 + 0.1 * 6.764956951141357
Epoch 220, val loss: 0.8787265419960022
Epoch 230, training loss: 1.260749340057373 = 0.5866544246673584 + 0.1 * 6.740949630737305
Epoch 230, val loss: 0.8503172993659973
Epoch 240, training loss: 1.21549654006958 = 0.5427635312080383 + 0.1 * 6.727329254150391
Epoch 240, val loss: 0.8256509900093079
Epoch 250, training loss: 1.1732356548309326 = 0.5013012290000916 + 0.1 * 6.719344139099121
Epoch 250, val loss: 0.8040309548377991
Epoch 260, training loss: 1.1328134536743164 = 0.46175888180732727 + 0.1 * 6.710545063018799
Epoch 260, val loss: 0.7848544716835022
Epoch 270, training loss: 1.0944184064865112 = 0.4233369529247284 + 0.1 * 6.710814952850342
Epoch 270, val loss: 0.7676610946655273
Epoch 280, training loss: 1.0554088354110718 = 0.3857010304927826 + 0.1 * 6.697077751159668
Epoch 280, val loss: 0.7523428797721863
Epoch 290, training loss: 1.018024206161499 = 0.348582923412323 + 0.1 * 6.694413185119629
Epoch 290, val loss: 0.7385508418083191
Epoch 300, training loss: 0.9812162518501282 = 0.31237292289733887 + 0.1 * 6.6884331703186035
Epoch 300, val loss: 0.7262076139450073
Epoch 310, training loss: 0.9460839033126831 = 0.2777203917503357 + 0.1 * 6.683635234832764
Epoch 310, val loss: 0.715589165687561
Epoch 320, training loss: 0.9150422215461731 = 0.24547432363033295 + 0.1 * 6.6956787109375
Epoch 320, val loss: 0.7070102095603943
Epoch 330, training loss: 0.8834333419799805 = 0.21650725603103638 + 0.1 * 6.6692609786987305
Epoch 330, val loss: 0.700905442237854
Epoch 340, training loss: 0.8571425080299377 = 0.19084496796131134 + 0.1 * 6.662975311279297
Epoch 340, val loss: 0.6972955465316772
Epoch 350, training loss: 0.8346661329269409 = 0.16837206482887268 + 0.1 * 6.66294002532959
Epoch 350, val loss: 0.6961239576339722
Epoch 360, training loss: 0.8162282109260559 = 0.14898090064525604 + 0.1 * 6.672472953796387
Epoch 360, val loss: 0.6972166895866394
Epoch 370, training loss: 0.7977405190467834 = 0.13233721256256104 + 0.1 * 6.6540327072143555
Epoch 370, val loss: 0.7001214623451233
Epoch 380, training loss: 0.7826102375984192 = 0.11795959621667862 + 0.1 * 6.646505832672119
Epoch 380, val loss: 0.7045453786849976
Epoch 390, training loss: 0.7721855044364929 = 0.10550161451101303 + 0.1 * 6.666839122772217
Epoch 390, val loss: 0.7103278040885925
Epoch 400, training loss: 0.7582842111587524 = 0.09475832432508469 + 0.1 * 6.635258674621582
Epoch 400, val loss: 0.7170566916465759
Epoch 410, training loss: 0.7495934963226318 = 0.08541781455278397 + 0.1 * 6.641757011413574
Epoch 410, val loss: 0.724603533744812
Epoch 420, training loss: 0.7406184673309326 = 0.07730435580015182 + 0.1 * 6.633141040802002
Epoch 420, val loss: 0.7326943874359131
Epoch 430, training loss: 0.7324187755584717 = 0.07020608335733414 + 0.1 * 6.622126579284668
Epoch 430, val loss: 0.7412465810775757
Epoch 440, training loss: 0.7272534966468811 = 0.06396249681711197 + 0.1 * 6.632909774780273
Epoch 440, val loss: 0.7501513361930847
Epoch 450, training loss: 0.719947338104248 = 0.058482203632593155 + 0.1 * 6.614650726318359
Epoch 450, val loss: 0.75920569896698
Epoch 460, training loss: 0.7160102128982544 = 0.05363888293504715 + 0.1 * 6.62371301651001
Epoch 460, val loss: 0.7683631181716919
Epoch 470, training loss: 0.710117757320404 = 0.04935409501194954 + 0.1 * 6.607636451721191
Epoch 470, val loss: 0.7775039076805115
Epoch 480, training loss: 0.7054364085197449 = 0.045538902282714844 + 0.1 * 6.598974704742432
Epoch 480, val loss: 0.7866789698600769
Epoch 490, training loss: 0.7015613317489624 = 0.042126208543777466 + 0.1 * 6.594351291656494
Epoch 490, val loss: 0.795754075050354
Epoch 500, training loss: 0.6992724537849426 = 0.03908015042543411 + 0.1 * 6.601922988891602
Epoch 500, val loss: 0.8047882914543152
Epoch 510, training loss: 0.6942602396011353 = 0.036352481693029404 + 0.1 * 6.57907772064209
Epoch 510, val loss: 0.8136415481567383
Epoch 520, training loss: 0.6943274140357971 = 0.0338917002081871 + 0.1 * 6.6043572425842285
Epoch 520, val loss: 0.8223115801811218
Epoch 530, training loss: 0.6903111934661865 = 0.031685441732406616 + 0.1 * 6.586257457733154
Epoch 530, val loss: 0.8308963179588318
Epoch 540, training loss: 0.6867738962173462 = 0.02969125285744667 + 0.1 * 6.570826053619385
Epoch 540, val loss: 0.8392379283905029
Epoch 550, training loss: 0.6848151087760925 = 0.02787315472960472 + 0.1 * 6.5694193840026855
Epoch 550, val loss: 0.8474409580230713
Epoch 560, training loss: 0.6819184422492981 = 0.02622101455926895 + 0.1 * 6.556973934173584
Epoch 560, val loss: 0.8555148839950562
Epoch 570, training loss: 0.6808595061302185 = 0.02471398003399372 + 0.1 * 6.561454772949219
Epoch 570, val loss: 0.8633913397789001
Epoch 580, training loss: 0.6788328289985657 = 0.02333647944033146 + 0.1 * 6.5549635887146
Epoch 580, val loss: 0.871099591255188
Epoch 590, training loss: 0.6777400374412537 = 0.022072575986385345 + 0.1 * 6.556674480438232
Epoch 590, val loss: 0.878619909286499
Epoch 600, training loss: 0.6753542423248291 = 0.020912252366542816 + 0.1 * 6.544419765472412
Epoch 600, val loss: 0.8860166668891907
Epoch 610, training loss: 0.6740384101867676 = 0.019844522699713707 + 0.1 * 6.541938781738281
Epoch 610, val loss: 0.893196702003479
Epoch 620, training loss: 0.6732416152954102 = 0.01885944977402687 + 0.1 * 6.543821811676025
Epoch 620, val loss: 0.9002065062522888
Epoch 630, training loss: 0.6716324687004089 = 0.017951970919966698 + 0.1 * 6.536804676055908
Epoch 630, val loss: 0.907123863697052
Epoch 640, training loss: 0.6705418229103088 = 0.017109453678131104 + 0.1 * 6.534323692321777
Epoch 640, val loss: 0.9138138890266418
Epoch 650, training loss: 0.6689551472663879 = 0.016330188140273094 + 0.1 * 6.526249408721924
Epoch 650, val loss: 0.9203185439109802
Epoch 660, training loss: 0.6676647663116455 = 0.015606245957314968 + 0.1 * 6.520585060119629
Epoch 660, val loss: 0.9267558455467224
Epoch 670, training loss: 0.6679739356040955 = 0.014931457117199898 + 0.1 * 6.53042459487915
Epoch 670, val loss: 0.9330081343650818
Epoch 680, training loss: 0.6668139696121216 = 0.014303133822977543 + 0.1 * 6.525108337402344
Epoch 680, val loss: 0.9391282200813293
Epoch 690, training loss: 0.6656330823898315 = 0.013714863918721676 + 0.1 * 6.519182205200195
Epoch 690, val loss: 0.9451534152030945
Epoch 700, training loss: 0.6647490859031677 = 0.013165134936571121 + 0.1 * 6.515839576721191
Epoch 700, val loss: 0.9510021209716797
Epoch 710, training loss: 0.6630833148956299 = 0.012649103067815304 + 0.1 * 6.5043416023254395
Epoch 710, val loss: 0.9567763805389404
Epoch 720, training loss: 0.6640751957893372 = 0.012164206244051456 + 0.1 * 6.519109725952148
Epoch 720, val loss: 0.9624112844467163
Epoch 730, training loss: 0.664164662361145 = 0.011710341088473797 + 0.1 * 6.524542808532715
Epoch 730, val loss: 0.9679186940193176
Epoch 740, training loss: 0.6622892022132874 = 0.0112857511267066 + 0.1 * 6.510034561157227
Epoch 740, val loss: 0.9734152555465698
Epoch 750, training loss: 0.6601865291595459 = 0.010885274037718773 + 0.1 * 6.493012428283691
Epoch 750, val loss: 0.9787233471870422
Epoch 760, training loss: 0.660381019115448 = 0.01050538569688797 + 0.1 * 6.498756408691406
Epoch 760, val loss: 0.9838782548904419
Epoch 770, training loss: 0.659360408782959 = 0.010146901942789555 + 0.1 * 6.4921345710754395
Epoch 770, val loss: 0.9889967441558838
Epoch 780, training loss: 0.6587712168693542 = 0.009808058850467205 + 0.1 * 6.489631175994873
Epoch 780, val loss: 0.993983268737793
Epoch 790, training loss: 0.6582384705543518 = 0.009488792158663273 + 0.1 * 6.487496376037598
Epoch 790, val loss: 0.9988583326339722
Epoch 800, training loss: 0.6575419902801514 = 0.00918861199170351 + 0.1 * 6.48353385925293
Epoch 800, val loss: 1.0036842823028564
Epoch 810, training loss: 0.6565884947776794 = 0.008901823312044144 + 0.1 * 6.476866245269775
Epoch 810, val loss: 1.008431315422058
Epoch 820, training loss: 0.6572835445404053 = 0.008628393523395061 + 0.1 * 6.486551761627197
Epoch 820, val loss: 1.013014316558838
Epoch 830, training loss: 0.6567217707633972 = 0.008368800394237041 + 0.1 * 6.483529567718506
Epoch 830, val loss: 1.017470121383667
Epoch 840, training loss: 0.6551509499549866 = 0.008123788051307201 + 0.1 * 6.470271110534668
Epoch 840, val loss: 1.0219836235046387
Epoch 850, training loss: 0.655265212059021 = 0.007889938540756702 + 0.1 * 6.473752498626709
Epoch 850, val loss: 1.0263659954071045
Epoch 860, training loss: 0.6545171737670898 = 0.00766709353774786 + 0.1 * 6.468501091003418
Epoch 860, val loss: 1.0305825471878052
Epoch 870, training loss: 0.6549921035766602 = 0.0074544632807374 + 0.1 * 6.475376129150391
Epoch 870, val loss: 1.034790277481079
Epoch 880, training loss: 0.6532303094863892 = 0.007253474090248346 + 0.1 * 6.459768295288086
Epoch 880, val loss: 1.0389505624771118
Epoch 890, training loss: 0.6529439091682434 = 0.0070604728534817696 + 0.1 * 6.458834171295166
Epoch 890, val loss: 1.0430643558502197
Epoch 900, training loss: 0.654559314250946 = 0.006875453516840935 + 0.1 * 6.4768385887146
Epoch 900, val loss: 1.0470446348190308
Epoch 910, training loss: 0.6530684232711792 = 0.006698661483824253 + 0.1 * 6.46369743347168
Epoch 910, val loss: 1.0508897304534912
Epoch 920, training loss: 0.6530733704566956 = 0.006529470905661583 + 0.1 * 6.465439319610596
Epoch 920, val loss: 1.0547970533370972
Epoch 930, training loss: 0.6513100862503052 = 0.006367404479533434 + 0.1 * 6.449427127838135
Epoch 930, val loss: 1.0585620403289795
Epoch 940, training loss: 0.65477055311203 = 0.006211793515831232 + 0.1 * 6.485587120056152
Epoch 940, val loss: 1.0623124837875366
Epoch 950, training loss: 0.6505749821662903 = 0.006063575390726328 + 0.1 * 6.4451141357421875
Epoch 950, val loss: 1.065901517868042
Epoch 960, training loss: 0.652569591999054 = 0.005920774303376675 + 0.1 * 6.466487884521484
Epoch 960, val loss: 1.0695425271987915
Epoch 970, training loss: 0.6509401202201843 = 0.005783385597169399 + 0.1 * 6.45156717300415
Epoch 970, val loss: 1.0730483531951904
Epoch 980, training loss: 0.6504619121551514 = 0.005652299150824547 + 0.1 * 6.448095798492432
Epoch 980, val loss: 1.0765588283538818
Epoch 990, training loss: 0.6492557525634766 = 0.005525414366275072 + 0.1 * 6.43730354309082
Epoch 990, val loss: 1.0799423456192017
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.790708065032959 = 1.9310219287872314 + 0.1 * 8.596860885620117
Epoch 0, val loss: 1.9319097995758057
Epoch 10, training loss: 2.78119158744812 = 1.9215095043182373 + 0.1 * 8.596819877624512
Epoch 10, val loss: 1.9223766326904297
Epoch 20, training loss: 2.769705295562744 = 1.9100470542907715 + 0.1 * 8.59658145904541
Epoch 20, val loss: 1.9109340906143188
Epoch 30, training loss: 2.753591537475586 = 1.894127368927002 + 0.1 * 8.59464168548584
Epoch 30, val loss: 1.8951117992401123
Epoch 40, training loss: 2.728348970413208 = 1.8706380128860474 + 0.1 * 8.577110290527344
Epoch 40, val loss: 1.872379183769226
Epoch 50, training loss: 2.6856627464294434 = 1.8378536701202393 + 0.1 * 8.478089332580566
Epoch 50, val loss: 1.8423570394515991
Epoch 60, training loss: 2.6111929416656494 = 1.8004099130630493 + 0.1 * 8.107830047607422
Epoch 60, val loss: 1.8100817203521729
Epoch 70, training loss: 2.5543437004089355 = 1.7607656717300415 + 0.1 * 7.9357805252075195
Epoch 70, val loss: 1.7753344774246216
Epoch 80, training loss: 2.4789862632751465 = 1.711730718612671 + 0.1 * 7.6725544929504395
Epoch 80, val loss: 1.7302907705307007
Epoch 90, training loss: 2.390352725982666 = 1.6506973505020142 + 0.1 * 7.396554946899414
Epoch 90, val loss: 1.6772242784500122
Epoch 100, training loss: 2.298297882080078 = 1.5748322010040283 + 0.1 * 7.2346577644348145
Epoch 100, val loss: 1.611432433128357
Epoch 110, training loss: 2.2004315853118896 = 1.4858992099761963 + 0.1 * 7.145322799682617
Epoch 110, val loss: 1.5340793132781982
Epoch 120, training loss: 2.0984225273132324 = 1.3925065994262695 + 0.1 * 7.059159755706787
Epoch 120, val loss: 1.45591402053833
Epoch 130, training loss: 1.9977166652679443 = 1.297792673110962 + 0.1 * 6.999240398406982
Epoch 130, val loss: 1.3792115449905396
Epoch 140, training loss: 1.8998005390167236 = 1.2028617858886719 + 0.1 * 6.969388008117676
Epoch 140, val loss: 1.3034995794296265
Epoch 150, training loss: 1.8055684566497803 = 1.110188364982605 + 0.1 * 6.953801155090332
Epoch 150, val loss: 1.230602502822876
Epoch 160, training loss: 1.7175939083099365 = 1.0230334997177124 + 0.1 * 6.945603847503662
Epoch 160, val loss: 1.1637797355651855
Epoch 170, training loss: 1.6354022026062012 = 0.9413384795188904 + 0.1 * 6.94063663482666
Epoch 170, val loss: 1.1029562950134277
Epoch 180, training loss: 1.5573949813842773 = 0.8639310598373413 + 0.1 * 6.934639930725098
Epoch 180, val loss: 1.0456480979919434
Epoch 190, training loss: 1.4833345413208008 = 0.7903175354003906 + 0.1 * 6.93017053604126
Epoch 190, val loss: 0.9914364218711853
Epoch 200, training loss: 1.4139025211334229 = 0.7213094234466553 + 0.1 * 6.925930500030518
Epoch 200, val loss: 0.9413810968399048
Epoch 210, training loss: 1.3503681421279907 = 0.6582739949226379 + 0.1 * 6.920941352844238
Epoch 210, val loss: 0.8974735140800476
Epoch 220, training loss: 1.293466329574585 = 0.6021350026130676 + 0.1 * 6.9133124351501465
Epoch 220, val loss: 0.8613964319229126
Epoch 230, training loss: 1.243411898612976 = 0.5527499914169312 + 0.1 * 6.906619071960449
Epoch 230, val loss: 0.8332010507583618
Epoch 240, training loss: 1.1990495920181274 = 0.5092829465866089 + 0.1 * 6.8976664543151855
Epoch 240, val loss: 0.8122720718383789
Epoch 250, training loss: 1.160048246383667 = 0.47026240825653076 + 0.1 * 6.8978590965271
Epoch 250, val loss: 0.7973006367683411
Epoch 260, training loss: 1.1227383613586426 = 0.4348384141921997 + 0.1 * 6.87899923324585
Epoch 260, val loss: 0.787352979183197
Epoch 270, training loss: 1.088523030281067 = 0.4018188416957855 + 0.1 * 6.86704158782959
Epoch 270, val loss: 0.7810693383216858
Epoch 280, training loss: 1.0563006401062012 = 0.3705039620399475 + 0.1 * 6.857966899871826
Epoch 280, val loss: 0.7779005765914917
Epoch 290, training loss: 1.0256999731063843 = 0.3408326208591461 + 0.1 * 6.8486738204956055
Epoch 290, val loss: 0.7773373126983643
Epoch 300, training loss: 0.9960134029388428 = 0.3125667870044708 + 0.1 * 6.834465980529785
Epoch 300, val loss: 0.7789126634597778
Epoch 310, training loss: 0.9681611061096191 = 0.2855016887187958 + 0.1 * 6.826594352722168
Epoch 310, val loss: 0.7821533679962158
Epoch 320, training loss: 0.9406188726425171 = 0.2596031129360199 + 0.1 * 6.810157775878906
Epoch 320, val loss: 0.786870002746582
Epoch 330, training loss: 0.9148421883583069 = 0.23486532270908356 + 0.1 * 6.799768447875977
Epoch 330, val loss: 0.7929242253303528
Epoch 340, training loss: 0.8909381628036499 = 0.21167150139808655 + 0.1 * 6.792666912078857
Epoch 340, val loss: 0.8003167510032654
Epoch 350, training loss: 0.8687188625335693 = 0.19046463072299957 + 0.1 * 6.7825422286987305
Epoch 350, val loss: 0.809156596660614
Epoch 360, training loss: 0.8489516377449036 = 0.17117123305797577 + 0.1 * 6.777803897857666
Epoch 360, val loss: 0.8195809125900269
Epoch 370, training loss: 0.8308582901954651 = 0.1538216471672058 + 0.1 * 6.770366191864014
Epoch 370, val loss: 0.8314328789710999
Epoch 380, training loss: 0.8139955997467041 = 0.13830086588859558 + 0.1 * 6.7569475173950195
Epoch 380, val loss: 0.8443745970726013
Epoch 390, training loss: 0.8001665472984314 = 0.12446927279233932 + 0.1 * 6.756972312927246
Epoch 390, val loss: 0.8582388162612915
Epoch 400, training loss: 0.7868298888206482 = 0.11222299188375473 + 0.1 * 6.746068954467773
Epoch 400, val loss: 0.8727166652679443
Epoch 410, training loss: 0.7758590579032898 = 0.10142350941896439 + 0.1 * 6.744355201721191
Epoch 410, val loss: 0.8875694274902344
Epoch 420, training loss: 0.7653742432594299 = 0.09195708483457565 + 0.1 * 6.734171390533447
Epoch 420, val loss: 0.9026358127593994
Epoch 430, training loss: 0.7559189796447754 = 0.08361819386482239 + 0.1 * 6.7230072021484375
Epoch 430, val loss: 0.9179736375808716
Epoch 440, training loss: 0.7482005953788757 = 0.07628180831670761 + 0.1 * 6.719188213348389
Epoch 440, val loss: 0.9333248138427734
Epoch 450, training loss: 0.7411547303199768 = 0.06981438398361206 + 0.1 * 6.713403224945068
Epoch 450, val loss: 0.9487494826316833
Epoch 460, training loss: 0.734642505645752 = 0.06407826393842697 + 0.1 * 6.705642223358154
Epoch 460, val loss: 0.9641764163970947
Epoch 470, training loss: 0.7304607629776001 = 0.0589708648622036 + 0.1 * 6.714899063110352
Epoch 470, val loss: 0.9795202016830444
Epoch 480, training loss: 0.7239888310432434 = 0.05442512035369873 + 0.1 * 6.695636749267578
Epoch 480, val loss: 0.994616687297821
Epoch 490, training loss: 0.7202922105789185 = 0.050354182720184326 + 0.1 * 6.699380397796631
Epoch 490, val loss: 1.0095452070236206
Epoch 500, training loss: 0.7153849005699158 = 0.0467086099088192 + 0.1 * 6.686762809753418
Epoch 500, val loss: 1.0241174697875977
Epoch 510, training loss: 0.711900532245636 = 0.043422598391771317 + 0.1 * 6.684779167175293
Epoch 510, val loss: 1.0385026931762695
Epoch 520, training loss: 0.7082898020744324 = 0.04045187309384346 + 0.1 * 6.678379058837891
Epoch 520, val loss: 1.0526721477508545
Epoch 530, training loss: 0.7046226263046265 = 0.03776044771075249 + 0.1 * 6.66862154006958
Epoch 530, val loss: 1.066550374031067
Epoch 540, training loss: 0.7015453577041626 = 0.035318825393915176 + 0.1 * 6.662264823913574
Epoch 540, val loss: 1.0801502466201782
Epoch 550, training loss: 0.6988380551338196 = 0.03309730067849159 + 0.1 * 6.657407283782959
Epoch 550, val loss: 1.093461275100708
Epoch 560, training loss: 0.6958264112472534 = 0.031069204211235046 + 0.1 * 6.647572040557861
Epoch 560, val loss: 1.1065711975097656
Epoch 570, training loss: 0.6967538595199585 = 0.029213611036539078 + 0.1 * 6.675402641296387
Epoch 570, val loss: 1.119411826133728
Epoch 580, training loss: 0.6928282380104065 = 0.027522513642907143 + 0.1 * 6.653057098388672
Epoch 580, val loss: 1.1317476034164429
Epoch 590, training loss: 0.6912563443183899 = 0.02596963196992874 + 0.1 * 6.652867317199707
Epoch 590, val loss: 1.1439685821533203
Epoch 600, training loss: 0.6883559823036194 = 0.024543624371290207 + 0.1 * 6.638123035430908
Epoch 600, val loss: 1.1558489799499512
Epoch 610, training loss: 0.6862525343894958 = 0.023227233439683914 + 0.1 * 6.630253314971924
Epoch 610, val loss: 1.1675843000411987
Epoch 620, training loss: 0.6847731471061707 = 0.02201344259083271 + 0.1 * 6.627596855163574
Epoch 620, val loss: 1.1789630651474
Epoch 630, training loss: 0.6835845708847046 = 0.02089163288474083 + 0.1 * 6.626929759979248
Epoch 630, val loss: 1.1901341676712036
Epoch 640, training loss: 0.6810818314552307 = 0.01985201984643936 + 0.1 * 6.612297534942627
Epoch 640, val loss: 1.2010769844055176
Epoch 650, training loss: 0.6798648834228516 = 0.01888599805533886 + 0.1 * 6.60978889465332
Epoch 650, val loss: 1.2119046449661255
Epoch 660, training loss: 0.6783537864685059 = 0.017988452687859535 + 0.1 * 6.603653430938721
Epoch 660, val loss: 1.222419261932373
Epoch 670, training loss: 0.6790941953659058 = 0.017156552523374557 + 0.1 * 6.619376182556152
Epoch 670, val loss: 1.2325955629348755
Epoch 680, training loss: 0.6763677000999451 = 0.01638355478644371 + 0.1 * 6.599841594696045
Epoch 680, val loss: 1.2425012588500977
Epoch 690, training loss: 0.6751020550727844 = 0.015663448721170425 + 0.1 * 6.594386100769043
Epoch 690, val loss: 1.2522244453430176
Epoch 700, training loss: 0.6745200157165527 = 0.014989364892244339 + 0.1 * 6.595306396484375
Epoch 700, val loss: 1.2618449926376343
Epoch 710, training loss: 0.6732659339904785 = 0.014358474873006344 + 0.1 * 6.589074611663818
Epoch 710, val loss: 1.2712287902832031
Epoch 720, training loss: 0.673851728439331 = 0.013767612166702747 + 0.1 * 6.600841045379639
Epoch 720, val loss: 1.280426263809204
Epoch 730, training loss: 0.6714153289794922 = 0.01321372203528881 + 0.1 * 6.582016468048096
Epoch 730, val loss: 1.289355754852295
Epoch 740, training loss: 0.6710230112075806 = 0.01269437838345766 + 0.1 * 6.583286285400391
Epoch 740, val loss: 1.2981528043746948
Epoch 750, training loss: 0.6701828241348267 = 0.012206760235130787 + 0.1 * 6.579760551452637
Epoch 750, val loss: 1.3067610263824463
Epoch 760, training loss: 0.6698101758956909 = 0.011748328804969788 + 0.1 * 6.580618381500244
Epoch 760, val loss: 1.3151605129241943
Epoch 770, training loss: 0.6685011386871338 = 0.01131611317396164 + 0.1 * 6.571849822998047
Epoch 770, val loss: 1.323462724685669
Epoch 780, training loss: 0.6689726114273071 = 0.010907935909926891 + 0.1 * 6.580646514892578
Epoch 780, val loss: 1.3315777778625488
Epoch 790, training loss: 0.6676174402236938 = 0.01052352786064148 + 0.1 * 6.570938587188721
Epoch 790, val loss: 1.3394514322280884
Epoch 800, training loss: 0.6682384610176086 = 0.010159668512642384 + 0.1 * 6.580787658691406
Epoch 800, val loss: 1.3472598791122437
Epoch 810, training loss: 0.6663660407066345 = 0.009816275909543037 + 0.1 * 6.565497398376465
Epoch 810, val loss: 1.3548303842544556
Epoch 820, training loss: 0.6655955910682678 = 0.00949104130268097 + 0.1 * 6.5610456466674805
Epoch 820, val loss: 1.3622913360595703
Epoch 830, training loss: 0.6648745536804199 = 0.00918290950357914 + 0.1 * 6.556916236877441
Epoch 830, val loss: 1.369596242904663
Epoch 840, training loss: 0.6656302809715271 = 0.008891233243048191 + 0.1 * 6.567390441894531
Epoch 840, val loss: 1.3767094612121582
Epoch 850, training loss: 0.6634827256202698 = 0.008613933809101582 + 0.1 * 6.548687934875488
Epoch 850, val loss: 1.3837214708328247
Epoch 860, training loss: 0.6633471846580505 = 0.008350884541869164 + 0.1 * 6.549962520599365
Epoch 860, val loss: 1.3906004428863525
Epoch 870, training loss: 0.6632392406463623 = 0.008100263774394989 + 0.1 * 6.551389694213867
Epoch 870, val loss: 1.3974031209945679
Epoch 880, training loss: 0.6624951362609863 = 0.007861465215682983 + 0.1 * 6.546336650848389
Epoch 880, val loss: 1.4040409326553345
Epoch 890, training loss: 0.6625473499298096 = 0.007635171990841627 + 0.1 * 6.549121379852295
Epoch 890, val loss: 1.4103970527648926
Epoch 900, training loss: 0.6608842611312866 = 0.007419018540531397 + 0.1 * 6.5346527099609375
Epoch 900, val loss: 1.4167917966842651
Epoch 910, training loss: 0.6647656559944153 = 0.0072125098668038845 + 0.1 * 6.575531482696533
Epoch 910, val loss: 1.4230858087539673
Epoch 920, training loss: 0.660315215587616 = 0.0070156822912395 + 0.1 * 6.532994747161865
Epoch 920, val loss: 1.429142713546753
Epoch 930, training loss: 0.6595544219017029 = 0.0068280333653092384 + 0.1 * 6.52726411819458
Epoch 930, val loss: 1.4351046085357666
Epoch 940, training loss: 0.6606764793395996 = 0.006648148875683546 + 0.1 * 6.540283203125
Epoch 940, val loss: 1.4410401582717896
Epoch 950, training loss: 0.6596084237098694 = 0.006476348266005516 + 0.1 * 6.531320571899414
Epoch 950, val loss: 1.4467520713806152
Epoch 960, training loss: 0.6583194732666016 = 0.006312160287052393 + 0.1 * 6.520073413848877
Epoch 960, val loss: 1.452354907989502
Epoch 970, training loss: 0.6583063006401062 = 0.006154872011393309 + 0.1 * 6.521513938903809
Epoch 970, val loss: 1.457944631576538
Epoch 980, training loss: 0.6590573191642761 = 0.006003598682582378 + 0.1 * 6.530537128448486
Epoch 980, val loss: 1.4634474515914917
Epoch 990, training loss: 0.657913863658905 = 0.005858911667019129 + 0.1 * 6.520549297332764
Epoch 990, val loss: 1.4687621593475342
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 2.8037331104278564 = 1.944049596786499 + 0.1 * 8.596834182739258
Epoch 0, val loss: 1.9418915510177612
Epoch 10, training loss: 2.793550729751587 = 1.9338748455047607 + 0.1 * 8.596758842468262
Epoch 10, val loss: 1.9314913749694824
Epoch 20, training loss: 2.781450033187866 = 1.9218244552612305 + 0.1 * 8.5962553024292
Epoch 20, val loss: 1.919061303138733
Epoch 30, training loss: 2.764526844024658 = 1.9053140878677368 + 0.1 * 8.592127799987793
Epoch 30, val loss: 1.90194833278656
Epoch 40, training loss: 2.737683057785034 = 1.881248950958252 + 0.1 * 8.56434154510498
Epoch 40, val loss: 1.8772772550582886
Epoch 50, training loss: 2.692099094390869 = 1.8475896120071411 + 0.1 * 8.445096015930176
Epoch 50, val loss: 1.8441849946975708
Epoch 60, training loss: 2.6175501346588135 = 1.807466745376587 + 0.1 * 8.100833892822266
Epoch 60, val loss: 1.8073809146881104
Epoch 70, training loss: 2.5432629585266113 = 1.7680679559707642 + 0.1 * 7.751948833465576
Epoch 70, val loss: 1.7732242345809937
Epoch 80, training loss: 2.4647462368011475 = 1.7254163026809692 + 0.1 * 7.393298625946045
Epoch 80, val loss: 1.7352153062820435
Epoch 90, training loss: 2.3918542861938477 = 1.6712310314178467 + 0.1 * 7.206233501434326
Epoch 90, val loss: 1.6855448484420776
Epoch 100, training loss: 2.31372332572937 = 1.601453423500061 + 0.1 * 7.1226983070373535
Epoch 100, val loss: 1.6226177215576172
Epoch 110, training loss: 2.222790241241455 = 1.5149589776992798 + 0.1 * 7.078312397003174
Epoch 110, val loss: 1.547086477279663
Epoch 120, training loss: 2.119997501373291 = 1.41558039188385 + 0.1 * 7.044170379638672
Epoch 120, val loss: 1.4610066413879395
Epoch 130, training loss: 2.0104424953460693 = 1.3092474937438965 + 0.1 * 7.01194953918457
Epoch 130, val loss: 1.370396375656128
Epoch 140, training loss: 1.8993703126907349 = 1.200846552848816 + 0.1 * 6.9852375984191895
Epoch 140, val loss: 1.2793769836425781
Epoch 150, training loss: 1.7910752296447754 = 1.094240427017212 + 0.1 * 6.968347549438477
Epoch 150, val loss: 1.1903127431869507
Epoch 160, training loss: 1.6879372596740723 = 0.992017924785614 + 0.1 * 6.959192752838135
Epoch 160, val loss: 1.1059808731079102
Epoch 170, training loss: 1.5908758640289307 = 0.8954219818115234 + 0.1 * 6.9545392990112305
Epoch 170, val loss: 1.0268746614456177
Epoch 180, training loss: 1.50028395652771 = 0.805040180683136 + 0.1 * 6.952437400817871
Epoch 180, val loss: 0.9544227719306946
Epoch 190, training loss: 1.4167938232421875 = 0.7217902541160583 + 0.1 * 6.950035095214844
Epoch 190, val loss: 0.8893482685089111
Epoch 200, training loss: 1.3406641483306885 = 0.6459161043167114 + 0.1 * 6.947481155395508
Epoch 200, val loss: 0.8318975567817688
Epoch 210, training loss: 1.2720069885253906 = 0.577576220035553 + 0.1 * 6.94430685043335
Epoch 210, val loss: 0.7823746800422668
Epoch 220, training loss: 1.2104120254516602 = 0.5163405537605286 + 0.1 * 6.940713882446289
Epoch 220, val loss: 0.7404398322105408
Epoch 230, training loss: 1.154903531074524 = 0.46115800738334656 + 0.1 * 6.937455177307129
Epoch 230, val loss: 0.705876350402832
Epoch 240, training loss: 1.1042490005493164 = 0.41081422567367554 + 0.1 * 6.934347152709961
Epoch 240, val loss: 0.677940845489502
Epoch 250, training loss: 1.0570300817489624 = 0.3643456995487213 + 0.1 * 6.926843643188477
Epoch 250, val loss: 0.6555241942405701
Epoch 260, training loss: 1.0133390426635742 = 0.3211424946784973 + 0.1 * 6.921965599060059
Epoch 260, val loss: 0.6373729705810547
Epoch 270, training loss: 0.9736090898513794 = 0.2815099060535431 + 0.1 * 6.920991897583008
Epoch 270, val loss: 0.6229642033576965
Epoch 280, training loss: 0.9372596740722656 = 0.24595554172992706 + 0.1 * 6.913041114807129
Epoch 280, val loss: 0.612200140953064
Epoch 290, training loss: 0.9051129817962646 = 0.21457615494728088 + 0.1 * 6.905368328094482
Epoch 290, val loss: 0.6049513220787048
Epoch 300, training loss: 0.8772913217544556 = 0.1873449981212616 + 0.1 * 6.899463176727295
Epoch 300, val loss: 0.6009172797203064
Epoch 310, training loss: 0.8535817861557007 = 0.16424022614955902 + 0.1 * 6.893415451049805
Epoch 310, val loss: 0.5996439456939697
Epoch 320, training loss: 0.8328602313995361 = 0.14469054341316223 + 0.1 * 6.881696701049805
Epoch 320, val loss: 0.600707471370697
Epoch 330, training loss: 0.8151007294654846 = 0.1280020922422409 + 0.1 * 6.870985984802246
Epoch 330, val loss: 0.6037784814834595
Epoch 340, training loss: 0.8019883632659912 = 0.11370258033275604 + 0.1 * 6.882857799530029
Epoch 340, val loss: 0.6084233522415161
Epoch 350, training loss: 0.7864131927490234 = 0.10151218622922897 + 0.1 * 6.849009990692139
Epoch 350, val loss: 0.6142163276672363
Epoch 360, training loss: 0.7745738625526428 = 0.090970478951931 + 0.1 * 6.836033821105957
Epoch 360, val loss: 0.6209656000137329
Epoch 370, training loss: 0.7643515467643738 = 0.08181051164865494 + 0.1 * 6.825409889221191
Epoch 370, val loss: 0.6284955739974976
Epoch 380, training loss: 0.754810094833374 = 0.07386312633752823 + 0.1 * 6.809469223022461
Epoch 380, val loss: 0.6363540887832642
Epoch 390, training loss: 0.7468932867050171 = 0.06689406931400299 + 0.1 * 6.799992084503174
Epoch 390, val loss: 0.6446331739425659
Epoch 400, training loss: 0.7401096820831299 = 0.060762789100408554 + 0.1 * 6.793468952178955
Epoch 400, val loss: 0.6531237959861755
Epoch 410, training loss: 0.733343243598938 = 0.055387772619724274 + 0.1 * 6.77955436706543
Epoch 410, val loss: 0.6615867614746094
Epoch 420, training loss: 0.7280619740486145 = 0.050616756081581116 + 0.1 * 6.774452209472656
Epoch 420, val loss: 0.6702092289924622
Epoch 430, training loss: 0.7233337759971619 = 0.046377673745155334 + 0.1 * 6.769561290740967
Epoch 430, val loss: 0.6788321733474731
Epoch 440, training loss: 0.7187068462371826 = 0.04261130467057228 + 0.1 * 6.760955333709717
Epoch 440, val loss: 0.68730229139328
Epoch 450, training loss: 0.7150193452835083 = 0.039246756583452225 + 0.1 * 6.757725715637207
Epoch 450, val loss: 0.6957355737686157
Epoch 460, training loss: 0.7113624215126038 = 0.03623673692345619 + 0.1 * 6.751256465911865
Epoch 460, val loss: 0.7040590643882751
Epoch 470, training loss: 0.7076161503791809 = 0.03354338929057121 + 0.1 * 6.740727424621582
Epoch 470, val loss: 0.712226927280426
Epoch 480, training loss: 0.7056501507759094 = 0.031129848212003708 + 0.1 * 6.745203018188477
Epoch 480, val loss: 0.7201982140541077
Epoch 490, training loss: 0.7023362517356873 = 0.028961453586816788 + 0.1 * 6.733747482299805
Epoch 490, val loss: 0.7280157804489136
Epoch 500, training loss: 0.7004684209823608 = 0.027000080794095993 + 0.1 * 6.734683513641357
Epoch 500, val loss: 0.7356783747673035
Epoch 510, training loss: 0.6969796419143677 = 0.02522956393659115 + 0.1 * 6.717500686645508
Epoch 510, val loss: 0.7431377172470093
Epoch 520, training loss: 0.6955559253692627 = 0.023623760789632797 + 0.1 * 6.719321250915527
Epoch 520, val loss: 0.7503951787948608
Epoch 530, training loss: 0.6921517252922058 = 0.022162672132253647 + 0.1 * 6.699890613555908
Epoch 530, val loss: 0.7575868368148804
Epoch 540, training loss: 0.6913073658943176 = 0.02083057351410389 + 0.1 * 6.70476770401001
Epoch 540, val loss: 0.7645831108093262
Epoch 550, training loss: 0.6890557408332825 = 0.01962175779044628 + 0.1 * 6.694340229034424
Epoch 550, val loss: 0.7714271545410156
Epoch 560, training loss: 0.6872045993804932 = 0.01851949281990528 + 0.1 * 6.686850547790527
Epoch 560, val loss: 0.7779608964920044
Epoch 570, training loss: 0.6852822303771973 = 0.017508942633867264 + 0.1 * 6.677732944488525
Epoch 570, val loss: 0.7844401597976685
Epoch 580, training loss: 0.6827068328857422 = 0.0165823083370924 + 0.1 * 6.661245346069336
Epoch 580, val loss: 0.7907072901725769
Epoch 590, training loss: 0.6813896298408508 = 0.015726543962955475 + 0.1 * 6.656630516052246
Epoch 590, val loss: 0.7969544529914856
Epoch 600, training loss: 0.6794421076774597 = 0.014941693283617496 + 0.1 * 6.6450042724609375
Epoch 600, val loss: 0.8029292821884155
Epoch 610, training loss: 0.6805582642555237 = 0.014213730581104755 + 0.1 * 6.663444995880127
Epoch 610, val loss: 0.8088117837905884
Epoch 620, training loss: 0.6771155595779419 = 0.013542961329221725 + 0.1 * 6.635725975036621
Epoch 620, val loss: 0.8145153522491455
Epoch 630, training loss: 0.6758988499641418 = 0.01291901245713234 + 0.1 * 6.629798412322998
Epoch 630, val loss: 0.8201211094856262
Epoch 640, training loss: 0.6757943630218506 = 0.012338020838797092 + 0.1 * 6.634562969207764
Epoch 640, val loss: 0.8256441354751587
Epoch 650, training loss: 0.6741697788238525 = 0.011802523396909237 + 0.1 * 6.623672962188721
Epoch 650, val loss: 0.8309508562088013
Epoch 660, training loss: 0.6753877997398376 = 0.011301528662443161 + 0.1 * 6.640862941741943
Epoch 660, val loss: 0.836139976978302
Epoch 670, training loss: 0.6715359687805176 = 0.010837256908416748 + 0.1 * 6.606986999511719
Epoch 670, val loss: 0.8411990404129028
Epoch 680, training loss: 0.671032190322876 = 0.010404507629573345 + 0.1 * 6.606276512145996
Epoch 680, val loss: 0.8460695743560791
Epoch 690, training loss: 0.6695165634155273 = 0.00999691616743803 + 0.1 * 6.595196723937988
Epoch 690, val loss: 0.8508725762367249
Epoch 700, training loss: 0.6740986704826355 = 0.00961232464760542 + 0.1 * 6.644863605499268
Epoch 700, val loss: 0.8556734323501587
Epoch 710, training loss: 0.6679121255874634 = 0.009255344048142433 + 0.1 * 6.5865678787231445
Epoch 710, val loss: 0.8603028059005737
Epoch 720, training loss: 0.6678168773651123 = 0.008919322863221169 + 0.1 * 6.588975429534912
Epoch 720, val loss: 0.864811360836029
Epoch 730, training loss: 0.6669771671295166 = 0.008600316941738129 + 0.1 * 6.583768367767334
Epoch 730, val loss: 0.8692829012870789
Epoch 740, training loss: 0.6652999520301819 = 0.008300002664327621 + 0.1 * 6.5699992179870605
Epoch 740, val loss: 0.873665988445282
Epoch 750, training loss: 0.6672903299331665 = 0.008016500622034073 + 0.1 * 6.592738151550293
Epoch 750, val loss: 0.8780518770217896
Epoch 760, training loss: 0.6651073098182678 = 0.0077519831247627735 + 0.1 * 6.573553085327148
Epoch 760, val loss: 0.882212221622467
Epoch 770, training loss: 0.6662613153457642 = 0.007500638719648123 + 0.1 * 6.587606430053711
Epoch 770, val loss: 0.886320948600769
Epoch 780, training loss: 0.6633119583129883 = 0.007262764032930136 + 0.1 * 6.560492038726807
Epoch 780, val loss: 0.8903021216392517
Epoch 790, training loss: 0.6637216806411743 = 0.00703769363462925 + 0.1 * 6.566839694976807
Epoch 790, val loss: 0.8942350149154663
Epoch 800, training loss: 0.6628071069717407 = 0.006823693867772818 + 0.1 * 6.559834003448486
Epoch 800, val loss: 0.8981288075447083
Epoch 810, training loss: 0.6621237993240356 = 0.006621339824050665 + 0.1 * 6.555024147033691
Epoch 810, val loss: 0.9019003510475159
Epoch 820, training loss: 0.6603535413742065 = 0.006427929736673832 + 0.1 * 6.539255619049072
Epoch 820, val loss: 0.9056371450424194
Epoch 830, training loss: 0.6605449914932251 = 0.006243904586881399 + 0.1 * 6.54301118850708
Epoch 830, val loss: 0.9093087911605835
Epoch 840, training loss: 0.6597486734390259 = 0.006068659480661154 + 0.1 * 6.536800384521484
Epoch 840, val loss: 0.9129689335823059
Epoch 850, training loss: 0.6602211594581604 = 0.005903125274926424 + 0.1 * 6.543180465698242
Epoch 850, val loss: 0.9163908958435059
Epoch 860, training loss: 0.661018967628479 = 0.005744745023548603 + 0.1 * 6.552742004394531
Epoch 860, val loss: 0.9198170900344849
Epoch 870, training loss: 0.6581814289093018 = 0.0055933608673512936 + 0.1 * 6.525880813598633
Epoch 870, val loss: 0.9232029914855957
Epoch 880, training loss: 0.6578652858734131 = 0.005448687355965376 + 0.1 * 6.524165630340576
Epoch 880, val loss: 0.9265034794807434
Epoch 890, training loss: 0.6632035970687866 = 0.005310019478201866 + 0.1 * 6.578935623168945
Epoch 890, val loss: 0.9297874569892883
Epoch 900, training loss: 0.6571925282478333 = 0.005178367719054222 + 0.1 * 6.520141124725342
Epoch 900, val loss: 0.9330112338066101
Epoch 910, training loss: 0.6568999886512756 = 0.005053353030234575 + 0.1 * 6.518466472625732
Epoch 910, val loss: 0.9360206723213196
Epoch 920, training loss: 0.6569690704345703 = 0.0049318578094244 + 0.1 * 6.520371913909912
Epoch 920, val loss: 0.9391342997550964
Epoch 930, training loss: 0.65596604347229 = 0.00481618195772171 + 0.1 * 6.51149845123291
Epoch 930, val loss: 0.9421502947807312
Epoch 940, training loss: 0.6554039716720581 = 0.004705158527940512 + 0.1 * 6.506988525390625
Epoch 940, val loss: 0.9450699687004089
Epoch 950, training loss: 0.6552709341049194 = 0.004597309976816177 + 0.1 * 6.5067362785339355
Epoch 950, val loss: 0.9481327533721924
Epoch 960, training loss: 0.6556179523468018 = 0.004495448432862759 + 0.1 * 6.51122522354126
Epoch 960, val loss: 0.9509574770927429
Epoch 970, training loss: 0.6564149856567383 = 0.004397430457174778 + 0.1 * 6.520174980163574
Epoch 970, val loss: 0.9537757039070129
Epoch 980, training loss: 0.6540841460227966 = 0.0043031214736402035 + 0.1 * 6.497810363769531
Epoch 980, val loss: 0.9565404653549194
Epoch 990, training loss: 0.6544598340988159 = 0.0042120967991650105 + 0.1 * 6.502477169036865
Epoch 990, val loss: 0.9592742919921875
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8397469688982605
The final CL Acc:0.79753, 0.01552, The final GNN Acc:0.84045, 0.00138
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9556])
updated graph: torch.Size([2, 10634])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.819478750228882 = 1.9597923755645752 + 0.1 * 8.596863746643066
Epoch 0, val loss: 1.9639052152633667
Epoch 10, training loss: 2.808518409729004 = 1.9488376379013062 + 0.1 * 8.596806526184082
Epoch 10, val loss: 1.9534475803375244
Epoch 20, training loss: 2.7951011657714844 = 1.9354511499404907 + 0.1 * 8.596501350402832
Epoch 20, val loss: 1.9402583837509155
Epoch 30, training loss: 2.7763512134552 = 1.9169578552246094 + 0.1 * 8.593934059143066
Epoch 30, val loss: 1.9216104745864868
Epoch 40, training loss: 2.7476305961608887 = 1.8902336359024048 + 0.1 * 8.573968887329102
Epoch 40, val loss: 1.8945941925048828
Epoch 50, training loss: 2.702754259109497 = 1.8542109727859497 + 0.1 * 8.485432624816895
Epoch 50, val loss: 1.8594802618026733
Epoch 60, training loss: 2.6376333236694336 = 1.8165900707244873 + 0.1 * 8.210432052612305
Epoch 60, val loss: 1.8255385160446167
Epoch 70, training loss: 2.590963840484619 = 1.7864136695861816 + 0.1 * 8.045502662658691
Epoch 70, val loss: 1.7987229824066162
Epoch 80, training loss: 2.525221109390259 = 1.7541760206222534 + 0.1 * 7.710451126098633
Epoch 80, val loss: 1.7658268213272095
Epoch 90, training loss: 2.4561009407043457 = 1.7124643325805664 + 0.1 * 7.436366081237793
Epoch 90, val loss: 1.7251107692718506
Epoch 100, training loss: 2.3841848373413086 = 1.657835841178894 + 0.1 * 7.263490200042725
Epoch 100, val loss: 1.6761445999145508
Epoch 110, training loss: 2.3017632961273193 = 1.5862613916397095 + 0.1 * 7.155019283294678
Epoch 110, val loss: 1.6116642951965332
Epoch 120, training loss: 2.2099246978759766 = 1.5014126300811768 + 0.1 * 7.085120677947998
Epoch 120, val loss: 1.5346697568893433
Epoch 130, training loss: 2.113513946533203 = 1.4095784425735474 + 0.1 * 7.03935432434082
Epoch 130, val loss: 1.4554139375686646
Epoch 140, training loss: 2.0148682594299316 = 1.3145155906677246 + 0.1 * 7.003525257110596
Epoch 140, val loss: 1.3762462139129639
Epoch 150, training loss: 1.9137027263641357 = 1.2172797918319702 + 0.1 * 6.964229106903076
Epoch 150, val loss: 1.2965925931930542
Epoch 160, training loss: 1.812060832977295 = 1.1190760135650635 + 0.1 * 6.929847240447998
Epoch 160, val loss: 1.2181843519210815
Epoch 170, training loss: 1.7110209465026855 = 1.0209490060806274 + 0.1 * 6.900719165802002
Epoch 170, val loss: 1.1399058103561401
Epoch 180, training loss: 1.6151537895202637 = 0.9267589449882507 + 0.1 * 6.883948802947998
Epoch 180, val loss: 1.0656107664108276
Epoch 190, training loss: 1.5278851985931396 = 0.8410738110542297 + 0.1 * 6.8681135177612305
Epoch 190, val loss: 0.9994974732398987
Epoch 200, training loss: 1.450395941734314 = 0.7649601697921753 + 0.1 * 6.854357719421387
Epoch 200, val loss: 0.9434687495231628
Epoch 210, training loss: 1.3828585147857666 = 0.6979883313179016 + 0.1 * 6.848702430725098
Epoch 210, val loss: 0.898454487323761
Epoch 220, training loss: 1.3233983516693115 = 0.6389991044998169 + 0.1 * 6.843992710113525
Epoch 220, val loss: 0.8638457655906677
Epoch 230, training loss: 1.268949031829834 = 0.5854328870773315 + 0.1 * 6.835160732269287
Epoch 230, val loss: 0.836876392364502
Epoch 240, training loss: 1.2178189754486084 = 0.5349804759025574 + 0.1 * 6.8283843994140625
Epoch 240, val loss: 0.8151450157165527
Epoch 250, training loss: 1.1689484119415283 = 0.48642057180404663 + 0.1 * 6.8252787590026855
Epoch 250, val loss: 0.7975391745567322
Epoch 260, training loss: 1.123092532157898 = 0.4401501715183258 + 0.1 * 6.829422950744629
Epoch 260, val loss: 0.784548819065094
Epoch 270, training loss: 1.0787041187286377 = 0.396705687046051 + 0.1 * 6.8199849128723145
Epoch 270, val loss: 0.7765397429466248
Epoch 280, training loss: 1.0376300811767578 = 0.35651394724845886 + 0.1 * 6.811161041259766
Epoch 280, val loss: 0.7734915614128113
Epoch 290, training loss: 1.0006968975067139 = 0.3200675845146179 + 0.1 * 6.80629301071167
Epoch 290, val loss: 0.7751197814941406
Epoch 300, training loss: 0.9690313339233398 = 0.28740009665489197 + 0.1 * 6.816312313079834
Epoch 300, val loss: 0.7810072302818298
Epoch 310, training loss: 0.938201904296875 = 0.2584562599658966 + 0.1 * 6.797456741333008
Epoch 310, val loss: 0.7902296185493469
Epoch 320, training loss: 0.912413477897644 = 0.23262088000774384 + 0.1 * 6.79792594909668
Epoch 320, val loss: 0.8022167682647705
Epoch 330, training loss: 0.8888457417488098 = 0.20957249402999878 + 0.1 * 6.792732238769531
Epoch 330, val loss: 0.8162104487419128
Epoch 340, training loss: 0.8669028282165527 = 0.18891389667987823 + 0.1 * 6.7798895835876465
Epoch 340, val loss: 0.8318836092948914
Epoch 350, training loss: 0.8478694558143616 = 0.17032837867736816 + 0.1 * 6.7754106521606445
Epoch 350, val loss: 0.8490254878997803
Epoch 360, training loss: 0.830858588218689 = 0.15365555882453918 + 0.1 * 6.772030353546143
Epoch 360, val loss: 0.8670387268066406
Epoch 370, training loss: 0.8148020505905151 = 0.13877181708812714 + 0.1 * 6.7603020668029785
Epoch 370, val loss: 0.8858397603034973
Epoch 380, training loss: 0.8024317026138306 = 0.125442773103714 + 0.1 * 6.769888877868652
Epoch 380, val loss: 0.9052589535713196
Epoch 390, training loss: 0.7894964814186096 = 0.11361865699291229 + 0.1 * 6.758778095245361
Epoch 390, val loss: 0.9247899651527405
Epoch 400, training loss: 0.7767342925071716 = 0.10309123247861862 + 0.1 * 6.7364301681518555
Epoch 400, val loss: 0.9444491863250732
Epoch 410, training loss: 0.7670859098434448 = 0.09368284046649933 + 0.1 * 6.734030723571777
Epoch 410, val loss: 0.9643459916114807
Epoch 420, training loss: 0.7577623128890991 = 0.08530804514884949 + 0.1 * 6.72454309463501
Epoch 420, val loss: 0.9841009974479675
Epoch 430, training loss: 0.7504180669784546 = 0.07785031199455261 + 0.1 * 6.725677967071533
Epoch 430, val loss: 1.0037611722946167
Epoch 440, training loss: 0.7423498630523682 = 0.07120617479085922 + 0.1 * 6.711437225341797
Epoch 440, val loss: 1.023199439048767
Epoch 450, training loss: 0.735423743724823 = 0.06528552621603012 + 0.1 * 6.701382160186768
Epoch 450, val loss: 1.0423833131790161
Epoch 460, training loss: 0.729019045829773 = 0.060008127242326736 + 0.1 * 6.6901092529296875
Epoch 460, val loss: 1.061052680015564
Epoch 470, training loss: 0.724205493927002 = 0.05530952662229538 + 0.1 * 6.68895959854126
Epoch 470, val loss: 1.0793198347091675
Epoch 480, training loss: 0.7186492085456848 = 0.0510922446846962 + 0.1 * 6.6755690574646
Epoch 480, val loss: 1.0973529815673828
Epoch 490, training loss: 0.7156010866165161 = 0.047298215329647064 + 0.1 * 6.683028697967529
Epoch 490, val loss: 1.1149553060531616
Epoch 500, training loss: 0.7121045589447021 = 0.04389101266860962 + 0.1 * 6.682135105133057
Epoch 500, val loss: 1.1321362257003784
Epoch 510, training loss: 0.7077866792678833 = 0.040829986333847046 + 0.1 * 6.669567108154297
Epoch 510, val loss: 1.1489070653915405
Epoch 520, training loss: 0.7039766907691956 = 0.03807389736175537 + 0.1 * 6.659028053283691
Epoch 520, val loss: 1.1650141477584839
Epoch 530, training loss: 0.6998552680015564 = 0.03558476269245148 + 0.1 * 6.642704963684082
Epoch 530, val loss: 1.1807873249053955
Epoch 540, training loss: 0.6972246766090393 = 0.03332343325018883 + 0.1 * 6.639012813568115
Epoch 540, val loss: 1.1962751150131226
Epoch 550, training loss: 0.6955075263977051 = 0.0312664732336998 + 0.1 * 6.642410755157471
Epoch 550, val loss: 1.2113122940063477
Epoch 560, training loss: 0.6921157836914062 = 0.029401445761322975 + 0.1 * 6.627143383026123
Epoch 560, val loss: 1.22577965259552
Epoch 570, training loss: 0.6897534132003784 = 0.027700865641236305 + 0.1 * 6.62052583694458
Epoch 570, val loss: 1.239973545074463
Epoch 580, training loss: 0.6887787580490112 = 0.026142746210098267 + 0.1 * 6.626359939575195
Epoch 580, val loss: 1.2538660764694214
Epoch 590, training loss: 0.687651515007019 = 0.024715306237339973 + 0.1 * 6.629362106323242
Epoch 590, val loss: 1.2673838138580322
Epoch 600, training loss: 0.6858307719230652 = 0.023407693952322006 + 0.1 * 6.624230861663818
Epoch 600, val loss: 1.2803016901016235
Epoch 610, training loss: 0.6817699074745178 = 0.022208869457244873 + 0.1 * 6.59561014175415
Epoch 610, val loss: 1.2930688858032227
Epoch 620, training loss: 0.6800479888916016 = 0.021100293844938278 + 0.1 * 6.589477062225342
Epoch 620, val loss: 1.305576205253601
Epoch 630, training loss: 0.6808894276618958 = 0.020074518397450447 + 0.1 * 6.608148574829102
Epoch 630, val loss: 1.3177517652511597
Epoch 640, training loss: 0.6776803135871887 = 0.019126512110233307 + 0.1 * 6.585537910461426
Epoch 640, val loss: 1.329641580581665
Epoch 650, training loss: 0.6763519644737244 = 0.01824590004980564 + 0.1 * 6.581060886383057
Epoch 650, val loss: 1.341176152229309
Epoch 660, training loss: 0.6768488883972168 = 0.017430385574698448 + 0.1 * 6.5941853523254395
Epoch 660, val loss: 1.3525480031967163
Epoch 670, training loss: 0.6738404631614685 = 0.016670873388648033 + 0.1 * 6.571695804595947
Epoch 670, val loss: 1.3634480237960815
Epoch 680, training loss: 0.6722527146339417 = 0.015964480116963387 + 0.1 * 6.562881946563721
Epoch 680, val loss: 1.3742483854293823
Epoch 690, training loss: 0.6715349555015564 = 0.015303386375308037 + 0.1 * 6.562315464019775
Epoch 690, val loss: 1.3847579956054688
Epoch 700, training loss: 0.6701399683952332 = 0.01468588411808014 + 0.1 * 6.554541110992432
Epoch 700, val loss: 1.3951038122177124
Epoch 710, training loss: 0.6696200966835022 = 0.014106824062764645 + 0.1 * 6.555132865905762
Epoch 710, val loss: 1.4051698446273804
Epoch 720, training loss: 0.6679161787033081 = 0.013564509339630604 + 0.1 * 6.543516159057617
Epoch 720, val loss: 1.4150358438491821
Epoch 730, training loss: 0.6671701073646545 = 0.01305566355586052 + 0.1 * 6.541143894195557
Epoch 730, val loss: 1.4246492385864258
Epoch 740, training loss: 0.6658678650856018 = 0.012577492743730545 + 0.1 * 6.532903671264648
Epoch 740, val loss: 1.434190034866333
Epoch 750, training loss: 0.6661539077758789 = 0.012125886045396328 + 0.1 * 6.540279865264893
Epoch 750, val loss: 1.4433889389038086
Epoch 760, training loss: 0.6665860414505005 = 0.011701253242790699 + 0.1 * 6.5488481521606445
Epoch 760, val loss: 1.452463150024414
Epoch 770, training loss: 0.6633655428886414 = 0.011301194317638874 + 0.1 * 6.52064323425293
Epoch 770, val loss: 1.461311936378479
Epoch 780, training loss: 0.6631909608840942 = 0.010922604240477085 + 0.1 * 6.522683143615723
Epoch 780, val loss: 1.470088243484497
Epoch 790, training loss: 0.6644718050956726 = 0.010563673451542854 + 0.1 * 6.53908109664917
Epoch 790, val loss: 1.4786487817764282
Epoch 800, training loss: 0.6617075800895691 = 0.01022407878190279 + 0.1 * 6.514834403991699
Epoch 800, val loss: 1.4870141744613647
Epoch 810, training loss: 0.661114513874054 = 0.009902384132146835 + 0.1 * 6.512121200561523
Epoch 810, val loss: 1.4952608346939087
Epoch 820, training loss: 0.6607410907745361 = 0.009596440941095352 + 0.1 * 6.511445999145508
Epoch 820, val loss: 1.5033265352249146
Epoch 830, training loss: 0.659224271774292 = 0.009306440129876137 + 0.1 * 6.499178409576416
Epoch 830, val loss: 1.5112619400024414
Epoch 840, training loss: 0.6603060960769653 = 0.009030782617628574 + 0.1 * 6.512753009796143
Epoch 840, val loss: 1.5190467834472656
Epoch 850, training loss: 0.6592958569526672 = 0.008767884224653244 + 0.1 * 6.505279541015625
Epoch 850, val loss: 1.5266642570495605
Epoch 860, training loss: 0.6590520739555359 = 0.008517616428434849 + 0.1 * 6.505344390869141
Epoch 860, val loss: 1.5341354608535767
Epoch 870, training loss: 0.6574700474739075 = 0.008279659785330296 + 0.1 * 6.491903781890869
Epoch 870, val loss: 1.541571855545044
Epoch 880, training loss: 0.6585248708724976 = 0.008051556535065174 + 0.1 * 6.504732608795166
Epoch 880, val loss: 1.5488334894180298
Epoch 890, training loss: 0.6570324897766113 = 0.00783427432179451 + 0.1 * 6.491982460021973
Epoch 890, val loss: 1.5558788776397705
Epoch 900, training loss: 0.6556024551391602 = 0.007627312559634447 + 0.1 * 6.4797515869140625
Epoch 900, val loss: 1.562966227531433
Epoch 910, training loss: 0.6572079062461853 = 0.007428328040987253 + 0.1 * 6.497795581817627
Epoch 910, val loss: 1.5698134899139404
Epoch 920, training loss: 0.6561311483383179 = 0.007239083293825388 + 0.1 * 6.488920211791992
Epoch 920, val loss: 1.5766181945800781
Epoch 930, training loss: 0.6547480225563049 = 0.007057378999888897 + 0.1 * 6.4769062995910645
Epoch 930, val loss: 1.583274006843567
Epoch 940, training loss: 0.6554689407348633 = 0.006883166264742613 + 0.1 * 6.485857963562012
Epoch 940, val loss: 1.589766025543213
Epoch 950, training loss: 0.6545400023460388 = 0.00671613123267889 + 0.1 * 6.478238582611084
Epoch 950, val loss: 1.5962488651275635
Epoch 960, training loss: 0.653944730758667 = 0.006556264124810696 + 0.1 * 6.473884582519531
Epoch 960, val loss: 1.6025910377502441
Epoch 970, training loss: 0.653687596321106 = 0.006402177270501852 + 0.1 * 6.472853660583496
Epoch 970, val loss: 1.6088355779647827
Epoch 980, training loss: 0.6538670063018799 = 0.006253750994801521 + 0.1 * 6.476132392883301
Epoch 980, val loss: 1.6148898601531982
Epoch 990, training loss: 0.6534870862960815 = 0.006112352479249239 + 0.1 * 6.473747253417969
Epoch 990, val loss: 1.620970606803894
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 2.8123650550842285 = 1.9526824951171875 + 0.1 * 8.59682559967041
Epoch 0, val loss: 1.951623558998108
Epoch 10, training loss: 2.802109718322754 = 1.9424409866333008 + 0.1 * 8.596686363220215
Epoch 10, val loss: 1.9413701295852661
Epoch 20, training loss: 2.788944959640503 = 1.9293688535690308 + 0.1 * 8.595760345458984
Epoch 20, val loss: 1.927735686302185
Epoch 30, training loss: 2.769669771194458 = 1.9108710289001465 + 0.1 * 8.587986946105957
Epoch 30, val loss: 1.9080419540405273
Epoch 40, training loss: 2.73860239982605 = 1.883872389793396 + 0.1 * 8.5472993850708
Epoch 40, val loss: 1.8793702125549316
Epoch 50, training loss: 2.684338092803955 = 1.848646640777588 + 0.1 * 8.356913566589355
Epoch 50, val loss: 1.8437598943710327
Epoch 60, training loss: 2.628032684326172 = 1.812477946281433 + 0.1 * 8.155546188354492
Epoch 60, val loss: 1.8106741905212402
Epoch 70, training loss: 2.5676968097686768 = 1.7830936908721924 + 0.1 * 7.846031665802002
Epoch 70, val loss: 1.7858482599258423
Epoch 80, training loss: 2.492429494857788 = 1.7515512704849243 + 0.1 * 7.4087815284729
Epoch 80, val loss: 1.7577875852584839
Epoch 90, training loss: 2.4340264797210693 = 1.7125122547149658 + 0.1 * 7.215141296386719
Epoch 90, val loss: 1.7220945358276367
Epoch 100, training loss: 2.3717665672302246 = 1.6608657836914062 + 0.1 * 7.109008312225342
Epoch 100, val loss: 1.6759262084960938
Epoch 110, training loss: 2.2989749908447266 = 1.5928975343704224 + 0.1 * 7.0607733726501465
Epoch 110, val loss: 1.6167267560958862
Epoch 120, training loss: 2.2156176567077637 = 1.5122469663619995 + 0.1 * 7.033706188201904
Epoch 120, val loss: 1.5475739240646362
Epoch 130, training loss: 2.1245388984680176 = 1.424233317375183 + 0.1 * 7.003056526184082
Epoch 130, val loss: 1.473480224609375
Epoch 140, training loss: 2.027932643890381 = 1.330935001373291 + 0.1 * 6.96997594833374
Epoch 140, val loss: 1.3971697092056274
Epoch 150, training loss: 1.9239819049835205 = 1.23024320602417 + 0.1 * 6.937386512756348
Epoch 150, val loss: 1.3135331869125366
Epoch 160, training loss: 1.8146917819976807 = 1.1230041980743408 + 0.1 * 6.91687536239624
Epoch 160, val loss: 1.2246782779693604
Epoch 170, training loss: 1.7065370082855225 = 1.0166751146316528 + 0.1 * 6.898619174957275
Epoch 170, val loss: 1.1383123397827148
Epoch 180, training loss: 1.6068761348724365 = 0.9181740283966064 + 0.1 * 6.887021541595459
Epoch 180, val loss: 1.0614556074142456
Epoch 190, training loss: 1.521252989768982 = 0.834651529788971 + 0.1 * 6.86601448059082
Epoch 190, val loss: 1.0017427206039429
Epoch 200, training loss: 1.4503357410430908 = 0.7651026844978333 + 0.1 * 6.852331161499023
Epoch 200, val loss: 0.9577805399894714
Epoch 210, training loss: 1.3904560804367065 = 0.7063185572624207 + 0.1 * 6.84137487411499
Epoch 210, val loss: 0.9259392023086548
Epoch 220, training loss: 1.3382809162139893 = 0.6559359431266785 + 0.1 * 6.82344913482666
Epoch 220, val loss: 0.9029914140701294
Epoch 230, training loss: 1.2922648191452026 = 0.6110544204711914 + 0.1 * 6.812103748321533
Epoch 230, val loss: 0.8854285478591919
Epoch 240, training loss: 1.2504723072052002 = 0.5700064301490784 + 0.1 * 6.804657936096191
Epoch 240, val loss: 0.8714064359664917
Epoch 250, training loss: 1.210935115814209 = 0.531702995300293 + 0.1 * 6.792321681976318
Epoch 250, val loss: 0.8595984578132629
Epoch 260, training loss: 1.172770619392395 = 0.49497824907302856 + 0.1 * 6.777923583984375
Epoch 260, val loss: 0.8489845991134644
Epoch 270, training loss: 1.1376471519470215 = 0.4591518044471741 + 0.1 * 6.784954071044922
Epoch 270, val loss: 0.8391890525817871
Epoch 280, training loss: 1.1002968549728394 = 0.42400750517845154 + 0.1 * 6.7628936767578125
Epoch 280, val loss: 0.8303418159484863
Epoch 290, training loss: 1.064087152481079 = 0.3886988162994385 + 0.1 * 6.753882884979248
Epoch 290, val loss: 0.8221637606620789
Epoch 300, training loss: 1.03022301197052 = 0.3533202111721039 + 0.1 * 6.769028186798096
Epoch 300, val loss: 0.8151792287826538
Epoch 310, training loss: 0.9931364059448242 = 0.318837434053421 + 0.1 * 6.742989540100098
Epoch 310, val loss: 0.8101218938827515
Epoch 320, training loss: 0.9593250751495361 = 0.285929411649704 + 0.1 * 6.733956336975098
Epoch 320, val loss: 0.8075826168060303
Epoch 330, training loss: 0.9321885108947754 = 0.2553890347480774 + 0.1 * 6.7679948806762695
Epoch 330, val loss: 0.8079148530960083
Epoch 340, training loss: 0.9016613960266113 = 0.22807902097702026 + 0.1 * 6.735823631286621
Epoch 340, val loss: 0.8111048340797424
Epoch 350, training loss: 0.8754706382751465 = 0.20376284420490265 + 0.1 * 6.71707820892334
Epoch 350, val loss: 0.8168820142745972
Epoch 360, training loss: 0.8533132672309875 = 0.18212050199508667 + 0.1 * 6.71192741394043
Epoch 360, val loss: 0.8249521255493164
Epoch 370, training loss: 0.837070882320404 = 0.16290022432804108 + 0.1 * 6.741706371307373
Epoch 370, val loss: 0.8346982002258301
Epoch 380, training loss: 0.8166708946228027 = 0.14604288339614868 + 0.1 * 6.70628023147583
Epoch 380, val loss: 0.8457085490226746
Epoch 390, training loss: 0.801929235458374 = 0.13114890456199646 + 0.1 * 6.707802772521973
Epoch 390, val loss: 0.8574420213699341
Epoch 400, training loss: 0.7876514196395874 = 0.1179877519607544 + 0.1 * 6.69663667678833
Epoch 400, val loss: 0.8697823286056519
Epoch 410, training loss: 0.7743208408355713 = 0.10631458461284637 + 0.1 * 6.680062294006348
Epoch 410, val loss: 0.8825050592422485
Epoch 420, training loss: 0.7679152488708496 = 0.09594589471817017 + 0.1 * 6.719693183898926
Epoch 420, val loss: 0.8953945636749268
Epoch 430, training loss: 0.7552774548530579 = 0.08683504909276962 + 0.1 * 6.684423923492432
Epoch 430, val loss: 0.9083654880523682
Epoch 440, training loss: 0.7449289560317993 = 0.07876220345497131 + 0.1 * 6.661667346954346
Epoch 440, val loss: 0.9212504625320435
Epoch 450, training loss: 0.7391546964645386 = 0.07161511480808258 + 0.1 * 6.675395965576172
Epoch 450, val loss: 0.9340502023696899
Epoch 460, training loss: 0.7302546501159668 = 0.06529755145311356 + 0.1 * 6.649570941925049
Epoch 460, val loss: 0.9467114806175232
Epoch 470, training loss: 0.7235389947891235 = 0.05968433618545532 + 0.1 * 6.638546466827393
Epoch 470, val loss: 0.9592478275299072
Epoch 480, training loss: 0.720604419708252 = 0.05468754097819328 + 0.1 * 6.659168243408203
Epoch 480, val loss: 0.9716067910194397
Epoch 490, training loss: 0.7142822742462158 = 0.05026800185441971 + 0.1 * 6.640142917633057
Epoch 490, val loss: 0.9835532903671265
Epoch 500, training loss: 0.7086467742919922 = 0.04635373502969742 + 0.1 * 6.62293004989624
Epoch 500, val loss: 0.9952874183654785
Epoch 510, training loss: 0.7057498693466187 = 0.0428600013256073 + 0.1 * 6.6288981437683105
Epoch 510, val loss: 1.0067352056503296
Epoch 520, training loss: 0.7003659009933472 = 0.03974303603172302 + 0.1 * 6.606228828430176
Epoch 520, val loss: 1.0178613662719727
Epoch 530, training loss: 0.6990930438041687 = 0.036947283893823624 + 0.1 * 6.621457099914551
Epoch 530, val loss: 1.0286962985992432
Epoch 540, training loss: 0.6941047310829163 = 0.034440118819475174 + 0.1 * 6.596646308898926
Epoch 540, val loss: 1.0392897129058838
Epoch 550, training loss: 0.6918256282806396 = 0.032176923006772995 + 0.1 * 6.596487045288086
Epoch 550, val loss: 1.049566388130188
Epoch 560, training loss: 0.6903445720672607 = 0.0301311407238245 + 0.1 * 6.6021342277526855
Epoch 560, val loss: 1.0596219301223755
Epoch 570, training loss: 0.6868360042572021 = 0.028280671685934067 + 0.1 * 6.585553169250488
Epoch 570, val loss: 1.0693230628967285
Epoch 580, training loss: 0.6855392456054688 = 0.02660081908106804 + 0.1 * 6.589384078979492
Epoch 580, val loss: 1.0788748264312744
Epoch 590, training loss: 0.6832015514373779 = 0.025068828836083412 + 0.1 * 6.581326961517334
Epoch 590, val loss: 1.088135838508606
Epoch 600, training loss: 0.6807212233543396 = 0.023670673370361328 + 0.1 * 6.570505619049072
Epoch 600, val loss: 1.0971258878707886
Epoch 610, training loss: 0.6818374395370483 = 0.022392453625798225 + 0.1 * 6.594449996948242
Epoch 610, val loss: 1.1059201955795288
Epoch 620, training loss: 0.6780627965927124 = 0.021221637725830078 + 0.1 * 6.568411350250244
Epoch 620, val loss: 1.1143940687179565
Epoch 630, training loss: 0.6762629747390747 = 0.02014606073498726 + 0.1 * 6.561169147491455
Epoch 630, val loss: 1.1227117776870728
Epoch 640, training loss: 0.6757458448410034 = 0.01915232464671135 + 0.1 * 6.565934658050537
Epoch 640, val loss: 1.130799412727356
Epoch 650, training loss: 0.6746646761894226 = 0.018234238028526306 + 0.1 * 6.564304351806641
Epoch 650, val loss: 1.138708233833313
Epoch 660, training loss: 0.6735677123069763 = 0.017385262995958328 + 0.1 * 6.561824798583984
Epoch 660, val loss: 1.1462478637695312
Epoch 670, training loss: 0.6711391806602478 = 0.016600893810391426 + 0.1 * 6.545382499694824
Epoch 670, val loss: 1.1537824869155884
Epoch 680, training loss: 0.6708518862724304 = 0.015870818868279457 + 0.1 * 6.549810409545898
Epoch 680, val loss: 1.1610945463180542
Epoch 690, training loss: 0.6685972809791565 = 0.015191076323390007 + 0.1 * 6.534061908721924
Epoch 690, val loss: 1.168150782585144
Epoch 700, training loss: 0.6712727546691895 = 0.014557166025042534 + 0.1 * 6.567155361175537
Epoch 700, val loss: 1.1750850677490234
Epoch 710, training loss: 0.6693175435066223 = 0.01396749448031187 + 0.1 * 6.553500175476074
Epoch 710, val loss: 1.1817642450332642
Epoch 720, training loss: 0.6668903231620789 = 0.013416843488812447 + 0.1 * 6.534734725952148
Epoch 720, val loss: 1.1884222030639648
Epoch 730, training loss: 0.6662165522575378 = 0.012899147346615791 + 0.1 * 6.53317403793335
Epoch 730, val loss: 1.1948221921920776
Epoch 740, training loss: 0.6647156476974487 = 0.012413415126502514 + 0.1 * 6.523021697998047
Epoch 740, val loss: 1.2010691165924072
Epoch 750, training loss: 0.6635194420814514 = 0.011957735754549503 + 0.1 * 6.515616416931152
Epoch 750, val loss: 1.2072755098342896
Epoch 760, training loss: 0.6662938594818115 = 0.011528206057846546 + 0.1 * 6.547656536102295
Epoch 760, val loss: 1.213307499885559
Epoch 770, training loss: 0.6629038453102112 = 0.011123470962047577 + 0.1 * 6.51780366897583
Epoch 770, val loss: 1.2191743850708008
Epoch 780, training loss: 0.6618938446044922 = 0.010742882266640663 + 0.1 * 6.511509418487549
Epoch 780, val loss: 1.224966287612915
Epoch 790, training loss: 0.6646758913993835 = 0.010382863692939281 + 0.1 * 6.542929649353027
Epoch 790, val loss: 1.2305670976638794
Epoch 800, training loss: 0.6603623032569885 = 0.010043242014944553 + 0.1 * 6.503190517425537
Epoch 800, val loss: 1.2360835075378418
Epoch 810, training loss: 0.6620089411735535 = 0.009721972048282623 + 0.1 * 6.522869110107422
Epoch 810, val loss: 1.2415746450424194
Epoch 820, training loss: 0.6597337126731873 = 0.009417377412319183 + 0.1 * 6.5031633377075195
Epoch 820, val loss: 1.2468276023864746
Epoch 830, training loss: 0.6583251953125 = 0.009128628298640251 + 0.1 * 6.4919657707214355
Epoch 830, val loss: 1.2520819902420044
Epoch 840, training loss: 0.6619067192077637 = 0.008854266256093979 + 0.1 * 6.530524253845215
Epoch 840, val loss: 1.2572271823883057
Epoch 850, training loss: 0.6581976413726807 = 0.008593447506427765 + 0.1 * 6.496041774749756
Epoch 850, val loss: 1.2621920108795166
Epoch 860, training loss: 0.6569656133651733 = 0.008345977403223515 + 0.1 * 6.486196517944336
Epoch 860, val loss: 1.267157793045044
Epoch 870, training loss: 0.6585595607757568 = 0.008109821006655693 + 0.1 * 6.504497528076172
Epoch 870, val loss: 1.2719924449920654
Epoch 880, training loss: 0.6564193964004517 = 0.00788476038724184 + 0.1 * 6.48534631729126
Epoch 880, val loss: 1.276750087738037
Epoch 890, training loss: 0.6564540266990662 = 0.007670160848647356 + 0.1 * 6.487838268280029
Epoch 890, val loss: 1.2813996076583862
Epoch 900, training loss: 0.6563231945037842 = 0.007465594448149204 + 0.1 * 6.4885759353637695
Epoch 900, val loss: 1.2860349416732788
Epoch 910, training loss: 0.6562361717224121 = 0.007269549183547497 + 0.1 * 6.489665985107422
Epoch 910, val loss: 1.2905170917510986
Epoch 920, training loss: 0.6540898084640503 = 0.007082473952323198 + 0.1 * 6.470073223114014
Epoch 920, val loss: 1.294973611831665
Epoch 930, training loss: 0.6557079553604126 = 0.006903462111949921 + 0.1 * 6.488044738769531
Epoch 930, val loss: 1.2993851900100708
Epoch 940, training loss: 0.6540628671646118 = 0.006731731351464987 + 0.1 * 6.473310947418213
Epoch 940, val loss: 1.3035680055618286
Epoch 950, training loss: 0.65458744764328 = 0.006568138487637043 + 0.1 * 6.480193138122559
Epoch 950, val loss: 1.307888388633728
Epoch 960, training loss: 0.6542556285858154 = 0.006410469766706228 + 0.1 * 6.478451251983643
Epoch 960, val loss: 1.3119795322418213
Epoch 970, training loss: 0.6528270840644836 = 0.00625962158665061 + 0.1 * 6.46567440032959
Epoch 970, val loss: 1.3160978555679321
Epoch 980, training loss: 0.6528376340866089 = 0.006114404648542404 + 0.1 * 6.467231750488281
Epoch 980, val loss: 1.3201220035552979
Epoch 990, training loss: 0.6523991227149963 = 0.005975076928734779 + 0.1 * 6.464240550994873
Epoch 990, val loss: 1.3240934610366821
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8023194517659463
=== training gcn model ===
Epoch 0, training loss: 2.792356491088867 = 1.93267023563385 + 0.1 * 8.59686279296875
Epoch 0, val loss: 1.9370112419128418
Epoch 10, training loss: 2.7835350036621094 = 1.9238518476486206 + 0.1 * 8.596831321716309
Epoch 10, val loss: 1.928548812866211
Epoch 20, training loss: 2.7729434967041016 = 1.9132777452468872 + 0.1 * 8.596657752990723
Epoch 20, val loss: 1.9181437492370605
Epoch 30, training loss: 2.758124589920044 = 1.898585319519043 + 0.1 * 8.595393180847168
Epoch 30, val loss: 1.9034678936004639
Epoch 40, training loss: 2.735374927520752 = 1.8771220445632935 + 0.1 * 8.582529067993164
Epoch 40, val loss: 1.8821297883987427
Epoch 50, training loss: 2.6965956687927246 = 1.8474220037460327 + 0.1 * 8.491735458374023
Epoch 50, val loss: 1.8536725044250488
Epoch 60, training loss: 2.631254196166992 = 1.8144794702529907 + 0.1 * 8.16774845123291
Epoch 60, val loss: 1.8241472244262695
Epoch 70, training loss: 2.576143264770508 = 1.7829915285110474 + 0.1 * 7.931516647338867
Epoch 70, val loss: 1.7950319051742554
Epoch 80, training loss: 2.500138759613037 = 1.7478070259094238 + 0.1 * 7.523317813873291
Epoch 80, val loss: 1.7604705095291138
Epoch 90, training loss: 2.4257421493530273 = 1.7046235799789429 + 0.1 * 7.21118688583374
Epoch 90, val loss: 1.7209768295288086
Epoch 100, training loss: 2.3543107509613037 = 1.6465957164764404 + 0.1 * 7.077150344848633
Epoch 100, val loss: 1.6693344116210938
Epoch 110, training loss: 2.271554946899414 = 1.5714894533157349 + 0.1 * 7.0006561279296875
Epoch 110, val loss: 1.6009162664413452
Epoch 120, training loss: 2.1795313358306885 = 1.485001564025879 + 0.1 * 6.945297718048096
Epoch 120, val loss: 1.5256496667861938
Epoch 130, training loss: 2.0847651958465576 = 1.3939166069030762 + 0.1 * 6.908484935760498
Epoch 130, val loss: 1.4491304159164429
Epoch 140, training loss: 1.9909534454345703 = 1.3023494482040405 + 0.1 * 6.886040687561035
Epoch 140, val loss: 1.3751686811447144
Epoch 150, training loss: 1.9009253978729248 = 1.2142653465270996 + 0.1 * 6.866600036621094
Epoch 150, val loss: 1.3064073324203491
Epoch 160, training loss: 1.815016508102417 = 1.1294236183166504 + 0.1 * 6.855928897857666
Epoch 160, val loss: 1.2423449754714966
Epoch 170, training loss: 1.7340917587280273 = 1.049181342124939 + 0.1 * 6.8491034507751465
Epoch 170, val loss: 1.1836384534835815
Epoch 180, training loss: 1.654444694519043 = 0.9708413481712341 + 0.1 * 6.836033821105957
Epoch 180, val loss: 1.1263972520828247
Epoch 190, training loss: 1.576146125793457 = 0.8933098912239075 + 0.1 * 6.828362941741943
Epoch 190, val loss: 1.0695966482162476
Epoch 200, training loss: 1.4996296167373657 = 0.8174477815628052 + 0.1 * 6.8218183517456055
Epoch 200, val loss: 1.0136656761169434
Epoch 210, training loss: 1.4268487691879272 = 0.7452971339225769 + 0.1 * 6.815515995025635
Epoch 210, val loss: 0.9612216353416443
Epoch 220, training loss: 1.3602056503295898 = 0.6792050004005432 + 0.1 * 6.8100056648254395
Epoch 220, val loss: 0.9155620336532593
Epoch 230, training loss: 1.2992773056030273 = 0.6190875768661499 + 0.1 * 6.801897048950195
Epoch 230, val loss: 0.8775889873504639
Epoch 240, training loss: 1.2444075345993042 = 0.564723789691925 + 0.1 * 6.796837329864502
Epoch 240, val loss: 0.8479198217391968
Epoch 250, training loss: 1.1944656372070312 = 0.515801191329956 + 0.1 * 6.7866435050964355
Epoch 250, val loss: 0.826168417930603
Epoch 260, training loss: 1.148898720741272 = 0.4712810516357422 + 0.1 * 6.776176452636719
Epoch 260, val loss: 0.810906171798706
Epoch 270, training loss: 1.108710765838623 = 0.4307312071323395 + 0.1 * 6.7797956466674805
Epoch 270, val loss: 0.8012879490852356
Epoch 280, training loss: 1.069652795791626 = 0.3939048945903778 + 0.1 * 6.757479190826416
Epoch 280, val loss: 0.7963083982467651
Epoch 290, training loss: 1.0356624126434326 = 0.36002930998802185 + 0.1 * 6.756331443786621
Epoch 290, val loss: 0.7952033281326294
Epoch 300, training loss: 1.0035173892974854 = 0.32893872261047363 + 0.1 * 6.745785713195801
Epoch 300, val loss: 0.7974579334259033
Epoch 310, training loss: 0.974157452583313 = 0.3003484904766083 + 0.1 * 6.738089084625244
Epoch 310, val loss: 0.8027833104133606
Epoch 320, training loss: 0.947189211845398 = 0.2740701735019684 + 0.1 * 6.7311906814575195
Epoch 320, val loss: 0.8106741309165955
Epoch 330, training loss: 0.9229145646095276 = 0.2500677704811096 + 0.1 * 6.72846794128418
Epoch 330, val loss: 0.8207317590713501
Epoch 340, training loss: 0.8998959064483643 = 0.22815735638141632 + 0.1 * 6.717385292053223
Epoch 340, val loss: 0.8324552774429321
Epoch 350, training loss: 0.8798075914382935 = 0.20810332894325256 + 0.1 * 6.717042446136475
Epoch 350, val loss: 0.8456345200538635
Epoch 360, training loss: 0.8600963354110718 = 0.18983355164527893 + 0.1 * 6.702627182006836
Epoch 360, val loss: 0.860031008720398
Epoch 370, training loss: 0.8426472544670105 = 0.17313294112682343 + 0.1 * 6.69514274597168
Epoch 370, val loss: 0.8754861354827881
Epoch 380, training loss: 0.827714204788208 = 0.15794794261455536 + 0.1 * 6.697662353515625
Epoch 380, val loss: 0.8916349411010742
Epoch 390, training loss: 0.8129940032958984 = 0.14421804249286652 + 0.1 * 6.6877593994140625
Epoch 390, val loss: 0.9082098007202148
Epoch 400, training loss: 0.7992230653762817 = 0.1317225694656372 + 0.1 * 6.675004959106445
Epoch 400, val loss: 0.9252745509147644
Epoch 410, training loss: 0.7886102199554443 = 0.12035258114337921 + 0.1 * 6.682576656341553
Epoch 410, val loss: 0.9425757527351379
Epoch 420, training loss: 0.7767786979675293 = 0.11006999760866165 + 0.1 * 6.667087078094482
Epoch 420, val loss: 0.9599565267562866
Epoch 430, training loss: 0.7669708728790283 = 0.10073339194059372 + 0.1 * 6.662374496459961
Epoch 430, val loss: 0.9773985743522644
Epoch 440, training loss: 0.7572488784790039 = 0.09227898716926575 + 0.1 * 6.649698257446289
Epoch 440, val loss: 0.9946535229682922
Epoch 450, training loss: 0.7489635944366455 = 0.08461534231901169 + 0.1 * 6.643482208251953
Epoch 450, val loss: 1.011853814125061
Epoch 460, training loss: 0.7410799860954285 = 0.07767842710018158 + 0.1 * 6.6340155601501465
Epoch 460, val loss: 1.0287401676177979
Epoch 470, training loss: 0.7346024513244629 = 0.07141995429992676 + 0.1 * 6.631824970245361
Epoch 470, val loss: 1.0454258918762207
Epoch 480, training loss: 0.7285006046295166 = 0.06574800610542297 + 0.1 * 6.627525806427002
Epoch 480, val loss: 1.0618963241577148
Epoch 490, training loss: 0.7228980660438538 = 0.060618508607149124 + 0.1 * 6.622795581817627
Epoch 490, val loss: 1.0780020952224731
Epoch 500, training loss: 0.7180589437484741 = 0.05598670244216919 + 0.1 * 6.62072229385376
Epoch 500, val loss: 1.0936774015426636
Epoch 510, training loss: 0.7133245468139648 = 0.051800671964883804 + 0.1 * 6.615238666534424
Epoch 510, val loss: 1.1089487075805664
Epoch 520, training loss: 0.7085626721382141 = 0.04801382124423981 + 0.1 * 6.6054887771606445
Epoch 520, val loss: 1.1238054037094116
Epoch 530, training loss: 0.7048620581626892 = 0.044578876346349716 + 0.1 * 6.602831840515137
Epoch 530, val loss: 1.1383992433547974
Epoch 540, training loss: 0.7032835483551025 = 0.041459906846284866 + 0.1 * 6.618236064910889
Epoch 540, val loss: 1.1525835990905762
Epoch 550, training loss: 0.6981520652770996 = 0.03863944485783577 + 0.1 * 6.595126152038574
Epoch 550, val loss: 1.1663440465927124
Epoch 560, training loss: 0.695112943649292 = 0.03607646003365517 + 0.1 * 6.590364456176758
Epoch 560, val loss: 1.1796844005584717
Epoch 570, training loss: 0.6936847567558289 = 0.033740971237421036 + 0.1 * 6.599437713623047
Epoch 570, val loss: 1.1926568746566772
Epoch 580, training loss: 0.6904788613319397 = 0.03162181004881859 + 0.1 * 6.5885701179504395
Epoch 580, val loss: 1.205232858657837
Epoch 590, training loss: 0.6876868009567261 = 0.02969268336892128 + 0.1 * 6.5799407958984375
Epoch 590, val loss: 1.2173681259155273
Epoch 600, training loss: 0.6852124333381653 = 0.02792748063802719 + 0.1 * 6.572849750518799
Epoch 600, val loss: 1.2292661666870117
Epoch 610, training loss: 0.6849268674850464 = 0.02630901150405407 + 0.1 * 6.586178302764893
Epoch 610, val loss: 1.2408400774002075
Epoch 620, training loss: 0.6825999617576599 = 0.02482892945408821 + 0.1 * 6.577710151672363
Epoch 620, val loss: 1.252034306526184
Epoch 630, training loss: 0.6804465055465698 = 0.02347150817513466 + 0.1 * 6.56974983215332
Epoch 630, val loss: 1.262892484664917
Epoch 640, training loss: 0.6787692904472351 = 0.022223731502890587 + 0.1 * 6.565455436706543
Epoch 640, val loss: 1.2733954191207886
Epoch 650, training loss: 0.6770759224891663 = 0.021075062453746796 + 0.1 * 6.560008525848389
Epoch 650, val loss: 1.2836412191390991
Epoch 660, training loss: 0.6764352321624756 = 0.020012855529785156 + 0.1 * 6.564223766326904
Epoch 660, val loss: 1.2935781478881836
Epoch 670, training loss: 0.6739867925643921 = 0.01902925781905651 + 0.1 * 6.549575328826904
Epoch 670, val loss: 1.303313136100769
Epoch 680, training loss: 0.6747735142707825 = 0.01811610907316208 + 0.1 * 6.5665740966796875
Epoch 680, val loss: 1.3126294612884521
Epoch 690, training loss: 0.672286331653595 = 0.017272375524044037 + 0.1 * 6.550139427185059
Epoch 690, val loss: 1.3217666149139404
Epoch 700, training loss: 0.6709524393081665 = 0.01648738794028759 + 0.1 * 6.544650554656982
Epoch 700, val loss: 1.3306046724319458
Epoch 710, training loss: 0.6701640486717224 = 0.0157563928514719 + 0.1 * 6.544076442718506
Epoch 710, val loss: 1.3392605781555176
Epoch 720, training loss: 0.6695146560668945 = 0.015074571594595909 + 0.1 * 6.544401168823242
Epoch 720, val loss: 1.3477367162704468
Epoch 730, training loss: 0.6688048243522644 = 0.01443884614855051 + 0.1 * 6.543659687042236
Epoch 730, val loss: 1.3559644222259521
Epoch 740, training loss: 0.6673138737678528 = 0.013845935463905334 + 0.1 * 6.534679412841797
Epoch 740, val loss: 1.3639330863952637
Epoch 750, training loss: 0.6658963561058044 = 0.013292099349200726 + 0.1 * 6.526042461395264
Epoch 750, val loss: 1.3716541528701782
Epoch 760, training loss: 0.6674755811691284 = 0.012772396206855774 + 0.1 * 6.547031879425049
Epoch 760, val loss: 1.3791917562484741
Epoch 770, training loss: 0.6641678214073181 = 0.012284977361559868 + 0.1 * 6.51882791519165
Epoch 770, val loss: 1.3866000175476074
Epoch 780, training loss: 0.6654426455497742 = 0.011826685629785061 + 0.1 * 6.536159515380859
Epoch 780, val loss: 1.3938883543014526
Epoch 790, training loss: 0.6623134613037109 = 0.011396034620702267 + 0.1 * 6.50917387008667
Epoch 790, val loss: 1.4007790088653564
Epoch 800, training loss: 0.66475909948349 = 0.010990843176841736 + 0.1 * 6.537682056427002
Epoch 800, val loss: 1.4076555967330933
Epoch 810, training loss: 0.6620672941207886 = 0.010609340853989124 + 0.1 * 6.514579772949219
Epoch 810, val loss: 1.4142786264419556
Epoch 820, training loss: 0.6612704992294312 = 0.010248486883938313 + 0.1 * 6.510220050811768
Epoch 820, val loss: 1.4209123849868774
Epoch 830, training loss: 0.6594306826591492 = 0.009907829575240612 + 0.1 * 6.495228290557861
Epoch 830, val loss: 1.4273016452789307
Epoch 840, training loss: 0.6602370738983154 = 0.00958435982465744 + 0.1 * 6.506526947021484
Epoch 840, val loss: 1.4336155652999878
Epoch 850, training loss: 0.6583923101425171 = 0.009277846664190292 + 0.1 * 6.49114465713501
Epoch 850, val loss: 1.4396919012069702
Epoch 860, training loss: 0.6601765155792236 = 0.008987842127680779 + 0.1 * 6.5118865966796875
Epoch 860, val loss: 1.445788860321045
Epoch 870, training loss: 0.6574487090110779 = 0.008712421171367168 + 0.1 * 6.487362861633301
Epoch 870, val loss: 1.4514914751052856
Epoch 880, training loss: 0.6576090455055237 = 0.008451210334897041 + 0.1 * 6.491578102111816
Epoch 880, val loss: 1.4573321342468262
Epoch 890, training loss: 0.6563998460769653 = 0.008202916011214256 + 0.1 * 6.481969356536865
Epoch 890, val loss: 1.4628469944000244
Epoch 900, training loss: 0.6562668681144714 = 0.00796635914593935 + 0.1 * 6.483005046844482
Epoch 900, val loss: 1.468457579612732
Epoch 910, training loss: 0.6563934087753296 = 0.007740868721157312 + 0.1 * 6.486525535583496
Epoch 910, val loss: 1.4738240242004395
Epoch 920, training loss: 0.6559189558029175 = 0.007525595836341381 + 0.1 * 6.483933448791504
Epoch 920, val loss: 1.4790153503417969
Epoch 930, training loss: 0.6553812026977539 = 0.00732063502073288 + 0.1 * 6.480605602264404
Epoch 930, val loss: 1.4841238260269165
Epoch 940, training loss: 0.654793381690979 = 0.007125153671950102 + 0.1 * 6.476682186126709
Epoch 940, val loss: 1.489330768585205
Epoch 950, training loss: 0.6544616222381592 = 0.006937616970390081 + 0.1 * 6.4752397537231445
Epoch 950, val loss: 1.4942669868469238
Epoch 960, training loss: 0.6562175750732422 = 0.006758770905435085 + 0.1 * 6.4945878982543945
Epoch 960, val loss: 1.4990696907043457
Epoch 970, training loss: 0.6550964713096619 = 0.006587538402527571 + 0.1 * 6.48508882522583
Epoch 970, val loss: 1.5038396120071411
Epoch 980, training loss: 0.6528317928314209 = 0.00642376858741045 + 0.1 * 6.464079856872559
Epoch 980, val loss: 1.5085129737854004
Epoch 990, training loss: 0.6525227427482605 = 0.006266841199249029 + 0.1 * 6.462558746337891
Epoch 990, val loss: 1.5132620334625244
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8007380073800738
The final CL Acc:0.76543, 0.01222, The final GNN Acc:0.80250, 0.00151
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13202])
remove edge: torch.Size([2, 7954])
updated graph: torch.Size([2, 10600])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.7930569648742676 = 1.933377981185913 + 0.1 * 8.596790313720703
Epoch 0, val loss: 1.9382163286209106
Epoch 10, training loss: 2.7828099727630615 = 1.9231449365615845 + 0.1 * 8.596651077270508
Epoch 10, val loss: 1.9274476766586304
Epoch 20, training loss: 2.7699742317199707 = 1.9104013442993164 + 0.1 * 8.595727920532227
Epoch 20, val loss: 1.9137946367263794
Epoch 30, training loss: 2.7514145374298096 = 1.8926371335983276 + 0.1 * 8.587773323059082
Epoch 30, val loss: 1.8947348594665527
Epoch 40, training loss: 2.720606565475464 = 1.8669557571411133 + 0.1 * 8.536507606506348
Epoch 40, val loss: 1.8676570653915405
Epoch 50, training loss: 2.660151481628418 = 1.8342796564102173 + 0.1 * 8.258719444274902
Epoch 50, val loss: 1.835131287574768
Epoch 60, training loss: 2.5983381271362305 = 1.800915241241455 + 0.1 * 7.97422981262207
Epoch 60, val loss: 1.804775595664978
Epoch 70, training loss: 2.531200408935547 = 1.7690308094024658 + 0.1 * 7.6216959953308105
Epoch 70, val loss: 1.7770640850067139
Epoch 80, training loss: 2.4584031105041504 = 1.7330657243728638 + 0.1 * 7.253375053405762
Epoch 80, val loss: 1.7447937726974487
Epoch 90, training loss: 2.3923001289367676 = 1.6869810819625854 + 0.1 * 7.053191661834717
Epoch 90, val loss: 1.7031499147415161
Epoch 100, training loss: 2.323666572570801 = 1.624093770980835 + 0.1 * 6.995726585388184
Epoch 100, val loss: 1.6465296745300293
Epoch 110, training loss: 2.24141788482666 = 1.5455900430679321 + 0.1 * 6.958278656005859
Epoch 110, val loss: 1.578161358833313
Epoch 120, training loss: 2.1503548622131348 = 1.4570449590682983 + 0.1 * 6.933098793029785
Epoch 120, val loss: 1.5028539896011353
Epoch 130, training loss: 2.052565813064575 = 1.3609602451324463 + 0.1 * 6.916055679321289
Epoch 130, val loss: 1.4207121133804321
Epoch 140, training loss: 1.9480791091918945 = 1.2579619884490967 + 0.1 * 6.901170253753662
Epoch 140, val loss: 1.33469820022583
Epoch 150, training loss: 1.8378645181655884 = 1.1490150690078735 + 0.1 * 6.888494491577148
Epoch 150, val loss: 1.2464771270751953
Epoch 160, training loss: 1.726172924041748 = 1.0382394790649414 + 0.1 * 6.879334449768066
Epoch 160, val loss: 1.1596037149429321
Epoch 170, training loss: 1.6169993877410889 = 0.929826021194458 + 0.1 * 6.871734142303467
Epoch 170, val loss: 1.0765423774719238
Epoch 180, training loss: 1.5138790607452393 = 0.8277064561843872 + 0.1 * 6.861725330352783
Epoch 180, val loss: 1.0000464916229248
Epoch 190, training loss: 1.4223039150238037 = 0.7371621131896973 + 0.1 * 6.851417541503906
Epoch 190, val loss: 0.9341617822647095
Epoch 200, training loss: 1.3428922891616821 = 0.6589979529380798 + 0.1 * 6.8389434814453125
Epoch 200, val loss: 0.8795679807662964
Epoch 210, training loss: 1.274345874786377 = 0.5913867950439453 + 0.1 * 6.829590320587158
Epoch 210, val loss: 0.8356289267539978
Epoch 220, training loss: 1.2142698764801025 = 0.532553493976593 + 0.1 * 6.817163467407227
Epoch 220, val loss: 0.8015992641448975
Epoch 230, training loss: 1.1609677076339722 = 0.4806452989578247 + 0.1 * 6.803224086761475
Epoch 230, val loss: 0.7759450674057007
Epoch 240, training loss: 1.1139192581176758 = 0.43448910117149353 + 0.1 * 6.794301509857178
Epoch 240, val loss: 0.7572758197784424
Epoch 250, training loss: 1.0721845626831055 = 0.39297422766685486 + 0.1 * 6.792103290557861
Epoch 250, val loss: 0.7440785765647888
Epoch 260, training loss: 1.0337464809417725 = 0.3558022379875183 + 0.1 * 6.779441833496094
Epoch 260, val loss: 0.7354884743690491
Epoch 270, training loss: 0.9987717866897583 = 0.3220752775669098 + 0.1 * 6.766964912414551
Epoch 270, val loss: 0.7304863333702087
Epoch 280, training loss: 0.9669785499572754 = 0.29122859239578247 + 0.1 * 6.7574992179870605
Epoch 280, val loss: 0.7284820675849915
Epoch 290, training loss: 0.9381461143493652 = 0.26313334703445435 + 0.1 * 6.75012731552124
Epoch 290, val loss: 0.729208767414093
Epoch 300, training loss: 0.912039041519165 = 0.23784515261650085 + 0.1 * 6.741939067840576
Epoch 300, val loss: 0.7323890328407288
Epoch 310, training loss: 0.8883953094482422 = 0.21502284705638885 + 0.1 * 6.733724117279053
Epoch 310, val loss: 0.7377137541770935
Epoch 320, training loss: 0.8667495846748352 = 0.19453148543834686 + 0.1 * 6.7221808433532715
Epoch 320, val loss: 0.7449485659599304
Epoch 330, training loss: 0.8477897644042969 = 0.17608579993247986 + 0.1 * 6.717039108276367
Epoch 330, val loss: 0.7538130283355713
Epoch 340, training loss: 0.8306442499160767 = 0.15966376662254333 + 0.1 * 6.709805011749268
Epoch 340, val loss: 0.7639670372009277
Epoch 350, training loss: 0.8143534660339355 = 0.1450672596693039 + 0.1 * 6.692861557006836
Epoch 350, val loss: 0.7751224040985107
Epoch 360, training loss: 0.8005416393280029 = 0.13198146224021912 + 0.1 * 6.685601711273193
Epoch 360, val loss: 0.7872023582458496
Epoch 370, training loss: 0.7883276343345642 = 0.12028707563877106 + 0.1 * 6.680405616760254
Epoch 370, val loss: 0.7999048829078674
Epoch 380, training loss: 0.7765070199966431 = 0.10984846949577332 + 0.1 * 6.666585445404053
Epoch 380, val loss: 0.813042163848877
Epoch 390, training loss: 0.7667151093482971 = 0.10050905495882034 + 0.1 * 6.662060260772705
Epoch 390, val loss: 0.82650226354599
Epoch 400, training loss: 0.7566288709640503 = 0.09213472157716751 + 0.1 * 6.644941329956055
Epoch 400, val loss: 0.8401358127593994
Epoch 410, training loss: 0.7509143352508545 = 0.08459863066673279 + 0.1 * 6.663156509399414
Epoch 410, val loss: 0.8539197444915771
Epoch 420, training loss: 0.7414692044258118 = 0.07785707712173462 + 0.1 * 6.6361212730407715
Epoch 420, val loss: 0.8676337003707886
Epoch 430, training loss: 0.7352349758148193 = 0.07179668545722961 + 0.1 * 6.634383201599121
Epoch 430, val loss: 0.8812758922576904
Epoch 440, training loss: 0.7288493514060974 = 0.06633538007736206 + 0.1 * 6.6251397132873535
Epoch 440, val loss: 0.8948151469230652
Epoch 450, training loss: 0.7235810160636902 = 0.06140124797821045 + 0.1 * 6.621797561645508
Epoch 450, val loss: 0.9082415699958801
Epoch 460, training loss: 0.7185197472572327 = 0.05694498494267464 + 0.1 * 6.615747928619385
Epoch 460, val loss: 0.9215103983879089
Epoch 470, training loss: 0.7155961394309998 = 0.052912354469299316 + 0.1 * 6.626837730407715
Epoch 470, val loss: 0.9345082640647888
Epoch 480, training loss: 0.7103424072265625 = 0.049265313893556595 + 0.1 * 6.6107707023620605
Epoch 480, val loss: 0.9472398161888123
Epoch 490, training loss: 0.7076955437660217 = 0.045950911939144135 + 0.1 * 6.617446422576904
Epoch 490, val loss: 0.9597276449203491
Epoch 500, training loss: 0.7025390863418579 = 0.042941510677337646 + 0.1 * 6.595975399017334
Epoch 500, val loss: 0.9719641804695129
Epoch 510, training loss: 0.7017326354980469 = 0.040196262300014496 + 0.1 * 6.615364074707031
Epoch 510, val loss: 0.9839076399803162
Epoch 520, training loss: 0.6967529058456421 = 0.03770088404417038 + 0.1 * 6.590519905090332
Epoch 520, val loss: 0.9955673813819885
Epoch 530, training loss: 0.6952563524246216 = 0.03542068973183632 + 0.1 * 6.598356246948242
Epoch 530, val loss: 1.0069732666015625
Epoch 540, training loss: 0.6917461156845093 = 0.03333144634962082 + 0.1 * 6.584146499633789
Epoch 540, val loss: 1.0181405544281006
Epoch 550, training loss: 0.6894311904907227 = 0.03141804412007332 + 0.1 * 6.580131530761719
Epoch 550, val loss: 1.0289573669433594
Epoch 560, training loss: 0.6871711611747742 = 0.029663018882274628 + 0.1 * 6.5750813484191895
Epoch 560, val loss: 1.0395737886428833
Epoch 570, training loss: 0.6836687326431274 = 0.028048569336533546 + 0.1 * 6.556201457977295
Epoch 570, val loss: 1.0499091148376465
Epoch 580, training loss: 0.6848178505897522 = 0.026559237390756607 + 0.1 * 6.58258581161499
Epoch 580, val loss: 1.0599595308303833
Epoch 590, training loss: 0.6807909607887268 = 0.02519185282289982 + 0.1 * 6.555991172790527
Epoch 590, val loss: 1.0697835683822632
Epoch 600, training loss: 0.6817341446876526 = 0.023924732580780983 + 0.1 * 6.578094005584717
Epoch 600, val loss: 1.0792614221572876
Epoch 610, training loss: 0.678283154964447 = 0.022755607962608337 + 0.1 * 6.555274963378906
Epoch 610, val loss: 1.0885239839553833
Epoch 620, training loss: 0.6756513714790344 = 0.02166968397796154 + 0.1 * 6.539816856384277
Epoch 620, val loss: 1.0975720882415771
Epoch 630, training loss: 0.673474133014679 = 0.020658230409026146 + 0.1 * 6.528158664703369
Epoch 630, val loss: 1.106398582458496
Epoch 640, training loss: 0.6746079325675964 = 0.019715484231710434 + 0.1 * 6.548924446105957
Epoch 640, val loss: 1.115075945854187
Epoch 650, training loss: 0.6714206337928772 = 0.018839199095964432 + 0.1 * 6.525814533233643
Epoch 650, val loss: 1.123481035232544
Epoch 660, training loss: 0.6733813881874084 = 0.018022267147898674 + 0.1 * 6.553591251373291
Epoch 660, val loss: 1.131682276725769
Epoch 670, training loss: 0.6694874167442322 = 0.01726081222295761 + 0.1 * 6.522265911102295
Epoch 670, val loss: 1.1396113634109497
Epoch 680, training loss: 0.6684136390686035 = 0.016547223553061485 + 0.1 * 6.518664360046387
Epoch 680, val loss: 1.1474322080612183
Epoch 690, training loss: 0.6664233207702637 = 0.01587863638997078 + 0.1 * 6.505446910858154
Epoch 690, val loss: 1.1549497842788696
Epoch 700, training loss: 0.6656472682952881 = 0.015251529403030872 + 0.1 * 6.503957271575928
Epoch 700, val loss: 1.1624348163604736
Epoch 710, training loss: 0.6647529602050781 = 0.014661634340882301 + 0.1 * 6.500913143157959
Epoch 710, val loss: 1.1696466207504272
Epoch 720, training loss: 0.6648756861686707 = 0.014107493683695793 + 0.1 * 6.5076823234558105
Epoch 720, val loss: 1.1768125295639038
Epoch 730, training loss: 0.6634732484817505 = 0.013584247790277004 + 0.1 * 6.498889923095703
Epoch 730, val loss: 1.1837180852890015
Epoch 740, training loss: 0.6632791757583618 = 0.013092204928398132 + 0.1 * 6.501869201660156
Epoch 740, val loss: 1.1904709339141846
Epoch 750, training loss: 0.6632431745529175 = 0.01262922678142786 + 0.1 * 6.506139755249023
Epoch 750, val loss: 1.197130799293518
Epoch 760, training loss: 0.6605533957481384 = 0.012191031128168106 + 0.1 * 6.483623504638672
Epoch 760, val loss: 1.2036350965499878
Epoch 770, training loss: 0.6602831482887268 = 0.011776214465498924 + 0.1 * 6.485069274902344
Epoch 770, val loss: 1.2099130153656006
Epoch 780, training loss: 0.6597877740859985 = 0.011384604498744011 + 0.1 * 6.484031677246094
Epoch 780, val loss: 1.2162216901779175
Epoch 790, training loss: 0.6604012250900269 = 0.011012150906026363 + 0.1 * 6.493890762329102
Epoch 790, val loss: 1.2223244905471802
Epoch 800, training loss: 0.6583516597747803 = 0.010659943334758282 + 0.1 * 6.476917266845703
Epoch 800, val loss: 1.228196144104004
Epoch 810, training loss: 0.6576479077339172 = 0.010326486080884933 + 0.1 * 6.473214149475098
Epoch 810, val loss: 1.234081506729126
Epoch 820, training loss: 0.6570559740066528 = 0.010008816607296467 + 0.1 * 6.470471382141113
Epoch 820, val loss: 1.2398464679718018
Epoch 830, training loss: 0.656609833240509 = 0.009706518612802029 + 0.1 * 6.469033241271973
Epoch 830, val loss: 1.2453974485397339
Epoch 840, training loss: 0.6573899984359741 = 0.00941936019808054 + 0.1 * 6.479706287384033
Epoch 840, val loss: 1.2509037256240845
Epoch 850, training loss: 0.6549360156059265 = 0.009145759977400303 + 0.1 * 6.457902431488037
Epoch 850, val loss: 1.2563201189041138
Epoch 860, training loss: 0.6553972959518433 = 0.008884504437446594 + 0.1 * 6.465127944946289
Epoch 860, val loss: 1.2616273164749146
Epoch 870, training loss: 0.6548649668693542 = 0.00863600056618452 + 0.1 * 6.462289810180664
Epoch 870, val loss: 1.2667851448059082
Epoch 880, training loss: 0.6538164019584656 = 0.008398620411753654 + 0.1 * 6.4541778564453125
Epoch 880, val loss: 1.2718747854232788
Epoch 890, training loss: 0.6537517309188843 = 0.008172105066478252 + 0.1 * 6.455796241760254
Epoch 890, val loss: 1.2769291400909424
Epoch 900, training loss: 0.6535069942474365 = 0.007955485954880714 + 0.1 * 6.455514907836914
Epoch 900, val loss: 1.28183114528656
Epoch 910, training loss: 0.6527350544929504 = 0.007747896946966648 + 0.1 * 6.44987154006958
Epoch 910, val loss: 1.28661048412323
Epoch 920, training loss: 0.652439296245575 = 0.007549593690782785 + 0.1 * 6.448896884918213
Epoch 920, val loss: 1.2914084196090698
Epoch 930, training loss: 0.6524746417999268 = 0.0073592099361121655 + 0.1 * 6.4511542320251465
Epoch 930, val loss: 1.2961031198501587
Epoch 940, training loss: 0.6541038155555725 = 0.007176265586167574 + 0.1 * 6.469274997711182
Epoch 940, val loss: 1.300614833831787
Epoch 950, training loss: 0.6527907252311707 = 0.00700142839923501 + 0.1 * 6.457892894744873
Epoch 950, val loss: 1.3051726818084717
Epoch 960, training loss: 0.6513140201568604 = 0.006833859719336033 + 0.1 * 6.4448018074035645
Epoch 960, val loss: 1.309539556503296
Epoch 970, training loss: 0.6523100137710571 = 0.006673402152955532 + 0.1 * 6.456366062164307
Epoch 970, val loss: 1.3139379024505615
Epoch 980, training loss: 0.6495570540428162 = 0.006518426816910505 + 0.1 * 6.430386066436768
Epoch 980, val loss: 1.3181889057159424
Epoch 990, training loss: 0.6506595611572266 = 0.006369463168084621 + 0.1 * 6.442900657653809
Epoch 990, val loss: 1.3224401473999023
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8318397469688983
=== training gcn model ===
Epoch 0, training loss: 2.7998249530792236 = 1.9401453733444214 + 0.1 * 8.596796035766602
Epoch 0, val loss: 1.9477367401123047
Epoch 10, training loss: 2.7894845008850098 = 1.9298189878463745 + 0.1 * 8.59665584564209
Epoch 10, val loss: 1.9372437000274658
Epoch 20, training loss: 2.776397228240967 = 1.9168322086334229 + 0.1 * 8.595650672912598
Epoch 20, val loss: 1.9239366054534912
Epoch 30, training loss: 2.7569737434387207 = 1.898293137550354 + 0.1 * 8.586807250976562
Epoch 30, val loss: 1.9049997329711914
Epoch 40, training loss: 2.7244205474853516 = 1.8711336851119995 + 0.1 * 8.532869338989258
Epoch 40, val loss: 1.8778589963912964
Epoch 50, training loss: 2.659524917602539 = 1.836108922958374 + 0.1 * 8.234158515930176
Epoch 50, val loss: 1.8446825742721558
Epoch 60, training loss: 2.5916128158569336 = 1.7977100610733032 + 0.1 * 7.939026832580566
Epoch 60, val loss: 1.809922218322754
Epoch 70, training loss: 2.525251865386963 = 1.7606184482574463 + 0.1 * 7.646332740783691
Epoch 70, val loss: 1.7765406370162964
Epoch 80, training loss: 2.4548895359039307 = 1.721795678138733 + 0.1 * 7.330939292907715
Epoch 80, val loss: 1.7404638528823853
Epoch 90, training loss: 2.382523536682129 = 1.6713098287582397 + 0.1 * 7.112137317657471
Epoch 90, val loss: 1.693928599357605
Epoch 100, training loss: 2.306187629699707 = 1.6022905111312866 + 0.1 * 7.038969993591309
Epoch 100, val loss: 1.633548617362976
Epoch 110, training loss: 2.2171733379364014 = 1.5175411701202393 + 0.1 * 6.996322154998779
Epoch 110, val loss: 1.562322974205017
Epoch 120, training loss: 2.123821258544922 = 1.4270751476287842 + 0.1 * 6.9674601554870605
Epoch 120, val loss: 1.4867799282073975
Epoch 130, training loss: 2.0310657024383545 = 1.3368690013885498 + 0.1 * 6.941967010498047
Epoch 130, val loss: 1.4124096632003784
Epoch 140, training loss: 1.941267967224121 = 1.249395728111267 + 0.1 * 6.918721675872803
Epoch 140, val loss: 1.3406131267547607
Epoch 150, training loss: 1.8547194004058838 = 1.1648002862930298 + 0.1 * 6.899191856384277
Epoch 150, val loss: 1.272499680519104
Epoch 160, training loss: 1.7723007202148438 = 1.0837736129760742 + 0.1 * 6.885270118713379
Epoch 160, val loss: 1.2091912031173706
Epoch 170, training loss: 1.6941558122634888 = 1.0069118738174438 + 0.1 * 6.872439384460449
Epoch 170, val loss: 1.1512774229049683
Epoch 180, training loss: 1.6186909675598145 = 0.9327977299690247 + 0.1 * 6.858932018280029
Epoch 180, val loss: 1.0960252285003662
Epoch 190, training loss: 1.54532790184021 = 0.8605813384056091 + 0.1 * 6.847465515136719
Epoch 190, val loss: 1.042466402053833
Epoch 200, training loss: 1.4738564491271973 = 0.7898005843162537 + 0.1 * 6.8405585289001465
Epoch 200, val loss: 0.9900780320167542
Epoch 210, training loss: 1.4040865898132324 = 0.7209804654121399 + 0.1 * 6.831061840057373
Epoch 210, val loss: 0.9397799968719482
Epoch 220, training loss: 1.3373762369155884 = 0.655162513256073 + 0.1 * 6.822136878967285
Epoch 220, val loss: 0.8928712010383606
Epoch 230, training loss: 1.2767157554626465 = 0.5948473215103149 + 0.1 * 6.818685054779053
Epoch 230, val loss: 0.8525716066360474
Epoch 240, training loss: 1.2223985195159912 = 0.5413806438446045 + 0.1 * 6.810178279876709
Epoch 240, val loss: 0.8206586837768555
Epoch 250, training loss: 1.1741981506347656 = 0.4940697252750397 + 0.1 * 6.801284313201904
Epoch 250, val loss: 0.7967353463172913
Epoch 260, training loss: 1.1312624216079712 = 0.45182985067367554 + 0.1 * 6.794325828552246
Epoch 260, val loss: 0.7792842388153076
Epoch 270, training loss: 1.091951608657837 = 0.41321736574172974 + 0.1 * 6.787341594696045
Epoch 270, val loss: 0.7662084102630615
Epoch 280, training loss: 1.0549638271331787 = 0.3770228624343872 + 0.1 * 6.779409885406494
Epoch 280, val loss: 0.7557425498962402
Epoch 290, training loss: 1.0202041864395142 = 0.3428615629673004 + 0.1 * 6.773426532745361
Epoch 290, val loss: 0.7471327781677246
Epoch 300, training loss: 0.9871317148208618 = 0.31064459681510925 + 0.1 * 6.764870643615723
Epoch 300, val loss: 0.7398874163627625
Epoch 310, training loss: 0.9561309814453125 = 0.2802814841270447 + 0.1 * 6.758494853973389
Epoch 310, val loss: 0.7338654398918152
Epoch 320, training loss: 0.9273537397384644 = 0.25214889645576477 + 0.1 * 6.752048015594482
Epoch 320, val loss: 0.729247510433197
Epoch 330, training loss: 0.900727391242981 = 0.22629615664482117 + 0.1 * 6.744312763214111
Epoch 330, val loss: 0.7260036468505859
Epoch 340, training loss: 0.8769444227218628 = 0.20255129039287567 + 0.1 * 6.743930816650391
Epoch 340, val loss: 0.7240599393844604
Epoch 350, training loss: 0.854771614074707 = 0.1809573918581009 + 0.1 * 6.738142013549805
Epoch 350, val loss: 0.7232735753059387
Epoch 360, training loss: 0.8342577219009399 = 0.1612941324710846 + 0.1 * 6.729635715484619
Epoch 360, val loss: 0.7235555648803711
Epoch 370, training loss: 0.8159883618354797 = 0.14340710639953613 + 0.1 * 6.7258124351501465
Epoch 370, val loss: 0.7247668504714966
Epoch 380, training loss: 0.7995349168777466 = 0.12726444005966187 + 0.1 * 6.7227044105529785
Epoch 380, val loss: 0.7269481420516968
Epoch 390, training loss: 0.784906268119812 = 0.11290968954563141 + 0.1 * 6.719965934753418
Epoch 390, val loss: 0.7300279140472412
Epoch 400, training loss: 0.7716463208198547 = 0.10028734058141708 + 0.1 * 6.713590145111084
Epoch 400, val loss: 0.7339507937431335
Epoch 410, training loss: 0.7602826952934265 = 0.0892316922545433 + 0.1 * 6.71051025390625
Epoch 410, val loss: 0.7387052774429321
Epoch 420, training loss: 0.7501844167709351 = 0.07958875596523285 + 0.1 * 6.70595645904541
Epoch 420, val loss: 0.7442395091056824
Epoch 430, training loss: 0.7421040534973145 = 0.07122529298067093 + 0.1 * 6.708787441253662
Epoch 430, val loss: 0.7504891157150269
Epoch 440, training loss: 0.7341614365577698 = 0.06399232894182205 + 0.1 * 6.701691150665283
Epoch 440, val loss: 0.7571730017662048
Epoch 450, training loss: 0.726989209651947 = 0.05772851034998894 + 0.1 * 6.6926069259643555
Epoch 450, val loss: 0.7642955183982849
Epoch 460, training loss: 0.7214240431785583 = 0.052287887781858444 + 0.1 * 6.691361427307129
Epoch 460, val loss: 0.7717423439025879
Epoch 470, training loss: 0.7161264419555664 = 0.04754161834716797 + 0.1 * 6.685848236083984
Epoch 470, val loss: 0.7794205546379089
Epoch 480, training loss: 0.7119579911231995 = 0.043391887098550797 + 0.1 * 6.6856608390808105
Epoch 480, val loss: 0.7871890068054199
Epoch 490, training loss: 0.7075027823448181 = 0.03976142033934593 + 0.1 * 6.677413463592529
Epoch 490, val loss: 0.7950830459594727
Epoch 500, training loss: 0.7035669684410095 = 0.036557886749506 + 0.1 * 6.670090675354004
Epoch 500, val loss: 0.8029646277427673
Epoch 510, training loss: 0.7022449970245361 = 0.03372246026992798 + 0.1 * 6.685225009918213
Epoch 510, val loss: 0.8107885718345642
Epoch 520, training loss: 0.697870135307312 = 0.03121924214065075 + 0.1 * 6.666508674621582
Epoch 520, val loss: 0.8184930682182312
Epoch 530, training loss: 0.6945040225982666 = 0.028988778591156006 + 0.1 * 6.655152320861816
Epoch 530, val loss: 0.8262154459953308
Epoch 540, training loss: 0.6933366656303406 = 0.026990441605448723 + 0.1 * 6.663462162017822
Epoch 540, val loss: 0.8337816596031189
Epoch 550, training loss: 0.6900083422660828 = 0.025200407952070236 + 0.1 * 6.648078918457031
Epoch 550, val loss: 0.8411639928817749
Epoch 560, training loss: 0.6875635385513306 = 0.023593755438923836 + 0.1 * 6.639697551727295
Epoch 560, val loss: 0.8485428690910339
Epoch 570, training loss: 0.6867476105690002 = 0.02214055135846138 + 0.1 * 6.64607048034668
Epoch 570, val loss: 0.8557008504867554
Epoch 580, training loss: 0.6839902400970459 = 0.02082967758178711 + 0.1 * 6.631605625152588
Epoch 580, val loss: 0.8625059127807617
Epoch 590, training loss: 0.6816123127937317 = 0.019642647355794907 + 0.1 * 6.619696617126465
Epoch 590, val loss: 0.869329035282135
Epoch 600, training loss: 0.6818512082099915 = 0.01855848915874958 + 0.1 * 6.632926940917969
Epoch 600, val loss: 0.8760204911231995
Epoch 610, training loss: 0.6787968277931213 = 0.017569158226251602 + 0.1 * 6.612276554107666
Epoch 610, val loss: 0.8824514746665955
Epoch 620, training loss: 0.6774799823760986 = 0.016661880537867546 + 0.1 * 6.608180999755859
Epoch 620, val loss: 0.888816237449646
Epoch 630, training loss: 0.6756016612052917 = 0.015827560797333717 + 0.1 * 6.59774112701416
Epoch 630, val loss: 0.8950386047363281
Epoch 640, training loss: 0.6748917102813721 = 0.015059862285852432 + 0.1 * 6.598318099975586
Epoch 640, val loss: 0.9011240601539612
Epoch 650, training loss: 0.6742505431175232 = 0.014352346770465374 + 0.1 * 6.598981857299805
Epoch 650, val loss: 0.9069268703460693
Epoch 660, training loss: 0.6721060276031494 = 0.01369984820485115 + 0.1 * 6.584061622619629
Epoch 660, val loss: 0.9126893877983093
Epoch 670, training loss: 0.6714206337928772 = 0.013093463145196438 + 0.1 * 6.583271503448486
Epoch 670, val loss: 0.9184032082557678
Epoch 680, training loss: 0.670283854007721 = 0.012529523111879826 + 0.1 * 6.577542781829834
Epoch 680, val loss: 0.9238364696502686
Epoch 690, training loss: 0.6687867045402527 = 0.012006297707557678 + 0.1 * 6.567803859710693
Epoch 690, val loss: 0.9292483329772949
Epoch 700, training loss: 0.6698914170265198 = 0.011516750790178776 + 0.1 * 6.583746433258057
Epoch 700, val loss: 0.9345977306365967
Epoch 710, training loss: 0.6681928038597107 = 0.011060747317969799 + 0.1 * 6.571320056915283
Epoch 710, val loss: 0.9395403265953064
Epoch 720, training loss: 0.6664566397666931 = 0.010636790655553341 + 0.1 * 6.55819845199585
Epoch 720, val loss: 0.9445610642433167
Epoch 730, training loss: 0.6651319265365601 = 0.010238266550004482 + 0.1 * 6.548936367034912
Epoch 730, val loss: 0.9495826959609985
Epoch 740, training loss: 0.6659008860588074 = 0.009863434359431267 + 0.1 * 6.5603742599487305
Epoch 740, val loss: 0.9541451930999756
Epoch 750, training loss: 0.6635688543319702 = 0.009513551369309425 + 0.1 * 6.540552616119385
Epoch 750, val loss: 0.9588046669960022
Epoch 760, training loss: 0.6646192073822021 = 0.009183349087834358 + 0.1 * 6.55435848236084
Epoch 760, val loss: 0.9634857177734375
Epoch 770, training loss: 0.6616783142089844 = 0.008871355094015598 + 0.1 * 6.528069019317627
Epoch 770, val loss: 0.9678512215614319
Epoch 780, training loss: 0.6611933708190918 = 0.008577865548431873 + 0.1 * 6.5261549949646
Epoch 780, val loss: 0.9723186492919922
Epoch 790, training loss: 0.660825252532959 = 0.008299194276332855 + 0.1 * 6.5252604484558105
Epoch 790, val loss: 0.9765636324882507
Epoch 800, training loss: 0.6596871018409729 = 0.008037003688514233 + 0.1 * 6.516500473022461
Epoch 800, val loss: 0.9807330369949341
Epoch 810, training loss: 0.6587371230125427 = 0.007788920775055885 + 0.1 * 6.509481906890869
Epoch 810, val loss: 0.9850287437438965
Epoch 820, training loss: 0.6601943373680115 = 0.007551833987236023 + 0.1 * 6.526424884796143
Epoch 820, val loss: 0.9890819191932678
Epoch 830, training loss: 0.6586083769798279 = 0.007327072322368622 + 0.1 * 6.512812614440918
Epoch 830, val loss: 0.9929567575454712
Epoch 840, training loss: 0.6596835255622864 = 0.0071145640686154366 + 0.1 * 6.525689125061035
Epoch 840, val loss: 0.996886670589447
Epoch 850, training loss: 0.6569024920463562 = 0.006912237033247948 + 0.1 * 6.499902725219727
Epoch 850, val loss: 1.0006906986236572
Epoch 860, training loss: 0.6560166478157043 = 0.006719920784235001 + 0.1 * 6.492967128753662
Epoch 860, val loss: 1.004532814025879
Epoch 870, training loss: 0.6563014984130859 = 0.006535899359732866 + 0.1 * 6.497655868530273
Epoch 870, val loss: 1.0082931518554688
Epoch 880, training loss: 0.6566033363342285 = 0.0063601224683225155 + 0.1 * 6.502431869506836
Epoch 880, val loss: 1.0118962526321411
Epoch 890, training loss: 0.6554177403450012 = 0.006192721426486969 + 0.1 * 6.492249965667725
Epoch 890, val loss: 1.0154889822006226
Epoch 900, training loss: 0.6561794877052307 = 0.006032511591911316 + 0.1 * 6.501469612121582
Epoch 900, val loss: 1.019036889076233
Epoch 910, training loss: 0.6538236141204834 = 0.005879277363419533 + 0.1 * 6.479443073272705
Epoch 910, val loss: 1.0225095748901367
Epoch 920, training loss: 0.6547129154205322 = 0.005733054131269455 + 0.1 * 6.489798545837402
Epoch 920, val loss: 1.0259602069854736
Epoch 930, training loss: 0.6547780632972717 = 0.005592660512775183 + 0.1 * 6.491854190826416
Epoch 930, val loss: 1.0292870998382568
Epoch 940, training loss: 0.6532765626907349 = 0.005458425730466843 + 0.1 * 6.4781813621521
Epoch 940, val loss: 1.0326281785964966
Epoch 950, training loss: 0.6547112464904785 = 0.005329886917024851 + 0.1 * 6.4938130378723145
Epoch 950, val loss: 1.035937786102295
Epoch 960, training loss: 0.6519853472709656 = 0.005206095054745674 + 0.1 * 6.46779203414917
Epoch 960, val loss: 1.039078712463379
Epoch 970, training loss: 0.6526350975036621 = 0.005087717901915312 + 0.1 * 6.475473403930664
Epoch 970, val loss: 1.0422654151916504
Epoch 980, training loss: 0.6515507698059082 = 0.004973621107637882 + 0.1 * 6.465771198272705
Epoch 980, val loss: 1.0453227758407593
Epoch 990, training loss: 0.6507369875907898 = 0.004864591173827648 + 0.1 * 6.458724021911621
Epoch 990, val loss: 1.0484107732772827
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.8079960346221924 = 1.9483119249343872 + 0.1 * 8.596841812133789
Epoch 0, val loss: 1.9525123834609985
Epoch 10, training loss: 2.7980942726135254 = 1.9384171962738037 + 0.1 * 8.596771240234375
Epoch 10, val loss: 1.941909909248352
Epoch 20, training loss: 2.7858386039733887 = 1.9262033700942993 + 0.1 * 8.596351623535156
Epoch 20, val loss: 1.928750991821289
Epoch 30, training loss: 2.7681968212127686 = 1.90890371799469 + 0.1 * 8.592930793762207
Epoch 30, val loss: 1.9101706743240356
Epoch 40, training loss: 2.740200996398926 = 1.8832049369812012 + 0.1 * 8.569961547851562
Epoch 40, val loss: 1.8827567100524902
Epoch 50, training loss: 2.69640851020813 = 1.847251057624817 + 0.1 * 8.491575241088867
Epoch 50, val loss: 1.846182107925415
Epoch 60, training loss: 2.6251726150512695 = 1.8067547082901 + 0.1 * 8.184179306030273
Epoch 60, val loss: 1.809401512145996
Epoch 70, training loss: 2.572481870651245 = 1.767661213874817 + 0.1 * 8.048206329345703
Epoch 70, val loss: 1.7780436277389526
Epoch 80, training loss: 2.509342908859253 = 1.7244163751602173 + 0.1 * 7.849266052246094
Epoch 80, val loss: 1.7421751022338867
Epoch 90, training loss: 2.423309326171875 = 1.6690422296524048 + 0.1 * 7.5426716804504395
Epoch 90, val loss: 1.6944748163223267
Epoch 100, training loss: 2.3281853199005127 = 1.5984752178192139 + 0.1 * 7.297100067138672
Epoch 100, val loss: 1.6336678266525269
Epoch 110, training loss: 2.2295875549316406 = 1.510612964630127 + 0.1 * 7.189745903015137
Epoch 110, val loss: 1.5592451095581055
Epoch 120, training loss: 2.1240057945251465 = 1.4116182327270508 + 0.1 * 7.123875617980957
Epoch 120, val loss: 1.4791207313537598
Epoch 130, training loss: 2.017862319946289 = 1.3088531494140625 + 0.1 * 7.090091705322266
Epoch 130, val loss: 1.3994303941726685
Epoch 140, training loss: 1.9142998456954956 = 1.2072967290878296 + 0.1 * 7.07003116607666
Epoch 140, val loss: 1.3214459419250488
Epoch 150, training loss: 1.8140912055969238 = 1.1083459854125977 + 0.1 * 7.05745267868042
Epoch 150, val loss: 1.245549201965332
Epoch 160, training loss: 1.7166818380355835 = 1.0122871398925781 + 0.1 * 7.043946743011475
Epoch 160, val loss: 1.1713212728500366
Epoch 170, training loss: 1.6215903759002686 = 0.9190112948417664 + 0.1 * 7.025790691375732
Epoch 170, val loss: 1.0985183715820312
Epoch 180, training loss: 1.5294444561004639 = 0.8293309807777405 + 0.1 * 7.001134872436523
Epoch 180, val loss: 1.028859257698059
Epoch 190, training loss: 1.4431712627410889 = 0.7452261447906494 + 0.1 * 6.979451656341553
Epoch 190, val loss: 0.965088427066803
Epoch 200, training loss: 1.3647207021713257 = 0.669209897518158 + 0.1 * 6.955108165740967
Epoch 200, val loss: 0.9100232720375061
Epoch 210, training loss: 1.294816493988037 = 0.6007274985313416 + 0.1 * 6.940889835357666
Epoch 210, val loss: 0.8632869720458984
Epoch 220, training loss: 1.2319824695587158 = 0.5390051603317261 + 0.1 * 6.929773807525635
Epoch 220, val loss: 0.8246213793754578
Epoch 230, training loss: 1.1753334999084473 = 0.4831761419773102 + 0.1 * 6.921573162078857
Epoch 230, val loss: 0.7932649254798889
Epoch 240, training loss: 1.1243414878845215 = 0.4331802427768707 + 0.1 * 6.911612510681152
Epoch 240, val loss: 0.7687700390815735
Epoch 250, training loss: 1.0776143074035645 = 0.3877123296260834 + 0.1 * 6.899020195007324
Epoch 250, val loss: 0.7494275569915771
Epoch 260, training loss: 1.0343471765518188 = 0.34567469358444214 + 0.1 * 6.886724948883057
Epoch 260, val loss: 0.7338572144508362
Epoch 270, training loss: 0.9947451949119568 = 0.30680912733078003 + 0.1 * 6.879360675811768
Epoch 270, val loss: 0.7215523719787598
Epoch 280, training loss: 0.9576328992843628 = 0.27113476395606995 + 0.1 * 6.864981651306152
Epoch 280, val loss: 0.7121014595031738
Epoch 290, training loss: 0.924224317073822 = 0.23850589990615845 + 0.1 * 6.857183933258057
Epoch 290, val loss: 0.705398440361023
Epoch 300, training loss: 0.8937651515007019 = 0.20933009684085846 + 0.1 * 6.844350814819336
Epoch 300, val loss: 0.7013096809387207
Epoch 310, training loss: 0.8670490980148315 = 0.18353807926177979 + 0.1 * 6.835110187530518
Epoch 310, val loss: 0.6998054385185242
Epoch 320, training loss: 0.8436123132705688 = 0.16096340119838715 + 0.1 * 6.826488971710205
Epoch 320, val loss: 0.7008079290390015
Epoch 330, training loss: 0.8235397934913635 = 0.14148737490177155 + 0.1 * 6.820524215698242
Epoch 330, val loss: 0.704023540019989
Epoch 340, training loss: 0.8065314292907715 = 0.12482934445142746 + 0.1 * 6.817020416259766
Epoch 340, val loss: 0.7090609669685364
Epoch 350, training loss: 0.792549192905426 = 0.11049807071685791 + 0.1 * 6.820511341094971
Epoch 350, val loss: 0.7157960534095764
Epoch 360, training loss: 0.7794113159179688 = 0.09829643368721008 + 0.1 * 6.811148166656494
Epoch 360, val loss: 0.7237004637718201
Epoch 370, training loss: 0.7674849033355713 = 0.0878048986196518 + 0.1 * 6.796799659729004
Epoch 370, val loss: 0.7325143218040466
Epoch 380, training loss: 0.757912278175354 = 0.07869748026132584 + 0.1 * 6.792147636413574
Epoch 380, val loss: 0.7422220706939697
Epoch 390, training loss: 0.7494162917137146 = 0.07075798511505127 + 0.1 * 6.786582946777344
Epoch 390, val loss: 0.7526395320892334
Epoch 400, training loss: 0.742938756942749 = 0.06385811418294907 + 0.1 * 6.790806293487549
Epoch 400, val loss: 0.7634246945381165
Epoch 410, training loss: 0.7359278798103333 = 0.05786953121423721 + 0.1 * 6.780583381652832
Epoch 410, val loss: 0.7743556499481201
Epoch 420, training loss: 0.7296477556228638 = 0.05260750651359558 + 0.1 * 6.770402431488037
Epoch 420, val loss: 0.78544020652771
Epoch 430, training loss: 0.7263304591178894 = 0.047985635697841644 + 0.1 * 6.783448219299316
Epoch 430, val loss: 0.7965743541717529
Epoch 440, training loss: 0.7202757000923157 = 0.04392335191369057 + 0.1 * 6.763523101806641
Epoch 440, val loss: 0.8075655698776245
Epoch 450, training loss: 0.7155709266662598 = 0.040322866290807724 + 0.1 * 6.752480983734131
Epoch 450, val loss: 0.818548858165741
Epoch 460, training loss: 0.7127965688705444 = 0.037117861211299896 + 0.1 * 6.756787300109863
Epoch 460, val loss: 0.8294106125831604
Epoch 470, training loss: 0.7093817591667175 = 0.03429339453577995 + 0.1 * 6.75088357925415
Epoch 470, val loss: 0.8400651216506958
Epoch 480, training loss: 0.7055203914642334 = 0.031789276748895645 + 0.1 * 6.737310886383057
Epoch 480, val loss: 0.850287914276123
Epoch 490, training loss: 0.7026016712188721 = 0.029551303014159203 + 0.1 * 6.730503082275391
Epoch 490, val loss: 0.86038738489151
Epoch 500, training loss: 0.7000145316123962 = 0.02754642628133297 + 0.1 * 6.724681377410889
Epoch 500, val loss: 0.8701044917106628
Epoch 510, training loss: 0.6971646547317505 = 0.025735797360539436 + 0.1 * 6.714288234710693
Epoch 510, val loss: 0.8796857595443726
Epoch 520, training loss: 0.6947442293167114 = 0.02409701980650425 + 0.1 * 6.706471920013428
Epoch 520, val loss: 0.8890781402587891
Epoch 530, training loss: 0.6947630047798157 = 0.022620344534516335 + 0.1 * 6.721426486968994
Epoch 530, val loss: 0.8983327150344849
Epoch 540, training loss: 0.6921077966690063 = 0.021298125386238098 + 0.1 * 6.708096504211426
Epoch 540, val loss: 0.9069535732269287
Epoch 550, training loss: 0.6892321705818176 = 0.02008754201233387 + 0.1 * 6.691446304321289
Epoch 550, val loss: 0.9154130816459656
Epoch 560, training loss: 0.6877752542495728 = 0.018981393426656723 + 0.1 * 6.687938213348389
Epoch 560, val loss: 0.9238415360450745
Epoch 570, training loss: 0.6855009198188782 = 0.017970265820622444 + 0.1 * 6.67530632019043
Epoch 570, val loss: 0.9318758845329285
Epoch 580, training loss: 0.6841351985931396 = 0.01703556254506111 + 0.1 * 6.670996189117432
Epoch 580, val loss: 0.9398064017295837
Epoch 590, training loss: 0.6827457547187805 = 0.01617552526295185 + 0.1 * 6.6657023429870605
Epoch 590, val loss: 0.9476615190505981
Epoch 600, training loss: 0.6815774440765381 = 0.015384103171527386 + 0.1 * 6.661933422088623
Epoch 600, val loss: 0.9552447199821472
Epoch 610, training loss: 0.6807581782341003 = 0.014651224948465824 + 0.1 * 6.661069393157959
Epoch 610, val loss: 0.9626428484916687
Epoch 620, training loss: 0.6802581548690796 = 0.013974552974104881 + 0.1 * 6.662836074829102
Epoch 620, val loss: 0.9700542092323303
Epoch 630, training loss: 0.678629457950592 = 0.013351152651011944 + 0.1 * 6.652782917022705
Epoch 630, val loss: 0.9770119190216064
Epoch 640, training loss: 0.6773293614387512 = 0.01276824064552784 + 0.1 * 6.645610809326172
Epoch 640, val loss: 0.9838642477989197
Epoch 650, training loss: 0.6791533827781677 = 0.012224101461470127 + 0.1 * 6.66929292678833
Epoch 650, val loss: 0.9907451272010803
Epoch 660, training loss: 0.6755104064941406 = 0.011720236390829086 + 0.1 * 6.637901306152344
Epoch 660, val loss: 0.997390627861023
Epoch 670, training loss: 0.6749182343482971 = 0.011248710565268993 + 0.1 * 6.636695384979248
Epoch 670, val loss: 1.0038117170333862
Epoch 680, training loss: 0.6748968362808228 = 0.010805411264300346 + 0.1 * 6.640913963317871
Epoch 680, val loss: 1.0102287530899048
Epoch 690, training loss: 0.6738929748535156 = 0.010393158532679081 + 0.1 * 6.634998321533203
Epoch 690, val loss: 1.0165939331054688
Epoch 700, training loss: 0.6729276776313782 = 0.010007280856370926 + 0.1 * 6.629203796386719
Epoch 700, val loss: 1.02265465259552
Epoch 710, training loss: 0.6712137460708618 = 0.009643950499594212 + 0.1 * 6.615697383880615
Epoch 710, val loss: 1.0286290645599365
Epoch 720, training loss: 0.6739148497581482 = 0.009302635677158833 + 0.1 * 6.646121978759766
Epoch 720, val loss: 1.034564733505249
Epoch 730, training loss: 0.6707653999328613 = 0.0089828846976161 + 0.1 * 6.617825031280518
Epoch 730, val loss: 1.0403136014938354
Epoch 740, training loss: 0.669492781162262 = 0.00868206936866045 + 0.1 * 6.608107089996338
Epoch 740, val loss: 1.0458698272705078
Epoch 750, training loss: 0.6688486933708191 = 0.00839561503380537 + 0.1 * 6.6045308113098145
Epoch 750, val loss: 1.051493763923645
Epoch 760, training loss: 0.6685913801193237 = 0.008126371540129185 + 0.1 * 6.604650020599365
Epoch 760, val loss: 1.0569524765014648
Epoch 770, training loss: 0.6719218492507935 = 0.007871023379266262 + 0.1 * 6.64050817489624
Epoch 770, val loss: 1.0623184442520142
Epoch 780, training loss: 0.6660913825035095 = 0.007631085813045502 + 0.1 * 6.5846028327941895
Epoch 780, val loss: 1.0676008462905884
Epoch 790, training loss: 0.6668649911880493 = 0.007403703406453133 + 0.1 * 6.5946125984191895
Epoch 790, val loss: 1.0726438760757446
Epoch 800, training loss: 0.6663124561309814 = 0.007186895236372948 + 0.1 * 6.5912556648254395
Epoch 800, val loss: 1.0776042938232422
Epoch 810, training loss: 0.6639993786811829 = 0.006981533020734787 + 0.1 * 6.570178031921387
Epoch 810, val loss: 1.082443118095398
Epoch 820, training loss: 0.6643374562263489 = 0.006785401608794928 + 0.1 * 6.5755205154418945
Epoch 820, val loss: 1.0872488021850586
Epoch 830, training loss: 0.6638556718826294 = 0.006599860731512308 + 0.1 * 6.572558403015137
Epoch 830, val loss: 1.0919134616851807
Epoch 840, training loss: 0.6637634634971619 = 0.006422199308872223 + 0.1 * 6.5734124183654785
Epoch 840, val loss: 1.0964829921722412
Epoch 850, training loss: 0.6627984046936035 = 0.006253689527511597 + 0.1 * 6.565446853637695
Epoch 850, val loss: 1.1009336709976196
Epoch 860, training loss: 0.6644524931907654 = 0.0060920133255422115 + 0.1 * 6.58360481262207
Epoch 860, val loss: 1.1053564548492432
Epoch 870, training loss: 0.6615025401115417 = 0.005938265472650528 + 0.1 * 6.555642127990723
Epoch 870, val loss: 1.1096638441085815
Epoch 880, training loss: 0.664091944694519 = 0.005790516268461943 + 0.1 * 6.583014011383057
Epoch 880, val loss: 1.1138575077056885
Epoch 890, training loss: 0.6593543887138367 = 0.005650028120726347 + 0.1 * 6.53704309463501
Epoch 890, val loss: 1.1180793046951294
Epoch 900, training loss: 0.6587489247322083 = 0.00551516842097044 + 0.1 * 6.532337665557861
Epoch 900, val loss: 1.1221561431884766
Epoch 910, training loss: 0.6605521440505981 = 0.005385565105825663 + 0.1 * 6.551665306091309
Epoch 910, val loss: 1.1261910200119019
Epoch 920, training loss: 0.6587774753570557 = 0.005261765327304602 + 0.1 * 6.535156726837158
Epoch 920, val loss: 1.1301345825195312
Epoch 930, training loss: 0.6580950617790222 = 0.005142570938915014 + 0.1 * 6.529524803161621
Epoch 930, val loss: 1.1340631246566772
Epoch 940, training loss: 0.6574923396110535 = 0.005028955638408661 + 0.1 * 6.524633407592773
Epoch 940, val loss: 1.1378450393676758
Epoch 950, training loss: 0.6582939624786377 = 0.004918685182929039 + 0.1 * 6.53375244140625
Epoch 950, val loss: 1.14155113697052
Epoch 960, training loss: 0.6561487913131714 = 0.00481307553127408 + 0.1 * 6.513357162475586
Epoch 960, val loss: 1.1452691555023193
Epoch 970, training loss: 0.6568692326545715 = 0.004711208399385214 + 0.1 * 6.521580219268799
Epoch 970, val loss: 1.1488940715789795
Epoch 980, training loss: 0.6547889113426208 = 0.004613132681697607 + 0.1 * 6.501757621765137
Epoch 980, val loss: 1.1524373292922974
Epoch 990, training loss: 0.6559462547302246 = 0.004518191795796156 + 0.1 * 6.514280319213867
Epoch 990, val loss: 1.1560031175613403
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8355297838692674
The final CL Acc:0.80864, 0.00462, The final GNN Acc:0.83412, 0.00163
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9590])
updated graph: torch.Size([2, 10658])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8094005584716797 = 1.9497178792953491 + 0.1 * 8.596826553344727
Epoch 0, val loss: 1.9362802505493164
Epoch 10, training loss: 2.7985236644744873 = 1.9388504028320312 + 0.1 * 8.596733093261719
Epoch 10, val loss: 1.9263800382614136
Epoch 20, training loss: 2.7848920822143555 = 1.9252853393554688 + 0.1 * 8.596067428588867
Epoch 20, val loss: 1.9137777090072632
Epoch 30, training loss: 2.7646853923797607 = 1.9057071208953857 + 0.1 * 8.58978271484375
Epoch 30, val loss: 1.895461916923523
Epoch 40, training loss: 2.731595039367676 = 1.876556158065796 + 0.1 * 8.55038833618164
Epoch 40, val loss: 1.8686034679412842
Epoch 50, training loss: 2.6739463806152344 = 1.8386996984481812 + 0.1 * 8.35246753692627
Epoch 50, val loss: 1.8357096910476685
Epoch 60, training loss: 2.61894154548645 = 1.8020755052566528 + 0.1 * 8.168660163879395
Epoch 60, val loss: 1.807124376296997
Epoch 70, training loss: 2.574112892150879 = 1.7716645002365112 + 0.1 * 8.024482727050781
Epoch 70, val loss: 1.7819452285766602
Epoch 80, training loss: 2.510329246520996 = 1.7346985340118408 + 0.1 * 7.756307601928711
Epoch 80, val loss: 1.7450554370880127
Epoch 90, training loss: 2.4345686435699463 = 1.6891263723373413 + 0.1 * 7.454422950744629
Epoch 90, val loss: 1.7017247676849365
Epoch 100, training loss: 2.35272479057312 = 1.6320372819900513 + 0.1 * 7.206875324249268
Epoch 100, val loss: 1.6524845361709595
Epoch 110, training loss: 2.2692885398864746 = 1.5649325847625732 + 0.1 * 7.043558597564697
Epoch 110, val loss: 1.5947868824005127
Epoch 120, training loss: 2.193010091781616 = 1.497780203819275 + 0.1 * 6.95229959487915
Epoch 120, val loss: 1.540196418762207
Epoch 130, training loss: 2.1268417835235596 = 1.4353801012039185 + 0.1 * 6.914616107940674
Epoch 130, val loss: 1.4902338981628418
Epoch 140, training loss: 2.0654280185699463 = 1.3768253326416016 + 0.1 * 6.886026859283447
Epoch 140, val loss: 1.4462246894836426
Epoch 150, training loss: 2.0056374073028564 = 1.3192898035049438 + 0.1 * 6.863475799560547
Epoch 150, val loss: 1.402839183807373
Epoch 160, training loss: 1.9455158710479736 = 1.261202096939087 + 0.1 * 6.843136787414551
Epoch 160, val loss: 1.358525276184082
Epoch 170, training loss: 1.8841787576675415 = 1.2008980512619019 + 0.1 * 6.8328070640563965
Epoch 170, val loss: 1.3134067058563232
Epoch 180, training loss: 1.8202340602874756 = 1.1387845277786255 + 0.1 * 6.814496040344238
Epoch 180, val loss: 1.2676074504852295
Epoch 190, training loss: 1.7535250186920166 = 1.0731416940689087 + 0.1 * 6.803832530975342
Epoch 190, val loss: 1.2194392681121826
Epoch 200, training loss: 1.6838493347167969 = 1.00336754322052 + 0.1 * 6.804817199707031
Epoch 200, val loss: 1.1687639951705933
Epoch 210, training loss: 1.6101292371749878 = 0.9315740466117859 + 0.1 * 6.785552024841309
Epoch 210, val loss: 1.116803526878357
Epoch 220, training loss: 1.5380481481552124 = 0.8600300550460815 + 0.1 * 6.780180931091309
Epoch 220, val loss: 1.0662537813186646
Epoch 230, training loss: 1.4678219556808472 = 0.791204571723938 + 0.1 * 6.766173839569092
Epoch 230, val loss: 1.0194274187088013
Epoch 240, training loss: 1.4040319919586182 = 0.7276467084884644 + 0.1 * 6.763853549957275
Epoch 240, val loss: 0.9790289998054504
Epoch 250, training loss: 1.3455588817596436 = 0.6707830429077148 + 0.1 * 6.747758388519287
Epoch 250, val loss: 0.9466155171394348
Epoch 260, training loss: 1.2961316108703613 = 0.6208834648132324 + 0.1 * 6.752481460571289
Epoch 260, val loss: 0.9228541851043701
Epoch 270, training loss: 1.2509028911590576 = 0.5777080059051514 + 0.1 * 6.731947898864746
Epoch 270, val loss: 0.9072736501693726
Epoch 280, training loss: 1.2117855548858643 = 0.5397430062294006 + 0.1 * 6.720426082611084
Epoch 280, val loss: 0.8984647989273071
Epoch 290, training loss: 1.1780049800872803 = 0.5058549046516418 + 0.1 * 6.721500873565674
Epoch 290, val loss: 0.8946313858032227
Epoch 300, training loss: 1.1455836296081543 = 0.4750082492828369 + 0.1 * 6.705754280090332
Epoch 300, val loss: 0.8944206237792969
Epoch 310, training loss: 1.1164382696151733 = 0.44581788778305054 + 0.1 * 6.706203937530518
Epoch 310, val loss: 0.8964251279830933
Epoch 320, training loss: 1.0867968797683716 = 0.4172962009906769 + 0.1 * 6.695006370544434
Epoch 320, val loss: 0.8998331427574158
Epoch 330, training loss: 1.0588555335998535 = 0.38874197006225586 + 0.1 * 6.701135635375977
Epoch 330, val loss: 0.9038829207420349
Epoch 340, training loss: 1.027849555015564 = 0.3597947955131531 + 0.1 * 6.68054723739624
Epoch 340, val loss: 0.9086017608642578
Epoch 350, training loss: 0.9979870319366455 = 0.3302134573459625 + 0.1 * 6.677735805511475
Epoch 350, val loss: 0.913967490196228
Epoch 360, training loss: 0.9692001342773438 = 0.30035319924354553 + 0.1 * 6.688469409942627
Epoch 360, val loss: 0.9201524257659912
Epoch 370, training loss: 0.9379583597183228 = 0.2711631953716278 + 0.1 * 6.6679511070251465
Epoch 370, val loss: 0.9278525114059448
Epoch 380, training loss: 0.9098548293113708 = 0.24335439503192902 + 0.1 * 6.665004253387451
Epoch 380, val loss: 0.9370949268341064
Epoch 390, training loss: 0.884724497795105 = 0.21761314570903778 + 0.1 * 6.671113014221191
Epoch 390, val loss: 0.9482845664024353
Epoch 400, training loss: 0.8597263097763062 = 0.1942736804485321 + 0.1 * 6.654526233673096
Epoch 400, val loss: 0.9612504839897156
Epoch 410, training loss: 0.8382393717765808 = 0.17332769930362701 + 0.1 * 6.6491169929504395
Epoch 410, val loss: 0.9760748744010925
Epoch 420, training loss: 0.8216766119003296 = 0.1547183096408844 + 0.1 * 6.669583320617676
Epoch 420, val loss: 0.9921246767044067
Epoch 430, training loss: 0.8025356531143188 = 0.13838589191436768 + 0.1 * 6.641497611999512
Epoch 430, val loss: 1.0095051527023315
Epoch 440, training loss: 0.7875800132751465 = 0.1239965409040451 + 0.1 * 6.635834217071533
Epoch 440, val loss: 1.0277694463729858
Epoch 450, training loss: 0.775730550289154 = 0.11130503565073013 + 0.1 * 6.644254684448242
Epoch 450, val loss: 1.0465363264083862
Epoch 460, training loss: 0.7634691596031189 = 0.10013929754495621 + 0.1 * 6.633298397064209
Epoch 460, val loss: 1.065711498260498
Epoch 470, training loss: 0.7535725235939026 = 0.09030668437480927 + 0.1 * 6.632658004760742
Epoch 470, val loss: 1.0849478244781494
Epoch 480, training loss: 0.7441971898078918 = 0.08165992796421051 + 0.1 * 6.625372886657715
Epoch 480, val loss: 1.104148507118225
Epoch 490, training loss: 0.7358092069625854 = 0.07403412461280823 + 0.1 * 6.61775016784668
Epoch 490, val loss: 1.1231178045272827
Epoch 500, training loss: 0.7283427715301514 = 0.06731405854225159 + 0.1 * 6.610286712646484
Epoch 500, val loss: 1.141793131828308
Epoch 510, training loss: 0.7222632765769958 = 0.061390358954668045 + 0.1 * 6.608729362487793
Epoch 510, val loss: 1.1600662469863892
Epoch 520, training loss: 0.7176763415336609 = 0.05614263191819191 + 0.1 * 6.615336894989014
Epoch 520, val loss: 1.1780519485473633
Epoch 530, training loss: 0.7120904326438904 = 0.051496271044015884 + 0.1 * 6.6059417724609375
Epoch 530, val loss: 1.1955432891845703
Epoch 540, training loss: 0.7067201733589172 = 0.04736294969916344 + 0.1 * 6.593572616577148
Epoch 540, val loss: 1.2125768661499023
Epoch 550, training loss: 0.7049099206924438 = 0.043675314635038376 + 0.1 * 6.612346172332764
Epoch 550, val loss: 1.229035496711731
Epoch 560, training loss: 0.7001861929893494 = 0.04039214178919792 + 0.1 * 6.597940444946289
Epoch 560, val loss: 1.245133399963379
Epoch 570, training loss: 0.6956890225410461 = 0.03745228052139282 + 0.1 * 6.582367420196533
Epoch 570, val loss: 1.2606706619262695
Epoch 580, training loss: 0.6964423060417175 = 0.03480851650238037 + 0.1 * 6.616337776184082
Epoch 580, val loss: 1.2756439447402954
Epoch 590, training loss: 0.6897120475769043 = 0.03243482857942581 + 0.1 * 6.572772026062012
Epoch 590, val loss: 1.2903788089752197
Epoch 600, training loss: 0.6877402663230896 = 0.030290473252534866 + 0.1 * 6.574497699737549
Epoch 600, val loss: 1.304534673690796
Epoch 610, training loss: 0.6851176619529724 = 0.028349732980132103 + 0.1 * 6.567678928375244
Epoch 610, val loss: 1.3181076049804688
Epoch 620, training loss: 0.6826310157775879 = 0.02659361995756626 + 0.1 * 6.560373783111572
Epoch 620, val loss: 1.3316256999969482
Epoch 630, training loss: 0.6841824054718018 = 0.02499440312385559 + 0.1 * 6.5918803215026855
Epoch 630, val loss: 1.3444515466690063
Epoch 640, training loss: 0.6799976825714111 = 0.02354327216744423 + 0.1 * 6.564543724060059
Epoch 640, val loss: 1.3569587469100952
Epoch 650, training loss: 0.6781600117683411 = 0.022220410406589508 + 0.1 * 6.559395790100098
Epoch 650, val loss: 1.3691402673721313
Epoch 660, training loss: 0.677640438079834 = 0.021007182076573372 + 0.1 * 6.5663323402404785
Epoch 660, val loss: 1.3807703256607056
Epoch 670, training loss: 0.6752406358718872 = 0.019895240664482117 + 0.1 * 6.5534539222717285
Epoch 670, val loss: 1.3922358751296997
Epoch 680, training loss: 0.6743853688240051 = 0.018872395157814026 + 0.1 * 6.5551300048828125
Epoch 680, val loss: 1.4032776355743408
Epoch 690, training loss: 0.672244668006897 = 0.017929481342434883 + 0.1 * 6.54315185546875
Epoch 690, val loss: 1.4139171838760376
Epoch 700, training loss: 0.6708477735519409 = 0.017061909660696983 + 0.1 * 6.537858486175537
Epoch 700, val loss: 1.4246171712875366
Epoch 710, training loss: 0.6693356037139893 = 0.016257554292678833 + 0.1 * 6.530779838562012
Epoch 710, val loss: 1.4346232414245605
Epoch 720, training loss: 0.67104572057724 = 0.015511916019022465 + 0.1 * 6.555337905883789
Epoch 720, val loss: 1.4445266723632812
Epoch 730, training loss: 0.6679356098175049 = 0.014820791780948639 + 0.1 * 6.5311479568481445
Epoch 730, val loss: 1.4541003704071045
Epoch 740, training loss: 0.6674917936325073 = 0.014178413897752762 + 0.1 * 6.5331339836120605
Epoch 740, val loss: 1.4632480144500732
Epoch 750, training loss: 0.6657420992851257 = 0.01358107104897499 + 0.1 * 6.521609783172607
Epoch 750, val loss: 1.4725041389465332
Epoch 760, training loss: 0.6651145219802856 = 0.013022763654589653 + 0.1 * 6.5209174156188965
Epoch 760, val loss: 1.4813868999481201
Epoch 770, training loss: 0.6659887433052063 = 0.012500417418777943 + 0.1 * 6.534883499145508
Epoch 770, val loss: 1.4896153211593628
Epoch 780, training loss: 0.6643194556236267 = 0.012013419531285763 + 0.1 * 6.523060321807861
Epoch 780, val loss: 1.4982528686523438
Epoch 790, training loss: 0.6627753376960754 = 0.011556271463632584 + 0.1 * 6.512190818786621
Epoch 790, val loss: 1.5063440799713135
Epoch 800, training loss: 0.6628037095069885 = 0.011126621626317501 + 0.1 * 6.516770362854004
Epoch 800, val loss: 1.513979196548462
Epoch 810, training loss: 0.661322832107544 = 0.010723644867539406 + 0.1 * 6.5059919357299805
Epoch 810, val loss: 1.5220314264297485
Epoch 820, training loss: 0.6608738899230957 = 0.010343674570322037 + 0.1 * 6.5053019523620605
Epoch 820, val loss: 1.5294156074523926
Epoch 830, training loss: 0.6606795787811279 = 0.00998547114431858 + 0.1 * 6.506941318511963
Epoch 830, val loss: 1.5367162227630615
Epoch 840, training loss: 0.6598458290100098 = 0.009648383595049381 + 0.1 * 6.501974582672119
Epoch 840, val loss: 1.5441312789916992
Epoch 850, training loss: 0.6599961519241333 = 0.009328299202024937 + 0.1 * 6.506678581237793
Epoch 850, val loss: 1.5508633852005005
Epoch 860, training loss: 0.6587395668029785 = 0.009027045220136642 + 0.1 * 6.497125148773193
Epoch 860, val loss: 1.5577722787857056
Epoch 870, training loss: 0.6588186621665955 = 0.008740988560020924 + 0.1 * 6.500776767730713
Epoch 870, val loss: 1.5643084049224854
Epoch 880, training loss: 0.6572996973991394 = 0.008471455425024033 + 0.1 * 6.488282680511475
Epoch 880, val loss: 1.5709091424942017
Epoch 890, training loss: 0.6583682894706726 = 0.008214976638555527 + 0.1 * 6.501533031463623
Epoch 890, val loss: 1.5773346424102783
Epoch 900, training loss: 0.6564364433288574 = 0.007971096783876419 + 0.1 * 6.484653472900391
Epoch 900, val loss: 1.5833442211151123
Epoch 910, training loss: 0.6561163663864136 = 0.007739990949630737 + 0.1 * 6.483763217926025
Epoch 910, val loss: 1.5896962881088257
Epoch 920, training loss: 0.6560075283050537 = 0.007518945261836052 + 0.1 * 6.4848856925964355
Epoch 920, val loss: 1.595582127571106
Epoch 930, training loss: 0.6553375720977783 = 0.007309091277420521 + 0.1 * 6.480284690856934
Epoch 930, val loss: 1.6013867855072021
Epoch 940, training loss: 0.6546498537063599 = 0.007108587305992842 + 0.1 * 6.475412845611572
Epoch 940, val loss: 1.607182264328003
Epoch 950, training loss: 0.6571219563484192 = 0.006917587947100401 + 0.1 * 6.502043724060059
Epoch 950, val loss: 1.612660527229309
Epoch 960, training loss: 0.6547664403915405 = 0.0067352126352488995 + 0.1 * 6.480312347412109
Epoch 960, val loss: 1.6180223226547241
Epoch 970, training loss: 0.654813289642334 = 0.006561448331922293 + 0.1 * 6.482518196105957
Epoch 970, val loss: 1.6236497163772583
Epoch 980, training loss: 0.6529866456985474 = 0.006394743919372559 + 0.1 * 6.465919017791748
Epoch 980, val loss: 1.6286031007766724
Epoch 990, training loss: 0.6547855734825134 = 0.006235366687178612 + 0.1 * 6.485502243041992
Epoch 990, val loss: 1.6337740421295166
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 2.794726848602295 = 1.9350396394729614 + 0.1 * 8.596872329711914
Epoch 0, val loss: 1.934147596359253
Epoch 10, training loss: 2.7854950428009033 = 1.9258126020431519 + 0.1 * 8.596823692321777
Epoch 10, val loss: 1.9248977899551392
Epoch 20, training loss: 2.7740938663482666 = 1.9144411087036133 + 0.1 * 8.596528053283691
Epoch 20, val loss: 1.913193941116333
Epoch 30, training loss: 2.7580339908599854 = 1.898634433746338 + 0.1 * 8.593996047973633
Epoch 30, val loss: 1.8967499732971191
Epoch 40, training loss: 2.733393669128418 = 1.8758912086486816 + 0.1 * 8.575024604797363
Epoch 40, val loss: 1.873293161392212
Epoch 50, training loss: 2.694525718688965 = 1.8452004194259644 + 0.1 * 8.493252754211426
Epoch 50, val loss: 1.8427848815917969
Epoch 60, training loss: 2.633211851119995 = 1.811366081237793 + 0.1 * 8.21845817565918
Epoch 60, val loss: 1.8115178346633911
Epoch 70, training loss: 2.592013359069824 = 1.7789921760559082 + 0.1 * 8.130212783813477
Epoch 70, val loss: 1.7837377786636353
Epoch 80, training loss: 2.5360071659088135 = 1.7410550117492676 + 0.1 * 7.949521541595459
Epoch 80, val loss: 1.7493786811828613
Epoch 90, training loss: 2.4589905738830566 = 1.6923553943634033 + 0.1 * 7.666350364685059
Epoch 90, val loss: 1.705241322517395
Epoch 100, training loss: 2.367888927459717 = 1.629776954650879 + 0.1 * 7.3811187744140625
Epoch 100, val loss: 1.6509934663772583
Epoch 110, training loss: 2.278386116027832 = 1.551281452178955 + 0.1 * 7.271047115325928
Epoch 110, val loss: 1.5815061330795288
Epoch 120, training loss: 2.179030418395996 = 1.4606317281723022 + 0.1 * 7.183988094329834
Epoch 120, val loss: 1.5042750835418701
Epoch 130, training loss: 2.07846999168396 = 1.36452317237854 + 0.1 * 7.139468193054199
Epoch 130, val loss: 1.4273144006729126
Epoch 140, training loss: 1.9773483276367188 = 1.2663722038269043 + 0.1 * 7.109760761260986
Epoch 140, val loss: 1.3507025241851807
Epoch 150, training loss: 1.8784563541412354 = 1.1707097291946411 + 0.1 * 7.07746696472168
Epoch 150, val loss: 1.2799164056777954
Epoch 160, training loss: 1.7880877256393433 = 1.083978533744812 + 0.1 * 7.0410919189453125
Epoch 160, val loss: 1.2177977561950684
Epoch 170, training loss: 1.7085044384002686 = 1.0072777271270752 + 0.1 * 7.012266635894775
Epoch 170, val loss: 1.1656352281570435
Epoch 180, training loss: 1.6378741264343262 = 0.938653290271759 + 0.1 * 6.992208480834961
Epoch 180, val loss: 1.1200543642044067
Epoch 190, training loss: 1.5724618434906006 = 0.8747419118881226 + 0.1 * 6.977200031280518
Epoch 190, val loss: 1.0780794620513916
Epoch 200, training loss: 1.5091277360916138 = 0.8131915330886841 + 0.1 * 6.959362030029297
Epoch 200, val loss: 1.0380228757858276
Epoch 210, training loss: 1.4468928575515747 = 0.7525238990783691 + 0.1 * 6.943689346313477
Epoch 210, val loss: 0.998982846736908
Epoch 220, training loss: 1.3857133388519287 = 0.6921492218971252 + 0.1 * 6.935640335083008
Epoch 220, val loss: 0.9613925218582153
Epoch 230, training loss: 1.3245302438735962 = 0.6326123476028442 + 0.1 * 6.9191789627075195
Epoch 230, val loss: 0.926809549331665
Epoch 240, training loss: 1.265470266342163 = 0.5747859477996826 + 0.1 * 6.9068427085876465
Epoch 240, val loss: 0.8965981602668762
Epoch 250, training loss: 1.2116118669509888 = 0.5201444625854492 + 0.1 * 6.914673805236816
Epoch 250, val loss: 0.8720806837081909
Epoch 260, training loss: 1.1595959663391113 = 0.47064244747161865 + 0.1 * 6.889534950256348
Epoch 260, val loss: 0.8541113138198853
Epoch 270, training loss: 1.1148103475570679 = 0.4264572858810425 + 0.1 * 6.883530616760254
Epoch 270, val loss: 0.8420331478118896
Epoch 280, training loss: 1.0749431848526 = 0.3872823417186737 + 0.1 * 6.876608848571777
Epoch 280, val loss: 0.8351196050643921
Epoch 290, training loss: 1.0399168729782104 = 0.35266491770744324 + 0.1 * 6.872519493103027
Epoch 290, val loss: 0.8324589133262634
Epoch 300, training loss: 1.0081045627593994 = 0.32183125615119934 + 0.1 * 6.862732410430908
Epoch 300, val loss: 0.8330241441726685
Epoch 310, training loss: 0.9803574681282043 = 0.2940171957015991 + 0.1 * 6.863402366638184
Epoch 310, val loss: 0.8358772993087769
Epoch 320, training loss: 0.9540702104568481 = 0.268699049949646 + 0.1 * 6.8537116050720215
Epoch 320, val loss: 0.8402459621429443
Epoch 330, training loss: 0.9297685623168945 = 0.24531400203704834 + 0.1 * 6.844545364379883
Epoch 330, val loss: 0.8458164930343628
Epoch 340, training loss: 0.9078736305236816 = 0.22358091175556183 + 0.1 * 6.842926979064941
Epoch 340, val loss: 0.8520800471305847
Epoch 350, training loss: 0.8877320885658264 = 0.2035301923751831 + 0.1 * 6.842019081115723
Epoch 350, val loss: 0.8589426875114441
Epoch 360, training loss: 0.8678204417228699 = 0.18489623069763184 + 0.1 * 6.82924222946167
Epoch 360, val loss: 0.8661853075027466
Epoch 370, training loss: 0.8501632809638977 = 0.16763059794902802 + 0.1 * 6.825326442718506
Epoch 370, val loss: 0.8739182949066162
Epoch 380, training loss: 0.8333804607391357 = 0.15186405181884766 + 0.1 * 6.815164089202881
Epoch 380, val loss: 0.8819580674171448
Epoch 390, training loss: 0.8184999227523804 = 0.13753852248191833 + 0.1 * 6.809614181518555
Epoch 390, val loss: 0.8902546167373657
Epoch 400, training loss: 0.8044033050537109 = 0.12456027418375015 + 0.1 * 6.7984299659729
Epoch 400, val loss: 0.8990832567214966
Epoch 410, training loss: 0.791862428188324 = 0.11290527880191803 + 0.1 * 6.789571762084961
Epoch 410, val loss: 0.908301055431366
Epoch 420, training loss: 0.7808027267456055 = 0.10253707319498062 + 0.1 * 6.782656192779541
Epoch 420, val loss: 0.9179145693778992
Epoch 430, training loss: 0.7708817720413208 = 0.09328950941562653 + 0.1 * 6.775922775268555
Epoch 430, val loss: 0.9278820157051086
Epoch 440, training loss: 0.762040913105011 = 0.0850561261177063 + 0.1 * 6.769847869873047
Epoch 440, val loss: 0.9381833076477051
Epoch 450, training loss: 0.7549688220024109 = 0.07773154973983765 + 0.1 * 6.772372722625732
Epoch 450, val loss: 0.9488034844398499
Epoch 460, training loss: 0.7459615468978882 = 0.07123962789773941 + 0.1 * 6.747218608856201
Epoch 460, val loss: 0.9595839977264404
Epoch 470, training loss: 0.7394952178001404 = 0.06546385586261749 + 0.1 * 6.740313529968262
Epoch 470, val loss: 0.9704210162162781
Epoch 480, training loss: 0.7345908284187317 = 0.060304298996925354 + 0.1 * 6.742865562438965
Epoch 480, val loss: 0.9814091324806213
Epoch 490, training loss: 0.7276908159255981 = 0.055690448731184006 + 0.1 * 6.720003604888916
Epoch 490, val loss: 0.992439866065979
Epoch 500, training loss: 0.7240514159202576 = 0.05154680460691452 + 0.1 * 6.725046157836914
Epoch 500, val loss: 1.0033223628997803
Epoch 510, training loss: 0.718809962272644 = 0.0478287972509861 + 0.1 * 6.709811687469482
Epoch 510, val loss: 1.0142524242401123
Epoch 520, training loss: 0.7150614857673645 = 0.0444708913564682 + 0.1 * 6.705905914306641
Epoch 520, val loss: 1.0250588655471802
Epoch 530, training loss: 0.7104021906852722 = 0.041439104825258255 + 0.1 * 6.689630508422852
Epoch 530, val loss: 1.035685420036316
Epoch 540, training loss: 0.7077240943908691 = 0.038698453456163406 + 0.1 * 6.690256595611572
Epoch 540, val loss: 1.0461982488632202
Epoch 550, training loss: 0.705651581287384 = 0.03620794042944908 + 0.1 * 6.694436550140381
Epoch 550, val loss: 1.056513786315918
Epoch 560, training loss: 0.7022147178649902 = 0.03394131734967232 + 0.1 * 6.68273401260376
Epoch 560, val loss: 1.06673002243042
Epoch 570, training loss: 0.6987146139144897 = 0.031871434301137924 + 0.1 * 6.668431758880615
Epoch 570, val loss: 1.0767427682876587
Epoch 580, training loss: 0.6974808573722839 = 0.029977016150951385 + 0.1 * 6.6750383377075195
Epoch 580, val loss: 1.0865141153335571
Epoch 590, training loss: 0.6949328780174255 = 0.028241511434316635 + 0.1 * 6.6669135093688965
Epoch 590, val loss: 1.096152663230896
Epoch 600, training loss: 0.6917369961738586 = 0.02664864994585514 + 0.1 * 6.650883197784424
Epoch 600, val loss: 1.1055526733398438
Epoch 610, training loss: 0.6907495856285095 = 0.025185856968164444 + 0.1 * 6.655636787414551
Epoch 610, val loss: 1.11475670337677
Epoch 620, training loss: 0.6888548135757446 = 0.023838922381401062 + 0.1 * 6.650158882141113
Epoch 620, val loss: 1.1238582134246826
Epoch 630, training loss: 0.6862564086914062 = 0.02259446494281292 + 0.1 * 6.636619567871094
Epoch 630, val loss: 1.1327016353607178
Epoch 640, training loss: 0.6848766803741455 = 0.02144458144903183 + 0.1 * 6.634321212768555
Epoch 640, val loss: 1.1412962675094604
Epoch 650, training loss: 0.6835687756538391 = 0.020381474867463112 + 0.1 * 6.63187313079834
Epoch 650, val loss: 1.1497936248779297
Epoch 660, training loss: 0.6828039884567261 = 0.019396059215068817 + 0.1 * 6.6340789794921875
Epoch 660, val loss: 1.1579464673995972
Epoch 670, training loss: 0.6806133985519409 = 0.018482139334082603 + 0.1 * 6.621312141418457
Epoch 670, val loss: 1.1660929918289185
Epoch 680, training loss: 0.6795842051506042 = 0.01763078384101391 + 0.1 * 6.619534492492676
Epoch 680, val loss: 1.1740220785140991
Epoch 690, training loss: 0.6795434355735779 = 0.016835933551192284 + 0.1 * 6.627074718475342
Epoch 690, val loss: 1.1818127632141113
Epoch 700, training loss: 0.6782166361808777 = 0.01609540358185768 + 0.1 * 6.621212005615234
Epoch 700, val loss: 1.1892985105514526
Epoch 710, training loss: 0.6761703491210938 = 0.0154056865721941 + 0.1 * 6.6076459884643555
Epoch 710, val loss: 1.1967555284500122
Epoch 720, training loss: 0.6756035089492798 = 0.014760546386241913 + 0.1 * 6.608429431915283
Epoch 720, val loss: 1.2039457559585571
Epoch 730, training loss: 0.6750956177711487 = 0.014155792072415352 + 0.1 * 6.609397888183594
Epoch 730, val loss: 1.2111059427261353
Epoch 740, training loss: 0.6733324527740479 = 0.013588254339993 + 0.1 * 6.597441673278809
Epoch 740, val loss: 1.2180070877075195
Epoch 750, training loss: 0.6748060584068298 = 0.013056480325758457 + 0.1 * 6.617496013641357
Epoch 750, val loss: 1.2247555255889893
Epoch 760, training loss: 0.6726970076560974 = 0.012557478621602058 + 0.1 * 6.601395130157471
Epoch 760, val loss: 1.2314088344573975
Epoch 770, training loss: 0.6706263422966003 = 0.012088309973478317 + 0.1 * 6.585380554199219
Epoch 770, val loss: 1.2379209995269775
Epoch 780, training loss: 0.6707180738449097 = 0.01164525281637907 + 0.1 * 6.590727806091309
Epoch 780, val loss: 1.2443008422851562
Epoch 790, training loss: 0.6709729433059692 = 0.011226807720959187 + 0.1 * 6.597460746765137
Epoch 790, val loss: 1.2505210638046265
Epoch 800, training loss: 0.6689249277114868 = 0.010832478292286396 + 0.1 * 6.5809245109558105
Epoch 800, val loss: 1.2566684484481812
Epoch 810, training loss: 0.6682189106941223 = 0.010460348799824715 + 0.1 * 6.577585220336914
Epoch 810, val loss: 1.262684941291809
Epoch 820, training loss: 0.6673673391342163 = 0.010107804089784622 + 0.1 * 6.572595119476318
Epoch 820, val loss: 1.2684801816940308
Epoch 830, training loss: 0.6663709282875061 = 0.009774868376553059 + 0.1 * 6.56596040725708
Epoch 830, val loss: 1.274293065071106
Epoch 840, training loss: 0.6686981916427612 = 0.009458675980567932 + 0.1 * 6.592395305633545
Epoch 840, val loss: 1.2799328565597534
Epoch 850, training loss: 0.6671693921089172 = 0.009159442968666553 + 0.1 * 6.580099582672119
Epoch 850, val loss: 1.2853683233261108
Epoch 860, training loss: 0.6660111546516418 = 0.00887573603540659 + 0.1 * 6.571353912353516
Epoch 860, val loss: 1.29087233543396
Epoch 870, training loss: 0.6651867032051086 = 0.008605655282735825 + 0.1 * 6.565810203552246
Epoch 870, val loss: 1.2960699796676636
Epoch 880, training loss: 0.6639397144317627 = 0.00834894273430109 + 0.1 * 6.555907726287842
Epoch 880, val loss: 1.3013447523117065
Epoch 890, training loss: 0.6647032499313354 = 0.00810405146330595 + 0.1 * 6.56599235534668
Epoch 890, val loss: 1.3064651489257812
Epoch 900, training loss: 0.664054811000824 = 0.00787101686000824 + 0.1 * 6.561837673187256
Epoch 900, val loss: 1.3114242553710938
Epoch 910, training loss: 0.6635650992393494 = 0.007649105973541737 + 0.1 * 6.559159755706787
Epoch 910, val loss: 1.3164336681365967
Epoch 920, training loss: 0.6615573763847351 = 0.007437488064169884 + 0.1 * 6.541199207305908
Epoch 920, val loss: 1.3212634325027466
Epoch 930, training loss: 0.6620500087738037 = 0.00723492493852973 + 0.1 * 6.548150539398193
Epoch 930, val loss: 1.32601797580719
Epoch 940, training loss: 0.6631327271461487 = 0.007041384000331163 + 0.1 * 6.5609130859375
Epoch 940, val loss: 1.3305870294570923
Epoch 950, training loss: 0.6609774231910706 = 0.006857409607619047 + 0.1 * 6.541200160980225
Epoch 950, val loss: 1.3351960182189941
Epoch 960, training loss: 0.6617854237556458 = 0.006681608501821756 + 0.1 * 6.551037788391113
Epoch 960, val loss: 1.3397578001022339
Epoch 970, training loss: 0.6602766513824463 = 0.006512440741062164 + 0.1 * 6.537641525268555
Epoch 970, val loss: 1.3441187143325806
Epoch 980, training loss: 0.6592479944229126 = 0.006350731011480093 + 0.1 * 6.528972148895264
Epoch 980, val loss: 1.3485383987426758
Epoch 990, training loss: 0.6606108546257019 = 0.006195399444550276 + 0.1 * 6.544154167175293
Epoch 990, val loss: 1.3528099060058594
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8186610437532947
=== training gcn model ===
Epoch 0, training loss: 2.809114694595337 = 1.9494285583496094 + 0.1 * 8.596860885620117
Epoch 0, val loss: 1.947326898574829
Epoch 10, training loss: 2.7993531227111816 = 1.9396753311157227 + 0.1 * 8.59677791595459
Epoch 10, val loss: 1.9380135536193848
Epoch 20, training loss: 2.787444829940796 = 1.9278203248977661 + 0.1 * 8.596244812011719
Epoch 20, val loss: 1.9263631105422974
Epoch 30, training loss: 2.7707691192626953 = 1.911594033241272 + 0.1 * 8.591750144958496
Epoch 30, val loss: 1.9100972414016724
Epoch 40, training loss: 2.744375705718994 = 1.887943148612976 + 0.1 * 8.564325332641602
Epoch 40, val loss: 1.8863928318023682
Epoch 50, training loss: 2.6995596885681152 = 1.8550724983215332 + 0.1 * 8.44487190246582
Epoch 50, val loss: 1.8546065092086792
Epoch 60, training loss: 2.6381475925445557 = 1.8170526027679443 + 0.1 * 8.210948944091797
Epoch 60, val loss: 1.8202773332595825
Epoch 70, training loss: 2.5910613536834717 = 1.782383680343628 + 0.1 * 8.086775779724121
Epoch 70, val loss: 1.7904263734817505
Epoch 80, training loss: 2.529317617416382 = 1.745835542678833 + 0.1 * 7.834820747375488
Epoch 80, val loss: 1.7560739517211914
Epoch 90, training loss: 2.4577577114105225 = 1.7001831531524658 + 0.1 * 7.57574462890625
Epoch 90, val loss: 1.7144067287445068
Epoch 100, training loss: 2.3788669109344482 = 1.6426502466201782 + 0.1 * 7.362166881561279
Epoch 100, val loss: 1.6661354303359985
Epoch 110, training loss: 2.292018413543701 = 1.5707695484161377 + 0.1 * 7.212489604949951
Epoch 110, val loss: 1.6050702333450317
Epoch 120, training loss: 2.1990792751312256 = 1.4884775876998901 + 0.1 * 7.106017589569092
Epoch 120, val loss: 1.5357189178466797
Epoch 130, training loss: 2.1092309951782227 = 1.4049593210220337 + 0.1 * 7.042716026306152
Epoch 130, val loss: 1.4682425260543823
Epoch 140, training loss: 2.023339033126831 = 1.3221626281738281 + 0.1 * 7.011764049530029
Epoch 140, val loss: 1.4031524658203125
Epoch 150, training loss: 1.9386497735977173 = 1.2392991781234741 + 0.1 * 6.993505954742432
Epoch 150, val loss: 1.3386479616165161
Epoch 160, training loss: 1.8566169738769531 = 1.1579991579055786 + 0.1 * 6.986178874969482
Epoch 160, val loss: 1.2762341499328613
Epoch 170, training loss: 1.77986478805542 = 1.0823395252227783 + 0.1 * 6.975253105163574
Epoch 170, val loss: 1.2199243307113647
Epoch 180, training loss: 1.710062026977539 = 1.0130746364593506 + 0.1 * 6.969873428344727
Epoch 180, val loss: 1.1693964004516602
Epoch 190, training loss: 1.646136999130249 = 0.9496822357177734 + 0.1 * 6.964548110961914
Epoch 190, val loss: 1.1238707304000854
Epoch 200, training loss: 1.5860811471939087 = 0.8897872567176819 + 0.1 * 6.9629387855529785
Epoch 200, val loss: 1.081236720085144
Epoch 210, training loss: 1.5267271995544434 = 0.8309670686721802 + 0.1 * 6.957601070404053
Epoch 210, val loss: 1.039914608001709
Epoch 220, training loss: 1.4661613702774048 = 0.7708233594894409 + 0.1 * 6.953380107879639
Epoch 220, val loss: 0.9981133341789246
Epoch 230, training loss: 1.4034826755523682 = 0.7085974216461182 + 0.1 * 6.948852062225342
Epoch 230, val loss: 0.9557285904884338
Epoch 240, training loss: 1.3397387266159058 = 0.6453636884689331 + 0.1 * 6.943750381469727
Epoch 240, val loss: 0.9144394397735596
Epoch 250, training loss: 1.2763924598693848 = 0.5826337337493896 + 0.1 * 6.937586307525635
Epoch 250, val loss: 0.8763582706451416
Epoch 260, training loss: 1.2147797346115112 = 0.5219285488128662 + 0.1 * 6.928511619567871
Epoch 260, val loss: 0.8433355093002319
Epoch 270, training loss: 1.156638503074646 = 0.46445760130882263 + 0.1 * 6.92180871963501
Epoch 270, val loss: 0.8164774179458618
Epoch 280, training loss: 1.102640151977539 = 0.41137146949768066 + 0.1 * 6.912686824798584
Epoch 280, val loss: 0.7956745028495789
Epoch 290, training loss: 1.053391695022583 = 0.36304956674575806 + 0.1 * 6.903420925140381
Epoch 290, val loss: 0.7803282737731934
Epoch 300, training loss: 1.0089647769927979 = 0.32011592388153076 + 0.1 * 6.888489246368408
Epoch 300, val loss: 0.7698217034339905
Epoch 310, training loss: 0.9701113700866699 = 0.28207162022590637 + 0.1 * 6.880396842956543
Epoch 310, val loss: 0.7636305689811707
Epoch 320, training loss: 0.9352318644523621 = 0.2483297735452652 + 0.1 * 6.869020938873291
Epoch 320, val loss: 0.7608427405357361
Epoch 330, training loss: 0.9055167436599731 = 0.21852219104766846 + 0.1 * 6.869945526123047
Epoch 330, val loss: 0.7611996531486511
Epoch 340, training loss: 0.8779372572898865 = 0.19254733622074127 + 0.1 * 6.853899002075195
Epoch 340, val loss: 0.7642157077789307
Epoch 350, training loss: 0.8547879457473755 = 0.1698741763830185 + 0.1 * 6.849137306213379
Epoch 350, val loss: 0.7694306373596191
Epoch 360, training loss: 0.8355010747909546 = 0.150077685713768 + 0.1 * 6.854233741760254
Epoch 360, val loss: 0.7765063047409058
Epoch 370, training loss: 0.817113995552063 = 0.13293376564979553 + 0.1 * 6.841802597045898
Epoch 370, val loss: 0.7850409746170044
Epoch 380, training loss: 0.80122309923172 = 0.1180383712053299 + 0.1 * 6.831847190856934
Epoch 380, val loss: 0.7949584722518921
Epoch 390, training loss: 0.7876687049865723 = 0.10513873398303986 + 0.1 * 6.8252997398376465
Epoch 390, val loss: 0.805662989616394
Epoch 400, training loss: 0.7760429382324219 = 0.09403174370527267 + 0.1 * 6.8201117515563965
Epoch 400, val loss: 0.8171132206916809
Epoch 410, training loss: 0.7655470371246338 = 0.0843900516629219 + 0.1 * 6.811569690704346
Epoch 410, val loss: 0.8290637135505676
Epoch 420, training loss: 0.7562968134880066 = 0.07598292827606201 + 0.1 * 6.803138732910156
Epoch 420, val loss: 0.8413997292518616
Epoch 430, training loss: 0.7490118741989136 = 0.06863819062709808 + 0.1 * 6.803736686706543
Epoch 430, val loss: 0.8538570404052734
Epoch 440, training loss: 0.7423056364059448 = 0.06225339323282242 + 0.1 * 6.800522327423096
Epoch 440, val loss: 0.86639004945755
Epoch 450, training loss: 0.7356756329536438 = 0.05666844919323921 + 0.1 * 6.790071487426758
Epoch 450, val loss: 0.8789394497871399
Epoch 460, training loss: 0.7312701344490051 = 0.05174887925386429 + 0.1 * 6.795212268829346
Epoch 460, val loss: 0.8913631439208984
Epoch 470, training loss: 0.72458416223526 = 0.04741980507969856 + 0.1 * 6.77164363861084
Epoch 470, val loss: 0.9037228226661682
Epoch 480, training loss: 0.7214534878730774 = 0.0435870885848999 + 0.1 * 6.778663635253906
Epoch 480, val loss: 0.9157367944717407
Epoch 490, training loss: 0.7169671058654785 = 0.04019329324364662 + 0.1 * 6.767737865447998
Epoch 490, val loss: 0.9276849627494812
Epoch 500, training loss: 0.7123311758041382 = 0.03716635704040527 + 0.1 * 6.75164794921875
Epoch 500, val loss: 0.9393828511238098
Epoch 510, training loss: 0.712568461894989 = 0.0344555638730526 + 0.1 * 6.781128883361816
Epoch 510, val loss: 0.9507169127464294
Epoch 520, training loss: 0.7068350911140442 = 0.03203992173075676 + 0.1 * 6.747951507568359
Epoch 520, val loss: 0.9620399475097656
Epoch 530, training loss: 0.7048390507698059 = 0.02987184002995491 + 0.1 * 6.7496724128723145
Epoch 530, val loss: 0.9725650548934937
Epoch 540, training loss: 0.701434314250946 = 0.027920935302972794 + 0.1 * 6.735133647918701
Epoch 540, val loss: 0.9833132028579712
Epoch 550, training loss: 0.7003358602523804 = 0.02615867741405964 + 0.1 * 6.741771697998047
Epoch 550, val loss: 0.9933456778526306
Epoch 560, training loss: 0.6956073045730591 = 0.024564921855926514 + 0.1 * 6.710423946380615
Epoch 560, val loss: 1.0034606456756592
Epoch 570, training loss: 0.6944790482521057 = 0.023116113618016243 + 0.1 * 6.713629245758057
Epoch 570, val loss: 1.0130152702331543
Epoch 580, training loss: 0.691487729549408 = 0.021797414869070053 + 0.1 * 6.696903228759766
Epoch 580, val loss: 1.022544503211975
Epoch 590, training loss: 0.6892340183258057 = 0.020595362409949303 + 0.1 * 6.6863861083984375
Epoch 590, val loss: 1.0314700603485107
Epoch 600, training loss: 0.6874478459358215 = 0.01949707418680191 + 0.1 * 6.679507255554199
Epoch 600, val loss: 1.040549397468567
Epoch 610, training loss: 0.687493622303009 = 0.01848648488521576 + 0.1 * 6.690071105957031
Epoch 610, val loss: 1.0490350723266602
Epoch 620, training loss: 0.6841649413108826 = 0.017559630796313286 + 0.1 * 6.66605281829834
Epoch 620, val loss: 1.057517409324646
Epoch 630, training loss: 0.6849037408828735 = 0.016703054308891296 + 0.1 * 6.682006359100342
Epoch 630, val loss: 1.0656505823135376
Epoch 640, training loss: 0.6804364323616028 = 0.015914782881736755 + 0.1 * 6.645216464996338
Epoch 640, val loss: 1.0734881162643433
Epoch 650, training loss: 0.6794438362121582 = 0.015186604112386703 + 0.1 * 6.642572402954102
Epoch 650, val loss: 1.081351637840271
Epoch 660, training loss: 0.6792389154434204 = 0.014507824555039406 + 0.1 * 6.647310733795166
Epoch 660, val loss: 1.0886691808700562
Epoch 670, training loss: 0.6769267320632935 = 0.01387735828757286 + 0.1 * 6.630494117736816
Epoch 670, val loss: 1.095859169960022
Epoch 680, training loss: 0.6761760115623474 = 0.013292979449033737 + 0.1 * 6.628830432891846
Epoch 680, val loss: 1.103248119354248
Epoch 690, training loss: 0.6755383014678955 = 0.012746306136250496 + 0.1 * 6.627919673919678
Epoch 690, val loss: 1.1099404096603394
Epoch 700, training loss: 0.6740700006484985 = 0.012235600501298904 + 0.1 * 6.618344306945801
Epoch 700, val loss: 1.1168763637542725
Epoch 710, training loss: 0.6733211278915405 = 0.011756864376366138 + 0.1 * 6.615642547607422
Epoch 710, val loss: 1.1234924793243408
Epoch 720, training loss: 0.6739935278892517 = 0.01130686979740858 + 0.1 * 6.626866340637207
Epoch 720, val loss: 1.1297231912612915
Epoch 730, training loss: 0.6722328066825867 = 0.010887082666158676 + 0.1 * 6.613457202911377
Epoch 730, val loss: 1.1362614631652832
Epoch 740, training loss: 0.6700723171234131 = 0.010492515750229359 + 0.1 * 6.595798015594482
Epoch 740, val loss: 1.1424514055252075
Epoch 750, training loss: 0.6697828769683838 = 0.01011969055980444 + 0.1 * 6.59663200378418
Epoch 750, val loss: 1.1482216119766235
Epoch 760, training loss: 0.670201301574707 = 0.009768559597432613 + 0.1 * 6.60432767868042
Epoch 760, val loss: 1.1541883945465088
Epoch 770, training loss: 0.6677049994468689 = 0.00943764764815569 + 0.1 * 6.5826735496521
Epoch 770, val loss: 1.160016417503357
Epoch 780, training loss: 0.6679626107215881 = 0.00912464875727892 + 0.1 * 6.588379383087158
Epoch 780, val loss: 1.1656335592269897
Epoch 790, training loss: 0.6679801940917969 = 0.008828293532133102 + 0.1 * 6.591518878936768
Epoch 790, val loss: 1.171127438545227
Epoch 800, training loss: 0.6664115190505981 = 0.008548478595912457 + 0.1 * 6.578629970550537
Epoch 800, val loss: 1.1763592958450317
Epoch 810, training loss: 0.6651155352592468 = 0.008283894509077072 + 0.1 * 6.5683159828186035
Epoch 810, val loss: 1.181829810142517
Epoch 820, training loss: 0.6650774478912354 = 0.008032133802771568 + 0.1 * 6.57045316696167
Epoch 820, val loss: 1.1869803667068481
Epoch 830, training loss: 0.664797842502594 = 0.00779200904071331 + 0.1 * 6.570058345794678
Epoch 830, val loss: 1.191817045211792
Epoch 840, training loss: 0.663993239402771 = 0.007564755156636238 + 0.1 * 6.564284324645996
Epoch 840, val loss: 1.1969404220581055
Epoch 850, training loss: 0.664330780506134 = 0.007347604259848595 + 0.1 * 6.569831848144531
Epoch 850, val loss: 1.2018921375274658
Epoch 860, training loss: 0.6641931533813477 = 0.00714068440720439 + 0.1 * 6.570524215698242
Epoch 860, val loss: 1.2063685655593872
Epoch 870, training loss: 0.6621987819671631 = 0.0069433534517884254 + 0.1 * 6.552554130554199
Epoch 870, val loss: 1.2112668752670288
Epoch 880, training loss: 0.6621490120887756 = 0.006755921058356762 + 0.1 * 6.553930282592773
Epoch 880, val loss: 1.2158583402633667
Epoch 890, training loss: 0.6608837246894836 = 0.006575543433427811 + 0.1 * 6.543081760406494
Epoch 890, val loss: 1.2203634977340698
Epoch 900, training loss: 0.6642937660217285 = 0.0064032725058496 + 0.1 * 6.57890510559082
Epoch 900, val loss: 1.224714756011963
Epoch 910, training loss: 0.6604891419410706 = 0.006239606067538261 + 0.1 * 6.542495250701904
Epoch 910, val loss: 1.228962779045105
Epoch 920, training loss: 0.6598604917526245 = 0.006083175539970398 + 0.1 * 6.5377726554870605
Epoch 920, val loss: 1.2335535287857056
Epoch 930, training loss: 0.6604063510894775 = 0.005932442843914032 + 0.1 * 6.54473876953125
Epoch 930, val loss: 1.2374194860458374
Epoch 940, training loss: 0.6596447229385376 = 0.00578939076513052 + 0.1 * 6.538553237915039
Epoch 940, val loss: 1.2417036294937134
Epoch 950, training loss: 0.6588931083679199 = 0.00565143721178174 + 0.1 * 6.532416820526123
Epoch 950, val loss: 1.2458713054656982
Epoch 960, training loss: 0.6596891283988953 = 0.005519438534975052 + 0.1 * 6.541696548461914
Epoch 960, val loss: 1.2498170137405396
Epoch 970, training loss: 0.6584321856498718 = 0.005392399616539478 + 0.1 * 6.530397891998291
Epoch 970, val loss: 1.2535802125930786
Epoch 980, training loss: 0.6581247448921204 = 0.005270795430988073 + 0.1 * 6.528539657592773
Epoch 980, val loss: 1.2577012777328491
Epoch 990, training loss: 0.6575228571891785 = 0.0051535689271986485 + 0.1 * 6.523692607879639
Epoch 990, val loss: 1.2612922191619873
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8218239325250396
The final CL Acc:0.76543, 0.03531, The final GNN Acc:0.81761, 0.00394
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13204])
remove edge: torch.Size([2, 7844])
updated graph: torch.Size([2, 10492])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8136653900146484 = 1.953980803489685 + 0.1 * 8.596846580505371
Epoch 0, val loss: 1.9574940204620361
Epoch 10, training loss: 2.8037338256835938 = 1.9440579414367676 + 0.1 * 8.596758842468262
Epoch 10, val loss: 1.9479173421859741
Epoch 20, training loss: 2.7915382385253906 = 1.9319312572479248 + 0.1 * 8.5960693359375
Epoch 20, val loss: 1.9357720613479614
Epoch 30, training loss: 2.7737393379211426 = 1.9147840738296509 + 0.1 * 8.58955192565918
Epoch 30, val loss: 1.9183365106582642
Epoch 40, training loss: 2.743587017059326 = 1.8891078233718872 + 0.1 * 8.544793128967285
Epoch 40, val loss: 1.8922890424728394
Epoch 50, training loss: 2.6867289543151855 = 1.853078842163086 + 0.1 * 8.336502075195312
Epoch 50, val loss: 1.8570808172225952
Epoch 60, training loss: 2.6274540424346924 = 1.8113834857940674 + 0.1 * 8.16070556640625
Epoch 60, val loss: 1.8193914890289307
Epoch 70, training loss: 2.5671024322509766 = 1.77301025390625 + 0.1 * 7.940922260284424
Epoch 70, val loss: 1.7867859601974487
Epoch 80, training loss: 2.485697031021118 = 1.7346961498260498 + 0.1 * 7.510007858276367
Epoch 80, val loss: 1.751391887664795
Epoch 90, training loss: 2.412853240966797 = 1.685418725013733 + 0.1 * 7.274343967437744
Epoch 90, val loss: 1.7063382863998413
Epoch 100, training loss: 2.3389010429382324 = 1.6192764043807983 + 0.1 * 7.196247100830078
Epoch 100, val loss: 1.6493479013442993
Epoch 110, training loss: 2.246650218963623 = 1.5356504917144775 + 0.1 * 7.1099958419799805
Epoch 110, val loss: 1.579303503036499
Epoch 120, training loss: 2.1435117721557617 = 1.4415409564971924 + 0.1 * 7.019708633422852
Epoch 120, val loss: 1.501649022102356
Epoch 130, training loss: 2.0408337116241455 = 1.3449982404708862 + 0.1 * 6.95835542678833
Epoch 130, val loss: 1.4210286140441895
Epoch 140, training loss: 1.9437012672424316 = 1.2536158561706543 + 0.1 * 6.900854110717773
Epoch 140, val loss: 1.3456323146820068
Epoch 150, training loss: 1.8524720668792725 = 1.166229248046875 + 0.1 * 6.862427234649658
Epoch 150, val loss: 1.2737693786621094
Epoch 160, training loss: 1.7677662372589111 = 1.0844056606292725 + 0.1 * 6.833606243133545
Epoch 160, val loss: 1.2081258296966553
Epoch 170, training loss: 1.691906452178955 = 1.0110057592391968 + 0.1 * 6.80900764465332
Epoch 170, val loss: 1.1513724327087402
Epoch 180, training loss: 1.6237962245941162 = 0.9451180696487427 + 0.1 * 6.7867817878723145
Epoch 180, val loss: 1.1023871898651123
Epoch 190, training loss: 1.56146240234375 = 0.8832377195358276 + 0.1 * 6.7822465896606445
Epoch 190, val loss: 1.0568808317184448
Epoch 200, training loss: 1.5006340742111206 = 0.8246597051620483 + 0.1 * 6.759743690490723
Epoch 200, val loss: 1.0140100717544556
Epoch 210, training loss: 1.443587064743042 = 0.7684994339942932 + 0.1 * 6.750875473022461
Epoch 210, val loss: 0.9725989699363708
Epoch 220, training loss: 1.3898656368255615 = 0.7151951789855957 + 0.1 * 6.746705055236816
Epoch 220, val loss: 0.9336391091346741
Epoch 230, training loss: 1.3392894268035889 = 0.665917694568634 + 0.1 * 6.73371696472168
Epoch 230, val loss: 0.8986231088638306
Epoch 240, training loss: 1.2931522130966187 = 0.6201111078262329 + 0.1 * 6.730411052703857
Epoch 240, val loss: 0.8676775097846985
Epoch 250, training loss: 1.2497563362121582 = 0.5769531726837158 + 0.1 * 6.728030681610107
Epoch 250, val loss: 0.8404980897903442
Epoch 260, training loss: 1.2068885564804077 = 0.5353844165802002 + 0.1 * 6.715041160583496
Epoch 260, val loss: 0.8163910508155823
Epoch 270, training loss: 1.1671628952026367 = 0.4947131276130676 + 0.1 * 6.724496841430664
Epoch 270, val loss: 0.7950783967971802
Epoch 280, training loss: 1.1255947351455688 = 0.45510533452033997 + 0.1 * 6.704893589019775
Epoch 280, val loss: 0.7766674160957336
Epoch 290, training loss: 1.0866484642028809 = 0.4166709780693054 + 0.1 * 6.699775218963623
Epoch 290, val loss: 0.7613157629966736
Epoch 300, training loss: 1.050844669342041 = 0.38000309467315674 + 0.1 * 6.7084150314331055
Epoch 300, val loss: 0.7492800951004028
Epoch 310, training loss: 1.0144374370574951 = 0.345623254776001 + 0.1 * 6.6881422996521
Epoch 310, val loss: 0.7402093410491943
Epoch 320, training loss: 0.9821902513504028 = 0.3135044276714325 + 0.1 * 6.686858177185059
Epoch 320, val loss: 0.7338551878929138
Epoch 330, training loss: 0.9521942138671875 = 0.2838408946990967 + 0.1 * 6.683533191680908
Epoch 330, val loss: 0.7297648191452026
Epoch 340, training loss: 0.9239895939826965 = 0.25654923915863037 + 0.1 * 6.674403667449951
Epoch 340, val loss: 0.7276791334152222
Epoch 350, training loss: 0.8994093537330627 = 0.23157228529453278 + 0.1 * 6.678370475769043
Epoch 350, val loss: 0.7273078560829163
Epoch 360, training loss: 0.8754934072494507 = 0.20890222489833832 + 0.1 * 6.665911674499512
Epoch 360, val loss: 0.7284748554229736
Epoch 370, training loss: 0.8538773655891418 = 0.18846005201339722 + 0.1 * 6.654172897338867
Epoch 370, val loss: 0.731067419052124
Epoch 380, training loss: 0.8358140587806702 = 0.17012852430343628 + 0.1 * 6.65685510635376
Epoch 380, val loss: 0.7348840236663818
Epoch 390, training loss: 0.8190910816192627 = 0.1537829041481018 + 0.1 * 6.65308141708374
Epoch 390, val loss: 0.7397294640541077
Epoch 400, training loss: 0.8037756085395813 = 0.1391962766647339 + 0.1 * 6.645793437957764
Epoch 400, val loss: 0.7455611228942871
Epoch 410, training loss: 0.7904554605484009 = 0.12620267271995544 + 0.1 * 6.6425275802612305
Epoch 410, val loss: 0.7521935701370239
Epoch 420, training loss: 0.7779678106307983 = 0.11463095247745514 + 0.1 * 6.633368492126465
Epoch 420, val loss: 0.7595295310020447
Epoch 430, training loss: 0.7680389881134033 = 0.10431066155433655 + 0.1 * 6.6372833251953125
Epoch 430, val loss: 0.7673400640487671
Epoch 440, training loss: 0.75791996717453 = 0.09511587768793106 + 0.1 * 6.628040790557861
Epoch 440, val loss: 0.775490939617157
Epoch 450, training loss: 0.7489951252937317 = 0.08688981086015701 + 0.1 * 6.621053218841553
Epoch 450, val loss: 0.7839377522468567
Epoch 460, training loss: 0.740928053855896 = 0.07951518893241882 + 0.1 * 6.614129066467285
Epoch 460, val loss: 0.7925529479980469
Epoch 470, training loss: 0.7347039580345154 = 0.07289738208055496 + 0.1 * 6.61806583404541
Epoch 470, val loss: 0.801313579082489
Epoch 480, training loss: 0.7274749279022217 = 0.06696105003356934 + 0.1 * 6.605138778686523
Epoch 480, val loss: 0.8101556897163391
Epoch 490, training loss: 0.7231754064559937 = 0.061609603464603424 + 0.1 * 6.615657806396484
Epoch 490, val loss: 0.8189830780029297
Epoch 500, training loss: 0.7170144319534302 = 0.056799791753292084 + 0.1 * 6.602146625518799
Epoch 500, val loss: 0.8276974558830261
Epoch 510, training loss: 0.7115265727043152 = 0.052454475313425064 + 0.1 * 6.5907206535339355
Epoch 510, val loss: 0.8363903760910034
Epoch 520, training loss: 0.7095126509666443 = 0.04852497950196266 + 0.1 * 6.60987663269043
Epoch 520, val loss: 0.8449952602386475
Epoch 530, training loss: 0.703364372253418 = 0.044980838894844055 + 0.1 * 6.583835124969482
Epoch 530, val loss: 0.8534224629402161
Epoch 540, training loss: 0.7008433938026428 = 0.041771192103624344 + 0.1 * 6.59072208404541
Epoch 540, val loss: 0.8616949915885925
Epoch 550, training loss: 0.6968390941619873 = 0.03886349871754646 + 0.1 * 6.579756259918213
Epoch 550, val loss: 0.8698012232780457
Epoch 560, training loss: 0.694313108921051 = 0.03622088208794594 + 0.1 * 6.580921649932861
Epoch 560, val loss: 0.8777725696563721
Epoch 570, training loss: 0.6926854848861694 = 0.03381982818245888 + 0.1 * 6.588656425476074
Epoch 570, val loss: 0.8856583833694458
Epoch 580, training loss: 0.6888154745101929 = 0.03163847327232361 + 0.1 * 6.571770191192627
Epoch 580, val loss: 0.8933162093162537
Epoch 590, training loss: 0.6860925555229187 = 0.029651401564478874 + 0.1 * 6.564411163330078
Epoch 590, val loss: 0.9008608460426331
Epoch 600, training loss: 0.685136079788208 = 0.027834957465529442 + 0.1 * 6.57301139831543
Epoch 600, val loss: 0.9082205295562744
Epoch 610, training loss: 0.6829540729522705 = 0.026180708780884743 + 0.1 * 6.5677337646484375
Epoch 610, val loss: 0.9154074192047119
Epoch 620, training loss: 0.6805657148361206 = 0.024668676778674126 + 0.1 * 6.558969974517822
Epoch 620, val loss: 0.922447919845581
Epoch 630, training loss: 0.6784543991088867 = 0.023282121866941452 + 0.1 * 6.551722526550293
Epoch 630, val loss: 0.9293569326400757
Epoch 640, training loss: 0.6774746775627136 = 0.02200676128268242 + 0.1 * 6.554678916931152
Epoch 640, val loss: 0.9361276626586914
Epoch 650, training loss: 0.6757116317749023 = 0.02083299309015274 + 0.1 * 6.548786163330078
Epoch 650, val loss: 0.9427299499511719
Epoch 660, training loss: 0.6754931211471558 = 0.019750751554965973 + 0.1 * 6.5574235916137695
Epoch 660, val loss: 0.9492095708847046
Epoch 670, training loss: 0.6726893186569214 = 0.01875307224690914 + 0.1 * 6.53936243057251
Epoch 670, val loss: 0.9555537700653076
Epoch 680, training loss: 0.6734405159950256 = 0.0178301390260458 + 0.1 * 6.556103706359863
Epoch 680, val loss: 0.9617222547531128
Epoch 690, training loss: 0.6711120009422302 = 0.016976380720734596 + 0.1 * 6.541356086730957
Epoch 690, val loss: 0.9677304625511169
Epoch 700, training loss: 0.6710407733917236 = 0.016184384003281593 + 0.1 * 6.548563480377197
Epoch 700, val loss: 0.9736292958259583
Epoch 710, training loss: 0.6682997941970825 = 0.015449590981006622 + 0.1 * 6.528501987457275
Epoch 710, val loss: 0.9794057011604309
Epoch 720, training loss: 0.6691949963569641 = 0.014765270054340363 + 0.1 * 6.544297218322754
Epoch 720, val loss: 0.9851041436195374
Epoch 730, training loss: 0.6672707200050354 = 0.014127534814178944 + 0.1 * 6.531431674957275
Epoch 730, val loss: 0.9906196594238281
Epoch 740, training loss: 0.6655704379081726 = 0.013532663695514202 + 0.1 * 6.5203776359558105
Epoch 740, val loss: 0.9960537552833557
Epoch 750, training loss: 0.6662797927856445 = 0.012975333258509636 + 0.1 * 6.533044338226318
Epoch 750, val loss: 1.0013830661773682
Epoch 760, training loss: 0.6650545597076416 = 0.012454627081751823 + 0.1 * 6.525999069213867
Epoch 760, val loss: 1.0065536499023438
Epoch 770, training loss: 0.6658163666725159 = 0.01196705736219883 + 0.1 * 6.538492679595947
Epoch 770, val loss: 1.0116188526153564
Epoch 780, training loss: 0.6627641320228577 = 0.011510321870446205 + 0.1 * 6.512537956237793
Epoch 780, val loss: 1.0166041851043701
Epoch 790, training loss: 0.6631813049316406 = 0.01108033861964941 + 0.1 * 6.52100944519043
Epoch 790, val loss: 1.0214859247207642
Epoch 800, training loss: 0.6618248224258423 = 0.010675327852368355 + 0.1 * 6.511495113372803
Epoch 800, val loss: 1.0262385606765747
Epoch 810, training loss: 0.6616800427436829 = 0.010294295847415924 + 0.1 * 6.513856887817383
Epoch 810, val loss: 1.030913233757019
Epoch 820, training loss: 0.6614823341369629 = 0.009934836067259312 + 0.1 * 6.515474796295166
Epoch 820, val loss: 1.0355143547058105
Epoch 830, training loss: 0.6605145335197449 = 0.009596112184226513 + 0.1 * 6.50918436050415
Epoch 830, val loss: 1.0399672985076904
Epoch 840, training loss: 0.6600803136825562 = 0.009275841526687145 + 0.1 * 6.508044719696045
Epoch 840, val loss: 1.0443806648254395
Epoch 850, training loss: 0.65946364402771 = 0.008973506279289722 + 0.1 * 6.50490140914917
Epoch 850, val loss: 1.048675537109375
Epoch 860, training loss: 0.6588467359542847 = 0.008686507120728493 + 0.1 * 6.501602649688721
Epoch 860, val loss: 1.0528959035873413
Epoch 870, training loss: 0.6585509181022644 = 0.00841453392058611 + 0.1 * 6.501364231109619
Epoch 870, val loss: 1.0570807456970215
Epoch 880, training loss: 0.6574721336364746 = 0.008156215772032738 + 0.1 * 6.49315881729126
Epoch 880, val loss: 1.0610828399658203
Epoch 890, training loss: 0.659009575843811 = 0.007911867462098598 + 0.1 * 6.510977268218994
Epoch 890, val loss: 1.0650463104248047
Epoch 900, training loss: 0.657494068145752 = 0.007679492235183716 + 0.1 * 6.498146057128906
Epoch 900, val loss: 1.0689197778701782
Epoch 910, training loss: 0.6562036871910095 = 0.007458581123501062 + 0.1 * 6.487451076507568
Epoch 910, val loss: 1.0727465152740479
Epoch 920, training loss: 0.6572465896606445 = 0.007247547619044781 + 0.1 * 6.499989986419678
Epoch 920, val loss: 1.0764994621276855
Epoch 930, training loss: 0.6569796800613403 = 0.007046899292618036 + 0.1 * 6.499327659606934
Epoch 930, val loss: 1.0801010131835938
Epoch 940, training loss: 0.6559503674507141 = 0.006856084801256657 + 0.1 * 6.490942478179932
Epoch 940, val loss: 1.0837008953094482
Epoch 950, training loss: 0.6577749848365784 = 0.0066732182167470455 + 0.1 * 6.511017799377441
Epoch 950, val loss: 1.0872267484664917
Epoch 960, training loss: 0.6555451154708862 = 0.006499282084405422 + 0.1 * 6.490458011627197
Epoch 960, val loss: 1.0906716585159302
Epoch 970, training loss: 0.6557665467262268 = 0.006332860328257084 + 0.1 * 6.4943366050720215
Epoch 970, val loss: 1.0940542221069336
Epoch 980, training loss: 0.6532807350158691 = 0.006173799745738506 + 0.1 * 6.4710693359375
Epoch 980, val loss: 1.0973901748657227
Epoch 990, training loss: 0.654334306716919 = 0.006020919419825077 + 0.1 * 6.483133792877197
Epoch 990, val loss: 1.1006860733032227
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8439641539272537
=== training gcn model ===
Epoch 0, training loss: 2.8082265853881836 = 1.948540210723877 + 0.1 * 8.59686279296875
Epoch 0, val loss: 1.9435245990753174
Epoch 10, training loss: 2.798433780670166 = 1.9387538433074951 + 0.1 * 8.596797943115234
Epoch 10, val loss: 1.933894157409668
Epoch 20, training loss: 2.7865071296691895 = 1.9268602132797241 + 0.1 * 8.596468925476074
Epoch 20, val loss: 1.922169804573059
Epoch 30, training loss: 2.769680976867676 = 1.910305142402649 + 0.1 * 8.593758583068848
Epoch 30, val loss: 1.9059199094772339
Epoch 40, training loss: 2.7432544231414795 = 1.8859760761260986 + 0.1 * 8.572783470153809
Epoch 40, val loss: 1.8823773860931396
Epoch 50, training loss: 2.697577953338623 = 1.8512754440307617 + 0.1 * 8.46302604675293
Epoch 50, val loss: 1.8500264883041382
Epoch 60, training loss: 2.621239185333252 = 1.8101122379302979 + 0.1 * 8.111268997192383
Epoch 60, val loss: 1.8145827054977417
Epoch 70, training loss: 2.5544819831848145 = 1.7713545560836792 + 0.1 * 7.831275463104248
Epoch 70, val loss: 1.782481074333191
Epoch 80, training loss: 2.484077215194702 = 1.731600046157837 + 0.1 * 7.524771690368652
Epoch 80, val loss: 1.746661901473999
Epoch 90, training loss: 2.412130832672119 = 1.6808522939682007 + 0.1 * 7.312784194946289
Epoch 90, val loss: 1.6991853713989258
Epoch 100, training loss: 2.327033519744873 = 1.614150881767273 + 0.1 * 7.1288251876831055
Epoch 100, val loss: 1.6384263038635254
Epoch 110, training loss: 2.2313199043273926 = 1.5286978483200073 + 0.1 * 7.026219844818115
Epoch 110, val loss: 1.5637602806091309
Epoch 120, training loss: 2.1260409355163574 = 1.4272912740707397 + 0.1 * 6.987497329711914
Epoch 120, val loss: 1.4757680892944336
Epoch 130, training loss: 2.015867233276367 = 1.318720817565918 + 0.1 * 6.971463680267334
Epoch 130, val loss: 1.3839085102081299
Epoch 140, training loss: 1.9070783853530884 = 1.210805058479309 + 0.1 * 6.962733268737793
Epoch 140, val loss: 1.292910099029541
Epoch 150, training loss: 1.8045685291290283 = 1.1087013483047485 + 0.1 * 6.958672523498535
Epoch 150, val loss: 1.2076133489608765
Epoch 160, training loss: 1.7117350101470947 = 1.0162558555603027 + 0.1 * 6.954791069030762
Epoch 160, val loss: 1.1308199167251587
Epoch 170, training loss: 1.6281936168670654 = 0.933023989200592 + 0.1 * 6.951696872711182
Epoch 170, val loss: 1.0624804496765137
Epoch 180, training loss: 1.553023338317871 = 0.8581328988075256 + 0.1 * 6.948904037475586
Epoch 180, val loss: 1.0019807815551758
Epoch 190, training loss: 1.4852169752120972 = 0.7906949520111084 + 0.1 * 6.945219993591309
Epoch 190, val loss: 0.9484956860542297
Epoch 200, training loss: 1.4245455265045166 = 0.7304086089134216 + 0.1 * 6.941368579864502
Epoch 200, val loss: 0.9022271633148193
Epoch 210, training loss: 1.3699955940246582 = 0.6764698624610901 + 0.1 * 6.935257434844971
Epoch 210, val loss: 0.8630638122558594
Epoch 220, training loss: 1.3208227157592773 = 0.6276020407676697 + 0.1 * 6.932206630706787
Epoch 220, val loss: 0.829961895942688
Epoch 230, training loss: 1.2747482061386108 = 0.582351565361023 + 0.1 * 6.923966407775879
Epoch 230, val loss: 0.8017671704292297
Epoch 240, training loss: 1.231177806854248 = 0.5391533374786377 + 0.1 * 6.9202446937561035
Epoch 240, val loss: 0.776968777179718
Epoch 250, training loss: 1.1886481046676636 = 0.4973680377006531 + 0.1 * 6.9128007888793945
Epoch 250, val loss: 0.7550211548805237
Epoch 260, training loss: 1.1470543146133423 = 0.456463485956192 + 0.1 * 6.905908584594727
Epoch 260, val loss: 0.7355163097381592
Epoch 270, training loss: 1.1059318780899048 = 0.41618192195892334 + 0.1 * 6.8974995613098145
Epoch 270, val loss: 0.7183371782302856
Epoch 280, training loss: 1.065805196762085 = 0.3765935003757477 + 0.1 * 6.892117023468018
Epoch 280, val loss: 0.7031590342521667
Epoch 290, training loss: 1.026759147644043 = 0.33787834644317627 + 0.1 * 6.888808250427246
Epoch 290, val loss: 0.6898476481437683
Epoch 300, training loss: 0.9887225031852722 = 0.3009600043296814 + 0.1 * 6.877624988555908
Epoch 300, val loss: 0.6785351037979126
Epoch 310, training loss: 0.953589677810669 = 0.26657894253730774 + 0.1 * 6.870107173919678
Epoch 310, val loss: 0.6693140864372253
Epoch 320, training loss: 0.9207088947296143 = 0.23511531949043274 + 0.1 * 6.855936050415039
Epoch 320, val loss: 0.662516176700592
Epoch 330, training loss: 0.8952963948249817 = 0.20698271691799164 + 0.1 * 6.883136749267578
Epoch 330, val loss: 0.658240795135498
Epoch 340, training loss: 0.865972101688385 = 0.18262161314487457 + 0.1 * 6.833504676818848
Epoch 340, val loss: 0.6561737656593323
Epoch 350, training loss: 0.8441530466079712 = 0.16157761216163635 + 0.1 * 6.825754642486572
Epoch 350, val loss: 0.6562365889549255
Epoch 360, training loss: 0.8285280466079712 = 0.14343714714050293 + 0.1 * 6.8509087562561035
Epoch 360, val loss: 0.6582716107368469
Epoch 370, training loss: 0.8077887296676636 = 0.12799496948719025 + 0.1 * 6.797937393188477
Epoch 370, val loss: 0.6616665124893188
Epoch 380, training loss: 0.7935431003570557 = 0.11469537019729614 + 0.1 * 6.788477420806885
Epoch 380, val loss: 0.6664393544197083
Epoch 390, training loss: 0.7804666757583618 = 0.10315529257059097 + 0.1 * 6.77311372756958
Epoch 390, val loss: 0.6722144484519958
Epoch 400, training loss: 0.7691829204559326 = 0.09309893846511841 + 0.1 * 6.760839939117432
Epoch 400, val loss: 0.6788195371627808
Epoch 410, training loss: 0.7599245309829712 = 0.08432967215776443 + 0.1 * 6.755948543548584
Epoch 410, val loss: 0.6857964396476746
Epoch 420, training loss: 0.7521829605102539 = 0.07661601901054382 + 0.1 * 6.755669593811035
Epoch 420, val loss: 0.6932520866394043
Epoch 430, training loss: 0.7431058883666992 = 0.06982067227363586 + 0.1 * 6.732852458953857
Epoch 430, val loss: 0.7007623314857483
Epoch 440, training loss: 0.7364341020584106 = 0.06378480046987534 + 0.1 * 6.7264933586120605
Epoch 440, val loss: 0.7085281014442444
Epoch 450, training loss: 0.7309516668319702 = 0.05841844528913498 + 0.1 * 6.725331783294678
Epoch 450, val loss: 0.7162913084030151
Epoch 460, training loss: 0.7240431308746338 = 0.0536419115960598 + 0.1 * 6.704011917114258
Epoch 460, val loss: 0.7239276170730591
Epoch 470, training loss: 0.7193043828010559 = 0.04936517775058746 + 0.1 * 6.699391841888428
Epoch 470, val loss: 0.73163241147995
Epoch 480, training loss: 0.7141690850257874 = 0.045524001121520996 + 0.1 * 6.686450958251953
Epoch 480, val loss: 0.7393201589584351
Epoch 490, training loss: 0.7128393650054932 = 0.042076800018548965 + 0.1 * 6.707625389099121
Epoch 490, val loss: 0.7469411492347717
Epoch 500, training loss: 0.7072983384132385 = 0.03899337351322174 + 0.1 * 6.68304967880249
Epoch 500, val loss: 0.754204511642456
Epoch 510, training loss: 0.7028936743736267 = 0.03621624410152435 + 0.1 * 6.666774272918701
Epoch 510, val loss: 0.7614865899085999
Epoch 520, training loss: 0.7013031840324402 = 0.03370406851172447 + 0.1 * 6.675991058349609
Epoch 520, val loss: 0.7687432169914246
Epoch 530, training loss: 0.6973581910133362 = 0.031434424221515656 + 0.1 * 6.659237384796143
Epoch 530, val loss: 0.7757734656333923
Epoch 540, training loss: 0.6947264075279236 = 0.029374778270721436 + 0.1 * 6.6535162925720215
Epoch 540, val loss: 0.7826752066612244
Epoch 550, training loss: 0.6918267011642456 = 0.027505092322826385 + 0.1 * 6.643215656280518
Epoch 550, val loss: 0.7895218729972839
Epoch 560, training loss: 0.6900082230567932 = 0.0258033387362957 + 0.1 * 6.6420488357543945
Epoch 560, val loss: 0.7961306571960449
Epoch 570, training loss: 0.691772997379303 = 0.024252142757177353 + 0.1 * 6.675208568572998
Epoch 570, val loss: 0.8026492595672607
Epoch 580, training loss: 0.6862068176269531 = 0.022838404402136803 + 0.1 * 6.633683681488037
Epoch 580, val loss: 0.8090176582336426
Epoch 590, training loss: 0.6838210225105286 = 0.02154487743973732 + 0.1 * 6.622761249542236
Epoch 590, val loss: 0.8151450753211975
Epoch 600, training loss: 0.682494044303894 = 0.02036096528172493 + 0.1 * 6.621330738067627
Epoch 600, val loss: 0.8213309049606323
Epoch 610, training loss: 0.6807403564453125 = 0.019277915358543396 + 0.1 * 6.6146240234375
Epoch 610, val loss: 0.827095091342926
Epoch 620, training loss: 0.6795729994773865 = 0.018281925469636917 + 0.1 * 6.612910270690918
Epoch 620, val loss: 0.8327540159225464
Epoch 630, training loss: 0.6784670948982239 = 0.01736043021082878 + 0.1 * 6.6110663414001465
Epoch 630, val loss: 0.8384464383125305
Epoch 640, training loss: 0.6791777610778809 = 0.016508737578988075 + 0.1 * 6.626689910888672
Epoch 640, val loss: 0.8440570831298828
Epoch 650, training loss: 0.6761340498924255 = 0.015722496435046196 + 0.1 * 6.6041154861450195
Epoch 650, val loss: 0.8492917418479919
Epoch 660, training loss: 0.6759787201881409 = 0.014994005672633648 + 0.1 * 6.609846591949463
Epoch 660, val loss: 0.8544731140136719
Epoch 670, training loss: 0.6739016771316528 = 0.01431648712605238 + 0.1 * 6.595851421356201
Epoch 670, val loss: 0.85965895652771
Epoch 680, training loss: 0.6729344725608826 = 0.013686666265130043 + 0.1 * 6.592477798461914
Epoch 680, val loss: 0.8645971417427063
Epoch 690, training loss: 0.6718721985816956 = 0.013099678792059422 + 0.1 * 6.5877251625061035
Epoch 690, val loss: 0.8695371747016907
Epoch 700, training loss: 0.6713614463806152 = 0.012553525157272816 + 0.1 * 6.588079452514648
Epoch 700, val loss: 0.8742206692695618
Epoch 710, training loss: 0.6697771549224854 = 0.012042262591421604 + 0.1 * 6.577348709106445
Epoch 710, val loss: 0.8788183331489563
Epoch 720, training loss: 0.6702321767807007 = 0.011562931351363659 + 0.1 * 6.5866923332214355
Epoch 720, val loss: 0.8834450244903564
Epoch 730, training loss: 0.6684544682502747 = 0.011115278117358685 + 0.1 * 6.573391914367676
Epoch 730, val loss: 0.8879790306091309
Epoch 740, training loss: 0.6683221459388733 = 0.010696612298488617 + 0.1 * 6.5762553215026855
Epoch 740, val loss: 0.8922102451324463
Epoch 750, training loss: 0.6662759780883789 = 0.010303094051778316 + 0.1 * 6.559728622436523
Epoch 750, val loss: 0.8963676691055298
Epoch 760, training loss: 0.6690890192985535 = 0.009931306354701519 + 0.1 * 6.59157657623291
Epoch 760, val loss: 0.9005928635597229
Epoch 770, training loss: 0.6643704175949097 = 0.009581281803548336 + 0.1 * 6.547891616821289
Epoch 770, val loss: 0.9047158360481262
Epoch 780, training loss: 0.6657302975654602 = 0.00925145298242569 + 0.1 * 6.564787864685059
Epoch 780, val loss: 0.9086293578147888
Epoch 790, training loss: 0.6644220352172852 = 0.008940289728343487 + 0.1 * 6.554817199707031
Epoch 790, val loss: 0.9124577641487122
Epoch 800, training loss: 0.6626827120780945 = 0.008646054193377495 + 0.1 * 6.540366172790527
Epoch 800, val loss: 0.9162557125091553
Epoch 810, training loss: 0.6627382636070251 = 0.008367245085537434 + 0.1 * 6.543710231781006
Epoch 810, val loss: 0.9199574589729309
Epoch 820, training loss: 0.6648200750350952 = 0.008102445863187313 + 0.1 * 6.567176342010498
Epoch 820, val loss: 0.9236420392990112
Epoch 830, training loss: 0.6626371145248413 = 0.007851514033973217 + 0.1 * 6.547855854034424
Epoch 830, val loss: 0.9272838830947876
Epoch 840, training loss: 0.6635504961013794 = 0.0076141357421875 + 0.1 * 6.55936336517334
Epoch 840, val loss: 0.930817186832428
Epoch 850, training loss: 0.6600484848022461 = 0.007389265112578869 + 0.1 * 6.526592254638672
Epoch 850, val loss: 0.934201180934906
Epoch 860, training loss: 0.6593580842018127 = 0.007175151724368334 + 0.1 * 6.521829128265381
Epoch 860, val loss: 0.9375168681144714
Epoch 870, training loss: 0.6615047454833984 = 0.006970840971916914 + 0.1 * 6.545339107513428
Epoch 870, val loss: 0.9407975077629089
Epoch 880, training loss: 0.6594415903091431 = 0.006775364279747009 + 0.1 * 6.5266618728637695
Epoch 880, val loss: 0.9441533088684082
Epoch 890, training loss: 0.661536455154419 = 0.006589566357433796 + 0.1 * 6.549468517303467
Epoch 890, val loss: 0.9473576545715332
Epoch 900, training loss: 0.6575251221656799 = 0.00641227513551712 + 0.1 * 6.5111284255981445
Epoch 900, val loss: 0.950508713722229
Epoch 910, training loss: 0.6571078896522522 = 0.006243361625820398 + 0.1 * 6.508645534515381
Epoch 910, val loss: 0.9535366296768188
Epoch 920, training loss: 0.6578673124313354 = 0.006081542931497097 + 0.1 * 6.517858028411865
Epoch 920, val loss: 0.9565235376358032
Epoch 930, training loss: 0.6573036909103394 = 0.005926221609115601 + 0.1 * 6.513774871826172
Epoch 930, val loss: 0.9596148729324341
Epoch 940, training loss: 0.656214714050293 = 0.005778009071946144 + 0.1 * 6.504366874694824
Epoch 940, val loss: 0.9625744819641113
Epoch 950, training loss: 0.6575332283973694 = 0.005636223126202822 + 0.1 * 6.518970012664795
Epoch 950, val loss: 0.9654390811920166
Epoch 960, training loss: 0.6562747955322266 = 0.00550056342035532 + 0.1 * 6.507742404937744
Epoch 960, val loss: 0.9682057499885559
Epoch 970, training loss: 0.6547935009002686 = 0.005370193626731634 + 0.1 * 6.494232654571533
Epoch 970, val loss: 0.9709908962249756
Epoch 980, training loss: 0.6543924808502197 = 0.005244543310254812 + 0.1 * 6.49147891998291
Epoch 980, val loss: 0.9737663865089417
Epoch 990, training loss: 0.6536270380020142 = 0.005124214105308056 + 0.1 * 6.485028266906738
Epoch 990, val loss: 0.9764763712882996
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.81935977935791 = 1.9596750736236572 + 0.1 * 8.596845626831055
Epoch 0, val loss: 1.9560279846191406
Epoch 10, training loss: 2.8080594539642334 = 1.9483819007873535 + 0.1 * 8.596776008605957
Epoch 10, val loss: 1.9443061351776123
Epoch 20, training loss: 2.7943572998046875 = 1.9347244501113892 + 0.1 * 8.596327781677246
Epoch 20, val loss: 1.929861068725586
Epoch 30, training loss: 2.7750749588012695 = 1.9158434867858887 + 0.1 * 8.592315673828125
Epoch 30, val loss: 1.9099302291870117
Epoch 40, training loss: 2.744460105895996 = 1.888303279876709 + 0.1 * 8.561567306518555
Epoch 40, val loss: 1.8814830780029297
Epoch 50, training loss: 2.691220283508301 = 1.8507755994796753 + 0.1 * 8.40444564819336
Epoch 50, val loss: 1.8449935913085938
Epoch 60, training loss: 2.6259701251983643 = 1.810110092163086 + 0.1 * 8.158600807189941
Epoch 60, val loss: 1.809470534324646
Epoch 70, training loss: 2.5656352043151855 = 1.7737997770309448 + 0.1 * 7.918353080749512
Epoch 70, val loss: 1.7803912162780762
Epoch 80, training loss: 2.482954740524292 = 1.7365144491195679 + 0.1 * 7.4644036293029785
Epoch 80, val loss: 1.7481597661972046
Epoch 90, training loss: 2.411513328552246 = 1.6885900497436523 + 0.1 * 7.229231834411621
Epoch 90, val loss: 1.7052001953125
Epoch 100, training loss: 2.3397581577301025 = 1.6238528490066528 + 0.1 * 7.15905237197876
Epoch 100, val loss: 1.6486135721206665
Epoch 110, training loss: 2.24991774559021 = 1.5426899194717407 + 0.1 * 7.07227897644043
Epoch 110, val loss: 1.5788660049438477
Epoch 120, training loss: 2.1540589332580566 = 1.45305335521698 + 0.1 * 7.0100555419921875
Epoch 120, val loss: 1.5040438175201416
Epoch 130, training loss: 2.0583887100219727 = 1.3622798919677734 + 0.1 * 6.96108865737915
Epoch 130, val loss: 1.429524540901184
Epoch 140, training loss: 1.9651217460632324 = 1.2729977369308472 + 0.1 * 6.921240329742432
Epoch 140, val loss: 1.3576745986938477
Epoch 150, training loss: 1.8761532306671143 = 1.186355710029602 + 0.1 * 6.897974967956543
Epoch 150, val loss: 1.2889903783798218
Epoch 160, training loss: 1.7897995710372925 = 1.1016581058502197 + 0.1 * 6.881414413452148
Epoch 160, val loss: 1.2242181301116943
Epoch 170, training loss: 1.706315040588379 = 1.0201377868652344 + 0.1 * 6.861772060394287
Epoch 170, val loss: 1.163337230682373
Epoch 180, training loss: 1.6268216371536255 = 0.9418722987174988 + 0.1 * 6.849493026733398
Epoch 180, val loss: 1.1057549715042114
Epoch 190, training loss: 1.5507540702819824 = 0.8676798939704895 + 0.1 * 6.8307414054870605
Epoch 190, val loss: 1.0514544248580933
Epoch 200, training loss: 1.4797223806381226 = 0.7974901795387268 + 0.1 * 6.822321891784668
Epoch 200, val loss: 1.0002819299697876
Epoch 210, training loss: 1.4125536680221558 = 0.7323197722434998 + 0.1 * 6.80233907699585
Epoch 210, val loss: 0.9537097811698914
Epoch 220, training loss: 1.3510329723358154 = 0.6717513799667358 + 0.1 * 6.792815208435059
Epoch 220, val loss: 0.9115427732467651
Epoch 230, training loss: 1.29392409324646 = 0.6158304214477539 + 0.1 * 6.780936241149902
Epoch 230, val loss: 0.8746531009674072
Epoch 240, training loss: 1.2407894134521484 = 0.5641046762466431 + 0.1 * 6.766847133636475
Epoch 240, val loss: 0.8427562713623047
Epoch 250, training loss: 1.1923997402191162 = 0.5154760479927063 + 0.1 * 6.769237518310547
Epoch 250, val loss: 0.815129280090332
Epoch 260, training loss: 1.1440529823303223 = 0.46958857774734497 + 0.1 * 6.744643688201904
Epoch 260, val loss: 0.791985273361206
Epoch 270, training loss: 1.0993471145629883 = 0.42548802495002747 + 0.1 * 6.738591194152832
Epoch 270, val loss: 0.7729443311691284
Epoch 280, training loss: 1.056706428527832 = 0.38347098231315613 + 0.1 * 6.732354640960693
Epoch 280, val loss: 0.7581931352615356
Epoch 290, training loss: 1.0168907642364502 = 0.3443027436733246 + 0.1 * 6.725880146026611
Epoch 290, val loss: 0.7475495934486389
Epoch 300, training loss: 0.9806991815567017 = 0.3086972236633301 + 0.1 * 6.720019340515137
Epoch 300, val loss: 0.740898609161377
Epoch 310, training loss: 0.9485361576080322 = 0.27699095010757446 + 0.1 * 6.715452194213867
Epoch 310, val loss: 0.7377036213874817
Epoch 320, training loss: 0.91958087682724 = 0.24886548519134521 + 0.1 * 6.707153797149658
Epoch 320, val loss: 0.7377025485038757
Epoch 330, training loss: 0.8960042595863342 = 0.22408916056156158 + 0.1 * 6.719151020050049
Epoch 330, val loss: 0.7403150796890259
Epoch 340, training loss: 0.8724895715713501 = 0.202355295419693 + 0.1 * 6.7013421058654785
Epoch 340, val loss: 0.7448357939720154
Epoch 350, training loss: 0.8524144887924194 = 0.18308505415916443 + 0.1 * 6.693294525146484
Epoch 350, val loss: 0.751133918762207
Epoch 360, training loss: 0.8354724645614624 = 0.16592952609062195 + 0.1 * 6.69542932510376
Epoch 360, val loss: 0.7588438391685486
Epoch 370, training loss: 0.819517195224762 = 0.15074169635772705 + 0.1 * 6.6877546310424805
Epoch 370, val loss: 0.7677212953567505
Epoch 380, training loss: 0.8051912784576416 = 0.13723643124103546 + 0.1 * 6.679548740386963
Epoch 380, val loss: 0.7775247097015381
Epoch 390, training loss: 0.7926424741744995 = 0.12513694167137146 + 0.1 * 6.675055027008057
Epoch 390, val loss: 0.7882096171379089
Epoch 400, training loss: 0.7831397652626038 = 0.11426889151334763 + 0.1 * 6.688708782196045
Epoch 400, val loss: 0.7996006608009338
Epoch 410, training loss: 0.7712705731391907 = 0.10454930365085602 + 0.1 * 6.667212963104248
Epoch 410, val loss: 0.8113889098167419
Epoch 420, training loss: 0.7625306248664856 = 0.09582376480102539 + 0.1 * 6.6670684814453125
Epoch 420, val loss: 0.8235048055648804
Epoch 430, training loss: 0.754162073135376 = 0.08798163384199142 + 0.1 * 6.66180419921875
Epoch 430, val loss: 0.8359286785125732
Epoch 440, training loss: 0.7465122938156128 = 0.0809016227722168 + 0.1 * 6.656106472015381
Epoch 440, val loss: 0.8485870957374573
Epoch 450, training loss: 0.7392996549606323 = 0.0745181068778038 + 0.1 * 6.647815704345703
Epoch 450, val loss: 0.8612249493598938
Epoch 460, training loss: 0.7364160418510437 = 0.06875483691692352 + 0.1 * 6.67661190032959
Epoch 460, val loss: 0.8738892674446106
Epoch 470, training loss: 0.7272549867630005 = 0.06358888000249863 + 0.1 * 6.636661052703857
Epoch 470, val loss: 0.8863375782966614
Epoch 480, training loss: 0.7223743796348572 = 0.05891790613532066 + 0.1 * 6.6345648765563965
Epoch 480, val loss: 0.8986290693283081
Epoch 490, training loss: 0.7172085642814636 = 0.05467430129647255 + 0.1 * 6.62534236907959
Epoch 490, val loss: 0.9109015464782715
Epoch 500, training loss: 0.7153491377830505 = 0.05081208050251007 + 0.1 * 6.6453704833984375
Epoch 500, val loss: 0.9230465292930603
Epoch 510, training loss: 0.7086772918701172 = 0.04731080308556557 + 0.1 * 6.613664627075195
Epoch 510, val loss: 0.9349860548973083
Epoch 520, training loss: 0.7076804637908936 = 0.044123824685811996 + 0.1 * 6.635566234588623
Epoch 520, val loss: 0.9466126561164856
Epoch 530, training loss: 0.7018234133720398 = 0.04122691601514816 + 0.1 * 6.605964660644531
Epoch 530, val loss: 0.9580675959587097
Epoch 540, training loss: 0.6994915008544922 = 0.03858112543821335 + 0.1 * 6.609103679656982
Epoch 540, val loss: 0.9693185091018677
Epoch 550, training loss: 0.6954898238182068 = 0.036166030913591385 + 0.1 * 6.59323787689209
Epoch 550, val loss: 0.9802934527397156
Epoch 560, training loss: 0.6931200623512268 = 0.03395425155758858 + 0.1 * 6.591658115386963
Epoch 560, val loss: 0.9910294413566589
Epoch 570, training loss: 0.6912907361984253 = 0.031923893839120865 + 0.1 * 6.593668460845947
Epoch 570, val loss: 1.0016558170318604
Epoch 580, training loss: 0.6892160773277283 = 0.030062783509492874 + 0.1 * 6.5915327072143555
Epoch 580, val loss: 1.0118743181228638
Epoch 590, training loss: 0.6870455145835876 = 0.0283522866666317 + 0.1 * 6.58693265914917
Epoch 590, val loss: 1.021985650062561
Epoch 600, training loss: 0.6835697293281555 = 0.026780737563967705 + 0.1 * 6.56788969039917
Epoch 600, val loss: 1.0317988395690918
Epoch 610, training loss: 0.6829684972763062 = 0.02533239871263504 + 0.1 * 6.576360702514648
Epoch 610, val loss: 1.04133141040802
Epoch 620, training loss: 0.6804722547531128 = 0.023998934775590897 + 0.1 * 6.564733028411865
Epoch 620, val loss: 1.0506855249404907
Epoch 630, training loss: 0.6802958846092224 = 0.022764932364225388 + 0.1 * 6.5753092765808105
Epoch 630, val loss: 1.059777021408081
Epoch 640, training loss: 0.6770919561386108 = 0.021623486652970314 + 0.1 * 6.554684638977051
Epoch 640, val loss: 1.0687540769577026
Epoch 650, training loss: 0.6762019395828247 = 0.020564299076795578 + 0.1 * 6.556375980377197
Epoch 650, val loss: 1.0774575471878052
Epoch 660, training loss: 0.6757113337516785 = 0.019582726061344147 + 0.1 * 6.561285495758057
Epoch 660, val loss: 1.0859074592590332
Epoch 670, training loss: 0.6735657453536987 = 0.01867169514298439 + 0.1 * 6.548940658569336
Epoch 670, val loss: 1.094187617301941
Epoch 680, training loss: 0.672272264957428 = 0.01782183162868023 + 0.1 * 6.544504165649414
Epoch 680, val loss: 1.102259635925293
Epoch 690, training loss: 0.6711289882659912 = 0.017030784860253334 + 0.1 * 6.540982246398926
Epoch 690, val loss: 1.1101564168930054
Epoch 700, training loss: 0.6708146333694458 = 0.016292154788970947 + 0.1 * 6.545224666595459
Epoch 700, val loss: 1.1178609132766724
Epoch 710, training loss: 0.6697480082511902 = 0.015601912513375282 + 0.1 * 6.5414605140686035
Epoch 710, val loss: 1.1254245042800903
Epoch 720, training loss: 0.668073296546936 = 0.014957415871322155 + 0.1 * 6.531158447265625
Epoch 720, val loss: 1.1327344179153442
Epoch 730, training loss: 0.6682087779045105 = 0.014353249222040176 + 0.1 * 6.538554668426514
Epoch 730, val loss: 1.1399705410003662
Epoch 740, training loss: 0.6674010753631592 = 0.01378689892590046 + 0.1 * 6.536141395568848
Epoch 740, val loss: 1.1469398736953735
Epoch 750, training loss: 0.6657997369766235 = 0.013256722129881382 + 0.1 * 6.525430202484131
Epoch 750, val loss: 1.1538007259368896
Epoch 760, training loss: 0.6646836400032043 = 0.012757917866110802 + 0.1 * 6.519257545471191
Epoch 760, val loss: 1.1604574918746948
Epoch 770, training loss: 0.6639270782470703 = 0.01228839810937643 + 0.1 * 6.51638650894165
Epoch 770, val loss: 1.167034387588501
Epoch 780, training loss: 0.6649431586265564 = 0.011845729313790798 + 0.1 * 6.530974388122559
Epoch 780, val loss: 1.1734107732772827
Epoch 790, training loss: 0.6627805233001709 = 0.011429364793002605 + 0.1 * 6.513511657714844
Epoch 790, val loss: 1.1796514987945557
Epoch 800, training loss: 0.6625587344169617 = 0.011035180650651455 + 0.1 * 6.515235424041748
Epoch 800, val loss: 1.185774564743042
Epoch 810, training loss: 0.6639257073402405 = 0.010662490501999855 + 0.1 * 6.532631874084473
Epoch 810, val loss: 1.1917901039123535
Epoch 820, training loss: 0.660648763179779 = 0.010311346501111984 + 0.1 * 6.503373622894287
Epoch 820, val loss: 1.1975629329681396
Epoch 830, training loss: 0.6602432727813721 = 0.009977991692721844 + 0.1 * 6.502652645111084
Epoch 830, val loss: 1.203319787979126
Epoch 840, training loss: 0.6611397862434387 = 0.00966193899512291 + 0.1 * 6.5147786140441895
Epoch 840, val loss: 1.2089513540267944
Epoch 850, training loss: 0.6604247689247131 = 0.009362820535898209 + 0.1 * 6.510619640350342
Epoch 850, val loss: 1.2143397331237793
Epoch 860, training loss: 0.6577007174491882 = 0.009078997187316418 + 0.1 * 6.486217021942139
Epoch 860, val loss: 1.2197271585464478
Epoch 870, training loss: 0.660004198551178 = 0.008808762766420841 + 0.1 * 6.511953830718994
Epoch 870, val loss: 1.224992036819458
Epoch 880, training loss: 0.6576603055000305 = 0.008551161736249924 + 0.1 * 6.491091251373291
Epoch 880, val loss: 1.2301667928695679
Epoch 890, training loss: 0.6591090559959412 = 0.008306331932544708 + 0.1 * 6.508027076721191
Epoch 890, val loss: 1.2351469993591309
Epoch 900, training loss: 0.6567429900169373 = 0.008073115721344948 + 0.1 * 6.486698627471924
Epoch 900, val loss: 1.2401041984558105
Epoch 910, training loss: 0.6558232307434082 = 0.007850633934140205 + 0.1 * 6.4797258377075195
Epoch 910, val loss: 1.2449591159820557
Epoch 920, training loss: 0.6548345685005188 = 0.007638376671820879 + 0.1 * 6.471961498260498
Epoch 920, val loss: 1.2497081756591797
Epoch 930, training loss: 0.6553018093109131 = 0.007435652427375317 + 0.1 * 6.478661060333252
Epoch 930, val loss: 1.25442636013031
Epoch 940, training loss: 0.6539409160614014 = 0.0072409468702971935 + 0.1 * 6.466999530792236
Epoch 940, val loss: 1.2590546607971191
Epoch 950, training loss: 0.6565584540367126 = 0.007054748944938183 + 0.1 * 6.495037078857422
Epoch 950, val loss: 1.2635236978530884
Epoch 960, training loss: 0.6543821692466736 = 0.006876478902995586 + 0.1 * 6.4750566482543945
Epoch 960, val loss: 1.2679001092910767
Epoch 970, training loss: 0.6528756618499756 = 0.006706109270453453 + 0.1 * 6.461695194244385
Epoch 970, val loss: 1.2723029851913452
Epoch 980, training loss: 0.653888463973999 = 0.0065423352643847466 + 0.1 * 6.473461151123047
Epoch 980, val loss: 1.2765686511993408
Epoch 990, training loss: 0.6537582874298096 = 0.0063853794708848 + 0.1 * 6.473729133605957
Epoch 990, val loss: 1.2807215452194214
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8365840801265156
The final CL Acc:0.80864, 0.00175, The final GNN Acc:0.84027, 0.00301
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9574])
updated graph: torch.Size([2, 10644])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8176276683807373 = 1.9579466581344604 + 0.1 * 8.596810340881348
Epoch 0, val loss: 1.955957055091858
Epoch 10, training loss: 2.8064234256744385 = 1.9467554092407227 + 0.1 * 8.5966796875
Epoch 10, val loss: 1.9455374479293823
Epoch 20, training loss: 2.792449951171875 = 1.9328773021697998 + 0.1 * 8.595726013183594
Epoch 20, val loss: 1.9322222471237183
Epoch 30, training loss: 2.772141456604004 = 1.9134634733200073 + 0.1 * 8.58677864074707
Epoch 30, val loss: 1.9133563041687012
Epoch 40, training loss: 2.7382283210754395 = 1.8852635622024536 + 0.1 * 8.529646873474121
Epoch 40, val loss: 1.8861336708068848
Epoch 50, training loss: 2.6770246028900146 = 1.8492063283920288 + 0.1 * 8.278182029724121
Epoch 50, val loss: 1.8530954122543335
Epoch 60, training loss: 2.6297457218170166 = 1.8119966983795166 + 0.1 * 8.177489280700684
Epoch 60, val loss: 1.8221219778060913
Epoch 70, training loss: 2.5801682472229004 = 1.7821531295776367 + 0.1 * 7.980151176452637
Epoch 70, val loss: 1.7973077297210693
Epoch 80, training loss: 2.5064148902893066 = 1.7499829530715942 + 0.1 * 7.564318656921387
Epoch 80, val loss: 1.7659696340560913
Epoch 90, training loss: 2.4286060333251953 = 1.7106865644454956 + 0.1 * 7.17919397354126
Epoch 90, val loss: 1.7296886444091797
Epoch 100, training loss: 2.3641743659973145 = 1.6560908555984497 + 0.1 * 7.080833911895752
Epoch 100, val loss: 1.6826437711715698
Epoch 110, training loss: 2.2879223823547363 = 1.5859650373458862 + 0.1 * 7.019572734832764
Epoch 110, val loss: 1.6248494386672974
Epoch 120, training loss: 2.2081503868103027 = 1.511364221572876 + 0.1 * 6.967862606048584
Epoch 120, val loss: 1.5687750577926636
Epoch 130, training loss: 2.1336963176727295 = 1.4406116008758545 + 0.1 * 6.93084716796875
Epoch 130, val loss: 1.5194414854049683
Epoch 140, training loss: 2.065509796142578 = 1.3767496347427368 + 0.1 * 6.88760232925415
Epoch 140, val loss: 1.4782897233963013
Epoch 150, training loss: 2.0013670921325684 = 1.3155417442321777 + 0.1 * 6.8582539558410645
Epoch 150, val loss: 1.4391560554504395
Epoch 160, training loss: 1.937924861907959 = 1.2544680833816528 + 0.1 * 6.834567070007324
Epoch 160, val loss: 1.400869607925415
Epoch 170, training loss: 1.874518632888794 = 1.193182110786438 + 0.1 * 6.813365936279297
Epoch 170, val loss: 1.3631871938705444
Epoch 180, training loss: 1.8109357357025146 = 1.1317542791366577 + 0.1 * 6.791814804077148
Epoch 180, val loss: 1.3259152173995972
Epoch 190, training loss: 1.7480504512786865 = 1.0702720880508423 + 0.1 * 6.777783393859863
Epoch 190, val loss: 1.2885316610336304
Epoch 200, training loss: 1.6860798597335815 = 1.009615421295166 + 0.1 * 6.764644145965576
Epoch 200, val loss: 1.2521334886550903
Epoch 210, training loss: 1.6254899501800537 = 0.9505128860473633 + 0.1 * 6.749770641326904
Epoch 210, val loss: 1.2165852785110474
Epoch 220, training loss: 1.5672557353973389 = 0.8924562931060791 + 0.1 * 6.747994899749756
Epoch 220, val loss: 1.181122899055481
Epoch 230, training loss: 1.509194016456604 = 0.8358769416809082 + 0.1 * 6.733170509338379
Epoch 230, val loss: 1.1466175317764282
Epoch 240, training loss: 1.4517650604248047 = 0.7799948453903198 + 0.1 * 6.717702388763428
Epoch 240, val loss: 1.1128640174865723
Epoch 250, training loss: 1.3956382274627686 = 0.724822461605072 + 0.1 * 6.708157062530518
Epoch 250, val loss: 1.0806279182434082
Epoch 260, training loss: 1.341949224472046 = 0.671730637550354 + 0.1 * 6.702185153961182
Epoch 260, val loss: 1.0519148111343384
Epoch 270, training loss: 1.2911298274993896 = 0.6218951344490051 + 0.1 * 6.692346572875977
Epoch 270, val loss: 1.0278904438018799
Epoch 280, training loss: 1.2443495988845825 = 0.5761482119560242 + 0.1 * 6.682013511657715
Epoch 280, val loss: 1.0095072984695435
Epoch 290, training loss: 1.2029445171356201 = 0.5349761247634888 + 0.1 * 6.679684638977051
Epoch 290, val loss: 0.9971197247505188
Epoch 300, training loss: 1.1642273664474487 = 0.4980413019657135 + 0.1 * 6.661860942840576
Epoch 300, val loss: 0.9898211359977722
Epoch 310, training loss: 1.1298060417175293 = 0.4640311002731323 + 0.1 * 6.657748699188232
Epoch 310, val loss: 0.9865712523460388
Epoch 320, training loss: 1.096765160560608 = 0.4321865141391754 + 0.1 * 6.645786285400391
Epoch 320, val loss: 0.9862625598907471
Epoch 330, training loss: 1.0654056072235107 = 0.4017869234085083 + 0.1 * 6.6361870765686035
Epoch 330, val loss: 0.9881467223167419
Epoch 340, training loss: 1.036645770072937 = 0.37232518196105957 + 0.1 * 6.643205642700195
Epoch 340, val loss: 0.9919580817222595
Epoch 350, training loss: 1.0058610439300537 = 0.3438042402267456 + 0.1 * 6.620568752288818
Epoch 350, val loss: 0.9974420666694641
Epoch 360, training loss: 0.9790825843811035 = 0.31627577543258667 + 0.1 * 6.628067970275879
Epoch 360, val loss: 1.0045230388641357
Epoch 370, training loss: 0.9510480761528015 = 0.29015642404556274 + 0.1 * 6.608916282653809
Epoch 370, val loss: 1.0132027864456177
Epoch 380, training loss: 0.927737832069397 = 0.2655462920665741 + 0.1 * 6.621915817260742
Epoch 380, val loss: 1.0235475301742554
Epoch 390, training loss: 0.9023830890655518 = 0.24277263879776 + 0.1 * 6.596104145050049
Epoch 390, val loss: 1.0354478359222412
Epoch 400, training loss: 0.8816022276878357 = 0.22177369892597198 + 0.1 * 6.59828519821167
Epoch 400, val loss: 1.048709511756897
Epoch 410, training loss: 0.8613060116767883 = 0.20254868268966675 + 0.1 * 6.587573051452637
Epoch 410, val loss: 1.0632110834121704
Epoch 420, training loss: 0.8439594507217407 = 0.1849873960018158 + 0.1 * 6.589720249176025
Epoch 420, val loss: 1.0785489082336426
Epoch 430, training loss: 0.8284473419189453 = 0.16904166340827942 + 0.1 * 6.594056606292725
Epoch 430, val loss: 1.0945274829864502
Epoch 440, training loss: 0.8115100264549255 = 0.15462835133075714 + 0.1 * 6.568816661834717
Epoch 440, val loss: 1.110863208770752
Epoch 450, training loss: 0.7977094650268555 = 0.1415640115737915 + 0.1 * 6.5614542961120605
Epoch 450, val loss: 1.1273927688598633
Epoch 460, training loss: 0.7857104539871216 = 0.12970301508903503 + 0.1 * 6.560074329376221
Epoch 460, val loss: 1.144135594367981
Epoch 470, training loss: 0.7759641408920288 = 0.11897604912519455 + 0.1 * 6.569880962371826
Epoch 470, val loss: 1.160823106765747
Epoch 480, training loss: 0.7653133869171143 = 0.10928145051002502 + 0.1 * 6.560318946838379
Epoch 480, val loss: 1.1773369312286377
Epoch 490, training loss: 0.7566118240356445 = 0.10052957385778427 + 0.1 * 6.560822486877441
Epoch 490, val loss: 1.1937018632888794
Epoch 500, training loss: 0.7469156980514526 = 0.09260397404432297 + 0.1 * 6.543117046356201
Epoch 500, val loss: 1.2098333835601807
Epoch 510, training loss: 0.7406074404716492 = 0.08540654182434082 + 0.1 * 6.552008628845215
Epoch 510, val loss: 1.2258412837982178
Epoch 520, training loss: 0.7325989007949829 = 0.07887565344572067 + 0.1 * 6.537232398986816
Epoch 520, val loss: 1.2416974306106567
Epoch 530, training loss: 0.7271118760108948 = 0.0729406401515007 + 0.1 * 6.541712284088135
Epoch 530, val loss: 1.2571966648101807
Epoch 540, training loss: 0.7204142808914185 = 0.06755712628364563 + 0.1 * 6.528571128845215
Epoch 540, val loss: 1.27243971824646
Epoch 550, training loss: 0.7154288291931152 = 0.06266111135482788 + 0.1 * 6.527677059173584
Epoch 550, val loss: 1.2873313426971436
Epoch 560, training loss: 0.7111403942108154 = 0.0582004189491272 + 0.1 * 6.529399394989014
Epoch 560, val loss: 1.3018724918365479
Epoch 570, training loss: 0.7071057558059692 = 0.05414411053061485 + 0.1 * 6.529616832733154
Epoch 570, val loss: 1.3159786462783813
Epoch 580, training loss: 0.7022476196289062 = 0.050460804253816605 + 0.1 * 6.5178680419921875
Epoch 580, val loss: 1.3296176195144653
Epoch 590, training loss: 0.6982063055038452 = 0.047105494886636734 + 0.1 * 6.511008262634277
Epoch 590, val loss: 1.3428959846496582
Epoch 600, training loss: 0.6944435834884644 = 0.04403911158442497 + 0.1 * 6.504044532775879
Epoch 600, val loss: 1.3557437658309937
Epoch 610, training loss: 0.6927675008773804 = 0.041232965886592865 + 0.1 * 6.515345573425293
Epoch 610, val loss: 1.3682481050491333
Epoch 620, training loss: 0.6899560689926147 = 0.03866879269480705 + 0.1 * 6.512872695922852
Epoch 620, val loss: 1.3803951740264893
Epoch 630, training loss: 0.6869222521781921 = 0.03632602468132973 + 0.1 * 6.505961894989014
Epoch 630, val loss: 1.3920471668243408
Epoch 640, training loss: 0.6847254633903503 = 0.0341804139316082 + 0.1 * 6.505450248718262
Epoch 640, val loss: 1.4034816026687622
Epoch 650, training loss: 0.6821858286857605 = 0.03221171349287033 + 0.1 * 6.499741077423096
Epoch 650, val loss: 1.4144834280014038
Epoch 660, training loss: 0.6794530749320984 = 0.030400874093174934 + 0.1 * 6.4905219078063965
Epoch 660, val loss: 1.425230622291565
Epoch 670, training loss: 0.6797442436218262 = 0.028732905164361 + 0.1 * 6.51011323928833
Epoch 670, val loss: 1.435696005821228
Epoch 680, training loss: 0.6769640445709229 = 0.027195869013667107 + 0.1 * 6.497681617736816
Epoch 680, val loss: 1.4457926750183105
Epoch 690, training loss: 0.6747652292251587 = 0.02578166127204895 + 0.1 * 6.489835262298584
Epoch 690, val loss: 1.4556574821472168
Epoch 700, training loss: 0.6727347373962402 = 0.02447238378226757 + 0.1 * 6.48262357711792
Epoch 700, val loss: 1.4652646780014038
Epoch 710, training loss: 0.6719183921813965 = 0.02326066792011261 + 0.1 * 6.48657751083374
Epoch 710, val loss: 1.4745171070098877
Epoch 720, training loss: 0.6700137853622437 = 0.02213851921260357 + 0.1 * 6.478752136230469
Epoch 720, val loss: 1.4835028648376465
Epoch 730, training loss: 0.6689676642417908 = 0.021098218858242035 + 0.1 * 6.478694438934326
Epoch 730, val loss: 1.492463231086731
Epoch 740, training loss: 0.6672816276550293 = 0.020129242911934853 + 0.1 * 6.471523761749268
Epoch 740, val loss: 1.5009573698043823
Epoch 750, training loss: 0.6658939719200134 = 0.019228264689445496 + 0.1 * 6.466656684875488
Epoch 750, val loss: 1.509218692779541
Epoch 760, training loss: 0.6653356552124023 = 0.018388574942946434 + 0.1 * 6.469470977783203
Epoch 760, val loss: 1.5174673795700073
Epoch 770, training loss: 0.6651731133460999 = 0.01760268025100231 + 0.1 * 6.475704193115234
Epoch 770, val loss: 1.5254085063934326
Epoch 780, training loss: 0.6639754772186279 = 0.016868555918335915 + 0.1 * 6.4710693359375
Epoch 780, val loss: 1.532983422279358
Epoch 790, training loss: 0.6636339426040649 = 0.01618213579058647 + 0.1 * 6.474518299102783
Epoch 790, val loss: 1.54054856300354
Epoch 800, training loss: 0.6617227792739868 = 0.015540217980742455 + 0.1 * 6.461825370788574
Epoch 800, val loss: 1.5478439331054688
Epoch 810, training loss: 0.6608417630195618 = 0.014937377534806728 + 0.1 * 6.459043502807617
Epoch 810, val loss: 1.5549687147140503
Epoch 820, training loss: 0.659587025642395 = 0.014369891956448555 + 0.1 * 6.452171325683594
Epoch 820, val loss: 1.5619144439697266
Epoch 830, training loss: 0.6596543192863464 = 0.013834966346621513 + 0.1 * 6.458193778991699
Epoch 830, val loss: 1.5685900449752808
Epoch 840, training loss: 0.6594179272651672 = 0.013332314789295197 + 0.1 * 6.460855960845947
Epoch 840, val loss: 1.5753449201583862
Epoch 850, training loss: 0.6575649976730347 = 0.012858915142714977 + 0.1 * 6.447060585021973
Epoch 850, val loss: 1.5817608833312988
Epoch 860, training loss: 0.6573724150657654 = 0.012412790209054947 + 0.1 * 6.449595928192139
Epoch 860, val loss: 1.588210940361023
Epoch 870, training loss: 0.6569978594779968 = 0.01198984868824482 + 0.1 * 6.450079917907715
Epoch 870, val loss: 1.594382405281067
Epoch 880, training loss: 0.6560108661651611 = 0.011589103378355503 + 0.1 * 6.444217681884766
Epoch 880, val loss: 1.6002689599990845
Epoch 890, training loss: 0.6565192937850952 = 0.011210364289581776 + 0.1 * 6.453088760375977
Epoch 890, val loss: 1.6062350273132324
Epoch 900, training loss: 0.6550905108451843 = 0.010852036066353321 + 0.1 * 6.442384719848633
Epoch 900, val loss: 1.6120432615280151
Epoch 910, training loss: 0.6543643474578857 = 0.010511576198041439 + 0.1 * 6.438527584075928
Epoch 910, val loss: 1.6177371740341187
Epoch 920, training loss: 0.6545132398605347 = 0.010188131593167782 + 0.1 * 6.443251132965088
Epoch 920, val loss: 1.6233067512512207
Epoch 930, training loss: 0.6537072062492371 = 0.009879792109131813 + 0.1 * 6.438274383544922
Epoch 930, val loss: 1.6285450458526611
Epoch 940, training loss: 0.6537812352180481 = 0.009587625041604042 + 0.1 * 6.4419355392456055
Epoch 940, val loss: 1.633946418762207
Epoch 950, training loss: 0.6524900197982788 = 0.009309337474405766 + 0.1 * 6.431806564331055
Epoch 950, val loss: 1.639303207397461
Epoch 960, training loss: 0.6541165113449097 = 0.009043634869158268 + 0.1 * 6.450728893280029
Epoch 960, val loss: 1.6443123817443848
Epoch 970, training loss: 0.653224527835846 = 0.008790194056928158 + 0.1 * 6.444343090057373
Epoch 970, val loss: 1.6492820978164673
Epoch 980, training loss: 0.652613639831543 = 0.008548379875719547 + 0.1 * 6.440652370452881
Epoch 980, val loss: 1.6541378498077393
Epoch 990, training loss: 0.6512594223022461 = 0.008318044245243073 + 0.1 * 6.429413318634033
Epoch 990, val loss: 1.6590325832366943
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8186610437532947
=== training gcn model ===
Epoch 0, training loss: 2.7963345050811768 = 1.9366511106491089 + 0.1 * 8.596834182739258
Epoch 0, val loss: 1.9388896226882935
Epoch 10, training loss: 2.787630558013916 = 1.927956461906433 + 0.1 * 8.596739768981934
Epoch 10, val loss: 1.9296565055847168
Epoch 20, training loss: 2.7770137786865234 = 1.9173892736434937 + 0.1 * 8.596245765686035
Epoch 20, val loss: 1.9181405305862427
Epoch 30, training loss: 2.7619709968566895 = 1.9027721881866455 + 0.1 * 8.591986656188965
Epoch 30, val loss: 1.9019601345062256
Epoch 40, training loss: 2.7369155883789062 = 1.8812503814697266 + 0.1 * 8.556653022766113
Epoch 40, val loss: 1.8780920505523682
Epoch 50, training loss: 2.6836061477661133 = 1.851701021194458 + 0.1 * 8.319049835205078
Epoch 50, val loss: 1.8466522693634033
Epoch 60, training loss: 2.6168854236602783 = 1.8198240995407104 + 0.1 * 7.9706130027771
Epoch 60, val loss: 1.8153976202011108
Epoch 70, training loss: 2.5408570766448975 = 1.7917709350585938 + 0.1 * 7.490860462188721
Epoch 70, val loss: 1.7899401187896729
Epoch 80, training loss: 2.4787750244140625 = 1.7641565799713135 + 0.1 * 7.146183967590332
Epoch 80, val loss: 1.766431450843811
Epoch 90, training loss: 2.43025541305542 = 1.7295774221420288 + 0.1 * 7.00678014755249
Epoch 90, val loss: 1.7371326684951782
Epoch 100, training loss: 2.3767971992492676 = 1.6807153224945068 + 0.1 * 6.960819244384766
Epoch 100, val loss: 1.6938291788101196
Epoch 110, training loss: 2.3090248107910156 = 1.6152585744857788 + 0.1 * 6.937663555145264
Epoch 110, val loss: 1.6369870901107788
Epoch 120, training loss: 2.229140281677246 = 1.5370442867279053 + 0.1 * 6.920960426330566
Epoch 120, val loss: 1.5739479064941406
Epoch 130, training loss: 2.144479513168335 = 1.4536004066467285 + 0.1 * 6.908790111541748
Epoch 130, val loss: 1.5105775594711304
Epoch 140, training loss: 2.058391571044922 = 1.3686082363128662 + 0.1 * 6.897832870483398
Epoch 140, val loss: 1.4494324922561646
Epoch 150, training loss: 1.971838116645813 = 1.2833001613616943 + 0.1 * 6.885379314422607
Epoch 150, val loss: 1.3907194137573242
Epoch 160, training loss: 1.8842473030090332 = 1.1970722675323486 + 0.1 * 6.8717498779296875
Epoch 160, val loss: 1.332706093788147
Epoch 170, training loss: 1.7960901260375977 = 1.1097257137298584 + 0.1 * 6.863643169403076
Epoch 170, val loss: 1.2749875783920288
Epoch 180, training loss: 1.7090728282928467 = 1.024560809135437 + 0.1 * 6.845119476318359
Epoch 180, val loss: 1.2195661067962646
Epoch 190, training loss: 1.6258556842803955 = 0.9429253339767456 + 0.1 * 6.82930326461792
Epoch 190, val loss: 1.166948914527893
Epoch 200, training loss: 1.548696756362915 = 0.8670386672019958 + 0.1 * 6.8165812492370605
Epoch 200, val loss: 1.118862271308899
Epoch 210, training loss: 1.479057788848877 = 0.7985485792160034 + 0.1 * 6.805091381072998
Epoch 210, val loss: 1.077324628829956
Epoch 220, training loss: 1.4171664714813232 = 0.7369294762611389 + 0.1 * 6.802369117736816
Epoch 220, val loss: 1.0425184965133667
Epoch 230, training loss: 1.360430121421814 = 0.6818268895149231 + 0.1 * 6.786032199859619
Epoch 230, val loss: 1.014466404914856
Epoch 240, training loss: 1.3091928958892822 = 0.6313624382019043 + 0.1 * 6.778304100036621
Epoch 240, val loss: 0.9916931986808777
Epoch 250, training loss: 1.2620201110839844 = 0.5848922729492188 + 0.1 * 6.771277904510498
Epoch 250, val loss: 0.9735726714134216
Epoch 260, training loss: 1.2179441452026367 = 0.541580080986023 + 0.1 * 6.763641357421875
Epoch 260, val loss: 0.9593281149864197
Epoch 270, training loss: 1.1773871183395386 = 0.5007966160774231 + 0.1 * 6.765904903411865
Epoch 270, val loss: 0.9485452175140381
Epoch 280, training loss: 1.1386125087738037 = 0.46277090907096863 + 0.1 * 6.758416175842285
Epoch 280, val loss: 0.9410885572433472
Epoch 290, training loss: 1.1017067432403564 = 0.4270498752593994 + 0.1 * 6.74656867980957
Epoch 290, val loss: 0.9367374777793884
Epoch 300, training loss: 1.0674679279327393 = 0.3931671380996704 + 0.1 * 6.743008136749268
Epoch 300, val loss: 0.9350776672363281
Epoch 310, training loss: 1.0352057218551636 = 0.36096885800361633 + 0.1 * 6.742368221282959
Epoch 310, val loss: 0.9355989694595337
Epoch 320, training loss: 1.0036441087722778 = 0.33018210530281067 + 0.1 * 6.734620094299316
Epoch 320, val loss: 0.9378183484077454
Epoch 330, training loss: 0.9736447930335999 = 0.30058372020721436 + 0.1 * 6.7306108474731445
Epoch 330, val loss: 0.9414751529693604
Epoch 340, training loss: 0.9443297982215881 = 0.27199727296829224 + 0.1 * 6.723325252532959
Epoch 340, val loss: 0.946548581123352
Epoch 350, training loss: 0.9165593981742859 = 0.24443626403808594 + 0.1 * 6.721230983734131
Epoch 350, val loss: 0.9529545307159424
Epoch 360, training loss: 0.8908333778381348 = 0.21814408898353577 + 0.1 * 6.726892471313477
Epoch 360, val loss: 0.9605070948600769
Epoch 370, training loss: 0.8642028570175171 = 0.1934783160686493 + 0.1 * 6.707245349884033
Epoch 370, val loss: 0.96915203332901
Epoch 380, training loss: 0.8417076468467712 = 0.17077206075191498 + 0.1 * 6.70935583114624
Epoch 380, val loss: 0.978981077671051
Epoch 390, training loss: 0.8201818466186523 = 0.15039293467998505 + 0.1 * 6.6978888511657715
Epoch 390, val loss: 0.9897317290306091
Epoch 400, training loss: 0.8013461232185364 = 0.1324269026517868 + 0.1 * 6.689191818237305
Epoch 400, val loss: 1.0014135837554932
Epoch 410, training loss: 0.7849878072738647 = 0.11683745682239532 + 0.1 * 6.6815032958984375
Epoch 410, val loss: 1.0138338804244995
Epoch 420, training loss: 0.7717263102531433 = 0.10341097414493561 + 0.1 * 6.6831536293029785
Epoch 420, val loss: 1.0269241333007812
Epoch 430, training loss: 0.7587781548500061 = 0.0919354036450386 + 0.1 * 6.668427467346191
Epoch 430, val loss: 1.0403060913085938
Epoch 440, training loss: 0.7503378987312317 = 0.08213581889867783 + 0.1 * 6.682021141052246
Epoch 440, val loss: 1.0539478063583374
Epoch 450, training loss: 0.7389849424362183 = 0.07378865033388138 + 0.1 * 6.651962757110596
Epoch 450, val loss: 1.0671998262405396
Epoch 460, training loss: 0.7311022281646729 = 0.06657534092664719 + 0.1 * 6.64526891708374
Epoch 460, val loss: 1.0805237293243408
Epoch 470, training loss: 0.7262104153633118 = 0.060303546488285065 + 0.1 * 6.659068584442139
Epoch 470, val loss: 1.093875527381897
Epoch 480, training loss: 0.7182549834251404 = 0.05486216023564339 + 0.1 * 6.633928298950195
Epoch 480, val loss: 1.1069382429122925
Epoch 490, training loss: 0.7129720449447632 = 0.05010616406798363 + 0.1 * 6.628658771514893
Epoch 490, val loss: 1.1197922229766846
Epoch 500, training loss: 0.7085127830505371 = 0.0459337942302227 + 0.1 * 6.625790119171143
Epoch 500, val loss: 1.1324167251586914
Epoch 510, training loss: 0.7037497162818909 = 0.04226423427462578 + 0.1 * 6.61485481262207
Epoch 510, val loss: 1.1446559429168701
Epoch 520, training loss: 0.7019444108009338 = 0.03901063650846481 + 0.1 * 6.629337310791016
Epoch 520, val loss: 1.1567705869674683
Epoch 530, training loss: 0.6964900493621826 = 0.03612988442182541 + 0.1 * 6.603601455688477
Epoch 530, val loss: 1.1685214042663574
Epoch 540, training loss: 0.6942042708396912 = 0.03356124088168144 + 0.1 * 6.606430530548096
Epoch 540, val loss: 1.1801377534866333
Epoch 550, training loss: 0.690788745880127 = 0.03126661106944084 + 0.1 * 6.595221042633057
Epoch 550, val loss: 1.1912988424301147
Epoch 560, training loss: 0.6890758872032166 = 0.029206395149230957 + 0.1 * 6.598694801330566
Epoch 560, val loss: 1.2023160457611084
Epoch 570, training loss: 0.6863208413124084 = 0.027356361970305443 + 0.1 * 6.589644432067871
Epoch 570, val loss: 1.2128193378448486
Epoch 580, training loss: 0.6848703622817993 = 0.025685755535960197 + 0.1 * 6.591845989227295
Epoch 580, val loss: 1.2232359647750854
Epoch 590, training loss: 0.6818711161613464 = 0.024176770821213722 + 0.1 * 6.576943397521973
Epoch 590, val loss: 1.2330551147460938
Epoch 600, training loss: 0.6815605163574219 = 0.022801265120506287 + 0.1 * 6.587592124938965
Epoch 600, val loss: 1.2428491115570068
Epoch 610, training loss: 0.6791968941688538 = 0.021548986434936523 + 0.1 * 6.576478958129883
Epoch 610, val loss: 1.2522499561309814
Epoch 620, training loss: 0.6780432462692261 = 0.020404482260346413 + 0.1 * 6.576387882232666
Epoch 620, val loss: 1.2615509033203125
Epoch 630, training loss: 0.6754735708236694 = 0.019356688484549522 + 0.1 * 6.561169147491455
Epoch 630, val loss: 1.2704216241836548
Epoch 640, training loss: 0.6751276850700378 = 0.01839185506105423 + 0.1 * 6.567358016967773
Epoch 640, val loss: 1.279229760169983
Epoch 650, training loss: 0.6732259392738342 = 0.017502203583717346 + 0.1 * 6.55723762512207
Epoch 650, val loss: 1.287846326828003
Epoch 660, training loss: 0.6739053726196289 = 0.01668359525501728 + 0.1 * 6.57221794128418
Epoch 660, val loss: 1.2960491180419922
Epoch 670, training loss: 0.6715610027313232 = 0.01592576690018177 + 0.1 * 6.556352138519287
Epoch 670, val loss: 1.3041210174560547
Epoch 680, training loss: 0.6702025532722473 = 0.015223355032503605 + 0.1 * 6.54979133605957
Epoch 680, val loss: 1.3119258880615234
Epoch 690, training loss: 0.6700190901756287 = 0.014568107202649117 + 0.1 * 6.554510116577148
Epoch 690, val loss: 1.3197848796844482
Epoch 700, training loss: 0.6695415377616882 = 0.01396121270954609 + 0.1 * 6.555803298950195
Epoch 700, val loss: 1.3271863460540771
Epoch 710, training loss: 0.6667312383651733 = 0.01339487824589014 + 0.1 * 6.533363342285156
Epoch 710, val loss: 1.3345234394073486
Epoch 720, training loss: 0.6663951873779297 = 0.012865503318607807 + 0.1 * 6.53529691696167
Epoch 720, val loss: 1.3416305780410767
Epoch 730, training loss: 0.666887104511261 = 0.012368389405310154 + 0.1 * 6.545186996459961
Epoch 730, val loss: 1.3487188816070557
Epoch 740, training loss: 0.6650547385215759 = 0.011902383528649807 + 0.1 * 6.53152322769165
Epoch 740, val loss: 1.3556132316589355
Epoch 750, training loss: 0.6647116541862488 = 0.011465098708868027 + 0.1 * 6.532465934753418
Epoch 750, val loss: 1.3623368740081787
Epoch 760, training loss: 0.6640352010726929 = 0.011053958907723427 + 0.1 * 6.529811859130859
Epoch 760, val loss: 1.36899995803833
Epoch 770, training loss: 0.6627770066261292 = 0.01066839974373579 + 0.1 * 6.5210862159729
Epoch 770, val loss: 1.3753454685211182
Epoch 780, training loss: 0.6622774004936218 = 0.010303503833711147 + 0.1 * 6.519738674163818
Epoch 780, val loss: 1.3816035985946655
Epoch 790, training loss: 0.6620897650718689 = 0.009957781992852688 + 0.1 * 6.52131986618042
Epoch 790, val loss: 1.387954831123352
Epoch 800, training loss: 0.6615668535232544 = 0.009632113389670849 + 0.1 * 6.519347190856934
Epoch 800, val loss: 1.3939911127090454
Epoch 810, training loss: 0.66072016954422 = 0.009324682876467705 + 0.1 * 6.5139546394348145
Epoch 810, val loss: 1.3999656438827515
Epoch 820, training loss: 0.6595011949539185 = 0.009034560061991215 + 0.1 * 6.504666328430176
Epoch 820, val loss: 1.4056353569030762
Epoch 830, training loss: 0.6595498323440552 = 0.008758028037846088 + 0.1 * 6.507917881011963
Epoch 830, val loss: 1.411350965499878
Epoch 840, training loss: 0.6603630185127258 = 0.008494649082422256 + 0.1 * 6.518683433532715
Epoch 840, val loss: 1.4171011447906494
Epoch 850, training loss: 0.6598766446113586 = 0.008245446719229221 + 0.1 * 6.5163116455078125
Epoch 850, val loss: 1.4224956035614014
Epoch 860, training loss: 0.6581434011459351 = 0.008007836528122425 + 0.1 * 6.5013556480407715
Epoch 860, val loss: 1.4278916120529175
Epoch 870, training loss: 0.6585795283317566 = 0.007782433647662401 + 0.1 * 6.507970809936523
Epoch 870, val loss: 1.4331786632537842
Epoch 880, training loss: 0.6569298505783081 = 0.007567838300019503 + 0.1 * 6.493619918823242
Epoch 880, val loss: 1.438222885131836
Epoch 890, training loss: 0.6581344604492188 = 0.007362454663962126 + 0.1 * 6.507719993591309
Epoch 890, val loss: 1.4432990550994873
Epoch 900, training loss: 0.6573159694671631 = 0.007166511379182339 + 0.1 * 6.501494407653809
Epoch 900, val loss: 1.4483455419540405
Epoch 910, training loss: 0.6564837694168091 = 0.006979984696954489 + 0.1 * 6.49503755569458
Epoch 910, val loss: 1.4531058073043823
Epoch 920, training loss: 0.6557928919792175 = 0.0068014697171747684 + 0.1 * 6.4899139404296875
Epoch 920, val loss: 1.4578120708465576
Epoch 930, training loss: 0.6553152799606323 = 0.006630779709666967 + 0.1 * 6.486845016479492
Epoch 930, val loss: 1.4624791145324707
Epoch 940, training loss: 0.6569898724555969 = 0.006467394530773163 + 0.1 * 6.505224704742432
Epoch 940, val loss: 1.4670264720916748
Epoch 950, training loss: 0.6549571752548218 = 0.006310512777417898 + 0.1 * 6.486466407775879
Epoch 950, val loss: 1.4715511798858643
Epoch 960, training loss: 0.6556602716445923 = 0.006160097196698189 + 0.1 * 6.495001792907715
Epoch 960, val loss: 1.4760137796401978
Epoch 970, training loss: 0.6549518704414368 = 0.0060160220600664616 + 0.1 * 6.489358425140381
Epoch 970, val loss: 1.4803651571273804
Epoch 980, training loss: 0.6546423435211182 = 0.005877816583961248 + 0.1 * 6.487645626068115
Epoch 980, val loss: 1.4846141338348389
Epoch 990, training loss: 0.6542062163352966 = 0.005744991824030876 + 0.1 * 6.484611988067627
Epoch 990, val loss: 1.4888553619384766
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 2.8086822032928467 = 1.9489989280700684 + 0.1 * 8.596833229064941
Epoch 0, val loss: 1.9527153968811035
Epoch 10, training loss: 2.798764944076538 = 1.9390884637832642 + 0.1 * 8.59676456451416
Epoch 10, val loss: 1.9430042505264282
Epoch 20, training loss: 2.786731481552124 = 1.9270944595336914 + 0.1 * 8.596369743347168
Epoch 20, val loss: 1.930914282798767
Epoch 30, training loss: 2.769700765609741 = 1.9104094505310059 + 0.1 * 8.592913627624512
Epoch 30, val loss: 1.9138059616088867
Epoch 40, training loss: 2.7424139976501465 = 1.8860417604446411 + 0.1 * 8.56372356414795
Epoch 40, val loss: 1.8892349004745483
Epoch 50, training loss: 2.690727949142456 = 1.853119134902954 + 0.1 * 8.37608814239502
Epoch 50, val loss: 1.85768723487854
Epoch 60, training loss: 2.627826452255249 = 1.8186208009719849 + 0.1 * 8.092056274414062
Epoch 60, val loss: 1.826717734336853
Epoch 70, training loss: 2.550870180130005 = 1.7888922691345215 + 0.1 * 7.619779586791992
Epoch 70, val loss: 1.7989299297332764
Epoch 80, training loss: 2.4826741218566895 = 1.75892174243927 + 0.1 * 7.237523078918457
Epoch 80, val loss: 1.769655466079712
Epoch 90, training loss: 2.4304559230804443 = 1.7208489179611206 + 0.1 * 7.096070766448975
Epoch 90, val loss: 1.7343202829360962
Epoch 100, training loss: 2.3732566833496094 = 1.6690703630447388 + 0.1 * 7.041864395141602
Epoch 100, val loss: 1.6887258291244507
Epoch 110, training loss: 2.3006391525268555 = 1.60066819190979 + 0.1 * 6.999709129333496
Epoch 110, val loss: 1.629998803138733
Epoch 120, training loss: 2.21372127532959 = 1.5164116621017456 + 0.1 * 6.97309684753418
Epoch 120, val loss: 1.5606921911239624
Epoch 130, training loss: 2.117941379547119 = 1.4220296144485474 + 0.1 * 6.9591169357299805
Epoch 130, val loss: 1.4863444566726685
Epoch 140, training loss: 2.0179858207702637 = 1.3230494260787964 + 0.1 * 6.949362754821777
Epoch 140, val loss: 1.412469744682312
Epoch 150, training loss: 1.9166481494903564 = 1.2225267887115479 + 0.1 * 6.941214084625244
Epoch 150, val loss: 1.3399983644485474
Epoch 160, training loss: 1.8185288906097412 = 1.1255568265914917 + 0.1 * 6.929720401763916
Epoch 160, val loss: 1.2730590105056763
Epoch 170, training loss: 1.7260050773620605 = 1.0344606637954712 + 0.1 * 6.915444374084473
Epoch 170, val loss: 1.2122421264648438
Epoch 180, training loss: 1.6408865451812744 = 0.9505370259284973 + 0.1 * 6.903494834899902
Epoch 180, val loss: 1.1579357385635376
Epoch 190, training loss: 1.5619778633117676 = 0.873378336429596 + 0.1 * 6.885995864868164
Epoch 190, val loss: 1.1087969541549683
Epoch 200, training loss: 1.48928701877594 = 0.8020011782646179 + 0.1 * 6.87285852432251
Epoch 200, val loss: 1.0638375282287598
Epoch 210, training loss: 1.4217417240142822 = 0.7358564138412476 + 0.1 * 6.858852863311768
Epoch 210, val loss: 1.0225855112075806
Epoch 220, training loss: 1.358892560005188 = 0.6742583513259888 + 0.1 * 6.846342086791992
Epoch 220, val loss: 0.9847785234451294
Epoch 230, training loss: 1.3005151748657227 = 0.6170057058334351 + 0.1 * 6.835094928741455
Epoch 230, val loss: 0.9509524703025818
Epoch 240, training loss: 1.2456982135772705 = 0.5632269382476807 + 0.1 * 6.824711799621582
Epoch 240, val loss: 0.9212188720703125
Epoch 250, training loss: 1.1951779127120972 = 0.5130340456962585 + 0.1 * 6.821438789367676
Epoch 250, val loss: 0.8958856463432312
Epoch 260, training loss: 1.1458849906921387 = 0.4663704037666321 + 0.1 * 6.795145511627197
Epoch 260, val loss: 0.8747473955154419
Epoch 270, training loss: 1.101697325706482 = 0.42269399762153625 + 0.1 * 6.790033340454102
Epoch 270, val loss: 0.8574681878089905
Epoch 280, training loss: 1.061699390411377 = 0.3825590908527374 + 0.1 * 6.791403293609619
Epoch 280, val loss: 0.8444268703460693
Epoch 290, training loss: 1.0227797031402588 = 0.3460061550140381 + 0.1 * 6.767735004425049
Epoch 290, val loss: 0.835331916809082
Epoch 300, training loss: 0.9878617525100708 = 0.31257572770118713 + 0.1 * 6.7528605461120605
Epoch 300, val loss: 0.8299828171730042
Epoch 310, training loss: 0.9573739171028137 = 0.28230684995651245 + 0.1 * 6.750670433044434
Epoch 310, val loss: 0.8280283808708191
Epoch 320, training loss: 0.9280709624290466 = 0.2550455927848816 + 0.1 * 6.73025369644165
Epoch 320, val loss: 0.8291546702384949
Epoch 330, training loss: 0.9050919413566589 = 0.2304166704416275 + 0.1 * 6.746752738952637
Epoch 330, val loss: 0.8329449892044067
Epoch 340, training loss: 0.8799717426300049 = 0.20838035643100739 + 0.1 * 6.715913772583008
Epoch 340, val loss: 0.8390322327613831
Epoch 350, training loss: 0.8609896302223206 = 0.18852360546588898 + 0.1 * 6.724660396575928
Epoch 350, val loss: 0.8469757437705994
Epoch 360, training loss: 0.8408331871032715 = 0.17074736952781677 + 0.1 * 6.7008585929870605
Epoch 360, val loss: 0.8566586375236511
Epoch 370, training loss: 0.8270458579063416 = 0.15480546653270721 + 0.1 * 6.722403526306152
Epoch 370, val loss: 0.8676789999008179
Epoch 380, training loss: 0.8098130822181702 = 0.1405773162841797 + 0.1 * 6.692357540130615
Epoch 380, val loss: 0.8797966241836548
Epoch 390, training loss: 0.7964780330657959 = 0.12780609726905823 + 0.1 * 6.6867194175720215
Epoch 390, val loss: 0.8927407264709473
Epoch 400, training loss: 0.7850143313407898 = 0.11638233810663223 + 0.1 * 6.686319828033447
Epoch 400, val loss: 0.9064225554466248
Epoch 410, training loss: 0.773739755153656 = 0.10618211328983307 + 0.1 * 6.675576210021973
Epoch 410, val loss: 0.920617401599884
Epoch 420, training loss: 0.7635794878005981 = 0.09703175723552704 + 0.1 * 6.665477275848389
Epoch 420, val loss: 0.9352567195892334
Epoch 430, training loss: 0.7543004155158997 = 0.08883567899465561 + 0.1 * 6.654647350311279
Epoch 430, val loss: 0.9501152038574219
Epoch 440, training loss: 0.7468401789665222 = 0.08147476613521576 + 0.1 * 6.653654098510742
Epoch 440, val loss: 0.9651820659637451
Epoch 450, training loss: 0.7395243644714355 = 0.0748738944530487 + 0.1 * 6.6465044021606445
Epoch 450, val loss: 0.9803021550178528
Epoch 460, training loss: 0.7326977849006653 = 0.06895668804645538 + 0.1 * 6.637411117553711
Epoch 460, val loss: 0.9954115152359009
Epoch 470, training loss: 0.7266480326652527 = 0.06361784785985947 + 0.1 * 6.6303019523620605
Epoch 470, val loss: 1.0105667114257812
Epoch 480, training loss: 0.7224482893943787 = 0.0588047169148922 + 0.1 * 6.6364359855651855
Epoch 480, val loss: 1.0254461765289307
Epoch 490, training loss: 0.7172240614891052 = 0.05447928607463837 + 0.1 * 6.627447605133057
Epoch 490, val loss: 1.0403016805648804
Epoch 500, training loss: 0.7132701873779297 = 0.05056200176477432 + 0.1 * 6.627081394195557
Epoch 500, val loss: 1.0547960996627808
Epoch 510, training loss: 0.7110203504562378 = 0.04701746255159378 + 0.1 * 6.640028953552246
Epoch 510, val loss: 1.0692247152328491
Epoch 520, training loss: 0.7047985196113586 = 0.04381037876009941 + 0.1 * 6.609881401062012
Epoch 520, val loss: 1.0832685232162476
Epoch 530, training loss: 0.702299952507019 = 0.04089409112930298 + 0.1 * 6.614058494567871
Epoch 530, val loss: 1.0970385074615479
Epoch 540, training loss: 0.6979047060012817 = 0.038242436945438385 + 0.1 * 6.596622467041016
Epoch 540, val loss: 1.110667109489441
Epoch 550, training loss: 0.6965864896774292 = 0.03582193702459335 + 0.1 * 6.607645511627197
Epoch 550, val loss: 1.1238397359848022
Epoch 560, training loss: 0.6932470798492432 = 0.03361675888299942 + 0.1 * 6.5963029861450195
Epoch 560, val loss: 1.1369787454605103
Epoch 570, training loss: 0.694978654384613 = 0.03159981593489647 + 0.1 * 6.633788108825684
Epoch 570, val loss: 1.1495240926742554
Epoch 580, training loss: 0.6887797713279724 = 0.029760301113128662 + 0.1 * 6.5901947021484375
Epoch 580, val loss: 1.161994218826294
Epoch 590, training loss: 0.6869800686836243 = 0.028071962296962738 + 0.1 * 6.589081287384033
Epoch 590, val loss: 1.1739733219146729
Epoch 600, training loss: 0.6845496296882629 = 0.026521243155002594 + 0.1 * 6.5802836418151855
Epoch 600, val loss: 1.1857510805130005
Epoch 610, training loss: 0.6837459206581116 = 0.025093547999858856 + 0.1 * 6.586523532867432
Epoch 610, val loss: 1.1972262859344482
Epoch 620, training loss: 0.6820098757743835 = 0.023779364302754402 + 0.1 * 6.582304954528809
Epoch 620, val loss: 1.2084331512451172
Epoch 630, training loss: 0.6797072291374207 = 0.0225667841732502 + 0.1 * 6.571404457092285
Epoch 630, val loss: 1.2193660736083984
Epoch 640, training loss: 0.6813805103302002 = 0.02144279144704342 + 0.1 * 6.599377155303955
Epoch 640, val loss: 1.2299716472625732
Epoch 650, training loss: 0.6772112250328064 = 0.020405134186148643 + 0.1 * 6.568060874938965
Epoch 650, val loss: 1.2403745651245117
Epoch 660, training loss: 0.6751292943954468 = 0.019443245604634285 + 0.1 * 6.556859970092773
Epoch 660, val loss: 1.2505238056182861
Epoch 670, training loss: 0.6752572059631348 = 0.01854828931391239 + 0.1 * 6.567088603973389
Epoch 670, val loss: 1.2602884769439697
Epoch 680, training loss: 0.6733334064483643 = 0.01771722547709942 + 0.1 * 6.556161403656006
Epoch 680, val loss: 1.2700129747390747
Epoch 690, training loss: 0.6721434593200684 = 0.016942590475082397 + 0.1 * 6.552008628845215
Epoch 690, val loss: 1.2793265581130981
Epoch 700, training loss: 0.6713687181472778 = 0.0162203386425972 + 0.1 * 6.551483631134033
Epoch 700, val loss: 1.2885693311691284
Epoch 710, training loss: 0.6708430051803589 = 0.015544166788458824 + 0.1 * 6.552988052368164
Epoch 710, val loss: 1.2975518703460693
Epoch 720, training loss: 0.6691684722900391 = 0.014911681413650513 + 0.1 * 6.542567729949951
Epoch 720, val loss: 1.3061718940734863
Epoch 730, training loss: 0.6704474687576294 = 0.014320354908704758 + 0.1 * 6.561270713806152
Epoch 730, val loss: 1.314802885055542
Epoch 740, training loss: 0.666844367980957 = 0.013765520416200161 + 0.1 * 6.530788421630859
Epoch 740, val loss: 1.323096752166748
Epoch 750, training loss: 0.6700391173362732 = 0.013244002126157284 + 0.1 * 6.567951202392578
Epoch 750, val loss: 1.3311843872070312
Epoch 760, training loss: 0.6662182807922363 = 0.0127552580088377 + 0.1 * 6.534629821777344
Epoch 760, val loss: 1.339131236076355
Epoch 770, training loss: 0.6667441129684448 = 0.012294447049498558 + 0.1 * 6.544496059417725
Epoch 770, val loss: 1.346955418586731
Epoch 780, training loss: 0.6642452478408813 = 0.011858980171382427 + 0.1 * 6.523862361907959
Epoch 780, val loss: 1.3544732332229614
Epoch 790, training loss: 0.6651103496551514 = 0.011448752135038376 + 0.1 * 6.53661584854126
Epoch 790, val loss: 1.3620295524597168
Epoch 800, training loss: 0.6624129414558411 = 0.011059907265007496 + 0.1 * 6.513530254364014
Epoch 800, val loss: 1.3691414594650269
Epoch 810, training loss: 0.663966178894043 = 0.010693162679672241 + 0.1 * 6.532729625701904
Epoch 810, val loss: 1.3763344287872314
Epoch 820, training loss: 0.6620350480079651 = 0.010346904397010803 + 0.1 * 6.516881465911865
Epoch 820, val loss: 1.3832252025604248
Epoch 830, training loss: 0.6598690748214722 = 0.010018502362072468 + 0.1 * 6.498505592346191
Epoch 830, val loss: 1.3901450634002686
Epoch 840, training loss: 0.661970317363739 = 0.00970589928328991 + 0.1 * 6.52264404296875
Epoch 840, val loss: 1.3967119455337524
Epoch 850, training loss: 0.6591992378234863 = 0.009409952908754349 + 0.1 * 6.497892379760742
Epoch 850, val loss: 1.4032447338104248
Epoch 860, training loss: 0.6595175862312317 = 0.009128022007644176 + 0.1 * 6.503895282745361
Epoch 860, val loss: 1.4098081588745117
Epoch 870, training loss: 0.6588090062141418 = 0.008859656751155853 + 0.1 * 6.499493598937988
Epoch 870, val loss: 1.4159151315689087
Epoch 880, training loss: 0.6581379771232605 = 0.008604738861322403 + 0.1 * 6.495331764221191
Epoch 880, val loss: 1.4221807718276978
Epoch 890, training loss: 0.6585127711296082 = 0.008361329324543476 + 0.1 * 6.501514434814453
Epoch 890, val loss: 1.4281672239303589
Epoch 900, training loss: 0.6566848754882812 = 0.008129620924592018 + 0.1 * 6.485552787780762
Epoch 900, val loss: 1.4340792894363403
Epoch 910, training loss: 0.6584349870681763 = 0.007908040657639503 + 0.1 * 6.5052690505981445
Epoch 910, val loss: 1.4399603605270386
Epoch 920, training loss: 0.6570691466331482 = 0.007695908658206463 + 0.1 * 6.493732452392578
Epoch 920, val loss: 1.4455790519714355
Epoch 930, training loss: 0.6575042605400085 = 0.007493746932595968 + 0.1 * 6.500105381011963
Epoch 930, val loss: 1.4511525630950928
Epoch 940, training loss: 0.6566174030303955 = 0.007300246972590685 + 0.1 * 6.493171215057373
Epoch 940, val loss: 1.4566423892974854
Epoch 950, training loss: 0.655644416809082 = 0.007115406449884176 + 0.1 * 6.485289573669434
Epoch 950, val loss: 1.4620901346206665
Epoch 960, training loss: 0.6556974053382874 = 0.006937774363905191 + 0.1 * 6.487596035003662
Epoch 960, val loss: 1.467401146888733
Epoch 970, training loss: 0.6543557643890381 = 0.006767445243895054 + 0.1 * 6.4758830070495605
Epoch 970, val loss: 1.4725570678710938
Epoch 980, training loss: 0.6557285189628601 = 0.006604225840419531 + 0.1 * 6.491242408752441
Epoch 980, val loss: 1.4777300357818604
Epoch 990, training loss: 0.6554939150810242 = 0.006447651423513889 + 0.1 * 6.490462303161621
Epoch 990, val loss: 1.4827545881271362
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8170795993674222
The final CL Acc:0.76420, 0.03029, The final GNN Acc:0.81726, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13218])
remove edge: torch.Size([2, 7940])
updated graph: torch.Size([2, 10602])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.83313250541687 = 1.9734495878219604 + 0.1 * 8.59682846069336
Epoch 0, val loss: 1.9814114570617676
Epoch 10, training loss: 2.8219077587127686 = 1.9622327089309692 + 0.1 * 8.596750259399414
Epoch 10, val loss: 1.970474123954773
Epoch 20, training loss: 2.8080501556396484 = 1.9484233856201172 + 0.1 * 8.596266746520996
Epoch 20, val loss: 1.9564330577850342
Epoch 30, training loss: 2.7883574962615967 = 1.929116129875183 + 0.1 * 8.592412948608398
Epoch 30, val loss: 1.9364086389541626
Epoch 40, training loss: 2.757086992263794 = 1.900470495223999 + 0.1 * 8.56616497039795
Epoch 40, val loss: 1.9066944122314453
Epoch 50, training loss: 2.704029083251953 = 1.8601641654968262 + 0.1 * 8.438650131225586
Epoch 50, val loss: 1.8663382530212402
Epoch 60, training loss: 2.6304128170013428 = 1.8153084516525269 + 0.1 * 8.151043891906738
Epoch 60, val loss: 1.824500560760498
Epoch 70, training loss: 2.565338134765625 = 1.7781651020050049 + 0.1 * 7.871731281280518
Epoch 70, val loss: 1.79124915599823
Epoch 80, training loss: 2.4905762672424316 = 1.7445425987243652 + 0.1 * 7.4603352546691895
Epoch 80, val loss: 1.7591075897216797
Epoch 90, training loss: 2.4253408908843994 = 1.7028077840805054 + 0.1 * 7.225331783294678
Epoch 90, val loss: 1.7202812433242798
Epoch 100, training loss: 2.358726739883423 = 1.6488937139511108 + 0.1 * 7.098329544067383
Epoch 100, val loss: 1.6728001832962036
Epoch 110, training loss: 2.283534526824951 = 1.5788651704788208 + 0.1 * 7.046693325042725
Epoch 110, val loss: 1.6123731136322021
Epoch 120, training loss: 2.1974122524261475 = 1.4958292245864868 + 0.1 * 7.015829563140869
Epoch 120, val loss: 1.540282130241394
Epoch 130, training loss: 2.105893850326538 = 1.406949758529663 + 0.1 * 6.98944091796875
Epoch 130, val loss: 1.465430498123169
Epoch 140, training loss: 2.0127451419830322 = 1.3160786628723145 + 0.1 * 6.966665267944336
Epoch 140, val loss: 1.390464186668396
Epoch 150, training loss: 1.915665864944458 = 1.2207547426223755 + 0.1 * 6.949110507965088
Epoch 150, val loss: 1.3117108345031738
Epoch 160, training loss: 1.811910629272461 = 1.118381381034851 + 0.1 * 6.935292720794678
Epoch 160, val loss: 1.2262204885482788
Epoch 170, training loss: 1.7036553621292114 = 1.0109063386917114 + 0.1 * 6.927490234375
Epoch 170, val loss: 1.1360288858413696
Epoch 180, training loss: 1.598219871520996 = 0.9067240953445435 + 0.1 * 6.9149580001831055
Epoch 180, val loss: 1.0486962795257568
Epoch 190, training loss: 1.502221941947937 = 0.8121586441993713 + 0.1 * 6.900632858276367
Epoch 190, val loss: 0.9700276851654053
Epoch 200, training loss: 1.4193966388702393 = 0.7304397225379944 + 0.1 * 6.8895697593688965
Epoch 200, val loss: 0.9041905403137207
Epoch 210, training loss: 1.348428726196289 = 0.661356508731842 + 0.1 * 6.870721340179443
Epoch 210, val loss: 0.8514727354049683
Epoch 220, training loss: 1.2861695289611816 = 0.6015074253082275 + 0.1 * 6.846620559692383
Epoch 220, val loss: 0.8094306588172913
Epoch 230, training loss: 1.2326624393463135 = 0.5478554964065552 + 0.1 * 6.84807014465332
Epoch 230, val loss: 0.7751837372779846
Epoch 240, training loss: 1.181269645690918 = 0.499676376581192 + 0.1 * 6.815932750701904
Epoch 240, val loss: 0.7472282648086548
Epoch 250, training loss: 1.1351752281188965 = 0.455409437417984 + 0.1 * 6.797657489776611
Epoch 250, val loss: 0.7238374948501587
Epoch 260, training loss: 1.0924322605133057 = 0.41457706689834595 + 0.1 * 6.77855110168457
Epoch 260, val loss: 0.7045355439186096
Epoch 270, training loss: 1.052991509437561 = 0.3763836920261383 + 0.1 * 6.766077995300293
Epoch 270, val loss: 0.6885865330696106
Epoch 280, training loss: 1.0162380933761597 = 0.3406643271446228 + 0.1 * 6.7557373046875
Epoch 280, val loss: 0.6756353974342346
Epoch 290, training loss: 0.9815310835838318 = 0.30712950229644775 + 0.1 * 6.744015693664551
Epoch 290, val loss: 0.6652571558952332
Epoch 300, training loss: 0.9494938254356384 = 0.27559894323349 + 0.1 * 6.738948822021484
Epoch 300, val loss: 0.6571239233016968
Epoch 310, training loss: 0.9187422394752502 = 0.2462642788887024 + 0.1 * 6.7247796058654785
Epoch 310, val loss: 0.6511802077293396
Epoch 320, training loss: 0.8921626210212708 = 0.21929557621479034 + 0.1 * 6.728670597076416
Epoch 320, val loss: 0.6474710702896118
Epoch 330, training loss: 0.8663867115974426 = 0.19504843652248383 + 0.1 * 6.713382244110107
Epoch 330, val loss: 0.6462218165397644
Epoch 340, training loss: 0.8448901772499084 = 0.17360728979110718 + 0.1 * 6.712828636169434
Epoch 340, val loss: 0.6473575234413147
Epoch 350, training loss: 0.8252769112586975 = 0.15493397414684296 + 0.1 * 6.703429222106934
Epoch 350, val loss: 0.6505990624427795
Epoch 360, training loss: 0.8081204295158386 = 0.13871292769908905 + 0.1 * 6.694074630737305
Epoch 360, val loss: 0.655678391456604
Epoch 370, training loss: 0.7941141724586487 = 0.12465100735425949 + 0.1 * 6.694631576538086
Epoch 370, val loss: 0.6622976064682007
Epoch 380, training loss: 0.7810531258583069 = 0.11248301714658737 + 0.1 * 6.6857008934021
Epoch 380, val loss: 0.6699621677398682
Epoch 390, training loss: 0.7700502872467041 = 0.10185179114341736 + 0.1 * 6.681985378265381
Epoch 390, val loss: 0.678554356098175
Epoch 400, training loss: 0.7592030763626099 = 0.09252103418111801 + 0.1 * 6.666820526123047
Epoch 400, val loss: 0.6878154873847961
Epoch 410, training loss: 0.751499593257904 = 0.08426988869905472 + 0.1 * 6.67229700088501
Epoch 410, val loss: 0.6975168585777283
Epoch 420, training loss: 0.7435752749443054 = 0.07696013152599335 + 0.1 * 6.66615104675293
Epoch 420, val loss: 0.7075040340423584
Epoch 430, training loss: 0.73572838306427 = 0.0704529732465744 + 0.1 * 6.652753829956055
Epoch 430, val loss: 0.7176614999771118
Epoch 440, training loss: 0.7314841151237488 = 0.06462300568819046 + 0.1 * 6.6686110496521
Epoch 440, val loss: 0.7279566526412964
Epoch 450, training loss: 0.723953902721405 = 0.05942152440547943 + 0.1 * 6.645323753356934
Epoch 450, val loss: 0.7381497621536255
Epoch 460, training loss: 0.7187089323997498 = 0.054754577577114105 + 0.1 * 6.639543533325195
Epoch 460, val loss: 0.7482697367668152
Epoch 470, training loss: 0.7144012451171875 = 0.05054335296154022 + 0.1 * 6.63857889175415
Epoch 470, val loss: 0.7584831118583679
Epoch 480, training loss: 0.710127055644989 = 0.046752430498600006 + 0.1 * 6.633746147155762
Epoch 480, val loss: 0.7685705423355103
Epoch 490, training loss: 0.7059367895126343 = 0.0433296337723732 + 0.1 * 6.626071453094482
Epoch 490, val loss: 0.7785344123840332
Epoch 500, training loss: 0.7020026445388794 = 0.040240708738565445 + 0.1 * 6.617619514465332
Epoch 500, val loss: 0.7883996963500977
Epoch 510, training loss: 0.6993198990821838 = 0.037455964833498 + 0.1 * 6.61863899230957
Epoch 510, val loss: 0.7980291247367859
Epoch 520, training loss: 0.6974851489067078 = 0.034936174750328064 + 0.1 * 6.625489711761475
Epoch 520, val loss: 0.807540237903595
Epoch 530, training loss: 0.693953812122345 = 0.03266369551420212 + 0.1 * 6.612901210784912
Epoch 530, val loss: 0.81667560338974
Epoch 540, training loss: 0.6908425092697144 = 0.030598532408475876 + 0.1 * 6.602439880371094
Epoch 540, val loss: 0.8257646560668945
Epoch 550, training loss: 0.6906742453575134 = 0.02871560864150524 + 0.1 * 6.619585990905762
Epoch 550, val loss: 0.8346449136734009
Epoch 560, training loss: 0.6859706044197083 = 0.02700277790427208 + 0.1 * 6.5896782875061035
Epoch 560, val loss: 0.8433178663253784
Epoch 570, training loss: 0.6860814094543457 = 0.025438381358981133 + 0.1 * 6.6064300537109375
Epoch 570, val loss: 0.8517941236495972
Epoch 580, training loss: 0.6831997632980347 = 0.024011066183447838 + 0.1 * 6.5918869972229
Epoch 580, val loss: 0.8601091504096985
Epoch 590, training loss: 0.681524395942688 = 0.02270582690834999 + 0.1 * 6.5881853103637695
Epoch 590, val loss: 0.8680883049964905
Epoch 600, training loss: 0.6791508197784424 = 0.02150755561888218 + 0.1 * 6.576432228088379
Epoch 600, val loss: 0.8760263919830322
Epoch 610, training loss: 0.6773278713226318 = 0.020405907183885574 + 0.1 * 6.569219589233398
Epoch 610, val loss: 0.8836176991462708
Epoch 620, training loss: 0.6775907278060913 = 0.019384821876883507 + 0.1 * 6.582059383392334
Epoch 620, val loss: 0.891124427318573
Epoch 630, training loss: 0.674763560295105 = 0.018443048000335693 + 0.1 * 6.563205242156982
Epoch 630, val loss: 0.8985313773155212
Epoch 640, training loss: 0.6751869916915894 = 0.01756948046386242 + 0.1 * 6.576174736022949
Epoch 640, val loss: 0.9057350158691406
Epoch 650, training loss: 0.6732798218727112 = 0.016759881749749184 + 0.1 * 6.565199375152588
Epoch 650, val loss: 0.912757396697998
Epoch 660, training loss: 0.67026686668396 = 0.016011185944080353 + 0.1 * 6.5425567626953125
Epoch 660, val loss: 0.9195557236671448
Epoch 670, training loss: 0.6713032722473145 = 0.015311149880290031 + 0.1 * 6.5599212646484375
Epoch 670, val loss: 0.926283061504364
Epoch 680, training loss: 0.6687063574790955 = 0.014660869725048542 + 0.1 * 6.540454387664795
Epoch 680, val loss: 0.9328126311302185
Epoch 690, training loss: 0.670210599899292 = 0.01405332237482071 + 0.1 * 6.561572551727295
Epoch 690, val loss: 0.9391730427742004
Epoch 700, training loss: 0.667290985584259 = 0.013484615832567215 + 0.1 * 6.538064002990723
Epoch 700, val loss: 0.9453958868980408
Epoch 710, training loss: 0.6663898229598999 = 0.012953231111168861 + 0.1 * 6.534365653991699
Epoch 710, val loss: 0.9514508247375488
Epoch 720, training loss: 0.6655158996582031 = 0.012452870607376099 + 0.1 * 6.530629634857178
Epoch 720, val loss: 0.9574200510978699
Epoch 730, training loss: 0.6642084121704102 = 0.011984656564891338 + 0.1 * 6.522237300872803
Epoch 730, val loss: 0.9632245302200317
Epoch 740, training loss: 0.6659613847732544 = 0.011543244123458862 + 0.1 * 6.544181823730469
Epoch 740, val loss: 0.9689285755157471
Epoch 750, training loss: 0.6636245846748352 = 0.011127537116408348 + 0.1 * 6.524970531463623
Epoch 750, val loss: 0.9745491147041321
Epoch 760, training loss: 0.6620467305183411 = 0.01073913462460041 + 0.1 * 6.513075828552246
Epoch 760, val loss: 0.9798879027366638
Epoch 770, training loss: 0.6631577014923096 = 0.010369637981057167 + 0.1 * 6.5278801918029785
Epoch 770, val loss: 0.9851769208908081
Epoch 780, training loss: 0.6609292030334473 = 0.010020828805863857 + 0.1 * 6.509083271026611
Epoch 780, val loss: 0.9904744625091553
Epoch 790, training loss: 0.6606987714767456 = 0.009691175073385239 + 0.1 * 6.510075569152832
Epoch 790, val loss: 0.9955614805221558
Epoch 800, training loss: 0.6607489585876465 = 0.009378896094858646 + 0.1 * 6.513700485229492
Epoch 800, val loss: 1.0006554126739502
Epoch 810, training loss: 0.6592921018600464 = 0.009083925746381283 + 0.1 * 6.502081394195557
Epoch 810, val loss: 1.0055278539657593
Epoch 820, training loss: 0.6590953469276428 = 0.008803357370197773 + 0.1 * 6.502919673919678
Epoch 820, val loss: 1.0103007555007935
Epoch 830, training loss: 0.6582256555557251 = 0.008536743000149727 + 0.1 * 6.496888637542725
Epoch 830, val loss: 1.0150710344314575
Epoch 840, training loss: 0.6568077206611633 = 0.008283672854304314 + 0.1 * 6.485240459442139
Epoch 840, val loss: 1.0197076797485352
Epoch 850, training loss: 0.6572247743606567 = 0.008042475208640099 + 0.1 * 6.491823196411133
Epoch 850, val loss: 1.0242102146148682
Epoch 860, training loss: 0.6581504940986633 = 0.007813170552253723 + 0.1 * 6.503373146057129
Epoch 860, val loss: 1.0286962985992432
Epoch 870, training loss: 0.6566585302352905 = 0.007594001945108175 + 0.1 * 6.490645408630371
Epoch 870, val loss: 1.03312087059021
Epoch 880, training loss: 0.6550592184066772 = 0.007386356592178345 + 0.1 * 6.476728916168213
Epoch 880, val loss: 1.0373594760894775
Epoch 890, training loss: 0.6554447412490845 = 0.007187121547758579 + 0.1 * 6.4825758934021
Epoch 890, val loss: 1.0415364503860474
Epoch 900, training loss: 0.6537653803825378 = 0.006996863055974245 + 0.1 * 6.467685222625732
Epoch 900, val loss: 1.0456515550613403
Epoch 910, training loss: 0.6572503447532654 = 0.006814540829509497 + 0.1 * 6.504357814788818
Epoch 910, val loss: 1.0496740341186523
Epoch 920, training loss: 0.6540189981460571 = 0.006640513893216848 + 0.1 * 6.473784923553467
Epoch 920, val loss: 1.0537158250808716
Epoch 930, training loss: 0.6535325050354004 = 0.006474816706031561 + 0.1 * 6.470576763153076
Epoch 930, val loss: 1.0575053691864014
Epoch 940, training loss: 0.6528372168540955 = 0.006315253209322691 + 0.1 * 6.465219497680664
Epoch 940, val loss: 1.0613020658493042
Epoch 950, training loss: 0.6524856090545654 = 0.006161645520478487 + 0.1 * 6.463239669799805
Epoch 950, val loss: 1.065062403678894
Epoch 960, training loss: 0.6533591747283936 = 0.00601463345810771 + 0.1 * 6.473444938659668
Epoch 960, val loss: 1.0687798261642456
Epoch 970, training loss: 0.651679277420044 = 0.005873866844922304 + 0.1 * 6.458054065704346
Epoch 970, val loss: 1.072381615638733
Epoch 980, training loss: 0.65350741147995 = 0.005738789681345224 + 0.1 * 6.477685928344727
Epoch 980, val loss: 1.0759544372558594
Epoch 990, training loss: 0.6510657072067261 = 0.005608509760349989 + 0.1 * 6.454571723937988
Epoch 990, val loss: 1.0794681310653687
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.8090806007385254 = 1.9493966102600098 + 0.1 * 8.59683895111084
Epoch 0, val loss: 1.9431068897247314
Epoch 10, training loss: 2.799311399459839 = 1.9396350383758545 + 0.1 * 8.596763610839844
Epoch 10, val loss: 1.933653473854065
Epoch 20, training loss: 2.787169933319092 = 1.927533507347107 + 0.1 * 8.596364974975586
Epoch 20, val loss: 1.921522617340088
Epoch 30, training loss: 2.7696216106414795 = 1.9103281497955322 + 0.1 * 8.592933654785156
Epoch 30, val loss: 1.9040902853012085
Epoch 40, training loss: 2.7411346435546875 = 1.8846471309661865 + 0.1 * 8.564875602722168
Epoch 40, val loss: 1.8783925771713257
Epoch 50, training loss: 2.6889703273773193 = 1.848742127418518 + 0.1 * 8.402281761169434
Epoch 50, val loss: 1.8441635370254517
Epoch 60, training loss: 2.62324595451355 = 1.8082290887832642 + 0.1 * 8.150169372558594
Epoch 60, val loss: 1.8082866668701172
Epoch 70, training loss: 2.5583717823028564 = 1.7697738409042358 + 0.1 * 7.885978698730469
Epoch 70, val loss: 1.775852084159851
Epoch 80, training loss: 2.474551200866699 = 1.7295159101486206 + 0.1 * 7.450352191925049
Epoch 80, val loss: 1.7407869100570679
Epoch 90, training loss: 2.3981549739837646 = 1.6804009675979614 + 0.1 * 7.177539348602295
Epoch 90, val loss: 1.6975715160369873
Epoch 100, training loss: 2.3213746547698975 = 1.6154673099517822 + 0.1 * 7.059073448181152
Epoch 100, val loss: 1.6411052942276
Epoch 110, training loss: 2.2346954345703125 = 1.5353986024856567 + 0.1 * 6.992968559265137
Epoch 110, val loss: 1.5731256008148193
Epoch 120, training loss: 2.144695997238159 = 1.4499212503433228 + 0.1 * 6.947748184204102
Epoch 120, val loss: 1.5043643712997437
Epoch 130, training loss: 2.0556387901306152 = 1.3648028373718262 + 0.1 * 6.908358573913574
Epoch 130, val loss: 1.4372795820236206
Epoch 140, training loss: 1.9683911800384521 = 1.2799878120422363 + 0.1 * 6.884033203125
Epoch 140, val loss: 1.3718092441558838
Epoch 150, training loss: 1.883223533630371 = 1.1973488330841064 + 0.1 * 6.858746528625488
Epoch 150, val loss: 1.3083163499832153
Epoch 160, training loss: 1.8003849983215332 = 1.1165128946304321 + 0.1 * 6.838720798492432
Epoch 160, val loss: 1.246755838394165
Epoch 170, training loss: 1.7233037948608398 = 1.040640950202942 + 0.1 * 6.8266282081604
Epoch 170, val loss: 1.1907027959823608
Epoch 180, training loss: 1.653306245803833 = 0.9719550609588623 + 0.1 * 6.813510894775391
Epoch 180, val loss: 1.141879677772522
Epoch 190, training loss: 1.5894218683242798 = 0.9091602563858032 + 0.1 * 6.802616119384766
Epoch 190, val loss: 1.0982627868652344
Epoch 200, training loss: 1.5308703184127808 = 0.8511587381362915 + 0.1 * 6.797115802764893
Epoch 200, val loss: 1.0584102869033813
Epoch 210, training loss: 1.4750282764434814 = 0.7965782284736633 + 0.1 * 6.784499645233154
Epoch 210, val loss: 1.0213817358016968
Epoch 220, training loss: 1.4208803176879883 = 0.7435231804847717 + 0.1 * 6.773571491241455
Epoch 220, val loss: 0.985524594783783
Epoch 230, training loss: 1.3681188821792603 = 0.6910421848297119 + 0.1 * 6.770766735076904
Epoch 230, val loss: 0.9506725072860718
Epoch 240, training loss: 1.3144454956054688 = 0.6389968991279602 + 0.1 * 6.754485130310059
Epoch 240, val loss: 0.9168154001235962
Epoch 250, training loss: 1.2609062194824219 = 0.5866520404815674 + 0.1 * 6.742542266845703
Epoch 250, val loss: 0.8834245204925537
Epoch 260, training loss: 1.211082100868225 = 0.5340922474861145 + 0.1 * 6.769898414611816
Epoch 260, val loss: 0.8507739305496216
Epoch 270, training loss: 1.1561707258224487 = 0.48314955830574036 + 0.1 * 6.73021125793457
Epoch 270, val loss: 0.8205093145370483
Epoch 280, training loss: 1.105542778968811 = 0.4340873658657074 + 0.1 * 6.7145538330078125
Epoch 280, val loss: 0.7928546667098999
Epoch 290, training loss: 1.0580198764801025 = 0.3876536786556244 + 0.1 * 6.7036614418029785
Epoch 290, val loss: 0.7689471244812012
Epoch 300, training loss: 1.0156235694885254 = 0.3449461758136749 + 0.1 * 6.7067742347717285
Epoch 300, val loss: 0.7496170997619629
Epoch 310, training loss: 0.975436270236969 = 0.3066061735153198 + 0.1 * 6.688301086425781
Epoch 310, val loss: 0.7351282238960266
Epoch 320, training loss: 0.940550684928894 = 0.2724033296108246 + 0.1 * 6.681473731994629
Epoch 320, val loss: 0.7249069213867188
Epoch 330, training loss: 0.9105725884437561 = 0.2421942949295044 + 0.1 * 6.683782577514648
Epoch 330, val loss: 0.7183111310005188
Epoch 340, training loss: 0.8824072480201721 = 0.21578764915466309 + 0.1 * 6.666195869445801
Epoch 340, val loss: 0.7150853872299194
Epoch 350, training loss: 0.8580992221832275 = 0.1925656944513321 + 0.1 * 6.655335426330566
Epoch 350, val loss: 0.7145692706108093
Epoch 360, training loss: 0.838191032409668 = 0.17218056321144104 + 0.1 * 6.660104274749756
Epoch 360, val loss: 0.7163941264152527
Epoch 370, training loss: 0.8189796209335327 = 0.15446467697620392 + 0.1 * 6.6451497077941895
Epoch 370, val loss: 0.720346987247467
Epoch 380, training loss: 0.8029687404632568 = 0.13897143304347992 + 0.1 * 6.639972686767578
Epoch 380, val loss: 0.7259862422943115
Epoch 390, training loss: 0.7892364859580994 = 0.12546582520008087 + 0.1 * 6.637706756591797
Epoch 390, val loss: 0.7331936955451965
Epoch 400, training loss: 0.7765244841575623 = 0.11367131024599075 + 0.1 * 6.6285319328308105
Epoch 400, val loss: 0.7416041493415833
Epoch 410, training loss: 0.7676762342453003 = 0.10330477356910706 + 0.1 * 6.643714904785156
Epoch 410, val loss: 0.7509978413581848
Epoch 420, training loss: 0.7557531595230103 = 0.09419413655996323 + 0.1 * 6.615590572357178
Epoch 420, val loss: 0.761139988899231
Epoch 430, training loss: 0.7477617859840393 = 0.08612220734357834 + 0.1 * 6.616395473480225
Epoch 430, val loss: 0.7718489170074463
Epoch 440, training loss: 0.7416748404502869 = 0.07896411418914795 + 0.1 * 6.6271071434021
Epoch 440, val loss: 0.7829765677452087
Epoch 450, training loss: 0.7327226400375366 = 0.07260574400424957 + 0.1 * 6.601169109344482
Epoch 450, val loss: 0.7944204807281494
Epoch 460, training loss: 0.7262185215950012 = 0.06690607219934464 + 0.1 * 6.5931243896484375
Epoch 460, val loss: 0.8059605360031128
Epoch 470, training loss: 0.720517098903656 = 0.06178748235106468 + 0.1 * 6.587296009063721
Epoch 470, val loss: 0.8176361322402954
Epoch 480, training loss: 0.7152432203292847 = 0.057180359959602356 + 0.1 * 6.580628871917725
Epoch 480, val loss: 0.8294174671173096
Epoch 490, training loss: 0.7123141288757324 = 0.05302077904343605 + 0.1 * 6.592933654785156
Epoch 490, val loss: 0.8410477638244629
Epoch 500, training loss: 0.7075905799865723 = 0.04928458854556084 + 0.1 * 6.583059787750244
Epoch 500, val loss: 0.8526597023010254
Epoch 510, training loss: 0.7032460570335388 = 0.04590180143713951 + 0.1 * 6.573442459106445
Epoch 510, val loss: 0.8640931844711304
Epoch 520, training loss: 0.6994102001190186 = 0.042828269302845 + 0.1 * 6.565819263458252
Epoch 520, val loss: 0.8753795623779297
Epoch 530, training loss: 0.6986111402511597 = 0.04003411531448364 + 0.1 * 6.585770130157471
Epoch 530, val loss: 0.8865123391151428
Epoch 540, training loss: 0.6943918466567993 = 0.03750087320804596 + 0.1 * 6.568909645080566
Epoch 540, val loss: 0.8974654674530029
Epoch 550, training loss: 0.6908285617828369 = 0.03518712893128395 + 0.1 * 6.5564141273498535
Epoch 550, val loss: 0.9082015752792358
Epoch 560, training loss: 0.6889154314994812 = 0.03306788206100464 + 0.1 * 6.558475494384766
Epoch 560, val loss: 0.9187686443328857
Epoch 570, training loss: 0.6864508986473083 = 0.031129367649555206 + 0.1 * 6.553215503692627
Epoch 570, val loss: 0.9290845990180969
Epoch 580, training loss: 0.684471070766449 = 0.029354488477110863 + 0.1 * 6.551165580749512
Epoch 580, val loss: 0.939245879650116
Epoch 590, training loss: 0.6828819513320923 = 0.0277214627712965 + 0.1 * 6.551604747772217
Epoch 590, val loss: 0.9490804076194763
Epoch 600, training loss: 0.6810370683670044 = 0.02621958591043949 + 0.1 * 6.5481743812561035
Epoch 600, val loss: 0.9588740468025208
Epoch 610, training loss: 0.6783572435379028 = 0.024832556024193764 + 0.1 * 6.535246849060059
Epoch 610, val loss: 0.9683348536491394
Epoch 620, training loss: 0.6794202327728271 = 0.02355123870074749 + 0.1 * 6.558690071105957
Epoch 620, val loss: 0.9776297211647034
Epoch 630, training loss: 0.6760258674621582 = 0.022370189428329468 + 0.1 * 6.536556243896484
Epoch 630, val loss: 0.9867398738861084
Epoch 640, training loss: 0.6743637323379517 = 0.02127481997013092 + 0.1 * 6.53088903427124
Epoch 640, val loss: 0.9956615567207336
Epoch 650, training loss: 0.6748338937759399 = 0.020255999639630318 + 0.1 * 6.545778751373291
Epoch 650, val loss: 1.004305124282837
Epoch 660, training loss: 0.6722080707550049 = 0.019313178956508636 + 0.1 * 6.52894926071167
Epoch 660, val loss: 1.0127298831939697
Epoch 670, training loss: 0.6705586910247803 = 0.01843966357409954 + 0.1 * 6.521190166473389
Epoch 670, val loss: 1.0211529731750488
Epoch 680, training loss: 0.6698778867721558 = 0.017622891813516617 + 0.1 * 6.522550106048584
Epoch 680, val loss: 1.029214859008789
Epoch 690, training loss: 0.6686487793922424 = 0.016859648749232292 + 0.1 * 6.517890930175781
Epoch 690, val loss: 1.037125825881958
Epoch 700, training loss: 0.6673682332038879 = 0.016145942732691765 + 0.1 * 6.512222766876221
Epoch 700, val loss: 1.0450222492218018
Epoch 710, training loss: 0.6673352718353271 = 0.015476712957024574 + 0.1 * 6.518585681915283
Epoch 710, val loss: 1.0525321960449219
Epoch 720, training loss: 0.6660148501396179 = 0.01485141646116972 + 0.1 * 6.511634349822998
Epoch 720, val loss: 1.0599712133407593
Epoch 730, training loss: 0.6657547950744629 = 0.01426604948937893 + 0.1 * 6.514886856079102
Epoch 730, val loss: 1.0673178434371948
Epoch 740, training loss: 0.6640413999557495 = 0.013715337961912155 + 0.1 * 6.503260612487793
Epoch 740, val loss: 1.0743900537490845
Epoch 750, training loss: 0.6631454825401306 = 0.013197219930589199 + 0.1 * 6.499482154846191
Epoch 750, val loss: 1.0812541246414185
Epoch 760, training loss: 0.6630533337593079 = 0.012711317278444767 + 0.1 * 6.503419876098633
Epoch 760, val loss: 1.0881528854370117
Epoch 770, training loss: 0.6651073694229126 = 0.012251467444002628 + 0.1 * 6.528558731079102
Epoch 770, val loss: 1.0947703123092651
Epoch 780, training loss: 0.661700963973999 = 0.011819010600447655 + 0.1 * 6.498819351196289
Epoch 780, val loss: 1.1012046337127686
Epoch 790, training loss: 0.6606466174125671 = 0.0114108482375741 + 0.1 * 6.4923577308654785
Epoch 790, val loss: 1.1077162027359009
Epoch 800, training loss: 0.6598076224327087 = 0.011023527011275291 + 0.1 * 6.4878411293029785
Epoch 800, val loss: 1.1138211488723755
Epoch 810, training loss: 0.660125732421875 = 0.01065831445157528 + 0.1 * 6.494674205780029
Epoch 810, val loss: 1.1200374364852905
Epoch 820, training loss: 0.6583815217018127 = 0.010310987010598183 + 0.1 * 6.480705261230469
Epoch 820, val loss: 1.1260814666748047
Epoch 830, training loss: 0.6598318815231323 = 0.009981161914765835 + 0.1 * 6.498507022857666
Epoch 830, val loss: 1.1318942308425903
Epoch 840, training loss: 0.6579704880714417 = 0.009668729268014431 + 0.1 * 6.483017444610596
Epoch 840, val loss: 1.1376032829284668
Epoch 850, training loss: 0.6575887799263 = 0.009373162873089314 + 0.1 * 6.482155799865723
Epoch 850, val loss: 1.1434028148651123
Epoch 860, training loss: 0.6574199795722961 = 0.00909077376127243 + 0.1 * 6.4832916259765625
Epoch 860, val loss: 1.148983120918274
Epoch 870, training loss: 0.656561553478241 = 0.008822394534945488 + 0.1 * 6.477391719818115
Epoch 870, val loss: 1.1543169021606445
Epoch 880, training loss: 0.6559320688247681 = 0.008567357435822487 + 0.1 * 6.473647117614746
Epoch 880, val loss: 1.1598042249679565
Epoch 890, training loss: 0.6563896536827087 = 0.00832298956811428 + 0.1 * 6.480666160583496
Epoch 890, val loss: 1.1649452447891235
Epoch 900, training loss: 0.6543316841125488 = 0.008091098628938198 + 0.1 * 6.462405681610107
Epoch 900, val loss: 1.1701204776763916
Epoch 910, training loss: 0.6548138856887817 = 0.007869645953178406 + 0.1 * 6.469442367553711
Epoch 910, val loss: 1.175331711769104
Epoch 920, training loss: 0.6546247005462646 = 0.007657330483198166 + 0.1 * 6.4696736335754395
Epoch 920, val loss: 1.1801360845565796
Epoch 930, training loss: 0.6543747782707214 = 0.007455223705619574 + 0.1 * 6.469195365905762
Epoch 930, val loss: 1.185048222541809
Epoch 940, training loss: 0.6534556150436401 = 0.00726132420822978 + 0.1 * 6.461942672729492
Epoch 940, val loss: 1.189896821975708
Epoch 950, training loss: 0.6538655757904053 = 0.007075921632349491 + 0.1 * 6.467896461486816
Epoch 950, val loss: 1.1945432424545288
Epoch 960, training loss: 0.6528796553611755 = 0.00689795333892107 + 0.1 * 6.459817409515381
Epoch 960, val loss: 1.1992493867874146
Epoch 970, training loss: 0.6533291339874268 = 0.006727511528879404 + 0.1 * 6.4660162925720215
Epoch 970, val loss: 1.2038075923919678
Epoch 980, training loss: 0.6523404121398926 = 0.006564016453921795 + 0.1 * 6.457763671875
Epoch 980, val loss: 1.2081456184387207
Epoch 990, training loss: 0.6510053277015686 = 0.006407426204532385 + 0.1 * 6.44597864151001
Epoch 990, val loss: 1.2126598358154297
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.7933475971221924 = 1.933663010597229 + 0.1 * 8.596845626831055
Epoch 0, val loss: 1.9356646537780762
Epoch 10, training loss: 2.783907175064087 = 1.9242304563522339 + 0.1 * 8.59676742553711
Epoch 10, val loss: 1.9264930486679077
Epoch 20, training loss: 2.772099018096924 = 1.9124755859375 + 0.1 * 8.596234321594238
Epoch 20, val loss: 1.914571762084961
Epoch 30, training loss: 2.7553634643554688 = 1.8961879014968872 + 0.1 * 8.591754913330078
Epoch 30, val loss: 1.8975403308868408
Epoch 40, training loss: 2.729003429412842 = 1.872550368309021 + 0.1 * 8.564530372619629
Epoch 40, val loss: 1.8726507425308228
Epoch 50, training loss: 2.6858911514282227 = 1.8400182723999023 + 0.1 * 8.458727836608887
Epoch 50, val loss: 1.8395408391952515
Epoch 60, training loss: 2.6155333518981934 = 1.8022546768188477 + 0.1 * 8.132787704467773
Epoch 60, val loss: 1.8038537502288818
Epoch 70, training loss: 2.5538763999938965 = 1.7636548280715942 + 0.1 * 7.90221643447876
Epoch 70, val loss: 1.7703224420547485
Epoch 80, training loss: 2.481436252593994 = 1.7198094129562378 + 0.1 * 7.616267681121826
Epoch 80, val loss: 1.7322344779968262
Epoch 90, training loss: 2.407069444656372 = 1.6640992164611816 + 0.1 * 7.429701805114746
Epoch 90, val loss: 1.684752345085144
Epoch 100, training loss: 2.325714588165283 = 1.5916613340377808 + 0.1 * 7.340531826019287
Epoch 100, val loss: 1.6216323375701904
Epoch 110, training loss: 2.2300238609313965 = 1.5017924308776855 + 0.1 * 7.282313823699951
Epoch 110, val loss: 1.5428303480148315
Epoch 120, training loss: 2.123931884765625 = 1.4010417461395264 + 0.1 * 7.2289018630981445
Epoch 120, val loss: 1.4580767154693604
Epoch 130, training loss: 2.0142529010772705 = 1.295728325843811 + 0.1 * 7.185245037078857
Epoch 130, val loss: 1.3706390857696533
Epoch 140, training loss: 1.9062130451202393 = 1.1908503770828247 + 0.1 * 7.153625965118408
Epoch 140, val loss: 1.2861822843551636
Epoch 150, training loss: 1.8034889698028564 = 1.0905382633209229 + 0.1 * 7.1295061111450195
Epoch 150, val loss: 1.2079535722732544
Epoch 160, training loss: 1.7089475393295288 = 0.9976059794425964 + 0.1 * 7.113415241241455
Epoch 160, val loss: 1.1376864910125732
Epoch 170, training loss: 1.6228290796279907 = 0.9129038453102112 + 0.1 * 7.099252223968506
Epoch 170, val loss: 1.075538992881775
Epoch 180, training loss: 1.543592095375061 = 0.8355134725570679 + 0.1 * 7.080786228179932
Epoch 180, val loss: 1.0197651386260986
Epoch 190, training loss: 1.4702365398406982 = 0.7647057175636292 + 0.1 * 7.055307388305664
Epoch 190, val loss: 0.9692814350128174
Epoch 200, training loss: 1.4029628038406372 = 0.7003463506698608 + 0.1 * 7.026164531707764
Epoch 200, val loss: 0.9244070053100586
Epoch 210, training loss: 1.3411660194396973 = 0.6413097381591797 + 0.1 * 6.998563289642334
Epoch 210, val loss: 0.8849073648452759
Epoch 220, training loss: 1.2840888500213623 = 0.586615800857544 + 0.1 * 6.974730491638184
Epoch 220, val loss: 0.8508121967315674
Epoch 230, training loss: 1.2308073043823242 = 0.5350443124771118 + 0.1 * 6.957630634307861
Epoch 230, val loss: 0.8215681910514832
Epoch 240, training loss: 1.1808027029037476 = 0.4862930476665497 + 0.1 * 6.945096015930176
Epoch 240, val loss: 0.7967742085456848
Epoch 250, training loss: 1.1330335140228271 = 0.43981748819351196 + 0.1 * 6.932160377502441
Epoch 250, val loss: 0.7759960293769836
Epoch 260, training loss: 1.0872621536254883 = 0.3952549993991852 + 0.1 * 6.920071125030518
Epoch 260, val loss: 0.7590440511703491
Epoch 270, training loss: 1.0445526838302612 = 0.3531918227672577 + 0.1 * 6.913608074188232
Epoch 270, val loss: 0.7460604906082153
Epoch 280, training loss: 1.0042625665664673 = 0.31457215547561646 + 0.1 * 6.896903991699219
Epoch 280, val loss: 0.7370170950889587
Epoch 290, training loss: 0.9686336517333984 = 0.27967703342437744 + 0.1 * 6.889565944671631
Epoch 290, val loss: 0.7316562533378601
Epoch 300, training loss: 0.9361576437950134 = 0.2487000972032547 + 0.1 * 6.874575614929199
Epoch 300, val loss: 0.7298197150230408
Epoch 310, training loss: 0.9080392122268677 = 0.2215111404657364 + 0.1 * 6.865280628204346
Epoch 310, val loss: 0.7310218811035156
Epoch 320, training loss: 0.8828600645065308 = 0.19795668125152588 + 0.1 * 6.849033832550049
Epoch 320, val loss: 0.735044538974762
Epoch 330, training loss: 0.8622057437896729 = 0.1774279773235321 + 0.1 * 6.847777366638184
Epoch 330, val loss: 0.7415361404418945
Epoch 340, training loss: 0.8423000574111938 = 0.15965501964092255 + 0.1 * 6.826450347900391
Epoch 340, val loss: 0.7500171661376953
Epoch 350, training loss: 0.8260221481323242 = 0.14414763450622559 + 0.1 * 6.818745136260986
Epoch 350, val loss: 0.7601533532142639
Epoch 360, training loss: 0.8113702535629272 = 0.1305583417415619 + 0.1 * 6.80811882019043
Epoch 360, val loss: 0.7716919183731079
Epoch 370, training loss: 0.8003020882606506 = 0.11866390705108643 + 0.1 * 6.816381454467773
Epoch 370, val loss: 0.7839862108230591
Epoch 380, training loss: 0.7879749536514282 = 0.10823991894721985 + 0.1 * 6.79734992980957
Epoch 380, val loss: 0.7968810200691223
Epoch 390, training loss: 0.7774591445922852 = 0.09897717088460922 + 0.1 * 6.784820079803467
Epoch 390, val loss: 0.810329794883728
Epoch 400, training loss: 0.7688477039337158 = 0.09069427102804184 + 0.1 * 6.781534194946289
Epoch 400, val loss: 0.8241721391677856
Epoch 410, training loss: 0.7601628303527832 = 0.08329801261425018 + 0.1 * 6.76864767074585
Epoch 410, val loss: 0.8380014300346375
Epoch 420, training loss: 0.7540184259414673 = 0.0766618549823761 + 0.1 * 6.773565769195557
Epoch 420, val loss: 0.8519474864006042
Epoch 430, training loss: 0.7460779547691345 = 0.07071688026189804 + 0.1 * 6.753610610961914
Epoch 430, val loss: 0.865790069103241
Epoch 440, training loss: 0.7399243116378784 = 0.06535103917121887 + 0.1 * 6.745732307434082
Epoch 440, val loss: 0.8795381784439087
Epoch 450, training loss: 0.734157145023346 = 0.060481805354356766 + 0.1 * 6.736753463745117
Epoch 450, val loss: 0.8932151198387146
Epoch 460, training loss: 0.7292240858078003 = 0.056051939725875854 + 0.1 * 6.731720924377441
Epoch 460, val loss: 0.9067842960357666
Epoch 470, training loss: 0.7253296375274658 = 0.05203578621149063 + 0.1 * 6.732938289642334
Epoch 470, val loss: 0.9200363159179688
Epoch 480, training loss: 0.720419704914093 = 0.04841144382953644 + 0.1 * 6.720082759857178
Epoch 480, val loss: 0.9329460859298706
Epoch 490, training loss: 0.716440737247467 = 0.04510511830449104 + 0.1 * 6.7133564949035645
Epoch 490, val loss: 0.9456328749656677
Epoch 500, training loss: 0.712805986404419 = 0.04207738861441612 + 0.1 * 6.707286357879639
Epoch 500, val loss: 0.9581230878829956
Epoch 510, training loss: 0.7109494209289551 = 0.039299335330724716 + 0.1 * 6.716501235961914
Epoch 510, val loss: 0.9703801870346069
Epoch 520, training loss: 0.7074573636054993 = 0.03675913065671921 + 0.1 * 6.706982135772705
Epoch 520, val loss: 0.9823079705238342
Epoch 530, training loss: 0.7041459083557129 = 0.034429822117090225 + 0.1 * 6.697160243988037
Epoch 530, val loss: 0.9940207004547119
Epoch 540, training loss: 0.7022418975830078 = 0.03228931128978729 + 0.1 * 6.699525356292725
Epoch 540, val loss: 1.0054150819778442
Epoch 550, training loss: 0.6997283697128296 = 0.030327510088682175 + 0.1 * 6.6940083503723145
Epoch 550, val loss: 1.0165958404541016
Epoch 560, training loss: 0.6968265771865845 = 0.028521882370114326 + 0.1 * 6.683046817779541
Epoch 560, val loss: 1.0275039672851562
Epoch 570, training loss: 0.6961156725883484 = 0.026858646422624588 + 0.1 * 6.692570209503174
Epoch 570, val loss: 1.0381503105163574
Epoch 580, training loss: 0.693726658821106 = 0.02532903291285038 + 0.1 * 6.683975696563721
Epoch 580, val loss: 1.048496127128601
Epoch 590, training loss: 0.6912589073181152 = 0.023919804021716118 + 0.1 * 6.673390865325928
Epoch 590, val loss: 1.0585600137710571
Epoch 600, training loss: 0.6894452571868896 = 0.022619230672717094 + 0.1 * 6.668260097503662
Epoch 600, val loss: 1.0684301853179932
Epoch 610, training loss: 0.688696026802063 = 0.02141764387488365 + 0.1 * 6.672783851623535
Epoch 610, val loss: 1.0781017541885376
Epoch 620, training loss: 0.6864326000213623 = 0.020304488018155098 + 0.1 * 6.661281108856201
Epoch 620, val loss: 1.0874525308609009
Epoch 630, training loss: 0.6850365400314331 = 0.01927305944263935 + 0.1 * 6.657634735107422
Epoch 630, val loss: 1.0966267585754395
Epoch 640, training loss: 0.6852561831474304 = 0.018318135291337967 + 0.1 * 6.6693806648254395
Epoch 640, val loss: 1.1055251359939575
Epoch 650, training loss: 0.6825271248817444 = 0.017433738335967064 + 0.1 * 6.650933742523193
Epoch 650, val loss: 1.1141958236694336
Epoch 660, training loss: 0.6813587546348572 = 0.016613179817795753 + 0.1 * 6.64745569229126
Epoch 660, val loss: 1.1226434707641602
Epoch 670, training loss: 0.6799869537353516 = 0.0158493984490633 + 0.1 * 6.641375541687012
Epoch 670, val loss: 1.131021499633789
Epoch 680, training loss: 0.6821289658546448 = 0.015136047266423702 + 0.1 * 6.669929027557373
Epoch 680, val loss: 1.1390597820281982
Epoch 690, training loss: 0.6789701581001282 = 0.014474324882030487 + 0.1 * 6.644958019256592
Epoch 690, val loss: 1.1469289064407349
Epoch 700, training loss: 0.6767279505729675 = 0.013855747878551483 + 0.1 * 6.628721714019775
Epoch 700, val loss: 1.154642939567566
Epoch 710, training loss: 0.6764398813247681 = 0.013277010060846806 + 0.1 * 6.63162899017334
Epoch 710, val loss: 1.1622529029846191
Epoch 720, training loss: 0.677817165851593 = 0.012734068557620049 + 0.1 * 6.65083122253418
Epoch 720, val loss: 1.169586181640625
Epoch 730, training loss: 0.6753657460212708 = 0.012227419763803482 + 0.1 * 6.631383419036865
Epoch 730, val loss: 1.1767643690109253
Epoch 740, training loss: 0.6749451160430908 = 0.011751707643270493 + 0.1 * 6.631933689117432
Epoch 740, val loss: 1.1838878393173218
Epoch 750, training loss: 0.6731708645820618 = 0.011305581778287888 + 0.1 * 6.618652820587158
Epoch 750, val loss: 1.1907533407211304
Epoch 760, training loss: 0.6718705892562866 = 0.010886173695325851 + 0.1 * 6.609843730926514
Epoch 760, val loss: 1.1975611448287964
Epoch 770, training loss: 0.6724753975868225 = 0.010491042397916317 + 0.1 * 6.619843482971191
Epoch 770, val loss: 1.2042717933654785
Epoch 780, training loss: 0.6706980466842651 = 0.01011769287288189 + 0.1 * 6.605803489685059
Epoch 780, val loss: 1.2108350992202759
Epoch 790, training loss: 0.6709682941436768 = 0.009765489026904106 + 0.1 * 6.612027645111084
Epoch 790, val loss: 1.217193365097046
Epoch 800, training loss: 0.6698211431503296 = 0.009433931671082973 + 0.1 * 6.603871822357178
Epoch 800, val loss: 1.2234834432601929
Epoch 810, training loss: 0.6693459749221802 = 0.009120851755142212 + 0.1 * 6.602250576019287
Epoch 810, val loss: 1.2296727895736694
Epoch 820, training loss: 0.6684352159500122 = 0.00882387813180685 + 0.1 * 6.596113204956055
Epoch 820, val loss: 1.2356481552124023
Epoch 830, training loss: 0.6673492789268494 = 0.008543187752366066 + 0.1 * 6.5880608558654785
Epoch 830, val loss: 1.2415904998779297
Epoch 840, training loss: 0.6687522530555725 = 0.008277258835732937 + 0.1 * 6.604750156402588
Epoch 840, val loss: 1.2474150657653809
Epoch 850, training loss: 0.6661992073059082 = 0.00802443828433752 + 0.1 * 6.581747531890869
Epoch 850, val loss: 1.2530592679977417
Epoch 860, training loss: 0.6672232151031494 = 0.0077849035151302814 + 0.1 * 6.594383239746094
Epoch 860, val loss: 1.2587051391601562
Epoch 870, training loss: 0.667765736579895 = 0.007557405158877373 + 0.1 * 6.602083206176758
Epoch 870, val loss: 1.2641115188598633
Epoch 880, training loss: 0.6649186611175537 = 0.007341827731579542 + 0.1 * 6.57576847076416
Epoch 880, val loss: 1.269469141960144
Epoch 890, training loss: 0.6642917990684509 = 0.00713681522756815 + 0.1 * 6.571549892425537
Epoch 890, val loss: 1.2747858762741089
Epoch 900, training loss: 0.6653675436973572 = 0.006940575782209635 + 0.1 * 6.584269046783447
Epoch 900, val loss: 1.2799930572509766
Epoch 910, training loss: 0.6650373935699463 = 0.006753157824277878 + 0.1 * 6.5828423500061035
Epoch 910, val loss: 1.285104513168335
Epoch 920, training loss: 0.6630447506904602 = 0.006574289407581091 + 0.1 * 6.564704895019531
Epoch 920, val loss: 1.2899935245513916
Epoch 930, training loss: 0.6631855964660645 = 0.006404321175068617 + 0.1 * 6.567812442779541
Epoch 930, val loss: 1.2949225902557373
Epoch 940, training loss: 0.6634706258773804 = 0.006241907831281424 + 0.1 * 6.572287082672119
Epoch 940, val loss: 1.2997621297836304
Epoch 950, training loss: 0.6608502864837646 = 0.006086441688239574 + 0.1 * 6.547637939453125
Epoch 950, val loss: 1.3044971227645874
Epoch 960, training loss: 0.661698043346405 = 0.005937620997428894 + 0.1 * 6.55760383605957
Epoch 960, val loss: 1.309157133102417
Epoch 970, training loss: 0.6609327793121338 = 0.005794632248580456 + 0.1 * 6.5513811111450195
Epoch 970, val loss: 1.313704490661621
Epoch 980, training loss: 0.6616060137748718 = 0.005657778587192297 + 0.1 * 6.559482097625732
Epoch 980, val loss: 1.3182317018508911
Epoch 990, training loss: 0.6611395478248596 = 0.005526118446141481 + 0.1 * 6.556134223937988
Epoch 990, val loss: 1.3226350545883179
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8434370057986295
The final CL Acc:0.81852, 0.00605, The final GNN Acc:0.83940, 0.00305
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9524])
updated graph: torch.Size([2, 10590])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8087117671966553 = 1.9490309953689575 + 0.1 * 8.596807479858398
Epoch 0, val loss: 1.9499948024749756
Epoch 10, training loss: 2.797556161880493 = 1.937888264656067 + 0.1 * 8.5966796875
Epoch 10, val loss: 1.9393426179885864
Epoch 20, training loss: 2.783616542816162 = 1.9240310192108154 + 0.1 * 8.595855712890625
Epoch 20, val loss: 1.925629734992981
Epoch 30, training loss: 2.763593912124634 = 1.9047467708587646 + 0.1 * 8.588471412658691
Epoch 30, val loss: 1.906202793121338
Epoch 40, training loss: 2.7307751178741455 = 1.8768501281738281 + 0.1 * 8.539249420166016
Epoch 40, val loss: 1.8783584833145142
Epoch 50, training loss: 2.66802716255188 = 1.841278076171875 + 0.1 * 8.26749038696289
Epoch 50, val loss: 1.844787359237671
Epoch 60, training loss: 2.6113128662109375 = 1.8059428930282593 + 0.1 * 8.05370044708252
Epoch 60, val loss: 1.814240574836731
Epoch 70, training loss: 2.544776678085327 = 1.7763348817825317 + 0.1 * 7.684417247772217
Epoch 70, val loss: 1.789552927017212
Epoch 80, training loss: 2.477074384689331 = 1.7457398176193237 + 0.1 * 7.313344955444336
Epoch 80, val loss: 1.7624038457870483
Epoch 90, training loss: 2.415966033935547 = 1.7070170640945435 + 0.1 * 7.089489459991455
Epoch 90, val loss: 1.7278016805648804
Epoch 100, training loss: 2.3513078689575195 = 1.653269648551941 + 0.1 * 6.980381488800049
Epoch 100, val loss: 1.6809875965118408
Epoch 110, training loss: 2.2751388549804688 = 1.582900047302246 + 0.1 * 6.922388553619385
Epoch 110, val loss: 1.6204473972320557
Epoch 120, training loss: 2.19045352935791 = 1.5021978616714478 + 0.1 * 6.8825554847717285
Epoch 120, val loss: 1.554385781288147
Epoch 130, training loss: 2.1051745414733887 = 1.419463872909546 + 0.1 * 6.857105731964111
Epoch 130, val loss: 1.4888216257095337
Epoch 140, training loss: 2.0223097801208496 = 1.3381726741790771 + 0.1 * 6.841370582580566
Epoch 140, val loss: 1.4263569116592407
Epoch 150, training loss: 1.939967155456543 = 1.2570031881332397 + 0.1 * 6.829639434814453
Epoch 150, val loss: 1.3650987148284912
Epoch 160, training loss: 1.857896327972412 = 1.1755576133728027 + 0.1 * 6.823387145996094
Epoch 160, val loss: 1.30343496799469
Epoch 170, training loss: 1.7798258066177368 = 1.0984784364700317 + 0.1 * 6.813473701477051
Epoch 170, val loss: 1.2468748092651367
Epoch 180, training loss: 1.7079310417175293 = 1.0276055335998535 + 0.1 * 6.803255081176758
Epoch 180, val loss: 1.1961591243743896
Epoch 190, training loss: 1.6427183151245117 = 0.9630881547927856 + 0.1 * 6.796302318572998
Epoch 190, val loss: 1.1511669158935547
Epoch 200, training loss: 1.581960916519165 = 0.9032917618751526 + 0.1 * 6.786690711975098
Epoch 200, val loss: 1.1103134155273438
Epoch 210, training loss: 1.5234270095825195 = 0.8457664251327515 + 0.1 * 6.776606559753418
Epoch 210, val loss: 1.070937156677246
Epoch 220, training loss: 1.4654704332351685 = 0.7887997031211853 + 0.1 * 6.766706943511963
Epoch 220, val loss: 1.031881332397461
Epoch 230, training loss: 1.4077413082122803 = 0.7320066094398499 + 0.1 * 6.7573466300964355
Epoch 230, val loss: 0.9937321543693542
Epoch 240, training loss: 1.3515704870224 = 0.676968514919281 + 0.1 * 6.74601936340332
Epoch 240, val loss: 0.9583012461662292
Epoch 250, training loss: 1.2984199523925781 = 0.6246731877326965 + 0.1 * 6.7374677658081055
Epoch 250, val loss: 0.9273368120193481
Epoch 260, training loss: 1.2477651834487915 = 0.5750990509986877 + 0.1 * 6.726661205291748
Epoch 260, val loss: 0.9013409614562988
Epoch 270, training loss: 1.2033214569091797 = 0.5285324454307556 + 0.1 * 6.747889995574951
Epoch 270, val loss: 0.8805590867996216
Epoch 280, training loss: 1.1571829319000244 = 0.4859056770801544 + 0.1 * 6.712772369384766
Epoch 280, val loss: 0.8654146790504456
Epoch 290, training loss: 1.1167223453521729 = 0.4463514983654022 + 0.1 * 6.703708171844482
Epoch 290, val loss: 0.8546715974807739
Epoch 300, training loss: 1.0801351070404053 = 0.40920570492744446 + 0.1 * 6.709294319152832
Epoch 300, val loss: 0.8473570346832275
Epoch 310, training loss: 1.0429139137268066 = 0.3743055760860443 + 0.1 * 6.68608283996582
Epoch 310, val loss: 0.8433641195297241
Epoch 320, training loss: 1.009070873260498 = 0.3411617875099182 + 0.1 * 6.679091453552246
Epoch 320, val loss: 0.8416633605957031
Epoch 330, training loss: 0.9770114421844482 = 0.3097999691963196 + 0.1 * 6.672114372253418
Epoch 330, val loss: 0.8423028588294983
Epoch 340, training loss: 0.9468368291854858 = 0.28046220541000366 + 0.1 * 6.663746356964111
Epoch 340, val loss: 0.8451597690582275
Epoch 350, training loss: 0.9190270304679871 = 0.25340157747268677 + 0.1 * 6.656254291534424
Epoch 350, val loss: 0.8503459692001343
Epoch 360, training loss: 0.8943480849266052 = 0.22864477336406708 + 0.1 * 6.6570329666137695
Epoch 360, val loss: 0.8576536178588867
Epoch 370, training loss: 0.8708389401435852 = 0.2063087671995163 + 0.1 * 6.645301342010498
Epoch 370, val loss: 0.866936206817627
Epoch 380, training loss: 0.8495117425918579 = 0.18627718091011047 + 0.1 * 6.632345199584961
Epoch 380, val loss: 0.8781670331954956
Epoch 390, training loss: 0.8320721387863159 = 0.1682901531457901 + 0.1 * 6.637819766998291
Epoch 390, val loss: 0.891215980052948
Epoch 400, training loss: 0.8144549131393433 = 0.1522897481918335 + 0.1 * 6.621651649475098
Epoch 400, val loss: 0.9054819941520691
Epoch 410, training loss: 0.7992876768112183 = 0.138030007481575 + 0.1 * 6.612576961517334
Epoch 410, val loss: 0.9209513664245605
Epoch 420, training loss: 0.7858915328979492 = 0.12528398633003235 + 0.1 * 6.606075286865234
Epoch 420, val loss: 0.9376050233840942
Epoch 430, training loss: 0.7748183608055115 = 0.11390876770019531 + 0.1 * 6.609095573425293
Epoch 430, val loss: 0.9548377394676208
Epoch 440, training loss: 0.7637132406234741 = 0.10380841791629791 + 0.1 * 6.599048137664795
Epoch 440, val loss: 0.9727658033370972
Epoch 450, training loss: 0.7541126012802124 = 0.09480635821819305 + 0.1 * 6.593061923980713
Epoch 450, val loss: 0.9910984635353088
Epoch 460, training loss: 0.745261013507843 = 0.08677861839532852 + 0.1 * 6.5848236083984375
Epoch 460, val loss: 1.0094963312149048
Epoch 470, training loss: 0.7376772165298462 = 0.0796145498752594 + 0.1 * 6.580626010894775
Epoch 470, val loss: 1.028183102607727
Epoch 480, training loss: 0.7334425449371338 = 0.07320336252450943 + 0.1 * 6.602391242980957
Epoch 480, val loss: 1.0466151237487793
Epoch 490, training loss: 0.7242104411125183 = 0.06747995316982269 + 0.1 * 6.567304611206055
Epoch 490, val loss: 1.0650782585144043
Epoch 500, training loss: 0.7200630307197571 = 0.06234040483832359 + 0.1 * 6.577226161956787
Epoch 500, val loss: 1.0831904411315918
Epoch 510, training loss: 0.7137609124183655 = 0.05772455409169197 + 0.1 * 6.56036376953125
Epoch 510, val loss: 1.1010620594024658
Epoch 520, training loss: 0.7098607420921326 = 0.05356503278017044 + 0.1 * 6.562957286834717
Epoch 520, val loss: 1.1185795068740845
Epoch 530, training loss: 0.7056227326393127 = 0.04981102794408798 + 0.1 * 6.558117389678955
Epoch 530, val loss: 1.1359753608703613
Epoch 540, training loss: 0.702103853225708 = 0.04641963914036751 + 0.1 * 6.55684232711792
Epoch 540, val loss: 1.1526223421096802
Epoch 550, training loss: 0.6983914971351624 = 0.04334980994462967 + 0.1 * 6.550416946411133
Epoch 550, val loss: 1.1692383289337158
Epoch 560, training loss: 0.6951762437820435 = 0.040566377341747284 + 0.1 * 6.546098709106445
Epoch 560, val loss: 1.185148000717163
Epoch 570, training loss: 0.6921833753585815 = 0.038039688020944595 + 0.1 * 6.541436672210693
Epoch 570, val loss: 1.2010630369186401
Epoch 580, training loss: 0.6897186040878296 = 0.03573312610387802 + 0.1 * 6.539855003356934
Epoch 580, val loss: 1.2164679765701294
Epoch 590, training loss: 0.6875346302986145 = 0.033625055104494095 + 0.1 * 6.539095401763916
Epoch 590, val loss: 1.2313084602355957
Epoch 600, training loss: 0.6844515800476074 = 0.03169803321361542 + 0.1 * 6.527535438537598
Epoch 600, val loss: 1.246147871017456
Epoch 610, training loss: 0.6840102076530457 = 0.02992815524339676 + 0.1 * 6.540820598602295
Epoch 610, val loss: 1.260324239730835
Epoch 620, training loss: 0.6836835741996765 = 0.028303729370236397 + 0.1 * 6.553798198699951
Epoch 620, val loss: 1.2742195129394531
Epoch 630, training loss: 0.6788662075996399 = 0.02681431919336319 + 0.1 * 6.5205183029174805
Epoch 630, val loss: 1.2878526449203491
Epoch 640, training loss: 0.6768659949302673 = 0.025440029799938202 + 0.1 * 6.514259338378906
Epoch 640, val loss: 1.3011869192123413
Epoch 650, training loss: 0.6749782562255859 = 0.024168359115719795 + 0.1 * 6.508098602294922
Epoch 650, val loss: 1.314225435256958
Epoch 660, training loss: 0.6743214726448059 = 0.022989870980381966 + 0.1 * 6.513315677642822
Epoch 660, val loss: 1.326595425605774
Epoch 670, training loss: 0.6722239851951599 = 0.021900653839111328 + 0.1 * 6.503233432769775
Epoch 670, val loss: 1.3392677307128906
Epoch 680, training loss: 0.6721935272216797 = 0.020887350663542747 + 0.1 * 6.5130615234375
Epoch 680, val loss: 1.351286768913269
Epoch 690, training loss: 0.6701856255531311 = 0.019946040585637093 + 0.1 * 6.5023956298828125
Epoch 690, val loss: 1.3632352352142334
Epoch 700, training loss: 0.6689431071281433 = 0.01906801015138626 + 0.1 * 6.498750686645508
Epoch 700, val loss: 1.374467134475708
Epoch 710, training loss: 0.6675707101821899 = 0.018250860273838043 + 0.1 * 6.493197917938232
Epoch 710, val loss: 1.3860615491867065
Epoch 720, training loss: 0.6670534014701843 = 0.01748618669807911 + 0.1 * 6.49567174911499
Epoch 720, val loss: 1.3970777988433838
Epoch 730, training loss: 0.6660892367362976 = 0.01677027903497219 + 0.1 * 6.493189334869385
Epoch 730, val loss: 1.4074259996414185
Epoch 740, training loss: 0.6651573777198792 = 0.016102934256196022 + 0.1 * 6.490544319152832
Epoch 740, val loss: 1.4183329343795776
Epoch 750, training loss: 0.66558837890625 = 0.015475628897547722 + 0.1 * 6.501127243041992
Epoch 750, val loss: 1.428575038909912
Epoch 760, training loss: 0.6630204319953918 = 0.014886567369103432 + 0.1 * 6.4813385009765625
Epoch 760, val loss: 1.4386975765228271
Epoch 770, training loss: 0.6617989540100098 = 0.014332924969494343 + 0.1 * 6.4746599197387695
Epoch 770, val loss: 1.4487498998641968
Epoch 780, training loss: 0.663227915763855 = 0.013809860683977604 + 0.1 * 6.494180679321289
Epoch 780, val loss: 1.4581490755081177
Epoch 790, training loss: 0.6610031127929688 = 0.013317398726940155 + 0.1 * 6.476856708526611
Epoch 790, val loss: 1.4676135778427124
Epoch 800, training loss: 0.6596423983573914 = 0.01285340916365385 + 0.1 * 6.46789026260376
Epoch 800, val loss: 1.477132797241211
Epoch 810, training loss: 0.6612052917480469 = 0.012414121069014072 + 0.1 * 6.487911701202393
Epoch 810, val loss: 1.4860471487045288
Epoch 820, training loss: 0.6592622995376587 = 0.011999434791505337 + 0.1 * 6.472628593444824
Epoch 820, val loss: 1.495081901550293
Epoch 830, training loss: 0.6602798700332642 = 0.011606120504438877 + 0.1 * 6.4867377281188965
Epoch 830, val loss: 1.5037202835083008
Epoch 840, training loss: 0.6581186056137085 = 0.011233961209654808 + 0.1 * 6.468846321105957
Epoch 840, val loss: 1.5122547149658203
Epoch 850, training loss: 0.6571248173713684 = 0.010880882851779461 + 0.1 * 6.462439060211182
Epoch 850, val loss: 1.5209259986877441
Epoch 860, training loss: 0.6582481265068054 = 0.01054514292627573 + 0.1 * 6.477029800415039
Epoch 860, val loss: 1.5290589332580566
Epoch 870, training loss: 0.6573545932769775 = 0.010225621983408928 + 0.1 * 6.471290111541748
Epoch 870, val loss: 1.5369690656661987
Epoch 880, training loss: 0.6553378701210022 = 0.009922798722982407 + 0.1 * 6.454150676727295
Epoch 880, val loss: 1.5451010465621948
Epoch 890, training loss: 0.6562569737434387 = 0.009633954614400864 + 0.1 * 6.4662299156188965
Epoch 890, val loss: 1.5528502464294434
Epoch 900, training loss: 0.6544723510742188 = 0.009358664974570274 + 0.1 * 6.451137065887451
Epoch 900, val loss: 1.560579776763916
Epoch 910, training loss: 0.6552789211273193 = 0.009096306748688221 + 0.1 * 6.461825847625732
Epoch 910, val loss: 1.5681300163269043
Epoch 920, training loss: 0.6542927622795105 = 0.008845106698572636 + 0.1 * 6.454476356506348
Epoch 920, val loss: 1.5752755403518677
Epoch 930, training loss: 0.6536787748336792 = 0.008606648072600365 + 0.1 * 6.45072078704834
Epoch 930, val loss: 1.5830729007720947
Epoch 940, training loss: 0.6524644494056702 = 0.00837736576795578 + 0.1 * 6.440870761871338
Epoch 940, val loss: 1.5900518894195557
Epoch 950, training loss: 0.6547687649726868 = 0.008158327080309391 + 0.1 * 6.466104507446289
Epoch 950, val loss: 1.5971356630325317
Epoch 960, training loss: 0.6523535847663879 = 0.00794794037938118 + 0.1 * 6.444056510925293
Epoch 960, val loss: 1.6036880016326904
Epoch 970, training loss: 0.6522608995437622 = 0.007747524883598089 + 0.1 * 6.445133686065674
Epoch 970, val loss: 1.6108531951904297
Epoch 980, training loss: 0.6525235772132874 = 0.007555087096989155 + 0.1 * 6.449685096740723
Epoch 980, val loss: 1.6173869371414185
Epoch 990, training loss: 0.6510928273200989 = 0.007370674051344395 + 0.1 * 6.437221527099609
Epoch 990, val loss: 1.6241670846939087
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 2.7997913360595703 = 1.940106749534607 + 0.1 * 8.596844673156738
Epoch 0, val loss: 1.9352437257766724
Epoch 10, training loss: 2.790820837020874 = 1.9311426877975464 + 0.1 * 8.596780776977539
Epoch 10, val loss: 1.9258944988250732
Epoch 20, training loss: 2.779792308807373 = 1.9201483726501465 + 0.1 * 8.596440315246582
Epoch 20, val loss: 1.9142781496047974
Epoch 30, training loss: 2.763856887817383 = 1.9044606685638428 + 0.1 * 8.593962669372559
Epoch 30, val loss: 1.8978393077850342
Epoch 40, training loss: 2.7380757331848145 = 1.8807590007781982 + 0.1 * 8.573165893554688
Epoch 40, val loss: 1.8733525276184082
Epoch 50, training loss: 2.6920294761657715 = 1.8472270965576172 + 0.1 * 8.448022842407227
Epoch 50, val loss: 1.840356707572937
Epoch 60, training loss: 2.6169075965881348 = 1.8095440864562988 + 0.1 * 8.073634147644043
Epoch 60, val loss: 1.8066447973251343
Epoch 70, training loss: 2.547598361968994 = 1.7758708000183105 + 0.1 * 7.717275619506836
Epoch 70, val loss: 1.7786141633987427
Epoch 80, training loss: 2.4780843257904053 = 1.7409651279449463 + 0.1 * 7.371191501617432
Epoch 80, val loss: 1.7489702701568604
Epoch 90, training loss: 2.4192280769348145 = 1.6979578733444214 + 0.1 * 7.212701320648193
Epoch 90, val loss: 1.711167335510254
Epoch 100, training loss: 2.352273941040039 = 1.6402742862701416 + 0.1 * 7.119996070861816
Epoch 100, val loss: 1.6601238250732422
Epoch 110, training loss: 2.271834135055542 = 1.5664485692977905 + 0.1 * 7.053855895996094
Epoch 110, val loss: 1.5965163707733154
Epoch 120, training loss: 2.1815903186798096 = 1.4815130233764648 + 0.1 * 7.000772953033447
Epoch 120, val loss: 1.5265463590621948
Epoch 130, training loss: 2.088939905166626 = 1.393487572669983 + 0.1 * 6.954524040222168
Epoch 130, val loss: 1.4573158025741577
Epoch 140, training loss: 1.9973660707473755 = 1.3049570322036743 + 0.1 * 6.924090385437012
Epoch 140, val loss: 1.3905521631240845
Epoch 150, training loss: 1.9077463150024414 = 1.217119574546814 + 0.1 * 6.906267166137695
Epoch 150, val loss: 1.3258401155471802
Epoch 160, training loss: 1.8224449157714844 = 1.1341407299041748 + 0.1 * 6.883042335510254
Epoch 160, val loss: 1.2658370733261108
Epoch 170, training loss: 1.7453279495239258 = 1.058681845664978 + 0.1 * 6.866461753845215
Epoch 170, val loss: 1.2124029397964478
Epoch 180, training loss: 1.6772898435592651 = 0.9918028712272644 + 0.1 * 6.854869842529297
Epoch 180, val loss: 1.166431188583374
Epoch 190, training loss: 1.6160376071929932 = 0.9315513968467712 + 0.1 * 6.84486198425293
Epoch 190, val loss: 1.1263606548309326
Epoch 200, training loss: 1.5586354732513428 = 0.8748776912689209 + 0.1 * 6.837576866149902
Epoch 200, val loss: 1.0889651775360107
Epoch 210, training loss: 1.5018384456634521 = 0.8189584016799927 + 0.1 * 6.828799724578857
Epoch 210, val loss: 1.0525933504104614
Epoch 220, training loss: 1.443461537361145 = 0.760919451713562 + 0.1 * 6.82542085647583
Epoch 220, val loss: 1.0142818689346313
Epoch 230, training loss: 1.3817106485366821 = 0.7001987099647522 + 0.1 * 6.81511926651001
Epoch 230, val loss: 0.974285900592804
Epoch 240, training loss: 1.3177955150604248 = 0.6369366645812988 + 0.1 * 6.808588027954102
Epoch 240, val loss: 0.933099627494812
Epoch 250, training loss: 1.253281831741333 = 0.5728276968002319 + 0.1 * 6.80454158782959
Epoch 250, val loss: 0.893140435218811
Epoch 260, training loss: 1.1904044151306152 = 0.510774552822113 + 0.1 * 6.796298027038574
Epoch 260, val loss: 0.8572279214859009
Epoch 270, training loss: 1.1327987909317017 = 0.4532947242259979 + 0.1 * 6.795040130615234
Epoch 270, val loss: 0.8272732496261597
Epoch 280, training loss: 1.0804200172424316 = 0.4016288220882416 + 0.1 * 6.787911891937256
Epoch 280, val loss: 0.8038132786750793
Epoch 290, training loss: 1.035699486732483 = 0.3561902940273285 + 0.1 * 6.7950921058654785
Epoch 290, val loss: 0.786318302154541
Epoch 300, training loss: 0.9959924817085266 = 0.31703388690948486 + 0.1 * 6.789586067199707
Epoch 300, val loss: 0.7740628123283386
Epoch 310, training loss: 0.9607764482498169 = 0.28289031982421875 + 0.1 * 6.778861045837402
Epoch 310, val loss: 0.765679121017456
Epoch 320, training loss: 0.9298582673072815 = 0.25251829624176025 + 0.1 * 6.773399353027344
Epoch 320, val loss: 0.7601869702339172
Epoch 330, training loss: 0.9019845724105835 = 0.22509489953517914 + 0.1 * 6.768896579742432
Epoch 330, val loss: 0.756866991519928
Epoch 340, training loss: 0.878212571144104 = 0.20012174546718597 + 0.1 * 6.780908107757568
Epoch 340, val loss: 0.7550584077835083
Epoch 350, training loss: 0.8538313508033752 = 0.17751985788345337 + 0.1 * 6.763114929199219
Epoch 350, val loss: 0.7548863291740417
Epoch 360, training loss: 0.832861065864563 = 0.1570332944393158 + 0.1 * 6.758277416229248
Epoch 360, val loss: 0.7558438777923584
Epoch 370, training loss: 0.8148202896118164 = 0.13857302069664001 + 0.1 * 6.762473106384277
Epoch 370, val loss: 0.7579750418663025
Epoch 380, training loss: 0.7969964146614075 = 0.12224054336547852 + 0.1 * 6.74755859375
Epoch 380, val loss: 0.7612740993499756
Epoch 390, training loss: 0.7821973562240601 = 0.10788846760988235 + 0.1 * 6.743088722229004
Epoch 390, val loss: 0.7656269073486328
Epoch 400, training loss: 0.7689965963363647 = 0.09534445405006409 + 0.1 * 6.736521244049072
Epoch 400, val loss: 0.7710269093513489
Epoch 410, training loss: 0.7590271234512329 = 0.08445319533348083 + 0.1 * 6.745738983154297
Epoch 410, val loss: 0.7773975729942322
Epoch 420, training loss: 0.748760998249054 = 0.07514394819736481 + 0.1 * 6.736170768737793
Epoch 420, val loss: 0.7842928767204285
Epoch 430, training loss: 0.7390622496604919 = 0.06716183573007584 + 0.1 * 6.719003677368164
Epoch 430, val loss: 0.7914673686027527
Epoch 440, training loss: 0.7317723631858826 = 0.060278747230768204 + 0.1 * 6.714935779571533
Epoch 440, val loss: 0.799042284488678
Epoch 450, training loss: 0.7248364686965942 = 0.054353125393390656 + 0.1 * 6.704833507537842
Epoch 450, val loss: 0.8068246245384216
Epoch 460, training loss: 0.7190254926681519 = 0.049232035875320435 + 0.1 * 6.697934150695801
Epoch 460, val loss: 0.8146538138389587
Epoch 470, training loss: 0.7147448062896729 = 0.044790979474782944 + 0.1 * 6.699538230895996
Epoch 470, val loss: 0.8225079774856567
Epoch 480, training loss: 0.7099075317382812 = 0.04094202071428299 + 0.1 * 6.68965482711792
Epoch 480, val loss: 0.8304743766784668
Epoch 490, training loss: 0.7077792286872864 = 0.03756938502192497 + 0.1 * 6.7020978927612305
Epoch 490, val loss: 0.8380782604217529
Epoch 500, training loss: 0.7024657726287842 = 0.03461472690105438 + 0.1 * 6.6785101890563965
Epoch 500, val loss: 0.8456767201423645
Epoch 510, training loss: 0.6980019807815552 = 0.03200237825512886 + 0.1 * 6.659996032714844
Epoch 510, val loss: 0.8532138466835022
Epoch 520, training loss: 0.6979678869247437 = 0.029679052531719208 + 0.1 * 6.682888031005859
Epoch 520, val loss: 0.8603994846343994
Epoch 530, training loss: 0.6929471492767334 = 0.027615385130047798 + 0.1 * 6.653317451477051
Epoch 530, val loss: 0.8677896857261658
Epoch 540, training loss: 0.6896539926528931 = 0.02576591819524765 + 0.1 * 6.638880729675293
Epoch 540, val loss: 0.8748661875724792
Epoch 550, training loss: 0.688902735710144 = 0.024101344868540764 + 0.1 * 6.648014068603516
Epoch 550, val loss: 0.881804347038269
Epoch 560, training loss: 0.6865116953849792 = 0.022604217752814293 + 0.1 * 6.639074802398682
Epoch 560, val loss: 0.8883948922157288
Epoch 570, training loss: 0.6842942833900452 = 0.021257417276501656 + 0.1 * 6.630368709564209
Epoch 570, val loss: 0.8950859308242798
Epoch 580, training loss: 0.6819174885749817 = 0.020034313201904297 + 0.1 * 6.618831634521484
Epoch 580, val loss: 0.901587963104248
Epoch 590, training loss: 0.6822308301925659 = 0.018919989466667175 + 0.1 * 6.633108139038086
Epoch 590, val loss: 0.9076254367828369
Epoch 600, training loss: 0.6794950366020203 = 0.01790882833302021 + 0.1 * 6.615861892700195
Epoch 600, val loss: 0.9139497876167297
Epoch 610, training loss: 0.6769887804985046 = 0.016982562839984894 + 0.1 * 6.600062370300293
Epoch 610, val loss: 0.9199148416519165
Epoch 620, training loss: 0.6776331663131714 = 0.016130654141306877 + 0.1 * 6.615025043487549
Epoch 620, val loss: 0.9257507920265198
Epoch 630, training loss: 0.674827516078949 = 0.015347368083894253 + 0.1 * 6.594801425933838
Epoch 630, val loss: 0.9314407110214233
Epoch 640, training loss: 0.6741669774055481 = 0.01462574489414692 + 0.1 * 6.595412254333496
Epoch 640, val loss: 0.9371833801269531
Epoch 650, training loss: 0.6723051071166992 = 0.013958146795630455 + 0.1 * 6.583469390869141
Epoch 650, val loss: 0.9424871802330017
Epoch 660, training loss: 0.670907199382782 = 0.013340643607079983 + 0.1 * 6.575665473937988
Epoch 660, val loss: 0.9480051398277283
Epoch 670, training loss: 0.6694610118865967 = 0.012766742147505283 + 0.1 * 6.5669426918029785
Epoch 670, val loss: 0.9529979825019836
Epoch 680, training loss: 0.6688405871391296 = 0.01223373506218195 + 0.1 * 6.566068649291992
Epoch 680, val loss: 0.9581595063209534
Epoch 690, training loss: 0.6688226461410522 = 0.011737184599041939 + 0.1 * 6.570854663848877
Epoch 690, val loss: 0.9632061719894409
Epoch 700, training loss: 0.6666275858879089 = 0.011273761279881 + 0.1 * 6.5535383224487305
Epoch 700, val loss: 0.9679726958274841
Epoch 710, training loss: 0.6662684082984924 = 0.010840573348104954 + 0.1 * 6.5542778968811035
Epoch 710, val loss: 0.972804069519043
Epoch 720, training loss: 0.6674617528915405 = 0.010433865711092949 + 0.1 * 6.570278644561768
Epoch 720, val loss: 0.977353036403656
Epoch 730, training loss: 0.6640298366546631 = 0.010051648132503033 + 0.1 * 6.53978157043457
Epoch 730, val loss: 0.9818874597549438
Epoch 740, training loss: 0.6659638285636902 = 0.009692803025245667 + 0.1 * 6.562710285186768
Epoch 740, val loss: 0.9863515496253967
Epoch 750, training loss: 0.6639710664749146 = 0.009355701506137848 + 0.1 * 6.546153545379639
Epoch 750, val loss: 0.9905612468719482
Epoch 760, training loss: 0.6622671484947205 = 0.009038678370416164 + 0.1 * 6.532284736633301
Epoch 760, val loss: 0.9948906898498535
Epoch 770, training loss: 0.6615694165229797 = 0.00873938575387001 + 0.1 * 6.528299808502197
Epoch 770, val loss: 0.9990607500076294
Epoch 780, training loss: 0.6609603762626648 = 0.008455946110188961 + 0.1 * 6.5250444412231445
Epoch 780, val loss: 1.003233551979065
Epoch 790, training loss: 0.6602694392204285 = 0.008187273517251015 + 0.1 * 6.520821571350098
Epoch 790, val loss: 1.007111668586731
Epoch 800, training loss: 0.6598054766654968 = 0.007933367975056171 + 0.1 * 6.518720626831055
Epoch 800, val loss: 1.0111020803451538
Epoch 810, training loss: 0.6598384380340576 = 0.007692153565585613 + 0.1 * 6.521462440490723
Epoch 810, val loss: 1.0150388479232788
Epoch 820, training loss: 0.659994900226593 = 0.0074630966410040855 + 0.1 * 6.525317668914795
Epoch 820, val loss: 1.0187838077545166
Epoch 830, training loss: 0.6576948165893555 = 0.007246173452585936 + 0.1 * 6.504486083984375
Epoch 830, val loss: 1.022388219833374
Epoch 840, training loss: 0.6585854887962341 = 0.007040233351290226 + 0.1 * 6.5154523849487305
Epoch 840, val loss: 1.0261696577072144
Epoch 850, training loss: 0.6566898226737976 = 0.006843310780823231 + 0.1 * 6.498465061187744
Epoch 850, val loss: 1.0297002792358398
Epoch 860, training loss: 0.6582182049751282 = 0.006655386183410883 + 0.1 * 6.515627861022949
Epoch 860, val loss: 1.0332326889038086
Epoch 870, training loss: 0.6569775938987732 = 0.006476600654423237 + 0.1 * 6.50501012802124
Epoch 870, val loss: 1.0366172790527344
Epoch 880, training loss: 0.6568700671195984 = 0.00630566943436861 + 0.1 * 6.505643367767334
Epoch 880, val loss: 1.0400177240371704
Epoch 890, training loss: 0.6569357514381409 = 0.006142687052488327 + 0.1 * 6.507930755615234
Epoch 890, val loss: 1.0432941913604736
Epoch 900, training loss: 0.6563544869422913 = 0.0059869554825127125 + 0.1 * 6.50367546081543
Epoch 900, val loss: 1.0464259386062622
Epoch 910, training loss: 0.6543933153152466 = 0.005838257260620594 + 0.1 * 6.485550403594971
Epoch 910, val loss: 1.049643635749817
Epoch 920, training loss: 0.6542539596557617 = 0.005695853382349014 + 0.1 * 6.485580921173096
Epoch 920, val loss: 1.0529295206069946
Epoch 930, training loss: 0.6549206376075745 = 0.00555883813649416 + 0.1 * 6.493617534637451
Epoch 930, val loss: 1.055856466293335
Epoch 940, training loss: 0.6537635922431946 = 0.005427505820989609 + 0.1 * 6.483360767364502
Epoch 940, val loss: 1.0588407516479492
Epoch 950, training loss: 0.6528974771499634 = 0.005302154924720526 + 0.1 * 6.475953578948975
Epoch 950, val loss: 1.061976432800293
Epoch 960, training loss: 0.6540979743003845 = 0.005181020125746727 + 0.1 * 6.489169597625732
Epoch 960, val loss: 1.0648967027664185
Epoch 970, training loss: 0.6537162065505981 = 0.0050648911856114864 + 0.1 * 6.486513137817383
Epoch 970, val loss: 1.0675745010375977
Epoch 980, training loss: 0.652405321598053 = 0.004953476600348949 + 0.1 * 6.474518299102783
Epoch 980, val loss: 1.0705288648605347
Epoch 990, training loss: 0.6527597308158875 = 0.004846442956477404 + 0.1 * 6.479132652282715
Epoch 990, val loss: 1.073410987854004
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 2.8009886741638184 = 1.9413084983825684 + 0.1 * 8.5968017578125
Epoch 0, val loss: 1.9514503479003906
Epoch 10, training loss: 2.7911839485168457 = 1.9315176010131836 + 0.1 * 8.596662521362305
Epoch 10, val loss: 1.9410852193832397
Epoch 20, training loss: 2.77880859375 = 1.9192367792129517 + 0.1 * 8.595718383789062
Epoch 20, val loss: 1.9277626276016235
Epoch 30, training loss: 2.7606093883514404 = 1.9018898010253906 + 0.1 * 8.587196350097656
Epoch 30, val loss: 1.9088828563690186
Epoch 40, training loss: 2.7293901443481445 = 1.876447319984436 + 0.1 * 8.52942943572998
Epoch 40, val loss: 1.8814841508865356
Epoch 50, training loss: 2.6633658409118652 = 1.8433115482330322 + 0.1 * 8.200542449951172
Epoch 50, val loss: 1.8476232290267944
Epoch 60, training loss: 2.6063289642333984 = 1.8100353479385376 + 0.1 * 7.962937355041504
Epoch 60, val loss: 1.815818190574646
Epoch 70, training loss: 2.5309877395629883 = 1.7832750082015991 + 0.1 * 7.477126598358154
Epoch 70, val loss: 1.7911077737808228
Epoch 80, training loss: 2.4703080654144287 = 1.7565646171569824 + 0.1 * 7.137434959411621
Epoch 80, val loss: 1.766176462173462
Epoch 90, training loss: 2.4251232147216797 = 1.7219202518463135 + 0.1 * 7.032029628753662
Epoch 90, val loss: 1.7349762916564941
Epoch 100, training loss: 2.371375799179077 = 1.6741808652877808 + 0.1 * 6.971949100494385
Epoch 100, val loss: 1.693533182144165
Epoch 110, training loss: 2.303205966949463 = 1.6111184358596802 + 0.1 * 6.92087459564209
Epoch 110, val loss: 1.6394397020339966
Epoch 120, training loss: 2.2229907512664795 = 1.5348297357559204 + 0.1 * 6.8816094398498535
Epoch 120, val loss: 1.574326753616333
Epoch 130, training loss: 2.1365537643432617 = 1.4514014720916748 + 0.1 * 6.851523399353027
Epoch 130, val loss: 1.505116581916809
Epoch 140, training loss: 2.048632860183716 = 1.365648865699768 + 0.1 * 6.829840660095215
Epoch 140, val loss: 1.4366812705993652
Epoch 150, training loss: 1.9619295597076416 = 1.2803757190704346 + 0.1 * 6.81553840637207
Epoch 150, val loss: 1.3709596395492554
Epoch 160, training loss: 1.8770878314971924 = 1.1968708038330078 + 0.1 * 6.802170276641846
Epoch 160, val loss: 1.3085604906082153
Epoch 170, training loss: 1.7961434125900269 = 1.1174203157424927 + 0.1 * 6.787230968475342
Epoch 170, val loss: 1.2505415678024292
Epoch 180, training loss: 1.7217700481414795 = 1.0441688299179077 + 0.1 * 6.776012420654297
Epoch 180, val loss: 1.1985725164413452
Epoch 190, training loss: 1.6536755561828613 = 0.9777035713195801 + 0.1 * 6.7597198486328125
Epoch 190, val loss: 1.1518067121505737
Epoch 200, training loss: 1.592000126838684 = 0.9162024259567261 + 0.1 * 6.75797700881958
Epoch 200, val loss: 1.1086881160736084
Epoch 210, training loss: 1.5319312810897827 = 0.8589198589324951 + 0.1 * 6.730113983154297
Epoch 210, val loss: 1.0687813758850098
Epoch 220, training loss: 1.475439429283142 = 0.8036543130874634 + 0.1 * 6.717851161956787
Epoch 220, val loss: 1.0301706790924072
Epoch 230, training loss: 1.4206047058105469 = 0.7499626874923706 + 0.1 * 6.7064208984375
Epoch 230, val loss: 0.9932951927185059
Epoch 240, training loss: 1.367429494857788 = 0.6982389688491821 + 0.1 * 6.691905975341797
Epoch 240, val loss: 0.9586910605430603
Epoch 250, training loss: 1.3172061443328857 = 0.648808479309082 + 0.1 * 6.683975696563721
Epoch 250, val loss: 0.9278305172920227
Epoch 260, training loss: 1.2697213888168335 = 0.6022338271141052 + 0.1 * 6.674875259399414
Epoch 260, val loss: 0.9014660716056824
Epoch 270, training loss: 1.2258927822113037 = 0.5584344863891602 + 0.1 * 6.674582481384277
Epoch 270, val loss: 0.8795318603515625
Epoch 280, training loss: 1.182513952255249 = 0.5172232389450073 + 0.1 * 6.652907848358154
Epoch 280, val loss: 0.8616708517074585
Epoch 290, training loss: 1.1424832344055176 = 0.47810423374176025 + 0.1 * 6.643790245056152
Epoch 290, val loss: 0.8468924164772034
Epoch 300, training loss: 1.1042840480804443 = 0.441008061170578 + 0.1 * 6.6327595710754395
Epoch 300, val loss: 0.8351436853408813
Epoch 310, training loss: 1.0690276622772217 = 0.4061276614665985 + 0.1 * 6.628999710083008
Epoch 310, val loss: 0.826063871383667
Epoch 320, training loss: 1.0359241962432861 = 0.3739628493785858 + 0.1 * 6.6196136474609375
Epoch 320, val loss: 0.8197986483573914
Epoch 330, training loss: 1.006353735923767 = 0.344322144985199 + 0.1 * 6.620316028594971
Epoch 330, val loss: 0.8157845735549927
Epoch 340, training loss: 0.9784286022186279 = 0.3172948658466339 + 0.1 * 6.611336708068848
Epoch 340, val loss: 0.8139207363128662
Epoch 350, training loss: 0.9532239437103271 = 0.2924581468105316 + 0.1 * 6.607658386230469
Epoch 350, val loss: 0.8138180375099182
Epoch 360, training loss: 0.9292603731155396 = 0.26916834712028503 + 0.1 * 6.600920677185059
Epoch 360, val loss: 0.8151088356971741
Epoch 370, training loss: 0.906354546546936 = 0.24705123901367188 + 0.1 * 6.5930328369140625
Epoch 370, val loss: 0.8172433972358704
Epoch 380, training loss: 0.8854281306266785 = 0.22589099407196045 + 0.1 * 6.595371246337891
Epoch 380, val loss: 0.8204165697097778
Epoch 390, training loss: 0.8652641177177429 = 0.20558768510818481 + 0.1 * 6.596764087677002
Epoch 390, val loss: 0.8241102695465088
Epoch 400, training loss: 0.844914972782135 = 0.18633230030536652 + 0.1 * 6.585826873779297
Epoch 400, val loss: 0.8286465406417847
Epoch 410, training loss: 0.8260762691497803 = 0.16834409534931183 + 0.1 * 6.577321529388428
Epoch 410, val loss: 0.8336839079856873
Epoch 420, training loss: 0.8092690110206604 = 0.15186399221420288 + 0.1 * 6.574049949645996
Epoch 420, val loss: 0.839764416217804
Epoch 430, training loss: 0.7945318818092346 = 0.13692240417003632 + 0.1 * 6.576094627380371
Epoch 430, val loss: 0.8467252850532532
Epoch 440, training loss: 0.7814722657203674 = 0.12360230833292007 + 0.1 * 6.578699111938477
Epoch 440, val loss: 0.8544671535491943
Epoch 450, training loss: 0.7673855423927307 = 0.11181710660457611 + 0.1 * 6.5556840896606445
Epoch 450, val loss: 0.8631552457809448
Epoch 460, training loss: 0.7571491599082947 = 0.10137752443552017 + 0.1 * 6.557715892791748
Epoch 460, val loss: 0.8727201223373413
Epoch 470, training loss: 0.7471164464950562 = 0.09215757995843887 + 0.1 * 6.549588203430176
Epoch 470, val loss: 0.8829270005226135
Epoch 480, training loss: 0.738821268081665 = 0.08402140438556671 + 0.1 * 6.547998428344727
Epoch 480, val loss: 0.8939470648765564
Epoch 490, training loss: 0.7342311143875122 = 0.0768112987279892 + 0.1 * 6.574197769165039
Epoch 490, val loss: 0.9053269028663635
Epoch 500, training loss: 0.7241318225860596 = 0.07043558359146118 + 0.1 * 6.536962032318115
Epoch 500, val loss: 0.9172030687332153
Epoch 510, training loss: 0.7182592749595642 = 0.0647583082318306 + 0.1 * 6.535009860992432
Epoch 510, val loss: 0.9293486475944519
Epoch 520, training loss: 0.7126635909080505 = 0.059697628021240234 + 0.1 * 6.529659748077393
Epoch 520, val loss: 0.9417405724525452
Epoch 530, training loss: 0.7076696753501892 = 0.055178962647914886 + 0.1 * 6.524907112121582
Epoch 530, val loss: 0.9543244242668152
Epoch 540, training loss: 0.7048958539962769 = 0.05111762508749962 + 0.1 * 6.537782669067383
Epoch 540, val loss: 0.966956377029419
Epoch 550, training loss: 0.7006811499595642 = 0.04746942222118378 + 0.1 * 6.532116889953613
Epoch 550, val loss: 0.9795112609863281
Epoch 560, training loss: 0.6980032324790955 = 0.0441848449409008 + 0.1 * 6.538183689117432
Epoch 560, val loss: 0.9920459985733032
Epoch 570, training loss: 0.6932835578918457 = 0.04122217744588852 + 0.1 * 6.520613670349121
Epoch 570, val loss: 1.0045310258865356
Epoch 580, training loss: 0.6896266937255859 = 0.038537073880434036 + 0.1 * 6.5108962059021
Epoch 580, val loss: 1.0168074369430542
Epoch 590, training loss: 0.6875281929969788 = 0.0360960029065609 + 0.1 * 6.514322280883789
Epoch 590, val loss: 1.0289158821105957
Epoch 600, training loss: 0.6840806603431702 = 0.03387583792209625 + 0.1 * 6.502048015594482
Epoch 600, val loss: 1.0410352945327759
Epoch 610, training loss: 0.683167040348053 = 0.03184715285897255 + 0.1 * 6.513198375701904
Epoch 610, val loss: 1.052736759185791
Epoch 620, training loss: 0.680090069770813 = 0.029994921758770943 + 0.1 * 6.500951290130615
Epoch 620, val loss: 1.06441068649292
Epoch 630, training loss: 0.6801825165748596 = 0.028298601508140564 + 0.1 * 6.518838882446289
Epoch 630, val loss: 1.0757167339324951
Epoch 640, training loss: 0.6759966611862183 = 0.026744399219751358 + 0.1 * 6.492522239685059
Epoch 640, val loss: 1.086890459060669
Epoch 650, training loss: 0.6740326285362244 = 0.02531379833817482 + 0.1 * 6.487188339233398
Epoch 650, val loss: 1.0978832244873047
Epoch 660, training loss: 0.6735431551933289 = 0.023993685841560364 + 0.1 * 6.495494842529297
Epoch 660, val loss: 1.108561396598816
Epoch 670, training loss: 0.6715923547744751 = 0.02277657762169838 + 0.1 * 6.488157272338867
Epoch 670, val loss: 1.1191259622573853
Epoch 680, training loss: 0.6692014932632446 = 0.021651331335306168 + 0.1 * 6.475501537322998
Epoch 680, val loss: 1.1293694972991943
Epoch 690, training loss: 0.6683075428009033 = 0.02060902677476406 + 0.1 * 6.476984977722168
Epoch 690, val loss: 1.1395553350448608
Epoch 700, training loss: 0.6675039529800415 = 0.01964067667722702 + 0.1 * 6.47863245010376
Epoch 700, val loss: 1.1492868661880493
Epoch 710, training loss: 0.6658028960227966 = 0.01874302700161934 + 0.1 * 6.470598220825195
Epoch 710, val loss: 1.159111738204956
Epoch 720, training loss: 0.6658799052238464 = 0.01790597476065159 + 0.1 * 6.479739665985107
Epoch 720, val loss: 1.1685887575149536
Epoch 730, training loss: 0.6645299792289734 = 0.01712607592344284 + 0.1 * 6.474038600921631
Epoch 730, val loss: 1.1779537200927734
Epoch 740, training loss: 0.6632113456726074 = 0.016398238018155098 + 0.1 * 6.468131065368652
Epoch 740, val loss: 1.18697988986969
Epoch 750, training loss: 0.6627534031867981 = 0.015718866139650345 + 0.1 * 6.4703450202941895
Epoch 750, val loss: 1.1959326267242432
Epoch 760, training loss: 0.6612588763237 = 0.015082544647157192 + 0.1 * 6.461763381958008
Epoch 760, val loss: 1.2046595811843872
Epoch 770, training loss: 0.6601688265800476 = 0.014486271888017654 + 0.1 * 6.4568257331848145
Epoch 770, val loss: 1.2132999897003174
Epoch 780, training loss: 0.6611447334289551 = 0.013925369828939438 + 0.1 * 6.472193241119385
Epoch 780, val loss: 1.2217146158218384
Epoch 790, training loss: 0.6596710681915283 = 0.01339772716164589 + 0.1 * 6.462733268737793
Epoch 790, val loss: 1.2298157215118408
Epoch 800, training loss: 0.6599519848823547 = 0.012903091497719288 + 0.1 * 6.470488548278809
Epoch 800, val loss: 1.2379401922225952
Epoch 810, training loss: 0.657799482345581 = 0.0124367019161582 + 0.1 * 6.453628063201904
Epoch 810, val loss: 1.2457700967788696
Epoch 820, training loss: 0.6581275463104248 = 0.011997250840067863 + 0.1 * 6.461303234100342
Epoch 820, val loss: 1.253627061843872
Epoch 830, training loss: 0.6570514440536499 = 0.011581094935536385 + 0.1 * 6.454702854156494
Epoch 830, val loss: 1.2611161470413208
Epoch 840, training loss: 0.6561220288276672 = 0.011189145967364311 + 0.1 * 6.449328422546387
Epoch 840, val loss: 1.2686294317245483
Epoch 850, training loss: 0.6564167737960815 = 0.010817749425768852 + 0.1 * 6.455989837646484
Epoch 850, val loss: 1.2759798765182495
Epoch 860, training loss: 0.654670000076294 = 0.010465910658240318 + 0.1 * 6.442040920257568
Epoch 860, val loss: 1.2831518650054932
Epoch 870, training loss: 0.6566342115402222 = 0.010132278315722942 + 0.1 * 6.465019702911377
Epoch 870, val loss: 1.2901556491851807
Epoch 880, training loss: 0.6538474559783936 = 0.009815900586545467 + 0.1 * 6.4403157234191895
Epoch 880, val loss: 1.2970656156539917
Epoch 890, training loss: 0.6542590260505676 = 0.009516134858131409 + 0.1 * 6.4474287033081055
Epoch 890, val loss: 1.3039038181304932
Epoch 900, training loss: 0.6529063582420349 = 0.009230490773916245 + 0.1 * 6.436758518218994
Epoch 900, val loss: 1.3104426860809326
Epoch 910, training loss: 0.6526525020599365 = 0.008959731087088585 + 0.1 * 6.436927795410156
Epoch 910, val loss: 1.317162036895752
Epoch 920, training loss: 0.6555152535438538 = 0.008701262064278126 + 0.1 * 6.4681396484375
Epoch 920, val loss: 1.323490023612976
Epoch 930, training loss: 0.6524659395217896 = 0.008454899303615093 + 0.1 * 6.440110206604004
Epoch 930, val loss: 1.329701542854309
Epoch 940, training loss: 0.6522778868675232 = 0.008220933377742767 + 0.1 * 6.4405694007873535
Epoch 940, val loss: 1.3360675573349
Epoch 950, training loss: 0.6506314873695374 = 0.007996664382517338 + 0.1 * 6.4263482093811035
Epoch 950, val loss: 1.3421757221221924
Epoch 960, training loss: 0.6513090133666992 = 0.007782434578984976 + 0.1 * 6.43526554107666
Epoch 960, val loss: 1.3482401371002197
Epoch 970, training loss: 0.6504205465316772 = 0.007576960138976574 + 0.1 * 6.428435802459717
Epoch 970, val loss: 1.3540793657302856
Epoch 980, training loss: 0.6511393189430237 = 0.007380621507763863 + 0.1 * 6.437587261199951
Epoch 980, val loss: 1.3599815368652344
Epoch 990, training loss: 0.650206446647644 = 0.007192593067884445 + 0.1 * 6.430138111114502
Epoch 990, val loss: 1.3655318021774292
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.816025303110174
The final CL Acc:0.77531, 0.01259, The final GNN Acc:0.81585, 0.00151
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13172])
remove edge: torch.Size([2, 7806])
updated graph: torch.Size([2, 10422])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8061363697052 = 1.9464521408081055 + 0.1 * 8.596841812133789
Epoch 0, val loss: 1.9380276203155518
Epoch 10, training loss: 2.7962417602539062 = 1.936564326286316 + 0.1 * 8.59677505493164
Epoch 10, val loss: 1.9286413192749023
Epoch 20, training loss: 2.784310817718506 = 1.9246747493743896 + 0.1 * 8.59636116027832
Epoch 20, val loss: 1.9170401096343994
Epoch 30, training loss: 2.767571449279785 = 1.9082705974578857 + 0.1 * 8.59300708770752
Epoch 30, val loss: 1.9008188247680664
Epoch 40, training loss: 2.740758180618286 = 1.884211540222168 + 0.1 * 8.565465927124023
Epoch 40, val loss: 1.8770745992660522
Epoch 50, training loss: 2.6903820037841797 = 1.8503615856170654 + 0.1 * 8.4002046585083
Epoch 50, val loss: 1.8448525667190552
Epoch 60, training loss: 2.619284152984619 = 1.810304880142212 + 0.1 * 8.089791297912598
Epoch 60, val loss: 1.8090975284576416
Epoch 70, training loss: 2.545064687728882 = 1.7692216634750366 + 0.1 * 7.7584309577941895
Epoch 70, val loss: 1.773777723312378
Epoch 80, training loss: 2.46344256401062 = 1.726216197013855 + 0.1 * 7.372263431549072
Epoch 80, val loss: 1.7370314598083496
Epoch 90, training loss: 2.3895175457000732 = 1.6741976737976074 + 0.1 * 7.153199195861816
Epoch 90, val loss: 1.6910171508789062
Epoch 100, training loss: 2.313608407974243 = 1.6067149639129639 + 0.1 * 7.068933963775635
Epoch 100, val loss: 1.630794882774353
Epoch 110, training loss: 2.2248427867889404 = 1.522679090499878 + 0.1 * 7.021636962890625
Epoch 110, val loss: 1.5581207275390625
Epoch 120, training loss: 2.125124454498291 = 1.4264953136444092 + 0.1 * 6.986290454864502
Epoch 120, val loss: 1.478026270866394
Epoch 130, training loss: 2.0207250118255615 = 1.324871301651001 + 0.1 * 6.958536148071289
Epoch 130, val loss: 1.3954793214797974
Epoch 140, training loss: 1.9168198108673096 = 1.222743034362793 + 0.1 * 6.940767765045166
Epoch 140, val loss: 1.3157488107681274
Epoch 150, training loss: 1.816383719444275 = 1.123652696609497 + 0.1 * 6.927309989929199
Epoch 150, val loss: 1.2395936250686646
Epoch 160, training loss: 1.721695899963379 = 1.0297797918319702 + 0.1 * 6.919161319732666
Epoch 160, val loss: 1.1679103374481201
Epoch 170, training loss: 1.6331312656402588 = 0.9427312016487122 + 0.1 * 6.904001235961914
Epoch 170, val loss: 1.1017807722091675
Epoch 180, training loss: 1.549527883529663 = 0.8604267835617065 + 0.1 * 6.8910112380981445
Epoch 180, val loss: 1.0391522645950317
Epoch 190, training loss: 1.4697319269180298 = 0.7817735075950623 + 0.1 * 6.879584312438965
Epoch 190, val loss: 0.9793741703033447
Epoch 200, training loss: 1.393059253692627 = 0.7062159776687622 + 0.1 * 6.868432998657227
Epoch 200, val loss: 0.9225683212280273
Epoch 210, training loss: 1.3199067115783691 = 0.6335873007774353 + 0.1 * 6.863194465637207
Epoch 210, val loss: 0.8692300915718079
Epoch 220, training loss: 1.2507822513580322 = 0.5654836297035217 + 0.1 * 6.852985858917236
Epoch 220, val loss: 0.8219032287597656
Epoch 230, training loss: 1.1867039203643799 = 0.5025517344474792 + 0.1 * 6.841521263122559
Epoch 230, val loss: 0.7815110683441162
Epoch 240, training loss: 1.130138635635376 = 0.4453977048397064 + 0.1 * 6.847409248352051
Epoch 240, val loss: 0.7490586042404175
Epoch 250, training loss: 1.0776026248931885 = 0.39445456862449646 + 0.1 * 6.831480026245117
Epoch 250, val loss: 0.7241437435150146
Epoch 260, training loss: 1.0310291051864624 = 0.3487793803215027 + 0.1 * 6.8224968910217285
Epoch 260, val loss: 0.7057053446769714
Epoch 270, training loss: 0.9906848073005676 = 0.30798524618148804 + 0.1 * 6.826995372772217
Epoch 270, val loss: 0.6926178932189941
Epoch 280, training loss: 0.9535945057868958 = 0.2720761299133301 + 0.1 * 6.815183639526367
Epoch 280, val loss: 0.6839863657951355
Epoch 290, training loss: 0.9208568334579468 = 0.24047157168388367 + 0.1 * 6.803852081298828
Epoch 290, val loss: 0.6790716648101807
Epoch 300, training loss: 0.8944233655929565 = 0.21269194781780243 + 0.1 * 6.817314147949219
Epoch 300, val loss: 0.6772945523262024
Epoch 310, training loss: 0.8681767582893372 = 0.18861377239227295 + 0.1 * 6.795629978179932
Epoch 310, val loss: 0.6782011389732361
Epoch 320, training loss: 0.8457897901535034 = 0.16761073470115662 + 0.1 * 6.781790733337402
Epoch 320, val loss: 0.6812750697135925
Epoch 330, training loss: 0.8288458585739136 = 0.14928002655506134 + 0.1 * 6.795658111572266
Epoch 330, val loss: 0.6860665082931519
Epoch 340, training loss: 0.8097237348556519 = 0.13341081142425537 + 0.1 * 6.763129234313965
Epoch 340, val loss: 0.6922115087509155
Epoch 350, training loss: 0.7955449223518372 = 0.11954762041568756 + 0.1 * 6.759973049163818
Epoch 350, val loss: 0.699565589427948
Epoch 360, training loss: 0.7833634614944458 = 0.10746455937623978 + 0.1 * 6.758988857269287
Epoch 360, val loss: 0.7077840566635132
Epoch 370, training loss: 0.7710747718811035 = 0.09689035266637802 + 0.1 * 6.741844177246094
Epoch 370, val loss: 0.7166416645050049
Epoch 380, training loss: 0.7620306611061096 = 0.08756992965936661 + 0.1 * 6.744606971740723
Epoch 380, val loss: 0.7261108756065369
Epoch 390, training loss: 0.7517075538635254 = 0.07938583940267563 + 0.1 * 6.723217010498047
Epoch 390, val loss: 0.7357692122459412
Epoch 400, training loss: 0.7446334958076477 = 0.07214901596307755 + 0.1 * 6.724844455718994
Epoch 400, val loss: 0.7455735802650452
Epoch 410, training loss: 0.7374774217605591 = 0.06574903428554535 + 0.1 * 6.717283725738525
Epoch 410, val loss: 0.7556084394454956
Epoch 420, training loss: 0.7309807538986206 = 0.06005917862057686 + 0.1 * 6.7092156410217285
Epoch 420, val loss: 0.7655686140060425
Epoch 430, training loss: 0.726841151714325 = 0.0549907349050045 + 0.1 * 6.718504428863525
Epoch 430, val loss: 0.7757673263549805
Epoch 440, training loss: 0.7195548415184021 = 0.05050065740942955 + 0.1 * 6.690542221069336
Epoch 440, val loss: 0.7856405973434448
Epoch 450, training loss: 0.7148227095603943 = 0.04649297147989273 + 0.1 * 6.683297157287598
Epoch 450, val loss: 0.7954671382904053
Epoch 460, training loss: 0.7127025723457336 = 0.042897455394268036 + 0.1 * 6.6980509757995605
Epoch 460, val loss: 0.8054120540618896
Epoch 470, training loss: 0.7096112966537476 = 0.0396881178021431 + 0.1 * 6.6992316246032715
Epoch 470, val loss: 0.8151470422744751
Epoch 480, training loss: 0.7035226821899414 = 0.03682776540517807 + 0.1 * 6.666949272155762
Epoch 480, val loss: 0.8245677351951599
Epoch 490, training loss: 0.7005470991134644 = 0.034253448247909546 + 0.1 * 6.662936687469482
Epoch 490, val loss: 0.8339608311653137
Epoch 500, training loss: 0.6971365213394165 = 0.03193215653300285 + 0.1 * 6.652043342590332
Epoch 500, val loss: 0.843238890171051
Epoch 510, training loss: 0.6946174502372742 = 0.02983953431248665 + 0.1 * 6.64777946472168
Epoch 510, val loss: 0.8523002862930298
Epoch 520, training loss: 0.6912800073623657 = 0.027951007708907127 + 0.1 * 6.633289813995361
Epoch 520, val loss: 0.8612145185470581
Epoch 530, training loss: 0.69167560338974 = 0.026237109676003456 + 0.1 * 6.654384613037109
Epoch 530, val loss: 0.8699429631233215
Epoch 540, training loss: 0.687242865562439 = 0.024684399366378784 + 0.1 * 6.625584125518799
Epoch 540, val loss: 0.8783945441246033
Epoch 550, training loss: 0.6862744092941284 = 0.023272603750228882 + 0.1 * 6.63001823425293
Epoch 550, val loss: 0.8865994215011597
Epoch 560, training loss: 0.682746171951294 = 0.02198365516960621 + 0.1 * 6.6076250076293945
Epoch 560, val loss: 0.8947635293006897
Epoch 570, training loss: 0.6840922832489014 = 0.020801540464162827 + 0.1 * 6.632907390594482
Epoch 570, val loss: 0.9026514887809753
Epoch 580, training loss: 0.6791328191757202 = 0.019717708230018616 + 0.1 * 6.594151020050049
Epoch 580, val loss: 0.9103905558586121
Epoch 590, training loss: 0.6771361827850342 = 0.018719136714935303 + 0.1 * 6.584170341491699
Epoch 590, val loss: 0.9179777503013611
Epoch 600, training loss: 0.6773105263710022 = 0.01779630407691002 + 0.1 * 6.595142364501953
Epoch 600, val loss: 0.9254006743431091
Epoch 610, training loss: 0.6744531393051147 = 0.01694989763200283 + 0.1 * 6.5750322341918945
Epoch 610, val loss: 0.932614803314209
Epoch 620, training loss: 0.6737575531005859 = 0.01616603322327137 + 0.1 * 6.5759148597717285
Epoch 620, val loss: 0.9396077394485474
Epoch 630, training loss: 0.6723765134811401 = 0.015435578301548958 + 0.1 * 6.569409370422363
Epoch 630, val loss: 0.9465361833572388
Epoch 640, training loss: 0.6712121367454529 = 0.014758839271962643 + 0.1 * 6.564533233642578
Epoch 640, val loss: 0.9532375931739807
Epoch 650, training loss: 0.672307014465332 = 0.014129464514553547 + 0.1 * 6.581775665283203
Epoch 650, val loss: 0.9598051905632019
Epoch 660, training loss: 0.6685837507247925 = 0.013542355969548225 + 0.1 * 6.550414085388184
Epoch 660, val loss: 0.966249942779541
Epoch 670, training loss: 0.6682556867599487 = 0.012993457727134228 + 0.1 * 6.552622318267822
Epoch 670, val loss: 0.9725456833839417
Epoch 680, training loss: 0.6662906408309937 = 0.012479319237172604 + 0.1 * 6.538113117218018
Epoch 680, val loss: 0.9786911606788635
Epoch 690, training loss: 0.6680458784103394 = 0.011997330002486706 + 0.1 * 6.560485363006592
Epoch 690, val loss: 0.9847833514213562
Epoch 700, training loss: 0.6651343703269958 = 0.01154780387878418 + 0.1 * 6.535865783691406
Epoch 700, val loss: 0.9905960559844971
Epoch 710, training loss: 0.6637395024299622 = 0.011125251650810242 + 0.1 * 6.526142120361328
Epoch 710, val loss: 0.9963467121124268
Epoch 720, training loss: 0.6642309427261353 = 0.010727647691965103 + 0.1 * 6.535032749176025
Epoch 720, val loss: 1.001968264579773
Epoch 730, training loss: 0.6634342074394226 = 0.010352208279073238 + 0.1 * 6.530819892883301
Epoch 730, val loss: 1.0074790716171265
Epoch 740, training loss: 0.6623865962028503 = 0.009999468922615051 + 0.1 * 6.523871421813965
Epoch 740, val loss: 1.012912392616272
Epoch 750, training loss: 0.663898229598999 = 0.009664868004620075 + 0.1 * 6.542333126068115
Epoch 750, val loss: 1.0182240009307861
Epoch 760, training loss: 0.6613651514053345 = 0.009349402971565723 + 0.1 * 6.520157337188721
Epoch 760, val loss: 1.0233993530273438
Epoch 770, training loss: 0.6610022783279419 = 0.009050780907273293 + 0.1 * 6.519515037536621
Epoch 770, val loss: 1.0285438299179077
Epoch 780, training loss: 0.6597505807876587 = 0.00876705627888441 + 0.1 * 6.5098347663879395
Epoch 780, val loss: 1.033439040184021
Epoch 790, training loss: 0.6599202752113342 = 0.008498718030750751 + 0.1 * 6.514215469360352
Epoch 790, val loss: 1.0383872985839844
Epoch 800, training loss: 0.6579093933105469 = 0.008243141695857048 + 0.1 * 6.496662139892578
Epoch 800, val loss: 1.0431854724884033
Epoch 810, training loss: 0.6588133573532104 = 0.008001136593520641 + 0.1 * 6.508121967315674
Epoch 810, val loss: 1.0479611158370972
Epoch 820, training loss: 0.6580691933631897 = 0.007770007010549307 + 0.1 * 6.502991676330566
Epoch 820, val loss: 1.0525695085525513
Epoch 830, training loss: 0.6582403779029846 = 0.007550101261585951 + 0.1 * 6.50690221786499
Epoch 830, val loss: 1.057142972946167
Epoch 840, training loss: 0.6567549109458923 = 0.007340224459767342 + 0.1 * 6.494146347045898
Epoch 840, val loss: 1.0615653991699219
Epoch 850, training loss: 0.6583721041679382 = 0.007140589877963066 + 0.1 * 6.512315273284912
Epoch 850, val loss: 1.0660290718078613
Epoch 860, training loss: 0.6550072431564331 = 0.006949394941329956 + 0.1 * 6.480578422546387
Epoch 860, val loss: 1.070298194885254
Epoch 870, training loss: 0.6556839346885681 = 0.006767253391444683 + 0.1 * 6.489166736602783
Epoch 870, val loss: 1.0745046138763428
Epoch 880, training loss: 0.6541514992713928 = 0.006592859514057636 + 0.1 * 6.4755859375
Epoch 880, val loss: 1.0787136554718018
Epoch 890, training loss: 0.6531954407691956 = 0.00642575416713953 + 0.1 * 6.467696666717529
Epoch 890, val loss: 1.082785725593567
Epoch 900, training loss: 0.6528341174125671 = 0.0062663136050105095 + 0.1 * 6.4656782150268555
Epoch 900, val loss: 1.0868498086929321
Epoch 910, training loss: 0.6531752943992615 = 0.006112818140536547 + 0.1 * 6.470624923706055
Epoch 910, val loss: 1.0907996892929077
Epoch 920, training loss: 0.6553456783294678 = 0.005965555552393198 + 0.1 * 6.493801593780518
Epoch 920, val loss: 1.0947636365890503
Epoch 930, training loss: 0.6530314087867737 = 0.005825150292366743 + 0.1 * 6.472062587738037
Epoch 930, val loss: 1.0985956192016602
Epoch 940, training loss: 0.6512715220451355 = 0.005689827725291252 + 0.1 * 6.455817222595215
Epoch 940, val loss: 1.1023856401443481
Epoch 950, training loss: 0.6527054309844971 = 0.005559644661843777 + 0.1 * 6.4714579582214355
Epoch 950, val loss: 1.1060932874679565
Epoch 960, training loss: 0.6512448787689209 = 0.005434561520814896 + 0.1 * 6.458102703094482
Epoch 960, val loss: 1.1097668409347534
Epoch 970, training loss: 0.6508577466011047 = 0.005314011126756668 + 0.1 * 6.455437183380127
Epoch 970, val loss: 1.1134201288223267
Epoch 980, training loss: 0.6498932242393494 = 0.005198854487389326 + 0.1 * 6.446943759918213
Epoch 980, val loss: 1.1169980764389038
Epoch 990, training loss: 0.6511241793632507 = 0.0050876559689641 + 0.1 * 6.460364818572998
Epoch 990, val loss: 1.1205164194107056
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 2.829212188720703 = 1.9695340394973755 + 0.1 * 8.596782684326172
Epoch 0, val loss: 1.9742923974990845
Epoch 10, training loss: 2.8182168006896973 = 1.9585518836975098 + 0.1 * 8.596648216247559
Epoch 10, val loss: 1.9632712602615356
Epoch 20, training loss: 2.804539680480957 = 1.9449641704559326 + 0.1 * 8.59575366973877
Epoch 20, val loss: 1.9494538307189941
Epoch 30, training loss: 2.7842252254486084 = 1.9254603385925293 + 0.1 * 8.58764934539795
Epoch 30, val loss: 1.9298322200775146
Epoch 40, training loss: 2.749157190322876 = 1.895747184753418 + 0.1 * 8.534100532531738
Epoch 40, val loss: 1.900330662727356
Epoch 50, training loss: 2.67877459526062 = 1.8543339967727661 + 0.1 * 8.244406700134277
Epoch 50, val loss: 1.8606125116348267
Epoch 60, training loss: 2.6091160774230957 = 1.8067951202392578 + 0.1 * 8.023209571838379
Epoch 60, val loss: 1.8178263902664185
Epoch 70, training loss: 2.5266261100769043 = 1.7641948461532593 + 0.1 * 7.624311923980713
Epoch 70, val loss: 1.7810195684432983
Epoch 80, training loss: 2.4466347694396973 = 1.723899006843567 + 0.1 * 7.227357387542725
Epoch 80, val loss: 1.7439243793487549
Epoch 90, training loss: 2.380068302154541 = 1.6734658479690552 + 0.1 * 7.066023826599121
Epoch 90, val loss: 1.6969069242477417
Epoch 100, training loss: 2.3060994148254395 = 1.6064893007278442 + 0.1 * 6.996099948883057
Epoch 100, val loss: 1.6370893716812134
Epoch 110, training loss: 2.217435359954834 = 1.5232397317886353 + 0.1 * 6.941955089569092
Epoch 110, val loss: 1.5658818483352661
Epoch 120, training loss: 2.119774580001831 = 1.4305614233016968 + 0.1 * 6.89213228225708
Epoch 120, val loss: 1.4877302646636963
Epoch 130, training loss: 2.0208802223205566 = 1.3364737033843994 + 0.1 * 6.844064235687256
Epoch 130, val loss: 1.409242033958435
Epoch 140, training loss: 1.927631139755249 = 1.2466309070587158 + 0.1 * 6.810001373291016
Epoch 140, val loss: 1.3356878757476807
Epoch 150, training loss: 1.8425898551940918 = 1.164504885673523 + 0.1 * 6.780850410461426
Epoch 150, val loss: 1.2688783407211304
Epoch 160, training loss: 1.7664597034454346 = 1.0904581546783447 + 0.1 * 6.76001501083374
Epoch 160, val loss: 1.2092925310134888
Epoch 170, training loss: 1.7005620002746582 = 1.0261062383651733 + 0.1 * 6.7445573806762695
Epoch 170, val loss: 1.1591829061508179
Epoch 180, training loss: 1.6435736417770386 = 0.9703636765480042 + 0.1 * 6.732099533081055
Epoch 180, val loss: 1.1173391342163086
Epoch 190, training loss: 1.592958927154541 = 0.9209374785423279 + 0.1 * 6.720213890075684
Epoch 190, val loss: 1.0811973810195923
Epoch 200, training loss: 1.5458824634552002 = 0.8746867179870605 + 0.1 * 6.711957931518555
Epoch 200, val loss: 1.0475772619247437
Epoch 210, training loss: 1.498130202293396 = 0.8283233046531677 + 0.1 * 6.698069095611572
Epoch 210, val loss: 1.012963891029358
Epoch 220, training loss: 1.4495036602020264 = 0.7799055576324463 + 0.1 * 6.695980072021484
Epoch 220, val loss: 0.976038932800293
Epoch 230, training loss: 1.3978837728500366 = 0.7291920185089111 + 0.1 * 6.686917304992676
Epoch 230, val loss: 0.9369887113571167
Epoch 240, training loss: 1.3437786102294922 = 0.6760464906692505 + 0.1 * 6.67732048034668
Epoch 240, val loss: 0.8961040377616882
Epoch 250, training loss: 1.2897350788116455 = 0.6223529577255249 + 0.1 * 6.673820972442627
Epoch 250, val loss: 0.8560699820518494
Epoch 260, training loss: 1.2366950511932373 = 0.570390522480011 + 0.1 * 6.663045406341553
Epoch 260, val loss: 0.8191068768501282
Epoch 270, training loss: 1.1887550354003906 = 0.5216152667999268 + 0.1 * 6.6713972091674805
Epoch 270, val loss: 0.7865452170372009
Epoch 280, training loss: 1.142892599105835 = 0.47706741094589233 + 0.1 * 6.658252239227295
Epoch 280, val loss: 0.7592726945877075
Epoch 290, training loss: 1.1016685962677002 = 0.43651798367500305 + 0.1 * 6.651505470275879
Epoch 290, val loss: 0.736883282661438
Epoch 300, training loss: 1.0639277696609497 = 0.39952608942985535 + 0.1 * 6.644016265869141
Epoch 300, val loss: 0.7187677025794983
Epoch 310, training loss: 1.0283684730529785 = 0.3650439977645874 + 0.1 * 6.63324499130249
Epoch 310, val loss: 0.703719437122345
Epoch 320, training loss: 0.9971789121627808 = 0.33254799246788025 + 0.1 * 6.646308898925781
Epoch 320, val loss: 0.6912364363670349
Epoch 330, training loss: 0.9652978181838989 = 0.30205902457237244 + 0.1 * 6.632387638092041
Epoch 330, val loss: 0.6810399889945984
Epoch 340, training loss: 0.9352192878723145 = 0.27318933606147766 + 0.1 * 6.620299339294434
Epoch 340, val loss: 0.6727617383003235
Epoch 350, training loss: 0.9086022973060608 = 0.24606557190418243 + 0.1 * 6.625367164611816
Epoch 350, val loss: 0.6664813160896301
Epoch 360, training loss: 0.882099986076355 = 0.22110293805599213 + 0.1 * 6.6099700927734375
Epoch 360, val loss: 0.662170946598053
Epoch 370, training loss: 0.8591632843017578 = 0.19836461544036865 + 0.1 * 6.6079864501953125
Epoch 370, val loss: 0.6598841547966003
Epoch 380, training loss: 0.838258683681488 = 0.17803098261356354 + 0.1 * 6.6022772789001465
Epoch 380, val loss: 0.6595204472541809
Epoch 390, training loss: 0.8191001415252686 = 0.16000966727733612 + 0.1 * 6.590904712677002
Epoch 390, val loss: 0.6609234809875488
Epoch 400, training loss: 0.8032759428024292 = 0.14406487345695496 + 0.1 * 6.592111110687256
Epoch 400, val loss: 0.6637606620788574
Epoch 410, training loss: 0.7887960076332092 = 0.1300096958875656 + 0.1 * 6.587862968444824
Epoch 410, val loss: 0.667900562286377
Epoch 420, training loss: 0.7758335471153259 = 0.11756496131420135 + 0.1 * 6.582685470581055
Epoch 420, val loss: 0.6729999780654907
Epoch 430, training loss: 0.766523540019989 = 0.10658539086580276 + 0.1 * 6.599381446838379
Epoch 430, val loss: 0.6787943243980408
Epoch 440, training loss: 0.7540817260742188 = 0.09690717607736588 + 0.1 * 6.5717453956604
Epoch 440, val loss: 0.6851508021354675
Epoch 450, training loss: 0.7447805404663086 = 0.0882926806807518 + 0.1 * 6.564878463745117
Epoch 450, val loss: 0.6919766068458557
Epoch 460, training loss: 0.7360987663269043 = 0.08061974495649338 + 0.1 * 6.554790019989014
Epoch 460, val loss: 0.6991317272186279
Epoch 470, training loss: 0.730132520198822 = 0.0737719014286995 + 0.1 * 6.563606262207031
Epoch 470, val loss: 0.7065823078155518
Epoch 480, training loss: 0.72287517786026 = 0.06766533106565475 + 0.1 * 6.552098274230957
Epoch 480, val loss: 0.7142037749290466
Epoch 490, training loss: 0.7169578671455383 = 0.06220755726099014 + 0.1 * 6.5475029945373535
Epoch 490, val loss: 0.7220104336738586
Epoch 500, training loss: 0.7111639976501465 = 0.05732684209942818 + 0.1 * 6.538371562957764
Epoch 500, val loss: 0.7299835085868835
Epoch 510, training loss: 0.7056453227996826 = 0.05296572670340538 + 0.1 * 6.526796340942383
Epoch 510, val loss: 0.7380083203315735
Epoch 520, training loss: 0.7028756737709045 = 0.04904124513268471 + 0.1 * 6.538343906402588
Epoch 520, val loss: 0.7461583614349365
Epoch 530, training loss: 0.698502779006958 = 0.04551185294985771 + 0.1 * 6.529909133911133
Epoch 530, val loss: 0.7543517351150513
Epoch 540, training loss: 0.6938993334770203 = 0.04233148321509361 + 0.1 * 6.515678882598877
Epoch 540, val loss: 0.7625483274459839
Epoch 550, training loss: 0.6909318566322327 = 0.03944867104291916 + 0.1 * 6.51483154296875
Epoch 550, val loss: 0.7707798480987549
Epoch 560, training loss: 0.6899068355560303 = 0.03684327378869057 + 0.1 * 6.530635356903076
Epoch 560, val loss: 0.7788340449333191
Epoch 570, training loss: 0.6860409379005432 = 0.034488238394260406 + 0.1 * 6.51552677154541
Epoch 570, val loss: 0.7867813110351562
Epoch 580, training loss: 0.6817296147346497 = 0.03234859183430672 + 0.1 * 6.493810176849365
Epoch 580, val loss: 0.79459148645401
Epoch 590, training loss: 0.6807363629341125 = 0.03039580024778843 + 0.1 * 6.503405570983887
Epoch 590, val loss: 0.8022831082344055
Epoch 600, training loss: 0.6772862076759338 = 0.02860865741968155 + 0.1 * 6.4867753982543945
Epoch 600, val loss: 0.809817373752594
Epoch 610, training loss: 0.6756007671356201 = 0.026969054713845253 + 0.1 * 6.486316680908203
Epoch 610, val loss: 0.817247211933136
Epoch 620, training loss: 0.6753453612327576 = 0.025466706603765488 + 0.1 * 6.498786926269531
Epoch 620, val loss: 0.8245282769203186
Epoch 630, training loss: 0.6721915602684021 = 0.02408764697611332 + 0.1 * 6.481038570404053
Epoch 630, val loss: 0.8316252827644348
Epoch 640, training loss: 0.6699894666671753 = 0.02281963638961315 + 0.1 * 6.47169828414917
Epoch 640, val loss: 0.8385955691337585
Epoch 650, training loss: 0.6705510020256042 = 0.02164626494050026 + 0.1 * 6.489047050476074
Epoch 650, val loss: 0.8454205393791199
Epoch 660, training loss: 0.6691827774047852 = 0.020561499521136284 + 0.1 * 6.486212730407715
Epoch 660, val loss: 0.852087676525116
Epoch 670, training loss: 0.6663717031478882 = 0.019559547305107117 + 0.1 * 6.468121528625488
Epoch 670, val loss: 0.8586081266403198
Epoch 680, training loss: 0.6653398871421814 = 0.018628187477588654 + 0.1 * 6.467116832733154
Epoch 680, val loss: 0.8649852871894836
Epoch 690, training loss: 0.6646298170089722 = 0.01776380091905594 + 0.1 * 6.468660354614258
Epoch 690, val loss: 0.8711914420127869
Epoch 700, training loss: 0.6631921529769897 = 0.016964109614491463 + 0.1 * 6.462279796600342
Epoch 700, val loss: 0.8772593140602112
Epoch 710, training loss: 0.6615103483200073 = 0.016218038275837898 + 0.1 * 6.452922821044922
Epoch 710, val loss: 0.883227527141571
Epoch 720, training loss: 0.6642454862594604 = 0.0155184930190444 + 0.1 * 6.487269878387451
Epoch 720, val loss: 0.889039158821106
Epoch 730, training loss: 0.6609025597572327 = 0.014867442660033703 + 0.1 * 6.46035099029541
Epoch 730, val loss: 0.8946809768676758
Epoch 740, training loss: 0.6590256094932556 = 0.014259492978453636 + 0.1 * 6.44766092300415
Epoch 740, val loss: 0.900283694267273
Epoch 750, training loss: 0.6590167284011841 = 0.01368822529911995 + 0.1 * 6.453285217285156
Epoch 750, val loss: 0.9057285785675049
Epoch 760, training loss: 0.6586236953735352 = 0.013151820749044418 + 0.1 * 6.454718589782715
Epoch 760, val loss: 0.9110682010650635
Epoch 770, training loss: 0.6561273336410522 = 0.01264851912856102 + 0.1 * 6.434788227081299
Epoch 770, val loss: 0.9162983298301697
Epoch 780, training loss: 0.6568086743354797 = 0.01217592041939497 + 0.1 * 6.4463276863098145
Epoch 780, val loss: 0.9214435815811157
Epoch 790, training loss: 0.6554979085922241 = 0.01173043716698885 + 0.1 * 6.437674522399902
Epoch 790, val loss: 0.926472008228302
Epoch 800, training loss: 0.6554532647132874 = 0.011310109868645668 + 0.1 * 6.441431522369385
Epoch 800, val loss: 0.9313963651657104
Epoch 810, training loss: 0.6536656618118286 = 0.010913732461631298 + 0.1 * 6.42751932144165
Epoch 810, val loss: 0.9362404346466064
Epoch 820, training loss: 0.6542116403579712 = 0.010539990849792957 + 0.1 * 6.436716079711914
Epoch 820, val loss: 0.941011369228363
Epoch 830, training loss: 0.6563805341720581 = 0.010185412131249905 + 0.1 * 6.46195125579834
Epoch 830, val loss: 0.9456174969673157
Epoch 840, training loss: 0.6529898643493652 = 0.009850217029452324 + 0.1 * 6.431396484375
Epoch 840, val loss: 0.9501602649688721
Epoch 850, training loss: 0.6523615121841431 = 0.009534021839499474 + 0.1 * 6.428275108337402
Epoch 850, val loss: 0.9546509981155396
Epoch 860, training loss: 0.6513934135437012 = 0.009232645854353905 + 0.1 * 6.421607494354248
Epoch 860, val loss: 0.9590525031089783
Epoch 870, training loss: 0.6517680287361145 = 0.0089457081630826 + 0.1 * 6.428223133087158
Epoch 870, val loss: 0.9633679389953613
Epoch 880, training loss: 0.6518396735191345 = 0.008673490025103092 + 0.1 * 6.431661605834961
Epoch 880, val loss: 0.9676181077957153
Epoch 890, training loss: 0.6497119665145874 = 0.008415180258452892 + 0.1 * 6.412967681884766
Epoch 890, val loss: 0.9717998504638672
Epoch 900, training loss: 0.6499842405319214 = 0.008169475011527538 + 0.1 * 6.418147563934326
Epoch 900, val loss: 0.9759435653686523
Epoch 910, training loss: 0.6521756052970886 = 0.00793418474495411 + 0.1 * 6.442413806915283
Epoch 910, val loss: 0.9799784421920776
Epoch 920, training loss: 0.650009274482727 = 0.007709992118179798 + 0.1 * 6.422992706298828
Epoch 920, val loss: 0.9839442372322083
Epoch 930, training loss: 0.6485244631767273 = 0.007497408427298069 + 0.1 * 6.4102702140808105
Epoch 930, val loss: 0.9878290295600891
Epoch 940, training loss: 0.6479325294494629 = 0.007294406648725271 + 0.1 * 6.406381130218506
Epoch 940, val loss: 0.991689145565033
Epoch 950, training loss: 0.6492149233818054 = 0.007099838461726904 + 0.1 * 6.4211506843566895
Epoch 950, val loss: 0.9954593181610107
Epoch 960, training loss: 0.6480952501296997 = 0.006913589779287577 + 0.1 * 6.411816596984863
Epoch 960, val loss: 0.9991322755813599
Epoch 970, training loss: 0.6475226879119873 = 0.006735954899340868 + 0.1 * 6.407866954803467
Epoch 970, val loss: 1.0027995109558105
Epoch 980, training loss: 0.6475380659103394 = 0.0065654609352350235 + 0.1 * 6.409726142883301
Epoch 980, val loss: 1.006377100944519
Epoch 990, training loss: 0.6473793983459473 = 0.006402014754712582 + 0.1 * 6.409773826599121
Epoch 990, val loss: 1.0099031925201416
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.8097269535064697 = 1.95004141330719 + 0.1 * 8.596855163574219
Epoch 0, val loss: 1.9485012292861938
Epoch 10, training loss: 2.7989418506622314 = 1.9392647743225098 + 0.1 * 8.596770286560059
Epoch 10, val loss: 1.93699049949646
Epoch 20, training loss: 2.785066604614258 = 1.9254281520843506 + 0.1 * 8.59638500213623
Epoch 20, val loss: 1.9219458103179932
Epoch 30, training loss: 2.7649474143981934 = 1.9056470394134521 + 0.1 * 8.59300422668457
Epoch 30, val loss: 1.90036940574646
Epoch 40, training loss: 2.7334322929382324 = 1.8766040802001953 + 0.1 * 8.568283081054688
Epoch 40, val loss: 1.8692841529846191
Epoch 50, training loss: 2.684988021850586 = 1.8379111289978027 + 0.1 * 8.470768928527832
Epoch 50, val loss: 1.8304307460784912
Epoch 60, training loss: 2.618122100830078 = 1.7982783317565918 + 0.1 * 8.19843864440918
Epoch 60, val loss: 1.795854926109314
Epoch 70, training loss: 2.5659518241882324 = 1.7629040479660034 + 0.1 * 8.030477523803711
Epoch 70, val loss: 1.7683706283569336
Epoch 80, training loss: 2.4873461723327637 = 1.719727635383606 + 0.1 * 7.676185607910156
Epoch 80, val loss: 1.7319351434707642
Epoch 90, training loss: 2.4044079780578613 = 1.665413737297058 + 0.1 * 7.389941215515137
Epoch 90, val loss: 1.684656023979187
Epoch 100, training loss: 2.3186452388763428 = 1.5969951152801514 + 0.1 * 7.216501235961914
Epoch 100, val loss: 1.6255680322647095
Epoch 110, training loss: 2.228724956512451 = 1.5167837142944336 + 0.1 * 7.119411468505859
Epoch 110, val loss: 1.5581138134002686
Epoch 120, training loss: 2.140108823776245 = 1.433224081993103 + 0.1 * 7.068846702575684
Epoch 120, val loss: 1.493245244026184
Epoch 130, training loss: 2.0558407306671143 = 1.3529112339019775 + 0.1 * 7.029294490814209
Epoch 130, val loss: 1.4332469701766968
Epoch 140, training loss: 1.9752767086029053 = 1.2759524583816528 + 0.1 * 6.993243217468262
Epoch 140, val loss: 1.376739501953125
Epoch 150, training loss: 1.8953146934509277 = 1.1989408731460571 + 0.1 * 6.963737964630127
Epoch 150, val loss: 1.3203965425491333
Epoch 160, training loss: 1.8157224655151367 = 1.1216667890548706 + 0.1 * 6.940556526184082
Epoch 160, val loss: 1.2628589868545532
Epoch 170, training loss: 1.7391035556793213 = 1.0467376708984375 + 0.1 * 6.9236578941345215
Epoch 170, val loss: 1.2071905136108398
Epoch 180, training loss: 1.6673427820205688 = 0.9765334129333496 + 0.1 * 6.908093452453613
Epoch 180, val loss: 1.1555233001708984
Epoch 190, training loss: 1.599487543106079 = 0.9103097319602966 + 0.1 * 6.891777515411377
Epoch 190, val loss: 1.1064636707305908
Epoch 200, training loss: 1.5348243713378906 = 0.8472115993499756 + 0.1 * 6.876127243041992
Epoch 200, val loss: 1.059484839439392
Epoch 210, training loss: 1.4725182056427002 = 0.7865691781044006 + 0.1 * 6.859489440917969
Epoch 210, val loss: 1.0137602090835571
Epoch 220, training loss: 1.4131207466125488 = 0.7286995053291321 + 0.1 * 6.844213008880615
Epoch 220, val loss: 0.9701097011566162
Epoch 230, training loss: 1.356501579284668 = 0.6738957166671753 + 0.1 * 6.826059341430664
Epoch 230, val loss: 0.9293276071548462
Epoch 240, training loss: 1.302773356437683 = 0.6216785311698914 + 0.1 * 6.810948371887207
Epoch 240, val loss: 0.8920471668243408
Epoch 250, training loss: 1.251345157623291 = 0.5715925693511963 + 0.1 * 6.7975263595581055
Epoch 250, val loss: 0.8588138222694397
Epoch 260, training loss: 1.2005637884140015 = 0.5222867727279663 + 0.1 * 6.782770156860352
Epoch 260, val loss: 0.8288397192955017
Epoch 270, training loss: 1.1520764827728271 = 0.4738653898239136 + 0.1 * 6.782111167907715
Epoch 270, val loss: 0.8023747205734253
Epoch 280, training loss: 1.103940486907959 = 0.42755165696144104 + 0.1 * 6.763887882232666
Epoch 280, val loss: 0.7803036570549011
Epoch 290, training loss: 1.0593228340148926 = 0.3837968707084656 + 0.1 * 6.755259037017822
Epoch 290, val loss: 0.7626612782478333
Epoch 300, training loss: 1.0182385444641113 = 0.34315168857574463 + 0.1 * 6.750868320465088
Epoch 300, val loss: 0.7497419118881226
Epoch 310, training loss: 0.9814969897270203 = 0.3061712980270386 + 0.1 * 6.753256797790527
Epoch 310, val loss: 0.7415437698364258
Epoch 320, training loss: 0.9469878077507019 = 0.2730470299720764 + 0.1 * 6.739407539367676
Epoch 320, val loss: 0.7372565865516663
Epoch 330, training loss: 0.9166296720504761 = 0.2432391494512558 + 0.1 * 6.733904838562012
Epoch 330, val loss: 0.7364027500152588
Epoch 340, training loss: 0.8891472220420837 = 0.21644844114780426 + 0.1 * 6.726987838745117
Epoch 340, val loss: 0.7383052110671997
Epoch 350, training loss: 0.8648636341094971 = 0.19252371788024902 + 0.1 * 6.7233991622924805
Epoch 350, val loss: 0.7425091862678528
Epoch 360, training loss: 0.8436551094055176 = 0.1713835597038269 + 0.1 * 6.722715377807617
Epoch 360, val loss: 0.7486099600791931
Epoch 370, training loss: 0.8242210745811462 = 0.15273325145244598 + 0.1 * 6.714878082275391
Epoch 370, val loss: 0.7563013434410095
Epoch 380, training loss: 0.8068363666534424 = 0.13635583221912384 + 0.1 * 6.704805374145508
Epoch 380, val loss: 0.7652341723442078
Epoch 390, training loss: 0.7920186519622803 = 0.12203869223594666 + 0.1 * 6.69980001449585
Epoch 390, val loss: 0.7751367688179016
Epoch 400, training loss: 0.7792465090751648 = 0.10953837633132935 + 0.1 * 6.697081089019775
Epoch 400, val loss: 0.7857704758644104
Epoch 410, training loss: 0.7671119570732117 = 0.09861254692077637 + 0.1 * 6.684994220733643
Epoch 410, val loss: 0.7968543171882629
Epoch 420, training loss: 0.7572325468063354 = 0.08905311673879623 + 0.1 * 6.6817946434021
Epoch 420, val loss: 0.8084969520568848
Epoch 430, training loss: 0.7476569414138794 = 0.0806962177157402 + 0.1 * 6.669607162475586
Epoch 430, val loss: 0.8202321529388428
Epoch 440, training loss: 0.7430810332298279 = 0.07332928478717804 + 0.1 * 6.697517395019531
Epoch 440, val loss: 0.8321083784103394
Epoch 450, training loss: 0.7323731184005737 = 0.06685460358858109 + 0.1 * 6.655185222625732
Epoch 450, val loss: 0.8439568877220154
Epoch 460, training loss: 0.7258694171905518 = 0.06110105663537979 + 0.1 * 6.647683143615723
Epoch 460, val loss: 0.855772852897644
Epoch 470, training loss: 0.7224441170692444 = 0.055974677205085754 + 0.1 * 6.664694309234619
Epoch 470, val loss: 0.8675339221954346
Epoch 480, training loss: 0.714852511882782 = 0.05142699554562569 + 0.1 * 6.6342549324035645
Epoch 480, val loss: 0.8791836500167847
Epoch 490, training loss: 0.7109146118164062 = 0.04736776649951935 + 0.1 * 6.635468482971191
Epoch 490, val loss: 0.8906721472740173
Epoch 500, training loss: 0.7057138085365295 = 0.04373525455594063 + 0.1 * 6.619785308837891
Epoch 500, val loss: 0.9019538760185242
Epoch 510, training loss: 0.7030636072158813 = 0.040478046983480453 + 0.1 * 6.625855445861816
Epoch 510, val loss: 0.912994921207428
Epoch 520, training loss: 0.6985445022583008 = 0.03755192831158638 + 0.1 * 6.609925270080566
Epoch 520, val loss: 0.923827588558197
Epoch 530, training loss: 0.6966816186904907 = 0.03491136059165001 + 0.1 * 6.617702960968018
Epoch 530, val loss: 0.9342840909957886
Epoch 540, training loss: 0.6924971342086792 = 0.032528724521398544 + 0.1 * 6.599684238433838
Epoch 540, val loss: 0.9446021318435669
Epoch 550, training loss: 0.690086841583252 = 0.030369985848665237 + 0.1 * 6.597168445587158
Epoch 550, val loss: 0.9546358585357666
Epoch 560, training loss: 0.6875415444374084 = 0.028413033112883568 + 0.1 * 6.59128475189209
Epoch 560, val loss: 0.9642724990844727
Epoch 570, training loss: 0.6850904226303101 = 0.026640459895133972 + 0.1 * 6.584499359130859
Epoch 570, val loss: 0.9737837314605713
Epoch 580, training loss: 0.6831563711166382 = 0.025023799389600754 + 0.1 * 6.581325531005859
Epoch 580, val loss: 0.9829084873199463
Epoch 590, training loss: 0.6812306642532349 = 0.023548318073153496 + 0.1 * 6.5768232345581055
Epoch 590, val loss: 0.9918393492698669
Epoch 600, training loss: 0.6821395754814148 = 0.02219747006893158 + 0.1 * 6.59942102432251
Epoch 600, val loss: 1.0005743503570557
Epoch 610, training loss: 0.6783499121665955 = 0.0209655724465847 + 0.1 * 6.573843002319336
Epoch 610, val loss: 1.0088647603988647
Epoch 620, training loss: 0.6764814257621765 = 0.019835619255900383 + 0.1 * 6.566458225250244
Epoch 620, val loss: 1.0171699523925781
Epoch 630, training loss: 0.674531102180481 = 0.01879187487065792 + 0.1 * 6.557392120361328
Epoch 630, val loss: 1.0251481533050537
Epoch 640, training loss: 0.676003098487854 = 0.017829252406954765 + 0.1 * 6.581738471984863
Epoch 640, val loss: 1.0328762531280518
Epoch 650, training loss: 0.6731795072555542 = 0.016945533454418182 + 0.1 * 6.562339782714844
Epoch 650, val loss: 1.0403835773468018
Epoch 660, training loss: 0.6728886961936951 = 0.016128486022353172 + 0.1 * 6.567602157592773
Epoch 660, val loss: 1.047876238822937
Epoch 670, training loss: 0.6705234050750732 = 0.015372187830507755 + 0.1 * 6.551511764526367
Epoch 670, val loss: 1.0547964572906494
Epoch 680, training loss: 0.6685810089111328 = 0.014671366661787033 + 0.1 * 6.539096355438232
Epoch 680, val loss: 1.0618376731872559
Epoch 690, training loss: 0.6682754755020142 = 0.014017496258020401 + 0.1 * 6.542579174041748
Epoch 690, val loss: 1.0686053037643433
Epoch 700, training loss: 0.6672993302345276 = 0.013407972641289234 + 0.1 * 6.538913726806641
Epoch 700, val loss: 1.0751566886901855
Epoch 710, training loss: 0.6681374311447144 = 0.012840182520449162 + 0.1 * 6.552972316741943
Epoch 710, val loss: 1.0815898180007935
Epoch 720, training loss: 0.6654998064041138 = 0.012309925630688667 + 0.1 * 6.5318989753723145
Epoch 720, val loss: 1.0877913236618042
Epoch 730, training loss: 0.6644192337989807 = 0.011814317665994167 + 0.1 * 6.5260491371154785
Epoch 730, val loss: 1.0938756465911865
Epoch 740, training loss: 0.6639456748962402 = 0.011351384222507477 + 0.1 * 6.525942802429199
Epoch 740, val loss: 1.0998735427856445
Epoch 750, training loss: 0.6650413274765015 = 0.010916192084550858 + 0.1 * 6.541251182556152
Epoch 750, val loss: 1.1057082414627075
Epoch 760, training loss: 0.6625085473060608 = 0.010507556609809399 + 0.1 * 6.520009517669678
Epoch 760, val loss: 1.1112842559814453
Epoch 770, training loss: 0.6645345687866211 = 0.010123459622263908 + 0.1 * 6.544111251831055
Epoch 770, val loss: 1.1168818473815918
Epoch 780, training loss: 0.6609768867492676 = 0.009762558154761791 + 0.1 * 6.512142658233643
Epoch 780, val loss: 1.1222379207611084
Epoch 790, training loss: 0.6612852811813354 = 0.009422705508768559 + 0.1 * 6.518625259399414
Epoch 790, val loss: 1.1275297403335571
Epoch 800, training loss: 0.6591331958770752 = 0.009100992232561111 + 0.1 * 6.500322341918945
Epoch 800, val loss: 1.1327191591262817
Epoch 810, training loss: 0.6597784161567688 = 0.008796747773885727 + 0.1 * 6.509816646575928
Epoch 810, val loss: 1.137864589691162
Epoch 820, training loss: 0.6594204306602478 = 0.008508406579494476 + 0.1 * 6.509120464324951
Epoch 820, val loss: 1.1427173614501953
Epoch 830, training loss: 0.658183217048645 = 0.008236216381192207 + 0.1 * 6.499469757080078
Epoch 830, val loss: 1.147603154182434
Epoch 840, training loss: 0.6586498618125916 = 0.00797728355973959 + 0.1 * 6.506725311279297
Epoch 840, val loss: 1.152434229850769
Epoch 850, training loss: 0.6569266319274902 = 0.007731793913990259 + 0.1 * 6.491948127746582
Epoch 850, val loss: 1.1569477319717407
Epoch 860, training loss: 0.6572213172912598 = 0.007498736958950758 + 0.1 * 6.497225761413574
Epoch 860, val loss: 1.16154944896698
Epoch 870, training loss: 0.6565611362457275 = 0.007277209777384996 + 0.1 * 6.492839336395264
Epoch 870, val loss: 1.1660387516021729
Epoch 880, training loss: 0.6558859944343567 = 0.007066867779940367 + 0.1 * 6.488191604614258
Epoch 880, val loss: 1.1704219579696655
Epoch 890, training loss: 0.6546450853347778 = 0.00686622504144907 + 0.1 * 6.47778844833374
Epoch 890, val loss: 1.1747342348098755
Epoch 900, training loss: 0.6553877592086792 = 0.006674876436591148 + 0.1 * 6.487128734588623
Epoch 900, val loss: 1.1790436506271362
Epoch 910, training loss: 0.6542550921440125 = 0.006491627544164658 + 0.1 * 6.477634906768799
Epoch 910, val loss: 1.1829930543899536
Epoch 920, training loss: 0.6538106799125671 = 0.006318131927400827 + 0.1 * 6.4749250411987305
Epoch 920, val loss: 1.1870726346969604
Epoch 930, training loss: 0.6566395163536072 = 0.006151723209768534 + 0.1 * 6.50487756729126
Epoch 930, val loss: 1.1911131143569946
Epoch 940, training loss: 0.6532390713691711 = 0.005992955528199673 + 0.1 * 6.472461223602295
Epoch 940, val loss: 1.1949739456176758
Epoch 950, training loss: 0.654235303401947 = 0.005841048434376717 + 0.1 * 6.483942031860352
Epoch 950, val loss: 1.1987979412078857
Epoch 960, training loss: 0.6518651843070984 = 0.005695386789739132 + 0.1 * 6.461697578430176
Epoch 960, val loss: 1.2025765180587769
Epoch 970, training loss: 0.654758095741272 = 0.005555886309593916 + 0.1 * 6.4920220375061035
Epoch 970, val loss: 1.2062166929244995
Epoch 980, training loss: 0.6519272327423096 = 0.00542190857231617 + 0.1 * 6.465052604675293
Epoch 980, val loss: 1.2098703384399414
Epoch 990, training loss: 0.6509169340133667 = 0.005293792113661766 + 0.1 * 6.456231117248535
Epoch 990, val loss: 1.2134557962417603
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8328940432261466
The final CL Acc:0.81481, 0.01600, The final GNN Acc:0.83553, 0.00269
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10566])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.793379545211792 = 1.9336957931518555 + 0.1 * 8.596837997436523
Epoch 0, val loss: 1.9345576763153076
Epoch 10, training loss: 2.7835488319396973 = 1.923871636390686 + 0.1 * 8.596773147583008
Epoch 10, val loss: 1.9245223999023438
Epoch 20, training loss: 2.771761178970337 = 1.9121215343475342 + 0.1 * 8.596396446228027
Epoch 20, val loss: 1.9125174283981323
Epoch 30, training loss: 2.7555642127990723 = 1.8962702751159668 + 0.1 * 8.592938423156738
Epoch 30, val loss: 1.89658522605896
Epoch 40, training loss: 2.729520320892334 = 1.873682975769043 + 0.1 * 8.558374404907227
Epoch 40, val loss: 1.8745214939117432
Epoch 50, training loss: 2.6728663444519043 = 1.8436158895492554 + 0.1 * 8.292505264282227
Epoch 50, val loss: 1.8464454412460327
Epoch 60, training loss: 2.6077651977539062 = 1.8128015995025635 + 0.1 * 7.949636936187744
Epoch 60, val loss: 1.8191684484481812
Epoch 70, training loss: 2.540844678878784 = 1.783360242843628 + 0.1 * 7.574843406677246
Epoch 70, val loss: 1.7939516305923462
Epoch 80, training loss: 2.4775660037994385 = 1.7517526149749756 + 0.1 * 7.258133888244629
Epoch 80, val loss: 1.7663722038269043
Epoch 90, training loss: 2.4181630611419678 = 1.710870623588562 + 0.1 * 7.072924613952637
Epoch 90, val loss: 1.730850338935852
Epoch 100, training loss: 2.353806734085083 = 1.6546318531036377 + 0.1 * 6.991748332977295
Epoch 100, val loss: 1.681007981300354
Epoch 110, training loss: 2.274892568588257 = 1.580361008644104 + 0.1 * 6.945315837860107
Epoch 110, val loss: 1.6157371997833252
Epoch 120, training loss: 2.181565761566162 = 1.490207552909851 + 0.1 * 6.913583278656006
Epoch 120, val loss: 1.5400725603103638
Epoch 130, training loss: 2.080965042114258 = 1.391654133796692 + 0.1 * 6.893109321594238
Epoch 130, val loss: 1.4603943824768066
Epoch 140, training loss: 1.977684497833252 = 1.2907207012176514 + 0.1 * 6.869637966156006
Epoch 140, val loss: 1.3795173168182373
Epoch 150, training loss: 1.8759069442749023 = 1.190476655960083 + 0.1 * 6.854302883148193
Epoch 150, val loss: 1.302345871925354
Epoch 160, training loss: 1.7808829545974731 = 1.0977513790130615 + 0.1 * 6.831315517425537
Epoch 160, val loss: 1.2338045835494995
Epoch 170, training loss: 1.6928539276123047 = 1.0120042562484741 + 0.1 * 6.808496952056885
Epoch 170, val loss: 1.1718584299087524
Epoch 180, training loss: 1.6118900775909424 = 0.9325035810470581 + 0.1 * 6.79386568069458
Epoch 180, val loss: 1.115181565284729
Epoch 190, training loss: 1.5383620262145996 = 0.859375536441803 + 0.1 * 6.789864540100098
Epoch 190, val loss: 1.062852382659912
Epoch 200, training loss: 1.4699888229370117 = 0.7927507758140564 + 0.1 * 6.772380828857422
Epoch 200, val loss: 1.0157147645950317
Epoch 210, training loss: 1.4073431491851807 = 0.7316237092018127 + 0.1 * 6.757193565368652
Epoch 210, val loss: 0.9730282425880432
Epoch 220, training loss: 1.3523751497268677 = 0.6758388876914978 + 0.1 * 6.76536226272583
Epoch 220, val loss: 0.936178982257843
Epoch 230, training loss: 1.2997117042541504 = 0.6256102323532104 + 0.1 * 6.741015434265137
Epoch 230, val loss: 0.9060600996017456
Epoch 240, training loss: 1.2529284954071045 = 0.5797799229621887 + 0.1 * 6.731485366821289
Epoch 240, val loss: 0.8820604085922241
Epoch 250, training loss: 1.2116353511810303 = 0.5375955104827881 + 0.1 * 6.74039888381958
Epoch 250, val loss: 0.8636218309402466
Epoch 260, training loss: 1.1703567504882812 = 0.49885594844818115 + 0.1 * 6.71500825881958
Epoch 260, val loss: 0.8503319025039673
Epoch 270, training loss: 1.134257197380066 = 0.46297764778137207 + 0.1 * 6.712795257568359
Epoch 270, val loss: 0.8408879041671753
Epoch 280, training loss: 1.1009914875030518 = 0.4298331141471863 + 0.1 * 6.711583614349365
Epoch 280, val loss: 0.8351098895072937
Epoch 290, training loss: 1.0690860748291016 = 0.3993695378303528 + 0.1 * 6.697164535522461
Epoch 290, val loss: 0.8322721719741821
Epoch 300, training loss: 1.0394337177276611 = 0.37109026312828064 + 0.1 * 6.683434963226318
Epoch 300, val loss: 0.8319909572601318
Epoch 310, training loss: 1.0139721632003784 = 0.3445613384246826 + 0.1 * 6.694108009338379
Epoch 310, val loss: 0.8340023756027222
Epoch 320, training loss: 0.9865477085113525 = 0.3196447193622589 + 0.1 * 6.669029712677002
Epoch 320, val loss: 0.8378723859786987
Epoch 330, training loss: 0.9625254273414612 = 0.29600465297698975 + 0.1 * 6.665207862854004
Epoch 330, val loss: 0.8434159159660339
Epoch 340, training loss: 0.9411286115646362 = 0.273630291223526 + 0.1 * 6.674983501434326
Epoch 340, val loss: 0.8507285118103027
Epoch 350, training loss: 0.9177160263061523 = 0.2525232136249542 + 0.1 * 6.651927947998047
Epoch 350, val loss: 0.8595503568649292
Epoch 360, training loss: 0.8987845182418823 = 0.23266619443893433 + 0.1 * 6.6611833572387695
Epoch 360, val loss: 0.8698611259460449
Epoch 370, training loss: 0.878960371017456 = 0.21422725915908813 + 0.1 * 6.6473307609558105
Epoch 370, val loss: 0.8818457126617432
Epoch 380, training loss: 0.8628579378128052 = 0.19713467359542847 + 0.1 * 6.657232284545898
Epoch 380, val loss: 0.894935667514801
Epoch 390, training loss: 0.8442073464393616 = 0.18141807615756989 + 0.1 * 6.627892971038818
Epoch 390, val loss: 0.9092392921447754
Epoch 400, training loss: 0.8290838003158569 = 0.16693805158138275 + 0.1 * 6.621457099914551
Epoch 400, val loss: 0.9244133234024048
Epoch 410, training loss: 0.8158769607543945 = 0.15363745391368866 + 0.1 * 6.622394561767578
Epoch 410, val loss: 0.9402123689651489
Epoch 420, training loss: 0.8024594783782959 = 0.1414595991373062 + 0.1 * 6.60999870300293
Epoch 420, val loss: 0.9564995765686035
Epoch 430, training loss: 0.7928178310394287 = 0.1302945613861084 + 0.1 * 6.625232696533203
Epoch 430, val loss: 0.9729397296905518
Epoch 440, training loss: 0.7802061438560486 = 0.1201106458902359 + 0.1 * 6.600955009460449
Epoch 440, val loss: 0.9895342588424683
Epoch 450, training loss: 0.7709866762161255 = 0.11079257726669312 + 0.1 * 6.601941108703613
Epoch 450, val loss: 1.0060784816741943
Epoch 460, training loss: 0.7614884376525879 = 0.10228829830884933 + 0.1 * 6.592001438140869
Epoch 460, val loss: 1.0225121974945068
Epoch 470, training loss: 0.7535057663917542 = 0.09453010559082031 + 0.1 * 6.589756488800049
Epoch 470, val loss: 1.038804292678833
Epoch 480, training loss: 0.7474774122238159 = 0.08745638281106949 + 0.1 * 6.600210189819336
Epoch 480, val loss: 1.0548932552337646
Epoch 490, training loss: 0.7388428449630737 = 0.08101752400398254 + 0.1 * 6.578253269195557
Epoch 490, val loss: 1.0707075595855713
Epoch 500, training loss: 0.7337197661399841 = 0.07514975219964981 + 0.1 * 6.585699558258057
Epoch 500, val loss: 1.0861462354660034
Epoch 510, training loss: 0.7272902131080627 = 0.06980526447296143 + 0.1 * 6.574849605560303
Epoch 510, val loss: 1.1012428998947144
Epoch 520, training loss: 0.721835732460022 = 0.06493702530860901 + 0.1 * 6.5689873695373535
Epoch 520, val loss: 1.1160763502120972
Epoch 530, training loss: 0.7171193361282349 = 0.060490790754556656 + 0.1 * 6.566285610198975
Epoch 530, val loss: 1.130466341972351
Epoch 540, training loss: 0.7127686142921448 = 0.05643019825220108 + 0.1 * 6.563384056091309
Epoch 540, val loss: 1.1446267366409302
Epoch 550, training loss: 0.708638072013855 = 0.05271768942475319 + 0.1 * 6.559203624725342
Epoch 550, val loss: 1.1582459211349487
Epoch 560, training loss: 0.7054715156555176 = 0.04931870847940445 + 0.1 * 6.561527729034424
Epoch 560, val loss: 1.1716886758804321
Epoch 570, training loss: 0.7005578875541687 = 0.04620535671710968 + 0.1 * 6.543525218963623
Epoch 570, val loss: 1.1847120523452759
Epoch 580, training loss: 0.6990898847579956 = 0.04335011541843414 + 0.1 * 6.557397842407227
Epoch 580, val loss: 1.1972498893737793
Epoch 590, training loss: 0.6948949694633484 = 0.04073188081383705 + 0.1 * 6.541630744934082
Epoch 590, val loss: 1.2096880674362183
Epoch 600, training loss: 0.69235759973526 = 0.03832056745886803 + 0.1 * 6.540370464324951
Epoch 600, val loss: 1.2215791940689087
Epoch 610, training loss: 0.6888605356216431 = 0.03610293194651604 + 0.1 * 6.527575969696045
Epoch 610, val loss: 1.2333356142044067
Epoch 620, training loss: 0.687903881072998 = 0.034058086574077606 + 0.1 * 6.538457870483398
Epoch 620, val loss: 1.2446482181549072
Epoch 630, training loss: 0.6856844425201416 = 0.03217562288045883 + 0.1 * 6.535088539123535
Epoch 630, val loss: 1.2556812763214111
Epoch 640, training loss: 0.6819829940795898 = 0.030436493456363678 + 0.1 * 6.515464782714844
Epoch 640, val loss: 1.2663874626159668
Epoch 650, training loss: 0.6829922795295715 = 0.0288256648927927 + 0.1 * 6.541666030883789
Epoch 650, val loss: 1.2767693996429443
Epoch 660, training loss: 0.6795028448104858 = 0.027334513142704964 + 0.1 * 6.521683216094971
Epoch 660, val loss: 1.2869582176208496
Epoch 670, training loss: 0.6775854825973511 = 0.0259530209004879 + 0.1 * 6.516324520111084
Epoch 670, val loss: 1.2968494892120361
Epoch 680, training loss: 0.6757026314735413 = 0.024669675156474113 + 0.1 * 6.510329723358154
Epoch 680, val loss: 1.3063832521438599
Epoch 690, training loss: 0.6752959489822388 = 0.02347676269710064 + 0.1 * 6.518191814422607
Epoch 690, val loss: 1.315774917602539
Epoch 700, training loss: 0.673090398311615 = 0.022365981712937355 + 0.1 * 6.507244110107422
Epoch 700, val loss: 1.3248693943023682
Epoch 710, training loss: 0.6729558110237122 = 0.021331187337636948 + 0.1 * 6.5162458419799805
Epoch 710, val loss: 1.333801507949829
Epoch 720, training loss: 0.6703530550003052 = 0.020366081967949867 + 0.1 * 6.4998698234558105
Epoch 720, val loss: 1.342306137084961
Epoch 730, training loss: 0.6702146530151367 = 0.01946546509861946 + 0.1 * 6.5074920654296875
Epoch 730, val loss: 1.3507863283157349
Epoch 740, training loss: 0.6676597595214844 = 0.018622806295752525 + 0.1 * 6.4903693199157715
Epoch 740, val loss: 1.3588603734970093
Epoch 750, training loss: 0.667789101600647 = 0.017834369093179703 + 0.1 * 6.499547481536865
Epoch 750, val loss: 1.3668936491012573
Epoch 760, training loss: 0.6674426198005676 = 0.01709585264325142 + 0.1 * 6.503467559814453
Epoch 760, val loss: 1.374543309211731
Epoch 770, training loss: 0.6655660271644592 = 0.01640213653445244 + 0.1 * 6.491638660430908
Epoch 770, val loss: 1.382164716720581
Epoch 780, training loss: 0.6642566323280334 = 0.01575235277414322 + 0.1 * 6.485042572021484
Epoch 780, val loss: 1.3894072771072388
Epoch 790, training loss: 0.6631563901901245 = 0.0151404133066535 + 0.1 * 6.480159759521484
Epoch 790, val loss: 1.3966675996780396
Epoch 800, training loss: 0.6618461012840271 = 0.014564821496605873 + 0.1 * 6.472812652587891
Epoch 800, val loss: 1.4034799337387085
Epoch 810, training loss: 0.6614106297492981 = 0.01402172539383173 + 0.1 * 6.473888874053955
Epoch 810, val loss: 1.4104617834091187
Epoch 820, training loss: 0.6610987782478333 = 0.013508198782801628 + 0.1 * 6.475905418395996
Epoch 820, val loss: 1.4171110391616821
Epoch 830, training loss: 0.6614472270011902 = 0.013023339211940765 + 0.1 * 6.484239101409912
Epoch 830, val loss: 1.4234836101531982
Epoch 840, training loss: 0.6594173312187195 = 0.012567020021378994 + 0.1 * 6.468502521514893
Epoch 840, val loss: 1.4298005104064941
Epoch 850, training loss: 0.658253014087677 = 0.012135445140302181 + 0.1 * 6.461175441741943
Epoch 850, val loss: 1.436138391494751
Epoch 860, training loss: 0.6577141880989075 = 0.01172560639679432 + 0.1 * 6.459885597229004
Epoch 860, val loss: 1.4421651363372803
Epoch 870, training loss: 0.6594053506851196 = 0.011337228119373322 + 0.1 * 6.480681419372559
Epoch 870, val loss: 1.448029637336731
Epoch 880, training loss: 0.657922625541687 = 0.010970202274620533 + 0.1 * 6.469524383544922
Epoch 880, val loss: 1.4536916017532349
Epoch 890, training loss: 0.6569609045982361 = 0.010621638968586922 + 0.1 * 6.46339225769043
Epoch 890, val loss: 1.4594213962554932
Epoch 900, training loss: 0.6561026573181152 = 0.010290117003023624 + 0.1 * 6.458125114440918
Epoch 900, val loss: 1.4648975133895874
Epoch 910, training loss: 0.6557375192642212 = 0.009974434040486813 + 0.1 * 6.457630634307861
Epoch 910, val loss: 1.470259189605713
Epoch 920, training loss: 0.6577879786491394 = 0.009673663415014744 + 0.1 * 6.481142997741699
Epoch 920, val loss: 1.4755001068115234
Epoch 930, training loss: 0.6553476452827454 = 0.009388298727571964 + 0.1 * 6.459592819213867
Epoch 930, val loss: 1.4806207418441772
Epoch 940, training loss: 0.6553279161453247 = 0.009116953238844872 + 0.1 * 6.462109565734863
Epoch 940, val loss: 1.485740303993225
Epoch 950, training loss: 0.6533423662185669 = 0.008857645094394684 + 0.1 * 6.4448466300964355
Epoch 950, val loss: 1.4905339479446411
Epoch 960, training loss: 0.6537777185440063 = 0.008610359393060207 + 0.1 * 6.45167350769043
Epoch 960, val loss: 1.4954925775527954
Epoch 970, training loss: 0.6538404226303101 = 0.008373580873012543 + 0.1 * 6.4546685218811035
Epoch 970, val loss: 1.500128149986267
Epoch 980, training loss: 0.653264045715332 = 0.00814877636730671 + 0.1 * 6.451152324676514
Epoch 980, val loss: 1.5045326948165894
Epoch 990, training loss: 0.6518709659576416 = 0.007933049462735653 + 0.1 * 6.4393792152404785
Epoch 990, val loss: 1.5093086957931519
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 2.802628993988037 = 1.9429435729980469 + 0.1 * 8.596854209899902
Epoch 0, val loss: 1.9396432638168335
Epoch 10, training loss: 2.792891502380371 = 1.9332138299942017 + 0.1 * 8.596776008605957
Epoch 10, val loss: 1.9294706583023071
Epoch 20, training loss: 2.7808189392089844 = 1.9211924076080322 + 0.1 * 8.59626579284668
Epoch 20, val loss: 1.9169989824295044
Epoch 30, training loss: 2.7635297775268555 = 1.9043489694595337 + 0.1 * 8.59180736541748
Epoch 30, val loss: 1.8993998765945435
Epoch 40, training loss: 2.7355761528015137 = 1.879413366317749 + 0.1 * 8.561627388000488
Epoch 40, val loss: 1.8735376596450806
Epoch 50, training loss: 2.6870477199554443 = 1.8453247547149658 + 0.1 * 8.417229652404785
Epoch 50, val loss: 1.8397879600524902
Epoch 60, training loss: 2.631380081176758 = 1.8084609508514404 + 0.1 * 8.2291898727417
Epoch 60, val loss: 1.806578278541565
Epoch 70, training loss: 2.579362392425537 = 1.776923418045044 + 0.1 * 8.024388313293457
Epoch 70, val loss: 1.7813632488250732
Epoch 80, training loss: 2.5073013305664062 = 1.742830514907837 + 0.1 * 7.644707202911377
Epoch 80, val loss: 1.7525266408920288
Epoch 90, training loss: 2.435636043548584 = 1.699265718460083 + 0.1 * 7.36370325088501
Epoch 90, val loss: 1.7163282632827759
Epoch 100, training loss: 2.363166570663452 = 1.6408394575119019 + 0.1 * 7.22327184677124
Epoch 100, val loss: 1.6680270433425903
Epoch 110, training loss: 2.276582717895508 = 1.5645540952682495 + 0.1 * 7.120286464691162
Epoch 110, val loss: 1.6026628017425537
Epoch 120, training loss: 2.179659605026245 = 1.473785161972046 + 0.1 * 7.058744430541992
Epoch 120, val loss: 1.5283207893371582
Epoch 130, training loss: 2.0773086547851562 = 1.375917673110962 + 0.1 * 7.013908386230469
Epoch 130, val loss: 1.4512444734573364
Epoch 140, training loss: 1.97610342502594 = 1.2773828506469727 + 0.1 * 6.987205505371094
Epoch 140, val loss: 1.375214695930481
Epoch 150, training loss: 1.8802411556243896 = 1.1828006505966187 + 0.1 * 6.974404335021973
Epoch 150, val loss: 1.3049018383026123
Epoch 160, training loss: 1.7904019355773926 = 1.094632863998413 + 0.1 * 6.957690238952637
Epoch 160, val loss: 1.2412796020507812
Epoch 170, training loss: 1.7085845470428467 = 1.0140748023986816 + 0.1 * 6.945096492767334
Epoch 170, val loss: 1.1850665807724
Epoch 180, training loss: 1.6335753202438354 = 0.9406578540802002 + 0.1 * 6.929174423217773
Epoch 180, val loss: 1.1345139741897583
Epoch 190, training loss: 1.5641118288040161 = 0.8724823594093323 + 0.1 * 6.916294574737549
Epoch 190, val loss: 1.0870566368103027
Epoch 200, training loss: 1.4999628067016602 = 0.8100005388259888 + 0.1 * 6.899622917175293
Epoch 200, val loss: 1.0432965755462646
Epoch 210, training loss: 1.4405299425125122 = 0.75246661901474 + 0.1 * 6.8806328773498535
Epoch 210, val loss: 1.002493977546692
Epoch 220, training loss: 1.3856579065322876 = 0.6994034647941589 + 0.1 * 6.862544059753418
Epoch 220, val loss: 0.9652365446090698
Epoch 230, training loss: 1.3362531661987305 = 0.6507743000984192 + 0.1 * 6.854787826538086
Epoch 230, val loss: 0.9323123693466187
Epoch 240, training loss: 1.2888119220733643 = 0.6052849888801575 + 0.1 * 6.835269451141357
Epoch 240, val loss: 0.9033761024475098
Epoch 250, training loss: 1.2440407276153564 = 0.5609232187271118 + 0.1 * 6.831174373626709
Epoch 250, val loss: 0.8772476315498352
Epoch 260, training loss: 1.1974844932556152 = 0.5165625214576721 + 0.1 * 6.809219837188721
Epoch 260, val loss: 0.853121280670166
Epoch 270, training loss: 1.15159010887146 = 0.4714730381965637 + 0.1 * 6.8011698722839355
Epoch 270, val loss: 0.8298652172088623
Epoch 280, training loss: 1.1052837371826172 = 0.4264267385005951 + 0.1 * 6.788570404052734
Epoch 280, val loss: 0.8078337907791138
Epoch 290, training loss: 1.0610129833221436 = 0.38245248794555664 + 0.1 * 6.785605430603027
Epoch 290, val loss: 0.7875721454620361
Epoch 300, training loss: 1.0186823606491089 = 0.3404344618320465 + 0.1 * 6.7824788093566895
Epoch 300, val loss: 0.7698221802711487
Epoch 310, training loss: 0.9781986474990845 = 0.301651269197464 + 0.1 * 6.765473365783691
Epoch 310, val loss: 0.7553462386131287
Epoch 320, training loss: 0.942787766456604 = 0.2662610709667206 + 0.1 * 6.7652668952941895
Epoch 320, val loss: 0.7441554069519043
Epoch 330, training loss: 0.9105390906333923 = 0.23459582030773163 + 0.1 * 6.759432315826416
Epoch 330, val loss: 0.7364679574966431
Epoch 340, training loss: 0.8815088272094727 = 0.20654520392417908 + 0.1 * 6.749636173248291
Epoch 340, val loss: 0.7320584654808044
Epoch 350, training loss: 0.8564468622207642 = 0.1819646656513214 + 0.1 * 6.744822025299072
Epoch 350, val loss: 0.7307274341583252
Epoch 360, training loss: 0.8339158296585083 = 0.16062326729297638 + 0.1 * 6.732925891876221
Epoch 360, val loss: 0.7321407198905945
Epoch 370, training loss: 0.8163743019104004 = 0.14212706685066223 + 0.1 * 6.742472171783447
Epoch 370, val loss: 0.7357934713363647
Epoch 380, training loss: 0.7980914115905762 = 0.12619498372077942 + 0.1 * 6.718964576721191
Epoch 380, val loss: 0.7414272427558899
Epoch 390, training loss: 0.7859312295913696 = 0.11241499334573746 + 0.1 * 6.735162258148193
Epoch 390, val loss: 0.7484818696975708
Epoch 400, training loss: 0.7717258930206299 = 0.10058405995368958 + 0.1 * 6.711418628692627
Epoch 400, val loss: 0.7568440437316895
Epoch 410, training loss: 0.7606123089790344 = 0.09034951776266098 + 0.1 * 6.702628135681152
Epoch 410, val loss: 0.7660109400749207
Epoch 420, training loss: 0.7523676753044128 = 0.08144863694906235 + 0.1 * 6.709190368652344
Epoch 420, val loss: 0.7759501338005066
Epoch 430, training loss: 0.743186354637146 = 0.07370118796825409 + 0.1 * 6.694851398468018
Epoch 430, val loss: 0.7863951325416565
Epoch 440, training loss: 0.7352159023284912 = 0.06693317741155624 + 0.1 * 6.682827472686768
Epoch 440, val loss: 0.7970651984214783
Epoch 450, training loss: 0.7304036021232605 = 0.06099754571914673 + 0.1 * 6.694060325622559
Epoch 450, val loss: 0.8079909086227417
Epoch 460, training loss: 0.7236461043357849 = 0.0557892769575119 + 0.1 * 6.678567886352539
Epoch 460, val loss: 0.8189748525619507
Epoch 470, training loss: 0.7230179309844971 = 0.051199331879615784 + 0.1 * 6.718185901641846
Epoch 470, val loss: 0.8298465013504028
Epoch 480, training loss: 0.7142808437347412 = 0.047159962356090546 + 0.1 * 6.671208381652832
Epoch 480, val loss: 0.8407315015792847
Epoch 490, training loss: 0.7095314860343933 = 0.04357067123055458 + 0.1 * 6.659607887268066
Epoch 490, val loss: 0.8513679504394531
Epoch 500, training loss: 0.7057227492332458 = 0.04036242887377739 + 0.1 * 6.6536030769348145
Epoch 500, val loss: 0.861939549446106
Epoch 510, training loss: 0.7046021223068237 = 0.03750010207295418 + 0.1 * 6.671020030975342
Epoch 510, val loss: 0.8722562193870544
Epoch 520, training loss: 0.7001264691352844 = 0.034944888204336166 + 0.1 * 6.651815891265869
Epoch 520, val loss: 0.8823716044425964
Epoch 530, training loss: 0.696908175945282 = 0.03263990953564644 + 0.1 * 6.6426825523376465
Epoch 530, val loss: 0.8921748399734497
Epoch 540, training loss: 0.6943762898445129 = 0.030552001670002937 + 0.1 * 6.638242721557617
Epoch 540, val loss: 0.9018341898918152
Epoch 550, training loss: 0.6923924684524536 = 0.02866092137992382 + 0.1 * 6.63731575012207
Epoch 550, val loss: 0.9112837910652161
Epoch 560, training loss: 0.6910226345062256 = 0.026951827108860016 + 0.1 * 6.640707969665527
Epoch 560, val loss: 0.9205257296562195
Epoch 570, training loss: 0.6907781958580017 = 0.025392631068825722 + 0.1 * 6.653855323791504
Epoch 570, val loss: 0.9294029474258423
Epoch 580, training loss: 0.6862430572509766 = 0.023975584656000137 + 0.1 * 6.622674465179443
Epoch 580, val loss: 0.9381901621818542
Epoch 590, training loss: 0.6862252950668335 = 0.02267778106033802 + 0.1 * 6.635475158691406
Epoch 590, val loss: 0.9466071128845215
Epoch 600, training loss: 0.6830959320068359 = 0.02148563787341118 + 0.1 * 6.616102695465088
Epoch 600, val loss: 0.9549157023429871
Epoch 610, training loss: 0.6827208399772644 = 0.02038833126425743 + 0.1 * 6.623325347900391
Epoch 610, val loss: 0.9628651738166809
Epoch 620, training loss: 0.6800349950790405 = 0.01938069425523281 + 0.1 * 6.606543064117432
Epoch 620, val loss: 0.9708243012428284
Epoch 630, training loss: 0.6792600750923157 = 0.018448077142238617 + 0.1 * 6.608119964599609
Epoch 630, val loss: 0.9784225821495056
Epoch 640, training loss: 0.6779530048370361 = 0.01758607104420662 + 0.1 * 6.6036696434021
Epoch 640, val loss: 0.9858844876289368
Epoch 650, training loss: 0.6773117184638977 = 0.016786623746156693 + 0.1 * 6.605250835418701
Epoch 650, val loss: 0.9931466579437256
Epoch 660, training loss: 0.6760559678077698 = 0.016045434400439262 + 0.1 * 6.600105285644531
Epoch 660, val loss: 1.0001251697540283
Epoch 670, training loss: 0.6743277311325073 = 0.015358062461018562 + 0.1 * 6.589696407318115
Epoch 670, val loss: 1.0071206092834473
Epoch 680, training loss: 0.6765137314796448 = 0.014715868048369884 + 0.1 * 6.617978572845459
Epoch 680, val loss: 1.0137499570846558
Epoch 690, training loss: 0.6729391813278198 = 0.014116641134023666 + 0.1 * 6.588225364685059
Epoch 690, val loss: 1.0203611850738525
Epoch 700, training loss: 0.6733920574188232 = 0.01355732697993517 + 0.1 * 6.598347187042236
Epoch 700, val loss: 1.0267776250839233
Epoch 710, training loss: 0.6722344160079956 = 0.013030940666794777 + 0.1 * 6.592034816741943
Epoch 710, val loss: 1.0329339504241943
Epoch 720, training loss: 0.6690787672996521 = 0.01253997627645731 + 0.1 * 6.565387725830078
Epoch 720, val loss: 1.039057970046997
Epoch 730, training loss: 0.6714509129524231 = 0.01207703910768032 + 0.1 * 6.593738555908203
Epoch 730, val loss: 1.0449821949005127
Epoch 740, training loss: 0.6685660481452942 = 0.011642005294561386 + 0.1 * 6.569240570068359
Epoch 740, val loss: 1.0508244037628174
Epoch 750, training loss: 0.6680099964141846 = 0.011233635246753693 + 0.1 * 6.567763328552246
Epoch 750, val loss: 1.0564656257629395
Epoch 760, training loss: 0.6672111749649048 = 0.010848104022443295 + 0.1 * 6.5636305809021
Epoch 760, val loss: 1.062070608139038
Epoch 770, training loss: 0.6657633781433105 = 0.010484166443347931 + 0.1 * 6.552792072296143
Epoch 770, val loss: 1.067468285560608
Epoch 780, training loss: 0.6648175120353699 = 0.010139452293515205 + 0.1 * 6.546780586242676
Epoch 780, val loss: 1.0727444887161255
Epoch 790, training loss: 0.6647841930389404 = 0.009813612326979637 + 0.1 * 6.549705982208252
Epoch 790, val loss: 1.0779865980148315
Epoch 800, training loss: 0.6647734642028809 = 0.009503873996436596 + 0.1 * 6.552695274353027
Epoch 800, val loss: 1.0829662084579468
Epoch 810, training loss: 0.6626611351966858 = 0.009211582131683826 + 0.1 * 6.5344953536987305
Epoch 810, val loss: 1.0880273580551147
Epoch 820, training loss: 0.6647032499313354 = 0.008933174423873425 + 0.1 * 6.5577006340026855
Epoch 820, val loss: 1.0928997993469238
Epoch 830, training loss: 0.6617344617843628 = 0.008669476956129074 + 0.1 * 6.530649662017822
Epoch 830, val loss: 1.09760582447052
Epoch 840, training loss: 0.6637158989906311 = 0.008418452925980091 + 0.1 * 6.552974224090576
Epoch 840, val loss: 1.1023330688476562
Epoch 850, training loss: 0.660831868648529 = 0.008178209885954857 + 0.1 * 6.526535987854004
Epoch 850, val loss: 1.106872320175171
Epoch 860, training loss: 0.6610686779022217 = 0.007950138300657272 + 0.1 * 6.531185150146484
Epoch 860, val loss: 1.1113841533660889
Epoch 870, training loss: 0.6609795093536377 = 0.007732425816357136 + 0.1 * 6.532470703125
Epoch 870, val loss: 1.1157408952713013
Epoch 880, training loss: 0.6600131392478943 = 0.007524261251091957 + 0.1 * 6.524888515472412
Epoch 880, val loss: 1.1201039552688599
Epoch 890, training loss: 0.6588564515113831 = 0.007325123995542526 + 0.1 * 6.515313148498535
Epoch 890, val loss: 1.1243672370910645
Epoch 900, training loss: 0.6594476699829102 = 0.0071342866867780685 + 0.1 * 6.523133754730225
Epoch 900, val loss: 1.1284713745117188
Epoch 910, training loss: 0.6585256457328796 = 0.006953561212867498 + 0.1 * 6.515720844268799
Epoch 910, val loss: 1.1326003074645996
Epoch 920, training loss: 0.6585552096366882 = 0.006780087482184172 + 0.1 * 6.517751216888428
Epoch 920, val loss: 1.1366807222366333
Epoch 930, training loss: 0.6569725871086121 = 0.006613267585635185 + 0.1 * 6.5035929679870605
Epoch 930, val loss: 1.140562653541565
Epoch 940, training loss: 0.6583802103996277 = 0.006453632842749357 + 0.1 * 6.519265651702881
Epoch 940, val loss: 1.1444904804229736
Epoch 950, training loss: 0.65750652551651 = 0.00629987521097064 + 0.1 * 6.51206636428833
Epoch 950, val loss: 1.1482278108596802
Epoch 960, training loss: 0.6565057635307312 = 0.006152848247438669 + 0.1 * 6.503529071807861
Epoch 960, val loss: 1.1520605087280273
Epoch 970, training loss: 0.655678927898407 = 0.006011430639773607 + 0.1 * 6.49667501449585
Epoch 970, val loss: 1.155690312385559
Epoch 980, training loss: 0.6571590900421143 = 0.005875894799828529 + 0.1 * 6.512832164764404
Epoch 980, val loss: 1.15935480594635
Epoch 990, training loss: 0.6550529599189758 = 0.0057451180182397366 + 0.1 * 6.493078231811523
Epoch 990, val loss: 1.1629397869110107
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.797575118608329
=== training gcn model ===
Epoch 0, training loss: 2.7988951206207275 = 1.9392085075378418 + 0.1 * 8.5968656539917
Epoch 0, val loss: 1.9339592456817627
Epoch 10, training loss: 2.789330005645752 = 1.9296493530273438 + 0.1 * 8.596806526184082
Epoch 10, val loss: 1.9252291917800903
Epoch 20, training loss: 2.7778124809265137 = 1.9181655645370483 + 0.1 * 8.59646987915039
Epoch 20, val loss: 1.9145112037658691
Epoch 30, training loss: 2.7617290019989014 = 1.9023538827896118 + 0.1 * 8.593750953674316
Epoch 30, val loss: 1.8996367454528809
Epoch 40, training loss: 2.736630916595459 = 1.8792738914489746 + 0.1 * 8.573569297790527
Epoch 40, val loss: 1.8780218362808228
Epoch 50, training loss: 2.6950511932373047 = 1.8468296527862549 + 0.1 * 8.482215881347656
Epoch 50, val loss: 1.8488507270812988
Epoch 60, training loss: 2.6340863704681396 = 1.8092074394226074 + 0.1 * 8.248788833618164
Epoch 60, val loss: 1.8169764280319214
Epoch 70, training loss: 2.586951732635498 = 1.7741793394088745 + 0.1 * 8.12772274017334
Epoch 70, val loss: 1.7870967388153076
Epoch 80, training loss: 2.524456739425659 = 1.7362700700759888 + 0.1 * 7.881867408752441
Epoch 80, val loss: 1.750251054763794
Epoch 90, training loss: 2.4472150802612305 = 1.6880909204483032 + 0.1 * 7.591241359710693
Epoch 90, val loss: 1.7059004306793213
Epoch 100, training loss: 2.3643925189971924 = 1.6259270906448364 + 0.1 * 7.384654521942139
Epoch 100, val loss: 1.653801441192627
Epoch 110, training loss: 2.2785849571228027 = 1.5485080480575562 + 0.1 * 7.300769329071045
Epoch 110, val loss: 1.5889034271240234
Epoch 120, training loss: 2.1844029426574707 = 1.4604439735412598 + 0.1 * 7.239589214324951
Epoch 120, val loss: 1.5141433477401733
Epoch 130, training loss: 2.0886082649230957 = 1.3697667121887207 + 0.1 * 7.188416004180908
Epoch 130, val loss: 1.4380171298980713
Epoch 140, training loss: 1.9946950674057007 = 1.2797709703445435 + 0.1 * 7.149240970611572
Epoch 140, val loss: 1.3651254177093506
Epoch 150, training loss: 1.9041516780853271 = 1.1917259693145752 + 0.1 * 7.1242570877075195
Epoch 150, val loss: 1.2966673374176025
Epoch 160, training loss: 1.8145995140075684 = 1.1045347452163696 + 0.1 * 7.100648403167725
Epoch 160, val loss: 1.231191873550415
Epoch 170, training loss: 1.7235554456710815 = 1.015913724899292 + 0.1 * 7.076416969299316
Epoch 170, val loss: 1.1650117635726929
Epoch 180, training loss: 1.6309118270874023 = 0.9253319501876831 + 0.1 * 7.055798053741455
Epoch 180, val loss: 1.096833348274231
Epoch 190, training loss: 1.5384293794631958 = 0.8344990611076355 + 0.1 * 7.039303302764893
Epoch 190, val loss: 1.0283446311950684
Epoch 200, training loss: 1.4500318765640259 = 0.7471742033958435 + 0.1 * 7.028576850891113
Epoch 200, val loss: 0.9633881449699402
Epoch 210, training loss: 1.3697795867919922 = 0.667863667011261 + 0.1 * 7.019158840179443
Epoch 210, val loss: 0.906459391117096
Epoch 220, training loss: 1.298126220703125 = 0.5967381596565247 + 0.1 * 7.013880729675293
Epoch 220, val loss: 0.8582019209861755
Epoch 230, training loss: 1.2331126928329468 = 0.5326157808303833 + 0.1 * 7.004969120025635
Epoch 230, val loss: 0.8183996677398682
Epoch 240, training loss: 1.1747095584869385 = 0.47476693987846375 + 0.1 * 6.999425888061523
Epoch 240, val loss: 0.7861529588699341
Epoch 250, training loss: 1.1216315031051636 = 0.42265525460243225 + 0.1 * 6.989761829376221
Epoch 250, val loss: 0.7604717016220093
Epoch 260, training loss: 1.073303461074829 = 0.37531647086143494 + 0.1 * 6.979869842529297
Epoch 260, val loss: 0.7401342391967773
Epoch 270, training loss: 1.0294334888458252 = 0.3324678838253021 + 0.1 * 6.969655513763428
Epoch 270, val loss: 0.7245062589645386
Epoch 280, training loss: 0.9900076389312744 = 0.2940453588962555 + 0.1 * 6.959622383117676
Epoch 280, val loss: 0.7127528786659241
Epoch 290, training loss: 0.9539619088172913 = 0.25941020250320435 + 0.1 * 6.945517063140869
Epoch 290, val loss: 0.7042565941810608
Epoch 300, training loss: 0.9221370816230774 = 0.22835861146450043 + 0.1 * 6.937784194946289
Epoch 300, val loss: 0.6985610723495483
Epoch 310, training loss: 0.8934643268585205 = 0.2009221613407135 + 0.1 * 6.925421714782715
Epoch 310, val loss: 0.695279061794281
Epoch 320, training loss: 0.8684593439102173 = 0.17674842476844788 + 0.1 * 6.91710901260376
Epoch 320, val loss: 0.6940174698829651
Epoch 330, training loss: 0.8459845185279846 = 0.15562643110752106 + 0.1 * 6.903580665588379
Epoch 330, val loss: 0.6946151256561279
Epoch 340, training loss: 0.8257526755332947 = 0.137191042304039 + 0.1 * 6.885615825653076
Epoch 340, val loss: 0.696691632270813
Epoch 350, training loss: 0.8093684315681458 = 0.12120179086923599 + 0.1 * 6.88166618347168
Epoch 350, val loss: 0.7000397443771362
Epoch 360, training loss: 0.7937811613082886 = 0.10739129036664963 + 0.1 * 6.863898277282715
Epoch 360, val loss: 0.7042591571807861
Epoch 370, training loss: 0.7807743549346924 = 0.09540390223264694 + 0.1 * 6.853704452514648
Epoch 370, val loss: 0.7096439599990845
Epoch 380, training loss: 0.7715458869934082 = 0.08501113951206207 + 0.1 * 6.865347385406494
Epoch 380, val loss: 0.7156526446342468
Epoch 390, training loss: 0.7595013380050659 = 0.0760626271367073 + 0.1 * 6.834386825561523
Epoch 390, val loss: 0.7222370505332947
Epoch 400, training loss: 0.7513212561607361 = 0.06828169524669647 + 0.1 * 6.830395221710205
Epoch 400, val loss: 0.7292686700820923
Epoch 410, training loss: 0.7443899512290955 = 0.061559390276670456 + 0.1 * 6.828305721282959
Epoch 410, val loss: 0.7364482879638672
Epoch 420, training loss: 0.7362860441207886 = 0.05572686716914177 + 0.1 * 6.805591583251953
Epoch 420, val loss: 0.7438330054283142
Epoch 430, training loss: 0.730314314365387 = 0.05062395706772804 + 0.1 * 6.796903610229492
Epoch 430, val loss: 0.7514623403549194
Epoch 440, training loss: 0.7268402576446533 = 0.046154409646987915 + 0.1 * 6.806858539581299
Epoch 440, val loss: 0.7591060400009155
Epoch 450, training loss: 0.7202353477478027 = 0.04224582388997078 + 0.1 * 6.779895305633545
Epoch 450, val loss: 0.766799807548523
Epoch 460, training loss: 0.7160496711730957 = 0.03880014270544052 + 0.1 * 6.772495269775391
Epoch 460, val loss: 0.7743420600891113
Epoch 470, training loss: 0.7122984528541565 = 0.035756465047597885 + 0.1 * 6.765419960021973
Epoch 470, val loss: 0.78182053565979
Epoch 480, training loss: 0.7091655731201172 = 0.03306037560105324 + 0.1 * 6.761052131652832
Epoch 480, val loss: 0.7892566323280334
Epoch 490, training loss: 0.705206036567688 = 0.030658859759569168 + 0.1 * 6.745471954345703
Epoch 490, val loss: 0.7966204285621643
Epoch 500, training loss: 0.7068291902542114 = 0.028510039672255516 + 0.1 * 6.783191680908203
Epoch 500, val loss: 0.8038172721862793
Epoch 510, training loss: 0.7005105018615723 = 0.026596669107675552 + 0.1 * 6.739138126373291
Epoch 510, val loss: 0.8109498620033264
Epoch 520, training loss: 0.6976400017738342 = 0.024873219430446625 + 0.1 * 6.727667808532715
Epoch 520, val loss: 0.8178192377090454
Epoch 530, training loss: 0.6961895227432251 = 0.02331482246518135 + 0.1 * 6.7287468910217285
Epoch 530, val loss: 0.824617326259613
Epoch 540, training loss: 0.6932973265647888 = 0.021905837580561638 + 0.1 * 6.71391487121582
Epoch 540, val loss: 0.8313260078430176
Epoch 550, training loss: 0.6916061639785767 = 0.02062433585524559 + 0.1 * 6.709817886352539
Epoch 550, val loss: 0.8376833200454712
Epoch 560, training loss: 0.6896551847457886 = 0.019461683928966522 + 0.1 * 6.701934814453125
Epoch 560, val loss: 0.8442235589027405
Epoch 570, training loss: 0.689163327217102 = 0.01839728280901909 + 0.1 * 6.707660675048828
Epoch 570, val loss: 0.8503485918045044
Epoch 580, training loss: 0.6868334412574768 = 0.017422346398234367 + 0.1 * 6.694110870361328
Epoch 580, val loss: 0.8565162420272827
Epoch 590, training loss: 0.6853420734405518 = 0.016528146341443062 + 0.1 * 6.68813943862915
Epoch 590, val loss: 0.8624399900436401
Epoch 600, training loss: 0.6842836141586304 = 0.01570672169327736 + 0.1 * 6.685769081115723
Epoch 600, val loss: 0.8681829571723938
Epoch 610, training loss: 0.6831798553466797 = 0.014948605559766293 + 0.1 * 6.682312488555908
Epoch 610, val loss: 0.8739065527915955
Epoch 620, training loss: 0.6830764412879944 = 0.014250698499381542 + 0.1 * 6.688257217407227
Epoch 620, val loss: 0.8793111443519592
Epoch 630, training loss: 0.6810522675514221 = 0.013607398606836796 + 0.1 * 6.674448490142822
Epoch 630, val loss: 0.8847424983978271
Epoch 640, training loss: 0.6793079972267151 = 0.013008473441004753 + 0.1 * 6.662994861602783
Epoch 640, val loss: 0.8898915648460388
Epoch 650, training loss: 0.680671215057373 = 0.012450053356587887 + 0.1 * 6.682211875915527
Epoch 650, val loss: 0.8951829671859741
Epoch 660, training loss: 0.6779243350028992 = 0.011931651271879673 + 0.1 * 6.659926891326904
Epoch 660, val loss: 0.9002484083175659
Epoch 670, training loss: 0.676910936832428 = 0.01144657377153635 + 0.1 * 6.6546430587768555
Epoch 670, val loss: 0.9050938487052917
Epoch 680, training loss: 0.6756182909011841 = 0.010992931202054024 + 0.1 * 6.64625358581543
Epoch 680, val loss: 0.9099709987640381
Epoch 690, training loss: 0.6785446405410767 = 0.010567239485681057 + 0.1 * 6.679773807525635
Epoch 690, val loss: 0.9146258234977722
Epoch 700, training loss: 0.6745617389678955 = 0.010171070694923401 + 0.1 * 6.643906593322754
Epoch 700, val loss: 0.9192784428596497
Epoch 710, training loss: 0.6740773320198059 = 0.009798054583370686 + 0.1 * 6.642792701721191
Epoch 710, val loss: 0.9237075448036194
Epoch 720, training loss: 0.6733322739601135 = 0.009446327574551105 + 0.1 * 6.638859272003174
Epoch 720, val loss: 0.9280308485031128
Epoch 730, training loss: 0.6731756925582886 = 0.009115295484662056 + 0.1 * 6.640604019165039
Epoch 730, val loss: 0.9325161576271057
Epoch 740, training loss: 0.673286497592926 = 0.00880292896181345 + 0.1 * 6.644835472106934
Epoch 740, val loss: 0.9364879727363586
Epoch 750, training loss: 0.6721160411834717 = 0.00850859098136425 + 0.1 * 6.636074542999268
Epoch 750, val loss: 0.9406099915504456
Epoch 760, training loss: 0.6699195504188538 = 0.008230027742683887 + 0.1 * 6.6168951988220215
Epoch 760, val loss: 0.9445260763168335
Epoch 770, training loss: 0.671598494052887 = 0.007966628298163414 + 0.1 * 6.636318206787109
Epoch 770, val loss: 0.948555052280426
Epoch 780, training loss: 0.6692669987678528 = 0.007715695071965456 + 0.1 * 6.615512847900391
Epoch 780, val loss: 0.9523965120315552
Epoch 790, training loss: 0.669148862361908 = 0.007479032967239618 + 0.1 * 6.616698265075684
Epoch 790, val loss: 0.9562292098999023
Epoch 800, training loss: 0.6687678098678589 = 0.007253947667777538 + 0.1 * 6.615138530731201
Epoch 800, val loss: 0.9598343372344971
Epoch 810, training loss: 0.6679192185401917 = 0.0070404428988695145 + 0.1 * 6.608787536621094
Epoch 810, val loss: 0.9635264277458191
Epoch 820, training loss: 0.6680126190185547 = 0.006837152875959873 + 0.1 * 6.611754894256592
Epoch 820, val loss: 0.9670420289039612
Epoch 830, training loss: 0.6666062474250793 = 0.006643875502049923 + 0.1 * 6.599623680114746
Epoch 830, val loss: 0.9706199169158936
Epoch 840, training loss: 0.6662021279335022 = 0.00646072207018733 + 0.1 * 6.597414016723633
Epoch 840, val loss: 0.9740822315216064
Epoch 850, training loss: 0.667020857334137 = 0.006285110488533974 + 0.1 * 6.607357025146484
Epoch 850, val loss: 0.9773998260498047
Epoch 860, training loss: 0.6653945446014404 = 0.0061174193397164345 + 0.1 * 6.592771053314209
Epoch 860, val loss: 0.9807340502738953
Epoch 870, training loss: 0.6639509797096252 = 0.005958909634500742 + 0.1 * 6.579920768737793
Epoch 870, val loss: 0.9840284585952759
Epoch 880, training loss: 0.6656426787376404 = 0.005806213244795799 + 0.1 * 6.598364353179932
Epoch 880, val loss: 0.9872308969497681
Epoch 890, training loss: 0.6652483344078064 = 0.005660078022629023 + 0.1 * 6.595882415771484
Epoch 890, val loss: 0.9904046058654785
Epoch 900, training loss: 0.6623206734657288 = 0.005521400365978479 + 0.1 * 6.567992210388184
Epoch 900, val loss: 0.9934729933738708
Epoch 910, training loss: 0.6637469530105591 = 0.005388712510466576 + 0.1 * 6.583581924438477
Epoch 910, val loss: 0.9965242743492126
Epoch 920, training loss: 0.6642460823059082 = 0.005260697100311518 + 0.1 * 6.589853763580322
Epoch 920, val loss: 0.9994552135467529
Epoch 930, training loss: 0.6615028381347656 = 0.005138445179909468 + 0.1 * 6.563643932342529
Epoch 930, val loss: 1.0023895502090454
Epoch 940, training loss: 0.663006603717804 = 0.005021586082875729 + 0.1 * 6.579850196838379
Epoch 940, val loss: 1.0052934885025024
Epoch 950, training loss: 0.6618620753288269 = 0.0049087353982031345 + 0.1 * 6.569533348083496
Epoch 950, val loss: 1.0081427097320557
Epoch 960, training loss: 0.6597527861595154 = 0.004800691734999418 + 0.1 * 6.549520969390869
Epoch 960, val loss: 1.010988473892212
Epoch 970, training loss: 0.6611549258232117 = 0.004696803633123636 + 0.1 * 6.564581394195557
Epoch 970, val loss: 1.013802409172058
Epoch 980, training loss: 0.6618370413780212 = 0.004595661535859108 + 0.1 * 6.572413921356201
Epoch 980, val loss: 1.016433596611023
Epoch 990, training loss: 0.6591195464134216 = 0.0044997925870120525 + 0.1 * 6.546197414398193
Epoch 990, val loss: 1.0191007852554321
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.808645229309436
The final CL Acc:0.75309, 0.01823, The final GNN Acc:0.80425, 0.00480
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13148])
remove edge: torch.Size([2, 7788])
updated graph: torch.Size([2, 10380])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8019843101501465 = 1.9422988891601562 + 0.1 * 8.596854209899902
Epoch 0, val loss: 1.9373587369918823
Epoch 10, training loss: 2.791471242904663 = 1.9317907094955444 + 0.1 * 8.59680461883545
Epoch 10, val loss: 1.9270509481430054
Epoch 20, training loss: 2.7787270545959473 = 1.9190754890441895 + 0.1 * 8.596514701843262
Epoch 20, val loss: 1.9146127700805664
Epoch 30, training loss: 2.7610926628112793 = 1.901686429977417 + 0.1 * 8.594062805175781
Epoch 30, val loss: 1.8974798917770386
Epoch 40, training loss: 2.733950614929199 = 1.876570701599121 + 0.1 * 8.573800086975098
Epoch 40, val loss: 1.8729170560836792
Epoch 50, training loss: 2.68973970413208 = 1.842083215713501 + 0.1 * 8.47656536102295
Epoch 50, val loss: 1.8407082557678223
Epoch 60, training loss: 2.624149799346924 = 1.8037668466567993 + 0.1 * 8.20383071899414
Epoch 60, val loss: 1.807801365852356
Epoch 70, training loss: 2.5718419551849365 = 1.7683733701705933 + 0.1 * 8.034686088562012
Epoch 70, val loss: 1.7783819437026978
Epoch 80, training loss: 2.5017073154449463 = 1.7279980182647705 + 0.1 * 7.7370924949646
Epoch 80, val loss: 1.7419931888580322
Epoch 90, training loss: 2.420452356338501 = 1.6773887872695923 + 0.1 * 7.430636405944824
Epoch 90, val loss: 1.6966493129730225
Epoch 100, training loss: 2.333664655685425 = 1.613350749015808 + 0.1 * 7.203139305114746
Epoch 100, val loss: 1.6411484479904175
Epoch 110, training loss: 2.24428653717041 = 1.5344691276550293 + 0.1 * 7.098174571990967
Epoch 110, val loss: 1.5734611749649048
Epoch 120, training loss: 2.151947498321533 = 1.4486037492752075 + 0.1 * 7.033438205718994
Epoch 120, val loss: 1.5021525621414185
Epoch 130, training loss: 2.0615787506103516 = 1.362249493598938 + 0.1 * 6.993292808532715
Epoch 130, val loss: 1.4314144849777222
Epoch 140, training loss: 1.9703245162963867 = 1.2734344005584717 + 0.1 * 6.968900680541992
Epoch 140, val loss: 1.359649419784546
Epoch 150, training loss: 1.875089406967163 = 1.1795880794525146 + 0.1 * 6.955013275146484
Epoch 150, val loss: 1.2843321561813354
Epoch 160, training loss: 1.7764892578125 = 1.0826109647750854 + 0.1 * 6.938783645629883
Epoch 160, val loss: 1.2069664001464844
Epoch 170, training loss: 1.678361177444458 = 0.9857196807861328 + 0.1 * 6.926414489746094
Epoch 170, val loss: 1.130837082862854
Epoch 180, training loss: 1.5852506160736084 = 0.8937273621559143 + 0.1 * 6.915233135223389
Epoch 180, val loss: 1.0593379735946655
Epoch 190, training loss: 1.501204013824463 = 0.8112999200820923 + 0.1 * 6.899041652679443
Epoch 190, val loss: 0.996658205986023
Epoch 200, training loss: 1.4277393817901611 = 0.7389310002326965 + 0.1 * 6.888084411621094
Epoch 200, val loss: 0.9432392120361328
Epoch 210, training loss: 1.362923502922058 = 0.6756278872489929 + 0.1 * 6.872955799102783
Epoch 210, val loss: 0.8992105722427368
Epoch 220, training loss: 1.305566430091858 = 0.6193770170211792 + 0.1 * 6.861894130706787
Epoch 220, val loss: 0.8631305694580078
Epoch 230, training loss: 1.2545056343078613 = 0.5684121251106262 + 0.1 * 6.860934734344482
Epoch 230, val loss: 0.8337036967277527
Epoch 240, training loss: 1.2066189050674438 = 0.5220723748207092 + 0.1 * 6.845465183258057
Epoch 240, val loss: 0.8101383447647095
Epoch 250, training loss: 1.1626510620117188 = 0.47913461923599243 + 0.1 * 6.8351640701293945
Epoch 250, val loss: 0.7913457155227661
Epoch 260, training loss: 1.1230747699737549 = 0.43884462118148804 + 0.1 * 6.842301368713379
Epoch 260, val loss: 0.7764164209365845
Epoch 270, training loss: 1.0827431678771973 = 0.4007774591445923 + 0.1 * 6.819657325744629
Epoch 270, val loss: 0.7645102143287659
Epoch 280, training loss: 1.046569585800171 = 0.36420223116874695 + 0.1 * 6.823673248291016
Epoch 280, val loss: 0.7549520134925842
Epoch 290, training loss: 1.0103943347930908 = 0.32926881313323975 + 0.1 * 6.811254978179932
Epoch 290, val loss: 0.7471952438354492
Epoch 300, training loss: 0.9767247438430786 = 0.296074241399765 + 0.1 * 6.806504726409912
Epoch 300, val loss: 0.7412405610084534
Epoch 310, training loss: 0.9452458620071411 = 0.26505976915359497 + 0.1 * 6.801860809326172
Epoch 310, val loss: 0.7372022867202759
Epoch 320, training loss: 0.9159982204437256 = 0.23655147850513458 + 0.1 * 6.794467449188232
Epoch 320, val loss: 0.7353644967079163
Epoch 330, training loss: 0.8892253637313843 = 0.21076294779777527 + 0.1 * 6.784623622894287
Epoch 330, val loss: 0.7357260584831238
Epoch 340, training loss: 0.8665584325790405 = 0.18777963519096375 + 0.1 * 6.787787437438965
Epoch 340, val loss: 0.7382347583770752
Epoch 350, training loss: 0.8440030217170715 = 0.16762898862361908 + 0.1 * 6.763740062713623
Epoch 350, val loss: 0.7426512837409973
Epoch 360, training loss: 0.8249807357788086 = 0.1499740481376648 + 0.1 * 6.750066757202148
Epoch 360, val loss: 0.7488306760787964
Epoch 370, training loss: 0.810164749622345 = 0.13455694913864136 + 0.1 * 6.756077766418457
Epoch 370, val loss: 0.756321370601654
Epoch 380, training loss: 0.7945126891136169 = 0.12119854241609573 + 0.1 * 6.7331414222717285
Epoch 380, val loss: 0.7646853923797607
Epoch 390, training loss: 0.7812559604644775 = 0.10948038101196289 + 0.1 * 6.7177557945251465
Epoch 390, val loss: 0.7739720940589905
Epoch 400, training loss: 0.7724371552467346 = 0.09918397665023804 + 0.1 * 6.732531547546387
Epoch 400, val loss: 0.7838480472564697
Epoch 410, training loss: 0.7613441944122314 = 0.09012236446142197 + 0.1 * 6.712218284606934
Epoch 410, val loss: 0.7939481735229492
Epoch 420, training loss: 0.7521008849143982 = 0.08208392560482025 + 0.1 * 6.700169086456299
Epoch 420, val loss: 0.8044504523277283
Epoch 430, training loss: 0.7434444427490234 = 0.0749342143535614 + 0.1 * 6.685102462768555
Epoch 430, val loss: 0.8150759935379028
Epoch 440, training loss: 0.7364289164543152 = 0.06854242831468582 + 0.1 * 6.6788649559021
Epoch 440, val loss: 0.825799822807312
Epoch 450, training loss: 0.731158971786499 = 0.06283216923475266 + 0.1 * 6.683267593383789
Epoch 450, val loss: 0.8366706967353821
Epoch 460, training loss: 0.7248871326446533 = 0.05774679407477379 + 0.1 * 6.671403408050537
Epoch 460, val loss: 0.847087025642395
Epoch 470, training loss: 0.7218140363693237 = 0.05318538472056389 + 0.1 * 6.686285972595215
Epoch 470, val loss: 0.8576027154922485
Epoch 480, training loss: 0.714123010635376 = 0.049096282571554184 + 0.1 * 6.650267124176025
Epoch 480, val loss: 0.8677505850791931
Epoch 490, training loss: 0.7093950510025024 = 0.045416299253702164 + 0.1 * 6.639787673950195
Epoch 490, val loss: 0.8778428435325623
Epoch 500, training loss: 0.7070581316947937 = 0.042097270488739014 + 0.1 * 6.649608612060547
Epoch 500, val loss: 0.8878076076507568
Epoch 510, training loss: 0.7032025456428528 = 0.03910740464925766 + 0.1 * 6.640951633453369
Epoch 510, val loss: 0.897542417049408
Epoch 520, training loss: 0.6990078091621399 = 0.03640881925821304 + 0.1 * 6.6259894371032715
Epoch 520, val loss: 0.9070906043052673
Epoch 530, training loss: 0.6968712210655212 = 0.03396988287568092 + 0.1 * 6.6290130615234375
Epoch 530, val loss: 0.9164068102836609
Epoch 540, training loss: 0.6932880878448486 = 0.031760331243276596 + 0.1 * 6.615277290344238
Epoch 540, val loss: 0.9254818558692932
Epoch 550, training loss: 0.6913731098175049 = 0.029754236340522766 + 0.1 * 6.616189002990723
Epoch 550, val loss: 0.9344409704208374
Epoch 560, training loss: 0.6882196664810181 = 0.02793322317302227 + 0.1 * 6.6028642654418945
Epoch 560, val loss: 0.9431259632110596
Epoch 570, training loss: 0.68535315990448 = 0.026274051517248154 + 0.1 * 6.59079122543335
Epoch 570, val loss: 0.9515746831893921
Epoch 580, training loss: 0.6825000643730164 = 0.024757476523518562 + 0.1 * 6.577425479888916
Epoch 580, val loss: 0.9598455429077148
Epoch 590, training loss: 0.6865764260292053 = 0.023368876427412033 + 0.1 * 6.632075309753418
Epoch 590, val loss: 0.9679446816444397
Epoch 600, training loss: 0.6787939071655273 = 0.022102758288383484 + 0.1 * 6.566911220550537
Epoch 600, val loss: 0.97575843334198
Epoch 610, training loss: 0.6780312061309814 = 0.020938999950885773 + 0.1 * 6.570921897888184
Epoch 610, val loss: 0.9833140969276428
Epoch 620, training loss: 0.6754223108291626 = 0.01986585557460785 + 0.1 * 6.5555644035339355
Epoch 620, val loss: 0.9907673001289368
Epoch 630, training loss: 0.6747058629989624 = 0.01887514442205429 + 0.1 * 6.55830717086792
Epoch 630, val loss: 0.9978746175765991
Epoch 640, training loss: 0.6730636358261108 = 0.017957070842385292 + 0.1 * 6.551065444946289
Epoch 640, val loss: 1.0050041675567627
Epoch 650, training loss: 0.67185378074646 = 0.017109477892518044 + 0.1 * 6.547443389892578
Epoch 650, val loss: 1.0117777585983276
Epoch 660, training loss: 0.6713438034057617 = 0.01632142812013626 + 0.1 * 6.5502238273620605
Epoch 660, val loss: 1.0185022354125977
Epoch 670, training loss: 0.670003354549408 = 0.015590065158903599 + 0.1 * 6.544132709503174
Epoch 670, val loss: 1.024901032447815
Epoch 680, training loss: 0.6680176258087158 = 0.014908766373991966 + 0.1 * 6.531088352203369
Epoch 680, val loss: 1.031338095664978
Epoch 690, training loss: 0.6679206490516663 = 0.01427360437810421 + 0.1 * 6.536470413208008
Epoch 690, val loss: 1.0374281406402588
Epoch 700, training loss: 0.6667182445526123 = 0.01368031743913889 + 0.1 * 6.530378818511963
Epoch 700, val loss: 1.0435227155685425
Epoch 710, training loss: 0.6651102900505066 = 0.013125372119247913 + 0.1 * 6.5198493003845215
Epoch 710, val loss: 1.049307107925415
Epoch 720, training loss: 0.6644095182418823 = 0.012604232877492905 + 0.1 * 6.518052577972412
Epoch 720, val loss: 1.055095911026001
Epoch 730, training loss: 0.6641709208488464 = 0.012115884572267532 + 0.1 * 6.52055025100708
Epoch 730, val loss: 1.0607481002807617
Epoch 740, training loss: 0.6628872156143188 = 0.011657493188977242 + 0.1 * 6.5122971534729
Epoch 740, val loss: 1.0661709308624268
Epoch 750, training loss: 0.6623669862747192 = 0.011225885711610317 + 0.1 * 6.511411190032959
Epoch 750, val loss: 1.0715733766555786
Epoch 760, training loss: 0.6616931557655334 = 0.010819380171597004 + 0.1 * 6.508737564086914
Epoch 760, val loss: 1.0768100023269653
Epoch 770, training loss: 0.6601529717445374 = 0.01043648924678564 + 0.1 * 6.497164726257324
Epoch 770, val loss: 1.082013726234436
Epoch 780, training loss: 0.6595211625099182 = 0.010075417347252369 + 0.1 * 6.494457721710205
Epoch 780, val loss: 1.0869609117507935
Epoch 790, training loss: 0.6604013442993164 = 0.009733708575367928 + 0.1 * 6.506676197052002
Epoch 790, val loss: 1.0918772220611572
Epoch 800, training loss: 0.6605848073959351 = 0.009410526603460312 + 0.1 * 6.511743068695068
Epoch 800, val loss: 1.0967425107955933
Epoch 810, training loss: 0.6594493985176086 = 0.009105303324759007 + 0.1 * 6.5034403800964355
Epoch 810, val loss: 1.1014759540557861
Epoch 820, training loss: 0.6577749252319336 = 0.008815466426312923 + 0.1 * 6.489594459533691
Epoch 820, val loss: 1.1060816049575806
Epoch 830, training loss: 0.6568934321403503 = 0.008540025912225246 + 0.1 * 6.48353385925293
Epoch 830, val loss: 1.1106711626052856
Epoch 840, training loss: 0.6569192409515381 = 0.008279097266495228 + 0.1 * 6.486401557922363
Epoch 840, val loss: 1.115090012550354
Epoch 850, training loss: 0.6573992967605591 = 0.008030223660171032 + 0.1 * 6.493690490722656
Epoch 850, val loss: 1.1194754838943481
Epoch 860, training loss: 0.6569410562515259 = 0.007794140372425318 + 0.1 * 6.491468906402588
Epoch 860, val loss: 1.1237784624099731
Epoch 870, training loss: 0.6559048295021057 = 0.007568798493593931 + 0.1 * 6.483360290527344
Epoch 870, val loss: 1.128015398979187
Epoch 880, training loss: 0.6582834124565125 = 0.0073547097854316235 + 0.1 * 6.509286403656006
Epoch 880, val loss: 1.1321860551834106
Epoch 890, training loss: 0.65497225522995 = 0.007150338031351566 + 0.1 * 6.478219032287598
Epoch 890, val loss: 1.1363390684127808
Epoch 900, training loss: 0.6545756459236145 = 0.006956017576158047 + 0.1 * 6.476195812225342
Epoch 900, val loss: 1.1402281522750854
Epoch 910, training loss: 0.6536449193954468 = 0.006769456900656223 + 0.1 * 6.468754291534424
Epoch 910, val loss: 1.1441668272018433
Epoch 920, training loss: 0.6531603336334229 = 0.006591450423002243 + 0.1 * 6.465688705444336
Epoch 920, val loss: 1.1479606628417969
Epoch 930, training loss: 0.6524483561515808 = 0.0064208609983325005 + 0.1 * 6.460274696350098
Epoch 930, val loss: 1.1517691612243652
Epoch 940, training loss: 0.6527848243713379 = 0.006257183384150267 + 0.1 * 6.46527624130249
Epoch 940, val loss: 1.1553958654403687
Epoch 950, training loss: 0.6528657674789429 = 0.006100839469581842 + 0.1 * 6.467649459838867
Epoch 950, val loss: 1.159117579460144
Epoch 960, training loss: 0.652034342288971 = 0.005950783379375935 + 0.1 * 6.460834980010986
Epoch 960, val loss: 1.1627410650253296
Epoch 970, training loss: 0.6521769762039185 = 0.005807407200336456 + 0.1 * 6.463695526123047
Epoch 970, val loss: 1.1662013530731201
Epoch 980, training loss: 0.651077389717102 = 0.0056692976504564285 + 0.1 * 6.454080581665039
Epoch 980, val loss: 1.1696207523345947
Epoch 990, training loss: 0.6526578664779663 = 0.005536629352718592 + 0.1 * 6.471212387084961
Epoch 990, val loss: 1.173014760017395
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8497627833421192
=== training gcn model ===
Epoch 0, training loss: 2.7993531227111816 = 1.9396684169769287 + 0.1 * 8.596845626831055
Epoch 0, val loss: 1.931544542312622
Epoch 10, training loss: 2.789243221282959 = 1.929569959640503 + 0.1 * 8.596732139587402
Epoch 10, val loss: 1.9221587181091309
Epoch 20, training loss: 2.776517152786255 = 1.9169219732284546 + 0.1 * 8.595951080322266
Epoch 20, val loss: 1.9100712537765503
Epoch 30, training loss: 2.758127450942993 = 1.8991737365722656 + 0.1 * 8.589536666870117
Epoch 30, val loss: 1.8929109573364258
Epoch 40, training loss: 2.7287304401397705 = 1.8733347654342651 + 0.1 * 8.553956031799316
Epoch 40, val loss: 1.868358850479126
Epoch 50, training loss: 2.678239345550537 = 1.838786244392395 + 0.1 * 8.394532203674316
Epoch 50, val loss: 1.8371998071670532
Epoch 60, training loss: 2.618252992630005 = 1.800938367843628 + 0.1 * 8.173145294189453
Epoch 60, val loss: 1.8055830001831055
Epoch 70, training loss: 2.5615525245666504 = 1.7647838592529297 + 0.1 * 7.967686653137207
Epoch 70, val loss: 1.7763421535491943
Epoch 80, training loss: 2.4780657291412354 = 1.7252670526504517 + 0.1 * 7.527987003326416
Epoch 80, val loss: 1.7409077882766724
Epoch 90, training loss: 2.3960158824920654 = 1.6757529973983765 + 0.1 * 7.202628135681152
Epoch 90, val loss: 1.6959182024002075
Epoch 100, training loss: 2.3214657306671143 = 1.6098480224609375 + 0.1 * 7.116176128387451
Epoch 100, val loss: 1.638207197189331
Epoch 110, training loss: 2.2354605197906494 = 1.5284420251846313 + 0.1 * 7.07018518447876
Epoch 110, val loss: 1.5696693658828735
Epoch 120, training loss: 2.142789602279663 = 1.4392879009246826 + 0.1 * 7.035017490386963
Epoch 120, val loss: 1.496042013168335
Epoch 130, training loss: 2.048767566680908 = 1.3480923175811768 + 0.1 * 7.006753444671631
Epoch 130, val loss: 1.421176791191101
Epoch 140, training loss: 1.953124761581421 = 1.254692554473877 + 0.1 * 6.984321594238281
Epoch 140, val loss: 1.3455091714859009
Epoch 150, training loss: 1.8546987771987915 = 1.1582773923873901 + 0.1 * 6.964213848114014
Epoch 150, val loss: 1.2680195569992065
Epoch 160, training loss: 1.756061315536499 = 1.0611790418624878 + 0.1 * 6.948822498321533
Epoch 160, val loss: 1.1908432245254517
Epoch 170, training loss: 1.6609233617782593 = 0.9677048325538635 + 0.1 * 6.932185173034668
Epoch 170, val loss: 1.1176462173461914
Epoch 180, training loss: 1.5702555179595947 = 0.8783938884735107 + 0.1 * 6.91861629486084
Epoch 180, val loss: 1.0480859279632568
Epoch 190, training loss: 1.4845061302185059 = 0.7944050431251526 + 0.1 * 6.901010990142822
Epoch 190, val loss: 0.9831380248069763
Epoch 200, training loss: 1.405272364616394 = 0.7162966728210449 + 0.1 * 6.889756679534912
Epoch 200, val loss: 0.9242494702339172
Epoch 210, training loss: 1.3334800004959106 = 0.6452111005783081 + 0.1 * 6.882688999176025
Epoch 210, val loss: 0.8729897737503052
Epoch 220, training loss: 1.2691138982772827 = 0.5817124843597412 + 0.1 * 6.874013900756836
Epoch 220, val loss: 0.8308311104774475
Epoch 230, training loss: 1.2114852666854858 = 0.5247189998626709 + 0.1 * 6.86766242980957
Epoch 230, val loss: 0.7967032790184021
Epoch 240, training loss: 1.1603214740753174 = 0.4733419716358185 + 0.1 * 6.869795322418213
Epoch 240, val loss: 0.7694782018661499
Epoch 250, training loss: 1.1129032373428345 = 0.4270891845226288 + 0.1 * 6.85814094543457
Epoch 250, val loss: 0.7479091286659241
Epoch 260, training loss: 1.0703463554382324 = 0.38497093319892883 + 0.1 * 6.853754043579102
Epoch 260, val loss: 0.7307687997817993
Epoch 270, training loss: 1.0315629243850708 = 0.3465094566345215 + 0.1 * 6.850534439086914
Epoch 270, val loss: 0.7176403403282166
Epoch 280, training loss: 0.9973645210266113 = 0.311475932598114 + 0.1 * 6.858885765075684
Epoch 280, val loss: 0.7082441449165344
Epoch 290, training loss: 0.9644299745559692 = 0.2798021137714386 + 0.1 * 6.846278667449951
Epoch 290, val loss: 0.7022464871406555
Epoch 300, training loss: 0.935124397277832 = 0.25092366337776184 + 0.1 * 6.842007160186768
Epoch 300, val loss: 0.6991162300109863
Epoch 310, training loss: 0.9088586568832397 = 0.22449545562267303 + 0.1 * 6.843631744384766
Epoch 310, val loss: 0.6984245777130127
Epoch 320, training loss: 0.88438880443573 = 0.2005101889371872 + 0.1 * 6.8387861251831055
Epoch 320, val loss: 0.6998559832572937
Epoch 330, training loss: 0.8623753786087036 = 0.17891976237297058 + 0.1 * 6.834555625915527
Epoch 330, val loss: 0.7032614946365356
Epoch 340, training loss: 0.8427987098693848 = 0.1597326248884201 + 0.1 * 6.830660820007324
Epoch 340, val loss: 0.7083966135978699
Epoch 350, training loss: 0.825851559638977 = 0.14288684725761414 + 0.1 * 6.829647541046143
Epoch 350, val loss: 0.71498042345047
Epoch 360, training loss: 0.8105305433273315 = 0.12819017469882965 + 0.1 * 6.823403835296631
Epoch 360, val loss: 0.7226987481117249
Epoch 370, training loss: 0.7971317172050476 = 0.11531905829906464 + 0.1 * 6.818126201629639
Epoch 370, val loss: 0.7314287424087524
Epoch 380, training loss: 0.7860438823699951 = 0.10401418060064316 + 0.1 * 6.8202972412109375
Epoch 380, val loss: 0.7408838272094727
Epoch 390, training loss: 0.775867223739624 = 0.0940849557518959 + 0.1 * 6.817822456359863
Epoch 390, val loss: 0.7508127093315125
Epoch 400, training loss: 0.7663009166717529 = 0.08535169810056686 + 0.1 * 6.809492111206055
Epoch 400, val loss: 0.761046826839447
Epoch 410, training loss: 0.757638156414032 = 0.07761929929256439 + 0.1 * 6.800188064575195
Epoch 410, val loss: 0.7715223431587219
Epoch 420, training loss: 0.7504780292510986 = 0.07075652480125427 + 0.1 * 6.797214508056641
Epoch 420, val loss: 0.7821707129478455
Epoch 430, training loss: 0.7437636256217957 = 0.06466318666934967 + 0.1 * 6.791004657745361
Epoch 430, val loss: 0.792812705039978
Epoch 440, training loss: 0.7376431226730347 = 0.05922764539718628 + 0.1 * 6.784154415130615
Epoch 440, val loss: 0.8034884333610535
Epoch 450, training loss: 0.7339469194412231 = 0.05436914041638374 + 0.1 * 6.795777797698975
Epoch 450, val loss: 0.8140833973884583
Epoch 460, training loss: 0.728399395942688 = 0.05004623159766197 + 0.1 * 6.783531665802002
Epoch 460, val loss: 0.8244792819023132
Epoch 470, training loss: 0.723239541053772 = 0.04617804288864136 + 0.1 * 6.7706146240234375
Epoch 470, val loss: 0.8347320556640625
Epoch 480, training loss: 0.7191814184188843 = 0.0427030511200428 + 0.1 * 6.76478385925293
Epoch 480, val loss: 0.8448604345321655
Epoch 490, training loss: 0.7160439491271973 = 0.03958100825548172 + 0.1 * 6.764629364013672
Epoch 490, val loss: 0.8547964692115784
Epoch 500, training loss: 0.7119242548942566 = 0.036771323531866074 + 0.1 * 6.751529693603516
Epoch 500, val loss: 0.8644986748695374
Epoch 510, training loss: 0.7091645002365112 = 0.03424113988876343 + 0.1 * 6.749233722686768
Epoch 510, val loss: 0.8739884495735168
Epoch 520, training loss: 0.7059043049812317 = 0.03196443244814873 + 0.1 * 6.739398956298828
Epoch 520, val loss: 0.8831233382225037
Epoch 530, training loss: 0.7033096551895142 = 0.02990086004137993 + 0.1 * 6.734087944030762
Epoch 530, val loss: 0.8921692967414856
Epoch 540, training loss: 0.7008659839630127 = 0.02802162617444992 + 0.1 * 6.728443145751953
Epoch 540, val loss: 0.9010263085365295
Epoch 550, training loss: 0.6988816261291504 = 0.026308059692382812 + 0.1 * 6.725735664367676
Epoch 550, val loss: 0.9096583724021912
Epoch 560, training loss: 0.6967858076095581 = 0.02474878542125225 + 0.1 * 6.720369815826416
Epoch 560, val loss: 0.9180312752723694
Epoch 570, training loss: 0.6945279836654663 = 0.02332836389541626 + 0.1 * 6.711996078491211
Epoch 570, val loss: 0.9261254072189331
Epoch 580, training loss: 0.6925092935562134 = 0.02202647551894188 + 0.1 * 6.704827785491943
Epoch 580, val loss: 0.9341409802436829
Epoch 590, training loss: 0.6907677054405212 = 0.020831162109971046 + 0.1 * 6.699365139007568
Epoch 590, val loss: 0.941861093044281
Epoch 600, training loss: 0.6891303062438965 = 0.019736332818865776 + 0.1 * 6.693939685821533
Epoch 600, val loss: 0.9493027329444885
Epoch 610, training loss: 0.6876786947250366 = 0.01872774213552475 + 0.1 * 6.689509868621826
Epoch 610, val loss: 0.9566840529441833
Epoch 620, training loss: 0.6857855916023254 = 0.0177953839302063 + 0.1 * 6.679902076721191
Epoch 620, val loss: 0.9639195203781128
Epoch 630, training loss: 0.6855339407920837 = 0.016931556165218353 + 0.1 * 6.686024188995361
Epoch 630, val loss: 0.9709159731864929
Epoch 640, training loss: 0.6837962865829468 = 0.016132157295942307 + 0.1 * 6.67664098739624
Epoch 640, val loss: 0.9777679443359375
Epoch 650, training loss: 0.6839517951011658 = 0.015390175394713879 + 0.1 * 6.685616493225098
Epoch 650, val loss: 0.9845144748687744
Epoch 660, training loss: 0.6809667944908142 = 0.01470208540558815 + 0.1 * 6.662647247314453
Epoch 660, val loss: 0.9909998178482056
Epoch 670, training loss: 0.6799161434173584 = 0.014061241410672665 + 0.1 * 6.6585493087768555
Epoch 670, val loss: 0.9974123239517212
Epoch 680, training loss: 0.6786611676216125 = 0.013463882729411125 + 0.1 * 6.651972770690918
Epoch 680, val loss: 1.0036342144012451
Epoch 690, training loss: 0.6775254011154175 = 0.012907999567687511 + 0.1 * 6.64617395401001
Epoch 690, val loss: 1.0096032619476318
Epoch 700, training loss: 0.676802933216095 = 0.012388496659696102 + 0.1 * 6.644144535064697
Epoch 700, val loss: 1.0155653953552246
Epoch 710, training loss: 0.6756830215454102 = 0.011900641024112701 + 0.1 * 6.637823581695557
Epoch 710, val loss: 1.021409511566162
Epoch 720, training loss: 0.6750184297561646 = 0.011441641487181187 + 0.1 * 6.635767459869385
Epoch 720, val loss: 1.027120590209961
Epoch 730, training loss: 0.6744006872177124 = 0.011011063121259212 + 0.1 * 6.6338958740234375
Epoch 730, val loss: 1.0326660871505737
Epoch 740, training loss: 0.6743685603141785 = 0.010606041178107262 + 0.1 * 6.637625217437744
Epoch 740, val loss: 1.0381197929382324
Epoch 750, training loss: 0.6732478141784668 = 0.01022403035312891 + 0.1 * 6.630237579345703
Epoch 750, val loss: 1.0434892177581787
Epoch 760, training loss: 0.6723493933677673 = 0.00986457895487547 + 0.1 * 6.624847888946533
Epoch 760, val loss: 1.048661231994629
Epoch 770, training loss: 0.6715841889381409 = 0.009525880217552185 + 0.1 * 6.6205830574035645
Epoch 770, val loss: 1.0537396669387817
Epoch 780, training loss: 0.6704150438308716 = 0.009205744601786137 + 0.1 * 6.6120924949646
Epoch 780, val loss: 1.058825969696045
Epoch 790, training loss: 0.6712247729301453 = 0.008902283385396004 + 0.1 * 6.623224258422852
Epoch 790, val loss: 1.0637613534927368
Epoch 800, training loss: 0.6706704497337341 = 0.008614606224000454 + 0.1 * 6.620558261871338
Epoch 800, val loss: 1.0685738325119019
Epoch 810, training loss: 0.6692942976951599 = 0.00834352895617485 + 0.1 * 6.6095075607299805
Epoch 810, val loss: 1.0732696056365967
Epoch 820, training loss: 0.6683606505393982 = 0.00808622594922781 + 0.1 * 6.6027445793151855
Epoch 820, val loss: 1.0779392719268799
Epoch 830, training loss: 0.6679288148880005 = 0.007842476479709148 + 0.1 * 6.600863456726074
Epoch 830, val loss: 1.0824483633041382
Epoch 840, training loss: 0.6675662994384766 = 0.007610619068145752 + 0.1 * 6.5995564460754395
Epoch 840, val loss: 1.0869603157043457
Epoch 850, training loss: 0.6666592359542847 = 0.007390158250927925 + 0.1 * 6.592690944671631
Epoch 850, val loss: 1.0913927555084229
Epoch 860, training loss: 0.6680116057395935 = 0.007180183194577694 + 0.1 * 6.608314514160156
Epoch 860, val loss: 1.0957517623901367
Epoch 870, training loss: 0.6664796471595764 = 0.0069797709584236145 + 0.1 * 6.594998836517334
Epoch 870, val loss: 1.0999337434768677
Epoch 880, training loss: 0.6651671528816223 = 0.0067897979170084 + 0.1 * 6.583773136138916
Epoch 880, val loss: 1.1040713787078857
Epoch 890, training loss: 0.66411954164505 = 0.006608559284359217 + 0.1 * 6.575109958648682
Epoch 890, val loss: 1.1082208156585693
Epoch 900, training loss: 0.6675963997840881 = 0.006435285788029432 + 0.1 * 6.611611366271973
Epoch 900, val loss: 1.112256646156311
Epoch 910, training loss: 0.6629392504692078 = 0.006269204430282116 + 0.1 * 6.566700458526611
Epoch 910, val loss: 1.1161916255950928
Epoch 920, training loss: 0.6633847951889038 = 0.006111800670623779 + 0.1 * 6.57273006439209
Epoch 920, val loss: 1.1200681924819946
Epoch 930, training loss: 0.661761462688446 = 0.005960356444120407 + 0.1 * 6.558011054992676
Epoch 930, val loss: 1.1238985061645508
Epoch 940, training loss: 0.6616439819335938 = 0.005815831944346428 + 0.1 * 6.558281898498535
Epoch 940, val loss: 1.127616047859192
Epoch 950, training loss: 0.6619911789894104 = 0.005677293986082077 + 0.1 * 6.563138961791992
Epoch 950, val loss: 1.131371021270752
Epoch 960, training loss: 0.6635884642601013 = 0.005543964449316263 + 0.1 * 6.580445289611816
Epoch 960, val loss: 1.1350423097610474
Epoch 970, training loss: 0.6610743403434753 = 0.005415759980678558 + 0.1 * 6.556585788726807
Epoch 970, val loss: 1.1385834217071533
Epoch 980, training loss: 0.6610033512115479 = 0.0052932314574718475 + 0.1 * 6.557100772857666
Epoch 980, val loss: 1.1421083211898804
Epoch 990, training loss: 0.6597280502319336 = 0.005175394471734762 + 0.1 * 6.545526027679443
Epoch 990, val loss: 1.1455438137054443
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 2.8182973861694336 = 1.9586118459701538 + 0.1 * 8.596855163574219
Epoch 0, val loss: 1.963968276977539
Epoch 10, training loss: 2.8066325187683105 = 1.9469544887542725 + 0.1 * 8.596778869628906
Epoch 10, val loss: 1.9519052505493164
Epoch 20, training loss: 2.792066812515259 = 1.932431936264038 + 0.1 * 8.596348762512207
Epoch 20, val loss: 1.9369120597839355
Epoch 30, training loss: 2.771263599395752 = 1.9120148420333862 + 0.1 * 8.592488288879395
Epoch 30, val loss: 1.9160237312316895
Epoch 40, training loss: 2.738508701324463 = 1.8820993900299072 + 0.1 * 8.564091682434082
Epoch 40, val loss: 1.8858375549316406
Epoch 50, training loss: 2.6853013038635254 = 1.842058539390564 + 0.1 * 8.432426452636719
Epoch 50, val loss: 1.8473851680755615
Epoch 60, training loss: 2.622445821762085 = 1.8002307415008545 + 0.1 * 8.222150802612305
Epoch 60, val loss: 1.8101425170898438
Epoch 70, training loss: 2.5687665939331055 = 1.7638808488845825 + 0.1 * 8.048857688903809
Epoch 70, val loss: 1.7779299020767212
Epoch 80, training loss: 2.4954001903533936 = 1.72299063205719 + 0.1 * 7.724096298217773
Epoch 80, val loss: 1.738398551940918
Epoch 90, training loss: 2.4167237281799316 = 1.6710139513015747 + 0.1 * 7.45709753036499
Epoch 90, val loss: 1.6906030178070068
Epoch 100, training loss: 2.327887773513794 = 1.60323166847229 + 0.1 * 7.246561050415039
Epoch 100, val loss: 1.6323550939559937
Epoch 110, training loss: 2.2372477054595947 = 1.5206310749053955 + 0.1 * 7.166165351867676
Epoch 110, val loss: 1.560057282447815
Epoch 120, training loss: 2.1444203853607178 = 1.4314428567886353 + 0.1 * 7.129774570465088
Epoch 120, val loss: 1.482316493988037
Epoch 130, training loss: 2.052983522415161 = 1.3420310020446777 + 0.1 * 7.109524250030518
Epoch 130, val loss: 1.4060152769088745
Epoch 140, training loss: 1.9608726501464844 = 1.2508745193481445 + 0.1 * 7.099981307983398
Epoch 140, val loss: 1.3300420045852661
Epoch 150, training loss: 1.8652417659759521 = 1.1557523012161255 + 0.1 * 7.094893932342529
Epoch 150, val loss: 1.2506487369537354
Epoch 160, training loss: 1.765392780303955 = 1.0563517808914185 + 0.1 * 7.0904107093811035
Epoch 160, val loss: 1.1680315732955933
Epoch 170, training loss: 1.6633604764938354 = 0.9550642371177673 + 0.1 * 7.0829620361328125
Epoch 170, val loss: 1.0848361253738403
Epoch 180, training loss: 1.563850998878479 = 0.8568413853645325 + 0.1 * 7.070096015930176
Epoch 180, val loss: 1.0054723024368286
Epoch 190, training loss: 1.4721026420593262 = 0.7670472860336304 + 0.1 * 7.050553798675537
Epoch 190, val loss: 0.9348257184028625
Epoch 200, training loss: 1.3916780948638916 = 0.6891818642616272 + 0.1 * 7.024961948394775
Epoch 200, val loss: 0.8762210011482239
Epoch 210, training loss: 1.3236725330352783 = 0.6234486699104309 + 0.1 * 7.002238750457764
Epoch 210, val loss: 0.8306519389152527
Epoch 220, training loss: 1.2660049200057983 = 0.5680651068687439 + 0.1 * 6.979398250579834
Epoch 220, val loss: 0.7960257530212402
Epoch 230, training loss: 1.216926097869873 = 0.5205621719360352 + 0.1 * 6.963639259338379
Epoch 230, val loss: 0.769883930683136
Epoch 240, training loss: 1.1735928058624268 = 0.4794907867908478 + 0.1 * 6.941019535064697
Epoch 240, val loss: 0.7502351999282837
Epoch 250, training loss: 1.1366727352142334 = 0.44281187653541565 + 0.1 * 6.938608646392822
Epoch 250, val loss: 0.7349212169647217
Epoch 260, training loss: 1.1003220081329346 = 0.40943682193756104 + 0.1 * 6.9088521003723145
Epoch 260, val loss: 0.7229726314544678
Epoch 270, training loss: 1.0670382976531982 = 0.3781052827835083 + 0.1 * 6.889330863952637
Epoch 270, val loss: 0.71314537525177
Epoch 280, training loss: 1.0355713367462158 = 0.3479747772216797 + 0.1 * 6.875965118408203
Epoch 280, val loss: 0.7050105333328247
Epoch 290, training loss: 1.0069175958633423 = 0.31880179047584534 + 0.1 * 6.881158351898193
Epoch 290, val loss: 0.6984825134277344
Epoch 300, training loss: 0.9757572412490845 = 0.2904678285121918 + 0.1 * 6.852893829345703
Epoch 300, val loss: 0.6928903460502625
Epoch 310, training loss: 0.9469341039657593 = 0.26260942220687866 + 0.1 * 6.843246936798096
Epoch 310, val loss: 0.688506543636322
Epoch 320, training loss: 0.9220107197761536 = 0.2354816049337387 + 0.1 * 6.865290641784668
Epoch 320, val loss: 0.6852520704269409
Epoch 330, training loss: 0.8932334780693054 = 0.20995770394802094 + 0.1 * 6.832757472991943
Epoch 330, val loss: 0.6835120320320129
Epoch 340, training loss: 0.8684059977531433 = 0.18642772734165192 + 0.1 * 6.819782733917236
Epoch 340, val loss: 0.6835463643074036
Epoch 350, training loss: 0.8476903438568115 = 0.1654191017150879 + 0.1 * 6.822712421417236
Epoch 350, val loss: 0.6856269240379333
Epoch 360, training loss: 0.8276800513267517 = 0.14708995819091797 + 0.1 * 6.805901050567627
Epoch 360, val loss: 0.6897981762886047
Epoch 370, training loss: 0.810823917388916 = 0.1311858594417572 + 0.1 * 6.796380996704102
Epoch 370, val loss: 0.6959720849990845
Epoch 380, training loss: 0.7984280586242676 = 0.11747340857982635 + 0.1 * 6.80954647064209
Epoch 380, val loss: 0.7036932706832886
Epoch 390, training loss: 0.7828671336174011 = 0.10568827390670776 + 0.1 * 6.771788597106934
Epoch 390, val loss: 0.7126949429512024
Epoch 400, training loss: 0.774073600769043 = 0.09546962380409241 + 0.1 * 6.78603982925415
Epoch 400, val loss: 0.7226141691207886
Epoch 410, training loss: 0.7622814178466797 = 0.08660329878330231 + 0.1 * 6.756781101226807
Epoch 410, val loss: 0.7330643534660339
Epoch 420, training loss: 0.7530587911605835 = 0.07884915173053741 + 0.1 * 6.742096424102783
Epoch 420, val loss: 0.743838906288147
Epoch 430, training loss: 0.7454484701156616 = 0.07198747992515564 + 0.1 * 6.734610080718994
Epoch 430, val loss: 0.7549754977226257
Epoch 440, training loss: 0.7396827340126038 = 0.06591357290744781 + 0.1 * 6.737691879272461
Epoch 440, val loss: 0.7662270665168762
Epoch 450, training loss: 0.7323680520057678 = 0.060514114797115326 + 0.1 * 6.718539237976074
Epoch 450, val loss: 0.7773649096488953
Epoch 460, training loss: 0.7268787622451782 = 0.05569776892662048 + 0.1 * 6.711810111999512
Epoch 460, val loss: 0.7885530591011047
Epoch 470, training loss: 0.7213164567947388 = 0.051399875432252884 + 0.1 * 6.6991658210754395
Epoch 470, val loss: 0.7994166612625122
Epoch 480, training loss: 0.7185172438621521 = 0.0475337915122509 + 0.1 * 6.709834098815918
Epoch 480, val loss: 0.8101399540901184
Epoch 490, training loss: 0.713081419467926 = 0.0440579392015934 + 0.1 * 6.690234184265137
Epoch 490, val loss: 0.8206574320793152
Epoch 500, training loss: 0.7099679708480835 = 0.04091586917638779 + 0.1 * 6.690521240234375
Epoch 500, val loss: 0.8309609889984131
Epoch 510, training loss: 0.7065317630767822 = 0.038081973791122437 + 0.1 * 6.684497833251953
Epoch 510, val loss: 0.8410434722900391
Epoch 520, training loss: 0.7020495533943176 = 0.0355195626616478 + 0.1 * 6.665299892425537
Epoch 520, val loss: 0.8507684469223022
Epoch 530, training loss: 0.7027403712272644 = 0.03319152444601059 + 0.1 * 6.695488452911377
Epoch 530, val loss: 0.8603271245956421
Epoch 540, training loss: 0.6969267725944519 = 0.031084606423974037 + 0.1 * 6.658421516418457
Epoch 540, val loss: 0.8695065975189209
Epoch 550, training loss: 0.6932772994041443 = 0.029165728017687798 + 0.1 * 6.641115188598633
Epoch 550, val loss: 0.8783891201019287
Epoch 560, training loss: 0.6929870843887329 = 0.027409058064222336 + 0.1 * 6.655779838562012
Epoch 560, val loss: 0.8871812224388123
Epoch 570, training loss: 0.6899129748344421 = 0.02581080049276352 + 0.1 * 6.641021728515625
Epoch 570, val loss: 0.8956100940704346
Epoch 580, training loss: 0.690883219242096 = 0.024354202672839165 + 0.1 * 6.665289878845215
Epoch 580, val loss: 0.9037114977836609
Epoch 590, training loss: 0.6859196424484253 = 0.02302474156022072 + 0.1 * 6.628949165344238
Epoch 590, val loss: 0.9115362167358398
Epoch 600, training loss: 0.6839309930801392 = 0.021798202767968178 + 0.1 * 6.621327877044678
Epoch 600, val loss: 0.9192739129066467
Epoch 610, training loss: 0.6829253435134888 = 0.02066868357360363 + 0.1 * 6.622566223144531
Epoch 610, val loss: 0.9266893863677979
Epoch 620, training loss: 0.6798731088638306 = 0.01962759904563427 + 0.1 * 6.602455139160156
Epoch 620, val loss: 0.9340011477470398
Epoch 630, training loss: 0.678871750831604 = 0.018664652481675148 + 0.1 * 6.6020708084106445
Epoch 630, val loss: 0.9410719275474548
Epoch 640, training loss: 0.67884361743927 = 0.01777665503323078 + 0.1 * 6.6106696128845215
Epoch 640, val loss: 0.9479984045028687
Epoch 650, training loss: 0.6775182485580444 = 0.01695149764418602 + 0.1 * 6.605667591094971
Epoch 650, val loss: 0.9546419382095337
Epoch 660, training loss: 0.6770730018615723 = 0.01618655025959015 + 0.1 * 6.6088643074035645
Epoch 660, val loss: 0.9611618518829346
Epoch 670, training loss: 0.6742317080497742 = 0.015477257780730724 + 0.1 * 6.5875444412231445
Epoch 670, val loss: 0.967526912689209
Epoch 680, training loss: 0.6732475161552429 = 0.014813301153481007 + 0.1 * 6.584342002868652
Epoch 680, val loss: 0.9737350940704346
Epoch 690, training loss: 0.6717007160186768 = 0.014192882925271988 + 0.1 * 6.57507848739624
Epoch 690, val loss: 0.9797838926315308
Epoch 700, training loss: 0.6703967452049255 = 0.013614446856081486 + 0.1 * 6.567822456359863
Epoch 700, val loss: 0.985709547996521
Epoch 710, training loss: 0.670495867729187 = 0.01307317428290844 + 0.1 * 6.574227333068848
Epoch 710, val loss: 0.9914739727973938
Epoch 720, training loss: 0.6705811619758606 = 0.012565440498292446 + 0.1 * 6.580157279968262
Epoch 720, val loss: 0.9971309900283813
Epoch 730, training loss: 0.6690252423286438 = 0.012089376337826252 + 0.1 * 6.569358825683594
Epoch 730, val loss: 1.0025972127914429
Epoch 740, training loss: 0.6683837175369263 = 0.011642562225461006 + 0.1 * 6.567411422729492
Epoch 740, val loss: 1.0079586505889893
Epoch 750, training loss: 0.6657218337059021 = 0.011223841458559036 + 0.1 * 6.544979572296143
Epoch 750, val loss: 1.013214349746704
Epoch 760, training loss: 0.6652497053146362 = 0.010828185826539993 + 0.1 * 6.544214725494385
Epoch 760, val loss: 1.01833975315094
Epoch 770, training loss: 0.6657196879386902 = 0.01045371312648058 + 0.1 * 6.55265998840332
Epoch 770, val loss: 1.0233489274978638
Epoch 780, training loss: 0.664528489112854 = 0.010100975632667542 + 0.1 * 6.544275283813477
Epoch 780, val loss: 1.0283331871032715
Epoch 790, training loss: 0.6642624735832214 = 0.009768023155629635 + 0.1 * 6.544944763183594
Epoch 790, val loss: 1.0331525802612305
Epoch 800, training loss: 0.663565993309021 = 0.009452094323933125 + 0.1 * 6.541138648986816
Epoch 800, val loss: 1.0378917455673218
Epoch 810, training loss: 0.6661596894264221 = 0.009152635931968689 + 0.1 * 6.570070266723633
Epoch 810, val loss: 1.0425102710723877
Epoch 820, training loss: 0.6622735261917114 = 0.008869914337992668 + 0.1 * 6.534036159515381
Epoch 820, val loss: 1.0471248626708984
Epoch 830, training loss: 0.6620660424232483 = 0.008601061068475246 + 0.1 * 6.534649848937988
Epoch 830, val loss: 1.0515811443328857
Epoch 840, training loss: 0.6604205965995789 = 0.00834488682448864 + 0.1 * 6.52075719833374
Epoch 840, val loss: 1.056012749671936
Epoch 850, training loss: 0.6592461466789246 = 0.008101163432002068 + 0.1 * 6.511449813842773
Epoch 850, val loss: 1.0603324174880981
Epoch 860, training loss: 0.6588210463523865 = 0.007869322784245014 + 0.1 * 6.509517192840576
Epoch 860, val loss: 1.0646041631698608
Epoch 870, training loss: 0.658288300037384 = 0.007647844962775707 + 0.1 * 6.506403923034668
Epoch 870, val loss: 1.0687978267669678
Epoch 880, training loss: 0.6604609489440918 = 0.007437524385750294 + 0.1 * 6.530233860015869
Epoch 880, val loss: 1.0728965997695923
Epoch 890, training loss: 0.6589856743812561 = 0.007237385027110577 + 0.1 * 6.517482757568359
Epoch 890, val loss: 1.0769171714782715
Epoch 900, training loss: 0.656421959400177 = 0.00704585388302803 + 0.1 * 6.49376106262207
Epoch 900, val loss: 1.0808594226837158
Epoch 910, training loss: 0.6593749523162842 = 0.0068619404919445515 + 0.1 * 6.525130271911621
Epoch 910, val loss: 1.084738850593567
Epoch 920, training loss: 0.6578564643859863 = 0.006685923784971237 + 0.1 * 6.51170539855957
Epoch 920, val loss: 1.0885885953903198
Epoch 930, training loss: 0.6580501794815063 = 0.0065177734941244125 + 0.1 * 6.515324115753174
Epoch 930, val loss: 1.0923488140106201
Epoch 940, training loss: 0.6578272581100464 = 0.006356655620038509 + 0.1 * 6.514706134796143
Epoch 940, val loss: 1.0960499048233032
Epoch 950, training loss: 0.6551292538642883 = 0.006202322896569967 + 0.1 * 6.489269256591797
Epoch 950, val loss: 1.0997066497802734
Epoch 960, training loss: 0.6558977961540222 = 0.006053842604160309 + 0.1 * 6.498439311981201
Epoch 960, val loss: 1.1032780408859253
Epoch 970, training loss: 0.65473473072052 = 0.0059112259186804295 + 0.1 * 6.488234996795654
Epoch 970, val loss: 1.1067899465560913
Epoch 980, training loss: 0.654968798160553 = 0.005774231627583504 + 0.1 * 6.491945266723633
Epoch 980, val loss: 1.1102328300476074
Epoch 990, training loss: 0.6550893783569336 = 0.005642944481223822 + 0.1 * 6.49446439743042
Epoch 990, val loss: 1.1136928796768188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8439641539272537
The final CL Acc:0.80864, 0.00175, The final GNN Acc:0.84555, 0.00301
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11556])
remove edge: torch.Size([2, 9458])
updated graph: torch.Size([2, 10458])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.809471607208252 = 1.9497878551483154 + 0.1 * 8.596837043762207
Epoch 0, val loss: 1.9503720998764038
Epoch 10, training loss: 2.799622058868408 = 1.9399434328079224 + 0.1 * 8.596787452697754
Epoch 10, val loss: 1.9401121139526367
Epoch 20, training loss: 2.7880892753601074 = 1.928438425064087 + 0.1 * 8.596508979797363
Epoch 20, val loss: 1.9279767274856567
Epoch 30, training loss: 2.772247552871704 = 1.9128011465072632 + 0.1 * 8.594464302062988
Epoch 30, val loss: 1.9114409685134888
Epoch 40, training loss: 2.7478158473968506 = 1.8899884223937988 + 0.1 * 8.578274726867676
Epoch 40, val loss: 1.8873392343521118
Epoch 50, training loss: 2.7066450119018555 = 1.8574374914169312 + 0.1 * 8.49207592010498
Epoch 50, val loss: 1.854063868522644
Epoch 60, training loss: 2.63749623298645 = 1.8190929889678955 + 0.1 * 8.184032440185547
Epoch 60, val loss: 1.817690372467041
Epoch 70, training loss: 2.572909116744995 = 1.7832621335983276 + 0.1 * 7.8964691162109375
Epoch 70, val loss: 1.7863478660583496
Epoch 80, training loss: 2.498990297317505 = 1.74904203414917 + 0.1 * 7.499482154846191
Epoch 80, val loss: 1.757495403289795
Epoch 90, training loss: 2.439819812774658 = 1.7079286575317383 + 0.1 * 7.318910121917725
Epoch 90, val loss: 1.7226132154464722
Epoch 100, training loss: 2.375258207321167 = 1.6524630784988403 + 0.1 * 7.227952003479004
Epoch 100, val loss: 1.6752187013626099
Epoch 110, training loss: 2.293513536453247 = 1.5804855823516846 + 0.1 * 7.130278587341309
Epoch 110, val loss: 1.6150004863739014
Epoch 120, training loss: 2.2002975940704346 = 1.4947327375411987 + 0.1 * 7.0556488037109375
Epoch 120, val loss: 1.5448205471038818
Epoch 130, training loss: 2.102485179901123 = 1.4010388851165771 + 0.1 * 7.014462947845459
Epoch 130, val loss: 1.469403624534607
Epoch 140, training loss: 2.0034728050231934 = 1.3041603565216064 + 0.1 * 6.993124961853027
Epoch 140, val loss: 1.3941218852996826
Epoch 150, training loss: 1.9055864810943604 = 1.2074307203292847 + 0.1 * 6.981557846069336
Epoch 150, val loss: 1.32244074344635
Epoch 160, training loss: 1.8123061656951904 = 1.1157499551773071 + 0.1 * 6.965561866760254
Epoch 160, val loss: 1.2576059103012085
Epoch 170, training loss: 1.7253968715667725 = 1.0303447246551514 + 0.1 * 6.950520992279053
Epoch 170, val loss: 1.1991881132125854
Epoch 180, training loss: 1.643911600112915 = 0.9503718614578247 + 0.1 * 6.935396671295166
Epoch 180, val loss: 1.1451510190963745
Epoch 190, training loss: 1.5676907300949097 = 0.8754501342773438 + 0.1 * 6.92240571975708
Epoch 190, val loss: 1.0947315692901611
Epoch 200, training loss: 1.494868516921997 = 0.8036466836929321 + 0.1 * 6.91221809387207
Epoch 200, val loss: 1.0464287996292114
Epoch 210, training loss: 1.4234338998794556 = 0.7332812547683716 + 0.1 * 6.90152645111084
Epoch 210, val loss: 1.000036358833313
Epoch 220, training loss: 1.3544013500213623 = 0.6651016473770142 + 0.1 * 6.892996311187744
Epoch 220, val loss: 0.9571770429611206
Epoch 230, training loss: 1.2889719009399414 = 0.600493311882019 + 0.1 * 6.8847856521606445
Epoch 230, val loss: 0.9200783967971802
Epoch 240, training loss: 1.2280091047286987 = 0.5400290489196777 + 0.1 * 6.879800319671631
Epoch 240, val loss: 0.8898316025733948
Epoch 250, training loss: 1.1717543601989746 = 0.4845782518386841 + 0.1 * 6.871761322021484
Epoch 250, val loss: 0.8667137026786804
Epoch 260, training loss: 1.1206789016723633 = 0.43412619829177856 + 0.1 * 6.865527629852295
Epoch 260, val loss: 0.8499317169189453
Epoch 270, training loss: 1.0739697217941284 = 0.38815662264823914 + 0.1 * 6.858131408691406
Epoch 270, val loss: 0.8384873270988464
Epoch 280, training loss: 1.031894326210022 = 0.346572607755661 + 0.1 * 6.853217124938965
Epoch 280, val loss: 0.8318966627120972
Epoch 290, training loss: 0.9934387803077698 = 0.3091898560523987 + 0.1 * 6.842489242553711
Epoch 290, val loss: 0.8294742107391357
Epoch 300, training loss: 0.9594468474388123 = 0.2757524251937866 + 0.1 * 6.836944103240967
Epoch 300, val loss: 0.830599844455719
Epoch 310, training loss: 0.9292457699775696 = 0.24621455371379852 + 0.1 * 6.8303117752075195
Epoch 310, val loss: 0.8345640301704407
Epoch 320, training loss: 0.9025592803955078 = 0.22017118334770203 + 0.1 * 6.823881149291992
Epoch 320, val loss: 0.8408800959587097
Epoch 330, training loss: 0.8789746761322021 = 0.19729895889759064 + 0.1 * 6.8167572021484375
Epoch 330, val loss: 0.848972737789154
Epoch 340, training loss: 0.8578584790229797 = 0.1771305352449417 + 0.1 * 6.807279586791992
Epoch 340, val loss: 0.8584885001182556
Epoch 350, training loss: 0.8393259644508362 = 0.15921609103679657 + 0.1 * 6.801098346710205
Epoch 350, val loss: 0.8691206574440002
Epoch 360, training loss: 0.8229032754898071 = 0.14325125515460968 + 0.1 * 6.796520233154297
Epoch 360, val loss: 0.8806383013725281
Epoch 370, training loss: 0.8079174757003784 = 0.12906232476234436 + 0.1 * 6.7885518074035645
Epoch 370, val loss: 0.8926758170127869
Epoch 380, training loss: 0.7945719957351685 = 0.11643962562084198 + 0.1 * 6.781323432922363
Epoch 380, val loss: 0.9051538109779358
Epoch 390, training loss: 0.7833457589149475 = 0.10519292205572128 + 0.1 * 6.781528472900391
Epoch 390, val loss: 0.9179208278656006
Epoch 400, training loss: 0.7724632620811462 = 0.0952172800898552 + 0.1 * 6.772459506988525
Epoch 400, val loss: 0.9308010339736938
Epoch 410, training loss: 0.7630553245544434 = 0.08634761720895767 + 0.1 * 6.76707649230957
Epoch 410, val loss: 0.9437791109085083
Epoch 420, training loss: 0.7545498609542847 = 0.07846039533615112 + 0.1 * 6.760894775390625
Epoch 420, val loss: 0.9567981958389282
Epoch 430, training loss: 0.7470754981040955 = 0.07145971804857254 + 0.1 * 6.756157875061035
Epoch 430, val loss: 0.9696519374847412
Epoch 440, training loss: 0.7399876117706299 = 0.06521636992692947 + 0.1 * 6.7477126121521
Epoch 440, val loss: 0.9825023412704468
Epoch 450, training loss: 0.736629843711853 = 0.05964310094714165 + 0.1 * 6.769867420196533
Epoch 450, val loss: 0.9952825307846069
Epoch 460, training loss: 0.7286867499351501 = 0.05470507964491844 + 0.1 * 6.739816665649414
Epoch 460, val loss: 1.0077005624771118
Epoch 470, training loss: 0.7236305475234985 = 0.05030151829123497 + 0.1 * 6.733290672302246
Epoch 470, val loss: 1.019931674003601
Epoch 480, training loss: 0.7182327508926392 = 0.04636392369866371 + 0.1 * 6.718688011169434
Epoch 480, val loss: 1.0319890975952148
Epoch 490, training loss: 0.7187058329582214 = 0.04282999411225319 + 0.1 * 6.758758544921875
Epoch 490, val loss: 1.0438976287841797
Epoch 500, training loss: 0.7113062143325806 = 0.03968585655093193 + 0.1 * 6.716203689575195
Epoch 500, val loss: 1.0553679466247559
Epoch 510, training loss: 0.7068291306495667 = 0.03686751425266266 + 0.1 * 6.699615955352783
Epoch 510, val loss: 1.0665761232376099
Epoch 520, training loss: 0.7036449909210205 = 0.03432639315724373 + 0.1 * 6.693186283111572
Epoch 520, val loss: 1.0775188207626343
Epoch 530, training loss: 0.7020940184593201 = 0.032027292996644974 + 0.1 * 6.700666904449463
Epoch 530, val loss: 1.0883371829986572
Epoch 540, training loss: 0.6990362405776978 = 0.029956264421343803 + 0.1 * 6.690799713134766
Epoch 540, val loss: 1.0988367795944214
Epoch 550, training loss: 0.6957577466964722 = 0.028075484558939934 + 0.1 * 6.676822662353516
Epoch 550, val loss: 1.1090738773345947
Epoch 560, training loss: 0.6958615779876709 = 0.02636699005961418 + 0.1 * 6.694945335388184
Epoch 560, val loss: 1.1191245317459106
Epoch 570, training loss: 0.6915209293365479 = 0.024815179407596588 + 0.1 * 6.667057037353516
Epoch 570, val loss: 1.128843069076538
Epoch 580, training loss: 0.6901847720146179 = 0.02339785546064377 + 0.1 * 6.6678690910339355
Epoch 580, val loss: 1.1384066343307495
Epoch 590, training loss: 0.6885362863540649 = 0.02210216037929058 + 0.1 * 6.664340972900391
Epoch 590, val loss: 1.1477628946304321
Epoch 600, training loss: 0.6863369941711426 = 0.020916495472192764 + 0.1 * 6.654204368591309
Epoch 600, val loss: 1.15683913230896
Epoch 610, training loss: 0.6865650415420532 = 0.019827058538794518 + 0.1 * 6.667379379272461
Epoch 610, val loss: 1.1657357215881348
Epoch 620, training loss: 0.6838954091072083 = 0.01882803998887539 + 0.1 * 6.6506733894348145
Epoch 620, val loss: 1.1743602752685547
Epoch 630, training loss: 0.6842095255851746 = 0.017905138432979584 + 0.1 * 6.663043975830078
Epoch 630, val loss: 1.182858943939209
Epoch 640, training loss: 0.6816199421882629 = 0.01705331541597843 + 0.1 * 6.645666122436523
Epoch 640, val loss: 1.191107988357544
Epoch 650, training loss: 0.6803253293037415 = 0.016264719888567924 + 0.1 * 6.640605926513672
Epoch 650, val loss: 1.1991392374038696
Epoch 660, training loss: 0.6790811419487 = 0.015533674508333206 + 0.1 * 6.635474681854248
Epoch 660, val loss: 1.2070614099502563
Epoch 670, training loss: 0.678384006023407 = 0.014856318943202496 + 0.1 * 6.6352763175964355
Epoch 670, val loss: 1.214681625366211
Epoch 680, training loss: 0.6776553988456726 = 0.014225958846509457 + 0.1 * 6.634294033050537
Epoch 680, val loss: 1.2221667766571045
Epoch 690, training loss: 0.6759322881698608 = 0.01363889779895544 + 0.1 * 6.622933387756348
Epoch 690, val loss: 1.2295321226119995
Epoch 700, training loss: 0.6760877370834351 = 0.013091080822050571 + 0.1 * 6.6299662590026855
Epoch 700, val loss: 1.2366498708724976
Epoch 710, training loss: 0.6737154722213745 = 0.012577821500599384 + 0.1 * 6.6113762855529785
Epoch 710, val loss: 1.2436492443084717
Epoch 720, training loss: 0.6750061511993408 = 0.012096425518393517 + 0.1 * 6.6290974617004395
Epoch 720, val loss: 1.250488042831421
Epoch 730, training loss: 0.6721662282943726 = 0.011647051200270653 + 0.1 * 6.605191707611084
Epoch 730, val loss: 1.2572203874588013
Epoch 740, training loss: 0.6717438697814941 = 0.011224672198295593 + 0.1 * 6.605191707611084
Epoch 740, val loss: 1.2637965679168701
Epoch 750, training loss: 0.6714366674423218 = 0.010827852413058281 + 0.1 * 6.606088161468506
Epoch 750, val loss: 1.2701585292816162
Epoch 760, training loss: 0.6694740653038025 = 0.010454036295413971 + 0.1 * 6.590199947357178
Epoch 760, val loss: 1.2763862609863281
Epoch 770, training loss: 0.6700860261917114 = 0.010100413113832474 + 0.1 * 6.599855899810791
Epoch 770, val loss: 1.2825928926467896
Epoch 780, training loss: 0.6678091287612915 = 0.009768439456820488 + 0.1 * 6.580406665802002
Epoch 780, val loss: 1.2885265350341797
Epoch 790, training loss: 0.6671114563941956 = 0.009453949518501759 + 0.1 * 6.576574802398682
Epoch 790, val loss: 1.2943705320358276
Epoch 800, training loss: 0.6706926226615906 = 0.009154943749308586 + 0.1 * 6.615376949310303
Epoch 800, val loss: 1.3001960515975952
Epoch 810, training loss: 0.6658810377120972 = 0.00887187011539936 + 0.1 * 6.570091724395752
Epoch 810, val loss: 1.3058761358261108
Epoch 820, training loss: 0.6664279699325562 = 0.008603575639426708 + 0.1 * 6.578243732452393
Epoch 820, val loss: 1.3114787340164185
Epoch 830, training loss: 0.6646955013275146 = 0.008348693139851093 + 0.1 * 6.563467979431152
Epoch 830, val loss: 1.3169344663619995
Epoch 840, training loss: 0.6648234128952026 = 0.008106445893645287 + 0.1 * 6.567169189453125
Epoch 840, val loss: 1.3222295045852661
Epoch 850, training loss: 0.6648825407028198 = 0.007876270450651646 + 0.1 * 6.570062637329102
Epoch 850, val loss: 1.3274955749511719
Epoch 860, training loss: 0.6654431819915771 = 0.007655777502804995 + 0.1 * 6.577873706817627
Epoch 860, val loss: 1.332634687423706
Epoch 870, training loss: 0.6632837653160095 = 0.007446747273206711 + 0.1 * 6.558370113372803
Epoch 870, val loss: 1.337721586227417
Epoch 880, training loss: 0.6625013947486877 = 0.007246751803904772 + 0.1 * 6.552546501159668
Epoch 880, val loss: 1.3426756858825684
Epoch 890, training loss: 0.6629583239555359 = 0.007055898662656546 + 0.1 * 6.559024333953857
Epoch 890, val loss: 1.3475874662399292
Epoch 900, training loss: 0.6618051528930664 = 0.006873604375869036 + 0.1 * 6.549315452575684
Epoch 900, val loss: 1.352373480796814
Epoch 910, training loss: 0.661773145198822 = 0.006699203513562679 + 0.1 * 6.550739288330078
Epoch 910, val loss: 1.3570737838745117
Epoch 920, training loss: 0.6607832908630371 = 0.006532530300319195 + 0.1 * 6.542507171630859
Epoch 920, val loss: 1.361667275428772
Epoch 930, training loss: 0.661455512046814 = 0.006372853647917509 + 0.1 * 6.550826549530029
Epoch 930, val loss: 1.3662359714508057
Epoch 940, training loss: 0.6596687436103821 = 0.006219649221748114 + 0.1 * 6.534491062164307
Epoch 940, val loss: 1.3706437349319458
Epoch 950, training loss: 0.6599815487861633 = 0.006073284894227982 + 0.1 * 6.5390825271606445
Epoch 950, val loss: 1.3749555349349976
Epoch 960, training loss: 0.6605931520462036 = 0.005932510830461979 + 0.1 * 6.546606063842773
Epoch 960, val loss: 1.3793100118637085
Epoch 970, training loss: 0.6592255234718323 = 0.005797000136226416 + 0.1 * 6.534285068511963
Epoch 970, val loss: 1.3834434747695923
Epoch 980, training loss: 0.6585741639137268 = 0.005666983313858509 + 0.1 * 6.529071807861328
Epoch 980, val loss: 1.387629747390747
Epoch 990, training loss: 0.6603014469146729 = 0.005542001221328974 + 0.1 * 6.5475945472717285
Epoch 990, val loss: 1.391741394996643
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 2.812138319015503 = 1.9524590969085693 + 0.1 * 8.59679126739502
Epoch 0, val loss: 1.9542430639266968
Epoch 10, training loss: 2.8018951416015625 = 1.9422253370285034 + 0.1 * 8.596696853637695
Epoch 10, val loss: 1.9436254501342773
Epoch 20, training loss: 2.789228677749634 = 1.9296144247055054 + 0.1 * 8.596142768859863
Epoch 20, val loss: 1.9304603338241577
Epoch 30, training loss: 2.7709603309631348 = 1.9118094444274902 + 0.1 * 8.591509819030762
Epoch 30, val loss: 1.911810278892517
Epoch 40, training loss: 2.7417588233947754 = 1.8854907751083374 + 0.1 * 8.5626802444458
Epoch 40, val loss: 1.88455069065094
Epoch 50, training loss: 2.6917266845703125 = 1.8494181632995605 + 0.1 * 8.423086166381836
Epoch 50, val loss: 1.8488011360168457
Epoch 60, training loss: 2.631418466567993 = 1.810215950012207 + 0.1 * 8.212024688720703
Epoch 60, val loss: 1.8128846883773804
Epoch 70, training loss: 2.5759220123291016 = 1.7763406038284302 + 0.1 * 7.995812892913818
Epoch 70, val loss: 1.7832012176513672
Epoch 80, training loss: 2.4924094676971436 = 1.741698980331421 + 0.1 * 7.50710391998291
Epoch 80, val loss: 1.7513688802719116
Epoch 90, training loss: 2.417032480239868 = 1.700112223625183 + 0.1 * 7.169203281402588
Epoch 90, val loss: 1.7137441635131836
Epoch 100, training loss: 2.3504176139831543 = 1.6448315382003784 + 0.1 * 7.0558600425720215
Epoch 100, val loss: 1.664315938949585
Epoch 110, training loss: 2.2725136280059814 = 1.5734304189682007 + 0.1 * 6.990832805633545
Epoch 110, val loss: 1.6019536256790161
Epoch 120, training loss: 2.18745756149292 = 1.4924112558364868 + 0.1 * 6.9504618644714355
Epoch 120, val loss: 1.5338797569274902
Epoch 130, training loss: 2.1016407012939453 = 1.409377932548523 + 0.1 * 6.922626495361328
Epoch 130, val loss: 1.4657325744628906
Epoch 140, training loss: 2.0168392658233643 = 1.3265427350997925 + 0.1 * 6.902966022491455
Epoch 140, val loss: 1.3999155759811401
Epoch 150, training loss: 1.93234121799469 = 1.2433772087097168 + 0.1 * 6.889639854431152
Epoch 150, val loss: 1.3350805044174194
Epoch 160, training loss: 1.8457589149475098 = 1.158254623413086 + 0.1 * 6.875042915344238
Epoch 160, val loss: 1.2698692083358765
Epoch 170, training loss: 1.75956392288208 = 1.0726611614227295 + 0.1 * 6.869028091430664
Epoch 170, val loss: 1.2053385972976685
Epoch 180, training loss: 1.6744062900543213 = 0.9890571236610413 + 0.1 * 6.853492259979248
Epoch 180, val loss: 1.1431286334991455
Epoch 190, training loss: 1.591477632522583 = 0.9074487090110779 + 0.1 * 6.840289115905762
Epoch 190, val loss: 1.083443284034729
Epoch 200, training loss: 1.5124011039733887 = 0.8291555047035217 + 0.1 * 6.832455158233643
Epoch 200, val loss: 1.0277786254882812
Epoch 210, training loss: 1.4374656677246094 = 0.7560752034187317 + 0.1 * 6.813904762268066
Epoch 210, val loss: 0.9781832098960876
Epoch 220, training loss: 1.3690037727355957 = 0.6883323192596436 + 0.1 * 6.806714057922363
Epoch 220, val loss: 0.9350118637084961
Epoch 230, training loss: 1.3061113357543945 = 0.6269729137420654 + 0.1 * 6.791383266448975
Epoch 230, val loss: 0.8990559577941895
Epoch 240, training loss: 1.2487657070159912 = 0.5713384747505188 + 0.1 * 6.774272441864014
Epoch 240, val loss: 0.8697776794433594
Epoch 250, training loss: 1.1972296237945557 = 0.520474910736084 + 0.1 * 6.767547130584717
Epoch 250, val loss: 0.8463723659515381
Epoch 260, training loss: 1.1492177248001099 = 0.4736751616001129 + 0.1 * 6.755424976348877
Epoch 260, val loss: 0.8276665806770325
Epoch 270, training loss: 1.1068036556243896 = 0.4299766421318054 + 0.1 * 6.7682695388793945
Epoch 270, val loss: 0.8126308917999268
Epoch 280, training loss: 1.0627036094665527 = 0.38926374912261963 + 0.1 * 6.734397888183594
Epoch 280, val loss: 0.8005505800247192
Epoch 290, training loss: 1.023887276649475 = 0.35105976462364197 + 0.1 * 6.728274822235107
Epoch 290, val loss: 0.7910147309303284
Epoch 300, training loss: 0.9899705052375793 = 0.3152579069137573 + 0.1 * 6.747125625610352
Epoch 300, val loss: 0.7837494611740112
Epoch 310, training loss: 0.9539587497711182 = 0.28222087025642395 + 0.1 * 6.717378616333008
Epoch 310, val loss: 0.7787501215934753
Epoch 320, training loss: 0.9225665330886841 = 0.2518039643764496 + 0.1 * 6.707625389099121
Epoch 320, val loss: 0.7762256860733032
Epoch 330, training loss: 0.8947288990020752 = 0.22420723736286163 + 0.1 * 6.705216407775879
Epoch 330, val loss: 0.7762146592140198
Epoch 340, training loss: 0.8688098192214966 = 0.1995556652545929 + 0.1 * 6.692541122436523
Epoch 340, val loss: 0.7783958315849304
Epoch 350, training loss: 0.84754478931427 = 0.17768263816833496 + 0.1 * 6.6986212730407715
Epoch 350, val loss: 0.7826933264732361
Epoch 360, training loss: 0.8279926776885986 = 0.15855509042739868 + 0.1 * 6.694375991821289
Epoch 360, val loss: 0.7886870503425598
Epoch 370, training loss: 0.8098671436309814 = 0.14186543226242065 + 0.1 * 6.680016994476318
Epoch 370, val loss: 0.7961202263832092
Epoch 380, training loss: 0.7959047555923462 = 0.12725037336349487 + 0.1 * 6.6865434646606445
Epoch 380, val loss: 0.80471271276474
Epoch 390, training loss: 0.781411349773407 = 0.1145024299621582 + 0.1 * 6.669088840484619
Epoch 390, val loss: 0.8141300678253174
Epoch 400, training loss: 0.7707439661026001 = 0.10334768146276474 + 0.1 * 6.6739630699157715
Epoch 400, val loss: 0.8241731524467468
Epoch 410, training loss: 0.7592807412147522 = 0.09358260780572891 + 0.1 * 6.656980991363525
Epoch 410, val loss: 0.8344395756721497
Epoch 420, training loss: 0.7496935725212097 = 0.08496614545583725 + 0.1 * 6.647274494171143
Epoch 420, val loss: 0.8450706005096436
Epoch 430, training loss: 0.7424188256263733 = 0.07736185938119888 + 0.1 * 6.650569438934326
Epoch 430, val loss: 0.8558509945869446
Epoch 440, training loss: 0.7345428466796875 = 0.07064255326986313 + 0.1 * 6.639003276824951
Epoch 440, val loss: 0.8665788173675537
Epoch 450, training loss: 0.7282242774963379 = 0.0646873340010643 + 0.1 * 6.635369777679443
Epoch 450, val loss: 0.8773757815361023
Epoch 460, training loss: 0.7220067381858826 = 0.05938918888568878 + 0.1 * 6.626175403594971
Epoch 460, val loss: 0.8880221843719482
Epoch 470, training loss: 0.7184861302375793 = 0.054647114127874374 + 0.1 * 6.63839054107666
Epoch 470, val loss: 0.8987243175506592
Epoch 480, training loss: 0.7110257148742676 = 0.05041378363966942 + 0.1 * 6.606119155883789
Epoch 480, val loss: 0.9093711972236633
Epoch 490, training loss: 0.708115816116333 = 0.04661654308438301 + 0.1 * 6.614992618560791
Epoch 490, val loss: 0.9199479818344116
Epoch 500, training loss: 0.7052561640739441 = 0.04321200028061867 + 0.1 * 6.620441436767578
Epoch 500, val loss: 0.9304136037826538
Epoch 510, training loss: 0.6995242238044739 = 0.040161553770303726 + 0.1 * 6.593626499176025
Epoch 510, val loss: 0.9407342672348022
Epoch 520, training loss: 0.69624263048172 = 0.03741158917546272 + 0.1 * 6.588310241699219
Epoch 520, val loss: 0.9509085416793823
Epoch 530, training loss: 0.6948362588882446 = 0.0349188894033432 + 0.1 * 6.599173545837402
Epoch 530, val loss: 0.9610378742218018
Epoch 540, training loss: 0.6916517019271851 = 0.03266304358839989 + 0.1 * 6.589886665344238
Epoch 540, val loss: 0.9709736108779907
Epoch 550, training loss: 0.6878384351730347 = 0.03061843104660511 + 0.1 * 6.572199821472168
Epoch 550, val loss: 0.9807634353637695
Epoch 560, training loss: 0.6854469776153564 = 0.028759051114320755 + 0.1 * 6.5668792724609375
Epoch 560, val loss: 0.9903115630149841
Epoch 570, training loss: 0.6847102642059326 = 0.027057595551013947 + 0.1 * 6.576526641845703
Epoch 570, val loss: 0.999735951423645
Epoch 580, training loss: 0.6831036806106567 = 0.0255031306296587 + 0.1 * 6.576004981994629
Epoch 580, val loss: 1.0090270042419434
Epoch 590, training loss: 0.6804195642471313 = 0.024084381759166718 + 0.1 * 6.563351631164551
Epoch 590, val loss: 1.018025517463684
Epoch 600, training loss: 0.6781603693962097 = 0.0227807704359293 + 0.1 * 6.55379581451416
Epoch 600, val loss: 1.026841402053833
Epoch 610, training loss: 0.6763278245925903 = 0.021580155938863754 + 0.1 * 6.547476291656494
Epoch 610, val loss: 1.0354795455932617
Epoch 620, training loss: 0.6751559376716614 = 0.02047235518693924 + 0.1 * 6.546835899353027
Epoch 620, val loss: 1.0440555810928345
Epoch 630, training loss: 0.6739102005958557 = 0.019451890140771866 + 0.1 * 6.544582843780518
Epoch 630, val loss: 1.0523827075958252
Epoch 640, training loss: 0.6731837391853333 = 0.01850792206823826 + 0.1 * 6.54675817489624
Epoch 640, val loss: 1.060584306716919
Epoch 650, training loss: 0.6730189919471741 = 0.017633303999900818 + 0.1 * 6.553856372833252
Epoch 650, val loss: 1.0685808658599854
Epoch 660, training loss: 0.6708393096923828 = 0.01682308316230774 + 0.1 * 6.540161609649658
Epoch 660, val loss: 1.07643461227417
Epoch 670, training loss: 0.66936856508255 = 0.016072051599621773 + 0.1 * 6.532965183258057
Epoch 670, val loss: 1.0840647220611572
Epoch 680, training loss: 0.6692712903022766 = 0.015369494445621967 + 0.1 * 6.539018154144287
Epoch 680, val loss: 1.0915508270263672
Epoch 690, training loss: 0.6666887402534485 = 0.014715095050632954 + 0.1 * 6.5197367668151855
Epoch 690, val loss: 1.0988980531692505
Epoch 700, training loss: 0.6672342419624329 = 0.014103453606367111 + 0.1 * 6.531307697296143
Epoch 700, val loss: 1.1061300039291382
Epoch 710, training loss: 0.6662185788154602 = 0.013531089760363102 + 0.1 * 6.526875019073486
Epoch 710, val loss: 1.113188624382019
Epoch 720, training loss: 0.6639755368232727 = 0.012996230274438858 + 0.1 * 6.509792804718018
Epoch 720, val loss: 1.1200894117355347
Epoch 730, training loss: 0.6629331707954407 = 0.012494654394686222 + 0.1 * 6.504384994506836
Epoch 730, val loss: 1.1268422603607178
Epoch 740, training loss: 0.6650961637496948 = 0.012022181414067745 + 0.1 * 6.530739784240723
Epoch 740, val loss: 1.1334643363952637
Epoch 750, training loss: 0.6630592942237854 = 0.011578448116779327 + 0.1 * 6.514808654785156
Epoch 750, val loss: 1.1399520635604858
Epoch 760, training loss: 0.6627858877182007 = 0.011161775328218937 + 0.1 * 6.51624059677124
Epoch 760, val loss: 1.1462234258651733
Epoch 770, training loss: 0.6608111262321472 = 0.0107683464884758 + 0.1 * 6.500427722930908
Epoch 770, val loss: 1.152428388595581
Epoch 780, training loss: 0.6626731157302856 = 0.01039664912968874 + 0.1 * 6.522764682769775
Epoch 780, val loss: 1.158478856086731
Epoch 790, training loss: 0.6603813767433167 = 0.010044853202998638 + 0.1 * 6.5033650398254395
Epoch 790, val loss: 1.1644538640975952
Epoch 800, training loss: 0.659335732460022 = 0.009714525192975998 + 0.1 * 6.496212005615234
Epoch 800, val loss: 1.1702502965927124
Epoch 810, training loss: 0.6585049033164978 = 0.009400478564202785 + 0.1 * 6.491044044494629
Epoch 810, val loss: 1.1759432554244995
Epoch 820, training loss: 0.6586549878120422 = 0.009102585725486279 + 0.1 * 6.495523929595947
Epoch 820, val loss: 1.1815153360366821
Epoch 830, training loss: 0.6572014093399048 = 0.00882017146795988 + 0.1 * 6.48381233215332
Epoch 830, val loss: 1.187011480331421
Epoch 840, training loss: 0.6570059061050415 = 0.008551478385925293 + 0.1 * 6.484544277191162
Epoch 840, val loss: 1.19243323802948
Epoch 850, training loss: 0.6567609310150146 = 0.008297308348119259 + 0.1 * 6.484635829925537
Epoch 850, val loss: 1.197685718536377
Epoch 860, training loss: 0.6568045616149902 = 0.008055087178945541 + 0.1 * 6.487494468688965
Epoch 860, val loss: 1.202854871749878
Epoch 870, training loss: 0.6564236283302307 = 0.00782514363527298 + 0.1 * 6.485984802246094
Epoch 870, val loss: 1.207897663116455
Epoch 880, training loss: 0.6567620635032654 = 0.0076049938797950745 + 0.1 * 6.491570472717285
Epoch 880, val loss: 1.212907075881958
Epoch 890, training loss: 0.6557486653327942 = 0.007395950611680746 + 0.1 * 6.483527183532715
Epoch 890, val loss: 1.2178086042404175
Epoch 900, training loss: 0.654535710811615 = 0.007195867598056793 + 0.1 * 6.473398208618164
Epoch 900, val loss: 1.2226320505142212
Epoch 910, training loss: 0.6539947986602783 = 0.007004554383456707 + 0.1 * 6.469902515411377
Epoch 910, val loss: 1.2274246215820312
Epoch 920, training loss: 0.6538512110710144 = 0.006822081748396158 + 0.1 * 6.4702911376953125
Epoch 920, val loss: 1.232101559638977
Epoch 930, training loss: 0.6543490886688232 = 0.006647230125963688 + 0.1 * 6.477018356323242
Epoch 930, val loss: 1.2366986274719238
Epoch 940, training loss: 0.6536043882369995 = 0.006479120347648859 + 0.1 * 6.47125244140625
Epoch 940, val loss: 1.2413166761398315
Epoch 950, training loss: 0.6546810269355774 = 0.006319377571344376 + 0.1 * 6.483616352081299
Epoch 950, val loss: 1.2457971572875977
Epoch 960, training loss: 0.6527410745620728 = 0.0061661554500460625 + 0.1 * 6.465749263763428
Epoch 960, val loss: 1.2501188516616821
Epoch 970, training loss: 0.6526516079902649 = 0.0060195401310920715 + 0.1 * 6.466320514678955
Epoch 970, val loss: 1.2544188499450684
Epoch 980, training loss: 0.6523433327674866 = 0.005878114607185125 + 0.1 * 6.4646525382995605
Epoch 980, val loss: 1.2586702108383179
Epoch 990, training loss: 0.6512005925178528 = 0.005742422305047512 + 0.1 * 6.454581260681152
Epoch 990, val loss: 1.262811303138733
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 2.8011765480041504 = 1.941495418548584 + 0.1 * 8.596810340881348
Epoch 0, val loss: 1.9354215860366821
Epoch 10, training loss: 2.790832281112671 = 1.9311593770980835 + 0.1 * 8.596728324890137
Epoch 10, val loss: 1.9253113269805908
Epoch 20, training loss: 2.778040885925293 = 1.9184184074401855 + 0.1 * 8.596223831176758
Epoch 20, val loss: 1.9124188423156738
Epoch 30, training loss: 2.759894371032715 = 1.900728464126587 + 0.1 * 8.591657638549805
Epoch 30, val loss: 1.8942277431488037
Epoch 40, training loss: 2.7310519218444824 = 1.875339150428772 + 0.1 * 8.557126998901367
Epoch 40, val loss: 1.8682875633239746
Epoch 50, training loss: 2.6798465251922607 = 1.8424054384231567 + 0.1 * 8.374411582946777
Epoch 50, val loss: 1.836642861366272
Epoch 60, training loss: 2.6276071071624756 = 1.808828592300415 + 0.1 * 8.187785148620605
Epoch 60, val loss: 1.8073705434799194
Epoch 70, training loss: 2.5518555641174316 = 1.7803899049758911 + 0.1 * 7.71465539932251
Epoch 70, val loss: 1.7837799787521362
Epoch 80, training loss: 2.4746012687683105 = 1.7501838207244873 + 0.1 * 7.244174957275391
Epoch 80, val loss: 1.7581982612609863
Epoch 90, training loss: 2.4174423217773438 = 1.7104334831237793 + 0.1 * 7.070088863372803
Epoch 90, val loss: 1.725462555885315
Epoch 100, training loss: 2.3534042835235596 = 1.6552547216415405 + 0.1 * 6.9814958572387695
Epoch 100, val loss: 1.680267572402954
Epoch 110, training loss: 2.2760095596313477 = 1.583398461341858 + 0.1 * 6.926112174987793
Epoch 110, val loss: 1.6202995777130127
Epoch 120, training loss: 2.1877048015594482 = 1.4983012676239014 + 0.1 * 6.8940348625183105
Epoch 120, val loss: 1.5508053302764893
Epoch 130, training loss: 2.0952394008636475 = 1.4072985649108887 + 0.1 * 6.879408836364746
Epoch 130, val loss: 1.478952169418335
Epoch 140, training loss: 2.0053603649139404 = 1.3183693885803223 + 0.1 * 6.869909286499023
Epoch 140, val loss: 1.412261962890625
Epoch 150, training loss: 1.9192132949829102 = 1.2331494092941284 + 0.1 * 6.860638618469238
Epoch 150, val loss: 1.3507757186889648
Epoch 160, training loss: 1.8379100561141968 = 1.1522718667984009 + 0.1 * 6.856381893157959
Epoch 160, val loss: 1.2949200868606567
Epoch 170, training loss: 1.7613637447357178 = 1.0768464803695679 + 0.1 * 6.845171928405762
Epoch 170, val loss: 1.2441763877868652
Epoch 180, training loss: 1.68943452835083 = 1.005854845046997 + 0.1 * 6.835797309875488
Epoch 180, val loss: 1.1971217393875122
Epoch 190, training loss: 1.6215665340423584 = 0.9383766055107117 + 0.1 * 6.831899166107178
Epoch 190, val loss: 1.1522552967071533
Epoch 200, training loss: 1.5589535236358643 = 0.8756685853004456 + 0.1 * 6.832849502563477
Epoch 200, val loss: 1.1104854345321655
Epoch 210, training loss: 1.4977900981903076 = 0.8157251477241516 + 0.1 * 6.820649147033691
Epoch 210, val loss: 1.0706478357315063
Epoch 220, training loss: 1.4380161762237549 = 0.7567228674888611 + 0.1 * 6.812933444976807
Epoch 220, val loss: 1.0319807529449463
Epoch 230, training loss: 1.3792004585266113 = 0.6986824870109558 + 0.1 * 6.805179119110107
Epoch 230, val loss: 0.9955613613128662
Epoch 240, training loss: 1.322064995765686 = 0.6424096822738647 + 0.1 * 6.796553134918213
Epoch 240, val loss: 0.9626877307891846
Epoch 250, training loss: 1.2691292762756348 = 0.5886880159378052 + 0.1 * 6.804413318634033
Epoch 250, val loss: 0.9346721172332764
Epoch 260, training loss: 1.2171956300735474 = 0.5385358929634094 + 0.1 * 6.78659725189209
Epoch 260, val loss: 0.9121936559677124
Epoch 270, training loss: 1.1697373390197754 = 0.4919643998146057 + 0.1 * 6.7777299880981445
Epoch 270, val loss: 0.8950201869010925
Epoch 280, training loss: 1.1250853538513184 = 0.44874683022499084 + 0.1 * 6.76338529586792
Epoch 280, val loss: 0.8825198411941528
Epoch 290, training loss: 1.084141731262207 = 0.4082707166671753 + 0.1 * 6.75870943069458
Epoch 290, val loss: 0.8736149072647095
Epoch 300, training loss: 1.0454387664794922 = 0.3702770173549652 + 0.1 * 6.751616954803467
Epoch 300, val loss: 0.8670154213905334
Epoch 310, training loss: 1.0081418752670288 = 0.3343765139579773 + 0.1 * 6.737653732299805
Epoch 310, val loss: 0.8618914484977722
Epoch 320, training loss: 0.9734412431716919 = 0.30012789368629456 + 0.1 * 6.733132839202881
Epoch 320, val loss: 0.8575667142868042
Epoch 330, training loss: 0.9402901530265808 = 0.26780468225479126 + 0.1 * 6.724854469299316
Epoch 330, val loss: 0.8540200591087341
Epoch 340, training loss: 0.9099287390708923 = 0.2376735806465149 + 0.1 * 6.722551345825195
Epoch 340, val loss: 0.8516117930412292
Epoch 350, training loss: 0.8808282017707825 = 0.2100190371274948 + 0.1 * 6.7080912590026855
Epoch 350, val loss: 0.8504447340965271
Epoch 360, training loss: 0.8547270894050598 = 0.18495188653469086 + 0.1 * 6.697751522064209
Epoch 360, val loss: 0.8507189750671387
Epoch 370, training loss: 0.8334305286407471 = 0.1626186966896057 + 0.1 * 6.708117961883545
Epoch 370, val loss: 0.852458655834198
Epoch 380, training loss: 0.8113573789596558 = 0.14310617744922638 + 0.1 * 6.682512283325195
Epoch 380, val loss: 0.8554920554161072
Epoch 390, training loss: 0.7934285998344421 = 0.1260986477136612 + 0.1 * 6.673299312591553
Epoch 390, val loss: 0.8598353266716003
Epoch 400, training loss: 0.7794299125671387 = 0.11138466745615005 + 0.1 * 6.6804518699646
Epoch 400, val loss: 0.865307092666626
Epoch 410, training loss: 0.7646024227142334 = 0.0987330824136734 + 0.1 * 6.658693313598633
Epoch 410, val loss: 0.8716169595718384
Epoch 420, training loss: 0.7530386447906494 = 0.08779887855052948 + 0.1 * 6.652397155761719
Epoch 420, val loss: 0.87887042760849
Epoch 430, training loss: 0.7422505021095276 = 0.07835592329502106 + 0.1 * 6.638945579528809
Epoch 430, val loss: 0.8868361711502075
Epoch 440, training loss: 0.7347398400306702 = 0.07019107788801193 + 0.1 * 6.645487308502197
Epoch 440, val loss: 0.8952757716178894
Epoch 450, training loss: 0.7266190052032471 = 0.06314139813184738 + 0.1 * 6.634775638580322
Epoch 450, val loss: 0.9040882587432861
Epoch 460, training loss: 0.7193131446838379 = 0.05702487379312515 + 0.1 * 6.62288236618042
Epoch 460, val loss: 0.9132742881774902
Epoch 470, training loss: 0.713554859161377 = 0.05170803889632225 + 0.1 * 6.618468284606934
Epoch 470, val loss: 0.9226240515708923
Epoch 480, training loss: 0.7105901837348938 = 0.04707365855574608 + 0.1 * 6.635165214538574
Epoch 480, val loss: 0.9321205615997314
Epoch 490, training loss: 0.7033373117446899 = 0.04303188621997833 + 0.1 * 6.603054046630859
Epoch 490, val loss: 0.941572904586792
Epoch 500, training loss: 0.6989899277687073 = 0.039476457983255386 + 0.1 * 6.595134735107422
Epoch 500, val loss: 0.9511371850967407
Epoch 510, training loss: 0.6968193054199219 = 0.03633233159780502 + 0.1 * 6.604869842529297
Epoch 510, val loss: 0.9606941342353821
Epoch 520, training loss: 0.6925454139709473 = 0.033553604036569595 + 0.1 * 6.58991813659668
Epoch 520, val loss: 0.9701128005981445
Epoch 530, training loss: 0.6890687942504883 = 0.031086256727576256 + 0.1 * 6.579825401306152
Epoch 530, val loss: 0.9794334173202515
Epoch 540, training loss: 0.6881008744239807 = 0.02888120338320732 + 0.1 * 6.592196464538574
Epoch 540, val loss: 0.9886866807937622
Epoch 550, training loss: 0.6848068237304688 = 0.026913508772850037 + 0.1 * 6.578932762145996
Epoch 550, val loss: 0.9977229237556458
Epoch 560, training loss: 0.681681752204895 = 0.025149468332529068 + 0.1 * 6.5653228759765625
Epoch 560, val loss: 1.0065715312957764
Epoch 570, training loss: 0.6820195913314819 = 0.023556431755423546 + 0.1 * 6.58463191986084
Epoch 570, val loss: 1.015334963798523
Epoch 580, training loss: 0.6792910695075989 = 0.022120574489235878 + 0.1 * 6.571704864501953
Epoch 580, val loss: 1.0238094329833984
Epoch 590, training loss: 0.6759886741638184 = 0.020820440724492073 + 0.1 * 6.551682472229004
Epoch 590, val loss: 1.0321141481399536
Epoch 600, training loss: 0.6744071245193481 = 0.01963467337191105 + 0.1 * 6.547724723815918
Epoch 600, val loss: 1.0402861833572388
Epoch 610, training loss: 0.6749251484870911 = 0.018551692366600037 + 0.1 * 6.563734531402588
Epoch 610, val loss: 1.0482763051986694
Epoch 620, training loss: 0.6729683876037598 = 0.0175681971013546 + 0.1 * 6.554002285003662
Epoch 620, val loss: 1.0559457540512085
Epoch 630, training loss: 0.6709093451499939 = 0.016667628660798073 + 0.1 * 6.542417526245117
Epoch 630, val loss: 1.0634944438934326
Epoch 640, training loss: 0.669804573059082 = 0.015836644917726517 + 0.1 * 6.539679050445557
Epoch 640, val loss: 1.0709311962127686
Epoch 650, training loss: 0.6692789793014526 = 0.015069954097270966 + 0.1 * 6.542089939117432
Epoch 650, val loss: 1.078184962272644
Epoch 660, training loss: 0.6694273948669434 = 0.014363170601427555 + 0.1 * 6.550642013549805
Epoch 660, val loss: 1.0852559804916382
Epoch 670, training loss: 0.6669831275939941 = 0.013711273670196533 + 0.1 * 6.532718181610107
Epoch 670, val loss: 1.092138409614563
Epoch 680, training loss: 0.665723979473114 = 0.013105947524309158 + 0.1 * 6.526180267333984
Epoch 680, val loss: 1.0989196300506592
Epoch 690, training loss: 0.6665781140327454 = 0.012542369775474072 + 0.1 * 6.54035758972168
Epoch 690, val loss: 1.1055757999420166
Epoch 700, training loss: 0.6649172306060791 = 0.012018169276416302 + 0.1 * 6.528990745544434
Epoch 700, val loss: 1.1119917631149292
Epoch 710, training loss: 0.6635233163833618 = 0.011530917137861252 + 0.1 * 6.519923686981201
Epoch 710, val loss: 1.1182612180709839
Epoch 720, training loss: 0.662911593914032 = 0.011074764654040337 + 0.1 * 6.518368244171143
Epoch 720, val loss: 1.1243895292282104
Epoch 730, training loss: 0.6617249846458435 = 0.010648181661963463 + 0.1 * 6.510767936706543
Epoch 730, val loss: 1.130355715751648
Epoch 740, training loss: 0.6620602011680603 = 0.010248578153550625 + 0.1 * 6.518115997314453
Epoch 740, val loss: 1.136230230331421
Epoch 750, training loss: 0.6614972352981567 = 0.009872806258499622 + 0.1 * 6.516244411468506
Epoch 750, val loss: 1.1419272422790527
Epoch 760, training loss: 0.6606647372245789 = 0.009521003812551498 + 0.1 * 6.51143741607666
Epoch 760, val loss: 1.1474297046661377
Epoch 770, training loss: 0.6591298580169678 = 0.009190022014081478 + 0.1 * 6.499398231506348
Epoch 770, val loss: 1.152869462966919
Epoch 780, training loss: 0.6588726043701172 = 0.008878235705196857 + 0.1 * 6.499943733215332
Epoch 780, val loss: 1.1581854820251465
Epoch 790, training loss: 0.6584521532058716 = 0.008583358488976955 + 0.1 * 6.498687744140625
Epoch 790, val loss: 1.163398265838623
Epoch 800, training loss: 0.6576187014579773 = 0.008304751478135586 + 0.1 * 6.493139266967773
Epoch 800, val loss: 1.168506145477295
Epoch 810, training loss: 0.6570683717727661 = 0.008041981607675552 + 0.1 * 6.490263938903809
Epoch 810, val loss: 1.1734521389007568
Epoch 820, training loss: 0.6582003831863403 = 0.007792412769049406 + 0.1 * 6.504079341888428
Epoch 820, val loss: 1.1783809661865234
Epoch 830, training loss: 0.656665027141571 = 0.007555995136499405 + 0.1 * 6.491089820861816
Epoch 830, val loss: 1.1831895112991333
Epoch 840, training loss: 0.6558713912963867 = 0.007331551052629948 + 0.1 * 6.485398292541504
Epoch 840, val loss: 1.187851071357727
Epoch 850, training loss: 0.6552674770355225 = 0.007118923123925924 + 0.1 * 6.481485366821289
Epoch 850, val loss: 1.1923993825912476
Epoch 860, training loss: 0.6557547450065613 = 0.006916568148881197 + 0.1 * 6.488381385803223
Epoch 860, val loss: 1.1969269514083862
Epoch 870, training loss: 0.6554168462753296 = 0.006723332218825817 + 0.1 * 6.486935138702393
Epoch 870, val loss: 1.2013787031173706
Epoch 880, training loss: 0.6542657613754272 = 0.006539538968354464 + 0.1 * 6.477262020111084
Epoch 880, val loss: 1.205680251121521
Epoch 890, training loss: 0.653926432132721 = 0.006364692002534866 + 0.1 * 6.475617408752441
Epoch 890, val loss: 1.2099368572235107
Epoch 900, training loss: 0.6543834209442139 = 0.006197312381118536 + 0.1 * 6.481860637664795
Epoch 900, val loss: 1.2141690254211426
Epoch 910, training loss: 0.6542679071426392 = 0.006037237122654915 + 0.1 * 6.482306480407715
Epoch 910, val loss: 1.218312382698059
Epoch 920, training loss: 0.6523070931434631 = 0.005884605459868908 + 0.1 * 6.464224815368652
Epoch 920, val loss: 1.2222932577133179
Epoch 930, training loss: 0.6530436277389526 = 0.0057386429980397224 + 0.1 * 6.473050117492676
Epoch 930, val loss: 1.226292371749878
Epoch 940, training loss: 0.6517966985702515 = 0.00559837045148015 + 0.1 * 6.4619832038879395
Epoch 940, val loss: 1.2301843166351318
Epoch 950, training loss: 0.652863621711731 = 0.005464226473122835 + 0.1 * 6.473993301391602
Epoch 950, val loss: 1.2340145111083984
Epoch 960, training loss: 0.6517814993858337 = 0.005335826892405748 + 0.1 * 6.464457035064697
Epoch 960, val loss: 1.2377641201019287
Epoch 970, training loss: 0.6521461606025696 = 0.0052127111703157425 + 0.1 * 6.469334125518799
Epoch 970, val loss: 1.2414155006408691
Epoch 980, training loss: 0.6506732702255249 = 0.005094372667372227 + 0.1 * 6.455789089202881
Epoch 980, val loss: 1.2450437545776367
Epoch 990, training loss: 0.6509494185447693 = 0.004980576224625111 + 0.1 * 6.459688186645508
Epoch 990, val loss: 1.2485960721969604
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8075909330521878
The final CL Acc:0.78889, 0.00524, The final GNN Acc:0.80952, 0.00138
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13210])
remove edge: torch.Size([2, 7820])
updated graph: torch.Size([2, 10474])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.815112352371216 = 1.955432415008545 + 0.1 * 8.59679889678955
Epoch 0, val loss: 1.9463815689086914
Epoch 10, training loss: 2.8044509887695312 = 1.9447832107543945 + 0.1 * 8.596677780151367
Epoch 10, val loss: 1.9363292455673218
Epoch 20, training loss: 2.791449546813965 = 1.931862473487854 + 0.1 * 8.595871925354004
Epoch 20, val loss: 1.9236947298049927
Epoch 30, training loss: 2.773078680038452 = 1.9141486883163452 + 0.1 * 8.589300155639648
Epoch 30, val loss: 1.9060277938842773
Epoch 40, training loss: 2.7432947158813477 = 1.8882850408554077 + 0.1 * 8.550097465515137
Epoch 40, val loss: 1.8803311586380005
Epoch 50, training loss: 2.6888954639434814 = 1.8526538610458374 + 0.1 * 8.362415313720703
Epoch 50, val loss: 1.8467661142349243
Epoch 60, training loss: 2.6230297088623047 = 1.8113590478897095 + 0.1 * 8.116705894470215
Epoch 60, val loss: 1.810741662979126
Epoch 70, training loss: 2.548112154006958 = 1.7730803489685059 + 0.1 * 7.750317096710205
Epoch 70, val loss: 1.7797083854675293
Epoch 80, training loss: 2.4679627418518066 = 1.7376973628997803 + 0.1 * 7.302654266357422
Epoch 80, val loss: 1.749931812286377
Epoch 90, training loss: 2.407410144805908 = 1.694771409034729 + 0.1 * 7.1263861656188965
Epoch 90, val loss: 1.7104543447494507
Epoch 100, training loss: 2.344388961791992 = 1.6385525465011597 + 0.1 * 7.058364391326904
Epoch 100, val loss: 1.6598902940750122
Epoch 110, training loss: 2.2682321071624756 = 1.5674967765808105 + 0.1 * 7.007352828979492
Epoch 110, val loss: 1.5992507934570312
Epoch 120, training loss: 2.1808958053588867 = 1.484013319015503 + 0.1 * 6.968824863433838
Epoch 120, val loss: 1.5295987129211426
Epoch 130, training loss: 2.0870347023010254 = 1.392949104309082 + 0.1 * 6.940855979919434
Epoch 130, val loss: 1.4552770853042603
Epoch 140, training loss: 1.9900879859924316 = 1.298279881477356 + 0.1 * 6.918081283569336
Epoch 140, val loss: 1.380142331123352
Epoch 150, training loss: 1.8915469646453857 = 1.2018743753433228 + 0.1 * 6.896726608276367
Epoch 150, val loss: 1.3043307065963745
Epoch 160, training loss: 1.7933921813964844 = 1.1053279638290405 + 0.1 * 6.880642890930176
Epoch 160, val loss: 1.2282812595367432
Epoch 170, training loss: 1.6993257999420166 = 1.0127930641174316 + 0.1 * 6.865327835083008
Epoch 170, val loss: 1.1553281545639038
Epoch 180, training loss: 1.6109647750854492 = 0.9254114031791687 + 0.1 * 6.855534076690674
Epoch 180, val loss: 1.0865517854690552
Epoch 190, training loss: 1.5291458368301392 = 0.8445008993148804 + 0.1 * 6.846449375152588
Epoch 190, val loss: 1.0241442918777466
Epoch 200, training loss: 1.4544233083724976 = 0.7705959677696228 + 0.1 * 6.838273525238037
Epoch 200, val loss: 0.9682751893997192
Epoch 210, training loss: 1.386989951133728 = 0.7038275003433228 + 0.1 * 6.831624507904053
Epoch 210, val loss: 0.9195801615715027
Epoch 220, training loss: 1.326572299003601 = 0.6438701152801514 + 0.1 * 6.827021598815918
Epoch 220, val loss: 0.8782697319984436
Epoch 230, training loss: 1.2716786861419678 = 0.5896264910697937 + 0.1 * 6.820522308349609
Epoch 230, val loss: 0.8438877463340759
Epoch 240, training loss: 1.2202521562576294 = 0.5389307737350464 + 0.1 * 6.81321382522583
Epoch 240, val loss: 0.8144086003303528
Epoch 250, training loss: 1.1721415519714355 = 0.4908595085144043 + 0.1 * 6.812819957733154
Epoch 250, val loss: 0.7889672517776489
Epoch 260, training loss: 1.1249465942382812 = 0.44494882225990295 + 0.1 * 6.799977779388428
Epoch 260, val loss: 0.7670818567276001
Epoch 270, training loss: 1.081165075302124 = 0.4010944366455078 + 0.1 * 6.800705909729004
Epoch 270, val loss: 0.7487186789512634
Epoch 280, training loss: 1.039191484451294 = 0.3601353168487549 + 0.1 * 6.790561199188232
Epoch 280, val loss: 0.7344251871109009
Epoch 290, training loss: 1.000424861907959 = 0.32238757610321045 + 0.1 * 6.780372619628906
Epoch 290, val loss: 0.7243770956993103
Epoch 300, training loss: 0.9672821760177612 = 0.2883320748806 + 0.1 * 6.789500713348389
Epoch 300, val loss: 0.718380868434906
Epoch 310, training loss: 0.9350100159645081 = 0.2581592798233032 + 0.1 * 6.76850700378418
Epoch 310, val loss: 0.7161691188812256
Epoch 320, training loss: 0.9076182246208191 = 0.2313561588525772 + 0.1 * 6.76262092590332
Epoch 320, val loss: 0.7169915437698364
Epoch 330, training loss: 0.8841160535812378 = 0.20761056244373322 + 0.1 * 6.765055179595947
Epoch 330, val loss: 0.7200921773910522
Epoch 340, training loss: 0.8619095087051392 = 0.18672503530979156 + 0.1 * 6.751844882965088
Epoch 340, val loss: 0.7250897288322449
Epoch 350, training loss: 0.8429471254348755 = 0.16821424663066864 + 0.1 * 6.747328281402588
Epoch 350, val loss: 0.731492280960083
Epoch 360, training loss: 0.8266071677207947 = 0.15180040895938873 + 0.1 * 6.748067855834961
Epoch 360, val loss: 0.7390155792236328
Epoch 370, training loss: 0.8111167550086975 = 0.13726364076137543 + 0.1 * 6.73853063583374
Epoch 370, val loss: 0.7474476099014282
Epoch 380, training loss: 0.7976653575897217 = 0.12434488534927368 + 0.1 * 6.733204364776611
Epoch 380, val loss: 0.7564820647239685
Epoch 390, training loss: 0.7855046987533569 = 0.11284750699996948 + 0.1 * 6.726571559906006
Epoch 390, val loss: 0.7660877704620361
Epoch 400, training loss: 0.7754605412483215 = 0.10260466486215591 + 0.1 * 6.728558540344238
Epoch 400, val loss: 0.7759659886360168
Epoch 410, training loss: 0.7649913430213928 = 0.09348752349615097 + 0.1 * 6.715038299560547
Epoch 410, val loss: 0.786080002784729
Epoch 420, training loss: 0.7561621069908142 = 0.08531083911657333 + 0.1 * 6.708512783050537
Epoch 420, val loss: 0.7963564395904541
Epoch 430, training loss: 0.7479711174964905 = 0.07795701175928116 + 0.1 * 6.700140476226807
Epoch 430, val loss: 0.8067920207977295
Epoch 440, training loss: 0.7413961291313171 = 0.07135643810033798 + 0.1 * 6.70039701461792
Epoch 440, val loss: 0.8171097636222839
Epoch 450, training loss: 0.7351158261299133 = 0.06547930091619492 + 0.1 * 6.6963653564453125
Epoch 450, val loss: 0.8276812434196472
Epoch 460, training loss: 0.7286932468414307 = 0.060199886560440063 + 0.1 * 6.684933662414551
Epoch 460, val loss: 0.8379870653152466
Epoch 470, training loss: 0.7231078147888184 = 0.05544225126504898 + 0.1 * 6.676655292510986
Epoch 470, val loss: 0.848298966884613
Epoch 480, training loss: 0.7196170687675476 = 0.05116702616214752 + 0.1 * 6.684500217437744
Epoch 480, val loss: 0.8585864305496216
Epoch 490, training loss: 0.7141677141189575 = 0.04733406752347946 + 0.1 * 6.668336391448975
Epoch 490, val loss: 0.868706464767456
Epoch 500, training loss: 0.7098290324211121 = 0.043868791311979294 + 0.1 * 6.659602165222168
Epoch 500, val loss: 0.8787420392036438
Epoch 510, training loss: 0.7071419358253479 = 0.040726322680711746 + 0.1 * 6.664155960083008
Epoch 510, val loss: 0.8886071443557739
Epoch 520, training loss: 0.7024458646774292 = 0.037881460040807724 + 0.1 * 6.645644187927246
Epoch 520, val loss: 0.8983222842216492
Epoch 530, training loss: 0.7001891732215881 = 0.035308048129081726 + 0.1 * 6.648810863494873
Epoch 530, val loss: 0.907751202583313
Epoch 540, training loss: 0.6963741183280945 = 0.03297647461295128 + 0.1 * 6.633976459503174
Epoch 540, val loss: 0.9170836806297302
Epoch 550, training loss: 0.6937811374664307 = 0.030852055177092552 + 0.1 * 6.629290580749512
Epoch 550, val loss: 0.9261910915374756
Epoch 560, training loss: 0.6920488476753235 = 0.02891123853623867 + 0.1 * 6.631376266479492
Epoch 560, val loss: 0.9352303743362427
Epoch 570, training loss: 0.6903584599494934 = 0.027143405750393867 + 0.1 * 6.632150173187256
Epoch 570, val loss: 0.9438848495483398
Epoch 580, training loss: 0.687242329120636 = 0.025530444458127022 + 0.1 * 6.617118835449219
Epoch 580, val loss: 0.9525014162063599
Epoch 590, training loss: 0.685065507888794 = 0.02405121922492981 + 0.1 * 6.610142230987549
Epoch 590, val loss: 0.9609120488166809
Epoch 600, training loss: 0.686464786529541 = 0.022691572085022926 + 0.1 * 6.63773250579834
Epoch 600, val loss: 0.9690913558006287
Epoch 610, training loss: 0.6821059584617615 = 0.02144646644592285 + 0.1 * 6.606595039367676
Epoch 610, val loss: 0.9771493673324585
Epoch 620, training loss: 0.6803129315376282 = 0.020301207900047302 + 0.1 * 6.600117206573486
Epoch 620, val loss: 0.9849441051483154
Epoch 630, training loss: 0.6785963177680969 = 0.01924498938024044 + 0.1 * 6.593513488769531
Epoch 630, val loss: 0.9926998615264893
Epoch 640, training loss: 0.6774816513061523 = 0.018269319087266922 + 0.1 * 6.592123508453369
Epoch 640, val loss: 1.0002752542495728
Epoch 650, training loss: 0.6772081851959229 = 0.01736537367105484 + 0.1 * 6.598427772521973
Epoch 650, val loss: 1.0076358318328857
Epoch 660, training loss: 0.6760203838348389 = 0.01652940921485424 + 0.1 * 6.594910144805908
Epoch 660, val loss: 1.0147699117660522
Epoch 670, training loss: 0.6738147735595703 = 0.01575632393360138 + 0.1 * 6.580584526062012
Epoch 670, val loss: 1.0219683647155762
Epoch 680, training loss: 0.6736770868301392 = 0.015036760829389095 + 0.1 * 6.5864033699035645
Epoch 680, val loss: 1.0287970304489136
Epoch 690, training loss: 0.673598051071167 = 0.014368673786520958 + 0.1 * 6.592293739318848
Epoch 690, val loss: 1.0353871583938599
Epoch 700, training loss: 0.6709439754486084 = 0.013748896308243275 + 0.1 * 6.571950912475586
Epoch 700, val loss: 1.04209303855896
Epoch 710, training loss: 0.6700608730316162 = 0.013169473968446255 + 0.1 * 6.56891393661499
Epoch 710, val loss: 1.0484387874603271
Epoch 720, training loss: 0.66957688331604 = 0.012626943178474903 + 0.1 * 6.5694990158081055
Epoch 720, val loss: 1.0545909404754639
Epoch 730, training loss: 0.6689029932022095 = 0.012120452709496021 + 0.1 * 6.5678253173828125
Epoch 730, val loss: 1.0608757734298706
Epoch 740, training loss: 0.6674746870994568 = 0.011645379476249218 + 0.1 * 6.558292865753174
Epoch 740, val loss: 1.0668072700500488
Epoch 750, training loss: 0.6683934926986694 = 0.011198543012142181 + 0.1 * 6.571949481964111
Epoch 750, val loss: 1.0725445747375488
Epoch 760, training loss: 0.6665523052215576 = 0.010780048556625843 + 0.1 * 6.557722568511963
Epoch 760, val loss: 1.0783591270446777
Epoch 770, training loss: 0.6664329171180725 = 0.01038624532520771 + 0.1 * 6.560466766357422
Epoch 770, val loss: 1.0839760303497314
Epoch 780, training loss: 0.6648070216178894 = 0.01001538336277008 + 0.1 * 6.547916412353516
Epoch 780, val loss: 1.0892629623413086
Epoch 790, training loss: 0.664237380027771 = 0.009666210040450096 + 0.1 * 6.545711517333984
Epoch 790, val loss: 1.0948104858398438
Epoch 800, training loss: 0.6660438776016235 = 0.009336477145552635 + 0.1 * 6.567073822021484
Epoch 800, val loss: 1.0998936891555786
Epoch 810, training loss: 0.6629910469055176 = 0.009025388397276402 + 0.1 * 6.539656162261963
Epoch 810, val loss: 1.1050381660461426
Epoch 820, training loss: 0.6626195311546326 = 0.008731316775083542 + 0.1 * 6.538882255554199
Epoch 820, val loss: 1.1101939678192139
Epoch 830, training loss: 0.6626251935958862 = 0.00845210812985897 + 0.1 * 6.541730880737305
Epoch 830, val loss: 1.1148645877838135
Epoch 840, training loss: 0.6613578200340271 = 0.008188625797629356 + 0.1 * 6.531692028045654
Epoch 840, val loss: 1.1198678016662598
Epoch 850, training loss: 0.6621543765068054 = 0.007938357070088387 + 0.1 * 6.542159557342529
Epoch 850, val loss: 1.124544620513916
Epoch 860, training loss: 0.6602993607521057 = 0.007701181806623936 + 0.1 * 6.525981903076172
Epoch 860, val loss: 1.1291778087615967
Epoch 870, training loss: 0.659247875213623 = 0.007475647609680891 + 0.1 * 6.517722129821777
Epoch 870, val loss: 1.133849859237671
Epoch 880, training loss: 0.6586162447929382 = 0.007260403595864773 + 0.1 * 6.513558387756348
Epoch 880, val loss: 1.1381406784057617
Epoch 890, training loss: 0.6592029333114624 = 0.007056122645735741 + 0.1 * 6.521468162536621
Epoch 890, val loss: 1.1427054405212402
Epoch 900, training loss: 0.6597763299942017 = 0.006861243396997452 + 0.1 * 6.529150485992432
Epoch 900, val loss: 1.1470030546188354
Epoch 910, training loss: 0.6578200459480286 = 0.006675574462860823 + 0.1 * 6.511444568634033
Epoch 910, val loss: 1.1512298583984375
Epoch 920, training loss: 0.6582838296890259 = 0.006498171482235193 + 0.1 * 6.517856597900391
Epoch 920, val loss: 1.1554718017578125
Epoch 930, training loss: 0.6567731499671936 = 0.006328759714961052 + 0.1 * 6.504443645477295
Epoch 930, val loss: 1.159524917602539
Epoch 940, training loss: 0.657016396522522 = 0.006166647654026747 + 0.1 * 6.508497714996338
Epoch 940, val loss: 1.1634514331817627
Epoch 950, training loss: 0.6557119488716125 = 0.006012205965816975 + 0.1 * 6.496997356414795
Epoch 950, val loss: 1.167445182800293
Epoch 960, training loss: 0.656062662601471 = 0.005864179693162441 + 0.1 * 6.501984596252441
Epoch 960, val loss: 1.1714497804641724
Epoch 970, training loss: 0.6552172899246216 = 0.0057220906019210815 + 0.1 * 6.4949517250061035
Epoch 970, val loss: 1.175047516822815
Epoch 980, training loss: 0.6551361083984375 = 0.00558603648096323 + 0.1 * 6.495500564575195
Epoch 980, val loss: 1.1789073944091797
Epoch 990, training loss: 0.6552067399024963 = 0.005455637350678444 + 0.1 * 6.497511386871338
Epoch 990, val loss: 1.1826764345169067
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 2.8164119720458984 = 1.956729531288147 + 0.1 * 8.596823692321777
Epoch 0, val loss: 1.9475780725479126
Epoch 10, training loss: 2.805725336074829 = 1.946051001548767 + 0.1 * 8.596742630004883
Epoch 10, val loss: 1.937486171722412
Epoch 20, training loss: 2.7928802967071533 = 1.9332517385482788 + 0.1 * 8.596284866333008
Epoch 20, val loss: 1.9249484539031982
Epoch 30, training loss: 2.7746944427490234 = 1.915432095527649 + 0.1 * 8.59262466430664
Epoch 30, val loss: 1.9071422815322876
Epoch 40, training loss: 2.745373249053955 = 1.888993740081787 + 0.1 * 8.563794136047363
Epoch 40, val loss: 1.880763292312622
Epoch 50, training loss: 2.6926889419555664 = 1.8516006469726562 + 0.1 * 8.410881996154785
Epoch 50, val loss: 1.8449784517288208
Epoch 60, training loss: 2.616960287094116 = 1.8085507154464722 + 0.1 * 8.084095001220703
Epoch 60, val loss: 1.8073786497116089
Epoch 70, training loss: 2.5376062393188477 = 1.7681268453598022 + 0.1 * 7.694793224334717
Epoch 70, val loss: 1.7747539281845093
Epoch 80, training loss: 2.4582791328430176 = 1.727414608001709 + 0.1 * 7.3086442947387695
Epoch 80, val loss: 1.7401772737503052
Epoch 90, training loss: 2.3924145698547363 = 1.677093744277954 + 0.1 * 7.153208255767822
Epoch 90, val loss: 1.6955622434616089
Epoch 100, training loss: 2.3171024322509766 = 1.610834002494812 + 0.1 * 7.06268310546875
Epoch 100, val loss: 1.6365705728530884
Epoch 110, training loss: 2.226898431777954 = 1.5265687704086304 + 0.1 * 7.0032958984375
Epoch 110, val loss: 1.562734603881836
Epoch 120, training loss: 2.126023769378662 = 1.4295203685760498 + 0.1 * 6.965032577514648
Epoch 120, val loss: 1.48160982131958
Epoch 130, training loss: 2.0229310989379883 = 1.3286540508270264 + 0.1 * 6.9427690505981445
Epoch 130, val loss: 1.4002927541732788
Epoch 140, training loss: 1.923337459564209 = 1.2307913303375244 + 0.1 * 6.9254608154296875
Epoch 140, val loss: 1.3213940858840942
Epoch 150, training loss: 1.8311161994934082 = 1.1404203176498413 + 0.1 * 6.906959533691406
Epoch 150, val loss: 1.2496857643127441
Epoch 160, training loss: 1.749997854232788 = 1.0609582662582397 + 0.1 * 6.8903961181640625
Epoch 160, val loss: 1.1876646280288696
Epoch 170, training loss: 1.6812593936920166 = 0.9935452938079834 + 0.1 * 6.877140522003174
Epoch 170, val loss: 1.1374975442886353
Epoch 180, training loss: 1.619931697845459 = 0.9335802793502808 + 0.1 * 6.8635149002075195
Epoch 180, val loss: 1.0945857763290405
Epoch 190, training loss: 1.563044548034668 = 0.8774257898330688 + 0.1 * 6.856186866760254
Epoch 190, val loss: 1.0556800365447998
Epoch 200, training loss: 1.5068621635437012 = 0.821980893611908 + 0.1 * 6.848812103271484
Epoch 200, val loss: 1.0170472860336304
Epoch 210, training loss: 1.4494918584823608 = 0.7653154730796814 + 0.1 * 6.841763496398926
Epoch 210, val loss: 0.9771795272827148
Epoch 220, training loss: 1.3932199478149414 = 0.7078877687454224 + 0.1 * 6.8533220291137695
Epoch 220, val loss: 0.9367542862892151
Epoch 230, training loss: 1.3359662294387817 = 0.6525772213935852 + 0.1 * 6.833889961242676
Epoch 230, val loss: 0.8988059759140015
Epoch 240, training loss: 1.2826285362243652 = 0.6001843810081482 + 0.1 * 6.824441432952881
Epoch 240, val loss: 0.864226758480072
Epoch 250, training loss: 1.2334418296813965 = 0.5514636039733887 + 0.1 * 6.819782257080078
Epoch 250, val loss: 0.8345011472702026
Epoch 260, training loss: 1.1881870031356812 = 0.506974995136261 + 0.1 * 6.812119960784912
Epoch 260, val loss: 0.8102942705154419
Epoch 270, training loss: 1.1477152109146118 = 0.46668925881385803 + 0.1 * 6.810258865356445
Epoch 270, val loss: 0.7914122939109802
Epoch 280, training loss: 1.1100612878799438 = 0.4300353527069092 + 0.1 * 6.800259113311768
Epoch 280, val loss: 0.7771424651145935
Epoch 290, training loss: 1.076656699180603 = 0.396505743265152 + 0.1 * 6.801509380340576
Epoch 290, val loss: 0.766644299030304
Epoch 300, training loss: 1.0448582172393799 = 0.36555302143096924 + 0.1 * 6.7930521965026855
Epoch 300, val loss: 0.759093165397644
Epoch 310, training loss: 1.0141321420669556 = 0.33613601326942444 + 0.1 * 6.779961585998535
Epoch 310, val loss: 0.753724217414856
Epoch 320, training loss: 0.98505699634552 = 0.30782464146614075 + 0.1 * 6.7723236083984375
Epoch 320, val loss: 0.7499787211418152
Epoch 330, training loss: 0.9571922421455383 = 0.28064489364624023 + 0.1 * 6.765473365783691
Epoch 330, val loss: 0.7477514147758484
Epoch 340, training loss: 0.9309336543083191 = 0.25483477115631104 + 0.1 * 6.760988712310791
Epoch 340, val loss: 0.7469823360443115
Epoch 350, training loss: 0.9085164666175842 = 0.23050810396671295 + 0.1 * 6.780083656311035
Epoch 350, val loss: 0.7476711273193359
Epoch 360, training loss: 0.8829852342605591 = 0.20801812410354614 + 0.1 * 6.74967098236084
Epoch 360, val loss: 0.749805212020874
Epoch 370, training loss: 0.8634822964668274 = 0.18745839595794678 + 0.1 * 6.7602386474609375
Epoch 370, val loss: 0.7534686326980591
Epoch 380, training loss: 0.8430384993553162 = 0.16902905702590942 + 0.1 * 6.740094184875488
Epoch 380, val loss: 0.7585115432739258
Epoch 390, training loss: 0.8261563777923584 = 0.15262283384799957 + 0.1 * 6.735335350036621
Epoch 390, val loss: 0.7649253010749817
Epoch 400, training loss: 0.8113422393798828 = 0.13811464607715607 + 0.1 * 6.73227596282959
Epoch 400, val loss: 0.7725233435630798
Epoch 410, training loss: 0.7971380352973938 = 0.12536166608333588 + 0.1 * 6.717763423919678
Epoch 410, val loss: 0.7809382081031799
Epoch 420, training loss: 0.786383867263794 = 0.11413794755935669 + 0.1 * 6.722459316253662
Epoch 420, val loss: 0.7900518178939819
Epoch 430, training loss: 0.7753124833106995 = 0.10424482822418213 + 0.1 * 6.710676193237305
Epoch 430, val loss: 0.7996993064880371
Epoch 440, training loss: 0.7673577070236206 = 0.09546338766813278 + 0.1 * 6.718943119049072
Epoch 440, val loss: 0.8096684813499451
Epoch 450, training loss: 0.7577815651893616 = 0.08767490088939667 + 0.1 * 6.701066493988037
Epoch 450, val loss: 0.8198463916778564
Epoch 460, training loss: 0.7499889135360718 = 0.0807153731584549 + 0.1 * 6.692735195159912
Epoch 460, val loss: 0.8301491141319275
Epoch 470, training loss: 0.7427230477333069 = 0.07446925342082977 + 0.1 * 6.68253755569458
Epoch 470, val loss: 0.8405618667602539
Epoch 480, training loss: 0.7373508810997009 = 0.06885101646184921 + 0.1 * 6.684998512268066
Epoch 480, val loss: 0.8509002923965454
Epoch 490, training loss: 0.73106849193573 = 0.06381174176931381 + 0.1 * 6.672567844390869
Epoch 490, val loss: 0.8611630201339722
Epoch 500, training loss: 0.7264586687088013 = 0.05924998223781586 + 0.1 * 6.672086715698242
Epoch 500, val loss: 0.871364951133728
Epoch 510, training loss: 0.7216541171073914 = 0.0551089271903038 + 0.1 * 6.665452003479004
Epoch 510, val loss: 0.8815137147903442
Epoch 520, training loss: 0.7190866470336914 = 0.05133793130517006 + 0.1 * 6.677486896514893
Epoch 520, val loss: 0.8916086554527283
Epoch 530, training loss: 0.7137628197669983 = 0.04790744557976723 + 0.1 * 6.658553600311279
Epoch 530, val loss: 0.9014827013015747
Epoch 540, training loss: 0.7100809812545776 = 0.0447806753218174 + 0.1 * 6.653002738952637
Epoch 540, val loss: 0.9112298488616943
Epoch 550, training loss: 0.7065321803092957 = 0.04192255064845085 + 0.1 * 6.646096229553223
Epoch 550, val loss: 0.9208517670631409
Epoch 560, training loss: 0.7045092582702637 = 0.039303816854953766 + 0.1 * 6.652054309844971
Epoch 560, val loss: 0.9303371906280518
Epoch 570, training loss: 0.7002575993537903 = 0.03690453991293907 + 0.1 * 6.633530616760254
Epoch 570, val loss: 0.9396803379058838
Epoch 580, training loss: 0.6976286768913269 = 0.034701015800237656 + 0.1 * 6.629276752471924
Epoch 580, val loss: 0.9488318562507629
Epoch 590, training loss: 0.699404239654541 = 0.03267597407102585 + 0.1 * 6.667282581329346
Epoch 590, val loss: 0.9578236937522888
Epoch 600, training loss: 0.6932805180549622 = 0.030820736661553383 + 0.1 * 6.624597549438477
Epoch 600, val loss: 0.9665253758430481
Epoch 610, training loss: 0.6911736130714417 = 0.0291119534522295 + 0.1 * 6.6206159591674805
Epoch 610, val loss: 0.9751223921775818
Epoch 620, training loss: 0.6889320015907288 = 0.027532614767551422 + 0.1 * 6.6139936447143555
Epoch 620, val loss: 0.9835834503173828
Epoch 630, training loss: 0.6881994009017944 = 0.026069680228829384 + 0.1 * 6.6212968826293945
Epoch 630, val loss: 0.991905152797699
Epoch 640, training loss: 0.6868171095848083 = 0.02471914328634739 + 0.1 * 6.6209797859191895
Epoch 640, val loss: 0.9999630451202393
Epoch 650, training loss: 0.6837959289550781 = 0.023470789194107056 + 0.1 * 6.6032514572143555
Epoch 650, val loss: 1.007920742034912
Epoch 660, training loss: 0.6821322441101074 = 0.022313423454761505 + 0.1 * 6.598188400268555
Epoch 660, val loss: 1.0155901908874512
Epoch 670, training loss: 0.6806746125221252 = 0.02124141901731491 + 0.1 * 6.594331741333008
Epoch 670, val loss: 1.0232248306274414
Epoch 680, training loss: 0.6802293062210083 = 0.020244143903255463 + 0.1 * 6.599851608276367
Epoch 680, val loss: 1.0305936336517334
Epoch 690, training loss: 0.6791989803314209 = 0.019317662343382835 + 0.1 * 6.598813056945801
Epoch 690, val loss: 1.0379163026809692
Epoch 700, training loss: 0.6769017577171326 = 0.018453044816851616 + 0.1 * 6.584486961364746
Epoch 700, val loss: 1.045024037361145
Epoch 710, training loss: 0.6765437722206116 = 0.017645757645368576 + 0.1 * 6.588979721069336
Epoch 710, val loss: 1.0520811080932617
Epoch 720, training loss: 0.6742720603942871 = 0.01689050905406475 + 0.1 * 6.57381534576416
Epoch 720, val loss: 1.058896541595459
Epoch 730, training loss: 0.6728224754333496 = 0.01618376187980175 + 0.1 * 6.566387176513672
Epoch 730, val loss: 1.0656647682189941
Epoch 740, training loss: 0.674371063709259 = 0.015521029941737652 + 0.1 * 6.588500022888184
Epoch 740, val loss: 1.0721590518951416
Epoch 750, training loss: 0.6713805198669434 = 0.014901827089488506 + 0.1 * 6.564786911010742
Epoch 750, val loss: 1.0785815715789795
Epoch 760, training loss: 0.6713149547576904 = 0.014320902526378632 + 0.1 * 6.569940090179443
Epoch 760, val loss: 1.0848621129989624
Epoch 770, training loss: 0.6693957448005676 = 0.013775135390460491 + 0.1 * 6.556206226348877
Epoch 770, val loss: 1.0909796953201294
Epoch 780, training loss: 0.6708566546440125 = 0.01326129399240017 + 0.1 * 6.575953006744385
Epoch 780, val loss: 1.097004771232605
Epoch 790, training loss: 0.6681556701660156 = 0.012776937335729599 + 0.1 * 6.5537872314453125
Epoch 790, val loss: 1.1028871536254883
Epoch 800, training loss: 0.6686232089996338 = 0.012319997884333134 + 0.1 * 6.563032150268555
Epoch 800, val loss: 1.108679175376892
Epoch 810, training loss: 0.6671133637428284 = 0.011887828819453716 + 0.1 * 6.552255153656006
Epoch 810, val loss: 1.1142741441726685
Epoch 820, training loss: 0.6666203737258911 = 0.01148038450628519 + 0.1 * 6.5513997077941895
Epoch 820, val loss: 1.1198327541351318
Epoch 830, training loss: 0.6650525331497192 = 0.011094971559941769 + 0.1 * 6.539575099945068
Epoch 830, val loss: 1.1252248287200928
Epoch 840, training loss: 0.6655614972114563 = 0.010730988346040249 + 0.1 * 6.548305034637451
Epoch 840, val loss: 1.1305452585220337
Epoch 850, training loss: 0.6632475852966309 = 0.010386007837951183 + 0.1 * 6.528615951538086
Epoch 850, val loss: 1.135772705078125
Epoch 860, training loss: 0.6640599966049194 = 0.010058600455522537 + 0.1 * 6.540014266967773
Epoch 860, val loss: 1.140916109085083
Epoch 870, training loss: 0.6624845862388611 = 0.009747232310473919 + 0.1 * 6.527373790740967
Epoch 870, val loss: 1.1458886861801147
Epoch 880, training loss: 0.6628627777099609 = 0.009452025406062603 + 0.1 * 6.534107208251953
Epoch 880, val loss: 1.1508556604385376
Epoch 890, training loss: 0.6609814167022705 = 0.009171238169074059 + 0.1 * 6.518101692199707
Epoch 890, val loss: 1.1557167768478394
Epoch 900, training loss: 0.6636881828308105 = 0.008903841488063335 + 0.1 * 6.547842979431152
Epoch 900, val loss: 1.1604264974594116
Epoch 910, training loss: 0.66118985414505 = 0.008649476803839207 + 0.1 * 6.52540397644043
Epoch 910, val loss: 1.1651222705841064
Epoch 920, training loss: 0.6595616936683655 = 0.00840723142027855 + 0.1 * 6.511544227600098
Epoch 920, val loss: 1.1696945428848267
Epoch 930, training loss: 0.6614871025085449 = 0.008175899274647236 + 0.1 * 6.533112049102783
Epoch 930, val loss: 1.1742275953292847
Epoch 940, training loss: 0.6596876978874207 = 0.007954741828143597 + 0.1 * 6.517329692840576
Epoch 940, val loss: 1.1786441802978516
Epoch 950, training loss: 0.659457266330719 = 0.007743490859866142 + 0.1 * 6.51713752746582
Epoch 950, val loss: 1.1829928159713745
Epoch 960, training loss: 0.6582575440406799 = 0.007541789673268795 + 0.1 * 6.507157325744629
Epoch 960, val loss: 1.1872618198394775
Epoch 970, training loss: 0.6576191186904907 = 0.0073484815657138824 + 0.1 * 6.502706050872803
Epoch 970, val loss: 1.191467046737671
Epoch 980, training loss: 0.656642735004425 = 0.0071636284701526165 + 0.1 * 6.494791030883789
Epoch 980, val loss: 1.1955721378326416
Epoch 990, training loss: 0.656474232673645 = 0.006986767984926701 + 0.1 * 6.494874000549316
Epoch 990, val loss: 1.199680209159851
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.811342716217041 = 1.9516581296920776 + 0.1 * 8.596845626831055
Epoch 0, val loss: 1.9457659721374512
Epoch 10, training loss: 2.799839735031128 = 1.9401636123657227 + 0.1 * 8.596760749816895
Epoch 10, val loss: 1.9341681003570557
Epoch 20, training loss: 2.7851555347442627 = 1.925521969795227 + 0.1 * 8.596336364746094
Epoch 20, val loss: 1.919281244277954
Epoch 30, training loss: 2.7640631198883057 = 1.9047718048095703 + 0.1 * 8.592912673950195
Epoch 30, val loss: 1.8984060287475586
Epoch 40, training loss: 2.7310924530029297 = 1.87447988986969 + 0.1 * 8.566125869750977
Epoch 40, val loss: 1.8687151670455933
Epoch 50, training loss: 2.6730756759643555 = 1.8338651657104492 + 0.1 * 8.392105102539062
Epoch 50, val loss: 1.8311054706573486
Epoch 60, training loss: 2.594956398010254 = 1.7905460596084595 + 0.1 * 8.04410457611084
Epoch 60, val loss: 1.7941359281539917
Epoch 70, training loss: 2.5320138931274414 = 1.748549222946167 + 0.1 * 7.834646224975586
Epoch 70, val loss: 1.7580809593200684
Epoch 80, training loss: 2.468182325363159 = 1.6988983154296875 + 0.1 * 7.692840099334717
Epoch 80, val loss: 1.7140923738479614
Epoch 90, training loss: 2.393399238586426 = 1.6364449262619019 + 0.1 * 7.569544315338135
Epoch 90, val loss: 1.660077452659607
Epoch 100, training loss: 2.30161714553833 = 1.5615125894546509 + 0.1 * 7.4010467529296875
Epoch 100, val loss: 1.5976868867874146
Epoch 110, training loss: 2.2044410705566406 = 1.4822590351104736 + 0.1 * 7.2218194007873535
Epoch 110, val loss: 1.5330336093902588
Epoch 120, training loss: 2.1158742904663086 = 1.4060415029525757 + 0.1 * 7.098329067230225
Epoch 120, val loss: 1.4734290838241577
Epoch 130, training loss: 2.038564682006836 = 1.3366256952285767 + 0.1 * 7.0193891525268555
Epoch 130, val loss: 1.4212000370025635
Epoch 140, training loss: 1.9728801250457764 = 1.2757351398468018 + 0.1 * 6.971450328826904
Epoch 140, val loss: 1.3766623735427856
Epoch 150, training loss: 1.9179761409759521 = 1.2236418724060059 + 0.1 * 6.943343162536621
Epoch 150, val loss: 1.3411533832550049
Epoch 160, training loss: 1.8706082105636597 = 1.1781960725784302 + 0.1 * 6.924121379852295
Epoch 160, val loss: 1.312107801437378
Epoch 170, training loss: 1.825426459312439 = 1.1345447301864624 + 0.1 * 6.908817291259766
Epoch 170, val loss: 1.284448266029358
Epoch 180, training loss: 1.7781636714935303 = 1.0884547233581543 + 0.1 * 6.897088527679443
Epoch 180, val loss: 1.2540233135223389
Epoch 190, training loss: 1.725154161453247 = 1.0365493297576904 + 0.1 * 6.886048793792725
Epoch 190, val loss: 1.2174468040466309
Epoch 200, training loss: 1.6645960807800293 = 0.9769678711891174 + 0.1 * 6.876282215118408
Epoch 200, val loss: 1.1736687421798706
Epoch 210, training loss: 1.5970065593719482 = 0.9099268317222595 + 0.1 * 6.8707966804504395
Epoch 210, val loss: 1.123680591583252
Epoch 220, training loss: 1.52485990524292 = 0.8384630680084229 + 0.1 * 6.863968372344971
Epoch 220, val loss: 1.0697227716445923
Epoch 230, training loss: 1.451854944229126 = 0.7661463618278503 + 0.1 * 6.857085704803467
Epoch 230, val loss: 1.0154495239257812
Epoch 240, training loss: 1.3818252086639404 = 0.6965106725692749 + 0.1 * 6.853145599365234
Epoch 240, val loss: 0.9640931487083435
Epoch 250, training loss: 1.3177564144134521 = 0.6327748894691467 + 0.1 * 6.8498148918151855
Epoch 250, val loss: 0.9189083576202393
Epoch 260, training loss: 1.2605047225952148 = 0.5766398906707764 + 0.1 * 6.838647842407227
Epoch 260, val loss: 0.8820133805274963
Epoch 270, training loss: 1.2118959426879883 = 0.5279334187507629 + 0.1 * 6.839625835418701
Epoch 270, val loss: 0.8535984754562378
Epoch 280, training loss: 1.1687443256378174 = 0.4862426519393921 + 0.1 * 6.825016498565674
Epoch 280, val loss: 0.8331004977226257
Epoch 290, training loss: 1.1317601203918457 = 0.45011740922927856 + 0.1 * 6.816427230834961
Epoch 290, val loss: 0.8193211555480957
Epoch 300, training loss: 1.1001335382461548 = 0.4182761609554291 + 0.1 * 6.818573951721191
Epoch 300, val loss: 0.8106259703636169
Epoch 310, training loss: 1.069867730140686 = 0.3897368013858795 + 0.1 * 6.801309108734131
Epoch 310, val loss: 0.8057447075843811
Epoch 320, training loss: 1.044245958328247 = 0.36326542496681213 + 0.1 * 6.809805393218994
Epoch 320, val loss: 0.8034282922744751
Epoch 330, training loss: 1.0164180994033813 = 0.33787134289741516 + 0.1 * 6.785467624664307
Epoch 330, val loss: 0.8024163246154785
Epoch 340, training loss: 0.9897355437278748 = 0.31221866607666016 + 0.1 * 6.775168418884277
Epoch 340, val loss: 0.8018143177032471
Epoch 350, training loss: 0.9620778560638428 = 0.28531286120414734 + 0.1 * 6.767650127410889
Epoch 350, val loss: 0.8009743690490723
Epoch 360, training loss: 0.9330220222473145 = 0.2567616105079651 + 0.1 * 6.762604236602783
Epoch 360, val loss: 0.7997875213623047
Epoch 370, training loss: 0.9033806324005127 = 0.2275046408176422 + 0.1 * 6.75875997543335
Epoch 370, val loss: 0.7987346649169922
Epoch 380, training loss: 0.8747144341468811 = 0.19942151010036469 + 0.1 * 6.752928733825684
Epoch 380, val loss: 0.798559844493866
Epoch 390, training loss: 0.8490420579910278 = 0.17414133250713348 + 0.1 * 6.749006748199463
Epoch 390, val loss: 0.8002511262893677
Epoch 400, training loss: 0.8264449834823608 = 0.1524617224931717 + 0.1 * 6.739832401275635
Epoch 400, val loss: 0.8037233948707581
Epoch 410, training loss: 0.8073928356170654 = 0.13416849076747894 + 0.1 * 6.732243061065674
Epoch 410, val loss: 0.8087420463562012
Epoch 420, training loss: 0.7922065854072571 = 0.11872795969247818 + 0.1 * 6.734786033630371
Epoch 420, val loss: 0.81526118516922
Epoch 430, training loss: 0.7770436406135559 = 0.10561404377222061 + 0.1 * 6.714295387268066
Epoch 430, val loss: 0.8228469491004944
Epoch 440, training loss: 0.7664171457290649 = 0.09435945004224777 + 0.1 * 6.720576763153076
Epoch 440, val loss: 0.831298828125
Epoch 450, training loss: 0.7550730109214783 = 0.08465318381786346 + 0.1 * 6.704197883605957
Epoch 450, val loss: 0.8403885364532471
Epoch 460, training loss: 0.7456348538398743 = 0.07621254771947861 + 0.1 * 6.694223403930664
Epoch 460, val loss: 0.849938154220581
Epoch 470, training loss: 0.7380617260932922 = 0.06884894520044327 + 0.1 * 6.692127227783203
Epoch 470, val loss: 0.859763503074646
Epoch 480, training loss: 0.7302656173706055 = 0.06240847706794739 + 0.1 * 6.678571701049805
Epoch 480, val loss: 0.8697105646133423
Epoch 490, training loss: 0.7244412899017334 = 0.05674256384372711 + 0.1 * 6.676987171173096
Epoch 490, val loss: 0.8798194527626038
Epoch 500, training loss: 0.7186030745506287 = 0.0517522469162941 + 0.1 * 6.668508052825928
Epoch 500, val loss: 0.8899748921394348
Epoch 510, training loss: 0.7135449051856995 = 0.04734411463141441 + 0.1 * 6.662008285522461
Epoch 510, val loss: 0.9001118540763855
Epoch 520, training loss: 0.7094141840934753 = 0.043436527252197266 + 0.1 * 6.65977668762207
Epoch 520, val loss: 0.9101347327232361
Epoch 530, training loss: 0.7069029211997986 = 0.03997091203927994 + 0.1 * 6.6693196296691895
Epoch 530, val loss: 0.9200765490531921
Epoch 540, training loss: 0.7013379335403442 = 0.036888714879751205 + 0.1 * 6.644492149353027
Epoch 540, val loss: 0.9298334121704102
Epoch 550, training loss: 0.6982206702232361 = 0.0341353677213192 + 0.1 * 6.640852928161621
Epoch 550, val loss: 0.9394133687019348
Epoch 560, training loss: 0.6951051950454712 = 0.03166840597987175 + 0.1 * 6.6343674659729
Epoch 560, val loss: 0.9487849473953247
Epoch 570, training loss: 0.6922457814216614 = 0.029457535594701767 + 0.1 * 6.627882480621338
Epoch 570, val loss: 0.958048939704895
Epoch 580, training loss: 0.6904585361480713 = 0.027464618906378746 + 0.1 * 6.629939079284668
Epoch 580, val loss: 0.967069149017334
Epoch 590, training loss: 0.6896629333496094 = 0.025667116045951843 + 0.1 * 6.639957904815674
Epoch 590, val loss: 0.9757802486419678
Epoch 600, training loss: 0.6855790615081787 = 0.02404453046619892 + 0.1 * 6.615345478057861
Epoch 600, val loss: 0.9843358993530273
Epoch 610, training loss: 0.6832148432731628 = 0.02257324568927288 + 0.1 * 6.60641622543335
Epoch 610, val loss: 0.99263995885849
Epoch 620, training loss: 0.6839456558227539 = 0.02123289741575718 + 0.1 * 6.627127647399902
Epoch 620, val loss: 1.000797152519226
Epoch 630, training loss: 0.6799389719963074 = 0.020011665299534798 + 0.1 * 6.599273204803467
Epoch 630, val loss: 1.0086880922317505
Epoch 640, training loss: 0.6794615983963013 = 0.01889747753739357 + 0.1 * 6.605640888214111
Epoch 640, val loss: 1.016302227973938
Epoch 650, training loss: 0.6773673892021179 = 0.017877820879220963 + 0.1 * 6.594895362854004
Epoch 650, val loss: 1.0238430500030518
Epoch 660, training loss: 0.6765271425247192 = 0.016941020265221596 + 0.1 * 6.595861434936523
Epoch 660, val loss: 1.0310827493667603
Epoch 670, training loss: 0.6749485731124878 = 0.016079606488347054 + 0.1 * 6.588689804077148
Epoch 670, val loss: 1.0382510423660278
Epoch 680, training loss: 0.6733253598213196 = 0.015284737572073936 + 0.1 * 6.580406188964844
Epoch 680, val loss: 1.045198917388916
Epoch 690, training loss: 0.6733289361000061 = 0.014549313113093376 + 0.1 * 6.587796211242676
Epoch 690, val loss: 1.0519627332687378
Epoch 700, training loss: 0.6717527508735657 = 0.013869489543139935 + 0.1 * 6.578832626342773
Epoch 700, val loss: 1.0584839582443237
Epoch 710, training loss: 0.6704927682876587 = 0.013240109197795391 + 0.1 * 6.572526454925537
Epoch 710, val loss: 1.0650181770324707
Epoch 720, training loss: 0.669768214225769 = 0.012654785066843033 + 0.1 * 6.571134090423584
Epoch 720, val loss: 1.0712170600891113
Epoch 730, training loss: 0.6689271926879883 = 0.012110662646591663 + 0.1 * 6.568165302276611
Epoch 730, val loss: 1.0773159265518188
Epoch 740, training loss: 0.6676762104034424 = 0.011603787541389465 + 0.1 * 6.560724258422852
Epoch 740, val loss: 1.0833669900894165
Epoch 750, training loss: 0.6668516397476196 = 0.011130261234939098 + 0.1 * 6.55721378326416
Epoch 750, val loss: 1.0891728401184082
Epoch 760, training loss: 0.6657539010047913 = 0.010686966590583324 + 0.1 * 6.550668716430664
Epoch 760, val loss: 1.094731330871582
Epoch 770, training loss: 0.6650403738021851 = 0.010272610932588577 + 0.1 * 6.547677516937256
Epoch 770, val loss: 1.1003894805908203
Epoch 780, training loss: 0.6659216284751892 = 0.009883930906653404 + 0.1 * 6.5603766441345215
Epoch 780, val loss: 1.1057060956954956
Epoch 790, training loss: 0.6644654273986816 = 0.009519175626337528 + 0.1 * 6.549462795257568
Epoch 790, val loss: 1.1109651327133179
Epoch 800, training loss: 0.6627995371818542 = 0.009176652878522873 + 0.1 * 6.536228656768799
Epoch 800, val loss: 1.116178274154663
Epoch 810, training loss: 0.6638057827949524 = 0.008853713981807232 + 0.1 * 6.549520492553711
Epoch 810, val loss: 1.1211802959442139
Epoch 820, training loss: 0.6623256206512451 = 0.00854866486042738 + 0.1 * 6.537769317626953
Epoch 820, val loss: 1.1260796785354614
Epoch 830, training loss: 0.6612878441810608 = 0.008261318318545818 + 0.1 * 6.530264854431152
Epoch 830, val loss: 1.131011962890625
Epoch 840, training loss: 0.6610237956047058 = 0.007989085279405117 + 0.1 * 6.530346870422363
Epoch 840, val loss: 1.13551926612854
Epoch 850, training loss: 0.6608160138130188 = 0.007732543628662825 + 0.1 * 6.530834197998047
Epoch 850, val loss: 1.1402525901794434
Epoch 860, training loss: 0.6607497930526733 = 0.007489098701626062 + 0.1 * 6.532607078552246
Epoch 860, val loss: 1.1447575092315674
Epoch 870, training loss: 0.6597896814346313 = 0.007258333265781403 + 0.1 * 6.525313377380371
Epoch 870, val loss: 1.1490670442581177
Epoch 880, training loss: 0.6583584547042847 = 0.007039695978164673 + 0.1 * 6.513187885284424
Epoch 880, val loss: 1.153528094291687
Epoch 890, training loss: 0.6601529717445374 = 0.006831742357462645 + 0.1 * 6.533211708068848
Epoch 890, val loss: 1.1576802730560303
Epoch 900, training loss: 0.6584177017211914 = 0.006633494049310684 + 0.1 * 6.517841815948486
Epoch 900, val loss: 1.161790132522583
Epoch 910, training loss: 0.6575579643249512 = 0.006445418577641249 + 0.1 * 6.511125564575195
Epoch 910, val loss: 1.1659367084503174
Epoch 920, training loss: 0.659304141998291 = 0.006265784613788128 + 0.1 * 6.530383586883545
Epoch 920, val loss: 1.1698023080825806
Epoch 930, training loss: 0.6570441126823425 = 0.0060945232398808 + 0.1 * 6.509495735168457
Epoch 930, val loss: 1.1736353635787964
Epoch 940, training loss: 0.6560591459274292 = 0.0059314873069524765 + 0.1 * 6.50127649307251
Epoch 940, val loss: 1.1775939464569092
Epoch 950, training loss: 0.656387448310852 = 0.005775362253189087 + 0.1 * 6.506120204925537
Epoch 950, val loss: 1.181196928024292
Epoch 960, training loss: 0.6556583046913147 = 0.005626144353300333 + 0.1 * 6.500321388244629
Epoch 960, val loss: 1.1847655773162842
Epoch 970, training loss: 0.6553199887275696 = 0.0054839663207530975 + 0.1 * 6.498359680175781
Epoch 970, val loss: 1.1884840726852417
Epoch 980, training loss: 0.6553237438201904 = 0.005347786005586386 + 0.1 * 6.499759674072266
Epoch 980, val loss: 1.1920242309570312
Epoch 990, training loss: 0.6551108360290527 = 0.005216747522354126 + 0.1 * 6.498940944671631
Epoch 990, val loss: 1.1953222751617432
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.838165524512388
The final CL Acc:0.80741, 0.00524, The final GNN Acc:0.84010, 0.00237
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9592])
updated graph: torch.Size([2, 10622])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8060860633850098 = 1.9464055299758911 + 0.1 * 8.596806526184082
Epoch 0, val loss: 1.946435809135437
Epoch 10, training loss: 2.795656442642212 = 1.9359878301620483 + 0.1 * 8.596686363220215
Epoch 10, val loss: 1.9368679523468018
Epoch 20, training loss: 2.782376289367676 = 1.922780990600586 + 0.1 * 8.595952033996582
Epoch 20, val loss: 1.9243794679641724
Epoch 30, training loss: 2.762821674346924 = 1.9038665294647217 + 0.1 * 8.589550018310547
Epoch 30, val loss: 1.9063078165054321
Epoch 40, training loss: 2.7314114570617676 = 1.8760420083999634 + 0.1 * 8.553695678710938
Epoch 40, val loss: 1.8801593780517578
Epoch 50, training loss: 2.6794629096984863 = 1.8398480415344238 + 0.1 * 8.396149635314941
Epoch 50, val loss: 1.8482301235198975
Epoch 60, training loss: 2.6217799186706543 = 1.803141713142395 + 0.1 * 8.186381340026855
Epoch 60, val loss: 1.817915678024292
Epoch 70, training loss: 2.555720806121826 = 1.7709890604019165 + 0.1 * 7.847317218780518
Epoch 70, val loss: 1.7890863418579102
Epoch 80, training loss: 2.480883836746216 = 1.7364702224731445 + 0.1 * 7.4441351890563965
Epoch 80, val loss: 1.7566003799438477
Epoch 90, training loss: 2.411012887954712 = 1.694786787033081 + 0.1 * 7.162260055541992
Epoch 90, val loss: 1.7213313579559326
Epoch 100, training loss: 2.3426780700683594 = 1.6369640827178955 + 0.1 * 7.057140827178955
Epoch 100, val loss: 1.6735576391220093
Epoch 110, training loss: 2.263091802597046 = 1.5642551183700562 + 0.1 * 6.988367557525635
Epoch 110, val loss: 1.615330696105957
Epoch 120, training loss: 2.1821563243865967 = 1.4873825311660767 + 0.1 * 6.947737216949463
Epoch 120, val loss: 1.5552585124969482
Epoch 130, training loss: 2.1064393520355225 = 1.4142608642578125 + 0.1 * 6.9217848777771
Epoch 130, val loss: 1.4986079931259155
Epoch 140, training loss: 2.03627872467041 = 1.3470388650894165 + 0.1 * 6.892397880554199
Epoch 140, val loss: 1.4466993808746338
Epoch 150, training loss: 1.9703505039215088 = 1.283669352531433 + 0.1 * 6.8668107986450195
Epoch 150, val loss: 1.3986965417861938
Epoch 160, training loss: 1.9076859951019287 = 1.2231895923614502 + 0.1 * 6.844964504241943
Epoch 160, val loss: 1.3543436527252197
Epoch 170, training loss: 1.8471434116363525 = 1.1642634868621826 + 0.1 * 6.828799247741699
Epoch 170, val loss: 1.3134340047836304
Epoch 180, training loss: 1.786219835281372 = 1.1047101020812988 + 0.1 * 6.815097808837891
Epoch 180, val loss: 1.272552251815796
Epoch 190, training loss: 1.7219717502593994 = 1.041347861289978 + 0.1 * 6.806239604949951
Epoch 190, val loss: 1.228533387184143
Epoch 200, training loss: 1.6536505222320557 = 0.9739616513252258 + 0.1 * 6.7968878746032715
Epoch 200, val loss: 1.180938720703125
Epoch 210, training loss: 1.5817545652389526 = 0.9029709696769714 + 0.1 * 6.787836074829102
Epoch 210, val loss: 1.1303057670593262
Epoch 220, training loss: 1.5086016654968262 = 0.8304898738861084 + 0.1 * 6.781118392944336
Epoch 220, val loss: 1.0784854888916016
Epoch 230, training loss: 1.437483787536621 = 0.7599883675575256 + 0.1 * 6.774953365325928
Epoch 230, val loss: 1.0285632610321045
Epoch 240, training loss: 1.371595859527588 = 0.6940463185310364 + 0.1 * 6.775494575500488
Epoch 240, val loss: 0.9831647872924805
Epoch 250, training loss: 1.3111801147460938 = 0.6345936059951782 + 0.1 * 6.765865802764893
Epoch 250, val loss: 0.9444535970687866
Epoch 260, training loss: 1.2564582824707031 = 0.5807844996452332 + 0.1 * 6.75673770904541
Epoch 260, val loss: 0.9126600027084351
Epoch 270, training loss: 1.2093311548233032 = 0.5318783521652222 + 0.1 * 6.7745280265808105
Epoch 270, val loss: 0.8879963755607605
Epoch 280, training loss: 1.16337251663208 = 0.48802289366722107 + 0.1 * 6.753495693206787
Epoch 280, val loss: 0.8701335787773132
Epoch 290, training loss: 1.1219561100006104 = 0.4479551613330841 + 0.1 * 6.740009307861328
Epoch 290, val loss: 0.8576713800430298
Epoch 300, training loss: 1.086341142654419 = 0.41098231077194214 + 0.1 * 6.753588676452637
Epoch 300, val loss: 0.8494310975074768
Epoch 310, training loss: 1.0501753091812134 = 0.376926451921463 + 0.1 * 6.73248815536499
Epoch 310, val loss: 0.8441994190216064
Epoch 320, training loss: 1.0172662734985352 = 0.34506720304489136 + 0.1 * 6.721991062164307
Epoch 320, val loss: 0.8411751985549927
Epoch 330, training loss: 0.9874930381774902 = 0.3150961101055145 + 0.1 * 6.723968982696533
Epoch 330, val loss: 0.8397707939147949
Epoch 340, training loss: 0.9587035775184631 = 0.2871120572090149 + 0.1 * 6.715915203094482
Epoch 340, val loss: 0.8401309251785278
Epoch 350, training loss: 0.9311257600784302 = 0.26095083355903625 + 0.1 * 6.701748847961426
Epoch 350, val loss: 0.8425645232200623
Epoch 360, training loss: 0.9066598415374756 = 0.23678408563137054 + 0.1 * 6.698757171630859
Epoch 360, val loss: 0.8469470143318176
Epoch 370, training loss: 0.8837816119194031 = 0.21479345858097076 + 0.1 * 6.689881324768066
Epoch 370, val loss: 0.8532073497772217
Epoch 380, training loss: 0.8627489805221558 = 0.1948670744895935 + 0.1 * 6.678819179534912
Epoch 380, val loss: 0.8613090515136719
Epoch 390, training loss: 0.8445731401443481 = 0.17685100436210632 + 0.1 * 6.677220821380615
Epoch 390, val loss: 0.8708460330963135
Epoch 400, training loss: 0.8279176950454712 = 0.16064149141311646 + 0.1 * 6.672761917114258
Epoch 400, val loss: 0.8816383481025696
Epoch 410, training loss: 0.8131706118583679 = 0.1460549682378769 + 0.1 * 6.67115592956543
Epoch 410, val loss: 0.8933274149894714
Epoch 420, training loss: 0.7983694076538086 = 0.1329662799835205 + 0.1 * 6.654031276702881
Epoch 420, val loss: 0.9058741331100464
Epoch 430, training loss: 0.7859998345375061 = 0.1211705207824707 + 0.1 * 6.6482930183410645
Epoch 430, val loss: 0.9191112518310547
Epoch 440, training loss: 0.7746455073356628 = 0.11054625362157822 + 0.1 * 6.640992641448975
Epoch 440, val loss: 0.932945966720581
Epoch 450, training loss: 0.7645546197891235 = 0.10099329799413681 + 0.1 * 6.635613441467285
Epoch 450, val loss: 0.9471514225006104
Epoch 460, training loss: 0.7551074028015137 = 0.09239965677261353 + 0.1 * 6.627077102661133
Epoch 460, val loss: 0.961768388748169
Epoch 470, training loss: 0.7467476725578308 = 0.08467937260866165 + 0.1 * 6.620682716369629
Epoch 470, val loss: 0.976561963558197
Epoch 480, training loss: 0.7392416596412659 = 0.07774343341588974 + 0.1 * 6.614982604980469
Epoch 480, val loss: 0.9914543032646179
Epoch 490, training loss: 0.7343749403953552 = 0.07149092853069305 + 0.1 * 6.62883996963501
Epoch 490, val loss: 1.0065205097198486
Epoch 500, training loss: 0.7263036370277405 = 0.06588487327098846 + 0.1 * 6.604187488555908
Epoch 500, val loss: 1.0214858055114746
Epoch 510, training loss: 0.7209280729293823 = 0.06083991006016731 + 0.1 * 6.600881576538086
Epoch 510, val loss: 1.03635835647583
Epoch 520, training loss: 0.7153682708740234 = 0.0562952384352684 + 0.1 * 6.590730667114258
Epoch 520, val loss: 1.0510408878326416
Epoch 530, training loss: 0.7111918926239014 = 0.05219946429133415 + 0.1 * 6.589923858642578
Epoch 530, val loss: 1.0656867027282715
Epoch 540, training loss: 0.7088765501976013 = 0.04849674180150032 + 0.1 * 6.6037983894348145
Epoch 540, val loss: 1.0800156593322754
Epoch 550, training loss: 0.7036784291267395 = 0.04516037553548813 + 0.1 * 6.585180759429932
Epoch 550, val loss: 1.094110131263733
Epoch 560, training loss: 0.700546145439148 = 0.0421389639377594 + 0.1 * 6.584071636199951
Epoch 560, val loss: 1.1078269481658936
Epoch 570, training loss: 0.6970748901367188 = 0.03939822316169739 + 0.1 * 6.576766014099121
Epoch 570, val loss: 1.1214450597763062
Epoch 580, training loss: 0.6936774253845215 = 0.036902498453855515 + 0.1 * 6.5677490234375
Epoch 580, val loss: 1.1347993612289429
Epoch 590, training loss: 0.6918776631355286 = 0.034625984728336334 + 0.1 * 6.572516441345215
Epoch 590, val loss: 1.1478626728057861
Epoch 600, training loss: 0.6890956163406372 = 0.03255065530538559 + 0.1 * 6.5654497146606445
Epoch 600, val loss: 1.1605889797210693
Epoch 610, training loss: 0.6866360306739807 = 0.030657146126031876 + 0.1 * 6.559788703918457
Epoch 610, val loss: 1.1730551719665527
Epoch 620, training loss: 0.6841778755187988 = 0.028919782489538193 + 0.1 * 6.552581310272217
Epoch 620, val loss: 1.1852904558181763
Epoch 630, training loss: 0.6831868886947632 = 0.027325082570314407 + 0.1 * 6.55861759185791
Epoch 630, val loss: 1.1970185041427612
Epoch 640, training loss: 0.681298017501831 = 0.02586440183222294 + 0.1 * 6.554335594177246
Epoch 640, val loss: 1.2088431119918823
Epoch 650, training loss: 0.6786414384841919 = 0.024516208097338676 + 0.1 * 6.541252136230469
Epoch 650, val loss: 1.2201766967773438
Epoch 660, training loss: 0.6784037351608276 = 0.02327081188559532 + 0.1 * 6.551329135894775
Epoch 660, val loss: 1.2312673330307007
Epoch 670, training loss: 0.6763378977775574 = 0.022118931636214256 + 0.1 * 6.542189598083496
Epoch 670, val loss: 1.2423033714294434
Epoch 680, training loss: 0.6778135299682617 = 0.02105111815035343 + 0.1 * 6.567624092102051
Epoch 680, val loss: 1.252798080444336
Epoch 690, training loss: 0.6736540198326111 = 0.020062804222106934 + 0.1 * 6.535912036895752
Epoch 690, val loss: 1.2632989883422852
Epoch 700, training loss: 0.6716895699501038 = 0.019144142046570778 + 0.1 * 6.525454521179199
Epoch 700, val loss: 1.2734612226486206
Epoch 710, training loss: 0.6730149984359741 = 0.01828809455037117 + 0.1 * 6.547268867492676
Epoch 710, val loss: 1.2832356691360474
Epoch 720, training loss: 0.6698868870735168 = 0.017492184415459633 + 0.1 * 6.523946762084961
Epoch 720, val loss: 1.2931208610534668
Epoch 730, training loss: 0.6687812209129333 = 0.016748182475566864 + 0.1 * 6.520330429077148
Epoch 730, val loss: 1.3025914430618286
Epoch 740, training loss: 0.6680154800415039 = 0.01605183444917202 + 0.1 * 6.519636631011963
Epoch 740, val loss: 1.3118093013763428
Epoch 750, training loss: 0.6669056415557861 = 0.015400043688714504 + 0.1 * 6.5150556564331055
Epoch 750, val loss: 1.321056604385376
Epoch 760, training loss: 0.6663714647293091 = 0.014788329601287842 + 0.1 * 6.515830993652344
Epoch 760, val loss: 1.3298101425170898
Epoch 770, training loss: 0.6655957698822021 = 0.014214812777936459 + 0.1 * 6.513809680938721
Epoch 770, val loss: 1.3386579751968384
Epoch 780, training loss: 0.6646165251731873 = 0.01367573719471693 + 0.1 * 6.5094075202941895
Epoch 780, val loss: 1.3470410108566284
Epoch 790, training loss: 0.6638740301132202 = 0.013169013895094395 + 0.1 * 6.507050514221191
Epoch 790, val loss: 1.3555082082748413
Epoch 800, training loss: 0.664383590221405 = 0.012690790928900242 + 0.1 * 6.516928195953369
Epoch 800, val loss: 1.3636113405227661
Epoch 810, training loss: 0.6627947092056274 = 0.012240828946232796 + 0.1 * 6.5055389404296875
Epoch 810, val loss: 1.3716962337493896
Epoch 820, training loss: 0.6632285118103027 = 0.011815155856311321 + 0.1 * 6.514133453369141
Epoch 820, val loss: 1.3796190023422241
Epoch 830, training loss: 0.6612721085548401 = 0.011413432657718658 + 0.1 * 6.498586177825928
Epoch 830, val loss: 1.3870854377746582
Epoch 840, training loss: 0.6607388257980347 = 0.01103397086262703 + 0.1 * 6.497048377990723
Epoch 840, val loss: 1.3948501348495483
Epoch 850, training loss: 0.6603043675422668 = 0.010673310607671738 + 0.1 * 6.496310710906982
Epoch 850, val loss: 1.4020463228225708
Epoch 860, training loss: 0.6595096588134766 = 0.01033219788223505 + 0.1 * 6.491774082183838
Epoch 860, val loss: 1.4092283248901367
Epoch 870, training loss: 0.6592816710472107 = 0.010009001940488815 + 0.1 * 6.492726802825928
Epoch 870, val loss: 1.4165318012237549
Epoch 880, training loss: 0.6585901975631714 = 0.009700958617031574 + 0.1 * 6.488892555236816
Epoch 880, val loss: 1.4234892129898071
Epoch 890, training loss: 0.6583647131919861 = 0.009408139623701572 + 0.1 * 6.489565849304199
Epoch 890, val loss: 1.4301289319992065
Epoch 900, training loss: 0.6571705937385559 = 0.009129861369729042 + 0.1 * 6.480406761169434
Epoch 900, val loss: 1.437029242515564
Epoch 910, training loss: 0.6577593088150024 = 0.008864386938512325 + 0.1 * 6.488948822021484
Epoch 910, val loss: 1.4433720111846924
Epoch 920, training loss: 0.6566659808158875 = 0.0086121316999197 + 0.1 * 6.4805378913879395
Epoch 920, val loss: 1.4499945640563965
Epoch 930, training loss: 0.6565355658531189 = 0.008371477946639061 + 0.1 * 6.481640815734863
Epoch 930, val loss: 1.456373691558838
Epoch 940, training loss: 0.6567181348800659 = 0.008141557686030865 + 0.1 * 6.48576545715332
Epoch 940, val loss: 1.4623979330062866
Epoch 950, training loss: 0.6554701924324036 = 0.007922741584479809 + 0.1 * 6.475473880767822
Epoch 950, val loss: 1.4685924053192139
Epoch 960, training loss: 0.6544639468193054 = 0.007713318802416325 + 0.1 * 6.467506408691406
Epoch 960, val loss: 1.474730134010315
Epoch 970, training loss: 0.6564404964447021 = 0.007512540556490421 + 0.1 * 6.489279270172119
Epoch 970, val loss: 1.4804950952529907
Epoch 980, training loss: 0.6551387310028076 = 0.007320486009120941 + 0.1 * 6.478182315826416
Epoch 980, val loss: 1.4863845109939575
Epoch 990, training loss: 0.6547927260398865 = 0.007136588450521231 + 0.1 * 6.476561546325684
Epoch 990, val loss: 1.4919618368148804
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 2.7992453575134277 = 1.939564824104309 + 0.1 * 8.59680461883545
Epoch 0, val loss: 1.9417109489440918
Epoch 10, training loss: 2.7895941734313965 = 1.9299240112304688 + 0.1 * 8.596700668334961
Epoch 10, val loss: 1.9320719242095947
Epoch 20, training loss: 2.777653694152832 = 1.91803777217865 + 0.1 * 8.596159934997559
Epoch 20, val loss: 1.92003333568573
Epoch 30, training loss: 2.7607502937316895 = 1.901532769203186 + 0.1 * 8.59217643737793
Epoch 30, val loss: 1.9031982421875
Epoch 40, training loss: 2.7341747283935547 = 1.87748122215271 + 0.1 * 8.566933631896973
Epoch 40, val loss: 1.8789509534835815
Epoch 50, training loss: 2.688486099243164 = 1.8450543880462646 + 0.1 * 8.434317588806152
Epoch 50, val loss: 1.8476665019989014
Epoch 60, training loss: 2.6263937950134277 = 1.8101997375488281 + 0.1 * 8.161941528320312
Epoch 60, val loss: 1.816065788269043
Epoch 70, training loss: 2.560336112976074 = 1.7780933380126953 + 0.1 * 7.822426795959473
Epoch 70, val loss: 1.7874506711959839
Epoch 80, training loss: 2.482553005218506 = 1.7463679313659668 + 0.1 * 7.361851692199707
Epoch 80, val loss: 1.7581268548965454
Epoch 90, training loss: 2.421553611755371 = 1.70758056640625 + 0.1 * 7.139731407165527
Epoch 90, val loss: 1.723007321357727
Epoch 100, training loss: 2.36228609085083 = 1.6553170680999756 + 0.1 * 7.0696892738342285
Epoch 100, val loss: 1.6781057119369507
Epoch 110, training loss: 2.289098024368286 = 1.5882470607757568 + 0.1 * 7.008509635925293
Epoch 110, val loss: 1.62293541431427
Epoch 120, training loss: 2.207310914993286 = 1.5111238956451416 + 0.1 * 6.961869239807129
Epoch 120, val loss: 1.5602432489395142
Epoch 130, training loss: 2.1224982738494873 = 1.4287378787994385 + 0.1 * 6.937603950500488
Epoch 130, val loss: 1.493238925933838
Epoch 140, training loss: 2.0342135429382324 = 1.3421940803527832 + 0.1 * 6.920195579528809
Epoch 140, val loss: 1.4226371049880981
Epoch 150, training loss: 1.940251350402832 = 1.249850869178772 + 0.1 * 6.904004096984863
Epoch 150, val loss: 1.3470094203948975
Epoch 160, training loss: 1.8423845767974854 = 1.1532440185546875 + 0.1 * 6.89140510559082
Epoch 160, val loss: 1.2683961391448975
Epoch 170, training loss: 1.7420485019683838 = 1.0543848276138306 + 0.1 * 6.876636981964111
Epoch 170, val loss: 1.188849687576294
Epoch 180, training loss: 1.6446845531463623 = 0.9582017660140991 + 0.1 * 6.864828109741211
Epoch 180, val loss: 1.1125859022140503
Epoch 190, training loss: 1.5537011623382568 = 0.868268609046936 + 0.1 * 6.854325294494629
Epoch 190, val loss: 1.0426324605941772
Epoch 200, training loss: 1.4710023403167725 = 0.7856596112251282 + 0.1 * 6.853427410125732
Epoch 200, val loss: 0.9803354740142822
Epoch 210, training loss: 1.3957507610321045 = 0.7119455933570862 + 0.1 * 6.838052272796631
Epoch 210, val loss: 0.9272695183753967
Epoch 220, training loss: 1.3276442289352417 = 0.6452816724777222 + 0.1 * 6.823625564575195
Epoch 220, val loss: 0.8822287321090698
Epoch 230, training loss: 1.2660044431686401 = 0.5838896632194519 + 0.1 * 6.821147441864014
Epoch 230, val loss: 0.8438538312911987
Epoch 240, training loss: 1.2074497938156128 = 0.5275022387504578 + 0.1 * 6.79947566986084
Epoch 240, val loss: 0.8121948838233948
Epoch 250, training loss: 1.1558736562728882 = 0.47533100843429565 + 0.1 * 6.805426597595215
Epoch 250, val loss: 0.7863718271255493
Epoch 260, training loss: 1.1062583923339844 = 0.4283944368362427 + 0.1 * 6.778640270233154
Epoch 260, val loss: 0.7668009400367737
Epoch 270, training loss: 1.0631464719772339 = 0.38674476742744446 + 0.1 * 6.764016628265381
Epoch 270, val loss: 0.7529565691947937
Epoch 280, training loss: 1.026128888130188 = 0.3507322072982788 + 0.1 * 6.753966808319092
Epoch 280, val loss: 0.7445122003555298
Epoch 290, training loss: 0.9939463138580322 = 0.31976762413978577 + 0.1 * 6.741786479949951
Epoch 290, val loss: 0.7403199076652527
Epoch 300, training loss: 0.9652675986289978 = 0.2926443815231323 + 0.1 * 6.726232051849365
Epoch 300, val loss: 0.7391836643218994
Epoch 310, training loss: 0.9446808099746704 = 0.26840195059776306 + 0.1 * 6.762788772583008
Epoch 310, val loss: 0.7399459481239319
Epoch 320, training loss: 0.9196233153343201 = 0.24659763276576996 + 0.1 * 6.7302565574646
Epoch 320, val loss: 0.7419725656509399
Epoch 330, training loss: 0.8951940536499023 = 0.22625935077667236 + 0.1 * 6.689346790313721
Epoch 330, val loss: 0.74448162317276
Epoch 340, training loss: 0.8746599555015564 = 0.20673954486846924 + 0.1 * 6.679203987121582
Epoch 340, val loss: 0.7474357485771179
Epoch 350, training loss: 0.8604271411895752 = 0.1878245770931244 + 0.1 * 6.726025581359863
Epoch 350, val loss: 0.7504264712333679
Epoch 360, training loss: 0.8359012007713318 = 0.16979515552520752 + 0.1 * 6.661060333251953
Epoch 360, val loss: 0.7538678646087646
Epoch 370, training loss: 0.8183879852294922 = 0.15271636843681335 + 0.1 * 6.6567158699035645
Epoch 370, val loss: 0.7573380470275879
Epoch 380, training loss: 0.8031610250473022 = 0.1368795484304428 + 0.1 * 6.662814617156982
Epoch 380, val loss: 0.7614621520042419
Epoch 390, training loss: 0.786909282207489 = 0.1226612851023674 + 0.1 * 6.64247989654541
Epoch 390, val loss: 0.7664713859558105
Epoch 400, training loss: 0.7725762128829956 = 0.11000091582536697 + 0.1 * 6.625752925872803
Epoch 400, val loss: 0.7720671892166138
Epoch 410, training loss: 0.7638021111488342 = 0.0988272950053215 + 0.1 * 6.649747848510742
Epoch 410, val loss: 0.7785453200340271
Epoch 420, training loss: 0.7513559460639954 = 0.08909710496664047 + 0.1 * 6.622588634490967
Epoch 420, val loss: 0.7857712507247925
Epoch 430, training loss: 0.7416483759880066 = 0.08061423152685165 + 0.1 * 6.610341548919678
Epoch 430, val loss: 0.793447732925415
Epoch 440, training loss: 0.733381986618042 = 0.07321769744157791 + 0.1 * 6.601643085479736
Epoch 440, val loss: 0.80169677734375
Epoch 450, training loss: 0.727570652961731 = 0.06675923615694046 + 0.1 * 6.608114242553711
Epoch 450, val loss: 0.8101754784584045
Epoch 460, training loss: 0.7216891050338745 = 0.06110626831650734 + 0.1 * 6.605828285217285
Epoch 460, val loss: 0.8189665079116821
Epoch 470, training loss: 0.7150432467460632 = 0.05613458529114723 + 0.1 * 6.589086532592773
Epoch 470, val loss: 0.8278670310974121
Epoch 480, training loss: 0.7111078500747681 = 0.05174040049314499 + 0.1 * 6.593674182891846
Epoch 480, val loss: 0.8368110656738281
Epoch 490, training loss: 0.7054694890975952 = 0.04785219952464104 + 0.1 * 6.576172351837158
Epoch 490, val loss: 0.8457738161087036
Epoch 500, training loss: 0.7018937468528748 = 0.04439165070652962 + 0.1 * 6.575020790100098
Epoch 500, val loss: 0.8547603487968445
Epoch 510, training loss: 0.6981770396232605 = 0.041299693286418915 + 0.1 * 6.56877326965332
Epoch 510, val loss: 0.8635419011116028
Epoch 520, training loss: 0.6944456696510315 = 0.03853107616305351 + 0.1 * 6.559145927429199
Epoch 520, val loss: 0.8724824786186218
Epoch 530, training loss: 0.691840648651123 = 0.03602975234389305 + 0.1 * 6.558108806610107
Epoch 530, val loss: 0.8811464905738831
Epoch 540, training loss: 0.6896711587905884 = 0.033769574016332626 + 0.1 * 6.559016227722168
Epoch 540, val loss: 0.8897984027862549
Epoch 550, training loss: 0.6868653893470764 = 0.031724654138088226 + 0.1 * 6.551407337188721
Epoch 550, val loss: 0.8983935713768005
Epoch 560, training loss: 0.6847805380821228 = 0.029865222051739693 + 0.1 * 6.5491533279418945
Epoch 560, val loss: 0.9067376852035522
Epoch 570, training loss: 0.6817679405212402 = 0.02817220240831375 + 0.1 * 6.535957336425781
Epoch 570, val loss: 0.9149768352508545
Epoch 580, training loss: 0.6819989085197449 = 0.026622775942087173 + 0.1 * 6.5537614822387695
Epoch 580, val loss: 0.9230629205703735
Epoch 590, training loss: 0.6806970238685608 = 0.025202633813023567 + 0.1 * 6.554943561553955
Epoch 590, val loss: 0.9309927225112915
Epoch 600, training loss: 0.6768278479576111 = 0.023903775960206985 + 0.1 * 6.529240608215332
Epoch 600, val loss: 0.9388322830200195
Epoch 610, training loss: 0.6772834062576294 = 0.02270609326660633 + 0.1 * 6.545773029327393
Epoch 610, val loss: 0.9464387893676758
Epoch 620, training loss: 0.6740796566009521 = 0.021598929539322853 + 0.1 * 6.524806976318359
Epoch 620, val loss: 0.9539995193481445
Epoch 630, training loss: 0.6746603846549988 = 0.020574096590280533 + 0.1 * 6.540862560272217
Epoch 630, val loss: 0.9614260196685791
Epoch 640, training loss: 0.6713172197341919 = 0.019624250009655952 + 0.1 * 6.516929626464844
Epoch 640, val loss: 0.9686242938041687
Epoch 650, training loss: 0.6702559590339661 = 0.018742108717560768 + 0.1 * 6.515138626098633
Epoch 650, val loss: 0.975820004940033
Epoch 660, training loss: 0.6704750061035156 = 0.017919741570949554 + 0.1 * 6.525552749633789
Epoch 660, val loss: 0.9828152060508728
Epoch 670, training loss: 0.6680224537849426 = 0.017154881730675697 + 0.1 * 6.508675575256348
Epoch 670, val loss: 0.9896577000617981
Epoch 680, training loss: 0.6673814058303833 = 0.016441509127616882 + 0.1 * 6.509398937225342
Epoch 680, val loss: 0.9965164661407471
Epoch 690, training loss: 0.6667813062667847 = 0.015771744772791862 + 0.1 * 6.510095596313477
Epoch 690, val loss: 1.0029642581939697
Epoch 700, training loss: 0.6655632257461548 = 0.015147616155445576 + 0.1 * 6.50415563583374
Epoch 700, val loss: 1.0095057487487793
Epoch 710, training loss: 0.6657496690750122 = 0.014562319032847881 + 0.1 * 6.511873722076416
Epoch 710, val loss: 1.0161080360412598
Epoch 720, training loss: 0.6632261872291565 = 0.014011179096996784 + 0.1 * 6.492149829864502
Epoch 720, val loss: 1.022181749343872
Epoch 730, training loss: 0.6626293063163757 = 0.013493246398866177 + 0.1 * 6.491360664367676
Epoch 730, val loss: 1.0285253524780273
Epoch 740, training loss: 0.6645413041114807 = 0.013003834523260593 + 0.1 * 6.515374660491943
Epoch 740, val loss: 1.0345067977905273
Epoch 750, training loss: 0.6612461805343628 = 0.012544333934783936 + 0.1 * 6.48701810836792
Epoch 750, val loss: 1.0404653549194336
Epoch 760, training loss: 0.6606612801551819 = 0.012110305950045586 + 0.1 * 6.485509872436523
Epoch 760, val loss: 1.046420693397522
Epoch 770, training loss: 0.6597276926040649 = 0.011698753573000431 + 0.1 * 6.480288982391357
Epoch 770, val loss: 1.0520368814468384
Epoch 780, training loss: 0.659731388092041 = 0.011310339905321598 + 0.1 * 6.48421049118042
Epoch 780, val loss: 1.0579323768615723
Epoch 790, training loss: 0.6593632102012634 = 0.010941036976873875 + 0.1 * 6.484221935272217
Epoch 790, val loss: 1.0632914304733276
Epoch 800, training loss: 0.6580557823181152 = 0.010592405684292316 + 0.1 * 6.474633693695068
Epoch 800, val loss: 1.068934440612793
Epoch 810, training loss: 0.6590624451637268 = 0.01026065181940794 + 0.1 * 6.488018035888672
Epoch 810, val loss: 1.074149250984192
Epoch 820, training loss: 0.6578714847564697 = 0.00994683988392353 + 0.1 * 6.479246616363525
Epoch 820, val loss: 1.0794605016708374
Epoch 830, training loss: 0.6578966975212097 = 0.00964903924614191 + 0.1 * 6.482476711273193
Epoch 830, val loss: 1.0848640203475952
Epoch 840, training loss: 0.6559488773345947 = 0.009364528581500053 + 0.1 * 6.465843200683594
Epoch 840, val loss: 1.0897852182388306
Epoch 850, training loss: 0.6572690010070801 = 0.009094304405152798 + 0.1 * 6.481746673583984
Epoch 850, val loss: 1.0949304103851318
Epoch 860, training loss: 0.6550571322441101 = 0.008835295215249062 + 0.1 * 6.462217807769775
Epoch 860, val loss: 1.099859595298767
Epoch 870, training loss: 0.6554399132728577 = 0.008588646538555622 + 0.1 * 6.468512058258057
Epoch 870, val loss: 1.104798436164856
Epoch 880, training loss: 0.6556353569030762 = 0.008352791890501976 + 0.1 * 6.472825050354004
Epoch 880, val loss: 1.1095393896102905
Epoch 890, training loss: 0.654125452041626 = 0.00812789797782898 + 0.1 * 6.459975242614746
Epoch 890, val loss: 1.1140563488006592
Epoch 900, training loss: 0.6533104777336121 = 0.00791406910866499 + 0.1 * 6.4539642333984375
Epoch 900, val loss: 1.1189701557159424
Epoch 910, training loss: 0.6529033780097961 = 0.007708435878157616 + 0.1 * 6.451949119567871
Epoch 910, val loss: 1.1236083507537842
Epoch 920, training loss: 0.6535132527351379 = 0.007510800380259752 + 0.1 * 6.460024356842041
Epoch 920, val loss: 1.127895474433899
Epoch 930, training loss: 0.6524407267570496 = 0.007321836426854134 + 0.1 * 6.451189041137695
Epoch 930, val loss: 1.132204532623291
Epoch 940, training loss: 0.6531409621238708 = 0.007141378242522478 + 0.1 * 6.459996223449707
Epoch 940, val loss: 1.13689386844635
Epoch 950, training loss: 0.6517457365989685 = 0.0069672418758273125 + 0.1 * 6.447784900665283
Epoch 950, val loss: 1.1409392356872559
Epoch 960, training loss: 0.6526023745536804 = 0.006800999399274588 + 0.1 * 6.458013534545898
Epoch 960, val loss: 1.1452218294143677
Epoch 970, training loss: 0.6510883569717407 = 0.006641310174018145 + 0.1 * 6.444469928741455
Epoch 970, val loss: 1.1494632959365845
Epoch 980, training loss: 0.6514933705329895 = 0.006487704813480377 + 0.1 * 6.450056552886963
Epoch 980, val loss: 1.153709053993225
Epoch 990, training loss: 0.6514459848403931 = 0.006339282728731632 + 0.1 * 6.451066493988037
Epoch 990, val loss: 1.1576206684112549
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.7970479704797049
=== training gcn model ===
Epoch 0, training loss: 2.808544158935547 = 1.9488624334335327 + 0.1 * 8.596817016601562
Epoch 0, val loss: 1.9454072713851929
Epoch 10, training loss: 2.798370361328125 = 1.938698649406433 + 0.1 * 8.59671688079834
Epoch 10, val loss: 1.935479760169983
Epoch 20, training loss: 2.7859039306640625 = 1.9262828826904297 + 0.1 * 8.596211433410645
Epoch 20, val loss: 1.9229356050491333
Epoch 30, training loss: 2.768378973007202 = 1.9091559648513794 + 0.1 * 8.592230796813965
Epoch 30, val loss: 1.9052560329437256
Epoch 40, training loss: 2.7408447265625 = 1.8844150304794312 + 0.1 * 8.56429672241211
Epoch 40, val loss: 1.879694938659668
Epoch 50, training loss: 2.690485954284668 = 1.8508151769638062 + 0.1 * 8.396706581115723
Epoch 50, val loss: 1.8465269804000854
Epoch 60, training loss: 2.621428966522217 = 1.8139957189559937 + 0.1 * 8.074332237243652
Epoch 60, val loss: 1.8126747608184814
Epoch 70, training loss: 2.5411555767059326 = 1.7820591926574707 + 0.1 * 7.590962886810303
Epoch 70, val loss: 1.7853463888168335
Epoch 80, training loss: 2.471648931503296 = 1.7515146732330322 + 0.1 * 7.2013421058654785
Epoch 80, val loss: 1.7599226236343384
Epoch 90, training loss: 2.4186015129089355 = 1.7130805253982544 + 0.1 * 7.055210113525391
Epoch 90, val loss: 1.7277053594589233
Epoch 100, training loss: 2.35979962348938 = 1.6607671976089478 + 0.1 * 6.990323543548584
Epoch 100, val loss: 1.6830687522888184
Epoch 110, training loss: 2.2879467010498047 = 1.5925400257110596 + 0.1 * 6.954066753387451
Epoch 110, val loss: 1.6251392364501953
Epoch 120, training loss: 2.203126907348633 = 1.5106192827224731 + 0.1 * 6.925076961517334
Epoch 120, val loss: 1.5582242012023926
Epoch 130, training loss: 2.111797332763672 = 1.421354055404663 + 0.1 * 6.9044318199157715
Epoch 130, val loss: 1.4875805377960205
Epoch 140, training loss: 2.0176587104797363 = 1.3288315534591675 + 0.1 * 6.888271331787109
Epoch 140, val loss: 1.414994716644287
Epoch 150, training loss: 1.9227888584136963 = 1.2350560426712036 + 0.1 * 6.877328872680664
Epoch 150, val loss: 1.3436095714569092
Epoch 160, training loss: 1.829206943511963 = 1.1423629522323608 + 0.1 * 6.868439674377441
Epoch 160, val loss: 1.2740360498428345
Epoch 170, training loss: 1.7377362251281738 = 1.051743507385254 + 0.1 * 6.859927177429199
Epoch 170, val loss: 1.2071452140808105
Epoch 180, training loss: 1.6507551670074463 = 0.9655364751815796 + 0.1 * 6.852187156677246
Epoch 180, val loss: 1.1444584131240845
Epoch 190, training loss: 1.5698497295379639 = 0.8854897618293762 + 0.1 * 6.843599796295166
Epoch 190, val loss: 1.0861551761627197
Epoch 200, training loss: 1.4953900575637817 = 0.812108039855957 + 0.1 * 6.832819938659668
Epoch 200, val loss: 1.032384991645813
Epoch 210, training loss: 1.4282119274139404 = 0.7455639243125916 + 0.1 * 6.826479911804199
Epoch 210, val loss: 0.9835114479064941
Epoch 220, training loss: 1.3678429126739502 = 0.6865116953849792 + 0.1 * 6.81331205368042
Epoch 220, val loss: 0.94060218334198
Epoch 230, training loss: 1.313509464263916 = 0.63334059715271 + 0.1 * 6.8016886711120605
Epoch 230, val loss: 0.9029279947280884
Epoch 240, training loss: 1.2648730278015137 = 0.5846425294876099 + 0.1 * 6.802305221557617
Epoch 240, val loss: 0.8700817823410034
Epoch 250, training loss: 1.2196197509765625 = 0.5403063893318176 + 0.1 * 6.79313325881958
Epoch 250, val loss: 0.841995358467102
Epoch 260, training loss: 1.176255702972412 = 0.4988386929035187 + 0.1 * 6.774169921875
Epoch 260, val loss: 0.8177063465118408
Epoch 270, training loss: 1.135359764099121 = 0.45905405282974243 + 0.1 * 6.763057231903076
Epoch 270, val loss: 0.7961571216583252
Epoch 280, training loss: 1.0980967283248901 = 0.4204139709472656 + 0.1 * 6.776827335357666
Epoch 280, val loss: 0.7765212059020996
Epoch 290, training loss: 1.0578086376190186 = 0.38306725025177 + 0.1 * 6.747413635253906
Epoch 290, val loss: 0.7586687803268433
Epoch 300, training loss: 1.0227997303009033 = 0.3468109965324402 + 0.1 * 6.759886741638184
Epoch 300, val loss: 0.7420462965965271
Epoch 310, training loss: 0.9853142499923706 = 0.3121704161167145 + 0.1 * 6.731438159942627
Epoch 310, val loss: 0.7268824577331543
Epoch 320, training loss: 0.951356828212738 = 0.27919191122055054 + 0.1 * 6.721649169921875
Epoch 320, val loss: 0.7133238315582275
Epoch 330, training loss: 0.919504702091217 = 0.24813400208950043 + 0.1 * 6.713706970214844
Epoch 330, val loss: 0.7016827464103699
Epoch 340, training loss: 0.8905515074729919 = 0.21963243186473846 + 0.1 * 6.709190368652344
Epoch 340, val loss: 0.6922246217727661
Epoch 350, training loss: 0.8638560175895691 = 0.19400034844875336 + 0.1 * 6.698556423187256
Epoch 350, val loss: 0.6850873827934265
Epoch 360, training loss: 0.8408259749412537 = 0.17114301025867462 + 0.1 * 6.696829795837402
Epoch 360, val loss: 0.6801952123641968
Epoch 370, training loss: 0.8208633065223694 = 0.15114063024520874 + 0.1 * 6.697226524353027
Epoch 370, val loss: 0.6774911880493164
Epoch 380, training loss: 0.8017774820327759 = 0.1338287889957428 + 0.1 * 6.6794867515563965
Epoch 380, val loss: 0.6768320202827454
Epoch 390, training loss: 0.7860283255577087 = 0.11885811388492584 + 0.1 * 6.6717023849487305
Epoch 390, val loss: 0.6777541041374207
Epoch 400, training loss: 0.7741701602935791 = 0.10594717413187027 + 0.1 * 6.682229518890381
Epoch 400, val loss: 0.6800742149353027
Epoch 410, training loss: 0.7618795037269592 = 0.0948777049779892 + 0.1 * 6.670017719268799
Epoch 410, val loss: 0.6834001541137695
Epoch 420, training loss: 0.7516382932662964 = 0.08530201762914658 + 0.1 * 6.663362979888916
Epoch 420, val loss: 0.6876360774040222
Epoch 430, training loss: 0.742933452129364 = 0.07701627165079117 + 0.1 * 6.6591715812683105
Epoch 430, val loss: 0.6924727559089661
Epoch 440, training loss: 0.7344421744346619 = 0.0698113963007927 + 0.1 * 6.646307468414307
Epoch 440, val loss: 0.6978587508201599
Epoch 450, training loss: 0.7270615696907043 = 0.0635278970003128 + 0.1 * 6.635336875915527
Epoch 450, val loss: 0.7035269737243652
Epoch 460, training loss: 0.7226801514625549 = 0.05802527442574501 + 0.1 * 6.646548271179199
Epoch 460, val loss: 0.709549605846405
Epoch 470, training loss: 0.7163992524147034 = 0.05319555476307869 + 0.1 * 6.632037162780762
Epoch 470, val loss: 0.7156233787536621
Epoch 480, training loss: 0.7109163403511047 = 0.04893328621983528 + 0.1 * 6.61983060836792
Epoch 480, val loss: 0.7218878865242004
Epoch 490, training loss: 0.7078073024749756 = 0.04514586552977562 + 0.1 * 6.626614093780518
Epoch 490, val loss: 0.7282161116600037
Epoch 500, training loss: 0.7031757831573486 = 0.04178417846560478 + 0.1 * 6.613915920257568
Epoch 500, val loss: 0.7345389127731323
Epoch 510, training loss: 0.6995518803596497 = 0.03878462314605713 + 0.1 * 6.607672691345215
Epoch 510, val loss: 0.7408627271652222
Epoch 520, training loss: 0.695954442024231 = 0.036099955439567566 + 0.1 * 6.598544597625732
Epoch 520, val loss: 0.7471616864204407
Epoch 530, training loss: 0.6925843954086304 = 0.03369010612368584 + 0.1 * 6.588943004608154
Epoch 530, val loss: 0.753416895866394
Epoch 540, training loss: 0.6948180198669434 = 0.0315135158598423 + 0.1 * 6.633044719696045
Epoch 540, val loss: 0.7595962285995483
Epoch 550, training loss: 0.6888396143913269 = 0.02955605648458004 + 0.1 * 6.592835903167725
Epoch 550, val loss: 0.7655466198921204
Epoch 560, training loss: 0.6858658194541931 = 0.02778252214193344 + 0.1 * 6.5808329582214355
Epoch 560, val loss: 0.7715818881988525
Epoch 570, training loss: 0.6841316223144531 = 0.026164498180150986 + 0.1 * 6.579671382904053
Epoch 570, val loss: 0.7774609923362732
Epoch 580, training loss: 0.6826453804969788 = 0.02468840777873993 + 0.1 * 6.579569339752197
Epoch 580, val loss: 0.7832276821136475
Epoch 590, training loss: 0.680435299873352 = 0.023338843137025833 + 0.1 * 6.570964336395264
Epoch 590, val loss: 0.7889553308486938
Epoch 600, training loss: 0.6788634657859802 = 0.02210150845348835 + 0.1 * 6.567619323730469
Epoch 600, val loss: 0.7945355176925659
Epoch 610, training loss: 0.6761699914932251 = 0.020965641364455223 + 0.1 * 6.552043437957764
Epoch 610, val loss: 0.7999730706214905
Epoch 620, training loss: 0.6752008199691772 = 0.019921110942959785 + 0.1 * 6.552796840667725
Epoch 620, val loss: 0.8054118156433105
Epoch 630, training loss: 0.6755015850067139 = 0.01895432360470295 + 0.1 * 6.56547212600708
Epoch 630, val loss: 0.8107177019119263
Epoch 640, training loss: 0.673666775226593 = 0.018063466995954514 + 0.1 * 6.556032657623291
Epoch 640, val loss: 0.8158645033836365
Epoch 650, training loss: 0.6717969179153442 = 0.017238110303878784 + 0.1 * 6.54558801651001
Epoch 650, val loss: 0.8209643363952637
Epoch 660, training loss: 0.6723925471305847 = 0.016470152884721756 + 0.1 * 6.5592241287231445
Epoch 660, val loss: 0.8259850740432739
Epoch 670, training loss: 0.669797956943512 = 0.01575629599392414 + 0.1 * 6.540416240692139
Epoch 670, val loss: 0.8308801054954529
Epoch 680, training loss: 0.6707359552383423 = 0.015091108158230782 + 0.1 * 6.556448459625244
Epoch 680, val loss: 0.8357488512992859
Epoch 690, training loss: 0.6687086820602417 = 0.014470220543444157 + 0.1 * 6.542384624481201
Epoch 690, val loss: 0.840416431427002
Epoch 700, training loss: 0.6676141023635864 = 0.013891149312257767 + 0.1 * 6.537229061126709
Epoch 700, val loss: 0.8450927138328552
Epoch 710, training loss: 0.6660964488983154 = 0.013347432017326355 + 0.1 * 6.527490139007568
Epoch 710, val loss: 0.849662184715271
Epoch 720, training loss: 0.6653158068656921 = 0.012838785536587238 + 0.1 * 6.524770259857178
Epoch 720, val loss: 0.8541681170463562
Epoch 730, training loss: 0.6673173308372498 = 0.012359769083559513 + 0.1 * 6.549575328826904
Epoch 730, val loss: 0.858612596988678
Epoch 740, training loss: 0.6632576584815979 = 0.011909648776054382 + 0.1 * 6.513479709625244
Epoch 740, val loss: 0.862895667552948
Epoch 750, training loss: 0.6635517477989197 = 0.011485983617603779 + 0.1 * 6.520657539367676
Epoch 750, val loss: 0.867209255695343
Epoch 760, training loss: 0.6618595123291016 = 0.01108582690358162 + 0.1 * 6.507737159729004
Epoch 760, val loss: 0.8713794350624084
Epoch 770, training loss: 0.6635211706161499 = 0.010708827525377274 + 0.1 * 6.528123378753662
Epoch 770, val loss: 0.8754950165748596
Epoch 780, training loss: 0.6621148586273193 = 0.010352455079555511 + 0.1 * 6.517624378204346
Epoch 780, val loss: 0.879533588886261
Epoch 790, training loss: 0.6605737805366516 = 0.01001636404544115 + 0.1 * 6.505573749542236
Epoch 790, val loss: 0.8835328817367554
Epoch 800, training loss: 0.6633722186088562 = 0.009696859866380692 + 0.1 * 6.5367536544799805
Epoch 800, val loss: 0.887495219707489
Epoch 810, training loss: 0.6600797176361084 = 0.009393981657922268 + 0.1 * 6.506857395172119
Epoch 810, val loss: 0.891261875629425
Epoch 820, training loss: 0.6589993238449097 = 0.009107755497097969 + 0.1 * 6.498915195465088
Epoch 820, val loss: 0.8951232433319092
Epoch 830, training loss: 0.6582493782043457 = 0.008834202773869038 + 0.1 * 6.494152069091797
Epoch 830, val loss: 0.8988646864891052
Epoch 840, training loss: 0.657864511013031 = 0.008574053645133972 + 0.1 * 6.492904186248779
Epoch 840, val loss: 0.90251624584198
Epoch 850, training loss: 0.659295380115509 = 0.00832708552479744 + 0.1 * 6.509683132171631
Epoch 850, val loss: 0.9061260223388672
Epoch 860, training loss: 0.657443106174469 = 0.008092203177511692 + 0.1 * 6.493508815765381
Epoch 860, val loss: 0.9096855521202087
Epoch 870, training loss: 0.6567798852920532 = 0.007868253625929356 + 0.1 * 6.489116191864014
Epoch 870, val loss: 0.9132055640220642
Epoch 880, training loss: 0.6566610932350159 = 0.007653959095478058 + 0.1 * 6.4900712966918945
Epoch 880, val loss: 0.9166755080223083
Epoch 890, training loss: 0.6558307409286499 = 0.007449628785252571 + 0.1 * 6.483810901641846
Epoch 890, val loss: 0.9200354218482971
Epoch 900, training loss: 0.6566309928894043 = 0.007254881784319878 + 0.1 * 6.493760585784912
Epoch 900, val loss: 0.9234069585800171
Epoch 910, training loss: 0.6552445888519287 = 0.007068017963320017 + 0.1 * 6.481765270233154
Epoch 910, val loss: 0.9267451763153076
Epoch 920, training loss: 0.6545746922492981 = 0.0068893833085894585 + 0.1 * 6.476852893829346
Epoch 920, val loss: 0.9299981594085693
Epoch 930, training loss: 0.6553764939308167 = 0.00671814801171422 + 0.1 * 6.486583232879639
Epoch 930, val loss: 0.933208167552948
Epoch 940, training loss: 0.65325528383255 = 0.0065542240627110004 + 0.1 * 6.467010021209717
Epoch 940, val loss: 0.9363552927970886
Epoch 950, training loss: 0.6547476649284363 = 0.006397122982889414 + 0.1 * 6.4835052490234375
Epoch 950, val loss: 0.9395032525062561
Epoch 960, training loss: 0.6544512510299683 = 0.006245752330869436 + 0.1 * 6.482055187225342
Epoch 960, val loss: 0.9425339102745056
Epoch 970, training loss: 0.652755081653595 = 0.006100838538259268 + 0.1 * 6.4665422439575195
Epoch 970, val loss: 0.9455719590187073
Epoch 980, training loss: 0.6541318893432617 = 0.005961822345852852 + 0.1 * 6.481700420379639
Epoch 980, val loss: 0.9485704302787781
Epoch 990, training loss: 0.6523086428642273 = 0.005827909801155329 + 0.1 * 6.464807510375977
Epoch 990, val loss: 0.9514832496643066
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8017923036373221
The final CL Acc:0.75432, 0.01492, The final GNN Acc:0.80250, 0.00476
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13248])
remove edge: torch.Size([2, 7948])
updated graph: torch.Size([2, 10640])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.824680805206299 = 1.965002417564392 + 0.1 * 8.596783638000488
Epoch 0, val loss: 1.9662898778915405
Epoch 10, training loss: 2.813621759414673 = 1.9539592266082764 + 0.1 * 8.596624374389648
Epoch 10, val loss: 1.9551423788070679
Epoch 20, training loss: 2.7997617721557617 = 1.9402055740356445 + 0.1 * 8.595562934875488
Epoch 20, val loss: 1.941105842590332
Epoch 30, training loss: 2.779388904571533 = 1.92068612575531 + 0.1 * 8.587027549743652
Epoch 30, val loss: 1.9210035800933838
Epoch 40, training loss: 2.7453322410583496 = 1.8915939331054688 + 0.1 * 8.537383079528809
Epoch 40, val loss: 1.8912482261657715
Epoch 50, training loss: 2.679849147796631 = 1.8518034219741821 + 0.1 * 8.280457496643066
Epoch 50, val loss: 1.8522191047668457
Epoch 60, training loss: 2.6071088314056396 = 1.8077110052108765 + 0.1 * 7.993978977203369
Epoch 60, val loss: 1.8123396635055542
Epoch 70, training loss: 2.526252508163452 = 1.769676923751831 + 0.1 * 7.565755367279053
Epoch 70, val loss: 1.7799575328826904
Epoch 80, training loss: 2.454252004623413 = 1.7339224815368652 + 0.1 * 7.203295707702637
Epoch 80, val loss: 1.748468279838562
Epoch 90, training loss: 2.392270565032959 = 1.6884244680404663 + 0.1 * 7.038462162017822
Epoch 90, val loss: 1.7080668210983276
Epoch 100, training loss: 2.3251993656158447 = 1.6268091201782227 + 0.1 * 6.983901500701904
Epoch 100, val loss: 1.654114007949829
Epoch 110, training loss: 2.2440004348754883 = 1.5479390621185303 + 0.1 * 6.960612773895264
Epoch 110, val loss: 1.5868488550186157
Epoch 120, training loss: 2.152250051498413 = 1.4576294422149658 + 0.1 * 6.946205139160156
Epoch 120, val loss: 1.5116947889328003
Epoch 130, training loss: 2.058126449584961 = 1.3642916679382324 + 0.1 * 6.9383463859558105
Epoch 130, val loss: 1.4356549978256226
Epoch 140, training loss: 1.963346004486084 = 1.270140528678894 + 0.1 * 6.9320549964904785
Epoch 140, val loss: 1.359438180923462
Epoch 150, training loss: 1.8651063442230225 = 1.1726031303405762 + 0.1 * 6.9250311851501465
Epoch 150, val loss: 1.281383991241455
Epoch 160, training loss: 1.7631897926330566 = 1.0713543891906738 + 0.1 * 6.91835355758667
Epoch 160, val loss: 1.2006316184997559
Epoch 170, training loss: 1.6616443395614624 = 0.9708874821662903 + 0.1 * 6.907568454742432
Epoch 170, val loss: 1.1214402914047241
Epoch 180, training loss: 1.56534743309021 = 0.8759907484054565 + 0.1 * 6.893566131591797
Epoch 180, val loss: 1.047186017036438
Epoch 190, training loss: 1.4790304899215698 = 0.7907466292381287 + 0.1 * 6.882838249206543
Epoch 190, val loss: 0.9813971519470215
Epoch 200, training loss: 1.4038560390472412 = 0.7178180813789368 + 0.1 * 6.860378742218018
Epoch 200, val loss: 0.9272385239601135
Epoch 210, training loss: 1.3399240970611572 = 0.6558575630187988 + 0.1 * 6.840664386749268
Epoch 210, val loss: 0.8843722939491272
Epoch 220, training loss: 1.2846217155456543 = 0.6025328040122986 + 0.1 * 6.820888519287109
Epoch 220, val loss: 0.8513928651809692
Epoch 230, training loss: 1.2362086772918701 = 0.5557668209075928 + 0.1 * 6.804418563842773
Epoch 230, val loss: 0.8263145089149475
Epoch 240, training loss: 1.193131446838379 = 0.514430582523346 + 0.1 * 6.787008285522461
Epoch 240, val loss: 0.8071736693382263
Epoch 250, training loss: 1.1540478467941284 = 0.4766651690006256 + 0.1 * 6.773826599121094
Epoch 250, val loss: 0.7919943928718567
Epoch 260, training loss: 1.1174694299697876 = 0.44167396426200867 + 0.1 * 6.757955074310303
Epoch 260, val loss: 0.779794454574585
Epoch 270, training loss: 1.0827081203460693 = 0.40860089659690857 + 0.1 * 6.741072177886963
Epoch 270, val loss: 0.7698315978050232
Epoch 280, training loss: 1.050169587135315 = 0.3765524923801422 + 0.1 * 6.736170768737793
Epoch 280, val loss: 0.761579155921936
Epoch 290, training loss: 1.0174405574798584 = 0.3451921045780182 + 0.1 * 6.722484588623047
Epoch 290, val loss: 0.7547959089279175
Epoch 300, training loss: 0.9858469367027283 = 0.3145418167114258 + 0.1 * 6.713050842285156
Epoch 300, val loss: 0.7491863965988159
Epoch 310, training loss: 0.9545400142669678 = 0.28455850481987 + 0.1 * 6.699815273284912
Epoch 310, val loss: 0.745074450969696
Epoch 320, training loss: 0.9259433746337891 = 0.2556612193584442 + 0.1 * 6.702821731567383
Epoch 320, val loss: 0.7426401376724243
Epoch 330, training loss: 0.8981577754020691 = 0.22858239710330963 + 0.1 * 6.695753574371338
Epoch 330, val loss: 0.7415097951889038
Epoch 340, training loss: 0.8713350892066956 = 0.20345056056976318 + 0.1 * 6.678844928741455
Epoch 340, val loss: 0.7421489953994751
Epoch 350, training loss: 0.8490892648696899 = 0.1805097609758377 + 0.1 * 6.685794830322266
Epoch 350, val loss: 0.7444238066673279
Epoch 360, training loss: 0.8277222514152527 = 0.1600085347890854 + 0.1 * 6.6771368980407715
Epoch 360, val loss: 0.7482557892799377
Epoch 370, training loss: 0.8077283501625061 = 0.1419091820716858 + 0.1 * 6.658191680908203
Epoch 370, val loss: 0.7536264061927795
Epoch 380, training loss: 0.7912001013755798 = 0.12595145404338837 + 0.1 * 6.652486324310303
Epoch 380, val loss: 0.7603253126144409
Epoch 390, training loss: 0.779862642288208 = 0.11191349476575851 + 0.1 * 6.67949104309082
Epoch 390, val loss: 0.7681980729103088
Epoch 400, training loss: 0.7647616863250732 = 0.09968698024749756 + 0.1 * 6.650746822357178
Epoch 400, val loss: 0.77684086561203
Epoch 410, training loss: 0.753688633441925 = 0.0890217199921608 + 0.1 * 6.646668910980225
Epoch 410, val loss: 0.7862164378166199
Epoch 420, training loss: 0.7431638836860657 = 0.07975876331329346 + 0.1 * 6.634051322937012
Epoch 420, val loss: 0.7959616184234619
Epoch 430, training loss: 0.7342167496681213 = 0.0717088058590889 + 0.1 * 6.625079154968262
Epoch 430, val loss: 0.8060473203659058
Epoch 440, training loss: 0.7283203601837158 = 0.06470710039138794 + 0.1 * 6.63613224029541
Epoch 440, val loss: 0.8162955641746521
Epoch 450, training loss: 0.720698356628418 = 0.05863599106669426 + 0.1 * 6.620623588562012
Epoch 450, val loss: 0.8263807892799377
Epoch 460, training loss: 0.7164931297302246 = 0.053336210548877716 + 0.1 * 6.631568908691406
Epoch 460, val loss: 0.8365737795829773
Epoch 470, training loss: 0.709296464920044 = 0.04871454834938049 + 0.1 * 6.605819225311279
Epoch 470, val loss: 0.8466021418571472
Epoch 480, training loss: 0.70474773645401 = 0.04465574026107788 + 0.1 * 6.600919723510742
Epoch 480, val loss: 0.8565963506698608
Epoch 490, training loss: 0.7021656036376953 = 0.0410911999642849 + 0.1 * 6.610743999481201
Epoch 490, val loss: 0.8664054870605469
Epoch 500, training loss: 0.6971220374107361 = 0.03795444965362549 + 0.1 * 6.591675758361816
Epoch 500, val loss: 0.8758437633514404
Epoch 510, training loss: 0.6930181384086609 = 0.03516794741153717 + 0.1 * 6.578502178192139
Epoch 510, val loss: 0.8852618932723999
Epoch 520, training loss: 0.6953197717666626 = 0.032680198550224304 + 0.1 * 6.6263957023620605
Epoch 520, val loss: 0.8945289254188538
Epoch 530, training loss: 0.6875601410865784 = 0.030466150492429733 + 0.1 * 6.570940017700195
Epoch 530, val loss: 0.9034565687179565
Epoch 540, training loss: 0.6865637898445129 = 0.028480052947998047 + 0.1 * 6.580837249755859
Epoch 540, val loss: 0.9121488928794861
Epoch 550, training loss: 0.6847330927848816 = 0.02669443190097809 + 0.1 * 6.580386638641357
Epoch 550, val loss: 0.9206361174583435
Epoch 560, training loss: 0.6807281970977783 = 0.025084102526307106 + 0.1 * 6.556441307067871
Epoch 560, val loss: 0.9287983179092407
Epoch 570, training loss: 0.6796656847000122 = 0.023620350286364555 + 0.1 * 6.560452938079834
Epoch 570, val loss: 0.9369262456893921
Epoch 580, training loss: 0.6782262921333313 = 0.02229331061244011 + 0.1 * 6.559329509735107
Epoch 580, val loss: 0.9446662068367004
Epoch 590, training loss: 0.6758889555931091 = 0.021083155646920204 + 0.1 * 6.548057556152344
Epoch 590, val loss: 0.9522792100906372
Epoch 600, training loss: 0.6754235625267029 = 0.019977344200015068 + 0.1 * 6.554462432861328
Epoch 600, val loss: 0.9596095085144043
Epoch 610, training loss: 0.6729037761688232 = 0.01896260306239128 + 0.1 * 6.539411544799805
Epoch 610, val loss: 0.9668872952461243
Epoch 620, training loss: 0.6725172400474548 = 0.018030326813459396 + 0.1 * 6.5448689460754395
Epoch 620, val loss: 0.9739135503768921
Epoch 630, training loss: 0.6719183921813965 = 0.017169469967484474 + 0.1 * 6.547489166259766
Epoch 630, val loss: 0.9808141589164734
Epoch 640, training loss: 0.6687864661216736 = 0.016376467421650887 + 0.1 * 6.524099349975586
Epoch 640, val loss: 0.9875064492225647
Epoch 650, training loss: 0.6682477593421936 = 0.015642259269952774 + 0.1 * 6.526054859161377
Epoch 650, val loss: 0.9940204620361328
Epoch 660, training loss: 0.6672383546829224 = 0.014959802851080894 + 0.1 * 6.522785186767578
Epoch 660, val loss: 1.0004808902740479
Epoch 670, training loss: 0.6672916412353516 = 0.014327039010822773 + 0.1 * 6.529645919799805
Epoch 670, val loss: 1.0066807270050049
Epoch 680, training loss: 0.6648606061935425 = 0.013737487606704235 + 0.1 * 6.511230945587158
Epoch 680, val loss: 1.0127098560333252
Epoch 690, training loss: 0.6642033457756042 = 0.013188735581934452 + 0.1 * 6.510146141052246
Epoch 690, val loss: 1.018553614616394
Epoch 700, training loss: 0.6635763049125671 = 0.012673741206526756 + 0.1 * 6.509025573730469
Epoch 700, val loss: 1.0243775844573975
Epoch 710, training loss: 0.6627110242843628 = 0.012192010879516602 + 0.1 * 6.505189895629883
Epoch 710, val loss: 1.03001868724823
Epoch 720, training loss: 0.6615982055664062 = 0.011739450506865978 + 0.1 * 6.498587608337402
Epoch 720, val loss: 1.0355535745620728
Epoch 730, training loss: 0.6631259322166443 = 0.011314082890748978 + 0.1 * 6.518118381500244
Epoch 730, val loss: 1.040950059890747
Epoch 740, training loss: 0.6611062288284302 = 0.010914583690464497 + 0.1 * 6.501916408538818
Epoch 740, val loss: 1.0462064743041992
Epoch 750, training loss: 0.6604130864143372 = 0.010540135204792023 + 0.1 * 6.498729228973389
Epoch 750, val loss: 1.0512853860855103
Epoch 760, training loss: 0.6597568988800049 = 0.010185040533542633 + 0.1 * 6.495718479156494
Epoch 760, val loss: 1.0563874244689941
Epoch 770, training loss: 0.6578836441040039 = 0.009850336238741875 + 0.1 * 6.480332851409912
Epoch 770, val loss: 1.061440110206604
Epoch 780, training loss: 0.6578065156936646 = 0.00953490100800991 + 0.1 * 6.482716083526611
Epoch 780, val loss: 1.0661494731903076
Epoch 790, training loss: 0.6564487218856812 = 0.009235301986336708 + 0.1 * 6.472134113311768
Epoch 790, val loss: 1.070897102355957
Epoch 800, training loss: 0.6581563949584961 = 0.008949756622314453 + 0.1 * 6.492066383361816
Epoch 800, val loss: 1.075684666633606
Epoch 810, training loss: 0.6567167043685913 = 0.008679087273776531 + 0.1 * 6.480376243591309
Epoch 810, val loss: 1.0803297758102417
Epoch 820, training loss: 0.6560116410255432 = 0.008422479964792728 + 0.1 * 6.47589111328125
Epoch 820, val loss: 1.0848581790924072
Epoch 830, training loss: 0.6553074717521667 = 0.008180007338523865 + 0.1 * 6.471274375915527
Epoch 830, val loss: 1.0891717672348022
Epoch 840, training loss: 0.6551570296287537 = 0.007948821410536766 + 0.1 * 6.472082138061523
Epoch 840, val loss: 1.0934263467788696
Epoch 850, training loss: 0.6541590690612793 = 0.007727456279098988 + 0.1 * 6.464316368103027
Epoch 850, val loss: 1.0978813171386719
Epoch 860, training loss: 0.6536554098129272 = 0.007517572026699781 + 0.1 * 6.46137809753418
Epoch 860, val loss: 1.1019686460494995
Epoch 870, training loss: 0.6527513861656189 = 0.007316833361983299 + 0.1 * 6.454345226287842
Epoch 870, val loss: 1.1060229539871216
Epoch 880, training loss: 0.6535511612892151 = 0.007124158088117838 + 0.1 * 6.464269638061523
Epoch 880, val loss: 1.110184669494629
Epoch 890, training loss: 0.6529598236083984 = 0.006940288469195366 + 0.1 * 6.460195064544678
Epoch 890, val loss: 1.1142873764038086
Epoch 900, training loss: 0.6528736352920532 = 0.006765768397599459 + 0.1 * 6.46107816696167
Epoch 900, val loss: 1.1180888414382935
Epoch 910, training loss: 0.6513094305992126 = 0.006598189938813448 + 0.1 * 6.447112560272217
Epoch 910, val loss: 1.1219433546066284
Epoch 920, training loss: 0.6524157524108887 = 0.00643738592043519 + 0.1 * 6.459784030914307
Epoch 920, val loss: 1.1257776021957397
Epoch 930, training loss: 0.650757908821106 = 0.006282762158662081 + 0.1 * 6.444751739501953
Epoch 930, val loss: 1.129488229751587
Epoch 940, training loss: 0.6508564352989197 = 0.006134318187832832 + 0.1 * 6.447220802307129
Epoch 940, val loss: 1.1332684755325317
Epoch 950, training loss: 0.6513849496841431 = 0.005992639344185591 + 0.1 * 6.453922748565674
Epoch 950, val loss: 1.1367613077163696
Epoch 960, training loss: 0.6495376229286194 = 0.005856368225067854 + 0.1 * 6.436811923980713
Epoch 960, val loss: 1.1403647661209106
Epoch 970, training loss: 0.6500945091247559 = 0.005726170726120472 + 0.1 * 6.443683624267578
Epoch 970, val loss: 1.1437126398086548
Epoch 980, training loss: 0.649590253829956 = 0.005600057542324066 + 0.1 * 6.439901828765869
Epoch 980, val loss: 1.1471437215805054
Epoch 990, training loss: 0.6494489908218384 = 0.005478627048432827 + 0.1 * 6.439703464508057
Epoch 990, val loss: 1.1505153179168701
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 2.800293207168579 = 1.9406124353408813 + 0.1 * 8.596807479858398
Epoch 0, val loss: 1.9328144788742065
Epoch 10, training loss: 2.790135145187378 = 1.930466890335083 + 0.1 * 8.59668254852295
Epoch 10, val loss: 1.9231292009353638
Epoch 20, training loss: 2.777682304382324 = 1.9180901050567627 + 0.1 * 8.595922470092773
Epoch 20, val loss: 1.9110816717147827
Epoch 30, training loss: 2.7600150108337402 = 1.9010651111602783 + 0.1 * 8.589499473571777
Epoch 30, val loss: 1.8941236734390259
Epoch 40, training loss: 2.730949640274048 = 1.876501202583313 + 0.1 * 8.544485092163086
Epoch 40, val loss: 1.8696670532226562
Epoch 50, training loss: 2.6730239391326904 = 1.8436496257781982 + 0.1 * 8.293742179870605
Epoch 50, val loss: 1.8379466533660889
Epoch 60, training loss: 2.610687494277954 = 1.8079688549041748 + 0.1 * 8.027186393737793
Epoch 60, val loss: 1.805436372756958
Epoch 70, training loss: 2.529153823852539 = 1.774566888809204 + 0.1 * 7.545867919921875
Epoch 70, val loss: 1.775647521018982
Epoch 80, training loss: 2.45361065864563 = 1.7382451295852661 + 0.1 * 7.153655052185059
Epoch 80, val loss: 1.743919849395752
Epoch 90, training loss: 2.392824649810791 = 1.6906803846359253 + 0.1 * 7.021442413330078
Epoch 90, val loss: 1.7026441097259521
Epoch 100, training loss: 2.3220181465148926 = 1.6262645721435547 + 0.1 * 6.957534313201904
Epoch 100, val loss: 1.645320177078247
Epoch 110, training loss: 2.236680507659912 = 1.544934868812561 + 0.1 * 6.91745662689209
Epoch 110, val loss: 1.573521375656128
Epoch 120, training loss: 2.1407229900360107 = 1.4518948793411255 + 0.1 * 6.888280868530273
Epoch 120, val loss: 1.4937124252319336
Epoch 130, training loss: 2.038794994354248 = 1.3524961471557617 + 0.1 * 6.862987518310547
Epoch 130, val loss: 1.4091360569000244
Epoch 140, training loss: 1.937121033668518 = 1.2526001930236816 + 0.1 * 6.845208168029785
Epoch 140, val loss: 1.326977014541626
Epoch 150, training loss: 1.8428537845611572 = 1.16011381149292 + 0.1 * 6.827399253845215
Epoch 150, val loss: 1.2524147033691406
Epoch 160, training loss: 1.7586116790771484 = 1.0773643255233765 + 0.1 * 6.812473297119141
Epoch 160, val loss: 1.186745524406433
Epoch 170, training loss: 1.6862528324127197 = 1.0065135955810547 + 0.1 * 6.797392845153809
Epoch 170, val loss: 1.132736086845398
Epoch 180, training loss: 1.6239323616027832 = 0.9455252289772034 + 0.1 * 6.784071445465088
Epoch 180, val loss: 1.0885732173919678
Epoch 190, training loss: 1.568340539932251 = 0.8905040621757507 + 0.1 * 6.778364181518555
Epoch 190, val loss: 1.0510103702545166
Epoch 200, training loss: 1.5163185596466064 = 0.8400247097015381 + 0.1 * 6.762937545776367
Epoch 200, val loss: 1.018429160118103
Epoch 210, training loss: 1.4674534797668457 = 0.7918722629547119 + 0.1 * 6.75581169128418
Epoch 210, val loss: 0.9894183278083801
Epoch 220, training loss: 1.4190270900726318 = 0.7444812655448914 + 0.1 * 6.745457649230957
Epoch 220, val loss: 0.9621203541755676
Epoch 230, training loss: 1.3706989288330078 = 0.6968355178833008 + 0.1 * 6.738633155822754
Epoch 230, val loss: 0.9352878928184509
Epoch 240, training loss: 1.3220694065093994 = 0.6488681435585022 + 0.1 * 6.732012748718262
Epoch 240, val loss: 0.9079760313034058
Epoch 250, training loss: 1.2732324600219727 = 0.6005509495735168 + 0.1 * 6.7268147468566895
Epoch 250, val loss: 0.8800853490829468
Epoch 260, training loss: 1.2238736152648926 = 0.5517587065696716 + 0.1 * 6.721148490905762
Epoch 260, val loss: 0.8512340784072876
Epoch 270, training loss: 1.1752318143844604 = 0.5031623244285583 + 0.1 * 6.7206950187683105
Epoch 270, val loss: 0.8221330046653748
Epoch 280, training loss: 1.1275047063827515 = 0.456137478351593 + 0.1 * 6.713672161102295
Epoch 280, val loss: 0.7937131524085999
Epoch 290, training loss: 1.0817641019821167 = 0.4112544059753418 + 0.1 * 6.70509672164917
Epoch 290, val loss: 0.7668716907501221
Epoch 300, training loss: 1.0388401746749878 = 0.36901748180389404 + 0.1 * 6.6982269287109375
Epoch 300, val loss: 0.7422904968261719
Epoch 310, training loss: 0.9995709657669067 = 0.3300516903400421 + 0.1 * 6.695192813873291
Epoch 310, val loss: 0.7204661965370178
Epoch 320, training loss: 0.9635010957717896 = 0.2942439019680023 + 0.1 * 6.692571640014648
Epoch 320, val loss: 0.7014046907424927
Epoch 330, training loss: 0.9300031661987305 = 0.2617183029651642 + 0.1 * 6.68284797668457
Epoch 330, val loss: 0.6852671504020691
Epoch 340, training loss: 0.8996171355247498 = 0.2324058562517166 + 0.1 * 6.672112941741943
Epoch 340, val loss: 0.6719743013381958
Epoch 350, training loss: 0.8737735152244568 = 0.20633520185947418 + 0.1 * 6.674383163452148
Epoch 350, val loss: 0.661411464214325
Epoch 360, training loss: 0.8500247001647949 = 0.18338339030742645 + 0.1 * 6.666412830352783
Epoch 360, val loss: 0.653454601764679
Epoch 370, training loss: 0.8280370235443115 = 0.16323095560073853 + 0.1 * 6.648060321807861
Epoch 370, val loss: 0.6477909684181213
Epoch 380, training loss: 0.8117749691009521 = 0.1455421894788742 + 0.1 * 6.662327289581299
Epoch 380, val loss: 0.6441885232925415
Epoch 390, training loss: 0.7939469814300537 = 0.13012559711933136 + 0.1 * 6.638213634490967
Epoch 390, val loss: 0.6423207521438599
Epoch 400, training loss: 0.7808146476745605 = 0.11660771816968918 + 0.1 * 6.642068862915039
Epoch 400, val loss: 0.6420027613639832
Epoch 410, training loss: 0.7668486833572388 = 0.10482946038246155 + 0.1 * 6.620192050933838
Epoch 410, val loss: 0.6428613066673279
Epoch 420, training loss: 0.755977988243103 = 0.09450766444206238 + 0.1 * 6.6147027015686035
Epoch 420, val loss: 0.6447441577911377
Epoch 430, training loss: 0.746806800365448 = 0.08542034775018692 + 0.1 * 6.613863945007324
Epoch 430, val loss: 0.6474705934524536
Epoch 440, training loss: 0.7377395033836365 = 0.07741308957338333 + 0.1 * 6.603264331817627
Epoch 440, val loss: 0.6508650183677673
Epoch 450, training loss: 0.7297544479370117 = 0.07033155113458633 + 0.1 * 6.594228744506836
Epoch 450, val loss: 0.654824435710907
Epoch 460, training loss: 0.7241778373718262 = 0.06407219171524048 + 0.1 * 6.601056098937988
Epoch 460, val loss: 0.6592047214508057
Epoch 470, training loss: 0.7190983295440674 = 0.05854369327425957 + 0.1 * 6.605546474456787
Epoch 470, val loss: 0.6638912558555603
Epoch 480, training loss: 0.7117127776145935 = 0.053661417216062546 + 0.1 * 6.5805134773254395
Epoch 480, val loss: 0.6687952280044556
Epoch 490, training loss: 0.7067697048187256 = 0.049316443502902985 + 0.1 * 6.574532508850098
Epoch 490, val loss: 0.6739180684089661
Epoch 500, training loss: 0.704410195350647 = 0.04544372856616974 + 0.1 * 6.589664936065674
Epoch 500, val loss: 0.6791726350784302
Epoch 510, training loss: 0.6986815929412842 = 0.04199931025505066 + 0.1 * 6.5668230056762695
Epoch 510, val loss: 0.6844415068626404
Epoch 520, training loss: 0.6947420239448547 = 0.03891945257782936 + 0.1 * 6.558225631713867
Epoch 520, val loss: 0.6897838115692139
Epoch 530, training loss: 0.6911544799804688 = 0.03615262731909752 + 0.1 * 6.550018787384033
Epoch 530, val loss: 0.6951565146446228
Epoch 540, training loss: 0.6911822557449341 = 0.0336555652320385 + 0.1 * 6.575266361236572
Epoch 540, val loss: 0.700517475605011
Epoch 550, training loss: 0.6878277659416199 = 0.031413089483976364 + 0.1 * 6.564146995544434
Epoch 550, val loss: 0.7057732343673706
Epoch 560, training loss: 0.683902382850647 = 0.029394783079624176 + 0.1 * 6.545075416564941
Epoch 560, val loss: 0.7110095620155334
Epoch 570, training loss: 0.6812487244606018 = 0.02756093256175518 + 0.1 * 6.536877632141113
Epoch 570, val loss: 0.7161867022514343
Epoch 580, training loss: 0.6798017024993896 = 0.02588992938399315 + 0.1 * 6.539117336273193
Epoch 580, val loss: 0.721311628818512
Epoch 590, training loss: 0.6774963736534119 = 0.024368766695261 + 0.1 * 6.531275749206543
Epoch 590, val loss: 0.7263426780700684
Epoch 600, training loss: 0.6757875084877014 = 0.022980576381087303 + 0.1 * 6.528069019317627
Epoch 600, val loss: 0.7313358187675476
Epoch 610, training loss: 0.6748901605606079 = 0.021708253771066666 + 0.1 * 6.531818866729736
Epoch 610, val loss: 0.7362188100814819
Epoch 620, training loss: 0.6727012395858765 = 0.020539725199341774 + 0.1 * 6.521615505218506
Epoch 620, val loss: 0.7409713864326477
Epoch 630, training loss: 0.6722947359085083 = 0.019466441124677658 + 0.1 * 6.528282642364502
Epoch 630, val loss: 0.7456766366958618
Epoch 640, training loss: 0.6703943014144897 = 0.018478577956557274 + 0.1 * 6.5191569328308105
Epoch 640, val loss: 0.7502565979957581
Epoch 650, training loss: 0.668408989906311 = 0.01756661757826805 + 0.1 * 6.508423805236816
Epoch 650, val loss: 0.754753828048706
Epoch 660, training loss: 0.6674207448959351 = 0.016725260764360428 + 0.1 * 6.506954669952393
Epoch 660, val loss: 0.7591400146484375
Epoch 670, training loss: 0.6663621068000793 = 0.01594277285039425 + 0.1 * 6.504193305969238
Epoch 670, val loss: 0.763477623462677
Epoch 680, training loss: 0.6651841998100281 = 0.015215123072266579 + 0.1 * 6.499691009521484
Epoch 680, val loss: 0.7676600217819214
Epoch 690, training loss: 0.6649102568626404 = 0.014539706520736217 + 0.1 * 6.503705024719238
Epoch 690, val loss: 0.7718058228492737
Epoch 700, training loss: 0.6629309058189392 = 0.013910516165196896 + 0.1 * 6.490203857421875
Epoch 700, val loss: 0.7758445143699646
Epoch 710, training loss: 0.6629034280776978 = 0.013326240703463554 + 0.1 * 6.495771884918213
Epoch 710, val loss: 0.7797571420669556
Epoch 720, training loss: 0.6621556282043457 = 0.012779941782355309 + 0.1 * 6.4937567710876465
Epoch 720, val loss: 0.7836331129074097
Epoch 730, training loss: 0.6621574759483337 = 0.012265940196812153 + 0.1 * 6.498915195465088
Epoch 730, val loss: 0.7874150276184082
Epoch 740, training loss: 0.6608635187149048 = 0.011784722097218037 + 0.1 * 6.490787982940674
Epoch 740, val loss: 0.7911016345024109
Epoch 750, training loss: 0.6595985293388367 = 0.011334262788295746 + 0.1 * 6.482642650604248
Epoch 750, val loss: 0.7947369813919067
Epoch 760, training loss: 0.6589353680610657 = 0.010911577381193638 + 0.1 * 6.48023796081543
Epoch 760, val loss: 0.7982975244522095
Epoch 770, training loss: 0.6602200865745544 = 0.01051102951169014 + 0.1 * 6.4970903396606445
Epoch 770, val loss: 0.8017496466636658
Epoch 780, training loss: 0.6575982570648193 = 0.010137711651623249 + 0.1 * 6.474605083465576
Epoch 780, val loss: 0.8051313757896423
Epoch 790, training loss: 0.6566976308822632 = 0.009782504290342331 + 0.1 * 6.469151020050049
Epoch 790, val loss: 0.8084721565246582
Epoch 800, training loss: 0.6580056548118591 = 0.009447585791349411 + 0.1 * 6.485580921173096
Epoch 800, val loss: 0.8117316365242004
Epoch 810, training loss: 0.6560162305831909 = 0.00913119688630104 + 0.1 * 6.468850135803223
Epoch 810, val loss: 0.8149065971374512
Epoch 820, training loss: 0.6555226445198059 = 0.008832545951008797 + 0.1 * 6.466900825500488
Epoch 820, val loss: 0.8180734515190125
Epoch 830, training loss: 0.6551864743232727 = 0.008548899553716183 + 0.1 * 6.466375350952148
Epoch 830, val loss: 0.8211615085601807
Epoch 840, training loss: 0.6540136933326721 = 0.008280005306005478 + 0.1 * 6.457336902618408
Epoch 840, val loss: 0.8241833448410034
Epoch 850, training loss: 0.6557418704032898 = 0.008024749346077442 + 0.1 * 6.477170944213867
Epoch 850, val loss: 0.8271629810333252
Epoch 860, training loss: 0.653562605381012 = 0.007782905362546444 + 0.1 * 6.457797050476074
Epoch 860, val loss: 0.8300569653511047
Epoch 870, training loss: 0.6552005410194397 = 0.007552274502813816 + 0.1 * 6.476482391357422
Epoch 870, val loss: 0.8329386711120605
Epoch 880, training loss: 0.6526386737823486 = 0.007333290763199329 + 0.1 * 6.453053951263428
Epoch 880, val loss: 0.8357325792312622
Epoch 890, training loss: 0.6540542244911194 = 0.007125239819288254 + 0.1 * 6.469289302825928
Epoch 890, val loss: 0.838491678237915
Epoch 900, training loss: 0.6515294909477234 = 0.006926192902028561 + 0.1 * 6.446033000946045
Epoch 900, val loss: 0.8411866426467896
Epoch 910, training loss: 0.6523324847221375 = 0.006736824754625559 + 0.1 * 6.45595645904541
Epoch 910, val loss: 0.8438537120819092
Epoch 920, training loss: 0.6509385704994202 = 0.00655519962310791 + 0.1 * 6.443833827972412
Epoch 920, val loss: 0.846467912197113
Epoch 930, training loss: 0.652470588684082 = 0.006382430903613567 + 0.1 * 6.46088171005249
Epoch 930, val loss: 0.8490283489227295
Epoch 940, training loss: 0.6502411961555481 = 0.0062173218466341496 + 0.1 * 6.440238952636719
Epoch 940, val loss: 0.8515579104423523
Epoch 950, training loss: 0.6490663290023804 = 0.006059123203158379 + 0.1 * 6.430071830749512
Epoch 950, val loss: 0.8540490865707397
Epoch 960, training loss: 0.650714635848999 = 0.0059076217003166676 + 0.1 * 6.448070526123047
Epoch 960, val loss: 0.8564899563789368
Epoch 970, training loss: 0.6488627791404724 = 0.005762415938079357 + 0.1 * 6.431003570556641
Epoch 970, val loss: 0.8588786721229553
Epoch 980, training loss: 0.6486414670944214 = 0.005623141769319773 + 0.1 * 6.430182933807373
Epoch 980, val loss: 0.861251950263977
Epoch 990, training loss: 0.6485118269920349 = 0.005489112809300423 + 0.1 * 6.430226802825928
Epoch 990, val loss: 0.8635788559913635
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8313125988402742
=== training gcn model ===
Epoch 0, training loss: 2.8156063556671143 = 1.955927848815918 + 0.1 * 8.596784591674805
Epoch 0, val loss: 1.9698165655136108
Epoch 10, training loss: 2.8048453330993652 = 1.9451838731765747 + 0.1 * 8.5966157913208
Epoch 10, val loss: 1.9584108591079712
Epoch 20, training loss: 2.7913622856140137 = 1.9318150281906128 + 0.1 * 8.595471382141113
Epoch 20, val loss: 1.9438657760620117
Epoch 30, training loss: 2.7715070247650146 = 1.9129489660263062 + 0.1 * 8.585579872131348
Epoch 30, val loss: 1.9232473373413086
Epoch 40, training loss: 2.7371013164520264 = 1.8850195407867432 + 0.1 * 8.520817756652832
Epoch 40, val loss: 1.8929091691970825
Epoch 50, training loss: 2.664346218109131 = 1.8472089767456055 + 0.1 * 8.171372413635254
Epoch 50, val loss: 1.853400468826294
Epoch 60, training loss: 2.5870513916015625 = 1.8052836656570435 + 0.1 * 7.8176774978637695
Epoch 60, val loss: 1.8119741678237915
Epoch 70, training loss: 2.5076780319213867 = 1.764098882675171 + 0.1 * 7.435791015625
Epoch 70, val loss: 1.7736308574676514
Epoch 80, training loss: 2.4487509727478027 = 1.7210828065872192 + 0.1 * 7.276681423187256
Epoch 80, val loss: 1.7348175048828125
Epoch 90, training loss: 2.387444019317627 = 1.667319893836975 + 0.1 * 7.201241493225098
Epoch 90, val loss: 1.685378074645996
Epoch 100, training loss: 2.3098390102386475 = 1.5962544679641724 + 0.1 * 7.135845184326172
Epoch 100, val loss: 1.6207153797149658
Epoch 110, training loss: 2.215769052505493 = 1.5073314905166626 + 0.1 * 7.084375381469727
Epoch 110, val loss: 1.542089819908142
Epoch 120, training loss: 2.1091699600219727 = 1.4041106700897217 + 0.1 * 7.050591945648193
Epoch 120, val loss: 1.4526698589324951
Epoch 130, training loss: 1.9945893287658691 = 1.2928173542022705 + 0.1 * 7.0177202224731445
Epoch 130, val loss: 1.3584544658660889
Epoch 140, training loss: 1.8774809837341309 = 1.1797268390655518 + 0.1 * 6.977541923522949
Epoch 140, val loss: 1.2646052837371826
Epoch 150, training loss: 1.7666172981262207 = 1.0712254047393799 + 0.1 * 6.95391845703125
Epoch 150, val loss: 1.176760196685791
Epoch 160, training loss: 1.6668548583984375 = 0.9750627875328064 + 0.1 * 6.91792106628418
Epoch 160, val loss: 1.1005496978759766
Epoch 170, training loss: 1.579437494277954 = 0.8902835845947266 + 0.1 * 6.891538143157959
Epoch 170, val loss: 1.0344984531402588
Epoch 180, training loss: 1.502903699874878 = 0.8157567977905273 + 0.1 * 6.8714680671691895
Epoch 180, val loss: 0.9781357645988464
Epoch 190, training loss: 1.4357285499572754 = 0.7509504556655884 + 0.1 * 6.847780704498291
Epoch 190, val loss: 0.931272029876709
Epoch 200, training loss: 1.3756632804870605 = 0.6929861307144165 + 0.1 * 6.8267717361450195
Epoch 200, val loss: 0.8915682435035706
Epoch 210, training loss: 1.322034478187561 = 0.6403214931488037 + 0.1 * 6.817129611968994
Epoch 210, val loss: 0.8583184480667114
Epoch 220, training loss: 1.2718238830566406 = 0.5918796062469482 + 0.1 * 6.799442291259766
Epoch 220, val loss: 0.8306496143341064
Epoch 230, training loss: 1.2261810302734375 = 0.5461576581001282 + 0.1 * 6.800233840942383
Epoch 230, val loss: 0.8072814345359802
Epoch 240, training loss: 1.1810905933380127 = 0.5034384727478027 + 0.1 * 6.7765212059021
Epoch 240, val loss: 0.788330614566803
Epoch 250, training loss: 1.1402696371078491 = 0.4636155068874359 + 0.1 * 6.766541481018066
Epoch 250, val loss: 0.7735602855682373
Epoch 260, training loss: 1.1026434898376465 = 0.42727604508399963 + 0.1 * 6.753674507141113
Epoch 260, val loss: 0.7632440328598022
Epoch 270, training loss: 1.0683178901672363 = 0.3940642476081848 + 0.1 * 6.742535591125488
Epoch 270, val loss: 0.7570115327835083
Epoch 280, training loss: 1.0371122360229492 = 0.3636157512664795 + 0.1 * 6.7349653244018555
Epoch 280, val loss: 0.7542806267738342
Epoch 290, training loss: 1.0061744451522827 = 0.33550605177879333 + 0.1 * 6.70668363571167
Epoch 290, val loss: 0.7544145584106445
Epoch 300, training loss: 0.9786014556884766 = 0.30918416380882263 + 0.1 * 6.694173336029053
Epoch 300, val loss: 0.7570717334747314
Epoch 310, training loss: 0.9538916349411011 = 0.2844747006893158 + 0.1 * 6.694169044494629
Epoch 310, val loss: 0.7619727849960327
Epoch 320, training loss: 0.9303791522979736 = 0.2615492641925812 + 0.1 * 6.688299179077148
Epoch 320, val loss: 0.7686600089073181
Epoch 330, training loss: 0.906194806098938 = 0.24020670354366302 + 0.1 * 6.659880638122559
Epoch 330, val loss: 0.7770670056343079
Epoch 340, training loss: 0.8861905932426453 = 0.22030551731586456 + 0.1 * 6.65885066986084
Epoch 340, val loss: 0.7870293259620667
Epoch 350, training loss: 0.868350088596344 = 0.20194651186466217 + 0.1 * 6.664035797119141
Epoch 350, val loss: 0.7983916401863098
Epoch 360, training loss: 0.8483390212059021 = 0.18513567745685577 + 0.1 * 6.632033348083496
Epoch 360, val loss: 0.8107940554618835
Epoch 370, training loss: 0.8330416083335876 = 0.16970153152942657 + 0.1 * 6.633400917053223
Epoch 370, val loss: 0.824303150177002
Epoch 380, training loss: 0.8179271817207336 = 0.15564332902431488 + 0.1 * 6.622838497161865
Epoch 380, val loss: 0.8384289741516113
Epoch 390, training loss: 0.8043333888053894 = 0.14283491671085358 + 0.1 * 6.614984512329102
Epoch 390, val loss: 0.8532351851463318
Epoch 400, training loss: 0.793018102645874 = 0.13115176558494568 + 0.1 * 6.6186628341674805
Epoch 400, val loss: 0.8685945868492126
Epoch 410, training loss: 0.7811195850372314 = 0.1205369159579277 + 0.1 * 6.605826377868652
Epoch 410, val loss: 0.8842705488204956
Epoch 420, training loss: 0.7722599506378174 = 0.11088827252388 + 0.1 * 6.6137166023254395
Epoch 420, val loss: 0.9003000855445862
Epoch 430, training loss: 0.7621562480926514 = 0.1021631732583046 + 0.1 * 6.599930286407471
Epoch 430, val loss: 0.9164343476295471
Epoch 440, training loss: 0.752936840057373 = 0.09425218403339386 + 0.1 * 6.586846351623535
Epoch 440, val loss: 0.9326357245445251
Epoch 450, training loss: 0.7447842359542847 = 0.08705513924360275 + 0.1 * 6.5772905349731445
Epoch 450, val loss: 0.9490459561347961
Epoch 460, training loss: 0.7379259467124939 = 0.08051835000514984 + 0.1 * 6.574075698852539
Epoch 460, val loss: 0.9654480218887329
Epoch 470, training loss: 0.7316378355026245 = 0.07458475977182388 + 0.1 * 6.570530414581299
Epoch 470, val loss: 0.9817560315132141
Epoch 480, training loss: 0.7255186438560486 = 0.06920386850833893 + 0.1 * 6.56314754486084
Epoch 480, val loss: 0.9979326128959656
Epoch 490, training loss: 0.7203912138938904 = 0.06431660801172256 + 0.1 * 6.560745716094971
Epoch 490, val loss: 1.0138726234436035
Epoch 500, training loss: 0.7153873443603516 = 0.059858743101358414 + 0.1 * 6.555285930633545
Epoch 500, val loss: 1.0297894477844238
Epoch 510, training loss: 0.7102726101875305 = 0.055793676525354385 + 0.1 * 6.544788837432861
Epoch 510, val loss: 1.045522928237915
Epoch 520, training loss: 0.7086880803108215 = 0.052080731838941574 + 0.1 * 6.566072940826416
Epoch 520, val loss: 1.061078429222107
Epoch 530, training loss: 0.7033979892730713 = 0.04869085177779198 + 0.1 * 6.5470709800720215
Epoch 530, val loss: 1.0763314962387085
Epoch 540, training loss: 0.700275719165802 = 0.045590512454509735 + 0.1 * 6.546851634979248
Epoch 540, val loss: 1.0913265943527222
Epoch 550, training loss: 0.6958464980125427 = 0.042752500623464584 + 0.1 * 6.53093957901001
Epoch 550, val loss: 1.1059995889663696
Epoch 560, training loss: 0.6946994662284851 = 0.04014507681131363 + 0.1 * 6.545543670654297
Epoch 560, val loss: 1.120558738708496
Epoch 570, training loss: 0.6906222105026245 = 0.03775331377983093 + 0.1 * 6.528688907623291
Epoch 570, val loss: 1.13479483127594
Epoch 580, training loss: 0.6882990002632141 = 0.0355517640709877 + 0.1 * 6.527472019195557
Epoch 580, val loss: 1.1487871408462524
Epoch 590, training loss: 0.6843891143798828 = 0.03352433443069458 + 0.1 * 6.508647441864014
Epoch 590, val loss: 1.1625843048095703
Epoch 600, training loss: 0.6861089468002319 = 0.031652871519327164 + 0.1 * 6.544560432434082
Epoch 600, val loss: 1.1761338710784912
Epoch 610, training loss: 0.6825470328330994 = 0.02993251569569111 + 0.1 * 6.526144981384277
Epoch 610, val loss: 1.1893125772476196
Epoch 620, training loss: 0.6788805723190308 = 0.028345100581645966 + 0.1 * 6.505354404449463
Epoch 620, val loss: 1.202115774154663
Epoch 630, training loss: 0.6766539216041565 = 0.026874152943491936 + 0.1 * 6.497797966003418
Epoch 630, val loss: 1.2148374319076538
Epoch 640, training loss: 0.6764407753944397 = 0.025509335100650787 + 0.1 * 6.509314060211182
Epoch 640, val loss: 1.2273228168487549
Epoch 650, training loss: 0.675019383430481 = 0.0242425799369812 + 0.1 * 6.507767677307129
Epoch 650, val loss: 1.2395600080490112
Epoch 660, training loss: 0.6745380163192749 = 0.023067781701683998 + 0.1 * 6.514702320098877
Epoch 660, val loss: 1.2514938116073608
Epoch 670, training loss: 0.6712599992752075 = 0.02197789028286934 + 0.1 * 6.492820739746094
Epoch 670, val loss: 1.2631086111068726
Epoch 680, training loss: 0.6699731349945068 = 0.020963313058018684 + 0.1 * 6.490097999572754
Epoch 680, val loss: 1.274470567703247
Epoch 690, training loss: 0.6679782867431641 = 0.02001693844795227 + 0.1 * 6.479613780975342
Epoch 690, val loss: 1.2856428623199463
Epoch 700, training loss: 0.668106734752655 = 0.019131653010845184 + 0.1 * 6.489750862121582
Epoch 700, val loss: 1.2966725826263428
Epoch 710, training loss: 0.6677539348602295 = 0.018304379656910896 + 0.1 * 6.494495391845703
Epoch 710, val loss: 1.3074506521224976
Epoch 720, training loss: 0.6658008694648743 = 0.017531905323266983 + 0.1 * 6.482689380645752
Epoch 720, val loss: 1.317859172821045
Epoch 730, training loss: 0.6659494638442993 = 0.01680874079465866 + 0.1 * 6.4914069175720215
Epoch 730, val loss: 1.328065037727356
Epoch 740, training loss: 0.6637458801269531 = 0.01613275520503521 + 0.1 * 6.476130962371826
Epoch 740, val loss: 1.3380109071731567
Epoch 750, training loss: 0.6628118753433228 = 0.015497514046728611 + 0.1 * 6.473143577575684
Epoch 750, val loss: 1.3477360010147095
Epoch 760, training loss: 0.6611003279685974 = 0.01489928737282753 + 0.1 * 6.462009906768799
Epoch 760, val loss: 1.3573921918869019
Epoch 770, training loss: 0.6633080840110779 = 0.014335501939058304 + 0.1 * 6.489725589752197
Epoch 770, val loss: 1.3668830394744873
Epoch 780, training loss: 0.6600027680397034 = 0.013805586844682693 + 0.1 * 6.461971759796143
Epoch 780, val loss: 1.3761078119277954
Epoch 790, training loss: 0.6612947583198547 = 0.01330569852143526 + 0.1 * 6.4798903465271
Epoch 790, val loss: 1.3851042985916138
Epoch 800, training loss: 0.6594355702400208 = 0.01283406000584364 + 0.1 * 6.466014862060547
Epoch 800, val loss: 1.3939496278762817
Epoch 810, training loss: 0.6586436629295349 = 0.012388763017952442 + 0.1 * 6.462548732757568
Epoch 810, val loss: 1.4025664329528809
Epoch 820, training loss: 0.6577557921409607 = 0.011967338621616364 + 0.1 * 6.457884311676025
Epoch 820, val loss: 1.411026954650879
Epoch 830, training loss: 0.6575751304626465 = 0.011568475514650345 + 0.1 * 6.460066318511963
Epoch 830, val loss: 1.4193775653839111
Epoch 840, training loss: 0.6559107899665833 = 0.011189717799425125 + 0.1 * 6.447210788726807
Epoch 840, val loss: 1.4275610446929932
Epoch 850, training loss: 0.6555774807929993 = 0.010831295512616634 + 0.1 * 6.4474616050720215
Epoch 850, val loss: 1.4355058670043945
Epoch 860, training loss: 0.6555709838867188 = 0.010490456596016884 + 0.1 * 6.450804710388184
Epoch 860, val loss: 1.4433605670928955
Epoch 870, training loss: 0.6574327349662781 = 0.0101668955758214 + 0.1 * 6.472658157348633
Epoch 870, val loss: 1.4510681629180908
Epoch 880, training loss: 0.6550719141960144 = 0.009859167039394379 + 0.1 * 6.452127456665039
Epoch 880, val loss: 1.45855712890625
Epoch 890, training loss: 0.654497504234314 = 0.009566831402480602 + 0.1 * 6.449306488037109
Epoch 890, val loss: 1.465868353843689
Epoch 900, training loss: 0.6538400650024414 = 0.009287672117352486 + 0.1 * 6.445524215698242
Epoch 900, val loss: 1.4731271266937256
Epoch 910, training loss: 0.6538328528404236 = 0.00902182050049305 + 0.1 * 6.448110103607178
Epoch 910, val loss: 1.4802390336990356
Epoch 920, training loss: 0.6526527404785156 = 0.008767799474298954 + 0.1 * 6.438849449157715
Epoch 920, val loss: 1.487244963645935
Epoch 930, training loss: 0.6529123187065125 = 0.008525868877768517 + 0.1 * 6.443864822387695
Epoch 930, val loss: 1.4940483570098877
Epoch 940, training loss: 0.6517612338066101 = 0.008294382132589817 + 0.1 * 6.434668064117432
Epoch 940, val loss: 1.5007750988006592
Epoch 950, training loss: 0.6519477367401123 = 0.008073387667536736 + 0.1 * 6.4387431144714355
Epoch 950, val loss: 1.5073500871658325
Epoch 960, training loss: 0.6515262126922607 = 0.007861639373004436 + 0.1 * 6.436645984649658
Epoch 960, val loss: 1.513830304145813
Epoch 970, training loss: 0.6512782573699951 = 0.007659798488020897 + 0.1 * 6.436184406280518
Epoch 970, val loss: 1.5201084613800049
Epoch 980, training loss: 0.6502478718757629 = 0.00746545847505331 + 0.1 * 6.427823543548584
Epoch 980, val loss: 1.5263508558273315
Epoch 990, training loss: 0.6502088904380798 = 0.0072796097956597805 + 0.1 * 6.42929220199585
Epoch 990, val loss: 1.5325241088867188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8365840801265156
The final CL Acc:0.81235, 0.01522, The final GNN Acc:0.83658, 0.00430
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9400])
updated graph: torch.Size([2, 10430])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.790736198425293 = 1.931052327156067 + 0.1 * 8.596839904785156
Epoch 0, val loss: 1.9338585138320923
Epoch 10, training loss: 2.781275749206543 = 1.9215962886810303 + 0.1 * 8.596795082092285
Epoch 10, val loss: 1.9234752655029297
Epoch 20, training loss: 2.7697577476501465 = 1.9101089239120483 + 0.1 * 8.596488952636719
Epoch 20, val loss: 1.9105952978134155
Epoch 30, training loss: 2.753351926803589 = 1.8939391374588013 + 0.1 * 8.594127655029297
Epoch 30, val loss: 1.8924070596694946
Epoch 40, training loss: 2.7281532287597656 = 1.8703256845474243 + 0.1 * 8.578276634216309
Epoch 40, val loss: 1.866057276725769
Epoch 50, training loss: 2.690871477127075 = 1.8388200998306274 + 0.1 * 8.520514488220215
Epoch 50, val loss: 1.8325306177139282
Epoch 60, training loss: 2.6393752098083496 = 1.8062078952789307 + 0.1 * 8.331671714782715
Epoch 60, val loss: 1.8017185926437378
Epoch 70, training loss: 2.5975260734558105 = 1.7757487297058105 + 0.1 * 8.217772483825684
Epoch 70, val loss: 1.7765560150146484
Epoch 80, training loss: 2.5258522033691406 = 1.7366693019866943 + 0.1 * 7.891829967498779
Epoch 80, val loss: 1.7440952062606812
Epoch 90, training loss: 2.4400742053985596 = 1.6856731176376343 + 0.1 * 7.544010162353516
Epoch 90, val loss: 1.6979538202285767
Epoch 100, training loss: 2.3590033054351807 = 1.619110107421875 + 0.1 * 7.398931980133057
Epoch 100, val loss: 1.6358940601348877
Epoch 110, training loss: 2.2670154571533203 = 1.5367989540100098 + 0.1 * 7.302165985107422
Epoch 110, val loss: 1.5651710033416748
Epoch 120, training loss: 2.165233612060547 = 1.441970944404602 + 0.1 * 7.2326273918151855
Epoch 120, val loss: 1.487968921661377
Epoch 130, training loss: 2.057286262512207 = 1.3388439416885376 + 0.1 * 7.18442440032959
Epoch 130, val loss: 1.4043984413146973
Epoch 140, training loss: 1.946104645729065 = 1.2310562133789062 + 0.1 * 7.150484085083008
Epoch 140, val loss: 1.3189740180969238
Epoch 150, training loss: 1.8338536024093628 = 1.1220163106918335 + 0.1 * 7.118372917175293
Epoch 150, val loss: 1.2331619262695312
Epoch 160, training loss: 1.7265398502349854 = 1.0179028511047363 + 0.1 * 7.08636999130249
Epoch 160, val loss: 1.1530431509017944
Epoch 170, training loss: 1.629477620124817 = 0.9230169653892517 + 0.1 * 7.064606666564941
Epoch 170, val loss: 1.0825380086898804
Epoch 180, training loss: 1.5445852279663086 = 0.8401452898979187 + 0.1 * 7.044399738311768
Epoch 180, val loss: 1.0250616073608398
Epoch 190, training loss: 1.471541404724121 = 0.7685384154319763 + 0.1 * 7.030030250549316
Epoch 190, val loss: 0.9787598848342896
Epoch 200, training loss: 1.4077939987182617 = 0.7059940099716187 + 0.1 * 7.018000602722168
Epoch 200, val loss: 0.9412880539894104
Epoch 210, training loss: 1.3511362075805664 = 0.6503511667251587 + 0.1 * 7.0078511238098145
Epoch 210, val loss: 0.9104881882667542
Epoch 220, training loss: 1.298957347869873 = 0.599585771560669 + 0.1 * 6.993716239929199
Epoch 220, val loss: 0.8849555253982544
Epoch 230, training loss: 1.2502727508544922 = 0.5523974299430847 + 0.1 * 6.978753566741943
Epoch 230, val loss: 0.8636676669120789
Epoch 240, training loss: 1.2046558856964111 = 0.5083777904510498 + 0.1 * 6.962780952453613
Epoch 240, val loss: 0.8468732237815857
Epoch 250, training loss: 1.163089394569397 = 0.467269629240036 + 0.1 * 6.958198070526123
Epoch 250, val loss: 0.834383487701416
Epoch 260, training loss: 1.1222262382507324 = 0.4292128384113312 + 0.1 * 6.930134296417236
Epoch 260, val loss: 0.8260863423347473
Epoch 270, training loss: 1.087439775466919 = 0.3936409056186676 + 0.1 * 6.937988758087158
Epoch 270, val loss: 0.8212389945983887
Epoch 280, training loss: 1.0511865615844727 = 0.36052656173706055 + 0.1 * 6.906599044799805
Epoch 280, val loss: 0.8192841410636902
Epoch 290, training loss: 1.0180943012237549 = 0.3292112648487091 + 0.1 * 6.888830184936523
Epoch 290, val loss: 0.8197484612464905
Epoch 300, training loss: 0.9871189594268799 = 0.29945799708366394 + 0.1 * 6.876609802246094
Epoch 300, val loss: 0.8220869302749634
Epoch 310, training loss: 0.9583663940429688 = 0.2714712917804718 + 0.1 * 6.868951320648193
Epoch 310, val loss: 0.8263234496116638
Epoch 320, training loss: 0.9310764074325562 = 0.24523860216140747 + 0.1 * 6.858377933502197
Epoch 320, val loss: 0.8324111104011536
Epoch 330, training loss: 0.9071308970451355 = 0.22100211679935455 + 0.1 * 6.861287593841553
Epoch 330, val loss: 0.8407013416290283
Epoch 340, training loss: 0.8852135539054871 = 0.19924229383468628 + 0.1 * 6.859712600708008
Epoch 340, val loss: 0.8507335186004639
Epoch 350, training loss: 0.8637256026268005 = 0.1798373907804489 + 0.1 * 6.838881969451904
Epoch 350, val loss: 0.8627187013626099
Epoch 360, training loss: 0.8470189571380615 = 0.16255749762058258 + 0.1 * 6.844614505767822
Epoch 360, val loss: 0.876652181148529
Epoch 370, training loss: 0.8289579153060913 = 0.14731624722480774 + 0.1 * 6.816416263580322
Epoch 370, val loss: 0.8919049501419067
Epoch 380, training loss: 0.8147894740104675 = 0.13381479680538177 + 0.1 * 6.809746265411377
Epoch 380, val loss: 0.9084540605545044
Epoch 390, training loss: 0.8032008409500122 = 0.12183426320552826 + 0.1 * 6.813665390014648
Epoch 390, val loss: 0.9260048866271973
Epoch 400, training loss: 0.7906669974327087 = 0.11123345792293549 + 0.1 * 6.79433536529541
Epoch 400, val loss: 0.9441693425178528
Epoch 410, training loss: 0.7800616025924683 = 0.10180702805519104 + 0.1 * 6.78254508972168
Epoch 410, val loss: 0.9626990556716919
Epoch 420, training loss: 0.7720615267753601 = 0.09339921921491623 + 0.1 * 6.786623001098633
Epoch 420, val loss: 0.9815508723258972
Epoch 430, training loss: 0.7629629373550415 = 0.08595148473978043 + 0.1 * 6.770113945007324
Epoch 430, val loss: 1.0001440048217773
Epoch 440, training loss: 0.7551242709159851 = 0.0792999267578125 + 0.1 * 6.758243560791016
Epoch 440, val loss: 1.0187698602676392
Epoch 450, training loss: 0.748084545135498 = 0.07331374287605286 + 0.1 * 6.747707843780518
Epoch 450, val loss: 1.0373115539550781
Epoch 460, training loss: 0.7442060708999634 = 0.0679115429520607 + 0.1 * 6.76294469833374
Epoch 460, val loss: 1.0555790662765503
Epoch 470, training loss: 0.7368152737617493 = 0.06304635852575302 + 0.1 * 6.73768949508667
Epoch 470, val loss: 1.0736572742462158
Epoch 480, training loss: 0.7311229705810547 = 0.058643050491809845 + 0.1 * 6.724799156188965
Epoch 480, val loss: 1.091604232788086
Epoch 490, training loss: 0.7300480604171753 = 0.05464585870504379 + 0.1 * 6.754022121429443
Epoch 490, val loss: 1.1092002391815186
Epoch 500, training loss: 0.7226753830909729 = 0.051042258739471436 + 0.1 * 6.7163310050964355
Epoch 500, val loss: 1.1263136863708496
Epoch 510, training loss: 0.7182368636131287 = 0.047765009105205536 + 0.1 * 6.704718589782715
Epoch 510, val loss: 1.1431689262390137
Epoch 520, training loss: 0.7142912149429321 = 0.04476726055145264 + 0.1 * 6.695239543914795
Epoch 520, val loss: 1.1598337888717651
Epoch 530, training loss: 0.7119607925415039 = 0.0420251190662384 + 0.1 * 6.699357032775879
Epoch 530, val loss: 1.1760807037353516
Epoch 540, training loss: 0.7087323069572449 = 0.03951453045010567 + 0.1 * 6.692177772521973
Epoch 540, val loss: 1.1920676231384277
Epoch 550, training loss: 0.7045331597328186 = 0.0372096411883831 + 0.1 * 6.673234939575195
Epoch 550, val loss: 1.2077069282531738
Epoch 560, training loss: 0.7022451758384705 = 0.03509082272648811 + 0.1 * 6.671543598175049
Epoch 560, val loss: 1.2231180667877197
Epoch 570, training loss: 0.700761079788208 = 0.03314119204878807 + 0.1 * 6.676198482513428
Epoch 570, val loss: 1.2382148504257202
Epoch 580, training loss: 0.6971895098686218 = 0.031345345079898834 + 0.1 * 6.658441066741943
Epoch 580, val loss: 1.2529345750808716
Epoch 590, training loss: 0.6965836882591248 = 0.029686439782381058 + 0.1 * 6.668972015380859
Epoch 590, val loss: 1.2673602104187012
Epoch 600, training loss: 0.6936142444610596 = 0.02815350890159607 + 0.1 * 6.654606819152832
Epoch 600, val loss: 1.28147292137146
Epoch 610, training loss: 0.6911186575889587 = 0.02673192322254181 + 0.1 * 6.643867492675781
Epoch 610, val loss: 1.2953550815582275
Epoch 620, training loss: 0.6951336860656738 = 0.02541145123541355 + 0.1 * 6.697222709655762
Epoch 620, val loss: 1.3089293241500854
Epoch 630, training loss: 0.6897295713424683 = 0.024189887568354607 + 0.1 * 6.655396461486816
Epoch 630, val loss: 1.3219568729400635
Epoch 640, training loss: 0.6865285038948059 = 0.023057499900460243 + 0.1 * 6.63470983505249
Epoch 640, val loss: 1.3348010778427124
Epoch 650, training loss: 0.6848286986351013 = 0.02200212888419628 + 0.1 * 6.628265857696533
Epoch 650, val loss: 1.347393274307251
Epoch 660, training loss: 0.6839451193809509 = 0.021016322076320648 + 0.1 * 6.629288196563721
Epoch 660, val loss: 1.3597187995910645
Epoch 670, training loss: 0.682013750076294 = 0.020095203071832657 + 0.1 * 6.619184970855713
Epoch 670, val loss: 1.371840238571167
Epoch 680, training loss: 0.6831220388412476 = 0.01923295110464096 + 0.1 * 6.638891220092773
Epoch 680, val loss: 1.3836467266082764
Epoch 690, training loss: 0.6811724901199341 = 0.01842646673321724 + 0.1 * 6.62746000289917
Epoch 690, val loss: 1.3952491283416748
Epoch 700, training loss: 0.680366575717926 = 0.01767119951546192 + 0.1 * 6.626953125
Epoch 700, val loss: 1.4064863920211792
Epoch 710, training loss: 0.6767861247062683 = 0.016963636502623558 + 0.1 * 6.598224639892578
Epoch 710, val loss: 1.4175366163253784
Epoch 720, training loss: 0.6771395206451416 = 0.016298934817314148 + 0.1 * 6.608405590057373
Epoch 720, val loss: 1.4284034967422485
Epoch 730, training loss: 0.6750326752662659 = 0.015673372894525528 + 0.1 * 6.593592643737793
Epoch 730, val loss: 1.43900465965271
Epoch 740, training loss: 0.6750141978263855 = 0.015083921141922474 + 0.1 * 6.599302768707275
Epoch 740, val loss: 1.4493324756622314
Epoch 750, training loss: 0.6742791533470154 = 0.014529207721352577 + 0.1 * 6.597498893737793
Epoch 750, val loss: 1.459404706954956
Epoch 760, training loss: 0.6731939911842346 = 0.014006262645125389 + 0.1 * 6.591877460479736
Epoch 760, val loss: 1.4693958759307861
Epoch 770, training loss: 0.6733580231666565 = 0.013512340374290943 + 0.1 * 6.598456859588623
Epoch 770, val loss: 1.479067325592041
Epoch 780, training loss: 0.6705315709114075 = 0.013045381754636765 + 0.1 * 6.574862003326416
Epoch 780, val loss: 1.4885308742523193
Epoch 790, training loss: 0.6719376444816589 = 0.01260378211736679 + 0.1 * 6.593338966369629
Epoch 790, val loss: 1.4978669881820679
Epoch 800, training loss: 0.6703566908836365 = 0.012185323052108288 + 0.1 * 6.581713676452637
Epoch 800, val loss: 1.5069864988327026
Epoch 810, training loss: 0.6689041256904602 = 0.011788489297032356 + 0.1 * 6.571156024932861
Epoch 810, val loss: 1.515875220298767
Epoch 820, training loss: 0.6698455810546875 = 0.011412368156015873 + 0.1 * 6.58433198928833
Epoch 820, val loss: 1.5245964527130127
Epoch 830, training loss: 0.6676437854766846 = 0.011055031791329384 + 0.1 * 6.565887451171875
Epoch 830, val loss: 1.5331625938415527
Epoch 840, training loss: 0.6673809289932251 = 0.010715498588979244 + 0.1 * 6.566654205322266
Epoch 840, val loss: 1.5416101217269897
Epoch 850, training loss: 0.6677592992782593 = 0.010392379015684128 + 0.1 * 6.573668956756592
Epoch 850, val loss: 1.5498594045639038
Epoch 860, training loss: 0.6656221747398376 = 0.010084628127515316 + 0.1 * 6.555375576019287
Epoch 860, val loss: 1.5577902793884277
Epoch 870, training loss: 0.6652916669845581 = 0.009792083874344826 + 0.1 * 6.554995536804199
Epoch 870, val loss: 1.565693974494934
Epoch 880, training loss: 0.6647601127624512 = 0.009512852877378464 + 0.1 * 6.5524725914001465
Epoch 880, val loss: 1.5735169649124146
Epoch 890, training loss: 0.665406346321106 = 0.009246214292943478 + 0.1 * 6.561601161956787
Epoch 890, val loss: 1.5811272859573364
Epoch 900, training loss: 0.6653968691825867 = 0.008991463109850883 + 0.1 * 6.564054012298584
Epoch 900, val loss: 1.5885056257247925
Epoch 910, training loss: 0.6628085970878601 = 0.008748568594455719 + 0.1 * 6.540599822998047
Epoch 910, val loss: 1.5959281921386719
Epoch 920, training loss: 0.6638279557228088 = 0.008515932597219944 + 0.1 * 6.553120136260986
Epoch 920, val loss: 1.603209137916565
Epoch 930, training loss: 0.6630885601043701 = 0.008293169550597668 + 0.1 * 6.547954082489014
Epoch 930, val loss: 1.6101806163787842
Epoch 940, training loss: 0.6615331768989563 = 0.008080367930233479 + 0.1 * 6.534527778625488
Epoch 940, val loss: 1.6171414852142334
Epoch 950, training loss: 0.6623989343643188 = 0.00787658616900444 + 0.1 * 6.545223236083984
Epoch 950, val loss: 1.6240830421447754
Epoch 960, training loss: 0.6625884175300598 = 0.007680824026465416 + 0.1 * 6.549075603485107
Epoch 960, val loss: 1.6308302879333496
Epoch 970, training loss: 0.6607003808021545 = 0.007493144366890192 + 0.1 * 6.532072067260742
Epoch 970, val loss: 1.6373909711837769
Epoch 980, training loss: 0.6611843705177307 = 0.007313595153391361 + 0.1 * 6.538707733154297
Epoch 980, val loss: 1.644028663635254
Epoch 990, training loss: 0.6605983972549438 = 0.007140690460801125 + 0.1 * 6.534576892852783
Epoch 990, val loss: 1.650453805923462
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8028465998945704
=== training gcn model ===
Epoch 0, training loss: 2.810030460357666 = 1.950348973274231 + 0.1 * 8.59681510925293
Epoch 0, val loss: 1.9514412879943848
Epoch 10, training loss: 2.800629138946533 = 1.9409561157226562 + 0.1 * 8.59673023223877
Epoch 10, val loss: 1.9423929452896118
Epoch 20, training loss: 2.789189100265503 = 1.9295669794082642 + 0.1 * 8.596221923828125
Epoch 20, val loss: 1.9308133125305176
Epoch 30, training loss: 2.772801160812378 = 1.9136220216751099 + 0.1 * 8.591792106628418
Epoch 30, val loss: 1.9141931533813477
Epoch 40, training loss: 2.745836019515991 = 1.8899643421173096 + 0.1 * 8.5587158203125
Epoch 40, val loss: 1.8895918130874634
Epoch 50, training loss: 2.6969428062438965 = 1.85722017288208 + 0.1 * 8.397225379943848
Epoch 50, val loss: 1.8569391965866089
Epoch 60, training loss: 2.6447947025299072 = 1.8197859525680542 + 0.1 * 8.250086784362793
Epoch 60, val loss: 1.8219239711761475
Epoch 70, training loss: 2.5844879150390625 = 1.7864278554916382 + 0.1 * 7.980599880218506
Epoch 70, val loss: 1.792553186416626
Epoch 80, training loss: 2.501432418823242 = 1.7555776834487915 + 0.1 * 7.458548545837402
Epoch 80, val loss: 1.7644083499908447
Epoch 90, training loss: 2.436798095703125 = 1.7181683778762817 + 0.1 * 7.186298370361328
Epoch 90, val loss: 1.7297300100326538
Epoch 100, training loss: 2.3753154277801514 = 1.6692277193069458 + 0.1 * 7.060876369476318
Epoch 100, val loss: 1.6862238645553589
Epoch 110, training loss: 2.3047516345977783 = 1.6058775186538696 + 0.1 * 6.988741874694824
Epoch 110, val loss: 1.6309751272201538
Epoch 120, training loss: 2.2247495651245117 = 1.5295767784118652 + 0.1 * 6.951726913452148
Epoch 120, val loss: 1.5652885437011719
Epoch 130, training loss: 2.138362407684326 = 1.4458434581756592 + 0.1 * 6.92518949508667
Epoch 130, val loss: 1.494924783706665
Epoch 140, training loss: 2.0505380630493164 = 1.3597451448440552 + 0.1 * 6.907928943634033
Epoch 140, val loss: 1.4251306056976318
Epoch 150, training loss: 1.9620182514190674 = 1.272570013999939 + 0.1 * 6.894482135772705
Epoch 150, val loss: 1.3569140434265137
Epoch 160, training loss: 1.872645378112793 = 1.1844230890274048 + 0.1 * 6.8822221755981445
Epoch 160, val loss: 1.2892918586730957
Epoch 170, training loss: 1.7836501598358154 = 1.096812129020691 + 0.1 * 6.868380069732666
Epoch 170, val loss: 1.2228319644927979
Epoch 180, training loss: 1.6981620788574219 = 1.0120437145233154 + 0.1 * 6.8611836433410645
Epoch 180, val loss: 1.1584218740463257
Epoch 190, training loss: 1.6162053346633911 = 0.9322282075881958 + 0.1 * 6.839771270751953
Epoch 190, val loss: 1.0973044633865356
Epoch 200, training loss: 1.5407146215438843 = 0.8577718138694763 + 0.1 * 6.829428195953369
Epoch 200, val loss: 1.0402029752731323
Epoch 210, training loss: 1.471666693687439 = 0.7900480031967163 + 0.1 * 6.816186904907227
Epoch 210, val loss: 0.9883406758308411
Epoch 220, training loss: 1.4077045917510986 = 0.7274374961853027 + 0.1 * 6.802670001983643
Epoch 220, val loss: 0.9416846632957458
Epoch 230, training loss: 1.3494296073913574 = 0.6704112887382507 + 0.1 * 6.790183067321777
Epoch 230, val loss: 0.9016885757446289
Epoch 240, training loss: 1.2958118915557861 = 0.6182270646095276 + 0.1 * 6.775848388671875
Epoch 240, val loss: 0.8685266971588135
Epoch 250, training loss: 1.2468880414962769 = 0.570323646068573 + 0.1 * 6.76564359664917
Epoch 250, val loss: 0.8419128060340881
Epoch 260, training loss: 1.2026185989379883 = 0.5262722969055176 + 0.1 * 6.763462066650391
Epoch 260, val loss: 0.8212714791297913
Epoch 270, training loss: 1.160724401473999 = 0.48616674542427063 + 0.1 * 6.745575904846191
Epoch 270, val loss: 0.8058359622955322
Epoch 280, training loss: 1.1222610473632812 = 0.4492571949958801 + 0.1 * 6.730038642883301
Epoch 280, val loss: 0.7942891716957092
Epoch 290, training loss: 1.0879414081573486 = 0.41475915908813477 + 0.1 * 6.731822967529297
Epoch 290, val loss: 0.7855077385902405
Epoch 300, training loss: 1.0537556409835815 = 0.38233456015586853 + 0.1 * 6.714210510253906
Epoch 300, val loss: 0.7785133719444275
Epoch 310, training loss: 1.0261130332946777 = 0.35130178928375244 + 0.1 * 6.748112201690674
Epoch 310, val loss: 0.7727658152580261
Epoch 320, training loss: 0.9923233985900879 = 0.3217259347438812 + 0.1 * 6.70597505569458
Epoch 320, val loss: 0.7679315805435181
Epoch 330, training loss: 0.9621545076370239 = 0.2933194637298584 + 0.1 * 6.688350200653076
Epoch 330, val loss: 0.7640114426612854
Epoch 340, training loss: 0.9359420537948608 = 0.2661789059638977 + 0.1 * 6.697631359100342
Epoch 340, val loss: 0.7611715793609619
Epoch 350, training loss: 0.9087421894073486 = 0.24071699380874634 + 0.1 * 6.6802520751953125
Epoch 350, val loss: 0.7594059705734253
Epoch 360, training loss: 0.8836604356765747 = 0.21705284714698792 + 0.1 * 6.666076183319092
Epoch 360, val loss: 0.7590839862823486
Epoch 370, training loss: 0.86282879114151 = 0.1953328400850296 + 0.1 * 6.674959659576416
Epoch 370, val loss: 0.7601729035377502
Epoch 380, training loss: 0.8408153057098389 = 0.17576366662979126 + 0.1 * 6.650516510009766
Epoch 380, val loss: 0.7626128792762756
Epoch 390, training loss: 0.8226820230484009 = 0.1581893265247345 + 0.1 * 6.644927024841309
Epoch 390, val loss: 0.7664693593978882
Epoch 400, training loss: 0.8060985803604126 = 0.1425323784351349 + 0.1 * 6.635662078857422
Epoch 400, val loss: 0.7714706063270569
Epoch 410, training loss: 0.7920922636985779 = 0.12867914140224457 + 0.1 * 6.634130954742432
Epoch 410, val loss: 0.777527391910553
Epoch 420, training loss: 0.7809095978736877 = 0.11640916019678116 + 0.1 * 6.6450042724609375
Epoch 420, val loss: 0.7845214009284973
Epoch 430, training loss: 0.7680436968803406 = 0.10560055822134018 + 0.1 * 6.624431610107422
Epoch 430, val loss: 0.792064368724823
Epoch 440, training loss: 0.7574692964553833 = 0.09605449438095093 + 0.1 * 6.614148139953613
Epoch 440, val loss: 0.800238847732544
Epoch 450, training loss: 0.7492454051971436 = 0.08758994191884995 + 0.1 * 6.6165547370910645
Epoch 450, val loss: 0.8088393807411194
Epoch 460, training loss: 0.7413104772567749 = 0.0800921767950058 + 0.1 * 6.6121826171875
Epoch 460, val loss: 0.817720890045166
Epoch 470, training loss: 0.7327793836593628 = 0.0734371468424797 + 0.1 * 6.593421936035156
Epoch 470, val loss: 0.8268224596977234
Epoch 480, training loss: 0.7295815944671631 = 0.06750521063804626 + 0.1 * 6.620763778686523
Epoch 480, val loss: 0.8360418677330017
Epoch 490, training loss: 0.7208649516105652 = 0.06222797930240631 + 0.1 * 6.58636999130249
Epoch 490, val loss: 0.8452717065811157
Epoch 500, training loss: 0.715322732925415 = 0.057506825774908066 + 0.1 * 6.578159332275391
Epoch 500, val loss: 0.8545680642127991
Epoch 510, training loss: 0.7143347859382629 = 0.05326570197939873 + 0.1 * 6.610690593719482
Epoch 510, val loss: 0.8638508915901184
Epoch 520, training loss: 0.7068716287612915 = 0.04946434870362282 + 0.1 * 6.57407283782959
Epoch 520, val loss: 0.8729883432388306
Epoch 530, training loss: 0.7025139331817627 = 0.046036604791879654 + 0.1 * 6.564773082733154
Epoch 530, val loss: 0.8821450471878052
Epoch 540, training loss: 0.6993024349212646 = 0.04293516278266907 + 0.1 * 6.5636725425720215
Epoch 540, val loss: 0.891198456287384
Epoch 550, training loss: 0.6956408619880676 = 0.0401265025138855 + 0.1 * 6.555143356323242
Epoch 550, val loss: 0.9001333713531494
Epoch 560, training loss: 0.6938341856002808 = 0.037574417889118195 + 0.1 * 6.562597751617432
Epoch 560, val loss: 0.9089906811714172
Epoch 570, training loss: 0.6897611021995544 = 0.03525102138519287 + 0.1 * 6.545100688934326
Epoch 570, val loss: 0.9176375269889832
Epoch 580, training loss: 0.6884154081344604 = 0.033129606395959854 + 0.1 * 6.552857398986816
Epoch 580, val loss: 0.9262577295303345
Epoch 590, training loss: 0.6872197389602661 = 0.03119155578315258 + 0.1 * 6.560282230377197
Epoch 590, val loss: 0.9346888661384583
Epoch 600, training loss: 0.6834374070167542 = 0.029419947415590286 + 0.1 * 6.54017448425293
Epoch 600, val loss: 0.9429740905761719
Epoch 610, training loss: 0.6812263131141663 = 0.027795808389782906 + 0.1 * 6.534305095672607
Epoch 610, val loss: 0.9510873556137085
Epoch 620, training loss: 0.6794944405555725 = 0.026302795857191086 + 0.1 * 6.53191614151001
Epoch 620, val loss: 0.9591056704521179
Epoch 630, training loss: 0.6788526773452759 = 0.024925313889980316 + 0.1 * 6.539273738861084
Epoch 630, val loss: 0.9669511318206787
Epoch 640, training loss: 0.6763882637023926 = 0.023654287680983543 + 0.1 * 6.527339458465576
Epoch 640, val loss: 0.97463458776474
Epoch 650, training loss: 0.6742534041404724 = 0.022480608895421028 + 0.1 * 6.517727851867676
Epoch 650, val loss: 0.9822011590003967
Epoch 660, training loss: 0.674700915813446 = 0.021391592919826508 + 0.1 * 6.533093452453613
Epoch 660, val loss: 0.9896205067634583
Epoch 670, training loss: 0.6723361015319824 = 0.02038353867828846 + 0.1 * 6.519525527954102
Epoch 670, val loss: 0.9968504905700684
Epoch 680, training loss: 0.6704700589179993 = 0.019447242841124535 + 0.1 * 6.510227680206299
Epoch 680, val loss: 1.004043698310852
Epoch 690, training loss: 0.6703497767448425 = 0.01857345923781395 + 0.1 * 6.517763137817383
Epoch 690, val loss: 1.0110112428665161
Epoch 700, training loss: 0.6687241196632385 = 0.017759786918759346 + 0.1 * 6.509643077850342
Epoch 700, val loss: 1.0178806781768799
Epoch 710, training loss: 0.6673368215560913 = 0.01700030080974102 + 0.1 * 6.5033650398254395
Epoch 710, val loss: 1.0246702432632446
Epoch 720, training loss: 0.6664137244224548 = 0.016288498416543007 + 0.1 * 6.501252174377441
Epoch 720, val loss: 1.0311837196350098
Epoch 730, training loss: 0.6660246849060059 = 0.0156240900978446 + 0.1 * 6.504005432128906
Epoch 730, val loss: 1.0377014875411987
Epoch 740, training loss: 0.6645417213439941 = 0.015000662766397 + 0.1 * 6.495410442352295
Epoch 740, val loss: 1.0439397096633911
Epoch 750, training loss: 0.6638964414596558 = 0.014416944235563278 + 0.1 * 6.494794845581055
Epoch 750, val loss: 1.050162672996521
Epoch 760, training loss: 0.6648812294006348 = 0.01386838685721159 + 0.1 * 6.510128498077393
Epoch 760, val loss: 1.0562191009521484
Epoch 770, training loss: 0.6621856093406677 = 0.013351883739233017 + 0.1 * 6.488337516784668
Epoch 770, val loss: 1.0621867179870605
Epoch 780, training loss: 0.6616477966308594 = 0.012865869328379631 + 0.1 * 6.487819671630859
Epoch 780, val loss: 1.0680649280548096
Epoch 790, training loss: 0.6614118814468384 = 0.012406451627612114 + 0.1 * 6.490054130554199
Epoch 790, val loss: 1.0738060474395752
Epoch 800, training loss: 0.6598654389381409 = 0.011972909793257713 + 0.1 * 6.4789252281188965
Epoch 800, val loss: 1.0794349908828735
Epoch 810, training loss: 0.6611877679824829 = 0.011563016101717949 + 0.1 * 6.496247291564941
Epoch 810, val loss: 1.084981083869934
Epoch 820, training loss: 0.6597943305969238 = 0.011175134219229221 + 0.1 * 6.486191749572754
Epoch 820, val loss: 1.0902788639068604
Epoch 830, training loss: 0.6590538620948792 = 0.010809633880853653 + 0.1 * 6.4824419021606445
Epoch 830, val loss: 1.0956319570541382
Epoch 840, training loss: 0.6583125591278076 = 0.010462788864970207 + 0.1 * 6.4784979820251465
Epoch 840, val loss: 1.1008546352386475
Epoch 850, training loss: 0.6575140357017517 = 0.010133159346878529 + 0.1 * 6.473808765411377
Epoch 850, val loss: 1.105980396270752
Epoch 860, training loss: 0.658462643623352 = 0.009819661267101765 + 0.1 * 6.486429691314697
Epoch 860, val loss: 1.1108911037445068
Epoch 870, training loss: 0.6567155718803406 = 0.009522455744445324 + 0.1 * 6.471931457519531
Epoch 870, val loss: 1.1158697605133057
Epoch 880, training loss: 0.6557295918464661 = 0.00923976767808199 + 0.1 * 6.464897632598877
Epoch 880, val loss: 1.1207294464111328
Epoch 890, training loss: 0.6563787460327148 = 0.008970104157924652 + 0.1 * 6.474086284637451
Epoch 890, val loss: 1.1255279779434204
Epoch 900, training loss: 0.6547257304191589 = 0.008713021874427795 + 0.1 * 6.460126876831055
Epoch 900, val loss: 1.13016939163208
Epoch 910, training loss: 0.6562027335166931 = 0.008467883802950382 + 0.1 * 6.477348327636719
Epoch 910, val loss: 1.1347700357437134
Epoch 920, training loss: 0.6541035771369934 = 0.008233567699790001 + 0.1 * 6.458700180053711
Epoch 920, val loss: 1.1392964124679565
Epoch 930, training loss: 0.653911292552948 = 0.008010398596525192 + 0.1 * 6.459008693695068
Epoch 930, val loss: 1.1437393426895142
Epoch 940, training loss: 0.6547627449035645 = 0.0077965958043932915 + 0.1 * 6.469661712646484
Epoch 940, val loss: 1.1480656862258911
Epoch 950, training loss: 0.6538605093955994 = 0.007592248730361462 + 0.1 * 6.462682247161865
Epoch 950, val loss: 1.1522678136825562
Epoch 960, training loss: 0.6539385318756104 = 0.0073974719271063805 + 0.1 * 6.4654107093811035
Epoch 960, val loss: 1.1565337181091309
Epoch 970, training loss: 0.6531174182891846 = 0.007210214622318745 + 0.1 * 6.459072113037109
Epoch 970, val loss: 1.160589337348938
Epoch 980, training loss: 0.6518154144287109 = 0.00703106913715601 + 0.1 * 6.447843074798584
Epoch 980, val loss: 1.1647043228149414
Epoch 990, training loss: 0.6527735590934753 = 0.006859047804027796 + 0.1 * 6.459144592285156
Epoch 990, val loss: 1.1687275171279907
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 2.811316967010498 = 1.9516303539276123 + 0.1 * 8.596866607666016
Epoch 0, val loss: 1.9626989364624023
Epoch 10, training loss: 2.8005638122558594 = 1.9408818483352661 + 0.1 * 8.596820831298828
Epoch 10, val loss: 1.9513753652572632
Epoch 20, training loss: 2.7878479957580566 = 1.9281892776489258 + 0.1 * 8.596587181091309
Epoch 20, val loss: 1.9378273487091064
Epoch 30, training loss: 2.7707743644714355 = 1.9112941026687622 + 0.1 * 8.594803810119629
Epoch 30, val loss: 1.9196571111679077
Epoch 40, training loss: 2.7454049587249756 = 1.8874075412750244 + 0.1 * 8.579973220825195
Epoch 40, val loss: 1.8940483331680298
Epoch 50, training loss: 2.7043395042419434 = 1.8548202514648438 + 0.1 * 8.49519157409668
Epoch 50, val loss: 1.8601000308990479
Epoch 60, training loss: 2.6485722064971924 = 1.8183164596557617 + 0.1 * 8.302556991577148
Epoch 60, val loss: 1.824247121810913
Epoch 70, training loss: 2.5980498790740967 = 1.785998821258545 + 0.1 * 8.120511054992676
Epoch 70, val loss: 1.7938402891159058
Epoch 80, training loss: 2.5201191902160645 = 1.7534441947937012 + 0.1 * 7.666750431060791
Epoch 80, val loss: 1.7623685598373413
Epoch 90, training loss: 2.4514262676239014 = 1.7131006717681885 + 0.1 * 7.383255958557129
Epoch 90, val loss: 1.7256910800933838
Epoch 100, training loss: 2.3842906951904297 = 1.659085988998413 + 0.1 * 7.252046585083008
Epoch 100, val loss: 1.6781436204910278
Epoch 110, training loss: 2.305229663848877 = 1.58797025680542 + 0.1 * 7.1725945472717285
Epoch 110, val loss: 1.614166498184204
Epoch 120, training loss: 2.213197946548462 = 1.499973177909851 + 0.1 * 7.132246971130371
Epoch 120, val loss: 1.5370250940322876
Epoch 130, training loss: 2.1112799644470215 = 1.4015939235687256 + 0.1 * 7.096860408782959
Epoch 130, val loss: 1.4527710676193237
Epoch 140, training loss: 2.0040597915649414 = 1.2978371381759644 + 0.1 * 7.062225818634033
Epoch 140, val loss: 1.3653124570846558
Epoch 150, training loss: 1.8975393772125244 = 1.1936886310577393 + 0.1 * 7.038506984710693
Epoch 150, val loss: 1.2799310684204102
Epoch 160, training loss: 1.7972769737243652 = 1.0949770212173462 + 0.1 * 7.023000240325928
Epoch 160, val loss: 1.2017431259155273
Epoch 170, training loss: 1.7043530941009521 = 1.003456950187683 + 0.1 * 7.008960723876953
Epoch 170, val loss: 1.131678819656372
Epoch 180, training loss: 1.6185972690582275 = 0.9188891649246216 + 0.1 * 6.997081756591797
Epoch 180, val loss: 1.0689640045166016
Epoch 190, training loss: 1.5387442111968994 = 0.8401307463645935 + 0.1 * 6.9861345291137695
Epoch 190, val loss: 1.0121665000915527
Epoch 200, training loss: 1.4637006521224976 = 0.7665308117866516 + 0.1 * 6.97169828414917
Epoch 200, val loss: 0.960649847984314
Epoch 210, training loss: 1.3939591646194458 = 0.6979239583015442 + 0.1 * 6.960351943969727
Epoch 210, val loss: 0.9148581027984619
Epoch 220, training loss: 1.329561710357666 = 0.6350117921829224 + 0.1 * 6.945499897003174
Epoch 220, val loss: 0.8759356141090393
Epoch 230, training loss: 1.2713830471038818 = 0.5777977108955383 + 0.1 * 6.935852527618408
Epoch 230, val loss: 0.8444704413414001
Epoch 240, training loss: 1.2183568477630615 = 0.5261622071266174 + 0.1 * 6.921945571899414
Epoch 240, val loss: 0.8203957080841064
Epoch 250, training loss: 1.1708282232284546 = 0.4794803857803345 + 0.1 * 6.913478374481201
Epoch 250, val loss: 0.8027514219284058
Epoch 260, training loss: 1.128955602645874 = 0.4372299611568451 + 0.1 * 6.917255878448486
Epoch 260, val loss: 0.7901356816291809
Epoch 270, training loss: 1.0889475345611572 = 0.39900916814804077 + 0.1 * 6.899383544921875
Epoch 270, val loss: 0.7812298536300659
Epoch 280, training loss: 1.052068829536438 = 0.3638124167919159 + 0.1 * 6.882564067840576
Epoch 280, val loss: 0.775113582611084
Epoch 290, training loss: 1.0205352306365967 = 0.331054151058197 + 0.1 * 6.894811153411865
Epoch 290, val loss: 0.7712658643722534
Epoch 300, training loss: 0.9886296391487122 = 0.30083173513412476 + 0.1 * 6.877978801727295
Epoch 300, val loss: 0.7695455551147461
Epoch 310, training loss: 0.9588629603385925 = 0.27284181118011475 + 0.1 * 6.860211372375488
Epoch 310, val loss: 0.7696486115455627
Epoch 320, training loss: 0.9322341680526733 = 0.24692226946353912 + 0.1 * 6.853118896484375
Epoch 320, val loss: 0.7716145515441895
Epoch 330, training loss: 0.9072996973991394 = 0.22305887937545776 + 0.1 * 6.842408180236816
Epoch 330, val loss: 0.7753739953041077
Epoch 340, training loss: 0.8849241137504578 = 0.20124466717243195 + 0.1 * 6.836794376373291
Epoch 340, val loss: 0.7809057831764221
Epoch 350, training loss: 0.8641062378883362 = 0.18146099150180817 + 0.1 * 6.826452255249023
Epoch 350, val loss: 0.7881119251251221
Epoch 360, training loss: 0.8448605537414551 = 0.16362397372722626 + 0.1 * 6.812365531921387
Epoch 360, val loss: 0.7966705560684204
Epoch 370, training loss: 0.8300551772117615 = 0.1475122570991516 + 0.1 * 6.8254289627075195
Epoch 370, val loss: 0.8067551851272583
Epoch 380, training loss: 0.8128522038459778 = 0.13310329616069794 + 0.1 * 6.797488689422607
Epoch 380, val loss: 0.8180276155471802
Epoch 390, training loss: 0.7990546226501465 = 0.12021385878324509 + 0.1 * 6.788407325744629
Epoch 390, val loss: 0.8303996324539185
Epoch 400, training loss: 0.7896293997764587 = 0.10873943567276001 + 0.1 * 6.808899402618408
Epoch 400, val loss: 0.8436331152915955
Epoch 410, training loss: 0.7757636308670044 = 0.09861984103918076 + 0.1 * 6.771438121795654
Epoch 410, val loss: 0.8572336435317993
Epoch 420, training loss: 0.7663548588752747 = 0.08964245021343231 + 0.1 * 6.767123699188232
Epoch 420, val loss: 0.8713503479957581
Epoch 430, training loss: 0.756533682346344 = 0.08169150352478027 + 0.1 * 6.748421669006348
Epoch 430, val loss: 0.8856518268585205
Epoch 440, training loss: 0.7489461302757263 = 0.07461705058813095 + 0.1 * 6.743290901184082
Epoch 440, val loss: 0.900187611579895
Epoch 450, training loss: 0.743503212928772 = 0.0683106854557991 + 0.1 * 6.751924991607666
Epoch 450, val loss: 0.9147763252258301
Epoch 460, training loss: 0.7365682721138 = 0.06270338594913483 + 0.1 * 6.738648891448975
Epoch 460, val loss: 0.9292542338371277
Epoch 470, training loss: 0.7292647957801819 = 0.05769742652773857 + 0.1 * 6.715673923492432
Epoch 470, val loss: 0.9435611367225647
Epoch 480, training loss: 0.7249466776847839 = 0.0532216839492321 + 0.1 * 6.717249870300293
Epoch 480, val loss: 0.9576519727706909
Epoch 490, training loss: 0.7203044891357422 = 0.04922199994325638 + 0.1 * 6.710824966430664
Epoch 490, val loss: 0.9714592695236206
Epoch 500, training loss: 0.7165473699569702 = 0.045615896582603455 + 0.1 * 6.709314346313477
Epoch 500, val loss: 0.9850907921791077
Epoch 510, training loss: 0.7133588790893555 = 0.04237888753414154 + 0.1 * 6.709799766540527
Epoch 510, val loss: 0.9983471035957336
Epoch 520, training loss: 0.7077760100364685 = 0.039457544684410095 + 0.1 * 6.6831841468811035
Epoch 520, val loss: 1.0112690925598145
Epoch 530, training loss: 0.7066166996955872 = 0.036807190626859665 + 0.1 * 6.698094844818115
Epoch 530, val loss: 1.023986577987671
Epoch 540, training loss: 0.7024901509284973 = 0.034414421766996384 + 0.1 * 6.680757522583008
Epoch 540, val loss: 1.0362952947616577
Epoch 550, training loss: 0.698546290397644 = 0.03224652633070946 + 0.1 * 6.662997245788574
Epoch 550, val loss: 1.0481809377670288
Epoch 560, training loss: 0.6968133449554443 = 0.030269311740994453 + 0.1 * 6.665440082550049
Epoch 560, val loss: 1.0598677396774292
Epoch 570, training loss: 0.6947439312934875 = 0.028467752039432526 + 0.1 * 6.662761211395264
Epoch 570, val loss: 1.0712482929229736
Epoch 580, training loss: 0.6921190619468689 = 0.026824738830327988 + 0.1 * 6.652943134307861
Epoch 580, val loss: 1.082203984260559
Epoch 590, training loss: 0.6905831694602966 = 0.025317346677184105 + 0.1 * 6.652657985687256
Epoch 590, val loss: 1.0930273532867432
Epoch 600, training loss: 0.6892731189727783 = 0.023936176672577858 + 0.1 * 6.653369426727295
Epoch 600, val loss: 1.1034666299819946
Epoch 610, training loss: 0.6871277093887329 = 0.022670187056064606 + 0.1 * 6.644575119018555
Epoch 610, val loss: 1.113513708114624
Epoch 620, training loss: 0.685360848903656 = 0.02150346338748932 + 0.1 * 6.63857364654541
Epoch 620, val loss: 1.1234010457992554
Epoch 630, training loss: 0.6839523315429688 = 0.020427029579877853 + 0.1 * 6.635252952575684
Epoch 630, val loss: 1.133007526397705
Epoch 640, training loss: 0.6824816465377808 = 0.019432980567216873 + 0.1 * 6.630486965179443
Epoch 640, val loss: 1.142371654510498
Epoch 650, training loss: 0.679914653301239 = 0.018511615693569183 + 0.1 * 6.614030361175537
Epoch 650, val loss: 1.1515252590179443
Epoch 660, training loss: 0.6809917688369751 = 0.017656156793236732 + 0.1 * 6.633356094360352
Epoch 660, val loss: 1.1604207754135132
Epoch 670, training loss: 0.6792863011360168 = 0.016865098848938942 + 0.1 * 6.624212265014648
Epoch 670, val loss: 1.1691980361938477
Epoch 680, training loss: 0.6768652200698853 = 0.01613190583884716 + 0.1 * 6.607333183288574
Epoch 680, val loss: 1.1774872541427612
Epoch 690, training loss: 0.6774486899375916 = 0.015445754863321781 + 0.1 * 6.620028972625732
Epoch 690, val loss: 1.185762882232666
Epoch 700, training loss: 0.6748996376991272 = 0.014805653132498264 + 0.1 * 6.6009392738342285
Epoch 700, val loss: 1.1938294172286987
Epoch 710, training loss: 0.6761157512664795 = 0.01420547254383564 + 0.1 * 6.619102954864502
Epoch 710, val loss: 1.2017902135849
Epoch 720, training loss: 0.672650933265686 = 0.013643854297697544 + 0.1 * 6.590070724487305
Epoch 720, val loss: 1.2094844579696655
Epoch 730, training loss: 0.6714845895767212 = 0.013118201866745949 + 0.1 * 6.5836639404296875
Epoch 730, val loss: 1.217047095298767
Epoch 740, training loss: 0.6728758215904236 = 0.012624107301235199 + 0.1 * 6.602517127990723
Epoch 740, val loss: 1.2244882583618164
Epoch 750, training loss: 0.6714276075363159 = 0.01215833704918623 + 0.1 * 6.5926923751831055
Epoch 750, val loss: 1.2317126989364624
Epoch 760, training loss: 0.669414758682251 = 0.01172166969627142 + 0.1 * 6.576930522918701
Epoch 760, val loss: 1.2387769222259521
Epoch 770, training loss: 0.6686092615127563 = 0.0113086998462677 + 0.1 * 6.573005676269531
Epoch 770, val loss: 1.2457380294799805
Epoch 780, training loss: 0.669907808303833 = 0.010920708067715168 + 0.1 * 6.589870929718018
Epoch 780, val loss: 1.2525219917297363
Epoch 790, training loss: 0.6672775745391846 = 0.010553920641541481 + 0.1 * 6.56723690032959
Epoch 790, val loss: 1.259161353111267
Epoch 800, training loss: 0.6675814390182495 = 0.01020736899226904 + 0.1 * 6.5737409591674805
Epoch 800, val loss: 1.265745759010315
Epoch 810, training loss: 0.6670532822608948 = 0.009878315962851048 + 0.1 * 6.571749210357666
Epoch 810, val loss: 1.2722183465957642
Epoch 820, training loss: 0.666024386882782 = 0.009566965512931347 + 0.1 * 6.564574241638184
Epoch 820, val loss: 1.2784466743469238
Epoch 830, training loss: 0.6642788052558899 = 0.009271292947232723 + 0.1 * 6.550075531005859
Epoch 830, val loss: 1.2846955060958862
Epoch 840, training loss: 0.6644622683525085 = 0.008990955539047718 + 0.1 * 6.554713249206543
Epoch 840, val loss: 1.2907899618148804
Epoch 850, training loss: 0.6622438430786133 = 0.008723907172679901 + 0.1 * 6.535199165344238
Epoch 850, val loss: 1.296712875366211
Epoch 860, training loss: 0.6644216179847717 = 0.008469202555716038 + 0.1 * 6.559524059295654
Epoch 860, val loss: 1.3025873899459839
Epoch 870, training loss: 0.6617815494537354 = 0.008226403035223484 + 0.1 * 6.53555154800415
Epoch 870, val loss: 1.3084052801132202
Epoch 880, training loss: 0.6618193984031677 = 0.00799511093646288 + 0.1 * 6.538242816925049
Epoch 880, val loss: 1.3141030073165894
Epoch 890, training loss: 0.6610811948776245 = 0.00777632649987936 + 0.1 * 6.533048629760742
Epoch 890, val loss: 1.3195432424545288
Epoch 900, training loss: 0.6602267622947693 = 0.007567316759377718 + 0.1 * 6.526594161987305
Epoch 900, val loss: 1.3248769044876099
Epoch 910, training loss: 0.6610798835754395 = 0.007366680074483156 + 0.1 * 6.537132263183594
Epoch 910, val loss: 1.3301751613616943
Epoch 920, training loss: 0.6586596369743347 = 0.007174480706453323 + 0.1 * 6.514851093292236
Epoch 920, val loss: 1.3354934453964233
Epoch 930, training loss: 0.6603316068649292 = 0.006990403868257999 + 0.1 * 6.533411502838135
Epoch 930, val loss: 1.3406903743743896
Epoch 940, training loss: 0.658388078212738 = 0.006814869586378336 + 0.1 * 6.5157318115234375
Epoch 940, val loss: 1.3457716703414917
Epoch 950, training loss: 0.6586657166481018 = 0.00664652232080698 + 0.1 * 6.520191669464111
Epoch 950, val loss: 1.3507355451583862
Epoch 960, training loss: 0.660933256149292 = 0.006484928075224161 + 0.1 * 6.544483184814453
Epoch 960, val loss: 1.3556938171386719
Epoch 970, training loss: 0.6579396724700928 = 0.006330573931336403 + 0.1 * 6.516091346740723
Epoch 970, val loss: 1.3605831861495972
Epoch 980, training loss: 0.6569584012031555 = 0.006181759759783745 + 0.1 * 6.507766246795654
Epoch 980, val loss: 1.3653624057769775
Epoch 990, training loss: 0.65762859582901 = 0.006038816645741463 + 0.1 * 6.515897274017334
Epoch 990, val loss: 1.3701201677322388
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8002108592514497
The final CL Acc:0.77407, 0.01814, The final GNN Acc:0.80636, 0.00692
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13178])
remove edge: torch.Size([2, 8038])
updated graph: torch.Size([2, 10660])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.7952330112457275 = 1.9355450868606567 + 0.1 * 8.596879005432129
Epoch 0, val loss: 1.9383870363235474
Epoch 10, training loss: 2.7855184078216553 = 1.925835371017456 + 0.1 * 8.596829414367676
Epoch 10, val loss: 1.9283661842346191
Epoch 20, training loss: 2.7735676765441895 = 1.9139106273651123 + 0.1 * 8.59657096862793
Epoch 20, val loss: 1.916025996208191
Epoch 30, training loss: 2.756700038909912 = 1.8972560167312622 + 0.1 * 8.594439506530762
Epoch 30, val loss: 1.898848295211792
Epoch 40, training loss: 2.7302637100219727 = 1.8726307153701782 + 0.1 * 8.576329231262207
Epoch 40, val loss: 1.8738950490951538
Epoch 50, training loss: 2.686971664428711 = 1.8380439281463623 + 0.1 * 8.489275932312012
Epoch 50, val loss: 1.8403263092041016
Epoch 60, training loss: 2.62294864654541 = 1.798920750617981 + 0.1 * 8.240279197692871
Epoch 60, val loss: 1.804629921913147
Epoch 70, training loss: 2.5698678493499756 = 1.7613624334335327 + 0.1 * 8.085054397583008
Epoch 70, val loss: 1.7712453603744507
Epoch 80, training loss: 2.50530743598938 = 1.7174817323684692 + 0.1 * 7.878256797790527
Epoch 80, val loss: 1.7304794788360596
Epoch 90, training loss: 2.4317333698272705 = 1.660465955734253 + 0.1 * 7.712673187255859
Epoch 90, val loss: 1.6801996231079102
Epoch 100, training loss: 2.341200590133667 = 1.5885943174362183 + 0.1 * 7.526062488555908
Epoch 100, val loss: 1.6211318969726562
Epoch 110, training loss: 2.241483688354492 = 1.5048983097076416 + 0.1 * 7.3658528327941895
Epoch 110, val loss: 1.5495015382766724
Epoch 120, training loss: 2.139328718185425 = 1.4154585599899292 + 0.1 * 7.238701820373535
Epoch 120, val loss: 1.471382975578308
Epoch 130, training loss: 2.0413575172424316 = 1.3258193731307983 + 0.1 * 7.1553802490234375
Epoch 130, val loss: 1.3935450315475464
Epoch 140, training loss: 1.9470716714859009 = 1.2358256578445435 + 0.1 * 7.112460136413574
Epoch 140, val loss: 1.3180029392242432
Epoch 150, training loss: 1.8547279834747314 = 1.146593689918518 + 0.1 * 7.081343173980713
Epoch 150, val loss: 1.2446730136871338
Epoch 160, training loss: 1.76649010181427 = 1.0614360570907593 + 0.1 * 7.050540447235107
Epoch 160, val loss: 1.1775091886520386
Epoch 170, training loss: 1.6836082935333252 = 0.9807193279266357 + 0.1 * 7.028890132904053
Epoch 170, val loss: 1.1159017086029053
Epoch 180, training loss: 1.6062064170837402 = 0.9046007394790649 + 0.1 * 7.01605749130249
Epoch 180, val loss: 1.0589289665222168
Epoch 190, training loss: 1.5333621501922607 = 0.833930253982544 + 0.1 * 6.994318008422852
Epoch 190, val loss: 1.0069119930267334
Epoch 200, training loss: 1.4657816886901855 = 0.7678843140602112 + 0.1 * 6.978972911834717
Epoch 200, val loss: 0.9588867425918579
Epoch 210, training loss: 1.4026999473571777 = 0.7062406539916992 + 0.1 * 6.964591979980469
Epoch 210, val loss: 0.9152395129203796
Epoch 220, training loss: 1.343849539756775 = 0.6487003564834595 + 0.1 * 6.951491832733154
Epoch 220, val loss: 0.8760905265808105
Epoch 230, training loss: 1.2890658378601074 = 0.5949556827545166 + 0.1 * 6.94110107421875
Epoch 230, val loss: 0.8417092561721802
Epoch 240, training loss: 1.238551139831543 = 0.5452579855918884 + 0.1 * 6.932931423187256
Epoch 240, val loss: 0.8119677305221558
Epoch 250, training loss: 1.1920181512832642 = 0.49934813380241394 + 0.1 * 6.926700115203857
Epoch 250, val loss: 0.786625862121582
Epoch 260, training loss: 1.1485919952392578 = 0.45673874020576477 + 0.1 * 6.918532848358154
Epoch 260, val loss: 0.7650746703147888
Epoch 270, training loss: 1.1086206436157227 = 0.41667959094047546 + 0.1 * 6.919410228729248
Epoch 270, val loss: 0.7469619512557983
Epoch 280, training loss: 1.0691946744918823 = 0.3783051669597626 + 0.1 * 6.908895492553711
Epoch 280, val loss: 0.731645405292511
Epoch 290, training loss: 1.031234622001648 = 0.340848833322525 + 0.1 * 6.903858184814453
Epoch 290, val loss: 0.7186062335968018
Epoch 300, training loss: 0.9950000047683716 = 0.3043116629123688 + 0.1 * 6.906883239746094
Epoch 300, val loss: 0.7078241109848022
Epoch 310, training loss: 0.9590398073196411 = 0.26943090558052063 + 0.1 * 6.89608907699585
Epoch 310, val loss: 0.699459433555603
Epoch 320, training loss: 0.9261358976364136 = 0.23690815269947052 + 0.1 * 6.892277717590332
Epoch 320, val loss: 0.6937336921691895
Epoch 330, training loss: 0.8963382840156555 = 0.20746290683746338 + 0.1 * 6.888753414154053
Epoch 330, val loss: 0.6908887028694153
Epoch 340, training loss: 0.8698310256004333 = 0.1815684586763382 + 0.1 * 6.882625579833984
Epoch 340, val loss: 0.6907686591148376
Epoch 350, training loss: 0.8473948240280151 = 0.15917719900608063 + 0.1 * 6.882176399230957
Epoch 350, val loss: 0.6931131482124329
Epoch 360, training loss: 0.827698826789856 = 0.1399819552898407 + 0.1 * 6.877169132232666
Epoch 360, val loss: 0.6975873112678528
Epoch 370, training loss: 0.8104103207588196 = 0.12352029234170914 + 0.1 * 6.868899822235107
Epoch 370, val loss: 0.7037876844406128
Epoch 380, training loss: 0.7961225509643555 = 0.10931006073951721 + 0.1 * 6.868124485015869
Epoch 380, val loss: 0.7112622857093811
Epoch 390, training loss: 0.7828363180160522 = 0.0970701351761818 + 0.1 * 6.857661247253418
Epoch 390, val loss: 0.7197887897491455
Epoch 400, training loss: 0.7714790105819702 = 0.0864628478884697 + 0.1 * 6.850161552429199
Epoch 400, val loss: 0.7290076017379761
Epoch 410, training loss: 0.7619160413742065 = 0.07724129408597946 + 0.1 * 6.846747398376465
Epoch 410, val loss: 0.7387714385986328
Epoch 420, training loss: 0.7532953023910522 = 0.06924121081829071 + 0.1 * 6.840540409088135
Epoch 420, val loss: 0.7490424513816833
Epoch 430, training loss: 0.7455946207046509 = 0.06229493021965027 + 0.1 * 6.832996845245361
Epoch 430, val loss: 0.7593786120414734
Epoch 440, training loss: 0.7385398745536804 = 0.056255850940942764 + 0.1 * 6.822839736938477
Epoch 440, val loss: 0.7698681950569153
Epoch 450, training loss: 0.7326976656913757 = 0.05099212005734444 + 0.1 * 6.8170552253723145
Epoch 450, val loss: 0.7803512215614319
Epoch 460, training loss: 0.7285382151603699 = 0.04638359323143959 + 0.1 * 6.821545600891113
Epoch 460, val loss: 0.7906807065010071
Epoch 470, training loss: 0.7230517268180847 = 0.04235466569662094 + 0.1 * 6.806970596313477
Epoch 470, val loss: 0.8009512424468994
Epoch 480, training loss: 0.718517005443573 = 0.0388023741543293 + 0.1 * 6.797145843505859
Epoch 480, val loss: 0.8111042976379395
Epoch 490, training loss: 0.7144340872764587 = 0.035663630813360214 + 0.1 * 6.787704944610596
Epoch 490, val loss: 0.821027934551239
Epoch 500, training loss: 0.7117274403572083 = 0.032883938401937485 + 0.1 * 6.788434982299805
Epoch 500, val loss: 0.8306808471679688
Epoch 510, training loss: 0.7081767916679382 = 0.030414652079343796 + 0.1 * 6.777621269226074
Epoch 510, val loss: 0.8401482105255127
Epoch 520, training loss: 0.7050349116325378 = 0.028214162215590477 + 0.1 * 6.768207550048828
Epoch 520, val loss: 0.8493621945381165
Epoch 530, training loss: 0.704413115978241 = 0.026241682469844818 + 0.1 * 6.78171443939209
Epoch 530, val loss: 0.8582583665847778
Epoch 540, training loss: 0.7000362277030945 = 0.0244727935642004 + 0.1 * 6.755634307861328
Epoch 540, val loss: 0.8668481707572937
Epoch 550, training loss: 0.6976591348648071 = 0.022878998890519142 + 0.1 * 6.747801303863525
Epoch 550, val loss: 0.875287652015686
Epoch 560, training loss: 0.6971429586410522 = 0.021435661241412163 + 0.1 * 6.757072448730469
Epoch 560, val loss: 0.8833891153335571
Epoch 570, training loss: 0.6942028403282166 = 0.02013237774372101 + 0.1 * 6.740704536437988
Epoch 570, val loss: 0.8912428021430969
Epoch 580, training loss: 0.6923906207084656 = 0.018948959186673164 + 0.1 * 6.734416484832764
Epoch 580, val loss: 0.8988428115844727
Epoch 590, training loss: 0.6912433505058289 = 0.01786942221224308 + 0.1 * 6.733739376068115
Epoch 590, val loss: 0.9062196612358093
Epoch 600, training loss: 0.6897045373916626 = 0.01688305102288723 + 0.1 * 6.728214740753174
Epoch 600, val loss: 0.9132827520370483
Epoch 610, training loss: 0.6878507137298584 = 0.015979059040546417 + 0.1 * 6.718716621398926
Epoch 610, val loss: 0.9202108383178711
Epoch 620, training loss: 0.6878950595855713 = 0.015149555169045925 + 0.1 * 6.727455139160156
Epoch 620, val loss: 0.9269814491271973
Epoch 630, training loss: 0.685676097869873 = 0.014389210380613804 + 0.1 * 6.712868690490723
Epoch 630, val loss: 0.9333964586257935
Epoch 640, training loss: 0.6855375170707703 = 0.013687619008123875 + 0.1 * 6.718498706817627
Epoch 640, val loss: 0.9396514296531677
Epoch 650, training loss: 0.6840236783027649 = 0.013040239922702312 + 0.1 * 6.709834098815918
Epoch 650, val loss: 0.9457911252975464
Epoch 660, training loss: 0.6822729706764221 = 0.012439650483429432 + 0.1 * 6.698332786560059
Epoch 660, val loss: 0.9516687393188477
Epoch 670, training loss: 0.682131290435791 = 0.011881117708981037 + 0.1 * 6.7025017738342285
Epoch 670, val loss: 0.9574199318885803
Epoch 680, training loss: 0.6809004545211792 = 0.01136248093098402 + 0.1 * 6.695379257202148
Epoch 680, val loss: 0.9630420804023743
Epoch 690, training loss: 0.6801027059555054 = 0.010880730114877224 + 0.1 * 6.6922197341918945
Epoch 690, val loss: 0.9683971405029297
Epoch 700, training loss: 0.6789261102676392 = 0.010431288741528988 + 0.1 * 6.684947967529297
Epoch 700, val loss: 0.973774790763855
Epoch 710, training loss: 0.6793516278266907 = 0.010010622441768646 + 0.1 * 6.693410396575928
Epoch 710, val loss: 0.978827714920044
Epoch 720, training loss: 0.678071916103363 = 0.009619072079658508 + 0.1 * 6.684528350830078
Epoch 720, val loss: 0.9837959408760071
Epoch 730, training loss: 0.6769223213195801 = 0.009252688847482204 + 0.1 * 6.676696300506592
Epoch 730, val loss: 0.9885764718055725
Epoch 740, training loss: 0.6768592596054077 = 0.008908417075872421 + 0.1 * 6.679508686065674
Epoch 740, val loss: 0.9932681322097778
Epoch 750, training loss: 0.6758187413215637 = 0.00858509074896574 + 0.1 * 6.672336578369141
Epoch 750, val loss: 0.9978599548339844
Epoch 760, training loss: 0.6795276403427124 = 0.008280563168227673 + 0.1 * 6.712470531463623
Epoch 760, val loss: 1.0022668838500977
Epoch 770, training loss: 0.6738574504852295 = 0.007995452731847763 + 0.1 * 6.6586198806762695
Epoch 770, val loss: 1.0065162181854248
Epoch 780, training loss: 0.673764705657959 = 0.007727404590696096 + 0.1 * 6.660373210906982
Epoch 780, val loss: 1.010748267173767
Epoch 790, training loss: 0.6728039383888245 = 0.007473036181181669 + 0.1 * 6.653308868408203
Epoch 790, val loss: 1.0148844718933105
Epoch 800, training loss: 0.6725387573242188 = 0.007231987547129393 + 0.1 * 6.653067588806152
Epoch 800, val loss: 1.0188733339309692
Epoch 810, training loss: 0.6720573902130127 = 0.007003961596637964 + 0.1 * 6.650534152984619
Epoch 810, val loss: 1.0227046012878418
Epoch 820, training loss: 0.6727103590965271 = 0.006788390688598156 + 0.1 * 6.659219741821289
Epoch 820, val loss: 1.0265992879867554
Epoch 830, training loss: 0.6708086133003235 = 0.006583808921277523 + 0.1 * 6.642247676849365
Epoch 830, val loss: 1.030301809310913
Epoch 840, training loss: 0.6706346869468689 = 0.006389523856341839 + 0.1 * 6.642451763153076
Epoch 840, val loss: 1.033907413482666
Epoch 850, training loss: 0.6716330647468567 = 0.0062048789113759995 + 0.1 * 6.6542816162109375
Epoch 850, val loss: 1.037481427192688
Epoch 860, training loss: 0.6689822673797607 = 0.006030134856700897 + 0.1 * 6.629521369934082
Epoch 860, val loss: 1.0408908128738403
Epoch 870, training loss: 0.6685854196548462 = 0.005863707046955824 + 0.1 * 6.627216815948486
Epoch 870, val loss: 1.0442464351654053
Epoch 880, training loss: 0.6678059101104736 = 0.005705156829208136 + 0.1 * 6.621006965637207
Epoch 880, val loss: 1.047600507736206
Epoch 890, training loss: 0.6707822680473328 = 0.0055529335513710976 + 0.1 * 6.6522932052612305
Epoch 890, val loss: 1.0508227348327637
Epoch 900, training loss: 0.6666126847267151 = 0.0054084500297904015 + 0.1 * 6.61204195022583
Epoch 900, val loss: 1.0538798570632935
Epoch 910, training loss: 0.6669982075691223 = 0.005271391477435827 + 0.1 * 6.617268085479736
Epoch 910, val loss: 1.0570106506347656
Epoch 920, training loss: 0.6656460762023926 = 0.005140257067978382 + 0.1 * 6.605058193206787
Epoch 920, val loss: 1.0601072311401367
Epoch 930, training loss: 0.665650486946106 = 0.0050136675126850605 + 0.1 * 6.606368541717529
Epoch 930, val loss: 1.0630815029144287
Epoch 940, training loss: 0.6648374199867249 = 0.004893190693110228 + 0.1 * 6.599442005157471
Epoch 940, val loss: 1.0659593343734741
Epoch 950, training loss: 0.6654853224754333 = 0.004777730442583561 + 0.1 * 6.6070756912231445
Epoch 950, val loss: 1.0688234567642212
Epoch 960, training loss: 0.6646547317504883 = 0.004666171967983246 + 0.1 * 6.5998854637146
Epoch 960, val loss: 1.0716683864593506
Epoch 970, training loss: 0.6642531752586365 = 0.004559725057333708 + 0.1 * 6.596934795379639
Epoch 970, val loss: 1.0743563175201416
Epoch 980, training loss: 0.664176881313324 = 0.0044571030884981155 + 0.1 * 6.597197532653809
Epoch 980, val loss: 1.0771262645721436
Epoch 990, training loss: 0.6639779806137085 = 0.004358392674475908 + 0.1 * 6.596195697784424
Epoch 990, val loss: 1.0797653198242188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.808776617050171 = 1.949090838432312 + 0.1 * 8.596857070922852
Epoch 0, val loss: 1.9499330520629883
Epoch 10, training loss: 2.7986185550689697 = 1.9389393329620361 + 0.1 * 8.596792221069336
Epoch 10, val loss: 1.9398077726364136
Epoch 20, training loss: 2.786116600036621 = 1.9264767169952393 + 0.1 * 8.59639835357666
Epoch 20, val loss: 1.9271275997161865
Epoch 30, training loss: 2.7682833671569824 = 1.9089903831481934 + 0.1 * 8.59292984008789
Epoch 30, val loss: 1.909417986869812
Epoch 40, training loss: 2.7401034832000732 = 1.8833913803100586 + 0.1 * 8.567121505737305
Epoch 40, val loss: 1.8838833570480347
Epoch 50, training loss: 2.692230701446533 = 1.8481467962265015 + 0.1 * 8.440838813781738
Epoch 50, val loss: 1.8500887155532837
Epoch 60, training loss: 2.6288938522338867 = 1.8085099458694458 + 0.1 * 8.203839302062988
Epoch 60, val loss: 1.8142085075378418
Epoch 70, training loss: 2.574009418487549 = 1.7697780132293701 + 0.1 * 8.042314529418945
Epoch 70, val loss: 1.7799220085144043
Epoch 80, training loss: 2.509000062942505 = 1.7255167961120605 + 0.1 * 7.834832191467285
Epoch 80, val loss: 1.7383747100830078
Epoch 90, training loss: 2.4398574829101562 = 1.6695674657821655 + 0.1 * 7.702901363372803
Epoch 90, val loss: 1.6881048679351807
Epoch 100, training loss: 2.3515961170196533 = 1.5993256568908691 + 0.1 * 7.522704601287842
Epoch 100, val loss: 1.6274917125701904
Epoch 110, training loss: 2.252469062805176 = 1.5154709815979004 + 0.1 * 7.369980812072754
Epoch 110, val loss: 1.5539286136627197
Epoch 120, training loss: 2.1541197299957275 = 1.4264874458312988 + 0.1 * 7.276322841644287
Epoch 120, val loss: 1.4797608852386475
Epoch 130, training loss: 2.058539390563965 = 1.3403464555740356 + 0.1 * 7.1819305419921875
Epoch 130, val loss: 1.4107004404067993
Epoch 140, training loss: 1.967329502105713 = 1.2561752796173096 + 0.1 * 7.111542224884033
Epoch 140, val loss: 1.3431353569030762
Epoch 150, training loss: 1.8811566829681396 = 1.1749992370605469 + 0.1 * 7.061574459075928
Epoch 150, val loss: 1.2792173624038696
Epoch 160, training loss: 1.802657127380371 = 1.1005306243896484 + 0.1 * 7.021265506744385
Epoch 160, val loss: 1.223638892173767
Epoch 170, training loss: 1.7316067218780518 = 1.032849669456482 + 0.1 * 6.987570762634277
Epoch 170, val loss: 1.175704836845398
Epoch 180, training loss: 1.6682121753692627 = 0.9714996218681335 + 0.1 * 6.967124938964844
Epoch 180, val loss: 1.1336954832077026
Epoch 190, training loss: 1.6101258993148804 = 0.9157039523124695 + 0.1 * 6.94421911239624
Epoch 190, val loss: 1.0959872007369995
Epoch 200, training loss: 1.5549869537353516 = 0.8622229695320129 + 0.1 * 6.927639961242676
Epoch 200, val loss: 1.0594384670257568
Epoch 210, training loss: 1.4996581077575684 = 0.8081441521644592 + 0.1 * 6.915139675140381
Epoch 210, val loss: 1.022303581237793
Epoch 220, training loss: 1.4430327415466309 = 0.7526035308837891 + 0.1 * 6.904291152954102
Epoch 220, val loss: 0.9843129515647888
Epoch 230, training loss: 1.3852375745773315 = 0.6955274343490601 + 0.1 * 6.897101402282715
Epoch 230, val loss: 0.9456208348274231
Epoch 240, training loss: 1.3273273706436157 = 0.6383955478668213 + 0.1 * 6.889317989349365
Epoch 240, val loss: 0.908079206943512
Epoch 250, training loss: 1.2737390995025635 = 0.5831959843635559 + 0.1 * 6.905431747436523
Epoch 250, val loss: 0.87394118309021
Epoch 260, training loss: 1.2217843532562256 = 0.5328879356384277 + 0.1 * 6.888963222503662
Epoch 260, val loss: 0.8457273840904236
Epoch 270, training loss: 1.174881935119629 = 0.48768144845962524 + 0.1 * 6.872005462646484
Epoch 270, val loss: 0.8238843679428101
Epoch 280, training loss: 1.1337307691574097 = 0.44710320234298706 + 0.1 * 6.866275787353516
Epoch 280, val loss: 0.8078433275222778
Epoch 290, training loss: 1.0962042808532715 = 0.4103597402572632 + 0.1 * 6.85844612121582
Epoch 290, val loss: 0.7962663173675537
Epoch 300, training loss: 1.0626556873321533 = 0.376664936542511 + 0.1 * 6.859907150268555
Epoch 300, val loss: 0.7881485819816589
Epoch 310, training loss: 1.0298866033554077 = 0.3453122675418854 + 0.1 * 6.845743179321289
Epoch 310, val loss: 0.7823923826217651
Epoch 320, training loss: 0.9991846084594727 = 0.31533297896385193 + 0.1 * 6.838515758514404
Epoch 320, val loss: 0.778111457824707
Epoch 330, training loss: 0.9693379998207092 = 0.286126971244812 + 0.1 * 6.832110404968262
Epoch 330, val loss: 0.7746848464012146
Epoch 340, training loss: 0.9402488470077515 = 0.25765544176101685 + 0.1 * 6.825933933258057
Epoch 340, val loss: 0.7720153331756592
Epoch 350, training loss: 0.9119188785552979 = 0.23027107119560242 + 0.1 * 6.816478252410889
Epoch 350, val loss: 0.7702668309211731
Epoch 360, training loss: 0.8861352205276489 = 0.2044578492641449 + 0.1 * 6.816773414611816
Epoch 360, val loss: 0.7697886228561401
Epoch 370, training loss: 0.8615491986274719 = 0.1810198873281479 + 0.1 * 6.80529260635376
Epoch 370, val loss: 0.77098548412323
Epoch 380, training loss: 0.840356707572937 = 0.1603143811225891 + 0.1 * 6.8004231452941895
Epoch 380, val loss: 0.7739906311035156
Epoch 390, training loss: 0.8215728402137756 = 0.1423109769821167 + 0.1 * 6.792618274688721
Epoch 390, val loss: 0.7788952589035034
Epoch 400, training loss: 0.805482804775238 = 0.12676723301410675 + 0.1 * 6.787155628204346
Epoch 400, val loss: 0.785396158695221
Epoch 410, training loss: 0.7914522290229797 = 0.11337768286466599 + 0.1 * 6.780745029449463
Epoch 410, val loss: 0.7931486964225769
Epoch 420, training loss: 0.779585599899292 = 0.10181010514497757 + 0.1 * 6.777754783630371
Epoch 420, val loss: 0.8018694519996643
Epoch 430, training loss: 0.7688924074172974 = 0.09173266589641571 + 0.1 * 6.771597385406494
Epoch 430, val loss: 0.8114615082740784
Epoch 440, training loss: 0.7600008249282837 = 0.08292610943317413 + 0.1 * 6.770747184753418
Epoch 440, val loss: 0.8216654658317566
Epoch 450, training loss: 0.751420259475708 = 0.0752023458480835 + 0.1 * 6.762178897857666
Epoch 450, val loss: 0.8323145508766174
Epoch 460, training loss: 0.7439974546432495 = 0.0683947280049324 + 0.1 * 6.7560272216796875
Epoch 460, val loss: 0.8434287905693054
Epoch 470, training loss: 0.7381649613380432 = 0.0623774416744709 + 0.1 * 6.757874965667725
Epoch 470, val loss: 0.8548536896705627
Epoch 480, training loss: 0.7324743270874023 = 0.0570683479309082 + 0.1 * 6.754059791564941
Epoch 480, val loss: 0.8664613962173462
Epoch 490, training loss: 0.7268621921539307 = 0.05236664041876793 + 0.1 * 6.744955539703369
Epoch 490, val loss: 0.8781025409698486
Epoch 500, training loss: 0.7225634455680847 = 0.04818987846374512 + 0.1 * 6.7437357902526855
Epoch 500, val loss: 0.8897836208343506
Epoch 510, training loss: 0.7181480526924133 = 0.0444733127951622 + 0.1 * 6.736747741699219
Epoch 510, val loss: 0.9013981819152832
Epoch 520, training loss: 0.7142892479896545 = 0.04115105792880058 + 0.1 * 6.731381893157959
Epoch 520, val loss: 0.9129109978675842
Epoch 530, training loss: 0.7107177376747131 = 0.03817383572459221 + 0.1 * 6.725438594818115
Epoch 530, val loss: 0.924235463142395
Epoch 540, training loss: 0.7076782584190369 = 0.03550032898783684 + 0.1 * 6.721778869628906
Epoch 540, val loss: 0.9354256391525269
Epoch 550, training loss: 0.7049084901809692 = 0.0330936461687088 + 0.1 * 6.718148231506348
Epoch 550, val loss: 0.9464489221572876
Epoch 560, training loss: 0.7042639255523682 = 0.030915575101971626 + 0.1 * 6.73348331451416
Epoch 560, val loss: 0.95725017786026
Epoch 570, training loss: 0.7000580430030823 = 0.028951456770300865 + 0.1 * 6.711065769195557
Epoch 570, val loss: 0.9676864147186279
Epoch 580, training loss: 0.69765704870224 = 0.027172312140464783 + 0.1 * 6.70484733581543
Epoch 580, val loss: 0.9778671860694885
Epoch 590, training loss: 0.6957379579544067 = 0.025552207604050636 + 0.1 * 6.701857089996338
Epoch 590, val loss: 0.9879899621009827
Epoch 600, training loss: 0.6939362287521362 = 0.024070659652352333 + 0.1 * 6.698655128479004
Epoch 600, val loss: 0.9978656768798828
Epoch 610, training loss: 0.692449688911438 = 0.022716449573636055 + 0.1 * 6.69733190536499
Epoch 610, val loss: 1.0074559450149536
Epoch 620, training loss: 0.6904844641685486 = 0.02147604525089264 + 0.1 * 6.690083980560303
Epoch 620, val loss: 1.0168620347976685
Epoch 630, training loss: 0.6909085512161255 = 0.020335756242275238 + 0.1 * 6.705728054046631
Epoch 630, val loss: 1.026063084602356
Epoch 640, training loss: 0.687856137752533 = 0.019288888201117516 + 0.1 * 6.685672283172607
Epoch 640, val loss: 1.0349103212356567
Epoch 650, training loss: 0.6865041851997375 = 0.018324552103877068 + 0.1 * 6.681796073913574
Epoch 650, val loss: 1.0435569286346436
Epoch 660, training loss: 0.6849234700202942 = 0.01743333786725998 + 0.1 * 6.674901008605957
Epoch 660, val loss: 1.0519675016403198
Epoch 670, training loss: 0.6838494539260864 = 0.01661035418510437 + 0.1 * 6.672391414642334
Epoch 670, val loss: 1.0602689981460571
Epoch 680, training loss: 0.6826561689376831 = 0.01584572345018387 + 0.1 * 6.66810417175293
Epoch 680, val loss: 1.0683327913284302
Epoch 690, training loss: 0.6813941597938538 = 0.015135477297008038 + 0.1 * 6.662586688995361
Epoch 690, val loss: 1.0761516094207764
Epoch 700, training loss: 0.6806595325469971 = 0.01447542104870081 + 0.1 * 6.661840915679932
Epoch 700, val loss: 1.0838817358016968
Epoch 710, training loss: 0.682124137878418 = 0.013860389590263367 + 0.1 * 6.6826372146606445
Epoch 710, val loss: 1.0912859439849854
Epoch 720, training loss: 0.678790807723999 = 0.013285918161273003 + 0.1 * 6.655048847198486
Epoch 720, val loss: 1.0985455513000488
Epoch 730, training loss: 0.6778069138526917 = 0.012750298716127872 + 0.1 * 6.6505656242370605
Epoch 730, val loss: 1.1056029796600342
Epoch 740, training loss: 0.6776003837585449 = 0.012248570099473 + 0.1 * 6.653517723083496
Epoch 740, val loss: 1.1125277280807495
Epoch 750, training loss: 0.6769296526908875 = 0.011776858009397984 + 0.1 * 6.6515278816223145
Epoch 750, val loss: 1.119246244430542
Epoch 760, training loss: 0.6753934621810913 = 0.011334570124745369 + 0.1 * 6.640588760375977
Epoch 760, val loss: 1.1259047985076904
Epoch 770, training loss: 0.6757706999778748 = 0.01091888640075922 + 0.1 * 6.648517608642578
Epoch 770, val loss: 1.1322047710418701
Epoch 780, training loss: 0.6745381355285645 = 0.01052655279636383 + 0.1 * 6.640115737915039
Epoch 780, val loss: 1.1385458707809448
Epoch 790, training loss: 0.6731399893760681 = 0.010157682001590729 + 0.1 * 6.629822731018066
Epoch 790, val loss: 1.1445976495742798
Epoch 800, training loss: 0.6725385785102844 = 0.009809057228267193 + 0.1 * 6.627295017242432
Epoch 800, val loss: 1.1505331993103027
Epoch 810, training loss: 0.6720149517059326 = 0.009479674510657787 + 0.1 * 6.625352382659912
Epoch 810, val loss: 1.1563628911972046
Epoch 820, training loss: 0.6716529130935669 = 0.009169849567115307 + 0.1 * 6.62483024597168
Epoch 820, val loss: 1.1619981527328491
Epoch 830, training loss: 0.6707108020782471 = 0.008875478059053421 + 0.1 * 6.618352890014648
Epoch 830, val loss: 1.1675479412078857
Epoch 840, training loss: 0.6714364886283875 = 0.008597607724368572 + 0.1 * 6.628388404846191
Epoch 840, val loss: 1.1730183362960815
Epoch 850, training loss: 0.6701017618179321 = 0.008332346566021442 + 0.1 * 6.617693901062012
Epoch 850, val loss: 1.1781342029571533
Epoch 860, training loss: 0.6685619354248047 = 0.008082181215286255 + 0.1 * 6.604797840118408
Epoch 860, val loss: 1.1834185123443604
Epoch 870, training loss: 0.66957688331604 = 0.007845456711947918 + 0.1 * 6.617314338684082
Epoch 870, val loss: 1.1883769035339355
Epoch 880, training loss: 0.6675354242324829 = 0.007618672214448452 + 0.1 * 6.599167346954346
Epoch 880, val loss: 1.193380355834961
Epoch 890, training loss: 0.6666899919509888 = 0.0074033066630363464 + 0.1 * 6.592866897583008
Epoch 890, val loss: 1.1982357501983643
Epoch 900, training loss: 0.66653972864151 = 0.007198141887784004 + 0.1 * 6.5934157371521
Epoch 900, val loss: 1.2029873132705688
Epoch 910, training loss: 0.6666048765182495 = 0.007001589518040419 + 0.1 * 6.596033096313477
Epoch 910, val loss: 1.2076694965362549
Epoch 920, training loss: 0.6669200658798218 = 0.00681456970050931 + 0.1 * 6.601054668426514
Epoch 920, val loss: 1.2121195793151855
Epoch 930, training loss: 0.664457380771637 = 0.006635683123022318 + 0.1 * 6.578217029571533
Epoch 930, val loss: 1.2166671752929688
Epoch 940, training loss: 0.6660048961639404 = 0.006465180777013302 + 0.1 * 6.595396995544434
Epoch 940, val loss: 1.2211023569107056
Epoch 950, training loss: 0.6646153926849365 = 0.00630105659365654 + 0.1 * 6.58314323425293
Epoch 950, val loss: 1.225179672241211
Epoch 960, training loss: 0.6634140014648438 = 0.006144667975604534 + 0.1 * 6.572693347930908
Epoch 960, val loss: 1.2295299768447876
Epoch 970, training loss: 0.6648617386817932 = 0.00599456112831831 + 0.1 * 6.588671684265137
Epoch 970, val loss: 1.2335820198059082
Epoch 980, training loss: 0.6626697182655334 = 0.005849937442690134 + 0.1 * 6.568197727203369
Epoch 980, val loss: 1.237581729888916
Epoch 990, training loss: 0.6618308424949646 = 0.005712817423045635 + 0.1 * 6.561180114746094
Epoch 990, val loss: 1.2416942119598389
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8323668950975225
=== training gcn model ===
Epoch 0, training loss: 2.790574073791504 = 1.93088698387146 + 0.1 * 8.596871376037598
Epoch 0, val loss: 1.9264609813690186
Epoch 10, training loss: 2.78134822845459 = 1.9216676950454712 + 0.1 * 8.59680461883545
Epoch 10, val loss: 1.9176514148712158
Epoch 20, training loss: 2.770106792449951 = 1.9104608297348022 + 0.1 * 8.596460342407227
Epoch 20, val loss: 1.9065402746200562
Epoch 30, training loss: 2.7544965744018555 = 1.8951143026351929 + 0.1 * 8.59382152557373
Epoch 30, val loss: 1.8909212350845337
Epoch 40, training loss: 2.730064630508423 = 1.8725650310516357 + 0.1 * 8.574995040893555
Epoch 40, val loss: 1.8680139780044556
Epoch 50, training loss: 2.690408706665039 = 1.8406550884246826 + 0.1 * 8.497535705566406
Epoch 50, val loss: 1.8368608951568604
Epoch 60, training loss: 2.624958038330078 = 1.8030664920806885 + 0.1 * 8.218915939331055
Epoch 60, val loss: 1.8028310537338257
Epoch 70, training loss: 2.5715813636779785 = 1.7649240493774414 + 0.1 * 8.066573143005371
Epoch 70, val loss: 1.7706849575042725
Epoch 80, training loss: 2.505321502685547 = 1.7191828489303589 + 0.1 * 7.861387252807617
Epoch 80, val loss: 1.7297983169555664
Epoch 90, training loss: 2.428502082824707 = 1.6594374179840088 + 0.1 * 7.690647125244141
Epoch 90, val loss: 1.678099513053894
Epoch 100, training loss: 2.3340067863464355 = 1.5849604606628418 + 0.1 * 7.490462303161621
Epoch 100, val loss: 1.616962194442749
Epoch 110, training loss: 2.235145092010498 = 1.4984142780303955 + 0.1 * 7.367308616638184
Epoch 110, val loss: 1.5437257289886475
Epoch 120, training loss: 2.1349220275878906 = 1.4082504510879517 + 0.1 * 7.266716957092285
Epoch 120, val loss: 1.4684351682662964
Epoch 130, training loss: 2.039492130279541 = 1.3209598064422607 + 0.1 * 7.185323715209961
Epoch 130, val loss: 1.3960615396499634
Epoch 140, training loss: 1.9490554332733154 = 1.235787272453308 + 0.1 * 7.1326823234558105
Epoch 140, val loss: 1.3267604112625122
Epoch 150, training loss: 1.8614416122436523 = 1.1526641845703125 + 0.1 * 7.08777379989624
Epoch 150, val loss: 1.2611397504806519
Epoch 160, training loss: 1.7785638570785522 = 1.074233889579773 + 0.1 * 7.043299674987793
Epoch 160, val loss: 1.2016688585281372
Epoch 170, training loss: 1.7015306949615479 = 1.0004510879516602 + 0.1 * 7.010795593261719
Epoch 170, val loss: 1.1480754613876343
Epoch 180, training loss: 1.6293377876281738 = 0.9306812286376953 + 0.1 * 6.986566066741943
Epoch 180, val loss: 1.0985403060913086
Epoch 190, training loss: 1.560422658920288 = 0.8634715676307678 + 0.1 * 6.96951150894165
Epoch 190, val loss: 1.0511460304260254
Epoch 200, training loss: 1.4937922954559326 = 0.7982453107833862 + 0.1 * 6.955469608306885
Epoch 200, val loss: 1.0053526163101196
Epoch 210, training loss: 1.4306061267852783 = 0.7361897230148315 + 0.1 * 6.9441633224487305
Epoch 210, val loss: 0.9626035094261169
Epoch 220, training loss: 1.370356559753418 = 0.6776822209358215 + 0.1 * 6.926743030548096
Epoch 220, val loss: 0.9239386320114136
Epoch 230, training loss: 1.3145334720611572 = 0.6225050091743469 + 0.1 * 6.920283794403076
Epoch 230, val loss: 0.8899611234664917
Epoch 240, training loss: 1.2614562511444092 = 0.5710088014602661 + 0.1 * 6.904474258422852
Epoch 240, val loss: 0.8606611490249634
Epoch 250, training loss: 1.211151123046875 = 0.5222684144973755 + 0.1 * 6.888827323913574
Epoch 250, val loss: 0.8350313901901245
Epoch 260, training loss: 1.164252519607544 = 0.4761582016944885 + 0.1 * 6.880943775177002
Epoch 260, val loss: 0.8124994039535522
Epoch 270, training loss: 1.120386004447937 = 0.4331357181072235 + 0.1 * 6.87250280380249
Epoch 270, val loss: 0.7931455373764038
Epoch 280, training loss: 1.0789217948913574 = 0.39275985956192017 + 0.1 * 6.861619472503662
Epoch 280, val loss: 0.7770951390266418
Epoch 290, training loss: 1.0415709018707275 = 0.35496288537979126 + 0.1 * 6.866080284118652
Epoch 290, val loss: 0.7643871307373047
Epoch 300, training loss: 1.0045536756515503 = 0.3198460042476654 + 0.1 * 6.847076892852783
Epoch 300, val loss: 0.7549508810043335
Epoch 310, training loss: 0.9706273674964905 = 0.28707343339920044 + 0.1 * 6.8355393409729
Epoch 310, val loss: 0.7485556602478027
Epoch 320, training loss: 0.9412671327590942 = 0.25667184591293335 + 0.1 * 6.84595251083374
Epoch 320, val loss: 0.7450593709945679
Epoch 330, training loss: 0.9109367728233337 = 0.22876495122909546 + 0.1 * 6.821718215942383
Epoch 330, val loss: 0.74427729845047
Epoch 340, training loss: 0.8847616910934448 = 0.20322486758232117 + 0.1 * 6.815367698669434
Epoch 340, val loss: 0.7463940382003784
Epoch 350, training loss: 0.8607507944107056 = 0.18013478815555573 + 0.1 * 6.806159973144531
Epoch 350, val loss: 0.7515047788619995
Epoch 360, training loss: 0.841931939125061 = 0.15954890847206116 + 0.1 * 6.823830604553223
Epoch 360, val loss: 0.7593358755111694
Epoch 370, training loss: 0.821904718875885 = 0.14162969589233398 + 0.1 * 6.802750110626221
Epoch 370, val loss: 0.7690334320068359
Epoch 380, training loss: 0.804853618144989 = 0.12606263160705566 + 0.1 * 6.787909984588623
Epoch 380, val loss: 0.7804479598999023
Epoch 390, training loss: 0.7906118035316467 = 0.11251509189605713 + 0.1 * 6.7809672355651855
Epoch 390, val loss: 0.793373167514801
Epoch 400, training loss: 0.7783328890800476 = 0.10073210299015045 + 0.1 * 6.776007652282715
Epoch 400, val loss: 0.8073245882987976
Epoch 410, training loss: 0.7674896121025085 = 0.09050347656011581 + 0.1 * 6.769861221313477
Epoch 410, val loss: 0.8218104243278503
Epoch 420, training loss: 0.7584466338157654 = 0.08160043507814407 + 0.1 * 6.768462181091309
Epoch 420, val loss: 0.8366615176200867
Epoch 430, training loss: 0.7499942779541016 = 0.07383222877979279 + 0.1 * 6.76162052154541
Epoch 430, val loss: 0.8516719937324524
Epoch 440, training loss: 0.7425193786621094 = 0.0670299232006073 + 0.1 * 6.754894256591797
Epoch 440, val loss: 0.8667114973068237
Epoch 450, training loss: 0.7363186478614807 = 0.06103818863630295 + 0.1 * 6.752804279327393
Epoch 450, val loss: 0.881696343421936
Epoch 460, training loss: 0.7303610444068909 = 0.055745840072631836 + 0.1 * 6.746151924133301
Epoch 460, val loss: 0.896531879901886
Epoch 470, training loss: 0.7251461148262024 = 0.05106494575738907 + 0.1 * 6.740811347961426
Epoch 470, val loss: 0.9112260937690735
Epoch 480, training loss: 0.7210774421691895 = 0.04693058133125305 + 0.1 * 6.74146842956543
Epoch 480, val loss: 0.9253374934196472
Epoch 490, training loss: 0.718017578125 = 0.043248485773801804 + 0.1 * 6.747690677642822
Epoch 490, val loss: 0.9392787218093872
Epoch 500, training loss: 0.7131331562995911 = 0.03997816517949104 + 0.1 * 6.7315497398376465
Epoch 500, val loss: 0.952735960483551
Epoch 510, training loss: 0.7096338868141174 = 0.03705442696809769 + 0.1 * 6.725794792175293
Epoch 510, val loss: 0.965927004814148
Epoch 520, training loss: 0.7066327929496765 = 0.03442411497235298 + 0.1 * 6.722086429595947
Epoch 520, val loss: 0.9787675738334656
Epoch 530, training loss: 0.7038682699203491 = 0.03204914554953575 + 0.1 * 6.718191623687744
Epoch 530, val loss: 0.9914605021476746
Epoch 540, training loss: 0.7014619708061218 = 0.029905812814831734 + 0.1 * 6.715561389923096
Epoch 540, val loss: 1.0037919282913208
Epoch 550, training loss: 0.699404776096344 = 0.027971355244517326 + 0.1 * 6.714334487915039
Epoch 550, val loss: 1.01566481590271
Epoch 560, training loss: 0.6982173323631287 = 0.026214370504021645 + 0.1 * 6.720029354095459
Epoch 560, val loss: 1.0272750854492188
Epoch 570, training loss: 0.6949470639228821 = 0.024620411917567253 + 0.1 * 6.703266143798828
Epoch 570, val loss: 1.0385806560516357
Epoch 580, training loss: 0.6947581768035889 = 0.023167818784713745 + 0.1 * 6.7159037590026855
Epoch 580, val loss: 1.049639344215393
Epoch 590, training loss: 0.6920703053474426 = 0.021842578426003456 + 0.1 * 6.702276706695557
Epoch 590, val loss: 1.0601447820663452
Epoch 600, training loss: 0.6901612877845764 = 0.020627779886126518 + 0.1 * 6.6953349113464355
Epoch 600, val loss: 1.070667028427124
Epoch 610, training loss: 0.6886338591575623 = 0.01951245218515396 + 0.1 * 6.691213607788086
Epoch 610, val loss: 1.0807664394378662
Epoch 620, training loss: 0.6887903213500977 = 0.018487494438886642 + 0.1 * 6.703028202056885
Epoch 620, val loss: 1.0906294584274292
Epoch 630, training loss: 0.6869280934333801 = 0.01754642464220524 + 0.1 * 6.693816661834717
Epoch 630, val loss: 1.1000701189041138
Epoch 640, training loss: 0.684952437877655 = 0.01667843386530876 + 0.1 * 6.682739734649658
Epoch 640, val loss: 1.1093196868896484
Epoch 650, training loss: 0.68439120054245 = 0.015874305739998817 + 0.1 * 6.685168743133545
Epoch 650, val loss: 1.1184241771697998
Epoch 660, training loss: 0.6837095618247986 = 0.015128876082599163 + 0.1 * 6.685807228088379
Epoch 660, val loss: 1.1272026300430298
Epoch 670, training loss: 0.6818096041679382 = 0.01443874929100275 + 0.1 * 6.673708438873291
Epoch 670, val loss: 1.1359140872955322
Epoch 680, training loss: 0.6817169785499573 = 0.013796591199934483 + 0.1 * 6.679203987121582
Epoch 680, val loss: 1.1441383361816406
Epoch 690, training loss: 0.6813451051712036 = 0.013201179914176464 + 0.1 * 6.681439399719238
Epoch 690, val loss: 1.1523489952087402
Epoch 700, training loss: 0.6788029074668884 = 0.012647676281630993 + 0.1 * 6.6615519523620605
Epoch 700, val loss: 1.159997820854187
Epoch 710, training loss: 0.6780081987380981 = 0.012128855101764202 + 0.1 * 6.6587934494018555
Epoch 710, val loss: 1.167750358581543
Epoch 720, training loss: 0.6771267652511597 = 0.011642612516880035 + 0.1 * 6.654841423034668
Epoch 720, val loss: 1.1752294301986694
Epoch 730, training loss: 0.6771408915519714 = 0.011186378076672554 + 0.1 * 6.659545421600342
Epoch 730, val loss: 1.182898759841919
Epoch 740, training loss: 0.675652027130127 = 0.010760156437754631 + 0.1 * 6.648919105529785
Epoch 740, val loss: 1.189703106880188
Epoch 750, training loss: 0.6760565042495728 = 0.010360825806856155 + 0.1 * 6.656956672668457
Epoch 750, val loss: 1.1969974040985107
Epoch 760, training loss: 0.6740110516548157 = 0.009987027384340763 + 0.1 * 6.640239715576172
Epoch 760, val loss: 1.203188180923462
Epoch 770, training loss: 0.6734653115272522 = 0.00963416788727045 + 0.1 * 6.63831090927124
Epoch 770, val loss: 1.2098745107650757
Epoch 780, training loss: 0.6748607754707336 = 0.009301066398620605 + 0.1 * 6.65559720993042
Epoch 780, val loss: 1.2163223028182983
Epoch 790, training loss: 0.6724991798400879 = 0.00898750964552164 + 0.1 * 6.6351165771484375
Epoch 790, val loss: 1.2225701808929443
Epoch 800, training loss: 0.671430230140686 = 0.008690139278769493 + 0.1 * 6.627400875091553
Epoch 800, val loss: 1.2286978960037231
Epoch 810, training loss: 0.6719865202903748 = 0.00840885378420353 + 0.1 * 6.635776519775391
Epoch 810, val loss: 1.2348740100860596
Epoch 820, training loss: 0.6699711084365845 = 0.008142216131091118 + 0.1 * 6.618288516998291
Epoch 820, val loss: 1.2406665086746216
Epoch 830, training loss: 0.6707499623298645 = 0.00789007917046547 + 0.1 * 6.628598690032959
Epoch 830, val loss: 1.246467113494873
Epoch 840, training loss: 0.6690229177474976 = 0.007650579791516066 + 0.1 * 6.6137237548828125
Epoch 840, val loss: 1.2520310878753662
Epoch 850, training loss: 0.6704866290092468 = 0.007423337548971176 + 0.1 * 6.6306328773498535
Epoch 850, val loss: 1.2576018571853638
Epoch 860, training loss: 0.6674249768257141 = 0.007207160349935293 + 0.1 * 6.60217809677124
Epoch 860, val loss: 1.2629776000976562
Epoch 870, training loss: 0.6689243316650391 = 0.007000913377851248 + 0.1 * 6.619234085083008
Epoch 870, val loss: 1.2683614492416382
Epoch 880, training loss: 0.6680219769477844 = 0.006805259734392166 + 0.1 * 6.6121673583984375
Epoch 880, val loss: 1.2736157178878784
Epoch 890, training loss: 0.6680368781089783 = 0.006618363317102194 + 0.1 * 6.614184856414795
Epoch 890, val loss: 1.2787162065505981
Epoch 900, training loss: 0.6664868593215942 = 0.006440543103963137 + 0.1 * 6.600463390350342
Epoch 900, val loss: 1.28374445438385
Epoch 910, training loss: 0.6655519604682922 = 0.006270298268646002 + 0.1 * 6.592816352844238
Epoch 910, val loss: 1.288652777671814
Epoch 920, training loss: 0.6652491688728333 = 0.006107629742473364 + 0.1 * 6.5914154052734375
Epoch 920, val loss: 1.2936280965805054
Epoch 930, training loss: 0.6658088564872742 = 0.005952152423560619 + 0.1 * 6.59856653213501
Epoch 930, val loss: 1.2982419729232788
Epoch 940, training loss: 0.6646779775619507 = 0.005803050938993692 + 0.1 * 6.588748931884766
Epoch 940, val loss: 1.3030065298080444
Epoch 950, training loss: 0.6651474237442017 = 0.0056600384414196014 + 0.1 * 6.594873428344727
Epoch 950, val loss: 1.307524561882019
Epoch 960, training loss: 0.6639314889907837 = 0.005523402709513903 + 0.1 * 6.584080696105957
Epoch 960, val loss: 1.3122717142105103
Epoch 970, training loss: 0.6635259389877319 = 0.005392495542764664 + 0.1 * 6.581334590911865
Epoch 970, val loss: 1.3164985179901123
Epoch 980, training loss: 0.6630584597587585 = 0.005267017055302858 + 0.1 * 6.577914237976074
Epoch 980, val loss: 1.3210351467132568
Epoch 990, training loss: 0.6620930433273315 = 0.005146924406290054 + 0.1 * 6.569461345672607
Epoch 990, val loss: 1.3250961303710938
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8365840801265156
The final CL Acc:0.79753, 0.01772, The final GNN Acc:0.83465, 0.00174
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9522])
updated graph: torch.Size([2, 10574])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8148059844970703 = 1.9551182985305786 + 0.1 * 8.59687614440918
Epoch 0, val loss: 1.9452216625213623
Epoch 10, training loss: 2.8050696849823 = 1.945386528968811 + 0.1 * 8.596831321716309
Epoch 10, val loss: 1.9355961084365845
Epoch 20, training loss: 2.7929437160491943 = 1.9332830905914307 + 0.1 * 8.596606254577637
Epoch 20, val loss: 1.9231295585632324
Epoch 30, training loss: 2.7758708000183105 = 1.9163802862167358 + 0.1 * 8.594903945922852
Epoch 30, val loss: 1.905172348022461
Epoch 40, training loss: 2.7497081756591797 = 1.8916767835617065 + 0.1 * 8.580314636230469
Epoch 40, val loss: 1.8789006471633911
Epoch 50, training loss: 2.7080092430114746 = 1.8573174476623535 + 0.1 * 8.506916999816895
Epoch 50, val loss: 1.8440173864364624
Epoch 60, training loss: 2.6407196521759033 = 1.819404125213623 + 0.1 * 8.213154792785645
Epoch 60, val loss: 1.8102046251296997
Epoch 70, training loss: 2.592729091644287 = 1.7852802276611328 + 0.1 * 8.074487686157227
Epoch 70, val loss: 1.7838594913482666
Epoch 80, training loss: 2.5167083740234375 = 1.7490360736846924 + 0.1 * 7.676723957061768
Epoch 80, val loss: 1.7547399997711182
Epoch 90, training loss: 2.434828042984009 = 1.7039411067962646 + 0.1 * 7.308868408203125
Epoch 90, val loss: 1.7152868509292603
Epoch 100, training loss: 2.362980365753174 = 1.6451101303100586 + 0.1 * 7.178700923919678
Epoch 100, val loss: 1.6621224880218506
Epoch 110, training loss: 2.2830965518951416 = 1.5718517303466797 + 0.1 * 7.112447261810303
Epoch 110, val loss: 1.5965348482131958
Epoch 120, training loss: 2.1978397369384766 = 1.490590214729309 + 0.1 * 7.072494029998779
Epoch 120, val loss: 1.527221441268921
Epoch 130, training loss: 2.112921953201294 = 1.4082218408584595 + 0.1 * 7.047000408172607
Epoch 130, val loss: 1.4601057767868042
Epoch 140, training loss: 2.027052402496338 = 1.3243924379348755 + 0.1 * 7.0266008377075195
Epoch 140, val loss: 1.3950328826904297
Epoch 150, training loss: 1.9363842010498047 = 1.2358072996139526 + 0.1 * 7.005768299102783
Epoch 150, val loss: 1.3279844522476196
Epoch 160, training loss: 1.8386850357055664 = 1.1402695178985596 + 0.1 * 6.984155654907227
Epoch 160, val loss: 1.255280613899231
Epoch 170, training loss: 1.7366669178009033 = 1.039495587348938 + 0.1 * 6.971714019775391
Epoch 170, val loss: 1.1786623001098633
Epoch 180, training loss: 1.634391188621521 = 0.9384992122650146 + 0.1 * 6.958919525146484
Epoch 180, val loss: 1.1023674011230469
Epoch 190, training loss: 1.5361099243164062 = 0.8410518169403076 + 0.1 * 6.95058012008667
Epoch 190, val loss: 1.0294604301452637
Epoch 200, training loss: 1.4455218315124512 = 0.7515376210212708 + 0.1 * 6.9398417472839355
Epoch 200, val loss: 0.9644553661346436
Epoch 210, training loss: 1.3663201332092285 = 0.6729629039764404 + 0.1 * 6.9335713386535645
Epoch 210, val loss: 0.9113510847091675
Epoch 220, training loss: 1.2970995903015137 = 0.6057382822036743 + 0.1 * 6.9136128425598145
Epoch 220, val loss: 0.8700661659240723
Epoch 230, training loss: 1.2381871938705444 = 0.5479778051376343 + 0.1 * 6.902093887329102
Epoch 230, val loss: 0.839074432849884
Epoch 240, training loss: 1.1856602430343628 = 0.4977114200592041 + 0.1 * 6.879487991333008
Epoch 240, val loss: 0.8152687549591064
Epoch 250, training loss: 1.1402229070663452 = 0.4528786540031433 + 0.1 * 6.87344217300415
Epoch 250, val loss: 0.7963433265686035
Epoch 260, training loss: 1.0970220565795898 = 0.41284534335136414 + 0.1 * 6.841767311096191
Epoch 260, val loss: 0.7815260291099548
Epoch 270, training loss: 1.061236023902893 = 0.3768315017223358 + 0.1 * 6.844045639038086
Epoch 270, val loss: 0.7701137065887451
Epoch 280, training loss: 1.0260049104690552 = 0.34498313069343567 + 0.1 * 6.81021785736084
Epoch 280, val loss: 0.761923611164093
Epoch 290, training loss: 0.9961804151535034 = 0.3167519271373749 + 0.1 * 6.794284343719482
Epoch 290, val loss: 0.7563347220420837
Epoch 300, training loss: 0.9698714017868042 = 0.29170024394989014 + 0.1 * 6.781711578369141
Epoch 300, val loss: 0.7528865337371826
Epoch 310, training loss: 0.9462701678276062 = 0.26938915252685547 + 0.1 * 6.768809795379639
Epoch 310, val loss: 0.7511228919029236
Epoch 320, training loss: 0.9253783226013184 = 0.24920496344566345 + 0.1 * 6.761733531951904
Epoch 320, val loss: 0.7507843375205994
Epoch 330, training loss: 0.9051644802093506 = 0.2306365966796875 + 0.1 * 6.745278835296631
Epoch 330, val loss: 0.7513052225112915
Epoch 340, training loss: 0.8866124749183655 = 0.21294696629047394 + 0.1 * 6.736655235290527
Epoch 340, val loss: 0.7525121569633484
Epoch 350, training loss: 0.8690249919891357 = 0.19581250846385956 + 0.1 * 6.7321248054504395
Epoch 350, val loss: 0.7541402578353882
Epoch 360, training loss: 0.8510882258415222 = 0.17908668518066406 + 0.1 * 6.720015525817871
Epoch 360, val loss: 0.7558932304382324
Epoch 370, training loss: 0.8339337110519409 = 0.16290071606636047 + 0.1 * 6.710329532623291
Epoch 370, val loss: 0.7577934861183167
Epoch 380, training loss: 0.8179405331611633 = 0.14753592014312744 + 0.1 * 6.70404577255249
Epoch 380, val loss: 0.7600336074829102
Epoch 390, training loss: 0.8029943108558655 = 0.13326813280582428 + 0.1 * 6.697261810302734
Epoch 390, val loss: 0.7627120018005371
Epoch 400, training loss: 0.7889738082885742 = 0.12032181024551392 + 0.1 * 6.686519622802734
Epoch 400, val loss: 0.7661040425300598
Epoch 410, training loss: 0.7772145867347717 = 0.10871557146310806 + 0.1 * 6.684989929199219
Epoch 410, val loss: 0.7702640295028687
Epoch 420, training loss: 0.768322765827179 = 0.09844043105840683 + 0.1 * 6.69882345199585
Epoch 420, val loss: 0.7752249240875244
Epoch 430, training loss: 0.7566118836402893 = 0.08938901126384735 + 0.1 * 6.6722283363342285
Epoch 430, val loss: 0.7807739973068237
Epoch 440, training loss: 0.748801052570343 = 0.08134986460208893 + 0.1 * 6.674511909484863
Epoch 440, val loss: 0.7870277762413025
Epoch 450, training loss: 0.739781379699707 = 0.07419604808092117 + 0.1 * 6.655853271484375
Epoch 450, val loss: 0.7939338088035583
Epoch 460, training loss: 0.7336692810058594 = 0.06782231479883194 + 0.1 * 6.658469200134277
Epoch 460, val loss: 0.801277220249176
Epoch 470, training loss: 0.7269227504730225 = 0.062152713537216187 + 0.1 * 6.647700309753418
Epoch 470, val loss: 0.8090184926986694
Epoch 480, training loss: 0.7212913036346436 = 0.057090189307928085 + 0.1 * 6.6420111656188965
Epoch 480, val loss: 0.8170274496078491
Epoch 490, training loss: 0.716619074344635 = 0.05257735773921013 + 0.1 * 6.640416622161865
Epoch 490, val loss: 0.8253206610679626
Epoch 500, training loss: 0.7107305526733398 = 0.04856307432055473 + 0.1 * 6.62167501449585
Epoch 500, val loss: 0.8336834907531738
Epoch 510, training loss: 0.7062776684761047 = 0.04497634992003441 + 0.1 * 6.613012790679932
Epoch 510, val loss: 0.842069685459137
Epoch 520, training loss: 0.7028335928916931 = 0.04176200553774834 + 0.1 * 6.610715866088867
Epoch 520, val loss: 0.8504957556724548
Epoch 530, training loss: 0.7001495957374573 = 0.038885924965143204 + 0.1 * 6.612636089324951
Epoch 530, val loss: 0.8589197397232056
Epoch 540, training loss: 0.6974106431007385 = 0.03630170598626137 + 0.1 * 6.611088752746582
Epoch 540, val loss: 0.8672229647636414
Epoch 550, training loss: 0.6938532590866089 = 0.033976960927248 + 0.1 * 6.598762512207031
Epoch 550, val loss: 0.8754770159721375
Epoch 560, training loss: 0.6914696097373962 = 0.03186871483922005 + 0.1 * 6.596008777618408
Epoch 560, val loss: 0.8835822343826294
Epoch 570, training loss: 0.6892198920249939 = 0.029952537268400192 + 0.1 * 6.5926737785339355
Epoch 570, val loss: 0.8917140364646912
Epoch 580, training loss: 0.6866204142570496 = 0.028207730501890182 + 0.1 * 6.584126949310303
Epoch 580, val loss: 0.8996381759643555
Epoch 590, training loss: 0.6838066577911377 = 0.026613321155309677 + 0.1 * 6.571933746337891
Epoch 590, val loss: 0.9075400233268738
Epoch 600, training loss: 0.6825348138809204 = 0.025155337527394295 + 0.1 * 6.573794364929199
Epoch 600, val loss: 0.9152290225028992
Epoch 610, training loss: 0.6819591522216797 = 0.023815913125872612 + 0.1 * 6.581432342529297
Epoch 610, val loss: 0.9228466749191284
Epoch 620, training loss: 0.679344892501831 = 0.022588331252336502 + 0.1 * 6.56756591796875
Epoch 620, val loss: 0.9302939176559448
Epoch 630, training loss: 0.6777176856994629 = 0.02145569398999214 + 0.1 * 6.562619686126709
Epoch 630, val loss: 0.9376068711280823
Epoch 640, training loss: 0.6765092611312866 = 0.020408272743225098 + 0.1 * 6.561009883880615
Epoch 640, val loss: 0.9448226690292358
Epoch 650, training loss: 0.6743288040161133 = 0.0194376353174448 + 0.1 * 6.5489115715026855
Epoch 650, val loss: 0.9518972635269165
Epoch 660, training loss: 0.6740021705627441 = 0.018537331372499466 + 0.1 * 6.554648399353027
Epoch 660, val loss: 0.958859920501709
Epoch 670, training loss: 0.6725690960884094 = 0.01770213432610035 + 0.1 * 6.548669338226318
Epoch 670, val loss: 0.9656974673271179
Epoch 680, training loss: 0.6705396771430969 = 0.016924159601330757 + 0.1 * 6.5361552238464355
Epoch 680, val loss: 0.972353994846344
Epoch 690, training loss: 0.6703205704689026 = 0.016200123354792595 + 0.1 * 6.541204452514648
Epoch 690, val loss: 0.9789209365844727
Epoch 700, training loss: 0.6690772771835327 = 0.015522354282438755 + 0.1 * 6.535549163818359
Epoch 700, val loss: 0.9853267073631287
Epoch 710, training loss: 0.6680229902267456 = 0.014891508035361767 + 0.1 * 6.531314849853516
Epoch 710, val loss: 0.9915964603424072
Epoch 720, training loss: 0.6668756008148193 = 0.014299008995294571 + 0.1 * 6.525765419006348
Epoch 720, val loss: 0.9978082776069641
Epoch 730, training loss: 0.6662246584892273 = 0.013740775175392628 + 0.1 * 6.524838924407959
Epoch 730, val loss: 1.0038446187973022
Epoch 740, training loss: 0.6653539538383484 = 0.013217758387327194 + 0.1 * 6.521361827850342
Epoch 740, val loss: 1.0098445415496826
Epoch 750, training loss: 0.6653085947036743 = 0.012725540436804295 + 0.1 * 6.525830268859863
Epoch 750, val loss: 1.0156652927398682
Epoch 760, training loss: 0.6638757586479187 = 0.01226444449275732 + 0.1 * 6.51611328125
Epoch 760, val loss: 1.0213940143585205
Epoch 770, training loss: 0.665860652923584 = 0.011828146874904633 + 0.1 * 6.540325164794922
Epoch 770, val loss: 1.026953101158142
Epoch 780, training loss: 0.6631062030792236 = 0.0114164799451828 + 0.1 * 6.516897201538086
Epoch 780, val loss: 1.0324783325195312
Epoch 790, training loss: 0.6644805669784546 = 0.011028105393052101 + 0.1 * 6.534524440765381
Epoch 790, val loss: 1.0379019975662231
Epoch 800, training loss: 0.6609243154525757 = 0.01066036056727171 + 0.1 * 6.5026397705078125
Epoch 800, val loss: 1.0431753396987915
Epoch 810, training loss: 0.660835862159729 = 0.010312692262232304 + 0.1 * 6.505231857299805
Epoch 810, val loss: 1.0484535694122314
Epoch 820, training loss: 0.6619089245796204 = 0.009982232935726643 + 0.1 * 6.5192670822143555
Epoch 820, val loss: 1.0536125898361206
Epoch 830, training loss: 0.6605962514877319 = 0.009668621234595776 + 0.1 * 6.509275913238525
Epoch 830, val loss: 1.0586285591125488
Epoch 840, training loss: 0.6586939692497253 = 0.009371750988066196 + 0.1 * 6.493222236633301
Epoch 840, val loss: 1.0636351108551025
Epoch 850, training loss: 0.6589015126228333 = 0.009089570492506027 + 0.1 * 6.498119354248047
Epoch 850, val loss: 1.0685063600540161
Epoch 860, training loss: 0.6592567563056946 = 0.008820825256407261 + 0.1 * 6.504359245300293
Epoch 860, val loss: 1.0732827186584473
Epoch 870, training loss: 0.6577308773994446 = 0.008564496412873268 + 0.1 * 6.491663932800293
Epoch 870, val loss: 1.0779484510421753
Epoch 880, training loss: 0.6576624512672424 = 0.008320816792547703 + 0.1 * 6.4934163093566895
Epoch 880, val loss: 1.082600712776184
Epoch 890, training loss: 0.6576228141784668 = 0.00808821339160204 + 0.1 * 6.495345592498779
Epoch 890, val loss: 1.0871316194534302
Epoch 900, training loss: 0.6561244130134583 = 0.007867119275033474 + 0.1 * 6.48257303237915
Epoch 900, val loss: 1.0915969610214233
Epoch 910, training loss: 0.6575636267662048 = 0.007656036410480738 + 0.1 * 6.499075889587402
Epoch 910, val loss: 1.0959833860397339
Epoch 920, training loss: 0.6550439596176147 = 0.007453614380210638 + 0.1 * 6.475903034210205
Epoch 920, val loss: 1.100291132926941
Epoch 930, training loss: 0.6537811756134033 = 0.007260865997523069 + 0.1 * 6.465202808380127
Epoch 930, val loss: 1.1045159101486206
Epoch 940, training loss: 0.6578683257102966 = 0.007075906731188297 + 0.1 * 6.5079240798950195
Epoch 940, val loss: 1.1087045669555664
Epoch 950, training loss: 0.6533337831497192 = 0.006898372899740934 + 0.1 * 6.464354515075684
Epoch 950, val loss: 1.1127800941467285
Epoch 960, training loss: 0.6533148288726807 = 0.0067291222512722015 + 0.1 * 6.465857028961182
Epoch 960, val loss: 1.1168508529663086
Epoch 970, training loss: 0.6525205373764038 = 0.006566186435520649 + 0.1 * 6.459543704986572
Epoch 970, val loss: 1.1208064556121826
Epoch 980, training loss: 0.6545031070709229 = 0.006409443449229002 + 0.1 * 6.480936050415039
Epoch 980, val loss: 1.1247131824493408
Epoch 990, training loss: 0.6519609093666077 = 0.006259960122406483 + 0.1 * 6.457009315490723
Epoch 990, val loss: 1.128604531288147
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 2.7909891605377197 = 1.931302785873413 + 0.1 * 8.596863746643066
Epoch 0, val loss: 1.9225420951843262
Epoch 10, training loss: 2.7815678119659424 = 1.9218883514404297 + 0.1 * 8.596795082092285
Epoch 10, val loss: 1.9129000902175903
Epoch 20, training loss: 2.7697205543518066 = 1.9100794792175293 + 0.1 * 8.596409797668457
Epoch 20, val loss: 1.9008216857910156
Epoch 30, training loss: 2.752713918685913 = 1.8934060335159302 + 0.1 * 8.59307861328125
Epoch 30, val loss: 1.883805274963379
Epoch 40, training loss: 2.726001501083374 = 1.869113802909851 + 0.1 * 8.568877220153809
Epoch 40, val loss: 1.8594270944595337
Epoch 50, training loss: 2.68324875831604 = 1.8366611003875732 + 0.1 * 8.465876579284668
Epoch 50, val loss: 1.828758716583252
Epoch 60, training loss: 2.617950916290283 = 1.8017973899841309 + 0.1 * 8.16153621673584
Epoch 60, val loss: 1.7991746664047241
Epoch 70, training loss: 2.5702152252197266 = 1.7674274444580078 + 0.1 * 8.027878761291504
Epoch 70, val loss: 1.7708299160003662
Epoch 80, training loss: 2.5065512657165527 = 1.724938988685608 + 0.1 * 7.8161234855651855
Epoch 80, val loss: 1.7329400777816772
Epoch 90, training loss: 2.4277801513671875 = 1.6701046228408813 + 0.1 * 7.576754570007324
Epoch 90, val loss: 1.6838897466659546
Epoch 100, training loss: 2.335543155670166 = 1.6015037298202515 + 0.1 * 7.340394020080566
Epoch 100, val loss: 1.6253349781036377
Epoch 110, training loss: 2.2427761554718018 = 1.5186063051223755 + 0.1 * 7.24169921875
Epoch 110, val loss: 1.551680088043213
Epoch 120, training loss: 2.145763635635376 = 1.4284080266952515 + 0.1 * 7.173555374145508
Epoch 120, val loss: 1.4746201038360596
Epoch 130, training loss: 2.0492563247680664 = 1.3359931707382202 + 0.1 * 7.132632255554199
Epoch 130, val loss: 1.4005109071731567
Epoch 140, training loss: 1.954336404800415 = 1.2429981231689453 + 0.1 * 7.1133832931518555
Epoch 140, val loss: 1.3280155658721924
Epoch 150, training loss: 1.8620895147323608 = 1.1516526937484741 + 0.1 * 7.104368209838867
Epoch 150, val loss: 1.259560465812683
Epoch 160, training loss: 1.7735854387283325 = 1.063551664352417 + 0.1 * 7.100337505340576
Epoch 160, val loss: 1.1953489780426025
Epoch 170, training loss: 1.689976692199707 = 0.980225145816803 + 0.1 * 7.09751558303833
Epoch 170, val loss: 1.1368398666381836
Epoch 180, training loss: 1.6116260290145874 = 0.9022237062454224 + 0.1 * 7.09402322769165
Epoch 180, val loss: 1.0840567350387573
Epoch 190, training loss: 1.5380115509033203 = 0.8290857076644897 + 0.1 * 7.089258193969727
Epoch 190, val loss: 1.0368295907974243
Epoch 200, training loss: 1.468227505683899 = 0.7600950598716736 + 0.1 * 7.081324100494385
Epoch 200, val loss: 0.994832456111908
Epoch 210, training loss: 1.4017205238342285 = 0.6947591304779053 + 0.1 * 7.069612979888916
Epoch 210, val loss: 0.9569635391235352
Epoch 220, training loss: 1.3387250900268555 = 0.6335185766220093 + 0.1 * 7.052064895629883
Epoch 220, val loss: 0.9234389066696167
Epoch 230, training loss: 1.2795031070709229 = 0.5770324468612671 + 0.1 * 7.024706840515137
Epoch 230, val loss: 0.8941934108734131
Epoch 240, training loss: 1.225942850112915 = 0.5255548357963562 + 0.1 * 7.003880023956299
Epoch 240, val loss: 0.8696169853210449
Epoch 250, training loss: 1.1760835647583008 = 0.47876545786857605 + 0.1 * 6.973181247711182
Epoch 250, val loss: 0.8492916226387024
Epoch 260, training loss: 1.131201982498169 = 0.435721755027771 + 0.1 * 6.954802513122559
Epoch 260, val loss: 0.8325197696685791
Epoch 270, training loss: 1.0892730951309204 = 0.39582008123397827 + 0.1 * 6.934529781341553
Epoch 270, val loss: 0.8188266754150391
Epoch 280, training loss: 1.0529565811157227 = 0.35844168066978455 + 0.1 * 6.945148468017578
Epoch 280, val loss: 0.8079020380973816
Epoch 290, training loss: 1.014825701713562 = 0.3239019215106964 + 0.1 * 6.909237384796143
Epoch 290, val loss: 0.7991831302642822
Epoch 300, training loss: 0.9805858135223389 = 0.29158148169517517 + 0.1 * 6.89004373550415
Epoch 300, val loss: 0.7925271987915039
Epoch 310, training loss: 0.9540098905563354 = 0.26122501492500305 + 0.1 * 6.9278483390808105
Epoch 310, val loss: 0.7877747416496277
Epoch 320, training loss: 0.920474648475647 = 0.23343126475811005 + 0.1 * 6.870433807373047
Epoch 320, val loss: 0.7846041321754456
Epoch 330, training loss: 0.8913986682891846 = 0.20792421698570251 + 0.1 * 6.834744453430176
Epoch 330, val loss: 0.7831962704658508
Epoch 340, training loss: 0.8667199611663818 = 0.18467721343040466 + 0.1 * 6.820427894592285
Epoch 340, val loss: 0.7834741473197937
Epoch 350, training loss: 0.8446866273880005 = 0.16373655200004578 + 0.1 * 6.8095011711120605
Epoch 350, val loss: 0.7855150699615479
Epoch 360, training loss: 0.8252356052398682 = 0.14525015652179718 + 0.1 * 6.799854278564453
Epoch 360, val loss: 0.7890713810920715
Epoch 370, training loss: 0.8072687387466431 = 0.12901346385478973 + 0.1 * 6.782552242279053
Epoch 370, val loss: 0.7936994433403015
Epoch 380, training loss: 0.793829619884491 = 0.1147179827094078 + 0.1 * 6.791116237640381
Epoch 380, val loss: 0.7996639609336853
Epoch 390, training loss: 0.7787262201309204 = 0.1022331640124321 + 0.1 * 6.764930248260498
Epoch 390, val loss: 0.8062605857849121
Epoch 400, training loss: 0.7662769556045532 = 0.09127796441316605 + 0.1 * 6.749989986419678
Epoch 400, val loss: 0.8137550354003906
Epoch 410, training loss: 0.756580650806427 = 0.08167573064565659 + 0.1 * 6.749049186706543
Epoch 410, val loss: 0.821824848651886
Epoch 420, training loss: 0.7470874786376953 = 0.07329095900058746 + 0.1 * 6.737965106964111
Epoch 420, val loss: 0.8302554488182068
Epoch 430, training loss: 0.7385594248771667 = 0.06594560295343399 + 0.1 * 6.726138114929199
Epoch 430, val loss: 0.8391281366348267
Epoch 440, training loss: 0.731455385684967 = 0.05950276181101799 + 0.1 * 6.7195258140563965
Epoch 440, val loss: 0.8483222723007202
Epoch 450, training loss: 0.7262735366821289 = 0.05385215952992439 + 0.1 * 6.72421407699585
Epoch 450, val loss: 0.8577290177345276
Epoch 460, training loss: 0.7202380299568176 = 0.0489218570291996 + 0.1 * 6.713161468505859
Epoch 460, val loss: 0.8671548366546631
Epoch 470, training loss: 0.7147111892700195 = 0.04462173953652382 + 0.1 * 6.700893878936768
Epoch 470, val loss: 0.8765814900398254
Epoch 480, training loss: 0.7105562090873718 = 0.040845837444067 + 0.1 * 6.697103977203369
Epoch 480, val loss: 0.8858712315559387
Epoch 490, training loss: 0.7078842520713806 = 0.03751203790307045 + 0.1 * 6.70372200012207
Epoch 490, val loss: 0.8951186537742615
Epoch 500, training loss: 0.7033993601799011 = 0.03456520289182663 + 0.1 * 6.6883416175842285
Epoch 500, val loss: 0.9041416049003601
Epoch 510, training loss: 0.7003439664840698 = 0.03194871544837952 + 0.1 * 6.683952808380127
Epoch 510, val loss: 0.9130597710609436
Epoch 520, training loss: 0.6977607607841492 = 0.029623517766594887 + 0.1 * 6.681372165679932
Epoch 520, val loss: 0.9217934012413025
Epoch 530, training loss: 0.6937921047210693 = 0.02754765935242176 + 0.1 * 6.662444114685059
Epoch 530, val loss: 0.930281400680542
Epoch 540, training loss: 0.6937136054039001 = 0.025685716420412064 + 0.1 * 6.680278778076172
Epoch 540, val loss: 0.9386231303215027
Epoch 550, training loss: 0.6903284192085266 = 0.024022148922085762 + 0.1 * 6.663062572479248
Epoch 550, val loss: 0.9466897249221802
Epoch 560, training loss: 0.6879470944404602 = 0.022521035745739937 + 0.1 * 6.654260635375977
Epoch 560, val loss: 0.9545866847038269
Epoch 570, training loss: 0.6858230829238892 = 0.021159643307328224 + 0.1 * 6.646634101867676
Epoch 570, val loss: 0.9623287916183472
Epoch 580, training loss: 0.6837184429168701 = 0.019926611334085464 + 0.1 * 6.637917995452881
Epoch 580, val loss: 0.9698694348335266
Epoch 590, training loss: 0.683138370513916 = 0.01880144141614437 + 0.1 * 6.643369197845459
Epoch 590, val loss: 0.977236270904541
Epoch 600, training loss: 0.6819620132446289 = 0.017776163294911385 + 0.1 * 6.641858100891113
Epoch 600, val loss: 0.9844498038291931
Epoch 610, training loss: 0.6798447966575623 = 0.016840476542711258 + 0.1 * 6.630043029785156
Epoch 610, val loss: 0.9914413094520569
Epoch 620, training loss: 0.6808928847312927 = 0.015979979187250137 + 0.1 * 6.649128437042236
Epoch 620, val loss: 0.9982940554618835
Epoch 630, training loss: 0.6777148246765137 = 0.015192768536508083 + 0.1 * 6.625220775604248
Epoch 630, val loss: 1.0049220323562622
Epoch 640, training loss: 0.6756958365440369 = 0.014466814696788788 + 0.1 * 6.612289905548096
Epoch 640, val loss: 1.011364459991455
Epoch 650, training loss: 0.6750965118408203 = 0.013792555779218674 + 0.1 * 6.613039016723633
Epoch 650, val loss: 1.0176935195922852
Epoch 660, training loss: 0.6764262914657593 = 0.013171027414500713 + 0.1 * 6.632552623748779
Epoch 660, val loss: 1.023911476135254
Epoch 670, training loss: 0.6734611392021179 = 0.012598240748047829 + 0.1 * 6.60862922668457
Epoch 670, val loss: 1.029875636100769
Epoch 680, training loss: 0.6722763180732727 = 0.012063735164701939 + 0.1 * 6.602125644683838
Epoch 680, val loss: 1.0356862545013428
Epoch 690, training loss: 0.6705578565597534 = 0.011565333232283592 + 0.1 * 6.589925289154053
Epoch 690, val loss: 1.0414124727249146
Epoch 700, training loss: 0.6718273758888245 = 0.01109993364661932 + 0.1 * 6.607274532318115
Epoch 700, val loss: 1.0470263957977295
Epoch 710, training loss: 0.6689692139625549 = 0.010665320791304111 + 0.1 * 6.583039283752441
Epoch 710, val loss: 1.052484154701233
Epoch 720, training loss: 0.6694642305374146 = 0.010258502326905727 + 0.1 * 6.592057228088379
Epoch 720, val loss: 1.0578217506408691
Epoch 730, training loss: 0.6689956188201904 = 0.00987748522311449 + 0.1 * 6.591180801391602
Epoch 730, val loss: 1.0630677938461304
Epoch 740, training loss: 0.6678658127784729 = 0.009520813822746277 + 0.1 * 6.583449840545654
Epoch 740, val loss: 1.0681723356246948
Epoch 750, training loss: 0.6667467355728149 = 0.009185797534883022 + 0.1 * 6.57560920715332
Epoch 750, val loss: 1.073134183883667
Epoch 760, training loss: 0.6677994728088379 = 0.008870082907378674 + 0.1 * 6.589293956756592
Epoch 760, val loss: 1.0779997110366821
Epoch 770, training loss: 0.6650338172912598 = 0.008572989143431187 + 0.1 * 6.564608097076416
Epoch 770, val loss: 1.0827088356018066
Epoch 780, training loss: 0.6648611426353455 = 0.008292495273053646 + 0.1 * 6.565686225891113
Epoch 780, val loss: 1.0873568058013916
Epoch 790, training loss: 0.6637042760848999 = 0.008027661591768265 + 0.1 * 6.556766033172607
Epoch 790, val loss: 1.0919382572174072
Epoch 800, training loss: 0.6644627451896667 = 0.0077766356989741325 + 0.1 * 6.566861152648926
Epoch 800, val loss: 1.0963752269744873
Epoch 810, training loss: 0.6624059677124023 = 0.007540157064795494 + 0.1 * 6.5486578941345215
Epoch 810, val loss: 1.1007962226867676
Epoch 820, training loss: 0.6629268527030945 = 0.00731554813683033 + 0.1 * 6.556112766265869
Epoch 820, val loss: 1.1050622463226318
Epoch 830, training loss: 0.6615036129951477 = 0.0071028550155460835 + 0.1 * 6.544007301330566
Epoch 830, val loss: 1.109251856803894
Epoch 840, training loss: 0.6605983376502991 = 0.0069005656987428665 + 0.1 * 6.536977767944336
Epoch 840, val loss: 1.1133198738098145
Epoch 850, training loss: 0.6624866724014282 = 0.006707711145281792 + 0.1 * 6.557789325714111
Epoch 850, val loss: 1.1173046827316284
Epoch 860, training loss: 0.6609219312667847 = 0.0065248627215623856 + 0.1 * 6.543970584869385
Epoch 860, val loss: 1.1212702989578247
Epoch 870, training loss: 0.6596556305885315 = 0.0063506537117064 + 0.1 * 6.533050060272217
Epoch 870, val loss: 1.12509286403656
Epoch 880, training loss: 0.6599722504615784 = 0.006183851510286331 + 0.1 * 6.537883758544922
Epoch 880, val loss: 1.1289161443710327
Epoch 890, training loss: 0.6588612794876099 = 0.006024948321282864 + 0.1 * 6.528363227844238
Epoch 890, val loss: 1.1326593160629272
Epoch 900, training loss: 0.6574722528457642 = 0.005872989539057016 + 0.1 * 6.515992641448975
Epoch 900, val loss: 1.1363275051116943
Epoch 910, training loss: 0.6592252254486084 = 0.0057275788858532906 + 0.1 * 6.534976482391357
Epoch 910, val loss: 1.1399383544921875
Epoch 920, training loss: 0.6574938297271729 = 0.0055885533802211285 + 0.1 * 6.519052982330322
Epoch 920, val loss: 1.1435216665267944
Epoch 930, training loss: 0.6567080020904541 = 0.005455491133034229 + 0.1 * 6.5125250816345215
Epoch 930, val loss: 1.1470072269439697
Epoch 940, training loss: 0.6585817933082581 = 0.005327411461621523 + 0.1 * 6.532543659210205
Epoch 940, val loss: 1.1504578590393066
Epoch 950, training loss: 0.6562976241111755 = 0.005205236375331879 + 0.1 * 6.510923862457275
Epoch 950, val loss: 1.1538903713226318
Epoch 960, training loss: 0.655052125453949 = 0.005087718833237886 + 0.1 * 6.499643802642822
Epoch 960, val loss: 1.1572202444076538
Epoch 970, training loss: 0.6573726534843445 = 0.004974697716534138 + 0.1 * 6.523979187011719
Epoch 970, val loss: 1.1605219841003418
Epoch 980, training loss: 0.6544281840324402 = 0.004866317845880985 + 0.1 * 6.49561882019043
Epoch 980, val loss: 1.1637027263641357
Epoch 990, training loss: 0.6538620591163635 = 0.004762035794556141 + 0.1 * 6.491000175476074
Epoch 990, val loss: 1.166886568069458
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 2.804966688156128 = 1.9452834129333496 + 0.1 * 8.596833229064941
Epoch 0, val loss: 1.953262448310852
Epoch 10, training loss: 2.7952473163604736 = 1.935573697090149 + 0.1 * 8.596736907958984
Epoch 10, val loss: 1.9436973333358765
Epoch 20, training loss: 2.782999038696289 = 1.9233808517456055 + 0.1 * 8.596182823181152
Epoch 20, val loss: 1.9311180114746094
Epoch 30, training loss: 2.765251636505127 = 1.906118631362915 + 0.1 * 8.591330528259277
Epoch 30, val loss: 1.9127229452133179
Epoch 40, training loss: 2.7364070415496826 = 1.8805102109909058 + 0.1 * 8.558967590332031
Epoch 40, val loss: 1.8853516578674316
Epoch 50, training loss: 2.684490442276001 = 1.8460859060287476 + 0.1 * 8.384045600891113
Epoch 50, val loss: 1.850205898284912
Epoch 60, training loss: 2.6266732215881348 = 1.810318946838379 + 0.1 * 8.163541793823242
Epoch 60, val loss: 1.8165998458862305
Epoch 70, training loss: 2.572421073913574 = 1.7794896364212036 + 0.1 * 7.929313659667969
Epoch 70, val loss: 1.789502501487732
Epoch 80, training loss: 2.4974870681762695 = 1.7454763650894165 + 0.1 * 7.520108222961426
Epoch 80, val loss: 1.7600257396697998
Epoch 90, training loss: 2.423297882080078 = 1.7024564743041992 + 0.1 * 7.208413600921631
Epoch 90, val loss: 1.7238417863845825
Epoch 100, training loss: 2.3539466857910156 = 1.6450388431549072 + 0.1 * 7.089078426361084
Epoch 100, val loss: 1.6769154071807861
Epoch 110, training loss: 2.2766666412353516 = 1.5718847513198853 + 0.1 * 7.047819137573242
Epoch 110, val loss: 1.6173303127288818
Epoch 120, training loss: 2.193971872329712 = 1.491987705230713 + 0.1 * 7.019840717315674
Epoch 120, val loss: 1.5541189908981323
Epoch 130, training loss: 2.114377975463867 = 1.4151489734649658 + 0.1 * 6.992290496826172
Epoch 130, val loss: 1.4957598447799683
Epoch 140, training loss: 2.0381288528442383 = 1.3419564962387085 + 0.1 * 6.961724281311035
Epoch 140, val loss: 1.4433059692382812
Epoch 150, training loss: 1.9635264873504639 = 1.2706197500228882 + 0.1 * 6.929067134857178
Epoch 150, val loss: 1.3937242031097412
Epoch 160, training loss: 1.889983892440796 = 1.200161337852478 + 0.1 * 6.898224830627441
Epoch 160, val loss: 1.3452106714248657
Epoch 170, training loss: 1.8181021213531494 = 1.1311522722244263 + 0.1 * 6.869498252868652
Epoch 170, val loss: 1.2981605529785156
Epoch 180, training loss: 1.7486968040466309 = 1.063886046409607 + 0.1 * 6.848106861114502
Epoch 180, val loss: 1.2520008087158203
Epoch 190, training loss: 1.6826322078704834 = 0.9994924068450928 + 0.1 * 6.8313984870910645
Epoch 190, val loss: 1.2074096202850342
Epoch 200, training loss: 1.6199138164520264 = 0.9388943910598755 + 0.1 * 6.810194969177246
Epoch 200, val loss: 1.165251612663269
Epoch 210, training loss: 1.5601816177368164 = 0.8803887963294983 + 0.1 * 6.797927379608154
Epoch 210, val loss: 1.12489914894104
Epoch 220, training loss: 1.501549243927002 = 0.822374701499939 + 0.1 * 6.791745185852051
Epoch 220, val loss: 1.0855193138122559
Epoch 230, training loss: 1.441175937652588 = 0.7635294795036316 + 0.1 * 6.776464939117432
Epoch 230, val loss: 1.0464271306991577
Epoch 240, training loss: 1.379373550415039 = 0.7023146748542786 + 0.1 * 6.770589351654053
Epoch 240, val loss: 1.0074975490570068
Epoch 250, training loss: 1.3156323432922363 = 0.6393392086029053 + 0.1 * 6.762930393218994
Epoch 250, val loss: 0.9695987105369568
Epoch 260, training loss: 1.2510061264038086 = 0.5755801796913147 + 0.1 * 6.7542595863342285
Epoch 260, val loss: 0.9337916970252991
Epoch 270, training loss: 1.187955379486084 = 0.5125147104263306 + 0.1 * 6.7544074058532715
Epoch 270, val loss: 0.9006529450416565
Epoch 280, training loss: 1.1260977983474731 = 0.45249468088150024 + 0.1 * 6.7360310554504395
Epoch 280, val loss: 0.8714547753334045
Epoch 290, training loss: 1.0693718194961548 = 0.3966309130191803 + 0.1 * 6.727408409118652
Epoch 290, val loss: 0.8460890054702759
Epoch 300, training loss: 1.0176200866699219 = 0.345803827047348 + 0.1 * 6.718162536621094
Epoch 300, val loss: 0.8246946930885315
Epoch 310, training loss: 0.9722588062286377 = 0.30059748888015747 + 0.1 * 6.716612815856934
Epoch 310, val loss: 0.8074753880500793
Epoch 320, training loss: 0.9329015016555786 = 0.26143816113471985 + 0.1 * 6.714632987976074
Epoch 320, val loss: 0.7945972681045532
Epoch 330, training loss: 0.8971801996231079 = 0.22794407606124878 + 0.1 * 6.692360877990723
Epoch 330, val loss: 0.7856968641281128
Epoch 340, training loss: 0.8676133751869202 = 0.19939975440502167 + 0.1 * 6.682136058807373
Epoch 340, val loss: 0.7804036736488342
Epoch 350, training loss: 0.8425662517547607 = 0.17513200640678406 + 0.1 * 6.674342155456543
Epoch 350, val loss: 0.778281033039093
Epoch 360, training loss: 0.8225236535072327 = 0.15444804728031158 + 0.1 * 6.680756092071533
Epoch 360, val loss: 0.7788251638412476
Epoch 370, training loss: 0.8038953542709351 = 0.13694700598716736 + 0.1 * 6.669483184814453
Epoch 370, val loss: 0.7815266251564026
Epoch 380, training loss: 0.7874287366867065 = 0.1220855861902237 + 0.1 * 6.653431415557861
Epoch 380, val loss: 0.785836935043335
Epoch 390, training loss: 0.7736159563064575 = 0.1092858612537384 + 0.1 * 6.643300533294678
Epoch 390, val loss: 0.7915772795677185
Epoch 400, training loss: 0.7616859674453735 = 0.09822457283735275 + 0.1 * 6.634613513946533
Epoch 400, val loss: 0.7985144853591919
Epoch 410, training loss: 0.751640260219574 = 0.08862747997045517 + 0.1 * 6.630127906799316
Epoch 410, val loss: 0.8062320351600647
Epoch 420, training loss: 0.7436397075653076 = 0.08025848120450974 + 0.1 * 6.633811950683594
Epoch 420, val loss: 0.8146657347679138
Epoch 430, training loss: 0.7348025441169739 = 0.07293744385242462 + 0.1 * 6.618650913238525
Epoch 430, val loss: 0.8235399723052979
Epoch 440, training loss: 0.7274278402328491 = 0.06649019569158554 + 0.1 * 6.609376430511475
Epoch 440, val loss: 0.8328245878219604
Epoch 450, training loss: 0.7217090725898743 = 0.06080922856926918 + 0.1 * 6.6089982986450195
Epoch 450, val loss: 0.8423629403114319
Epoch 460, training loss: 0.7154281735420227 = 0.05581251159310341 + 0.1 * 6.596156120300293
Epoch 460, val loss: 0.8518368005752563
Epoch 470, training loss: 0.7116096019744873 = 0.05137298256158829 + 0.1 * 6.602365970611572
Epoch 470, val loss: 0.861395001411438
Epoch 480, training loss: 0.7065300345420837 = 0.04742654040455818 + 0.1 * 6.591034889221191
Epoch 480, val loss: 0.8709632158279419
Epoch 490, training loss: 0.7022668719291687 = 0.043900925666093826 + 0.1 * 6.583659648895264
Epoch 490, val loss: 0.8804669380187988
Epoch 500, training loss: 0.700819194316864 = 0.04074098542332649 + 0.1 * 6.6007819175720215
Epoch 500, val loss: 0.8899041414260864
Epoch 510, training loss: 0.6958067417144775 = 0.03791375458240509 + 0.1 * 6.578929901123047
Epoch 510, val loss: 0.8991616368293762
Epoch 520, training loss: 0.691929817199707 = 0.03536525368690491 + 0.1 * 6.565645217895508
Epoch 520, val loss: 0.9082449674606323
Epoch 530, training loss: 0.6925765872001648 = 0.03305787965655327 + 0.1 * 6.595187187194824
Epoch 530, val loss: 0.9171987175941467
Epoch 540, training loss: 0.6873704791069031 = 0.03098132088780403 + 0.1 * 6.563891887664795
Epoch 540, val loss: 0.9259682893753052
Epoch 550, training loss: 0.6849093437194824 = 0.02910068817436695 + 0.1 * 6.558085918426514
Epoch 550, val loss: 0.9345051646232605
Epoch 560, training loss: 0.6825611591339111 = 0.027385631576180458 + 0.1 * 6.551754951477051
Epoch 560, val loss: 0.9429028034210205
Epoch 570, training loss: 0.6807183027267456 = 0.0258186012506485 + 0.1 * 6.548996925354004
Epoch 570, val loss: 0.9511370658874512
Epoch 580, training loss: 0.6798410415649414 = 0.024383114650845528 + 0.1 * 6.554579257965088
Epoch 580, val loss: 0.9592360854148865
Epoch 590, training loss: 0.6775357127189636 = 0.023072686046361923 + 0.1 * 6.544630527496338
Epoch 590, val loss: 0.9670886993408203
Epoch 600, training loss: 0.6791034936904907 = 0.02186564914882183 + 0.1 * 6.572378158569336
Epoch 600, val loss: 0.9747680425643921
Epoch 610, training loss: 0.673779308795929 = 0.020758379250764847 + 0.1 * 6.530209064483643
Epoch 610, val loss: 0.9822676777839661
Epoch 620, training loss: 0.6726768016815186 = 0.019738180562853813 + 0.1 * 6.529386043548584
Epoch 620, val loss: 0.9895917177200317
Epoch 630, training loss: 0.6749641299247742 = 0.018792618066072464 + 0.1 * 6.561715126037598
Epoch 630, val loss: 0.9967662692070007
Epoch 640, training loss: 0.6700487732887268 = 0.017917858436703682 + 0.1 * 6.521308898925781
Epoch 640, val loss: 1.0037750005722046
Epoch 650, training loss: 0.6692337393760681 = 0.017107388004660606 + 0.1 * 6.521263599395752
Epoch 650, val loss: 1.010634183883667
Epoch 660, training loss: 0.6686351299285889 = 0.01635269820690155 + 0.1 * 6.522824287414551
Epoch 660, val loss: 1.0173239707946777
Epoch 670, training loss: 0.6667560935020447 = 0.015651188790798187 + 0.1 * 6.511049270629883
Epoch 670, val loss: 1.0239323377609253
Epoch 680, training loss: 0.6665253043174744 = 0.01499534584581852 + 0.1 * 6.5152997970581055
Epoch 680, val loss: 1.0303915739059448
Epoch 690, training loss: 0.6659767031669617 = 0.014382382854819298 + 0.1 * 6.5159430503845215
Epoch 690, val loss: 1.0367039442062378
Epoch 700, training loss: 0.6649330258369446 = 0.013809670694172382 + 0.1 * 6.511233329772949
Epoch 700, val loss: 1.0429282188415527
Epoch 710, training loss: 0.6640176773071289 = 0.01327380072325468 + 0.1 * 6.507438659667969
Epoch 710, val loss: 1.0489546060562134
Epoch 720, training loss: 0.6630334854125977 = 0.012770457193255424 + 0.1 * 6.50262975692749
Epoch 720, val loss: 1.0548802614212036
Epoch 730, training loss: 0.6640621423721313 = 0.012298033572733402 + 0.1 * 6.517640590667725
Epoch 730, val loss: 1.0607091188430786
Epoch 740, training loss: 0.662177562713623 = 0.011853761970996857 + 0.1 * 6.503237724304199
Epoch 740, val loss: 1.066375494003296
Epoch 750, training loss: 0.6608259081840515 = 0.01143629290163517 + 0.1 * 6.493896007537842
Epoch 750, val loss: 1.0719435214996338
Epoch 760, training loss: 0.6612319946289062 = 0.011040758341550827 + 0.1 * 6.5019121170043945
Epoch 760, val loss: 1.077402114868164
Epoch 770, training loss: 0.6601824760437012 = 0.010666895657777786 + 0.1 * 6.4951558113098145
Epoch 770, val loss: 1.082757592201233
Epoch 780, training loss: 0.6592156887054443 = 0.010315600782632828 + 0.1 * 6.4890007972717285
Epoch 780, val loss: 1.0880268812179565
Epoch 790, training loss: 0.6581883430480957 = 0.009981023147702217 + 0.1 * 6.482072830200195
Epoch 790, val loss: 1.093164324760437
Epoch 800, training loss: 0.6589129567146301 = 0.009664725512266159 + 0.1 * 6.4924821853637695
Epoch 800, val loss: 1.0982252359390259
Epoch 810, training loss: 0.6572549939155579 = 0.00936517957597971 + 0.1 * 6.478897571563721
Epoch 810, val loss: 1.1031891107559204
Epoch 820, training loss: 0.6596999168395996 = 0.009080364368855953 + 0.1 * 6.506195545196533
Epoch 820, val loss: 1.1080727577209473
Epoch 830, training loss: 0.6559197902679443 = 0.008810360915958881 + 0.1 * 6.471094608306885
Epoch 830, val loss: 1.11284339427948
Epoch 840, training loss: 0.6556860208511353 = 0.008554462343454361 + 0.1 * 6.471315383911133
Epoch 840, val loss: 1.117508888244629
Epoch 850, training loss: 0.6572425365447998 = 0.008309522643685341 + 0.1 * 6.489329814910889
Epoch 850, val loss: 1.1220911741256714
Epoch 860, training loss: 0.6552034616470337 = 0.008075403980910778 + 0.1 * 6.471280574798584
Epoch 860, val loss: 1.1266391277313232
Epoch 870, training loss: 0.6554427146911621 = 0.007853547111153603 + 0.1 * 6.475891590118408
Epoch 870, val loss: 1.1311076879501343
Epoch 880, training loss: 0.6553919315338135 = 0.0076401750557124615 + 0.1 * 6.477517127990723
Epoch 880, val loss: 1.135501503944397
Epoch 890, training loss: 0.6543130874633789 = 0.0074380869045853615 + 0.1 * 6.46875
Epoch 890, val loss: 1.139823317527771
Epoch 900, training loss: 0.6539610624313354 = 0.007243867497891188 + 0.1 * 6.467171669006348
Epoch 900, val loss: 1.1440802812576294
Epoch 910, training loss: 0.6565905213356018 = 0.007057615555822849 + 0.1 * 6.495328903198242
Epoch 910, val loss: 1.148262858390808
Epoch 920, training loss: 0.6538509130477905 = 0.006880609318614006 + 0.1 * 6.469703197479248
Epoch 920, val loss: 1.152403473854065
Epoch 930, training loss: 0.6530206203460693 = 0.0067108008079230785 + 0.1 * 6.463098049163818
Epoch 930, val loss: 1.1564332246780396
Epoch 940, training loss: 0.6523223519325256 = 0.006547583732753992 + 0.1 * 6.457747936248779
Epoch 940, val loss: 1.1604241132736206
Epoch 950, training loss: 0.6514634490013123 = 0.006391257978975773 + 0.1 * 6.450721740722656
Epoch 950, val loss: 1.164324402809143
Epoch 960, training loss: 0.65291827917099 = 0.006241100374609232 + 0.1 * 6.466772079467773
Epoch 960, val loss: 1.1682113409042358
Epoch 970, training loss: 0.6511185169219971 = 0.0060972413048148155 + 0.1 * 6.450212478637695
Epoch 970, val loss: 1.1720060110092163
Epoch 980, training loss: 0.6510366797447205 = 0.005958777852356434 + 0.1 * 6.450778484344482
Epoch 980, val loss: 1.175751805305481
Epoch 990, training loss: 0.6527036428451538 = 0.0058249421417713165 + 0.1 * 6.46878719329834
Epoch 990, val loss: 1.1794620752334595
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8154981549815499
The final CL Acc:0.77901, 0.01364, The final GNN Acc:0.81603, 0.00043
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13202])
remove edge: torch.Size([2, 7780])
updated graph: torch.Size([2, 10426])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8141348361968994 = 1.954451084136963 + 0.1 * 8.596837997436523
Epoch 0, val loss: 1.965072512626648
Epoch 10, training loss: 2.804108142852783 = 1.9444316625595093 + 0.1 * 8.596763610839844
Epoch 10, val loss: 1.9543558359146118
Epoch 20, training loss: 2.7918713092803955 = 1.9322469234466553 + 0.1 * 8.596242904663086
Epoch 20, val loss: 1.9412193298339844
Epoch 30, training loss: 2.774258852005005 = 1.9150643348693848 + 0.1 * 8.59194564819336
Epoch 30, val loss: 1.9225958585739136
Epoch 40, training loss: 2.7457773685455322 = 1.889417052268982 + 0.1 * 8.563603401184082
Epoch 40, val loss: 1.8947181701660156
Epoch 50, training loss: 2.6971726417541504 = 1.8533098697662354 + 0.1 * 8.438628196716309
Epoch 50, val loss: 1.8570274114608765
Epoch 60, training loss: 2.6247329711914062 = 1.81167733669281 + 0.1 * 8.1305570602417
Epoch 60, val loss: 1.816934585571289
Epoch 70, training loss: 2.5614662170410156 = 1.7747560739517212 + 0.1 * 7.867100715637207
Epoch 70, val loss: 1.7848739624023438
Epoch 80, training loss: 2.485325813293457 = 1.7359941005706787 + 0.1 * 7.4933180809021
Epoch 80, val loss: 1.750289797782898
Epoch 90, training loss: 2.4131417274475098 = 1.687266230583191 + 0.1 * 7.258753776550293
Epoch 90, val loss: 1.7069820165634155
Epoch 100, training loss: 2.3362061977386475 = 1.6240588426589966 + 0.1 * 7.121474266052246
Epoch 100, val loss: 1.6516828536987305
Epoch 110, training loss: 2.248796224594116 = 1.5421648025512695 + 0.1 * 7.066314697265625
Epoch 110, val loss: 1.5788699388504028
Epoch 120, training loss: 2.148475170135498 = 1.4455198049545288 + 0.1 * 7.02955436706543
Epoch 120, val loss: 1.4947865009307861
Epoch 130, training loss: 2.0395984649658203 = 1.3395147323608398 + 0.1 * 7.00083589553833
Epoch 130, val loss: 1.406812071800232
Epoch 140, training loss: 1.9286534786224365 = 1.2313263416290283 + 0.1 * 6.973270416259766
Epoch 140, val loss: 1.317248821258545
Epoch 150, training loss: 1.8206846714019775 = 1.1259208917617798 + 0.1 * 6.947637557983398
Epoch 150, val loss: 1.2322604656219482
Epoch 160, training loss: 1.7202130556106567 = 1.0275534391403198 + 0.1 * 6.926596164703369
Epoch 160, val loss: 1.1547003984451294
Epoch 170, training loss: 1.6304590702056885 = 0.9390887022018433 + 0.1 * 6.9137043952941895
Epoch 170, val loss: 1.0869940519332886
Epoch 180, training loss: 1.5505404472351074 = 0.860651433467865 + 0.1 * 6.898890495300293
Epoch 180, val loss: 1.0285180807113647
Epoch 190, training loss: 1.4796159267425537 = 0.790908932685852 + 0.1 * 6.8870697021484375
Epoch 190, val loss: 0.9776893854141235
Epoch 200, training loss: 1.4167038202285767 = 0.7296937108039856 + 0.1 * 6.870100975036621
Epoch 200, val loss: 0.9342889785766602
Epoch 210, training loss: 1.3625802993774414 = 0.6759341955184937 + 0.1 * 6.86646032333374
Epoch 210, val loss: 0.8975745439529419
Epoch 220, training loss: 1.3130546808242798 = 0.6283555626869202 + 0.1 * 6.846991062164307
Epoch 220, val loss: 0.866689145565033
Epoch 230, training loss: 1.268136978149414 = 0.5844047665596008 + 0.1 * 6.83732271194458
Epoch 230, val loss: 0.8394869565963745
Epoch 240, training loss: 1.2253601551055908 = 0.5424871444702148 + 0.1 * 6.828730583190918
Epoch 240, val loss: 0.8149944543838501
Epoch 250, training loss: 1.1848468780517578 = 0.5019088387489319 + 0.1 * 6.829380512237549
Epoch 250, val loss: 0.7929062247276306
Epoch 260, training loss: 1.1448352336883545 = 0.46287983655929565 + 0.1 * 6.819553852081299
Epoch 260, val loss: 0.7739600539207458
Epoch 270, training loss: 1.1059954166412354 = 0.424583375453949 + 0.1 * 6.814120292663574
Epoch 270, val loss: 0.7577939629554749
Epoch 280, training loss: 1.0676370859146118 = 0.3866279423236847 + 0.1 * 6.810091495513916
Epoch 280, val loss: 0.744717538356781
Epoch 290, training loss: 1.0297012329101562 = 0.34912633895874023 + 0.1 * 6.805748462677002
Epoch 290, val loss: 0.7343424558639526
Epoch 300, training loss: 0.9939918518066406 = 0.3126814365386963 + 0.1 * 6.813104152679443
Epoch 300, val loss: 0.7264584898948669
Epoch 310, training loss: 0.9584218263626099 = 0.27829688787460327 + 0.1 * 6.801249027252197
Epoch 310, val loss: 0.7208852767944336
Epoch 320, training loss: 0.9259772300720215 = 0.2463587075471878 + 0.1 * 6.796185493469238
Epoch 320, val loss: 0.7174161076545715
Epoch 330, training loss: 0.8965367078781128 = 0.2172575742006302 + 0.1 * 6.792791366577148
Epoch 330, val loss: 0.7157431244850159
Epoch 340, training loss: 0.8705207705497742 = 0.1913253515958786 + 0.1 * 6.791954040527344
Epoch 340, val loss: 0.7157968878746033
Epoch 350, training loss: 0.8472120761871338 = 0.16868050396442413 + 0.1 * 6.785315990447998
Epoch 350, val loss: 0.7174845337867737
Epoch 360, training loss: 0.8281970024108887 = 0.14906886219978333 + 0.1 * 6.791281700134277
Epoch 360, val loss: 0.7205707430839539
Epoch 370, training loss: 0.8105227947235107 = 0.13229943811893463 + 0.1 * 6.782233715057373
Epoch 370, val loss: 0.7250492572784424
Epoch 380, training loss: 0.7950428128242493 = 0.11788526922464371 + 0.1 * 6.771574974060059
Epoch 380, val loss: 0.7305682301521301
Epoch 390, training loss: 0.7822603583335876 = 0.10542033612728119 + 0.1 * 6.768399715423584
Epoch 390, val loss: 0.7370783090591431
Epoch 400, training loss: 0.7711998224258423 = 0.09465546160936356 + 0.1 * 6.765443325042725
Epoch 400, val loss: 0.7443880438804626
Epoch 410, training loss: 0.7608792781829834 = 0.0853031575679779 + 0.1 * 6.75576114654541
Epoch 410, val loss: 0.7521951794624329
Epoch 420, training loss: 0.7550594806671143 = 0.07710159569978714 + 0.1 * 6.779578685760498
Epoch 420, val loss: 0.7605012059211731
Epoch 430, training loss: 0.7441515326499939 = 0.06993807107210159 + 0.1 * 6.7421345710754395
Epoch 430, val loss: 0.7689604759216309
Epoch 440, training loss: 0.7369791865348816 = 0.06361869722604752 + 0.1 * 6.733604431152344
Epoch 440, val loss: 0.7776783108711243
Epoch 450, training loss: 0.7304201126098633 = 0.058006368577480316 + 0.1 * 6.724137783050537
Epoch 450, val loss: 0.7865571975708008
Epoch 460, training loss: 0.7256473898887634 = 0.053030963987112045 + 0.1 * 6.7261643409729
Epoch 460, val loss: 0.7953518033027649
Epoch 470, training loss: 0.7200537919998169 = 0.0486261360347271 + 0.1 * 6.7142767906188965
Epoch 470, val loss: 0.8040734529495239
Epoch 480, training loss: 0.7150300145149231 = 0.04468710720539093 + 0.1 * 6.703429222106934
Epoch 480, val loss: 0.8127784132957458
Epoch 490, training loss: 0.7111653685569763 = 0.041150446981191635 + 0.1 * 6.700149059295654
Epoch 490, val loss: 0.8213730454444885
Epoch 500, training loss: 0.7076753377914429 = 0.037982795387506485 + 0.1 * 6.696925163269043
Epoch 500, val loss: 0.8298740983009338
Epoch 510, training loss: 0.7034399509429932 = 0.035146504640579224 + 0.1 * 6.682934284210205
Epoch 510, val loss: 0.8381261229515076
Epoch 520, training loss: 0.700619101524353 = 0.03259221091866493 + 0.1 * 6.68026876449585
Epoch 520, val loss: 0.8462859392166138
Epoch 530, training loss: 0.6970549821853638 = 0.030292578041553497 + 0.1 * 6.667623996734619
Epoch 530, val loss: 0.8543015122413635
Epoch 540, training loss: 0.6942678093910217 = 0.02821146324276924 + 0.1 * 6.660562992095947
Epoch 540, val loss: 0.8621194362640381
Epoch 550, training loss: 0.6918278932571411 = 0.02632739581167698 + 0.1 * 6.655004978179932
Epoch 550, val loss: 0.8698585629463196
Epoch 560, training loss: 0.6900739669799805 = 0.02462005615234375 + 0.1 * 6.654539108276367
Epoch 560, val loss: 0.8774409890174866
Epoch 570, training loss: 0.6870298385620117 = 0.023069942370057106 + 0.1 * 6.639599323272705
Epoch 570, val loss: 0.8848369121551514
Epoch 580, training loss: 0.6854801774024963 = 0.02166188508272171 + 0.1 * 6.638182640075684
Epoch 580, val loss: 0.8920791149139404
Epoch 590, training loss: 0.6832338571548462 = 0.020375218242406845 + 0.1 * 6.628586769104004
Epoch 590, val loss: 0.8992325663566589
Epoch 600, training loss: 0.6819876432418823 = 0.019202658906579018 + 0.1 * 6.62785005569458
Epoch 600, val loss: 0.9062036275863647
Epoch 610, training loss: 0.6805855631828308 = 0.018131719902157784 + 0.1 * 6.624538421630859
Epoch 610, val loss: 0.9130107164382935
Epoch 620, training loss: 0.6785940527915955 = 0.017150504514575005 + 0.1 * 6.614435195922852
Epoch 620, val loss: 0.9196253418922424
Epoch 630, training loss: 0.6774101853370667 = 0.0162490326911211 + 0.1 * 6.6116108894348145
Epoch 630, val loss: 0.9261454343795776
Epoch 640, training loss: 0.674656331539154 = 0.015418188646435738 + 0.1 * 6.592381477355957
Epoch 640, val loss: 0.9325549006462097
Epoch 650, training loss: 0.6755942106246948 = 0.014650966972112656 + 0.1 * 6.609432697296143
Epoch 650, val loss: 0.938904345035553
Epoch 660, training loss: 0.6738559007644653 = 0.013943168334662914 + 0.1 * 6.599126815795898
Epoch 660, val loss: 0.9449582099914551
Epoch 670, training loss: 0.6713082790374756 = 0.013290435075759888 + 0.1 * 6.580178737640381
Epoch 670, val loss: 0.9509381055831909
Epoch 680, training loss: 0.6719520092010498 = 0.012683454900979996 + 0.1 * 6.592685222625732
Epoch 680, val loss: 0.9568116068840027
Epoch 690, training loss: 0.6697367429733276 = 0.012120253406465054 + 0.1 * 6.576164722442627
Epoch 690, val loss: 0.9625560641288757
Epoch 700, training loss: 0.6678144931793213 = 0.01159626804292202 + 0.1 * 6.5621819496154785
Epoch 700, val loss: 0.9681684970855713
Epoch 710, training loss: 0.6724099516868591 = 0.011106260120868683 + 0.1 * 6.613037109375
Epoch 710, val loss: 0.9736756682395935
Epoch 720, training loss: 0.6677766442298889 = 0.010650829412043095 + 0.1 * 6.571258068084717
Epoch 720, val loss: 0.9791011214256287
Epoch 730, training loss: 0.667049765586853 = 0.010225946083664894 + 0.1 * 6.568238258361816
Epoch 730, val loss: 0.9843161106109619
Epoch 740, training loss: 0.6650491952896118 = 0.009827696718275547 + 0.1 * 6.552215099334717
Epoch 740, val loss: 0.9893874526023865
Epoch 750, training loss: 0.6651057600975037 = 0.009453377686440945 + 0.1 * 6.55652379989624
Epoch 750, val loss: 0.9944549798965454
Epoch 760, training loss: 0.6640088558197021 = 0.009103094227612019 + 0.1 * 6.549057483673096
Epoch 760, val loss: 0.9994121193885803
Epoch 770, training loss: 0.6625601649284363 = 0.00877293199300766 + 0.1 * 6.537872314453125
Epoch 770, val loss: 1.0041940212249756
Epoch 780, training loss: 0.6623237729072571 = 0.008461831137537956 + 0.1 * 6.538619518280029
Epoch 780, val loss: 1.0089387893676758
Epoch 790, training loss: 0.6614323854446411 = 0.008168859407305717 + 0.1 * 6.532634735107422
Epoch 790, val loss: 1.0135947465896606
Epoch 800, training loss: 0.6628979444503784 = 0.007892221212387085 + 0.1 * 6.5500569343566895
Epoch 800, val loss: 1.0180978775024414
Epoch 810, training loss: 0.6610119342803955 = 0.007631893269717693 + 0.1 * 6.53380012512207
Epoch 810, val loss: 1.022586703300476
Epoch 820, training loss: 0.659760057926178 = 0.007385750766843557 + 0.1 * 6.523743152618408
Epoch 820, val loss: 1.026858925819397
Epoch 830, training loss: 0.6605233550071716 = 0.00715184910222888 + 0.1 * 6.53371524810791
Epoch 830, val loss: 1.0311083793640137
Epoch 840, training loss: 0.6590847373008728 = 0.0069296532310545444 + 0.1 * 6.521551132202148
Epoch 840, val loss: 1.0353904962539673
Epoch 850, training loss: 0.6587797999382019 = 0.006719963159412146 + 0.1 * 6.520598411560059
Epoch 850, val loss: 1.039467215538025
Epoch 860, training loss: 0.6580550670623779 = 0.006520096678286791 + 0.1 * 6.515349388122559
Epoch 860, val loss: 1.0434712171554565
Epoch 870, training loss: 0.659042477607727 = 0.0063298726454377174 + 0.1 * 6.527125835418701
Epoch 870, val loss: 1.0474039316177368
Epoch 880, training loss: 0.6580837965011597 = 0.006149020511657 + 0.1 * 6.51934814453125
Epoch 880, val loss: 1.0513746738433838
Epoch 890, training loss: 0.6572777032852173 = 0.005977061577141285 + 0.1 * 6.513006687164307
Epoch 890, val loss: 1.055135726928711
Epoch 900, training loss: 0.6565945744514465 = 0.005813271272927523 + 0.1 * 6.5078125
Epoch 900, val loss: 1.0588489770889282
Epoch 910, training loss: 0.6572569608688354 = 0.005657150875777006 + 0.1 * 6.515998363494873
Epoch 910, val loss: 1.0625208616256714
Epoch 920, training loss: 0.656040370464325 = 0.005507550667971373 + 0.1 * 6.505328178405762
Epoch 920, val loss: 1.0661556720733643
Epoch 930, training loss: 0.655497133731842 = 0.0053655714727938175 + 0.1 * 6.501315593719482
Epoch 930, val loss: 1.0696991682052612
Epoch 940, training loss: 0.6573536396026611 = 0.005229777656495571 + 0.1 * 6.521238803863525
Epoch 940, val loss: 1.0731492042541504
Epoch 950, training loss: 0.6555745005607605 = 0.005099523812532425 + 0.1 * 6.504749298095703
Epoch 950, val loss: 1.0765031576156616
Epoch 960, training loss: 0.6550629138946533 = 0.004975127521902323 + 0.1 * 6.500877857208252
Epoch 960, val loss: 1.079910397529602
Epoch 970, training loss: 0.6545379161834717 = 0.004855898208916187 + 0.1 * 6.496820449829102
Epoch 970, val loss: 1.083182454109192
Epoch 980, training loss: 0.6543418765068054 = 0.004741783253848553 + 0.1 * 6.49600076675415
Epoch 980, val loss: 1.0863913297653198
Epoch 990, training loss: 0.6534133553504944 = 0.004632147960364819 + 0.1 * 6.48781156539917
Epoch 990, val loss: 1.0895427465438843
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.827648639678955 = 1.967963695526123 + 0.1 * 8.596848487854004
Epoch 0, val loss: 1.9668834209442139
Epoch 10, training loss: 2.816418170928955 = 1.9567453861236572 + 0.1 * 8.59672737121582
Epoch 10, val loss: 1.95626962184906
Epoch 20, training loss: 2.8022069931030273 = 1.9426251649856567 + 0.1 * 8.595818519592285
Epoch 20, val loss: 1.942515254020691
Epoch 30, training loss: 2.7812485694885254 = 1.922542691230774 + 0.1 * 8.587059020996094
Epoch 30, val loss: 1.922547459602356
Epoch 40, training loss: 2.7461276054382324 = 1.89268159866333 + 0.1 * 8.53446102142334
Epoch 40, val loss: 1.8929861783981323
Epoch 50, training loss: 2.680037260055542 = 1.8523606061935425 + 0.1 * 8.276766777038574
Epoch 50, val loss: 1.8551222085952759
Epoch 60, training loss: 2.6174702644348145 = 1.8080581426620483 + 0.1 * 8.094121932983398
Epoch 60, val loss: 1.816603183746338
Epoch 70, training loss: 2.560009002685547 = 1.7688347101211548 + 0.1 * 7.911743640899658
Epoch 70, val loss: 1.7834248542785645
Epoch 80, training loss: 2.4976205825805664 = 1.7276864051818848 + 0.1 * 7.699342727661133
Epoch 80, val loss: 1.7450324296951294
Epoch 90, training loss: 2.4280481338500977 = 1.6756138801574707 + 0.1 * 7.524341583251953
Epoch 90, val loss: 1.6971633434295654
Epoch 100, training loss: 2.3431782722473145 = 1.6103532314300537 + 0.1 * 7.328250408172607
Epoch 100, val loss: 1.6415656805038452
Epoch 110, training loss: 2.2468128204345703 = 1.5322914123535156 + 0.1 * 7.145213603973389
Epoch 110, val loss: 1.5767769813537598
Epoch 120, training loss: 2.1536760330200195 = 1.451846718788147 + 0.1 * 7.01829195022583
Epoch 120, val loss: 1.5112435817718506
Epoch 130, training loss: 2.073431968688965 = 1.3791030645370483 + 0.1 * 6.9432878494262695
Epoch 130, val loss: 1.4516018629074097
Epoch 140, training loss: 2.0041146278381348 = 1.3134211301803589 + 0.1 * 6.906935214996338
Epoch 140, val loss: 1.3987497091293335
Epoch 150, training loss: 1.9383642673492432 = 1.2500073909759521 + 0.1 * 6.883568286895752
Epoch 150, val loss: 1.348719596862793
Epoch 160, training loss: 1.8715275526046753 = 1.1849464178085327 + 0.1 * 6.865811347961426
Epoch 160, val loss: 1.2968722581863403
Epoch 170, training loss: 1.8020434379577637 = 1.1169219017028809 + 0.1 * 6.85121488571167
Epoch 170, val loss: 1.244042158126831
Epoch 180, training loss: 1.7297003269195557 = 1.0459983348846436 + 0.1 * 6.837019443511963
Epoch 180, val loss: 1.190077543258667
Epoch 190, training loss: 1.6552681922912598 = 0.9727180600166321 + 0.1 * 6.825500965118408
Epoch 190, val loss: 1.1342198848724365
Epoch 200, training loss: 1.5807507038116455 = 0.8990409970283508 + 0.1 * 6.8170976638793945
Epoch 200, val loss: 1.0781868696212769
Epoch 210, training loss: 1.50749671459198 = 0.8263418078422546 + 0.1 * 6.811549186706543
Epoch 210, val loss: 1.022750735282898
Epoch 220, training loss: 1.4368975162506104 = 0.7567849159240723 + 0.1 * 6.801126003265381
Epoch 220, val loss: 0.9697695970535278
Epoch 230, training loss: 1.3704938888549805 = 0.6911506056785583 + 0.1 * 6.79343318939209
Epoch 230, val loss: 0.9204123020172119
Epoch 240, training loss: 1.309523105621338 = 0.6308921575546265 + 0.1 * 6.786310195922852
Epoch 240, val loss: 0.8766399025917053
Epoch 250, training loss: 1.2541760206222534 = 0.5762490034103394 + 0.1 * 6.779270172119141
Epoch 250, val loss: 0.8393591046333313
Epoch 260, training loss: 1.2034010887145996 = 0.5262800455093384 + 0.1 * 6.771210193634033
Epoch 260, val loss: 0.808735191822052
Epoch 270, training loss: 1.1566791534423828 = 0.4804719090461731 + 0.1 * 6.7620720863342285
Epoch 270, val loss: 0.7845476865768433
Epoch 280, training loss: 1.1131305694580078 = 0.43803539872169495 + 0.1 * 6.750951766967773
Epoch 280, val loss: 0.7657241821289062
Epoch 290, training loss: 1.0743744373321533 = 0.3981829583644867 + 0.1 * 6.7619147300720215
Epoch 290, val loss: 0.751153826713562
Epoch 300, training loss: 1.0351855754852295 = 0.36105412244796753 + 0.1 * 6.741314888000488
Epoch 300, val loss: 0.7400619387626648
Epoch 310, training loss: 0.9992402195930481 = 0.32633864879608154 + 0.1 * 6.729015350341797
Epoch 310, val loss: 0.731874406337738
Epoch 320, training loss: 0.9671598672866821 = 0.2943820059299469 + 0.1 * 6.727778434753418
Epoch 320, val loss: 0.7267463207244873
Epoch 330, training loss: 0.937058687210083 = 0.26538753509521484 + 0.1 * 6.716711521148682
Epoch 330, val loss: 0.7246772646903992
Epoch 340, training loss: 0.9102652072906494 = 0.2390986531972885 + 0.1 * 6.711665153503418
Epoch 340, val loss: 0.7254039645195007
Epoch 350, training loss: 0.8878899812698364 = 0.21538639068603516 + 0.1 * 6.725035667419434
Epoch 350, val loss: 0.7286295294761658
Epoch 360, training loss: 0.8648887872695923 = 0.19427672028541565 + 0.1 * 6.706120014190674
Epoch 360, val loss: 0.7341580986976624
Epoch 370, training loss: 0.844658374786377 = 0.1754336953163147 + 0.1 * 6.692246913909912
Epoch 370, val loss: 0.7415447235107422
Epoch 380, training loss: 0.8294818997383118 = 0.158628448843956 + 0.1 * 6.708534240722656
Epoch 380, val loss: 0.7507098913192749
Epoch 390, training loss: 0.8125388026237488 = 0.14375121891498566 + 0.1 * 6.687875747680664
Epoch 390, val loss: 0.7613067030906677
Epoch 400, training loss: 0.7988869547843933 = 0.1305161863565445 + 0.1 * 6.6837077140808105
Epoch 400, val loss: 0.7730149030685425
Epoch 410, training loss: 0.7859907150268555 = 0.11878994107246399 + 0.1 * 6.672007083892822
Epoch 410, val loss: 0.7855583429336548
Epoch 420, training loss: 0.7747775912284851 = 0.10835343599319458 + 0.1 * 6.664241313934326
Epoch 420, val loss: 0.7988013625144958
Epoch 430, training loss: 0.7648040056228638 = 0.0990385189652443 + 0.1 * 6.657654762268066
Epoch 430, val loss: 0.812605082988739
Epoch 440, training loss: 0.7560077905654907 = 0.09072583168745041 + 0.1 * 6.652819633483887
Epoch 440, val loss: 0.8268321752548218
Epoch 450, training loss: 0.7491427063941956 = 0.08326893299818039 + 0.1 * 6.658737659454346
Epoch 450, val loss: 0.8414613604545593
Epoch 460, training loss: 0.7410458326339722 = 0.07659703493118286 + 0.1 * 6.6444878578186035
Epoch 460, val loss: 0.8560847640037537
Epoch 470, training loss: 0.7339999675750732 = 0.07059667259454727 + 0.1 * 6.634032726287842
Epoch 470, val loss: 0.8708969950675964
Epoch 480, training loss: 0.7283888459205627 = 0.06520078331232071 + 0.1 * 6.631880283355713
Epoch 480, val loss: 0.8856557607650757
Epoch 490, training loss: 0.7231642007827759 = 0.06035260111093521 + 0.1 * 6.628115653991699
Epoch 490, val loss: 0.9003911018371582
Epoch 500, training loss: 0.7180920839309692 = 0.0559663251042366 + 0.1 * 6.621257305145264
Epoch 500, val loss: 0.9150378704071045
Epoch 510, training loss: 0.7152698040008545 = 0.05198844522237778 + 0.1 * 6.632813930511475
Epoch 510, val loss: 0.929607629776001
Epoch 520, training loss: 0.7096245288848877 = 0.04838946461677551 + 0.1 * 6.612349987030029
Epoch 520, val loss: 0.9440314173698425
Epoch 530, training loss: 0.7059302926063538 = 0.04512559995055199 + 0.1 * 6.608046531677246
Epoch 530, val loss: 0.9581848382949829
Epoch 540, training loss: 0.7034066915512085 = 0.042157500982284546 + 0.1 * 6.612492084503174
Epoch 540, val loss: 0.9721788167953491
Epoch 550, training loss: 0.6993465423583984 = 0.03945298120379448 + 0.1 * 6.598935604095459
Epoch 550, val loss: 0.9860371947288513
Epoch 560, training loss: 0.696844220161438 = 0.03698638826608658 + 0.1 * 6.598577976226807
Epoch 560, val loss: 0.9994785785675049
Epoch 570, training loss: 0.6930071711540222 = 0.0347299799323082 + 0.1 * 6.5827717781066895
Epoch 570, val loss: 1.0128906965255737
Epoch 580, training loss: 0.6927055716514587 = 0.03265894949436188 + 0.1 * 6.600466251373291
Epoch 580, val loss: 1.026025414466858
Epoch 590, training loss: 0.6885227560997009 = 0.03076508268713951 + 0.1 * 6.577576637268066
Epoch 590, val loss: 1.0389591455459595
Epoch 600, training loss: 0.6864974498748779 = 0.02902151830494404 + 0.1 * 6.574759483337402
Epoch 600, val loss: 1.0516552925109863
Epoch 610, training loss: 0.6852662563323975 = 0.027417553588747978 + 0.1 * 6.578486919403076
Epoch 610, val loss: 1.0641542673110962
Epoch 620, training loss: 0.6823176145553589 = 0.025941545143723488 + 0.1 * 6.563760757446289
Epoch 620, val loss: 1.076390027999878
Epoch 630, training loss: 0.680370032787323 = 0.02457469515502453 + 0.1 * 6.557953357696533
Epoch 630, val loss: 1.088516354560852
Epoch 640, training loss: 0.6806015372276306 = 0.023309694603085518 + 0.1 * 6.57291841506958
Epoch 640, val loss: 1.1002928018569946
Epoch 650, training loss: 0.6769155263900757 = 0.022141408175230026 + 0.1 * 6.547740936279297
Epoch 650, val loss: 1.111994981765747
Epoch 660, training loss: 0.676162838935852 = 0.021056700497865677 + 0.1 * 6.551061153411865
Epoch 660, val loss: 1.1233645677566528
Epoch 670, training loss: 0.6746939420700073 = 0.02005094848573208 + 0.1 * 6.546429634094238
Epoch 670, val loss: 1.1345829963684082
Epoch 680, training loss: 0.6729809045791626 = 0.01911572739481926 + 0.1 * 6.538651466369629
Epoch 680, val loss: 1.1454308032989502
Epoch 690, training loss: 0.6721442937850952 = 0.0182466022670269 + 0.1 * 6.538976669311523
Epoch 690, val loss: 1.15621018409729
Epoch 700, training loss: 0.6721562743186951 = 0.017435630783438683 + 0.1 * 6.547206401824951
Epoch 700, val loss: 1.1666452884674072
Epoch 710, training loss: 0.6688439249992371 = 0.016680458560585976 + 0.1 * 6.521634101867676
Epoch 710, val loss: 1.177006483078003
Epoch 720, training loss: 0.6679038405418396 = 0.015973320230841637 + 0.1 * 6.5193047523498535
Epoch 720, val loss: 1.1871267557144165
Epoch 730, training loss: 0.6701827049255371 = 0.01531126070767641 + 0.1 * 6.548714637756348
Epoch 730, val loss: 1.196986198425293
Epoch 740, training loss: 0.666469395160675 = 0.014691951684653759 + 0.1 * 6.5177741050720215
Epoch 740, val loss: 1.206764817237854
Epoch 750, training loss: 0.6667526960372925 = 0.014110722579061985 + 0.1 * 6.526419162750244
Epoch 750, val loss: 1.2162840366363525
Epoch 760, training loss: 0.66440349817276 = 0.01356536615639925 + 0.1 * 6.508381366729736
Epoch 760, val loss: 1.225672960281372
Epoch 770, training loss: 0.66417396068573 = 0.013050899840891361 + 0.1 * 6.51123046875
Epoch 770, val loss: 1.2347893714904785
Epoch 780, training loss: 0.6651220917701721 = 0.012567422352731228 + 0.1 * 6.525547027587891
Epoch 780, val loss: 1.2438699007034302
Epoch 790, training loss: 0.6629791259765625 = 0.012111025862395763 + 0.1 * 6.508680820465088
Epoch 790, val loss: 1.2526692152023315
Epoch 800, training loss: 0.6615321636199951 = 0.011681011877954006 + 0.1 * 6.49851131439209
Epoch 800, val loss: 1.261423110961914
Epoch 810, training loss: 0.6618994474411011 = 0.011274081654846668 + 0.1 * 6.506253242492676
Epoch 810, val loss: 1.2698702812194824
Epoch 820, training loss: 0.6621922850608826 = 0.01089047733694315 + 0.1 * 6.5130181312561035
Epoch 820, val loss: 1.2782641649246216
Epoch 830, training loss: 0.6599684953689575 = 0.010527082718908787 + 0.1 * 6.494414329528809
Epoch 830, val loss: 1.286553144454956
Epoch 840, training loss: 0.6600518226623535 = 0.010182897560298443 + 0.1 * 6.498688697814941
Epoch 840, val loss: 1.2945976257324219
Epoch 850, training loss: 0.6595085859298706 = 0.009856955148279667 + 0.1 * 6.496516227722168
Epoch 850, val loss: 1.3025314807891846
Epoch 860, training loss: 0.6583652496337891 = 0.009547262452542782 + 0.1 * 6.488180160522461
Epoch 860, val loss: 1.310180425643921
Epoch 870, training loss: 0.658446192741394 = 0.009254858829081059 + 0.1 * 6.491913318634033
Epoch 870, val loss: 1.317840814590454
Epoch 880, training loss: 0.6571772694587708 = 0.008975446224212646 + 0.1 * 6.482017993927002
Epoch 880, val loss: 1.325318455696106
Epoch 890, training loss: 0.6577317118644714 = 0.00871031079441309 + 0.1 * 6.490213871002197
Epoch 890, val loss: 1.332715392112732
Epoch 900, training loss: 0.6562857031822205 = 0.008456745184957981 + 0.1 * 6.4782891273498535
Epoch 900, val loss: 1.3398934602737427
Epoch 910, training loss: 0.6565688848495483 = 0.008216054178774357 + 0.1 * 6.483528137207031
Epoch 910, val loss: 1.3470567464828491
Epoch 920, training loss: 0.6553472280502319 = 0.00798618234694004 + 0.1 * 6.4736104011535645
Epoch 920, val loss: 1.3539466857910156
Epoch 930, training loss: 0.6555551886558533 = 0.007767100818455219 + 0.1 * 6.477880954742432
Epoch 930, val loss: 1.360953688621521
Epoch 940, training loss: 0.6544542908668518 = 0.0075570219196379185 + 0.1 * 6.468972206115723
Epoch 940, val loss: 1.3675423860549927
Epoch 950, training loss: 0.6552579402923584 = 0.007357209920883179 + 0.1 * 6.479007244110107
Epoch 950, val loss: 1.3742650747299194
Epoch 960, training loss: 0.6556594371795654 = 0.007165372371673584 + 0.1 * 6.484940528869629
Epoch 960, val loss: 1.3807260990142822
Epoch 970, training loss: 0.654556393623352 = 0.006982127670198679 + 0.1 * 6.475742340087891
Epoch 970, val loss: 1.3871515989303589
Epoch 980, training loss: 0.6528756022453308 = 0.006806877441704273 + 0.1 * 6.460687637329102
Epoch 980, val loss: 1.3934141397476196
Epoch 990, training loss: 0.652254581451416 = 0.006639185827225447 + 0.1 * 6.456153869628906
Epoch 990, val loss: 1.3996866941452026
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 2.825611114501953 = 1.9659252166748047 + 0.1 * 8.5968599319458
Epoch 0, val loss: 1.9648311138153076
Epoch 10, training loss: 2.8149359226226807 = 1.9552586078643799 + 0.1 * 8.596772193908691
Epoch 10, val loss: 1.95400071144104
Epoch 20, training loss: 2.8020079135894775 = 1.9423787593841553 + 0.1 * 8.596290588378906
Epoch 20, val loss: 1.940791130065918
Epoch 30, training loss: 2.784078598022461 = 1.9248493909835815 + 0.1 * 8.592292785644531
Epoch 30, val loss: 1.9227887392044067
Epoch 40, training loss: 2.755721092224121 = 1.8992775678634644 + 0.1 * 8.564435005187988
Epoch 40, val loss: 1.8965250253677368
Epoch 50, training loss: 2.7072622776031494 = 1.8629082441329956 + 0.1 * 8.4435396194458
Epoch 50, val loss: 1.8602592945098877
Epoch 60, training loss: 2.6338651180267334 = 1.8189692497253418 + 0.1 * 8.148959159851074
Epoch 60, val loss: 1.819277048110962
Epoch 70, training loss: 2.5781311988830566 = 1.7776151895523071 + 0.1 * 8.00516128540039
Epoch 70, val loss: 1.784355878829956
Epoch 80, training loss: 2.5121402740478516 = 1.737367033958435 + 0.1 * 7.747731685638428
Epoch 80, val loss: 1.7497445344924927
Epoch 90, training loss: 2.4368350505828857 = 1.6872280836105347 + 0.1 * 7.49606990814209
Epoch 90, val loss: 1.7059593200683594
Epoch 100, training loss: 2.350153923034668 = 1.6220293045043945 + 0.1 * 7.28124475479126
Epoch 100, val loss: 1.6500838994979858
Epoch 110, training loss: 2.2570085525512695 = 1.541456937789917 + 0.1 * 7.155515670776367
Epoch 110, val loss: 1.5800700187683105
Epoch 120, training loss: 2.1598622798919678 = 1.452797293663025 + 0.1 * 7.070650100708008
Epoch 120, val loss: 1.5037909746170044
Epoch 130, training loss: 2.067188262939453 = 1.3642994165420532 + 0.1 * 7.0288872718811035
Epoch 130, val loss: 1.4324259757995605
Epoch 140, training loss: 1.9766497611999512 = 1.2768404483795166 + 0.1 * 6.998092174530029
Epoch 140, val loss: 1.3638582229614258
Epoch 150, training loss: 1.8867299556732178 = 1.1894901990890503 + 0.1 * 6.972397327423096
Epoch 150, val loss: 1.2955790758132935
Epoch 160, training loss: 1.7983187437057495 = 1.1031174659729004 + 0.1 * 6.952012538909912
Epoch 160, val loss: 1.2295740842819214
Epoch 170, training loss: 1.7149540185928345 = 1.0206685066223145 + 0.1 * 6.942854881286621
Epoch 170, val loss: 1.1677876710891724
Epoch 180, training loss: 1.63716459274292 = 0.9443280696868896 + 0.1 * 6.928365230560303
Epoch 180, val loss: 1.1109033823013306
Epoch 190, training loss: 1.564864158630371 = 0.8727105855941772 + 0.1 * 6.921535968780518
Epoch 190, val loss: 1.0567642450332642
Epoch 200, training loss: 1.4964892864227295 = 0.8051389455795288 + 0.1 * 6.913504123687744
Epoch 200, val loss: 1.0051593780517578
Epoch 210, training loss: 1.4325835704803467 = 0.7420372366905212 + 0.1 * 6.905463695526123
Epoch 210, val loss: 0.9570900201797485
Epoch 220, training loss: 1.374798059463501 = 0.6843767166137695 + 0.1 * 6.904212951660156
Epoch 220, val loss: 0.9141636490821838
Epoch 230, training loss: 1.3215693235397339 = 0.6327519416809082 + 0.1 * 6.888173580169678
Epoch 230, val loss: 0.8775103092193604
Epoch 240, training loss: 1.273467779159546 = 0.5858224034309387 + 0.1 * 6.876453876495361
Epoch 240, val loss: 0.8466118574142456
Epoch 250, training loss: 1.2293272018432617 = 0.54266756772995 + 0.1 * 6.866596698760986
Epoch 250, val loss: 0.821074366569519
Epoch 260, training loss: 1.1892985105514526 = 0.5030840635299683 + 0.1 * 6.862144470214844
Epoch 260, val loss: 0.8002867698669434
Epoch 270, training loss: 1.1517459154129028 = 0.4669533371925354 + 0.1 * 6.847925662994385
Epoch 270, val loss: 0.7840652465820312
Epoch 280, training loss: 1.1172094345092773 = 0.43325382471084595 + 0.1 * 6.839556694030762
Epoch 280, val loss: 0.7712203860282898
Epoch 290, training loss: 1.0839685201644897 = 0.4008104205131531 + 0.1 * 6.831581115722656
Epoch 290, val loss: 0.7604981064796448
Epoch 300, training loss: 1.052833080291748 = 0.3686058819293976 + 0.1 * 6.842271327972412
Epoch 300, val loss: 0.7509363889694214
Epoch 310, training loss: 1.0187424421310425 = 0.33641454577445984 + 0.1 * 6.82327938079834
Epoch 310, val loss: 0.741925835609436
Epoch 320, training loss: 0.9856683015823364 = 0.3041723072528839 + 0.1 * 6.81496000289917
Epoch 320, val loss: 0.7337194085121155
Epoch 330, training loss: 0.9540649652481079 = 0.2727109491825104 + 0.1 * 6.813540458679199
Epoch 330, val loss: 0.7267687916755676
Epoch 340, training loss: 0.9235967397689819 = 0.24323274195194244 + 0.1 * 6.803639888763428
Epoch 340, val loss: 0.7215003371238708
Epoch 350, training loss: 0.8966506123542786 = 0.2164536863565445 + 0.1 * 6.801969528198242
Epoch 350, val loss: 0.7181729078292847
Epoch 360, training loss: 0.8719848990440369 = 0.19272615015506744 + 0.1 * 6.7925872802734375
Epoch 360, val loss: 0.7170661687850952
Epoch 370, training loss: 0.8507533073425293 = 0.1719934046268463 + 0.1 * 6.787599086761475
Epoch 370, val loss: 0.7179341316223145
Epoch 380, training loss: 0.8317846655845642 = 0.15396510064601898 + 0.1 * 6.778195381164551
Epoch 380, val loss: 0.7207731604576111
Epoch 390, training loss: 0.8154424428939819 = 0.13829544186592102 + 0.1 * 6.771469593048096
Epoch 390, val loss: 0.7251791954040527
Epoch 400, training loss: 0.801468014717102 = 0.12461428344249725 + 0.1 * 6.7685370445251465
Epoch 400, val loss: 0.7309891581535339
Epoch 410, training loss: 0.7886766195297241 = 0.11265596747398376 + 0.1 * 6.760206699371338
Epoch 410, val loss: 0.7378731966018677
Epoch 420, training loss: 0.7772824764251709 = 0.1021464616060257 + 0.1 * 6.751359939575195
Epoch 420, val loss: 0.7455482482910156
Epoch 430, training loss: 0.7673884034156799 = 0.09283553063869476 + 0.1 * 6.745528221130371
Epoch 430, val loss: 0.7540220022201538
Epoch 440, training loss: 0.7585522532463074 = 0.08457491546869278 + 0.1 * 6.739772796630859
Epoch 440, val loss: 0.7629201412200928
Epoch 450, training loss: 0.7506505250930786 = 0.07721468806266785 + 0.1 * 6.734358310699463
Epoch 450, val loss: 0.7722269296646118
Epoch 460, training loss: 0.743262529373169 = 0.0706375390291214 + 0.1 * 6.726250171661377
Epoch 460, val loss: 0.7817040681838989
Epoch 470, training loss: 0.7365619540214539 = 0.06474976986646652 + 0.1 * 6.718121528625488
Epoch 470, val loss: 0.791307270526886
Epoch 480, training loss: 0.731955349445343 = 0.059454552829265594 + 0.1 * 6.725008010864258
Epoch 480, val loss: 0.801030695438385
Epoch 490, training loss: 0.7258663773536682 = 0.05470970645546913 + 0.1 * 6.71156644821167
Epoch 490, val loss: 0.8107362985610962
Epoch 500, training loss: 0.721173882484436 = 0.0504443384706974 + 0.1 * 6.707294940948486
Epoch 500, val loss: 0.8204367756843567
Epoch 510, training loss: 0.7164528965950012 = 0.04659979045391083 + 0.1 * 6.698530673980713
Epoch 510, val loss: 0.8299793601036072
Epoch 520, training loss: 0.7131354808807373 = 0.043125662952661514 + 0.1 * 6.700098037719727
Epoch 520, val loss: 0.8393491506576538
Epoch 530, training loss: 0.7100557684898376 = 0.039995256811380386 + 0.1 * 6.7006049156188965
Epoch 530, val loss: 0.8486137390136719
Epoch 540, training loss: 0.7056034803390503 = 0.03716779127717018 + 0.1 * 6.684356689453125
Epoch 540, val loss: 0.8577214479446411
Epoch 550, training loss: 0.7037253975868225 = 0.03460530936717987 + 0.1 * 6.6912007331848145
Epoch 550, val loss: 0.8666898608207703
Epoch 560, training loss: 0.6997396349906921 = 0.032284971326589584 + 0.1 * 6.674546241760254
Epoch 560, val loss: 0.8753511905670166
Epoch 570, training loss: 0.697347104549408 = 0.030176671221852303 + 0.1 * 6.671703815460205
Epoch 570, val loss: 0.8838678002357483
Epoch 580, training loss: 0.6949190497398376 = 0.028260059654712677 + 0.1 * 6.666589736938477
Epoch 580, val loss: 0.8922944068908691
Epoch 590, training loss: 0.694611668586731 = 0.02651544101536274 + 0.1 * 6.680962562561035
Epoch 590, val loss: 0.9004387855529785
Epoch 600, training loss: 0.6901050806045532 = 0.024925662204623222 + 0.1 * 6.651793956756592
Epoch 600, val loss: 0.9082373976707458
Epoch 610, training loss: 0.6886811852455139 = 0.023471491411328316 + 0.1 * 6.652096748352051
Epoch 610, val loss: 0.9160152077674866
Epoch 620, training loss: 0.687039315700531 = 0.02214176580309868 + 0.1 * 6.648975372314453
Epoch 620, val loss: 0.9234178066253662
Epoch 630, training loss: 0.6850885152816772 = 0.020924225449562073 + 0.1 * 6.641643047332764
Epoch 630, val loss: 0.9307398796081543
Epoch 640, training loss: 0.6828847527503967 = 0.01980482041835785 + 0.1 * 6.630799293518066
Epoch 640, val loss: 0.937883734703064
Epoch 650, training loss: 0.682604193687439 = 0.018771808594465256 + 0.1 * 6.6383233070373535
Epoch 650, val loss: 0.9447829723358154
Epoch 660, training loss: 0.6809555292129517 = 0.017820095643401146 + 0.1 * 6.631353855133057
Epoch 660, val loss: 0.9515697360038757
Epoch 670, training loss: 0.6793336868286133 = 0.016939278692007065 + 0.1 * 6.62394380569458
Epoch 670, val loss: 0.9581315517425537
Epoch 680, training loss: 0.6775460839271545 = 0.016124462708830833 + 0.1 * 6.614215850830078
Epoch 680, val loss: 0.9645805358886719
Epoch 690, training loss: 0.676114559173584 = 0.01536861713975668 + 0.1 * 6.60745906829834
Epoch 690, val loss: 0.9708653092384338
Epoch 700, training loss: 0.6768329739570618 = 0.014666465111076832 + 0.1 * 6.621665000915527
Epoch 700, val loss: 0.9769454598426819
Epoch 710, training loss: 0.6739775538444519 = 0.014014985412359238 + 0.1 * 6.599626064300537
Epoch 710, val loss: 0.9828665256500244
Epoch 720, training loss: 0.6732770800590515 = 0.013406429439783096 + 0.1 * 6.598706245422363
Epoch 720, val loss: 0.9888015389442444
Epoch 730, training loss: 0.6747825741767883 = 0.01283793430775404 + 0.1 * 6.619446277618408
Epoch 730, val loss: 0.9945136308670044
Epoch 740, training loss: 0.6722316145896912 = 0.012308093719184399 + 0.1 * 6.5992350578308105
Epoch 740, val loss: 1.0000948905944824
Epoch 750, training loss: 0.6714568138122559 = 0.011811497621238232 + 0.1 * 6.5964531898498535
Epoch 750, val loss: 1.0055367946624756
Epoch 760, training loss: 0.6700787544250488 = 0.011347084306180477 + 0.1 * 6.587316989898682
Epoch 760, val loss: 1.0109037160873413
Epoch 770, training loss: 0.6700518727302551 = 0.010910280048847198 + 0.1 * 6.591415882110596
Epoch 770, val loss: 1.016159176826477
Epoch 780, training loss: 0.668571949005127 = 0.010499597527086735 + 0.1 * 6.580723285675049
Epoch 780, val loss: 1.0212730169296265
Epoch 790, training loss: 0.6681140661239624 = 0.010113158263266087 + 0.1 * 6.5800089836120605
Epoch 790, val loss: 1.0262863636016846
Epoch 800, training loss: 0.6675503849983215 = 0.009749427437782288 + 0.1 * 6.578009605407715
Epoch 800, val loss: 1.0311959981918335
Epoch 810, training loss: 0.6673047542572021 = 0.00940728560090065 + 0.1 * 6.578974723815918
Epoch 810, val loss: 1.0359642505645752
Epoch 820, training loss: 0.6664407849311829 = 0.009084131568670273 + 0.1 * 6.573566436767578
Epoch 820, val loss: 1.0407447814941406
Epoch 830, training loss: 0.6656209230422974 = 0.008779359981417656 + 0.1 * 6.568415641784668
Epoch 830, val loss: 1.0452555418014526
Epoch 840, training loss: 0.6669623851776123 = 0.008489814586937428 + 0.1 * 6.584725856781006
Epoch 840, val loss: 1.0498276948928833
Epoch 850, training loss: 0.6655397415161133 = 0.008217304944992065 + 0.1 * 6.5732245445251465
Epoch 850, val loss: 1.054258108139038
Epoch 860, training loss: 0.6640915274620056 = 0.007958021946251392 + 0.1 * 6.56133508682251
Epoch 860, val loss: 1.0585380792617798
Epoch 870, training loss: 0.6637257933616638 = 0.0077121676877141 + 0.1 * 6.560135841369629
Epoch 870, val loss: 1.0627915859222412
Epoch 880, training loss: 0.6637942790985107 = 0.007479064632207155 + 0.1 * 6.563151836395264
Epoch 880, val loss: 1.066938042640686
Epoch 890, training loss: 0.6634910106658936 = 0.00725717656314373 + 0.1 * 6.562338352203369
Epoch 890, val loss: 1.0710163116455078
Epoch 900, training loss: 0.6623042225837708 = 0.007046106271445751 + 0.1 * 6.552581310272217
Epoch 900, val loss: 1.0749794244766235
Epoch 910, training loss: 0.6652572751045227 = 0.006845090072602034 + 0.1 * 6.584121227264404
Epoch 910, val loss: 1.0788774490356445
Epoch 920, training loss: 0.6613255739212036 = 0.006654202006757259 + 0.1 * 6.546713352203369
Epoch 920, val loss: 1.0826503038406372
Epoch 930, training loss: 0.6609634757041931 = 0.00647213589400053 + 0.1 * 6.544913291931152
Epoch 930, val loss: 1.0864434242248535
Epoch 940, training loss: 0.6602790355682373 = 0.006298088002949953 + 0.1 * 6.539809703826904
Epoch 940, val loss: 1.0902049541473389
Epoch 950, training loss: 0.6610426306724548 = 0.006132342852652073 + 0.1 * 6.549102783203125
Epoch 950, val loss: 1.0937154293060303
Epoch 960, training loss: 0.6602618098258972 = 0.0059748622588813305 + 0.1 * 6.5428690910339355
Epoch 960, val loss: 1.0972298383712769
Epoch 970, training loss: 0.6594603061676025 = 0.005823219660669565 + 0.1 * 6.536370754241943
Epoch 970, val loss: 1.1007475852966309
Epoch 980, training loss: 0.6586911678314209 = 0.005677793640643358 + 0.1 * 6.530133247375488
Epoch 980, val loss: 1.104156255722046
Epoch 990, training loss: 0.6589466333389282 = 0.005539040081202984 + 0.1 * 6.534075736999512
Epoch 990, val loss: 1.107534646987915
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8350026357406432
The final CL Acc:0.81481, 0.01684, The final GNN Acc:0.83834, 0.00280
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9432])
updated graph: torch.Size([2, 10508])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8133058547973633 = 1.953622579574585 + 0.1 * 8.596831321716309
Epoch 0, val loss: 1.9500689506530762
Epoch 10, training loss: 2.8031768798828125 = 1.9434993267059326 + 0.1 * 8.596776008605957
Epoch 10, val loss: 1.9404473304748535
Epoch 20, training loss: 2.7909936904907227 = 1.931354284286499 + 0.1 * 8.596393585205078
Epoch 20, val loss: 1.9285550117492676
Epoch 30, training loss: 2.773664712905884 = 1.9143402576446533 + 0.1 * 8.593243598937988
Epoch 30, val loss: 1.911661982536316
Epoch 40, training loss: 2.7461605072021484 = 1.8891594409942627 + 0.1 * 8.570009231567383
Epoch 40, val loss: 1.8867226839065552
Epoch 50, training loss: 2.700838088989258 = 1.8543965816497803 + 0.1 * 8.464415550231934
Epoch 50, val loss: 1.8539273738861084
Epoch 60, training loss: 2.633894681930542 = 1.8168030977249146 + 0.1 * 8.170915603637695
Epoch 60, val loss: 1.821848750114441
Epoch 70, training loss: 2.579723358154297 = 1.7853498458862305 + 0.1 * 7.943735122680664
Epoch 70, val loss: 1.7965017557144165
Epoch 80, training loss: 2.4998586177825928 = 1.7538307905197144 + 0.1 * 7.460278034210205
Epoch 80, val loss: 1.7667275667190552
Epoch 90, training loss: 2.4269771575927734 = 1.7155029773712158 + 0.1 * 7.11474084854126
Epoch 90, val loss: 1.7298401594161987
Epoch 100, training loss: 2.364210605621338 = 1.6638416051864624 + 0.1 * 7.00369119644165
Epoch 100, val loss: 1.6821495294570923
Epoch 110, training loss: 2.292640447616577 = 1.595739722251892 + 0.1 * 6.969006538391113
Epoch 110, val loss: 1.620310664176941
Epoch 120, training loss: 2.2106335163116455 = 1.5152603387832642 + 0.1 * 6.953732013702393
Epoch 120, val loss: 1.5486118793487549
Epoch 130, training loss: 2.1244654655456543 = 1.430110216140747 + 0.1 * 6.943551063537598
Epoch 130, val loss: 1.4758659601211548
Epoch 140, training loss: 2.038020372390747 = 1.344208002090454 + 0.1 * 6.9381232261657715
Epoch 140, val loss: 1.4056990146636963
Epoch 150, training loss: 1.9498333930969238 = 1.2562601566314697 + 0.1 * 6.935732841491699
Epoch 150, val loss: 1.3363782167434692
Epoch 160, training loss: 1.8588693141937256 = 1.1654716730117798 + 0.1 * 6.933975696563721
Epoch 160, val loss: 1.2664016485214233
Epoch 170, training loss: 1.76676344871521 = 1.0735002756118774 + 0.1 * 6.932631492614746
Epoch 170, val loss: 1.1979860067367554
Epoch 180, training loss: 1.676300048828125 = 0.9831295609474182 + 0.1 * 6.931705474853516
Epoch 180, val loss: 1.13270103931427
Epoch 190, training loss: 1.589442491531372 = 0.8964129686355591 + 0.1 * 6.930295944213867
Epoch 190, val loss: 1.071712851524353
Epoch 200, training loss: 1.5066794157028198 = 0.813896119594574 + 0.1 * 6.927833080291748
Epoch 200, val loss: 1.0153571367263794
Epoch 210, training loss: 1.4281213283538818 = 0.7356581687927246 + 0.1 * 6.924631595611572
Epoch 210, val loss: 0.9638198018074036
Epoch 220, training loss: 1.35404372215271 = 0.6621177792549133 + 0.1 * 6.919259548187256
Epoch 220, val loss: 0.9176510572433472
Epoch 230, training loss: 1.2854703664779663 = 0.5942324995994568 + 0.1 * 6.912378787994385
Epoch 230, val loss: 0.8774771690368652
Epoch 240, training loss: 1.222497582435608 = 0.5321725606918335 + 0.1 * 6.903250217437744
Epoch 240, val loss: 0.8431570529937744
Epoch 250, training loss: 1.1640512943267822 = 0.4749795198440552 + 0.1 * 6.890718460083008
Epoch 250, val loss: 0.8137344121932983
Epoch 260, training loss: 1.1104800701141357 = 0.422090619802475 + 0.1 * 6.883894920349121
Epoch 260, val loss: 0.7884352803230286
Epoch 270, training loss: 1.0603110790252686 = 0.3732282817363739 + 0.1 * 6.870828151702881
Epoch 270, val loss: 0.7666705250740051
Epoch 280, training loss: 1.0123885869979858 = 0.32738810777664185 + 0.1 * 6.85000467300415
Epoch 280, val loss: 0.7478170394897461
Epoch 290, training loss: 0.9684736132621765 = 0.28497886657714844 + 0.1 * 6.834947109222412
Epoch 290, val loss: 0.7328089475631714
Epoch 300, training loss: 0.9299306273460388 = 0.246928870677948 + 0.1 * 6.830017566680908
Epoch 300, val loss: 0.7221563458442688
Epoch 310, training loss: 0.8949176073074341 = 0.21400384604930878 + 0.1 * 6.809137344360352
Epoch 310, val loss: 0.7157490253448486
Epoch 320, training loss: 0.8656498193740845 = 0.18573662638664246 + 0.1 * 6.799132347106934
Epoch 320, val loss: 0.7134315967559814
Epoch 330, training loss: 0.8430962562561035 = 0.16170300543308258 + 0.1 * 6.813932418823242
Epoch 330, val loss: 0.7147243618965149
Epoch 340, training loss: 0.820178747177124 = 0.14162595570087433 + 0.1 * 6.785528182983398
Epoch 340, val loss: 0.7184472680091858
Epoch 350, training loss: 0.8018038272857666 = 0.12464911490678787 + 0.1 * 6.771546840667725
Epoch 350, val loss: 0.724230170249939
Epoch 360, training loss: 0.7866818904876709 = 0.11019832640886307 + 0.1 * 6.764835834503174
Epoch 360, val loss: 0.7318087816238403
Epoch 370, training loss: 0.7747210264205933 = 0.09784471243619919 + 0.1 * 6.768763065338135
Epoch 370, val loss: 0.7406164407730103
Epoch 380, training loss: 0.7628050446510315 = 0.08732294291257858 + 0.1 * 6.754820823669434
Epoch 380, val loss: 0.7500855922698975
Epoch 390, training loss: 0.753020703792572 = 0.07828903198242188 + 0.1 * 6.747316837310791
Epoch 390, val loss: 0.7601391077041626
Epoch 400, training loss: 0.7449637055397034 = 0.07046379894018173 + 0.1 * 6.744998931884766
Epoch 400, val loss: 0.770747184753418
Epoch 410, training loss: 0.7380982637405396 = 0.06368710845708847 + 0.1 * 6.74411153793335
Epoch 410, val loss: 0.781413733959198
Epoch 420, training loss: 0.7312305569648743 = 0.057794924825429916 + 0.1 * 6.734355926513672
Epoch 420, val loss: 0.7921972870826721
Epoch 430, training loss: 0.7262184023857117 = 0.052627306431531906 + 0.1 * 6.735910892486572
Epoch 430, val loss: 0.803071916103363
Epoch 440, training loss: 0.720439612865448 = 0.04809591546654701 + 0.1 * 6.72343635559082
Epoch 440, val loss: 0.8138269782066345
Epoch 450, training loss: 0.7154620289802551 = 0.044097259640693665 + 0.1 * 6.713647842407227
Epoch 450, val loss: 0.8245503306388855
Epoch 460, training loss: 0.712394654750824 = 0.04055140167474747 + 0.1 * 6.7184319496154785
Epoch 460, val loss: 0.8351745009422302
Epoch 470, training loss: 0.7080923914909363 = 0.03741651028394699 + 0.1 * 6.706758499145508
Epoch 470, val loss: 0.8454816937446594
Epoch 480, training loss: 0.7042826414108276 = 0.034631069749593735 + 0.1 * 6.6965155601501465
Epoch 480, val loss: 0.8554900884628296
Epoch 490, training loss: 0.7008952498435974 = 0.032141681760549545 + 0.1 * 6.687535762786865
Epoch 490, val loss: 0.8655083179473877
Epoch 500, training loss: 0.6984986066818237 = 0.02991446852684021 + 0.1 * 6.685841083526611
Epoch 500, val loss: 0.875058650970459
Epoch 510, training loss: 0.6958297491073608 = 0.02791532129049301 + 0.1 * 6.679144382476807
Epoch 510, val loss: 0.8845201134681702
Epoch 520, training loss: 0.6936150193214417 = 0.02611244097352028 + 0.1 * 6.675025463104248
Epoch 520, val loss: 0.893646776676178
Epoch 530, training loss: 0.6921149492263794 = 0.02448459528386593 + 0.1 * 6.676303863525391
Epoch 530, val loss: 0.902773916721344
Epoch 540, training loss: 0.6894188523292542 = 0.023013275116682053 + 0.1 * 6.664055347442627
Epoch 540, val loss: 0.9112564921379089
Epoch 550, training loss: 0.6910663843154907 = 0.021673602983355522 + 0.1 * 6.693927764892578
Epoch 550, val loss: 0.9200209975242615
Epoch 560, training loss: 0.6864386796951294 = 0.020459702238440514 + 0.1 * 6.659789562225342
Epoch 560, val loss: 0.9279922842979431
Epoch 570, training loss: 0.6837000846862793 = 0.01935063861310482 + 0.1 * 6.643494129180908
Epoch 570, val loss: 0.93594890832901
Epoch 580, training loss: 0.6842753887176514 = 0.018330393359065056 + 0.1 * 6.659450054168701
Epoch 580, val loss: 0.9438760876655579
Epoch 590, training loss: 0.6808770895004272 = 0.01739591918885708 + 0.1 * 6.634811878204346
Epoch 590, val loss: 0.9515542387962341
Epoch 600, training loss: 0.6796255707740784 = 0.016537059098482132 + 0.1 * 6.630884647369385
Epoch 600, val loss: 0.9587439894676208
Epoch 610, training loss: 0.6809236407279968 = 0.015741726383566856 + 0.1 * 6.651819229125977
Epoch 610, val loss: 0.9661238789558411
Epoch 620, training loss: 0.6775367259979248 = 0.015008000656962395 + 0.1 * 6.6252875328063965
Epoch 620, val loss: 0.9732804894447327
Epoch 630, training loss: 0.6765186190605164 = 0.014330043457448483 + 0.1 * 6.621885776519775
Epoch 630, val loss: 0.979992687702179
Epoch 640, training loss: 0.6772220134735107 = 0.013698888942599297 + 0.1 * 6.6352314949035645
Epoch 640, val loss: 0.9867802262306213
Epoch 650, training loss: 0.6749517917633057 = 0.013112570159137249 + 0.1 * 6.618392467498779
Epoch 650, val loss: 0.9935851097106934
Epoch 660, training loss: 0.6736330986022949 = 0.01256946288049221 + 0.1 * 6.610636234283447
Epoch 660, val loss: 0.9996113181114197
Epoch 670, training loss: 0.6732061505317688 = 0.012061254121363163 + 0.1 * 6.611449241638184
Epoch 670, val loss: 1.0059503316879272
Epoch 680, training loss: 0.6716002225875854 = 0.011586811393499374 + 0.1 * 6.600134372711182
Epoch 680, val loss: 1.0119274854660034
Epoch 690, training loss: 0.6714448928833008 = 0.011141413822770119 + 0.1 * 6.603034496307373
Epoch 690, val loss: 1.0182735919952393
Epoch 700, training loss: 0.6707046031951904 = 0.010725898668169975 + 0.1 * 6.599786758422852
Epoch 700, val loss: 1.0237956047058105
Epoch 710, training loss: 0.671239972114563 = 0.010335845872759819 + 0.1 * 6.6090407371521
Epoch 710, val loss: 1.0294499397277832
Epoch 720, training loss: 0.6700166463851929 = 0.009969132021069527 + 0.1 * 6.600475311279297
Epoch 720, val loss: 1.0351279973983765
Epoch 730, training loss: 0.6678158640861511 = 0.009625126607716084 + 0.1 * 6.581907272338867
Epoch 730, val loss: 1.0404245853424072
Epoch 740, training loss: 0.6693722009658813 = 0.009300142526626587 + 0.1 * 6.6007208824157715
Epoch 740, val loss: 1.0456370115280151
Epoch 750, training loss: 0.6670846343040466 = 0.008993353694677353 + 0.1 * 6.580913066864014
Epoch 750, val loss: 1.0510464906692505
Epoch 760, training loss: 0.6670951247215271 = 0.00870520155876875 + 0.1 * 6.583899021148682
Epoch 760, val loss: 1.0558700561523438
Epoch 770, training loss: 0.6659497618675232 = 0.008431574329733849 + 0.1 * 6.57518196105957
Epoch 770, val loss: 1.0608711242675781
Epoch 780, training loss: 0.6659141778945923 = 0.008171952329576015 + 0.1 * 6.577422142028809
Epoch 780, val loss: 1.0660080909729004
Epoch 790, training loss: 0.6637202501296997 = 0.007927671074867249 + 0.1 * 6.557925701141357
Epoch 790, val loss: 1.0705522298812866
Epoch 800, training loss: 0.6637259721755981 = 0.00769668398424983 + 0.1 * 6.560292720794678
Epoch 800, val loss: 1.0749115943908691
Epoch 810, training loss: 0.6629012227058411 = 0.007475508376955986 + 0.1 * 6.554256916046143
Epoch 810, val loss: 1.0797815322875977
Epoch 820, training loss: 0.6625586748123169 = 0.0072664543986320496 + 0.1 * 6.552921772003174
Epoch 820, val loss: 1.0838606357574463
Epoch 830, training loss: 0.6616341471672058 = 0.0070672063156962395 + 0.1 * 6.545669078826904
Epoch 830, val loss: 1.0883150100708008
Epoch 840, training loss: 0.6622770428657532 = 0.006877261213958263 + 0.1 * 6.553997993469238
Epoch 840, val loss: 1.0925768613815308
Epoch 850, training loss: 0.661084771156311 = 0.006695696618407965 + 0.1 * 6.543890953063965
Epoch 850, val loss: 1.0967260599136353
Epoch 860, training loss: 0.6604136228561401 = 0.006523636635392904 + 0.1 * 6.538899898529053
Epoch 860, val loss: 1.1005611419677734
Epoch 870, training loss: 0.6594799160957336 = 0.006358072627335787 + 0.1 * 6.5312180519104
Epoch 870, val loss: 1.1047621965408325
Epoch 880, training loss: 0.6586636304855347 = 0.0062003713101148605 + 0.1 * 6.524632453918457
Epoch 880, val loss: 1.1084564924240112
Epoch 890, training loss: 0.6607097387313843 = 0.006049801595509052 + 0.1 * 6.546599388122559
Epoch 890, val loss: 1.1122058629989624
Epoch 900, training loss: 0.657967746257782 = 0.0059045650996267796 + 0.1 * 6.520631790161133
Epoch 900, val loss: 1.1162670850753784
Epoch 910, training loss: 0.6579362750053406 = 0.005766567308455706 + 0.1 * 6.521697044372559
Epoch 910, val loss: 1.1197245121002197
Epoch 920, training loss: 0.6587182879447937 = 0.005633240565657616 + 0.1 * 6.530850410461426
Epoch 920, val loss: 1.1234707832336426
Epoch 930, training loss: 0.6563677191734314 = 0.005505249835550785 + 0.1 * 6.50862455368042
Epoch 930, val loss: 1.1269911527633667
Epoch 940, training loss: 0.6586449146270752 = 0.005383078940212727 + 0.1 * 6.532618045806885
Epoch 940, val loss: 1.1304274797439575
Epoch 950, training loss: 0.6564579010009766 = 0.0052645946852862835 + 0.1 * 6.511933326721191
Epoch 950, val loss: 1.1339802742004395
Epoch 960, training loss: 0.6574516892433167 = 0.005151237826794386 + 0.1 * 6.523004531860352
Epoch 960, val loss: 1.1373785734176636
Epoch 970, training loss: 0.6550561785697937 = 0.005041805561631918 + 0.1 * 6.500144004821777
Epoch 970, val loss: 1.1406530141830444
Epoch 980, training loss: 0.6554359793663025 = 0.004936709068715572 + 0.1 * 6.504992485046387
Epoch 980, val loss: 1.1439393758773804
Epoch 990, training loss: 0.6553049683570862 = 0.0048347399570047855 + 0.1 * 6.504702568054199
Epoch 990, val loss: 1.1473151445388794
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 2.8003320693969727 = 1.940646767616272 + 0.1 * 8.596854209899902
Epoch 0, val loss: 1.9426259994506836
Epoch 10, training loss: 2.790356397628784 = 1.9306787252426147 + 0.1 * 8.596776008605957
Epoch 10, val loss: 1.9323174953460693
Epoch 20, training loss: 2.7779927253723145 = 1.91835618019104 + 0.1 * 8.59636402130127
Epoch 20, val loss: 1.9194833040237427
Epoch 30, training loss: 2.7605576515197754 = 1.9012545347213745 + 0.1 * 8.593031883239746
Epoch 30, val loss: 1.9018678665161133
Epoch 40, training loss: 2.733708143234253 = 1.8767297267913818 + 0.1 * 8.569783210754395
Epoch 40, val loss: 1.8772720098495483
Epoch 50, training loss: 2.691734790802002 = 1.8444851636886597 + 0.1 * 8.472496032714844
Epoch 50, val loss: 1.8465584516525269
Epoch 60, training loss: 2.627960681915283 = 1.810782790184021 + 0.1 * 8.17177963256836
Epoch 60, val loss: 1.8163131475448608
Epoch 70, training loss: 2.5771563053131104 = 1.7803488969802856 + 0.1 * 7.96807336807251
Epoch 70, val loss: 1.7876616716384888
Epoch 80, training loss: 2.5019044876098633 = 1.7461212873458862 + 0.1 * 7.557833194732666
Epoch 80, val loss: 1.7535890340805054
Epoch 90, training loss: 2.4318385124206543 = 1.7038662433624268 + 0.1 * 7.279722213745117
Epoch 90, val loss: 1.7143480777740479
Epoch 100, training loss: 2.3615500926971436 = 1.6466501951217651 + 0.1 * 7.148999214172363
Epoch 100, val loss: 1.6641879081726074
Epoch 110, training loss: 2.277836322784424 = 1.5742107629776 + 0.1 * 7.036255836486816
Epoch 110, val loss: 1.6015831232070923
Epoch 120, training loss: 2.1903955936431885 = 1.4923640489578247 + 0.1 * 6.980315685272217
Epoch 120, val loss: 1.5327733755111694
Epoch 130, training loss: 2.104980230331421 = 1.409377932548523 + 0.1 * 6.9560227394104
Epoch 130, val loss: 1.4652416706085205
Epoch 140, training loss: 2.0231847763061523 = 1.3289756774902344 + 0.1 * 6.942091941833496
Epoch 140, val loss: 1.403145432472229
Epoch 150, training loss: 1.9449414014816284 = 1.2514618635177612 + 0.1 * 6.934795379638672
Epoch 150, val loss: 1.3465570211410522
Epoch 160, training loss: 1.8705546855926514 = 1.1773810386657715 + 0.1 * 6.931736946105957
Epoch 160, val loss: 1.2947828769683838
Epoch 170, training loss: 1.7975656986236572 = 1.1046817302703857 + 0.1 * 6.928839206695557
Epoch 170, val loss: 1.244467854499817
Epoch 180, training loss: 1.7229260206222534 = 1.0302046537399292 + 0.1 * 6.927213668823242
Epoch 180, val loss: 1.192062258720398
Epoch 190, training loss: 1.6459035873413086 = 0.9533964395523071 + 0.1 * 6.925070762634277
Epoch 190, val loss: 1.1370105743408203
Epoch 200, training loss: 1.5687350034713745 = 0.8765988349914551 + 0.1 * 6.921361446380615
Epoch 200, val loss: 1.0813871622085571
Epoch 210, training loss: 1.49588942527771 = 0.8037379384040833 + 0.1 * 6.921515464782715
Epoch 210, val loss: 1.0293304920196533
Epoch 220, training loss: 1.4295425415039062 = 0.7385886311531067 + 0.1 * 6.909538745880127
Epoch 220, val loss: 0.9841461181640625
Epoch 230, training loss: 1.3713102340698242 = 0.6811950206756592 + 0.1 * 6.90115213394165
Epoch 230, val loss: 0.9468016624450684
Epoch 240, training loss: 1.3189494609832764 = 0.6300018429756165 + 0.1 * 6.889476776123047
Epoch 240, val loss: 0.9161044955253601
Epoch 250, training loss: 1.2709455490112305 = 0.5831692814826965 + 0.1 * 6.877762317657471
Epoch 250, val loss: 0.8912367224693298
Epoch 260, training loss: 1.2263329029083252 = 0.5395691990852356 + 0.1 * 6.867637634277344
Epoch 260, val loss: 0.8708087205886841
Epoch 270, training loss: 1.183197259902954 = 0.4979909062385559 + 0.1 * 6.852062702178955
Epoch 270, val loss: 0.8540356755256653
Epoch 280, training loss: 1.1429919004440308 = 0.4581225514411926 + 0.1 * 6.848693370819092
Epoch 280, val loss: 0.8405851721763611
Epoch 290, training loss: 1.1043025255203247 = 0.42053303122520447 + 0.1 * 6.837695121765137
Epoch 290, val loss: 0.8299320340156555
Epoch 300, training loss: 1.0676908493041992 = 0.3851853013038635 + 0.1 * 6.825055122375488
Epoch 300, val loss: 0.8219010233879089
Epoch 310, training loss: 1.0337163209915161 = 0.351982980966568 + 0.1 * 6.817333698272705
Epoch 310, val loss: 0.8158224821090698
Epoch 320, training loss: 1.0019170045852661 = 0.32093897461891174 + 0.1 * 6.809780597686768
Epoch 320, val loss: 0.8116559386253357
Epoch 330, training loss: 0.9723111391067505 = 0.2919553816318512 + 0.1 * 6.803557872772217
Epoch 330, val loss: 0.808893620967865
Epoch 340, training loss: 0.9439631700515747 = 0.2644357681274414 + 0.1 * 6.795273780822754
Epoch 340, val loss: 0.8073605298995972
Epoch 350, training loss: 0.9181787371635437 = 0.23806379735469818 + 0.1 * 6.801148891448975
Epoch 350, val loss: 0.8071799278259277
Epoch 360, training loss: 0.8919540643692017 = 0.21304890513420105 + 0.1 * 6.789051055908203
Epoch 360, val loss: 0.8078502416610718
Epoch 370, training loss: 0.8669065833091736 = 0.18957240879535675 + 0.1 * 6.773341655731201
Epoch 370, val loss: 0.8101719617843628
Epoch 380, training loss: 0.8452883958816528 = 0.16810373961925507 + 0.1 * 6.771846294403076
Epoch 380, val loss: 0.8140732645988464
Epoch 390, training loss: 0.8251223564147949 = 0.14921046793460846 + 0.1 * 6.759119033813477
Epoch 390, val loss: 0.8193212747573853
Epoch 400, training loss: 0.808102011680603 = 0.13280156254768372 + 0.1 * 6.753004550933838
Epoch 400, val loss: 0.8260491490364075
Epoch 410, training loss: 0.795229971408844 = 0.11856682598590851 + 0.1 * 6.766631603240967
Epoch 410, val loss: 0.8339534997940063
Epoch 420, training loss: 0.7802997827529907 = 0.1062990128993988 + 0.1 * 6.7400078773498535
Epoch 420, val loss: 0.8414537310600281
Epoch 430, training loss: 0.7681403160095215 = 0.09559659659862518 + 0.1 * 6.725437164306641
Epoch 430, val loss: 0.8498744964599609
Epoch 440, training loss: 0.7587589025497437 = 0.08622416853904724 + 0.1 * 6.72534704208374
Epoch 440, val loss: 0.8589141368865967
Epoch 450, training loss: 0.7494413256645203 = 0.0780329555273056 + 0.1 * 6.714083194732666
Epoch 450, val loss: 0.8669883012771606
Epoch 460, training loss: 0.7408345937728882 = 0.07079607248306274 + 0.1 * 6.700385093688965
Epoch 460, val loss: 0.8755435943603516
Epoch 470, training loss: 0.7333964109420776 = 0.06439444422721863 + 0.1 * 6.690019130706787
Epoch 470, val loss: 0.8843908905982971
Epoch 480, training loss: 0.7265151739120483 = 0.058737702667713165 + 0.1 * 6.677774429321289
Epoch 480, val loss: 0.8924558162689209
Epoch 490, training loss: 0.7212544679641724 = 0.05369170382618904 + 0.1 * 6.675627708435059
Epoch 490, val loss: 0.9008038640022278
Epoch 500, training loss: 0.7167077660560608 = 0.04919252544641495 + 0.1 * 6.67515230178833
Epoch 500, val loss: 0.9092109799385071
Epoch 510, training loss: 0.7112270593643188 = 0.04517858847975731 + 0.1 * 6.660484313964844
Epoch 510, val loss: 0.9171631336212158
Epoch 520, training loss: 0.7067396640777588 = 0.04158635064959526 + 0.1 * 6.651533126831055
Epoch 520, val loss: 0.9251898527145386
Epoch 530, training loss: 0.704214870929718 = 0.03837353736162186 + 0.1 * 6.658413410186768
Epoch 530, val loss: 0.9332205653190613
Epoch 540, training loss: 0.6996079683303833 = 0.03550436720252037 + 0.1 * 6.641036033630371
Epoch 540, val loss: 0.9406260848045349
Epoch 550, training loss: 0.6959317922592163 = 0.032931286841630936 + 0.1 * 6.630005359649658
Epoch 550, val loss: 0.9482646584510803
Epoch 560, training loss: 0.6943867802619934 = 0.03061356581747532 + 0.1 * 6.637732028961182
Epoch 560, val loss: 0.9557496309280396
Epoch 570, training loss: 0.6904004216194153 = 0.02853511832654476 + 0.1 * 6.618652820587158
Epoch 570, val loss: 0.9627872109413147
Epoch 580, training loss: 0.6885718703269958 = 0.02666124887764454 + 0.1 * 6.619105815887451
Epoch 580, val loss: 0.9697694182395935
Epoch 590, training loss: 0.6853488683700562 = 0.024969058111310005 + 0.1 * 6.6037983894348145
Epoch 590, val loss: 0.9767104387283325
Epoch 600, training loss: 0.6835821270942688 = 0.023435017094016075 + 0.1 * 6.601471424102783
Epoch 600, val loss: 0.9832700490951538
Epoch 610, training loss: 0.6820593476295471 = 0.022041430696845055 + 0.1 * 6.600179195404053
Epoch 610, val loss: 0.989855170249939
Epoch 620, training loss: 0.6814777851104736 = 0.020773500204086304 + 0.1 * 6.60704231262207
Epoch 620, val loss: 0.9961149096488953
Epoch 630, training loss: 0.6780980229377747 = 0.019616255536675453 + 0.1 * 6.584817409515381
Epoch 630, val loss: 1.0022571086883545
Epoch 640, training loss: 0.6776061654090881 = 0.018557198345661163 + 0.1 * 6.590489864349365
Epoch 640, val loss: 1.0081850290298462
Epoch 650, training loss: 0.6763445734977722 = 0.01758711226284504 + 0.1 * 6.587574481964111
Epoch 650, val loss: 1.014106035232544
Epoch 660, training loss: 0.6737305521965027 = 0.016693828627467155 + 0.1 * 6.570367336273193
Epoch 660, val loss: 1.0196683406829834
Epoch 670, training loss: 0.6740541458129883 = 0.015870172530412674 + 0.1 * 6.5818400382995605
Epoch 670, val loss: 1.0254614353179932
Epoch 680, training loss: 0.672072172164917 = 0.015111810527741909 + 0.1 * 6.569603443145752
Epoch 680, val loss: 1.030762791633606
Epoch 690, training loss: 0.6699758768081665 = 0.014410238713026047 + 0.1 * 6.5556559562683105
Epoch 690, val loss: 1.035969853401184
Epoch 700, training loss: 0.6712852716445923 = 0.013758584856987 + 0.1 * 6.575266361236572
Epoch 700, val loss: 1.0413670539855957
Epoch 710, training loss: 0.6686031818389893 = 0.013154569081962109 + 0.1 * 6.554486274719238
Epoch 710, val loss: 1.0464006662368774
Epoch 720, training loss: 0.6664946675300598 = 0.012592894956469536 + 0.1 * 6.539017677307129
Epoch 720, val loss: 1.051161527633667
Epoch 730, training loss: 0.6675464510917664 = 0.012066739611327648 + 0.1 * 6.554797172546387
Epoch 730, val loss: 1.056102991104126
Epoch 740, training loss: 0.6659020781517029 = 0.011576325632631779 + 0.1 * 6.543257236480713
Epoch 740, val loss: 1.0608141422271729
Epoch 750, training loss: 0.6647384166717529 = 0.01111761387437582 + 0.1 * 6.536207675933838
Epoch 750, val loss: 1.0656055212020874
Epoch 760, training loss: 0.6641243100166321 = 0.010689320042729378 + 0.1 * 6.5343499183654785
Epoch 760, val loss: 1.070067286491394
Epoch 770, training loss: 0.6625282764434814 = 0.010285933502018452 + 0.1 * 6.522423267364502
Epoch 770, val loss: 1.0746557712554932
Epoch 780, training loss: 0.6628037095069885 = 0.009907805360853672 + 0.1 * 6.528959274291992
Epoch 780, val loss: 1.0789170265197754
Epoch 790, training loss: 0.6621095538139343 = 0.009551878087222576 + 0.1 * 6.525576591491699
Epoch 790, val loss: 1.0832406282424927
Epoch 800, training loss: 0.6610779762268066 = 0.009216555394232273 + 0.1 * 6.518614292144775
Epoch 800, val loss: 1.0874779224395752
Epoch 810, training loss: 0.6602950692176819 = 0.008901075460016727 + 0.1 * 6.51393985748291
Epoch 810, val loss: 1.0913639068603516
Epoch 820, training loss: 0.6615443229675293 = 0.008602509275078773 + 0.1 * 6.529417991638184
Epoch 820, val loss: 1.0953375101089478
Epoch 830, training loss: 0.6599453091621399 = 0.008319472894072533 + 0.1 * 6.516258239746094
Epoch 830, val loss: 1.0995731353759766
Epoch 840, training loss: 0.6591352224349976 = 0.008053372614085674 + 0.1 * 6.5108184814453125
Epoch 840, val loss: 1.1031848192214966
Epoch 850, training loss: 0.6578438878059387 = 0.007800491061061621 + 0.1 * 6.500433444976807
Epoch 850, val loss: 1.107056736946106
Epoch 860, training loss: 0.6583684682846069 = 0.007560411933809519 + 0.1 * 6.508080005645752
Epoch 860, val loss: 1.1107820272445679
Epoch 870, training loss: 0.6569266319274902 = 0.007332851178944111 + 0.1 * 6.495937824249268
Epoch 870, val loss: 1.114819884300232
Epoch 880, training loss: 0.6564686894416809 = 0.007117788307368755 + 0.1 * 6.493508815765381
Epoch 880, val loss: 1.1180812120437622
Epoch 890, training loss: 0.6561407446861267 = 0.006913245189934969 + 0.1 * 6.492274761199951
Epoch 890, val loss: 1.1214702129364014
Epoch 900, training loss: 0.6582196354866028 = 0.006717011798173189 + 0.1 * 6.515026092529297
Epoch 900, val loss: 1.12507963180542
Epoch 910, training loss: 0.6558415293693542 = 0.006530815735459328 + 0.1 * 6.493106842041016
Epoch 910, val loss: 1.1285934448242188
Epoch 920, training loss: 0.6554307341575623 = 0.006353180389851332 + 0.1 * 6.490775108337402
Epoch 920, val loss: 1.131756067276001
Epoch 930, training loss: 0.6547031402587891 = 0.006183689460158348 + 0.1 * 6.485194206237793
Epoch 930, val loss: 1.1350926160812378
Epoch 940, training loss: 0.6553497910499573 = 0.006021545734256506 + 0.1 * 6.493282318115234
Epoch 940, val loss: 1.138590693473816
Epoch 950, training loss: 0.6548275351524353 = 0.00586752500385046 + 0.1 * 6.489599704742432
Epoch 950, val loss: 1.141565203666687
Epoch 960, training loss: 0.6538740396499634 = 0.005720003508031368 + 0.1 * 6.481540203094482
Epoch 960, val loss: 1.1445729732513428
Epoch 970, training loss: 0.6532291769981384 = 0.005578950047492981 + 0.1 * 6.476502418518066
Epoch 970, val loss: 1.1476359367370605
Epoch 980, training loss: 0.6522080302238464 = 0.005442829802632332 + 0.1 * 6.4676513671875
Epoch 980, val loss: 1.1507794857025146
Epoch 990, training loss: 0.6529533863067627 = 0.005312724970281124 + 0.1 * 6.476406574249268
Epoch 990, val loss: 1.1536450386047363
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.7954665260938324
=== training gcn model ===
Epoch 0, training loss: 2.7977800369262695 = 1.9380977153778076 + 0.1 * 8.596821784973145
Epoch 0, val loss: 1.929124355316162
Epoch 10, training loss: 2.7876996994018555 = 1.9280328750610352 + 0.1 * 8.59666919708252
Epoch 10, val loss: 1.9193150997161865
Epoch 20, training loss: 2.7746715545654297 = 1.915102243423462 + 0.1 * 8.595691680908203
Epoch 20, val loss: 1.9061392545700073
Epoch 30, training loss: 2.7554259300231934 = 1.8967490196228027 + 0.1 * 8.586769104003906
Epoch 30, val loss: 1.8870341777801514
Epoch 40, training loss: 2.7239420413970947 = 1.870323657989502 + 0.1 * 8.53618335723877
Epoch 40, val loss: 1.8598721027374268
Epoch 50, training loss: 2.666212797164917 = 1.8373980522155762 + 0.1 * 8.288147926330566
Epoch 50, val loss: 1.8281431198120117
Epoch 60, training loss: 2.6105594635009766 = 1.8041936159133911 + 0.1 * 8.063657760620117
Epoch 60, val loss: 1.798833966255188
Epoch 70, training loss: 2.5444083213806152 = 1.7739441394805908 + 0.1 * 7.704641819000244
Epoch 70, val loss: 1.7734907865524292
Epoch 80, training loss: 2.478912115097046 = 1.7397688627243042 + 0.1 * 7.391432762145996
Epoch 80, val loss: 1.7436403036117554
Epoch 90, training loss: 2.420088291168213 = 1.6952576637268066 + 0.1 * 7.2483062744140625
Epoch 90, val loss: 1.7030996084213257
Epoch 100, training loss: 2.3484106063842773 = 1.6364572048187256 + 0.1 * 7.119533538818359
Epoch 100, val loss: 1.6502934694290161
Epoch 110, training loss: 2.267605781555176 = 1.5650930404663086 + 0.1 * 7.0251264572143555
Epoch 110, val loss: 1.5883523225784302
Epoch 120, training loss: 2.1848561763763428 = 1.4886614084243774 + 0.1 * 6.961946964263916
Epoch 120, val loss: 1.5258350372314453
Epoch 130, training loss: 2.1049258708953857 = 1.413805603981018 + 0.1 * 6.9112019538879395
Epoch 130, val loss: 1.4686049222946167
Epoch 140, training loss: 2.0285139083862305 = 1.3403338193893433 + 0.1 * 6.881801128387451
Epoch 140, val loss: 1.4157562255859375
Epoch 150, training loss: 1.9515831470489502 = 1.265842080116272 + 0.1 * 6.857410430908203
Epoch 150, val loss: 1.3640031814575195
Epoch 160, training loss: 1.8750885725021362 = 1.1906418800354004 + 0.1 * 6.844466686248779
Epoch 160, val loss: 1.3129630088806152
Epoch 170, training loss: 1.801701307296753 = 1.1186920404434204 + 0.1 * 6.830092430114746
Epoch 170, val loss: 1.2655357122421265
Epoch 180, training loss: 1.7346165180206299 = 1.0522444248199463 + 0.1 * 6.823720455169678
Epoch 180, val loss: 1.2230700254440308
Epoch 190, training loss: 1.674283742904663 = 0.9931598901748657 + 0.1 * 6.811237812042236
Epoch 190, val loss: 1.1865949630737305
Epoch 200, training loss: 1.6203497648239136 = 0.9400911927223206 + 0.1 * 6.802585601806641
Epoch 200, val loss: 1.1545008420944214
Epoch 210, training loss: 1.5708447694778442 = 0.8907220959663391 + 0.1 * 6.801226615905762
Epoch 210, val loss: 1.1247034072875977
Epoch 220, training loss: 1.5216741561889648 = 0.8429856300354004 + 0.1 * 6.786885738372803
Epoch 220, val loss: 1.0955586433410645
Epoch 230, training loss: 1.4722665548324585 = 0.7942957282066345 + 0.1 * 6.779707908630371
Epoch 230, val loss: 1.0651448965072632
Epoch 240, training loss: 1.4209198951721191 = 0.7439495921134949 + 0.1 * 6.769703388214111
Epoch 240, val loss: 1.0333894491195679
Epoch 250, training loss: 1.3698970079421997 = 0.6922852396965027 + 0.1 * 6.776117324829102
Epoch 250, val loss: 1.0011364221572876
Epoch 260, training loss: 1.3164381980895996 = 0.6413313150405884 + 0.1 * 6.751068592071533
Epoch 260, val loss: 0.9706037640571594
Epoch 270, training loss: 1.2664128541946411 = 0.592241644859314 + 0.1 * 6.7417120933532715
Epoch 270, val loss: 0.943743884563446
Epoch 280, training loss: 1.2187764644622803 = 0.5457899570465088 + 0.1 * 6.729864120483398
Epoch 280, val loss: 0.9217224717140198
Epoch 290, training loss: 1.175065517425537 = 0.5028885006904602 + 0.1 * 6.7217698097229
Epoch 290, val loss: 0.9048267602920532
Epoch 300, training loss: 1.13419771194458 = 0.4632418751716614 + 0.1 * 6.709558486938477
Epoch 300, val loss: 0.8922154307365417
Epoch 310, training loss: 1.09779691696167 = 0.42611613869667053 + 0.1 * 6.7168073654174805
Epoch 310, val loss: 0.8835740685462952
Epoch 320, training loss: 1.061427116394043 = 0.39188554883003235 + 0.1 * 6.695415496826172
Epoch 320, val loss: 0.8779120445251465
Epoch 330, training loss: 1.0282704830169678 = 0.36021551489830017 + 0.1 * 6.6805500984191895
Epoch 330, val loss: 0.875669538974762
Epoch 340, training loss: 1.0012725591659546 = 0.33107420802116394 + 0.1 * 6.7019829750061035
Epoch 340, val loss: 0.8765015602111816
Epoch 350, training loss: 0.9714148640632629 = 0.3045715093612671 + 0.1 * 6.66843318939209
Epoch 350, val loss: 0.8791728615760803
Epoch 360, training loss: 0.9459311962127686 = 0.2801355719566345 + 0.1 * 6.657956123352051
Epoch 360, val loss: 0.8840616941452026
Epoch 370, training loss: 0.9236141443252563 = 0.25724783539772034 + 0.1 * 6.663662910461426
Epoch 370, val loss: 0.8900371193885803
Epoch 380, training loss: 0.8995290994644165 = 0.23561283946037292 + 0.1 * 6.639163017272949
Epoch 380, val loss: 0.8967478275299072
Epoch 390, training loss: 0.878481388092041 = 0.2149648368358612 + 0.1 * 6.635165214538574
Epoch 390, val loss: 0.904181718826294
Epoch 400, training loss: 0.8581615686416626 = 0.19531098008155823 + 0.1 * 6.628505706787109
Epoch 400, val loss: 0.9121053814888
Epoch 410, training loss: 0.8391761183738708 = 0.17680613696575165 + 0.1 * 6.62369966506958
Epoch 410, val loss: 0.92027348279953
Epoch 420, training loss: 0.8246738910675049 = 0.15955200791358948 + 0.1 * 6.651218414306641
Epoch 420, val loss: 0.9293434619903564
Epoch 430, training loss: 0.8070383667945862 = 0.14378972351551056 + 0.1 * 6.632486343383789
Epoch 430, val loss: 0.9386701583862305
Epoch 440, training loss: 0.7901740670204163 = 0.12954580783843994 + 0.1 * 6.6062822341918945
Epoch 440, val loss: 0.9487071633338928
Epoch 450, training loss: 0.776364803314209 = 0.11674773693084717 + 0.1 * 6.596170425415039
Epoch 450, val loss: 0.9595098495483398
Epoch 460, training loss: 0.766507089138031 = 0.10529851913452148 + 0.1 * 6.612085342407227
Epoch 460, val loss: 0.9710012674331665
Epoch 470, training loss: 0.7542849183082581 = 0.09512899816036224 + 0.1 * 6.591558933258057
Epoch 470, val loss: 0.9825918674468994
Epoch 480, training loss: 0.7467910051345825 = 0.08613366633653641 + 0.1 * 6.606573581695557
Epoch 480, val loss: 0.9948140382766724
Epoch 490, training loss: 0.7364628314971924 = 0.07818326354026794 + 0.1 * 6.582795143127441
Epoch 490, val loss: 1.007017731666565
Epoch 500, training loss: 0.7285392880439758 = 0.07112657278776169 + 0.1 * 6.574127197265625
Epoch 500, val loss: 1.0193886756896973
Epoch 510, training loss: 0.7233467102050781 = 0.06485515832901001 + 0.1 * 6.5849151611328125
Epoch 510, val loss: 1.032057285308838
Epoch 520, training loss: 0.7162166833877563 = 0.059292472898960114 + 0.1 * 6.569242000579834
Epoch 520, val loss: 1.0444190502166748
Epoch 530, training loss: 0.7149052619934082 = 0.05434383451938629 + 0.1 * 6.605614185333252
Epoch 530, val loss: 1.0570623874664307
Epoch 540, training loss: 0.7078126072883606 = 0.04996489733457565 + 0.1 * 6.578476905822754
Epoch 540, val loss: 1.069103479385376
Epoch 550, training loss: 0.7025156021118164 = 0.04606587812304497 + 0.1 * 6.564497470855713
Epoch 550, val loss: 1.0812389850616455
Epoch 560, training loss: 0.6977840065956116 = 0.042567960917949677 + 0.1 * 6.552160263061523
Epoch 560, val loss: 1.0930042266845703
Epoch 570, training loss: 0.6967912316322327 = 0.039424046874046326 + 0.1 * 6.573671817779541
Epoch 570, val loss: 1.1048122644424438
Epoch 580, training loss: 0.6913256049156189 = 0.036603473126888275 + 0.1 * 6.5472211837768555
Epoch 580, val loss: 1.115990400314331
Epoch 590, training loss: 0.6889039874076843 = 0.034058827906847 + 0.1 * 6.5484514236450195
Epoch 590, val loss: 1.12727952003479
Epoch 600, training loss: 0.6852965354919434 = 0.03176739811897278 + 0.1 * 6.5352911949157715
Epoch 600, val loss: 1.1380751132965088
Epoch 610, training loss: 0.6832473278045654 = 0.029696250334382057 + 0.1 * 6.535511016845703
Epoch 610, val loss: 1.1486045122146606
Epoch 620, training loss: 0.6813369989395142 = 0.02782076969742775 + 0.1 * 6.535161972045898
Epoch 620, val loss: 1.1589654684066772
Epoch 630, training loss: 0.6800025105476379 = 0.026117321103811264 + 0.1 * 6.538851737976074
Epoch 630, val loss: 1.168918251991272
Epoch 640, training loss: 0.6776298880577087 = 0.02456299215555191 + 0.1 * 6.53066873550415
Epoch 640, val loss: 1.1787748336791992
Epoch 650, training loss: 0.675616979598999 = 0.02314576506614685 + 0.1 * 6.524711608886719
Epoch 650, val loss: 1.1882802248001099
Epoch 660, training loss: 0.6753186583518982 = 0.021848268806934357 + 0.1 * 6.534703731536865
Epoch 660, val loss: 1.1975558996200562
Epoch 670, training loss: 0.6727880239486694 = 0.020662488415837288 + 0.1 * 6.5212554931640625
Epoch 670, val loss: 1.2065200805664062
Epoch 680, training loss: 0.6735496520996094 = 0.019574914127588272 + 0.1 * 6.5397467613220215
Epoch 680, val loss: 1.2153352499008179
Epoch 690, training loss: 0.6701101660728455 = 0.01857839524745941 + 0.1 * 6.515317440032959
Epoch 690, val loss: 1.223724603652954
Epoch 700, training loss: 0.669058620929718 = 0.017658818513154984 + 0.1 * 6.513997554779053
Epoch 700, val loss: 1.2320702075958252
Epoch 710, training loss: 0.6678213477134705 = 0.01680804416537285 + 0.1 * 6.510133266448975
Epoch 710, val loss: 1.2402613162994385
Epoch 720, training loss: 0.6669696569442749 = 0.016020657494664192 + 0.1 * 6.509490013122559
Epoch 720, val loss: 1.248061180114746
Epoch 730, training loss: 0.6659356951713562 = 0.015290317125618458 + 0.1 * 6.506453514099121
Epoch 730, val loss: 1.255783200263977
Epoch 740, training loss: 0.6661434769630432 = 0.014611600898206234 + 0.1 * 6.515318393707275
Epoch 740, val loss: 1.2633459568023682
Epoch 750, training loss: 0.6641473174095154 = 0.013980998657643795 + 0.1 * 6.5016632080078125
Epoch 750, val loss: 1.2705068588256836
Epoch 760, training loss: 0.6636229157447815 = 0.01339325774461031 + 0.1 * 6.502296447753906
Epoch 760, val loss: 1.2778013944625854
Epoch 770, training loss: 0.6617224812507629 = 0.012845143675804138 + 0.1 * 6.488772869110107
Epoch 770, val loss: 1.2846095561981201
Epoch 780, training loss: 0.6621788144111633 = 0.012332114391028881 + 0.1 * 6.498466491699219
Epoch 780, val loss: 1.2914644479751587
Epoch 790, training loss: 0.6605975031852722 = 0.011852285824716091 + 0.1 * 6.487452507019043
Epoch 790, val loss: 1.2981261014938354
Epoch 800, training loss: 0.6602446436882019 = 0.011401657946407795 + 0.1 * 6.488429546356201
Epoch 800, val loss: 1.304586410522461
Epoch 810, training loss: 0.6588035225868225 = 0.01097808126360178 + 0.1 * 6.4782538414001465
Epoch 810, val loss: 1.3109803199768066
Epoch 820, training loss: 0.6585521101951599 = 0.010578234679996967 + 0.1 * 6.479738712310791
Epoch 820, val loss: 1.3174028396606445
Epoch 830, training loss: 0.659023642539978 = 0.010203085839748383 + 0.1 * 6.488205432891846
Epoch 830, val loss: 1.323415994644165
Epoch 840, training loss: 0.6592785120010376 = 0.009850341826677322 + 0.1 * 6.494281768798828
Epoch 840, val loss: 1.3294546604156494
Epoch 850, training loss: 0.6573546528816223 = 0.009518262930214405 + 0.1 * 6.478363513946533
Epoch 850, val loss: 1.3351068496704102
Epoch 860, training loss: 0.6565810441970825 = 0.009204333648085594 + 0.1 * 6.473766803741455
Epoch 860, val loss: 1.3407490253448486
Epoch 870, training loss: 0.6562062501907349 = 0.008906523697078228 + 0.1 * 6.472996711730957
Epoch 870, val loss: 1.3464999198913574
Epoch 880, training loss: 0.6550055742263794 = 0.0086244847625494 + 0.1 * 6.463810920715332
Epoch 880, val loss: 1.352026104927063
Epoch 890, training loss: 0.654282808303833 = 0.008356817066669464 + 0.1 * 6.459259986877441
Epoch 890, val loss: 1.3575602769851685
Epoch 900, training loss: 0.6541606783866882 = 0.008103830739855766 + 0.1 * 6.460567951202393
Epoch 900, val loss: 1.3626710176467896
Epoch 910, training loss: 0.6551487445831299 = 0.00786281656473875 + 0.1 * 6.4728593826293945
Epoch 910, val loss: 1.3679488897323608
Epoch 920, training loss: 0.6539195775985718 = 0.007634683046489954 + 0.1 * 6.462848663330078
Epoch 920, val loss: 1.3728841543197632
Epoch 930, training loss: 0.6524391174316406 = 0.007416993845254183 + 0.1 * 6.450221061706543
Epoch 930, val loss: 1.3777694702148438
Epoch 940, training loss: 0.653418242931366 = 0.007209230680018663 + 0.1 * 6.462090015411377
Epoch 940, val loss: 1.3828731775283813
Epoch 950, training loss: 0.6519332528114319 = 0.007011933717876673 + 0.1 * 6.449213027954102
Epoch 950, val loss: 1.3874415159225464
Epoch 960, training loss: 0.650951087474823 = 0.006824043113738298 + 0.1 * 6.441270351409912
Epoch 960, val loss: 1.3918644189834595
Epoch 970, training loss: 0.6542418003082275 = 0.006643808912485838 + 0.1 * 6.475979804992676
Epoch 970, val loss: 1.3966174125671387
Epoch 980, training loss: 0.6518704891204834 = 0.006471266970038414 + 0.1 * 6.453991889953613
Epoch 980, val loss: 1.401252269744873
Epoch 990, training loss: 0.650806188583374 = 0.006307014264166355 + 0.1 * 6.444991588592529
Epoch 990, val loss: 1.4054557085037231
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.7944122298365841
The final CL Acc:0.75309, 0.01259, The final GNN Acc:0.79951, 0.00648
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13206])
remove edge: torch.Size([2, 7854])
updated graph: torch.Size([2, 10504])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8025026321411133 = 1.9428205490112305 + 0.1 * 8.596819877624512
Epoch 0, val loss: 1.9481827020645142
Epoch 10, training loss: 2.7927331924438477 = 1.93306303024292 + 0.1 * 8.596702575683594
Epoch 10, val loss: 1.9388879537582397
Epoch 20, training loss: 2.7804479598999023 = 1.920859456062317 + 0.1 * 8.595884323120117
Epoch 20, val loss: 1.9268983602523804
Epoch 30, training loss: 2.7623748779296875 = 1.9034934043884277 + 0.1 * 8.588813781738281
Epoch 30, val loss: 1.909449577331543
Epoch 40, training loss: 2.7322568893432617 = 1.8776612281799316 + 0.1 * 8.545957565307617
Epoch 40, val loss: 1.883541464805603
Epoch 50, training loss: 2.677523612976074 = 1.8423885107040405 + 0.1 * 8.351351737976074
Epoch 50, val loss: 1.8493616580963135
Epoch 60, training loss: 2.606255054473877 = 1.801790475845337 + 0.1 * 8.044644355773926
Epoch 60, val loss: 1.8117735385894775
Epoch 70, training loss: 2.5394113063812256 = 1.7623767852783203 + 0.1 * 7.7703447341918945
Epoch 70, val loss: 1.7747788429260254
Epoch 80, training loss: 2.4647698402404785 = 1.720154047012329 + 0.1 * 7.446156978607178
Epoch 80, val loss: 1.734065055847168
Epoch 90, training loss: 2.391824960708618 = 1.6670026779174805 + 0.1 * 7.248223304748535
Epoch 90, val loss: 1.6842656135559082
Epoch 100, training loss: 2.313913345336914 = 1.5967650413513184 + 0.1 * 7.171481609344482
Epoch 100, val loss: 1.621322751045227
Epoch 110, training loss: 2.2208683490753174 = 1.510597586631775 + 0.1 * 7.102708339691162
Epoch 110, val loss: 1.5474497079849243
Epoch 120, training loss: 2.1210861206054688 = 1.4169193506240845 + 0.1 * 7.041668891906738
Epoch 120, val loss: 1.4692925214767456
Epoch 130, training loss: 2.0219242572784424 = 1.3221051692962646 + 0.1 * 6.998190879821777
Epoch 130, val loss: 1.3922275304794312
Epoch 140, training loss: 1.9255057573318481 = 1.2288389205932617 + 0.1 * 6.966668128967285
Epoch 140, val loss: 1.3186206817626953
Epoch 150, training loss: 1.833262324333191 = 1.1386256217956543 + 0.1 * 6.946366786956787
Epoch 150, val loss: 1.248567819595337
Epoch 160, training loss: 1.745848536491394 = 1.0525859594345093 + 0.1 * 6.932625770568848
Epoch 160, val loss: 1.182692289352417
Epoch 170, training loss: 1.6643688678741455 = 0.9720652103424072 + 0.1 * 6.923036575317383
Epoch 170, val loss: 1.121929407119751
Epoch 180, training loss: 1.5892889499664307 = 0.8981443643569946 + 0.1 * 6.911445617675781
Epoch 180, val loss: 1.0664573907852173
Epoch 190, training loss: 1.5198954343795776 = 0.830039381980896 + 0.1 * 6.898560523986816
Epoch 190, val loss: 1.0154160261154175
Epoch 200, training loss: 1.4553149938583374 = 0.7668442130088806 + 0.1 * 6.884707927703857
Epoch 200, val loss: 0.9684689044952393
Epoch 210, training loss: 1.3951480388641357 = 0.7080642580986023 + 0.1 * 6.870837688446045
Epoch 210, val loss: 0.9254607558250427
Epoch 220, training loss: 1.339911937713623 = 0.6538742780685425 + 0.1 * 6.860375881195068
Epoch 220, val loss: 0.8873884677886963
Epoch 230, training loss: 1.2877459526062012 = 0.603121280670166 + 0.1 * 6.846246242523193
Epoch 230, val loss: 0.8531782031059265
Epoch 240, training loss: 1.2386457920074463 = 0.5543833374977112 + 0.1 * 6.842624187469482
Epoch 240, val loss: 0.8222928643226624
Epoch 250, training loss: 1.19045889377594 = 0.50738126039505 + 0.1 * 6.830776214599609
Epoch 250, val loss: 0.7946158051490784
Epoch 260, training loss: 1.1441594362258911 = 0.46170774102211 + 0.1 * 6.824517250061035
Epoch 260, val loss: 0.7699207067489624
Epoch 270, training loss: 1.099328637123108 = 0.4174577295780182 + 0.1 * 6.818708896636963
Epoch 270, val loss: 0.7484822869300842
Epoch 280, training loss: 1.0563814640045166 = 0.37490713596343994 + 0.1 * 6.814743518829346
Epoch 280, val loss: 0.7302516102790833
Epoch 290, training loss: 1.0162537097930908 = 0.33471059799194336 + 0.1 * 6.815431118011475
Epoch 290, val loss: 0.7152684330940247
Epoch 300, training loss: 0.9779317378997803 = 0.29716143012046814 + 0.1 * 6.807703018188477
Epoch 300, val loss: 0.7030603885650635
Epoch 310, training loss: 0.9426733255386353 = 0.2623538672924042 + 0.1 * 6.803194522857666
Epoch 310, val loss: 0.6938393712043762
Epoch 320, training loss: 0.9113869667053223 = 0.23059803247451782 + 0.1 * 6.807889461517334
Epoch 320, val loss: 0.6877993941307068
Epoch 330, training loss: 0.8820564150810242 = 0.20232032239437103 + 0.1 * 6.797360420227051
Epoch 330, val loss: 0.6850930452346802
Epoch 340, training loss: 0.8564631938934326 = 0.17736533284187317 + 0.1 * 6.790978908538818
Epoch 340, val loss: 0.6853394508361816
Epoch 350, training loss: 0.8346992135047913 = 0.15556621551513672 + 0.1 * 6.791329860687256
Epoch 350, val loss: 0.6883751749992371
Epoch 360, training loss: 0.8154126405715942 = 0.1367894411087036 + 0.1 * 6.786231994628906
Epoch 360, val loss: 0.6937717795372009
Epoch 370, training loss: 0.7983994483947754 = 0.12061478942632675 + 0.1 * 6.777846813201904
Epoch 370, val loss: 0.701171338558197
Epoch 380, training loss: 0.7838467359542847 = 0.10668127238750458 + 0.1 * 6.7716546058654785
Epoch 380, val loss: 0.7100703716278076
Epoch 390, training loss: 0.772129476070404 = 0.09462922066450119 + 0.1 * 6.775002479553223
Epoch 390, val loss: 0.7202995419502258
Epoch 400, training loss: 0.7612525224685669 = 0.0842767283320427 + 0.1 * 6.7697577476501465
Epoch 400, val loss: 0.7314233183860779
Epoch 410, training loss: 0.7508726119995117 = 0.07533938437700272 + 0.1 * 6.755331993103027
Epoch 410, val loss: 0.7431617975234985
Epoch 420, training loss: 0.742363691329956 = 0.06757672876119614 + 0.1 * 6.747869491577148
Epoch 420, val loss: 0.7554327845573425
Epoch 430, training loss: 0.7361521124839783 = 0.06084996461868286 + 0.1 * 6.753021240234375
Epoch 430, val loss: 0.7678812742233276
Epoch 440, training loss: 0.7279771566390991 = 0.05503619834780693 + 0.1 * 6.729409694671631
Epoch 440, val loss: 0.7802358269691467
Epoch 450, training loss: 0.7232314944267273 = 0.04997070133686066 + 0.1 * 6.732607841491699
Epoch 450, val loss: 0.7925431132316589
Epoch 460, training loss: 0.7167349457740784 = 0.04555349051952362 + 0.1 * 6.7118144035339355
Epoch 460, val loss: 0.8046239614486694
Epoch 470, training loss: 0.7127152681350708 = 0.04167833924293518 + 0.1 * 6.710369110107422
Epoch 470, val loss: 0.8164780735969543
Epoch 480, training loss: 0.7119589447975159 = 0.038276556879282 + 0.1 * 6.736823558807373
Epoch 480, val loss: 0.8279383182525635
Epoch 490, training loss: 0.7039194107055664 = 0.03530344367027283 + 0.1 * 6.686159610748291
Epoch 490, val loss: 0.8389613628387451
Epoch 500, training loss: 0.7001069188117981 = 0.0326683446764946 + 0.1 * 6.6743855476379395
Epoch 500, val loss: 0.8495644927024841
Epoch 510, training loss: 0.6973099112510681 = 0.03032422997057438 + 0.1 * 6.669857025146484
Epoch 510, val loss: 0.8598378896713257
Epoch 520, training loss: 0.6945701837539673 = 0.02822859026491642 + 0.1 * 6.663415908813477
Epoch 520, val loss: 0.8697233200073242
Epoch 530, training loss: 0.6915627717971802 = 0.026346281170845032 + 0.1 * 6.652164936065674
Epoch 530, val loss: 0.8792878985404968
Epoch 540, training loss: 0.6899576187133789 = 0.024649353697896004 + 0.1 * 6.653082370758057
Epoch 540, val loss: 0.888576328754425
Epoch 550, training loss: 0.6880269646644592 = 0.0231166984885931 + 0.1 * 6.649102210998535
Epoch 550, val loss: 0.8975945115089417
Epoch 560, training loss: 0.6857695579528809 = 0.021727552637457848 + 0.1 * 6.640419960021973
Epoch 560, val loss: 0.9063323140144348
Epoch 570, training loss: 0.6858831644058228 = 0.020468221977353096 + 0.1 * 6.654149055480957
Epoch 570, val loss: 0.9147922396659851
Epoch 580, training loss: 0.6829444169998169 = 0.019321773201227188 + 0.1 * 6.636226177215576
Epoch 580, val loss: 0.9229742288589478
Epoch 590, training loss: 0.6808909177780151 = 0.018273644149303436 + 0.1 * 6.6261725425720215
Epoch 590, val loss: 0.930915892124176
Epoch 600, training loss: 0.6803692579269409 = 0.017311440780758858 + 0.1 * 6.63057804107666
Epoch 600, val loss: 0.9386270642280579
Epoch 610, training loss: 0.6785997152328491 = 0.01642768643796444 + 0.1 * 6.621720314025879
Epoch 610, val loss: 0.9461582899093628
Epoch 620, training loss: 0.6769441366195679 = 0.015613735653460026 + 0.1 * 6.613304138183594
Epoch 620, val loss: 0.9534780979156494
Epoch 630, training loss: 0.6757465600967407 = 0.014858228154480457 + 0.1 * 6.608882904052734
Epoch 630, val loss: 0.9606199860572815
Epoch 640, training loss: 0.6799267530441284 = 0.014161380007863045 + 0.1 * 6.657653331756592
Epoch 640, val loss: 0.9675762057304382
Epoch 650, training loss: 0.6748384833335876 = 0.01351788267493248 + 0.1 * 6.613205909729004
Epoch 650, val loss: 0.9743034243583679
Epoch 660, training loss: 0.6731827855110168 = 0.012922627851366997 + 0.1 * 6.602601528167725
Epoch 660, val loss: 0.980900764465332
Epoch 670, training loss: 0.6721842288970947 = 0.012368110939860344 + 0.1 * 6.598161220550537
Epoch 670, val loss: 0.987223744392395
Epoch 680, training loss: 0.6720043420791626 = 0.011851131916046143 + 0.1 * 6.601531982421875
Epoch 680, val loss: 0.9934325218200684
Epoch 690, training loss: 0.6719603538513184 = 0.011369360610842705 + 0.1 * 6.605909824371338
Epoch 690, val loss: 0.9995629191398621
Epoch 700, training loss: 0.6701017022132874 = 0.010918878950178623 + 0.1 * 6.591828346252441
Epoch 700, val loss: 1.005433201789856
Epoch 710, training loss: 0.6691257953643799 = 0.01049706432968378 + 0.1 * 6.586287021636963
Epoch 710, val loss: 1.011296272277832
Epoch 720, training loss: 0.670050323009491 = 0.010101309046149254 + 0.1 * 6.599489688873291
Epoch 720, val loss: 1.0169397592544556
Epoch 730, training loss: 0.6669038534164429 = 0.009730972349643707 + 0.1 * 6.571728706359863
Epoch 730, val loss: 1.0224885940551758
Epoch 740, training loss: 0.6672378182411194 = 0.009383777156472206 + 0.1 * 6.578540325164795
Epoch 740, val loss: 1.027950644493103
Epoch 750, training loss: 0.6680917739868164 = 0.009055621922016144 + 0.1 * 6.590361595153809
Epoch 750, val loss: 1.0331889390945435
Epoch 760, training loss: 0.6654068231582642 = 0.008747467771172523 + 0.1 * 6.566593647003174
Epoch 760, val loss: 1.038334846496582
Epoch 770, training loss: 0.6642726063728333 = 0.00845767930150032 + 0.1 * 6.558149337768555
Epoch 770, val loss: 1.0434290170669556
Epoch 780, training loss: 0.6658931970596313 = 0.008183117024600506 + 0.1 * 6.57710075378418
Epoch 780, val loss: 1.0483678579330444
Epoch 790, training loss: 0.663769543170929 = 0.007924073375761509 + 0.1 * 6.558454513549805
Epoch 790, val loss: 1.0531381368637085
Epoch 800, training loss: 0.6632020473480225 = 0.007678810972720385 + 0.1 * 6.555232524871826
Epoch 800, val loss: 1.0579276084899902
Epoch 810, training loss: 0.662148654460907 = 0.007446787785738707 + 0.1 * 6.547018527984619
Epoch 810, val loss: 1.0624685287475586
Epoch 820, training loss: 0.6624488234519958 = 0.007226142566651106 + 0.1 * 6.552226543426514
Epoch 820, val loss: 1.066997766494751
Epoch 830, training loss: 0.6616589426994324 = 0.007017544936388731 + 0.1 * 6.546413898468018
Epoch 830, val loss: 1.0714226961135864
Epoch 840, training loss: 0.6599918007850647 = 0.006818401627242565 + 0.1 * 6.531733989715576
Epoch 840, val loss: 1.0757285356521606
Epoch 850, training loss: 0.6621766090393066 = 0.006629123818129301 + 0.1 * 6.555474758148193
Epoch 850, val loss: 1.0799859762191772
Epoch 860, training loss: 0.6595708131790161 = 0.006448572967201471 + 0.1 * 6.531222343444824
Epoch 860, val loss: 1.084070086479187
Epoch 870, training loss: 0.6588520407676697 = 0.006278062704950571 + 0.1 * 6.525739669799805
Epoch 870, val loss: 1.0881257057189941
Epoch 880, training loss: 0.6590489745140076 = 0.006113952491432428 + 0.1 * 6.529350280761719
Epoch 880, val loss: 1.0920804738998413
Epoch 890, training loss: 0.6575157642364502 = 0.005958270747214556 + 0.1 * 6.515574932098389
Epoch 890, val loss: 1.0960065126419067
Epoch 900, training loss: 0.6570631861686707 = 0.005809368100017309 + 0.1 * 6.512537956237793
Epoch 900, val loss: 1.0997389554977417
Epoch 910, training loss: 0.656898558139801 = 0.005666232202202082 + 0.1 * 6.512322902679443
Epoch 910, val loss: 1.1034502983093262
Epoch 920, training loss: 0.6602559089660645 = 0.005529656540602446 + 0.1 * 6.547262668609619
Epoch 920, val loss: 1.1071375608444214
Epoch 930, training loss: 0.6560400128364563 = 0.005398674868047237 + 0.1 * 6.506412982940674
Epoch 930, val loss: 1.1105906963348389
Epoch 940, training loss: 0.6552389860153198 = 0.0052734436467289925 + 0.1 * 6.499655246734619
Epoch 940, val loss: 1.114092469215393
Epoch 950, training loss: 0.6554749011993408 = 0.005153209902346134 + 0.1 * 6.503216743469238
Epoch 950, val loss: 1.1175364255905151
Epoch 960, training loss: 0.6549908518791199 = 0.005037729628384113 + 0.1 * 6.499531269073486
Epoch 960, val loss: 1.1208330392837524
Epoch 970, training loss: 0.6547181010246277 = 0.004926745314151049 + 0.1 * 6.497913360595703
Epoch 970, val loss: 1.124096155166626
Epoch 980, training loss: 0.6535459160804749 = 0.00482036080211401 + 0.1 * 6.487255573272705
Epoch 980, val loss: 1.1273058652877808
Epoch 990, training loss: 0.6562307476997375 = 0.004717699717730284 + 0.1 * 6.515130043029785
Epoch 990, val loss: 1.1304824352264404
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.803281545639038 = 1.9435992240905762 + 0.1 * 8.596823692321777
Epoch 0, val loss: 1.9476239681243896
Epoch 10, training loss: 2.792807102203369 = 1.9331395626068115 + 0.1 * 8.596674919128418
Epoch 10, val loss: 1.9374185800552368
Epoch 20, training loss: 2.779271125793457 = 1.9197022914886475 + 0.1 * 8.595687866210938
Epoch 20, val loss: 1.92377769947052
Epoch 30, training loss: 2.759276866912842 = 1.9006011486053467 + 0.1 * 8.586755752563477
Epoch 30, val loss: 1.903895378112793
Epoch 40, training loss: 2.72625732421875 = 1.8729074001312256 + 0.1 * 8.533499717712402
Epoch 40, val loss: 1.8750672340393066
Epoch 50, training loss: 2.6646740436553955 = 1.8372060060501099 + 0.1 * 8.274681091308594
Epoch 50, val loss: 1.8393856287002563
Epoch 60, training loss: 2.598348379135132 = 1.7981632947921753 + 0.1 * 8.001851081848145
Epoch 60, val loss: 1.8023630380630493
Epoch 70, training loss: 2.5252692699432373 = 1.7615426778793335 + 0.1 * 7.637265205383301
Epoch 70, val loss: 1.769106149673462
Epoch 80, training loss: 2.4505438804626465 = 1.7216378450393677 + 0.1 * 7.289059638977051
Epoch 80, val loss: 1.732234001159668
Epoch 90, training loss: 2.385666847229004 = 1.671522855758667 + 0.1 * 7.1414384841918945
Epoch 90, val loss: 1.6867458820343018
Epoch 100, training loss: 2.312713146209717 = 1.6070326566696167 + 0.1 * 7.056804180145264
Epoch 100, val loss: 1.6299723386764526
Epoch 110, training loss: 2.2279906272888184 = 1.527457356452942 + 0.1 * 7.005332946777344
Epoch 110, val loss: 1.561103343963623
Epoch 120, training loss: 2.1339497566223145 = 1.437120795249939 + 0.1 * 6.968289852142334
Epoch 120, val loss: 1.4844452142715454
Epoch 130, training loss: 2.035736083984375 = 1.3415398597717285 + 0.1 * 6.941962718963623
Epoch 130, val loss: 1.4052897691726685
Epoch 140, training loss: 1.939314365386963 = 1.2469016313552856 + 0.1 * 6.924127101898193
Epoch 140, val loss: 1.3279674053192139
Epoch 150, training loss: 1.8472368717193604 = 1.1559211015701294 + 0.1 * 6.913157939910889
Epoch 150, val loss: 1.254016637802124
Epoch 160, training loss: 1.7593494653701782 = 1.0688748359680176 + 0.1 * 6.904746055603027
Epoch 160, val loss: 1.182725191116333
Epoch 170, training loss: 1.6754791736602783 = 0.9855742454528809 + 0.1 * 6.899049758911133
Epoch 170, val loss: 1.1145288944244385
Epoch 180, training loss: 1.5952422618865967 = 0.9065619707107544 + 0.1 * 6.88680362701416
Epoch 180, val loss: 1.0510660409927368
Epoch 190, training loss: 1.517500877380371 = 0.8301470875740051 + 0.1 * 6.873538494110107
Epoch 190, val loss: 0.9907216429710388
Epoch 200, training loss: 1.4420361518859863 = 0.7556231021881104 + 0.1 * 6.86413049697876
Epoch 200, val loss: 0.9324442744255066
Epoch 210, training loss: 1.3677029609680176 = 0.6829324960708618 + 0.1 * 6.8477044105529785
Epoch 210, val loss: 0.8760647773742676
Epoch 220, training loss: 1.2971209287643433 = 0.6133073568344116 + 0.1 * 6.838135719299316
Epoch 220, val loss: 0.8234184384346008
Epoch 230, training loss: 1.2301852703094482 = 0.5477664470672607 + 0.1 * 6.824188232421875
Epoch 230, val loss: 0.7757467031478882
Epoch 240, training loss: 1.1692767143249512 = 0.48686614632606506 + 0.1 * 6.824105262756348
Epoch 240, val loss: 0.734364926815033
Epoch 250, training loss: 1.112739086151123 = 0.43169769644737244 + 0.1 * 6.810413360595703
Epoch 250, val loss: 0.7004903554916382
Epoch 260, training loss: 1.0623815059661865 = 0.3819156289100647 + 0.1 * 6.80465841293335
Epoch 260, val loss: 0.6740391850471497
Epoch 270, training loss: 1.0174745321273804 = 0.3374938666820526 + 0.1 * 6.799806118011475
Epoch 270, val loss: 0.6545193791389465
Epoch 280, training loss: 0.9778604507446289 = 0.29856422543525696 + 0.1 * 6.792961597442627
Epoch 280, val loss: 0.6408683657646179
Epoch 290, training loss: 0.9429948329925537 = 0.26453712582588196 + 0.1 * 6.784576416015625
Epoch 290, val loss: 0.6321920156478882
Epoch 300, training loss: 0.9125241637229919 = 0.23466293513774872 + 0.1 * 6.77861213684082
Epoch 300, val loss: 0.6274084448814392
Epoch 310, training loss: 0.8857569694519043 = 0.20864877104759216 + 0.1 * 6.771081924438477
Epoch 310, val loss: 0.6256662607192993
Epoch 320, training loss: 0.861779510974884 = 0.185596764087677 + 0.1 * 6.76182746887207
Epoch 320, val loss: 0.6263790130615234
Epoch 330, training loss: 0.8403615951538086 = 0.16508927941322327 + 0.1 * 6.75272274017334
Epoch 330, val loss: 0.6289131045341492
Epoch 340, training loss: 0.8231814503669739 = 0.1469840258359909 + 0.1 * 6.761973857879639
Epoch 340, val loss: 0.6330152750015259
Epoch 350, training loss: 0.8051771521568298 = 0.13117028772830963 + 0.1 * 6.740068435668945
Epoch 350, val loss: 0.6382004618644714
Epoch 360, training loss: 0.789932131767273 = 0.11732982844114304 + 0.1 * 6.726022720336914
Epoch 360, val loss: 0.6445146799087524
Epoch 370, training loss: 0.7768428325653076 = 0.10526637732982635 + 0.1 * 6.715764045715332
Epoch 370, val loss: 0.6514829397201538
Epoch 380, training loss: 0.7649791240692139 = 0.09470054507255554 + 0.1 * 6.702785968780518
Epoch 380, val loss: 0.6592087149620056
Epoch 390, training loss: 0.754642128944397 = 0.08547834306955338 + 0.1 * 6.6916375160217285
Epoch 390, val loss: 0.6674538850784302
Epoch 400, training loss: 0.7463858127593994 = 0.07745090126991272 + 0.1 * 6.689349174499512
Epoch 400, val loss: 0.6759863495826721
Epoch 410, training loss: 0.7374708652496338 = 0.07043644040822983 + 0.1 * 6.670344352722168
Epoch 410, val loss: 0.6847521662712097
Epoch 420, training loss: 0.7306525111198425 = 0.06424761563539505 + 0.1 * 6.66404914855957
Epoch 420, val loss: 0.6937868595123291
Epoch 430, training loss: 0.7260298728942871 = 0.05876949429512024 + 0.1 * 6.672603130340576
Epoch 430, val loss: 0.7029282450675964
Epoch 440, training loss: 0.7205961346626282 = 0.0539608933031559 + 0.1 * 6.666352272033691
Epoch 440, val loss: 0.7121022343635559
Epoch 450, training loss: 0.7142238020896912 = 0.04970552399754524 + 0.1 * 6.6451826095581055
Epoch 450, val loss: 0.7210714817047119
Epoch 460, training loss: 0.7088620662689209 = 0.04591348394751549 + 0.1 * 6.629486083984375
Epoch 460, val loss: 0.7300645709037781
Epoch 470, training loss: 0.7050794959068298 = 0.042528700083494186 + 0.1 * 6.625507831573486
Epoch 470, val loss: 0.7389140725135803
Epoch 480, training loss: 0.7005566358566284 = 0.03948233649134636 + 0.1 * 6.610742568969727
Epoch 480, val loss: 0.7476543188095093
Epoch 490, training loss: 0.6995327472686768 = 0.03673103079199791 + 0.1 * 6.628017425537109
Epoch 490, val loss: 0.7563416361808777
Epoch 500, training loss: 0.6957340240478516 = 0.0342586450278759 + 0.1 * 6.614753723144531
Epoch 500, val loss: 0.7647959589958191
Epoch 510, training loss: 0.6914975643157959 = 0.03201913461089134 + 0.1 * 6.594784259796143
Epoch 510, val loss: 0.7730502486228943
Epoch 520, training loss: 0.689787745475769 = 0.02998071350157261 + 0.1 * 6.59807014465332
Epoch 520, val loss: 0.7811568975448608
Epoch 530, training loss: 0.687563419342041 = 0.028130603954195976 + 0.1 * 6.5943284034729
Epoch 530, val loss: 0.789138674736023
Epoch 540, training loss: 0.6842631101608276 = 0.026444975286722183 + 0.1 * 6.578181266784668
Epoch 540, val loss: 0.7968842387199402
Epoch 550, training loss: 0.6821749210357666 = 0.02490248903632164 + 0.1 * 6.572724342346191
Epoch 550, val loss: 0.8044503331184387
Epoch 560, training loss: 0.6835091710090637 = 0.023490093648433685 + 0.1 * 6.60019063949585
Epoch 560, val loss: 0.8119043707847595
Epoch 570, training loss: 0.6786826252937317 = 0.022198213264346123 + 0.1 * 6.564844131469727
Epoch 570, val loss: 0.8191069960594177
Epoch 580, training loss: 0.6773014664649963 = 0.02100880816578865 + 0.1 * 6.562926769256592
Epoch 580, val loss: 0.8261847496032715
Epoch 590, training loss: 0.6766963601112366 = 0.019913824275135994 + 0.1 * 6.567824840545654
Epoch 590, val loss: 0.8330461978912354
Epoch 600, training loss: 0.6746187210083008 = 0.018907101824879646 + 0.1 * 6.55711555480957
Epoch 600, val loss: 0.839812159538269
Epoch 610, training loss: 0.6745052337646484 = 0.01797512173652649 + 0.1 * 6.565300941467285
Epoch 610, val loss: 0.8463360667228699
Epoch 620, training loss: 0.671699583530426 = 0.01711406372487545 + 0.1 * 6.5458550453186035
Epoch 620, val loss: 0.8527204990386963
Epoch 630, training loss: 0.670951783657074 = 0.016314735636115074 + 0.1 * 6.546370029449463
Epoch 630, val loss: 0.85902339220047
Epoch 640, training loss: 0.6722334027290344 = 0.015570529736578465 + 0.1 * 6.566628456115723
Epoch 640, val loss: 0.8651085495948792
Epoch 650, training loss: 0.6688075065612793 = 0.014879914000630379 + 0.1 * 6.539275646209717
Epoch 650, val loss: 0.8711646199226379
Epoch 660, training loss: 0.6689936518669128 = 0.014236724935472012 + 0.1 * 6.5475687980651855
Epoch 660, val loss: 0.8770011067390442
Epoch 670, training loss: 0.6673845052719116 = 0.013635531067848206 + 0.1 * 6.537489891052246
Epoch 670, val loss: 0.8827519416809082
Epoch 680, training loss: 0.6668689250946045 = 0.013074015267193317 + 0.1 * 6.5379486083984375
Epoch 680, val loss: 0.8883273601531982
Epoch 690, training loss: 0.6657817363739014 = 0.012549197301268578 + 0.1 * 6.532325267791748
Epoch 690, val loss: 0.8938391208648682
Epoch 700, training loss: 0.6643794178962708 = 0.012057405896484852 + 0.1 * 6.523219585418701
Epoch 700, val loss: 0.8991715312004089
Epoch 710, training loss: 0.6626774668693542 = 0.011596712283790112 + 0.1 * 6.510807037353516
Epoch 710, val loss: 0.9044448733329773
Epoch 720, training loss: 0.6636886596679688 = 0.01116307731717825 + 0.1 * 6.525256156921387
Epoch 720, val loss: 0.9095236659049988
Epoch 730, training loss: 0.6618964076042175 = 0.010756193660199642 + 0.1 * 6.511402130126953
Epoch 730, val loss: 0.9145826101303101
Epoch 740, training loss: 0.6653302907943726 = 0.010372893884778023 + 0.1 * 6.54957389831543
Epoch 740, val loss: 0.9194886684417725
Epoch 750, training loss: 0.6606045961380005 = 0.010012468323111534 + 0.1 * 6.505921363830566
Epoch 750, val loss: 0.9242962002754211
Epoch 760, training loss: 0.659267783164978 = 0.009672589600086212 + 0.1 * 6.495952129364014
Epoch 760, val loss: 0.9290454387664795
Epoch 770, training loss: 0.6611124277114868 = 0.009349819272756577 + 0.1 * 6.51762580871582
Epoch 770, val loss: 0.9335679411888123
Epoch 780, training loss: 0.658938467502594 = 0.00904435757547617 + 0.1 * 6.498940944671631
Epoch 780, val loss: 0.9381365776062012
Epoch 790, training loss: 0.6585636734962463 = 0.008756296709179878 + 0.1 * 6.498073577880859
Epoch 790, val loss: 0.9425792098045349
Epoch 800, training loss: 0.657243549823761 = 0.008482333272695541 + 0.1 * 6.487611770629883
Epoch 800, val loss: 0.9469234347343445
Epoch 810, training loss: 0.6570901870727539 = 0.008221510797739029 + 0.1 * 6.488686561584473
Epoch 810, val loss: 0.9511885046958923
Epoch 820, training loss: 0.6557360887527466 = 0.007974990643560886 + 0.1 * 6.477611064910889
Epoch 820, val loss: 0.9553675055503845
Epoch 830, training loss: 0.6555877327919006 = 0.007740468718111515 + 0.1 * 6.4784722328186035
Epoch 830, val loss: 0.9594690203666687
Epoch 840, training loss: 0.6556166410446167 = 0.007517488673329353 + 0.1 * 6.480991363525391
Epoch 840, val loss: 0.9635080099105835
Epoch 850, training loss: 0.655937910079956 = 0.007304790895432234 + 0.1 * 6.486330986022949
Epoch 850, val loss: 0.967465877532959
Epoch 860, training loss: 0.6558408141136169 = 0.007101443596184254 + 0.1 * 6.487393856048584
Epoch 860, val loss: 0.971308708190918
Epoch 870, training loss: 0.6542472839355469 = 0.006908703129738569 + 0.1 * 6.473385334014893
Epoch 870, val loss: 0.9751495122909546
Epoch 880, training loss: 0.6542351841926575 = 0.006724050734192133 + 0.1 * 6.47511100769043
Epoch 880, val loss: 0.9788573384284973
Epoch 890, training loss: 0.652606725692749 = 0.006547495257109404 + 0.1 * 6.460591793060303
Epoch 890, val loss: 0.9825719594955444
Epoch 900, training loss: 0.6535724997520447 = 0.006379019469022751 + 0.1 * 6.471934795379639
Epoch 900, val loss: 0.9862447381019592
Epoch 910, training loss: 0.6536113023757935 = 0.00621736841276288 + 0.1 * 6.473939418792725
Epoch 910, val loss: 0.9898077845573425
Epoch 920, training loss: 0.6516026854515076 = 0.0060629635117948055 + 0.1 * 6.45539665222168
Epoch 920, val loss: 0.9933410882949829
Epoch 930, training loss: 0.65205979347229 = 0.0059150331653654575 + 0.1 * 6.461447238922119
Epoch 930, val loss: 0.9967888593673706
Epoch 940, training loss: 0.6512189507484436 = 0.0057725668884813786 + 0.1 * 6.454463481903076
Epoch 940, val loss: 1.000283122062683
Epoch 950, training loss: 0.6499620676040649 = 0.005636537913233042 + 0.1 * 6.443255424499512
Epoch 950, val loss: 1.0036194324493408
Epoch 960, training loss: 0.6516242027282715 = 0.005505361594259739 + 0.1 * 6.461187839508057
Epoch 960, val loss: 1.0069935321807861
Epoch 970, training loss: 0.6509972214698792 = 0.005379303824156523 + 0.1 * 6.456179141998291
Epoch 970, val loss: 1.010223388671875
Epoch 980, training loss: 0.6491170525550842 = 0.005258654244244099 + 0.1 * 6.4385833740234375
Epoch 980, val loss: 1.0134857892990112
Epoch 990, training loss: 0.6505871415138245 = 0.005142123904079199 + 0.1 * 6.454449653625488
Epoch 990, val loss: 1.0166492462158203
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.789625406265259 = 1.92994225025177 + 0.1 * 8.596832275390625
Epoch 0, val loss: 1.9225773811340332
Epoch 10, training loss: 2.7798941135406494 = 1.9202208518981934 + 0.1 * 8.596732139587402
Epoch 10, val loss: 1.913267970085144
Epoch 20, training loss: 2.767890214920044 = 1.90828275680542 + 0.1 * 8.596074104309082
Epoch 20, val loss: 1.9015828371047974
Epoch 30, training loss: 2.7508633136749268 = 1.891817331314087 + 0.1 * 8.590459823608398
Epoch 30, val loss: 1.8851653337478638
Epoch 40, training loss: 2.7230000495910645 = 1.8679858446121216 + 0.1 * 8.550143241882324
Epoch 40, val loss: 1.8616228103637695
Epoch 50, training loss: 2.6682846546173096 = 1.835971713066101 + 0.1 * 8.323128700256348
Epoch 50, val loss: 1.8316749334335327
Epoch 60, training loss: 2.599958896636963 = 1.8003060817718506 + 0.1 * 7.996527194976807
Epoch 60, val loss: 1.800957441329956
Epoch 70, training loss: 2.541072130203247 = 1.7659348249435425 + 0.1 * 7.751373291015625
Epoch 70, val loss: 1.7721810340881348
Epoch 80, training loss: 2.4785404205322266 = 1.7280634641647339 + 0.1 * 7.504769325256348
Epoch 80, val loss: 1.7396516799926758
Epoch 90, training loss: 2.403442859649658 = 1.6809293031692505 + 0.1 * 7.225135803222656
Epoch 90, val loss: 1.6991405487060547
Epoch 100, training loss: 2.3245251178741455 = 1.617986798286438 + 0.1 * 7.065382480621338
Epoch 100, val loss: 1.6446270942687988
Epoch 110, training loss: 2.237656831741333 = 1.5369415283203125 + 0.1 * 7.007153034210205
Epoch 110, val loss: 1.574597954750061
Epoch 120, training loss: 2.143467664718628 = 1.4469189643859863 + 0.1 * 6.965487003326416
Epoch 120, val loss: 1.4995036125183105
Epoch 130, training loss: 2.0487005710601807 = 1.3549253940582275 + 0.1 * 6.9377522468566895
Epoch 130, val loss: 1.4246107339859009
Epoch 140, training loss: 1.9548578262329102 = 1.262223720550537 + 0.1 * 6.926340103149414
Epoch 140, val loss: 1.3499869108200073
Epoch 150, training loss: 1.8612494468688965 = 1.1692187786102295 + 0.1 * 6.92030668258667
Epoch 150, val loss: 1.2765263319015503
Epoch 160, training loss: 1.7680928707122803 = 1.0764633417129517 + 0.1 * 6.916296005249023
Epoch 160, val loss: 1.2038443088531494
Epoch 170, training loss: 1.676153302192688 = 0.9848029017448425 + 0.1 * 6.913504123687744
Epoch 170, val loss: 1.1318676471710205
Epoch 180, training loss: 1.5867773294448853 = 0.8956363201141357 + 0.1 * 6.911409854888916
Epoch 180, val loss: 1.0609744787216187
Epoch 190, training loss: 1.5017870664596558 = 0.810793936252594 + 0.1 * 6.909931182861328
Epoch 190, val loss: 0.9932979941368103
Epoch 200, training loss: 1.422630786895752 = 0.7317352294921875 + 0.1 * 6.908956050872803
Epoch 200, val loss: 0.930901050567627
Epoch 210, training loss: 1.350278615951538 = 0.6597220301628113 + 0.1 * 6.90556526184082
Epoch 210, val loss: 0.8762824535369873
Epoch 220, training loss: 1.2851717472076416 = 0.594809889793396 + 0.1 * 6.903618812561035
Epoch 220, val loss: 0.8303695917129517
Epoch 230, training loss: 1.2263083457946777 = 0.536496102809906 + 0.1 * 6.898122787475586
Epoch 230, val loss: 0.793003261089325
Epoch 240, training loss: 1.1730854511260986 = 0.4837496876716614 + 0.1 * 6.893356800079346
Epoch 240, val loss: 0.763146162033081
Epoch 250, training loss: 1.1245818138122559 = 0.43548905849456787 + 0.1 * 6.890926837921143
Epoch 250, val loss: 0.7394593358039856
Epoch 260, training loss: 1.0794755220413208 = 0.39133021235466003 + 0.1 * 6.881453037261963
Epoch 260, val loss: 0.7207233905792236
Epoch 270, training loss: 1.0380825996398926 = 0.35062938928604126 + 0.1 * 6.874532699584961
Epoch 270, val loss: 0.706111490726471
Epoch 280, training loss: 0.9993849992752075 = 0.31294649839401245 + 0.1 * 6.86438512802124
Epoch 280, val loss: 0.694838285446167
Epoch 290, training loss: 0.9643740057945251 = 0.27826356887817383 + 0.1 * 6.861104488372803
Epoch 290, val loss: 0.6866959929466248
Epoch 300, training loss: 0.9309468269348145 = 0.24656465649604797 + 0.1 * 6.843822002410889
Epoch 300, val loss: 0.6812437176704407
Epoch 310, training loss: 0.900973916053772 = 0.21760578453540802 + 0.1 * 6.833681106567383
Epoch 310, val loss: 0.6783572435379028
Epoch 320, training loss: 0.8734232783317566 = 0.19140367209911346 + 0.1 * 6.82019567489624
Epoch 320, val loss: 0.6779934763908386
Epoch 330, training loss: 0.850883960723877 = 0.16813752055168152 + 0.1 * 6.8274641036987305
Epoch 330, val loss: 0.6799631118774414
Epoch 340, training loss: 0.8287146687507629 = 0.14785398542881012 + 0.1 * 6.80860710144043
Epoch 340, val loss: 0.6837073564529419
Epoch 350, training loss: 0.8096954822540283 = 0.13014894723892212 + 0.1 * 6.795465469360352
Epoch 350, val loss: 0.6891738176345825
Epoch 360, training loss: 0.795772910118103 = 0.11477888375520706 + 0.1 * 6.809940338134766
Epoch 360, val loss: 0.6963391900062561
Epoch 370, training loss: 0.7798165678977966 = 0.10168002545833588 + 0.1 * 6.781365394592285
Epoch 370, val loss: 0.7043709754943848
Epoch 380, training loss: 0.7673819065093994 = 0.09040167927742004 + 0.1 * 6.769802093505859
Epoch 380, val loss: 0.7133070230484009
Epoch 390, training loss: 0.7567757368087769 = 0.08064868301153183 + 0.1 * 6.761270523071289
Epoch 390, val loss: 0.7228187322616577
Epoch 400, training loss: 0.7475044131278992 = 0.0721907690167427 + 0.1 * 6.75313663482666
Epoch 400, val loss: 0.7327641844749451
Epoch 410, training loss: 0.7407836318016052 = 0.06488769501447678 + 0.1 * 6.7589592933654785
Epoch 410, val loss: 0.7428701519966125
Epoch 420, training loss: 0.7331104278564453 = 0.058592814952135086 + 0.1 * 6.745176315307617
Epoch 420, val loss: 0.7528382539749146
Epoch 430, training loss: 0.7265963554382324 = 0.05310722440481186 + 0.1 * 6.734891414642334
Epoch 430, val loss: 0.7628278732299805
Epoch 440, training loss: 0.72118079662323 = 0.04830314591526985 + 0.1 * 6.728776454925537
Epoch 440, val loss: 0.7728757262229919
Epoch 450, training loss: 0.7162497043609619 = 0.044083863496780396 + 0.1 * 6.721657752990723
Epoch 450, val loss: 0.7828644514083862
Epoch 460, training loss: 0.7134141325950623 = 0.040372107177972794 + 0.1 * 6.730419635772705
Epoch 460, val loss: 0.7928391098976135
Epoch 470, training loss: 0.7089482545852661 = 0.03712933138012886 + 0.1 * 6.718188762664795
Epoch 470, val loss: 0.8021742701530457
Epoch 480, training loss: 0.7049698233604431 = 0.03425629809498787 + 0.1 * 6.707135200500488
Epoch 480, val loss: 0.8114984035491943
Epoch 490, training loss: 0.7025954127311707 = 0.03169598802924156 + 0.1 * 6.708994388580322
Epoch 490, val loss: 0.8205471038818359
Epoch 500, training loss: 0.6998320817947388 = 0.02941761538386345 + 0.1 * 6.704144477844238
Epoch 500, val loss: 0.8294163942337036
Epoch 510, training loss: 0.6965212225914001 = 0.027379991486668587 + 0.1 * 6.691411972045898
Epoch 510, val loss: 0.8380808234214783
Epoch 520, training loss: 0.6946449875831604 = 0.02554548718035221 + 0.1 * 6.690995216369629
Epoch 520, val loss: 0.846499502658844
Epoch 530, training loss: 0.6922397613525391 = 0.02389397844672203 + 0.1 * 6.683457851409912
Epoch 530, val loss: 0.8548206686973572
Epoch 540, training loss: 0.6903567314147949 = 0.022403409704566002 + 0.1 * 6.679533004760742
Epoch 540, val loss: 0.8627580404281616
Epoch 550, training loss: 0.6889038681983948 = 0.021049467846751213 + 0.1 * 6.678544044494629
Epoch 550, val loss: 0.8706102967262268
Epoch 560, training loss: 0.6870934963226318 = 0.01981770247220993 + 0.1 * 6.672757625579834
Epoch 560, val loss: 0.878108561038971
Epoch 570, training loss: 0.6865928769111633 = 0.018697403371334076 + 0.1 * 6.678954601287842
Epoch 570, val loss: 0.8856695890426636
Epoch 580, training loss: 0.683927595615387 = 0.017674993723630905 + 0.1 * 6.6625261306762695
Epoch 580, val loss: 0.8926841020584106
Epoch 590, training loss: 0.6827242970466614 = 0.01673898659646511 + 0.1 * 6.659852504730225
Epoch 590, val loss: 0.8995822668075562
Epoch 600, training loss: 0.6825094223022461 = 0.01588256098330021 + 0.1 * 6.666268825531006
Epoch 600, val loss: 0.9065006971359253
Epoch 610, training loss: 0.680114209651947 = 0.015097619965672493 + 0.1 * 6.650166034698486
Epoch 610, val loss: 0.9127062559127808
Epoch 620, training loss: 0.6789416074752808 = 0.014371688477694988 + 0.1 * 6.6456990242004395
Epoch 620, val loss: 0.9191300868988037
Epoch 630, training loss: 0.6770948171615601 = 0.013702956959605217 + 0.1 * 6.633918762207031
Epoch 630, val loss: 0.9252473711967468
Epoch 640, training loss: 0.6759443879127502 = 0.013083375990390778 + 0.1 * 6.628609657287598
Epoch 640, val loss: 0.9311690926551819
Epoch 650, training loss: 0.6766690015792847 = 0.012506872415542603 + 0.1 * 6.6416215896606445
Epoch 650, val loss: 0.9370497465133667
Epoch 660, training loss: 0.6742830276489258 = 0.01197241060435772 + 0.1 * 6.623106002807617
Epoch 660, val loss: 0.9428143501281738
Epoch 670, training loss: 0.6763098835945129 = 0.011476953513920307 + 0.1 * 6.648329257965088
Epoch 670, val loss: 0.9482672810554504
Epoch 680, training loss: 0.6729426383972168 = 0.011015457101166248 + 0.1 * 6.619271755218506
Epoch 680, val loss: 0.9535335898399353
Epoch 690, training loss: 0.6715959906578064 = 0.01058318093419075 + 0.1 * 6.610127925872803
Epoch 690, val loss: 0.9586070775985718
Epoch 700, training loss: 0.6710720658302307 = 0.010177718475461006 + 0.1 * 6.608942985534668
Epoch 700, val loss: 0.963803768157959
Epoch 710, training loss: 0.6700781583786011 = 0.009798895567655563 + 0.1 * 6.602792739868164
Epoch 710, val loss: 0.9686596989631653
Epoch 720, training loss: 0.6698412299156189 = 0.009441972710192204 + 0.1 * 6.603992462158203
Epoch 720, val loss: 0.9737123847007751
Epoch 730, training loss: 0.6695457100868225 = 0.009108873084187508 + 0.1 * 6.604368209838867
Epoch 730, val loss: 0.9782070517539978
Epoch 740, training loss: 0.6671044826507568 = 0.00879437755793333 + 0.1 * 6.583101272583008
Epoch 740, val loss: 0.9828402996063232
Epoch 750, training loss: 0.6696074604988098 = 0.008497733622789383 + 0.1 * 6.61109733581543
Epoch 750, val loss: 0.9874261021614075
Epoch 760, training loss: 0.6669207811355591 = 0.008218237198889256 + 0.1 * 6.587025165557861
Epoch 760, val loss: 0.9919147491455078
Epoch 770, training loss: 0.6656153798103333 = 0.007955092005431652 + 0.1 * 6.576602935791016
Epoch 770, val loss: 0.9960493445396423
Epoch 780, training loss: 0.665277898311615 = 0.007705586031079292 + 0.1 * 6.575723171234131
Epoch 780, val loss: 1.0001097917556763
Epoch 790, training loss: 0.6662519574165344 = 0.0074685849249362946 + 0.1 * 6.587833404541016
Epoch 790, val loss: 1.0044307708740234
Epoch 800, training loss: 0.6648104786872864 = 0.007244969252496958 + 0.1 * 6.575655460357666
Epoch 800, val loss: 1.008258581161499
Epoch 810, training loss: 0.6631088852882385 = 0.0070331706665456295 + 0.1 * 6.560757160186768
Epoch 810, val loss: 1.0122102499008179
Epoch 820, training loss: 0.6652270555496216 = 0.0068320478312671185 + 0.1 * 6.583949565887451
Epoch 820, val loss: 1.0160179138183594
Epoch 830, training loss: 0.6625203490257263 = 0.006640655919909477 + 0.1 * 6.558796405792236
Epoch 830, val loss: 1.0198827981948853
Epoch 840, training loss: 0.6631086468696594 = 0.006459853611886501 + 0.1 * 6.566488265991211
Epoch 840, val loss: 1.02329421043396
Epoch 850, training loss: 0.6604718565940857 = 0.0062857880257070065 + 0.1 * 6.541860580444336
Epoch 850, val loss: 1.0268131494522095
Epoch 860, training loss: 0.6602367162704468 = 0.006120732519775629 + 0.1 * 6.541159629821777
Epoch 860, val loss: 1.0303750038146973
Epoch 870, training loss: 0.660768985748291 = 0.005962902680039406 + 0.1 * 6.548060417175293
Epoch 870, val loss: 1.0337989330291748
Epoch 880, training loss: 0.6592713594436646 = 0.005812311545014381 + 0.1 * 6.534590721130371
Epoch 880, val loss: 1.0371261835098267
Epoch 890, training loss: 0.6579632759094238 = 0.005668490193784237 + 0.1 * 6.522948265075684
Epoch 890, val loss: 1.0403051376342773
Epoch 900, training loss: 0.6585645079612732 = 0.005530164577066898 + 0.1 * 6.530343055725098
Epoch 900, val loss: 1.043729543685913
Epoch 910, training loss: 0.6571166515350342 = 0.005399038549512625 + 0.1 * 6.517175674438477
Epoch 910, val loss: 1.046911358833313
Epoch 920, training loss: 0.6567162275314331 = 0.005273293703794479 + 0.1 * 6.514429569244385
Epoch 920, val loss: 1.0499372482299805
Epoch 930, training loss: 0.6557468175888062 = 0.005152642261236906 + 0.1 * 6.505941390991211
Epoch 930, val loss: 1.0529193878173828
Epoch 940, training loss: 0.6604316234588623 = 0.005036535672843456 + 0.1 * 6.553950786590576
Epoch 940, val loss: 1.0560444593429565
Epoch 950, training loss: 0.6565086245536804 = 0.004925054032355547 + 0.1 * 6.515835285186768
Epoch 950, val loss: 1.0591719150543213
Epoch 960, training loss: 0.6552234292030334 = 0.004818660207092762 + 0.1 * 6.504047393798828
Epoch 960, val loss: 1.06187105178833
Epoch 970, training loss: 0.6543490886688232 = 0.004715536255389452 + 0.1 * 6.496335029602051
Epoch 970, val loss: 1.0645976066589355
Epoch 980, training loss: 0.6539323925971985 = 0.004615661222487688 + 0.1 * 6.493166923522949
Epoch 980, val loss: 1.0677649974822998
Epoch 990, training loss: 0.6538946032524109 = 0.00451995525509119 + 0.1 * 6.493746757507324
Epoch 990, val loss: 1.070496678352356
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8386926726410122
The final CL Acc:0.82222, 0.00800, The final GNN Acc:0.83799, 0.00066
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10588])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.809448480606079 = 1.949760913848877 + 0.1 * 8.59687614440918
Epoch 0, val loss: 1.9486182928085327
Epoch 10, training loss: 2.7993738651275635 = 1.939692497253418 + 0.1 * 8.596813201904297
Epoch 10, val loss: 1.9390132427215576
Epoch 20, training loss: 2.787229537963867 = 1.9275859594345093 + 0.1 * 8.596434593200684
Epoch 20, val loss: 1.9270281791687012
Epoch 30, training loss: 2.770235061645508 = 1.9109238386154175 + 0.1 * 8.593111991882324
Epoch 30, val loss: 1.9101133346557617
Epoch 40, training loss: 2.7434353828430176 = 1.886668086051941 + 0.1 * 8.567673683166504
Epoch 40, val loss: 1.885420322418213
Epoch 50, training loss: 2.69767689704895 = 1.8530125617980957 + 0.1 * 8.446642875671387
Epoch 50, val loss: 1.8526334762573242
Epoch 60, training loss: 2.6339547634124756 = 1.8150813579559326 + 0.1 * 8.18873405456543
Epoch 60, val loss: 1.8188992738723755
Epoch 70, training loss: 2.5880439281463623 = 1.7818313837051392 + 0.1 * 8.062125205993652
Epoch 70, val loss: 1.7908822298049927
Epoch 80, training loss: 2.5296285152435303 = 1.7478340864181519 + 0.1 * 7.817943572998047
Epoch 80, val loss: 1.7580416202545166
Epoch 90, training loss: 2.462345600128174 = 1.703736662864685 + 0.1 * 7.586089611053467
Epoch 90, val loss: 1.716402530670166
Epoch 100, training loss: 2.3829357624053955 = 1.6459981203079224 + 0.1 * 7.369375705718994
Epoch 100, val loss: 1.6665593385696411
Epoch 110, training loss: 2.2908949851989746 = 1.5739758014678955 + 0.1 * 7.169191837310791
Epoch 110, val loss: 1.6041860580444336
Epoch 120, training loss: 2.2020137310028076 = 1.4946004152297974 + 0.1 * 7.074133396148682
Epoch 120, val loss: 1.534550428390503
Epoch 130, training loss: 2.117079019546509 = 1.4155205488204956 + 0.1 * 7.015584468841553
Epoch 130, val loss: 1.4638290405273438
Epoch 140, training loss: 2.0383591651916504 = 1.3407186269760132 + 0.1 * 6.976405620574951
Epoch 140, val loss: 1.399156093597412
Epoch 150, training loss: 1.9656498432159424 = 1.270235300064087 + 0.1 * 6.954145908355713
Epoch 150, val loss: 1.340190052986145
Epoch 160, training loss: 1.8949861526489258 = 1.200727939605713 + 0.1 * 6.942581653594971
Epoch 160, val loss: 1.2832015752792358
Epoch 170, training loss: 1.8230457305908203 = 1.1295831203460693 + 0.1 * 6.934625148773193
Epoch 170, val loss: 1.226008415222168
Epoch 180, training loss: 1.7472293376922607 = 1.0543668270111084 + 0.1 * 6.928625583648682
Epoch 180, val loss: 1.166841983795166
Epoch 190, training loss: 1.667141079902649 = 0.9748267531394958 + 0.1 * 6.92314338684082
Epoch 190, val loss: 1.10516357421875
Epoch 200, training loss: 1.5847969055175781 = 0.8932494521141052 + 0.1 * 6.9154744148254395
Epoch 200, val loss: 1.0430858135223389
Epoch 210, training loss: 1.5031200647354126 = 0.8126166462898254 + 0.1 * 6.905034065246582
Epoch 210, val loss: 0.9830694198608398
Epoch 220, training loss: 1.4262534379959106 = 0.7368453145027161 + 0.1 * 6.894081115722656
Epoch 220, val loss: 0.9292891621589661
Epoch 230, training loss: 1.357149362564087 = 0.6689777970314026 + 0.1 * 6.881714820861816
Epoch 230, val loss: 0.8840498328208923
Epoch 240, training loss: 1.2947142124176025 = 0.6080646514892578 + 0.1 * 6.866495132446289
Epoch 240, val loss: 0.847068727016449
Epoch 250, training loss: 1.2387970685958862 = 0.5532591342926025 + 0.1 * 6.855379104614258
Epoch 250, val loss: 0.817263662815094
Epoch 260, training loss: 1.1881269216537476 = 0.5040162801742554 + 0.1 * 6.841106414794922
Epoch 260, val loss: 0.7936288714408875
Epoch 270, training loss: 1.140810489654541 = 0.45863425731658936 + 0.1 * 6.821762561798096
Epoch 270, val loss: 0.7745963931083679
Epoch 280, training loss: 1.098142385482788 = 0.4163200259208679 + 0.1 * 6.818223476409912
Epoch 280, val loss: 0.7590659260749817
Epoch 290, training loss: 1.056823492050171 = 0.376620352268219 + 0.1 * 6.80203104019165
Epoch 290, val loss: 0.7464888691902161
Epoch 300, training loss: 1.0181570053100586 = 0.3390117883682251 + 0.1 * 6.791451930999756
Epoch 300, val loss: 0.736180305480957
Epoch 310, training loss: 0.9822928309440613 = 0.30352795124053955 + 0.1 * 6.787648677825928
Epoch 310, val loss: 0.7282171845436096
Epoch 320, training loss: 0.9475958347320557 = 0.27065399289131165 + 0.1 * 6.769418716430664
Epoch 320, val loss: 0.7222933173179626
Epoch 330, training loss: 0.9168513417243958 = 0.24065959453582764 + 0.1 * 6.7619171142578125
Epoch 330, val loss: 0.7186024785041809
Epoch 340, training loss: 0.8894986510276794 = 0.21387147903442383 + 0.1 * 6.756271839141846
Epoch 340, val loss: 0.7171211242675781
Epoch 350, training loss: 0.865311861038208 = 0.1903291493654251 + 0.1 * 6.749826908111572
Epoch 350, val loss: 0.7177312970161438
Epoch 360, training loss: 0.8438272476196289 = 0.1697586327791214 + 0.1 * 6.740685939788818
Epoch 360, val loss: 0.7200125455856323
Epoch 370, training loss: 0.8257029056549072 = 0.1517832726240158 + 0.1 * 6.739196300506592
Epoch 370, val loss: 0.7238442897796631
Epoch 380, training loss: 0.8088825345039368 = 0.13608847558498383 + 0.1 * 6.727940082550049
Epoch 380, val loss: 0.728757917881012
Epoch 390, training loss: 0.7941136956214905 = 0.12226410955190659 + 0.1 * 6.718495845794678
Epoch 390, val loss: 0.7347074747085571
Epoch 400, training loss: 0.7856865525245667 = 0.11005198955535889 + 0.1 * 6.756345272064209
Epoch 400, val loss: 0.7413488626480103
Epoch 410, training loss: 0.7711329460144043 = 0.09937205910682678 + 0.1 * 6.71760892868042
Epoch 410, val loss: 0.7485260963439941
Epoch 420, training loss: 0.7599820494651794 = 0.08997048437595367 + 0.1 * 6.70011568069458
Epoch 420, val loss: 0.7562933564186096
Epoch 430, training loss: 0.7522090077400208 = 0.081661157310009 + 0.1 * 6.705478191375732
Epoch 430, val loss: 0.764365553855896
Epoch 440, training loss: 0.7429412603378296 = 0.07433553040027618 + 0.1 * 6.6860575675964355
Epoch 440, val loss: 0.7727081775665283
Epoch 450, training loss: 0.7357429265975952 = 0.06783508509397507 + 0.1 * 6.679078102111816
Epoch 450, val loss: 0.7813327312469482
Epoch 460, training loss: 0.7337013483047485 = 0.06204807385802269 + 0.1 * 6.7165327072143555
Epoch 460, val loss: 0.7901449203491211
Epoch 470, training loss: 0.7246028780937195 = 0.05692492425441742 + 0.1 * 6.676779270172119
Epoch 470, val loss: 0.7990104556083679
Epoch 480, training loss: 0.7183955311775208 = 0.05235300958156586 + 0.1 * 6.660425186157227
Epoch 480, val loss: 0.808006763458252
Epoch 490, training loss: 0.7135517597198486 = 0.04825780168175697 + 0.1 * 6.652939319610596
Epoch 490, val loss: 0.8170101642608643
Epoch 500, training loss: 0.7105826735496521 = 0.04458796605467796 + 0.1 * 6.659946918487549
Epoch 500, val loss: 0.8259929418563843
Epoch 510, training loss: 0.7053833603858948 = 0.04129991680383682 + 0.1 * 6.640833854675293
Epoch 510, val loss: 0.8349183797836304
Epoch 520, training loss: 0.7021237015724182 = 0.03834424912929535 + 0.1 * 6.637794017791748
Epoch 520, val loss: 0.8438165783882141
Epoch 530, training loss: 0.6996384263038635 = 0.03568069636821747 + 0.1 * 6.6395769119262695
Epoch 530, val loss: 0.8525692820549011
Epoch 540, training loss: 0.6961479783058167 = 0.03328143060207367 + 0.1 * 6.628665447235107
Epoch 540, val loss: 0.861244261264801
Epoch 550, training loss: 0.6935494542121887 = 0.031110337004065514 + 0.1 * 6.624391078948975
Epoch 550, val loss: 0.8698222637176514
Epoch 560, training loss: 0.6915788650512695 = 0.029143817722797394 + 0.1 * 6.624350547790527
Epoch 560, val loss: 0.878197968006134
Epoch 570, training loss: 0.6883612275123596 = 0.02736244536936283 + 0.1 * 6.609987258911133
Epoch 570, val loss: 0.8864505887031555
Epoch 580, training loss: 0.6891077756881714 = 0.02573976293206215 + 0.1 * 6.6336798667907715
Epoch 580, val loss: 0.894618034362793
Epoch 590, training loss: 0.6848315000534058 = 0.024260807782411575 + 0.1 * 6.605706691741943
Epoch 590, val loss: 0.9025132060050964
Epoch 600, training loss: 0.6832932829856873 = 0.02291010320186615 + 0.1 * 6.6038312911987305
Epoch 600, val loss: 0.9102885723114014
Epoch 610, training loss: 0.6818406581878662 = 0.021670453250408173 + 0.1 * 6.601701736450195
Epoch 610, val loss: 0.9178432822227478
Epoch 620, training loss: 0.680065929889679 = 0.020532583817839622 + 0.1 * 6.595333099365234
Epoch 620, val loss: 0.92535799741745
Epoch 630, training loss: 0.6796279549598694 = 0.019488485530018806 + 0.1 * 6.601394176483154
Epoch 630, val loss: 0.9325690865516663
Epoch 640, training loss: 0.6769880056381226 = 0.018526624888181686 + 0.1 * 6.584613800048828
Epoch 640, val loss: 0.9397180676460266
Epoch 650, training loss: 0.67610102891922 = 0.017637260258197784 + 0.1 * 6.584637641906738
Epoch 650, val loss: 0.9466568231582642
Epoch 660, training loss: 0.674286961555481 = 0.016814187169075012 + 0.1 * 6.574728012084961
Epoch 660, val loss: 0.9534032940864563
Epoch 670, training loss: 0.6747419834136963 = 0.016051091253757477 + 0.1 * 6.58690881729126
Epoch 670, val loss: 0.9600567817687988
Epoch 680, training loss: 0.6719291806221008 = 0.015341553837060928 + 0.1 * 6.565876007080078
Epoch 680, val loss: 0.9664897918701172
Epoch 690, training loss: 0.6709307432174683 = 0.014683663845062256 + 0.1 * 6.562470436096191
Epoch 690, val loss: 0.9728378653526306
Epoch 700, training loss: 0.6691842079162598 = 0.014069336466491222 + 0.1 * 6.551148891448975
Epoch 700, val loss: 0.9790353178977966
Epoch 710, training loss: 0.6717734932899475 = 0.013493554666638374 + 0.1 * 6.582799434661865
Epoch 710, val loss: 0.985087513923645
Epoch 720, training loss: 0.6675184369087219 = 0.012955749407410622 + 0.1 * 6.545627117156982
Epoch 720, val loss: 0.9909893870353699
Epoch 730, training loss: 0.6681570410728455 = 0.012452845461666584 + 0.1 * 6.557041645050049
Epoch 730, val loss: 0.996830940246582
Epoch 740, training loss: 0.6653715372085571 = 0.011979726143181324 + 0.1 * 6.533918380737305
Epoch 740, val loss: 1.0024369955062866
Epoch 750, training loss: 0.6656033396720886 = 0.011536678299307823 + 0.1 * 6.540666580200195
Epoch 750, val loss: 1.0079628229141235
Epoch 760, training loss: 0.6632694602012634 = 0.011119423434138298 + 0.1 * 6.521500110626221
Epoch 760, val loss: 1.013423204421997
Epoch 770, training loss: 0.666777491569519 = 0.010724740102887154 + 0.1 * 6.560527324676514
Epoch 770, val loss: 1.0187445878982544
Epoch 780, training loss: 0.6627433896064758 = 0.010353260673582554 + 0.1 * 6.523900985717773
Epoch 780, val loss: 1.0238991975784302
Epoch 790, training loss: 0.6618583798408508 = 0.010003633797168732 + 0.1 * 6.518547534942627
Epoch 790, val loss: 1.0289970636367798
Epoch 800, training loss: 0.6645838022232056 = 0.009672081097960472 + 0.1 * 6.549116611480713
Epoch 800, val loss: 1.033998727798462
Epoch 810, training loss: 0.6609453558921814 = 0.00935819000005722 + 0.1 * 6.515871524810791
Epoch 810, val loss: 1.0388402938842773
Epoch 820, training loss: 0.6601960062980652 = 0.009061642922461033 + 0.1 * 6.511343479156494
Epoch 820, val loss: 1.0437620878219604
Epoch 830, training loss: 0.6594765186309814 = 0.008779661729931831 + 0.1 * 6.5069684982299805
Epoch 830, val loss: 1.0485135316848755
Epoch 840, training loss: 0.6586592197418213 = 0.00851086899638176 + 0.1 * 6.501482963562012
Epoch 840, val loss: 1.0530494451522827
Epoch 850, training loss: 0.6602483987808228 = 0.008256029337644577 + 0.1 * 6.519923686981201
Epoch 850, val loss: 1.0576112270355225
Epoch 860, training loss: 0.6583797931671143 = 0.008013701066374779 + 0.1 * 6.503660678863525
Epoch 860, val loss: 1.0620173215866089
Epoch 870, training loss: 0.6577659249305725 = 0.00778355123475194 + 0.1 * 6.499824047088623
Epoch 870, val loss: 1.0664703845977783
Epoch 880, training loss: 0.6570397019386292 = 0.0075638387352228165 + 0.1 * 6.494758605957031
Epoch 880, val loss: 1.070717215538025
Epoch 890, training loss: 0.6572126150131226 = 0.007353859022259712 + 0.1 * 6.498587608337402
Epoch 890, val loss: 1.074900507926941
Epoch 900, training loss: 0.6566565632820129 = 0.007153710350394249 + 0.1 * 6.495028495788574
Epoch 900, val loss: 1.079027533531189
Epoch 910, training loss: 0.6579744815826416 = 0.006962557323276997 + 0.1 * 6.510119438171387
Epoch 910, val loss: 1.0831657648086548
Epoch 920, training loss: 0.6557455062866211 = 0.006779630668461323 + 0.1 * 6.489658832550049
Epoch 920, val loss: 1.0870535373687744
Epoch 930, training loss: 0.6545530557632446 = 0.006604868918657303 + 0.1 * 6.479482173919678
Epoch 930, val loss: 1.0910502672195435
Epoch 940, training loss: 0.6575685739517212 = 0.006437075324356556 + 0.1 * 6.511314868927002
Epoch 940, val loss: 1.0949058532714844
Epoch 950, training loss: 0.654705286026001 = 0.0062765819020569324 + 0.1 * 6.484286785125732
Epoch 950, val loss: 1.0986251831054688
Epoch 960, training loss: 0.655279815196991 = 0.006122814957052469 + 0.1 * 6.491569995880127
Epoch 960, val loss: 1.1024202108383179
Epoch 970, training loss: 0.6539309024810791 = 0.005975157022476196 + 0.1 * 6.479557037353516
Epoch 970, val loss: 1.1060923337936401
Epoch 980, training loss: 0.6527281999588013 = 0.005833327304571867 + 0.1 * 6.468948841094971
Epoch 980, val loss: 1.1097002029418945
Epoch 990, training loss: 0.6528950929641724 = 0.005697683896869421 + 0.1 * 6.471973896026611
Epoch 990, val loss: 1.1132748126983643
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 2.8061037063598633 = 1.9464174509048462 + 0.1 * 8.59686279296875
Epoch 0, val loss: 1.9434211254119873
Epoch 10, training loss: 2.796294927597046 = 1.9366146326065063 + 0.1 * 8.596803665161133
Epoch 10, val loss: 1.933049201965332
Epoch 20, training loss: 2.7839224338531494 = 1.9242780208587646 + 0.1 * 8.596444129943848
Epoch 20, val loss: 1.9198601245880127
Epoch 30, training loss: 2.766263484954834 = 1.9069197177886963 + 0.1 * 8.593436241149902
Epoch 30, val loss: 1.9012919664382935
Epoch 40, training loss: 2.73879337310791 = 1.8815174102783203 + 0.1 * 8.572760581970215
Epoch 40, val loss: 1.8746111392974854
Epoch 50, training loss: 2.695035696029663 = 1.8470567464828491 + 0.1 * 8.479788780212402
Epoch 50, val loss: 1.8404858112335205
Epoch 60, training loss: 2.631157636642456 = 1.8100950717926025 + 0.1 * 8.210625648498535
Epoch 60, val loss: 1.8080053329467773
Epoch 70, training loss: 2.586580991744995 = 1.7774181365966797 + 0.1 * 8.091629028320312
Epoch 70, val loss: 1.7824594974517822
Epoch 80, training loss: 2.525238275527954 = 1.740047574043274 + 0.1 * 7.851907730102539
Epoch 80, val loss: 1.7506378889083862
Epoch 90, training loss: 2.4530065059661865 = 1.691774606704712 + 0.1 * 7.612318992614746
Epoch 90, val loss: 1.7085686922073364
Epoch 100, training loss: 2.3681886196136475 = 1.6326494216918945 + 0.1 * 7.355391979217529
Epoch 100, val loss: 1.6587307453155518
Epoch 110, training loss: 2.2775473594665527 = 1.5614409446716309 + 0.1 * 7.161063194274902
Epoch 110, val loss: 1.5982269048690796
Epoch 120, training loss: 2.1896023750305176 = 1.4816378355026245 + 0.1 * 7.079645156860352
Epoch 120, val loss: 1.533168077468872
Epoch 130, training loss: 2.10353684425354 = 1.4012722969055176 + 0.1 * 7.022645473480225
Epoch 130, val loss: 1.4681105613708496
Epoch 140, training loss: 2.020521640777588 = 1.321847915649414 + 0.1 * 6.986738204956055
Epoch 140, val loss: 1.4053833484649658
Epoch 150, training loss: 1.9392673969268799 = 1.242165207862854 + 0.1 * 6.9710211753845215
Epoch 150, val loss: 1.3446992635726929
Epoch 160, training loss: 1.8595430850982666 = 1.1637598276138306 + 0.1 * 6.957832336425781
Epoch 160, val loss: 1.2854564189910889
Epoch 170, training loss: 1.7828912734985352 = 1.0879547595977783 + 0.1 * 6.949364185333252
Epoch 170, val loss: 1.2295408248901367
Epoch 180, training loss: 1.7109159231185913 = 1.0167499780654907 + 0.1 * 6.941659450531006
Epoch 180, val loss: 1.1778067350387573
Epoch 190, training loss: 1.6442575454711914 = 0.9506735801696777 + 0.1 * 6.935839653015137
Epoch 190, val loss: 1.1306599378585815
Epoch 200, training loss: 1.5809205770492554 = 0.88787841796875 + 0.1 * 6.930421352386475
Epoch 200, val loss: 1.0857393741607666
Epoch 210, training loss: 1.518203616142273 = 0.8257761597633362 + 0.1 * 6.924274444580078
Epoch 210, val loss: 1.0409661531448364
Epoch 220, training loss: 1.4541020393371582 = 0.7622919678688049 + 0.1 * 6.9181013107299805
Epoch 220, val loss: 0.9946344494819641
Epoch 230, training loss: 1.3882839679718018 = 0.6972992420196533 + 0.1 * 6.909847259521484
Epoch 230, val loss: 0.9471372961997986
Epoch 240, training loss: 1.322984218597412 = 0.6328039169311523 + 0.1 * 6.9018025398254395
Epoch 240, val loss: 0.900053083896637
Epoch 250, training loss: 1.2610890865325928 = 0.571734607219696 + 0.1 * 6.893544673919678
Epoch 250, val loss: 0.8560965657234192
Epoch 260, training loss: 1.2036572694778442 = 0.5155402421951294 + 0.1 * 6.881170272827148
Epoch 260, val loss: 0.8169811964035034
Epoch 270, training loss: 1.1509054899215698 = 0.464145690202713 + 0.1 * 6.867597579956055
Epoch 270, val loss: 0.7833675146102905
Epoch 280, training loss: 1.103712797164917 = 0.4170348644256592 + 0.1 * 6.866779327392578
Epoch 280, val loss: 0.7553154826164246
Epoch 290, training loss: 1.0588656663894653 = 0.37321552634239197 + 0.1 * 6.856501579284668
Epoch 290, val loss: 0.7318920493125916
Epoch 300, training loss: 1.016952395439148 = 0.3316812217235565 + 0.1 * 6.852712154388428
Epoch 300, val loss: 0.7123682498931885
Epoch 310, training loss: 0.9761136174201965 = 0.29244905710220337 + 0.1 * 6.836645603179932
Epoch 310, val loss: 0.6965712904930115
Epoch 320, training loss: 0.9386591911315918 = 0.25570449233055115 + 0.1 * 6.829546928405762
Epoch 320, val loss: 0.6843869090080261
Epoch 330, training loss: 0.9066439867019653 = 0.22196120023727417 + 0.1 * 6.846827983856201
Epoch 330, val loss: 0.6758156418800354
Epoch 340, training loss: 0.8748112916946411 = 0.1920061707496643 + 0.1 * 6.8280510902404785
Epoch 340, val loss: 0.6710566282272339
Epoch 350, training loss: 0.8466302752494812 = 0.16587994992733002 + 0.1 * 6.8075032234191895
Epoch 350, val loss: 0.6697477698326111
Epoch 360, training loss: 0.826991856098175 = 0.14343605935573578 + 0.1 * 6.835557460784912
Epoch 360, val loss: 0.6715191602706909
Epoch 370, training loss: 0.8046646118164062 = 0.12464004755020142 + 0.1 * 6.800245761871338
Epoch 370, val loss: 0.6757181286811829
Epoch 380, training loss: 0.7880237698554993 = 0.10884071886539459 + 0.1 * 6.791830539703369
Epoch 380, val loss: 0.6819681525230408
Epoch 390, training loss: 0.774795651435852 = 0.09551365673542023 + 0.1 * 6.792819976806641
Epoch 390, val loss: 0.6897505521774292
Epoch 400, training loss: 0.762067437171936 = 0.08429130911827087 + 0.1 * 6.777760982513428
Epoch 400, val loss: 0.6984936594963074
Epoch 410, training loss: 0.7516533732414246 = 0.07480404525995255 + 0.1 * 6.768493175506592
Epoch 410, val loss: 0.7081373929977417
Epoch 420, training loss: 0.7420387268066406 = 0.06675286591053009 + 0.1 * 6.752858638763428
Epoch 420, val loss: 0.718195378780365
Epoch 430, training loss: 0.7352938652038574 = 0.059903763234615326 + 0.1 * 6.75390100479126
Epoch 430, val loss: 0.7285392880439758
Epoch 440, training loss: 0.7277326583862305 = 0.05406121537089348 + 0.1 * 6.7367143630981445
Epoch 440, val loss: 0.7388179898262024
Epoch 450, training loss: 0.7247307300567627 = 0.049016520380973816 + 0.1 * 6.757141590118408
Epoch 450, val loss: 0.7492119073867798
Epoch 460, training loss: 0.7161353230476379 = 0.04465758427977562 + 0.1 * 6.71477746963501
Epoch 460, val loss: 0.7593553066253662
Epoch 470, training loss: 0.7125807404518127 = 0.040864188224077225 + 0.1 * 6.717165470123291
Epoch 470, val loss: 0.7694840431213379
Epoch 480, training loss: 0.7091604471206665 = 0.03754712641239166 + 0.1 * 6.716133117675781
Epoch 480, val loss: 0.7792608737945557
Epoch 490, training loss: 0.7038153409957886 = 0.034630924463272095 + 0.1 * 6.691844463348389
Epoch 490, val loss: 0.7890047430992126
Epoch 500, training loss: 0.701003909111023 = 0.03204037621617317 + 0.1 * 6.689635276794434
Epoch 500, val loss: 0.7984023094177246
Epoch 510, training loss: 0.6994870901107788 = 0.029738008975982666 + 0.1 * 6.697490692138672
Epoch 510, val loss: 0.8077927231788635
Epoch 520, training loss: 0.695640504360199 = 0.027693629264831543 + 0.1 * 6.679468631744385
Epoch 520, val loss: 0.8166463375091553
Epoch 530, training loss: 0.6925468444824219 = 0.025860631838440895 + 0.1 * 6.6668620109558105
Epoch 530, val loss: 0.8255089521408081
Epoch 540, training loss: 0.6900381445884705 = 0.024207312613725662 + 0.1 * 6.658308506011963
Epoch 540, val loss: 0.8339754939079285
Epoch 550, training loss: 0.6884273290634155 = 0.022721098735928535 + 0.1 * 6.65706205368042
Epoch 550, val loss: 0.8422233462333679
Epoch 560, training loss: 0.6880257725715637 = 0.02137225680053234 + 0.1 * 6.666534900665283
Epoch 560, val loss: 0.8504195809364319
Epoch 570, training loss: 0.6844325065612793 = 0.020149439573287964 + 0.1 * 6.6428303718566895
Epoch 570, val loss: 0.8582180738449097
Epoch 580, training loss: 0.6832872033119202 = 0.019036393612623215 + 0.1 * 6.642508029937744
Epoch 580, val loss: 0.8658579587936401
Epoch 590, training loss: 0.6816886067390442 = 0.018016450107097626 + 0.1 * 6.636721611022949
Epoch 590, val loss: 0.8734411001205444
Epoch 600, training loss: 0.6796767711639404 = 0.017083875834941864 + 0.1 * 6.62592887878418
Epoch 600, val loss: 0.8805984258651733
Epoch 610, training loss: 0.680017352104187 = 0.01622612774372101 + 0.1 * 6.637912273406982
Epoch 610, val loss: 0.8878372311592102
Epoch 620, training loss: 0.6778761744499207 = 0.015438312664628029 + 0.1 * 6.624378204345703
Epoch 620, val loss: 0.8946456909179688
Epoch 630, training loss: 0.6773988008499146 = 0.014711757190525532 + 0.1 * 6.626870632171631
Epoch 630, val loss: 0.9013487696647644
Epoch 640, training loss: 0.6752750277519226 = 0.01404001098126173 + 0.1 * 6.612349987030029
Epoch 640, val loss: 0.907940685749054
Epoch 650, training loss: 0.6753717660903931 = 0.013417239300906658 + 0.1 * 6.619544982910156
Epoch 650, val loss: 0.9142654538154602
Epoch 660, training loss: 0.6748548746109009 = 0.012837364338338375 + 0.1 * 6.620174884796143
Epoch 660, val loss: 0.9205978512763977
Epoch 670, training loss: 0.6724408864974976 = 0.012299629859626293 + 0.1 * 6.601412296295166
Epoch 670, val loss: 0.9265520572662354
Epoch 680, training loss: 0.673507034778595 = 0.011798320338129997 + 0.1 * 6.617086887359619
Epoch 680, val loss: 0.9324989318847656
Epoch 690, training loss: 0.6715406179428101 = 0.01133017335087061 + 0.1 * 6.602104187011719
Epoch 690, val loss: 0.938411295413971
Epoch 700, training loss: 0.6706700921058655 = 0.010893234983086586 + 0.1 * 6.597768783569336
Epoch 700, val loss: 0.9438913464546204
Epoch 710, training loss: 0.6689704060554504 = 0.010483740828931332 + 0.1 * 6.584866523742676
Epoch 710, val loss: 0.9494882225990295
Epoch 720, training loss: 0.6684316992759705 = 0.010099493898451328 + 0.1 * 6.583322048187256
Epoch 720, val loss: 0.9548678994178772
Epoch 730, training loss: 0.669873058795929 = 0.009737416170537472 + 0.1 * 6.601356029510498
Epoch 730, val loss: 0.9601171612739563
Epoch 740, training loss: 0.669108510017395 = 0.009396006353199482 + 0.1 * 6.597125053405762
Epoch 740, val loss: 0.96543949842453
Epoch 750, training loss: 0.6662442684173584 = 0.009077119641005993 + 0.1 * 6.571671009063721
Epoch 750, val loss: 0.9704365134239197
Epoch 760, training loss: 0.6675730347633362 = 0.008776180446147919 + 0.1 * 6.587968349456787
Epoch 760, val loss: 0.9752891063690186
Epoch 770, training loss: 0.6650281548500061 = 0.008490224368870258 + 0.1 * 6.5653791427612305
Epoch 770, val loss: 0.9801639318466187
Epoch 780, training loss: 0.6684995889663696 = 0.008220799267292023 + 0.1 * 6.602787971496582
Epoch 780, val loss: 0.9848232865333557
Epoch 790, training loss: 0.6639258861541748 = 0.007965871132910252 + 0.1 * 6.559600353240967
Epoch 790, val loss: 0.9895869493484497
Epoch 800, training loss: 0.6641630530357361 = 0.00772466417402029 + 0.1 * 6.5643839836120605
Epoch 800, val loss: 0.9939948320388794
Epoch 810, training loss: 0.662487804889679 = 0.007495199795812368 + 0.1 * 6.549925804138184
Epoch 810, val loss: 0.998542845249176
Epoch 820, training loss: 0.6621367335319519 = 0.007277988363057375 + 0.1 * 6.548587799072266
Epoch 820, val loss: 1.0027399063110352
Epoch 830, training loss: 0.6616427302360535 = 0.007070187944918871 + 0.1 * 6.545725345611572
Epoch 830, val loss: 1.0070128440856934
Epoch 840, training loss: 0.6629225015640259 = 0.006872585508972406 + 0.1 * 6.56049919128418
Epoch 840, val loss: 1.011433482170105
Epoch 850, training loss: 0.6614516377449036 = 0.006684798747301102 + 0.1 * 6.54766845703125
Epoch 850, val loss: 1.0153659582138062
Epoch 860, training loss: 0.6635552048683167 = 0.0065063186921179295 + 0.1 * 6.570488452911377
Epoch 860, val loss: 1.019379734992981
Epoch 870, training loss: 0.659589409828186 = 0.006335011217743158 + 0.1 * 6.532543659210205
Epoch 870, val loss: 1.023525595664978
Epoch 880, training loss: 0.6605316996574402 = 0.006172487512230873 + 0.1 * 6.5435919761657715
Epoch 880, val loss: 1.0271679162979126
Epoch 890, training loss: 0.6605564951896667 = 0.006016430910676718 + 0.1 * 6.545400619506836
Epoch 890, val loss: 1.0311864614486694
Epoch 900, training loss: 0.6574600338935852 = 0.005867547821253538 + 0.1 * 6.51592493057251
Epoch 900, val loss: 1.0349491834640503
Epoch 910, training loss: 0.6607935428619385 = 0.0057251290418207645 + 0.1 * 6.550683975219727
Epoch 910, val loss: 1.038495659828186
Epoch 920, training loss: 0.6579886078834534 = 0.0055879829451441765 + 0.1 * 6.524005889892578
Epoch 920, val loss: 1.0422993898391724
Epoch 930, training loss: 0.6590750217437744 = 0.005457211751490831 + 0.1 * 6.536178112030029
Epoch 930, val loss: 1.0458778142929077
Epoch 940, training loss: 0.6571811437606812 = 0.005331459920853376 + 0.1 * 6.518496990203857
Epoch 940, val loss: 1.0494054555892944
Epoch 950, training loss: 0.6557490825653076 = 0.005211036652326584 + 0.1 * 6.505380153656006
Epoch 950, val loss: 1.052794337272644
Epoch 960, training loss: 0.6569556593894958 = 0.005094632506370544 + 0.1 * 6.518610000610352
Epoch 960, val loss: 1.0562816858291626
Epoch 970, training loss: 0.6593122482299805 = 0.004982978105545044 + 0.1 * 6.543292999267578
Epoch 970, val loss: 1.0597612857818604
Epoch 980, training loss: 0.6555505990982056 = 0.004875822924077511 + 0.1 * 6.506747722625732
Epoch 980, val loss: 1.0631048679351807
Epoch 990, training loss: 0.6562021374702454 = 0.004773060325533152 + 0.1 * 6.5142903327941895
Epoch 990, val loss: 1.0662275552749634
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 2.8173365592956543 = 1.9576492309570312 + 0.1 * 8.596872329711914
Epoch 0, val loss: 1.9678690433502197
Epoch 10, training loss: 2.806741952896118 = 1.9470601081848145 + 0.1 * 8.596818923950195
Epoch 10, val loss: 1.9570188522338867
Epoch 20, training loss: 2.793548345565796 = 1.9338945150375366 + 0.1 * 8.596538543701172
Epoch 20, val loss: 1.9434340000152588
Epoch 30, training loss: 2.7750115394592285 = 1.9155923128128052 + 0.1 * 8.594191551208496
Epoch 30, val loss: 1.9242717027664185
Epoch 40, training loss: 2.7465217113494873 = 1.8890089988708496 + 0.1 * 8.575127601623535
Epoch 40, val loss: 1.8964807987213135
Epoch 50, training loss: 2.7012157440185547 = 1.8526973724365234 + 0.1 * 8.485182762145996
Epoch 50, val loss: 1.859908938407898
Epoch 60, training loss: 2.633446455001831 = 1.813520073890686 + 0.1 * 8.199264526367188
Epoch 60, val loss: 1.8232102394104004
Epoch 70, training loss: 2.5870392322540283 = 1.7791610956192017 + 0.1 * 8.078781127929688
Epoch 70, val loss: 1.7924679517745972
Epoch 80, training loss: 2.5275375843048096 = 1.741395354270935 + 0.1 * 7.861423015594482
Epoch 80, val loss: 1.7566767930984497
Epoch 90, training loss: 2.4544687271118164 = 1.6922001838684082 + 0.1 * 7.62268590927124
Epoch 90, val loss: 1.7124292850494385
Epoch 100, training loss: 2.3664042949676514 = 1.6292890310287476 + 0.1 * 7.371152877807617
Epoch 100, val loss: 1.6593632698059082
Epoch 110, training loss: 2.270643711090088 = 1.5518906116485596 + 0.1 * 7.187531471252441
Epoch 110, val loss: 1.59198796749115
Epoch 120, training loss: 2.1744508743286133 = 1.4646092653274536 + 0.1 * 7.098415374755859
Epoch 120, val loss: 1.5143651962280273
Epoch 130, training loss: 2.0828843116760254 = 1.3781601190567017 + 0.1 * 7.047242641448975
Epoch 130, val loss: 1.4399293661117554
Epoch 140, training loss: 1.9955246448516846 = 1.2947431802749634 + 0.1 * 7.007815361022949
Epoch 140, val loss: 1.3718338012695312
Epoch 150, training loss: 1.9115781784057617 = 1.2138497829437256 + 0.1 * 6.977283477783203
Epoch 150, val loss: 1.3072007894515991
Epoch 160, training loss: 1.8305314779281616 = 1.135399341583252 + 0.1 * 6.951321125030518
Epoch 160, val loss: 1.2472635507583618
Epoch 170, training loss: 1.7499866485595703 = 1.0565799474716187 + 0.1 * 6.9340667724609375
Epoch 170, val loss: 1.1887688636779785
Epoch 180, training loss: 1.6695009469985962 = 0.9781273007392883 + 0.1 * 6.913736343383789
Epoch 180, val loss: 1.1309478282928467
Epoch 190, training loss: 1.5905929803848267 = 0.9009746313095093 + 0.1 * 6.896183490753174
Epoch 190, val loss: 1.0745347738265991
Epoch 200, training loss: 1.5159127712249756 = 0.8273710012435913 + 0.1 * 6.8854169845581055
Epoch 200, val loss: 1.021847128868103
Epoch 210, training loss: 1.4459933042526245 = 0.7584452033042908 + 0.1 * 6.875481128692627
Epoch 210, val loss: 0.9741147756576538
Epoch 220, training loss: 1.3816725015640259 = 0.6943351030349731 + 0.1 * 6.873373985290527
Epoch 220, val loss: 0.9321596622467041
Epoch 230, training loss: 1.3222754001617432 = 0.6359146237373352 + 0.1 * 6.863608360290527
Epoch 230, val loss: 0.8965038657188416
Epoch 240, training loss: 1.2688515186309814 = 0.5830361843109131 + 0.1 * 6.858152389526367
Epoch 240, val loss: 0.8672502040863037
Epoch 250, training loss: 1.2210091352462769 = 0.535483181476593 + 0.1 * 6.855259418487549
Epoch 250, val loss: 0.844009518623352
Epoch 260, training loss: 1.1779370307922363 = 0.492721289396286 + 0.1 * 6.8521575927734375
Epoch 260, val loss: 0.8259620666503906
Epoch 270, training loss: 1.1382144689559937 = 0.4535488784313202 + 0.1 * 6.846656322479248
Epoch 270, val loss: 0.8118689060211182
Epoch 280, training loss: 1.1018872261047363 = 0.4168136715888977 + 0.1 * 6.850735187530518
Epoch 280, val loss: 0.8006923198699951
Epoch 290, training loss: 1.0660661458969116 = 0.3818274438381195 + 0.1 * 6.842386722564697
Epoch 290, val loss: 0.7916397452354431
Epoch 300, training loss: 1.03178071975708 = 0.34782108664512634 + 0.1 * 6.839596748352051
Epoch 300, val loss: 0.7837037444114685
Epoch 310, training loss: 0.9980759620666504 = 0.31437888741493225 + 0.1 * 6.836970329284668
Epoch 310, val loss: 0.776655375957489
Epoch 320, training loss: 0.9651678800582886 = 0.28149503469467163 + 0.1 * 6.836728572845459
Epoch 320, val loss: 0.7705332636833191
Epoch 330, training loss: 0.9332082271575928 = 0.24964961409568787 + 0.1 * 6.835585594177246
Epoch 330, val loss: 0.7655808329582214
Epoch 340, training loss: 0.9026961922645569 = 0.21966272592544556 + 0.1 * 6.830334663391113
Epoch 340, val loss: 0.7622969746589661
Epoch 350, training loss: 0.8748490214347839 = 0.19236065447330475 + 0.1 * 6.824883460998535
Epoch 350, val loss: 0.7618110775947571
Epoch 360, training loss: 0.850330650806427 = 0.16835074126720428 + 0.1 * 6.819798946380615
Epoch 360, val loss: 0.7642632126808167
Epoch 370, training loss: 0.8294262886047363 = 0.14769801497459412 + 0.1 * 6.817282199859619
Epoch 370, val loss: 0.769644558429718
Epoch 380, training loss: 0.8110973834991455 = 0.13005177676677704 + 0.1 * 6.810455799102783
Epoch 380, val loss: 0.7772247195243835
Epoch 390, training loss: 0.7966984510421753 = 0.11500665545463562 + 0.1 * 6.81691837310791
Epoch 390, val loss: 0.7867391109466553
Epoch 400, training loss: 0.7818381190299988 = 0.10216616839170456 + 0.1 * 6.796719074249268
Epoch 400, val loss: 0.7968122959136963
Epoch 410, training loss: 0.7700773477554321 = 0.09104875475168228 + 0.1 * 6.790285587310791
Epoch 410, val loss: 0.8075515627861023
Epoch 420, training loss: 0.7598317265510559 = 0.08137240260839462 + 0.1 * 6.78459358215332
Epoch 420, val loss: 0.8186556100845337
Epoch 430, training loss: 0.7503556609153748 = 0.07293850928544998 + 0.1 * 6.774171352386475
Epoch 430, val loss: 0.8297671675682068
Epoch 440, training loss: 0.742436945438385 = 0.06558752804994583 + 0.1 * 6.768494129180908
Epoch 440, val loss: 0.8408011794090271
Epoch 450, training loss: 0.7344797253608704 = 0.05918705835938454 + 0.1 * 6.752926826477051
Epoch 450, val loss: 0.8517754673957825
Epoch 460, training loss: 0.7282811403274536 = 0.05362735316157341 + 0.1 * 6.746537685394287
Epoch 460, val loss: 0.8627110719680786
Epoch 470, training loss: 0.723703145980835 = 0.04880528151988983 + 0.1 * 6.748978614807129
Epoch 470, val loss: 0.8732917904853821
Epoch 480, training loss: 0.717120349407196 = 0.04460660368204117 + 0.1 * 6.725137233734131
Epoch 480, val loss: 0.8834632039070129
Epoch 490, training loss: 0.7129846215248108 = 0.040927570313215256 + 0.1 * 6.720570087432861
Epoch 490, val loss: 0.8937383890151978
Epoch 500, training loss: 0.7108963131904602 = 0.037697866559028625 + 0.1 * 6.731984615325928
Epoch 500, val loss: 0.9034809470176697
Epoch 510, training loss: 0.7052503824234009 = 0.03485410287976265 + 0.1 * 6.703962326049805
Epoch 510, val loss: 0.9127862453460693
Epoch 520, training loss: 0.7016539573669434 = 0.03232031688094139 + 0.1 * 6.693336009979248
Epoch 520, val loss: 0.9221252799034119
Epoch 530, training loss: 0.6996073722839355 = 0.030057258903980255 + 0.1 * 6.695501327514648
Epoch 530, val loss: 0.9311870336532593
Epoch 540, training loss: 0.6969691514968872 = 0.028033480048179626 + 0.1 * 6.689356327056885
Epoch 540, val loss: 0.9397721886634827
Epoch 550, training loss: 0.6938727498054504 = 0.0262228362262249 + 0.1 * 6.676499366760254
Epoch 550, val loss: 0.9482492804527283
Epoch 560, training loss: 0.6915892958641052 = 0.02459070272743702 + 0.1 * 6.669985294342041
Epoch 560, val loss: 0.9564135670661926
Epoch 570, training loss: 0.68903648853302 = 0.023114528506994247 + 0.1 * 6.659219741821289
Epoch 570, val loss: 0.9643953442573547
Epoch 580, training loss: 0.6871911287307739 = 0.021774718537926674 + 0.1 * 6.654163837432861
Epoch 580, val loss: 0.9721482992172241
Epoch 590, training loss: 0.685704231262207 = 0.020556103438138962 + 0.1 * 6.6514811515808105
Epoch 590, val loss: 0.9797860383987427
Epoch 600, training loss: 0.6839109659194946 = 0.019445709884166718 + 0.1 * 6.644652366638184
Epoch 600, val loss: 0.9868730306625366
Epoch 610, training loss: 0.6827160120010376 = 0.018426349386572838 + 0.1 * 6.64289665222168
Epoch 610, val loss: 0.9941286444664001
Epoch 620, training loss: 0.6814432740211487 = 0.01749175786972046 + 0.1 * 6.639514923095703
Epoch 620, val loss: 1.0011259317398071
Epoch 630, training loss: 0.6791064143180847 = 0.0166335366666317 + 0.1 * 6.624728679656982
Epoch 630, val loss: 1.0077385902404785
Epoch 640, training loss: 0.6777696013450623 = 0.015839681029319763 + 0.1 * 6.619298934936523
Epoch 640, val loss: 1.0143705606460571
Epoch 650, training loss: 0.6788328886032104 = 0.015104556456208229 + 0.1 * 6.6372833251953125
Epoch 650, val loss: 1.0209180116653442
Epoch 660, training loss: 0.6766558885574341 = 0.01442380528897047 + 0.1 * 6.622320652008057
Epoch 660, val loss: 1.027175784111023
Epoch 670, training loss: 0.6755809187889099 = 0.013794160448014736 + 0.1 * 6.6178669929504395
Epoch 670, val loss: 1.0331536531448364
Epoch 680, training loss: 0.6732432246208191 = 0.013210286386311054 + 0.1 * 6.6003289222717285
Epoch 680, val loss: 1.0389881134033203
Epoch 690, training loss: 0.6740860939025879 = 0.012665788643062115 + 0.1 * 6.614202976226807
Epoch 690, val loss: 1.0448483228683472
Epoch 700, training loss: 0.6721675395965576 = 0.012156994082033634 + 0.1 * 6.600105285644531
Epoch 700, val loss: 1.0505450963974
Epoch 710, training loss: 0.6708782911300659 = 0.011682206764817238 + 0.1 * 6.591960430145264
Epoch 710, val loss: 1.0559442043304443
Epoch 720, training loss: 0.6704633235931396 = 0.011236181482672691 + 0.1 * 6.592271327972412
Epoch 720, val loss: 1.061540961265564
Epoch 730, training loss: 0.6689287424087524 = 0.010817923583090305 + 0.1 * 6.581108093261719
Epoch 730, val loss: 1.0668261051177979
Epoch 740, training loss: 0.6722040772438049 = 0.010424662381410599 + 0.1 * 6.617794036865234
Epoch 740, val loss: 1.071990966796875
Epoch 750, training loss: 0.6682498455047607 = 0.010056231170892715 + 0.1 * 6.581935882568359
Epoch 750, val loss: 1.0772603750228882
Epoch 760, training loss: 0.6672607660293579 = 0.009710024110972881 + 0.1 * 6.575507640838623
Epoch 760, val loss: 1.0819079875946045
Epoch 770, training loss: 0.6686236262321472 = 0.009382453747093678 + 0.1 * 6.592411994934082
Epoch 770, val loss: 1.0870057344436646
Epoch 780, training loss: 0.6660338640213013 = 0.009073708206415176 + 0.1 * 6.569601058959961
Epoch 780, val loss: 1.0916517972946167
Epoch 790, training loss: 0.6675990223884583 = 0.008781795389950275 + 0.1 * 6.588172435760498
Epoch 790, val loss: 1.096238136291504
Epoch 800, training loss: 0.6653635501861572 = 0.008505409583449364 + 0.1 * 6.568581581115723
Epoch 800, val loss: 1.1010832786560059
Epoch 810, training loss: 0.6648669242858887 = 0.008244598284363747 + 0.1 * 6.56622314453125
Epoch 810, val loss: 1.1052577495574951
Epoch 820, training loss: 0.6631577610969543 = 0.007996122352778912 + 0.1 * 6.551616191864014
Epoch 820, val loss: 1.1099956035614014
Epoch 830, training loss: 0.6628442406654358 = 0.0077615780755877495 + 0.1 * 6.550826072692871
Epoch 830, val loss: 1.1140625476837158
Epoch 840, training loss: 0.6620566248893738 = 0.007538065779954195 + 0.1 * 6.545185089111328
Epoch 840, val loss: 1.1182401180267334
Epoch 850, training loss: 0.6640682220458984 = 0.007324738893657923 + 0.1 * 6.567434787750244
Epoch 850, val loss: 1.1226444244384766
Epoch 860, training loss: 0.6616117358207703 = 0.007121450267732143 + 0.1 * 6.544902801513672
Epoch 860, val loss: 1.126756191253662
Epoch 870, training loss: 0.6627519726753235 = 0.006928194779902697 + 0.1 * 6.558237552642822
Epoch 870, val loss: 1.1306028366088867
Epoch 880, training loss: 0.6613965630531311 = 0.006743995472788811 + 0.1 * 6.546525955200195
Epoch 880, val loss: 1.1347486972808838
Epoch 890, training loss: 0.6618425846099854 = 0.006568406708538532 + 0.1 * 6.552741527557373
Epoch 890, val loss: 1.1385101079940796
Epoch 900, training loss: 0.6600633263587952 = 0.006400469224900007 + 0.1 * 6.536628723144531
Epoch 900, val loss: 1.1424331665039062
Epoch 910, training loss: 0.6598108410835266 = 0.0062407529912889 + 0.1 * 6.535701274871826
Epoch 910, val loss: 1.1458808183670044
Epoch 920, training loss: 0.6597288250923157 = 0.006087151356041431 + 0.1 * 6.536417007446289
Epoch 920, val loss: 1.1496400833129883
Epoch 930, training loss: 0.6598494648933411 = 0.005940096918493509 + 0.1 * 6.539093494415283
Epoch 930, val loss: 1.1534569263458252
Epoch 940, training loss: 0.6586654186248779 = 0.005799147766083479 + 0.1 * 6.528662204742432
Epoch 940, val loss: 1.1567590236663818
Epoch 950, training loss: 0.6593353152275085 = 0.005664117634296417 + 0.1 * 6.536711692810059
Epoch 950, val loss: 1.1602219343185425
Epoch 960, training loss: 0.6585344672203064 = 0.0055343229323625565 + 0.1 * 6.530001163482666
Epoch 960, val loss: 1.1638374328613281
Epoch 970, training loss: 0.6583418250083923 = 0.005410150159150362 + 0.1 * 6.5293169021606445
Epoch 970, val loss: 1.167033314704895
Epoch 980, training loss: 0.6570892930030823 = 0.005290606990456581 + 0.1 * 6.51798677444458
Epoch 980, val loss: 1.1702873706817627
Epoch 990, training loss: 0.65798419713974 = 0.0051756566390395164 + 0.1 * 6.528085231781006
Epoch 990, val loss: 1.1734936237335205
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8128624143384291
The final CL Acc:0.76667, 0.01090, The final GNN Acc:0.81128, 0.00383
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13114])
remove edge: torch.Size([2, 7862])
updated graph: torch.Size([2, 10420])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.797743320465088 = 1.9380570650100708 + 0.1 * 8.59686279296875
Epoch 0, val loss: 1.9341168403625488
Epoch 10, training loss: 2.787123203277588 = 1.9274410009384155 + 0.1 * 8.596821784973145
Epoch 10, val loss: 1.924127221107483
Epoch 20, training loss: 2.774456739425659 = 1.914800763130188 + 0.1 * 8.59656047821045
Epoch 20, val loss: 1.9118268489837646
Epoch 30, training loss: 2.757059097290039 = 1.8976267576217651 + 0.1 * 8.594322204589844
Epoch 30, val loss: 1.8948732614517212
Epoch 40, training loss: 2.730698585510254 = 1.873208999633789 + 0.1 * 8.574894905090332
Epoch 40, val loss: 1.870898962020874
Epoch 50, training loss: 2.687276601791382 = 1.839998722076416 + 0.1 * 8.4727783203125
Epoch 50, val loss: 1.839606523513794
Epoch 60, training loss: 2.6228561401367188 = 1.8031710386276245 + 0.1 * 8.196849822998047
Epoch 60, val loss: 1.8077250719070435
Epoch 70, training loss: 2.5667710304260254 = 1.7682379484176636 + 0.1 * 7.985331058502197
Epoch 70, val loss: 1.7785218954086304
Epoch 80, training loss: 2.485944986343384 = 1.7284979820251465 + 0.1 * 7.574469566345215
Epoch 80, val loss: 1.742440938949585
Epoch 90, training loss: 2.403621196746826 = 1.6760042905807495 + 0.1 * 7.2761688232421875
Epoch 90, val loss: 1.6938745975494385
Epoch 100, training loss: 2.3216543197631836 = 1.6073039770126343 + 0.1 * 7.143504619598389
Epoch 100, val loss: 1.6328727006912231
Epoch 110, training loss: 2.2305517196655273 = 1.5246022939682007 + 0.1 * 7.059494972229004
Epoch 110, val loss: 1.5613540410995483
Epoch 120, training loss: 2.1346378326416016 = 1.4333134889602661 + 0.1 * 7.01324462890625
Epoch 120, val loss: 1.4841080904006958
Epoch 130, training loss: 2.035799026489258 = 1.337660312652588 + 0.1 * 6.981387615203857
Epoch 130, val loss: 1.4057127237319946
Epoch 140, training loss: 1.9342637062072754 = 1.2378449440002441 + 0.1 * 6.964186668395996
Epoch 140, val loss: 1.3235613107681274
Epoch 150, training loss: 1.8315582275390625 = 1.1360632181167603 + 0.1 * 6.95495080947876
Epoch 150, val loss: 1.240971565246582
Epoch 160, training loss: 1.7309314012527466 = 1.0360981225967407 + 0.1 * 6.948332786560059
Epoch 160, val loss: 1.1602503061294556
Epoch 170, training loss: 1.6347124576568604 = 0.9402885437011719 + 0.1 * 6.944239139556885
Epoch 170, val loss: 1.0829439163208008
Epoch 180, training loss: 1.5443496704101562 = 0.850458025932312 + 0.1 * 6.938916206359863
Epoch 180, val loss: 1.0101715326309204
Epoch 190, training loss: 1.4606822729110718 = 0.7669420838356018 + 0.1 * 6.93740177154541
Epoch 190, val loss: 0.942703902721405
Epoch 200, training loss: 1.3835930824279785 = 0.6907839775085449 + 0.1 * 6.928091049194336
Epoch 200, val loss: 0.882939338684082
Epoch 210, training loss: 1.313582181930542 = 0.6216690540313721 + 0.1 * 6.919131278991699
Epoch 210, val loss: 0.8316972851753235
Epoch 220, training loss: 1.2508678436279297 = 0.5593202710151672 + 0.1 * 6.915475845336914
Epoch 220, val loss: 0.7896478772163391
Epoch 230, training loss: 1.1939880847930908 = 0.5036692023277283 + 0.1 * 6.903188705444336
Epoch 230, val loss: 0.7562446594238281
Epoch 240, training loss: 1.1423835754394531 = 0.45335376262664795 + 0.1 * 6.8902974128723145
Epoch 240, val loss: 0.7296884059906006
Epoch 250, training loss: 1.0953304767608643 = 0.4072740972042084 + 0.1 * 6.880563735961914
Epoch 250, val loss: 0.7085734605789185
Epoch 260, training loss: 1.052354335784912 = 0.3649234473705292 + 0.1 * 6.874309062957764
Epoch 260, val loss: 0.691862940788269
Epoch 270, training loss: 1.0124375820159912 = 0.3261317014694214 + 0.1 * 6.8630595207214355
Epoch 270, val loss: 0.6792771816253662
Epoch 280, training loss: 0.9753067493438721 = 0.2906716465950012 + 0.1 * 6.846351146697998
Epoch 280, val loss: 0.6701705455780029
Epoch 290, training loss: 0.9433261752128601 = 0.25851136445999146 + 0.1 * 6.848147869110107
Epoch 290, val loss: 0.6643860340118408
Epoch 300, training loss: 0.9130150675773621 = 0.22985821962356567 + 0.1 * 6.831568241119385
Epoch 300, val loss: 0.6614463329315186
Epoch 310, training loss: 0.8875531554222107 = 0.20455314218997955 + 0.1 * 6.829999923706055
Epoch 310, val loss: 0.6612226366996765
Epoch 320, training loss: 0.8645111918449402 = 0.18241238594055176 + 0.1 * 6.820988178253174
Epoch 320, val loss: 0.6632558107376099
Epoch 330, training loss: 0.844560980796814 = 0.1631699502468109 + 0.1 * 6.813910484313965
Epoch 330, val loss: 0.6674054861068726
Epoch 340, training loss: 0.8273060917854309 = 0.14648999273777008 + 0.1 * 6.808160781860352
Epoch 340, val loss: 0.6730286478996277
Epoch 350, training loss: 0.8121528625488281 = 0.13192127645015717 + 0.1 * 6.802315711975098
Epoch 350, val loss: 0.680063784122467
Epoch 360, training loss: 0.7992271780967712 = 0.11917056143283844 + 0.1 * 6.80056619644165
Epoch 360, val loss: 0.688173770904541
Epoch 370, training loss: 0.7867214679718018 = 0.10798906534910202 + 0.1 * 6.787323951721191
Epoch 370, val loss: 0.6969236731529236
Epoch 380, training loss: 0.7769201397895813 = 0.09812082350254059 + 0.1 * 6.787993431091309
Epoch 380, val loss: 0.7063069343566895
Epoch 390, training loss: 0.7670225501060486 = 0.08941208571195602 + 0.1 * 6.77610445022583
Epoch 390, val loss: 0.7159553170204163
Epoch 400, training loss: 0.7584839463233948 = 0.0816817432641983 + 0.1 * 6.768022060394287
Epoch 400, val loss: 0.7259352803230286
Epoch 410, training loss: 0.7508636116981506 = 0.07479187101125717 + 0.1 * 6.760717391967773
Epoch 410, val loss: 0.7360672354698181
Epoch 420, training loss: 0.7439025640487671 = 0.0686638206243515 + 0.1 * 6.752387046813965
Epoch 420, val loss: 0.746242105960846
Epoch 430, training loss: 0.7375865578651428 = 0.06319788843393326 + 0.1 * 6.743886470794678
Epoch 430, val loss: 0.7562434673309326
Epoch 440, training loss: 0.7330632209777832 = 0.058284249156713486 + 0.1 * 6.7477898597717285
Epoch 440, val loss: 0.7664220333099365
Epoch 450, training loss: 0.7277798056602478 = 0.05387832596898079 + 0.1 * 6.739014625549316
Epoch 450, val loss: 0.7765183448791504
Epoch 460, training loss: 0.7226651310920715 = 0.04990947246551514 + 0.1 * 6.727556228637695
Epoch 460, val loss: 0.786556601524353
Epoch 470, training loss: 0.7182459235191345 = 0.04631596431136131 + 0.1 * 6.71929931640625
Epoch 470, val loss: 0.7965325713157654
Epoch 480, training loss: 0.7147897481918335 = 0.04306171461939812 + 0.1 * 6.71727991104126
Epoch 480, val loss: 0.8063729405403137
Epoch 490, training loss: 0.711898148059845 = 0.04012443125247955 + 0.1 * 6.717737197875977
Epoch 490, val loss: 0.8158717751502991
Epoch 500, training loss: 0.707327663898468 = 0.037465691566467285 + 0.1 * 6.698619365692139
Epoch 500, val loss: 0.8252339959144592
Epoch 510, training loss: 0.7039806246757507 = 0.03504061698913574 + 0.1 * 6.689399719238281
Epoch 510, val loss: 0.8345628976821899
Epoch 520, training loss: 0.7010140419006348 = 0.03282728046178818 + 0.1 * 6.681867599487305
Epoch 520, val loss: 0.8437684178352356
Epoch 530, training loss: 0.6986668109893799 = 0.03080565482378006 + 0.1 * 6.6786112785339355
Epoch 530, val loss: 0.8525394797325134
Epoch 540, training loss: 0.695610761642456 = 0.02895268425345421 + 0.1 * 6.666581153869629
Epoch 540, val loss: 0.8614059686660767
Epoch 550, training loss: 0.69504314661026 = 0.02725154161453247 + 0.1 * 6.677916049957275
Epoch 550, val loss: 0.8700124621391296
Epoch 560, training loss: 0.6924079060554504 = 0.02569255605340004 + 0.1 * 6.6671528816223145
Epoch 560, val loss: 0.878304123878479
Epoch 570, training loss: 0.6903102993965149 = 0.02425994910299778 + 0.1 * 6.66050386428833
Epoch 570, val loss: 0.8864203095436096
Epoch 580, training loss: 0.6875950694084167 = 0.022943517193198204 + 0.1 * 6.646515846252441
Epoch 580, val loss: 0.8944025635719299
Epoch 590, training loss: 0.6863645911216736 = 0.021728403866291046 + 0.1 * 6.646361827850342
Epoch 590, val loss: 0.9021640419960022
Epoch 600, training loss: 0.6845086812973022 = 0.020604509860277176 + 0.1 * 6.639041900634766
Epoch 600, val loss: 0.909739077091217
Epoch 610, training loss: 0.6833851337432861 = 0.019564904272556305 + 0.1 * 6.638201713562012
Epoch 610, val loss: 0.9171258211135864
Epoch 620, training loss: 0.6811838746070862 = 0.018602347001433372 + 0.1 * 6.625814914703369
Epoch 620, val loss: 0.9243344068527222
Epoch 630, training loss: 0.6830741763114929 = 0.017707664519548416 + 0.1 * 6.653665065765381
Epoch 630, val loss: 0.931422770023346
Epoch 640, training loss: 0.6788827776908875 = 0.01687764748930931 + 0.1 * 6.620051383972168
Epoch 640, val loss: 0.9383533596992493
Epoch 650, training loss: 0.6804201006889343 = 0.016104692593216896 + 0.1 * 6.643153667449951
Epoch 650, val loss: 0.9450239539146423
Epoch 660, training loss: 0.6769923567771912 = 0.015385594218969345 + 0.1 * 6.616067886352539
Epoch 660, val loss: 0.9515505433082581
Epoch 670, training loss: 0.6761237382888794 = 0.014714200049638748 + 0.1 * 6.614095211029053
Epoch 670, val loss: 0.9579018950462341
Epoch 680, training loss: 0.6769419312477112 = 0.014087116345763206 + 0.1 * 6.628547668457031
Epoch 680, val loss: 0.964215099811554
Epoch 690, training loss: 0.6739283204078674 = 0.013501758687198162 + 0.1 * 6.6042656898498535
Epoch 690, val loss: 0.9702409505844116
Epoch 700, training loss: 0.6726855635643005 = 0.012953286059200764 + 0.1 * 6.597322940826416
Epoch 700, val loss: 0.9761379957199097
Epoch 710, training loss: 0.6724950671195984 = 0.012438627891242504 + 0.1 * 6.600564002990723
Epoch 710, val loss: 0.982009768486023
Epoch 720, training loss: 0.6732310056686401 = 0.011954773217439651 + 0.1 * 6.612761974334717
Epoch 720, val loss: 0.9877507090568542
Epoch 730, training loss: 0.6708655953407288 = 0.011502359993755817 + 0.1 * 6.593632221221924
Epoch 730, val loss: 0.9931629300117493
Epoch 740, training loss: 0.6695870161056519 = 0.011076084338128567 + 0.1 * 6.585109233856201
Epoch 740, val loss: 0.9984834790229797
Epoch 750, training loss: 0.6703165769577026 = 0.010673959739506245 + 0.1 * 6.596426486968994
Epoch 750, val loss: 1.0038812160491943
Epoch 760, training loss: 0.6694396734237671 = 0.010295982472598553 + 0.1 * 6.591436862945557
Epoch 760, val loss: 1.0090612173080444
Epoch 770, training loss: 0.6679239869117737 = 0.009937839582562447 + 0.1 * 6.579861640930176
Epoch 770, val loss: 1.013981819152832
Epoch 780, training loss: 0.6680160164833069 = 0.009599683806300163 + 0.1 * 6.584163188934326
Epoch 780, val loss: 1.0190054178237915
Epoch 790, training loss: 0.6657800078392029 = 0.00928046740591526 + 0.1 * 6.564995288848877
Epoch 790, val loss: 1.0237895250320435
Epoch 800, training loss: 0.667181670665741 = 0.00897824764251709 + 0.1 * 6.582034111022949
Epoch 800, val loss: 1.0284687280654907
Epoch 810, training loss: 0.6661241054534912 = 0.008691566996276379 + 0.1 * 6.5743255615234375
Epoch 810, val loss: 1.0331889390945435
Epoch 820, training loss: 0.6641470789909363 = 0.008419381454586983 + 0.1 * 6.557276725769043
Epoch 820, val loss: 1.0375951528549194
Epoch 830, training loss: 0.6664029359817505 = 0.008161785081028938 + 0.1 * 6.582411289215088
Epoch 830, val loss: 1.0419780015945435
Epoch 840, training loss: 0.6636241674423218 = 0.007915947586297989 + 0.1 * 6.557081699371338
Epoch 840, val loss: 1.0464133024215698
Epoch 850, training loss: 0.6644715666770935 = 0.007683690637350082 + 0.1 * 6.567878723144531
Epoch 850, val loss: 1.0506149530410767
Epoch 860, training loss: 0.6640316843986511 = 0.0074613383039832115 + 0.1 * 6.565703392028809
Epoch 860, val loss: 1.0547447204589844
Epoch 870, training loss: 0.6618263721466064 = 0.007250247523188591 + 0.1 * 6.5457611083984375
Epoch 870, val loss: 1.0587878227233887
Epoch 880, training loss: 0.6634847521781921 = 0.007048530969768763 + 0.1 * 6.564362049102783
Epoch 880, val loss: 1.0627638101577759
Epoch 890, training loss: 0.6611926555633545 = 0.006857156753540039 + 0.1 * 6.5433549880981445
Epoch 890, val loss: 1.0667431354522705
Epoch 900, training loss: 0.6603980660438538 = 0.006673476193100214 + 0.1 * 6.537246227264404
Epoch 900, val loss: 1.0705187320709229
Epoch 910, training loss: 0.6592546701431274 = 0.006498061586171389 + 0.1 * 6.527565956115723
Epoch 910, val loss: 1.0742697715759277
Epoch 920, training loss: 0.660172700881958 = 0.00633059348911047 + 0.1 * 6.538420677185059
Epoch 920, val loss: 1.078041672706604
Epoch 930, training loss: 0.6592830419540405 = 0.006170071195811033 + 0.1 * 6.531129360198975
Epoch 930, val loss: 1.0815863609313965
Epoch 940, training loss: 0.6591342687606812 = 0.006016694474965334 + 0.1 * 6.5311760902404785
Epoch 940, val loss: 1.0850111246109009
Epoch 950, training loss: 0.6576507091522217 = 0.005870170425623655 + 0.1 * 6.517805099487305
Epoch 950, val loss: 1.0884616374969482
Epoch 960, training loss: 0.6592794060707092 = 0.005728837568312883 + 0.1 * 6.535505294799805
Epoch 960, val loss: 1.091981053352356
Epoch 970, training loss: 0.6566383838653564 = 0.005593826062977314 + 0.1 * 6.5104451179504395
Epoch 970, val loss: 1.0953065156936646
Epoch 980, training loss: 0.6575515866279602 = 0.005464514717459679 + 0.1 * 6.520870685577393
Epoch 980, val loss: 1.0985088348388672
Epoch 990, training loss: 0.656299889087677 = 0.005339934956282377 + 0.1 * 6.509599685668945
Epoch 990, val loss: 1.1017396450042725
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.8079421520233154 = 1.9482563734054565 + 0.1 * 8.596857070922852
Epoch 0, val loss: 1.9521610736846924
Epoch 10, training loss: 2.7977840900421143 = 1.9381083250045776 + 0.1 * 8.596756935119629
Epoch 10, val loss: 1.9422998428344727
Epoch 20, training loss: 2.784973621368408 = 1.9253591299057007 + 0.1 * 8.596145629882812
Epoch 20, val loss: 1.929407000541687
Epoch 30, training loss: 2.7664527893066406 = 1.9074032306671143 + 0.1 * 8.590495109558105
Epoch 30, val loss: 1.9108500480651855
Epoch 40, training loss: 2.7361185550689697 = 1.8809319734573364 + 0.1 * 8.551865577697754
Epoch 40, val loss: 1.8834556341171265
Epoch 50, training loss: 2.679602861404419 = 1.8450613021850586 + 0.1 * 8.345416069030762
Epoch 50, val loss: 1.8477799892425537
Epoch 60, training loss: 2.6211342811584473 = 1.8059879541397095 + 0.1 * 8.15146255493164
Epoch 60, val loss: 1.8118678331375122
Epoch 70, training loss: 2.567425012588501 = 1.770646095275879 + 0.1 * 7.967789173126221
Epoch 70, val loss: 1.7812707424163818
Epoch 80, training loss: 2.4886438846588135 = 1.7321193218231201 + 0.1 * 7.565244674682617
Epoch 80, val loss: 1.7457939386367798
Epoch 90, training loss: 2.407172679901123 = 1.6830209493637085 + 0.1 * 7.24151611328125
Epoch 90, val loss: 1.701633334159851
Epoch 100, training loss: 2.332475185394287 = 1.617095947265625 + 0.1 * 7.1537933349609375
Epoch 100, val loss: 1.6433122158050537
Epoch 110, training loss: 2.2408201694488525 = 1.5337207317352295 + 0.1 * 7.070994853973389
Epoch 110, val loss: 1.5695985555648804
Epoch 120, training loss: 2.141477108001709 = 1.4404205083847046 + 0.1 * 7.010564804077148
Epoch 120, val loss: 1.490412712097168
Epoch 130, training loss: 2.0409464836120605 = 1.3442646265029907 + 0.1 * 6.966818332672119
Epoch 130, val loss: 1.4110549688339233
Epoch 140, training loss: 1.9423000812530518 = 1.248972773551941 + 0.1 * 6.9332733154296875
Epoch 140, val loss: 1.3333579301834106
Epoch 150, training loss: 1.8477435111999512 = 1.1574983596801758 + 0.1 * 6.902451992034912
Epoch 150, val loss: 1.2593848705291748
Epoch 160, training loss: 1.7597984075546265 = 1.0719618797302246 + 0.1 * 6.8783650398254395
Epoch 160, val loss: 1.1913418769836426
Epoch 170, training loss: 1.680466651916504 = 0.9949430823326111 + 0.1 * 6.8552350997924805
Epoch 170, val loss: 1.1316229104995728
Epoch 180, training loss: 1.6081335544586182 = 0.9243642091751099 + 0.1 * 6.837692737579346
Epoch 180, val loss: 1.0779651403427124
Epoch 190, training loss: 1.5407946109771729 = 0.858547568321228 + 0.1 * 6.822470188140869
Epoch 190, val loss: 1.027910828590393
Epoch 200, training loss: 1.4750783443450928 = 0.7948004603385925 + 0.1 * 6.802778244018555
Epoch 200, val loss: 0.9788870215415955
Epoch 210, training loss: 1.412434697151184 = 0.7324681282043457 + 0.1 * 6.799665451049805
Epoch 210, val loss: 0.9311333298683167
Epoch 220, training loss: 1.3515536785125732 = 0.6733771562576294 + 0.1 * 6.781764984130859
Epoch 220, val loss: 0.8868393898010254
Epoch 230, training loss: 1.295075535774231 = 0.6180315017700195 + 0.1 * 6.770440101623535
Epoch 230, val loss: 0.8474825620651245
Epoch 240, training loss: 1.2441675662994385 = 0.5671895742416382 + 0.1 * 6.769780158996582
Epoch 240, val loss: 0.8143395185470581
Epoch 250, training loss: 1.1959997415542603 = 0.5207471251487732 + 0.1 * 6.752525806427002
Epoch 250, val loss: 0.7875463366508484
Epoch 260, training loss: 1.1518893241882324 = 0.4779461622238159 + 0.1 * 6.739432334899902
Epoch 260, val loss: 0.7664285898208618
Epoch 270, training loss: 1.11173677444458 = 0.4383517801761627 + 0.1 * 6.733850002288818
Epoch 270, val loss: 0.7498754858970642
Epoch 280, training loss: 1.0740879774093628 = 0.4016389548778534 + 0.1 * 6.724489688873291
Epoch 280, val loss: 0.7368203997612
Epoch 290, training loss: 1.0398086309432983 = 0.3675725758075714 + 0.1 * 6.722360610961914
Epoch 290, val loss: 0.7262548804283142
Epoch 300, training loss: 1.0066759586334229 = 0.3354620933532715 + 0.1 * 6.712139129638672
Epoch 300, val loss: 0.7174426913261414
Epoch 310, training loss: 0.975153923034668 = 0.3048148453235626 + 0.1 * 6.703390121459961
Epoch 310, val loss: 0.7099996209144592
Epoch 320, training loss: 0.9448699951171875 = 0.2754497826099396 + 0.1 * 6.694201469421387
Epoch 320, val loss: 0.7036367058753967
Epoch 330, training loss: 0.9188477396965027 = 0.2475861757993698 + 0.1 * 6.712615489959717
Epoch 330, val loss: 0.6983399391174316
Epoch 340, training loss: 0.8905406594276428 = 0.22183547914028168 + 0.1 * 6.687051296234131
Epoch 340, val loss: 0.6942493319511414
Epoch 350, training loss: 0.8666677474975586 = 0.19849199056625366 + 0.1 * 6.68175745010376
Epoch 350, val loss: 0.6917043328285217
Epoch 360, training loss: 0.84637850522995 = 0.17784114181995392 + 0.1 * 6.685373783111572
Epoch 360, val loss: 0.690887451171875
Epoch 370, training loss: 0.8264672756195068 = 0.1598253697156906 + 0.1 * 6.666418552398682
Epoch 370, val loss: 0.6917632818222046
Epoch 380, training loss: 0.8099141120910645 = 0.14405961334705353 + 0.1 * 6.658545017242432
Epoch 380, val loss: 0.6941742897033691
Epoch 390, training loss: 0.7967631816864014 = 0.13030283153057098 + 0.1 * 6.664603233337402
Epoch 390, val loss: 0.6977738738059998
Epoch 400, training loss: 0.7826059460639954 = 0.11828148365020752 + 0.1 * 6.64324426651001
Epoch 400, val loss: 0.7024410963058472
Epoch 410, training loss: 0.7715497612953186 = 0.10766340047121048 + 0.1 * 6.638863563537598
Epoch 410, val loss: 0.7079707980155945
Epoch 420, training loss: 0.7613164186477661 = 0.09825770556926727 + 0.1 * 6.630586624145508
Epoch 420, val loss: 0.7141472697257996
Epoch 430, training loss: 0.7527355551719666 = 0.08989334106445312 + 0.1 * 6.628422260284424
Epoch 430, val loss: 0.7208541035652161
Epoch 440, training loss: 0.7468829154968262 = 0.08240993320941925 + 0.1 * 6.644730091094971
Epoch 440, val loss: 0.727942943572998
Epoch 450, training loss: 0.7375749349594116 = 0.07571857422590256 + 0.1 * 6.618563652038574
Epoch 450, val loss: 0.7352748513221741
Epoch 460, training loss: 0.731920063495636 = 0.06972765922546387 + 0.1 * 6.621923923492432
Epoch 460, val loss: 0.742770254611969
Epoch 470, training loss: 0.7249158024787903 = 0.06433894485235214 + 0.1 * 6.60576868057251
Epoch 470, val loss: 0.7504128217697144
Epoch 480, training loss: 0.720050036907196 = 0.05946853384375572 + 0.1 * 6.6058149337768555
Epoch 480, val loss: 0.7581614255905151
Epoch 490, training loss: 0.7145751118659973 = 0.05506974831223488 + 0.1 * 6.595053672790527
Epoch 490, val loss: 0.7659178376197815
Epoch 500, training loss: 0.7122485041618347 = 0.051094088703393936 + 0.1 * 6.611543655395508
Epoch 500, val loss: 0.7736546397209167
Epoch 510, training loss: 0.706318736076355 = 0.0474972128868103 + 0.1 * 6.588214874267578
Epoch 510, val loss: 0.7813616991043091
Epoch 520, training loss: 0.702595591545105 = 0.04423046112060547 + 0.1 * 6.583651065826416
Epoch 520, val loss: 0.7890118360519409
Epoch 530, training loss: 0.6986832618713379 = 0.041261378675699234 + 0.1 * 6.57421875
Epoch 530, val loss: 0.7965060472488403
Epoch 540, training loss: 0.695310652256012 = 0.03856760635972023 + 0.1 * 6.56743049621582
Epoch 540, val loss: 0.8039573431015015
Epoch 550, training loss: 0.6924607753753662 = 0.03610686585307121 + 0.1 * 6.563539028167725
Epoch 550, val loss: 0.811283528804779
Epoch 560, training loss: 0.6912750005722046 = 0.03386346250772476 + 0.1 * 6.574114799499512
Epoch 560, val loss: 0.8184672594070435
Epoch 570, training loss: 0.6872657537460327 = 0.0318208709359169 + 0.1 * 6.55444860458374
Epoch 570, val loss: 0.8255486488342285
Epoch 580, training loss: 0.6855626702308655 = 0.02994723431766033 + 0.1 * 6.556154251098633
Epoch 580, val loss: 0.8325029611587524
Epoch 590, training loss: 0.684232771396637 = 0.02822655811905861 + 0.1 * 6.560061931610107
Epoch 590, val loss: 0.8393424153327942
Epoch 600, training loss: 0.6814440488815308 = 0.02664981223642826 + 0.1 * 6.547942161560059
Epoch 600, val loss: 0.8460180163383484
Epoch 610, training loss: 0.679962158203125 = 0.025199562311172485 + 0.1 * 6.547625541687012
Epoch 610, val loss: 0.8525940775871277
Epoch 620, training loss: 0.6775372624397278 = 0.02386271394789219 + 0.1 * 6.536745548248291
Epoch 620, val loss: 0.8589928150177002
Epoch 630, training loss: 0.6781229972839355 = 0.02262716367840767 + 0.1 * 6.554957866668701
Epoch 630, val loss: 0.8653497695922852
Epoch 640, training loss: 0.6758848428726196 = 0.021486550569534302 + 0.1 * 6.54398250579834
Epoch 640, val loss: 0.8714068531990051
Epoch 650, training loss: 0.6739295125007629 = 0.020434094592928886 + 0.1 * 6.534954071044922
Epoch 650, val loss: 0.877491295337677
Epoch 660, training loss: 0.6712651252746582 = 0.019456828013062477 + 0.1 * 6.518083095550537
Epoch 660, val loss: 0.883295476436615
Epoch 670, training loss: 0.6700339913368225 = 0.018551845103502274 + 0.1 * 6.514821529388428
Epoch 670, val loss: 0.8890646696090698
Epoch 680, training loss: 0.6703932881355286 = 0.017707830294966698 + 0.1 * 6.526854515075684
Epoch 680, val loss: 0.8947376012802124
Epoch 690, training loss: 0.6688478589057922 = 0.01692013256251812 + 0.1 * 6.519277095794678
Epoch 690, val loss: 0.900230884552002
Epoch 700, training loss: 0.6669678688049316 = 0.016187211498618126 + 0.1 * 6.507806777954102
Epoch 700, val loss: 0.9056428670883179
Epoch 710, training loss: 0.6706468462944031 = 0.015502145513892174 + 0.1 * 6.551446914672852
Epoch 710, val loss: 0.9109217524528503
Epoch 720, training loss: 0.6658732891082764 = 0.014863102696835995 + 0.1 * 6.510102272033691
Epoch 720, val loss: 0.9160318970680237
Epoch 730, training loss: 0.6651709079742432 = 0.014265339821577072 + 0.1 * 6.5090556144714355
Epoch 730, val loss: 0.9211058616638184
Epoch 740, training loss: 0.6632651686668396 = 0.01370379887521267 + 0.1 * 6.495613098144531
Epoch 740, val loss: 0.926019012928009
Epoch 750, training loss: 0.6638639569282532 = 0.013176619075238705 + 0.1 * 6.50687313079834
Epoch 750, val loss: 0.9309289455413818
Epoch 760, training loss: 0.6631843447685242 = 0.012680432759225368 + 0.1 * 6.505039215087891
Epoch 760, val loss: 0.9356490969657898
Epoch 770, training loss: 0.6613133549690247 = 0.012213526293635368 + 0.1 * 6.490998268127441
Epoch 770, val loss: 0.9403043985366821
Epoch 780, training loss: 0.660688579082489 = 0.01177448034286499 + 0.1 * 6.48914098739624
Epoch 780, val loss: 0.9448972940444946
Epoch 790, training loss: 0.6615322828292847 = 0.011358704417943954 + 0.1 * 6.501735687255859
Epoch 790, val loss: 0.9493357539176941
Epoch 800, training loss: 0.6600538492202759 = 0.010967063717544079 + 0.1 * 6.490867614746094
Epoch 800, val loss: 0.9537261724472046
Epoch 810, training loss: 0.6591501235961914 = 0.010595850646495819 + 0.1 * 6.4855427742004395
Epoch 810, val loss: 0.9580320119857788
Epoch 820, training loss: 0.6580488681793213 = 0.010245412588119507 + 0.1 * 6.478034973144531
Epoch 820, val loss: 0.9622167348861694
Epoch 830, training loss: 0.6571581959724426 = 0.009913651272654533 + 0.1 * 6.472445487976074
Epoch 830, val loss: 0.9663395881652832
Epoch 840, training loss: 0.656913161277771 = 0.009598671458661556 + 0.1 * 6.47314453125
Epoch 840, val loss: 0.9703766107559204
Epoch 850, training loss: 0.6553837656974792 = 0.009299403056502342 + 0.1 * 6.460843563079834
Epoch 850, val loss: 0.9743706583976746
Epoch 860, training loss: 0.6570793390274048 = 0.009014862589538097 + 0.1 * 6.480644702911377
Epoch 860, val loss: 0.9783110618591309
Epoch 870, training loss: 0.6545001268386841 = 0.008743970654904842 + 0.1 * 6.45756196975708
Epoch 870, val loss: 0.9820435643196106
Epoch 880, training loss: 0.6545930504798889 = 0.008487490937113762 + 0.1 * 6.461055755615234
Epoch 880, val loss: 0.98580402135849
Epoch 890, training loss: 0.6554040908813477 = 0.00824291817843914 + 0.1 * 6.471611976623535
Epoch 890, val loss: 0.9895341396331787
Epoch 900, training loss: 0.6537179946899414 = 0.008008957840502262 + 0.1 * 6.457090377807617
Epoch 900, val loss: 0.9931541085243225
Epoch 910, training loss: 0.6534625291824341 = 0.0077857584692537785 + 0.1 * 6.456768035888672
Epoch 910, val loss: 0.9967052340507507
Epoch 920, training loss: 0.6525385975837708 = 0.007572596427053213 + 0.1 * 6.449659824371338
Epoch 920, val loss: 1.000213861465454
Epoch 930, training loss: 0.6545525789260864 = 0.007368760649114847 + 0.1 * 6.471837997436523
Epoch 930, val loss: 1.0036237239837646
Epoch 940, training loss: 0.6521628499031067 = 0.007174370810389519 + 0.1 * 6.44988489151001
Epoch 940, val loss: 1.006984829902649
Epoch 950, training loss: 0.6514483094215393 = 0.0069888075813651085 + 0.1 * 6.444594860076904
Epoch 950, val loss: 1.010347604751587
Epoch 960, training loss: 0.6511690020561218 = 0.006810420658439398 + 0.1 * 6.4435858726501465
Epoch 960, val loss: 1.0136728286743164
Epoch 970, training loss: 0.6510387063026428 = 0.006639112252742052 + 0.1 * 6.443995475769043
Epoch 970, val loss: 1.0169017314910889
Epoch 980, training loss: 0.6507232785224915 = 0.006474641151726246 + 0.1 * 6.442486763000488
Epoch 980, val loss: 1.0200546979904175
Epoch 990, training loss: 0.6502383351325989 = 0.006317534018307924 + 0.1 * 6.439207553863525
Epoch 990, val loss: 1.023132562637329
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.825972557067871 = 1.9662855863571167 + 0.1 * 8.596869468688965
Epoch 0, val loss: 1.9632278680801392
Epoch 10, training loss: 2.8150010108947754 = 1.9553191661834717 + 0.1 * 8.596818923950195
Epoch 10, val loss: 1.9525305032730103
Epoch 20, training loss: 2.8015201091766357 = 1.9418692588806152 + 0.1 * 8.596508979797363
Epoch 20, val loss: 1.9388717412948608
Epoch 30, training loss: 2.7823691368103027 = 1.9229822158813477 + 0.1 * 8.593870162963867
Epoch 30, val loss: 1.9190300703048706
Epoch 40, training loss: 2.751919746398926 = 1.8946093320846558 + 0.1 * 8.573104858398438
Epoch 40, val loss: 1.8889676332473755
Epoch 50, training loss: 2.7028658390045166 = 1.8543061017990112 + 0.1 * 8.485597610473633
Epoch 50, val loss: 1.8478440046310425
Epoch 60, training loss: 2.6278138160705566 = 1.8098667860031128 + 0.1 * 8.179469108581543
Epoch 60, val loss: 1.806457757949829
Epoch 70, training loss: 2.5786616802215576 = 1.7719987630844116 + 0.1 * 8.066629409790039
Epoch 70, val loss: 1.7755202054977417
Epoch 80, training loss: 2.5153863430023193 = 1.732784390449524 + 0.1 * 7.826018810272217
Epoch 80, val loss: 1.742307186126709
Epoch 90, training loss: 2.433070182800293 = 1.6826283931732178 + 0.1 * 7.504417419433594
Epoch 90, val loss: 1.6980193853378296
Epoch 100, training loss: 2.3485448360443115 = 1.6186991930007935 + 0.1 * 7.298457145690918
Epoch 100, val loss: 1.641856074333191
Epoch 110, training loss: 2.2585320472717285 = 1.5399222373962402 + 0.1 * 7.186099052429199
Epoch 110, val loss: 1.5737158060073853
Epoch 120, training loss: 2.1644446849823 = 1.4534767866134644 + 0.1 * 7.109679222106934
Epoch 120, val loss: 1.5007117986679077
Epoch 130, training loss: 2.073016405105591 = 1.367026925086975 + 0.1 * 7.059894561767578
Epoch 130, val loss: 1.4310940504074097
Epoch 140, training loss: 1.9847347736358643 = 1.2816493511199951 + 0.1 * 7.03085470199585
Epoch 140, val loss: 1.3638979196548462
Epoch 150, training loss: 1.8972896337509155 = 1.1966148614883423 + 0.1 * 7.006747722625732
Epoch 150, val loss: 1.2971725463867188
Epoch 160, training loss: 1.8116567134857178 = 1.1131213903427124 + 0.1 * 6.985352993011475
Epoch 160, val loss: 1.2313263416290283
Epoch 170, training loss: 1.7307205200195312 = 1.0338528156280518 + 0.1 * 6.9686760902404785
Epoch 170, val loss: 1.169951319694519
Epoch 180, training loss: 1.656773328781128 = 0.9607166051864624 + 0.1 * 6.960566520690918
Epoch 180, val loss: 1.1143083572387695
Epoch 190, training loss: 1.5883315801620483 = 0.8930894732475281 + 0.1 * 6.952420711517334
Epoch 190, val loss: 1.0648303031921387
Epoch 200, training loss: 1.523146390914917 = 0.8283005952835083 + 0.1 * 6.948458194732666
Epoch 200, val loss: 1.019018292427063
Epoch 210, training loss: 1.4590966701507568 = 0.7645124793052673 + 0.1 * 6.9458417892456055
Epoch 210, val loss: 0.9748614430427551
Epoch 220, training loss: 1.3948862552642822 = 0.7004629373550415 + 0.1 * 6.944233417510986
Epoch 220, val loss: 0.930850625038147
Epoch 230, training loss: 1.3296687602996826 = 0.6353248357772827 + 0.1 * 6.943439483642578
Epoch 230, val loss: 0.8854600787162781
Epoch 240, training loss: 1.2635645866394043 = 0.5693258047103882 + 0.1 * 6.942387104034424
Epoch 240, val loss: 0.8391929268836975
Epoch 250, training loss: 1.1983003616333008 = 0.5041487812995911 + 0.1 * 6.941515922546387
Epoch 250, val loss: 0.7933767437934875
Epoch 260, training loss: 1.1365976333618164 = 0.4424956142902374 + 0.1 * 6.941019535064697
Epoch 260, val loss: 0.7506589293479919
Epoch 270, training loss: 1.0799407958984375 = 0.38618358969688416 + 0.1 * 6.937572002410889
Epoch 270, val loss: 0.7130371928215027
Epoch 280, training loss: 1.0296276807785034 = 0.3359401524066925 + 0.1 * 6.936875343322754
Epoch 280, val loss: 0.6815840005874634
Epoch 290, training loss: 0.985063910484314 = 0.291865736246109 + 0.1 * 6.931981086730957
Epoch 290, val loss: 0.6568466424942017
Epoch 300, training loss: 0.9462435245513916 = 0.2533727288246155 + 0.1 * 6.928708076477051
Epoch 300, val loss: 0.6376307606697083
Epoch 310, training loss: 0.9125400185585022 = 0.21991050243377686 + 0.1 * 6.926295280456543
Epoch 310, val loss: 0.6231223344802856
Epoch 320, training loss: 0.8830559849739075 = 0.19105561077594757 + 0.1 * 6.920003890991211
Epoch 320, val loss: 0.6123604774475098
Epoch 330, training loss: 0.8583579063415527 = 0.16625410318374634 + 0.1 * 6.921037673950195
Epoch 330, val loss: 0.6047558784484863
Epoch 340, training loss: 0.8361454606056213 = 0.14516226947307587 + 0.1 * 6.909831523895264
Epoch 340, val loss: 0.5999298691749573
Epoch 350, training loss: 0.8170347809791565 = 0.1271997094154358 + 0.1 * 6.898350715637207
Epoch 350, val loss: 0.5972117185592651
Epoch 360, training loss: 0.801007866859436 = 0.11187805980443954 + 0.1 * 6.891298294067383
Epoch 360, val loss: 0.5964204668998718
Epoch 370, training loss: 0.7865037322044373 = 0.09884466230869293 + 0.1 * 6.876590728759766
Epoch 370, val loss: 0.5974024534225464
Epoch 380, training loss: 0.7742606401443481 = 0.08773607015609741 + 0.1 * 6.865245819091797
Epoch 380, val loss: 0.5997921824455261
Epoch 390, training loss: 0.7636943459510803 = 0.07824959605932236 + 0.1 * 6.854447841644287
Epoch 390, val loss: 0.6032999157905579
Epoch 400, training loss: 0.7541986107826233 = 0.07009715586900711 + 0.1 * 6.841014385223389
Epoch 400, val loss: 0.6078699827194214
Epoch 410, training loss: 0.7456172108650208 = 0.06305424869060516 + 0.1 * 6.825629234313965
Epoch 410, val loss: 0.6131793260574341
Epoch 420, training loss: 0.7380931377410889 = 0.056945860385894775 + 0.1 * 6.8114728927612305
Epoch 420, val loss: 0.6191421151161194
Epoch 430, training loss: 0.7320083379745483 = 0.05167744308710098 + 0.1 * 6.803308486938477
Epoch 430, val loss: 0.6256291270256042
Epoch 440, training loss: 0.7264293432235718 = 0.0470728762447834 + 0.1 * 6.793564319610596
Epoch 440, val loss: 0.632232129573822
Epoch 450, training loss: 0.7220156788825989 = 0.04302273318171501 + 0.1 * 6.789929389953613
Epoch 450, val loss: 0.6391699314117432
Epoch 460, training loss: 0.7175437211990356 = 0.03947758674621582 + 0.1 * 6.780661106109619
Epoch 460, val loss: 0.6462714076042175
Epoch 470, training loss: 0.7126684784889221 = 0.036339420825242996 + 0.1 * 6.763290882110596
Epoch 470, val loss: 0.6533038020133972
Epoch 480, training loss: 0.7089945673942566 = 0.03354193642735481 + 0.1 * 6.754526615142822
Epoch 480, val loss: 0.6605300307273865
Epoch 490, training loss: 0.7057342529296875 = 0.031033098697662354 + 0.1 * 6.747011184692383
Epoch 490, val loss: 0.6677328944206238
Epoch 500, training loss: 0.7035334706306458 = 0.02878730744123459 + 0.1 * 6.747461318969727
Epoch 500, val loss: 0.6749292612075806
Epoch 510, training loss: 0.700660228729248 = 0.026779241859912872 + 0.1 * 6.738809585571289
Epoch 510, val loss: 0.6820259094238281
Epoch 520, training loss: 0.6977161765098572 = 0.024968866258859634 + 0.1 * 6.72747278213501
Epoch 520, val loss: 0.6889944076538086
Epoch 530, training loss: 0.6957584619522095 = 0.023332707583904266 + 0.1 * 6.724257469177246
Epoch 530, val loss: 0.695893406867981
Epoch 540, training loss: 0.6944481730461121 = 0.021863924339413643 + 0.1 * 6.725841999053955
Epoch 540, val loss: 0.7026766538619995
Epoch 550, training loss: 0.6918736696243286 = 0.020533697679638863 + 0.1 * 6.713399887084961
Epoch 550, val loss: 0.7092366814613342
Epoch 560, training loss: 0.6895886659622192 = 0.01931917294859886 + 0.1 * 6.702694892883301
Epoch 560, val loss: 0.7156848907470703
Epoch 570, training loss: 0.6898900270462036 = 0.018207738175988197 + 0.1 * 6.716822624206543
Epoch 570, val loss: 0.7220845818519592
Epoch 580, training loss: 0.6864035725593567 = 0.017193971201777458 + 0.1 * 6.692095756530762
Epoch 580, val loss: 0.7283027172088623
Epoch 590, training loss: 0.6860110759735107 = 0.016266463324427605 + 0.1 * 6.697445869445801
Epoch 590, val loss: 0.7343733310699463
Epoch 600, training loss: 0.6838166117668152 = 0.015414729714393616 + 0.1 * 6.684018611907959
Epoch 600, val loss: 0.7403351068496704
Epoch 610, training loss: 0.684820294380188 = 0.014630265533924103 + 0.1 * 6.701900005340576
Epoch 610, val loss: 0.7462105751037598
Epoch 620, training loss: 0.6823076605796814 = 0.013907724991440773 + 0.1 * 6.683999061584473
Epoch 620, val loss: 0.7518893480300903
Epoch 630, training loss: 0.680604875087738 = 0.013241804204881191 + 0.1 * 6.673630237579346
Epoch 630, val loss: 0.7574213147163391
Epoch 640, training loss: 0.6798825263977051 = 0.012623891234397888 + 0.1 * 6.672585964202881
Epoch 640, val loss: 0.7628331184387207
Epoch 650, training loss: 0.6792185306549072 = 0.012053380720317364 + 0.1 * 6.671651363372803
Epoch 650, val loss: 0.7681373953819275
Epoch 660, training loss: 0.677947998046875 = 0.011521965265274048 + 0.1 * 6.664259910583496
Epoch 660, val loss: 0.7733374834060669
Epoch 670, training loss: 0.6808443069458008 = 0.011024056933820248 + 0.1 * 6.698202610015869
Epoch 670, val loss: 0.7784309983253479
Epoch 680, training loss: 0.6772633194923401 = 0.010565895587205887 + 0.1 * 6.6669745445251465
Epoch 680, val loss: 0.7833420038223267
Epoch 690, training loss: 0.6752700209617615 = 0.010138751938939095 + 0.1 * 6.651312351226807
Epoch 690, val loss: 0.7881243228912354
Epoch 700, training loss: 0.6755697727203369 = 0.00973548460751772 + 0.1 * 6.6583428382873535
Epoch 700, val loss: 0.7928284406661987
Epoch 710, training loss: 0.6752731204032898 = 0.009359482675790787 + 0.1 * 6.6591362953186035
Epoch 710, val loss: 0.7974052429199219
Epoch 720, training loss: 0.6735608577728271 = 0.009007026441395283 + 0.1 * 6.645537853240967
Epoch 720, val loss: 0.8019263744354248
Epoch 730, training loss: 0.6729308366775513 = 0.008676345460116863 + 0.1 * 6.642544746398926
Epoch 730, val loss: 0.8062820434570312
Epoch 740, training loss: 0.6731133460998535 = 0.008363572880625725 + 0.1 * 6.64749813079834
Epoch 740, val loss: 0.8105380535125732
Epoch 750, training loss: 0.6712738275527954 = 0.008070460520684719 + 0.1 * 6.632033348083496
Epoch 750, val loss: 0.8147658109664917
Epoch 760, training loss: 0.672658383846283 = 0.007793019525706768 + 0.1 * 6.648653507232666
Epoch 760, val loss: 0.8188630938529968
Epoch 770, training loss: 0.6722564697265625 = 0.007531707175076008 + 0.1 * 6.647247314453125
Epoch 770, val loss: 0.8228533864021301
Epoch 780, training loss: 0.6700394749641418 = 0.007286889478564262 + 0.1 * 6.627525806427002
Epoch 780, val loss: 0.8267641663551331
Epoch 790, training loss: 0.6694256067276001 = 0.0070548648945987225 + 0.1 * 6.623707294464111
Epoch 790, val loss: 0.8305399417877197
Epoch 800, training loss: 0.6691122651100159 = 0.006833988707512617 + 0.1 * 6.6227827072143555
Epoch 800, val loss: 0.8342359662055969
Epoch 810, training loss: 0.6675279140472412 = 0.006625313777476549 + 0.1 * 6.609025955200195
Epoch 810, val loss: 0.8378955721855164
Epoch 820, training loss: 0.6689448356628418 = 0.006427632179111242 + 0.1 * 6.625171661376953
Epoch 820, val loss: 0.8414995074272156
Epoch 830, training loss: 0.6664640307426453 = 0.00623940397053957 + 0.1 * 6.602246284484863
Epoch 830, val loss: 0.844982922077179
Epoch 840, training loss: 0.6663833260536194 = 0.006060937885195017 + 0.1 * 6.6032233238220215
Epoch 840, val loss: 0.8484001159667969
Epoch 850, training loss: 0.6670611500740051 = 0.005890824366360903 + 0.1 * 6.611702919006348
Epoch 850, val loss: 0.8517456650733948
Epoch 860, training loss: 0.6654102206230164 = 0.005729134194552898 + 0.1 * 6.596810817718506
Epoch 860, val loss: 0.8550834655761719
Epoch 870, training loss: 0.6661625504493713 = 0.005574819166213274 + 0.1 * 6.605876922607422
Epoch 870, val loss: 0.8583401441574097
Epoch 880, training loss: 0.6653605699539185 = 0.005427617579698563 + 0.1 * 6.599329471588135
Epoch 880, val loss: 0.8615162968635559
Epoch 890, training loss: 0.6636301279067993 = 0.005287800915539265 + 0.1 * 6.583423137664795
Epoch 890, val loss: 0.8646759390830994
Epoch 900, training loss: 0.6631671190261841 = 0.00515462551265955 + 0.1 * 6.580124855041504
Epoch 900, val loss: 0.8677160739898682
Epoch 910, training loss: 0.6628561615943909 = 0.0050266580656170845 + 0.1 * 6.5782952308654785
Epoch 910, val loss: 0.8707367181777954
Epoch 920, training loss: 0.664520263671875 = 0.004903832450509071 + 0.1 * 6.596163749694824
Epoch 920, val loss: 0.8737401366233826
Epoch 930, training loss: 0.662335991859436 = 0.004786556586623192 + 0.1 * 6.575494289398193
Epoch 930, val loss: 0.8766956329345703
Epoch 940, training loss: 0.6649606227874756 = 0.004674011841416359 + 0.1 * 6.602866172790527
Epoch 940, val loss: 0.8795841932296753
Epoch 950, training loss: 0.6610046029090881 = 0.004566647578030825 + 0.1 * 6.5643792152404785
Epoch 950, val loss: 0.8824210166931152
Epoch 960, training loss: 0.6624191999435425 = 0.004463537596166134 + 0.1 * 6.579556465148926
Epoch 960, val loss: 0.88519686460495
Epoch 970, training loss: 0.6608770489692688 = 0.004365409258753061 + 0.1 * 6.5651164054870605
Epoch 970, val loss: 0.8879018425941467
Epoch 980, training loss: 0.6598308086395264 = 0.004270434379577637 + 0.1 * 6.555603504180908
Epoch 980, val loss: 0.8905601501464844
Epoch 990, training loss: 0.6616402864456177 = 0.004178459290415049 + 0.1 * 6.574618339538574
Epoch 990, val loss: 0.893196165561676
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8334211913547708
The final CL Acc:0.83457, 0.00175, The final GNN Acc:0.83694, 0.00252
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11654])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10628])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8085758686065674 = 1.9488935470581055 + 0.1 * 8.596822738647461
Epoch 0, val loss: 1.9524221420288086
Epoch 10, training loss: 2.7987866401672363 = 1.9391111135482788 + 0.1 * 8.596755027770996
Epoch 10, val loss: 1.9423197507858276
Epoch 20, training loss: 2.787403106689453 = 1.927781105041504 + 0.1 * 8.596220970153809
Epoch 20, val loss: 1.9303934574127197
Epoch 30, training loss: 2.7714922428131104 = 1.9123499393463135 + 0.1 * 8.591422080993652
Epoch 30, val loss: 1.914271354675293
Epoch 40, training loss: 2.745028257369995 = 1.889774203300476 + 0.1 * 8.55254077911377
Epoch 40, val loss: 1.8912560939788818
Epoch 50, training loss: 2.690938711166382 = 1.858510971069336 + 0.1 * 8.3242769241333
Epoch 50, val loss: 1.861043095588684
Epoch 60, training loss: 2.6309967041015625 = 1.823281168937683 + 0.1 * 8.077154159545898
Epoch 60, val loss: 1.8290445804595947
Epoch 70, training loss: 2.548790454864502 = 1.7903200387954712 + 0.1 * 7.58470344543457
Epoch 70, val loss: 1.7997573614120483
Epoch 80, training loss: 2.481858730316162 = 1.7593120336532593 + 0.1 * 7.225466728210449
Epoch 80, val loss: 1.7726402282714844
Epoch 90, training loss: 2.432967185974121 = 1.7235229015350342 + 0.1 * 7.0944414138793945
Epoch 90, val loss: 1.7409532070159912
Epoch 100, training loss: 2.3789310455322266 = 1.6765451431274414 + 0.1 * 7.023858070373535
Epoch 100, val loss: 1.6980220079421997
Epoch 110, training loss: 2.3120384216308594 = 1.6146636009216309 + 0.1 * 6.9737467765808105
Epoch 110, val loss: 1.6428542137145996
Epoch 120, training loss: 2.230503559112549 = 1.537030577659607 + 0.1 * 6.934728622436523
Epoch 120, val loss: 1.5766916275024414
Epoch 130, training loss: 2.1371734142303467 = 1.4464914798736572 + 0.1 * 6.906818389892578
Epoch 130, val loss: 1.500388741493225
Epoch 140, training loss: 2.0365445613861084 = 1.3479938507080078 + 0.1 * 6.885507106781006
Epoch 140, val loss: 1.4190095663070679
Epoch 150, training loss: 1.9342150688171387 = 1.2470101118087769 + 0.1 * 6.872049808502197
Epoch 150, val loss: 1.3377928733825684
Epoch 160, training loss: 1.8334386348724365 = 1.1476045846939087 + 0.1 * 6.858339786529541
Epoch 160, val loss: 1.2598227262496948
Epoch 170, training loss: 1.7378935813903809 = 1.052350640296936 + 0.1 * 6.855429172515869
Epoch 170, val loss: 1.1863601207733154
Epoch 180, training loss: 1.6489713191986084 = 0.9650671482086182 + 0.1 * 6.839041709899902
Epoch 180, val loss: 1.1201789379119873
Epoch 190, training loss: 1.5680668354034424 = 0.8850883841514587 + 0.1 * 6.829784393310547
Epoch 190, val loss: 1.0606706142425537
Epoch 200, training loss: 1.4944758415222168 = 0.8128738403320312 + 0.1 * 6.816019535064697
Epoch 200, val loss: 1.0089707374572754
Epoch 210, training loss: 1.4278228282928467 = 0.7469073534011841 + 0.1 * 6.809154510498047
Epoch 210, val loss: 0.9641464948654175
Epoch 220, training loss: 1.3662950992584229 = 0.6869826316833496 + 0.1 * 6.793124198913574
Epoch 220, val loss: 0.9266600012779236
Epoch 230, training loss: 1.310188889503479 = 0.63236004114151 + 0.1 * 6.7782883644104
Epoch 230, val loss: 0.8960077166557312
Epoch 240, training loss: 1.260160207748413 = 0.5825505256652832 + 0.1 * 6.776095867156982
Epoch 240, val loss: 0.8717150688171387
Epoch 250, training loss: 1.212262749671936 = 0.5369901061058044 + 0.1 * 6.752726078033447
Epoch 250, val loss: 0.8531628847122192
Epoch 260, training loss: 1.169676423072815 = 0.49464553594589233 + 0.1 * 6.750308513641357
Epoch 260, val loss: 0.8387002348899841
Epoch 270, training loss: 1.1285576820373535 = 0.4553758203983307 + 0.1 * 6.731818675994873
Epoch 270, val loss: 0.8281092047691345
Epoch 280, training loss: 1.0909342765808105 = 0.4186131954193115 + 0.1 * 6.723211288452148
Epoch 280, val loss: 0.8206075429916382
Epoch 290, training loss: 1.0547759532928467 = 0.38419538736343384 + 0.1 * 6.705806255340576
Epoch 290, val loss: 0.8159270882606506
Epoch 300, training loss: 1.023179054260254 = 0.35208606719970703 + 0.1 * 6.710930347442627
Epoch 300, val loss: 0.8138605952262878
Epoch 310, training loss: 0.9916337728500366 = 0.3223631680011749 + 0.1 * 6.6927056312561035
Epoch 310, val loss: 0.814542293548584
Epoch 320, training loss: 0.9633949995040894 = 0.2947027385234833 + 0.1 * 6.686923027038574
Epoch 320, val loss: 0.8171991109848022
Epoch 330, training loss: 0.9375715255737305 = 0.26914864778518677 + 0.1 * 6.684228420257568
Epoch 330, val loss: 0.8218494057655334
Epoch 340, training loss: 0.9125285148620605 = 0.24558882415294647 + 0.1 * 6.669396877288818
Epoch 340, val loss: 0.8284245729446411
Epoch 350, training loss: 0.8910714387893677 = 0.22389978170394897 + 0.1 * 6.671716213226318
Epoch 350, val loss: 0.8365071415901184
Epoch 360, training loss: 0.8695144653320312 = 0.204106405377388 + 0.1 * 6.654080390930176
Epoch 360, val loss: 0.846185564994812
Epoch 370, training loss: 0.8508561849594116 = 0.18598003685474396 + 0.1 * 6.64876127243042
Epoch 370, val loss: 0.8570802807807922
Epoch 380, training loss: 0.833210289478302 = 0.1694268137216568 + 0.1 * 6.6378350257873535
Epoch 380, val loss: 0.8690369725227356
Epoch 390, training loss: 0.8175366520881653 = 0.15433645248413086 + 0.1 * 6.632001876831055
Epoch 390, val loss: 0.8819073438644409
Epoch 400, training loss: 0.8043064475059509 = 0.14059144258499146 + 0.1 * 6.637149810791016
Epoch 400, val loss: 0.8956119418144226
Epoch 410, training loss: 0.7908157110214233 = 0.1281248927116394 + 0.1 * 6.626908302307129
Epoch 410, val loss: 0.9097901582717896
Epoch 420, training loss: 0.77904212474823 = 0.11687010526657104 + 0.1 * 6.621719837188721
Epoch 420, val loss: 0.9241290092468262
Epoch 430, training loss: 0.7681131362915039 = 0.10675149410963058 + 0.1 * 6.613616466522217
Epoch 430, val loss: 0.9390376210212708
Epoch 440, training loss: 0.7579352855682373 = 0.097608283162117 + 0.1 * 6.603270053863525
Epoch 440, val loss: 0.9540882110595703
Epoch 450, training loss: 0.7495052814483643 = 0.08933771401643753 + 0.1 * 6.601675510406494
Epoch 450, val loss: 0.9692868590354919
Epoch 460, training loss: 0.742074728012085 = 0.08188454061746597 + 0.1 * 6.601901531219482
Epoch 460, val loss: 0.9844409823417664
Epoch 470, training loss: 0.7352324724197388 = 0.07517596334218979 + 0.1 * 6.600565433502197
Epoch 470, val loss: 0.9995715022087097
Epoch 480, training loss: 0.7284356355667114 = 0.06914065778255463 + 0.1 * 6.592949390411377
Epoch 480, val loss: 1.01456880569458
Epoch 490, training loss: 0.7223196029663086 = 0.06369795650243759 + 0.1 * 6.586216449737549
Epoch 490, val loss: 1.029354453086853
Epoch 500, training loss: 0.7163594961166382 = 0.05879371613264084 + 0.1 * 6.575657367706299
Epoch 500, val loss: 1.0439573526382446
Epoch 510, training loss: 0.7112487554550171 = 0.05436297506093979 + 0.1 * 6.568857669830322
Epoch 510, val loss: 1.0582950115203857
Epoch 520, training loss: 0.7078174352645874 = 0.05035766214132309 + 0.1 * 6.5745978355407715
Epoch 520, val loss: 1.0721971988677979
Epoch 530, training loss: 0.7038324475288391 = 0.04674602672457695 + 0.1 * 6.570864200592041
Epoch 530, val loss: 1.0859328508377075
Epoch 540, training loss: 0.6998779773712158 = 0.043471649289131165 + 0.1 * 6.564063549041748
Epoch 540, val loss: 1.0992767810821533
Epoch 550, training loss: 0.6966304779052734 = 0.040505681186914444 + 0.1 * 6.561247825622559
Epoch 550, val loss: 1.112255573272705
Epoch 560, training loss: 0.6941614151000977 = 0.0378127247095108 + 0.1 * 6.5634870529174805
Epoch 560, val loss: 1.124945878982544
Epoch 570, training loss: 0.6903247833251953 = 0.03536694124341011 + 0.1 * 6.5495781898498535
Epoch 570, val loss: 1.137237310409546
Epoch 580, training loss: 0.688692033290863 = 0.033135153353214264 + 0.1 * 6.555568695068359
Epoch 580, val loss: 1.1492470502853394
Epoch 590, training loss: 0.6858187913894653 = 0.03110140934586525 + 0.1 * 6.547173976898193
Epoch 590, val loss: 1.1608136892318726
Epoch 600, training loss: 0.6832755208015442 = 0.02924482151865959 + 0.1 * 6.540306568145752
Epoch 600, val loss: 1.1721993684768677
Epoch 610, training loss: 0.6809849739074707 = 0.02754206582903862 + 0.1 * 6.53442907333374
Epoch 610, val loss: 1.1832789182662964
Epoch 620, training loss: 0.6789249777793884 = 0.025977110490202904 + 0.1 * 6.529478549957275
Epoch 620, val loss: 1.1940306425094604
Epoch 630, training loss: 0.6787548661231995 = 0.024537302553653717 + 0.1 * 6.54217529296875
Epoch 630, val loss: 1.2043612003326416
Epoch 640, training loss: 0.6754056215286255 = 0.02321491576731205 + 0.1 * 6.521906852722168
Epoch 640, val loss: 1.2144938707351685
Epoch 650, training loss: 0.6769771575927734 = 0.021996092051267624 + 0.1 * 6.549810409545898
Epoch 650, val loss: 1.22419273853302
Epoch 660, training loss: 0.6725959181785583 = 0.020873447880148888 + 0.1 * 6.5172247886657715
Epoch 660, val loss: 1.2337452173233032
Epoch 670, training loss: 0.6713489890098572 = 0.019834747537970543 + 0.1 * 6.515142440795898
Epoch 670, val loss: 1.243000864982605
Epoch 680, training loss: 0.670559287071228 = 0.018871264532208443 + 0.1 * 6.516880035400391
Epoch 680, val loss: 1.2519351243972778
Epoch 690, training loss: 0.6691731214523315 = 0.017978128045797348 + 0.1 * 6.5119500160217285
Epoch 690, val loss: 1.2606114149093628
Epoch 700, training loss: 0.6673712134361267 = 0.01714865118265152 + 0.1 * 6.502225399017334
Epoch 700, val loss: 1.2691916227340698
Epoch 710, training loss: 0.6675717234611511 = 0.016374466940760612 + 0.1 * 6.511972427368164
Epoch 710, val loss: 1.2775413990020752
Epoch 720, training loss: 0.6659274697303772 = 0.015653718262910843 + 0.1 * 6.502737522125244
Epoch 720, val loss: 1.2853822708129883
Epoch 730, training loss: 0.664774477481842 = 0.014981605112552643 + 0.1 * 6.497928619384766
Epoch 730, val loss: 1.2933632135391235
Epoch 740, training loss: 0.6646591424942017 = 0.014352967031300068 + 0.1 * 6.503061771392822
Epoch 740, val loss: 1.300931453704834
Epoch 750, training loss: 0.6631394624710083 = 0.013764593750238419 + 0.1 * 6.493748188018799
Epoch 750, val loss: 1.3083287477493286
Epoch 760, training loss: 0.6639519929885864 = 0.013213947415351868 + 0.1 * 6.507380485534668
Epoch 760, val loss: 1.3156367540359497
Epoch 770, training loss: 0.6618700623512268 = 0.01269676722586155 + 0.1 * 6.491733074188232
Epoch 770, val loss: 1.322647213935852
Epoch 780, training loss: 0.6607025265693665 = 0.0122110890224576 + 0.1 * 6.484914302825928
Epoch 780, val loss: 1.3296042680740356
Epoch 790, training loss: 0.6607179641723633 = 0.011753551661968231 + 0.1 * 6.4896440505981445
Epoch 790, val loss: 1.3362276554107666
Epoch 800, training loss: 0.659419059753418 = 0.011323842220008373 + 0.1 * 6.48095178604126
Epoch 800, val loss: 1.3429099321365356
Epoch 810, training loss: 0.6607039570808411 = 0.01091795414686203 + 0.1 * 6.497859954833984
Epoch 810, val loss: 1.349203109741211
Epoch 820, training loss: 0.6587332487106323 = 0.010536440648138523 + 0.1 * 6.481967926025391
Epoch 820, val loss: 1.3554633855819702
Epoch 830, training loss: 0.6591781377792358 = 0.010175631381571293 + 0.1 * 6.490025043487549
Epoch 830, val loss: 1.3615713119506836
Epoch 840, training loss: 0.6569797992706299 = 0.00983456615358591 + 0.1 * 6.471452236175537
Epoch 840, val loss: 1.3675217628479004
Epoch 850, training loss: 0.6582372188568115 = 0.00951164960861206 + 0.1 * 6.487255573272705
Epoch 850, val loss: 1.3733954429626465
Epoch 860, training loss: 0.65630042552948 = 0.00920529942959547 + 0.1 * 6.470951080322266
Epoch 860, val loss: 1.3790661096572876
Epoch 870, training loss: 0.6564670205116272 = 0.008915104903280735 + 0.1 * 6.475518703460693
Epoch 870, val loss: 1.3846708536148071
Epoch 880, training loss: 0.6552597880363464 = 0.00863957405090332 + 0.1 * 6.466202259063721
Epoch 880, val loss: 1.3900320529937744
Epoch 890, training loss: 0.6553462743759155 = 0.008378353901207447 + 0.1 * 6.469679355621338
Epoch 890, val loss: 1.395390510559082
Epoch 900, training loss: 0.6545035243034363 = 0.008129444904625416 + 0.1 * 6.463740825653076
Epoch 900, val loss: 1.4004992246627808
Epoch 910, training loss: 0.6542496085166931 = 0.007893246598541737 + 0.1 * 6.463563442230225
Epoch 910, val loss: 1.4056518077850342
Epoch 920, training loss: 0.6548009514808655 = 0.007667859550565481 + 0.1 * 6.4713311195373535
Epoch 920, val loss: 1.4106249809265137
Epoch 930, training loss: 0.653752863407135 = 0.0074528539553284645 + 0.1 * 6.463000297546387
Epoch 930, val loss: 1.415422797203064
Epoch 940, training loss: 0.6528672575950623 = 0.007248114328831434 + 0.1 * 6.456191539764404
Epoch 940, val loss: 1.420268177986145
Epoch 950, training loss: 0.6525960564613342 = 0.0070524369366467 + 0.1 * 6.4554362297058105
Epoch 950, val loss: 1.4249905347824097
Epoch 960, training loss: 0.6533740758895874 = 0.006864921189844608 + 0.1 * 6.465091705322266
Epoch 960, val loss: 1.4294400215148926
Epoch 970, training loss: 0.6535321474075317 = 0.006686144042760134 + 0.1 * 6.4684600830078125
Epoch 970, val loss: 1.4338736534118652
Epoch 980, training loss: 0.6515563130378723 = 0.006515608634799719 + 0.1 * 6.450407028198242
Epoch 980, val loss: 1.4382274150848389
Epoch 990, training loss: 0.651397705078125 = 0.006352354772388935 + 0.1 * 6.450453281402588
Epoch 990, val loss: 1.4426512718200684
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.8183770179748535 = 1.9586917161941528 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.960139274597168
Epoch 10, training loss: 2.8078112602233887 = 1.948136329650879 + 0.1 * 8.596750259399414
Epoch 10, val loss: 1.9495489597320557
Epoch 20, training loss: 2.794616222381592 = 1.9350073337554932 + 0.1 * 8.596089363098145
Epoch 20, val loss: 1.9366086721420288
Epoch 30, training loss: 2.775665760040283 = 1.9167031049728394 + 0.1 * 8.589627265930176
Epoch 30, val loss: 1.9190250635147095
Epoch 40, training loss: 2.744311809539795 = 1.8898433446884155 + 0.1 * 8.544683456420898
Epoch 40, val loss: 1.8939584493637085
Epoch 50, training loss: 2.686350107192993 = 1.8542717695236206 + 0.1 * 8.320782661437988
Epoch 50, val loss: 1.86272394657135
Epoch 60, training loss: 2.6364359855651855 = 1.815662145614624 + 0.1 * 8.207737922668457
Epoch 60, val loss: 1.831159234046936
Epoch 70, training loss: 2.5832998752593994 = 1.7826906442642212 + 0.1 * 8.00609302520752
Epoch 70, val loss: 1.8028326034545898
Epoch 80, training loss: 2.506364107131958 = 1.7508065700531006 + 0.1 * 7.555574417114258
Epoch 80, val loss: 1.77053701877594
Epoch 90, training loss: 2.4317023754119873 = 1.7125252485275269 + 0.1 * 7.191771030426025
Epoch 90, val loss: 1.7352336645126343
Epoch 100, training loss: 2.3685340881347656 = 1.6631542444229126 + 0.1 * 7.053798198699951
Epoch 100, val loss: 1.6952590942382812
Epoch 110, training loss: 2.298875331878662 = 1.599746823310852 + 0.1 * 6.9912848472595215
Epoch 110, val loss: 1.6423815488815308
Epoch 120, training loss: 2.2239060401916504 = 1.5271483659744263 + 0.1 * 6.967575550079346
Epoch 120, val loss: 1.583187222480774
Epoch 130, training loss: 2.1480751037597656 = 1.4530830383300781 + 0.1 * 6.949920177459717
Epoch 130, val loss: 1.5235470533370972
Epoch 140, training loss: 2.075084686279297 = 1.3812830448150635 + 0.1 * 6.938016891479492
Epoch 140, val loss: 1.4654985666275024
Epoch 150, training loss: 2.003056049346924 = 1.3102548122406006 + 0.1 * 6.928012847900391
Epoch 150, val loss: 1.4089407920837402
Epoch 160, training loss: 1.9290225505828857 = 1.2372266054153442 + 0.1 * 6.917959213256836
Epoch 160, val loss: 1.3521684408187866
Epoch 170, training loss: 1.8516545295715332 = 1.1609501838684082 + 0.1 * 6.90704345703125
Epoch 170, val loss: 1.2941231727600098
Epoch 180, training loss: 1.7720980644226074 = 1.082230567932129 + 0.1 * 6.898675441741943
Epoch 180, val loss: 1.2355633974075317
Epoch 190, training loss: 1.6902689933776855 = 1.0019898414611816 + 0.1 * 6.882790565490723
Epoch 190, val loss: 1.176606297492981
Epoch 200, training loss: 1.6076111793518066 = 0.9208172559738159 + 0.1 * 6.867938995361328
Epoch 200, val loss: 1.116922378540039
Epoch 210, training loss: 1.5272495746612549 = 0.841344952583313 + 0.1 * 6.85904598236084
Epoch 210, val loss: 1.0584925413131714
Epoch 220, training loss: 1.4507567882537842 = 0.766948401927948 + 0.1 * 6.838083267211914
Epoch 220, val loss: 1.0048359632492065
Epoch 230, training loss: 1.3807806968688965 = 0.6989142894744873 + 0.1 * 6.818663597106934
Epoch 230, val loss: 0.9572206735610962
Epoch 240, training loss: 1.3192222118377686 = 0.6388350129127502 + 0.1 * 6.803872585296631
Epoch 240, val loss: 0.917987048625946
Epoch 250, training loss: 1.2655692100524902 = 0.5866213440895081 + 0.1 * 6.789477825164795
Epoch 250, val loss: 0.8875586986541748
Epoch 260, training loss: 1.2187660932540894 = 0.5405852794647217 + 0.1 * 6.781807899475098
Epoch 260, val loss: 0.8645821809768677
Epoch 270, training loss: 1.1753714084625244 = 0.4992940425872803 + 0.1 * 6.760773658752441
Epoch 270, val loss: 0.8476219773292542
Epoch 280, training loss: 1.1367626190185547 = 0.4611039459705353 + 0.1 * 6.756587028503418
Epoch 280, val loss: 0.8354047536849976
Epoch 290, training loss: 1.1001677513122559 = 0.42528092861175537 + 0.1 * 6.748868942260742
Epoch 290, val loss: 0.8265687823295593
Epoch 300, training loss: 1.0644112825393677 = 0.39114654064178467 + 0.1 * 6.73264741897583
Epoch 300, val loss: 0.8204426169395447
Epoch 310, training loss: 1.0319647789001465 = 0.358237624168396 + 0.1 * 6.737271308898926
Epoch 310, val loss: 0.8160858750343323
Epoch 320, training loss: 0.9986206293106079 = 0.32685551047325134 + 0.1 * 6.717650890350342
Epoch 320, val loss: 0.8136841654777527
Epoch 330, training loss: 0.9677168130874634 = 0.2968754768371582 + 0.1 * 6.708413124084473
Epoch 330, val loss: 0.8127853274345398
Epoch 340, training loss: 0.9396102428436279 = 0.26857444643974304 + 0.1 * 6.710358142852783
Epoch 340, val loss: 0.8130130767822266
Epoch 350, training loss: 0.9116538763046265 = 0.2423277646303177 + 0.1 * 6.69326114654541
Epoch 350, val loss: 0.8145647048950195
Epoch 360, training loss: 0.8873186707496643 = 0.2181645631790161 + 0.1 * 6.6915411949157715
Epoch 360, val loss: 0.8172619342803955
Epoch 370, training loss: 0.865342378616333 = 0.19625070691108704 + 0.1 * 6.690917015075684
Epoch 370, val loss: 0.8210077881813049
Epoch 380, training loss: 0.8447465896606445 = 0.1764838695526123 + 0.1 * 6.682627201080322
Epoch 380, val loss: 0.8258814811706543
Epoch 390, training loss: 0.8271207809448242 = 0.15873689949512482 + 0.1 * 6.683838844299316
Epoch 390, val loss: 0.8318718075752258
Epoch 400, training loss: 0.8100274205207825 = 0.14298087358474731 + 0.1 * 6.670465469360352
Epoch 400, val loss: 0.8389230966567993
Epoch 410, training loss: 0.794809103012085 = 0.12892086803913116 + 0.1 * 6.6588826179504395
Epoch 410, val loss: 0.8468471169471741
Epoch 420, training loss: 0.7852770090103149 = 0.11638731509447098 + 0.1 * 6.688897132873535
Epoch 420, val loss: 0.8555040955543518
Epoch 430, training loss: 0.7709055542945862 = 0.10532904416322708 + 0.1 * 6.655765056610107
Epoch 430, val loss: 0.8649725914001465
Epoch 440, training loss: 0.7596033215522766 = 0.09550748765468597 + 0.1 * 6.640958309173584
Epoch 440, val loss: 0.874789834022522
Epoch 450, training loss: 0.750917911529541 = 0.0867922231554985 + 0.1 * 6.641256809234619
Epoch 450, val loss: 0.8850482702255249
Epoch 460, training loss: 0.7433905601501465 = 0.07907822728157043 + 0.1 * 6.643123149871826
Epoch 460, val loss: 0.89566969871521
Epoch 470, training loss: 0.7349826097488403 = 0.07223477959632874 + 0.1 * 6.62747859954834
Epoch 470, val loss: 0.906387448310852
Epoch 480, training loss: 0.7287919521331787 = 0.06613902747631073 + 0.1 * 6.626528739929199
Epoch 480, val loss: 0.9172913432121277
Epoch 490, training loss: 0.7234166860580444 = 0.06071585789322853 + 0.1 * 6.627007961273193
Epoch 490, val loss: 0.9281688332557678
Epoch 500, training loss: 0.717051088809967 = 0.05588499456644058 + 0.1 * 6.611660480499268
Epoch 500, val loss: 0.9390053153038025
Epoch 510, training loss: 0.7135597467422485 = 0.051567062735557556 + 0.1 * 6.619926452636719
Epoch 510, val loss: 0.9496992230415344
Epoch 520, training loss: 0.7082934975624084 = 0.04770751670002937 + 0.1 * 6.605859756469727
Epoch 520, val loss: 0.9603511095046997
Epoch 530, training loss: 0.7037383317947388 = 0.04424175247550011 + 0.1 * 6.594965934753418
Epoch 530, val loss: 0.970858633518219
Epoch 540, training loss: 0.700840950012207 = 0.041125982999801636 + 0.1 * 6.597149848937988
Epoch 540, val loss: 0.9809793829917908
Epoch 550, training loss: 0.6969757676124573 = 0.03832678496837616 + 0.1 * 6.586489677429199
Epoch 550, val loss: 0.9912179708480835
Epoch 560, training loss: 0.6956349611282349 = 0.035793010145425797 + 0.1 * 6.598419189453125
Epoch 560, val loss: 1.0010828971862793
Epoch 570, training loss: 0.6921452879905701 = 0.03350250422954559 + 0.1 * 6.586427688598633
Epoch 570, val loss: 1.0107407569885254
Epoch 580, training loss: 0.6889142394065857 = 0.03142128139734268 + 0.1 * 6.574929237365723
Epoch 580, val loss: 1.0203105211257935
Epoch 590, training loss: 0.686026930809021 = 0.02952205389738083 + 0.1 * 6.565048694610596
Epoch 590, val loss: 1.0296194553375244
Epoch 600, training loss: 0.6843852400779724 = 0.027790095657110214 + 0.1 * 6.565951824188232
Epoch 600, val loss: 1.0385345220565796
Epoch 610, training loss: 0.6826273798942566 = 0.026216721162199974 + 0.1 * 6.564105987548828
Epoch 610, val loss: 1.04750657081604
Epoch 620, training loss: 0.6823238134384155 = 0.02477426454424858 + 0.1 * 6.575495719909668
Epoch 620, val loss: 1.0560295581817627
Epoch 630, training loss: 0.6796663999557495 = 0.023449059575796127 + 0.1 * 6.562173366546631
Epoch 630, val loss: 1.064515233039856
Epoch 640, training loss: 0.679095983505249 = 0.022230016067624092 + 0.1 * 6.56865930557251
Epoch 640, val loss: 1.072554349899292
Epoch 650, training loss: 0.6754738688468933 = 0.021109377965331078 + 0.1 * 6.543644428253174
Epoch 650, val loss: 1.080674171447754
Epoch 660, training loss: 0.6755269169807434 = 0.020072752609848976 + 0.1 * 6.55454158782959
Epoch 660, val loss: 1.0885323286056519
Epoch 670, training loss: 0.673475444316864 = 0.01911439746618271 + 0.1 * 6.543610572814941
Epoch 670, val loss: 1.0960594415664673
Epoch 680, training loss: 0.6727259159088135 = 0.018225401639938354 + 0.1 * 6.5450053215026855
Epoch 680, val loss: 1.1036608219146729
Epoch 690, training loss: 0.6723082661628723 = 0.017398981377482414 + 0.1 * 6.549093246459961
Epoch 690, val loss: 1.1107831001281738
Epoch 700, training loss: 0.6702395081520081 = 0.01663147658109665 + 0.1 * 6.5360798835754395
Epoch 700, val loss: 1.1180123090744019
Epoch 710, training loss: 0.6688063144683838 = 0.015915483236312866 + 0.1 * 6.528908729553223
Epoch 710, val loss: 1.1250067949295044
Epoch 720, training loss: 0.668350338935852 = 0.01524683739989996 + 0.1 * 6.53103494644165
Epoch 720, val loss: 1.1315538883209229
Epoch 730, training loss: 0.6670584678649902 = 0.014624294824898243 + 0.1 * 6.524341583251953
Epoch 730, val loss: 1.1383668184280396
Epoch 740, training loss: 0.6669666767120361 = 0.014039776287972927 + 0.1 * 6.529269218444824
Epoch 740, val loss: 1.1448744535446167
Epoch 750, training loss: 0.6654849648475647 = 0.013492001220583916 + 0.1 * 6.5199294090271
Epoch 750, val loss: 1.151052474975586
Epoch 760, training loss: 0.6646484732627869 = 0.012978537008166313 + 0.1 * 6.516699314117432
Epoch 760, val loss: 1.1574286222457886
Epoch 770, training loss: 0.6657719016075134 = 0.012495486997067928 + 0.1 * 6.532764434814453
Epoch 770, val loss: 1.163540244102478
Epoch 780, training loss: 0.6633050441741943 = 0.012040983885526657 + 0.1 * 6.512640476226807
Epoch 780, val loss: 1.169399619102478
Epoch 790, training loss: 0.6644227504730225 = 0.011612650007009506 + 0.1 * 6.528100967407227
Epoch 790, val loss: 1.1753175258636475
Epoch 800, training loss: 0.6622527837753296 = 0.011209340766072273 + 0.1 * 6.510434150695801
Epoch 800, val loss: 1.1810444593429565
Epoch 810, training loss: 0.6629327535629272 = 0.010827598161995411 + 0.1 * 6.521051406860352
Epoch 810, val loss: 1.186800479888916
Epoch 820, training loss: 0.6603907346725464 = 0.010467239655554295 + 0.1 * 6.499234676361084
Epoch 820, val loss: 1.1920063495635986
Epoch 830, training loss: 0.6595500111579895 = 0.01012694463133812 + 0.1 * 6.4942307472229
Epoch 830, val loss: 1.197625756263733
Epoch 840, training loss: 0.6597689986228943 = 0.009803448803722858 + 0.1 * 6.499655246734619
Epoch 840, val loss: 1.2028948068618774
Epoch 850, training loss: 0.6588377952575684 = 0.009496542625129223 + 0.1 * 6.493412017822266
Epoch 850, val loss: 1.2079904079437256
Epoch 860, training loss: 0.658667802810669 = 0.009205254726111889 + 0.1 * 6.494625091552734
Epoch 860, val loss: 1.2130122184753418
Epoch 870, training loss: 0.6576892137527466 = 0.008929028175771236 + 0.1 * 6.4876017570495605
Epoch 870, val loss: 1.218168020248413
Epoch 880, training loss: 0.6586901545524597 = 0.008665935136377811 + 0.1 * 6.500242233276367
Epoch 880, val loss: 1.22309148311615
Epoch 890, training loss: 0.6575088500976562 = 0.00841530505567789 + 0.1 * 6.490935802459717
Epoch 890, val loss: 1.2275902032852173
Epoch 900, training loss: 0.6574273705482483 = 0.008177774958312511 + 0.1 * 6.492496013641357
Epoch 900, val loss: 1.232566237449646
Epoch 910, training loss: 0.6556536555290222 = 0.00795032735913992 + 0.1 * 6.4770331382751465
Epoch 910, val loss: 1.2372410297393799
Epoch 920, training loss: 0.6569290161132812 = 0.0077327219769358635 + 0.1 * 6.491962432861328
Epoch 920, val loss: 1.2416728734970093
Epoch 930, training loss: 0.6555965542793274 = 0.0075253997929394245 + 0.1 * 6.480711460113525
Epoch 930, val loss: 1.2461037635803223
Epoch 940, training loss: 0.6557605862617493 = 0.007327580824494362 + 0.1 * 6.484330177307129
Epoch 940, val loss: 1.250488042831421
Epoch 950, training loss: 0.6545204520225525 = 0.007138415239751339 + 0.1 * 6.473820209503174
Epoch 950, val loss: 1.2548301219940186
Epoch 960, training loss: 0.6546885371208191 = 0.006957079283893108 + 0.1 * 6.477314472198486
Epoch 960, val loss: 1.2591407299041748
Epoch 970, training loss: 0.6532147526741028 = 0.0067831482738256454 + 0.1 * 6.464316368103027
Epoch 970, val loss: 1.2632914781570435
Epoch 980, training loss: 0.6548016667366028 = 0.0066162534058094025 + 0.1 * 6.48185396194458
Epoch 980, val loss: 1.2672785520553589
Epoch 990, training loss: 0.6531592607498169 = 0.006456879898905754 + 0.1 * 6.467023849487305
Epoch 990, val loss: 1.2712366580963135
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 2.799898862838745 = 1.940212607383728 + 0.1 * 8.59686279296875
Epoch 0, val loss: 1.9409446716308594
Epoch 10, training loss: 2.789475917816162 = 1.929797649383545 + 0.1 * 8.596781730651855
Epoch 10, val loss: 1.9315434694290161
Epoch 20, training loss: 2.7762575149536133 = 1.9166219234466553 + 0.1 * 8.596354484558105
Epoch 20, val loss: 1.9194051027297974
Epoch 30, training loss: 2.75736141204834 = 1.8980762958526611 + 0.1 * 8.592849731445312
Epoch 30, val loss: 1.9022103548049927
Epoch 40, training loss: 2.7284178733825684 = 1.8713523149490356 + 0.1 * 8.570656776428223
Epoch 40, val loss: 1.8779264688491821
Epoch 50, training loss: 2.6849429607391357 = 1.83656907081604 + 0.1 * 8.48373794555664
Epoch 50, val loss: 1.848117709159851
Epoch 60, training loss: 2.624722480773926 = 1.800877332687378 + 0.1 * 8.23845100402832
Epoch 60, val loss: 1.819296956062317
Epoch 70, training loss: 2.579439163208008 = 1.766649603843689 + 0.1 * 8.127896308898926
Epoch 70, val loss: 1.789203405380249
Epoch 80, training loss: 2.518982410430908 = 1.722582221031189 + 0.1 * 7.96400260925293
Epoch 80, val loss: 1.7477279901504517
Epoch 90, training loss: 2.445707321166992 = 1.6643120050430298 + 0.1 * 7.8139543533325195
Epoch 90, val loss: 1.696812391281128
Epoch 100, training loss: 2.3536064624786377 = 1.5915004014968872 + 0.1 * 7.621061325073242
Epoch 100, val loss: 1.6382496356964111
Epoch 110, training loss: 2.255788803100586 = 1.510212779045105 + 0.1 * 7.455759048461914
Epoch 110, val loss: 1.5711705684661865
Epoch 120, training loss: 2.1626033782958984 = 1.4269232749938965 + 0.1 * 7.356800079345703
Epoch 120, val loss: 1.506399393081665
Epoch 130, training loss: 2.072732925415039 = 1.3471355438232422 + 0.1 * 7.2559733390808105
Epoch 130, val loss: 1.4489905834197998
Epoch 140, training loss: 1.9883108139038086 = 1.268979549407959 + 0.1 * 7.19331169128418
Epoch 140, val loss: 1.393384337425232
Epoch 150, training loss: 1.9065184593200684 = 1.190865397453308 + 0.1 * 7.156529903411865
Epoch 150, val loss: 1.3392149209976196
Epoch 160, training loss: 1.828086018562317 = 1.1151355504989624 + 0.1 * 7.129504680633545
Epoch 160, val loss: 1.2878069877624512
Epoch 170, training loss: 1.7557313442230225 = 1.0451951026916504 + 0.1 * 7.1053619384765625
Epoch 170, val loss: 1.2415339946746826
Epoch 180, training loss: 1.6877342462539673 = 0.9807837605476379 + 0.1 * 7.069504737854004
Epoch 180, val loss: 1.1989970207214355
Epoch 190, training loss: 1.624558687210083 = 0.9207432270050049 + 0.1 * 7.0381550788879395
Epoch 190, val loss: 1.160149335861206
Epoch 200, training loss: 1.5646307468414307 = 0.8636449575424194 + 0.1 * 7.009857177734375
Epoch 200, val loss: 1.1246665716171265
Epoch 210, training loss: 1.5078792572021484 = 0.808295488357544 + 0.1 * 6.9958367347717285
Epoch 210, val loss: 1.0926008224487305
Epoch 220, training loss: 1.4538531303405762 = 0.7552726864814758 + 0.1 * 6.985804557800293
Epoch 220, val loss: 1.0651823282241821
Epoch 230, training loss: 1.402828335762024 = 0.7051440477371216 + 0.1 * 6.976842880249023
Epoch 230, val loss: 1.0431104898452759
Epoch 240, training loss: 1.354891300201416 = 0.6579941511154175 + 0.1 * 6.968972206115723
Epoch 240, val loss: 1.0268471240997314
Epoch 250, training loss: 1.310436725616455 = 0.6131942868232727 + 0.1 * 6.972424030303955
Epoch 250, val loss: 1.015032410621643
Epoch 260, training loss: 1.2662091255187988 = 0.5702555775642395 + 0.1 * 6.959534645080566
Epoch 260, val loss: 1.0064555406570435
Epoch 270, training loss: 1.2235900163650513 = 0.528160810470581 + 0.1 * 6.954291820526123
Epoch 270, val loss: 0.9993641972541809
Epoch 280, training loss: 1.1813381910324097 = 0.48643624782562256 + 0.1 * 6.949019432067871
Epoch 280, val loss: 0.9926005005836487
Epoch 290, training loss: 1.1395450830459595 = 0.4452066123485565 + 0.1 * 6.943385124206543
Epoch 290, val loss: 0.9858311414718628
Epoch 300, training loss: 1.0985808372497559 = 0.40483078360557556 + 0.1 * 6.9375
Epoch 300, val loss: 0.9793512225151062
Epoch 310, training loss: 1.0588592290878296 = 0.36576047539711 + 0.1 * 6.9309868812561035
Epoch 310, val loss: 0.9738848805427551
Epoch 320, training loss: 1.0208874940872192 = 0.3287755250930786 + 0.1 * 6.921119689941406
Epoch 320, val loss: 0.970346987247467
Epoch 330, training loss: 0.9859189391136169 = 0.29443466663360596 + 0.1 * 6.91484260559082
Epoch 330, val loss: 0.9695979356765747
Epoch 340, training loss: 0.953221321105957 = 0.26289084553718567 + 0.1 * 6.9033050537109375
Epoch 340, val loss: 0.9720829725265503
Epoch 350, training loss: 0.9242650866508484 = 0.2342376559972763 + 0.1 * 6.900274276733398
Epoch 350, val loss: 0.977857232093811
Epoch 360, training loss: 0.8971416354179382 = 0.20840288698673248 + 0.1 * 6.887387752532959
Epoch 360, val loss: 0.9862912893295288
Epoch 370, training loss: 0.8732560873031616 = 0.18507446348667145 + 0.1 * 6.881816387176514
Epoch 370, val loss: 0.9972744584083557
Epoch 380, training loss: 0.8519924283027649 = 0.16407263278961182 + 0.1 * 6.879197597503662
Epoch 380, val loss: 1.0100445747375488
Epoch 390, training loss: 0.8324694633483887 = 0.14528916776180267 + 0.1 * 6.871802806854248
Epoch 390, val loss: 1.0240528583526611
Epoch 400, training loss: 0.8155584931373596 = 0.12864287197589874 + 0.1 * 6.869156360626221
Epoch 400, val loss: 1.0389635562896729
Epoch 410, training loss: 0.8001182079315186 = 0.114003486931324 + 0.1 * 6.861146926879883
Epoch 410, val loss: 1.0543787479400635
Epoch 420, training loss: 0.7865981459617615 = 0.1011650413274765 + 0.1 * 6.854330539703369
Epoch 420, val loss: 1.0702072381973267
Epoch 430, training loss: 0.7778332233428955 = 0.08994326740503311 + 0.1 * 6.878899097442627
Epoch 430, val loss: 1.0863921642303467
Epoch 440, training loss: 0.7654680609703064 = 0.0802755281329155 + 0.1 * 6.851925373077393
Epoch 440, val loss: 1.1025044918060303
Epoch 450, training loss: 0.7557734251022339 = 0.07190009951591492 + 0.1 * 6.838732719421387
Epoch 450, val loss: 1.1185318231582642
Epoch 460, training loss: 0.7476715445518494 = 0.06460808962583542 + 0.1 * 6.830634593963623
Epoch 460, val loss: 1.13454008102417
Epoch 470, training loss: 0.7411412596702576 = 0.05824146792292595 + 0.1 * 6.828997611999512
Epoch 470, val loss: 1.1505215167999268
Epoch 480, training loss: 0.7356030344963074 = 0.05270536243915558 + 0.1 * 6.828976631164551
Epoch 480, val loss: 1.166083812713623
Epoch 490, training loss: 0.7293668389320374 = 0.04788578301668167 + 0.1 * 6.814810752868652
Epoch 490, val loss: 1.1810855865478516
Epoch 500, training loss: 0.7245472073554993 = 0.0436629094183445 + 0.1 * 6.80884313583374
Epoch 500, val loss: 1.196010708808899
Epoch 510, training loss: 0.7209762334823608 = 0.03994550183415413 + 0.1 * 6.810307502746582
Epoch 510, val loss: 1.210681438446045
Epoch 520, training loss: 0.7172921895980835 = 0.036674708127975464 + 0.1 * 6.8061747550964355
Epoch 520, val loss: 1.2248773574829102
Epoch 530, training loss: 0.7130860090255737 = 0.03378568962216377 + 0.1 * 6.793003082275391
Epoch 530, val loss: 1.2384705543518066
Epoch 540, training loss: 0.7102315425872803 = 0.03122561424970627 + 0.1 * 6.7900590896606445
Epoch 540, val loss: 1.2514965534210205
Epoch 550, training loss: 0.707084059715271 = 0.0289536714553833 + 0.1 * 6.781303882598877
Epoch 550, val loss: 1.2642698287963867
Epoch 560, training loss: 0.7049092054367065 = 0.02692137472331524 + 0.1 * 6.77987813949585
Epoch 560, val loss: 1.2768572568893433
Epoch 570, training loss: 0.7021439075469971 = 0.025101792067289352 + 0.1 * 6.770421028137207
Epoch 570, val loss: 1.2887338399887085
Epoch 580, training loss: 0.7002535462379456 = 0.02346847578883171 + 0.1 * 6.767850399017334
Epoch 580, val loss: 1.300188422203064
Epoch 590, training loss: 0.6986490488052368 = 0.021994782611727715 + 0.1 * 6.766542434692383
Epoch 590, val loss: 1.3113415241241455
Epoch 600, training loss: 0.6958593130111694 = 0.020663117989897728 + 0.1 * 6.751961708068848
Epoch 600, val loss: 1.3219791650772095
Epoch 610, training loss: 0.6938211917877197 = 0.019454091787338257 + 0.1 * 6.743670463562012
Epoch 610, val loss: 1.3322700262069702
Epoch 620, training loss: 0.6927855610847473 = 0.018355417996644974 + 0.1 * 6.7443013191223145
Epoch 620, val loss: 1.3423163890838623
Epoch 630, training loss: 0.690596342086792 = 0.017351286485791206 + 0.1 * 6.732450485229492
Epoch 630, val loss: 1.3520736694335938
Epoch 640, training loss: 0.6898953914642334 = 0.01643080823123455 + 0.1 * 6.734645843505859
Epoch 640, val loss: 1.3613545894622803
Epoch 650, training loss: 0.6886546611785889 = 0.01558740809559822 + 0.1 * 6.730672836303711
Epoch 650, val loss: 1.3705984354019165
Epoch 660, training loss: 0.6865846514701843 = 0.014813470654189587 + 0.1 * 6.717711448669434
Epoch 660, val loss: 1.3792438507080078
Epoch 670, training loss: 0.6857059001922607 = 0.014100631698966026 + 0.1 * 6.716053009033203
Epoch 670, val loss: 1.387860655784607
Epoch 680, training loss: 0.6862114667892456 = 0.01344076544046402 + 0.1 * 6.727706432342529
Epoch 680, val loss: 1.396066427230835
Epoch 690, training loss: 0.684316098690033 = 0.012830785475671291 + 0.1 * 6.714852809906006
Epoch 690, val loss: 1.4042176008224487
Epoch 700, training loss: 0.6825066208839417 = 0.012265689671039581 + 0.1 * 6.702409267425537
Epoch 700, val loss: 1.4119205474853516
Epoch 710, training loss: 0.6819724440574646 = 0.011739475652575493 + 0.1 * 6.702329158782959
Epoch 710, val loss: 1.4192126989364624
Epoch 720, training loss: 0.681778609752655 = 0.011251323856413364 + 0.1 * 6.705272674560547
Epoch 720, val loss: 1.426859736442566
Epoch 730, training loss: 0.6795167326927185 = 0.010795418173074722 + 0.1 * 6.687212944030762
Epoch 730, val loss: 1.4335970878601074
Epoch 740, training loss: 0.6802024841308594 = 0.010371198877692223 + 0.1 * 6.698312759399414
Epoch 740, val loss: 1.440761685371399
Epoch 750, training loss: 0.6782661080360413 = 0.009973114356398582 + 0.1 * 6.682929992675781
Epoch 750, val loss: 1.4473669528961182
Epoch 760, training loss: 0.6778026223182678 = 0.009601258672773838 + 0.1 * 6.682013988494873
Epoch 760, val loss: 1.4538321495056152
Epoch 770, training loss: 0.6794716119766235 = 0.009252391755580902 + 0.1 * 6.702192306518555
Epoch 770, val loss: 1.4602245092391968
Epoch 780, training loss: 0.6760178208351135 = 0.008924846537411213 + 0.1 * 6.670929431915283
Epoch 780, val loss: 1.4662083387374878
Epoch 790, training loss: 0.674636721611023 = 0.008618433959782124 + 0.1 * 6.660182952880859
Epoch 790, val loss: 1.472357988357544
Epoch 800, training loss: 0.6745615005493164 = 0.008328505791723728 + 0.1 * 6.662330150604248
Epoch 800, val loss: 1.478248953819275
Epoch 810, training loss: 0.6778728365898132 = 0.008055206388235092 + 0.1 * 6.69817590713501
Epoch 810, val loss: 1.4841022491455078
Epoch 820, training loss: 0.6745222210884094 = 0.007796827703714371 + 0.1 * 6.6672539710998535
Epoch 820, val loss: 1.4889168739318848
Epoch 830, training loss: 0.6724381446838379 = 0.007554987445473671 + 0.1 * 6.648831367492676
Epoch 830, val loss: 1.4949320554733276
Epoch 840, training loss: 0.6728720664978027 = 0.0073239910416305065 + 0.1 * 6.655480861663818
Epoch 840, val loss: 1.5000295639038086
Epoch 850, training loss: 0.6719806790351868 = 0.00710523035377264 + 0.1 * 6.648754596710205
Epoch 850, val loss: 1.5049223899841309
Epoch 860, training loss: 0.6704307198524475 = 0.006898736581206322 + 0.1 * 6.635319232940674
Epoch 860, val loss: 1.5104453563690186
Epoch 870, training loss: 0.6707977652549744 = 0.006701274309307337 + 0.1 * 6.640964508056641
Epoch 870, val loss: 1.5146757364273071
Epoch 880, training loss: 0.669625997543335 = 0.006514648906886578 + 0.1 * 6.631113052368164
Epoch 880, val loss: 1.519926905632019
Epoch 890, training loss: 0.6681209802627563 = 0.006336637772619724 + 0.1 * 6.6178436279296875
Epoch 890, val loss: 1.5246108770370483
Epoch 900, training loss: 0.6687485575675964 = 0.006166777107864618 + 0.1 * 6.625817775726318
Epoch 900, val loss: 1.5291701555252075
Epoch 910, training loss: 0.6673216223716736 = 0.006004582159221172 + 0.1 * 6.613170146942139
Epoch 910, val loss: 1.5336161851882935
Epoch 920, training loss: 0.6668184399604797 = 0.005850190296769142 + 0.1 * 6.609682083129883
Epoch 920, val loss: 1.5381766557693481
Epoch 930, training loss: 0.6668117046356201 = 0.005702548660337925 + 0.1 * 6.611091613769531
Epoch 930, val loss: 1.5423927307128906
Epoch 940, training loss: 0.6681721806526184 = 0.005561203230172396 + 0.1 * 6.6261091232299805
Epoch 940, val loss: 1.5465975999832153
Epoch 950, training loss: 0.6657427549362183 = 0.005426086485385895 + 0.1 * 6.603166580200195
Epoch 950, val loss: 1.5506449937820435
Epoch 960, training loss: 0.6643482446670532 = 0.005297154188156128 + 0.1 * 6.590511322021484
Epoch 960, val loss: 1.5550950765609741
Epoch 970, training loss: 0.6666762828826904 = 0.005172599107027054 + 0.1 * 6.615036487579346
Epoch 970, val loss: 1.5586473941802979
Epoch 980, training loss: 0.6647586822509766 = 0.00505382614210248 + 0.1 * 6.597048759460449
Epoch 980, val loss: 1.5624899864196777
Epoch 990, training loss: 0.6637962460517883 = 0.004940316081047058 + 0.1 * 6.588559150695801
Epoch 990, val loss: 1.5666495561599731
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8176067474960464
The final CL Acc:0.75062, 0.01552, The final GNN Acc:0.81550, 0.00336
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13190])
remove edge: torch.Size([2, 7914])
updated graph: torch.Size([2, 10548])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8141772747039795 = 1.9544925689697266 + 0.1 * 8.596847534179688
Epoch 0, val loss: 1.958537220954895
Epoch 10, training loss: 2.8038792610168457 = 1.9442009925842285 + 0.1 * 8.596781730651855
Epoch 10, val loss: 1.9477055072784424
Epoch 20, training loss: 2.791707992553711 = 1.9320684671401978 + 0.1 * 8.596395492553711
Epoch 20, val loss: 1.9347654581069946
Epoch 30, training loss: 2.7748329639434814 = 1.9154894351959229 + 0.1 * 8.59343433380127
Epoch 30, val loss: 1.9169195890426636
Epoch 40, training loss: 2.7486767768859863 = 1.8914543390274048 + 0.1 * 8.572223663330078
Epoch 40, val loss: 1.8910415172576904
Epoch 50, training loss: 2.7033419609069824 = 1.8573291301727295 + 0.1 * 8.460126876831055
Epoch 50, val loss: 1.85545015335083
Epoch 60, training loss: 2.628232717514038 = 1.8155373334884644 + 0.1 * 8.126953125
Epoch 60, val loss: 1.8148940801620483
Epoch 70, training loss: 2.555462598800659 = 1.7747248411178589 + 0.1 * 7.80737829208374
Epoch 70, val loss: 1.7788875102996826
Epoch 80, training loss: 2.475069046020508 = 1.7336699962615967 + 0.1 * 7.413991451263428
Epoch 80, val loss: 1.743361473083496
Epoch 90, training loss: 2.4074292182922363 = 1.6841388940811157 + 0.1 * 7.232903480529785
Epoch 90, val loss: 1.6991019248962402
Epoch 100, training loss: 2.335721015930176 = 1.6186037063598633 + 0.1 * 7.171173095703125
Epoch 100, val loss: 1.640130639076233
Epoch 110, training loss: 2.24782133102417 = 1.5340588092803955 + 0.1 * 7.137625694274902
Epoch 110, val loss: 1.5653984546661377
Epoch 120, training loss: 2.143592119216919 = 1.432310938835144 + 0.1 * 7.11281156539917
Epoch 120, val loss: 1.4780998229980469
Epoch 130, training loss: 2.029183864593506 = 1.3198398351669312 + 0.1 * 7.09343957901001
Epoch 130, val loss: 1.383771538734436
Epoch 140, training loss: 1.911904215812683 = 1.2046517133712769 + 0.1 * 7.0725250244140625
Epoch 140, val loss: 1.2898473739624023
Epoch 150, training loss: 1.7984247207641602 = 1.0938255786895752 + 0.1 * 7.04599142074585
Epoch 150, val loss: 1.2017359733581543
Epoch 160, training loss: 1.6942203044891357 = 0.9926225543022156 + 0.1 * 7.015976905822754
Epoch 160, val loss: 1.1234543323516846
Epoch 170, training loss: 1.6021478176116943 = 0.9029034376144409 + 0.1 * 6.992444038391113
Epoch 170, val loss: 1.0554847717285156
Epoch 180, training loss: 1.5222511291503906 = 0.8244777321815491 + 0.1 * 6.977733135223389
Epoch 180, val loss: 0.996714174747467
Epoch 190, training loss: 1.451012134552002 = 0.7545996904373169 + 0.1 * 6.964125156402588
Epoch 190, val loss: 0.944939374923706
Epoch 200, training loss: 1.3870606422424316 = 0.6921331286430359 + 0.1 * 6.949275493621826
Epoch 200, val loss: 0.8997480273246765
Epoch 210, training loss: 1.3287866115570068 = 0.6354023218154907 + 0.1 * 6.93384313583374
Epoch 210, val loss: 0.8609350323677063
Epoch 220, training loss: 1.2753649950027466 = 0.5831125974655151 + 0.1 * 6.9225239753723145
Epoch 220, val loss: 0.8282853960990906
Epoch 230, training loss: 1.2242352962493896 = 0.5335617661476135 + 0.1 * 6.906735897064209
Epoch 230, val loss: 0.8005983829498291
Epoch 240, training loss: 1.1744604110717773 = 0.48514002561569214 + 0.1 * 6.893204212188721
Epoch 240, val loss: 0.7764336466789246
Epoch 250, training loss: 1.1260839700698853 = 0.4378841519355774 + 0.1 * 6.881998062133789
Epoch 250, val loss: 0.7552156448364258
Epoch 260, training loss: 1.079634666442871 = 0.39225924015045166 + 0.1 * 6.873754501342773
Epoch 260, val loss: 0.7366775870323181
Epoch 270, training loss: 1.0354063510894775 = 0.34894922375679016 + 0.1 * 6.864570617675781
Epoch 270, val loss: 0.7211897969245911
Epoch 280, training loss: 0.995521068572998 = 0.30912530422210693 + 0.1 * 6.863957405090332
Epoch 280, val loss: 0.70914626121521
Epoch 290, training loss: 0.9602766633033752 = 0.27414608001708984 + 0.1 * 6.8613057136535645
Epoch 290, val loss: 0.7010416388511658
Epoch 300, training loss: 0.9284056425094604 = 0.24389313161373138 + 0.1 * 6.8451247215271
Epoch 300, val loss: 0.6966615319252014
Epoch 310, training loss: 0.9012635946273804 = 0.21776530146598816 + 0.1 * 6.834982395172119
Epoch 310, val loss: 0.695762574672699
Epoch 320, training loss: 0.8777891397476196 = 0.1951996088027954 + 0.1 * 6.825895309448242
Epoch 320, val loss: 0.6976274251937866
Epoch 330, training loss: 0.8574997186660767 = 0.17572033405303955 + 0.1 * 6.817793846130371
Epoch 330, val loss: 0.7015949487686157
Epoch 340, training loss: 0.8412951231002808 = 0.15884560346603394 + 0.1 * 6.8244948387146
Epoch 340, val loss: 0.7070267200469971
Epoch 350, training loss: 0.8241749405860901 = 0.14419187605381012 + 0.1 * 6.799830436706543
Epoch 350, val loss: 0.7136980891227722
Epoch 360, training loss: 0.8104557394981384 = 0.13130773603916168 + 0.1 * 6.79148006439209
Epoch 360, val loss: 0.7211765646934509
Epoch 370, training loss: 0.798884391784668 = 0.11988940834999084 + 0.1 * 6.789949417114258
Epoch 370, val loss: 0.7293879389762878
Epoch 380, training loss: 0.7872823476791382 = 0.10976119339466095 + 0.1 * 6.775211811065674
Epoch 380, val loss: 0.738079845905304
Epoch 390, training loss: 0.7772203683853149 = 0.10071238875389099 + 0.1 * 6.765079498291016
Epoch 390, val loss: 0.7470795512199402
Epoch 400, training loss: 0.7682338356971741 = 0.09259621053934097 + 0.1 * 6.756375789642334
Epoch 400, val loss: 0.7563747763633728
Epoch 410, training loss: 0.7597657442092896 = 0.08530577272176743 + 0.1 * 6.74459981918335
Epoch 410, val loss: 0.7658583521842957
Epoch 420, training loss: 0.753506064414978 = 0.07872332632541656 + 0.1 * 6.747827529907227
Epoch 420, val loss: 0.7754544019699097
Epoch 430, training loss: 0.7461607456207275 = 0.07277528941631317 + 0.1 * 6.733854293823242
Epoch 430, val loss: 0.7851829528808594
Epoch 440, training loss: 0.7396108508110046 = 0.06738011538982391 + 0.1 * 6.722307205200195
Epoch 440, val loss: 0.7949351072311401
Epoch 450, training loss: 0.7351670861244202 = 0.0624690018594265 + 0.1 * 6.726980686187744
Epoch 450, val loss: 0.804787814617157
Epoch 460, training loss: 0.7306098341941833 = 0.05801445245742798 + 0.1 * 6.725953578948975
Epoch 460, val loss: 0.8145740032196045
Epoch 470, training loss: 0.7248103022575378 = 0.05397200211882591 + 0.1 * 6.708382606506348
Epoch 470, val loss: 0.8243554830551147
Epoch 480, training loss: 0.7199269533157349 = 0.050283078104257584 + 0.1 * 6.696438789367676
Epoch 480, val loss: 0.8340731263160706
Epoch 490, training loss: 0.7188238501548767 = 0.046908847987651825 + 0.1 * 6.719149589538574
Epoch 490, val loss: 0.8437381982803345
Epoch 500, training loss: 0.7128010988235474 = 0.04383210465312004 + 0.1 * 6.689689636230469
Epoch 500, val loss: 0.8533562421798706
Epoch 510, training loss: 0.7098965644836426 = 0.0410144180059433 + 0.1 * 6.688821315765381
Epoch 510, val loss: 0.8626947999000549
Epoch 520, training loss: 0.7057177424430847 = 0.03843012824654579 + 0.1 * 6.672875881195068
Epoch 520, val loss: 0.8721125721931458
Epoch 530, training loss: 0.7049245238304138 = 0.036053676158189774 + 0.1 * 6.688708782196045
Epoch 530, val loss: 0.8813894987106323
Epoch 540, training loss: 0.7022581100463867 = 0.03387264907360077 + 0.1 * 6.683854103088379
Epoch 540, val loss: 0.8905271291732788
Epoch 550, training loss: 0.6975273489952087 = 0.03186821565032005 + 0.1 * 6.656590938568115
Epoch 550, val loss: 0.8994058966636658
Epoch 560, training loss: 0.696784496307373 = 0.030019929632544518 + 0.1 * 6.667645454406738
Epoch 560, val loss: 0.9081649780273438
Epoch 570, training loss: 0.6931208968162537 = 0.028315408155322075 + 0.1 * 6.648054599761963
Epoch 570, val loss: 0.9168619513511658
Epoch 580, training loss: 0.6919277906417847 = 0.02673904225230217 + 0.1 * 6.651886940002441
Epoch 580, val loss: 0.9252423048019409
Epoch 590, training loss: 0.6893983483314514 = 0.025283364579081535 + 0.1 * 6.641149997711182
Epoch 590, val loss: 0.9337093830108643
Epoch 600, training loss: 0.6880918145179749 = 0.0239360760897398 + 0.1 * 6.641557216644287
Epoch 600, val loss: 0.9419016242027283
Epoch 610, training loss: 0.685092031955719 = 0.02268819883465767 + 0.1 * 6.624038219451904
Epoch 610, val loss: 0.9499577283859253
Epoch 620, training loss: 0.6838783025741577 = 0.021528394892811775 + 0.1 * 6.623498916625977
Epoch 620, val loss: 0.9579421281814575
Epoch 630, training loss: 0.684359073638916 = 0.020450575277209282 + 0.1 * 6.639084815979004
Epoch 630, val loss: 0.9657012820243835
Epoch 640, training loss: 0.6808842420578003 = 0.019452005624771118 + 0.1 * 6.614322662353516
Epoch 640, val loss: 0.9734242558479309
Epoch 650, training loss: 0.6808241605758667 = 0.018521681427955627 + 0.1 * 6.623024940490723
Epoch 650, val loss: 0.981084406375885
Epoch 660, training loss: 0.6791050434112549 = 0.017655635252594948 + 0.1 * 6.614494323730469
Epoch 660, val loss: 0.9883744120597839
Epoch 670, training loss: 0.6770575046539307 = 0.016846681013703346 + 0.1 * 6.602108478546143
Epoch 670, val loss: 0.9957361221313477
Epoch 680, training loss: 0.6754657626152039 = 0.01609133370220661 + 0.1 * 6.593744277954102
Epoch 680, val loss: 1.0028973817825317
Epoch 690, training loss: 0.675476610660553 = 0.015385864302515984 + 0.1 * 6.600907325744629
Epoch 690, val loss: 1.0097310543060303
Epoch 700, training loss: 0.6733033061027527 = 0.014729481190443039 + 0.1 * 6.585738182067871
Epoch 700, val loss: 1.016704797744751
Epoch 710, training loss: 0.6732027530670166 = 0.014115780591964722 + 0.1 * 6.590869426727295
Epoch 710, val loss: 1.0232489109039307
Epoch 720, training loss: 0.6726198792457581 = 0.013541745953261852 + 0.1 * 6.590781211853027
Epoch 720, val loss: 1.0297722816467285
Epoch 730, training loss: 0.6709169745445251 = 0.013002917170524597 + 0.1 * 6.5791401863098145
Epoch 730, val loss: 1.0362753868103027
Epoch 740, training loss: 0.671379566192627 = 0.012496060691773891 + 0.1 * 6.588834762573242
Epoch 740, val loss: 1.0424458980560303
Epoch 750, training loss: 0.6698814630508423 = 0.01201950665563345 + 0.1 * 6.578619480133057
Epoch 750, val loss: 1.048701286315918
Epoch 760, training loss: 0.6694365739822388 = 0.011570960283279419 + 0.1 * 6.57865571975708
Epoch 760, val loss: 1.0546633005142212
Epoch 770, training loss: 0.6674207448959351 = 0.011148649267852306 + 0.1 * 6.562720775604248
Epoch 770, val loss: 1.0606074333190918
Epoch 780, training loss: 0.6677836775779724 = 0.01075027510523796 + 0.1 * 6.570333957672119
Epoch 780, val loss: 1.0663669109344482
Epoch 790, training loss: 0.6671209931373596 = 0.01037498377263546 + 0.1 * 6.567460060119629
Epoch 790, val loss: 1.0720343589782715
Epoch 800, training loss: 0.6658934950828552 = 0.01002026442438364 + 0.1 * 6.558732032775879
Epoch 800, val loss: 1.077591061592102
Epoch 810, training loss: 0.6648702025413513 = 0.009685336612164974 + 0.1 * 6.551848411560059
Epoch 810, val loss: 1.0829482078552246
Epoch 820, training loss: 0.665239691734314 = 0.009368311613798141 + 0.1 * 6.558713912963867
Epoch 820, val loss: 1.088330626487732
Epoch 830, training loss: 0.6631004810333252 = 0.009067285805940628 + 0.1 * 6.540331840515137
Epoch 830, val loss: 1.0935287475585938
Epoch 840, training loss: 0.6630344986915588 = 0.008781909011304379 + 0.1 * 6.542525768280029
Epoch 840, val loss: 1.0986307859420776
Epoch 850, training loss: 0.6619601845741272 = 0.00851207785308361 + 0.1 * 6.534481048583984
Epoch 850, val loss: 1.1036747694015503
Epoch 860, training loss: 0.6626453995704651 = 0.008255590684711933 + 0.1 * 6.54389762878418
Epoch 860, val loss: 1.108549952507019
Epoch 870, training loss: 0.6615951657295227 = 0.008011465892195702 + 0.1 * 6.535837173461914
Epoch 870, val loss: 1.1134012937545776
Epoch 880, training loss: 0.6596202850341797 = 0.007778799626976252 + 0.1 * 6.518414497375488
Epoch 880, val loss: 1.118146538734436
Epoch 890, training loss: 0.6599127650260925 = 0.007557874079793692 + 0.1 * 6.523548603057861
Epoch 890, val loss: 1.1228245496749878
Epoch 900, training loss: 0.6631869673728943 = 0.007346391677856445 + 0.1 * 6.558405876159668
Epoch 900, val loss: 1.1272825002670288
Epoch 910, training loss: 0.6593263149261475 = 0.0071454583667218685 + 0.1 * 6.521808624267578
Epoch 910, val loss: 1.1317790746688843
Epoch 920, training loss: 0.657575786113739 = 0.00695368368178606 + 0.1 * 6.506221294403076
Epoch 920, val loss: 1.1362768411636353
Epoch 930, training loss: 0.6577994227409363 = 0.006769876461476088 + 0.1 * 6.5102949142456055
Epoch 930, val loss: 1.1405595541000366
Epoch 940, training loss: 0.6558754444122314 = 0.00659416476264596 + 0.1 * 6.492812633514404
Epoch 940, val loss: 1.1447316408157349
Epoch 950, training loss: 0.6575385928153992 = 0.006425936706364155 + 0.1 * 6.511126518249512
Epoch 950, val loss: 1.1489334106445312
Epoch 960, training loss: 0.6563836932182312 = 0.006264757364988327 + 0.1 * 6.501189231872559
Epoch 960, val loss: 1.1530789136886597
Epoch 970, training loss: 0.6565210819244385 = 0.006110542919486761 + 0.1 * 6.504105091094971
Epoch 970, val loss: 1.1571520566940308
Epoch 980, training loss: 0.6558130383491516 = 0.0059629217721521854 + 0.1 * 6.498501300811768
Epoch 980, val loss: 1.1612356901168823
Epoch 990, training loss: 0.653536856174469 = 0.005820739548653364 + 0.1 * 6.477160930633545
Epoch 990, val loss: 1.1651121377944946
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.8127505779266357 = 1.9530701637268066 + 0.1 * 8.59680461883545
Epoch 0, val loss: 1.944132685661316
Epoch 10, training loss: 2.8028981685638428 = 1.943225622177124 + 0.1 * 8.596724510192871
Epoch 10, val loss: 1.9352747201919556
Epoch 20, training loss: 2.791260004043579 = 1.9316519498825073 + 0.1 * 8.59607982635498
Epoch 20, val loss: 1.9248074293136597
Epoch 30, training loss: 2.774925708770752 = 1.9159154891967773 + 0.1 * 8.590103149414062
Epoch 30, val loss: 1.9104846715927124
Epoch 40, training loss: 2.7472128868103027 = 1.8928707838058472 + 0.1 * 8.543421745300293
Epoch 40, val loss: 1.8896363973617554
Epoch 50, training loss: 2.687889337539673 = 1.8606643676757812 + 0.1 * 8.272250175476074
Epoch 50, val loss: 1.8612635135650635
Epoch 60, training loss: 2.630235195159912 = 1.8226583003997803 + 0.1 * 8.075769424438477
Epoch 60, val loss: 1.8293098211288452
Epoch 70, training loss: 2.546231269836426 = 1.7865971326828003 + 0.1 * 7.596340656280518
Epoch 70, val loss: 1.7992877960205078
Epoch 80, training loss: 2.4686520099639893 = 1.7532589435577393 + 0.1 * 7.153930187225342
Epoch 80, val loss: 1.7693613767623901
Epoch 90, training loss: 2.4152348041534424 = 1.7137075662612915 + 0.1 * 7.015273094177246
Epoch 90, val loss: 1.7327007055282593
Epoch 100, training loss: 2.3574821949005127 = 1.6617107391357422 + 0.1 * 6.957715034484863
Epoch 100, val loss: 1.6858433485031128
Epoch 110, training loss: 2.285823345184326 = 1.5946873426437378 + 0.1 * 6.911360740661621
Epoch 110, val loss: 1.6273434162139893
Epoch 120, training loss: 2.2009308338165283 = 1.5128289461135864 + 0.1 * 6.881019115447998
Epoch 120, val loss: 1.5567747354507446
Epoch 130, training loss: 2.1066956520080566 = 1.420983076095581 + 0.1 * 6.857125282287598
Epoch 130, val loss: 1.4778233766555786
Epoch 140, training loss: 2.008577346801758 = 1.3249815702438354 + 0.1 * 6.835958480834961
Epoch 140, val loss: 1.396699070930481
Epoch 150, training loss: 1.91267728805542 = 1.2300770282745361 + 0.1 * 6.826003074645996
Epoch 150, val loss: 1.3179171085357666
Epoch 160, training loss: 1.8192613124847412 = 1.1384495496749878 + 0.1 * 6.808117389678955
Epoch 160, val loss: 1.2436442375183105
Epoch 170, training loss: 1.729048252105713 = 1.0494812726974487 + 0.1 * 6.795670509338379
Epoch 170, val loss: 1.1733729839324951
Epoch 180, training loss: 1.6444216966629028 = 0.9654890894889832 + 0.1 * 6.789326190948486
Epoch 180, val loss: 1.1087231636047363
Epoch 190, training loss: 1.5635349750518799 = 0.8859199285507202 + 0.1 * 6.776149749755859
Epoch 190, val loss: 1.0486090183258057
Epoch 200, training loss: 1.4864654541015625 = 0.8090399503707886 + 0.1 * 6.774254322052002
Epoch 200, val loss: 0.991282045841217
Epoch 210, training loss: 1.4118565320968628 = 0.7356487512588501 + 0.1 * 6.762077808380127
Epoch 210, val loss: 0.9367797374725342
Epoch 220, training loss: 1.3417816162109375 = 0.6664055585861206 + 0.1 * 6.753761291503906
Epoch 220, val loss: 0.886355459690094
Epoch 230, training loss: 1.2788994312286377 = 0.6036214232444763 + 0.1 * 6.752779483795166
Epoch 230, val loss: 0.8430076241493225
Epoch 240, training loss: 1.2230080366134644 = 0.5486489534378052 + 0.1 * 6.743590831756592
Epoch 240, val loss: 0.8083617091178894
Epoch 250, training loss: 1.174481749534607 = 0.500688374042511 + 0.1 * 6.73793363571167
Epoch 250, val loss: 0.7821072340011597
Epoch 260, training loss: 1.1324336528778076 = 0.4589369297027588 + 0.1 * 6.734966278076172
Epoch 260, val loss: 0.763324499130249
Epoch 270, training loss: 1.0944178104400635 = 0.42184481024742126 + 0.1 * 6.725729942321777
Epoch 270, val loss: 0.7502394914627075
Epoch 280, training loss: 1.0601868629455566 = 0.3879125118255615 + 0.1 * 6.722743988037109
Epoch 280, val loss: 0.7410903573036194
Epoch 290, training loss: 1.0274338722229004 = 0.35621339082717896 + 0.1 * 6.712205410003662
Epoch 290, val loss: 0.7345563769340515
Epoch 300, training loss: 0.9965928792953491 = 0.32588016986846924 + 0.1 * 6.707127094268799
Epoch 300, val loss: 0.7297793626785278
Epoch 310, training loss: 0.9668362140655518 = 0.2966669201850891 + 0.1 * 6.701692581176758
Epoch 310, val loss: 0.7262199521064758
Epoch 320, training loss: 0.9375019073486328 = 0.2685742676258087 + 0.1 * 6.689275741577148
Epoch 320, val loss: 0.7236305475234985
Epoch 330, training loss: 0.9116899371147156 = 0.24167273938655853 + 0.1 * 6.700171947479248
Epoch 330, val loss: 0.7219182848930359
Epoch 340, training loss: 0.8838870525360107 = 0.2165270894765854 + 0.1 * 6.6735992431640625
Epoch 340, val loss: 0.7212158441543579
Epoch 350, training loss: 0.860176682472229 = 0.19330362975597382 + 0.1 * 6.66873025894165
Epoch 350, val loss: 0.7216737866401672
Epoch 360, training loss: 0.8392078280448914 = 0.17224276065826416 + 0.1 * 6.669650554656982
Epoch 360, val loss: 0.7234185338020325
Epoch 370, training loss: 0.8193892240524292 = 0.1535620540380478 + 0.1 * 6.658271312713623
Epoch 370, val loss: 0.7264082431793213
Epoch 380, training loss: 0.8018975853919983 = 0.13713230192661285 + 0.1 * 6.647652626037598
Epoch 380, val loss: 0.7307919859886169
Epoch 390, training loss: 0.7873084545135498 = 0.12280774116516113 + 0.1 * 6.645007133483887
Epoch 390, val loss: 0.7363309264183044
Epoch 400, training loss: 0.7740414142608643 = 0.1103118360042572 + 0.1 * 6.637295722961426
Epoch 400, val loss: 0.7427995204925537
Epoch 410, training loss: 0.7615752816200256 = 0.09938397258520126 + 0.1 * 6.621912956237793
Epoch 410, val loss: 0.7500170469284058
Epoch 420, training loss: 0.7512853145599365 = 0.08978192508220673 + 0.1 * 6.6150336265563965
Epoch 420, val loss: 0.7579693794250488
Epoch 430, training loss: 0.7420918345451355 = 0.0813327506184578 + 0.1 * 6.607590675354004
Epoch 430, val loss: 0.7663684487342834
Epoch 440, training loss: 0.735221266746521 = 0.07391902804374695 + 0.1 * 6.613022804260254
Epoch 440, val loss: 0.7751277685165405
Epoch 450, training loss: 0.7273264527320862 = 0.0673985704779625 + 0.1 * 6.599278926849365
Epoch 450, val loss: 0.7839472889900208
Epoch 460, training loss: 0.7215753793716431 = 0.06161446124315262 + 0.1 * 6.599609375
Epoch 460, val loss: 0.7931274175643921
Epoch 470, training loss: 0.7155832052230835 = 0.05648872256278992 + 0.1 * 6.590944290161133
Epoch 470, val loss: 0.8022453188896179
Epoch 480, training loss: 0.7104677557945251 = 0.05193250998854637 + 0.1 * 6.585352420806885
Epoch 480, val loss: 0.8114408850669861
Epoch 490, training loss: 0.705762505531311 = 0.04786064848303795 + 0.1 * 6.579018592834473
Epoch 490, val loss: 0.820637583732605
Epoch 500, training loss: 0.702418863773346 = 0.044223323464393616 + 0.1 * 6.581955432891846
Epoch 500, val loss: 0.8298142552375793
Epoch 510, training loss: 0.6973594427108765 = 0.04097120836377144 + 0.1 * 6.563882350921631
Epoch 510, val loss: 0.838731586933136
Epoch 520, training loss: 0.6955677270889282 = 0.038051020354032516 + 0.1 * 6.575167179107666
Epoch 520, val loss: 0.8476980328559875
Epoch 530, training loss: 0.6912214756011963 = 0.03543385863304138 + 0.1 * 6.5578765869140625
Epoch 530, val loss: 0.8562424182891846
Epoch 540, training loss: 0.6884613037109375 = 0.03306728973984718 + 0.1 * 6.553940296173096
Epoch 540, val loss: 0.8648661375045776
Epoch 550, training loss: 0.685985267162323 = 0.030922172591090202 + 0.1 * 6.550631046295166
Epoch 550, val loss: 0.8732523918151855
Epoch 560, training loss: 0.6840122938156128 = 0.02898222766816616 + 0.1 * 6.550300598144531
Epoch 560, val loss: 0.8813925385475159
Epoch 570, training loss: 0.6831228733062744 = 0.027212809771299362 + 0.1 * 6.55910062789917
Epoch 570, val loss: 0.889505922794342
Epoch 580, training loss: 0.6796869039535522 = 0.025602590292692184 + 0.1 * 6.5408430099487305
Epoch 580, val loss: 0.8973445296287537
Epoch 590, training loss: 0.6772992014884949 = 0.02412964403629303 + 0.1 * 6.531695365905762
Epoch 590, val loss: 0.9049897789955139
Epoch 600, training loss: 0.6791608333587646 = 0.022775541990995407 + 0.1 * 6.563852787017822
Epoch 600, val loss: 0.9125526547431946
Epoch 610, training loss: 0.6740461587905884 = 0.021541688591241837 + 0.1 * 6.5250444412231445
Epoch 610, val loss: 0.9199639558792114
Epoch 620, training loss: 0.6730097532272339 = 0.02040821872651577 + 0.1 * 6.526015281677246
Epoch 620, val loss: 0.9270123839378357
Epoch 630, training loss: 0.6717278361320496 = 0.019360220059752464 + 0.1 * 6.523675918579102
Epoch 630, val loss: 0.9341093897819519
Epoch 640, training loss: 0.6711406111717224 = 0.01839311048388481 + 0.1 * 6.527474880218506
Epoch 640, val loss: 0.9409594535827637
Epoch 650, training loss: 0.6690192818641663 = 0.017498018220067024 + 0.1 * 6.515212535858154
Epoch 650, val loss: 0.9476447701454163
Epoch 660, training loss: 0.6689208745956421 = 0.016669301316142082 + 0.1 * 6.522515773773193
Epoch 660, val loss: 0.9542368650436401
Epoch 670, training loss: 0.6665094494819641 = 0.015900868922472 + 0.1 * 6.5060858726501465
Epoch 670, val loss: 0.9606710076332092
Epoch 680, training loss: 0.6660112142562866 = 0.015186566859483719 + 0.1 * 6.508246421813965
Epoch 680, val loss: 0.9669214487075806
Epoch 690, training loss: 0.6651090383529663 = 0.01451938133686781 + 0.1 * 6.50589656829834
Epoch 690, val loss: 0.9731345772743225
Epoch 700, training loss: 0.6638374924659729 = 0.01389950793236494 + 0.1 * 6.499379634857178
Epoch 700, val loss: 0.9790969491004944
Epoch 710, training loss: 0.6631802916526794 = 0.01331971026957035 + 0.1 * 6.498605728149414
Epoch 710, val loss: 0.9849615097045898
Epoch 720, training loss: 0.6631083488464355 = 0.012775965966284275 + 0.1 * 6.503323554992676
Epoch 720, val loss: 0.990752637386322
Epoch 730, training loss: 0.6615082025527954 = 0.012268959544599056 + 0.1 * 6.492392063140869
Epoch 730, val loss: 0.9963777661323547
Epoch 740, training loss: 0.6619178652763367 = 0.011792621575295925 + 0.1 * 6.5012526512146
Epoch 740, val loss: 1.0019433498382568
Epoch 750, training loss: 0.6596472859382629 = 0.011345812119543552 + 0.1 * 6.4830145835876465
Epoch 750, val loss: 1.0073542594909668
Epoch 760, training loss: 0.6590128540992737 = 0.010925005190074444 + 0.1 * 6.4808783531188965
Epoch 760, val loss: 1.01255202293396
Epoch 770, training loss: 0.6584663987159729 = 0.01052867155522108 + 0.1 * 6.479377269744873
Epoch 770, val loss: 1.0178548097610474
Epoch 780, training loss: 0.6578916907310486 = 0.010156122967600822 + 0.1 * 6.47735595703125
Epoch 780, val loss: 1.0228490829467773
Epoch 790, training loss: 0.6587634086608887 = 0.00980397779494524 + 0.1 * 6.489594459533691
Epoch 790, val loss: 1.027839183807373
Epoch 800, training loss: 0.6584311723709106 = 0.009471184574067593 + 0.1 * 6.489599704742432
Epoch 800, val loss: 1.0328019857406616
Epoch 810, training loss: 0.6570190787315369 = 0.009157894179224968 + 0.1 * 6.478611469268799
Epoch 810, val loss: 1.0374679565429688
Epoch 820, training loss: 0.6568692922592163 = 0.008860685862600803 + 0.1 * 6.480085849761963
Epoch 820, val loss: 1.0421504974365234
Epoch 830, training loss: 0.6553294658660889 = 0.008578520268201828 + 0.1 * 6.4675092697143555
Epoch 830, val loss: 1.0467439889907837
Epoch 840, training loss: 0.6563235521316528 = 0.008310873061418533 + 0.1 * 6.480126857757568
Epoch 840, val loss: 1.0512795448303223
Epoch 850, training loss: 0.6542519927024841 = 0.008056418038904667 + 0.1 * 6.461955547332764
Epoch 850, val loss: 1.0556983947753906
Epoch 860, training loss: 0.6546205878257751 = 0.00781510304659605 + 0.1 * 6.46805477142334
Epoch 860, val loss: 1.0600273609161377
Epoch 870, training loss: 0.6546098589897156 = 0.0075850170105695724 + 0.1 * 6.470248699188232
Epoch 870, val loss: 1.0643824338912964
Epoch 880, training loss: 0.6531897187232971 = 0.0073663219809532166 + 0.1 * 6.4582343101501465
Epoch 880, val loss: 1.068600058555603
Epoch 890, training loss: 0.6531343460083008 = 0.0071585774421691895 + 0.1 * 6.459757328033447
Epoch 890, val loss: 1.0727328062057495
Epoch 900, training loss: 0.6525552868843079 = 0.006960062310099602 + 0.1 * 6.455952167510986
Epoch 900, val loss: 1.0767862796783447
Epoch 910, training loss: 0.6529763340950012 = 0.006770492531359196 + 0.1 * 6.462058067321777
Epoch 910, val loss: 1.0807477235794067
Epoch 920, training loss: 0.6524728536605835 = 0.006589605938643217 + 0.1 * 6.458832740783691
Epoch 920, val loss: 1.084696650505066
Epoch 930, training loss: 0.6523547172546387 = 0.006416144780814648 + 0.1 * 6.459385395050049
Epoch 930, val loss: 1.0885450839996338
Epoch 940, training loss: 0.6513890624046326 = 0.006250686477869749 + 0.1 * 6.451383590698242
Epoch 940, val loss: 1.0923590660095215
Epoch 950, training loss: 0.6510956287384033 = 0.006092570256441832 + 0.1 * 6.45003080368042
Epoch 950, val loss: 1.0960899591445923
Epoch 960, training loss: 0.6508864164352417 = 0.005941429175436497 + 0.1 * 6.44944953918457
Epoch 960, val loss: 1.0997803211212158
Epoch 970, training loss: 0.6507407426834106 = 0.005796381738036871 + 0.1 * 6.449443817138672
Epoch 970, val loss: 1.1034002304077148
Epoch 980, training loss: 0.6495212912559509 = 0.005657019093632698 + 0.1 * 6.438642501831055
Epoch 980, val loss: 1.1069309711456299
Epoch 990, training loss: 0.6501643061637878 = 0.005523221101611853 + 0.1 * 6.446410655975342
Epoch 990, val loss: 1.1104652881622314
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.8203582763671875 = 1.9606754779815674 + 0.1 * 8.59682846069336
Epoch 0, val loss: 1.962199330329895
Epoch 10, training loss: 2.810197114944458 = 1.9505231380462646 + 0.1 * 8.596739768981934
Epoch 10, val loss: 1.9525047540664673
Epoch 20, training loss: 2.7975575923919678 = 1.9379417896270752 + 0.1 * 8.59615707397461
Epoch 20, val loss: 1.9400019645690918
Epoch 30, training loss: 2.7792000770568848 = 1.9200619459152222 + 0.1 * 8.59138011932373
Epoch 30, val loss: 1.9218497276306152
Epoch 40, training loss: 2.749433755874634 = 1.8930587768554688 + 0.1 * 8.563749313354492
Epoch 40, val loss: 1.894443392753601
Epoch 50, training loss: 2.7006747722625732 = 1.854796290397644 + 0.1 * 8.458784103393555
Epoch 50, val loss: 1.8571809530258179
Epoch 60, training loss: 2.6342852115631104 = 1.8105803728103638 + 0.1 * 8.237048149108887
Epoch 60, val loss: 1.8173699378967285
Epoch 70, training loss: 2.577897548675537 = 1.7692965269088745 + 0.1 * 8.086010932922363
Epoch 70, val loss: 1.7822391986846924
Epoch 80, training loss: 2.504000186920166 = 1.7234526872634888 + 0.1 * 7.80547571182251
Epoch 80, val loss: 1.7397769689559937
Epoch 90, training loss: 2.4143588542938232 = 1.666100263595581 + 0.1 * 7.4825849533081055
Epoch 90, val loss: 1.6872347593307495
Epoch 100, training loss: 2.321326732635498 = 1.594591498374939 + 0.1 * 7.2673516273498535
Epoch 100, val loss: 1.624196171760559
Epoch 110, training loss: 2.226597547531128 = 1.50899076461792 + 0.1 * 7.176068305969238
Epoch 110, val loss: 1.5500646829605103
Epoch 120, training loss: 2.130460262298584 = 1.4196488857269287 + 0.1 * 7.108114719390869
Epoch 120, val loss: 1.4757877588272095
Epoch 130, training loss: 2.0397017002105713 = 1.3335597515106201 + 0.1 * 7.061418533325195
Epoch 130, val loss: 1.4064254760742188
Epoch 140, training loss: 1.9523794651031494 = 1.2503561973571777 + 0.1 * 7.020232200622559
Epoch 140, val loss: 1.3417530059814453
Epoch 150, training loss: 1.8684563636779785 = 1.1699985265731812 + 0.1 * 6.984577655792236
Epoch 150, val loss: 1.2811490297317505
Epoch 160, training loss: 1.789381742477417 = 1.0932203531265259 + 0.1 * 6.961613178253174
Epoch 160, val loss: 1.2253893613815308
Epoch 170, training loss: 1.714587688446045 = 1.0204099416732788 + 0.1 * 6.94177770614624
Epoch 170, val loss: 1.1739565134048462
Epoch 180, training loss: 1.6427686214447021 = 0.9502303004264832 + 0.1 * 6.9253830909729
Epoch 180, val loss: 1.1243281364440918
Epoch 190, training loss: 1.5730655193328857 = 0.8820527195930481 + 0.1 * 6.91012716293335
Epoch 190, val loss: 1.0757449865341187
Epoch 200, training loss: 1.5044066905975342 = 0.8148343563079834 + 0.1 * 6.895723819732666
Epoch 200, val loss: 1.026788353919983
Epoch 210, training loss: 1.4359439611434937 = 0.7478041052818298 + 0.1 * 6.881398677825928
Epoch 210, val loss: 0.9772360324859619
Epoch 220, training loss: 1.3686429262161255 = 0.6814883351325989 + 0.1 * 6.871545791625977
Epoch 220, val loss: 0.9277774691581726
Epoch 230, training loss: 1.3037433624267578 = 0.6173020005226135 + 0.1 * 6.864413261413574
Epoch 230, val loss: 0.8805886507034302
Epoch 240, training loss: 1.241896629333496 = 0.556472897529602 + 0.1 * 6.854236602783203
Epoch 240, val loss: 0.8373622298240662
Epoch 250, training loss: 1.1842272281646729 = 0.49944669008255005 + 0.1 * 6.847805500030518
Epoch 250, val loss: 0.7995805740356445
Epoch 260, training loss: 1.132392406463623 = 0.4473840296268463 + 0.1 * 6.850083827972412
Epoch 260, val loss: 0.7685436010360718
Epoch 270, training loss: 1.0850695371627808 = 0.4010140597820282 + 0.1 * 6.840554714202881
Epoch 270, val loss: 0.7447624206542969
Epoch 280, training loss: 1.0429821014404297 = 0.35969552397727966 + 0.1 * 6.832866191864014
Epoch 280, val loss: 0.7273402810096741
Epoch 290, training loss: 1.0066149234771729 = 0.3229950964450836 + 0.1 * 6.836198329925537
Epoch 290, val loss: 0.7151766419410706
Epoch 300, training loss: 0.9728081226348877 = 0.29058194160461426 + 0.1 * 6.822261810302734
Epoch 300, val loss: 0.707177996635437
Epoch 310, training loss: 0.9431207180023193 = 0.2616024315357208 + 0.1 * 6.815182685852051
Epoch 310, val loss: 0.7023102641105652
Epoch 320, training loss: 0.9167963862419128 = 0.23549950122833252 + 0.1 * 6.812968730926514
Epoch 320, val loss: 0.6999602317810059
Epoch 330, training loss: 0.8938822746276855 = 0.21206340193748474 + 0.1 * 6.818188190460205
Epoch 330, val loss: 0.6995959281921387
Epoch 340, training loss: 0.8710958361625671 = 0.19109143316745758 + 0.1 * 6.800044059753418
Epoch 340, val loss: 0.7007594704627991
Epoch 350, training loss: 0.8513870239257812 = 0.17219975590705872 + 0.1 * 6.791872501373291
Epoch 350, val loss: 0.7033053636550903
Epoch 360, training loss: 0.8337334394454956 = 0.15526005625724792 + 0.1 * 6.784733295440674
Epoch 360, val loss: 0.7071500420570374
Epoch 370, training loss: 0.8175280094146729 = 0.14009496569633484 + 0.1 * 6.774330139160156
Epoch 370, val loss: 0.712044358253479
Epoch 380, training loss: 0.8022825717926025 = 0.1265808641910553 + 0.1 * 6.757016658782959
Epoch 380, val loss: 0.7179089188575745
Epoch 390, training loss: 0.7913908958435059 = 0.11459390819072723 + 0.1 * 6.767969608306885
Epoch 390, val loss: 0.7244179844856262
Epoch 400, training loss: 0.7784274220466614 = 0.10402233898639679 + 0.1 * 6.744050979614258
Epoch 400, val loss: 0.7315561771392822
Epoch 410, training loss: 0.7681176066398621 = 0.09462423622608185 + 0.1 * 6.734933853149414
Epoch 410, val loss: 0.7392411231994629
Epoch 420, training loss: 0.7604575157165527 = 0.08625610917806625 + 0.1 * 6.742014408111572
Epoch 420, val loss: 0.7473371624946594
Epoch 430, training loss: 0.7520401477813721 = 0.07887603342533112 + 0.1 * 6.7316412925720215
Epoch 430, val loss: 0.7556681036949158
Epoch 440, training loss: 0.7436304688453674 = 0.07231844216585159 + 0.1 * 6.713119983673096
Epoch 440, val loss: 0.7641111612319946
Epoch 450, training loss: 0.7364501953125 = 0.0664542093873024 + 0.1 * 6.6999592781066895
Epoch 450, val loss: 0.7727571725845337
Epoch 460, training loss: 0.731728732585907 = 0.06120520830154419 + 0.1 * 6.705235004425049
Epoch 460, val loss: 0.7813857197761536
Epoch 470, training loss: 0.725631594657898 = 0.05653219670057297 + 0.1 * 6.690993785858154
Epoch 470, val loss: 0.7900805473327637
Epoch 480, training loss: 0.7207786440849304 = 0.05233214423060417 + 0.1 * 6.684464931488037
Epoch 480, val loss: 0.7987094521522522
Epoch 490, training loss: 0.7208869457244873 = 0.04854520410299301 + 0.1 * 6.72341775894165
Epoch 490, val loss: 0.8072786927223206
Epoch 500, training loss: 0.7131137251853943 = 0.04515014588832855 + 0.1 * 6.679635524749756
Epoch 500, val loss: 0.8158490061759949
Epoch 510, training loss: 0.7082594633102417 = 0.042078763246536255 + 0.1 * 6.661807060241699
Epoch 510, val loss: 0.8242020606994629
Epoch 520, training loss: 0.7075417637825012 = 0.03928697481751442 + 0.1 * 6.6825480461120605
Epoch 520, val loss: 0.8325885534286499
Epoch 530, training loss: 0.701466977596283 = 0.03676300868391991 + 0.1 * 6.647039413452148
Epoch 530, val loss: 0.8407338261604309
Epoch 540, training loss: 0.6980941295623779 = 0.03446770831942558 + 0.1 * 6.636264324188232
Epoch 540, val loss: 0.8487845659255981
Epoch 550, training loss: 0.6953296661376953 = 0.0323677659034729 + 0.1 * 6.6296186447143555
Epoch 550, val loss: 0.8567957878112793
Epoch 560, training loss: 0.6937692761421204 = 0.030443498864769936 + 0.1 * 6.633257865905762
Epoch 560, val loss: 0.8645588159561157
Epoch 570, training loss: 0.6915502548217773 = 0.028685523197054863 + 0.1 * 6.628647327423096
Epoch 570, val loss: 0.8723363876342773
Epoch 580, training loss: 0.6907127499580383 = 0.02707027457654476 + 0.1 * 6.636424541473389
Epoch 580, val loss: 0.8797943592071533
Epoch 590, training loss: 0.6869165897369385 = 0.025587765499949455 + 0.1 * 6.613288402557373
Epoch 590, val loss: 0.8872296214103699
Epoch 600, training loss: 0.685706377029419 = 0.024219125509262085 + 0.1 * 6.614872932434082
Epoch 600, val loss: 0.894471287727356
Epoch 610, training loss: 0.6842015981674194 = 0.02295631915330887 + 0.1 * 6.612452983856201
Epoch 610, val loss: 0.9015697836875916
Epoch 620, training loss: 0.6821014881134033 = 0.021789416670799255 + 0.1 * 6.60312032699585
Epoch 620, val loss: 0.9085719585418701
Epoch 630, training loss: 0.6801273822784424 = 0.020708320662379265 + 0.1 * 6.59419059753418
Epoch 630, val loss: 0.9153532385826111
Epoch 640, training loss: 0.680248498916626 = 0.019705435261130333 + 0.1 * 6.605430603027344
Epoch 640, val loss: 0.9220492839813232
Epoch 650, training loss: 0.6781598329544067 = 0.01877634972333908 + 0.1 * 6.59383487701416
Epoch 650, val loss: 0.9284882545471191
Epoch 660, training loss: 0.6764087677001953 = 0.017913904041051865 + 0.1 * 6.584948539733887
Epoch 660, val loss: 0.9348858594894409
Epoch 670, training loss: 0.6748787760734558 = 0.017108336091041565 + 0.1 * 6.577704429626465
Epoch 670, val loss: 0.9411283731460571
Epoch 680, training loss: 0.6758788824081421 = 0.016354842111468315 + 0.1 * 6.595240116119385
Epoch 680, val loss: 0.9472838640213013
Epoch 690, training loss: 0.6744951605796814 = 0.0156534593552351 + 0.1 * 6.588417053222656
Epoch 690, val loss: 0.9531397223472595
Epoch 700, training loss: 0.671918511390686 = 0.01499965414404869 + 0.1 * 6.569188594818115
Epoch 700, val loss: 0.9590623378753662
Epoch 710, training loss: 0.6745509505271912 = 0.014386161230504513 + 0.1 * 6.60164737701416
Epoch 710, val loss: 0.9646489024162292
Epoch 720, training loss: 0.6707531213760376 = 0.013813463039696217 + 0.1 * 6.569396495819092
Epoch 720, val loss: 0.9702394604682922
Epoch 730, training loss: 0.6696169376373291 = 0.013275028206408024 + 0.1 * 6.563418865203857
Epoch 730, val loss: 0.9757362604141235
Epoch 740, training loss: 0.6687192320823669 = 0.012766982428729534 + 0.1 * 6.55952262878418
Epoch 740, val loss: 0.9811189770698547
Epoch 750, training loss: 0.6692087650299072 = 0.012288056313991547 + 0.1 * 6.569207191467285
Epoch 750, val loss: 0.9863118529319763
Epoch 760, training loss: 0.6675409078598022 = 0.011839332059025764 + 0.1 * 6.557015419006348
Epoch 760, val loss: 0.9914043545722961
Epoch 770, training loss: 0.6674163341522217 = 0.011416901834309101 + 0.1 * 6.559994220733643
Epoch 770, val loss: 0.9964969158172607
Epoch 780, training loss: 0.6666564345359802 = 0.011017494834959507 + 0.1 * 6.556389331817627
Epoch 780, val loss: 1.001339077949524
Epoch 790, training loss: 0.6678410172462463 = 0.010639284737408161 + 0.1 * 6.572017192840576
Epoch 790, val loss: 1.006188154220581
Epoch 800, training loss: 0.6655013561248779 = 0.010282309725880623 + 0.1 * 6.55219030380249
Epoch 800, val loss: 1.0108768939971924
Epoch 810, training loss: 0.6648821234703064 = 0.009944240562617779 + 0.1 * 6.549378395080566
Epoch 810, val loss: 1.0155285596847534
Epoch 820, training loss: 0.6642837524414062 = 0.009624199941754341 + 0.1 * 6.546595573425293
Epoch 820, val loss: 1.0199439525604248
Epoch 830, training loss: 0.6630914211273193 = 0.009321545250713825 + 0.1 * 6.537698268890381
Epoch 830, val loss: 1.0244371891021729
Epoch 840, training loss: 0.6648620963096619 = 0.00903318077325821 + 0.1 * 6.558289051055908
Epoch 840, val loss: 1.0288010835647583
Epoch 850, training loss: 0.6621715426445007 = 0.008759712800383568 + 0.1 * 6.534117698669434
Epoch 850, val loss: 1.0330005884170532
Epoch 860, training loss: 0.6622208952903748 = 0.00849966611713171 + 0.1 * 6.537211894989014
Epoch 860, val loss: 1.037248134613037
Epoch 870, training loss: 0.6623407602310181 = 0.008251567371189594 + 0.1 * 6.540892124176025
Epoch 870, val loss: 1.0413734912872314
Epoch 880, training loss: 0.6608206033706665 = 0.008015306666493416 + 0.1 * 6.528052806854248
Epoch 880, val loss: 1.0453261137008667
Epoch 890, training loss: 0.6601870656013489 = 0.007791030220687389 + 0.1 * 6.523960113525391
Epoch 890, val loss: 1.0493701696395874
Epoch 900, training loss: 0.6608128547668457 = 0.007576204836368561 + 0.1 * 6.5323662757873535
Epoch 900, val loss: 1.0532339811325073
Epoch 910, training loss: 0.659633457660675 = 0.007371655199676752 + 0.1 * 6.522617816925049
Epoch 910, val loss: 1.0570756196975708
Epoch 920, training loss: 0.6594393253326416 = 0.007175506558269262 + 0.1 * 6.522638320922852
Epoch 920, val loss: 1.0608521699905396
Epoch 930, training loss: 0.6581680178642273 = 0.006988213863223791 + 0.1 * 6.511797904968262
Epoch 930, val loss: 1.0645960569381714
Epoch 940, training loss: 0.6579057574272156 = 0.006808526813983917 + 0.1 * 6.510972023010254
Epoch 940, val loss: 1.06820547580719
Epoch 950, training loss: 0.6584413647651672 = 0.006636698264628649 + 0.1 * 6.518046855926514
Epoch 950, val loss: 1.071808934211731
Epoch 960, training loss: 0.6585051417350769 = 0.006471920758485794 + 0.1 * 6.520331859588623
Epoch 960, val loss: 1.0752638578414917
Epoch 970, training loss: 0.6577726006507874 = 0.00631526205688715 + 0.1 * 6.514573097229004
Epoch 970, val loss: 1.078784704208374
Epoch 980, training loss: 0.6565914750099182 = 0.006164100952446461 + 0.1 * 6.504273891448975
Epoch 980, val loss: 1.082215428352356
Epoch 990, training loss: 0.6599692702293396 = 0.00601904047653079 + 0.1 * 6.539502143859863
Epoch 990, val loss: 1.0855334997177124
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8371112282551397
The final CL Acc:0.81852, 0.00302, The final GNN Acc:0.83904, 0.00138
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11552])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10478])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8060731887817383 = 1.9463921785354614 + 0.1 * 8.596810340881348
Epoch 0, val loss: 1.938867449760437
Epoch 10, training loss: 2.7963778972625732 = 1.936707615852356 + 0.1 * 8.596702575683594
Epoch 10, val loss: 1.9298272132873535
Epoch 20, training loss: 2.7846357822418213 = 1.9250293970108032 + 0.1 * 8.596064567565918
Epoch 20, val loss: 1.9184918403625488
Epoch 30, training loss: 2.7680933475494385 = 1.9090338945388794 + 0.1 * 8.590595245361328
Epoch 30, val loss: 1.9027067422866821
Epoch 40, training loss: 2.740612506866455 = 1.8856604099273682 + 0.1 * 8.549520492553711
Epoch 40, val loss: 1.8797343969345093
Epoch 50, training loss: 2.6830270290374756 = 1.8535854816436768 + 0.1 * 8.294414520263672
Epoch 50, val loss: 1.8495688438415527
Epoch 60, training loss: 2.6265721321105957 = 1.8175606727600098 + 0.1 * 8.09011459350586
Epoch 60, val loss: 1.8179610967636108
Epoch 70, training loss: 2.5466060638427734 = 1.7851816415786743 + 0.1 * 7.61424446105957
Epoch 70, val loss: 1.7905995845794678
Epoch 80, training loss: 2.474036693572998 = 1.755325198173523 + 0.1 * 7.1871161460876465
Epoch 80, val loss: 1.7650728225708008
Epoch 90, training loss: 2.4237449169158936 = 1.7195065021514893 + 0.1 * 7.042384624481201
Epoch 90, val loss: 1.7337443828582764
Epoch 100, training loss: 2.366816759109497 = 1.6698644161224365 + 0.1 * 6.969522953033447
Epoch 100, val loss: 1.690502643585205
Epoch 110, training loss: 2.295025587081909 = 1.603406548500061 + 0.1 * 6.916190147399902
Epoch 110, val loss: 1.6343544721603394
Epoch 120, training loss: 2.209533214569092 = 1.5217152833938599 + 0.1 * 6.878178596496582
Epoch 120, val loss: 1.5674400329589844
Epoch 130, training loss: 2.1161787509918213 = 1.431479573249817 + 0.1 * 6.846992492675781
Epoch 130, val loss: 1.4951043128967285
Epoch 140, training loss: 2.0229973793029785 = 1.3404101133346558 + 0.1 * 6.825871467590332
Epoch 140, val loss: 1.4240896701812744
Epoch 150, training loss: 1.931398868560791 = 1.250874400138855 + 0.1 * 6.805243968963623
Epoch 150, val loss: 1.3556559085845947
Epoch 160, training loss: 1.840954303741455 = 1.1619548797607422 + 0.1 * 6.789994239807129
Epoch 160, val loss: 1.2879959344863892
Epoch 170, training loss: 1.752617359161377 = 1.0750901699066162 + 0.1 * 6.775272369384766
Epoch 170, val loss: 1.2227295637130737
Epoch 180, training loss: 1.6688249111175537 = 0.9925005435943604 + 0.1 * 6.763243198394775
Epoch 180, val loss: 1.1620171070098877
Epoch 190, training loss: 1.5904464721679688 = 0.9144114851951599 + 0.1 * 6.760349750518799
Epoch 190, val loss: 1.1059472560882568
Epoch 200, training loss: 1.516998052597046 = 0.84260493516922 + 0.1 * 6.743930816650391
Epoch 200, val loss: 1.0563706159591675
Epoch 210, training loss: 1.4494974613189697 = 0.776212215423584 + 0.1 * 6.732852935791016
Epoch 210, val loss: 1.012117862701416
Epoch 220, training loss: 1.3882784843444824 = 0.7160991430282593 + 0.1 * 6.7217936515808105
Epoch 220, val loss: 0.9748623371124268
Epoch 230, training loss: 1.3323111534118652 = 0.6615897417068481 + 0.1 * 6.707213401794434
Epoch 230, val loss: 0.9437106847763062
Epoch 240, training loss: 1.282414197921753 = 0.6117199659347534 + 0.1 * 6.706943035125732
Epoch 240, val loss: 0.9179213643074036
Epoch 250, training loss: 1.2347981929779053 = 0.5660409927368164 + 0.1 * 6.687572002410889
Epoch 250, val loss: 0.8968160152435303
Epoch 260, training loss: 1.19191575050354 = 0.5235482454299927 + 0.1 * 6.683675765991211
Epoch 260, val loss: 0.8791615962982178
Epoch 270, training loss: 1.1508734226226807 = 0.4838249385356903 + 0.1 * 6.67048454284668
Epoch 270, val loss: 0.864421010017395
Epoch 280, training loss: 1.112095594406128 = 0.4464542865753174 + 0.1 * 6.6564130783081055
Epoch 280, val loss: 0.8520663976669312
Epoch 290, training loss: 1.076257586479187 = 0.41125935316085815 + 0.1 * 6.649982452392578
Epoch 290, val loss: 0.8422091603279114
Epoch 300, training loss: 1.042368769645691 = 0.37806013226509094 + 0.1 * 6.6430864334106445
Epoch 300, val loss: 0.834591269493103
Epoch 310, training loss: 1.0111762285232544 = 0.3468259572982788 + 0.1 * 6.643502712249756
Epoch 310, val loss: 0.8290919065475464
Epoch 320, training loss: 0.9804503917694092 = 0.3175599277019501 + 0.1 * 6.628904819488525
Epoch 320, val loss: 0.8255800008773804
Epoch 330, training loss: 0.9524948000907898 = 0.2899606227874756 + 0.1 * 6.625341892242432
Epoch 330, val loss: 0.8240633010864258
Epoch 340, training loss: 0.9259481430053711 = 0.2640259265899658 + 0.1 * 6.619222164154053
Epoch 340, val loss: 0.8246193528175354
Epoch 350, training loss: 0.9017477035522461 = 0.23980018496513367 + 0.1 * 6.619475364685059
Epoch 350, val loss: 0.827392578125
Epoch 360, training loss: 0.8777363300323486 = 0.2173667848110199 + 0.1 * 6.603694915771484
Epoch 360, val loss: 0.832254946231842
Epoch 370, training loss: 0.8569278717041016 = 0.1967199444770813 + 0.1 * 6.602078914642334
Epoch 370, val loss: 0.839231550693512
Epoch 380, training loss: 0.837388277053833 = 0.17796070873737335 + 0.1 * 6.59427547454834
Epoch 380, val loss: 0.8482321500778198
Epoch 390, training loss: 0.8202742338180542 = 0.16111275553703308 + 0.1 * 6.591615200042725
Epoch 390, val loss: 0.8587840795516968
Epoch 400, training loss: 0.8039239645004272 = 0.14600656926631927 + 0.1 * 6.579173564910889
Epoch 400, val loss: 0.8707444667816162
Epoch 410, training loss: 0.7922746539115906 = 0.13255639374256134 + 0.1 * 6.597182750701904
Epoch 410, val loss: 0.8837591409683228
Epoch 420, training loss: 0.7783178091049194 = 0.12065135687589645 + 0.1 * 6.576664447784424
Epoch 420, val loss: 0.8974037170410156
Epoch 430, training loss: 0.7675125598907471 = 0.11006714403629303 + 0.1 * 6.574454307556152
Epoch 430, val loss: 0.9116963744163513
Epoch 440, training loss: 0.7567098140716553 = 0.10065848380327225 + 0.1 * 6.560513496398926
Epoch 440, val loss: 0.9263003468513489
Epoch 450, training loss: 0.7522111535072327 = 0.09225780516862869 + 0.1 * 6.599533557891846
Epoch 450, val loss: 0.9411750435829163
Epoch 460, training loss: 0.7403619885444641 = 0.08478975296020508 + 0.1 * 6.555722236633301
Epoch 460, val loss: 0.9559779167175293
Epoch 470, training loss: 0.7326885461807251 = 0.07809967547655106 + 0.1 * 6.545888423919678
Epoch 470, val loss: 0.9708075523376465
Epoch 480, training loss: 0.7270869612693787 = 0.07208903133869171 + 0.1 * 6.549979209899902
Epoch 480, val loss: 0.9855095744132996
Epoch 490, training loss: 0.720803439617157 = 0.06668642908334732 + 0.1 * 6.5411696434021
Epoch 490, val loss: 0.9999738931655884
Epoch 500, training loss: 0.7155314683914185 = 0.06180444359779358 + 0.1 * 6.537269592285156
Epoch 500, val loss: 1.0143448114395142
Epoch 510, training loss: 0.711571216583252 = 0.05740383639931679 + 0.1 * 6.541673183441162
Epoch 510, val loss: 1.028473138809204
Epoch 520, training loss: 0.7057742476463318 = 0.053426723927259445 + 0.1 * 6.52347469329834
Epoch 520, val loss: 1.0421828031539917
Epoch 530, training loss: 0.7024313807487488 = 0.04982499033212662 + 0.1 * 6.526063442230225
Epoch 530, val loss: 1.0557198524475098
Epoch 540, training loss: 0.6988121271133423 = 0.04656052216887474 + 0.1 * 6.522515773773193
Epoch 540, val loss: 1.0687800645828247
Epoch 550, training loss: 0.6968189477920532 = 0.04358812794089317 + 0.1 * 6.532308578491211
Epoch 550, val loss: 1.0816633701324463
Epoch 560, training loss: 0.6919214129447937 = 0.04087752103805542 + 0.1 * 6.510438919067383
Epoch 560, val loss: 1.094199776649475
Epoch 570, training loss: 0.6893951296806335 = 0.03839706629514694 + 0.1 * 6.509980201721191
Epoch 570, val loss: 1.1065281629562378
Epoch 580, training loss: 0.6868664026260376 = 0.03612421080470085 + 0.1 * 6.507421493530273
Epoch 580, val loss: 1.118625521659851
Epoch 590, training loss: 0.6854254007339478 = 0.03404659405350685 + 0.1 * 6.513787746429443
Epoch 590, val loss: 1.130339503288269
Epoch 600, training loss: 0.6824904680252075 = 0.03214164450764656 + 0.1 * 6.503488540649414
Epoch 600, val loss: 1.1417187452316284
Epoch 610, training loss: 0.679503321647644 = 0.030387839302420616 + 0.1 * 6.49115514755249
Epoch 610, val loss: 1.1528692245483398
Epoch 620, training loss: 0.6778873205184937 = 0.028767602518200874 + 0.1 * 6.491197109222412
Epoch 620, val loss: 1.1637656688690186
Epoch 630, training loss: 0.67719966173172 = 0.02726949006319046 + 0.1 * 6.499301910400391
Epoch 630, val loss: 1.174505591392517
Epoch 640, training loss: 0.6745853424072266 = 0.025886179879307747 + 0.1 * 6.4869914054870605
Epoch 640, val loss: 1.1848838329315186
Epoch 650, training loss: 0.6736417412757874 = 0.02459726296365261 + 0.1 * 6.490444660186768
Epoch 650, val loss: 1.195230484008789
Epoch 660, training loss: 0.6721200346946716 = 0.02340567111968994 + 0.1 * 6.487143516540527
Epoch 660, val loss: 1.2051628828048706
Epoch 670, training loss: 0.6698942184448242 = 0.02229367196559906 + 0.1 * 6.4760050773620605
Epoch 670, val loss: 1.214943528175354
Epoch 680, training loss: 0.6701530814170837 = 0.02125300094485283 + 0.1 * 6.4890007972717285
Epoch 680, val loss: 1.224652886390686
Epoch 690, training loss: 0.6695858240127563 = 0.020283980295062065 + 0.1 * 6.49301815032959
Epoch 690, val loss: 1.2341099977493286
Epoch 700, training loss: 0.6661362648010254 = 0.019378524273633957 + 0.1 * 6.46757698059082
Epoch 700, val loss: 1.2433249950408936
Epoch 710, training loss: 0.6660523414611816 = 0.018526611849665642 + 0.1 * 6.47525691986084
Epoch 710, val loss: 1.2525132894515991
Epoch 720, training loss: 0.6649613380432129 = 0.017732705920934677 + 0.1 * 6.472286701202393
Epoch 720, val loss: 1.261317491531372
Epoch 730, training loss: 0.6643469929695129 = 0.0169846024364233 + 0.1 * 6.473624229431152
Epoch 730, val loss: 1.2701884508132935
Epoch 740, training loss: 0.662380039691925 = 0.0162858534604311 + 0.1 * 6.460941791534424
Epoch 740, val loss: 1.2786904573440552
Epoch 750, training loss: 0.662479817867279 = 0.01562882587313652 + 0.1 * 6.468509674072266
Epoch 750, val loss: 1.2871671915054321
Epoch 760, training loss: 0.6602420210838318 = 0.015011514537036419 + 0.1 * 6.452305316925049
Epoch 760, val loss: 1.2954341173171997
Epoch 770, training loss: 0.659379780292511 = 0.014433410950005054 + 0.1 * 6.449463844299316
Epoch 770, val loss: 1.3034093379974365
Epoch 780, training loss: 0.6595315933227539 = 0.01388612575829029 + 0.1 * 6.456454277038574
Epoch 780, val loss: 1.3113737106323242
Epoch 790, training loss: 0.6587058305740356 = 0.01337172370404005 + 0.1 * 6.453341007232666
Epoch 790, val loss: 1.319089651107788
Epoch 800, training loss: 0.6586726903915405 = 0.012885785661637783 + 0.1 * 6.457868576049805
Epoch 800, val loss: 1.3267148733139038
Epoch 810, training loss: 0.656589150428772 = 0.012427770532667637 + 0.1 * 6.441613674163818
Epoch 810, val loss: 1.3341178894042969
Epoch 820, training loss: 0.6564993262290955 = 0.011994343250989914 + 0.1 * 6.44504976272583
Epoch 820, val loss: 1.3414167165756226
Epoch 830, training loss: 0.6555418372154236 = 0.011583245359361172 + 0.1 * 6.4395856857299805
Epoch 830, val loss: 1.348587155342102
Epoch 840, training loss: 0.6549666523933411 = 0.011193844489753246 + 0.1 * 6.437727928161621
Epoch 840, val loss: 1.3557140827178955
Epoch 850, training loss: 0.6547738909721375 = 0.010826300829648972 + 0.1 * 6.439476013183594
Epoch 850, val loss: 1.3625017404556274
Epoch 860, training loss: 0.6549236178398132 = 0.010476862080395222 + 0.1 * 6.444467067718506
Epoch 860, val loss: 1.3692866563796997
Epoch 870, training loss: 0.6537576913833618 = 0.010145452804863453 + 0.1 * 6.436122417449951
Epoch 870, val loss: 1.3758608102798462
Epoch 880, training loss: 0.6533877849578857 = 0.009829595685005188 + 0.1 * 6.435582160949707
Epoch 880, val loss: 1.3824803829193115
Epoch 890, training loss: 0.6528453826904297 = 0.009531048126518726 + 0.1 * 6.433143615722656
Epoch 890, val loss: 1.3887529373168945
Epoch 900, training loss: 0.652100145816803 = 0.009245429188013077 + 0.1 * 6.428546905517578
Epoch 900, val loss: 1.3950576782226562
Epoch 910, training loss: 0.652349591255188 = 0.008973779156804085 + 0.1 * 6.43375825881958
Epoch 910, val loss: 1.401179313659668
Epoch 920, training loss: 0.6512479782104492 = 0.008714373223483562 + 0.1 * 6.425335884094238
Epoch 920, val loss: 1.4072846174240112
Epoch 930, training loss: 0.6516008377075195 = 0.00846725981682539 + 0.1 * 6.43133544921875
Epoch 930, val loss: 1.4132040739059448
Epoch 940, training loss: 0.6523311138153076 = 0.008231173269450665 + 0.1 * 6.440999507904053
Epoch 940, val loss: 1.4190760850906372
Epoch 950, training loss: 0.6502715945243835 = 0.00800620298832655 + 0.1 * 6.422653675079346
Epoch 950, val loss: 1.424757480621338
Epoch 960, training loss: 0.6503050327301025 = 0.0077910954132676125 + 0.1 * 6.425139427185059
Epoch 960, val loss: 1.430373191833496
Epoch 970, training loss: 0.650069534778595 = 0.007584933191537857 + 0.1 * 6.424846172332764
Epoch 970, val loss: 1.435970664024353
Epoch 980, training loss: 0.6494969129562378 = 0.007387581747025251 + 0.1 * 6.421092987060547
Epoch 980, val loss: 1.4414801597595215
Epoch 990, training loss: 0.648814857006073 = 0.007198474369943142 + 0.1 * 6.416163921356201
Epoch 990, val loss: 1.4468649625778198
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 2.814566135406494 = 1.9548834562301636 + 0.1 * 8.596827507019043
Epoch 0, val loss: 1.9453271627426147
Epoch 10, training loss: 2.804042339324951 = 1.9443690776824951 + 0.1 * 8.596733093261719
Epoch 10, val loss: 1.9353671073913574
Epoch 20, training loss: 2.7914698123931885 = 1.9318478107452393 + 0.1 * 8.596219062805176
Epoch 20, val loss: 1.9230881929397583
Epoch 30, training loss: 2.774277925491333 = 1.9150835275650024 + 0.1 * 8.591943740844727
Epoch 30, val loss: 1.9064651727676392
Epoch 40, training loss: 2.747354030609131 = 1.8910636901855469 + 0.1 * 8.56290340423584
Epoch 40, val loss: 1.8829057216644287
Epoch 50, training loss: 2.6990928649902344 = 1.8580363988876343 + 0.1 * 8.410565376281738
Epoch 50, val loss: 1.8522356748580933
Epoch 60, training loss: 2.6387860774993896 = 1.8203133344650269 + 0.1 * 8.184727668762207
Epoch 60, val loss: 1.8198162317276
Epoch 70, training loss: 2.5794219970703125 = 1.7851159572601318 + 0.1 * 7.943058967590332
Epoch 70, val loss: 1.791404128074646
Epoch 80, training loss: 2.5087945461273193 = 1.7502894401550293 + 0.1 * 7.5850510597229
Epoch 80, val loss: 1.7623704671859741
Epoch 90, training loss: 2.4388716220855713 = 1.7095916271209717 + 0.1 * 7.292800426483154
Epoch 90, val loss: 1.7287949323654175
Epoch 100, training loss: 2.368295669555664 = 1.6542657613754272 + 0.1 * 7.1402997970581055
Epoch 100, val loss: 1.682238221168518
Epoch 110, training loss: 2.28425931930542 = 1.5815776586532593 + 0.1 * 7.026817321777344
Epoch 110, val loss: 1.6206951141357422
Epoch 120, training loss: 2.195587158203125 = 1.4992008209228516 + 0.1 * 6.96386194229126
Epoch 120, val loss: 1.5541610717773438
Epoch 130, training loss: 2.109297275543213 = 1.4168559312820435 + 0.1 * 6.924412727355957
Epoch 130, val loss: 1.491782546043396
Epoch 140, training loss: 2.0298192501068115 = 1.3399728536605835 + 0.1 * 6.898463249206543
Epoch 140, val loss: 1.4366059303283691
Epoch 150, training loss: 1.956323504447937 = 1.2684193849563599 + 0.1 * 6.8790411949157715
Epoch 150, val loss: 1.3880919218063354
Epoch 160, training loss: 1.8867859840393066 = 1.2001334428787231 + 0.1 * 6.866524696350098
Epoch 160, val loss: 1.3444596529006958
Epoch 170, training loss: 1.8193635940551758 = 1.134192705154419 + 0.1 * 6.851708889007568
Epoch 170, val loss: 1.3036764860153198
Epoch 180, training loss: 1.7522509098052979 = 1.0678319931030273 + 0.1 * 6.844188213348389
Epoch 180, val loss: 1.2627094984054565
Epoch 190, training loss: 1.683489441871643 = 0.9997960329055786 + 0.1 * 6.8369340896606445
Epoch 190, val loss: 1.2191789150238037
Epoch 200, training loss: 1.6141756772994995 = 0.9308580160140991 + 0.1 * 6.833176612854004
Epoch 200, val loss: 1.1739964485168457
Epoch 210, training loss: 1.5449790954589844 = 0.862321674823761 + 0.1 * 6.826574325561523
Epoch 210, val loss: 1.1286250352859497
Epoch 220, training loss: 1.4764955043792725 = 0.7941936254501343 + 0.1 * 6.823019027709961
Epoch 220, val loss: 1.0841219425201416
Epoch 230, training loss: 1.4094732999801636 = 0.728051483631134 + 0.1 * 6.814218044281006
Epoch 230, val loss: 1.0428069829940796
Epoch 240, training loss: 1.3445777893066406 = 0.6641861200332642 + 0.1 * 6.8039164543151855
Epoch 240, val loss: 1.0056349039077759
Epoch 250, training loss: 1.283028244972229 = 0.6028821468353271 + 0.1 * 6.8014607429504395
Epoch 250, val loss: 0.9735639691352844
Epoch 260, training loss: 1.2245080471038818 = 0.5456628799438477 + 0.1 * 6.7884521484375
Epoch 260, val loss: 0.9474354386329651
Epoch 270, training loss: 1.1703464984893799 = 0.49286583065986633 + 0.1 * 6.774806976318359
Epoch 270, val loss: 0.926773190498352
Epoch 280, training loss: 1.1209121942520142 = 0.4445447325706482 + 0.1 * 6.763674736022949
Epoch 280, val loss: 0.9106824994087219
Epoch 290, training loss: 1.0774860382080078 = 0.40087223052978516 + 0.1 * 6.76613712310791
Epoch 290, val loss: 0.8983246684074402
Epoch 300, training loss: 1.0359241962432861 = 0.36147913336753845 + 0.1 * 6.744450569152832
Epoch 300, val loss: 0.8888229727745056
Epoch 310, training loss: 0.9991574883460999 = 0.3255375623703003 + 0.1 * 6.736198902130127
Epoch 310, val loss: 0.8817154765129089
Epoch 320, training loss: 0.9654468894004822 = 0.29297131299972534 + 0.1 * 6.724755764007568
Epoch 320, val loss: 0.8769482374191284
Epoch 330, training loss: 0.9353619813919067 = 0.26351863145828247 + 0.1 * 6.718433380126953
Epoch 330, val loss: 0.874266505241394
Epoch 340, training loss: 0.9081826210021973 = 0.23682841658592224 + 0.1 * 6.713542461395264
Epoch 340, val loss: 0.8738512396812439
Epoch 350, training loss: 0.8833184242248535 = 0.21288660168647766 + 0.1 * 6.704317569732666
Epoch 350, val loss: 0.8755267262458801
Epoch 360, training loss: 0.8602652549743652 = 0.191486656665802 + 0.1 * 6.687785625457764
Epoch 360, val loss: 0.8790389895439148
Epoch 370, training loss: 0.840798556804657 = 0.17243246734142303 + 0.1 * 6.683660507202148
Epoch 370, val loss: 0.8842955827713013
Epoch 380, training loss: 0.8229652643203735 = 0.15556327998638153 + 0.1 * 6.674019813537598
Epoch 380, val loss: 0.8911412358283997
Epoch 390, training loss: 0.8113542199134827 = 0.14062917232513428 + 0.1 * 6.707250595092773
Epoch 390, val loss: 0.8993592858314514
Epoch 400, training loss: 0.7927054166793823 = 0.12755045294761658 + 0.1 * 6.651549339294434
Epoch 400, val loss: 0.9086318612098694
Epoch 410, training loss: 0.7799142599105835 = 0.11600280553102493 + 0.1 * 6.6391143798828125
Epoch 410, val loss: 0.9188530445098877
Epoch 420, training loss: 0.7725754976272583 = 0.10577099770307541 + 0.1 * 6.6680450439453125
Epoch 420, val loss: 0.9299730062484741
Epoch 430, training loss: 0.7598900198936462 = 0.09674956649541855 + 0.1 * 6.631404399871826
Epoch 430, val loss: 0.9417698383331299
Epoch 440, training loss: 0.7501291036605835 = 0.08875459432601929 + 0.1 * 6.613744735717773
Epoch 440, val loss: 0.9538806080818176
Epoch 450, training loss: 0.7425832152366638 = 0.08163953572511673 + 0.1 * 6.609436988830566
Epoch 450, val loss: 0.9664580225944519
Epoch 460, training loss: 0.7349687218666077 = 0.07526284456253052 + 0.1 * 6.5970587730407715
Epoch 460, val loss: 0.9794654250144958
Epoch 470, training loss: 0.7305160760879517 = 0.0695454478263855 + 0.1 * 6.609706401824951
Epoch 470, val loss: 0.9924761652946472
Epoch 480, training loss: 0.7227683067321777 = 0.0644325390458107 + 0.1 * 6.583357334136963
Epoch 480, val loss: 1.005768895149231
Epoch 490, training loss: 0.7179190516471863 = 0.059816181659698486 + 0.1 * 6.581028461456299
Epoch 490, val loss: 1.019130825996399
Epoch 500, training loss: 0.714717447757721 = 0.055647075176239014 + 0.1 * 6.59070348739624
Epoch 500, val loss: 1.0323545932769775
Epoch 510, training loss: 0.7094845175743103 = 0.05189124867320061 + 0.1 * 6.575932502746582
Epoch 510, val loss: 1.0457401275634766
Epoch 520, training loss: 0.7042872905731201 = 0.04847652465105057 + 0.1 * 6.558107376098633
Epoch 520, val loss: 1.058928370475769
Epoch 530, training loss: 0.7024132013320923 = 0.04536176472902298 + 0.1 * 6.57051420211792
Epoch 530, val loss: 1.0720789432525635
Epoch 540, training loss: 0.6982765197753906 = 0.04252251982688904 + 0.1 * 6.557540416717529
Epoch 540, val loss: 1.085205316543579
Epoch 550, training loss: 0.6961293816566467 = 0.039927009493112564 + 0.1 * 6.562023639678955
Epoch 550, val loss: 1.0980217456817627
Epoch 560, training loss: 0.6918526887893677 = 0.03755814582109451 + 0.1 * 6.542945384979248
Epoch 560, val loss: 1.1108312606811523
Epoch 570, training loss: 0.6898491382598877 = 0.035378262400627136 + 0.1 * 6.544708251953125
Epoch 570, val loss: 1.123382329940796
Epoch 580, training loss: 0.68733811378479 = 0.03337474167346954 + 0.1 * 6.539633750915527
Epoch 580, val loss: 1.1357944011688232
Epoch 590, training loss: 0.6864364147186279 = 0.031528376042842865 + 0.1 * 6.5490803718566895
Epoch 590, val loss: 1.148023247718811
Epoch 600, training loss: 0.6832939386367798 = 0.02983146533370018 + 0.1 * 6.5346245765686035
Epoch 600, val loss: 1.1600265502929688
Epoch 610, training loss: 0.6808930039405823 = 0.028263123705983162 + 0.1 * 6.526298522949219
Epoch 610, val loss: 1.171858549118042
Epoch 620, training loss: 0.6794162392616272 = 0.02681099809706211 + 0.1 * 6.526052474975586
Epoch 620, val loss: 1.1834216117858887
Epoch 630, training loss: 0.6793121695518494 = 0.025466224178671837 + 0.1 * 6.538459300994873
Epoch 630, val loss: 1.19489586353302
Epoch 640, training loss: 0.6760939955711365 = 0.024220624938607216 + 0.1 * 6.518733501434326
Epoch 640, val loss: 1.2060942649841309
Epoch 650, training loss: 0.674705982208252 = 0.023065341636538506 + 0.1 * 6.516406536102295
Epoch 650, val loss: 1.2171416282653809
Epoch 660, training loss: 0.6726597547531128 = 0.021986814215779305 + 0.1 * 6.5067291259765625
Epoch 660, val loss: 1.2280147075653076
Epoch 670, training loss: 0.673896074295044 = 0.020980851724743843 + 0.1 * 6.5291523933410645
Epoch 670, val loss: 1.238477349281311
Epoch 680, training loss: 0.6720119714736938 = 0.020048100501298904 + 0.1 * 6.519638538360596
Epoch 680, val loss: 1.2489848136901855
Epoch 690, training loss: 0.6690678596496582 = 0.019176313653588295 + 0.1 * 6.498915195465088
Epoch 690, val loss: 1.259192943572998
Epoch 700, training loss: 0.669346809387207 = 0.018357720226049423 + 0.1 * 6.509891033172607
Epoch 700, val loss: 1.269124150276184
Epoch 710, training loss: 0.6671199202537537 = 0.01759149692952633 + 0.1 * 6.495284557342529
Epoch 710, val loss: 1.2790623903274536
Epoch 720, training loss: 0.66633141040802 = 0.016872074455022812 + 0.1 * 6.494593620300293
Epoch 720, val loss: 1.288610577583313
Epoch 730, training loss: 0.665539562702179 = 0.016198350116610527 + 0.1 * 6.493412017822266
Epoch 730, val loss: 1.2979238033294678
Epoch 740, training loss: 0.6641899347305298 = 0.015564930625259876 + 0.1 * 6.486249923706055
Epoch 740, val loss: 1.307306170463562
Epoch 750, training loss: 0.6646729111671448 = 0.01496528834104538 + 0.1 * 6.497076034545898
Epoch 750, val loss: 1.3163604736328125
Epoch 760, training loss: 0.6632330417633057 = 0.014400646090507507 + 0.1 * 6.48832368850708
Epoch 760, val loss: 1.325219988822937
Epoch 770, training loss: 0.6628687381744385 = 0.013866073451936245 + 0.1 * 6.490026473999023
Epoch 770, val loss: 1.3340656757354736
Epoch 780, training loss: 0.6617023348808289 = 0.013361325487494469 + 0.1 * 6.483409881591797
Epoch 780, val loss: 1.342557668685913
Epoch 790, training loss: 0.6618992686271667 = 0.012883175164461136 + 0.1 * 6.4901604652404785
Epoch 790, val loss: 1.3511877059936523
Epoch 800, training loss: 0.6602404117584229 = 0.01243266649544239 + 0.1 * 6.478076934814453
Epoch 800, val loss: 1.3592959642410278
Epoch 810, training loss: 0.6597912311553955 = 0.012005127966403961 + 0.1 * 6.477860450744629
Epoch 810, val loss: 1.367509365081787
Epoch 820, training loss: 0.6589595675468445 = 0.011598888784646988 + 0.1 * 6.473606586456299
Epoch 820, val loss: 1.375545859336853
Epoch 830, training loss: 0.6577335000038147 = 0.011214764788746834 + 0.1 * 6.465187072753906
Epoch 830, val loss: 1.383292317390442
Epoch 840, training loss: 0.6575436592102051 = 0.010849989950656891 + 0.1 * 6.4669365882873535
Epoch 840, val loss: 1.3911992311477661
Epoch 850, training loss: 0.6564024090766907 = 0.010503103025257587 + 0.1 * 6.458992958068848
Epoch 850, val loss: 1.3987542390823364
Epoch 860, training loss: 0.6591213941574097 = 0.010172579437494278 + 0.1 * 6.489487648010254
Epoch 860, val loss: 1.4062293767929077
Epoch 870, training loss: 0.6566914916038513 = 0.009861971251666546 + 0.1 * 6.468295574188232
Epoch 870, val loss: 1.4133909940719604
Epoch 880, training loss: 0.6580067873001099 = 0.009563436731696129 + 0.1 * 6.484433174133301
Epoch 880, val loss: 1.420660376548767
Epoch 890, training loss: 0.6554844379425049 = 0.009281228296458721 + 0.1 * 6.462032318115234
Epoch 890, val loss: 1.427595615386963
Epoch 900, training loss: 0.654161810874939 = 0.00901127140969038 + 0.1 * 6.451505184173584
Epoch 900, val loss: 1.4346188306808472
Epoch 910, training loss: 0.6574411988258362 = 0.008752726018428802 + 0.1 * 6.486884593963623
Epoch 910, val loss: 1.4413373470306396
Epoch 920, training loss: 0.6545406579971313 = 0.00850743893533945 + 0.1 * 6.46033239364624
Epoch 920, val loss: 1.4477356672286987
Epoch 930, training loss: 0.6527904868125916 = 0.008273125626146793 + 0.1 * 6.445173740386963
Epoch 930, val loss: 1.4545139074325562
Epoch 940, training loss: 0.6557455658912659 = 0.008048455230891705 + 0.1 * 6.476970672607422
Epoch 940, val loss: 1.4608681201934814
Epoch 950, training loss: 0.6523100733757019 = 0.007833763025701046 + 0.1 * 6.444762706756592
Epoch 950, val loss: 1.4669339656829834
Epoch 960, training loss: 0.65287184715271 = 0.007628091610968113 + 0.1 * 6.452437400817871
Epoch 960, val loss: 1.4732826948165894
Epoch 970, training loss: 0.6514536738395691 = 0.00743121886625886 + 0.1 * 6.440224647521973
Epoch 970, val loss: 1.4793232679367065
Epoch 980, training loss: 0.6512277722358704 = 0.0072417715564370155 + 0.1 * 6.439859867095947
Epoch 980, val loss: 1.4851047992706299
Epoch 990, training loss: 0.6509301066398621 = 0.007061269599944353 + 0.1 * 6.438688278198242
Epoch 990, val loss: 1.4910049438476562
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 2.8208553791046143 = 1.961172342300415 + 0.1 * 8.596830368041992
Epoch 0, val loss: 1.965474247932434
Epoch 10, training loss: 2.8105573654174805 = 1.9508814811706543 + 0.1 * 8.596757888793945
Epoch 10, val loss: 1.9554495811462402
Epoch 20, training loss: 2.797732353210449 = 1.938094139099121 + 0.1 * 8.596382141113281
Epoch 20, val loss: 1.9423778057098389
Epoch 30, training loss: 2.7793102264404297 = 1.9199947118759155 + 0.1 * 8.593153953552246
Epoch 30, val loss: 1.9233628511428833
Epoch 40, training loss: 2.7499892711639404 = 1.89310884475708 + 0.1 * 8.568803787231445
Epoch 40, val loss: 1.8951141834259033
Epoch 50, training loss: 2.701669692993164 = 1.8558974266052246 + 0.1 * 8.457723617553711
Epoch 50, val loss: 1.8576247692108154
Epoch 60, training loss: 2.6386327743530273 = 1.816219687461853 + 0.1 * 8.224130630493164
Epoch 60, val loss: 1.8215044736862183
Epoch 70, training loss: 2.5861032009124756 = 1.7856731414794922 + 0.1 * 8.004301071166992
Epoch 70, val loss: 1.7961868047714233
Epoch 80, training loss: 2.510573148727417 = 1.7575305700302124 + 0.1 * 7.530425548553467
Epoch 80, val loss: 1.7710013389587402
Epoch 90, training loss: 2.4416913986206055 = 1.7223361730575562 + 0.1 * 7.193552494049072
Epoch 90, val loss: 1.7409082651138306
Epoch 100, training loss: 2.381683588027954 = 1.6755218505859375 + 0.1 * 7.061617851257324
Epoch 100, val loss: 1.7027151584625244
Epoch 110, training loss: 2.311262369155884 = 1.6118932962417603 + 0.1 * 6.9936909675598145
Epoch 110, val loss: 1.650275707244873
Epoch 120, training loss: 2.22947096824646 = 1.534500002861023 + 0.1 * 6.949709892272949
Epoch 120, val loss: 1.5866236686706543
Epoch 130, training loss: 2.142646312713623 = 1.4504042863845825 + 0.1 * 6.922420024871826
Epoch 130, val loss: 1.5187615156173706
Epoch 140, training loss: 2.0551180839538574 = 1.3650851249694824 + 0.1 * 6.900328636169434
Epoch 140, val loss: 1.451693058013916
Epoch 150, training loss: 1.968808889389038 = 1.2808538675308228 + 0.1 * 6.879549503326416
Epoch 150, val loss: 1.3859022855758667
Epoch 160, training loss: 1.8841617107391357 = 1.1984435319900513 + 0.1 * 6.857181072235107
Epoch 160, val loss: 1.3222674131393433
Epoch 170, training loss: 1.8022726774215698 = 1.1191847324371338 + 0.1 * 6.830879211425781
Epoch 170, val loss: 1.2623093128204346
Epoch 180, training loss: 1.728151559829712 = 1.0469183921813965 + 0.1 * 6.812331199645996
Epoch 180, val loss: 1.2095966339111328
Epoch 190, training loss: 1.6632441282272339 = 0.9838125705718994 + 0.1 * 6.794315338134766
Epoch 190, val loss: 1.1655981540679932
Epoch 200, training loss: 1.6047559976577759 = 0.9267024397850037 + 0.1 * 6.780535697937012
Epoch 200, val loss: 1.1262502670288086
Epoch 210, training loss: 1.55051851272583 = 0.8733895421028137 + 0.1 * 6.771290302276611
Epoch 210, val loss: 1.0896289348602295
Epoch 220, training loss: 1.4977846145629883 = 0.8214830160140991 + 0.1 * 6.763016700744629
Epoch 220, val loss: 1.0537296533584595
Epoch 230, training loss: 1.4438233375549316 = 0.7687327861785889 + 0.1 * 6.7509050369262695
Epoch 230, val loss: 1.0170029401779175
Epoch 240, training loss: 1.3903467655181885 = 0.7149538993835449 + 0.1 * 6.753927707672119
Epoch 240, val loss: 0.9795112609863281
Epoch 250, training loss: 1.3372297286987305 = 0.6625736355781555 + 0.1 * 6.746561527252197
Epoch 250, val loss: 0.9436015486717224
Epoch 260, training loss: 1.284938097000122 = 0.6121120452880859 + 0.1 * 6.7282609939575195
Epoch 260, val loss: 0.9094821214675903
Epoch 270, training loss: 1.2364641427993774 = 0.5643287897109985 + 0.1 * 6.721353530883789
Epoch 270, val loss: 0.8784055113792419
Epoch 280, training loss: 1.1909658908843994 = 0.5193734765052795 + 0.1 * 6.715924263000488
Epoch 280, val loss: 0.8509852886199951
Epoch 290, training loss: 1.14976167678833 = 0.4773392081260681 + 0.1 * 6.724225044250488
Epoch 290, val loss: 0.8273959755897522
Epoch 300, training loss: 1.1077245473861694 = 0.4374719560146332 + 0.1 * 6.702526092529297
Epoch 300, val loss: 0.8072337508201599
Epoch 310, training loss: 1.0679750442504883 = 0.39862069487571716 + 0.1 * 6.693543910980225
Epoch 310, val loss: 0.7897445559501648
Epoch 320, training loss: 1.029709815979004 = 0.36051177978515625 + 0.1 * 6.69197940826416
Epoch 320, val loss: 0.7743518352508545
Epoch 330, training loss: 0.9921178221702576 = 0.3238186240196228 + 0.1 * 6.682991981506348
Epoch 330, val loss: 0.761164128780365
Epoch 340, training loss: 0.9565897583961487 = 0.28919553756713867 + 0.1 * 6.6739420890808105
Epoch 340, val loss: 0.7503347992897034
Epoch 350, training loss: 0.9254434108734131 = 0.2572079598903656 + 0.1 * 6.682353973388672
Epoch 350, val loss: 0.7421738505363464
Epoch 360, training loss: 0.8952966332435608 = 0.22850997745990753 + 0.1 * 6.6678667068481445
Epoch 360, val loss: 0.7369223237037659
Epoch 370, training loss: 0.8695840239524841 = 0.20318572223186493 + 0.1 * 6.66398286819458
Epoch 370, val loss: 0.7344169020652771
Epoch 380, training loss: 0.8491102457046509 = 0.1810850203037262 + 0.1 * 6.6802520751953125
Epoch 380, val loss: 0.734512984752655
Epoch 390, training loss: 0.8269652128219604 = 0.16202399134635925 + 0.1 * 6.649412631988525
Epoch 390, val loss: 0.7368218302726746
Epoch 400, training loss: 0.8100796937942505 = 0.1455298662185669 + 0.1 * 6.645498275756836
Epoch 400, val loss: 0.7408055663108826
Epoch 410, training loss: 0.796285092830658 = 0.13118861615657806 + 0.1 * 6.650964736938477
Epoch 410, val loss: 0.7462498545646667
Epoch 420, training loss: 0.7830408215522766 = 0.11872200667858124 + 0.1 * 6.643187999725342
Epoch 420, val loss: 0.7528485059738159
Epoch 430, training loss: 0.7708132863044739 = 0.10781230032444 + 0.1 * 6.630009651184082
Epoch 430, val loss: 0.7603012919425964
Epoch 440, training loss: 0.7604480385780334 = 0.09821759909391403 + 0.1 * 6.6223039627075195
Epoch 440, val loss: 0.7684170007705688
Epoch 450, training loss: 0.7513912916183472 = 0.08973145484924316 + 0.1 * 6.616598129272461
Epoch 450, val loss: 0.7770692110061646
Epoch 460, training loss: 0.744515597820282 = 0.08217106014490128 + 0.1 * 6.6234450340271
Epoch 460, val loss: 0.7861322164535522
Epoch 470, training loss: 0.7370137572288513 = 0.07542963325977325 + 0.1 * 6.615841388702393
Epoch 470, val loss: 0.7956016659736633
Epoch 480, training loss: 0.7298939228057861 = 0.06939288973808289 + 0.1 * 6.605010032653809
Epoch 480, val loss: 0.8052517771720886
Epoch 490, training loss: 0.7237511873245239 = 0.06398110836744308 + 0.1 * 6.597700595855713
Epoch 490, val loss: 0.8149718046188354
Epoch 500, training loss: 0.7185630798339844 = 0.05912313982844353 + 0.1 * 6.5943989753723145
Epoch 500, val loss: 0.8249724507331848
Epoch 510, training loss: 0.713837742805481 = 0.05473378300666809 + 0.1 * 6.591039657592773
Epoch 510, val loss: 0.8349160552024841
Epoch 520, training loss: 0.7097457051277161 = 0.05076064541935921 + 0.1 * 6.589850902557373
Epoch 520, val loss: 0.8449279069900513
Epoch 530, training loss: 0.7068088054656982 = 0.047169409692287445 + 0.1 * 6.596394062042236
Epoch 530, val loss: 0.8548679947853088
Epoch 540, training loss: 0.7012901306152344 = 0.04392238333821297 + 0.1 * 6.5736775398254395
Epoch 540, val loss: 0.8649037480354309
Epoch 550, training loss: 0.6983816027641296 = 0.04096483811736107 + 0.1 * 6.574167251586914
Epoch 550, val loss: 0.8747130036354065
Epoch 560, training loss: 0.6963063478469849 = 0.03827064856886864 + 0.1 * 6.580356597900391
Epoch 560, val loss: 0.8843709826469421
Epoch 570, training loss: 0.6930519342422485 = 0.03582591190934181 + 0.1 * 6.572259902954102
Epoch 570, val loss: 0.8941519260406494
Epoch 580, training loss: 0.6897079348564148 = 0.03359456732869148 + 0.1 * 6.561133861541748
Epoch 580, val loss: 0.9036388993263245
Epoch 590, training loss: 0.6880462169647217 = 0.0315491184592247 + 0.1 * 6.564970970153809
Epoch 590, val loss: 0.9130879640579224
Epoch 600, training loss: 0.6860563158988953 = 0.029675466939806938 + 0.1 * 6.563807964324951
Epoch 600, val loss: 0.9222601652145386
Epoch 610, training loss: 0.6835341453552246 = 0.027959102764725685 + 0.1 * 6.555750370025635
Epoch 610, val loss: 0.9315256476402283
Epoch 620, training loss: 0.6820347905158997 = 0.026379982009530067 + 0.1 * 6.55654764175415
Epoch 620, val loss: 0.940398097038269
Epoch 630, training loss: 0.6800758838653564 = 0.024929523468017578 + 0.1 * 6.551463603973389
Epoch 630, val loss: 0.9493392109870911
Epoch 640, training loss: 0.6775360703468323 = 0.023591112345457077 + 0.1 * 6.539449691772461
Epoch 640, val loss: 0.9579840898513794
Epoch 650, training loss: 0.6772962808609009 = 0.022353321313858032 + 0.1 * 6.549428939819336
Epoch 650, val loss: 0.9664239883422852
Epoch 660, training loss: 0.6770803332328796 = 0.021210569888353348 + 0.1 * 6.558697700500488
Epoch 660, val loss: 0.9746986627578735
Epoch 670, training loss: 0.6743030548095703 = 0.02015460655093193 + 0.1 * 6.541484355926514
Epoch 670, val loss: 0.9829695224761963
Epoch 680, training loss: 0.671376645565033 = 0.019174890592694283 + 0.1 * 6.522017478942871
Epoch 680, val loss: 0.990939199924469
Epoch 690, training loss: 0.6725248694419861 = 0.018263813108205795 + 0.1 * 6.5426106452941895
Epoch 690, val loss: 0.9985540509223938
Epoch 700, training loss: 0.6707611680030823 = 0.01742088608443737 + 0.1 * 6.533402919769287
Epoch 700, val loss: 1.0063656568527222
Epoch 710, training loss: 0.6686966419219971 = 0.016636094078421593 + 0.1 * 6.520605564117432
Epoch 710, val loss: 1.0138729810714722
Epoch 720, training loss: 0.6682044267654419 = 0.015902960672974586 + 0.1 * 6.523014545440674
Epoch 720, val loss: 1.0212205648422241
Epoch 730, training loss: 0.6669256687164307 = 0.0152186444029212 + 0.1 * 6.517070293426514
Epoch 730, val loss: 1.0283180475234985
Epoch 740, training loss: 0.666085422039032 = 0.0145801967009902 + 0.1 * 6.515052318572998
Epoch 740, val loss: 1.0355607271194458
Epoch 750, training loss: 0.6648411750793457 = 0.013981036841869354 + 0.1 * 6.508601188659668
Epoch 750, val loss: 1.0422817468643188
Epoch 760, training loss: 0.6647395491600037 = 0.013420043513178825 + 0.1 * 6.513195037841797
Epoch 760, val loss: 1.0492496490478516
Epoch 770, training loss: 0.6638017892837524 = 0.012892210856080055 + 0.1 * 6.509096145629883
Epoch 770, val loss: 1.0556635856628418
Epoch 780, training loss: 0.662219762802124 = 0.012397357262670994 + 0.1 * 6.498224258422852
Epoch 780, val loss: 1.06234872341156
Epoch 790, training loss: 0.6619086861610413 = 0.011930721811950207 + 0.1 * 6.49977970123291
Epoch 790, val loss: 1.068518042564392
Epoch 800, training loss: 0.6614345908164978 = 0.011491723358631134 + 0.1 * 6.499428749084473
Epoch 800, val loss: 1.0749027729034424
Epoch 810, training loss: 0.6619001030921936 = 0.011077839881181717 + 0.1 * 6.508222579956055
Epoch 810, val loss: 1.0808740854263306
Epoch 820, training loss: 0.6597636342048645 = 0.010687976144254208 + 0.1 * 6.490756988525391
Epoch 820, val loss: 1.0869412422180176
Epoch 830, training loss: 0.660879373550415 = 0.010318966582417488 + 0.1 * 6.505603790283203
Epoch 830, val loss: 1.0926380157470703
Epoch 840, training loss: 0.6596039533615112 = 0.009971607476472855 + 0.1 * 6.496323108673096
Epoch 840, val loss: 1.098439335823059
Epoch 850, training loss: 0.6578673720359802 = 0.009642506018280983 + 0.1 * 6.482248783111572
Epoch 850, val loss: 1.1041922569274902
Epoch 860, training loss: 0.6584895849227905 = 0.009329401887953281 + 0.1 * 6.491601943969727
Epoch 860, val loss: 1.1096192598342896
Epoch 870, training loss: 0.6567418575286865 = 0.00903222057968378 + 0.1 * 6.477096080780029
Epoch 870, val loss: 1.114884614944458
Epoch 880, training loss: 0.6569746732711792 = 0.008751116693019867 + 0.1 * 6.482235431671143
Epoch 880, val loss: 1.1203798055648804
Epoch 890, training loss: 0.6570569276809692 = 0.0084832813590765 + 0.1 * 6.48573637008667
Epoch 890, val loss: 1.1254606246948242
Epoch 900, training loss: 0.6559016704559326 = 0.00822936650365591 + 0.1 * 6.476722717285156
Epoch 900, val loss: 1.1306637525558472
Epoch 910, training loss: 0.6547622680664062 = 0.007987257093191147 + 0.1 * 6.467750072479248
Epoch 910, val loss: 1.1356006860733032
Epoch 920, training loss: 0.6558428406715393 = 0.007756806910037994 + 0.1 * 6.480859756469727
Epoch 920, val loss: 1.1405279636383057
Epoch 930, training loss: 0.6543903350830078 = 0.00753654632717371 + 0.1 * 6.4685378074646
Epoch 930, val loss: 1.1451432704925537
Epoch 940, training loss: 0.6542487144470215 = 0.00732757244259119 + 0.1 * 6.469211578369141
Epoch 940, val loss: 1.1500674486160278
Epoch 950, training loss: 0.6556453108787537 = 0.007127612363547087 + 0.1 * 6.485177040100098
Epoch 950, val loss: 1.1547353267669678
Epoch 960, training loss: 0.6536728739738464 = 0.006936340127140284 + 0.1 * 6.467365264892578
Epoch 960, val loss: 1.1589796543121338
Epoch 970, training loss: 0.6524986028671265 = 0.006754536647349596 + 0.1 * 6.4574408531188965
Epoch 970, val loss: 1.1637402772903442
Epoch 980, training loss: 0.654218316078186 = 0.0065798647701740265 + 0.1 * 6.476384162902832
Epoch 980, val loss: 1.1680736541748047
Epoch 990, training loss: 0.6517964601516724 = 0.0064120301976799965 + 0.1 * 6.45384407043457
Epoch 990, val loss: 1.1722432374954224
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8033737480231946
The final CL Acc:0.77778, 0.02181, The final GNN Acc:0.80619, 0.00361
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13276])
remove edge: torch.Size([2, 8002])
updated graph: torch.Size([2, 10722])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8072774410247803 = 1.9475939273834229 + 0.1 * 8.596835136413574
Epoch 0, val loss: 1.9522740840911865
Epoch 10, training loss: 2.7974205017089844 = 1.9377471208572388 + 0.1 * 8.596734046936035
Epoch 10, val loss: 1.943170189857483
Epoch 20, training loss: 2.7851600646972656 = 1.925542950630188 + 0.1 * 8.596171379089355
Epoch 20, val loss: 1.9316121339797974
Epoch 30, training loss: 2.7676796913146973 = 1.9085443019866943 + 0.1 * 8.591353416442871
Epoch 30, val loss: 1.915323257446289
Epoch 40, training loss: 2.7394556999206543 = 1.883699655532837 + 0.1 * 8.557560920715332
Epoch 40, val loss: 1.891760230064392
Epoch 50, training loss: 2.688703775405884 = 1.8496675491333008 + 0.1 * 8.390362739562988
Epoch 50, val loss: 1.8604850769042969
Epoch 60, training loss: 2.6154046058654785 = 1.8100517988204956 + 0.1 * 8.05352783203125
Epoch 60, val loss: 1.824822187423706
Epoch 70, training loss: 2.5515987873077393 = 1.7702207565307617 + 0.1 * 7.813780784606934
Epoch 70, val loss: 1.7875499725341797
Epoch 80, training loss: 2.480710506439209 = 1.7281378507614136 + 0.1 * 7.525727272033691
Epoch 80, val loss: 1.7458728551864624
Epoch 90, training loss: 2.403940200805664 = 1.6776719093322754 + 0.1 * 7.262683391571045
Epoch 90, val loss: 1.698121428489685
Epoch 100, training loss: 2.320180892944336 = 1.6120089292526245 + 0.1 * 7.081718921661377
Epoch 100, val loss: 1.6382588148117065
Epoch 110, training loss: 2.230926036834717 = 1.5295981168746948 + 0.1 * 7.013279914855957
Epoch 110, val loss: 1.5640541315078735
Epoch 120, training loss: 2.135540246963501 = 1.438766598701477 + 0.1 * 6.967735767364502
Epoch 120, val loss: 1.4837690591812134
Epoch 130, training loss: 2.0422372817993164 = 1.3485167026519775 + 0.1 * 6.937206268310547
Epoch 130, val loss: 1.405226230621338
Epoch 140, training loss: 1.9532761573791504 = 1.2615936994552612 + 0.1 * 6.916825294494629
Epoch 140, val loss: 1.3310965299606323
Epoch 150, training loss: 1.8697682619094849 = 1.1794962882995605 + 0.1 * 6.902719497680664
Epoch 150, val loss: 1.2630094289779663
Epoch 160, training loss: 1.7892382144927979 = 1.1009807586669922 + 0.1 * 6.88257360458374
Epoch 160, val loss: 1.199771523475647
Epoch 170, training loss: 1.7089625597000122 = 1.0227553844451904 + 0.1 * 6.862071514129639
Epoch 170, val loss: 1.138074278831482
Epoch 180, training loss: 1.6289942264556885 = 0.9441333413124084 + 0.1 * 6.848609447479248
Epoch 180, val loss: 1.0762981176376343
Epoch 190, training loss: 1.549835205078125 = 0.8671189546585083 + 0.1 * 6.827163219451904
Epoch 190, val loss: 1.0156829357147217
Epoch 200, training loss: 1.475066065788269 = 0.7932326197624207 + 0.1 * 6.818334102630615
Epoch 200, val loss: 0.9576505422592163
Epoch 210, training loss: 1.405935287475586 = 0.7257360816001892 + 0.1 * 6.8019914627075195
Epoch 210, val loss: 0.9063059091567993
Epoch 220, training loss: 1.3440699577331543 = 0.6650689244270325 + 0.1 * 6.790010929107666
Epoch 220, val loss: 0.8626163005828857
Epoch 230, training loss: 1.2887858152389526 = 0.6109740138053894 + 0.1 * 6.778118133544922
Epoch 230, val loss: 0.8278087377548218
Epoch 240, training loss: 1.2399075031280518 = 0.5633733868598938 + 0.1 * 6.765340805053711
Epoch 240, val loss: 0.8016446828842163
Epoch 250, training loss: 1.1960718631744385 = 0.5206866264343262 + 0.1 * 6.753852844238281
Epoch 250, val loss: 0.7825219631195068
Epoch 260, training loss: 1.156198263168335 = 0.4821926951408386 + 0.1 * 6.740055561065674
Epoch 260, val loss: 0.7689462900161743
Epoch 270, training loss: 1.1195955276489258 = 0.4468343257904053 + 0.1 * 6.727611064910889
Epoch 270, val loss: 0.7592363953590393
Epoch 280, training loss: 1.085713505744934 = 0.41364696621894836 + 0.1 * 6.720665454864502
Epoch 280, val loss: 0.7522074580192566
Epoch 290, training loss: 1.0536949634552002 = 0.38234028220176697 + 0.1 * 6.7135467529296875
Epoch 290, val loss: 0.7471113801002502
Epoch 300, training loss: 1.0240724086761475 = 0.3525491952896118 + 0.1 * 6.715232849121094
Epoch 300, val loss: 0.7435824871063232
Epoch 310, training loss: 0.9938793182373047 = 0.32438090443611145 + 0.1 * 6.69498348236084
Epoch 310, val loss: 0.7414270639419556
Epoch 320, training loss: 0.9671412706375122 = 0.2976509630680084 + 0.1 * 6.6949028968811035
Epoch 320, val loss: 0.7405261993408203
Epoch 330, training loss: 0.9405630826950073 = 0.27242884039878845 + 0.1 * 6.681342601776123
Epoch 330, val loss: 0.740810751914978
Epoch 340, training loss: 0.91685950756073 = 0.2486758679151535 + 0.1 * 6.681836128234863
Epoch 340, val loss: 0.7422679662704468
Epoch 350, training loss: 0.8935640454292297 = 0.22653323411941528 + 0.1 * 6.6703081130981445
Epoch 350, val loss: 0.744654655456543
Epoch 360, training loss: 0.8731720447540283 = 0.20605775713920593 + 0.1 * 6.671142578125
Epoch 360, val loss: 0.7480713129043579
Epoch 370, training loss: 0.8525750041007996 = 0.18732015788555145 + 0.1 * 6.652548313140869
Epoch 370, val loss: 0.7523901462554932
Epoch 380, training loss: 0.8364295959472656 = 0.17026977241039276 + 0.1 * 6.661597728729248
Epoch 380, val loss: 0.7575758099555969
Epoch 390, training loss: 0.8196665048599243 = 0.15493610501289368 + 0.1 * 6.647303581237793
Epoch 390, val loss: 0.7634394764900208
Epoch 400, training loss: 0.8049411773681641 = 0.1410938948392868 + 0.1 * 6.638472557067871
Epoch 400, val loss: 0.7700521349906921
Epoch 410, training loss: 0.7925382852554321 = 0.12859433889389038 + 0.1 * 6.639439105987549
Epoch 410, val loss: 0.7772824168205261
Epoch 420, training loss: 0.7805954217910767 = 0.11733639985322952 + 0.1 * 6.632590293884277
Epoch 420, val loss: 0.7850192785263062
Epoch 430, training loss: 0.7707754373550415 = 0.1071426197886467 + 0.1 * 6.636327743530273
Epoch 430, val loss: 0.7932009100914001
Epoch 440, training loss: 0.7595532536506653 = 0.0979468822479248 + 0.1 * 6.616063594818115
Epoch 440, val loss: 0.8016216158866882
Epoch 450, training loss: 0.751556396484375 = 0.08961938321590424 + 0.1 * 6.619369983673096
Epoch 450, val loss: 0.8104045391082764
Epoch 460, training loss: 0.7426093220710754 = 0.08206770569086075 + 0.1 * 6.6054158210754395
Epoch 460, val loss: 0.8194616436958313
Epoch 470, training loss: 0.735170841217041 = 0.07520279288291931 + 0.1 * 6.599679946899414
Epoch 470, val loss: 0.8288359642028809
Epoch 480, training loss: 0.7319362163543701 = 0.06897716224193573 + 0.1 * 6.6295905113220215
Epoch 480, val loss: 0.8384444713592529
Epoch 490, training loss: 0.723840594291687 = 0.06340525299310684 + 0.1 * 6.604352951049805
Epoch 490, val loss: 0.8479412794113159
Epoch 500, training loss: 0.7178106307983398 = 0.05837491527199745 + 0.1 * 6.594357013702393
Epoch 500, val loss: 0.8576804399490356
Epoch 510, training loss: 0.71212238073349 = 0.05382045358419418 + 0.1 * 6.583019256591797
Epoch 510, val loss: 0.8674649000167847
Epoch 520, training loss: 0.7102125287055969 = 0.04969515651464462 + 0.1 * 6.605173587799072
Epoch 520, val loss: 0.8773659467697144
Epoch 530, training loss: 0.7037187218666077 = 0.045977964997291565 + 0.1 * 6.5774078369140625
Epoch 530, val loss: 0.8871505260467529
Epoch 540, training loss: 0.7009531855583191 = 0.042620740830898285 + 0.1 * 6.583324432373047
Epoch 540, val loss: 0.8969240784645081
Epoch 550, training loss: 0.6964855790138245 = 0.03959007188677788 + 0.1 * 6.568954944610596
Epoch 550, val loss: 0.9065459370613098
Epoch 560, training loss: 0.6955431699752808 = 0.03684084489941597 + 0.1 * 6.58702278137207
Epoch 560, val loss: 0.9161779880523682
Epoch 570, training loss: 0.6904945969581604 = 0.03435886278748512 + 0.1 * 6.561357498168945
Epoch 570, val loss: 0.9254825115203857
Epoch 580, training loss: 0.6886473298072815 = 0.03210233151912689 + 0.1 * 6.5654497146606445
Epoch 580, val loss: 0.934812068939209
Epoch 590, training loss: 0.6859797239303589 = 0.030054546892642975 + 0.1 * 6.559251308441162
Epoch 590, val loss: 0.9438341856002808
Epoch 600, training loss: 0.6836630702018738 = 0.02819151058793068 + 0.1 * 6.554715633392334
Epoch 600, val loss: 0.9527491927146912
Epoch 610, training loss: 0.6822418570518494 = 0.026494136080145836 + 0.1 * 6.557476997375488
Epoch 610, val loss: 0.9614450931549072
Epoch 620, training loss: 0.6791120767593384 = 0.024945467710494995 + 0.1 * 6.541666030883789
Epoch 620, val loss: 0.9699294567108154
Epoch 630, training loss: 0.6785792112350464 = 0.02352619729936123 + 0.1 * 6.550529956817627
Epoch 630, val loss: 0.9782692193984985
Epoch 640, training loss: 0.6759634017944336 = 0.022228974848985672 + 0.1 * 6.537344455718994
Epoch 640, val loss: 0.9863111972808838
Epoch 650, training loss: 0.675057590007782 = 0.021037859842181206 + 0.1 * 6.540196895599365
Epoch 650, val loss: 0.994295597076416
Epoch 660, training loss: 0.674119234085083 = 0.019945669919252396 + 0.1 * 6.5417351722717285
Epoch 660, val loss: 1.0019252300262451
Epoch 670, training loss: 0.6713348627090454 = 0.018936436623334885 + 0.1 * 6.523983955383301
Epoch 670, val loss: 1.009472370147705
Epoch 680, training loss: 0.6734767556190491 = 0.018002115190029144 + 0.1 * 6.554746627807617
Epoch 680, val loss: 1.016817331314087
Epoch 690, training loss: 0.6700997948646545 = 0.017139099538326263 + 0.1 * 6.529606819152832
Epoch 690, val loss: 1.0240097045898438
Epoch 700, training loss: 0.6690511703491211 = 0.016341298818588257 + 0.1 * 6.527098178863525
Epoch 700, val loss: 1.0310477018356323
Epoch 710, training loss: 0.666983425617218 = 0.01560139935463667 + 0.1 * 6.513820171356201
Epoch 710, val loss: 1.0378143787384033
Epoch 720, training loss: 0.6660754680633545 = 0.0149112269282341 + 0.1 * 6.511641979217529
Epoch 720, val loss: 1.0445231199264526
Epoch 730, training loss: 0.6664202213287354 = 0.014268651604652405 + 0.1 * 6.521515846252441
Epoch 730, val loss: 1.0510830879211426
Epoch 740, training loss: 0.6644368171691895 = 0.013671377673745155 + 0.1 * 6.507654190063477
Epoch 740, val loss: 1.057377815246582
Epoch 750, training loss: 0.663712739944458 = 0.013112964108586311 + 0.1 * 6.505997657775879
Epoch 750, val loss: 1.0636489391326904
Epoch 760, training loss: 0.6629613637924194 = 0.012591538950800896 + 0.1 * 6.503698348999023
Epoch 760, val loss: 1.069674015045166
Epoch 770, training loss: 0.6618357300758362 = 0.012100692838430405 + 0.1 * 6.497350215911865
Epoch 770, val loss: 1.0756672620773315
Epoch 780, training loss: 0.6638525724411011 = 0.011642402969300747 + 0.1 * 6.522101402282715
Epoch 780, val loss: 1.0814627408981323
Epoch 790, training loss: 0.6607884168624878 = 0.01121237501502037 + 0.1 * 6.495760440826416
Epoch 790, val loss: 1.0870399475097656
Epoch 800, training loss: 0.6597957611083984 = 0.010808098129928112 + 0.1 * 6.4898762702941895
Epoch 800, val loss: 1.0925776958465576
Epoch 810, training loss: 0.6614515781402588 = 0.010425171814858913 + 0.1 * 6.5102643966674805
Epoch 810, val loss: 1.098012089729309
Epoch 820, training loss: 0.6588427424430847 = 0.010065238922834396 + 0.1 * 6.487774848937988
Epoch 820, val loss: 1.1032507419586182
Epoch 830, training loss: 0.6575427651405334 = 0.009726492688059807 + 0.1 * 6.47816276550293
Epoch 830, val loss: 1.1084129810333252
Epoch 840, training loss: 0.6576145887374878 = 0.009407256729900837 + 0.1 * 6.482072830200195
Epoch 840, val loss: 1.1134119033813477
Epoch 850, training loss: 0.6563866138458252 = 0.009103343822062016 + 0.1 * 6.472832202911377
Epoch 850, val loss: 1.1183574199676514
Epoch 860, training loss: 0.6560702323913574 = 0.008814971894025803 + 0.1 * 6.472552299499512
Epoch 860, val loss: 1.1232150793075562
Epoch 870, training loss: 0.6574188470840454 = 0.00854070670902729 + 0.1 * 6.488781452178955
Epoch 870, val loss: 1.1280081272125244
Epoch 880, training loss: 0.6557857394218445 = 0.008282539434731007 + 0.1 * 6.475031852722168
Epoch 880, val loss: 1.1325610876083374
Epoch 890, training loss: 0.6543807983398438 = 0.008037162944674492 + 0.1 * 6.463436603546143
Epoch 890, val loss: 1.1371561288833618
Epoch 900, training loss: 0.6542686820030212 = 0.007803707849234343 + 0.1 * 6.4646501541137695
Epoch 900, val loss: 1.1415945291519165
Epoch 910, training loss: 0.6564033031463623 = 0.007580531295388937 + 0.1 * 6.488227844238281
Epoch 910, val loss: 1.1458805799484253
Epoch 920, training loss: 0.6530736684799194 = 0.007368521764874458 + 0.1 * 6.4570512771606445
Epoch 920, val loss: 1.1502357721328735
Epoch 930, training loss: 0.6526854634284973 = 0.007167209405452013 + 0.1 * 6.455182075500488
Epoch 930, val loss: 1.1543558835983276
Epoch 940, training loss: 0.6533941030502319 = 0.006974293850362301 + 0.1 * 6.464198112487793
Epoch 940, val loss: 1.158458948135376
Epoch 950, training loss: 0.6516872048377991 = 0.006789716426283121 + 0.1 * 6.448974609375
Epoch 950, val loss: 1.1625157594680786
Epoch 960, training loss: 0.6518832445144653 = 0.006613217759877443 + 0.1 * 6.452700138092041
Epoch 960, val loss: 1.1664905548095703
Epoch 970, training loss: 0.651477575302124 = 0.006445455364882946 + 0.1 * 6.450320720672607
Epoch 970, val loss: 1.1703249216079712
Epoch 980, training loss: 0.6534650325775146 = 0.006284037139266729 + 0.1 * 6.4718098640441895
Epoch 980, val loss: 1.1741565465927124
Epoch 990, training loss: 0.6511064767837524 = 0.006129655987024307 + 0.1 * 6.449767589569092
Epoch 990, val loss: 1.1778807640075684
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.7958648204803467 = 1.936184048652649 + 0.1 * 8.596807479858398
Epoch 0, val loss: 1.938340663909912
Epoch 10, training loss: 2.785966634750366 = 1.9263020753860474 + 0.1 * 8.596646308898926
Epoch 10, val loss: 1.9281833171844482
Epoch 20, training loss: 2.7730183601379395 = 1.9134536981582642 + 0.1 * 8.595646858215332
Epoch 20, val loss: 1.9151520729064941
Epoch 30, training loss: 2.7535717487335205 = 1.894832730293274 + 0.1 * 8.587390899658203
Epoch 30, val loss: 1.8964215517044067
Epoch 40, training loss: 2.7211949825286865 = 1.8673428297042847 + 0.1 * 8.538520812988281
Epoch 40, val loss: 1.869340419769287
Epoch 50, training loss: 2.6601388454437256 = 1.831794261932373 + 0.1 * 8.283446311950684
Epoch 50, val loss: 1.8358676433563232
Epoch 60, training loss: 2.5993690490722656 = 1.7935106754302979 + 0.1 * 8.058582305908203
Epoch 60, val loss: 1.8017479181289673
Epoch 70, training loss: 2.5410003662109375 = 1.7553032636642456 + 0.1 * 7.856970310211182
Epoch 70, val loss: 1.7675944566726685
Epoch 80, training loss: 2.473471164703369 = 1.7105015516281128 + 0.1 * 7.629695892333984
Epoch 80, val loss: 1.726790428161621
Epoch 90, training loss: 2.391890525817871 = 1.6553102731704712 + 0.1 * 7.365803241729736
Epoch 90, val loss: 1.6793339252471924
Epoch 100, training loss: 2.3028485774993896 = 1.5873311758041382 + 0.1 * 7.1551737785339355
Epoch 100, val loss: 1.6219407320022583
Epoch 110, training loss: 2.213239908218384 = 1.5088067054748535 + 0.1 * 7.044331073760986
Epoch 110, val loss: 1.5556600093841553
Epoch 120, training loss: 2.128959894180298 = 1.4296691417694092 + 0.1 * 6.9929070472717285
Epoch 120, val loss: 1.490445852279663
Epoch 130, training loss: 2.049370288848877 = 1.3532596826553345 + 0.1 * 6.961106300354004
Epoch 130, val loss: 1.4285783767700195
Epoch 140, training loss: 1.9713488817214966 = 1.2773891687393188 + 0.1 * 6.939597129821777
Epoch 140, val loss: 1.3671001195907593
Epoch 150, training loss: 1.8935127258300781 = 1.2007853984832764 + 0.1 * 6.927272796630859
Epoch 150, val loss: 1.3059533834457397
Epoch 160, training loss: 1.8169519901275635 = 1.1250605583190918 + 0.1 * 6.918914794921875
Epoch 160, val loss: 1.2482035160064697
Epoch 170, training loss: 1.742672085762024 = 1.051330804824829 + 0.1 * 6.913412570953369
Epoch 170, val loss: 1.1943960189819336
Epoch 180, training loss: 1.670273780822754 = 0.9797322750091553 + 0.1 * 6.905414581298828
Epoch 180, val loss: 1.143589735031128
Epoch 190, training loss: 1.60121488571167 = 0.911461591720581 + 0.1 * 6.897532939910889
Epoch 190, val loss: 1.0963218212127686
Epoch 200, training loss: 1.5366952419281006 = 0.8477367162704468 + 0.1 * 6.889585018157959
Epoch 200, val loss: 1.0530234575271606
Epoch 210, training loss: 1.4767301082611084 = 0.7886897325515747 + 0.1 * 6.880403995513916
Epoch 210, val loss: 1.0141024589538574
Epoch 220, training loss: 1.4204421043395996 = 0.7334496974945068 + 0.1 * 6.869923114776611
Epoch 220, val loss: 0.9789098501205444
Epoch 230, training loss: 1.367842197418213 = 0.6816897988319397 + 0.1 * 6.8615241050720215
Epoch 230, val loss: 0.9471609592437744
Epoch 240, training loss: 1.31727933883667 = 0.632217526435852 + 0.1 * 6.850617408752441
Epoch 240, val loss: 0.917405903339386
Epoch 250, training loss: 1.2676763534545898 = 0.5839498043060303 + 0.1 * 6.837264537811279
Epoch 250, val loss: 0.8886824250221252
Epoch 260, training loss: 1.2198915481567383 = 0.5367388725280762 + 0.1 * 6.831526279449463
Epoch 260, val loss: 0.8604185581207275
Epoch 270, training loss: 1.172670602798462 = 0.4913211762905121 + 0.1 * 6.813493728637695
Epoch 270, val loss: 0.8331140875816345
Epoch 280, training loss: 1.127561330795288 = 0.4476611316204071 + 0.1 * 6.799001693725586
Epoch 280, val loss: 0.806997537612915
Epoch 290, training loss: 1.086867094039917 = 0.4057081341743469 + 0.1 * 6.811588764190674
Epoch 290, val loss: 0.7824892401695251
Epoch 300, training loss: 1.0441094636917114 = 0.36598917841911316 + 0.1 * 6.78120231628418
Epoch 300, val loss: 0.7604225873947144
Epoch 310, training loss: 1.0049434900283813 = 0.32807689905166626 + 0.1 * 6.768665790557861
Epoch 310, val loss: 0.7405586242675781
Epoch 320, training loss: 0.969023585319519 = 0.2921040654182434 + 0.1 * 6.769195079803467
Epoch 320, val loss: 0.7232078909873962
Epoch 330, training loss: 0.9344145059585571 = 0.2588403522968292 + 0.1 * 6.755741119384766
Epoch 330, val loss: 0.7086994647979736
Epoch 340, training loss: 0.903219997882843 = 0.22844183444976807 + 0.1 * 6.747781276702881
Epoch 340, val loss: 0.696887731552124
Epoch 350, training loss: 0.8776453733444214 = 0.2010606825351715 + 0.1 * 6.765847206115723
Epoch 350, val loss: 0.6878923773765564
Epoch 360, training loss: 0.8513870239257812 = 0.17708437144756317 + 0.1 * 6.743026256561279
Epoch 360, val loss: 0.6816611289978027
Epoch 370, training loss: 0.8287588357925415 = 0.15619519352912903 + 0.1 * 6.7256364822387695
Epoch 370, val loss: 0.6779152154922485
Epoch 380, training loss: 0.8100664615631104 = 0.13805969059467316 + 0.1 * 6.720067501068115
Epoch 380, val loss: 0.676537811756134
Epoch 390, training loss: 0.795290470123291 = 0.1223917305469513 + 0.1 * 6.728987216949463
Epoch 390, val loss: 0.6771473288536072
Epoch 400, training loss: 0.7801417112350464 = 0.10897108912467957 + 0.1 * 6.711705684661865
Epoch 400, val loss: 0.6794482469558716
Epoch 410, training loss: 0.7673428654670715 = 0.09733986109495163 + 0.1 * 6.7000298500061035
Epoch 410, val loss: 0.6830015182495117
Epoch 420, training loss: 0.7570199370384216 = 0.08719094842672348 + 0.1 * 6.69828987121582
Epoch 420, val loss: 0.6876625418663025
Epoch 430, training loss: 0.7483237385749817 = 0.0783759132027626 + 0.1 * 6.6994781494140625
Epoch 430, val loss: 0.6931507587432861
Epoch 440, training loss: 0.7388080358505249 = 0.07071200758218765 + 0.1 * 6.680960178375244
Epoch 440, val loss: 0.6991286277770996
Epoch 450, training loss: 0.7320848703384399 = 0.06398367881774902 + 0.1 * 6.68101167678833
Epoch 450, val loss: 0.7055835127830505
Epoch 460, training loss: 0.7252869009971619 = 0.0580795556306839 + 0.1 * 6.6720733642578125
Epoch 460, val loss: 0.7124530673027039
Epoch 470, training loss: 0.7199419140815735 = 0.05289163440465927 + 0.1 * 6.67050313949585
Epoch 470, val loss: 0.7195178270339966
Epoch 480, training loss: 0.7138854265213013 = 0.04832056537270546 + 0.1 * 6.655648231506348
Epoch 480, val loss: 0.7267061471939087
Epoch 490, training loss: 0.7089293599128723 = 0.04428033158183098 + 0.1 * 6.646490097045898
Epoch 490, val loss: 0.7340507507324219
Epoch 500, training loss: 0.7056803107261658 = 0.0407027043402195 + 0.1 * 6.649775981903076
Epoch 500, val loss: 0.7414308190345764
Epoch 510, training loss: 0.7006412148475647 = 0.03752537816762924 + 0.1 * 6.631158351898193
Epoch 510, val loss: 0.748797595500946
Epoch 520, training loss: 0.69818514585495 = 0.034696973860263824 + 0.1 * 6.634881496429443
Epoch 520, val loss: 0.7561498284339905
Epoch 530, training loss: 0.6942301988601685 = 0.03217759728431702 + 0.1 * 6.62052583694458
Epoch 530, val loss: 0.7633827924728394
Epoch 540, training loss: 0.6926869750022888 = 0.029917316511273384 + 0.1 * 6.627696514129639
Epoch 540, val loss: 0.7705832719802856
Epoch 550, training loss: 0.6884850859642029 = 0.02789059840142727 + 0.1 * 6.605944633483887
Epoch 550, val loss: 0.7777201533317566
Epoch 560, training loss: 0.6859738826751709 = 0.026063820347189903 + 0.1 * 6.599100589752197
Epoch 560, val loss: 0.7847204804420471
Epoch 570, training loss: 0.6834455728530884 = 0.024410158395767212 + 0.1 * 6.5903544425964355
Epoch 570, val loss: 0.7916796803474426
Epoch 580, training loss: 0.6834068298339844 = 0.022912297397851944 + 0.1 * 6.604945182800293
Epoch 580, val loss: 0.7985001802444458
Epoch 590, training loss: 0.6810027956962585 = 0.02155611477792263 + 0.1 * 6.594466686248779
Epoch 590, val loss: 0.8051557540893555
Epoch 600, training loss: 0.6790167689323425 = 0.02032083459198475 + 0.1 * 6.586959362030029
Epoch 600, val loss: 0.8117145299911499
Epoch 610, training loss: 0.6768025755882263 = 0.019192179664969444 + 0.1 * 6.576104164123535
Epoch 610, val loss: 0.8181408047676086
Epoch 620, training loss: 0.6747927665710449 = 0.0181594081223011 + 0.1 * 6.566333770751953
Epoch 620, val loss: 0.8244937658309937
Epoch 630, training loss: 0.6762670874595642 = 0.01720910705626011 + 0.1 * 6.590579509735107
Epoch 630, val loss: 0.8307509422302246
Epoch 640, training loss: 0.6727728843688965 = 0.016338495537638664 + 0.1 * 6.5643439292907715
Epoch 640, val loss: 0.8367828130722046
Epoch 650, training loss: 0.6717144250869751 = 0.015535742044448853 + 0.1 * 6.561787128448486
Epoch 650, val loss: 0.8427781462669373
Epoch 660, training loss: 0.6700990796089172 = 0.014793912880122662 + 0.1 * 6.553051471710205
Epoch 660, val loss: 0.8486424684524536
Epoch 670, training loss: 0.669451892375946 = 0.014110224321484566 + 0.1 * 6.553416728973389
Epoch 670, val loss: 0.8543559908866882
Epoch 680, training loss: 0.6678917407989502 = 0.01347382739186287 + 0.1 * 6.5441789627075195
Epoch 680, val loss: 0.8600102663040161
Epoch 690, training loss: 0.6676137447357178 = 0.01288184430450201 + 0.1 * 6.547318935394287
Epoch 690, val loss: 0.8655261397361755
Epoch 700, training loss: 0.6661669015884399 = 0.01233209203928709 + 0.1 * 6.538348197937012
Epoch 700, val loss: 0.8708901405334473
Epoch 710, training loss: 0.666135311126709 = 0.011819450184702873 + 0.1 * 6.543159008026123
Epoch 710, val loss: 0.8762335777282715
Epoch 720, training loss: 0.666418194770813 = 0.011339891701936722 + 0.1 * 6.550782680511475
Epoch 720, val loss: 0.8814507126808167
Epoch 730, training loss: 0.6639890670776367 = 0.010892914608120918 + 0.1 * 6.530961513519287
Epoch 730, val loss: 0.8864914178848267
Epoch 740, training loss: 0.6629478335380554 = 0.01047470886260271 + 0.1 * 6.524731159210205
Epoch 740, val loss: 0.8914629220962524
Epoch 750, training loss: 0.6618644595146179 = 0.01008168887346983 + 0.1 * 6.51782751083374
Epoch 750, val loss: 0.8964033126831055
Epoch 760, training loss: 0.6627827882766724 = 0.009712129831314087 + 0.1 * 6.530706882476807
Epoch 760, val loss: 0.9011787176132202
Epoch 770, training loss: 0.6605610251426697 = 0.009365593083202839 + 0.1 * 6.511953830718994
Epoch 770, val loss: 0.9058630466461182
Epoch 780, training loss: 0.6599612832069397 = 0.009038850665092468 + 0.1 * 6.509223937988281
Epoch 780, val loss: 0.9105008244514465
Epoch 790, training loss: 0.6596317887306213 = 0.008730228990316391 + 0.1 * 6.5090155601501465
Epoch 790, val loss: 0.9150201678276062
Epoch 800, training loss: 0.6585468053817749 = 0.008439335972070694 + 0.1 * 6.501074314117432
Epoch 800, val loss: 0.9194256067276001
Epoch 810, training loss: 0.6601025462150574 = 0.008164351806044579 + 0.1 * 6.519381999969482
Epoch 810, val loss: 0.923812210559845
Epoch 820, training loss: 0.6577176451683044 = 0.007903225719928741 + 0.1 * 6.498144149780273
Epoch 820, val loss: 0.9280790090560913
Epoch 830, training loss: 0.6574746966362 = 0.0076569002121686935 + 0.1 * 6.498178005218506
Epoch 830, val loss: 0.9322781562805176
Epoch 840, training loss: 0.6562374830245972 = 0.00742262601852417 + 0.1 * 6.488148212432861
Epoch 840, val loss: 0.9364282488822937
Epoch 850, training loss: 0.6568318009376526 = 0.007199737709015608 + 0.1 * 6.4963202476501465
Epoch 850, val loss: 0.9405013918876648
Epoch 860, training loss: 0.6599506735801697 = 0.006988305598497391 + 0.1 * 6.529623985290527
Epoch 860, val loss: 0.9444271922111511
Epoch 870, training loss: 0.6565208435058594 = 0.006787721998989582 + 0.1 * 6.497331142425537
Epoch 870, val loss: 0.9483779072761536
Epoch 880, training loss: 0.6550439596176147 = 0.0065972767770290375 + 0.1 * 6.484466552734375
Epoch 880, val loss: 0.9521771669387817
Epoch 890, training loss: 0.6558823585510254 = 0.006415204610675573 + 0.1 * 6.494671821594238
Epoch 890, val loss: 0.955975353717804
Epoch 900, training loss: 0.654148519039154 = 0.006241439376026392 + 0.1 * 6.479070663452148
Epoch 900, val loss: 0.9596577882766724
Epoch 910, training loss: 0.6534886956214905 = 0.006076004356145859 + 0.1 * 6.474126815795898
Epoch 910, val loss: 0.9632365107536316
Epoch 920, training loss: 0.6549082398414612 = 0.005917530041188002 + 0.1 * 6.4899067878723145
Epoch 920, val loss: 0.9668010473251343
Epoch 930, training loss: 0.6527812480926514 = 0.005766429007053375 + 0.1 * 6.470147609710693
Epoch 930, val loss: 0.9703280925750732
Epoch 940, training loss: 0.6531393527984619 = 0.005621677730232477 + 0.1 * 6.475176811218262
Epoch 940, val loss: 0.9737585186958313
Epoch 950, training loss: 0.6534441113471985 = 0.005482768639922142 + 0.1 * 6.479613304138184
Epoch 950, val loss: 0.977164626121521
Epoch 960, training loss: 0.6521028876304626 = 0.005350370891392231 + 0.1 * 6.467525005340576
Epoch 960, val loss: 0.9805194735527039
Epoch 970, training loss: 0.6510090827941895 = 0.005223338957875967 + 0.1 * 6.457857131958008
Epoch 970, val loss: 0.9837855696678162
Epoch 980, training loss: 0.651755154132843 = 0.005101080052554607 + 0.1 * 6.466540813446045
Epoch 980, val loss: 0.9870135188102722
Epoch 990, training loss: 0.6510105133056641 = 0.0049834405072033405 + 0.1 * 6.460270881652832
Epoch 990, val loss: 0.9902616143226624
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.8028628826141357 = 1.943178415298462 + 0.1 * 8.596843719482422
Epoch 0, val loss: 1.9445432424545288
Epoch 10, training loss: 2.792661666870117 = 1.9329843521118164 + 0.1 * 8.596772193908691
Epoch 10, val loss: 1.9347246885299683
Epoch 20, training loss: 2.779787063598633 = 1.9201459884643555 + 0.1 * 8.596410751342773
Epoch 20, val loss: 1.9218214750289917
Epoch 30, training loss: 2.7613468170166016 = 1.9019876718521118 + 0.1 * 8.59359073638916
Epoch 30, val loss: 1.9035284519195557
Epoch 40, training loss: 2.732853889465332 = 1.8755502700805664 + 0.1 * 8.57303524017334
Epoch 40, val loss: 1.8773269653320312
Epoch 50, training loss: 2.6878044605255127 = 1.8395655155181885 + 0.1 * 8.482388496398926
Epoch 50, val loss: 1.843354344367981
Epoch 60, training loss: 2.612218141555786 = 1.7990590333938599 + 0.1 * 8.131591796875
Epoch 60, val loss: 1.8075780868530273
Epoch 70, training loss: 2.5380938053131104 = 1.7576711177825928 + 0.1 * 7.804225921630859
Epoch 70, val loss: 1.7686694860458374
Epoch 80, training loss: 2.4639878273010254 = 1.7095041275024414 + 0.1 * 7.544836521148682
Epoch 80, val loss: 1.7233891487121582
Epoch 90, training loss: 2.38515305519104 = 1.64914071559906 + 0.1 * 7.360123634338379
Epoch 90, val loss: 1.6705071926116943
Epoch 100, training loss: 2.2972254753112793 = 1.571821928024292 + 0.1 * 7.254035949707031
Epoch 100, val loss: 1.6014407873153687
Epoch 110, training loss: 2.198362350463867 = 1.4801405668258667 + 0.1 * 7.182216644287109
Epoch 110, val loss: 1.520580530166626
Epoch 120, training loss: 2.0943832397460938 = 1.3823637962341309 + 0.1 * 7.1201934814453125
Epoch 120, val loss: 1.436547040939331
Epoch 130, training loss: 1.9935803413391113 = 1.2859920263290405 + 0.1 * 7.075883388519287
Epoch 130, val loss: 1.3569976091384888
Epoch 140, training loss: 1.8992910385131836 = 1.195518970489502 + 0.1 * 7.037720203399658
Epoch 140, val loss: 1.2839020490646362
Epoch 150, training loss: 1.8117780685424805 = 1.111664056777954 + 0.1 * 7.0011396408081055
Epoch 150, val loss: 1.2184251546859741
Epoch 160, training loss: 1.7286391258239746 = 1.0319793224334717 + 0.1 * 6.966597080230713
Epoch 160, val loss: 1.1578222513198853
Epoch 170, training loss: 1.6490654945373535 = 0.9549059867858887 + 0.1 * 6.941594123840332
Epoch 170, val loss: 1.099800944328308
Epoch 180, training loss: 1.5725436210632324 = 0.8802571892738342 + 0.1 * 6.922863483428955
Epoch 180, val loss: 1.0439307689666748
Epoch 190, training loss: 1.4978657960891724 = 0.8071940541267395 + 0.1 * 6.906717300415039
Epoch 190, val loss: 0.9892415404319763
Epoch 200, training loss: 1.425483226776123 = 0.7359837889671326 + 0.1 * 6.894994258880615
Epoch 200, val loss: 0.9358514547348022
Epoch 210, training loss: 1.355563998222351 = 0.6676291823387146 + 0.1 * 6.879348278045654
Epoch 210, val loss: 0.8852212429046631
Epoch 220, training loss: 1.2891899347305298 = 0.6021119952201843 + 0.1 * 6.870779037475586
Epoch 220, val loss: 0.8383373618125916
Epoch 230, training loss: 1.2270084619522095 = 0.5402953028678894 + 0.1 * 6.86713171005249
Epoch 230, val loss: 0.7968912124633789
Epoch 240, training loss: 1.1677536964416504 = 0.48239052295684814 + 0.1 * 6.85363245010376
Epoch 240, val loss: 0.7613771557807922
Epoch 250, training loss: 1.1133298873901367 = 0.42865312099456787 + 0.1 * 6.846766948699951
Epoch 250, val loss: 0.7320125102996826
Epoch 260, training loss: 1.065104365348816 = 0.37971237301826477 + 0.1 * 6.853919506072998
Epoch 260, val loss: 0.7087632417678833
Epoch 270, training loss: 1.019836187362671 = 0.33629608154296875 + 0.1 * 6.835400581359863
Epoch 270, val loss: 0.6915355324745178
Epoch 280, training loss: 0.9808264970779419 = 0.297737717628479 + 0.1 * 6.830887794494629
Epoch 280, val loss: 0.6791077852249146
Epoch 290, training loss: 0.9469528198242188 = 0.26347145438194275 + 0.1 * 6.834813594818115
Epoch 290, val loss: 0.6707627177238464
Epoch 300, training loss: 0.9158145189285278 = 0.23331747949123383 + 0.1 * 6.824970245361328
Epoch 300, val loss: 0.6657111644744873
Epoch 310, training loss: 0.887944221496582 = 0.20668332278728485 + 0.1 * 6.81260871887207
Epoch 310, val loss: 0.6632819771766663
Epoch 320, training loss: 0.8638135194778442 = 0.18315425515174866 + 0.1 * 6.80659294128418
Epoch 320, val loss: 0.6632937788963318
Epoch 330, training loss: 0.8432286977767944 = 0.1625806838274002 + 0.1 * 6.806480407714844
Epoch 330, val loss: 0.6652071475982666
Epoch 340, training loss: 0.823774516582489 = 0.14471553266048431 + 0.1 * 6.790589332580566
Epoch 340, val loss: 0.6688183546066284
Epoch 350, training loss: 0.808610200881958 = 0.12915334105491638 + 0.1 * 6.79456901550293
Epoch 350, val loss: 0.6739702820777893
Epoch 360, training loss: 0.7943637371063232 = 0.11573760956525803 + 0.1 * 6.786261081695557
Epoch 360, val loss: 0.680169939994812
Epoch 370, training loss: 0.7811790108680725 = 0.1041020080447197 + 0.1 * 6.7707695960998535
Epoch 370, val loss: 0.6871741414070129
Epoch 380, training loss: 0.7706326246261597 = 0.09391983598470688 + 0.1 * 6.767127990722656
Epoch 380, val loss: 0.69498610496521
Epoch 390, training loss: 0.7602521181106567 = 0.08504880964756012 + 0.1 * 6.752033233642578
Epoch 390, val loss: 0.7033108472824097
Epoch 400, training loss: 0.7515808343887329 = 0.07727214694023132 + 0.1 * 6.743087291717529
Epoch 400, val loss: 0.7119320631027222
Epoch 410, training loss: 0.7470560669898987 = 0.07041655480861664 + 0.1 * 6.766395092010498
Epoch 410, val loss: 0.7208474278450012
Epoch 420, training loss: 0.7377815246582031 = 0.06440000236034393 + 0.1 * 6.7338151931762695
Epoch 420, val loss: 0.729843020439148
Epoch 430, training loss: 0.7314999103546143 = 0.05907316505908966 + 0.1 * 6.724267482757568
Epoch 430, val loss: 0.7387945055961609
Epoch 440, training loss: 0.7261996865272522 = 0.0543273463845253 + 0.1 * 6.718723297119141
Epoch 440, val loss: 0.7478463649749756
Epoch 450, training loss: 0.7233816385269165 = 0.05008245259523392 + 0.1 * 6.732991695404053
Epoch 450, val loss: 0.7568953037261963
Epoch 460, training loss: 0.718026340007782 = 0.046302564442157745 + 0.1 * 6.71723747253418
Epoch 460, val loss: 0.7658511400222778
Epoch 470, training loss: 0.7131049036979675 = 0.042923808097839355 + 0.1 * 6.701810836791992
Epoch 470, val loss: 0.7746304869651794
Epoch 480, training loss: 0.7102813124656677 = 0.039884813129901886 + 0.1 * 6.703965187072754
Epoch 480, val loss: 0.7832224369049072
Epoch 490, training loss: 0.7062265872955322 = 0.03715018555521965 + 0.1 * 6.690763473510742
Epoch 490, val loss: 0.7918170094490051
Epoch 500, training loss: 0.7038440704345703 = 0.03467234969139099 + 0.1 * 6.691717624664307
Epoch 500, val loss: 0.8001842498779297
Epoch 510, training loss: 0.7008792161941528 = 0.032428909093141556 + 0.1 * 6.684502601623535
Epoch 510, val loss: 0.8084154725074768
Epoch 520, training loss: 0.6982425451278687 = 0.030394908040761948 + 0.1 * 6.678476333618164
Epoch 520, val loss: 0.8164793848991394
Epoch 530, training loss: 0.6956883072853088 = 0.028540736064314842 + 0.1 * 6.671475887298584
Epoch 530, val loss: 0.8243374228477478
Epoch 540, training loss: 0.6930420994758606 = 0.026851652190089226 + 0.1 * 6.661904335021973
Epoch 540, val loss: 0.8321498036384583
Epoch 550, training loss: 0.6916001439094543 = 0.02530558966100216 + 0.1 * 6.662945747375488
Epoch 550, val loss: 0.8396537899971008
Epoch 560, training loss: 0.689063310623169 = 0.023894909769296646 + 0.1 * 6.651683807373047
Epoch 560, val loss: 0.8471567630767822
Epoch 570, training loss: 0.6861512660980225 = 0.02259768359363079 + 0.1 * 6.635535717010498
Epoch 570, val loss: 0.8543912172317505
Epoch 580, training loss: 0.6849852800369263 = 0.021401386708021164 + 0.1 * 6.635838508605957
Epoch 580, val loss: 0.8615384697914124
Epoch 590, training loss: 0.6865347027778625 = 0.020297234877943993 + 0.1 * 6.662374496459961
Epoch 590, val loss: 0.8684119582176208
Epoch 600, training loss: 0.6820365190505981 = 0.019285377115011215 + 0.1 * 6.627511024475098
Epoch 600, val loss: 0.8752866983413696
Epoch 610, training loss: 0.6799690127372742 = 0.018348876386880875 + 0.1 * 6.616201400756836
Epoch 610, val loss: 0.8819186687469482
Epoch 620, training loss: 0.6786463260650635 = 0.01747843436896801 + 0.1 * 6.6116790771484375
Epoch 620, val loss: 0.8883969187736511
Epoch 630, training loss: 0.6770825386047363 = 0.016668491065502167 + 0.1 * 6.604140281677246
Epoch 630, val loss: 0.8947651386260986
Epoch 640, training loss: 0.6764115691184998 = 0.015916598960757256 + 0.1 * 6.604949951171875
Epoch 640, val loss: 0.901073157787323
Epoch 650, training loss: 0.6747679114341736 = 0.015216377563774586 + 0.1 * 6.595515251159668
Epoch 650, val loss: 0.907088041305542
Epoch 660, training loss: 0.6740644574165344 = 0.014565362595021725 + 0.1 * 6.594991207122803
Epoch 660, val loss: 0.9131259322166443
Epoch 670, training loss: 0.6729294657707214 = 0.013954761438071728 + 0.1 * 6.589746952056885
Epoch 670, val loss: 0.9188534021377563
Epoch 680, training loss: 0.6717127561569214 = 0.01338548120111227 + 0.1 * 6.583272933959961
Epoch 680, val loss: 0.924609899520874
Epoch 690, training loss: 0.67011559009552 = 0.01285126619040966 + 0.1 * 6.572643280029297
Epoch 690, val loss: 0.9302083849906921
Epoch 700, training loss: 0.6696460247039795 = 0.012349779717624187 + 0.1 * 6.572962284088135
Epoch 700, val loss: 0.9357431530952454
Epoch 710, training loss: 0.6698293089866638 = 0.011877253651618958 + 0.1 * 6.5795207023620605
Epoch 710, val loss: 0.9410498738288879
Epoch 720, training loss: 0.6687043905258179 = 0.011434447020292282 + 0.1 * 6.572699546813965
Epoch 720, val loss: 0.9464043974876404
Epoch 730, training loss: 0.6672898530960083 = 0.011017554439604282 + 0.1 * 6.562722682952881
Epoch 730, val loss: 0.951553463935852
Epoch 740, training loss: 0.665785551071167 = 0.0106252022087574 + 0.1 * 6.551603317260742
Epoch 740, val loss: 0.9566529393196106
Epoch 750, training loss: 0.6669019460678101 = 0.010254105553030968 + 0.1 * 6.566478252410889
Epoch 750, val loss: 0.9616624712944031
Epoch 760, training loss: 0.6661534905433655 = 0.009902586229145527 + 0.1 * 6.562509059906006
Epoch 760, val loss: 0.9664963483810425
Epoch 770, training loss: 0.6646338105201721 = 0.009571514092385769 + 0.1 * 6.550622940063477
Epoch 770, val loss: 0.9713380932807922
Epoch 780, training loss: 0.6637693047523499 = 0.00925837829709053 + 0.1 * 6.545109272003174
Epoch 780, val loss: 0.9760835766792297
Epoch 790, training loss: 0.6630182266235352 = 0.008960237726569176 + 0.1 * 6.540579795837402
Epoch 790, val loss: 0.9806098937988281
Epoch 800, training loss: 0.663459300994873 = 0.008678898215293884 + 0.1 * 6.54780387878418
Epoch 800, val loss: 0.9851902723312378
Epoch 810, training loss: 0.6621667742729187 = 0.008411584421992302 + 0.1 * 6.5375518798828125
Epoch 810, val loss: 0.9896710515022278
Epoch 820, training loss: 0.6625814437866211 = 0.008157314732670784 + 0.1 * 6.544241428375244
Epoch 820, val loss: 0.9940736889839172
Epoch 830, training loss: 0.6616871953010559 = 0.007914407178759575 + 0.1 * 6.5377278327941895
Epoch 830, val loss: 0.9982720017433167
Epoch 840, training loss: 0.6619610786437988 = 0.007685299962759018 + 0.1 * 6.542757511138916
Epoch 840, val loss: 1.0025746822357178
Epoch 850, training loss: 0.6609916090965271 = 0.007466076407581568 + 0.1 * 6.535255432128906
Epoch 850, val loss: 1.0066684484481812
Epoch 860, training loss: 0.6601017713546753 = 0.007257485296577215 + 0.1 * 6.528442859649658
Epoch 860, val loss: 1.0106736421585083
Epoch 870, training loss: 0.6591292023658752 = 0.00705887982621789 + 0.1 * 6.520703315734863
Epoch 870, val loss: 1.014689564704895
Epoch 880, training loss: 0.6585140228271484 = 0.00686843553557992 + 0.1 * 6.51645565032959
Epoch 880, val loss: 1.018551230430603
Epoch 890, training loss: 0.6586599946022034 = 0.006686834152787924 + 0.1 * 6.519731521606445
Epoch 890, val loss: 1.0223758220672607
Epoch 900, training loss: 0.6580682396888733 = 0.006513225380331278 + 0.1 * 6.515549659729004
Epoch 900, val loss: 1.0261698961257935
Epoch 910, training loss: 0.6574934720993042 = 0.006346879526972771 + 0.1 * 6.511466026306152
Epoch 910, val loss: 1.0298622846603394
Epoch 920, training loss: 0.6571809649467468 = 0.006188104394823313 + 0.1 * 6.5099287033081055
Epoch 920, val loss: 1.0335627794265747
Epoch 930, training loss: 0.6574458479881287 = 0.006035362835973501 + 0.1 * 6.51410436630249
Epoch 930, val loss: 1.0371572971343994
Epoch 940, training loss: 0.6560700535774231 = 0.005888900253921747 + 0.1 * 6.501811504364014
Epoch 940, val loss: 1.0406635999679565
Epoch 950, training loss: 0.6560237407684326 = 0.005748495925217867 + 0.1 * 6.502752304077148
Epoch 950, val loss: 1.044075608253479
Epoch 960, training loss: 0.6562210321426392 = 0.005614735186100006 + 0.1 * 6.5060625076293945
Epoch 960, val loss: 1.047537088394165
Epoch 970, training loss: 0.6544148325920105 = 0.005485777277499437 + 0.1 * 6.489290237426758
Epoch 970, val loss: 1.0509347915649414
Epoch 980, training loss: 0.6551517844200134 = 0.0053616538643836975 + 0.1 * 6.49790096282959
Epoch 980, val loss: 1.05425226688385
Epoch 990, training loss: 0.6545151472091675 = 0.005241312552243471 + 0.1 * 6.492738246917725
Epoch 990, val loss: 1.0574002265930176
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8408012651555088
The final CL Acc:0.81111, 0.01571, The final GNN Acc:0.83922, 0.00262
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9544])
updated graph: torch.Size([2, 10610])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.790017604827881 = 1.9303325414657593 + 0.1 * 8.59684944152832
Epoch 0, val loss: 1.9297356605529785
Epoch 10, training loss: 2.7809343338012695 = 1.9212572574615479 + 0.1 * 8.596769332885742
Epoch 10, val loss: 1.9214199781417847
Epoch 20, training loss: 2.7695322036743164 = 1.9099048376083374 + 0.1 * 8.596273422241211
Epoch 20, val loss: 1.9108178615570068
Epoch 30, training loss: 2.7530555725097656 = 1.8938796520233154 + 0.1 * 8.59175968170166
Epoch 30, val loss: 1.895790696144104
Epoch 40, training loss: 2.726536273956299 = 1.8704369068145752 + 0.1 * 8.560994148254395
Epoch 40, val loss: 1.8742213249206543
Epoch 50, training loss: 2.6797618865966797 = 1.8389434814453125 + 0.1 * 8.408184051513672
Epoch 50, val loss: 1.8466546535491943
Epoch 60, training loss: 2.6290268898010254 = 1.8044369220733643 + 0.1 * 8.245899200439453
Epoch 60, val loss: 1.8184127807617188
Epoch 70, training loss: 2.5699338912963867 = 1.7722407579421997 + 0.1 * 7.976930141448975
Epoch 70, val loss: 1.7908393144607544
Epoch 80, training loss: 2.4838366508483887 = 1.7367229461669922 + 0.1 * 7.471137046813965
Epoch 80, val loss: 1.7573403120040894
Epoch 90, training loss: 2.413848400115967 = 1.6919398307800293 + 0.1 * 7.219086170196533
Epoch 90, val loss: 1.7187035083770752
Epoch 100, training loss: 2.3439197540283203 = 1.6327357292175293 + 0.1 * 7.11184024810791
Epoch 100, val loss: 1.6711958646774292
Epoch 110, training loss: 2.2627105712890625 = 1.5584224462509155 + 0.1 * 7.042881965637207
Epoch 110, val loss: 1.6095560789108276
Epoch 120, training loss: 2.174252510070801 = 1.4741195440292358 + 0.1 * 7.001328468322754
Epoch 120, val loss: 1.5425359010696411
Epoch 130, training loss: 2.082636594772339 = 1.3850120306015015 + 0.1 * 6.976244926452637
Epoch 130, val loss: 1.4725068807601929
Epoch 140, training loss: 1.9904625415802002 = 1.2944170236587524 + 0.1 * 6.960454940795898
Epoch 140, val loss: 1.4029096364974976
Epoch 150, training loss: 1.899975061416626 = 1.2057373523712158 + 0.1 * 6.942377090454102
Epoch 150, val loss: 1.3362642526626587
Epoch 160, training loss: 1.8145514726638794 = 1.1218663454055786 + 0.1 * 6.926851272583008
Epoch 160, val loss: 1.2751880884170532
Epoch 170, training loss: 1.7367005348205566 = 1.045645833015442 + 0.1 * 6.910547256469727
Epoch 170, val loss: 1.2218997478485107
Epoch 180, training loss: 1.6671100854873657 = 0.977414071559906 + 0.1 * 6.896960258483887
Epoch 180, val loss: 1.1757144927978516
Epoch 190, training loss: 1.6029155254364014 = 0.9145064949989319 + 0.1 * 6.884090423583984
Epoch 190, val loss: 1.1347150802612305
Epoch 200, training loss: 1.5413150787353516 = 0.8534249663352966 + 0.1 * 6.87890100479126
Epoch 200, val loss: 1.09531831741333
Epoch 210, training loss: 1.4794254302978516 = 0.7929320931434631 + 0.1 * 6.864932537078857
Epoch 210, val loss: 1.0566387176513672
Epoch 220, training loss: 1.4180312156677246 = 0.7324070930480957 + 0.1 * 6.856241703033447
Epoch 220, val loss: 1.0187187194824219
Epoch 230, training loss: 1.3575799465179443 = 0.6733989715576172 + 0.1 * 6.841808795928955
Epoch 230, val loss: 0.9838912487030029
Epoch 240, training loss: 1.3006609678268433 = 0.6169974207878113 + 0.1 * 6.836635589599609
Epoch 240, val loss: 0.9534136652946472
Epoch 250, training loss: 1.246632695198059 = 0.5642167925834656 + 0.1 * 6.824158668518066
Epoch 250, val loss: 0.9286867380142212
Epoch 260, training loss: 1.1972739696502686 = 0.5153325796127319 + 0.1 * 6.819413185119629
Epoch 260, val loss: 0.9096289277076721
Epoch 270, training loss: 1.1510595083236694 = 0.47044649720191956 + 0.1 * 6.8061299324035645
Epoch 270, val loss: 0.8958829641342163
Epoch 280, training loss: 1.1090428829193115 = 0.42913153767585754 + 0.1 * 6.799113750457764
Epoch 280, val loss: 0.8860965967178345
Epoch 290, training loss: 1.070443034172058 = 0.3911253809928894 + 0.1 * 6.793176174163818
Epoch 290, val loss: 0.8796478509902954
Epoch 300, training loss: 1.035024881362915 = 0.35627999901771545 + 0.1 * 6.787448406219482
Epoch 300, val loss: 0.8759967088699341
Epoch 310, training loss: 1.0022225379943848 = 0.32422786951065063 + 0.1 * 6.779947280883789
Epoch 310, val loss: 0.8744186162948608
Epoch 320, training loss: 0.9722200632095337 = 0.2948512136936188 + 0.1 * 6.773687839508057
Epoch 320, val loss: 0.8747081160545349
Epoch 330, training loss: 0.9437870383262634 = 0.2677520513534546 + 0.1 * 6.760349750518799
Epoch 330, val loss: 0.8765292167663574
Epoch 340, training loss: 0.9177828431129456 = 0.24283017218112946 + 0.1 * 6.749526500701904
Epoch 340, val loss: 0.8795928359031677
Epoch 350, training loss: 0.8951690793037415 = 0.22002582252025604 + 0.1 * 6.751432418823242
Epoch 350, val loss: 0.8838053941726685
Epoch 360, training loss: 0.8744601011276245 = 0.1992073804140091 + 0.1 * 6.752527236938477
Epoch 360, val loss: 0.8888898491859436
Epoch 370, training loss: 0.8535561561584473 = 0.18029983341693878 + 0.1 * 6.732563018798828
Epoch 370, val loss: 0.8948765993118286
Epoch 380, training loss: 0.8352012634277344 = 0.1630498766899109 + 0.1 * 6.721513748168945
Epoch 380, val loss: 0.9015815854072571
Epoch 390, training loss: 0.8228536248207092 = 0.14734095335006714 + 0.1 * 6.755126476287842
Epoch 390, val loss: 0.9090601205825806
Epoch 400, training loss: 0.8043752908706665 = 0.13324087858200073 + 0.1 * 6.711344242095947
Epoch 400, val loss: 0.9171906113624573
Epoch 410, training loss: 0.7901363372802734 = 0.12054465711116791 + 0.1 * 6.695916652679443
Epoch 410, val loss: 0.9258237481117249
Epoch 420, training loss: 0.7790277004241943 = 0.10913689434528351 + 0.1 * 6.698907852172852
Epoch 420, val loss: 0.9349492192268372
Epoch 430, training loss: 0.7669475078582764 = 0.09899294376373291 + 0.1 * 6.6795454025268555
Epoch 430, val loss: 0.9447212219238281
Epoch 440, training loss: 0.7574702501296997 = 0.08991383761167526 + 0.1 * 6.675563812255859
Epoch 440, val loss: 0.9547564387321472
Epoch 450, training loss: 0.750530481338501 = 0.08178212493658066 + 0.1 * 6.687483787536621
Epoch 450, val loss: 0.9651437401771545
Epoch 460, training loss: 0.7413568496704102 = 0.0745285302400589 + 0.1 * 6.668282985687256
Epoch 460, val loss: 0.9757855534553528
Epoch 470, training loss: 0.7333794236183167 = 0.06807219237089157 + 0.1 * 6.653071880340576
Epoch 470, val loss: 0.9865689873695374
Epoch 480, training loss: 0.7274338603019714 = 0.062316231429576874 + 0.1 * 6.6511759757995605
Epoch 480, val loss: 0.9975974559783936
Epoch 490, training loss: 0.7225203514099121 = 0.057175640016794205 + 0.1 * 6.653447151184082
Epoch 490, val loss: 1.0084936618804932
Epoch 500, training loss: 0.7165041565895081 = 0.05258442461490631 + 0.1 * 6.63919734954834
Epoch 500, val loss: 1.0193994045257568
Epoch 510, training loss: 0.7137691378593445 = 0.04847243055701256 + 0.1 * 6.65296745300293
Epoch 510, val loss: 1.0300891399383545
Epoch 520, training loss: 0.7066783905029297 = 0.0447879284620285 + 0.1 * 6.6189045906066895
Epoch 520, val loss: 1.040746808052063
Epoch 530, training loss: 0.7078039646148682 = 0.041476890444755554 + 0.1 * 6.663270473480225
Epoch 530, val loss: 1.051174521446228
Epoch 540, training loss: 0.7003172039985657 = 0.03851187601685524 + 0.1 * 6.618052959442139
Epoch 540, val loss: 1.0612494945526123
Epoch 550, training loss: 0.6965325474739075 = 0.035840146243572235 + 0.1 * 6.606924057006836
Epoch 550, val loss: 1.071260929107666
Epoch 560, training loss: 0.692814826965332 = 0.03342501074075699 + 0.1 * 6.593897819519043
Epoch 560, val loss: 1.0808970928192139
Epoch 570, training loss: 0.6903607845306396 = 0.031236454844474792 + 0.1 * 6.591242790222168
Epoch 570, val loss: 1.0904368162155151
Epoch 580, training loss: 0.6882079243659973 = 0.029247688129544258 + 0.1 * 6.589601993560791
Epoch 580, val loss: 1.099666953086853
Epoch 590, training loss: 0.6854972243309021 = 0.02743982896208763 + 0.1 * 6.580574035644531
Epoch 590, val loss: 1.1088032722473145
Epoch 600, training loss: 0.6833548545837402 = 0.025790968909859657 + 0.1 * 6.575638771057129
Epoch 600, val loss: 1.1175645589828491
Epoch 610, training loss: 0.6814804077148438 = 0.024287424981594086 + 0.1 * 6.571929931640625
Epoch 610, val loss: 1.126105546951294
Epoch 620, training loss: 0.6793643832206726 = 0.022913847118616104 + 0.1 * 6.564505100250244
Epoch 620, val loss: 1.1344438791275024
Epoch 630, training loss: 0.6796927452087402 = 0.021652840077877045 + 0.1 * 6.580399036407471
Epoch 630, val loss: 1.1425007581710815
Epoch 640, training loss: 0.6778024435043335 = 0.020495623350143433 + 0.1 * 6.573067665100098
Epoch 640, val loss: 1.1504261493682861
Epoch 650, training loss: 0.674920916557312 = 0.019432121887803078 + 0.1 * 6.554887771606445
Epoch 650, val loss: 1.1580619812011719
Epoch 660, training loss: 0.6735016107559204 = 0.018450209870934486 + 0.1 * 6.550514221191406
Epoch 660, val loss: 1.1655938625335693
Epoch 670, training loss: 0.6730743050575256 = 0.017542237415909767 + 0.1 * 6.555320739746094
Epoch 670, val loss: 1.1728545427322388
Epoch 680, training loss: 0.6719799041748047 = 0.016702188178896904 + 0.1 * 6.552777290344238
Epoch 680, val loss: 1.1800177097320557
Epoch 690, training loss: 0.6722472906112671 = 0.015922661870718002 + 0.1 * 6.563246250152588
Epoch 690, val loss: 1.1869139671325684
Epoch 700, training loss: 0.6693962216377258 = 0.015199479646980762 + 0.1 * 6.541967391967773
Epoch 700, val loss: 1.1936614513397217
Epoch 710, training loss: 0.6683645248413086 = 0.014526331797242165 + 0.1 * 6.538382053375244
Epoch 710, val loss: 1.2002404928207397
Epoch 720, training loss: 0.6680160760879517 = 0.013898437842726707 + 0.1 * 6.5411763191223145
Epoch 720, val loss: 1.2066148519515991
Epoch 730, training loss: 0.6663135886192322 = 0.013313760049641132 + 0.1 * 6.529997825622559
Epoch 730, val loss: 1.2128983736038208
Epoch 740, training loss: 0.6662257313728333 = 0.012765794061124325 + 0.1 * 6.534599304199219
Epoch 740, val loss: 1.2190148830413818
Epoch 750, training loss: 0.6651721596717834 = 0.012253301218152046 + 0.1 * 6.529188632965088
Epoch 750, val loss: 1.2249993085861206
Epoch 760, training loss: 0.6631456017494202 = 0.011772947385907173 + 0.1 * 6.513726711273193
Epoch 760, val loss: 1.230830192565918
Epoch 770, training loss: 0.6635076999664307 = 0.01132231019437313 + 0.1 * 6.521853446960449
Epoch 770, val loss: 1.2365875244140625
Epoch 780, training loss: 0.6638275384902954 = 0.010898047126829624 + 0.1 * 6.529294490814209
Epoch 780, val loss: 1.2421655654907227
Epoch 790, training loss: 0.6624770164489746 = 0.010499133728444576 + 0.1 * 6.519778728485107
Epoch 790, val loss: 1.2475453615188599
Epoch 800, training loss: 0.6615131497383118 = 0.010124665684998035 + 0.1 * 6.513884544372559
Epoch 800, val loss: 1.2529003620147705
Epoch 810, training loss: 0.6615455150604248 = 0.009770531207323074 + 0.1 * 6.517749786376953
Epoch 810, val loss: 1.2581695318222046
Epoch 820, training loss: 0.6594756841659546 = 0.009435840882360935 + 0.1 * 6.500398635864258
Epoch 820, val loss: 1.2632349729537964
Epoch 830, training loss: 0.6609336733818054 = 0.009119434282183647 + 0.1 * 6.518142223358154
Epoch 830, val loss: 1.26824951171875
Epoch 840, training loss: 0.6595046520233154 = 0.008820564486086369 + 0.1 * 6.506840229034424
Epoch 840, val loss: 1.2731467485427856
Epoch 850, training loss: 0.6581081748008728 = 0.00853748619556427 + 0.1 * 6.495707035064697
Epoch 850, val loss: 1.2779152393341064
Epoch 860, training loss: 0.6580197215080261 = 0.008268897421658039 + 0.1 * 6.4975080490112305
Epoch 860, val loss: 1.2826229333877563
Epoch 870, training loss: 0.6573000550270081 = 0.008014101535081863 + 0.1 * 6.492859363555908
Epoch 870, val loss: 1.2872198820114136
Epoch 880, training loss: 0.6583508849143982 = 0.007771703414618969 + 0.1 * 6.505791664123535
Epoch 880, val loss: 1.2917444705963135
Epoch 890, training loss: 0.6564818620681763 = 0.00754142738878727 + 0.1 * 6.489404201507568
Epoch 890, val loss: 1.2961641550064087
Epoch 900, training loss: 0.6563506722450256 = 0.007321884389966726 + 0.1 * 6.490287780761719
Epoch 900, val loss: 1.300477385520935
Epoch 910, training loss: 0.6553366184234619 = 0.0071132369339466095 + 0.1 * 6.482234001159668
Epoch 910, val loss: 1.3047360181808472
Epoch 920, training loss: 0.6581479907035828 = 0.006914060562849045 + 0.1 * 6.512339115142822
Epoch 920, val loss: 1.3088955879211426
Epoch 930, training loss: 0.6553804874420166 = 0.006724154110997915 + 0.1 * 6.486563205718994
Epoch 930, val loss: 1.312940239906311
Epoch 940, training loss: 0.6542726159095764 = 0.006543515715748072 + 0.1 * 6.477290630340576
Epoch 940, val loss: 1.316921591758728
Epoch 950, training loss: 0.6567031741142273 = 0.006370247807353735 + 0.1 * 6.503329277038574
Epoch 950, val loss: 1.320862889289856
Epoch 960, training loss: 0.6551160216331482 = 0.0062049636617302895 + 0.1 * 6.489110469818115
Epoch 960, val loss: 1.32470703125
Epoch 970, training loss: 0.6545388102531433 = 0.006046769674867392 + 0.1 * 6.484920024871826
Epoch 970, val loss: 1.3284531831741333
Epoch 980, training loss: 0.6530683040618896 = 0.005895555950701237 + 0.1 * 6.47172737121582
Epoch 980, val loss: 1.332213044166565
Epoch 990, training loss: 0.6548982858657837 = 0.005749941803514957 + 0.1 * 6.491483211517334
Epoch 990, val loss: 1.3358622789382935
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 2.8055851459503174 = 1.9458978176116943 + 0.1 * 8.596872329711914
Epoch 0, val loss: 1.947083830833435
Epoch 10, training loss: 2.7958948612213135 = 1.9362115859985352 + 0.1 * 8.596832275390625
Epoch 10, val loss: 1.9376758337020874
Epoch 20, training loss: 2.7842695713043213 = 1.9246097803115845 + 0.1 * 8.596597671508789
Epoch 20, val loss: 1.9260609149932861
Epoch 30, training loss: 2.7681596279144287 = 1.9086966514587402 + 0.1 * 8.594629287719727
Epoch 30, val loss: 1.9097923040390015
Epoch 40, training loss: 2.743212938308716 = 1.8854955434799194 + 0.1 * 8.577173233032227
Epoch 40, val loss: 1.8862059116363525
Epoch 50, training loss: 2.7002053260803223 = 1.852808952331543 + 0.1 * 8.47396469116211
Epoch 50, val loss: 1.8543274402618408
Epoch 60, training loss: 2.636587619781494 = 1.8151166439056396 + 0.1 * 8.21470832824707
Epoch 60, val loss: 1.8207526206970215
Epoch 70, training loss: 2.5859389305114746 = 1.7813607454299927 + 0.1 * 8.045783042907715
Epoch 70, val loss: 1.7923364639282227
Epoch 80, training loss: 2.5136122703552246 = 1.747138261795044 + 0.1 * 7.664740085601807
Epoch 80, val loss: 1.7622019052505493
Epoch 90, training loss: 2.4408435821533203 = 1.7042187452316284 + 0.1 * 7.36624813079834
Epoch 90, val loss: 1.724504828453064
Epoch 100, training loss: 2.3709053993225098 = 1.6456668376922607 + 0.1 * 7.252384185791016
Epoch 100, val loss: 1.6744158267974854
Epoch 110, training loss: 2.2843997478485107 = 1.5682194232940674 + 0.1 * 7.161803245544434
Epoch 110, val loss: 1.6105811595916748
Epoch 120, training loss: 2.1898281574249268 = 1.4797638654708862 + 0.1 * 7.100642204284668
Epoch 120, val loss: 1.5404950380325317
Epoch 130, training loss: 2.0943267345428467 = 1.389151692390442 + 0.1 * 7.051750659942627
Epoch 130, val loss: 1.4704394340515137
Epoch 140, training loss: 2.003262996673584 = 1.3023159503936768 + 0.1 * 7.009469985961914
Epoch 140, val loss: 1.407576322555542
Epoch 150, training loss: 1.9166409969329834 = 1.218662142753601 + 0.1 * 6.979788303375244
Epoch 150, val loss: 1.349058747291565
Epoch 160, training loss: 1.8365528583526611 = 1.139922857284546 + 0.1 * 6.9663004875183105
Epoch 160, val loss: 1.2958179712295532
Epoch 170, training loss: 1.7637102603912354 = 1.0701402425765991 + 0.1 * 6.9357008934021
Epoch 170, val loss: 1.2514123916625977
Epoch 180, training loss: 1.699894666671753 = 1.0083246231079102 + 0.1 * 6.915699481964111
Epoch 180, val loss: 1.213762879371643
Epoch 190, training loss: 1.64443838596344 = 0.9531188011169434 + 0.1 * 6.913195610046387
Epoch 190, val loss: 1.181243896484375
Epoch 200, training loss: 1.5905437469482422 = 0.9031146168708801 + 0.1 * 6.87429141998291
Epoch 200, val loss: 1.1525715589523315
Epoch 210, training loss: 1.5405187606811523 = 0.854812741279602 + 0.1 * 6.85706090927124
Epoch 210, val loss: 1.1238449811935425
Epoch 220, training loss: 1.490234136581421 = 0.8065202832221985 + 0.1 * 6.837137699127197
Epoch 220, val loss: 1.0942492485046387
Epoch 230, training loss: 1.4404664039611816 = 0.7572116851806641 + 0.1 * 6.832547664642334
Epoch 230, val loss: 1.0634117126464844
Epoch 240, training loss: 1.3875120878219604 = 0.7073925137519836 + 0.1 * 6.8011956214904785
Epoch 240, val loss: 1.0321648120880127
Epoch 250, training loss: 1.3362606763839722 = 0.6573194861412048 + 0.1 * 6.789412021636963
Epoch 250, val loss: 1.0010497570037842
Epoch 260, training loss: 1.2874865531921387 = 0.6087418794631958 + 0.1 * 6.787446975708008
Epoch 260, val loss: 0.9720983505249023
Epoch 270, training loss: 1.2397067546844482 = 0.5632343888282776 + 0.1 * 6.764723300933838
Epoch 270, val loss: 0.9469196796417236
Epoch 280, training loss: 1.1977207660675049 = 0.5211958885192871 + 0.1 * 6.765249252319336
Epoch 280, val loss: 0.9261444211006165
Epoch 290, training loss: 1.1582709550857544 = 0.48314589262008667 + 0.1 * 6.751250267028809
Epoch 290, val loss: 0.9104312658309937
Epoch 300, training loss: 1.1219677925109863 = 0.4486749470233917 + 0.1 * 6.732928276062012
Epoch 300, val loss: 0.8995418548583984
Epoch 310, training loss: 1.0907866954803467 = 0.4170892536640167 + 0.1 * 6.736974239349365
Epoch 310, val loss: 0.8926668763160706
Epoch 320, training loss: 1.0615471601486206 = 0.3881022334098816 + 0.1 * 6.73444938659668
Epoch 320, val loss: 0.8888500928878784
Epoch 330, training loss: 1.0320913791656494 = 0.36093688011169434 + 0.1 * 6.711544513702393
Epoch 330, val loss: 0.8872148394584656
Epoch 340, training loss: 1.005513310432434 = 0.3348749876022339 + 0.1 * 6.706383228302002
Epoch 340, val loss: 0.8871080279350281
Epoch 350, training loss: 0.9809832572937012 = 0.3095158040523529 + 0.1 * 6.71467399597168
Epoch 350, val loss: 0.8881481289863586
Epoch 360, training loss: 0.9543981552124023 = 0.28482189774513245 + 0.1 * 6.695762634277344
Epoch 360, val loss: 0.8899245262145996
Epoch 370, training loss: 0.9299827814102173 = 0.2606861889362335 + 0.1 * 6.692965984344482
Epoch 370, val loss: 0.8923847675323486
Epoch 380, training loss: 0.9053015112876892 = 0.23696191608905792 + 0.1 * 6.683395862579346
Epoch 380, val loss: 0.8956153988838196
Epoch 390, training loss: 0.8827760219573975 = 0.21384896337985992 + 0.1 * 6.689270496368408
Epoch 390, val loss: 0.8996859788894653
Epoch 400, training loss: 0.8604235649108887 = 0.19193290174007416 + 0.1 * 6.684906482696533
Epoch 400, val loss: 0.9046822786331177
Epoch 410, training loss: 0.8392481803894043 = 0.1716906875371933 + 0.1 * 6.675574779510498
Epoch 410, val loss: 0.9108718037605286
Epoch 420, training loss: 0.8196072578430176 = 0.15342094004154205 + 0.1 * 6.661863327026367
Epoch 420, val loss: 0.9183083772659302
Epoch 430, training loss: 0.8055095672607422 = 0.13726645708084106 + 0.1 * 6.682430744171143
Epoch 430, val loss: 0.9268836379051208
Epoch 440, training loss: 0.7895626425743103 = 0.12323415279388428 + 0.1 * 6.663284778594971
Epoch 440, val loss: 0.9364508986473083
Epoch 450, training loss: 0.7763345241546631 = 0.11104931682348251 + 0.1 * 6.6528520584106445
Epoch 450, val loss: 0.9466590881347656
Epoch 460, training loss: 0.7654183506965637 = 0.10045038163661957 + 0.1 * 6.649679183959961
Epoch 460, val loss: 0.9575502872467041
Epoch 470, training loss: 0.7550714015960693 = 0.09121257811784744 + 0.1 * 6.638587951660156
Epoch 470, val loss: 0.9688621163368225
Epoch 480, training loss: 0.7470054030418396 = 0.08310725539922714 + 0.1 * 6.638981342315674
Epoch 480, val loss: 0.9805009365081787
Epoch 490, training loss: 0.7398865818977356 = 0.0759657695889473 + 0.1 * 6.6392083168029785
Epoch 490, val loss: 0.9923204183578491
Epoch 500, training loss: 0.7337613105773926 = 0.069659523665905 + 0.1 * 6.641017913818359
Epoch 500, val loss: 1.0042496919631958
Epoch 510, training loss: 0.7270309329032898 = 0.06407380849123001 + 0.1 * 6.629570960998535
Epoch 510, val loss: 1.0161972045898438
Epoch 520, training loss: 0.721261203289032 = 0.05909370630979538 + 0.1 * 6.62167501449585
Epoch 520, val loss: 1.0281707048416138
Epoch 530, training loss: 0.7170056104660034 = 0.054643526673316956 + 0.1 * 6.623620986938477
Epoch 530, val loss: 1.0399469137191772
Epoch 540, training loss: 0.7119545340538025 = 0.05066347122192383 + 0.1 * 6.612910270690918
Epoch 540, val loss: 1.0518053770065308
Epoch 550, training loss: 0.7094380259513855 = 0.047082994133234024 + 0.1 * 6.623549938201904
Epoch 550, val loss: 1.063302993774414
Epoch 560, training loss: 0.704576313495636 = 0.043854646384716034 + 0.1 * 6.6072163581848145
Epoch 560, val loss: 1.0748851299285889
Epoch 570, training loss: 0.7007600665092468 = 0.04092678800225258 + 0.1 * 6.598332405090332
Epoch 570, val loss: 1.086167335510254
Epoch 580, training loss: 0.6993942260742188 = 0.03826222941279411 + 0.1 * 6.611319541931152
Epoch 580, val loss: 1.0973931550979614
Epoch 590, training loss: 0.6968801617622375 = 0.03584371134638786 + 0.1 * 6.6103644371032715
Epoch 590, val loss: 1.1083046197891235
Epoch 600, training loss: 0.6931372880935669 = 0.03363943099975586 + 0.1 * 6.594978332519531
Epoch 600, val loss: 1.119127631187439
Epoch 610, training loss: 0.6922147870063782 = 0.031625911593437195 + 0.1 * 6.605888366699219
Epoch 610, val loss: 1.1296592950820923
Epoch 620, training loss: 0.6889832615852356 = 0.029787367209792137 + 0.1 * 6.591958999633789
Epoch 620, val loss: 1.1401093006134033
Epoch 630, training loss: 0.686861515045166 = 0.028102926909923553 + 0.1 * 6.58758544921875
Epoch 630, val loss: 1.1500813961029053
Epoch 640, training loss: 0.6846413016319275 = 0.026557914912700653 + 0.1 * 6.580833911895752
Epoch 640, val loss: 1.1601728200912476
Epoch 650, training loss: 0.6826165914535522 = 0.025133194401860237 + 0.1 * 6.574833869934082
Epoch 650, val loss: 1.1698424816131592
Epoch 660, training loss: 0.6820620894432068 = 0.023814700543880463 + 0.1 * 6.5824737548828125
Epoch 660, val loss: 1.1795077323913574
Epoch 670, training loss: 0.6814397573471069 = 0.022597990930080414 + 0.1 * 6.5884175300598145
Epoch 670, val loss: 1.1886804103851318
Epoch 680, training loss: 0.678536057472229 = 0.02147597074508667 + 0.1 * 6.570600986480713
Epoch 680, val loss: 1.1978915929794312
Epoch 690, training loss: 0.6772393584251404 = 0.020436590537428856 + 0.1 * 6.568027496337891
Epoch 690, val loss: 1.2067099809646606
Epoch 700, training loss: 0.675837516784668 = 0.019472939893603325 + 0.1 * 6.563645362854004
Epoch 700, val loss: 1.2155314683914185
Epoch 710, training loss: 0.6743557453155518 = 0.01857457496225834 + 0.1 * 6.557811260223389
Epoch 710, val loss: 1.224090337753296
Epoch 720, training loss: 0.6741948127746582 = 0.017736800014972687 + 0.1 * 6.564579963684082
Epoch 720, val loss: 1.232386589050293
Epoch 730, training loss: 0.6736019253730774 = 0.016957031562924385 + 0.1 * 6.56644868850708
Epoch 730, val loss: 1.2406171560287476
Epoch 740, training loss: 0.6711858510971069 = 0.01622956246137619 + 0.1 * 6.549562931060791
Epoch 740, val loss: 1.2485376596450806
Epoch 750, training loss: 0.6708611249923706 = 0.015549885109066963 + 0.1 * 6.553112030029297
Epoch 750, val loss: 1.2564334869384766
Epoch 760, training loss: 0.6705043911933899 = 0.014913042075932026 + 0.1 * 6.55591344833374
Epoch 760, val loss: 1.2641334533691406
Epoch 770, training loss: 0.6690713167190552 = 0.01431606151163578 + 0.1 * 6.547552108764648
Epoch 770, val loss: 1.271422266960144
Epoch 780, training loss: 0.6686820387840271 = 0.01375844981521368 + 0.1 * 6.549236297607422
Epoch 780, val loss: 1.2789064645767212
Epoch 790, training loss: 0.6689963340759277 = 0.013233530335128307 + 0.1 * 6.5576276779174805
Epoch 790, val loss: 1.2860099077224731
Epoch 800, training loss: 0.6659944653511047 = 0.012740051373839378 + 0.1 * 6.532543659210205
Epoch 800, val loss: 1.2930206060409546
Epoch 810, training loss: 0.6665552854537964 = 0.012274425476789474 + 0.1 * 6.542808532714844
Epoch 810, val loss: 1.2999356985092163
Epoch 820, training loss: 0.6660247445106506 = 0.011834627948701382 + 0.1 * 6.541901111602783
Epoch 820, val loss: 1.3066543340682983
Epoch 830, training loss: 0.665336012840271 = 0.011420977301895618 + 0.1 * 6.539150238037109
Epoch 830, val loss: 1.3130782842636108
Epoch 840, training loss: 0.66435706615448 = 0.011030775494873524 + 0.1 * 6.533262729644775
Epoch 840, val loss: 1.319619059562683
Epoch 850, training loss: 0.6644951105117798 = 0.010661784559488297 + 0.1 * 6.538333415985107
Epoch 850, val loss: 1.3258647918701172
Epoch 860, training loss: 0.6621378064155579 = 0.010312296450138092 + 0.1 * 6.51825475692749
Epoch 860, val loss: 1.3320095539093018
Epoch 870, training loss: 0.663199782371521 = 0.009980631060898304 + 0.1 * 6.532191753387451
Epoch 870, val loss: 1.3381719589233398
Epoch 880, training loss: 0.6618194580078125 = 0.00966504868119955 + 0.1 * 6.521543502807617
Epoch 880, val loss: 1.3439857959747314
Epoch 890, training loss: 0.6624955534934998 = 0.00936637632548809 + 0.1 * 6.531291484832764
Epoch 890, val loss: 1.3497549295425415
Epoch 900, training loss: 0.661360502243042 = 0.009083271957933903 + 0.1 * 6.522772312164307
Epoch 900, val loss: 1.3555455207824707
Epoch 910, training loss: 0.6600747108459473 = 0.00881393626332283 + 0.1 * 6.512608051300049
Epoch 910, val loss: 1.3610745668411255
Epoch 920, training loss: 0.6614348888397217 = 0.008557174354791641 + 0.1 * 6.5287766456604
Epoch 920, val loss: 1.3666685819625854
Epoch 930, training loss: 0.6601330637931824 = 0.008312252350151539 + 0.1 * 6.5182085037231445
Epoch 930, val loss: 1.3719006776809692
Epoch 940, training loss: 0.6600464582443237 = 0.008079520426690578 + 0.1 * 6.519669055938721
Epoch 940, val loss: 1.37730872631073
Epoch 950, training loss: 0.6591412425041199 = 0.007857303135097027 + 0.1 * 6.512839317321777
Epoch 950, val loss: 1.3823860883712769
Epoch 960, training loss: 0.6581194400787354 = 0.007645219098776579 + 0.1 * 6.504742622375488
Epoch 960, val loss: 1.3875725269317627
Epoch 970, training loss: 0.6587741374969482 = 0.0074421013705432415 + 0.1 * 6.513319969177246
Epoch 970, val loss: 1.392575740814209
Epoch 980, training loss: 0.6582768559455872 = 0.007248261943459511 + 0.1 * 6.510285377502441
Epoch 980, val loss: 1.3973640203475952
Epoch 990, training loss: 0.6573315858840942 = 0.0070628272369503975 + 0.1 * 6.502687454223633
Epoch 990, val loss: 1.4023572206497192
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 2.8161630630493164 = 1.956475019454956 + 0.1 * 8.596879005432129
Epoch 0, val loss: 1.94941246509552
Epoch 10, training loss: 2.805485725402832 = 1.945802092552185 + 0.1 * 8.596837043762207
Epoch 10, val loss: 1.9388127326965332
Epoch 20, training loss: 2.7928123474121094 = 1.933148741722107 + 0.1 * 8.596634864807129
Epoch 20, val loss: 1.926121711730957
Epoch 30, training loss: 2.775348663330078 = 1.915828824043274 + 0.1 * 8.595199584960938
Epoch 30, val loss: 1.908930778503418
Epoch 40, training loss: 2.748609781265259 = 1.8903816938400269 + 0.1 * 8.582281112670898
Epoch 40, val loss: 1.8843003511428833
Epoch 50, training loss: 2.7054688930511475 = 1.8546288013458252 + 0.1 * 8.508400917053223
Epoch 50, val loss: 1.8516696691513062
Epoch 60, training loss: 2.645784378051758 = 1.815481424331665 + 0.1 * 8.303030014038086
Epoch 60, val loss: 1.8202582597732544
Epoch 70, training loss: 2.6036765575408936 = 1.7837101221084595 + 0.1 * 8.199664115905762
Epoch 70, val loss: 1.7970774173736572
Epoch 80, training loss: 2.5383431911468506 = 1.7509026527404785 + 0.1 * 7.8744049072265625
Epoch 80, val loss: 1.7675087451934814
Epoch 90, training loss: 2.4485924243927 = 1.7092405557632446 + 0.1 * 7.393518447875977
Epoch 90, val loss: 1.7291918992996216
Epoch 100, training loss: 2.3736021518707275 = 1.6545023918151855 + 0.1 * 7.190997123718262
Epoch 100, val loss: 1.6822563409805298
Epoch 110, training loss: 2.2967894077301025 = 1.5833371877670288 + 0.1 * 7.134521484375
Epoch 110, val loss: 1.6235896348953247
Epoch 120, training loss: 2.212557792663574 = 1.50321364402771 + 0.1 * 7.093442440032959
Epoch 120, val loss: 1.558393955230713
Epoch 130, training loss: 2.127868890762329 = 1.422049880027771 + 0.1 * 7.058189868927002
Epoch 130, val loss: 1.4951986074447632
Epoch 140, training loss: 2.0448787212371826 = 1.3426345586776733 + 0.1 * 7.022441864013672
Epoch 140, val loss: 1.434511661529541
Epoch 150, training loss: 1.96107816696167 = 1.262350082397461 + 0.1 * 6.987281322479248
Epoch 150, val loss: 1.372296690940857
Epoch 160, training loss: 1.8760995864868164 = 1.1795244216918945 + 0.1 * 6.965750694274902
Epoch 160, val loss: 1.3086093664169312
Epoch 170, training loss: 1.7867240905761719 = 1.092345118522644 + 0.1 * 6.943789958953857
Epoch 170, val loss: 1.2414729595184326
Epoch 180, training loss: 1.6956768035888672 = 1.002994418144226 + 0.1 * 6.92682409286499
Epoch 180, val loss: 1.1731809377670288
Epoch 190, training loss: 1.6085472106933594 = 0.9174460768699646 + 0.1 * 6.911011695861816
Epoch 190, val loss: 1.109293818473816
Epoch 200, training loss: 1.5278692245483398 = 0.8385911583900452 + 0.1 * 6.8927812576293945
Epoch 200, val loss: 1.0520747900009155
Epoch 210, training loss: 1.455925464630127 = 0.7673459053039551 + 0.1 * 6.885795593261719
Epoch 210, val loss: 1.0021531581878662
Epoch 220, training loss: 1.390990972518921 = 0.7050454616546631 + 0.1 * 6.859455108642578
Epoch 220, val loss: 0.960891604423523
Epoch 230, training loss: 1.3338711261749268 = 0.6494364142417908 + 0.1 * 6.84434700012207
Epoch 230, val loss: 0.9262657165527344
Epoch 240, training loss: 1.2824006080627441 = 0.5988432765007019 + 0.1 * 6.835573673248291
Epoch 240, val loss: 0.8967411518096924
Epoch 250, training loss: 1.2335386276245117 = 0.551971971988678 + 0.1 * 6.815666198730469
Epoch 250, val loss: 0.8713892102241516
Epoch 260, training loss: 1.187628984451294 = 0.5072206258773804 + 0.1 * 6.804083824157715
Epoch 260, val loss: 0.848353922367096
Epoch 270, training loss: 1.143509030342102 = 0.46452632546424866 + 0.1 * 6.789826393127441
Epoch 270, val loss: 0.8278839588165283
Epoch 280, training loss: 1.1018993854522705 = 0.42378318309783936 + 0.1 * 6.781162738800049
Epoch 280, val loss: 0.8103942275047302
Epoch 290, training loss: 1.0619860887527466 = 0.38475197553634644 + 0.1 * 6.772341251373291
Epoch 290, val loss: 0.7956809401512146
Epoch 300, training loss: 1.0258451700210571 = 0.3479313254356384 + 0.1 * 6.779138088226318
Epoch 300, val loss: 0.7842520475387573
Epoch 310, training loss: 0.9897881150245667 = 0.31355810165405273 + 0.1 * 6.76230001449585
Epoch 310, val loss: 0.7763906121253967
Epoch 320, training loss: 0.9570779800415039 = 0.28141745924949646 + 0.1 * 6.75660514831543
Epoch 320, val loss: 0.7715022563934326
Epoch 330, training loss: 0.9262048006057739 = 0.2517209053039551 + 0.1 * 6.744838714599609
Epoch 330, val loss: 0.7695574164390564
Epoch 340, training loss: 0.9005479216575623 = 0.22446449100971222 + 0.1 * 6.760834217071533
Epoch 340, val loss: 0.7703052163124084
Epoch 350, training loss: 0.8728510141372681 = 0.19985170662403107 + 0.1 * 6.7299933433532715
Epoch 350, val loss: 0.7734102010726929
Epoch 360, training loss: 0.8506970405578613 = 0.1776975393295288 + 0.1 * 6.729994773864746
Epoch 360, val loss: 0.7785489559173584
Epoch 370, training loss: 0.8312432765960693 = 0.15794913470745087 + 0.1 * 6.732941150665283
Epoch 370, val loss: 0.7855053544044495
Epoch 380, training loss: 0.8116525411605835 = 0.14061713218688965 + 0.1 * 6.710353851318359
Epoch 380, val loss: 0.7941074967384338
Epoch 390, training loss: 0.795601487159729 = 0.1254117786884308 + 0.1 * 6.701897144317627
Epoch 390, val loss: 0.8038674592971802
Epoch 400, training loss: 0.7829926609992981 = 0.11215133965015411 + 0.1 * 6.708413124084473
Epoch 400, val loss: 0.8146552443504333
Epoch 410, training loss: 0.7699944376945496 = 0.10075713694095612 + 0.1 * 6.692372798919678
Epoch 410, val loss: 0.8260952830314636
Epoch 420, training loss: 0.7594788074493408 = 0.09090027213096619 + 0.1 * 6.685784816741943
Epoch 420, val loss: 0.8379495739936829
Epoch 430, training loss: 0.7503857016563416 = 0.08230500668287277 + 0.1 * 6.680806636810303
Epoch 430, val loss: 0.8501898646354675
Epoch 440, training loss: 0.742587685585022 = 0.07481665909290314 + 0.1 * 6.67771053314209
Epoch 440, val loss: 0.8624853491783142
Epoch 450, training loss: 0.7346848249435425 = 0.06828970462083817 + 0.1 * 6.663951396942139
Epoch 450, val loss: 0.875044047832489
Epoch 460, training loss: 0.731300413608551 = 0.06254806369543076 + 0.1 * 6.68752384185791
Epoch 460, val loss: 0.8873286843299866
Epoch 470, training loss: 0.7236577868461609 = 0.05750342085957527 + 0.1 * 6.661543846130371
Epoch 470, val loss: 0.8998375535011292
Epoch 480, training loss: 0.7189311385154724 = 0.05302818864583969 + 0.1 * 6.659029483795166
Epoch 480, val loss: 0.9118636250495911
Epoch 490, training loss: 0.7146768569946289 = 0.04906420409679413 + 0.1 * 6.656126499176025
Epoch 490, val loss: 0.923782229423523
Epoch 500, training loss: 0.7090978622436523 = 0.04554075002670288 + 0.1 * 6.635571002960205
Epoch 500, val loss: 0.9355415105819702
Epoch 510, training loss: 0.7061126828193665 = 0.042369578033685684 + 0.1 * 6.637430667877197
Epoch 510, val loss: 0.9469542503356934
Epoch 520, training loss: 0.7027087211608887 = 0.03952644020318985 + 0.1 * 6.63182258605957
Epoch 520, val loss: 0.9580966234207153
Epoch 530, training loss: 0.6995376348495483 = 0.036971282213926315 + 0.1 * 6.6256632804870605
Epoch 530, val loss: 0.969161331653595
Epoch 540, training loss: 0.6971582770347595 = 0.03465235233306885 + 0.1 * 6.625059127807617
Epoch 540, val loss: 0.9798443913459778
Epoch 550, training loss: 0.6949231028556824 = 0.03255056217312813 + 0.1 * 6.623725414276123
Epoch 550, val loss: 0.9901027083396912
Epoch 560, training loss: 0.6911196112632751 = 0.03065193071961403 + 0.1 * 6.604676723480225
Epoch 560, val loss: 1.0004068613052368
Epoch 570, training loss: 0.689379096031189 = 0.02891155704855919 + 0.1 * 6.604675769805908
Epoch 570, val loss: 1.0102124214172363
Epoch 580, training loss: 0.6872385144233704 = 0.027318671345710754 + 0.1 * 6.599198341369629
Epoch 580, val loss: 1.0198888778686523
Epoch 590, training loss: 0.6866987943649292 = 0.02585989609360695 + 0.1 * 6.608388900756836
Epoch 590, val loss: 1.0291143655776978
Epoch 600, training loss: 0.6846861839294434 = 0.024524889886379242 + 0.1 * 6.6016130447387695
Epoch 600, val loss: 1.0384128093719482
Epoch 610, training loss: 0.682053804397583 = 0.02329665794968605 + 0.1 * 6.587571620941162
Epoch 610, val loss: 1.0472460985183716
Epoch 620, training loss: 0.6804795265197754 = 0.02216009423136711 + 0.1 * 6.583193778991699
Epoch 620, val loss: 1.0558918714523315
Epoch 630, training loss: 0.6821857690811157 = 0.021107390522956848 + 0.1 * 6.61078405380249
Epoch 630, val loss: 1.0643649101257324
Epoch 640, training loss: 0.677575409412384 = 0.020134804770350456 + 0.1 * 6.574406147003174
Epoch 640, val loss: 1.0726289749145508
Epoch 650, training loss: 0.6767915487289429 = 0.019233623519539833 + 0.1 * 6.575578689575195
Epoch 650, val loss: 1.0807828903198242
Epoch 660, training loss: 0.6749993562698364 = 0.018392732366919518 + 0.1 * 6.566066265106201
Epoch 660, val loss: 1.088597297668457
Epoch 670, training loss: 0.6765934824943542 = 0.017611153423786163 + 0.1 * 6.589823246002197
Epoch 670, val loss: 1.0963327884674072
Epoch 680, training loss: 0.6721376180648804 = 0.01688133366405964 + 0.1 * 6.552562713623047
Epoch 680, val loss: 1.1039139032363892
Epoch 690, training loss: 0.6708229780197144 = 0.016199631616473198 + 0.1 * 6.546233177185059
Epoch 690, val loss: 1.1113474369049072
Epoch 700, training loss: 0.6711038947105408 = 0.015559820458292961 + 0.1 * 6.555440902709961
Epoch 700, val loss: 1.1183781623840332
Epoch 710, training loss: 0.6704238057136536 = 0.014962312765419483 + 0.1 * 6.554615020751953
Epoch 710, val loss: 1.125662088394165
Epoch 720, training loss: 0.6697128415107727 = 0.014401332475244999 + 0.1 * 6.553114891052246
Epoch 720, val loss: 1.1325193643569946
Epoch 730, training loss: 0.6667107939720154 = 0.013872400857508183 + 0.1 * 6.528383731842041
Epoch 730, val loss: 1.1391738653182983
Epoch 740, training loss: 0.6676245331764221 = 0.01337469182908535 + 0.1 * 6.542498588562012
Epoch 740, val loss: 1.145972728729248
Epoch 750, training loss: 0.6681574583053589 = 0.012906285002827644 + 0.1 * 6.552511215209961
Epoch 750, val loss: 1.1523523330688477
Epoch 760, training loss: 0.6648077368736267 = 0.012464366853237152 + 0.1 * 6.523433685302734
Epoch 760, val loss: 1.1588077545166016
Epoch 770, training loss: 0.6666048765182495 = 0.012046083807945251 + 0.1 * 6.545587539672852
Epoch 770, val loss: 1.1652182340621948
Epoch 780, training loss: 0.6639452576637268 = 0.01164994202554226 + 0.1 * 6.522952556610107
Epoch 780, val loss: 1.1712297201156616
Epoch 790, training loss: 0.6627906560897827 = 0.011275936849415302 + 0.1 * 6.5151472091674805
Epoch 790, val loss: 1.1774539947509766
Epoch 800, training loss: 0.6616597771644592 = 0.01091943122446537 + 0.1 * 6.5074028968811035
Epoch 800, val loss: 1.1832517385482788
Epoch 810, training loss: 0.6622519493103027 = 0.010582201182842255 + 0.1 * 6.516697406768799
Epoch 810, val loss: 1.189176321029663
Epoch 820, training loss: 0.6616390943527222 = 0.0102621428668499 + 0.1 * 6.513769626617432
Epoch 820, val loss: 1.1949207782745361
Epoch 830, training loss: 0.6637111306190491 = 0.009956513531506062 + 0.1 * 6.537545680999756
Epoch 830, val loss: 1.2005407810211182
Epoch 840, training loss: 0.6605125665664673 = 0.009666978381574154 + 0.1 * 6.508455753326416
Epoch 840, val loss: 1.2060927152633667
Epoch 850, training loss: 0.6602430939674377 = 0.009390885941684246 + 0.1 * 6.508522033691406
Epoch 850, val loss: 1.2116175889968872
Epoch 860, training loss: 0.6586876511573792 = 0.009127041324973106 + 0.1 * 6.495605945587158
Epoch 860, val loss: 1.2168354988098145
Epoch 870, training loss: 0.6623643040657043 = 0.008874627761542797 + 0.1 * 6.534896373748779
Epoch 870, val loss: 1.2220854759216309
Epoch 880, training loss: 0.6591358184814453 = 0.008635216392576694 + 0.1 * 6.505005836486816
Epoch 880, val loss: 1.227189302444458
Epoch 890, training loss: 0.6583133935928345 = 0.008405710570514202 + 0.1 * 6.499076843261719
Epoch 890, val loss: 1.2324458360671997
Epoch 900, training loss: 0.6577403545379639 = 0.008185822516679764 + 0.1 * 6.495545387268066
Epoch 900, val loss: 1.237195372581482
Epoch 910, training loss: 0.6560419797897339 = 0.007976262830197811 + 0.1 * 6.48065710067749
Epoch 910, val loss: 1.2421565055847168
Epoch 920, training loss: 0.6573639512062073 = 0.007775230798870325 + 0.1 * 6.49588680267334
Epoch 920, val loss: 1.2471256256103516
Epoch 930, training loss: 0.6562582850456238 = 0.007583141326904297 + 0.1 * 6.486751556396484
Epoch 930, val loss: 1.251760482788086
Epoch 940, training loss: 0.65446937084198 = 0.007398194633424282 + 0.1 * 6.470711708068848
Epoch 940, val loss: 1.2565118074417114
Epoch 950, training loss: 0.6559945344924927 = 0.007220510393381119 + 0.1 * 6.4877400398254395
Epoch 950, val loss: 1.2611606121063232
Epoch 960, training loss: 0.6543475985527039 = 0.007050071842968464 + 0.1 * 6.47297477722168
Epoch 960, val loss: 1.2657079696655273
Epoch 970, training loss: 0.657380998134613 = 0.006886073388159275 + 0.1 * 6.50494909286499
Epoch 970, val loss: 1.270293116569519
Epoch 980, training loss: 0.6545520424842834 = 0.006728603970259428 + 0.1 * 6.47823429107666
Epoch 980, val loss: 1.2745753526687622
Epoch 990, training loss: 0.6558020114898682 = 0.006577474530786276 + 0.1 * 6.492245197296143
Epoch 990, val loss: 1.2789901494979858
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8170795993674222
The final CL Acc:0.74691, 0.00175, The final GNN Acc:0.81989, 0.00203
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13224])
remove edge: torch.Size([2, 7944])
updated graph: torch.Size([2, 10612])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8027706146240234 = 1.9430843591690063 + 0.1 * 8.596863746643066
Epoch 0, val loss: 1.9460575580596924
Epoch 10, training loss: 2.7925078868865967 = 1.9328281879425049 + 0.1 * 8.596796035766602
Epoch 10, val loss: 1.9353052377700806
Epoch 20, training loss: 2.779963493347168 = 1.9203298091888428 + 0.1 * 8.59633731842041
Epoch 20, val loss: 1.9220000505447388
Epoch 30, training loss: 2.7622110843658447 = 1.903049349784851 + 0.1 * 8.591617584228516
Epoch 30, val loss: 1.9036484956741333
Epoch 40, training loss: 2.7334952354431152 = 1.87786066532135 + 0.1 * 8.55634593963623
Epoch 40, val loss: 1.8773974180221558
Epoch 50, training loss: 2.680154800415039 = 1.8437919616699219 + 0.1 * 8.363627433776855
Epoch 50, val loss: 1.843956708908081
Epoch 60, training loss: 2.6280136108398438 = 1.8063846826553345 + 0.1 * 8.216290473937988
Epoch 60, val loss: 1.810465931892395
Epoch 70, training loss: 2.578963279724121 = 1.7724214792251587 + 0.1 * 8.06541919708252
Epoch 70, val loss: 1.782530665397644
Epoch 80, training loss: 2.5045254230499268 = 1.7345167398452759 + 0.1 * 7.700087070465088
Epoch 80, val loss: 1.7492485046386719
Epoch 90, training loss: 2.4153099060058594 = 1.685625672340393 + 0.1 * 7.2968430519104
Epoch 90, val loss: 1.7061712741851807
Epoch 100, training loss: 2.336580753326416 = 1.620699405670166 + 0.1 * 7.158812999725342
Epoch 100, val loss: 1.6508255004882812
Epoch 110, training loss: 2.2473621368408203 = 1.5394469499588013 + 0.1 * 7.079152584075928
Epoch 110, val loss: 1.581869125366211
Epoch 120, training loss: 2.1530797481536865 = 1.449992060661316 + 0.1 * 7.030877590179443
Epoch 120, val loss: 1.5077083110809326
Epoch 130, training loss: 2.057417869567871 = 1.357688307762146 + 0.1 * 6.997295379638672
Epoch 130, val loss: 1.433653712272644
Epoch 140, training loss: 1.9621855020523071 = 1.2644838094711304 + 0.1 * 6.977016925811768
Epoch 140, val loss: 1.3599811792373657
Epoch 150, training loss: 1.8662066459655762 = 1.170212745666504 + 0.1 * 6.959939002990723
Epoch 150, val loss: 1.28581702709198
Epoch 160, training loss: 1.769707202911377 = 1.075284719467163 + 0.1 * 6.944225311279297
Epoch 160, val loss: 1.212701678276062
Epoch 170, training loss: 1.6760480403900146 = 0.982772171497345 + 0.1 * 6.93275785446167
Epoch 170, val loss: 1.1432002782821655
Epoch 180, training loss: 1.5873013734817505 = 0.8958801627159119 + 0.1 * 6.914212226867676
Epoch 180, val loss: 1.078853726387024
Epoch 190, training loss: 1.5057647228240967 = 0.8158782720565796 + 0.1 * 6.898864269256592
Epoch 190, val loss: 1.020558476448059
Epoch 200, training loss: 1.432755470275879 = 0.744240939617157 + 0.1 * 6.8851447105407715
Epoch 200, val loss: 0.9698054790496826
Epoch 210, training loss: 1.3683701753616333 = 0.6809963583946228 + 0.1 * 6.873737812042236
Epoch 210, val loss: 0.9271790385246277
Epoch 220, training loss: 1.3122152090072632 = 0.6257139444351196 + 0.1 * 6.8650126457214355
Epoch 220, val loss: 0.8927135467529297
Epoch 230, training loss: 1.2620618343353271 = 0.5766301155090332 + 0.1 * 6.854317665100098
Epoch 230, val loss: 0.8650848865509033
Epoch 240, training loss: 1.2193225622177124 = 0.5326287746429443 + 0.1 * 6.866937637329102
Epoch 240, val loss: 0.8432326912879944
Epoch 250, training loss: 1.1780729293823242 = 0.49320968985557556 + 0.1 * 6.8486328125
Epoch 250, val loss: 0.8262860178947449
Epoch 260, training loss: 1.1404929161071777 = 0.4570039212703705 + 0.1 * 6.834889888763428
Epoch 260, val loss: 0.8129846453666687
Epoch 270, training loss: 1.105676293373108 = 0.42287299036979675 + 0.1 * 6.828032493591309
Epoch 270, val loss: 0.8025482892990112
Epoch 280, training loss: 1.0747041702270508 = 0.3900456726551056 + 0.1 * 6.846585273742676
Epoch 280, val loss: 0.7943288683891296
Epoch 290, training loss: 1.0399315357208252 = 0.35822540521621704 + 0.1 * 6.817060947418213
Epoch 290, val loss: 0.7878401875495911
Epoch 300, training loss: 1.008100986480713 = 0.3266851305961609 + 0.1 * 6.814158916473389
Epoch 300, val loss: 0.7825326323509216
Epoch 310, training loss: 0.9772685766220093 = 0.2952125370502472 + 0.1 * 6.820560455322266
Epoch 310, val loss: 0.7781790494918823
Epoch 320, training loss: 0.9445971250534058 = 0.26418811082839966 + 0.1 * 6.8040900230407715
Epoch 320, val loss: 0.7745241522789001
Epoch 330, training loss: 0.9133304357528687 = 0.23411284387111664 + 0.1 * 6.792175769805908
Epoch 330, val loss: 0.7718232274055481
Epoch 340, training loss: 0.8864570260047913 = 0.20574666559696198 + 0.1 * 6.807103633880615
Epoch 340, val loss: 0.77048259973526
Epoch 350, training loss: 0.8586616516113281 = 0.1801750361919403 + 0.1 * 6.7848663330078125
Epoch 350, val loss: 0.7706891894340515
Epoch 360, training loss: 0.8349456191062927 = 0.15764261782169342 + 0.1 * 6.773029804229736
Epoch 360, val loss: 0.7724828124046326
Epoch 370, training loss: 0.8160004615783691 = 0.1380949765443802 + 0.1 * 6.779055118560791
Epoch 370, val loss: 0.7760424613952637
Epoch 380, training loss: 0.7969868779182434 = 0.12143663316965103 + 0.1 * 6.755502223968506
Epoch 380, val loss: 0.7808765172958374
Epoch 390, training loss: 0.7810986042022705 = 0.1072562113404274 + 0.1 * 6.738423824310303
Epoch 390, val loss: 0.7870486974716187
Epoch 400, training loss: 0.7704365253448486 = 0.09518350660800934 + 0.1 * 6.752530097961426
Epoch 400, val loss: 0.7941458821296692
Epoch 410, training loss: 0.7573826313018799 = 0.08494075387716293 + 0.1 * 6.724418640136719
Epoch 410, val loss: 0.801863968372345
Epoch 420, training loss: 0.7470352649688721 = 0.0761447548866272 + 0.1 * 6.70890474319458
Epoch 420, val loss: 0.8100899457931519
Epoch 430, training loss: 0.7393922805786133 = 0.0685659795999527 + 0.1 * 6.708262920379639
Epoch 430, val loss: 0.8185902833938599
Epoch 440, training loss: 0.7311017513275146 = 0.06200224906206131 + 0.1 * 6.690995216369629
Epoch 440, val loss: 0.8272460699081421
Epoch 450, training loss: 0.7269744873046875 = 0.056275494396686554 + 0.1 * 6.706989765167236
Epoch 450, val loss: 0.8359475135803223
Epoch 460, training loss: 0.7192867994308472 = 0.05129001662135124 + 0.1 * 6.679967403411865
Epoch 460, val loss: 0.8445748686790466
Epoch 470, training loss: 0.713401198387146 = 0.046913351863622665 + 0.1 * 6.664878845214844
Epoch 470, val loss: 0.8532010316848755
Epoch 480, training loss: 0.7118463516235352 = 0.04304657131433487 + 0.1 * 6.687997341156006
Epoch 480, val loss: 0.8617312908172607
Epoch 490, training loss: 0.7053022980690002 = 0.03963253274559975 + 0.1 * 6.6566972732543945
Epoch 490, val loss: 0.8700165748596191
Epoch 500, training loss: 0.700128436088562 = 0.036602430045604706 + 0.1 * 6.635260105133057
Epoch 500, val loss: 0.8781853914260864
Epoch 510, training loss: 0.697649359703064 = 0.03389447554945946 + 0.1 * 6.637548923492432
Epoch 510, val loss: 0.886265754699707
Epoch 520, training loss: 0.6952499151229858 = 0.031478118151426315 + 0.1 * 6.637718200683594
Epoch 520, val loss: 0.8940922617912292
Epoch 530, training loss: 0.6917591691017151 = 0.029314350336790085 + 0.1 * 6.624448299407959
Epoch 530, val loss: 0.9017102718353271
Epoch 540, training loss: 0.688612699508667 = 0.027365485206246376 + 0.1 * 6.612472057342529
Epoch 540, val loss: 0.9092492461204529
Epoch 550, training loss: 0.68700110912323 = 0.02560153231024742 + 0.1 * 6.6139960289001465
Epoch 550, val loss: 0.916620135307312
Epoch 560, training loss: 0.6852205991744995 = 0.02400965243577957 + 0.1 * 6.612109661102295
Epoch 560, val loss: 0.9237834215164185
Epoch 570, training loss: 0.6832495331764221 = 0.022572731599211693 + 0.1 * 6.606767654418945
Epoch 570, val loss: 0.9306655526161194
Epoch 580, training loss: 0.680684506893158 = 0.021263401955366135 + 0.1 * 6.594210624694824
Epoch 580, val loss: 0.9375503063201904
Epoch 590, training loss: 0.6792097687721252 = 0.02006930485367775 + 0.1 * 6.591404438018799
Epoch 590, val loss: 0.9441925287246704
Epoch 600, training loss: 0.6777350902557373 = 0.018978334963321686 + 0.1 * 6.587567329406738
Epoch 600, val loss: 0.950675368309021
Epoch 610, training loss: 0.6759544610977173 = 0.017978832125663757 + 0.1 * 6.579755783081055
Epoch 610, val loss: 0.956995964050293
Epoch 620, training loss: 0.675798237323761 = 0.01705937460064888 + 0.1 * 6.587388515472412
Epoch 620, val loss: 0.9632166028022766
Epoch 630, training loss: 0.6741201877593994 = 0.016212990507483482 + 0.1 * 6.579071998596191
Epoch 630, val loss: 0.9692729711532593
Epoch 640, training loss: 0.6729679703712463 = 0.015431978739798069 + 0.1 * 6.57535982131958
Epoch 640, val loss: 0.9751570820808411
Epoch 650, training loss: 0.6714547872543335 = 0.014713541604578495 + 0.1 * 6.567412376403809
Epoch 650, val loss: 0.9808711409568787
Epoch 660, training loss: 0.6704892516136169 = 0.014046236872673035 + 0.1 * 6.564429759979248
Epoch 660, val loss: 0.9865232110023499
Epoch 670, training loss: 0.6700738072395325 = 0.013426036573946476 + 0.1 * 6.566477298736572
Epoch 670, val loss: 0.9920037388801575
Epoch 680, training loss: 0.6681531667709351 = 0.012848868034780025 + 0.1 * 6.553042888641357
Epoch 680, val loss: 0.9973641633987427
Epoch 690, training loss: 0.6680605411529541 = 0.01231062039732933 + 0.1 * 6.557498931884766
Epoch 690, val loss: 1.0026054382324219
Epoch 700, training loss: 0.6681079864501953 = 0.01180922519415617 + 0.1 * 6.562987327575684
Epoch 700, val loss: 1.0077351331710815
Epoch 710, training loss: 0.6655991077423096 = 0.011340627446770668 + 0.1 * 6.5425848960876465
Epoch 710, val loss: 1.0126980543136597
Epoch 720, training loss: 0.6672747135162354 = 0.010901388712227345 + 0.1 * 6.563733100891113
Epoch 720, val loss: 1.0176489353179932
Epoch 730, training loss: 0.6656278371810913 = 0.010489976033568382 + 0.1 * 6.55137825012207
Epoch 730, val loss: 1.0223733186721802
Epoch 740, training loss: 0.6654133200645447 = 0.010104672983288765 + 0.1 * 6.553086757659912
Epoch 740, val loss: 1.0270363092422485
Epoch 750, training loss: 0.6634063720703125 = 0.009741994552314281 + 0.1 * 6.5366435050964355
Epoch 750, val loss: 1.0315940380096436
Epoch 760, training loss: 0.6633901000022888 = 0.009400770999491215 + 0.1 * 6.53989315032959
Epoch 760, val loss: 1.036060094833374
Epoch 770, training loss: 0.6638498902320862 = 0.009077888913452625 + 0.1 * 6.547719955444336
Epoch 770, val loss: 1.040475606918335
Epoch 780, training loss: 0.6624022722244263 = 0.008773735724389553 + 0.1 * 6.536285400390625
Epoch 780, val loss: 1.0447914600372314
Epoch 790, training loss: 0.6606033444404602 = 0.008486682549118996 + 0.1 * 6.5211663246154785
Epoch 790, val loss: 1.0489599704742432
Epoch 800, training loss: 0.6605467796325684 = 0.00821450725197792 + 0.1 * 6.523322105407715
Epoch 800, val loss: 1.0530861616134644
Epoch 810, training loss: 0.6619033217430115 = 0.007957003079354763 + 0.1 * 6.539463520050049
Epoch 810, val loss: 1.057145595550537
Epoch 820, training loss: 0.660187840461731 = 0.00771231297403574 + 0.1 * 6.524755001068115
Epoch 820, val loss: 1.0610461235046387
Epoch 830, training loss: 0.6613662838935852 = 0.007481265813112259 + 0.1 * 6.538849830627441
Epoch 830, val loss: 1.0648925304412842
Epoch 840, training loss: 0.6586787104606628 = 0.007261406164616346 + 0.1 * 6.5141730308532715
Epoch 840, val loss: 1.0687180757522583
Epoch 850, training loss: 0.6575206518173218 = 0.007052355445921421 + 0.1 * 6.504683017730713
Epoch 850, val loss: 1.0724403858184814
Epoch 860, training loss: 0.6614612936973572 = 0.006852565798908472 + 0.1 * 6.546087265014648
Epoch 860, val loss: 1.0761839151382446
Epoch 870, training loss: 0.6579197645187378 = 0.006662820931524038 + 0.1 * 6.512568950653076
Epoch 870, val loss: 1.0797245502471924
Epoch 880, training loss: 0.657077431678772 = 0.006482461467385292 + 0.1 * 6.5059494972229
Epoch 880, val loss: 1.083247184753418
Epoch 890, training loss: 0.6569334864616394 = 0.006309919524937868 + 0.1 * 6.506235599517822
Epoch 890, val loss: 1.0867829322814941
Epoch 900, training loss: 0.6557420492172241 = 0.006144652143120766 + 0.1 * 6.495974063873291
Epoch 900, val loss: 1.0902429819107056
Epoch 910, training loss: 0.6558913588523865 = 0.005986702628433704 + 0.1 * 6.499046325683594
Epoch 910, val loss: 1.0936273336410522
Epoch 920, training loss: 0.6551897525787354 = 0.005835710093379021 + 0.1 * 6.493540287017822
Epoch 920, val loss: 1.0969399213790894
Epoch 930, training loss: 0.6569582223892212 = 0.005690940655767918 + 0.1 * 6.5126729011535645
Epoch 930, val loss: 1.1002252101898193
Epoch 940, training loss: 0.6549723744392395 = 0.005553110036998987 + 0.1 * 6.494192600250244
Epoch 940, val loss: 1.1034563779830933
Epoch 950, training loss: 0.6549170613288879 = 0.0054211742244660854 + 0.1 * 6.494958877563477
Epoch 950, val loss: 1.1065897941589355
Epoch 960, training loss: 0.6538931727409363 = 0.00529440538957715 + 0.1 * 6.485987663269043
Epoch 960, val loss: 1.1097111701965332
Epoch 970, training loss: 0.656079113483429 = 0.005172503646463156 + 0.1 * 6.509066104888916
Epoch 970, val loss: 1.1127777099609375
Epoch 980, training loss: 0.654288113117218 = 0.005055313464254141 + 0.1 * 6.49232816696167
Epoch 980, val loss: 1.115798830986023
Epoch 990, training loss: 0.6550724506378174 = 0.004942756611853838 + 0.1 * 6.501296520233154
Epoch 990, val loss: 1.1187411546707153
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.8064827919006348 = 1.946797251701355 + 0.1 * 8.596854209899902
Epoch 0, val loss: 1.9478099346160889
Epoch 10, training loss: 2.7961902618408203 = 1.9365112781524658 + 0.1 * 8.596789360046387
Epoch 10, val loss: 1.937861442565918
Epoch 20, training loss: 2.7834675312042236 = 1.9238276481628418 + 0.1 * 8.596399307250977
Epoch 20, val loss: 1.9251209497451782
Epoch 30, training loss: 2.765287160873413 = 1.9059937000274658 + 0.1 * 8.592934608459473
Epoch 30, val loss: 1.9068353176116943
Epoch 40, training loss: 2.7361249923706055 = 1.8794649839401245 + 0.1 * 8.566600799560547
Epoch 40, val loss: 1.879677176475525
Epoch 50, training loss: 2.685884952545166 = 1.8429830074310303 + 0.1 * 8.429019927978516
Epoch 50, val loss: 1.843964695930481
Epoch 60, training loss: 2.622046709060669 = 1.8034011125564575 + 0.1 * 8.186455726623535
Epoch 60, val loss: 1.8083680868148804
Epoch 70, training loss: 2.5546905994415283 = 1.7696974277496338 + 0.1 * 7.8499321937561035
Epoch 70, val loss: 1.7796753644943237
Epoch 80, training loss: 2.4779186248779297 = 1.733122706413269 + 0.1 * 7.4479594230651855
Epoch 80, val loss: 1.746337890625
Epoch 90, training loss: 2.4068942070007324 = 1.685213327407837 + 0.1 * 7.2168097496032715
Epoch 90, val loss: 1.7018300294876099
Epoch 100, training loss: 2.335390567779541 = 1.6210153102874756 + 0.1 * 7.1437530517578125
Epoch 100, val loss: 1.6440330743789673
Epoch 110, training loss: 2.2509658336639404 = 1.541380763053894 + 0.1 * 7.095851421356201
Epoch 110, val loss: 1.5765880346298218
Epoch 120, training loss: 2.158229351043701 = 1.452741026878357 + 0.1 * 7.054884433746338
Epoch 120, val loss: 1.5033729076385498
Epoch 130, training loss: 2.062080144882202 = 1.3609859943389893 + 0.1 * 7.010941028594971
Epoch 130, val loss: 1.4287831783294678
Epoch 140, training loss: 1.9660413265228271 = 1.2686524391174316 + 0.1 * 6.973889350891113
Epoch 140, val loss: 1.3555471897125244
Epoch 150, training loss: 1.871626615524292 = 1.1772129535675049 + 0.1 * 6.944136619567871
Epoch 150, val loss: 1.2827551364898682
Epoch 160, training loss: 1.7794742584228516 = 1.0878875255584717 + 0.1 * 6.915866374969482
Epoch 160, val loss: 1.2119306325912476
Epoch 170, training loss: 1.6929230690002441 = 1.0032389163970947 + 0.1 * 6.896842002868652
Epoch 170, val loss: 1.1452226638793945
Epoch 180, training loss: 1.6127188205718994 = 0.9251269698143005 + 0.1 * 6.875918865203857
Epoch 180, val loss: 1.0843546390533447
Epoch 190, training loss: 1.5395935773849487 = 0.8532697558403015 + 0.1 * 6.863238334655762
Epoch 190, val loss: 1.0286892652511597
Epoch 200, training loss: 1.472459316253662 = 0.7872620224952698 + 0.1 * 6.8519721031188965
Epoch 200, val loss: 0.9778992533683777
Epoch 210, training loss: 1.410771131515503 = 0.7265512943267822 + 0.1 * 6.842197895050049
Epoch 210, val loss: 0.9320679306983948
Epoch 220, training loss: 1.3544087409973145 = 0.6709514856338501 + 0.1 * 6.834572792053223
Epoch 220, val loss: 0.8921945095062256
Epoch 230, training loss: 1.3021098375320435 = 0.6191800832748413 + 0.1 * 6.8292975425720215
Epoch 230, val loss: 0.858195424079895
Epoch 240, training loss: 1.2525999546051025 = 0.5709774494171143 + 0.1 * 6.816224575042725
Epoch 240, val loss: 0.8305400609970093
Epoch 250, training loss: 1.2070621252059937 = 0.5258423686027527 + 0.1 * 6.812197685241699
Epoch 250, val loss: 0.8084195852279663
Epoch 260, training loss: 1.1638731956481934 = 0.483691930770874 + 0.1 * 6.801812648773193
Epoch 260, val loss: 0.7910364866256714
Epoch 270, training loss: 1.1231025457382202 = 0.4441356360912323 + 0.1 * 6.789669513702393
Epoch 270, val loss: 0.7773317098617554
Epoch 280, training loss: 1.0849686861038208 = 0.40711212158203125 + 0.1 * 6.778565406799316
Epoch 280, val loss: 0.7668630480766296
Epoch 290, training loss: 1.0501184463500977 = 0.3728996217250824 + 0.1 * 6.772188663482666
Epoch 290, val loss: 0.7593980431556702
Epoch 300, training loss: 1.0183709859848022 = 0.3417404890060425 + 0.1 * 6.766304969787598
Epoch 300, val loss: 0.7547435760498047
Epoch 310, training loss: 0.988542914390564 = 0.3132925033569336 + 0.1 * 6.752503871917725
Epoch 310, val loss: 0.7528282999992371
Epoch 320, training loss: 0.962824285030365 = 0.2872133255004883 + 0.1 * 6.756109714508057
Epoch 320, val loss: 0.7533109784126282
Epoch 330, training loss: 0.9370470643043518 = 0.2633058428764343 + 0.1 * 6.737411975860596
Epoch 330, val loss: 0.7556003928184509
Epoch 340, training loss: 0.9147534966468811 = 0.24106110632419586 + 0.1 * 6.736923694610596
Epoch 340, val loss: 0.7594500780105591
Epoch 350, training loss: 0.8932899236679077 = 0.22027499973773956 + 0.1 * 6.730149269104004
Epoch 350, val loss: 0.764529824256897
Epoch 360, training loss: 0.8724591135978699 = 0.20082330703735352 + 0.1 * 6.716358184814453
Epoch 360, val loss: 0.770753026008606
Epoch 370, training loss: 0.854764461517334 = 0.1826893836259842 + 0.1 * 6.72075080871582
Epoch 370, val loss: 0.7780330777168274
Epoch 380, training loss: 0.8370442390441895 = 0.16601285338401794 + 0.1 * 6.7103142738342285
Epoch 380, val loss: 0.7862595915794373
Epoch 390, training loss: 0.8215060830116272 = 0.15079350769519806 + 0.1 * 6.707125663757324
Epoch 390, val loss: 0.7953803539276123
Epoch 400, training loss: 0.8063645362854004 = 0.13703519105911255 + 0.1 * 6.693293571472168
Epoch 400, val loss: 0.8053166270256042
Epoch 410, training loss: 0.7930228114128113 = 0.1246332973241806 + 0.1 * 6.683895111083984
Epoch 410, val loss: 0.8159810900688171
Epoch 420, training loss: 0.7823426723480225 = 0.11347189545631409 + 0.1 * 6.68870735168457
Epoch 420, val loss: 0.8273301124572754
Epoch 430, training loss: 0.7719098329544067 = 0.10346262902021408 + 0.1 * 6.68447208404541
Epoch 430, val loss: 0.8391996026039124
Epoch 440, training loss: 0.7608572244644165 = 0.09451327472925186 + 0.1 * 6.6634392738342285
Epoch 440, val loss: 0.8513593673706055
Epoch 450, training loss: 0.752543032169342 = 0.0864894911646843 + 0.1 * 6.6605353355407715
Epoch 450, val loss: 0.8637946248054504
Epoch 460, training loss: 0.7455629706382751 = 0.07930346578359604 + 0.1 * 6.662595272064209
Epoch 460, val loss: 0.876482367515564
Epoch 470, training loss: 0.7376000285148621 = 0.07287100702524185 + 0.1 * 6.647289752960205
Epoch 470, val loss: 0.8890237808227539
Epoch 480, training loss: 0.7312631011009216 = 0.06708317250013351 + 0.1 * 6.641798973083496
Epoch 480, val loss: 0.9018468260765076
Epoch 490, training loss: 0.7265564799308777 = 0.06186778098344803 + 0.1 * 6.646886825561523
Epoch 490, val loss: 0.9146788716316223
Epoch 500, training loss: 0.7203280925750732 = 0.057170361280441284 + 0.1 * 6.631577491760254
Epoch 500, val loss: 0.9273108839988708
Epoch 510, training loss: 0.7167799472808838 = 0.05292418599128723 + 0.1 * 6.6385579109191895
Epoch 510, val loss: 0.9399728178977966
Epoch 520, training loss: 0.7119200229644775 = 0.04909088835120201 + 0.1 * 6.628291606903076
Epoch 520, val loss: 0.9525582790374756
Epoch 530, training loss: 0.7081533074378967 = 0.04562707990407944 + 0.1 * 6.625262260437012
Epoch 530, val loss: 0.9647185206413269
Epoch 540, training loss: 0.7046727538108826 = 0.04247957840561867 + 0.1 * 6.621931552886963
Epoch 540, val loss: 0.9769784212112427
Epoch 550, training loss: 0.7006707191467285 = 0.03962244093418121 + 0.1 * 6.610482692718506
Epoch 550, val loss: 0.988995373249054
Epoch 560, training loss: 0.6986867785453796 = 0.037023503333330154 + 0.1 * 6.61663293838501
Epoch 560, val loss: 1.000730037689209
Epoch 570, training loss: 0.6952610611915588 = 0.03465719893574715 + 0.1 * 6.606038570404053
Epoch 570, val loss: 1.0122928619384766
Epoch 580, training loss: 0.6939522624015808 = 0.032496314495801926 + 0.1 * 6.614559173583984
Epoch 580, val loss: 1.0236910581588745
Epoch 590, training loss: 0.6908153295516968 = 0.030523056164383888 + 0.1 * 6.602922439575195
Epoch 590, val loss: 1.0348403453826904
Epoch 600, training loss: 0.6887159943580627 = 0.028719959780573845 + 0.1 * 6.5999603271484375
Epoch 600, val loss: 1.0457870960235596
Epoch 610, training loss: 0.6864139437675476 = 0.02707013674080372 + 0.1 * 6.593437671661377
Epoch 610, val loss: 1.0563468933105469
Epoch 620, training loss: 0.6853554248809814 = 0.025553306564688683 + 0.1 * 6.598021030426025
Epoch 620, val loss: 1.0668350458145142
Epoch 630, training loss: 0.6830053925514221 = 0.024157658219337463 + 0.1 * 6.588477611541748
Epoch 630, val loss: 1.0771375894546509
Epoch 640, training loss: 0.6807668209075928 = 0.022872384637594223 + 0.1 * 6.578944206237793
Epoch 640, val loss: 1.0872174501419067
Epoch 650, training loss: 0.6804009675979614 = 0.02168685756623745 + 0.1 * 6.587141036987305
Epoch 650, val loss: 1.0970265865325928
Epoch 660, training loss: 0.6784570813179016 = 0.02059098891913891 + 0.1 * 6.57866096496582
Epoch 660, val loss: 1.1067646741867065
Epoch 670, training loss: 0.6764788627624512 = 0.01957816258072853 + 0.1 * 6.56900691986084
Epoch 670, val loss: 1.1160036325454712
Epoch 680, training loss: 0.6759047508239746 = 0.01863880641758442 + 0.1 * 6.572659492492676
Epoch 680, val loss: 1.1251404285430908
Epoch 690, training loss: 0.6751416325569153 = 0.017765481024980545 + 0.1 * 6.573761463165283
Epoch 690, val loss: 1.1342977285385132
Epoch 700, training loss: 0.6764898896217346 = 0.016955040395259857 + 0.1 * 6.595348834991455
Epoch 700, val loss: 1.1430730819702148
Epoch 710, training loss: 0.6719790101051331 = 0.016203073784708977 + 0.1 * 6.557759761810303
Epoch 710, val loss: 1.1514911651611328
Epoch 720, training loss: 0.6705795526504517 = 0.015501676127314568 + 0.1 * 6.550778388977051
Epoch 720, val loss: 1.1596415042877197
Epoch 730, training loss: 0.6699814200401306 = 0.014845414087176323 + 0.1 * 6.5513596534729
Epoch 730, val loss: 1.16782546043396
Epoch 740, training loss: 0.6702168583869934 = 0.014231536537408829 + 0.1 * 6.5598530769348145
Epoch 740, val loss: 1.1758521795272827
Epoch 750, training loss: 0.6693997979164124 = 0.013657258823513985 + 0.1 * 6.557425022125244
Epoch 750, val loss: 1.183737874031067
Epoch 760, training loss: 0.6679987907409668 = 0.013119727373123169 + 0.1 * 6.548790454864502
Epoch 760, val loss: 1.19119393825531
Epoch 770, training loss: 0.6669718623161316 = 0.012614442966878414 + 0.1 * 6.543574333190918
Epoch 770, val loss: 1.1986010074615479
Epoch 780, training loss: 0.6670404076576233 = 0.012140167877078056 + 0.1 * 6.549002647399902
Epoch 780, val loss: 1.205888271331787
Epoch 790, training loss: 0.6646820306777954 = 0.011693629436194897 + 0.1 * 6.529883861541748
Epoch 790, val loss: 1.2129929065704346
Epoch 800, training loss: 0.6660333871841431 = 0.011272775940597057 + 0.1 * 6.547605991363525
Epoch 800, val loss: 1.2199286222457886
Epoch 810, training loss: 0.6642397046089172 = 0.010876200161874294 + 0.1 * 6.533635139465332
Epoch 810, val loss: 1.2267197370529175
Epoch 820, training loss: 0.6632261276245117 = 0.01050165481865406 + 0.1 * 6.5272440910339355
Epoch 820, val loss: 1.2333738803863525
Epoch 830, training loss: 0.6617900133132935 = 0.010147974826395512 + 0.1 * 6.516419887542725
Epoch 830, val loss: 1.2398148775100708
Epoch 840, training loss: 0.6631656289100647 = 0.009813175536692142 + 0.1 * 6.533524513244629
Epoch 840, val loss: 1.2460901737213135
Epoch 850, training loss: 0.661164402961731 = 0.009495047852396965 + 0.1 * 6.516693592071533
Epoch 850, val loss: 1.2524747848510742
Epoch 860, training loss: 0.6618332862854004 = 0.00919443741440773 + 0.1 * 6.526388168334961
Epoch 860, val loss: 1.2585830688476562
Epoch 870, training loss: 0.6616930961608887 = 0.008909502997994423 + 0.1 * 6.527835369110107
Epoch 870, val loss: 1.264548897743225
Epoch 880, training loss: 0.6601337790489197 = 0.008639189414680004 + 0.1 * 6.5149455070495605
Epoch 880, val loss: 1.270229697227478
Epoch 890, training loss: 0.658605694770813 = 0.008382064290344715 + 0.1 * 6.5022358894348145
Epoch 890, val loss: 1.2758301496505737
Epoch 900, training loss: 0.6591257452964783 = 0.008136517368257046 + 0.1 * 6.509892463684082
Epoch 900, val loss: 1.2814918756484985
Epoch 910, training loss: 0.6589264273643494 = 0.007902567274868488 + 0.1 * 6.5102386474609375
Epoch 910, val loss: 1.2871363162994385
Epoch 920, training loss: 0.6575189828872681 = 0.00768054137006402 + 0.1 * 6.498384475708008
Epoch 920, val loss: 1.292504906654358
Epoch 930, training loss: 0.6579504013061523 = 0.007468490861356258 + 0.1 * 6.504819393157959
Epoch 930, val loss: 1.2975189685821533
Epoch 940, training loss: 0.6567853689193726 = 0.00726587139070034 + 0.1 * 6.495194911956787
Epoch 940, val loss: 1.3028522729873657
Epoch 950, training loss: 0.6569231152534485 = 0.0070725418627262115 + 0.1 * 6.498505592346191
Epoch 950, val loss: 1.3078452348709106
Epoch 960, training loss: 0.6561582088470459 = 0.006887277588248253 + 0.1 * 6.492709636688232
Epoch 960, val loss: 1.312846302986145
Epoch 970, training loss: 0.6564912796020508 = 0.0067101153545081615 + 0.1 * 6.497811794281006
Epoch 970, val loss: 1.3177834749221802
Epoch 980, training loss: 0.6562049388885498 = 0.006540564354509115 + 0.1 * 6.496644020080566
Epoch 980, val loss: 1.322642207145691
Epoch 990, training loss: 0.6553837656974792 = 0.006378002464771271 + 0.1 * 6.490057468414307
Epoch 990, val loss: 1.3273286819458008
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.8292784690856934 = 1.969594120979309 + 0.1 * 8.596843719482422
Epoch 0, val loss: 1.9704054594039917
Epoch 10, training loss: 2.8177695274353027 = 1.958092212677002 + 0.1 * 8.596773147583008
Epoch 10, val loss: 1.9591288566589355
Epoch 20, training loss: 2.803145408630371 = 1.9435145854949951 + 0.1 * 8.596307754516602
Epoch 20, val loss: 1.9443341493606567
Epoch 30, training loss: 2.781891345977783 = 1.9226843118667603 + 0.1 * 8.592071533203125
Epoch 30, val loss: 1.9228205680847168
Epoch 40, training loss: 2.7481582164764404 = 1.8917168378829956 + 0.1 * 8.564414024353027
Epoch 40, val loss: 1.8909645080566406
Epoch 50, training loss: 2.6944010257720947 = 1.8496661186218262 + 0.1 * 8.447349548339844
Epoch 50, val loss: 1.8497896194458008
Epoch 60, training loss: 2.629171133041382 = 1.8053888082504272 + 0.1 * 8.237822532653809
Epoch 60, val loss: 1.8099114894866943
Epoch 70, training loss: 2.572409152984619 = 1.766730546951294 + 0.1 * 8.056784629821777
Epoch 70, val loss: 1.7757595777511597
Epoch 80, training loss: 2.4912123680114746 = 1.7242720127105713 + 0.1 * 7.669404029846191
Epoch 80, val loss: 1.7355828285217285
Epoch 90, training loss: 2.4126529693603516 = 1.6723157167434692 + 0.1 * 7.403372764587402
Epoch 90, val loss: 1.6865575313568115
Epoch 100, training loss: 2.3326776027679443 = 1.6061432361602783 + 0.1 * 7.26534366607666
Epoch 100, val loss: 1.6248747110366821
Epoch 110, training loss: 2.24463152885437 = 1.527941107749939 + 0.1 * 7.166904926300049
Epoch 110, val loss: 1.5532528162002563
Epoch 120, training loss: 2.152812957763672 = 1.444500207901001 + 0.1 * 7.083126068115234
Epoch 120, val loss: 1.4807076454162598
Epoch 130, training loss: 2.062516689300537 = 1.3609566688537598 + 0.1 * 7.015599727630615
Epoch 130, val loss: 1.4118754863739014
Epoch 140, training loss: 1.9761404991149902 = 1.2792006731033325 + 0.1 * 6.9693989753723145
Epoch 140, val loss: 1.3483188152313232
Epoch 150, training loss: 1.8938724994659424 = 1.2002214193344116 + 0.1 * 6.936511039733887
Epoch 150, val loss: 1.2889727354049683
Epoch 160, training loss: 1.813474416732788 = 1.1221014261245728 + 0.1 * 6.913730621337891
Epoch 160, val loss: 1.2305395603179932
Epoch 170, training loss: 1.7336727380752563 = 1.0439468622207642 + 0.1 * 6.897258758544922
Epoch 170, val loss: 1.172369360923767
Epoch 180, training loss: 1.6546978950500488 = 0.9658671617507935 + 0.1 * 6.888306617736816
Epoch 180, val loss: 1.1140804290771484
Epoch 190, training loss: 1.5765889883041382 = 0.8887990713119507 + 0.1 * 6.877899169921875
Epoch 190, val loss: 1.0555146932601929
Epoch 200, training loss: 1.4992692470550537 = 0.8125916719436646 + 0.1 * 6.8667755126953125
Epoch 200, val loss: 0.9980724453926086
Epoch 210, training loss: 1.4251420497894287 = 0.7390410900115967 + 0.1 * 6.861009120941162
Epoch 210, val loss: 0.9440045356750488
Epoch 220, training loss: 1.3559881448745728 = 0.6710067391395569 + 0.1 * 6.849813938140869
Epoch 220, val loss: 0.8969007134437561
Epoch 230, training loss: 1.2932255268096924 = 0.6089739799499512 + 0.1 * 6.842514991760254
Epoch 230, val loss: 0.8573502898216248
Epoch 240, training loss: 1.2363871335983276 = 0.5532280206680298 + 0.1 * 6.8315911293029785
Epoch 240, val loss: 0.8254806995391846
Epoch 250, training loss: 1.1860156059265137 = 0.5033885836601257 + 0.1 * 6.826270580291748
Epoch 250, val loss: 0.8006728291511536
Epoch 260, training loss: 1.1433954238891602 = 0.45887240767478943 + 0.1 * 6.845229625701904
Epoch 260, val loss: 0.781441867351532
Epoch 270, training loss: 1.0996536016464233 = 0.41878312826156616 + 0.1 * 6.808704853057861
Epoch 270, val loss: 0.7666464447975159
Epoch 280, training loss: 1.060624361038208 = 0.3813069462776184 + 0.1 * 6.793173313140869
Epoch 280, val loss: 0.7546305060386658
Epoch 290, training loss: 1.0236928462982178 = 0.34567731618881226 + 0.1 * 6.780155181884766
Epoch 290, val loss: 0.7446545362472534
Epoch 300, training loss: 0.9915908575057983 = 0.31179916858673096 + 0.1 * 6.797916889190674
Epoch 300, val loss: 0.736446738243103
Epoch 310, training loss: 0.9560562372207642 = 0.2802116274833679 + 0.1 * 6.758446216583252
Epoch 310, val loss: 0.7297826409339905
Epoch 320, training loss: 0.9283262491226196 = 0.25108951330184937 + 0.1 * 6.772367000579834
Epoch 320, val loss: 0.7245400547981262
Epoch 330, training loss: 0.8990750908851624 = 0.2249278873205185 + 0.1 * 6.741471767425537
Epoch 330, val loss: 0.7208757996559143
Epoch 340, training loss: 0.8743529319763184 = 0.2015414983034134 + 0.1 * 6.728114604949951
Epoch 340, val loss: 0.7188148498535156
Epoch 350, training loss: 0.8531573414802551 = 0.18081964552402496 + 0.1 * 6.723377227783203
Epoch 350, val loss: 0.7184510231018066
Epoch 360, training loss: 0.8344246745109558 = 0.16265588998794556 + 0.1 * 6.717687606811523
Epoch 360, val loss: 0.7195771336555481
Epoch 370, training loss: 0.8173223733901978 = 0.1468137949705124 + 0.1 * 6.705085754394531
Epoch 370, val loss: 0.7220244407653809
Epoch 380, training loss: 0.8035088777542114 = 0.13294683396816254 + 0.1 * 6.705620288848877
Epoch 380, val loss: 0.7257062792778015
Epoch 390, training loss: 0.7895025610923767 = 0.12077325582504272 + 0.1 * 6.68729305267334
Epoch 390, val loss: 0.7304583787918091
Epoch 400, training loss: 0.7787970304489136 = 0.1100887581706047 + 0.1 * 6.687082767486572
Epoch 400, val loss: 0.7361527681350708
Epoch 410, training loss: 0.767853856086731 = 0.10067799687385559 + 0.1 * 6.67175817489624
Epoch 410, val loss: 0.7424715161323547
Epoch 420, training loss: 0.7591134905815125 = 0.09234239906072617 + 0.1 * 6.66771125793457
Epoch 420, val loss: 0.7494330406188965
Epoch 430, training loss: 0.751090943813324 = 0.08493119478225708 + 0.1 * 6.66159725189209
Epoch 430, val loss: 0.7568637132644653
Epoch 440, training loss: 0.7444521188735962 = 0.07829618453979492 + 0.1 * 6.661559104919434
Epoch 440, val loss: 0.7648124098777771
Epoch 450, training loss: 0.7370707988739014 = 0.07237014174461365 + 0.1 * 6.647006511688232
Epoch 450, val loss: 0.773047149181366
Epoch 460, training loss: 0.7313516736030579 = 0.06704144179821014 + 0.1 * 6.643102169036865
Epoch 460, val loss: 0.7814980745315552
Epoch 470, training loss: 0.7251825332641602 = 0.062233760952949524 + 0.1 * 6.629487991333008
Epoch 470, val loss: 0.7901840209960938
Epoch 480, training loss: 0.7206753492355347 = 0.05788346752524376 + 0.1 * 6.627918720245361
Epoch 480, val loss: 0.7989435791969299
Epoch 490, training loss: 0.7180783152580261 = 0.05392933636903763 + 0.1 * 6.641489505767822
Epoch 490, val loss: 0.8078013062477112
Epoch 500, training loss: 0.7124592661857605 = 0.05033678933978081 + 0.1 * 6.621224880218506
Epoch 500, val loss: 0.8167026042938232
Epoch 510, training loss: 0.7086122035980225 = 0.04706321284174919 + 0.1 * 6.615489482879639
Epoch 510, val loss: 0.825584352016449
Epoch 520, training loss: 0.7056020498275757 = 0.04406709223985672 + 0.1 * 6.615349769592285
Epoch 520, val loss: 0.8344579339027405
Epoch 530, training loss: 0.7015026211738586 = 0.04132469743490219 + 0.1 * 6.601779460906982
Epoch 530, val loss: 0.8433157801628113
Epoch 540, training loss: 0.7013368606567383 = 0.03881010413169861 + 0.1 * 6.62526798248291
Epoch 540, val loss: 0.85208660364151
Epoch 550, training loss: 0.6957939267158508 = 0.036503709852695465 + 0.1 * 6.592901706695557
Epoch 550, val loss: 0.8607088923454285
Epoch 560, training loss: 0.6931544542312622 = 0.034376323223114014 + 0.1 * 6.587780952453613
Epoch 560, val loss: 0.8693342208862305
Epoch 570, training loss: 0.6920566558837891 = 0.032413288950920105 + 0.1 * 6.596433639526367
Epoch 570, val loss: 0.8778801560401917
Epoch 580, training loss: 0.6885359287261963 = 0.03060457669198513 + 0.1 * 6.579313278198242
Epoch 580, val loss: 0.8863436579704285
Epoch 590, training loss: 0.6869334578514099 = 0.02893582545220852 + 0.1 * 6.5799760818481445
Epoch 590, val loss: 0.8946507573127747
Epoch 600, training loss: 0.6848114728927612 = 0.027390887960791588 + 0.1 * 6.57420539855957
Epoch 600, val loss: 0.9028573632240295
Epoch 610, training loss: 0.6823951005935669 = 0.025959437713027 + 0.1 * 6.564356327056885
Epoch 610, val loss: 0.9110005497932434
Epoch 620, training loss: 0.6819889545440674 = 0.02463231235742569 + 0.1 * 6.57356595993042
Epoch 620, val loss: 0.9189938306808472
Epoch 630, training loss: 0.6790215969085693 = 0.023402376100420952 + 0.1 * 6.556191921234131
Epoch 630, val loss: 0.9268592000007629
Epoch 640, training loss: 0.6781255602836609 = 0.022259240970015526 + 0.1 * 6.558663368225098
Epoch 640, val loss: 0.9345640540122986
Epoch 650, training loss: 0.6763697266578674 = 0.021193649619817734 + 0.1 * 6.551760673522949
Epoch 650, val loss: 0.9421871304512024
Epoch 660, training loss: 0.6749255657196045 = 0.0202014222741127 + 0.1 * 6.5472412109375
Epoch 660, val loss: 0.9496810436248779
Epoch 670, training loss: 0.6739367246627808 = 0.019278036430478096 + 0.1 * 6.546586513519287
Epoch 670, val loss: 0.9569767713546753
Epoch 680, training loss: 0.6745811104774475 = 0.018415628001093864 + 0.1 * 6.561654567718506
Epoch 680, val loss: 0.964164674282074
Epoch 690, training loss: 0.6712883114814758 = 0.01761050894856453 + 0.1 * 6.536777973175049
Epoch 690, val loss: 0.9711956977844238
Epoch 700, training loss: 0.6702967286109924 = 0.016856230795383453 + 0.1 * 6.534404754638672
Epoch 700, val loss: 0.9781287312507629
Epoch 710, training loss: 0.6723088622093201 = 0.016149038448929787 + 0.1 * 6.56159782409668
Epoch 710, val loss: 0.9849622845649719
Epoch 720, training loss: 0.6684077978134155 = 0.015489023178815842 + 0.1 * 6.529187202453613
Epoch 720, val loss: 0.9915825724601746
Epoch 730, training loss: 0.6682437062263489 = 0.014867697842419147 + 0.1 * 6.533759593963623
Epoch 730, val loss: 0.9980922937393188
Epoch 740, training loss: 0.6678253412246704 = 0.014283685944974422 + 0.1 * 6.535416603088379
Epoch 740, val loss: 1.0045011043548584
Epoch 750, training loss: 0.6659956574440002 = 0.01373398769646883 + 0.1 * 6.522616386413574
Epoch 750, val loss: 1.0107759237289429
Epoch 760, training loss: 0.6667739152908325 = 0.013217310421168804 + 0.1 * 6.5355658531188965
Epoch 760, val loss: 1.0169475078582764
Epoch 770, training loss: 0.6639304757118225 = 0.012731662020087242 + 0.1 * 6.511988162994385
Epoch 770, val loss: 1.0229425430297852
Epoch 780, training loss: 0.663429319858551 = 0.0122732138261199 + 0.1 * 6.511561393737793
Epoch 780, val loss: 1.0288422107696533
Epoch 790, training loss: 0.6637700200080872 = 0.011838997714221478 + 0.1 * 6.519310474395752
Epoch 790, val loss: 1.0346908569335938
Epoch 800, training loss: 0.6621185541152954 = 0.011429021134972572 + 0.1 * 6.506895542144775
Epoch 800, val loss: 1.0404237508773804
Epoch 810, training loss: 0.662480890750885 = 0.011041643097996712 + 0.1 * 6.514392375946045
Epoch 810, val loss: 1.0460418462753296
Epoch 820, training loss: 0.6607082486152649 = 0.010674375109374523 + 0.1 * 6.500339031219482
Epoch 820, val loss: 1.051547884941101
Epoch 830, training loss: 0.6605353951454163 = 0.010326847434043884 + 0.1 * 6.502085208892822
Epoch 830, val loss: 1.0569343566894531
Epoch 840, training loss: 0.6614797115325928 = 0.009996929205954075 + 0.1 * 6.514827251434326
Epoch 840, val loss: 1.0622448921203613
Epoch 850, training loss: 0.6590718626976013 = 0.00968307163566351 + 0.1 * 6.493887901306152
Epoch 850, val loss: 1.0674980878829956
Epoch 860, training loss: 0.658584475517273 = 0.009385766461491585 + 0.1 * 6.4919867515563965
Epoch 860, val loss: 1.0726242065429688
Epoch 870, training loss: 0.6586033701896667 = 0.009102286770939827 + 0.1 * 6.495010852813721
Epoch 870, val loss: 1.0777071714401245
Epoch 880, training loss: 0.660933792591095 = 0.008832857944071293 + 0.1 * 6.5210089683532715
Epoch 880, val loss: 1.0827223062515259
Epoch 890, training loss: 0.6573420166969299 = 0.00857665203511715 + 0.1 * 6.487653732299805
Epoch 890, val loss: 1.0876020193099976
Epoch 900, training loss: 0.6565933227539062 = 0.008332803845405579 + 0.1 * 6.48260498046875
Epoch 900, val loss: 1.0923478603363037
Epoch 910, training loss: 0.6581538915634155 = 0.008099235594272614 + 0.1 * 6.500545978546143
Epoch 910, val loss: 1.0970734357833862
Epoch 920, training loss: 0.6555759310722351 = 0.007877410389482975 + 0.1 * 6.476984977722168
Epoch 920, val loss: 1.1016861200332642
Epoch 930, training loss: 0.6556394696235657 = 0.007665263954550028 + 0.1 * 6.47974157333374
Epoch 930, val loss: 1.1062175035476685
Epoch 940, training loss: 0.654986560344696 = 0.007461864966899157 + 0.1 * 6.475246906280518
Epoch 940, val loss: 1.1106717586517334
Epoch 950, training loss: 0.6554867625236511 = 0.007267247419804335 + 0.1 * 6.4821953773498535
Epoch 950, val loss: 1.1151059865951538
Epoch 960, training loss: 0.6547664999961853 = 0.00708141690120101 + 0.1 * 6.476850986480713
Epoch 960, val loss: 1.1194337606430054
Epoch 970, training loss: 0.6537957787513733 = 0.006903789471834898 + 0.1 * 6.4689202308654785
Epoch 970, val loss: 1.1236830949783325
Epoch 980, training loss: 0.655066728591919 = 0.00673275999724865 + 0.1 * 6.483339786529541
Epoch 980, val loss: 1.1278836727142334
Epoch 990, training loss: 0.6539684534072876 = 0.006569112651050091 + 0.1 * 6.473993301391602
Epoch 990, val loss: 1.132014513015747
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8328940432261466
The final CL Acc:0.80000, 0.01090, The final GNN Acc:0.83588, 0.00237
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9522])
updated graph: torch.Size([2, 10584])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8038763999938965 = 1.9441931247711182 + 0.1 * 8.596832275390625
Epoch 0, val loss: 1.9360747337341309
Epoch 10, training loss: 2.7942471504211426 = 1.934571623802185 + 0.1 * 8.59675407409668
Epoch 10, val loss: 1.9269979000091553
Epoch 20, training loss: 2.7822823524475098 = 1.9226552248001099 + 0.1 * 8.596272468566895
Epoch 20, val loss: 1.9154289960861206
Epoch 30, training loss: 2.764939785003662 = 1.9057118892669678 + 0.1 * 8.592278480529785
Epoch 30, val loss: 1.8989897966384888
Epoch 40, training loss: 2.737168788909912 = 1.8804174661636353 + 0.1 * 8.567512512207031
Epoch 40, val loss: 1.8750901222229004
Epoch 50, training loss: 2.6911981105804443 = 1.8450931310653687 + 0.1 * 8.46104907989502
Epoch 50, val loss: 1.843743920326233
Epoch 60, training loss: 2.6307103633880615 = 1.8051507472991943 + 0.1 * 8.255595207214355
Epoch 60, val loss: 1.8113369941711426
Epoch 70, training loss: 2.5667123794555664 = 1.7691712379455566 + 0.1 * 7.975411415100098
Epoch 70, val loss: 1.7824422121047974
Epoch 80, training loss: 2.484612464904785 = 1.7314345836639404 + 0.1 * 7.531777858734131
Epoch 80, val loss: 1.7486284971237183
Epoch 90, training loss: 2.4178287982940674 = 1.6830142736434937 + 0.1 * 7.348145961761475
Epoch 90, val loss: 1.7049193382263184
Epoch 100, training loss: 2.3445513248443604 = 1.6187187433242798 + 0.1 * 7.258326053619385
Epoch 100, val loss: 1.6494418382644653
Epoch 110, training loss: 2.257479667663574 = 1.5401917695999146 + 0.1 * 7.172878265380859
Epoch 110, val loss: 1.5838347673416138
Epoch 120, training loss: 2.1662607192993164 = 1.4541503190994263 + 0.1 * 7.121103763580322
Epoch 120, val loss: 1.5136827230453491
Epoch 130, training loss: 2.075587511062622 = 1.3670759201049805 + 0.1 * 7.085116386413574
Epoch 130, val loss: 1.4447674751281738
Epoch 140, training loss: 1.9853665828704834 = 1.2806158065795898 + 0.1 * 7.047508239746094
Epoch 140, val loss: 1.3773503303527832
Epoch 150, training loss: 1.8946499824523926 = 1.1933213472366333 + 0.1 * 7.0132856369018555
Epoch 150, val loss: 1.311004638671875
Epoch 160, training loss: 1.805185317993164 = 1.1052272319793701 + 0.1 * 6.999579906463623
Epoch 160, val loss: 1.2460927963256836
Epoch 170, training loss: 1.717451810836792 = 1.019528865814209 + 0.1 * 6.97922945022583
Epoch 170, val loss: 1.1848036050796509
Epoch 180, training loss: 1.633115530014038 = 0.9363570809364319 + 0.1 * 6.967584133148193
Epoch 180, val loss: 1.1254223585128784
Epoch 190, training loss: 1.5521900653839111 = 0.8566881418228149 + 0.1 * 6.955019474029541
Epoch 190, val loss: 1.068871259689331
Epoch 200, training loss: 1.4766203165054321 = 0.7821571230888367 + 0.1 * 6.944631576538086
Epoch 200, val loss: 1.0166139602661133
Epoch 210, training loss: 1.4069390296936035 = 0.7140731811523438 + 0.1 * 6.928658962249756
Epoch 210, val loss: 0.9705030918121338
Epoch 220, training loss: 1.343849539756775 = 0.652175784111023 + 0.1 * 6.9167375564575195
Epoch 220, val loss: 0.9313992261886597
Epoch 230, training loss: 1.287123680114746 = 0.5962581038475037 + 0.1 * 6.908655166625977
Epoch 230, val loss: 0.899861752986908
Epoch 240, training loss: 1.2355990409851074 = 0.5457420945167542 + 0.1 * 6.8985700607299805
Epoch 240, val loss: 0.8755400776863098
Epoch 250, training loss: 1.1882567405700684 = 0.4992721974849701 + 0.1 * 6.889845371246338
Epoch 250, val loss: 0.8571462631225586
Epoch 260, training loss: 1.144949197769165 = 0.4562264680862427 + 0.1 * 6.887226581573486
Epoch 260, val loss: 0.8436765670776367
Epoch 270, training loss: 1.104191541671753 = 0.41623610258102417 + 0.1 * 6.879554271697998
Epoch 270, val loss: 0.8339987397193909
Epoch 280, training loss: 1.0657716989517212 = 0.3787324130535126 + 0.1 * 6.8703932762146
Epoch 280, val loss: 0.827248215675354
Epoch 290, training loss: 1.0316107273101807 = 0.3436461091041565 + 0.1 * 6.879645347595215
Epoch 290, val loss: 0.8230765461921692
Epoch 300, training loss: 0.997917652130127 = 0.311137318611145 + 0.1 * 6.86780309677124
Epoch 300, val loss: 0.8211413025856018
Epoch 310, training loss: 0.9663991928100586 = 0.28083130717277527 + 0.1 * 6.855678558349609
Epoch 310, val loss: 0.8211098909378052
Epoch 320, training loss: 0.9377056360244751 = 0.2526561915874481 + 0.1 * 6.850494861602783
Epoch 320, val loss: 0.8227830529212952
Epoch 330, training loss: 0.9129987359046936 = 0.2266177088022232 + 0.1 * 6.8638105392456055
Epoch 330, val loss: 0.8261274099349976
Epoch 340, training loss: 0.8874740600585938 = 0.20298165082931519 + 0.1 * 6.844923973083496
Epoch 340, val loss: 0.8310481309890747
Epoch 350, training loss: 0.8649907112121582 = 0.1815090775489807 + 0.1 * 6.8348164558410645
Epoch 350, val loss: 0.8374255895614624
Epoch 360, training loss: 0.8459967970848083 = 0.16203095018863678 + 0.1 * 6.839658737182617
Epoch 360, val loss: 0.8453872203826904
Epoch 370, training loss: 0.8287454843521118 = 0.14468833804130554 + 0.1 * 6.840571403503418
Epoch 370, val loss: 0.854637086391449
Epoch 380, training loss: 0.8116137981414795 = 0.12926819920539856 + 0.1 * 6.823456287384033
Epoch 380, val loss: 0.8650850057601929
Epoch 390, training loss: 0.7965667247772217 = 0.11555203795433044 + 0.1 * 6.810146331787109
Epoch 390, val loss: 0.8767889142036438
Epoch 400, training loss: 0.7835497260093689 = 0.1033904179930687 + 0.1 * 6.80159330368042
Epoch 400, val loss: 0.889634907245636
Epoch 410, training loss: 0.7756611108779907 = 0.09266439825296402 + 0.1 * 6.829967021942139
Epoch 410, val loss: 0.9035084247589111
Epoch 420, training loss: 0.7630045413970947 = 0.0833306610584259 + 0.1 * 6.796739101409912
Epoch 420, val loss: 0.9176463484764099
Epoch 430, training loss: 0.7539283633232117 = 0.07516644895076752 + 0.1 * 6.787618637084961
Epoch 430, val loss: 0.9323228001594543
Epoch 440, training loss: 0.746549129486084 = 0.06801919639110565 + 0.1 * 6.785298824310303
Epoch 440, val loss: 0.9470785856246948
Epoch 450, training loss: 0.7393515110015869 = 0.061762887984514236 + 0.1 * 6.775886535644531
Epoch 450, val loss: 0.9617211222648621
Epoch 460, training loss: 0.7323620319366455 = 0.0562761127948761 + 0.1 * 6.76085901260376
Epoch 460, val loss: 0.9764230251312256
Epoch 470, training loss: 0.7272387146949768 = 0.05144767090678215 + 0.1 * 6.757910251617432
Epoch 470, val loss: 0.9908713102340698
Epoch 480, training loss: 0.7224879264831543 = 0.04719075933098793 + 0.1 * 6.752971172332764
Epoch 480, val loss: 1.004988193511963
Epoch 490, training loss: 0.7179428935050964 = 0.043432947248220444 + 0.1 * 6.7450995445251465
Epoch 490, val loss: 1.0189799070358276
Epoch 500, training loss: 0.7145952582359314 = 0.040098171681165695 + 0.1 * 6.744970798492432
Epoch 500, val loss: 1.0324369668960571
Epoch 510, training loss: 0.7081195712089539 = 0.03713971748948097 + 0.1 * 6.709798336029053
Epoch 510, val loss: 1.0456970930099487
Epoch 520, training loss: 0.7053735256195068 = 0.03450079634785652 + 0.1 * 6.7087273597717285
Epoch 520, val loss: 1.058410882949829
Epoch 530, training loss: 0.7032930254936218 = 0.03213432803750038 + 0.1 * 6.7115864753723145
Epoch 530, val loss: 1.0708372592926025
Epoch 540, training loss: 0.6996186971664429 = 0.030007261782884598 + 0.1 * 6.696114540100098
Epoch 540, val loss: 1.0829567909240723
Epoch 550, training loss: 0.6982265114784241 = 0.028090812265872955 + 0.1 * 6.701356410980225
Epoch 550, val loss: 1.0947067737579346
Epoch 560, training loss: 0.6939597129821777 = 0.026362258940935135 + 0.1 * 6.675974369049072
Epoch 560, val loss: 1.1060711145401
Epoch 570, training loss: 0.6935862302780151 = 0.024793466553092003 + 0.1 * 6.687927722930908
Epoch 570, val loss: 1.1171759366989136
Epoch 580, training loss: 0.6897135376930237 = 0.023369740694761276 + 0.1 * 6.663437843322754
Epoch 580, val loss: 1.1277955770492554
Epoch 590, training loss: 0.6901538372039795 = 0.0220688134431839 + 0.1 * 6.680850028991699
Epoch 590, val loss: 1.1381251811981201
Epoch 600, training loss: 0.6865589618682861 = 0.02088005840778351 + 0.1 * 6.656789302825928
Epoch 600, val loss: 1.1484109163284302
Epoch 610, training loss: 0.6837910413742065 = 0.0197900403290987 + 0.1 * 6.640009880065918
Epoch 610, val loss: 1.1581281423568726
Epoch 620, training loss: 0.6832240223884583 = 0.018790310248732567 + 0.1 * 6.644337177276611
Epoch 620, val loss: 1.1678462028503418
Epoch 630, training loss: 0.6806551218032837 = 0.017871953547000885 + 0.1 * 6.62783145904541
Epoch 630, val loss: 1.1769343614578247
Epoch 640, training loss: 0.6822735071182251 = 0.017022352665662766 + 0.1 * 6.652511119842529
Epoch 640, val loss: 1.1859303712844849
Epoch 650, training loss: 0.6798954606056213 = 0.01623806543648243 + 0.1 * 6.636573791503906
Epoch 650, val loss: 1.1947882175445557
Epoch 660, training loss: 0.6769317984580994 = 0.015511632896959782 + 0.1 * 6.614201545715332
Epoch 660, val loss: 1.20306396484375
Epoch 670, training loss: 0.6760451793670654 = 0.014834638684988022 + 0.1 * 6.612105369567871
Epoch 670, val loss: 1.211479663848877
Epoch 680, training loss: 0.6748745441436768 = 0.014205322600901127 + 0.1 * 6.606691837310791
Epoch 680, val loss: 1.219505786895752
Epoch 690, training loss: 0.676626980304718 = 0.013618641532957554 + 0.1 * 6.630083084106445
Epoch 690, val loss: 1.2273271083831787
Epoch 700, training loss: 0.6727359294891357 = 0.013070592656731606 + 0.1 * 6.596653461456299
Epoch 700, val loss: 1.235181212425232
Epoch 710, training loss: 0.67182856798172 = 0.012559052556753159 + 0.1 * 6.592695236206055
Epoch 710, val loss: 1.2424640655517578
Epoch 720, training loss: 0.6713917255401611 = 0.012079154141247272 + 0.1 * 6.593125343322754
Epoch 720, val loss: 1.2499406337738037
Epoch 730, training loss: 0.6709551215171814 = 0.01163086760789156 + 0.1 * 6.593242168426514
Epoch 730, val loss: 1.2569262981414795
Epoch 740, training loss: 0.6691041588783264 = 0.011208880692720413 + 0.1 * 6.578952789306641
Epoch 740, val loss: 1.2636444568634033
Epoch 750, training loss: 0.6694785356521606 = 0.010810921899974346 + 0.1 * 6.586676120758057
Epoch 750, val loss: 1.2707546949386597
Epoch 760, training loss: 0.6679816246032715 = 0.010437315329909325 + 0.1 * 6.575442790985107
Epoch 760, val loss: 1.2773056030273438
Epoch 770, training loss: 0.6672152876853943 = 0.010084949433803558 + 0.1 * 6.571303367614746
Epoch 770, val loss: 1.2835485935211182
Epoch 780, training loss: 0.6673055291175842 = 0.009750370867550373 + 0.1 * 6.575551509857178
Epoch 780, val loss: 1.2900148630142212
Epoch 790, training loss: 0.6655137538909912 = 0.009435545653104782 + 0.1 * 6.560781955718994
Epoch 790, val loss: 1.2962507009506226
Epoch 800, training loss: 0.665006160736084 = 0.009137586690485477 + 0.1 * 6.558685779571533
Epoch 800, val loss: 1.3023368120193481
Epoch 810, training loss: 0.6641923189163208 = 0.00885613914579153 + 0.1 * 6.553361892700195
Epoch 810, val loss: 1.3082356452941895
Epoch 820, training loss: 0.66441410779953 = 0.008588616736233234 + 0.1 * 6.558254718780518
Epoch 820, val loss: 1.3137917518615723
Epoch 830, training loss: 0.6624078154563904 = 0.008333643898367882 + 0.1 * 6.540741920471191
Epoch 830, val loss: 1.3195677995681763
Epoch 840, training loss: 0.6646804213523865 = 0.008091368712484837 + 0.1 * 6.565890312194824
Epoch 840, val loss: 1.3251235485076904
Epoch 850, training loss: 0.6624839901924133 = 0.007860898971557617 + 0.1 * 6.546230792999268
Epoch 850, val loss: 1.3306465148925781
Epoch 860, training loss: 0.6632357239723206 = 0.007641625590622425 + 0.1 * 6.555941104888916
Epoch 860, val loss: 1.3359960317611694
Epoch 870, training loss: 0.6613801121711731 = 0.007433081511408091 + 0.1 * 6.539470672607422
Epoch 870, val loss: 1.3412710428237915
Epoch 880, training loss: 0.6616785526275635 = 0.007233876734972 + 0.1 * 6.54444694519043
Epoch 880, val loss: 1.3463947772979736
Epoch 890, training loss: 0.6608859300613403 = 0.0070436918176710606 + 0.1 * 6.538422107696533
Epoch 890, val loss: 1.3515079021453857
Epoch 900, training loss: 0.6592006087303162 = 0.0068617709912359715 + 0.1 * 6.523388385772705
Epoch 900, val loss: 1.356441855430603
Epoch 910, training loss: 0.659512460231781 = 0.006687887012958527 + 0.1 * 6.52824592590332
Epoch 910, val loss: 1.3614107370376587
Epoch 920, training loss: 0.6593965888023376 = 0.006521766539663076 + 0.1 * 6.528748035430908
Epoch 920, val loss: 1.3661611080169678
Epoch 930, training loss: 0.6581171751022339 = 0.006362580694258213 + 0.1 * 6.5175461769104
Epoch 930, val loss: 1.3708878755569458
Epoch 940, training loss: 0.6600254774093628 = 0.006209815386682749 + 0.1 * 6.538156986236572
Epoch 940, val loss: 1.375439167022705
Epoch 950, training loss: 0.6588345170021057 = 0.006063011009246111 + 0.1 * 6.52771520614624
Epoch 950, val loss: 1.380080223083496
Epoch 960, training loss: 0.657426118850708 = 0.0059225899167358875 + 0.1 * 6.5150346755981445
Epoch 960, val loss: 1.3846441507339478
Epoch 970, training loss: 0.6573083400726318 = 0.005787952803075314 + 0.1 * 6.515203475952148
Epoch 970, val loss: 1.3889323472976685
Epoch 980, training loss: 0.6560743451118469 = 0.005658041685819626 + 0.1 * 6.504162788391113
Epoch 980, val loss: 1.3932727575302124
Epoch 990, training loss: 0.6566606163978577 = 0.0055333576165139675 + 0.1 * 6.51127290725708
Epoch 990, val loss: 1.3975993394851685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 2.809086322784424 = 1.9494022130966187 + 0.1 * 8.596839904785156
Epoch 0, val loss: 1.9496238231658936
Epoch 10, training loss: 2.7985363006591797 = 1.9388599395751953 + 0.1 * 8.596762657165527
Epoch 10, val loss: 1.9384750127792358
Epoch 20, training loss: 2.7852721214294434 = 1.92563796043396 + 0.1 * 8.59634017944336
Epoch 20, val loss: 1.9241663217544556
Epoch 30, training loss: 2.7663919925689697 = 1.907116174697876 + 0.1 * 8.592758178710938
Epoch 30, val loss: 1.90402352809906
Epoch 40, training loss: 2.737271785736084 = 1.8802883625030518 + 0.1 * 8.56983470916748
Epoch 40, val loss: 1.8750863075256348
Epoch 50, training loss: 2.691757917404175 = 1.8447117805480957 + 0.1 * 8.470460891723633
Epoch 50, val loss: 1.8383724689483643
Epoch 60, training loss: 2.6336851119995117 = 1.8075238466262817 + 0.1 * 8.261612892150879
Epoch 60, val loss: 1.8042716979980469
Epoch 70, training loss: 2.5834453105926514 = 1.7756576538085938 + 0.1 * 8.077876091003418
Epoch 70, val loss: 1.7788031101226807
Epoch 80, training loss: 2.5139074325561523 = 1.7405657768249512 + 0.1 * 7.73341703414917
Epoch 80, val loss: 1.7491848468780518
Epoch 90, training loss: 2.4376869201660156 = 1.6967884302139282 + 0.1 * 7.4089860916137695
Epoch 90, val loss: 1.711198329925537
Epoch 100, training loss: 2.3629579544067383 = 1.6382489204406738 + 0.1 * 7.24708890914917
Epoch 100, val loss: 1.6614925861358643
Epoch 110, training loss: 2.277817964553833 = 1.5610674619674683 + 0.1 * 7.167504787445068
Epoch 110, val loss: 1.5941920280456543
Epoch 120, training loss: 2.182976245880127 = 1.4703929424285889 + 0.1 * 7.125831604003906
Epoch 120, val loss: 1.5174078941345215
Epoch 130, training loss: 2.082939386367798 = 1.3737400770187378 + 0.1 * 7.0919928550720215
Epoch 130, val loss: 1.4396579265594482
Epoch 140, training loss: 1.9799370765686035 = 1.2741031646728516 + 0.1 * 7.058339595794678
Epoch 140, val loss: 1.36191725730896
Epoch 150, training loss: 1.874788522720337 = 1.1713248491287231 + 0.1 * 7.034637451171875
Epoch 150, val loss: 1.283466100692749
Epoch 160, training loss: 1.7704328298568726 = 1.069793939590454 + 0.1 * 7.0063886642456055
Epoch 160, val loss: 1.2070552110671997
Epoch 170, training loss: 1.670853853225708 = 0.9720919132232666 + 0.1 * 6.987618923187256
Epoch 170, val loss: 1.1347142457962036
Epoch 180, training loss: 1.5795737504959106 = 0.8818925619125366 + 0.1 * 6.97681188583374
Epoch 180, val loss: 1.0698399543762207
Epoch 190, training loss: 1.4983367919921875 = 0.8036856055259705 + 0.1 * 6.946511268615723
Epoch 190, val loss: 1.0160903930664062
Epoch 200, training loss: 1.4290345907211304 = 0.7368230223655701 + 0.1 * 6.922115802764893
Epoch 200, val loss: 0.9731167554855347
Epoch 210, training loss: 1.3704392910003662 = 0.6794207096099854 + 0.1 * 6.910186290740967
Epoch 210, val loss: 0.9395265579223633
Epoch 220, training loss: 1.3177690505981445 = 0.6291214227676392 + 0.1 * 6.886476039886475
Epoch 220, val loss: 0.9137209057807922
Epoch 230, training loss: 1.270754337310791 = 0.5835052728652954 + 0.1 * 6.872491359710693
Epoch 230, val loss: 0.8938482999801636
Epoch 240, training loss: 1.2268157005310059 = 0.5411925315856934 + 0.1 * 6.856231689453125
Epoch 240, val loss: 0.8785737156867981
Epoch 250, training loss: 1.1887850761413574 = 0.5014064311981201 + 0.1 * 6.873785495758057
Epoch 250, val loss: 0.8671597242355347
Epoch 260, training loss: 1.1476019620895386 = 0.4642886519432068 + 0.1 * 6.833133220672607
Epoch 260, val loss: 0.8590869903564453
Epoch 270, training loss: 1.111217975616455 = 0.42920225858688354 + 0.1 * 6.820156574249268
Epoch 270, val loss: 0.8537470698356628
Epoch 280, training loss: 1.0784484148025513 = 0.3962215185165405 + 0.1 * 6.822268962860107
Epoch 280, val loss: 0.851032555103302
Epoch 290, training loss: 1.0455608367919922 = 0.36545684933662415 + 0.1 * 6.801039218902588
Epoch 290, val loss: 0.8503650426864624
Epoch 300, training loss: 1.0149688720703125 = 0.33654865622520447 + 0.1 * 6.784202575683594
Epoch 300, val loss: 0.8515623807907104
Epoch 310, training loss: 0.9904338121414185 = 0.30940166115760803 + 0.1 * 6.810321807861328
Epoch 310, val loss: 0.8545313477516174
Epoch 320, training loss: 0.9611688852310181 = 0.28418609499931335 + 0.1 * 6.769827365875244
Epoch 320, val loss: 0.8590899705886841
Epoch 330, training loss: 0.9360189437866211 = 0.26049214601516724 + 0.1 * 6.75526762008667
Epoch 330, val loss: 0.8650027513504028
Epoch 340, training loss: 0.913045346736908 = 0.23810695111751556 + 0.1 * 6.749383926391602
Epoch 340, val loss: 0.8722342252731323
Epoch 350, training loss: 0.8907362818717957 = 0.21697594225406647 + 0.1 * 6.737603187561035
Epoch 350, val loss: 0.8806430697441101
Epoch 360, training loss: 0.8698939681053162 = 0.19694112241268158 + 0.1 * 6.729527950286865
Epoch 360, val loss: 0.8902385830879211
Epoch 370, training loss: 0.8507070541381836 = 0.1781536489725113 + 0.1 * 6.725533962249756
Epoch 370, val loss: 0.9008718132972717
Epoch 380, training loss: 0.831642746925354 = 0.1606663167476654 + 0.1 * 6.70976448059082
Epoch 380, val loss: 0.9125626683235168
Epoch 390, training loss: 0.8144228458404541 = 0.1445637196302414 + 0.1 * 6.698591232299805
Epoch 390, val loss: 0.9251532554626465
Epoch 400, training loss: 0.8003935813903809 = 0.12992079555988312 + 0.1 * 6.704727649688721
Epoch 400, val loss: 0.9385315179824829
Epoch 410, training loss: 0.786287248134613 = 0.11672460287809372 + 0.1 * 6.695626258850098
Epoch 410, val loss: 0.9525371193885803
Epoch 420, training loss: 0.7728638648986816 = 0.10486659407615662 + 0.1 * 6.679973125457764
Epoch 420, val loss: 0.9670957326889038
Epoch 430, training loss: 0.7612451910972595 = 0.09425459802150726 + 0.1 * 6.669905662536621
Epoch 430, val loss: 0.9822306036949158
Epoch 440, training loss: 0.7547475695610046 = 0.08482759445905685 + 0.1 * 6.699199676513672
Epoch 440, val loss: 0.9979097843170166
Epoch 450, training loss: 0.7443151473999023 = 0.07656577229499817 + 0.1 * 6.677493572235107
Epoch 450, val loss: 1.0135438442230225
Epoch 460, training loss: 0.7349753379821777 = 0.06928520649671555 + 0.1 * 6.6569013595581055
Epoch 460, val loss: 1.0296052694320679
Epoch 470, training loss: 0.7274308204650879 = 0.06286904215812683 + 0.1 * 6.645617485046387
Epoch 470, val loss: 1.0458189249038696
Epoch 480, training loss: 0.7244154214859009 = 0.05722633749246597 + 0.1 * 6.671890735626221
Epoch 480, val loss: 1.0622119903564453
Epoch 490, training loss: 0.7164456844329834 = 0.052293211221694946 + 0.1 * 6.641524791717529
Epoch 490, val loss: 1.0784776210784912
Epoch 500, training loss: 0.7112724184989929 = 0.047965679317712784 + 0.1 * 6.6330671310424805
Epoch 500, val loss: 1.0947071313858032
Epoch 510, training loss: 0.7091771960258484 = 0.04415768384933472 + 0.1 * 6.650195121765137
Epoch 510, val loss: 1.11074697971344
Epoch 520, training loss: 0.7036786079406738 = 0.040815215557813644 + 0.1 * 6.628633499145508
Epoch 520, val loss: 1.1264585256576538
Epoch 530, training loss: 0.7001281380653381 = 0.037862446159124374 + 0.1 * 6.622657299041748
Epoch 530, val loss: 1.1417347192764282
Epoch 540, training loss: 0.6969177722930908 = 0.03523031994700432 + 0.1 * 6.6168742179870605
Epoch 540, val loss: 1.1567963361740112
Epoch 550, training loss: 0.6945375204086304 = 0.03287757188081741 + 0.1 * 6.6165995597839355
Epoch 550, val loss: 1.1716095209121704
Epoch 560, training loss: 0.6926620006561279 = 0.03077155165374279 + 0.1 * 6.6189045906066895
Epoch 560, val loss: 1.1858993768692017
Epoch 570, training loss: 0.6892281770706177 = 0.028872882947325706 + 0.1 * 6.603552341461182
Epoch 570, val loss: 1.1999634504318237
Epoch 580, training loss: 0.6873766779899597 = 0.027155382558703423 + 0.1 * 6.602212905883789
Epoch 580, val loss: 1.2135393619537354
Epoch 590, training loss: 0.6852344274520874 = 0.02559499815106392 + 0.1 * 6.596394062042236
Epoch 590, val loss: 1.2270262241363525
Epoch 600, training loss: 0.6840344071388245 = 0.024179313331842422 + 0.1 * 6.598550319671631
Epoch 600, val loss: 1.2398171424865723
Epoch 610, training loss: 0.6820282340049744 = 0.022885214537382126 + 0.1 * 6.591430187225342
Epoch 610, val loss: 1.2525172233581543
Epoch 620, training loss: 0.680377185344696 = 0.021700065582990646 + 0.1 * 6.586771011352539
Epoch 620, val loss: 1.264818549156189
Epoch 630, training loss: 0.6788589954376221 = 0.020611586049199104 + 0.1 * 6.5824737548828125
Epoch 630, val loss: 1.2767901420593262
Epoch 640, training loss: 0.6799658536911011 = 0.01960868202149868 + 0.1 * 6.603571891784668
Epoch 640, val loss: 1.2885133028030396
Epoch 650, training loss: 0.6769126057624817 = 0.018687652423977852 + 0.1 * 6.582249164581299
Epoch 650, val loss: 1.299778699874878
Epoch 660, training loss: 0.6754423379898071 = 0.017835259437561035 + 0.1 * 6.576070785522461
Epoch 660, val loss: 1.3107682466506958
Epoch 670, training loss: 0.6755707859992981 = 0.017044253647327423 + 0.1 * 6.585265159606934
Epoch 670, val loss: 1.3216136693954468
Epoch 680, training loss: 0.6734229326248169 = 0.016310302540659904 + 0.1 * 6.571126461029053
Epoch 680, val loss: 1.3319624662399292
Epoch 690, training loss: 0.6722822785377502 = 0.015626810491085052 + 0.1 * 6.566554546356201
Epoch 690, val loss: 1.342349886894226
Epoch 700, training loss: 0.6701180934906006 = 0.014991643838584423 + 0.1 * 6.55126428604126
Epoch 700, val loss: 1.3521051406860352
Epoch 710, training loss: 0.669700562953949 = 0.014397122897207737 + 0.1 * 6.55303430557251
Epoch 710, val loss: 1.3618091344833374
Epoch 720, training loss: 0.6694640517234802 = 0.013839682564139366 + 0.1 * 6.556243896484375
Epoch 720, val loss: 1.3714585304260254
Epoch 730, training loss: 0.668104350566864 = 0.013318873010575771 + 0.1 * 6.547854423522949
Epoch 730, val loss: 1.380711555480957
Epoch 740, training loss: 0.6679245233535767 = 0.012830185703933239 + 0.1 * 6.550943374633789
Epoch 740, val loss: 1.3896294832229614
Epoch 750, training loss: 0.6665180325508118 = 0.01237000897526741 + 0.1 * 6.541480541229248
Epoch 750, val loss: 1.3985764980316162
Epoch 760, training loss: 0.6686081886291504 = 0.011936829425394535 + 0.1 * 6.566713333129883
Epoch 760, val loss: 1.4073032140731812
Epoch 770, training loss: 0.6655213236808777 = 0.011529338546097279 + 0.1 * 6.539919853210449
Epoch 770, val loss: 1.4155800342559814
Epoch 780, training loss: 0.6646274328231812 = 0.0111453952267766 + 0.1 * 6.534820079803467
Epoch 780, val loss: 1.4237765073776245
Epoch 790, training loss: 0.6636066436767578 = 0.010781544260680676 + 0.1 * 6.5282511711120605
Epoch 790, val loss: 1.431929111480713
Epoch 800, training loss: 0.6636534333229065 = 0.010437997058033943 + 0.1 * 6.532154083251953
Epoch 800, val loss: 1.4396597146987915
Epoch 810, training loss: 0.6643720865249634 = 0.010111607611179352 + 0.1 * 6.542604446411133
Epoch 810, val loss: 1.4475115537643433
Epoch 820, training loss: 0.6634786128997803 = 0.009802876971662045 + 0.1 * 6.536757469177246
Epoch 820, val loss: 1.4551336765289307
Epoch 830, training loss: 0.6623903512954712 = 0.009509533643722534 + 0.1 * 6.52880859375
Epoch 830, val loss: 1.4625016450881958
Epoch 840, training loss: 0.661429762840271 = 0.009230278432369232 + 0.1 * 6.5219950675964355
Epoch 840, val loss: 1.4698978662490845
Epoch 850, training loss: 0.660372793674469 = 0.00896559376269579 + 0.1 * 6.514071941375732
Epoch 850, val loss: 1.476938247680664
Epoch 860, training loss: 0.6600824594497681 = 0.00871310941874981 + 0.1 * 6.513693332672119
Epoch 860, val loss: 1.4839859008789062
Epoch 870, training loss: 0.6612402200698853 = 0.008472374640405178 + 0.1 * 6.527678489685059
Epoch 870, val loss: 1.4910027980804443
Epoch 880, training loss: 0.6592772603034973 = 0.008242563344538212 + 0.1 * 6.51034688949585
Epoch 880, val loss: 1.4977638721466064
Epoch 890, training loss: 0.6597253084182739 = 0.00802432931959629 + 0.1 * 6.517009735107422
Epoch 890, val loss: 1.5041451454162598
Epoch 900, training loss: 0.6585310101509094 = 0.007814849726855755 + 0.1 * 6.507161617279053
Epoch 900, val loss: 1.510812759399414
Epoch 910, training loss: 0.6577680110931396 = 0.007614848669618368 + 0.1 * 6.501531600952148
Epoch 910, val loss: 1.517216444015503
Epoch 920, training loss: 0.6575967073440552 = 0.007423733826726675 + 0.1 * 6.501729488372803
Epoch 920, val loss: 1.5233440399169922
Epoch 930, training loss: 0.657308042049408 = 0.007239923346787691 + 0.1 * 6.500680923461914
Epoch 930, val loss: 1.529775619506836
Epoch 940, training loss: 0.6567184329032898 = 0.0070646535605192184 + 0.1 * 6.496537685394287
Epoch 940, val loss: 1.5357980728149414
Epoch 950, training loss: 0.6579634547233582 = 0.006896308157593012 + 0.1 * 6.510671138763428
Epoch 950, val loss: 1.5416524410247803
Epoch 960, training loss: 0.6554464101791382 = 0.006734317634254694 + 0.1 * 6.487120628356934
Epoch 960, val loss: 1.5477604866027832
Epoch 970, training loss: 0.6557653546333313 = 0.00657935393974185 + 0.1 * 6.4918599128723145
Epoch 970, val loss: 1.5534175634384155
Epoch 980, training loss: 0.6568995118141174 = 0.006430050823837519 + 0.1 * 6.504694938659668
Epoch 980, val loss: 1.559046745300293
Epoch 990, training loss: 0.6551037430763245 = 0.006286436226218939 + 0.1 * 6.488173007965088
Epoch 990, val loss: 1.5649324655532837
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 2.820077419281006 = 1.9603933095932007 + 0.1 * 8.596840858459473
Epoch 0, val loss: 1.9517076015472412
Epoch 10, training loss: 2.8092329502105713 = 1.9495576620101929 + 0.1 * 8.596753120422363
Epoch 10, val loss: 1.9417150020599365
Epoch 20, training loss: 2.795854330062866 = 1.9362486600875854 + 0.1 * 8.59605598449707
Epoch 20, val loss: 1.9290400743484497
Epoch 30, training loss: 2.7763562202453613 = 1.9173924922943115 + 0.1 * 8.589635848999023
Epoch 30, val loss: 1.910766363143921
Epoch 40, training loss: 2.7448816299438477 = 1.8895283937454224 + 0.1 * 8.553532600402832
Epoch 40, val loss: 1.883894920349121
Epoch 50, training loss: 2.691293239593506 = 1.8516947031021118 + 0.1 * 8.395984649658203
Epoch 50, val loss: 1.8492857217788696
Epoch 60, training loss: 2.6368770599365234 = 1.810137391090393 + 0.1 * 8.2673978805542
Epoch 60, val loss: 1.815307855606079
Epoch 70, training loss: 2.5818192958831787 = 1.7744224071502686 + 0.1 * 8.073967933654785
Epoch 70, val loss: 1.788225769996643
Epoch 80, training loss: 2.5088348388671875 = 1.7371982336044312 + 0.1 * 7.716366767883301
Epoch 80, val loss: 1.7556986808776855
Epoch 90, training loss: 2.43936824798584 = 1.6908282041549683 + 0.1 * 7.48539924621582
Epoch 90, val loss: 1.712764024734497
Epoch 100, training loss: 2.3643527030944824 = 1.6318265199661255 + 0.1 * 7.32526159286499
Epoch 100, val loss: 1.660672903060913
Epoch 110, training loss: 2.277555465698242 = 1.557313084602356 + 0.1 * 7.202423572540283
Epoch 110, val loss: 1.5986722707748413
Epoch 120, training loss: 2.1840522289276123 = 1.4709714651107788 + 0.1 * 7.130807399749756
Epoch 120, val loss: 1.5288162231445312
Epoch 130, training loss: 2.0880203247070312 = 1.3802471160888672 + 0.1 * 7.077731132507324
Epoch 130, val loss: 1.4584071636199951
Epoch 140, training loss: 1.9948456287384033 = 1.2913289070129395 + 0.1 * 7.035166263580322
Epoch 140, val loss: 1.3941824436187744
Epoch 150, training loss: 1.907336711883545 = 1.208242654800415 + 0.1 * 6.990941047668457
Epoch 150, val loss: 1.3365179300308228
Epoch 160, training loss: 1.8278007507324219 = 1.1316183805465698 + 0.1 * 6.961824417114258
Epoch 160, val loss: 1.2853386402130127
Epoch 170, training loss: 1.7566883563995361 = 1.0630395412445068 + 0.1 * 6.936488151550293
Epoch 170, val loss: 1.2419428825378418
Epoch 180, training loss: 1.694133996963501 = 1.0027661323547363 + 0.1 * 6.9136786460876465
Epoch 180, val loss: 1.2067639827728271
Epoch 190, training loss: 1.6396102905273438 = 0.9491142630577087 + 0.1 * 6.9049601554870605
Epoch 190, val loss: 1.1785470247268677
Epoch 200, training loss: 1.5894012451171875 = 0.9003903865814209 + 0.1 * 6.89010763168335
Epoch 200, val loss: 1.1563433408737183
Epoch 210, training loss: 1.5405323505401611 = 0.8530240654945374 + 0.1 * 6.875082015991211
Epoch 210, val loss: 1.1367100477218628
Epoch 220, training loss: 1.4915459156036377 = 0.8047870397567749 + 0.1 * 6.867588520050049
Epoch 220, val loss: 1.1179428100585938
Epoch 230, training loss: 1.440958023071289 = 0.7549054622650146 + 0.1 * 6.860524654388428
Epoch 230, val loss: 1.0995268821716309
Epoch 240, training loss: 1.3882286548614502 = 0.7027080059051514 + 0.1 * 6.85520601272583
Epoch 240, val loss: 1.0803112983703613
Epoch 250, training loss: 1.3344494104385376 = 0.6494342684745789 + 0.1 * 6.850151062011719
Epoch 250, val loss: 1.0608603954315186
Epoch 260, training loss: 1.2805066108703613 = 0.5964081883430481 + 0.1 * 6.8409833908081055
Epoch 260, val loss: 1.0417205095291138
Epoch 270, training loss: 1.2281509637832642 = 0.5446962714195251 + 0.1 * 6.83454704284668
Epoch 270, val loss: 1.024336576461792
Epoch 280, training loss: 1.1783288717269897 = 0.4954703152179718 + 0.1 * 6.828585147857666
Epoch 280, val loss: 1.0094764232635498
Epoch 290, training loss: 1.1328340768814087 = 0.44992151856422424 + 0.1 * 6.82912540435791
Epoch 290, val loss: 0.9978107810020447
Epoch 300, training loss: 1.0906784534454346 = 0.40848684310913086 + 0.1 * 6.821915149688721
Epoch 300, val loss: 0.9899582266807556
Epoch 310, training loss: 1.0514020919799805 = 0.370704710483551 + 0.1 * 6.806973934173584
Epoch 310, val loss: 0.9858627915382385
Epoch 320, training loss: 1.016592025756836 = 0.3365229070186615 + 0.1 * 6.8006911277771
Epoch 320, val loss: 0.985198974609375
Epoch 330, training loss: 0.9851212501525879 = 0.30575352907180786 + 0.1 * 6.793676853179932
Epoch 330, val loss: 0.9879010319709778
Epoch 340, training loss: 0.9562431573867798 = 0.277906209230423 + 0.1 * 6.783369064331055
Epoch 340, val loss: 0.9939808249473572
Epoch 350, training loss: 0.9318559765815735 = 0.2525092363357544 + 0.1 * 6.7934675216674805
Epoch 350, val loss: 1.0030996799468994
Epoch 360, training loss: 0.9055740833282471 = 0.2291540950536728 + 0.1 * 6.764199733734131
Epoch 360, val loss: 1.0150705575942993
Epoch 370, training loss: 0.8829697370529175 = 0.20740291476249695 + 0.1 * 6.7556681632995605
Epoch 370, val loss: 1.0295836925506592
Epoch 380, training loss: 0.8629839420318604 = 0.18722288310527802 + 0.1 * 6.757610321044922
Epoch 380, val loss: 1.0462068319320679
Epoch 390, training loss: 0.8425871729850769 = 0.16870073974132538 + 0.1 * 6.738863945007324
Epoch 390, val loss: 1.0643246173858643
Epoch 400, training loss: 0.8243892788887024 = 0.15193022787570953 + 0.1 * 6.724590301513672
Epoch 400, val loss: 1.0836808681488037
Epoch 410, training loss: 0.809230625629425 = 0.13692797720432281 + 0.1 * 6.723026752471924
Epoch 410, val loss: 1.1035730838775635
Epoch 420, training loss: 0.796501636505127 = 0.12365570664405823 + 0.1 * 6.728459358215332
Epoch 420, val loss: 1.1239691972732544
Epoch 430, training loss: 0.782091498374939 = 0.11201195418834686 + 0.1 * 6.7007951736450195
Epoch 430, val loss: 1.1445271968841553
Epoch 440, training loss: 0.7716565132141113 = 0.10175265371799469 + 0.1 * 6.699038505554199
Epoch 440, val loss: 1.1650365591049194
Epoch 450, training loss: 0.7613013982772827 = 0.09271574020385742 + 0.1 * 6.685856342315674
Epoch 450, val loss: 1.1857584714889526
Epoch 460, training loss: 0.7523611187934875 = 0.08473528921604156 + 0.1 * 6.676258563995361
Epoch 460, val loss: 1.2064414024353027
Epoch 470, training loss: 0.7448899149894714 = 0.07766295969486237 + 0.1 * 6.672269344329834
Epoch 470, val loss: 1.2271335124969482
Epoch 480, training loss: 0.7404730916023254 = 0.07138688117265701 + 0.1 * 6.69086217880249
Epoch 480, val loss: 1.2475870847702026
Epoch 490, training loss: 0.7321379780769348 = 0.06581670790910721 + 0.1 * 6.663212776184082
Epoch 490, val loss: 1.267891526222229
Epoch 500, training loss: 0.7263695001602173 = 0.06083495914936066 + 0.1 * 6.655345439910889
Epoch 500, val loss: 1.2877705097198486
Epoch 510, training loss: 0.7232820987701416 = 0.05636407434940338 + 0.1 * 6.669180393218994
Epoch 510, val loss: 1.3075226545333862
Epoch 520, training loss: 0.717552125453949 = 0.052349526435136795 + 0.1 * 6.652026176452637
Epoch 520, val loss: 1.3268780708312988
Epoch 530, training loss: 0.7133063077926636 = 0.04872836172580719 + 0.1 * 6.645779609680176
Epoch 530, val loss: 1.3458741903305054
Epoch 540, training loss: 0.7098649740219116 = 0.04546157270669937 + 0.1 * 6.644033432006836
Epoch 540, val loss: 1.36448073387146
Epoch 550, training loss: 0.7067968249320984 = 0.04250534623861313 + 0.1 * 6.642914772033691
Epoch 550, val loss: 1.3826051950454712
Epoch 560, training loss: 0.7016299962997437 = 0.03982197120785713 + 0.1 * 6.618080139160156
Epoch 560, val loss: 1.4004584550857544
Epoch 570, training loss: 0.7020969390869141 = 0.037376292049884796 + 0.1 * 6.6472063064575195
Epoch 570, val loss: 1.41776442527771
Epoch 580, training loss: 0.6977234482765198 = 0.03514568880200386 + 0.1 * 6.625777244567871
Epoch 580, val loss: 1.4348000288009644
Epoch 590, training loss: 0.6940162181854248 = 0.03310566768050194 + 0.1 * 6.609105110168457
Epoch 590, val loss: 1.451345443725586
Epoch 600, training loss: 0.6957248449325562 = 0.031230147927999496 + 0.1 * 6.644946575164795
Epoch 600, val loss: 1.4676388502120972
Epoch 610, training loss: 0.6897496581077576 = 0.029516682028770447 + 0.1 * 6.602329730987549
Epoch 610, val loss: 1.4833881855010986
Epoch 620, training loss: 0.6889848709106445 = 0.02794080600142479 + 0.1 * 6.610440254211426
Epoch 620, val loss: 1.4986910820007324
Epoch 630, training loss: 0.6867432594299316 = 0.026489609852433205 + 0.1 * 6.602536201477051
Epoch 630, val loss: 1.5136624574661255
Epoch 640, training loss: 0.6842821836471558 = 0.02515103481709957 + 0.1 * 6.591311454772949
Epoch 640, val loss: 1.5281990766525269
Epoch 650, training loss: 0.6826727390289307 = 0.023907514289021492 + 0.1 * 6.58765172958374
Epoch 650, val loss: 1.542419672012329
Epoch 660, training loss: 0.6809266805648804 = 0.02275506779551506 + 0.1 * 6.581716060638428
Epoch 660, val loss: 1.5563604831695557
Epoch 670, training loss: 0.6818198561668396 = 0.021683378145098686 + 0.1 * 6.601364612579346
Epoch 670, val loss: 1.5699083805084229
Epoch 680, training loss: 0.6788806915283203 = 0.02068922482430935 + 0.1 * 6.58191442489624
Epoch 680, val loss: 1.5831445455551147
Epoch 690, training loss: 0.6765215396881104 = 0.019762936979532242 + 0.1 * 6.5675859451293945
Epoch 690, val loss: 1.596064567565918
Epoch 700, training loss: 0.6754493117332458 = 0.018897823989391327 + 0.1 * 6.56551456451416
Epoch 700, val loss: 1.6085395812988281
Epoch 710, training loss: 0.6742768287658691 = 0.018093325197696686 + 0.1 * 6.561834812164307
Epoch 710, val loss: 1.6208902597427368
Epoch 720, training loss: 0.6737000942230225 = 0.017338937148451805 + 0.1 * 6.5636115074157715
Epoch 720, val loss: 1.6327314376831055
Epoch 730, training loss: 0.6736545562744141 = 0.01663452945649624 + 0.1 * 6.570199966430664
Epoch 730, val loss: 1.644594430923462
Epoch 740, training loss: 0.6718458533287048 = 0.015972185879945755 + 0.1 * 6.558736324310303
Epoch 740, val loss: 1.6558446884155273
Epoch 750, training loss: 0.6708899140357971 = 0.015351882204413414 + 0.1 * 6.555380344390869
Epoch 750, val loss: 1.66700279712677
Epoch 760, training loss: 0.6686269044876099 = 0.014768748544156551 + 0.1 * 6.538581848144531
Epoch 760, val loss: 1.677863359451294
Epoch 770, training loss: 0.6687606573104858 = 0.014218214899301529 + 0.1 * 6.545424461364746
Epoch 770, val loss: 1.6885452270507812
Epoch 780, training loss: 0.669873058795929 = 0.01369843352586031 + 0.1 * 6.561745643615723
Epoch 780, val loss: 1.6988506317138672
Epoch 790, training loss: 0.6676720976829529 = 0.013212122954428196 + 0.1 * 6.544599533081055
Epoch 790, val loss: 1.7090272903442383
Epoch 800, training loss: 0.6657814979553223 = 0.01275058463215828 + 0.1 * 6.530309200286865
Epoch 800, val loss: 1.7187855243682861
Epoch 810, training loss: 0.6666505932807922 = 0.012315366417169571 + 0.1 * 6.543352127075195
Epoch 810, val loss: 1.728507161140442
Epoch 820, training loss: 0.6642560958862305 = 0.011902712285518646 + 0.1 * 6.523533821105957
Epoch 820, val loss: 1.7379122972488403
Epoch 830, training loss: 0.6650869846343994 = 0.011511406861245632 + 0.1 * 6.535755634307861
Epoch 830, val loss: 1.7471675872802734
Epoch 840, training loss: 0.6630837917327881 = 0.011139958165585995 + 0.1 * 6.51943826675415
Epoch 840, val loss: 1.7562509775161743
Epoch 850, training loss: 0.6629148125648499 = 0.0107876006513834 + 0.1 * 6.521271705627441
Epoch 850, val loss: 1.765092134475708
Epoch 860, training loss: 0.662630021572113 = 0.010453125461935997 + 0.1 * 6.521769046783447
Epoch 860, val loss: 1.773866057395935
Epoch 870, training loss: 0.6629058122634888 = 0.010134920477867126 + 0.1 * 6.527708530426025
Epoch 870, val loss: 1.7824307680130005
Epoch 880, training loss: 0.6624355316162109 = 0.009831833653151989 + 0.1 * 6.526036739349365
Epoch 880, val loss: 1.7906889915466309
Epoch 890, training loss: 0.6612744331359863 = 0.009543932043015957 + 0.1 * 6.517304420471191
Epoch 890, val loss: 1.7989920377731323
Epoch 900, training loss: 0.6625210046768188 = 0.009269088506698608 + 0.1 * 6.532519340515137
Epoch 900, val loss: 1.8070721626281738
Epoch 910, training loss: 0.6595948338508606 = 0.00900715310126543 + 0.1 * 6.505876541137695
Epoch 910, val loss: 1.8148545026779175
Epoch 920, training loss: 0.6604721546173096 = 0.008757632225751877 + 0.1 * 6.517144680023193
Epoch 920, val loss: 1.8227407932281494
Epoch 930, training loss: 0.6590666174888611 = 0.00851805042475462 + 0.1 * 6.505485534667969
Epoch 930, val loss: 1.830251693725586
Epoch 940, training loss: 0.6582326292991638 = 0.008290058001875877 + 0.1 * 6.499425888061523
Epoch 940, val loss: 1.8379312753677368
Epoch 950, training loss: 0.6600597500801086 = 0.008070801384747028 + 0.1 * 6.519888877868652
Epoch 950, val loss: 1.8450725078582764
Epoch 960, training loss: 0.6576642394065857 = 0.007861224934458733 + 0.1 * 6.498030185699463
Epoch 960, val loss: 1.852310299873352
Epoch 970, training loss: 0.6588383913040161 = 0.0076607284136116505 + 0.1 * 6.511776447296143
Epoch 970, val loss: 1.8594417572021484
Epoch 980, training loss: 0.6563317179679871 = 0.007468412164598703 + 0.1 * 6.488633155822754
Epoch 980, val loss: 1.8662184476852417
Epoch 990, training loss: 0.6584819555282593 = 0.00728428503498435 + 0.1 * 6.511976718902588
Epoch 990, val loss: 1.8731591701507568
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8112809699525567
The final CL Acc:0.77284, 0.01552, The final GNN Acc:0.80952, 0.00326
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13212])
remove edge: torch.Size([2, 7922])
updated graph: torch.Size([2, 10578])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.79929780960083 = 1.9396121501922607 + 0.1 * 8.596857070922852
Epoch 0, val loss: 1.9361299276351929
Epoch 10, training loss: 2.7893216609954834 = 1.9296419620513916 + 0.1 * 8.596796035766602
Epoch 10, val loss: 1.9265512228012085
Epoch 20, training loss: 2.777421236038208 = 1.9177736043930054 + 0.1 * 8.596476554870605
Epoch 20, val loss: 1.91482675075531
Epoch 30, training loss: 2.761039972305298 = 1.9016350507736206 + 0.1 * 8.594048500061035
Epoch 30, val loss: 1.8985666036605835
Epoch 40, training loss: 2.7357776165008545 = 1.8784258365631104 + 0.1 * 8.573517799377441
Epoch 40, val loss: 1.8752515316009521
Epoch 50, training loss: 2.6915228366851807 = 1.8463411331176758 + 0.1 * 8.45181655883789
Epoch 50, val loss: 1.844576358795166
Epoch 60, training loss: 2.627380132675171 = 1.8099462985992432 + 0.1 * 8.174338340759277
Epoch 60, val loss: 1.8131543397903442
Epoch 70, training loss: 2.571352481842041 = 1.7759150266647339 + 0.1 * 7.954373836517334
Epoch 70, val loss: 1.7853885889053345
Epoch 80, training loss: 2.5017364025115967 = 1.7387726306915283 + 0.1 * 7.629637241363525
Epoch 80, val loss: 1.752830982208252
Epoch 90, training loss: 2.424889087677002 = 1.6930975914001465 + 0.1 * 7.317915439605713
Epoch 90, val loss: 1.7118926048278809
Epoch 100, training loss: 2.34786057472229 = 1.6307613849639893 + 0.1 * 7.170991897583008
Epoch 100, val loss: 1.6570470333099365
Epoch 110, training loss: 2.26098370552063 = 1.5523629188537598 + 0.1 * 7.086208343505859
Epoch 110, val loss: 1.5907258987426758
Epoch 120, training loss: 2.1727962493896484 = 1.4688535928726196 + 0.1 * 7.039425849914551
Epoch 120, val loss: 1.5225640535354614
Epoch 130, training loss: 2.0872488021850586 = 1.387746810913086 + 0.1 * 6.995020389556885
Epoch 130, val loss: 1.4590226411819458
Epoch 140, training loss: 2.006012439727783 = 1.308773159980774 + 0.1 * 6.972391605377197
Epoch 140, val loss: 1.4001858234405518
Epoch 150, training loss: 1.9262151718139648 = 1.230517029762268 + 0.1 * 6.956982135772705
Epoch 150, val loss: 1.3426592350006104
Epoch 160, training loss: 1.8471839427947998 = 1.1526660919189453 + 0.1 * 6.945178985595703
Epoch 160, val loss: 1.2850477695465088
Epoch 170, training loss: 1.771043062210083 = 1.0774431228637695 + 0.1 * 6.935999870300293
Epoch 170, val loss: 1.2300758361816406
Epoch 180, training loss: 1.699263095855713 = 1.006148099899292 + 0.1 * 6.931149959564209
Epoch 180, val loss: 1.1781002283096313
Epoch 190, training loss: 1.6314440965652466 = 0.9392456412315369 + 0.1 * 6.921984672546387
Epoch 190, val loss: 1.129450798034668
Epoch 200, training loss: 1.5658864974975586 = 0.8746148943901062 + 0.1 * 6.912716388702393
Epoch 200, val loss: 1.0821658372879028
Epoch 210, training loss: 1.5008095502853394 = 0.8095718622207642 + 0.1 * 6.912376880645752
Epoch 210, val loss: 1.0338166952133179
Epoch 220, training loss: 1.4331107139587402 = 0.7433392405509949 + 0.1 * 6.897714614868164
Epoch 220, val loss: 0.9844465255737305
Epoch 230, training loss: 1.3650562763214111 = 0.6763513684272766 + 0.1 * 6.887048721313477
Epoch 230, val loss: 0.9346572756767273
Epoch 240, training loss: 1.298640251159668 = 0.6113322973251343 + 0.1 * 6.873079776763916
Epoch 240, val loss: 0.8878458738327026
Epoch 250, training loss: 1.2396299839019775 = 0.5512157678604126 + 0.1 * 6.884141445159912
Epoch 250, val loss: 0.8470317125320435
Epoch 260, training loss: 1.1830562353134155 = 0.49846330285072327 + 0.1 * 6.84592866897583
Epoch 260, val loss: 0.8146662712097168
Epoch 270, training loss: 1.1354725360870361 = 0.45237624645233154 + 0.1 * 6.830963134765625
Epoch 270, val loss: 0.7901525497436523
Epoch 280, training loss: 1.094637155532837 = 0.4120538532733917 + 0.1 * 6.825832843780518
Epoch 280, val loss: 0.7720556259155273
Epoch 290, training loss: 1.0563466548919678 = 0.3765299618244171 + 0.1 * 6.798166751861572
Epoch 290, val loss: 0.7590369582176208
Epoch 300, training loss: 1.0241267681121826 = 0.3444863557815552 + 0.1 * 6.796403884887695
Epoch 300, val loss: 0.7496683597564697
Epoch 310, training loss: 0.9928101301193237 = 0.31518539786338806 + 0.1 * 6.776247501373291
Epoch 310, val loss: 0.7429203391075134
Epoch 320, training loss: 0.9641538262367249 = 0.2878187894821167 + 0.1 * 6.763350009918213
Epoch 320, val loss: 0.7381693720817566
Epoch 330, training loss: 0.9389644265174866 = 0.26214903593063354 + 0.1 * 6.768153667449951
Epoch 330, val loss: 0.7349968552589417
Epoch 340, training loss: 0.9126532077789307 = 0.23803609609603882 + 0.1 * 6.746170997619629
Epoch 340, val loss: 0.7330422401428223
Epoch 350, training loss: 0.888386607170105 = 0.2152203917503357 + 0.1 * 6.731662273406982
Epoch 350, val loss: 0.7324937582015991
Epoch 360, training loss: 0.8697713613510132 = 0.19374117255210876 + 0.1 * 6.7603020668029785
Epoch 360, val loss: 0.7332794666290283
Epoch 370, training loss: 0.846443772315979 = 0.17392340302467346 + 0.1 * 6.725203514099121
Epoch 370, val loss: 0.7352434992790222
Epoch 380, training loss: 0.8271209597587585 = 0.1557711809873581 + 0.1 * 6.713497638702393
Epoch 380, val loss: 0.7387317419052124
Epoch 390, training loss: 0.8091930747032166 = 0.13936461508274078 + 0.1 * 6.69828462600708
Epoch 390, val loss: 0.7434645891189575
Epoch 400, training loss: 0.7936530113220215 = 0.12467923760414124 + 0.1 * 6.689737319946289
Epoch 400, val loss: 0.7494873404502869
Epoch 410, training loss: 0.7817984223365784 = 0.11163505166769028 + 0.1 * 6.701633930206299
Epoch 410, val loss: 0.756650447845459
Epoch 420, training loss: 0.7692232131958008 = 0.1001811996102333 + 0.1 * 6.690419673919678
Epoch 420, val loss: 0.7645355463027954
Epoch 430, training loss: 0.757749617099762 = 0.09014235436916351 + 0.1 * 6.676072597503662
Epoch 430, val loss: 0.7731807827949524
Epoch 440, training loss: 0.7480894923210144 = 0.08136359602212906 + 0.1 * 6.6672587394714355
Epoch 440, val loss: 0.7824634313583374
Epoch 450, training loss: 0.7408359050750732 = 0.07367612421512604 + 0.1 * 6.671597957611084
Epoch 450, val loss: 0.7922549247741699
Epoch 460, training loss: 0.7326632738113403 = 0.06694567948579788 + 0.1 * 6.6571760177612305
Epoch 460, val loss: 0.8022868037223816
Epoch 470, training loss: 0.727397084236145 = 0.06103917583823204 + 0.1 * 6.663578987121582
Epoch 470, val loss: 0.8126278519630432
Epoch 480, training loss: 0.7203245162963867 = 0.05587165430188179 + 0.1 * 6.644528388977051
Epoch 480, val loss: 0.8225888013839722
Epoch 490, training loss: 0.7149096727371216 = 0.05130448937416077 + 0.1 * 6.636052131652832
Epoch 490, val loss: 0.8328551054000854
Epoch 500, training loss: 0.7102309465408325 = 0.04723867028951645 + 0.1 * 6.629922866821289
Epoch 500, val loss: 0.8429955840110779
Epoch 510, training loss: 0.7070134878158569 = 0.04361569136381149 + 0.1 * 6.633977890014648
Epoch 510, val loss: 0.8531519770622253
Epoch 520, training loss: 0.7022809982299805 = 0.04038986191153526 + 0.1 * 6.618911266326904
Epoch 520, val loss: 0.8628979921340942
Epoch 530, training loss: 0.7023652791976929 = 0.037495438009500504 + 0.1 * 6.648698329925537
Epoch 530, val loss: 0.8727033734321594
Epoch 540, training loss: 0.6965723633766174 = 0.03490031510591507 + 0.1 * 6.616720199584961
Epoch 540, val loss: 0.8821211457252502
Epoch 550, training loss: 0.6928528547286987 = 0.03256222605705261 + 0.1 * 6.602906703948975
Epoch 550, val loss: 0.8913277387619019
Epoch 560, training loss: 0.6917238831520081 = 0.030443301424384117 + 0.1 * 6.6128058433532715
Epoch 560, val loss: 0.9003564715385437
Epoch 570, training loss: 0.6891701817512512 = 0.028526170179247856 + 0.1 * 6.606439590454102
Epoch 570, val loss: 0.909231424331665
Epoch 580, training loss: 0.686436653137207 = 0.026785094290971756 + 0.1 * 6.596515655517578
Epoch 580, val loss: 0.9176885485649109
Epoch 590, training loss: 0.6843408942222595 = 0.025197716429829597 + 0.1 * 6.591431617736816
Epoch 590, val loss: 0.9261506795883179
Epoch 600, training loss: 0.6824796199798584 = 0.023749688640236855 + 0.1 * 6.58729887008667
Epoch 600, val loss: 0.9341400861740112
Epoch 610, training loss: 0.6812648773193359 = 0.022422654554247856 + 0.1 * 6.588421821594238
Epoch 610, val loss: 0.9420860409736633
Epoch 620, training loss: 0.6790361404418945 = 0.021203922107815742 + 0.1 * 6.578321933746338
Epoch 620, val loss: 0.9498288631439209
Epoch 630, training loss: 0.6782724857330322 = 0.020085608586668968 + 0.1 * 6.5818681716918945
Epoch 630, val loss: 0.9574117064476013
Epoch 640, training loss: 0.6765502095222473 = 0.01905895210802555 + 0.1 * 6.5749125480651855
Epoch 640, val loss: 0.9645745158195496
Epoch 650, training loss: 0.6754613518714905 = 0.018111584708094597 + 0.1 * 6.573497295379639
Epoch 650, val loss: 0.9715744853019714
Epoch 660, training loss: 0.6732255816459656 = 0.01723378337919712 + 0.1 * 6.55991792678833
Epoch 660, val loss: 0.9785486459732056
Epoch 670, training loss: 0.6730190515518188 = 0.016420844942331314 + 0.1 * 6.565981864929199
Epoch 670, val loss: 0.9852631688117981
Epoch 680, training loss: 0.6714233160018921 = 0.015666170045733452 + 0.1 * 6.557570934295654
Epoch 680, val loss: 0.9918514490127563
Epoch 690, training loss: 0.670257031917572 = 0.014964806847274303 + 0.1 * 6.552921772003174
Epoch 690, val loss: 0.9981825351715088
Epoch 700, training loss: 0.6690894365310669 = 0.014310689643025398 + 0.1 * 6.547787189483643
Epoch 700, val loss: 1.0045547485351562
Epoch 710, training loss: 0.6694818139076233 = 0.01370193064212799 + 0.1 * 6.557798862457275
Epoch 710, val loss: 1.0106343030929565
Epoch 720, training loss: 0.6670637726783752 = 0.013134269043803215 + 0.1 * 6.539295196533203
Epoch 720, val loss: 1.0165284872055054
Epoch 730, training loss: 0.6671283841133118 = 0.012601970694959164 + 0.1 * 6.545263767242432
Epoch 730, val loss: 1.0223501920700073
Epoch 740, training loss: 0.6656479835510254 = 0.012102663516998291 + 0.1 * 6.5354533195495605
Epoch 740, val loss: 1.0280946493148804
Epoch 750, training loss: 0.6653578877449036 = 0.01163413841277361 + 0.1 * 6.537237644195557
Epoch 750, val loss: 1.0336782932281494
Epoch 760, training loss: 0.6673826575279236 = 0.011194108985364437 + 0.1 * 6.561885356903076
Epoch 760, val loss: 1.0392578840255737
Epoch 770, training loss: 0.664324939250946 = 0.010782082565128803 + 0.1 * 6.535428524017334
Epoch 770, val loss: 1.0445433855056763
Epoch 780, training loss: 0.6636444330215454 = 0.010394508019089699 + 0.1 * 6.532498836517334
Epoch 780, val loss: 1.0495675802230835
Epoch 790, training loss: 0.6628865599632263 = 0.010027394630014896 + 0.1 * 6.528591156005859
Epoch 790, val loss: 1.0547125339508057
Epoch 800, training loss: 0.6618718504905701 = 0.009680723771452904 + 0.1 * 6.521911144256592
Epoch 800, val loss: 1.0598700046539307
Epoch 810, training loss: 0.6633639335632324 = 0.009353519417345524 + 0.1 * 6.540103912353516
Epoch 810, val loss: 1.0647685527801514
Epoch 820, training loss: 0.6614757180213928 = 0.009043814614415169 + 0.1 * 6.524318695068359
Epoch 820, val loss: 1.0696351528167725
Epoch 830, training loss: 0.6617826819419861 = 0.008751222863793373 + 0.1 * 6.5303144454956055
Epoch 830, val loss: 1.0743874311447144
Epoch 840, training loss: 0.6600332856178284 = 0.008474123664200306 + 0.1 * 6.515591621398926
Epoch 840, val loss: 1.0789588689804077
Epoch 850, training loss: 0.6605583429336548 = 0.008210868574678898 + 0.1 * 6.523474216461182
Epoch 850, val loss: 1.083489179611206
Epoch 860, training loss: 0.6588358879089355 = 0.007960613816976547 + 0.1 * 6.508752822875977
Epoch 860, val loss: 1.0880177021026611
Epoch 870, training loss: 0.6591448783874512 = 0.007723358925431967 + 0.1 * 6.514215469360352
Epoch 870, val loss: 1.0923327207565308
Epoch 880, training loss: 0.6578390598297119 = 0.007496972102671862 + 0.1 * 6.503420829772949
Epoch 880, val loss: 1.096604585647583
Epoch 890, training loss: 0.6583357453346252 = 0.007281412370502949 + 0.1 * 6.510542869567871
Epoch 890, val loss: 1.1008517742156982
Epoch 900, training loss: 0.6591870188713074 = 0.007076256908476353 + 0.1 * 6.5211076736450195
Epoch 900, val loss: 1.1050647497177124
Epoch 910, training loss: 0.6565810441970825 = 0.006880928296595812 + 0.1 * 6.4970011711120605
Epoch 910, val loss: 1.1091245412826538
Epoch 920, training loss: 0.656275749206543 = 0.006694782059639692 + 0.1 * 6.495809555053711
Epoch 920, val loss: 1.1129560470581055
Epoch 930, training loss: 0.6571890711784363 = 0.006516290828585625 + 0.1 * 6.506727695465088
Epoch 930, val loss: 1.1168389320373535
Epoch 940, training loss: 0.655546247959137 = 0.0063458760268986225 + 0.1 * 6.492003440856934
Epoch 940, val loss: 1.1207926273345947
Epoch 950, training loss: 0.6549113988876343 = 0.0061831227503716946 + 0.1 * 6.4872822761535645
Epoch 950, val loss: 1.1244159936904907
Epoch 960, training loss: 0.655983030796051 = 0.006026916671544313 + 0.1 * 6.499560832977295
Epoch 960, val loss: 1.1280392408370972
Epoch 970, training loss: 0.6545157432556152 = 0.005877432879060507 + 0.1 * 6.486382961273193
Epoch 970, val loss: 1.1316759586334229
Epoch 980, training loss: 0.6546581983566284 = 0.00573418103158474 + 0.1 * 6.489239692687988
Epoch 980, val loss: 1.1351875066757202
Epoch 990, training loss: 0.654332160949707 = 0.005596756935119629 + 0.1 * 6.487353801727295
Epoch 990, val loss: 1.138695478439331
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 2.8078079223632812 = 1.9481266736984253 + 0.1 * 8.59681224822998
Epoch 0, val loss: 1.9537363052368164
Epoch 10, training loss: 2.7975916862487793 = 1.9379233121871948 + 0.1 * 8.59668254852295
Epoch 10, val loss: 1.9433003664016724
Epoch 20, training loss: 2.7849862575531006 = 1.925398588180542 + 0.1 * 8.595876693725586
Epoch 20, val loss: 1.9303127527236938
Epoch 30, training loss: 2.7666573524475098 = 1.9078161716461182 + 0.1 * 8.588412284851074
Epoch 30, val loss: 1.9120430946350098
Epoch 40, training loss: 2.734227180480957 = 1.8817600011825562 + 0.1 * 8.524672508239746
Epoch 40, val loss: 1.8853002786636353
Epoch 50, training loss: 2.662811517715454 = 1.8468005657196045 + 0.1 * 8.160109519958496
Epoch 50, val loss: 1.8507146835327148
Epoch 60, training loss: 2.5950427055358887 = 1.8099091053009033 + 0.1 * 7.851334571838379
Epoch 60, val loss: 1.816010594367981
Epoch 70, training loss: 2.5162181854248047 = 1.7752759456634521 + 0.1 * 7.409421920776367
Epoch 70, val loss: 1.7840579748153687
Epoch 80, training loss: 2.454857587814331 = 1.7394071817398071 + 0.1 * 7.154504299163818
Epoch 80, val loss: 1.7509702444076538
Epoch 90, training loss: 2.398221492767334 = 1.6941813230514526 + 0.1 * 7.040401935577393
Epoch 90, val loss: 1.7103877067565918
Epoch 100, training loss: 2.33146595954895 = 1.6341650485992432 + 0.1 * 6.973008155822754
Epoch 100, val loss: 1.6567623615264893
Epoch 110, training loss: 2.2523670196533203 = 1.5593022108078003 + 0.1 * 6.9306488037109375
Epoch 110, val loss: 1.5913161039352417
Epoch 120, training loss: 2.1648688316345215 = 1.474578857421875 + 0.1 * 6.90289831161499
Epoch 120, val loss: 1.5198901891708374
Epoch 130, training loss: 2.0759828090667725 = 1.387850284576416 + 0.1 * 6.881324291229248
Epoch 130, val loss: 1.4496378898620605
Epoch 140, training loss: 1.9878942966461182 = 1.3017864227294922 + 0.1 * 6.861078262329102
Epoch 140, val loss: 1.3808289766311646
Epoch 150, training loss: 1.8993046283721924 = 1.2151763439178467 + 0.1 * 6.841281890869141
Epoch 150, val loss: 1.3116695880889893
Epoch 160, training loss: 1.8120872974395752 = 1.129960536956787 + 0.1 * 6.821267127990723
Epoch 160, val loss: 1.2443182468414307
Epoch 170, training loss: 1.7289760112762451 = 1.0492616891860962 + 0.1 * 6.797142505645752
Epoch 170, val loss: 1.181207537651062
Epoch 180, training loss: 1.653836965560913 = 0.9758116006851196 + 0.1 * 6.780252933502197
Epoch 180, val loss: 1.1250797510147095
Epoch 190, training loss: 1.585568904876709 = 0.9093385338783264 + 0.1 * 6.762303352355957
Epoch 190, val loss: 1.0750644207000732
Epoch 200, training loss: 1.522193431854248 = 0.8472203612327576 + 0.1 * 6.749731063842773
Epoch 200, val loss: 1.0283325910568237
Epoch 210, training loss: 1.4617209434509277 = 0.7882334589958191 + 0.1 * 6.734875202178955
Epoch 210, val loss: 0.9837191104888916
Epoch 220, training loss: 1.4022536277770996 = 0.7302871346473694 + 0.1 * 6.719665050506592
Epoch 220, val loss: 0.9393229484558105
Epoch 230, training loss: 1.346235990524292 = 0.6736291646957397 + 0.1 * 6.726067543029785
Epoch 230, val loss: 0.8963689804077148
Epoch 240, training loss: 1.2896146774291992 = 0.6195158958435059 + 0.1 * 6.700987815856934
Epoch 240, val loss: 0.8563265800476074
Epoch 250, training loss: 1.2363008260726929 = 0.5677057504653931 + 0.1 * 6.685950756072998
Epoch 250, val loss: 0.819175660610199
Epoch 260, training loss: 1.1873970031738281 = 0.5186759233474731 + 0.1 * 6.687210559844971
Epoch 260, val loss: 0.7860581278800964
Epoch 270, training loss: 1.1394786834716797 = 0.4726612865924835 + 0.1 * 6.668173313140869
Epoch 270, val loss: 0.7574302554130554
Epoch 280, training loss: 1.0963530540466309 = 0.42935481667518616 + 0.1 * 6.669982433319092
Epoch 280, val loss: 0.7331421971321106
Epoch 290, training loss: 1.055111289024353 = 0.3890363574028015 + 0.1 * 6.6607489585876465
Epoch 290, val loss: 0.7133062481880188
Epoch 300, training loss: 1.0158815383911133 = 0.35110998153686523 + 0.1 * 6.647714614868164
Epoch 300, val loss: 0.6968692541122437
Epoch 310, training loss: 0.9810678362846375 = 0.3156999945640564 + 0.1 * 6.6536784172058105
Epoch 310, val loss: 0.6836124062538147
Epoch 320, training loss: 0.9466404914855957 = 0.2829890549182892 + 0.1 * 6.636513710021973
Epoch 320, val loss: 0.6731911301612854
Epoch 330, training loss: 0.9175493717193604 = 0.2528285086154938 + 0.1 * 6.647208213806152
Epoch 330, val loss: 0.6653491854667664
Epoch 340, training loss: 0.888180673122406 = 0.22543372213840485 + 0.1 * 6.627469539642334
Epoch 340, val loss: 0.6599859595298767
Epoch 350, training loss: 0.8623183965682983 = 0.20073087513446808 + 0.1 * 6.615875244140625
Epoch 350, val loss: 0.6570026278495789
Epoch 360, training loss: 0.8404965996742249 = 0.17858661711215973 + 0.1 * 6.6190996170043945
Epoch 360, val loss: 0.6561709642410278
Epoch 370, training loss: 0.8206984400749207 = 0.15893493592739105 + 0.1 * 6.6176347732543945
Epoch 370, val loss: 0.6573377847671509
Epoch 380, training loss: 0.8018211126327515 = 0.14157739281654358 + 0.1 * 6.6024370193481445
Epoch 380, val loss: 0.6601694822311401
Epoch 390, training loss: 0.7858467102050781 = 0.12634798884391785 + 0.1 * 6.594987392425537
Epoch 390, val loss: 0.6645252108573914
Epoch 400, training loss: 0.7717872858047485 = 0.11304842680692673 + 0.1 * 6.587388515472412
Epoch 400, val loss: 0.6700215339660645
Epoch 410, training loss: 0.7611828446388245 = 0.10143310576677322 + 0.1 * 6.597497463226318
Epoch 410, val loss: 0.6764887571334839
Epoch 420, training loss: 0.7490243911743164 = 0.09133626520633698 + 0.1 * 6.576881408691406
Epoch 420, val loss: 0.6836727261543274
Epoch 430, training loss: 0.7391245365142822 = 0.08252723515033722 + 0.1 * 6.565973281860352
Epoch 430, val loss: 0.6913869380950928
Epoch 440, training loss: 0.7337487936019897 = 0.07482526451349258 + 0.1 * 6.589234828948975
Epoch 440, val loss: 0.6995199918746948
Epoch 450, training loss: 0.725006103515625 = 0.06812170892953873 + 0.1 * 6.568843364715576
Epoch 450, val loss: 0.7078707218170166
Epoch 460, training loss: 0.7185382843017578 = 0.06224929541349411 + 0.1 * 6.562889575958252
Epoch 460, val loss: 0.7163045406341553
Epoch 470, training loss: 0.7119380235671997 = 0.05708641931414604 + 0.1 * 6.548516273498535
Epoch 470, val loss: 0.7248249650001526
Epoch 480, training loss: 0.708318293094635 = 0.05251027271151543 + 0.1 * 6.558080196380615
Epoch 480, val loss: 0.7333489060401917
Epoch 490, training loss: 0.7031334042549133 = 0.04846922308206558 + 0.1 * 6.5466413497924805
Epoch 490, val loss: 0.7418344616889954
Epoch 500, training loss: 0.6983553171157837 = 0.044874295592308044 + 0.1 * 6.5348100662231445
Epoch 500, val loss: 0.7502467632293701
Epoch 510, training loss: 0.6956338286399841 = 0.04165194556117058 + 0.1 * 6.539818286895752
Epoch 510, val loss: 0.7585529685020447
Epoch 520, training loss: 0.6915656924247742 = 0.03876369446516037 + 0.1 * 6.528019905090332
Epoch 520, val loss: 0.7668024301528931
Epoch 530, training loss: 0.689083456993103 = 0.036161601543426514 + 0.1 * 6.529218673706055
Epoch 530, val loss: 0.7749044299125671
Epoch 540, training loss: 0.686784029006958 = 0.0338127426803112 + 0.1 * 6.529712677001953
Epoch 540, val loss: 0.7828478217124939
Epoch 550, training loss: 0.6834774017333984 = 0.031689468771219254 + 0.1 * 6.517879486083984
Epoch 550, val loss: 0.7906755208969116
Epoch 560, training loss: 0.6804080009460449 = 0.02976422756910324 + 0.1 * 6.5064377784729
Epoch 560, val loss: 0.7983296513557434
Epoch 570, training loss: 0.6798297762870789 = 0.02800763212144375 + 0.1 * 6.518221378326416
Epoch 570, val loss: 0.8058335781097412
Epoch 580, training loss: 0.6761782169342041 = 0.026407530531287193 + 0.1 * 6.497706413269043
Epoch 580, val loss: 0.8131774663925171
Epoch 590, training loss: 0.6747183799743652 = 0.024943644180893898 + 0.1 * 6.49774694442749
Epoch 590, val loss: 0.8203855156898499
Epoch 600, training loss: 0.6731520295143127 = 0.023599332198500633 + 0.1 * 6.4955267906188965
Epoch 600, val loss: 0.8273986577987671
Epoch 610, training loss: 0.6716453433036804 = 0.02236475981771946 + 0.1 * 6.4928059577941895
Epoch 610, val loss: 0.8342911601066589
Epoch 620, training loss: 0.6695624589920044 = 0.021229205653071404 + 0.1 * 6.48333215713501
Epoch 620, val loss: 0.8410305976867676
Epoch 630, training loss: 0.6689978837966919 = 0.02017783559858799 + 0.1 * 6.4882001876831055
Epoch 630, val loss: 0.8476147651672363
Epoch 640, training loss: 0.6679523587226868 = 0.0192071832716465 + 0.1 * 6.487451553344727
Epoch 640, val loss: 0.8540157079696655
Epoch 650, training loss: 0.666012704372406 = 0.018309561535716057 + 0.1 * 6.477031707763672
Epoch 650, val loss: 0.8603569269180298
Epoch 660, training loss: 0.6667943596839905 = 0.017474770545959473 + 0.1 * 6.493195533752441
Epoch 660, val loss: 0.8664808869361877
Epoch 670, training loss: 0.6637081503868103 = 0.016699308529496193 + 0.1 * 6.470088481903076
Epoch 670, val loss: 0.87251216173172
Epoch 680, training loss: 0.6635938882827759 = 0.015975289046764374 + 0.1 * 6.4761857986450195
Epoch 680, val loss: 0.8784353137016296
Epoch 690, training loss: 0.662560760974884 = 0.015300407074391842 + 0.1 * 6.4726033210754395
Epoch 690, val loss: 0.884160578250885
Epoch 700, training loss: 0.6618197560310364 = 0.014670928940176964 + 0.1 * 6.4714884757995605
Epoch 700, val loss: 0.8898476362228394
Epoch 710, training loss: 0.6603907942771912 = 0.01408172957599163 + 0.1 * 6.463090419769287
Epoch 710, val loss: 0.8953378200531006
Epoch 720, training loss: 0.6593164801597595 = 0.013530189171433449 + 0.1 * 6.457862854003906
Epoch 720, val loss: 0.9007830619812012
Epoch 730, training loss: 0.6616580486297607 = 0.013010791502892971 + 0.1 * 6.486472129821777
Epoch 730, val loss: 0.9060856103897095
Epoch 740, training loss: 0.6577449440956116 = 0.012524116784334183 + 0.1 * 6.452208042144775
Epoch 740, val loss: 0.9112879633903503
Epoch 750, training loss: 0.6573835015296936 = 0.012065872550010681 + 0.1 * 6.453176021575928
Epoch 750, val loss: 0.9164425134658813
Epoch 760, training loss: 0.6573261022567749 = 0.011633533053100109 + 0.1 * 6.456925868988037
Epoch 760, val loss: 0.9214048385620117
Epoch 770, training loss: 0.6553246378898621 = 0.01122689712792635 + 0.1 * 6.440977573394775
Epoch 770, val loss: 0.926335871219635
Epoch 780, training loss: 0.6572742462158203 = 0.010842479765415192 + 0.1 * 6.464317321777344
Epoch 780, val loss: 0.9311994314193726
Epoch 790, training loss: 0.6549272537231445 = 0.010478283278644085 + 0.1 * 6.4444899559021
Epoch 790, val loss: 0.9358812570571899
Epoch 800, training loss: 0.6543794274330139 = 0.01013450138270855 + 0.1 * 6.44244909286499
Epoch 800, val loss: 0.940577507019043
Epoch 810, training loss: 0.6539480686187744 = 0.009807695634663105 + 0.1 * 6.441403388977051
Epoch 810, val loss: 0.9451178312301636
Epoch 820, training loss: 0.6550930738449097 = 0.009498641826212406 + 0.1 * 6.455944061279297
Epoch 820, val loss: 0.9495659470558167
Epoch 830, training loss: 0.6527490615844727 = 0.00920531339943409 + 0.1 * 6.435437202453613
Epoch 830, val loss: 0.9539560079574585
Epoch 840, training loss: 0.6532749533653259 = 0.008926860988140106 + 0.1 * 6.443480968475342
Epoch 840, val loss: 0.9582247138023376
Epoch 850, training loss: 0.6516999006271362 = 0.00866229459643364 + 0.1 * 6.430376052856445
Epoch 850, val loss: 0.9624476432800293
Epoch 860, training loss: 0.6508015394210815 = 0.008410096168518066 + 0.1 * 6.423914432525635
Epoch 860, val loss: 0.9666111469268799
Epoch 870, training loss: 0.6528899669647217 = 0.008169293403625488 + 0.1 * 6.447206497192383
Epoch 870, val loss: 0.9706617593765259
Epoch 880, training loss: 0.6511295437812805 = 0.0079401396214962 + 0.1 * 6.431894302368164
Epoch 880, val loss: 0.9746479392051697
Epoch 890, training loss: 0.6522436141967773 = 0.007722199894487858 + 0.1 * 6.44521427154541
Epoch 890, val loss: 0.9786220788955688
Epoch 900, training loss: 0.6498506665229797 = 0.007513668388128281 + 0.1 * 6.423369884490967
Epoch 900, val loss: 0.9824451804161072
Epoch 910, training loss: 0.6499145030975342 = 0.007314568385481834 + 0.1 * 6.425999164581299
Epoch 910, val loss: 0.9862731695175171
Epoch 920, training loss: 0.6485937237739563 = 0.007123833987861872 + 0.1 * 6.414698600769043
Epoch 920, val loss: 0.9899578094482422
Epoch 930, training loss: 0.6482734084129333 = 0.006941568572074175 + 0.1 * 6.413318157196045
Epoch 930, val loss: 0.9936648607254028
Epoch 940, training loss: 0.6485897898674011 = 0.006766651291400194 + 0.1 * 6.418231010437012
Epoch 940, val loss: 0.9972788691520691
Epoch 950, training loss: 0.6487237215042114 = 0.00659936061128974 + 0.1 * 6.421243190765381
Epoch 950, val loss: 1.0007916688919067
Epoch 960, training loss: 0.6469557285308838 = 0.0064393007196486 + 0.1 * 6.405163764953613
Epoch 960, val loss: 1.0043284893035889
Epoch 970, training loss: 0.6494223475456238 = 0.006285236682742834 + 0.1 * 6.431370735168457
Epoch 970, val loss: 1.0077805519104004
Epoch 980, training loss: 0.6473033428192139 = 0.006137423682957888 + 0.1 * 6.411659240722656
Epoch 980, val loss: 1.0111302137374878
Epoch 990, training loss: 0.649113655090332 = 0.005995563697069883 + 0.1 * 6.431180477142334
Epoch 990, val loss: 1.0144736766815186
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.786721706390381 = 1.9270401000976562 + 0.1 * 8.596816062927246
Epoch 0, val loss: 1.9264541864395142
Epoch 10, training loss: 2.777294397354126 = 1.9176214933395386 + 0.1 * 8.596729278564453
Epoch 10, val loss: 1.9176554679870605
Epoch 20, training loss: 2.76560115814209 = 1.9059864282608032 + 0.1 * 8.596148490905762
Epoch 20, val loss: 1.9063842296600342
Epoch 30, training loss: 2.7486777305603027 = 1.8895527124404907 + 0.1 * 8.591251373291016
Epoch 30, val loss: 1.8902668952941895
Epoch 40, training loss: 2.7212905883789062 = 1.8654630184173584 + 0.1 * 8.55827522277832
Epoch 40, val loss: 1.8669211864471436
Epoch 50, training loss: 2.673447608947754 = 1.833012342453003 + 0.1 * 8.404351234436035
Epoch 50, val loss: 1.8370269536972046
Epoch 60, training loss: 2.6128103733062744 = 1.7962759733200073 + 0.1 * 8.165343284606934
Epoch 60, val loss: 1.804967999458313
Epoch 70, training loss: 2.5431571006774902 = 1.7562720775604248 + 0.1 * 7.868851184844971
Epoch 70, val loss: 1.7690590620040894
Epoch 80, training loss: 2.4579617977142334 = 1.7083101272583008 + 0.1 * 7.496517181396484
Epoch 80, val loss: 1.7244330644607544
Epoch 90, training loss: 2.377683639526367 = 1.6487988233566284 + 0.1 * 7.28884744644165
Epoch 90, val loss: 1.669826626777649
Epoch 100, training loss: 2.2890782356262207 = 1.573896050453186 + 0.1 * 7.151820659637451
Epoch 100, val loss: 1.602645754814148
Epoch 110, training loss: 2.192450761795044 = 1.4870755672454834 + 0.1 * 7.053750991821289
Epoch 110, val loss: 1.529166579246521
Epoch 120, training loss: 2.09299635887146 = 1.392764687538147 + 0.1 * 7.002315998077393
Epoch 120, val loss: 1.451517105102539
Epoch 130, training loss: 1.9917417764663696 = 1.294235348701477 + 0.1 * 6.975064277648926
Epoch 130, val loss: 1.3724550008773804
Epoch 140, training loss: 1.889897108078003 = 1.1934975385665894 + 0.1 * 6.963995456695557
Epoch 140, val loss: 1.292981743812561
Epoch 150, training loss: 1.792085886001587 = 1.0975958108901978 + 0.1 * 6.944901466369629
Epoch 150, val loss: 1.2183237075805664
Epoch 160, training loss: 1.7011146545410156 = 1.007914423942566 + 0.1 * 6.932002544403076
Epoch 160, val loss: 1.1493667364120483
Epoch 170, training loss: 1.6165164709091187 = 0.9248337745666504 + 0.1 * 6.9168267250061035
Epoch 170, val loss: 1.0858310461044312
Epoch 180, training loss: 1.539602279663086 = 0.848143994808197 + 0.1 * 6.914583206176758
Epoch 180, val loss: 1.0280348062515259
Epoch 190, training loss: 1.4684937000274658 = 0.7792379260063171 + 0.1 * 6.892557621002197
Epoch 190, val loss: 0.977699339389801
Epoch 200, training loss: 1.4049184322357178 = 0.7170522809028625 + 0.1 * 6.878660678863525
Epoch 200, val loss: 0.9347097277641296
Epoch 210, training loss: 1.3475767374038696 = 0.6607353687286377 + 0.1 * 6.86841344833374
Epoch 210, val loss: 0.899398148059845
Epoch 220, training loss: 1.2966814041137695 = 0.6097509860992432 + 0.1 * 6.8693037033081055
Epoch 220, val loss: 0.8714033365249634
Epoch 230, training loss: 1.2497862577438354 = 0.5641046762466431 + 0.1 * 6.856815814971924
Epoch 230, val loss: 0.8501151204109192
Epoch 240, training loss: 1.207362413406372 = 0.5225460529327393 + 0.1 * 6.848164081573486
Epoch 240, val loss: 0.8339948654174805
Epoch 250, training loss: 1.1683273315429688 = 0.48425906896591187 + 0.1 * 6.8406829833984375
Epoch 250, val loss: 0.8219221234321594
Epoch 260, training loss: 1.1321918964385986 = 0.4486733675003052 + 0.1 * 6.835186004638672
Epoch 260, val loss: 0.8130461573600769
Epoch 270, training loss: 1.0989290475845337 = 0.415503591299057 + 0.1 * 6.834254741668701
Epoch 270, val loss: 0.8068393468856812
Epoch 280, training loss: 1.0668097734451294 = 0.3840946555137634 + 0.1 * 6.827151298522949
Epoch 280, val loss: 0.8030214309692383
Epoch 290, training loss: 1.036420226097107 = 0.35425204038619995 + 0.1 * 6.821681499481201
Epoch 290, val loss: 0.8014957904815674
Epoch 300, training loss: 1.0075644254684448 = 0.32594284415245056 + 0.1 * 6.816215515136719
Epoch 300, val loss: 0.8022786378860474
Epoch 310, training loss: 0.9810417294502258 = 0.2992182970046997 + 0.1 * 6.818233966827393
Epoch 310, val loss: 0.8051394820213318
Epoch 320, training loss: 0.9547488689422607 = 0.2741546034812927 + 0.1 * 6.805942535400391
Epoch 320, val loss: 0.8097776174545288
Epoch 330, training loss: 0.9311327934265137 = 0.2506272792816162 + 0.1 * 6.805055141448975
Epoch 330, val loss: 0.8161406517028809
Epoch 340, training loss: 0.9081893563270569 = 0.22858679294586182 + 0.1 * 6.796025276184082
Epoch 340, val loss: 0.8240352272987366
Epoch 350, training loss: 0.8874361515045166 = 0.20800739526748657 + 0.1 * 6.794287204742432
Epoch 350, val loss: 0.8331456780433655
Epoch 360, training loss: 0.8673790097236633 = 0.18891394138336182 + 0.1 * 6.7846503257751465
Epoch 360, val loss: 0.8434378504753113
Epoch 370, training loss: 0.8488104343414307 = 0.1713401824235916 + 0.1 * 6.774702548980713
Epoch 370, val loss: 0.8546055555343628
Epoch 380, training loss: 0.8334176540374756 = 0.15525668859481812 + 0.1 * 6.781609535217285
Epoch 380, val loss: 0.8665608763694763
Epoch 390, training loss: 0.8175693154335022 = 0.1407204270362854 + 0.1 * 6.768488883972168
Epoch 390, val loss: 0.8790256977081299
Epoch 400, training loss: 0.8029652833938599 = 0.12763163447380066 + 0.1 * 6.753336429595947
Epoch 400, val loss: 0.8919733762741089
Epoch 410, training loss: 0.7905715107917786 = 0.11584728956222534 + 0.1 * 6.747241973876953
Epoch 410, val loss: 0.9054552912712097
Epoch 420, training loss: 0.7798067927360535 = 0.10529377311468124 + 0.1 * 6.74513053894043
Epoch 420, val loss: 0.9192091822624207
Epoch 430, training loss: 0.7698696851730347 = 0.09590686112642288 + 0.1 * 6.739628314971924
Epoch 430, val loss: 0.9331623315811157
Epoch 440, training loss: 0.759697675704956 = 0.0875355452299118 + 0.1 * 6.721621036529541
Epoch 440, val loss: 0.9471584558486938
Epoch 450, training loss: 0.7516229748725891 = 0.08001384884119034 + 0.1 * 6.716090679168701
Epoch 450, val loss: 0.9613242745399475
Epoch 460, training loss: 0.7441101670265198 = 0.07325788587331772 + 0.1 * 6.708522796630859
Epoch 460, val loss: 0.9755935072898865
Epoch 470, training loss: 0.737598717212677 = 0.06720484048128128 + 0.1 * 6.7039384841918945
Epoch 470, val loss: 0.9896579384803772
Epoch 480, training loss: 0.7315076589584351 = 0.061754774302244186 + 0.1 * 6.697528839111328
Epoch 480, val loss: 1.003552794456482
Epoch 490, training loss: 0.7262097001075745 = 0.05685095116496086 + 0.1 * 6.693587303161621
Epoch 490, val loss: 1.0173747539520264
Epoch 500, training loss: 0.7219089269638062 = 0.05243389308452606 + 0.1 * 6.6947503089904785
Epoch 500, val loss: 1.0308337211608887
Epoch 510, training loss: 0.7159700393676758 = 0.04845452681183815 + 0.1 * 6.675155162811279
Epoch 510, val loss: 1.0439125299453735
Epoch 520, training loss: 0.7120587229728699 = 0.044855520129203796 + 0.1 * 6.672031879425049
Epoch 520, val loss: 1.0568965673446655
Epoch 530, training loss: 0.7097597122192383 = 0.04159316420555115 + 0.1 * 6.681664943695068
Epoch 530, val loss: 1.069562554359436
Epoch 540, training loss: 0.704779326915741 = 0.038647715002298355 + 0.1 * 6.66131591796875
Epoch 540, val loss: 1.08171808719635
Epoch 550, training loss: 0.7019544839859009 = 0.03597542271018028 + 0.1 * 6.659790992736816
Epoch 550, val loss: 1.093740701675415
Epoch 560, training loss: 0.6988572478294373 = 0.033552445471286774 + 0.1 * 6.653047561645508
Epoch 560, val loss: 1.105474591255188
Epoch 570, training loss: 0.6947467923164368 = 0.03135407716035843 + 0.1 * 6.633927345275879
Epoch 570, val loss: 1.116542935371399
Epoch 580, training loss: 0.6931837201118469 = 0.029351921752095222 + 0.1 * 6.638317584991455
Epoch 580, val loss: 1.1276664733886719
Epoch 590, training loss: 0.691461443901062 = 0.027529465034604073 + 0.1 * 6.63931941986084
Epoch 590, val loss: 1.1382020711898804
Epoch 600, training loss: 0.6885404586791992 = 0.025867022573947906 + 0.1 * 6.626734256744385
Epoch 600, val loss: 1.1485908031463623
Epoch 610, training loss: 0.6870325207710266 = 0.02434607595205307 + 0.1 * 6.626864433288574
Epoch 610, val loss: 1.158596396446228
Epoch 620, training loss: 0.6849210262298584 = 0.022954769432544708 + 0.1 * 6.619662761688232
Epoch 620, val loss: 1.1685736179351807
Epoch 630, training loss: 0.6836580634117126 = 0.021680912002921104 + 0.1 * 6.619771480560303
Epoch 630, val loss: 1.1778509616851807
Epoch 640, training loss: 0.6810666918754578 = 0.020511837676167488 + 0.1 * 6.605547904968262
Epoch 640, val loss: 1.1870903968811035
Epoch 650, training loss: 0.6788198947906494 = 0.019436005502939224 + 0.1 * 6.593838691711426
Epoch 650, val loss: 1.1958836317062378
Epoch 660, training loss: 0.6785114407539368 = 0.01844325289130211 + 0.1 * 6.600681781768799
Epoch 660, val loss: 1.2049351930618286
Epoch 670, training loss: 0.6771128177642822 = 0.017530260607600212 + 0.1 * 6.595825672149658
Epoch 670, val loss: 1.2130285501480103
Epoch 680, training loss: 0.6758624315261841 = 0.016686351969838142 + 0.1 * 6.591760635375977
Epoch 680, val loss: 1.221360206604004
Epoch 690, training loss: 0.6744096875190735 = 0.015905434265732765 + 0.1 * 6.585042476654053
Epoch 690, val loss: 1.2292879819869995
Epoch 700, training loss: 0.6723459362983704 = 0.015181419439613819 + 0.1 * 6.571645259857178
Epoch 700, val loss: 1.2369393110275269
Epoch 710, training loss: 0.6721230745315552 = 0.014507667161524296 + 0.1 * 6.5761542320251465
Epoch 710, val loss: 1.2447655200958252
Epoch 720, training loss: 0.6702225208282471 = 0.01388224121183157 + 0.1 * 6.5634026527404785
Epoch 720, val loss: 1.2518868446350098
Epoch 730, training loss: 0.6695539355278015 = 0.013298986479640007 + 0.1 * 6.562549591064453
Epoch 730, val loss: 1.259169340133667
Epoch 740, training loss: 0.6699579954147339 = 0.012754789553582668 + 0.1 * 6.5720319747924805
Epoch 740, val loss: 1.2660505771636963
Epoch 750, training loss: 0.6678022742271423 = 0.012246579863131046 + 0.1 * 6.555556297302246
Epoch 750, val loss: 1.2732295989990234
Epoch 760, training loss: 0.6672977209091187 = 0.011772500351071358 + 0.1 * 6.5552520751953125
Epoch 760, val loss: 1.2794560194015503
Epoch 770, training loss: 0.6663514375686646 = 0.011326722800731659 + 0.1 * 6.550246715545654
Epoch 770, val loss: 1.2861971855163574
Epoch 780, training loss: 0.6648869514465332 = 0.010908037424087524 + 0.1 * 6.539788722991943
Epoch 780, val loss: 1.2924190759658813
Epoch 790, training loss: 0.6647890210151672 = 0.010513897985219955 + 0.1 * 6.542750835418701
Epoch 790, val loss: 1.2989065647125244
Epoch 800, training loss: 0.6643308401107788 = 0.010143452323973179 + 0.1 * 6.541873931884766
Epoch 800, val loss: 1.3049821853637695
Epoch 810, training loss: 0.6630337834358215 = 0.00979494210332632 + 0.1 * 6.532388210296631
Epoch 810, val loss: 1.3109323978424072
Epoch 820, training loss: 0.6621997356414795 = 0.009466932155191898 + 0.1 * 6.527327537536621
Epoch 820, val loss: 1.316612958908081
Epoch 830, training loss: 0.6625125408172607 = 0.009156300686299801 + 0.1 * 6.533562183380127
Epoch 830, val loss: 1.3223892450332642
Epoch 840, training loss: 0.6616837382316589 = 0.008862395770847797 + 0.1 * 6.5282135009765625
Epoch 840, val loss: 1.327912449836731
Epoch 850, training loss: 0.6618788242340088 = 0.00858398713171482 + 0.1 * 6.5329484939575195
Epoch 850, val loss: 1.3335044384002686
Epoch 860, training loss: 0.6624316573143005 = 0.008320041932165623 + 0.1 * 6.541116237640381
Epoch 860, val loss: 1.338896632194519
Epoch 870, training loss: 0.6592251658439636 = 0.008069965057075024 + 0.1 * 6.511551380157471
Epoch 870, val loss: 1.3442339897155762
Epoch 880, training loss: 0.6591692566871643 = 0.007833249866962433 + 0.1 * 6.513360023498535
Epoch 880, val loss: 1.3491517305374146
Epoch 890, training loss: 0.6581162214279175 = 0.007607311941683292 + 0.1 * 6.505088806152344
Epoch 890, val loss: 1.3542652130126953
Epoch 900, training loss: 0.6576910614967346 = 0.007392138708382845 + 0.1 * 6.502988815307617
Epoch 900, val loss: 1.3594805002212524
Epoch 910, training loss: 0.6571468114852905 = 0.007187807932496071 + 0.1 * 6.4995903968811035
Epoch 910, val loss: 1.3641999959945679
Epoch 920, training loss: 0.65812087059021 = 0.006992613896727562 + 0.1 * 6.511282444000244
Epoch 920, val loss: 1.3691551685333252
Epoch 930, training loss: 0.6572810411453247 = 0.0068068839609622955 + 0.1 * 6.504741191864014
Epoch 930, val loss: 1.3738843202590942
Epoch 940, training loss: 0.6570096611976624 = 0.006629651878029108 + 0.1 * 6.503799915313721
Epoch 940, val loss: 1.3783353567123413
Epoch 950, training loss: 0.6558587551116943 = 0.006460010539740324 + 0.1 * 6.493987560272217
Epoch 950, val loss: 1.3829890489578247
Epoch 960, training loss: 0.6555849313735962 = 0.006297742016613483 + 0.1 * 6.492871284484863
Epoch 960, val loss: 1.3874127864837646
Epoch 970, training loss: 0.6574218273162842 = 0.0061424486339092255 + 0.1 * 6.512794017791748
Epoch 970, val loss: 1.3918647766113281
Epoch 980, training loss: 0.6542761921882629 = 0.005993726663291454 + 0.1 * 6.482824802398682
Epoch 980, val loss: 1.396415114402771
Epoch 990, training loss: 0.6550635695457458 = 0.00585158308967948 + 0.1 * 6.492119789123535
Epoch 990, val loss: 1.400519609451294
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8350026357406432
The final CL Acc:0.78889, 0.00800, The final GNN Acc:0.83694, 0.00311
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9556])
updated graph: torch.Size([2, 10630])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8128273487091064 = 1.9531441926956177 + 0.1 * 8.596832275390625
Epoch 0, val loss: 1.9500004053115845
Epoch 10, training loss: 2.8030505180358887 = 1.9433780908584595 + 0.1 * 8.596724510192871
Epoch 10, val loss: 1.9397953748703003
Epoch 20, training loss: 2.790916919708252 = 1.9312992095947266 + 0.1 * 8.596176147460938
Epoch 20, val loss: 1.9272607564926147
Epoch 30, training loss: 2.773770332336426 = 1.9145663976669312 + 0.1 * 8.592040061950684
Epoch 30, val loss: 1.9097979068756104
Epoch 40, training loss: 2.7465298175811768 = 1.889949917793274 + 0.1 * 8.56579875946045
Epoch 40, val loss: 1.8841673135757446
Epoch 50, training loss: 2.6996612548828125 = 1.8556721210479736 + 0.1 * 8.439889907836914
Epoch 50, val loss: 1.8499653339385986
Epoch 60, training loss: 2.63478684425354 = 1.8170338869094849 + 0.1 * 8.177529335021973
Epoch 60, val loss: 1.814079999923706
Epoch 70, training loss: 2.562782049179077 = 1.7850714921951294 + 0.1 * 7.777105808258057
Epoch 70, val loss: 1.7859580516815186
Epoch 80, training loss: 2.4878427982330322 = 1.7553831338882446 + 0.1 * 7.324596405029297
Epoch 80, val loss: 1.759412169456482
Epoch 90, training loss: 2.4336562156677246 = 1.7190368175506592 + 0.1 * 7.146193027496338
Epoch 90, val loss: 1.7279037237167358
Epoch 100, training loss: 2.378828763961792 = 1.6699538230895996 + 0.1 * 7.088748931884766
Epoch 100, val loss: 1.6857267618179321
Epoch 110, training loss: 2.30886173248291 = 1.6047452688217163 + 0.1 * 7.041165351867676
Epoch 110, val loss: 1.62996244430542
Epoch 120, training loss: 2.2242729663848877 = 1.5243613719940186 + 0.1 * 6.999115467071533
Epoch 120, val loss: 1.562601923942566
Epoch 130, training loss: 2.129214286804199 = 1.4329386949539185 + 0.1 * 6.9627556800842285
Epoch 130, val loss: 1.4881547689437866
Epoch 140, training loss: 2.0294950008392334 = 1.3359657526016235 + 0.1 * 6.935292720794678
Epoch 140, val loss: 1.4104737043380737
Epoch 150, training loss: 1.9285166263580322 = 1.237073302268982 + 0.1 * 6.914432525634766
Epoch 150, val loss: 1.332434058189392
Epoch 160, training loss: 1.8294703960418701 = 1.1392419338226318 + 0.1 * 6.902284622192383
Epoch 160, val loss: 1.2570571899414062
Epoch 170, training loss: 1.7330660820007324 = 1.0442259311676025 + 0.1 * 6.888401031494141
Epoch 170, val loss: 1.184959053993225
Epoch 180, training loss: 1.641150951385498 = 0.9534263610839844 + 0.1 * 6.877245903015137
Epoch 180, val loss: 1.1163394451141357
Epoch 190, training loss: 1.5558278560638428 = 0.8691213130950928 + 0.1 * 6.8670654296875
Epoch 190, val loss: 1.0527642965316772
Epoch 200, training loss: 1.4779715538024902 = 0.7921497225761414 + 0.1 * 6.858218193054199
Epoch 200, val loss: 0.9952581524848938
Epoch 210, training loss: 1.4068093299865723 = 0.7224634885787964 + 0.1 * 6.843458652496338
Epoch 210, val loss: 0.944120466709137
Epoch 220, training loss: 1.3430769443511963 = 0.6598259210586548 + 0.1 * 6.832509517669678
Epoch 220, val loss: 0.9005702137947083
Epoch 230, training loss: 1.2850968837738037 = 0.6032044291496277 + 0.1 * 6.8189239501953125
Epoch 230, val loss: 0.8639146089553833
Epoch 240, training loss: 1.2322282791137695 = 0.5512262582778931 + 0.1 * 6.810020446777344
Epoch 240, val loss: 0.8335494995117188
Epoch 250, training loss: 1.1829688549041748 = 0.5034803152084351 + 0.1 * 6.794885635375977
Epoch 250, val loss: 0.808436393737793
Epoch 260, training loss: 1.1379783153533936 = 0.45932677388191223 + 0.1 * 6.786515712738037
Epoch 260, val loss: 0.7878870368003845
Epoch 270, training loss: 1.0962576866149902 = 0.41880685091018677 + 0.1 * 6.774508476257324
Epoch 270, val loss: 0.7712346911430359
Epoch 280, training loss: 1.0596282482147217 = 0.38159507513046265 + 0.1 * 6.780331134796143
Epoch 280, val loss: 0.7576441168785095
Epoch 290, training loss: 1.023869276046753 = 0.3475832939147949 + 0.1 * 6.762860298156738
Epoch 290, val loss: 0.7467440366744995
Epoch 300, training loss: 0.991408109664917 = 0.31632307171821594 + 0.1 * 6.750850200653076
Epoch 300, val loss: 0.7380374073982239
Epoch 310, training loss: 0.9633241891860962 = 0.2874523997306824 + 0.1 * 6.758718013763428
Epoch 310, val loss: 0.7312560081481934
Epoch 320, training loss: 0.9348307251930237 = 0.26096683740615845 + 0.1 * 6.738638877868652
Epoch 320, val loss: 0.7264670133590698
Epoch 330, training loss: 0.910235583782196 = 0.23653863370418549 + 0.1 * 6.736968994140625
Epoch 330, val loss: 0.7234825491905212
Epoch 340, training loss: 0.8863488435745239 = 0.2140624076128006 + 0.1 * 6.722864627838135
Epoch 340, val loss: 0.7222315073013306
Epoch 350, training loss: 0.8687289953231812 = 0.1934179961681366 + 0.1 * 6.753109931945801
Epoch 350, val loss: 0.7226825952529907
Epoch 360, training loss: 0.8469020128250122 = 0.17484524846076965 + 0.1 * 6.720567226409912
Epoch 360, val loss: 0.724680483341217
Epoch 370, training loss: 0.8288737535476685 = 0.1581396609544754 + 0.1 * 6.707341194152832
Epoch 370, val loss: 0.7280228137969971
Epoch 380, training loss: 0.8130166530609131 = 0.1431036740541458 + 0.1 * 6.699130058288574
Epoch 380, val loss: 0.732505202293396
Epoch 390, training loss: 0.800910472869873 = 0.12959663569927216 + 0.1 * 6.713138103485107
Epoch 390, val loss: 0.7378374338150024
Epoch 400, training loss: 0.7872397899627686 = 0.11756115406751633 + 0.1 * 6.696786403656006
Epoch 400, val loss: 0.74373859167099
Epoch 410, training loss: 0.774890124797821 = 0.1067630872130394 + 0.1 * 6.681270599365234
Epoch 410, val loss: 0.7500982880592346
Epoch 420, training loss: 0.7701013088226318 = 0.09706000238656998 + 0.1 * 6.73041296005249
Epoch 420, val loss: 0.7568436861038208
Epoch 430, training loss: 0.7563464045524597 = 0.08841659128665924 + 0.1 * 6.679297924041748
Epoch 430, val loss: 0.7637255787849426
Epoch 440, training loss: 0.7470168471336365 = 0.08067584037780762 + 0.1 * 6.66340970993042
Epoch 440, val loss: 0.770808219909668
Epoch 450, training loss: 0.7408351898193359 = 0.0737280547618866 + 0.1 * 6.671071529388428
Epoch 450, val loss: 0.7780928611755371
Epoch 460, training loss: 0.733291745185852 = 0.06752162426710129 + 0.1 * 6.657701015472412
Epoch 460, val loss: 0.7854413986206055
Epoch 470, training loss: 0.7268469929695129 = 0.061971548944711685 + 0.1 * 6.648754596710205
Epoch 470, val loss: 0.7928833365440369
Epoch 480, training loss: 0.7210617661476135 = 0.05700342357158661 + 0.1 * 6.640583038330078
Epoch 480, val loss: 0.8003944754600525
Epoch 490, training loss: 0.7175315618515015 = 0.0525711365044117 + 0.1 * 6.649604320526123
Epoch 490, val loss: 0.8078426122665405
Epoch 500, training loss: 0.7117149233818054 = 0.04860946908593178 + 0.1 * 6.631053924560547
Epoch 500, val loss: 0.8152626156806946
Epoch 510, training loss: 0.7081177830696106 = 0.045054804533720016 + 0.1 * 6.630630016326904
Epoch 510, val loss: 0.8226978778839111
Epoch 520, training loss: 0.7038850784301758 = 0.04187231510877609 + 0.1 * 6.620127201080322
Epoch 520, val loss: 0.8300939202308655
Epoch 530, training loss: 0.6997090578079224 = 0.03901228308677673 + 0.1 * 6.606967449188232
Epoch 530, val loss: 0.8373504877090454
Epoch 540, training loss: 0.6983171701431274 = 0.03642527014017105 + 0.1 * 6.618918418884277
Epoch 540, val loss: 0.8445568084716797
Epoch 550, training loss: 0.6943173408508301 = 0.034084200859069824 + 0.1 * 6.602331161499023
Epoch 550, val loss: 0.8517007231712341
Epoch 560, training loss: 0.6927528381347656 = 0.03195768967270851 + 0.1 * 6.607951641082764
Epoch 560, val loss: 0.8586772680282593
Epoch 570, training loss: 0.6898931860923767 = 0.030023688450455666 + 0.1 * 6.598694801330566
Epoch 570, val loss: 0.8655609488487244
Epoch 580, training loss: 0.6886455416679382 = 0.028261424973607063 + 0.1 * 6.6038408279418945
Epoch 580, val loss: 0.8723430633544922
Epoch 590, training loss: 0.6866909861564636 = 0.02665024809539318 + 0.1 * 6.600407123565674
Epoch 590, val loss: 0.8789647221565247
Epoch 600, training loss: 0.6834132075309753 = 0.02517773024737835 + 0.1 * 6.58235502243042
Epoch 600, val loss: 0.8854769468307495
Epoch 610, training loss: 0.6815881133079529 = 0.023825569078326225 + 0.1 * 6.577625274658203
Epoch 610, val loss: 0.891817569732666
Epoch 620, training loss: 0.6817871928215027 = 0.022580839693546295 + 0.1 * 6.5920634269714355
Epoch 620, val loss: 0.8980523943901062
Epoch 630, training loss: 0.6793486475944519 = 0.021434593945741653 + 0.1 * 6.5791401863098145
Epoch 630, val loss: 0.90416020154953
Epoch 640, training loss: 0.6773743033409119 = 0.020375266671180725 + 0.1 * 6.569990634918213
Epoch 640, val loss: 0.910133421421051
Epoch 650, training loss: 0.6782958507537842 = 0.019393961876630783 + 0.1 * 6.58901834487915
Epoch 650, val loss: 0.9160005450248718
Epoch 660, training loss: 0.6748760938644409 = 0.018485723063349724 + 0.1 * 6.563903331756592
Epoch 660, val loss: 0.9217190146446228
Epoch 670, training loss: 0.6739192008972168 = 0.017642701044678688 + 0.1 * 6.562765121459961
Epoch 670, val loss: 0.9273584485054016
Epoch 680, training loss: 0.6720837950706482 = 0.01685701496899128 + 0.1 * 6.552267551422119
Epoch 680, val loss: 0.9328617453575134
Epoch 690, training loss: 0.6726267337799072 = 0.016123220324516296 + 0.1 * 6.565034866333008
Epoch 690, val loss: 0.938267707824707
Epoch 700, training loss: 0.6733091473579407 = 0.015437887981534004 + 0.1 * 6.578712463378906
Epoch 700, val loss: 0.9435777068138123
Epoch 710, training loss: 0.6701325178146362 = 0.014799550175666809 + 0.1 * 6.553329944610596
Epoch 710, val loss: 0.9487593173980713
Epoch 720, training loss: 0.6692640781402588 = 0.014203266240656376 + 0.1 * 6.550608158111572
Epoch 720, val loss: 0.953842043876648
Epoch 730, training loss: 0.6674681305885315 = 0.01364312507212162 + 0.1 * 6.538249492645264
Epoch 730, val loss: 0.9588238596916199
Epoch 740, training loss: 0.6681609153747559 = 0.013117371127009392 + 0.1 * 6.5504350662231445
Epoch 740, val loss: 0.9637296199798584
Epoch 750, training loss: 0.6667894124984741 = 0.012624106369912624 + 0.1 * 6.541653156280518
Epoch 750, val loss: 0.9685380458831787
Epoch 760, training loss: 0.6666749119758606 = 0.012159792706370354 + 0.1 * 6.545151233673096
Epoch 760, val loss: 0.9732546210289001
Epoch 770, training loss: 0.665587842464447 = 0.011721648275852203 + 0.1 * 6.538661479949951
Epoch 770, val loss: 0.9778916835784912
Epoch 780, training loss: 0.6670719981193542 = 0.011309689842164516 + 0.1 * 6.557622909545898
Epoch 780, val loss: 0.9824459552764893
Epoch 790, training loss: 0.6641551852226257 = 0.010921020060777664 + 0.1 * 6.532341957092285
Epoch 790, val loss: 0.9868983626365662
Epoch 800, training loss: 0.66312175989151 = 0.010554531589150429 + 0.1 * 6.52567195892334
Epoch 800, val loss: 0.9912787675857544
Epoch 810, training loss: 0.661801815032959 = 0.010206951759755611 + 0.1 * 6.51594877243042
Epoch 810, val loss: 0.9955568909645081
Epoch 820, training loss: 0.6611539125442505 = 0.009879079647362232 + 0.1 * 6.512748718261719
Epoch 820, val loss: 0.9997727870941162
Epoch 830, training loss: 0.6608147025108337 = 0.00956769473850727 + 0.1 * 6.512470245361328
Epoch 830, val loss: 1.0039089918136597
Epoch 840, training loss: 0.6618872284889221 = 0.00927121564745903 + 0.1 * 6.52616024017334
Epoch 840, val loss: 1.0079854726791382
Epoch 850, training loss: 0.6592246890068054 = 0.008989622816443443 + 0.1 * 6.502350330352783
Epoch 850, val loss: 1.0119895935058594
Epoch 860, training loss: 0.6597959995269775 = 0.008722097612917423 + 0.1 * 6.510738372802734
Epoch 860, val loss: 1.0159105062484741
Epoch 870, training loss: 0.6588271856307983 = 0.008467319421470165 + 0.1 * 6.503598690032959
Epoch 870, val loss: 1.019774317741394
Epoch 880, training loss: 0.6597380638122559 = 0.00822525192052126 + 0.1 * 6.515128135681152
Epoch 880, val loss: 1.0235387086868286
Epoch 890, training loss: 0.6583503484725952 = 0.00799460057169199 + 0.1 * 6.5035576820373535
Epoch 890, val loss: 1.0272371768951416
Epoch 900, training loss: 0.6569763422012329 = 0.007774575147777796 + 0.1 * 6.4920172691345215
Epoch 900, val loss: 1.0308902263641357
Epoch 910, training loss: 0.6577045321464539 = 0.0075643183663487434 + 0.1 * 6.501401901245117
Epoch 910, val loss: 1.0344774723052979
Epoch 920, training loss: 0.6574426889419556 = 0.0073627629317343235 + 0.1 * 6.500799179077148
Epoch 920, val loss: 1.0380165576934814
Epoch 930, training loss: 0.6576115489006042 = 0.0071706827729940414 + 0.1 * 6.504408836364746
Epoch 930, val loss: 1.0414973497390747
Epoch 940, training loss: 0.6570755839347839 = 0.006986802909523249 + 0.1 * 6.500887870788574
Epoch 940, val loss: 1.0448862314224243
Epoch 950, training loss: 0.6557418704032898 = 0.006811113096773624 + 0.1 * 6.489307403564453
Epoch 950, val loss: 1.0482323169708252
Epoch 960, training loss: 0.6548274159431458 = 0.0066425856202840805 + 0.1 * 6.481848239898682
Epoch 960, val loss: 1.0515153408050537
Epoch 970, training loss: 0.654792070388794 = 0.006480981130152941 + 0.1 * 6.483110427856445
Epoch 970, val loss: 1.0547491312026978
Epoch 980, training loss: 0.6554283499717712 = 0.006325365509837866 + 0.1 * 6.491029739379883
Epoch 980, val loss: 1.0579538345336914
Epoch 990, training loss: 0.6547642946243286 = 0.006177170667797327 + 0.1 * 6.485870838165283
Epoch 990, val loss: 1.061073899269104
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 2.797926187515259 = 1.938244342803955 + 0.1 * 8.596817970275879
Epoch 0, val loss: 1.9302287101745605
Epoch 10, training loss: 2.788235902786255 = 1.9285638332366943 + 0.1 * 8.596720695495605
Epoch 10, val loss: 1.9210928678512573
Epoch 20, training loss: 2.7763590812683105 = 1.9167346954345703 + 0.1 * 8.596243858337402
Epoch 20, val loss: 1.9095425605773926
Epoch 30, training loss: 2.759838104248047 = 1.900574803352356 + 0.1 * 8.592632293701172
Epoch 30, val loss: 1.8933578729629517
Epoch 40, training loss: 2.7341766357421875 = 1.877347707748413 + 0.1 * 8.56828784942627
Epoch 40, val loss: 1.8701382875442505
Epoch 50, training loss: 2.6891050338745117 = 1.845770001411438 + 0.1 * 8.433350563049316
Epoch 50, val loss: 1.8399507999420166
Epoch 60, training loss: 2.6246237754821777 = 1.810731053352356 + 0.1 * 8.138927459716797
Epoch 60, val loss: 1.8086659908294678
Epoch 70, training loss: 2.558178663253784 = 1.7782320976257324 + 0.1 * 7.799465656280518
Epoch 70, val loss: 1.7798322439193726
Epoch 80, training loss: 2.4932992458343506 = 1.7442278861999512 + 0.1 * 7.490714073181152
Epoch 80, val loss: 1.749437928199768
Epoch 90, training loss: 2.423490524291992 = 1.7027066946029663 + 0.1 * 7.20783805847168
Epoch 90, val loss: 1.7138923406600952
Epoch 100, training loss: 2.3532047271728516 = 1.6468958854675293 + 0.1 * 7.063086986541748
Epoch 100, val loss: 1.6655598878860474
Epoch 110, training loss: 2.274636745452881 = 1.57273268699646 + 0.1 * 7.019040107727051
Epoch 110, val loss: 1.6008248329162598
Epoch 120, training loss: 2.185138463973999 = 1.4854576587677002 + 0.1 * 6.996807098388672
Epoch 120, val loss: 1.5283070802688599
Epoch 130, training loss: 2.0912511348724365 = 1.3932939767837524 + 0.1 * 6.979571342468262
Epoch 130, val loss: 1.45498526096344
Epoch 140, training loss: 1.9973851442337036 = 1.3010293245315552 + 0.1 * 6.963558197021484
Epoch 140, val loss: 1.3833339214324951
Epoch 150, training loss: 1.9059884548187256 = 1.2109973430633545 + 0.1 * 6.949911117553711
Epoch 150, val loss: 1.3152018785476685
Epoch 160, training loss: 1.8181402683258057 = 1.1252130270004272 + 0.1 * 6.929272651672363
Epoch 160, val loss: 1.2524054050445557
Epoch 170, training loss: 1.733688473701477 = 1.0422754287719727 + 0.1 * 6.914130210876465
Epoch 170, val loss: 1.1931359767913818
Epoch 180, training loss: 1.6529126167297363 = 0.9627053737640381 + 0.1 * 6.902072906494141
Epoch 180, val loss: 1.1366522312164307
Epoch 190, training loss: 1.576094150543213 = 0.886723518371582 + 0.1 * 6.893705368041992
Epoch 190, val loss: 1.082467794418335
Epoch 200, training loss: 1.5042493343353271 = 0.8150054812431335 + 0.1 * 6.892437934875488
Epoch 200, val loss: 1.0312585830688477
Epoch 210, training loss: 1.437268614768982 = 0.7489966154098511 + 0.1 * 6.882719993591309
Epoch 210, val loss: 0.984429121017456
Epoch 220, training loss: 1.3761861324310303 = 0.6883895993232727 + 0.1 * 6.877964973449707
Epoch 220, val loss: 0.9424909353256226
Epoch 230, training loss: 1.3202917575836182 = 0.6330690979957581 + 0.1 * 6.872225761413574
Epoch 230, val loss: 0.9062908887863159
Epoch 240, training loss: 1.26885986328125 = 0.582379162311554 + 0.1 * 6.86480712890625
Epoch 240, val loss: 0.8756710290908813
Epoch 250, training loss: 1.2216683626174927 = 0.5358136892318726 + 0.1 * 6.858546733856201
Epoch 250, val loss: 0.8504671454429626
Epoch 260, training loss: 1.1780641078948975 = 0.49296635389328003 + 0.1 * 6.850976943969727
Epoch 260, val loss: 0.8301834464073181
Epoch 270, training loss: 1.1379401683807373 = 0.45348063111305237 + 0.1 * 6.844594955444336
Epoch 270, val loss: 0.8144007921218872
Epoch 280, training loss: 1.10075843334198 = 0.4169609844684601 + 0.1 * 6.8379740715026855
Epoch 280, val loss: 0.8025153279304504
Epoch 290, training loss: 1.0672852993011475 = 0.3827243149280548 + 0.1 * 6.845609188079834
Epoch 290, val loss: 0.7938240170478821
Epoch 300, training loss: 1.0329375267028809 = 0.35064491629600525 + 0.1 * 6.822926044464111
Epoch 300, val loss: 0.7878603935241699
Epoch 310, training loss: 1.0010820627212524 = 0.32007282972335815 + 0.1 * 6.810092449188232
Epoch 310, val loss: 0.7840677499771118
Epoch 320, training loss: 0.9722520112991333 = 0.2907772362232208 + 0.1 * 6.814747333526611
Epoch 320, val loss: 0.78241366147995
Epoch 330, training loss: 0.9419317245483398 = 0.26306456327438354 + 0.1 * 6.788671493530273
Epoch 330, val loss: 0.78281569480896
Epoch 340, training loss: 0.9160434007644653 = 0.23711545765399933 + 0.1 * 6.789279460906982
Epoch 340, val loss: 0.7854416370391846
Epoch 350, training loss: 0.8907760977745056 = 0.21337826550006866 + 0.1 * 6.773978233337402
Epoch 350, val loss: 0.7899821996688843
Epoch 360, training loss: 0.8680258393287659 = 0.19200338423252106 + 0.1 * 6.760224342346191
Epoch 360, val loss: 0.7962691783905029
Epoch 370, training loss: 0.8478253483772278 = 0.17294840514659882 + 0.1 * 6.748769283294678
Epoch 370, val loss: 0.8039274215698242
Epoch 380, training loss: 0.830613374710083 = 0.15616969764232635 + 0.1 * 6.744436264038086
Epoch 380, val loss: 0.8127536177635193
Epoch 390, training loss: 0.8143817782402039 = 0.14129799604415894 + 0.1 * 6.730837821960449
Epoch 390, val loss: 0.8225389719009399
Epoch 400, training loss: 0.8024047613143921 = 0.12809962034225464 + 0.1 * 6.743051528930664
Epoch 400, val loss: 0.8328531980514526
Epoch 410, training loss: 0.788182258605957 = 0.11643368750810623 + 0.1 * 6.717485427856445
Epoch 410, val loss: 0.8436253070831299
Epoch 420, training loss: 0.7771227359771729 = 0.10605499148368835 + 0.1 * 6.710677623748779
Epoch 420, val loss: 0.8547971248626709
Epoch 430, training loss: 0.7693182229995728 = 0.09681156277656555 + 0.1 * 6.725066661834717
Epoch 430, val loss: 0.8661114573478699
Epoch 440, training loss: 0.7592858076095581 = 0.08863835036754608 + 0.1 * 6.706474304199219
Epoch 440, val loss: 0.8776195645332336
Epoch 450, training loss: 0.750573456287384 = 0.0813412144780159 + 0.1 * 6.692322254180908
Epoch 450, val loss: 0.8890107274055481
Epoch 460, training loss: 0.7429587841033936 = 0.0747949555516243 + 0.1 * 6.681638240814209
Epoch 460, val loss: 0.9003710150718689
Epoch 470, training loss: 0.7399948835372925 = 0.06891307979822159 + 0.1 * 6.710817813873291
Epoch 470, val loss: 0.9116855263710022
Epoch 480, training loss: 0.7323678135871887 = 0.06367762386798859 + 0.1 * 6.686902046203613
Epoch 480, val loss: 0.9227478504180908
Epoch 490, training loss: 0.7264106273651123 = 0.058988794684410095 + 0.1 * 6.67421817779541
Epoch 490, val loss: 0.9334944486618042
Epoch 500, training loss: 0.7230678200721741 = 0.05475769564509392 + 0.1 * 6.683101177215576
Epoch 500, val loss: 0.9440542459487915
Epoch 510, training loss: 0.7171785831451416 = 0.050940994173288345 + 0.1 * 6.6623759269714355
Epoch 510, val loss: 0.9543105363845825
Epoch 520, training loss: 0.7128698229789734 = 0.04747488349676132 + 0.1 * 6.653949737548828
Epoch 520, val loss: 0.9642783999443054
Epoch 530, training loss: 0.7092490792274475 = 0.04432355985045433 + 0.1 * 6.649255275726318
Epoch 530, val loss: 0.9740347266197205
Epoch 540, training loss: 0.7074587345123291 = 0.041452497243881226 + 0.1 * 6.660062313079834
Epoch 540, val loss: 0.9835715889930725
Epoch 550, training loss: 0.7030913829803467 = 0.038840267807245255 + 0.1 * 6.642510890960693
Epoch 550, val loss: 0.9929010272026062
Epoch 560, training loss: 0.6993887424468994 = 0.036449383944272995 + 0.1 * 6.629393577575684
Epoch 560, val loss: 1.0017962455749512
Epoch 570, training loss: 0.6977502703666687 = 0.034256767481565475 + 0.1 * 6.634934902191162
Epoch 570, val loss: 1.0104120969772339
Epoch 580, training loss: 0.6952045559883118 = 0.032251596450805664 + 0.1 * 6.6295294761657715
Epoch 580, val loss: 1.0190153121948242
Epoch 590, training loss: 0.6919136047363281 = 0.0304083414375782 + 0.1 * 6.615052700042725
Epoch 590, val loss: 1.0271443128585815
Epoch 600, training loss: 0.6913148164749146 = 0.028711820021271706 + 0.1 * 6.626029968261719
Epoch 600, val loss: 1.0352052450180054
Epoch 610, training loss: 0.6881987452507019 = 0.027145400643348694 + 0.1 * 6.610533237457275
Epoch 610, val loss: 1.0429961681365967
Epoch 620, training loss: 0.6853588819503784 = 0.025697650387883186 + 0.1 * 6.596612453460693
Epoch 620, val loss: 1.0505341291427612
Epoch 630, training loss: 0.684970498085022 = 0.024357127025723457 + 0.1 * 6.606133460998535
Epoch 630, val loss: 1.0578845739364624
Epoch 640, training loss: 0.6832069158554077 = 0.023115193471312523 + 0.1 * 6.600916862487793
Epoch 640, val loss: 1.0651251077651978
Epoch 650, training loss: 0.68150395154953 = 0.021965090185403824 + 0.1 * 6.595388412475586
Epoch 650, val loss: 1.071945309638977
Epoch 660, training loss: 0.6788801550865173 = 0.020902903750538826 + 0.1 * 6.579771995544434
Epoch 660, val loss: 1.0789389610290527
Epoch 670, training loss: 0.6778015494346619 = 0.019912999123334885 + 0.1 * 6.578885078430176
Epoch 670, val loss: 1.0855482816696167
Epoch 680, training loss: 0.6759134531021118 = 0.01899212971329689 + 0.1 * 6.569213390350342
Epoch 680, val loss: 1.0919134616851807
Epoch 690, training loss: 0.6755062341690063 = 0.018133485689759254 + 0.1 * 6.573727607727051
Epoch 690, val loss: 1.0984054803848267
Epoch 700, training loss: 0.6742969751358032 = 0.017331354320049286 + 0.1 * 6.569655895233154
Epoch 700, val loss: 1.104472041130066
Epoch 710, training loss: 0.6724772453308105 = 0.016581423580646515 + 0.1 * 6.558958053588867
Epoch 710, val loss: 1.1105703115463257
Epoch 720, training loss: 0.6739150881767273 = 0.015881041064858437 + 0.1 * 6.5803399085998535
Epoch 720, val loss: 1.116484522819519
Epoch 730, training loss: 0.6712365746498108 = 0.015226893126964569 + 0.1 * 6.560096740722656
Epoch 730, val loss: 1.1222057342529297
Epoch 740, training loss: 0.6696838140487671 = 0.014613542705774307 + 0.1 * 6.5507025718688965
Epoch 740, val loss: 1.1280443668365479
Epoch 750, training loss: 0.6703366041183472 = 0.014038842171430588 + 0.1 * 6.562977313995361
Epoch 750, val loss: 1.133384346961975
Epoch 760, training loss: 0.6683589816093445 = 0.013500557281076908 + 0.1 * 6.548583984375
Epoch 760, val loss: 1.138863205909729
Epoch 770, training loss: 0.6665521860122681 = 0.012993194162845612 + 0.1 * 6.535589694976807
Epoch 770, val loss: 1.1442065238952637
Epoch 780, training loss: 0.6658121347427368 = 0.012515864335000515 + 0.1 * 6.532962799072266
Epoch 780, val loss: 1.1492239236831665
Epoch 790, training loss: 0.6651104092597961 = 0.012066182680428028 + 0.1 * 6.530442237854004
Epoch 790, val loss: 1.1544145345687866
Epoch 800, training loss: 0.6653814911842346 = 0.011642388068139553 + 0.1 * 6.53739070892334
Epoch 800, val loss: 1.1593358516693115
Epoch 810, training loss: 0.6666728258132935 = 0.011241113767027855 + 0.1 * 6.554316520690918
Epoch 810, val loss: 1.164098858833313
Epoch 820, training loss: 0.6636619567871094 = 0.01086476817727089 + 0.1 * 6.5279717445373535
Epoch 820, val loss: 1.1689845323562622
Epoch 830, training loss: 0.6622880697250366 = 0.010507041588425636 + 0.1 * 6.517810344696045
Epoch 830, val loss: 1.173680305480957
Epoch 840, training loss: 0.6622130274772644 = 0.01016855426132679 + 0.1 * 6.520444869995117
Epoch 840, val loss: 1.1782296895980835
Epoch 850, training loss: 0.6610189080238342 = 0.009847396053373814 + 0.1 * 6.511714935302734
Epoch 850, val loss: 1.1827839612960815
Epoch 860, training loss: 0.6619825959205627 = 0.009542059153318405 + 0.1 * 6.524405479431152
Epoch 860, val loss: 1.1870648860931396
Epoch 870, training loss: 0.6597287058830261 = 0.009252209216356277 + 0.1 * 6.504765033721924
Epoch 870, val loss: 1.1914336681365967
Epoch 880, training loss: 0.6600064635276794 = 0.00897713378071785 + 0.1 * 6.510293483734131
Epoch 880, val loss: 1.1958280801773071
Epoch 890, training loss: 0.6596668362617493 = 0.008715252391994 + 0.1 * 6.509515762329102
Epoch 890, val loss: 1.199778437614441
Epoch 900, training loss: 0.6589295864105225 = 0.008466149680316448 + 0.1 * 6.504634380340576
Epoch 900, val loss: 1.2040754556655884
Epoch 910, training loss: 0.6579609513282776 = 0.008227730169892311 + 0.1 * 6.497332572937012
Epoch 910, val loss: 1.208080530166626
Epoch 920, training loss: 0.6586710214614868 = 0.0080005694180727 + 0.1 * 6.506704330444336
Epoch 920, val loss: 1.2120397090911865
Epoch 930, training loss: 0.6577972769737244 = 0.0077834418043494225 + 0.1 * 6.500138282775879
Epoch 930, val loss: 1.215792179107666
Epoch 940, training loss: 0.6574012637138367 = 0.0075762621127069 + 0.1 * 6.498249530792236
Epoch 940, val loss: 1.219793677330017
Epoch 950, training loss: 0.6582887172698975 = 0.007378105539828539 + 0.1 * 6.509106159210205
Epoch 950, val loss: 1.223466396331787
Epoch 960, training loss: 0.6564639806747437 = 0.007187962532043457 + 0.1 * 6.492760181427002
Epoch 960, val loss: 1.2269155979156494
Epoch 970, training loss: 0.6555191278457642 = 0.007006778847426176 + 0.1 * 6.485123634338379
Epoch 970, val loss: 1.2307984828948975
Epoch 980, training loss: 0.655676543712616 = 0.006833003368228674 + 0.1 * 6.4884352684021
Epoch 980, val loss: 1.2343847751617432
Epoch 990, training loss: 0.6551878452301025 = 0.0066655660048127174 + 0.1 * 6.485222339630127
Epoch 990, val loss: 1.2375648021697998
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 2.796821117401123 = 1.9371414184570312 + 0.1 * 8.596797943115234
Epoch 0, val loss: 1.9334179162979126
Epoch 10, training loss: 2.7871322631835938 = 1.9274715185165405 + 0.1 * 8.596606254577637
Epoch 10, val loss: 1.9240964651107788
Epoch 20, training loss: 2.7749271392822266 = 1.9153872728347778 + 0.1 * 8.595399856567383
Epoch 20, val loss: 1.9126299619674683
Epoch 30, training loss: 2.756958484649658 = 1.898453712463379 + 0.1 * 8.585047721862793
Epoch 30, val loss: 1.8966403007507324
Epoch 40, training loss: 2.7263519763946533 = 1.8736751079559326 + 0.1 * 8.52676773071289
Epoch 40, val loss: 1.8735473155975342
Epoch 50, training loss: 2.6650476455688477 = 1.841435194015503 + 0.1 * 8.236123085021973
Epoch 50, val loss: 1.8442127704620361
Epoch 60, training loss: 2.6029212474823 = 1.8070708513259888 + 0.1 * 7.958503723144531
Epoch 60, val loss: 1.8138422966003418
Epoch 70, training loss: 2.5276522636413574 = 1.7753747701644897 + 0.1 * 7.522774696350098
Epoch 70, val loss: 1.7858178615570068
Epoch 80, training loss: 2.467630386352539 = 1.7422916889190674 + 0.1 * 7.253385543823242
Epoch 80, val loss: 1.7564102411270142
Epoch 90, training loss: 2.4109385013580322 = 1.6989628076553345 + 0.1 * 7.119757175445557
Epoch 90, val loss: 1.718177080154419
Epoch 100, training loss: 2.3440113067626953 = 1.6406331062316895 + 0.1 * 7.033782958984375
Epoch 100, val loss: 1.6677035093307495
Epoch 110, training loss: 2.265319585800171 = 1.5677528381347656 + 0.1 * 6.975667953491211
Epoch 110, val loss: 1.6067535877227783
Epoch 120, training loss: 2.1802077293395996 = 1.4862329959869385 + 0.1 * 6.939745903015137
Epoch 120, val loss: 1.540753722190857
Epoch 130, training loss: 2.0935611724853516 = 1.4024258852005005 + 0.1 * 6.911352157592773
Epoch 130, val loss: 1.4746016263961792
Epoch 140, training loss: 2.0092873573303223 = 1.3199971914291382 + 0.1 * 6.892902374267578
Epoch 140, val loss: 1.4110106229782104
Epoch 150, training loss: 1.9257336854934692 = 1.2383270263671875 + 0.1 * 6.874066352844238
Epoch 150, val loss: 1.3487696647644043
Epoch 160, training loss: 1.8439581394195557 = 1.157798409461975 + 0.1 * 6.861597537994385
Epoch 160, val loss: 1.2889819145202637
Epoch 170, training loss: 1.7663089036941528 = 1.081478238105774 + 0.1 * 6.848306655883789
Epoch 170, val loss: 1.2340420484542847
Epoch 180, training loss: 1.6934013366699219 = 1.0096900463104248 + 0.1 * 6.837112903594971
Epoch 180, val loss: 1.184644341468811
Epoch 190, training loss: 1.625962257385254 = 0.9428846836090088 + 0.1 * 6.830774784088135
Epoch 190, val loss: 1.1398544311523438
Epoch 200, training loss: 1.5621064901351929 = 0.8806143999099731 + 0.1 * 6.814920902252197
Epoch 200, val loss: 1.0990334749221802
Epoch 210, training loss: 1.5023293495178223 = 0.821716845035553 + 0.1 * 6.806124210357666
Epoch 210, val loss: 1.0612131357192993
Epoch 220, training loss: 1.4452970027923584 = 0.7662103772163391 + 0.1 * 6.790865421295166
Epoch 220, val loss: 1.0263960361480713
Epoch 230, training loss: 1.3925501108169556 = 0.7132424116134644 + 0.1 * 6.793076992034912
Epoch 230, val loss: 0.9942317008972168
Epoch 240, training loss: 1.340345859527588 = 0.6634142398834229 + 0.1 * 6.769315242767334
Epoch 240, val loss: 0.9653955698013306
Epoch 250, training loss: 1.2907569408416748 = 0.6157612204551697 + 0.1 * 6.749957084655762
Epoch 250, val loss: 0.9392606019973755
Epoch 260, training loss: 1.2456848621368408 = 0.5696952939033508 + 0.1 * 6.759895324707031
Epoch 260, val loss: 0.9154996275901794
Epoch 270, training loss: 1.1988472938537598 = 0.5256438255310059 + 0.1 * 6.732033729553223
Epoch 270, val loss: 0.8947656750679016
Epoch 280, training loss: 1.154395341873169 = 0.4830871820449829 + 0.1 * 6.713080883026123
Epoch 280, val loss: 0.8761826753616333
Epoch 290, training loss: 1.1134352684020996 = 0.44214609265327454 + 0.1 * 6.712892055511475
Epoch 290, val loss: 0.8601040244102478
Epoch 300, training loss: 1.0722064971923828 = 0.403133749961853 + 0.1 * 6.6907267570495605
Epoch 300, val loss: 0.8470126390457153
Epoch 310, training loss: 1.034210443496704 = 0.36575964093208313 + 0.1 * 6.684508323669434
Epoch 310, val loss: 0.8364356160163879
Epoch 320, training loss: 0.9984595775604248 = 0.33048707246780396 + 0.1 * 6.67972469329834
Epoch 320, val loss: 0.82862389087677
Epoch 330, training loss: 0.9641917943954468 = 0.29749631881713867 + 0.1 * 6.666954517364502
Epoch 330, val loss: 0.8235183358192444
Epoch 340, training loss: 0.9322366714477539 = 0.26692500710487366 + 0.1 * 6.653116226196289
Epoch 340, val loss: 0.8208962082862854
Epoch 350, training loss: 0.9046334624290466 = 0.23887532949447632 + 0.1 * 6.657581329345703
Epoch 350, val loss: 0.8203873634338379
Epoch 360, training loss: 0.8781566619873047 = 0.21345742046833038 + 0.1 * 6.6469926834106445
Epoch 360, val loss: 0.8218737244606018
Epoch 370, training loss: 0.8539245128631592 = 0.19058279693126678 + 0.1 * 6.633416652679443
Epoch 370, val loss: 0.8252384662628174
Epoch 380, training loss: 0.836591899394989 = 0.17025430500507355 + 0.1 * 6.6633758544921875
Epoch 380, val loss: 0.830267071723938
Epoch 390, training loss: 0.8143489360809326 = 0.15240970253944397 + 0.1 * 6.619392395019531
Epoch 390, val loss: 0.8370482325553894
Epoch 400, training loss: 0.7994610667228699 = 0.13667182624340057 + 0.1 * 6.627892017364502
Epoch 400, val loss: 0.845162034034729
Epoch 410, training loss: 0.7853107452392578 = 0.12286540865898132 + 0.1 * 6.624453067779541
Epoch 410, val loss: 0.8544394969940186
Epoch 420, training loss: 0.7714987397193909 = 0.11078836023807526 + 0.1 * 6.6071038246154785
Epoch 420, val loss: 0.8646371364593506
Epoch 430, training loss: 0.7603565454483032 = 0.10021266341209412 + 0.1 * 6.601438522338867
Epoch 430, val loss: 0.875542402267456
Epoch 440, training loss: 0.7509781122207642 = 0.09094627946615219 + 0.1 * 6.600318431854248
Epoch 440, val loss: 0.8868030309677124
Epoch 450, training loss: 0.741091787815094 = 0.08282722532749176 + 0.1 * 6.582645416259766
Epoch 450, val loss: 0.8984819650650024
Epoch 460, training loss: 0.7352929711341858 = 0.07567749917507172 + 0.1 * 6.596154689788818
Epoch 460, val loss: 0.910306990146637
Epoch 470, training loss: 0.7272509932518005 = 0.06938375532627106 + 0.1 * 6.578672409057617
Epoch 470, val loss: 0.9222748875617981
Epoch 480, training loss: 0.7225414514541626 = 0.06380075216293335 + 0.1 * 6.587406635284424
Epoch 480, val loss: 0.9341585636138916
Epoch 490, training loss: 0.7160338163375854 = 0.05885394662618637 + 0.1 * 6.571798801422119
Epoch 490, val loss: 0.9460185766220093
Epoch 500, training loss: 0.7115073204040527 = 0.05444531515240669 + 0.1 * 6.570619583129883
Epoch 500, val loss: 0.9576808214187622
Epoch 510, training loss: 0.7072272896766663 = 0.050514306873083115 + 0.1 * 6.567130088806152
Epoch 510, val loss: 0.9691354036331177
Epoch 520, training loss: 0.7027871012687683 = 0.04698394984006882 + 0.1 * 6.55803108215332
Epoch 520, val loss: 0.9804267287254333
Epoch 530, training loss: 0.6998687982559204 = 0.04380873218178749 + 0.1 * 6.560600280761719
Epoch 530, val loss: 0.9913905262947083
Epoch 540, training loss: 0.6959378123283386 = 0.04094826430082321 + 0.1 * 6.549895286560059
Epoch 540, val loss: 1.0023870468139648
Epoch 550, training loss: 0.6931071877479553 = 0.03835870325565338 + 0.1 * 6.547484874725342
Epoch 550, val loss: 1.012957215309143
Epoch 560, training loss: 0.6896905899047852 = 0.03601137548685074 + 0.1 * 6.536791801452637
Epoch 560, val loss: 1.0234602689743042
Epoch 570, training loss: 0.6899725198745728 = 0.033869508653879166 + 0.1 * 6.56102991104126
Epoch 570, val loss: 1.0335501432418823
Epoch 580, training loss: 0.6853572130203247 = 0.03192131966352463 + 0.1 * 6.534358978271484
Epoch 580, val loss: 1.0436880588531494
Epoch 590, training loss: 0.6854400038719177 = 0.030136968940496445 + 0.1 * 6.553030490875244
Epoch 590, val loss: 1.0533932447433472
Epoch 600, training loss: 0.6817810535430908 = 0.028503669425845146 + 0.1 * 6.532773971557617
Epoch 600, val loss: 1.0629761219024658
Epoch 610, training loss: 0.6792061924934387 = 0.02701057493686676 + 0.1 * 6.521955966949463
Epoch 610, val loss: 1.0724438428878784
Epoch 620, training loss: 0.6776267290115356 = 0.025630168616771698 + 0.1 * 6.519965171813965
Epoch 620, val loss: 1.081501841545105
Epoch 630, training loss: 0.6771515607833862 = 0.024356557056307793 + 0.1 * 6.527950286865234
Epoch 630, val loss: 1.0905985832214355
Epoch 640, training loss: 0.6750524044036865 = 0.023178687319159508 + 0.1 * 6.518736839294434
Epoch 640, val loss: 1.0992870330810547
Epoch 650, training loss: 0.6731917262077332 = 0.022089047357439995 + 0.1 * 6.511026382446289
Epoch 650, val loss: 1.1080739498138428
Epoch 660, training loss: 0.6714254021644592 = 0.02107471600174904 + 0.1 * 6.503506660461426
Epoch 660, val loss: 1.1162986755371094
Epoch 670, training loss: 0.6709304451942444 = 0.020136015489697456 + 0.1 * 6.507944583892822
Epoch 670, val loss: 1.1247974634170532
Epoch 680, training loss: 0.6696865558624268 = 0.019258815795183182 + 0.1 * 6.50427770614624
Epoch 680, val loss: 1.1326545476913452
Epoch 690, training loss: 0.6681787371635437 = 0.018445167690515518 + 0.1 * 6.497335433959961
Epoch 690, val loss: 1.140832543373108
Epoch 700, training loss: 0.6668651700019836 = 0.017681263387203217 + 0.1 * 6.4918389320373535
Epoch 700, val loss: 1.1485083103179932
Epoch 710, training loss: 0.6670609712600708 = 0.016966547816991806 + 0.1 * 6.5009446144104
Epoch 710, val loss: 1.1561806201934814
Epoch 720, training loss: 0.6663036346435547 = 0.01629583351314068 + 0.1 * 6.500077724456787
Epoch 720, val loss: 1.1635054349899292
Epoch 730, training loss: 0.664967954158783 = 0.015670046210289 + 0.1 * 6.492978572845459
Epoch 730, val loss: 1.1709918975830078
Epoch 740, training loss: 0.6642952561378479 = 0.015079417265951633 + 0.1 * 6.492157936096191
Epoch 740, val loss: 1.177979588508606
Epoch 750, training loss: 0.6634401679039001 = 0.014525446109473705 + 0.1 * 6.489146709442139
Epoch 750, val loss: 1.1851627826690674
Epoch 760, training loss: 0.6633703708648682 = 0.014001958072185516 + 0.1 * 6.493683815002441
Epoch 760, val loss: 1.191827654838562
Epoch 770, training loss: 0.661829948425293 = 0.01351019088178873 + 0.1 * 6.4831976890563965
Epoch 770, val loss: 1.1986325979232788
Epoch 780, training loss: 0.6614302396774292 = 0.013045095838606358 + 0.1 * 6.483851432800293
Epoch 780, val loss: 1.2053289413452148
Epoch 790, training loss: 0.6603758335113525 = 0.01260404009371996 + 0.1 * 6.477717876434326
Epoch 790, val loss: 1.2115387916564941
Epoch 800, training loss: 0.6598612666130066 = 0.012188944965600967 + 0.1 * 6.476722717285156
Epoch 800, val loss: 1.2181013822555542
Epoch 810, training loss: 0.6602136492729187 = 0.011793863028287888 + 0.1 * 6.484197616577148
Epoch 810, val loss: 1.2242283821105957
Epoch 820, training loss: 0.658566951751709 = 0.011420208029448986 + 0.1 * 6.4714674949646
Epoch 820, val loss: 1.2303823232650757
Epoch 830, training loss: 0.6592547297477722 = 0.011064399033784866 + 0.1 * 6.481903076171875
Epoch 830, val loss: 1.2362433671951294
Epoch 840, training loss: 0.6574854254722595 = 0.010728749446570873 + 0.1 * 6.46756649017334
Epoch 840, val loss: 1.2422326803207397
Epoch 850, training loss: 0.6577820777893066 = 0.010408619418740273 + 0.1 * 6.473734378814697
Epoch 850, val loss: 1.2481006383895874
Epoch 860, training loss: 0.6557117104530334 = 0.010103083215653896 + 0.1 * 6.456086158752441
Epoch 860, val loss: 1.253687858581543
Epoch 870, training loss: 0.6577008366584778 = 0.009812370873987675 + 0.1 * 6.478884696960449
Epoch 870, val loss: 1.2592321634292603
Epoch 880, training loss: 0.6554638743400574 = 0.00953566562384367 + 0.1 * 6.459281921386719
Epoch 880, val loss: 1.2647969722747803
Epoch 890, training loss: 0.6562944054603577 = 0.009272008202970028 + 0.1 * 6.470223903656006
Epoch 890, val loss: 1.270297884941101
Epoch 900, training loss: 0.6543784141540527 = 0.009018194861710072 + 0.1 * 6.453602313995361
Epoch 900, val loss: 1.2752935886383057
Epoch 910, training loss: 0.6536309719085693 = 0.008778167888522148 + 0.1 * 6.448528289794922
Epoch 910, val loss: 1.2807880640029907
Epoch 920, training loss: 0.6535938382148743 = 0.008547338657081127 + 0.1 * 6.450464725494385
Epoch 920, val loss: 1.2857725620269775
Epoch 930, training loss: 0.653482973575592 = 0.008327696472406387 + 0.1 * 6.451552391052246
Epoch 930, val loss: 1.2909291982650757
Epoch 940, training loss: 0.6529091000556946 = 0.008116689510643482 + 0.1 * 6.4479241371154785
Epoch 940, val loss: 1.2960363626480103
Epoch 950, training loss: 0.6526317596435547 = 0.007913615554571152 + 0.1 * 6.447181701660156
Epoch 950, val loss: 1.3008744716644287
Epoch 960, training loss: 0.6529092788696289 = 0.0077189914882183075 + 0.1 * 6.451902866363525
Epoch 960, val loss: 1.3056668043136597
Epoch 970, training loss: 0.651821494102478 = 0.007532957009971142 + 0.1 * 6.442885398864746
Epoch 970, val loss: 1.3105264902114868
Epoch 980, training loss: 0.6528642177581787 = 0.007354100700467825 + 0.1 * 6.455101013183594
Epoch 980, val loss: 1.3151733875274658
Epoch 990, training loss: 0.6513458490371704 = 0.007181396707892418 + 0.1 * 6.441644191741943
Epoch 990, val loss: 1.3196523189544678
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8107538218239325
The final CL Acc:0.78025, 0.00630, The final GNN Acc:0.81321, 0.00277
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13186])
remove edge: torch.Size([2, 7936])
updated graph: torch.Size([2, 10566])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.804272413253784 = 1.9445888996124268 + 0.1 * 8.596835136413574
Epoch 0, val loss: 1.9400596618652344
Epoch 10, training loss: 2.794318675994873 = 1.9346433877944946 + 0.1 * 8.596753120422363
Epoch 10, val loss: 1.9308134317398071
Epoch 20, training loss: 2.782050132751465 = 1.9224169254302979 + 0.1 * 8.596331596374512
Epoch 20, val loss: 1.919027328491211
Epoch 30, training loss: 2.764472484588623 = 1.9051743745803833 + 0.1 * 8.592981338500977
Epoch 30, val loss: 1.9021283388137817
Epoch 40, training loss: 2.7365469932556152 = 1.879631757736206 + 0.1 * 8.569150924682617
Epoch 40, val loss: 1.877264380455017
Epoch 50, training loss: 2.6896753311157227 = 1.8441267013549805 + 0.1 * 8.455485343933105
Epoch 50, val loss: 1.8441914319992065
Epoch 60, training loss: 2.6232120990753174 = 1.8044546842575073 + 0.1 * 8.187573432922363
Epoch 60, val loss: 1.809827446937561
Epoch 70, training loss: 2.5615766048431396 = 1.7668951749801636 + 0.1 * 7.946814060211182
Epoch 70, val loss: 1.7772358655929565
Epoch 80, training loss: 2.4768853187561035 = 1.7246923446655273 + 0.1 * 7.521929740905762
Epoch 80, val loss: 1.737379789352417
Epoch 90, training loss: 2.3973822593688965 = 1.6722370386123657 + 0.1 * 7.251451015472412
Epoch 90, val loss: 1.6896001100540161
Epoch 100, training loss: 2.3168625831604004 = 1.6045278310775757 + 0.1 * 7.123348236083984
Epoch 100, val loss: 1.6292299032211304
Epoch 110, training loss: 2.231057643890381 = 1.5247832536697388 + 0.1 * 7.062745094299316
Epoch 110, val loss: 1.5597848892211914
Epoch 120, training loss: 2.1448094844818115 = 1.4423320293426514 + 0.1 * 7.024774551391602
Epoch 120, val loss: 1.4914120435714722
Epoch 130, training loss: 2.061920642852783 = 1.3625941276550293 + 0.1 * 6.9932661056518555
Epoch 130, val loss: 1.4279215335845947
Epoch 140, training loss: 1.9808704853057861 = 1.2840005159378052 + 0.1 * 6.968699932098389
Epoch 140, val loss: 1.366620421409607
Epoch 150, training loss: 1.8983900547027588 = 1.2036359310150146 + 0.1 * 6.9475417137146
Epoch 150, val loss: 1.3045896291732788
Epoch 160, training loss: 1.815967321395874 = 1.122636318206787 + 0.1 * 6.933309555053711
Epoch 160, val loss: 1.2430338859558105
Epoch 170, training loss: 1.7356228828430176 = 1.0441123247146606 + 0.1 * 6.91510534286499
Epoch 170, val loss: 1.1856199502944946
Epoch 180, training loss: 1.65809965133667 = 0.9683743119239807 + 0.1 * 6.89725399017334
Epoch 180, val loss: 1.1305805444717407
Epoch 190, training loss: 1.5835659503936768 = 0.8951715230941772 + 0.1 * 6.883944511413574
Epoch 190, val loss: 1.0771570205688477
Epoch 200, training loss: 1.5104341506958008 = 0.8238780498504639 + 0.1 * 6.865561485290527
Epoch 200, val loss: 1.024517297744751
Epoch 210, training loss: 1.439203143119812 = 0.753858208656311 + 0.1 * 6.85344934463501
Epoch 210, val loss: 0.9726176857948303
Epoch 220, training loss: 1.3703500032424927 = 0.6861454248428345 + 0.1 * 6.842045783996582
Epoch 220, val loss: 0.9224693179130554
Epoch 230, training loss: 1.3056519031524658 = 0.6218898892402649 + 0.1 * 6.837619304656982
Epoch 230, val loss: 0.8763688802719116
Epoch 240, training loss: 1.2450278997421265 = 0.5625725388526917 + 0.1 * 6.824553489685059
Epoch 240, val loss: 0.836699903011322
Epoch 250, training loss: 1.1898565292358398 = 0.508277416229248 + 0.1 * 6.815791606903076
Epoch 250, val loss: 0.8044590950012207
Epoch 260, training loss: 1.1401865482330322 = 0.4593895673751831 + 0.1 * 6.807969093322754
Epoch 260, val loss: 0.7800004482269287
Epoch 270, training loss: 1.0954811573028564 = 0.41566571593284607 + 0.1 * 6.798154830932617
Epoch 270, val loss: 0.7622594237327576
Epoch 280, training loss: 1.056497573852539 = 0.3765992522239685 + 0.1 * 6.798983573913574
Epoch 280, val loss: 0.7500378489494324
Epoch 290, training loss: 1.0200225114822388 = 0.3415747880935669 + 0.1 * 6.784477233886719
Epoch 290, val loss: 0.742063581943512
Epoch 300, training loss: 0.9873994588851929 = 0.3097783625125885 + 0.1 * 6.776211261749268
Epoch 300, val loss: 0.7373553514480591
Epoch 310, training loss: 0.9579824805259705 = 0.2808038592338562 + 0.1 * 6.771786212921143
Epoch 310, val loss: 0.735304594039917
Epoch 320, training loss: 0.9321558475494385 = 0.2546089291572571 + 0.1 * 6.7754693031311035
Epoch 320, val loss: 0.7354355454444885
Epoch 330, training loss: 0.9068635702133179 = 0.23104852437973022 + 0.1 * 6.758150100708008
Epoch 330, val loss: 0.7373656034469604
Epoch 340, training loss: 0.8857455253601074 = 0.20973089337348938 + 0.1 * 6.760146141052246
Epoch 340, val loss: 0.7407321333885193
Epoch 350, training loss: 0.8657230734825134 = 0.19050616025924683 + 0.1 * 6.752169132232666
Epoch 350, val loss: 0.7453592419624329
Epoch 360, training loss: 0.8472205400466919 = 0.17307546734809875 + 0.1 * 6.741450786590576
Epoch 360, val loss: 0.7508915066719055
Epoch 370, training loss: 0.8317638635635376 = 0.15721118450164795 + 0.1 * 6.7455267906188965
Epoch 370, val loss: 0.7573124170303345
Epoch 380, training loss: 0.8163087368011475 = 0.14280717074871063 + 0.1 * 6.735015392303467
Epoch 380, val loss: 0.7644097805023193
Epoch 390, training loss: 0.8039607405662537 = 0.12974952161312103 + 0.1 * 6.742112159729004
Epoch 390, val loss: 0.7719988226890564
Epoch 400, training loss: 0.7905266880989075 = 0.11797039955854416 + 0.1 * 6.725562572479248
Epoch 400, val loss: 0.7800120711326599
Epoch 410, training loss: 0.7803812623023987 = 0.10731782764196396 + 0.1 * 6.730634689331055
Epoch 410, val loss: 0.7883673906326294
Epoch 420, training loss: 0.770117998123169 = 0.09775125980377197 + 0.1 * 6.723667144775391
Epoch 420, val loss: 0.7970564961433411
Epoch 430, training loss: 0.7604943513870239 = 0.08915248513221741 + 0.1 * 6.713418960571289
Epoch 430, val loss: 0.8058013319969177
Epoch 440, training loss: 0.7525827288627625 = 0.08143744617700577 + 0.1 * 6.711452484130859
Epoch 440, val loss: 0.8147188425064087
Epoch 450, training loss: 0.7448244094848633 = 0.07454003393650055 + 0.1 * 6.70284366607666
Epoch 450, val loss: 0.8238121867179871
Epoch 460, training loss: 0.7394903302192688 = 0.06836307048797607 + 0.1 * 6.711272716522217
Epoch 460, val loss: 0.8328190445899963
Epoch 470, training loss: 0.7317052483558655 = 0.06284940987825394 + 0.1 * 6.688558101654053
Epoch 470, val loss: 0.841939389705658
Epoch 480, training loss: 0.7260064482688904 = 0.057900380343198776 + 0.1 * 6.681060791015625
Epoch 480, val loss: 0.8509535193443298
Epoch 490, training loss: 0.7215076684951782 = 0.05344709753990173 + 0.1 * 6.680605411529541
Epoch 490, val loss: 0.860041618347168
Epoch 500, training loss: 0.7208085060119629 = 0.0494539812207222 + 0.1 * 6.713545322418213
Epoch 500, val loss: 0.86885005235672
Epoch 510, training loss: 0.7135670781135559 = 0.04588466137647629 + 0.1 * 6.67682409286499
Epoch 510, val loss: 0.8776884078979492
Epoch 520, training loss: 0.7089514136314392 = 0.04266859591007233 + 0.1 * 6.66282844543457
Epoch 520, val loss: 0.8862548470497131
Epoch 530, training loss: 0.7050938010215759 = 0.03976348415017128 + 0.1 * 6.6533026695251465
Epoch 530, val loss: 0.8947641849517822
Epoch 540, training loss: 0.7054470181465149 = 0.03713425621390343 + 0.1 * 6.6831278800964355
Epoch 540, val loss: 0.9030584692955017
Epoch 550, training loss: 0.6993300914764404 = 0.034758709371089935 + 0.1 * 6.645713806152344
Epoch 550, val loss: 0.9112948775291443
Epoch 560, training loss: 0.6969794631004333 = 0.03259890154004097 + 0.1 * 6.643805503845215
Epoch 560, val loss: 0.919243335723877
Epoch 570, training loss: 0.696471631526947 = 0.030628906562924385 + 0.1 * 6.658426761627197
Epoch 570, val loss: 0.9270918369293213
Epoch 580, training loss: 0.6919804811477661 = 0.028834901750087738 + 0.1 * 6.631455898284912
Epoch 580, val loss: 0.9348649978637695
Epoch 590, training loss: 0.6905728578567505 = 0.027193231508135796 + 0.1 * 6.633796215057373
Epoch 590, val loss: 0.9422349333763123
Epoch 600, training loss: 0.688011109828949 = 0.02569412812590599 + 0.1 * 6.623169898986816
Epoch 600, val loss: 0.9496068954467773
Epoch 610, training loss: 0.685491681098938 = 0.0243171788752079 + 0.1 * 6.6117448806762695
Epoch 610, val loss: 0.9567108154296875
Epoch 620, training loss: 0.6855044960975647 = 0.02304873615503311 + 0.1 * 6.624557018280029
Epoch 620, val loss: 0.9637534618377686
Epoch 630, training loss: 0.6834268569946289 = 0.021881073713302612 + 0.1 * 6.615458011627197
Epoch 630, val loss: 0.970530092716217
Epoch 640, training loss: 0.6804917454719543 = 0.020802520215511322 + 0.1 * 6.596892356872559
Epoch 640, val loss: 0.9772640466690063
Epoch 650, training loss: 0.679884672164917 = 0.019802968949079514 + 0.1 * 6.60081672668457
Epoch 650, val loss: 0.9836211204528809
Epoch 660, training loss: 0.678523600101471 = 0.018878374248743057 + 0.1 * 6.596451759338379
Epoch 660, val loss: 0.9899261593818665
Epoch 670, training loss: 0.6755395531654358 = 0.018023241311311722 + 0.1 * 6.575162887573242
Epoch 670, val loss: 0.9961711764335632
Epoch 680, training loss: 0.6776856780052185 = 0.017222832888364792 + 0.1 * 6.604628562927246
Epoch 680, val loss: 1.00200617313385
Epoch 690, training loss: 0.6733470559120178 = 0.01648111641407013 + 0.1 * 6.56865930557251
Epoch 690, val loss: 1.0079752206802368
Epoch 700, training loss: 0.6712707877159119 = 0.015787070617079735 + 0.1 * 6.554837226867676
Epoch 700, val loss: 1.0137051343917847
Epoch 710, training loss: 0.6712696552276611 = 0.01513623259961605 + 0.1 * 6.561334133148193
Epoch 710, val loss: 1.0193485021591187
Epoch 720, training loss: 0.6699143648147583 = 0.01452753134071827 + 0.1 * 6.553867816925049
Epoch 720, val loss: 1.0246943235397339
Epoch 730, training loss: 0.6686086058616638 = 0.013958079740405083 + 0.1 * 6.546504974365234
Epoch 730, val loss: 1.0302752256393433
Epoch 740, training loss: 0.670642077922821 = 0.013421437703073025 + 0.1 * 6.572206497192383
Epoch 740, val loss: 1.0352962017059326
Epoch 750, training loss: 0.6660032272338867 = 0.012918896973133087 + 0.1 * 6.530843257904053
Epoch 750, val loss: 1.0405298471450806
Epoch 760, training loss: 0.6658941507339478 = 0.012445018626749516 + 0.1 * 6.534491062164307
Epoch 760, val loss: 1.0456088781356812
Epoch 770, training loss: 0.6641623377799988 = 0.011996866203844547 + 0.1 * 6.5216546058654785
Epoch 770, val loss: 1.0502922534942627
Epoch 780, training loss: 0.6640823483467102 = 0.011576480232179165 + 0.1 * 6.525058746337891
Epoch 780, val loss: 1.0553232431411743
Epoch 790, training loss: 0.665168285369873 = 0.011176792904734612 + 0.1 * 6.539915084838867
Epoch 790, val loss: 1.0598708391189575
Epoch 800, training loss: 0.6630059480667114 = 0.010799771174788475 + 0.1 * 6.522061824798584
Epoch 800, val loss: 1.0645700693130493
Epoch 810, training loss: 0.6622105836868286 = 0.010441659949719906 + 0.1 * 6.517689228057861
Epoch 810, val loss: 1.0690641403198242
Epoch 820, training loss: 0.6629630327224731 = 0.010102106258273125 + 0.1 * 6.528609275817871
Epoch 820, val loss: 1.0734981298446655
Epoch 830, training loss: 0.6608086824417114 = 0.009778909385204315 + 0.1 * 6.510297775268555
Epoch 830, val loss: 1.0777913331985474
Epoch 840, training loss: 0.6597832441329956 = 0.009474479593336582 + 0.1 * 6.503087997436523
Epoch 840, val loss: 1.0821752548217773
Epoch 850, training loss: 0.6614149212837219 = 0.009184030815958977 + 0.1 * 6.522309303283691
Epoch 850, val loss: 1.0863642692565918
Epoch 860, training loss: 0.659104585647583 = 0.008908405900001526 + 0.1 * 6.501961708068848
Epoch 860, val loss: 1.0903394222259521
Epoch 870, training loss: 0.6582050919532776 = 0.008645864203572273 + 0.1 * 6.49559211730957
Epoch 870, val loss: 1.0944973230361938
Epoch 880, training loss: 0.6585057377815247 = 0.008395529352128506 + 0.1 * 6.501101970672607
Epoch 880, val loss: 1.0983580350875854
Epoch 890, training loss: 0.6575582027435303 = 0.008156552910804749 + 0.1 * 6.494016647338867
Epoch 890, val loss: 1.1021403074264526
Epoch 900, training loss: 0.657338559627533 = 0.007928833365440369 + 0.1 * 6.4940972328186035
Epoch 900, val loss: 1.1060380935668945
Epoch 910, training loss: 0.6564863920211792 = 0.007711995858699083 + 0.1 * 6.487743854522705
Epoch 910, val loss: 1.1096851825714111
Epoch 920, training loss: 0.656301736831665 = 0.007504377048462629 + 0.1 * 6.487973213195801
Epoch 920, val loss: 1.1132463216781616
Epoch 930, training loss: 0.65520179271698 = 0.007305967155843973 + 0.1 * 6.4789581298828125
Epoch 930, val loss: 1.1168838739395142
Epoch 940, training loss: 0.6565790176391602 = 0.007116254419088364 + 0.1 * 6.494627475738525
Epoch 940, val loss: 1.1204159259796143
Epoch 950, training loss: 0.6558297872543335 = 0.006933677010238171 + 0.1 * 6.4889607429504395
Epoch 950, val loss: 1.1237034797668457
Epoch 960, training loss: 0.6539599299430847 = 0.006760226096957922 + 0.1 * 6.471996784210205
Epoch 960, val loss: 1.1271307468414307
Epoch 970, training loss: 0.6540181636810303 = 0.006593502592295408 + 0.1 * 6.474246501922607
Epoch 970, val loss: 1.130568265914917
Epoch 980, training loss: 0.6571332216262817 = 0.006432726047933102 + 0.1 * 6.507004737854004
Epoch 980, val loss: 1.1337240934371948
Epoch 990, training loss: 0.6542229056358337 = 0.006278451066464186 + 0.1 * 6.47944450378418
Epoch 990, val loss: 1.1369272470474243
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.806126594543457 = 1.9464472532272339 + 0.1 * 8.596793174743652
Epoch 0, val loss: 1.9410673379898071
Epoch 10, training loss: 2.7960166931152344 = 1.9363452196121216 + 0.1 * 8.596714973449707
Epoch 10, val loss: 1.9315369129180908
Epoch 20, training loss: 2.7837142944335938 = 1.9240955114364624 + 0.1 * 8.596187591552734
Epoch 20, val loss: 1.9195303916931152
Epoch 30, training loss: 2.7659687995910645 = 1.9067890644073486 + 0.1 * 8.591797828674316
Epoch 30, val loss: 1.9023048877716064
Epoch 40, training loss: 2.737337589263916 = 1.8810112476348877 + 0.1 * 8.563261985778809
Epoch 40, val loss: 1.8769540786743164
Epoch 50, training loss: 2.6881022453308105 = 1.8450708389282227 + 0.1 * 8.430313110351562
Epoch 50, val loss: 1.8431758880615234
Epoch 60, training loss: 2.6166701316833496 = 1.8035184144973755 + 0.1 * 8.13151741027832
Epoch 60, val loss: 1.8064903020858765
Epoch 70, training loss: 2.553309440612793 = 1.7623720169067383 + 0.1 * 7.909374237060547
Epoch 70, val loss: 1.7701717615127563
Epoch 80, training loss: 2.4770381450653076 = 1.7158458232879639 + 0.1 * 7.611923694610596
Epoch 80, val loss: 1.7265820503234863
Epoch 90, training loss: 2.39408016204834 = 1.6590291261672974 + 0.1 * 7.35051155090332
Epoch 90, val loss: 1.675135612487793
Epoch 100, training loss: 2.3007521629333496 = 1.5877784490585327 + 0.1 * 7.12973690032959
Epoch 100, val loss: 1.612502098083496
Epoch 110, training loss: 2.207815647125244 = 1.5020699501037598 + 0.1 * 7.057455539703369
Epoch 110, val loss: 1.5382206439971924
Epoch 120, training loss: 2.115814447402954 = 1.4144374132156372 + 0.1 * 7.013771057128906
Epoch 120, val loss: 1.4670435190200806
Epoch 130, training loss: 2.029310703277588 = 1.3315036296844482 + 0.1 * 6.978071212768555
Epoch 130, val loss: 1.4019278287887573
Epoch 140, training loss: 1.9487128257751465 = 1.253756046295166 + 0.1 * 6.949567794799805
Epoch 140, val loss: 1.3435386419296265
Epoch 150, training loss: 1.8725264072418213 = 1.1795332431793213 + 0.1 * 6.929931163787842
Epoch 150, val loss: 1.2903223037719727
Epoch 160, training loss: 1.7980189323425293 = 1.106417179107666 + 0.1 * 6.916017055511475
Epoch 160, val loss: 1.2390995025634766
Epoch 170, training loss: 1.723968505859375 = 1.0332677364349365 + 0.1 * 6.907007217407227
Epoch 170, val loss: 1.1878998279571533
Epoch 180, training loss: 1.6494495868682861 = 0.9599521160125732 + 0.1 * 6.894974708557129
Epoch 180, val loss: 1.1362046003341675
Epoch 190, training loss: 1.5753791332244873 = 0.8871896862983704 + 0.1 * 6.881895065307617
Epoch 190, val loss: 1.0838613510131836
Epoch 200, training loss: 1.503556251525879 = 0.8166615962982178 + 0.1 * 6.868945598602295
Epoch 200, val loss: 1.0329060554504395
Epoch 210, training loss: 1.435596227645874 = 0.7502838373184204 + 0.1 * 6.853123664855957
Epoch 210, val loss: 0.9852147102355957
Epoch 220, training loss: 1.3730823993682861 = 0.6888909339904785 + 0.1 * 6.841915130615234
Epoch 220, val loss: 0.9419635534286499
Epoch 230, training loss: 1.3150713443756104 = 0.6326452493667603 + 0.1 * 6.824260234832764
Epoch 230, val loss: 0.9037653207778931
Epoch 240, training loss: 1.2614952325820923 = 0.580398440361023 + 0.1 * 6.810967922210693
Epoch 240, val loss: 0.8698766231536865
Epoch 250, training loss: 1.212410807609558 = 0.5318657755851746 + 0.1 * 6.805450439453125
Epoch 250, val loss: 0.8399547934532166
Epoch 260, training loss: 1.1661326885223389 = 0.4867575168609619 + 0.1 * 6.7937517166137695
Epoch 260, val loss: 0.8134422302246094
Epoch 270, training loss: 1.1229543685913086 = 0.4441837668418884 + 0.1 * 6.78770637512207
Epoch 270, val loss: 0.7897723913192749
Epoch 280, training loss: 1.0820399522781372 = 0.40368586778640747 + 0.1 * 6.783540725708008
Epoch 280, val loss: 0.7687088847160339
Epoch 290, training loss: 1.0439889430999756 = 0.3652661144733429 + 0.1 * 6.787228107452393
Epoch 290, val loss: 0.7502805590629578
Epoch 300, training loss: 1.006991982460022 = 0.32887861132621765 + 0.1 * 6.78113317489624
Epoch 300, val loss: 0.7344603538513184
Epoch 310, training loss: 0.9718660116195679 = 0.2942812740802765 + 0.1 * 6.7758469581604
Epoch 310, val loss: 0.7211800217628479
Epoch 320, training loss: 0.9390988945960999 = 0.26168954372406006 + 0.1 * 6.7740936279296875
Epoch 320, val loss: 0.7107818722724915
Epoch 330, training loss: 0.9086954593658447 = 0.23152543604373932 + 0.1 * 6.771700382232666
Epoch 330, val loss: 0.7036257386207581
Epoch 340, training loss: 0.8824564814567566 = 0.2042214721441269 + 0.1 * 6.782349586486816
Epoch 340, val loss: 0.6998987793922424
Epoch 350, training loss: 0.8573376536369324 = 0.18017898499965668 + 0.1 * 6.7715864181518555
Epoch 350, val loss: 0.6992879509925842
Epoch 360, training loss: 0.8357371687889099 = 0.1592157632112503 + 0.1 * 6.765213966369629
Epoch 360, val loss: 0.7016320824623108
Epoch 370, training loss: 0.8172831535339355 = 0.14105777442455292 + 0.1 * 6.762253284454346
Epoch 370, val loss: 0.7064984440803528
Epoch 380, training loss: 0.8019838333129883 = 0.12543538212776184 + 0.1 * 6.765484809875488
Epoch 380, val loss: 0.7133445739746094
Epoch 390, training loss: 0.7881045937538147 = 0.11203921586275101 + 0.1 * 6.760653495788574
Epoch 390, val loss: 0.7215399742126465
Epoch 400, training loss: 0.7757124900817871 = 0.10043929517269135 + 0.1 * 6.752731800079346
Epoch 400, val loss: 0.7309362888336182
Epoch 410, training loss: 0.7652410864830017 = 0.0903393104672432 + 0.1 * 6.749017715454102
Epoch 410, val loss: 0.7412115931510925
Epoch 420, training loss: 0.7560670375823975 = 0.08150413632392883 + 0.1 * 6.74562931060791
Epoch 420, val loss: 0.7521978616714478
Epoch 430, training loss: 0.7481863498687744 = 0.07376661896705627 + 0.1 * 6.744197368621826
Epoch 430, val loss: 0.7636353373527527
Epoch 440, training loss: 0.7409590482711792 = 0.06697004288434982 + 0.1 * 6.739889621734619
Epoch 440, val loss: 0.7752677798271179
Epoch 450, training loss: 0.7349925637245178 = 0.060998622328042984 + 0.1 * 6.739939212799072
Epoch 450, val loss: 0.7870365381240845
Epoch 460, training loss: 0.7291421890258789 = 0.05574972927570343 + 0.1 * 6.733924865722656
Epoch 460, val loss: 0.7985547184944153
Epoch 470, training loss: 0.7238166928291321 = 0.051092199981212616 + 0.1 * 6.727244853973389
Epoch 470, val loss: 0.8100894093513489
Epoch 480, training loss: 0.72037273645401 = 0.04694066196680069 + 0.1 * 6.734320163726807
Epoch 480, val loss: 0.8216015100479126
Epoch 490, training loss: 0.7150822281837463 = 0.043250832706689835 + 0.1 * 6.718313694000244
Epoch 490, val loss: 0.8329140543937683
Epoch 500, training loss: 0.7113208770751953 = 0.03995021805167198 + 0.1 * 6.713706970214844
Epoch 500, val loss: 0.8440712094306946
Epoch 510, training loss: 0.7079631090164185 = 0.03698473423719406 + 0.1 * 6.709783554077148
Epoch 510, val loss: 0.8550808429718018
Epoch 520, training loss: 0.7046994566917419 = 0.034322645515203476 + 0.1 * 6.703768253326416
Epoch 520, val loss: 0.8659281134605408
Epoch 530, training loss: 0.70216304063797 = 0.03193175047636032 + 0.1 * 6.70231294631958
Epoch 530, val loss: 0.8763404488563538
Epoch 540, training loss: 0.7005981206893921 = 0.029770048335194588 + 0.1 * 6.708280086517334
Epoch 540, val loss: 0.8866303563117981
Epoch 550, training loss: 0.6978467702865601 = 0.02782244049012661 + 0.1 * 6.7002434730529785
Epoch 550, val loss: 0.8966580033302307
Epoch 560, training loss: 0.6943730711936951 = 0.026058165356516838 + 0.1 * 6.683149337768555
Epoch 560, val loss: 0.9063202738761902
Epoch 570, training loss: 0.6939382553100586 = 0.02445283904671669 + 0.1 * 6.694853782653809
Epoch 570, val loss: 0.915901243686676
Epoch 580, training loss: 0.6907317042350769 = 0.02299373969435692 + 0.1 * 6.677379608154297
Epoch 580, val loss: 0.9250973463058472
Epoch 590, training loss: 0.6891710758209229 = 0.021660437807440758 + 0.1 * 6.675106048583984
Epoch 590, val loss: 0.9341250061988831
Epoch 600, training loss: 0.6885564923286438 = 0.020446134731173515 + 0.1 * 6.681103706359863
Epoch 600, val loss: 0.9429950714111328
Epoch 610, training loss: 0.6856186389923096 = 0.019338218495249748 + 0.1 * 6.66280460357666
Epoch 610, val loss: 0.951265811920166
Epoch 620, training loss: 0.6835887432098389 = 0.01831870526075363 + 0.1 * 6.652699947357178
Epoch 620, val loss: 0.9595001339912415
Epoch 630, training loss: 0.6848191618919373 = 0.017377721145749092 + 0.1 * 6.674414157867432
Epoch 630, val loss: 0.9676206111907959
Epoch 640, training loss: 0.6812443733215332 = 0.016513191163539886 + 0.1 * 6.647312164306641
Epoch 640, val loss: 0.9755009412765503
Epoch 650, training loss: 0.6794309616088867 = 0.015714984387159348 + 0.1 * 6.63715934753418
Epoch 650, val loss: 0.9829448461532593
Epoch 660, training loss: 0.6781556010246277 = 0.014976459555327892 + 0.1 * 6.631791591644287
Epoch 660, val loss: 0.9905231595039368
Epoch 670, training loss: 0.6769784688949585 = 0.014297605492174625 + 0.1 * 6.6268086433410645
Epoch 670, val loss: 0.9972956776618958
Epoch 680, training loss: 0.675825834274292 = 0.013666712678968906 + 0.1 * 6.621591091156006
Epoch 680, val loss: 1.0040537118911743
Epoch 690, training loss: 0.6748433113098145 = 0.01307725440710783 + 0.1 * 6.6176605224609375
Epoch 690, val loss: 1.0106691122055054
Epoch 700, training loss: 0.6736655831336975 = 0.012527084909379482 + 0.1 * 6.611384391784668
Epoch 700, val loss: 1.0173097848892212
Epoch 710, training loss: 0.6731544733047485 = 0.01201475877314806 + 0.1 * 6.6113972663879395
Epoch 710, val loss: 1.0234053134918213
Epoch 720, training loss: 0.6717489361763 = 0.011533834971487522 + 0.1 * 6.602150917053223
Epoch 720, val loss: 1.0294700860977173
Epoch 730, training loss: 0.6761136054992676 = 0.0110831493511796 + 0.1 * 6.650304794311523
Epoch 730, val loss: 1.0355204343795776
Epoch 740, training loss: 0.671074628829956 = 0.010664217174053192 + 0.1 * 6.604104042053223
Epoch 740, val loss: 1.0412174463272095
Epoch 750, training loss: 0.6695750951766968 = 0.010272478684782982 + 0.1 * 6.5930256843566895
Epoch 750, val loss: 1.0464909076690674
Epoch 760, training loss: 0.6685156226158142 = 0.00990197155624628 + 0.1 * 6.5861358642578125
Epoch 760, val loss: 1.0518310070037842
Epoch 770, training loss: 0.6676382422447205 = 0.009552319534122944 + 0.1 * 6.580859184265137
Epoch 770, val loss: 1.0572329759597778
Epoch 780, training loss: 0.6672334671020508 = 0.009223422966897488 + 0.1 * 6.5801005363464355
Epoch 780, val loss: 1.0622888803482056
Epoch 790, training loss: 0.6678938865661621 = 0.008912279270589352 + 0.1 * 6.589816093444824
Epoch 790, val loss: 1.0672825574874878
Epoch 800, training loss: 0.6662786602973938 = 0.008618450723588467 + 0.1 * 6.576601505279541
Epoch 800, val loss: 1.072137475013733
Epoch 810, training loss: 0.6656413078308105 = 0.008340691216289997 + 0.1 * 6.573005676269531
Epoch 810, val loss: 1.0768764019012451
Epoch 820, training loss: 0.6650142073631287 = 0.008077443577349186 + 0.1 * 6.569367408752441
Epoch 820, val loss: 1.0816395282745361
Epoch 830, training loss: 0.6638900637626648 = 0.007828978821635246 + 0.1 * 6.560610771179199
Epoch 830, val loss: 1.0860258340835571
Epoch 840, training loss: 0.6651473045349121 = 0.0075928703881800175 + 0.1 * 6.575544357299805
Epoch 840, val loss: 1.0904852151870728
Epoch 850, training loss: 0.6627309918403625 = 0.007368955295532942 + 0.1 * 6.553619861602783
Epoch 850, val loss: 1.0947452783584595
Epoch 860, training loss: 0.6635789275169373 = 0.0071565802209079266 + 0.1 * 6.564223766326904
Epoch 860, val loss: 1.0988012552261353
Epoch 870, training loss: 0.6629617810249329 = 0.006953806150704622 + 0.1 * 6.560079574584961
Epoch 870, val loss: 1.1030763387680054
Epoch 880, training loss: 0.6618127226829529 = 0.0067621273919939995 + 0.1 * 6.550505638122559
Epoch 880, val loss: 1.1068754196166992
Epoch 890, training loss: 0.661091148853302 = 0.006578902713954449 + 0.1 * 6.545122146606445
Epoch 890, val loss: 1.110731840133667
Epoch 900, training loss: 0.6601646542549133 = 0.006403991021215916 + 0.1 * 6.537606716156006
Epoch 900, val loss: 1.1144123077392578
Epoch 910, training loss: 0.661068320274353 = 0.006236539222300053 + 0.1 * 6.5483174324035645
Epoch 910, val loss: 1.1182398796081543
Epoch 920, training loss: 0.6602991819381714 = 0.006077170372009277 + 0.1 * 6.542220115661621
Epoch 920, val loss: 1.1218303442001343
Epoch 930, training loss: 0.6615901589393616 = 0.005925047677010298 + 0.1 * 6.556650638580322
Epoch 930, val loss: 1.1252708435058594
Epoch 940, training loss: 0.6590518355369568 = 0.005779052618891001 + 0.1 * 6.5327277183532715
Epoch 940, val loss: 1.1287583112716675
Epoch 950, training loss: 0.658348023891449 = 0.005639655981212854 + 0.1 * 6.527083396911621
Epoch 950, val loss: 1.1320321559906006
Epoch 960, training loss: 0.6592437624931335 = 0.005504969973117113 + 0.1 * 6.537387847900391
Epoch 960, val loss: 1.1355996131896973
Epoch 970, training loss: 0.6571924686431885 = 0.005376921501010656 + 0.1 * 6.518155097961426
Epoch 970, val loss: 1.1388102769851685
Epoch 980, training loss: 0.6570771932601929 = 0.005254646297544241 + 0.1 * 6.51822566986084
Epoch 980, val loss: 1.1417678594589233
Epoch 990, training loss: 0.6563643217086792 = 0.005136399995535612 + 0.1 * 6.512279510498047
Epoch 990, val loss: 1.1447162628173828
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 2.8024401664733887 = 1.9427616596221924 + 0.1 * 8.596783638000488
Epoch 0, val loss: 1.9438819885253906
Epoch 10, training loss: 2.793365001678467 = 1.9336971044540405 + 0.1 * 8.596678733825684
Epoch 10, val loss: 1.9348022937774658
Epoch 20, training loss: 2.7826709747314453 = 1.923073649406433 + 0.1 * 8.59597396850586
Epoch 20, val loss: 1.9240168333053589
Epoch 30, training loss: 2.7676870822906494 = 1.908607006072998 + 0.1 * 8.590800285339355
Epoch 30, val loss: 1.9094023704528809
Epoch 40, training loss: 2.742443561553955 = 1.8873995542526245 + 0.1 * 8.550440788269043
Epoch 40, val loss: 1.8881384134292603
Epoch 50, training loss: 2.680980682373047 = 1.857332468032837 + 0.1 * 8.236482620239258
Epoch 50, val loss: 1.858530044555664
Epoch 60, training loss: 2.5879929065704346 = 1.8230136632919312 + 0.1 * 7.6497931480407715
Epoch 60, val loss: 1.826149582862854
Epoch 70, training loss: 2.51460862159729 = 1.78907310962677 + 0.1 * 7.255354881286621
Epoch 70, val loss: 1.7943106889724731
Epoch 80, training loss: 2.4606704711914062 = 1.7525763511657715 + 0.1 * 7.080942153930664
Epoch 80, val loss: 1.7615123987197876
Epoch 90, training loss: 2.412747621536255 = 1.7112780809402466 + 0.1 * 7.014695167541504
Epoch 90, val loss: 1.7249751091003418
Epoch 100, training loss: 2.3557276725769043 = 1.657953143119812 + 0.1 * 6.977746486663818
Epoch 100, val loss: 1.6773278713226318
Epoch 110, training loss: 2.283031940460205 = 1.5877548456192017 + 0.1 * 6.952770233154297
Epoch 110, val loss: 1.6152921915054321
Epoch 120, training loss: 2.192765235900879 = 1.4994107484817505 + 0.1 * 6.933545112609863
Epoch 120, val loss: 1.5398619174957275
Epoch 130, training loss: 2.087541103363037 = 1.3959002494812012 + 0.1 * 6.916407108306885
Epoch 130, val loss: 1.4523606300354004
Epoch 140, training loss: 1.9743969440460205 = 1.2844675779342651 + 0.1 * 6.899293422698975
Epoch 140, val loss: 1.3576997518539429
Epoch 150, training loss: 1.8606736660003662 = 1.1724262237548828 + 0.1 * 6.882473468780518
Epoch 150, val loss: 1.2625826597213745
Epoch 160, training loss: 1.7510180473327637 = 1.064419150352478 + 0.1 * 6.865988731384277
Epoch 160, val loss: 1.170416235923767
Epoch 170, training loss: 1.6483314037322998 = 0.9630715847015381 + 0.1 * 6.852598667144775
Epoch 170, val loss: 1.0848770141601562
Epoch 180, training loss: 1.5510598421096802 = 0.8670688271522522 + 0.1 * 6.83991003036499
Epoch 180, val loss: 1.0044193267822266
Epoch 190, training loss: 1.4582335948944092 = 0.7753517627716064 + 0.1 * 6.828817844390869
Epoch 190, val loss: 0.9283282160758972
Epoch 200, training loss: 1.3715659379959106 = 0.6903039216995239 + 0.1 * 6.812620162963867
Epoch 200, val loss: 0.8589493632316589
Epoch 210, training loss: 1.2923765182495117 = 0.6127538084983826 + 0.1 * 6.796226501464844
Epoch 210, val loss: 0.7974734902381897
Epoch 220, training loss: 1.2218642234802246 = 0.5436139106750488 + 0.1 * 6.782503604888916
Epoch 220, val loss: 0.7461243271827698
Epoch 230, training loss: 1.1578847169876099 = 0.48130518198013306 + 0.1 * 6.7657952308654785
Epoch 230, val loss: 0.703819990158081
Epoch 240, training loss: 1.1001161336898804 = 0.4243689477443695 + 0.1 * 6.757472038269043
Epoch 240, val loss: 0.6688822507858276
Epoch 250, training loss: 1.0473721027374268 = 0.37292712926864624 + 0.1 * 6.744449138641357
Epoch 250, val loss: 0.6398006081581116
Epoch 260, training loss: 0.9998384714126587 = 0.3261442482471466 + 0.1 * 6.736941814422607
Epoch 260, val loss: 0.6150347590446472
Epoch 270, training loss: 0.9584522247314453 = 0.2841357886791229 + 0.1 * 6.7431640625
Epoch 270, val loss: 0.5942178964614868
Epoch 280, training loss: 0.9208346605300903 = 0.2474995255470276 + 0.1 * 6.733351230621338
Epoch 280, val loss: 0.5777079463005066
Epoch 290, training loss: 0.8877776861190796 = 0.21627290546894073 + 0.1 * 6.715047836303711
Epoch 290, val loss: 0.5653067827224731
Epoch 300, training loss: 0.8605968356132507 = 0.1897016167640686 + 0.1 * 6.708951950073242
Epoch 300, val loss: 0.5567494034767151
Epoch 310, training loss: 0.8371727466583252 = 0.16719560325145721 + 0.1 * 6.699770927429199
Epoch 310, val loss: 0.5516628623008728
Epoch 320, training loss: 0.8180944919586182 = 0.14818283915519714 + 0.1 * 6.699116230010986
Epoch 320, val loss: 0.5495207905769348
Epoch 330, training loss: 0.8019140362739563 = 0.1321975588798523 + 0.1 * 6.697164535522461
Epoch 330, val loss: 0.5497050881385803
Epoch 340, training loss: 0.7904236316680908 = 0.11854515224695206 + 0.1 * 6.718784332275391
Epoch 340, val loss: 0.551771879196167
Epoch 350, training loss: 0.7752294540405273 = 0.1068856418132782 + 0.1 * 6.683437824249268
Epoch 350, val loss: 0.555133044719696
Epoch 360, training loss: 0.7641735672950745 = 0.09674444049596786 + 0.1 * 6.674291610717773
Epoch 360, val loss: 0.5596924424171448
Epoch 370, training loss: 0.7546675801277161 = 0.08783376216888428 + 0.1 * 6.668337821960449
Epoch 370, val loss: 0.5650813579559326
Epoch 380, training loss: 0.7469786405563354 = 0.07999740540981293 + 0.1 * 6.669812202453613
Epoch 380, val loss: 0.5711994767189026
Epoch 390, training loss: 0.7393413782119751 = 0.07309789955615997 + 0.1 * 6.6624345779418945
Epoch 390, val loss: 0.5777790546417236
Epoch 400, training loss: 0.7322957515716553 = 0.06696110218763351 + 0.1 * 6.653346061706543
Epoch 400, val loss: 0.5848139524459839
Epoch 410, training loss: 0.7267856001853943 = 0.06147513538599014 + 0.1 * 6.653104305267334
Epoch 410, val loss: 0.5921468734741211
Epoch 420, training loss: 0.7207402586936951 = 0.056616779416799545 + 0.1 * 6.641234874725342
Epoch 420, val loss: 0.5996630191802979
Epoch 430, training loss: 0.7169331312179565 = 0.052294500172138214 + 0.1 * 6.64638614654541
Epoch 430, val loss: 0.6072078347206116
Epoch 440, training loss: 0.7124731540679932 = 0.04840158298611641 + 0.1 * 6.640715599060059
Epoch 440, val loss: 0.6149128675460815
Epoch 450, training loss: 0.7084563374519348 = 0.044898100197315216 + 0.1 * 6.635582447052002
Epoch 450, val loss: 0.6225709319114685
Epoch 460, training loss: 0.7040669918060303 = 0.041736356914043427 + 0.1 * 6.6233062744140625
Epoch 460, val loss: 0.6303348541259766
Epoch 470, training loss: 0.7004620432853699 = 0.03887758031487465 + 0.1 * 6.615844249725342
Epoch 470, val loss: 0.638085126876831
Epoch 480, training loss: 0.6978488564491272 = 0.03629293292760849 + 0.1 * 6.6155595779418945
Epoch 480, val loss: 0.64566570520401
Epoch 490, training loss: 0.6955353021621704 = 0.033940888941287994 + 0.1 * 6.615943908691406
Epoch 490, val loss: 0.6532933115959167
Epoch 500, training loss: 0.6924899816513062 = 0.03180328384041786 + 0.1 * 6.606866359710693
Epoch 500, val loss: 0.660779595375061
Epoch 510, training loss: 0.690470814704895 = 0.029854942113161087 + 0.1 * 6.60615873336792
Epoch 510, val loss: 0.668114185333252
Epoch 520, training loss: 0.689409613609314 = 0.02808874472975731 + 0.1 * 6.613208770751953
Epoch 520, val loss: 0.6753725409507751
Epoch 530, training loss: 0.6857687830924988 = 0.02648509107530117 + 0.1 * 6.592836856842041
Epoch 530, val loss: 0.6823235154151917
Epoch 540, training loss: 0.6829553842544556 = 0.02501148357987404 + 0.1 * 6.579439163208008
Epoch 540, val loss: 0.6892739534378052
Epoch 550, training loss: 0.6831702589988708 = 0.023651737719774246 + 0.1 * 6.595185279846191
Epoch 550, val loss: 0.696089506149292
Epoch 560, training loss: 0.6809055209159851 = 0.022405147552490234 + 0.1 * 6.58500337600708
Epoch 560, val loss: 0.7027066349983215
Epoch 570, training loss: 0.6781622171401978 = 0.021257886663079262 + 0.1 * 6.569043159484863
Epoch 570, val loss: 0.7092336416244507
Epoch 580, training loss: 0.679967999458313 = 0.020195819437503815 + 0.1 * 6.597721576690674
Epoch 580, val loss: 0.7156457901000977
Epoch 590, training loss: 0.6751601696014404 = 0.01921745575964451 + 0.1 * 6.559427261352539
Epoch 590, val loss: 0.7218326926231384
Epoch 600, training loss: 0.674648106098175 = 0.01831122674047947 + 0.1 * 6.563368797302246
Epoch 600, val loss: 0.7279391288757324
Epoch 610, training loss: 0.6733991503715515 = 0.01746837981045246 + 0.1 * 6.55930757522583
Epoch 610, val loss: 0.7338945269584656
Epoch 620, training loss: 0.6719577312469482 = 0.01668577641248703 + 0.1 * 6.552719593048096
Epoch 620, val loss: 0.7396793365478516
Epoch 630, training loss: 0.6711382269859314 = 0.015956494957208633 + 0.1 * 6.551816940307617
Epoch 630, val loss: 0.745391309261322
Epoch 640, training loss: 0.6717332005500793 = 0.015277215279638767 + 0.1 * 6.5645599365234375
Epoch 640, val loss: 0.7509573101997375
Epoch 650, training loss: 0.6681458353996277 = 0.014644192531704903 + 0.1 * 6.535016059875488
Epoch 650, val loss: 0.7563514709472656
Epoch 660, training loss: 0.6676859259605408 = 0.014051191508769989 + 0.1 * 6.536347389221191
Epoch 660, val loss: 0.7616849541664124
Epoch 670, training loss: 0.6671292185783386 = 0.013494891114532948 + 0.1 * 6.536343097686768
Epoch 670, val loss: 0.76689612865448
Epoch 680, training loss: 0.6658878922462463 = 0.012975446879863739 + 0.1 * 6.5291242599487305
Epoch 680, val loss: 0.7719438076019287
Epoch 690, training loss: 0.6668910980224609 = 0.01248712744563818 + 0.1 * 6.544039726257324
Epoch 690, val loss: 0.7769062519073486
Epoch 700, training loss: 0.6641862392425537 = 0.012028011493384838 + 0.1 * 6.521582126617432
Epoch 700, val loss: 0.781781017780304
Epoch 710, training loss: 0.6643242835998535 = 0.011595537886023521 + 0.1 * 6.527287483215332
Epoch 710, val loss: 0.7865537405014038
Epoch 720, training loss: 0.6626179218292236 = 0.011187217198312283 + 0.1 * 6.514307022094727
Epoch 720, val loss: 0.7912207841873169
Epoch 730, training loss: 0.6617071628570557 = 0.010801514610648155 + 0.1 * 6.509056568145752
Epoch 730, val loss: 0.7957953810691833
Epoch 740, training loss: 0.6611567735671997 = 0.01043780893087387 + 0.1 * 6.507189750671387
Epoch 740, val loss: 0.8002570867538452
Epoch 750, training loss: 0.6614319682121277 = 0.010093212127685547 + 0.1 * 6.513387680053711
Epoch 750, val loss: 0.8046533465385437
Epoch 760, training loss: 0.6597785353660583 = 0.009768142364919186 + 0.1 * 6.500103950500488
Epoch 760, val loss: 0.8089100122451782
Epoch 770, training loss: 0.6597907543182373 = 0.00945980940014124 + 0.1 * 6.503309726715088
Epoch 770, val loss: 0.8131111860275269
Epoch 780, training loss: 0.6587287187576294 = 0.009166356176137924 + 0.1 * 6.495623588562012
Epoch 780, val loss: 0.8172629475593567
Epoch 790, training loss: 0.659818172454834 = 0.008887886069715023 + 0.1 * 6.509302616119385
Epoch 790, val loss: 0.8213390111923218
Epoch 800, training loss: 0.6587997078895569 = 0.008623046800494194 + 0.1 * 6.501766204833984
Epoch 800, val loss: 0.8253389000892639
Epoch 810, training loss: 0.6588490605354309 = 0.008371271193027496 + 0.1 * 6.504777431488037
Epoch 810, val loss: 0.8292632699012756
Epoch 820, training loss: 0.6566424369812012 = 0.008131447248160839 + 0.1 * 6.485109806060791
Epoch 820, val loss: 0.8331285715103149
Epoch 830, training loss: 0.6585339903831482 = 0.007902952842414379 + 0.1 * 6.506309986114502
Epoch 830, val loss: 0.8369468450546265
Epoch 840, training loss: 0.6569516658782959 = 0.0076848892495036125 + 0.1 * 6.4926676750183105
Epoch 840, val loss: 0.8406175374984741
Epoch 850, training loss: 0.6563350558280945 = 0.007477237842977047 + 0.1 * 6.4885783195495605
Epoch 850, val loss: 0.8442913293838501
Epoch 860, training loss: 0.6549946069717407 = 0.007278406992554665 + 0.1 * 6.4771623611450195
Epoch 860, val loss: 0.8478692173957825
Epoch 870, training loss: 0.6549732685089111 = 0.007088484708219767 + 0.1 * 6.478847503662109
Epoch 870, val loss: 0.8514158725738525
Epoch 880, training loss: 0.6542326807975769 = 0.00690618297085166 + 0.1 * 6.473264694213867
Epoch 880, val loss: 0.8548851013183594
Epoch 890, training loss: 0.6545736193656921 = 0.006732113193720579 + 0.1 * 6.478415012359619
Epoch 890, val loss: 0.8582820296287537
Epoch 900, training loss: 0.6538152098655701 = 0.006565007381141186 + 0.1 * 6.472501754760742
Epoch 900, val loss: 0.8616473078727722
Epoch 910, training loss: 0.6543092727661133 = 0.006405153311789036 + 0.1 * 6.479040622711182
Epoch 910, val loss: 0.8649296164512634
Epoch 920, training loss: 0.6539146304130554 = 0.006251428741961718 + 0.1 * 6.4766316413879395
Epoch 920, val loss: 0.8682156801223755
Epoch 930, training loss: 0.6524425148963928 = 0.006104086525738239 + 0.1 * 6.463383674621582
Epoch 930, val loss: 0.8714107871055603
Epoch 940, training loss: 0.6527554988861084 = 0.005962402559816837 + 0.1 * 6.467930316925049
Epoch 940, val loss: 0.8745852708816528
Epoch 950, training loss: 0.6523333191871643 = 0.005825887434184551 + 0.1 * 6.46507453918457
Epoch 950, val loss: 0.877683162689209
Epoch 960, training loss: 0.6518746018409729 = 0.005694993771612644 + 0.1 * 6.461795806884766
Epoch 960, val loss: 0.8807533979415894
Epoch 970, training loss: 0.6515912413597107 = 0.005569537170231342 + 0.1 * 6.460216999053955
Epoch 970, val loss: 0.8837113380432129
Epoch 980, training loss: 0.6503936648368835 = 0.005448267329484224 + 0.1 * 6.449453830718994
Epoch 980, val loss: 0.8866963982582092
Epoch 990, training loss: 0.651030957698822 = 0.0053311861120164394 + 0.1 * 6.456997871398926
Epoch 990, val loss: 0.889630913734436
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.838165524512388
The final CL Acc:0.81358, 0.01921, The final GNN Acc:0.83694, 0.00174
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9572])
updated graph: torch.Size([2, 10640])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.806213855743408 = 1.9465266466140747 + 0.1 * 8.596871376037598
Epoch 0, val loss: 1.9368383884429932
Epoch 10, training loss: 2.796668529510498 = 1.9369853734970093 + 0.1 * 8.596832275390625
Epoch 10, val loss: 1.928223967552185
Epoch 20, training loss: 2.785255193710327 = 1.9255952835083008 + 0.1 * 8.596598625183105
Epoch 20, val loss: 1.917535662651062
Epoch 30, training loss: 2.7695343494415283 = 1.9100624322891235 + 0.1 * 8.594719886779785
Epoch 30, val loss: 1.9025808572769165
Epoch 40, training loss: 2.7452621459960938 = 1.887369990348816 + 0.1 * 8.578922271728516
Epoch 40, val loss: 1.8806177377700806
Epoch 50, training loss: 2.705536365509033 = 1.8549755811691284 + 0.1 * 8.505608558654785
Epoch 50, val loss: 1.850162148475647
Epoch 60, training loss: 2.647352933883667 = 1.8165295124053955 + 0.1 * 8.308233261108398
Epoch 60, val loss: 1.8167946338653564
Epoch 70, training loss: 2.5960400104522705 = 1.7790651321411133 + 0.1 * 8.169748306274414
Epoch 70, val loss: 1.786237120628357
Epoch 80, training loss: 2.5263490676879883 = 1.7399958372116089 + 0.1 * 7.863532066345215
Epoch 80, val loss: 1.7514173984527588
Epoch 90, training loss: 2.447801351547241 = 1.6915130615234375 + 0.1 * 7.562883377075195
Epoch 90, val loss: 1.7079336643218994
Epoch 100, training loss: 2.372941255569458 = 1.6295586824417114 + 0.1 * 7.4338250160217285
Epoch 100, val loss: 1.6546632051467896
Epoch 110, training loss: 2.288003921508789 = 1.5520683526992798 + 0.1 * 7.359355926513672
Epoch 110, val loss: 1.5868407487869263
Epoch 120, training loss: 2.194072961807251 = 1.4650276899337769 + 0.1 * 7.29045295715332
Epoch 120, val loss: 1.5124130249023438
Epoch 130, training loss: 2.097379684448242 = 1.3750826120376587 + 0.1 * 7.222970008850098
Epoch 130, val loss: 1.4394185543060303
Epoch 140, training loss: 2.0021934509277344 = 1.2857120037078857 + 0.1 * 7.164814472198486
Epoch 140, val loss: 1.3687101602554321
Epoch 150, training loss: 1.9114866256713867 = 1.1986905336380005 + 0.1 * 7.127961158752441
Epoch 150, val loss: 1.3016313314437866
Epoch 160, training loss: 1.8262598514556885 = 1.1196383237838745 + 0.1 * 7.066215515136719
Epoch 160, val loss: 1.2434122562408447
Epoch 170, training loss: 1.7491097450256348 = 1.047052264213562 + 0.1 * 7.020574569702148
Epoch 170, val loss: 1.1919842958450317
Epoch 180, training loss: 1.6790242195129395 = 0.9802702069282532 + 0.1 * 6.987539768218994
Epoch 180, val loss: 1.1461395025253296
Epoch 190, training loss: 1.6144013404846191 = 0.9184615612030029 + 0.1 * 6.95939826965332
Epoch 190, val loss: 1.104137897491455
Epoch 200, training loss: 1.5537598133087158 = 0.8597868084907532 + 0.1 * 6.939730167388916
Epoch 200, val loss: 1.0648483037948608
Epoch 210, training loss: 1.4953502416610718 = 0.8028043508529663 + 0.1 * 6.925458908081055
Epoch 210, val loss: 1.0271453857421875
Epoch 220, training loss: 1.437787413597107 = 0.746108889579773 + 0.1 * 6.91678524017334
Epoch 220, val loss: 0.9907806515693665
Epoch 230, training loss: 1.3801456689834595 = 0.6898009181022644 + 0.1 * 6.903447151184082
Epoch 230, val loss: 0.955639660358429
Epoch 240, training loss: 1.3233246803283691 = 0.6336800456047058 + 0.1 * 6.896446704864502
Epoch 240, val loss: 0.921873927116394
Epoch 250, training loss: 1.268981695175171 = 0.5783878564834595 + 0.1 * 6.905939102172852
Epoch 250, val loss: 0.8908765912055969
Epoch 260, training loss: 1.214737057685852 = 0.5261080265045166 + 0.1 * 6.886290073394775
Epoch 260, val loss: 0.8650060296058655
Epoch 270, training loss: 1.165549397468567 = 0.47771117091178894 + 0.1 * 6.878382682800293
Epoch 270, val loss: 0.844907820224762
Epoch 280, training loss: 1.1218433380126953 = 0.4336819052696228 + 0.1 * 6.8816142082214355
Epoch 280, val loss: 0.8310356140136719
Epoch 290, training loss: 1.0820767879486084 = 0.39462560415267944 + 0.1 * 6.874512195587158
Epoch 290, val loss: 0.8229401707649231
Epoch 300, training loss: 1.0455645322799683 = 0.35958144068717957 + 0.1 * 6.8598313331604
Epoch 300, val loss: 0.8194115161895752
Epoch 310, training loss: 1.0124808549880981 = 0.32756373286247253 + 0.1 * 6.8491716384887695
Epoch 310, val loss: 0.8197580575942993
Epoch 320, training loss: 0.9825262427330017 = 0.29807841777801514 + 0.1 * 6.844478130340576
Epoch 320, val loss: 0.8231587409973145
Epoch 330, training loss: 0.954715371131897 = 0.27092596888542175 + 0.1 * 6.837893486022949
Epoch 330, val loss: 0.8289443850517273
Epoch 340, training loss: 0.9289506673812866 = 0.24591174721717834 + 0.1 * 6.830389499664307
Epoch 340, val loss: 0.8363980650901794
Epoch 350, training loss: 0.9050435423851013 = 0.22302347421646118 + 0.1 * 6.820200443267822
Epoch 350, val loss: 0.8451261520385742
Epoch 360, training loss: 0.8844375610351562 = 0.20212846994400024 + 0.1 * 6.823090553283691
Epoch 360, val loss: 0.8549694418907166
Epoch 370, training loss: 0.8641191720962524 = 0.18319816887378693 + 0.1 * 6.809210300445557
Epoch 370, val loss: 0.8657531142234802
Epoch 380, training loss: 0.8467088937759399 = 0.16616491973400116 + 0.1 * 6.805439472198486
Epoch 380, val loss: 0.8773490786552429
Epoch 390, training loss: 0.8303109407424927 = 0.15090127289295197 + 0.1 * 6.79409646987915
Epoch 390, val loss: 0.8894770741462708
Epoch 400, training loss: 0.8156505227088928 = 0.13722121715545654 + 0.1 * 6.784292697906494
Epoch 400, val loss: 0.9022087454795837
Epoch 410, training loss: 0.8020071983337402 = 0.12497798353433609 + 0.1 * 6.770291805267334
Epoch 410, val loss: 0.9154582023620605
Epoch 420, training loss: 0.7903276085853577 = 0.11400949954986572 + 0.1 * 6.763181209564209
Epoch 420, val loss: 0.9291632771492004
Epoch 430, training loss: 0.7795996069908142 = 0.10420607775449753 + 0.1 * 6.753934860229492
Epoch 430, val loss: 0.9433141946792603
Epoch 440, training loss: 0.7706814408302307 = 0.09540284425020218 + 0.1 * 6.752786159515381
Epoch 440, val loss: 0.9577042460441589
Epoch 450, training loss: 0.7606195211410522 = 0.08750077337026596 + 0.1 * 6.73118782043457
Epoch 450, val loss: 0.9724007844924927
Epoch 460, training loss: 0.7551853060722351 = 0.08038638532161713 + 0.1 * 6.747989177703857
Epoch 460, val loss: 0.9871360659599304
Epoch 470, training loss: 0.747555673122406 = 0.0740148276090622 + 0.1 * 6.735408306121826
Epoch 470, val loss: 1.001984715461731
Epoch 480, training loss: 0.7406827211380005 = 0.06829772889614105 + 0.1 * 6.723849773406982
Epoch 480, val loss: 1.0167063474655151
Epoch 490, training loss: 0.7336331009864807 = 0.06314345449209213 + 0.1 * 6.704896450042725
Epoch 490, val loss: 1.0315560102462769
Epoch 500, training loss: 0.7293226718902588 = 0.05848470330238342 + 0.1 * 6.708379745483398
Epoch 500, val loss: 1.0460737943649292
Epoch 510, training loss: 0.7240838408470154 = 0.05427592247724533 + 0.1 * 6.6980791091918945
Epoch 510, val loss: 1.0607020854949951
Epoch 520, training loss: 0.7207581400871277 = 0.050457168370485306 + 0.1 * 6.703009605407715
Epoch 520, val loss: 1.075258731842041
Epoch 530, training loss: 0.7157283425331116 = 0.04700721055269241 + 0.1 * 6.687211513519287
Epoch 530, val loss: 1.0892890691757202
Epoch 540, training loss: 0.7118072509765625 = 0.043885327875614166 + 0.1 * 6.679218769073486
Epoch 540, val loss: 1.1030807495117188
Epoch 550, training loss: 0.7089693546295166 = 0.04104386270046234 + 0.1 * 6.679254531860352
Epoch 550, val loss: 1.1169005632400513
Epoch 560, training loss: 0.705467164516449 = 0.03845473751425743 + 0.1 * 6.670124530792236
Epoch 560, val loss: 1.1302175521850586
Epoch 570, training loss: 0.7038744688034058 = 0.036093246191740036 + 0.1 * 6.677812099456787
Epoch 570, val loss: 1.1433782577514648
Epoch 580, training loss: 0.6997451782226562 = 0.033937763422727585 + 0.1 * 6.658074378967285
Epoch 580, val loss: 1.1564185619354248
Epoch 590, training loss: 0.6979190707206726 = 0.03196549788117409 + 0.1 * 6.659535884857178
Epoch 590, val loss: 1.1687333583831787
Epoch 600, training loss: 0.6954657435417175 = 0.030163008719682693 + 0.1 * 6.653027534484863
Epoch 600, val loss: 1.181264042854309
Epoch 610, training loss: 0.6940946578979492 = 0.028504937887191772 + 0.1 * 6.6558966636657715
Epoch 610, val loss: 1.1933305263519287
Epoch 620, training loss: 0.6922913789749146 = 0.026981541886925697 + 0.1 * 6.653098106384277
Epoch 620, val loss: 1.204979658126831
Epoch 630, training loss: 0.6894400119781494 = 0.025578558444976807 + 0.1 * 6.638614177703857
Epoch 630, val loss: 1.216520071029663
Epoch 640, training loss: 0.6891983151435852 = 0.024280531331896782 + 0.1 * 6.649177551269531
Epoch 640, val loss: 1.2276673316955566
Epoch 650, training loss: 0.6876552700996399 = 0.023081641644239426 + 0.1 * 6.645736217498779
Epoch 650, val loss: 1.2384954690933228
Epoch 660, training loss: 0.6848964691162109 = 0.021973926573991776 + 0.1 * 6.629225730895996
Epoch 660, val loss: 1.2492585182189941
Epoch 670, training loss: 0.6854517459869385 = 0.0209442637860775 + 0.1 * 6.645074844360352
Epoch 670, val loss: 1.2595610618591309
Epoch 680, training loss: 0.6823254823684692 = 0.019989684224128723 + 0.1 * 6.623358249664307
Epoch 680, val loss: 1.2698036432266235
Epoch 690, training loss: 0.6812439560890198 = 0.01910036988556385 + 0.1 * 6.621435642242432
Epoch 690, val loss: 1.279599905014038
Epoch 700, training loss: 0.6794427633285522 = 0.018270879983901978 + 0.1 * 6.61171817779541
Epoch 700, val loss: 1.2894803285598755
Epoch 710, training loss: 0.6796519756317139 = 0.017497269436717033 + 0.1 * 6.621547222137451
Epoch 710, val loss: 1.2987475395202637
Epoch 720, training loss: 0.6774129867553711 = 0.016774892807006836 + 0.1 * 6.606380939483643
Epoch 720, val loss: 1.308064579963684
Epoch 730, training loss: 0.6756799817085266 = 0.016098685562610626 + 0.1 * 6.595812797546387
Epoch 730, val loss: 1.317168116569519
Epoch 740, training loss: 0.6776261329650879 = 0.015463785268366337 + 0.1 * 6.621623516082764
Epoch 740, val loss: 1.3259556293487549
Epoch 750, training loss: 0.6746379733085632 = 0.014867987483739853 + 0.1 * 6.597700119018555
Epoch 750, val loss: 1.334526777267456
Epoch 760, training loss: 0.6735277771949768 = 0.014309220016002655 + 0.1 * 6.5921854972839355
Epoch 760, val loss: 1.342964768409729
Epoch 770, training loss: 0.6721747517585754 = 0.013782388530671597 + 0.1 * 6.583923816680908
Epoch 770, val loss: 1.3513964414596558
Epoch 780, training loss: 0.6721052527427673 = 0.013285013847053051 + 0.1 * 6.588201999664307
Epoch 780, val loss: 1.3590747117996216
Epoch 790, training loss: 0.6718144416809082 = 0.012818229384720325 + 0.1 * 6.589962005615234
Epoch 790, val loss: 1.3672904968261719
Epoch 800, training loss: 0.6694292426109314 = 0.012375901453197002 + 0.1 * 6.570533275604248
Epoch 800, val loss: 1.374876618385315
Epoch 810, training loss: 0.6715505123138428 = 0.011957828886806965 + 0.1 * 6.595926761627197
Epoch 810, val loss: 1.382242202758789
Epoch 820, training loss: 0.6683147549629211 = 0.011563495732843876 + 0.1 * 6.567512512207031
Epoch 820, val loss: 1.389846920967102
Epoch 830, training loss: 0.6671255230903625 = 0.011189297772943974 + 0.1 * 6.559362411499023
Epoch 830, val loss: 1.3971781730651855
Epoch 840, training loss: 0.6679965853691101 = 0.010833583772182465 + 0.1 * 6.571630001068115
Epoch 840, val loss: 1.4044018983840942
Epoch 850, training loss: 0.6665952205657959 = 0.010496268048882484 + 0.1 * 6.5609893798828125
Epoch 850, val loss: 1.4111478328704834
Epoch 860, training loss: 0.6680243611335754 = 0.010175954550504684 + 0.1 * 6.578484058380127
Epoch 860, val loss: 1.4181541204452515
Epoch 870, training loss: 0.6658662557601929 = 0.009871172718703747 + 0.1 * 6.559950828552246
Epoch 870, val loss: 1.4248008728027344
Epoch 880, training loss: 0.6649803519248962 = 0.009581553749740124 + 0.1 * 6.553987979888916
Epoch 880, val loss: 1.4315756559371948
Epoch 890, training loss: 0.6661432981491089 = 0.009304956533014774 + 0.1 * 6.568383693695068
Epoch 890, val loss: 1.437804937362671
Epoch 900, training loss: 0.6645013093948364 = 0.009041515178978443 + 0.1 * 6.554597854614258
Epoch 900, val loss: 1.4443544149398804
Epoch 910, training loss: 0.6637291312217712 = 0.008790255524218082 + 0.1 * 6.549388885498047
Epoch 910, val loss: 1.450486421585083
Epoch 920, training loss: 0.6638262867927551 = 0.008549829944968224 + 0.1 * 6.552764415740967
Epoch 920, val loss: 1.4566066265106201
Epoch 930, training loss: 0.662572979927063 = 0.008320425637066364 + 0.1 * 6.542525291442871
Epoch 930, val loss: 1.4627097845077515
Epoch 940, training loss: 0.662268877029419 = 0.00810087937861681 + 0.1 * 6.541679859161377
Epoch 940, val loss: 1.4682143926620483
Epoch 950, training loss: 0.6621593832969666 = 0.007891424931585789 + 0.1 * 6.542679309844971
Epoch 950, val loss: 1.474323034286499
Epoch 960, training loss: 0.6614323854446411 = 0.007690593600273132 + 0.1 * 6.537417888641357
Epoch 960, val loss: 1.4800446033477783
Epoch 970, training loss: 0.6613423228263855 = 0.007498112507164478 + 0.1 * 6.538442134857178
Epoch 970, val loss: 1.4851024150848389
Epoch 980, training loss: 0.6607068777084351 = 0.007314033806324005 + 0.1 * 6.533928394317627
Epoch 980, val loss: 1.4909520149230957
Epoch 990, training loss: 0.6592555642127991 = 0.007137738633900881 + 0.1 * 6.521178245544434
Epoch 990, val loss: 1.4963679313659668
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 2.807441234588623 = 1.9477592706680298 + 0.1 * 8.596820831298828
Epoch 0, val loss: 1.9502217769622803
Epoch 10, training loss: 2.7967913150787354 = 1.937121868133545 + 0.1 * 8.596693992614746
Epoch 10, val loss: 1.9393434524536133
Epoch 20, training loss: 2.78322696685791 = 1.9236335754394531 + 0.1 * 8.59593391418457
Epoch 20, val loss: 1.9256783723831177
Epoch 30, training loss: 2.7635247707366943 = 1.904518723487854 + 0.1 * 8.590060234069824
Epoch 30, val loss: 1.9064220190048218
Epoch 40, training loss: 2.73249888420105 = 1.8765448331832886 + 0.1 * 8.559539794921875
Epoch 40, val loss: 1.8786441087722778
Epoch 50, training loss: 2.683011770248413 = 1.8399790525436401 + 0.1 * 8.430327415466309
Epoch 50, val loss: 1.8442751169204712
Epoch 60, training loss: 2.628460168838501 = 1.8036731481552124 + 0.1 * 8.247870445251465
Epoch 60, val loss: 1.812735915184021
Epoch 70, training loss: 2.57708477973938 = 1.772905707359314 + 0.1 * 8.041790008544922
Epoch 70, val loss: 1.7856539487838745
Epoch 80, training loss: 2.500702142715454 = 1.737865686416626 + 0.1 * 7.628363609313965
Epoch 80, val loss: 1.7524569034576416
Epoch 90, training loss: 2.4226126670837402 = 1.6944483518600464 + 0.1 * 7.281642913818359
Epoch 90, val loss: 1.7149267196655273
Epoch 100, training loss: 2.3532888889312744 = 1.6363441944122314 + 0.1 * 7.1694464683532715
Epoch 100, val loss: 1.6659753322601318
Epoch 110, training loss: 2.273282527923584 = 1.5606074333190918 + 0.1 * 7.126751899719238
Epoch 110, val loss: 1.5986789464950562
Epoch 120, training loss: 2.1862375736236572 = 1.476319432258606 + 0.1 * 7.099180698394775
Epoch 120, val loss: 1.5282232761383057
Epoch 130, training loss: 2.0975472927093506 = 1.3907378911972046 + 0.1 * 7.068094730377197
Epoch 130, val loss: 1.4576237201690674
Epoch 140, training loss: 2.0087037086486816 = 1.305867314338684 + 0.1 * 7.028364181518555
Epoch 140, val loss: 1.388816237449646
Epoch 150, training loss: 1.9215843677520752 = 1.222474455833435 + 0.1 * 6.991098403930664
Epoch 150, val loss: 1.3220856189727783
Epoch 160, training loss: 1.8351714611053467 = 1.1402838230133057 + 0.1 * 6.948875427246094
Epoch 160, val loss: 1.2584656476974487
Epoch 170, training loss: 1.7518799304962158 = 1.0591636896133423 + 0.1 * 6.927161693572998
Epoch 170, val loss: 1.1973731517791748
Epoch 180, training loss: 1.6707096099853516 = 0.9810927510261536 + 0.1 * 6.896168231964111
Epoch 180, val loss: 1.1398073434829712
Epoch 190, training loss: 1.5929694175720215 = 0.9057096242904663 + 0.1 * 6.872598171234131
Epoch 190, val loss: 1.0849602222442627
Epoch 200, training loss: 1.5202975273132324 = 0.8348312377929688 + 0.1 * 6.8546624183654785
Epoch 200, val loss: 1.0338836908340454
Epoch 210, training loss: 1.453765869140625 = 0.7698317766189575 + 0.1 * 6.839341640472412
Epoch 210, val loss: 0.9883856773376465
Epoch 220, training loss: 1.3931424617767334 = 0.7108150124549866 + 0.1 * 6.823275089263916
Epoch 220, val loss: 0.9487042427062988
Epoch 230, training loss: 1.3374682664871216 = 0.6569327712059021 + 0.1 * 6.805355072021484
Epoch 230, val loss: 0.9143537282943726
Epoch 240, training loss: 1.2871673107147217 = 0.6072062253952026 + 0.1 * 6.799610614776611
Epoch 240, val loss: 0.8852386474609375
Epoch 250, training loss: 1.238471269607544 = 0.561084508895874 + 0.1 * 6.773867130279541
Epoch 250, val loss: 0.860657274723053
Epoch 260, training loss: 1.1946582794189453 = 0.5173317790031433 + 0.1 * 6.773264408111572
Epoch 260, val loss: 0.8398807644844055
Epoch 270, training loss: 1.1534888744354248 = 0.47620689868927 + 0.1 * 6.772819995880127
Epoch 270, val loss: 0.8226826786994934
Epoch 280, training loss: 1.1118710041046143 = 0.43741533160209656 + 0.1 * 6.744556427001953
Epoch 280, val loss: 0.8083000183105469
Epoch 290, training loss: 1.073641061782837 = 0.4003765881061554 + 0.1 * 6.732644081115723
Epoch 290, val loss: 0.7962954640388489
Epoch 300, training loss: 1.038674235343933 = 0.3650151789188385 + 0.1 * 6.7365899085998535
Epoch 300, val loss: 0.7861312031745911
Epoch 310, training loss: 1.0040875673294067 = 0.33158591389656067 + 0.1 * 6.725016117095947
Epoch 310, val loss: 0.7779316902160645
Epoch 320, training loss: 0.9730597734451294 = 0.300123393535614 + 0.1 * 6.729363441467285
Epoch 320, val loss: 0.7716798782348633
Epoch 330, training loss: 0.9416090250015259 = 0.2711265981197357 + 0.1 * 6.704823970794678
Epoch 330, val loss: 0.7675291895866394
Epoch 340, training loss: 0.9150691032409668 = 0.24458745121955872 + 0.1 * 6.704816818237305
Epoch 340, val loss: 0.7655149102210999
Epoch 350, training loss: 0.8903848528862 = 0.22065545618534088 + 0.1 * 6.697294235229492
Epoch 350, val loss: 0.765598475933075
Epoch 360, training loss: 0.8681751489639282 = 0.19919545948505402 + 0.1 * 6.6897969245910645
Epoch 360, val loss: 0.7675217390060425
Epoch 370, training loss: 0.8489771485328674 = 0.18009567260742188 + 0.1 * 6.688814640045166
Epoch 370, val loss: 0.7711573839187622
Epoch 380, training loss: 0.8321449756622314 = 0.1631079912185669 + 0.1 * 6.690369606018066
Epoch 380, val loss: 0.7761776447296143
Epoch 390, training loss: 0.8164302110671997 = 0.14804428815841675 + 0.1 * 6.683858871459961
Epoch 390, val loss: 0.7824656963348389
Epoch 400, training loss: 0.801079273223877 = 0.1346440464258194 + 0.1 * 6.6643524169921875
Epoch 400, val loss: 0.789720892906189
Epoch 410, training loss: 0.7892882823944092 = 0.12267131358385086 + 0.1 * 6.6661696434021
Epoch 410, val loss: 0.7978288531303406
Epoch 420, training loss: 0.7787038087844849 = 0.11200200021266937 + 0.1 * 6.667017936706543
Epoch 420, val loss: 0.8065337538719177
Epoch 430, training loss: 0.7682030200958252 = 0.10248227417469025 + 0.1 * 6.657207489013672
Epoch 430, val loss: 0.8157171607017517
Epoch 440, training loss: 0.7586225271224976 = 0.09398023039102554 + 0.1 * 6.646422386169434
Epoch 440, val loss: 0.8252027034759521
Epoch 450, training loss: 0.7507413029670715 = 0.08638980239629745 + 0.1 * 6.643515110015869
Epoch 450, val loss: 0.8349609375
Epoch 460, training loss: 0.7431290745735168 = 0.07959329336881638 + 0.1 * 6.635357856750488
Epoch 460, val loss: 0.8449313044548035
Epoch 470, training loss: 0.7393317222595215 = 0.07348349690437317 + 0.1 * 6.658481597900391
Epoch 470, val loss: 0.8548550605773926
Epoch 480, training loss: 0.7312495708465576 = 0.06800641119480133 + 0.1 * 6.632431507110596
Epoch 480, val loss: 0.8648302555084229
Epoch 490, training loss: 0.7252776622772217 = 0.06306455284357071 + 0.1 * 6.622130870819092
Epoch 490, val loss: 0.8747523427009583
Epoch 500, training loss: 0.721489667892456 = 0.058587461709976196 + 0.1 * 6.629021644592285
Epoch 500, val loss: 0.8846728205680847
Epoch 510, training loss: 0.7168749570846558 = 0.054536037147045135 + 0.1 * 6.623388767242432
Epoch 510, val loss: 0.8943684697151184
Epoch 520, training loss: 0.7130308747291565 = 0.05086107552051544 + 0.1 * 6.621697902679443
Epoch 520, val loss: 0.904105544090271
Epoch 530, training loss: 0.7081006169319153 = 0.04751421883702278 + 0.1 * 6.60586404800415
Epoch 530, val loss: 0.9136496186256409
Epoch 540, training loss: 0.705353319644928 = 0.04445631802082062 + 0.1 * 6.6089701652526855
Epoch 540, val loss: 0.9230906963348389
Epoch 550, training loss: 0.7023501992225647 = 0.04166436940431595 + 0.1 * 6.606858253479004
Epoch 550, val loss: 0.9323439598083496
Epoch 560, training loss: 0.7000969648361206 = 0.03911849483847618 + 0.1 * 6.609785079956055
Epoch 560, val loss: 0.9414002299308777
Epoch 570, training loss: 0.6970316171646118 = 0.036791007965803146 + 0.1 * 6.602406024932861
Epoch 570, val loss: 0.9504281282424927
Epoch 580, training loss: 0.6936697363853455 = 0.0346468910574913 + 0.1 * 6.59022855758667
Epoch 580, val loss: 0.9592277407646179
Epoch 590, training loss: 0.6922550201416016 = 0.03267080336809158 + 0.1 * 6.595842361450195
Epoch 590, val loss: 0.9678022861480713
Epoch 600, training loss: 0.690224826335907 = 0.030853338539600372 + 0.1 * 6.593714714050293
Epoch 600, val loss: 0.9762455821037292
Epoch 610, training loss: 0.6874091625213623 = 0.029178470373153687 + 0.1 * 6.582306861877441
Epoch 610, val loss: 0.9845951795578003
Epoch 620, training loss: 0.6870390772819519 = 0.027630029246211052 + 0.1 * 6.594090461730957
Epoch 620, val loss: 0.9926419854164124
Epoch 630, training loss: 0.6838914752006531 = 0.02619827724993229 + 0.1 * 6.576931953430176
Epoch 630, val loss: 1.0005896091461182
Epoch 640, training loss: 0.6825956702232361 = 0.024871574714779854 + 0.1 * 6.577240467071533
Epoch 640, val loss: 1.0083191394805908
Epoch 650, training loss: 0.6821051836013794 = 0.023643681779503822 + 0.1 * 6.584615230560303
Epoch 650, val loss: 1.0158122777938843
Epoch 660, training loss: 0.68000727891922 = 0.022505152970552444 + 0.1 * 6.575020790100098
Epoch 660, val loss: 1.023241639137268
Epoch 670, training loss: 0.6777075529098511 = 0.021445363759994507 + 0.1 * 6.562621593475342
Epoch 670, val loss: 1.0304127931594849
Epoch 680, training loss: 0.6788672804832458 = 0.020457740873098373 + 0.1 * 6.584095478057861
Epoch 680, val loss: 1.0375195741653442
Epoch 690, training loss: 0.6771014928817749 = 0.019538801163434982 + 0.1 * 6.575626373291016
Epoch 690, val loss: 1.0442813634872437
Epoch 700, training loss: 0.6754415035247803 = 0.018682489171624184 + 0.1 * 6.567590236663818
Epoch 700, val loss: 1.0510647296905518
Epoch 710, training loss: 0.6741669774055481 = 0.017882032319903374 + 0.1 * 6.562849044799805
Epoch 710, val loss: 1.0576046705245972
Epoch 720, training loss: 0.6754721403121948 = 0.017132556065917015 + 0.1 * 6.583395481109619
Epoch 720, val loss: 1.063948392868042
Epoch 730, training loss: 0.6722114086151123 = 0.01643264666199684 + 0.1 * 6.5577874183654785
Epoch 730, val loss: 1.0702364444732666
Epoch 740, training loss: 0.6713815927505493 = 0.015775615349411964 + 0.1 * 6.55605936050415
Epoch 740, val loss: 1.0763460397720337
Epoch 750, training loss: 0.669400691986084 = 0.01515783742070198 + 0.1 * 6.542428493499756
Epoch 750, val loss: 1.0822619199752808
Epoch 760, training loss: 0.6718498468399048 = 0.014577604830265045 + 0.1 * 6.5727219581604
Epoch 760, val loss: 1.0880879163742065
Epoch 770, training loss: 0.6686848402023315 = 0.014032548293471336 + 0.1 * 6.546523094177246
Epoch 770, val loss: 1.093697190284729
Epoch 780, training loss: 0.667870283126831 = 0.013519436120986938 + 0.1 * 6.543508052825928
Epoch 780, val loss: 1.0993586778640747
Epoch 790, training loss: 0.6671090722084045 = 0.013034659437835217 + 0.1 * 6.540744304656982
Epoch 790, val loss: 1.1046231985092163
Epoch 800, training loss: 0.6668939590454102 = 0.012578601948916912 + 0.1 * 6.543153285980225
Epoch 800, val loss: 1.110001564025879
Epoch 810, training loss: 0.6654692888259888 = 0.0121461758390069 + 0.1 * 6.533231258392334
Epoch 810, val loss: 1.1152105331420898
Epoch 820, training loss: 0.6655974388122559 = 0.01173701323568821 + 0.1 * 6.538604259490967
Epoch 820, val loss: 1.1202577352523804
Epoch 830, training loss: 0.6638516783714294 = 0.011349907144904137 + 0.1 * 6.525017738342285
Epoch 830, val loss: 1.1251144409179688
Epoch 840, training loss: 0.664333164691925 = 0.010983278974890709 + 0.1 * 6.533498764038086
Epoch 840, val loss: 1.1300575733184814
Epoch 850, training loss: 0.6647177934646606 = 0.01063553337007761 + 0.1 * 6.5408220291137695
Epoch 850, val loss: 1.1347321271896362
Epoch 860, training loss: 0.6632382273674011 = 0.010305275209248066 + 0.1 * 6.529329299926758
Epoch 860, val loss: 1.1393548250198364
Epoch 870, training loss: 0.6629902124404907 = 0.009991411119699478 + 0.1 * 6.529987812042236
Epoch 870, val loss: 1.1438958644866943
Epoch 880, training loss: 0.6638717651367188 = 0.009692647494375706 + 0.1 * 6.541790962219238
Epoch 880, val loss: 1.148297905921936
Epoch 890, training loss: 0.660585880279541 = 0.009409049525856972 + 0.1 * 6.511767864227295
Epoch 890, val loss: 1.152592420578003
Epoch 900, training loss: 0.6604762673377991 = 0.009138987399637699 + 0.1 * 6.513372421264648
Epoch 900, val loss: 1.1569280624389648
Epoch 910, training loss: 0.6609642505645752 = 0.008880607783794403 + 0.1 * 6.520836353302002
Epoch 910, val loss: 1.1610075235366821
Epoch 920, training loss: 0.6615831255912781 = 0.008634260855615139 + 0.1 * 6.529488563537598
Epoch 920, val loss: 1.1651005744934082
Epoch 930, training loss: 0.6601492166519165 = 0.00839988887310028 + 0.1 * 6.517492771148682
Epoch 930, val loss: 1.1690545082092285
Epoch 940, training loss: 0.6597183346748352 = 0.00817570649087429 + 0.1 * 6.515425682067871
Epoch 940, val loss: 1.1730672121047974
Epoch 950, training loss: 0.6588180065155029 = 0.00796144362539053 + 0.1 * 6.508565425872803
Epoch 950, val loss: 1.176855206489563
Epoch 960, training loss: 0.6589785218238831 = 0.007755972445011139 + 0.1 * 6.512225151062012
Epoch 960, val loss: 1.180687427520752
Epoch 970, training loss: 0.6581580638885498 = 0.007559322286397219 + 0.1 * 6.505987644195557
Epoch 970, val loss: 1.184354305267334
Epoch 980, training loss: 0.6598193049430847 = 0.007370528765022755 + 0.1 * 6.524487495422363
Epoch 980, val loss: 1.1880964040756226
Epoch 990, training loss: 0.6579995155334473 = 0.007189562078565359 + 0.1 * 6.508099555969238
Epoch 990, val loss: 1.1914992332458496
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 2.8111863136291504 = 1.9515011310577393 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.9423621892929077
Epoch 10, training loss: 2.8002798557281494 = 1.9405993223190308 + 0.1 * 8.59680461883545
Epoch 10, val loss: 1.9312489032745361
Epoch 20, training loss: 2.78749680519104 = 1.9278473854064941 + 0.1 * 8.596494674682617
Epoch 20, val loss: 1.9181556701660156
Epoch 30, training loss: 2.770303726196289 = 1.9108985662460327 + 0.1 * 8.5940523147583
Epoch 30, val loss: 1.9009846448898315
Epoch 40, training loss: 2.7443368434906006 = 1.8867048025131226 + 0.1 * 8.57632064819336
Epoch 40, val loss: 1.8771004676818848
Epoch 50, training loss: 2.7030582427978516 = 1.8534727096557617 + 0.1 * 8.495854377746582
Epoch 50, val loss: 1.8460166454315186
Epoch 60, training loss: 2.643414258956909 = 1.815986156463623 + 0.1 * 8.27428150177002
Epoch 60, val loss: 1.8141132593154907
Epoch 70, training loss: 2.595959186553955 = 1.7813031673431396 + 0.1 * 8.14655876159668
Epoch 70, val loss: 1.7868622541427612
Epoch 80, training loss: 2.531611442565918 = 1.7453209161758423 + 0.1 * 7.8629045486450195
Epoch 80, val loss: 1.7551331520080566
Epoch 90, training loss: 2.4601778984069824 = 1.7004523277282715 + 0.1 * 7.597254276275635
Epoch 90, val loss: 1.7148956060409546
Epoch 100, training loss: 2.3811111450195312 = 1.642350673675537 + 0.1 * 7.3876051902771
Epoch 100, val loss: 1.665437936782837
Epoch 110, training loss: 2.2990221977233887 = 1.5674386024475098 + 0.1 * 7.315836429595947
Epoch 110, val loss: 1.598123550415039
Epoch 120, training loss: 2.205418109893799 = 1.4811688661575317 + 0.1 * 7.242491722106934
Epoch 120, val loss: 1.5231882333755493
Epoch 130, training loss: 2.1093082427978516 = 1.3909920454025269 + 0.1 * 7.183160781860352
Epoch 130, val loss: 1.4485951662063599
Epoch 140, training loss: 2.01499605178833 = 1.299686074256897 + 0.1 * 7.1530985832214355
Epoch 140, val loss: 1.3770519495010376
Epoch 150, training loss: 1.9215046167373657 = 1.2082983255386353 + 0.1 * 7.132062911987305
Epoch 150, val loss: 1.3075019121170044
Epoch 160, training loss: 1.8281257152557373 = 1.1167060136795044 + 0.1 * 7.114197731018066
Epoch 160, val loss: 1.2403687238693237
Epoch 170, training loss: 1.7345877885818481 = 1.025928258895874 + 0.1 * 7.086595058441162
Epoch 170, val loss: 1.1751641035079956
Epoch 180, training loss: 1.6415133476257324 = 0.9359777569770813 + 0.1 * 7.055356025695801
Epoch 180, val loss: 1.1108382940292358
Epoch 190, training loss: 1.5523128509521484 = 0.8499718904495239 + 0.1 * 7.023409843444824
Epoch 190, val loss: 1.04926335811615
Epoch 200, training loss: 1.4699292182922363 = 0.7715924978256226 + 0.1 * 6.9833664894104
Epoch 200, val loss: 0.9942433834075928
Epoch 210, training loss: 1.3984074592590332 = 0.7025725245475769 + 0.1 * 6.958348751068115
Epoch 210, val loss: 0.9482616186141968
Epoch 220, training loss: 1.3357725143432617 = 0.6429196000099182 + 0.1 * 6.928529262542725
Epoch 220, val loss: 0.9116807579994202
Epoch 230, training loss: 1.2814476490020752 = 0.5905974507331848 + 0.1 * 6.908502101898193
Epoch 230, val loss: 0.8833353519439697
Epoch 240, training loss: 1.233620524406433 = 0.5446317195892334 + 0.1 * 6.889887809753418
Epoch 240, val loss: 0.8618502616882324
Epoch 250, training loss: 1.190807580947876 = 0.5035163164138794 + 0.1 * 6.872912406921387
Epoch 250, val loss: 0.8455424904823303
Epoch 260, training loss: 1.1525317430496216 = 0.46603402495384216 + 0.1 * 6.86497688293457
Epoch 260, val loss: 0.8332166075706482
Epoch 270, training loss: 1.1163712739944458 = 0.4316156208515167 + 0.1 * 6.8475565910339355
Epoch 270, val loss: 0.8239572048187256
Epoch 280, training loss: 1.0832581520080566 = 0.39942067861557007 + 0.1 * 6.838374614715576
Epoch 280, val loss: 0.8171674609184265
Epoch 290, training loss: 1.0521492958068848 = 0.36941829323768616 + 0.1 * 6.827310085296631
Epoch 290, val loss: 0.8122708797454834
Epoch 300, training loss: 1.0225964784622192 = 0.34146183729171753 + 0.1 * 6.811346054077148
Epoch 300, val loss: 0.808868944644928
Epoch 310, training loss: 0.9955330491065979 = 0.31542831659317017 + 0.1 * 6.801047325134277
Epoch 310, val loss: 0.8065332174301147
Epoch 320, training loss: 0.970544695854187 = 0.29124394059181213 + 0.1 * 6.7930073738098145
Epoch 320, val loss: 0.8050950169563293
Epoch 330, training loss: 0.9477131366729736 = 0.26857641339302063 + 0.1 * 6.7913665771484375
Epoch 330, val loss: 0.8040779232978821
Epoch 340, training loss: 0.9250484108924866 = 0.24732643365859985 + 0.1 * 6.777219772338867
Epoch 340, val loss: 0.803283154964447
Epoch 350, training loss: 0.9041044116020203 = 0.22719387710094452 + 0.1 * 6.769104957580566
Epoch 350, val loss: 0.8025632500648499
Epoch 360, training loss: 0.8848035931587219 = 0.20803481340408325 + 0.1 * 6.767687797546387
Epoch 360, val loss: 0.8018582463264465
Epoch 370, training loss: 0.8647419214248657 = 0.18978974223136902 + 0.1 * 6.749521732330322
Epoch 370, val loss: 0.8011680841445923
Epoch 380, training loss: 0.8466893434524536 = 0.17240649461746216 + 0.1 * 6.742828369140625
Epoch 380, val loss: 0.8005118370056152
Epoch 390, training loss: 0.8318070769309998 = 0.15609048306941986 + 0.1 * 6.757165908813477
Epoch 390, val loss: 0.8000874519348145
Epoch 400, training loss: 0.8147596716880798 = 0.1410396546125412 + 0.1 * 6.737199783325195
Epoch 400, val loss: 0.8002318739891052
Epoch 410, training loss: 0.799377977848053 = 0.12734077870845795 + 0.1 * 6.720371723175049
Epoch 410, val loss: 0.8010331988334656
Epoch 420, training loss: 0.7863379120826721 = 0.11500108987092972 + 0.1 * 6.713367938995361
Epoch 420, val loss: 0.8026354908943176
Epoch 430, training loss: 0.7774975895881653 = 0.10398108512163162 + 0.1 * 6.735165119171143
Epoch 430, val loss: 0.8053538799285889
Epoch 440, training loss: 0.7647555470466614 = 0.09426233917474747 + 0.1 * 6.704931735992432
Epoch 440, val loss: 0.8089070916175842
Epoch 450, training loss: 0.7549903392791748 = 0.0856684073805809 + 0.1 * 6.693219184875488
Epoch 450, val loss: 0.8134052753448486
Epoch 460, training loss: 0.7505582571029663 = 0.07805042713880539 + 0.1 * 6.725078105926514
Epoch 460, val loss: 0.8186750411987305
Epoch 470, training loss: 0.74090576171875 = 0.07134193927049637 + 0.1 * 6.695638179779053
Epoch 470, val loss: 0.8244010210037231
Epoch 480, training loss: 0.7328595519065857 = 0.06539665162563324 + 0.1 * 6.674628734588623
Epoch 480, val loss: 0.8306881189346313
Epoch 490, training loss: 0.7274192571640015 = 0.06009350344538689 + 0.1 * 6.673257350921631
Epoch 490, val loss: 0.8374241590499878
Epoch 500, training loss: 0.7221379280090332 = 0.05536070466041565 + 0.1 * 6.667771816253662
Epoch 500, val loss: 0.8443722724914551
Epoch 510, training loss: 0.7199683785438538 = 0.05113256722688675 + 0.1 * 6.688358306884766
Epoch 510, val loss: 0.8515773415565491
Epoch 520, training loss: 0.7124812006950378 = 0.04735933989286423 + 0.1 * 6.651218414306641
Epoch 520, val loss: 0.8587923645973206
Epoch 530, training loss: 0.7103346586227417 = 0.04397638887166977 + 0.1 * 6.663582801818848
Epoch 530, val loss: 0.8661194443702698
Epoch 540, training loss: 0.7058009505271912 = 0.040938589721918106 + 0.1 * 6.648623466491699
Epoch 540, val loss: 0.8733645081520081
Epoch 550, training loss: 0.7029990553855896 = 0.03819988667964935 + 0.1 * 6.64799165725708
Epoch 550, val loss: 0.8807286620140076
Epoch 560, training loss: 0.6996269226074219 = 0.03572272136807442 + 0.1 * 6.639041900634766
Epoch 560, val loss: 0.8878225684165955
Epoch 570, training loss: 0.6961456537246704 = 0.03347846865653992 + 0.1 * 6.62667179107666
Epoch 570, val loss: 0.8950304985046387
Epoch 580, training loss: 0.6943979263305664 = 0.03143618628382683 + 0.1 * 6.629617214202881
Epoch 580, val loss: 0.9019388556480408
Epoch 590, training loss: 0.6927938461303711 = 0.02957969531416893 + 0.1 * 6.63214111328125
Epoch 590, val loss: 0.9088696241378784
Epoch 600, training loss: 0.6903586387634277 = 0.02788613922894001 + 0.1 * 6.624724864959717
Epoch 600, val loss: 0.9156292676925659
Epoch 610, training loss: 0.6874018907546997 = 0.026335299015045166 + 0.1 * 6.610665798187256
Epoch 610, val loss: 0.9222918748855591
Epoch 620, training loss: 0.686721920967102 = 0.02490944229066372 + 0.1 * 6.618124961853027
Epoch 620, val loss: 0.9288643598556519
Epoch 630, training loss: 0.6853137612342834 = 0.023598790168762207 + 0.1 * 6.617149829864502
Epoch 630, val loss: 0.9352401494979858
Epoch 640, training loss: 0.6837818026542664 = 0.022391565144062042 + 0.1 * 6.6139020919799805
Epoch 640, val loss: 0.941415011882782
Epoch 650, training loss: 0.6810870170593262 = 0.021280206739902496 + 0.1 * 6.598067760467529
Epoch 650, val loss: 0.9476680159568787
Epoch 660, training loss: 0.679802656173706 = 0.020250171422958374 + 0.1 * 6.595524311065674
Epoch 660, val loss: 0.9536227583885193
Epoch 670, training loss: 0.6789094805717468 = 0.01929543912410736 + 0.1 * 6.596139907836914
Epoch 670, val loss: 0.959564208984375
Epoch 680, training loss: 0.6766215562820435 = 0.01840757392346859 + 0.1 * 6.582139492034912
Epoch 680, val loss: 0.9653003811836243
Epoch 690, training loss: 0.6756012439727783 = 0.017582450062036514 + 0.1 * 6.580187797546387
Epoch 690, val loss: 0.9709852337837219
Epoch 700, training loss: 0.6742950081825256 = 0.016813714057207108 + 0.1 * 6.574812889099121
Epoch 700, val loss: 0.9765192270278931
Epoch 710, training loss: 0.6736827492713928 = 0.016094785183668137 + 0.1 * 6.5758795738220215
Epoch 710, val loss: 0.9820553064346313
Epoch 720, training loss: 0.6718670725822449 = 0.015423636883497238 + 0.1 * 6.564434051513672
Epoch 720, val loss: 0.9871636033058167
Epoch 730, training loss: 0.6713356971740723 = 0.014797048643231392 + 0.1 * 6.5653862953186035
Epoch 730, val loss: 0.9924991130828857
Epoch 740, training loss: 0.6731836199760437 = 0.014208612963557243 + 0.1 * 6.589749813079834
Epoch 740, val loss: 0.9975634813308716
Epoch 750, training loss: 0.6708840131759644 = 0.013656018301844597 + 0.1 * 6.572279930114746
Epoch 750, val loss: 1.0025355815887451
Epoch 760, training loss: 0.6689283847808838 = 0.01313889492303133 + 0.1 * 6.557894706726074
Epoch 760, val loss: 1.0073233842849731
Epoch 770, training loss: 0.6674292683601379 = 0.012651980854570866 + 0.1 * 6.5477728843688965
Epoch 770, val loss: 1.0122120380401611
Epoch 780, training loss: 0.6680059432983398 = 0.012191390618681908 + 0.1 * 6.558145046234131
Epoch 780, val loss: 1.0169652700424194
Epoch 790, training loss: 0.6679986119270325 = 0.01175679825246334 + 0.1 * 6.562418460845947
Epoch 790, val loss: 1.0213875770568848
Epoch 800, training loss: 0.6665002107620239 = 0.011348665691912174 + 0.1 * 6.551515579223633
Epoch 800, val loss: 1.0258185863494873
Epoch 810, training loss: 0.6653781533241272 = 0.010963513515889645 + 0.1 * 6.544146537780762
Epoch 810, val loss: 1.030321717262268
Epoch 820, training loss: 0.6652428507804871 = 0.010597926564514637 + 0.1 * 6.546449184417725
Epoch 820, val loss: 1.034554362297058
Epoch 830, training loss: 0.6642518043518066 = 0.01025296188890934 + 0.1 * 6.5399885177612305
Epoch 830, val loss: 1.0388082265853882
Epoch 840, training loss: 0.6636493802070618 = 0.009924767538905144 + 0.1 * 6.537246227264404
Epoch 840, val loss: 1.043110728263855
Epoch 850, training loss: 0.6632491946220398 = 0.009612943977117538 + 0.1 * 6.536362648010254
Epoch 850, val loss: 1.0469838380813599
Epoch 860, training loss: 0.6620643138885498 = 0.009317495860159397 + 0.1 * 6.527467727661133
Epoch 860, val loss: 1.0510393381118774
Epoch 870, training loss: 0.6628961563110352 = 0.009036466479301453 + 0.1 * 6.5385966300964355
Epoch 870, val loss: 1.0549843311309814
Epoch 880, training loss: 0.6617239117622375 = 0.008768941275775433 + 0.1 * 6.529550075531006
Epoch 880, val loss: 1.058794379234314
Epoch 890, training loss: 0.6608712673187256 = 0.008514619432389736 + 0.1 * 6.523566246032715
Epoch 890, val loss: 1.0625849962234497
Epoch 900, training loss: 0.6615453362464905 = 0.008271683007478714 + 0.1 * 6.532736301422119
Epoch 900, val loss: 1.0663310289382935
Epoch 910, training loss: 0.6613108515739441 = 0.008039966225624084 + 0.1 * 6.532709121704102
Epoch 910, val loss: 1.0699087381362915
Epoch 920, training loss: 0.6590867638587952 = 0.00781930796802044 + 0.1 * 6.512674808502197
Epoch 920, val loss: 1.0734866857528687
Epoch 930, training loss: 0.660298228263855 = 0.007608313113451004 + 0.1 * 6.526899337768555
Epoch 930, val loss: 1.0770565271377563
Epoch 940, training loss: 0.6584417819976807 = 0.0074066659435629845 + 0.1 * 6.510351181030273
Epoch 940, val loss: 1.0804438591003418
Epoch 950, training loss: 0.6604562401771545 = 0.007214002311229706 + 0.1 * 6.532422065734863
Epoch 950, val loss: 1.0838619470596313
Epoch 960, training loss: 0.6580975651741028 = 0.007029348984360695 + 0.1 * 6.510682106018066
Epoch 960, val loss: 1.0871267318725586
Epoch 970, training loss: 0.6578943729400635 = 0.006852904334664345 + 0.1 * 6.5104146003723145
Epoch 970, val loss: 1.0904990434646606
Epoch 980, training loss: 0.6582476496696472 = 0.006683191284537315 + 0.1 * 6.515644550323486
Epoch 980, val loss: 1.0936086177825928
Epoch 990, training loss: 0.6573613286018372 = 0.006520983297377825 + 0.1 * 6.508403301239014
Epoch 990, val loss: 1.0967655181884766
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.816025303110174
The final CL Acc:0.77037, 0.00000, The final GNN Acc:0.81831, 0.00252
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13282])
remove edge: torch.Size([2, 7876])
updated graph: torch.Size([2, 10602])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.813939094543457 = 1.954256296157837 + 0.1 * 8.596827507019043
Epoch 0, val loss: 1.951258659362793
Epoch 10, training loss: 2.8031985759735107 = 1.9435213804244995 + 0.1 * 8.596772193908691
Epoch 10, val loss: 1.9408859014511108
Epoch 20, training loss: 2.7904326915740967 = 1.9307929277420044 + 0.1 * 8.596397399902344
Epoch 20, val loss: 1.9280011653900146
Epoch 30, training loss: 2.7728734016418457 = 1.9135428667068481 + 0.1 * 8.593305587768555
Epoch 30, val loss: 1.9100477695465088
Epoch 40, training loss: 2.7455034255981445 = 1.8886581659317017 + 0.1 * 8.568452835083008
Epoch 40, val loss: 1.8840738534927368
Epoch 50, training loss: 2.697502613067627 = 1.8540120124816895 + 0.1 * 8.434905052185059
Epoch 50, val loss: 1.8491421937942505
Epoch 60, training loss: 2.6294004917144775 = 1.8144222497940063 + 0.1 * 8.149782180786133
Epoch 60, val loss: 1.8121991157531738
Epoch 70, training loss: 2.550832748413086 = 1.7778921127319336 + 0.1 * 7.729406833648682
Epoch 70, val loss: 1.7809178829193115
Epoch 80, training loss: 2.4710605144500732 = 1.740956425666809 + 0.1 * 7.3010406494140625
Epoch 80, val loss: 1.7479780912399292
Epoch 90, training loss: 2.40692138671875 = 1.694764256477356 + 0.1 * 7.121570110321045
Epoch 90, val loss: 1.7055686712265015
Epoch 100, training loss: 2.3400845527648926 = 1.634461522102356 + 0.1 * 7.0562310218811035
Epoch 100, val loss: 1.6508346796035767
Epoch 110, training loss: 2.259211540222168 = 1.5581419467926025 + 0.1 * 7.01069450378418
Epoch 110, val loss: 1.5833767652511597
Epoch 120, training loss: 2.167335271835327 = 1.4689968824386597 + 0.1 * 6.983383655548096
Epoch 120, val loss: 1.5068151950836182
Epoch 130, training loss: 2.0702929496765137 = 1.3734890222549438 + 0.1 * 6.968038558959961
Epoch 130, val loss: 1.4279323816299438
Epoch 140, training loss: 1.9694273471832275 = 1.273891806602478 + 0.1 * 6.955355167388916
Epoch 140, val loss: 1.348389744758606
Epoch 150, training loss: 1.8651483058929443 = 1.1709482669830322 + 0.1 * 6.942000389099121
Epoch 150, val loss: 1.2674371004104614
Epoch 160, training loss: 1.7603247165679932 = 1.0675359964370728 + 0.1 * 6.927886486053467
Epoch 160, val loss: 1.1868667602539062
Epoch 170, training loss: 1.6602321863174438 = 0.9692774415016174 + 0.1 * 6.909547328948975
Epoch 170, val loss: 1.110759973526001
Epoch 180, training loss: 1.5697962045669556 = 0.8802311420440674 + 0.1 * 6.895650386810303
Epoch 180, val loss: 1.0423500537872314
Epoch 190, training loss: 1.4909718036651611 = 0.8031349182128906 + 0.1 * 6.878369331359863
Epoch 190, val loss: 0.9844862222671509
Epoch 200, training loss: 1.4244624376296997 = 0.7377948760986328 + 0.1 * 6.86667537689209
Epoch 200, val loss: 0.937142550945282
Epoch 210, training loss: 1.368152141571045 = 0.6830681562423706 + 0.1 * 6.850839614868164
Epoch 210, val loss: 0.8997056484222412
Epoch 220, training loss: 1.3192040920257568 = 0.6353384852409363 + 0.1 * 6.838656425476074
Epoch 220, val loss: 0.8691865801811218
Epoch 230, training loss: 1.27498197555542 = 0.5919139385223389 + 0.1 * 6.830680847167969
Epoch 230, val loss: 0.8432220220565796
Epoch 240, training loss: 1.2317934036254883 = 0.5504637956619263 + 0.1 * 6.813296318054199
Epoch 240, val loss: 0.8197954893112183
Epoch 250, training loss: 1.1897532939910889 = 0.5095980763435364 + 0.1 * 6.8015522956848145
Epoch 250, val loss: 0.7977910041809082
Epoch 260, training loss: 1.1498079299926758 = 0.46947479248046875 + 0.1 * 6.803330898284912
Epoch 260, val loss: 0.7776329517364502
Epoch 270, training loss: 1.109511137008667 = 0.43077969551086426 + 0.1 * 6.787313938140869
Epoch 270, val loss: 0.7597505450248718
Epoch 280, training loss: 1.0717582702636719 = 0.3937099874019623 + 0.1 * 6.780482292175293
Epoch 280, val loss: 0.7445909380912781
Epoch 290, training loss: 1.03635573387146 = 0.35887059569358826 + 0.1 * 6.774850845336914
Epoch 290, val loss: 0.732654869556427
Epoch 300, training loss: 1.0032024383544922 = 0.3265095353126526 + 0.1 * 6.766928195953369
Epoch 300, val loss: 0.7238702178001404
Epoch 310, training loss: 0.9729458689689636 = 0.2964441180229187 + 0.1 * 6.765017509460449
Epoch 310, val loss: 0.7179745435714722
Epoch 320, training loss: 0.9446688890457153 = 0.26856890320777893 + 0.1 * 6.76099967956543
Epoch 320, val loss: 0.7144325971603394
Epoch 330, training loss: 0.9178900718688965 = 0.24257907271385193 + 0.1 * 6.753110408782959
Epoch 330, val loss: 0.7128857374191284
Epoch 340, training loss: 0.8929889798164368 = 0.21843236684799194 + 0.1 * 6.745565891265869
Epoch 340, val loss: 0.7128069400787354
Epoch 350, training loss: 0.8699653148651123 = 0.1961754560470581 + 0.1 * 6.737898349761963
Epoch 350, val loss: 0.7140878438949585
Epoch 360, training loss: 0.8511898517608643 = 0.17579385638237 + 0.1 * 6.753959655761719
Epoch 360, val loss: 0.7165197730064392
Epoch 370, training loss: 0.8306142091751099 = 0.15744933485984802 + 0.1 * 6.7316484451293945
Epoch 370, val loss: 0.7199369072914124
Epoch 380, training loss: 0.8130787014961243 = 0.14102613925933838 + 0.1 * 6.720525741577148
Epoch 380, val loss: 0.7242426872253418
Epoch 390, training loss: 0.7996392846107483 = 0.12642665207386017 + 0.1 * 6.732126235961914
Epoch 390, val loss: 0.729373037815094
Epoch 400, training loss: 0.784198522567749 = 0.11357361078262329 + 0.1 * 6.706249237060547
Epoch 400, val loss: 0.7351959347724915
Epoch 410, training loss: 0.7731187343597412 = 0.10222890973091125 + 0.1 * 6.708897590637207
Epoch 410, val loss: 0.741639256477356
Epoch 420, training loss: 0.7617347836494446 = 0.09226512908935547 + 0.1 * 6.694696426391602
Epoch 420, val loss: 0.7484646439552307
Epoch 430, training loss: 0.7522329688072205 = 0.08347643166780472 + 0.1 * 6.687565326690674
Epoch 430, val loss: 0.7556080222129822
Epoch 440, training loss: 0.7435879707336426 = 0.07570916414260864 + 0.1 * 6.678788185119629
Epoch 440, val loss: 0.7629543542861938
Epoch 450, training loss: 0.7370291352272034 = 0.06882424652576447 + 0.1 * 6.682048797607422
Epoch 450, val loss: 0.770332932472229
Epoch 460, training loss: 0.7292520999908447 = 0.06273926049470901 + 0.1 * 6.665128231048584
Epoch 460, val loss: 0.7779083251953125
Epoch 470, training loss: 0.7225397229194641 = 0.05733334645628929 + 0.1 * 6.652063846588135
Epoch 470, val loss: 0.7854829430580139
Epoch 480, training loss: 0.7205290198326111 = 0.05250680819153786 + 0.1 * 6.680222034454346
Epoch 480, val loss: 0.7931281328201294
Epoch 490, training loss: 0.7129983305931091 = 0.04821191355586052 + 0.1 * 6.647863864898682
Epoch 490, val loss: 0.8006823062896729
Epoch 500, training loss: 0.707801878452301 = 0.04436955228447914 + 0.1 * 6.634322643280029
Epoch 500, val loss: 0.808270275592804
Epoch 510, training loss: 0.7049606442451477 = 0.040926821529865265 + 0.1 * 6.64033842086792
Epoch 510, val loss: 0.8157814145088196
Epoch 520, training loss: 0.7002899646759033 = 0.037842683494091034 + 0.1 * 6.624472618103027
Epoch 520, val loss: 0.8231512904167175
Epoch 530, training loss: 0.6982359290122986 = 0.03507450595498085 + 0.1 * 6.631613731384277
Epoch 530, val loss: 0.8305236101150513
Epoch 540, training loss: 0.6942506432533264 = 0.03259164094924927 + 0.1 * 6.6165900230407715
Epoch 540, val loss: 0.8376145362854004
Epoch 550, training loss: 0.6912683248519897 = 0.0303530041128397 + 0.1 * 6.609152793884277
Epoch 550, val loss: 0.8446388840675354
Epoch 560, training loss: 0.6888139843940735 = 0.028328729793429375 + 0.1 * 6.604852676391602
Epoch 560, val loss: 0.8515096306800842
Epoch 570, training loss: 0.6875278353691101 = 0.02649829350411892 + 0.1 * 6.610295295715332
Epoch 570, val loss: 0.8583443760871887
Epoch 580, training loss: 0.6849174499511719 = 0.0248445812612772 + 0.1 * 6.600728511810303
Epoch 580, val loss: 0.8648177981376648
Epoch 590, training loss: 0.682489812374115 = 0.023342104628682137 + 0.1 * 6.5914764404296875
Epoch 590, val loss: 0.8712302446365356
Epoch 600, training loss: 0.6806496381759644 = 0.02197238802909851 + 0.1 * 6.586772441864014
Epoch 600, val loss: 0.8774608969688416
Epoch 610, training loss: 0.6791204214096069 = 0.020721828565001488 + 0.1 * 6.583985805511475
Epoch 610, val loss: 0.8835954666137695
Epoch 620, training loss: 0.6775488257408142 = 0.019576046615839005 + 0.1 * 6.579727649688721
Epoch 620, val loss: 0.8895721435546875
Epoch 630, training loss: 0.6757600903511047 = 0.01852618344128132 + 0.1 * 6.572339057922363
Epoch 630, val loss: 0.8954172134399414
Epoch 640, training loss: 0.6753039360046387 = 0.017563704401254654 + 0.1 * 6.577402114868164
Epoch 640, val loss: 0.9010389447212219
Epoch 650, training loss: 0.6734265685081482 = 0.016681069508194923 + 0.1 * 6.567454814910889
Epoch 650, val loss: 0.9064770936965942
Epoch 660, training loss: 0.6725387573242188 = 0.015865888446569443 + 0.1 * 6.566728591918945
Epoch 660, val loss: 0.911882221698761
Epoch 670, training loss: 0.6717116832733154 = 0.015111473388969898 + 0.1 * 6.566002368927002
Epoch 670, val loss: 0.9171121120452881
Epoch 680, training loss: 0.6703211069107056 = 0.014413402415812016 + 0.1 * 6.55907678604126
Epoch 680, val loss: 0.9222504496574402
Epoch 690, training loss: 0.6697574257850647 = 0.013766748830676079 + 0.1 * 6.559906482696533
Epoch 690, val loss: 0.9272130727767944
Epoch 700, training loss: 0.6685341596603394 = 0.013164699077606201 + 0.1 * 6.553694725036621
Epoch 700, val loss: 0.9321247339248657
Epoch 710, training loss: 0.6666485667228699 = 0.012605649419128895 + 0.1 * 6.54042911529541
Epoch 710, val loss: 0.9368905425071716
Epoch 720, training loss: 0.6660621166229248 = 0.012084067799150944 + 0.1 * 6.539780616760254
Epoch 720, val loss: 0.9415253400802612
Epoch 730, training loss: 0.6664303541183472 = 0.011596867814660072 + 0.1 * 6.54833459854126
Epoch 730, val loss: 0.9460552930831909
Epoch 740, training loss: 0.6648352146148682 = 0.011141825467348099 + 0.1 * 6.536933422088623
Epoch 740, val loss: 0.9505503177642822
Epoch 750, training loss: 0.6640263199806213 = 0.010716072283685207 + 0.1 * 6.533102512359619
Epoch 750, val loss: 0.9547992944717407
Epoch 760, training loss: 0.662753701210022 = 0.010316062718629837 + 0.1 * 6.524376392364502
Epoch 760, val loss: 0.9590820074081421
Epoch 770, training loss: 0.6621327996253967 = 0.009940528310835361 + 0.1 * 6.521922588348389
Epoch 770, val loss: 0.9632033705711365
Epoch 780, training loss: 0.6631941795349121 = 0.009586390107870102 + 0.1 * 6.536077976226807
Epoch 780, val loss: 0.9673035740852356
Epoch 790, training loss: 0.6619234085083008 = 0.009253165684640408 + 0.1 * 6.526702404022217
Epoch 790, val loss: 0.9713122248649597
Epoch 800, training loss: 0.6601623296737671 = 0.008938985876739025 + 0.1 * 6.512233734130859
Epoch 800, val loss: 0.9752025604248047
Epoch 810, training loss: 0.65952068567276 = 0.008642510510981083 + 0.1 * 6.508781909942627
Epoch 810, val loss: 0.9790281057357788
Epoch 820, training loss: 0.6604671478271484 = 0.008361396379768848 + 0.1 * 6.52105712890625
Epoch 820, val loss: 0.9828075170516968
Epoch 830, training loss: 0.6591542959213257 = 0.008095473051071167 + 0.1 * 6.510587692260742
Epoch 830, val loss: 0.9864672422409058
Epoch 840, training loss: 0.6608434319496155 = 0.007844354957342148 + 0.1 * 6.529991149902344
Epoch 840, val loss: 0.9900466203689575
Epoch 850, training loss: 0.6582329869270325 = 0.007606377825140953 + 0.1 * 6.506266117095947
Epoch 850, val loss: 0.9935177564620972
Epoch 860, training loss: 0.657703697681427 = 0.007380383554846048 + 0.1 * 6.503233432769775
Epoch 860, val loss: 0.9969857931137085
Epoch 870, training loss: 0.6567254662513733 = 0.00716473488137126 + 0.1 * 6.495607376098633
Epoch 870, val loss: 1.0003600120544434
Epoch 880, training loss: 0.6564463376998901 = 0.006959365680813789 + 0.1 * 6.494869709014893
Epoch 880, val loss: 1.0036858320236206
Epoch 890, training loss: 0.6553702354431152 = 0.00676381541416049 + 0.1 * 6.4860639572143555
Epoch 890, val loss: 1.006953477859497
Epoch 900, training loss: 0.6558394432067871 = 0.00657782843336463 + 0.1 * 6.492615699768066
Epoch 900, val loss: 1.0101009607315063
Epoch 910, training loss: 0.6554217338562012 = 0.006400342099368572 + 0.1 * 6.490213871002197
Epoch 910, val loss: 1.0132776498794556
Epoch 920, training loss: 0.6549649238586426 = 0.006231056991964579 + 0.1 * 6.487338542938232
Epoch 920, val loss: 1.016316294670105
Epoch 930, training loss: 0.6549893021583557 = 0.0060690101236104965 + 0.1 * 6.489202976226807
Epoch 930, val loss: 1.0193923711776733
Epoch 940, training loss: 0.6551730632781982 = 0.005914050154387951 + 0.1 * 6.492589950561523
Epoch 940, val loss: 1.0223485231399536
Epoch 950, training loss: 0.653458833694458 = 0.00576565507799387 + 0.1 * 6.476931571960449
Epoch 950, val loss: 1.0252612829208374
Epoch 960, training loss: 0.6530836820602417 = 0.005624023266136646 + 0.1 * 6.474596977233887
Epoch 960, val loss: 1.028143048286438
Epoch 970, training loss: 0.6523216366767883 = 0.0054883165284991264 + 0.1 * 6.468332767486572
Epoch 970, val loss: 1.0309399366378784
Epoch 980, training loss: 0.653687596321106 = 0.005357665475457907 + 0.1 * 6.4832987785339355
Epoch 980, val loss: 1.0336920022964478
Epoch 990, training loss: 0.6526342034339905 = 0.005232279188930988 + 0.1 * 6.4740190505981445
Epoch 990, val loss: 1.0364394187927246
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 2.7986977100372314 = 1.9390110969543457 + 0.1 * 8.596866607666016
Epoch 0, val loss: 1.9325649738311768
Epoch 10, training loss: 2.7891721725463867 = 1.9294936656951904 + 0.1 * 8.596784591674805
Epoch 10, val loss: 1.9239411354064941
Epoch 20, training loss: 2.7772395610809326 = 1.9176058769226074 + 0.1 * 8.59633731842041
Epoch 20, val loss: 1.912913203239441
Epoch 30, training loss: 2.760155200958252 = 1.9009037017822266 + 0.1 * 8.592514991760254
Epoch 30, val loss: 1.897221565246582
Epoch 40, training loss: 2.73270845413208 = 1.8761502504348755 + 0.1 * 8.565581321716309
Epoch 40, val loss: 1.8741902112960815
Epoch 50, training loss: 2.6848111152648926 = 1.841633915901184 + 0.1 * 8.43177318572998
Epoch 50, val loss: 1.8433490991592407
Epoch 60, training loss: 2.6263089179992676 = 1.8027081489562988 + 0.1 * 8.236006736755371
Epoch 60, val loss: 1.8106987476348877
Epoch 70, training loss: 2.568436622619629 = 1.76491379737854 + 0.1 * 8.03522777557373
Epoch 70, val loss: 1.7786979675292969
Epoch 80, training loss: 2.489877939224243 = 1.722893238067627 + 0.1 * 7.66984748840332
Epoch 80, val loss: 1.7393118143081665
Epoch 90, training loss: 2.4058845043182373 = 1.669971227645874 + 0.1 * 7.359132289886475
Epoch 90, val loss: 1.6924766302108765
Epoch 100, training loss: 2.3187077045440674 = 1.6012279987335205 + 0.1 * 7.174796104431152
Epoch 100, val loss: 1.6340501308441162
Epoch 110, training loss: 2.226665735244751 = 1.5177377462387085 + 0.1 * 7.089279651641846
Epoch 110, val loss: 1.561795949935913
Epoch 120, training loss: 2.1347873210906982 = 1.431199312210083 + 0.1 * 7.035880088806152
Epoch 120, val loss: 1.4909902811050415
Epoch 130, training loss: 2.0485451221466064 = 1.3479613065719604 + 0.1 * 7.005838871002197
Epoch 130, val loss: 1.4240087270736694
Epoch 140, training loss: 1.9660923480987549 = 1.2677123546600342 + 0.1 * 6.983799457550049
Epoch 140, val loss: 1.359715461730957
Epoch 150, training loss: 1.8850748538970947 = 1.18806791305542 + 0.1 * 6.970068454742432
Epoch 150, val loss: 1.2960922718048096
Epoch 160, training loss: 1.8061904907226562 = 1.1110308170318604 + 0.1 * 6.951596736907959
Epoch 160, val loss: 1.2359566688537598
Epoch 170, training loss: 1.7319210767745972 = 1.0382630825042725 + 0.1 * 6.936579704284668
Epoch 170, val loss: 1.1807981729507446
Epoch 180, training loss: 1.6631484031677246 = 0.9711345434188843 + 0.1 * 6.920137882232666
Epoch 180, val loss: 1.1310871839523315
Epoch 190, training loss: 1.5991218090057373 = 0.9084240198135376 + 0.1 * 6.906978130340576
Epoch 190, val loss: 1.0846951007843018
Epoch 200, training loss: 1.5372288227081299 = 0.8477699756622314 + 0.1 * 6.894588947296143
Epoch 200, val loss: 1.0394929647445679
Epoch 210, training loss: 1.4753985404968262 = 0.7869337201118469 + 0.1 * 6.884647369384766
Epoch 210, val loss: 0.993563711643219
Epoch 220, training loss: 1.4125434160232544 = 0.7249647378921509 + 0.1 * 6.875786781311035
Epoch 220, val loss: 0.9467907547950745
Epoch 230, training loss: 1.3506481647491455 = 0.6638455390930176 + 0.1 * 6.868025302886963
Epoch 230, val loss: 0.901288628578186
Epoch 240, training loss: 1.2912659645080566 = 0.60565185546875 + 0.1 * 6.856141567230225
Epoch 240, val loss: 0.859689474105835
Epoch 250, training loss: 1.23673677444458 = 0.5515861511230469 + 0.1 * 6.851506233215332
Epoch 250, val loss: 0.8237469792366028
Epoch 260, training loss: 1.1873314380645752 = 0.502722978591919 + 0.1 * 6.8460845947265625
Epoch 260, val loss: 0.7947048544883728
Epoch 270, training loss: 1.142048954963684 = 0.4587714374065399 + 0.1 * 6.832775592803955
Epoch 270, val loss: 0.772127628326416
Epoch 280, training loss: 1.100268840789795 = 0.4185791611671448 + 0.1 * 6.816895961761475
Epoch 280, val loss: 0.7549533247947693
Epoch 290, training loss: 1.0651137828826904 = 0.3815917670726776 + 0.1 * 6.835219860076904
Epoch 290, val loss: 0.7418741583824158
Epoch 300, training loss: 1.0285979509353638 = 0.34798505902290344 + 0.1 * 6.80612850189209
Epoch 300, val loss: 0.7324536442756653
Epoch 310, training loss: 0.9960925579071045 = 0.3168727159500122 + 0.1 * 6.792198181152344
Epoch 310, val loss: 0.7258883118629456
Epoch 320, training loss: 0.9662351608276367 = 0.287946492433548 + 0.1 * 6.782886981964111
Epoch 320, val loss: 0.721930742263794
Epoch 330, training loss: 0.9389263391494751 = 0.26120612025260925 + 0.1 * 6.777202129364014
Epoch 330, val loss: 0.7202262878417969
Epoch 340, training loss: 0.9135963916778564 = 0.23658975958824158 + 0.1 * 6.770066261291504
Epoch 340, val loss: 0.720504641532898
Epoch 350, training loss: 0.891139566898346 = 0.21404333412647247 + 0.1 * 6.770962238311768
Epoch 350, val loss: 0.7226066589355469
Epoch 360, training loss: 0.8691189289093018 = 0.19356009364128113 + 0.1 * 6.755588054656982
Epoch 360, val loss: 0.7263392806053162
Epoch 370, training loss: 0.849683940410614 = 0.17501313984394073 + 0.1 * 6.746707916259766
Epoch 370, val loss: 0.73149573802948
Epoch 380, training loss: 0.8344521522521973 = 0.1583116054534912 + 0.1 * 6.7614054679870605
Epoch 380, val loss: 0.7378782033920288
Epoch 390, training loss: 0.8173272609710693 = 0.14339299499988556 + 0.1 * 6.73934268951416
Epoch 390, val loss: 0.7452107667922974
Epoch 400, training loss: 0.8022189140319824 = 0.1300393044948578 + 0.1 * 6.72179651260376
Epoch 400, val loss: 0.7535091638565063
Epoch 410, training loss: 0.790120542049408 = 0.1181279867887497 + 0.1 * 6.719925403594971
Epoch 410, val loss: 0.7623867988586426
Epoch 420, training loss: 0.7785634398460388 = 0.10751382261514664 + 0.1 * 6.710495948791504
Epoch 420, val loss: 0.7716943621635437
Epoch 430, training loss: 0.7685616612434387 = 0.09803058207035065 + 0.1 * 6.705310821533203
Epoch 430, val loss: 0.7814508676528931
Epoch 440, training loss: 0.7599385380744934 = 0.089568592607975 + 0.1 * 6.703699111938477
Epoch 440, val loss: 0.7913646101951599
Epoch 450, training loss: 0.750738263130188 = 0.08201750367879868 + 0.1 * 6.6872076988220215
Epoch 450, val loss: 0.8014904856681824
Epoch 460, training loss: 0.7438642382621765 = 0.07528088241815567 + 0.1 * 6.685832977294922
Epoch 460, val loss: 0.8116312623023987
Epoch 470, training loss: 0.7402061223983765 = 0.06924310326576233 + 0.1 * 6.709630489349365
Epoch 470, val loss: 0.8218215107917786
Epoch 480, training loss: 0.7300910949707031 = 0.06384044140577316 + 0.1 * 6.662506103515625
Epoch 480, val loss: 0.8319128751754761
Epoch 490, training loss: 0.7272676229476929 = 0.05898399278521538 + 0.1 * 6.682836532592773
Epoch 490, val loss: 0.8419966697692871
Epoch 500, training loss: 0.7212504744529724 = 0.054620515555143356 + 0.1 * 6.666299819946289
Epoch 500, val loss: 0.8518688678741455
Epoch 510, training loss: 0.7154267430305481 = 0.05068569257855415 + 0.1 * 6.6474103927612305
Epoch 510, val loss: 0.8615955710411072
Epoch 520, training loss: 0.7115824818611145 = 0.047119829803705215 + 0.1 * 6.644626140594482
Epoch 520, val loss: 0.8711791634559631
Epoch 530, training loss: 0.7089247107505798 = 0.04388120025396347 + 0.1 * 6.650434970855713
Epoch 530, val loss: 0.8805974125862122
Epoch 540, training loss: 0.704028844833374 = 0.04094480350613594 + 0.1 * 6.630840301513672
Epoch 540, val loss: 0.8898260593414307
Epoch 550, training loss: 0.7011570334434509 = 0.03827660530805588 + 0.1 * 6.6288042068481445
Epoch 550, val loss: 0.8988296985626221
Epoch 560, training loss: 0.699352502822876 = 0.03584003821015358 + 0.1 * 6.635124683380127
Epoch 560, val loss: 0.9076967835426331
Epoch 570, training loss: 0.6959308385848999 = 0.033615682274103165 + 0.1 * 6.6231513023376465
Epoch 570, val loss: 0.9162948727607727
Epoch 580, training loss: 0.6938382983207703 = 0.03157748281955719 + 0.1 * 6.622608184814453
Epoch 580, val loss: 0.9247743487358093
Epoch 590, training loss: 0.6909387111663818 = 0.02970891073346138 + 0.1 * 6.612297534942627
Epoch 590, val loss: 0.9330884218215942
Epoch 600, training loss: 0.6904615163803101 = 0.027994586154818535 + 0.1 * 6.624669551849365
Epoch 600, val loss: 0.9411949515342712
Epoch 610, training loss: 0.686089277267456 = 0.02642424777150154 + 0.1 * 6.59665060043335
Epoch 610, val loss: 0.9490859508514404
Epoch 620, training loss: 0.6852590441703796 = 0.02497742883861065 + 0.1 * 6.602816104888916
Epoch 620, val loss: 0.9568232893943787
Epoch 630, training loss: 0.6828882694244385 = 0.02364182472229004 + 0.1 * 6.592464447021484
Epoch 630, val loss: 0.9644403457641602
Epoch 640, training loss: 0.6810237169265747 = 0.022409915924072266 + 0.1 * 6.586137771606445
Epoch 640, val loss: 0.971860408782959
Epoch 650, training loss: 0.6826500296592712 = 0.02126835100352764 + 0.1 * 6.613816261291504
Epoch 650, val loss: 0.9792112112045288
Epoch 660, training loss: 0.6786116361618042 = 0.020212899893522263 + 0.1 * 6.583987236022949
Epoch 660, val loss: 0.9862989783287048
Epoch 670, training loss: 0.6779294013977051 = 0.01923668198287487 + 0.1 * 6.5869269371032715
Epoch 670, val loss: 0.9932300448417664
Epoch 680, training loss: 0.6760247945785522 = 0.01832861267030239 + 0.1 * 6.576961994171143
Epoch 680, val loss: 1.0000683069229126
Epoch 690, training loss: 0.6768401861190796 = 0.01748349703848362 + 0.1 * 6.59356689453125
Epoch 690, val loss: 1.0067369937896729
Epoch 700, training loss: 0.6740511059761047 = 0.016696443781256676 + 0.1 * 6.573546409606934
Epoch 700, val loss: 1.0132395029067993
Epoch 710, training loss: 0.6729762554168701 = 0.015963343903422356 + 0.1 * 6.57012939453125
Epoch 710, val loss: 1.0196058750152588
Epoch 720, training loss: 0.6708726286888123 = 0.015279114246368408 + 0.1 * 6.555934906005859
Epoch 720, val loss: 1.0258318185806274
Epoch 730, training loss: 0.6723219156265259 = 0.014639043249189854 + 0.1 * 6.576828956604004
Epoch 730, val loss: 1.0319137573242188
Epoch 740, training loss: 0.6699218153953552 = 0.014039861038327217 + 0.1 * 6.55881929397583
Epoch 740, val loss: 1.0378355979919434
Epoch 750, training loss: 0.6698474884033203 = 0.013479645363986492 + 0.1 * 6.56367826461792
Epoch 750, val loss: 1.0436184406280518
Epoch 760, training loss: 0.6678030490875244 = 0.012952961958944798 + 0.1 * 6.5485005378723145
Epoch 760, val loss: 1.0493050813674927
Epoch 770, training loss: 0.6665903329849243 = 0.012458927929401398 + 0.1 * 6.541314125061035
Epoch 770, val loss: 1.054834008216858
Epoch 780, training loss: 0.6656134128570557 = 0.011995035223662853 + 0.1 * 6.536183834075928
Epoch 780, val loss: 1.0602728128433228
Epoch 790, training loss: 0.667550265789032 = 0.011557170189917088 + 0.1 * 6.559930801391602
Epoch 790, val loss: 1.0655691623687744
Epoch 800, training loss: 0.6661943197250366 = 0.011144978925585747 + 0.1 * 6.550493240356445
Epoch 800, val loss: 1.0707111358642578
Epoch 810, training loss: 0.667034924030304 = 0.010756335221230984 + 0.1 * 6.562786102294922
Epoch 810, val loss: 1.0758012533187866
Epoch 820, training loss: 0.6636016964912415 = 0.010388456284999847 + 0.1 * 6.532132625579834
Epoch 820, val loss: 1.0807286500930786
Epoch 830, training loss: 0.6621366739273071 = 0.010041195899248123 + 0.1 * 6.5209550857543945
Epoch 830, val loss: 1.0855823755264282
Epoch 840, training loss: 0.6639491319656372 = 0.009711440652608871 + 0.1 * 6.54237699508667
Epoch 840, val loss: 1.090396761894226
Epoch 850, training loss: 0.6627253293991089 = 0.00939883291721344 + 0.1 * 6.533265113830566
Epoch 850, val loss: 1.0949593782424927
Epoch 860, training loss: 0.6605149507522583 = 0.009104076772928238 + 0.1 * 6.514108657836914
Epoch 860, val loss: 1.0994961261749268
Epoch 870, training loss: 0.6625849008560181 = 0.008823626674711704 + 0.1 * 6.537612438201904
Epoch 870, val loss: 1.10399329662323
Epoch 880, training loss: 0.6607576608657837 = 0.008555595763027668 + 0.1 * 6.5220208168029785
Epoch 880, val loss: 1.1083484888076782
Epoch 890, training loss: 0.6598394513130188 = 0.008302342146635056 + 0.1 * 6.515370845794678
Epoch 890, val loss: 1.1125558614730835
Epoch 900, training loss: 0.6615527272224426 = 0.008060866966843605 + 0.1 * 6.534918308258057
Epoch 900, val loss: 1.1167395114898682
Epoch 910, training loss: 0.659980833530426 = 0.007829681970179081 + 0.1 * 6.521511554718018
Epoch 910, val loss: 1.120859980583191
Epoch 920, training loss: 0.6594495177268982 = 0.007611141540110111 + 0.1 * 6.518383502960205
Epoch 920, val loss: 1.1248393058776855
Epoch 930, training loss: 0.6574514508247375 = 0.007401596754789352 + 0.1 * 6.5004987716674805
Epoch 930, val loss: 1.1287566423416138
Epoch 940, training loss: 0.6581500768661499 = 0.007201519329100847 + 0.1 * 6.509485721588135
Epoch 940, val loss: 1.132649302482605
Epoch 950, training loss: 0.65720134973526 = 0.007009622175246477 + 0.1 * 6.501916885375977
Epoch 950, val loss: 1.1363525390625
Epoch 960, training loss: 0.6580276489257812 = 0.006827197968959808 + 0.1 * 6.512004375457764
Epoch 960, val loss: 1.1400666236877441
Epoch 970, training loss: 0.6576458215713501 = 0.0066515253856778145 + 0.1 * 6.509943008422852
Epoch 970, val loss: 1.1437326669692993
Epoch 980, training loss: 0.6552714109420776 = 0.006484044715762138 + 0.1 * 6.487873554229736
Epoch 980, val loss: 1.1472456455230713
Epoch 990, training loss: 0.6561654806137085 = 0.006323886103928089 + 0.1 * 6.498415946960449
Epoch 990, val loss: 1.15078604221344
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 2.821017265319824 = 1.9613319635391235 + 0.1 * 8.596854209899902
Epoch 0, val loss: 1.9614275693893433
Epoch 10, training loss: 2.8106937408447266 = 1.9510204792022705 + 0.1 * 8.596733093261719
Epoch 10, val loss: 1.9517368078231812
Epoch 20, training loss: 2.7972512245178223 = 1.937659502029419 + 0.1 * 8.595916748046875
Epoch 20, val loss: 1.9387880563735962
Epoch 30, training loss: 2.7772138118743896 = 1.9183766841888428 + 0.1 * 8.588370323181152
Epoch 30, val loss: 1.9197397232055664
Epoch 40, training loss: 2.743788003921509 = 1.8895078897476196 + 0.1 * 8.542801856994629
Epoch 40, val loss: 1.8913763761520386
Epoch 50, training loss: 2.6826419830322266 = 1.849993348121643 + 0.1 * 8.32648754119873
Epoch 50, val loss: 1.8544189929962158
Epoch 60, training loss: 2.6239776611328125 = 1.8075459003448486 + 0.1 * 8.164318084716797
Epoch 60, val loss: 1.8181148767471313
Epoch 70, training loss: 2.5523738861083984 = 1.7738771438598633 + 0.1 * 7.784968376159668
Epoch 70, val loss: 1.7900062799453735
Epoch 80, training loss: 2.4791224002838135 = 1.740511178970337 + 0.1 * 7.386112689971924
Epoch 80, val loss: 1.7572345733642578
Epoch 90, training loss: 2.416616439819336 = 1.6956058740615845 + 0.1 * 7.210104942321777
Epoch 90, val loss: 1.714633822441101
Epoch 100, training loss: 2.346729278564453 = 1.6352143287658691 + 0.1 * 7.115149021148682
Epoch 100, val loss: 1.6628409624099731
Epoch 110, training loss: 2.26737642288208 = 1.560341715812683 + 0.1 * 7.070347785949707
Epoch 110, val loss: 1.599471092224121
Epoch 120, training loss: 2.1827359199523926 = 1.4794080257415771 + 0.1 * 7.0332794189453125
Epoch 120, val loss: 1.531956672668457
Epoch 130, training loss: 2.10219407081604 = 1.4026333093643188 + 0.1 * 6.995607376098633
Epoch 130, val loss: 1.4710955619812012
Epoch 140, training loss: 2.029325485229492 = 1.3331856727600098 + 0.1 * 6.961399078369141
Epoch 140, val loss: 1.4180132150650024
Epoch 150, training loss: 1.9614293575286865 = 1.2684494256973267 + 0.1 * 6.929800033569336
Epoch 150, val loss: 1.3687604665756226
Epoch 160, training loss: 1.8946239948272705 = 1.2044411897659302 + 0.1 * 6.901828765869141
Epoch 160, val loss: 1.3195632696151733
Epoch 170, training loss: 1.8271416425704956 = 1.1395872831344604 + 0.1 * 6.875543594360352
Epoch 170, val loss: 1.269999623298645
Epoch 180, training loss: 1.7612099647521973 = 1.0757946968078613 + 0.1 * 6.854153156280518
Epoch 180, val loss: 1.2216566801071167
Epoch 190, training loss: 1.697441816329956 = 1.0141639709472656 + 0.1 * 6.832778453826904
Epoch 190, val loss: 1.17467200756073
Epoch 200, training loss: 1.6369051933288574 = 0.9553125500679016 + 0.1 * 6.815925598144531
Epoch 200, val loss: 1.1298333406448364
Epoch 210, training loss: 1.5797269344329834 = 0.8998091220855713 + 0.1 * 6.799177646636963
Epoch 210, val loss: 1.0872507095336914
Epoch 220, training loss: 1.523995280265808 = 0.8457490801811218 + 0.1 * 6.782462120056152
Epoch 220, val loss: 1.0450561046600342
Epoch 230, training loss: 1.4701799154281616 = 0.7927291393280029 + 0.1 * 6.774507522583008
Epoch 230, val loss: 1.0034548044204712
Epoch 240, training loss: 1.4156646728515625 = 0.7401852607727051 + 0.1 * 6.754794597625732
Epoch 240, val loss: 0.961855947971344
Epoch 250, training loss: 1.3628182411193848 = 0.6877186298370361 + 0.1 * 6.7509965896606445
Epoch 250, val loss: 0.9203474521636963
Epoch 260, training loss: 1.3102257251739502 = 0.6366766691207886 + 0.1 * 6.735489845275879
Epoch 260, val loss: 0.8804168701171875
Epoch 270, training loss: 1.258932113647461 = 0.5871861577033997 + 0.1 * 6.7174601554870605
Epoch 270, val loss: 0.8426892757415771
Epoch 280, training loss: 1.2104272842407227 = 0.5393282175064087 + 0.1 * 6.710990905761719
Epoch 280, val loss: 0.8075414299964905
Epoch 290, training loss: 1.1631568670272827 = 0.4934428334236145 + 0.1 * 6.697140216827393
Epoch 290, val loss: 0.775682806968689
Epoch 300, training loss: 1.118203043937683 = 0.4498197138309479 + 0.1 * 6.683833599090576
Epoch 300, val loss: 0.7472583055496216
Epoch 310, training loss: 1.0779786109924316 = 0.4085142910480499 + 0.1 * 6.694642543792725
Epoch 310, val loss: 0.7221920490264893
Epoch 320, training loss: 1.03691828250885 = 0.3698503375053406 + 0.1 * 6.670679092407227
Epoch 320, val loss: 0.7003368735313416
Epoch 330, training loss: 1.0004273653030396 = 0.33366215229034424 + 0.1 * 6.667652130126953
Epoch 330, val loss: 0.6813034415245056
Epoch 340, training loss: 0.9652374982833862 = 0.29982423782348633 + 0.1 * 6.65413236618042
Epoch 340, val loss: 0.6645542979240417
Epoch 350, training loss: 0.9339431524276733 = 0.2681523859500885 + 0.1 * 6.657907485961914
Epoch 350, val loss: 0.6500489711761475
Epoch 360, training loss: 0.9048838019371033 = 0.23886752128601074 + 0.1 * 6.660162448883057
Epoch 360, val loss: 0.6379250288009644
Epoch 370, training loss: 0.8767454028129578 = 0.21233682334423065 + 0.1 * 6.64408540725708
Epoch 370, val loss: 0.6283847093582153
Epoch 380, training loss: 0.8519027233123779 = 0.1885506808757782 + 0.1 * 6.633520603179932
Epoch 380, val loss: 0.6214463114738464
Epoch 390, training loss: 0.8318951725959778 = 0.16743744909763336 + 0.1 * 6.644577503204346
Epoch 390, val loss: 0.6170610785484314
Epoch 400, training loss: 0.8109695911407471 = 0.14900444447994232 + 0.1 * 6.6196513175964355
Epoch 400, val loss: 0.6150864958763123
Epoch 410, training loss: 0.7953343391418457 = 0.13294193148612976 + 0.1 * 6.6239237785339355
Epoch 410, val loss: 0.6151713132858276
Epoch 420, training loss: 0.7817183136940002 = 0.11899340897798538 + 0.1 * 6.627248764038086
Epoch 420, val loss: 0.6171055436134338
Epoch 430, training loss: 0.7674663066864014 = 0.10684999823570251 + 0.1 * 6.606163024902344
Epoch 430, val loss: 0.6204050779342651
Epoch 440, training loss: 0.7565629482269287 = 0.09624414145946503 + 0.1 * 6.603187561035156
Epoch 440, val loss: 0.6249303817749023
Epoch 450, training loss: 0.7473766207695007 = 0.08698266744613647 + 0.1 * 6.603939533233643
Epoch 450, val loss: 0.63030606508255
Epoch 460, training loss: 0.7377009391784668 = 0.07886508107185364 + 0.1 * 6.588357925415039
Epoch 460, val loss: 0.6364399790763855
Epoch 470, training loss: 0.7301663160324097 = 0.07170957326889038 + 0.1 * 6.584567070007324
Epoch 470, val loss: 0.6430763006210327
Epoch 480, training loss: 0.7258459329605103 = 0.06540212035179138 + 0.1 * 6.604437828063965
Epoch 480, val loss: 0.6501391530036926
Epoch 490, training loss: 0.7172185778617859 = 0.059857629239559174 + 0.1 * 6.573609352111816
Epoch 490, val loss: 0.657426118850708
Epoch 500, training loss: 0.7123213410377502 = 0.054937005043029785 + 0.1 * 6.573843479156494
Epoch 500, val loss: 0.6649796366691589
Epoch 510, training loss: 0.7091120481491089 = 0.050574950873851776 + 0.1 * 6.5853705406188965
Epoch 510, val loss: 0.6726323962211609
Epoch 520, training loss: 0.7036865949630737 = 0.046702053397893906 + 0.1 * 6.569845676422119
Epoch 520, val loss: 0.6802845001220703
Epoch 530, training loss: 0.6994447112083435 = 0.04323755204677582 + 0.1 * 6.562071323394775
Epoch 530, val loss: 0.688024640083313
Epoch 540, training loss: 0.6965243220329285 = 0.0401306189596653 + 0.1 * 6.563936710357666
Epoch 540, val loss: 0.6956722736358643
Epoch 550, training loss: 0.6932339072227478 = 0.0373491607606411 + 0.1 * 6.558847427368164
Epoch 550, val loss: 0.7032880783081055
Epoch 560, training loss: 0.6902645230293274 = 0.034846048802137375 + 0.1 * 6.554184436798096
Epoch 560, val loss: 0.7107811570167542
Epoch 570, training loss: 0.687775731086731 = 0.03258443996310234 + 0.1 * 6.551912784576416
Epoch 570, val loss: 0.7182199358940125
Epoch 580, training loss: 0.6850986480712891 = 0.03053470514714718 + 0.1 * 6.545639514923096
Epoch 580, val loss: 0.725448489189148
Epoch 590, training loss: 0.6832275986671448 = 0.02867821790277958 + 0.1 * 6.5454936027526855
Epoch 590, val loss: 0.7326421737670898
Epoch 600, training loss: 0.6811637878417969 = 0.026990940794348717 + 0.1 * 6.5417280197143555
Epoch 600, val loss: 0.7396225333213806
Epoch 610, training loss: 0.6789987683296204 = 0.025450117886066437 + 0.1 * 6.535486221313477
Epoch 610, val loss: 0.7465089559555054
Epoch 620, training loss: 0.6775311231613159 = 0.024040361866354942 + 0.1 * 6.534907817840576
Epoch 620, val loss: 0.7532678246498108
Epoch 630, training loss: 0.676281750202179 = 0.022749390453100204 + 0.1 * 6.535323619842529
Epoch 630, val loss: 0.7598390579223633
Epoch 640, training loss: 0.673192024230957 = 0.021563898772001266 + 0.1 * 6.5162811279296875
Epoch 640, val loss: 0.7663135528564453
Epoch 650, training loss: 0.6744180917739868 = 0.020469922572374344 + 0.1 * 6.5394816398620605
Epoch 650, val loss: 0.772673487663269
Epoch 660, training loss: 0.6713049411773682 = 0.019462935626506805 + 0.1 * 6.5184197425842285
Epoch 660, val loss: 0.7788416147232056
Epoch 670, training loss: 0.6726059913635254 = 0.018531862646341324 + 0.1 * 6.540741443634033
Epoch 670, val loss: 0.7848832607269287
Epoch 680, training loss: 0.6687086224555969 = 0.01767033524811268 + 0.1 * 6.510382652282715
Epoch 680, val loss: 0.7908239960670471
Epoch 690, training loss: 0.6681914925575256 = 0.016871565952897072 + 0.1 * 6.513199329376221
Epoch 690, val loss: 0.7965962886810303
Epoch 700, training loss: 0.6663016676902771 = 0.016127586364746094 + 0.1 * 6.5017409324646
Epoch 700, val loss: 0.8022433519363403
Epoch 710, training loss: 0.6657013297080994 = 0.015436789020895958 + 0.1 * 6.502645492553711
Epoch 710, val loss: 0.8077654838562012
Epoch 720, training loss: 0.6654660105705261 = 0.014789842069149017 + 0.1 * 6.5067620277404785
Epoch 720, val loss: 0.81322181224823
Epoch 730, training loss: 0.6643441319465637 = 0.014186190441250801 + 0.1 * 6.501579284667969
Epoch 730, val loss: 0.8185767531394958
Epoch 740, training loss: 0.6634795069694519 = 0.013622156344354153 + 0.1 * 6.498573303222656
Epoch 740, val loss: 0.8237937688827515
Epoch 750, training loss: 0.6619200706481934 = 0.01309245079755783 + 0.1 * 6.48827600479126
Epoch 750, val loss: 0.8289294838905334
Epoch 760, training loss: 0.6626937985420227 = 0.012594704516232014 + 0.1 * 6.500990867614746
Epoch 760, val loss: 0.8339842557907104
Epoch 770, training loss: 0.6615064144134521 = 0.012127089314162731 + 0.1 * 6.49379301071167
Epoch 770, val loss: 0.8388667106628418
Epoch 780, training loss: 0.6608142256736755 = 0.011688495054841042 + 0.1 * 6.491257667541504
Epoch 780, val loss: 0.8437414765357971
Epoch 790, training loss: 0.6611039042472839 = 0.011274956166744232 + 0.1 * 6.498289585113525
Epoch 790, val loss: 0.8484636545181274
Epoch 800, training loss: 0.6591660976409912 = 0.010885277763009071 + 0.1 * 6.4828081130981445
Epoch 800, val loss: 0.8531321287155151
Epoch 810, training loss: 0.6595081090927124 = 0.010516052134335041 + 0.1 * 6.489920616149902
Epoch 810, val loss: 0.8576993942260742
Epoch 820, training loss: 0.6579528450965881 = 0.010168133303523064 + 0.1 * 6.477847099304199
Epoch 820, val loss: 0.862210750579834
Epoch 830, training loss: 0.6581581830978394 = 0.009838193655014038 + 0.1 * 6.4832000732421875
Epoch 830, val loss: 0.8666242957115173
Epoch 840, training loss: 0.6568894982337952 = 0.009525786153972149 + 0.1 * 6.473637104034424
Epoch 840, val loss: 0.8709772229194641
Epoch 850, training loss: 0.6572519540786743 = 0.009229145012795925 + 0.1 * 6.480227947235107
Epoch 850, val loss: 0.8752389550209045
Epoch 860, training loss: 0.6559480428695679 = 0.008948220871388912 + 0.1 * 6.469998359680176
Epoch 860, val loss: 0.8794066905975342
Epoch 870, training loss: 0.6565788388252258 = 0.008680534549057484 + 0.1 * 6.478982925415039
Epoch 870, val loss: 0.8835346102714539
Epoch 880, training loss: 0.6553705334663391 = 0.008426125161349773 + 0.1 * 6.4694437980651855
Epoch 880, val loss: 0.8875977993011475
Epoch 890, training loss: 0.655685305595398 = 0.008183961734175682 + 0.1 * 6.475013256072998
Epoch 890, val loss: 0.8916218280792236
Epoch 900, training loss: 0.6548311114311218 = 0.007953894324600697 + 0.1 * 6.4687724113464355
Epoch 900, val loss: 0.895535945892334
Epoch 910, training loss: 0.6538714170455933 = 0.007734199054539204 + 0.1 * 6.461372375488281
Epoch 910, val loss: 0.8994171023368835
Epoch 920, training loss: 0.6537104845046997 = 0.007524052169173956 + 0.1 * 6.461863994598389
Epoch 920, val loss: 0.9032392501831055
Epoch 930, training loss: 0.6527839303016663 = 0.00732360128313303 + 0.1 * 6.4546027183532715
Epoch 930, val loss: 0.9069544672966003
Epoch 940, training loss: 0.6530470252037048 = 0.007131802849471569 + 0.1 * 6.4591522216796875
Epoch 940, val loss: 0.9106257557868958
Epoch 950, training loss: 0.6539095044136047 = 0.00694853812456131 + 0.1 * 6.46960973739624
Epoch 950, val loss: 0.9143053293228149
Epoch 960, training loss: 0.6540232300758362 = 0.006772867403924465 + 0.1 * 6.472503185272217
Epoch 960, val loss: 0.9178504943847656
Epoch 970, training loss: 0.6512668132781982 = 0.006605925504118204 + 0.1 * 6.446609020233154
Epoch 970, val loss: 0.9213483929634094
Epoch 980, training loss: 0.6515718102455139 = 0.006445147562772036 + 0.1 * 6.451266288757324
Epoch 980, val loss: 0.9248014092445374
Epoch 990, training loss: 0.6506426334381104 = 0.006290193647146225 + 0.1 * 6.443524360656738
Epoch 990, val loss: 0.9282544851303101
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8392198207696363
The final CL Acc:0.81358, 0.00873, The final GNN Acc:0.83992, 0.00221
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10612])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8049392700195312 = 1.9452556371688843 + 0.1 * 8.596835136413574
Epoch 0, val loss: 1.9451698064804077
Epoch 10, training loss: 2.7957236766815186 = 1.936046838760376 + 0.1 * 8.59676742553711
Epoch 10, val loss: 1.9359352588653564
Epoch 20, training loss: 2.784390926361084 = 1.9247488975524902 + 0.1 * 8.596420288085938
Epoch 20, val loss: 1.924570918083191
Epoch 30, training loss: 2.768465518951416 = 1.9090813398361206 + 0.1 * 8.593840599060059
Epoch 30, val loss: 1.9086507558822632
Epoch 40, training loss: 2.7435173988342285 = 1.8860344886779785 + 0.1 * 8.574828147888184
Epoch 40, val loss: 1.8852462768554688
Epoch 50, training loss: 2.7020726203918457 = 1.8534632921218872 + 0.1 * 8.486093521118164
Epoch 50, val loss: 1.853246808052063
Epoch 60, training loss: 2.6414451599121094 = 1.815282940864563 + 0.1 * 8.26162338256836
Epoch 60, val loss: 1.818581461906433
Epoch 70, training loss: 2.5796585083007812 = 1.7798281908035278 + 0.1 * 7.998303413391113
Epoch 70, val loss: 1.7877317667007446
Epoch 80, training loss: 2.503345251083374 = 1.7439385652542114 + 0.1 * 7.594067096710205
Epoch 80, val loss: 1.7545528411865234
Epoch 90, training loss: 2.440532684326172 = 1.7002662420272827 + 0.1 * 7.402663707733154
Epoch 90, val loss: 1.714475154876709
Epoch 100, training loss: 2.3724260330200195 = 1.6431808471679688 + 0.1 * 7.292450904846191
Epoch 100, val loss: 1.663749098777771
Epoch 110, training loss: 2.288121223449707 = 1.5712919235229492 + 0.1 * 7.1682915687561035
Epoch 110, val loss: 1.6012855768203735
Epoch 120, training loss: 2.194222927093506 = 1.487207293510437 + 0.1 * 7.070155620574951
Epoch 120, val loss: 1.5301623344421387
Epoch 130, training loss: 2.0968575477600098 = 1.394765019416809 + 0.1 * 7.020925045013428
Epoch 130, val loss: 1.4519339799880981
Epoch 140, training loss: 1.9975059032440186 = 1.2984131574630737 + 0.1 * 6.990926742553711
Epoch 140, val loss: 1.3733378648757935
Epoch 150, training loss: 1.8964303731918335 = 1.1995855569839478 + 0.1 * 6.968448162078857
Epoch 150, val loss: 1.2943583726882935
Epoch 160, training loss: 1.7971746921539307 = 1.1017132997512817 + 0.1 * 6.95461368560791
Epoch 160, val loss: 1.2180330753326416
Epoch 170, training loss: 1.7039027214050293 = 1.010036826133728 + 0.1 * 6.93865966796875
Epoch 170, val loss: 1.147648572921753
Epoch 180, training loss: 1.618401050567627 = 0.9257462620735168 + 0.1 * 6.926547050476074
Epoch 180, val loss: 1.0838960409164429
Epoch 190, training loss: 1.5420036315917969 = 0.8491774201393127 + 0.1 * 6.9282612800598145
Epoch 190, val loss: 1.0274754762649536
Epoch 200, training loss: 1.4725399017333984 = 0.7817816734313965 + 0.1 * 6.907582759857178
Epoch 200, val loss: 0.9799697399139404
Epoch 210, training loss: 1.411574363708496 = 0.7218904495239258 + 0.1 * 6.896839618682861
Epoch 210, val loss: 0.9401028156280518
Epoch 220, training loss: 1.358405590057373 = 0.6680351495742798 + 0.1 * 6.903703689575195
Epoch 220, val loss: 0.9065555930137634
Epoch 230, training loss: 1.307415246963501 = 0.61935955286026 + 0.1 * 6.880556106567383
Epoch 230, val loss: 0.8785578012466431
Epoch 240, training loss: 1.2611041069030762 = 0.5734151601791382 + 0.1 * 6.876889705657959
Epoch 240, val loss: 0.8536316752433777
Epoch 250, training loss: 1.2159143686294556 = 0.5292618274688721 + 0.1 * 6.866525173187256
Epoch 250, val loss: 0.8318251371383667
Epoch 260, training loss: 1.1722699403762817 = 0.48646143078804016 + 0.1 * 6.8580851554870605
Epoch 260, val loss: 0.8129867911338806
Epoch 270, training loss: 1.1304839849472046 = 0.44547271728515625 + 0.1 * 6.850112438201904
Epoch 270, val loss: 0.7977450489997864
Epoch 280, training loss: 1.0907723903656006 = 0.4070708155632019 + 0.1 * 6.8370161056518555
Epoch 280, val loss: 0.7866125106811523
Epoch 290, training loss: 1.0546157360076904 = 0.3716967701911926 + 0.1 * 6.829190254211426
Epoch 290, val loss: 0.779719352722168
Epoch 300, training loss: 1.021362543106079 = 0.33925890922546387 + 0.1 * 6.821035861968994
Epoch 300, val loss: 0.7765175104141235
Epoch 310, training loss: 0.9926728010177612 = 0.3095645606517792 + 0.1 * 6.831081867218018
Epoch 310, val loss: 0.7765650153160095
Epoch 320, training loss: 0.9637838006019592 = 0.2824937701225281 + 0.1 * 6.812900066375732
Epoch 320, val loss: 0.779405951499939
Epoch 330, training loss: 0.9391565322875977 = 0.2576054632663727 + 0.1 * 6.815510272979736
Epoch 330, val loss: 0.7842987179756165
Epoch 340, training loss: 0.9146054983139038 = 0.23477992415428162 + 0.1 * 6.798255443572998
Epoch 340, val loss: 0.7911971807479858
Epoch 350, training loss: 0.8927530646324158 = 0.21374821662902832 + 0.1 * 6.790048122406006
Epoch 350, val loss: 0.799771249294281
Epoch 360, training loss: 0.8735154867172241 = 0.1943453550338745 + 0.1 * 6.791701316833496
Epoch 360, val loss: 0.80994713306427
Epoch 370, training loss: 0.8552452921867371 = 0.17658187448978424 + 0.1 * 6.7866339683532715
Epoch 370, val loss: 0.8214620351791382
Epoch 380, training loss: 0.8380228281021118 = 0.1603544056415558 + 0.1 * 6.776683807373047
Epoch 380, val loss: 0.8342308402061462
Epoch 390, training loss: 0.8228645920753479 = 0.1455584615468979 + 0.1 * 6.7730607986450195
Epoch 390, val loss: 0.8481261134147644
Epoch 400, training loss: 0.810712993144989 = 0.13212543725967407 + 0.1 * 6.78587532043457
Epoch 400, val loss: 0.8629175424575806
Epoch 410, training loss: 0.7968785762786865 = 0.12004435062408447 + 0.1 * 6.768342018127441
Epoch 410, val loss: 0.8783226013183594
Epoch 420, training loss: 0.784777820110321 = 0.10917157679796219 + 0.1 * 6.7560625076293945
Epoch 420, val loss: 0.8942365050315857
Epoch 430, training loss: 0.7738569974899292 = 0.09938489645719528 + 0.1 * 6.744720935821533
Epoch 430, val loss: 0.9107162952423096
Epoch 440, training loss: 0.7700148224830627 = 0.09058915823698044 + 0.1 * 6.794256687164307
Epoch 440, val loss: 0.9272730946540833
Epoch 450, training loss: 0.7571739554405212 = 0.08277761191129684 + 0.1 * 6.743963241577148
Epoch 450, val loss: 0.9437447786331177
Epoch 460, training loss: 0.748637855052948 = 0.075801320374012 + 0.1 * 6.728365421295166
Epoch 460, val loss: 0.9599353671073914
Epoch 470, training loss: 0.7423940300941467 = 0.06954807788133621 + 0.1 * 6.728459358215332
Epoch 470, val loss: 0.976053774356842
Epoch 480, training loss: 0.7361268997192383 = 0.06395217776298523 + 0.1 * 6.721747398376465
Epoch 480, val loss: 0.9918120503425598
Epoch 490, training loss: 0.7300826907157898 = 0.05894092470407486 + 0.1 * 6.711417198181152
Epoch 490, val loss: 1.0072475671768188
Epoch 500, training loss: 0.7260299324989319 = 0.05443894863128662 + 0.1 * 6.715909481048584
Epoch 500, val loss: 1.0222930908203125
Epoch 510, training loss: 0.7199198007583618 = 0.05040198937058449 + 0.1 * 6.695178031921387
Epoch 510, val loss: 1.0368009805679321
Epoch 520, training loss: 0.7159983515739441 = 0.04677087441086769 + 0.1 * 6.692274570465088
Epoch 520, val loss: 1.0509183406829834
Epoch 530, training loss: 0.7122966647148132 = 0.04349428415298462 + 0.1 * 6.688023567199707
Epoch 530, val loss: 1.0647093057632446
Epoch 540, training loss: 0.7083178758621216 = 0.040533408522605896 + 0.1 * 6.677844524383545
Epoch 540, val loss: 1.077881097793579
Epoch 550, training loss: 0.7053456902503967 = 0.037855733186006546 + 0.1 * 6.674899101257324
Epoch 550, val loss: 1.0908253192901611
Epoch 560, training loss: 0.703294575214386 = 0.035423632711172104 + 0.1 * 6.678709506988525
Epoch 560, val loss: 1.1033148765563965
Epoch 570, training loss: 0.7006030082702637 = 0.033212415874004364 + 0.1 * 6.673905849456787
Epoch 570, val loss: 1.1152013540267944
Epoch 580, training loss: 0.6972019672393799 = 0.031202398240566254 + 0.1 * 6.659996032714844
Epoch 580, val loss: 1.126835823059082
Epoch 590, training loss: 0.6944659352302551 = 0.029367441311478615 + 0.1 * 6.650984764099121
Epoch 590, val loss: 1.1379854679107666
Epoch 600, training loss: 0.6916841268539429 = 0.027688436210155487 + 0.1 * 6.639956951141357
Epoch 600, val loss: 1.148952603340149
Epoch 610, training loss: 0.6897732019424438 = 0.02614614926278591 + 0.1 * 6.636270046234131
Epoch 610, val loss: 1.1592309474945068
Epoch 620, training loss: 0.6884607076644897 = 0.02473522536456585 + 0.1 * 6.63725471496582
Epoch 620, val loss: 1.169516921043396
Epoch 630, training loss: 0.6875765323638916 = 0.023434318602085114 + 0.1 * 6.641421794891357
Epoch 630, val loss: 1.1791983842849731
Epoch 640, training loss: 0.6870202422142029 = 0.022237349301576614 + 0.1 * 6.647828578948975
Epoch 640, val loss: 1.1886893510818481
Epoch 650, training loss: 0.6835538744926453 = 0.021132908761501312 + 0.1 * 6.624209880828857
Epoch 650, val loss: 1.197782039642334
Epoch 660, training loss: 0.6819875836372375 = 0.02011028490960598 + 0.1 * 6.618772506713867
Epoch 660, val loss: 1.206758737564087
Epoch 670, training loss: 0.680603563785553 = 0.019160643219947815 + 0.1 * 6.614429473876953
Epoch 670, val loss: 1.2151795625686646
Epoch 680, training loss: 0.6786007285118103 = 0.01828090101480484 + 0.1 * 6.603198051452637
Epoch 680, val loss: 1.2235816717147827
Epoch 690, training loss: 0.679158627986908 = 0.01746099255979061 + 0.1 * 6.616976261138916
Epoch 690, val loss: 1.2316969633102417
Epoch 700, training loss: 0.677501916885376 = 0.01669708825647831 + 0.1 * 6.608047962188721
Epoch 700, val loss: 1.2394278049468994
Epoch 710, training loss: 0.6758177280426025 = 0.015985481441020966 + 0.1 * 6.59832239151001
Epoch 710, val loss: 1.2471758127212524
Epoch 720, training loss: 0.6772124171257019 = 0.01531869638711214 + 0.1 * 6.618937015533447
Epoch 720, val loss: 1.254434585571289
Epoch 730, training loss: 0.674645721912384 = 0.014696012251079082 + 0.1 * 6.599497318267822
Epoch 730, val loss: 1.2616723775863647
Epoch 740, training loss: 0.672961413860321 = 0.014113197103142738 + 0.1 * 6.588481903076172
Epoch 740, val loss: 1.2687033414840698
Epoch 750, training loss: 0.671394944190979 = 0.013565040193498135 + 0.1 * 6.578298568725586
Epoch 750, val loss: 1.2753702402114868
Epoch 760, training loss: 0.6710098385810852 = 0.013052110560238361 + 0.1 * 6.579577445983887
Epoch 760, val loss: 1.2821433544158936
Epoch 770, training loss: 0.6713441014289856 = 0.012568670324981213 + 0.1 * 6.587754249572754
Epoch 770, val loss: 1.2886600494384766
Epoch 780, training loss: 0.6698058843612671 = 0.01211352925747633 + 0.1 * 6.576923370361328
Epoch 780, val loss: 1.2948846817016602
Epoch 790, training loss: 0.6681714057922363 = 0.011685658246278763 + 0.1 * 6.564857006072998
Epoch 790, val loss: 1.3012295961380005
Epoch 800, training loss: 0.6677939891815186 = 0.011280556209385395 + 0.1 * 6.565134525299072
Epoch 800, val loss: 1.307250738143921
Epoch 810, training loss: 0.6669183373451233 = 0.010897223837673664 + 0.1 * 6.560211181640625
Epoch 810, val loss: 1.3129316568374634
Epoch 820, training loss: 0.666263997554779 = 0.010536937974393368 + 0.1 * 6.5572710037231445
Epoch 820, val loss: 1.3186900615692139
Epoch 830, training loss: 0.6651149988174438 = 0.010195172391831875 + 0.1 * 6.549198150634766
Epoch 830, val loss: 1.3243048191070557
Epoch 840, training loss: 0.6661949157714844 = 0.009870604611933231 + 0.1 * 6.5632429122924805
Epoch 840, val loss: 1.329757571220398
Epoch 850, training loss: 0.6664837002754211 = 0.009561805985867977 + 0.1 * 6.56921911239624
Epoch 850, val loss: 1.3349130153656006
Epoch 860, training loss: 0.6653705835342407 = 0.009269699454307556 + 0.1 * 6.561008453369141
Epoch 860, val loss: 1.340047836303711
Epoch 870, training loss: 0.6636333465576172 = 0.008992796763777733 + 0.1 * 6.54640531539917
Epoch 870, val loss: 1.3450947999954224
Epoch 880, training loss: 0.6629765629768372 = 0.008729188703000546 + 0.1 * 6.542473793029785
Epoch 880, val loss: 1.3500823974609375
Epoch 890, training loss: 0.6644822955131531 = 0.00847739540040493 + 0.1 * 6.560049057006836
Epoch 890, val loss: 1.3548130989074707
Epoch 900, training loss: 0.6623774170875549 = 0.008237632922828197 + 0.1 * 6.541397571563721
Epoch 900, val loss: 1.3594530820846558
Epoch 910, training loss: 0.6613914370536804 = 0.00800996832549572 + 0.1 * 6.533814430236816
Epoch 910, val loss: 1.3641431331634521
Epoch 920, training loss: 0.6615331172943115 = 0.007792145945131779 + 0.1 * 6.53740930557251
Epoch 920, val loss: 1.3686981201171875
Epoch 930, training loss: 0.661503255367279 = 0.007583379279822111 + 0.1 * 6.539198875427246
Epoch 930, val loss: 1.3728700876235962
Epoch 940, training loss: 0.6599795818328857 = 0.007384833414107561 + 0.1 * 6.525947093963623
Epoch 940, val loss: 1.3772040605545044
Epoch 950, training loss: 0.660413920879364 = 0.007195151410996914 + 0.1 * 6.532187461853027
Epoch 950, val loss: 1.3815010786056519
Epoch 960, training loss: 0.6593638062477112 = 0.007013583090156317 + 0.1 * 6.523502349853516
Epoch 960, val loss: 1.385664939880371
Epoch 970, training loss: 0.6586253046989441 = 0.006838904228061438 + 0.1 * 6.517863750457764
Epoch 970, val loss: 1.389709234237671
Epoch 980, training loss: 0.6593421101570129 = 0.006671618204563856 + 0.1 * 6.526704788208008
Epoch 980, val loss: 1.3935894966125488
Epoch 990, training loss: 0.6582595705986023 = 0.006511699873954058 + 0.1 * 6.5174784660339355
Epoch 990, val loss: 1.397533655166626
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 2.8229284286499023 = 1.9632434844970703 + 0.1 * 8.59684944152832
Epoch 0, val loss: 1.9632683992385864
Epoch 10, training loss: 2.8120837211608887 = 1.952407956123352 + 0.1 * 8.596756935119629
Epoch 10, val loss: 1.9521691799163818
Epoch 20, training loss: 2.798231601715088 = 1.9385932683944702 + 0.1 * 8.596384048461914
Epoch 20, val loss: 1.937855839729309
Epoch 30, training loss: 2.778360605239868 = 1.9190001487731934 + 0.1 * 8.59360408782959
Epoch 30, val loss: 1.917647123336792
Epoch 40, training loss: 2.7474703788757324 = 1.8901193141937256 + 0.1 * 8.573509216308594
Epoch 40, val loss: 1.8884897232055664
Epoch 50, training loss: 2.6987781524658203 = 1.851428747177124 + 0.1 * 8.473494529724121
Epoch 50, val loss: 1.851670742034912
Epoch 60, training loss: 2.641608476638794 = 1.8128128051757812 + 0.1 * 8.287956237792969
Epoch 60, val loss: 1.8184947967529297
Epoch 70, training loss: 2.5789222717285156 = 1.7825268507003784 + 0.1 * 7.963954925537109
Epoch 70, val loss: 1.7930424213409424
Epoch 80, training loss: 2.4981324672698975 = 1.7505748271942139 + 0.1 * 7.475576400756836
Epoch 80, val loss: 1.7634648084640503
Epoch 90, training loss: 2.4319963455200195 = 1.711930274963379 + 0.1 * 7.2006611824035645
Epoch 90, val loss: 1.729034662246704
Epoch 100, training loss: 2.370443105697632 = 1.6609845161437988 + 0.1 * 7.094584941864014
Epoch 100, val loss: 1.6843373775482178
Epoch 110, training loss: 2.298161745071411 = 1.595033049583435 + 0.1 * 7.03128719329834
Epoch 110, val loss: 1.6252710819244385
Epoch 120, training loss: 2.2170822620391846 = 1.5185211896896362 + 0.1 * 6.985610485076904
Epoch 120, val loss: 1.5591102838516235
Epoch 130, training loss: 2.1333634853363037 = 1.4384045600891113 + 0.1 * 6.949589252471924
Epoch 130, val loss: 1.4920976161956787
Epoch 140, training loss: 2.0511112213134766 = 1.3595852851867676 + 0.1 * 6.915258407592773
Epoch 140, val loss: 1.4304405450820923
Epoch 150, training loss: 1.9707533121109009 = 1.2819595336914062 + 0.1 * 6.887937545776367
Epoch 150, val loss: 1.3718777894973755
Epoch 160, training loss: 1.8933807611465454 = 1.207090139389038 + 0.1 * 6.862905979156494
Epoch 160, val loss: 1.317017912864685
Epoch 170, training loss: 1.81899094581604 = 1.134438395500183 + 0.1 * 6.845524787902832
Epoch 170, val loss: 1.2651172876358032
Epoch 180, training loss: 1.7464499473571777 = 1.0634628534317017 + 0.1 * 6.82987117767334
Epoch 180, val loss: 1.2152031660079956
Epoch 190, training loss: 1.677537202835083 = 0.995343029499054 + 0.1 * 6.821942329406738
Epoch 190, val loss: 1.167023777961731
Epoch 200, training loss: 1.6114070415496826 = 0.9303856492042542 + 0.1 * 6.810213565826416
Epoch 200, val loss: 1.1205211877822876
Epoch 210, training loss: 1.5470471382141113 = 0.8668059706687927 + 0.1 * 6.802411079406738
Epoch 210, val loss: 1.0744470357894897
Epoch 220, training loss: 1.4860424995422363 = 0.8042635917663574 + 0.1 * 6.817788124084473
Epoch 220, val loss: 1.028388500213623
Epoch 230, training loss: 1.423598289489746 = 0.744735836982727 + 0.1 * 6.788623809814453
Epoch 230, val loss: 0.9849871397018433
Epoch 240, training loss: 1.365593433380127 = 0.6882598400115967 + 0.1 * 6.7733354568481445
Epoch 240, val loss: 0.9446509480476379
Epoch 250, training loss: 1.3117276430130005 = 0.6356642246246338 + 0.1 * 6.760633945465088
Epoch 250, val loss: 0.9088205099105835
Epoch 260, training loss: 1.2641228437423706 = 0.5877223014831543 + 0.1 * 6.764005184173584
Epoch 260, val loss: 0.8789383172988892
Epoch 270, training loss: 1.219416856765747 = 0.5447460412979126 + 0.1 * 6.746707439422607
Epoch 270, val loss: 0.8558164834976196
Epoch 280, training loss: 1.1793723106384277 = 0.5057788491249084 + 0.1 * 6.735934257507324
Epoch 280, val loss: 0.8382346034049988
Epoch 290, training loss: 1.1421566009521484 = 0.4696768522262573 + 0.1 * 6.724796772003174
Epoch 290, val loss: 0.8251479268074036
Epoch 300, training loss: 1.1071386337280273 = 0.43553152680397034 + 0.1 * 6.716071128845215
Epoch 300, val loss: 0.8154927492141724
Epoch 310, training loss: 1.0737487077713013 = 0.4027230441570282 + 0.1 * 6.710256576538086
Epoch 310, val loss: 0.8085609078407288
Epoch 320, training loss: 1.0415126085281372 = 0.3709161579608917 + 0.1 * 6.7059645652771
Epoch 320, val loss: 0.8035503625869751
Epoch 330, training loss: 1.0105724334716797 = 0.3401726484298706 + 0.1 * 6.703997611999512
Epoch 330, val loss: 0.8003872036933899
Epoch 340, training loss: 0.9796649813652039 = 0.31067919731140137 + 0.1 * 6.689857482910156
Epoch 340, val loss: 0.7990248799324036
Epoch 350, training loss: 0.9508031606674194 = 0.28265565633773804 + 0.1 * 6.681474685668945
Epoch 350, val loss: 0.7992323040962219
Epoch 360, training loss: 0.9241546392440796 = 0.2562159299850464 + 0.1 * 6.679387092590332
Epoch 360, val loss: 0.8011488318443298
Epoch 370, training loss: 0.8996554613113403 = 0.23130522668361664 + 0.1 * 6.683502197265625
Epoch 370, val loss: 0.8044691681861877
Epoch 380, training loss: 0.8744159936904907 = 0.20808541774749756 + 0.1 * 6.663305759429932
Epoch 380, val loss: 0.8093442916870117
Epoch 390, training loss: 0.8523791432380676 = 0.18661034107208252 + 0.1 * 6.657688140869141
Epoch 390, val loss: 0.8155193328857422
Epoch 400, training loss: 0.833957314491272 = 0.16702574491500854 + 0.1 * 6.669315814971924
Epoch 400, val loss: 0.8230719566345215
Epoch 410, training loss: 0.8147828578948975 = 0.1495320200920105 + 0.1 * 6.65250825881958
Epoch 410, val loss: 0.8321186304092407
Epoch 420, training loss: 0.7981470227241516 = 0.13399456441402435 + 0.1 * 6.641524314880371
Epoch 420, val loss: 0.8421666622161865
Epoch 430, training loss: 0.7851144075393677 = 0.12023107707500458 + 0.1 * 6.64883279800415
Epoch 430, val loss: 0.8534313440322876
Epoch 440, training loss: 0.771894097328186 = 0.10814879834651947 + 0.1 * 6.637452602386475
Epoch 440, val loss: 0.8655562996864319
Epoch 450, training loss: 0.7603418827056885 = 0.097507543861866 + 0.1 * 6.62834358215332
Epoch 450, val loss: 0.8783133625984192
Epoch 460, training loss: 0.7508494853973389 = 0.08811928331851959 + 0.1 * 6.627302169799805
Epoch 460, val loss: 0.8915966749191284
Epoch 470, training loss: 0.741597592830658 = 0.07984286546707153 + 0.1 * 6.617547035217285
Epoch 470, val loss: 0.9052639007568359
Epoch 480, training loss: 0.734463095664978 = 0.07250387966632843 + 0.1 * 6.619592189788818
Epoch 480, val loss: 0.9190914630889893
Epoch 490, training loss: 0.7276029586791992 = 0.06600004434585571 + 0.1 * 6.616029262542725
Epoch 490, val loss: 0.932819128036499
Epoch 500, training loss: 0.7210285663604736 = 0.06022222340106964 + 0.1 * 6.608063697814941
Epoch 500, val loss: 0.9466360807418823
Epoch 510, training loss: 0.7164826393127441 = 0.05508141219615936 + 0.1 * 6.614011764526367
Epoch 510, val loss: 0.9602153897285461
Epoch 520, training loss: 0.7100031971931458 = 0.05051765963435173 + 0.1 * 6.594854831695557
Epoch 520, val loss: 0.9738195538520813
Epoch 530, training loss: 0.7060644030570984 = 0.046444348990917206 + 0.1 * 6.596200466156006
Epoch 530, val loss: 0.9870425462722778
Epoch 540, training loss: 0.7019548416137695 = 0.04281311109662056 + 0.1 * 6.59141731262207
Epoch 540, val loss: 1.0000258684158325
Epoch 550, training loss: 0.698180615901947 = 0.03957267478108406 + 0.1 * 6.5860795974731445
Epoch 550, val loss: 1.0128370523452759
Epoch 560, training loss: 0.6946960687637329 = 0.03667288273572922 + 0.1 * 6.580231666564941
Epoch 560, val loss: 1.0250996351242065
Epoch 570, training loss: 0.6915397644042969 = 0.03407619148492813 + 0.1 * 6.5746355056762695
Epoch 570, val loss: 1.0373038053512573
Epoch 580, training loss: 0.6893197298049927 = 0.031734973192214966 + 0.1 * 6.575847148895264
Epoch 580, val loss: 1.0491586923599243
Epoch 590, training loss: 0.6866061091423035 = 0.029626131057739258 + 0.1 * 6.569799423217773
Epoch 590, val loss: 1.0604307651519775
Epoch 600, training loss: 0.6847711801528931 = 0.02772611938416958 + 0.1 * 6.570450305938721
Epoch 600, val loss: 1.0715208053588867
Epoch 610, training loss: 0.6823364496231079 = 0.026009516790509224 + 0.1 * 6.56326961517334
Epoch 610, val loss: 1.0823408365249634
Epoch 620, training loss: 0.6796274185180664 = 0.024448616430163383 + 0.1 * 6.551787853240967
Epoch 620, val loss: 1.0927478075027466
Epoch 630, training loss: 0.6813643574714661 = 0.023025240749120712 + 0.1 * 6.583391189575195
Epoch 630, val loss: 1.1027610301971436
Epoch 640, training loss: 0.6782532334327698 = 0.021732209250330925 + 0.1 * 6.565210342407227
Epoch 640, val loss: 1.1123921871185303
Epoch 650, training loss: 0.675299882888794 = 0.020551765337586403 + 0.1 * 6.547481536865234
Epoch 650, val loss: 1.1221411228179932
Epoch 660, training loss: 0.6745192408561707 = 0.01946522295475006 + 0.1 * 6.550540447235107
Epoch 660, val loss: 1.1310863494873047
Epoch 670, training loss: 0.6718524098396301 = 0.018466608598828316 + 0.1 * 6.533857822418213
Epoch 670, val loss: 1.1400989294052124
Epoch 680, training loss: 0.6709531545639038 = 0.017545970156788826 + 0.1 * 6.534071445465088
Epoch 680, val loss: 1.1486438512802124
Epoch 690, training loss: 0.6726341247558594 = 0.016697492450475693 + 0.1 * 6.559366703033447
Epoch 690, val loss: 1.1567579507827759
Epoch 700, training loss: 0.6686826348304749 = 0.01591753400862217 + 0.1 * 6.527650833129883
Epoch 700, val loss: 1.1650352478027344
Epoch 710, training loss: 0.667625367641449 = 0.015194468200206757 + 0.1 * 6.524308681488037
Epoch 710, val loss: 1.1731024980545044
Epoch 720, training loss: 0.6662662029266357 = 0.014520065858960152 + 0.1 * 6.51746129989624
Epoch 720, val loss: 1.1807292699813843
Epoch 730, training loss: 0.6673991680145264 = 0.013891294598579407 + 0.1 * 6.535079002380371
Epoch 730, val loss: 1.1882649660110474
Epoch 740, training loss: 0.6657983064651489 = 0.013306349515914917 + 0.1 * 6.524919033050537
Epoch 740, val loss: 1.195250391960144
Epoch 750, training loss: 0.6650079488754272 = 0.012761213816702366 + 0.1 * 6.522467136383057
Epoch 750, val loss: 1.20240318775177
Epoch 760, training loss: 0.6640472412109375 = 0.012251646257936954 + 0.1 * 6.517955303192139
Epoch 760, val loss: 1.2090860605239868
Epoch 770, training loss: 0.6627526879310608 = 0.011775374412536621 + 0.1 * 6.509772777557373
Epoch 770, val loss: 1.2158454656600952
Epoch 780, training loss: 0.6618261933326721 = 0.011327964253723621 + 0.1 * 6.504981994628906
Epoch 780, val loss: 1.2224464416503906
Epoch 790, training loss: 0.6612585783004761 = 0.010906885378062725 + 0.1 * 6.503516674041748
Epoch 790, val loss: 1.228495478630066
Epoch 800, training loss: 0.660780668258667 = 0.010512416251003742 + 0.1 * 6.502682685852051
Epoch 800, val loss: 1.2349214553833008
Epoch 810, training loss: 0.6602076292037964 = 0.010140273720026016 + 0.1 * 6.500673294067383
Epoch 810, val loss: 1.2406549453735352
Epoch 820, training loss: 0.6594861745834351 = 0.009791248477995396 + 0.1 * 6.496949195861816
Epoch 820, val loss: 1.246766209602356
Epoch 830, training loss: 0.6592718958854675 = 0.009461170993745327 + 0.1 * 6.498107433319092
Epoch 830, val loss: 1.2526218891143799
Epoch 840, training loss: 0.6583800315856934 = 0.009148157201707363 + 0.1 * 6.492318630218506
Epoch 840, val loss: 1.2579134702682495
Epoch 850, training loss: 0.6579527258872986 = 0.008853117935359478 + 0.1 * 6.49099588394165
Epoch 850, val loss: 1.2635661363601685
Epoch 860, training loss: 0.6577433943748474 = 0.008572977036237717 + 0.1 * 6.491703987121582
Epoch 860, val loss: 1.2688945531845093
Epoch 870, training loss: 0.6574582457542419 = 0.008306956849992275 + 0.1 * 6.491512298583984
Epoch 870, val loss: 1.2739139795303345
Epoch 880, training loss: 0.6564649939537048 = 0.008055422455072403 + 0.1 * 6.484095096588135
Epoch 880, val loss: 1.2792469263076782
Epoch 890, training loss: 0.6571519374847412 = 0.007815679535269737 + 0.1 * 6.493362903594971
Epoch 890, val loss: 1.2841236591339111
Epoch 900, training loss: 0.6553243398666382 = 0.007587751839309931 + 0.1 * 6.477365970611572
Epoch 900, val loss: 1.288953423500061
Epoch 910, training loss: 0.6572176814079285 = 0.007370597682893276 + 0.1 * 6.498470783233643
Epoch 910, val loss: 1.293609857559204
Epoch 920, training loss: 0.6549740433692932 = 0.007164348382502794 + 0.1 * 6.4780964851379395
Epoch 920, val loss: 1.2982492446899414
Epoch 930, training loss: 0.6536861658096313 = 0.006968221161514521 + 0.1 * 6.467179775238037
Epoch 930, val loss: 1.3030152320861816
Epoch 940, training loss: 0.6561090350151062 = 0.006780216470360756 + 0.1 * 6.493288040161133
Epoch 940, val loss: 1.3074816465377808
Epoch 950, training loss: 0.6539663076400757 = 0.006600564811378717 + 0.1 * 6.473657131195068
Epoch 950, val loss: 1.3115029335021973
Epoch 960, training loss: 0.6530871987342834 = 0.006429300643503666 + 0.1 * 6.466578960418701
Epoch 960, val loss: 1.3158659934997559
Epoch 970, training loss: 0.6530822515487671 = 0.006265630479902029 + 0.1 * 6.468165874481201
Epoch 970, val loss: 1.3202075958251953
Epoch 980, training loss: 0.6522445678710938 = 0.006108736153692007 + 0.1 * 6.461358070373535
Epoch 980, val loss: 1.3241156339645386
Epoch 990, training loss: 0.6525893807411194 = 0.005958604626357555 + 0.1 * 6.466307640075684
Epoch 990, val loss: 1.3282748460769653
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 2.8048698902130127 = 1.945190191268921 + 0.1 * 8.596796035766602
Epoch 0, val loss: 1.9473700523376465
Epoch 10, training loss: 2.7954154014587402 = 1.9357470273971558 + 0.1 * 8.596683502197266
Epoch 10, val loss: 1.9381940364837646
Epoch 20, training loss: 2.7841744422912598 = 1.9245655536651611 + 0.1 * 8.596089363098145
Epoch 20, val loss: 1.926702618598938
Epoch 30, training loss: 2.768580436706543 = 1.9094048738479614 + 0.1 * 8.591755867004395
Epoch 30, val loss: 1.9105721712112427
Epoch 40, training loss: 2.7438247203826904 = 1.8873997926712036 + 0.1 * 8.564249992370605
Epoch 40, val loss: 1.8870023488998413
Epoch 50, training loss: 2.699861526489258 = 1.8570464849472046 + 0.1 * 8.428149223327637
Epoch 50, val loss: 1.8557000160217285
Epoch 60, training loss: 2.626563310623169 = 1.8216465711593628 + 0.1 * 8.049166679382324
Epoch 60, val loss: 1.8213958740234375
Epoch 70, training loss: 2.5461673736572266 = 1.7875332832336426 + 0.1 * 7.586341381072998
Epoch 70, val loss: 1.7911722660064697
Epoch 80, training loss: 2.482673406600952 = 1.751840353012085 + 0.1 * 7.30833101272583
Epoch 80, val loss: 1.759695291519165
Epoch 90, training loss: 2.425764560699463 = 1.7089582681655884 + 0.1 * 7.168062210083008
Epoch 90, val loss: 1.7215574979782104
Epoch 100, training loss: 2.361741065979004 = 1.6531126499176025 + 0.1 * 7.08628511428833
Epoch 100, val loss: 1.6724363565444946
Epoch 110, training loss: 2.2850499153137207 = 1.5807254314422607 + 0.1 * 7.043245792388916
Epoch 110, val loss: 1.609109878540039
Epoch 120, training loss: 2.194289445877075 = 1.4926507472991943 + 0.1 * 7.01638650894165
Epoch 120, val loss: 1.533290982246399
Epoch 130, training loss: 2.0942013263702393 = 1.394450306892395 + 0.1 * 6.997509479522705
Epoch 130, val loss: 1.4505032300949097
Epoch 140, training loss: 1.9894152879714966 = 1.2915210723876953 + 0.1 * 6.978941917419434
Epoch 140, val loss: 1.3666638135910034
Epoch 150, training loss: 1.8863451480865479 = 1.1900829076766968 + 0.1 * 6.962623119354248
Epoch 150, val loss: 1.2868733406066895
Epoch 160, training loss: 1.7889916896820068 = 1.0946931838989258 + 0.1 * 6.942984104156494
Epoch 160, val loss: 1.2134225368499756
Epoch 170, training loss: 1.701328158378601 = 1.0086177587509155 + 0.1 * 6.9271039962768555
Epoch 170, val loss: 1.1484471559524536
Epoch 180, training loss: 1.6233062744140625 = 0.9321516752243042 + 0.1 * 6.91154670715332
Epoch 180, val loss: 1.0917887687683105
Epoch 190, training loss: 1.5533456802368164 = 0.8634873032569885 + 0.1 * 6.898583889007568
Epoch 190, val loss: 1.0424355268478394
Epoch 200, training loss: 1.4899648427963257 = 0.8013225197792053 + 0.1 * 6.886423110961914
Epoch 200, val loss: 0.9990267157554626
Epoch 210, training loss: 1.4329071044921875 = 0.7442828416824341 + 0.1 * 6.8862433433532715
Epoch 210, val loss: 0.9605029225349426
Epoch 220, training loss: 1.3785998821258545 = 0.6923055648803711 + 0.1 * 6.862942218780518
Epoch 220, val loss: 0.9272862076759338
Epoch 230, training loss: 1.3286662101745605 = 0.6435877680778503 + 0.1 * 6.8507843017578125
Epoch 230, val loss: 0.8978033065795898
Epoch 240, training loss: 1.2812566757202148 = 0.5968561172485352 + 0.1 * 6.844006061553955
Epoch 240, val loss: 0.8719764351844788
Epoch 250, training loss: 1.2348906993865967 = 0.5519499778747559 + 0.1 * 6.829407215118408
Epoch 250, val loss: 0.8496811389923096
Epoch 260, training loss: 1.1896767616271973 = 0.5079792737960815 + 0.1 * 6.816974639892578
Epoch 260, val loss: 0.8304062485694885
Epoch 270, training loss: 1.1471097469329834 = 0.46557730436325073 + 0.1 * 6.815324306488037
Epoch 270, val loss: 0.8145484924316406
Epoch 280, training loss: 1.104416012763977 = 0.4250906705856323 + 0.1 * 6.793253421783447
Epoch 280, val loss: 0.802282452583313
Epoch 290, training loss: 1.0665203332901 = 0.3869546949863434 + 0.1 * 6.795655727386475
Epoch 290, val loss: 0.7935395836830139
Epoch 300, training loss: 1.0298237800598145 = 0.35178956389427185 + 0.1 * 6.7803425788879395
Epoch 300, val loss: 0.788080632686615
Epoch 310, training loss: 0.9968301057815552 = 0.31976115703582764 + 0.1 * 6.770689487457275
Epoch 310, val loss: 0.7855013012886047
Epoch 320, training loss: 0.9658898115158081 = 0.29056939482688904 + 0.1 * 6.753204345703125
Epoch 320, val loss: 0.785215437412262
Epoch 330, training loss: 0.9399601221084595 = 0.26383844017982483 + 0.1 * 6.761216640472412
Epoch 330, val loss: 0.7872449159622192
Epoch 340, training loss: 0.9135684967041016 = 0.23956194519996643 + 0.1 * 6.740065574645996
Epoch 340, val loss: 0.7910405397415161
Epoch 350, training loss: 0.8907496929168701 = 0.2174392193555832 + 0.1 * 6.733104228973389
Epoch 350, val loss: 0.796273410320282
Epoch 360, training loss: 0.8720272183418274 = 0.1972624659538269 + 0.1 * 6.747647285461426
Epoch 360, val loss: 0.8030000925064087
Epoch 370, training loss: 0.8508471250534058 = 0.17903560400009155 + 0.1 * 6.718115329742432
Epoch 370, val loss: 0.8109708428382874
Epoch 380, training loss: 0.8340549468994141 = 0.16253000497817993 + 0.1 * 6.715249061584473
Epoch 380, val loss: 0.8199698328971863
Epoch 390, training loss: 0.8191803693771362 = 0.14765793085098267 + 0.1 * 6.715224266052246
Epoch 390, val loss: 0.8296802639961243
Epoch 400, training loss: 0.804043173789978 = 0.13426771759986877 + 0.1 * 6.69775390625
Epoch 400, val loss: 0.840185821056366
Epoch 410, training loss: 0.7913213968276978 = 0.12218380719423294 + 0.1 * 6.691375732421875
Epoch 410, val loss: 0.8513399362564087
Epoch 420, training loss: 0.7809305191040039 = 0.1112985908985138 + 0.1 * 6.696319103240967
Epoch 420, val loss: 0.8628382682800293
Epoch 430, training loss: 0.7700641751289368 = 0.10153604298830032 + 0.1 * 6.685281276702881
Epoch 430, val loss: 0.8746228218078613
Epoch 440, training loss: 0.7615931034088135 = 0.0927898958325386 + 0.1 * 6.688032150268555
Epoch 440, val loss: 0.8866453170776367
Epoch 450, training loss: 0.7528165578842163 = 0.08495784550905228 + 0.1 * 6.678586959838867
Epoch 450, val loss: 0.8984537720680237
Epoch 460, training loss: 0.7438725829124451 = 0.0779295563697815 + 0.1 * 6.659430027008057
Epoch 460, val loss: 0.910477340221405
Epoch 470, training loss: 0.7408179044723511 = 0.07160559296607971 + 0.1 * 6.692122936248779
Epoch 470, val loss: 0.9223146438598633
Epoch 480, training loss: 0.7307284474372864 = 0.06594180315732956 + 0.1 * 6.647866249084473
Epoch 480, val loss: 0.9341922402381897
Epoch 490, training loss: 0.7283613681793213 = 0.06084846705198288 + 0.1 * 6.675128936767578
Epoch 490, val loss: 0.9457898139953613
Epoch 500, training loss: 0.720565140247345 = 0.0562748983502388 + 0.1 * 6.642902374267578
Epoch 500, val loss: 0.9572585225105286
Epoch 510, training loss: 0.7152889966964722 = 0.05214809626340866 + 0.1 * 6.63140869140625
Epoch 510, val loss: 0.9685307145118713
Epoch 520, training loss: 0.7142260670661926 = 0.048414990305900574 + 0.1 * 6.658110618591309
Epoch 520, val loss: 0.9794846177101135
Epoch 530, training loss: 0.7081435322761536 = 0.045047953724861145 + 0.1 * 6.630955696105957
Epoch 530, val loss: 0.9902727007865906
Epoch 540, training loss: 0.7046036720275879 = 0.04200199618935585 + 0.1 * 6.626016139984131
Epoch 540, val loss: 1.0005348920822144
Epoch 550, training loss: 0.7010459303855896 = 0.03924348205327988 + 0.1 * 6.618024826049805
Epoch 550, val loss: 1.0108195543289185
Epoch 560, training loss: 0.6981582045555115 = 0.03673052787780762 + 0.1 * 6.61427640914917
Epoch 560, val loss: 1.0207631587982178
Epoch 570, training loss: 0.6952909231185913 = 0.03444169834256172 + 0.1 * 6.608491897583008
Epoch 570, val loss: 1.0304388999938965
Epoch 580, training loss: 0.6930387020111084 = 0.03235369548201561 + 0.1 * 6.606849670410156
Epoch 580, val loss: 1.0398733615875244
Epoch 590, training loss: 0.6918622851371765 = 0.030445031821727753 + 0.1 * 6.614172458648682
Epoch 590, val loss: 1.0489528179168701
Epoch 600, training loss: 0.6875311136245728 = 0.0287009309977293 + 0.1 * 6.588301658630371
Epoch 600, val loss: 1.0579075813293457
Epoch 610, training loss: 0.6857732534408569 = 0.0270991288125515 + 0.1 * 6.586740970611572
Epoch 610, val loss: 1.0665812492370605
Epoch 620, training loss: 0.6845450401306152 = 0.025625871494412422 + 0.1 * 6.589191436767578
Epoch 620, val loss: 1.0749224424362183
Epoch 630, training loss: 0.6834081411361694 = 0.02427130751311779 + 0.1 * 6.591368198394775
Epoch 630, val loss: 1.0830720663070679
Epoch 640, training loss: 0.6810743808746338 = 0.02302258275449276 + 0.1 * 6.580517768859863
Epoch 640, val loss: 1.091093897819519
Epoch 650, training loss: 0.6810714602470398 = 0.02186623401939869 + 0.1 * 6.592051982879639
Epoch 650, val loss: 1.09877610206604
Epoch 660, training loss: 0.6782991886138916 = 0.020796841010451317 + 0.1 * 6.575023651123047
Epoch 660, val loss: 1.106359601020813
Epoch 670, training loss: 0.6760881543159485 = 0.01980396918952465 + 0.1 * 6.562841892242432
Epoch 670, val loss: 1.1136505603790283
Epoch 680, training loss: 0.6784509420394897 = 0.01888101175427437 + 0.1 * 6.595698833465576
Epoch 680, val loss: 1.1208134889602661
Epoch 690, training loss: 0.6736245155334473 = 0.018023137003183365 + 0.1 * 6.556013584136963
Epoch 690, val loss: 1.1277753114700317
Epoch 700, training loss: 0.6733384132385254 = 0.017223887145519257 + 0.1 * 6.561145305633545
Epoch 700, val loss: 1.1345105171203613
Epoch 710, training loss: 0.6716610789299011 = 0.016477633267641068 + 0.1 * 6.5518341064453125
Epoch 710, val loss: 1.141192078590393
Epoch 720, training loss: 0.6726932525634766 = 0.015779316425323486 + 0.1 * 6.56913948059082
Epoch 720, val loss: 1.147606611251831
Epoch 730, training loss: 0.6696311235427856 = 0.015127519145607948 + 0.1 * 6.5450358390808105
Epoch 730, val loss: 1.1536706686019897
Epoch 740, training loss: 0.668886125087738 = 0.014519535936415195 + 0.1 * 6.543665885925293
Epoch 740, val loss: 1.1598619222640991
Epoch 750, training loss: 0.6674630045890808 = 0.01394823007285595 + 0.1 * 6.535147666931152
Epoch 750, val loss: 1.1658772230148315
Epoch 760, training loss: 0.6675013303756714 = 0.013409729115664959 + 0.1 * 6.5409159660339355
Epoch 760, val loss: 1.1717520952224731
Epoch 770, training loss: 0.6667993664741516 = 0.012902194634079933 + 0.1 * 6.538971424102783
Epoch 770, val loss: 1.1774629354476929
Epoch 780, training loss: 0.6681458950042725 = 0.012423803098499775 + 0.1 * 6.557220935821533
Epoch 780, val loss: 1.1828646659851074
Epoch 790, training loss: 0.6659074425697327 = 0.011975122615695 + 0.1 * 6.539323329925537
Epoch 790, val loss: 1.1883233785629272
Epoch 800, training loss: 0.6643455028533936 = 0.01155126467347145 + 0.1 * 6.527942180633545
Epoch 800, val loss: 1.193672776222229
Epoch 810, training loss: 0.6659240126609802 = 0.011150119826197624 + 0.1 * 6.547739028930664
Epoch 810, val loss: 1.198834776878357
Epoch 820, training loss: 0.6628729104995728 = 0.01077082846313715 + 0.1 * 6.521020412445068
Epoch 820, val loss: 1.2037612199783325
Epoch 830, training loss: 0.6630668044090271 = 0.010412885807454586 + 0.1 * 6.526539325714111
Epoch 830, val loss: 1.2087173461914062
Epoch 840, training loss: 0.6621895432472229 = 0.010073697194457054 + 0.1 * 6.521158218383789
Epoch 840, val loss: 1.2136329412460327
Epoch 850, training loss: 0.6616687178611755 = 0.00975092500448227 + 0.1 * 6.5191779136657715
Epoch 850, val loss: 1.2182910442352295
Epoch 860, training loss: 0.6616170406341553 = 0.009445062838494778 + 0.1 * 6.521719455718994
Epoch 860, val loss: 1.2228847742080688
Epoch 870, training loss: 0.6607975959777832 = 0.00915471836924553 + 0.1 * 6.516428470611572
Epoch 870, val loss: 1.227339744567871
Epoch 880, training loss: 0.6599826216697693 = 0.00887871440500021 + 0.1 * 6.5110392570495605
Epoch 880, val loss: 1.2318501472473145
Epoch 890, training loss: 0.6598941087722778 = 0.008615363389253616 + 0.1 * 6.512787342071533
Epoch 890, val loss: 1.2360585927963257
Epoch 900, training loss: 0.659480631351471 = 0.00836514588445425 + 0.1 * 6.511154651641846
Epoch 900, val loss: 1.2403113842010498
Epoch 910, training loss: 0.6588817834854126 = 0.008126387372612953 + 0.1 * 6.507554054260254
Epoch 910, val loss: 1.2444792985916138
Epoch 920, training loss: 0.6577528119087219 = 0.007898387499153614 + 0.1 * 6.498544216156006
Epoch 920, val loss: 1.2484471797943115
Epoch 930, training loss: 0.6593148708343506 = 0.007680834736675024 + 0.1 * 6.516340255737305
Epoch 930, val loss: 1.2524728775024414
Epoch 940, training loss: 0.6571404933929443 = 0.00747299287468195 + 0.1 * 6.49667501449585
Epoch 940, val loss: 1.256232738494873
Epoch 950, training loss: 0.6576804518699646 = 0.0072745936922729015 + 0.1 * 6.504058837890625
Epoch 950, val loss: 1.260071873664856
Epoch 960, training loss: 0.6568494439125061 = 0.00708451122045517 + 0.1 * 6.497649192810059
Epoch 960, val loss: 1.2638746500015259
Epoch 970, training loss: 0.656722366809845 = 0.006902485620230436 + 0.1 * 6.498198986053467
Epoch 970, val loss: 1.2674967050552368
Epoch 980, training loss: 0.6567752361297607 = 0.006728349253535271 + 0.1 * 6.500468730926514
Epoch 980, val loss: 1.2711470127105713
Epoch 990, training loss: 0.6558317542076111 = 0.006561299320310354 + 0.1 * 6.492704391479492
Epoch 990, val loss: 1.2746047973632812
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8181338956246705
The final CL Acc:0.75432, 0.00924, The final GNN Acc:0.81392, 0.00301
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13258])
remove edge: torch.Size([2, 7918])
updated graph: torch.Size([2, 10620])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.786890745162964 = 1.9272102117538452 + 0.1 * 8.59680461883545
Epoch 0, val loss: 1.9284148216247559
Epoch 10, training loss: 2.77764630317688 = 1.9179768562316895 + 0.1 * 8.596694946289062
Epoch 10, val loss: 1.9187567234039307
Epoch 20, training loss: 2.766071319580078 = 1.9064598083496094 + 0.1 * 8.596115112304688
Epoch 20, val loss: 1.9066447019577026
Epoch 30, training loss: 2.749250650405884 = 1.8900998830795288 + 0.1 * 8.591507911682129
Epoch 30, val loss: 1.8892911672592163
Epoch 40, training loss: 2.7215347290039062 = 1.8658558130264282 + 0.1 * 8.55678939819336
Epoch 40, val loss: 1.8634138107299805
Epoch 50, training loss: 2.6608943939208984 = 1.8328115940093994 + 0.1 * 8.280827522277832
Epoch 50, val loss: 1.8298964500427246
Epoch 60, training loss: 2.577953815460205 = 1.796440601348877 + 0.1 * 7.815133094787598
Epoch 60, val loss: 1.7957427501678467
Epoch 70, training loss: 2.501530885696411 = 1.7595235109329224 + 0.1 * 7.42007303237915
Epoch 70, val loss: 1.7615797519683838
Epoch 80, training loss: 2.440372943878174 = 1.7170196771621704 + 0.1 * 7.233532428741455
Epoch 80, val loss: 1.722482442855835
Epoch 90, training loss: 2.376596689224243 = 1.664052128791809 + 0.1 * 7.125446319580078
Epoch 90, val loss: 1.6749898195266724
Epoch 100, training loss: 2.299901247024536 = 1.595017433166504 + 0.1 * 7.048838138580322
Epoch 100, val loss: 1.6141324043273926
Epoch 110, training loss: 2.2079081535339355 = 1.5082608461380005 + 0.1 * 6.9964728355407715
Epoch 110, val loss: 1.5377575159072876
Epoch 120, training loss: 2.103140354156494 = 1.4070576429367065 + 0.1 * 6.960827827453613
Epoch 120, val loss: 1.450731635093689
Epoch 130, training loss: 1.9905357360839844 = 1.296906590461731 + 0.1 * 6.936291217803955
Epoch 130, val loss: 1.3576874732971191
Epoch 140, training loss: 1.876225233078003 = 1.1848070621490479 + 0.1 * 6.914181232452393
Epoch 140, val loss: 1.2646113634109497
Epoch 150, training loss: 1.7678065299987793 = 1.078774333000183 + 0.1 * 6.890322685241699
Epoch 150, val loss: 1.178251028060913
Epoch 160, training loss: 1.6679158210754395 = 0.9815471768379211 + 0.1 * 6.863686561584473
Epoch 160, val loss: 1.10056471824646
Epoch 170, training loss: 1.579057216644287 = 0.8959817886352539 + 0.1 * 6.830754280090332
Epoch 170, val loss: 1.0340349674224854
Epoch 180, training loss: 1.5014616250991821 = 0.8209907412528992 + 0.1 * 6.804708957672119
Epoch 180, val loss: 0.9776914119720459
Epoch 190, training loss: 1.4328348636627197 = 0.7551879286766052 + 0.1 * 6.776468753814697
Epoch 190, val loss: 0.9308149814605713
Epoch 200, training loss: 1.3715832233428955 = 0.6965219974517822 + 0.1 * 6.750611305236816
Epoch 200, val loss: 0.8914198279380798
Epoch 210, training loss: 1.318512201309204 = 0.6444227695465088 + 0.1 * 6.740893363952637
Epoch 210, val loss: 0.8590644001960754
Epoch 220, training loss: 1.2690789699554443 = 0.5977964401245117 + 0.1 * 6.712825775146484
Epoch 220, val loss: 0.8327293395996094
Epoch 230, training loss: 1.2245509624481201 = 0.5548917055130005 + 0.1 * 6.696592807769775
Epoch 230, val loss: 0.8110854625701904
Epoch 240, training loss: 1.1852185726165771 = 0.5153959393501282 + 0.1 * 6.698225975036621
Epoch 240, val loss: 0.7939531207084656
Epoch 250, training loss: 1.1460838317871094 = 0.47918277978897095 + 0.1 * 6.669010162353516
Epoch 250, val loss: 0.7808319926261902
Epoch 260, training loss: 1.112067461013794 = 0.4456554651260376 + 0.1 * 6.664119243621826
Epoch 260, val loss: 0.7708813548088074
Epoch 270, training loss: 1.0794165134429932 = 0.41460227966308594 + 0.1 * 6.648141860961914
Epoch 270, val loss: 0.763798177242279
Epoch 280, training loss: 1.050535798072815 = 0.385463684797287 + 0.1 * 6.650720596313477
Epoch 280, val loss: 0.7590039372444153
Epoch 290, training loss: 1.0217622518539429 = 0.3581889569759369 + 0.1 * 6.635732650756836
Epoch 290, val loss: 0.7563918232917786
Epoch 300, training loss: 0.9955329895019531 = 0.3325040340423584 + 0.1 * 6.630289554595947
Epoch 300, val loss: 0.7556198239326477
Epoch 310, training loss: 0.9714705348014832 = 0.30832791328430176 + 0.1 * 6.6314263343811035
Epoch 310, val loss: 0.7565690279006958
Epoch 320, training loss: 0.9469501972198486 = 0.2855967581272125 + 0.1 * 6.613534450531006
Epoch 320, val loss: 0.7591776847839355
Epoch 330, training loss: 0.9249509572982788 = 0.2641933262348175 + 0.1 * 6.6075758934021
Epoch 330, val loss: 0.7634682655334473
Epoch 340, training loss: 0.9081272482872009 = 0.24415995180606842 + 0.1 * 6.639672756195068
Epoch 340, val loss: 0.7693340182304382
Epoch 350, training loss: 0.8862770795822144 = 0.22572918236255646 + 0.1 * 6.605478763580322
Epoch 350, val loss: 0.7766276001930237
Epoch 360, training loss: 0.868476152420044 = 0.2086588740348816 + 0.1 * 6.598172664642334
Epoch 360, val loss: 0.7852823138237
Epoch 370, training loss: 0.8519253730773926 = 0.19289043545722961 + 0.1 * 6.590348720550537
Epoch 370, val loss: 0.7951779365539551
Epoch 380, training loss: 0.8368496894836426 = 0.17835594713687897 + 0.1 * 6.584937572479248
Epoch 380, val loss: 0.8062383532524109
Epoch 390, training loss: 0.8234109282493591 = 0.1649356484413147 + 0.1 * 6.584752559661865
Epoch 390, val loss: 0.8184168934822083
Epoch 400, training loss: 0.8101568818092346 = 0.1525992602109909 + 0.1 * 6.575575828552246
Epoch 400, val loss: 0.8314808011054993
Epoch 410, training loss: 0.7990306615829468 = 0.14126580953598022 + 0.1 * 6.577648639678955
Epoch 410, val loss: 0.8453598022460938
Epoch 420, training loss: 0.78858482837677 = 0.13088580965995789 + 0.1 * 6.576989650726318
Epoch 420, val loss: 0.8598120212554932
Epoch 430, training loss: 0.7772759795188904 = 0.1213851347565651 + 0.1 * 6.558907985687256
Epoch 430, val loss: 0.874809205532074
Epoch 440, training loss: 0.7697521448135376 = 0.11267013102769852 + 0.1 * 6.570820331573486
Epoch 440, val loss: 0.8902474641799927
Epoch 450, training loss: 0.76168292760849 = 0.10471902787685394 + 0.1 * 6.569638729095459
Epoch 450, val loss: 0.9059346318244934
Epoch 460, training loss: 0.752896249294281 = 0.09744668751955032 + 0.1 * 6.554495334625244
Epoch 460, val loss: 0.921791672706604
Epoch 470, training loss: 0.7459822297096252 = 0.09079714119434357 + 0.1 * 6.55185079574585
Epoch 470, val loss: 0.9376376867294312
Epoch 480, training loss: 0.7394546270370483 = 0.08470921218395233 + 0.1 * 6.547453880310059
Epoch 480, val loss: 0.9535503387451172
Epoch 490, training loss: 0.7364529371261597 = 0.07911700010299683 + 0.1 * 6.573359489440918
Epoch 490, val loss: 0.969419002532959
Epoch 500, training loss: 0.7278940081596375 = 0.0740056037902832 + 0.1 * 6.538883686065674
Epoch 500, val loss: 0.9850552082061768
Epoch 510, training loss: 0.7224282026290894 = 0.06931157410144806 + 0.1 * 6.531166076660156
Epoch 510, val loss: 1.0006508827209473
Epoch 520, training loss: 0.7198030352592468 = 0.06498973816633224 + 0.1 * 6.54813289642334
Epoch 520, val loss: 1.01615309715271
Epoch 530, training loss: 0.7143418192863464 = 0.061023466289043427 + 0.1 * 6.533183574676514
Epoch 530, val loss: 1.0312821865081787
Epoch 540, training loss: 0.7096827626228333 = 0.05737053602933884 + 0.1 * 6.523122310638428
Epoch 540, val loss: 1.0462474822998047
Epoch 550, training loss: 0.7060564756393433 = 0.05400457605719566 + 0.1 * 6.520519256591797
Epoch 550, val loss: 1.0609526634216309
Epoch 560, training loss: 0.7035118937492371 = 0.05089901387691498 + 0.1 * 6.52612829208374
Epoch 560, val loss: 1.0753493309020996
Epoch 570, training loss: 0.6996569037437439 = 0.04803207144141197 + 0.1 * 6.5162482261657715
Epoch 570, val loss: 1.0894355773925781
Epoch 580, training loss: 0.6965115070343018 = 0.045379675924777985 + 0.1 * 6.511317729949951
Epoch 580, val loss: 1.1032871007919312
Epoch 590, training loss: 0.6981728672981262 = 0.04291923716664314 + 0.1 * 6.552536487579346
Epoch 590, val loss: 1.1168749332427979
Epoch 600, training loss: 0.6917965412139893 = 0.04064629599452019 + 0.1 * 6.511502265930176
Epoch 600, val loss: 1.1301854848861694
Epoch 610, training loss: 0.688761830329895 = 0.03853472322225571 + 0.1 * 6.5022711753845215
Epoch 610, val loss: 1.1432338953018188
Epoch 620, training loss: 0.6867235898971558 = 0.0365685410797596 + 0.1 * 6.501550674438477
Epoch 620, val loss: 1.1560595035552979
Epoch 630, training loss: 0.685856819152832 = 0.034737590700387955 + 0.1 * 6.511192321777344
Epoch 630, val loss: 1.1686294078826904
Epoch 640, training loss: 0.6830161213874817 = 0.03303513303399086 + 0.1 * 6.499809265136719
Epoch 640, val loss: 1.1809636354446411
Epoch 650, training loss: 0.6815013885498047 = 0.03144655376672745 + 0.1 * 6.500547885894775
Epoch 650, val loss: 1.193047285079956
Epoch 660, training loss: 0.6799601316452026 = 0.029965592548251152 + 0.1 * 6.499945163726807
Epoch 660, val loss: 1.2048754692077637
Epoch 670, training loss: 0.6782787442207336 = 0.028579551726579666 + 0.1 * 6.496992111206055
Epoch 670, val loss: 1.2164931297302246
Epoch 680, training loss: 0.6772342920303345 = 0.027283480390906334 + 0.1 * 6.499507904052734
Epoch 680, val loss: 1.2279125452041626
Epoch 690, training loss: 0.6745166182518005 = 0.026070596650242805 + 0.1 * 6.48445987701416
Epoch 690, val loss: 1.2389945983886719
Epoch 700, training loss: 0.6731863617897034 = 0.02493356727063656 + 0.1 * 6.482527732849121
Epoch 700, val loss: 1.2499008178710938
Epoch 710, training loss: 0.6719374060630798 = 0.023865895345807076 + 0.1 * 6.480715274810791
Epoch 710, val loss: 1.2606014013290405
Epoch 720, training loss: 0.6716515421867371 = 0.022865526378154755 + 0.1 * 6.487859725952148
Epoch 720, val loss: 1.2710249423980713
Epoch 730, training loss: 0.6700194478034973 = 0.021925974637269974 + 0.1 * 6.4809346199035645
Epoch 730, val loss: 1.2812743186950684
Epoch 740, training loss: 0.6694720387458801 = 0.0210415069013834 + 0.1 * 6.484304904937744
Epoch 740, val loss: 1.2913944721221924
Epoch 750, training loss: 0.6674935817718506 = 0.020208265632390976 + 0.1 * 6.472853183746338
Epoch 750, val loss: 1.3012444972991943
Epoch 760, training loss: 0.6670838594436646 = 0.019423391669988632 + 0.1 * 6.47660493850708
Epoch 760, val loss: 1.3109829425811768
Epoch 770, training loss: 0.6652235388755798 = 0.018682587891817093 + 0.1 * 6.465409278869629
Epoch 770, val loss: 1.3205171823501587
Epoch 780, training loss: 0.6649194359779358 = 0.01798313297331333 + 0.1 * 6.469362735748291
Epoch 780, val loss: 1.3298736810684204
Epoch 790, training loss: 0.6650430560112 = 0.017321651801466942 + 0.1 * 6.4772138595581055
Epoch 790, val loss: 1.339096188545227
Epoch 800, training loss: 0.6636165976524353 = 0.01669817976653576 + 0.1 * 6.469183921813965
Epoch 800, val loss: 1.3481029272079468
Epoch 810, training loss: 0.661874532699585 = 0.016107074916362762 + 0.1 * 6.457674503326416
Epoch 810, val loss: 1.35699462890625
Epoch 820, training loss: 0.6621278524398804 = 0.015546907670795918 + 0.1 * 6.465809345245361
Epoch 820, val loss: 1.3656829595565796
Epoch 830, training loss: 0.6606298685073853 = 0.01501631923019886 + 0.1 * 6.456135272979736
Epoch 830, val loss: 1.3741812705993652
Epoch 840, training loss: 0.6613383889198303 = 0.014512293040752411 + 0.1 * 6.468260765075684
Epoch 840, val loss: 1.382568597793579
Epoch 850, training loss: 0.6595777273178101 = 0.014033738523721695 + 0.1 * 6.455439567565918
Epoch 850, val loss: 1.3907846212387085
Epoch 860, training loss: 0.6591023206710815 = 0.013579024001955986 + 0.1 * 6.455233097076416
Epoch 860, val loss: 1.398859977722168
Epoch 870, training loss: 0.6585220694541931 = 0.013146509416401386 + 0.1 * 6.4537553787231445
Epoch 870, val loss: 1.4068231582641602
Epoch 880, training loss: 0.6604483127593994 = 0.012735052034258842 + 0.1 * 6.477132320404053
Epoch 880, val loss: 1.4146746397018433
Epoch 890, training loss: 0.6577967405319214 = 0.012343637645244598 + 0.1 * 6.454530715942383
Epoch 890, val loss: 1.4222608804702759
Epoch 900, training loss: 0.6565778255462646 = 0.011971287429332733 + 0.1 * 6.446065425872803
Epoch 900, val loss: 1.4298173189163208
Epoch 910, training loss: 0.6572295427322388 = 0.011615200899541378 + 0.1 * 6.456143379211426
Epoch 910, val loss: 1.4372247457504272
Epoch 920, training loss: 0.6549797058105469 = 0.011275502853095531 + 0.1 * 6.437041759490967
Epoch 920, val loss: 1.4444496631622314
Epoch 930, training loss: 0.655146062374115 = 0.010951371863484383 + 0.1 * 6.441946506500244
Epoch 930, val loss: 1.451608419418335
Epoch 940, training loss: 0.6551418304443359 = 0.010641886852681637 + 0.1 * 6.4449992179870605
Epoch 940, val loss: 1.4586454629898071
Epoch 950, training loss: 0.6554647088050842 = 0.01034562848508358 + 0.1 * 6.45119047164917
Epoch 950, val loss: 1.465552568435669
Epoch 960, training loss: 0.6536123752593994 = 0.010062476620078087 + 0.1 * 6.43549919128418
Epoch 960, val loss: 1.472319483757019
Epoch 970, training loss: 0.6535151600837708 = 0.009791020303964615 + 0.1 * 6.437241554260254
Epoch 970, val loss: 1.479050874710083
Epoch 980, training loss: 0.6545001268386841 = 0.009531192481517792 + 0.1 * 6.449688911437988
Epoch 980, val loss: 1.4855806827545166
Epoch 990, training loss: 0.6528408527374268 = 0.009283030405640602 + 0.1 * 6.435578346252441
Epoch 990, val loss: 1.4920121431350708
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.826136827468872 = 1.9664572477340698 + 0.1 * 8.596795082092285
Epoch 0, val loss: 1.9685651063919067
Epoch 10, training loss: 2.8155641555786133 = 1.9558979272842407 + 0.1 * 8.596662521362305
Epoch 10, val loss: 1.9582682847976685
Epoch 20, training loss: 2.802518129348755 = 1.942934513092041 + 0.1 * 8.595836639404297
Epoch 20, val loss: 1.9451334476470947
Epoch 30, training loss: 2.7837207317352295 = 1.9248992204666138 + 0.1 * 8.588215827941895
Epoch 30, val loss: 1.926443338394165
Epoch 40, training loss: 2.751091957092285 = 1.8982406854629517 + 0.1 * 8.528512001037598
Epoch 40, val loss: 1.898655891418457
Epoch 50, training loss: 2.6793875694274902 = 1.8622719049453735 + 0.1 * 8.17115592956543
Epoch 50, val loss: 1.8624637126922607
Epoch 60, training loss: 2.5936203002929688 = 1.8241413831710815 + 0.1 * 7.694788932800293
Epoch 60, val loss: 1.8267449140548706
Epoch 70, training loss: 2.5124478340148926 = 1.7884621620178223 + 0.1 * 7.239855766296387
Epoch 70, val loss: 1.79463791847229
Epoch 80, training loss: 2.459463357925415 = 1.752557635307312 + 0.1 * 7.069057464599609
Epoch 80, val loss: 1.7633414268493652
Epoch 90, training loss: 2.4116158485412598 = 1.7113701105117798 + 0.1 * 7.002457141876221
Epoch 90, val loss: 1.7269717454910278
Epoch 100, training loss: 2.354144334793091 = 1.6572344303131104 + 0.1 * 6.969099521636963
Epoch 100, val loss: 1.678573489189148
Epoch 110, training loss: 2.2815003395080566 = 1.5871425867080688 + 0.1 * 6.943576812744141
Epoch 110, val loss: 1.6174226999282837
Epoch 120, training loss: 2.197205066680908 = 1.504823088645935 + 0.1 * 6.9238200187683105
Epoch 120, val loss: 1.5480875968933105
Epoch 130, training loss: 2.1089391708374023 = 1.4187579154968262 + 0.1 * 6.901812553405762
Epoch 130, val loss: 1.4784150123596191
Epoch 140, training loss: 2.0216615200042725 = 1.3339624404907227 + 0.1 * 6.87699031829834
Epoch 140, val loss: 1.4126235246658325
Epoch 150, training loss: 1.9374282360076904 = 1.2517660856246948 + 0.1 * 6.856621265411377
Epoch 150, val loss: 1.350448727607727
Epoch 160, training loss: 1.8547847270965576 = 1.1713533401489258 + 0.1 * 6.834314346313477
Epoch 160, val loss: 1.2899901866912842
Epoch 170, training loss: 1.7744524478912354 = 1.0928446054458618 + 0.1 * 6.8160786628723145
Epoch 170, val loss: 1.2306513786315918
Epoch 180, training loss: 1.698952317237854 = 1.0187875032424927 + 0.1 * 6.801648139953613
Epoch 180, val loss: 1.1744123697280884
Epoch 190, training loss: 1.6261440515518188 = 0.947379469871521 + 0.1 * 6.7876458168029785
Epoch 190, val loss: 1.120218276977539
Epoch 200, training loss: 1.5557317733764648 = 0.8782737851142883 + 0.1 * 6.774579048156738
Epoch 200, val loss: 1.0678203105926514
Epoch 210, training loss: 1.4877119064331055 = 0.8113923072814941 + 0.1 * 6.7631964683532715
Epoch 210, val loss: 1.017675757408142
Epoch 220, training loss: 1.4228116273880005 = 0.7474644184112549 + 0.1 * 6.753471851348877
Epoch 220, val loss: 0.9696637392044067
Epoch 230, training loss: 1.3618285655975342 = 0.6875454783439636 + 0.1 * 6.742831230163574
Epoch 230, val loss: 0.9253827929496765
Epoch 240, training loss: 1.3048278093338013 = 0.6317278742790222 + 0.1 * 6.73099946975708
Epoch 240, val loss: 0.8856194019317627
Epoch 250, training loss: 1.2527698278427124 = 0.579502284526825 + 0.1 * 6.732675075531006
Epoch 250, val loss: 0.8510081768035889
Epoch 260, training loss: 1.201690673828125 = 0.5308312177658081 + 0.1 * 6.708594799041748
Epoch 260, val loss: 0.8220042586326599
Epoch 270, training loss: 1.1554265022277832 = 0.4852081835269928 + 0.1 * 6.702182769775391
Epoch 270, val loss: 0.797755241394043
Epoch 280, training loss: 1.1121701002120972 = 0.4430336654186249 + 0.1 * 6.691364288330078
Epoch 280, val loss: 0.7780465483665466
Epoch 290, training loss: 1.0725555419921875 = 0.40428245067596436 + 0.1 * 6.6827311515808105
Epoch 290, val loss: 0.76253741979599
Epoch 300, training loss: 1.0367546081542969 = 0.36904487013816833 + 0.1 * 6.677097320556641
Epoch 300, val loss: 0.7511231303215027
Epoch 310, training loss: 1.0037047863006592 = 0.33703774213790894 + 0.1 * 6.666670322418213
Epoch 310, val loss: 0.7434245944023132
Epoch 320, training loss: 0.9737265706062317 = 0.307692289352417 + 0.1 * 6.660342693328857
Epoch 320, val loss: 0.7387515306472778
Epoch 330, training loss: 0.9463589191436768 = 0.2808555066585541 + 0.1 * 6.65503454208374
Epoch 330, val loss: 0.7364429831504822
Epoch 340, training loss: 0.920029878616333 = 0.25624415278434753 + 0.1 * 6.637857437133789
Epoch 340, val loss: 0.7360097169876099
Epoch 350, training loss: 0.8975220918655396 = 0.233552023768425 + 0.1 * 6.639700412750244
Epoch 350, val loss: 0.7371125817298889
Epoch 360, training loss: 0.8760520815849304 = 0.21274590492248535 + 0.1 * 6.633061408996582
Epoch 360, val loss: 0.7395431995391846
Epoch 370, training loss: 0.8583560585975647 = 0.19374047219753265 + 0.1 * 6.64615535736084
Epoch 370, val loss: 0.7430610060691833
Epoch 380, training loss: 0.8376084566116333 = 0.17648105323314667 + 0.1 * 6.611273765563965
Epoch 380, val loss: 0.7475217580795288
Epoch 390, training loss: 0.8209108710289001 = 0.16075016558170319 + 0.1 * 6.601606845855713
Epoch 390, val loss: 0.7529024481773376
Epoch 400, training loss: 0.806467592716217 = 0.14649666845798492 + 0.1 * 6.599709510803223
Epoch 400, val loss: 0.7591676712036133
Epoch 410, training loss: 0.7932678461074829 = 0.1336691975593567 + 0.1 * 6.595986366271973
Epoch 410, val loss: 0.7659966945648193
Epoch 420, training loss: 0.7806022763252258 = 0.12214615195989609 + 0.1 * 6.584561347961426
Epoch 420, val loss: 0.7734220623970032
Epoch 430, training loss: 0.7697612047195435 = 0.11180276423692703 + 0.1 * 6.57958459854126
Epoch 430, val loss: 0.7812373638153076
Epoch 440, training loss: 0.7628010511398315 = 0.10251518338918686 + 0.1 * 6.602858543395996
Epoch 440, val loss: 0.7895345091819763
Epoch 450, training loss: 0.7516762614250183 = 0.09421674907207489 + 0.1 * 6.574594974517822
Epoch 450, val loss: 0.7980750203132629
Epoch 460, training loss: 0.7428166270256042 = 0.0867614597082138 + 0.1 * 6.560551166534424
Epoch 460, val loss: 0.8068055510520935
Epoch 470, training loss: 0.7392829656600952 = 0.0800473541021347 + 0.1 * 6.592355728149414
Epoch 470, val loss: 0.8157708644866943
Epoch 480, training loss: 0.7299399971961975 = 0.07402624189853668 + 0.1 * 6.559137344360352
Epoch 480, val loss: 0.8248136043548584
Epoch 490, training loss: 0.7239845991134644 = 0.06861349940299988 + 0.1 * 6.553711414337158
Epoch 490, val loss: 0.8338318467140198
Epoch 500, training loss: 0.7186924815177917 = 0.06372267752885818 + 0.1 * 6.5496978759765625
Epoch 500, val loss: 0.8429248332977295
Epoch 510, training loss: 0.713519811630249 = 0.05929119139909744 + 0.1 * 6.542286396026611
Epoch 510, val loss: 0.8520755171775818
Epoch 520, training loss: 0.7083042860031128 = 0.05527276173233986 + 0.1 * 6.530314922332764
Epoch 520, val loss: 0.8611654043197632
Epoch 530, training loss: 0.7057677507400513 = 0.05161493644118309 + 0.1 * 6.54152774810791
Epoch 530, val loss: 0.8702296018600464
Epoch 540, training loss: 0.7015721797943115 = 0.048289958387613297 + 0.1 * 6.532822132110596
Epoch 540, val loss: 0.8792296648025513
Epoch 550, training loss: 0.6982417702674866 = 0.04525554180145264 + 0.1 * 6.529862403869629
Epoch 550, val loss: 0.8880826830863953
Epoch 560, training loss: 0.6945298314094543 = 0.04247735068202019 + 0.1 * 6.520524501800537
Epoch 560, val loss: 0.8968881964683533
Epoch 570, training loss: 0.6929227113723755 = 0.03992754966020584 + 0.1 * 6.529951572418213
Epoch 570, val loss: 0.9055942893028259
Epoch 580, training loss: 0.6899549961090088 = 0.03759316727519035 + 0.1 * 6.523618221282959
Epoch 580, val loss: 0.9140518307685852
Epoch 590, training loss: 0.6858621835708618 = 0.035446897149086 + 0.1 * 6.504152774810791
Epoch 590, val loss: 0.9224498867988586
Epoch 600, training loss: 0.6855964660644531 = 0.03346734866499901 + 0.1 * 6.5212907791137695
Epoch 600, val loss: 0.9307077527046204
Epoch 610, training loss: 0.6817672252655029 = 0.03164420649409294 + 0.1 * 6.501229763031006
Epoch 610, val loss: 0.9388366341590881
Epoch 620, training loss: 0.6798099279403687 = 0.02995864674448967 + 0.1 * 6.4985127449035645
Epoch 620, val loss: 0.9467717409133911
Epoch 630, training loss: 0.6793665289878845 = 0.02839544229209423 + 0.1 * 6.509710788726807
Epoch 630, val loss: 0.9546244144439697
Epoch 640, training loss: 0.6773413419723511 = 0.02694964036345482 + 0.1 * 6.5039167404174805
Epoch 640, val loss: 0.9623417854309082
Epoch 650, training loss: 0.6750347018241882 = 0.025608940050005913 + 0.1 * 6.49425745010376
Epoch 650, val loss: 0.969858705997467
Epoch 660, training loss: 0.6741557121276855 = 0.024360936135053635 + 0.1 * 6.4979472160339355
Epoch 660, val loss: 0.9772075414657593
Epoch 670, training loss: 0.6725507974624634 = 0.02319989539682865 + 0.1 * 6.493508815765381
Epoch 670, val loss: 0.9845390319824219
Epoch 680, training loss: 0.6710652709007263 = 0.022118844091892242 + 0.1 * 6.489463806152344
Epoch 680, val loss: 0.9916694760322571
Epoch 690, training loss: 0.6693482398986816 = 0.021112000569701195 + 0.1 * 6.482361793518066
Epoch 690, val loss: 0.9986310005187988
Epoch 700, training loss: 0.667589545249939 = 0.020168855786323547 + 0.1 * 6.474206924438477
Epoch 700, val loss: 1.0054906606674194
Epoch 710, training loss: 0.6683547496795654 = 0.01928749494254589 + 0.1 * 6.490672588348389
Epoch 710, val loss: 1.01226007938385
Epoch 720, training loss: 0.6661251187324524 = 0.018462758511304855 + 0.1 * 6.47662353515625
Epoch 720, val loss: 1.0188864469528198
Epoch 730, training loss: 0.6653611063957214 = 0.017689188942313194 + 0.1 * 6.476719379425049
Epoch 730, val loss: 1.0253735780715942
Epoch 740, training loss: 0.6640115976333618 = 0.01696203090250492 + 0.1 * 6.470495223999023
Epoch 740, val loss: 1.0317920446395874
Epoch 750, training loss: 0.663044810295105 = 0.01628117263317108 + 0.1 * 6.4676361083984375
Epoch 750, val loss: 1.0380723476409912
Epoch 760, training loss: 0.6623847484588623 = 0.015640316531062126 + 0.1 * 6.46744441986084
Epoch 760, val loss: 1.0442161560058594
Epoch 770, training loss: 0.661313533782959 = 0.015038383193314075 + 0.1 * 6.462751388549805
Epoch 770, val loss: 1.0502630472183228
Epoch 780, training loss: 0.6620859503746033 = 0.014469599351286888 + 0.1 * 6.476162910461426
Epoch 780, val loss: 1.056204080581665
Epoch 790, training loss: 0.6607738733291626 = 0.013932389207184315 + 0.1 * 6.468414306640625
Epoch 790, val loss: 1.061962366104126
Epoch 800, training loss: 0.6603802442550659 = 0.013427815400063992 + 0.1 * 6.469524383544922
Epoch 800, val loss: 1.067704677581787
Epoch 810, training loss: 0.658058762550354 = 0.012950103729963303 + 0.1 * 6.451086044311523
Epoch 810, val loss: 1.0732496976852417
Epoch 820, training loss: 0.65777987241745 = 0.012498259544372559 + 0.1 * 6.452816009521484
Epoch 820, val loss: 1.0787080526351929
Epoch 830, training loss: 0.6579220294952393 = 0.012069773860275745 + 0.1 * 6.458522796630859
Epoch 830, val loss: 1.0841258764266968
Epoch 840, training loss: 0.6568632125854492 = 0.01166378241032362 + 0.1 * 6.451993942260742
Epoch 840, val loss: 1.0894354581832886
Epoch 850, training loss: 0.65803062915802 = 0.011279570870101452 + 0.1 * 6.467510223388672
Epoch 850, val loss: 1.0946507453918457
Epoch 860, training loss: 0.655366837978363 = 0.010914178565144539 + 0.1 * 6.444526195526123
Epoch 860, val loss: 1.0997216701507568
Epoch 870, training loss: 0.6557323336601257 = 0.010566924698650837 + 0.1 * 6.451653480529785
Epoch 870, val loss: 1.1047532558441162
Epoch 880, training loss: 0.6543995141983032 = 0.010237153619527817 + 0.1 * 6.441623210906982
Epoch 880, val loss: 1.1096516847610474
Epoch 890, training loss: 0.6534602642059326 = 0.009923585690557957 + 0.1 * 6.435366630554199
Epoch 890, val loss: 1.1145330667495728
Epoch 900, training loss: 0.6535604000091553 = 0.009624465368688107 + 0.1 * 6.439359188079834
Epoch 900, val loss: 1.1192599534988403
Epoch 910, training loss: 0.6525920629501343 = 0.009338743053376675 + 0.1 * 6.432533264160156
Epoch 910, val loss: 1.1239280700683594
Epoch 920, training loss: 0.6538406610488892 = 0.00906664039939642 + 0.1 * 6.447740077972412
Epoch 920, val loss: 1.1285697221755981
Epoch 930, training loss: 0.6547440886497498 = 0.008807692676782608 + 0.1 * 6.4593634605407715
Epoch 930, val loss: 1.1330926418304443
Epoch 940, training loss: 0.6518716812133789 = 0.008560794405639172 + 0.1 * 6.433108806610107
Epoch 940, val loss: 1.137494683265686
Epoch 950, training loss: 0.652058482170105 = 0.00832460355013609 + 0.1 * 6.437338352203369
Epoch 950, val loss: 1.141815185546875
Epoch 960, training loss: 0.6510308384895325 = 0.008098199032247066 + 0.1 * 6.42932653427124
Epoch 960, val loss: 1.1460250616073608
Epoch 970, training loss: 0.6510858535766602 = 0.007881450466811657 + 0.1 * 6.432043552398682
Epoch 970, val loss: 1.150239109992981
Epoch 980, training loss: 0.6503840088844299 = 0.007673881482332945 + 0.1 * 6.4271016120910645
Epoch 980, val loss: 1.1544113159179688
Epoch 990, training loss: 0.6516250967979431 = 0.007474666461348534 + 0.1 * 6.441504001617432
Epoch 990, val loss: 1.1584527492523193
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.8154215812683105 = 1.9557377099990845 + 0.1 * 8.596837997436523
Epoch 0, val loss: 1.9587208032608032
Epoch 10, training loss: 2.804926872253418 = 1.9452520608901978 + 0.1 * 8.596748352050781
Epoch 10, val loss: 1.9486865997314453
Epoch 20, training loss: 2.792001485824585 = 1.9323737621307373 + 0.1 * 8.59627628326416
Epoch 20, val loss: 1.9359856843948364
Epoch 30, training loss: 2.7737574577331543 = 1.9145022630691528 + 0.1 * 8.592551231384277
Epoch 30, val loss: 1.9180386066436768
Epoch 40, training loss: 2.7450027465820312 = 1.8883239030838013 + 0.1 * 8.566788673400879
Epoch 40, val loss: 1.8917664289474487
Epoch 50, training loss: 2.6951169967651367 = 1.8519929647445679 + 0.1 * 8.43124008178711
Epoch 50, val loss: 1.8566819429397583
Epoch 60, training loss: 2.629422903060913 = 1.8105124235153198 + 0.1 * 8.189105033874512
Epoch 60, val loss: 1.819332242012024
Epoch 70, training loss: 2.559124708175659 = 1.7705495357513428 + 0.1 * 7.885751247406006
Epoch 70, val loss: 1.7844396829605103
Epoch 80, training loss: 2.47929048538208 = 1.7284950017929077 + 0.1 * 7.507955074310303
Epoch 80, val loss: 1.7465003728866577
Epoch 90, training loss: 2.404594898223877 = 1.6773054599761963 + 0.1 * 7.27289342880249
Epoch 90, val loss: 1.7002508640289307
Epoch 100, training loss: 2.3287410736083984 = 1.6118195056915283 + 0.1 * 7.169214725494385
Epoch 100, val loss: 1.6413477659225464
Epoch 110, training loss: 2.242393970489502 = 1.534326195716858 + 0.1 * 7.0806779861450195
Epoch 110, val loss: 1.572729468345642
Epoch 120, training loss: 2.1537246704101562 = 1.450972318649292 + 0.1 * 7.027522563934326
Epoch 120, val loss: 1.502210021018982
Epoch 130, training loss: 2.065927505493164 = 1.3667659759521484 + 0.1 * 6.991614818572998
Epoch 130, val loss: 1.4346445798873901
Epoch 140, training loss: 1.9784187078475952 = 1.2816393375396729 + 0.1 * 6.9677934646606445
Epoch 140, val loss: 1.367890477180481
Epoch 150, training loss: 1.8925001621246338 = 1.197487473487854 + 0.1 * 6.950127601623535
Epoch 150, val loss: 1.3026233911514282
Epoch 160, training loss: 1.8099771738052368 = 1.1165589094161987 + 0.1 * 6.934182643890381
Epoch 160, val loss: 1.2404463291168213
Epoch 170, training loss: 1.7333157062530518 = 1.0420893430709839 + 0.1 * 6.9122633934021
Epoch 170, val loss: 1.1846321821212769
Epoch 180, training loss: 1.6645381450653076 = 0.9750974178314209 + 0.1 * 6.894407749176025
Epoch 180, val loss: 1.136073350906372
Epoch 190, training loss: 1.6033133268356323 = 0.9154378771781921 + 0.1 * 6.878754138946533
Epoch 190, val loss: 1.0949770212173462
Epoch 200, training loss: 1.5488181114196777 = 0.8615886569023132 + 0.1 * 6.872293949127197
Epoch 200, val loss: 1.0605521202087402
Epoch 210, training loss: 1.4988882541656494 = 0.8131424188613892 + 0.1 * 6.85745906829834
Epoch 210, val loss: 1.0325406789779663
Epoch 220, training loss: 1.4536395072937012 = 0.7687059640884399 + 0.1 * 6.849335193634033
Epoch 220, val loss: 1.0096715688705444
Epoch 230, training loss: 1.411407470703125 = 0.7265061140060425 + 0.1 * 6.8490142822265625
Epoch 230, val loss: 0.9898800253868103
Epoch 240, training loss: 1.3689308166503906 = 0.6851009130477905 + 0.1 * 6.838298797607422
Epoch 240, val loss: 0.9712172746658325
Epoch 250, training loss: 1.3257687091827393 = 0.6427620053291321 + 0.1 * 6.830067157745361
Epoch 250, val loss: 0.9518112540245056
Epoch 260, training loss: 1.2806003093719482 = 0.5984020829200745 + 0.1 * 6.821981430053711
Epoch 260, val loss: 0.930453360080719
Epoch 270, training loss: 1.2332594394683838 = 0.55154949426651 + 0.1 * 6.817099094390869
Epoch 270, val loss: 0.9067240953445435
Epoch 280, training loss: 1.1839196681976318 = 0.5028857588768005 + 0.1 * 6.810338973999023
Epoch 280, val loss: 0.8814865946769714
Epoch 290, training loss: 1.135278344154358 = 0.4538799822330475 + 0.1 * 6.81398344039917
Epoch 290, val loss: 0.8565280437469482
Epoch 300, training loss: 1.0863313674926758 = 0.4066237807273865 + 0.1 * 6.797075271606445
Epoch 300, val loss: 0.8339427709579468
Epoch 310, training loss: 1.0411412715911865 = 0.36236143112182617 + 0.1 * 6.787797451019287
Epoch 310, val loss: 0.8151488900184631
Epoch 320, training loss: 0.9999680519104004 = 0.32202935218811035 + 0.1 * 6.7793869972229
Epoch 320, val loss: 0.8009056448936462
Epoch 330, training loss: 0.9631344079971313 = 0.2862892746925354 + 0.1 * 6.76845121383667
Epoch 330, val loss: 0.7910846471786499
Epoch 340, training loss: 0.9304133653640747 = 0.25486767292022705 + 0.1 * 6.755456924438477
Epoch 340, val loss: 0.7849860787391663
Epoch 350, training loss: 0.9015511870384216 = 0.22723160684108734 + 0.1 * 6.743195533752441
Epoch 350, val loss: 0.782149076461792
Epoch 360, training loss: 0.8766562342643738 = 0.20313070714473724 + 0.1 * 6.735254764556885
Epoch 360, val loss: 0.7818187475204468
Epoch 370, training loss: 0.8539433479309082 = 0.18212875723838806 + 0.1 * 6.718145370483398
Epoch 370, val loss: 0.7833930850028992
Epoch 380, training loss: 0.8377525806427002 = 0.16369092464447021 + 0.1 * 6.740616321563721
Epoch 380, val loss: 0.7866528034210205
Epoch 390, training loss: 0.8177419900894165 = 0.14755864441394806 + 0.1 * 6.701833724975586
Epoch 390, val loss: 0.7911413908004761
Epoch 400, training loss: 0.8036836385726929 = 0.13334818184375763 + 0.1 * 6.703354835510254
Epoch 400, val loss: 0.7965371012687683
Epoch 410, training loss: 0.7884534597396851 = 0.1208856850862503 + 0.1 * 6.67567777633667
Epoch 410, val loss: 0.8027275204658508
Epoch 420, training loss: 0.7764437794685364 = 0.10988409072160721 + 0.1 * 6.665596961975098
Epoch 420, val loss: 0.8096506595611572
Epoch 430, training loss: 0.7685360312461853 = 0.1001342386007309 + 0.1 * 6.684017658233643
Epoch 430, val loss: 0.8172324895858765
Epoch 440, training loss: 0.7573772072792053 = 0.09152571856975555 + 0.1 * 6.658514499664307
Epoch 440, val loss: 0.825170636177063
Epoch 450, training loss: 0.7482137084007263 = 0.08389807492494583 + 0.1 * 6.6431565284729
Epoch 450, val loss: 0.833517849445343
Epoch 460, training loss: 0.7413753271102905 = 0.07709776610136032 + 0.1 * 6.642775535583496
Epoch 460, val loss: 0.8422611951828003
Epoch 470, training loss: 0.7343342900276184 = 0.0710224062204361 + 0.1 * 6.633119106292725
Epoch 470, val loss: 0.8512926697731018
Epoch 480, training loss: 0.72856605052948 = 0.06557497382164001 + 0.1 * 6.629910945892334
Epoch 480, val loss: 0.8605216145515442
Epoch 490, training loss: 0.7232319712638855 = 0.06067688390612602 + 0.1 * 6.625550746917725
Epoch 490, val loss: 0.8698865175247192
Epoch 500, training loss: 0.7173821330070496 = 0.05626894533634186 + 0.1 * 6.61113166809082
Epoch 500, val loss: 0.8793407082557678
Epoch 510, training loss: 0.7137444019317627 = 0.052290644496679306 + 0.1 * 6.614537715911865
Epoch 510, val loss: 0.8888416290283203
Epoch 520, training loss: 0.7088548541069031 = 0.048689939081668854 + 0.1 * 6.601648807525635
Epoch 520, val loss: 0.8984143137931824
Epoch 530, training loss: 0.7077609300613403 = 0.04541537165641785 + 0.1 * 6.623455047607422
Epoch 530, val loss: 0.9078869819641113
Epoch 540, training loss: 0.7019122838973999 = 0.04245173558592796 + 0.1 * 6.594605445861816
Epoch 540, val loss: 0.9172596335411072
Epoch 550, training loss: 0.6992720365524292 = 0.03975303843617439 + 0.1 * 6.595190048217773
Epoch 550, val loss: 0.9266155958175659
Epoch 560, training loss: 0.6961535811424255 = 0.0372953899204731 + 0.1 * 6.588581562042236
Epoch 560, val loss: 0.9357708692550659
Epoch 570, training loss: 0.69297194480896 = 0.03504471108317375 + 0.1 * 6.5792717933654785
Epoch 570, val loss: 0.944920539855957
Epoch 580, training loss: 0.6899940967559814 = 0.032986752688884735 + 0.1 * 6.570073127746582
Epoch 580, val loss: 0.9539340734481812
Epoch 590, training loss: 0.6901129484176636 = 0.03109050542116165 + 0.1 * 6.590224266052246
Epoch 590, val loss: 0.9628516435623169
Epoch 600, training loss: 0.6863556504249573 = 0.02935093082487583 + 0.1 * 6.570046901702881
Epoch 600, val loss: 0.9714844822883606
Epoch 610, training loss: 0.6837476491928101 = 0.02775108441710472 + 0.1 * 6.55996561050415
Epoch 610, val loss: 0.9800609350204468
Epoch 620, training loss: 0.6825913190841675 = 0.026276711374521255 + 0.1 * 6.563146114349365
Epoch 620, val loss: 0.9884254932403564
Epoch 630, training loss: 0.6828432083129883 = 0.024915874004364014 + 0.1 * 6.579273223876953
Epoch 630, val loss: 0.9967106580734253
Epoch 640, training loss: 0.6787492036819458 = 0.023654881864786148 + 0.1 * 6.550943374633789
Epoch 640, val loss: 1.0048015117645264
Epoch 650, training loss: 0.6773083209991455 = 0.022488653659820557 + 0.1 * 6.548196315765381
Epoch 650, val loss: 1.0127822160720825
Epoch 660, training loss: 0.6775088906288147 = 0.021403446793556213 + 0.1 * 6.561054706573486
Epoch 660, val loss: 1.0205528736114502
Epoch 670, training loss: 0.6751896142959595 = 0.020396968349814415 + 0.1 * 6.547926425933838
Epoch 670, val loss: 1.0281790494918823
Epoch 680, training loss: 0.6733604669570923 = 0.019458496943116188 + 0.1 * 6.539019584655762
Epoch 680, val loss: 1.0356253385543823
Epoch 690, training loss: 0.6720592975616455 = 0.01858418621122837 + 0.1 * 6.534750938415527
Epoch 690, val loss: 1.043013334274292
Epoch 700, training loss: 0.6719645261764526 = 0.017767610028386116 + 0.1 * 6.541968822479248
Epoch 700, val loss: 1.0501635074615479
Epoch 710, training loss: 0.6701655387878418 = 0.017003869637846947 + 0.1 * 6.5316162109375
Epoch 710, val loss: 1.0572311878204346
Epoch 720, training loss: 0.6702442765235901 = 0.01629229448735714 + 0.1 * 6.539519786834717
Epoch 720, val loss: 1.0641732215881348
Epoch 730, training loss: 0.6676382422447205 = 0.015623991377651691 + 0.1 * 6.520142555236816
Epoch 730, val loss: 1.0708236694335938
Epoch 740, training loss: 0.6673829555511475 = 0.014999660663306713 + 0.1 * 6.52383279800415
Epoch 740, val loss: 1.0775258541107178
Epoch 750, training loss: 0.6662080883979797 = 0.01441276166588068 + 0.1 * 6.517952919006348
Epoch 750, val loss: 1.084072232246399
Epoch 760, training loss: 0.6654927134513855 = 0.013859671540558338 + 0.1 * 6.516330242156982
Epoch 760, val loss: 1.0904314517974854
Epoch 770, training loss: 0.6658832430839539 = 0.013338958844542503 + 0.1 * 6.525442600250244
Epoch 770, val loss: 1.0967305898666382
Epoch 780, training loss: 0.6638652086257935 = 0.012850462459027767 + 0.1 * 6.510147571563721
Epoch 780, val loss: 1.102869987487793
Epoch 790, training loss: 0.6635120511054993 = 0.012387804687023163 + 0.1 * 6.511242389678955
Epoch 790, val loss: 1.108972191810608
Epoch 800, training loss: 0.6625632047653198 = 0.011950291693210602 + 0.1 * 6.506128787994385
Epoch 800, val loss: 1.114881157875061
Epoch 810, training loss: 0.6632766723632812 = 0.01153760589659214 + 0.1 * 6.517390251159668
Epoch 810, val loss: 1.1206679344177246
Epoch 820, training loss: 0.6611269116401672 = 0.011147256009280682 + 0.1 * 6.499796390533447
Epoch 820, val loss: 1.1263753175735474
Epoch 830, training loss: 0.6604938507080078 = 0.010778321884572506 + 0.1 * 6.49715518951416
Epoch 830, val loss: 1.1320222616195679
Epoch 840, training loss: 0.6624684929847717 = 0.010428790003061295 + 0.1 * 6.520396709442139
Epoch 840, val loss: 1.1374939680099487
Epoch 850, training loss: 0.6597252488136292 = 0.01009468361735344 + 0.1 * 6.496305465698242
Epoch 850, val loss: 1.142859935760498
Epoch 860, training loss: 0.6600151658058167 = 0.009779845364391804 + 0.1 * 6.502352714538574
Epoch 860, val loss: 1.1482043266296387
Epoch 870, training loss: 0.6593413352966309 = 0.00948003027588129 + 0.1 * 6.498612880706787
Epoch 870, val loss: 1.1533209085464478
Epoch 880, training loss: 0.6576862931251526 = 0.009194436483085155 + 0.1 * 6.484918594360352
Epoch 880, val loss: 1.158425211906433
Epoch 890, training loss: 0.6580410599708557 = 0.008923341520130634 + 0.1 * 6.491177082061768
Epoch 890, val loss: 1.1635252237319946
Epoch 900, training loss: 0.6577441096305847 = 0.00866437517106533 + 0.1 * 6.490797519683838
Epoch 900, val loss: 1.1684303283691406
Epoch 910, training loss: 0.6573919057846069 = 0.008417333476245403 + 0.1 * 6.48974609375
Epoch 910, val loss: 1.1732726097106934
Epoch 920, training loss: 0.6572617292404175 = 0.008181735873222351 + 0.1 * 6.490799903869629
Epoch 920, val loss: 1.1780718564987183
Epoch 930, training loss: 0.6558358073234558 = 0.007956841960549355 + 0.1 * 6.478789329528809
Epoch 930, val loss: 1.1827884912490845
Epoch 940, training loss: 0.657038152217865 = 0.0077418372966349125 + 0.1 * 6.4929633140563965
Epoch 940, val loss: 1.1873384714126587
Epoch 950, training loss: 0.6550033092498779 = 0.007536808028817177 + 0.1 * 6.474664688110352
Epoch 950, val loss: 1.1918491125106812
Epoch 960, training loss: 0.654772937297821 = 0.007340550422668457 + 0.1 * 6.474323749542236
Epoch 960, val loss: 1.1963704824447632
Epoch 970, training loss: 0.6547542214393616 = 0.007152410224080086 + 0.1 * 6.476017951965332
Epoch 970, val loss: 1.2006713151931763
Epoch 980, training loss: 0.6560381054878235 = 0.006971289403736591 + 0.1 * 6.490667819976807
Epoch 980, val loss: 1.2049411535263062
Epoch 990, training loss: 0.65435791015625 = 0.006799470633268356 + 0.1 * 6.475584506988525
Epoch 990, val loss: 1.2091401815414429
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8360569319978914
The final CL Acc:0.79136, 0.00462, The final GNN Acc:0.83834, 0.00163
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9524])
updated graph: torch.Size([2, 10624])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.810692310333252 = 1.9510093927383423 + 0.1 * 8.59682846069336
Epoch 0, val loss: 1.9472373723983765
Epoch 10, training loss: 2.8008627891540527 = 1.9411873817443848 + 0.1 * 8.59675407409668
Epoch 10, val loss: 1.9381641149520874
Epoch 20, training loss: 2.78885555267334 = 1.9292227029800415 + 0.1 * 8.596328735351562
Epoch 20, val loss: 1.926824927330017
Epoch 30, training loss: 2.772019863128662 = 1.9127219915390015 + 0.1 * 8.592979431152344
Epoch 30, val loss: 1.9109998941421509
Epoch 40, training loss: 2.7452871799468994 = 1.888312578201294 + 0.1 * 8.569746017456055
Epoch 40, val loss: 1.8875467777252197
Epoch 50, training loss: 2.6996798515319824 = 1.8542946577072144 + 0.1 * 8.453851699829102
Epoch 50, val loss: 1.8560458421707153
Epoch 60, training loss: 2.641293525695801 = 1.8162343502044678 + 0.1 * 8.250590324401855
Epoch 60, val loss: 1.822928786277771
Epoch 70, training loss: 2.583906888961792 = 1.7830816507339478 + 0.1 * 8.008252143859863
Epoch 70, val loss: 1.7937400341033936
Epoch 80, training loss: 2.5133867263793945 = 1.750577449798584 + 0.1 * 7.628093242645264
Epoch 80, val loss: 1.7627558708190918
Epoch 90, training loss: 2.439767599105835 = 1.7128407955169678 + 0.1 * 7.2692670822143555
Epoch 90, val loss: 1.7295669317245483
Epoch 100, training loss: 2.374332904815674 = 1.6623146533966064 + 0.1 * 7.12018346786499
Epoch 100, val loss: 1.6878410577774048
Epoch 110, training loss: 2.298060655593872 = 1.5943790674209595 + 0.1 * 7.036815166473389
Epoch 110, val loss: 1.631515383720398
Epoch 120, training loss: 2.215064525604248 = 1.5158636569976807 + 0.1 * 6.992008686065674
Epoch 120, val loss: 1.5672825574874878
Epoch 130, training loss: 2.130582332611084 = 1.4338284730911255 + 0.1 * 6.967538356781006
Epoch 130, val loss: 1.5003811120986938
Epoch 140, training loss: 2.0471363067626953 = 1.3525190353393555 + 0.1 * 6.946171283721924
Epoch 140, val loss: 1.4354859590530396
Epoch 150, training loss: 1.9670281410217285 = 1.2739990949630737 + 0.1 * 6.930290222167969
Epoch 150, val loss: 1.3741486072540283
Epoch 160, training loss: 1.889336347579956 = 1.1981910467147827 + 0.1 * 6.911453723907471
Epoch 160, val loss: 1.3161166906356812
Epoch 170, training loss: 1.8135768175125122 = 1.1238678693771362 + 0.1 * 6.89708948135376
Epoch 170, val loss: 1.2602509260177612
Epoch 180, training loss: 1.7390217781066895 = 1.0509357452392578 + 0.1 * 6.880859375
Epoch 180, val loss: 1.2060433626174927
Epoch 190, training loss: 1.6650512218475342 = 0.9782445430755615 + 0.1 * 6.868067264556885
Epoch 190, val loss: 1.1521793603897095
Epoch 200, training loss: 1.5926470756530762 = 0.9057610034942627 + 0.1 * 6.868860721588135
Epoch 200, val loss: 1.0991442203521729
Epoch 210, training loss: 1.5206278562545776 = 0.8351206183433533 + 0.1 * 6.855072021484375
Epoch 210, val loss: 1.0484386682510376
Epoch 220, training loss: 1.4503698348999023 = 0.7659928798675537 + 0.1 * 6.8437700271606445
Epoch 220, val loss: 1.0001133680343628
Epoch 230, training loss: 1.382361888885498 = 0.6989530324935913 + 0.1 * 6.834088325500488
Epoch 230, val loss: 0.9550610780715942
Epoch 240, training loss: 1.3187260627746582 = 0.6356304883956909 + 0.1 * 6.830955505371094
Epoch 240, val loss: 0.9154572486877441
Epoch 250, training loss: 1.2592440843582153 = 0.5773487091064453 + 0.1 * 6.818953514099121
Epoch 250, val loss: 0.8824396133422852
Epoch 260, training loss: 1.2042732238769531 = 0.5236093997955322 + 0.1 * 6.806637763977051
Epoch 260, val loss: 0.8557428121566772
Epoch 270, training loss: 1.1555463075637817 = 0.47504255175590515 + 0.1 * 6.805037021636963
Epoch 270, val loss: 0.8356022238731384
Epoch 280, training loss: 1.1106376647949219 = 0.4317392408847809 + 0.1 * 6.7889838218688965
Epoch 280, val loss: 0.8212805390357971
Epoch 290, training loss: 1.0701971054077148 = 0.3928390145301819 + 0.1 * 6.773581504821777
Epoch 290, val loss: 0.8120391368865967
Epoch 300, training loss: 1.0342730283737183 = 0.35789260268211365 + 0.1 * 6.7638044357299805
Epoch 300, val loss: 0.8070599436759949
Epoch 310, training loss: 1.0031265020370483 = 0.32661619782447815 + 0.1 * 6.765103340148926
Epoch 310, val loss: 0.8055724501609802
Epoch 320, training loss: 0.9735190868377686 = 0.2986343801021576 + 0.1 * 6.748846530914307
Epoch 320, val loss: 0.8069429993629456
Epoch 330, training loss: 0.9490550756454468 = 0.27326837182044983 + 0.1 * 6.757867336273193
Epoch 330, val loss: 0.8105594515800476
Epoch 340, training loss: 0.9228436946868896 = 0.2500632107257843 + 0.1 * 6.727804183959961
Epoch 340, val loss: 0.8158609867095947
Epoch 350, training loss: 0.9012335538864136 = 0.22840604186058044 + 0.1 * 6.728275299072266
Epoch 350, val loss: 0.8223064541816711
Epoch 360, training loss: 0.880083441734314 = 0.20803776383399963 + 0.1 * 6.720456123352051
Epoch 360, val loss: 0.8295809626579285
Epoch 370, training loss: 0.8595314621925354 = 0.1888059377670288 + 0.1 * 6.707254886627197
Epoch 370, val loss: 0.8373799324035645
Epoch 380, training loss: 0.8406349420547485 = 0.17070883512496948 + 0.1 * 6.69926118850708
Epoch 380, val loss: 0.8456579446792603
Epoch 390, training loss: 0.8265379667282104 = 0.1538938283920288 + 0.1 * 6.726441383361816
Epoch 390, val loss: 0.8541095852851868
Epoch 400, training loss: 0.807174563407898 = 0.13862349092960358 + 0.1 * 6.685510635375977
Epoch 400, val loss: 0.8629521131515503
Epoch 410, training loss: 0.7934033274650574 = 0.12486368417739868 + 0.1 * 6.685396194458008
Epoch 410, val loss: 0.8719576597213745
Epoch 420, training loss: 0.7801854610443115 = 0.11260410398244858 + 0.1 * 6.675813674926758
Epoch 420, val loss: 0.8812295794487
Epoch 430, training loss: 0.7687348127365112 = 0.10174819827079773 + 0.1 * 6.669865608215332
Epoch 430, val loss: 0.8906980752944946
Epoch 440, training loss: 0.7591590285301208 = 0.09217315167188644 + 0.1 * 6.669858455657959
Epoch 440, val loss: 0.9003450870513916
Epoch 450, training loss: 0.7491031289100647 = 0.08374737948179245 + 0.1 * 6.653557300567627
Epoch 450, val loss: 0.910210371017456
Epoch 460, training loss: 0.7412020564079285 = 0.07631509006023407 + 0.1 * 6.648869514465332
Epoch 460, val loss: 0.9201522469520569
Epoch 470, training loss: 0.7342639565467834 = 0.06978414952754974 + 0.1 * 6.6447978019714355
Epoch 470, val loss: 0.9298728108406067
Epoch 480, training loss: 0.7276407480239868 = 0.06400884687900543 + 0.1 * 6.636319160461426
Epoch 480, val loss: 0.9396981000900269
Epoch 490, training loss: 0.721885085105896 = 0.05889908969402313 + 0.1 * 6.629859447479248
Epoch 490, val loss: 0.9494002461433411
Epoch 500, training loss: 0.7172228097915649 = 0.054354265332221985 + 0.1 * 6.628685474395752
Epoch 500, val loss: 0.9590016603469849
Epoch 510, training loss: 0.7127245664596558 = 0.05029240623116493 + 0.1 * 6.624320983886719
Epoch 510, val loss: 0.9684870839118958
Epoch 520, training loss: 0.7093930244445801 = 0.04665122553706169 + 0.1 * 6.62741756439209
Epoch 520, val loss: 0.977778434753418
Epoch 530, training loss: 0.7050938010215759 = 0.043378133326768875 + 0.1 * 6.617156982421875
Epoch 530, val loss: 0.9869990348815918
Epoch 540, training loss: 0.702491044998169 = 0.040427066385746 + 0.1 * 6.620639324188232
Epoch 540, val loss: 0.9959019422531128
Epoch 550, training loss: 0.6977412104606628 = 0.03775684908032417 + 0.1 * 6.599843502044678
Epoch 550, val loss: 1.0046601295471191
Epoch 560, training loss: 0.6954587697982788 = 0.03534245863556862 + 0.1 * 6.601162910461426
Epoch 560, val loss: 1.013060212135315
Epoch 570, training loss: 0.6922896504402161 = 0.0331493578851223 + 0.1 * 6.591403007507324
Epoch 570, val loss: 1.0214906930923462
Epoch 580, training loss: 0.6927970051765442 = 0.03115476667881012 + 0.1 * 6.616422653198242
Epoch 580, val loss: 1.0293800830841064
Epoch 590, training loss: 0.6876654624938965 = 0.0293379295617342 + 0.1 * 6.583275318145752
Epoch 590, val loss: 1.0374586582183838
Epoch 600, training loss: 0.6852831244468689 = 0.027676986530423164 + 0.1 * 6.576061248779297
Epoch 600, val loss: 1.045056939125061
Epoch 610, training loss: 0.6840717196464539 = 0.026151135563850403 + 0.1 * 6.5792059898376465
Epoch 610, val loss: 1.0526622533798218
Epoch 620, training loss: 0.6826891899108887 = 0.024750305339694023 + 0.1 * 6.579388618469238
Epoch 620, val loss: 1.0598715543746948
Epoch 630, training loss: 0.6840518712997437 = 0.023461367934942245 + 0.1 * 6.605905055999756
Epoch 630, val loss: 1.0669336318969727
Epoch 640, training loss: 0.6785637140274048 = 0.022274330258369446 + 0.1 * 6.562893867492676
Epoch 640, val loss: 1.0739426612854004
Epoch 650, training loss: 0.6765267252922058 = 0.02118229679763317 + 0.1 * 6.553443908691406
Epoch 650, val loss: 1.0807013511657715
Epoch 660, training loss: 0.6761740446090698 = 0.02016756497323513 + 0.1 * 6.560064315795898
Epoch 660, val loss: 1.0873298645019531
Epoch 670, training loss: 0.6745355129241943 = 0.019226454198360443 + 0.1 * 6.553090572357178
Epoch 670, val loss: 1.0936672687530518
Epoch 680, training loss: 0.6731650233268738 = 0.018350569531321526 + 0.1 * 6.548144817352295
Epoch 680, val loss: 1.1000474691390991
Epoch 690, training loss: 0.6716970205307007 = 0.017537551000714302 + 0.1 * 6.541594505310059
Epoch 690, val loss: 1.1061338186264038
Epoch 700, training loss: 0.6710911989212036 = 0.016781510785222054 + 0.1 * 6.543097019195557
Epoch 700, val loss: 1.1119937896728516
Epoch 710, training loss: 0.6701419353485107 = 0.016071641817688942 + 0.1 * 6.540703296661377
Epoch 710, val loss: 1.1181280612945557
Epoch 720, training loss: 0.6699491739273071 = 0.015410790219902992 + 0.1 * 6.545383930206299
Epoch 720, val loss: 1.1236473321914673
Epoch 730, training loss: 0.6677197217941284 = 0.01479070633649826 + 0.1 * 6.529290199279785
Epoch 730, val loss: 1.1292831897735596
Epoch 740, training loss: 0.6685829162597656 = 0.014211559668183327 + 0.1 * 6.543713569641113
Epoch 740, val loss: 1.1346104145050049
Epoch 750, training loss: 0.667383074760437 = 0.013668167404830456 + 0.1 * 6.537148952484131
Epoch 750, val loss: 1.1399012804031372
Epoch 760, training loss: 0.6660747528076172 = 0.013158200308680534 + 0.1 * 6.529165267944336
Epoch 760, val loss: 1.1453077793121338
Epoch 770, training loss: 0.6643760204315186 = 0.01267600804567337 + 0.1 * 6.517000198364258
Epoch 770, val loss: 1.1502825021743774
Epoch 780, training loss: 0.6640045046806335 = 0.01222283486276865 + 0.1 * 6.517816543579102
Epoch 780, val loss: 1.155469298362732
Epoch 790, training loss: 0.6637732982635498 = 0.011795668862760067 + 0.1 * 6.519776344299316
Epoch 790, val loss: 1.16016685962677
Epoch 800, training loss: 0.6630938053131104 = 0.011391116306185722 + 0.1 * 6.517026901245117
Epoch 800, val loss: 1.1651182174682617
Epoch 810, training loss: 0.6629519462585449 = 0.011010079644620419 + 0.1 * 6.519418716430664
Epoch 810, val loss: 1.1696604490280151
Epoch 820, training loss: 0.6623521447181702 = 0.010649212636053562 + 0.1 * 6.517029285430908
Epoch 820, val loss: 1.1742099523544312
Epoch 830, training loss: 0.6611815690994263 = 0.010307837277650833 + 0.1 * 6.508737087249756
Epoch 830, val loss: 1.1787105798721313
Epoch 840, training loss: 0.6603445410728455 = 0.009984076954424381 + 0.1 * 6.503604412078857
Epoch 840, val loss: 1.1830759048461914
Epoch 850, training loss: 0.659707248210907 = 0.009677433408796787 + 0.1 * 6.500298500061035
Epoch 850, val loss: 1.1872024536132812
Epoch 860, training loss: 0.6607567071914673 = 0.009384345263242722 + 0.1 * 6.513723850250244
Epoch 860, val loss: 1.1915210485458374
Epoch 870, training loss: 0.6598402261734009 = 0.009106245823204517 + 0.1 * 6.507339954376221
Epoch 870, val loss: 1.1954494714736938
Epoch 880, training loss: 0.6593529582023621 = 0.008842295967042446 + 0.1 * 6.505106449127197
Epoch 880, val loss: 1.199379801750183
Epoch 890, training loss: 0.6578702330589294 = 0.0085897296667099 + 0.1 * 6.492804527282715
Epoch 890, val loss: 1.2034106254577637
Epoch 900, training loss: 0.657921314239502 = 0.00835058931261301 + 0.1 * 6.495707035064697
Epoch 900, val loss: 1.20717191696167
Epoch 910, training loss: 0.657464325428009 = 0.008120950311422348 + 0.1 * 6.493433475494385
Epoch 910, val loss: 1.2109687328338623
Epoch 920, training loss: 0.657145082950592 = 0.007902499288320541 + 0.1 * 6.492425441741943
Epoch 920, val loss: 1.2144955396652222
Epoch 930, training loss: 0.6557103991508484 = 0.0076923491433262825 + 0.1 * 6.480180740356445
Epoch 930, val loss: 1.218241810798645
Epoch 940, training loss: 0.6576130986213684 = 0.007492917589843273 + 0.1 * 6.501201629638672
Epoch 940, val loss: 1.2218072414398193
Epoch 950, training loss: 0.6557023525238037 = 0.007301914039999247 + 0.1 * 6.484004020690918
Epoch 950, val loss: 1.2251677513122559
Epoch 960, training loss: 0.6555679440498352 = 0.00711878901347518 + 0.1 * 6.48449182510376
Epoch 960, val loss: 1.228695273399353
Epoch 970, training loss: 0.6542357802391052 = 0.006943511310964823 + 0.1 * 6.472922325134277
Epoch 970, val loss: 1.2319608926773071
Epoch 980, training loss: 0.6552239656448364 = 0.006775223184376955 + 0.1 * 6.484487533569336
Epoch 980, val loss: 1.2353620529174805
Epoch 990, training loss: 0.6549700498580933 = 0.006614042911678553 + 0.1 * 6.483560085296631
Epoch 990, val loss: 1.2385817766189575
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 2.812875270843506 = 1.953193187713623 + 0.1 * 8.596819877624512
Epoch 0, val loss: 1.9532581567764282
Epoch 10, training loss: 2.8027772903442383 = 1.9431058168411255 + 0.1 * 8.596715927124023
Epoch 10, val loss: 1.9427013397216797
Epoch 20, training loss: 2.7902374267578125 = 1.9306164979934692 + 0.1 * 8.596209526062012
Epoch 20, val loss: 1.9296300411224365
Epoch 30, training loss: 2.7723464965820312 = 1.913108229637146 + 0.1 * 8.592382431030273
Epoch 30, val loss: 1.9110629558563232
Epoch 40, training loss: 2.743711233139038 = 1.8871971368789673 + 0.1 * 8.565141677856445
Epoch 40, val loss: 1.8834930658340454
Epoch 50, training loss: 2.6929452419281006 = 1.8516185283660889 + 0.1 * 8.413267135620117
Epoch 50, val loss: 1.8470994234085083
Epoch 60, training loss: 2.625115156173706 = 1.8136450052261353 + 0.1 * 8.114702224731445
Epoch 60, val loss: 1.8118358850479126
Epoch 70, training loss: 2.5412821769714355 = 1.7818845510482788 + 0.1 * 7.593975067138672
Epoch 70, val loss: 1.7854118347167969
Epoch 80, training loss: 2.47609543800354 = 1.7505700588226318 + 0.1 * 7.255253314971924
Epoch 80, val loss: 1.7586745023727417
Epoch 90, training loss: 2.426293134689331 = 1.711604356765747 + 0.1 * 7.146886825561523
Epoch 90, val loss: 1.7252174615859985
Epoch 100, training loss: 2.369126081466675 = 1.6598325967788696 + 0.1 * 7.0929341316223145
Epoch 100, val loss: 1.6814167499542236
Epoch 110, training loss: 2.2958788871765137 = 1.5913670063018799 + 0.1 * 7.045117378234863
Epoch 110, val loss: 1.6244301795959473
Epoch 120, training loss: 2.2067041397094727 = 1.5064771175384521 + 0.1 * 7.002269744873047
Epoch 120, val loss: 1.5545294284820557
Epoch 130, training loss: 2.1056439876556396 = 1.4092477560043335 + 0.1 * 6.963962078094482
Epoch 130, val loss: 1.4750919342041016
Epoch 140, training loss: 1.999643087387085 = 1.3062143325805664 + 0.1 * 6.934288024902344
Epoch 140, val loss: 1.3920397758483887
Epoch 150, training loss: 1.8933110237121582 = 1.2024751901626587 + 0.1 * 6.908358573913574
Epoch 150, val loss: 1.3103383779525757
Epoch 160, training loss: 1.7912757396697998 = 1.1017603874206543 + 0.1 * 6.895153522491455
Epoch 160, val loss: 1.2327386140823364
Epoch 170, training loss: 1.6940906047821045 = 1.0068676471710205 + 0.1 * 6.872228622436523
Epoch 170, val loss: 1.1611828804016113
Epoch 180, training loss: 1.6024733781814575 = 0.9171713590621948 + 0.1 * 6.853020191192627
Epoch 180, val loss: 1.094298005104065
Epoch 190, training loss: 1.5179851055145264 = 0.8346124887466431 + 0.1 * 6.833725929260254
Epoch 190, val loss: 1.0335453748703003
Epoch 200, training loss: 1.4424238204956055 = 0.761055052280426 + 0.1 * 6.813686847686768
Epoch 200, val loss: 0.980413019657135
Epoch 210, training loss: 1.3764889240264893 = 0.6965302228927612 + 0.1 * 6.799587249755859
Epoch 210, val loss: 0.93558269739151
Epoch 220, training loss: 1.3206833600997925 = 0.6406657099723816 + 0.1 * 6.800176620483398
Epoch 220, val loss: 0.8988364934921265
Epoch 230, training loss: 1.2698135375976562 = 0.5928223729133606 + 0.1 * 6.769911766052246
Epoch 230, val loss: 0.8704452514648438
Epoch 240, training loss: 1.2268891334533691 = 0.5506677627563477 + 0.1 * 6.762213230133057
Epoch 240, val loss: 0.8481045365333557
Epoch 250, training loss: 1.1873984336853027 = 0.5134352445602417 + 0.1 * 6.7396321296691895
Epoch 250, val loss: 0.8313755989074707
Epoch 260, training loss: 1.1525585651397705 = 0.479856938123703 + 0.1 * 6.727015972137451
Epoch 260, val loss: 0.818886935710907
Epoch 270, training loss: 1.1226086616516113 = 0.44914278388023376 + 0.1 * 6.734659194946289
Epoch 270, val loss: 0.8098999857902527
Epoch 280, training loss: 1.0910810232162476 = 0.42067185044288635 + 0.1 * 6.704091548919678
Epoch 280, val loss: 0.8040083646774292
Epoch 290, training loss: 1.0637871026992798 = 0.3935396373271942 + 0.1 * 6.702475070953369
Epoch 290, val loss: 0.8002790808677673
Epoch 300, training loss: 1.0358654260635376 = 0.36741721630096436 + 0.1 * 6.684482097625732
Epoch 300, val loss: 0.7983286380767822
Epoch 310, training loss: 1.010459065437317 = 0.3422677516937256 + 0.1 * 6.681912899017334
Epoch 310, val loss: 0.7975865602493286
Epoch 320, training loss: 0.9863697290420532 = 0.318502277135849 + 0.1 * 6.678674697875977
Epoch 320, val loss: 0.7980446815490723
Epoch 330, training loss: 0.9630463123321533 = 0.2960505187511444 + 0.1 * 6.669957637786865
Epoch 330, val loss: 0.7988471984863281
Epoch 340, training loss: 0.9406431913375854 = 0.27501147985458374 + 0.1 * 6.656316757202148
Epoch 340, val loss: 0.8004083633422852
Epoch 350, training loss: 0.9209092259407043 = 0.25530314445495605 + 0.1 * 6.656060695648193
Epoch 350, val loss: 0.8024205565452576
Epoch 360, training loss: 0.9013867378234863 = 0.236880823969841 + 0.1 * 6.645059108734131
Epoch 360, val loss: 0.8047167658805847
Epoch 370, training loss: 0.883995771408081 = 0.2196030169725418 + 0.1 * 6.643927574157715
Epoch 370, val loss: 0.8074350953102112
Epoch 380, training loss: 0.8659927845001221 = 0.20324325561523438 + 0.1 * 6.627495288848877
Epoch 380, val loss: 0.8105933666229248
Epoch 390, training loss: 0.8503400087356567 = 0.18762852251529694 + 0.1 * 6.627114772796631
Epoch 390, val loss: 0.8140488862991333
Epoch 400, training loss: 0.8355879783630371 = 0.17281392216682434 + 0.1 * 6.627740383148193
Epoch 400, val loss: 0.817933201789856
Epoch 410, training loss: 0.8196725845336914 = 0.15882229804992676 + 0.1 * 6.6085028648376465
Epoch 410, val loss: 0.8225274085998535
Epoch 420, training loss: 0.8062728047370911 = 0.1457158923149109 + 0.1 * 6.605568885803223
Epoch 420, val loss: 0.8278958797454834
Epoch 430, training loss: 0.7932475209236145 = 0.13352735340595245 + 0.1 * 6.597201824188232
Epoch 430, val loss: 0.8342457413673401
Epoch 440, training loss: 0.7833234071731567 = 0.1222476065158844 + 0.1 * 6.610758304595947
Epoch 440, val loss: 0.841427206993103
Epoch 450, training loss: 0.7712161540985107 = 0.11189495027065277 + 0.1 * 6.593211650848389
Epoch 450, val loss: 0.8492571115493774
Epoch 460, training loss: 0.7613610029220581 = 0.1024191677570343 + 0.1 * 6.589417934417725
Epoch 460, val loss: 0.8576779365539551
Epoch 470, training loss: 0.751779317855835 = 0.09379418194293976 + 0.1 * 6.5798516273498535
Epoch 470, val loss: 0.8665333986282349
Epoch 480, training loss: 0.7434197664260864 = 0.08596306294202805 + 0.1 * 6.574566841125488
Epoch 480, val loss: 0.875907838344574
Epoch 490, training loss: 0.736345112323761 = 0.07886452227830887 + 0.1 * 6.574805736541748
Epoch 490, val loss: 0.8856566548347473
Epoch 500, training loss: 0.728879988193512 = 0.07245435565710068 + 0.1 * 6.564256191253662
Epoch 500, val loss: 0.8956146836280823
Epoch 510, training loss: 0.7238667011260986 = 0.06667189300060272 + 0.1 * 6.571948051452637
Epoch 510, val loss: 0.9058656692504883
Epoch 520, training loss: 0.7189078330993652 = 0.061476774513721466 + 0.1 * 6.574310779571533
Epoch 520, val loss: 0.9161909222602844
Epoch 530, training loss: 0.7128153443336487 = 0.056804753839969635 + 0.1 * 6.560105323791504
Epoch 530, val loss: 0.9267135262489319
Epoch 540, training loss: 0.7079470157623291 = 0.05259731411933899 + 0.1 * 6.553497314453125
Epoch 540, val loss: 0.9372226595878601
Epoch 550, training loss: 0.7038078308105469 = 0.04880492389202118 + 0.1 * 6.5500288009643555
Epoch 550, val loss: 0.9478357434272766
Epoch 560, training loss: 0.6998953223228455 = 0.045379187911748886 + 0.1 * 6.545161724090576
Epoch 560, val loss: 0.958440363407135
Epoch 570, training loss: 0.6959934234619141 = 0.04228103533387184 + 0.1 * 6.537123680114746
Epoch 570, val loss: 0.9689144492149353
Epoch 580, training loss: 0.6955471634864807 = 0.039477840065956116 + 0.1 * 6.560693264007568
Epoch 580, val loss: 0.9792478084564209
Epoch 590, training loss: 0.6903870701789856 = 0.0369427353143692 + 0.1 * 6.534443378448486
Epoch 590, val loss: 0.9895923733711243
Epoch 600, training loss: 0.6875374913215637 = 0.03463732823729515 + 0.1 * 6.529001235961914
Epoch 600, val loss: 0.999703049659729
Epoch 610, training loss: 0.6864013671875 = 0.032537490129470825 + 0.1 * 6.538638114929199
Epoch 610, val loss: 1.0096060037612915
Epoch 620, training loss: 0.6830631494522095 = 0.030622122809290886 + 0.1 * 6.524410247802734
Epoch 620, val loss: 1.0194116830825806
Epoch 630, training loss: 0.6811493635177612 = 0.028870832175016403 + 0.1 * 6.522785186767578
Epoch 630, val loss: 1.0290313959121704
Epoch 640, training loss: 0.6793141961097717 = 0.027266625314950943 + 0.1 * 6.5204758644104
Epoch 640, val loss: 1.038575530052185
Epoch 650, training loss: 0.6778448820114136 = 0.025792928412556648 + 0.1 * 6.520519256591797
Epoch 650, val loss: 1.0479127168655396
Epoch 660, training loss: 0.6751831769943237 = 0.024438152089715004 + 0.1 * 6.507450103759766
Epoch 660, val loss: 1.0570573806762695
Epoch 670, training loss: 0.6755342483520508 = 0.02318834513425827 + 0.1 * 6.523458957672119
Epoch 670, val loss: 1.0660443305969238
Epoch 680, training loss: 0.672663152217865 = 0.022036384791135788 + 0.1 * 6.506267547607422
Epoch 680, val loss: 1.07485032081604
Epoch 690, training loss: 0.671484112739563 = 0.02097056806087494 + 0.1 * 6.5051350593566895
Epoch 690, val loss: 1.0835102796554565
Epoch 700, training loss: 0.6705642938613892 = 0.019982963800430298 + 0.1 * 6.505813121795654
Epoch 700, val loss: 1.0919073820114136
Epoch 710, training loss: 0.6692497730255127 = 0.019065862521529198 + 0.1 * 6.5018391609191895
Epoch 710, val loss: 1.1002559661865234
Epoch 720, training loss: 0.6680867671966553 = 0.018213320523500443 + 0.1 * 6.498733997344971
Epoch 720, val loss: 1.1083738803863525
Epoch 730, training loss: 0.6683887243270874 = 0.01741878315806389 + 0.1 * 6.50969934463501
Epoch 730, val loss: 1.1162980794906616
Epoch 740, training loss: 0.6675788760185242 = 0.016678636893630028 + 0.1 * 6.509002208709717
Epoch 740, val loss: 1.1240328550338745
Epoch 750, training loss: 0.6648499369621277 = 0.015988308936357498 + 0.1 * 6.488616466522217
Epoch 750, val loss: 1.1317081451416016
Epoch 760, training loss: 0.6658791899681091 = 0.015342684462666512 + 0.1 * 6.505364418029785
Epoch 760, val loss: 1.1392693519592285
Epoch 770, training loss: 0.6634761691093445 = 0.014735940843820572 + 0.1 * 6.487401962280273
Epoch 770, val loss: 1.1465777158737183
Epoch 780, training loss: 0.6638797521591187 = 0.014168094843626022 + 0.1 * 6.497116565704346
Epoch 780, val loss: 1.1537737846374512
Epoch 790, training loss: 0.6619238257408142 = 0.013635645620524883 + 0.1 * 6.482882022857666
Epoch 790, val loss: 1.1608760356903076
Epoch 800, training loss: 0.6610894799232483 = 0.013133704662322998 + 0.1 * 6.479557514190674
Epoch 800, val loss: 1.1678643226623535
Epoch 810, training loss: 0.6600927114486694 = 0.012660440988838673 + 0.1 * 6.474322319030762
Epoch 810, val loss: 1.1747127771377563
Epoch 820, training loss: 0.6614232659339905 = 0.012213444337248802 + 0.1 * 6.492097854614258
Epoch 820, val loss: 1.1812398433685303
Epoch 830, training loss: 0.659887969493866 = 0.011792194098234177 + 0.1 * 6.480957508087158
Epoch 830, val loss: 1.1878708600997925
Epoch 840, training loss: 0.6594336032867432 = 0.011394075118005276 + 0.1 * 6.4803948402404785
Epoch 840, val loss: 1.1942062377929688
Epoch 850, training loss: 0.657824695110321 = 0.011018241755664349 + 0.1 * 6.468064785003662
Epoch 850, val loss: 1.2005584239959717
Epoch 860, training loss: 0.6575663089752197 = 0.010661745443940163 + 0.1 * 6.469045639038086
Epoch 860, val loss: 1.2067546844482422
Epoch 870, training loss: 0.6568658947944641 = 0.010323618538677692 + 0.1 * 6.4654221534729
Epoch 870, val loss: 1.212764024734497
Epoch 880, training loss: 0.6559528708457947 = 0.010003004223108292 + 0.1 * 6.459498405456543
Epoch 880, val loss: 1.2188546657562256
Epoch 890, training loss: 0.656495213508606 = 0.0096968375146389 + 0.1 * 6.467983722686768
Epoch 890, val loss: 1.224584937095642
Epoch 900, training loss: 0.6561276316642761 = 0.009407204575836658 + 0.1 * 6.4672040939331055
Epoch 900, val loss: 1.2303458452224731
Epoch 910, training loss: 0.6554957032203674 = 0.009131212718784809 + 0.1 * 6.463644981384277
Epoch 910, val loss: 1.2358711957931519
Epoch 920, training loss: 0.655592143535614 = 0.008869464509189129 + 0.1 * 6.467226505279541
Epoch 920, val loss: 1.2415517568588257
Epoch 930, training loss: 0.6543219089508057 = 0.008618995547294617 + 0.1 * 6.457028865814209
Epoch 930, val loss: 1.2469854354858398
Epoch 940, training loss: 0.653795599937439 = 0.008379930630326271 + 0.1 * 6.454156398773193
Epoch 940, val loss: 1.2522599697113037
Epoch 950, training loss: 0.6537743806838989 = 0.008152449503540993 + 0.1 * 6.45621919631958
Epoch 950, val loss: 1.2575669288635254
Epoch 960, training loss: 0.654524564743042 = 0.007934602908790112 + 0.1 * 6.465899467468262
Epoch 960, val loss: 1.2627118825912476
Epoch 970, training loss: 0.6525368094444275 = 0.007726303301751614 + 0.1 * 6.4481048583984375
Epoch 970, val loss: 1.2677441835403442
Epoch 980, training loss: 0.6522123217582703 = 0.0075269122608006 + 0.1 * 6.4468536376953125
Epoch 980, val loss: 1.2727320194244385
Epoch 990, training loss: 0.6526660919189453 = 0.007336136419326067 + 0.1 * 6.453299522399902
Epoch 990, val loss: 1.2776479721069336
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 2.7936205863952637 = 1.9339367151260376 + 0.1 * 8.596839904785156
Epoch 0, val loss: 1.9363408088684082
Epoch 10, training loss: 2.7847723960876465 = 1.9250950813293457 + 0.1 * 8.596774101257324
Epoch 10, val loss: 1.9269944429397583
Epoch 20, training loss: 2.774050235748291 = 1.9144049882888794 + 0.1 * 8.596452713012695
Epoch 20, val loss: 1.9155611991882324
Epoch 30, training loss: 2.759040355682373 = 1.899626612663269 + 0.1 * 8.594138145446777
Epoch 30, val loss: 1.8996925354003906
Epoch 40, training loss: 2.7357380390167236 = 1.8780391216278076 + 0.1 * 8.576988220214844
Epoch 40, val loss: 1.8765689134597778
Epoch 50, training loss: 2.696502685546875 = 1.8479357957839966 + 0.1 * 8.485669136047363
Epoch 50, val loss: 1.8453353643417358
Epoch 60, training loss: 2.638315200805664 = 1.8142114877700806 + 0.1 * 8.241037368774414
Epoch 60, val loss: 1.8129231929779053
Epoch 70, training loss: 2.5689761638641357 = 1.7843797206878662 + 0.1 * 7.8459649085998535
Epoch 70, val loss: 1.7869651317596436
Epoch 80, training loss: 2.493896722793579 = 1.7543233633041382 + 0.1 * 7.3957343101501465
Epoch 80, val loss: 1.7614821195602417
Epoch 90, training loss: 2.4374771118164062 = 1.717475414276123 + 0.1 * 7.200017929077148
Epoch 90, val loss: 1.72968590259552
Epoch 100, training loss: 2.3796093463897705 = 1.668190836906433 + 0.1 * 7.114184856414795
Epoch 100, val loss: 1.6871720552444458
Epoch 110, training loss: 2.3085358142852783 = 1.6018099784851074 + 0.1 * 7.067257404327393
Epoch 110, val loss: 1.6308900117874146
Epoch 120, training loss: 2.2230310440063477 = 1.5186611413955688 + 0.1 * 7.043698310852051
Epoch 120, val loss: 1.5609450340270996
Epoch 130, training loss: 2.1251254081726074 = 1.4219809770584106 + 0.1 * 7.031444072723389
Epoch 130, val loss: 1.4809861183166504
Epoch 140, training loss: 2.0194435119628906 = 1.3172204494476318 + 0.1 * 7.022231578826904
Epoch 140, val loss: 1.3957688808441162
Epoch 150, training loss: 1.909752368927002 = 1.2083767652511597 + 0.1 * 7.0137553215026855
Epoch 150, val loss: 1.3086955547332764
Epoch 160, training loss: 1.8002197742462158 = 1.0998685359954834 + 0.1 * 7.003511428833008
Epoch 160, val loss: 1.224166989326477
Epoch 170, training loss: 1.69697904586792 = 0.9979073405265808 + 0.1 * 6.990716934204102
Epoch 170, val loss: 1.1473852396011353
Epoch 180, training loss: 1.6033222675323486 = 0.9056676030158997 + 0.1 * 6.976546287536621
Epoch 180, val loss: 1.0801030397415161
Epoch 190, training loss: 1.520395040512085 = 0.8247736692428589 + 0.1 * 6.956214427947998
Epoch 190, val loss: 1.0231834650039673
Epoch 200, training loss: 1.4504616260528564 = 0.7557779550552368 + 0.1 * 6.946836948394775
Epoch 200, val loss: 0.9767906069755554
Epoch 210, training loss: 1.388840913772583 = 0.6973682641983032 + 0.1 * 6.914727210998535
Epoch 210, val loss: 0.9397850036621094
Epoch 220, training loss: 1.3352851867675781 = 0.6457070708274841 + 0.1 * 6.895781517028809
Epoch 220, val loss: 0.908841073513031
Epoch 230, training loss: 1.2859742641448975 = 0.5982707142829895 + 0.1 * 6.877035140991211
Epoch 230, val loss: 0.8822158575057983
Epoch 240, training loss: 1.2403422594070435 = 0.5529322624206543 + 0.1 * 6.8740997314453125
Epoch 240, val loss: 0.8585039973258972
Epoch 250, training loss: 1.1938973665237427 = 0.5092673301696777 + 0.1 * 6.84630012512207
Epoch 250, val loss: 0.8373609185218811
Epoch 260, training loss: 1.1495624780654907 = 0.46634766459465027 + 0.1 * 6.83214807510376
Epoch 260, val loss: 0.8179639577865601
Epoch 270, training loss: 1.1060683727264404 = 0.4244380593299866 + 0.1 * 6.816303730010986
Epoch 270, val loss: 0.8006235361099243
Epoch 280, training loss: 1.0650017261505127 = 0.3837868869304657 + 0.1 * 6.812148094177246
Epoch 280, val loss: 0.7852999567985535
Epoch 290, training loss: 1.0241668224334717 = 0.3447415232658386 + 0.1 * 6.794252395629883
Epoch 290, val loss: 0.772077202796936
Epoch 300, training loss: 0.9869405031204224 = 0.3075999319553375 + 0.1 * 6.793405055999756
Epoch 300, val loss: 0.760858952999115
Epoch 310, training loss: 0.9512889981269836 = 0.27303850650787354 + 0.1 * 6.782505035400391
Epoch 310, val loss: 0.7520745396614075
Epoch 320, training loss: 0.9177777171134949 = 0.2415243238210678 + 0.1 * 6.762533664703369
Epoch 320, val loss: 0.7459577322006226
Epoch 330, training loss: 0.8882865309715271 = 0.21325278282165527 + 0.1 * 6.750337600708008
Epoch 330, val loss: 0.7427754998207092
Epoch 340, training loss: 0.8634015321731567 = 0.18840375542640686 + 0.1 * 6.749978065490723
Epoch 340, val loss: 0.7427313327789307
Epoch 350, training loss: 0.8402762413024902 = 0.16683462262153625 + 0.1 * 6.734416484832764
Epoch 350, val loss: 0.7453338503837585
Epoch 360, training loss: 0.8228704333305359 = 0.14806343615055084 + 0.1 * 6.748069763183594
Epoch 360, val loss: 0.7505337595939636
Epoch 370, training loss: 0.8029652833938599 = 0.13185924291610718 + 0.1 * 6.711060523986816
Epoch 370, val loss: 0.7575198411941528
Epoch 380, training loss: 0.7888134717941284 = 0.11781099438667297 + 0.1 * 6.710024356842041
Epoch 380, val loss: 0.7660139203071594
Epoch 390, training loss: 0.7759215831756592 = 0.10567906498908997 + 0.1 * 6.702425479888916
Epoch 390, val loss: 0.7757086753845215
Epoch 400, training loss: 0.764030396938324 = 0.09514664858579636 + 0.1 * 6.688837051391602
Epoch 400, val loss: 0.7862508893013
Epoch 410, training loss: 0.7546118497848511 = 0.08600492030382156 + 0.1 * 6.686069011688232
Epoch 410, val loss: 0.7972123026847839
Epoch 420, training loss: 0.7455182075500488 = 0.07806507498025894 + 0.1 * 6.674530982971191
Epoch 420, val loss: 0.8087131977081299
Epoch 430, training loss: 0.7366366386413574 = 0.07111934572458267 + 0.1 * 6.655172824859619
Epoch 430, val loss: 0.8203908205032349
Epoch 440, training loss: 0.73304682970047 = 0.06501234322786331 + 0.1 * 6.680345058441162
Epoch 440, val loss: 0.8320851922035217
Epoch 450, training loss: 0.7242823839187622 = 0.05966319143772125 + 0.1 * 6.6461920738220215
Epoch 450, val loss: 0.8439475893974304
Epoch 460, training loss: 0.7193039059638977 = 0.054925911128520966 + 0.1 * 6.643779754638672
Epoch 460, val loss: 0.8555259704589844
Epoch 470, training loss: 0.7139574885368347 = 0.05072807893157005 + 0.1 * 6.632293701171875
Epoch 470, val loss: 0.8671888113021851
Epoch 480, training loss: 0.7102764248847961 = 0.046984340995550156 + 0.1 * 6.63292121887207
Epoch 480, val loss: 0.8785275220870972
Epoch 490, training loss: 0.7073339223861694 = 0.04364989325404167 + 0.1 * 6.636839866638184
Epoch 490, val loss: 0.8898584842681885
Epoch 500, training loss: 0.7017094492912292 = 0.04065970331430435 + 0.1 * 6.610496997833252
Epoch 500, val loss: 0.9009118676185608
Epoch 510, training loss: 0.6995210647583008 = 0.03796215355396271 + 0.1 * 6.615589141845703
Epoch 510, val loss: 0.9118813872337341
Epoch 520, training loss: 0.6962897181510925 = 0.035528916865587234 + 0.1 * 6.607607841491699
Epoch 520, val loss: 0.9225199818611145
Epoch 530, training loss: 0.6929660439491272 = 0.033327486366033554 + 0.1 * 6.596385478973389
Epoch 530, val loss: 0.9330177307128906
Epoch 540, training loss: 0.6914389729499817 = 0.03132571652531624 + 0.1 * 6.601132392883301
Epoch 540, val loss: 0.9433478116989136
Epoch 550, training loss: 0.6885043978691101 = 0.029505329206585884 + 0.1 * 6.589990615844727
Epoch 550, val loss: 0.9533123970031738
Epoch 560, training loss: 0.6865326762199402 = 0.027841897681355476 + 0.1 * 6.586907386779785
Epoch 560, val loss: 0.9632155895233154
Epoch 570, training loss: 0.687178909778595 = 0.026317449286580086 + 0.1 * 6.608613967895508
Epoch 570, val loss: 0.9728448987007141
Epoch 580, training loss: 0.6840397119522095 = 0.024919750168919563 + 0.1 * 6.5911993980407715
Epoch 580, val loss: 0.9822240471839905
Epoch 590, training loss: 0.6812023520469666 = 0.023636605590581894 + 0.1 * 6.575657367706299
Epoch 590, val loss: 0.9914991855621338
Epoch 600, training loss: 0.6791393756866455 = 0.022456228733062744 + 0.1 * 6.566831588745117
Epoch 600, val loss: 1.0005091428756714
Epoch 610, training loss: 0.6778075695037842 = 0.02136223018169403 + 0.1 * 6.564453125
Epoch 610, val loss: 1.009176254272461
Epoch 620, training loss: 0.6764726638793945 = 0.020354611799120903 + 0.1 * 6.561180114746094
Epoch 620, val loss: 1.0179561376571655
Epoch 630, training loss: 0.6750872731208801 = 0.01941698230803013 + 0.1 * 6.556702613830566
Epoch 630, val loss: 1.0262399911880493
Epoch 640, training loss: 0.675808310508728 = 0.018550366163253784 + 0.1 * 6.572579860687256
Epoch 640, val loss: 1.034542441368103
Epoch 650, training loss: 0.6729609966278076 = 0.01774223893880844 + 0.1 * 6.552187442779541
Epoch 650, val loss: 1.0425008535385132
Epoch 660, training loss: 0.6712523102760315 = 0.01698986254632473 + 0.1 * 6.542623996734619
Epoch 660, val loss: 1.0504311323165894
Epoch 670, training loss: 0.6730597019195557 = 0.016285963356494904 + 0.1 * 6.567737579345703
Epoch 670, val loss: 1.058123230934143
Epoch 680, training loss: 0.6697375774383545 = 0.015627367421984673 + 0.1 * 6.541101932525635
Epoch 680, val loss: 1.0656402111053467
Epoch 690, training loss: 0.6706794500350952 = 0.015011366456747055 + 0.1 * 6.556680679321289
Epoch 690, val loss: 1.0730409622192383
Epoch 700, training loss: 0.668741762638092 = 0.014433247968554497 + 0.1 * 6.543084621429443
Epoch 700, val loss: 1.080229640007019
Epoch 710, training loss: 0.668228805065155 = 0.013891003094613552 + 0.1 * 6.5433783531188965
Epoch 710, val loss: 1.0874580144882202
Epoch 720, training loss: 0.6667096614837646 = 0.01338102761656046 + 0.1 * 6.5332865715026855
Epoch 720, val loss: 1.0943423509597778
Epoch 730, training loss: 0.6646593809127808 = 0.01290025096386671 + 0.1 * 6.51759147644043
Epoch 730, val loss: 1.1011273860931396
Epoch 740, training loss: 0.6642062067985535 = 0.012447956949472427 + 0.1 * 6.517582416534424
Epoch 740, val loss: 1.1079542636871338
Epoch 750, training loss: 0.6652604341506958 = 0.012019424699246883 + 0.1 * 6.532410144805908
Epoch 750, val loss: 1.1144580841064453
Epoch 760, training loss: 0.6639304161071777 = 0.011616608127951622 + 0.1 * 6.523138046264648
Epoch 760, val loss: 1.1208813190460205
Epoch 770, training loss: 0.6627890467643738 = 0.011234099045395851 + 0.1 * 6.515549182891846
Epoch 770, val loss: 1.1271899938583374
Epoch 780, training loss: 0.6626942157745361 = 0.010874539613723755 + 0.1 * 6.5181965827941895
Epoch 780, val loss: 1.1334590911865234
Epoch 790, training loss: 0.6634356379508972 = 0.010530858300626278 + 0.1 * 6.529047966003418
Epoch 790, val loss: 1.1394741535186768
Epoch 800, training loss: 0.66086345911026 = 0.01020568236708641 + 0.1 * 6.506577491760254
Epoch 800, val loss: 1.145544171333313
Epoch 810, training loss: 0.6605220437049866 = 0.009896669536828995 + 0.1 * 6.506253242492676
Epoch 810, val loss: 1.1513736248016357
Epoch 820, training loss: 0.660323441028595 = 0.009603622369468212 + 0.1 * 6.507197856903076
Epoch 820, val loss: 1.1571863889694214
Epoch 830, training loss: 0.6588805317878723 = 0.009323786944150925 + 0.1 * 6.495567321777344
Epoch 830, val loss: 1.162912130355835
Epoch 840, training loss: 0.6598368883132935 = 0.009057577699422836 + 0.1 * 6.507792949676514
Epoch 840, val loss: 1.1684746742248535
Epoch 850, training loss: 0.6579857468605042 = 0.008803662844002247 + 0.1 * 6.491820812225342
Epoch 850, val loss: 1.1739858388900757
Epoch 860, training loss: 0.6587427258491516 = 0.008561010472476482 + 0.1 * 6.501817226409912
Epoch 860, val loss: 1.1793967485427856
Epoch 870, training loss: 0.6564534902572632 = 0.008329452015459538 + 0.1 * 6.481240272521973
Epoch 870, val loss: 1.1846853494644165
Epoch 880, training loss: 0.658180296421051 = 0.008107499219477177 + 0.1 * 6.500728130340576
Epoch 880, val loss: 1.1900211572647095
Epoch 890, training loss: 0.6572355628013611 = 0.007897200994193554 + 0.1 * 6.493383407592773
Epoch 890, val loss: 1.1950688362121582
Epoch 900, training loss: 0.6563658714294434 = 0.007694766391068697 + 0.1 * 6.486711025238037
Epoch 900, val loss: 1.2001752853393555
Epoch 910, training loss: 0.6566296219825745 = 0.007501996122300625 + 0.1 * 6.491275787353516
Epoch 910, val loss: 1.205201268196106
Epoch 920, training loss: 0.655269980430603 = 0.007316297385841608 + 0.1 * 6.479537010192871
Epoch 920, val loss: 1.2100471258163452
Epoch 930, training loss: 0.6549405455589294 = 0.007138701155781746 + 0.1 * 6.478017807006836
Epoch 930, val loss: 1.2149450778961182
Epoch 940, training loss: 0.6547871232032776 = 0.0069679017178714275 + 0.1 * 6.47819185256958
Epoch 940, val loss: 1.2196511030197144
Epoch 950, training loss: 0.6545301079750061 = 0.006803718861192465 + 0.1 * 6.477263927459717
Epoch 950, val loss: 1.2244535684585571
Epoch 960, training loss: 0.6543003916740417 = 0.00664546201005578 + 0.1 * 6.47654914855957
Epoch 960, val loss: 1.2289842367172241
Epoch 970, training loss: 0.6544051766395569 = 0.006493436172604561 + 0.1 * 6.479116916656494
Epoch 970, val loss: 1.2334445714950562
Epoch 980, training loss: 0.653556227684021 = 0.006348536349833012 + 0.1 * 6.472076416015625
Epoch 980, val loss: 1.2380239963531494
Epoch 990, training loss: 0.652411162853241 = 0.006208167411386967 + 0.1 * 6.462029457092285
Epoch 990, val loss: 1.2424399852752686
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.808645229309436
The final CL Acc:0.74815, 0.01048, The final GNN Acc:0.80865, 0.00215
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13148])
remove edge: torch.Size([2, 7942])
updated graph: torch.Size([2, 10534])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.7990448474884033 = 1.9393659830093384 + 0.1 * 8.596789360046387
Epoch 0, val loss: 1.9370462894439697
Epoch 10, training loss: 2.7890422344207764 = 1.9293758869171143 + 0.1 * 8.596663475036621
Epoch 10, val loss: 1.9269156455993652
Epoch 20, training loss: 2.77655029296875 = 1.916960597038269 + 0.1 * 8.59589672088623
Epoch 20, val loss: 1.9142135381698608
Epoch 30, training loss: 2.758373498916626 = 1.8994280099868774 + 0.1 * 8.589454650878906
Epoch 30, val loss: 1.895926833152771
Epoch 40, training loss: 2.7278754711151123 = 1.8733916282653809 + 0.1 * 8.544837951660156
Epoch 40, val loss: 1.8689662218093872
Epoch 50, training loss: 2.666450262069702 = 1.8380070924758911 + 0.1 * 8.284432411193848
Epoch 50, val loss: 1.8340692520141602
Epoch 60, training loss: 2.5972342491149902 = 1.799868106842041 + 0.1 * 7.973661422729492
Epoch 60, val loss: 1.7992202043533325
Epoch 70, training loss: 2.515895366668701 = 1.7645955085754395 + 0.1 * 7.512998580932617
Epoch 70, val loss: 1.767761468887329
Epoch 80, training loss: 2.4451701641082764 = 1.726470947265625 + 0.1 * 7.186991214752197
Epoch 80, val loss: 1.734131932258606
Epoch 90, training loss: 2.3860018253326416 = 1.6761704683303833 + 0.1 * 7.098313808441162
Epoch 90, val loss: 1.6900458335876465
Epoch 100, training loss: 2.3157131671905518 = 1.6093957424163818 + 0.1 * 7.063174724578857
Epoch 100, val loss: 1.6322436332702637
Epoch 110, training loss: 2.2353458404541016 = 1.532096028327942 + 0.1 * 7.032497406005859
Epoch 110, val loss: 1.568938136100769
Epoch 120, training loss: 2.153783082962036 = 1.4536904096603394 + 0.1 * 7.0009260177612305
Epoch 120, val loss: 1.506736397743225
Epoch 130, training loss: 2.074775457382202 = 1.3780889511108398 + 0.1 * 6.966865062713623
Epoch 130, val loss: 1.4488574266433716
Epoch 140, training loss: 1.9953622817993164 = 1.3027440309524536 + 0.1 * 6.926182270050049
Epoch 140, val loss: 1.391865849494934
Epoch 150, training loss: 1.9131672382354736 = 1.2241756916046143 + 0.1 * 6.889915943145752
Epoch 150, val loss: 1.3321020603179932
Epoch 160, training loss: 1.8298083543777466 = 1.1440266370773315 + 0.1 * 6.85781717300415
Epoch 160, val loss: 1.2709208726882935
Epoch 170, training loss: 1.7456941604614258 = 1.0626323223114014 + 0.1 * 6.830617427825928
Epoch 170, val loss: 1.2082985639572144
Epoch 180, training loss: 1.665478229522705 = 0.9841756224632263 + 0.1 * 6.81302547454834
Epoch 180, val loss: 1.1479934453964233
Epoch 190, training loss: 1.590040922164917 = 0.910643458366394 + 0.1 * 6.793973922729492
Epoch 190, val loss: 1.0916717052459717
Epoch 200, training loss: 1.5183846950531006 = 0.8408080339431763 + 0.1 * 6.775766372680664
Epoch 200, val loss: 1.0375858545303345
Epoch 210, training loss: 1.4521338939666748 = 0.7755416035652161 + 0.1 * 6.765923500061035
Epoch 210, val loss: 0.9868448972702026
Epoch 220, training loss: 1.3899624347686768 = 0.7150858640670776 + 0.1 * 6.748764991760254
Epoch 220, val loss: 0.9400281310081482
Epoch 230, training loss: 1.33247971534729 = 0.6589121222496033 + 0.1 * 6.735675811767578
Epoch 230, val loss: 0.8975647687911987
Epoch 240, training loss: 1.2807681560516357 = 0.6078243255615234 + 0.1 * 6.729438781738281
Epoch 240, val loss: 0.8606539964675903
Epoch 250, training loss: 1.232252597808838 = 0.5612155199050903 + 0.1 * 6.7103705406188965
Epoch 250, val loss: 0.8292880654335022
Epoch 260, training loss: 1.19075345993042 = 0.5184099674224854 + 0.1 * 6.723434925079346
Epoch 260, val loss: 0.8031917214393616
Epoch 270, training loss: 1.1494543552398682 = 0.47972503304481506 + 0.1 * 6.697292804718018
Epoch 270, val loss: 0.7820228338241577
Epoch 280, training loss: 1.1131834983825684 = 0.4443989098072052 + 0.1 * 6.6878461837768555
Epoch 280, val loss: 0.7651176452636719
Epoch 290, training loss: 1.0798616409301758 = 0.41199058294296265 + 0.1 * 6.678709983825684
Epoch 290, val loss: 0.7517263889312744
Epoch 300, training loss: 1.0485445261001587 = 0.3817856013774872 + 0.1 * 6.66758918762207
Epoch 300, val loss: 0.7409674525260925
Epoch 310, training loss: 1.0193252563476562 = 0.35324907302856445 + 0.1 * 6.660761833190918
Epoch 310, val loss: 0.7321804761886597
Epoch 320, training loss: 0.9910870790481567 = 0.3261200189590454 + 0.1 * 6.649670600891113
Epoch 320, val loss: 0.7246091961860657
Epoch 330, training loss: 0.9667099714279175 = 0.30005690455436707 + 0.1 * 6.666531085968018
Epoch 330, val loss: 0.7179971933364868
Epoch 340, training loss: 0.9389519691467285 = 0.27511242032051086 + 0.1 * 6.638395309448242
Epoch 340, val loss: 0.7119895815849304
Epoch 350, training loss: 0.9139062166213989 = 0.25108686089515686 + 0.1 * 6.628193378448486
Epoch 350, val loss: 0.7064968347549438
Epoch 360, training loss: 0.8899025917053223 = 0.2279857099056244 + 0.1 * 6.619168758392334
Epoch 360, val loss: 0.701480507850647
Epoch 370, training loss: 0.8681190013885498 = 0.2058042734861374 + 0.1 * 6.623147010803223
Epoch 370, val loss: 0.6971275210380554
Epoch 380, training loss: 0.8459461331367493 = 0.18483664095401764 + 0.1 * 6.611094951629639
Epoch 380, val loss: 0.6933578252792358
Epoch 390, training loss: 0.827822208404541 = 0.1652374416589737 + 0.1 * 6.625847816467285
Epoch 390, val loss: 0.6904507875442505
Epoch 400, training loss: 0.8076999187469482 = 0.14726553857326508 + 0.1 * 6.604343414306641
Epoch 400, val loss: 0.6883968114852905
Epoch 410, training loss: 0.7916404604911804 = 0.13100242614746094 + 0.1 * 6.606380462646484
Epoch 410, val loss: 0.6874799132347107
Epoch 420, training loss: 0.7754796743392944 = 0.11649692803621292 + 0.1 * 6.589827537536621
Epoch 420, val loss: 0.6876103281974792
Epoch 430, training loss: 0.761481761932373 = 0.10370507836341858 + 0.1 * 6.5777668952941895
Epoch 430, val loss: 0.6888734698295593
Epoch 440, training loss: 0.7502068877220154 = 0.0925137847661972 + 0.1 * 6.576930522918701
Epoch 440, val loss: 0.6911735534667969
Epoch 450, training loss: 0.7404112815856934 = 0.08277373760938644 + 0.1 * 6.5763750076293945
Epoch 450, val loss: 0.6944541335105896
Epoch 460, training loss: 0.7306323051452637 = 0.07434400916099548 + 0.1 * 6.562883377075195
Epoch 460, val loss: 0.6985614895820618
Epoch 470, training loss: 0.7239515781402588 = 0.06702346354722977 + 0.1 * 6.569281101226807
Epoch 470, val loss: 0.7033830881118774
Epoch 480, training loss: 0.7160525918006897 = 0.06066635623574257 + 0.1 * 6.55386209487915
Epoch 480, val loss: 0.7087756395339966
Epoch 490, training loss: 0.7097916007041931 = 0.05514201894402504 + 0.1 * 6.54649543762207
Epoch 490, val loss: 0.7145808935165405
Epoch 500, training loss: 0.7048778533935547 = 0.05032304674386978 + 0.1 * 6.545547962188721
Epoch 500, val loss: 0.720633327960968
Epoch 510, training loss: 0.6998220682144165 = 0.046094514429569244 + 0.1 * 6.537275314331055
Epoch 510, val loss: 0.7269880771636963
Epoch 520, training loss: 0.6961073279380798 = 0.04237145930528641 + 0.1 * 6.53735876083374
Epoch 520, val loss: 0.7334724068641663
Epoch 530, training loss: 0.6931188106536865 = 0.039087146520614624 + 0.1 * 6.540316104888916
Epoch 530, val loss: 0.7399454116821289
Epoch 540, training loss: 0.6889205574989319 = 0.03617193549871445 + 0.1 * 6.527486324310303
Epoch 540, val loss: 0.7465294003486633
Epoch 550, training loss: 0.6890319585800171 = 0.03356856480240822 + 0.1 * 6.554634094238281
Epoch 550, val loss: 0.7530824542045593
Epoch 560, training loss: 0.6831632256507874 = 0.03124977834522724 + 0.1 * 6.519134521484375
Epoch 560, val loss: 0.7594839334487915
Epoch 570, training loss: 0.6815398335456848 = 0.02916943095624447 + 0.1 * 6.5237040519714355
Epoch 570, val loss: 0.7658201456069946
Epoch 580, training loss: 0.6783387064933777 = 0.027292026206851006 + 0.1 * 6.510467052459717
Epoch 580, val loss: 0.772095263004303
Epoch 590, training loss: 0.6763997077941895 = 0.025595391169190407 + 0.1 * 6.50804328918457
Epoch 590, val loss: 0.778200626373291
Epoch 600, training loss: 0.6751859188079834 = 0.024055300280451775 + 0.1 * 6.511306285858154
Epoch 600, val loss: 0.784232497215271
Epoch 610, training loss: 0.6739969253540039 = 0.02265685237944126 + 0.1 * 6.513400554656982
Epoch 610, val loss: 0.7901908159255981
Epoch 620, training loss: 0.6715446710586548 = 0.02138660103082657 + 0.1 * 6.501580238342285
Epoch 620, val loss: 0.7958804368972778
Epoch 630, training loss: 0.6704298257827759 = 0.020223094150424004 + 0.1 * 6.5020670890808105
Epoch 630, val loss: 0.8015866875648499
Epoch 640, training loss: 0.6684018969535828 = 0.019157156348228455 + 0.1 * 6.4924468994140625
Epoch 640, val loss: 0.8071228861808777
Epoch 650, training loss: 0.6667049527168274 = 0.01817748136818409 + 0.1 * 6.485274314880371
Epoch 650, val loss: 0.8125522136688232
Epoch 660, training loss: 0.6679582595825195 = 0.01727277785539627 + 0.1 * 6.50685453414917
Epoch 660, val loss: 0.8179128170013428
Epoch 670, training loss: 0.6652143597602844 = 0.016439488157629967 + 0.1 * 6.487748622894287
Epoch 670, val loss: 0.8230786323547363
Epoch 680, training loss: 0.6634712219238281 = 0.01567039079964161 + 0.1 * 6.478007793426514
Epoch 680, val loss: 0.828057050704956
Epoch 690, training loss: 0.6624928116798401 = 0.014955970458686352 + 0.1 * 6.475368022918701
Epoch 690, val loss: 0.8330751657485962
Epoch 700, training loss: 0.6617178320884705 = 0.014292486011981964 + 0.1 * 6.474253177642822
Epoch 700, val loss: 0.8379246592521667
Epoch 710, training loss: 0.663277804851532 = 0.01367491390556097 + 0.1 * 6.496028900146484
Epoch 710, val loss: 0.8427152037620544
Epoch 720, training loss: 0.6601617336273193 = 0.013103175908327103 + 0.1 * 6.470585346221924
Epoch 720, val loss: 0.8472544550895691
Epoch 730, training loss: 0.6601118445396423 = 0.012568698264658451 + 0.1 * 6.475431442260742
Epoch 730, val loss: 0.8517445921897888
Epoch 740, training loss: 0.6586300134658813 = 0.012068239971995354 + 0.1 * 6.465618133544922
Epoch 740, val loss: 0.8562625050544739
Epoch 750, training loss: 0.6578102707862854 = 0.01159995049238205 + 0.1 * 6.462102890014648
Epoch 750, val loss: 0.8605315089225769
Epoch 760, training loss: 0.6575417518615723 = 0.011159277521073818 + 0.1 * 6.463824272155762
Epoch 760, val loss: 0.8648641109466553
Epoch 770, training loss: 0.6576181650161743 = 0.010747207328677177 + 0.1 * 6.468709468841553
Epoch 770, val loss: 0.8689494132995605
Epoch 780, training loss: 0.6562252044677734 = 0.010359548032283783 + 0.1 * 6.4586567878723145
Epoch 780, val loss: 0.8730753064155579
Epoch 790, training loss: 0.655918300151825 = 0.009994247928261757 + 0.1 * 6.459240436553955
Epoch 790, val loss: 0.877080500125885
Epoch 800, training loss: 0.656691312789917 = 0.00964924693107605 + 0.1 * 6.4704203605651855
Epoch 800, val loss: 0.8810432553291321
Epoch 810, training loss: 0.6546549797058105 = 0.00932486355304718 + 0.1 * 6.453300952911377
Epoch 810, val loss: 0.8848628997802734
Epoch 820, training loss: 0.6550585031509399 = 0.009018407203257084 + 0.1 * 6.460400581359863
Epoch 820, val loss: 0.8886224627494812
Epoch 830, training loss: 0.6534083485603333 = 0.008727792650461197 + 0.1 * 6.446805477142334
Epoch 830, val loss: 0.8923901319503784
Epoch 840, training loss: 0.653509795665741 = 0.008453614078462124 + 0.1 * 6.4505615234375
Epoch 840, val loss: 0.8959686756134033
Epoch 850, training loss: 0.6535663604736328 = 0.008192802779376507 + 0.1 * 6.453735828399658
Epoch 850, val loss: 0.8995702862739563
Epoch 860, training loss: 0.6523306965827942 = 0.007946145720779896 + 0.1 * 6.443845272064209
Epoch 860, val loss: 0.9030845165252686
Epoch 870, training loss: 0.652320384979248 = 0.007710687816143036 + 0.1 * 6.446096897125244
Epoch 870, val loss: 0.9064992666244507
Epoch 880, training loss: 0.6519268751144409 = 0.007487359456717968 + 0.1 * 6.444395065307617
Epoch 880, val loss: 0.9099229574203491
Epoch 890, training loss: 0.6508715748786926 = 0.0072750551626086235 + 0.1 * 6.435965061187744
Epoch 890, val loss: 0.913192629814148
Epoch 900, training loss: 0.6504416465759277 = 0.007072398904711008 + 0.1 * 6.433692455291748
Epoch 900, val loss: 0.9165084362030029
Epoch 910, training loss: 0.6501869559288025 = 0.006878798361867666 + 0.1 * 6.43308162689209
Epoch 910, val loss: 0.9197432398796082
Epoch 920, training loss: 0.6522591710090637 = 0.006694658659398556 + 0.1 * 6.4556450843811035
Epoch 920, val loss: 0.9228344559669495
Epoch 930, training loss: 0.6506130695343018 = 0.006518303416669369 + 0.1 * 6.440947532653809
Epoch 930, val loss: 0.9260198473930359
Epoch 940, training loss: 0.6507971286773682 = 0.006350929383188486 + 0.1 * 6.444461822509766
Epoch 940, val loss: 0.9289968609809875
Epoch 950, training loss: 0.6484843492507935 = 0.0061902194283902645 + 0.1 * 6.422940731048584
Epoch 950, val loss: 0.9319812059402466
Epoch 960, training loss: 0.648912787437439 = 0.006036568433046341 + 0.1 * 6.428761959075928
Epoch 960, val loss: 0.9349218010902405
Epoch 970, training loss: 0.6481186747550964 = 0.005888931918889284 + 0.1 * 6.422297477722168
Epoch 970, val loss: 0.9378781914710999
Epoch 980, training loss: 0.6491138339042664 = 0.005747686605900526 + 0.1 * 6.433661460876465
Epoch 980, val loss: 0.9407128691673279
Epoch 990, training loss: 0.6476075649261475 = 0.005611703265458345 + 0.1 * 6.419958591461182
Epoch 990, val loss: 0.9435436725616455
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 2.794513702392578 = 1.9348323345184326 + 0.1 * 8.596814155578613
Epoch 0, val loss: 1.9305074214935303
Epoch 10, training loss: 2.7852296829223633 = 1.925557017326355 + 0.1 * 8.596726417541504
Epoch 10, val loss: 1.92201828956604
Epoch 20, training loss: 2.773980140686035 = 1.9143587350845337 + 0.1 * 8.596213340759277
Epoch 20, val loss: 1.9114614725112915
Epoch 30, training loss: 2.758026599884033 = 1.8987966775894165 + 0.1 * 8.592300415039062
Epoch 30, val loss: 1.896632194519043
Epoch 40, training loss: 2.732287883758545 = 1.875798225402832 + 0.1 * 8.564897537231445
Epoch 40, val loss: 1.87485671043396
Epoch 50, training loss: 2.684556484222412 = 1.8436028957366943 + 0.1 * 8.409534454345703
Epoch 50, val loss: 1.8455195426940918
Epoch 60, training loss: 2.6183061599731445 = 1.806174397468567 + 0.1 * 8.121318817138672
Epoch 60, val loss: 1.8130027055740356
Epoch 70, training loss: 2.5494863986968994 = 1.7684470415115356 + 0.1 * 7.810393810272217
Epoch 70, val loss: 1.779927134513855
Epoch 80, training loss: 2.472048759460449 = 1.7281206846237183 + 0.1 * 7.439281463623047
Epoch 80, val loss: 1.7443270683288574
Epoch 90, training loss: 2.3991599082946777 = 1.6780555248260498 + 0.1 * 7.211042881011963
Epoch 90, val loss: 1.700440526008606
Epoch 100, training loss: 2.321369171142578 = 1.6113300323486328 + 0.1 * 7.1003923416137695
Epoch 100, val loss: 1.6432461738586426
Epoch 110, training loss: 2.23095703125 = 1.528727412223816 + 0.1 * 7.02229642868042
Epoch 110, val loss: 1.575024127960205
Epoch 120, training loss: 2.1333916187286377 = 1.4363939762115479 + 0.1 * 6.969976902008057
Epoch 120, val loss: 1.5004464387893677
Epoch 130, training loss: 2.034726142883301 = 1.3412045240402222 + 0.1 * 6.935214996337891
Epoch 130, val loss: 1.424562692642212
Epoch 140, training loss: 1.9371254444122314 = 1.2464239597320557 + 0.1 * 6.9070143699646
Epoch 140, val loss: 1.349576473236084
Epoch 150, training loss: 1.843515157699585 = 1.1554594039916992 + 0.1 * 6.880557060241699
Epoch 150, val loss: 1.2796438932418823
Epoch 160, training loss: 1.7584178447723389 = 1.071629285812378 + 0.1 * 6.867886066436768
Epoch 160, val loss: 1.2168821096420288
Epoch 170, training loss: 1.6828207969665527 = 0.99847412109375 + 0.1 * 6.843465805053711
Epoch 170, val loss: 1.1645698547363281
Epoch 180, training loss: 1.6162691116333008 = 0.9331965446472168 + 0.1 * 6.83072566986084
Epoch 180, val loss: 1.118934988975525
Epoch 190, training loss: 1.554924726486206 = 0.8728306889533997 + 0.1 * 6.8209404945373535
Epoch 190, val loss: 1.0775429010391235
Epoch 200, training loss: 1.4954746961593628 = 0.814599335193634 + 0.1 * 6.808753490447998
Epoch 200, val loss: 1.0378496646881104
Epoch 210, training loss: 1.4370285272598267 = 0.7565169930458069 + 0.1 * 6.805115222930908
Epoch 210, val loss: 0.9983773827552795
Epoch 220, training loss: 1.3783290386199951 = 0.6987506151199341 + 0.1 * 6.7957844734191895
Epoch 220, val loss: 0.9597352147102356
Epoch 230, training loss: 1.3195388317108154 = 0.6409430503845215 + 0.1 * 6.785956859588623
Epoch 230, val loss: 0.9228248596191406
Epoch 240, training loss: 1.2622416019439697 = 0.5830917358398438 + 0.1 * 6.79149866104126
Epoch 240, val loss: 0.8886399269104004
Epoch 250, training loss: 1.2038538455963135 = 0.5263568758964539 + 0.1 * 6.774970054626465
Epoch 250, val loss: 0.8583754301071167
Epoch 260, training loss: 1.1484557390213013 = 0.471495658159256 + 0.1 * 6.769600868225098
Epoch 260, val loss: 0.8324214816093445
Epoch 270, training loss: 1.0975768566131592 = 0.41966819763183594 + 0.1 * 6.779086112976074
Epoch 270, val loss: 0.8107741475105286
Epoch 280, training loss: 1.0487536191940308 = 0.3722478151321411 + 0.1 * 6.7650580406188965
Epoch 280, val loss: 0.7932969927787781
Epoch 290, training loss: 1.0044751167297363 = 0.328870952129364 + 0.1 * 6.756041526794434
Epoch 290, val loss: 0.7791611552238464
Epoch 300, training loss: 0.9643230438232422 = 0.28930047154426575 + 0.1 * 6.750225067138672
Epoch 300, val loss: 0.7678928375244141
Epoch 310, training loss: 0.9292199611663818 = 0.25349709391593933 + 0.1 * 6.757228374481201
Epoch 310, val loss: 0.7594139575958252
Epoch 320, training loss: 0.8961535692214966 = 0.22169038653373718 + 0.1 * 6.744632244110107
Epoch 320, val loss: 0.7536779642105103
Epoch 330, training loss: 0.8683664798736572 = 0.19377294182777405 + 0.1 * 6.745935440063477
Epoch 330, val loss: 0.7506269812583923
Epoch 340, training loss: 0.8426694869995117 = 0.16978411376476288 + 0.1 * 6.728853225708008
Epoch 340, val loss: 0.7502517700195312
Epoch 350, training loss: 0.8213869333267212 = 0.14930926263332367 + 0.1 * 6.720776557922363
Epoch 350, val loss: 0.752318799495697
Epoch 360, training loss: 0.8050429224967957 = 0.1319248080253601 + 0.1 * 6.7311811447143555
Epoch 360, val loss: 0.7566134929656982
Epoch 370, training loss: 0.7886883020401001 = 0.11724254488945007 + 0.1 * 6.7144575119018555
Epoch 370, val loss: 0.7625930905342102
Epoch 380, training loss: 0.7750452756881714 = 0.10470649600028992 + 0.1 * 6.703387260437012
Epoch 380, val loss: 0.7699981331825256
Epoch 390, training loss: 0.7639861106872559 = 0.09395119547843933 + 0.1 * 6.7003493309021
Epoch 390, val loss: 0.778403103351593
Epoch 400, training loss: 0.7543394565582275 = 0.08467621356248856 + 0.1 * 6.696632385253906
Epoch 400, val loss: 0.7875059843063354
Epoch 410, training loss: 0.7454133033752441 = 0.07660236954689026 + 0.1 * 6.688109397888184
Epoch 410, val loss: 0.7971035242080688
Epoch 420, training loss: 0.7394881248474121 = 0.06957650929689407 + 0.1 * 6.699116230010986
Epoch 420, val loss: 0.8069552779197693
Epoch 430, training loss: 0.731223464012146 = 0.06344465911388397 + 0.1 * 6.677787780761719
Epoch 430, val loss: 0.8168347477912903
Epoch 440, training loss: 0.7262097597122192 = 0.0580349937081337 + 0.1 * 6.6817474365234375
Epoch 440, val loss: 0.826901376247406
Epoch 450, training loss: 0.7206409573554993 = 0.05326826870441437 + 0.1 * 6.673727035522461
Epoch 450, val loss: 0.8368751406669617
Epoch 460, training loss: 0.715237557888031 = 0.04903716966509819 + 0.1 * 6.662003993988037
Epoch 460, val loss: 0.846781849861145
Epoch 470, training loss: 0.7112988233566284 = 0.04526980221271515 + 0.1 * 6.660290241241455
Epoch 470, val loss: 0.8565821647644043
Epoch 480, training loss: 0.7073964476585388 = 0.04190729930996895 + 0.1 * 6.654891014099121
Epoch 480, val loss: 0.8662484884262085
Epoch 490, training loss: 0.7032828330993652 = 0.03889664262533188 + 0.1 * 6.643861770629883
Epoch 490, val loss: 0.8757628202438354
Epoch 500, training loss: 0.7018095850944519 = 0.036198634654283524 + 0.1 * 6.656109809875488
Epoch 500, val loss: 0.8849626183509827
Epoch 510, training loss: 0.6985554099082947 = 0.03378338739275932 + 0.1 * 6.6477203369140625
Epoch 510, val loss: 0.8940014839172363
Epoch 520, training loss: 0.6950482726097107 = 0.03160476312041283 + 0.1 * 6.634435176849365
Epoch 520, val loss: 0.9028470516204834
Epoch 530, training loss: 0.6924012303352356 = 0.02963073179125786 + 0.1 * 6.627705097198486
Epoch 530, val loss: 0.9115501046180725
Epoch 540, training loss: 0.6907325983047485 = 0.027835294604301453 + 0.1 * 6.62897253036499
Epoch 540, val loss: 0.9200606942176819
Epoch 550, training loss: 0.6879098415374756 = 0.026203658431768417 + 0.1 * 6.617062091827393
Epoch 550, val loss: 0.9283605217933655
Epoch 560, training loss: 0.6879985928535461 = 0.024710657075047493 + 0.1 * 6.632879257202148
Epoch 560, val loss: 0.936492383480072
Epoch 570, training loss: 0.684087336063385 = 0.02334892377257347 + 0.1 * 6.607383728027344
Epoch 570, val loss: 0.9444603323936462
Epoch 580, training loss: 0.6822834014892578 = 0.022098582237958908 + 0.1 * 6.601848125457764
Epoch 580, val loss: 0.9522374868392944
Epoch 590, training loss: 0.680527925491333 = 0.02094714716076851 + 0.1 * 6.5958075523376465
Epoch 590, val loss: 0.9598212242126465
Epoch 600, training loss: 0.6795759797096252 = 0.019886374473571777 + 0.1 * 6.596895694732666
Epoch 600, val loss: 0.9673689007759094
Epoch 610, training loss: 0.6812506914138794 = 0.01890559494495392 + 0.1 * 6.623450756072998
Epoch 610, val loss: 0.9745990037918091
Epoch 620, training loss: 0.6768049001693726 = 0.01800282672047615 + 0.1 * 6.5880208015441895
Epoch 620, val loss: 0.9816955327987671
Epoch 630, training loss: 0.6753060817718506 = 0.017165714874863625 + 0.1 * 6.581403732299805
Epoch 630, val loss: 0.9886428117752075
Epoch 640, training loss: 0.6741871237754822 = 0.01638936810195446 + 0.1 * 6.577977657318115
Epoch 640, val loss: 0.9953951239585876
Epoch 650, training loss: 0.6725229024887085 = 0.015667608007788658 + 0.1 * 6.568552494049072
Epoch 650, val loss: 1.0021281242370605
Epoch 660, training loss: 0.6722586154937744 = 0.014993912540376186 + 0.1 * 6.5726470947265625
Epoch 660, val loss: 1.0086413621902466
Epoch 670, training loss: 0.6709140539169312 = 0.014365836046636105 + 0.1 * 6.565482139587402
Epoch 670, val loss: 1.0151101350784302
Epoch 680, training loss: 0.6711134910583496 = 0.013776838779449463 + 0.1 * 6.573366165161133
Epoch 680, val loss: 1.021397352218628
Epoch 690, training loss: 0.669426679611206 = 0.013227475807070732 + 0.1 * 6.561992168426514
Epoch 690, val loss: 1.0274624824523926
Epoch 700, training loss: 0.6677342057228088 = 0.012711245566606522 + 0.1 * 6.550229549407959
Epoch 700, val loss: 1.0335924625396729
Epoch 710, training loss: 0.6674568057060242 = 0.012226218357682228 + 0.1 * 6.552305698394775
Epoch 710, val loss: 1.0394459962844849
Epoch 720, training loss: 0.6670887470245361 = 0.011771759949624538 + 0.1 * 6.5531697273254395
Epoch 720, val loss: 1.0452793836593628
Epoch 730, training loss: 0.6656362414360046 = 0.011344539932906628 + 0.1 * 6.542916774749756
Epoch 730, val loss: 1.050873041152954
Epoch 740, training loss: 0.6648852825164795 = 0.01094177644699812 + 0.1 * 6.539434432983398
Epoch 740, val loss: 1.0564913749694824
Epoch 750, training loss: 0.6653454899787903 = 0.010560769587755203 + 0.1 * 6.547847270965576
Epoch 750, val loss: 1.061893105506897
Epoch 760, training loss: 0.6633349657058716 = 0.010201609693467617 + 0.1 * 6.5313334465026855
Epoch 760, val loss: 1.0672966241836548
Epoch 770, training loss: 0.6629170179367065 = 0.0098616573959589 + 0.1 * 6.530553817749023
Epoch 770, val loss: 1.0726059675216675
Epoch 780, training loss: 0.6624208688735962 = 0.009539250284433365 + 0.1 * 6.528816223144531
Epoch 780, val loss: 1.0778056383132935
Epoch 790, training loss: 0.6623553037643433 = 0.009233861230313778 + 0.1 * 6.531214714050293
Epoch 790, val loss: 1.0827864408493042
Epoch 800, training loss: 0.661381721496582 = 0.008945071138441563 + 0.1 * 6.52436637878418
Epoch 800, val loss: 1.0878764390945435
Epoch 810, training loss: 0.6625796556472778 = 0.008670289069414139 + 0.1 * 6.539093971252441
Epoch 810, val loss: 1.0928223133087158
Epoch 820, training loss: 0.6611956357955933 = 0.008409015834331512 + 0.1 * 6.527865886688232
Epoch 820, val loss: 1.0975195169448853
Epoch 830, training loss: 0.6600750088691711 = 0.008161348290741444 + 0.1 * 6.519136428833008
Epoch 830, val loss: 1.1022891998291016
Epoch 840, training loss: 0.6596749424934387 = 0.007925216108560562 + 0.1 * 6.5174970626831055
Epoch 840, val loss: 1.107016921043396
Epoch 850, training loss: 0.6598358750343323 = 0.007699940819293261 + 0.1 * 6.521358966827393
Epoch 850, val loss: 1.1115938425064087
Epoch 860, training loss: 0.6587572693824768 = 0.007485541515052319 + 0.1 * 6.512716770172119
Epoch 860, val loss: 1.1161266565322876
Epoch 870, training loss: 0.6577873826026917 = 0.007280659396201372 + 0.1 * 6.505067348480225
Epoch 870, val loss: 1.120466709136963
Epoch 880, training loss: 0.6580057740211487 = 0.007086132653057575 + 0.1 * 6.509195804595947
Epoch 880, val loss: 1.1248700618743896
Epoch 890, training loss: 0.656731903553009 = 0.006899491883814335 + 0.1 * 6.498324394226074
Epoch 890, val loss: 1.1292240619659424
Epoch 900, training loss: 0.6580447554588318 = 0.006720420438796282 + 0.1 * 6.513243198394775
Epoch 900, val loss: 1.133397102355957
Epoch 910, training loss: 0.6563140153884888 = 0.006549430545419455 + 0.1 * 6.497645854949951
Epoch 910, val loss: 1.1375575065612793
Epoch 920, training loss: 0.6558102965354919 = 0.006385818589478731 + 0.1 * 6.494244575500488
Epoch 920, val loss: 1.1417399644851685
Epoch 930, training loss: 0.6583212614059448 = 0.006228783633559942 + 0.1 * 6.5209245681762695
Epoch 930, val loss: 1.145797848701477
Epoch 940, training loss: 0.6562795639038086 = 0.0060777184553444386 + 0.1 * 6.502018451690674
Epoch 940, val loss: 1.149619698524475
Epoch 950, training loss: 0.6546999216079712 = 0.005934102926403284 + 0.1 * 6.4876580238342285
Epoch 950, val loss: 1.1536030769348145
Epoch 960, training loss: 0.6550320386886597 = 0.0057958015240728855 + 0.1 * 6.4923624992370605
Epoch 960, val loss: 1.1574780941009521
Epoch 970, training loss: 0.6545393466949463 = 0.005662505980581045 + 0.1 * 6.488768577575684
Epoch 970, val loss: 1.1612824201583862
Epoch 980, training loss: 0.6546041369438171 = 0.005534542724490166 + 0.1 * 6.490695953369141
Epoch 980, val loss: 1.1649954319000244
Epoch 990, training loss: 0.6536598801612854 = 0.005411472637206316 + 0.1 * 6.482484340667725
Epoch 990, val loss: 1.1687382459640503
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.8212931156158447 = 1.961612343788147 + 0.1 * 8.596807479858398
Epoch 0, val loss: 1.9653164148330688
Epoch 10, training loss: 2.8104257583618164 = 1.950758695602417 + 0.1 * 8.596670150756836
Epoch 10, val loss: 1.9549630880355835
Epoch 20, training loss: 2.7967960834503174 = 1.937195897102356 + 0.1 * 8.596002578735352
Epoch 20, val loss: 1.9418020248413086
Epoch 30, training loss: 2.777357339859009 = 1.918276071548462 + 0.1 * 8.590812683105469
Epoch 30, val loss: 1.923202395439148
Epoch 40, training loss: 2.745882987976074 = 1.8904337882995605 + 0.1 * 8.55449104309082
Epoch 40, val loss: 1.8959168195724487
Epoch 50, training loss: 2.687619924545288 = 1.852253794670105 + 0.1 * 8.35366153717041
Epoch 50, val loss: 1.8598413467407227
Epoch 60, training loss: 2.609236240386963 = 1.809974193572998 + 0.1 * 7.992620944976807
Epoch 60, val loss: 1.8217591047286987
Epoch 70, training loss: 2.540339469909668 = 1.7702314853668213 + 0.1 * 7.701080799102783
Epoch 70, val loss: 1.785834789276123
Epoch 80, training loss: 2.4788217544555664 = 1.7299457788467407 + 0.1 * 7.488758563995361
Epoch 80, val loss: 1.7490390539169312
Epoch 90, training loss: 2.4063310623168945 = 1.6793802976608276 + 0.1 * 7.2695088386535645
Epoch 90, val loss: 1.7033581733703613
Epoch 100, training loss: 2.328153133392334 = 1.612751841545105 + 0.1 * 7.1540141105651855
Epoch 100, val loss: 1.643314003944397
Epoch 110, training loss: 2.238546848297119 = 1.5323715209960938 + 0.1 * 7.061751842498779
Epoch 110, val loss: 1.574197769165039
Epoch 120, training loss: 2.1463212966918945 = 1.4482169151306152 + 0.1 * 6.981042861938477
Epoch 120, val loss: 1.5044952630996704
Epoch 130, training loss: 2.0589377880096436 = 1.3661845922470093 + 0.1 * 6.927532196044922
Epoch 130, val loss: 1.4385826587677002
Epoch 140, training loss: 1.9782891273498535 = 1.288833498954773 + 0.1 * 6.894556522369385
Epoch 140, val loss: 1.3740122318267822
Epoch 150, training loss: 1.8991572856903076 = 1.2133127450942993 + 0.1 * 6.85844612121582
Epoch 150, val loss: 1.3119863271713257
Epoch 160, training loss: 1.8201360702514648 = 1.1374847888946533 + 0.1 * 6.826512336730957
Epoch 160, val loss: 1.250945806503296
Epoch 170, training loss: 1.7422456741333008 = 1.0620371103286743 + 0.1 * 6.802085876464844
Epoch 170, val loss: 1.1918398141860962
Epoch 180, training loss: 1.665076494216919 = 0.9871171712875366 + 0.1 * 6.779592514038086
Epoch 180, val loss: 1.1345404386520386
Epoch 190, training loss: 1.5890158414840698 = 0.9128716588020325 + 0.1 * 6.761441707611084
Epoch 190, val loss: 1.0783185958862305
Epoch 200, training loss: 1.5165975093841553 = 0.8415894508361816 + 0.1 * 6.750080108642578
Epoch 200, val loss: 1.0242968797683716
Epoch 210, training loss: 1.447613000869751 = 0.7746692299842834 + 0.1 * 6.729437351226807
Epoch 210, val loss: 0.9739886522293091
Epoch 220, training loss: 1.3842647075653076 = 0.712663471698761 + 0.1 * 6.716012001037598
Epoch 220, val loss: 0.9278092384338379
Epoch 230, training loss: 1.3271596431732178 = 0.6563329696655273 + 0.1 * 6.708265781402588
Epoch 230, val loss: 0.8874384760856628
Epoch 240, training loss: 1.2738633155822754 = 0.6049180030822754 + 0.1 * 6.689453601837158
Epoch 240, val loss: 0.8527249097824097
Epoch 250, training loss: 1.2254575490951538 = 0.5576050877571106 + 0.1 * 6.678524494171143
Epoch 250, val loss: 0.823284387588501
Epoch 260, training loss: 1.1824584007263184 = 0.5141880512237549 + 0.1 * 6.682703495025635
Epoch 260, val loss: 0.7990545630455017
Epoch 270, training loss: 1.1406364440917969 = 0.47422900795936584 + 0.1 * 6.664074420928955
Epoch 270, val loss: 0.7795715928077698
Epoch 280, training loss: 1.1021695137023926 = 0.43679046630859375 + 0.1 * 6.653790473937988
Epoch 280, val loss: 0.7636699080467224
Epoch 290, training loss: 1.066247582435608 = 0.4014032185077667 + 0.1 * 6.648443222045898
Epoch 290, val loss: 0.7508805990219116
Epoch 300, training loss: 1.0321986675262451 = 0.367728054523468 + 0.1 * 6.644705772399902
Epoch 300, val loss: 0.740341305732727
Epoch 310, training loss: 0.9996166229248047 = 0.33573290705680847 + 0.1 * 6.638836860656738
Epoch 310, val loss: 0.7320786714553833
Epoch 320, training loss: 0.9685959815979004 = 0.30563876032829285 + 0.1 * 6.62957239151001
Epoch 320, val loss: 0.7260555624961853
Epoch 330, training loss: 0.9409099817276001 = 0.2779666483402252 + 0.1 * 6.629432678222656
Epoch 330, val loss: 0.7224500179290771
Epoch 340, training loss: 0.914534330368042 = 0.25266948342323303 + 0.1 * 6.618648529052734
Epoch 340, val loss: 0.7213336825370789
Epoch 350, training loss: 0.8916207551956177 = 0.22975197434425354 + 0.1 * 6.618687629699707
Epoch 350, val loss: 0.7226248979568481
Epoch 360, training loss: 0.8703731298446655 = 0.2093144804239273 + 0.1 * 6.610586643218994
Epoch 360, val loss: 0.7259887456893921
Epoch 370, training loss: 0.8513345718383789 = 0.19112925231456757 + 0.1 * 6.602052688598633
Epoch 370, val loss: 0.7310253977775574
Epoch 380, training loss: 0.8344277143478394 = 0.17486104369163513 + 0.1 * 6.595666885375977
Epoch 380, val loss: 0.7375631928443909
Epoch 390, training loss: 0.8206662535667419 = 0.16031122207641602 + 0.1 * 6.603549957275391
Epoch 390, val loss: 0.7452013492584229
Epoch 400, training loss: 0.8059124946594238 = 0.1473730206489563 + 0.1 * 6.585394859313965
Epoch 400, val loss: 0.7536671757698059
Epoch 410, training loss: 0.7944133281707764 = 0.13576136529445648 + 0.1 * 6.586519241333008
Epoch 410, val loss: 0.7628731727600098
Epoch 420, training loss: 0.7830287218093872 = 0.12530964612960815 + 0.1 * 6.57719087600708
Epoch 420, val loss: 0.7725986242294312
Epoch 430, training loss: 0.7742496728897095 = 0.11586014926433563 + 0.1 * 6.583894729614258
Epoch 430, val loss: 0.7827743291854858
Epoch 440, training loss: 0.7655664682388306 = 0.10731522738933563 + 0.1 * 6.582512378692627
Epoch 440, val loss: 0.7932578921318054
Epoch 450, training loss: 0.7565672993659973 = 0.09955678880214691 + 0.1 * 6.570104598999023
Epoch 450, val loss: 0.8039547204971313
Epoch 460, training loss: 0.7485677003860474 = 0.09248387068510056 + 0.1 * 6.560838222503662
Epoch 460, val loss: 0.814880907535553
Epoch 470, training loss: 0.7423546314239502 = 0.0860205665230751 + 0.1 * 6.563340187072754
Epoch 470, val loss: 0.825930118560791
Epoch 480, training loss: 0.7351989150047302 = 0.08009974658489227 + 0.1 * 6.550991535186768
Epoch 480, val loss: 0.8370961546897888
Epoch 490, training loss: 0.7305648922920227 = 0.074656181037426 + 0.1 * 6.559086799621582
Epoch 490, val loss: 0.8483719825744629
Epoch 500, training loss: 0.7243034243583679 = 0.06966686993837357 + 0.1 * 6.546365737915039
Epoch 500, val loss: 0.8595763444900513
Epoch 510, training loss: 0.7190568447113037 = 0.06507764756679535 + 0.1 * 6.539791584014893
Epoch 510, val loss: 0.8708338737487793
Epoch 520, training loss: 0.7146898508071899 = 0.06085231900215149 + 0.1 * 6.538374900817871
Epoch 520, val loss: 0.8820457458496094
Epoch 530, training loss: 0.7102790474891663 = 0.056957993656396866 + 0.1 * 6.533210277557373
Epoch 530, val loss: 0.8932316303253174
Epoch 540, training loss: 0.7067452669143677 = 0.05336685851216316 + 0.1 * 6.53378438949585
Epoch 540, val loss: 0.9043652415275574
Epoch 550, training loss: 0.7033575177192688 = 0.05006011575460434 + 0.1 * 6.5329742431640625
Epoch 550, val loss: 0.9153916835784912
Epoch 560, training loss: 0.7001813650131226 = 0.047006066888570786 + 0.1 * 6.531753063201904
Epoch 560, val loss: 0.9263560771942139
Epoch 570, training loss: 0.6971740126609802 = 0.044186051934957504 + 0.1 * 6.529879570007324
Epoch 570, val loss: 0.9372149705886841
Epoch 580, training loss: 0.6939279437065125 = 0.04157997667789459 + 0.1 * 6.52347993850708
Epoch 580, val loss: 0.9479685425758362
Epoch 590, training loss: 0.6918293237686157 = 0.03917227312922478 + 0.1 * 6.5265703201293945
Epoch 590, val loss: 0.9585641622543335
Epoch 600, training loss: 0.6883684992790222 = 0.03695022314786911 + 0.1 * 6.5141825675964355
Epoch 600, val loss: 0.9689927697181702
Epoch 610, training loss: 0.6859710812568665 = 0.03489477559924126 + 0.1 * 6.510763168334961
Epoch 610, val loss: 0.9792695045471191
Epoch 620, training loss: 0.6845062375068665 = 0.032989416271448135 + 0.1 * 6.515167713165283
Epoch 620, val loss: 0.989434003829956
Epoch 630, training loss: 0.6817063093185425 = 0.03122539445757866 + 0.1 * 6.5048089027404785
Epoch 630, val loss: 0.9994261860847473
Epoch 640, training loss: 0.6820884943008423 = 0.02959100529551506 + 0.1 * 6.524974822998047
Epoch 640, val loss: 1.0092675685882568
Epoch 650, training loss: 0.6784167289733887 = 0.028073348104953766 + 0.1 * 6.503433704376221
Epoch 650, val loss: 1.0189223289489746
Epoch 660, training loss: 0.6773342490196228 = 0.026662778109312057 + 0.1 * 6.506714820861816
Epoch 660, val loss: 1.0284712314605713
Epoch 670, training loss: 0.6747046709060669 = 0.025351205840706825 + 0.1 * 6.493534564971924
Epoch 670, val loss: 1.037864327430725
Epoch 680, training loss: 0.6730415225028992 = 0.024130143225193024 + 0.1 * 6.4891133308410645
Epoch 680, val loss: 1.047115445137024
Epoch 690, training loss: 0.6737542748451233 = 0.02299090288579464 + 0.1 * 6.507633686065674
Epoch 690, val loss: 1.0562163591384888
Epoch 700, training loss: 0.6717464327812195 = 0.021929211914539337 + 0.1 * 6.498172283172607
Epoch 700, val loss: 1.0650968551635742
Epoch 710, training loss: 0.6706567406654358 = 0.020938880741596222 + 0.1 * 6.497178554534912
Epoch 710, val loss: 1.073837399482727
Epoch 720, training loss: 0.6684752106666565 = 0.020011799409985542 + 0.1 * 6.484633922576904
Epoch 720, val loss: 1.0824320316314697
Epoch 730, training loss: 0.6682195663452148 = 0.019144142046570778 + 0.1 * 6.490754127502441
Epoch 730, val loss: 1.0908852815628052
Epoch 740, training loss: 0.666406512260437 = 0.018332520499825478 + 0.1 * 6.480739593505859
Epoch 740, val loss: 1.099107027053833
Epoch 750, training loss: 0.6652597188949585 = 0.01757260411977768 + 0.1 * 6.476871013641357
Epoch 750, val loss: 1.1071974039077759
Epoch 760, training loss: 0.6638315916061401 = 0.016858551651239395 + 0.1 * 6.469729900360107
Epoch 760, val loss: 1.115190029144287
Epoch 770, training loss: 0.6650112867355347 = 0.016186758875846863 + 0.1 * 6.488245010375977
Epoch 770, val loss: 1.1230531930923462
Epoch 780, training loss: 0.6634676456451416 = 0.015556412748992443 + 0.1 * 6.47911262512207
Epoch 780, val loss: 1.1306860446929932
Epoch 790, training loss: 0.6619613170623779 = 0.014962875284254551 + 0.1 * 6.469984531402588
Epoch 790, val loss: 1.1382213830947876
Epoch 800, training loss: 0.6631247401237488 = 0.01440310850739479 + 0.1 * 6.487215995788574
Epoch 800, val loss: 1.1456284523010254
Epoch 810, training loss: 0.6611618995666504 = 0.013876393437385559 + 0.1 * 6.472855091094971
Epoch 810, val loss: 1.152841567993164
Epoch 820, training loss: 0.6594079732894897 = 0.013379479758441448 + 0.1 * 6.460285186767578
Epoch 820, val loss: 1.1599327325820923
Epoch 830, training loss: 0.6589789390563965 = 0.012908292934298515 + 0.1 * 6.46070671081543
Epoch 830, val loss: 1.166943907737732
Epoch 840, training loss: 0.6580438613891602 = 0.012463082559406757 + 0.1 * 6.455807685852051
Epoch 840, val loss: 1.1737968921661377
Epoch 850, training loss: 0.6573240160942078 = 0.012042107060551643 + 0.1 * 6.452818870544434
Epoch 850, val loss: 1.180499792098999
Epoch 860, training loss: 0.658367395401001 = 0.011642794124782085 + 0.1 * 6.467245578765869
Epoch 860, val loss: 1.1871370077133179
Epoch 870, training loss: 0.6581007242202759 = 0.011263791471719742 + 0.1 * 6.468369007110596
Epoch 870, val loss: 1.193613052368164
Epoch 880, training loss: 0.6570814847946167 = 0.010904666036367416 + 0.1 * 6.46176815032959
Epoch 880, val loss: 1.1999691724777222
Epoch 890, training loss: 0.6559004783630371 = 0.010564323514699936 + 0.1 * 6.453361511230469
Epoch 890, val loss: 1.2061599493026733
Epoch 900, training loss: 0.6559782028198242 = 0.010239736177027225 + 0.1 * 6.4573845863342285
Epoch 900, val loss: 1.2123280763626099
Epoch 910, training loss: 0.6553342342376709 = 0.009931221604347229 + 0.1 * 6.4540300369262695
Epoch 910, val loss: 1.2183765172958374
Epoch 920, training loss: 0.6548119187355042 = 0.009637466631829739 + 0.1 * 6.451744556427002
Epoch 920, val loss: 1.2243071794509888
Epoch 930, training loss: 0.654133141040802 = 0.009357970207929611 + 0.1 * 6.447751522064209
Epoch 930, val loss: 1.2301188707351685
Epoch 940, training loss: 0.6539594531059265 = 0.009090685285627842 + 0.1 * 6.448687553405762
Epoch 940, val loss: 1.2358747720718384
Epoch 950, training loss: 0.6524620056152344 = 0.00883581954985857 + 0.1 * 6.436262130737305
Epoch 950, val loss: 1.2415196895599365
Epoch 960, training loss: 0.6534980535507202 = 0.008592303842306137 + 0.1 * 6.449057102203369
Epoch 960, val loss: 1.2470932006835938
Epoch 970, training loss: 0.6528062224388123 = 0.008359155617654324 + 0.1 * 6.4444708824157715
Epoch 970, val loss: 1.2525798082351685
Epoch 980, training loss: 0.6523841023445129 = 0.008136351592838764 + 0.1 * 6.442477703094482
Epoch 980, val loss: 1.2579874992370605
Epoch 990, training loss: 0.6537713408470154 = 0.007923253811895847 + 0.1 * 6.4584808349609375
Epoch 990, val loss: 1.2632827758789062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.838165524512388
The final CL Acc:0.80988, 0.00698, The final GNN Acc:0.83834, 0.00108
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10542])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.803680181503296 = 1.9439942836761475 + 0.1 * 8.596858024597168
Epoch 0, val loss: 1.9397903680801392
Epoch 10, training loss: 2.7937686443328857 = 1.9340903759002686 + 0.1 * 8.596781730651855
Epoch 10, val loss: 1.929742455482483
Epoch 20, training loss: 2.781508684158325 = 1.9218610525131226 + 0.1 * 8.596476554870605
Epoch 20, val loss: 1.9172910451889038
Epoch 30, training loss: 2.764153242111206 = 1.9047315120697021 + 0.1 * 8.594217300415039
Epoch 30, val loss: 1.9000329971313477
Epoch 40, training loss: 2.737311363220215 = 1.879603624343872 + 0.1 * 8.577075958251953
Epoch 40, val loss: 1.8753492832183838
Epoch 50, training loss: 2.69392466545105 = 1.8450831174850464 + 0.1 * 8.488414764404297
Epoch 50, val loss: 1.843335747718811
Epoch 60, training loss: 2.637754440307617 = 1.8075052499771118 + 0.1 * 8.30249309539795
Epoch 60, val loss: 1.8119394779205322
Epoch 70, training loss: 2.5747358798980713 = 1.7731437683105469 + 0.1 * 8.015921592712402
Epoch 70, val loss: 1.7839823961257935
Epoch 80, training loss: 2.495727062225342 = 1.7359771728515625 + 0.1 * 7.597497463226318
Epoch 80, val loss: 1.751434087753296
Epoch 90, training loss: 2.4239563941955566 = 1.689919352531433 + 0.1 * 7.340371608734131
Epoch 90, val loss: 1.7114802598953247
Epoch 100, training loss: 2.350090980529785 = 1.6298707723617554 + 0.1 * 7.202201843261719
Epoch 100, val loss: 1.6607340574264526
Epoch 110, training loss: 2.2652487754821777 = 1.5538692474365234 + 0.1 * 7.113796234130859
Epoch 110, val loss: 1.5959320068359375
Epoch 120, training loss: 2.171177625656128 = 1.4646632671356201 + 0.1 * 7.065144062042236
Epoch 120, val loss: 1.5223965644836426
Epoch 130, training loss: 2.0728201866149902 = 1.369449257850647 + 0.1 * 7.033708572387695
Epoch 130, val loss: 1.4450325965881348
Epoch 140, training loss: 1.976015567779541 = 1.274905800819397 + 0.1 * 7.011098384857178
Epoch 140, val loss: 1.3687471151351929
Epoch 150, training loss: 1.882420539855957 = 1.1835789680480957 + 0.1 * 6.988415241241455
Epoch 150, val loss: 1.2955169677734375
Epoch 160, training loss: 1.7942030429840088 = 1.0975724458694458 + 0.1 * 6.966306209564209
Epoch 160, val loss: 1.228123426437378
Epoch 170, training loss: 1.7140218019485474 = 1.019303321838379 + 0.1 * 6.9471845626831055
Epoch 170, val loss: 1.1687188148498535
Epoch 180, training loss: 1.6402831077575684 = 0.9470754265785217 + 0.1 * 6.932077407836914
Epoch 180, val loss: 1.1155201196670532
Epoch 190, training loss: 1.5714730024337769 = 0.8800906538963318 + 0.1 * 6.91382360458374
Epoch 190, val loss: 1.067813515663147
Epoch 200, training loss: 1.5069379806518555 = 0.8169100880622864 + 0.1 * 6.900278091430664
Epoch 200, val loss: 1.0239046812057495
Epoch 210, training loss: 1.4468557834625244 = 0.7586647868156433 + 0.1 * 6.88191032409668
Epoch 210, val loss: 0.9851171374320984
Epoch 220, training loss: 1.392181396484375 = 0.7057760953903198 + 0.1 * 6.864052772521973
Epoch 220, val loss: 0.9524388909339905
Epoch 230, training loss: 1.3444181680679321 = 0.6579264998435974 + 0.1 * 6.864916801452637
Epoch 230, val loss: 0.9261694550514221
Epoch 240, training loss: 1.2993226051330566 = 0.6152330040931702 + 0.1 * 6.840895175933838
Epoch 240, val loss: 0.906384289264679
Epoch 250, training loss: 1.2585554122924805 = 0.5765021443367004 + 0.1 * 6.820531845092773
Epoch 250, val loss: 0.891898512840271
Epoch 260, training loss: 1.2216817140579224 = 0.5413187742233276 + 0.1 * 6.803629398345947
Epoch 260, val loss: 0.8819922208786011
Epoch 270, training loss: 1.1877022981643677 = 0.5090080499649048 + 0.1 * 6.786942481994629
Epoch 270, val loss: 0.8753830194473267
Epoch 280, training loss: 1.1566708087921143 = 0.47866082191467285 + 0.1 * 6.780100345611572
Epoch 280, val loss: 0.871293842792511
Epoch 290, training loss: 1.1266453266143799 = 0.44970759749412537 + 0.1 * 6.769376754760742
Epoch 290, val loss: 0.8693202137947083
Epoch 300, training loss: 1.0986785888671875 = 0.4219563603401184 + 0.1 * 6.767221927642822
Epoch 300, val loss: 0.8692082762718201
Epoch 310, training loss: 1.0694268941879272 = 0.3952292203903198 + 0.1 * 6.741976737976074
Epoch 310, val loss: 0.8707126975059509
Epoch 320, training loss: 1.0423887968063354 = 0.36918672919273376 + 0.1 * 6.732020854949951
Epoch 320, val loss: 0.8740257024765015
Epoch 330, training loss: 1.0166603326797485 = 0.34391579031944275 + 0.1 * 6.727445125579834
Epoch 330, val loss: 0.8792300820350647
Epoch 340, training loss: 0.9937713146209717 = 0.3195915222167969 + 0.1 * 6.741797924041748
Epoch 340, val loss: 0.8863174915313721
Epoch 350, training loss: 0.9668110609054565 = 0.29627397656440735 + 0.1 * 6.705370903015137
Epoch 350, val loss: 0.8953045010566711
Epoch 360, training loss: 0.945107638835907 = 0.27387285232543945 + 0.1 * 6.712347984313965
Epoch 360, val loss: 0.9061189293861389
Epoch 370, training loss: 0.9219520688056946 = 0.2524665594100952 + 0.1 * 6.694855213165283
Epoch 370, val loss: 0.9187746644020081
Epoch 380, training loss: 0.9013031125068665 = 0.23206929862499237 + 0.1 * 6.692337989807129
Epoch 380, val loss: 0.9331498146057129
Epoch 390, training loss: 0.8823769092559814 = 0.21289442479610443 + 0.1 * 6.694824695587158
Epoch 390, val loss: 0.949263870716095
Epoch 400, training loss: 0.8628906011581421 = 0.1950032263994217 + 0.1 * 6.6788740158081055
Epoch 400, val loss: 0.9672281742095947
Epoch 410, training loss: 0.8451129198074341 = 0.1782630831003189 + 0.1 * 6.668498516082764
Epoch 410, val loss: 0.9870792031288147
Epoch 420, training loss: 0.8285461664199829 = 0.1626105010509491 + 0.1 * 6.659357070922852
Epoch 420, val loss: 1.0085804462432861
Epoch 430, training loss: 0.814759373664856 = 0.14807067811489105 + 0.1 * 6.666886806488037
Epoch 430, val loss: 1.031426191329956
Epoch 440, training loss: 0.8012820482254028 = 0.1346846967935562 + 0.1 * 6.665973663330078
Epoch 440, val loss: 1.0553257465362549
Epoch 450, training loss: 0.7871899604797363 = 0.12242134660482407 + 0.1 * 6.64768648147583
Epoch 450, val loss: 1.080321192741394
Epoch 460, training loss: 0.7757403254508972 = 0.11123111099004745 + 0.1 * 6.645092010498047
Epoch 460, val loss: 1.1058818101882935
Epoch 470, training loss: 0.7642619013786316 = 0.10110140591859818 + 0.1 * 6.63160514831543
Epoch 470, val loss: 1.1320096254348755
Epoch 480, training loss: 0.7566027641296387 = 0.09197989106178284 + 0.1 * 6.646228790283203
Epoch 480, val loss: 1.158250093460083
Epoch 490, training loss: 0.7485471963882446 = 0.08383705466985703 + 0.1 * 6.647101402282715
Epoch 490, val loss: 1.1844028234481812
Epoch 500, training loss: 0.7390536665916443 = 0.07657808810472488 + 0.1 * 6.624755859375
Epoch 500, val loss: 1.2103846073150635
Epoch 510, training loss: 0.7317888140678406 = 0.07010055333375931 + 0.1 * 6.61688232421875
Epoch 510, val loss: 1.236025094985962
Epoch 520, training loss: 0.7253203988075256 = 0.06431927531957626 + 0.1 * 6.610011100769043
Epoch 520, val loss: 1.2613343000411987
Epoch 530, training loss: 0.7204515933990479 = 0.05916190147399902 + 0.1 * 6.612896919250488
Epoch 530, val loss: 1.2859686613082886
Epoch 540, training loss: 0.7156914472579956 = 0.05456611514091492 + 0.1 * 6.611252784729004
Epoch 540, val loss: 1.310062050819397
Epoch 550, training loss: 0.7113592624664307 = 0.05045635253190994 + 0.1 * 6.609029293060303
Epoch 550, val loss: 1.3335751295089722
Epoch 560, training loss: 0.7076839804649353 = 0.04677737131714821 + 0.1 * 6.609066009521484
Epoch 560, val loss: 1.3564878702163696
Epoch 570, training loss: 0.7031010985374451 = 0.04347699508070946 + 0.1 * 6.596240997314453
Epoch 570, val loss: 1.3786602020263672
Epoch 580, training loss: 0.6996412873268127 = 0.040503304451704025 + 0.1 * 6.591379642486572
Epoch 580, val loss: 1.4003430604934692
Epoch 590, training loss: 0.6963104605674744 = 0.03782149404287338 + 0.1 * 6.584889888763428
Epoch 590, val loss: 1.4214357137680054
Epoch 600, training loss: 0.6938472986221313 = 0.0353979729115963 + 0.1 * 6.584493637084961
Epoch 600, val loss: 1.4418796300888062
Epoch 610, training loss: 0.6912010908126831 = 0.0332023985683918 + 0.1 * 6.579986572265625
Epoch 610, val loss: 1.4616739749908447
Epoch 620, training loss: 0.689555287361145 = 0.031211091205477715 + 0.1 * 6.583441734313965
Epoch 620, val loss: 1.480818271636963
Epoch 630, training loss: 0.6867612600326538 = 0.02939530275762081 + 0.1 * 6.573659420013428
Epoch 630, val loss: 1.4994409084320068
Epoch 640, training loss: 0.6850374937057495 = 0.027735063806176186 + 0.1 * 6.573024272918701
Epoch 640, val loss: 1.5175033807754517
Epoch 650, training loss: 0.6832080483436584 = 0.026214739307761192 + 0.1 * 6.56993293762207
Epoch 650, val loss: 1.5350757837295532
Epoch 660, training loss: 0.6817722320556641 = 0.024820752441883087 + 0.1 * 6.569514751434326
Epoch 660, val loss: 1.5521036386489868
Epoch 670, training loss: 0.6795760989189148 = 0.02354096807539463 + 0.1 * 6.560351371765137
Epoch 670, val loss: 1.5687397718429565
Epoch 680, training loss: 0.6784419417381287 = 0.022360732778906822 + 0.1 * 6.560811996459961
Epoch 680, val loss: 1.5849409103393555
Epoch 690, training loss: 0.6765788793563843 = 0.021270383149385452 + 0.1 * 6.553084850311279
Epoch 690, val loss: 1.6006437540054321
Epoch 700, training loss: 0.6759912371635437 = 0.020262692123651505 + 0.1 * 6.557285308837891
Epoch 700, val loss: 1.6158225536346436
Epoch 710, training loss: 0.674725353717804 = 0.019330408424139023 + 0.1 * 6.553949356079102
Epoch 710, val loss: 1.6306636333465576
Epoch 720, training loss: 0.673458456993103 = 0.01846271939575672 + 0.1 * 6.549957275390625
Epoch 720, val loss: 1.6451364755630493
Epoch 730, training loss: 0.6721712946891785 = 0.01765499636530876 + 0.1 * 6.545162677764893
Epoch 730, val loss: 1.6592068672180176
Epoch 740, training loss: 0.6725287437438965 = 0.016904372721910477 + 0.1 * 6.556243896484375
Epoch 740, val loss: 1.672922968864441
Epoch 750, training loss: 0.6697891354560852 = 0.01620354875922203 + 0.1 * 6.535856246948242
Epoch 750, val loss: 1.686213731765747
Epoch 760, training loss: 0.6690275073051453 = 0.015547695569694042 + 0.1 * 6.5347981452941895
Epoch 760, val loss: 1.6993118524551392
Epoch 770, training loss: 0.6688166856765747 = 0.014933645725250244 + 0.1 * 6.538830280303955
Epoch 770, val loss: 1.7121306657791138
Epoch 780, training loss: 0.6668680310249329 = 0.014357101172208786 + 0.1 * 6.52510929107666
Epoch 780, val loss: 1.72452712059021
Epoch 790, training loss: 0.667758584022522 = 0.013817459344863892 + 0.1 * 6.539411544799805
Epoch 790, val loss: 1.736743688583374
Epoch 800, training loss: 0.6665549874305725 = 0.013309779576957226 + 0.1 * 6.53245210647583
Epoch 800, val loss: 1.748537302017212
Epoch 810, training loss: 0.6650822162628174 = 0.012832441367208958 + 0.1 * 6.522497653961182
Epoch 810, val loss: 1.7600741386413574
Epoch 820, training loss: 0.6646401882171631 = 0.012383040972054005 + 0.1 * 6.522571086883545
Epoch 820, val loss: 1.7714288234710693
Epoch 830, training loss: 0.6639606356620789 = 0.011957904323935509 + 0.1 * 6.520027160644531
Epoch 830, val loss: 1.7825547456741333
Epoch 840, training loss: 0.6635754704475403 = 0.011556042358279228 + 0.1 * 6.520194053649902
Epoch 840, val loss: 1.7933871746063232
Epoch 850, training loss: 0.6628878116607666 = 0.011176426894962788 + 0.1 * 6.51711368560791
Epoch 850, val loss: 1.8039498329162598
Epoch 860, training loss: 0.6619088649749756 = 0.01081719808280468 + 0.1 * 6.510916709899902
Epoch 860, val loss: 1.8143290281295776
Epoch 870, training loss: 0.6621176600456238 = 0.010475506074726582 + 0.1 * 6.516421318054199
Epoch 870, val loss: 1.8244174718856812
Epoch 880, training loss: 0.6628797650337219 = 0.010151988826692104 + 0.1 * 6.52727746963501
Epoch 880, val loss: 1.8343042135238647
Epoch 890, training loss: 0.6605958938598633 = 0.009844496846199036 + 0.1 * 6.507513999938965
Epoch 890, val loss: 1.8439358472824097
Epoch 900, training loss: 0.6601982116699219 = 0.009552985429763794 + 0.1 * 6.5064520835876465
Epoch 900, val loss: 1.853479266166687
Epoch 910, training loss: 0.6599481105804443 = 0.00927476305514574 + 0.1 * 6.506733417510986
Epoch 910, val loss: 1.862653374671936
Epoch 920, training loss: 0.659817636013031 = 0.009010860696434975 + 0.1 * 6.508067607879639
Epoch 920, val loss: 1.8718191385269165
Epoch 930, training loss: 0.6595782041549683 = 0.008758609183132648 + 0.1 * 6.508195877075195
Epoch 930, val loss: 1.8807798624038696
Epoch 940, training loss: 0.6576184630393982 = 0.008517765440046787 + 0.1 * 6.491007328033447
Epoch 940, val loss: 1.8895691633224487
Epoch 950, training loss: 0.6591114401817322 = 0.00828749593347311 + 0.1 * 6.508239269256592
Epoch 950, val loss: 1.8981850147247314
Epoch 960, training loss: 0.6571251153945923 = 0.008068153634667397 + 0.1 * 6.490569591522217
Epoch 960, val loss: 1.9065784215927124
Epoch 970, training loss: 0.6572523713111877 = 0.007858571596443653 + 0.1 * 6.493937969207764
Epoch 970, val loss: 1.9149008989334106
Epoch 980, training loss: 0.6569449305534363 = 0.007657860871404409 + 0.1 * 6.492870330810547
Epoch 980, val loss: 1.9230793714523315
Epoch 990, training loss: 0.6560631394386292 = 0.007465751376003027 + 0.1 * 6.485973358154297
Epoch 990, val loss: 1.9310551881790161
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 2.8021304607391357 = 1.9424457550048828 + 0.1 * 8.596846580505371
Epoch 0, val loss: 1.9340507984161377
Epoch 10, training loss: 2.7918498516082764 = 1.9321728944778442 + 0.1 * 8.596769332885742
Epoch 10, val loss: 1.9232152700424194
Epoch 20, training loss: 2.7786550521850586 = 1.9190123081207275 + 0.1 * 8.596426963806152
Epoch 20, val loss: 1.9090384244918823
Epoch 30, training loss: 2.759366512298584 = 1.9000086784362793 + 0.1 * 8.593578338623047
Epoch 30, val loss: 1.8887155055999756
Epoch 40, training loss: 2.7293288707733154 = 1.8721786737442017 + 0.1 * 8.571501731872559
Epoch 40, val loss: 1.859731912612915
Epoch 50, training loss: 2.6819117069244385 = 1.8362443447113037 + 0.1 * 8.456673622131348
Epoch 50, val loss: 1.825316309928894
Epoch 60, training loss: 2.6301989555358887 = 1.8018194437026978 + 0.1 * 8.283795356750488
Epoch 60, val loss: 1.7972451448440552
Epoch 70, training loss: 2.5803709030151367 = 1.770519495010376 + 0.1 * 8.098512649536133
Epoch 70, val loss: 1.7735856771469116
Epoch 80, training loss: 2.504565477371216 = 1.7321075201034546 + 0.1 * 7.724578857421875
Epoch 80, val loss: 1.7425283193588257
Epoch 90, training loss: 2.4269356727600098 = 1.6851497888565063 + 0.1 * 7.417857646942139
Epoch 90, val loss: 1.7034364938735962
Epoch 100, training loss: 2.3482730388641357 = 1.6248170137405396 + 0.1 * 7.234560966491699
Epoch 100, val loss: 1.6525830030441284
Epoch 110, training loss: 2.266265630722046 = 1.553218126296997 + 0.1 * 7.1304755210876465
Epoch 110, val loss: 1.594571828842163
Epoch 120, training loss: 2.1852574348449707 = 1.479434847831726 + 0.1 * 7.058227062225342
Epoch 120, val loss: 1.53963303565979
Epoch 130, training loss: 2.109583854675293 = 1.4084135293960571 + 0.1 * 7.011702537536621
Epoch 130, val loss: 1.4867491722106934
Epoch 140, training loss: 2.0373711585998535 = 1.3395909070968628 + 0.1 * 6.977802753448486
Epoch 140, val loss: 1.4376622438430786
Epoch 150, training loss: 1.9643887281417847 = 1.2694873809814453 + 0.1 * 6.9490132331848145
Epoch 150, val loss: 1.3885806798934937
Epoch 160, training loss: 1.8890026807785034 = 1.1963376998901367 + 0.1 * 6.926649570465088
Epoch 160, val loss: 1.3369616270065308
Epoch 170, training loss: 1.8126466274261475 = 1.1218748092651367 + 0.1 * 6.907718181610107
Epoch 170, val loss: 1.2850446701049805
Epoch 180, training loss: 1.7363756895065308 = 1.0467208623886108 + 0.1 * 6.896548271179199
Epoch 180, val loss: 1.2329949140548706
Epoch 190, training loss: 1.6609725952148438 = 0.9732549786567688 + 0.1 * 6.877175331115723
Epoch 190, val loss: 1.182417869567871
Epoch 200, training loss: 1.589662790298462 = 0.9030885696411133 + 0.1 * 6.86574125289917
Epoch 200, val loss: 1.1345911026000977
Epoch 210, training loss: 1.5238745212554932 = 0.8386449217796326 + 0.1 * 6.852295875549316
Epoch 210, val loss: 1.0913467407226562
Epoch 220, training loss: 1.4652901887893677 = 0.7813056707382202 + 0.1 * 6.839845180511475
Epoch 220, val loss: 1.054234504699707
Epoch 230, training loss: 1.416109561920166 = 0.7328106760978699 + 0.1 * 6.832988739013672
Epoch 230, val loss: 1.0243172645568848
Epoch 240, training loss: 1.3737897872924805 = 0.6924059987068176 + 0.1 * 6.813837051391602
Epoch 240, val loss: 1.0012120008468628
Epoch 250, training loss: 1.3375303745269775 = 0.6578227281570435 + 0.1 * 6.79707670211792
Epoch 250, val loss: 0.9827809929847717
Epoch 260, training loss: 1.306857943534851 = 0.6273232102394104 + 0.1 * 6.795347213745117
Epoch 260, val loss: 0.9672900438308716
Epoch 270, training loss: 1.2764220237731934 = 0.5991225242614746 + 0.1 * 6.772994518280029
Epoch 270, val loss: 0.9537479877471924
Epoch 280, training loss: 1.2496237754821777 = 0.5715787410736084 + 0.1 * 6.780449867248535
Epoch 280, val loss: 0.9407252073287964
Epoch 290, training loss: 1.2192435264587402 = 0.544092059135437 + 0.1 * 6.751513957977295
Epoch 290, val loss: 0.9278865456581116
Epoch 300, training loss: 1.1902296543121338 = 0.5163105726242065 + 0.1 * 6.73919153213501
Epoch 300, val loss: 0.9153059124946594
Epoch 310, training loss: 1.1621780395507812 = 0.4887100160121918 + 0.1 * 6.734680652618408
Epoch 310, val loss: 0.9033571481704712
Epoch 320, training loss: 1.1333945989608765 = 0.461586594581604 + 0.1 * 6.718080043792725
Epoch 320, val loss: 0.8930804133415222
Epoch 330, training loss: 1.1059534549713135 = 0.4349747598171234 + 0.1 * 6.709786415100098
Epoch 330, val loss: 0.8840569853782654
Epoch 340, training loss: 1.0791425704956055 = 0.4088280200958252 + 0.1 * 6.7031450271606445
Epoch 340, val loss: 0.8766472339630127
Epoch 350, training loss: 1.0526012182235718 = 0.38289305567741394 + 0.1 * 6.697081089019775
Epoch 350, val loss: 0.8701595664024353
Epoch 360, training loss: 1.0259901285171509 = 0.35716986656188965 + 0.1 * 6.688202381134033
Epoch 360, val loss: 0.8646530508995056
Epoch 370, training loss: 1.0006484985351562 = 0.33161190152168274 + 0.1 * 6.690365314483643
Epoch 370, val loss: 0.8601070046424866
Epoch 380, training loss: 0.9744123220443726 = 0.30648842453956604 + 0.1 * 6.679238796234131
Epoch 380, val loss: 0.8563450574874878
Epoch 390, training loss: 0.9491285085678101 = 0.28208228945732117 + 0.1 * 6.670462608337402
Epoch 390, val loss: 0.8538733720779419
Epoch 400, training loss: 0.926196813583374 = 0.25860920548439026 + 0.1 * 6.675876140594482
Epoch 400, val loss: 0.8524993062019348
Epoch 410, training loss: 0.9031707048416138 = 0.23625366389751434 + 0.1 * 6.669170379638672
Epoch 410, val loss: 0.8526173830032349
Epoch 420, training loss: 0.8809925317764282 = 0.2151421755552292 + 0.1 * 6.658503532409668
Epoch 420, val loss: 0.8543121814727783
Epoch 430, training loss: 0.8608272075653076 = 0.19524244964122772 + 0.1 * 6.655847549438477
Epoch 430, val loss: 0.8577311635017395
Epoch 440, training loss: 0.8425657153129578 = 0.17664726078510284 + 0.1 * 6.659184455871582
Epoch 440, val loss: 0.8626205921173096
Epoch 450, training loss: 0.8238732814788818 = 0.1594403237104416 + 0.1 * 6.644329071044922
Epoch 450, val loss: 0.8688827753067017
Epoch 460, training loss: 0.8092958927154541 = 0.14360664784908295 + 0.1 * 6.6568922996521
Epoch 460, val loss: 0.8762543201446533
Epoch 470, training loss: 0.7920439839363098 = 0.12919817864894867 + 0.1 * 6.628458023071289
Epoch 470, val loss: 0.8844870924949646
Epoch 480, training loss: 0.7796509861946106 = 0.1161462739109993 + 0.1 * 6.63504695892334
Epoch 480, val loss: 0.8932707905769348
Epoch 490, training loss: 0.7669997811317444 = 0.10443127900362015 + 0.1 * 6.625685214996338
Epoch 490, val loss: 0.9023987650871277
Epoch 500, training loss: 0.7571090459823608 = 0.09397450089454651 + 0.1 * 6.631344795227051
Epoch 500, val loss: 0.9116449952125549
Epoch 510, training loss: 0.7466959953308105 = 0.08469150960445404 + 0.1 * 6.620044708251953
Epoch 510, val loss: 0.9208257794380188
Epoch 520, training loss: 0.7390903830528259 = 0.07645010203123093 + 0.1 * 6.626402854919434
Epoch 520, val loss: 0.9300163984298706
Epoch 530, training loss: 0.7305480241775513 = 0.06915658712387085 + 0.1 * 6.613914489746094
Epoch 530, val loss: 0.938941240310669
Epoch 540, training loss: 0.7231338620185852 = 0.06270445138216019 + 0.1 * 6.6042938232421875
Epoch 540, val loss: 0.9478828310966492
Epoch 550, training loss: 0.7179398536682129 = 0.05699455365538597 + 0.1 * 6.609452724456787
Epoch 550, val loss: 0.9565196633338928
Epoch 560, training loss: 0.7125784158706665 = 0.051948294043540955 + 0.1 * 6.6063008308410645
Epoch 560, val loss: 0.9651492834091187
Epoch 570, training loss: 0.7073765993118286 = 0.047484274953603745 + 0.1 * 6.598923206329346
Epoch 570, val loss: 0.9735464453697205
Epoch 580, training loss: 0.7026553153991699 = 0.04352454096078873 + 0.1 * 6.591307640075684
Epoch 580, val loss: 0.981857419013977
Epoch 590, training loss: 0.6989647746086121 = 0.040013838559389114 + 0.1 * 6.589509010314941
Epoch 590, val loss: 0.9899176955223083
Epoch 600, training loss: 0.6952190399169922 = 0.03689703717827797 + 0.1 * 6.583219528198242
Epoch 600, val loss: 0.9979773759841919
Epoch 610, training loss: 0.6925966739654541 = 0.0341167226433754 + 0.1 * 6.584799289703369
Epoch 610, val loss: 1.0057485103607178
Epoch 620, training loss: 0.6905545592308044 = 0.031633615493774414 + 0.1 * 6.58920955657959
Epoch 620, val loss: 1.0132850408554077
Epoch 630, training loss: 0.6873416304588318 = 0.029412437230348587 + 0.1 * 6.579291820526123
Epoch 630, val loss: 1.0207780599594116
Epoch 640, training loss: 0.684660792350769 = 0.02741546370089054 + 0.1 * 6.572453498840332
Epoch 640, val loss: 1.028084397315979
Epoch 650, training loss: 0.6826177835464478 = 0.025614529848098755 + 0.1 * 6.570032596588135
Epoch 650, val loss: 1.0352033376693726
Epoch 660, training loss: 0.6815140843391418 = 0.0239905696362257 + 0.1 * 6.575234889984131
Epoch 660, val loss: 1.0421297550201416
Epoch 670, training loss: 0.6789420247077942 = 0.022523507475852966 + 0.1 * 6.564184665679932
Epoch 670, val loss: 1.0488556623458862
Epoch 680, training loss: 0.6778274178504944 = 0.02119169756770134 + 0.1 * 6.566357135772705
Epoch 680, val loss: 1.0554522275924683
Epoch 690, training loss: 0.6756555438041687 = 0.019979331642389297 + 0.1 * 6.556761741638184
Epoch 690, val loss: 1.0617268085479736
Epoch 700, training loss: 0.6772074103355408 = 0.0188737865537405 + 0.1 * 6.583335876464844
Epoch 700, val loss: 1.0679423809051514
Epoch 710, training loss: 0.6732822060585022 = 0.01786506362259388 + 0.1 * 6.554171562194824
Epoch 710, val loss: 1.0739476680755615
Epoch 720, training loss: 0.6710373163223267 = 0.01693911664187908 + 0.1 * 6.540981769561768
Epoch 720, val loss: 1.0798661708831787
Epoch 730, training loss: 0.6713235378265381 = 0.016085149720311165 + 0.1 * 6.552383899688721
Epoch 730, val loss: 1.085645318031311
Epoch 740, training loss: 0.6695994138717651 = 0.015297605656087399 + 0.1 * 6.543018341064453
Epoch 740, val loss: 1.0912230014801025
Epoch 750, training loss: 0.6696226596832275 = 0.014570540748536587 + 0.1 * 6.550521373748779
Epoch 750, val loss: 1.0967506170272827
Epoch 760, training loss: 0.6698673963546753 = 0.01389852724969387 + 0.1 * 6.559688568115234
Epoch 760, val loss: 1.1020301580429077
Epoch 770, training loss: 0.6671163439750671 = 0.013277706690132618 + 0.1 * 6.538386344909668
Epoch 770, val loss: 1.107157826423645
Epoch 780, training loss: 0.6662087440490723 = 0.012700945138931274 + 0.1 * 6.535078048706055
Epoch 780, val loss: 1.1123170852661133
Epoch 790, training loss: 0.6657112836837769 = 0.01216308306902647 + 0.1 * 6.535481929779053
Epoch 790, val loss: 1.1172173023223877
Epoch 800, training loss: 0.6643971800804138 = 0.011661923490464687 + 0.1 * 6.527352333068848
Epoch 800, val loss: 1.122023105621338
Epoch 810, training loss: 0.6656700372695923 = 0.01119385939091444 + 0.1 * 6.544761657714844
Epoch 810, val loss: 1.1267362833023071
Epoch 820, training loss: 0.663821816444397 = 0.010756700299680233 + 0.1 * 6.530651092529297
Epoch 820, val loss: 1.1313284635543823
Epoch 830, training loss: 0.6627522706985474 = 0.010347443632781506 + 0.1 * 6.524048328399658
Epoch 830, val loss: 1.1357464790344238
Epoch 840, training loss: 0.6619826555252075 = 0.009964094497263432 + 0.1 * 6.520185470581055
Epoch 840, val loss: 1.1401722431182861
Epoch 850, training loss: 0.6619906425476074 = 0.009603078477084637 + 0.1 * 6.5238752365112305
Epoch 850, val loss: 1.144494652748108
Epoch 860, training loss: 0.6615074872970581 = 0.009263062849640846 + 0.1 * 6.522444248199463
Epoch 860, val loss: 1.1486130952835083
Epoch 870, training loss: 0.6604670286178589 = 0.00894346833229065 + 0.1 * 6.515235424041748
Epoch 870, val loss: 1.1526389122009277
Epoch 880, training loss: 0.6594525575637817 = 0.008641697466373444 + 0.1 * 6.508108139038086
Epoch 880, val loss: 1.1567171812057495
Epoch 890, training loss: 0.6605203747749329 = 0.008355828933417797 + 0.1 * 6.5216450691223145
Epoch 890, val loss: 1.1606123447418213
Epoch 900, training loss: 0.6609816551208496 = 0.008086013607680798 + 0.1 * 6.528956413269043
Epoch 900, val loss: 1.1642882823944092
Epoch 910, training loss: 0.6587570905685425 = 0.007832003757357597 + 0.1 * 6.509250640869141
Epoch 910, val loss: 1.1679749488830566
Epoch 920, training loss: 0.6586582660675049 = 0.007590589113533497 + 0.1 * 6.510676383972168
Epoch 920, val loss: 1.1716095209121704
Epoch 930, training loss: 0.6584790945053101 = 0.007361366879194975 + 0.1 * 6.5111775398254395
Epoch 930, val loss: 1.1751625537872314
Epoch 940, training loss: 0.6572501063346863 = 0.007143833674490452 + 0.1 * 6.501062393188477
Epoch 940, val loss: 1.1785250902175903
Epoch 950, training loss: 0.6585971117019653 = 0.00693703256547451 + 0.1 * 6.516600608825684
Epoch 950, val loss: 1.181919813156128
Epoch 960, training loss: 0.657008945941925 = 0.006740941200405359 + 0.1 * 6.502679824829102
Epoch 960, val loss: 1.185210943222046
Epoch 970, training loss: 0.6564555168151855 = 0.006554008927196264 + 0.1 * 6.499014854431152
Epoch 970, val loss: 1.1884499788284302
Epoch 980, training loss: 0.6554650068283081 = 0.006375030614435673 + 0.1 * 6.490900039672852
Epoch 980, val loss: 1.1916425228118896
Epoch 990, training loss: 0.6562962532043457 = 0.006204065401107073 + 0.1 * 6.500921726226807
Epoch 990, val loss: 1.194800853729248
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 2.8033642768859863 = 1.943678617477417 + 0.1 * 8.596855163574219
Epoch 0, val loss: 1.9312058687210083
Epoch 10, training loss: 2.792696714401245 = 1.9330157041549683 + 0.1 * 8.596810340881348
Epoch 10, val loss: 1.9211491346359253
Epoch 20, training loss: 2.779592275619507 = 1.919937014579773 + 0.1 * 8.596551895141602
Epoch 20, val loss: 1.9083480834960938
Epoch 30, training loss: 2.7612698078155518 = 1.9018207788467407 + 0.1 * 8.594491004943848
Epoch 30, val loss: 1.8903738260269165
Epoch 40, training loss: 2.733584403991699 = 1.8757507801055908 + 0.1 * 8.578335762023926
Epoch 40, val loss: 1.864849328994751
Epoch 50, training loss: 2.691524028778076 = 1.840973138809204 + 0.1 * 8.505509376525879
Epoch 50, val loss: 1.8327659368515015
Epoch 60, training loss: 2.6372900009155273 = 1.8047538995742798 + 0.1 * 8.325362205505371
Epoch 60, val loss: 1.8030363321304321
Epoch 70, training loss: 2.591113567352295 = 1.7721354961395264 + 0.1 * 8.189779281616211
Epoch 70, val loss: 1.7781728506088257
Epoch 80, training loss: 2.5195648670196533 = 1.7344123125076294 + 0.1 * 7.851526260375977
Epoch 80, val loss: 1.745652437210083
Epoch 90, training loss: 2.4357569217681885 = 1.6874710321426392 + 0.1 * 7.482858657836914
Epoch 90, val loss: 1.7054229974746704
Epoch 100, training loss: 2.3532512187957764 = 1.6274746656417847 + 0.1 * 7.257766246795654
Epoch 100, val loss: 1.6562131643295288
Epoch 110, training loss: 2.267720937728882 = 1.55233895778656 + 0.1 * 7.1538190841674805
Epoch 110, val loss: 1.5920751094818115
Epoch 120, training loss: 2.1774191856384277 = 1.467741847038269 + 0.1 * 7.096774578094482
Epoch 120, val loss: 1.5224485397338867
Epoch 130, training loss: 2.0857417583465576 = 1.37916100025177 + 0.1 * 7.065806865692139
Epoch 130, val loss: 1.451797604560852
Epoch 140, training loss: 1.9936023950576782 = 1.2891587018966675 + 0.1 * 7.044436931610107
Epoch 140, val loss: 1.38019597530365
Epoch 150, training loss: 1.9019911289215088 = 1.1991385221481323 + 0.1 * 7.028526306152344
Epoch 150, val loss: 1.3099095821380615
Epoch 160, training loss: 1.8110909461975098 = 1.1103321313858032 + 0.1 * 7.007587432861328
Epoch 160, val loss: 1.241418719291687
Epoch 170, training loss: 1.722907304763794 = 1.0240792036056519 + 0.1 * 6.98828125
Epoch 170, val loss: 1.1761682033538818
Epoch 180, training loss: 1.638890027999878 = 0.9420408606529236 + 0.1 * 6.968491554260254
Epoch 180, val loss: 1.1146283149719238
Epoch 190, training loss: 1.560318946838379 = 0.8651081323623657 + 0.1 * 6.952107906341553
Epoch 190, val loss: 1.0576142072677612
Epoch 200, training loss: 1.487489104270935 = 0.7935974597930908 + 0.1 * 6.938916206359863
Epoch 200, val loss: 1.0053473711013794
Epoch 210, training loss: 1.4211816787719727 = 0.727603554725647 + 0.1 * 6.935781955718994
Epoch 210, val loss: 0.9588693380355835
Epoch 220, training loss: 1.3602488040924072 = 0.6683480739593506 + 0.1 * 6.919007301330566
Epoch 220, val loss: 0.9198551774024963
Epoch 230, training loss: 1.3068023920059204 = 0.6155983805656433 + 0.1 * 6.9120402336120605
Epoch 230, val loss: 0.8884952664375305
Epoch 240, training loss: 1.2597568035125732 = 0.5693039894104004 + 0.1 * 6.9045281410217285
Epoch 240, val loss: 0.8643231391906738
Epoch 250, training loss: 1.2179131507873535 = 0.5286111235618591 + 0.1 * 6.8930206298828125
Epoch 250, val loss: 0.8461309671401978
Epoch 260, training loss: 1.1819771528244019 = 0.49245956540107727 + 0.1 * 6.895175933837891
Epoch 260, val loss: 0.8324721455574036
Epoch 270, training loss: 1.1476237773895264 = 0.460012286901474 + 0.1 * 6.876114368438721
Epoch 270, val loss: 0.8218368291854858
Epoch 280, training loss: 1.1168286800384521 = 0.4298461675643921 + 0.1 * 6.869824409484863
Epoch 280, val loss: 0.813201904296875
Epoch 290, training loss: 1.087022304534912 = 0.4009111225605011 + 0.1 * 6.861111640930176
Epoch 290, val loss: 0.8054927587509155
Epoch 300, training loss: 1.0578148365020752 = 0.37238186597824097 + 0.1 * 6.8543291091918945
Epoch 300, val loss: 0.7979902029037476
Epoch 310, training loss: 1.0288056135177612 = 0.3436581492424011 + 0.1 * 6.851474285125732
Epoch 310, val loss: 0.7904470562934875
Epoch 320, training loss: 0.9986163973808289 = 0.3145008683204651 + 0.1 * 6.841155052185059
Epoch 320, val loss: 0.7826449871063232
Epoch 330, training loss: 0.9689319133758545 = 0.2850782573223114 + 0.1 * 6.838536739349365
Epoch 330, val loss: 0.7751356363296509
Epoch 340, training loss: 0.9397888779640198 = 0.25619155168533325 + 0.1 * 6.835973262786865
Epoch 340, val loss: 0.767737865447998
Epoch 350, training loss: 0.9113157987594604 = 0.22833411395549774 + 0.1 * 6.829816818237305
Epoch 350, val loss: 0.7616404294967651
Epoch 360, training loss: 0.8842382431030273 = 0.2021704912185669 + 0.1 * 6.820677280426025
Epoch 360, val loss: 0.7570688724517822
Epoch 370, training loss: 0.8602736592292786 = 0.17830412089824677 + 0.1 * 6.819694995880127
Epoch 370, val loss: 0.7543061971664429
Epoch 380, training loss: 0.8386886119842529 = 0.15731403231620789 + 0.1 * 6.813745975494385
Epoch 380, val loss: 0.7537000775337219
Epoch 390, training loss: 0.8206297159194946 = 0.13918791711330414 + 0.1 * 6.814417839050293
Epoch 390, val loss: 0.7549474239349365
Epoch 400, training loss: 0.8049288392066956 = 0.12358131259679794 + 0.1 * 6.813475131988525
Epoch 400, val loss: 0.757622480392456
Epoch 410, training loss: 0.7903528213500977 = 0.11019990593194962 + 0.1 * 6.8015289306640625
Epoch 410, val loss: 0.7613446116447449
Epoch 420, training loss: 0.7786141633987427 = 0.09869657456874847 + 0.1 * 6.79917573928833
Epoch 420, val loss: 0.7660210728645325
Epoch 430, training loss: 0.7675471305847168 = 0.08876709640026093 + 0.1 * 6.787800312042236
Epoch 430, val loss: 0.7712371349334717
Epoch 440, training loss: 0.7592599391937256 = 0.08016445487737656 + 0.1 * 6.790955066680908
Epoch 440, val loss: 0.7770817875862122
Epoch 450, training loss: 0.7505492568016052 = 0.07270703464746475 + 0.1 * 6.778421878814697
Epoch 450, val loss: 0.7828024625778198
Epoch 460, training loss: 0.7434960007667542 = 0.06617393344640732 + 0.1 * 6.773221015930176
Epoch 460, val loss: 0.7888211011886597
Epoch 470, training loss: 0.7388778328895569 = 0.06041323393583298 + 0.1 * 6.7846455574035645
Epoch 470, val loss: 0.7949722409248352
Epoch 480, training loss: 0.7325922250747681 = 0.05534234642982483 + 0.1 * 6.772499084472656
Epoch 480, val loss: 0.8010461330413818
Epoch 490, training loss: 0.726917564868927 = 0.050838589668273926 + 0.1 * 6.76078987121582
Epoch 490, val loss: 0.8072183132171631
Epoch 500, training loss: 0.7238225340843201 = 0.046841274946928024 + 0.1 * 6.76981258392334
Epoch 500, val loss: 0.813320517539978
Epoch 510, training loss: 0.718907356262207 = 0.04328203201293945 + 0.1 * 6.756253242492676
Epoch 510, val loss: 0.8191413879394531
Epoch 520, training loss: 0.7148144245147705 = 0.04009578377008438 + 0.1 * 6.747186183929443
Epoch 520, val loss: 0.8251404762268066
Epoch 530, training loss: 0.7112539410591125 = 0.03723340108990669 + 0.1 * 6.74020528793335
Epoch 530, val loss: 0.8309977054595947
Epoch 540, training loss: 0.7089889645576477 = 0.03466056287288666 + 0.1 * 6.743283748626709
Epoch 540, val loss: 0.8369529247283936
Epoch 550, training loss: 0.7051598429679871 = 0.032347504049539566 + 0.1 * 6.728123188018799
Epoch 550, val loss: 0.8424069285392761
Epoch 560, training loss: 0.7026780843734741 = 0.0302564799785614 + 0.1 * 6.724215507507324
Epoch 560, val loss: 0.848042368888855
Epoch 570, training loss: 0.7011038661003113 = 0.028356419876217842 + 0.1 * 6.727474212646484
Epoch 570, val loss: 0.8535702228546143
Epoch 580, training loss: 0.6988535523414612 = 0.02663065493106842 + 0.1 * 6.72222900390625
Epoch 580, val loss: 0.8589100241661072
Epoch 590, training loss: 0.6973296403884888 = 0.025059981271624565 + 0.1 * 6.722696304321289
Epoch 590, val loss: 0.8641963601112366
Epoch 600, training loss: 0.6968355774879456 = 0.023621398955583572 + 0.1 * 6.732141971588135
Epoch 600, val loss: 0.8694418668746948
Epoch 610, training loss: 0.6921154856681824 = 0.022310635074973106 + 0.1 * 6.6980485916137695
Epoch 610, val loss: 0.8744470477104187
Epoch 620, training loss: 0.6909096837043762 = 0.02110818587243557 + 0.1 * 6.698014736175537
Epoch 620, val loss: 0.8794302940368652
Epoch 630, training loss: 0.6884506344795227 = 0.020002951845526695 + 0.1 * 6.684476375579834
Epoch 630, val loss: 0.8844842910766602
Epoch 640, training loss: 0.686946451663971 = 0.018985802307724953 + 0.1 * 6.6796064376831055
Epoch 640, val loss: 0.8890860676765442
Epoch 650, training loss: 0.6862120628356934 = 0.01804823987185955 + 0.1 * 6.681638240814209
Epoch 650, val loss: 0.8940145969390869
Epoch 660, training loss: 0.6839630603790283 = 0.01718265376985073 + 0.1 * 6.66780424118042
Epoch 660, val loss: 0.8983857035636902
Epoch 670, training loss: 0.683331310749054 = 0.016380971297621727 + 0.1 * 6.669503211975098
Epoch 670, val loss: 0.9027827978134155
Epoch 680, training loss: 0.6814429759979248 = 0.015635212883353233 + 0.1 * 6.658077716827393
Epoch 680, val loss: 0.9072393774986267
Epoch 690, training loss: 0.6802496910095215 = 0.01494058221578598 + 0.1 * 6.653090953826904
Epoch 690, val loss: 0.911587119102478
Epoch 700, training loss: 0.6798567771911621 = 0.014294219203293324 + 0.1 * 6.655625343322754
Epoch 700, val loss: 0.9156824350357056
Epoch 710, training loss: 0.6788381338119507 = 0.013692308217287064 + 0.1 * 6.651457786560059
Epoch 710, val loss: 0.9197582602500916
Epoch 720, training loss: 0.6774796843528748 = 0.01313086412847042 + 0.1 * 6.64348840713501
Epoch 720, val loss: 0.9237044453620911
Epoch 730, training loss: 0.6758917570114136 = 0.012605108320713043 + 0.1 * 6.632865905761719
Epoch 730, val loss: 0.9276816248893738
Epoch 740, training loss: 0.6764488220214844 = 0.012111715041100979 + 0.1 * 6.643370628356934
Epoch 740, val loss: 0.9315370321273804
Epoch 750, training loss: 0.6750388741493225 = 0.011648095212876797 + 0.1 * 6.633907318115234
Epoch 750, val loss: 0.9352953433990479
Epoch 760, training loss: 0.6744391918182373 = 0.011213465593755245 + 0.1 * 6.632256984710693
Epoch 760, val loss: 0.9390782117843628
Epoch 770, training loss: 0.6729031801223755 = 0.010803895071148872 + 0.1 * 6.620993137359619
Epoch 770, val loss: 0.9426921010017395
Epoch 780, training loss: 0.672587513923645 = 0.010418465360999107 + 0.1 * 6.621690273284912
Epoch 780, val loss: 0.9462378621101379
Epoch 790, training loss: 0.6721749901771545 = 0.010053561069071293 + 0.1 * 6.621213912963867
Epoch 790, val loss: 0.949928879737854
Epoch 800, training loss: 0.6713864803314209 = 0.009711315855383873 + 0.1 * 6.616751670837402
Epoch 800, val loss: 0.9532167911529541
Epoch 810, training loss: 0.6697167754173279 = 0.00938745029270649 + 0.1 * 6.603293418884277
Epoch 810, val loss: 0.9566095471382141
Epoch 820, training loss: 0.6688703298568726 = 0.009081191383302212 + 0.1 * 6.597891330718994
Epoch 820, val loss: 0.9598707556724548
Epoch 830, training loss: 0.6674578785896301 = 0.00878962967544794 + 0.1 * 6.586682319641113
Epoch 830, val loss: 0.9632463455200195
Epoch 840, training loss: 0.6682124733924866 = 0.008513503707945347 + 0.1 * 6.596989631652832
Epoch 840, val loss: 0.9664295315742493
Epoch 850, training loss: 0.6675567626953125 = 0.008252176456153393 + 0.1 * 6.593045711517334
Epoch 850, val loss: 0.9696476459503174
Epoch 860, training loss: 0.6676918864250183 = 0.008004343137145042 + 0.1 * 6.596875190734863
Epoch 860, val loss: 0.9727452993392944
Epoch 870, training loss: 0.6658713817596436 = 0.0077691227197647095 + 0.1 * 6.5810227394104
Epoch 870, val loss: 0.9756649732589722
Epoch 880, training loss: 0.6672544479370117 = 0.007544760592281818 + 0.1 * 6.597096920013428
Epoch 880, val loss: 0.9786789417266846
Epoch 890, training loss: 0.6666331887245178 = 0.00733049213886261 + 0.1 * 6.593027114868164
Epoch 890, val loss: 0.981658399105072
Epoch 900, training loss: 0.6651467084884644 = 0.007126845885068178 + 0.1 * 6.580198287963867
Epoch 900, val loss: 0.9845677018165588
Epoch 910, training loss: 0.6639420390129089 = 0.006932944990694523 + 0.1 * 6.570090293884277
Epoch 910, val loss: 0.9872845411300659
Epoch 920, training loss: 0.6636740565299988 = 0.006746999453753233 + 0.1 * 6.569270133972168
Epoch 920, val loss: 0.9901441931724548
Epoch 930, training loss: 0.6637178063392639 = 0.006568877026438713 + 0.1 * 6.571489334106445
Epoch 930, val loss: 0.9930852651596069
Epoch 940, training loss: 0.6628566980361938 = 0.006399704143404961 + 0.1 * 6.56456995010376
Epoch 940, val loss: 0.9957274198532104
Epoch 950, training loss: 0.6619997024536133 = 0.00623799953609705 + 0.1 * 6.5576171875
Epoch 950, val loss: 0.9982370138168335
Epoch 960, training loss: 0.662453830242157 = 0.0060820747166872025 + 0.1 * 6.563717365264893
Epoch 960, val loss: 1.001067042350769
Epoch 970, training loss: 0.6618704199790955 = 0.005933181848376989 + 0.1 * 6.559372425079346
Epoch 970, val loss: 1.0037477016448975
Epoch 980, training loss: 0.6625822186470032 = 0.005790807772427797 + 0.1 * 6.56791353225708
Epoch 980, val loss: 1.0062274932861328
Epoch 990, training loss: 0.6610733270645142 = 0.005653976928442717 + 0.1 * 6.554193496704102
Epoch 990, val loss: 1.0087995529174805
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8112809699525567
The final CL Acc:0.74444, 0.01386, The final GNN Acc:0.81005, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13212])
remove edge: torch.Size([2, 7840])
updated graph: torch.Size([2, 10496])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.819004774093628 = 1.9593217372894287 + 0.1 * 8.596829414367676
Epoch 0, val loss: 1.9593909978866577
Epoch 10, training loss: 2.8085272312164307 = 1.9488492012023926 + 0.1 * 8.596780776977539
Epoch 10, val loss: 1.9486303329467773
Epoch 20, training loss: 2.796015739440918 = 1.9363654851913452 + 0.1 * 8.596503257751465
Epoch 20, val loss: 1.9357686042785645
Epoch 30, training loss: 2.778488874435425 = 1.919039011001587 + 0.1 * 8.594498634338379
Epoch 30, val loss: 1.917979121208191
Epoch 40, training loss: 2.751220941543579 = 1.8933125734329224 + 0.1 * 8.579083442687988
Epoch 40, val loss: 1.8918882608413696
Epoch 50, training loss: 2.7073609828948975 = 1.8562650680541992 + 0.1 * 8.510958671569824
Epoch 50, val loss: 1.8559187650680542
Epoch 60, training loss: 2.638401985168457 = 1.8125590085983276 + 0.1 * 8.258428573608398
Epoch 60, val loss: 1.8170204162597656
Epoch 70, training loss: 2.575085401535034 = 1.771371603012085 + 0.1 * 8.037137031555176
Epoch 70, val loss: 1.7820814847946167
Epoch 80, training loss: 2.496555805206299 = 1.728770136833191 + 0.1 * 7.677857398986816
Epoch 80, val loss: 1.742680311203003
Epoch 90, training loss: 2.412707805633545 = 1.6766685247421265 + 0.1 * 7.36039400100708
Epoch 90, val loss: 1.69584321975708
Epoch 100, training loss: 2.3279809951782227 = 1.6091609001159668 + 0.1 * 7.188200950622559
Epoch 100, val loss: 1.6380808353424072
Epoch 110, training loss: 2.2377238273620605 = 1.5237044095993042 + 0.1 * 7.140194892883301
Epoch 110, val loss: 1.5622336864471436
Epoch 120, training loss: 2.1389853954315186 = 1.428275227546692 + 0.1 * 7.1071014404296875
Epoch 120, val loss: 1.4799786806106567
Epoch 130, training loss: 2.0390961170196533 = 1.3310121297836304 + 0.1 * 7.08083963394165
Epoch 130, val loss: 1.399006724357605
Epoch 140, training loss: 1.9413045644760132 = 1.2357689142227173 + 0.1 * 7.055356502532959
Epoch 140, val loss: 1.3216155767440796
Epoch 150, training loss: 1.8462812900543213 = 1.1433237791061401 + 0.1 * 7.029575347900391
Epoch 150, val loss: 1.2478255033493042
Epoch 160, training loss: 1.7544975280761719 = 1.0535352230072021 + 0.1 * 7.0096235275268555
Epoch 160, val loss: 1.1780365705490112
Epoch 170, training loss: 1.667032241821289 = 0.9671366810798645 + 0.1 * 6.998955726623535
Epoch 170, val loss: 1.1118260622024536
Epoch 180, training loss: 1.5857486724853516 = 0.8867269158363342 + 0.1 * 6.9902167320251465
Epoch 180, val loss: 1.0505653619766235
Epoch 190, training loss: 1.5112287998199463 = 0.813094973564148 + 0.1 * 6.981337547302246
Epoch 190, val loss: 0.9945103526115417
Epoch 200, training loss: 1.4441404342651367 = 0.7467638254165649 + 0.1 * 6.973766803741455
Epoch 200, val loss: 0.9444966912269592
Epoch 210, training loss: 1.3845115900039673 = 0.687527596950531 + 0.1 * 6.969839572906494
Epoch 210, val loss: 0.9014045596122742
Epoch 220, training loss: 1.3303966522216797 = 0.6345914006233215 + 0.1 * 6.958053112030029
Epoch 220, val loss: 0.8651015162467957
Epoch 230, training loss: 1.2812681198120117 = 0.5865093469619751 + 0.1 * 6.947587013244629
Epoch 230, val loss: 0.8347877264022827
Epoch 240, training loss: 1.2358651161193848 = 0.5419442653656006 + 0.1 * 6.939207553863525
Epoch 240, val loss: 0.8093023300170898
Epoch 250, training loss: 1.1928586959838867 = 0.5002063512802124 + 0.1 * 6.926522731781006
Epoch 250, val loss: 0.7881336808204651
Epoch 260, training loss: 1.1516540050506592 = 0.4605293869972229 + 0.1 * 6.911245822906494
Epoch 260, val loss: 0.7705174684524536
Epoch 270, training loss: 1.111616849899292 = 0.42219191789627075 + 0.1 * 6.894248962402344
Epoch 270, val loss: 0.7559518218040466
Epoch 280, training loss: 1.0741690397262573 = 0.3850228190422058 + 0.1 * 6.891462326049805
Epoch 280, val loss: 0.7442546486854553
Epoch 290, training loss: 1.0357224941253662 = 0.34873825311660767 + 0.1 * 6.869843006134033
Epoch 290, val loss: 0.7346239686012268
Epoch 300, training loss: 0.9987483024597168 = 0.31331801414489746 + 0.1 * 6.854302883148193
Epoch 300, val loss: 0.7272027134895325
Epoch 310, training loss: 0.9639940857887268 = 0.2795810103416443 + 0.1 * 6.844130516052246
Epoch 310, val loss: 0.7223767042160034
Epoch 320, training loss: 0.9330199956893921 = 0.24878868460655212 + 0.1 * 6.842313289642334
Epoch 320, val loss: 0.7203842997550964
Epoch 330, training loss: 0.9041650295257568 = 0.22156649827957153 + 0.1 * 6.825984954833984
Epoch 330, val loss: 0.7210658192634583
Epoch 340, training loss: 0.879328727722168 = 0.19768469035625458 + 0.1 * 6.816440105438232
Epoch 340, val loss: 0.7238628268241882
Epoch 350, training loss: 0.8576260805130005 = 0.1768108755350113 + 0.1 * 6.808152198791504
Epoch 350, val loss: 0.7284829616546631
Epoch 360, training loss: 0.8376840353012085 = 0.1585753858089447 + 0.1 * 6.791086196899414
Epoch 360, val loss: 0.7342864871025085
Epoch 370, training loss: 0.8204641938209534 = 0.14256834983825684 + 0.1 * 6.778958320617676
Epoch 370, val loss: 0.7408457398414612
Epoch 380, training loss: 0.8054301142692566 = 0.12845362722873688 + 0.1 * 6.7697649002075195
Epoch 380, val loss: 0.748121440410614
Epoch 390, training loss: 0.7941059470176697 = 0.11596596986055374 + 0.1 * 6.781399726867676
Epoch 390, val loss: 0.7557924389839172
Epoch 400, training loss: 0.7808083891868591 = 0.10496387630701065 + 0.1 * 6.758444786071777
Epoch 400, val loss: 0.7638821601867676
Epoch 410, training loss: 0.7689734697341919 = 0.0951840952038765 + 0.1 * 6.737893581390381
Epoch 410, val loss: 0.7722064256668091
Epoch 420, training loss: 0.762984037399292 = 0.08646757900714874 + 0.1 * 6.765164852142334
Epoch 420, val loss: 0.7809149622917175
Epoch 430, training loss: 0.7516443729400635 = 0.07877359539270401 + 0.1 * 6.728707313537598
Epoch 430, val loss: 0.7895792126655579
Epoch 440, training loss: 0.7432522773742676 = 0.07191739231348038 + 0.1 * 6.713348388671875
Epoch 440, val loss: 0.7983129024505615
Epoch 450, training loss: 0.7358070611953735 = 0.06577570736408234 + 0.1 * 6.700313568115234
Epoch 450, val loss: 0.8071379661560059
Epoch 460, training loss: 0.7303371429443359 = 0.06026682257652283 + 0.1 * 6.700703144073486
Epoch 460, val loss: 0.815991222858429
Epoch 470, training loss: 0.7244256734848022 = 0.0553467757999897 + 0.1 * 6.690788745880127
Epoch 470, val loss: 0.8246870040893555
Epoch 480, training loss: 0.7194432616233826 = 0.05092667043209076 + 0.1 * 6.685165882110596
Epoch 480, val loss: 0.8332681655883789
Epoch 490, training loss: 0.716673731803894 = 0.046973325312137604 + 0.1 * 6.6970038414001465
Epoch 490, val loss: 0.8416944742202759
Epoch 500, training loss: 0.7105379104614258 = 0.043429184705019 + 0.1 * 6.671087265014648
Epoch 500, val loss: 0.8498803973197937
Epoch 510, training loss: 0.7062975764274597 = 0.040233053267002106 + 0.1 * 6.660645008087158
Epoch 510, val loss: 0.8579797744750977
Epoch 520, training loss: 0.7050363421440125 = 0.03734441474080086 + 0.1 * 6.676919460296631
Epoch 520, val loss: 0.8659684062004089
Epoch 530, training loss: 0.7001246213912964 = 0.03474466875195503 + 0.1 * 6.653799057006836
Epoch 530, val loss: 0.8737173080444336
Epoch 540, training loss: 0.6969848275184631 = 0.032389771193265915 + 0.1 * 6.6459503173828125
Epoch 540, val loss: 0.8812964558601379
Epoch 550, training loss: 0.6938532590866089 = 0.03025885298848152 + 0.1 * 6.63594388961792
Epoch 550, val loss: 0.8887194991111755
Epoch 560, training loss: 0.6917458772659302 = 0.028328903019428253 + 0.1 * 6.634169578552246
Epoch 560, val loss: 0.8959550857543945
Epoch 570, training loss: 0.6903185844421387 = 0.026571601629257202 + 0.1 * 6.637469291687012
Epoch 570, val loss: 0.9030063152313232
Epoch 580, training loss: 0.6881682276725769 = 0.02497137151658535 + 0.1 * 6.6319684982299805
Epoch 580, val loss: 0.9099352359771729
Epoch 590, training loss: 0.6867250204086304 = 0.023511404171586037 + 0.1 * 6.63213586807251
Epoch 590, val loss: 0.9166632294654846
Epoch 600, training loss: 0.6840164661407471 = 0.0221765898168087 + 0.1 * 6.618399143218994
Epoch 600, val loss: 0.9232459664344788
Epoch 610, training loss: 0.6823670864105225 = 0.020953558385372162 + 0.1 * 6.614135265350342
Epoch 610, val loss: 0.9296655058860779
Epoch 620, training loss: 0.6807845234870911 = 0.019830381497740746 + 0.1 * 6.609540939331055
Epoch 620, val loss: 0.9359385967254639
Epoch 630, training loss: 0.6794202923774719 = 0.018796982243657112 + 0.1 * 6.6062331199646
Epoch 630, val loss: 0.9420644640922546
Epoch 640, training loss: 0.6775005459785461 = 0.017846016213297844 + 0.1 * 6.596545219421387
Epoch 640, val loss: 0.9480253458023071
Epoch 650, training loss: 0.6772900223731995 = 0.016971472650766373 + 0.1 * 6.603185653686523
Epoch 650, val loss: 0.9538099765777588
Epoch 660, training loss: 0.6752769947052002 = 0.01616017520427704 + 0.1 * 6.59116792678833
Epoch 660, val loss: 0.9594584107398987
Epoch 670, training loss: 0.6746277213096619 = 0.01540596317499876 + 0.1 * 6.592217445373535
Epoch 670, val loss: 0.9650672078132629
Epoch 680, training loss: 0.6740968227386475 = 0.014706151559948921 + 0.1 * 6.593906879425049
Epoch 680, val loss: 0.9705350399017334
Epoch 690, training loss: 0.6727787256240845 = 0.014058304950594902 + 0.1 * 6.5872039794921875
Epoch 690, val loss: 0.9759082794189453
Epoch 700, training loss: 0.6726263165473938 = 0.013455267995595932 + 0.1 * 6.591710090637207
Epoch 700, val loss: 0.9810934662818909
Epoch 710, training loss: 0.671076774597168 = 0.012892534025013447 + 0.1 * 6.581841945648193
Epoch 710, val loss: 0.986180305480957
Epoch 720, training loss: 0.6691601276397705 = 0.012366486713290215 + 0.1 * 6.567936420440674
Epoch 720, val loss: 0.9911656379699707
Epoch 730, training loss: 0.6714340448379517 = 0.011873848736286163 + 0.1 * 6.595602035522461
Epoch 730, val loss: 0.9960474967956543
Epoch 740, training loss: 0.6678861975669861 = 0.011414059437811375 + 0.1 * 6.56472110748291
Epoch 740, val loss: 1.0008060932159424
Epoch 750, training loss: 0.6678141951560974 = 0.010983001440763474 + 0.1 * 6.568312168121338
Epoch 750, val loss: 1.0054060220718384
Epoch 760, training loss: 0.6678981781005859 = 0.0105775510892272 + 0.1 * 6.573205947875977
Epoch 760, val loss: 1.0099565982818604
Epoch 770, training loss: 0.6663228273391724 = 0.010196017101407051 + 0.1 * 6.561267852783203
Epoch 770, val loss: 1.0143835544586182
Epoch 780, training loss: 0.6662324070930481 = 0.009836740791797638 + 0.1 * 6.5639567375183105
Epoch 780, val loss: 1.0187571048736572
Epoch 790, training loss: 0.6650083661079407 = 0.009498448111116886 + 0.1 * 6.555098533630371
Epoch 790, val loss: 1.0230224132537842
Epoch 800, training loss: 0.6655499935150146 = 0.009178702719509602 + 0.1 * 6.563713073730469
Epoch 800, val loss: 1.0272046327590942
Epoch 810, training loss: 0.6634236574172974 = 0.008876828476786613 + 0.1 * 6.545468330383301
Epoch 810, val loss: 1.031317949295044
Epoch 820, training loss: 0.6625993847846985 = 0.008590422570705414 + 0.1 * 6.5400896072387695
Epoch 820, val loss: 1.0353196859359741
Epoch 830, training loss: 0.6627581119537354 = 0.008319322019815445 + 0.1 * 6.5443878173828125
Epoch 830, val loss: 1.0392986536026
Epoch 840, training loss: 0.6628111004829407 = 0.008062179200351238 + 0.1 * 6.547489166259766
Epoch 840, val loss: 1.043156385421753
Epoch 850, training loss: 0.6610802412033081 = 0.007818485610187054 + 0.1 * 6.532617568969727
Epoch 850, val loss: 1.0469975471496582
Epoch 860, training loss: 0.663529098033905 = 0.007586964871734381 + 0.1 * 6.559421539306641
Epoch 860, val loss: 1.0507044792175293
Epoch 870, training loss: 0.6611201763153076 = 0.007367120124399662 + 0.1 * 6.537530422210693
Epoch 870, val loss: 1.0543832778930664
Epoch 880, training loss: 0.6607542634010315 = 0.007158183958381414 + 0.1 * 6.535961151123047
Epoch 880, val loss: 1.0579122304916382
Epoch 890, training loss: 0.6593919396400452 = 0.006958682090044022 + 0.1 * 6.524332523345947
Epoch 890, val loss: 1.0614362955093384
Epoch 900, training loss: 0.6594763398170471 = 0.00676812743768096 + 0.1 * 6.5270819664001465
Epoch 900, val loss: 1.064895510673523
Epoch 910, training loss: 0.6585535407066345 = 0.006586320698261261 + 0.1 * 6.51967191696167
Epoch 910, val loss: 1.0682718753814697
Epoch 920, training loss: 0.6593596339225769 = 0.006412653252482414 + 0.1 * 6.529469966888428
Epoch 920, val loss: 1.0715605020523071
Epoch 930, training loss: 0.6581573486328125 = 0.0062463595531880856 + 0.1 * 6.51910924911499
Epoch 930, val loss: 1.0748560428619385
Epoch 940, training loss: 0.6576933860778809 = 0.006087930873036385 + 0.1 * 6.516054153442383
Epoch 940, val loss: 1.0780550241470337
Epoch 950, training loss: 0.6571530103683472 = 0.005936391185969114 + 0.1 * 6.5121660232543945
Epoch 950, val loss: 1.0812071561813354
Epoch 960, training loss: 0.6586333513259888 = 0.005790864583104849 + 0.1 * 6.528424263000488
Epoch 960, val loss: 1.0843005180358887
Epoch 970, training loss: 0.6578954458236694 = 0.005651420447975397 + 0.1 * 6.522440433502197
Epoch 970, val loss: 1.0873786211013794
Epoch 980, training loss: 0.6561701893806458 = 0.005517930723726749 + 0.1 * 6.506522178649902
Epoch 980, val loss: 1.0903701782226562
Epoch 990, training loss: 0.6552005410194397 = 0.005389797501266003 + 0.1 * 6.498107433319092
Epoch 990, val loss: 1.0933287143707275
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.8043668270111084 = 1.9446816444396973 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.9569636583328247
Epoch 10, training loss: 2.795093297958374 = 1.9354134798049927 + 0.1 * 8.59679889678955
Epoch 10, val loss: 1.947479486465454
Epoch 20, training loss: 2.7836503982543945 = 1.9240020513534546 + 0.1 * 8.596484184265137
Epoch 20, val loss: 1.9353811740875244
Epoch 30, training loss: 2.767190933227539 = 1.9077870845794678 + 0.1 * 8.594038009643555
Epoch 30, val loss: 1.917837381362915
Epoch 40, training loss: 2.7411913871765137 = 1.883516788482666 + 0.1 * 8.576746940612793
Epoch 40, val loss: 1.8916270732879639
Epoch 50, training loss: 2.698847770690918 = 1.8488049507141113 + 0.1 * 8.500428199768066
Epoch 50, val loss: 1.8554834127426147
Epoch 60, training loss: 2.6351981163024902 = 1.808165192604065 + 0.1 * 8.270330429077148
Epoch 60, val loss: 1.8158811330795288
Epoch 70, training loss: 2.5754199028015137 = 1.7689388990402222 + 0.1 * 8.06480884552002
Epoch 70, val loss: 1.7794287204742432
Epoch 80, training loss: 2.498995780944824 = 1.7256321907043457 + 0.1 * 7.733636379241943
Epoch 80, val loss: 1.7381279468536377
Epoch 90, training loss: 2.421616315841675 = 1.6702464818954468 + 0.1 * 7.513699054718018
Epoch 90, val loss: 1.6880139112472534
Epoch 100, training loss: 2.3366665840148926 = 1.5988274812698364 + 0.1 * 7.378389835357666
Epoch 100, val loss: 1.6253387928009033
Epoch 110, training loss: 2.2402503490448 = 1.5116721391677856 + 0.1 * 7.285782814025879
Epoch 110, val loss: 1.5470991134643555
Epoch 120, training loss: 2.135608673095703 = 1.4154784679412842 + 0.1 * 7.201303005218506
Epoch 120, val loss: 1.4646188020706177
Epoch 130, training loss: 2.030938148498535 = 1.3154401779174805 + 0.1 * 7.154980659484863
Epoch 130, val loss: 1.3817102909088135
Epoch 140, training loss: 1.9286482334136963 = 1.2162688970565796 + 0.1 * 7.123794078826904
Epoch 140, val loss: 1.3001456260681152
Epoch 150, training loss: 1.8321313858032227 = 1.1225652694702148 + 0.1 * 7.095661163330078
Epoch 150, val loss: 1.2245373725891113
Epoch 160, training loss: 1.7451673746109009 = 1.0386799573898315 + 0.1 * 7.064874172210693
Epoch 160, val loss: 1.1588584184646606
Epoch 170, training loss: 1.6676628589630127 = 0.9636479616165161 + 0.1 * 7.040149211883545
Epoch 170, val loss: 1.1010702848434448
Epoch 180, training loss: 1.5972720384597778 = 0.8962018489837646 + 0.1 * 7.010701656341553
Epoch 180, val loss: 1.050079584121704
Epoch 190, training loss: 1.532771348953247 = 0.8341937065124512 + 0.1 * 6.985776424407959
Epoch 190, val loss: 1.0040515661239624
Epoch 200, training loss: 1.4725630283355713 = 0.7761925458908081 + 0.1 * 6.963705539703369
Epoch 200, val loss: 0.9617782235145569
Epoch 210, training loss: 1.4158391952514648 = 0.7210763692855835 + 0.1 * 6.947627544403076
Epoch 210, val loss: 0.9223644137382507
Epoch 220, training loss: 1.361250877380371 = 0.6679446697235107 + 0.1 * 6.933062553405762
Epoch 220, val loss: 0.8856920599937439
Epoch 230, training loss: 1.3081090450286865 = 0.6160934567451477 + 0.1 * 6.920156002044678
Epoch 230, val loss: 0.8512225151062012
Epoch 240, training loss: 1.2560415267944336 = 0.5649687051773071 + 0.1 * 6.910728931427002
Epoch 240, val loss: 0.81844162940979
Epoch 250, training loss: 1.2057650089263916 = 0.5147078633308411 + 0.1 * 6.910571575164795
Epoch 250, val loss: 0.7874177098274231
Epoch 260, training loss: 1.1546645164489746 = 0.4655148684978485 + 0.1 * 6.891496181488037
Epoch 260, val loss: 0.7582311034202576
Epoch 270, training loss: 1.106547474861145 = 0.41763758659362793 + 0.1 * 6.889098644256592
Epoch 270, val loss: 0.7314362525939941
Epoch 280, training loss: 1.0600459575653076 = 0.37219369411468506 + 0.1 * 6.8785223960876465
Epoch 280, val loss: 0.7082895636558533
Epoch 290, training loss: 1.0169378519058228 = 0.32978519797325134 + 0.1 * 6.87152624130249
Epoch 290, val loss: 0.6890988945960999
Epoch 300, training loss: 0.9778897166252136 = 0.29076480865478516 + 0.1 * 6.871249198913574
Epoch 300, val loss: 0.6742404103279114
Epoch 310, training loss: 0.941351056098938 = 0.25564953684806824 + 0.1 * 6.857015609741211
Epoch 310, val loss: 0.6633709669113159
Epoch 320, training loss: 0.9090887904167175 = 0.22426724433898926 + 0.1 * 6.848215103149414
Epoch 320, val loss: 0.6561602354049683
Epoch 330, training loss: 0.8805649280548096 = 0.19637827575206757 + 0.1 * 6.8418660163879395
Epoch 330, val loss: 0.6521877646446228
Epoch 340, training loss: 0.8555538654327393 = 0.17200438678264618 + 0.1 * 6.835494518280029
Epoch 340, val loss: 0.6510521173477173
Epoch 350, training loss: 0.8327289819717407 = 0.15088090300559998 + 0.1 * 6.818480491638184
Epoch 350, val loss: 0.6520881652832031
Epoch 360, training loss: 0.8141123652458191 = 0.13259007036685944 + 0.1 * 6.815223217010498
Epoch 360, val loss: 0.655179500579834
Epoch 370, training loss: 0.7973761558532715 = 0.11692299693822861 + 0.1 * 6.804531574249268
Epoch 370, val loss: 0.6601470112800598
Epoch 380, training loss: 0.7831591367721558 = 0.10352489352226257 + 0.1 * 6.796342372894287
Epoch 380, val loss: 0.6662283539772034
Epoch 390, training loss: 0.771747887134552 = 0.0920180082321167 + 0.1 * 6.797298908233643
Epoch 390, val loss: 0.673506498336792
Epoch 400, training loss: 0.7605578303337097 = 0.08220233768224716 + 0.1 * 6.783555030822754
Epoch 400, val loss: 0.6816322803497314
Epoch 410, training loss: 0.7513816952705383 = 0.07378564774990082 + 0.1 * 6.775960445404053
Epoch 410, val loss: 0.690229058265686
Epoch 420, training loss: 0.743798017501831 = 0.06651826947927475 + 0.1 * 6.772797584533691
Epoch 420, val loss: 0.6993622779846191
Epoch 430, training loss: 0.7362412810325623 = 0.06024741753935814 + 0.1 * 6.759938716888428
Epoch 430, val loss: 0.7086400389671326
Epoch 440, training loss: 0.7298944592475891 = 0.05478328838944435 + 0.1 * 6.7511115074157715
Epoch 440, val loss: 0.7181265354156494
Epoch 450, training loss: 0.7249960899353027 = 0.050012435764074326 + 0.1 * 6.7498369216918945
Epoch 450, val loss: 0.727631688117981
Epoch 460, training loss: 0.7199987173080444 = 0.04585598409175873 + 0.1 * 6.741426944732666
Epoch 460, val loss: 0.7369782328605652
Epoch 470, training loss: 0.7155165076255798 = 0.04219309985637665 + 0.1 * 6.73323392868042
Epoch 470, val loss: 0.7464357018470764
Epoch 480, training loss: 0.7116878032684326 = 0.03894808143377304 + 0.1 * 6.727397441864014
Epoch 480, val loss: 0.7556286454200745
Epoch 490, training loss: 0.7102881073951721 = 0.03606283292174339 + 0.1 * 6.742252349853516
Epoch 490, val loss: 0.7646834850311279
Epoch 500, training loss: 0.704916775226593 = 0.03350698947906494 + 0.1 * 6.71409797668457
Epoch 500, val loss: 0.7735909819602966
Epoch 510, training loss: 0.7017784714698792 = 0.03122178465127945 + 0.1 * 6.70556640625
Epoch 510, val loss: 0.7821853160858154
Epoch 520, training loss: 0.7005535960197449 = 0.029163023456931114 + 0.1 * 6.7139058113098145
Epoch 520, val loss: 0.7907235622406006
Epoch 530, training loss: 0.696504533290863 = 0.027307607233524323 + 0.1 * 6.69196891784668
Epoch 530, val loss: 0.7990019917488098
Epoch 540, training loss: 0.6945624351501465 = 0.02562646195292473 + 0.1 * 6.689359188079834
Epoch 540, val loss: 0.8071172833442688
Epoch 550, training loss: 0.6954370737075806 = 0.024101529270410538 + 0.1 * 6.71335506439209
Epoch 550, val loss: 0.8149269223213196
Epoch 560, training loss: 0.6907541155815125 = 0.02272002398967743 + 0.1 * 6.680340766906738
Epoch 560, val loss: 0.822570264339447
Epoch 570, training loss: 0.6884082555770874 = 0.02146078273653984 + 0.1 * 6.6694746017456055
Epoch 570, val loss: 0.8299412131309509
Epoch 580, training loss: 0.6869383454322815 = 0.02030632458627224 + 0.1 * 6.666319847106934
Epoch 580, val loss: 0.8372982144355774
Epoch 590, training loss: 0.6856884360313416 = 0.01924779638648033 + 0.1 * 6.6644062995910645
Epoch 590, val loss: 0.8443342447280884
Epoch 600, training loss: 0.6847177743911743 = 0.018272778019309044 + 0.1 * 6.664450168609619
Epoch 600, val loss: 0.8513521552085876
Epoch 610, training loss: 0.6818722486495972 = 0.017377041280269623 + 0.1 * 6.644952297210693
Epoch 610, val loss: 0.8580073714256287
Epoch 620, training loss: 0.6817572712898254 = 0.016548272222280502 + 0.1 * 6.652089595794678
Epoch 620, val loss: 0.8644565343856812
Epoch 630, training loss: 0.6823217272758484 = 0.01578052155673504 + 0.1 * 6.665411949157715
Epoch 630, val loss: 0.8710013031959534
Epoch 640, training loss: 0.6795414090156555 = 0.01507403515279293 + 0.1 * 6.644673824310303
Epoch 640, val loss: 0.8771528601646423
Epoch 650, training loss: 0.6776548027992249 = 0.014414298348128796 + 0.1 * 6.632404804229736
Epoch 650, val loss: 0.8831549286842346
Epoch 660, training loss: 0.6751367449760437 = 0.013798209838569164 + 0.1 * 6.613385200500488
Epoch 660, val loss: 0.8891695141792297
Epoch 670, training loss: 0.6796285510063171 = 0.013222825713455677 + 0.1 * 6.66405725479126
Epoch 670, val loss: 0.8950016498565674
Epoch 680, training loss: 0.6752958297729492 = 0.012689504772424698 + 0.1 * 6.626062870025635
Epoch 680, val loss: 0.9008209109306335
Epoch 690, training loss: 0.6727527379989624 = 0.012192023918032646 + 0.1 * 6.605607032775879
Epoch 690, val loss: 0.9060976505279541
Epoch 700, training loss: 0.6734892129898071 = 0.011721937917172909 + 0.1 * 6.617672920227051
Epoch 700, val loss: 0.9115070700645447
Epoch 710, training loss: 0.6703959107398987 = 0.011282005347311497 + 0.1 * 6.59113883972168
Epoch 710, val loss: 0.9168621301651001
Epoch 720, training loss: 0.6724258661270142 = 0.010867075994610786 + 0.1 * 6.6155877113342285
Epoch 720, val loss: 0.9220255613327026
Epoch 730, training loss: 0.6713763475418091 = 0.010476949624717236 + 0.1 * 6.608994007110596
Epoch 730, val loss: 0.9270470142364502
Epoch 740, training loss: 0.6682063341140747 = 0.010109864175319672 + 0.1 * 6.580965042114258
Epoch 740, val loss: 0.9320862889289856
Epoch 750, training loss: 0.6681817770004272 = 0.009764832444489002 + 0.1 * 6.584168910980225
Epoch 750, val loss: 0.9368162751197815
Epoch 760, training loss: 0.6676558256149292 = 0.009437833912670612 + 0.1 * 6.582179546356201
Epoch 760, val loss: 0.9415861368179321
Epoch 770, training loss: 0.6675509214401245 = 0.00912812352180481 + 0.1 * 6.584228038787842
Epoch 770, val loss: 0.9462111592292786
Epoch 780, training loss: 0.6670825481414795 = 0.008835301734507084 + 0.1 * 6.582472324371338
Epoch 780, val loss: 0.9509202837944031
Epoch 790, training loss: 0.6652209758758545 = 0.008558595553040504 + 0.1 * 6.566624164581299
Epoch 790, val loss: 0.9553321003913879
Epoch 800, training loss: 0.6661304831504822 = 0.008296041749417782 + 0.1 * 6.578344821929932
Epoch 800, val loss: 0.9596539735794067
Epoch 810, training loss: 0.6648963689804077 = 0.008046120405197144 + 0.1 * 6.568502902984619
Epoch 810, val loss: 0.9640724658966064
Epoch 820, training loss: 0.663418173789978 = 0.007809790782630444 + 0.1 * 6.556083679199219
Epoch 820, val loss: 0.9682623744010925
Epoch 830, training loss: 0.6630080342292786 = 0.007583016995340586 + 0.1 * 6.5542497634887695
Epoch 830, val loss: 0.9723418354988098
Epoch 840, training loss: 0.6642401814460754 = 0.007367496378719807 + 0.1 * 6.568726539611816
Epoch 840, val loss: 0.9766181111335754
Epoch 850, training loss: 0.6623599529266357 = 0.007163605187088251 + 0.1 * 6.5519633293151855
Epoch 850, val loss: 0.9805721640586853
Epoch 860, training loss: 0.6613152027130127 = 0.006967656314373016 + 0.1 * 6.543475151062012
Epoch 860, val loss: 0.9845133423805237
Epoch 870, training loss: 0.661250650882721 = 0.006782262586057186 + 0.1 * 6.544683933258057
Epoch 870, val loss: 0.988330066204071
Epoch 880, training loss: 0.6603236198425293 = 0.00660368287935853 + 0.1 * 6.5371994972229
Epoch 880, val loss: 0.9921677112579346
Epoch 890, training loss: 0.6617060303688049 = 0.006433106493204832 + 0.1 * 6.552728652954102
Epoch 890, val loss: 0.9958885908126831
Epoch 900, training loss: 0.6607964038848877 = 0.006269730161875486 + 0.1 * 6.545266628265381
Epoch 900, val loss: 0.9996066689491272
Epoch 910, training loss: 0.6585385799407959 = 0.006113334558904171 + 0.1 * 6.524252414703369
Epoch 910, val loss: 1.003265142440796
Epoch 920, training loss: 0.6597560048103333 = 0.005963869392871857 + 0.1 * 6.53792142868042
Epoch 920, val loss: 1.006768822669983
Epoch 930, training loss: 0.6585418581962585 = 0.0058203102089464664 + 0.1 * 6.527215003967285
Epoch 930, val loss: 1.010263204574585
Epoch 940, training loss: 0.6589549779891968 = 0.005682057235389948 + 0.1 * 6.532729148864746
Epoch 940, val loss: 1.013771891593933
Epoch 950, training loss: 0.6582468748092651 = 0.005551034584641457 + 0.1 * 6.526957988739014
Epoch 950, val loss: 1.0171446800231934
Epoch 960, training loss: 0.6569516658782959 = 0.0054237800650298595 + 0.1 * 6.5152788162231445
Epoch 960, val loss: 1.0203968286514282
Epoch 970, training loss: 0.6596463322639465 = 0.00530185317620635 + 0.1 * 6.543444633483887
Epoch 970, val loss: 1.0237199068069458
Epoch 980, training loss: 0.6568073034286499 = 0.005184297449886799 + 0.1 * 6.51623010635376
Epoch 980, val loss: 1.0269535779953003
Epoch 990, training loss: 0.6565908193588257 = 0.005071823485195637 + 0.1 * 6.515190124511719
Epoch 990, val loss: 1.0300849676132202
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.806119918823242 = 1.9464341402053833 + 0.1 * 8.596858024597168
Epoch 0, val loss: 1.942793607711792
Epoch 10, training loss: 2.7960898876190186 = 1.9364101886749268 + 0.1 * 8.596796035766602
Epoch 10, val loss: 1.9331990480422974
Epoch 20, training loss: 2.783783435821533 = 1.924132227897644 + 0.1 * 8.596511840820312
Epoch 20, val loss: 1.9209574460983276
Epoch 30, training loss: 2.7664031982421875 = 1.9069808721542358 + 0.1 * 8.594223022460938
Epoch 30, val loss: 1.903437852859497
Epoch 40, training loss: 2.7387912273406982 = 1.8813328742980957 + 0.1 * 8.574584007263184
Epoch 40, val loss: 1.8771955966949463
Epoch 50, training loss: 2.689667224884033 = 1.844773769378662 + 0.1 * 8.448933601379395
Epoch 50, val loss: 1.8411849737167358
Epoch 60, training loss: 2.6254770755767822 = 1.8032710552215576 + 0.1 * 8.222060203552246
Epoch 60, val loss: 1.8036777973175049
Epoch 70, training loss: 2.5556180477142334 = 1.7648299932479858 + 0.1 * 7.907879829406738
Epoch 70, val loss: 1.7709550857543945
Epoch 80, training loss: 2.464790105819702 = 1.7232681512832642 + 0.1 * 7.415219783782959
Epoch 80, val loss: 1.7345855236053467
Epoch 90, training loss: 2.392881393432617 = 1.6712876558303833 + 0.1 * 7.21593713760376
Epoch 90, val loss: 1.6879138946533203
Epoch 100, training loss: 2.318011999130249 = 1.6034226417541504 + 0.1 * 7.14589262008667
Epoch 100, val loss: 1.6280277967453003
Epoch 110, training loss: 2.2335753440856934 = 1.5222282409667969 + 0.1 * 7.113471508026123
Epoch 110, val loss: 1.5600699186325073
Epoch 120, training loss: 2.146118402481079 = 1.4370274543762207 + 0.1 * 7.090909481048584
Epoch 120, val loss: 1.4904654026031494
Epoch 130, training loss: 2.0590906143188477 = 1.3523982763290405 + 0.1 * 7.066923141479492
Epoch 130, val loss: 1.4223082065582275
Epoch 140, training loss: 1.9711761474609375 = 1.2672693729400635 + 0.1 * 7.039066791534424
Epoch 140, val loss: 1.3558573722839355
Epoch 150, training loss: 1.8806235790252686 = 1.1800824403762817 + 0.1 * 7.0054121017456055
Epoch 150, val loss: 1.2870211601257324
Epoch 160, training loss: 1.7912793159484863 = 1.0937259197235107 + 0.1 * 6.975533962249756
Epoch 160, val loss: 1.2189102172851562
Epoch 170, training loss: 1.7054191827774048 = 1.009912371635437 + 0.1 * 6.955068111419678
Epoch 170, val loss: 1.1525479555130005
Epoch 180, training loss: 1.622429609298706 = 0.9293384552001953 + 0.1 * 6.930910587310791
Epoch 180, val loss: 1.0894683599472046
Epoch 190, training loss: 1.5419447422027588 = 0.8509615063667297 + 0.1 * 6.909831523895264
Epoch 190, val loss: 1.0287656784057617
Epoch 200, training loss: 1.464935064315796 = 0.7756832242012024 + 0.1 * 6.892518997192383
Epoch 200, val loss: 0.971489667892456
Epoch 210, training loss: 1.393648624420166 = 0.7060723900794983 + 0.1 * 6.875761985778809
Epoch 210, val loss: 0.919895350933075
Epoch 220, training loss: 1.3307569026947021 = 0.6446304321289062 + 0.1 * 6.861263751983643
Epoch 220, val loss: 0.8769479990005493
Epoch 230, training loss: 1.2757186889648438 = 0.591219425201416 + 0.1 * 6.844991683959961
Epoch 230, val loss: 0.8427193760871887
Epoch 240, training loss: 1.228745937347412 = 0.5454477667808533 + 0.1 * 6.832981109619141
Epoch 240, val loss: 0.8173002004623413
Epoch 250, training loss: 1.1872466802597046 = 0.5055403113365173 + 0.1 * 6.817063808441162
Epoch 250, val loss: 0.7987465262413025
Epoch 260, training loss: 1.150644063949585 = 0.4696880280971527 + 0.1 * 6.809560298919678
Epoch 260, val loss: 0.7849738001823425
Epoch 270, training loss: 1.1161333322525024 = 0.4369870722293854 + 0.1 * 6.791462421417236
Epoch 270, val loss: 0.7746965885162354
Epoch 280, training loss: 1.0853122472763062 = 0.40630635619163513 + 0.1 * 6.7900590896606445
Epoch 280, val loss: 0.7666199803352356
Epoch 290, training loss: 1.0546305179595947 = 0.3775705099105835 + 0.1 * 6.770599842071533
Epoch 290, val loss: 0.7606754302978516
Epoch 300, training loss: 1.0265288352966309 = 0.3509168326854706 + 0.1 * 6.756119728088379
Epoch 300, val loss: 0.7568489909172058
Epoch 310, training loss: 1.0025017261505127 = 0.32655516266822815 + 0.1 * 6.759465217590332
Epoch 310, val loss: 0.7551067471504211
Epoch 320, training loss: 0.9776700735092163 = 0.3044933080673218 + 0.1 * 6.731767654418945
Epoch 320, val loss: 0.7552858591079712
Epoch 330, training loss: 0.9578076601028442 = 0.28426405787467957 + 0.1 * 6.73543643951416
Epoch 330, val loss: 0.7570919990539551
Epoch 340, training loss: 0.9382861852645874 = 0.2654804289340973 + 0.1 * 6.728056907653809
Epoch 340, val loss: 0.7600510716438293
Epoch 350, training loss: 0.9181302785873413 = 0.2474927008152008 + 0.1 * 6.706376075744629
Epoch 350, val loss: 0.7637176513671875
Epoch 360, training loss: 0.9010497331619263 = 0.2297164350748062 + 0.1 * 6.7133331298828125
Epoch 360, val loss: 0.7676030397415161
Epoch 370, training loss: 0.8810126781463623 = 0.21168695390224457 + 0.1 * 6.693256855010986
Epoch 370, val loss: 0.7713062167167664
Epoch 380, training loss: 0.8623883724212646 = 0.19329553842544556 + 0.1 * 6.690927982330322
Epoch 380, val loss: 0.7747217416763306
Epoch 390, training loss: 0.8429944515228271 = 0.17506450414657593 + 0.1 * 6.679299354553223
Epoch 390, val loss: 0.7779706716537476
Epoch 400, training loss: 0.824588418006897 = 0.1576113998889923 + 0.1 * 6.669769763946533
Epoch 400, val loss: 0.7816242575645447
Epoch 410, training loss: 0.8089303970336914 = 0.14160007238388062 + 0.1 * 6.673303127288818
Epoch 410, val loss: 0.7861729860305786
Epoch 420, training loss: 0.7962052226066589 = 0.1273774951696396 + 0.1 * 6.688277244567871
Epoch 420, val loss: 0.7919350862503052
Epoch 430, training loss: 0.780731737613678 = 0.1150059700012207 + 0.1 * 6.657257556915283
Epoch 430, val loss: 0.79899001121521
Epoch 440, training loss: 0.7683762311935425 = 0.10427757352590561 + 0.1 * 6.640986919403076
Epoch 440, val loss: 0.8073528409004211
Epoch 450, training loss: 0.7596404552459717 = 0.09495218098163605 + 0.1 * 6.6468825340271
Epoch 450, val loss: 0.8167835474014282
Epoch 460, training loss: 0.751122772693634 = 0.08680988103151321 + 0.1 * 6.643128871917725
Epoch 460, val loss: 0.8270059823989868
Epoch 470, training loss: 0.7439523339271545 = 0.07966931164264679 + 0.1 * 6.6428303718566895
Epoch 470, val loss: 0.8378117680549622
Epoch 480, training loss: 0.7354492545127869 = 0.07335156947374344 + 0.1 * 6.62097692489624
Epoch 480, val loss: 0.8489588499069214
Epoch 490, training loss: 0.7304554581642151 = 0.06773318350315094 + 0.1 * 6.627222537994385
Epoch 490, val loss: 0.8603224754333496
Epoch 500, training loss: 0.7241960167884827 = 0.06270880252122879 + 0.1 * 6.614871978759766
Epoch 500, val loss: 0.8717349171638489
Epoch 510, training loss: 0.7194745540618896 = 0.0581839382648468 + 0.1 * 6.612905979156494
Epoch 510, val loss: 0.8832489848136902
Epoch 520, training loss: 0.7156870365142822 = 0.054094910621643066 + 0.1 * 6.6159210205078125
Epoch 520, val loss: 0.894733726978302
Epoch 530, training loss: 0.7107697129249573 = 0.05040038004517555 + 0.1 * 6.603693008422852
Epoch 530, val loss: 0.9061456918716431
Epoch 540, training loss: 0.7064412236213684 = 0.04704847186803818 + 0.1 * 6.593927383422852
Epoch 540, val loss: 0.9173556566238403
Epoch 550, training loss: 0.7024213671684265 = 0.043996039777994156 + 0.1 * 6.584253311157227
Epoch 550, val loss: 0.9285017251968384
Epoch 560, training loss: 0.7000299692153931 = 0.04121767356991768 + 0.1 * 6.588122844696045
Epoch 560, val loss: 0.9394347667694092
Epoch 570, training loss: 0.6964606046676636 = 0.03868184611201286 + 0.1 * 6.577787399291992
Epoch 570, val loss: 0.9501133561134338
Epoch 580, training loss: 0.6946588158607483 = 0.03635778650641441 + 0.1 * 6.583010673522949
Epoch 580, val loss: 0.9606961607933044
Epoch 590, training loss: 0.6947667598724365 = 0.03423082455992699 + 0.1 * 6.605359077453613
Epoch 590, val loss: 0.9710456132888794
Epoch 600, training loss: 0.688713788986206 = 0.03227987885475159 + 0.1 * 6.5643391609191895
Epoch 600, val loss: 0.9811426401138306
Epoch 610, training loss: 0.688256025314331 = 0.03048381395637989 + 0.1 * 6.57772159576416
Epoch 610, val loss: 0.9910879731178284
Epoch 620, training loss: 0.6851956844329834 = 0.028828587383031845 + 0.1 * 6.563671112060547
Epoch 620, val loss: 1.000754475593567
Epoch 630, training loss: 0.6828884482383728 = 0.027299968525767326 + 0.1 * 6.555884838104248
Epoch 630, val loss: 1.0102760791778564
Epoch 640, training loss: 0.6817387342453003 = 0.025885548442602158 + 0.1 * 6.558532238006592
Epoch 640, val loss: 1.019603967666626
Epoch 650, training loss: 0.679931640625 = 0.02457510679960251 + 0.1 * 6.55356502532959
Epoch 650, val loss: 1.0286530256271362
Epoch 660, training loss: 0.6789771914482117 = 0.023361021652817726 + 0.1 * 6.556161403656006
Epoch 660, val loss: 1.0375468730926514
Epoch 670, training loss: 0.6769686341285706 = 0.022235965356230736 + 0.1 * 6.547326564788818
Epoch 670, val loss: 1.0461922883987427
Epoch 680, training loss: 0.6768835783004761 = 0.02118939347565174 + 0.1 * 6.556941986083984
Epoch 680, val loss: 1.054647445678711
Epoch 690, training loss: 0.674555778503418 = 0.020214645192027092 + 0.1 * 6.5434112548828125
Epoch 690, val loss: 1.0629254579544067
Epoch 700, training loss: 0.6730669736862183 = 0.01930508203804493 + 0.1 * 6.537618637084961
Epoch 700, val loss: 1.0710252523422241
Epoch 710, training loss: 0.6754246950149536 = 0.01845697872340679 + 0.1 * 6.569676876068115
Epoch 710, val loss: 1.0789936780929565
Epoch 720, training loss: 0.671081006526947 = 0.017666256055235863 + 0.1 * 6.534147262573242
Epoch 720, val loss: 1.086708903312683
Epoch 730, training loss: 0.670059084892273 = 0.016926728188991547 + 0.1 * 6.531323432922363
Epoch 730, val loss: 1.0941709280014038
Epoch 740, training loss: 0.6708680987358093 = 0.01623263768851757 + 0.1 * 6.5463547706604
Epoch 740, val loss: 1.101624846458435
Epoch 750, training loss: 0.6691370606422424 = 0.015582440420985222 + 0.1 * 6.53554630279541
Epoch 750, val loss: 1.1088229417800903
Epoch 760, training loss: 0.6672067642211914 = 0.014972526580095291 + 0.1 * 6.522342681884766
Epoch 760, val loss: 1.115861177444458
Epoch 770, training loss: 0.6670963168144226 = 0.014397541992366314 + 0.1 * 6.526987552642822
Epoch 770, val loss: 1.1227352619171143
Epoch 780, training loss: 0.6664220690727234 = 0.013855630531907082 + 0.1 * 6.525664329528809
Epoch 780, val loss: 1.129554033279419
Epoch 790, training loss: 0.6644296050071716 = 0.013347177766263485 + 0.1 * 6.510824203491211
Epoch 790, val loss: 1.1360995769500732
Epoch 800, training loss: 0.6642067432403564 = 0.012864975258708 + 0.1 * 6.513417720794678
Epoch 800, val loss: 1.1425803899765015
Epoch 810, training loss: 0.6671308875083923 = 0.012409467250108719 + 0.1 * 6.547214508056641
Epoch 810, val loss: 1.149005651473999
Epoch 820, training loss: 0.6629341244697571 = 0.011981870047748089 + 0.1 * 6.509522438049316
Epoch 820, val loss: 1.1551775932312012
Epoch 830, training loss: 0.6624470949172974 = 0.011576405726373196 + 0.1 * 6.508706569671631
Epoch 830, val loss: 1.1611744165420532
Epoch 840, training loss: 0.6629422903060913 = 0.011191616766154766 + 0.1 * 6.5175065994262695
Epoch 840, val loss: 1.167201042175293
Epoch 850, training loss: 0.6609880924224854 = 0.010828832164406776 + 0.1 * 6.50159215927124
Epoch 850, val loss: 1.1729967594146729
Epoch 860, training loss: 0.6599774360656738 = 0.010483535937964916 + 0.1 * 6.494938373565674
Epoch 860, val loss: 1.178665041923523
Epoch 870, training loss: 0.6601685881614685 = 0.010155035182833672 + 0.1 * 6.50013542175293
Epoch 870, val loss: 1.1842714548110962
Epoch 880, training loss: 0.659546971321106 = 0.009842890314757824 + 0.1 * 6.497040748596191
Epoch 880, val loss: 1.1897696256637573
Epoch 890, training loss: 0.658232569694519 = 0.009545094333589077 + 0.1 * 6.486874580383301
Epoch 890, val loss: 1.195211410522461
Epoch 900, training loss: 0.6594417095184326 = 0.009262223728001118 + 0.1 * 6.501794815063477
Epoch 900, val loss: 1.2004468441009521
Epoch 910, training loss: 0.6589967608451843 = 0.008992641232907772 + 0.1 * 6.500041484832764
Epoch 910, val loss: 1.205679178237915
Epoch 920, training loss: 0.657001256942749 = 0.008736656047403812 + 0.1 * 6.482645511627197
Epoch 920, val loss: 1.2107349634170532
Epoch 930, training loss: 0.6567717790603638 = 0.008491978980600834 + 0.1 * 6.482797622680664
Epoch 930, val loss: 1.215657114982605
Epoch 940, training loss: 0.6575050950050354 = 0.008257512003183365 + 0.1 * 6.492475986480713
Epoch 940, val loss: 1.220616340637207
Epoch 950, training loss: 0.6575446128845215 = 0.008033672347664833 + 0.1 * 6.4951090812683105
Epoch 950, val loss: 1.225489854812622
Epoch 960, training loss: 0.6554780006408691 = 0.007819774560630322 + 0.1 * 6.476582050323486
Epoch 960, val loss: 1.230150818824768
Epoch 970, training loss: 0.6560540795326233 = 0.007615254260599613 + 0.1 * 6.4843878746032715
Epoch 970, val loss: 1.2348774671554565
Epoch 980, training loss: 0.655290961265564 = 0.007419708650559187 + 0.1 * 6.478712558746338
Epoch 980, val loss: 1.239380121231079
Epoch 990, training loss: 0.6557987332344055 = 0.007232308853417635 + 0.1 * 6.485664367675781
Epoch 990, val loss: 1.2438491582870483
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8281497100685293
The final CL Acc:0.79753, 0.02710, The final GNN Acc:0.83500, 0.00485
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10520])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.819216728210449 = 1.9595335721969604 + 0.1 * 8.596832275390625
Epoch 0, val loss: 1.956323266029358
Epoch 10, training loss: 2.808546543121338 = 1.9488694667816162 + 0.1 * 8.596769332885742
Epoch 10, val loss: 1.9454309940338135
Epoch 20, training loss: 2.795973777770996 = 1.9363343715667725 + 0.1 * 8.596393585205078
Epoch 20, val loss: 1.9326086044311523
Epoch 30, training loss: 2.7787580490112305 = 1.9194058179855347 + 0.1 * 8.593523025512695
Epoch 30, val loss: 1.9154410362243652
Epoch 40, training loss: 2.7519149780273438 = 1.8947783708572388 + 0.1 * 8.571366310119629
Epoch 40, val loss: 1.8906455039978027
Epoch 50, training loss: 2.705929756164551 = 1.8601826429367065 + 0.1 * 8.457470893859863
Epoch 50, val loss: 1.8571587800979614
Epoch 60, training loss: 2.641582727432251 = 1.8208422660827637 + 0.1 * 8.207405090332031
Epoch 60, val loss: 1.821691632270813
Epoch 70, training loss: 2.5831260681152344 = 1.787524938583374 + 0.1 * 7.9560112953186035
Epoch 70, val loss: 1.7938493490219116
Epoch 80, training loss: 2.512462615966797 = 1.7572263479232788 + 0.1 * 7.552363395690918
Epoch 80, val loss: 1.7676225900650024
Epoch 90, training loss: 2.4456794261932373 = 1.7206130027770996 + 0.1 * 7.2506632804870605
Epoch 90, val loss: 1.7367165088653564
Epoch 100, training loss: 2.3830950260162354 = 1.6719911098480225 + 0.1 * 7.111039638519287
Epoch 100, val loss: 1.6962642669677734
Epoch 110, training loss: 2.3088631629943848 = 1.6068456172943115 + 0.1 * 7.020175933837891
Epoch 110, val loss: 1.641607403755188
Epoch 120, training loss: 2.2238285541534424 = 1.526576280593872 + 0.1 * 6.972522735595703
Epoch 120, val loss: 1.5757020711898804
Epoch 130, training loss: 2.131847858428955 = 1.4377834796905518 + 0.1 * 6.940644264221191
Epoch 130, val loss: 1.5048776865005493
Epoch 140, training loss: 2.0381791591644287 = 1.346300482749939 + 0.1 * 6.918786525726318
Epoch 140, val loss: 1.4340282678604126
Epoch 150, training loss: 1.944697618484497 = 1.2549868822097778 + 0.1 * 6.897106647491455
Epoch 150, val loss: 1.3648124933242798
Epoch 160, training loss: 1.8528566360473633 = 1.1645551919937134 + 0.1 * 6.88301420211792
Epoch 160, val loss: 1.2974836826324463
Epoch 170, training loss: 1.7666665315628052 = 1.0797213315963745 + 0.1 * 6.869451999664307
Epoch 170, val loss: 1.236251711845398
Epoch 180, training loss: 1.6890318393707275 = 1.0029728412628174 + 0.1 * 6.860589504241943
Epoch 180, val loss: 1.182366967201233
Epoch 190, training loss: 1.618790864944458 = 0.933547854423523 + 0.1 * 6.852429389953613
Epoch 190, val loss: 1.1350359916687012
Epoch 200, training loss: 1.5528101921081543 = 0.8681235313415527 + 0.1 * 6.846865653991699
Epoch 200, val loss: 1.0914783477783203
Epoch 210, training loss: 1.488111972808838 = 0.8041203022003174 + 0.1 * 6.839916229248047
Epoch 210, val loss: 1.049795150756836
Epoch 220, training loss: 1.4239757061004639 = 0.7406007051467896 + 0.1 * 6.8337507247924805
Epoch 220, val loss: 1.0098294019699097
Epoch 230, training loss: 1.3614206314086914 = 0.678622841835022 + 0.1 * 6.827977657318115
Epoch 230, val loss: 0.9732159376144409
Epoch 240, training loss: 1.3026652336120605 = 0.6205286383628845 + 0.1 * 6.821366310119629
Epoch 240, val loss: 0.942340075969696
Epoch 250, training loss: 1.247714638710022 = 0.5665825009346008 + 0.1 * 6.811321258544922
Epoch 250, val loss: 0.9178940653800964
Epoch 260, training loss: 1.1974432468414307 = 0.5170490741729736 + 0.1 * 6.8039422035217285
Epoch 260, val loss: 0.8996163606643677
Epoch 270, training loss: 1.1513733863830566 = 0.4720079302787781 + 0.1 * 6.793653964996338
Epoch 270, val loss: 0.8867802023887634
Epoch 280, training loss: 1.1089071035385132 = 0.4305711090564728 + 0.1 * 6.783360004425049
Epoch 280, val loss: 0.8779751658439636
Epoch 290, training loss: 1.0708205699920654 = 0.39226192235946655 + 0.1 * 6.785585880279541
Epoch 290, val loss: 0.8721815347671509
Epoch 300, training loss: 1.0335384607315063 = 0.3567165732383728 + 0.1 * 6.768218994140625
Epoch 300, val loss: 0.8687368035316467
Epoch 310, training loss: 0.9991864562034607 = 0.32356083393096924 + 0.1 * 6.756256103515625
Epoch 310, val loss: 0.8671738505363464
Epoch 320, training loss: 0.9671316146850586 = 0.292504221200943 + 0.1 * 6.746273994445801
Epoch 320, val loss: 0.8672389388084412
Epoch 330, training loss: 0.9372626543045044 = 0.2635076642036438 + 0.1 * 6.737549781799316
Epoch 330, val loss: 0.8685253262519836
Epoch 340, training loss: 0.9099048972129822 = 0.23671786487102509 + 0.1 * 6.731870174407959
Epoch 340, val loss: 0.8710521459579468
Epoch 350, training loss: 0.8849124908447266 = 0.21208970248699188 + 0.1 * 6.728227615356445
Epoch 350, val loss: 0.8747444152832031
Epoch 360, training loss: 0.862263560295105 = 0.18969577550888062 + 0.1 * 6.725677490234375
Epoch 360, val loss: 0.8795498013496399
Epoch 370, training loss: 0.8406410217285156 = 0.1695808619260788 + 0.1 * 6.710601329803467
Epoch 370, val loss: 0.8854348063468933
Epoch 380, training loss: 0.8212133646011353 = 0.15159094333648682 + 0.1 * 6.696224212646484
Epoch 380, val loss: 0.8923265933990479
Epoch 390, training loss: 0.8047130107879639 = 0.13562896847724915 + 0.1 * 6.690840721130371
Epoch 390, val loss: 0.9003049731254578
Epoch 400, training loss: 0.7910381555557251 = 0.12160441279411316 + 0.1 * 6.694337844848633
Epoch 400, val loss: 0.9091601371765137
Epoch 410, training loss: 0.7772154211997986 = 0.10932587087154388 + 0.1 * 6.678895473480225
Epoch 410, val loss: 0.918889045715332
Epoch 420, training loss: 0.7653157114982605 = 0.0985371470451355 + 0.1 * 6.66778564453125
Epoch 420, val loss: 0.9293549656867981
Epoch 430, training loss: 0.7555193305015564 = 0.08907242864370346 + 0.1 * 6.664469242095947
Epoch 430, val loss: 0.9403935074806213
Epoch 440, training loss: 0.7467103600502014 = 0.08080877363681793 + 0.1 * 6.659015655517578
Epoch 440, val loss: 0.9517775774002075
Epoch 450, training loss: 0.7401726245880127 = 0.07352858781814575 + 0.1 * 6.666440486907959
Epoch 450, val loss: 0.9634884595870972
Epoch 460, training loss: 0.7319247126579285 = 0.06710676103830338 + 0.1 * 6.648179054260254
Epoch 460, val loss: 0.9753345847129822
Epoch 470, training loss: 0.7254161238670349 = 0.061401668936014175 + 0.1 * 6.640144348144531
Epoch 470, val loss: 0.9872991442680359
Epoch 480, training loss: 0.7200959324836731 = 0.05633002519607544 + 0.1 * 6.637659072875977
Epoch 480, val loss: 0.999178946018219
Epoch 490, training loss: 0.7147091627120972 = 0.05182694271206856 + 0.1 * 6.628821849822998
Epoch 490, val loss: 1.0108758211135864
Epoch 500, training loss: 0.7101757526397705 = 0.04779691621661186 + 0.1 * 6.623788356781006
Epoch 500, val loss: 1.0225462913513184
Epoch 510, training loss: 0.7071347832679749 = 0.0441778339445591 + 0.1 * 6.6295695304870605
Epoch 510, val loss: 1.0340474843978882
Epoch 520, training loss: 0.7026941180229187 = 0.040925320237874985 + 0.1 * 6.617687702178955
Epoch 520, val loss: 1.0452920198440552
Epoch 530, training loss: 0.7009131908416748 = 0.0379975251853466 + 0.1 * 6.629156589508057
Epoch 530, val loss: 1.056395173072815
Epoch 540, training loss: 0.6960936188697815 = 0.0353650264441967 + 0.1 * 6.607285499572754
Epoch 540, val loss: 1.0671179294586182
Epoch 550, training loss: 0.6927765607833862 = 0.03298841789364815 + 0.1 * 6.597881317138672
Epoch 550, val loss: 1.0777243375778198
Epoch 560, training loss: 0.6902356147766113 = 0.030834505334496498 + 0.1 * 6.594010829925537
Epoch 560, val loss: 1.0880694389343262
Epoch 570, training loss: 0.6889154314994812 = 0.0288782250136137 + 0.1 * 6.600372314453125
Epoch 570, val loss: 1.0982329845428467
Epoch 580, training loss: 0.6858686208724976 = 0.027103066444396973 + 0.1 * 6.587655544281006
Epoch 580, val loss: 1.1081140041351318
Epoch 590, training loss: 0.6832237243652344 = 0.02548895962536335 + 0.1 * 6.577347278594971
Epoch 590, val loss: 1.1177904605865479
Epoch 600, training loss: 0.6841673851013184 = 0.02401241660118103 + 0.1 * 6.60154914855957
Epoch 600, val loss: 1.1273095607757568
Epoch 610, training loss: 0.6806361675262451 = 0.0226663239300251 + 0.1 * 6.579698085784912
Epoch 610, val loss: 1.136458396911621
Epoch 620, training loss: 0.6780622601509094 = 0.02143419161438942 + 0.1 * 6.566280841827393
Epoch 620, val loss: 1.1455062627792358
Epoch 630, training loss: 0.6767130494117737 = 0.020299186930060387 + 0.1 * 6.564138412475586
Epoch 630, val loss: 1.1543625593185425
Epoch 640, training loss: 0.6775590777397156 = 0.0192506555467844 + 0.1 * 6.5830841064453125
Epoch 640, val loss: 1.1630566120147705
Epoch 650, training loss: 0.6757174134254456 = 0.01828468032181263 + 0.1 * 6.57432746887207
Epoch 650, val loss: 1.1714353561401367
Epoch 660, training loss: 0.6734462976455688 = 0.017397066578269005 + 0.1 * 6.560492515563965
Epoch 660, val loss: 1.179613471031189
Epoch 670, training loss: 0.672197163105011 = 0.016575103625655174 + 0.1 * 6.556220531463623
Epoch 670, val loss: 1.1876274347305298
Epoch 680, training loss: 0.6711645126342773 = 0.015810653567314148 + 0.1 * 6.5535383224487305
Epoch 680, val loss: 1.1954716444015503
Epoch 690, training loss: 0.670586884021759 = 0.01509977225214243 + 0.1 * 6.554871082305908
Epoch 690, val loss: 1.2031630277633667
Epoch 700, training loss: 0.6689368486404419 = 0.014438721351325512 + 0.1 * 6.544981002807617
Epoch 700, val loss: 1.2106459140777588
Epoch 710, training loss: 0.6681811809539795 = 0.013823747634887695 + 0.1 * 6.543574333190918
Epoch 710, val loss: 1.2179665565490723
Epoch 720, training loss: 0.6677327752113342 = 0.013248774223029613 + 0.1 * 6.544839382171631
Epoch 720, val loss: 1.2251123189926147
Epoch 730, training loss: 0.666642427444458 = 0.01271156407892704 + 0.1 * 6.539308547973633
Epoch 730, val loss: 1.2320412397384644
Epoch 740, training loss: 0.6670156717300415 = 0.012208274565637112 + 0.1 * 6.548073768615723
Epoch 740, val loss: 1.2388489246368408
Epoch 750, training loss: 0.6652514338493347 = 0.011736520566046238 + 0.1 * 6.535149097442627
Epoch 750, val loss: 1.24543297290802
Epoch 760, training loss: 0.6663526892662048 = 0.011293887160718441 + 0.1 * 6.5505876541137695
Epoch 760, val loss: 1.2519307136535645
Epoch 770, training loss: 0.6646237969398499 = 0.010877742432057858 + 0.1 * 6.5374603271484375
Epoch 770, val loss: 1.2581543922424316
Epoch 780, training loss: 0.6631545424461365 = 0.010488114319741726 + 0.1 * 6.5266642570495605
Epoch 780, val loss: 1.2642985582351685
Epoch 790, training loss: 0.6652377247810364 = 0.010118880309164524 + 0.1 * 6.551187992095947
Epoch 790, val loss: 1.2703715562820435
Epoch 800, training loss: 0.6622348427772522 = 0.009771357290446758 + 0.1 * 6.524634838104248
Epoch 800, val loss: 1.2762311697006226
Epoch 810, training loss: 0.6610481142997742 = 0.009443762712180614 + 0.1 * 6.516043186187744
Epoch 810, val loss: 1.2819664478302002
Epoch 820, training loss: 0.6621972918510437 = 0.009132462553679943 + 0.1 * 6.530648231506348
Epoch 820, val loss: 1.2876659631729126
Epoch 830, training loss: 0.6609119176864624 = 0.008837788365781307 + 0.1 * 6.5207414627075195
Epoch 830, val loss: 1.2931374311447144
Epoch 840, training loss: 0.659794270992279 = 0.008560191839933395 + 0.1 * 6.512340545654297
Epoch 840, val loss: 1.2984695434570312
Epoch 850, training loss: 0.6594445109367371 = 0.008296229876577854 + 0.1 * 6.5114827156066895
Epoch 850, val loss: 1.303802251815796
Epoch 860, training loss: 0.6592678427696228 = 0.008044948801398277 + 0.1 * 6.512228488922119
Epoch 860, val loss: 1.3089861869812012
Epoch 870, training loss: 0.6582597494125366 = 0.007806732784956694 + 0.1 * 6.50452995300293
Epoch 870, val loss: 1.314002275466919
Epoch 880, training loss: 0.6580960154533386 = 0.0075805033557116985 + 0.1 * 6.505155086517334
Epoch 880, val loss: 1.3190151453018188
Epoch 890, training loss: 0.6584158539772034 = 0.007364245131611824 + 0.1 * 6.5105156898498535
Epoch 890, val loss: 1.323948860168457
Epoch 900, training loss: 0.6569872498512268 = 0.007158131804317236 + 0.1 * 6.498291015625
Epoch 900, val loss: 1.3287416696548462
Epoch 910, training loss: 0.6572815775871277 = 0.006962563376873732 + 0.1 * 6.503190517425537
Epoch 910, val loss: 1.3334118127822876
Epoch 920, training loss: 0.6562067866325378 = 0.0067752874456346035 + 0.1 * 6.494315147399902
Epoch 920, val loss: 1.3380323648452759
Epoch 930, training loss: 0.6576688885688782 = 0.0065968525595963 + 0.1 * 6.510720252990723
Epoch 930, val loss: 1.342581868171692
Epoch 940, training loss: 0.6554508805274963 = 0.006425749510526657 + 0.1 * 6.490251064300537
Epoch 940, val loss: 1.3469775915145874
Epoch 950, training loss: 0.6560660600662231 = 0.006262737326323986 + 0.1 * 6.49803352355957
Epoch 950, val loss: 1.3512961864471436
Epoch 960, training loss: 0.6553269624710083 = 0.006106467451900244 + 0.1 * 6.492204666137695
Epoch 960, val loss: 1.3555716276168823
Epoch 970, training loss: 0.6537742018699646 = 0.005957057233899832 + 0.1 * 6.478171348571777
Epoch 970, val loss: 1.3597166538238525
Epoch 980, training loss: 0.65418541431427 = 0.005814140196889639 + 0.1 * 6.483712673187256
Epoch 980, val loss: 1.363840937614441
Epoch 990, training loss: 0.654013991355896 = 0.005676475819200277 + 0.1 * 6.483375072479248
Epoch 990, val loss: 1.36795175075531
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 2.794199228286743 = 1.934516191482544 + 0.1 * 8.596830368041992
Epoch 0, val loss: 1.937134027481079
Epoch 10, training loss: 2.78421688079834 = 1.9245436191558838 + 0.1 * 8.596733093261719
Epoch 10, val loss: 1.9278171062469482
Epoch 20, training loss: 2.7715139389038086 = 1.9118999242782593 + 0.1 * 8.596138954162598
Epoch 20, val loss: 1.9156832695007324
Epoch 30, training loss: 2.7530500888824463 = 1.8939911127090454 + 0.1 * 8.590590476989746
Epoch 30, val loss: 1.8981373310089111
Epoch 40, training loss: 2.7226805686950684 = 1.8680143356323242 + 0.1 * 8.546662330627441
Epoch 40, val loss: 1.8728539943695068
Epoch 50, training loss: 2.6609199047088623 = 1.8348546028137207 + 0.1 * 8.260653495788574
Epoch 50, val loss: 1.8422377109527588
Epoch 60, training loss: 2.6050939559936523 = 1.8015061616897583 + 0.1 * 8.035879135131836
Epoch 60, val loss: 1.8131952285766602
Epoch 70, training loss: 2.525557518005371 = 1.7703953981399536 + 0.1 * 7.551621913909912
Epoch 70, val loss: 1.7847293615341187
Epoch 80, training loss: 2.4569013118743896 = 1.734883189201355 + 0.1 * 7.220181465148926
Epoch 80, val loss: 1.7515637874603271
Epoch 90, training loss: 2.40080189704895 = 1.6872071027755737 + 0.1 * 7.135948181152344
Epoch 90, val loss: 1.7092113494873047
Epoch 100, training loss: 2.3340439796447754 = 1.6232467889785767 + 0.1 * 7.10797119140625
Epoch 100, val loss: 1.6550902128219604
Epoch 110, training loss: 2.252800464630127 = 1.544057011604309 + 0.1 * 7.087434768676758
Epoch 110, val loss: 1.5904607772827148
Epoch 120, training loss: 2.1621572971343994 = 1.455064058303833 + 0.1 * 7.070932388305664
Epoch 120, val loss: 1.5190378427505493
Epoch 130, training loss: 2.0677084922790527 = 1.362439751625061 + 0.1 * 7.05268669128418
Epoch 130, val loss: 1.4465606212615967
Epoch 140, training loss: 1.9711246490478516 = 1.268328070640564 + 0.1 * 7.027966022491455
Epoch 140, val loss: 1.3746943473815918
Epoch 150, training loss: 1.8751763105392456 = 1.1752690076828003 + 0.1 * 6.999073028564453
Epoch 150, val loss: 1.3041077852249146
Epoch 160, training loss: 1.7836089134216309 = 1.0878204107284546 + 0.1 * 6.9578857421875
Epoch 160, val loss: 1.2397665977478027
Epoch 170, training loss: 1.7004181146621704 = 1.0077499151229858 + 0.1 * 6.926681995391846
Epoch 170, val loss: 1.1819746494293213
Epoch 180, training loss: 1.624481439590454 = 0.9354131817817688 + 0.1 * 6.890682697296143
Epoch 180, val loss: 1.1311336755752563
Epoch 190, training loss: 1.553617238998413 = 0.86724853515625 + 0.1 * 6.863686561584473
Epoch 190, val loss: 1.0838574171066284
Epoch 200, training loss: 1.486066460609436 = 0.8011167645454407 + 0.1 * 6.849496841430664
Epoch 200, val loss: 1.038609266281128
Epoch 210, training loss: 1.4202654361724854 = 0.7379969954490662 + 0.1 * 6.822683811187744
Epoch 210, val loss: 0.9970896244049072
Epoch 220, training loss: 1.3578274250030518 = 0.6774369478225708 + 0.1 * 6.803905010223389
Epoch 220, val loss: 0.9589915871620178
Epoch 230, training loss: 1.299238681793213 = 0.6206649541854858 + 0.1 * 6.78573751449585
Epoch 230, val loss: 0.9262602925300598
Epoch 240, training loss: 1.2457263469696045 = 0.5688874125480652 + 0.1 * 6.768388748168945
Epoch 240, val loss: 0.9002704620361328
Epoch 250, training loss: 1.1982598304748535 = 0.5217688083648682 + 0.1 * 6.764909267425537
Epoch 250, val loss: 0.8804253339767456
Epoch 260, training loss: 1.1532275676727295 = 0.4788118898868561 + 0.1 * 6.744156360626221
Epoch 260, val loss: 0.8662786483764648
Epoch 270, training loss: 1.1115648746490479 = 0.43919476866722107 + 0.1 * 6.723700523376465
Epoch 270, val loss: 0.8565669655799866
Epoch 280, training loss: 1.0770809650421143 = 0.4022364914417267 + 0.1 * 6.7484450340271
Epoch 280, val loss: 0.8503908514976501
Epoch 290, training loss: 1.039476990699768 = 0.36809465289115906 + 0.1 * 6.7138237953186035
Epoch 290, val loss: 0.8474299907684326
Epoch 300, training loss: 1.0055923461914062 = 0.3365629315376282 + 0.1 * 6.6902947425842285
Epoch 300, val loss: 0.8474254012107849
Epoch 310, training loss: 0.9766165018081665 = 0.3075380027294159 + 0.1 * 6.690784454345703
Epoch 310, val loss: 0.8502134680747986
Epoch 320, training loss: 0.9503525495529175 = 0.2811015546321869 + 0.1 * 6.69251012802124
Epoch 320, val loss: 0.8557046055793762
Epoch 330, training loss: 0.9245179891586304 = 0.2572297751903534 + 0.1 * 6.672881603240967
Epoch 330, val loss: 0.8635220527648926
Epoch 340, training loss: 0.9016361832618713 = 0.2355658859014511 + 0.1 * 6.660702705383301
Epoch 340, val loss: 0.8733159303665161
Epoch 350, training loss: 0.8814213871955872 = 0.21594668924808502 + 0.1 * 6.654747009277344
Epoch 350, val loss: 0.8845703601837158
Epoch 360, training loss: 0.8630949854850769 = 0.19818450510501862 + 0.1 * 6.649105072021484
Epoch 360, val loss: 0.8972567319869995
Epoch 370, training loss: 0.848155677318573 = 0.1820216327905655 + 0.1 * 6.661340236663818
Epoch 370, val loss: 0.9109101891517639
Epoch 380, training loss: 0.831307053565979 = 0.1674070507287979 + 0.1 * 6.638999938964844
Epoch 380, val loss: 0.9251708984375
Epoch 390, training loss: 0.8172668218612671 = 0.15412497520446777 + 0.1 * 6.631418228149414
Epoch 390, val loss: 0.9400640726089478
Epoch 400, training loss: 0.804614782333374 = 0.14204339683055878 + 0.1 * 6.625713348388672
Epoch 400, val loss: 0.9551845788955688
Epoch 410, training loss: 0.7931641936302185 = 0.1310683935880661 + 0.1 * 6.620957851409912
Epoch 410, val loss: 0.9704911708831787
Epoch 420, training loss: 0.7835214138031006 = 0.12104432284832001 + 0.1 * 6.624770641326904
Epoch 420, val loss: 0.9858916997909546
Epoch 430, training loss: 0.772371232509613 = 0.11189723759889603 + 0.1 * 6.604740142822266
Epoch 430, val loss: 1.0011358261108398
Epoch 440, training loss: 0.7637777328491211 = 0.103532575070858 + 0.1 * 6.602451801300049
Epoch 440, val loss: 1.01626455783844
Epoch 450, training loss: 0.7569859623908997 = 0.09590321034193039 + 0.1 * 6.610827445983887
Epoch 450, val loss: 1.0311191082000732
Epoch 460, training loss: 0.7490664720535278 = 0.08895169943571091 + 0.1 * 6.601147651672363
Epoch 460, val loss: 1.0457037687301636
Epoch 470, training loss: 0.7419508099555969 = 0.08258209377527237 + 0.1 * 6.593687057495117
Epoch 470, val loss: 1.0601075887680054
Epoch 480, training loss: 0.7378087639808655 = 0.0767584815621376 + 0.1 * 6.61050271987915
Epoch 480, val loss: 1.074022889137268
Epoch 490, training loss: 0.7295665144920349 = 0.07144521921873093 + 0.1 * 6.581212520599365
Epoch 490, val loss: 1.0877474546432495
Epoch 500, training loss: 0.7241453528404236 = 0.06656887382268906 + 0.1 * 6.575765132904053
Epoch 500, val loss: 1.1013164520263672
Epoch 510, training loss: 0.7190473675727844 = 0.062092192471027374 + 0.1 * 6.569551944732666
Epoch 510, val loss: 1.1144853830337524
Epoch 520, training loss: 0.7154328227043152 = 0.05798367038369179 + 0.1 * 6.574491024017334
Epoch 520, val loss: 1.1274967193603516
Epoch 530, training loss: 0.7118121981620789 = 0.054217323660850525 + 0.1 * 6.575948238372803
Epoch 530, val loss: 1.1402286291122437
Epoch 540, training loss: 0.7072092890739441 = 0.05076455697417259 + 0.1 * 6.564447402954102
Epoch 540, val loss: 1.1526426076889038
Epoch 550, training loss: 0.7043877840042114 = 0.04759106785058975 + 0.1 * 6.567966938018799
Epoch 550, val loss: 1.164905309677124
Epoch 560, training loss: 0.700117290019989 = 0.04467565193772316 + 0.1 * 6.554416179656982
Epoch 560, val loss: 1.1768898963928223
Epoch 570, training loss: 0.6984783411026001 = 0.04199725016951561 + 0.1 * 6.564810752868652
Epoch 570, val loss: 1.188632607460022
Epoch 580, training loss: 0.694827675819397 = 0.03953781723976135 + 0.1 * 6.55289888381958
Epoch 580, val loss: 1.2001920938491821
Epoch 590, training loss: 0.6916123032569885 = 0.037269216030836105 + 0.1 * 6.543430328369141
Epoch 590, val loss: 1.211585521697998
Epoch 600, training loss: 0.6887460350990295 = 0.03517815098166466 + 0.1 * 6.535678863525391
Epoch 600, val loss: 1.2227070331573486
Epoch 610, training loss: 0.6863766312599182 = 0.033250439912080765 + 0.1 * 6.531261444091797
Epoch 610, val loss: 1.23367178440094
Epoch 620, training loss: 0.6857202053070068 = 0.03146747872233391 + 0.1 * 6.542527198791504
Epoch 620, val loss: 1.244459867477417
Epoch 630, training loss: 0.6824527978897095 = 0.029821304604411125 + 0.1 * 6.526315212249756
Epoch 630, val loss: 1.2550060749053955
Epoch 640, training loss: 0.6813289523124695 = 0.02829626016318798 + 0.1 * 6.530326843261719
Epoch 640, val loss: 1.265317440032959
Epoch 650, training loss: 0.6781044006347656 = 0.026881860569119453 + 0.1 * 6.512225151062012
Epoch 650, val loss: 1.2755199670791626
Epoch 660, training loss: 0.6776373386383057 = 0.025564027950167656 + 0.1 * 6.520732879638672
Epoch 660, val loss: 1.2855066061019897
Epoch 670, training loss: 0.6763258576393127 = 0.02433938905596733 + 0.1 * 6.519864559173584
Epoch 670, val loss: 1.295270323753357
Epoch 680, training loss: 0.6771215200424194 = 0.02319955639541149 + 0.1 * 6.539219379425049
Epoch 680, val loss: 1.304868221282959
Epoch 690, training loss: 0.6725476980209351 = 0.02213902957737446 + 0.1 * 6.504086494445801
Epoch 690, val loss: 1.3142743110656738
Epoch 700, training loss: 0.6705412864685059 = 0.021148670464754105 + 0.1 * 6.493926048278809
Epoch 700, val loss: 1.3235350847244263
Epoch 710, training loss: 0.6699399352073669 = 0.020220957696437836 + 0.1 * 6.497189998626709
Epoch 710, val loss: 1.3326362371444702
Epoch 720, training loss: 0.6710800528526306 = 0.019352836534380913 + 0.1 * 6.517271995544434
Epoch 720, val loss: 1.3414523601531982
Epoch 730, training loss: 0.6682626605033875 = 0.018542082980275154 + 0.1 * 6.4972052574157715
Epoch 730, val loss: 1.350091576576233
Epoch 740, training loss: 0.6680241823196411 = 0.017781196162104607 + 0.1 * 6.502429962158203
Epoch 740, val loss: 1.3586528301239014
Epoch 750, training loss: 0.6663309931755066 = 0.017065661028027534 + 0.1 * 6.4926533699035645
Epoch 750, val loss: 1.3669579029083252
Epoch 760, training loss: 0.6655532121658325 = 0.016394920647144318 + 0.1 * 6.491582870483398
Epoch 760, val loss: 1.3751755952835083
Epoch 770, training loss: 0.6651076674461365 = 0.015761394053697586 + 0.1 * 6.493462562561035
Epoch 770, val loss: 1.3831558227539062
Epoch 780, training loss: 0.6634591817855835 = 0.015166361816227436 + 0.1 * 6.482928276062012
Epoch 780, val loss: 1.3909962177276611
Epoch 790, training loss: 0.6633749604225159 = 0.014605130068957806 + 0.1 * 6.487698554992676
Epoch 790, val loss: 1.3986855745315552
Epoch 800, training loss: 0.6612778902053833 = 0.014076094143092632 + 0.1 * 6.472017765045166
Epoch 800, val loss: 1.406266212463379
Epoch 810, training loss: 0.660689115524292 = 0.013574929907917976 + 0.1 * 6.471141338348389
Epoch 810, val loss: 1.4137306213378906
Epoch 820, training loss: 0.6595110893249512 = 0.013100581243634224 + 0.1 * 6.464105129241943
Epoch 820, val loss: 1.420967936515808
Epoch 830, training loss: 0.6613832116127014 = 0.012652461417019367 + 0.1 * 6.487307548522949
Epoch 830, val loss: 1.4280996322631836
Epoch 840, training loss: 0.659103274345398 = 0.012228419072926044 + 0.1 * 6.468748569488525
Epoch 840, val loss: 1.4350261688232422
Epoch 850, training loss: 0.6579418182373047 = 0.011826575733721256 + 0.1 * 6.461152076721191
Epoch 850, val loss: 1.4419444799423218
Epoch 860, training loss: 0.658903181552887 = 0.011444177478551865 + 0.1 * 6.474590301513672
Epoch 860, val loss: 1.4487360715866089
Epoch 870, training loss: 0.657794713973999 = 0.011080977506935596 + 0.1 * 6.467137336730957
Epoch 870, val loss: 1.4553683996200562
Epoch 880, training loss: 0.6578121781349182 = 0.01073580514639616 + 0.1 * 6.470763683319092
Epoch 880, val loss: 1.4618204832077026
Epoch 890, training loss: 0.6561623811721802 = 0.01040876004844904 + 0.1 * 6.457536220550537
Epoch 890, val loss: 1.4682296514511108
Epoch 900, training loss: 0.6570050120353699 = 0.010096573270857334 + 0.1 * 6.469084739685059
Epoch 900, val loss: 1.4746007919311523
Epoch 910, training loss: 0.6550284624099731 = 0.00979800894856453 + 0.1 * 6.452304840087891
Epoch 910, val loss: 1.480716586112976
Epoch 920, training loss: 0.6555786728858948 = 0.00951435323804617 + 0.1 * 6.460643291473389
Epoch 920, val loss: 1.4867932796478271
Epoch 930, training loss: 0.6549555659294128 = 0.009243345819413662 + 0.1 * 6.457121849060059
Epoch 930, val loss: 1.4927774667739868
Epoch 940, training loss: 0.6543381810188293 = 0.008984589949250221 + 0.1 * 6.453536033630371
Epoch 940, val loss: 1.4986109733581543
Epoch 950, training loss: 0.654015839099884 = 0.008737229742109776 + 0.1 * 6.452786445617676
Epoch 950, val loss: 1.5043872594833374
Epoch 960, training loss: 0.6522578001022339 = 0.008501021191477776 + 0.1 * 6.437568187713623
Epoch 960, val loss: 1.5100959539413452
Epoch 970, training loss: 0.6554549932479858 = 0.008274148218333721 + 0.1 * 6.471808433532715
Epoch 970, val loss: 1.5157092809677124
Epoch 980, training loss: 0.6531533598899841 = 0.008057139813899994 + 0.1 * 6.450962543487549
Epoch 980, val loss: 1.52114737033844
Epoch 990, training loss: 0.6532495021820068 = 0.007849877700209618 + 0.1 * 6.453996181488037
Epoch 990, val loss: 1.5265363454818726
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 2.8040428161621094 = 1.9443589448928833 + 0.1 * 8.596837997436523
Epoch 0, val loss: 1.947036623954773
Epoch 10, training loss: 2.793649196624756 = 1.9339728355407715 + 0.1 * 8.596763610839844
Epoch 10, val loss: 1.9365839958190918
Epoch 20, training loss: 2.7810723781585693 = 1.9214445352554321 + 0.1 * 8.59627914428711
Epoch 20, val loss: 1.923577904701233
Epoch 30, training loss: 2.763428211212158 = 1.9042558670043945 + 0.1 * 8.591723442077637
Epoch 30, val loss: 1.9050713777542114
Epoch 40, training loss: 2.7353670597076416 = 1.8793623447418213 + 0.1 * 8.560047149658203
Epoch 40, val loss: 1.878003478050232
Epoch 50, training loss: 2.686755657196045 = 1.8461909294128418 + 0.1 * 8.405648231506348
Epoch 50, val loss: 1.8435052633285522
Epoch 60, training loss: 2.635094165802002 = 1.810408592224121 + 0.1 * 8.246856689453125
Epoch 60, val loss: 1.8096942901611328
Epoch 70, training loss: 2.58123517036438 = 1.7781950235366821 + 0.1 * 8.030401229858398
Epoch 70, val loss: 1.7824617624282837
Epoch 80, training loss: 2.501119613647461 = 1.7425302267074585 + 0.1 * 7.5858941078186035
Epoch 80, val loss: 1.7521320581436157
Epoch 90, training loss: 2.426609516143799 = 1.6989902257919312 + 0.1 * 7.276192665100098
Epoch 90, val loss: 1.7156611680984497
Epoch 100, training loss: 2.3563990592956543 = 1.641897201538086 + 0.1 * 7.145018577575684
Epoch 100, val loss: 1.66746985912323
Epoch 110, training loss: 2.2761785984039307 = 1.5693089962005615 + 0.1 * 7.068695545196533
Epoch 110, val loss: 1.6055583953857422
Epoch 120, training loss: 2.1882193088531494 = 1.4853918552398682 + 0.1 * 7.028274059295654
Epoch 120, val loss: 1.538032054901123
Epoch 130, training loss: 2.097106456756592 = 1.3969248533248901 + 0.1 * 7.001815319061279
Epoch 130, val loss: 1.4678879976272583
Epoch 140, training loss: 2.006082057952881 = 1.3079320192337036 + 0.1 * 6.981500148773193
Epoch 140, val loss: 1.3995027542114258
Epoch 150, training loss: 1.9156990051269531 = 1.2193918228149414 + 0.1 * 6.963070869445801
Epoch 150, val loss: 1.3317172527313232
Epoch 160, training loss: 1.8273447751998901 = 1.1328388452529907 + 0.1 * 6.945059299468994
Epoch 160, val loss: 1.2663538455963135
Epoch 170, training loss: 1.744680404663086 = 1.0521268844604492 + 0.1 * 6.925534248352051
Epoch 170, val loss: 1.2065808773040771
Epoch 180, training loss: 1.6704723834991455 = 0.9790400266647339 + 0.1 * 6.914323329925537
Epoch 180, val loss: 1.154111623764038
Epoch 190, training loss: 1.602241039276123 = 0.9124343395233154 + 0.1 * 6.898066520690918
Epoch 190, val loss: 1.1070626974105835
Epoch 200, training loss: 1.5383515357971191 = 0.8490669131278992 + 0.1 * 6.892845630645752
Epoch 200, val loss: 1.0625574588775635
Epoch 210, training loss: 1.4757405519485474 = 0.7874988317489624 + 0.1 * 6.88241720199585
Epoch 210, val loss: 1.0190459489822388
Epoch 220, training loss: 1.4140359163284302 = 0.7268508076667786 + 0.1 * 6.871850967407227
Epoch 220, val loss: 0.9763555526733398
Epoch 230, training loss: 1.356665849685669 = 0.6677684187889099 + 0.1 * 6.888974666595459
Epoch 230, val loss: 0.935621976852417
Epoch 240, training loss: 1.2977819442749023 = 0.6120296716690063 + 0.1 * 6.857522487640381
Epoch 240, val loss: 0.8991948366165161
Epoch 250, training loss: 1.2434887886047363 = 0.5590739846229553 + 0.1 * 6.8441481590271
Epoch 250, val loss: 0.8672172427177429
Epoch 260, training loss: 1.1936261653900146 = 0.5089354515075684 + 0.1 * 6.8469061851501465
Epoch 260, val loss: 0.8399129509925842
Epoch 270, training loss: 1.145076036453247 = 0.4621478021144867 + 0.1 * 6.829282283782959
Epoch 270, val loss: 0.8172476887702942
Epoch 280, training loss: 1.100938320159912 = 0.41895321011543274 + 0.1 * 6.819850921630859
Epoch 280, val loss: 0.7986778020858765
Epoch 290, training loss: 1.0605719089508057 = 0.3799493610858917 + 0.1 * 6.806224822998047
Epoch 290, val loss: 0.784047544002533
Epoch 300, training loss: 1.025359869003296 = 0.3452719449996948 + 0.1 * 6.800878524780273
Epoch 300, val loss: 0.7731558084487915
Epoch 310, training loss: 0.993912935256958 = 0.31483060121536255 + 0.1 * 6.790823459625244
Epoch 310, val loss: 0.7654983997344971
Epoch 320, training loss: 0.9658459424972534 = 0.28777846693992615 + 0.1 * 6.780674457550049
Epoch 320, val loss: 0.7604239583015442
Epoch 330, training loss: 0.9412779808044434 = 0.2635660767555237 + 0.1 * 6.777118682861328
Epoch 330, val loss: 0.7574328184127808
Epoch 340, training loss: 0.9175706505775452 = 0.24146562814712524 + 0.1 * 6.761050224304199
Epoch 340, val loss: 0.7561428546905518
Epoch 350, training loss: 0.8957059383392334 = 0.22068139910697937 + 0.1 * 6.750245094299316
Epoch 350, val loss: 0.7560248970985413
Epoch 360, training loss: 0.8772891759872437 = 0.2007177174091339 + 0.1 * 6.765714168548584
Epoch 360, val loss: 0.7568253874778748
Epoch 370, training loss: 0.8552983403205872 = 0.1815103441476822 + 0.1 * 6.737880229949951
Epoch 370, val loss: 0.7582094669342041
Epoch 380, training loss: 0.8372552394866943 = 0.16305449604988098 + 0.1 * 6.742007255554199
Epoch 380, val loss: 0.7600642442703247
Epoch 390, training loss: 0.8180458545684814 = 0.14567433297634125 + 0.1 * 6.723714828491211
Epoch 390, val loss: 0.7626847624778748
Epoch 400, training loss: 0.801457941532135 = 0.1296537071466446 + 0.1 * 6.718042373657227
Epoch 400, val loss: 0.7659329175949097
Epoch 410, training loss: 0.786385178565979 = 0.11526766419410706 + 0.1 * 6.711175441741943
Epoch 410, val loss: 0.7700820565223694
Epoch 420, training loss: 0.7718338370323181 = 0.1025753766298294 + 0.1 * 6.69258451461792
Epoch 420, val loss: 0.7751485109329224
Epoch 430, training loss: 0.7630645036697388 = 0.09148051589727402 + 0.1 * 6.715839862823486
Epoch 430, val loss: 0.7810707688331604
Epoch 440, training loss: 0.7499406337738037 = 0.08187476545572281 + 0.1 * 6.680658340454102
Epoch 440, val loss: 0.7877354621887207
Epoch 450, training loss: 0.7449369430541992 = 0.07354455441236496 + 0.1 * 6.713923454284668
Epoch 450, val loss: 0.7948456406593323
Epoch 460, training loss: 0.7335852980613708 = 0.0663607269525528 + 0.1 * 6.672245502471924
Epoch 460, val loss: 0.8024094104766846
Epoch 470, training loss: 0.7252966165542603 = 0.06011058762669563 + 0.1 * 6.651860237121582
Epoch 470, val loss: 0.8101349472999573
Epoch 480, training loss: 0.7198190093040466 = 0.05465157330036163 + 0.1 * 6.651674270629883
Epoch 480, val loss: 0.818037211894989
Epoch 490, training loss: 0.7150577306747437 = 0.04987941309809685 + 0.1 * 6.651782989501953
Epoch 490, val loss: 0.8260862231254578
Epoch 500, training loss: 0.7107332348823547 = 0.045695263892412186 + 0.1 * 6.650379657745361
Epoch 500, val loss: 0.834041953086853
Epoch 510, training loss: 0.7053570747375488 = 0.04201921448111534 + 0.1 * 6.633378505706787
Epoch 510, val loss: 0.8420490622520447
Epoch 520, training loss: 0.7022053599357605 = 0.03876615688204765 + 0.1 * 6.634391784667969
Epoch 520, val loss: 0.8499050140380859
Epoch 530, training loss: 0.6976394057273865 = 0.035880111157894135 + 0.1 * 6.617592811584473
Epoch 530, val loss: 0.8576624989509583
Epoch 540, training loss: 0.6937161684036255 = 0.03330850973725319 + 0.1 * 6.604076385498047
Epoch 540, val loss: 0.865277111530304
Epoch 550, training loss: 0.6947839260101318 = 0.031005771830677986 + 0.1 * 6.637781143188477
Epoch 550, val loss: 0.8727073669433594
Epoch 560, training loss: 0.6893036961555481 = 0.028947485610842705 + 0.1 * 6.603561878204346
Epoch 560, val loss: 0.880101203918457
Epoch 570, training loss: 0.6861571073532104 = 0.027093790471553802 + 0.1 * 6.590633392333984
Epoch 570, val loss: 0.8872786164283752
Epoch 580, training loss: 0.684917688369751 = 0.025417346507310867 + 0.1 * 6.595003604888916
Epoch 580, val loss: 0.8940904140472412
Epoch 590, training loss: 0.6821175217628479 = 0.023905767127871513 + 0.1 * 6.582117557525635
Epoch 590, val loss: 0.9010756015777588
Epoch 600, training loss: 0.6800539493560791 = 0.02252902276813984 + 0.1 * 6.575249195098877
Epoch 600, val loss: 0.9077714681625366
Epoch 610, training loss: 0.6785977482795715 = 0.021269584074616432 + 0.1 * 6.573281764984131
Epoch 610, val loss: 0.9141774773597717
Epoch 620, training loss: 0.6766630411148071 = 0.020119817927479744 + 0.1 * 6.565431594848633
Epoch 620, val loss: 0.920646607875824
Epoch 630, training loss: 0.6773465871810913 = 0.01906353421509266 + 0.1 * 6.582830429077148
Epoch 630, val loss: 0.9268223643302917
Epoch 640, training loss: 0.6738372445106506 = 0.0180946197360754 + 0.1 * 6.557426452636719
Epoch 640, val loss: 0.932824432849884
Epoch 650, training loss: 0.6726331114768982 = 0.017203014343976974 + 0.1 * 6.554300785064697
Epoch 650, val loss: 0.9387890100479126
Epoch 660, training loss: 0.6708999276161194 = 0.016378769651055336 + 0.1 * 6.5452117919921875
Epoch 660, val loss: 0.944474995136261
Epoch 670, training loss: 0.671831488609314 = 0.015615899115800858 + 0.1 * 6.562155723571777
Epoch 670, val loss: 0.9501215219497681
Epoch 680, training loss: 0.6703827381134033 = 0.014908785000443459 + 0.1 * 6.554739475250244
Epoch 680, val loss: 0.9555553197860718
Epoch 690, training loss: 0.6675391793251038 = 0.01425228826701641 + 0.1 * 6.5328688621521
Epoch 690, val loss: 0.9607791304588318
Epoch 700, training loss: 0.6669058203697205 = 0.013642099685966969 + 0.1 * 6.532637596130371
Epoch 700, val loss: 0.9660712480545044
Epoch 710, training loss: 0.6672817468643188 = 0.01307170931249857 + 0.1 * 6.542100429534912
Epoch 710, val loss: 0.9709990620613098
Epoch 720, training loss: 0.6649033427238464 = 0.012539672665297985 + 0.1 * 6.523636341094971
Epoch 720, val loss: 0.9759772419929504
Epoch 730, training loss: 0.6652169823646545 = 0.012041454203426838 + 0.1 * 6.531754970550537
Epoch 730, val loss: 0.9808704257011414
Epoch 740, training loss: 0.663888156414032 = 0.011574653908610344 + 0.1 * 6.523135185241699
Epoch 740, val loss: 0.9854549765586853
Epoch 750, training loss: 0.6624284982681274 = 0.011137130670249462 + 0.1 * 6.512913703918457
Epoch 750, val loss: 0.9900950193405151
Epoch 760, training loss: 0.6643876433372498 = 0.010725485160946846 + 0.1 * 6.53662109375
Epoch 760, val loss: 0.9946637153625488
Epoch 770, training loss: 0.6623197793960571 = 0.010338343679904938 + 0.1 * 6.5198140144348145
Epoch 770, val loss: 0.9989027380943298
Epoch 780, training loss: 0.6615039110183716 = 0.009973587468266487 + 0.1 * 6.515303134918213
Epoch 780, val loss: 1.0032588243484497
Epoch 790, training loss: 0.6608313918113708 = 0.009629789739847183 + 0.1 * 6.5120158195495605
Epoch 790, val loss: 1.0074206590652466
Epoch 800, training loss: 0.6589691042900085 = 0.00930433627218008 + 0.1 * 6.496647357940674
Epoch 800, val loss: 1.011505365371704
Epoch 810, training loss: 0.6596478819847107 = 0.008996491320431232 + 0.1 * 6.506513595581055
Epoch 810, val loss: 1.0155878067016602
Epoch 820, training loss: 0.6598435044288635 = 0.008705233223736286 + 0.1 * 6.511382579803467
Epoch 820, val loss: 1.019294023513794
Epoch 830, training loss: 0.6582750082015991 = 0.008430386893451214 + 0.1 * 6.498446464538574
Epoch 830, val loss: 1.0233099460601807
Epoch 840, training loss: 0.6583134531974792 = 0.008169173263013363 + 0.1 * 6.5014424324035645
Epoch 840, val loss: 1.0271321535110474
Epoch 850, training loss: 0.6585601568222046 = 0.007920593023300171 + 0.1 * 6.5063958168029785
Epoch 850, val loss: 1.030597448348999
Epoch 860, training loss: 0.6564961075782776 = 0.007685544900596142 + 0.1 * 6.488105297088623
Epoch 860, val loss: 1.0343091487884521
Epoch 870, training loss: 0.6565772294998169 = 0.00746172945946455 + 0.1 * 6.49115514755249
Epoch 870, val loss: 1.0378719568252563
Epoch 880, training loss: 0.6561415791511536 = 0.007248395588248968 + 0.1 * 6.488931655883789
Epoch 880, val loss: 1.0413196086883545
Epoch 890, training loss: 0.6561293601989746 = 0.0070450701750814915 + 0.1 * 6.490842819213867
Epoch 890, val loss: 1.0446733236312866
Epoch 900, training loss: 0.6546039581298828 = 0.0068509881384670734 + 0.1 * 6.477529048919678
Epoch 900, val loss: 1.048039436340332
Epoch 910, training loss: 0.6559917330741882 = 0.006665733177214861 + 0.1 * 6.493260383605957
Epoch 910, val loss: 1.0512778759002686
Epoch 920, training loss: 0.654115617275238 = 0.006489028688520193 + 0.1 * 6.4762654304504395
Epoch 920, val loss: 1.0545215606689453
Epoch 930, training loss: 0.6548087000846863 = 0.006319965701550245 + 0.1 * 6.484887599945068
Epoch 930, val loss: 1.0576512813568115
Epoch 940, training loss: 0.6534788012504578 = 0.0061589572578668594 + 0.1 * 6.473198413848877
Epoch 940, val loss: 1.0607028007507324
Epoch 950, training loss: 0.6527224183082581 = 0.006004665978252888 + 0.1 * 6.467177391052246
Epoch 950, val loss: 1.0637636184692383
Epoch 960, training loss: 0.6532078385353088 = 0.005856455769389868 + 0.1 * 6.473513603210449
Epoch 960, val loss: 1.0668132305145264
Epoch 970, training loss: 0.6524100303649902 = 0.005714466329663992 + 0.1 * 6.466955184936523
Epoch 970, val loss: 1.0695573091506958
Epoch 980, training loss: 0.6525136232376099 = 0.005578330252319574 + 0.1 * 6.469352722167969
Epoch 980, val loss: 1.0724550485610962
Epoch 990, training loss: 0.6512835025787354 = 0.005448064301162958 + 0.1 * 6.458354473114014
Epoch 990, val loss: 1.07538640499115
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8128624143384291
The final CL Acc:0.75062, 0.00924, The final GNN Acc:0.80917, 0.00325
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13224])
remove edge: torch.Size([2, 7890])
updated graph: torch.Size([2, 10558])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8070740699768066 = 1.9473897218704224 + 0.1 * 8.596842765808105
Epoch 0, val loss: 1.9452584981918335
Epoch 10, training loss: 2.796715259552002 = 1.9370405673980713 + 0.1 * 8.596747398376465
Epoch 10, val loss: 1.9354928731918335
Epoch 20, training loss: 2.7838504314422607 = 1.924232006072998 + 0.1 * 8.596183776855469
Epoch 20, val loss: 1.9229044914245605
Epoch 30, training loss: 2.7651712894439697 = 1.90603768825531 + 0.1 * 8.59133529663086
Epoch 30, val loss: 1.9048479795455933
Epoch 40, training loss: 2.734853982925415 = 1.8789443969726562 + 0.1 * 8.559096336364746
Epoch 40, val loss: 1.8783317804336548
Epoch 50, training loss: 2.6813480854034424 = 1.841941237449646 + 0.1 * 8.394067764282227
Epoch 50, val loss: 1.84375
Epoch 60, training loss: 2.616988182067871 = 1.8019514083862305 + 0.1 * 8.150367736816406
Epoch 60, val loss: 1.8083614110946655
Epoch 70, training loss: 2.550283193588257 = 1.765397310256958 + 0.1 * 7.8488593101501465
Epoch 70, val loss: 1.7750791311264038
Epoch 80, training loss: 2.4691309928894043 = 1.725842833518982 + 0.1 * 7.432882308959961
Epoch 80, val loss: 1.73772394657135
Epoch 90, training loss: 2.3937466144561768 = 1.675992727279663 + 0.1 * 7.17753791809082
Epoch 90, val loss: 1.6925904750823975
Epoch 100, training loss: 2.318519353866577 = 1.6088215112686157 + 0.1 * 7.096978187561035
Epoch 100, val loss: 1.6327390670776367
Epoch 110, training loss: 2.231898307800293 = 1.526620626449585 + 0.1 * 7.05277681350708
Epoch 110, val loss: 1.5597642660140991
Epoch 120, training loss: 2.14200758934021 = 1.4398380517959595 + 0.1 * 7.021696090698242
Epoch 120, val loss: 1.4855833053588867
Epoch 130, training loss: 2.052746534347534 = 1.3535358905792236 + 0.1 * 6.992105960845947
Epoch 130, val loss: 1.414250135421753
Epoch 140, training loss: 1.9641191959381104 = 1.2677205801010132 + 0.1 * 6.963986873626709
Epoch 140, val loss: 1.3441307544708252
Epoch 150, training loss: 1.8754689693450928 = 1.1814091205596924 + 0.1 * 6.940598964691162
Epoch 150, val loss: 1.2754091024398804
Epoch 160, training loss: 1.789306879043579 = 1.0971320867538452 + 0.1 * 6.92174768447876
Epoch 160, val loss: 1.2101693153381348
Epoch 170, training loss: 1.706110954284668 = 1.0157839059829712 + 0.1 * 6.9032697677612305
Epoch 170, val loss: 1.1485340595245361
Epoch 180, training loss: 1.6261842250823975 = 0.9377326369285583 + 0.1 * 6.884515762329102
Epoch 180, val loss: 1.090735912322998
Epoch 190, training loss: 1.5525884628295898 = 0.8646224737167358 + 0.1 * 6.879660606384277
Epoch 190, val loss: 1.036970615386963
Epoch 200, training loss: 1.482973337173462 = 0.7970409393310547 + 0.1 * 6.859323024749756
Epoch 200, val loss: 0.9878543615341187
Epoch 210, training loss: 1.4180288314819336 = 0.7336575388908386 + 0.1 * 6.843713283538818
Epoch 210, val loss: 0.9425992369651794
Epoch 220, training loss: 1.3590373992919922 = 0.6746485829353333 + 0.1 * 6.843888282775879
Epoch 220, val loss: 0.9017907381057739
Epoch 230, training loss: 1.3029601573944092 = 0.6204374432563782 + 0.1 * 6.825226306915283
Epoch 230, val loss: 0.866111695766449
Epoch 240, training loss: 1.252045750617981 = 0.5704731941223145 + 0.1 * 6.815725326538086
Epoch 240, val loss: 0.8355221748352051
Epoch 250, training loss: 1.2065513134002686 = 0.5245498418807983 + 0.1 * 6.8200154304504395
Epoch 250, val loss: 0.809898316860199
Epoch 260, training loss: 1.1626249551773071 = 0.48251715302467346 + 0.1 * 6.8010783195495605
Epoch 260, val loss: 0.7890768647193909
Epoch 270, training loss: 1.121826410293579 = 0.44290220737457275 + 0.1 * 6.789241790771484
Epoch 270, val loss: 0.771826446056366
Epoch 280, training loss: 1.0847997665405273 = 0.40457916259765625 + 0.1 * 6.802206516265869
Epoch 280, val loss: 0.7571917176246643
Epoch 290, training loss: 1.0443447828292847 = 0.3673759996891022 + 0.1 * 6.769688129425049
Epoch 290, val loss: 0.7447647452354431
Epoch 300, training loss: 1.0072201490402222 = 0.33108657598495483 + 0.1 * 6.761335849761963
Epoch 300, val loss: 0.7340993285179138
Epoch 310, training loss: 0.9722634553909302 = 0.29656046628952026 + 0.1 * 6.757030010223389
Epoch 310, val loss: 0.7254350781440735
Epoch 320, training loss: 0.9399135112762451 = 0.26492491364479065 + 0.1 * 6.7498860359191895
Epoch 320, val loss: 0.7193554043769836
Epoch 330, training loss: 0.9102395176887512 = 0.23657380044460297 + 0.1 * 6.736656665802002
Epoch 330, val loss: 0.7160142660140991
Epoch 340, training loss: 0.8849081993103027 = 0.2117612361907959 + 0.1 * 6.731469631195068
Epoch 340, val loss: 0.715336799621582
Epoch 350, training loss: 0.8627272248268127 = 0.19043885171413422 + 0.1 * 6.722883224487305
Epoch 350, val loss: 0.716989278793335
Epoch 360, training loss: 0.8436315059661865 = 0.17191244661808014 + 0.1 * 6.717190742492676
Epoch 360, val loss: 0.7207172513008118
Epoch 370, training loss: 0.8262896537780762 = 0.15567119419574738 + 0.1 * 6.706184387207031
Epoch 370, val loss: 0.7260288596153259
Epoch 380, training loss: 0.8116940259933472 = 0.1413680911064148 + 0.1 * 6.703258991241455
Epoch 380, val loss: 0.7325946688652039
Epoch 390, training loss: 0.7982684373855591 = 0.1287689059972763 + 0.1 * 6.694994926452637
Epoch 390, val loss: 0.740027666091919
Epoch 400, training loss: 0.7863641977310181 = 0.11754248291254044 + 0.1 * 6.688216686248779
Epoch 400, val loss: 0.7481817007064819
Epoch 410, training loss: 0.7762202620506287 = 0.1075161024928093 + 0.1 * 6.687041282653809
Epoch 410, val loss: 0.7568303346633911
Epoch 420, training loss: 0.7665625810623169 = 0.09857362508773804 + 0.1 * 6.67988920211792
Epoch 420, val loss: 0.7659195065498352
Epoch 430, training loss: 0.758013904094696 = 0.09051846712827682 + 0.1 * 6.674954414367676
Epoch 430, val loss: 0.7754102945327759
Epoch 440, training loss: 0.7502522468566895 = 0.08323123306035995 + 0.1 * 6.670210361480713
Epoch 440, val loss: 0.7852815985679626
Epoch 450, training loss: 0.7434136867523193 = 0.07662279903888702 + 0.1 * 6.667908668518066
Epoch 450, val loss: 0.795397937297821
Epoch 460, training loss: 0.7376718521118164 = 0.0706513449549675 + 0.1 * 6.6702046394348145
Epoch 460, val loss: 0.8056243062019348
Epoch 470, training loss: 0.7313500642776489 = 0.06525483727455139 + 0.1 * 6.660951614379883
Epoch 470, val loss: 0.8159283399581909
Epoch 480, training loss: 0.7263134717941284 = 0.060361068695783615 + 0.1 * 6.6595234870910645
Epoch 480, val loss: 0.8263511657714844
Epoch 490, training loss: 0.7209333777427673 = 0.05592254549264908 + 0.1 * 6.6501078605651855
Epoch 490, val loss: 0.8366953134536743
Epoch 500, training loss: 0.7176455855369568 = 0.05189644917845726 + 0.1 * 6.657491207122803
Epoch 500, val loss: 0.8469155430793762
Epoch 510, training loss: 0.7126161456108093 = 0.04824855178594589 + 0.1 * 6.643675804138184
Epoch 510, val loss: 0.8571200370788574
Epoch 520, training loss: 0.7097147107124329 = 0.04493149742484093 + 0.1 * 6.64783239364624
Epoch 520, val loss: 0.8671040534973145
Epoch 530, training loss: 0.7056131958961487 = 0.04191679507493973 + 0.1 * 6.636963844299316
Epoch 530, val loss: 0.8769267201423645
Epoch 540, training loss: 0.7019773721694946 = 0.03916890174150467 + 0.1 * 6.628084659576416
Epoch 540, val loss: 0.8865892887115479
Epoch 550, training loss: 0.699750542640686 = 0.03666216507554054 + 0.1 * 6.630883693695068
Epoch 550, val loss: 0.89609295129776
Epoch 560, training loss: 0.6960134506225586 = 0.03437025472521782 + 0.1 * 6.616432189941406
Epoch 560, val loss: 0.9053984880447388
Epoch 570, training loss: 0.6983763575553894 = 0.032269757241010666 + 0.1 * 6.661065578460693
Epoch 570, val loss: 0.9144434928894043
Epoch 580, training loss: 0.6924929618835449 = 0.03035442717373371 + 0.1 * 6.621385097503662
Epoch 580, val loss: 0.9232446551322937
Epoch 590, training loss: 0.6927282214164734 = 0.028600569814443588 + 0.1 * 6.6412763595581055
Epoch 590, val loss: 0.9318023920059204
Epoch 600, training loss: 0.6867760419845581 = 0.02699466235935688 + 0.1 * 6.597814083099365
Epoch 600, val loss: 0.9401569366455078
Epoch 610, training loss: 0.6849340796470642 = 0.025515539571642876 + 0.1 * 6.5941853523254395
Epoch 610, val loss: 0.9482911229133606
Epoch 620, training loss: 0.6853525042533875 = 0.024151833727955818 + 0.1 * 6.612006187438965
Epoch 620, val loss: 0.9562983512878418
Epoch 630, training loss: 0.6823474764823914 = 0.022894324734807014 + 0.1 * 6.594531536102295
Epoch 630, val loss: 0.9640749096870422
Epoch 640, training loss: 0.6810105443000793 = 0.02173159085214138 + 0.1 * 6.592789173126221
Epoch 640, val loss: 0.9716366529464722
Epoch 650, training loss: 0.6799320578575134 = 0.02065517008304596 + 0.1 * 6.592769145965576
Epoch 650, val loss: 0.9791547060012817
Epoch 660, training loss: 0.6779914498329163 = 0.019656142219901085 + 0.1 * 6.583353042602539
Epoch 660, val loss: 0.9862366914749146
Epoch 670, training loss: 0.677545428276062 = 0.018731022253632545 + 0.1 * 6.588144302368164
Epoch 670, val loss: 0.9933185577392578
Epoch 680, training loss: 0.6755465269088745 = 0.017872171476483345 + 0.1 * 6.5767436027526855
Epoch 680, val loss: 1.00009286403656
Epoch 690, training loss: 0.6732302904129028 = 0.017075009644031525 + 0.1 * 6.561553001403809
Epoch 690, val loss: 1.0067921876907349
Epoch 700, training loss: 0.6734192371368408 = 0.01632959395647049 + 0.1 * 6.570896625518799
Epoch 700, val loss: 1.013377070426941
Epoch 710, training loss: 0.6710321307182312 = 0.01563304103910923 + 0.1 * 6.553990840911865
Epoch 710, val loss: 1.0197181701660156
Epoch 720, training loss: 0.6718150973320007 = 0.014981498941779137 + 0.1 * 6.568336009979248
Epoch 720, val loss: 1.0259571075439453
Epoch 730, training loss: 0.6691567897796631 = 0.01437193900346756 + 0.1 * 6.547848701477051
Epoch 730, val loss: 1.0320703983306885
Epoch 740, training loss: 0.670274019241333 = 0.01380161102861166 + 0.1 * 6.564723968505859
Epoch 740, val loss: 1.0379130840301514
Epoch 750, training loss: 0.6682858467102051 = 0.013269096612930298 + 0.1 * 6.550167560577393
Epoch 750, val loss: 1.0436980724334717
Epoch 760, training loss: 0.6663653254508972 = 0.012768959626555443 + 0.1 * 6.53596305847168
Epoch 760, val loss: 1.0493513345718384
Epoch 770, training loss: 0.6658094525337219 = 0.012297740206122398 + 0.1 * 6.535116672515869
Epoch 770, val loss: 1.0549829006195068
Epoch 780, training loss: 0.6651142239570618 = 0.011852322146296501 + 0.1 * 6.532618999481201
Epoch 780, val loss: 1.0603822469711304
Epoch 790, training loss: 0.6648496389389038 = 0.011433163657784462 + 0.1 * 6.5341644287109375
Epoch 790, val loss: 1.065638542175293
Epoch 800, training loss: 0.6635578274726868 = 0.011039403267204762 + 0.1 * 6.525184154510498
Epoch 800, val loss: 1.0707646608352661
Epoch 810, training loss: 0.6628937125205994 = 0.010666990652680397 + 0.1 * 6.5222673416137695
Epoch 810, val loss: 1.0759365558624268
Epoch 820, training loss: 0.6627784371376038 = 0.0103133050724864 + 0.1 * 6.524651050567627
Epoch 820, val loss: 1.0809468030929565
Epoch 830, training loss: 0.662002444267273 = 0.00997825339436531 + 0.1 * 6.520242214202881
Epoch 830, val loss: 1.0858501195907593
Epoch 840, training loss: 0.6608906984329224 = 0.009660358540713787 + 0.1 * 6.512303352355957
Epoch 840, val loss: 1.0905413627624512
Epoch 850, training loss: 0.6616705656051636 = 0.009359446354210377 + 0.1 * 6.523111343383789
Epoch 850, val loss: 1.095258355140686
Epoch 860, training loss: 0.6594001054763794 = 0.009073356166481972 + 0.1 * 6.503267288208008
Epoch 860, val loss: 1.0998330116271973
Epoch 870, training loss: 0.6607012152671814 = 0.008801539428532124 + 0.1 * 6.518996715545654
Epoch 870, val loss: 1.104355812072754
Epoch 880, training loss: 0.6593519449234009 = 0.008542550727725029 + 0.1 * 6.50809383392334
Epoch 880, val loss: 1.108762502670288
Epoch 890, training loss: 0.6588947176933289 = 0.008296695537865162 + 0.1 * 6.505980491638184
Epoch 890, val loss: 1.1130621433258057
Epoch 900, training loss: 0.6574205756187439 = 0.008062594570219517 + 0.1 * 6.493579387664795
Epoch 900, val loss: 1.117289423942566
Epoch 910, training loss: 0.6569023132324219 = 0.00783892534673214 + 0.1 * 6.490633964538574
Epoch 910, val loss: 1.1214978694915771
Epoch 920, training loss: 0.657199501991272 = 0.007624868769198656 + 0.1 * 6.49574613571167
Epoch 920, val loss: 1.1256064176559448
Epoch 930, training loss: 0.6598723530769348 = 0.007420672103762627 + 0.1 * 6.524516582489014
Epoch 930, val loss: 1.1295956373214722
Epoch 940, training loss: 0.6565958857536316 = 0.007225993555039167 + 0.1 * 6.493698596954346
Epoch 940, val loss: 1.1334019899368286
Epoch 950, training loss: 0.655360996723175 = 0.007040263619273901 + 0.1 * 6.4832072257995605
Epoch 950, val loss: 1.1373482942581177
Epoch 960, training loss: 0.6547389030456543 = 0.006861962843686342 + 0.1 * 6.478769302368164
Epoch 960, val loss: 1.141186237335205
Epoch 970, training loss: 0.6539341807365417 = 0.006690425332635641 + 0.1 * 6.472437381744385
Epoch 970, val loss: 1.1449178457260132
Epoch 980, training loss: 0.6558834314346313 = 0.0065264469012618065 + 0.1 * 6.493569374084473
Epoch 980, val loss: 1.148533582687378
Epoch 990, training loss: 0.6542338728904724 = 0.006369073409587145 + 0.1 * 6.4786481857299805
Epoch 990, val loss: 1.1521037817001343
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.8139169216156006 = 1.9542322158813477 + 0.1 * 8.596846580505371
Epoch 0, val loss: 1.948578119277954
Epoch 10, training loss: 2.803741455078125 = 1.9440631866455078 + 0.1 * 8.596783638000488
Epoch 10, val loss: 1.938690185546875
Epoch 20, training loss: 2.791449546813965 = 1.9318053722381592 + 0.1 * 8.596440315246582
Epoch 20, val loss: 1.9262559413909912
Epoch 30, training loss: 2.774259090423584 = 1.9148858785629272 + 0.1 * 8.593732833862305
Epoch 30, val loss: 1.9085702896118164
Epoch 40, training loss: 2.7470455169677734 = 1.8899226188659668 + 0.1 * 8.571228981018066
Epoch 40, val loss: 1.8820900917053223
Epoch 50, training loss: 2.696639060974121 = 1.8542641401290894 + 0.1 * 8.423748970031738
Epoch 50, val loss: 1.8452035188674927
Epoch 60, training loss: 2.6102945804595947 = 1.8124483823776245 + 0.1 * 7.978462219238281
Epoch 60, val loss: 1.8051072359085083
Epoch 70, training loss: 2.5247843265533447 = 1.771836519241333 + 0.1 * 7.529478549957275
Epoch 70, val loss: 1.7687629461288452
Epoch 80, training loss: 2.4558863639831543 = 1.7279791831970215 + 0.1 * 7.279071807861328
Epoch 80, val loss: 1.7301719188690186
Epoch 90, training loss: 2.3903799057006836 = 1.6744091510772705 + 0.1 * 7.159706115722656
Epoch 90, val loss: 1.6835792064666748
Epoch 100, training loss: 2.3173327445983887 = 1.60614013671875 + 0.1 * 7.11192512512207
Epoch 100, val loss: 1.624051809310913
Epoch 110, training loss: 2.2307443618774414 = 1.5222214460372925 + 0.1 * 7.085230350494385
Epoch 110, val loss: 1.549897313117981
Epoch 120, training loss: 2.132645845413208 = 1.4263375997543335 + 0.1 * 7.063082695007324
Epoch 120, val loss: 1.4663969278335571
Epoch 130, training loss: 2.028900623321533 = 1.324617624282837 + 0.1 * 7.042829990386963
Epoch 130, val loss: 1.3811910152435303
Epoch 140, training loss: 1.923224925994873 = 1.2210612297058105 + 0.1 * 7.021636962890625
Epoch 140, val loss: 1.2948663234710693
Epoch 150, training loss: 1.8183903694152832 = 1.1184817552566528 + 0.1 * 6.999086856842041
Epoch 150, val loss: 1.2104010581970215
Epoch 160, training loss: 1.7164874076843262 = 1.0187464952468872 + 0.1 * 6.9774088859558105
Epoch 160, val loss: 1.1284373998641968
Epoch 170, training loss: 1.6200318336486816 = 0.9241836667060852 + 0.1 * 6.9584808349609375
Epoch 170, val loss: 1.051318645477295
Epoch 180, training loss: 1.5314970016479492 = 0.8373039364814758 + 0.1 * 6.941929817199707
Epoch 180, val loss: 0.9813202023506165
Epoch 190, training loss: 1.4517762660980225 = 0.7592259049415588 + 0.1 * 6.925503253936768
Epoch 190, val loss: 0.919488787651062
Epoch 200, training loss: 1.3820735216140747 = 0.6907123923301697 + 0.1 * 6.91361141204834
Epoch 200, val loss: 0.8663311004638672
Epoch 210, training loss: 1.320567011833191 = 0.6305426955223083 + 0.1 * 6.900243282318115
Epoch 210, val loss: 0.8209329843521118
Epoch 220, training loss: 1.2664390802383423 = 0.5761895775794983 + 0.1 * 6.90249490737915
Epoch 220, val loss: 0.7813750505447388
Epoch 230, training loss: 1.2143443822860718 = 0.5264065265655518 + 0.1 * 6.879378318786621
Epoch 230, val loss: 0.7468755841255188
Epoch 240, training loss: 1.166597604751587 = 0.47952035069465637 + 0.1 * 6.870771884918213
Epoch 240, val loss: 0.7162824869155884
Epoch 250, training loss: 1.1210989952087402 = 0.43474701046943665 + 0.1 * 6.863519191741943
Epoch 250, val loss: 0.6892619132995605
Epoch 260, training loss: 1.0781917572021484 = 0.39213231205940247 + 0.1 * 6.860593795776367
Epoch 260, val loss: 0.6661145091056824
Epoch 270, training loss: 1.0365127325057983 = 0.3518475294113159 + 0.1 * 6.846652030944824
Epoch 270, val loss: 0.6467604041099548
Epoch 280, training loss: 0.9979211091995239 = 0.313947856426239 + 0.1 * 6.839732646942139
Epoch 280, val loss: 0.6305432915687561
Epoch 290, training loss: 0.9637557864189148 = 0.2787867784500122 + 0.1 * 6.849689960479736
Epoch 290, val loss: 0.617099404335022
Epoch 300, training loss: 0.9292614459991455 = 0.2468741238117218 + 0.1 * 6.823873043060303
Epoch 300, val loss: 0.6062874794006348
Epoch 310, training loss: 0.8994107246398926 = 0.21804708242416382 + 0.1 * 6.813636302947998
Epoch 310, val loss: 0.5979085564613342
Epoch 320, training loss: 0.8741812705993652 = 0.19231918454170227 + 0.1 * 6.818620204925537
Epoch 320, val loss: 0.5920013785362244
Epoch 330, training loss: 0.8493114709854126 = 0.1698131263256073 + 0.1 * 6.794983863830566
Epoch 330, val loss: 0.5881956815719604
Epoch 340, training loss: 0.8285416960716248 = 0.1501370221376419 + 0.1 * 6.784046649932861
Epoch 340, val loss: 0.5864788889884949
Epoch 350, training loss: 0.810819149017334 = 0.1330716907978058 + 0.1 * 6.777474880218506
Epoch 350, val loss: 0.5865252017974854
Epoch 360, training loss: 0.7954351902008057 = 0.1183418482542038 + 0.1 * 6.770933151245117
Epoch 360, val loss: 0.5880455374717712
Epoch 370, training loss: 0.7817783951759338 = 0.10557187348604202 + 0.1 * 6.7620649337768555
Epoch 370, val loss: 0.5908710956573486
Epoch 380, training loss: 0.7703893780708313 = 0.09453421086072922 + 0.1 * 6.758551597595215
Epoch 380, val loss: 0.5948203802108765
Epoch 390, training loss: 0.7591594457626343 = 0.08498832583427429 + 0.1 * 6.741710662841797
Epoch 390, val loss: 0.5995085835456848
Epoch 400, training loss: 0.7512927055358887 = 0.07667236030101776 + 0.1 * 6.746203422546387
Epoch 400, val loss: 0.6049604415893555
Epoch 410, training loss: 0.7420133352279663 = 0.06945672631263733 + 0.1 * 6.725565433502197
Epoch 410, val loss: 0.6109694242477417
Epoch 420, training loss: 0.734592616558075 = 0.06315206736326218 + 0.1 * 6.714405059814453
Epoch 420, val loss: 0.6172658205032349
Epoch 430, training loss: 0.7297700643539429 = 0.05760662257671356 + 0.1 * 6.7216339111328125
Epoch 430, val loss: 0.6240012049674988
Epoch 440, training loss: 0.7231816053390503 = 0.05272004380822182 + 0.1 * 6.704615116119385
Epoch 440, val loss: 0.6309857368469238
Epoch 450, training loss: 0.7193073034286499 = 0.04839179664850235 + 0.1 * 6.7091546058654785
Epoch 450, val loss: 0.6381180882453918
Epoch 460, training loss: 0.714216947555542 = 0.044559307396411896 + 0.1 * 6.696576118469238
Epoch 460, val loss: 0.6453588604927063
Epoch 470, training loss: 0.7103831171989441 = 0.04114699363708496 + 0.1 * 6.692360877990723
Epoch 470, val loss: 0.6526115536689758
Epoch 480, training loss: 0.7062789797782898 = 0.038100142031908035 + 0.1 * 6.681788444519043
Epoch 480, val loss: 0.6599551439285278
Epoch 490, training loss: 0.7027345895767212 = 0.03536626696586609 + 0.1 * 6.6736836433410645
Epoch 490, val loss: 0.6672075390815735
Epoch 500, training loss: 0.702229917049408 = 0.03290506824851036 + 0.1 * 6.693248748779297
Epoch 500, val loss: 0.6744043231010437
Epoch 510, training loss: 0.6970950961112976 = 0.030695434659719467 + 0.1 * 6.66399621963501
Epoch 510, val loss: 0.6815587878227234
Epoch 520, training loss: 0.6977546215057373 = 0.028699379414319992 + 0.1 * 6.690552234649658
Epoch 520, val loss: 0.6885571479797363
Epoch 530, training loss: 0.6919987201690674 = 0.026898212730884552 + 0.1 * 6.651004791259766
Epoch 530, val loss: 0.695442795753479
Epoch 540, training loss: 0.6899502277374268 = 0.025259647518396378 + 0.1 * 6.646905422210693
Epoch 540, val loss: 0.7022245526313782
Epoch 550, training loss: 0.6885249614715576 = 0.02376212552189827 + 0.1 * 6.647628307342529
Epoch 550, val loss: 0.7089129090309143
Epoch 560, training loss: 0.6868112683296204 = 0.022397087886929512 + 0.1 * 6.644141674041748
Epoch 560, val loss: 0.7154620289802551
Epoch 570, training loss: 0.6849128603935242 = 0.02115141972899437 + 0.1 * 6.6376142501831055
Epoch 570, val loss: 0.7218764424324036
Epoch 580, training loss: 0.6832613348960876 = 0.020006073638796806 + 0.1 * 6.632552623748779
Epoch 580, val loss: 0.728105902671814
Epoch 590, training loss: 0.6817348003387451 = 0.01895434781908989 + 0.1 * 6.627804756164551
Epoch 590, val loss: 0.734311044216156
Epoch 600, training loss: 0.6799378991127014 = 0.017987344413995743 + 0.1 * 6.619505405426025
Epoch 600, val loss: 0.7402836084365845
Epoch 610, training loss: 0.6798977851867676 = 0.01709543727338314 + 0.1 * 6.628023147583008
Epoch 610, val loss: 0.7462300062179565
Epoch 620, training loss: 0.6770911812782288 = 0.016271129250526428 + 0.1 * 6.608200550079346
Epoch 620, val loss: 0.7519475817680359
Epoch 630, training loss: 0.6792416572570801 = 0.01550633180886507 + 0.1 * 6.637353420257568
Epoch 630, val loss: 0.7575234174728394
Epoch 640, training loss: 0.6766018867492676 = 0.014801055192947388 + 0.1 * 6.618008136749268
Epoch 640, val loss: 0.7630794644355774
Epoch 650, training loss: 0.6743443608283997 = 0.014145507477223873 + 0.1 * 6.601988792419434
Epoch 650, val loss: 0.768372654914856
Epoch 660, training loss: 0.6742694973945618 = 0.013533215038478374 + 0.1 * 6.607362747192383
Epoch 660, val loss: 0.773682713508606
Epoch 670, training loss: 0.6723037362098694 = 0.012962461449205875 + 0.1 * 6.59341287612915
Epoch 670, val loss: 0.7788028120994568
Epoch 680, training loss: 0.6724251508712769 = 0.012431340292096138 + 0.1 * 6.599937915802002
Epoch 680, val loss: 0.7838563323020935
Epoch 690, training loss: 0.6697677373886108 = 0.01193271018564701 + 0.1 * 6.578350067138672
Epoch 690, val loss: 0.7887632846832275
Epoch 700, training loss: 0.6710008978843689 = 0.011464488692581654 + 0.1 * 6.595364093780518
Epoch 700, val loss: 0.7936028838157654
Epoch 710, training loss: 0.669205367565155 = 0.0110273202881217 + 0.1 * 6.581779956817627
Epoch 710, val loss: 0.7983965277671814
Epoch 720, training loss: 0.6675751209259033 = 0.010616996325552464 + 0.1 * 6.569581031799316
Epoch 720, val loss: 0.8029845356941223
Epoch 730, training loss: 0.6681585907936096 = 0.010230452753603458 + 0.1 * 6.579280853271484
Epoch 730, val loss: 0.8075374960899353
Epoch 740, training loss: 0.6662927865982056 = 0.00986664928495884 + 0.1 * 6.564260959625244
Epoch 740, val loss: 0.8119735717773438
Epoch 750, training loss: 0.6689088344573975 = 0.00952323991805315 + 0.1 * 6.593855857849121
Epoch 750, val loss: 0.8162952661514282
Epoch 760, training loss: 0.6659632325172424 = 0.009201291017234325 + 0.1 * 6.567619323730469
Epoch 760, val loss: 0.8205878734588623
Epoch 770, training loss: 0.6651893258094788 = 0.008896236307919025 + 0.1 * 6.562930583953857
Epoch 770, val loss: 0.8247389197349548
Epoch 780, training loss: 0.6652134656906128 = 0.008607315830886364 + 0.1 * 6.566061496734619
Epoch 780, val loss: 0.8288735747337341
Epoch 790, training loss: 0.6637428998947144 = 0.008333627134561539 + 0.1 * 6.554092884063721
Epoch 790, val loss: 0.8328644037246704
Epoch 800, training loss: 0.6634290814399719 = 0.008074311539530754 + 0.1 * 6.5535478591918945
Epoch 800, val loss: 0.8368316888809204
Epoch 810, training loss: 0.6633837819099426 = 0.007828046567738056 + 0.1 * 6.5555572509765625
Epoch 810, val loss: 0.8407542705535889
Epoch 820, training loss: 0.6620317101478577 = 0.007594448979943991 + 0.1 * 6.544372081756592
Epoch 820, val loss: 0.8444825410842896
Epoch 830, training loss: 0.6634547114372253 = 0.007372905500233173 + 0.1 * 6.560817718505859
Epoch 830, val loss: 0.8482221364974976
Epoch 840, training loss: 0.6612029671669006 = 0.007161976303905249 + 0.1 * 6.540409564971924
Epoch 840, val loss: 0.8519237637519836
Epoch 850, training loss: 0.6607171893119812 = 0.006961197592318058 + 0.1 * 6.537559509277344
Epoch 850, val loss: 0.8554521203041077
Epoch 860, training loss: 0.6598756909370422 = 0.006769579369574785 + 0.1 * 6.531060695648193
Epoch 860, val loss: 0.8590212464332581
Epoch 870, training loss: 0.6612364649772644 = 0.006586884614080191 + 0.1 * 6.54649543762207
Epoch 870, val loss: 0.8624995946884155
Epoch 880, training loss: 0.659356951713562 = 0.00641290470957756 + 0.1 * 6.529440402984619
Epoch 880, val loss: 0.8658937811851501
Epoch 890, training loss: 0.6581900119781494 = 0.006247333716601133 + 0.1 * 6.519426345825195
Epoch 890, val loss: 0.8692154884338379
Epoch 900, training loss: 0.6576754450798035 = 0.006088301073759794 + 0.1 * 6.515871047973633
Epoch 900, val loss: 0.8724932074546814
Epoch 910, training loss: 0.6582575440406799 = 0.005935720168054104 + 0.1 * 6.523218154907227
Epoch 910, val loss: 0.8757755160331726
Epoch 920, training loss: 0.6575459241867065 = 0.005789651535451412 + 0.1 * 6.5175628662109375
Epoch 920, val loss: 0.878925085067749
Epoch 930, training loss: 0.6581941246986389 = 0.0056499894708395 + 0.1 * 6.5254411697387695
Epoch 930, val loss: 0.8820962309837341
Epoch 940, training loss: 0.6557807922363281 = 0.005516237113624811 + 0.1 * 6.502645492553711
Epoch 940, val loss: 0.8851804733276367
Epoch 950, training loss: 0.6575458645820618 = 0.005387764424085617 + 0.1 * 6.521580696105957
Epoch 950, val loss: 0.8882220983505249
Epoch 960, training loss: 0.6557572484016418 = 0.005264575127512217 + 0.1 * 6.504926681518555
Epoch 960, val loss: 0.8912246823310852
Epoch 970, training loss: 0.6560666561126709 = 0.005146130453795195 + 0.1 * 6.509204864501953
Epoch 970, val loss: 0.8941558003425598
Epoch 980, training loss: 0.6542910933494568 = 0.005032569635659456 + 0.1 * 6.492584705352783
Epoch 980, val loss: 0.897068977355957
Epoch 990, training loss: 0.6569664478302002 = 0.004922590218484402 + 0.1 * 6.5204386711120605
Epoch 990, val loss: 0.8998926877975464
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.7989518642425537 = 1.9392642974853516 + 0.1 * 8.59687614440918
Epoch 0, val loss: 1.9408879280090332
Epoch 10, training loss: 2.7887637615203857 = 1.9290814399719238 + 0.1 * 8.596822738647461
Epoch 10, val loss: 1.9308308362960815
Epoch 20, training loss: 2.7755398750305176 = 1.91588294506073 + 0.1 * 8.596570014953613
Epoch 20, val loss: 1.917567253112793
Epoch 30, training loss: 2.7563345432281494 = 1.8968867063522339 + 0.1 * 8.594477653503418
Epoch 30, val loss: 1.8983497619628906
Epoch 40, training loss: 2.726740598678589 = 1.868952751159668 + 0.1 * 8.577878952026367
Epoch 40, val loss: 1.8705731630325317
Epoch 50, training loss: 2.6832070350646973 = 1.8317726850509644 + 0.1 * 8.51434326171875
Epoch 50, val loss: 1.8357610702514648
Epoch 60, training loss: 2.6219887733459473 = 1.7938568592071533 + 0.1 * 8.281318664550781
Epoch 60, val loss: 1.8033512830734253
Epoch 70, training loss: 2.5692570209503174 = 1.7576814889907837 + 0.1 * 8.115756034851074
Epoch 70, val loss: 1.7722018957138062
Epoch 80, training loss: 2.4870553016662598 = 1.7105196714401245 + 0.1 * 7.765355587005615
Epoch 80, val loss: 1.7288545370101929
Epoch 90, training loss: 2.4018354415893555 = 1.6484254598617554 + 0.1 * 7.534099578857422
Epoch 90, val loss: 1.672783374786377
Epoch 100, training loss: 2.314561367034912 = 1.568923830986023 + 0.1 * 7.456376075744629
Epoch 100, val loss: 1.603379726409912
Epoch 110, training loss: 2.2178564071655273 = 1.4765509366989136 + 0.1 * 7.413054943084717
Epoch 110, val loss: 1.526197910308838
Epoch 120, training loss: 2.1175055503845215 = 1.37936532497406 + 0.1 * 7.381402969360352
Epoch 120, val loss: 1.446644902229309
Epoch 130, training loss: 2.0174779891967773 = 1.2831199169158936 + 0.1 * 7.343581199645996
Epoch 130, val loss: 1.3683916330337524
Epoch 140, training loss: 1.9186762571334839 = 1.1896848678588867 + 0.1 * 7.289913654327393
Epoch 140, val loss: 1.2924190759658813
Epoch 150, training loss: 1.8209424018859863 = 1.0991486310958862 + 0.1 * 7.21793794631958
Epoch 150, val loss: 1.2191959619522095
Epoch 160, training loss: 1.7256211042404175 = 1.0116357803344727 + 0.1 * 7.139853000640869
Epoch 160, val loss: 1.149285912513733
Epoch 170, training loss: 1.6349151134490967 = 0.9273152947425842 + 0.1 * 7.075997352600098
Epoch 170, val loss: 1.0826425552368164
Epoch 180, training loss: 1.5536189079284668 = 0.8492156267166138 + 0.1 * 7.044033050537109
Epoch 180, val loss: 1.0217171907424927
Epoch 190, training loss: 1.4797554016113281 = 0.7777104377746582 + 0.1 * 7.020448684692383
Epoch 190, val loss: 0.9660844802856445
Epoch 200, training loss: 1.4139349460601807 = 0.7132636904716492 + 0.1 * 7.006712436676025
Epoch 200, val loss: 0.9169264435768127
Epoch 210, training loss: 1.355536937713623 = 0.6558202505111694 + 0.1 * 6.997166156768799
Epoch 210, val loss: 0.874800443649292
Epoch 220, training loss: 1.3027973175048828 = 0.6040432453155518 + 0.1 * 6.987540245056152
Epoch 220, val loss: 0.8397811055183411
Epoch 230, training loss: 1.2542636394500732 = 0.5562168955802917 + 0.1 * 6.980467319488525
Epoch 230, val loss: 0.8108370304107666
Epoch 240, training loss: 1.2085164785385132 = 0.5115007162094116 + 0.1 * 6.970157623291016
Epoch 240, val loss: 0.7868339419364929
Epoch 250, training loss: 1.1659564971923828 = 0.4696452021598816 + 0.1 * 6.963112831115723
Epoch 250, val loss: 0.7672408223152161
Epoch 260, training loss: 1.1261776685714722 = 0.4308931231498718 + 0.1 * 6.952845096588135
Epoch 260, val loss: 0.7519010305404663
Epoch 270, training loss: 1.0899746417999268 = 0.3956165909767151 + 0.1 * 6.943580627441406
Epoch 270, val loss: 0.7409456968307495
Epoch 280, training loss: 1.057241439819336 = 0.36404842138290405 + 0.1 * 6.9319305419921875
Epoch 280, val loss: 0.7346699237823486
Epoch 290, training loss: 1.028031587600708 = 0.33599457144737244 + 0.1 * 6.920370578765869
Epoch 290, val loss: 0.7326443791389465
Epoch 300, training loss: 1.0018830299377441 = 0.311257928609848 + 0.1 * 6.906251430511475
Epoch 300, val loss: 0.7339526414871216
Epoch 310, training loss: 0.980124831199646 = 0.28951743245124817 + 0.1 * 6.906074047088623
Epoch 310, val loss: 0.7380757927894592
Epoch 320, training loss: 0.9592844247817993 = 0.27045315504074097 + 0.1 * 6.888312339782715
Epoch 320, val loss: 0.7442644238471985
Epoch 330, training loss: 0.9410003423690796 = 0.2534932792186737 + 0.1 * 6.875070571899414
Epoch 330, val loss: 0.7519151568412781
Epoch 340, training loss: 0.9254329204559326 = 0.2382504791021347 + 0.1 * 6.871824264526367
Epoch 340, val loss: 0.7608209848403931
Epoch 350, training loss: 0.9108506441116333 = 0.2245253622531891 + 0.1 * 6.86325216293335
Epoch 350, val loss: 0.7702311277389526
Epoch 360, training loss: 0.8974691033363342 = 0.2119203805923462 + 0.1 * 6.855486869812012
Epoch 360, val loss: 0.7801405787467957
Epoch 370, training loss: 0.885161817073822 = 0.20002888143062592 + 0.1 * 6.8513288497924805
Epoch 370, val loss: 0.7899543642997742
Epoch 380, training loss: 0.8730402588844299 = 0.1885039359331131 + 0.1 * 6.845363140106201
Epoch 380, val loss: 0.7995272278785706
Epoch 390, training loss: 0.8614386320114136 = 0.17701931297779083 + 0.1 * 6.844193458557129
Epoch 390, val loss: 0.8084700703620911
Epoch 400, training loss: 0.8498176336288452 = 0.16537687182426453 + 0.1 * 6.844407081604004
Epoch 400, val loss: 0.8162978291511536
Epoch 410, training loss: 0.83671635389328 = 0.15331847965717316 + 0.1 * 6.833978652954102
Epoch 410, val loss: 0.8229358792304993
Epoch 420, training loss: 0.8237830400466919 = 0.140631765127182 + 0.1 * 6.831512451171875
Epoch 420, val loss: 0.8281687498092651
Epoch 430, training loss: 0.8099730014801025 = 0.12735480070114136 + 0.1 * 6.826181888580322
Epoch 430, val loss: 0.8319756984710693
Epoch 440, training loss: 0.7961570620536804 = 0.11381208151578903 + 0.1 * 6.823449611663818
Epoch 440, val loss: 0.834807813167572
Epoch 450, training loss: 0.7861223220825195 = 0.10071074217557907 + 0.1 * 6.8541154861450195
Epoch 450, val loss: 0.8372403383255005
Epoch 460, training loss: 0.7702775597572327 = 0.08876003324985504 + 0.1 * 6.8151750564575195
Epoch 460, val loss: 0.8398123383522034
Epoch 470, training loss: 0.7591157555580139 = 0.07826396077871323 + 0.1 * 6.808517932891846
Epoch 470, val loss: 0.8433375954627991
Epoch 480, training loss: 0.7495811581611633 = 0.06930334120988846 + 0.1 * 6.802778244018555
Epoch 480, val loss: 0.8479180932044983
Epoch 490, training loss: 0.7415207028388977 = 0.06177828833460808 + 0.1 * 6.797423839569092
Epoch 490, val loss: 0.8535891175270081
Epoch 500, training loss: 0.7344436645507812 = 0.05549769476056099 + 0.1 * 6.789459705352783
Epoch 500, val loss: 0.8601245880126953
Epoch 510, training loss: 0.7285088300704956 = 0.05023124814033508 + 0.1 * 6.782775402069092
Epoch 510, val loss: 0.8671917915344238
Epoch 520, training loss: 0.7232312560081482 = 0.04575298726558685 + 0.1 * 6.774782657623291
Epoch 520, val loss: 0.8747108578681946
Epoch 530, training loss: 0.7203994393348694 = 0.04189521074295044 + 0.1 * 6.7850422859191895
Epoch 530, val loss: 0.8825209736824036
Epoch 540, training loss: 0.7178656458854675 = 0.03855070099234581 + 0.1 * 6.793149471282959
Epoch 540, val loss: 0.8904336094856262
Epoch 550, training loss: 0.711924135684967 = 0.03564415127038956 + 0.1 * 6.762799263000488
Epoch 550, val loss: 0.8981917500495911
Epoch 560, training loss: 0.7083515524864197 = 0.03307187184691429 + 0.1 * 6.752796649932861
Epoch 560, val loss: 0.9060081839561462
Epoch 570, training loss: 0.705035924911499 = 0.03077363222837448 + 0.1 * 6.7426228523254395
Epoch 570, val loss: 0.9137775301933289
Epoch 580, training loss: 0.70401531457901 = 0.028708631172776222 + 0.1 * 6.753066539764404
Epoch 580, val loss: 0.9215247631072998
Epoch 590, training loss: 0.7001277804374695 = 0.026852140203118324 + 0.1 * 6.732756614685059
Epoch 590, val loss: 0.9290711283683777
Epoch 600, training loss: 0.6977459192276001 = 0.02517721615731716 + 0.1 * 6.725686550140381
Epoch 600, val loss: 0.9365218281745911
Epoch 610, training loss: 0.6957930326461792 = 0.023659514263272285 + 0.1 * 6.721334934234619
Epoch 610, val loss: 0.9437654614448547
Epoch 620, training loss: 0.6944387555122375 = 0.02227817103266716 + 0.1 * 6.7216057777404785
Epoch 620, val loss: 0.9509142637252808
Epoch 630, training loss: 0.6922731399536133 = 0.021019425243139267 + 0.1 * 6.7125372886657715
Epoch 630, val loss: 0.9579006433486938
Epoch 640, training loss: 0.6907147765159607 = 0.01986888237297535 + 0.1 * 6.708458423614502
Epoch 640, val loss: 0.9647632837295532
Epoch 650, training loss: 0.6889225840568542 = 0.01881551183760166 + 0.1 * 6.701070785522461
Epoch 650, val loss: 0.9714224338531494
Epoch 660, training loss: 0.6893876791000366 = 0.01784627139568329 + 0.1 * 6.715413570404053
Epoch 660, val loss: 0.9779803156852722
Epoch 670, training loss: 0.6865493655204773 = 0.016956867650151253 + 0.1 * 6.695924758911133
Epoch 670, val loss: 0.9843381643295288
Epoch 680, training loss: 0.6852385401725769 = 0.016136644408106804 + 0.1 * 6.691018581390381
Epoch 680, val loss: 0.990547239780426
Epoch 690, training loss: 0.6851568222045898 = 0.015377899631857872 + 0.1 * 6.697789192199707
Epoch 690, val loss: 0.9966163039207458
Epoch 700, training loss: 0.68294358253479 = 0.014674493111670017 + 0.1 * 6.682690620422363
Epoch 700, val loss: 1.0026121139526367
Epoch 710, training loss: 0.684270977973938 = 0.014021654613316059 + 0.1 * 6.702493190765381
Epoch 710, val loss: 1.0084688663482666
Epoch 720, training loss: 0.6810836791992188 = 0.013416165485978127 + 0.1 * 6.676674842834473
Epoch 720, val loss: 1.0141061544418335
Epoch 730, training loss: 0.6796462535858154 = 0.012853242456912994 + 0.1 * 6.667929649353027
Epoch 730, val loss: 1.0195528268814087
Epoch 740, training loss: 0.6788909435272217 = 0.012328180484473705 + 0.1 * 6.665627479553223
Epoch 740, val loss: 1.024957537651062
Epoch 750, training loss: 0.6798801422119141 = 0.011835644021630287 + 0.1 * 6.680445194244385
Epoch 750, val loss: 1.030263900756836
Epoch 760, training loss: 0.6770696640014648 = 0.01137619186192751 + 0.1 * 6.6569342613220215
Epoch 760, val loss: 1.0353734493255615
Epoch 770, training loss: 0.6786170601844788 = 0.010944957844913006 + 0.1 * 6.676721096038818
Epoch 770, val loss: 1.0404527187347412
Epoch 780, training loss: 0.6772915124893188 = 0.010540776886045933 + 0.1 * 6.667507171630859
Epoch 780, val loss: 1.0454330444335938
Epoch 790, training loss: 0.6752980947494507 = 0.010161319747567177 + 0.1 * 6.651367664337158
Epoch 790, val loss: 1.0501933097839355
Epoch 800, training loss: 0.6746187210083008 = 0.009804359637200832 + 0.1 * 6.648143291473389
Epoch 800, val loss: 1.0547970533370972
Epoch 810, training loss: 0.6736623048782349 = 0.009468038566410542 + 0.1 * 6.641942501068115
Epoch 810, val loss: 1.0593913793563843
Epoch 820, training loss: 0.6746008396148682 = 0.009150546975433826 + 0.1 * 6.6545023918151855
Epoch 820, val loss: 1.0638612508773804
Epoch 830, training loss: 0.6726037859916687 = 0.008850396610796452 + 0.1 * 6.637533664703369
Epoch 830, val loss: 1.0682140588760376
Epoch 840, training loss: 0.6716417670249939 = 0.00856749527156353 + 0.1 * 6.63074254989624
Epoch 840, val loss: 1.0724663734436035
Epoch 850, training loss: 0.6709479093551636 = 0.00829960685223341 + 0.1 * 6.626482963562012
Epoch 850, val loss: 1.0765427350997925
Epoch 860, training loss: 0.6695664525032043 = 0.008046045899391174 + 0.1 * 6.615203857421875
Epoch 860, val loss: 1.080659031867981
Epoch 870, training loss: 0.6692039966583252 = 0.0078046368435025215 + 0.1 * 6.6139936447143555
Epoch 870, val loss: 1.084653377532959
Epoch 880, training loss: 0.668258547782898 = 0.007575688883662224 + 0.1 * 6.606828689575195
Epoch 880, val loss: 1.0885170698165894
Epoch 890, training loss: 0.6693851947784424 = 0.007357933092862368 + 0.1 * 6.620272159576416
Epoch 890, val loss: 1.0923810005187988
Epoch 900, training loss: 0.6674561500549316 = 0.007151623256504536 + 0.1 * 6.603045463562012
Epoch 900, val loss: 1.0960350036621094
Epoch 910, training loss: 0.6653264760971069 = 0.006955290678888559 + 0.1 * 6.583712100982666
Epoch 910, val loss: 1.0996432304382324
Epoch 920, training loss: 0.6673663854598999 = 0.006767250597476959 + 0.1 * 6.605990886688232
Epoch 920, val loss: 1.1033012866973877
Epoch 930, training loss: 0.6656674146652222 = 0.006588032469153404 + 0.1 * 6.590793609619141
Epoch 930, val loss: 1.1068263053894043
Epoch 940, training loss: 0.6656294465065002 = 0.006416702643036842 + 0.1 * 6.592127799987793
Epoch 940, val loss: 1.1102231740951538
Epoch 950, training loss: 0.6637685894966125 = 0.006253377068787813 + 0.1 * 6.575152397155762
Epoch 950, val loss: 1.1136201620101929
Epoch 960, training loss: 0.663426399230957 = 0.006096450611948967 + 0.1 * 6.573299884796143
Epoch 960, val loss: 1.1169930696487427
Epoch 970, training loss: 0.6634488701820374 = 0.005946247838437557 + 0.1 * 6.575026512145996
Epoch 970, val loss: 1.1203094720840454
Epoch 980, training loss: 0.6627036929130554 = 0.005802477709949017 + 0.1 * 6.56901216506958
Epoch 980, val loss: 1.1234285831451416
Epoch 990, training loss: 0.6631985902786255 = 0.005664639174938202 + 0.1 * 6.575339317321777
Epoch 990, val loss: 1.1266138553619385
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8376383763837639
The final CL Acc:0.80000, 0.03024, The final GNN Acc:0.83711, 0.00114
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10550])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8236746788024902 = 1.963993787765503 + 0.1 * 8.596808433532715
Epoch 0, val loss: 1.9659672975540161
Epoch 10, training loss: 2.812211751937866 = 1.9525432586669922 + 0.1 * 8.596685409545898
Epoch 10, val loss: 1.9547218084335327
Epoch 20, training loss: 2.7976315021514893 = 1.9380539655685425 + 0.1 * 8.595775604248047
Epoch 20, val loss: 1.9399495124816895
Epoch 30, training loss: 2.7761058807373047 = 1.917284369468689 + 0.1 * 8.588214874267578
Epoch 30, val loss: 1.9181649684906006
Epoch 40, training loss: 2.7414588928222656 = 1.8866835832595825 + 0.1 * 8.54775333404541
Epoch 40, val loss: 1.886095404624939
Epoch 50, training loss: 2.6833276748657227 = 1.8469351530075073 + 0.1 * 8.36392593383789
Epoch 50, val loss: 1.8467638492584229
Epoch 60, training loss: 2.6325535774230957 = 1.8075921535491943 + 0.1 * 8.249612808227539
Epoch 60, val loss: 1.8120037317276
Epoch 70, training loss: 2.5865283012390137 = 1.7775176763534546 + 0.1 * 8.090105056762695
Epoch 70, val loss: 1.7865489721298218
Epoch 80, training loss: 2.5170352458953857 = 1.7425618171691895 + 0.1 * 7.7447333335876465
Epoch 80, val loss: 1.754634976387024
Epoch 90, training loss: 2.4363014698028564 = 1.7001631259918213 + 0.1 * 7.361382961273193
Epoch 90, val loss: 1.7178946733474731
Epoch 100, training loss: 2.362372398376465 = 1.6442415714263916 + 0.1 * 7.181307792663574
Epoch 100, val loss: 1.670461893081665
Epoch 110, training loss: 2.2830593585968018 = 1.5720185041427612 + 0.1 * 7.110409259796143
Epoch 110, val loss: 1.6074894666671753
Epoch 120, training loss: 2.198939800262451 = 1.4925273656845093 + 0.1 * 7.06412410736084
Epoch 120, val loss: 1.5407824516296387
Epoch 130, training loss: 2.1146702766418457 = 1.4128702878952026 + 0.1 * 7.017999172210693
Epoch 130, val loss: 1.4756475687026978
Epoch 140, training loss: 2.0321366786956787 = 1.3350672721862793 + 0.1 * 6.970693111419678
Epoch 140, val loss: 1.413267731666565
Epoch 150, training loss: 1.9486198425292969 = 1.2559733390808105 + 0.1 * 6.926465034484863
Epoch 150, val loss: 1.3511642217636108
Epoch 160, training loss: 1.8643865585327148 = 1.1747361421585083 + 0.1 * 6.8965044021606445
Epoch 160, val loss: 1.2888824939727783
Epoch 170, training loss: 1.7813520431518555 = 1.0938515663146973 + 0.1 * 6.875004291534424
Epoch 170, val loss: 1.2288010120391846
Epoch 180, training loss: 1.6994826793670654 = 1.013974905014038 + 0.1 * 6.855077743530273
Epoch 180, val loss: 1.171100378036499
Epoch 190, training loss: 1.6220619678497314 = 0.9366786479949951 + 0.1 * 6.853832721710205
Epoch 190, val loss: 1.1168545484542847
Epoch 200, training loss: 1.5473469495773315 = 0.8644235134124756 + 0.1 * 6.8292341232299805
Epoch 200, val loss: 1.0676087141036987
Epoch 210, training loss: 1.4778671264648438 = 0.7966903448104858 + 0.1 * 6.811768531799316
Epoch 210, val loss: 1.0223698616027832
Epoch 220, training loss: 1.4144103527069092 = 0.734131395816803 + 0.1 * 6.802789211273193
Epoch 220, val loss: 0.9822914600372314
Epoch 230, training loss: 1.3563512563705444 = 0.6776678562164307 + 0.1 * 6.786833763122559
Epoch 230, val loss: 0.9480983018875122
Epoch 240, training loss: 1.3045927286148071 = 0.6267165541648865 + 0.1 * 6.778761386871338
Epoch 240, val loss: 0.9201716184616089
Epoch 250, training loss: 1.25841224193573 = 0.5811470150947571 + 0.1 * 6.7726521492004395
Epoch 250, val loss: 0.8984412550926208
Epoch 260, training loss: 1.2149752378463745 = 0.5400636792182922 + 0.1 * 6.749115467071533
Epoch 260, val loss: 0.8820320963859558
Epoch 270, training loss: 1.1780948638916016 = 0.5023065805435181 + 0.1 * 6.757882118225098
Epoch 270, val loss: 0.8702659606933594
Epoch 280, training loss: 1.1406582593917847 = 0.4677521288394928 + 0.1 * 6.729060649871826
Epoch 280, val loss: 0.8624195456504822
Epoch 290, training loss: 1.1070013046264648 = 0.43572771549224854 + 0.1 * 6.712735652923584
Epoch 290, val loss: 0.8578648567199707
Epoch 300, training loss: 1.0771639347076416 = 0.4057886600494385 + 0.1 * 6.7137532234191895
Epoch 300, val loss: 0.8559762239456177
Epoch 310, training loss: 1.0497348308563232 = 0.37806209921836853 + 0.1 * 6.7167277336120605
Epoch 310, val loss: 0.8564975261688232
Epoch 320, training loss: 1.0212078094482422 = 0.352226585149765 + 0.1 * 6.689812183380127
Epoch 320, val loss: 0.8588166236877441
Epoch 330, training loss: 0.9951717853546143 = 0.3275919556617737 + 0.1 * 6.675797939300537
Epoch 330, val loss: 0.8624709844589233
Epoch 340, training loss: 0.9735815525054932 = 0.3038085103034973 + 0.1 * 6.697730541229248
Epoch 340, val loss: 0.8673920035362244
Epoch 350, training loss: 0.9466879367828369 = 0.2809276878833771 + 0.1 * 6.657602787017822
Epoch 350, val loss: 0.8733772039413452
Epoch 360, training loss: 0.9255156517028809 = 0.2587985098361969 + 0.1 * 6.667171001434326
Epoch 360, val loss: 0.8807650804519653
Epoch 370, training loss: 0.9031888842582703 = 0.23764050006866455 + 0.1 * 6.655483722686768
Epoch 370, val loss: 0.8892790079116821
Epoch 380, training loss: 0.8824019432067871 = 0.21763333678245544 + 0.1 * 6.647686004638672
Epoch 380, val loss: 0.8990438580513
Epoch 390, training loss: 0.8624171018600464 = 0.19903206825256348 + 0.1 * 6.63385009765625
Epoch 390, val loss: 0.9102008938789368
Epoch 400, training loss: 0.845214307308197 = 0.18196509778499603 + 0.1 * 6.632491588592529
Epoch 400, val loss: 0.9228828549385071
Epoch 410, training loss: 0.8285529613494873 = 0.16649015247821808 + 0.1 * 6.6206278800964355
Epoch 410, val loss: 0.9368484616279602
Epoch 420, training loss: 0.8147904276847839 = 0.1525130420923233 + 0.1 * 6.622774124145508
Epoch 420, val loss: 0.9521461725234985
Epoch 430, training loss: 0.8009224534034729 = 0.13992778956890106 + 0.1 * 6.609946250915527
Epoch 430, val loss: 0.9684612154960632
Epoch 440, training loss: 0.789693295955658 = 0.12857259809970856 + 0.1 * 6.611207008361816
Epoch 440, val loss: 0.9856243133544922
Epoch 450, training loss: 0.7791993618011475 = 0.11835367977619171 + 0.1 * 6.608456611633301
Epoch 450, val loss: 1.0031883716583252
Epoch 460, training loss: 0.7680315971374512 = 0.10914526879787445 + 0.1 * 6.588862895965576
Epoch 460, val loss: 1.0211173295974731
Epoch 470, training loss: 0.7603877782821655 = 0.10080736875534058 + 0.1 * 6.595803737640381
Epoch 470, val loss: 1.0392223596572876
Epoch 480, training loss: 0.7540737390518188 = 0.09326513856649399 + 0.1 * 6.608086109161377
Epoch 480, val loss: 1.0573207139968872
Epoch 490, training loss: 0.7446740865707397 = 0.08645208179950714 + 0.1 * 6.582220077514648
Epoch 490, val loss: 1.075324535369873
Epoch 500, training loss: 0.7374846935272217 = 0.08026871085166931 + 0.1 * 6.572160243988037
Epoch 500, val loss: 1.0931485891342163
Epoch 510, training loss: 0.7324192523956299 = 0.07463863492012024 + 0.1 * 6.577805995941162
Epoch 510, val loss: 1.110785961151123
Epoch 520, training loss: 0.7275837659835815 = 0.06952325254678726 + 0.1 * 6.5806050300598145
Epoch 520, val loss: 1.1281633377075195
Epoch 530, training loss: 0.7242109775543213 = 0.06487652659416199 + 0.1 * 6.593344688415527
Epoch 530, val loss: 1.145174264907837
Epoch 540, training loss: 0.7165493965148926 = 0.060649871826171875 + 0.1 * 6.558995246887207
Epoch 540, val loss: 1.1615631580352783
Epoch 550, training loss: 0.7123421430587769 = 0.056784115731716156 + 0.1 * 6.555580139160156
Epoch 550, val loss: 1.1779130697250366
Epoch 560, training loss: 0.7079946994781494 = 0.053244296461343765 + 0.1 * 6.54750394821167
Epoch 560, val loss: 1.1938084363937378
Epoch 570, training loss: 0.7056688070297241 = 0.04999491944909096 + 0.1 * 6.556738376617432
Epoch 570, val loss: 1.2095091342926025
Epoch 580, training loss: 0.7019787430763245 = 0.047016579657793045 + 0.1 * 6.54962158203125
Epoch 580, val loss: 1.2248525619506836
Epoch 590, training loss: 0.6977165937423706 = 0.04428202286362648 + 0.1 * 6.534345626831055
Epoch 590, val loss: 1.239740252494812
Epoch 600, training loss: 0.6952800750732422 = 0.04176446422934532 + 0.1 * 6.53515625
Epoch 600, val loss: 1.2543056011199951
Epoch 610, training loss: 0.6942254304885864 = 0.039440326392650604 + 0.1 * 6.547850608825684
Epoch 610, val loss: 1.2687320709228516
Epoch 620, training loss: 0.6901938915252686 = 0.037298768758773804 + 0.1 * 6.5289506912231445
Epoch 620, val loss: 1.2826489210128784
Epoch 630, training loss: 0.6880199909210205 = 0.035315439105033875 + 0.1 * 6.527045249938965
Epoch 630, val loss: 1.296331763267517
Epoch 640, training loss: 0.6862809062004089 = 0.03348018229007721 + 0.1 * 6.5280070304870605
Epoch 640, val loss: 1.309782862663269
Epoch 650, training loss: 0.6836392879486084 = 0.03178093954920769 + 0.1 * 6.518583297729492
Epoch 650, val loss: 1.3228299617767334
Epoch 660, training loss: 0.682262122631073 = 0.030199889093637466 + 0.1 * 6.520622253417969
Epoch 660, val loss: 1.3356971740722656
Epoch 670, training loss: 0.6797841191291809 = 0.02873101644217968 + 0.1 * 6.510531425476074
Epoch 670, val loss: 1.3481632471084595
Epoch 680, training loss: 0.677897572517395 = 0.027364134788513184 + 0.1 * 6.505334377288818
Epoch 680, val loss: 1.360472559928894
Epoch 690, training loss: 0.6771506667137146 = 0.026090355589985847 + 0.1 * 6.510602951049805
Epoch 690, val loss: 1.3724528551101685
Epoch 700, training loss: 0.6758475303649902 = 0.024902433156967163 + 0.1 * 6.509450912475586
Epoch 700, val loss: 1.384320616722107
Epoch 710, training loss: 0.673663318157196 = 0.023793889209628105 + 0.1 * 6.49869441986084
Epoch 710, val loss: 1.3958300352096558
Epoch 720, training loss: 0.6737633347511292 = 0.02275608293712139 + 0.1 * 6.510072231292725
Epoch 720, val loss: 1.4071850776672363
Epoch 730, training loss: 0.671470582485199 = 0.021785594522953033 + 0.1 * 6.496849536895752
Epoch 730, val loss: 1.4182108640670776
Epoch 740, training loss: 0.6732516884803772 = 0.020874686539173126 + 0.1 * 6.523769855499268
Epoch 740, val loss: 1.4290353059768677
Epoch 750, training loss: 0.6697258949279785 = 0.020022783428430557 + 0.1 * 6.497030735015869
Epoch 750, val loss: 1.4396817684173584
Epoch 760, training loss: 0.6676738858222961 = 0.019222168251872063 + 0.1 * 6.4845170974731445
Epoch 760, val loss: 1.4500735998153687
Epoch 770, training loss: 0.6685506701469421 = 0.018467633053660393 + 0.1 * 6.50083065032959
Epoch 770, val loss: 1.460427165031433
Epoch 780, training loss: 0.6674569249153137 = 0.01775810867547989 + 0.1 * 6.496987819671631
Epoch 780, val loss: 1.4702266454696655
Epoch 790, training loss: 0.6660619974136353 = 0.01709171012043953 + 0.1 * 6.4897027015686035
Epoch 790, val loss: 1.4800418615341187
Epoch 800, training loss: 0.6640996932983398 = 0.016461273655295372 + 0.1 * 6.476384162902832
Epoch 800, val loss: 1.4896734952926636
Epoch 810, training loss: 0.6651486158370972 = 0.015866240486502647 + 0.1 * 6.492823600769043
Epoch 810, val loss: 1.4990965127944946
Epoch 820, training loss: 0.663690984249115 = 0.015305822715163231 + 0.1 * 6.483851432800293
Epoch 820, val loss: 1.5083881616592407
Epoch 830, training loss: 0.6618935465812683 = 0.014774559065699577 + 0.1 * 6.471189975738525
Epoch 830, val loss: 1.5174390077590942
Epoch 840, training loss: 0.6614561080932617 = 0.014270931482315063 + 0.1 * 6.471851348876953
Epoch 840, val loss: 1.5264875888824463
Epoch 850, training loss: 0.6632509231567383 = 0.013792544603347778 + 0.1 * 6.4945831298828125
Epoch 850, val loss: 1.53533136844635
Epoch 860, training loss: 0.6614519953727722 = 0.013340492732822895 + 0.1 * 6.481114864349365
Epoch 860, val loss: 1.5438324213027954
Epoch 870, training loss: 0.6601426601409912 = 0.012911789119243622 + 0.1 * 6.47230863571167
Epoch 870, val loss: 1.552275538444519
Epoch 880, training loss: 0.6585618257522583 = 0.012504283338785172 + 0.1 * 6.460575103759766
Epoch 880, val loss: 1.560526728630066
Epoch 890, training loss: 0.6591806411743164 = 0.012117108330130577 + 0.1 * 6.470635414123535
Epoch 890, val loss: 1.5687552690505981
Epoch 900, training loss: 0.6583582162857056 = 0.011747483164072037 + 0.1 * 6.466107368469238
Epoch 900, val loss: 1.576841115951538
Epoch 910, training loss: 0.6575547456741333 = 0.011396125890314579 + 0.1 * 6.461585998535156
Epoch 910, val loss: 1.584613561630249
Epoch 920, training loss: 0.6564102172851562 = 0.011061051860451698 + 0.1 * 6.4534912109375
Epoch 920, val loss: 1.5923773050308228
Epoch 930, training loss: 0.6563028693199158 = 0.010741975158452988 + 0.1 * 6.45560884475708
Epoch 930, val loss: 1.599898338317871
Epoch 940, training loss: 0.6561112403869629 = 0.01043686456978321 + 0.1 * 6.456743240356445
Epoch 940, val loss: 1.6074501276016235
Epoch 950, training loss: 0.6559758186340332 = 0.01014617644250393 + 0.1 * 6.458296298980713
Epoch 950, val loss: 1.6148191690444946
Epoch 960, training loss: 0.6565408110618591 = 0.00986744835972786 + 0.1 * 6.466733455657959
Epoch 960, val loss: 1.6220906972885132
Epoch 970, training loss: 0.6543317437171936 = 0.009601586498320103 + 0.1 * 6.447301387786865
Epoch 970, val loss: 1.6290733814239502
Epoch 980, training loss: 0.6550238728523254 = 0.009347011335194111 + 0.1 * 6.45676851272583
Epoch 980, val loss: 1.6360019445419312
Epoch 990, training loss: 0.6542065739631653 = 0.009103539399802685 + 0.1 * 6.451030254364014
Epoch 990, val loss: 1.6429766416549683
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 2.806053876876831 = 1.9463707208633423 + 0.1 * 8.596831321716309
Epoch 0, val loss: 1.9448013305664062
Epoch 10, training loss: 2.796262264251709 = 1.9365899562835693 + 0.1 * 8.596722602844238
Epoch 10, val loss: 1.9352166652679443
Epoch 20, training loss: 2.7841005325317383 = 1.9244905710220337 + 0.1 * 8.596099853515625
Epoch 20, val loss: 1.9226831197738647
Epoch 30, training loss: 2.766533374786377 = 1.9074269533157349 + 0.1 * 8.591063499450684
Epoch 30, val loss: 1.904382348060608
Epoch 40, training loss: 2.73797607421875 = 1.882392406463623 + 0.1 * 8.555835723876953
Epoch 40, val loss: 1.8772000074386597
Epoch 50, training loss: 2.681216239929199 = 1.8479219675064087 + 0.1 * 8.3329439163208
Epoch 50, val loss: 1.8409911394119263
Epoch 60, training loss: 2.5977396965026855 = 1.8096791505813599 + 0.1 * 7.880606651306152
Epoch 60, val loss: 1.8039747476577759
Epoch 70, training loss: 2.5207409858703613 = 1.774808645248413 + 0.1 * 7.459323883056641
Epoch 70, val loss: 1.7731099128723145
Epoch 80, training loss: 2.4607620239257812 = 1.7381688356399536 + 0.1 * 7.225930690765381
Epoch 80, val loss: 1.7407236099243164
Epoch 90, training loss: 2.4043688774108887 = 1.6946210861206055 + 0.1 * 7.097476482391357
Epoch 90, val loss: 1.702370047569275
Epoch 100, training loss: 2.33969783782959 = 1.6371537446975708 + 0.1 * 7.025439739227295
Epoch 100, val loss: 1.652344822883606
Epoch 110, training loss: 2.260047674179077 = 1.5614161491394043 + 0.1 * 6.986314296722412
Epoch 110, val loss: 1.5863285064697266
Epoch 120, training loss: 2.1626529693603516 = 1.4664534330368042 + 0.1 * 6.961996078491211
Epoch 120, val loss: 1.504520297050476
Epoch 130, training loss: 2.050894260406494 = 1.3566004037857056 + 0.1 * 6.942937850952148
Epoch 130, val loss: 1.411077618598938
Epoch 140, training loss: 1.9321255683898926 = 1.239676833152771 + 0.1 * 6.924487113952637
Epoch 140, val loss: 1.3137232065200806
Epoch 150, training loss: 1.8162378072738647 = 1.1245619058609009 + 0.1 * 6.916759014129639
Epoch 150, val loss: 1.2202483415603638
Epoch 160, training loss: 1.7108193635940552 = 1.0214742422103882 + 0.1 * 6.89345121383667
Epoch 160, val loss: 1.1390330791473389
Epoch 170, training loss: 1.618635892868042 = 0.93048095703125 + 0.1 * 6.881548881530762
Epoch 170, val loss: 1.068987488746643
Epoch 180, training loss: 1.5376663208007812 = 0.8499745726585388 + 0.1 * 6.876916885375977
Epoch 180, val loss: 1.0097501277923584
Epoch 190, training loss: 1.465083122253418 = 0.7793682217597961 + 0.1 * 6.857149600982666
Epoch 190, val loss: 0.9603583216667175
Epoch 200, training loss: 1.4002959728240967 = 0.7159292101860046 + 0.1 * 6.843667507171631
Epoch 200, val loss: 0.9184038639068604
Epoch 210, training loss: 1.3424508571624756 = 0.6590698957443237 + 0.1 * 6.833808898925781
Epoch 210, val loss: 0.8834500908851624
Epoch 220, training loss: 1.2899973392486572 = 0.6079338788986206 + 0.1 * 6.820634841918945
Epoch 220, val loss: 0.8549383878707886
Epoch 230, training loss: 1.241882085800171 = 0.5611487627029419 + 0.1 * 6.807332992553711
Epoch 230, val loss: 0.8319476842880249
Epoch 240, training loss: 1.1985697746276855 = 0.5182493925094604 + 0.1 * 6.803203105926514
Epoch 240, val loss: 0.8140162229537964
Epoch 250, training loss: 1.1585115194320679 = 0.4794444739818573 + 0.1 * 6.790669918060303
Epoch 250, val loss: 0.8003348112106323
Epoch 260, training loss: 1.1223666667938232 = 0.4441436231136322 + 0.1 * 6.782230854034424
Epoch 260, val loss: 0.7901013493537903
Epoch 270, training loss: 1.0891270637512207 = 0.41161149740219116 + 0.1 * 6.775155544281006
Epoch 270, val loss: 0.7828734517097473
Epoch 280, training loss: 1.0601859092712402 = 0.38132181763648987 + 0.1 * 6.78864049911499
Epoch 280, val loss: 0.7782007455825806
Epoch 290, training loss: 1.03013014793396 = 0.3531372547149658 + 0.1 * 6.769928455352783
Epoch 290, val loss: 0.775361180305481
Epoch 300, training loss: 1.0020922422409058 = 0.3263148367404938 + 0.1 * 6.757774353027344
Epoch 300, val loss: 0.7742332220077515
Epoch 310, training loss: 0.975286602973938 = 0.30032166838645935 + 0.1 * 6.749649524688721
Epoch 310, val loss: 0.7743569612503052
Epoch 320, training loss: 0.9505199193954468 = 0.2749707102775574 + 0.1 * 6.755492210388184
Epoch 320, val loss: 0.7757797241210938
Epoch 330, training loss: 0.9244729280471802 = 0.25052401423454285 + 0.1 * 6.73948860168457
Epoch 330, val loss: 0.7782696485519409
Epoch 340, training loss: 0.9012438058853149 = 0.22720620036125183 + 0.1 * 6.740375518798828
Epoch 340, val loss: 0.7823813557624817
Epoch 350, training loss: 0.878915548324585 = 0.2054433971643448 + 0.1 * 6.734721660614014
Epoch 350, val loss: 0.7877203226089478
Epoch 360, training loss: 0.8579387068748474 = 0.18536657094955444 + 0.1 * 6.72572135925293
Epoch 360, val loss: 0.794665515422821
Epoch 370, training loss: 0.838810920715332 = 0.16708317399024963 + 0.1 * 6.7172770500183105
Epoch 370, val loss: 0.8030278086662292
Epoch 380, training loss: 0.8257490396499634 = 0.150612473487854 + 0.1 * 6.751365661621094
Epoch 380, val loss: 0.8127185106277466
Epoch 390, training loss: 0.8070390224456787 = 0.13606368005275726 + 0.1 * 6.709753036499023
Epoch 390, val loss: 0.8226801753044128
Epoch 400, training loss: 0.7933432459831238 = 0.1231408640742302 + 0.1 * 6.702023506164551
Epoch 400, val loss: 0.8335630893707275
Epoch 410, training loss: 0.7810121178627014 = 0.1116243526339531 + 0.1 * 6.693877220153809
Epoch 410, val loss: 0.8447307348251343
Epoch 420, training loss: 0.7723309397697449 = 0.10135078430175781 + 0.1 * 6.709801197052002
Epoch 420, val loss: 0.8562868237495422
Epoch 430, training loss: 0.7631607055664062 = 0.09222916513681412 + 0.1 * 6.709315299987793
Epoch 430, val loss: 0.8677948117256165
Epoch 440, training loss: 0.7524523138999939 = 0.08414553850889206 + 0.1 * 6.683067321777344
Epoch 440, val loss: 0.8792712092399597
Epoch 450, training loss: 0.7439851760864258 = 0.0769445151090622 + 0.1 * 6.670406341552734
Epoch 450, val loss: 0.8908526301383972
Epoch 460, training loss: 0.7386803030967712 = 0.07050515711307526 + 0.1 * 6.681751728057861
Epoch 460, val loss: 0.9022230505943298
Epoch 470, training loss: 0.7318590879440308 = 0.06476394087076187 + 0.1 * 6.670950889587402
Epoch 470, val loss: 0.9134213328361511
Epoch 480, training loss: 0.7255192995071411 = 0.05961593985557556 + 0.1 * 6.65903377532959
Epoch 480, val loss: 0.924496591091156
Epoch 490, training loss: 0.7212079763412476 = 0.05499834567308426 + 0.1 * 6.66209602355957
Epoch 490, val loss: 0.9354407787322998
Epoch 500, training loss: 0.7154582738876343 = 0.05085841193795204 + 0.1 * 6.645998001098633
Epoch 500, val loss: 0.9460029602050781
Epoch 510, training loss: 0.7121408581733704 = 0.04712946340441704 + 0.1 * 6.650114059448242
Epoch 510, val loss: 0.9565684199333191
Epoch 520, training loss: 0.7083513140678406 = 0.043772246688604355 + 0.1 * 6.6457905769348145
Epoch 520, val loss: 0.9666420221328735
Epoch 530, training loss: 0.7031710743904114 = 0.040741778910160065 + 0.1 * 6.624292850494385
Epoch 530, val loss: 0.9768014550209045
Epoch 540, training loss: 0.7006341814994812 = 0.03799805790185928 + 0.1 * 6.626360893249512
Epoch 540, val loss: 0.9867013692855835
Epoch 550, training loss: 0.6966177225112915 = 0.03551546111702919 + 0.1 * 6.611021995544434
Epoch 550, val loss: 0.9962817430496216
Epoch 560, training loss: 0.6960128545761108 = 0.03326519578695297 + 0.1 * 6.627476692199707
Epoch 560, val loss: 1.0057055950164795
Epoch 570, training loss: 0.6920082569122314 = 0.03122447058558464 + 0.1 * 6.607837677001953
Epoch 570, val loss: 1.0146973133087158
Epoch 580, training loss: 0.6889443397521973 = 0.029361287131905556 + 0.1 * 6.595829963684082
Epoch 580, val loss: 1.0237112045288086
Epoch 590, training loss: 0.6898911595344543 = 0.02765554003417492 + 0.1 * 6.622355937957764
Epoch 590, val loss: 1.032577633857727
Epoch 600, training loss: 0.6863082647323608 = 0.026095956563949585 + 0.1 * 6.602122783660889
Epoch 600, val loss: 1.0411357879638672
Epoch 610, training loss: 0.6831349730491638 = 0.024671774357557297 + 0.1 * 6.58463191986084
Epoch 610, val loss: 1.0496197938919067
Epoch 620, training loss: 0.6811695694923401 = 0.023365482687950134 + 0.1 * 6.578040599822998
Epoch 620, val loss: 1.0576460361480713
Epoch 630, training loss: 0.6812770366668701 = 0.022160837426781654 + 0.1 * 6.591162204742432
Epoch 630, val loss: 1.0658478736877441
Epoch 640, training loss: 0.6787540912628174 = 0.021050911396741867 + 0.1 * 6.57703161239624
Epoch 640, val loss: 1.0734918117523193
Epoch 650, training loss: 0.6758753061294556 = 0.02002488449215889 + 0.1 * 6.558504104614258
Epoch 650, val loss: 1.0811954736709595
Epoch 660, training loss: 0.6761964559555054 = 0.019074443727731705 + 0.1 * 6.5712199211120605
Epoch 660, val loss: 1.0887483358383179
Epoch 670, training loss: 0.6734585165977478 = 0.01819394715130329 + 0.1 * 6.552645683288574
Epoch 670, val loss: 1.096011996269226
Epoch 680, training loss: 0.6751893162727356 = 0.01737622544169426 + 0.1 * 6.578130722045898
Epoch 680, val loss: 1.1032254695892334
Epoch 690, training loss: 0.6718877553939819 = 0.016616761684417725 + 0.1 * 6.552709579467773
Epoch 690, val loss: 1.1103678941726685
Epoch 700, training loss: 0.6704555749893188 = 0.01591029204428196 + 0.1 * 6.545453071594238
Epoch 700, val loss: 1.1170241832733154
Epoch 710, training loss: 0.6687676906585693 = 0.015248894691467285 + 0.1 * 6.535187721252441
Epoch 710, val loss: 1.1237996816635132
Epoch 720, training loss: 0.6687034964561462 = 0.014630306512117386 + 0.1 * 6.540731430053711
Epoch 720, val loss: 1.1304768323898315
Epoch 730, training loss: 0.6666324138641357 = 0.014050783589482307 + 0.1 * 6.525815963745117
Epoch 730, val loss: 1.1368985176086426
Epoch 740, training loss: 0.6674229502677917 = 0.013507465831935406 + 0.1 * 6.539154529571533
Epoch 740, val loss: 1.1431688070297241
Epoch 750, training loss: 0.6664047837257385 = 0.012996521778404713 + 0.1 * 6.534082889556885
Epoch 750, val loss: 1.1495126485824585
Epoch 760, training loss: 0.6655009984970093 = 0.012517422437667847 + 0.1 * 6.529836177825928
Epoch 760, val loss: 1.155656337738037
Epoch 770, training loss: 0.6645604372024536 = 0.012066648341715336 + 0.1 * 6.524937629699707
Epoch 770, val loss: 1.1615818738937378
Epoch 780, training loss: 0.6644940972328186 = 0.01164403185248375 + 0.1 * 6.528500556945801
Epoch 780, val loss: 1.1673558950424194
Epoch 790, training loss: 0.6617431640625 = 0.011244067922234535 + 0.1 * 6.504991054534912
Epoch 790, val loss: 1.1730079650878906
Epoch 800, training loss: 0.662742018699646 = 0.010865685530006886 + 0.1 * 6.518763065338135
Epoch 800, val loss: 1.1786772012710571
Epoch 810, training loss: 0.6609218120574951 = 0.010507089085876942 + 0.1 * 6.504147052764893
Epoch 810, val loss: 1.184360384941101
Epoch 820, training loss: 0.6607857346534729 = 0.010168216191232204 + 0.1 * 6.5061750411987305
Epoch 820, val loss: 1.1897473335266113
Epoch 830, training loss: 0.660902202129364 = 0.009846201166510582 + 0.1 * 6.510559558868408
Epoch 830, val loss: 1.1950401067733765
Epoch 840, training loss: 0.661419689655304 = 0.009541269391775131 + 0.1 * 6.518784046173096
Epoch 840, val loss: 1.2004467248916626
Epoch 850, training loss: 0.6588089466094971 = 0.009252026677131653 + 0.1 * 6.495569229125977
Epoch 850, val loss: 1.2054836750030518
Epoch 860, training loss: 0.6588125824928284 = 0.008977196179330349 + 0.1 * 6.498353958129883
Epoch 860, val loss: 1.2105497121810913
Epoch 870, training loss: 0.658527135848999 = 0.008714349940419197 + 0.1 * 6.498127460479736
Epoch 870, val loss: 1.2156224250793457
Epoch 880, training loss: 0.6571017503738403 = 0.008465028367936611 + 0.1 * 6.4863667488098145
Epoch 880, val loss: 1.220576286315918
Epoch 890, training loss: 0.6574094891548157 = 0.008227980695664883 + 0.1 * 6.491814613342285
Epoch 890, val loss: 1.2252819538116455
Epoch 900, training loss: 0.6569157838821411 = 0.008000991307199001 + 0.1 * 6.489148139953613
Epoch 900, val loss: 1.230124592781067
Epoch 910, training loss: 0.6557509303092957 = 0.0077846460044384 + 0.1 * 6.4796624183654785
Epoch 910, val loss: 1.2347606420516968
Epoch 920, training loss: 0.6552883982658386 = 0.007577638607472181 + 0.1 * 6.477107524871826
Epoch 920, val loss: 1.2394400835037231
Epoch 930, training loss: 0.656264066696167 = 0.007380201481282711 + 0.1 * 6.4888386726379395
Epoch 930, val loss: 1.2438963651657104
Epoch 940, training loss: 0.6543554663658142 = 0.007190985605120659 + 0.1 * 6.471644401550293
Epoch 940, val loss: 1.2484338283538818
Epoch 950, training loss: 0.6552335619926453 = 0.007010579574853182 + 0.1 * 6.482229709625244
Epoch 950, val loss: 1.2527389526367188
Epoch 960, training loss: 0.6541649699211121 = 0.006837240420281887 + 0.1 * 6.4732770919799805
Epoch 960, val loss: 1.257083535194397
Epoch 970, training loss: 0.6536093354225159 = 0.006670624483376741 + 0.1 * 6.469386577606201
Epoch 970, val loss: 1.261412501335144
Epoch 980, training loss: 0.6544731259346008 = 0.006511358078569174 + 0.1 * 6.479617595672607
Epoch 980, val loss: 1.2655996084213257
Epoch 990, training loss: 0.6525324583053589 = 0.006358446553349495 + 0.1 * 6.461740016937256
Epoch 990, val loss: 1.269615888595581
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 2.7956292629241943 = 1.9359462261199951 + 0.1 * 8.596829414367676
Epoch 0, val loss: 1.934728980064392
Epoch 10, training loss: 2.786334753036499 = 1.9266612529754639 + 0.1 * 8.596734046936035
Epoch 10, val loss: 1.925816297531128
Epoch 20, training loss: 2.7746362686157227 = 1.9150214195251465 + 0.1 * 8.596148490905762
Epoch 20, val loss: 1.9142223596572876
Epoch 30, training loss: 2.75765323638916 = 1.8985153436660767 + 0.1 * 8.591378211975098
Epoch 30, val loss: 1.8972760438919067
Epoch 40, training loss: 2.729879140853882 = 1.8740158081054688 + 0.1 * 8.558632850646973
Epoch 40, val loss: 1.8721270561218262
Epoch 50, training loss: 2.6785390377044678 = 1.8408147096633911 + 0.1 * 8.377243995666504
Epoch 50, val loss: 1.8395271301269531
Epoch 60, training loss: 2.621109962463379 = 1.8054591417312622 + 0.1 * 8.156508445739746
Epoch 60, val loss: 1.8076680898666382
Epoch 70, training loss: 2.5597915649414062 = 1.772979736328125 + 0.1 * 7.868119239807129
Epoch 70, val loss: 1.7794244289398193
Epoch 80, training loss: 2.484090566635132 = 1.7372369766235352 + 0.1 * 7.468536376953125
Epoch 80, val loss: 1.7482157945632935
Epoch 90, training loss: 2.4128711223602295 = 1.692512035369873 + 0.1 * 7.2035908699035645
Epoch 90, val loss: 1.7111518383026123
Epoch 100, training loss: 2.344318151473999 = 1.631652593612671 + 0.1 * 7.126655101776123
Epoch 100, val loss: 1.6595513820648193
Epoch 110, training loss: 2.2634644508361816 = 1.5555251836776733 + 0.1 * 7.0793914794921875
Epoch 110, val loss: 1.595083236694336
Epoch 120, training loss: 2.176255226135254 = 1.471254587173462 + 0.1 * 7.05000638961792
Epoch 120, val loss: 1.5276544094085693
Epoch 130, training loss: 2.085801362991333 = 1.3843414783477783 + 0.1 * 7.014598369598389
Epoch 130, val loss: 1.4581730365753174
Epoch 140, training loss: 1.9932703971862793 = 1.296164870262146 + 0.1 * 6.971055507659912
Epoch 140, val loss: 1.3892871141433716
Epoch 150, training loss: 1.899850606918335 = 1.206647515296936 + 0.1 * 6.932031631469727
Epoch 150, val loss: 1.3192373514175415
Epoch 160, training loss: 1.8109052181243896 = 1.121101975440979 + 0.1 * 6.898032188415527
Epoch 160, val loss: 1.2540256977081299
Epoch 170, training loss: 1.7279947996139526 = 1.0398753881454468 + 0.1 * 6.881194114685059
Epoch 170, val loss: 1.1931474208831787
Epoch 180, training loss: 1.6486544609069824 = 0.9620874524116516 + 0.1 * 6.8656697273254395
Epoch 180, val loss: 1.1371139287948608
Epoch 190, training loss: 1.5740578174591064 = 0.8884301781654358 + 0.1 * 6.856276512145996
Epoch 190, val loss: 1.085943579673767
Epoch 200, training loss: 1.503739833831787 = 0.8194223046302795 + 0.1 * 6.843174457550049
Epoch 200, val loss: 1.0395233631134033
Epoch 210, training loss: 1.4379692077636719 = 0.7545395493507385 + 0.1 * 6.834296703338623
Epoch 210, val loss: 0.9968506693840027
Epoch 220, training loss: 1.376977562904358 = 0.6943375468254089 + 0.1 * 6.826399803161621
Epoch 220, val loss: 0.9583717584609985
Epoch 230, training loss: 1.3206921815872192 = 0.6390735507011414 + 0.1 * 6.81618595123291
Epoch 230, val loss: 0.9243245124816895
Epoch 240, training loss: 1.268767237663269 = 0.5877059102058411 + 0.1 * 6.81061315536499
Epoch 240, val loss: 0.894549548625946
Epoch 250, training loss: 1.219957709312439 = 0.5401512980461121 + 0.1 * 6.798064231872559
Epoch 250, val loss: 0.8692859411239624
Epoch 260, training loss: 1.1741851568222046 = 0.4956434369087219 + 0.1 * 6.785417079925537
Epoch 260, val loss: 0.8488409519195557
Epoch 270, training loss: 1.1322475671768188 = 0.45413658022880554 + 0.1 * 6.781109809875488
Epoch 270, val loss: 0.833287239074707
Epoch 280, training loss: 1.092490553855896 = 0.4157395660877228 + 0.1 * 6.767509460449219
Epoch 280, val loss: 0.8220731616020203
Epoch 290, training loss: 1.0566846132278442 = 0.3796055018901825 + 0.1 * 6.7707905769348145
Epoch 290, val loss: 0.8147328495979309
Epoch 300, training loss: 1.0205144882202148 = 0.34568655490875244 + 0.1 * 6.748279571533203
Epoch 300, val loss: 0.810352087020874
Epoch 310, training loss: 0.9879441261291504 = 0.3136630058288574 + 0.1 * 6.74281120300293
Epoch 310, val loss: 0.8087390661239624
Epoch 320, training loss: 0.9565820693969727 = 0.28362545371055603 + 0.1 * 6.729565620422363
Epoch 320, val loss: 0.8093627095222473
Epoch 330, training loss: 0.9283761978149414 = 0.25570449233055115 + 0.1 * 6.726717472076416
Epoch 330, val loss: 0.8122851848602295
Epoch 340, training loss: 0.9022961258888245 = 0.23015256226062775 + 0.1 * 6.721435546875
Epoch 340, val loss: 0.8171245455741882
Epoch 350, training loss: 0.8819891214370728 = 0.20707359910011292 + 0.1 * 6.749155521392822
Epoch 350, val loss: 0.8241321444511414
Epoch 360, training loss: 0.8583488464355469 = 0.18665534257888794 + 0.1 * 6.716934680938721
Epoch 360, val loss: 0.8324969410896301
Epoch 370, training loss: 0.837714672088623 = 0.16858230531215668 + 0.1 * 6.691323280334473
Epoch 370, val loss: 0.8428381681442261
Epoch 380, training loss: 0.8206968307495117 = 0.15254142880439758 + 0.1 * 6.681553363800049
Epoch 380, val loss: 0.8543956279754639
Epoch 390, training loss: 0.8058269023895264 = 0.13828234374523163 + 0.1 * 6.675445079803467
Epoch 390, val loss: 0.8671308159828186
Epoch 400, training loss: 0.795256495475769 = 0.12567175924777985 + 0.1 * 6.695847511291504
Epoch 400, val loss: 0.8806694746017456
Epoch 410, training loss: 0.7807595729827881 = 0.11454436928033829 + 0.1 * 6.66215181350708
Epoch 410, val loss: 0.8947035074234009
Epoch 420, training loss: 0.7720088958740234 = 0.10462958365678787 + 0.1 * 6.673793315887451
Epoch 420, val loss: 0.9091091156005859
Epoch 430, training loss: 0.7604814171791077 = 0.09579405933618546 + 0.1 * 6.646873474121094
Epoch 430, val loss: 0.9237663149833679
Epoch 440, training loss: 0.7532454133033752 = 0.08788580447435379 + 0.1 * 6.653595924377441
Epoch 440, val loss: 0.9387773275375366
Epoch 450, training loss: 0.7449179291725159 = 0.08080222457647324 + 0.1 * 6.641157150268555
Epoch 450, val loss: 0.9535915851593018
Epoch 460, training loss: 0.7379833459854126 = 0.07443415373563766 + 0.1 * 6.635491847991943
Epoch 460, val loss: 0.9685564041137695
Epoch 470, training loss: 0.7326374650001526 = 0.06870407611131668 + 0.1 * 6.639333724975586
Epoch 470, val loss: 0.983372688293457
Epoch 480, training loss: 0.7263742089271545 = 0.06355124711990356 + 0.1 * 6.62822961807251
Epoch 480, val loss: 0.9979405999183655
Epoch 490, training loss: 0.7212448716163635 = 0.058901023119688034 + 0.1 * 6.623438358306885
Epoch 490, val loss: 1.0124768018722534
Epoch 500, training loss: 0.716461718082428 = 0.05470125749707222 + 0.1 * 6.6176042556762695
Epoch 500, val loss: 1.0265769958496094
Epoch 510, training loss: 0.711365282535553 = 0.05089602991938591 + 0.1 * 6.604692459106445
Epoch 510, val loss: 1.0406123399734497
Epoch 520, training loss: 0.7080137729644775 = 0.047446418553590775 + 0.1 * 6.605673313140869
Epoch 520, val loss: 1.0543471574783325
Epoch 530, training loss: 0.7033160328865051 = 0.04432280361652374 + 0.1 * 6.589932441711426
Epoch 530, val loss: 1.0677084922790527
Epoch 540, training loss: 0.7004660367965698 = 0.041475169360637665 + 0.1 * 6.589908599853516
Epoch 540, val loss: 1.0809199810028076
Epoch 550, training loss: 0.6968454718589783 = 0.03887559473514557 + 0.1 * 6.57969856262207
Epoch 550, val loss: 1.0939645767211914
Epoch 560, training loss: 0.6960394978523254 = 0.03650366887450218 + 0.1 * 6.595358371734619
Epoch 560, val loss: 1.1067044734954834
Epoch 570, training loss: 0.694036066532135 = 0.03433406352996826 + 0.1 * 6.597020149230957
Epoch 570, val loss: 1.1190708875656128
Epoch 580, training loss: 0.6901335120201111 = 0.03235066682100296 + 0.1 * 6.577828407287598
Epoch 580, val loss: 1.1312180757522583
Epoch 590, training loss: 0.6874595880508423 = 0.030536379665136337 + 0.1 * 6.569231986999512
Epoch 590, val loss: 1.14295494556427
Epoch 600, training loss: 0.6853646636009216 = 0.02886456437408924 + 0.1 * 6.565001010894775
Epoch 600, val loss: 1.1545660495758057
Epoch 610, training loss: 0.6838124990463257 = 0.027323665097355843 + 0.1 * 6.5648884773254395
Epoch 610, val loss: 1.1659504175186157
Epoch 620, training loss: 0.6811626553535461 = 0.025902818888425827 + 0.1 * 6.552598476409912
Epoch 620, val loss: 1.177053451538086
Epoch 630, training loss: 0.6793521046638489 = 0.02459084428846836 + 0.1 * 6.54761266708374
Epoch 630, val loss: 1.18792724609375
Epoch 640, training loss: 0.678748369216919 = 0.02337528020143509 + 0.1 * 6.5537309646606445
Epoch 640, val loss: 1.1986260414123535
Epoch 650, training loss: 0.6775073409080505 = 0.02225077897310257 + 0.1 * 6.552565574645996
Epoch 650, val loss: 1.2089478969573975
Epoch 660, training loss: 0.6758854389190674 = 0.021204814314842224 + 0.1 * 6.5468058586120605
Epoch 660, val loss: 1.2192054986953735
Epoch 670, training loss: 0.6737494468688965 = 0.020234422758221626 + 0.1 * 6.535150527954102
Epoch 670, val loss: 1.2291181087493896
Epoch 680, training loss: 0.6735013127326965 = 0.019331587478518486 + 0.1 * 6.541697025299072
Epoch 680, val loss: 1.2388360500335693
Epoch 690, training loss: 0.6710809469223022 = 0.018488135188817978 + 0.1 * 6.525928497314453
Epoch 690, val loss: 1.2484195232391357
Epoch 700, training loss: 0.6726932525634766 = 0.01770150475203991 + 0.1 * 6.549917221069336
Epoch 700, val loss: 1.2577024698257446
Epoch 710, training loss: 0.6692750453948975 = 0.01696663163602352 + 0.1 * 6.5230841636657715
Epoch 710, val loss: 1.266733169555664
Epoch 720, training loss: 0.6688794493675232 = 0.016279146075248718 + 0.1 * 6.526002883911133
Epoch 720, val loss: 1.2756539583206177
Epoch 730, training loss: 0.6677250266075134 = 0.01563369110226631 + 0.1 * 6.520913124084473
Epoch 730, val loss: 1.2843652963638306
Epoch 740, training loss: 0.6668480634689331 = 0.015025782398879528 + 0.1 * 6.518222808837891
Epoch 740, val loss: 1.2930185794830322
Epoch 750, training loss: 0.6661450862884521 = 0.014457344077527523 + 0.1 * 6.516877174377441
Epoch 750, val loss: 1.3012793064117432
Epoch 760, training loss: 0.6641918420791626 = 0.013920060358941555 + 0.1 * 6.5027174949646
Epoch 760, val loss: 1.309540867805481
Epoch 770, training loss: 0.664419949054718 = 0.013415210880339146 + 0.1 * 6.510047435760498
Epoch 770, val loss: 1.3175240755081177
Epoch 780, training loss: 0.6630615592002869 = 0.012938425876200199 + 0.1 * 6.5012311935424805
Epoch 780, val loss: 1.325453519821167
Epoch 790, training loss: 0.665052056312561 = 0.012488333508372307 + 0.1 * 6.525637149810791
Epoch 790, val loss: 1.333250880241394
Epoch 800, training loss: 0.663226842880249 = 0.012063630856573582 + 0.1 * 6.511631965637207
Epoch 800, val loss: 1.3407201766967773
Epoch 810, training loss: 0.6609424948692322 = 0.011661083437502384 + 0.1 * 6.492813587188721
Epoch 810, val loss: 1.3480889797210693
Epoch 820, training loss: 0.6615251898765564 = 0.011280056089162827 + 0.1 * 6.502451419830322
Epoch 820, val loss: 1.3554530143737793
Epoch 830, training loss: 0.6608676314353943 = 0.010919827967882156 + 0.1 * 6.499478340148926
Epoch 830, val loss: 1.3625622987747192
Epoch 840, training loss: 0.6588144898414612 = 0.010576387867331505 + 0.1 * 6.4823808670043945
Epoch 840, val loss: 1.3695766925811768
Epoch 850, training loss: 0.6596170663833618 = 0.010249549522995949 + 0.1 * 6.493675231933594
Epoch 850, val loss: 1.3766162395477295
Epoch 860, training loss: 0.6592019200325012 = 0.009941359050571918 + 0.1 * 6.492605686187744
Epoch 860, val loss: 1.3831937313079834
Epoch 870, training loss: 0.6582422256469727 = 0.009647136554121971 + 0.1 * 6.485950946807861
Epoch 870, val loss: 1.3897974491119385
Epoch 880, training loss: 0.6593418717384338 = 0.009367081336677074 + 0.1 * 6.499747276306152
Epoch 880, val loss: 1.3963581323623657
Epoch 890, training loss: 0.6574556827545166 = 0.00909977126866579 + 0.1 * 6.483558654785156
Epoch 890, val loss: 1.402672529220581
Epoch 900, training loss: 0.6566189527511597 = 0.008846540004014969 + 0.1 * 6.477724075317383
Epoch 900, val loss: 1.4088234901428223
Epoch 910, training loss: 0.656907856464386 = 0.008602865971624851 + 0.1 * 6.4830498695373535
Epoch 910, val loss: 1.4150550365447998
Epoch 920, training loss: 0.6557544469833374 = 0.008372028358280659 + 0.1 * 6.473824501037598
Epoch 920, val loss: 1.4210036993026733
Epoch 930, training loss: 0.655133843421936 = 0.008149740286171436 + 0.1 * 6.469841003417969
Epoch 930, val loss: 1.4269518852233887
Epoch 940, training loss: 0.6549704074859619 = 0.007937882095575333 + 0.1 * 6.470324993133545
Epoch 940, val loss: 1.4327605962753296
Epoch 950, training loss: 0.6533976793289185 = 0.007734089158475399 + 0.1 * 6.45663595199585
Epoch 950, val loss: 1.4385173320770264
Epoch 960, training loss: 0.6537783145904541 = 0.007540157996118069 + 0.1 * 6.462381362915039
Epoch 960, val loss: 1.4440244436264038
Epoch 970, training loss: 0.6540800333023071 = 0.007353977300226688 + 0.1 * 6.467260360717773
Epoch 970, val loss: 1.4495375156402588
Epoch 980, training loss: 0.653201699256897 = 0.007174819707870483 + 0.1 * 6.460268974304199
Epoch 980, val loss: 1.4550164937973022
Epoch 990, training loss: 0.6525219082832336 = 0.007003442849963903 + 0.1 * 6.4551849365234375
Epoch 990, val loss: 1.4603153467178345
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8033737480231946
The final CL Acc:0.76296, 0.01318, The final GNN Acc:0.80900, 0.00407
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13176])
remove edge: torch.Size([2, 7864])
updated graph: torch.Size([2, 10484])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.796190023422241 = 1.9365085363388062 + 0.1 * 8.59681510925293
Epoch 0, val loss: 1.9280223846435547
Epoch 10, training loss: 2.786210060119629 = 1.9265391826629639 + 0.1 * 8.596707344055176
Epoch 10, val loss: 1.9181760549545288
Epoch 20, training loss: 2.774044990539551 = 1.9144428968429565 + 0.1 * 8.596020698547363
Epoch 20, val loss: 1.9056628942489624
Epoch 30, training loss: 2.7567572593688965 = 1.897693395614624 + 0.1 * 8.59063720703125
Epoch 30, val loss: 1.887847661972046
Epoch 40, training loss: 2.7292869091033936 = 1.8735301494598389 + 0.1 * 8.55756664276123
Epoch 40, val loss: 1.8619914054870605
Epoch 50, training loss: 2.6817588806152344 = 1.8412134647369385 + 0.1 * 8.405454635620117
Epoch 50, val loss: 1.8291453123092651
Epoch 60, training loss: 2.6209497451782227 = 1.8051291704177856 + 0.1 * 8.158206939697266
Epoch 60, val loss: 1.795530080795288
Epoch 70, training loss: 2.553205728530884 = 1.7696174383163452 + 0.1 * 7.835882663726807
Epoch 70, val loss: 1.7653757333755493
Epoch 80, training loss: 2.4714343547821045 = 1.7310891151428223 + 0.1 * 7.403451442718506
Epoch 80, val loss: 1.733890414237976
Epoch 90, training loss: 2.4020838737487793 = 1.683115005493164 + 0.1 * 7.189688205718994
Epoch 90, val loss: 1.6927827596664429
Epoch 100, training loss: 2.329155445098877 = 1.6189261674880981 + 0.1 * 7.102292060852051
Epoch 100, val loss: 1.6362191438674927
Epoch 110, training loss: 2.243546485900879 = 1.5378103256225586 + 0.1 * 7.0573601722717285
Epoch 110, val loss: 1.5667459964752197
Epoch 120, training loss: 2.1461703777313232 = 1.443127989768982 + 0.1 * 7.030423164367676
Epoch 120, val loss: 1.4876794815063477
Epoch 130, training loss: 2.041269302368164 = 1.3407700061798096 + 0.1 * 7.004992485046387
Epoch 130, val loss: 1.4039967060089111
Epoch 140, training loss: 1.9317177534103394 = 1.233458399772644 + 0.1 * 6.982593536376953
Epoch 140, val loss: 1.3190969228744507
Epoch 150, training loss: 1.8206231594085693 = 1.1242669820785522 + 0.1 * 6.963562488555908
Epoch 150, val loss: 1.233506202697754
Epoch 160, training loss: 1.713835597038269 = 1.0183452367782593 + 0.1 * 6.954903602600098
Epoch 160, val loss: 1.152047038078308
Epoch 170, training loss: 1.6161799430847168 = 0.9221181273460388 + 0.1 * 6.940617561340332
Epoch 170, val loss: 1.0795036554336548
Epoch 180, training loss: 1.5298993587493896 = 0.8368411660194397 + 0.1 * 6.930581569671631
Epoch 180, val loss: 1.0165590047836304
Epoch 190, training loss: 1.4548444747924805 = 0.7628936767578125 + 0.1 * 6.9195075035095215
Epoch 190, val loss: 0.9629855751991272
Epoch 200, training loss: 1.3896698951721191 = 0.6993809342384338 + 0.1 * 6.902890205383301
Epoch 200, val loss: 0.9183323383331299
Epoch 210, training loss: 1.332981824874878 = 0.6435449719429016 + 0.1 * 6.894368648529053
Epoch 210, val loss: 0.8803226947784424
Epoch 220, training loss: 1.2809278964996338 = 0.5932083129882812 + 0.1 * 6.877195358276367
Epoch 220, val loss: 0.8480393290519714
Epoch 230, training loss: 1.2326409816741943 = 0.5461085438728333 + 0.1 * 6.865324974060059
Epoch 230, val loss: 0.8202689290046692
Epoch 240, training loss: 1.1868174076080322 = 0.5015060901641846 + 0.1 * 6.853113174438477
Epoch 240, val loss: 0.7969990968704224
Epoch 250, training loss: 1.1435747146606445 = 0.4593426287174225 + 0.1 * 6.842320442199707
Epoch 250, val loss: 0.7780683040618896
Epoch 260, training loss: 1.1038169860839844 = 0.4197099208831787 + 0.1 * 6.841071128845215
Epoch 260, val loss: 0.763098418712616
Epoch 270, training loss: 1.0653287172317505 = 0.3825903534889221 + 0.1 * 6.827383518218994
Epoch 270, val loss: 0.7513752579689026
Epoch 280, training loss: 1.0297753810882568 = 0.34744298458099365 + 0.1 * 6.823324203491211
Epoch 280, val loss: 0.7423403263092041
Epoch 290, training loss: 0.995660662651062 = 0.31424185633659363 + 0.1 * 6.814187526702881
Epoch 290, val loss: 0.7357217073440552
Epoch 300, training loss: 0.9635716080665588 = 0.28308045864105225 + 0.1 * 6.804911136627197
Epoch 300, val loss: 0.7313892245292664
Epoch 310, training loss: 0.9341161251068115 = 0.2544018030166626 + 0.1 * 6.79714298248291
Epoch 310, val loss: 0.7293180823326111
Epoch 320, training loss: 0.9073061943054199 = 0.2285653054714203 + 0.1 * 6.787408351898193
Epoch 320, val loss: 0.7296295166015625
Epoch 330, training loss: 0.8836746215820312 = 0.20559599995613098 + 0.1 * 6.78078556060791
Epoch 330, val loss: 0.732252836227417
Epoch 340, training loss: 0.8625844717025757 = 0.18526050448417664 + 0.1 * 6.773239612579346
Epoch 340, val loss: 0.7371113300323486
Epoch 350, training loss: 0.8436768651008606 = 0.16734202206134796 + 0.1 * 6.763348579406738
Epoch 350, val loss: 0.7440028190612793
Epoch 360, training loss: 0.8271551132202148 = 0.15157464146614075 + 0.1 * 6.755804538726807
Epoch 360, val loss: 0.7524436116218567
Epoch 370, training loss: 0.813330352306366 = 0.13770002126693726 + 0.1 * 6.756303310394287
Epoch 370, val loss: 0.7622637152671814
Epoch 380, training loss: 0.7997206449508667 = 0.12544861435890198 + 0.1 * 6.742720127105713
Epoch 380, val loss: 0.7731199264526367
Epoch 390, training loss: 0.7875422835350037 = 0.11455166339874268 + 0.1 * 6.72990608215332
Epoch 390, val loss: 0.7847273945808411
Epoch 400, training loss: 0.7791047096252441 = 0.10483560711145401 + 0.1 * 6.742690563201904
Epoch 400, val loss: 0.7969986796379089
Epoch 410, training loss: 0.7676684856414795 = 0.09619739651679993 + 0.1 * 6.714710712432861
Epoch 410, val loss: 0.8094831109046936
Epoch 420, training loss: 0.759394645690918 = 0.0884368047118187 + 0.1 * 6.709578514099121
Epoch 420, val loss: 0.8222746849060059
Epoch 430, training loss: 0.751602292060852 = 0.08142201602458954 + 0.1 * 6.701802730560303
Epoch 430, val loss: 0.8353893160820007
Epoch 440, training loss: 0.7458921670913696 = 0.07510332763195038 + 0.1 * 6.707888126373291
Epoch 440, val loss: 0.8484846353530884
Epoch 450, training loss: 0.7386184930801392 = 0.06942310184240341 + 0.1 * 6.691954135894775
Epoch 450, val loss: 0.8615115284919739
Epoch 460, training loss: 0.7329362034797668 = 0.06426902115345001 + 0.1 * 6.686671733856201
Epoch 460, val loss: 0.8745692372322083
Epoch 470, training loss: 0.728469729423523 = 0.05957666039466858 + 0.1 * 6.688930988311768
Epoch 470, val loss: 0.8876967430114746
Epoch 480, training loss: 0.7228527665138245 = 0.05532892793416977 + 0.1 * 6.675238609313965
Epoch 480, val loss: 0.9005045890808105
Epoch 490, training loss: 0.7183215618133545 = 0.05146944150328636 + 0.1 * 6.668520927429199
Epoch 490, val loss: 0.9132373332977295
Epoch 500, training loss: 0.714958906173706 = 0.047944631427526474 + 0.1 * 6.670143127441406
Epoch 500, val loss: 0.9258919358253479
Epoch 510, training loss: 0.7105752825737 = 0.044736601412296295 + 0.1 * 6.65838623046875
Epoch 510, val loss: 0.9382879137992859
Epoch 520, training loss: 0.7078819274902344 = 0.041807401925325394 + 0.1 * 6.660745143890381
Epoch 520, val loss: 0.9505904316902161
Epoch 530, training loss: 0.704161524772644 = 0.03913396969437599 + 0.1 * 6.650275230407715
Epoch 530, val loss: 0.9625822901725769
Epoch 540, training loss: 0.7017184495925903 = 0.0366855226457119 + 0.1 * 6.650329113006592
Epoch 540, val loss: 0.9744554162025452
Epoch 550, training loss: 0.699134886264801 = 0.03444661945104599 + 0.1 * 6.6468825340271
Epoch 550, val loss: 0.9860602617263794
Epoch 560, training loss: 0.69541996717453 = 0.03240083530545235 + 0.1 * 6.630190849304199
Epoch 560, val loss: 0.9973729848861694
Epoch 570, training loss: 0.6932048797607422 = 0.03052595444023609 + 0.1 * 6.62678861618042
Epoch 570, val loss: 1.0084105730056763
Epoch 580, training loss: 0.6907925605773926 = 0.02879888005554676 + 0.1 * 6.619936943054199
Epoch 580, val loss: 1.0193060636520386
Epoch 590, training loss: 0.6908590197563171 = 0.027203533798456192 + 0.1 * 6.636554718017578
Epoch 590, val loss: 1.0300854444503784
Epoch 600, training loss: 0.6871825456619263 = 0.02573583275079727 + 0.1 * 6.614466667175293
Epoch 600, val loss: 1.0405278205871582
Epoch 610, training loss: 0.6846128702163696 = 0.024380728602409363 + 0.1 * 6.602321147918701
Epoch 610, val loss: 1.0507891178131104
Epoch 620, training loss: 0.6858179569244385 = 0.023127766326069832 + 0.1 * 6.626901626586914
Epoch 620, val loss: 1.0608618259429932
Epoch 630, training loss: 0.6814143061637878 = 0.021970326080918312 + 0.1 * 6.594439506530762
Epoch 630, val loss: 1.0706191062927246
Epoch 640, training loss: 0.6802632212638855 = 0.020895084366202354 + 0.1 * 6.593681335449219
Epoch 640, val loss: 1.0802319049835205
Epoch 650, training loss: 0.6792122721672058 = 0.01989828608930111 + 0.1 * 6.5931396484375
Epoch 650, val loss: 1.0895789861679077
Epoch 660, training loss: 0.677690327167511 = 0.018972527235746384 + 0.1 * 6.587177753448486
Epoch 660, val loss: 1.0987036228179932
Epoch 670, training loss: 0.6764442324638367 = 0.018109813332557678 + 0.1 * 6.583343982696533
Epoch 670, val loss: 1.1077163219451904
Epoch 680, training loss: 0.6756123900413513 = 0.017304802313447 + 0.1 * 6.583076000213623
Epoch 680, val loss: 1.11649751663208
Epoch 690, training loss: 0.6741716265678406 = 0.016553567722439766 + 0.1 * 6.576180458068848
Epoch 690, val loss: 1.125087022781372
Epoch 700, training loss: 0.6748078465461731 = 0.01585259661078453 + 0.1 * 6.589552879333496
Epoch 700, val loss: 1.1334936618804932
Epoch 710, training loss: 0.6717982888221741 = 0.015197343192994595 + 0.1 * 6.566009521484375
Epoch 710, val loss: 1.1416736841201782
Epoch 720, training loss: 0.6707612872123718 = 0.014582871459424496 + 0.1 * 6.561784267425537
Epoch 720, val loss: 1.149752140045166
Epoch 730, training loss: 0.6715530157089233 = 0.014005916193127632 + 0.1 * 6.575470924377441
Epoch 730, val loss: 1.1576439142227173
Epoch 740, training loss: 0.6691809296607971 = 0.013465733267366886 + 0.1 * 6.5571513175964355
Epoch 740, val loss: 1.165339708328247
Epoch 750, training loss: 0.669766902923584 = 0.012958580628037453 + 0.1 * 6.568082809448242
Epoch 750, val loss: 1.172897458076477
Epoch 760, training loss: 0.666615903377533 = 0.012480415403842926 + 0.1 * 6.541354656219482
Epoch 760, val loss: 1.180230736732483
Epoch 770, training loss: 0.6662615537643433 = 0.012029875069856644 + 0.1 * 6.542316436767578
Epoch 770, val loss: 1.187488317489624
Epoch 780, training loss: 0.6685943603515625 = 0.01160496100783348 + 0.1 * 6.569893836975098
Epoch 780, val loss: 1.19459867477417
Epoch 790, training loss: 0.665219783782959 = 0.011203764006495476 + 0.1 * 6.540160179138184
Epoch 790, val loss: 1.2015070915222168
Epoch 800, training loss: 0.6659687161445618 = 0.010825038887560368 + 0.1 * 6.551436901092529
Epoch 800, val loss: 1.2083518505096436
Epoch 810, training loss: 0.6639822721481323 = 0.01046592928469181 + 0.1 * 6.535163402557373
Epoch 810, val loss: 1.2149746417999268
Epoch 820, training loss: 0.662329375743866 = 0.010125893168151379 + 0.1 * 6.522034645080566
Epoch 820, val loss: 1.2215512990951538
Epoch 830, training loss: 0.6636151075363159 = 0.009803473018109798 + 0.1 * 6.538115978240967
Epoch 830, val loss: 1.2279870510101318
Epoch 840, training loss: 0.6613897681236267 = 0.00949686300009489 + 0.1 * 6.5189290046691895
Epoch 840, val loss: 1.234291672706604
Epoch 850, training loss: 0.6625152826309204 = 0.009205923415720463 + 0.1 * 6.533093452453613
Epoch 850, val loss: 1.2404706478118896
Epoch 860, training loss: 0.6630246639251709 = 0.008930202573537827 + 0.1 * 6.540944576263428
Epoch 860, val loss: 1.2464653253555298
Epoch 870, training loss: 0.6613209843635559 = 0.008668231777846813 + 0.1 * 6.526527404785156
Epoch 870, val loss: 1.2523263692855835
Epoch 880, training loss: 0.6590830683708191 = 0.00841930229216814 + 0.1 * 6.506638050079346
Epoch 880, val loss: 1.25809645652771
Epoch 890, training loss: 0.6582867503166199 = 0.008181736804544926 + 0.1 * 6.501049995422363
Epoch 890, val loss: 1.263817548751831
Epoch 900, training loss: 0.657955527305603 = 0.007954771630465984 + 0.1 * 6.500007152557373
Epoch 900, val loss: 1.269463062286377
Epoch 910, training loss: 0.6586620211601257 = 0.0077374521642923355 + 0.1 * 6.5092453956604
Epoch 910, val loss: 1.2750147581100464
Epoch 920, training loss: 0.6579208970069885 = 0.007529824506491423 + 0.1 * 6.503911018371582
Epoch 920, val loss: 1.280403733253479
Epoch 930, training loss: 0.6561466455459595 = 0.007331865839660168 + 0.1 * 6.488147735595703
Epoch 930, val loss: 1.2857402563095093
Epoch 940, training loss: 0.6557450890541077 = 0.007142191752791405 + 0.1 * 6.486028671264648
Epoch 940, val loss: 1.2909889221191406
Epoch 950, training loss: 0.6584246754646301 = 0.006960701663047075 + 0.1 * 6.514639854431152
Epoch 950, val loss: 1.2961617708206177
Epoch 960, training loss: 0.654910683631897 = 0.006786299403756857 + 0.1 * 6.481244087219238
Epoch 960, val loss: 1.3011857271194458
Epoch 970, training loss: 0.6547877788543701 = 0.006619501858949661 + 0.1 * 6.481682300567627
Epoch 970, val loss: 1.3061659336090088
Epoch 980, training loss: 0.6552007794380188 = 0.006459766533225775 + 0.1 * 6.487410068511963
Epoch 980, val loss: 1.3110393285751343
Epoch 990, training loss: 0.6539731621742249 = 0.006306351628154516 + 0.1 * 6.476667881011963
Epoch 990, val loss: 1.3158445358276367
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8471270426989985
=== training gcn model ===
Epoch 0, training loss: 2.8052353858947754 = 1.9455530643463135 + 0.1 * 8.596822738647461
Epoch 0, val loss: 1.9454585313796997
Epoch 10, training loss: 2.7948291301727295 = 1.935159683227539 + 0.1 * 8.596694946289062
Epoch 10, val loss: 1.934611201286316
Epoch 20, training loss: 2.78145694732666 = 1.9218547344207764 + 0.1 * 8.59602165222168
Epoch 20, val loss: 1.9206479787826538
Epoch 30, training loss: 2.761662006378174 = 1.902599573135376 + 0.1 * 8.590622901916504
Epoch 30, val loss: 1.9006766080856323
Epoch 40, training loss: 2.729860305786133 = 1.874128818511963 + 0.1 * 8.5573148727417
Epoch 40, val loss: 1.8717879056930542
Epoch 50, training loss: 2.6740055084228516 = 1.8365874290466309 + 0.1 * 8.374181747436523
Epoch 50, val loss: 1.8359004259109497
Epoch 60, training loss: 2.6127254962921143 = 1.7979319095611572 + 0.1 * 8.147934913635254
Epoch 60, val loss: 1.8019225597381592
Epoch 70, training loss: 2.5423707962036133 = 1.7630534172058105 + 0.1 * 7.793174743652344
Epoch 70, val loss: 1.7708258628845215
Epoch 80, training loss: 2.4662978649139404 = 1.722591519355774 + 0.1 * 7.437062740325928
Epoch 80, val loss: 1.7332992553710938
Epoch 90, training loss: 2.3938984870910645 = 1.670922040939331 + 0.1 * 7.229763031005859
Epoch 90, val loss: 1.6871737241744995
Epoch 100, training loss: 2.3147690296173096 = 1.602333426475525 + 0.1 * 7.124356269836426
Epoch 100, val loss: 1.6277196407318115
Epoch 110, training loss: 2.2244198322296143 = 1.5174601078033447 + 0.1 * 7.069596290588379
Epoch 110, val loss: 1.5537916421890259
Epoch 120, training loss: 2.1260080337524414 = 1.4232556819915771 + 0.1 * 7.027523994445801
Epoch 120, val loss: 1.4735554456710815
Epoch 130, training loss: 2.024568796157837 = 1.3263269662857056 + 0.1 * 6.982417583465576
Epoch 130, val loss: 1.3928313255310059
Epoch 140, training loss: 1.9234156608581543 = 1.2297654151916504 + 0.1 * 6.936502933502197
Epoch 140, val loss: 1.3133776187896729
Epoch 150, training loss: 1.8265209197998047 = 1.1361857652664185 + 0.1 * 6.903351783752441
Epoch 150, val loss: 1.2374639511108398
Epoch 160, training loss: 1.7359840869903564 = 1.047746181488037 + 0.1 * 6.882378101348877
Epoch 160, val loss: 1.1675033569335938
Epoch 170, training loss: 1.6516196727752686 = 0.9639451503753662 + 0.1 * 6.876744747161865
Epoch 170, val loss: 1.1022567749023438
Epoch 180, training loss: 1.5717649459838867 = 0.8864295482635498 + 0.1 * 6.853353977203369
Epoch 180, val loss: 1.042313575744629
Epoch 190, training loss: 1.4977478981018066 = 0.814073383808136 + 0.1 * 6.836745262145996
Epoch 190, val loss: 0.9865595102310181
Epoch 200, training loss: 1.4293572902679443 = 0.7470212578773499 + 0.1 * 6.823360443115234
Epoch 200, val loss: 0.9359195828437805
Epoch 210, training loss: 1.369253158569336 = 0.6866274476051331 + 0.1 * 6.82625675201416
Epoch 210, val loss: 0.8924587965011597
Epoch 220, training loss: 1.3146464824676514 = 0.6341454982757568 + 0.1 * 6.805008888244629
Epoch 220, val loss: 0.8575165271759033
Epoch 230, training loss: 1.2695679664611816 = 0.5886093974113464 + 0.1 * 6.809586048126221
Epoch 230, val loss: 0.8307936191558838
Epoch 240, training loss: 1.229024887084961 = 0.5499610304832458 + 0.1 * 6.7906389236450195
Epoch 240, val loss: 0.8118446469306946
Epoch 250, training loss: 1.1942590475082397 = 0.5166794657707214 + 0.1 * 6.775795936584473
Epoch 250, val loss: 0.7991600036621094
Epoch 260, training loss: 1.1636885404586792 = 0.4872792959213257 + 0.1 * 6.764092445373535
Epoch 260, val loss: 0.7911615967750549
Epoch 270, training loss: 1.1383278369903564 = 0.4605339467525482 + 0.1 * 6.777939319610596
Epoch 270, val loss: 0.7861791253089905
Epoch 280, training loss: 1.1103627681732178 = 0.4355093836784363 + 0.1 * 6.748534202575684
Epoch 280, val loss: 0.7827827334403992
Epoch 290, training loss: 1.0841161012649536 = 0.41088512539863586 + 0.1 * 6.732309341430664
Epoch 290, val loss: 0.7799524664878845
Epoch 300, training loss: 1.0588178634643555 = 0.3860462009906769 + 0.1 * 6.72771692276001
Epoch 300, val loss: 0.7771729826927185
Epoch 310, training loss: 1.0323644876480103 = 0.36076268553733826 + 0.1 * 6.716017723083496
Epoch 310, val loss: 0.7744601368904114
Epoch 320, training loss: 1.008007287979126 = 0.3352358937263489 + 0.1 * 6.7277140617370605
Epoch 320, val loss: 0.7722945809364319
Epoch 330, training loss: 0.9806514978408813 = 0.3103410303592682 + 0.1 * 6.703104019165039
Epoch 330, val loss: 0.771034300327301
Epoch 340, training loss: 0.9557658433914185 = 0.28656262159347534 + 0.1 * 6.692032337188721
Epoch 340, val loss: 0.7711313962936401
Epoch 350, training loss: 0.9339572191238403 = 0.2641335725784302 + 0.1 * 6.698236465454102
Epoch 350, val loss: 0.7725092768669128
Epoch 360, training loss: 0.9109112620353699 = 0.24287916719913483 + 0.1 * 6.680320739746094
Epoch 360, val loss: 0.7750611305236816
Epoch 370, training loss: 0.8904293775558472 = 0.22252026200294495 + 0.1 * 6.679091453552246
Epoch 370, val loss: 0.7786609530448914
Epoch 380, training loss: 0.8694363236427307 = 0.20311449468135834 + 0.1 * 6.663218021392822
Epoch 380, val loss: 0.7832021713256836
Epoch 390, training loss: 0.8500226140022278 = 0.1846887469291687 + 0.1 * 6.653338432312012
Epoch 390, val loss: 0.7888766527175903
Epoch 400, training loss: 0.8362365365028381 = 0.1674591451883316 + 0.1 * 6.687773704528809
Epoch 400, val loss: 0.795608639717102
Epoch 410, training loss: 0.8170549869537354 = 0.15181998908519745 + 0.1 * 6.652349948883057
Epoch 410, val loss: 0.8032816648483276
Epoch 420, training loss: 0.8013877272605896 = 0.13775743544101715 + 0.1 * 6.636302947998047
Epoch 420, val loss: 0.811996579170227
Epoch 430, training loss: 0.7880775928497314 = 0.12521561980247498 + 0.1 * 6.628619194030762
Epoch 430, val loss: 0.8215540647506714
Epoch 440, training loss: 0.7763314247131348 = 0.11410632729530334 + 0.1 * 6.622250556945801
Epoch 440, val loss: 0.8318513035774231
Epoch 450, training loss: 0.767888605594635 = 0.10428564250469208 + 0.1 * 6.636029243469238
Epoch 450, val loss: 0.842741847038269
Epoch 460, training loss: 0.7571583986282349 = 0.09559223055839539 + 0.1 * 6.615661144256592
Epoch 460, val loss: 0.8540496230125427
Epoch 470, training loss: 0.7479654550552368 = 0.0878409668803215 + 0.1 * 6.601244926452637
Epoch 470, val loss: 0.86573326587677
Epoch 480, training loss: 0.740927517414093 = 0.0808943510055542 + 0.1 * 6.6003313064575195
Epoch 480, val loss: 0.8777573704719543
Epoch 490, training loss: 0.7349443435668945 = 0.07468612492084503 + 0.1 * 6.602581977844238
Epoch 490, val loss: 0.8898076415061951
Epoch 500, training loss: 0.727928638458252 = 0.0691157877445221 + 0.1 * 6.588128566741943
Epoch 500, val loss: 0.9019151926040649
Epoch 510, training loss: 0.7280350923538208 = 0.06408454477787018 + 0.1 * 6.639505386352539
Epoch 510, val loss: 0.9140914082527161
Epoch 520, training loss: 0.7175864577293396 = 0.059555068612098694 + 0.1 * 6.580313682556152
Epoch 520, val loss: 0.9260516166687012
Epoch 530, training loss: 0.7134620547294617 = 0.05544734746217728 + 0.1 * 6.5801472663879395
Epoch 530, val loss: 0.9379673600196838
Epoch 540, training loss: 0.7081825733184814 = 0.05171754211187363 + 0.1 * 6.564650535583496
Epoch 540, val loss: 0.9496518969535828
Epoch 550, training loss: 0.7043322920799255 = 0.04831526055932045 + 0.1 * 6.5601701736450195
Epoch 550, val loss: 0.9612459540367126
Epoch 560, training loss: 0.7030701041221619 = 0.04521029815077782 + 0.1 * 6.5785980224609375
Epoch 560, val loss: 0.9726184010505676
Epoch 570, training loss: 0.6973207592964172 = 0.04237804189324379 + 0.1 * 6.549426555633545
Epoch 570, val loss: 0.9837538003921509
Epoch 580, training loss: 0.6950396299362183 = 0.03978193178772926 + 0.1 * 6.552577018737793
Epoch 580, val loss: 0.9947091341018677
Epoch 590, training loss: 0.6923355460166931 = 0.03740241006016731 + 0.1 * 6.549331188201904
Epoch 590, val loss: 1.0054234266281128
Epoch 600, training loss: 0.6894030570983887 = 0.035221584141254425 + 0.1 * 6.541814804077148
Epoch 600, val loss: 1.015879511833191
Epoch 610, training loss: 0.6870314478874207 = 0.03321285918354988 + 0.1 * 6.538186073303223
Epoch 610, val loss: 1.026090145111084
Epoch 620, training loss: 0.6847193241119385 = 0.03136197105050087 + 0.1 * 6.533573150634766
Epoch 620, val loss: 1.0360835790634155
Epoch 630, training loss: 0.6866012811660767 = 0.02965252660214901 + 0.1 * 6.56948709487915
Epoch 630, val loss: 1.0458446741104126
Epoch 640, training loss: 0.6821264028549194 = 0.028081681579351425 + 0.1 * 6.540447235107422
Epoch 640, val loss: 1.055226445198059
Epoch 650, training loss: 0.6789342761039734 = 0.026629753410816193 + 0.1 * 6.523045063018799
Epoch 650, val loss: 1.0644687414169312
Epoch 660, training loss: 0.676949679851532 = 0.025283709168434143 + 0.1 * 6.516659736633301
Epoch 660, val loss: 1.073502540588379
Epoch 670, training loss: 0.6776350140571594 = 0.024032972753047943 + 0.1 * 6.536020278930664
Epoch 670, val loss: 1.0823497772216797
Epoch 680, training loss: 0.674848198890686 = 0.02287197858095169 + 0.1 * 6.51976203918457
Epoch 680, val loss: 1.0909712314605713
Epoch 690, training loss: 0.6739518642425537 = 0.02179303765296936 + 0.1 * 6.521588325500488
Epoch 690, val loss: 1.099388599395752
Epoch 700, training loss: 0.672825813293457 = 0.020787782967090607 + 0.1 * 6.520380020141602
Epoch 700, val loss: 1.1076055765151978
Epoch 710, training loss: 0.6701359152793884 = 0.0198514387011528 + 0.1 * 6.502844333648682
Epoch 710, val loss: 1.1156033277511597
Epoch 720, training loss: 0.6694825887680054 = 0.01897614821791649 + 0.1 * 6.505064487457275
Epoch 720, val loss: 1.1234527826309204
Epoch 730, training loss: 0.668090283870697 = 0.01815808191895485 + 0.1 * 6.499321937561035
Epoch 730, val loss: 1.1311208009719849
Epoch 740, training loss: 0.669868528842926 = 0.01739080622792244 + 0.1 * 6.524777412414551
Epoch 740, val loss: 1.1386358737945557
Epoch 750, training loss: 0.6674829721450806 = 0.01667545735836029 + 0.1 * 6.508074760437012
Epoch 750, val loss: 1.145890712738037
Epoch 760, training loss: 0.6647464036941528 = 0.016004357486963272 + 0.1 * 6.487420082092285
Epoch 760, val loss: 1.1530095338821411
Epoch 770, training loss: 0.6642584204673767 = 0.015375496819615364 + 0.1 * 6.488829135894775
Epoch 770, val loss: 1.1599806547164917
Epoch 780, training loss: 0.662692129611969 = 0.014782410115003586 + 0.1 * 6.47909688949585
Epoch 780, val loss: 1.166852593421936
Epoch 790, training loss: 0.6642381548881531 = 0.014222314581274986 + 0.1 * 6.500158309936523
Epoch 790, val loss: 1.1736223697662354
Epoch 800, training loss: 0.6618922352790833 = 0.013695082627236843 + 0.1 * 6.481971740722656
Epoch 800, val loss: 1.1801447868347168
Epoch 810, training loss: 0.6609567999839783 = 0.013199584558606148 + 0.1 * 6.477571964263916
Epoch 810, val loss: 1.186568021774292
Epoch 820, training loss: 0.6603739857673645 = 0.012731077149510384 + 0.1 * 6.476428985595703
Epoch 820, val loss: 1.1928372383117676
Epoch 830, training loss: 0.6599541902542114 = 0.01229031104594469 + 0.1 * 6.4766387939453125
Epoch 830, val loss: 1.1989328861236572
Epoch 840, training loss: 0.6587719917297363 = 0.011872258968651295 + 0.1 * 6.468997478485107
Epoch 840, val loss: 1.2049578428268433
Epoch 850, training loss: 0.658816933631897 = 0.011475702747702599 + 0.1 * 6.47341251373291
Epoch 850, val loss: 1.2108476161956787
Epoch 860, training loss: 0.6580678820610046 = 0.011099342256784439 + 0.1 * 6.4696855545043945
Epoch 860, val loss: 1.216614007949829
Epoch 870, training loss: 0.6592978239059448 = 0.010742193087935448 + 0.1 * 6.485556125640869
Epoch 870, val loss: 1.2222622632980347
Epoch 880, training loss: 0.6570268273353577 = 0.010404097847640514 + 0.1 * 6.466227054595947
Epoch 880, val loss: 1.2277498245239258
Epoch 890, training loss: 0.6560370922088623 = 0.010082479566335678 + 0.1 * 6.459546089172363
Epoch 890, val loss: 1.2331537008285522
Epoch 900, training loss: 0.6563068628311157 = 0.009776048362255096 + 0.1 * 6.46530818939209
Epoch 900, val loss: 1.2384648323059082
Epoch 910, training loss: 0.6555272340774536 = 0.009484215639531612 + 0.1 * 6.460430145263672
Epoch 910, val loss: 1.2436543703079224
Epoch 920, training loss: 0.653964102268219 = 0.009206511080265045 + 0.1 * 6.447575569152832
Epoch 920, val loss: 1.2487293481826782
Epoch 930, training loss: 0.653919517993927 = 0.008942121639847755 + 0.1 * 6.449773788452148
Epoch 930, val loss: 1.2537102699279785
Epoch 940, training loss: 0.6544234156608582 = 0.008689140900969505 + 0.1 * 6.457342624664307
Epoch 940, val loss: 1.2586040496826172
Epoch 950, training loss: 0.652872622013092 = 0.008448527194559574 + 0.1 * 6.444240570068359
Epoch 950, val loss: 1.2633750438690186
Epoch 960, training loss: 0.6524554491043091 = 0.008217994123697281 + 0.1 * 6.442374229431152
Epoch 960, val loss: 1.2680974006652832
Epoch 970, training loss: 0.6531441807746887 = 0.00799771212041378 + 0.1 * 6.451464653015137
Epoch 970, val loss: 1.2727257013320923
Epoch 980, training loss: 0.65134596824646 = 0.007786538451910019 + 0.1 * 6.43559455871582
Epoch 980, val loss: 1.2772718667984009
Epoch 990, training loss: 0.6538087725639343 = 0.007584359496831894 + 0.1 * 6.462244033813477
Epoch 990, val loss: 1.2817423343658447
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 2.8041961193084717 = 1.9445141553878784 + 0.1 * 8.596818923950195
Epoch 0, val loss: 1.940222144126892
Epoch 10, training loss: 2.794375419616699 = 1.9347039461135864 + 0.1 * 8.596715927124023
Epoch 10, val loss: 1.931005597114563
Epoch 20, training loss: 2.7820136547088623 = 1.9224090576171875 + 0.1 * 8.596046447753906
Epoch 20, val loss: 1.9191858768463135
Epoch 30, training loss: 2.763671398162842 = 1.9046416282653809 + 0.1 * 8.59029769897461
Epoch 30, val loss: 1.9018864631652832
Epoch 40, training loss: 2.7326107025146484 = 1.8776800632476807 + 0.1 * 8.549306869506836
Epoch 40, val loss: 1.8758926391601562
Epoch 50, training loss: 2.671818733215332 = 1.8403732776641846 + 0.1 * 8.314455032348633
Epoch 50, val loss: 1.841120719909668
Epoch 60, training loss: 2.6021504402160645 = 1.79861581325531 + 0.1 * 8.035345077514648
Epoch 60, val loss: 1.8040682077407837
Epoch 70, training loss: 2.531984329223633 = 1.7588661909103394 + 0.1 * 7.7311811447143555
Epoch 70, val loss: 1.768302321434021
Epoch 80, training loss: 2.460395336151123 = 1.7168633937835693 + 0.1 * 7.4353203773498535
Epoch 80, val loss: 1.729917287826538
Epoch 90, training loss: 2.3899760246276855 = 1.660933256149292 + 0.1 * 7.290428161621094
Epoch 90, val loss: 1.6786775588989258
Epoch 100, training loss: 2.309401273727417 = 1.5862563848495483 + 0.1 * 7.231449604034424
Epoch 100, val loss: 1.610082745552063
Epoch 110, training loss: 2.213115930557251 = 1.4965544939041138 + 0.1 * 7.165614604949951
Epoch 110, val loss: 1.5309994220733643
Epoch 120, training loss: 2.109731435775757 = 1.4007450342178345 + 0.1 * 7.089863300323486
Epoch 120, val loss: 1.448927640914917
Epoch 130, training loss: 2.006493091583252 = 1.3038517236709595 + 0.1 * 7.02641487121582
Epoch 130, val loss: 1.3683606386184692
Epoch 140, training loss: 1.905015468597412 = 1.2060465812683105 + 0.1 * 6.989688396453857
Epoch 140, val loss: 1.2886598110198975
Epoch 150, training loss: 1.8055799007415771 = 1.1089648008346558 + 0.1 * 6.966151237487793
Epoch 150, val loss: 1.2114589214324951
Epoch 160, training loss: 1.709683895111084 = 1.0152926445007324 + 0.1 * 6.943911552429199
Epoch 160, val loss: 1.139934778213501
Epoch 170, training loss: 1.6184148788452148 = 0.9261396527290344 + 0.1 * 6.922752380371094
Epoch 170, val loss: 1.073042869567871
Epoch 180, training loss: 1.534813404083252 = 0.843612015247345 + 0.1 * 6.912014007568359
Epoch 180, val loss: 1.0125184059143066
Epoch 190, training loss: 1.4577478170394897 = 0.7698637843132019 + 0.1 * 6.87883996963501
Epoch 190, val loss: 0.9596789479255676
Epoch 200, training loss: 1.390873908996582 = 0.7040990591049194 + 0.1 * 6.867748260498047
Epoch 200, val loss: 0.91401606798172
Epoch 210, training loss: 1.3296985626220703 = 0.6457784175872803 + 0.1 * 6.839200496673584
Epoch 210, val loss: 0.8754206895828247
Epoch 220, training loss: 1.2754889726638794 = 0.5929188132286072 + 0.1 * 6.825701713562012
Epoch 220, val loss: 0.8427255749702454
Epoch 230, training loss: 1.2251076698303223 = 0.5448570251464844 + 0.1 * 6.8025054931640625
Epoch 230, val loss: 0.8156404495239258
Epoch 240, training loss: 1.180511713027954 = 0.5009162425994873 + 0.1 * 6.795954704284668
Epoch 240, val loss: 0.793409526348114
Epoch 250, training loss: 1.1390860080718994 = 0.46000686287879944 + 0.1 * 6.790791034698486
Epoch 250, val loss: 0.7753767371177673
Epoch 260, training loss: 1.0978379249572754 = 0.42191529273986816 + 0.1 * 6.759225845336914
Epoch 260, val loss: 0.76080721616745
Epoch 270, training loss: 1.060728669166565 = 0.38586995005607605 + 0.1 * 6.748587131500244
Epoch 270, val loss: 0.7489891052246094
Epoch 280, training loss: 1.026989221572876 = 0.3520852327346802 + 0.1 * 6.749039173126221
Epoch 280, val loss: 0.7397983074188232
Epoch 290, training loss: 0.9935169219970703 = 0.32075169682502747 + 0.1 * 6.727652072906494
Epoch 290, val loss: 0.7331132888793945
Epoch 300, training loss: 0.9629662036895752 = 0.2916167378425598 + 0.1 * 6.713494300842285
Epoch 300, val loss: 0.7291373610496521
Epoch 310, training loss: 0.9349828958511353 = 0.264929860830307 + 0.1 * 6.700530052185059
Epoch 310, val loss: 0.7278839945793152
Epoch 320, training loss: 0.910574197769165 = 0.24091088771820068 + 0.1 * 6.6966328620910645
Epoch 320, val loss: 0.7290850877761841
Epoch 330, training loss: 0.8883880376815796 = 0.21940740942955017 + 0.1 * 6.6898064613342285
Epoch 330, val loss: 0.7325205206871033
Epoch 340, training loss: 0.8678781986236572 = 0.20026735961437225 + 0.1 * 6.676108360290527
Epoch 340, val loss: 0.7377864718437195
Epoch 350, training loss: 0.8500228524208069 = 0.1831125169992447 + 0.1 * 6.669103622436523
Epoch 350, val loss: 0.7447562217712402
Epoch 360, training loss: 0.8349703550338745 = 0.16771318018436432 + 0.1 * 6.672571659088135
Epoch 360, val loss: 0.7531424760818481
Epoch 370, training loss: 0.8191853761672974 = 0.15392650663852692 + 0.1 * 6.652588844299316
Epoch 370, val loss: 0.7626293897628784
Epoch 380, training loss: 0.8066567778587341 = 0.14153029024600983 + 0.1 * 6.6512651443481445
Epoch 380, val loss: 0.7731003761291504
Epoch 390, training loss: 0.7951938509941101 = 0.13035643100738525 + 0.1 * 6.648374080657959
Epoch 390, val loss: 0.7843625545501709
Epoch 400, training loss: 0.7832828164100647 = 0.12028170377016068 + 0.1 * 6.630011081695557
Epoch 400, val loss: 0.7961888909339905
Epoch 410, training loss: 0.774156928062439 = 0.11112692952156067 + 0.1 * 6.6302995681762695
Epoch 410, val loss: 0.8085678815841675
Epoch 420, training loss: 0.7647187113761902 = 0.10280036926269531 + 0.1 * 6.61918306350708
Epoch 420, val loss: 0.8213154673576355
Epoch 430, training loss: 0.7571505308151245 = 0.09520015120506287 + 0.1 * 6.619503974914551
Epoch 430, val loss: 0.8343780636787415
Epoch 440, training loss: 0.7492796182632446 = 0.08827216178178787 + 0.1 * 6.610074520111084
Epoch 440, val loss: 0.8476110100746155
Epoch 450, training loss: 0.7422903776168823 = 0.08193384110927582 + 0.1 * 6.603565216064453
Epoch 450, val loss: 0.8610150218009949
Epoch 460, training loss: 0.736946702003479 = 0.07612579315900803 + 0.1 * 6.608209133148193
Epoch 460, val loss: 0.8745082020759583
Epoch 470, training loss: 0.7305467128753662 = 0.07080968469381332 + 0.1 * 6.597370624542236
Epoch 470, val loss: 0.8879775404930115
Epoch 480, training loss: 0.7245766520500183 = 0.06593083590269089 + 0.1 * 6.586458206176758
Epoch 480, val loss: 0.9014774560928345
Epoch 490, training loss: 0.7200997471809387 = 0.06145305931568146 + 0.1 * 6.5864667892456055
Epoch 490, val loss: 0.9149417281150818
Epoch 500, training loss: 0.7167719602584839 = 0.05734273046255112 + 0.1 * 6.594292163848877
Epoch 500, val loss: 0.9282915592193604
Epoch 510, training loss: 0.7110333442687988 = 0.05357871577143669 + 0.1 * 6.574545860290527
Epoch 510, val loss: 0.9414686560630798
Epoch 520, training loss: 0.7066556215286255 = 0.05012022331357002 + 0.1 * 6.565353870391846
Epoch 520, val loss: 0.954577624797821
Epoch 530, training loss: 0.7041296362876892 = 0.04693738371133804 + 0.1 * 6.571922302246094
Epoch 530, val loss: 0.967549204826355
Epoch 540, training loss: 0.7000617384910583 = 0.04400802031159401 + 0.1 * 6.560536861419678
Epoch 540, val loss: 0.9803865551948547
Epoch 550, training loss: 0.7002179026603699 = 0.0413225032389164 + 0.1 * 6.588953495025635
Epoch 550, val loss: 0.9929548501968384
Epoch 560, training loss: 0.6946688890457153 = 0.03886276111006737 + 0.1 * 6.558061122894287
Epoch 560, val loss: 1.0051931142807007
Epoch 570, training loss: 0.6909390687942505 = 0.03659271448850632 + 0.1 * 6.543463230133057
Epoch 570, val loss: 1.0173954963684082
Epoch 580, training loss: 0.6893423199653625 = 0.03449394926428795 + 0.1 * 6.548483848571777
Epoch 580, val loss: 1.029436707496643
Epoch 590, training loss: 0.6909911632537842 = 0.03255624696612358 + 0.1 * 6.584349155426025
Epoch 590, val loss: 1.0412482023239136
Epoch 600, training loss: 0.685978353023529 = 0.03077591210603714 + 0.1 * 6.5520243644714355
Epoch 600, val loss: 1.052675724029541
Epoch 610, training loss: 0.6831710934638977 = 0.029129883274435997 + 0.1 * 6.540412425994873
Epoch 610, val loss: 1.0639851093292236
Epoch 620, training loss: 0.6809203624725342 = 0.027604153379797935 + 0.1 * 6.5331621170043945
Epoch 620, val loss: 1.075096845626831
Epoch 630, training loss: 0.6788439154624939 = 0.026187714189291 + 0.1 * 6.526561737060547
Epoch 630, val loss: 1.086025595664978
Epoch 640, training loss: 0.6777926087379456 = 0.02487431839108467 + 0.1 * 6.5291829109191895
Epoch 640, val loss: 1.0967174768447876
Epoch 650, training loss: 0.6760749816894531 = 0.02365393191576004 + 0.1 * 6.524210453033447
Epoch 650, val loss: 1.1071711778640747
Epoch 660, training loss: 0.6750608086585999 = 0.02251875400543213 + 0.1 * 6.525420665740967
Epoch 660, val loss: 1.1174907684326172
Epoch 670, training loss: 0.6726627945899963 = 0.021461190655827522 + 0.1 * 6.5120158195495605
Epoch 670, val loss: 1.1275336742401123
Epoch 680, training loss: 0.6730875372886658 = 0.020475512370467186 + 0.1 * 6.526120185852051
Epoch 680, val loss: 1.1374555826187134
Epoch 690, training loss: 0.6706870794296265 = 0.0195551086217165 + 0.1 * 6.511319637298584
Epoch 690, val loss: 1.147149920463562
Epoch 700, training loss: 0.6706386208534241 = 0.018695319071412086 + 0.1 * 6.51943302154541
Epoch 700, val loss: 1.1566752195358276
Epoch 710, training loss: 0.6687936186790466 = 0.01789325848221779 + 0.1 * 6.509003639221191
Epoch 710, val loss: 1.1659437417984009
Epoch 720, training loss: 0.666803240776062 = 0.01714250072836876 + 0.1 * 6.496607303619385
Epoch 720, val loss: 1.174979329109192
Epoch 730, training loss: 0.6674489974975586 = 0.016438234597444534 + 0.1 * 6.510107517242432
Epoch 730, val loss: 1.183916449546814
Epoch 740, training loss: 0.6651002764701843 = 0.01577794924378395 + 0.1 * 6.493223190307617
Epoch 740, val loss: 1.1926205158233643
Epoch 750, training loss: 0.6658850908279419 = 0.015156636945903301 + 0.1 * 6.507284641265869
Epoch 750, val loss: 1.2012182474136353
Epoch 760, training loss: 0.663777232170105 = 0.014571895822882652 + 0.1 * 6.492053031921387
Epoch 760, val loss: 1.20963454246521
Epoch 770, training loss: 0.6629250645637512 = 0.014021330513060093 + 0.1 * 6.489037036895752
Epoch 770, val loss: 1.2178667783737183
Epoch 780, training loss: 0.662041187286377 = 0.013502790592610836 + 0.1 * 6.485383987426758
Epoch 780, val loss: 1.2259491682052612
Epoch 790, training loss: 0.661260724067688 = 0.013013453222811222 + 0.1 * 6.4824724197387695
Epoch 790, val loss: 1.2338908910751343
Epoch 800, training loss: 0.6614985466003418 = 0.012550747953355312 + 0.1 * 6.489477634429932
Epoch 800, val loss: 1.241673469543457
Epoch 810, training loss: 0.6601008772850037 = 0.012113737873733044 + 0.1 * 6.4798712730407715
Epoch 810, val loss: 1.2493067979812622
Epoch 820, training loss: 0.660285234451294 = 0.01170108001679182 + 0.1 * 6.485841274261475
Epoch 820, val loss: 1.2567743062973022
Epoch 830, training loss: 0.658092737197876 = 0.01131031010299921 + 0.1 * 6.4678239822387695
Epoch 830, val loss: 1.2640621662139893
Epoch 840, training loss: 0.659925639629364 = 0.010939725674688816 + 0.1 * 6.489859104156494
Epoch 840, val loss: 1.271286129951477
Epoch 850, training loss: 0.6586602330207825 = 0.010587894357740879 + 0.1 * 6.4807233810424805
Epoch 850, val loss: 1.2783257961273193
Epoch 860, training loss: 0.6585763096809387 = 0.010254300199449062 + 0.1 * 6.483219623565674
Epoch 860, val loss: 1.2852728366851807
Epoch 870, training loss: 0.6577107906341553 = 0.00993699487298727 + 0.1 * 6.477737903594971
Epoch 870, val loss: 1.2920266389846802
Epoch 880, training loss: 0.656401515007019 = 0.009636007249355316 + 0.1 * 6.467655181884766
Epoch 880, val loss: 1.298689365386963
Epoch 890, training loss: 0.6554574966430664 = 0.009348604828119278 + 0.1 * 6.46108865737915
Epoch 890, val loss: 1.3052934408187866
Epoch 900, training loss: 0.6564882397651672 = 0.009075786918401718 + 0.1 * 6.474124431610107
Epoch 900, val loss: 1.3117069005966187
Epoch 910, training loss: 0.6547037363052368 = 0.008815090171992779 + 0.1 * 6.45888614654541
Epoch 910, val loss: 1.3180644512176514
Epoch 920, training loss: 0.6552428007125854 = 0.008566191419959068 + 0.1 * 6.466765880584717
Epoch 920, val loss: 1.3243048191070557
Epoch 930, training loss: 0.6540772914886475 = 0.008329251781105995 + 0.1 * 6.457479953765869
Epoch 930, val loss: 1.3304115533828735
Epoch 940, training loss: 0.6535540223121643 = 0.008103021420538425 + 0.1 * 6.45451021194458
Epoch 940, val loss: 1.3363838195800781
Epoch 950, training loss: 0.6538283824920654 = 0.007886099629104137 + 0.1 * 6.459422588348389
Epoch 950, val loss: 1.3422750234603882
Epoch 960, training loss: 0.6527683734893799 = 0.0076790498569607735 + 0.1 * 6.450893402099609
Epoch 960, val loss: 1.3481497764587402
Epoch 970, training loss: 0.6525425910949707 = 0.007480693515390158 + 0.1 * 6.450618743896484
Epoch 970, val loss: 1.3538341522216797
Epoch 980, training loss: 0.6517205834388733 = 0.007291419431567192 + 0.1 * 6.444291591644287
Epoch 980, val loss: 1.3594168424606323
Epoch 990, training loss: 0.6521267294883728 = 0.007109104190021753 + 0.1 * 6.450176239013672
Epoch 990, val loss: 1.3649680614471436
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8397469688982605
The final CL Acc:0.80123, 0.01429, The final GNN Acc:0.84273, 0.00317
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9458])
updated graph: torch.Size([2, 10540])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8135337829589844 = 1.9538480043411255 + 0.1 * 8.596858978271484
Epoch 0, val loss: 1.9462569952011108
Epoch 10, training loss: 2.8034305572509766 = 1.9437475204467773 + 0.1 * 8.596830368041992
Epoch 10, val loss: 1.9365143775939941
Epoch 20, training loss: 2.7915802001953125 = 1.9319190979003906 + 0.1 * 8.596611976623535
Epoch 20, val loss: 1.9245436191558838
Epoch 30, training loss: 2.775346040725708 = 1.915846824645996 + 0.1 * 8.594991683959961
Epoch 30, val loss: 1.907720923423767
Epoch 40, training loss: 2.750464916229248 = 1.8923500776290894 + 0.1 * 8.581149101257324
Epoch 40, val loss: 1.8828543424606323
Epoch 50, training loss: 2.7102274894714355 = 1.8588382005691528 + 0.1 * 8.513894081115723
Epoch 50, val loss: 1.8486019372940063
Epoch 60, training loss: 2.6460659503936768 = 1.8190280199050903 + 0.1 * 8.270379066467285
Epoch 60, val loss: 1.8120896816253662
Epoch 70, training loss: 2.584597587585449 = 1.7832331657409668 + 0.1 * 8.013644218444824
Epoch 70, val loss: 1.78380286693573
Epoch 80, training loss: 2.5036847591400146 = 1.7491540908813477 + 0.1 * 7.54530668258667
Epoch 80, val loss: 1.7558386325836182
Epoch 90, training loss: 2.436349868774414 = 1.7059792280197144 + 0.1 * 7.30370569229126
Epoch 90, val loss: 1.7179985046386719
Epoch 100, training loss: 2.3690879344940186 = 1.6493414640426636 + 0.1 * 7.197464466094971
Epoch 100, val loss: 1.669023871421814
Epoch 110, training loss: 2.2898106575012207 = 1.5758577585220337 + 0.1 * 7.139527797698975
Epoch 110, val loss: 1.6068090200424194
Epoch 120, training loss: 2.1958377361297607 = 1.4866634607315063 + 0.1 * 7.091742038726807
Epoch 120, val loss: 1.5333913564682007
Epoch 130, training loss: 2.0910770893096924 = 1.3854137659072876 + 0.1 * 7.056633472442627
Epoch 130, val loss: 1.4528268575668335
Epoch 140, training loss: 1.9806095361709595 = 1.2770602703094482 + 0.1 * 7.035492420196533
Epoch 140, val loss: 1.3684749603271484
Epoch 150, training loss: 1.868715524673462 = 1.1662706136703491 + 0.1 * 7.024449825286865
Epoch 150, val loss: 1.2846848964691162
Epoch 160, training loss: 1.7604804039001465 = 1.058441162109375 + 0.1 * 7.020391464233398
Epoch 160, val loss: 1.2051254510879517
Epoch 170, training loss: 1.6606645584106445 = 0.9591789841651917 + 0.1 * 7.014855861663818
Epoch 170, val loss: 1.1346817016601562
Epoch 180, training loss: 1.5717926025390625 = 0.8707155585289001 + 0.1 * 7.010770797729492
Epoch 180, val loss: 1.0745315551757812
Epoch 190, training loss: 1.494466781616211 = 0.7938091158866882 + 0.1 * 7.006576061248779
Epoch 190, val loss: 1.0252379179000854
Epoch 200, training loss: 1.4277957677841187 = 0.7278268337249756 + 0.1 * 6.999689102172852
Epoch 200, val loss: 0.9860910177230835
Epoch 210, training loss: 1.3700156211853027 = 0.6706882119178772 + 0.1 * 6.993273735046387
Epoch 210, val loss: 0.9551084041595459
Epoch 220, training loss: 1.3180599212646484 = 0.6195200085639954 + 0.1 * 6.985398292541504
Epoch 220, val loss: 0.9295333623886108
Epoch 230, training loss: 1.2701642513275146 = 0.5723366737365723 + 0.1 * 6.978274822235107
Epoch 230, val loss: 0.9075043797492981
Epoch 240, training loss: 1.2248663902282715 = 0.5280079245567322 + 0.1 * 6.968584060668945
Epoch 240, val loss: 0.8883878588676453
Epoch 250, training loss: 1.1818568706512451 = 0.48568207025527954 + 0.1 * 6.961748123168945
Epoch 250, val loss: 0.8717222213745117
Epoch 260, training loss: 1.1398863792419434 = 0.44488686323165894 + 0.1 * 6.9499945640563965
Epoch 260, val loss: 0.8573394417762756
Epoch 270, training loss: 1.099167823791504 = 0.40532952547073364 + 0.1 * 6.938382148742676
Epoch 270, val loss: 0.8447737097740173
Epoch 280, training loss: 1.0633641481399536 = 0.3668260872364044 + 0.1 * 6.9653801918029785
Epoch 280, val loss: 0.8336398005485535
Epoch 290, training loss: 1.022249460220337 = 0.3299449384212494 + 0.1 * 6.923045635223389
Epoch 290, val loss: 0.8238807916641235
Epoch 300, training loss: 0.9860213994979858 = 0.2941785156726837 + 0.1 * 6.918428897857666
Epoch 300, val loss: 0.8151843547821045
Epoch 310, training loss: 0.9505620002746582 = 0.2596531808376312 + 0.1 * 6.909088611602783
Epoch 310, val loss: 0.8079705238342285
Epoch 320, training loss: 0.9168872833251953 = 0.2268933206796646 + 0.1 * 6.89993953704834
Epoch 320, val loss: 0.8024953603744507
Epoch 330, training loss: 0.8860058188438416 = 0.196693554520607 + 0.1 * 6.893122673034668
Epoch 330, val loss: 0.7992428541183472
Epoch 340, training loss: 0.8611074686050415 = 0.16977612674236298 + 0.1 * 6.913312911987305
Epoch 340, val loss: 0.798620879650116
Epoch 350, training loss: 0.8347150087356567 = 0.14669440686702728 + 0.1 * 6.8802056312561035
Epoch 350, val loss: 0.8004443049430847
Epoch 360, training loss: 0.8143414258956909 = 0.12715093791484833 + 0.1 * 6.871904373168945
Epoch 360, val loss: 0.8048273921012878
Epoch 370, training loss: 0.8010034561157227 = 0.11070245504379272 + 0.1 * 6.90300989151001
Epoch 370, val loss: 0.8111794590950012
Epoch 380, training loss: 0.7842915654182434 = 0.09703666716814041 + 0.1 * 6.872549057006836
Epoch 380, val loss: 0.8192080855369568
Epoch 390, training loss: 0.7706938982009888 = 0.08556688576936722 + 0.1 * 6.8512701988220215
Epoch 390, val loss: 0.8285403847694397
Epoch 400, training loss: 0.7599464654922485 = 0.0758669525384903 + 0.1 * 6.840795040130615
Epoch 400, val loss: 0.8389328122138977
Epoch 410, training loss: 0.7531328797340393 = 0.06762252002954483 + 0.1 * 6.855103015899658
Epoch 410, val loss: 0.8500965237617493
Epoch 420, training loss: 0.7428082823753357 = 0.060626737773418427 + 0.1 * 6.821815490722656
Epoch 420, val loss: 0.8616449236869812
Epoch 430, training loss: 0.7429245710372925 = 0.054641906172037125 + 0.1 * 6.882826805114746
Epoch 430, val loss: 0.873319149017334
Epoch 440, training loss: 0.7312048673629761 = 0.049557432532310486 + 0.1 * 6.816473960876465
Epoch 440, val loss: 0.8850624561309814
Epoch 450, training loss: 0.7242385745048523 = 0.0451774075627327 + 0.1 * 6.790611743927002
Epoch 450, val loss: 0.8966925740242004
Epoch 460, training loss: 0.7194707989692688 = 0.0413614921271801 + 0.1 * 6.781092643737793
Epoch 460, val loss: 0.9084303975105286
Epoch 470, training loss: 0.7158970832824707 = 0.03801743686199188 + 0.1 * 6.778796195983887
Epoch 470, val loss: 0.9198529124259949
Epoch 480, training loss: 0.7119948863983154 = 0.03509055823087692 + 0.1 * 6.76904296875
Epoch 480, val loss: 0.9312117099761963
Epoch 490, training loss: 0.7085870504379272 = 0.032500434666872025 + 0.1 * 6.760866165161133
Epoch 490, val loss: 0.9421997666358948
Epoch 500, training loss: 0.7062309384346008 = 0.030207311734557152 + 0.1 * 6.760235786437988
Epoch 500, val loss: 0.9530795812606812
Epoch 510, training loss: 0.702106773853302 = 0.028163578361272812 + 0.1 * 6.739432334899902
Epoch 510, val loss: 0.9636625051498413
Epoch 520, training loss: 0.6990048885345459 = 0.02633409947156906 + 0.1 * 6.726707458496094
Epoch 520, val loss: 0.9738953709602356
Epoch 530, training loss: 0.6962083578109741 = 0.02469060570001602 + 0.1 * 6.715177059173584
Epoch 530, val loss: 0.9841030836105347
Epoch 540, training loss: 0.6952884793281555 = 0.023203901946544647 + 0.1 * 6.720845699310303
Epoch 540, val loss: 0.9939160346984863
Epoch 550, training loss: 0.6934471130371094 = 0.02185908332467079 + 0.1 * 6.715879917144775
Epoch 550, val loss: 1.0033098459243774
Epoch 560, training loss: 0.6898981928825378 = 0.020641671493649483 + 0.1 * 6.692564964294434
Epoch 560, val loss: 1.012792706489563
Epoch 570, training loss: 0.6904412508010864 = 0.01952844113111496 + 0.1 * 6.709127902984619
Epoch 570, val loss: 1.0218092203140259
Epoch 580, training loss: 0.6870071291923523 = 0.01850898005068302 + 0.1 * 6.684981346130371
Epoch 580, val loss: 1.0307254791259766
Epoch 590, training loss: 0.6849029064178467 = 0.017573196440935135 + 0.1 * 6.673296928405762
Epoch 590, val loss: 1.0393353700637817
Epoch 600, training loss: 0.6851596832275391 = 0.01671365275979042 + 0.1 * 6.684459686279297
Epoch 600, val loss: 1.0479236841201782
Epoch 610, training loss: 0.6832191348075867 = 0.015921983867883682 + 0.1 * 6.672971248626709
Epoch 610, val loss: 1.0560448169708252
Epoch 620, training loss: 0.6817607879638672 = 0.015191393904387951 + 0.1 * 6.665693759918213
Epoch 620, val loss: 1.0641708374023438
Epoch 630, training loss: 0.6804425716400146 = 0.014514188282191753 + 0.1 * 6.659283638000488
Epoch 630, val loss: 1.071901798248291
Epoch 640, training loss: 0.6794912815093994 = 0.013886917382478714 + 0.1 * 6.656043529510498
Epoch 640, val loss: 1.079711675643921
Epoch 650, training loss: 0.6792736649513245 = 0.01330223586410284 + 0.1 * 6.659714698791504
Epoch 650, val loss: 1.0872122049331665
Epoch 660, training loss: 0.6766157150268555 = 0.012758506461977959 + 0.1 * 6.6385722160339355
Epoch 660, val loss: 1.0945013761520386
Epoch 670, training loss: 0.6763953566551208 = 0.012251462787389755 + 0.1 * 6.641438961029053
Epoch 670, val loss: 1.1017775535583496
Epoch 680, training loss: 0.6764606833457947 = 0.011776181869208813 + 0.1 * 6.646844863891602
Epoch 680, val loss: 1.1088159084320068
Epoch 690, training loss: 0.6756357550621033 = 0.011332209222018719 + 0.1 * 6.643035411834717
Epoch 690, val loss: 1.1155191659927368
Epoch 700, training loss: 0.6732454895973206 = 0.010917266830801964 + 0.1 * 6.623282432556152
Epoch 700, val loss: 1.1224373579025269
Epoch 710, training loss: 0.6724235415458679 = 0.010526498779654503 + 0.1 * 6.6189703941345215
Epoch 710, val loss: 1.1290690898895264
Epoch 720, training loss: 0.6717661023139954 = 0.010157224722206593 + 0.1 * 6.616088390350342
Epoch 720, val loss: 1.1355724334716797
Epoch 730, training loss: 0.6734325289726257 = 0.009808104485273361 + 0.1 * 6.63624382019043
Epoch 730, val loss: 1.1417819261550903
Epoch 740, training loss: 0.6708173751831055 = 0.009480983018875122 + 0.1 * 6.613363265991211
Epoch 740, val loss: 1.1480661630630493
Epoch 750, training loss: 0.6701077222824097 = 0.009171959944069386 + 0.1 * 6.6093573570251465
Epoch 750, val loss: 1.1542435884475708
Epoch 760, training loss: 0.6689687967300415 = 0.008879107423126698 + 0.1 * 6.600896835327148
Epoch 760, val loss: 1.1601099967956543
Epoch 770, training loss: 0.6679338216781616 = 0.008602081798017025 + 0.1 * 6.593317031860352
Epoch 770, val loss: 1.1660590171813965
Epoch 780, training loss: 0.6672214865684509 = 0.008339033462107182 + 0.1 * 6.588824272155762
Epoch 780, val loss: 1.1716711521148682
Epoch 790, training loss: 0.6671389937400818 = 0.008090422488749027 + 0.1 * 6.590485572814941
Epoch 790, val loss: 1.177358627319336
Epoch 800, training loss: 0.6667356491088867 = 0.00785378459841013 + 0.1 * 6.588818073272705
Epoch 800, val loss: 1.1830406188964844
Epoch 810, training loss: 0.6660748720169067 = 0.007627925369888544 + 0.1 * 6.584469795227051
Epoch 810, val loss: 1.188319206237793
Epoch 820, training loss: 0.6649812459945679 = 0.007413086947053671 + 0.1 * 6.575681209564209
Epoch 820, val loss: 1.193575143814087
Epoch 830, training loss: 0.6651744842529297 = 0.007209138013422489 + 0.1 * 6.579653739929199
Epoch 830, val loss: 1.1989569664001465
Epoch 840, training loss: 0.6653308272361755 = 0.007014249451458454 + 0.1 * 6.583165645599365
Epoch 840, val loss: 1.20412278175354
Epoch 850, training loss: 0.663773775100708 = 0.00682789646089077 + 0.1 * 6.569458484649658
Epoch 850, val loss: 1.2090400457382202
Epoch 860, training loss: 0.6640112996101379 = 0.0066500818356871605 + 0.1 * 6.573612213134766
Epoch 860, val loss: 1.2140921354293823
Epoch 870, training loss: 0.6626681685447693 = 0.006479890085756779 + 0.1 * 6.561882495880127
Epoch 870, val loss: 1.218889832496643
Epoch 880, training loss: 0.6630344986915588 = 0.006317541468888521 + 0.1 * 6.567169189453125
Epoch 880, val loss: 1.223708987236023
Epoch 890, training loss: 0.6634069085121155 = 0.006161559373140335 + 0.1 * 6.572453022003174
Epoch 890, val loss: 1.2284891605377197
Epoch 900, training loss: 0.6614653468132019 = 0.0060122935101389885 + 0.1 * 6.554530143737793
Epoch 900, val loss: 1.2330267429351807
Epoch 910, training loss: 0.6620977520942688 = 0.00586981326341629 + 0.1 * 6.562279224395752
Epoch 910, val loss: 1.2376806735992432
Epoch 920, training loss: 0.660831093788147 = 0.005732528865337372 + 0.1 * 6.550985336303711
Epoch 920, val loss: 1.2421878576278687
Epoch 930, training loss: 0.6600540280342102 = 0.005600530654191971 + 0.1 * 6.544535160064697
Epoch 930, val loss: 1.2465912103652954
Epoch 940, training loss: 0.6610543727874756 = 0.005474027246236801 + 0.1 * 6.555803298950195
Epoch 940, val loss: 1.2508994340896606
Epoch 950, training loss: 0.6594964861869812 = 0.005352231208235025 + 0.1 * 6.541442394256592
Epoch 950, val loss: 1.255120038986206
Epoch 960, training loss: 0.660892128944397 = 0.0052353511564433575 + 0.1 * 6.556567668914795
Epoch 960, val loss: 1.2594387531280518
Epoch 970, training loss: 0.6586689949035645 = 0.005122683942317963 + 0.1 * 6.535463333129883
Epoch 970, val loss: 1.2635573148727417
Epoch 980, training loss: 0.6591015458106995 = 0.005014296621084213 + 0.1 * 6.540872573852539
Epoch 980, val loss: 1.2676206827163696
Epoch 990, training loss: 0.6592787504196167 = 0.004909544251859188 + 0.1 * 6.543692111968994
Epoch 990, val loss: 1.2716095447540283
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 2.786820411682129 = 1.9271353483200073 + 0.1 * 8.596851348876953
Epoch 0, val loss: 1.9253454208374023
Epoch 10, training loss: 2.7770495414733887 = 1.917370080947876 + 0.1 * 8.596793174743652
Epoch 10, val loss: 1.9157129526138306
Epoch 20, training loss: 2.7653491497039795 = 1.9057023525238037 + 0.1 * 8.596467018127441
Epoch 20, val loss: 1.9039604663848877
Epoch 30, training loss: 2.749110221862793 = 1.8897182941436768 + 0.1 * 8.593917846679688
Epoch 30, val loss: 1.887864351272583
Epoch 40, training loss: 2.7243196964263916 = 1.866788625717163 + 0.1 * 8.575309753417969
Epoch 40, val loss: 1.8651005029678345
Epoch 50, training loss: 2.6863012313842773 = 1.8361886739730835 + 0.1 * 8.501126289367676
Epoch 50, val loss: 1.836300015449524
Epoch 60, training loss: 2.6284961700439453 = 1.8035321235656738 + 0.1 * 8.249639511108398
Epoch 60, val loss: 1.8077725172042847
Epoch 70, training loss: 2.5737175941467285 = 1.7726612091064453 + 0.1 * 8.010562896728516
Epoch 70, val loss: 1.7801045179367065
Epoch 80, training loss: 2.4950754642486572 = 1.7367585897445679 + 0.1 * 7.583168983459473
Epoch 80, val loss: 1.7455910444259644
Epoch 90, training loss: 2.4241251945495605 = 1.6913858652114868 + 0.1 * 7.327392101287842
Epoch 90, val loss: 1.7035795450210571
Epoch 100, training loss: 2.354271173477173 = 1.631499171257019 + 0.1 * 7.227719306945801
Epoch 100, val loss: 1.650671362876892
Epoch 110, training loss: 2.2726359367370605 = 1.554145336151123 + 0.1 * 7.184905052185059
Epoch 110, val loss: 1.583201289176941
Epoch 120, training loss: 2.176887035369873 = 1.4610332250595093 + 0.1 * 7.158537864685059
Epoch 120, val loss: 1.5054155588150024
Epoch 130, training loss: 2.0707523822784424 = 1.3571144342422485 + 0.1 * 7.136380195617676
Epoch 130, val loss: 1.4208917617797852
Epoch 140, training loss: 1.9600749015808105 = 1.2484899759292603 + 0.1 * 7.115848541259766
Epoch 140, val loss: 1.3335962295532227
Epoch 150, training loss: 1.8490498065948486 = 1.1396337747573853 + 0.1 * 7.094160079956055
Epoch 150, val loss: 1.2482247352600098
Epoch 160, training loss: 1.741964340209961 = 1.0345577001571655 + 0.1 * 7.074067115783691
Epoch 160, val loss: 1.167913794517517
Epoch 170, training loss: 1.6429674625396729 = 0.9366604685783386 + 0.1 * 7.063069820404053
Epoch 170, val loss: 1.095298171043396
Epoch 180, training loss: 1.5545532703399658 = 0.8493163585662842 + 0.1 * 7.052368640899658
Epoch 180, val loss: 1.033122181892395
Epoch 190, training loss: 1.476940393447876 = 0.7724972367286682 + 0.1 * 7.044431209564209
Epoch 190, val loss: 0.9814954996109009
Epoch 200, training loss: 1.4084217548370361 = 0.7045971751213074 + 0.1 * 7.038245677947998
Epoch 200, val loss: 0.9390252232551575
Epoch 210, training loss: 1.3465062379837036 = 0.6432763338088989 + 0.1 * 7.032299041748047
Epoch 210, val loss: 0.9035173058509827
Epoch 220, training loss: 1.2890231609344482 = 0.586259126663208 + 0.1 * 7.027639865875244
Epoch 220, val loss: 0.8730375170707703
Epoch 230, training loss: 1.2342498302459717 = 0.5320095419883728 + 0.1 * 7.022402286529541
Epoch 230, val loss: 0.8462111949920654
Epoch 240, training loss: 1.1815786361694336 = 0.479814350605011 + 0.1 * 7.017643451690674
Epoch 240, val loss: 0.8226607441902161
Epoch 250, training loss: 1.1313492059707642 = 0.4297730028629303 + 0.1 * 7.015761375427246
Epoch 250, val loss: 0.8028952479362488
Epoch 260, training loss: 1.0838000774383545 = 0.38286539912223816 + 0.1 * 7.0093464851379395
Epoch 260, val loss: 0.7876529693603516
Epoch 270, training loss: 1.0399781465530396 = 0.339627742767334 + 0.1 * 7.003503799438477
Epoch 270, val loss: 0.7772364616394043
Epoch 280, training loss: 1.000069499015808 = 0.3006230294704437 + 0.1 * 6.994464874267578
Epoch 280, val loss: 0.7714275121688843
Epoch 290, training loss: 0.9643775224685669 = 0.2657648026943207 + 0.1 * 6.9861273765563965
Epoch 290, val loss: 0.7696370482444763
Epoch 300, training loss: 0.9335278868675232 = 0.23470497131347656 + 0.1 * 6.988229274749756
Epoch 300, val loss: 0.7710312008857727
Epoch 310, training loss: 0.9036285877227783 = 0.20733419060707092 + 0.1 * 6.962944030761719
Epoch 310, val loss: 0.7751411199569702
Epoch 320, training loss: 0.8774290084838867 = 0.18302327394485474 + 0.1 * 6.944057464599609
Epoch 320, val loss: 0.7812089920043945
Epoch 330, training loss: 0.8554983139038086 = 0.16138917207717896 + 0.1 * 6.941091537475586
Epoch 330, val loss: 0.7890543341636658
Epoch 340, training loss: 0.8332230448722839 = 0.14236073195934296 + 0.1 * 6.908622741699219
Epoch 340, val loss: 0.7981458902359009
Epoch 350, training loss: 0.815816342830658 = 0.12570668756961823 + 0.1 * 6.901096343994141
Epoch 350, val loss: 0.8084102272987366
Epoch 360, training loss: 0.7997385263442993 = 0.11133211106061935 + 0.1 * 6.884063720703125
Epoch 360, val loss: 0.8195577263832092
Epoch 370, training loss: 0.7852680087089539 = 0.09884236752986908 + 0.1 * 6.864256381988525
Epoch 370, val loss: 0.8315783143043518
Epoch 380, training loss: 0.7756045460700989 = 0.08798564970493317 + 0.1 * 6.876189231872559
Epoch 380, val loss: 0.844515860080719
Epoch 390, training loss: 0.7628958821296692 = 0.07862545549869537 + 0.1 * 6.8427042961120605
Epoch 390, val loss: 0.8577739596366882
Epoch 400, training loss: 0.753516674041748 = 0.07052119076251984 + 0.1 * 6.829954624176025
Epoch 400, val loss: 0.8714548945426941
Epoch 410, training loss: 0.7450509071350098 = 0.06351397186517715 + 0.1 * 6.815369129180908
Epoch 410, val loss: 0.8853026628494263
Epoch 420, training loss: 0.7377739548683167 = 0.057437777519226074 + 0.1 * 6.803361892700195
Epoch 420, val loss: 0.8992481827735901
Epoch 430, training loss: 0.7314485907554626 = 0.052147574722766876 + 0.1 * 6.793010234832764
Epoch 430, val loss: 0.9131550788879395
Epoch 440, training loss: 0.7268955707550049 = 0.04753846302628517 + 0.1 * 6.7935709953308105
Epoch 440, val loss: 0.9269790053367615
Epoch 450, training loss: 0.7210367321968079 = 0.04352088272571564 + 0.1 * 6.775158405303955
Epoch 450, val loss: 0.9404931664466858
Epoch 460, training loss: 0.7177863121032715 = 0.03998732194304466 + 0.1 * 6.777989864349365
Epoch 460, val loss: 0.9538825154304504
Epoch 470, training loss: 0.714192807674408 = 0.036889150738716125 + 0.1 * 6.773036479949951
Epoch 470, val loss: 0.966862678527832
Epoch 480, training loss: 0.7099137306213379 = 0.034160222858190536 + 0.1 * 6.757535457611084
Epoch 480, val loss: 0.9795399904251099
Epoch 490, training loss: 0.7068771123886108 = 0.03172692283987999 + 0.1 * 6.75150203704834
Epoch 490, val loss: 0.991976261138916
Epoch 500, training loss: 0.7037644386291504 = 0.029545122757554054 + 0.1 * 6.742193222045898
Epoch 500, val loss: 1.0041922330856323
Epoch 510, training loss: 0.7018771171569824 = 0.027586132287979126 + 0.1 * 6.7429094314575195
Epoch 510, val loss: 1.016167163848877
Epoch 520, training loss: 0.6994526982307434 = 0.025829631835222244 + 0.1 * 6.736230850219727
Epoch 520, val loss: 1.0277165174484253
Epoch 530, training loss: 0.6984920501708984 = 0.024243982508778572 + 0.1 * 6.742480278015137
Epoch 530, val loss: 1.039093017578125
Epoch 540, training loss: 0.6952618360519409 = 0.022813020274043083 + 0.1 * 6.724488258361816
Epoch 540, val loss: 1.050068974494934
Epoch 550, training loss: 0.694116473197937 = 0.02151000313460827 + 0.1 * 6.726064682006836
Epoch 550, val loss: 1.0608280897140503
Epoch 560, training loss: 0.6921265721321106 = 0.020324567332863808 + 0.1 * 6.718019962310791
Epoch 560, val loss: 1.0712385177612305
Epoch 570, training loss: 0.6904672384262085 = 0.019238939508795738 + 0.1 * 6.712282657623291
Epoch 570, val loss: 1.081446886062622
Epoch 580, training loss: 0.6888150572776794 = 0.018242839723825455 + 0.1 * 6.705721855163574
Epoch 580, val loss: 1.0914335250854492
Epoch 590, training loss: 0.6877506375312805 = 0.017329208552837372 + 0.1 * 6.704214096069336
Epoch 590, val loss: 1.101096510887146
Epoch 600, training loss: 0.686752200126648 = 0.016487056389451027 + 0.1 * 6.702651500701904
Epoch 600, val loss: 1.1105273962020874
Epoch 610, training loss: 0.6852046251296997 = 0.015711287036538124 + 0.1 * 6.69493293762207
Epoch 610, val loss: 1.1196972131729126
Epoch 620, training loss: 0.685189962387085 = 0.014991100877523422 + 0.1 * 6.701988220214844
Epoch 620, val loss: 1.1286866664886475
Epoch 630, training loss: 0.6836014986038208 = 0.014322791248559952 + 0.1 * 6.692786693572998
Epoch 630, val loss: 1.1374820470809937
Epoch 640, training loss: 0.6819543838500977 = 0.013702758587896824 + 0.1 * 6.682516098022461
Epoch 640, val loss: 1.1460031270980835
Epoch 650, training loss: 0.6817979216575623 = 0.013126984238624573 + 0.1 * 6.686709403991699
Epoch 650, val loss: 1.1543631553649902
Epoch 660, training loss: 0.6799582242965698 = 0.012588463723659515 + 0.1 * 6.6736979484558105
Epoch 660, val loss: 1.162580132484436
Epoch 670, training loss: 0.6797876954078674 = 0.012085252441465855 + 0.1 * 6.6770243644714355
Epoch 670, val loss: 1.1706104278564453
Epoch 680, training loss: 0.6792860627174377 = 0.011615621857345104 + 0.1 * 6.676704406738281
Epoch 680, val loss: 1.1784473657608032
Epoch 690, training loss: 0.6771298050880432 = 0.011178365908563137 + 0.1 * 6.659514427185059
Epoch 690, val loss: 1.1860088109970093
Epoch 700, training loss: 0.6773602366447449 = 0.010765538550913334 + 0.1 * 6.665946960449219
Epoch 700, val loss: 1.1935479640960693
Epoch 710, training loss: 0.6755996942520142 = 0.010376659221947193 + 0.1 * 6.652230262756348
Epoch 710, val loss: 1.2008830308914185
Epoch 720, training loss: 0.6781767010688782 = 0.010010480880737305 + 0.1 * 6.681662082672119
Epoch 720, val loss: 1.2080503702163696
Epoch 730, training loss: 0.6757720112800598 = 0.009666573256254196 + 0.1 * 6.6610541343688965
Epoch 730, val loss: 1.2150626182556152
Epoch 740, training loss: 0.6747695207595825 = 0.009342293255031109 + 0.1 * 6.654272079467773
Epoch 740, val loss: 1.2219384908676147
Epoch 750, training loss: 0.6737548112869263 = 0.00903489999473095 + 0.1 * 6.6471991539001465
Epoch 750, val loss: 1.2287390232086182
Epoch 760, training loss: 0.673276960849762 = 0.008743900805711746 + 0.1 * 6.645330429077148
Epoch 760, val loss: 1.2353742122650146
Epoch 770, training loss: 0.6722584366798401 = 0.008469480089843273 + 0.1 * 6.637889862060547
Epoch 770, val loss: 1.2418534755706787
Epoch 780, training loss: 0.6734694242477417 = 0.008208706974983215 + 0.1 * 6.652607440948486
Epoch 780, val loss: 1.2482541799545288
Epoch 790, training loss: 0.671384871006012 = 0.007962580770254135 + 0.1 * 6.634222984313965
Epoch 790, val loss: 1.254402756690979
Epoch 800, training loss: 0.6696203947067261 = 0.0077290842309594154 + 0.1 * 6.618912696838379
Epoch 800, val loss: 1.2605352401733398
Epoch 810, training loss: 0.671074390411377 = 0.007505386136472225 + 0.1 * 6.635690212249756
Epoch 810, val loss: 1.266602873802185
Epoch 820, training loss: 0.6700320839881897 = 0.007293847389519215 + 0.1 * 6.627382755279541
Epoch 820, val loss: 1.2724502086639404
Epoch 830, training loss: 0.6696645617485046 = 0.00709218205884099 + 0.1 * 6.625723838806152
Epoch 830, val loss: 1.2781907320022583
Epoch 840, training loss: 0.6691467761993408 = 0.006900331936776638 + 0.1 * 6.622464656829834
Epoch 840, val loss: 1.283850908279419
Epoch 850, training loss: 0.6677964925765991 = 0.006716627161949873 + 0.1 * 6.6107988357543945
Epoch 850, val loss: 1.2894561290740967
Epoch 860, training loss: 0.667360782623291 = 0.006541119888424873 + 0.1 * 6.608196258544922
Epoch 860, val loss: 1.2949563264846802
Epoch 870, training loss: 0.6686847805976868 = 0.0063734715804457664 + 0.1 * 6.62311315536499
Epoch 870, val loss: 1.3003376722335815
Epoch 880, training loss: 0.6664597988128662 = 0.0062135676853358746 + 0.1 * 6.602462291717529
Epoch 880, val loss: 1.305554986000061
Epoch 890, training loss: 0.6674681901931763 = 0.006060315761715174 + 0.1 * 6.614078998565674
Epoch 890, val loss: 1.3107768297195435
Epoch 900, training loss: 0.6652782559394836 = 0.005913309752941132 + 0.1 * 6.593649387359619
Epoch 900, val loss: 1.3158973455429077
Epoch 910, training loss: 0.6655065417289734 = 0.005772932432591915 + 0.1 * 6.5973358154296875
Epoch 910, val loss: 1.3209073543548584
Epoch 920, training loss: 0.6649335026741028 = 0.005638116970658302 + 0.1 * 6.592953681945801
Epoch 920, val loss: 1.3258397579193115
Epoch 930, training loss: 0.6638895869255066 = 0.00550857512280345 + 0.1 * 6.583809852600098
Epoch 930, val loss: 1.3306854963302612
Epoch 940, training loss: 0.668138325214386 = 0.00538373738527298 + 0.1 * 6.6275458335876465
Epoch 940, val loss: 1.3354557752609253
Epoch 950, training loss: 0.664162814617157 = 0.005264422856271267 + 0.1 * 6.58898401260376
Epoch 950, val loss: 1.3401366472244263
Epoch 960, training loss: 0.6641631722450256 = 0.00515005411580205 + 0.1 * 6.590130805969238
Epoch 960, val loss: 1.3447322845458984
Epoch 970, training loss: 0.6626687049865723 = 0.005039540585130453 + 0.1 * 6.576291561126709
Epoch 970, val loss: 1.3492522239685059
Epoch 980, training loss: 0.6621824502944946 = 0.004933575168251991 + 0.1 * 6.572488307952881
Epoch 980, val loss: 1.3537545204162598
Epoch 990, training loss: 0.6626160740852356 = 0.0048304651863873005 + 0.1 * 6.577856063842773
Epoch 990, val loss: 1.3581278324127197
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 2.8018798828125 = 1.9421939849853516 + 0.1 * 8.596858978271484
Epoch 0, val loss: 1.9385499954223633
Epoch 10, training loss: 2.7912235260009766 = 1.9315431118011475 + 0.1 * 8.596803665161133
Epoch 10, val loss: 1.928259253501892
Epoch 20, training loss: 2.7777202129364014 = 1.9180693626403809 + 0.1 * 8.596508979797363
Epoch 20, val loss: 1.9146698713302612
Epoch 30, training loss: 2.758307456970215 = 1.8989061117172241 + 0.1 * 8.594014167785645
Epoch 30, val loss: 1.8948215246200562
Epoch 40, training loss: 2.728634834289551 = 1.8710740804672241 + 0.1 * 8.575608253479004
Epoch 40, val loss: 1.8660861253738403
Epoch 50, training loss: 2.6854538917541504 = 1.8351473808288574 + 0.1 * 8.50306510925293
Epoch 50, val loss: 1.8311882019042969
Epoch 60, training loss: 2.6309077739715576 = 1.8008923530578613 + 0.1 * 8.300153732299805
Epoch 60, val loss: 1.8018109798431396
Epoch 70, training loss: 2.585212230682373 = 1.7701771259307861 + 0.1 * 8.150351524353027
Epoch 70, val loss: 1.776254653930664
Epoch 80, training loss: 2.5168650150299072 = 1.7318050861358643 + 0.1 * 7.8505988121032715
Epoch 80, val loss: 1.7416391372680664
Epoch 90, training loss: 2.442207098007202 = 1.6828889846801758 + 0.1 * 7.5931806564331055
Epoch 90, val loss: 1.6993331909179688
Epoch 100, training loss: 2.355097532272339 = 1.6217814683914185 + 0.1 * 7.333160400390625
Epoch 100, val loss: 1.6498218774795532
Epoch 110, training loss: 2.266040802001953 = 1.5457197427749634 + 0.1 * 7.20320987701416
Epoch 110, val loss: 1.5868912935256958
Epoch 120, training loss: 2.1714606285095215 = 1.458380937576294 + 0.1 * 7.130796432495117
Epoch 120, val loss: 1.5184292793273926
Epoch 130, training loss: 2.0748820304870605 = 1.3661458492279053 + 0.1 * 7.087360858917236
Epoch 130, val loss: 1.4486970901489258
Epoch 140, training loss: 1.9783084392547607 = 1.2730140686035156 + 0.1 * 7.052943229675293
Epoch 140, val loss: 1.383571982383728
Epoch 150, training loss: 1.8844444751739502 = 1.1819393634796143 + 0.1 * 7.025051593780518
Epoch 150, val loss: 1.3228737115859985
Epoch 160, training loss: 1.7959638833999634 = 1.0955784320831299 + 0.1 * 7.003854274749756
Epoch 160, val loss: 1.2670906782150269
Epoch 170, training loss: 1.7149178981781006 = 1.0159844160079956 + 0.1 * 6.989335536956787
Epoch 170, val loss: 1.2171493768692017
Epoch 180, training loss: 1.6392074823379517 = 0.9419983625411987 + 0.1 * 6.972091197967529
Epoch 180, val loss: 1.170845627784729
Epoch 190, training loss: 1.5684140920639038 = 0.8726555705070496 + 0.1 * 6.957584857940674
Epoch 190, val loss: 1.1266014575958252
Epoch 200, training loss: 1.5022391080856323 = 0.807140588760376 + 0.1 * 6.950984954833984
Epoch 200, val loss: 1.0842325687408447
Epoch 210, training loss: 1.4393317699432373 = 0.7454793453216553 + 0.1 * 6.93852424621582
Epoch 210, val loss: 1.044424057006836
Epoch 220, training loss: 1.3800163269042969 = 0.6870372891426086 + 0.1 * 6.9297895431518555
Epoch 220, val loss: 1.0072957277297974
Epoch 230, training loss: 1.3244926929473877 = 0.6319522857666016 + 0.1 * 6.9254045486450195
Epoch 230, val loss: 0.973950982093811
Epoch 240, training loss: 1.272174596786499 = 0.5802899599075317 + 0.1 * 6.9188456535339355
Epoch 240, val loss: 0.9452794790267944
Epoch 250, training loss: 1.222954273223877 = 0.5316856503486633 + 0.1 * 6.912686347961426
Epoch 250, val loss: 0.9215347766876221
Epoch 260, training loss: 1.176872968673706 = 0.48596152663230896 + 0.1 * 6.909113883972168
Epoch 260, val loss: 0.9028909802436829
Epoch 270, training loss: 1.1340153217315674 = 0.4428976774215698 + 0.1 * 6.911177158355713
Epoch 270, val loss: 0.8890080451965332
Epoch 280, training loss: 1.0933561325073242 = 0.4028354287147522 + 0.1 * 6.90520715713501
Epoch 280, val loss: 0.8792306780815125
Epoch 290, training loss: 1.0548524856567383 = 0.365219384431839 + 0.1 * 6.896331310272217
Epoch 290, val loss: 0.8725571632385254
Epoch 300, training loss: 1.0188472270965576 = 0.32965266704559326 + 0.1 * 6.891944885253906
Epoch 300, val loss: 0.868683397769928
Epoch 310, training loss: 0.9849971532821655 = 0.2961577773094177 + 0.1 * 6.888393402099609
Epoch 310, val loss: 0.8671748042106628
Epoch 320, training loss: 0.9538909792900085 = 0.2651190161705017 + 0.1 * 6.887719631195068
Epoch 320, val loss: 0.8679806590080261
Epoch 330, training loss: 0.9247993230819702 = 0.23677316308021545 + 0.1 * 6.8802618980407715
Epoch 330, val loss: 0.8709969520568848
Epoch 340, training loss: 0.8985164165496826 = 0.21108470857143402 + 0.1 * 6.874316692352295
Epoch 340, val loss: 0.8761324286460876
Epoch 350, training loss: 0.8757373094558716 = 0.18804267048835754 + 0.1 * 6.876945972442627
Epoch 350, val loss: 0.8832746148109436
Epoch 360, training loss: 0.8544688820838928 = 0.16768115758895874 + 0.1 * 6.867877006530762
Epoch 360, val loss: 0.8921193480491638
Epoch 370, training loss: 0.8356552720069885 = 0.1497109979391098 + 0.1 * 6.859442710876465
Epoch 370, val loss: 0.9025507569313049
Epoch 380, training loss: 0.8200582265853882 = 0.13386103510856628 + 0.1 * 6.861971855163574
Epoch 380, val loss: 0.9144531488418579
Epoch 390, training loss: 0.8050767779350281 = 0.11994337290525436 + 0.1 * 6.8513336181640625
Epoch 390, val loss: 0.9277134537696838
Epoch 400, training loss: 0.7922062277793884 = 0.1076706200838089 + 0.1 * 6.845355987548828
Epoch 400, val loss: 0.9420022368431091
Epoch 410, training loss: 0.7804114818572998 = 0.09686759114265442 + 0.1 * 6.835439205169678
Epoch 410, val loss: 0.9573146104812622
Epoch 420, training loss: 0.7703856229782104 = 0.08734015375375748 + 0.1 * 6.830454349517822
Epoch 420, val loss: 0.9733949899673462
Epoch 430, training loss: 0.7624322175979614 = 0.07892504334449768 + 0.1 * 6.835072040557861
Epoch 430, val loss: 0.989971935749054
Epoch 440, training loss: 0.7532009482383728 = 0.07152169197797775 + 0.1 * 6.8167924880981445
Epoch 440, val loss: 1.0068703889846802
Epoch 450, training loss: 0.7460394501686096 = 0.06498175859451294 + 0.1 * 6.810576915740967
Epoch 450, val loss: 1.0240641832351685
Epoch 460, training loss: 0.7394572496414185 = 0.05920819938182831 + 0.1 * 6.802490234375
Epoch 460, val loss: 1.0411642789840698
Epoch 470, training loss: 0.7338745594024658 = 0.05410083010792732 + 0.1 * 6.7977375984191895
Epoch 470, val loss: 1.0582635402679443
Epoch 480, training loss: 0.7282506227493286 = 0.04957933723926544 + 0.1 * 6.786712646484375
Epoch 480, val loss: 1.075179934501648
Epoch 490, training loss: 0.7231889963150024 = 0.045568838715553284 + 0.1 * 6.7762017250061035
Epoch 490, val loss: 1.0919016599655151
Epoch 500, training loss: 0.7196401357650757 = 0.04199230670928955 + 0.1 * 6.776478290557861
Epoch 500, val loss: 1.1084083318710327
Epoch 510, training loss: 0.7142789959907532 = 0.03880300372838974 + 0.1 * 6.754760265350342
Epoch 510, val loss: 1.1245996952056885
Epoch 520, training loss: 0.7116888761520386 = 0.03595118969678879 + 0.1 * 6.757376670837402
Epoch 520, val loss: 1.1403993368148804
Epoch 530, training loss: 0.7088385224342346 = 0.03339328244328499 + 0.1 * 6.754452705383301
Epoch 530, val loss: 1.1559474468231201
Epoch 540, training loss: 0.704860270023346 = 0.03109472617506981 + 0.1 * 6.737655162811279
Epoch 540, val loss: 1.171042561531067
Epoch 550, training loss: 0.7011713981628418 = 0.02902204357087612 + 0.1 * 6.721493244171143
Epoch 550, val loss: 1.1858898401260376
Epoch 560, training loss: 0.7002399563789368 = 0.0271440502256155 + 0.1 * 6.730958938598633
Epoch 560, val loss: 1.200222373008728
Epoch 570, training loss: 0.6966429948806763 = 0.025447452440857887 + 0.1 * 6.711955547332764
Epoch 570, val loss: 1.2143847942352295
Epoch 580, training loss: 0.6942241191864014 = 0.023905562236905098 + 0.1 * 6.703185558319092
Epoch 580, val loss: 1.2280365228652954
Epoch 590, training loss: 0.6941104531288147 = 0.02249811962246895 + 0.1 * 6.716123104095459
Epoch 590, val loss: 1.2414746284484863
Epoch 600, training loss: 0.6909098029136658 = 0.021213695406913757 + 0.1 * 6.696960926055908
Epoch 600, val loss: 1.2544505596160889
Epoch 610, training loss: 0.6915413737297058 = 0.020038671791553497 + 0.1 * 6.71502685546875
Epoch 610, val loss: 1.2672109603881836
Epoch 620, training loss: 0.6871735453605652 = 0.01896430365741253 + 0.1 * 6.682092189788818
Epoch 620, val loss: 1.2794297933578491
Epoch 630, training loss: 0.6858987808227539 = 0.01797773689031601 + 0.1 * 6.679210662841797
Epoch 630, val loss: 1.2916109561920166
Epoch 640, training loss: 0.6844276785850525 = 0.01706593856215477 + 0.1 * 6.673617362976074
Epoch 640, val loss: 1.3033088445663452
Epoch 650, training loss: 0.683249831199646 = 0.01622462272644043 + 0.1 * 6.670251846313477
Epoch 650, val loss: 1.3147493600845337
Epoch 660, training loss: 0.6816749572753906 = 0.015447461046278477 + 0.1 * 6.662275314331055
Epoch 660, val loss: 1.3260726928710938
Epoch 670, training loss: 0.6805883646011353 = 0.014726663008332253 + 0.1 * 6.65861701965332
Epoch 670, val loss: 1.3368417024612427
Epoch 680, training loss: 0.6792405843734741 = 0.01406046748161316 + 0.1 * 6.651801109313965
Epoch 680, val loss: 1.3475579023361206
Epoch 690, training loss: 0.6781830191612244 = 0.013440857641398907 + 0.1 * 6.647421360015869
Epoch 690, val loss: 1.357938528060913
Epoch 700, training loss: 0.678365170955658 = 0.012863701209425926 + 0.1 * 6.655014514923096
Epoch 700, val loss: 1.3678274154663086
Epoch 710, training loss: 0.6754949688911438 = 0.012327305972576141 + 0.1 * 6.631676197052002
Epoch 710, val loss: 1.3778623342514038
Epoch 720, training loss: 0.6778100728988647 = 0.011825046502053738 + 0.1 * 6.659850120544434
Epoch 720, val loss: 1.387213945388794
Epoch 730, training loss: 0.6742211580276489 = 0.011355412192642689 + 0.1 * 6.628657341003418
Epoch 730, val loss: 1.3965997695922852
Epoch 740, training loss: 0.6736140251159668 = 0.010915383696556091 + 0.1 * 6.626986026763916
Epoch 740, val loss: 1.4058887958526611
Epoch 750, training loss: 0.672191321849823 = 0.010501687414944172 + 0.1 * 6.616896152496338
Epoch 750, val loss: 1.4146575927734375
Epoch 760, training loss: 0.6716580390930176 = 0.010113406926393509 + 0.1 * 6.615446090698242
Epoch 760, val loss: 1.4234846830368042
Epoch 770, training loss: 0.6713230013847351 = 0.009747758507728577 + 0.1 * 6.615752220153809
Epoch 770, val loss: 1.431785225868225
Epoch 780, training loss: 0.6705400347709656 = 0.00940378662198782 + 0.1 * 6.611362457275391
Epoch 780, val loss: 1.4401934146881104
Epoch 790, training loss: 0.670235276222229 = 0.009079192765057087 + 0.1 * 6.611560344696045
Epoch 790, val loss: 1.448193073272705
Epoch 800, training loss: 0.6683850884437561 = 0.00877366028726101 + 0.1 * 6.596114158630371
Epoch 800, val loss: 1.4560753107070923
Epoch 810, training loss: 0.6678214073181152 = 0.00848434679210186 + 0.1 * 6.59337043762207
Epoch 810, val loss: 1.4640392065048218
Epoch 820, training loss: 0.668337345123291 = 0.008209522813558578 + 0.1 * 6.601278305053711
Epoch 820, val loss: 1.4713624715805054
Epoch 830, training loss: 0.667951226234436 = 0.007951194420456886 + 0.1 * 6.600000381469727
Epoch 830, val loss: 1.4788237810134888
Epoch 840, training loss: 0.6669029593467712 = 0.007705703377723694 + 0.1 * 6.591972827911377
Epoch 840, val loss: 1.4862630367279053
Epoch 850, training loss: 0.6663600206375122 = 0.007472429424524307 + 0.1 * 6.588875770568848
Epoch 850, val loss: 1.4931895732879639
Epoch 860, training loss: 0.6649396419525146 = 0.00725137535482645 + 0.1 * 6.576882362365723
Epoch 860, val loss: 1.5002409219741821
Epoch 870, training loss: 0.665057897567749 = 0.007040579803287983 + 0.1 * 6.580173015594482
Epoch 870, val loss: 1.5070371627807617
Epoch 880, training loss: 0.6644671559333801 = 0.006839857902377844 + 0.1 * 6.576272487640381
Epoch 880, val loss: 1.5138260126113892
Epoch 890, training loss: 0.6638844013214111 = 0.006648643873631954 + 0.1 * 6.572357177734375
Epoch 890, val loss: 1.520512342453003
Epoch 900, training loss: 0.6632015109062195 = 0.006465660408139229 + 0.1 * 6.567358016967773
Epoch 900, val loss: 1.526774525642395
Epoch 910, training loss: 0.6627227663993835 = 0.006292065605521202 + 0.1 * 6.56430721282959
Epoch 910, val loss: 1.5329532623291016
Epoch 920, training loss: 0.6620755195617676 = 0.00612698495388031 + 0.1 * 6.559484958648682
Epoch 920, val loss: 1.539242148399353
Epoch 930, training loss: 0.6618436574935913 = 0.005968477111309767 + 0.1 * 6.558752059936523
Epoch 930, val loss: 1.5454463958740234
Epoch 940, training loss: 0.6631543040275574 = 0.005816669203341007 + 0.1 * 6.573376178741455
Epoch 940, val loss: 1.551322340965271
Epoch 950, training loss: 0.6609449982643127 = 0.005671681836247444 + 0.1 * 6.552732944488525
Epoch 950, val loss: 1.5569864511489868
Epoch 960, training loss: 0.6612412333488464 = 0.005533036310225725 + 0.1 * 6.557081699371338
Epoch 960, val loss: 1.5629109144210815
Epoch 970, training loss: 0.6607030034065247 = 0.005399817135185003 + 0.1 * 6.5530314445495605
Epoch 970, val loss: 1.568527102470398
Epoch 980, training loss: 0.6599291563034058 = 0.005272075533866882 + 0.1 * 6.546570301055908
Epoch 980, val loss: 1.5739399194717407
Epoch 990, training loss: 0.6597548723220825 = 0.0051497709937393665 + 0.1 * 6.546051025390625
Epoch 990, val loss: 1.5794930458068848
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8033737480231946
The final CL Acc:0.76790, 0.00349, The final GNN Acc:0.80619, 0.00293
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13172])
remove edge: torch.Size([2, 7882])
updated graph: torch.Size([2, 10498])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8136417865753174 = 1.953957200050354 + 0.1 * 8.596845626831055
Epoch 0, val loss: 1.945418119430542
Epoch 10, training loss: 2.803891181945801 = 1.9442138671875 + 0.1 * 8.596773147583008
Epoch 10, val loss: 1.9367187023162842
Epoch 20, training loss: 2.7921335697174072 = 1.9325034618377686 + 0.1 * 8.596301078796387
Epoch 20, val loss: 1.9259978532791138
Epoch 30, training loss: 2.7755396366119385 = 1.9163436889648438 + 0.1 * 8.591958999633789
Epoch 30, val loss: 1.910984754562378
Epoch 40, training loss: 2.748443126678467 = 1.8924812078475952 + 0.1 * 8.55961799621582
Epoch 40, val loss: 1.888832926750183
Epoch 50, training loss: 2.696866273880005 = 1.8584673404693604 + 0.1 * 8.383988380432129
Epoch 50, val loss: 1.8579483032226562
Epoch 60, training loss: 2.6338844299316406 = 1.817562222480774 + 0.1 * 8.163223266601562
Epoch 60, val loss: 1.821754813194275
Epoch 70, training loss: 2.560616970062256 = 1.7764387130737305 + 0.1 * 7.8417816162109375
Epoch 70, val loss: 1.7857674360275269
Epoch 80, training loss: 2.488295793533325 = 1.737749457359314 + 0.1 * 7.50546407699585
Epoch 80, val loss: 1.7506020069122314
Epoch 90, training loss: 2.4171552658081055 = 1.692974328994751 + 0.1 * 7.241810321807861
Epoch 90, val loss: 1.710986852645874
Epoch 100, training loss: 2.342353105545044 = 1.6342569589614868 + 0.1 * 7.080962181091309
Epoch 100, val loss: 1.6591256856918335
Epoch 110, training loss: 2.2578537464141846 = 1.5572028160095215 + 0.1 * 7.006509780883789
Epoch 110, val loss: 1.5901169776916504
Epoch 120, training loss: 2.1625750064849854 = 1.4647244215011597 + 0.1 * 6.978506088256836
Epoch 120, val loss: 1.510118842124939
Epoch 130, training loss: 2.061474084854126 = 1.3654429912567139 + 0.1 * 6.960309982299805
Epoch 130, val loss: 1.425581693649292
Epoch 140, training loss: 1.9609098434448242 = 1.266013741493225 + 0.1 * 6.94896125793457
Epoch 140, val loss: 1.3412562608718872
Epoch 150, training loss: 1.8635215759277344 = 1.1701769828796387 + 0.1 * 6.933444976806641
Epoch 150, val loss: 1.2618383169174194
Epoch 160, training loss: 1.7705756425857544 = 1.0784833431243896 + 0.1 * 6.920922756195068
Epoch 160, val loss: 1.1880892515182495
Epoch 170, training loss: 1.6820859909057617 = 0.9916250705718994 + 0.1 * 6.904608249664307
Epoch 170, val loss: 1.1206839084625244
Epoch 180, training loss: 1.5996290445327759 = 0.9104918241500854 + 0.1 * 6.891372203826904
Epoch 180, val loss: 1.059373378753662
Epoch 190, training loss: 1.5223636627197266 = 0.834704577922821 + 0.1 * 6.876590251922607
Epoch 190, val loss: 1.0028337240219116
Epoch 200, training loss: 1.4509028196334839 = 0.7652750015258789 + 0.1 * 6.856277942657471
Epoch 200, val loss: 0.9516868591308594
Epoch 210, training loss: 1.3865690231323242 = 0.7023060917854309 + 0.1 * 6.8426289558410645
Epoch 210, val loss: 0.9060347676277161
Epoch 220, training loss: 1.3287570476531982 = 0.6456619501113892 + 0.1 * 6.830950736999512
Epoch 220, val loss: 0.8666492104530334
Epoch 230, training loss: 1.2766448259353638 = 0.5940000414848328 + 0.1 * 6.826447486877441
Epoch 230, val loss: 0.8331325650215149
Epoch 240, training loss: 1.2277703285217285 = 0.5466669797897339 + 0.1 * 6.811032772064209
Epoch 240, val loss: 0.8052374720573425
Epoch 250, training loss: 1.1821866035461426 = 0.502661943435669 + 0.1 * 6.795246124267578
Epoch 250, val loss: 0.7818955183029175
Epoch 260, training loss: 1.1399511098861694 = 0.4615192115306854 + 0.1 * 6.784318923950195
Epoch 260, val loss: 0.7628104090690613
Epoch 270, training loss: 1.1015205383300781 = 0.4233369827270508 + 0.1 * 6.781836032867432
Epoch 270, val loss: 0.7476910352706909
Epoch 280, training loss: 1.0648853778839111 = 0.38813653588294983 + 0.1 * 6.767488479614258
Epoch 280, val loss: 0.7361685633659363
Epoch 290, training loss: 1.0306813716888428 = 0.3554151952266693 + 0.1 * 6.752662181854248
Epoch 290, val loss: 0.7276644110679626
Epoch 300, training loss: 0.9992201328277588 = 0.32487156987190247 + 0.1 * 6.743485450744629
Epoch 300, val loss: 0.7216442823410034
Epoch 310, training loss: 0.9712972044944763 = 0.29655182361602783 + 0.1 * 6.747453689575195
Epoch 310, val loss: 0.7177848815917969
Epoch 320, training loss: 0.9430637359619141 = 0.2703876197338104 + 0.1 * 6.7267608642578125
Epoch 320, val loss: 0.7158542275428772
Epoch 330, training loss: 0.919782817363739 = 0.2462128847837448 + 0.1 * 6.73569917678833
Epoch 330, val loss: 0.7157840132713318
Epoch 340, training loss: 0.8950767517089844 = 0.2242119312286377 + 0.1 * 6.708648204803467
Epoch 340, val loss: 0.717482328414917
Epoch 350, training loss: 0.8741977214813232 = 0.20421603322029114 + 0.1 * 6.699817180633545
Epoch 350, val loss: 0.7208609580993652
Epoch 360, training loss: 0.8563418388366699 = 0.18615566194057465 + 0.1 * 6.701861381530762
Epoch 360, val loss: 0.7258545160293579
Epoch 370, training loss: 0.8387405872344971 = 0.16988934576511383 + 0.1 * 6.688512325286865
Epoch 370, val loss: 0.7322344779968262
Epoch 380, training loss: 0.8236598968505859 = 0.15521395206451416 + 0.1 * 6.684459209442139
Epoch 380, val loss: 0.7398701906204224
Epoch 390, training loss: 0.8106380105018616 = 0.1420779824256897 + 0.1 * 6.685600280761719
Epoch 390, val loss: 0.7484217882156372
Epoch 400, training loss: 0.7978464961051941 = 0.13031458854675293 + 0.1 * 6.675318717956543
Epoch 400, val loss: 0.7578202486038208
Epoch 410, training loss: 0.7863049507141113 = 0.1197175532579422 + 0.1 * 6.665874004364014
Epoch 410, val loss: 0.7679097056388855
Epoch 420, training loss: 0.7762709856033325 = 0.11018438637256622 + 0.1 * 6.660865783691406
Epoch 420, val loss: 0.7784026265144348
Epoch 430, training loss: 0.7667901515960693 = 0.10159245133399963 + 0.1 * 6.651976585388184
Epoch 430, val loss: 0.7892586588859558
Epoch 440, training loss: 0.7590697407722473 = 0.09382700175046921 + 0.1 * 6.6524271965026855
Epoch 440, val loss: 0.8004273772239685
Epoch 450, training loss: 0.7510233521461487 = 0.08681237697601318 + 0.1 * 6.642109394073486
Epoch 450, val loss: 0.8116753101348877
Epoch 460, training loss: 0.7438108921051025 = 0.08045320957899094 + 0.1 * 6.633576393127441
Epoch 460, val loss: 0.8230462074279785
Epoch 470, training loss: 0.7393354773521423 = 0.07467864453792572 + 0.1 * 6.6465678215026855
Epoch 470, val loss: 0.8344444036483765
Epoch 480, training loss: 0.7326745390892029 = 0.06944620609283447 + 0.1 * 6.6322832107543945
Epoch 480, val loss: 0.84581458568573
Epoch 490, training loss: 0.7273282408714294 = 0.06468240171670914 + 0.1 * 6.626458168029785
Epoch 490, val loss: 0.8570817708969116
Epoch 500, training loss: 0.7228003740310669 = 0.060337312519550323 + 0.1 * 6.624630451202393
Epoch 500, val loss: 0.8683802485466003
Epoch 510, training loss: 0.7173833250999451 = 0.05636502057313919 + 0.1 * 6.610182762145996
Epoch 510, val loss: 0.8795498609542847
Epoch 520, training loss: 0.7148563861846924 = 0.05272865295410156 + 0.1 * 6.621277332305908
Epoch 520, val loss: 0.890648365020752
Epoch 530, training loss: 0.7099114060401917 = 0.04940187558531761 + 0.1 * 6.605094909667969
Epoch 530, val loss: 0.9016249775886536
Epoch 540, training loss: 0.7062521576881409 = 0.04635137692093849 + 0.1 * 6.599007606506348
Epoch 540, val loss: 0.9123438000679016
Epoch 550, training loss: 0.7029833197593689 = 0.043555255979299545 + 0.1 * 6.59428071975708
Epoch 550, val loss: 0.9230807423591614
Epoch 560, training loss: 0.7007895708084106 = 0.04098238795995712 + 0.1 * 6.598072052001953
Epoch 560, val loss: 0.933552086353302
Epoch 570, training loss: 0.6968928575515747 = 0.03861406445503235 + 0.1 * 6.58278751373291
Epoch 570, val loss: 0.9439263343811035
Epoch 580, training loss: 0.6951382160186768 = 0.03642921522259712 + 0.1 * 6.587090015411377
Epoch 580, val loss: 0.9541155695915222
Epoch 590, training loss: 0.6937962770462036 = 0.03441167250275612 + 0.1 * 6.593845844268799
Epoch 590, val loss: 0.9642181396484375
Epoch 600, training loss: 0.6906110048294067 = 0.0325486958026886 + 0.1 * 6.580623149871826
Epoch 600, val loss: 0.9740518927574158
Epoch 610, training loss: 0.6879836916923523 = 0.030826540663838387 + 0.1 * 6.571570873260498
Epoch 610, val loss: 0.9837971925735474
Epoch 620, training loss: 0.6874343156814575 = 0.029228832572698593 + 0.1 * 6.58205509185791
Epoch 620, val loss: 0.9933621883392334
Epoch 630, training loss: 0.6847890019416809 = 0.02774883806705475 + 0.1 * 6.570401668548584
Epoch 630, val loss: 1.0026979446411133
Epoch 640, training loss: 0.6856749653816223 = 0.02637467160820961 + 0.1 * 6.593002796173096
Epoch 640, val loss: 1.0118674039840698
Epoch 650, training loss: 0.6815622448921204 = 0.025100020691752434 + 0.1 * 6.564622402191162
Epoch 650, val loss: 1.0208983421325684
Epoch 660, training loss: 0.679317057132721 = 0.023914385586977005 + 0.1 * 6.5540266036987305
Epoch 660, val loss: 1.029750943183899
Epoch 670, training loss: 0.6789742112159729 = 0.022806452587246895 + 0.1 * 6.5616774559021
Epoch 670, val loss: 1.0385046005249023
Epoch 680, training loss: 0.6764979958534241 = 0.02177371457219124 + 0.1 * 6.547242641448975
Epoch 680, val loss: 1.0470476150512695
Epoch 690, training loss: 0.6762896180152893 = 0.0208083838224411 + 0.1 * 6.554811954498291
Epoch 690, val loss: 1.055461049079895
Epoch 700, training loss: 0.6751711964607239 = 0.01990644820034504 + 0.1 * 6.552647113800049
Epoch 700, val loss: 1.0636768341064453
Epoch 710, training loss: 0.6734227538108826 = 0.019062699750065804 + 0.1 * 6.543600559234619
Epoch 710, val loss: 1.0717815160751343
Epoch 720, training loss: 0.6731463074684143 = 0.01827135495841503 + 0.1 * 6.548749923706055
Epoch 720, val loss: 1.079671859741211
Epoch 730, training loss: 0.6707258224487305 = 0.017529472708702087 + 0.1 * 6.531963348388672
Epoch 730, val loss: 1.0874706506729126
Epoch 740, training loss: 0.6705787777900696 = 0.01683173142373562 + 0.1 * 6.537469863891602
Epoch 740, val loss: 1.0950583219528198
Epoch 750, training loss: 0.6696777939796448 = 0.016175614669919014 + 0.1 * 6.535021781921387
Epoch 750, val loss: 1.102506399154663
Epoch 760, training loss: 0.6685355305671692 = 0.015559662133455276 + 0.1 * 6.529758453369141
Epoch 760, val loss: 1.1099021434783936
Epoch 770, training loss: 0.66739422082901 = 0.01497854944318533 + 0.1 * 6.52415657043457
Epoch 770, val loss: 1.1169534921646118
Epoch 780, training loss: 0.6668366193771362 = 0.014432283118367195 + 0.1 * 6.524043083190918
Epoch 780, val loss: 1.124082326889038
Epoch 790, training loss: 0.6660267114639282 = 0.013915807008743286 + 0.1 * 6.521109104156494
Epoch 790, val loss: 1.1309301853179932
Epoch 800, training loss: 0.6645399928092957 = 0.013427856378257275 + 0.1 * 6.5111212730407715
Epoch 800, val loss: 1.1377968788146973
Epoch 810, training loss: 0.6667777299880981 = 0.012966264970600605 + 0.1 * 6.538114547729492
Epoch 810, val loss: 1.1443735361099243
Epoch 820, training loss: 0.6646773815155029 = 0.012528936378657818 + 0.1 * 6.521484375
Epoch 820, val loss: 1.150972604751587
Epoch 830, training loss: 0.6657121181488037 = 0.012114942073822021 + 0.1 * 6.535971641540527
Epoch 830, val loss: 1.1573433876037598
Epoch 840, training loss: 0.6628389954566956 = 0.011722711846232414 + 0.1 * 6.511163234710693
Epoch 840, val loss: 1.163610577583313
Epoch 850, training loss: 0.661509096622467 = 0.011350286193192005 + 0.1 * 6.501587867736816
Epoch 850, val loss: 1.1698012351989746
Epoch 860, training loss: 0.6614737510681152 = 0.010995871387422085 + 0.1 * 6.504778861999512
Epoch 860, val loss: 1.175838828086853
Epoch 870, training loss: 0.6622983813285828 = 0.010659466497600079 + 0.1 * 6.5163893699646
Epoch 870, val loss: 1.1817735433578491
Epoch 880, training loss: 0.6607266664505005 = 0.010339546017348766 + 0.1 * 6.503870964050293
Epoch 880, val loss: 1.1875908374786377
Epoch 890, training loss: 0.6592583060264587 = 0.010034812614321709 + 0.1 * 6.49223518371582
Epoch 890, val loss: 1.1933505535125732
Epoch 900, training loss: 0.6595874428749084 = 0.009744307957589626 + 0.1 * 6.498431205749512
Epoch 900, val loss: 1.198980450630188
Epoch 910, training loss: 0.6584182977676392 = 0.009467445313930511 + 0.1 * 6.489508628845215
Epoch 910, val loss: 1.204527497291565
Epoch 920, training loss: 0.6582765579223633 = 0.009202767163515091 + 0.1 * 6.490737438201904
Epoch 920, val loss: 1.2100499868392944
Epoch 930, training loss: 0.6585592031478882 = 0.008949336595833302 + 0.1 * 6.496098518371582
Epoch 930, val loss: 1.215354323387146
Epoch 940, training loss: 0.657843828201294 = 0.008707884699106216 + 0.1 * 6.491359233856201
Epoch 940, val loss: 1.2206207513809204
Epoch 950, training loss: 0.658891499042511 = 0.008476600982248783 + 0.1 * 6.504148483276367
Epoch 950, val loss: 1.2257930040359497
Epoch 960, training loss: 0.6561294198036194 = 0.008255891501903534 + 0.1 * 6.478734970092773
Epoch 960, val loss: 1.2308895587921143
Epoch 970, training loss: 0.6555652022361755 = 0.008044629357755184 + 0.1 * 6.475205898284912
Epoch 970, val loss: 1.2359517812728882
Epoch 980, training loss: 0.6564953327178955 = 0.00784135889261961 + 0.1 * 6.486539840698242
Epoch 980, val loss: 1.2408089637756348
Epoch 990, training loss: 0.6545495986938477 = 0.0076467678882181644 + 0.1 * 6.469028472900391
Epoch 990, val loss: 1.2456612586975098
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 2.807826042175293 = 1.948140025138855 + 0.1 * 8.596860885620117
Epoch 0, val loss: 1.951350450515747
Epoch 10, training loss: 2.7979302406311035 = 1.9382518529891968 + 0.1 * 8.596783638000488
Epoch 10, val loss: 1.942097544670105
Epoch 20, training loss: 2.785362482070923 = 1.9257251024246216 + 0.1 * 8.596373558044434
Epoch 20, val loss: 1.9299730062484741
Epoch 30, training loss: 2.7668821811676025 = 1.9076162576675415 + 0.1 * 8.592658996582031
Epoch 30, val loss: 1.9122282266616821
Epoch 40, training loss: 2.7366373538970947 = 1.8802231550216675 + 0.1 * 8.564141273498535
Epoch 40, val loss: 1.885611891746521
Epoch 50, training loss: 2.685349702835083 = 1.8419573307037354 + 0.1 * 8.433923721313477
Epoch 50, val loss: 1.8498409986495972
Epoch 60, training loss: 2.623490571975708 = 1.7991092205047607 + 0.1 * 8.243812561035156
Epoch 60, val loss: 1.811889410018921
Epoch 70, training loss: 2.5673890113830566 = 1.7595173120498657 + 0.1 * 8.078716278076172
Epoch 70, val loss: 1.7755703926086426
Epoch 80, training loss: 2.4927926063537598 = 1.7148302793502808 + 0.1 * 7.779622554779053
Epoch 80, val loss: 1.7313120365142822
Epoch 90, training loss: 2.410381555557251 = 1.6587255001068115 + 0.1 * 7.516561031341553
Epoch 90, val loss: 1.6802376508712769
Epoch 100, training loss: 2.3143415451049805 = 1.5899547338485718 + 0.1 * 7.243867874145508
Epoch 100, val loss: 1.6232261657714844
Epoch 110, training loss: 2.220782995223999 = 1.5079840421676636 + 0.1 * 7.127990245819092
Epoch 110, val loss: 1.5523709058761597
Epoch 120, training loss: 2.127842664718628 = 1.4217113256454468 + 0.1 * 7.061312675476074
Epoch 120, val loss: 1.479562520980835
Epoch 130, training loss: 2.0383682250976562 = 1.337598443031311 + 0.1 * 7.0076985359191895
Epoch 130, val loss: 1.4118226766586304
Epoch 140, training loss: 1.952852487564087 = 1.2557330131530762 + 0.1 * 6.971194744110107
Epoch 140, val loss: 1.3448597192764282
Epoch 150, training loss: 1.86919367313385 = 1.1750012636184692 + 0.1 * 6.941924095153809
Epoch 150, val loss: 1.278383493423462
Epoch 160, training loss: 1.7896358966827393 = 1.0975779294967651 + 0.1 * 6.9205803871154785
Epoch 160, val loss: 1.2158665657043457
Epoch 170, training loss: 1.7159886360168457 = 1.025715708732605 + 0.1 * 6.9027299880981445
Epoch 170, val loss: 1.1589806079864502
Epoch 180, training loss: 1.648510217666626 = 0.9595272541046143 + 0.1 * 6.889830112457275
Epoch 180, val loss: 1.1080530881881714
Epoch 190, training loss: 1.584641695022583 = 0.8968819379806519 + 0.1 * 6.877597808837891
Epoch 190, val loss: 1.060613989830017
Epoch 200, training loss: 1.5229105949401855 = 0.8357230424880981 + 0.1 * 6.871875286102295
Epoch 200, val loss: 1.0150395631790161
Epoch 210, training loss: 1.4604449272155762 = 0.7742632031440735 + 0.1 * 6.861816883087158
Epoch 210, val loss: 0.9693154692649841
Epoch 220, training loss: 1.3977632522583008 = 0.7121348977088928 + 0.1 * 6.856283664703369
Epoch 220, val loss: 0.9238573312759399
Epoch 230, training loss: 1.3360199928283691 = 0.6509630084037781 + 0.1 * 6.850569248199463
Epoch 230, val loss: 0.8809211850166321
Epoch 240, training loss: 1.2784695625305176 = 0.5932440161705017 + 0.1 * 6.852255344390869
Epoch 240, val loss: 0.8438103795051575
Epoch 250, training loss: 1.2244987487792969 = 0.5404825806617737 + 0.1 * 6.84016227722168
Epoch 250, val loss: 0.8142096996307373
Epoch 260, training loss: 1.1769415140151978 = 0.49289703369140625 + 0.1 * 6.840444564819336
Epoch 260, val loss: 0.7921721339225769
Epoch 270, training loss: 1.132622241973877 = 0.44993856549263 + 0.1 * 6.826836585998535
Epoch 270, val loss: 0.7762653231620789
Epoch 280, training loss: 1.0928070545196533 = 0.41065672039985657 + 0.1 * 6.821503639221191
Epoch 280, val loss: 0.7647005915641785
Epoch 290, training loss: 1.0566266775131226 = 0.3744146525859833 + 0.1 * 6.822120666503906
Epoch 290, val loss: 0.7559911012649536
Epoch 300, training loss: 1.0214568376541138 = 0.34076377749443054 + 0.1 * 6.806931018829346
Epoch 300, val loss: 0.7490293383598328
Epoch 310, training loss: 0.9893199801445007 = 0.3090696334838867 + 0.1 * 6.80250358581543
Epoch 310, val loss: 0.7433384656906128
Epoch 320, training loss: 0.9586131572723389 = 0.2791191041469574 + 0.1 * 6.79494047164917
Epoch 320, val loss: 0.7383918762207031
Epoch 330, training loss: 0.9290384650230408 = 0.2506505250930786 + 0.1 * 6.783879280090332
Epoch 330, val loss: 0.7339496612548828
Epoch 340, training loss: 0.9015140533447266 = 0.22363297641277313 + 0.1 * 6.778810501098633
Epoch 340, val loss: 0.7300352454185486
Epoch 350, training loss: 0.875350296497345 = 0.19832052290439606 + 0.1 * 6.770298004150391
Epoch 350, val loss: 0.7267614603042603
Epoch 360, training loss: 0.8517423272132874 = 0.17491425573825836 + 0.1 * 6.768280506134033
Epoch 360, val loss: 0.724439263343811
Epoch 370, training loss: 0.8320724368095398 = 0.15374810993671417 + 0.1 * 6.783243179321289
Epoch 370, val loss: 0.7232863903045654
Epoch 380, training loss: 0.8109778165817261 = 0.1351345181465149 + 0.1 * 6.758432865142822
Epoch 380, val loss: 0.7234651446342468
Epoch 390, training loss: 0.7933713793754578 = 0.11891291290521622 + 0.1 * 6.744584083557129
Epoch 390, val loss: 0.7250840663909912
Epoch 400, training loss: 0.7785817384719849 = 0.10489639639854431 + 0.1 * 6.736853122711182
Epoch 400, val loss: 0.7281352877616882
Epoch 410, training loss: 0.7680271863937378 = 0.09293513745069504 + 0.1 * 6.750920295715332
Epoch 410, val loss: 0.7324126958847046
Epoch 420, training loss: 0.7551182508468628 = 0.08277051150798798 + 0.1 * 6.723476886749268
Epoch 420, val loss: 0.7376636862754822
Epoch 430, training loss: 0.7456578612327576 = 0.07408477365970612 + 0.1 * 6.715730667114258
Epoch 430, val loss: 0.7438986897468567
Epoch 440, training loss: 0.7396512031555176 = 0.06667675077915192 + 0.1 * 6.7297444343566895
Epoch 440, val loss: 0.7507973313331604
Epoch 450, training loss: 0.730643093585968 = 0.06033927947282791 + 0.1 * 6.703038215637207
Epoch 450, val loss: 0.7582300305366516
Epoch 460, training loss: 0.7244423627853394 = 0.05486326292157173 + 0.1 * 6.695790767669678
Epoch 460, val loss: 0.7660747766494751
Epoch 470, training loss: 0.7212388515472412 = 0.05012083053588867 + 0.1 * 6.711180210113525
Epoch 470, val loss: 0.7740956544876099
Epoch 480, training loss: 0.7150974273681641 = 0.04600462317466736 + 0.1 * 6.6909284591674805
Epoch 480, val loss: 0.7822412252426147
Epoch 490, training loss: 0.7117310762405396 = 0.042390353977680206 + 0.1 * 6.69340705871582
Epoch 490, val loss: 0.7904525995254517
Epoch 500, training loss: 0.7071104645729065 = 0.03920217975974083 + 0.1 * 6.679082870483398
Epoch 500, val loss: 0.7985664010047913
Epoch 510, training loss: 0.7033846378326416 = 0.03636961802840233 + 0.1 * 6.670149803161621
Epoch 510, val loss: 0.806740939617157
Epoch 520, training loss: 0.7016176581382751 = 0.033836591988801956 + 0.1 * 6.6778106689453125
Epoch 520, val loss: 0.8147090673446655
Epoch 530, training loss: 0.6981465816497803 = 0.031575191766023636 + 0.1 * 6.665713787078857
Epoch 530, val loss: 0.8226147890090942
Epoch 540, training loss: 0.6954138875007629 = 0.02953949011862278 + 0.1 * 6.658743858337402
Epoch 540, val loss: 0.8303642868995667
Epoch 550, training loss: 0.6927328705787659 = 0.027699438855051994 + 0.1 * 6.650334358215332
Epoch 550, val loss: 0.8379830121994019
Epoch 560, training loss: 0.6903819441795349 = 0.026030229404568672 + 0.1 * 6.643517017364502
Epoch 560, val loss: 0.8454347252845764
Epoch 570, training loss: 0.6888521909713745 = 0.024514883756637573 + 0.1 * 6.643373012542725
Epoch 570, val loss: 0.8527870178222656
Epoch 580, training loss: 0.6865144968032837 = 0.02313285320997238 + 0.1 * 6.633816242218018
Epoch 580, val loss: 0.85993492603302
Epoch 590, training loss: 0.684969425201416 = 0.021867461502552032 + 0.1 * 6.631019592285156
Epoch 590, val loss: 0.8669242858886719
Epoch 600, training loss: 0.6849579811096191 = 0.02070743963122368 + 0.1 * 6.642505168914795
Epoch 600, val loss: 0.8737872838973999
Epoch 610, training loss: 0.6830934882164001 = 0.019646327942609787 + 0.1 * 6.634471416473389
Epoch 610, val loss: 0.8804227709770203
Epoch 620, training loss: 0.6802053451538086 = 0.018669886514544487 + 0.1 * 6.615354061126709
Epoch 620, val loss: 0.8869288563728333
Epoch 630, training loss: 0.6790009140968323 = 0.01776537485420704 + 0.1 * 6.6123552322387695
Epoch 630, val loss: 0.8933365345001221
Epoch 640, training loss: 0.6779109239578247 = 0.01692679524421692 + 0.1 * 6.609841346740723
Epoch 640, val loss: 0.8995598554611206
Epoch 650, training loss: 0.6777909398078918 = 0.01614910550415516 + 0.1 * 6.616418361663818
Epoch 650, val loss: 0.9056463837623596
Epoch 660, training loss: 0.6759381890296936 = 0.01542985625565052 + 0.1 * 6.605082988739014
Epoch 660, val loss: 0.9115414619445801
Epoch 670, training loss: 0.6750745177268982 = 0.014760199002921581 + 0.1 * 6.603143215179443
Epoch 670, val loss: 0.9173877239227295
Epoch 680, training loss: 0.6747745871543884 = 0.014135140925645828 + 0.1 * 6.6063947677612305
Epoch 680, val loss: 0.9231153726577759
Epoch 690, training loss: 0.6725836396217346 = 0.013551098294556141 + 0.1 * 6.590324878692627
Epoch 690, val loss: 0.9286046624183655
Epoch 700, training loss: 0.6728180646896362 = 0.013006120920181274 + 0.1 * 6.598119735717773
Epoch 700, val loss: 0.9341162443161011
Epoch 710, training loss: 0.671057403087616 = 0.01249499898403883 + 0.1 * 6.5856242179870605
Epoch 710, val loss: 0.9393872022628784
Epoch 720, training loss: 0.6716729998588562 = 0.012015711516141891 + 0.1 * 6.5965728759765625
Epoch 720, val loss: 0.9446535706520081
Epoch 730, training loss: 0.668965220451355 = 0.011565626598894596 + 0.1 * 6.573995590209961
Epoch 730, val loss: 0.9497389197349548
Epoch 740, training loss: 0.6701098680496216 = 0.011143108829855919 + 0.1 * 6.589667797088623
Epoch 740, val loss: 0.9547132849693298
Epoch 750, training loss: 0.6679478883743286 = 0.010745545849204063 + 0.1 * 6.572023391723633
Epoch 750, val loss: 0.9595959782600403
Epoch 760, training loss: 0.6665694117546082 = 0.010370305739343166 + 0.1 * 6.561990737915039
Epoch 760, val loss: 0.9644312858581543
Epoch 770, training loss: 0.6701362133026123 = 0.010015311650931835 + 0.1 * 6.6012091636657715
Epoch 770, val loss: 0.9692061543464661
Epoch 780, training loss: 0.6655334830284119 = 0.00968027301132679 + 0.1 * 6.558531761169434
Epoch 780, val loss: 0.9737683534622192
Epoch 790, training loss: 0.6681725978851318 = 0.009363730438053608 + 0.1 * 6.5880889892578125
Epoch 790, val loss: 0.978286623954773
Epoch 800, training loss: 0.6653026342391968 = 0.009063947945833206 + 0.1 * 6.562386512756348
Epoch 800, val loss: 0.9827381372451782
Epoch 810, training loss: 0.6639586687088013 = 0.008780015632510185 + 0.1 * 6.551786422729492
Epoch 810, val loss: 0.9871045351028442
Epoch 820, training loss: 0.6640157103538513 = 0.008509545587003231 + 0.1 * 6.5550618171691895
Epoch 820, val loss: 0.9913806915283203
Epoch 830, training loss: 0.6630100607872009 = 0.00825404655188322 + 0.1 * 6.547560214996338
Epoch 830, val loss: 0.99555903673172
Epoch 840, training loss: 0.6628300547599792 = 0.008010560646653175 + 0.1 * 6.548194885253906
Epoch 840, val loss: 0.9997369647026062
Epoch 850, training loss: 0.6624844074249268 = 0.007778581231832504 + 0.1 * 6.54705810546875
Epoch 850, val loss: 1.0037723779678345
Epoch 860, training loss: 0.6619644165039062 = 0.0075578391551971436 + 0.1 * 6.544065475463867
Epoch 860, val loss: 1.0077646970748901
Epoch 870, training loss: 0.6617406606674194 = 0.007347671780735254 + 0.1 * 6.5439300537109375
Epoch 870, val loss: 1.011722445487976
Epoch 880, training loss: 0.662384569644928 = 0.007145997602492571 + 0.1 * 6.552385330200195
Epoch 880, val loss: 1.0155659914016724
Epoch 890, training loss: 0.6605595946311951 = 0.006954459473490715 + 0.1 * 6.536051273345947
Epoch 890, val loss: 1.019343376159668
Epoch 900, training loss: 0.6595429182052612 = 0.006771896034479141 + 0.1 * 6.5277099609375
Epoch 900, val loss: 1.0230265855789185
Epoch 910, training loss: 0.6622805595397949 = 0.006596422288566828 + 0.1 * 6.556840896606445
Epoch 910, val loss: 1.0267283916473389
Epoch 920, training loss: 0.6588098406791687 = 0.006428608205169439 + 0.1 * 6.523812294006348
Epoch 920, val loss: 1.0302953720092773
Epoch 930, training loss: 0.6602983474731445 = 0.006267913617193699 + 0.1 * 6.540304183959961
Epoch 930, val loss: 1.0338255167007446
Epoch 940, training loss: 0.6596725583076477 = 0.006114311050623655 + 0.1 * 6.535582542419434
Epoch 940, val loss: 1.0373331308364868
Epoch 950, training loss: 0.6574499607086182 = 0.005966963246464729 + 0.1 * 6.514830112457275
Epoch 950, val loss: 1.0407451391220093
Epoch 960, training loss: 0.6586707234382629 = 0.005825292784720659 + 0.1 * 6.528454303741455
Epoch 960, val loss: 1.0441497564315796
Epoch 970, training loss: 0.657494068145752 = 0.005689212586730719 + 0.1 * 6.518048286437988
Epoch 970, val loss: 1.0474647283554077
Epoch 980, training loss: 0.6568027138710022 = 0.005558733828365803 + 0.1 * 6.512439727783203
Epoch 980, val loss: 1.0507183074951172
Epoch 990, training loss: 0.6584632396697998 = 0.005433508660644293 + 0.1 * 6.53029727935791
Epoch 990, val loss: 1.0539307594299316
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.8188915252685547 = 1.9592050313949585 + 0.1 * 8.596863746643066
Epoch 0, val loss: 1.9701069593429565
Epoch 10, training loss: 2.8094146251678467 = 1.9497343301773071 + 0.1 * 8.596802711486816
Epoch 10, val loss: 1.9603952169418335
Epoch 20, training loss: 2.797476291656494 = 1.937829613685608 + 0.1 * 8.596466064453125
Epoch 20, val loss: 1.9481158256530762
Epoch 30, training loss: 2.7798049449920654 = 1.9204806089401245 + 0.1 * 8.593242645263672
Epoch 30, val loss: 1.9301735162734985
Epoch 40, training loss: 2.750563144683838 = 1.893757939338684 + 0.1 * 8.568052291870117
Epoch 40, val loss: 1.902786135673523
Epoch 50, training loss: 2.7000980377197266 = 1.8551796674728394 + 0.1 * 8.449183464050293
Epoch 50, val loss: 1.864750862121582
Epoch 60, training loss: 2.631908416748047 = 1.809695839881897 + 0.1 * 8.222126960754395
Epoch 60, val loss: 1.8222371339797974
Epoch 70, training loss: 2.5613577365875244 = 1.7659422159194946 + 0.1 * 7.954155445098877
Epoch 70, val loss: 1.7823244333267212
Epoch 80, training loss: 2.479672908782959 = 1.720991611480713 + 0.1 * 7.586812973022461
Epoch 80, val loss: 1.7390342950820923
Epoch 90, training loss: 2.398141384124756 = 1.665389060974121 + 0.1 * 7.327523231506348
Epoch 90, val loss: 1.6861735582351685
Epoch 100, training loss: 2.3139092922210693 = 1.5933879613876343 + 0.1 * 7.20521354675293
Epoch 100, val loss: 1.6223644018173218
Epoch 110, training loss: 2.220529079437256 = 1.506300926208496 + 0.1 * 7.1422810554504395
Epoch 110, val loss: 1.5467784404754639
Epoch 120, training loss: 2.1215732097625732 = 1.4097250699996948 + 0.1 * 7.1184821128845215
Epoch 120, val loss: 1.4643120765686035
Epoch 130, training loss: 2.0209405422210693 = 1.3107203245162964 + 0.1 * 7.102201461791992
Epoch 130, val loss: 1.3827478885650635
Epoch 140, training loss: 1.9221675395965576 = 1.213470697402954 + 0.1 * 7.086967468261719
Epoch 140, val loss: 1.304976224899292
Epoch 150, training loss: 1.827219843864441 = 1.1205697059631348 + 0.1 * 7.066501140594482
Epoch 150, val loss: 1.2318191528320312
Epoch 160, training loss: 1.7372066974639893 = 1.0334231853485107 + 0.1 * 7.037835597991943
Epoch 160, val loss: 1.1643378734588623
Epoch 170, training loss: 1.6549584865570068 = 0.9538958072662354 + 0.1 * 7.010626792907715
Epoch 170, val loss: 1.1040138006210327
Epoch 180, training loss: 1.5803887844085693 = 0.8816102743148804 + 0.1 * 6.9877848625183105
Epoch 180, val loss: 1.0505326986312866
Epoch 190, training loss: 1.5114706754684448 = 0.8150336742401123 + 0.1 * 6.964369773864746
Epoch 190, val loss: 1.0026946067810059
Epoch 200, training loss: 1.447927474975586 = 0.7534841895103455 + 0.1 * 6.944432258605957
Epoch 200, val loss: 0.9600769877433777
Epoch 210, training loss: 1.388573169708252 = 0.6954609751701355 + 0.1 * 6.931121349334717
Epoch 210, val loss: 0.9213880300521851
Epoch 220, training loss: 1.331742763519287 = 0.6409534215927124 + 0.1 * 6.907893657684326
Epoch 220, val loss: 0.8862918019294739
Epoch 230, training loss: 1.2776470184326172 = 0.5887216925621033 + 0.1 * 6.88925313949585
Epoch 230, val loss: 0.8537416458129883
Epoch 240, training loss: 1.2262792587280273 = 0.5385754704475403 + 0.1 * 6.87703800201416
Epoch 240, val loss: 0.8236436247825623
Epoch 250, training loss: 1.1764570474624634 = 0.4904725253582001 + 0.1 * 6.859845161437988
Epoch 250, val loss: 0.7957795262336731
Epoch 260, training loss: 1.1296212673187256 = 0.44442230463027954 + 0.1 * 6.851989269256592
Epoch 260, val loss: 0.7702707052230835
Epoch 270, training loss: 1.0850679874420166 = 0.4010869264602661 + 0.1 * 6.839811325073242
Epoch 270, val loss: 0.7476125955581665
Epoch 280, training loss: 1.0433697700500488 = 0.3607468605041504 + 0.1 * 6.826228141784668
Epoch 280, val loss: 0.7282175421714783
Epoch 290, training loss: 1.0050090551376343 = 0.32378965616226196 + 0.1 * 6.812193870544434
Epoch 290, val loss: 0.7121910452842712
Epoch 300, training loss: 0.9716205596923828 = 0.28999650478363037 + 0.1 * 6.816240310668945
Epoch 300, val loss: 0.6994739174842834
Epoch 310, training loss: 0.9391796588897705 = 0.2594495713710785 + 0.1 * 6.797300338745117
Epoch 310, val loss: 0.6899608373641968
Epoch 320, training loss: 0.9121496677398682 = 0.231858491897583 + 0.1 * 6.802911758422852
Epoch 320, val loss: 0.6833076477050781
Epoch 330, training loss: 0.8852554559707642 = 0.20721407234668732 + 0.1 * 6.78041410446167
Epoch 330, val loss: 0.6791801452636719
Epoch 340, training loss: 0.8633876442909241 = 0.18520916998386383 + 0.1 * 6.781785011291504
Epoch 340, val loss: 0.6773891448974609
Epoch 350, training loss: 0.8432003259658813 = 0.16573327779769897 + 0.1 * 6.774670124053955
Epoch 350, val loss: 0.6777722835540771
Epoch 360, training loss: 0.8247302770614624 = 0.14855705201625824 + 0.1 * 6.76173210144043
Epoch 360, val loss: 0.6799389719963074
Epoch 370, training loss: 0.810539186000824 = 0.133444145321846 + 0.1 * 6.7709503173828125
Epoch 370, val loss: 0.6837552785873413
Epoch 380, training loss: 0.7952121496200562 = 0.12027362734079361 + 0.1 * 6.749384880065918
Epoch 380, val loss: 0.6889625191688538
Epoch 390, training loss: 0.783261239528656 = 0.10876751691102982 + 0.1 * 6.744936943054199
Epoch 390, val loss: 0.6952816843986511
Epoch 400, training loss: 0.7724947929382324 = 0.09867298603057861 + 0.1 * 6.738217830657959
Epoch 400, val loss: 0.7025507688522339
Epoch 410, training loss: 0.7622066736221313 = 0.08980049937963486 + 0.1 * 6.724061965942383
Epoch 410, val loss: 0.7105140686035156
Epoch 420, training loss: 0.7556079626083374 = 0.08196301758289337 + 0.1 * 6.736449241638184
Epoch 420, val loss: 0.7191590666770935
Epoch 430, training loss: 0.7480225563049316 = 0.07507339864969254 + 0.1 * 6.729491233825684
Epoch 430, val loss: 0.7282575964927673
Epoch 440, training loss: 0.7404488921165466 = 0.0689874142408371 + 0.1 * 6.714614391326904
Epoch 440, val loss: 0.7375243306159973
Epoch 450, training loss: 0.7337146997451782 = 0.06355330348014832 + 0.1 * 6.7016143798828125
Epoch 450, val loss: 0.7471125721931458
Epoch 460, training loss: 0.7291333675384521 = 0.05867893621325493 + 0.1 * 6.7045440673828125
Epoch 460, val loss: 0.7568721175193787
Epoch 470, training loss: 0.7242807745933533 = 0.05432843044400215 + 0.1 * 6.699522972106934
Epoch 470, val loss: 0.7667341232299805
Epoch 480, training loss: 0.7192281484603882 = 0.05044250190258026 + 0.1 * 6.687856197357178
Epoch 480, val loss: 0.7762937545776367
Epoch 490, training loss: 0.7152321338653564 = 0.046929992735385895 + 0.1 * 6.683021068572998
Epoch 490, val loss: 0.7859722971916199
Epoch 500, training loss: 0.7123885154724121 = 0.043752506375312805 + 0.1 * 6.6863603591918945
Epoch 500, val loss: 0.7956286668777466
Epoch 510, training loss: 0.7081279158592224 = 0.04087517410516739 + 0.1 * 6.672526836395264
Epoch 510, val loss: 0.8050256371498108
Epoch 520, training loss: 0.7077875733375549 = 0.03825835511088371 + 0.1 * 6.695291996002197
Epoch 520, val loss: 0.8143539428710938
Epoch 530, training loss: 0.7026911377906799 = 0.035883720964193344 + 0.1 * 6.668074131011963
Epoch 530, val loss: 0.8235180974006653
Epoch 540, training loss: 0.6992378830909729 = 0.03371680527925491 + 0.1 * 6.655210494995117
Epoch 540, val loss: 0.8324009776115417
Epoch 550, training loss: 0.7017537355422974 = 0.0317329540848732 + 0.1 * 6.700207233428955
Epoch 550, val loss: 0.8412133455276489
Epoch 560, training loss: 0.6960718631744385 = 0.0299249105155468 + 0.1 * 6.661468982696533
Epoch 560, val loss: 0.8498534560203552
Epoch 570, training loss: 0.6919962167739868 = 0.028267569839954376 + 0.1 * 6.637286186218262
Epoch 570, val loss: 0.8581179976463318
Epoch 580, training loss: 0.6895117163658142 = 0.026738299056887627 + 0.1 * 6.627734184265137
Epoch 580, val loss: 0.8664308786392212
Epoch 590, training loss: 0.6879349946975708 = 0.025324737653136253 + 0.1 * 6.626101970672607
Epoch 590, val loss: 0.874573290348053
Epoch 600, training loss: 0.6868765354156494 = 0.024025235325098038 + 0.1 * 6.628512382507324
Epoch 600, val loss: 0.8823910355567932
Epoch 610, training loss: 0.6871522068977356 = 0.022823281586170197 + 0.1 * 6.643289089202881
Epoch 610, val loss: 0.8901585936546326
Epoch 620, training loss: 0.6840813159942627 = 0.0217097420245409 + 0.1 * 6.623715877532959
Epoch 620, val loss: 0.8977139592170715
Epoch 630, training loss: 0.6819397211074829 = 0.0206782016903162 + 0.1 * 6.612614631652832
Epoch 630, val loss: 0.9050675630569458
Epoch 640, training loss: 0.6796852350234985 = 0.019717149436473846 + 0.1 * 6.599680423736572
Epoch 640, val loss: 0.912318229675293
Epoch 650, training loss: 0.6804117560386658 = 0.018823327496647835 + 0.1 * 6.615884304046631
Epoch 650, val loss: 0.9193156361579895
Epoch 660, training loss: 0.677030622959137 = 0.01799134537577629 + 0.1 * 6.590392589569092
Epoch 660, val loss: 0.9262275695800781
Epoch 670, training loss: 0.6757681369781494 = 0.017213333398103714 + 0.1 * 6.585547924041748
Epoch 670, val loss: 0.9329494833946228
Epoch 680, training loss: 0.6744247674942017 = 0.01648631878197193 + 0.1 * 6.5793843269348145
Epoch 680, val loss: 0.9395723938941956
Epoch 690, training loss: 0.6738917231559753 = 0.015805473551154137 + 0.1 * 6.580862045288086
Epoch 690, val loss: 0.945940375328064
Epoch 700, training loss: 0.6764169335365295 = 0.015167809091508389 + 0.1 * 6.612490653991699
Epoch 700, val loss: 0.9522718787193298
Epoch 710, training loss: 0.6716368198394775 = 0.014569265767931938 + 0.1 * 6.570675373077393
Epoch 710, val loss: 0.958518922328949
Epoch 720, training loss: 0.6705487370491028 = 0.014008513651788235 + 0.1 * 6.565402030944824
Epoch 720, val loss: 0.9644915461540222
Epoch 730, training loss: 0.6729896068572998 = 0.013478878885507584 + 0.1 * 6.595107078552246
Epoch 730, val loss: 0.970458984375
Epoch 740, training loss: 0.6684768199920654 = 0.012981987558305264 + 0.1 * 6.554947853088379
Epoch 740, val loss: 0.9762709140777588
Epoch 750, training loss: 0.6686804890632629 = 0.012513453140854836 + 0.1 * 6.561670303344727
Epoch 750, val loss: 0.9819308519363403
Epoch 760, training loss: 0.6664637327194214 = 0.012070085853338242 + 0.1 * 6.543936729431152
Epoch 760, val loss: 0.987532913684845
Epoch 770, training loss: 0.6687869429588318 = 0.011650281958281994 + 0.1 * 6.571366786956787
Epoch 770, val loss: 0.9930202960968018
Epoch 780, training loss: 0.6664915084838867 = 0.0112533587962389 + 0.1 * 6.55238151550293
Epoch 780, val loss: 0.9984487891197205
Epoch 790, training loss: 0.6654054522514343 = 0.0108778802677989 + 0.1 * 6.5452752113342285
Epoch 790, val loss: 1.0037745237350464
Epoch 800, training loss: 0.6637091636657715 = 0.010523488745093346 + 0.1 * 6.531856536865234
Epoch 800, val loss: 1.0089073181152344
Epoch 810, training loss: 0.6663424372673035 = 0.010186065919697285 + 0.1 * 6.561563491821289
Epoch 810, val loss: 1.0139892101287842
Epoch 820, training loss: 0.6628639101982117 = 0.009865332394838333 + 0.1 * 6.529985427856445
Epoch 820, val loss: 1.019030213356018
Epoch 830, training loss: 0.6645539999008179 = 0.009561746381223202 + 0.1 * 6.549922943115234
Epoch 830, val loss: 1.023942232131958
Epoch 840, training loss: 0.6620702743530273 = 0.009271431714296341 + 0.1 * 6.527988433837891
Epoch 840, val loss: 1.0287606716156006
Epoch 850, training loss: 0.6629105806350708 = 0.008996644988656044 + 0.1 * 6.5391387939453125
Epoch 850, val loss: 1.033549189567566
Epoch 860, training loss: 0.6612800359725952 = 0.008734391070902348 + 0.1 * 6.525455951690674
Epoch 860, val loss: 1.0381693840026855
Epoch 870, training loss: 0.6598173975944519 = 0.008484561927616596 + 0.1 * 6.513328552246094
Epoch 870, val loss: 1.0427013635635376
Epoch 880, training loss: 0.6620823740959167 = 0.008244520984590054 + 0.1 * 6.538378715515137
Epoch 880, val loss: 1.0472294092178345
Epoch 890, training loss: 0.6590246558189392 = 0.008017252199351788 + 0.1 * 6.510074138641357
Epoch 890, val loss: 1.051653265953064
Epoch 900, training loss: 0.6592930555343628 = 0.007799060549587011 + 0.1 * 6.514939785003662
Epoch 900, val loss: 1.055937647819519
Epoch 910, training loss: 0.6574368476867676 = 0.007590687833726406 + 0.1 * 6.4984612464904785
Epoch 910, val loss: 1.06026029586792
Epoch 920, training loss: 0.6578433513641357 = 0.007391542661935091 + 0.1 * 6.504517555236816
Epoch 920, val loss: 1.064431071281433
Epoch 930, training loss: 0.6568963527679443 = 0.007201090920716524 + 0.1 * 6.496952533721924
Epoch 930, val loss: 1.0685606002807617
Epoch 940, training loss: 0.6576911807060242 = 0.007018567528575659 + 0.1 * 6.506726264953613
Epoch 940, val loss: 1.0726014375686646
Epoch 950, training loss: 0.6568583846092224 = 0.006842662114650011 + 0.1 * 6.500156879425049
Epoch 950, val loss: 1.0765438079833984
Epoch 960, training loss: 0.6567772030830383 = 0.006674439646303654 + 0.1 * 6.5010271072387695
Epoch 960, val loss: 1.0805119276046753
Epoch 970, training loss: 0.6558265686035156 = 0.006512745283544064 + 0.1 * 6.493137836456299
Epoch 970, val loss: 1.084350347518921
Epoch 980, training loss: 0.6560325622558594 = 0.006357535254210234 + 0.1 * 6.4967498779296875
Epoch 980, val loss: 1.0881661176681519
Epoch 990, training loss: 0.6550495624542236 = 0.0062085362151265144 + 0.1 * 6.488409996032715
Epoch 990, val loss: 1.091871976852417
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8328940432261466
The final CL Acc:0.81975, 0.00462, The final GNN Acc:0.83641, 0.00263
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9546])
updated graph: torch.Size([2, 10614])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.824587821960449 = 1.9649053812026978 + 0.1 * 8.596823692321777
Epoch 0, val loss: 1.960983395576477
Epoch 10, training loss: 2.8135499954223633 = 1.9538774490356445 + 0.1 * 8.596724510192871
Epoch 10, val loss: 1.9494222402572632
Epoch 20, training loss: 2.800464630126953 = 1.9408495426177979 + 0.1 * 8.596151351928711
Epoch 20, val loss: 1.9354084730148315
Epoch 30, training loss: 2.7824764251708984 = 1.923344373703003 + 0.1 * 8.591320037841797
Epoch 30, val loss: 1.916284203529358
Epoch 40, training loss: 2.7541167736053467 = 1.8981229066848755 + 0.1 * 8.55993938446045
Epoch 40, val loss: 1.8887457847595215
Epoch 50, training loss: 2.7032790184020996 = 1.8637958765029907 + 0.1 * 8.394830703735352
Epoch 50, val loss: 1.8528207540512085
Epoch 60, training loss: 2.6399130821228027 = 1.8252060413360596 + 0.1 * 8.14707088470459
Epoch 60, val loss: 1.81538987159729
Epoch 70, training loss: 2.565742254257202 = 1.7909682989120483 + 0.1 * 7.747739315032959
Epoch 70, val loss: 1.7862293720245361
Epoch 80, training loss: 2.5006637573242188 = 1.7586456537246704 + 0.1 * 7.420180797576904
Epoch 80, val loss: 1.7596174478530884
Epoch 90, training loss: 2.4444196224212646 = 1.7206578254699707 + 0.1 * 7.237618446350098
Epoch 90, val loss: 1.7264467477798462
Epoch 100, training loss: 2.383352756500244 = 1.6712905168533325 + 0.1 * 7.120623588562012
Epoch 100, val loss: 1.6828994750976562
Epoch 110, training loss: 2.3103013038635254 = 1.6059892177581787 + 0.1 * 7.04311990737915
Epoch 110, val loss: 1.625253677368164
Epoch 120, training loss: 2.2243573665618896 = 1.5240484476089478 + 0.1 * 7.003089427947998
Epoch 120, val loss: 1.5550177097320557
Epoch 130, training loss: 2.1283106803894043 = 1.4308109283447266 + 0.1 * 6.974997043609619
Epoch 130, val loss: 1.4784234762191772
Epoch 140, training loss: 2.0278737545013428 = 1.3323543071746826 + 0.1 * 6.955193519592285
Epoch 140, val loss: 1.3990375995635986
Epoch 150, training loss: 1.9266526699066162 = 1.2335442304611206 + 0.1 * 6.931083679199219
Epoch 150, val loss: 1.321630835533142
Epoch 160, training loss: 1.8278610706329346 = 1.1368497610092163 + 0.1 * 6.9101128578186035
Epoch 160, val loss: 1.2472929954528809
Epoch 170, training loss: 1.7357494831085205 = 1.046364188194275 + 0.1 * 6.893852233886719
Epoch 170, val loss: 1.1791585683822632
Epoch 180, training loss: 1.651782751083374 = 0.9637277722358704 + 0.1 * 6.88054895401001
Epoch 180, val loss: 1.118028163909912
Epoch 190, training loss: 1.5758473873138428 = 0.8892863392829895 + 0.1 * 6.865610122680664
Epoch 190, val loss: 1.064068078994751
Epoch 200, training loss: 1.5083131790161133 = 0.8227764368057251 + 0.1 * 6.855367660522461
Epoch 200, val loss: 1.0173766613006592
Epoch 210, training loss: 1.4490996599197388 = 0.7647040486335754 + 0.1 * 6.843955993652344
Epoch 210, val loss: 0.9786076545715332
Epoch 220, training loss: 1.3967676162719727 = 0.7139056921005249 + 0.1 * 6.828619480133057
Epoch 220, val loss: 0.9470787048339844
Epoch 230, training loss: 1.3506710529327393 = 0.6685210466384888 + 0.1 * 6.821500301361084
Epoch 230, val loss: 0.92138671875
Epoch 240, training loss: 1.3082243204116821 = 0.6269698739051819 + 0.1 * 6.812544345855713
Epoch 240, val loss: 0.900377094745636
Epoch 250, training loss: 1.2669538259506226 = 0.5878614187240601 + 0.1 * 6.790924072265625
Epoch 250, val loss: 0.8825840950012207
Epoch 260, training loss: 1.22866690158844 = 0.5497135519981384 + 0.1 * 6.7895331382751465
Epoch 260, val loss: 0.8669012784957886
Epoch 270, training loss: 1.189481258392334 = 0.5119457244873047 + 0.1 * 6.775354385375977
Epoch 270, val loss: 0.8528501391410828
Epoch 280, training loss: 1.150730013847351 = 0.4743308126926422 + 0.1 * 6.763991355895996
Epoch 280, val loss: 0.8401514887809753
Epoch 290, training loss: 1.1127715110778809 = 0.43725600838661194 + 0.1 * 6.755155086517334
Epoch 290, val loss: 0.8292189836502075
Epoch 300, training loss: 1.0761035680770874 = 0.4014202952384949 + 0.1 * 6.746832847595215
Epoch 300, val loss: 0.8205122947692871
Epoch 310, training loss: 1.0420336723327637 = 0.3673900365829468 + 0.1 * 6.746435642242432
Epoch 310, val loss: 0.8141756653785706
Epoch 320, training loss: 1.0091586112976074 = 0.3358217775821686 + 0.1 * 6.733367919921875
Epoch 320, val loss: 0.810274600982666
Epoch 330, training loss: 0.9798206090927124 = 0.3068830072879791 + 0.1 * 6.72937536239624
Epoch 330, val loss: 0.8086517453193665
Epoch 340, training loss: 0.9526119828224182 = 0.28064900636672974 + 0.1 * 6.719629764556885
Epoch 340, val loss: 0.8088886737823486
Epoch 350, training loss: 0.9279793500900269 = 0.2568432092666626 + 0.1 * 6.711361408233643
Epoch 350, val loss: 0.8106012940406799
Epoch 360, training loss: 0.9060440063476562 = 0.23509277403354645 + 0.1 * 6.709512233734131
Epoch 360, val loss: 0.8134317994117737
Epoch 370, training loss: 0.8850495219230652 = 0.21510325372219086 + 0.1 * 6.699462413787842
Epoch 370, val loss: 0.8170758485794067
Epoch 380, training loss: 0.8653073310852051 = 0.19659101963043213 + 0.1 * 6.68716287612915
Epoch 380, val loss: 0.8212757706642151
Epoch 390, training loss: 0.8477985858917236 = 0.17930682003498077 + 0.1 * 6.68491792678833
Epoch 390, val loss: 0.8258340954780579
Epoch 400, training loss: 0.8311005234718323 = 0.16315104067325592 + 0.1 * 6.679494857788086
Epoch 400, val loss: 0.8306290507316589
Epoch 410, training loss: 0.8150626420974731 = 0.1480599194765091 + 0.1 * 6.670027256011963
Epoch 410, val loss: 0.835594654083252
Epoch 420, training loss: 0.7997902035713196 = 0.13402009010314941 + 0.1 * 6.657701015472412
Epoch 420, val loss: 0.8407615423202515
Epoch 430, training loss: 0.7867645621299744 = 0.12108033895492554 + 0.1 * 6.656842231750488
Epoch 430, val loss: 0.8460862040519714
Epoch 440, training loss: 0.7751404047012329 = 0.10930441319942474 + 0.1 * 6.658359527587891
Epoch 440, val loss: 0.8515792489051819
Epoch 450, training loss: 0.7629526257514954 = 0.0986923947930336 + 0.1 * 6.64260196685791
Epoch 450, val loss: 0.8573140501976013
Epoch 460, training loss: 0.7544811964035034 = 0.08918583393096924 + 0.1 * 6.652953624725342
Epoch 460, val loss: 0.863346517086029
Epoch 470, training loss: 0.7449687123298645 = 0.08076342195272446 + 0.1 * 6.642053127288818
Epoch 470, val loss: 0.8696873784065247
Epoch 480, training loss: 0.735473096370697 = 0.07332470268011093 + 0.1 * 6.62148380279541
Epoch 480, val loss: 0.8762266039848328
Epoch 490, training loss: 0.7295472621917725 = 0.06675577908754349 + 0.1 * 6.627914905548096
Epoch 490, val loss: 0.8830596208572388
Epoch 500, training loss: 0.7231708765029907 = 0.060983166098594666 + 0.1 * 6.6218767166137695
Epoch 500, val loss: 0.8899716734886169
Epoch 510, training loss: 0.716610848903656 = 0.05590460076928139 + 0.1 * 6.607062339782715
Epoch 510, val loss: 0.8970500826835632
Epoch 520, training loss: 0.7116540670394897 = 0.05140767619013786 + 0.1 * 6.602463722229004
Epoch 520, val loss: 0.9042823910713196
Epoch 530, training loss: 0.7079678773880005 = 0.047417763620615005 + 0.1 * 6.605501174926758
Epoch 530, val loss: 0.911561906337738
Epoch 540, training loss: 0.7033792734146118 = 0.04387373477220535 + 0.1 * 6.595055103302002
Epoch 540, val loss: 0.9188437461853027
Epoch 550, training loss: 0.6997339129447937 = 0.040712617337703705 + 0.1 * 6.590212821960449
Epoch 550, val loss: 0.9261543154716492
Epoch 560, training loss: 0.6974238753318787 = 0.037877973169088364 + 0.1 * 6.595458984375
Epoch 560, val loss: 0.9334378242492676
Epoch 570, training loss: 0.6943026781082153 = 0.035331401973962784 + 0.1 * 6.589712142944336
Epoch 570, val loss: 0.9406745433807373
Epoch 580, training loss: 0.6902624368667603 = 0.03303530067205429 + 0.1 * 6.572271347045898
Epoch 580, val loss: 0.9478049278259277
Epoch 590, training loss: 0.6882330179214478 = 0.03095644898712635 + 0.1 * 6.572765350341797
Epoch 590, val loss: 0.9549407958984375
Epoch 600, training loss: 0.6881422400474548 = 0.02906942181289196 + 0.1 * 6.590727806091309
Epoch 600, val loss: 0.9620175957679749
Epoch 610, training loss: 0.6834134459495544 = 0.027353985235095024 + 0.1 * 6.56059455871582
Epoch 610, val loss: 0.9689356088638306
Epoch 620, training loss: 0.6822417378425598 = 0.02578987181186676 + 0.1 * 6.564518451690674
Epoch 620, val loss: 0.9758023619651794
Epoch 630, training loss: 0.6797448396682739 = 0.024360155686736107 + 0.1 * 6.55384635925293
Epoch 630, val loss: 0.9825479984283447
Epoch 640, training loss: 0.6786887645721436 = 0.023050347343087196 + 0.1 * 6.5563836097717285
Epoch 640, val loss: 0.9891506433486938
Epoch 650, training loss: 0.6764808297157288 = 0.021845834329724312 + 0.1 * 6.546350002288818
Epoch 650, val loss: 0.9957315921783447
Epoch 660, training loss: 0.6759573817253113 = 0.020735980942845345 + 0.1 * 6.5522141456604
Epoch 660, val loss: 1.0022103786468506
Epoch 670, training loss: 0.6759423017501831 = 0.01971190609037876 + 0.1 * 6.56230354309082
Epoch 670, val loss: 1.008543610572815
Epoch 680, training loss: 0.6720825433731079 = 0.018768277019262314 + 0.1 * 6.533143043518066
Epoch 680, val loss: 1.0147143602371216
Epoch 690, training loss: 0.6709869503974915 = 0.017896411940455437 + 0.1 * 6.530905246734619
Epoch 690, val loss: 1.0208189487457275
Epoch 700, training loss: 0.6709190607070923 = 0.017085250467061996 + 0.1 * 6.538337707519531
Epoch 700, val loss: 1.0268254280090332
Epoch 710, training loss: 0.6695539355278015 = 0.016332397237420082 + 0.1 * 6.532215118408203
Epoch 710, val loss: 1.032678484916687
Epoch 720, training loss: 0.6680260896682739 = 0.015630675479769707 + 0.1 * 6.523953914642334
Epoch 720, val loss: 1.0384379625320435
Epoch 730, training loss: 0.6676738262176514 = 0.0149758355692029 + 0.1 * 6.526979923248291
Epoch 730, val loss: 1.044126033782959
Epoch 740, training loss: 0.6660447716712952 = 0.014365128241479397 + 0.1 * 6.516796112060547
Epoch 740, val loss: 1.0496900081634521
Epoch 750, training loss: 0.6668471097946167 = 0.01379354391247034 + 0.1 * 6.530535697937012
Epoch 750, val loss: 1.0551025867462158
Epoch 760, training loss: 0.6648713946342468 = 0.013258027844130993 + 0.1 * 6.5161333084106445
Epoch 760, val loss: 1.0604335069656372
Epoch 770, training loss: 0.663469135761261 = 0.01275655534118414 + 0.1 * 6.5071258544921875
Epoch 770, val loss: 1.0656988620758057
Epoch 780, training loss: 0.6661106944084167 = 0.012284358963370323 + 0.1 * 6.538263320922852
Epoch 780, val loss: 1.0708271265029907
Epoch 790, training loss: 0.6628655791282654 = 0.011840292252600193 + 0.1 * 6.510252952575684
Epoch 790, val loss: 1.0759291648864746
Epoch 800, training loss: 0.6614640355110168 = 0.011423015035688877 + 0.1 * 6.500410079956055
Epoch 800, val loss: 1.0808323621749878
Epoch 810, training loss: 0.6611450910568237 = 0.011028547771275043 + 0.1 * 6.501165390014648
Epoch 810, val loss: 1.085728645324707
Epoch 820, training loss: 0.6601665616035461 = 0.010656776838004589 + 0.1 * 6.495097637176514
Epoch 820, val loss: 1.0905262231826782
Epoch 830, training loss: 0.6616652607917786 = 0.010304570198059082 + 0.1 * 6.513607025146484
Epoch 830, val loss: 1.0952306985855103
Epoch 840, training loss: 0.6601808071136475 = 0.009970786049962044 + 0.1 * 6.502099990844727
Epoch 840, val loss: 1.0998685359954834
Epoch 850, training loss: 0.6590790152549744 = 0.00965577457100153 + 0.1 * 6.494232177734375
Epoch 850, val loss: 1.1043894290924072
Epoch 860, training loss: 0.658004641532898 = 0.009356233291327953 + 0.1 * 6.486483573913574
Epoch 860, val loss: 1.10886549949646
Epoch 870, training loss: 0.6580317616462708 = 0.009072831831872463 + 0.1 * 6.489589214324951
Epoch 870, val loss: 1.1132686138153076
Epoch 880, training loss: 0.6600870490074158 = 0.008803358301520348 + 0.1 * 6.512836456298828
Epoch 880, val loss: 1.1175994873046875
Epoch 890, training loss: 0.6591752171516418 = 0.008546605706214905 + 0.1 * 6.506286144256592
Epoch 890, val loss: 1.121782660484314
Epoch 900, training loss: 0.6564397811889648 = 0.00830333586782217 + 0.1 * 6.4813642501831055
Epoch 900, val loss: 1.1259465217590332
Epoch 910, training loss: 0.656029462814331 = 0.008071907795965672 + 0.1 * 6.4795756340026855
Epoch 910, val loss: 1.1300408840179443
Epoch 920, training loss: 0.6565768718719482 = 0.007849995978176594 + 0.1 * 6.487268447875977
Epoch 920, val loss: 1.1341029405593872
Epoch 930, training loss: 0.6565474271774292 = 0.007638609502464533 + 0.1 * 6.4890875816345215
Epoch 930, val loss: 1.1380839347839355
Epoch 940, training loss: 0.6545435190200806 = 0.0074366359040141106 + 0.1 * 6.471068859100342
Epoch 940, val loss: 1.1420079469680786
Epoch 950, training loss: 0.6550316214561462 = 0.0072432453744113445 + 0.1 * 6.477883815765381
Epoch 950, val loss: 1.1458715200424194
Epoch 960, training loss: 0.6539840698242188 = 0.0070586614310741425 + 0.1 * 6.469254016876221
Epoch 960, val loss: 1.1497119665145874
Epoch 970, training loss: 0.6545647382736206 = 0.006882117595523596 + 0.1 * 6.476825714111328
Epoch 970, val loss: 1.1534589529037476
Epoch 980, training loss: 0.6533941030502319 = 0.006712301168590784 + 0.1 * 6.466817855834961
Epoch 980, val loss: 1.1571242809295654
Epoch 990, training loss: 0.6540092825889587 = 0.006550227291882038 + 0.1 * 6.474590301513672
Epoch 990, val loss: 1.160709261894226
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 2.7945444583892822 = 1.9348639249801636 + 0.1 * 8.59680461883545
Epoch 0, val loss: 1.9385625123977661
Epoch 10, training loss: 2.7845757007598877 = 1.9249067306518555 + 0.1 * 8.596689224243164
Epoch 10, val loss: 1.927933931350708
Epoch 20, training loss: 2.7727303504943848 = 1.9131314754486084 + 0.1 * 8.595988273620605
Epoch 20, val loss: 1.914984107017517
Epoch 30, training loss: 2.7562458515167236 = 1.89719557762146 + 0.1 * 8.590502738952637
Epoch 30, val loss: 1.8972665071487427
Epoch 40, training loss: 2.7300286293029785 = 1.8743857145309448 + 0.1 * 8.556427955627441
Epoch 40, val loss: 1.8722370862960815
Epoch 50, training loss: 2.6840829849243164 = 1.8442730903625488 + 0.1 * 8.398099899291992
Epoch 50, val loss: 1.840796947479248
Epoch 60, training loss: 2.6335349082946777 = 1.8119291067123413 + 0.1 * 8.216058731079102
Epoch 60, val loss: 1.8091955184936523
Epoch 70, training loss: 2.569730758666992 = 1.7821335792541504 + 0.1 * 7.87597131729126
Epoch 70, val loss: 1.7812460660934448
Epoch 80, training loss: 2.493732452392578 = 1.7509921789169312 + 0.1 * 7.427402973175049
Epoch 80, val loss: 1.7532036304473877
Epoch 90, training loss: 2.428018093109131 = 1.713194727897644 + 0.1 * 7.148233890533447
Epoch 90, val loss: 1.7201122045516968
Epoch 100, training loss: 2.368985652923584 = 1.6613035202026367 + 0.1 * 7.076821804046631
Epoch 100, val loss: 1.6732079982757568
Epoch 110, training loss: 2.2957191467285156 = 1.5933983325958252 + 0.1 * 7.0232086181640625
Epoch 110, val loss: 1.6120755672454834
Epoch 120, training loss: 2.212409257888794 = 1.513864278793335 + 0.1 * 6.985449314117432
Epoch 120, val loss: 1.5439692735671997
Epoch 130, training loss: 2.1230428218841553 = 1.4273871183395386 + 0.1 * 6.95655632019043
Epoch 130, val loss: 1.4715254306793213
Epoch 140, training loss: 2.030640125274658 = 1.336862325668335 + 0.1 * 6.937777042388916
Epoch 140, val loss: 1.398417592048645
Epoch 150, training loss: 1.9366939067840576 = 1.2453497648239136 + 0.1 * 6.913440704345703
Epoch 150, val loss: 1.3261600732803345
Epoch 160, training loss: 1.8413846492767334 = 1.1519407033920288 + 0.1 * 6.894440174102783
Epoch 160, val loss: 1.2538648843765259
Epoch 170, training loss: 1.7462122440338135 = 1.058185338973999 + 0.1 * 6.880268573760986
Epoch 170, val loss: 1.1826262474060059
Epoch 180, training loss: 1.6535146236419678 = 0.9666225910186768 + 0.1 * 6.86892032623291
Epoch 180, val loss: 1.1142743825912476
Epoch 190, training loss: 1.5650221109390259 = 0.8793707489967346 + 0.1 * 6.856513500213623
Epoch 190, val loss: 1.0501525402069092
Epoch 200, training loss: 1.483156442642212 = 0.7986551523208618 + 0.1 * 6.845012664794922
Epoch 200, val loss: 0.9924682378768921
Epoch 210, training loss: 1.410524606704712 = 0.7261168956756592 + 0.1 * 6.844077110290527
Epoch 210, val loss: 0.9429384469985962
Epoch 220, training loss: 1.345999002456665 = 0.663289487361908 + 0.1 * 6.827095031738281
Epoch 220, val loss: 0.9031727313995361
Epoch 230, training loss: 1.2899272441864014 = 0.6085518598556519 + 0.1 * 6.813754081726074
Epoch 230, val loss: 0.8717467188835144
Epoch 240, training loss: 1.242384672164917 = 0.5610841512680054 + 0.1 * 6.813004493713379
Epoch 240, val loss: 0.8477296233177185
Epoch 250, training loss: 1.2005701065063477 = 0.5205847024917603 + 0.1 * 6.799853801727295
Epoch 250, val loss: 0.8300638198852539
Epoch 260, training loss: 1.1641110181808472 = 0.4854993522167206 + 0.1 * 6.786116123199463
Epoch 260, val loss: 0.8172203898429871
Epoch 270, training loss: 1.1341181993484497 = 0.454757422208786 + 0.1 * 6.793607711791992
Epoch 270, val loss: 0.8079201579093933
Epoch 280, training loss: 1.1051515340805054 = 0.42750900983810425 + 0.1 * 6.776424884796143
Epoch 280, val loss: 0.8012115955352783
Epoch 290, training loss: 1.0800329446792603 = 0.4024151861667633 + 0.1 * 6.776177883148193
Epoch 290, val loss: 0.7960593104362488
Epoch 300, training loss: 1.0538333654403687 = 0.3783808648586273 + 0.1 * 6.7545247077941895
Epoch 300, val loss: 0.7915121912956238
Epoch 310, training loss: 1.030348300933838 = 0.3540908098220825 + 0.1 * 6.762574672698975
Epoch 310, val loss: 0.7869372367858887
Epoch 320, training loss: 1.0035650730133057 = 0.32865992188453674 + 0.1 * 6.749051570892334
Epoch 320, val loss: 0.7817802429199219
Epoch 330, training loss: 0.9746807217597961 = 0.30152076482772827 + 0.1 * 6.7315993309021
Epoch 330, val loss: 0.7761633396148682
Epoch 340, training loss: 0.9454894065856934 = 0.27290019392967224 + 0.1 * 6.725891590118408
Epoch 340, val loss: 0.7709416151046753
Epoch 350, training loss: 0.9157122373580933 = 0.2439567893743515 + 0.1 * 6.717554092407227
Epoch 350, val loss: 0.767076849937439
Epoch 360, training loss: 0.8877670764923096 = 0.21603980660438538 + 0.1 * 6.717272758483887
Epoch 360, val loss: 0.765632152557373
Epoch 370, training loss: 0.8612370491027832 = 0.19049111008644104 + 0.1 * 6.707458972930908
Epoch 370, val loss: 0.7670882940292358
Epoch 380, training loss: 0.8388472199440002 = 0.16807270050048828 + 0.1 * 6.70774507522583
Epoch 380, val loss: 0.771085262298584
Epoch 390, training loss: 0.8174868822097778 = 0.1487066149711609 + 0.1 * 6.687802791595459
Epoch 390, val loss: 0.7770472764968872
Epoch 400, training loss: 0.8005317449569702 = 0.13199499249458313 + 0.1 * 6.685367107391357
Epoch 400, val loss: 0.7844822406768799
Epoch 410, training loss: 0.7857942581176758 = 0.11760565638542175 + 0.1 * 6.681886196136475
Epoch 410, val loss: 0.7929702997207642
Epoch 420, training loss: 0.7734379172325134 = 0.1051979809999466 + 0.1 * 6.682399272918701
Epoch 420, val loss: 0.8021155595779419
Epoch 430, training loss: 0.7608809471130371 = 0.09444213658571243 + 0.1 * 6.664388179779053
Epoch 430, val loss: 0.8116257190704346
Epoch 440, training loss: 0.7507198452949524 = 0.08505367487668991 + 0.1 * 6.656661510467529
Epoch 440, val loss: 0.821499764919281
Epoch 450, training loss: 0.7433570027351379 = 0.07686079293489456 + 0.1 * 6.664962291717529
Epoch 450, val loss: 0.8315032124519348
Epoch 460, training loss: 0.734229564666748 = 0.06970909237861633 + 0.1 * 6.645204544067383
Epoch 460, val loss: 0.8414413928985596
Epoch 470, training loss: 0.7282301187515259 = 0.06341049820184708 + 0.1 * 6.648195743560791
Epoch 470, val loss: 0.8513733744621277
Epoch 480, training loss: 0.7225704789161682 = 0.057868633419275284 + 0.1 * 6.647017955780029
Epoch 480, val loss: 0.8611341118812561
Epoch 490, training loss: 0.7160550951957703 = 0.05297818034887314 + 0.1 * 6.6307692527771
Epoch 490, val loss: 0.8707602024078369
Epoch 500, training loss: 0.7137561440467834 = 0.04863893240690231 + 0.1 * 6.651171684265137
Epoch 500, val loss: 0.8802022933959961
Epoch 510, training loss: 0.7069216370582581 = 0.044798966497182846 + 0.1 * 6.6212263107299805
Epoch 510, val loss: 0.8894211649894714
Epoch 520, training loss: 0.7038201689720154 = 0.0413769856095314 + 0.1 * 6.624431610107422
Epoch 520, val loss: 0.8984525799751282
Epoch 530, training loss: 0.6991444826126099 = 0.03832253813743591 + 0.1 * 6.608219623565674
Epoch 530, val loss: 0.9072641730308533
Epoch 540, training loss: 0.6974946856498718 = 0.03558705374598503 + 0.1 * 6.619076251983643
Epoch 540, val loss: 0.9159078001976013
Epoch 550, training loss: 0.6945749521255493 = 0.03313533961772919 + 0.1 * 6.614396095275879
Epoch 550, val loss: 0.9243110418319702
Epoch 560, training loss: 0.6909945607185364 = 0.030934473499655724 + 0.1 * 6.600600719451904
Epoch 560, val loss: 0.9325076937675476
Epoch 570, training loss: 0.688368558883667 = 0.028943635523319244 + 0.1 * 6.5942487716674805
Epoch 570, val loss: 0.9405640959739685
Epoch 580, training loss: 0.6881698966026306 = 0.027139170095324516 + 0.1 * 6.610307216644287
Epoch 580, val loss: 0.94843989610672
Epoch 590, training loss: 0.6853410601615906 = 0.025507472455501556 + 0.1 * 6.5983357429504395
Epoch 590, val loss: 0.9560528993606567
Epoch 600, training loss: 0.6827362179756165 = 0.024024881422519684 + 0.1 * 6.587112903594971
Epoch 600, val loss: 0.9635632634162903
Epoch 610, training loss: 0.680755078792572 = 0.022675326094031334 + 0.1 * 6.58079719543457
Epoch 610, val loss: 0.9708011150360107
Epoch 620, training loss: 0.6823081374168396 = 0.021437890827655792 + 0.1 * 6.608702659606934
Epoch 620, val loss: 0.9779559969902039
Epoch 630, training loss: 0.6782817840576172 = 0.020305007696151733 + 0.1 * 6.57976770401001
Epoch 630, val loss: 0.9848793148994446
Epoch 640, training loss: 0.6770663261413574 = 0.019264953210949898 + 0.1 * 6.578013896942139
Epoch 640, val loss: 0.9916173815727234
Epoch 650, training loss: 0.6761626601219177 = 0.018304115161299706 + 0.1 * 6.578585624694824
Epoch 650, val loss: 0.9983190298080444
Epoch 660, training loss: 0.6736963391304016 = 0.017420949414372444 + 0.1 * 6.562753677368164
Epoch 660, val loss: 1.0047153234481812
Epoch 670, training loss: 0.6732077598571777 = 0.0166020467877388 + 0.1 * 6.566057205200195
Epoch 670, val loss: 1.0110315084457397
Epoch 680, training loss: 0.6724437475204468 = 0.015841500833630562 + 0.1 * 6.5660223960876465
Epoch 680, val loss: 1.0172637701034546
Epoch 690, training loss: 0.6707977056503296 = 0.01513854879885912 + 0.1 * 6.556591510772705
Epoch 690, val loss: 1.023199200630188
Epoch 700, training loss: 0.6702337265014648 = 0.01448291726410389 + 0.1 * 6.5575079917907715
Epoch 700, val loss: 1.0290988683700562
Epoch 710, training loss: 0.6718888878822327 = 0.013871010392904282 + 0.1 * 6.580178737640381
Epoch 710, val loss: 1.034907579421997
Epoch 720, training loss: 0.6690745949745178 = 0.013301328755915165 + 0.1 * 6.557732582092285
Epoch 720, val loss: 1.0405757427215576
Epoch 730, training loss: 0.6677237153053284 = 0.012771230190992355 + 0.1 * 6.549524784088135
Epoch 730, val loss: 1.0460504293441772
Epoch 740, training loss: 0.6679821610450745 = 0.012273618951439857 + 0.1 * 6.5570855140686035
Epoch 740, val loss: 1.0514582395553589
Epoch 750, training loss: 0.6669379472732544 = 0.011806913651525974 + 0.1 * 6.5513105392456055
Epoch 750, val loss: 1.0567214488983154
Epoch 760, training loss: 0.6656069755554199 = 0.01136909332126379 + 0.1 * 6.5423784255981445
Epoch 760, val loss: 1.0618484020233154
Epoch 770, training loss: 0.6648870706558228 = 0.010956447571516037 + 0.1 * 6.539306163787842
Epoch 770, val loss: 1.0669695138931274
Epoch 780, training loss: 0.6648712754249573 = 0.010569201782345772 + 0.1 * 6.543020725250244
Epoch 780, val loss: 1.071886420249939
Epoch 790, training loss: 0.6651946306228638 = 0.010203137062489986 + 0.1 * 6.549914836883545
Epoch 790, val loss: 1.0768002271652222
Epoch 800, training loss: 0.6635018587112427 = 0.00985912699252367 + 0.1 * 6.536427021026611
Epoch 800, val loss: 1.0815157890319824
Epoch 810, training loss: 0.6639891266822815 = 0.009533743374049664 + 0.1 * 6.544553756713867
Epoch 810, val loss: 1.0861597061157227
Epoch 820, training loss: 0.6619870662689209 = 0.00922446884214878 + 0.1 * 6.527625560760498
Epoch 820, val loss: 1.0908786058425903
Epoch 830, training loss: 0.6610543131828308 = 0.00893372017890215 + 0.1 * 6.521205902099609
Epoch 830, val loss: 1.0952717065811157
Epoch 840, training loss: 0.6607254147529602 = 0.008656611666083336 + 0.1 * 6.520688056945801
Epoch 840, val loss: 1.0997296571731567
Epoch 850, training loss: 0.6603721380233765 = 0.008394168689846992 + 0.1 * 6.519779205322266
Epoch 850, val loss: 1.1040655374526978
Epoch 860, training loss: 0.6607187390327454 = 0.008144518360495567 + 0.1 * 6.525742530822754
Epoch 860, val loss: 1.1082994937896729
Epoch 870, training loss: 0.6597514748573303 = 0.007907743565738201 + 0.1 * 6.518436908721924
Epoch 870, val loss: 1.1124310493469238
Epoch 880, training loss: 0.6586127877235413 = 0.007682041265070438 + 0.1 * 6.509307384490967
Epoch 880, val loss: 1.1165868043899536
Epoch 890, training loss: 0.6578516960144043 = 0.007467673625797033 + 0.1 * 6.50383996963501
Epoch 890, val loss: 1.120579481124878
Epoch 900, training loss: 0.6582056283950806 = 0.007262926083058119 + 0.1 * 6.509426593780518
Epoch 900, val loss: 1.1245096921920776
Epoch 910, training loss: 0.6584398746490479 = 0.007066880352795124 + 0.1 * 6.513729572296143
Epoch 910, val loss: 1.1284797191619873
Epoch 920, training loss: 0.6591342687606812 = 0.006879778578877449 + 0.1 * 6.5225443840026855
Epoch 920, val loss: 1.132399559020996
Epoch 930, training loss: 0.6565611362457275 = 0.006701722741127014 + 0.1 * 6.498594284057617
Epoch 930, val loss: 1.1361600160598755
Epoch 940, training loss: 0.6562023162841797 = 0.006531351245939732 + 0.1 * 6.49670934677124
Epoch 940, val loss: 1.1398497819900513
Epoch 950, training loss: 0.6562655568122864 = 0.006367549765855074 + 0.1 * 6.4989800453186035
Epoch 950, val loss: 1.1435720920562744
Epoch 960, training loss: 0.6571985483169556 = 0.006210969761013985 + 0.1 * 6.509875297546387
Epoch 960, val loss: 1.1471996307373047
Epoch 970, training loss: 0.655275821685791 = 0.006061529740691185 + 0.1 * 6.492142677307129
Epoch 970, val loss: 1.1507549285888672
Epoch 980, training loss: 0.6553760170936584 = 0.005917649250477552 + 0.1 * 6.4945831298828125
Epoch 980, val loss: 1.1542309522628784
Epoch 990, training loss: 0.6542227864265442 = 0.005778711289167404 + 0.1 * 6.484440326690674
Epoch 990, val loss: 1.1578047275543213
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.7995567321777344 = 1.939870834350586 + 0.1 * 8.5968599319458
Epoch 0, val loss: 1.9313691854476929
Epoch 10, training loss: 2.7896337509155273 = 1.9299533367156982 + 0.1 * 8.59680461883545
Epoch 10, val loss: 1.9224213361740112
Epoch 20, training loss: 2.7774035930633545 = 1.9177489280700684 + 0.1 * 8.59654712677002
Epoch 20, val loss: 1.9110909700393677
Epoch 30, training loss: 2.7601675987243652 = 1.900704026222229 + 0.1 * 8.594635963439941
Epoch 30, val loss: 1.8949589729309082
Epoch 40, training loss: 2.733626127243042 = 1.8758790493011475 + 0.1 * 8.577470779418945
Epoch 40, val loss: 1.871652603149414
Epoch 50, training loss: 2.6897716522216797 = 1.8424848318099976 + 0.1 * 8.472868919372559
Epoch 50, val loss: 1.8414705991744995
Epoch 60, training loss: 2.633491277694702 = 1.8078649044036865 + 0.1 * 8.25626277923584
Epoch 60, val loss: 1.8113504648208618
Epoch 70, training loss: 2.575258493423462 = 1.7753441333770752 + 0.1 * 7.999143600463867
Epoch 70, val loss: 1.7804620265960693
Epoch 80, training loss: 2.4997339248657227 = 1.7378240823745728 + 0.1 * 7.619098663330078
Epoch 80, val loss: 1.7444686889648438
Epoch 90, training loss: 2.4250969886779785 = 1.6918727159500122 + 0.1 * 7.332242965698242
Epoch 90, val loss: 1.7056983709335327
Epoch 100, training loss: 2.3494980335235596 = 1.632662057876587 + 0.1 * 7.168359756469727
Epoch 100, val loss: 1.655821681022644
Epoch 110, training loss: 2.266359567642212 = 1.5591415166854858 + 0.1 * 7.072180271148682
Epoch 110, val loss: 1.591531753540039
Epoch 120, training loss: 2.180591344833374 = 1.4798407554626465 + 0.1 * 7.007506370544434
Epoch 120, val loss: 1.5275846719741821
Epoch 130, training loss: 2.0970258712768555 = 1.399720549583435 + 0.1 * 6.973052978515625
Epoch 130, val loss: 1.464553713798523
Epoch 140, training loss: 2.0155415534973145 = 1.3203288316726685 + 0.1 * 6.9521260261535645
Epoch 140, val loss: 1.401383638381958
Epoch 150, training loss: 1.9354690313339233 = 1.2418664693832397 + 0.1 * 6.936025619506836
Epoch 150, val loss: 1.3410454988479614
Epoch 160, training loss: 1.8583977222442627 = 1.166751742362976 + 0.1 * 6.9164605140686035
Epoch 160, val loss: 1.2857096195220947
Epoch 170, training loss: 1.7853974103927612 = 1.0955713987350464 + 0.1 * 6.898260116577148
Epoch 170, val loss: 1.2354272603988647
Epoch 180, training loss: 1.7177627086639404 = 1.0295311212539673 + 0.1 * 6.8823161125183105
Epoch 180, val loss: 1.1914591789245605
Epoch 190, training loss: 1.656346321105957 = 0.9691035747528076 + 0.1 * 6.872427940368652
Epoch 190, val loss: 1.1534100770950317
Epoch 200, training loss: 1.598656415939331 = 0.913414716720581 + 0.1 * 6.852416515350342
Epoch 200, val loss: 1.120398998260498
Epoch 210, training loss: 1.5454046726226807 = 0.8610674738883972 + 0.1 * 6.843371868133545
Epoch 210, val loss: 1.091431736946106
Epoch 220, training loss: 1.4952484369277954 = 0.8119031190872192 + 0.1 * 6.833453178405762
Epoch 220, val loss: 1.06638503074646
Epoch 230, training loss: 1.4471898078918457 = 0.7645186185836792 + 0.1 * 6.826711177825928
Epoch 230, val loss: 1.0439528226852417
Epoch 240, training loss: 1.3995450735092163 = 0.7184954285621643 + 0.1 * 6.8104963302612305
Epoch 240, val loss: 1.023738980293274
Epoch 250, training loss: 1.3532507419586182 = 0.6728324890136719 + 0.1 * 6.8041815757751465
Epoch 250, val loss: 1.004794955253601
Epoch 260, training loss: 1.3062803745269775 = 0.6272062659263611 + 0.1 * 6.790741443634033
Epoch 260, val loss: 0.9866603016853333
Epoch 270, training loss: 1.2595933675765991 = 0.581257700920105 + 0.1 * 6.783356666564941
Epoch 270, val loss: 0.9689853191375732
Epoch 280, training loss: 1.2141358852386475 = 0.5351284742355347 + 0.1 * 6.790074825286865
Epoch 280, val loss: 0.9520565271377563
Epoch 290, training loss: 1.1671562194824219 = 0.48972922563552856 + 0.1 * 6.774270534515381
Epoch 290, val loss: 0.9369038939476013
Epoch 300, training loss: 1.1221414804458618 = 0.44548162817955017 + 0.1 * 6.766598701477051
Epoch 300, val loss: 0.9243848323822021
Epoch 310, training loss: 1.0788984298706055 = 0.4031520485877991 + 0.1 * 6.757463455200195
Epoch 310, val loss: 0.915851354598999
Epoch 320, training loss: 1.0403043031692505 = 0.36361292004585266 + 0.1 * 6.766913890838623
Epoch 320, val loss: 0.9118961691856384
Epoch 330, training loss: 1.0021352767944336 = 0.3274309039115906 + 0.1 * 6.747043609619141
Epoch 330, val loss: 0.9127315282821655
Epoch 340, training loss: 0.9683635234832764 = 0.29440176486968994 + 0.1 * 6.739617347717285
Epoch 340, val loss: 0.9180814027786255
Epoch 350, training loss: 0.9393422603607178 = 0.2645089626312256 + 0.1 * 6.748332977294922
Epoch 350, val loss: 0.9271387457847595
Epoch 360, training loss: 0.9105055332183838 = 0.2376108467578888 + 0.1 * 6.728946208953857
Epoch 360, val loss: 0.9390177130699158
Epoch 370, training loss: 0.8887474536895752 = 0.21344585716724396 + 0.1 * 6.753015995025635
Epoch 370, val loss: 0.953023374080658
Epoch 380, training loss: 0.8645000457763672 = 0.19203335046768188 + 0.1 * 6.724667072296143
Epoch 380, val loss: 0.9684699177742004
Epoch 390, training loss: 0.8436264991760254 = 0.17301464080810547 + 0.1 * 6.706118583679199
Epoch 390, val loss: 0.9848080277442932
Epoch 400, training loss: 0.8292365074157715 = 0.15611296892166138 + 0.1 * 6.731235504150391
Epoch 400, val loss: 1.0017467737197876
Epoch 410, training loss: 0.8118890523910522 = 0.14124006032943726 + 0.1 * 6.7064900398254395
Epoch 410, val loss: 1.0189718008041382
Epoch 420, training loss: 0.7980815768241882 = 0.12809687852859497 + 0.1 * 6.6998467445373535
Epoch 420, val loss: 1.0362892150878906
Epoch 430, training loss: 0.7843959331512451 = 0.11644075810909271 + 0.1 * 6.679551601409912
Epoch 430, val loss: 1.0537023544311523
Epoch 440, training loss: 0.7744485139846802 = 0.10607931017875671 + 0.1 * 6.683691501617432
Epoch 440, val loss: 1.0709441900253296
Epoch 450, training loss: 0.7641628980636597 = 0.09688077121973038 + 0.1 * 6.672821044921875
Epoch 450, val loss: 1.087920069694519
Epoch 460, training loss: 0.7563045024871826 = 0.08867199718952179 + 0.1 * 6.676324844360352
Epoch 460, val loss: 1.1047159433364868
Epoch 470, training loss: 0.7473957538604736 = 0.08135545998811722 + 0.1 * 6.660402774810791
Epoch 470, val loss: 1.1210495233535767
Epoch 480, training loss: 0.7405409812927246 = 0.0748196542263031 + 0.1 * 6.65721321105957
Epoch 480, val loss: 1.136846661567688
Epoch 490, training loss: 0.7330834865570068 = 0.06898514181375504 + 0.1 * 6.6409831047058105
Epoch 490, val loss: 1.1521923542022705
Epoch 500, training loss: 0.7284213900566101 = 0.06376054137945175 + 0.1 * 6.646608352661133
Epoch 500, val loss: 1.166982650756836
Epoch 510, training loss: 0.7216853499412537 = 0.0590655542910099 + 0.1 * 6.626197814941406
Epoch 510, val loss: 1.1814452409744263
Epoch 520, training loss: 0.7179874181747437 = 0.05483056977391243 + 0.1 * 6.631568431854248
Epoch 520, val loss: 1.1953105926513672
Epoch 530, training loss: 0.7136263847351074 = 0.05100671574473381 + 0.1 * 6.626196384429932
Epoch 530, val loss: 1.2089288234710693
Epoch 540, training loss: 0.7080152630805969 = 0.047547467052936554 + 0.1 * 6.604678153991699
Epoch 540, val loss: 1.2219990491867065
Epoch 550, training loss: 0.7053150534629822 = 0.0444178432226181 + 0.1 * 6.60897159576416
Epoch 550, val loss: 1.2346405982971191
Epoch 560, training loss: 0.7022749185562134 = 0.04157564043998718 + 0.1 * 6.606993198394775
Epoch 560, val loss: 1.2468681335449219
Epoch 570, training loss: 0.6991475224494934 = 0.038992203772068024 + 0.1 * 6.601552963256836
Epoch 570, val loss: 1.2587218284606934
Epoch 580, training loss: 0.695930540561676 = 0.036633607000112534 + 0.1 * 6.5929694175720215
Epoch 580, val loss: 1.270142912864685
Epoch 590, training loss: 0.6935626864433289 = 0.03448053076863289 + 0.1 * 6.590821743011475
Epoch 590, val loss: 1.2813096046447754
Epoch 600, training loss: 0.6914718151092529 = 0.032504767179489136 + 0.1 * 6.589670658111572
Epoch 600, val loss: 1.2921431064605713
Epoch 610, training loss: 0.688514232635498 = 0.030691055580973625 + 0.1 * 6.5782318115234375
Epoch 610, val loss: 1.302725076675415
Epoch 620, training loss: 0.6909587979316711 = 0.02902231737971306 + 0.1 * 6.619364261627197
Epoch 620, val loss: 1.312861680984497
Epoch 630, training loss: 0.6848172545433044 = 0.027488337829709053 + 0.1 * 6.573288917541504
Epoch 630, val loss: 1.3228707313537598
Epoch 640, training loss: 0.6822423338890076 = 0.02607407420873642 + 0.1 * 6.56168270111084
Epoch 640, val loss: 1.332482099533081
Epoch 650, training loss: 0.6807942390441895 = 0.024766985327005386 + 0.1 * 6.560272693634033
Epoch 650, val loss: 1.3418574333190918
Epoch 660, training loss: 0.6788337230682373 = 0.02355792745947838 + 0.1 * 6.55275821685791
Epoch 660, val loss: 1.3510938882827759
Epoch 670, training loss: 0.6816643476486206 = 0.02243465557694435 + 0.1 * 6.592297077178955
Epoch 670, val loss: 1.3599861860275269
Epoch 680, training loss: 0.6772851943969727 = 0.02139410376548767 + 0.1 * 6.558910846710205
Epoch 680, val loss: 1.368718147277832
Epoch 690, training loss: 0.6750215291976929 = 0.02042500115931034 + 0.1 * 6.54596471786499
Epoch 690, val loss: 1.3771765232086182
Epoch 700, training loss: 0.6745383143424988 = 0.019521145150065422 + 0.1 * 6.550171852111816
Epoch 700, val loss: 1.385446310043335
Epoch 710, training loss: 0.6726874113082886 = 0.018678007647395134 + 0.1 * 6.540093898773193
Epoch 710, val loss: 1.3935222625732422
Epoch 720, training loss: 0.6733468770980835 = 0.017889710143208504 + 0.1 * 6.554571151733398
Epoch 720, val loss: 1.4013432264328003
Epoch 730, training loss: 0.6712364554405212 = 0.01715366169810295 + 0.1 * 6.540827751159668
Epoch 730, val loss: 1.409087061882019
Epoch 740, training loss: 0.6699342727661133 = 0.016463590785861015 + 0.1 * 6.5347065925598145
Epoch 740, val loss: 1.4165126085281372
Epoch 750, training loss: 0.6695093512535095 = 0.015815559774637222 + 0.1 * 6.536937713623047
Epoch 750, val loss: 1.4238524436950684
Epoch 760, training loss: 0.6681346893310547 = 0.015207340009510517 + 0.1 * 6.52927303314209
Epoch 760, val loss: 1.4309829473495483
Epoch 770, training loss: 0.6667580604553223 = 0.014636065810918808 + 0.1 * 6.5212202072143555
Epoch 770, val loss: 1.4380433559417725
Epoch 780, training loss: 0.667536199092865 = 0.01409738976508379 + 0.1 * 6.534388065338135
Epoch 780, val loss: 1.4448602199554443
Epoch 790, training loss: 0.6659460663795471 = 0.013589287176728249 + 0.1 * 6.5235676765441895
Epoch 790, val loss: 1.4516732692718506
Epoch 800, training loss: 0.6659347414970398 = 0.013109342195093632 + 0.1 * 6.52825403213501
Epoch 800, val loss: 1.458153247833252
Epoch 810, training loss: 0.664362370967865 = 0.012657848186790943 + 0.1 * 6.517045497894287
Epoch 810, val loss: 1.464705467224121
Epoch 820, training loss: 0.6650851368904114 = 0.01222943514585495 + 0.1 * 6.528556823730469
Epoch 820, val loss: 1.4709802865982056
Epoch 830, training loss: 0.6624932289123535 = 0.01182451844215393 + 0.1 * 6.506687164306641
Epoch 830, val loss: 1.477184534072876
Epoch 840, training loss: 0.6621577739715576 = 0.011440405622124672 + 0.1 * 6.507173538208008
Epoch 840, val loss: 1.483229160308838
Epoch 850, training loss: 0.6615554094314575 = 0.011076648719608784 + 0.1 * 6.504787445068359
Epoch 850, val loss: 1.489229679107666
Epoch 860, training loss: 0.6613966226577759 = 0.010730210691690445 + 0.1 * 6.506664276123047
Epoch 860, val loss: 1.494930386543274
Epoch 870, training loss: 0.6602160334587097 = 0.010402154177427292 + 0.1 * 6.498138904571533
Epoch 870, val loss: 1.5007481575012207
Epoch 880, training loss: 0.6604472994804382 = 0.010089263319969177 + 0.1 * 6.503580093383789
Epoch 880, val loss: 1.506388545036316
Epoch 890, training loss: 0.659325122833252 = 0.009791171178221703 + 0.1 * 6.495339393615723
Epoch 890, val loss: 1.5117852687835693
Epoch 900, training loss: 0.6593838930130005 = 0.009508299641311169 + 0.1 * 6.498755931854248
Epoch 900, val loss: 1.5173789262771606
Epoch 910, training loss: 0.6583359241485596 = 0.009237567894160748 + 0.1 * 6.490983009338379
Epoch 910, val loss: 1.5225721597671509
Epoch 920, training loss: 0.6588535904884338 = 0.008980225771665573 + 0.1 * 6.4987335205078125
Epoch 920, val loss: 1.527801752090454
Epoch 930, training loss: 0.6569503545761108 = 0.008733866736292839 + 0.1 * 6.4821648597717285
Epoch 930, val loss: 1.5329324007034302
Epoch 940, training loss: 0.6572381258010864 = 0.008498767390847206 + 0.1 * 6.487393379211426
Epoch 940, val loss: 1.5379353761672974
Epoch 950, training loss: 0.6570309400558472 = 0.008273943327367306 + 0.1 * 6.487569808959961
Epoch 950, val loss: 1.5428770780563354
Epoch 960, training loss: 0.6561098098754883 = 0.008058417588472366 + 0.1 * 6.480513572692871
Epoch 960, val loss: 1.547732949256897
Epoch 970, training loss: 0.6565501689910889 = 0.007852292619645596 + 0.1 * 6.486978530883789
Epoch 970, val loss: 1.5525426864624023
Epoch 980, training loss: 0.6558675765991211 = 0.007654351182281971 + 0.1 * 6.4821319580078125
Epoch 980, val loss: 1.5571449995040894
Epoch 990, training loss: 0.654537558555603 = 0.007464843336492777 + 0.1 * 6.470727443695068
Epoch 990, val loss: 1.5617276430130005
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8154981549815499
The final CL Acc:0.74938, 0.00630, The final GNN Acc:0.81567, 0.00194
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13124])
remove edge: torch.Size([2, 7892])
updated graph: torch.Size([2, 10460])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8061065673828125 = 1.9464282989501953 + 0.1 * 8.596781730651855
Epoch 0, val loss: 1.9473707675933838
Epoch 10, training loss: 2.796156883239746 = 1.9364954233169556 + 0.1 * 8.5966157913208
Epoch 10, val loss: 1.9377129077911377
Epoch 20, training loss: 2.7834160327911377 = 1.9238587617874146 + 0.1 * 8.595573425292969
Epoch 20, val loss: 1.9250483512878418
Epoch 30, training loss: 2.7643394470214844 = 1.9056841135025024 + 0.1 * 8.586553573608398
Epoch 30, val loss: 1.9065473079681396
Epoch 40, training loss: 2.7315311431884766 = 1.8787399530410767 + 0.1 * 8.527912139892578
Epoch 40, val loss: 1.8794277906417847
Epoch 50, training loss: 2.666189670562744 = 1.8432286977767944 + 0.1 * 8.229610443115234
Epoch 50, val loss: 1.8455405235290527
Epoch 60, training loss: 2.5935237407684326 = 1.8055081367492676 + 0.1 * 7.88015604019165
Epoch 60, val loss: 1.8109309673309326
Epoch 70, training loss: 2.5134832859039307 = 1.770588994026184 + 0.1 * 7.428943157196045
Epoch 70, val loss: 1.779414176940918
Epoch 80, training loss: 2.4503865242004395 = 1.734391450881958 + 0.1 * 7.15994930267334
Epoch 80, val loss: 1.7468204498291016
Epoch 90, training loss: 2.39322566986084 = 1.6886988878250122 + 0.1 * 7.045268535614014
Epoch 90, val loss: 1.7051200866699219
Epoch 100, training loss: 2.325674295425415 = 1.6275100708007812 + 0.1 * 6.981642723083496
Epoch 100, val loss: 1.6506321430206299
Epoch 110, training loss: 2.244673252105713 = 1.5511260032653809 + 0.1 * 6.93547248840332
Epoch 110, val loss: 1.585690975189209
Epoch 120, training loss: 2.154966115951538 = 1.4644583463668823 + 0.1 * 6.905078411102295
Epoch 120, val loss: 1.5128647089004517
Epoch 130, training loss: 2.0630087852478027 = 1.3745626211166382 + 0.1 * 6.884462356567383
Epoch 130, val loss: 1.4393486976623535
Epoch 140, training loss: 1.970916509628296 = 1.2837156057357788 + 0.1 * 6.872008323669434
Epoch 140, val loss: 1.365684986114502
Epoch 150, training loss: 1.8776490688323975 = 1.1918120384216309 + 0.1 * 6.85836935043335
Epoch 150, val loss: 1.291729211807251
Epoch 160, training loss: 1.782983660697937 = 1.098081111907959 + 0.1 * 6.849025249481201
Epoch 160, val loss: 1.216389536857605
Epoch 170, training loss: 1.6900430917739868 = 1.0059144496917725 + 0.1 * 6.8412861824035645
Epoch 170, val loss: 1.14243483543396
Epoch 180, training loss: 1.6025519371032715 = 0.9189279675483704 + 0.1 * 6.836238861083984
Epoch 180, val loss: 1.0729589462280273
Epoch 190, training loss: 1.5201406478881836 = 0.8375896215438843 + 0.1 * 6.825509548187256
Epoch 190, val loss: 1.007647156715393
Epoch 200, training loss: 1.4451136589050293 = 0.763580322265625 + 0.1 * 6.815332889556885
Epoch 200, val loss: 0.9481868147850037
Epoch 210, training loss: 1.3789114952087402 = 0.6984423995018005 + 0.1 * 6.804690361022949
Epoch 210, val loss: 0.8971889019012451
Epoch 220, training loss: 1.3196924924850464 = 0.6409609913825989 + 0.1 * 6.7873148918151855
Epoch 220, val loss: 0.8540769219398499
Epoch 230, training loss: 1.2695651054382324 = 0.590416431427002 + 0.1 * 6.791485786437988
Epoch 230, val loss: 0.8187837600708008
Epoch 240, training loss: 1.2228357791900635 = 0.546440064907074 + 0.1 * 6.763957500457764
Epoch 240, val loss: 0.7909721732139587
Epoch 250, training loss: 1.1817543506622314 = 0.5071605443954468 + 0.1 * 6.745938777923584
Epoch 250, val loss: 0.7683586478233337
Epoch 260, training loss: 1.1462175846099854 = 0.4716850519180298 + 0.1 * 6.745324611663818
Epoch 260, val loss: 0.7499710321426392
Epoch 270, training loss: 1.1118872165679932 = 0.4395187497138977 + 0.1 * 6.723685264587402
Epoch 270, val loss: 0.734982967376709
Epoch 280, training loss: 1.0819627046585083 = 0.4092234671115875 + 0.1 * 6.727391719818115
Epoch 280, val loss: 0.7219489216804504
Epoch 290, training loss: 1.0497928857803345 = 0.3799895644187927 + 0.1 * 6.698032855987549
Epoch 290, val loss: 0.7105101346969604
Epoch 300, training loss: 1.0197895765304565 = 0.3508969843387604 + 0.1 * 6.688925743103027
Epoch 300, val loss: 0.7001330256462097
Epoch 310, training loss: 0.9896410703659058 = 0.32186159491539 + 0.1 * 6.677794933319092
Epoch 310, val loss: 0.6909529566764832
Epoch 320, training loss: 0.9595234990119934 = 0.2931065559387207 + 0.1 * 6.6641693115234375
Epoch 320, val loss: 0.6831863522529602
Epoch 330, training loss: 0.930693507194519 = 0.2651993930339813 + 0.1 * 6.654941558837891
Epoch 330, val loss: 0.6769988536834717
Epoch 340, training loss: 0.9043294191360474 = 0.23875072598457336 + 0.1 * 6.655786991119385
Epoch 340, val loss: 0.6726676225662231
Epoch 350, training loss: 0.87822425365448 = 0.21430686116218567 + 0.1 * 6.63917350769043
Epoch 350, val loss: 0.6701456308364868
Epoch 360, training loss: 0.861838698387146 = 0.1920662224292755 + 0.1 * 6.697724342346191
Epoch 360, val loss: 0.6693087220191956
Epoch 370, training loss: 0.8350954055786133 = 0.17235292494297028 + 0.1 * 6.627424716949463
Epoch 370, val loss: 0.6700926423072815
Epoch 380, training loss: 0.8163608312606812 = 0.15488258004188538 + 0.1 * 6.614782810211182
Epoch 380, val loss: 0.6721882224082947
Epoch 390, training loss: 0.799689531326294 = 0.13940732181072235 + 0.1 * 6.6028218269348145
Epoch 390, val loss: 0.6755518317222595
Epoch 400, training loss: 0.789108395576477 = 0.12576192617416382 + 0.1 * 6.633464336395264
Epoch 400, val loss: 0.6798571348190308
Epoch 410, training loss: 0.7736786603927612 = 0.11376386135816574 + 0.1 * 6.599147796630859
Epoch 410, val loss: 0.6850441098213196
Epoch 420, training loss: 0.7613899111747742 = 0.10313894599676132 + 0.1 * 6.582509994506836
Epoch 420, val loss: 0.6908230781555176
Epoch 430, training loss: 0.7533460855484009 = 0.09368905425071716 + 0.1 * 6.596570014953613
Epoch 430, val loss: 0.6972595453262329
Epoch 440, training loss: 0.7473491430282593 = 0.0853014588356018 + 0.1 * 6.620476722717285
Epoch 440, val loss: 0.7041320204734802
Epoch 450, training loss: 0.7351610064506531 = 0.07788672298192978 + 0.1 * 6.572742938995361
Epoch 450, val loss: 0.7113174200057983
Epoch 460, training loss: 0.7281362414360046 = 0.07128182798624039 + 0.1 * 6.568543910980225
Epoch 460, val loss: 0.7188467979431152
Epoch 470, training loss: 0.7222880125045776 = 0.06540073454380035 + 0.1 * 6.568872928619385
Epoch 470, val loss: 0.726621687412262
Epoch 480, training loss: 0.7161167860031128 = 0.06016199290752411 + 0.1 * 6.5595479011535645
Epoch 480, val loss: 0.7346250414848328
Epoch 490, training loss: 0.7108509540557861 = 0.055477701127529144 + 0.1 * 6.553732395172119
Epoch 490, val loss: 0.7427716851234436
Epoch 500, training loss: 0.706031858921051 = 0.0512918122112751 + 0.1 * 6.547399997711182
Epoch 500, val loss: 0.7510985136032104
Epoch 510, training loss: 0.703861653804779 = 0.047538403421640396 + 0.1 * 6.563232421875
Epoch 510, val loss: 0.7594191431999207
Epoch 520, training loss: 0.6982282996177673 = 0.04417216777801514 + 0.1 * 6.540561199188232
Epoch 520, val loss: 0.7677620053291321
Epoch 530, training loss: 0.6968017816543579 = 0.041138406842947006 + 0.1 * 6.556633472442627
Epoch 530, val loss: 0.7760878801345825
Epoch 540, training loss: 0.6916083097457886 = 0.0384085513651371 + 0.1 * 6.531997203826904
Epoch 540, val loss: 0.7843692898750305
Epoch 550, training loss: 0.6886849999427795 = 0.0359356515109539 + 0.1 * 6.527493476867676
Epoch 550, val loss: 0.7925822138786316
Epoch 560, training loss: 0.6856011152267456 = 0.03369097411632538 + 0.1 * 6.519101142883301
Epoch 560, val loss: 0.8006930947303772
Epoch 570, training loss: 0.683966875076294 = 0.03164980933070183 + 0.1 * 6.523170471191406
Epoch 570, val loss: 0.8086993098258972
Epoch 580, training loss: 0.6853112578392029 = 0.029787864536046982 + 0.1 * 6.555233955383301
Epoch 580, val loss: 0.8165801167488098
Epoch 590, training loss: 0.6794825792312622 = 0.028093397617340088 + 0.1 * 6.513891696929932
Epoch 590, val loss: 0.8242387175559998
Epoch 600, training loss: 0.6775552034378052 = 0.02654263935983181 + 0.1 * 6.510125160217285
Epoch 600, val loss: 0.831856906414032
Epoch 610, training loss: 0.6751134395599365 = 0.025116706266999245 + 0.1 * 6.499967575073242
Epoch 610, val loss: 0.8392313718795776
Epoch 620, training loss: 0.67447829246521 = 0.023804649710655212 + 0.1 * 6.5067362785339355
Epoch 620, val loss: 0.8465860486030579
Epoch 630, training loss: 0.6742297410964966 = 0.022594334557652473 + 0.1 * 6.516354084014893
Epoch 630, val loss: 0.8537020683288574
Epoch 640, training loss: 0.6713725924491882 = 0.02147681824862957 + 0.1 * 6.498957633972168
Epoch 640, val loss: 0.8607271313667297
Epoch 650, training loss: 0.669594407081604 = 0.020443296059966087 + 0.1 * 6.49151086807251
Epoch 650, val loss: 0.8676048517227173
Epoch 660, training loss: 0.6684888005256653 = 0.0194834154099226 + 0.1 * 6.490054130554199
Epoch 660, val loss: 0.874366283416748
Epoch 670, training loss: 0.6671544313430786 = 0.018591709434986115 + 0.1 * 6.485627174377441
Epoch 670, val loss: 0.8810514807701111
Epoch 680, training loss: 0.6662301421165466 = 0.017761068418622017 + 0.1 * 6.4846906661987305
Epoch 680, val loss: 0.8875243067741394
Epoch 690, training loss: 0.6665921807289124 = 0.01698821224272251 + 0.1 * 6.496039390563965
Epoch 690, val loss: 0.8939389586448669
Epoch 700, training loss: 0.6636280417442322 = 0.016268014907836914 + 0.1 * 6.473600387573242
Epoch 700, val loss: 0.9001854062080383
Epoch 710, training loss: 0.6638553738594055 = 0.015594334341585636 + 0.1 * 6.48261022567749
Epoch 710, val loss: 0.9063486456871033
Epoch 720, training loss: 0.6620519161224365 = 0.014962040819227695 + 0.1 * 6.4708991050720215
Epoch 720, val loss: 0.9123440980911255
Epoch 730, training loss: 0.662060558795929 = 0.014370409771800041 + 0.1 * 6.476901531219482
Epoch 730, val loss: 0.9182533025741577
Epoch 740, training loss: 0.660392701625824 = 0.013815394602715969 + 0.1 * 6.46577262878418
Epoch 740, val loss: 0.9240313768386841
Epoch 750, training loss: 0.6607794761657715 = 0.013292483985424042 + 0.1 * 6.474869728088379
Epoch 750, val loss: 0.929738461971283
Epoch 760, training loss: 0.6602240204811096 = 0.012799873948097229 + 0.1 * 6.474241256713867
Epoch 760, val loss: 0.9352999925613403
Epoch 770, training loss: 0.6589526534080505 = 0.012336280196905136 + 0.1 * 6.4661641120910645
Epoch 770, val loss: 0.9407811164855957
Epoch 780, training loss: 0.6575597524642944 = 0.011899064294993877 + 0.1 * 6.456606864929199
Epoch 780, val loss: 0.9462074637413025
Epoch 790, training loss: 0.6567732691764832 = 0.011485862545669079 + 0.1 * 6.452873706817627
Epoch 790, val loss: 0.9514138698577881
Epoch 800, training loss: 0.6563616394996643 = 0.011096752248704433 + 0.1 * 6.452648639678955
Epoch 800, val loss: 0.9566008448600769
Epoch 810, training loss: 0.6560666561126709 = 0.010728362016379833 + 0.1 * 6.453382968902588
Epoch 810, val loss: 0.9617444276809692
Epoch 820, training loss: 0.655384361743927 = 0.010378243401646614 + 0.1 * 6.450060844421387
Epoch 820, val loss: 0.9666905999183655
Epoch 830, training loss: 0.6554603576660156 = 0.010046929121017456 + 0.1 * 6.454134464263916
Epoch 830, val loss: 0.9716343879699707
Epoch 840, training loss: 0.6548810005187988 = 0.009732677601277828 + 0.1 * 6.451483249664307
Epoch 840, val loss: 0.9764744639396667
Epoch 850, training loss: 0.6538141965866089 = 0.009432908147573471 + 0.1 * 6.443812847137451
Epoch 850, val loss: 0.9812362194061279
Epoch 860, training loss: 0.6544994115829468 = 0.009148526936769485 + 0.1 * 6.453508377075195
Epoch 860, val loss: 0.9858761429786682
Epoch 870, training loss: 0.6533038020133972 = 0.00887774582952261 + 0.1 * 6.444260597229004
Epoch 870, val loss: 0.9905093908309937
Epoch 880, training loss: 0.6529830098152161 = 0.00861995480954647 + 0.1 * 6.443630218505859
Epoch 880, val loss: 0.9950124025344849
Epoch 890, training loss: 0.6523672342300415 = 0.008374189957976341 + 0.1 * 6.439929962158203
Epoch 890, val loss: 0.9994382262229919
Epoch 900, training loss: 0.6514627933502197 = 0.008139492012560368 + 0.1 * 6.43323278427124
Epoch 900, val loss: 1.0038281679153442
Epoch 910, training loss: 0.653377890586853 = 0.00791521929204464 + 0.1 * 6.45462703704834
Epoch 910, val loss: 1.0081154108047485
Epoch 920, training loss: 0.6518254280090332 = 0.007700934074819088 + 0.1 * 6.441245079040527
Epoch 920, val loss: 1.012326717376709
Epoch 930, training loss: 0.6530864834785461 = 0.007496678736060858 + 0.1 * 6.455898284912109
Epoch 930, val loss: 1.0164806842803955
Epoch 940, training loss: 0.6511327028274536 = 0.007300653029233217 + 0.1 * 6.438320159912109
Epoch 940, val loss: 1.020590901374817
Epoch 950, training loss: 0.6504689455032349 = 0.007113625295460224 + 0.1 * 6.433553218841553
Epoch 950, val loss: 1.024624228477478
Epoch 960, training loss: 0.6497411131858826 = 0.006934165954589844 + 0.1 * 6.428069591522217
Epoch 960, val loss: 1.0285714864730835
Epoch 970, training loss: 0.6498928666114807 = 0.006762185133993626 + 0.1 * 6.4313063621521
Epoch 970, val loss: 1.0324876308441162
Epoch 980, training loss: 0.6491805911064148 = 0.006597277242690325 + 0.1 * 6.425833225250244
Epoch 980, val loss: 1.036334753036499
Epoch 990, training loss: 0.649207353591919 = 0.006438798271119595 + 0.1 * 6.427685737609863
Epoch 990, val loss: 1.040132761001587
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 2.794283390045166 = 1.9345996379852295 + 0.1 * 8.596837043762207
Epoch 0, val loss: 1.9332895278930664
Epoch 10, training loss: 2.7846567630767822 = 1.9249788522720337 + 0.1 * 8.596778869628906
Epoch 10, val loss: 1.9237788915634155
Epoch 20, training loss: 2.7730460166931152 = 1.9134076833724976 + 0.1 * 8.596383094787598
Epoch 20, val loss: 1.9117350578308105
Epoch 30, training loss: 2.7568416595458984 = 1.8975536823272705 + 0.1 * 8.592878341674805
Epoch 30, val loss: 1.8948713541030884
Epoch 40, training loss: 2.731224536895752 = 1.8746905326843262 + 0.1 * 8.565340042114258
Epoch 40, val loss: 1.8705980777740479
Epoch 50, training loss: 2.6854474544525146 = 1.8437870740890503 + 0.1 * 8.416603088378906
Epoch 50, val loss: 1.8392841815948486
Epoch 60, training loss: 2.6273059844970703 = 1.808861494064331 + 0.1 * 8.184444427490234
Epoch 60, val loss: 1.8065576553344727
Epoch 70, training loss: 2.5630593299865723 = 1.7730053663253784 + 0.1 * 7.900538444519043
Epoch 70, val loss: 1.7749977111816406
Epoch 80, training loss: 2.482297897338867 = 1.7336479425430298 + 0.1 * 7.4864983558654785
Epoch 80, val loss: 1.741050124168396
Epoch 90, training loss: 2.409684658050537 = 1.684367299079895 + 0.1 * 7.253173351287842
Epoch 90, val loss: 1.6983070373535156
Epoch 100, training loss: 2.330923318862915 = 1.6172524690628052 + 0.1 * 7.136708736419678
Epoch 100, val loss: 1.6399102210998535
Epoch 110, training loss: 2.2397899627685547 = 1.534731388092041 + 0.1 * 7.050586223602295
Epoch 110, val loss: 1.5692178010940552
Epoch 120, training loss: 2.144104480743408 = 1.444948434829712 + 0.1 * 6.991559982299805
Epoch 120, val loss: 1.495017409324646
Epoch 130, training loss: 2.048959493637085 = 1.3540680408477783 + 0.1 * 6.948914051055908
Epoch 130, val loss: 1.4245275259017944
Epoch 140, training loss: 1.9582173824310303 = 1.2660608291625977 + 0.1 * 6.92156457901001
Epoch 140, val loss: 1.3605139255523682
Epoch 150, training loss: 1.8719565868377686 = 1.1817377805709839 + 0.1 * 6.902187824249268
Epoch 150, val loss: 1.3007677793502808
Epoch 160, training loss: 1.7910348176956177 = 1.1020431518554688 + 0.1 * 6.88991641998291
Epoch 160, val loss: 1.2448198795318604
Epoch 170, training loss: 1.7139432430267334 = 1.0257601737976074 + 0.1 * 6.881830215454102
Epoch 170, val loss: 1.1906460523605347
Epoch 180, training loss: 1.638519048690796 = 0.9509029984474182 + 0.1 * 6.87615966796875
Epoch 180, val loss: 1.1361924409866333
Epoch 190, training loss: 1.5646247863769531 = 0.8769184350967407 + 0.1 * 6.877064228057861
Epoch 190, val loss: 1.080653429031372
Epoch 200, training loss: 1.492153286933899 = 0.8049225807189941 + 0.1 * 6.872306823730469
Epoch 200, val loss: 1.025795340538025
Epoch 210, training loss: 1.4219542741775513 = 0.7353145480155945 + 0.1 * 6.866396903991699
Epoch 210, val loss: 0.9728118777275085
Epoch 220, training loss: 1.3559235334396362 = 0.6696290969848633 + 0.1 * 6.86294412612915
Epoch 220, val loss: 0.9237738251686096
Epoch 230, training loss: 1.2960374355316162 = 0.6094944477081299 + 0.1 * 6.865428924560547
Epoch 230, val loss: 0.8806899785995483
Epoch 240, training loss: 1.2415313720703125 = 0.5560188889503479 + 0.1 * 6.855125427246094
Epoch 240, val loss: 0.8447325825691223
Epoch 250, training loss: 1.1935276985168457 = 0.5084496140480042 + 0.1 * 6.850780487060547
Epoch 250, val loss: 0.8154999613761902
Epoch 260, training loss: 1.151193618774414 = 0.4658866822719574 + 0.1 * 6.85306978225708
Epoch 260, val loss: 0.7918516993522644
Epoch 270, training loss: 1.1113640069961548 = 0.4271778464317322 + 0.1 * 6.841861724853516
Epoch 270, val loss: 0.7726465463638306
Epoch 280, training loss: 1.0747207403182983 = 0.3911731243133545 + 0.1 * 6.835475921630859
Epoch 280, val loss: 0.7567737102508545
Epoch 290, training loss: 1.040367603302002 = 0.35735470056533813 + 0.1 * 6.8301286697387695
Epoch 290, val loss: 0.7437596321105957
Epoch 300, training loss: 1.0092501640319824 = 0.32576221227645874 + 0.1 * 6.8348798751831055
Epoch 300, val loss: 0.7335591316223145
Epoch 310, training loss: 0.9785573482513428 = 0.2965477406978607 + 0.1 * 6.820096015930176
Epoch 310, val loss: 0.7258457541465759
Epoch 320, training loss: 0.9510760307312012 = 0.2694861590862274 + 0.1 * 6.815898895263672
Epoch 320, val loss: 0.7206248641014099
Epoch 330, training loss: 0.9261422753334045 = 0.24452929198741913 + 0.1 * 6.816129684448242
Epoch 330, val loss: 0.7176283001899719
Epoch 340, training loss: 0.9018926024436951 = 0.2215922325849533 + 0.1 * 6.803003311157227
Epoch 340, val loss: 0.7164449095726013
Epoch 350, training loss: 0.8799465894699097 = 0.2003861516714096 + 0.1 * 6.795604228973389
Epoch 350, val loss: 0.7169371843338013
Epoch 360, training loss: 0.8596028089523315 = 0.18083450198173523 + 0.1 * 6.787683486938477
Epoch 360, val loss: 0.7188000679016113
Epoch 370, training loss: 0.8408514261245728 = 0.16285109519958496 + 0.1 * 6.780003070831299
Epoch 370, val loss: 0.7218948006629944
Epoch 380, training loss: 0.8241965770721436 = 0.14640742540359497 + 0.1 * 6.777891635894775
Epoch 380, val loss: 0.7260391712188721
Epoch 390, training loss: 0.8076790571212769 = 0.13146886229515076 + 0.1 * 6.762101650238037
Epoch 390, val loss: 0.7310606837272644
Epoch 400, training loss: 0.7939765453338623 = 0.1179923564195633 + 0.1 * 6.7598419189453125
Epoch 400, val loss: 0.7369008660316467
Epoch 410, training loss: 0.7805424928665161 = 0.10589433461427689 + 0.1 * 6.746481418609619
Epoch 410, val loss: 0.7434883117675781
Epoch 420, training loss: 0.7710228562355042 = 0.09507258236408234 + 0.1 * 6.75950288772583
Epoch 420, val loss: 0.7507908344268799
Epoch 430, training loss: 0.7589938044548035 = 0.0854816809296608 + 0.1 * 6.73512077331543
Epoch 430, val loss: 0.758465051651001
Epoch 440, training loss: 0.749264121055603 = 0.076961949467659 + 0.1 * 6.723021984100342
Epoch 440, val loss: 0.7667150497436523
Epoch 450, training loss: 0.741449236869812 = 0.06940111517906189 + 0.1 * 6.720480918884277
Epoch 450, val loss: 0.7752989530563354
Epoch 460, training loss: 0.7336026430130005 = 0.06272100657224655 + 0.1 * 6.708816051483154
Epoch 460, val loss: 0.7839627265930176
Epoch 470, training loss: 0.7296248078346252 = 0.056810274720191956 + 0.1 * 6.728145599365234
Epoch 470, val loss: 0.7928824424743652
Epoch 480, training loss: 0.7214021682739258 = 0.0516069121658802 + 0.1 * 6.697952747344971
Epoch 480, val loss: 0.8016935586929321
Epoch 490, training loss: 0.7166288495063782 = 0.047006119042634964 + 0.1 * 6.696227073669434
Epoch 490, val loss: 0.8105599284172058
Epoch 500, training loss: 0.7115253806114197 = 0.04293821379542351 + 0.1 * 6.685871124267578
Epoch 500, val loss: 0.8194265365600586
Epoch 510, training loss: 0.7074950933456421 = 0.039336107671260834 + 0.1 * 6.6815900802612305
Epoch 510, val loss: 0.8281501531600952
Epoch 520, training loss: 0.7038464546203613 = 0.036142975091934204 + 0.1 * 6.677034854888916
Epoch 520, val loss: 0.8368406891822815
Epoch 530, training loss: 0.7005077600479126 = 0.033307261765003204 + 0.1 * 6.672004699707031
Epoch 530, val loss: 0.8451780080795288
Epoch 540, training loss: 0.6991173028945923 = 0.030779512599110603 + 0.1 * 6.683377265930176
Epoch 540, val loss: 0.8535318970680237
Epoch 550, training loss: 0.694507896900177 = 0.028530556708574295 + 0.1 * 6.659773349761963
Epoch 550, val loss: 0.8615936040878296
Epoch 560, training loss: 0.6919909715652466 = 0.02652067504823208 + 0.1 * 6.654702663421631
Epoch 560, val loss: 0.8694567680358887
Epoch 570, training loss: 0.6902968287467957 = 0.024713722988963127 + 0.1 * 6.655831336975098
Epoch 570, val loss: 0.8771283030509949
Epoch 580, training loss: 0.6871028542518616 = 0.02309025079011917 + 0.1 * 6.640125751495361
Epoch 580, val loss: 0.884676992893219
Epoch 590, training loss: 0.6847989559173584 = 0.0216241255402565 + 0.1 * 6.631747722625732
Epoch 590, val loss: 0.891986608505249
Epoch 600, training loss: 0.6848438382148743 = 0.020295238122344017 + 0.1 * 6.6454854011535645
Epoch 600, val loss: 0.8992053270339966
Epoch 610, training loss: 0.6824645400047302 = 0.019092390313744545 + 0.1 * 6.633721351623535
Epoch 610, val loss: 0.9060892462730408
Epoch 620, training loss: 0.6820133328437805 = 0.017998458817601204 + 0.1 * 6.640148639678955
Epoch 620, val loss: 0.9128008484840393
Epoch 630, training loss: 0.6794647574424744 = 0.01700226403772831 + 0.1 * 6.624624729156494
Epoch 630, val loss: 0.9193999767303467
Epoch 640, training loss: 0.6775519251823425 = 0.016089193522930145 + 0.1 * 6.614626884460449
Epoch 640, val loss: 0.9257791638374329
Epoch 650, training loss: 0.6773556470870972 = 0.015252822078764439 + 0.1 * 6.621027946472168
Epoch 650, val loss: 0.9321665167808533
Epoch 660, training loss: 0.6747793555259705 = 0.014484193176031113 + 0.1 * 6.602951526641846
Epoch 660, val loss: 0.9383324980735779
Epoch 670, training loss: 0.6734962463378906 = 0.013778979890048504 + 0.1 * 6.597172737121582
Epoch 670, val loss: 0.9441940784454346
Epoch 680, training loss: 0.6761776208877563 = 0.013125265017151833 + 0.1 * 6.630523681640625
Epoch 680, val loss: 0.9500828385353088
Epoch 690, training loss: 0.6715933084487915 = 0.012523133307695389 + 0.1 * 6.590702056884766
Epoch 690, val loss: 0.9558799266815186
Epoch 700, training loss: 0.6706936359405518 = 0.011967179365456104 + 0.1 * 6.587264537811279
Epoch 700, val loss: 0.9612336158752441
Epoch 710, training loss: 0.6699267625808716 = 0.011448664590716362 + 0.1 * 6.584780693054199
Epoch 710, val loss: 0.9666327834129333
Epoch 720, training loss: 0.6692370772361755 = 0.010965201072394848 + 0.1 * 6.582718372344971
Epoch 720, val loss: 0.9720590114593506
Epoch 730, training loss: 0.6689479947090149 = 0.010515652596950531 + 0.1 * 6.584323406219482
Epoch 730, val loss: 0.97726970911026
Epoch 740, training loss: 0.6678160429000854 = 0.010093863122165203 + 0.1 * 6.577221870422363
Epoch 740, val loss: 0.9823898673057556
Epoch 750, training loss: 0.6680446267127991 = 0.009701288305222988 + 0.1 * 6.583433151245117
Epoch 750, val loss: 0.9874244928359985
Epoch 760, training loss: 0.6658239364624023 = 0.009333718568086624 + 0.1 * 6.564901828765869
Epoch 760, val loss: 0.9922237992286682
Epoch 770, training loss: 0.665948748588562 = 0.00898832455277443 + 0.1 * 6.56960391998291
Epoch 770, val loss: 0.9969667792320251
Epoch 780, training loss: 0.666659951210022 = 0.008664248511195183 + 0.1 * 6.579957008361816
Epoch 780, val loss: 1.0017201900482178
Epoch 790, training loss: 0.664837658405304 = 0.008359590545296669 + 0.1 * 6.564780235290527
Epoch 790, val loss: 1.006259799003601
Epoch 800, training loss: 0.663672924041748 = 0.008073288947343826 + 0.1 * 6.555996417999268
Epoch 800, val loss: 1.0106620788574219
Epoch 810, training loss: 0.6653017401695251 = 0.007802038453519344 + 0.1 * 6.574997425079346
Epoch 810, val loss: 1.0151194334030151
Epoch 820, training loss: 0.662592351436615 = 0.007547379471361637 + 0.1 * 6.550449848175049
Epoch 820, val loss: 1.01934814453125
Epoch 830, training loss: 0.6636792421340942 = 0.007305819075554609 + 0.1 * 6.563734531402588
Epoch 830, val loss: 1.0235650539398193
Epoch 840, training loss: 0.6619241237640381 = 0.007078732829540968 + 0.1 * 6.548454284667969
Epoch 840, val loss: 1.0275959968566895
Epoch 850, training loss: 0.661146342754364 = 0.006862359587103128 + 0.1 * 6.542840003967285
Epoch 850, val loss: 1.031640887260437
Epoch 860, training loss: 0.6610297560691833 = 0.006657222285866737 + 0.1 * 6.54372501373291
Epoch 860, val loss: 1.035649061203003
Epoch 870, training loss: 0.6610124707221985 = 0.006462692283093929 + 0.1 * 6.545497417449951
Epoch 870, val loss: 1.0395572185516357
Epoch 880, training loss: 0.6606741547584534 = 0.0062777260318398476 + 0.1 * 6.54396390914917
Epoch 880, val loss: 1.0433512926101685
Epoch 890, training loss: 0.6604514718055725 = 0.006101709324866533 + 0.1 * 6.543497085571289
Epoch 890, val loss: 1.0470130443572998
Epoch 900, training loss: 0.6589152812957764 = 0.005934919696301222 + 0.1 * 6.52980375289917
Epoch 900, val loss: 1.0507346391677856
Epoch 910, training loss: 0.6589717268943787 = 0.005775315221399069 + 0.1 * 6.53196382522583
Epoch 910, val loss: 1.054331660270691
Epoch 920, training loss: 0.6594239473342896 = 0.0056227645836770535 + 0.1 * 6.5380120277404785
Epoch 920, val loss: 1.0578941106796265
Epoch 930, training loss: 0.6584740877151489 = 0.005478299222886562 + 0.1 * 6.529957294464111
Epoch 930, val loss: 1.0613478422164917
Epoch 940, training loss: 0.657746434211731 = 0.005339331459254026 + 0.1 * 6.524070739746094
Epoch 940, val loss: 1.0647990703582764
Epoch 950, training loss: 0.6579028367996216 = 0.005208377726376057 + 0.1 * 6.526944637298584
Epoch 950, val loss: 1.068030595779419
Epoch 960, training loss: 0.6567428112030029 = 0.005081976298242807 + 0.1 * 6.516608715057373
Epoch 960, val loss: 1.0712175369262695
Epoch 970, training loss: 0.6618444919586182 = 0.004960373044013977 + 0.1 * 6.568840980529785
Epoch 970, val loss: 1.07455575466156
Epoch 980, training loss: 0.6560981273651123 = 0.004844753537327051 + 0.1 * 6.512533664703369
Epoch 980, val loss: 1.0777302980422974
Epoch 990, training loss: 0.6556004881858826 = 0.004733668174594641 + 0.1 * 6.508667945861816
Epoch 990, val loss: 1.0808011293411255
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.815427780151367 = 1.9557440280914307 + 0.1 * 8.596837043762207
Epoch 0, val loss: 1.958555817604065
Epoch 10, training loss: 2.8054046630859375 = 1.9457262754440308 + 0.1 * 8.596784591674805
Epoch 10, val loss: 1.9482101202011108
Epoch 20, training loss: 2.7931363582611084 = 1.9334933757781982 + 0.1 * 8.596428871154785
Epoch 20, val loss: 1.935418963432312
Epoch 30, training loss: 2.7755320072174072 = 1.9161654710769653 + 0.1 * 8.593666076660156
Epoch 30, val loss: 1.9173808097839355
Epoch 40, training loss: 2.7476582527160645 = 1.8901747465133667 + 0.1 * 8.574834823608398
Epoch 40, val loss: 1.890740990638733
Epoch 50, training loss: 2.7023212909698486 = 1.8528759479522705 + 0.1 * 8.494453430175781
Epoch 50, val loss: 1.8540902137756348
Epoch 60, training loss: 2.6346869468688965 = 1.8087337017059326 + 0.1 * 8.25953197479248
Epoch 60, val loss: 1.8133723735809326
Epoch 70, training loss: 2.5764591693878174 = 1.7661927938461304 + 0.1 * 8.10266399383545
Epoch 70, val loss: 1.7756344079971313
Epoch 80, training loss: 2.505392074584961 = 1.7202783823013306 + 0.1 * 7.851138114929199
Epoch 80, val loss: 1.7336063385009766
Epoch 90, training loss: 2.4248666763305664 = 1.6614258289337158 + 0.1 * 7.634408950805664
Epoch 90, val loss: 1.6828533411026
Epoch 100, training loss: 2.328740358352661 = 1.5880604982376099 + 0.1 * 7.406797885894775
Epoch 100, val loss: 1.6223479509353638
Epoch 110, training loss: 2.2302751541137695 = 1.4980019330978394 + 0.1 * 7.322731971740723
Epoch 110, val loss: 1.543645977973938
Epoch 120, training loss: 2.1231496334075928 = 1.399872064590454 + 0.1 * 7.232775688171387
Epoch 120, val loss: 1.461448311805725
Epoch 130, training loss: 2.0169243812561035 = 1.3025565147399902 + 0.1 * 7.143677711486816
Epoch 130, val loss: 1.3820044994354248
Epoch 140, training loss: 1.91615891456604 = 1.2081704139709473 + 0.1 * 7.0798845291137695
Epoch 140, val loss: 1.3067957162857056
Epoch 150, training loss: 1.8222229480743408 = 1.1187915802001953 + 0.1 * 7.034313201904297
Epoch 150, val loss: 1.2379133701324463
Epoch 160, training loss: 1.7381441593170166 = 1.0394692420959473 + 0.1 * 6.986748695373535
Epoch 160, val loss: 1.1783946752548218
Epoch 170, training loss: 1.6646687984466553 = 0.9689213633537292 + 0.1 * 6.9574737548828125
Epoch 170, val loss: 1.1270732879638672
Epoch 180, training loss: 1.5983492136001587 = 0.9046754240989685 + 0.1 * 6.936738014221191
Epoch 180, val loss: 1.0803269147872925
Epoch 190, training loss: 1.5349035263061523 = 0.8432698845863342 + 0.1 * 6.916335582733154
Epoch 190, val loss: 1.034846544265747
Epoch 200, training loss: 1.47158682346344 = 0.7815532088279724 + 0.1 * 6.900335788726807
Epoch 200, val loss: 0.9881942272186279
Epoch 210, training loss: 1.4087066650390625 = 0.7196547389030457 + 0.1 * 6.890518665313721
Epoch 210, val loss: 0.9410482048988342
Epoch 220, training loss: 1.346825122833252 = 0.6590638160705566 + 0.1 * 6.877613544464111
Epoch 220, val loss: 0.8956859111785889
Epoch 230, training loss: 1.2887908220291138 = 0.6014120578765869 + 0.1 * 6.8737874031066895
Epoch 230, val loss: 0.8546000123023987
Epoch 240, training loss: 1.2354273796081543 = 0.5485423803329468 + 0.1 * 6.868849277496338
Epoch 240, val loss: 0.8202177882194519
Epoch 250, training loss: 1.186330795288086 = 0.501312255859375 + 0.1 * 6.850185394287109
Epoch 250, val loss: 0.7934892177581787
Epoch 260, training loss: 1.143031358718872 = 0.4593355655670166 + 0.1 * 6.836958408355713
Epoch 260, val loss: 0.7738082408905029
Epoch 270, training loss: 1.104811429977417 = 0.42206525802612305 + 0.1 * 6.827462196350098
Epoch 270, val loss: 0.7601476311683655
Epoch 280, training loss: 1.0704224109649658 = 0.38856393098831177 + 0.1 * 6.818584442138672
Epoch 280, val loss: 0.7510900497436523
Epoch 290, training loss: 1.0379464626312256 = 0.35744792222976685 + 0.1 * 6.804986000061035
Epoch 290, val loss: 0.745418906211853
Epoch 300, training loss: 1.0096558332443237 = 0.3278748095035553 + 0.1 * 6.817809581756592
Epoch 300, val loss: 0.74215167760849
Epoch 310, training loss: 0.9787484407424927 = 0.29955917596817017 + 0.1 * 6.7918925285339355
Epoch 310, val loss: 0.7407544851303101
Epoch 320, training loss: 0.9501631259918213 = 0.2722686231136322 + 0.1 * 6.778944969177246
Epoch 320, val loss: 0.7409974932670593
Epoch 330, training loss: 0.9242609739303589 = 0.2464289367198944 + 0.1 * 6.7783203125
Epoch 330, val loss: 0.7427986264228821
Epoch 340, training loss: 0.8999032974243164 = 0.2227790653705597 + 0.1 * 6.771241664886475
Epoch 340, val loss: 0.7464644908905029
Epoch 350, training loss: 0.877113938331604 = 0.2015814334154129 + 0.1 * 6.7553253173828125
Epoch 350, val loss: 0.7518638372421265
Epoch 360, training loss: 0.8580885529518127 = 0.18277885019779205 + 0.1 * 6.753097057342529
Epoch 360, val loss: 0.7591215968132019
Epoch 370, training loss: 0.8446979522705078 = 0.1661933958530426 + 0.1 * 6.785045623779297
Epoch 370, val loss: 0.7679547071456909
Epoch 380, training loss: 0.825532853603363 = 0.15161748230457306 + 0.1 * 6.739153861999512
Epoch 380, val loss: 0.7780920267105103
Epoch 390, training loss: 0.8119579553604126 = 0.1386311650276184 + 0.1 * 6.733267784118652
Epoch 390, val loss: 0.7892364263534546
Epoch 400, training loss: 0.7992959022521973 = 0.1269715279340744 + 0.1 * 6.723243236541748
Epoch 400, val loss: 0.8013387322425842
Epoch 410, training loss: 0.7897365093231201 = 0.11652503162622452 + 0.1 * 6.732114315032959
Epoch 410, val loss: 0.8139218688011169
Epoch 420, training loss: 0.777826189994812 = 0.10714917629957199 + 0.1 * 6.706769943237305
Epoch 420, val loss: 0.8268505334854126
Epoch 430, training loss: 0.7687456607818604 = 0.0986705869436264 + 0.1 * 6.700750350952148
Epoch 430, val loss: 0.8401677012443542
Epoch 440, training loss: 0.7622209787368774 = 0.09098831564188004 + 0.1 * 6.712326526641846
Epoch 440, val loss: 0.8535397052764893
Epoch 450, training loss: 0.7533678412437439 = 0.08404946327209473 + 0.1 * 6.693183898925781
Epoch 450, val loss: 0.8670304417610168
Epoch 460, training loss: 0.7466530203819275 = 0.07776852697134018 + 0.1 * 6.688845157623291
Epoch 460, val loss: 0.8803476691246033
Epoch 470, training loss: 0.7401533126831055 = 0.07207589596509933 + 0.1 * 6.680773735046387
Epoch 470, val loss: 0.8936617374420166
Epoch 480, training loss: 0.7344664335250854 = 0.06690485775470734 + 0.1 * 6.6756157875061035
Epoch 480, val loss: 0.9069165587425232
Epoch 490, training loss: 0.7293953895568848 = 0.06221006065607071 + 0.1 * 6.671853065490723
Epoch 490, val loss: 0.9200245141983032
Epoch 500, training loss: 0.7253828644752502 = 0.057944100350141525 + 0.1 * 6.674387454986572
Epoch 500, val loss: 0.9329869747161865
Epoch 510, training loss: 0.7207825779914856 = 0.05407223477959633 + 0.1 * 6.667103290557861
Epoch 510, val loss: 0.9457487463951111
Epoch 520, training loss: 0.7152323126792908 = 0.050547026097774506 + 0.1 * 6.646852970123291
Epoch 520, val loss: 0.9583395719528198
Epoch 530, training loss: 0.7124934196472168 = 0.04732555150985718 + 0.1 * 6.651678562164307
Epoch 530, val loss: 0.9708109498023987
Epoch 540, training loss: 0.7095353603363037 = 0.044382937252521515 + 0.1 * 6.651524066925049
Epoch 540, val loss: 0.9830582141876221
Epoch 550, training loss: 0.7055680155754089 = 0.04169362410902977 + 0.1 * 6.638743877410889
Epoch 550, val loss: 0.9950162768363953
Epoch 560, training loss: 0.7026335000991821 = 0.039226993918418884 + 0.1 * 6.634064674377441
Epoch 560, val loss: 1.0068591833114624
Epoch 570, training loss: 0.7002259492874146 = 0.03695832937955856 + 0.1 * 6.632676124572754
Epoch 570, val loss: 1.0185045003890991
Epoch 580, training loss: 0.6987016797065735 = 0.03487288951873779 + 0.1 * 6.6382880210876465
Epoch 580, val loss: 1.0298452377319336
Epoch 590, training loss: 0.6945536136627197 = 0.032952409237623215 + 0.1 * 6.616011619567871
Epoch 590, val loss: 1.0410839319229126
Epoch 600, training loss: 0.6935244202613831 = 0.031177900731563568 + 0.1 * 6.623465061187744
Epoch 600, val loss: 1.052146553993225
Epoch 610, training loss: 0.6909676790237427 = 0.029538894072175026 + 0.1 * 6.614287853240967
Epoch 610, val loss: 1.0628780126571655
Epoch 620, training loss: 0.691976010799408 = 0.02802157588303089 + 0.1 * 6.639544486999512
Epoch 620, val loss: 1.0734245777130127
Epoch 630, training loss: 0.6869575381278992 = 0.026617879047989845 + 0.1 * 6.603396415710449
Epoch 630, val loss: 1.083775281906128
Epoch 640, training loss: 0.6849642395973206 = 0.025314263999462128 + 0.1 * 6.596499443054199
Epoch 640, val loss: 1.0938876867294312
Epoch 650, training loss: 0.6834456920623779 = 0.024101413786411285 + 0.1 * 6.593442440032959
Epoch 650, val loss: 1.103943943977356
Epoch 660, training loss: 0.6823424100875854 = 0.022970715537667274 + 0.1 * 6.593716621398926
Epoch 660, val loss: 1.1136575937271118
Epoch 670, training loss: 0.6813161969184875 = 0.021917112171649933 + 0.1 * 6.593990802764893
Epoch 670, val loss: 1.1233465671539307
Epoch 680, training loss: 0.6820021271705627 = 0.0209346953779459 + 0.1 * 6.6106743812561035
Epoch 680, val loss: 1.1326984167099
Epoch 690, training loss: 0.6787441372871399 = 0.020018501207232475 + 0.1 * 6.587255954742432
Epoch 690, val loss: 1.1419535875320435
Epoch 700, training loss: 0.6771989464759827 = 0.01916227489709854 + 0.1 * 6.580366611480713
Epoch 700, val loss: 1.1509978771209717
Epoch 710, training loss: 0.6767076253890991 = 0.01835998147726059 + 0.1 * 6.583476543426514
Epoch 710, val loss: 1.1599340438842773
Epoch 720, training loss: 0.6753127574920654 = 0.017607273533940315 + 0.1 * 6.577054500579834
Epoch 720, val loss: 1.1686904430389404
Epoch 730, training loss: 0.6764264106750488 = 0.01690049096941948 + 0.1 * 6.595259189605713
Epoch 730, val loss: 1.1772756576538086
Epoch 740, training loss: 0.6736611127853394 = 0.016236359253525734 + 0.1 * 6.574247360229492
Epoch 740, val loss: 1.1856448650360107
Epoch 750, training loss: 0.6728692650794983 = 0.015611800365149975 + 0.1 * 6.572574138641357
Epoch 750, val loss: 1.1938607692718506
Epoch 760, training loss: 0.6718252897262573 = 0.015023848041892052 + 0.1 * 6.568014621734619
Epoch 760, val loss: 1.2019277811050415
Epoch 770, training loss: 0.6710280776023865 = 0.014470367692410946 + 0.1 * 6.565577507019043
Epoch 770, val loss: 1.2098844051361084
Epoch 780, training loss: 0.6690735220909119 = 0.013947083614766598 + 0.1 * 6.55126428604126
Epoch 780, val loss: 1.2175865173339844
Epoch 790, training loss: 0.6699867844581604 = 0.013453604653477669 + 0.1 * 6.56533145904541
Epoch 790, val loss: 1.225252389907837
Epoch 800, training loss: 0.6680634617805481 = 0.012986645102500916 + 0.1 * 6.55076789855957
Epoch 800, val loss: 1.2327628135681152
Epoch 810, training loss: 0.6668213605880737 = 0.012545404024422169 + 0.1 * 6.542759418487549
Epoch 810, val loss: 1.2401578426361084
Epoch 820, training loss: 0.6670749187469482 = 0.012127031572163105 + 0.1 * 6.549478530883789
Epoch 820, val loss: 1.247473120689392
Epoch 830, training loss: 0.6670415997505188 = 0.011729399673640728 + 0.1 * 6.553121566772461
Epoch 830, val loss: 1.254525899887085
Epoch 840, training loss: 0.6676245927810669 = 0.011353249661624432 + 0.1 * 6.562713146209717
Epoch 840, val loss: 1.2614566087722778
Epoch 850, training loss: 0.6656049489974976 = 0.010996067896485329 + 0.1 * 6.546088695526123
Epoch 850, val loss: 1.2682017087936401
Epoch 860, training loss: 0.6641396880149841 = 0.010657643899321556 + 0.1 * 6.534820079803467
Epoch 860, val loss: 1.2749782800674438
Epoch 870, training loss: 0.663818359375 = 0.010335732251405716 + 0.1 * 6.534826278686523
Epoch 870, val loss: 1.2816063165664673
Epoch 880, training loss: 0.6627138257026672 = 0.010028458200395107 + 0.1 * 6.526853084564209
Epoch 880, val loss: 1.2881063222885132
Epoch 890, training loss: 0.6655307412147522 = 0.009735345840454102 + 0.1 * 6.557953834533691
Epoch 890, val loss: 1.2944326400756836
Epoch 900, training loss: 0.6625593900680542 = 0.009455852210521698 + 0.1 * 6.53103494644165
Epoch 900, val loss: 1.3006951808929443
Epoch 910, training loss: 0.6616874933242798 = 0.00918948370963335 + 0.1 * 6.524980068206787
Epoch 910, val loss: 1.3068125247955322
Epoch 920, training loss: 0.6618170738220215 = 0.008935222402215004 + 0.1 * 6.528818607330322
Epoch 920, val loss: 1.3128979206085205
Epoch 930, training loss: 0.6607435345649719 = 0.008691747672855854 + 0.1 * 6.520517826080322
Epoch 930, val loss: 1.3188284635543823
Epoch 940, training loss: 0.6624878644943237 = 0.008459221571683884 + 0.1 * 6.540286540985107
Epoch 940, val loss: 1.3247041702270508
Epoch 950, training loss: 0.6600859761238098 = 0.008236842229962349 + 0.1 * 6.518491268157959
Epoch 950, val loss: 1.3303848505020142
Epoch 960, training loss: 0.660163938999176 = 0.00802410114556551 + 0.1 * 6.521398544311523
Epoch 960, val loss: 1.3359978199005127
Epoch 970, training loss: 0.6588917374610901 = 0.007820739410817623 + 0.1 * 6.510709762573242
Epoch 970, val loss: 1.3415685892105103
Epoch 980, training loss: 0.6588524580001831 = 0.007625502068549395 + 0.1 * 6.512269496917725
Epoch 980, val loss: 1.3470522165298462
Epoch 990, training loss: 0.6595815420150757 = 0.00743830343708396 + 0.1 * 6.521432399749756
Epoch 990, val loss: 1.3524352312088013
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8376383763837639
The final CL Acc:0.79506, 0.00175, The final GNN Acc:0.83553, 0.00155
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10600])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.824441909790039 = 1.964759349822998 + 0.1 * 8.59682559967041
Epoch 0, val loss: 1.967050552368164
Epoch 10, training loss: 2.8138678073883057 = 1.9541927576065063 + 0.1 * 8.59675121307373
Epoch 10, val loss: 1.956038236618042
Epoch 20, training loss: 2.801150321960449 = 1.9415169954299927 + 0.1 * 8.596333503723145
Epoch 20, val loss: 1.9426000118255615
Epoch 30, training loss: 2.7834181785583496 = 1.9241091012954712 + 0.1 * 8.593090057373047
Epoch 30, val loss: 1.924249529838562
Epoch 40, training loss: 2.75585675239563 = 1.8988256454467773 + 0.1 * 8.570311546325684
Epoch 40, val loss: 1.897852897644043
Epoch 50, training loss: 2.7087268829345703 = 1.8635390996932983 + 0.1 * 8.451878547668457
Epoch 50, val loss: 1.8624992370605469
Epoch 60, training loss: 2.6371471881866455 = 1.8228306770324707 + 0.1 * 8.14316463470459
Epoch 60, val loss: 1.824836254119873
Epoch 70, training loss: 2.5568501949310303 = 1.7874654531478882 + 0.1 * 7.693846702575684
Epoch 70, val loss: 1.7949682474136353
Epoch 80, training loss: 2.4849328994750977 = 1.7557350397109985 + 0.1 * 7.291977405548096
Epoch 80, val loss: 1.767418384552002
Epoch 90, training loss: 2.433917760848999 = 1.718022346496582 + 0.1 * 7.158954620361328
Epoch 90, val loss: 1.73427414894104
Epoch 100, training loss: 2.379474639892578 = 1.668662190437317 + 0.1 * 7.108124256134033
Epoch 100, val loss: 1.692071557044983
Epoch 110, training loss: 2.3100595474243164 = 1.6037769317626953 + 0.1 * 7.0628252029418945
Epoch 110, val loss: 1.637386679649353
Epoch 120, training loss: 2.225332736968994 = 1.5231812000274658 + 0.1 * 7.021515846252441
Epoch 120, val loss: 1.5697237253189087
Epoch 130, training loss: 2.130155563354492 = 1.431431770324707 + 0.1 * 6.98723840713501
Epoch 130, val loss: 1.4942212104797363
Epoch 140, training loss: 2.0307724475860596 = 1.3348615169525146 + 0.1 * 6.959108829498291
Epoch 140, val loss: 1.4159830808639526
Epoch 150, training loss: 1.9303100109100342 = 1.236567497253418 + 0.1 * 6.93742561340332
Epoch 150, val loss: 1.3373059034347534
Epoch 160, training loss: 1.8295769691467285 = 1.1374874114990234 + 0.1 * 6.920895576477051
Epoch 160, val loss: 1.2578930854797363
Epoch 170, training loss: 1.7304108142852783 = 1.0391676425933838 + 0.1 * 6.912431716918945
Epoch 170, val loss: 1.1792817115783691
Epoch 180, training loss: 1.6361746788024902 = 0.945751965045929 + 0.1 * 6.9042277336120605
Epoch 180, val loss: 1.1048915386199951
Epoch 190, training loss: 1.5482527017593384 = 0.8584975600242615 + 0.1 * 6.897551536560059
Epoch 190, val loss: 1.0358113050460815
Epoch 200, training loss: 1.4686903953552246 = 0.7797386646270752 + 0.1 * 6.889516830444336
Epoch 200, val loss: 0.9747995734214783
Epoch 210, training loss: 1.398260235786438 = 0.7099353075027466 + 0.1 * 6.883249282836914
Epoch 210, val loss: 0.9230667948722839
Epoch 220, training loss: 1.3360495567321777 = 0.6481850147247314 + 0.1 * 6.878645896911621
Epoch 220, val loss: 0.880401074886322
Epoch 230, training loss: 1.2803963422775269 = 0.5928265452384949 + 0.1 * 6.875698089599609
Epoch 230, val loss: 0.8456067442893982
Epoch 240, training loss: 1.2285622358322144 = 0.5421759486198425 + 0.1 * 6.86386251449585
Epoch 240, val loss: 0.8168458342552185
Epoch 250, training loss: 1.1811937093734741 = 0.49491992592811584 + 0.1 * 6.86273717880249
Epoch 250, val loss: 0.7924781441688538
Epoch 260, training loss: 1.1363130807876587 = 0.45114246010780334 + 0.1 * 6.851706504821777
Epoch 260, val loss: 0.7719293236732483
Epoch 270, training loss: 1.0948960781097412 = 0.4105183780193329 + 0.1 * 6.843776702880859
Epoch 270, val loss: 0.7545300126075745
Epoch 280, training loss: 1.0565699338912964 = 0.37285009026527405 + 0.1 * 6.837198257446289
Epoch 280, val loss: 0.7401242852210999
Epoch 290, training loss: 1.0227525234222412 = 0.33809661865234375 + 0.1 * 6.846558570861816
Epoch 290, val loss: 0.7286791205406189
Epoch 300, training loss: 0.9885905385017395 = 0.30636632442474365 + 0.1 * 6.82224178314209
Epoch 300, val loss: 0.7199552059173584
Epoch 310, training loss: 0.9581138491630554 = 0.2769806981086731 + 0.1 * 6.811331272125244
Epoch 310, val loss: 0.7135118842124939
Epoch 320, training loss: 0.931120753288269 = 0.24965883791446686 + 0.1 * 6.814619064331055
Epoch 320, val loss: 0.7091509699821472
Epoch 330, training loss: 0.9040507674217224 = 0.22447456419467926 + 0.1 * 6.795762062072754
Epoch 330, val loss: 0.7067033648490906
Epoch 340, training loss: 0.8797248005867004 = 0.20118309557437897 + 0.1 * 6.785417079925537
Epoch 340, val loss: 0.7061072587966919
Epoch 350, training loss: 0.8595357537269592 = 0.17977602779865265 + 0.1 * 6.797597408294678
Epoch 350, val loss: 0.7073661684989929
Epoch 360, training loss: 0.8382439613342285 = 0.1604902446269989 + 0.1 * 6.777536869049072
Epoch 360, val loss: 0.7100680470466614
Epoch 370, training loss: 0.8191379904747009 = 0.1431136280298233 + 0.1 * 6.760243892669678
Epoch 370, val loss: 0.714476466178894
Epoch 380, training loss: 0.8034688830375671 = 0.1275557428598404 + 0.1 * 6.75913143157959
Epoch 380, val loss: 0.7202881574630737
Epoch 390, training loss: 0.7883078455924988 = 0.11383913457393646 + 0.1 * 6.744687080383301
Epoch 390, val loss: 0.7272862792015076
Epoch 400, training loss: 0.7758345603942871 = 0.10178978741168976 + 0.1 * 6.740447521209717
Epoch 400, val loss: 0.7352232933044434
Epoch 410, training loss: 0.765669584274292 = 0.0912126898765564 + 0.1 * 6.744568824768066
Epoch 410, val loss: 0.7440788149833679
Epoch 420, training loss: 0.7539921402931213 = 0.08199455589056015 + 0.1 * 6.71997594833374
Epoch 420, val loss: 0.7534539103507996
Epoch 430, training loss: 0.7454198002815247 = 0.0739324539899826 + 0.1 * 6.714873313903809
Epoch 430, val loss: 0.7634397149085999
Epoch 440, training loss: 0.7403304576873779 = 0.0668809786438942 + 0.1 * 6.734494686126709
Epoch 440, val loss: 0.773674488067627
Epoch 450, training loss: 0.7315760850906372 = 0.06075426563620567 + 0.1 * 6.708218097686768
Epoch 450, val loss: 0.7841140627861023
Epoch 460, training loss: 0.7245586514472961 = 0.05539463087916374 + 0.1 * 6.6916399002075195
Epoch 460, val loss: 0.794552206993103
Epoch 470, training loss: 0.7192683815956116 = 0.05067676305770874 + 0.1 * 6.685915946960449
Epoch 470, val loss: 0.8050395250320435
Epoch 480, training loss: 0.7149472236633301 = 0.046519603580236435 + 0.1 * 6.684276103973389
Epoch 480, val loss: 0.8155138492584229
Epoch 490, training loss: 0.7102434635162354 = 0.04286316782236099 + 0.1 * 6.673802852630615
Epoch 490, val loss: 0.8256173133850098
Epoch 500, training loss: 0.7077609300613403 = 0.03961847722530365 + 0.1 * 6.681424140930176
Epoch 500, val loss: 0.8357365131378174
Epoch 510, training loss: 0.7037549018859863 = 0.03673277050256729 + 0.1 * 6.670220851898193
Epoch 510, val loss: 0.8455843329429626
Epoch 520, training loss: 0.7010883688926697 = 0.0341532863676548 + 0.1 * 6.669350624084473
Epoch 520, val loss: 0.8552302718162537
Epoch 530, training loss: 0.6978815197944641 = 0.03184153884649277 + 0.1 * 6.660399913787842
Epoch 530, val loss: 0.8647561073303223
Epoch 540, training loss: 0.6953564286231995 = 0.02976054698228836 + 0.1 * 6.655958652496338
Epoch 540, val loss: 0.8740268349647522
Epoch 550, training loss: 0.6927697062492371 = 0.0278841070830822 + 0.1 * 6.648855686187744
Epoch 550, val loss: 0.8831655383110046
Epoch 560, training loss: 0.6906769275665283 = 0.02618749625980854 + 0.1 * 6.644894599914551
Epoch 560, val loss: 0.891945481300354
Epoch 570, training loss: 0.68849778175354 = 0.02464338205754757 + 0.1 * 6.638544082641602
Epoch 570, val loss: 0.9006320238113403
Epoch 580, training loss: 0.6867502331733704 = 0.023235339671373367 + 0.1 * 6.635149002075195
Epoch 580, val loss: 0.9091418981552124
Epoch 590, training loss: 0.6851692795753479 = 0.02194976434111595 + 0.1 * 6.632194995880127
Epoch 590, val loss: 0.9173928499221802
Epoch 600, training loss: 0.683197557926178 = 0.0207729060202837 + 0.1 * 6.624246120452881
Epoch 600, val loss: 0.9255110621452332
Epoch 610, training loss: 0.6826844215393066 = 0.01969262771308422 + 0.1 * 6.629918098449707
Epoch 610, val loss: 0.9333612322807312
Epoch 620, training loss: 0.68165522813797 = 0.01869993284344673 + 0.1 * 6.629552841186523
Epoch 620, val loss: 0.9411733150482178
Epoch 630, training loss: 0.6792986392974854 = 0.017787497490644455 + 0.1 * 6.615110874176025
Epoch 630, val loss: 0.9485027194023132
Epoch 640, training loss: 0.6792318820953369 = 0.01694134995341301 + 0.1 * 6.622905254364014
Epoch 640, val loss: 0.9558910131454468
Epoch 650, training loss: 0.6779154539108276 = 0.016158761456608772 + 0.1 * 6.6175665855407715
Epoch 650, val loss: 0.9630967974662781
Epoch 660, training loss: 0.6762629151344299 = 0.01543269120156765 + 0.1 * 6.608301639556885
Epoch 660, val loss: 0.9700854420661926
Epoch 670, training loss: 0.6766214966773987 = 0.014757346361875534 + 0.1 * 6.618641376495361
Epoch 670, val loss: 0.9769477844238281
Epoch 680, training loss: 0.6750826835632324 = 0.014129008166491985 + 0.1 * 6.609536647796631
Epoch 680, val loss: 0.9837535619735718
Epoch 690, training loss: 0.6742079257965088 = 0.013544616289436817 + 0.1 * 6.606632709503174
Epoch 690, val loss: 0.9901844263076782
Epoch 700, training loss: 0.673041045665741 = 0.012996788136661053 + 0.1 * 6.600442409515381
Epoch 700, val loss: 0.9966990351676941
Epoch 710, training loss: 0.6729350090026855 = 0.012485308572649956 + 0.1 * 6.604496479034424
Epoch 710, val loss: 1.0029278993606567
Epoch 720, training loss: 0.6717699766159058 = 0.012005210854113102 + 0.1 * 6.597647666931152
Epoch 720, val loss: 1.009186863899231
Epoch 730, training loss: 0.6708256006240845 = 0.01155563909560442 + 0.1 * 6.5926995277404785
Epoch 730, val loss: 1.0150867700576782
Epoch 740, training loss: 0.6709164381027222 = 0.0111316479742527 + 0.1 * 6.597847938537598
Epoch 740, val loss: 1.021216630935669
Epoch 750, training loss: 0.6700087189674377 = 0.010734223760664463 + 0.1 * 6.592745304107666
Epoch 750, val loss: 1.0269125699996948
Epoch 760, training loss: 0.6693554520606995 = 0.010359518229961395 + 0.1 * 6.589959144592285
Epoch 760, val loss: 1.0326308012008667
Epoch 770, training loss: 0.6682750582695007 = 0.010006875731050968 + 0.1 * 6.582681179046631
Epoch 770, val loss: 1.0381654500961304
Epoch 780, training loss: 0.6684813499450684 = 0.009673229418694973 + 0.1 * 6.588080883026123
Epoch 780, val loss: 1.0436426401138306
Epoch 790, training loss: 0.6672583222389221 = 0.009357263334095478 + 0.1 * 6.579010486602783
Epoch 790, val loss: 1.049147367477417
Epoch 800, training loss: 0.6701545119285583 = 0.009059513919055462 + 0.1 * 6.610949993133545
Epoch 800, val loss: 1.05446195602417
Epoch 810, training loss: 0.6656631231307983 = 0.008777214214205742 + 0.1 * 6.568859100341797
Epoch 810, val loss: 1.0596953630447388
Epoch 820, training loss: 0.6654077768325806 = 0.008510508574545383 + 0.1 * 6.568972110748291
Epoch 820, val loss: 1.0646106004714966
Epoch 830, training loss: 0.6682177782058716 = 0.008256559260189533 + 0.1 * 6.599612236022949
Epoch 830, val loss: 1.069695234298706
Epoch 840, training loss: 0.6651856899261475 = 0.008015206083655357 + 0.1 * 6.571704387664795
Epoch 840, val loss: 1.0747432708740234
Epoch 850, training loss: 0.6642125844955444 = 0.0077871535904705524 + 0.1 * 6.564253807067871
Epoch 850, val loss: 1.0793200731277466
Epoch 860, training loss: 0.6638838052749634 = 0.00756859639659524 + 0.1 * 6.563151836395264
Epoch 860, val loss: 1.0840944051742554
Epoch 870, training loss: 0.6634529232978821 = 0.007360042538493872 + 0.1 * 6.560928821563721
Epoch 870, val loss: 1.0889651775360107
Epoch 880, training loss: 0.6637923717498779 = 0.007162394933402538 + 0.1 * 6.5662994384765625
Epoch 880, val loss: 1.093408226966858
Epoch 890, training loss: 0.6624810695648193 = 0.006973099894821644 + 0.1 * 6.555079936981201
Epoch 890, val loss: 1.0979018211364746
Epoch 900, training loss: 0.661888837814331 = 0.006793087348341942 + 0.1 * 6.550957202911377
Epoch 900, val loss: 1.1022069454193115
Epoch 910, training loss: 0.6619510054588318 = 0.006620396859943867 + 0.1 * 6.553305625915527
Epoch 910, val loss: 1.1064997911453247
Epoch 920, training loss: 0.6617239117622375 = 0.006455036345869303 + 0.1 * 6.5526885986328125
Epoch 920, val loss: 1.1109248399734497
Epoch 930, training loss: 0.6615256667137146 = 0.006297222338616848 + 0.1 * 6.552284240722656
Epoch 930, val loss: 1.115077257156372
Epoch 940, training loss: 0.6603213548660278 = 0.006145648192614317 + 0.1 * 6.541757106781006
Epoch 940, val loss: 1.119314432144165
Epoch 950, training loss: 0.6598904728889465 = 0.0060012307949364185 + 0.1 * 6.5388922691345215
Epoch 950, val loss: 1.1232458353042603
Epoch 960, training loss: 0.6596164703369141 = 0.0058624944649636745 + 0.1 * 6.537539958953857
Epoch 960, val loss: 1.1271613836288452
Epoch 970, training loss: 0.6594022512435913 = 0.0057285381481051445 + 0.1 * 6.536736965179443
Epoch 970, val loss: 1.1311599016189575
Epoch 980, training loss: 0.6590920686721802 = 0.005599913652986288 + 0.1 * 6.534921169281006
Epoch 980, val loss: 1.1353217363357544
Epoch 990, training loss: 0.6585423350334167 = 0.0054774899035692215 + 0.1 * 6.530648231506348
Epoch 990, val loss: 1.1389563083648682
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 2.8149595260620117 = 1.9552747011184692 + 0.1 * 8.596848487854004
Epoch 0, val loss: 1.95966637134552
Epoch 10, training loss: 2.804819107055664 = 1.945142388343811 + 0.1 * 8.596768379211426
Epoch 10, val loss: 1.949188470840454
Epoch 20, training loss: 2.792191505432129 = 1.9325459003448486 + 0.1 * 8.596455574035645
Epoch 20, val loss: 1.9359017610549927
Epoch 30, training loss: 2.7741589546203613 = 1.9147306680679321 + 0.1 * 8.594283103942871
Epoch 30, val loss: 1.9166241884231567
Epoch 40, training loss: 2.746242046356201 = 1.88833487033844 + 0.1 * 8.579072952270508
Epoch 40, val loss: 1.88798987865448
Epoch 50, training loss: 2.702688217163086 = 1.8516738414764404 + 0.1 * 8.510143280029297
Epoch 50, val loss: 1.8495428562164307
Epoch 60, training loss: 2.637014389038086 = 1.8111895322799683 + 0.1 * 8.258248329162598
Epoch 60, val loss: 1.8111573457717896
Epoch 70, training loss: 2.570594072341919 = 1.7766773700714111 + 0.1 * 7.939167499542236
Epoch 70, val loss: 1.7814589738845825
Epoch 80, training loss: 2.4908628463745117 = 1.7403135299682617 + 0.1 * 7.505494117736816
Epoch 80, val loss: 1.7502150535583496
Epoch 90, training loss: 2.4258360862731934 = 1.6953679323196411 + 0.1 * 7.304680824279785
Epoch 90, val loss: 1.711743712425232
Epoch 100, training loss: 2.356624126434326 = 1.636784553527832 + 0.1 * 7.1983962059021
Epoch 100, val loss: 1.6616039276123047
Epoch 110, training loss: 2.2759532928466797 = 1.5618507862091064 + 0.1 * 7.141024589538574
Epoch 110, val loss: 1.5976057052612305
Epoch 120, training loss: 2.18475341796875 = 1.473426342010498 + 0.1 * 7.113271236419678
Epoch 120, val loss: 1.5224997997283936
Epoch 130, training loss: 2.087007522583008 = 1.3780403137207031 + 0.1 * 7.089672088623047
Epoch 130, val loss: 1.4427366256713867
Epoch 140, training loss: 1.9855372905731201 = 1.2791787385940552 + 0.1 * 7.06358528137207
Epoch 140, val loss: 1.3632456064224243
Epoch 150, training loss: 1.8814196586608887 = 1.1783329248428345 + 0.1 * 7.030867576599121
Epoch 150, val loss: 1.2845125198364258
Epoch 160, training loss: 1.778502345085144 = 1.078645944595337 + 0.1 * 6.998563766479492
Epoch 160, val loss: 1.2087669372558594
Epoch 170, training loss: 1.682828426361084 = 0.9847879409790039 + 0.1 * 6.980403900146484
Epoch 170, val loss: 1.1388473510742188
Epoch 180, training loss: 1.5961174964904785 = 0.9002427458763123 + 0.1 * 6.958747863769531
Epoch 180, val loss: 1.0767196416854858
Epoch 190, training loss: 1.5188173055648804 = 0.8245046734809875 + 0.1 * 6.943126201629639
Epoch 190, val loss: 1.0219323635101318
Epoch 200, training loss: 1.4506258964538574 = 0.757660984992981 + 0.1 * 6.929649829864502
Epoch 200, val loss: 0.9747884273529053
Epoch 210, training loss: 1.3904491662979126 = 0.6986123323440552 + 0.1 * 6.918368339538574
Epoch 210, val loss: 0.9353733658790588
Epoch 220, training loss: 1.336402416229248 = 0.6458351612091064 + 0.1 * 6.905673027038574
Epoch 220, val loss: 0.9027952551841736
Epoch 230, training loss: 1.2864253520965576 = 0.5969914793968201 + 0.1 * 6.894338607788086
Epoch 230, val loss: 0.8754963874816895
Epoch 240, training loss: 1.2385683059692383 = 0.550245463848114 + 0.1 * 6.883227825164795
Epoch 240, val loss: 0.8523045182228088
Epoch 250, training loss: 1.1930699348449707 = 0.5052666664123535 + 0.1 * 6.8780317306518555
Epoch 250, val loss: 0.8324471116065979
Epoch 260, training loss: 1.1482820510864258 = 0.4616771638393402 + 0.1 * 6.866049289703369
Epoch 260, val loss: 0.81513512134552
Epoch 270, training loss: 1.104121446609497 = 0.41886526346206665 + 0.1 * 6.852561950683594
Epoch 270, val loss: 0.7999624609947205
Epoch 280, training loss: 1.0643420219421387 = 0.37694287300109863 + 0.1 * 6.873991012573242
Epoch 280, val loss: 0.7872709035873413
Epoch 290, training loss: 1.0210106372833252 = 0.3370022177696228 + 0.1 * 6.840084552764893
Epoch 290, val loss: 0.7772758603096008
Epoch 300, training loss: 0.9821021556854248 = 0.2992325723171234 + 0.1 * 6.828696250915527
Epoch 300, val loss: 0.7699365615844727
Epoch 310, training loss: 0.9470179677009583 = 0.26414841413497925 + 0.1 * 6.828695297241211
Epoch 310, val loss: 0.7652963995933533
Epoch 320, training loss: 0.9141114950180054 = 0.2324984073638916 + 0.1 * 6.816130638122559
Epoch 320, val loss: 0.7635558247566223
Epoch 330, training loss: 0.8854581117630005 = 0.20443177223205566 + 0.1 * 6.810263156890869
Epoch 330, val loss: 0.7645179629325867
Epoch 340, training loss: 0.8601901531219482 = 0.17984101176261902 + 0.1 * 6.803491592407227
Epoch 340, val loss: 0.7679857611656189
Epoch 350, training loss: 0.8382993936538696 = 0.15855219960212708 + 0.1 * 6.79747200012207
Epoch 350, val loss: 0.7737199068069458
Epoch 360, training loss: 0.8194872140884399 = 0.1402996927499771 + 0.1 * 6.79187536239624
Epoch 360, val loss: 0.7812596559524536
Epoch 370, training loss: 0.803784966468811 = 0.1246056854724884 + 0.1 * 6.791792869567871
Epoch 370, val loss: 0.7903788685798645
Epoch 380, training loss: 0.7890912294387817 = 0.11118045449256897 + 0.1 * 6.779107570648193
Epoch 380, val loss: 0.8006513118743896
Epoch 390, training loss: 0.7768595218658447 = 0.09962490946054459 + 0.1 * 6.772345542907715
Epoch 390, val loss: 0.8118320107460022
Epoch 400, training loss: 0.7656623125076294 = 0.08959520608186722 + 0.1 * 6.760671138763428
Epoch 400, val loss: 0.8237772583961487
Epoch 410, training loss: 0.7584757208824158 = 0.08085217326879501 + 0.1 * 6.776235580444336
Epoch 410, val loss: 0.8363126516342163
Epoch 420, training loss: 0.7486096620559692 = 0.07326063513755798 + 0.1 * 6.753490447998047
Epoch 420, val loss: 0.8491747975349426
Epoch 430, training loss: 0.7411335110664368 = 0.06662128120660782 + 0.1 * 6.745121955871582
Epoch 430, val loss: 0.8624000549316406
Epoch 440, training loss: 0.7355141639709473 = 0.06078512221574783 + 0.1 * 6.747290134429932
Epoch 440, val loss: 0.8757970333099365
Epoch 450, training loss: 0.729669988155365 = 0.055652253329753876 + 0.1 * 6.740177154541016
Epoch 450, val loss: 0.8890691995620728
Epoch 460, training loss: 0.7235074639320374 = 0.05112528055906296 + 0.1 * 6.723821640014648
Epoch 460, val loss: 0.9024980068206787
Epoch 470, training loss: 0.7187599539756775 = 0.04711105301976204 + 0.1 * 6.716488838195801
Epoch 470, val loss: 0.9157085418701172
Epoch 480, training loss: 0.7160288095474243 = 0.04353184252977371 + 0.1 * 6.724969863891602
Epoch 480, val loss: 0.9288100600242615
Epoch 490, training loss: 0.7107974290847778 = 0.040349334478378296 + 0.1 * 6.7044806480407715
Epoch 490, val loss: 0.941584587097168
Epoch 500, training loss: 0.7071821689605713 = 0.03749831020832062 + 0.1 * 6.69683837890625
Epoch 500, val loss: 0.9542648792266846
Epoch 510, training loss: 0.7044355273246765 = 0.03493674099445343 + 0.1 * 6.694987773895264
Epoch 510, val loss: 0.9665990471839905
Epoch 520, training loss: 0.7028940320014954 = 0.032630804926157 + 0.1 * 6.702632427215576
Epoch 520, val loss: 0.9787301421165466
Epoch 530, training loss: 0.6979281306266785 = 0.030548708513379097 + 0.1 * 6.673794269561768
Epoch 530, val loss: 0.9903867244720459
Epoch 540, training loss: 0.6962301135063171 = 0.028659390285611153 + 0.1 * 6.67570686340332
Epoch 540, val loss: 1.0019643306732178
Epoch 550, training loss: 0.6932262182235718 = 0.026943597942590714 + 0.1 * 6.662825584411621
Epoch 550, val loss: 1.0131275653839111
Epoch 560, training loss: 0.6911055445671082 = 0.025376399978995323 + 0.1 * 6.657291412353516
Epoch 560, val loss: 1.0241461992263794
Epoch 570, training loss: 0.6898304224014282 = 0.023946987465023994 + 0.1 * 6.658833980560303
Epoch 570, val loss: 1.0347155332565308
Epoch 580, training loss: 0.6869127750396729 = 0.022640317678451538 + 0.1 * 6.642724514007568
Epoch 580, val loss: 1.0450212955474854
Epoch 590, training loss: 0.6865649223327637 = 0.021439462900161743 + 0.1 * 6.651254653930664
Epoch 590, val loss: 1.0550676584243774
Epoch 600, training loss: 0.683763861656189 = 0.02033536694943905 + 0.1 * 6.634284973144531
Epoch 600, val loss: 1.0648698806762695
Epoch 610, training loss: 0.6822545528411865 = 0.01931579038500786 + 0.1 * 6.629387378692627
Epoch 610, val loss: 1.074455976486206
Epoch 620, training loss: 0.6815616488456726 = 0.01837717741727829 + 0.1 * 6.631844520568848
Epoch 620, val loss: 1.0837234258651733
Epoch 630, training loss: 0.6798027157783508 = 0.017504330724477768 + 0.1 * 6.622983455657959
Epoch 630, val loss: 1.0928341150283813
Epoch 640, training loss: 0.6801105737686157 = 0.016696704551577568 + 0.1 * 6.634138584136963
Epoch 640, val loss: 1.1017286777496338
Epoch 650, training loss: 0.6772092580795288 = 0.015944935381412506 + 0.1 * 6.612642765045166
Epoch 650, val loss: 1.1104027032852173
Epoch 660, training loss: 0.6769939661026001 = 0.015248099341988564 + 0.1 * 6.617458343505859
Epoch 660, val loss: 1.1187385320663452
Epoch 670, training loss: 0.6750337481498718 = 0.014598115347325802 + 0.1 * 6.604355812072754
Epoch 670, val loss: 1.1270617246627808
Epoch 680, training loss: 0.6741961240768433 = 0.013988466002047062 + 0.1 * 6.602076053619385
Epoch 680, val loss: 1.1351255178451538
Epoch 690, training loss: 0.6743996143341064 = 0.013419585302472115 + 0.1 * 6.609800338745117
Epoch 690, val loss: 1.1429669857025146
Epoch 700, training loss: 0.6721205115318298 = 0.012890173122286797 + 0.1 * 6.592303276062012
Epoch 700, val loss: 1.1505355834960938
Epoch 710, training loss: 0.671402096748352 = 0.012392047792673111 + 0.1 * 6.590100288391113
Epoch 710, val loss: 1.1581424474716187
Epoch 720, training loss: 0.6696546077728271 = 0.011925064958631992 + 0.1 * 6.577295303344727
Epoch 720, val loss: 1.1654465198516846
Epoch 730, training loss: 0.669083297252655 = 0.011485636234283447 + 0.1 * 6.575976371765137
Epoch 730, val loss: 1.1726417541503906
Epoch 740, training loss: 0.6692211031913757 = 0.011072932742536068 + 0.1 * 6.581481456756592
Epoch 740, val loss: 1.17972993850708
Epoch 750, training loss: 0.6675058603286743 = 0.01068385224789381 + 0.1 * 6.568220138549805
Epoch 750, val loss: 1.1865487098693848
Epoch 760, training loss: 0.6680740714073181 = 0.010317104868590832 + 0.1 * 6.577569484710693
Epoch 760, val loss: 1.1933209896087646
Epoch 770, training loss: 0.6667585372924805 = 0.009969462640583515 + 0.1 * 6.567890644073486
Epoch 770, val loss: 1.1999858617782593
Epoch 780, training loss: 0.6676718592643738 = 0.009642998687922955 + 0.1 * 6.580288410186768
Epoch 780, val loss: 1.2064249515533447
Epoch 790, training loss: 0.6653587818145752 = 0.009332679212093353 + 0.1 * 6.560261249542236
Epoch 790, val loss: 1.2127470970153809
Epoch 800, training loss: 0.664965033531189 = 0.00904018897563219 + 0.1 * 6.559248447418213
Epoch 800, val loss: 1.2189199924468994
Epoch 810, training loss: 0.6637953519821167 = 0.008761318400502205 + 0.1 * 6.550339698791504
Epoch 810, val loss: 1.225056529045105
Epoch 820, training loss: 0.6630913615226746 = 0.008498077280819416 + 0.1 * 6.545932769775391
Epoch 820, val loss: 1.2309668064117432
Epoch 830, training loss: 0.6625942587852478 = 0.008245282806456089 + 0.1 * 6.543489456176758
Epoch 830, val loss: 1.2369338274002075
Epoch 840, training loss: 0.6617109179496765 = 0.008006065152585506 + 0.1 * 6.537048816680908
Epoch 840, val loss: 1.2426941394805908
Epoch 850, training loss: 0.662238359451294 = 0.007778959348797798 + 0.1 * 6.5445942878723145
Epoch 850, val loss: 1.2484203577041626
Epoch 860, training loss: 0.661878228187561 = 0.007560448721051216 + 0.1 * 6.543177604675293
Epoch 860, val loss: 1.2540684938430786
Epoch 870, training loss: 0.6605960130691528 = 0.007354198023676872 + 0.1 * 6.532418251037598
Epoch 870, val loss: 1.2594997882843018
Epoch 880, training loss: 0.6594851016998291 = 0.007157137617468834 + 0.1 * 6.523279190063477
Epoch 880, val loss: 1.2648340463638306
Epoch 890, training loss: 0.6582229733467102 = 0.00696889404207468 + 0.1 * 6.512540340423584
Epoch 890, val loss: 1.2702243328094482
Epoch 900, training loss: 0.6598748564720154 = 0.006789217237383127 + 0.1 * 6.530856132507324
Epoch 900, val loss: 1.2754102945327759
Epoch 910, training loss: 0.6591468453407288 = 0.0066163195297122 + 0.1 * 6.525305271148682
Epoch 910, val loss: 1.2805927991867065
Epoch 920, training loss: 0.6578065156936646 = 0.006452084518969059 + 0.1 * 6.513544082641602
Epoch 920, val loss: 1.2855228185653687
Epoch 930, training loss: 0.6578576564788818 = 0.006294390186667442 + 0.1 * 6.515632629394531
Epoch 930, val loss: 1.290529489517212
Epoch 940, training loss: 0.6570723056793213 = 0.006142419762909412 + 0.1 * 6.509298801422119
Epoch 940, val loss: 1.2954862117767334
Epoch 950, training loss: 0.6564362645149231 = 0.005998322274535894 + 0.1 * 6.5043792724609375
Epoch 950, val loss: 1.3001362085342407
Epoch 960, training loss: 0.6552914381027222 = 0.005858756601810455 + 0.1 * 6.494326591491699
Epoch 960, val loss: 1.304916501045227
Epoch 970, training loss: 0.6554173827171326 = 0.005725111812353134 + 0.1 * 6.496922969818115
Epoch 970, val loss: 1.3095844984054565
Epoch 980, training loss: 0.6552480459213257 = 0.00559611339122057 + 0.1 * 6.496519088745117
Epoch 980, val loss: 1.3141944408416748
Epoch 990, training loss: 0.6548016667366028 = 0.005472538527101278 + 0.1 * 6.49329137802124
Epoch 990, val loss: 1.3186956644058228
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 2.827033281326294 = 1.967353343963623 + 0.1 * 8.59679889678955
Epoch 0, val loss: 1.9708342552185059
Epoch 10, training loss: 2.8151729106903076 = 1.9555039405822754 + 0.1 * 8.59669017791748
Epoch 10, val loss: 1.958372712135315
Epoch 20, training loss: 2.8003273010253906 = 1.9407180547714233 + 0.1 * 8.596092224121094
Epoch 20, val loss: 1.942506194114685
Epoch 30, training loss: 2.77898907661438 = 1.9198390245437622 + 0.1 * 8.591500282287598
Epoch 30, val loss: 1.9200828075408936
Epoch 40, training loss: 2.746083974838257 = 1.889447569847107 + 0.1 * 8.566364288330078
Epoch 40, val loss: 1.8881717920303345
Epoch 50, training loss: 2.69679856300354 = 1.849525809288025 + 0.1 * 8.472726821899414
Epoch 50, val loss: 1.8490287065505981
Epoch 60, training loss: 2.641054153442383 = 1.8102753162384033 + 0.1 * 8.307787895202637
Epoch 60, val loss: 1.8153821229934692
Epoch 70, training loss: 2.587090015411377 = 1.7812869548797607 + 0.1 * 8.05803108215332
Epoch 70, val loss: 1.7922571897506714
Epoch 80, training loss: 2.506901979446411 = 1.7491440773010254 + 0.1 * 7.577578067779541
Epoch 80, val loss: 1.763240933418274
Epoch 90, training loss: 2.4418187141418457 = 1.7081505060195923 + 0.1 * 7.336682319641113
Epoch 90, val loss: 1.727197289466858
Epoch 100, training loss: 2.3723065853118896 = 1.652938723564148 + 0.1 * 7.193679332733154
Epoch 100, val loss: 1.6810261011123657
Epoch 110, training loss: 2.2910354137420654 = 1.5822468996047974 + 0.1 * 7.087884902954102
Epoch 110, val loss: 1.6216355562210083
Epoch 120, training loss: 2.206101894378662 = 1.5024125576019287 + 0.1 * 7.036892890930176
Epoch 120, val loss: 1.5555331707000732
Epoch 130, training loss: 2.122999906539917 = 1.4225521087646484 + 0.1 * 7.004477500915527
Epoch 130, val loss: 1.491493582725525
Epoch 140, training loss: 2.0435585975646973 = 1.3451189994812012 + 0.1 * 6.9843950271606445
Epoch 140, val loss: 1.4325718879699707
Epoch 150, training loss: 1.9669346809387207 = 1.270087718963623 + 0.1 * 6.968469142913818
Epoch 150, val loss: 1.3782720565795898
Epoch 160, training loss: 1.8924063444137573 = 1.1973570585250854 + 0.1 * 6.950492858886719
Epoch 160, val loss: 1.3272756338119507
Epoch 170, training loss: 1.8200263977050781 = 1.126845359802246 + 0.1 * 6.931809425354004
Epoch 170, val loss: 1.279348611831665
Epoch 180, training loss: 1.751084804534912 = 1.0595632791519165 + 0.1 * 6.915215492248535
Epoch 180, val loss: 1.2349947690963745
Epoch 190, training loss: 1.6860826015472412 = 0.9960962533950806 + 0.1 * 6.899862766265869
Epoch 190, val loss: 1.1938834190368652
Epoch 200, training loss: 1.6248564720153809 = 0.9363784790039062 + 0.1 * 6.884780406951904
Epoch 200, val loss: 1.155529499053955
Epoch 210, training loss: 1.5662517547607422 = 0.8790275454521179 + 0.1 * 6.872242450714111
Epoch 210, val loss: 1.1189614534378052
Epoch 220, training loss: 1.508104920387268 = 0.8216954469680786 + 0.1 * 6.8640947341918945
Epoch 220, val loss: 1.0819885730743408
Epoch 230, training loss: 1.4484484195709229 = 0.7628936767578125 + 0.1 * 6.855546474456787
Epoch 230, val loss: 1.043894648551941
Epoch 240, training loss: 1.3854209184646606 = 0.701363742351532 + 0.1 * 6.840571880340576
Epoch 240, val loss: 1.0033763647079468
Epoch 250, training loss: 1.319714903831482 = 0.636718213558197 + 0.1 * 6.829967021942139
Epoch 250, val loss: 0.960400402545929
Epoch 260, training loss: 1.2533282041549683 = 0.5711016058921814 + 0.1 * 6.822265625
Epoch 260, val loss: 0.9173747897148132
Epoch 270, training loss: 1.1874873638153076 = 0.5069657564163208 + 0.1 * 6.805215358734131
Epoch 270, val loss: 0.8771687150001526
Epoch 280, training loss: 1.1265575885772705 = 0.44669461250305176 + 0.1 * 6.798630237579346
Epoch 280, val loss: 0.8429983854293823
Epoch 290, training loss: 1.0712478160858154 = 0.3926924169063568 + 0.1 * 6.7855544090271
Epoch 290, val loss: 0.8171179294586182
Epoch 300, training loss: 1.024133324623108 = 0.34533676505088806 + 0.1 * 6.787965297698975
Epoch 300, val loss: 0.7998565435409546
Epoch 310, training loss: 0.9811967611312866 = 0.3045530617237091 + 0.1 * 6.76643705368042
Epoch 310, val loss: 0.7902960181236267
Epoch 320, training loss: 0.9448636770248413 = 0.26914182305336 + 0.1 * 6.757217884063721
Epoch 320, val loss: 0.786719799041748
Epoch 330, training loss: 0.9136717915534973 = 0.23832879960536957 + 0.1 * 6.753429412841797
Epoch 330, val loss: 0.7876874804496765
Epoch 340, training loss: 0.8857898116111755 = 0.21151213347911835 + 0.1 * 6.742776393890381
Epoch 340, val loss: 0.7921092510223389
Epoch 350, training loss: 0.8607776761054993 = 0.18794941902160645 + 0.1 * 6.728282451629639
Epoch 350, val loss: 0.7991954684257507
Epoch 360, training loss: 0.8397107124328613 = 0.16736552119255066 + 0.1 * 6.723451614379883
Epoch 360, val loss: 0.8083481192588806
Epoch 370, training loss: 0.8203924894332886 = 0.14949637651443481 + 0.1 * 6.708961009979248
Epoch 370, val loss: 0.8189523816108704
Epoch 380, training loss: 0.8068471550941467 = 0.1339692324399948 + 0.1 * 6.728778839111328
Epoch 380, val loss: 0.8306432366371155
Epoch 390, training loss: 0.7901525497436523 = 0.12057382613420486 + 0.1 * 6.695786952972412
Epoch 390, val loss: 0.8429890275001526
Epoch 400, training loss: 0.7776976227760315 = 0.10890448093414307 + 0.1 * 6.687931060791016
Epoch 400, val loss: 0.8557291030883789
Epoch 410, training loss: 0.77010577917099 = 0.09870810061693192 + 0.1 * 6.713976860046387
Epoch 410, val loss: 0.8687304854393005
Epoch 420, training loss: 0.7579189538955688 = 0.0898454487323761 + 0.1 * 6.680734634399414
Epoch 420, val loss: 0.8817027807235718
Epoch 430, training loss: 0.7492598295211792 = 0.08206485956907272 + 0.1 * 6.67194938659668
Epoch 430, val loss: 0.8946108818054199
Epoch 440, training loss: 0.7413670420646667 = 0.07520104199647903 + 0.1 * 6.6616597175598145
Epoch 440, val loss: 0.9075002670288086
Epoch 450, training loss: 0.7366374731063843 = 0.06912056356668472 + 0.1 * 6.675168991088867
Epoch 450, val loss: 0.9202116131782532
Epoch 460, training loss: 0.7307769656181335 = 0.06373599916696548 + 0.1 * 6.670409202575684
Epoch 460, val loss: 0.9327732920646667
Epoch 470, training loss: 0.7244216799736023 = 0.058959007263183594 + 0.1 * 6.654626369476318
Epoch 470, val loss: 0.9449872970581055
Epoch 480, training loss: 0.718276858329773 = 0.054685886949300766 + 0.1 * 6.635909557342529
Epoch 480, val loss: 0.9570059180259705
Epoch 490, training loss: 0.715947687625885 = 0.05084659531712532 + 0.1 * 6.651010990142822
Epoch 490, val loss: 0.9688324928283691
Epoch 500, training loss: 0.7103633284568787 = 0.04739990457892418 + 0.1 * 6.629634380340576
Epoch 500, val loss: 0.9803933501243591
Epoch 510, training loss: 0.7089211940765381 = 0.04428945854306221 + 0.1 * 6.646317005157471
Epoch 510, val loss: 0.99168461561203
Epoch 520, training loss: 0.7028656601905823 = 0.04148835316300392 + 0.1 * 6.613772869110107
Epoch 520, val loss: 1.0026997327804565
Epoch 530, training loss: 0.7004724740982056 = 0.03895160183310509 + 0.1 * 6.615209102630615
Epoch 530, val loss: 1.0133562088012695
Epoch 540, training loss: 0.6972280144691467 = 0.036637306213378906 + 0.1 * 6.605906963348389
Epoch 540, val loss: 1.0238169431686401
Epoch 550, training loss: 0.6952875852584839 = 0.034521158784627914 + 0.1 * 6.607664108276367
Epoch 550, val loss: 1.0340608358383179
Epoch 560, training loss: 0.691969096660614 = 0.03258582577109337 + 0.1 * 6.593832969665527
Epoch 560, val loss: 1.0440561771392822
Epoch 570, training loss: 0.6920216679573059 = 0.030807802453637123 + 0.1 * 6.612138748168945
Epoch 570, val loss: 1.053843379020691
Epoch 580, training loss: 0.6879776120185852 = 0.029175138100981712 + 0.1 * 6.588024616241455
Epoch 580, val loss: 1.0634255409240723
Epoch 590, training loss: 0.6859524846076965 = 0.02767280861735344 + 0.1 * 6.582796573638916
Epoch 590, val loss: 1.07275390625
Epoch 600, training loss: 0.6845329999923706 = 0.02628212608397007 + 0.1 * 6.582508563995361
Epoch 600, val loss: 1.081911563873291
Epoch 610, training loss: 0.681981086730957 = 0.024997267872095108 + 0.1 * 6.56983757019043
Epoch 610, val loss: 1.0907979011535645
Epoch 620, training loss: 0.6812808513641357 = 0.023806065320968628 + 0.1 * 6.5747480392456055
Epoch 620, val loss: 1.0995255708694458
Epoch 630, training loss: 0.6790220141410828 = 0.022697897627949715 + 0.1 * 6.5632405281066895
Epoch 630, val loss: 1.108051061630249
Epoch 640, training loss: 0.6777467131614685 = 0.02166563645005226 + 0.1 * 6.560810565948486
Epoch 640, val loss: 1.116384506225586
Epoch 650, training loss: 0.6759512424468994 = 0.020704397931694984 + 0.1 * 6.552468299865723
Epoch 650, val loss: 1.1245306730270386
Epoch 660, training loss: 0.6751043796539307 = 0.01980653405189514 + 0.1 * 6.552978515625
Epoch 660, val loss: 1.1324650049209595
Epoch 670, training loss: 0.6733878254890442 = 0.01896582543849945 + 0.1 * 6.544219970703125
Epoch 670, val loss: 1.1403007507324219
Epoch 680, training loss: 0.675779402256012 = 0.018179967999458313 + 0.1 * 6.575994491577148
Epoch 680, val loss: 1.1479045152664185
Epoch 690, training loss: 0.6720954775810242 = 0.017445102334022522 + 0.1 * 6.546504020690918
Epoch 690, val loss: 1.1553388833999634
Epoch 700, training loss: 0.6707552075386047 = 0.01675492711365223 + 0.1 * 6.540002822875977
Epoch 700, val loss: 1.162580966949463
Epoch 710, training loss: 0.6704124808311462 = 0.016104070469737053 + 0.1 * 6.543084144592285
Epoch 710, val loss: 1.169708490371704
Epoch 720, training loss: 0.6681053042411804 = 0.015492196194827557 + 0.1 * 6.526130676269531
Epoch 720, val loss: 1.1766897439956665
Epoch 730, training loss: 0.6688228845596313 = 0.0149153433740139 + 0.1 * 6.5390753746032715
Epoch 730, val loss: 1.1835410594940186
Epoch 740, training loss: 0.6673033237457275 = 0.014371400699019432 + 0.1 * 6.5293192863464355
Epoch 740, val loss: 1.1902222633361816
Epoch 750, training loss: 0.6688915491104126 = 0.013858005404472351 + 0.1 * 6.55033540725708
Epoch 750, val loss: 1.196765422821045
Epoch 760, training loss: 0.6657675504684448 = 0.013373131863772869 + 0.1 * 6.523943901062012
Epoch 760, val loss: 1.2032015323638916
Epoch 770, training loss: 0.6652883887290955 = 0.012915519066154957 + 0.1 * 6.523728847503662
Epoch 770, val loss: 1.2094461917877197
Epoch 780, training loss: 0.6634236574172974 = 0.012480647303164005 + 0.1 * 6.509429931640625
Epoch 780, val loss: 1.2156400680541992
Epoch 790, training loss: 0.6638443470001221 = 0.012068811804056168 + 0.1 * 6.517755508422852
Epoch 790, val loss: 1.221652865409851
Epoch 800, training loss: 0.6622462868690491 = 0.01167700719088316 + 0.1 * 6.505692481994629
Epoch 800, val loss: 1.2275707721710205
Epoch 810, training loss: 0.6646811366081238 = 0.011305737309157848 + 0.1 * 6.533753871917725
Epoch 810, val loss: 1.2333976030349731
Epoch 820, training loss: 0.6612671613693237 = 0.010953389108181 + 0.1 * 6.503138065338135
Epoch 820, val loss: 1.239050030708313
Epoch 830, training loss: 0.6599622964859009 = 0.01061816606670618 + 0.1 * 6.493441581726074
Epoch 830, val loss: 1.2446233034133911
Epoch 840, training loss: 0.6602786779403687 = 0.010297713801264763 + 0.1 * 6.499809265136719
Epoch 840, val loss: 1.2501170635223389
Epoch 850, training loss: 0.6607264280319214 = 0.009991711936891079 + 0.1 * 6.507347106933594
Epoch 850, val loss: 1.2555310726165771
Epoch 860, training loss: 0.6589281558990479 = 0.009701766073703766 + 0.1 * 6.4922637939453125
Epoch 860, val loss: 1.2607824802398682
Epoch 870, training loss: 0.6608064770698547 = 0.009424451738595963 + 0.1 * 6.513820171356201
Epoch 870, val loss: 1.2660032510757446
Epoch 880, training loss: 0.6582093238830566 = 0.009160099551081657 + 0.1 * 6.490492343902588
Epoch 880, val loss: 1.271094799041748
Epoch 890, training loss: 0.6577125191688538 = 0.008907729759812355 + 0.1 * 6.4880475997924805
Epoch 890, val loss: 1.2760729789733887
Epoch 900, training loss: 0.6569510698318481 = 0.00866592675447464 + 0.1 * 6.482851505279541
Epoch 900, val loss: 1.2809823751449585
Epoch 910, training loss: 0.6570354700088501 = 0.008433805778622627 + 0.1 * 6.486016273498535
Epoch 910, val loss: 1.2858221530914307
Epoch 920, training loss: 0.6580323576927185 = 0.008211997337639332 + 0.1 * 6.498203754425049
Epoch 920, val loss: 1.2906016111373901
Epoch 930, training loss: 0.65543133020401 = 0.007999930530786514 + 0.1 * 6.474313735961914
Epoch 930, val loss: 1.2952513694763184
Epoch 940, training loss: 0.656617283821106 = 0.0077964249067008495 + 0.1 * 6.488208770751953
Epoch 940, val loss: 1.299865961074829
Epoch 950, training loss: 0.6552605032920837 = 0.00760107534006238 + 0.1 * 6.476593971252441
Epoch 950, val loss: 1.3043805360794067
Epoch 960, training loss: 0.6568801403045654 = 0.007414183113723993 + 0.1 * 6.494659900665283
Epoch 960, val loss: 1.3087824583053589
Epoch 970, training loss: 0.6547427773475647 = 0.0072346762754023075 + 0.1 * 6.475080966949463
Epoch 970, val loss: 1.3131293058395386
Epoch 980, training loss: 0.6546570062637329 = 0.007062078453600407 + 0.1 * 6.475949287414551
Epoch 980, val loss: 1.317367672920227
Epoch 990, training loss: 0.6540769338607788 = 0.0068959398195147514 + 0.1 * 6.471809387207031
Epoch 990, val loss: 1.321592926979065
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8128624143384291
The final CL Acc:0.74938, 0.01772, The final GNN Acc:0.81128, 0.00155
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 7856])
updated graph: torch.Size([2, 10452])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.7906079292297363 = 1.9309266805648804 + 0.1 * 8.59681224822998
Epoch 0, val loss: 1.9287359714508057
Epoch 10, training loss: 2.780817985534668 = 1.921148657798767 + 0.1 * 8.596693992614746
Epoch 10, val loss: 1.9190253019332886
Epoch 20, training loss: 2.7688210010528564 = 1.9092257022857666 + 0.1 * 8.595952033996582
Epoch 20, val loss: 1.9066179990768433
Epoch 30, training loss: 2.7517218589782715 = 1.8927769660949707 + 0.1 * 8.589449882507324
Epoch 30, val loss: 1.8891438245773315
Epoch 40, training loss: 2.7236344814300537 = 1.8690342903137207 + 0.1 * 8.546002388000488
Epoch 40, val loss: 1.8640494346618652
Epoch 50, training loss: 2.6669464111328125 = 1.8375637531280518 + 0.1 * 8.29382610321045
Epoch 50, val loss: 1.8328617811203003
Epoch 60, training loss: 2.59885835647583 = 1.8026751279830933 + 0.1 * 7.9618330001831055
Epoch 60, val loss: 1.8009157180786133
Epoch 70, training loss: 2.5164384841918945 = 1.7677894830703735 + 0.1 * 7.4864888191223145
Epoch 70, val loss: 1.770560622215271
Epoch 80, training loss: 2.4483554363250732 = 1.727342128753662 + 0.1 * 7.210132598876953
Epoch 80, val loss: 1.7349494695663452
Epoch 90, training loss: 2.3870420455932617 = 1.6754908561706543 + 0.1 * 7.115511894226074
Epoch 90, val loss: 1.6890935897827148
Epoch 100, training loss: 2.315276622772217 = 1.6070399284362793 + 0.1 * 7.082365989685059
Epoch 100, val loss: 1.6290074586868286
Epoch 110, training loss: 2.228050947189331 = 1.5224573612213135 + 0.1 * 7.055934906005859
Epoch 110, val loss: 1.5566656589508057
Epoch 120, training loss: 2.1303699016571045 = 1.4271743297576904 + 0.1 * 7.031955242156982
Epoch 120, val loss: 1.4775238037109375
Epoch 130, training loss: 2.0294790267944336 = 1.329217791557312 + 0.1 * 7.002612113952637
Epoch 130, val loss: 1.3983538150787354
Epoch 140, training loss: 1.9292359352111816 = 1.2324607372283936 + 0.1 * 6.967751502990723
Epoch 140, val loss: 1.3210095167160034
Epoch 150, training loss: 1.8325834274291992 = 1.1394119262695312 + 0.1 * 6.931714057922363
Epoch 150, val loss: 1.2473031282424927
Epoch 160, training loss: 1.7441866397857666 = 1.053470253944397 + 0.1 * 6.907163143157959
Epoch 160, val loss: 1.1797657012939453
Epoch 170, training loss: 1.6640087366104126 = 0.9756242036819458 + 0.1 * 6.883845329284668
Epoch 170, val loss: 1.1193325519561768
Epoch 180, training loss: 1.591802716255188 = 0.9049603343009949 + 0.1 * 6.8684234619140625
Epoch 180, val loss: 1.0652902126312256
Epoch 190, training loss: 1.5251522064208984 = 0.839906632900238 + 0.1 * 6.852455139160156
Epoch 190, val loss: 1.0158822536468506
Epoch 200, training loss: 1.463797926902771 = 0.7798535823822021 + 0.1 * 6.839443206787109
Epoch 200, val loss: 0.9704177379608154
Epoch 210, training loss: 1.40911865234375 = 0.7259705662727356 + 0.1 * 6.831481456756592
Epoch 210, val loss: 0.9299583435058594
Epoch 220, training loss: 1.3607077598571777 = 0.6783800721168518 + 0.1 * 6.823276519775391
Epoch 220, val loss: 0.8953239917755127
Epoch 230, training loss: 1.3165749311447144 = 0.6352828741073608 + 0.1 * 6.812920570373535
Epoch 230, val loss: 0.8653783798217773
Epoch 240, training loss: 1.274867296218872 = 0.594933807849884 + 0.1 * 6.799334526062012
Epoch 240, val loss: 0.8386997580528259
Epoch 250, training loss: 1.2340242862701416 = 0.5554589629173279 + 0.1 * 6.785653114318848
Epoch 250, val loss: 0.8139471411705017
Epoch 260, training loss: 1.1940312385559082 = 0.5159457325935364 + 0.1 * 6.78085470199585
Epoch 260, val loss: 0.790451169013977
Epoch 270, training loss: 1.1538127660751343 = 0.47658196091651917 + 0.1 * 6.772307872772217
Epoch 270, val loss: 0.7684541344642639
Epoch 280, training loss: 1.1135883331298828 = 0.43756788969039917 + 0.1 * 6.760204792022705
Epoch 280, val loss: 0.748444139957428
Epoch 290, training loss: 1.074659824371338 = 0.3996608555316925 + 0.1 * 6.749989986419678
Epoch 290, val loss: 0.7311305403709412
Epoch 300, training loss: 1.0386567115783691 = 0.3636581599712372 + 0.1 * 6.749985218048096
Epoch 300, val loss: 0.7169115543365479
Epoch 310, training loss: 1.0037156343460083 = 0.3301480710506439 + 0.1 * 6.735675811767578
Epoch 310, val loss: 0.7060333490371704
Epoch 320, training loss: 0.9715765118598938 = 0.2991173267364502 + 0.1 * 6.7245917320251465
Epoch 320, val loss: 0.6982789039611816
Epoch 330, training loss: 0.9425173997879028 = 0.27059629559516907 + 0.1 * 6.719210624694824
Epoch 330, val loss: 0.6934545040130615
Epoch 340, training loss: 0.9166858196258545 = 0.24478499591350555 + 0.1 * 6.719007968902588
Epoch 340, val loss: 0.6912733316421509
Epoch 350, training loss: 0.8924229145050049 = 0.2216014266014099 + 0.1 * 6.70821475982666
Epoch 350, val loss: 0.6912668347358704
Epoch 360, training loss: 0.8706445097923279 = 0.20065659284591675 + 0.1 * 6.699879169464111
Epoch 360, val loss: 0.6933249831199646
Epoch 370, training loss: 0.8512434363365173 = 0.18174970149993896 + 0.1 * 6.694937229156494
Epoch 370, val loss: 0.6970057487487793
Epoch 380, training loss: 0.8325384855270386 = 0.16473093628883362 + 0.1 * 6.678075313568115
Epoch 380, val loss: 0.7020415663719177
Epoch 390, training loss: 0.8169877529144287 = 0.14938682317733765 + 0.1 * 6.676009178161621
Epoch 390, val loss: 0.7081788778305054
Epoch 400, training loss: 0.8038749098777771 = 0.13557006418704987 + 0.1 * 6.683048248291016
Epoch 400, val loss: 0.7152226567268372
Epoch 410, training loss: 0.7897402048110962 = 0.12320645898580551 + 0.1 * 6.665337562561035
Epoch 410, val loss: 0.7228502631187439
Epoch 420, training loss: 0.7774107456207275 = 0.11212560534477234 + 0.1 * 6.652851581573486
Epoch 420, val loss: 0.7310968637466431
Epoch 430, training loss: 0.7666622400283813 = 0.10216841846704483 + 0.1 * 6.644937992095947
Epoch 430, val loss: 0.7397902607917786
Epoch 440, training loss: 0.7602044939994812 = 0.09323249012231827 + 0.1 * 6.669719696044922
Epoch 440, val loss: 0.7488433122634888
Epoch 450, training loss: 0.7484539151191711 = 0.0852784514427185 + 0.1 * 6.631754398345947
Epoch 450, val loss: 0.7579835057258606
Epoch 460, training loss: 0.7412804365158081 = 0.07816676050424576 + 0.1 * 6.631136417388916
Epoch 460, val loss: 0.7672860026359558
Epoch 470, training loss: 0.7343450784683228 = 0.07179605960845947 + 0.1 * 6.625490188598633
Epoch 470, val loss: 0.7766217589378357
Epoch 480, training loss: 0.7279967069625854 = 0.06609442830085754 + 0.1 * 6.619022846221924
Epoch 480, val loss: 0.7858757376670837
Epoch 490, training loss: 0.7221069931983948 = 0.06097417697310448 + 0.1 * 6.611328125
Epoch 490, val loss: 0.7950952649116516
Epoch 500, training loss: 0.7171518206596375 = 0.05636823549866676 + 0.1 * 6.60783576965332
Epoch 500, val loss: 0.804229736328125
Epoch 510, training loss: 0.7120374441146851 = 0.052215103060007095 + 0.1 * 6.5982232093811035
Epoch 510, val loss: 0.8132409453392029
Epoch 520, training loss: 0.7099311947822571 = 0.04846346750855446 + 0.1 * 6.614677429199219
Epoch 520, val loss: 0.8221434950828552
Epoch 530, training loss: 0.7037985920906067 = 0.04507660120725632 + 0.1 * 6.587219715118408
Epoch 530, val loss: 0.8308613300323486
Epoch 540, training loss: 0.7029057145118713 = 0.04200488701462746 + 0.1 * 6.609008312225342
Epoch 540, val loss: 0.8394262790679932
Epoch 550, training loss: 0.6982097625732422 = 0.03921777009963989 + 0.1 * 6.5899200439453125
Epoch 550, val loss: 0.8478889465332031
Epoch 560, training loss: 0.695012629032135 = 0.03668294847011566 + 0.1 * 6.583296775817871
Epoch 560, val loss: 0.8561810255050659
Epoch 570, training loss: 0.6917873024940491 = 0.03437307849526405 + 0.1 * 6.574141979217529
Epoch 570, val loss: 0.8643416166305542
Epoch 580, training loss: 0.6901124119758606 = 0.03226612135767937 + 0.1 * 6.578462600708008
Epoch 580, val loss: 0.8723967671394348
Epoch 590, training loss: 0.6878698468208313 = 0.030336715281009674 + 0.1 * 6.57533073425293
Epoch 590, val loss: 0.8803383111953735
Epoch 600, training loss: 0.6842333674430847 = 0.028572559356689453 + 0.1 * 6.556608200073242
Epoch 600, val loss: 0.8880637884140015
Epoch 610, training loss: 0.6827295422554016 = 0.026950262486934662 + 0.1 * 6.557793140411377
Epoch 610, val loss: 0.8957610726356506
Epoch 620, training loss: 0.6817649006843567 = 0.025459548458456993 + 0.1 * 6.563053607940674
Epoch 620, val loss: 0.903244137763977
Epoch 630, training loss: 0.6794208288192749 = 0.02408592589199543 + 0.1 * 6.553348541259766
Epoch 630, val loss: 0.9106548428535461
Epoch 640, training loss: 0.6773681044578552 = 0.022818641737103462 + 0.1 * 6.545494556427002
Epoch 640, val loss: 0.9179328083992004
Epoch 650, training loss: 0.6773895621299744 = 0.02164708636701107 + 0.1 * 6.557424545288086
Epoch 650, val loss: 0.9250878691673279
Epoch 660, training loss: 0.6747940182685852 = 0.020564449951052666 + 0.1 * 6.542295932769775
Epoch 660, val loss: 0.9321321249008179
Epoch 670, training loss: 0.6724061965942383 = 0.019561689347028732 + 0.1 * 6.528444766998291
Epoch 670, val loss: 0.9390048980712891
Epoch 680, training loss: 0.6722378730773926 = 0.018627651035785675 + 0.1 * 6.536102294921875
Epoch 680, val loss: 0.9458441138267517
Epoch 690, training loss: 0.6709032654762268 = 0.01775806024670601 + 0.1 * 6.53145170211792
Epoch 690, val loss: 0.9525538682937622
Epoch 700, training loss: 0.6692253947257996 = 0.016950130462646484 + 0.1 * 6.522752285003662
Epoch 700, val loss: 0.959068238735199
Epoch 710, training loss: 0.6688355207443237 = 0.016196809709072113 + 0.1 * 6.5263872146606445
Epoch 710, val loss: 0.9654838442802429
Epoch 720, training loss: 0.6687279939651489 = 0.015494046732783318 + 0.1 * 6.532339096069336
Epoch 720, val loss: 0.9717827439308167
Epoch 730, training loss: 0.6666136384010315 = 0.01483759842813015 + 0.1 * 6.517760276794434
Epoch 730, val loss: 0.9779474139213562
Epoch 740, training loss: 0.6663697957992554 = 0.014223011210560799 + 0.1 * 6.521467685699463
Epoch 740, val loss: 0.9840044975280762
Epoch 750, training loss: 0.6656749248504639 = 0.013645443134009838 + 0.1 * 6.520294189453125
Epoch 750, val loss: 0.9899604916572571
Epoch 760, training loss: 0.6638503670692444 = 0.013105341233313084 + 0.1 * 6.507450103759766
Epoch 760, val loss: 0.9957724213600159
Epoch 770, training loss: 0.6627534031867981 = 0.012598587200045586 + 0.1 * 6.5015482902526855
Epoch 770, val loss: 1.0014755725860596
Epoch 780, training loss: 0.6636381149291992 = 0.01212045457214117 + 0.1 * 6.515176773071289
Epoch 780, val loss: 1.0071030855178833
Epoch 790, training loss: 0.6619220972061157 = 0.01167113147675991 + 0.1 * 6.502509593963623
Epoch 790, val loss: 1.0125588178634644
Epoch 800, training loss: 0.6622267365455627 = 0.011247890070080757 + 0.1 * 6.509788513183594
Epoch 800, val loss: 1.0179557800292969
Epoch 810, training loss: 0.6617434024810791 = 0.010847745463252068 + 0.1 * 6.508956432342529
Epoch 810, val loss: 1.0232563018798828
Epoch 820, training loss: 0.6599428057670593 = 0.01046989019960165 + 0.1 * 6.494729042053223
Epoch 820, val loss: 1.0284416675567627
Epoch 830, training loss: 0.6601608395576477 = 0.010112813673913479 + 0.1 * 6.5004801750183105
Epoch 830, val loss: 1.0335192680358887
Epoch 840, training loss: 0.6582836508750916 = 0.009774403646588326 + 0.1 * 6.485092639923096
Epoch 840, val loss: 1.0385262966156006
Epoch 850, training loss: 0.6608628630638123 = 0.009453155100345612 + 0.1 * 6.514096736907959
Epoch 850, val loss: 1.0434801578521729
Epoch 860, training loss: 0.6585842370986938 = 0.009149132296442986 + 0.1 * 6.494350910186768
Epoch 860, val loss: 1.0483418703079224
Epoch 870, training loss: 0.6583142876625061 = 0.00886171031743288 + 0.1 * 6.49452543258667
Epoch 870, val loss: 1.0530558824539185
Epoch 880, training loss: 0.6562519073486328 = 0.008588071912527084 + 0.1 * 6.476638317108154
Epoch 880, val loss: 1.0577727556228638
Epoch 890, training loss: 0.6572858691215515 = 0.008328410796821117 + 0.1 * 6.489574432373047
Epoch 890, val loss: 1.062336802482605
Epoch 900, training loss: 0.6562913060188293 = 0.008080835454165936 + 0.1 * 6.482104301452637
Epoch 900, val loss: 1.066857099533081
Epoch 910, training loss: 0.6564253568649292 = 0.007844612933695316 + 0.1 * 6.485807418823242
Epoch 910, val loss: 1.0713279247283936
Epoch 920, training loss: 0.655785322189331 = 0.007619595155119896 + 0.1 * 6.481657028198242
Epoch 920, val loss: 1.075715184211731
Epoch 930, training loss: 0.654560387134552 = 0.007405564654618502 + 0.1 * 6.471548080444336
Epoch 930, val loss: 1.0800029039382935
Epoch 940, training loss: 0.6552231907844543 = 0.007200837600976229 + 0.1 * 6.480223178863525
Epoch 940, val loss: 1.0842605829238892
Epoch 950, training loss: 0.6561053991317749 = 0.007004891987890005 + 0.1 * 6.491004467010498
Epoch 950, val loss: 1.0884382724761963
Epoch 960, training loss: 0.6545196771621704 = 0.006818740162998438 + 0.1 * 6.4770097732543945
Epoch 960, val loss: 1.0925244092941284
Epoch 970, training loss: 0.6540661454200745 = 0.006640597712248564 + 0.1 * 6.474255084991455
Epoch 970, val loss: 1.0965280532836914
Epoch 980, training loss: 0.6522679924964905 = 0.006469981744885445 + 0.1 * 6.457980155944824
Epoch 980, val loss: 1.1004996299743652
Epoch 990, training loss: 0.6531839370727539 = 0.00630575604736805 + 0.1 * 6.468781471252441
Epoch 990, val loss: 1.1044455766677856
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8465998945703743
=== training gcn model ===
Epoch 0, training loss: 2.804659605026245 = 1.9449760913848877 + 0.1 * 8.596834182739258
Epoch 0, val loss: 1.9394170045852661
Epoch 10, training loss: 2.794138193130493 = 1.934463620185852 + 0.1 * 8.596745491027832
Epoch 10, val loss: 1.9290448427200317
Epoch 20, training loss: 2.781236410140991 = 1.921608805656433 + 0.1 * 8.596275329589844
Epoch 20, val loss: 1.916459560394287
Epoch 30, training loss: 2.7632296085357666 = 1.903978705406189 + 0.1 * 8.592508316040039
Epoch 30, val loss: 1.899275302886963
Epoch 40, training loss: 2.7351438999176025 = 1.8786649703979492 + 0.1 * 8.564788818359375
Epoch 40, val loss: 1.874969482421875
Epoch 50, training loss: 2.6829066276550293 = 1.8438619375228882 + 0.1 * 8.390445709228516
Epoch 50, val loss: 1.8430533409118652
Epoch 60, training loss: 2.612072229385376 = 1.8048036098480225 + 0.1 * 8.072686195373535
Epoch 60, val loss: 1.8095471858978271
Epoch 70, training loss: 2.5365328788757324 = 1.7660547494888306 + 0.1 * 7.704780578613281
Epoch 70, val loss: 1.7763069868087769
Epoch 80, training loss: 2.4630768299102783 = 1.725697636604309 + 0.1 * 7.373791217803955
Epoch 80, val loss: 1.7402355670928955
Epoch 90, training loss: 2.3984246253967285 = 1.6750892400741577 + 0.1 * 7.233354091644287
Epoch 90, val loss: 1.693142056465149
Epoch 100, training loss: 2.3223907947540283 = 1.6082504987716675 + 0.1 * 7.141402244567871
Epoch 100, val loss: 1.6326667070388794
Epoch 110, training loss: 2.2310097217559814 = 1.5262079238891602 + 0.1 * 7.048017978668213
Epoch 110, val loss: 1.5621881484985352
Epoch 120, training loss: 2.1312897205352783 = 1.4336137771606445 + 0.1 * 6.976759910583496
Epoch 120, val loss: 1.4830673933029175
Epoch 130, training loss: 2.029406785964966 = 1.336018681526184 + 0.1 * 6.9338812828063965
Epoch 130, val loss: 1.402064323425293
Epoch 140, training loss: 1.9253184795379639 = 1.234730839729309 + 0.1 * 6.905876159667969
Epoch 140, val loss: 1.3193745613098145
Epoch 150, training loss: 1.8204271793365479 = 1.1315068006515503 + 0.1 * 6.889203071594238
Epoch 150, val loss: 1.236588478088379
Epoch 160, training loss: 1.7168551683425903 = 1.028963327407837 + 0.1 * 6.878918170928955
Epoch 160, val loss: 1.1552528142929077
Epoch 170, training loss: 1.617093801498413 = 0.9297491908073425 + 0.1 * 6.873446464538574
Epoch 170, val loss: 1.0774468183517456
Epoch 180, training loss: 1.5257186889648438 = 0.8397238254547119 + 0.1 * 6.859947681427002
Epoch 180, val loss: 1.008400797843933
Epoch 190, training loss: 1.4464932680130005 = 0.7613357305526733 + 0.1 * 6.8515753746032715
Epoch 190, val loss: 0.9502204060554504
Epoch 200, training loss: 1.3792059421539307 = 0.6945078372955322 + 0.1 * 6.846981048583984
Epoch 200, val loss: 0.9034702181816101
Epoch 210, training loss: 1.320566177368164 = 0.6370217800140381 + 0.1 * 6.83544397354126
Epoch 210, val loss: 0.866186797618866
Epoch 220, training loss: 1.2682570219039917 = 0.5857322812080383 + 0.1 * 6.825247287750244
Epoch 220, val loss: 0.8357968330383301
Epoch 230, training loss: 1.2206401824951172 = 0.5388574004173279 + 0.1 * 6.817828178405762
Epoch 230, val loss: 0.8105905055999756
Epoch 240, training loss: 1.1759560108184814 = 0.49553996324539185 + 0.1 * 6.804161071777344
Epoch 240, val loss: 0.7896326184272766
Epoch 250, training loss: 1.1351438760757446 = 0.4553009867668152 + 0.1 * 6.798428535461426
Epoch 250, val loss: 0.7723361253738403
Epoch 260, training loss: 1.0971288681030273 = 0.4185260236263275 + 0.1 * 6.786028861999512
Epoch 260, val loss: 0.7590294480323792
Epoch 270, training loss: 1.063485026359558 = 0.38528984785079956 + 0.1 * 6.781951427459717
Epoch 270, val loss: 0.7499095797538757
Epoch 280, training loss: 1.0326223373413086 = 0.35546234250068665 + 0.1 * 6.771600246429443
Epoch 280, val loss: 0.7447212338447571
Epoch 290, training loss: 1.0042893886566162 = 0.32838934659957886 + 0.1 * 6.759000301361084
Epoch 290, val loss: 0.7428619265556335
Epoch 300, training loss: 0.9800401926040649 = 0.30342963337898254 + 0.1 * 6.7661051750183105
Epoch 300, val loss: 0.7436416149139404
Epoch 310, training loss: 0.9545406103134155 = 0.28029564023017883 + 0.1 * 6.742449760437012
Epoch 310, val loss: 0.7463864088058472
Epoch 320, training loss: 0.932537853717804 = 0.25848495960235596 + 0.1 * 6.740528583526611
Epoch 320, val loss: 0.7507156729698181
Epoch 330, training loss: 0.9112749099731445 = 0.23783208429813385 + 0.1 * 6.734428405761719
Epoch 330, val loss: 0.7562674283981323
Epoch 340, training loss: 0.890325665473938 = 0.21823157370090485 + 0.1 * 6.720941066741943
Epoch 340, val loss: 0.7628868818283081
Epoch 350, training loss: 0.8715494275093079 = 0.1997220516204834 + 0.1 * 6.718273639678955
Epoch 350, val loss: 0.7705140113830566
Epoch 360, training loss: 0.85352623462677 = 0.1824790984392166 + 0.1 * 6.7104716300964355
Epoch 360, val loss: 0.7790704369544983
Epoch 370, training loss: 0.8369625210762024 = 0.1666048914194107 + 0.1 * 6.703576564788818
Epoch 370, val loss: 0.7885536551475525
Epoch 380, training loss: 0.822143018245697 = 0.15214340388774872 + 0.1 * 6.699995994567871
Epoch 380, val loss: 0.7989363074302673
Epoch 390, training loss: 0.8077160716056824 = 0.139036163687706 + 0.1 * 6.686799049377441
Epoch 390, val loss: 0.8099824786186218
Epoch 400, training loss: 0.7950623631477356 = 0.12714390456676483 + 0.1 * 6.679184436798096
Epoch 400, val loss: 0.821746826171875
Epoch 410, training loss: 0.7856737375259399 = 0.11640208959579468 + 0.1 * 6.692716121673584
Epoch 410, val loss: 0.8339049220085144
Epoch 420, training loss: 0.7733948826789856 = 0.10670701414346695 + 0.1 * 6.6668782234191895
Epoch 420, val loss: 0.8463642001152039
Epoch 430, training loss: 0.7635769844055176 = 0.0978982001543045 + 0.1 * 6.656787872314453
Epoch 430, val loss: 0.8592390418052673
Epoch 440, training loss: 0.7551565766334534 = 0.08988890796899796 + 0.1 * 6.652676105499268
Epoch 440, val loss: 0.8722946643829346
Epoch 450, training loss: 0.7474488615989685 = 0.08262210339307785 + 0.1 * 6.6482672691345215
Epoch 450, val loss: 0.8854075074195862
Epoch 460, training loss: 0.7403389811515808 = 0.0760200172662735 + 0.1 * 6.643189430236816
Epoch 460, val loss: 0.8986119031906128
Epoch 470, training loss: 0.7339521646499634 = 0.07002970576286316 + 0.1 * 6.639225006103516
Epoch 470, val loss: 0.9117934703826904
Epoch 480, training loss: 0.7291633486747742 = 0.06461916863918304 + 0.1 * 6.64544153213501
Epoch 480, val loss: 0.9248448610305786
Epoch 490, training loss: 0.722093939781189 = 0.05973153933882713 + 0.1 * 6.623623847961426
Epoch 490, val loss: 0.9376732707023621
Epoch 500, training loss: 0.7193396687507629 = 0.05530387908220291 + 0.1 * 6.640357971191406
Epoch 500, val loss: 0.9505107998847961
Epoch 510, training loss: 0.7131925225257874 = 0.051302243024110794 + 0.1 * 6.618902683258057
Epoch 510, val loss: 0.9631364345550537
Epoch 520, training loss: 0.7093632817268372 = 0.04767147824168205 + 0.1 * 6.616917610168457
Epoch 520, val loss: 0.9756377339363098
Epoch 530, training loss: 0.7047908306121826 = 0.044381946325302124 + 0.1 * 6.604088306427002
Epoch 530, val loss: 0.9879642128944397
Epoch 540, training loss: 0.7034843564033508 = 0.04138956218957901 + 0.1 * 6.620947360992432
Epoch 540, val loss: 1.0000795125961304
Epoch 550, training loss: 0.6992576122283936 = 0.03867282718420029 + 0.1 * 6.6058478355407715
Epoch 550, val loss: 1.0119658708572388
Epoch 560, training loss: 0.6958602070808411 = 0.0361969918012619 + 0.1 * 6.59663200378418
Epoch 560, val loss: 1.023541808128357
Epoch 570, training loss: 0.6935042142868042 = 0.033936887979507446 + 0.1 * 6.595673561096191
Epoch 570, val loss: 1.0349857807159424
Epoch 580, training loss: 0.6903067231178284 = 0.03186981379985809 + 0.1 * 6.584368705749512
Epoch 580, val loss: 1.0462160110473633
Epoch 590, training loss: 0.6909952163696289 = 0.029975658282637596 + 0.1 * 6.610195636749268
Epoch 590, val loss: 1.0572328567504883
Epoch 600, training loss: 0.6861761808395386 = 0.028245151042938232 + 0.1 * 6.579310417175293
Epoch 600, val loss: 1.067999243736267
Epoch 610, training loss: 0.6835434436798096 = 0.0266548004001379 + 0.1 * 6.5688862800598145
Epoch 610, val loss: 1.0784964561462402
Epoch 620, training loss: 0.6829285025596619 = 0.025188429281115532 + 0.1 * 6.577400207519531
Epoch 620, val loss: 1.08890962600708
Epoch 630, training loss: 0.6814704537391663 = 0.02383878082036972 + 0.1 * 6.576316833496094
Epoch 630, val loss: 1.0990480184555054
Epoch 640, training loss: 0.68098384141922 = 0.022594092413783073 + 0.1 * 6.583897113800049
Epoch 640, val loss: 1.1089245080947876
Epoch 650, training loss: 0.67775958776474 = 0.021446339786052704 + 0.1 * 6.563132286071777
Epoch 650, val loss: 1.1186282634735107
Epoch 660, training loss: 0.6782531142234802 = 0.02038174495100975 + 0.1 * 6.578713417053223
Epoch 660, val loss: 1.128106713294983
Epoch 670, training loss: 0.6750244498252869 = 0.019395845010876656 + 0.1 * 6.556285858154297
Epoch 670, val loss: 1.1374762058258057
Epoch 680, training loss: 0.6737695932388306 = 0.018479350954294205 + 0.1 * 6.5529022216796875
Epoch 680, val loss: 1.1465626955032349
Epoch 690, training loss: 0.6720159649848938 = 0.017627744004130363 + 0.1 * 6.543882369995117
Epoch 690, val loss: 1.1555538177490234
Epoch 700, training loss: 0.6722002029418945 = 0.016833728179335594 + 0.1 * 6.553664684295654
Epoch 700, val loss: 1.1643551588058472
Epoch 710, training loss: 0.6699826121330261 = 0.016094408929347992 + 0.1 * 6.538882255554199
Epoch 710, val loss: 1.1729627847671509
Epoch 720, training loss: 0.6694605350494385 = 0.015403969213366508 + 0.1 * 6.540565490722656
Epoch 720, val loss: 1.1813830137252808
Epoch 730, training loss: 0.6696265339851379 = 0.014758315868675709 + 0.1 * 6.54868221282959
Epoch 730, val loss: 1.1896743774414062
Epoch 740, training loss: 0.6683897972106934 = 0.014156240038573742 + 0.1 * 6.542335510253906
Epoch 740, val loss: 1.1977163553237915
Epoch 750, training loss: 0.6663920283317566 = 0.013591154478490353 + 0.1 * 6.528008460998535
Epoch 750, val loss: 1.2056937217712402
Epoch 760, training loss: 0.6662369966506958 = 0.013060777448117733 + 0.1 * 6.53176212310791
Epoch 760, val loss: 1.2134712934494019
Epoch 770, training loss: 0.6660302877426147 = 0.012562460266053677 + 0.1 * 6.5346784591674805
Epoch 770, val loss: 1.221138834953308
Epoch 780, training loss: 0.6661619544029236 = 0.01209372840821743 + 0.1 * 6.540682315826416
Epoch 780, val loss: 1.2286421060562134
Epoch 790, training loss: 0.6640201210975647 = 0.011652695015072823 + 0.1 * 6.523674011230469
Epoch 790, val loss: 1.2360093593597412
Epoch 800, training loss: 0.6635935306549072 = 0.011237316764891148 + 0.1 * 6.523561954498291
Epoch 800, val loss: 1.2432124614715576
Epoch 810, training loss: 0.6633647084236145 = 0.010844424366950989 + 0.1 * 6.525202751159668
Epoch 810, val loss: 1.2503153085708618
Epoch 820, training loss: 0.6621073484420776 = 0.010474332608282566 + 0.1 * 6.516330242156982
Epoch 820, val loss: 1.2571929693222046
Epoch 830, training loss: 0.6606032252311707 = 0.01012439839541912 + 0.1 * 6.504788398742676
Epoch 830, val loss: 1.2640000581741333
Epoch 840, training loss: 0.6614397764205933 = 0.009792824275791645 + 0.1 * 6.516469478607178
Epoch 840, val loss: 1.2706680297851562
Epoch 850, training loss: 0.6598599553108215 = 0.009478675201535225 + 0.1 * 6.503812313079834
Epoch 850, val loss: 1.2772526741027832
Epoch 860, training loss: 0.6593927145004272 = 0.009181014262139797 + 0.1 * 6.502116680145264
Epoch 860, val loss: 1.2837021350860596
Epoch 870, training loss: 0.6592920422554016 = 0.008898047730326653 + 0.1 * 6.503940105438232
Epoch 870, val loss: 1.289998173713684
Epoch 880, training loss: 0.6587821245193481 = 0.008630705066025257 + 0.1 * 6.501513957977295
Epoch 880, val loss: 1.2962089776992798
Epoch 890, training loss: 0.6573001146316528 = 0.008376047946512699 + 0.1 * 6.4892401695251465
Epoch 890, val loss: 1.3022496700286865
Epoch 900, training loss: 0.6585696935653687 = 0.00813276320695877 + 0.1 * 6.504369258880615
Epoch 900, val loss: 1.3082326650619507
Epoch 910, training loss: 0.6577531695365906 = 0.007901163771748543 + 0.1 * 6.4985198974609375
Epoch 910, val loss: 1.314157485961914
Epoch 920, training loss: 0.6562427878379822 = 0.0076804328709840775 + 0.1 * 6.485623836517334
Epoch 920, val loss: 1.3199130296707153
Epoch 930, training loss: 0.6566214561462402 = 0.007469977717846632 + 0.1 * 6.491515159606934
Epoch 930, val loss: 1.3255590200424194
Epoch 940, training loss: 0.6552952527999878 = 0.0072688995860517025 + 0.1 * 6.4802632331848145
Epoch 940, val loss: 1.33113694190979
Epoch 950, training loss: 0.6545721888542175 = 0.007076762150973082 + 0.1 * 6.474954128265381
Epoch 950, val loss: 1.3366349935531616
Epoch 960, training loss: 0.656467080116272 = 0.006892870180308819 + 0.1 * 6.495741844177246
Epoch 960, val loss: 1.3420406579971313
Epoch 970, training loss: 0.6547327041625977 = 0.006717034615576267 + 0.1 * 6.480156898498535
Epoch 970, val loss: 1.3472989797592163
Epoch 980, training loss: 0.6542567014694214 = 0.006548644043505192 + 0.1 * 6.477080345153809
Epoch 980, val loss: 1.3525406122207642
Epoch 990, training loss: 0.6535726189613342 = 0.0063874428160488605 + 0.1 * 6.471851348876953
Epoch 990, val loss: 1.3576502799987793
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 2.792874574661255 = 1.9331943988800049 + 0.1 * 8.5968017578125
Epoch 0, val loss: 1.929575800895691
Epoch 10, training loss: 2.7837204933166504 = 1.9240503311157227 + 0.1 * 8.596700668334961
Epoch 10, val loss: 1.9202532768249512
Epoch 20, training loss: 2.7727251052856445 = 1.9131172895431519 + 0.1 * 8.596076965332031
Epoch 20, val loss: 1.9090780019760132
Epoch 30, training loss: 2.757115602493286 = 1.8979873657226562 + 0.1 * 8.591282844543457
Epoch 30, val loss: 1.8938086032867432
Epoch 40, training loss: 2.7310996055603027 = 1.8755624294281006 + 0.1 * 8.55537223815918
Epoch 40, val loss: 1.8714065551757812
Epoch 50, training loss: 2.6742539405822754 = 1.8438373804092407 + 0.1 * 8.304165840148926
Epoch 50, val loss: 1.8410568237304688
Epoch 60, training loss: 2.5881783962249756 = 1.807613492012024 + 0.1 * 7.8056488037109375
Epoch 60, val loss: 1.807882308959961
Epoch 70, training loss: 2.508248805999756 = 1.7712839841842651 + 0.1 * 7.36964750289917
Epoch 70, val loss: 1.7754619121551514
Epoch 80, training loss: 2.4500467777252197 = 1.7311475276947021 + 0.1 * 7.188992977142334
Epoch 80, val loss: 1.7393978834152222
Epoch 90, training loss: 2.3912031650543213 = 1.6822781562805176 + 0.1 * 7.089249134063721
Epoch 90, val loss: 1.6945252418518066
Epoch 100, training loss: 2.319270610809326 = 1.616956114768982 + 0.1 * 7.023146152496338
Epoch 100, val loss: 1.6347589492797852
Epoch 110, training loss: 2.229649066925049 = 1.5322978496551514 + 0.1 * 6.973511695861816
Epoch 110, val loss: 1.5594463348388672
Epoch 120, training loss: 2.12278413772583 = 1.4292395114898682 + 0.1 * 6.9354472160339355
Epoch 120, val loss: 1.4699448347091675
Epoch 130, training loss: 2.0048272609710693 = 1.3141331672668457 + 0.1 * 6.9069414138793945
Epoch 130, val loss: 1.3713665008544922
Epoch 140, training loss: 1.8823590278625488 = 1.1940374374389648 + 0.1 * 6.883216381072998
Epoch 140, val loss: 1.27047598361969
Epoch 150, training loss: 1.764608383178711 = 1.0778400897979736 + 0.1 * 6.867681980133057
Epoch 150, val loss: 1.1759637594223022
Epoch 160, training loss: 1.6569191217422485 = 0.971510112285614 + 0.1 * 6.854089736938477
Epoch 160, val loss: 1.0911829471588135
Epoch 170, training loss: 1.5620133876800537 = 0.8782782554626465 + 0.1 * 6.837350845336914
Epoch 170, val loss: 1.0188428163528442
Epoch 180, training loss: 1.4802980422973633 = 0.7981694936752319 + 0.1 * 6.821284770965576
Epoch 180, val loss: 0.959098756313324
Epoch 190, training loss: 1.412346601486206 = 0.7307604551315308 + 0.1 * 6.815861701965332
Epoch 190, val loss: 0.9120017290115356
Epoch 200, training loss: 1.3530895709991455 = 0.6736059784889221 + 0.1 * 6.794835090637207
Epoch 200, val loss: 0.8752627372741699
Epoch 210, training loss: 1.3019418716430664 = 0.623610258102417 + 0.1 * 6.783315658569336
Epoch 210, val loss: 0.8463879823684692
Epoch 220, training loss: 1.2560853958129883 = 0.578842282295227 + 0.1 * 6.772430419921875
Epoch 220, val loss: 0.8234868049621582
Epoch 230, training loss: 1.213700532913208 = 0.5375945568084717 + 0.1 * 6.761059284210205
Epoch 230, val loss: 0.8050592541694641
Epoch 240, training loss: 1.175032615661621 = 0.4988638460636139 + 0.1 * 6.761687755584717
Epoch 240, val loss: 0.7900233268737793
Epoch 250, training loss: 1.136364221572876 = 0.4622650146484375 + 0.1 * 6.740991592407227
Epoch 250, val loss: 0.7779977321624756
Epoch 260, training loss: 1.1021479368209839 = 0.427327424287796 + 0.1 * 6.748204708099365
Epoch 260, val loss: 0.7687156200408936
Epoch 270, training loss: 1.067528247833252 = 0.3943879008293152 + 0.1 * 6.731403350830078
Epoch 270, val loss: 0.7619016766548157
Epoch 280, training loss: 1.0351307392120361 = 0.36331379413604736 + 0.1 * 6.718169212341309
Epoch 280, val loss: 0.7574358582496643
Epoch 290, training loss: 1.0054051876068115 = 0.33414795994758606 + 0.1 * 6.7125725746154785
Epoch 290, val loss: 0.755288302898407
Epoch 300, training loss: 0.9774525165557861 = 0.3070254325866699 + 0.1 * 6.704270839691162
Epoch 300, val loss: 0.7553920745849609
Epoch 310, training loss: 0.9518929719924927 = 0.2818085849285126 + 0.1 * 6.7008442878723145
Epoch 310, val loss: 0.757705807685852
Epoch 320, training loss: 0.9279100298881531 = 0.25859344005584717 + 0.1 * 6.6931657791137695
Epoch 320, val loss: 0.7621294856071472
Epoch 330, training loss: 0.905623733997345 = 0.237277552485466 + 0.1 * 6.683461666107178
Epoch 330, val loss: 0.768524706363678
Epoch 340, training loss: 0.8889309763908386 = 0.21767692267894745 + 0.1 * 6.712540149688721
Epoch 340, val loss: 0.7767543792724609
Epoch 350, training loss: 0.8685904741287231 = 0.1998542994260788 + 0.1 * 6.687361240386963
Epoch 350, val loss: 0.7864696383476257
Epoch 360, training loss: 0.8503966331481934 = 0.1835804283618927 + 0.1 * 6.668161392211914
Epoch 360, val loss: 0.7975440621376038
Epoch 370, training loss: 0.8352195024490356 = 0.1686805635690689 + 0.1 * 6.665389537811279
Epoch 370, val loss: 0.8098236918449402
Epoch 380, training loss: 0.8217099905014038 = 0.15503083169460297 + 0.1 * 6.6667914390563965
Epoch 380, val loss: 0.8231555819511414
Epoch 390, training loss: 0.808056116104126 = 0.14255662262439728 + 0.1 * 6.654994964599609
Epoch 390, val loss: 0.8372564911842346
Epoch 400, training loss: 0.7958936095237732 = 0.13118459284305573 + 0.1 * 6.647089958190918
Epoch 400, val loss: 0.8520257472991943
Epoch 410, training loss: 0.7846596837043762 = 0.12080429494380951 + 0.1 * 6.638553619384766
Epoch 410, val loss: 0.8672871589660645
Epoch 420, training loss: 0.7760956287384033 = 0.11131677776575089 + 0.1 * 6.647788047790527
Epoch 420, val loss: 0.8829845190048218
Epoch 430, training loss: 0.7662261128425598 = 0.10267902165651321 + 0.1 * 6.635470867156982
Epoch 430, val loss: 0.8989238142967224
Epoch 440, training loss: 0.7575383186340332 = 0.09480305761098862 + 0.1 * 6.627352714538574
Epoch 440, val loss: 0.9150002598762512
Epoch 450, training loss: 0.7508178949356079 = 0.08764954656362534 + 0.1 * 6.631683826446533
Epoch 450, val loss: 0.931090772151947
Epoch 460, training loss: 0.743005096912384 = 0.08114204555749893 + 0.1 * 6.6186299324035645
Epoch 460, val loss: 0.947158932685852
Epoch 470, training loss: 0.7368834018707275 = 0.07521706819534302 + 0.1 * 6.616662979125977
Epoch 470, val loss: 0.963045597076416
Epoch 480, training loss: 0.7301388382911682 = 0.06982975453138351 + 0.1 * 6.603090763092041
Epoch 480, val loss: 0.9787082076072693
Epoch 490, training loss: 0.7279824018478394 = 0.06491457670927048 + 0.1 * 6.630678176879883
Epoch 490, val loss: 0.9942349195480347
Epoch 500, training loss: 0.7201628684997559 = 0.0604468397796154 + 0.1 * 6.5971598625183105
Epoch 500, val loss: 1.0093965530395508
Epoch 510, training loss: 0.7160394191741943 = 0.056364890187978745 + 0.1 * 6.596745014190674
Epoch 510, val loss: 1.0243734121322632
Epoch 520, training loss: 0.7119296193122864 = 0.05263481289148331 + 0.1 * 6.592947959899902
Epoch 520, val loss: 1.0390821695327759
Epoch 530, training loss: 0.7081971168518066 = 0.049229156225919724 + 0.1 * 6.589679718017578
Epoch 530, val loss: 1.0534805059432983
Epoch 540, training loss: 0.7043363451957703 = 0.04611371457576752 + 0.1 * 6.582225799560547
Epoch 540, val loss: 1.0675462484359741
Epoch 550, training loss: 0.701251208782196 = 0.043257735669612885 + 0.1 * 6.579935073852539
Epoch 550, val loss: 1.0813312530517578
Epoch 560, training loss: 0.6977738738059998 = 0.040634866803884506 + 0.1 * 6.571389675140381
Epoch 560, val loss: 1.0947461128234863
Epoch 570, training loss: 0.6948534846305847 = 0.0382278710603714 + 0.1 * 6.566256046295166
Epoch 570, val loss: 1.1078230142593384
Epoch 580, training loss: 0.6931161284446716 = 0.036006707698106766 + 0.1 * 6.571094036102295
Epoch 580, val loss: 1.1206692457199097
Epoch 590, training loss: 0.6915709972381592 = 0.03396458178758621 + 0.1 * 6.576064109802246
Epoch 590, val loss: 1.133164882659912
Epoch 600, training loss: 0.6889827847480774 = 0.03208474442362785 + 0.1 * 6.5689802169799805
Epoch 600, val loss: 1.1452765464782715
Epoch 610, training loss: 0.6858875751495361 = 0.03035195916891098 + 0.1 * 6.555356502532959
Epoch 610, val loss: 1.15711510181427
Epoch 620, training loss: 0.6847544312477112 = 0.028745876625180244 + 0.1 * 6.560085296630859
Epoch 620, val loss: 1.1687229871749878
Epoch 630, training loss: 0.6817197799682617 = 0.02725907787680626 + 0.1 * 6.544607162475586
Epoch 630, val loss: 1.1800552606582642
Epoch 640, training loss: 0.6814704537391663 = 0.02587883733212948 + 0.1 * 6.555915832519531
Epoch 640, val loss: 1.1911237239837646
Epoch 650, training loss: 0.6793551445007324 = 0.024602621793746948 + 0.1 * 6.547524929046631
Epoch 650, val loss: 1.201866626739502
Epoch 660, training loss: 0.6776659488677979 = 0.023416690528392792 + 0.1 * 6.542492389678955
Epoch 660, val loss: 1.2123748064041138
Epoch 670, training loss: 0.6772801280021667 = 0.02231236733496189 + 0.1 * 6.549677848815918
Epoch 670, val loss: 1.2226264476776123
Epoch 680, training loss: 0.6747426390647888 = 0.021285297349095345 + 0.1 * 6.534573554992676
Epoch 680, val loss: 1.232592225074768
Epoch 690, training loss: 0.6733582019805908 = 0.020325958728790283 + 0.1 * 6.530322551727295
Epoch 690, val loss: 1.2423455715179443
Epoch 700, training loss: 0.6716148853302002 = 0.019429830834269524 + 0.1 * 6.5218505859375
Epoch 700, val loss: 1.2518748044967651
Epoch 710, training loss: 0.6717488765716553 = 0.018591314554214478 + 0.1 * 6.531575679779053
Epoch 710, val loss: 1.261194109916687
Epoch 720, training loss: 0.6700328588485718 = 0.0178096741437912 + 0.1 * 6.522231578826904
Epoch 720, val loss: 1.2702667713165283
Epoch 730, training loss: 0.6687187552452087 = 0.0170745849609375 + 0.1 * 6.516441345214844
Epoch 730, val loss: 1.2791800498962402
Epoch 740, training loss: 0.6688648462295532 = 0.01638447307050228 + 0.1 * 6.524803638458252
Epoch 740, val loss: 1.2879198789596558
Epoch 750, training loss: 0.66783607006073 = 0.015736794099211693 + 0.1 * 6.520992279052734
Epoch 750, val loss: 1.296492338180542
Epoch 760, training loss: 0.6670247912406921 = 0.015126660466194153 + 0.1 * 6.518981456756592
Epoch 760, val loss: 1.3048577308654785
Epoch 770, training loss: 0.6651884317398071 = 0.014553500339388847 + 0.1 * 6.506349563598633
Epoch 770, val loss: 1.3130311965942383
Epoch 780, training loss: 0.6644085049629211 = 0.014012644067406654 + 0.1 * 6.503958702087402
Epoch 780, val loss: 1.3210914134979248
Epoch 790, training loss: 0.6642162203788757 = 0.013502485118806362 + 0.1 * 6.507137298583984
Epoch 790, val loss: 1.3289376497268677
Epoch 800, training loss: 0.6633545756340027 = 0.013021924532949924 + 0.1 * 6.503326416015625
Epoch 800, val loss: 1.3366283178329468
Epoch 810, training loss: 0.6623200178146362 = 0.01256705541163683 + 0.1 * 6.49752950668335
Epoch 810, val loss: 1.3441578149795532
Epoch 820, training loss: 0.6618170142173767 = 0.012137418612837791 + 0.1 * 6.496796131134033
Epoch 820, val loss: 1.3515859842300415
Epoch 830, training loss: 0.6609069108963013 = 0.0117294080555439 + 0.1 * 6.491775035858154
Epoch 830, val loss: 1.3588279485702515
Epoch 840, training loss: 0.6606644988059998 = 0.011343283578753471 + 0.1 * 6.49321174621582
Epoch 840, val loss: 1.3659387826919556
Epoch 850, training loss: 0.659537136554718 = 0.010976459830999374 + 0.1 * 6.485606670379639
Epoch 850, val loss: 1.3728859424591064
Epoch 860, training loss: 0.6600023508071899 = 0.010628031566739082 + 0.1 * 6.493742942810059
Epoch 860, val loss: 1.3797167539596558
Epoch 870, training loss: 0.6595674157142639 = 0.010297257453203201 + 0.1 * 6.492701530456543
Epoch 870, val loss: 1.3863905668258667
Epoch 880, training loss: 0.6575348973274231 = 0.009983169846236706 + 0.1 * 6.475517272949219
Epoch 880, val loss: 1.3929418325424194
Epoch 890, training loss: 0.657321035861969 = 0.009683736599981785 + 0.1 * 6.476373195648193
Epoch 890, val loss: 1.3993921279907227
Epoch 900, training loss: 0.6572345495223999 = 0.009398155845701694 + 0.1 * 6.478363990783691
Epoch 900, val loss: 1.4057093858718872
Epoch 910, training loss: 0.657228946685791 = 0.009126558899879456 + 0.1 * 6.481023788452148
Epoch 910, val loss: 1.4119821786880493
Epoch 920, training loss: 0.6576698422431946 = 0.008866977877914906 + 0.1 * 6.488028526306152
Epoch 920, val loss: 1.4180562496185303
Epoch 930, training loss: 0.6553061604499817 = 0.008619849570095539 + 0.1 * 6.46686315536499
Epoch 930, val loss: 1.424035668373108
Epoch 940, training loss: 0.6549570560455322 = 0.008384300395846367 + 0.1 * 6.46572732925415
Epoch 940, val loss: 1.4299579858779907
Epoch 950, training loss: 0.656849205493927 = 0.008158114738762379 + 0.1 * 6.486910820007324
Epoch 950, val loss: 1.4357460737228394
Epoch 960, training loss: 0.6541921496391296 = 0.007941942662000656 + 0.1 * 6.462501525878906
Epoch 960, val loss: 1.4414982795715332
Epoch 970, training loss: 0.6541411876678467 = 0.007735130842775106 + 0.1 * 6.464060306549072
Epoch 970, val loss: 1.44719660282135
Epoch 980, training loss: 0.6534273624420166 = 0.007537032011896372 + 0.1 * 6.4589033126831055
Epoch 980, val loss: 1.4527722597122192
Epoch 990, training loss: 0.6538426876068115 = 0.00734657933935523 + 0.1 * 6.464961051940918
Epoch 990, val loss: 1.4582661390304565
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8344754876120191
The final CL Acc:0.80370, 0.01814, The final GNN Acc:0.84115, 0.00503
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10588])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.790691614151001 = 1.9310060739517212 + 0.1 * 8.596855163574219
Epoch 0, val loss: 1.9230692386627197
Epoch 10, training loss: 2.781803846359253 = 1.9221224784851074 + 0.1 * 8.596813201904297
Epoch 10, val loss: 1.9142062664031982
Epoch 20, training loss: 2.771113395690918 = 1.911456823348999 + 0.1 * 8.596565246582031
Epoch 20, val loss: 1.9035357236862183
Epoch 30, training loss: 2.7559800148010254 = 1.8965197801589966 + 0.1 * 8.594603538513184
Epoch 30, val loss: 1.888673186302185
Epoch 40, training loss: 2.7319750785827637 = 1.8742579221725464 + 0.1 * 8.577170372009277
Epoch 40, val loss: 1.8671629428863525
Epoch 50, training loss: 2.6915714740753174 = 1.8433624505996704 + 0.1 * 8.482090950012207
Epoch 50, val loss: 1.8388984203338623
Epoch 60, training loss: 2.638716697692871 = 1.8099634647369385 + 0.1 * 8.287532806396484
Epoch 60, val loss: 1.8113471269607544
Epoch 70, training loss: 2.581221342086792 = 1.78034245967865 + 0.1 * 8.008788108825684
Epoch 70, val loss: 1.787515640258789
Epoch 80, training loss: 2.507617473602295 = 1.7490129470825195 + 0.1 * 7.5860443115234375
Epoch 80, val loss: 1.7598350048065186
Epoch 90, training loss: 2.444247007369995 = 1.7097467184066772 + 0.1 * 7.345003604888916
Epoch 90, val loss: 1.7268378734588623
Epoch 100, training loss: 2.3791463375091553 = 1.6565157175064087 + 0.1 * 7.226306915283203
Epoch 100, val loss: 1.680991530418396
Epoch 110, training loss: 2.300295829772949 = 1.5880730152130127 + 0.1 * 7.122228145599365
Epoch 110, val loss: 1.621605634689331
Epoch 120, training loss: 2.212111711502075 = 1.5068132877349854 + 0.1 * 7.052984714508057
Epoch 120, val loss: 1.5556646585464478
Epoch 130, training loss: 2.119490385055542 = 1.4182735681533813 + 0.1 * 7.012168884277344
Epoch 130, val loss: 1.4843664169311523
Epoch 140, training loss: 2.024996757507324 = 1.326819896697998 + 0.1 * 6.98176908493042
Epoch 140, val loss: 1.411352276802063
Epoch 150, training loss: 1.9271761178970337 = 1.2316702604293823 + 0.1 * 6.955058574676514
Epoch 150, val loss: 1.3369865417480469
Epoch 160, training loss: 1.8272056579589844 = 1.1337049007415771 + 0.1 * 6.935006618499756
Epoch 160, val loss: 1.261626958847046
Epoch 170, training loss: 1.7283999919891357 = 1.0368670225143433 + 0.1 * 6.915329456329346
Epoch 170, val loss: 1.1892263889312744
Epoch 180, training loss: 1.6356356143951416 = 0.9453977346420288 + 0.1 * 6.902379035949707
Epoch 180, val loss: 1.1233398914337158
Epoch 190, training loss: 1.5533524751663208 = 0.8647499680519104 + 0.1 * 6.8860249519348145
Epoch 190, val loss: 1.067941427230835
Epoch 200, training loss: 1.4814190864562988 = 0.7946129441261292 + 0.1 * 6.868061542510986
Epoch 200, val loss: 1.0221067667007446
Epoch 210, training loss: 1.419797658920288 = 0.7345986366271973 + 0.1 * 6.851989269256592
Epoch 210, val loss: 0.9855926632881165
Epoch 220, training loss: 1.3660993576049805 = 0.6825642585754395 + 0.1 * 6.83535099029541
Epoch 220, val loss: 0.956263542175293
Epoch 230, training loss: 1.318500280380249 = 0.6361852288246155 + 0.1 * 6.823151111602783
Epoch 230, val loss: 0.932892918586731
Epoch 240, training loss: 1.2745263576507568 = 0.5936546325683594 + 0.1 * 6.808716773986816
Epoch 240, val loss: 0.9142290353775024
Epoch 250, training loss: 1.2332072257995605 = 0.5537591576576233 + 0.1 * 6.794480323791504
Epoch 250, val loss: 0.9000042676925659
Epoch 260, training loss: 1.1947731971740723 = 0.5159866809844971 + 0.1 * 6.78786563873291
Epoch 260, val loss: 0.8897539973258972
Epoch 270, training loss: 1.1578525304794312 = 0.4802936613559723 + 0.1 * 6.775588035583496
Epoch 270, val loss: 0.8831847310066223
Epoch 280, training loss: 1.1242272853851318 = 0.4467369019985199 + 0.1 * 6.774903774261475
Epoch 280, val loss: 0.8799282908439636
Epoch 290, training loss: 1.091498851776123 = 0.4155595898628235 + 0.1 * 6.759393215179443
Epoch 290, val loss: 0.8796562552452087
Epoch 300, training loss: 1.0620484352111816 = 0.38682106137275696 + 0.1 * 6.752273082733154
Epoch 300, val loss: 0.8821717500686646
Epoch 310, training loss: 1.035111427307129 = 0.36062532663345337 + 0.1 * 6.744861602783203
Epoch 310, val loss: 0.8870405554771423
Epoch 320, training loss: 1.010298252105713 = 0.3364476263523102 + 0.1 * 6.738506317138672
Epoch 320, val loss: 0.8938670754432678
Epoch 330, training loss: 0.9873628616333008 = 0.31380680203437805 + 0.1 * 6.735560417175293
Epoch 330, val loss: 0.9020017981529236
Epoch 340, training loss: 0.9652884006500244 = 0.29223257303237915 + 0.1 * 6.730557918548584
Epoch 340, val loss: 0.9109417796134949
Epoch 350, training loss: 0.943884015083313 = 0.2710971236228943 + 0.1 * 6.727868556976318
Epoch 350, val loss: 0.920227587223053
Epoch 360, training loss: 0.9222660064697266 = 0.2501347064971924 + 0.1 * 6.721312999725342
Epoch 360, val loss: 0.9295611381530762
Epoch 370, training loss: 0.9001122117042542 = 0.229376420378685 + 0.1 * 6.707357883453369
Epoch 370, val loss: 0.9389890432357788
Epoch 380, training loss: 0.8812663555145264 = 0.20926150679588318 + 0.1 * 6.720048904418945
Epoch 380, val loss: 0.9488533139228821
Epoch 390, training loss: 0.8609582185745239 = 0.19049254059791565 + 0.1 * 6.704656600952148
Epoch 390, val loss: 0.9595843553543091
Epoch 400, training loss: 0.8429023027420044 = 0.17342504858970642 + 0.1 * 6.694772243499756
Epoch 400, val loss: 0.9715988636016846
Epoch 410, training loss: 0.8300375938415527 = 0.15810763835906982 + 0.1 * 6.71929931640625
Epoch 410, val loss: 0.9850741028785706
Epoch 420, training loss: 0.8134405612945557 = 0.14451247453689575 + 0.1 * 6.6892805099487305
Epoch 420, val loss: 0.9998871684074402
Epoch 430, training loss: 0.8007634878158569 = 0.13238170742988586 + 0.1 * 6.683817386627197
Epoch 430, val loss: 1.0159388780593872
Epoch 440, training loss: 0.7891853451728821 = 0.121495820581913 + 0.1 * 6.676894664764404
Epoch 440, val loss: 1.0330663919448853
Epoch 450, training loss: 0.7789793014526367 = 0.11171029508113861 + 0.1 * 6.672689914703369
Epoch 450, val loss: 1.050944209098816
Epoch 460, training loss: 0.7710670232772827 = 0.10291871428489685 + 0.1 * 6.681483268737793
Epoch 460, val loss: 1.0692344903945923
Epoch 470, training loss: 0.7612869143486023 = 0.09498593956232071 + 0.1 * 6.663009166717529
Epoch 470, val loss: 1.0878080129623413
Epoch 480, training loss: 0.7531189918518066 = 0.08778394758701324 + 0.1 * 6.653350353240967
Epoch 480, val loss: 1.1066429615020752
Epoch 490, training loss: 0.7490198016166687 = 0.08122660964727402 + 0.1 * 6.677931785583496
Epoch 490, val loss: 1.1256749629974365
Epoch 500, training loss: 0.7408946752548218 = 0.07528471946716309 + 0.1 * 6.656099319458008
Epoch 500, val loss: 1.1446030139923096
Epoch 510, training loss: 0.7340821027755737 = 0.06988902390003204 + 0.1 * 6.641931056976318
Epoch 510, val loss: 1.1634646654129028
Epoch 520, training loss: 0.7310185432434082 = 0.06496712565422058 + 0.1 * 6.660513877868652
Epoch 520, val loss: 1.1822926998138428
Epoch 530, training loss: 0.7236939072608948 = 0.060490529984235764 + 0.1 * 6.632033348083496
Epoch 530, val loss: 1.2008023262023926
Epoch 540, training loss: 0.7191760540008545 = 0.056412629783153534 + 0.1 * 6.627634048461914
Epoch 540, val loss: 1.2191894054412842
Epoch 550, training loss: 0.7151899933815002 = 0.05269096791744232 + 0.1 * 6.624989986419678
Epoch 550, val loss: 1.237410306930542
Epoch 560, training loss: 0.7116786241531372 = 0.049292005598545074 + 0.1 * 6.623866081237793
Epoch 560, val loss: 1.2552951574325562
Epoch 570, training loss: 0.708448588848114 = 0.04618547856807709 + 0.1 * 6.622631072998047
Epoch 570, val loss: 1.272897720336914
Epoch 580, training loss: 0.7045426368713379 = 0.043337684124708176 + 0.1 * 6.612049102783203
Epoch 580, val loss: 1.2900794744491577
Epoch 590, training loss: 0.7023572325706482 = 0.04072190448641777 + 0.1 * 6.6163530349731445
Epoch 590, val loss: 1.3068886995315552
Epoch 600, training loss: 0.6986777782440186 = 0.03832630813121796 + 0.1 * 6.603514194488525
Epoch 600, val loss: 1.323301911354065
Epoch 610, training loss: 0.6970659494400024 = 0.0361211933195591 + 0.1 * 6.609447479248047
Epoch 610, val loss: 1.3393194675445557
Epoch 620, training loss: 0.693105161190033 = 0.034091632813215256 + 0.1 * 6.590135097503662
Epoch 620, val loss: 1.3549383878707886
Epoch 630, training loss: 0.6921400427818298 = 0.03222038969397545 + 0.1 * 6.599196910858154
Epoch 630, val loss: 1.3702481985092163
Epoch 640, training loss: 0.6902269124984741 = 0.030493758618831635 + 0.1 * 6.597331523895264
Epoch 640, val loss: 1.3849955797195435
Epoch 650, training loss: 0.6891968250274658 = 0.028900817036628723 + 0.1 * 6.602960109710693
Epoch 650, val loss: 1.3994040489196777
Epoch 660, training loss: 0.685352087020874 = 0.02742706425487995 + 0.1 * 6.579249858856201
Epoch 660, val loss: 1.413392186164856
Epoch 670, training loss: 0.6839919686317444 = 0.02606125734746456 + 0.1 * 6.5793070793151855
Epoch 670, val loss: 1.4270752668380737
Epoch 680, training loss: 0.6829816102981567 = 0.02479393593966961 + 0.1 * 6.581876754760742
Epoch 680, val loss: 1.4404332637786865
Epoch 690, training loss: 0.6837683320045471 = 0.02361408807337284 + 0.1 * 6.601541996002197
Epoch 690, val loss: 1.4533360004425049
Epoch 700, training loss: 0.6789030432701111 = 0.022518126294016838 + 0.1 * 6.563849449157715
Epoch 700, val loss: 1.4659066200256348
Epoch 710, training loss: 0.6783027648925781 = 0.021496908739209175 + 0.1 * 6.568058490753174
Epoch 710, val loss: 1.4781962633132935
Epoch 720, training loss: 0.6762666702270508 = 0.02054336480796337 + 0.1 * 6.5572333335876465
Epoch 720, val loss: 1.4901235103607178
Epoch 730, training loss: 0.6761891841888428 = 0.019651923328638077 + 0.1 * 6.565371990203857
Epoch 730, val loss: 1.5017945766448975
Epoch 740, training loss: 0.675187349319458 = 0.018818169832229614 + 0.1 * 6.563691139221191
Epoch 740, val loss: 1.5131512880325317
Epoch 750, training loss: 0.6737744808197021 = 0.018035929650068283 + 0.1 * 6.557384967803955
Epoch 750, val loss: 1.524247169494629
Epoch 760, training loss: 0.672519326210022 = 0.01730216108262539 + 0.1 * 6.55217170715332
Epoch 760, val loss: 1.5349774360656738
Epoch 770, training loss: 0.6715208292007446 = 0.01661490462720394 + 0.1 * 6.54905891418457
Epoch 770, val loss: 1.54550039768219
Epoch 780, training loss: 0.6700800657272339 = 0.015968075022101402 + 0.1 * 6.541119575500488
Epoch 780, val loss: 1.5558135509490967
Epoch 790, training loss: 0.6698446869850159 = 0.015358962118625641 + 0.1 * 6.544857025146484
Epoch 790, val loss: 1.5658984184265137
Epoch 800, training loss: 0.6726086139678955 = 0.014785132370889187 + 0.1 * 6.578234672546387
Epoch 800, val loss: 1.575643539428711
Epoch 810, training loss: 0.6681734323501587 = 0.014243083074688911 + 0.1 * 6.539303302764893
Epoch 810, val loss: 1.585129976272583
Epoch 820, training loss: 0.6686437726020813 = 0.013733245432376862 + 0.1 * 6.549105167388916
Epoch 820, val loss: 1.5944486856460571
Epoch 830, training loss: 0.6660908460617065 = 0.013252236880362034 + 0.1 * 6.528385639190674
Epoch 830, val loss: 1.6035088300704956
Epoch 840, training loss: 0.6654983162879944 = 0.012797468341886997 + 0.1 * 6.527008056640625
Epoch 840, val loss: 1.6124154329299927
Epoch 850, training loss: 0.667120099067688 = 0.012366443872451782 + 0.1 * 6.547536849975586
Epoch 850, val loss: 1.6210864782333374
Epoch 860, training loss: 0.6648404002189636 = 0.01195766031742096 + 0.1 * 6.52882719039917
Epoch 860, val loss: 1.6295068264007568
Epoch 870, training loss: 0.6639233827590942 = 0.01157055702060461 + 0.1 * 6.523528099060059
Epoch 870, val loss: 1.637815237045288
Epoch 880, training loss: 0.665389895439148 = 0.011202622205018997 + 0.1 * 6.541872501373291
Epoch 880, val loss: 1.6459317207336426
Epoch 890, training loss: 0.6625908613204956 = 0.010852300561964512 + 0.1 * 6.517385959625244
Epoch 890, val loss: 1.6537750959396362
Epoch 900, training loss: 0.6627013087272644 = 0.010520749725401402 + 0.1 * 6.521805286407471
Epoch 900, val loss: 1.661511778831482
Epoch 910, training loss: 0.6615261435508728 = 0.010205049999058247 + 0.1 * 6.513210773468018
Epoch 910, val loss: 1.6691020727157593
Epoch 920, training loss: 0.6612697243690491 = 0.009904036298394203 + 0.1 * 6.513657093048096
Epoch 920, val loss: 1.6765317916870117
Epoch 930, training loss: 0.6607213020324707 = 0.009617640636861324 + 0.1 * 6.511036396026611
Epoch 930, val loss: 1.6837878227233887
Epoch 940, training loss: 0.6598489880561829 = 0.009343958459794521 + 0.1 * 6.505050182342529
Epoch 940, val loss: 1.6909070014953613
Epoch 950, training loss: 0.6595674157142639 = 0.009082617238163948 + 0.1 * 6.504848003387451
Epoch 950, val loss: 1.6978607177734375
Epoch 960, training loss: 0.6602938175201416 = 0.008833312429487705 + 0.1 * 6.5146050453186035
Epoch 960, val loss: 1.7046664953231812
Epoch 970, training loss: 0.6586982011795044 = 0.008595342747867107 + 0.1 * 6.501028537750244
Epoch 970, val loss: 1.7113173007965088
Epoch 980, training loss: 0.6593350172042847 = 0.008367512375116348 + 0.1 * 6.509674549102783
Epoch 980, val loss: 1.7179067134857178
Epoch 990, training loss: 0.6582541465759277 = 0.008148940280079842 + 0.1 * 6.501051425933838
Epoch 990, val loss: 1.7243013381958008
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 2.8102452754974365 = 1.9505585432052612 + 0.1 * 8.596866607666016
Epoch 0, val loss: 1.9552950859069824
Epoch 10, training loss: 2.800875663757324 = 1.9411932229995728 + 0.1 * 8.59682559967041
Epoch 10, val loss: 1.9458961486816406
Epoch 20, training loss: 2.789306879043579 = 1.929651141166687 + 0.1 * 8.596556663513184
Epoch 20, val loss: 1.9342079162597656
Epoch 30, training loss: 2.772864818572998 = 1.913429617881775 + 0.1 * 8.594351768493652
Epoch 30, val loss: 1.9177935123443604
Epoch 40, training loss: 2.7471537590026855 = 1.8894760608673096 + 0.1 * 8.576776504516602
Epoch 40, val loss: 1.89382803440094
Epoch 50, training loss: 2.705718517303467 = 1.855865240097046 + 0.1 * 8.498531341552734
Epoch 50, val loss: 1.8615295886993408
Epoch 60, training loss: 2.650862455368042 = 1.8178166151046753 + 0.1 * 8.330458641052246
Epoch 60, val loss: 1.8273711204528809
Epoch 70, training loss: 2.6059958934783936 = 1.7841142416000366 + 0.1 * 8.218815803527832
Epoch 70, val loss: 1.7974379062652588
Epoch 80, training loss: 2.54514479637146 = 1.7503665685653687 + 0.1 * 7.947782516479492
Epoch 80, val loss: 1.763597011566162
Epoch 90, training loss: 2.4689857959747314 = 1.7069470882415771 + 0.1 * 7.620386600494385
Epoch 90, val loss: 1.7224916219711304
Epoch 100, training loss: 2.390373706817627 = 1.6524848937988281 + 0.1 * 7.3788886070251465
Epoch 100, val loss: 1.6760236024856567
Epoch 110, training loss: 2.304490327835083 = 1.5815588235855103 + 0.1 * 7.229314804077148
Epoch 110, val loss: 1.6155656576156616
Epoch 120, training loss: 2.210872173309326 = 1.4982634782791138 + 0.1 * 7.126086235046387
Epoch 120, val loss: 1.544705867767334
Epoch 130, training loss: 2.1186912059783936 = 1.4117097854614258 + 0.1 * 7.069813251495361
Epoch 130, val loss: 1.4724675416946411
Epoch 140, training loss: 2.0290069580078125 = 1.3258984088897705 + 0.1 * 7.0310845375061035
Epoch 140, val loss: 1.4036531448364258
Epoch 150, training loss: 1.9436845779418945 = 1.2430895566940308 + 0.1 * 7.005949974060059
Epoch 150, val loss: 1.339142918586731
Epoch 160, training loss: 1.8623402118682861 = 1.1639922857284546 + 0.1 * 6.983479022979736
Epoch 160, val loss: 1.2795151472091675
Epoch 170, training loss: 1.7825758457183838 = 1.0858676433563232 + 0.1 * 6.967081546783447
Epoch 170, val loss: 1.2219430208206177
Epoch 180, training loss: 1.7014739513397217 = 1.006173849105835 + 0.1 * 6.953000068664551
Epoch 180, val loss: 1.1631704568862915
Epoch 190, training loss: 1.6197866201400757 = 0.9250518679618835 + 0.1 * 6.947347164154053
Epoch 190, val loss: 1.1039035320281982
Epoch 200, training loss: 1.5390584468841553 = 0.8458114862442017 + 0.1 * 6.932469367980957
Epoch 200, val loss: 1.0471088886260986
Epoch 210, training loss: 1.4646692276000977 = 0.7722126245498657 + 0.1 * 6.924565315246582
Epoch 210, val loss: 0.9969106912612915
Epoch 220, training loss: 1.3991703987121582 = 0.7072468996047974 + 0.1 * 6.9192352294921875
Epoch 220, val loss: 0.9564471244812012
Epoch 230, training loss: 1.3429782390594482 = 0.6517575979232788 + 0.1 * 6.912207126617432
Epoch 230, val loss: 0.926643431186676
Epoch 240, training loss: 1.2943124771118164 = 0.6037880182266235 + 0.1 * 6.905243873596191
Epoch 240, val loss: 0.9057608246803284
Epoch 250, training loss: 1.2523937225341797 = 0.561680793762207 + 0.1 * 6.907129764556885
Epoch 250, val loss: 0.8919513821601868
Epoch 260, training loss: 1.2137298583984375 = 0.5242311954498291 + 0.1 * 6.894987106323242
Epoch 260, val loss: 0.8834789991378784
Epoch 270, training loss: 1.178961992263794 = 0.48994210362434387 + 0.1 * 6.890199184417725
Epoch 270, val loss: 0.8787412643432617
Epoch 280, training loss: 1.1472985744476318 = 0.4577246606349945 + 0.1 * 6.8957390785217285
Epoch 280, val loss: 0.8766363263130188
Epoch 290, training loss: 1.1152936220169067 = 0.4269665777683258 + 0.1 * 6.883269786834717
Epoch 290, val loss: 0.8764065504074097
Epoch 300, training loss: 1.084689974784851 = 0.39688774943351746 + 0.1 * 6.878021717071533
Epoch 300, val loss: 0.8776731491088867
Epoch 310, training loss: 1.0545942783355713 = 0.367055743932724 + 0.1 * 6.87538480758667
Epoch 310, val loss: 0.8800849914550781
Epoch 320, training loss: 1.0246809720993042 = 0.33760276436805725 + 0.1 * 6.870781898498535
Epoch 320, val loss: 0.8835836052894592
Epoch 330, training loss: 0.9953399896621704 = 0.30867961049079895 + 0.1 * 6.866603851318359
Epoch 330, val loss: 0.8880769610404968
Epoch 340, training loss: 0.9667533040046692 = 0.28045815229415894 + 0.1 * 6.862951278686523
Epoch 340, val loss: 0.8937981724739075
Epoch 350, training loss: 0.9402539730072021 = 0.2535293996334076 + 0.1 * 6.867245197296143
Epoch 350, val loss: 0.9009570479393005
Epoch 360, training loss: 0.9142413139343262 = 0.2286454141139984 + 0.1 * 6.855958461761475
Epoch 360, val loss: 0.9094505310058594
Epoch 370, training loss: 0.8920501470565796 = 0.2062496691942215 + 0.1 * 6.858004570007324
Epoch 370, val loss: 0.9196059107780457
Epoch 380, training loss: 0.8714401125907898 = 0.1864730715751648 + 0.1 * 6.84967041015625
Epoch 380, val loss: 0.9312698841094971
Epoch 390, training loss: 0.8531180024147034 = 0.16901223361492157 + 0.1 * 6.841057300567627
Epoch 390, val loss: 0.9445814490318298
Epoch 400, training loss: 0.8375663161277771 = 0.15348154306411743 + 0.1 * 6.840847492218018
Epoch 400, val loss: 0.959378182888031
Epoch 410, training loss: 0.8229708671569824 = 0.13962489366531372 + 0.1 * 6.833459854125977
Epoch 410, val loss: 0.9753825068473816
Epoch 420, training loss: 0.8098539113998413 = 0.12717853486537933 + 0.1 * 6.826753616333008
Epoch 420, val loss: 0.9923915266990662
Epoch 430, training loss: 0.7980830669403076 = 0.1158992126584053 + 0.1 * 6.82183837890625
Epoch 430, val loss: 1.0103200674057007
Epoch 440, training loss: 0.7880223989486694 = 0.10568453371524811 + 0.1 * 6.823378562927246
Epoch 440, val loss: 1.0289320945739746
Epoch 450, training loss: 0.7774906158447266 = 0.09643193334341049 + 0.1 * 6.810586929321289
Epoch 450, val loss: 1.0480350255966187
Epoch 460, training loss: 0.7689757943153381 = 0.08803307265043259 + 0.1 * 6.809427261352539
Epoch 460, val loss: 1.067508578300476
Epoch 470, training loss: 0.7608822584152222 = 0.08044099062681198 + 0.1 * 6.804412364959717
Epoch 470, val loss: 1.087098479270935
Epoch 480, training loss: 0.7526438236236572 = 0.07359810918569565 + 0.1 * 6.790457248687744
Epoch 480, val loss: 1.1068083047866821
Epoch 490, training loss: 0.7469172477722168 = 0.06744395941495895 + 0.1 * 6.794732570648193
Epoch 490, val loss: 1.1266241073608398
Epoch 500, training loss: 0.740531861782074 = 0.061926618218421936 + 0.1 * 6.786052703857422
Epoch 500, val loss: 1.1463525295257568
Epoch 510, training loss: 0.7346019148826599 = 0.056991156190633774 + 0.1 * 6.776107311248779
Epoch 510, val loss: 1.165858268737793
Epoch 520, training loss: 0.7295176386833191 = 0.052574239671230316 + 0.1 * 6.769433975219727
Epoch 520, val loss: 1.1851043701171875
Epoch 530, training loss: 0.7244421243667603 = 0.048621829599142075 + 0.1 * 6.75820255279541
Epoch 530, val loss: 1.2041246891021729
Epoch 540, training loss: 0.7205063104629517 = 0.04508109390735626 + 0.1 * 6.7542524337768555
Epoch 540, val loss: 1.2227860689163208
Epoch 550, training loss: 0.7159789800643921 = 0.04190058633685112 + 0.1 * 6.74078369140625
Epoch 550, val loss: 1.2411131858825684
Epoch 560, training loss: 0.7125992774963379 = 0.039043135941028595 + 0.1 * 6.735561370849609
Epoch 560, val loss: 1.258961796760559
Epoch 570, training loss: 0.7091368436813354 = 0.0364609956741333 + 0.1 * 6.7267584800720215
Epoch 570, val loss: 1.276594638824463
Epoch 580, training loss: 0.7073343396186829 = 0.03412359207868576 + 0.1 * 6.732107162475586
Epoch 580, val loss: 1.2936675548553467
Epoch 590, training loss: 0.7054182887077332 = 0.032009754329919815 + 0.1 * 6.7340850830078125
Epoch 590, val loss: 1.310301661491394
Epoch 600, training loss: 0.7014670372009277 = 0.030090808868408203 + 0.1 * 6.713762283325195
Epoch 600, val loss: 1.326441764831543
Epoch 610, training loss: 0.6987161636352539 = 0.02834182046353817 + 0.1 * 6.703742980957031
Epoch 610, val loss: 1.3422341346740723
Epoch 620, training loss: 0.6983082890510559 = 0.026742715388536453 + 0.1 * 6.71565580368042
Epoch 620, val loss: 1.3577141761779785
Epoch 630, training loss: 0.6943252682685852 = 0.02527616173028946 + 0.1 * 6.69049072265625
Epoch 630, val loss: 1.3727021217346191
Epoch 640, training loss: 0.6932507157325745 = 0.023929130285978317 + 0.1 * 6.693215847015381
Epoch 640, val loss: 1.387359857559204
Epoch 650, training loss: 0.6922475695610046 = 0.02269219420850277 + 0.1 * 6.695553302764893
Epoch 650, val loss: 1.401686429977417
Epoch 660, training loss: 0.6906458139419556 = 0.021547822281718254 + 0.1 * 6.690979957580566
Epoch 660, val loss: 1.4156204462051392
Epoch 670, training loss: 0.6883437037467957 = 0.02049267664551735 + 0.1 * 6.6785101890563965
Epoch 670, val loss: 1.4291027784347534
Epoch 680, training loss: 0.6870785355567932 = 0.019515419378876686 + 0.1 * 6.675631046295166
Epoch 680, val loss: 1.4424315690994263
Epoch 690, training loss: 0.685677170753479 = 0.018608979880809784 + 0.1 * 6.670681953430176
Epoch 690, val loss: 1.4552319049835205
Epoch 700, training loss: 0.6842976212501526 = 0.017766598612070084 + 0.1 * 6.665309906005859
Epoch 700, val loss: 1.4677588939666748
Epoch 710, training loss: 0.6853322982788086 = 0.01698250323534012 + 0.1 * 6.683497905731201
Epoch 710, val loss: 1.4799667596817017
Epoch 720, training loss: 0.6814358830451965 = 0.016253391280770302 + 0.1 * 6.651824474334717
Epoch 720, val loss: 1.491722822189331
Epoch 730, training loss: 0.6804627776145935 = 0.015571842901408672 + 0.1 * 6.648909568786621
Epoch 730, val loss: 1.5032777786254883
Epoch 740, training loss: 0.6821044087409973 = 0.014935544691979885 + 0.1 * 6.671688556671143
Epoch 740, val loss: 1.514611840248108
Epoch 750, training loss: 0.6783857941627502 = 0.014339171350002289 + 0.1 * 6.640466213226318
Epoch 750, val loss: 1.5255355834960938
Epoch 760, training loss: 0.6791785955429077 = 0.013779839500784874 + 0.1 * 6.653987884521484
Epoch 760, val loss: 1.5361778736114502
Epoch 770, training loss: 0.677041232585907 = 0.01325585413724184 + 0.1 * 6.637853622436523
Epoch 770, val loss: 1.5465805530548096
Epoch 780, training loss: 0.6765536665916443 = 0.012764004059135914 + 0.1 * 6.63789701461792
Epoch 780, val loss: 1.5567127466201782
Epoch 790, training loss: 0.6746035218238831 = 0.012300883419811726 + 0.1 * 6.623025894165039
Epoch 790, val loss: 1.5666661262512207
Epoch 800, training loss: 0.6739757061004639 = 0.01186484657227993 + 0.1 * 6.621108531951904
Epoch 800, val loss: 1.5763922929763794
Epoch 810, training loss: 0.6731410026550293 = 0.011452870443463326 + 0.1 * 6.616881370544434
Epoch 810, val loss: 1.5859146118164062
Epoch 820, training loss: 0.6734296679496765 = 0.011062596924602985 + 0.1 * 6.62367057800293
Epoch 820, val loss: 1.5950911045074463
Epoch 830, training loss: 0.673150897026062 = 0.010695023462176323 + 0.1 * 6.624558448791504
Epoch 830, val loss: 1.6039750576019287
Epoch 840, training loss: 0.6717365384101868 = 0.01034753117710352 + 0.1 * 6.613889694213867
Epoch 840, val loss: 1.6126984357833862
Epoch 850, training loss: 0.6709012389183044 = 0.010018902830779552 + 0.1 * 6.608823299407959
Epoch 850, val loss: 1.6212745904922485
Epoch 860, training loss: 0.6702131628990173 = 0.009707232937216759 + 0.1 * 6.6050591468811035
Epoch 860, val loss: 1.6296542882919312
Epoch 870, training loss: 0.6703086495399475 = 0.00941035058349371 + 0.1 * 6.608983039855957
Epoch 870, val loss: 1.637818455696106
Epoch 880, training loss: 0.6691009998321533 = 0.00912892073392868 + 0.1 * 6.599720478057861
Epoch 880, val loss: 1.6457691192626953
Epoch 890, training loss: 0.6683298945426941 = 0.008861424401402473 + 0.1 * 6.594684600830078
Epoch 890, val loss: 1.6536864042282104
Epoch 900, training loss: 0.668935239315033 = 0.008606760762631893 + 0.1 * 6.60328483581543
Epoch 900, val loss: 1.6614550352096558
Epoch 910, training loss: 0.6677295565605164 = 0.008363937959074974 + 0.1 * 6.593655586242676
Epoch 910, val loss: 1.6689131259918213
Epoch 920, training loss: 0.6667145490646362 = 0.008132881484925747 + 0.1 * 6.585816383361816
Epoch 920, val loss: 1.6763269901275635
Epoch 930, training loss: 0.668449878692627 = 0.007912584580481052 + 0.1 * 6.605372905731201
Epoch 930, val loss: 1.6836696863174438
Epoch 940, training loss: 0.6664009690284729 = 0.007701650727540255 + 0.1 * 6.586993217468262
Epoch 940, val loss: 1.690674901008606
Epoch 950, training loss: 0.6662908792495728 = 0.0075004976242780685 + 0.1 * 6.58790397644043
Epoch 950, val loss: 1.6977107524871826
Epoch 960, training loss: 0.667730987071991 = 0.00730791175737977 + 0.1 * 6.604230880737305
Epoch 960, val loss: 1.7045050859451294
Epoch 970, training loss: 0.6651522517204285 = 0.007122865412384272 + 0.1 * 6.580293655395508
Epoch 970, val loss: 1.7110475301742554
Epoch 980, training loss: 0.6644492745399475 = 0.006947151385247707 + 0.1 * 6.575020790100098
Epoch 980, val loss: 1.7176334857940674
Epoch 990, training loss: 0.6643532514572144 = 0.006778353825211525 + 0.1 * 6.575748920440674
Epoch 990, val loss: 1.7240633964538574
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 2.8043293952941895 = 1.944643259048462 + 0.1 * 8.596860885620117
Epoch 0, val loss: 1.9348939657211304
Epoch 10, training loss: 2.7940845489501953 = 1.9344035387039185 + 0.1 * 8.596809387207031
Epoch 10, val loss: 1.9250845909118652
Epoch 20, training loss: 2.781327247619629 = 1.9216723442077637 + 0.1 * 8.596549987792969
Epoch 20, val loss: 1.912729024887085
Epoch 30, training loss: 2.7630796432495117 = 1.9036301374435425 + 0.1 * 8.594493865966797
Epoch 30, val loss: 1.8952494859695435
Epoch 40, training loss: 2.7350451946258545 = 1.877182126045227 + 0.1 * 8.578631401062012
Epoch 40, val loss: 1.8706191778182983
Epoch 50, training loss: 2.6925394535064697 = 1.8420391082763672 + 0.1 * 8.505002975463867
Epoch 50, val loss: 1.840607762336731
Epoch 60, training loss: 2.642329216003418 = 1.807224154472351 + 0.1 * 8.351051330566406
Epoch 60, val loss: 1.8144912719726562
Epoch 70, training loss: 2.6017935276031494 = 1.7782535552978516 + 0.1 * 8.23539924621582
Epoch 70, val loss: 1.7913429737091064
Epoch 80, training loss: 2.5370583534240723 = 1.7429453134536743 + 0.1 * 7.941129684448242
Epoch 80, val loss: 1.7575167417526245
Epoch 90, training loss: 2.456220865249634 = 1.6979926824569702 + 0.1 * 7.582282066345215
Epoch 90, val loss: 1.7165652513504028
Epoch 100, training loss: 2.3768808841705322 = 1.6396324634552002 + 0.1 * 7.372483253479004
Epoch 100, val loss: 1.668511986732483
Epoch 110, training loss: 2.2909247875213623 = 1.5663024187088013 + 0.1 * 7.246223449707031
Epoch 110, val loss: 1.6044824123382568
Epoch 120, training loss: 2.201230049133301 = 1.4882241487503052 + 0.1 * 7.130060195922852
Epoch 120, val loss: 1.5395673513412476
Epoch 130, training loss: 2.118706464767456 = 1.413720965385437 + 0.1 * 7.0498552322387695
Epoch 130, val loss: 1.481795310974121
Epoch 140, training loss: 2.042731523513794 = 1.3431280851364136 + 0.1 * 6.996034622192383
Epoch 140, val loss: 1.4288839101791382
Epoch 150, training loss: 1.9697282314300537 = 1.2742023468017578 + 0.1 * 6.955258369445801
Epoch 150, val loss: 1.377543330192566
Epoch 160, training loss: 1.8978121280670166 = 1.2046420574188232 + 0.1 * 6.931701183319092
Epoch 160, val loss: 1.3264198303222656
Epoch 170, training loss: 1.8255677223205566 = 1.1348013877868652 + 0.1 * 6.907663345336914
Epoch 170, val loss: 1.27641761302948
Epoch 180, training loss: 1.752901554107666 = 1.0638725757598877 + 0.1 * 6.890288829803467
Epoch 180, val loss: 1.226719617843628
Epoch 190, training loss: 1.6807053089141846 = 0.9926338195800781 + 0.1 * 6.8807148933410645
Epoch 190, val loss: 1.1766470670700073
Epoch 200, training loss: 1.6092778444290161 = 0.9227015376091003 + 0.1 * 6.865762710571289
Epoch 200, val loss: 1.1279866695404053
Epoch 210, training loss: 1.541100263595581 = 0.8556389212608337 + 0.1 * 6.854613780975342
Epoch 210, val loss: 1.08206045627594
Epoch 220, training loss: 1.4773590564727783 = 0.7927259802818298 + 0.1 * 6.846330642700195
Epoch 220, val loss: 1.040412187576294
Epoch 230, training loss: 1.4185373783111572 = 0.7349913716316223 + 0.1 * 6.835460662841797
Epoch 230, val loss: 1.0042661428451538
Epoch 240, training loss: 1.3645339012145996 = 0.6823073625564575 + 0.1 * 6.822264671325684
Epoch 240, val loss: 0.9743637442588806
Epoch 250, training loss: 1.3178396224975586 = 0.6339683532714844 + 0.1 * 6.838712692260742
Epoch 250, val loss: 0.9505279660224915
Epoch 260, training loss: 1.271492838859558 = 0.590087890625 + 0.1 * 6.814049243927002
Epoch 260, val loss: 0.9330093860626221
Epoch 270, training loss: 1.2294607162475586 = 0.5495578050613403 + 0.1 * 6.79902982711792
Epoch 270, val loss: 0.9205143451690674
Epoch 280, training loss: 1.191805362701416 = 0.5118122696876526 + 0.1 * 6.799930572509766
Epoch 280, val loss: 0.9125238656997681
Epoch 290, training loss: 1.155092477798462 = 0.47676992416381836 + 0.1 * 6.7832255363464355
Epoch 290, val loss: 0.9087013602256775
Epoch 300, training loss: 1.1216411590576172 = 0.44409021735191345 + 0.1 * 6.775509357452393
Epoch 300, val loss: 0.9087467193603516
Epoch 310, training loss: 1.0908881425857544 = 0.4136374890804291 + 0.1 * 6.772506237030029
Epoch 310, val loss: 0.9123549461364746
Epoch 320, training loss: 1.062811255455017 = 0.38548845052719116 + 0.1 * 6.773228168487549
Epoch 320, val loss: 0.9193243384361267
Epoch 330, training loss: 1.0354264974594116 = 0.359190434217453 + 0.1 * 6.7623610496521
Epoch 330, val loss: 0.9291330575942993
Epoch 340, training loss: 1.011501431465149 = 0.33446449041366577 + 0.1 * 6.770369052886963
Epoch 340, val loss: 0.9412629008293152
Epoch 350, training loss: 0.9871931672096252 = 0.3112998604774475 + 0.1 * 6.758933067321777
Epoch 350, val loss: 0.9553304314613342
Epoch 360, training loss: 0.9644677042961121 = 0.28947925567626953 + 0.1 * 6.749884605407715
Epoch 360, val loss: 0.9709709882736206
Epoch 370, training loss: 0.9426206946372986 = 0.2687516212463379 + 0.1 * 6.7386908531188965
Epoch 370, val loss: 0.9880852699279785
Epoch 380, training loss: 0.9236921668052673 = 0.24899667501449585 + 0.1 * 6.746954917907715
Epoch 380, val loss: 1.0065994262695312
Epoch 390, training loss: 0.904679536819458 = 0.23032160103321075 + 0.1 * 6.743579387664795
Epoch 390, val loss: 1.0263442993164062
Epoch 400, training loss: 0.885301411151886 = 0.21280640363693237 + 0.1 * 6.724949836730957
Epoch 400, val loss: 1.0471893548965454
Epoch 410, training loss: 0.8684216141700745 = 0.19641482830047607 + 0.1 * 6.720067501068115
Epoch 410, val loss: 1.0693391561508179
Epoch 420, training loss: 0.8540835976600647 = 0.18116600811481476 + 0.1 * 6.729176044464111
Epoch 420, val loss: 1.0927501916885376
Epoch 430, training loss: 0.8388787508010864 = 0.1671183705329895 + 0.1 * 6.71760368347168
Epoch 430, val loss: 1.1172423362731934
Epoch 440, training loss: 0.8257058262825012 = 0.154166117310524 + 0.1 * 6.715397357940674
Epoch 440, val loss: 1.1426851749420166
Epoch 450, training loss: 0.81257164478302 = 0.14228259027004242 + 0.1 * 6.702890396118164
Epoch 450, val loss: 1.1688276529312134
Epoch 460, training loss: 0.8006069660186768 = 0.13136465847492218 + 0.1 * 6.692422866821289
Epoch 460, val loss: 1.1956764459609985
Epoch 470, training loss: 0.7907416820526123 = 0.12131944298744202 + 0.1 * 6.6942219734191895
Epoch 470, val loss: 1.223109483718872
Epoch 480, training loss: 0.7806387543678284 = 0.11209718883037567 + 0.1 * 6.685415267944336
Epoch 480, val loss: 1.2508355379104614
Epoch 490, training loss: 0.771725058555603 = 0.10364100337028503 + 0.1 * 6.680840015411377
Epoch 490, val loss: 1.2788623571395874
Epoch 500, training loss: 0.7650548219680786 = 0.09587445855140686 + 0.1 * 6.691803455352783
Epoch 500, val loss: 1.3070881366729736
Epoch 510, training loss: 0.7560244798660278 = 0.08875954896211624 + 0.1 * 6.672649383544922
Epoch 510, val loss: 1.3352901935577393
Epoch 520, training loss: 0.7493985891342163 = 0.08224794268608093 + 0.1 * 6.671506881713867
Epoch 520, val loss: 1.3633782863616943
Epoch 530, training loss: 0.7421039342880249 = 0.07628200203180313 + 0.1 * 6.658218860626221
Epoch 530, val loss: 1.39134681224823
Epoch 540, training loss: 0.7361483573913574 = 0.07081503421068192 + 0.1 * 6.6533331871032715
Epoch 540, val loss: 1.4192636013031006
Epoch 550, training loss: 0.7336886525154114 = 0.06579487770795822 + 0.1 * 6.6789374351501465
Epoch 550, val loss: 1.4468709230422974
Epoch 560, training loss: 0.7260521650314331 = 0.06121216341853142 + 0.1 * 6.648399829864502
Epoch 560, val loss: 1.4739246368408203
Epoch 570, training loss: 0.722042441368103 = 0.05702291429042816 + 0.1 * 6.650195121765137
Epoch 570, val loss: 1.5003935098648071
Epoch 580, training loss: 0.7168443202972412 = 0.05318790674209595 + 0.1 * 6.636564254760742
Epoch 580, val loss: 1.5264532566070557
Epoch 590, training loss: 0.7129150032997131 = 0.04967351257801056 + 0.1 * 6.632414817810059
Epoch 590, val loss: 1.5519580841064453
Epoch 600, training loss: 0.7097294926643372 = 0.046449076384305954 + 0.1 * 6.632803916931152
Epoch 600, val loss: 1.5768609046936035
Epoch 610, training loss: 0.7059294581413269 = 0.04349304363131523 + 0.1 * 6.624363899230957
Epoch 610, val loss: 1.6011418104171753
Epoch 620, training loss: 0.7035155296325684 = 0.040778227150440216 + 0.1 * 6.627372741699219
Epoch 620, val loss: 1.6248409748077393
Epoch 630, training loss: 0.6997489929199219 = 0.03827868402004242 + 0.1 * 6.6147027015686035
Epoch 630, val loss: 1.6479228734970093
Epoch 640, training loss: 0.6973228454589844 = 0.03598044067621231 + 0.1 * 6.613423824310303
Epoch 640, val loss: 1.6704264879226685
Epoch 650, training loss: 0.6952382326126099 = 0.03386080637574196 + 0.1 * 6.613774299621582
Epoch 650, val loss: 1.6924009323120117
Epoch 660, training loss: 0.6951476335525513 = 0.031904187053442 + 0.1 * 6.632434368133545
Epoch 660, val loss: 1.7136400938034058
Epoch 670, training loss: 0.6901000738143921 = 0.030104156583547592 + 0.1 * 6.599958896636963
Epoch 670, val loss: 1.7343755960464478
Epoch 680, training loss: 0.6873924732208252 = 0.028438547626137733 + 0.1 * 6.589539051055908
Epoch 680, val loss: 1.754460096359253
Epoch 690, training loss: 0.6869268417358398 = 0.026896953582763672 + 0.1 * 6.600298881530762
Epoch 690, val loss: 1.774118185043335
Epoch 700, training loss: 0.6844435930252075 = 0.025469325482845306 + 0.1 * 6.589742660522461
Epoch 700, val loss: 1.7932206392288208
Epoch 710, training loss: 0.6846314072608948 = 0.024148525670170784 + 0.1 * 6.604828834533691
Epoch 710, val loss: 1.811719298362732
Epoch 720, training loss: 0.6810488104820251 = 0.02292216569185257 + 0.1 * 6.581266403198242
Epoch 720, val loss: 1.8298612833023071
Epoch 730, training loss: 0.680134654045105 = 0.021783847361803055 + 0.1 * 6.583508014678955
Epoch 730, val loss: 1.8473674058914185
Epoch 740, training loss: 0.6780679225921631 = 0.02072630450129509 + 0.1 * 6.573415756225586
Epoch 740, val loss: 1.864561915397644
Epoch 750, training loss: 0.6768820285797119 = 0.01974215917289257 + 0.1 * 6.571398735046387
Epoch 750, val loss: 1.8813285827636719
Epoch 760, training loss: 0.6772530674934387 = 0.018824126571416855 + 0.1 * 6.58428955078125
Epoch 760, val loss: 1.897541880607605
Epoch 770, training loss: 0.6744498014450073 = 0.01796901784837246 + 0.1 * 6.564807891845703
Epoch 770, val loss: 1.9134804010391235
Epoch 780, training loss: 0.6729229688644409 = 0.01717117615044117 + 0.1 * 6.5575175285339355
Epoch 780, val loss: 1.9289076328277588
Epoch 790, training loss: 0.6736122965812683 = 0.016425687819719315 + 0.1 * 6.571866035461426
Epoch 790, val loss: 1.9438577890396118
Epoch 800, training loss: 0.6719732880592346 = 0.015727054327726364 + 0.1 * 6.562462329864502
Epoch 800, val loss: 1.9586063623428345
Epoch 810, training loss: 0.6708492636680603 = 0.015074116177856922 + 0.1 * 6.557751178741455
Epoch 810, val loss: 1.9728535413742065
Epoch 820, training loss: 0.6690806150436401 = 0.014460541307926178 + 0.1 * 6.546200275421143
Epoch 820, val loss: 1.9867949485778809
Epoch 830, training loss: 0.6688884496688843 = 0.013885333202779293 + 0.1 * 6.5500311851501465
Epoch 830, val loss: 2.00038480758667
Epoch 840, training loss: 0.6692057847976685 = 0.013345208019018173 + 0.1 * 6.558605670928955
Epoch 840, val loss: 2.0136239528656006
Epoch 850, training loss: 0.6677445769309998 = 0.012836862355470657 + 0.1 * 6.549077033996582
Epoch 850, val loss: 2.026632308959961
Epoch 860, training loss: 0.667625904083252 = 0.012358002364635468 + 0.1 * 6.55267858505249
Epoch 860, val loss: 2.039076805114746
Epoch 870, training loss: 0.6653197407722473 = 0.011907952837646008 + 0.1 * 6.534117698669434
Epoch 870, val loss: 2.051464557647705
Epoch 880, training loss: 0.6659629344940186 = 0.011483129113912582 + 0.1 * 6.544797897338867
Epoch 880, val loss: 2.0633866786956787
Epoch 890, training loss: 0.664841890335083 = 0.01108179148286581 + 0.1 * 6.537600517272949
Epoch 890, val loss: 2.075058937072754
Epoch 900, training loss: 0.6640914082527161 = 0.010702752508223057 + 0.1 * 6.533886909484863
Epoch 900, val loss: 2.0864009857177734
Epoch 910, training loss: 0.6634503602981567 = 0.010343486443161964 + 0.1 * 6.531068325042725
Epoch 910, val loss: 2.097543239593506
Epoch 920, training loss: 0.6629633903503418 = 0.010004105977714062 + 0.1 * 6.529592990875244
Epoch 920, val loss: 2.1083626747131348
Epoch 930, training loss: 0.66322922706604 = 0.009682592935860157 + 0.1 * 6.535466194152832
Epoch 930, val loss: 2.1189656257629395
Epoch 940, training loss: 0.6612148880958557 = 0.009376433677971363 + 0.1 * 6.5183844566345215
Epoch 940, val loss: 2.129377841949463
Epoch 950, training loss: 0.6615613102912903 = 0.009086809121072292 + 0.1 * 6.524744987487793
Epoch 950, val loss: 2.13956356048584
Epoch 960, training loss: 0.6617863774299622 = 0.008810954168438911 + 0.1 * 6.529754161834717
Epoch 960, val loss: 2.149477958679199
Epoch 970, training loss: 0.6600109338760376 = 0.008548717945814133 + 0.1 * 6.514621734619141
Epoch 970, val loss: 2.1593055725097656
Epoch 980, training loss: 0.6596729755401611 = 0.008299313485622406 + 0.1 * 6.513736724853516
Epoch 980, val loss: 2.1687886714935303
Epoch 990, training loss: 0.6587214469909668 = 0.008061191998422146 + 0.1 * 6.506602764129639
Epoch 990, val loss: 2.1782374382019043
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8091723774380601
The final CL Acc:0.70494, 0.00630, The final GNN Acc:0.81146, 0.00221
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13142])
remove edge: torch.Size([2, 7950])
updated graph: torch.Size([2, 10536])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8126933574676514 = 1.9530059099197388 + 0.1 * 8.596874237060547
Epoch 0, val loss: 1.9532328844070435
Epoch 10, training loss: 2.8026328086853027 = 1.9429500102996826 + 0.1 * 8.596827507019043
Epoch 10, val loss: 1.9433354139328003
Epoch 20, training loss: 2.7901740074157715 = 1.9305146932601929 + 0.1 * 8.59659194946289
Epoch 20, val loss: 1.9308395385742188
Epoch 30, training loss: 2.772409677505493 = 1.9129226207733154 + 0.1 * 8.594870567321777
Epoch 30, val loss: 1.9127542972564697
Epoch 40, training loss: 2.7445764541625977 = 1.8864402770996094 + 0.1 * 8.5813627243042
Epoch 40, val loss: 1.88572096824646
Epoch 50, training loss: 2.7004332542419434 = 1.8484773635864258 + 0.1 * 8.519559860229492
Epoch 50, val loss: 1.8487534523010254
Epoch 60, training loss: 2.6349477767944336 = 1.804810643196106 + 0.1 * 8.301370620727539
Epoch 60, val loss: 1.809971809387207
Epoch 70, training loss: 2.5771384239196777 = 1.765967607498169 + 0.1 * 8.11170768737793
Epoch 70, val loss: 1.776581883430481
Epoch 80, training loss: 2.4953770637512207 = 1.7231295108795166 + 0.1 * 7.722474575042725
Epoch 80, val loss: 1.7361071109771729
Epoch 90, training loss: 2.411301374435425 = 1.6682273149490356 + 0.1 * 7.430739879608154
Epoch 90, val loss: 1.6858007907867432
Epoch 100, training loss: 2.330821990966797 = 1.59614896774292 + 0.1 * 7.346728801727295
Epoch 100, val loss: 1.6221423149108887
Epoch 110, training loss: 2.2361409664154053 = 1.5081251859664917 + 0.1 * 7.280158042907715
Epoch 110, val loss: 1.545217752456665
Epoch 120, training loss: 2.1347439289093018 = 1.413724422454834 + 0.1 * 7.2101945877075195
Epoch 120, val loss: 1.4662638902664185
Epoch 130, training loss: 2.0320992469787598 = 1.3179752826690674 + 0.1 * 7.141239166259766
Epoch 130, val loss: 1.3878401517868042
Epoch 140, training loss: 1.929959774017334 = 1.221094012260437 + 0.1 * 7.088657855987549
Epoch 140, val loss: 1.3095239400863647
Epoch 150, training loss: 1.8275113105773926 = 1.1225413084030151 + 0.1 * 7.049699306488037
Epoch 150, val loss: 1.230964183807373
Epoch 160, training loss: 1.7280471324920654 = 1.0256468057632446 + 0.1 * 7.024003505706787
Epoch 160, val loss: 1.1551086902618408
Epoch 170, training loss: 1.6338841915130615 = 0.9333838224411011 + 0.1 * 7.005002975463867
Epoch 170, val loss: 1.0837589502334595
Epoch 180, training loss: 1.5463485717773438 = 0.8476054668426514 + 0.1 * 6.987430095672607
Epoch 180, val loss: 1.0179543495178223
Epoch 190, training loss: 1.4680614471435547 = 0.7707208395004272 + 0.1 * 6.973405361175537
Epoch 190, val loss: 0.9604987502098083
Epoch 200, training loss: 1.3999903202056885 = 0.7036817669868469 + 0.1 * 6.963085174560547
Epoch 200, val loss: 0.9129889607429504
Epoch 210, training loss: 1.3403675556182861 = 0.645647406578064 + 0.1 * 6.947200775146484
Epoch 210, val loss: 0.8754213452339172
Epoch 220, training loss: 1.2876949310302734 = 0.5948597192764282 + 0.1 * 6.928352355957031
Epoch 220, val loss: 0.8468156456947327
Epoch 230, training loss: 1.2410690784454346 = 0.5496315956115723 + 0.1 * 6.914375305175781
Epoch 230, val loss: 0.825501024723053
Epoch 240, training loss: 1.1989902257919312 = 0.5090071558952332 + 0.1 * 6.8998308181762695
Epoch 240, val loss: 0.8098230361938477
Epoch 250, training loss: 1.1604706048965454 = 0.47170424461364746 + 0.1 * 6.8876633644104
Epoch 250, val loss: 0.7983970642089844
Epoch 260, training loss: 1.1245192289352417 = 0.43753311038017273 + 0.1 * 6.869861602783203
Epoch 260, val loss: 0.7906253337860107
Epoch 270, training loss: 1.0921382904052734 = 0.40615272521972656 + 0.1 * 6.859854698181152
Epoch 270, val loss: 0.786194920539856
Epoch 280, training loss: 1.0627516508102417 = 0.3772391080856323 + 0.1 * 6.855125427246094
Epoch 280, val loss: 0.7848098278045654
Epoch 290, training loss: 1.0348901748657227 = 0.3509090840816498 + 0.1 * 6.839811325073242
Epoch 290, val loss: 0.786183774471283
Epoch 300, training loss: 1.009422779083252 = 0.32682615518569946 + 0.1 * 6.825966835021973
Epoch 300, val loss: 0.7897728085517883
Epoch 310, training loss: 0.9861471056938171 = 0.3046293258666992 + 0.1 * 6.815177917480469
Epoch 310, val loss: 0.795174241065979
Epoch 320, training loss: 0.9670392274856567 = 0.2841862738132477 + 0.1 * 6.828528881072998
Epoch 320, val loss: 0.8021801114082336
Epoch 330, training loss: 0.9451634287834167 = 0.2655162811279297 + 0.1 * 6.796471118927002
Epoch 330, val loss: 0.8104267716407776
Epoch 340, training loss: 0.9268510341644287 = 0.24805256724357605 + 0.1 * 6.787984848022461
Epoch 340, val loss: 0.8195271492004395
Epoch 350, training loss: 0.9095860719680786 = 0.23139432072639465 + 0.1 * 6.781917095184326
Epoch 350, val loss: 0.8294790983200073
Epoch 360, training loss: 0.8928585648536682 = 0.2152712345123291 + 0.1 * 6.775873184204102
Epoch 360, val loss: 0.840093731880188
Epoch 370, training loss: 0.8760929107666016 = 0.19950100779533386 + 0.1 * 6.765919208526611
Epoch 370, val loss: 0.8512900471687317
Epoch 380, training loss: 0.8620162606239319 = 0.18401466310024261 + 0.1 * 6.78001594543457
Epoch 380, val loss: 0.8630052208900452
Epoch 390, training loss: 0.8443771004676819 = 0.16901268064975739 + 0.1 * 6.7536444664001465
Epoch 390, val loss: 0.8751752376556396
Epoch 400, training loss: 0.8299571871757507 = 0.15471601486206055 + 0.1 * 6.752411842346191
Epoch 400, val loss: 0.8882895112037659
Epoch 410, training loss: 0.8164855241775513 = 0.14139795303344727 + 0.1 * 6.750875473022461
Epoch 410, val loss: 0.90250164270401
Epoch 420, training loss: 0.8044788241386414 = 0.12927371263504028 + 0.1 * 6.752050876617432
Epoch 420, val loss: 0.9175284504890442
Epoch 430, training loss: 0.7923576235771179 = 0.11833884567022324 + 0.1 * 6.740188121795654
Epoch 430, val loss: 0.9332535862922668
Epoch 440, training loss: 0.7842632532119751 = 0.10848113149404526 + 0.1 * 6.757821559906006
Epoch 440, val loss: 0.9496246576309204
Epoch 450, training loss: 0.7726711630821228 = 0.09964718669652939 + 0.1 * 6.7302398681640625
Epoch 450, val loss: 0.9664181470870972
Epoch 460, training loss: 0.7649770379066467 = 0.09168422967195511 + 0.1 * 6.732928276062012
Epoch 460, val loss: 0.9836440682411194
Epoch 470, training loss: 0.7568276524543762 = 0.08451485633850098 + 0.1 * 6.723127841949463
Epoch 470, val loss: 1.0010014772415161
Epoch 480, training loss: 0.7493730783462524 = 0.07803021371364594 + 0.1 * 6.713428497314453
Epoch 480, val loss: 1.0185176134109497
Epoch 490, training loss: 0.7443416118621826 = 0.0721500962972641 + 0.1 * 6.721914768218994
Epoch 490, val loss: 1.0359917879104614
Epoch 500, training loss: 0.7386526465415955 = 0.06683770567178726 + 0.1 * 6.718149185180664
Epoch 500, val loss: 1.0531015396118164
Epoch 510, training loss: 0.7322043776512146 = 0.0620122067630291 + 0.1 * 6.7019219398498535
Epoch 510, val loss: 1.0702059268951416
Epoch 520, training loss: 0.7280910611152649 = 0.05762510746717453 + 0.1 * 6.704659461975098
Epoch 520, val loss: 1.0869654417037964
Epoch 530, training loss: 0.7231318950653076 = 0.053638532757759094 + 0.1 * 6.694933891296387
Epoch 530, val loss: 1.1033003330230713
Epoch 540, training loss: 0.7208788394927979 = 0.050014678388834 + 0.1 * 6.708641529083252
Epoch 540, val loss: 1.119431495666504
Epoch 550, training loss: 0.7156767249107361 = 0.04672176018357277 + 0.1 * 6.689549446105957
Epoch 550, val loss: 1.1351017951965332
Epoch 560, training loss: 0.7119288444519043 = 0.04371126741170883 + 0.1 * 6.682175636291504
Epoch 560, val loss: 1.1502639055252075
Epoch 570, training loss: 0.7094112038612366 = 0.040955640375614166 + 0.1 * 6.684555530548096
Epoch 570, val loss: 1.1652711629867554
Epoch 580, training loss: 0.7060756087303162 = 0.03843863680958748 + 0.1 * 6.676369667053223
Epoch 580, val loss: 1.1797122955322266
Epoch 590, training loss: 0.7039455771446228 = 0.03613044321537018 + 0.1 * 6.678151607513428
Epoch 590, val loss: 1.1938927173614502
Epoch 600, training loss: 0.7000734806060791 = 0.03401220589876175 + 0.1 * 6.6606125831604
Epoch 600, val loss: 1.2076351642608643
Epoch 610, training loss: 0.6999677419662476 = 0.032063111662864685 + 0.1 * 6.679046154022217
Epoch 610, val loss: 1.2212145328521729
Epoch 620, training loss: 0.6974177360534668 = 0.03027697652578354 + 0.1 * 6.671407699584961
Epoch 620, val loss: 1.2341179847717285
Epoch 630, training loss: 0.6957441568374634 = 0.028626561164855957 + 0.1 * 6.671175956726074
Epoch 630, val loss: 1.2469260692596436
Epoch 640, training loss: 0.6935694813728333 = 0.027109043672680855 + 0.1 * 6.664604187011719
Epoch 640, val loss: 1.2592805624008179
Epoch 650, training loss: 0.6905325055122375 = 0.02570474147796631 + 0.1 * 6.648277759552002
Epoch 650, val loss: 1.2712510824203491
Epoch 660, training loss: 0.6886448264122009 = 0.024403082206845284 + 0.1 * 6.642416954040527
Epoch 660, val loss: 1.282897710800171
Epoch 670, training loss: 0.686741292476654 = 0.023196909576654434 + 0.1 * 6.635443687438965
Epoch 670, val loss: 1.2943506240844727
Epoch 680, training loss: 0.6864035725593567 = 0.022077806293964386 + 0.1 * 6.6432576179504395
Epoch 680, val loss: 1.3053489923477173
Epoch 690, training loss: 0.6839144229888916 = 0.021038131788372993 + 0.1 * 6.628763198852539
Epoch 690, val loss: 1.3161821365356445
Epoch 700, training loss: 0.6825121641159058 = 0.020073862746357918 + 0.1 * 6.624382495880127
Epoch 700, val loss: 1.3265987634658813
Epoch 710, training loss: 0.6813707947731018 = 0.01917274482548237 + 0.1 * 6.621980667114258
Epoch 710, val loss: 1.336816430091858
Epoch 720, training loss: 0.6823745965957642 = 0.01833086647093296 + 0.1 * 6.640437126159668
Epoch 720, val loss: 1.3467390537261963
Epoch 730, training loss: 0.6793307662010193 = 0.01754475012421608 + 0.1 * 6.617860317230225
Epoch 730, val loss: 1.3564095497131348
Epoch 740, training loss: 0.6788377165794373 = 0.016808370128273964 + 0.1 * 6.620293140411377
Epoch 740, val loss: 1.3658578395843506
Epoch 750, training loss: 0.6781735420227051 = 0.016119306907057762 + 0.1 * 6.620542526245117
Epoch 750, val loss: 1.3750723600387573
Epoch 760, training loss: 0.6764195561408997 = 0.015473699197173119 + 0.1 * 6.6094584465026855
Epoch 760, val loss: 1.384061336517334
Epoch 770, training loss: 0.6740585565567017 = 0.014866187237203121 + 0.1 * 6.591923713684082
Epoch 770, val loss: 1.3927571773529053
Epoch 780, training loss: 0.6766661405563354 = 0.014293486252427101 + 0.1 * 6.6237263679504395
Epoch 780, val loss: 1.4014053344726562
Epoch 790, training loss: 0.6730694770812988 = 0.013758743181824684 + 0.1 * 6.593107223510742
Epoch 790, val loss: 1.409543752670288
Epoch 800, training loss: 0.6711795330047607 = 0.013252807781100273 + 0.1 * 6.579267501831055
Epoch 800, val loss: 1.4176814556121826
Epoch 810, training loss: 0.6705586910247803 = 0.01277550496160984 + 0.1 * 6.577832221984863
Epoch 810, val loss: 1.4255708456039429
Epoch 820, training loss: 0.6708890199661255 = 0.012324406765401363 + 0.1 * 6.585646152496338
Epoch 820, val loss: 1.4333165884017944
Epoch 830, training loss: 0.6702227592468262 = 0.011898319236934185 + 0.1 * 6.583244323730469
Epoch 830, val loss: 1.440879225730896
Epoch 840, training loss: 0.668365478515625 = 0.011495486833155155 + 0.1 * 6.568699836730957
Epoch 840, val loss: 1.4483243227005005
Epoch 850, training loss: 0.6709996461868286 = 0.011113978922367096 + 0.1 * 6.598856449127197
Epoch 850, val loss: 1.4556156396865845
Epoch 860, training loss: 0.6679840087890625 = 0.010753272101283073 + 0.1 * 6.572307109832764
Epoch 860, val loss: 1.4626412391662598
Epoch 870, training loss: 0.6663421988487244 = 0.01040991023182869 + 0.1 * 6.559322834014893
Epoch 870, val loss: 1.4696341753005981
Epoch 880, training loss: 0.6668736338615417 = 0.010084165260195732 + 0.1 * 6.56789493560791
Epoch 880, val loss: 1.4764286279678345
Epoch 890, training loss: 0.6654917597770691 = 0.009775854647159576 + 0.1 * 6.557158470153809
Epoch 890, val loss: 1.4829970598220825
Epoch 900, training loss: 0.6646056175231934 = 0.009481430053710938 + 0.1 * 6.551241874694824
Epoch 900, val loss: 1.4895180463790894
Epoch 910, training loss: 0.6663739681243896 = 0.009201345965266228 + 0.1 * 6.571725845336914
Epoch 910, val loss: 1.495858073234558
Epoch 920, training loss: 0.6643957495689392 = 0.008934393525123596 + 0.1 * 6.5546135902404785
Epoch 920, val loss: 1.502118706703186
Epoch 930, training loss: 0.6668131351470947 = 0.008679722435772419 + 0.1 * 6.581333637237549
Epoch 930, val loss: 1.5082809925079346
Epoch 940, training loss: 0.6634694933891296 = 0.008438107557594776 + 0.1 * 6.550313949584961
Epoch 940, val loss: 1.5141891241073608
Epoch 950, training loss: 0.6631495952606201 = 0.008206653408706188 + 0.1 * 6.549428939819336
Epoch 950, val loss: 1.5200830698013306
Epoch 960, training loss: 0.6611770987510681 = 0.007985579781234264 + 0.1 * 6.531914710998535
Epoch 960, val loss: 1.5259078741073608
Epoch 970, training loss: 0.6624295115470886 = 0.007774048019200563 + 0.1 * 6.5465545654296875
Epoch 970, val loss: 1.5316094160079956
Epoch 980, training loss: 0.6628105640411377 = 0.0075714546255767345 + 0.1 * 6.552391052246094
Epoch 980, val loss: 1.537255883216858
Epoch 990, training loss: 0.6612356901168823 = 0.00737796863541007 + 0.1 * 6.538577079772949
Epoch 990, val loss: 1.5427008867263794
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.799980401992798 = 1.9402954578399658 + 0.1 * 8.596848487854004
Epoch 0, val loss: 1.9382904767990112
Epoch 10, training loss: 2.7905385494232178 = 1.9308592081069946 + 0.1 * 8.596794128417969
Epoch 10, val loss: 1.9287240505218506
Epoch 20, training loss: 2.779240846633911 = 1.9195917844772339 + 0.1 * 8.596489906311035
Epoch 20, val loss: 1.917314052581787
Epoch 30, training loss: 2.763504981994629 = 1.9040958881378174 + 0.1 * 8.594091415405273
Epoch 30, val loss: 1.9017232656478882
Epoch 40, training loss: 2.7389934062957764 = 1.8814542293548584 + 0.1 * 8.57539176940918
Epoch 40, val loss: 1.8790745735168457
Epoch 50, training loss: 2.6974105834960938 = 1.8495478630065918 + 0.1 * 8.478628158569336
Epoch 50, val loss: 1.8480048179626465
Epoch 60, training loss: 2.636876106262207 = 1.8116469383239746 + 0.1 * 8.252290725708008
Epoch 60, val loss: 1.813217282295227
Epoch 70, training loss: 2.5756304264068604 = 1.7750637531280518 + 0.1 * 8.005666732788086
Epoch 70, val loss: 1.780300498008728
Epoch 80, training loss: 2.4937236309051514 = 1.737837553024292 + 0.1 * 7.558859825134277
Epoch 80, val loss: 1.7454215288162231
Epoch 90, training loss: 2.4221737384796143 = 1.6924136877059937 + 0.1 * 7.297600269317627
Epoch 90, val loss: 1.704016923904419
Epoch 100, training loss: 2.347731113433838 = 1.6320236921310425 + 0.1 * 7.157074928283691
Epoch 100, val loss: 1.6513357162475586
Epoch 110, training loss: 2.264331579208374 = 1.5555927753448486 + 0.1 * 7.0873870849609375
Epoch 110, val loss: 1.5843316316604614
Epoch 120, training loss: 2.172107696533203 = 1.4677183628082275 + 0.1 * 7.043893814086914
Epoch 120, val loss: 1.507448434829712
Epoch 130, training loss: 2.075066089630127 = 1.3745898008346558 + 0.1 * 7.004762172698975
Epoch 130, val loss: 1.4290462732315063
Epoch 140, training loss: 1.9747819900512695 = 1.2775328159332275 + 0.1 * 6.97249174118042
Epoch 140, val loss: 1.3482023477554321
Epoch 150, training loss: 1.8712983131408691 = 1.1762681007385254 + 0.1 * 6.9503021240234375
Epoch 150, val loss: 1.2640866041183472
Epoch 160, training loss: 1.7690911293029785 = 1.076119303703308 + 0.1 * 6.929718494415283
Epoch 160, val loss: 1.1819162368774414
Epoch 170, training loss: 1.6703360080718994 = 0.9792481660842896 + 0.1 * 6.9108781814575195
Epoch 170, val loss: 1.1030793190002441
Epoch 180, training loss: 1.5785537958145142 = 0.888410747051239 + 0.1 * 6.901430606842041
Epoch 180, val loss: 1.0308713912963867
Epoch 190, training loss: 1.493459701538086 = 0.8046936988830566 + 0.1 * 6.887659072875977
Epoch 190, val loss: 0.9658976197242737
Epoch 200, training loss: 1.415298342704773 = 0.7277209162712097 + 0.1 * 6.875773906707764
Epoch 200, val loss: 0.9074997901916504
Epoch 210, training loss: 1.345885157585144 = 0.6583756804466248 + 0.1 * 6.875094890594482
Epoch 210, val loss: 0.85670405626297
Epoch 220, training loss: 1.2843940258026123 = 0.5985959768295288 + 0.1 * 6.857981204986572
Epoch 220, val loss: 0.8150368928909302
Epoch 230, training loss: 1.2309255599975586 = 0.5464953780174255 + 0.1 * 6.844302177429199
Epoch 230, val loss: 0.7811633348464966
Epoch 240, training loss: 1.1832504272460938 = 0.5000389218330383 + 0.1 * 6.8321146965026855
Epoch 240, val loss: 0.7535417675971985
Epoch 250, training loss: 1.1411223411560059 = 0.45753195881843567 + 0.1 * 6.835904121398926
Epoch 250, val loss: 0.730686366558075
Epoch 260, training loss: 1.0999891757965088 = 0.41804608702659607 + 0.1 * 6.819430828094482
Epoch 260, val loss: 0.7111935615539551
Epoch 270, training loss: 1.0607149600982666 = 0.38019728660583496 + 0.1 * 6.805177211761475
Epoch 270, val loss: 0.693585216999054
Epoch 280, training loss: 1.0231131315231323 = 0.34330514073371887 + 0.1 * 6.798079490661621
Epoch 280, val loss: 0.6771565079689026
Epoch 290, training loss: 0.9891929626464844 = 0.3076622486114502 + 0.1 * 6.815307140350342
Epoch 290, val loss: 0.6618984341621399
Epoch 300, training loss: 0.9535777568817139 = 0.2742668688297272 + 0.1 * 6.7931084632873535
Epoch 300, val loss: 0.648400604724884
Epoch 310, training loss: 0.9217674732208252 = 0.24340055882930756 + 0.1 * 6.7836689949035645
Epoch 310, val loss: 0.6368603110313416
Epoch 320, training loss: 0.893081784248352 = 0.21544261276721954 + 0.1 * 6.776391506195068
Epoch 320, val loss: 0.627752423286438
Epoch 330, training loss: 0.8684234619140625 = 0.19071346521377563 + 0.1 * 6.777099609375
Epoch 330, val loss: 0.6210862994194031
Epoch 340, training loss: 0.84602952003479 = 0.16908156871795654 + 0.1 * 6.769479274749756
Epoch 340, val loss: 0.616692304611206
Epoch 350, training loss: 0.8260529637336731 = 0.1502024531364441 + 0.1 * 6.758504867553711
Epoch 350, val loss: 0.6144294142723083
Epoch 360, training loss: 0.8114558458328247 = 0.1337086707353592 + 0.1 * 6.777472019195557
Epoch 360, val loss: 0.6140269041061401
Epoch 370, training loss: 0.7946146726608276 = 0.1193825826048851 + 0.1 * 6.752320766448975
Epoch 370, val loss: 0.6150527596473694
Epoch 380, training loss: 0.7812646627426147 = 0.10686169564723969 + 0.1 * 6.744029521942139
Epoch 380, val loss: 0.6174362897872925
Epoch 390, training loss: 0.7706191539764404 = 0.09593561291694641 + 0.1 * 6.746835708618164
Epoch 390, val loss: 0.6209301948547363
Epoch 400, training loss: 0.7597829103469849 = 0.08640455454587936 + 0.1 * 6.73378324508667
Epoch 400, val loss: 0.6252724528312683
Epoch 410, training loss: 0.7509070634841919 = 0.0780671164393425 + 0.1 * 6.728399276733398
Epoch 410, val loss: 0.6304978728294373
Epoch 420, training loss: 0.7427674531936646 = 0.0707806721329689 + 0.1 * 6.71986722946167
Epoch 420, val loss: 0.6361900568008423
Epoch 430, training loss: 0.7392529249191284 = 0.06437084078788757 + 0.1 * 6.748821258544922
Epoch 430, val loss: 0.6424741744995117
Epoch 440, training loss: 0.7298218011856079 = 0.05875474214553833 + 0.1 * 6.710670471191406
Epoch 440, val loss: 0.6489660143852234
Epoch 450, training loss: 0.7237998247146606 = 0.053791459649801254 + 0.1 * 6.700083255767822
Epoch 450, val loss: 0.6556572914123535
Epoch 460, training loss: 0.7189247608184814 = 0.04937591403722763 + 0.1 * 6.695488452911377
Epoch 460, val loss: 0.6625617146492004
Epoch 470, training loss: 0.7149161100387573 = 0.04544951021671295 + 0.1 * 6.694665908813477
Epoch 470, val loss: 0.6696369051933289
Epoch 480, training loss: 0.710651159286499 = 0.04196646809577942 + 0.1 * 6.68684720993042
Epoch 480, val loss: 0.676568865776062
Epoch 490, training loss: 0.706752359867096 = 0.03885255753993988 + 0.1 * 6.678997993469238
Epoch 490, val loss: 0.6834611296653748
Epoch 500, training loss: 0.703078031539917 = 0.03605039045214653 + 0.1 * 6.670276641845703
Epoch 500, val loss: 0.6903657913208008
Epoch 510, training loss: 0.7059077024459839 = 0.033526867628097534 + 0.1 * 6.7238078117370605
Epoch 510, val loss: 0.6972532272338867
Epoch 520, training loss: 0.6981881260871887 = 0.03126681596040726 + 0.1 * 6.66921329498291
Epoch 520, val loss: 0.7039248943328857
Epoch 530, training loss: 0.6941559314727783 = 0.02922678180038929 + 0.1 * 6.649291515350342
Epoch 530, val loss: 0.7104015946388245
Epoch 540, training loss: 0.6921742558479309 = 0.027368318289518356 + 0.1 * 6.648059368133545
Epoch 540, val loss: 0.7168815732002258
Epoch 550, training loss: 0.6921579241752625 = 0.025675123557448387 + 0.1 * 6.664827823638916
Epoch 550, val loss: 0.7232778072357178
Epoch 560, training loss: 0.689005434513092 = 0.024141572415828705 + 0.1 * 6.648638725280762
Epoch 560, val loss: 0.7295553088188171
Epoch 570, training loss: 0.6857498288154602 = 0.022746192291378975 + 0.1 * 6.630036354064941
Epoch 570, val loss: 0.7355724573135376
Epoch 580, training loss: 0.6879666447639465 = 0.021465562283992767 + 0.1 * 6.665010929107666
Epoch 580, val loss: 0.7415586113929749
Epoch 590, training loss: 0.6828168034553528 = 0.020294135436415672 + 0.1 * 6.6252264976501465
Epoch 590, val loss: 0.7474076151847839
Epoch 600, training loss: 0.6815603971481323 = 0.019218359142541885 + 0.1 * 6.623420238494873
Epoch 600, val loss: 0.7531030774116516
Epoch 610, training loss: 0.6805688142776489 = 0.018226198852062225 + 0.1 * 6.6234259605407715
Epoch 610, val loss: 0.7587381601333618
Epoch 620, training loss: 0.6796676516532898 = 0.017312664538621902 + 0.1 * 6.623549938201904
Epoch 620, val loss: 0.7642347812652588
Epoch 630, training loss: 0.678743839263916 = 0.016470732167363167 + 0.1 * 6.622730731964111
Epoch 630, val loss: 0.76961749792099
Epoch 640, training loss: 0.6763843297958374 = 0.015692580491304398 + 0.1 * 6.606917381286621
Epoch 640, val loss: 0.7748191356658936
Epoch 650, training loss: 0.6772757172584534 = 0.014968739822506905 + 0.1 * 6.6230692863464355
Epoch 650, val loss: 0.7799803018569946
Epoch 660, training loss: 0.6750507950782776 = 0.014296692796051502 + 0.1 * 6.607540607452393
Epoch 660, val loss: 0.7850020527839661
Epoch 670, training loss: 0.6744535565376282 = 0.013673027977347374 + 0.1 * 6.607805252075195
Epoch 670, val loss: 0.7899200916290283
Epoch 680, training loss: 0.6728784441947937 = 0.013091486878693104 + 0.1 * 6.597869396209717
Epoch 680, val loss: 0.794714629650116
Epoch 690, training loss: 0.6709550023078918 = 0.01254936121404171 + 0.1 * 6.584056377410889
Epoch 690, val loss: 0.7993814945220947
Epoch 700, training loss: 0.6718823313713074 = 0.012041402980685234 + 0.1 * 6.598409175872803
Epoch 700, val loss: 0.8039776086807251
Epoch 710, training loss: 0.6700921654701233 = 0.011566315777599812 + 0.1 * 6.5852580070495605
Epoch 710, val loss: 0.8084729313850403
Epoch 720, training loss: 0.6707354187965393 = 0.011121390387415886 + 0.1 * 6.596139907836914
Epoch 720, val loss: 0.8128464818000793
Epoch 730, training loss: 0.6680133938789368 = 0.010703597217798233 + 0.1 * 6.5730977058410645
Epoch 730, val loss: 0.8171532154083252
Epoch 740, training loss: 0.6680511832237244 = 0.010310663841664791 + 0.1 * 6.577404975891113
Epoch 740, val loss: 0.8213911652565002
Epoch 750, training loss: 0.6680011749267578 = 0.00994273740798235 + 0.1 * 6.580584526062012
Epoch 750, val loss: 0.8255302906036377
Epoch 760, training loss: 0.666601300239563 = 0.009595563635230064 + 0.1 * 6.570056915283203
Epoch 760, val loss: 0.8295202851295471
Epoch 770, training loss: 0.6662374138832092 = 0.009267112240195274 + 0.1 * 6.569703102111816
Epoch 770, val loss: 0.8335039615631104
Epoch 780, training loss: 0.6654369831085205 = 0.008958659134805202 + 0.1 * 6.564783096313477
Epoch 780, val loss: 0.8374000191688538
Epoch 790, training loss: 0.6679982542991638 = 0.008666304871439934 + 0.1 * 6.593319892883301
Epoch 790, val loss: 0.8411843776702881
Epoch 800, training loss: 0.6639067530632019 = 0.0083890026435256 + 0.1 * 6.555177211761475
Epoch 800, val loss: 0.844903290271759
Epoch 810, training loss: 0.6655909419059753 = 0.008126975037157536 + 0.1 * 6.574639797210693
Epoch 810, val loss: 0.848587155342102
Epoch 820, training loss: 0.6639705300331116 = 0.007878354750573635 + 0.1 * 6.560921669006348
Epoch 820, val loss: 0.8521503210067749
Epoch 830, training loss: 0.6634390354156494 = 0.0076425522565841675 + 0.1 * 6.55796480178833
Epoch 830, val loss: 0.8556403517723083
Epoch 840, training loss: 0.6616132855415344 = 0.007418114226311445 + 0.1 * 6.541951656341553
Epoch 840, val loss: 0.8591017127037048
Epoch 850, training loss: 0.6620385646820068 = 0.007205298636108637 + 0.1 * 6.548332691192627
Epoch 850, val loss: 0.8624604940414429
Epoch 860, training loss: 0.6606500744819641 = 0.007002897094935179 + 0.1 * 6.536471843719482
Epoch 860, val loss: 0.8657833337783813
Epoch 870, training loss: 0.6616086363792419 = 0.006809974554926157 + 0.1 * 6.54798698425293
Epoch 870, val loss: 0.8689923882484436
Epoch 880, training loss: 0.6602259874343872 = 0.00662557128816843 + 0.1 * 6.536004066467285
Epoch 880, val loss: 0.872189462184906
Epoch 890, training loss: 0.6620953679084778 = 0.0064495583064854145 + 0.1 * 6.556457996368408
Epoch 890, val loss: 0.8752853870391846
Epoch 900, training loss: 0.659559965133667 = 0.006282076705247164 + 0.1 * 6.532778739929199
Epoch 900, val loss: 0.8784435391426086
Epoch 910, training loss: 0.6592432260513306 = 0.00612197769805789 + 0.1 * 6.531212329864502
Epoch 910, val loss: 0.8814014792442322
Epoch 920, training loss: 0.6577948927879333 = 0.005968669895082712 + 0.1 * 6.518261909484863
Epoch 920, val loss: 0.8843842148780823
Epoch 930, training loss: 0.6575998067855835 = 0.0058222319930791855 + 0.1 * 6.517775535583496
Epoch 930, val loss: 0.8872949481010437
Epoch 940, training loss: 0.6583639979362488 = 0.00568150682374835 + 0.1 * 6.526824474334717
Epoch 940, val loss: 0.8901499509811401
Epoch 950, training loss: 0.6563219428062439 = 0.005546889267861843 + 0.1 * 6.507750511169434
Epoch 950, val loss: 0.8929883241653442
Epoch 960, training loss: 0.6568489670753479 = 0.0054178619757294655 + 0.1 * 6.514310836791992
Epoch 960, val loss: 0.8957249522209167
Epoch 970, training loss: 0.6554131507873535 = 0.005293558817356825 + 0.1 * 6.501195907592773
Epoch 970, val loss: 0.8984390497207642
Epoch 980, training loss: 0.6600775122642517 = 0.0051741646602749825 + 0.1 * 6.5490336418151855
Epoch 980, val loss: 0.9011597037315369
Epoch 990, training loss: 0.6547318696975708 = 0.005059314426034689 + 0.1 * 6.496725559234619
Epoch 990, val loss: 0.9038348197937012
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 2.799001932144165 = 1.9393177032470703 + 0.1 * 8.596841812133789
Epoch 0, val loss: 1.9241799116134644
Epoch 10, training loss: 2.7892215251922607 = 1.929545521736145 + 0.1 * 8.596760749816895
Epoch 10, val loss: 1.9150567054748535
Epoch 20, training loss: 2.7768828868865967 = 1.9172532558441162 + 0.1 * 8.596295356750488
Epoch 20, val loss: 1.9032378196716309
Epoch 30, training loss: 2.758958578109741 = 1.8997522592544556 + 0.1 * 8.592063903808594
Epoch 30, val loss: 1.8861116170883179
Epoch 40, training loss: 2.7300779819488525 = 1.8739471435546875 + 0.1 * 8.561307907104492
Epoch 40, val loss: 1.8611156940460205
Epoch 50, training loss: 2.6791586875915527 = 1.8388911485671997 + 0.1 * 8.40267562866211
Epoch 50, val loss: 1.829049825668335
Epoch 60, training loss: 2.6205506324768066 = 1.8005775213241577 + 0.1 * 8.199731826782227
Epoch 60, val loss: 1.7965511083602905
Epoch 70, training loss: 2.546232223510742 = 1.7634669542312622 + 0.1 * 7.827652931213379
Epoch 70, val loss: 1.7653388977050781
Epoch 80, training loss: 2.4593114852905273 = 1.722782850265503 + 0.1 * 7.365286350250244
Epoch 80, val loss: 1.729874849319458
Epoch 90, training loss: 2.388847827911377 = 1.6720770597457886 + 0.1 * 7.167708873748779
Epoch 90, val loss: 1.6855765581130981
Epoch 100, training loss: 2.3131959438323975 = 1.605123519897461 + 0.1 * 7.080723285675049
Epoch 100, val loss: 1.6266918182373047
Epoch 110, training loss: 2.228421688079834 = 1.525589942932129 + 0.1 * 7.028316497802734
Epoch 110, val loss: 1.5592072010040283
Epoch 120, training loss: 2.141308307647705 = 1.4414470195770264 + 0.1 * 6.998612880706787
Epoch 120, val loss: 1.4911227226257324
Epoch 130, training loss: 2.0539817810058594 = 1.3560489416122437 + 0.1 * 6.979329586029053
Epoch 130, val loss: 1.4228769540786743
Epoch 140, training loss: 1.9650535583496094 = 1.2683287858963013 + 0.1 * 6.967247009277344
Epoch 140, val loss: 1.353636622428894
Epoch 150, training loss: 1.8747724294662476 = 1.1801027059555054 + 0.1 * 6.946697235107422
Epoch 150, val loss: 1.2852702140808105
Epoch 160, training loss: 1.785768985748291 = 1.0928701162338257 + 0.1 * 6.928989410400391
Epoch 160, val loss: 1.2181386947631836
Epoch 170, training loss: 1.7022581100463867 = 1.0109572410583496 + 0.1 * 6.913007736206055
Epoch 170, val loss: 1.1559633016586304
Epoch 180, training loss: 1.6246999502182007 = 0.9349269270896912 + 0.1 * 6.897730350494385
Epoch 180, val loss: 1.0978301763534546
Epoch 190, training loss: 1.5527591705322266 = 0.8640251755714417 + 0.1 * 6.887340545654297
Epoch 190, val loss: 1.042677402496338
Epoch 200, training loss: 1.4857828617095947 = 0.7979782819747925 + 0.1 * 6.87804651260376
Epoch 200, val loss: 0.9909147024154663
Epoch 210, training loss: 1.4233014583587646 = 0.73602694272995 + 0.1 * 6.872745037078857
Epoch 210, val loss: 0.9427567720413208
Epoch 220, training loss: 1.364581823348999 = 0.6785185933113098 + 0.1 * 6.860631465911865
Epoch 220, val loss: 0.9000948071479797
Epoch 230, training loss: 1.309159517288208 = 0.6243243217468262 + 0.1 * 6.84835147857666
Epoch 230, val loss: 0.8623988628387451
Epoch 240, training loss: 1.2574183940887451 = 0.5728322863578796 + 0.1 * 6.845861434936523
Epoch 240, val loss: 0.8294870257377625
Epoch 250, training loss: 1.2075871229171753 = 0.5241382122039795 + 0.1 * 6.834488868713379
Epoch 250, val loss: 0.8010678887367249
Epoch 260, training loss: 1.1597920656204224 = 0.4777229130268097 + 0.1 * 6.820691108703613
Epoch 260, val loss: 0.776225209236145
Epoch 270, training loss: 1.1149916648864746 = 0.43400275707244873 + 0.1 * 6.80988883972168
Epoch 270, val loss: 0.7550786733627319
Epoch 280, training loss: 1.0739866495132446 = 0.39352211356163025 + 0.1 * 6.804645538330078
Epoch 280, val loss: 0.7377621531486511
Epoch 290, training loss: 1.0362193584442139 = 0.35678309202194214 + 0.1 * 6.7943620681762695
Epoch 290, val loss: 0.7245423197746277
Epoch 300, training loss: 1.0020356178283691 = 0.323522686958313 + 0.1 * 6.785128593444824
Epoch 300, val loss: 0.7152127027511597
Epoch 310, training loss: 0.9700132608413696 = 0.2935105860233307 + 0.1 * 6.765027046203613
Epoch 310, val loss: 0.7092764377593994
Epoch 320, training loss: 0.9418041110038757 = 0.2661914825439453 + 0.1 * 6.7561259269714355
Epoch 320, val loss: 0.706013023853302
Epoch 330, training loss: 0.9177259206771851 = 0.2412145435810089 + 0.1 * 6.765113353729248
Epoch 330, val loss: 0.7048437595367432
Epoch 340, training loss: 0.8919401168823242 = 0.21837683022022247 + 0.1 * 6.73563289642334
Epoch 340, val loss: 0.7053039073944092
Epoch 350, training loss: 0.8694160580635071 = 0.19737087190151215 + 0.1 * 6.720451831817627
Epoch 350, val loss: 0.7071267366409302
Epoch 360, training loss: 0.8514658808708191 = 0.1780957579612732 + 0.1 * 6.733701229095459
Epoch 360, val loss: 0.7101489901542664
Epoch 370, training loss: 0.8335244655609131 = 0.16065846383571625 + 0.1 * 6.728659629821777
Epoch 370, val loss: 0.7141947746276855
Epoch 380, training loss: 0.8152323961257935 = 0.14495033025741577 + 0.1 * 6.702820301055908
Epoch 380, val loss: 0.7191329598426819
Epoch 390, training loss: 0.8000863790512085 = 0.1307971030473709 + 0.1 * 6.692892551422119
Epoch 390, val loss: 0.7248265147209167
Epoch 400, training loss: 0.7894032001495361 = 0.11805923283100128 + 0.1 * 6.713439464569092
Epoch 400, val loss: 0.7313438057899475
Epoch 410, training loss: 0.7752628326416016 = 0.10671637952327728 + 0.1 * 6.685464382171631
Epoch 410, val loss: 0.7382491230964661
Epoch 420, training loss: 0.7650179266929626 = 0.09657169878482819 + 0.1 * 6.684462070465088
Epoch 420, val loss: 0.7457690834999084
Epoch 430, training loss: 0.7550274133682251 = 0.08753559738397598 + 0.1 * 6.674917697906494
Epoch 430, val loss: 0.753574013710022
Epoch 440, training loss: 0.7463432550430298 = 0.07948035001754761 + 0.1 * 6.668628692626953
Epoch 440, val loss: 0.7617324590682983
Epoch 450, training loss: 0.7384961843490601 = 0.0723167210817337 + 0.1 * 6.661794662475586
Epoch 450, val loss: 0.7701005339622498
Epoch 460, training loss: 0.7315394878387451 = 0.06593921035528183 + 0.1 * 6.656002998352051
Epoch 460, val loss: 0.7786492705345154
Epoch 470, training loss: 0.7242007851600647 = 0.06026291474699974 + 0.1 * 6.639378547668457
Epoch 470, val loss: 0.7872471213340759
Epoch 480, training loss: 0.7212492823600769 = 0.05521231144666672 + 0.1 * 6.660369396209717
Epoch 480, val loss: 0.7959299683570862
Epoch 490, training loss: 0.7144598364830017 = 0.05072566494345665 + 0.1 * 6.6373419761657715
Epoch 490, val loss: 0.8044393658638
Epoch 500, training loss: 0.7098103165626526 = 0.04672091826796532 + 0.1 * 6.630893707275391
Epoch 500, val loss: 0.8129876255989075
Epoch 510, training loss: 0.7054814100265503 = 0.04314461350440979 + 0.1 * 6.623367786407471
Epoch 510, val loss: 0.8213253021240234
Epoch 520, training loss: 0.7029343247413635 = 0.03993390128016472 + 0.1 * 6.630003929138184
Epoch 520, val loss: 0.8295879364013672
Epoch 530, training loss: 0.6997565031051636 = 0.03705509752035141 + 0.1 * 6.62701416015625
Epoch 530, val loss: 0.8377851843833923
Epoch 540, training loss: 0.6953132748603821 = 0.03446953371167183 + 0.1 * 6.6084370613098145
Epoch 540, val loss: 0.8456811308860779
Epoch 550, training loss: 0.6924571990966797 = 0.03213862329721451 + 0.1 * 6.603185653686523
Epoch 550, val loss: 0.8534619808197021
Epoch 560, training loss: 0.6894950270652771 = 0.030029920861124992 + 0.1 * 6.594651222229004
Epoch 560, val loss: 0.8610845804214478
Epoch 570, training loss: 0.6901102066040039 = 0.02811816707253456 + 0.1 * 6.619920253753662
Epoch 570, val loss: 0.8686029314994812
Epoch 580, training loss: 0.6848222017288208 = 0.02638840489089489 + 0.1 * 6.5843377113342285
Epoch 580, val loss: 0.8758923411369324
Epoch 590, training loss: 0.6848548650741577 = 0.024814780801534653 + 0.1 * 6.600400447845459
Epoch 590, val loss: 0.8829730153083801
Epoch 600, training loss: 0.6816544532775879 = 0.02338116429746151 + 0.1 * 6.582732677459717
Epoch 600, val loss: 0.889924168586731
Epoch 610, training loss: 0.6807257533073425 = 0.022068502381443977 + 0.1 * 6.586572170257568
Epoch 610, val loss: 0.8967142105102539
Epoch 620, training loss: 0.6782446503639221 = 0.020867759361863136 + 0.1 * 6.573768615722656
Epoch 620, val loss: 0.9033390283584595
Epoch 630, training loss: 0.6764671802520752 = 0.019764719530940056 + 0.1 * 6.567024230957031
Epoch 630, val loss: 0.9097830057144165
Epoch 640, training loss: 0.6749849915504456 = 0.018748564645648003 + 0.1 * 6.562364101409912
Epoch 640, val loss: 0.9161441922187805
Epoch 650, training loss: 0.6740517020225525 = 0.01781187392771244 + 0.1 * 6.5623979568481445
Epoch 650, val loss: 0.9223809838294983
Epoch 660, training loss: 0.6725339293479919 = 0.016948651522397995 + 0.1 * 6.555852890014648
Epoch 660, val loss: 0.9283379912376404
Epoch 670, training loss: 0.6733846664428711 = 0.016149451956152916 + 0.1 * 6.572351932525635
Epoch 670, val loss: 0.9342309236526489
Epoch 680, training loss: 0.6709933280944824 = 0.015409082174301147 + 0.1 * 6.55584192276001
Epoch 680, val loss: 0.9399691820144653
Epoch 690, training loss: 0.6702835559844971 = 0.014720916748046875 + 0.1 * 6.555626392364502
Epoch 690, val loss: 0.9456036686897278
Epoch 700, training loss: 0.6693549156188965 = 0.014080081135034561 + 0.1 * 6.55274772644043
Epoch 700, val loss: 0.9510834813117981
Epoch 710, training loss: 0.6671565771102905 = 0.01348349079489708 + 0.1 * 6.536730766296387
Epoch 710, val loss: 0.9564598798751831
Epoch 720, training loss: 0.6673786044120789 = 0.012926648370921612 + 0.1 * 6.544519424438477
Epoch 720, val loss: 0.961692750453949
Epoch 730, training loss: 0.6655437350273132 = 0.012405682355165482 + 0.1 * 6.5313801765441895
Epoch 730, val loss: 0.9668468236923218
Epoch 740, training loss: 0.6652393937110901 = 0.01191806048154831 + 0.1 * 6.533213138580322
Epoch 740, val loss: 0.971857488155365
Epoch 750, training loss: 0.6663802266120911 = 0.0114609869197011 + 0.1 * 6.549192428588867
Epoch 750, val loss: 0.9768007397651672
Epoch 760, training loss: 0.66395103931427 = 0.01103187259286642 + 0.1 * 6.529191493988037
Epoch 760, val loss: 0.9815658926963806
Epoch 770, training loss: 0.6626066565513611 = 0.010628609918057919 + 0.1 * 6.51978063583374
Epoch 770, val loss: 0.986260175704956
Epoch 780, training loss: 0.6629366874694824 = 0.01024817768484354 + 0.1 * 6.526885032653809
Epoch 780, val loss: 0.9908508062362671
Epoch 790, training loss: 0.6625179052352905 = 0.009889529086649418 + 0.1 * 6.526283264160156
Epoch 790, val loss: 0.995449960231781
Epoch 800, training loss: 0.6604326367378235 = 0.009551867842674255 + 0.1 * 6.50880765914917
Epoch 800, val loss: 0.9998433589935303
Epoch 810, training loss: 0.6606405377388 = 0.00923202931880951 + 0.1 * 6.514084815979004
Epoch 810, val loss: 1.0041625499725342
Epoch 820, training loss: 0.6592739224433899 = 0.008929035626351833 + 0.1 * 6.503448963165283
Epoch 820, val loss: 1.008479118347168
Epoch 830, training loss: 0.6617383360862732 = 0.008642412722110748 + 0.1 * 6.530959129333496
Epoch 830, val loss: 1.012692928314209
Epoch 840, training loss: 0.6597450971603394 = 0.008371525444090366 + 0.1 * 6.513735294342041
Epoch 840, val loss: 1.0168135166168213
Epoch 850, training loss: 0.6591086983680725 = 0.008114781230688095 + 0.1 * 6.509939193725586
Epoch 850, val loss: 1.0208004713058472
Epoch 860, training loss: 0.657785177230835 = 0.007870526053011417 + 0.1 * 6.499145984649658
Epoch 860, val loss: 1.024762511253357
Epoch 870, training loss: 0.6577662229537964 = 0.007638419978320599 + 0.1 * 6.501277923583984
Epoch 870, val loss: 1.0286427736282349
Epoch 880, training loss: 0.657632052898407 = 0.007417175453156233 + 0.1 * 6.502148628234863
Epoch 880, val loss: 1.0324671268463135
Epoch 890, training loss: 0.6571470499038696 = 0.007206151727586985 + 0.1 * 6.499409198760986
Epoch 890, val loss: 1.0362422466278076
Epoch 900, training loss: 0.6580692529678345 = 0.007004713173955679 + 0.1 * 6.510644912719727
Epoch 900, val loss: 1.0400086641311646
Epoch 910, training loss: 0.6574458479881287 = 0.00681341951712966 + 0.1 * 6.506324291229248
Epoch 910, val loss: 1.0436010360717773
Epoch 920, training loss: 0.655974805355072 = 0.006631052121520042 + 0.1 * 6.49343729019165
Epoch 920, val loss: 1.0471583604812622
Epoch 930, training loss: 0.6547622680664062 = 0.0064564538188278675 + 0.1 * 6.483058452606201
Epoch 930, val loss: 1.0506362915039062
Epoch 940, training loss: 0.6557499170303345 = 0.006288944743573666 + 0.1 * 6.494609355926514
Epoch 940, val loss: 1.054084062576294
Epoch 950, training loss: 0.6547381281852722 = 0.006128805689513683 + 0.1 * 6.486093044281006
Epoch 950, val loss: 1.0575236082077026
Epoch 960, training loss: 0.6543023586273193 = 0.0059751663357019424 + 0.1 * 6.483271598815918
Epoch 960, val loss: 1.060864806175232
Epoch 970, training loss: 0.6541842222213745 = 0.00582815520465374 + 0.1 * 6.483560562133789
Epoch 970, val loss: 1.064172387123108
Epoch 980, training loss: 0.6563345193862915 = 0.005687404423952103 + 0.1 * 6.506470680236816
Epoch 980, val loss: 1.0673927068710327
Epoch 990, training loss: 0.6532620787620544 = 0.00555291585624218 + 0.1 * 6.4770917892456055
Epoch 990, val loss: 1.0705288648605347
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8408012651555088
The final CL Acc:0.79753, 0.02906, The final GNN Acc:0.84080, 0.00172
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10564])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.809967041015625 = 1.9502865076065063 + 0.1 * 8.596806526184082
Epoch 0, val loss: 1.9481945037841797
Epoch 10, training loss: 2.800201177597046 = 1.9405328035354614 + 0.1 * 8.596683502197266
Epoch 10, val loss: 1.9389605522155762
Epoch 20, training loss: 2.787872791290283 = 1.928265929222107 + 0.1 * 8.596068382263184
Epoch 20, val loss: 1.9269951581954956
Epoch 30, training loss: 2.7700202465057373 = 1.9108620882034302 + 0.1 * 8.591581344604492
Epoch 30, val loss: 1.9096801280975342
Epoch 40, training loss: 2.741018056869507 = 1.8847345113754272 + 0.1 * 8.562835693359375
Epoch 40, val loss: 1.8836332559585571
Epoch 50, training loss: 2.6881933212280273 = 1.8486605882644653 + 0.1 * 8.395328521728516
Epoch 50, val loss: 1.8491606712341309
Epoch 60, training loss: 2.6148579120635986 = 1.8114886283874512 + 0.1 * 8.033693313598633
Epoch 60, val loss: 1.8172111511230469
Epoch 70, training loss: 2.5384626388549805 = 1.7828569412231445 + 0.1 * 7.556055545806885
Epoch 70, val loss: 1.7926100492477417
Epoch 80, training loss: 2.4791290760040283 = 1.7541470527648926 + 0.1 * 7.249819755554199
Epoch 80, val loss: 1.766204595565796
Epoch 90, training loss: 2.4325029850006104 = 1.7180852890014648 + 0.1 * 7.144176483154297
Epoch 90, val loss: 1.7336618900299072
Epoch 100, training loss: 2.378401279449463 = 1.6696062088012695 + 0.1 * 7.087951183319092
Epoch 100, val loss: 1.6921374797821045
Epoch 110, training loss: 2.310166835784912 = 1.6061862707138062 + 0.1 * 7.039805889129639
Epoch 110, val loss: 1.6383885145187378
Epoch 120, training loss: 2.228734016418457 = 1.5286893844604492 + 0.1 * 7.0004448890686035
Epoch 120, val loss: 1.5732778310775757
Epoch 130, training loss: 2.1380672454833984 = 1.441108226776123 + 0.1 * 6.9695892333984375
Epoch 130, val loss: 1.502009391784668
Epoch 140, training loss: 2.042393922805786 = 1.348181128501892 + 0.1 * 6.9421281814575195
Epoch 140, val loss: 1.4294861555099487
Epoch 150, training loss: 1.9436402320861816 = 1.2521743774414062 + 0.1 * 6.914659023284912
Epoch 150, val loss: 1.3570411205291748
Epoch 160, training loss: 1.845321774482727 = 1.1563118696212769 + 0.1 * 6.890099048614502
Epoch 160, val loss: 1.2870961427688599
Epoch 170, training loss: 1.7505788803100586 = 1.0635449886322021 + 0.1 * 6.870337963104248
Epoch 170, val loss: 1.2200531959533691
Epoch 180, training loss: 1.663090467453003 = 0.9763143062591553 + 0.1 * 6.867762088775635
Epoch 180, val loss: 1.1583269834518433
Epoch 190, training loss: 1.5822255611419678 = 0.8983616828918457 + 0.1 * 6.838637828826904
Epoch 190, val loss: 1.1038308143615723
Epoch 200, training loss: 1.5105202198028564 = 0.8278605341911316 + 0.1 * 6.826596736907959
Epoch 200, val loss: 1.0550570487976074
Epoch 210, training loss: 1.4453907012939453 = 0.7642145156860352 + 0.1 * 6.811761856079102
Epoch 210, val loss: 1.0114738941192627
Epoch 220, training loss: 1.38834810256958 = 0.7071915864944458 + 0.1 * 6.811564922332764
Epoch 220, val loss: 0.9734788537025452
Epoch 230, training loss: 1.337613582611084 = 0.6573276519775391 + 0.1 * 6.802858352661133
Epoch 230, val loss: 0.9419195055961609
Epoch 240, training loss: 1.291074514389038 = 0.6127306818962097 + 0.1 * 6.783438205718994
Epoch 240, val loss: 0.915607213973999
Epoch 250, training loss: 1.2484010457992554 = 0.5712797045707703 + 0.1 * 6.771213054656982
Epoch 250, val loss: 0.8931803703308105
Epoch 260, training loss: 1.208092212677002 = 0.5319713354110718 + 0.1 * 6.7612080574035645
Epoch 260, val loss: 0.8737435936927795
Epoch 270, training loss: 1.1701773405075073 = 0.49462834000587463 + 0.1 * 6.755489826202393
Epoch 270, val loss: 0.8565378189086914
Epoch 280, training loss: 1.1344460248947144 = 0.45950284600257874 + 0.1 * 6.74943208694458
Epoch 280, val loss: 0.8414881229400635
Epoch 290, training loss: 1.099570631980896 = 0.4259036183357239 + 0.1 * 6.736670017242432
Epoch 290, val loss: 0.8279306292533875
Epoch 300, training loss: 1.067842960357666 = 0.39334043860435486 + 0.1 * 6.745024681091309
Epoch 300, val loss: 0.8155298829078674
Epoch 310, training loss: 1.0337660312652588 = 0.3617109954357147 + 0.1 * 6.720550060272217
Epoch 310, val loss: 0.8042812943458557
Epoch 320, training loss: 1.004323124885559 = 0.3309005796909332 + 0.1 * 6.734225749969482
Epoch 320, val loss: 0.7944626808166504
Epoch 330, training loss: 0.972497820854187 = 0.3015584349632263 + 0.1 * 6.7093939781188965
Epoch 330, val loss: 0.786901593208313
Epoch 340, training loss: 0.9443356394767761 = 0.2740318179130554 + 0.1 * 6.703038215637207
Epoch 340, val loss: 0.7821639776229858
Epoch 350, training loss: 0.9191932678222656 = 0.2486228197813034 + 0.1 * 6.705704212188721
Epoch 350, val loss: 0.7805713415145874
Epoch 360, training loss: 0.8945178985595703 = 0.2252829521894455 + 0.1 * 6.692349433898926
Epoch 360, val loss: 0.7817062735557556
Epoch 370, training loss: 0.8724809885025024 = 0.20374815165996552 + 0.1 * 6.687328338623047
Epoch 370, val loss: 0.7852700352668762
Epoch 380, training loss: 0.8526912331581116 = 0.18392349779605865 + 0.1 * 6.687677383422852
Epoch 380, val loss: 0.7906549572944641
Epoch 390, training loss: 0.8332811594009399 = 0.16574785113334656 + 0.1 * 6.675332546234131
Epoch 390, val loss: 0.7975274920463562
Epoch 400, training loss: 0.8172640800476074 = 0.14917242527008057 + 0.1 * 6.6809163093566895
Epoch 400, val loss: 0.8055709004402161
Epoch 410, training loss: 0.8006198406219482 = 0.1342126727104187 + 0.1 * 6.664071559906006
Epoch 410, val loss: 0.8143481612205505
Epoch 420, training loss: 0.7882360219955444 = 0.12081727385520935 + 0.1 * 6.674187183380127
Epoch 420, val loss: 0.8236775994300842
Epoch 430, training loss: 0.7741568684577942 = 0.10891121625900269 + 0.1 * 6.652456283569336
Epoch 430, val loss: 0.8334029316902161
Epoch 440, training loss: 0.76331627368927 = 0.09832188487052917 + 0.1 * 6.6499433517456055
Epoch 440, val loss: 0.8434632420539856
Epoch 450, training loss: 0.7538669109344482 = 0.0889286920428276 + 0.1 * 6.6493821144104
Epoch 450, val loss: 0.8536388874053955
Epoch 460, training loss: 0.7449133992195129 = 0.08062134683132172 + 0.1 * 6.642920017242432
Epoch 460, val loss: 0.8640312552452087
Epoch 470, training loss: 0.7374906539916992 = 0.07327967137098312 + 0.1 * 6.642109394073486
Epoch 470, val loss: 0.8743652701377869
Epoch 480, training loss: 0.7297322750091553 = 0.06677776575088501 + 0.1 * 6.629545211791992
Epoch 480, val loss: 0.8848795890808105
Epoch 490, training loss: 0.7258137464523315 = 0.061004772782325745 + 0.1 * 6.64808988571167
Epoch 490, val loss: 0.8953965902328491
Epoch 500, training loss: 0.7185236811637878 = 0.05589728429913521 + 0.1 * 6.626263618469238
Epoch 500, val loss: 0.905748188495636
Epoch 510, training loss: 0.712934136390686 = 0.05135846883058548 + 0.1 * 6.615756511688232
Epoch 510, val loss: 0.9162247776985168
Epoch 520, training loss: 0.7085855007171631 = 0.047308046370744705 + 0.1 * 6.612774848937988
Epoch 520, val loss: 0.9266815781593323
Epoch 530, training loss: 0.7046633958816528 = 0.043691642582416534 + 0.1 * 6.60971736907959
Epoch 530, val loss: 0.9368629455566406
Epoch 540, training loss: 0.7008851766586304 = 0.0404568649828434 + 0.1 * 6.604282855987549
Epoch 540, val loss: 0.9470967054367065
Epoch 550, training loss: 0.6971179246902466 = 0.037554968148469925 + 0.1 * 6.5956292152404785
Epoch 550, val loss: 0.957097053527832
Epoch 560, training loss: 0.6945092678070068 = 0.034946639090776443 + 0.1 * 6.595626354217529
Epoch 560, val loss: 0.9670621752738953
Epoch 570, training loss: 0.6923303604125977 = 0.03258784860372543 + 0.1 * 6.59742546081543
Epoch 570, val loss: 0.9768984317779541
Epoch 580, training loss: 0.6890350580215454 = 0.03045674040913582 + 0.1 * 6.585783004760742
Epoch 580, val loss: 0.9864213466644287
Epoch 590, training loss: 0.6869181990623474 = 0.028527194634079933 + 0.1 * 6.58390998840332
Epoch 590, val loss: 0.9959002733230591
Epoch 600, training loss: 0.6849482655525208 = 0.02676970511674881 + 0.1 * 6.581785678863525
Epoch 600, val loss: 1.0051915645599365
Epoch 610, training loss: 0.683353066444397 = 0.02516711875796318 + 0.1 * 6.581859588623047
Epoch 610, val loss: 1.0143263339996338
Epoch 620, training loss: 0.6814982891082764 = 0.023706573992967606 + 0.1 * 6.577917098999023
Epoch 620, val loss: 1.022922158241272
Epoch 630, training loss: 0.6783450245857239 = 0.022373761981725693 + 0.1 * 6.559712886810303
Epoch 630, val loss: 1.0317556858062744
Epoch 640, training loss: 0.6772604584693909 = 0.021148324012756348 + 0.1 * 6.561120986938477
Epoch 640, val loss: 1.0403661727905273
Epoch 650, training loss: 0.6756500005722046 = 0.020022602751851082 + 0.1 * 6.556273460388184
Epoch 650, val loss: 1.0486986637115479
Epoch 660, training loss: 0.6741132140159607 = 0.01898665726184845 + 0.1 * 6.551265716552734
Epoch 660, val loss: 1.0568338632583618
Epoch 670, training loss: 0.672829806804657 = 0.018032778054475784 + 0.1 * 6.547970294952393
Epoch 670, val loss: 1.0648877620697021
Epoch 680, training loss: 0.6712227463722229 = 0.01715056598186493 + 0.1 * 6.540721416473389
Epoch 680, val loss: 1.0727659463882446
Epoch 690, training loss: 0.6714707016944885 = 0.01633344404399395 + 0.1 * 6.551372528076172
Epoch 690, val loss: 1.0804896354675293
Epoch 700, training loss: 0.6707618832588196 = 0.015576954931020737 + 0.1 * 6.551849365234375
Epoch 700, val loss: 1.0878056287765503
Epoch 710, training loss: 0.6688981056213379 = 0.014876649715006351 + 0.1 * 6.5402140617370605
Epoch 710, val loss: 1.0951915979385376
Epoch 720, training loss: 0.6677366495132446 = 0.01422454509884119 + 0.1 * 6.535120964050293
Epoch 720, val loss: 1.1024589538574219
Epoch 730, training loss: 0.6683730483055115 = 0.013617143034934998 + 0.1 * 6.547558784484863
Epoch 730, val loss: 1.1094979047775269
Epoch 740, training loss: 0.666077733039856 = 0.013049658387899399 + 0.1 * 6.530280590057373
Epoch 740, val loss: 1.1163461208343506
Epoch 750, training loss: 0.6651777625083923 = 0.012522284872829914 + 0.1 * 6.526554584503174
Epoch 750, val loss: 1.123207688331604
Epoch 760, training loss: 0.6646691560745239 = 0.012026900425553322 + 0.1 * 6.526422500610352
Epoch 760, val loss: 1.129868507385254
Epoch 770, training loss: 0.6636849045753479 = 0.011564339511096478 + 0.1 * 6.521205902099609
Epoch 770, val loss: 1.1364659070968628
Epoch 780, training loss: 0.6627205014228821 = 0.011129042133688927 + 0.1 * 6.5159149169921875
Epoch 780, val loss: 1.1428053379058838
Epoch 790, training loss: 0.6623414158821106 = 0.010720889084041119 + 0.1 * 6.516204833984375
Epoch 790, val loss: 1.149118185043335
Epoch 800, training loss: 0.6618216037750244 = 0.010337397456169128 + 0.1 * 6.514841556549072
Epoch 800, val loss: 1.1552752256393433
Epoch 810, training loss: 0.6626556515693665 = 0.009975277818739414 + 0.1 * 6.526803970336914
Epoch 810, val loss: 1.1613930463790894
Epoch 820, training loss: 0.6610522270202637 = 0.009634596295654774 + 0.1 * 6.514175891876221
Epoch 820, val loss: 1.167163610458374
Epoch 830, training loss: 0.6602227091789246 = 0.009313248097896576 + 0.1 * 6.509094715118408
Epoch 830, val loss: 1.1729212999343872
Epoch 840, training loss: 0.6598626971244812 = 0.00900991354137659 + 0.1 * 6.508527755737305
Epoch 840, val loss: 1.1786316633224487
Epoch 850, training loss: 0.6589449644088745 = 0.008722743950784206 + 0.1 * 6.502222061157227
Epoch 850, val loss: 1.1840561628341675
Epoch 860, training loss: 0.6586564183235168 = 0.008451338857412338 + 0.1 * 6.502050399780273
Epoch 860, val loss: 1.1895856857299805
Epoch 870, training loss: 0.6576176881790161 = 0.008193540386855602 + 0.1 * 6.494241237640381
Epoch 870, val loss: 1.1949456930160522
Epoch 880, training loss: 0.6577562093734741 = 0.007948342710733414 + 0.1 * 6.498078346252441
Epoch 880, val loss: 1.2001179456710815
Epoch 890, training loss: 0.6571925282478333 = 0.007717049680650234 + 0.1 * 6.494754314422607
Epoch 890, val loss: 1.2052193880081177
Epoch 900, training loss: 0.6569728255271912 = 0.007495942525565624 + 0.1 * 6.4947686195373535
Epoch 900, val loss: 1.2103759050369263
Epoch 910, training loss: 0.6561044454574585 = 0.007285520434379578 + 0.1 * 6.488188743591309
Epoch 910, val loss: 1.2153280973434448
Epoch 920, training loss: 0.6558545231819153 = 0.007085055578500032 + 0.1 * 6.487694263458252
Epoch 920, val loss: 1.2200384140014648
Epoch 930, training loss: 0.6556130051612854 = 0.006894887890666723 + 0.1 * 6.487181186676025
Epoch 930, val loss: 1.22484290599823
Epoch 940, training loss: 0.6551766991615295 = 0.006713090464472771 + 0.1 * 6.484635829925537
Epoch 940, val loss: 1.2295912504196167
Epoch 950, training loss: 0.6540603041648865 = 0.0065389359369874 + 0.1 * 6.475213527679443
Epoch 950, val loss: 1.2340967655181885
Epoch 960, training loss: 0.653968095779419 = 0.006373151671141386 + 0.1 * 6.475949287414551
Epoch 960, val loss: 1.238681435585022
Epoch 970, training loss: 0.6561191082000732 = 0.006214046385139227 + 0.1 * 6.499050617218018
Epoch 970, val loss: 1.2431272268295288
Epoch 980, training loss: 0.6537759304046631 = 0.006062325090169907 + 0.1 * 6.47713565826416
Epoch 980, val loss: 1.2473613023757935
Epoch 990, training loss: 0.6539514660835266 = 0.005916939117014408 + 0.1 * 6.480344772338867
Epoch 990, val loss: 1.2517660856246948
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 2.797436237335205 = 1.9377517700195312 + 0.1 * 8.596845626831055
Epoch 0, val loss: 1.942921757698059
Epoch 10, training loss: 2.787442445755005 = 1.927764892578125 + 0.1 * 8.596776008605957
Epoch 10, val loss: 1.9331218004226685
Epoch 20, training loss: 2.7749838829040527 = 1.9153501987457275 + 0.1 * 8.596336364746094
Epoch 20, val loss: 1.9205552339553833
Epoch 30, training loss: 2.757181406021118 = 1.8979426622390747 + 0.1 * 8.592388153076172
Epoch 30, val loss: 1.9026966094970703
Epoch 40, training loss: 2.7291147708892822 = 1.8725638389587402 + 0.1 * 8.565508842468262
Epoch 40, val loss: 1.8770053386688232
Epoch 50, training loss: 2.6823644638061523 = 1.8391660451889038 + 0.1 * 8.431983947753906
Epoch 50, val loss: 1.8449352979660034
Epoch 60, training loss: 2.6302008628845215 = 1.8042844533920288 + 0.1 * 8.259162902832031
Epoch 60, val loss: 1.8132925033569336
Epoch 70, training loss: 2.5755767822265625 = 1.7709205150604248 + 0.1 * 8.046561241149902
Epoch 70, val loss: 1.7822831869125366
Epoch 80, training loss: 2.5060200691223145 = 1.73122239112854 + 0.1 * 7.747975826263428
Epoch 80, val loss: 1.7454845905303955
Epoch 90, training loss: 2.427185297012329 = 1.6822340488433838 + 0.1 * 7.449512958526611
Epoch 90, val loss: 1.703124761581421
Epoch 100, training loss: 2.3507802486419678 = 1.6182420253753662 + 0.1 * 7.325381278991699
Epoch 100, val loss: 1.647376298904419
Epoch 110, training loss: 2.2605648040771484 = 1.539171814918518 + 0.1 * 7.213929176330566
Epoch 110, val loss: 1.5782995223999023
Epoch 120, training loss: 2.1658363342285156 = 1.4554871320724487 + 0.1 * 7.103490829467773
Epoch 120, val loss: 1.509292483329773
Epoch 130, training loss: 2.0742368698120117 = 1.3707119226455688 + 0.1 * 7.035249710083008
Epoch 130, val loss: 1.4433025121688843
Epoch 140, training loss: 1.9872667789459229 = 1.287330985069275 + 0.1 * 6.999357223510742
Epoch 140, val loss: 1.381099820137024
Epoch 150, training loss: 1.9008128643035889 = 1.2041350603103638 + 0.1 * 6.966778755187988
Epoch 150, val loss: 1.3210417032241821
Epoch 160, training loss: 1.813880205154419 = 1.1197500228881836 + 0.1 * 6.941300868988037
Epoch 160, val loss: 1.2629799842834473
Epoch 170, training loss: 1.726288080215454 = 1.0341085195541382 + 0.1 * 6.92179536819458
Epoch 170, val loss: 1.2040374279022217
Epoch 180, training loss: 1.6392970085144043 = 0.9486832022666931 + 0.1 * 6.906137466430664
Epoch 180, val loss: 1.1446424722671509
Epoch 190, training loss: 1.5572172403335571 = 0.8658466935157776 + 0.1 * 6.913705348968506
Epoch 190, val loss: 1.086658239364624
Epoch 200, training loss: 1.4805139303207397 = 0.7915221452713013 + 0.1 * 6.889917850494385
Epoch 200, val loss: 1.0352654457092285
Epoch 210, training loss: 1.4138507843017578 = 0.7263792157173157 + 0.1 * 6.874715805053711
Epoch 210, val loss: 0.9918737411499023
Epoch 220, training loss: 1.357032299041748 = 0.6695416569709778 + 0.1 * 6.87490701675415
Epoch 220, val loss: 0.9565789103507996
Epoch 230, training loss: 1.3053025007247925 = 0.6198816895484924 + 0.1 * 6.854207992553711
Epoch 230, val loss: 0.9286971092224121
Epoch 240, training loss: 1.2590134143829346 = 0.5747998356819153 + 0.1 * 6.842135906219482
Epoch 240, val loss: 0.9066721796989441
Epoch 250, training loss: 1.2176706790924072 = 0.5327828526496887 + 0.1 * 6.848877906799316
Epoch 250, val loss: 0.8894882202148438
Epoch 260, training loss: 1.1765224933624268 = 0.49366164207458496 + 0.1 * 6.828608989715576
Epoch 260, val loss: 0.8768478631973267
Epoch 270, training loss: 1.137753963470459 = 0.4566141963005066 + 0.1 * 6.811397075653076
Epoch 270, val loss: 0.868221640586853
Epoch 280, training loss: 1.1036440134048462 = 0.4215169847011566 + 0.1 * 6.821269989013672
Epoch 280, val loss: 0.8631910085678101
Epoch 290, training loss: 1.0676963329315186 = 0.3885729908943176 + 0.1 * 6.791233539581299
Epoch 290, val loss: 0.8613146543502808
Epoch 300, training loss: 1.0360602140426636 = 0.3573227524757385 + 0.1 * 6.787374496459961
Epoch 300, val loss: 0.8622156977653503
Epoch 310, training loss: 1.0051714181900024 = 0.32796838879585266 + 0.1 * 6.772030353546143
Epoch 310, val loss: 0.8654450178146362
Epoch 320, training loss: 0.9765152931213379 = 0.30037403106689453 + 0.1 * 6.761412620544434
Epoch 320, val loss: 0.8706103563308716
Epoch 330, training loss: 0.9496363401412964 = 0.27446576952934265 + 0.1 * 6.751705646514893
Epoch 330, val loss: 0.8774695992469788
Epoch 340, training loss: 0.9255671501159668 = 0.2502727210521698 + 0.1 * 6.752944469451904
Epoch 340, val loss: 0.8859341144561768
Epoch 350, training loss: 0.902368426322937 = 0.22791588306427002 + 0.1 * 6.74452543258667
Epoch 350, val loss: 0.8959738612174988
Epoch 360, training loss: 0.8804057240486145 = 0.20738565921783447 + 0.1 * 6.730200290679932
Epoch 360, val loss: 0.9074395895004272
Epoch 370, training loss: 0.8600717186927795 = 0.18863864243030548 + 0.1 * 6.714330673217773
Epoch 370, val loss: 0.9204511642456055
Epoch 380, training loss: 0.8457931876182556 = 0.1716204136610031 + 0.1 * 6.741727352142334
Epoch 380, val loss: 0.9348422288894653
Epoch 390, training loss: 0.8272517919540405 = 0.15632718801498413 + 0.1 * 6.7092461585998535
Epoch 390, val loss: 0.9504587650299072
Epoch 400, training loss: 0.8124039769172668 = 0.14251601696014404 + 0.1 * 6.698879718780518
Epoch 400, val loss: 0.967326283454895
Epoch 410, training loss: 0.7990299463272095 = 0.13007496297359467 + 0.1 * 6.689549446105957
Epoch 410, val loss: 0.9851707220077515
Epoch 420, training loss: 0.7880948185920715 = 0.11886943876743317 + 0.1 * 6.692254066467285
Epoch 420, val loss: 1.0038012266159058
Epoch 430, training loss: 0.7774870991706848 = 0.1088024452328682 + 0.1 * 6.6868462562561035
Epoch 430, val loss: 1.0229805707931519
Epoch 440, training loss: 0.7667319774627686 = 0.09978003054857254 + 0.1 * 6.669519424438477
Epoch 440, val loss: 1.042279839515686
Epoch 450, training loss: 0.7577717304229736 = 0.09167882055044174 + 0.1 * 6.660929203033447
Epoch 450, val loss: 1.0617965459823608
Epoch 460, training loss: 0.7525296807289124 = 0.08437896519899368 + 0.1 * 6.681507110595703
Epoch 460, val loss: 1.081356406211853
Epoch 470, training loss: 0.7434424757957458 = 0.07782841473817825 + 0.1 * 6.656140327453613
Epoch 470, val loss: 1.1008586883544922
Epoch 480, training loss: 0.7365606427192688 = 0.0719153881072998 + 0.1 * 6.6464524269104
Epoch 480, val loss: 1.1200816631317139
Epoch 490, training loss: 0.7313093543052673 = 0.06656860560178757 + 0.1 * 6.647407531738281
Epoch 490, val loss: 1.139204740524292
Epoch 500, training loss: 0.7247893214225769 = 0.061734557151794434 + 0.1 * 6.630547523498535
Epoch 500, val loss: 1.1581385135650635
Epoch 510, training loss: 0.7212508916854858 = 0.05735060200095177 + 0.1 * 6.639003276824951
Epoch 510, val loss: 1.1767178773880005
Epoch 520, training loss: 0.7180087566375732 = 0.053383372724056244 + 0.1 * 6.646254062652588
Epoch 520, val loss: 1.1951260566711426
Epoch 530, training loss: 0.7114396095275879 = 0.04978588595986366 + 0.1 * 6.616536617279053
Epoch 530, val loss: 1.2130117416381836
Epoch 540, training loss: 0.7088309526443481 = 0.04651172086596489 + 0.1 * 6.623192310333252
Epoch 540, val loss: 1.2305998802185059
Epoch 550, training loss: 0.7041281461715698 = 0.04352547600865364 + 0.1 * 6.606026649475098
Epoch 550, val loss: 1.2480413913726807
Epoch 560, training loss: 0.7027782797813416 = 0.0407990962266922 + 0.1 * 6.6197919845581055
Epoch 560, val loss: 1.264912724494934
Epoch 570, training loss: 0.6983749270439148 = 0.03831010311841965 + 0.1 * 6.600647926330566
Epoch 570, val loss: 1.2815684080123901
Epoch 580, training loss: 0.6960843801498413 = 0.03602878749370575 + 0.1 * 6.600555419921875
Epoch 580, val loss: 1.2978270053863525
Epoch 590, training loss: 0.6946910619735718 = 0.03393634036183357 + 0.1 * 6.607547283172607
Epoch 590, val loss: 1.3137567043304443
Epoch 600, training loss: 0.6919188499450684 = 0.03201429545879364 + 0.1 * 6.599045276641846
Epoch 600, val loss: 1.3294165134429932
Epoch 610, training loss: 0.690793514251709 = 0.030246004462242126 + 0.1 * 6.605474948883057
Epoch 610, val loss: 1.3445641994476318
Epoch 620, training loss: 0.6874059438705444 = 0.02861848473548889 + 0.1 * 6.587874412536621
Epoch 620, val loss: 1.3595340251922607
Epoch 630, training loss: 0.6879086494445801 = 0.027115659788250923 + 0.1 * 6.6079301834106445
Epoch 630, val loss: 1.374047040939331
Epoch 640, training loss: 0.6842731237411499 = 0.025729572400450706 + 0.1 * 6.585435390472412
Epoch 640, val loss: 1.388209581375122
Epoch 650, training loss: 0.6827086806297302 = 0.024446522817015648 + 0.1 * 6.582621097564697
Epoch 650, val loss: 1.4020861387252808
Epoch 660, training loss: 0.6804462671279907 = 0.023256229236721992 + 0.1 * 6.571900367736816
Epoch 660, val loss: 1.4156962633132935
Epoch 670, training loss: 0.679154634475708 = 0.02215079963207245 + 0.1 * 6.570038318634033
Epoch 670, val loss: 1.429157018661499
Epoch 680, training loss: 0.6773434281349182 = 0.021122368052601814 + 0.1 * 6.5622100830078125
Epoch 680, val loss: 1.4420419931411743
Epoch 690, training loss: 0.6779996752738953 = 0.020167119801044464 + 0.1 * 6.578325271606445
Epoch 690, val loss: 1.4547500610351562
Epoch 700, training loss: 0.6765357255935669 = 0.019278772175312042 + 0.1 * 6.572569370269775
Epoch 700, val loss: 1.4671597480773926
Epoch 710, training loss: 0.6742648482322693 = 0.01844714768230915 + 0.1 * 6.558176517486572
Epoch 710, val loss: 1.4793449640274048
Epoch 720, training loss: 0.6742603778839111 = 0.01766924001276493 + 0.1 * 6.565911293029785
Epoch 720, val loss: 1.4910885095596313
Epoch 730, training loss: 0.67165607213974 = 0.016943739727139473 + 0.1 * 6.547122955322266
Epoch 730, val loss: 1.502866268157959
Epoch 740, training loss: 0.6719990968704224 = 0.016262128949165344 + 0.1 * 6.557369709014893
Epoch 740, val loss: 1.514254093170166
Epoch 750, training loss: 0.6702864170074463 = 0.015622838400304317 + 0.1 * 6.546635627746582
Epoch 750, val loss: 1.525417685508728
Epoch 760, training loss: 0.670846164226532 = 0.01502198912203312 + 0.1 * 6.558241844177246
Epoch 760, val loss: 1.5362194776535034
Epoch 770, training loss: 0.6683843731880188 = 0.014458066783845425 + 0.1 * 6.539262771606445
Epoch 770, val loss: 1.5469586849212646
Epoch 780, training loss: 0.6687144041061401 = 0.01392649207264185 + 0.1 * 6.547878742218018
Epoch 780, val loss: 1.5573205947875977
Epoch 790, training loss: 0.6680520176887512 = 0.013426495715975761 + 0.1 * 6.546255111694336
Epoch 790, val loss: 1.5677522420883179
Epoch 800, training loss: 0.667885422706604 = 0.012953436002135277 + 0.1 * 6.549319744110107
Epoch 800, val loss: 1.5775538682937622
Epoch 810, training loss: 0.6658492684364319 = 0.01250782422721386 + 0.1 * 6.533414840698242
Epoch 810, val loss: 1.587406873703003
Epoch 820, training loss: 0.6657046675682068 = 0.012085665948688984 + 0.1 * 6.536189556121826
Epoch 820, val loss: 1.5969351530075073
Epoch 830, training loss: 0.6650158166885376 = 0.011686175130307674 + 0.1 * 6.533296585083008
Epoch 830, val loss: 1.6063647270202637
Epoch 840, training loss: 0.6653701066970825 = 0.011307675391435623 + 0.1 * 6.540624618530273
Epoch 840, val loss: 1.6156243085861206
Epoch 850, training loss: 0.6640622019767761 = 0.010949142277240753 + 0.1 * 6.531130790710449
Epoch 850, val loss: 1.6246387958526611
Epoch 860, training loss: 0.6620985269546509 = 0.010608364827930927 + 0.1 * 6.514901638031006
Epoch 860, val loss: 1.6334956884384155
Epoch 870, training loss: 0.6641097664833069 = 0.010284103453159332 + 0.1 * 6.5382561683654785
Epoch 870, val loss: 1.6422065496444702
Epoch 880, training loss: 0.6624086499214172 = 0.00997620727866888 + 0.1 * 6.524324417114258
Epoch 880, val loss: 1.6506783962249756
Epoch 890, training loss: 0.6640044450759888 = 0.009682794101536274 + 0.1 * 6.543216705322266
Epoch 890, val loss: 1.659063696861267
Epoch 900, training loss: 0.6615273952484131 = 0.009403692558407784 + 0.1 * 6.521236896514893
Epoch 900, val loss: 1.666959285736084
Epoch 910, training loss: 0.6606561541557312 = 0.00913852546364069 + 0.1 * 6.515176296234131
Epoch 910, val loss: 1.6751809120178223
Epoch 920, training loss: 0.6602651476860046 = 0.008884552866220474 + 0.1 * 6.513805866241455
Epoch 920, val loss: 1.6829277276992798
Epoch 930, training loss: 0.6600936055183411 = 0.008642314933240414 + 0.1 * 6.514512538909912
Epoch 930, val loss: 1.690748691558838
Epoch 940, training loss: 0.6589740514755249 = 0.008410732261836529 + 0.1 * 6.5056328773498535
Epoch 940, val loss: 1.698277235031128
Epoch 950, training loss: 0.6587168574333191 = 0.008189172483980656 + 0.1 * 6.505276679992676
Epoch 950, val loss: 1.7057650089263916
Epoch 960, training loss: 0.658514142036438 = 0.007977325469255447 + 0.1 * 6.505368232727051
Epoch 960, val loss: 1.7131202220916748
Epoch 970, training loss: 0.6574112772941589 = 0.007774281781166792 + 0.1 * 6.4963698387146
Epoch 970, val loss: 1.72037672996521
Epoch 980, training loss: 0.6567975878715515 = 0.007579512428492308 + 0.1 * 6.492180824279785
Epoch 980, val loss: 1.7272818088531494
Epoch 990, training loss: 0.6567788124084473 = 0.007393160834908485 + 0.1 * 6.493856430053711
Epoch 990, val loss: 1.7343162298202515
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.823405376910912
=== training gcn model ===
Epoch 0, training loss: 2.8070948123931885 = 1.9474114179611206 + 0.1 * 8.596834182739258
Epoch 0, val loss: 1.949580430984497
Epoch 10, training loss: 2.7964608669281006 = 1.9367858171463013 + 0.1 * 8.596750259399414
Epoch 10, val loss: 1.9393088817596436
Epoch 20, training loss: 2.7830991744995117 = 1.923462986946106 + 0.1 * 8.59636116027832
Epoch 20, val loss: 1.925904393196106
Epoch 30, training loss: 2.7642197608947754 = 1.9048577547073364 + 0.1 * 8.593620300292969
Epoch 30, val loss: 1.9068477153778076
Epoch 40, training loss: 2.7359273433685303 = 1.8781232833862305 + 0.1 * 8.57804012298584
Epoch 40, val loss: 1.879729151725769
Epoch 50, training loss: 2.696258544921875 = 1.8431265354156494 + 0.1 * 8.531320571899414
Epoch 50, val loss: 1.8461377620697021
Epoch 60, training loss: 2.6461427211761475 = 1.8085968494415283 + 0.1 * 8.375457763671875
Epoch 60, val loss: 1.8164259195327759
Epoch 70, training loss: 2.6068332195281982 = 1.780516266822815 + 0.1 * 8.263169288635254
Epoch 70, val loss: 1.7924816608428955
Epoch 80, training loss: 2.547572374343872 = 1.7466455698013306 + 0.1 * 8.009267807006836
Epoch 80, val loss: 1.7613186836242676
Epoch 90, training loss: 2.4671683311462402 = 1.7028272151947021 + 0.1 * 7.6434102058410645
Epoch 90, val loss: 1.7228630781173706
Epoch 100, training loss: 2.387361764907837 = 1.6465810537338257 + 0.1 * 7.407806396484375
Epoch 100, val loss: 1.6751878261566162
Epoch 110, training loss: 2.302377462387085 = 1.5769686698913574 + 0.1 * 7.254087924957275
Epoch 110, val loss: 1.6174765825271606
Epoch 120, training loss: 2.2150471210479736 = 1.4981284141540527 + 0.1 * 7.169187545776367
Epoch 120, val loss: 1.5536980628967285
Epoch 130, training loss: 2.126657485961914 = 1.4151729345321655 + 0.1 * 7.114846706390381
Epoch 130, val loss: 1.4880938529968262
Epoch 140, training loss: 2.0381572246551514 = 1.330582618713379 + 0.1 * 7.075746059417725
Epoch 140, val loss: 1.423038363456726
Epoch 150, training loss: 1.9485862255096436 = 1.2437150478363037 + 0.1 * 7.04871129989624
Epoch 150, val loss: 1.3577091693878174
Epoch 160, training loss: 1.8587521314620972 = 1.155809998512268 + 0.1 * 7.029421329498291
Epoch 160, val loss: 1.2929133176803589
Epoch 170, training loss: 1.7729535102844238 = 1.0714068412780762 + 0.1 * 7.015466213226318
Epoch 170, val loss: 1.2323942184448242
Epoch 180, training loss: 1.694125771522522 = 0.9937799572944641 + 0.1 * 7.003458023071289
Epoch 180, val loss: 1.1780654191970825
Epoch 190, training loss: 1.622232437133789 = 0.9235010147094727 + 0.1 * 6.987314701080322
Epoch 190, val loss: 1.1299623250961304
Epoch 200, training loss: 1.5557217597961426 = 0.8583654165267944 + 0.1 * 6.973563194274902
Epoch 200, val loss: 1.0858540534973145
Epoch 210, training loss: 1.4921951293945312 = 0.7966732382774353 + 0.1 * 6.955219268798828
Epoch 210, val loss: 1.0438894033432007
Epoch 220, training loss: 1.4315272569656372 = 0.7373985648155212 + 0.1 * 6.941286563873291
Epoch 220, val loss: 1.003693699836731
Epoch 230, training loss: 1.3748177289962769 = 0.6807551383972168 + 0.1 * 6.9406256675720215
Epoch 230, val loss: 0.9661986231803894
Epoch 240, training loss: 1.3200644254684448 = 0.6275942921638489 + 0.1 * 6.92470121383667
Epoch 240, val loss: 0.9326491951942444
Epoch 250, training loss: 1.2690415382385254 = 0.5774469971656799 + 0.1 * 6.915944576263428
Epoch 250, val loss: 0.9031537771224976
Epoch 260, training loss: 1.2209711074829102 = 0.529931902885437 + 0.1 * 6.910392761230469
Epoch 260, val loss: 0.8776517510414124
Epoch 270, training loss: 1.1766489744186401 = 0.4845687747001648 + 0.1 * 6.920801639556885
Epoch 270, val loss: 0.8554931879043579
Epoch 280, training loss: 1.1315064430236816 = 0.441306471824646 + 0.1 * 6.902000427246094
Epoch 280, val loss: 0.8360173106193542
Epoch 290, training loss: 1.0885862112045288 = 0.3989485800266266 + 0.1 * 6.896376609802246
Epoch 290, val loss: 0.8182141780853271
Epoch 300, training loss: 1.0459034442901611 = 0.3567837178707123 + 0.1 * 6.891197681427002
Epoch 300, val loss: 0.8016428351402283
Epoch 310, training loss: 1.0041415691375732 = 0.3149557113647461 + 0.1 * 6.891857624053955
Epoch 310, val loss: 0.7863960266113281
Epoch 320, training loss: 0.9631832242012024 = 0.2749250531196594 + 0.1 * 6.88258171081543
Epoch 320, val loss: 0.7732036709785461
Epoch 330, training loss: 0.9256108403205872 = 0.23794697225093842 + 0.1 * 6.876638412475586
Epoch 330, val loss: 0.7627337574958801
Epoch 340, training loss: 0.8932790756225586 = 0.2049282342195511 + 0.1 * 6.883508205413818
Epoch 340, val loss: 0.7554609775543213
Epoch 350, training loss: 0.8627766966819763 = 0.17651082575321198 + 0.1 * 6.862658500671387
Epoch 350, val loss: 0.7517111897468567
Epoch 360, training loss: 0.8378936648368835 = 0.15236663818359375 + 0.1 * 6.8552703857421875
Epoch 360, val loss: 0.7511355876922607
Epoch 370, training loss: 0.8171437382698059 = 0.13205458223819733 + 0.1 * 6.850891590118408
Epoch 370, val loss: 0.7533661127090454
Epoch 380, training loss: 0.7996506094932556 = 0.11492915451526642 + 0.1 * 6.847214698791504
Epoch 380, val loss: 0.757947564125061
Epoch 390, training loss: 0.7845685482025146 = 0.1005283072590828 + 0.1 * 6.840402603149414
Epoch 390, val loss: 0.7643430829048157
Epoch 400, training loss: 0.7707169055938721 = 0.08837083727121353 + 0.1 * 6.823460102081299
Epoch 400, val loss: 0.7721163034439087
Epoch 410, training loss: 0.7601708173751831 = 0.07806539535522461 + 0.1 * 6.821053981781006
Epoch 410, val loss: 0.780953586101532
Epoch 420, training loss: 0.7504115104675293 = 0.06935914605855942 + 0.1 * 6.810523509979248
Epoch 420, val loss: 0.7904589772224426
Epoch 430, training loss: 0.7425078749656677 = 0.061969559639692307 + 0.1 * 6.80538272857666
Epoch 430, val loss: 0.8005149960517883
Epoch 440, training loss: 0.7346560955047607 = 0.055682405829429626 + 0.1 * 6.789736747741699
Epoch 440, val loss: 0.8106304407119751
Epoch 450, training loss: 0.7300845980644226 = 0.05029759556055069 + 0.1 * 6.797869682312012
Epoch 450, val loss: 0.8209702372550964
Epoch 460, training loss: 0.7232152819633484 = 0.0456756129860878 + 0.1 * 6.775396347045898
Epoch 460, val loss: 0.8310009837150574
Epoch 470, training loss: 0.7184742093086243 = 0.04167637974023819 + 0.1 * 6.767977714538574
Epoch 470, val loss: 0.8410809636116028
Epoch 480, training loss: 0.7161839008331299 = 0.038186728954315186 + 0.1 * 6.779971599578857
Epoch 480, val loss: 0.8510096669197083
Epoch 490, training loss: 0.7110437154769897 = 0.03513479232788086 + 0.1 * 6.75908899307251
Epoch 490, val loss: 0.8605566024780273
Epoch 500, training loss: 0.7086511850357056 = 0.032445162534713745 + 0.1 * 6.762060642242432
Epoch 500, val loss: 0.8701424598693848
Epoch 510, training loss: 0.705666720867157 = 0.030070317909121513 + 0.1 * 6.7559638023376465
Epoch 510, val loss: 0.8791608214378357
Epoch 520, training loss: 0.7019659280776978 = 0.027962980791926384 + 0.1 * 6.740029335021973
Epoch 520, val loss: 0.8881223201751709
Epoch 530, training loss: 0.699897825717926 = 0.026076817885041237 + 0.1 * 6.738210201263428
Epoch 530, val loss: 0.8967572450637817
Epoch 540, training loss: 0.6975123286247253 = 0.024386348202824593 + 0.1 * 6.731259822845459
Epoch 540, val loss: 0.9053112268447876
Epoch 550, training loss: 0.695366382598877 = 0.022867057472467422 + 0.1 * 6.724992752075195
Epoch 550, val loss: 0.9135036468505859
Epoch 560, training loss: 0.6955021619796753 = 0.02149185910820961 + 0.1 * 6.740102767944336
Epoch 560, val loss: 0.9214656352996826
Epoch 570, training loss: 0.6912937164306641 = 0.02025044709444046 + 0.1 * 6.710432529449463
Epoch 570, val loss: 0.9292721152305603
Epoch 580, training loss: 0.6904608011245728 = 0.019124915823340416 + 0.1 * 6.713358402252197
Epoch 580, val loss: 0.9367331266403198
Epoch 590, training loss: 0.6883538365364075 = 0.018099604174494743 + 0.1 * 6.702542304992676
Epoch 590, val loss: 0.944126546382904
Epoch 600, training loss: 0.6865191459655762 = 0.01716243289411068 + 0.1 * 6.693566799163818
Epoch 600, val loss: 0.951129138469696
Epoch 610, training loss: 0.6873669624328613 = 0.016300112009048462 + 0.1 * 6.710668563842773
Epoch 610, val loss: 0.9581528306007385
Epoch 620, training loss: 0.6851289868354797 = 0.015508158132433891 + 0.1 * 6.6962080001831055
Epoch 620, val loss: 0.9648842215538025
Epoch 630, training loss: 0.6830678582191467 = 0.01477577444165945 + 0.1 * 6.682920932769775
Epoch 630, val loss: 0.971465528011322
Epoch 640, training loss: 0.683648943901062 = 0.01409924402832985 + 0.1 * 6.695497035980225
Epoch 640, val loss: 0.9782099723815918
Epoch 650, training loss: 0.6808012127876282 = 0.013476685620844364 + 0.1 * 6.673245429992676
Epoch 650, val loss: 0.9841470122337341
Epoch 660, training loss: 0.6806299686431885 = 0.012897937558591366 + 0.1 * 6.67732048034668
Epoch 660, val loss: 0.9903367757797241
Epoch 670, training loss: 0.6800196170806885 = 0.012359666638076305 + 0.1 * 6.676599025726318
Epoch 670, val loss: 0.996414303779602
Epoch 680, training loss: 0.6768777966499329 = 0.011860495433211327 + 0.1 * 6.650173187255859
Epoch 680, val loss: 1.0018948316574097
Epoch 690, training loss: 0.6763026714324951 = 0.011393441818654537 + 0.1 * 6.649092197418213
Epoch 690, val loss: 1.0076276063919067
Epoch 700, training loss: 0.675685703754425 = 0.01095536258071661 + 0.1 * 6.647303581237793
Epoch 700, val loss: 1.0131704807281494
Epoch 710, training loss: 0.6740937232971191 = 0.010544759221374989 + 0.1 * 6.635489463806152
Epoch 710, val loss: 1.018561840057373
Epoch 720, training loss: 0.6747939586639404 = 0.010159522294998169 + 0.1 * 6.646344184875488
Epoch 720, val loss: 1.0239009857177734
Epoch 730, training loss: 0.6730005145072937 = 0.009797146543860435 + 0.1 * 6.632033348083496
Epoch 730, val loss: 1.0290031433105469
Epoch 740, training loss: 0.6720816493034363 = 0.009457207284867764 + 0.1 * 6.626244068145752
Epoch 740, val loss: 1.0339465141296387
Epoch 750, training loss: 0.6714112758636475 = 0.00913613848388195 + 0.1 * 6.622751235961914
Epoch 750, val loss: 1.0390851497650146
Epoch 760, training loss: 0.6703438758850098 = 0.008834291249513626 + 0.1 * 6.615095615386963
Epoch 760, val loss: 1.0437027215957642
Epoch 770, training loss: 0.6712547540664673 = 0.008549139834940434 + 0.1 * 6.627056121826172
Epoch 770, val loss: 1.048403263092041
Epoch 780, training loss: 0.6697055101394653 = 0.00827894639223814 + 0.1 * 6.614265441894531
Epoch 780, val loss: 1.0531502962112427
Epoch 790, training loss: 0.6679641008377075 = 0.008023961447179317 + 0.1 * 6.599400997161865
Epoch 790, val loss: 1.0574592351913452
Epoch 800, training loss: 0.6700004935264587 = 0.007781628053635359 + 0.1 * 6.622188568115234
Epoch 800, val loss: 1.0619933605194092
Epoch 810, training loss: 0.6681978702545166 = 0.007551058195531368 + 0.1 * 6.6064677238464355
Epoch 810, val loss: 1.0662449598312378
Epoch 820, training loss: 0.6666855216026306 = 0.0073323436081409454 + 0.1 * 6.593532085418701
Epoch 820, val loss: 1.0704509019851685
Epoch 830, training loss: 0.6683867573738098 = 0.0071234190836548805 + 0.1 * 6.612633228302002
Epoch 830, val loss: 1.074755311012268
Epoch 840, training loss: 0.6653059124946594 = 0.006924956571310759 + 0.1 * 6.583809852600098
Epoch 840, val loss: 1.0788437128067017
Epoch 850, training loss: 0.6653637886047363 = 0.006736286915838718 + 0.1 * 6.586275100708008
Epoch 850, val loss: 1.0826420783996582
Epoch 860, training loss: 0.6641842722892761 = 0.006555269006639719 + 0.1 * 6.576290130615234
Epoch 860, val loss: 1.0868842601776123
Epoch 870, training loss: 0.6639121174812317 = 0.006383550353348255 + 0.1 * 6.575285911560059
Epoch 870, val loss: 1.0905332565307617
Epoch 880, training loss: 0.6637547016143799 = 0.006219423841685057 + 0.1 * 6.575352191925049
Epoch 880, val loss: 1.0942517518997192
Epoch 890, training loss: 0.6627117991447449 = 0.006061552092432976 + 0.1 * 6.566502571105957
Epoch 890, val loss: 1.0980932712554932
Epoch 900, training loss: 0.6641799807548523 = 0.005910282488912344 + 0.1 * 6.582696914672852
Epoch 900, val loss: 1.1019220352172852
Epoch 910, training loss: 0.6619569659233093 = 0.005765753332525492 + 0.1 * 6.5619120597839355
Epoch 910, val loss: 1.1054515838623047
Epoch 920, training loss: 0.6618360280990601 = 0.005627863574773073 + 0.1 * 6.562081336975098
Epoch 920, val loss: 1.1089105606079102
Epoch 930, training loss: 0.6610996723175049 = 0.0054955813102424145 + 0.1 * 6.556040287017822
Epoch 930, val loss: 1.1123099327087402
Epoch 940, training loss: 0.6606846451759338 = 0.005367943085730076 + 0.1 * 6.55316686630249
Epoch 940, val loss: 1.1158668994903564
Epoch 950, training loss: 0.6608636975288391 = 0.0052458313293755054 + 0.1 * 6.556179046630859
Epoch 950, val loss: 1.1191953420639038
Epoch 960, training loss: 0.6600491404533386 = 0.005128163844347 + 0.1 * 6.549210071563721
Epoch 960, val loss: 1.1225018501281738
Epoch 970, training loss: 0.6594688296318054 = 0.005015738774091005 + 0.1 * 6.544530868530273
Epoch 970, val loss: 1.1256486177444458
Epoch 980, training loss: 0.659641683101654 = 0.004907702095806599 + 0.1 * 6.54733943939209
Epoch 980, val loss: 1.1287226676940918
Epoch 990, training loss: 0.6597488522529602 = 0.004803123418241739 + 0.1 * 6.54945707321167
Epoch 990, val loss: 1.1319384574890137
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8218239325250396
The final CL Acc:0.79259, 0.01386, The final GNN Acc:0.82095, 0.00245
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13188])
remove edge: torch.Size([2, 7950])
updated graph: torch.Size([2, 10582])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.803774356842041 = 1.9440927505493164 + 0.1 * 8.596816062927246
Epoch 0, val loss: 1.9404627084732056
Epoch 10, training loss: 2.7941360473632812 = 1.9344642162322998 + 0.1 * 8.596717834472656
Epoch 10, val loss: 1.9304547309875488
Epoch 20, training loss: 2.782209873199463 = 1.9225925207138062 + 0.1 * 8.596173286437988
Epoch 20, val loss: 1.9180898666381836
Epoch 30, training loss: 2.765218496322632 = 1.9060192108154297 + 0.1 * 8.59199333190918
Epoch 30, val loss: 1.9008128643035889
Epoch 40, training loss: 2.737855911254883 = 1.8816053867340088 + 0.1 * 8.562503814697266
Epoch 40, val loss: 1.875415325164795
Epoch 50, training loss: 2.6849522590637207 = 1.8476678133010864 + 0.1 * 8.372844696044922
Epoch 50, val loss: 1.8414491415023804
Epoch 60, training loss: 2.608126640319824 = 1.809875726699829 + 0.1 * 7.982508182525635
Epoch 60, val loss: 1.8064415454864502
Epoch 70, training loss: 2.5301644802093506 = 1.7750444412231445 + 0.1 * 7.551200866699219
Epoch 70, val loss: 1.7756966352462769
Epoch 80, training loss: 2.4659106731414795 = 1.7392339706420898 + 0.1 * 7.266766548156738
Epoch 80, val loss: 1.743835687637329
Epoch 90, training loss: 2.4080960750579834 = 1.6945390701293945 + 0.1 * 7.1355695724487305
Epoch 90, val loss: 1.703473448753357
Epoch 100, training loss: 2.34491229057312 = 1.6352410316467285 + 0.1 * 7.096712112426758
Epoch 100, val loss: 1.6504359245300293
Epoch 110, training loss: 2.266758918762207 = 1.560139775276184 + 0.1 * 7.06619119644165
Epoch 110, val loss: 1.5845788717269897
Epoch 120, training loss: 2.1760330200195312 = 1.4719221591949463 + 0.1 * 7.041109085083008
Epoch 120, val loss: 1.5090807676315308
Epoch 130, training loss: 2.0767717361450195 = 1.375242829322815 + 0.1 * 7.015290260314941
Epoch 130, val loss: 1.4278771877288818
Epoch 140, training loss: 1.9708224534988403 = 1.2720571756362915 + 0.1 * 6.987652778625488
Epoch 140, val loss: 1.342342734336853
Epoch 150, training loss: 1.8591125011444092 = 1.1631156206130981 + 0.1 * 6.959968566894531
Epoch 150, val loss: 1.2538334131240845
Epoch 160, training loss: 1.745223045349121 = 1.0518358945846558 + 0.1 * 6.933872222900391
Epoch 160, val loss: 1.1639153957366943
Epoch 170, training loss: 1.6388405561447144 = 0.9467536807060242 + 0.1 * 6.920868873596191
Epoch 170, val loss: 1.0802710056304932
Epoch 180, training loss: 1.5444469451904297 = 0.8538488149642944 + 0.1 * 6.905981540679932
Epoch 180, val loss: 1.006639003753662
Epoch 190, training loss: 1.463857650756836 = 0.7747541666030884 + 0.1 * 6.891035079956055
Epoch 190, val loss: 0.9449998736381531
Epoch 200, training loss: 1.39829683303833 = 0.7087949514389038 + 0.1 * 6.895018577575684
Epoch 200, val loss: 0.8948721289634705
Epoch 210, training loss: 1.3409674167633057 = 0.6540129780769348 + 0.1 * 6.869543552398682
Epoch 210, val loss: 0.8548386096954346
Epoch 220, training loss: 1.2910571098327637 = 0.6053562164306641 + 0.1 * 6.85700798034668
Epoch 220, val loss: 0.8211410641670227
Epoch 230, training loss: 1.2441648244857788 = 0.5596724152565002 + 0.1 * 6.844923973083496
Epoch 230, val loss: 0.7914189100265503
Epoch 240, training loss: 1.1991279125213623 = 0.5154528021812439 + 0.1 * 6.8367509841918945
Epoch 240, val loss: 0.764435887336731
Epoch 250, training loss: 1.1542186737060547 = 0.4719374179840088 + 0.1 * 6.822812080383301
Epoch 250, val loss: 0.7395963668823242
Epoch 260, training loss: 1.110626459121704 = 0.42927420139312744 + 0.1 * 6.813523292541504
Epoch 260, val loss: 0.7167566418647766
Epoch 270, training loss: 1.069395661354065 = 0.3873746693134308 + 0.1 * 6.820209980010986
Epoch 270, val loss: 0.6959186792373657
Epoch 280, training loss: 1.0267618894577026 = 0.34701046347618103 + 0.1 * 6.797513961791992
Epoch 280, val loss: 0.6774657368659973
Epoch 290, training loss: 0.9868165254592896 = 0.30842337012290955 + 0.1 * 6.783931255340576
Epoch 290, val loss: 0.6615758538246155
Epoch 300, training loss: 0.9510090351104736 = 0.2725076973438263 + 0.1 * 6.785012722015381
Epoch 300, val loss: 0.6487745642662048
Epoch 310, training loss: 0.9171333312988281 = 0.2400525063276291 + 0.1 * 6.770808219909668
Epoch 310, val loss: 0.639281690120697
Epoch 320, training loss: 0.8895642757415771 = 0.21125178039073944 + 0.1 * 6.783124923706055
Epoch 320, val loss: 0.633092999458313
Epoch 330, training loss: 0.8622117042541504 = 0.18639783561229706 + 0.1 * 6.758138179779053
Epoch 330, val loss: 0.6298729777336121
Epoch 340, training loss: 0.8394104242324829 = 0.16492179036140442 + 0.1 * 6.74488639831543
Epoch 340, val loss: 0.6292020678520203
Epoch 350, training loss: 0.8202372789382935 = 0.14636307954788208 + 0.1 * 6.738741874694824
Epoch 350, val loss: 0.6306461095809937
Epoch 360, training loss: 0.8064644932746887 = 0.13041885197162628 + 0.1 * 6.760456562042236
Epoch 360, val loss: 0.6338226199150085
Epoch 370, training loss: 0.7891116142272949 = 0.11673647910356522 + 0.1 * 6.723751544952393
Epoch 370, val loss: 0.6381171345710754
Epoch 380, training loss: 0.7783479690551758 = 0.1048813909292221 + 0.1 * 6.734665393829346
Epoch 380, val loss: 0.6434531211853027
Epoch 390, training loss: 0.7660894393920898 = 0.09461870789527893 + 0.1 * 6.714707374572754
Epoch 390, val loss: 0.6493934392929077
Epoch 400, training loss: 0.7558631300926208 = 0.08563385158777237 + 0.1 * 6.702292442321777
Epoch 400, val loss: 0.6559483408927917
Epoch 410, training loss: 0.7478997707366943 = 0.07775121927261353 + 0.1 * 6.701485633850098
Epoch 410, val loss: 0.6628989577293396
Epoch 420, training loss: 0.7392362952232361 = 0.07082571089267731 + 0.1 * 6.68410587310791
Epoch 420, val loss: 0.6700261831283569
Epoch 430, training loss: 0.7364209890365601 = 0.06469091773033142 + 0.1 * 6.717300891876221
Epoch 430, val loss: 0.6774264574050903
Epoch 440, training loss: 0.7267918586730957 = 0.05927275866270065 + 0.1 * 6.6751909255981445
Epoch 440, val loss: 0.6848907470703125
Epoch 450, training loss: 0.7220426797866821 = 0.05444030091166496 + 0.1 * 6.676023483276367
Epoch 450, val loss: 0.6924091577529907
Epoch 460, training loss: 0.7172589898109436 = 0.050132106989622116 + 0.1 * 6.671268939971924
Epoch 460, val loss: 0.6999826431274414
Epoch 470, training loss: 0.7109704613685608 = 0.046294961124658585 + 0.1 * 6.646754741668701
Epoch 470, val loss: 0.7074106335639954
Epoch 480, training loss: 0.7095817923545837 = 0.04284423589706421 + 0.1 * 6.667375564575195
Epoch 480, val loss: 0.7148187756538391
Epoch 490, training loss: 0.7039293050765991 = 0.03974233940243721 + 0.1 * 6.64186954498291
Epoch 490, val loss: 0.7222198843955994
Epoch 500, training loss: 0.7037058472633362 = 0.03694480285048485 + 0.1 * 6.667610168457031
Epoch 500, val loss: 0.7294828295707703
Epoch 510, training loss: 0.6967424154281616 = 0.03442560136318207 + 0.1 * 6.623167991638184
Epoch 510, val loss: 0.7366918325424194
Epoch 520, training loss: 0.6956102252006531 = 0.03214722499251366 + 0.1 * 6.634629726409912
Epoch 520, val loss: 0.7437093257904053
Epoch 530, training loss: 0.6922523975372314 = 0.03008299134671688 + 0.1 * 6.6216936111450195
Epoch 530, val loss: 0.7507495284080505
Epoch 540, training loss: 0.688892126083374 = 0.028212351724505424 + 0.1 * 6.606797218322754
Epoch 540, val loss: 0.7575094699859619
Epoch 550, training loss: 0.687602162361145 = 0.02650574967265129 + 0.1 * 6.610963821411133
Epoch 550, val loss: 0.7642931342124939
Epoch 560, training loss: 0.6848058104515076 = 0.0249532051384449 + 0.1 * 6.5985260009765625
Epoch 560, val loss: 0.7708896398544312
Epoch 570, training loss: 0.6860299110412598 = 0.023533198982477188 + 0.1 * 6.624966621398926
Epoch 570, val loss: 0.7773693203926086
Epoch 580, training loss: 0.6801614165306091 = 0.022234071046113968 + 0.1 * 6.579273223876953
Epoch 580, val loss: 0.7837517261505127
Epoch 590, training loss: 0.6789147853851318 = 0.021041641011834145 + 0.1 * 6.578731536865234
Epoch 590, val loss: 0.7899620532989502
Epoch 600, training loss: 0.6784107089042664 = 0.019945379346609116 + 0.1 * 6.584653377532959
Epoch 600, val loss: 0.7961764931678772
Epoch 610, training loss: 0.6754361987113953 = 0.018942702561616898 + 0.1 * 6.564934730529785
Epoch 610, val loss: 0.8020569682121277
Epoch 620, training loss: 0.6738556623458862 = 0.01801445335149765 + 0.1 * 6.558412075042725
Epoch 620, val loss: 0.8078163266181946
Epoch 630, training loss: 0.6726868748664856 = 0.017151936888694763 + 0.1 * 6.555349349975586
Epoch 630, val loss: 0.8135395050048828
Epoch 640, training loss: 0.6713871955871582 = 0.016351131722331047 + 0.1 * 6.550360679626465
Epoch 640, val loss: 0.819212019443512
Epoch 650, training loss: 0.6727128624916077 = 0.015610801987349987 + 0.1 * 6.571020603179932
Epoch 650, val loss: 0.8246695399284363
Epoch 660, training loss: 0.6707820892333984 = 0.014923889189958572 + 0.1 * 6.558582305908203
Epoch 660, val loss: 0.8300589323043823
Epoch 670, training loss: 0.6689034700393677 = 0.014285983517765999 + 0.1 * 6.5461745262146
Epoch 670, val loss: 0.8352238535881042
Epoch 680, training loss: 0.6668123006820679 = 0.013688419945538044 + 0.1 * 6.531238555908203
Epoch 680, val loss: 0.84033203125
Epoch 690, training loss: 0.6690563559532166 = 0.01312767993658781 + 0.1 * 6.559286594390869
Epoch 690, val loss: 0.845377504825592
Epoch 700, training loss: 0.6662023663520813 = 0.01260423008352518 + 0.1 * 6.535981178283691
Epoch 700, val loss: 0.8502856492996216
Epoch 710, training loss: 0.6648146510124207 = 0.012114075012505054 + 0.1 * 6.527006149291992
Epoch 710, val loss: 0.8550859093666077
Epoch 720, training loss: 0.6640931367874146 = 0.01165362261235714 + 0.1 * 6.524394989013672
Epoch 720, val loss: 0.8597745895385742
Epoch 730, training loss: 0.664131224155426 = 0.011220870539546013 + 0.1 * 6.5291032791137695
Epoch 730, val loss: 0.8643442392349243
Epoch 740, training loss: 0.6645030975341797 = 0.010814073495566845 + 0.1 * 6.536890029907227
Epoch 740, val loss: 0.8688403367996216
Epoch 750, training loss: 0.6621310710906982 = 0.010430938564240932 + 0.1 * 6.517001152038574
Epoch 750, val loss: 0.8732603788375854
Epoch 760, training loss: 0.6604495644569397 = 0.010070163756608963 + 0.1 * 6.503794193267822
Epoch 760, val loss: 0.8775090575218201
Epoch 770, training loss: 0.6630418300628662 = 0.009727629832923412 + 0.1 * 6.533141613006592
Epoch 770, val loss: 0.8817204236984253
Epoch 780, training loss: 0.6598146557807922 = 0.009404532611370087 + 0.1 * 6.504100799560547
Epoch 780, val loss: 0.8858645558357239
Epoch 790, training loss: 0.6595986485481262 = 0.00909954309463501 + 0.1 * 6.504991054534912
Epoch 790, val loss: 0.8898686170578003
Epoch 800, training loss: 0.6583436727523804 = 0.008809595368802547 + 0.1 * 6.495340824127197
Epoch 800, val loss: 0.8938246965408325
Epoch 810, training loss: 0.6584951877593994 = 0.008534721098840237 + 0.1 * 6.49960470199585
Epoch 810, val loss: 0.8976659178733826
Epoch 820, training loss: 0.6570497155189514 = 0.008273069746792316 + 0.1 * 6.487766265869141
Epoch 820, val loss: 0.9014825224876404
Epoch 830, training loss: 0.6580074429512024 = 0.00802490022033453 + 0.1 * 6.499825477600098
Epoch 830, val loss: 0.905188798904419
Epoch 840, training loss: 0.6564667224884033 = 0.007788311690092087 + 0.1 * 6.486783981323242
Epoch 840, val loss: 0.9088895320892334
Epoch 850, training loss: 0.6556760668754578 = 0.0075645181350409985 + 0.1 * 6.481114864349365
Epoch 850, val loss: 0.9124485850334167
Epoch 860, training loss: 0.6558879017829895 = 0.007350547704845667 + 0.1 * 6.485373020172119
Epoch 860, val loss: 0.9159373641014099
Epoch 870, training loss: 0.6549612283706665 = 0.007146280258893967 + 0.1 * 6.478148937225342
Epoch 870, val loss: 0.9193684458732605
Epoch 880, training loss: 0.6550099849700928 = 0.0069509451277554035 + 0.1 * 6.480589866638184
Epoch 880, val loss: 0.9227555990219116
Epoch 890, training loss: 0.6548737287521362 = 0.00676434813067317 + 0.1 * 6.481093406677246
Epoch 890, val loss: 0.926078736782074
Epoch 900, training loss: 0.653599739074707 = 0.00658628111705184 + 0.1 * 6.470134735107422
Epoch 900, val loss: 0.9293667078018188
Epoch 910, training loss: 0.653927743434906 = 0.0064156739972531796 + 0.1 * 6.475120544433594
Epoch 910, val loss: 0.932569682598114
Epoch 920, training loss: 0.653511106967926 = 0.006252602208405733 + 0.1 * 6.472585201263428
Epoch 920, val loss: 0.9357030391693115
Epoch 930, training loss: 0.6540811657905579 = 0.006096282973885536 + 0.1 * 6.479848384857178
Epoch 930, val loss: 0.9387964606285095
Epoch 940, training loss: 0.6532790660858154 = 0.00594666600227356 + 0.1 * 6.473323822021484
Epoch 940, val loss: 0.941796600818634
Epoch 950, training loss: 0.6537916660308838 = 0.005803045351058245 + 0.1 * 6.479886054992676
Epoch 950, val loss: 0.9447693824768066
Epoch 960, training loss: 0.6527599096298218 = 0.005664807744324207 + 0.1 * 6.470950603485107
Epoch 960, val loss: 0.9477559328079224
Epoch 970, training loss: 0.6514394879341125 = 0.005532551556825638 + 0.1 * 6.45906925201416
Epoch 970, val loss: 0.9505915641784668
Epoch 980, training loss: 0.6522236466407776 = 0.005404741503298283 + 0.1 * 6.468188762664795
Epoch 980, val loss: 0.9534440636634827
Epoch 990, training loss: 0.6513811349868774 = 0.005282312165945768 + 0.1 * 6.460988521575928
Epoch 990, val loss: 0.95625239610672
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.7986412048339844 = 1.9389615058898926 + 0.1 * 8.596796035766602
Epoch 0, val loss: 1.941290020942688
Epoch 10, training loss: 2.7887983322143555 = 1.9291315078735352 + 0.1 * 8.59666919708252
Epoch 10, val loss: 1.9320217370986938
Epoch 20, training loss: 2.776423931121826 = 1.9168319702148438 + 0.1 * 8.595919609069824
Epoch 20, val loss: 1.920028805732727
Epoch 30, training loss: 2.758368492126465 = 1.8994508981704712 + 0.1 * 8.589177131652832
Epoch 30, val loss: 1.9027395248413086
Epoch 40, training loss: 2.7279856204986572 = 1.8738617897033691 + 0.1 * 8.541237831115723
Epoch 40, val loss: 1.8773452043533325
Epoch 50, training loss: 2.6674082279205322 = 1.839231014251709 + 0.1 * 8.281771659851074
Epoch 50, val loss: 1.8441753387451172
Epoch 60, training loss: 2.607605457305908 = 1.8008160591125488 + 0.1 * 8.067893028259277
Epoch 60, val loss: 1.8091977834701538
Epoch 70, training loss: 2.5276808738708496 = 1.7633864879608154 + 0.1 * 7.6429443359375
Epoch 70, val loss: 1.7749550342559814
Epoch 80, training loss: 2.4508416652679443 = 1.7248364686965942 + 0.1 * 7.260052680969238
Epoch 80, val loss: 1.7403064966201782
Epoch 90, training loss: 2.3870010375976562 = 1.6765645742416382 + 0.1 * 7.10436487197876
Epoch 90, val loss: 1.696974515914917
Epoch 100, training loss: 2.3162660598754883 = 1.6125059127807617 + 0.1 * 7.037602424621582
Epoch 100, val loss: 1.639827013015747
Epoch 110, training loss: 2.2348368167877197 = 1.5360536575317383 + 0.1 * 6.987831115722656
Epoch 110, val loss: 1.5756725072860718
Epoch 120, training loss: 2.1488397121429443 = 1.4542262554168701 + 0.1 * 6.946134567260742
Epoch 120, val loss: 1.5078551769256592
Epoch 130, training loss: 2.062045097351074 = 1.370689034461975 + 0.1 * 6.913560390472412
Epoch 130, val loss: 1.4396179914474487
Epoch 140, training loss: 1.974401593208313 = 1.285388469696045 + 0.1 * 6.890130996704102
Epoch 140, val loss: 1.37096107006073
Epoch 150, training loss: 1.885263204574585 = 1.1986052989959717 + 0.1 * 6.866578578948975
Epoch 150, val loss: 1.3012878894805908
Epoch 160, training loss: 1.79521644115448 = 1.11046302318573 + 0.1 * 6.8475341796875
Epoch 160, val loss: 1.231873631477356
Epoch 170, training loss: 1.7084561586380005 = 1.0251396894454956 + 0.1 * 6.833164691925049
Epoch 170, val loss: 1.1662148237228394
Epoch 180, training loss: 1.6269019842147827 = 0.9452285170555115 + 0.1 * 6.816734790802002
Epoch 180, val loss: 1.1056573390960693
Epoch 190, training loss: 1.552331566810608 = 0.8713656067848206 + 0.1 * 6.809659481048584
Epoch 190, val loss: 1.0503383874893188
Epoch 200, training loss: 1.482635736465454 = 0.8036600947380066 + 0.1 * 6.7897562980651855
Epoch 200, val loss: 1.0001509189605713
Epoch 210, training loss: 1.4186031818389893 = 0.7408169507980347 + 0.1 * 6.777862071990967
Epoch 210, val loss: 0.9542864561080933
Epoch 220, training loss: 1.3595165014266968 = 0.6827937960624695 + 0.1 * 6.767226696014404
Epoch 220, val loss: 0.913773238658905
Epoch 230, training loss: 1.3039183616638184 = 0.6292775273323059 + 0.1 * 6.746407985687256
Epoch 230, val loss: 0.8785176277160645
Epoch 240, training loss: 1.2525734901428223 = 0.5794239640235901 + 0.1 * 6.731494426727295
Epoch 240, val loss: 0.848484218120575
Epoch 250, training loss: 1.2046840190887451 = 0.5328854322433472 + 0.1 * 6.717985153198242
Epoch 250, val loss: 0.8238343000411987
Epoch 260, training loss: 1.1616116762161255 = 0.48973289132118225 + 0.1 * 6.718788146972656
Epoch 260, val loss: 0.8046607971191406
Epoch 270, training loss: 1.1197668313980103 = 0.4502517282962799 + 0.1 * 6.695150852203369
Epoch 270, val loss: 0.790869951248169
Epoch 280, training loss: 1.083787441253662 = 0.4144279360771179 + 0.1 * 6.693594932556152
Epoch 280, val loss: 0.781804621219635
Epoch 290, training loss: 1.04983651638031 = 0.3822789192199707 + 0.1 * 6.6755757331848145
Epoch 290, val loss: 0.7770987153053284
Epoch 300, training loss: 1.0202181339263916 = 0.35344231128692627 + 0.1 * 6.667757987976074
Epoch 300, val loss: 0.7759672999382019
Epoch 310, training loss: 0.9936957359313965 = 0.32759812474250793 + 0.1 * 6.660975456237793
Epoch 310, val loss: 0.7781832814216614
Epoch 320, training loss: 0.9717566967010498 = 0.30415958166122437 + 0.1 * 6.675971031188965
Epoch 320, val loss: 0.7830682992935181
Epoch 330, training loss: 0.947598934173584 = 0.28268107771873474 + 0.1 * 6.649178981781006
Epoch 330, val loss: 0.7902835011482239
Epoch 340, training loss: 0.9267667531967163 = 0.2624279856681824 + 0.1 * 6.643387794494629
Epoch 340, val loss: 0.7991957664489746
Epoch 350, training loss: 0.9070469737052917 = 0.24299754202365875 + 0.1 * 6.640494346618652
Epoch 350, val loss: 0.8093920946121216
Epoch 360, training loss: 0.8870381116867065 = 0.2240344136953354 + 0.1 * 6.6300368309021
Epoch 360, val loss: 0.8205124735832214
Epoch 370, training loss: 0.8676149845123291 = 0.20543989539146423 + 0.1 * 6.621751308441162
Epoch 370, val loss: 0.8324404954910278
Epoch 380, training loss: 0.8491657376289368 = 0.18746215105056763 + 0.1 * 6.617035865783691
Epoch 380, val loss: 0.8449863791465759
Epoch 390, training loss: 0.8341938257217407 = 0.17057070136070251 + 0.1 * 6.636230945587158
Epoch 390, val loss: 0.8579887747764587
Epoch 400, training loss: 0.8165277242660522 = 0.15513916313648224 + 0.1 * 6.613885402679443
Epoch 400, val loss: 0.8716860413551331
Epoch 410, training loss: 0.8034627437591553 = 0.14119388163089752 + 0.1 * 6.6226887702941895
Epoch 410, val loss: 0.8858784437179565
Epoch 420, training loss: 0.7895411252975464 = 0.12873445451259613 + 0.1 * 6.608066558837891
Epoch 420, val loss: 0.9005061388015747
Epoch 430, training loss: 0.7772448062896729 = 0.11760655045509338 + 0.1 * 6.5963826179504395
Epoch 430, val loss: 0.9155288338661194
Epoch 440, training loss: 0.7692052125930786 = 0.10766920447349548 + 0.1 * 6.615360260009766
Epoch 440, val loss: 0.9307892322540283
Epoch 450, training loss: 0.7575171589851379 = 0.09883612394332886 + 0.1 * 6.586810111999512
Epoch 450, val loss: 0.9463711380958557
Epoch 460, training loss: 0.7495373487472534 = 0.09092442691326141 + 0.1 * 6.586129188537598
Epoch 460, val loss: 0.961880624294281
Epoch 470, training loss: 0.7419745922088623 = 0.08382662385702133 + 0.1 * 6.581479549407959
Epoch 470, val loss: 0.9773418307304382
Epoch 480, training loss: 0.7348136901855469 = 0.07744631916284561 + 0.1 * 6.573673248291016
Epoch 480, val loss: 0.9928998947143555
Epoch 490, training loss: 0.7296874523162842 = 0.07169101387262344 + 0.1 * 6.5799641609191895
Epoch 490, val loss: 1.0082775354385376
Epoch 500, training loss: 0.7237887382507324 = 0.06649643927812576 + 0.1 * 6.572922706604004
Epoch 500, val loss: 1.023472547531128
Epoch 510, training loss: 0.7181850671768188 = 0.061799824237823486 + 0.1 * 6.563852310180664
Epoch 510, val loss: 1.038354754447937
Epoch 520, training loss: 0.713200032711029 = 0.0575471892952919 + 0.1 * 6.556528091430664
Epoch 520, val loss: 1.0530070066452026
Epoch 530, training loss: 0.7094147801399231 = 0.053678758442401886 + 0.1 * 6.5573601722717285
Epoch 530, val loss: 1.0673714876174927
Epoch 540, training loss: 0.7059085369110107 = 0.05015750601887703 + 0.1 * 6.5575103759765625
Epoch 540, val loss: 1.0814000368118286
Epoch 550, training loss: 0.7020056843757629 = 0.04694996401667595 + 0.1 * 6.550556659698486
Epoch 550, val loss: 1.0951151847839355
Epoch 560, training loss: 0.6976426243782043 = 0.04402044042944908 + 0.1 * 6.536221981048584
Epoch 560, val loss: 1.1084680557250977
Epoch 570, training loss: 0.6959531903266907 = 0.04133472964167595 + 0.1 * 6.546184539794922
Epoch 570, val loss: 1.121626853942871
Epoch 580, training loss: 0.6921384334564209 = 0.03887490555644035 + 0.1 * 6.53263521194458
Epoch 580, val loss: 1.1342391967773438
Epoch 590, training loss: 0.6897725462913513 = 0.036620210856199265 + 0.1 * 6.53152322769165
Epoch 590, val loss: 1.146729588508606
Epoch 600, training loss: 0.6876822710037231 = 0.03454558551311493 + 0.1 * 6.53136682510376
Epoch 600, val loss: 1.1586016416549683
Epoch 610, training loss: 0.6852934956550598 = 0.03263803571462631 + 0.1 * 6.526554584503174
Epoch 610, val loss: 1.170541524887085
Epoch 620, training loss: 0.6827775835990906 = 0.030874161049723625 + 0.1 * 6.519034385681152
Epoch 620, val loss: 1.181976556777954
Epoch 630, training loss: 0.6816753149032593 = 0.029241271317005157 + 0.1 * 6.5243401527404785
Epoch 630, val loss: 1.193177342414856
Epoch 640, training loss: 0.6790748834609985 = 0.027728769928216934 + 0.1 * 6.513460636138916
Epoch 640, val loss: 1.2040808200836182
Epoch 650, training loss: 0.6781576871871948 = 0.026326550170779228 + 0.1 * 6.518311500549316
Epoch 650, val loss: 1.2146815061569214
Epoch 660, training loss: 0.6766552329063416 = 0.0250259917229414 + 0.1 * 6.516292572021484
Epoch 660, val loss: 1.2250508069992065
Epoch 670, training loss: 0.6744005680084229 = 0.02381867915391922 + 0.1 * 6.505818843841553
Epoch 670, val loss: 1.235292673110962
Epoch 680, training loss: 0.6734214425086975 = 0.022693462669849396 + 0.1 * 6.507279872894287
Epoch 680, val loss: 1.2451741695404053
Epoch 690, training loss: 0.6722221374511719 = 0.021645208820700645 + 0.1 * 6.5057692527771
Epoch 690, val loss: 1.254669427871704
Epoch 700, training loss: 0.6715527772903442 = 0.02066788449883461 + 0.1 * 6.508848667144775
Epoch 700, val loss: 1.2642024755477905
Epoch 710, training loss: 0.6700340509414673 = 0.019755268469452858 + 0.1 * 6.502788066864014
Epoch 710, val loss: 1.273270845413208
Epoch 720, training loss: 0.6679503917694092 = 0.01890128292143345 + 0.1 * 6.490490913391113
Epoch 720, val loss: 1.2823418378829956
Epoch 730, training loss: 0.6693933010101318 = 0.01810009777545929 + 0.1 * 6.512931823730469
Epoch 730, val loss: 1.2910435199737549
Epoch 740, training loss: 0.6667205691337585 = 0.01735040917992592 + 0.1 * 6.493701457977295
Epoch 740, val loss: 1.2995914220809937
Epoch 750, training loss: 0.6661584973335266 = 0.01664627157151699 + 0.1 * 6.495121955871582
Epoch 750, val loss: 1.3078432083129883
Epoch 760, training loss: 0.6641992330551147 = 0.01598532125353813 + 0.1 * 6.4821391105651855
Epoch 760, val loss: 1.3161360025405884
Epoch 770, training loss: 0.6641516089439392 = 0.015362145379185677 + 0.1 * 6.487894535064697
Epoch 770, val loss: 1.32415771484375
Epoch 780, training loss: 0.6629553437232971 = 0.014775875955820084 + 0.1 * 6.481794357299805
Epoch 780, val loss: 1.3319785594940186
Epoch 790, training loss: 0.6622341275215149 = 0.01422253716737032 + 0.1 * 6.4801154136657715
Epoch 790, val loss: 1.3395885229110718
Epoch 800, training loss: 0.6614784598350525 = 0.013701419346034527 + 0.1 * 6.47776985168457
Epoch 800, val loss: 1.3472312688827515
Epoch 810, training loss: 0.662025511264801 = 0.013208270072937012 + 0.1 * 6.4881720542907715
Epoch 810, val loss: 1.3545081615447998
Epoch 820, training loss: 0.6597285866737366 = 0.012742888182401657 + 0.1 * 6.4698567390441895
Epoch 820, val loss: 1.3618278503417969
Epoch 830, training loss: 0.6597043871879578 = 0.012301984243094921 + 0.1 * 6.474023818969727
Epoch 830, val loss: 1.3688321113586426
Epoch 840, training loss: 0.6593688130378723 = 0.011885027401149273 + 0.1 * 6.474837303161621
Epoch 840, val loss: 1.3756603002548218
Epoch 850, training loss: 0.6581593751907349 = 0.011490236967802048 + 0.1 * 6.466691493988037
Epoch 850, val loss: 1.3825606107711792
Epoch 860, training loss: 0.6570292115211487 = 0.011115201748907566 + 0.1 * 6.459139823913574
Epoch 860, val loss: 1.389195442199707
Epoch 870, training loss: 0.6567826271057129 = 0.010758815333247185 + 0.1 * 6.460237979888916
Epoch 870, val loss: 1.3959031105041504
Epoch 880, training loss: 0.6572775840759277 = 0.01041929516941309 + 0.1 * 6.468582630157471
Epoch 880, val loss: 1.4021629095077515
Epoch 890, training loss: 0.6579246520996094 = 0.01009790226817131 + 0.1 * 6.478267192840576
Epoch 890, val loss: 1.4084925651550293
Epoch 900, training loss: 0.6554514765739441 = 0.009791629388928413 + 0.1 * 6.456598281860352
Epoch 900, val loss: 1.4146126508712769
Epoch 910, training loss: 0.6560869812965393 = 0.009500415064394474 + 0.1 * 6.465865135192871
Epoch 910, val loss: 1.4208059310913086
Epoch 920, training loss: 0.6551660299301147 = 0.00922271329909563 + 0.1 * 6.459433078765869
Epoch 920, val loss: 1.426712155342102
Epoch 930, training loss: 0.6537936925888062 = 0.008958056569099426 + 0.1 * 6.4483561515808105
Epoch 930, val loss: 1.4325860738754272
Epoch 940, training loss: 0.6546090245246887 = 0.008705275133252144 + 0.1 * 6.459037780761719
Epoch 940, val loss: 1.4384340047836304
Epoch 950, training loss: 0.6533359289169312 = 0.008463394828140736 + 0.1 * 6.44872522354126
Epoch 950, val loss: 1.4439380168914795
Epoch 960, training loss: 0.6535313129425049 = 0.008232397958636284 + 0.1 * 6.452989101409912
Epoch 960, val loss: 1.4495145082473755
Epoch 970, training loss: 0.6521657109260559 = 0.008011523634195328 + 0.1 * 6.44154167175293
Epoch 970, val loss: 1.45492422580719
Epoch 980, training loss: 0.6532014012336731 = 0.007800013292580843 + 0.1 * 6.454013347625732
Epoch 980, val loss: 1.4603582620620728
Epoch 990, training loss: 0.652727484703064 = 0.0075974296778440475 + 0.1 * 6.451300144195557
Epoch 990, val loss: 1.465300440788269
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.8108673095703125 = 1.9511903524398804 + 0.1 * 8.596770286560059
Epoch 0, val loss: 1.954293966293335
Epoch 10, training loss: 2.8007519245147705 = 1.9410946369171143 + 0.1 * 8.596571922302246
Epoch 10, val loss: 1.943328857421875
Epoch 20, training loss: 2.7878401279449463 = 1.9283171892166138 + 0.1 * 8.595230102539062
Epoch 20, val loss: 1.9291528463363647
Epoch 30, training loss: 2.7683370113372803 = 1.9098994731903076 + 0.1 * 8.58437442779541
Epoch 30, val loss: 1.9084619283676147
Epoch 40, training loss: 2.73520565032959 = 1.8824260234832764 + 0.1 * 8.527796745300293
Epoch 40, val loss: 1.8780345916748047
Epoch 50, training loss: 2.6762893199920654 = 1.8460525274276733 + 0.1 * 8.302367210388184
Epoch 50, val loss: 1.8403505086898804
Epoch 60, training loss: 2.614407777786255 = 1.8065825700759888 + 0.1 * 8.078252792358398
Epoch 60, val loss: 1.8035780191421509
Epoch 70, training loss: 2.5320518016815186 = 1.7726656198501587 + 0.1 * 7.593862533569336
Epoch 70, val loss: 1.7758609056472778
Epoch 80, training loss: 2.4628326892852783 = 1.7379655838012695 + 0.1 * 7.248671531677246
Epoch 80, val loss: 1.7470486164093018
Epoch 90, training loss: 2.4061453342437744 = 1.6932164430618286 + 0.1 * 7.129288196563721
Epoch 90, val loss: 1.7088059186935425
Epoch 100, training loss: 2.3413584232330322 = 1.6337143182754517 + 0.1 * 7.076441287994385
Epoch 100, val loss: 1.6585719585418701
Epoch 110, training loss: 2.262568712234497 = 1.5593986511230469 + 0.1 * 7.031700611114502
Epoch 110, val loss: 1.5964465141296387
Epoch 120, training loss: 2.1753573417663574 = 1.4764697551727295 + 0.1 * 6.988876819610596
Epoch 120, val loss: 1.5293502807617188
Epoch 130, training loss: 2.0856711864471436 = 1.3905161619186401 + 0.1 * 6.951549530029297
Epoch 130, val loss: 1.4623576402664185
Epoch 140, training loss: 1.996647834777832 = 1.3042465448379517 + 0.1 * 6.924012660980225
Epoch 140, val loss: 1.3970420360565186
Epoch 150, training loss: 1.9071402549743652 = 1.2171683311462402 + 0.1 * 6.899718284606934
Epoch 150, val loss: 1.331668496131897
Epoch 160, training loss: 1.8163208961486816 = 1.1283257007598877 + 0.1 * 6.879952430725098
Epoch 160, val loss: 1.2647929191589355
Epoch 170, training loss: 1.7263484001159668 = 1.0411510467529297 + 0.1 * 6.851973533630371
Epoch 170, val loss: 1.2002191543579102
Epoch 180, training loss: 1.6414802074432373 = 0.9576183557510376 + 0.1 * 6.838618755340576
Epoch 180, val loss: 1.1393225193023682
Epoch 190, training loss: 1.5618072748184204 = 0.8799057006835938 + 0.1 * 6.8190155029296875
Epoch 190, val loss: 1.0835092067718506
Epoch 200, training loss: 1.4885296821594238 = 0.808075487613678 + 0.1 * 6.80454158782959
Epoch 200, val loss: 1.0325618982315063
Epoch 210, training loss: 1.4226405620574951 = 0.7436192631721497 + 0.1 * 6.790212631225586
Epoch 210, val loss: 0.9869869947433472
Epoch 220, training loss: 1.3641560077667236 = 0.6863560080528259 + 0.1 * 6.7779998779296875
Epoch 220, val loss: 0.947239875793457
Epoch 230, training loss: 1.3113999366760254 = 0.6343366503715515 + 0.1 * 6.770633220672607
Epoch 230, val loss: 0.9119247794151306
Epoch 240, training loss: 1.2629504203796387 = 0.5864757299423218 + 0.1 * 6.764747142791748
Epoch 240, val loss: 0.8811190724372864
Epoch 250, training loss: 1.215576171875 = 0.5411142706871033 + 0.1 * 6.744619369506836
Epoch 250, val loss: 0.8535836935043335
Epoch 260, training loss: 1.1708407402038574 = 0.49710896611213684 + 0.1 * 6.7373175621032715
Epoch 260, val loss: 0.828709602355957
Epoch 270, training loss: 1.1280900239944458 = 0.45476216077804565 + 0.1 * 6.733278751373291
Epoch 270, val loss: 0.8066141605377197
Epoch 280, training loss: 1.086327075958252 = 0.4143768846988678 + 0.1 * 6.719501495361328
Epoch 280, val loss: 0.7874739170074463
Epoch 290, training loss: 1.0486000776290894 = 0.3764744699001312 + 0.1 * 6.721256256103516
Epoch 290, val loss: 0.7716423869132996
Epoch 300, training loss: 1.011927604675293 = 0.34133395552635193 + 0.1 * 6.705936431884766
Epoch 300, val loss: 0.759227454662323
Epoch 310, training loss: 0.9809800386428833 = 0.3089791238307953 + 0.1 * 6.7200093269348145
Epoch 310, val loss: 0.7500452399253845
Epoch 320, training loss: 0.9484367966651917 = 0.2794116735458374 + 0.1 * 6.690251350402832
Epoch 320, val loss: 0.74387526512146
Epoch 330, training loss: 0.9202154874801636 = 0.2522144019603729 + 0.1 * 6.6800103187561035
Epoch 330, val loss: 0.7402650117874146
Epoch 340, training loss: 0.8957386016845703 = 0.22736027836799622 + 0.1 * 6.683783531188965
Epoch 340, val loss: 0.7387363314628601
Epoch 350, training loss: 0.8718893527984619 = 0.20486673712730408 + 0.1 * 6.670226097106934
Epoch 350, val loss: 0.7392027974128723
Epoch 360, training loss: 0.8504316210746765 = 0.18453848361968994 + 0.1 * 6.658931255340576
Epoch 360, val loss: 0.7413320541381836
Epoch 370, training loss: 0.8334085941314697 = 0.16630925238132477 + 0.1 * 6.670993328094482
Epoch 370, val loss: 0.7449296116828918
Epoch 380, training loss: 0.8150346279144287 = 0.1501387506723404 + 0.1 * 6.648958683013916
Epoch 380, val loss: 0.7496659159660339
Epoch 390, training loss: 0.8020267486572266 = 0.135757714509964 + 0.1 * 6.66269063949585
Epoch 390, val loss: 0.7554084062576294
Epoch 400, training loss: 0.7877517342567444 = 0.12306036800146103 + 0.1 * 6.646914005279541
Epoch 400, val loss: 0.7619345188140869
Epoch 410, training loss: 0.774883508682251 = 0.11180176585912704 + 0.1 * 6.630817413330078
Epoch 410, val loss: 0.7690916061401367
Epoch 420, training loss: 0.7672138214111328 = 0.10178014636039734 + 0.1 * 6.654336452484131
Epoch 420, val loss: 0.7767464518547058
Epoch 430, training loss: 0.7549266815185547 = 0.09289563447237015 + 0.1 * 6.620310306549072
Epoch 430, val loss: 0.78472900390625
Epoch 440, training loss: 0.7463764548301697 = 0.08496681600809097 + 0.1 * 6.614096164703369
Epoch 440, val loss: 0.7928595542907715
Epoch 450, training loss: 0.7383700609207153 = 0.07787053287029266 + 0.1 * 6.604995250701904
Epoch 450, val loss: 0.8011165261268616
Epoch 460, training loss: 0.732659101486206 = 0.07150457799434662 + 0.1 * 6.611545085906982
Epoch 460, val loss: 0.8094462752342224
Epoch 470, training loss: 0.7256330847740173 = 0.06579006463289261 + 0.1 * 6.598430156707764
Epoch 470, val loss: 0.8177489042282104
Epoch 480, training loss: 0.7208104729652405 = 0.06066286563873291 + 0.1 * 6.601475715637207
Epoch 480, val loss: 0.825920820236206
Epoch 490, training loss: 0.7145089507102966 = 0.05604616552591324 + 0.1 * 6.584627628326416
Epoch 490, val loss: 0.8340994715690613
Epoch 500, training loss: 0.70992112159729 = 0.051872629672288895 + 0.1 * 6.580484867095947
Epoch 500, val loss: 0.842140793800354
Epoch 510, training loss: 0.7073572278022766 = 0.048103783279657364 + 0.1 * 6.592534065246582
Epoch 510, val loss: 0.8501268625259399
Epoch 520, training loss: 0.701521098613739 = 0.04471159726381302 + 0.1 * 6.568094730377197
Epoch 520, val loss: 0.8578643798828125
Epoch 530, training loss: 0.6979236006736755 = 0.04163840040564537 + 0.1 * 6.562851905822754
Epoch 530, val loss: 0.8654717803001404
Epoch 540, training loss: 0.696885347366333 = 0.03884342685341835 + 0.1 * 6.580418586730957
Epoch 540, val loss: 0.8729209899902344
Epoch 550, training loss: 0.6919213533401489 = 0.03630462661385536 + 0.1 * 6.556167125701904
Epoch 550, val loss: 0.8802480101585388
Epoch 560, training loss: 0.6893018484115601 = 0.033988915383815765 + 0.1 * 6.55312967300415
Epoch 560, val loss: 0.8874276280403137
Epoch 570, training loss: 0.6874274611473083 = 0.03187847137451172 + 0.1 * 6.555490016937256
Epoch 570, val loss: 0.8943833112716675
Epoch 580, training loss: 0.6842529773712158 = 0.029958708211779594 + 0.1 * 6.542942523956299
Epoch 580, val loss: 0.9012517333030701
Epoch 590, training loss: 0.6834185719490051 = 0.028201207518577576 + 0.1 * 6.552173137664795
Epoch 590, val loss: 0.907925009727478
Epoch 600, training loss: 0.6801103949546814 = 0.026591608300805092 + 0.1 * 6.535187721252441
Epoch 600, val loss: 0.9144390821456909
Epoch 610, training loss: 0.679035484790802 = 0.025112876668572426 + 0.1 * 6.539226055145264
Epoch 610, val loss: 0.9208744764328003
Epoch 620, training loss: 0.6772931218147278 = 0.023752139881253242 + 0.1 * 6.535409450531006
Epoch 620, val loss: 0.9271562695503235
Epoch 630, training loss: 0.6743400692939758 = 0.02249847538769245 + 0.1 * 6.518415927886963
Epoch 630, val loss: 0.9332616925239563
Epoch 640, training loss: 0.6762422919273376 = 0.021339915692806244 + 0.1 * 6.549023628234863
Epoch 640, val loss: 0.9392662048339844
Epoch 650, training loss: 0.671757698059082 = 0.020271558314561844 + 0.1 * 6.514861583709717
Epoch 650, val loss: 0.9451482892036438
Epoch 660, training loss: 0.6718805432319641 = 0.019284052774310112 + 0.1 * 6.525964736938477
Epoch 660, val loss: 0.95090252161026
Epoch 670, training loss: 0.670501172542572 = 0.018367614597082138 + 0.1 * 6.521335601806641
Epoch 670, val loss: 0.9564399123191833
Epoch 680, training loss: 0.6683332920074463 = 0.017516423016786575 + 0.1 * 6.508168697357178
Epoch 680, val loss: 0.9619061350822449
Epoch 690, training loss: 0.6678484082221985 = 0.016724975779652596 + 0.1 * 6.511233806610107
Epoch 690, val loss: 0.9672725200653076
Epoch 700, training loss: 0.665740966796875 = 0.0159858837723732 + 0.1 * 6.4975504875183105
Epoch 700, val loss: 0.9724925756454468
Epoch 710, training loss: 0.6675223112106323 = 0.015295758843421936 + 0.1 * 6.522265434265137
Epoch 710, val loss: 0.9775592088699341
Epoch 720, training loss: 0.6654320359230042 = 0.014652715995907784 + 0.1 * 6.507792949676514
Epoch 720, val loss: 0.9825102686882019
Epoch 730, training loss: 0.6634396314620972 = 0.014052066020667553 + 0.1 * 6.493875026702881
Epoch 730, val loss: 0.9873693585395813
Epoch 740, training loss: 0.6636582016944885 = 0.013487199321389198 + 0.1 * 6.501709938049316
Epoch 740, val loss: 0.9921350479125977
Epoch 750, training loss: 0.6616535186767578 = 0.012958241626620293 + 0.1 * 6.486952781677246
Epoch 750, val loss: 0.9967467188835144
Epoch 760, training loss: 0.6630489826202393 = 0.012461003847420216 + 0.1 * 6.5058794021606445
Epoch 760, val loss: 1.0012727975845337
Epoch 770, training loss: 0.6610729694366455 = 0.01199268363416195 + 0.1 * 6.490802764892578
Epoch 770, val loss: 1.005677580833435
Epoch 780, training loss: 0.6610795855522156 = 0.01155233196914196 + 0.1 * 6.495272636413574
Epoch 780, val loss: 1.0100146532058716
Epoch 790, training loss: 0.6598304510116577 = 0.011136227287352085 + 0.1 * 6.486942291259766
Epoch 790, val loss: 1.0142353773117065
Epoch 800, training loss: 0.6604387164115906 = 0.01074433047324419 + 0.1 * 6.496943473815918
Epoch 800, val loss: 1.018347144126892
Epoch 810, training loss: 0.6577844023704529 = 0.010373861528933048 + 0.1 * 6.474104881286621
Epoch 810, val loss: 1.022434949874878
Epoch 820, training loss: 0.6565833687782288 = 0.010024054907262325 + 0.1 * 6.465592861175537
Epoch 820, val loss: 1.0263818502426147
Epoch 830, training loss: 0.6580209136009216 = 0.009691460989415646 + 0.1 * 6.483294486999512
Epoch 830, val loss: 1.030291199684143
Epoch 840, training loss: 0.656362771987915 = 0.0093761021271348 + 0.1 * 6.4698662757873535
Epoch 840, val loss: 1.034088373184204
Epoch 850, training loss: 0.6559568643569946 = 0.009078129194676876 + 0.1 * 6.46878719329834
Epoch 850, val loss: 1.0378248691558838
Epoch 860, training loss: 0.6547786593437195 = 0.008794569410383701 + 0.1 * 6.459841251373291
Epoch 860, val loss: 1.0414971113204956
Epoch 870, training loss: 0.6563674807548523 = 0.008524760603904724 + 0.1 * 6.478426933288574
Epoch 870, val loss: 1.045119047164917
Epoch 880, training loss: 0.6546972990036011 = 0.008268284611403942 + 0.1 * 6.464289665222168
Epoch 880, val loss: 1.0486468076705933
Epoch 890, training loss: 0.654207706451416 = 0.0080240024253726 + 0.1 * 6.461837291717529
Epoch 890, val loss: 1.0521583557128906
Epoch 900, training loss: 0.6543293595314026 = 0.007790924981236458 + 0.1 * 6.465384483337402
Epoch 900, val loss: 1.0555843114852905
Epoch 910, training loss: 0.6541116833686829 = 0.007568421773612499 + 0.1 * 6.465432643890381
Epoch 910, val loss: 1.0588716268539429
Epoch 920, training loss: 0.6523428559303284 = 0.0073569887317717075 + 0.1 * 6.44985818862915
Epoch 920, val loss: 1.062139630317688
Epoch 930, training loss: 0.6533999443054199 = 0.007155462168157101 + 0.1 * 6.46244478225708
Epoch 930, val loss: 1.065321683883667
Epoch 940, training loss: 0.6515028476715088 = 0.006962140556424856 + 0.1 * 6.445406913757324
Epoch 940, val loss: 1.0684531927108765
Epoch 950, training loss: 0.6523348689079285 = 0.006777169648557901 + 0.1 * 6.4555768966674805
Epoch 950, val loss: 1.0715376138687134
Epoch 960, training loss: 0.652338981628418 = 0.006599965039640665 + 0.1 * 6.457390308380127
Epoch 960, val loss: 1.074569582939148
Epoch 970, training loss: 0.6516568064689636 = 0.00643141008913517 + 0.1 * 6.452253818511963
Epoch 970, val loss: 1.0774991512298584
Epoch 980, training loss: 0.6508827209472656 = 0.006269745994359255 + 0.1 * 6.446129322052002
Epoch 980, val loss: 1.0803955793380737
Epoch 990, training loss: 0.6508544087409973 = 0.006114406976848841 + 0.1 * 6.447399616241455
Epoch 990, val loss: 1.0832725763320923
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8408012651555088
The final CL Acc:0.79630, 0.02181, The final GNN Acc:0.83817, 0.00215
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10490])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8201804161071777 = 1.9604995250701904 + 0.1 * 8.596807479858398
Epoch 0, val loss: 1.9627832174301147
Epoch 10, training loss: 2.8093156814575195 = 1.9496443271636963 + 0.1 * 8.596713066101074
Epoch 10, val loss: 1.9520121812820435
Epoch 20, training loss: 2.796005964279175 = 1.93639075756073 + 0.1 * 8.596151351928711
Epoch 20, val loss: 1.9383338689804077
Epoch 30, training loss: 2.7768895626068115 = 1.9177169799804688 + 0.1 * 8.591726303100586
Epoch 30, val loss: 1.9186174869537354
Epoch 40, training loss: 2.746500015258789 = 1.8901128768920898 + 0.1 * 8.563871383666992
Epoch 40, val loss: 1.8893601894378662
Epoch 50, training loss: 2.6949939727783203 = 1.852859377861023 + 0.1 * 8.421344757080078
Epoch 50, val loss: 1.8515397310256958
Epoch 60, training loss: 2.6370081901550293 = 1.813882827758789 + 0.1 * 8.231254577636719
Epoch 60, val loss: 1.8153276443481445
Epoch 70, training loss: 2.5724964141845703 = 1.7819523811340332 + 0.1 * 7.905440807342529
Epoch 70, val loss: 1.7874177694320679
Epoch 80, training loss: 2.4979634284973145 = 1.7495591640472412 + 0.1 * 7.484041690826416
Epoch 80, val loss: 1.7584152221679688
Epoch 90, training loss: 2.434786558151245 = 1.7100164890289307 + 0.1 * 7.247699737548828
Epoch 90, val loss: 1.7240350246429443
Epoch 100, training loss: 2.3707234859466553 = 1.6564058065414429 + 0.1 * 7.143176078796387
Epoch 100, val loss: 1.6771924495697021
Epoch 110, training loss: 2.2971668243408203 = 1.5894688367843628 + 0.1 * 7.076981067657471
Epoch 110, val loss: 1.6193972826004028
Epoch 120, training loss: 2.218874454498291 = 1.5164121389389038 + 0.1 * 7.024622917175293
Epoch 120, val loss: 1.558752417564392
Epoch 130, training loss: 2.1428723335266113 = 1.4436477422714233 + 0.1 * 6.992246627807617
Epoch 130, val loss: 1.5011157989501953
Epoch 140, training loss: 2.0691137313842773 = 1.3720617294311523 + 0.1 * 6.970520496368408
Epoch 140, val loss: 1.4472428560256958
Epoch 150, training loss: 1.9966304302215576 = 1.3012639284133911 + 0.1 * 6.953665733337402
Epoch 150, val loss: 1.3958065509796143
Epoch 160, training loss: 1.9204909801483154 = 1.226991057395935 + 0.1 * 6.934998512268066
Epoch 160, val loss: 1.342460036277771
Epoch 170, training loss: 1.8393828868865967 = 1.1477704048156738 + 0.1 * 6.916123867034912
Epoch 170, val loss: 1.2864634990692139
Epoch 180, training loss: 1.754410982131958 = 1.0645558834075928 + 0.1 * 6.8985514640808105
Epoch 180, val loss: 1.2290763854980469
Epoch 190, training loss: 1.6679556369781494 = 0.9798175692558289 + 0.1 * 6.881380081176758
Epoch 190, val loss: 1.1714513301849365
Epoch 200, training loss: 1.5847302675247192 = 0.8980699181556702 + 0.1 * 6.866603374481201
Epoch 200, val loss: 1.1176036596298218
Epoch 210, training loss: 1.508953332901001 = 0.8237271904945374 + 0.1 * 6.852261066436768
Epoch 210, val loss: 1.071226954460144
Epoch 220, training loss: 1.4440100193023682 = 0.7592463493347168 + 0.1 * 6.847637176513672
Epoch 220, val loss: 1.0351849794387817
Epoch 230, training loss: 1.3868234157562256 = 0.7044985294342041 + 0.1 * 6.823249340057373
Epoch 230, val loss: 1.009081482887268
Epoch 240, training loss: 1.3384474515914917 = 0.6571601629257202 + 0.1 * 6.812872886657715
Epoch 240, val loss: 0.9908430576324463
Epoch 250, training loss: 1.2956440448760986 = 0.6154699325561523 + 0.1 * 6.801741123199463
Epoch 250, val loss: 0.9787307381629944
Epoch 260, training loss: 1.2572699785232544 = 0.5779024362564087 + 0.1 * 6.793675422668457
Epoch 260, val loss: 0.9709118008613586
Epoch 270, training loss: 1.221372365951538 = 0.5435535311698914 + 0.1 * 6.778188705444336
Epoch 270, val loss: 0.9666020274162292
Epoch 280, training loss: 1.1888787746429443 = 0.5118615031242371 + 0.1 * 6.770173072814941
Epoch 280, val loss: 0.9651141166687012
Epoch 290, training loss: 1.1582599878311157 = 0.4822414219379425 + 0.1 * 6.760185241699219
Epoch 290, val loss: 0.9660332798957825
Epoch 300, training loss: 1.1298630237579346 = 0.454157292842865 + 0.1 * 6.757056713104248
Epoch 300, val loss: 0.9684062600135803
Epoch 310, training loss: 1.101315975189209 = 0.42716729640960693 + 0.1 * 6.741486549377441
Epoch 310, val loss: 0.9716932773590088
Epoch 320, training loss: 1.0753051042556763 = 0.4006384313106537 + 0.1 * 6.746666431427002
Epoch 320, val loss: 0.9752387404441833
Epoch 330, training loss: 1.046861171722412 = 0.37410715222358704 + 0.1 * 6.727539539337158
Epoch 330, val loss: 0.9783340096473694
Epoch 340, training loss: 1.0196316242218018 = 0.34696727991104126 + 0.1 * 6.726643085479736
Epoch 340, val loss: 0.9811797142028809
Epoch 350, training loss: 0.9908547401428223 = 0.31938499212265015 + 0.1 * 6.714697360992432
Epoch 350, val loss: 0.9839417338371277
Epoch 360, training loss: 0.9635632634162903 = 0.29197847843170166 + 0.1 * 6.715847492218018
Epoch 360, val loss: 0.9874387383460999
Epoch 370, training loss: 0.9362779259681702 = 0.265644907951355 + 0.1 * 6.706330299377441
Epoch 370, val loss: 0.9922596216201782
Epoch 380, training loss: 0.9109221696853638 = 0.2411283552646637 + 0.1 * 6.697937488555908
Epoch 380, val loss: 0.9987117052078247
Epoch 390, training loss: 0.8890385627746582 = 0.21884098649024963 + 0.1 * 6.701975345611572
Epoch 390, val loss: 1.0067740678787231
Epoch 400, training loss: 0.868162989616394 = 0.19876936078071594 + 0.1 * 6.693936347961426
Epoch 400, val loss: 1.0161012411117554
Epoch 410, training loss: 0.8497364521026611 = 0.18068841099739075 + 0.1 * 6.690480709075928
Epoch 410, val loss: 1.0265159606933594
Epoch 420, training loss: 0.832007646560669 = 0.16437700390815735 + 0.1 * 6.676306247711182
Epoch 420, val loss: 1.0376648902893066
Epoch 430, training loss: 0.8164615631103516 = 0.14965704083442688 + 0.1 * 6.6680450439453125
Epoch 430, val loss: 1.0493066310882568
Epoch 440, training loss: 0.803020179271698 = 0.13639265298843384 + 0.1 * 6.6662750244140625
Epoch 440, val loss: 1.061229944229126
Epoch 450, training loss: 0.7927199006080627 = 0.12443376332521439 + 0.1 * 6.682861328125
Epoch 450, val loss: 1.0733152627944946
Epoch 460, training loss: 0.7798999547958374 = 0.11371966451406479 + 0.1 * 6.661803245544434
Epoch 460, val loss: 1.0854178667068481
Epoch 470, training loss: 0.7682042717933655 = 0.1041177362203598 + 0.1 * 6.640864849090576
Epoch 470, val loss: 1.0975030660629272
Epoch 480, training loss: 0.7605212330818176 = 0.09547577798366547 + 0.1 * 6.650454521179199
Epoch 480, val loss: 1.1095601320266724
Epoch 490, training loss: 0.7512243986129761 = 0.08771196007728577 + 0.1 * 6.635124683380127
Epoch 490, val loss: 1.1214721202850342
Epoch 500, training loss: 0.7439160346984863 = 0.08072451502084732 + 0.1 * 6.6319146156311035
Epoch 500, val loss: 1.1332263946533203
Epoch 510, training loss: 0.7362473011016846 = 0.07443241029977798 + 0.1 * 6.6181488037109375
Epoch 510, val loss: 1.1448147296905518
Epoch 520, training loss: 0.7298372983932495 = 0.06877532601356506 + 0.1 * 6.610619068145752
Epoch 520, val loss: 1.1561049222946167
Epoch 530, training loss: 0.7241531610488892 = 0.06367373466491699 + 0.1 * 6.604794025421143
Epoch 530, val loss: 1.1670385599136353
Epoch 540, training loss: 0.7189518213272095 = 0.05904384329915047 + 0.1 * 6.5990800857543945
Epoch 540, val loss: 1.177835464477539
Epoch 550, training loss: 0.7140867114067078 = 0.054838888347148895 + 0.1 * 6.592478275299072
Epoch 550, val loss: 1.1884922981262207
Epoch 560, training loss: 0.7116091251373291 = 0.05102429911494255 + 0.1 * 6.6058478355407715
Epoch 560, val loss: 1.198879599571228
Epoch 570, training loss: 0.7071527242660522 = 0.047566331923007965 + 0.1 * 6.5958638191223145
Epoch 570, val loss: 1.2089951038360596
Epoch 580, training loss: 0.7029188871383667 = 0.044424258172512054 + 0.1 * 6.584946155548096
Epoch 580, val loss: 1.218775749206543
Epoch 590, training loss: 0.699950098991394 = 0.04155716300010681 + 0.1 * 6.583929538726807
Epoch 590, val loss: 1.2283886671066284
Epoch 600, training loss: 0.6961876153945923 = 0.03893999755382538 + 0.1 * 6.572475910186768
Epoch 600, val loss: 1.237733006477356
Epoch 610, training loss: 0.6945662498474121 = 0.03653956949710846 + 0.1 * 6.580266952514648
Epoch 610, val loss: 1.2469582557678223
Epoch 620, training loss: 0.6913436651229858 = 0.034347619861364365 + 0.1 * 6.569960594177246
Epoch 620, val loss: 1.2559566497802734
Epoch 630, training loss: 0.6886053085327148 = 0.03233503922820091 + 0.1 * 6.562702655792236
Epoch 630, val loss: 1.2647303342819214
Epoch 640, training loss: 0.6862982511520386 = 0.030489612370729446 + 0.1 * 6.558086395263672
Epoch 640, val loss: 1.2734073400497437
Epoch 650, training loss: 0.6843695044517517 = 0.028794890269637108 + 0.1 * 6.555746078491211
Epoch 650, val loss: 1.281787395477295
Epoch 660, training loss: 0.6840670704841614 = 0.027232250198721886 + 0.1 * 6.568348407745361
Epoch 660, val loss: 1.2901384830474854
Epoch 670, training loss: 0.6806051731109619 = 0.025792323052883148 + 0.1 * 6.548128604888916
Epoch 670, val loss: 1.298251748085022
Epoch 680, training loss: 0.6797568798065186 = 0.024462001398205757 + 0.1 * 6.552948951721191
Epoch 680, val loss: 1.306319236755371
Epoch 690, training loss: 0.6784139275550842 = 0.02322910912334919 + 0.1 * 6.5518479347229
Epoch 690, val loss: 1.3141639232635498
Epoch 700, training loss: 0.6755536198616028 = 0.02208864875137806 + 0.1 * 6.534649848937988
Epoch 700, val loss: 1.3218326568603516
Epoch 710, training loss: 0.6759102940559387 = 0.021029356867074966 + 0.1 * 6.548809051513672
Epoch 710, val loss: 1.3294166326522827
Epoch 720, training loss: 0.6741567254066467 = 0.02004341594874859 + 0.1 * 6.541132926940918
Epoch 720, val loss: 1.336888313293457
Epoch 730, training loss: 0.6733424067497253 = 0.019125694409012794 + 0.1 * 6.542166709899902
Epoch 730, val loss: 1.3441368341445923
Epoch 740, training loss: 0.6714658141136169 = 0.018267206847667694 + 0.1 * 6.531985759735107
Epoch 740, val loss: 1.3513044118881226
Epoch 750, training loss: 0.67018723487854 = 0.01746806502342224 + 0.1 * 6.527191638946533
Epoch 750, val loss: 1.3584163188934326
Epoch 760, training loss: 0.6690102219581604 = 0.01672305166721344 + 0.1 * 6.522871971130371
Epoch 760, val loss: 1.3653572797775269
Epoch 770, training loss: 0.6674851775169373 = 0.016024533659219742 + 0.1 * 6.514606475830078
Epoch 770, val loss: 1.372195839881897
Epoch 780, training loss: 0.6675106883049011 = 0.015373026952147484 + 0.1 * 6.521376132965088
Epoch 780, val loss: 1.3788032531738281
Epoch 790, training loss: 0.6661470532417297 = 0.014761406928300858 + 0.1 * 6.513856410980225
Epoch 790, val loss: 1.3853224515914917
Epoch 800, training loss: 0.6667717099189758 = 0.014188517816364765 + 0.1 * 6.525831699371338
Epoch 800, val loss: 1.3917285203933716
Epoch 810, training loss: 0.6639806628227234 = 0.013648761436343193 + 0.1 * 6.503319263458252
Epoch 810, val loss: 1.3980175256729126
Epoch 820, training loss: 0.664773166179657 = 0.013141565024852753 + 0.1 * 6.516315937042236
Epoch 820, val loss: 1.4041260480880737
Epoch 830, training loss: 0.6644814610481262 = 0.012662318535149097 + 0.1 * 6.518191814422607
Epoch 830, val loss: 1.4102766513824463
Epoch 840, training loss: 0.6639388203620911 = 0.012210972607135773 + 0.1 * 6.51727819442749
Epoch 840, val loss: 1.4162347316741943
Epoch 850, training loss: 0.6612640023231506 = 0.011784898117184639 + 0.1 * 6.494791030883789
Epoch 850, val loss: 1.4220389127731323
Epoch 860, training loss: 0.6618912816047668 = 0.01138196513056755 + 0.1 * 6.505092620849609
Epoch 860, val loss: 1.42782723903656
Epoch 870, training loss: 0.6609269976615906 = 0.011000380851328373 + 0.1 * 6.499265670776367
Epoch 870, val loss: 1.4334615468978882
Epoch 880, training loss: 0.6599857807159424 = 0.010638327337801456 + 0.1 * 6.493474006652832
Epoch 880, val loss: 1.4391145706176758
Epoch 890, training loss: 0.659116268157959 = 0.010296259075403214 + 0.1 * 6.488199710845947
Epoch 890, val loss: 1.4445505142211914
Epoch 900, training loss: 0.6599912047386169 = 0.00997071247547865 + 0.1 * 6.500204563140869
Epoch 900, val loss: 1.45000159740448
Epoch 910, training loss: 0.6590433120727539 = 0.009662142023444176 + 0.1 * 6.49381160736084
Epoch 910, val loss: 1.455329418182373
Epoch 920, training loss: 0.6584461331367493 = 0.009368465282022953 + 0.1 * 6.490776538848877
Epoch 920, val loss: 1.4605152606964111
Epoch 930, training loss: 0.6585721969604492 = 0.009088869206607342 + 0.1 * 6.494832992553711
Epoch 930, val loss: 1.4657381772994995
Epoch 940, training loss: 0.6563908457756042 = 0.008822917938232422 + 0.1 * 6.475679397583008
Epoch 940, val loss: 1.4708102941513062
Epoch 950, training loss: 0.6567243933677673 = 0.00856947060674429 + 0.1 * 6.481549263000488
Epoch 950, val loss: 1.4756782054901123
Epoch 960, training loss: 0.6561353206634521 = 0.008326917886734009 + 0.1 * 6.478084087371826
Epoch 960, val loss: 1.4806632995605469
Epoch 970, training loss: 0.6574897170066833 = 0.008095953613519669 + 0.1 * 6.493937015533447
Epoch 970, val loss: 1.4854754209518433
Epoch 980, training loss: 0.6556734442710876 = 0.007875272072851658 + 0.1 * 6.4779815673828125
Epoch 980, val loss: 1.4903404712677002
Epoch 990, training loss: 0.6548017859458923 = 0.0076646688394248486 + 0.1 * 6.471370697021484
Epoch 990, val loss: 1.494897484779358
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 2.8145811557769775 = 1.9549005031585693 + 0.1 * 8.596806526184082
Epoch 0, val loss: 1.9564039707183838
Epoch 10, training loss: 2.804433822631836 = 1.9447658061981201 + 0.1 * 8.5966796875
Epoch 10, val loss: 1.9459750652313232
Epoch 20, training loss: 2.791522264480591 = 1.9319332838058472 + 0.1 * 8.5958890914917
Epoch 20, val loss: 1.9328300952911377
Epoch 30, training loss: 2.772404670715332 = 1.9134706258773804 + 0.1 * 8.589341163635254
Epoch 30, val loss: 1.9138115644454956
Epoch 40, training loss: 2.741065263748169 = 1.8860855102539062 + 0.1 * 8.549797058105469
Epoch 40, val loss: 1.88577139377594
Epoch 50, training loss: 2.683720827102661 = 1.8499219417572021 + 0.1 * 8.337987899780273
Epoch 50, val loss: 1.8505064249038696
Epoch 60, training loss: 2.630079984664917 = 1.8119070529937744 + 0.1 * 8.18172836303711
Epoch 60, val loss: 1.815847635269165
Epoch 70, training loss: 2.562018394470215 = 1.7806214094161987 + 0.1 * 7.813969612121582
Epoch 70, val loss: 1.788766622543335
Epoch 80, training loss: 2.482387065887451 = 1.7517273426055908 + 0.1 * 7.306595802307129
Epoch 80, val loss: 1.7634767293930054
Epoch 90, training loss: 2.423302173614502 = 1.7158581018447876 + 0.1 * 7.074440956115723
Epoch 90, val loss: 1.7322101593017578
Epoch 100, training loss: 2.3650336265563965 = 1.6647284030914307 + 0.1 * 7.003050804138184
Epoch 100, val loss: 1.6884900331497192
Epoch 110, training loss: 2.293461799621582 = 1.5976451635360718 + 0.1 * 6.958165645599365
Epoch 110, val loss: 1.6320806741714478
Epoch 120, training loss: 2.2116684913635254 = 1.5199429988861084 + 0.1 * 6.917254447937012
Epoch 120, val loss: 1.5686442852020264
Epoch 130, training loss: 2.128321409225464 = 1.439545750617981 + 0.1 * 6.887755870819092
Epoch 130, val loss: 1.5056219100952148
Epoch 140, training loss: 2.0485057830810547 = 1.3622022867202759 + 0.1 * 6.863034248352051
Epoch 140, val loss: 1.4479085206985474
Epoch 150, training loss: 1.9702833890914917 = 1.2861478328704834 + 0.1 * 6.841355323791504
Epoch 150, val loss: 1.3934358358383179
Epoch 160, training loss: 1.893727421760559 = 1.2113784551620483 + 0.1 * 6.823489665985107
Epoch 160, val loss: 1.3412303924560547
Epoch 170, training loss: 1.8187031745910645 = 1.1382806301116943 + 0.1 * 6.804225921630859
Epoch 170, val loss: 1.291639804840088
Epoch 180, training loss: 1.7475889921188354 = 1.0684564113616943 + 0.1 * 6.791325569152832
Epoch 180, val loss: 1.2456711530685425
Epoch 190, training loss: 1.6810227632522583 = 1.0035086870193481 + 0.1 * 6.775140762329102
Epoch 190, val loss: 1.2038873434066772
Epoch 200, training loss: 1.620500087738037 = 0.9429087042808533 + 0.1 * 6.775913238525391
Epoch 200, val loss: 1.1651908159255981
Epoch 210, training loss: 1.5624003410339355 = 0.8871368765830994 + 0.1 * 6.752634048461914
Epoch 210, val loss: 1.1293178796768188
Epoch 220, training loss: 1.5081756114959717 = 0.8339850902557373 + 0.1 * 6.741905212402344
Epoch 220, val loss: 1.0949171781539917
Epoch 230, training loss: 1.4564625024795532 = 0.7821770310401917 + 0.1 * 6.742854595184326
Epoch 230, val loss: 1.0613491535186768
Epoch 240, training loss: 1.4051012992858887 = 0.7320931553840637 + 0.1 * 6.730082035064697
Epoch 240, val loss: 1.0289647579193115
Epoch 250, training loss: 1.3550337553024292 = 0.6838276386260986 + 0.1 * 6.712060928344727
Epoch 250, val loss: 0.9983320236206055
Epoch 260, training loss: 1.3081109523773193 = 0.6372312307357788 + 0.1 * 6.708796501159668
Epoch 260, val loss: 0.9700340032577515
Epoch 270, training loss: 1.2620062828063965 = 0.5926320552825928 + 0.1 * 6.693741321563721
Epoch 270, val loss: 0.9443978667259216
Epoch 280, training loss: 1.2176103591918945 = 0.549612820148468 + 0.1 * 6.679975986480713
Epoch 280, val loss: 0.9215341210365295
Epoch 290, training loss: 1.178570032119751 = 0.5075650215148926 + 0.1 * 6.710050106048584
Epoch 290, val loss: 0.9010549187660217
Epoch 300, training loss: 1.1339421272277832 = 0.46688467264175415 + 0.1 * 6.670573711395264
Epoch 300, val loss: 0.8829519748687744
Epoch 310, training loss: 1.0936667919158936 = 0.427054762840271 + 0.1 * 6.666119575500488
Epoch 310, val loss: 0.8666740655899048
Epoch 320, training loss: 1.0543043613433838 = 0.3881238102912903 + 0.1 * 6.661804676055908
Epoch 320, val loss: 0.8520594239234924
Epoch 330, training loss: 1.0162866115570068 = 0.35058072209358215 + 0.1 * 6.657059192657471
Epoch 330, val loss: 0.839165449142456
Epoch 340, training loss: 0.9804027080535889 = 0.3151070773601532 + 0.1 * 6.652956008911133
Epoch 340, val loss: 0.8281949758529663
Epoch 350, training loss: 0.9476808905601501 = 0.2822672128677368 + 0.1 * 6.654136657714844
Epoch 350, val loss: 0.819388747215271
Epoch 360, training loss: 0.9161510467529297 = 0.25271978974342346 + 0.1 * 6.634312629699707
Epoch 360, val loss: 0.8130160570144653
Epoch 370, training loss: 0.8898155093193054 = 0.2265167236328125 + 0.1 * 6.6329874992370605
Epoch 370, val loss: 0.8090906143188477
Epoch 380, training loss: 0.8656520247459412 = 0.20347929000854492 + 0.1 * 6.621726989746094
Epoch 380, val loss: 0.8073529601097107
Epoch 390, training loss: 0.8471486568450928 = 0.18323010206222534 + 0.1 * 6.639185428619385
Epoch 390, val loss: 0.8075850605964661
Epoch 400, training loss: 0.8265975713729858 = 0.16553422808647156 + 0.1 * 6.61063289642334
Epoch 400, val loss: 0.8094344735145569
Epoch 410, training loss: 0.8113378286361694 = 0.14997436106204987 + 0.1 * 6.6136345863342285
Epoch 410, val loss: 0.8125612735748291
Epoch 420, training loss: 0.7977687120437622 = 0.1362564116716385 + 0.1 * 6.615123271942139
Epoch 420, val loss: 0.8167211413383484
Epoch 430, training loss: 0.7840685844421387 = 0.12413477897644043 + 0.1 * 6.599338054656982
Epoch 430, val loss: 0.8217731714248657
Epoch 440, training loss: 0.7736978530883789 = 0.11336580663919449 + 0.1 * 6.603320121765137
Epoch 440, val loss: 0.8275255560874939
Epoch 450, training loss: 0.7632066011428833 = 0.10379098355770111 + 0.1 * 6.594155788421631
Epoch 450, val loss: 0.8338186740875244
Epoch 460, training loss: 0.7547982931137085 = 0.09526943415403366 + 0.1 * 6.595288276672363
Epoch 460, val loss: 0.8405236005783081
Epoch 470, training loss: 0.7458337545394897 = 0.08766846358776093 + 0.1 * 6.581652641296387
Epoch 470, val loss: 0.8475658297538757
Epoch 480, training loss: 0.7386171221733093 = 0.08086103945970535 + 0.1 * 6.577560901641846
Epoch 480, val loss: 0.8548740744590759
Epoch 490, training loss: 0.7316396236419678 = 0.0747450739145279 + 0.1 * 6.568945407867432
Epoch 490, val loss: 0.8624054193496704
Epoch 500, training loss: 0.7272133827209473 = 0.06922591477632523 + 0.1 * 6.579874515533447
Epoch 500, val loss: 0.8701521158218384
Epoch 510, training loss: 0.7213597297668457 = 0.06425384432077408 + 0.1 * 6.57105827331543
Epoch 510, val loss: 0.8779775500297546
Epoch 520, training loss: 0.7156678438186646 = 0.05975371226668358 + 0.1 * 6.559141159057617
Epoch 520, val loss: 0.8859140872955322
Epoch 530, training loss: 0.7112076282501221 = 0.055675264447927475 + 0.1 * 6.555323600769043
Epoch 530, val loss: 0.8938868641853333
Epoch 540, training loss: 0.7076444625854492 = 0.051975518465042114 + 0.1 * 6.5566887855529785
Epoch 540, val loss: 0.9018567800521851
Epoch 550, training loss: 0.7047349810600281 = 0.048598773777484894 + 0.1 * 6.561361789703369
Epoch 550, val loss: 0.9098698496818542
Epoch 560, training loss: 0.7008323669433594 = 0.045520421117544174 + 0.1 * 6.55311918258667
Epoch 560, val loss: 0.9178502559661865
Epoch 570, training loss: 0.6968456506729126 = 0.04270536080002785 + 0.1 * 6.541402816772461
Epoch 570, val loss: 0.9257816076278687
Epoch 580, training loss: 0.6948800086975098 = 0.040125004947185516 + 0.1 * 6.547549724578857
Epoch 580, val loss: 0.9336367845535278
Epoch 590, training loss: 0.6913815140724182 = 0.037763964384794235 + 0.1 * 6.536175727844238
Epoch 590, val loss: 0.9414289593696594
Epoch 600, training loss: 0.6895512342453003 = 0.03559108078479767 + 0.1 * 6.5396013259887695
Epoch 600, val loss: 0.9491631388664246
Epoch 610, training loss: 0.6864186525344849 = 0.033592525869607925 + 0.1 * 6.528261184692383
Epoch 610, val loss: 0.9568194150924683
Epoch 620, training loss: 0.6846495866775513 = 0.03174997866153717 + 0.1 * 6.528995990753174
Epoch 620, val loss: 0.9643751382827759
Epoch 630, training loss: 0.6818691492080688 = 0.03004802204668522 + 0.1 * 6.518211364746094
Epoch 630, val loss: 0.9718394875526428
Epoch 640, training loss: 0.6805641055107117 = 0.028474442660808563 + 0.1 * 6.5208964347839355
Epoch 640, val loss: 0.9791751503944397
Epoch 650, training loss: 0.6795783042907715 = 0.027017906308174133 + 0.1 * 6.525603771209717
Epoch 650, val loss: 0.9863755702972412
Epoch 660, training loss: 0.6776250004768372 = 0.025670170783996582 + 0.1 * 6.519548416137695
Epoch 660, val loss: 0.9934468865394592
Epoch 670, training loss: 0.6759741306304932 = 0.024417819455266 + 0.1 * 6.515563011169434
Epoch 670, val loss: 1.000440001487732
Epoch 680, training loss: 0.6739412546157837 = 0.023252040147781372 + 0.1 * 6.506892204284668
Epoch 680, val loss: 1.0072652101516724
Epoch 690, training loss: 0.6734519600868225 = 0.022167524322867393 + 0.1 * 6.512844562530518
Epoch 690, val loss: 1.0139955282211304
Epoch 700, training loss: 0.6717225313186646 = 0.02115624211728573 + 0.1 * 6.50566291809082
Epoch 700, val loss: 1.020621657371521
Epoch 710, training loss: 0.6705765128135681 = 0.02021203190088272 + 0.1 * 6.5036444664001465
Epoch 710, val loss: 1.0271341800689697
Epoch 720, training loss: 0.6694702506065369 = 0.019329192116856575 + 0.1 * 6.501410484313965
Epoch 720, val loss: 1.0335372686386108
Epoch 730, training loss: 0.6678165197372437 = 0.01850329525768757 + 0.1 * 6.4931321144104
Epoch 730, val loss: 1.0397623777389526
Epoch 740, training loss: 0.666684091091156 = 0.017731649801135063 + 0.1 * 6.4895243644714355
Epoch 740, val loss: 1.0459413528442383
Epoch 750, training loss: 0.6677683591842651 = 0.01700688526034355 + 0.1 * 6.507614612579346
Epoch 750, val loss: 1.0519866943359375
Epoch 760, training loss: 0.6655861139297485 = 0.016325978562235832 + 0.1 * 6.49260139465332
Epoch 760, val loss: 1.0579394102096558
Epoch 770, training loss: 0.6655213832855225 = 0.015687238425016403 + 0.1 * 6.4983415603637695
Epoch 770, val loss: 1.0637468099594116
Epoch 780, training loss: 0.6632341146469116 = 0.015085863880813122 + 0.1 * 6.48148250579834
Epoch 780, val loss: 1.0694944858551025
Epoch 790, training loss: 0.6646519899368286 = 0.014517971314489841 + 0.1 * 6.501339912414551
Epoch 790, val loss: 1.0751334428787231
Epoch 800, training loss: 0.6620700359344482 = 0.013982885517179966 + 0.1 * 6.480871200561523
Epoch 800, val loss: 1.0806481838226318
Epoch 810, training loss: 0.6625144481658936 = 0.013477840460836887 + 0.1 * 6.490365505218506
Epoch 810, val loss: 1.086079478263855
Epoch 820, training loss: 0.6617589592933655 = 0.013000727631151676 + 0.1 * 6.487582206726074
Epoch 820, val loss: 1.0913528203964233
Epoch 830, training loss: 0.66022127866745 = 0.012551217339932919 + 0.1 * 6.476700782775879
Epoch 830, val loss: 1.096598744392395
Epoch 840, training loss: 0.6592148542404175 = 0.012125367298722267 + 0.1 * 6.470894813537598
Epoch 840, val loss: 1.1017404794692993
Epoch 850, training loss: 0.6602609157562256 = 0.011720257811248302 + 0.1 * 6.485406398773193
Epoch 850, val loss: 1.1067920923233032
Epoch 860, training loss: 0.6587679982185364 = 0.01133668515831232 + 0.1 * 6.474312782287598
Epoch 860, val loss: 1.1117677688598633
Epoch 870, training loss: 0.6594722270965576 = 0.010972792282700539 + 0.1 * 6.484994411468506
Epoch 870, val loss: 1.1166212558746338
Epoch 880, training loss: 0.6577081680297852 = 0.01062736101448536 + 0.1 * 6.4708075523376465
Epoch 880, val loss: 1.1214302778244019
Epoch 890, training loss: 0.6565983295440674 = 0.010299524292349815 + 0.1 * 6.462987899780273
Epoch 890, val loss: 1.1261670589447021
Epoch 900, training loss: 0.6569305658340454 = 0.009986435063183308 + 0.1 * 6.469440937042236
Epoch 900, val loss: 1.1308207511901855
Epoch 910, training loss: 0.6566895842552185 = 0.009688797406852245 + 0.1 * 6.47000789642334
Epoch 910, val loss: 1.135344386100769
Epoch 920, training loss: 0.6549863219261169 = 0.009405460208654404 + 0.1 * 6.455808639526367
Epoch 920, val loss: 1.1398309469223022
Epoch 930, training loss: 0.6568309664726257 = 0.009135326370596886 + 0.1 * 6.476956367492676
Epoch 930, val loss: 1.1442251205444336
Epoch 940, training loss: 0.6539239287376404 = 0.008877263404428959 + 0.1 * 6.450466156005859
Epoch 940, val loss: 1.1485451459884644
Epoch 950, training loss: 0.6539168357849121 = 0.008631340228021145 + 0.1 * 6.452854633331299
Epoch 950, val loss: 1.152817964553833
Epoch 960, training loss: 0.6546445488929749 = 0.008395538665354252 + 0.1 * 6.462490081787109
Epoch 960, val loss: 1.1570231914520264
Epoch 970, training loss: 0.6529826521873474 = 0.008170456625521183 + 0.1 * 6.448121547698975
Epoch 970, val loss: 1.1611422300338745
Epoch 980, training loss: 0.6541016101837158 = 0.007955123670399189 + 0.1 * 6.4614644050598145
Epoch 980, val loss: 1.1652060747146606
Epoch 990, training loss: 0.6534377336502075 = 0.007748303469270468 + 0.1 * 6.4568939208984375
Epoch 990, val loss: 1.1691906452178955
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 2.8083078861236572 = 1.9486260414123535 + 0.1 * 8.596818923950195
Epoch 0, val loss: 1.9586249589920044
Epoch 10, training loss: 2.7983691692352295 = 1.9386963844299316 + 0.1 * 8.596728324890137
Epoch 10, val loss: 1.9478994607925415
Epoch 20, training loss: 2.7860679626464844 = 1.9264411926269531 + 0.1 * 8.596268653869629
Epoch 20, val loss: 1.9342418909072876
Epoch 30, training loss: 2.768357515335083 = 1.9090718030929565 + 0.1 * 8.592857360839844
Epoch 30, val loss: 1.914652705192566
Epoch 40, training loss: 2.740492582321167 = 1.8833668231964111 + 0.1 * 8.571257591247559
Epoch 40, val loss: 1.8857650756835938
Epoch 50, training loss: 2.6950716972351074 = 1.8483065366744995 + 0.1 * 8.467652320861816
Epoch 50, val loss: 1.848097324371338
Epoch 60, training loss: 2.632995843887329 = 1.8115040063858032 + 0.1 * 8.214919090270996
Epoch 60, val loss: 1.811928629875183
Epoch 70, training loss: 2.554104804992676 = 1.781643271446228 + 0.1 * 7.724616527557373
Epoch 70, val loss: 1.7852734327316284
Epoch 80, training loss: 2.48124623298645 = 1.7498847246170044 + 0.1 * 7.313615798950195
Epoch 80, val loss: 1.756807804107666
Epoch 90, training loss: 2.42452335357666 = 1.7097136974334717 + 0.1 * 7.148097038269043
Epoch 90, val loss: 1.720786690711975
Epoch 100, training loss: 2.363969326019287 = 1.655748724937439 + 0.1 * 7.082205772399902
Epoch 100, val loss: 1.6732332706451416
Epoch 110, training loss: 2.289567232131958 = 1.5857746601104736 + 0.1 * 7.037926197052002
Epoch 110, val loss: 1.6130423545837402
Epoch 120, training loss: 2.202091693878174 = 1.5011751651763916 + 0.1 * 7.009165287017822
Epoch 120, val loss: 1.5414483547210693
Epoch 130, training loss: 2.1034719944000244 = 1.4046367406845093 + 0.1 * 6.988353252410889
Epoch 130, val loss: 1.4615099430084229
Epoch 140, training loss: 1.9972118139266968 = 1.3002445697784424 + 0.1 * 6.969672203063965
Epoch 140, val loss: 1.377052664756775
Epoch 150, training loss: 1.887195110321045 = 1.1923984289169312 + 0.1 * 6.947967052459717
Epoch 150, val loss: 1.2924840450286865
Epoch 160, training loss: 1.7796236276626587 = 1.0872471332550049 + 0.1 * 6.923764705657959
Epoch 160, val loss: 1.2134301662445068
Epoch 170, training loss: 1.6829172372817993 = 0.992464005947113 + 0.1 * 6.904531955718994
Epoch 170, val loss: 1.1452404260635376
Epoch 180, training loss: 1.5978420972824097 = 0.9095281362533569 + 0.1 * 6.883139610290527
Epoch 180, val loss: 1.0885541439056396
Epoch 190, training loss: 1.5251635313034058 = 0.8385335803031921 + 0.1 * 6.866299629211426
Epoch 190, val loss: 1.0433539152145386
Epoch 200, training loss: 1.4638830423355103 = 0.778588593006134 + 0.1 * 6.852944374084473
Epoch 200, val loss: 1.008766531944275
Epoch 210, training loss: 1.412116289138794 = 0.7286995053291321 + 0.1 * 6.83416748046875
Epoch 210, val loss: 0.9834787845611572
Epoch 220, training loss: 1.3689380884170532 = 0.6864699721336365 + 0.1 * 6.824680805206299
Epoch 220, val loss: 0.9652454257011414
Epoch 230, training loss: 1.3299810886383057 = 0.6493224501609802 + 0.1 * 6.806585788726807
Epoch 230, val loss: 0.9516592621803284
Epoch 240, training loss: 1.2954065799713135 = 0.6148318648338318 + 0.1 * 6.8057475090026855
Epoch 240, val loss: 0.940710186958313
Epoch 250, training loss: 1.2601622343063354 = 0.5818142890930176 + 0.1 * 6.7834792137146
Epoch 250, val loss: 0.9312052130699158
Epoch 260, training loss: 1.2275575399398804 = 0.5492827892303467 + 0.1 * 6.782747268676758
Epoch 260, val loss: 0.9224565029144287
Epoch 270, training loss: 1.1934223175048828 = 0.5172361135482788 + 0.1 * 6.761861324310303
Epoch 270, val loss: 0.914519190788269
Epoch 280, training loss: 1.1614933013916016 = 0.48549264669418335 + 0.1 * 6.760006427764893
Epoch 280, val loss: 0.907719075679779
Epoch 290, training loss: 1.1282167434692383 = 0.45385926961898804 + 0.1 * 6.743575096130371
Epoch 290, val loss: 0.9022194743156433
Epoch 300, training loss: 1.0965521335601807 = 0.42180493474006653 + 0.1 * 6.747471809387207
Epoch 300, val loss: 0.8978368639945984
Epoch 310, training loss: 1.0621507167816162 = 0.38914215564727783 + 0.1 * 6.730085849761963
Epoch 310, val loss: 0.8944849371910095
Epoch 320, training loss: 1.0279386043548584 = 0.35576725006103516 + 0.1 * 6.721712589263916
Epoch 320, val loss: 0.8929197788238525
Epoch 330, training loss: 0.9941404461860657 = 0.32251209020614624 + 0.1 * 6.716283321380615
Epoch 330, val loss: 0.8939527869224548
Epoch 340, training loss: 0.9615511894226074 = 0.2904868423938751 + 0.1 * 6.710643291473389
Epoch 340, val loss: 0.8982725739479065
Epoch 350, training loss: 0.9307299852371216 = 0.2606995403766632 + 0.1 * 6.70030403137207
Epoch 350, val loss: 0.9060401320457458
Epoch 360, training loss: 0.9028633236885071 = 0.2336779683828354 + 0.1 * 6.6918535232543945
Epoch 360, val loss: 0.9169297814369202
Epoch 370, training loss: 0.8791729211807251 = 0.20944052934646606 + 0.1 * 6.697323799133301
Epoch 370, val loss: 0.9305809736251831
Epoch 380, training loss: 0.8564423322677612 = 0.18794462084770203 + 0.1 * 6.684977054595947
Epoch 380, val loss: 0.9462765455245972
Epoch 390, training loss: 0.8367067575454712 = 0.16897085309028625 + 0.1 * 6.677358627319336
Epoch 390, val loss: 0.9634885191917419
Epoch 400, training loss: 0.8190455436706543 = 0.1522345244884491 + 0.1 * 6.668109893798828
Epoch 400, val loss: 0.9818576574325562
Epoch 410, training loss: 0.8039999008178711 = 0.1374397724866867 + 0.1 * 6.665600776672363
Epoch 410, val loss: 1.0009621381759644
Epoch 420, training loss: 0.7900408506393433 = 0.1243664100766182 + 0.1 * 6.656744480133057
Epoch 420, val loss: 1.0204190015792847
Epoch 430, training loss: 0.7794284820556641 = 0.11275777220726013 + 0.1 * 6.666706562042236
Epoch 430, val loss: 1.0402777194976807
Epoch 440, training loss: 0.7689719200134277 = 0.10246405005455017 + 0.1 * 6.665079116821289
Epoch 440, val loss: 1.0601283311843872
Epoch 450, training loss: 0.7573317885398865 = 0.09331848472356796 + 0.1 * 6.640132904052734
Epoch 450, val loss: 1.0798227787017822
Epoch 460, training loss: 0.7492424845695496 = 0.08517288416624069 + 0.1 * 6.640695571899414
Epoch 460, val loss: 1.0996257066726685
Epoch 470, training loss: 0.7410126328468323 = 0.07790917158126831 + 0.1 * 6.6310343742370605
Epoch 470, val loss: 1.119341254234314
Epoch 480, training loss: 0.7345291376113892 = 0.07142606377601624 + 0.1 * 6.631031036376953
Epoch 480, val loss: 1.1389381885528564
Epoch 490, training loss: 0.7275750637054443 = 0.0656319260597229 + 0.1 * 6.619431018829346
Epoch 490, val loss: 1.158297061920166
Epoch 500, training loss: 0.7229164242744446 = 0.06043948605656624 + 0.1 * 6.62476921081543
Epoch 500, val loss: 1.1775336265563965
Epoch 510, training loss: 0.7176464796066284 = 0.05579346790909767 + 0.1 * 6.6185302734375
Epoch 510, val loss: 1.1963573694229126
Epoch 520, training loss: 0.7134845852851868 = 0.051613666117191315 + 0.1 * 6.618708610534668
Epoch 520, val loss: 1.2150434255599976
Epoch 530, training loss: 0.708315372467041 = 0.04785838723182678 + 0.1 * 6.604569911956787
Epoch 530, val loss: 1.2333123683929443
Epoch 540, training loss: 0.7045587301254272 = 0.044471483677625656 + 0.1 * 6.600872039794922
Epoch 540, val loss: 1.2513582706451416
Epoch 550, training loss: 0.7017240524291992 = 0.041409462690353394 + 0.1 * 6.603146076202393
Epoch 550, val loss: 1.2690508365631104
Epoch 560, training loss: 0.6978475451469421 = 0.03863830864429474 + 0.1 * 6.592092514038086
Epoch 560, val loss: 1.2863283157348633
Epoch 570, training loss: 0.6958703994750977 = 0.036125022917985916 + 0.1 * 6.597454071044922
Epoch 570, val loss: 1.3032582998275757
Epoch 580, training loss: 0.6926417946815491 = 0.033843994140625 + 0.1 * 6.587977886199951
Epoch 580, val loss: 1.3198057413101196
Epoch 590, training loss: 0.6900569796562195 = 0.03176688030362129 + 0.1 * 6.5829010009765625
Epoch 590, val loss: 1.3359655141830444
Epoch 600, training loss: 0.6868698596954346 = 0.029872968792915344 + 0.1 * 6.569969177246094
Epoch 600, val loss: 1.3517229557037354
Epoch 610, training loss: 0.6870265007019043 = 0.028138021007180214 + 0.1 * 6.588884353637695
Epoch 610, val loss: 1.3672757148742676
Epoch 620, training loss: 0.6836696863174438 = 0.026552453637123108 + 0.1 * 6.57117223739624
Epoch 620, val loss: 1.3822987079620361
Epoch 630, training loss: 0.6815919876098633 = 0.02509910799562931 + 0.1 * 6.564929008483887
Epoch 630, val loss: 1.3969544172286987
Epoch 640, training loss: 0.6795415282249451 = 0.02376159280538559 + 0.1 * 6.557799339294434
Epoch 640, val loss: 1.411350965499878
Epoch 650, training loss: 0.6789206862449646 = 0.02252795360982418 + 0.1 * 6.563927173614502
Epoch 650, val loss: 1.425445795059204
Epoch 660, training loss: 0.677918016910553 = 0.021389590576291084 + 0.1 * 6.56528377532959
Epoch 660, val loss: 1.4392353296279907
Epoch 670, training loss: 0.675685703754425 = 0.020337505266070366 + 0.1 * 6.553481578826904
Epoch 670, val loss: 1.4525668621063232
Epoch 680, training loss: 0.6737086772918701 = 0.019365357235074043 + 0.1 * 6.54343318939209
Epoch 680, val loss: 1.4655498266220093
Epoch 690, training loss: 0.6732404232025146 = 0.01846199482679367 + 0.1 * 6.547784328460693
Epoch 690, val loss: 1.478376865386963
Epoch 700, training loss: 0.6725769639015198 = 0.017623161897063255 + 0.1 * 6.549537658691406
Epoch 700, val loss: 1.4908086061477661
Epoch 710, training loss: 0.6710430979728699 = 0.01684243232011795 + 0.1 * 6.542006492614746
Epoch 710, val loss: 1.5029888153076172
Epoch 720, training loss: 0.6691400408744812 = 0.016115795820951462 + 0.1 * 6.530242443084717
Epoch 720, val loss: 1.5148402452468872
Epoch 730, training loss: 0.6680528521537781 = 0.015438620932400227 + 0.1 * 6.526142120361328
Epoch 730, val loss: 1.5263597965240479
Epoch 740, training loss: 0.6678394079208374 = 0.014804300852119923 + 0.1 * 6.530351161956787
Epoch 740, val loss: 1.5377659797668457
Epoch 750, training loss: 0.6670610904693604 = 0.01421027909964323 + 0.1 * 6.528508186340332
Epoch 750, val loss: 1.5488951206207275
Epoch 760, training loss: 0.6666908264160156 = 0.013654314912855625 + 0.1 * 6.530365467071533
Epoch 760, val loss: 1.5596295595169067
Epoch 770, training loss: 0.6653541326522827 = 0.013132434338331223 + 0.1 * 6.522216796875
Epoch 770, val loss: 1.570238471031189
Epoch 780, training loss: 0.6644124388694763 = 0.012641730718314648 + 0.1 * 6.517706871032715
Epoch 780, val loss: 1.580610752105713
Epoch 790, training loss: 0.6636025309562683 = 0.012179255485534668 + 0.1 * 6.514232635498047
Epoch 790, val loss: 1.5908422470092773
Epoch 800, training loss: 0.6628021597862244 = 0.011744275689125061 + 0.1 * 6.5105791091918945
Epoch 800, val loss: 1.6007614135742188
Epoch 810, training loss: 0.6628383994102478 = 0.011333424597978592 + 0.1 * 6.515049934387207
Epoch 810, val loss: 1.61057448387146
Epoch 820, training loss: 0.6632923483848572 = 0.010945812799036503 + 0.1 * 6.523465633392334
Epoch 820, val loss: 1.620070219039917
Epoch 830, training loss: 0.6610687971115112 = 0.010580169968307018 + 0.1 * 6.504886150360107
Epoch 830, val loss: 1.6293400526046753
Epoch 840, training loss: 0.6603046655654907 = 0.010235538706183434 + 0.1 * 6.500690937042236
Epoch 840, val loss: 1.638323187828064
Epoch 850, training loss: 0.6596189737319946 = 0.009907669387757778 + 0.1 * 6.497113227844238
Epoch 850, val loss: 1.6473344564437866
Epoch 860, training loss: 0.6596742272377014 = 0.009596616961061954 + 0.1 * 6.5007758140563965
Epoch 860, val loss: 1.656190037727356
Epoch 870, training loss: 0.6595782041549683 = 0.00930110178887844 + 0.1 * 6.502770900726318
Epoch 870, val loss: 1.6648057699203491
Epoch 880, training loss: 0.6588183641433716 = 0.009020548313856125 + 0.1 * 6.4979777336120605
Epoch 880, val loss: 1.6732889413833618
Epoch 890, training loss: 0.6589124798774719 = 0.008753878995776176 + 0.1 * 6.501585960388184
Epoch 890, val loss: 1.6815919876098633
Epoch 900, training loss: 0.6582620143890381 = 0.008499730378389359 + 0.1 * 6.497622489929199
Epoch 900, val loss: 1.6897251605987549
Epoch 910, training loss: 0.6578934788703918 = 0.008258282206952572 + 0.1 * 6.496351718902588
Epoch 910, val loss: 1.6976673603057861
Epoch 920, training loss: 0.6564721465110779 = 0.00802815705537796 + 0.1 * 6.484439849853516
Epoch 920, val loss: 1.7055140733718872
Epoch 930, training loss: 0.6570526361465454 = 0.0078083607368171215 + 0.1 * 6.492442607879639
Epoch 930, val loss: 1.7132213115692139
Epoch 940, training loss: 0.6571052670478821 = 0.007598265074193478 + 0.1 * 6.495069980621338
Epoch 940, val loss: 1.7207387685775757
Epoch 950, training loss: 0.6555616855621338 = 0.007398523390293121 + 0.1 * 6.481631278991699
Epoch 950, val loss: 1.7280731201171875
Epoch 960, training loss: 0.6562461853027344 = 0.007206516340374947 + 0.1 * 6.490396499633789
Epoch 960, val loss: 1.7353483438491821
Epoch 970, training loss: 0.6548914909362793 = 0.007023361977189779 + 0.1 * 6.478681564331055
Epoch 970, val loss: 1.7424365282058716
Epoch 980, training loss: 0.6548601388931274 = 0.00684815039858222 + 0.1 * 6.480119705200195
Epoch 980, val loss: 1.7493661642074585
Epoch 990, training loss: 0.6538614630699158 = 0.0066804843954741955 + 0.1 * 6.471809387207031
Epoch 990, val loss: 1.756163477897644
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8133895624670533
The final CL Acc:0.75432, 0.02229, The final GNN Acc:0.81444, 0.00086
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13198])
remove edge: torch.Size([2, 7828])
updated graph: torch.Size([2, 10470])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8040781021118164 = 1.9443973302841187 + 0.1 * 8.596807479858398
Epoch 0, val loss: 1.950120449066162
Epoch 10, training loss: 2.7938425540924072 = 1.9341709613800049 + 0.1 * 8.596715927124023
Epoch 10, val loss: 1.940313458442688
Epoch 20, training loss: 2.7814559936523438 = 1.92183256149292 + 0.1 * 8.596235275268555
Epoch 20, val loss: 1.9279701709747314
Epoch 30, training loss: 2.764162063598633 = 1.9048973321914673 + 0.1 * 8.592646598815918
Epoch 30, val loss: 1.9105151891708374
Epoch 40, training loss: 2.736945629119873 = 1.8804051876068115 + 0.1 * 8.56540298461914
Epoch 40, val loss: 1.8852174282073975
Epoch 50, training loss: 2.6867752075195312 = 1.8468377590179443 + 0.1 * 8.399373054504395
Epoch 50, val loss: 1.8516268730163574
Epoch 60, training loss: 2.6040401458740234 = 1.8083690404891968 + 0.1 * 7.956711769104004
Epoch 60, val loss: 1.8150566816329956
Epoch 70, training loss: 2.5234384536743164 = 1.7723232507705688 + 0.1 * 7.511153221130371
Epoch 70, val loss: 1.781551718711853
Epoch 80, training loss: 2.4554004669189453 = 1.7341234683990479 + 0.1 * 7.212769985198975
Epoch 80, val loss: 1.7469861507415771
Epoch 90, training loss: 2.396901845932007 = 1.6847344636917114 + 0.1 * 7.121673583984375
Epoch 90, val loss: 1.7014849185943604
Epoch 100, training loss: 2.325996160507202 = 1.6173917055130005 + 0.1 * 7.0860443115234375
Epoch 100, val loss: 1.63963782787323
Epoch 110, training loss: 2.2389485836029053 = 1.5326716899871826 + 0.1 * 7.06276798248291
Epoch 110, val loss: 1.5655133724212646
Epoch 120, training loss: 2.139373302459717 = 1.435036063194275 + 0.1 * 7.043371677398682
Epoch 120, val loss: 1.481929063796997
Epoch 130, training loss: 2.0318188667297363 = 1.3293298482894897 + 0.1 * 7.024891376495361
Epoch 130, val loss: 1.393083095550537
Epoch 140, training loss: 1.9199656248092651 = 1.2191681861877441 + 0.1 * 7.007974147796631
Epoch 140, val loss: 1.3031020164489746
Epoch 150, training loss: 1.8074157238006592 = 1.1080007553100586 + 0.1 * 6.994149208068848
Epoch 150, val loss: 1.2144829034805298
Epoch 160, training loss: 1.6981005668640137 = 0.9998624920845032 + 0.1 * 6.982381343841553
Epoch 160, val loss: 1.1296840906143188
Epoch 170, training loss: 1.5960720777511597 = 0.8988635540008545 + 0.1 * 6.972084999084473
Epoch 170, val loss: 1.0509088039398193
Epoch 180, training loss: 1.5045018196105957 = 0.8082730174064636 + 0.1 * 6.962287425994873
Epoch 180, val loss: 0.9808191657066345
Epoch 190, training loss: 1.423006296157837 = 0.7280825972557068 + 0.1 * 6.94923734664917
Epoch 190, val loss: 0.919603168964386
Epoch 200, training loss: 1.3510980606079102 = 0.6576784253120422 + 0.1 * 6.934196472167969
Epoch 200, val loss: 0.8683788180351257
Epoch 210, training loss: 1.2883654832839966 = 0.5953928828239441 + 0.1 * 6.9297261238098145
Epoch 210, val loss: 0.8268178105354309
Epoch 220, training loss: 1.2308149337768555 = 0.5402122139930725 + 0.1 * 6.906027793884277
Epoch 220, val loss: 0.7944804430007935
Epoch 230, training loss: 1.1796224117279053 = 0.4904416799545288 + 0.1 * 6.891807556152344
Epoch 230, val loss: 0.7695348858833313
Epoch 240, training loss: 1.134230136871338 = 0.4456436336040497 + 0.1 * 6.885864734649658
Epoch 240, val loss: 0.7507816553115845
Epoch 250, training loss: 1.0923209190368652 = 0.4051465392112732 + 0.1 * 6.871744155883789
Epoch 250, val loss: 0.7370128035545349
Epoch 260, training loss: 1.0540112257003784 = 0.367954820394516 + 0.1 * 6.860564231872559
Epoch 260, val loss: 0.7273160219192505
Epoch 270, training loss: 1.020009994506836 = 0.3334237039089203 + 0.1 * 6.86586332321167
Epoch 270, val loss: 0.7210124135017395
Epoch 280, training loss: 0.9862366914749146 = 0.30176880955696106 + 0.1 * 6.84467887878418
Epoch 280, val loss: 0.7175492644309998
Epoch 290, training loss: 0.9565376043319702 = 0.27261829376220703 + 0.1 * 6.839192867279053
Epoch 290, val loss: 0.7162682414054871
Epoch 300, training loss: 0.9286125898361206 = 0.24580733478069305 + 0.1 * 6.828052043914795
Epoch 300, val loss: 0.7170875072479248
Epoch 310, training loss: 0.9035763740539551 = 0.22151075303554535 + 0.1 * 6.820655822753906
Epoch 310, val loss: 0.7197691798210144
Epoch 320, training loss: 0.8807675838470459 = 0.19972993433475494 + 0.1 * 6.8103766441345215
Epoch 320, val loss: 0.7241274118423462
Epoch 330, training loss: 0.8617344498634338 = 0.1802118420600891 + 0.1 * 6.815226078033447
Epoch 330, val loss: 0.7300536632537842
Epoch 340, training loss: 0.8427399396896362 = 0.16297772526741028 + 0.1 * 6.797622203826904
Epoch 340, val loss: 0.7372742891311646
Epoch 350, training loss: 0.8261710405349731 = 0.14768214523792267 + 0.1 * 6.784889221191406
Epoch 350, val loss: 0.745524525642395
Epoch 360, training loss: 0.8114916086196899 = 0.13404150307178497 + 0.1 * 6.774501323699951
Epoch 360, val loss: 0.7545744776725769
Epoch 370, training loss: 0.7986781001091003 = 0.12189895659685135 + 0.1 * 6.767791271209717
Epoch 370, val loss: 0.7643414735794067
Epoch 380, training loss: 0.7868936657905579 = 0.11109954118728638 + 0.1 * 6.757941246032715
Epoch 380, val loss: 0.7744361758232117
Epoch 390, training loss: 0.7764863967895508 = 0.10144852846860886 + 0.1 * 6.750378608703613
Epoch 390, val loss: 0.7849030494689941
Epoch 400, training loss: 0.7709160447120667 = 0.09281427413225174 + 0.1 * 6.781017780303955
Epoch 400, val loss: 0.7956408858299255
Epoch 410, training loss: 0.7594835758209229 = 0.08514407277107239 + 0.1 * 6.74339485168457
Epoch 410, val loss: 0.8062549829483032
Epoch 420, training loss: 0.7513347268104553 = 0.07827483862638474 + 0.1 * 6.7305989265441895
Epoch 420, val loss: 0.8169586062431335
Epoch 430, training loss: 0.7446967959403992 = 0.07209157198667526 + 0.1 * 6.7260518074035645
Epoch 430, val loss: 0.8276552557945251
Epoch 440, training loss: 0.739290714263916 = 0.06651949137449265 + 0.1 * 6.727712154388428
Epoch 440, val loss: 0.8383705615997314
Epoch 450, training loss: 0.7325546741485596 = 0.06150807440280914 + 0.1 * 6.710465908050537
Epoch 450, val loss: 0.8488928079605103
Epoch 460, training loss: 0.7274706363677979 = 0.056976690888404846 + 0.1 * 6.704939365386963
Epoch 460, val loss: 0.8593913912773132
Epoch 470, training loss: 0.7242326736450195 = 0.05287732183933258 + 0.1 * 6.713553428649902
Epoch 470, val loss: 0.8696258068084717
Epoch 480, training loss: 0.7185821533203125 = 0.049168653786182404 + 0.1 * 6.6941351890563965
Epoch 480, val loss: 0.8797355890274048
Epoch 490, training loss: 0.7149788737297058 = 0.045793261379003525 + 0.1 * 6.691856384277344
Epoch 490, val loss: 0.8896903991699219
Epoch 500, training loss: 0.7114831805229187 = 0.04271770268678665 + 0.1 * 6.687654972076416
Epoch 500, val loss: 0.8994876146316528
Epoch 510, training loss: 0.7094897627830505 = 0.03992084041237831 + 0.1 * 6.695688724517822
Epoch 510, val loss: 0.908983588218689
Epoch 520, training loss: 0.7050598859786987 = 0.03737780451774597 + 0.1 * 6.676820278167725
Epoch 520, val loss: 0.9182932376861572
Epoch 530, training loss: 0.7015865445137024 = 0.03504873067140579 + 0.1 * 6.665378093719482
Epoch 530, val loss: 0.9273898005485535
Epoch 540, training loss: 0.6985309720039368 = 0.03291318565607071 + 0.1 * 6.656177520751953
Epoch 540, val loss: 0.9363114237785339
Epoch 550, training loss: 0.6966456770896912 = 0.03095404990017414 + 0.1 * 6.65691614151001
Epoch 550, val loss: 0.9450337886810303
Epoch 560, training loss: 0.6940889954566956 = 0.029155530035495758 + 0.1 * 6.64933443069458
Epoch 560, val loss: 0.9535947442054749
Epoch 570, training loss: 0.6933658719062805 = 0.02750164084136486 + 0.1 * 6.658642292022705
Epoch 570, val loss: 0.9619396924972534
Epoch 580, training loss: 0.6908556818962097 = 0.02597961202263832 + 0.1 * 6.6487603187561035
Epoch 580, val loss: 0.9700457453727722
Epoch 590, training loss: 0.6877661943435669 = 0.024579262360930443 + 0.1 * 6.631868839263916
Epoch 590, val loss: 0.9780150055885315
Epoch 600, training loss: 0.6853678822517395 = 0.02328646369278431 + 0.1 * 6.620814323425293
Epoch 600, val loss: 0.9857809543609619
Epoch 610, training loss: 0.6843576431274414 = 0.022089892998337746 + 0.1 * 6.622677326202393
Epoch 610, val loss: 0.993319034576416
Epoch 620, training loss: 0.6838361024856567 = 0.0209850762039423 + 0.1 * 6.628509998321533
Epoch 620, val loss: 1.0007778406143188
Epoch 630, training loss: 0.6814159154891968 = 0.019961996003985405 + 0.1 * 6.61453914642334
Epoch 630, val loss: 1.0078840255737305
Epoch 640, training loss: 0.6797676086425781 = 0.019014690071344376 + 0.1 * 6.6075286865234375
Epoch 640, val loss: 1.014984369277954
Epoch 650, training loss: 0.6776008009910583 = 0.01813175156712532 + 0.1 * 6.594690322875977
Epoch 650, val loss: 1.0218510627746582
Epoch 660, training loss: 0.6758908629417419 = 0.017308473587036133 + 0.1 * 6.585824012756348
Epoch 660, val loss: 1.0286134481430054
Epoch 670, training loss: 0.6759310364723206 = 0.016542816534638405 + 0.1 * 6.5938825607299805
Epoch 670, val loss: 1.0351890325546265
Epoch 680, training loss: 0.6749854683876038 = 0.01582731492817402 + 0.1 * 6.59158182144165
Epoch 680, val loss: 1.0416924953460693
Epoch 690, training loss: 0.6750330328941345 = 0.015158276073634624 + 0.1 * 6.598747253417969
Epoch 690, val loss: 1.0479227304458618
Epoch 700, training loss: 0.6723166108131409 = 0.014534694142639637 + 0.1 * 6.577818870544434
Epoch 700, val loss: 1.0541642904281616
Epoch 710, training loss: 0.6711412072181702 = 0.013949516229331493 + 0.1 * 6.571916580200195
Epoch 710, val loss: 1.0601686239242554
Epoch 720, training loss: 0.6701312065124512 = 0.013400296680629253 + 0.1 * 6.5673089027404785
Epoch 720, val loss: 1.0661451816558838
Epoch 730, training loss: 0.6688306331634521 = 0.012882876209914684 + 0.1 * 6.55947732925415
Epoch 730, val loss: 1.0719451904296875
Epoch 740, training loss: 0.6679710745811462 = 0.012396873906254768 + 0.1 * 6.555741786956787
Epoch 740, val loss: 1.077675461769104
Epoch 750, training loss: 0.6684912443161011 = 0.011938250623643398 + 0.1 * 6.565529823303223
Epoch 750, val loss: 1.0831935405731201
Epoch 760, training loss: 0.6684085726737976 = 0.011506741866469383 + 0.1 * 6.569018363952637
Epoch 760, val loss: 1.0886352062225342
Epoch 770, training loss: 0.666534960269928 = 0.011100498028099537 + 0.1 * 6.554344654083252
Epoch 770, val loss: 1.0940455198287964
Epoch 780, training loss: 0.6655183434486389 = 0.010716315358877182 + 0.1 * 6.548020362854004
Epoch 780, val loss: 1.0993096828460693
Epoch 790, training loss: 0.6656723022460938 = 0.010352235287427902 + 0.1 * 6.5532002449035645
Epoch 790, val loss: 1.104422688484192
Epoch 800, training loss: 0.6645306348800659 = 0.010008753277361393 + 0.1 * 6.5452189445495605
Epoch 800, val loss: 1.1094777584075928
Epoch 810, training loss: 0.6635739803314209 = 0.00968392938375473 + 0.1 * 6.538900852203369
Epoch 810, val loss: 1.114482045173645
Epoch 820, training loss: 0.6627235412597656 = 0.009375193156301975 + 0.1 * 6.533483028411865
Epoch 820, val loss: 1.1193304061889648
Epoch 830, training loss: 0.6622575521469116 = 0.009081290103495121 + 0.1 * 6.53176212310791
Epoch 830, val loss: 1.1240904331207275
Epoch 840, training loss: 0.6617535948753357 = 0.008803474716842175 + 0.1 * 6.529500961303711
Epoch 840, val loss: 1.1287988424301147
Epoch 850, training loss: 0.6630285978317261 = 0.00853902567178011 + 0.1 * 6.544895648956299
Epoch 850, val loss: 1.1333872079849243
Epoch 860, training loss: 0.6609196662902832 = 0.008287741802632809 + 0.1 * 6.52631950378418
Epoch 860, val loss: 1.1378695964813232
Epoch 870, training loss: 0.6615555286407471 = 0.0080490130931139 + 0.1 * 6.535065174102783
Epoch 870, val loss: 1.1423827409744263
Epoch 880, training loss: 0.660007119178772 = 0.007820394821465015 + 0.1 * 6.521867275238037
Epoch 880, val loss: 1.1466803550720215
Epoch 890, training loss: 0.6596552133560181 = 0.007602902129292488 + 0.1 * 6.520522594451904
Epoch 890, val loss: 1.1510084867477417
Epoch 900, training loss: 0.6596875786781311 = 0.007394726388156414 + 0.1 * 6.522928237915039
Epoch 900, val loss: 1.1552425622940063
Epoch 910, training loss: 0.6633943915367126 = 0.007195699028670788 + 0.1 * 6.561986446380615
Epoch 910, val loss: 1.159379482269287
Epoch 920, training loss: 0.6589990854263306 = 0.007005968131124973 + 0.1 * 6.519930839538574
Epoch 920, val loss: 1.1633737087249756
Epoch 930, training loss: 0.6578061580657959 = 0.006825299933552742 + 0.1 * 6.50980806350708
Epoch 930, val loss: 1.1674569845199585
Epoch 940, training loss: 0.6585906744003296 = 0.00665185134857893 + 0.1 * 6.519387722015381
Epoch 940, val loss: 1.171448826789856
Epoch 950, training loss: 0.6573166251182556 = 0.006484764628112316 + 0.1 * 6.508318901062012
Epoch 950, val loss: 1.175227403640747
Epoch 960, training loss: 0.6576198935508728 = 0.006325745489448309 + 0.1 * 6.512941360473633
Epoch 960, val loss: 1.179121494293213
Epoch 970, training loss: 0.6579658389091492 = 0.006172656547278166 + 0.1 * 6.5179314613342285
Epoch 970, val loss: 1.182881236076355
Epoch 980, training loss: 0.6561358571052551 = 0.006025746464729309 + 0.1 * 6.501101016998291
Epoch 980, val loss: 1.1865721940994263
Epoch 990, training loss: 0.6560137271881104 = 0.0058849165216088295 + 0.1 * 6.501287460327148
Epoch 990, val loss: 1.1902261972427368
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 2.817293882369995 = 1.9576115608215332 + 0.1 * 8.596822738647461
Epoch 0, val loss: 1.9580118656158447
Epoch 10, training loss: 2.8060030937194824 = 1.9463307857513428 + 0.1 * 8.596722602844238
Epoch 10, val loss: 1.9462815523147583
Epoch 20, training loss: 2.792187452316284 = 1.932579517364502 + 0.1 * 8.596078872680664
Epoch 20, val loss: 1.9317377805709839
Epoch 30, training loss: 2.7724153995513916 = 1.9133050441741943 + 0.1 * 8.591102600097656
Epoch 30, val loss: 1.9113714694976807
Epoch 40, training loss: 2.7411293983459473 = 1.8850889205932617 + 0.1 * 8.560405731201172
Epoch 40, val loss: 1.882157325744629
Epoch 50, training loss: 2.686340808868408 = 1.8468371629714966 + 0.1 * 8.395035743713379
Epoch 50, val loss: 1.844783902168274
Epoch 60, training loss: 2.6234748363494873 = 1.8052500486373901 + 0.1 * 8.18224811553955
Epoch 60, val loss: 1.8081138134002686
Epoch 70, training loss: 2.560037136077881 = 1.7685102224349976 + 0.1 * 7.915268421173096
Epoch 70, val loss: 1.7784936428070068
Epoch 80, training loss: 2.4918177127838135 = 1.7310467958450317 + 0.1 * 7.607708930969238
Epoch 80, val loss: 1.7467530965805054
Epoch 90, training loss: 2.4142868518829346 = 1.6831904649734497 + 0.1 * 7.3109636306762695
Epoch 90, val loss: 1.7054824829101562
Epoch 100, training loss: 2.337205171585083 = 1.6209995746612549 + 0.1 * 7.162055969238281
Epoch 100, val loss: 1.6517901420593262
Epoch 110, training loss: 2.252058982849121 = 1.5428366661071777 + 0.1 * 7.092223644256592
Epoch 110, val loss: 1.5836888551712036
Epoch 120, training loss: 2.160022258758545 = 1.4561729431152344 + 0.1 * 7.0384931564331055
Epoch 120, val loss: 1.5107684135437012
Epoch 130, training loss: 2.066910743713379 = 1.3672338724136353 + 0.1 * 6.996768474578857
Epoch 130, val loss: 1.4367270469665527
Epoch 140, training loss: 1.974229097366333 = 1.2773994207382202 + 0.1 * 6.968297481536865
Epoch 140, val loss: 1.3627184629440308
Epoch 150, training loss: 1.8803818225860596 = 1.1851588487625122 + 0.1 * 6.952230453491211
Epoch 150, val loss: 1.2879477739334106
Epoch 160, training loss: 1.7855318784713745 = 1.0926883220672607 + 0.1 * 6.928435325622559
Epoch 160, val loss: 1.2141650915145874
Epoch 170, training loss: 1.6919910907745361 = 1.0008403062820435 + 0.1 * 6.911507606506348
Epoch 170, val loss: 1.1419107913970947
Epoch 180, training loss: 1.60202956199646 = 0.911806046962738 + 0.1 * 6.902235507965088
Epoch 180, val loss: 1.0722064971923828
Epoch 190, training loss: 1.5176066160202026 = 0.8299751877784729 + 0.1 * 6.876314163208008
Epoch 190, val loss: 1.0087782144546509
Epoch 200, training loss: 1.4441050291061401 = 0.7573738694190979 + 0.1 * 6.867311477661133
Epoch 200, val loss: 0.9532899260520935
Epoch 210, training loss: 1.3796682357788086 = 0.6947765946388245 + 0.1 * 6.848916053771973
Epoch 210, val loss: 0.9072732925415039
Epoch 220, training loss: 1.3248032331466675 = 0.6408888101577759 + 0.1 * 6.839144229888916
Epoch 220, val loss: 0.8702316880226135
Epoch 230, training loss: 1.2783430814743042 = 0.5949608087539673 + 0.1 * 6.833822727203369
Epoch 230, val loss: 0.8419154286384583
Epoch 240, training loss: 1.2365443706512451 = 0.5552523732185364 + 0.1 * 6.812919616699219
Epoch 240, val loss: 0.8205639719963074
Epoch 250, training loss: 1.2000067234039307 = 0.5200679302215576 + 0.1 * 6.799387454986572
Epoch 250, val loss: 0.8049697875976562
Epoch 260, training loss: 1.1683063507080078 = 0.488433837890625 + 0.1 * 6.798724174499512
Epoch 260, val loss: 0.7938787341117859
Epoch 270, training loss: 1.1370322704315186 = 0.45926734805107117 + 0.1 * 6.777649402618408
Epoch 270, val loss: 0.7858890891075134
Epoch 280, training loss: 1.1088924407958984 = 0.4311414659023285 + 0.1 * 6.777509689331055
Epoch 280, val loss: 0.7797664999961853
Epoch 290, training loss: 1.079084038734436 = 0.4030812680721283 + 0.1 * 6.760027885437012
Epoch 290, val loss: 0.7743933200836182
Epoch 300, training loss: 1.0502527952194214 = 0.37405121326446533 + 0.1 * 6.7620158195495605
Epoch 300, val loss: 0.769097089767456
Epoch 310, training loss: 1.0181701183319092 = 0.34371107816696167 + 0.1 * 6.744590759277344
Epoch 310, val loss: 0.7636341452598572
Epoch 320, training loss: 0.9859499931335449 = 0.312340646982193 + 0.1 * 6.736093521118164
Epoch 320, val loss: 0.7584735751152039
Epoch 330, training loss: 0.9550619125366211 = 0.2811618745326996 + 0.1 * 6.7390007972717285
Epoch 330, val loss: 0.754271924495697
Epoch 340, training loss: 0.924024760723114 = 0.251520574092865 + 0.1 * 6.72504186630249
Epoch 340, val loss: 0.7515937685966492
Epoch 350, training loss: 0.8975988626480103 = 0.2242833822965622 + 0.1 * 6.733154296875
Epoch 350, val loss: 0.750921905040741
Epoch 360, training loss: 0.8711221814155579 = 0.2001347541809082 + 0.1 * 6.709874153137207
Epoch 360, val loss: 0.7524014115333557
Epoch 370, training loss: 0.8510038256645203 = 0.17912881076335907 + 0.1 * 6.71875
Epoch 370, val loss: 0.7559642195701599
Epoch 380, training loss: 0.8315110802650452 = 0.16108161211013794 + 0.1 * 6.704294681549072
Epoch 380, val loss: 0.7614346146583557
Epoch 390, training loss: 0.8146975636482239 = 0.14551681280136108 + 0.1 * 6.691807270050049
Epoch 390, val loss: 0.7687076926231384
Epoch 400, training loss: 0.8008221387863159 = 0.13206227123737335 + 0.1 * 6.687598705291748
Epoch 400, val loss: 0.7773233652114868
Epoch 410, training loss: 0.7880456447601318 = 0.12037042528390884 + 0.1 * 6.676752090454102
Epoch 410, val loss: 0.7870498299598694
Epoch 420, training loss: 0.7778417468070984 = 0.11009122431278229 + 0.1 * 6.6775054931640625
Epoch 420, val loss: 0.7976101636886597
Epoch 430, training loss: 0.7675296068191528 = 0.10104221850633621 + 0.1 * 6.6648736000061035
Epoch 430, val loss: 0.808698296546936
Epoch 440, training loss: 0.7588715553283691 = 0.09300606697797775 + 0.1 * 6.658654689788818
Epoch 440, val loss: 0.8202000856399536
Epoch 450, training loss: 0.7521718740463257 = 0.08581075817346573 + 0.1 * 6.663610935211182
Epoch 450, val loss: 0.8320549726486206
Epoch 460, training loss: 0.7436469793319702 = 0.07935298979282379 + 0.1 * 6.642940044403076
Epoch 460, val loss: 0.8441581130027771
Epoch 470, training loss: 0.7373940348625183 = 0.07351357489824295 + 0.1 * 6.6388044357299805
Epoch 470, val loss: 0.8564241528511047
Epoch 480, training loss: 0.731605052947998 = 0.06821948289871216 + 0.1 * 6.633855819702148
Epoch 480, val loss: 0.8688440918922424
Epoch 490, training loss: 0.7276516556739807 = 0.06342601776123047 + 0.1 * 6.642256259918213
Epoch 490, val loss: 0.8812169432640076
Epoch 500, training loss: 0.7218686938285828 = 0.05908248573541641 + 0.1 * 6.627861976623535
Epoch 500, val loss: 0.8935505151748657
Epoch 510, training loss: 0.7175867557525635 = 0.055121973156929016 + 0.1 * 6.624648094177246
Epoch 510, val loss: 0.9058449268341064
Epoch 520, training loss: 0.7135869264602661 = 0.05151447653770447 + 0.1 * 6.620724201202393
Epoch 520, val loss: 0.9179180264472961
Epoch 530, training loss: 0.7092036008834839 = 0.048218585550785065 + 0.1 * 6.60984992980957
Epoch 530, val loss: 0.9299243688583374
Epoch 540, training loss: 0.7048935890197754 = 0.04519503563642502 + 0.1 * 6.596985340118408
Epoch 540, val loss: 0.9418393969535828
Epoch 550, training loss: 0.7024050354957581 = 0.04241073131561279 + 0.1 * 6.599942684173584
Epoch 550, val loss: 0.9536576867103577
Epoch 560, training loss: 0.7006250619888306 = 0.03985501080751419 + 0.1 * 6.607700347900391
Epoch 560, val loss: 0.965227484703064
Epoch 570, training loss: 0.6975612044334412 = 0.03750694915652275 + 0.1 * 6.6005425453186035
Epoch 570, val loss: 0.9766477346420288
Epoch 580, training loss: 0.6940373182296753 = 0.035348132252693176 + 0.1 * 6.586892127990723
Epoch 580, val loss: 0.9879893064498901
Epoch 590, training loss: 0.6908296942710876 = 0.03335285186767578 + 0.1 * 6.57476806640625
Epoch 590, val loss: 0.9992009997367859
Epoch 600, training loss: 0.689980685710907 = 0.03150438517332077 + 0.1 * 6.584763050079346
Epoch 600, val loss: 1.0102483034133911
Epoch 610, training loss: 0.6876227259635925 = 0.029796388000249863 + 0.1 * 6.578263282775879
Epoch 610, val loss: 1.0211752653121948
Epoch 620, training loss: 0.6852139234542847 = 0.028217395767569542 + 0.1 * 6.569965362548828
Epoch 620, val loss: 1.0318548679351807
Epoch 630, training loss: 0.6827970147132874 = 0.026754457503557205 + 0.1 * 6.560425758361816
Epoch 630, val loss: 1.042445421218872
Epoch 640, training loss: 0.6828075647354126 = 0.025396518409252167 + 0.1 * 6.574110507965088
Epoch 640, val loss: 1.052811622619629
Epoch 650, training loss: 0.6796621084213257 = 0.024133555591106415 + 0.1 * 6.555285453796387
Epoch 650, val loss: 1.0630635023117065
Epoch 660, training loss: 0.6788396835327148 = 0.02295735478401184 + 0.1 * 6.558823108673096
Epoch 660, val loss: 1.0732197761535645
Epoch 670, training loss: 0.6771816611289978 = 0.021863317117094994 + 0.1 * 6.553183078765869
Epoch 670, val loss: 1.0830563306808472
Epoch 680, training loss: 0.6769644021987915 = 0.02084556594491005 + 0.1 * 6.561187744140625
Epoch 680, val loss: 1.092812418937683
Epoch 690, training loss: 0.6745979189872742 = 0.01989702321588993 + 0.1 * 6.547008514404297
Epoch 690, val loss: 1.10231351852417
Epoch 700, training loss: 0.672854483127594 = 0.019012389704585075 + 0.1 * 6.538420677185059
Epoch 700, val loss: 1.1117455959320068
Epoch 710, training loss: 0.671850323677063 = 0.01818436197936535 + 0.1 * 6.536659240722656
Epoch 710, val loss: 1.1208865642547607
Epoch 720, training loss: 0.6717039942741394 = 0.017409436404705048 + 0.1 * 6.542945384979248
Epoch 720, val loss: 1.1299363374710083
Epoch 730, training loss: 0.6692038774490356 = 0.016683999449014664 + 0.1 * 6.525198459625244
Epoch 730, val loss: 1.1388379335403442
Epoch 740, training loss: 0.6698381900787354 = 0.016002744436264038 + 0.1 * 6.53835391998291
Epoch 740, val loss: 1.1475635766983032
Epoch 750, training loss: 0.6690507531166077 = 0.015363894402980804 + 0.1 * 6.536868572235107
Epoch 750, val loss: 1.1560989618301392
Epoch 760, training loss: 0.6675961017608643 = 0.014763493090867996 + 0.1 * 6.528326034545898
Epoch 760, val loss: 1.164490818977356
Epoch 770, training loss: 0.6666218638420105 = 0.014198792167007923 + 0.1 * 6.52423095703125
Epoch 770, val loss: 1.1727263927459717
Epoch 780, training loss: 0.6651743650436401 = 0.013666680082678795 + 0.1 * 6.515076637268066
Epoch 780, val loss: 1.180884599685669
Epoch 790, training loss: 0.6660785675048828 = 0.013164347968995571 + 0.1 * 6.529141902923584
Epoch 790, val loss: 1.1889184713363647
Epoch 800, training loss: 0.6643255352973938 = 0.012690041214227676 + 0.1 * 6.516354560852051
Epoch 800, val loss: 1.1966639757156372
Epoch 810, training loss: 0.6627395749092102 = 0.01224240381270647 + 0.1 * 6.504971981048584
Epoch 810, val loss: 1.2043997049331665
Epoch 820, training loss: 0.6637252569198608 = 0.011819104664027691 + 0.1 * 6.519061088562012
Epoch 820, val loss: 1.2120230197906494
Epoch 830, training loss: 0.6620807647705078 = 0.011418222449719906 + 0.1 * 6.506625175476074
Epoch 830, val loss: 1.2193982601165771
Epoch 840, training loss: 0.662648618221283 = 0.011038986966013908 + 0.1 * 6.516096591949463
Epoch 840, val loss: 1.226650595664978
Epoch 850, training loss: 0.6613831520080566 = 0.010680791921913624 + 0.1 * 6.507023334503174
Epoch 850, val loss: 1.233828067779541
Epoch 860, training loss: 0.6604757905006409 = 0.010340239852666855 + 0.1 * 6.5013556480407715
Epoch 860, val loss: 1.240870475769043
Epoch 870, training loss: 0.6597119569778442 = 0.010016322135925293 + 0.1 * 6.4969563484191895
Epoch 870, val loss: 1.2477641105651855
Epoch 880, training loss: 0.6587151288986206 = 0.009708678349852562 + 0.1 * 6.4900641441345215
Epoch 880, val loss: 1.2546614408493042
Epoch 890, training loss: 0.6583260297775269 = 0.009415946900844574 + 0.1 * 6.489100933074951
Epoch 890, val loss: 1.261239767074585
Epoch 900, training loss: 0.6591798663139343 = 0.009137030690908432 + 0.1 * 6.500428199768066
Epoch 900, val loss: 1.267874836921692
Epoch 910, training loss: 0.6573286652565002 = 0.008871565572917461 + 0.1 * 6.4845709800720215
Epoch 910, val loss: 1.2742969989776611
Epoch 920, training loss: 0.6594483852386475 = 0.008618634194135666 + 0.1 * 6.508296966552734
Epoch 920, val loss: 1.280653476715088
Epoch 930, training loss: 0.6567729115486145 = 0.008376771584153175 + 0.1 * 6.48396110534668
Epoch 930, val loss: 1.2868516445159912
Epoch 940, training loss: 0.6569110155105591 = 0.008146344684064388 + 0.1 * 6.487646579742432
Epoch 940, val loss: 1.2930101156234741
Epoch 950, training loss: 0.6565079092979431 = 0.007926151156425476 + 0.1 * 6.4858174324035645
Epoch 950, val loss: 1.2990251779556274
Epoch 960, training loss: 0.6566715240478516 = 0.007715436629951 + 0.1 * 6.489560604095459
Epoch 960, val loss: 1.3050591945648193
Epoch 970, training loss: 0.6552453637123108 = 0.0075141191482543945 + 0.1 * 6.4773125648498535
Epoch 970, val loss: 1.3107727766036987
Epoch 980, training loss: 0.654164731502533 = 0.007321090437471867 + 0.1 * 6.468436241149902
Epoch 980, val loss: 1.3166372776031494
Epoch 990, training loss: 0.655870258808136 = 0.007135963533073664 + 0.1 * 6.487342834472656
Epoch 990, val loss: 1.3222987651824951
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 2.8049468994140625 = 1.94527006149292 + 0.1 * 8.59676742553711
Epoch 0, val loss: 1.9428468942642212
Epoch 10, training loss: 2.7939624786376953 = 1.9343074560165405 + 0.1 * 8.596549034118652
Epoch 10, val loss: 1.9316024780273438
Epoch 20, training loss: 2.7799463272094727 = 1.920437216758728 + 0.1 * 8.59508991241455
Epoch 20, val loss: 1.917205810546875
Epoch 30, training loss: 2.7590959072113037 = 1.9007234573364258 + 0.1 * 8.583724975585938
Epoch 30, val loss: 1.8969511985778809
Epoch 40, training loss: 2.7243287563323975 = 1.872286319732666 + 0.1 * 8.520424842834473
Epoch 40, val loss: 1.868711233139038
Epoch 50, training loss: 2.660566806793213 = 1.8359907865524292 + 0.1 * 8.245759010314941
Epoch 50, val loss: 1.8349915742874146
Epoch 60, training loss: 2.5928940773010254 = 1.7986961603164673 + 0.1 * 7.941980361938477
Epoch 60, val loss: 1.8027364015579224
Epoch 70, training loss: 2.510402202606201 = 1.7643330097198486 + 0.1 * 7.460690498352051
Epoch 70, val loss: 1.7731791734695435
Epoch 80, training loss: 2.445814609527588 = 1.7266451120376587 + 0.1 * 7.1916961669921875
Epoch 80, val loss: 1.7399553060531616
Epoch 90, training loss: 2.3857085704803467 = 1.6767560243606567 + 0.1 * 7.0895256996154785
Epoch 90, val loss: 1.6951649188995361
Epoch 100, training loss: 2.31374454498291 = 1.6107996702194214 + 0.1 * 7.029449462890625
Epoch 100, val loss: 1.6368567943572998
Epoch 110, training loss: 2.230377197265625 = 1.532657504081726 + 0.1 * 6.977197647094727
Epoch 110, val loss: 1.5716748237609863
Epoch 120, training loss: 2.1443727016448975 = 1.4513810873031616 + 0.1 * 6.929916858673096
Epoch 120, val loss: 1.5044729709625244
Epoch 130, training loss: 2.0617449283599854 = 1.3720529079437256 + 0.1 * 6.8969197273254395
Epoch 130, val loss: 1.4406534433364868
Epoch 140, training loss: 1.982501745223999 = 1.2956230640411377 + 0.1 * 6.868785858154297
Epoch 140, val loss: 1.3801146745681763
Epoch 150, training loss: 1.9052722454071045 = 1.2203017473220825 + 0.1 * 6.849704265594482
Epoch 150, val loss: 1.3210241794586182
Epoch 160, training loss: 1.8292864561080933 = 1.145904541015625 + 0.1 * 6.8338189125061035
Epoch 160, val loss: 1.2645115852355957
Epoch 170, training loss: 1.7513575553894043 = 1.069526195526123 + 0.1 * 6.818313121795654
Epoch 170, val loss: 1.2078672647476196
Epoch 180, training loss: 1.6703975200653076 = 0.9897391200065613 + 0.1 * 6.806584358215332
Epoch 180, val loss: 1.1486810445785522
Epoch 190, training loss: 1.5869028568267822 = 0.9074309468269348 + 0.1 * 6.794719696044922
Epoch 190, val loss: 1.0872985124588013
Epoch 200, training loss: 1.5035614967346191 = 0.824918806552887 + 0.1 * 6.786427021026611
Epoch 200, val loss: 1.0257887840270996
Epoch 210, training loss: 1.4249974489212036 = 0.747786819934845 + 0.1 * 6.772106170654297
Epoch 210, val loss: 0.9695358872413635
Epoch 220, training loss: 1.3558223247528076 = 0.6790173649787903 + 0.1 * 6.7680487632751465
Epoch 220, val loss: 0.921752393245697
Epoch 230, training loss: 1.294325590133667 = 0.620102047920227 + 0.1 * 6.7422356605529785
Epoch 230, val loss: 0.884156346321106
Epoch 240, training loss: 1.242692470550537 = 0.5694452524185181 + 0.1 * 6.7324724197387695
Epoch 240, val loss: 0.855978786945343
Epoch 250, training loss: 1.1980905532836914 = 0.5253245830535889 + 0.1 * 6.727658748626709
Epoch 250, val loss: 0.8360084891319275
Epoch 260, training loss: 1.157414197921753 = 0.4865340292453766 + 0.1 * 6.708801746368408
Epoch 260, val loss: 0.8225222826004028
Epoch 270, training loss: 1.1207234859466553 = 0.45108872652053833 + 0.1 * 6.696346759796143
Epoch 270, val loss: 0.8134562969207764
Epoch 280, training loss: 1.087390422821045 = 0.4179391860961914 + 0.1 * 6.694511890411377
Epoch 280, val loss: 0.8072099089622498
Epoch 290, training loss: 1.0548264980316162 = 0.3868647515773773 + 0.1 * 6.679617404937744
Epoch 290, val loss: 0.8026337027549744
Epoch 300, training loss: 1.0242725610733032 = 0.3573879897594452 + 0.1 * 6.6688456535339355
Epoch 300, val loss: 0.7990927696228027
Epoch 310, training loss: 0.9962425827980042 = 0.3291807174682617 + 0.1 * 6.670618534088135
Epoch 310, val loss: 0.7963825464248657
Epoch 320, training loss: 0.9677488803863525 = 0.3022271692752838 + 0.1 * 6.655217170715332
Epoch 320, val loss: 0.7944110035896301
Epoch 330, training loss: 0.9422609806060791 = 0.2763611376285553 + 0.1 * 6.658998489379883
Epoch 330, val loss: 0.793416440486908
Epoch 340, training loss: 0.9163028001785278 = 0.25182196497917175 + 0.1 * 6.644808292388916
Epoch 340, val loss: 0.7935755252838135
Epoch 350, training loss: 0.8926910161972046 = 0.22890174388885498 + 0.1 * 6.637892723083496
Epoch 350, val loss: 0.7954385280609131
Epoch 360, training loss: 0.870620846748352 = 0.2078792154788971 + 0.1 * 6.627416133880615
Epoch 360, val loss: 0.7991830110549927
Epoch 370, training loss: 0.8519231081008911 = 0.18894091248512268 + 0.1 * 6.629822254180908
Epoch 370, val loss: 0.8050296902656555
Epoch 380, training loss: 0.8348564505577087 = 0.17215107381343842 + 0.1 * 6.627053260803223
Epoch 380, val loss: 0.812547504901886
Epoch 390, training loss: 0.8183891773223877 = 0.15725192427635193 + 0.1 * 6.611372470855713
Epoch 390, val loss: 0.8216255903244019
Epoch 400, training loss: 0.8049863576889038 = 0.14396578073501587 + 0.1 * 6.61020565032959
Epoch 400, val loss: 0.8319937586784363
Epoch 410, training loss: 0.7927791476249695 = 0.13211844861507416 + 0.1 * 6.606606960296631
Epoch 410, val loss: 0.8434154987335205
Epoch 420, training loss: 0.7818058729171753 = 0.1215457022190094 + 0.1 * 6.602601051330566
Epoch 420, val loss: 0.8554940223693848
Epoch 430, training loss: 0.7713605761528015 = 0.11206791549921036 + 0.1 * 6.592926025390625
Epoch 430, val loss: 0.8680400848388672
Epoch 440, training loss: 0.761760413646698 = 0.10350122302770615 + 0.1 * 6.582592010498047
Epoch 440, val loss: 0.8810586333274841
Epoch 450, training loss: 0.7535149455070496 = 0.09574717283248901 + 0.1 * 6.5776777267456055
Epoch 450, val loss: 0.8944351673126221
Epoch 460, training loss: 0.746494710445404 = 0.08872298896312714 + 0.1 * 6.577716827392578
Epoch 460, val loss: 0.9079583287239075
Epoch 470, training loss: 0.7391248345375061 = 0.08233547955751419 + 0.1 * 6.5678935050964355
Epoch 470, val loss: 0.9215989708900452
Epoch 480, training loss: 0.7332234382629395 = 0.07651443779468536 + 0.1 * 6.567090034484863
Epoch 480, val loss: 0.9352343678474426
Epoch 490, training loss: 0.7277982831001282 = 0.07118431478738785 + 0.1 * 6.566139221191406
Epoch 490, val loss: 0.9489405751228333
Epoch 500, training loss: 0.7226598262786865 = 0.06631666421890259 + 0.1 * 6.563431739807129
Epoch 500, val loss: 0.9625228047370911
Epoch 510, training loss: 0.7180187702178955 = 0.06186268478631973 + 0.1 * 6.56156063079834
Epoch 510, val loss: 0.9759544134140015
Epoch 520, training loss: 0.7137660980224609 = 0.057778164744377136 + 0.1 * 6.559878826141357
Epoch 520, val loss: 0.9892374873161316
Epoch 530, training loss: 0.7095882892608643 = 0.054038774222135544 + 0.1 * 6.555495262145996
Epoch 530, val loss: 1.0023521184921265
Epoch 540, training loss: 0.7058485150337219 = 0.0506146214902401 + 0.1 * 6.55233907699585
Epoch 540, val loss: 1.0151842832565308
Epoch 550, training loss: 0.7012388110160828 = 0.047466304153203964 + 0.1 * 6.53772497177124
Epoch 550, val loss: 1.0279189348220825
Epoch 560, training loss: 0.7005314230918884 = 0.04456983879208565 + 0.1 * 6.559615612030029
Epoch 560, val loss: 1.0404454469680786
Epoch 570, training loss: 0.6952479481697083 = 0.04191465675830841 + 0.1 * 6.533332824707031
Epoch 570, val loss: 1.0526524782180786
Epoch 580, training loss: 0.6948202848434448 = 0.0394708551466465 + 0.1 * 6.553494453430176
Epoch 580, val loss: 1.0646998882293701
Epoch 590, training loss: 0.6899966597557068 = 0.037224724888801575 + 0.1 * 6.527719497680664
Epoch 590, val loss: 1.0764005184173584
Epoch 600, training loss: 0.6882703304290771 = 0.03515032306313515 + 0.1 * 6.5311994552612305
Epoch 600, val loss: 1.087904930114746
Epoch 610, training loss: 0.6852704882621765 = 0.03323749825358391 + 0.1 * 6.520329475402832
Epoch 610, val loss: 1.0991333723068237
Epoch 620, training loss: 0.6827974319458008 = 0.0314716175198555 + 0.1 * 6.51325798034668
Epoch 620, val loss: 1.110068678855896
Epoch 630, training loss: 0.6820191740989685 = 0.02983509935438633 + 0.1 * 6.521840572357178
Epoch 630, val loss: 1.1208269596099854
Epoch 640, training loss: 0.6796224117279053 = 0.028319695964455605 + 0.1 * 6.513026714324951
Epoch 640, val loss: 1.1313457489013672
Epoch 650, training loss: 0.6787146925926208 = 0.02691425383090973 + 0.1 * 6.518004417419434
Epoch 650, val loss: 1.1415883302688599
Epoch 660, training loss: 0.6765182614326477 = 0.02560880221426487 + 0.1 * 6.509094715118408
Epoch 660, val loss: 1.1516083478927612
Epoch 670, training loss: 0.6751127243041992 = 0.02439335361123085 + 0.1 * 6.507193565368652
Epoch 670, val loss: 1.1613956689834595
Epoch 680, training loss: 0.6733765006065369 = 0.02326187677681446 + 0.1 * 6.501145839691162
Epoch 680, val loss: 1.170966625213623
Epoch 690, training loss: 0.6722783446311951 = 0.02220914699137211 + 0.1 * 6.5006914138793945
Epoch 690, val loss: 1.180282473564148
Epoch 700, training loss: 0.6707028150558472 = 0.021225683391094208 + 0.1 * 6.4947710037231445
Epoch 700, val loss: 1.1894172430038452
Epoch 710, training loss: 0.6697173118591309 = 0.020306715741753578 + 0.1 * 6.494105815887451
Epoch 710, val loss: 1.1983301639556885
Epoch 720, training loss: 0.6687072515487671 = 0.019446270540356636 + 0.1 * 6.492609977722168
Epoch 720, val loss: 1.2070887088775635
Epoch 730, training loss: 0.6671505570411682 = 0.018639545887708664 + 0.1 * 6.485109806060791
Epoch 730, val loss: 1.2156808376312256
Epoch 740, training loss: 0.666036069393158 = 0.017883896827697754 + 0.1 * 6.4815216064453125
Epoch 740, val loss: 1.2240575551986694
Epoch 750, training loss: 0.6656590700149536 = 0.01717286929488182 + 0.1 * 6.484861850738525
Epoch 750, val loss: 1.2322850227355957
Epoch 760, training loss: 0.6653544902801514 = 0.016504906117916107 + 0.1 * 6.488495826721191
Epoch 760, val loss: 1.240301251411438
Epoch 770, training loss: 0.6638293862342834 = 0.015878140926361084 + 0.1 * 6.4795122146606445
Epoch 770, val loss: 1.248155951499939
Epoch 780, training loss: 0.6631874442100525 = 0.015288392081856728 + 0.1 * 6.478990077972412
Epoch 780, val loss: 1.2558062076568604
Epoch 790, training loss: 0.661445677280426 = 0.014730596914887428 + 0.1 * 6.467150688171387
Epoch 790, val loss: 1.263351321220398
Epoch 800, training loss: 0.6634641289710999 = 0.014203213155269623 + 0.1 * 6.49260950088501
Epoch 800, val loss: 1.2707535028457642
Epoch 810, training loss: 0.660970151424408 = 0.013705277815461159 + 0.1 * 6.472649097442627
Epoch 810, val loss: 1.277956485748291
Epoch 820, training loss: 0.6609468460083008 = 0.013234532438218594 + 0.1 * 6.477122783660889
Epoch 820, val loss: 1.2849783897399902
Epoch 830, training loss: 0.6592679023742676 = 0.012789987958967686 + 0.1 * 6.464779376983643
Epoch 830, val loss: 1.2918211221694946
Epoch 840, training loss: 0.6596793532371521 = 0.012367842718958855 + 0.1 * 6.473114967346191
Epoch 840, val loss: 1.2985527515411377
Epoch 850, training loss: 0.6575330495834351 = 0.011967362836003304 + 0.1 * 6.4556565284729
Epoch 850, val loss: 1.3051677942276
Epoch 860, training loss: 0.6585521101951599 = 0.011586539447307587 + 0.1 * 6.469655513763428
Epoch 860, val loss: 1.3116422891616821
Epoch 870, training loss: 0.6570711731910706 = 0.011224616318941116 + 0.1 * 6.458465099334717
Epoch 870, val loss: 1.3180029392242432
Epoch 880, training loss: 0.6569514274597168 = 0.01088008377701044 + 0.1 * 6.460712909698486
Epoch 880, val loss: 1.3242621421813965
Epoch 890, training loss: 0.657239556312561 = 0.01055274810642004 + 0.1 * 6.466867923736572
Epoch 890, val loss: 1.33036470413208
Epoch 900, training loss: 0.6552459597587585 = 0.010240850038826466 + 0.1 * 6.4500508308410645
Epoch 900, val loss: 1.3364098072052002
Epoch 910, training loss: 0.6576996445655823 = 0.009943077340722084 + 0.1 * 6.477565765380859
Epoch 910, val loss: 1.3423320055007935
Epoch 920, training loss: 0.654609739780426 = 0.00965938065201044 + 0.1 * 6.449503421783447
Epoch 920, val loss: 1.3480870723724365
Epoch 930, training loss: 0.6534689664840698 = 0.009388944134116173 + 0.1 * 6.440800189971924
Epoch 930, val loss: 1.3537495136260986
Epoch 940, training loss: 0.6552426815032959 = 0.009129888378083706 + 0.1 * 6.461127758026123
Epoch 940, val loss: 1.3593292236328125
Epoch 950, training loss: 0.6538151502609253 = 0.008882595226168633 + 0.1 * 6.4493255615234375
Epoch 950, val loss: 1.3648324012756348
Epoch 960, training loss: 0.6543415188789368 = 0.008645928464829922 + 0.1 * 6.456955432891846
Epoch 960, val loss: 1.3702174425125122
Epoch 970, training loss: 0.6526859998703003 = 0.008419574238359928 + 0.1 * 6.44266414642334
Epoch 970, val loss: 1.3754937648773193
Epoch 980, training loss: 0.6520431637763977 = 0.008202724158763885 + 0.1 * 6.438404083251953
Epoch 980, val loss: 1.3806872367858887
Epoch 990, training loss: 0.6520680785179138 = 0.007994442246854305 + 0.1 * 6.440736770629883
Epoch 990, val loss: 1.3858284950256348
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8376383763837639
The final CL Acc:0.79630, 0.00605, The final GNN Acc:0.83922, 0.00188
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9450])
updated graph: torch.Size([2, 10518])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8131418228149414 = 1.9534578323364258 + 0.1 * 8.596839904785156
Epoch 0, val loss: 1.937011480331421
Epoch 10, training loss: 2.8022336959838867 = 1.9425554275512695 + 0.1 * 8.596783638000488
Epoch 10, val loss: 1.9266610145568848
Epoch 20, training loss: 2.789167881011963 = 1.9295239448547363 + 0.1 * 8.596440315246582
Epoch 20, val loss: 1.913873314857483
Epoch 30, training loss: 2.7710306644439697 = 1.9116686582565308 + 0.1 * 8.593620300292969
Epoch 30, val loss: 1.8959639072418213
Epoch 40, training loss: 2.7431962490081787 = 1.8858976364135742 + 0.1 * 8.572986602783203
Epoch 40, val loss: 1.8701629638671875
Epoch 50, training loss: 2.697110414505005 = 1.8508195877075195 + 0.1 * 8.462908744812012
Epoch 50, val loss: 1.836893081665039
Epoch 60, training loss: 2.64455246925354 = 1.8128210306167603 + 0.1 * 8.317315101623535
Epoch 60, val loss: 1.8045529127120972
Epoch 70, training loss: 2.584338426589966 = 1.7806499004364014 + 0.1 * 8.036884307861328
Epoch 70, val loss: 1.7796525955200195
Epoch 80, training loss: 2.498291492462158 = 1.7492419481277466 + 0.1 * 7.490495204925537
Epoch 80, val loss: 1.7534550428390503
Epoch 90, training loss: 2.4311158657073975 = 1.7109416723251343 + 0.1 * 7.201741695404053
Epoch 90, val loss: 1.7211577892303467
Epoch 100, training loss: 2.3675007820129395 = 1.6589744091033936 + 0.1 * 7.085262298583984
Epoch 100, val loss: 1.6769710779190063
Epoch 110, training loss: 2.2919704914093018 = 1.5895634889602661 + 0.1 * 7.024069786071777
Epoch 110, val loss: 1.6164575815200806
Epoch 120, training loss: 2.20633602142334 = 1.5074824094772339 + 0.1 * 6.988534927368164
Epoch 120, val loss: 1.548749566078186
Epoch 130, training loss: 2.116820812225342 = 1.4210025072097778 + 0.1 * 6.958183765411377
Epoch 130, val loss: 1.4802923202514648
Epoch 140, training loss: 2.0293660163879395 = 1.335721731185913 + 0.1 * 6.9364423751831055
Epoch 140, val loss: 1.4158259630203247
Epoch 150, training loss: 1.947584629058838 = 1.2560330629348755 + 0.1 * 6.915516376495361
Epoch 150, val loss: 1.3593841791152954
Epoch 160, training loss: 1.8704397678375244 = 1.1803398132324219 + 0.1 * 6.900999069213867
Epoch 160, val loss: 1.3085774183273315
Epoch 170, training loss: 1.7948154211044312 = 1.1058704853057861 + 0.1 * 6.889449119567871
Epoch 170, val loss: 1.2599438428878784
Epoch 180, training loss: 1.7196414470672607 = 1.0317459106445312 + 0.1 * 6.878955841064453
Epoch 180, val loss: 1.2123147249221802
Epoch 190, training loss: 1.6439876556396484 = 0.9571512937545776 + 0.1 * 6.868363857269287
Epoch 190, val loss: 1.1639893054962158
Epoch 200, training loss: 1.5714361667633057 = 0.884092390537262 + 0.1 * 6.873437881469727
Epoch 200, val loss: 1.1162054538726807
Epoch 210, training loss: 1.5010435581207275 = 0.8155970573425293 + 0.1 * 6.854464530944824
Epoch 210, val loss: 1.071300983428955
Epoch 220, training loss: 1.4354162216186523 = 0.7510931491851807 + 0.1 * 6.8432297706604
Epoch 220, val loss: 1.029106616973877
Epoch 230, training loss: 1.374485969543457 = 0.6910039186477661 + 0.1 * 6.834820747375488
Epoch 230, val loss: 0.9907433390617371
Epoch 240, training loss: 1.3198974132537842 = 0.6357378959655762 + 0.1 * 6.841594696044922
Epoch 240, val loss: 0.9569454789161682
Epoch 250, training loss: 1.2681005001068115 = 0.5862290263175964 + 0.1 * 6.818714618682861
Epoch 250, val loss: 0.928499698638916
Epoch 260, training loss: 1.222022533416748 = 0.5408730506896973 + 0.1 * 6.81149435043335
Epoch 260, val loss: 0.9043671488761902
Epoch 270, training loss: 1.1837395429611206 = 0.498694509267807 + 0.1 * 6.850450038909912
Epoch 270, val loss: 0.8842052817344666
Epoch 280, training loss: 1.139897108078003 = 0.45998936891555786 + 0.1 * 6.79907751083374
Epoch 280, val loss: 0.867810070514679
Epoch 290, training loss: 1.1024599075317383 = 0.4237571358680725 + 0.1 * 6.787026882171631
Epoch 290, val loss: 0.854783833026886
Epoch 300, training loss: 1.0677322149276733 = 0.389906644821167 + 0.1 * 6.778255462646484
Epoch 300, val loss: 0.8450555801391602
Epoch 310, training loss: 1.0357260704040527 = 0.35868698358535767 + 0.1 * 6.770391464233398
Epoch 310, val loss: 0.8385301232337952
Epoch 320, training loss: 1.0079094171524048 = 0.3305135667324066 + 0.1 * 6.773958206176758
Epoch 320, val loss: 0.8349428772926331
Epoch 330, training loss: 0.9805431365966797 = 0.30534449219703674 + 0.1 * 6.751986026763916
Epoch 330, val loss: 0.8337305784225464
Epoch 340, training loss: 0.9585099220275879 = 0.28271034359931946 + 0.1 * 6.757996082305908
Epoch 340, val loss: 0.8344616293907166
Epoch 350, training loss: 0.9365918636322021 = 0.2622283399105072 + 0.1 * 6.7436347007751465
Epoch 350, val loss: 0.8365899324417114
Epoch 360, training loss: 0.9160668849945068 = 0.24324525892734528 + 0.1 * 6.728216171264648
Epoch 360, val loss: 0.839531660079956
Epoch 370, training loss: 0.8998768925666809 = 0.2252701371908188 + 0.1 * 6.746067523956299
Epoch 370, val loss: 0.8429292440414429
Epoch 380, training loss: 0.8800911903381348 = 0.2081143856048584 + 0.1 * 6.719768047332764
Epoch 380, val loss: 0.8463868498802185
Epoch 390, training loss: 0.8625355958938599 = 0.19145236909389496 + 0.1 * 6.710832118988037
Epoch 390, val loss: 0.8496218323707581
Epoch 400, training loss: 0.8452305197715759 = 0.1752290576696396 + 0.1 * 6.700014114379883
Epoch 400, val loss: 0.8525647521018982
Epoch 410, training loss: 0.829551637172699 = 0.15952391922473907 + 0.1 * 6.700277328491211
Epoch 410, val loss: 0.8552368879318237
Epoch 420, training loss: 0.8152846097946167 = 0.1444869190454483 + 0.1 * 6.707976818084717
Epoch 420, val loss: 0.8578579425811768
Epoch 430, training loss: 0.7987344264984131 = 0.13039818406105042 + 0.1 * 6.683362007141113
Epoch 430, val loss: 0.8604703545570374
Epoch 440, training loss: 0.787742555141449 = 0.11743368208408356 + 0.1 * 6.703088760375977
Epoch 440, val loss: 0.8633708953857422
Epoch 450, training loss: 0.7732452750205994 = 0.10577364265918732 + 0.1 * 6.674716472625732
Epoch 450, val loss: 0.8666665554046631
Epoch 460, training loss: 0.7640337347984314 = 0.09535516053438187 + 0.1 * 6.686785697937012
Epoch 460, val loss: 0.8704496622085571
Epoch 470, training loss: 0.7529670000076294 = 0.08612747490406036 + 0.1 * 6.668395519256592
Epoch 470, val loss: 0.8745687007904053
Epoch 480, training loss: 0.7434976696968079 = 0.07796136289834976 + 0.1 * 6.655362606048584
Epoch 480, val loss: 0.8791519403457642
Epoch 490, training loss: 0.7370244264602661 = 0.07073266059160233 + 0.1 * 6.662917613983154
Epoch 490, val loss: 0.8840476274490356
Epoch 500, training loss: 0.7291443943977356 = 0.06435360014438629 + 0.1 * 6.6479082107543945
Epoch 500, val loss: 0.889288067817688
Epoch 510, training loss: 0.7235469818115234 = 0.058709561824798584 + 0.1 * 6.648374080657959
Epoch 510, val loss: 0.8946496844291687
Epoch 520, training loss: 0.7174720168113708 = 0.05372069776058197 + 0.1 * 6.637512683868408
Epoch 520, val loss: 0.9000979661941528
Epoch 530, training loss: 0.7137789130210876 = 0.049295227974653244 + 0.1 * 6.64483642578125
Epoch 530, val loss: 0.9057143330574036
Epoch 540, training loss: 0.7080960273742676 = 0.04537125304341316 + 0.1 * 6.6272478103637695
Epoch 540, val loss: 0.9113637804985046
Epoch 550, training loss: 0.7058504819869995 = 0.041874513030052185 + 0.1 * 6.639759540557861
Epoch 550, val loss: 0.9170645475387573
Epoch 560, training loss: 0.7016454935073853 = 0.03876044228672981 + 0.1 * 6.62885046005249
Epoch 560, val loss: 0.9228148460388184
Epoch 570, training loss: 0.6970614194869995 = 0.03597785159945488 + 0.1 * 6.610835552215576
Epoch 570, val loss: 0.9285076856613159
Epoch 580, training loss: 0.6961307525634766 = 0.03347698599100113 + 0.1 * 6.626537322998047
Epoch 580, val loss: 0.9342337250709534
Epoch 590, training loss: 0.6933409571647644 = 0.031239014118909836 + 0.1 * 6.62101936340332
Epoch 590, val loss: 0.9399678707122803
Epoch 600, training loss: 0.6896375417709351 = 0.029225118458271027 + 0.1 * 6.604124069213867
Epoch 600, val loss: 0.945410430431366
Epoch 610, training loss: 0.687949538230896 = 0.02739964798092842 + 0.1 * 6.605498790740967
Epoch 610, val loss: 0.9510052800178528
Epoch 620, training loss: 0.6862047910690308 = 0.025746334344148636 + 0.1 * 6.604584217071533
Epoch 620, val loss: 0.95659339427948
Epoch 630, training loss: 0.6842790842056274 = 0.024246342480182648 + 0.1 * 6.600327491760254
Epoch 630, val loss: 0.9619015455245972
Epoch 640, training loss: 0.6824234127998352 = 0.022878073155879974 + 0.1 * 6.595453262329102
Epoch 640, val loss: 0.9673125147819519
Epoch 650, training loss: 0.6813515424728394 = 0.021625854074954987 + 0.1 * 6.597256660461426
Epoch 650, val loss: 0.9725629091262817
Epoch 660, training loss: 0.6792886853218079 = 0.020477842539548874 + 0.1 * 6.588108539581299
Epoch 660, val loss: 0.9778459072113037
Epoch 670, training loss: 0.6808696985244751 = 0.01942366547882557 + 0.1 * 6.614460468292236
Epoch 670, val loss: 0.9829546809196472
Epoch 680, training loss: 0.6771304607391357 = 0.018456224352121353 + 0.1 * 6.586742401123047
Epoch 680, val loss: 0.9880651235580444
Epoch 690, training loss: 0.6749439835548401 = 0.01756293885409832 + 0.1 * 6.573810577392578
Epoch 690, val loss: 0.9929602742195129
Epoch 700, training loss: 0.6739979982376099 = 0.016735993325710297 + 0.1 * 6.572620391845703
Epoch 700, val loss: 0.9979467391967773
Epoch 710, training loss: 0.6733095645904541 = 0.015970837324857712 + 0.1 * 6.573387145996094
Epoch 710, val loss: 1.002670168876648
Epoch 720, training loss: 0.6737310886383057 = 0.015260378830134869 + 0.1 * 6.584706783294678
Epoch 720, val loss: 1.0074292421340942
Epoch 730, training loss: 0.6718019843101501 = 0.014599494636058807 + 0.1 * 6.572024822235107
Epoch 730, val loss: 1.0121153593063354
Epoch 740, training loss: 0.6706292033195496 = 0.01398413348942995 + 0.1 * 6.566450595855713
Epoch 740, val loss: 1.0165430307388306
Epoch 750, training loss: 0.6695618629455566 = 0.013409045524895191 + 0.1 * 6.561528205871582
Epoch 750, val loss: 1.0211331844329834
Epoch 760, training loss: 0.6697502732276917 = 0.012872224673628807 + 0.1 * 6.568780422210693
Epoch 760, val loss: 1.0254974365234375
Epoch 770, training loss: 0.6677342057228088 = 0.012371211312711239 + 0.1 * 6.5536298751831055
Epoch 770, val loss: 1.029775619506836
Epoch 780, training loss: 0.6670113801956177 = 0.011899598874151707 + 0.1 * 6.551117897033691
Epoch 780, val loss: 1.0339856147766113
Epoch 790, training loss: 0.6673492789268494 = 0.011457741260528564 + 0.1 * 6.558915138244629
Epoch 790, val loss: 1.0381677150726318
Epoch 800, training loss: 0.6657097339630127 = 0.0110426414757967 + 0.1 * 6.546670913696289
Epoch 800, val loss: 1.0422382354736328
Epoch 810, training loss: 0.6670064330101013 = 0.010650141164660454 + 0.1 * 6.563562870025635
Epoch 810, val loss: 1.0462310314178467
Epoch 820, training loss: 0.6647288799285889 = 0.010282362811267376 + 0.1 * 6.544465065002441
Epoch 820, val loss: 1.0501803159713745
Epoch 830, training loss: 0.6635230779647827 = 0.009934371337294579 + 0.1 * 6.535886764526367
Epoch 830, val loss: 1.0539536476135254
Epoch 840, training loss: 0.6641528010368347 = 0.009605152532458305 + 0.1 * 6.54547643661499
Epoch 840, val loss: 1.0576838254928589
Epoch 850, training loss: 0.6651411652565002 = 0.009293450973927975 + 0.1 * 6.55847692489624
Epoch 850, val loss: 1.0614831447601318
Epoch 860, training loss: 0.6640442609786987 = 0.008999749086797237 + 0.1 * 6.550445079803467
Epoch 860, val loss: 1.0651237964630127
Epoch 870, training loss: 0.6617470383644104 = 0.008721726015210152 + 0.1 * 6.530252933502197
Epoch 870, val loss: 1.0685926675796509
Epoch 880, training loss: 0.6618693470954895 = 0.008456428535282612 + 0.1 * 6.5341291427612305
Epoch 880, val loss: 1.072030782699585
Epoch 890, training loss: 0.6621260046958923 = 0.008205361664295197 + 0.1 * 6.539206504821777
Epoch 890, val loss: 1.0756033658981323
Epoch 900, training loss: 0.6600583791732788 = 0.007966692559421062 + 0.1 * 6.52091646194458
Epoch 900, val loss: 1.078872561454773
Epoch 910, training loss: 0.6610214710235596 = 0.007737958338111639 + 0.1 * 6.532835006713867
Epoch 910, val loss: 1.082174301147461
Epoch 920, training loss: 0.6592386364936829 = 0.0075217499397695065 + 0.1 * 6.517168998718262
Epoch 920, val loss: 1.0854898691177368
Epoch 930, training loss: 0.6595972180366516 = 0.007314345333725214 + 0.1 * 6.522828578948975
Epoch 930, val loss: 1.0886917114257812
Epoch 940, training loss: 0.6585102081298828 = 0.007117507513612509 + 0.1 * 6.513926982879639
Epoch 940, val loss: 1.0918153524398804
Epoch 950, training loss: 0.6585375666618347 = 0.006928641814738512 + 0.1 * 6.51608943939209
Epoch 950, val loss: 1.09491765499115
Epoch 960, training loss: 0.6581761837005615 = 0.0067492881789803505 + 0.1 * 6.51426887512207
Epoch 960, val loss: 1.097916603088379
Epoch 970, training loss: 0.657872200012207 = 0.006576784886419773 + 0.1 * 6.512954235076904
Epoch 970, val loss: 1.1009069681167603
Epoch 980, training loss: 0.657390296459198 = 0.006411561276763678 + 0.1 * 6.509787082672119
Epoch 980, val loss: 1.1038589477539062
Epoch 990, training loss: 0.6580206155776978 = 0.0062541663646698 + 0.1 * 6.517663955688477
Epoch 990, val loss: 1.1066666841506958
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8054823405376912
=== training gcn model ===
Epoch 0, training loss: 2.7925167083740234 = 1.9328320026397705 + 0.1 * 8.596846580505371
Epoch 0, val loss: 1.9283056259155273
Epoch 10, training loss: 2.782625436782837 = 1.9229458570480347 + 0.1 * 8.596795082092285
Epoch 10, val loss: 1.9189095497131348
Epoch 20, training loss: 2.77058744430542 = 1.9109338521957397 + 0.1 * 8.596535682678223
Epoch 20, val loss: 1.906952977180481
Epoch 30, training loss: 2.753875732421875 = 1.8944157361984253 + 0.1 * 8.594600677490234
Epoch 30, val loss: 1.8900516033172607
Epoch 40, training loss: 2.7285828590393066 = 1.8706884384155273 + 0.1 * 8.578944206237793
Epoch 40, val loss: 1.8656262159347534
Epoch 50, training loss: 2.6886343955993652 = 1.8390933275222778 + 0.1 * 8.495409965515137
Epoch 50, val loss: 1.83447265625
Epoch 60, training loss: 2.6359822750091553 = 1.8055100440979004 + 0.1 * 8.30472183227539
Epoch 60, val loss: 1.8047515153884888
Epoch 70, training loss: 2.5793023109436035 = 1.773237705230713 + 0.1 * 8.06064510345459
Epoch 70, val loss: 1.777600646018982
Epoch 80, training loss: 2.499619483947754 = 1.7361057996749878 + 0.1 * 7.635135650634766
Epoch 80, val loss: 1.745357871055603
Epoch 90, training loss: 2.429115056991577 = 1.6895737648010254 + 0.1 * 7.395413398742676
Epoch 90, val loss: 1.7064481973648071
Epoch 100, training loss: 2.3510329723358154 = 1.6279264688491821 + 0.1 * 7.23106575012207
Epoch 100, val loss: 1.6544790267944336
Epoch 110, training loss: 2.262119770050049 = 1.5510435104370117 + 0.1 * 7.1107611656188965
Epoch 110, val loss: 1.5893234014511108
Epoch 120, training loss: 2.1695144176483154 = 1.4633597135543823 + 0.1 * 7.061547756195068
Epoch 120, val loss: 1.5174089670181274
Epoch 130, training loss: 2.0758700370788574 = 1.3724850416183472 + 0.1 * 7.033849239349365
Epoch 130, val loss: 1.445892333984375
Epoch 140, training loss: 1.9843764305114746 = 1.2834147214889526 + 0.1 * 7.009616374969482
Epoch 140, val loss: 1.377191424369812
Epoch 150, training loss: 1.8955867290496826 = 1.196976900100708 + 0.1 * 6.986097812652588
Epoch 150, val loss: 1.3128087520599365
Epoch 160, training loss: 1.811458945274353 = 1.1152127981185913 + 0.1 * 6.962461471557617
Epoch 160, val loss: 1.2542917728424072
Epoch 170, training loss: 1.7339448928833008 = 1.0401467084884644 + 0.1 * 6.937982559204102
Epoch 170, val loss: 1.2030010223388672
Epoch 180, training loss: 1.6633894443511963 = 0.9716905355453491 + 0.1 * 6.916989326477051
Epoch 180, val loss: 1.1576977968215942
Epoch 190, training loss: 1.6002001762390137 = 0.9098634719848633 + 0.1 * 6.903366565704346
Epoch 190, val loss: 1.118431806564331
Epoch 200, training loss: 1.5419738292694092 = 0.8535146713256836 + 0.1 * 6.884591102600098
Epoch 200, val loss: 1.083829402923584
Epoch 210, training loss: 1.4877310991287231 = 0.8005349040031433 + 0.1 * 6.871962070465088
Epoch 210, val loss: 1.0525096654891968
Epoch 220, training loss: 1.436392068862915 = 0.7498843669891357 + 0.1 * 6.865077018737793
Epoch 220, val loss: 1.0239330530166626
Epoch 230, training loss: 1.3863060474395752 = 0.7007054686546326 + 0.1 * 6.8560051918029785
Epoch 230, val loss: 0.9975450038909912
Epoch 240, training loss: 1.336830735206604 = 0.6522895097732544 + 0.1 * 6.845412254333496
Epoch 240, val loss: 0.9728646874427795
Epoch 250, training loss: 1.288094162940979 = 0.6041338443756104 + 0.1 * 6.839602947235107
Epoch 250, val loss: 0.9493671655654907
Epoch 260, training loss: 1.2399265766143799 = 0.556206226348877 + 0.1 * 6.8372039794921875
Epoch 260, val loss: 0.926710844039917
Epoch 270, training loss: 1.1922990083694458 = 0.5094509720802307 + 0.1 * 6.828480243682861
Epoch 270, val loss: 0.905265748500824
Epoch 280, training loss: 1.1469266414642334 = 0.46433353424072266 + 0.1 * 6.825931072235107
Epoch 280, val loss: 0.8850635290145874
Epoch 290, training loss: 1.104257345199585 = 0.4212225079536438 + 0.1 * 6.830347537994385
Epoch 290, val loss: 0.866859495639801
Epoch 300, training loss: 1.0624315738677979 = 0.38077983260154724 + 0.1 * 6.816517353057861
Epoch 300, val loss: 0.8510791063308716
Epoch 310, training loss: 1.0241756439208984 = 0.3429693579673767 + 0.1 * 6.8120622634887695
Epoch 310, val loss: 0.8378045558929443
Epoch 320, training loss: 0.9888477325439453 = 0.3079969584941864 + 0.1 * 6.808507919311523
Epoch 320, val loss: 0.8274586796760559
Epoch 330, training loss: 0.9554315805435181 = 0.2758314907550812 + 0.1 * 6.7960004806518555
Epoch 330, val loss: 0.820046067237854
Epoch 340, training loss: 0.9262466430664062 = 0.24655410647392273 + 0.1 * 6.7969255447387695
Epoch 340, val loss: 0.8153847455978394
Epoch 350, training loss: 0.8984841704368591 = 0.2201511710882187 + 0.1 * 6.783329963684082
Epoch 350, val loss: 0.813109815120697
Epoch 360, training loss: 0.8749856352806091 = 0.19649100303649902 + 0.1 * 6.784946441650391
Epoch 360, val loss: 0.8132069110870361
Epoch 370, training loss: 0.8529099822044373 = 0.17541737854480743 + 0.1 * 6.77492618560791
Epoch 370, val loss: 0.8151242136955261
Epoch 380, training loss: 0.8339157700538635 = 0.1567593663930893 + 0.1 * 6.771563529968262
Epoch 380, val loss: 0.8188083171844482
Epoch 390, training loss: 0.8161311745643616 = 0.14033019542694092 + 0.1 * 6.758009910583496
Epoch 390, val loss: 0.8239662647247314
Epoch 400, training loss: 0.8019670844078064 = 0.1258663386106491 + 0.1 * 6.761007308959961
Epoch 400, val loss: 0.8304300904273987
Epoch 410, training loss: 0.7876388430595398 = 0.11319900304079056 + 0.1 * 6.744398593902588
Epoch 410, val loss: 0.8379227519035339
Epoch 420, training loss: 0.7792694568634033 = 0.10206032544374466 + 0.1 * 6.772091388702393
Epoch 420, val loss: 0.8462561368942261
Epoch 430, training loss: 0.7653434872627258 = 0.09231697767972946 + 0.1 * 6.730265140533447
Epoch 430, val loss: 0.8552778363227844
Epoch 440, training loss: 0.7553648948669434 = 0.08373746275901794 + 0.1 * 6.716274738311768
Epoch 440, val loss: 0.864714503288269
Epoch 450, training loss: 0.7507424354553223 = 0.07616062462329865 + 0.1 * 6.745818138122559
Epoch 450, val loss: 0.8747225999832153
Epoch 460, training loss: 0.7401304244995117 = 0.06949549168348312 + 0.1 * 6.7063493728637695
Epoch 460, val loss: 0.8849722146987915
Epoch 470, training loss: 0.7336633801460266 = 0.06359414756298065 + 0.1 * 6.700692176818848
Epoch 470, val loss: 0.8952609300613403
Epoch 480, training loss: 0.7273589968681335 = 0.05836379528045654 + 0.1 * 6.6899518966674805
Epoch 480, val loss: 0.9057118892669678
Epoch 490, training loss: 0.7222892045974731 = 0.053708549588918686 + 0.1 * 6.6858062744140625
Epoch 490, val loss: 0.9162096381187439
Epoch 500, training loss: 0.7175869345664978 = 0.049548644572496414 + 0.1 * 6.68038272857666
Epoch 500, val loss: 0.9266439080238342
Epoch 510, training loss: 0.713660717010498 = 0.04582376405596733 + 0.1 * 6.678369522094727
Epoch 510, val loss: 0.9371234178543091
Epoch 520, training loss: 0.7093861103057861 = 0.042482055723667145 + 0.1 * 6.669040203094482
Epoch 520, val loss: 0.9473899006843567
Epoch 530, training loss: 0.7054559588432312 = 0.039476774632930756 + 0.1 * 6.659791946411133
Epoch 530, val loss: 0.9575766324996948
Epoch 540, training loss: 0.70232093334198 = 0.03677036613225937 + 0.1 * 6.655505657196045
Epoch 540, val loss: 0.9675601720809937
Epoch 550, training loss: 0.6992467641830444 = 0.0343250073492527 + 0.1 * 6.64921760559082
Epoch 550, val loss: 0.9774381518363953
Epoch 560, training loss: 0.6957353949546814 = 0.03211198002099991 + 0.1 * 6.636233806610107
Epoch 560, val loss: 0.9870928525924683
Epoch 570, training loss: 0.693480372428894 = 0.03010174073278904 + 0.1 * 6.633786678314209
Epoch 570, val loss: 0.9966102838516235
Epoch 580, training loss: 0.6920769214630127 = 0.02827128767967224 + 0.1 * 6.638056755065918
Epoch 580, val loss: 1.0058820247650146
Epoch 590, training loss: 0.6898502707481384 = 0.026605326682329178 + 0.1 * 6.632449626922607
Epoch 590, val loss: 1.015043020248413
Epoch 600, training loss: 0.6878414750099182 = 0.02508375234901905 + 0.1 * 6.62757682800293
Epoch 600, val loss: 1.023830533027649
Epoch 610, training loss: 0.6853986382484436 = 0.023695245385169983 + 0.1 * 6.617033958435059
Epoch 610, val loss: 1.0325517654418945
Epoch 620, training loss: 0.6836848258972168 = 0.02241710014641285 + 0.1 * 6.612676620483398
Epoch 620, val loss: 1.0409808158874512
Epoch 630, training loss: 0.6819002032279968 = 0.021244056522846222 + 0.1 * 6.606561183929443
Epoch 630, val loss: 1.0493184328079224
Epoch 640, training loss: 0.6805061101913452 = 0.02016160823404789 + 0.1 * 6.603444576263428
Epoch 640, val loss: 1.057468295097351
Epoch 650, training loss: 0.6793473362922668 = 0.019162556156516075 + 0.1 * 6.601848125457764
Epoch 650, val loss: 1.0654664039611816
Epoch 660, training loss: 0.6791149377822876 = 0.01823846437036991 + 0.1 * 6.6087646484375
Epoch 660, val loss: 1.0731613636016846
Epoch 670, training loss: 0.6762818694114685 = 0.017383597791194916 + 0.1 * 6.588983058929443
Epoch 670, val loss: 1.0808374881744385
Epoch 680, training loss: 0.6748161911964417 = 0.016590123996138573 + 0.1 * 6.582260608673096
Epoch 680, val loss: 1.0883325338363647
Epoch 690, training loss: 0.6765123605728149 = 0.015851693227887154 + 0.1 * 6.606606960296631
Epoch 690, val loss: 1.0955519676208496
Epoch 700, training loss: 0.6728950142860413 = 0.01516493409872055 + 0.1 * 6.577301025390625
Epoch 700, val loss: 1.1026450395584106
Epoch 710, training loss: 0.6735680103302002 = 0.014525369741022587 + 0.1 * 6.590426445007324
Epoch 710, val loss: 1.1096947193145752
Epoch 720, training loss: 0.6721644997596741 = 0.013928075321018696 + 0.1 * 6.582364559173584
Epoch 720, val loss: 1.1164902448654175
Epoch 730, training loss: 0.6700239181518555 = 0.013368780724704266 + 0.1 * 6.566551208496094
Epoch 730, val loss: 1.12313973903656
Epoch 740, training loss: 0.6699805855751038 = 0.012845631688833237 + 0.1 * 6.571349620819092
Epoch 740, val loss: 1.1297000646591187
Epoch 750, training loss: 0.668096125125885 = 0.012354292906820774 + 0.1 * 6.557417869567871
Epoch 750, val loss: 1.136103868484497
Epoch 760, training loss: 0.6681671142578125 = 0.011892425827682018 + 0.1 * 6.562746524810791
Epoch 760, val loss: 1.1424063444137573
Epoch 770, training loss: 0.6663270592689514 = 0.011457806453108788 + 0.1 * 6.548692226409912
Epoch 770, val loss: 1.148579716682434
Epoch 780, training loss: 0.6687541604042053 = 0.011048703454434872 + 0.1 * 6.577054500579834
Epoch 780, val loss: 1.1546739339828491
Epoch 790, training loss: 0.6658617258071899 = 0.010662148706614971 + 0.1 * 6.551995754241943
Epoch 790, val loss: 1.1605035066604614
Epoch 800, training loss: 0.6646177768707275 = 0.01029918622225523 + 0.1 * 6.543185710906982
Epoch 800, val loss: 1.1664115190505981
Epoch 810, training loss: 0.6642633676528931 = 0.009955163113772869 + 0.1 * 6.543081760406494
Epoch 810, val loss: 1.1721347570419312
Epoch 820, training loss: 0.6648306846618652 = 0.00962883047759533 + 0.1 * 6.552018642425537
Epoch 820, val loss: 1.1776883602142334
Epoch 830, training loss: 0.663361668586731 = 0.009320277720689774 + 0.1 * 6.540413856506348
Epoch 830, val loss: 1.183085322380066
Epoch 840, training loss: 0.6619383692741394 = 0.009028630331158638 + 0.1 * 6.529097557067871
Epoch 840, val loss: 1.1885435581207275
Epoch 850, training loss: 0.6630343198776245 = 0.008751354180276394 + 0.1 * 6.542829990386963
Epoch 850, val loss: 1.1938745975494385
Epoch 860, training loss: 0.6618390083312988 = 0.008487282320857048 + 0.1 * 6.533517360687256
Epoch 860, val loss: 1.198975682258606
Epoch 870, training loss: 0.6611207127571106 = 0.00823702197521925 + 0.1 * 6.528836727142334
Epoch 870, val loss: 1.2040878534317017
Epoch 880, training loss: 0.6611924767494202 = 0.007998868823051453 + 0.1 * 6.531935691833496
Epoch 880, val loss: 1.2091786861419678
Epoch 890, training loss: 0.6602798700332642 = 0.007770338095724583 + 0.1 * 6.525094985961914
Epoch 890, val loss: 1.2139869928359985
Epoch 900, training loss: 0.6606371998786926 = 0.007553707808256149 + 0.1 * 6.530835151672363
Epoch 900, val loss: 1.218753695487976
Epoch 910, training loss: 0.6597934365272522 = 0.0073473891243338585 + 0.1 * 6.524460315704346
Epoch 910, val loss: 1.2235863208770752
Epoch 920, training loss: 0.6582158207893372 = 0.007150314748287201 + 0.1 * 6.510654926300049
Epoch 920, val loss: 1.228268027305603
Epoch 930, training loss: 0.6601691842079163 = 0.006961396429687738 + 0.1 * 6.532077789306641
Epoch 930, val loss: 1.2328375577926636
Epoch 940, training loss: 0.6583443284034729 = 0.006780911702662706 + 0.1 * 6.515634059906006
Epoch 940, val loss: 1.2372561693191528
Epoch 950, training loss: 0.6579971313476562 = 0.006608525291085243 + 0.1 * 6.513885974884033
Epoch 950, val loss: 1.2417699098587036
Epoch 960, training loss: 0.6591259241104126 = 0.006442931015044451 + 0.1 * 6.526830196380615
Epoch 960, val loss: 1.2461658716201782
Epoch 970, training loss: 0.6571505069732666 = 0.00628425320610404 + 0.1 * 6.508662700653076
Epoch 970, val loss: 1.2503979206085205
Epoch 980, training loss: 0.6575865149497986 = 0.006132598500698805 + 0.1 * 6.514538764953613
Epoch 980, val loss: 1.2547087669372559
Epoch 990, training loss: 0.6589628458023071 = 0.005986217875033617 + 0.1 * 6.529766082763672
Epoch 990, val loss: 1.258764624595642
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 2.807682991027832 = 1.947997808456421 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.9348413944244385
Epoch 10, training loss: 2.7973339557647705 = 1.9376531839370728 + 0.1 * 8.596807479858398
Epoch 10, val loss: 1.925063967704773
Epoch 20, training loss: 2.784669876098633 = 1.9250165224075317 + 0.1 * 8.596534729003906
Epoch 20, val loss: 1.9126182794570923
Epoch 30, training loss: 2.7667770385742188 = 1.9073511362075806 + 0.1 * 8.594259262084961
Epoch 30, val loss: 1.8952001333236694
Epoch 40, training loss: 2.7391533851623535 = 1.8816245794296265 + 0.1 * 8.575288772583008
Epoch 40, val loss: 1.8705723285675049
Epoch 50, training loss: 2.693272352218628 = 1.8465440273284912 + 0.1 * 8.467283248901367
Epoch 50, val loss: 1.8390949964523315
Epoch 60, training loss: 2.640268087387085 = 1.8081456422805786 + 0.1 * 8.321224212646484
Epoch 60, val loss: 1.8076324462890625
Epoch 70, training loss: 2.5892393589019775 = 1.771193504333496 + 0.1 * 8.180458068847656
Epoch 70, val loss: 1.7775057554244995
Epoch 80, training loss: 2.517883539199829 = 1.727013349533081 + 0.1 * 7.908701419830322
Epoch 80, val loss: 1.7363554239273071
Epoch 90, training loss: 2.4323325157165527 = 1.670275330543518 + 0.1 * 7.620572090148926
Epoch 90, val loss: 1.6845703125
Epoch 100, training loss: 2.330388069152832 = 1.59982168674469 + 0.1 * 7.305664539337158
Epoch 100, val loss: 1.6233134269714355
Epoch 110, training loss: 2.2336769104003906 = 1.518523097038269 + 0.1 * 7.1515374183654785
Epoch 110, val loss: 1.5538928508758545
Epoch 120, training loss: 2.1441376209259033 = 1.4372395277023315 + 0.1 * 7.068981647491455
Epoch 120, val loss: 1.4902594089508057
Epoch 130, training loss: 2.0648579597473145 = 1.3618464469909668 + 0.1 * 7.030114650726318
Epoch 130, val loss: 1.433853268623352
Epoch 140, training loss: 1.996354579925537 = 1.2969462871551514 + 0.1 * 6.994082927703857
Epoch 140, val loss: 1.390336275100708
Epoch 150, training loss: 1.9374525547027588 = 1.2406748533248901 + 0.1 * 6.967777729034424
Epoch 150, val loss: 1.3557575941085815
Epoch 160, training loss: 1.8840880393981934 = 1.189819574356079 + 0.1 * 6.942684173583984
Epoch 160, val loss: 1.327298879623413
Epoch 170, training loss: 1.8324413299560547 = 1.1401175260543823 + 0.1 * 6.9232378005981445
Epoch 170, val loss: 1.3015379905700684
Epoch 180, training loss: 1.779695987701416 = 1.0886623859405518 + 0.1 * 6.910336494445801
Epoch 180, val loss: 1.2754361629486084
Epoch 190, training loss: 1.7241054773330688 = 1.0344716310501099 + 0.1 * 6.89633846282959
Epoch 190, val loss: 1.2476755380630493
Epoch 200, training loss: 1.6654719114303589 = 0.9767769575119019 + 0.1 * 6.88694953918457
Epoch 200, val loss: 1.2179358005523682
Epoch 210, training loss: 1.6039917469024658 = 0.9161747097969055 + 0.1 * 6.878170490264893
Epoch 210, val loss: 1.1866482496261597
Epoch 220, training loss: 1.5435147285461426 = 0.8540722727775574 + 0.1 * 6.894424915313721
Epoch 220, val loss: 1.154892921447754
Epoch 230, training loss: 1.4798753261566162 = 0.7929019331932068 + 0.1 * 6.869734287261963
Epoch 230, val loss: 1.124241828918457
Epoch 240, training loss: 1.4183915853500366 = 0.7325641512870789 + 0.1 * 6.858274459838867
Epoch 240, val loss: 1.0946154594421387
Epoch 250, training loss: 1.358086109161377 = 0.6728523373603821 + 0.1 * 6.852336883544922
Epoch 250, val loss: 1.0664746761322021
Epoch 260, training loss: 1.2991478443145752 = 0.6140734553337097 + 0.1 * 6.850744247436523
Epoch 260, val loss: 1.0397560596466064
Epoch 270, training loss: 1.2410485744476318 = 0.5568387508392334 + 0.1 * 6.842097282409668
Epoch 270, val loss: 1.0151443481445312
Epoch 280, training loss: 1.1848695278167725 = 0.5012726187705994 + 0.1 * 6.835968971252441
Epoch 280, val loss: 0.9930610656738281
Epoch 290, training loss: 1.1315031051635742 = 0.448499858379364 + 0.1 * 6.830031871795654
Epoch 290, val loss: 0.9743525981903076
Epoch 300, training loss: 1.0815651416778564 = 0.3996852934360504 + 0.1 * 6.818798542022705
Epoch 300, val loss: 0.9595686197280884
Epoch 310, training loss: 1.0369322299957275 = 0.35536643862724304 + 0.1 * 6.815658092498779
Epoch 310, val loss: 0.9489949345588684
Epoch 320, training loss: 0.9967614412307739 = 0.3156777322292328 + 0.1 * 6.810837268829346
Epoch 320, val loss: 0.9425416588783264
Epoch 330, training loss: 0.9600619673728943 = 0.28046172857284546 + 0.1 * 6.796002388000488
Epoch 330, val loss: 0.9396316409111023
Epoch 340, training loss: 0.9278926849365234 = 0.24905474483966827 + 0.1 * 6.788379192352295
Epoch 340, val loss: 0.9398327469825745
Epoch 350, training loss: 0.9012908935546875 = 0.22132769227027893 + 0.1 * 6.799631595611572
Epoch 350, val loss: 0.9425206184387207
Epoch 360, training loss: 0.8738738894462585 = 0.19687636196613312 + 0.1 * 6.769975185394287
Epoch 360, val loss: 0.947249710559845
Epoch 370, training loss: 0.8510957360267639 = 0.17521212995052338 + 0.1 * 6.758835792541504
Epoch 370, val loss: 0.9539502263069153
Epoch 380, training loss: 0.833014965057373 = 0.15609247982501984 + 0.1 * 6.769224643707275
Epoch 380, val loss: 0.9622479677200317
Epoch 390, training loss: 0.8154110312461853 = 0.139470174908638 + 0.1 * 6.759408473968506
Epoch 390, val loss: 0.9716442823410034
Epoch 400, training loss: 0.7980815172195435 = 0.12500423192977905 + 0.1 * 6.730772972106934
Epoch 400, val loss: 0.981740415096283
Epoch 410, training loss: 0.7847108244895935 = 0.11235396564006805 + 0.1 * 6.723568439483643
Epoch 410, val loss: 0.9928494095802307
Epoch 420, training loss: 0.7740393877029419 = 0.10134720802307129 + 0.1 * 6.726921558380127
Epoch 420, val loss: 1.004506230354309
Epoch 430, training loss: 0.7618805766105652 = 0.09174559265375137 + 0.1 * 6.701349258422852
Epoch 430, val loss: 1.0164698362350464
Epoch 440, training loss: 0.756913423538208 = 0.08331640809774399 + 0.1 * 6.7359700202941895
Epoch 440, val loss: 1.0287789106369019
Epoch 450, training loss: 0.7457305192947388 = 0.07595273107290268 + 0.1 * 6.69777774810791
Epoch 450, val loss: 1.0412160158157349
Epoch 460, training loss: 0.7370795607566833 = 0.06947271525859833 + 0.1 * 6.676068305969238
Epoch 460, val loss: 1.0535353422164917
Epoch 470, training loss: 0.7320296764373779 = 0.06373289227485657 + 0.1 * 6.6829681396484375
Epoch 470, val loss: 1.065971851348877
Epoch 480, training loss: 0.7257130146026611 = 0.05863729864358902 + 0.1 * 6.670756816864014
Epoch 480, val loss: 1.0783857107162476
Epoch 490, training loss: 0.7192321419715881 = 0.054089393466711044 + 0.1 * 6.651427745819092
Epoch 490, val loss: 1.0905324220657349
Epoch 500, training loss: 0.7201375961303711 = 0.05001290515065193 + 0.1 * 6.701246738433838
Epoch 500, val loss: 1.1022969484329224
Epoch 510, training loss: 0.7112004160881042 = 0.04638955369591713 + 0.1 * 6.64810848236084
Epoch 510, val loss: 1.114181637763977
Epoch 520, training loss: 0.7067306041717529 = 0.04313264787197113 + 0.1 * 6.635979175567627
Epoch 520, val loss: 1.1253660917282104
Epoch 530, training loss: 0.7031569480895996 = 0.04018363356590271 + 0.1 * 6.629732608795166
Epoch 530, val loss: 1.1367378234863281
Epoch 540, training loss: 0.7006137371063232 = 0.037506408989429474 + 0.1 * 6.631073474884033
Epoch 540, val loss: 1.147553563117981
Epoch 550, training loss: 0.6989937424659729 = 0.03508077934384346 + 0.1 * 6.639129161834717
Epoch 550, val loss: 1.158342957496643
Epoch 560, training loss: 0.6944930553436279 = 0.03287642449140549 + 0.1 * 6.616166114807129
Epoch 560, val loss: 1.168773889541626
Epoch 570, training loss: 0.6926639080047607 = 0.030865663662552834 + 0.1 * 6.617982387542725
Epoch 570, val loss: 1.1788330078125
Epoch 580, training loss: 0.6898704171180725 = 0.029029669240117073 + 0.1 * 6.608407497406006
Epoch 580, val loss: 1.1889005899429321
Epoch 590, training loss: 0.6913621425628662 = 0.027347609400749207 + 0.1 * 6.640145301818848
Epoch 590, val loss: 1.1984440088272095
Epoch 600, training loss: 0.6859046816825867 = 0.02580961212515831 + 0.1 * 6.600950717926025
Epoch 600, val loss: 1.20796799659729
Epoch 610, training loss: 0.683874785900116 = 0.024396182969212532 + 0.1 * 6.594785690307617
Epoch 610, val loss: 1.2170885801315308
Epoch 620, training loss: 0.6821948885917664 = 0.023092588409781456 + 0.1 * 6.591022968292236
Epoch 620, val loss: 1.2262134552001953
Epoch 630, training loss: 0.6818766593933105 = 0.02188948169350624 + 0.1 * 6.5998711585998535
Epoch 630, val loss: 1.2347543239593506
Epoch 640, training loss: 0.6791238188743591 = 0.020782077684998512 + 0.1 * 6.5834174156188965
Epoch 640, val loss: 1.2435065507888794
Epoch 650, training loss: 0.6778151392936707 = 0.019756050780415535 + 0.1 * 6.580590724945068
Epoch 650, val loss: 1.251746416091919
Epoch 660, training loss: 0.6787695288658142 = 0.018804842606186867 + 0.1 * 6.59964656829834
Epoch 660, val loss: 1.2599328756332397
Epoch 670, training loss: 0.6758565306663513 = 0.017924146726727486 + 0.1 * 6.579323768615723
Epoch 670, val loss: 1.267853856086731
Epoch 680, training loss: 0.6752282381057739 = 0.0171071607619524 + 0.1 * 6.581210136413574
Epoch 680, val loss: 1.2755436897277832
Epoch 690, training loss: 0.6731645464897156 = 0.016347255557775497 + 0.1 * 6.568172931671143
Epoch 690, val loss: 1.283218502998352
Epoch 700, training loss: 0.6728084087371826 = 0.015636883676052094 + 0.1 * 6.571715354919434
Epoch 700, val loss: 1.2904367446899414
Epoch 710, training loss: 0.6716049909591675 = 0.014973203651607037 + 0.1 * 6.566317558288574
Epoch 710, val loss: 1.297783613204956
Epoch 720, training loss: 0.6706399917602539 = 0.014351941645145416 + 0.1 * 6.562880039215088
Epoch 720, val loss: 1.3047341108322144
Epoch 730, training loss: 0.6699199676513672 = 0.013770680874586105 + 0.1 * 6.561492443084717
Epoch 730, val loss: 1.3115949630737305
Epoch 740, training loss: 0.6695265173912048 = 0.013225558213889599 + 0.1 * 6.563009262084961
Epoch 740, val loss: 1.3182601928710938
Epoch 750, training loss: 0.6676946878433228 = 0.012716248631477356 + 0.1 * 6.549784183502197
Epoch 750, val loss: 1.3246569633483887
Epoch 760, training loss: 0.6671974062919617 = 0.012237714603543282 + 0.1 * 6.549596786499023
Epoch 760, val loss: 1.331198811531067
Epoch 770, training loss: 0.6680647730827332 = 0.011787057854235172 + 0.1 * 6.562777042388916
Epoch 770, val loss: 1.3372145891189575
Epoch 780, training loss: 0.6661188006401062 = 0.011362810619175434 + 0.1 * 6.547560214996338
Epoch 780, val loss: 1.3433873653411865
Epoch 790, training loss: 0.6651620268821716 = 0.01096351258456707 + 0.1 * 6.541985034942627
Epoch 790, val loss: 1.3494551181793213
Epoch 800, training loss: 0.665273129940033 = 0.010585136711597443 + 0.1 * 6.546879768371582
Epoch 800, val loss: 1.3551491498947144
Epoch 810, training loss: 0.6637046337127686 = 0.010228317230939865 + 0.1 * 6.534762859344482
Epoch 810, val loss: 1.361013650894165
Epoch 820, training loss: 0.6629866361618042 = 0.009890744462609291 + 0.1 * 6.530959129333496
Epoch 820, val loss: 1.366703987121582
Epoch 830, training loss: 0.6633453369140625 = 0.009570768103003502 + 0.1 * 6.537745475769043
Epoch 830, val loss: 1.3721343278884888
Epoch 840, training loss: 0.662126362323761 = 0.009267888031899929 + 0.1 * 6.528584957122803
Epoch 840, val loss: 1.3773900270462036
Epoch 850, training loss: 0.6615107655525208 = 0.008981902152299881 + 0.1 * 6.5252885818481445
Epoch 850, val loss: 1.382912039756775
Epoch 860, training loss: 0.6615385413169861 = 0.00870941299945116 + 0.1 * 6.52829122543335
Epoch 860, val loss: 1.3879607915878296
Epoch 870, training loss: 0.6616941094398499 = 0.00845038890838623 + 0.1 * 6.532436847686768
Epoch 870, val loss: 1.3930543661117554
Epoch 880, training loss: 0.6609993577003479 = 0.008203881792724133 + 0.1 * 6.527955055236816
Epoch 880, val loss: 1.3978191614151
Epoch 890, training loss: 0.6596879959106445 = 0.007970131933689117 + 0.1 * 6.517178535461426
Epoch 890, val loss: 1.4028449058532715
Epoch 900, training loss: 0.6603448987007141 = 0.007746835704892874 + 0.1 * 6.525980472564697
Epoch 900, val loss: 1.4075630903244019
Epoch 910, training loss: 0.6600058674812317 = 0.0075338976457715034 + 0.1 * 6.524719715118408
Epoch 910, val loss: 1.4121826887130737
Epoch 920, training loss: 0.6587331891059875 = 0.0073303719982504845 + 0.1 * 6.514028072357178
Epoch 920, val loss: 1.4167178869247437
Epoch 930, training loss: 0.6588906049728394 = 0.007136709056794643 + 0.1 * 6.517539024353027
Epoch 930, val loss: 1.4212634563446045
Epoch 940, training loss: 0.6583064198493958 = 0.0069513521157205105 + 0.1 * 6.513550758361816
Epoch 940, val loss: 1.425581932067871
Epoch 950, training loss: 0.6587899923324585 = 0.006774211768060923 + 0.1 * 6.520157814025879
Epoch 950, val loss: 1.4300189018249512
Epoch 960, training loss: 0.6566768884658813 = 0.006604364607483149 + 0.1 * 6.500724792480469
Epoch 960, val loss: 1.4340962171554565
Epoch 970, training loss: 0.6567785739898682 = 0.006441903300583363 + 0.1 * 6.503366470336914
Epoch 970, val loss: 1.4383597373962402
Epoch 980, training loss: 0.6571708917617798 = 0.006285859737545252 + 0.1 * 6.50885009765625
Epoch 980, val loss: 1.4422557353973389
Epoch 990, training loss: 0.6562352776527405 = 0.006136610638350248 + 0.1 * 6.500986576080322
Epoch 990, val loss: 1.4463220834732056
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.7970479704797049
The final CL Acc:0.74444, 0.00800, The final GNN Acc:0.80337, 0.00456
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13136])
remove edge: torch.Size([2, 7930])
updated graph: torch.Size([2, 10510])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.78918719291687 = 1.929502010345459 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.934362530708313
Epoch 10, training loss: 2.780799627304077 = 1.921120524406433 + 0.1 * 8.59679126739502
Epoch 10, val loss: 1.9260627031326294
Epoch 20, training loss: 2.770420551300049 = 1.9107779264450073 + 0.1 * 8.596426010131836
Epoch 20, val loss: 1.9157308340072632
Epoch 30, training loss: 2.75530743598938 = 1.8959614038467407 + 0.1 * 8.593460083007812
Epoch 30, val loss: 1.900847315788269
Epoch 40, training loss: 2.7304887771606445 = 1.8736032247543335 + 0.1 * 8.568854331970215
Epoch 40, val loss: 1.8784135580062866
Epoch 50, training loss: 2.6836555004119873 = 1.8417357206344604 + 0.1 * 8.419198036193848
Epoch 50, val loss: 1.8477401733398438
Epoch 60, training loss: 2.615738868713379 = 1.8044277429580688 + 0.1 * 8.113112449645996
Epoch 60, val loss: 1.8133684396743774
Epoch 70, training loss: 2.539340019226074 = 1.7676128149032593 + 0.1 * 7.717272758483887
Epoch 70, val loss: 1.7783657312393188
Epoch 80, training loss: 2.4672417640686035 = 1.7277804613113403 + 0.1 * 7.394611835479736
Epoch 80, val loss: 1.739700198173523
Epoch 90, training loss: 2.4005985260009766 = 1.676777720451355 + 0.1 * 7.2382073402404785
Epoch 90, val loss: 1.691868782043457
Epoch 100, training loss: 2.32438588142395 = 1.6093913316726685 + 0.1 * 7.149946212768555
Epoch 100, val loss: 1.630470871925354
Epoch 110, training loss: 2.2362794876098633 = 1.5259792804718018 + 0.1 * 7.103002071380615
Epoch 110, val loss: 1.555248498916626
Epoch 120, training loss: 2.1382980346679688 = 1.4317433834075928 + 0.1 * 7.065545082092285
Epoch 120, val loss: 1.4721441268920898
Epoch 130, training loss: 2.0360817909240723 = 1.332724690437317 + 0.1 * 7.033572196960449
Epoch 130, val loss: 1.3880852460861206
Epoch 140, training loss: 1.9334049224853516 = 1.2319878339767456 + 0.1 * 7.0141706466674805
Epoch 140, val loss: 1.3038402795791626
Epoch 150, training loss: 1.8301851749420166 = 1.1299023628234863 + 0.1 * 7.002828598022461
Epoch 150, val loss: 1.2199088335037231
Epoch 160, training loss: 1.7273261547088623 = 1.0274173021316528 + 0.1 * 6.999088287353516
Epoch 160, val loss: 1.1369237899780273
Epoch 170, training loss: 1.6276006698608398 = 0.9287392497062683 + 0.1 * 6.988614559173584
Epoch 170, val loss: 1.0578703880310059
Epoch 180, training loss: 1.5334219932556152 = 0.8357751965522766 + 0.1 * 6.976467132568359
Epoch 180, val loss: 0.984504759311676
Epoch 190, training loss: 1.4466910362243652 = 0.7504454851150513 + 0.1 * 6.962454795837402
Epoch 190, val loss: 0.9186484813690186
Epoch 200, training loss: 1.3690810203552246 = 0.6744964718818665 + 0.1 * 6.945845127105713
Epoch 200, val loss: 0.8626354336738586
Epoch 210, training loss: 1.3011419773101807 = 0.6078987121582031 + 0.1 * 6.932432174682617
Epoch 210, val loss: 0.8168586492538452
Epoch 220, training loss: 1.2397427558898926 = 0.5492104887962341 + 0.1 * 6.905322551727295
Epoch 220, val loss: 0.7803357243537903
Epoch 230, training loss: 1.184768557548523 = 0.49609723687171936 + 0.1 * 6.88671350479126
Epoch 230, val loss: 0.7512252926826477
Epoch 240, training loss: 1.134218454360962 = 0.44733333587646484 + 0.1 * 6.868851184844971
Epoch 240, val loss: 0.7278798222541809
Epoch 250, training loss: 1.0864890813827515 = 0.40201422572135925 + 0.1 * 6.844748020172119
Epoch 250, val loss: 0.708687424659729
Epoch 260, training loss: 1.0435134172439575 = 0.3596480190753937 + 0.1 * 6.838653564453125
Epoch 260, val loss: 0.6928411722183228
Epoch 270, training loss: 1.0024487972259521 = 0.3206607401371002 + 0.1 * 6.817880153656006
Epoch 270, val loss: 0.6802453398704529
Epoch 280, training loss: 0.9664309024810791 = 0.2846238911151886 + 0.1 * 6.818069934844971
Epoch 280, val loss: 0.6706559062004089
Epoch 290, training loss: 0.9323337078094482 = 0.2520640790462494 + 0.1 * 6.802696228027344
Epoch 290, val loss: 0.6641156673431396
Epoch 300, training loss: 0.9012700319290161 = 0.22277113795280457 + 0.1 * 6.784988880157471
Epoch 300, val loss: 0.6603695750236511
Epoch 310, training loss: 0.8753063678741455 = 0.19660021364688873 + 0.1 * 6.78706169128418
Epoch 310, val loss: 0.6593016982078552
Epoch 320, training loss: 0.851279616355896 = 0.17378994822502136 + 0.1 * 6.77489709854126
Epoch 320, val loss: 0.6605608463287354
Epoch 330, training loss: 0.8296548128128052 = 0.15395596623420715 + 0.1 * 6.756988048553467
Epoch 330, val loss: 0.6637696623802185
Epoch 340, training loss: 0.8146592378616333 = 0.13668353855609894 + 0.1 * 6.779757022857666
Epoch 340, val loss: 0.6687049865722656
Epoch 350, training loss: 0.7956259846687317 = 0.1217573955655098 + 0.1 * 6.738685607910156
Epoch 350, val loss: 0.6749789118766785
Epoch 360, training loss: 0.7813544869422913 = 0.10879933834075928 + 0.1 * 6.725551128387451
Epoch 360, val loss: 0.682317316532135
Epoch 370, training loss: 0.7707684636116028 = 0.09755389392375946 + 0.1 * 6.732145309448242
Epoch 370, val loss: 0.6905168294906616
Epoch 380, training loss: 0.7591253519058228 = 0.08783724159002304 + 0.1 * 6.712881088256836
Epoch 380, val loss: 0.69928377866745
Epoch 390, training loss: 0.7497246861457825 = 0.07935799658298492 + 0.1 * 6.703667163848877
Epoch 390, val loss: 0.7084988355636597
Epoch 400, training loss: 0.7426738142967224 = 0.07195408642292023 + 0.1 * 6.707197189331055
Epoch 400, val loss: 0.718067467212677
Epoch 410, training loss: 0.733663022518158 = 0.06547753512859344 + 0.1 * 6.681854724884033
Epoch 410, val loss: 0.7278240919113159
Epoch 420, training loss: 0.727766752243042 = 0.05978449434041977 + 0.1 * 6.67982292175293
Epoch 420, val loss: 0.7376983165740967
Epoch 430, training loss: 0.7231303453445435 = 0.05478398874402046 + 0.1 * 6.6834635734558105
Epoch 430, val loss: 0.7475212216377258
Epoch 440, training loss: 0.7160321474075317 = 0.050370655953884125 + 0.1 * 6.656614780426025
Epoch 440, val loss: 0.7573090195655823
Epoch 450, training loss: 0.7121790051460266 = 0.0464443638920784 + 0.1 * 6.657346248626709
Epoch 450, val loss: 0.7669700980186462
Epoch 460, training loss: 0.7072690725326538 = 0.04294298589229584 + 0.1 * 6.643260478973389
Epoch 460, val loss: 0.776619553565979
Epoch 470, training loss: 0.704484224319458 = 0.03981659188866615 + 0.1 * 6.646676063537598
Epoch 470, val loss: 0.7860240340232849
Epoch 480, training loss: 0.7003549933433533 = 0.037014733999967575 + 0.1 * 6.633402347564697
Epoch 480, val loss: 0.7953081727027893
Epoch 490, training loss: 0.6964818239212036 = 0.03449535742402077 + 0.1 * 6.619864463806152
Epoch 490, val loss: 0.8043827414512634
Epoch 500, training loss: 0.6931562423706055 = 0.032226674258708954 + 0.1 * 6.60929536819458
Epoch 500, val loss: 0.813275158405304
Epoch 510, training loss: 0.6932395100593567 = 0.030169302597641945 + 0.1 * 6.630702018737793
Epoch 510, val loss: 0.8219714164733887
Epoch 520, training loss: 0.6886329054832458 = 0.02831135131418705 + 0.1 * 6.603215217590332
Epoch 520, val loss: 0.8304712176322937
Epoch 530, training loss: 0.6868458986282349 = 0.026618631556630135 + 0.1 * 6.602272033691406
Epoch 530, val loss: 0.8386791944503784
Epoch 540, training loss: 0.6843269467353821 = 0.025071265175938606 + 0.1 * 6.592556953430176
Epoch 540, val loss: 0.8467815518379211
Epoch 550, training loss: 0.6828109622001648 = 0.023657813668251038 + 0.1 * 6.591531276702881
Epoch 550, val loss: 0.8546775579452515
Epoch 560, training loss: 0.6821795105934143 = 0.022363077849149704 + 0.1 * 6.5981645584106445
Epoch 560, val loss: 0.8623958230018616
Epoch 570, training loss: 0.6791151165962219 = 0.021176813170313835 + 0.1 * 6.57938289642334
Epoch 570, val loss: 0.8699160814285278
Epoch 580, training loss: 0.6780614852905273 = 0.020082227885723114 + 0.1 * 6.579792499542236
Epoch 580, val loss: 0.8772711753845215
Epoch 590, training loss: 0.6764946579933167 = 0.019072771072387695 + 0.1 * 6.57421875
Epoch 590, val loss: 0.8844407200813293
Epoch 600, training loss: 0.6754089593887329 = 0.018141919746994972 + 0.1 * 6.572669982910156
Epoch 600, val loss: 0.8914464116096497
Epoch 610, training loss: 0.6735567450523376 = 0.017279798164963722 + 0.1 * 6.562769412994385
Epoch 610, val loss: 0.8982802629470825
Epoch 620, training loss: 0.6727673411369324 = 0.01647988148033619 + 0.1 * 6.5628743171691895
Epoch 620, val loss: 0.9049932360649109
Epoch 630, training loss: 0.6720764636993408 = 0.015736062079668045 + 0.1 * 6.563404083251953
Epoch 630, val loss: 0.9115172624588013
Epoch 640, training loss: 0.6702677011489868 = 0.015047016553580761 + 0.1 * 6.552206516265869
Epoch 640, val loss: 0.9179109334945679
Epoch 650, training loss: 0.6717739701271057 = 0.01440398395061493 + 0.1 * 6.573699474334717
Epoch 650, val loss: 0.9241312742233276
Epoch 660, training loss: 0.6686793565750122 = 0.013804509304463863 + 0.1 * 6.54874849319458
Epoch 660, val loss: 0.9301802515983582
Epoch 670, training loss: 0.6669344305992126 = 0.013243268243968487 + 0.1 * 6.536911487579346
Epoch 670, val loss: 0.936148464679718
Epoch 680, training loss: 0.6664525270462036 = 0.012716465629637241 + 0.1 * 6.537360191345215
Epoch 680, val loss: 0.9419152736663818
Epoch 690, training loss: 0.6655595302581787 = 0.012223869562149048 + 0.1 * 6.533356666564941
Epoch 690, val loss: 0.9476478099822998
Epoch 700, training loss: 0.6659501194953918 = 0.011760831810534 + 0.1 * 6.5418925285339355
Epoch 700, val loss: 0.9531963467597961
Epoch 710, training loss: 0.663787841796875 = 0.011326388455927372 + 0.1 * 6.524614334106445
Epoch 710, val loss: 0.9586370587348938
Epoch 720, training loss: 0.6649700999259949 = 0.010917005129158497 + 0.1 * 6.540530681610107
Epoch 720, val loss: 0.9639529585838318
Epoch 730, training loss: 0.6627417802810669 = 0.010532510466873646 + 0.1 * 6.522092819213867
Epoch 730, val loss: 0.9691600799560547
Epoch 740, training loss: 0.6632276177406311 = 0.010168405249714851 + 0.1 * 6.53059196472168
Epoch 740, val loss: 0.9742463231086731
Epoch 750, training loss: 0.6638018488883972 = 0.009824366308748722 + 0.1 * 6.539774417877197
Epoch 750, val loss: 0.9791631698608398
Epoch 760, training loss: 0.661614179611206 = 0.00950092077255249 + 0.1 * 6.521132469177246
Epoch 760, val loss: 0.9840225577354431
Epoch 770, training loss: 0.6601875424385071 = 0.009193568490445614 + 0.1 * 6.509940147399902
Epoch 770, val loss: 0.9888138175010681
Epoch 780, training loss: 0.662126362323761 = 0.008901303634047508 + 0.1 * 6.53225040435791
Epoch 780, val loss: 0.9934622049331665
Epoch 790, training loss: 0.6600558161735535 = 0.00862524937838316 + 0.1 * 6.514305114746094
Epoch 790, val loss: 0.9979516863822937
Epoch 800, training loss: 0.6593771576881409 = 0.008363536559045315 + 0.1 * 6.510136127471924
Epoch 800, val loss: 1.0024604797363281
Epoch 810, training loss: 0.6586703658103943 = 0.008114758878946304 + 0.1 * 6.505556106567383
Epoch 810, val loss: 1.006833553314209
Epoch 820, training loss: 0.6580315232276917 = 0.007877404801547527 + 0.1 * 6.5015411376953125
Epoch 820, val loss: 1.0111051797866821
Epoch 830, training loss: 0.6585100293159485 = 0.0076513634994626045 + 0.1 * 6.508586406707764
Epoch 830, val loss: 1.0152621269226074
Epoch 840, training loss: 0.6572309732437134 = 0.0074369595386087894 + 0.1 * 6.4979400634765625
Epoch 840, val loss: 1.0194309949874878
Epoch 850, training loss: 0.6585823893547058 = 0.0072320192120969296 + 0.1 * 6.513503551483154
Epoch 850, val loss: 1.0234687328338623
Epoch 860, training loss: 0.656510591506958 = 0.007036998867988586 + 0.1 * 6.494736194610596
Epoch 860, val loss: 1.0273756980895996
Epoch 870, training loss: 0.6584528684616089 = 0.006850221194326878 + 0.1 * 6.516026020050049
Epoch 870, val loss: 1.0312798023223877
Epoch 880, training loss: 0.6559506058692932 = 0.006672261282801628 + 0.1 * 6.492783069610596
Epoch 880, val loss: 1.0350213050842285
Epoch 890, training loss: 0.6553380489349365 = 0.0065019214525818825 + 0.1 * 6.488361358642578
Epoch 890, val loss: 1.0387977361679077
Epoch 900, training loss: 0.6568503379821777 = 0.0063386345282197 + 0.1 * 6.505116939544678
Epoch 900, val loss: 1.0424652099609375
Epoch 910, training loss: 0.654680609703064 = 0.006182253360748291 + 0.1 * 6.484983444213867
Epoch 910, val loss: 1.0460625886917114
Epoch 920, training loss: 0.6535872220993042 = 0.006032310426235199 + 0.1 * 6.475549221038818
Epoch 920, val loss: 1.0496793985366821
Epoch 930, training loss: 0.6569200754165649 = 0.005887857172638178 + 0.1 * 6.510322093963623
Epoch 930, val loss: 1.0531717538833618
Epoch 940, training loss: 0.6535502672195435 = 0.005749892443418503 + 0.1 * 6.47800350189209
Epoch 940, val loss: 1.056589961051941
Epoch 950, training loss: 0.6546455025672913 = 0.005617664195597172 + 0.1 * 6.490278244018555
Epoch 950, val loss: 1.0600287914276123
Epoch 960, training loss: 0.6531343460083008 = 0.005490601994097233 + 0.1 * 6.476437091827393
Epoch 960, val loss: 1.0634037256240845
Epoch 970, training loss: 0.6521157622337341 = 0.0053684464655816555 + 0.1 * 6.467472553253174
Epoch 970, val loss: 1.0666841268539429
Epoch 980, training loss: 0.6528396010398865 = 0.005250614136457443 + 0.1 * 6.475889682769775
Epoch 980, val loss: 1.0699080228805542
Epoch 990, training loss: 0.6510425806045532 = 0.0051374551840126514 + 0.1 * 6.459051132202148
Epoch 990, val loss: 1.0730788707733154
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.791659355163574 = 1.931976318359375 + 0.1 * 8.596830368041992
Epoch 0, val loss: 1.9241517782211304
Epoch 10, training loss: 2.7814760208129883 = 1.9218019247055054 + 0.1 * 8.596741676330566
Epoch 10, val loss: 1.9146279096603394
Epoch 20, training loss: 2.768490791320801 = 1.9088635444641113 + 0.1 * 8.596271514892578
Epoch 20, val loss: 1.9022027254104614
Epoch 30, training loss: 2.749746322631836 = 1.8905123472213745 + 0.1 * 8.592339515686035
Epoch 30, val loss: 1.8844492435455322
Epoch 40, training loss: 2.719956636428833 = 1.8637202978134155 + 0.1 * 8.562362670898438
Epoch 40, val loss: 1.8589425086975098
Epoch 50, training loss: 2.66650390625 = 1.828206181526184 + 0.1 * 8.382978439331055
Epoch 50, val loss: 1.8269217014312744
Epoch 60, training loss: 2.606497287750244 = 1.7901225090026855 + 0.1 * 8.163748741149902
Epoch 60, val loss: 1.7944419384002686
Epoch 70, training loss: 2.5317554473876953 = 1.7506592273712158 + 0.1 * 7.810962677001953
Epoch 70, val loss: 1.759724497795105
Epoch 80, training loss: 2.4468419551849365 = 1.7042874097824097 + 0.1 * 7.425546169281006
Epoch 80, val loss: 1.7185477018356323
Epoch 90, training loss: 2.3661251068115234 = 1.6462136507034302 + 0.1 * 7.19911527633667
Epoch 90, val loss: 1.669264554977417
Epoch 100, training loss: 2.28094220161438 = 1.5727992057800293 + 0.1 * 7.0814290046691895
Epoch 100, val loss: 1.6076068878173828
Epoch 110, training loss: 2.1941678524017334 = 1.4917023181915283 + 0.1 * 7.024654865264893
Epoch 110, val loss: 1.5423136949539185
Epoch 120, training loss: 2.1126646995544434 = 1.4130221605300903 + 0.1 * 6.996424674987793
Epoch 120, val loss: 1.48163902759552
Epoch 130, training loss: 2.03538179397583 = 1.3376349210739136 + 0.1 * 6.9774675369262695
Epoch 130, val loss: 1.4245158433914185
Epoch 140, training loss: 1.960383415222168 = 1.2637819051742554 + 0.1 * 6.966014862060547
Epoch 140, val loss: 1.3696953058242798
Epoch 150, training loss: 1.8861254453659058 = 1.1911804676055908 + 0.1 * 6.94944953918457
Epoch 150, val loss: 1.3160003423690796
Epoch 160, training loss: 1.814249038696289 = 1.1207906007766724 + 0.1 * 6.934584617614746
Epoch 160, val loss: 1.2651830911636353
Epoch 170, training loss: 1.7469162940979004 = 1.0556721687316895 + 0.1 * 6.912441730499268
Epoch 170, val loss: 1.2190099954605103
Epoch 180, training loss: 1.6863430738449097 = 0.9964514970779419 + 0.1 * 6.898915767669678
Epoch 180, val loss: 1.177744746208191
Epoch 190, training loss: 1.6306030750274658 = 0.9426121115684509 + 0.1 * 6.879909992218018
Epoch 190, val loss: 1.140567660331726
Epoch 200, training loss: 1.5787067413330078 = 0.8925527334213257 + 0.1 * 6.861540794372559
Epoch 200, val loss: 1.1064772605895996
Epoch 210, training loss: 1.5286626815795898 = 0.8438420295715332 + 0.1 * 6.848206520080566
Epoch 210, val loss: 1.073759913444519
Epoch 220, training loss: 1.4791045188903809 = 0.7954402565956116 + 0.1 * 6.836642742156982
Epoch 220, val loss: 1.0419011116027832
Epoch 230, training loss: 1.4298017024993896 = 0.7472023963928223 + 0.1 * 6.825993537902832
Epoch 230, val loss: 1.0111103057861328
Epoch 240, training loss: 1.3791499137878418 = 0.6981955766677856 + 0.1 * 6.809544086456299
Epoch 240, val loss: 0.9809561371803284
Epoch 250, training loss: 1.3289990425109863 = 0.6482899188995361 + 0.1 * 6.807091236114502
Epoch 250, val loss: 0.9515973329544067
Epoch 260, training loss: 1.2778648138046265 = 0.5983391404151917 + 0.1 * 6.795256614685059
Epoch 260, val loss: 0.9238551259040833
Epoch 270, training loss: 1.2274951934814453 = 0.5494751930236816 + 0.1 * 6.7801995277404785
Epoch 270, val loss: 0.8987460136413574
Epoch 280, training loss: 1.1824523210525513 = 0.5024376511573792 + 0.1 * 6.800146579742432
Epoch 280, val loss: 0.8770034909248352
Epoch 290, training loss: 1.1348708868026733 = 0.45871320366859436 + 0.1 * 6.761577129364014
Epoch 290, val loss: 0.8595799207687378
Epoch 300, training loss: 1.0937554836273193 = 0.41857996582984924 + 0.1 * 6.751755237579346
Epoch 300, val loss: 0.8468914031982422
Epoch 310, training loss: 1.0573549270629883 = 0.3824956715106964 + 0.1 * 6.748592853546143
Epoch 310, val loss: 0.8388808369636536
Epoch 320, training loss: 1.0238240957260132 = 0.35028237104415894 + 0.1 * 6.735416889190674
Epoch 320, val loss: 0.8352739810943604
Epoch 330, training loss: 0.9947038888931274 = 0.32153552770614624 + 0.1 * 6.731683254241943
Epoch 330, val loss: 0.8353692889213562
Epoch 340, training loss: 0.9680050611495972 = 0.29578113555908203 + 0.1 * 6.722239017486572
Epoch 340, val loss: 0.8383256793022156
Epoch 350, training loss: 0.9435765743255615 = 0.27220961451530457 + 0.1 * 6.713669300079346
Epoch 350, val loss: 0.8435958027839661
Epoch 360, training loss: 0.9225481748580933 = 0.25019964575767517 + 0.1 * 6.723485469818115
Epoch 360, val loss: 0.8503643870353699
Epoch 370, training loss: 0.8995187282562256 = 0.22935709357261658 + 0.1 * 6.701615810394287
Epoch 370, val loss: 0.8581942319869995
Epoch 380, training loss: 0.8778193593025208 = 0.20932543277740479 + 0.1 * 6.684939384460449
Epoch 380, val loss: 0.8669610619544983
Epoch 390, training loss: 0.8598401546478271 = 0.19015100598335266 + 0.1 * 6.696891784667969
Epoch 390, val loss: 0.8764708638191223
Epoch 400, training loss: 0.839643657207489 = 0.17219847440719604 + 0.1 * 6.67445182800293
Epoch 400, val loss: 0.8864653706550598
Epoch 410, training loss: 0.8222820162773132 = 0.15563763678073883 + 0.1 * 6.666443824768066
Epoch 410, val loss: 0.8969696760177612
Epoch 420, training loss: 0.8085711002349854 = 0.14059306681156158 + 0.1 * 6.67978048324585
Epoch 420, val loss: 0.9078935980796814
Epoch 430, training loss: 0.7928107976913452 = 0.12708696722984314 + 0.1 * 6.657238006591797
Epoch 430, val loss: 0.9190859794616699
Epoch 440, training loss: 0.7810295820236206 = 0.11496239900588989 + 0.1 * 6.660671710968018
Epoch 440, val loss: 0.9307783246040344
Epoch 450, training loss: 0.7692755460739136 = 0.10416031628847122 + 0.1 * 6.65115213394165
Epoch 450, val loss: 0.9426547884941101
Epoch 460, training loss: 0.7578166127204895 = 0.09454361349344254 + 0.1 * 6.632729530334473
Epoch 460, val loss: 0.9547258019447327
Epoch 470, training loss: 0.7496609687805176 = 0.08596150577068329 + 0.1 * 6.636994361877441
Epoch 470, val loss: 0.9671343564987183
Epoch 480, training loss: 0.7402672171592712 = 0.07832371443510056 + 0.1 * 6.619434833526611
Epoch 480, val loss: 0.9796406030654907
Epoch 490, training loss: 0.7328505516052246 = 0.07152858376502991 + 0.1 * 6.613219738006592
Epoch 490, val loss: 0.9921777844429016
Epoch 500, training loss: 0.7266682386398315 = 0.06547123193740845 + 0.1 * 6.611969947814941
Epoch 500, val loss: 1.00489342212677
Epoch 510, training loss: 0.7206306457519531 = 0.06007599085569382 + 0.1 * 6.605546474456787
Epoch 510, val loss: 1.0174638032913208
Epoch 520, training loss: 0.7158887386322021 = 0.05526372045278549 + 0.1 * 6.606250286102295
Epoch 520, val loss: 1.0300889015197754
Epoch 530, training loss: 0.7107004523277283 = 0.05096748098731041 + 0.1 * 6.597330093383789
Epoch 530, val loss: 1.0424776077270508
Epoch 540, training loss: 0.7088286280632019 = 0.0471203587949276 + 0.1 * 6.617082595825195
Epoch 540, val loss: 1.0548925399780273
Epoch 550, training loss: 0.7031041979789734 = 0.04369046539068222 + 0.1 * 6.594137668609619
Epoch 550, val loss: 1.0669606924057007
Epoch 560, training loss: 0.6986799240112305 = 0.04060368612408638 + 0.1 * 6.580761909484863
Epoch 560, val loss: 1.0788955688476562
Epoch 570, training loss: 0.6969297528266907 = 0.03781801834702492 + 0.1 * 6.5911173820495605
Epoch 570, val loss: 1.0907697677612305
Epoch 580, training loss: 0.6927210688591003 = 0.035307757556438446 + 0.1 * 6.574132919311523
Epoch 580, val loss: 1.1023547649383545
Epoch 590, training loss: 0.689913809299469 = 0.03303198888897896 + 0.1 * 6.568818092346191
Epoch 590, val loss: 1.1137515306472778
Epoch 600, training loss: 0.6877254247665405 = 0.030968939885497093 + 0.1 * 6.567564487457275
Epoch 600, val loss: 1.1249849796295166
Epoch 610, training loss: 0.6873324513435364 = 0.0290945116430521 + 0.1 * 6.582379341125488
Epoch 610, val loss: 1.1359511613845825
Epoch 620, training loss: 0.6840805411338806 = 0.02739151380956173 + 0.1 * 6.566890239715576
Epoch 620, val loss: 1.146671175956726
Epoch 630, training loss: 0.6837151050567627 = 0.025834230706095695 + 0.1 * 6.578808784484863
Epoch 630, val loss: 1.1571786403656006
Epoch 640, training loss: 0.681215226650238 = 0.024414433166384697 + 0.1 * 6.568007946014404
Epoch 640, val loss: 1.167458176612854
Epoch 650, training loss: 0.6780930757522583 = 0.023110829293727875 + 0.1 * 6.549822807312012
Epoch 650, val loss: 1.1774848699569702
Epoch 660, training loss: 0.6768552660942078 = 0.021912576630711555 + 0.1 * 6.549426555633545
Epoch 660, val loss: 1.187395691871643
Epoch 670, training loss: 0.6756988167762756 = 0.020807115361094475 + 0.1 * 6.548917293548584
Epoch 670, val loss: 1.1971325874328613
Epoch 680, training loss: 0.6744006276130676 = 0.019788820296525955 + 0.1 * 6.546118259429932
Epoch 680, val loss: 1.2066583633422852
Epoch 690, training loss: 0.6729224324226379 = 0.01884809508919716 + 0.1 * 6.540742874145508
Epoch 690, val loss: 1.2159732580184937
Epoch 700, training loss: 0.6717216372489929 = 0.017975322902202606 + 0.1 * 6.5374627113342285
Epoch 700, val loss: 1.225038766860962
Epoch 710, training loss: 0.6732727289199829 = 0.017164628952741623 + 0.1 * 6.561080455780029
Epoch 710, val loss: 1.2340168952941895
Epoch 720, training loss: 0.6693556904792786 = 0.01641204208135605 + 0.1 * 6.5294365882873535
Epoch 720, val loss: 1.2427716255187988
Epoch 730, training loss: 0.6685899496078491 = 0.015711313113570213 + 0.1 * 6.5287861824035645
Epoch 730, val loss: 1.2512439489364624
Epoch 740, training loss: 0.6687623858451843 = 0.015056881122291088 + 0.1 * 6.537054538726807
Epoch 740, val loss: 1.2596112489700317
Epoch 750, training loss: 0.6666640043258667 = 0.014446718618273735 + 0.1 * 6.522172451019287
Epoch 750, val loss: 1.2678015232086182
Epoch 760, training loss: 0.6655162572860718 = 0.013875710777938366 + 0.1 * 6.51640510559082
Epoch 760, val loss: 1.2758415937423706
Epoch 770, training loss: 0.666558563709259 = 0.013339942321181297 + 0.1 * 6.532186031341553
Epoch 770, val loss: 1.28366219997406
Epoch 780, training loss: 0.664894163608551 = 0.012837398797273636 + 0.1 * 6.520567417144775
Epoch 780, val loss: 1.2914302349090576
Epoch 790, training loss: 0.663630485534668 = 0.01236612070351839 + 0.1 * 6.512643337249756
Epoch 790, val loss: 1.298959493637085
Epoch 800, training loss: 0.6642439961433411 = 0.011921877972781658 + 0.1 * 6.523221015930176
Epoch 800, val loss: 1.3063782453536987
Epoch 810, training loss: 0.6627638936042786 = 0.011502647772431374 + 0.1 * 6.512612819671631
Epoch 810, val loss: 1.3136987686157227
Epoch 820, training loss: 0.6613872647285461 = 0.011108255945146084 + 0.1 * 6.502789497375488
Epoch 820, val loss: 1.3208563327789307
Epoch 830, training loss: 0.6619612574577332 = 0.010735152289271355 + 0.1 * 6.512260913848877
Epoch 830, val loss: 1.3278541564941406
Epoch 840, training loss: 0.6609487533569336 = 0.01038246788084507 + 0.1 * 6.50566291809082
Epoch 840, val loss: 1.3347595930099487
Epoch 850, training loss: 0.6607189178466797 = 0.010049931704998016 + 0.1 * 6.506689548492432
Epoch 850, val loss: 1.3414618968963623
Epoch 860, training loss: 0.660291850566864 = 0.009734392166137695 + 0.1 * 6.5055742263793945
Epoch 860, val loss: 1.3480420112609863
Epoch 870, training loss: 0.6587094664573669 = 0.009435039944946766 + 0.1 * 6.492743968963623
Epoch 870, val loss: 1.354521632194519
Epoch 880, training loss: 0.6601586937904358 = 0.00914999283850193 + 0.1 * 6.510087013244629
Epoch 880, val loss: 1.360902190208435
Epoch 890, training loss: 0.6585062742233276 = 0.008879547007381916 + 0.1 * 6.496267318725586
Epoch 890, val loss: 1.3672083616256714
Epoch 900, training loss: 0.6587170362472534 = 0.008621989749372005 + 0.1 * 6.500950336456299
Epoch 900, val loss: 1.3733735084533691
Epoch 910, training loss: 0.6581757664680481 = 0.008377263322472572 + 0.1 * 6.497984886169434
Epoch 910, val loss: 1.3794211149215698
Epoch 920, training loss: 0.656511664390564 = 0.008143880404531956 + 0.1 * 6.483677387237549
Epoch 920, val loss: 1.3853224515914917
Epoch 930, training loss: 0.6566404700279236 = 0.007921372540295124 + 0.1 * 6.4871907234191895
Epoch 930, val loss: 1.3911633491516113
Epoch 940, training loss: 0.6563583016395569 = 0.007708746939897537 + 0.1 * 6.486495494842529
Epoch 940, val loss: 1.3968669176101685
Epoch 950, training loss: 0.6573176383972168 = 0.007505812682211399 + 0.1 * 6.498117923736572
Epoch 950, val loss: 1.4024968147277832
Epoch 960, training loss: 0.6552765369415283 = 0.007311963476240635 + 0.1 * 6.479645729064941
Epoch 960, val loss: 1.40802001953125
Epoch 970, training loss: 0.6553681492805481 = 0.007126680575311184 + 0.1 * 6.482414722442627
Epoch 970, val loss: 1.4134663343429565
Epoch 980, training loss: 0.6554282307624817 = 0.00694876117631793 + 0.1 * 6.484794616699219
Epoch 980, val loss: 1.4188414812088013
Epoch 990, training loss: 0.6544653177261353 = 0.006778456270694733 + 0.1 * 6.476868152618408
Epoch 990, val loss: 1.4241702556610107
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.8097827434539795 = 1.9500969648361206 + 0.1 * 8.596858024597168
Epoch 0, val loss: 1.9551111459732056
Epoch 10, training loss: 2.7992405891418457 = 1.9395610094070435 + 0.1 * 8.596796989440918
Epoch 10, val loss: 1.9441473484039307
Epoch 20, training loss: 2.7860870361328125 = 1.926438570022583 + 0.1 * 8.596485137939453
Epoch 20, val loss: 1.9302856922149658
Epoch 30, training loss: 2.767338991165161 = 1.9079235792160034 + 0.1 * 8.59415340423584
Epoch 30, val loss: 1.9107202291488647
Epoch 40, training loss: 2.738236427307129 = 1.880452275276184 + 0.1 * 8.577841758728027
Epoch 40, val loss: 1.882025957107544
Epoch 50, training loss: 2.6926796436309814 = 1.8422845602035522 + 0.1 * 8.503951072692871
Epoch 50, val loss: 1.8439645767211914
Epoch 60, training loss: 2.6273281574249268 = 1.8003499507904053 + 0.1 * 8.269781112670898
Epoch 60, val loss: 1.8059802055358887
Epoch 70, training loss: 2.563885450363159 = 1.7609397172927856 + 0.1 * 8.029458045959473
Epoch 70, val loss: 1.77216637134552
Epoch 80, training loss: 2.484299898147583 = 1.714741587638855 + 0.1 * 7.695583343505859
Epoch 80, val loss: 1.7312439680099487
Epoch 90, training loss: 2.403080701828003 = 1.656251311302185 + 0.1 * 7.468294143676758
Epoch 90, val loss: 1.6797298192977905
Epoch 100, training loss: 2.316401481628418 = 1.581753134727478 + 0.1 * 7.346484661102295
Epoch 100, val loss: 1.6141952276229858
Epoch 110, training loss: 2.218198776245117 = 1.4928869009017944 + 0.1 * 7.253119945526123
Epoch 110, val loss: 1.5360041856765747
Epoch 120, training loss: 2.1168365478515625 = 1.3970701694488525 + 0.1 * 7.1976637840271
Epoch 120, val loss: 1.4560327529907227
Epoch 130, training loss: 2.0168232917785645 = 1.3003839254379272 + 0.1 * 7.164394855499268
Epoch 130, val loss: 1.3780597448349
Epoch 140, training loss: 1.918658971786499 = 1.2053263187408447 + 0.1 * 7.133326530456543
Epoch 140, val loss: 1.3036165237426758
Epoch 150, training loss: 1.825157642364502 = 1.115458369255066 + 0.1 * 7.096992492675781
Epoch 150, val loss: 1.2341029644012451
Epoch 160, training loss: 1.740813970565796 = 1.0348631143569946 + 0.1 * 7.059508323669434
Epoch 160, val loss: 1.1728246212005615
Epoch 170, training loss: 1.6654870510101318 = 0.962861180305481 + 0.1 * 7.026258945465088
Epoch 170, val loss: 1.1191679239273071
Epoch 180, training loss: 1.5974931716918945 = 0.8976458311080933 + 0.1 * 6.998473167419434
Epoch 180, val loss: 1.0709195137023926
Epoch 190, training loss: 1.533945918083191 = 0.8367640972137451 + 0.1 * 6.971817970275879
Epoch 190, val loss: 1.0259308815002441
Epoch 200, training loss: 1.4743095636367798 = 0.7792328596115112 + 0.1 * 6.9507670402526855
Epoch 200, val loss: 0.9832807779312134
Epoch 210, training loss: 1.4192302227020264 = 0.7261615991592407 + 0.1 * 6.930685520172119
Epoch 210, val loss: 0.9441483020782471
Epoch 220, training loss: 1.3690297603607178 = 0.677522599697113 + 0.1 * 6.915071964263916
Epoch 220, val loss: 0.9092591404914856
Epoch 230, training loss: 1.3226828575134277 = 0.6325874328613281 + 0.1 * 6.900954723358154
Epoch 230, val loss: 0.878498375415802
Epoch 240, training loss: 1.2794289588928223 = 0.5901183485984802 + 0.1 * 6.893106460571289
Epoch 240, val loss: 0.851064145565033
Epoch 250, training loss: 1.2375984191894531 = 0.5491024255752563 + 0.1 * 6.884959697723389
Epoch 250, val loss: 0.8263910412788391
Epoch 260, training loss: 1.1965036392211914 = 0.5089631676673889 + 0.1 * 6.875404357910156
Epoch 260, val loss: 0.8039846420288086
Epoch 270, training loss: 1.1574461460113525 = 0.4700552225112915 + 0.1 * 6.873908519744873
Epoch 270, val loss: 0.7842209339141846
Epoch 280, training loss: 1.1189634799957275 = 0.43253853917121887 + 0.1 * 6.864249229431152
Epoch 280, val loss: 0.7672079205513
Epoch 290, training loss: 1.0822839736938477 = 0.39645788073539734 + 0.1 * 6.8582611083984375
Epoch 290, val loss: 0.7528895735740662
Epoch 300, training loss: 1.046736240386963 = 0.3617420792579651 + 0.1 * 6.849940776824951
Epoch 300, val loss: 0.7411121726036072
Epoch 310, training loss: 1.0125588178634644 = 0.3283226788043976 + 0.1 * 6.842360973358154
Epoch 310, val loss: 0.7315980195999146
Epoch 320, training loss: 0.9800800681114197 = 0.2964963912963867 + 0.1 * 6.835836887359619
Epoch 320, val loss: 0.7241799235343933
Epoch 330, training loss: 0.9505451917648315 = 0.2667848765850067 + 0.1 * 6.837603569030762
Epoch 330, val loss: 0.7189696431159973
Epoch 340, training loss: 0.9223340749740601 = 0.23964111506938934 + 0.1 * 6.826929092407227
Epoch 340, val loss: 0.7160703539848328
Epoch 350, training loss: 0.8975028991699219 = 0.2149513065814972 + 0.1 * 6.825515270233154
Epoch 350, val loss: 0.7153590321540833
Epoch 360, training loss: 0.8744778037071228 = 0.19274580478668213 + 0.1 * 6.817319869995117
Epoch 360, val loss: 0.7166624665260315
Epoch 370, training loss: 0.8537647128105164 = 0.1728508621454239 + 0.1 * 6.809138774871826
Epoch 370, val loss: 0.7198318243026733
Epoch 380, training loss: 0.8347183465957642 = 0.15510891377925873 + 0.1 * 6.796093940734863
Epoch 380, val loss: 0.7244991660118103
Epoch 390, training loss: 0.8186528086662292 = 0.13933300971984863 + 0.1 * 6.793198108673096
Epoch 390, val loss: 0.7303823232650757
Epoch 400, training loss: 0.8051021695137024 = 0.12540805339813232 + 0.1 * 6.79694128036499
Epoch 400, val loss: 0.7371464371681213
Epoch 410, training loss: 0.7913109064102173 = 0.11316514760255814 + 0.1 * 6.781457424163818
Epoch 410, val loss: 0.7444547414779663
Epoch 420, training loss: 0.7790606617927551 = 0.10232551395893097 + 0.1 * 6.7673516273498535
Epoch 420, val loss: 0.7523692846298218
Epoch 430, training loss: 0.7704610228538513 = 0.09270571917295456 + 0.1 * 6.777552604675293
Epoch 430, val loss: 0.7606751322746277
Epoch 440, training loss: 0.7600203156471252 = 0.08420778065919876 + 0.1 * 6.758125305175781
Epoch 440, val loss: 0.7692437767982483
Epoch 450, training loss: 0.7529700994491577 = 0.07668531686067581 + 0.1 * 6.762847900390625
Epoch 450, val loss: 0.7780103087425232
Epoch 460, training loss: 0.7445095181465149 = 0.07001479715108871 + 0.1 * 6.74494743347168
Epoch 460, val loss: 0.7867943048477173
Epoch 470, training loss: 0.7411824464797974 = 0.064069002866745 + 0.1 * 6.771133899688721
Epoch 470, val loss: 0.7957577705383301
Epoch 480, training loss: 0.7324687242507935 = 0.05879199877381325 + 0.1 * 6.736767292022705
Epoch 480, val loss: 0.804569661617279
Epoch 490, training loss: 0.7266501784324646 = 0.05407991260290146 + 0.1 * 6.72570276260376
Epoch 490, val loss: 0.8134168982505798
Epoch 500, training loss: 0.721630871295929 = 0.04985010623931885 + 0.1 * 6.717807292938232
Epoch 500, val loss: 0.8222735524177551
Epoch 510, training loss: 0.719465434551239 = 0.046044327318668365 + 0.1 * 6.73421049118042
Epoch 510, val loss: 0.8310720324516296
Epoch 520, training loss: 0.7145347595214844 = 0.042634058743715286 + 0.1 * 6.719006538391113
Epoch 520, val loss: 0.8396568894386292
Epoch 530, training loss: 0.7099896669387817 = 0.03956824913620949 + 0.1 * 6.704214096069336
Epoch 530, val loss: 0.8481388688087463
Epoch 540, training loss: 0.7081677913665771 = 0.03679585084319115 + 0.1 * 6.713719367980957
Epoch 540, val loss: 0.8565177917480469
Epoch 550, training loss: 0.704674482345581 = 0.03429216518998146 + 0.1 * 6.703823089599609
Epoch 550, val loss: 0.8646147847175598
Epoch 560, training loss: 0.7013375163078308 = 0.03202611207962036 + 0.1 * 6.693113803863525
Epoch 560, val loss: 0.872692883014679
Epoch 570, training loss: 0.6979960203170776 = 0.029970722272992134 + 0.1 * 6.680253028869629
Epoch 570, val loss: 0.8805330395698547
Epoch 580, training loss: 0.6964903473854065 = 0.02810196951031685 + 0.1 * 6.6838836669921875
Epoch 580, val loss: 0.8881819844245911
Epoch 590, training loss: 0.6936159729957581 = 0.026399249210953712 + 0.1 * 6.67216682434082
Epoch 590, val loss: 0.8956518769264221
Epoch 600, training loss: 0.6920467615127563 = 0.02484356053173542 + 0.1 * 6.672031879425049
Epoch 600, val loss: 0.9029698967933655
Epoch 610, training loss: 0.689162015914917 = 0.023421064019203186 + 0.1 * 6.65740966796875
Epoch 610, val loss: 0.9101634621620178
Epoch 620, training loss: 0.6871895790100098 = 0.02211783640086651 + 0.1 * 6.650717258453369
Epoch 620, val loss: 0.9171364903450012
Epoch 630, training loss: 0.6866559982299805 = 0.02092043310403824 + 0.1 * 6.657355308532715
Epoch 630, val loss: 0.9239975810050964
Epoch 640, training loss: 0.6857167482376099 = 0.019820677116513252 + 0.1 * 6.658960342407227
Epoch 640, val loss: 0.9305806756019592
Epoch 650, training loss: 0.6828957796096802 = 0.018808752298355103 + 0.1 * 6.640870094299316
Epoch 650, val loss: 0.9370954632759094
Epoch 660, training loss: 0.6830878257751465 = 0.017871612682938576 + 0.1 * 6.652162075042725
Epoch 660, val loss: 0.9434903860092163
Epoch 670, training loss: 0.6808013916015625 = 0.017005233094096184 + 0.1 * 6.6379618644714355
Epoch 670, val loss: 0.9496205449104309
Epoch 680, training loss: 0.6798489689826965 = 0.016203206032514572 + 0.1 * 6.636457443237305
Epoch 680, val loss: 0.9556474685668945
Epoch 690, training loss: 0.6789684295654297 = 0.015456940047442913 + 0.1 * 6.635115146636963
Epoch 690, val loss: 0.9615763425827026
Epoch 700, training loss: 0.6766484379768372 = 0.014764063991606236 + 0.1 * 6.6188435554504395
Epoch 700, val loss: 0.9673029184341431
Epoch 710, training loss: 0.6763038635253906 = 0.01411803625524044 + 0.1 * 6.6218581199646
Epoch 710, val loss: 0.9729843735694885
Epoch 720, training loss: 0.6776451468467712 = 0.013514748774468899 + 0.1 * 6.641303539276123
Epoch 720, val loss: 0.9784623980522156
Epoch 730, training loss: 0.6749259829521179 = 0.012952800840139389 + 0.1 * 6.619731903076172
Epoch 730, val loss: 0.983799934387207
Epoch 740, training loss: 0.6733299493789673 = 0.01242965366691351 + 0.1 * 6.609002590179443
Epoch 740, val loss: 0.9888991713523865
Epoch 750, training loss: 0.6713011860847473 = 0.011938695795834064 + 0.1 * 6.593624591827393
Epoch 750, val loss: 0.9940106272697449
Epoch 760, training loss: 0.671424150466919 = 0.011477023363113403 + 0.1 * 6.599470615386963
Epoch 760, val loss: 0.9989607334136963
Epoch 770, training loss: 0.673761248588562 = 0.011043112725019455 + 0.1 * 6.627181053161621
Epoch 770, val loss: 1.0037498474121094
Epoch 780, training loss: 0.6701979041099548 = 0.010636545717716217 + 0.1 * 6.5956130027771
Epoch 780, val loss: 1.0085159540176392
Epoch 790, training loss: 0.6684000492095947 = 0.010254760272800922 + 0.1 * 6.5814528465271
Epoch 790, val loss: 1.0130952596664429
Epoch 800, training loss: 0.6687389612197876 = 0.009893184527754784 + 0.1 * 6.5884575843811035
Epoch 800, val loss: 1.0176604986190796
Epoch 810, training loss: 0.6684891581535339 = 0.009551076218485832 + 0.1 * 6.589381217956543
Epoch 810, val loss: 1.0220388174057007
Epoch 820, training loss: 0.6669377684593201 = 0.00922915618866682 + 0.1 * 6.577085494995117
Epoch 820, val loss: 1.026374340057373
Epoch 830, training loss: 0.6658666729927063 = 0.008923751302063465 + 0.1 * 6.56942892074585
Epoch 830, val loss: 1.0305474996566772
Epoch 840, training loss: 0.6684597134590149 = 0.008634842932224274 + 0.1 * 6.598248481750488
Epoch 840, val loss: 1.0346866846084595
Epoch 850, training loss: 0.6639618277549744 = 0.008361627347767353 + 0.1 * 6.556001663208008
Epoch 850, val loss: 1.0387392044067383
Epoch 860, training loss: 0.664012610912323 = 0.008102419786155224 + 0.1 * 6.559101581573486
Epoch 860, val loss: 1.0426722764968872
Epoch 870, training loss: 0.6628087162971497 = 0.007855522446334362 + 0.1 * 6.549531936645508
Epoch 870, val loss: 1.0465571880340576
Epoch 880, training loss: 0.6632915139198303 = 0.007621324621140957 + 0.1 * 6.55670166015625
Epoch 880, val loss: 1.0502843856811523
Epoch 890, training loss: 0.6612230539321899 = 0.007397996261715889 + 0.1 * 6.53825044631958
Epoch 890, val loss: 1.053954005241394
Epoch 900, training loss: 0.6622522473335266 = 0.007184871472418308 + 0.1 * 6.550673484802246
Epoch 900, val loss: 1.0576374530792236
Epoch 910, training loss: 0.6623807549476624 = 0.006982756312936544 + 0.1 * 6.553979873657227
Epoch 910, val loss: 1.0611339807510376
Epoch 920, training loss: 0.6606337428092957 = 0.006790532264858484 + 0.1 * 6.5384321212768555
Epoch 920, val loss: 1.0645694732666016
Epoch 930, training loss: 0.659795880317688 = 0.0066059548407793045 + 0.1 * 6.5318989753723145
Epoch 930, val loss: 1.0680445432662964
Epoch 940, training loss: 0.6590947508811951 = 0.006430045235902071 + 0.1 * 6.526647090911865
Epoch 940, val loss: 1.0713589191436768
Epoch 950, training loss: 0.6601893901824951 = 0.00626160716637969 + 0.1 * 6.539278030395508
Epoch 950, val loss: 1.0746269226074219
Epoch 960, training loss: 0.6591699719429016 = 0.006100105587393045 + 0.1 * 6.530698299407959
Epoch 960, val loss: 1.077915072441101
Epoch 970, training loss: 0.6586501002311707 = 0.005946340039372444 + 0.1 * 6.527037620544434
Epoch 970, val loss: 1.081080436706543
Epoch 980, training loss: 0.6581391096115112 = 0.005798516795039177 + 0.1 * 6.5234055519104
Epoch 980, val loss: 1.0841556787490845
Epoch 990, training loss: 0.6574171185493469 = 0.005656737368553877 + 0.1 * 6.517603874206543
Epoch 990, val loss: 1.0871810913085938
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8392198207696363
The final CL Acc:0.79383, 0.00175, The final GNN Acc:0.83834, 0.00203
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11572])
remove edge: torch.Size([2, 9592])
updated graph: torch.Size([2, 10608])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.797107458114624 = 1.9374244213104248 + 0.1 * 8.596829414367676
Epoch 0, val loss: 1.9338417053222656
Epoch 10, training loss: 2.7869110107421875 = 1.9272388219833374 + 0.1 * 8.596722602844238
Epoch 10, val loss: 1.9238543510437012
Epoch 20, training loss: 2.7741353511810303 = 1.9145138263702393 + 0.1 * 8.59621524810791
Epoch 20, val loss: 1.9110392332077026
Epoch 30, training loss: 2.7560548782348633 = 1.8967781066894531 + 0.1 * 8.592766761779785
Epoch 30, val loss: 1.8930338621139526
Epoch 40, training loss: 2.7284913063049316 = 1.8711752891540527 + 0.1 * 8.573161125183105
Epoch 40, val loss: 1.8673745393753052
Epoch 50, training loss: 2.688016414642334 = 1.8380286693572998 + 0.1 * 8.4998779296875
Epoch 50, val loss: 1.8357445001602173
Epoch 60, training loss: 2.6367452144622803 = 1.8051903247833252 + 0.1 * 8.315547943115234
Epoch 60, val loss: 1.8067951202392578
Epoch 70, training loss: 2.587564706802368 = 1.7756378650665283 + 0.1 * 8.119268417358398
Epoch 70, val loss: 1.7796199321746826
Epoch 80, training loss: 2.5135695934295654 = 1.7379931211471558 + 0.1 * 7.755764007568359
Epoch 80, val loss: 1.7433985471725464
Epoch 90, training loss: 2.432509660720825 = 1.6905763149261475 + 0.1 * 7.4193339347839355
Epoch 90, val loss: 1.7018134593963623
Epoch 100, training loss: 2.352517604827881 = 1.6295641660690308 + 0.1 * 7.229535102844238
Epoch 100, val loss: 1.6498639583587646
Epoch 110, training loss: 2.268049955368042 = 1.5519074201583862 + 0.1 * 7.161426067352295
Epoch 110, val loss: 1.5797895193099976
Epoch 120, training loss: 2.1758980751037598 = 1.4646124839782715 + 0.1 * 7.112856864929199
Epoch 120, val loss: 1.5059224367141724
Epoch 130, training loss: 2.0800371170043945 = 1.3736368417739868 + 0.1 * 7.064002990722656
Epoch 130, val loss: 1.4302750825881958
Epoch 140, training loss: 1.9810128211975098 = 1.2794896364212036 + 0.1 * 7.015231132507324
Epoch 140, val loss: 1.3527675867080688
Epoch 150, training loss: 1.881575345993042 = 1.1843593120574951 + 0.1 * 6.972159385681152
Epoch 150, val loss: 1.2753489017486572
Epoch 160, training loss: 1.7823556661605835 = 1.0881977081298828 + 0.1 * 6.941579341888428
Epoch 160, val loss: 1.1973603963851929
Epoch 170, training loss: 1.6874628067016602 = 0.9948897361755371 + 0.1 * 6.9257307052612305
Epoch 170, val loss: 1.1229757070541382
Epoch 180, training loss: 1.5989501476287842 = 0.9079386591911316 + 0.1 * 6.910114288330078
Epoch 180, val loss: 1.0542597770690918
Epoch 190, training loss: 1.519510269165039 = 0.8299713730812073 + 0.1 * 6.895389556884766
Epoch 190, val loss: 0.9933497905731201
Epoch 200, training loss: 1.4511123895645142 = 0.7621819972991943 + 0.1 * 6.889303684234619
Epoch 200, val loss: 0.9420124888420105
Epoch 210, training loss: 1.3910776376724243 = 0.7045509219169617 + 0.1 * 6.865267276763916
Epoch 210, val loss: 0.9004865884780884
Epoch 220, training loss: 1.340135097503662 = 0.6545361876487732 + 0.1 * 6.855989456176758
Epoch 220, val loss: 0.866963267326355
Epoch 230, training loss: 1.2937285900115967 = 0.6101549863815308 + 0.1 * 6.8357367515563965
Epoch 230, val loss: 0.839876115322113
Epoch 240, training loss: 1.2517297267913818 = 0.5695385336875916 + 0.1 * 6.821911334991455
Epoch 240, val loss: 0.8177401423454285
Epoch 250, training loss: 1.2129102945327759 = 0.531493604183197 + 0.1 * 6.81416654586792
Epoch 250, val loss: 0.7997599840164185
Epoch 260, training loss: 1.1754188537597656 = 0.49558860063552856 + 0.1 * 6.798301696777344
Epoch 260, val loss: 0.7851986289024353
Epoch 270, training loss: 1.140737771987915 = 0.46120819449424744 + 0.1 * 6.795295238494873
Epoch 270, val loss: 0.7732933163642883
Epoch 280, training loss: 1.1059319972991943 = 0.4282335042953491 + 0.1 * 6.776985168457031
Epoch 280, val loss: 0.7638205885887146
Epoch 290, training loss: 1.0728704929351807 = 0.39654678106307983 + 0.1 * 6.763236999511719
Epoch 290, val loss: 0.7564120292663574
Epoch 300, training loss: 1.0413802862167358 = 0.3661001920700073 + 0.1 * 6.752800941467285
Epoch 300, val loss: 0.7509034872055054
Epoch 310, training loss: 1.0129033327102661 = 0.33708375692367554 + 0.1 * 6.758195877075195
Epoch 310, val loss: 0.7474881410598755
Epoch 320, training loss: 0.9837034940719604 = 0.3095661997795105 + 0.1 * 6.741373062133789
Epoch 320, val loss: 0.7460412979125977
Epoch 330, training loss: 0.9564501643180847 = 0.283475399017334 + 0.1 * 6.729747295379639
Epoch 330, val loss: 0.7464250922203064
Epoch 340, training loss: 0.9315480589866638 = 0.2588843107223511 + 0.1 * 6.726637363433838
Epoch 340, val loss: 0.7484626770019531
Epoch 350, training loss: 0.907145082950592 = 0.23587565124034882 + 0.1 * 6.71269416809082
Epoch 350, val loss: 0.7523425817489624
Epoch 360, training loss: 0.8867167234420776 = 0.21460385620594025 + 0.1 * 6.721128463745117
Epoch 360, val loss: 0.7577627301216125
Epoch 370, training loss: 0.8659433126449585 = 0.1953793466091156 + 0.1 * 6.705639839172363
Epoch 370, val loss: 0.7646026015281677
Epoch 380, training loss: 0.8484378457069397 = 0.17820008099079132 + 0.1 * 6.702377796173096
Epoch 380, val loss: 0.7727137207984924
Epoch 390, training loss: 0.8318321704864502 = 0.1629086285829544 + 0.1 * 6.689235210418701
Epoch 390, val loss: 0.7819161415100098
Epoch 400, training loss: 0.8175114393234253 = 0.14922136068344116 + 0.1 * 6.682900428771973
Epoch 400, val loss: 0.7919658422470093
Epoch 410, training loss: 0.8047690391540527 = 0.13694778084754944 + 0.1 * 6.678212642669678
Epoch 410, val loss: 0.8025294542312622
Epoch 420, training loss: 0.7965182065963745 = 0.12591861188411713 + 0.1 * 6.705995559692383
Epoch 420, val loss: 0.8134317994117737
Epoch 430, training loss: 0.7837533950805664 = 0.11604341864585876 + 0.1 * 6.677099704742432
Epoch 430, val loss: 0.824597954750061
Epoch 440, training loss: 0.7734267711639404 = 0.10709769278764725 + 0.1 * 6.663290500640869
Epoch 440, val loss: 0.8359711170196533
Epoch 450, training loss: 0.7642747163772583 = 0.09895194321870804 + 0.1 * 6.653227806091309
Epoch 450, val loss: 0.8476110696792603
Epoch 460, training loss: 0.7564936280250549 = 0.09151125699281693 + 0.1 * 6.6498236656188965
Epoch 460, val loss: 0.8594571948051453
Epoch 470, training loss: 0.7516250014305115 = 0.08473017066717148 + 0.1 * 6.668948173522949
Epoch 470, val loss: 0.8711987137794495
Epoch 480, training loss: 0.7430088520050049 = 0.07854446023702621 + 0.1 * 6.644644260406494
Epoch 480, val loss: 0.883097231388092
Epoch 490, training loss: 0.7377711534500122 = 0.07287601381540298 + 0.1 * 6.648951053619385
Epoch 490, val loss: 0.894996702671051
Epoch 500, training loss: 0.7326492071151733 = 0.06768988072872162 + 0.1 * 6.649592876434326
Epoch 500, val loss: 0.9067771434783936
Epoch 510, training loss: 0.7260637283325195 = 0.06294243782758713 + 0.1 * 6.6312127113342285
Epoch 510, val loss: 0.918592631816864
Epoch 520, training loss: 0.721737265586853 = 0.058588262647390366 + 0.1 * 6.6314897537231445
Epoch 520, val loss: 0.930324912071228
Epoch 530, training loss: 0.7175405025482178 = 0.05460149794816971 + 0.1 * 6.629390239715576
Epoch 530, val loss: 0.9418684244155884
Epoch 540, training loss: 0.7129690647125244 = 0.05095503479242325 + 0.1 * 6.620140075683594
Epoch 540, val loss: 0.9533184170722961
Epoch 550, training loss: 0.7086536288261414 = 0.047611698508262634 + 0.1 * 6.610419273376465
Epoch 550, val loss: 0.9647347927093506
Epoch 560, training loss: 0.7054271101951599 = 0.04454568773508072 + 0.1 * 6.608813762664795
Epoch 560, val loss: 0.9758784770965576
Epoch 570, training loss: 0.7046676278114319 = 0.041736919432878494 + 0.1 * 6.629307270050049
Epoch 570, val loss: 0.9869443774223328
Epoch 580, training loss: 0.6994647979736328 = 0.039165787398815155 + 0.1 * 6.602989673614502
Epoch 580, val loss: 0.9977524280548096
Epoch 590, training loss: 0.6966749429702759 = 0.036803875118494034 + 0.1 * 6.598710536956787
Epoch 590, val loss: 1.0084651708602905
Epoch 600, training loss: 0.6938785314559937 = 0.03463335335254669 + 0.1 * 6.592451572418213
Epoch 600, val loss: 1.0188804864883423
Epoch 610, training loss: 0.6916463375091553 = 0.03263774514198303 + 0.1 * 6.590085983276367
Epoch 610, val loss: 1.029261827468872
Epoch 620, training loss: 0.6914001107215881 = 0.03079855814576149 + 0.1 * 6.606015205383301
Epoch 620, val loss: 1.0393859148025513
Epoch 630, training loss: 0.6876160502433777 = 0.029106901958584785 + 0.1 * 6.5850911140441895
Epoch 630, val loss: 1.0491677522659302
Epoch 640, training loss: 0.6858636736869812 = 0.027548346668481827 + 0.1 * 6.583152770996094
Epoch 640, val loss: 1.0588905811309814
Epoch 650, training loss: 0.6842461824417114 = 0.02610676921904087 + 0.1 * 6.581393718719482
Epoch 650, val loss: 1.0684144496917725
Epoch 660, training loss: 0.6818731427192688 = 0.02477305382490158 + 0.1 * 6.571000576019287
Epoch 660, val loss: 1.0776022672653198
Epoch 670, training loss: 0.6809276938438416 = 0.023540716618299484 + 0.1 * 6.573869705200195
Epoch 670, val loss: 1.0866830348968506
Epoch 680, training loss: 0.6789408922195435 = 0.022396009415388107 + 0.1 * 6.56544828414917
Epoch 680, val loss: 1.095501184463501
Epoch 690, training loss: 0.6779688596725464 = 0.0213357824832201 + 0.1 * 6.566330432891846
Epoch 690, val loss: 1.104202389717102
Epoch 700, training loss: 0.6763553023338318 = 0.02034762129187584 + 0.1 * 6.560076713562012
Epoch 700, val loss: 1.1127339601516724
Epoch 710, training loss: 0.6760889887809753 = 0.019426854327321053 + 0.1 * 6.566620826721191
Epoch 710, val loss: 1.1209428310394287
Epoch 720, training loss: 0.6745032668113708 = 0.018570667132735252 + 0.1 * 6.559325695037842
Epoch 720, val loss: 1.1290351152420044
Epoch 730, training loss: 0.6738319993019104 = 0.017771480605006218 + 0.1 * 6.560605049133301
Epoch 730, val loss: 1.1369932889938354
Epoch 740, training loss: 0.6721194982528687 = 0.017023758962750435 + 0.1 * 6.550957202911377
Epoch 740, val loss: 1.144779920578003
Epoch 750, training loss: 0.6714121103286743 = 0.016323382034897804 + 0.1 * 6.550887107849121
Epoch 750, val loss: 1.1523076295852661
Epoch 760, training loss: 0.6702513098716736 = 0.01566791906952858 + 0.1 * 6.545833587646484
Epoch 760, val loss: 1.159650444984436
Epoch 770, training loss: 0.6697714328765869 = 0.015053235925734043 + 0.1 * 6.547182083129883
Epoch 770, val loss: 1.1669889688491821
Epoch 780, training loss: 0.668109118938446 = 0.014474375173449516 + 0.1 * 6.536346912384033
Epoch 780, val loss: 1.1740460395812988
Epoch 790, training loss: 0.6697486639022827 = 0.013930882327258587 + 0.1 * 6.558177947998047
Epoch 790, val loss: 1.180944800376892
Epoch 800, training loss: 0.6669944524765015 = 0.013419748283922672 + 0.1 * 6.535747051239014
Epoch 800, val loss: 1.1876648664474487
Epoch 810, training loss: 0.6657700538635254 = 0.01293810922652483 + 0.1 * 6.528319358825684
Epoch 810, val loss: 1.194357991218567
Epoch 820, training loss: 0.6651334762573242 = 0.012483176775276661 + 0.1 * 6.52650260925293
Epoch 820, val loss: 1.2009358406066895
Epoch 830, training loss: 0.6672194600105286 = 0.012052193284034729 + 0.1 * 6.551672458648682
Epoch 830, val loss: 1.2072670459747314
Epoch 840, training loss: 0.6646576523780823 = 0.011644921265542507 + 0.1 * 6.53012752532959
Epoch 840, val loss: 1.2134991884231567
Epoch 850, training loss: 0.6662116050720215 = 0.011259810999035835 + 0.1 * 6.54951810836792
Epoch 850, val loss: 1.219704270362854
Epoch 860, training loss: 0.6638705134391785 = 0.010894612409174442 + 0.1 * 6.529758930206299
Epoch 860, val loss: 1.2255151271820068
Epoch 870, training loss: 0.6629294753074646 = 0.010550100356340408 + 0.1 * 6.523793697357178
Epoch 870, val loss: 1.231499433517456
Epoch 880, training loss: 0.661882221698761 = 0.010221471078693867 + 0.1 * 6.516607284545898
Epoch 880, val loss: 1.237246036529541
Epoch 890, training loss: 0.6624559760093689 = 0.00990968570113182 + 0.1 * 6.525462627410889
Epoch 890, val loss: 1.2428367137908936
Epoch 900, training loss: 0.660520613193512 = 0.009613826870918274 + 0.1 * 6.509068012237549
Epoch 900, val loss: 1.2483861446380615
Epoch 910, training loss: 0.6606290340423584 = 0.009331521578133106 + 0.1 * 6.512975215911865
Epoch 910, val loss: 1.2539036273956299
Epoch 920, training loss: 0.6595091819763184 = 0.009061994031071663 + 0.1 * 6.504471778869629
Epoch 920, val loss: 1.2590487003326416
Epoch 930, training loss: 0.6606332063674927 = 0.008805991150438786 + 0.1 * 6.5182719230651855
Epoch 930, val loss: 1.2642159461975098
Epoch 940, training loss: 0.658789336681366 = 0.00856185145676136 + 0.1 * 6.502274513244629
Epoch 940, val loss: 1.269245982170105
Epoch 950, training loss: 0.6582881808280945 = 0.008328698575496674 + 0.1 * 6.499594688415527
Epoch 950, val loss: 1.2743191719055176
Epoch 960, training loss: 0.658063530921936 = 0.008105529472231865 + 0.1 * 6.499579906463623
Epoch 960, val loss: 1.279236912727356
Epoch 970, training loss: 0.6573776602745056 = 0.007891956716775894 + 0.1 * 6.494856834411621
Epoch 970, val loss: 1.2839652299880981
Epoch 980, training loss: 0.6569094657897949 = 0.007688409648835659 + 0.1 * 6.492210388183594
Epoch 980, val loss: 1.28861665725708
Epoch 990, training loss: 0.6565743684768677 = 0.007493225391954184 + 0.1 * 6.490811347961426
Epoch 990, val loss: 1.2933181524276733
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 2.813105583190918 = 1.9534235000610352 + 0.1 * 8.596821784973145
Epoch 0, val loss: 1.9487693309783936
Epoch 10, training loss: 2.802859306335449 = 1.9431837797164917 + 0.1 * 8.596755981445312
Epoch 10, val loss: 1.938070297241211
Epoch 20, training loss: 2.7907557487487793 = 1.931114673614502 + 0.1 * 8.596409797668457
Epoch 20, val loss: 1.9252654314041138
Epoch 30, training loss: 2.7738454341888428 = 1.9144659042358398 + 0.1 * 8.593794822692871
Epoch 30, val loss: 1.9074474573135376
Epoch 40, training loss: 2.7473511695861816 = 1.889986276626587 + 0.1 * 8.573647499084473
Epoch 40, val loss: 1.8813272714614868
Epoch 50, training loss: 2.702179431915283 = 1.855860948562622 + 0.1 * 8.463184356689453
Epoch 50, val loss: 1.8463034629821777
Epoch 60, training loss: 2.6365408897399902 = 1.8185220956802368 + 0.1 * 8.180188179016113
Epoch 60, val loss: 1.8117305040359497
Epoch 70, training loss: 2.556326150894165 = 1.787139892578125 + 0.1 * 7.691862106323242
Epoch 70, val loss: 1.7855793237686157
Epoch 80, training loss: 2.490750789642334 = 1.7565149068832397 + 0.1 * 7.3423590660095215
Epoch 80, val loss: 1.759423017501831
Epoch 90, training loss: 2.437539577484131 = 1.7195415496826172 + 0.1 * 7.179980754852295
Epoch 90, val loss: 1.726059079170227
Epoch 100, training loss: 2.381579875946045 = 1.6695629358291626 + 0.1 * 7.120169162750244
Epoch 100, val loss: 1.681536078453064
Epoch 110, training loss: 2.3087069988250732 = 1.6018480062484741 + 0.1 * 7.0685906410217285
Epoch 110, val loss: 1.6224833726882935
Epoch 120, training loss: 2.218503952026367 = 1.5165265798568726 + 0.1 * 7.019773960113525
Epoch 120, val loss: 1.5482792854309082
Epoch 130, training loss: 2.1145191192626953 = 1.4172056913375854 + 0.1 * 6.973134517669678
Epoch 130, val loss: 1.4634301662445068
Epoch 140, training loss: 2.0028882026672363 = 1.3085745573043823 + 0.1 * 6.943135738372803
Epoch 140, val loss: 1.3722813129425049
Epoch 150, training loss: 1.887803554534912 = 1.1960947513580322 + 0.1 * 6.917087078094482
Epoch 150, val loss: 1.2791659832000732
Epoch 160, training loss: 1.7740051746368408 = 1.0839897394180298 + 0.1 * 6.900154113769531
Epoch 160, val loss: 1.1870567798614502
Epoch 170, training loss: 1.6683330535888672 = 0.9798948168754578 + 0.1 * 6.884382724761963
Epoch 170, val loss: 1.1023776531219482
Epoch 180, training loss: 1.57586669921875 = 0.8890979886054993 + 0.1 * 6.867687702178955
Epoch 180, val loss: 1.0298601388931274
Epoch 190, training loss: 1.4984703063964844 = 0.8132683038711548 + 0.1 * 6.852020263671875
Epoch 190, val loss: 0.9716938138008118
Epoch 200, training loss: 1.4364386796951294 = 0.751910924911499 + 0.1 * 6.845277309417725
Epoch 200, val loss: 0.927953839302063
Epoch 210, training loss: 1.3831853866577148 = 0.7007346153259277 + 0.1 * 6.824507236480713
Epoch 210, val loss: 0.8944198489189148
Epoch 220, training loss: 1.336705207824707 = 0.6557894945144653 + 0.1 * 6.809156894683838
Epoch 220, val loss: 0.8675434589385986
Epoch 230, training loss: 1.2974929809570312 = 0.6147657036781311 + 0.1 * 6.827272415161133
Epoch 230, val loss: 0.8450897336006165
Epoch 240, training loss: 1.2563855648040771 = 0.5770719647407532 + 0.1 * 6.7931365966796875
Epoch 240, val loss: 0.8261409401893616
Epoch 250, training loss: 1.219624400138855 = 0.5414854884147644 + 0.1 * 6.781389236450195
Epoch 250, val loss: 0.8094901442527771
Epoch 260, training loss: 1.1842737197875977 = 0.5073304176330566 + 0.1 * 6.76943302154541
Epoch 260, val loss: 0.7950800061225891
Epoch 270, training loss: 1.150222897529602 = 0.4741930663585663 + 0.1 * 6.760297775268555
Epoch 270, val loss: 0.7824137806892395
Epoch 280, training loss: 1.1171011924743652 = 0.4418068528175354 + 0.1 * 6.752943992614746
Epoch 280, val loss: 0.771540105342865
Epoch 290, training loss: 1.0848767757415771 = 0.41013336181640625 + 0.1 * 6.747433662414551
Epoch 290, val loss: 0.7625323534011841
Epoch 300, training loss: 1.053592324256897 = 0.3793843686580658 + 0.1 * 6.742079734802246
Epoch 300, val loss: 0.7552981376647949
Epoch 310, training loss: 1.0229744911193848 = 0.3498916029930115 + 0.1 * 6.730829238891602
Epoch 310, val loss: 0.750044047832489
Epoch 320, training loss: 0.994896411895752 = 0.3218196630477905 + 0.1 * 6.730767250061035
Epoch 320, val loss: 0.7468137741088867
Epoch 330, training loss: 0.9679940938949585 = 0.2954529821872711 + 0.1 * 6.725411415100098
Epoch 330, val loss: 0.7454721927642822
Epoch 340, training loss: 0.9424253702163696 = 0.27060624957084656 + 0.1 * 6.718190670013428
Epoch 340, val loss: 0.7456835508346558
Epoch 350, training loss: 0.9184237718582153 = 0.24702312052249908 + 0.1 * 6.714006423950195
Epoch 350, val loss: 0.7473950386047363
Epoch 360, training loss: 0.8960859179496765 = 0.22476063668727875 + 0.1 * 6.713252544403076
Epoch 360, val loss: 0.7505596876144409
Epoch 370, training loss: 0.875022828578949 = 0.20405536890029907 + 0.1 * 6.70967435836792
Epoch 370, val loss: 0.7552976608276367
Epoch 380, training loss: 0.855360209941864 = 0.18521714210510254 + 0.1 * 6.701430797576904
Epoch 380, val loss: 0.7615240216255188
Epoch 390, training loss: 0.8374668955802917 = 0.168284073472023 + 0.1 * 6.69182825088501
Epoch 390, val loss: 0.7691344022750854
Epoch 400, training loss: 0.8218613862991333 = 0.15315601229667664 + 0.1 * 6.687053203582764
Epoch 400, val loss: 0.7780383825302124
Epoch 410, training loss: 0.8076199293136597 = 0.1396874338388443 + 0.1 * 6.679325103759766
Epoch 410, val loss: 0.7879093885421753
Epoch 420, training loss: 0.7960342168807983 = 0.1277197152376175 + 0.1 * 6.683145046234131
Epoch 420, val loss: 0.7984358668327332
Epoch 430, training loss: 0.7844526767730713 = 0.11712293326854706 + 0.1 * 6.673296928405762
Epoch 430, val loss: 0.8093851208686829
Epoch 440, training loss: 0.7744631171226501 = 0.10768652707338333 + 0.1 * 6.667766094207764
Epoch 440, val loss: 0.8206056356430054
Epoch 450, training loss: 0.7651898264884949 = 0.0992424413561821 + 0.1 * 6.659473896026611
Epoch 450, val loss: 0.8319836258888245
Epoch 460, training loss: 0.7570275068283081 = 0.09166738390922546 + 0.1 * 6.65360164642334
Epoch 460, val loss: 0.8434722423553467
Epoch 470, training loss: 0.751430869102478 = 0.08482711017131805 + 0.1 * 6.666037082672119
Epoch 470, val loss: 0.8550482392311096
Epoch 480, training loss: 0.7428375482559204 = 0.07866700738668442 + 0.1 * 6.641705513000488
Epoch 480, val loss: 0.8664588332176208
Epoch 490, training loss: 0.7370260953903198 = 0.0730866864323616 + 0.1 * 6.6393938064575195
Epoch 490, val loss: 0.8777823448181152
Epoch 500, training loss: 0.7315282225608826 = 0.06800893694162369 + 0.1 * 6.635192394256592
Epoch 500, val loss: 0.8890946507453918
Epoch 510, training loss: 0.727217972278595 = 0.06338857114315033 + 0.1 * 6.638293743133545
Epoch 510, val loss: 0.9001829624176025
Epoch 520, training loss: 0.7214065790176392 = 0.05918595939874649 + 0.1 * 6.62220573425293
Epoch 520, val loss: 0.9111889004707336
Epoch 530, training loss: 0.7170820832252502 = 0.05534452199935913 + 0.1 * 6.617375373840332
Epoch 530, val loss: 0.9219610691070557
Epoch 540, training loss: 0.7137597799301147 = 0.05182390287518501 + 0.1 * 6.619358539581299
Epoch 540, val loss: 0.93267822265625
Epoch 550, training loss: 0.7100443840026855 = 0.04860095679759979 + 0.1 * 6.614433765411377
Epoch 550, val loss: 0.9431118369102478
Epoch 560, training loss: 0.7071341276168823 = 0.04564518854022026 + 0.1 * 6.614889621734619
Epoch 560, val loss: 0.9534050822257996
Epoch 570, training loss: 0.7034971117973328 = 0.04293522983789444 + 0.1 * 6.605618476867676
Epoch 570, val loss: 0.9634957909584045
Epoch 580, training loss: 0.7002004384994507 = 0.04044535756111145 + 0.1 * 6.597550868988037
Epoch 580, val loss: 0.9733600616455078
Epoch 590, training loss: 0.6976481676101685 = 0.03814771771430969 + 0.1 * 6.595004081726074
Epoch 590, val loss: 0.9831385016441345
Epoch 600, training loss: 0.6952328085899353 = 0.03603040426969528 + 0.1 * 6.592023849487305
Epoch 600, val loss: 0.9926544427871704
Epoch 610, training loss: 0.6924854516983032 = 0.034077633172273636 + 0.1 * 6.584077835083008
Epoch 610, val loss: 1.001990795135498
Epoch 620, training loss: 0.6909295320510864 = 0.03226867690682411 + 0.1 * 6.586607933044434
Epoch 620, val loss: 1.0111851692199707
Epoch 630, training loss: 0.687971293926239 = 0.030595405027270317 + 0.1 * 6.573758602142334
Epoch 630, val loss: 1.0201208591461182
Epoch 640, training loss: 0.6860819458961487 = 0.029044603928923607 + 0.1 * 6.570373058319092
Epoch 640, val loss: 1.0289448499679565
Epoch 650, training loss: 0.6858372688293457 = 0.02760438248515129 + 0.1 * 6.582328796386719
Epoch 650, val loss: 1.037509799003601
Epoch 660, training loss: 0.6830336451530457 = 0.026268426328897476 + 0.1 * 6.567652225494385
Epoch 660, val loss: 1.0459715127944946
Epoch 670, training loss: 0.681819498538971 = 0.025022368878126144 + 0.1 * 6.567971229553223
Epoch 670, val loss: 1.054295539855957
Epoch 680, training loss: 0.680317759513855 = 0.023862339556217194 + 0.1 * 6.564553737640381
Epoch 680, val loss: 1.0623496770858765
Epoch 690, training loss: 0.6790667772293091 = 0.022782737389206886 + 0.1 * 6.562840461730957
Epoch 690, val loss: 1.0703351497650146
Epoch 700, training loss: 0.6776078343391418 = 0.021771814674139023 + 0.1 * 6.5583600997924805
Epoch 700, val loss: 1.0781021118164062
Epoch 710, training loss: 0.6758407354354858 = 0.02082652598619461 + 0.1 * 6.550142288208008
Epoch 710, val loss: 1.0858073234558105
Epoch 720, training loss: 0.6778709292411804 = 0.019939297810196877 + 0.1 * 6.579316139221191
Epoch 720, val loss: 1.0933012962341309
Epoch 730, training loss: 0.6749929189682007 = 0.019109325483441353 + 0.1 * 6.558835983276367
Epoch 730, val loss: 1.1006615161895752
Epoch 740, training loss: 0.6737979650497437 = 0.018331116065382957 + 0.1 * 6.554668426513672
Epoch 740, val loss: 1.1078650951385498
Epoch 750, training loss: 0.6720463633537292 = 0.017600107938051224 + 0.1 * 6.544462203979492
Epoch 750, val loss: 1.1149442195892334
Epoch 760, training loss: 0.671930730342865 = 0.016913345083594322 + 0.1 * 6.550173759460449
Epoch 760, val loss: 1.1218175888061523
Epoch 770, training loss: 0.6704574823379517 = 0.016267666593194008 + 0.1 * 6.541898250579834
Epoch 770, val loss: 1.1286463737487793
Epoch 780, training loss: 0.6699309945106506 = 0.01565791852772236 + 0.1 * 6.542730808258057
Epoch 780, val loss: 1.1353102922439575
Epoch 790, training loss: 0.6682774424552917 = 0.015083304606378078 + 0.1 * 6.531940937042236
Epoch 790, val loss: 1.1418309211730957
Epoch 800, training loss: 0.6675454378128052 = 0.014540651813149452 + 0.1 * 6.530047416687012
Epoch 800, val loss: 1.1482833623886108
Epoch 810, training loss: 0.6680474877357483 = 0.014026518911123276 + 0.1 * 6.5402092933654785
Epoch 810, val loss: 1.1546595096588135
Epoch 820, training loss: 0.6675341129302979 = 0.013540886342525482 + 0.1 * 6.5399322509765625
Epoch 820, val loss: 1.1608015298843384
Epoch 830, training loss: 0.6670498847961426 = 0.01308161299675703 + 0.1 * 6.539682865142822
Epoch 830, val loss: 1.1669148206710815
Epoch 840, training loss: 0.6656448841094971 = 0.012646764516830444 + 0.1 * 6.5299811363220215
Epoch 840, val loss: 1.1726956367492676
Epoch 850, training loss: 0.6649388670921326 = 0.012236109934747219 + 0.1 * 6.527027606964111
Epoch 850, val loss: 1.1785913705825806
Epoch 860, training loss: 0.6636043190956116 = 0.011844365857541561 + 0.1 * 6.517599582672119
Epoch 860, val loss: 1.1843405961990356
Epoch 870, training loss: 0.663985550403595 = 0.011471735313534737 + 0.1 * 6.525137901306152
Epoch 870, val loss: 1.190024733543396
Epoch 880, training loss: 0.6635038256645203 = 0.01111754309386015 + 0.1 * 6.523862838745117
Epoch 880, val loss: 1.195522665977478
Epoch 890, training loss: 0.6623111963272095 = 0.010780769400298595 + 0.1 * 6.5153045654296875
Epoch 890, val loss: 1.2009916305541992
Epoch 900, training loss: 0.6610181927680969 = 0.010460061021149158 + 0.1 * 6.505580902099609
Epoch 900, val loss: 1.2063857316970825
Epoch 910, training loss: 0.6625415086746216 = 0.010153642855584621 + 0.1 * 6.523878574371338
Epoch 910, val loss: 1.2116690874099731
Epoch 920, training loss: 0.6610215902328491 = 0.009862128645181656 + 0.1 * 6.511594772338867
Epoch 920, val loss: 1.2168112993240356
Epoch 930, training loss: 0.6602892875671387 = 0.009583755396306515 + 0.1 * 6.507055282592773
Epoch 930, val loss: 1.2218929529190063
Epoch 940, training loss: 0.6595526337623596 = 0.009318236261606216 + 0.1 * 6.502344131469727
Epoch 940, val loss: 1.2269247770309448
Epoch 950, training loss: 0.6599869728088379 = 0.009063837118446827 + 0.1 * 6.509231090545654
Epoch 950, val loss: 1.2318522930145264
Epoch 960, training loss: 0.6595033407211304 = 0.00882088951766491 + 0.1 * 6.506824493408203
Epoch 960, val loss: 1.2366331815719604
Epoch 970, training loss: 0.6597314476966858 = 0.008588409051299095 + 0.1 * 6.511430263519287
Epoch 970, val loss: 1.2414064407348633
Epoch 980, training loss: 0.657942533493042 = 0.008366139605641365 + 0.1 * 6.495763778686523
Epoch 980, val loss: 1.2460534572601318
Epoch 990, training loss: 0.6580021381378174 = 0.008153055794537067 + 0.1 * 6.498490333557129
Epoch 990, val loss: 1.2506728172302246
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8255139694254086
=== training gcn model ===
Epoch 0, training loss: 2.8236560821533203 = 1.9639779329299927 + 0.1 * 8.596782684326172
Epoch 0, val loss: 1.9667545557022095
Epoch 10, training loss: 2.8126795291900635 = 1.9530155658721924 + 0.1 * 8.596638679504395
Epoch 10, val loss: 1.955290675163269
Epoch 20, training loss: 2.798583745956421 = 1.939001202583313 + 0.1 * 8.5958251953125
Epoch 20, val loss: 1.9405648708343506
Epoch 30, training loss: 2.777977466583252 = 1.9190213680267334 + 0.1 * 8.589561462402344
Epoch 30, val loss: 1.9196053743362427
Epoch 40, training loss: 2.7451159954071045 = 1.8894822597503662 + 0.1 * 8.556337356567383
Epoch 40, val loss: 1.889039158821106
Epoch 50, training loss: 2.6925721168518066 = 1.8502291440963745 + 0.1 * 8.423430442810059
Epoch 50, val loss: 1.8505367040634155
Epoch 60, training loss: 2.6351640224456787 = 1.8097808361053467 + 0.1 * 8.25383186340332
Epoch 60, val loss: 1.8144301176071167
Epoch 70, training loss: 2.5636277198791504 = 1.778084397315979 + 0.1 * 7.855433940887451
Epoch 70, val loss: 1.787257194519043
Epoch 80, training loss: 2.4783077239990234 = 1.7474212646484375 + 0.1 * 7.308864593505859
Epoch 80, val loss: 1.7590320110321045
Epoch 90, training loss: 2.4210898876190186 = 1.7096095085144043 + 0.1 * 7.114804267883301
Epoch 90, val loss: 1.725128412246704
Epoch 100, training loss: 2.36152720451355 = 1.6570916175842285 + 0.1 * 7.044356346130371
Epoch 100, val loss: 1.6796923875808716
Epoch 110, training loss: 2.2912111282348633 = 1.589814305305481 + 0.1 * 7.013969421386719
Epoch 110, val loss: 1.6227197647094727
Epoch 120, training loss: 2.212395668029785 = 1.513586163520813 + 0.1 * 6.988094806671143
Epoch 120, val loss: 1.5587395429611206
Epoch 130, training loss: 2.1317851543426514 = 1.4352761507034302 + 0.1 * 6.965090274810791
Epoch 130, val loss: 1.4938538074493408
Epoch 140, training loss: 2.0532188415527344 = 1.3588404655456543 + 0.1 * 6.943783283233643
Epoch 140, val loss: 1.4321410655975342
Epoch 150, training loss: 1.9770896434783936 = 1.2850745916366577 + 0.1 * 6.920151233673096
Epoch 150, val loss: 1.3735493421554565
Epoch 160, training loss: 1.9004827737808228 = 1.210434913635254 + 0.1 * 6.900478363037109
Epoch 160, val loss: 1.3139407634735107
Epoch 170, training loss: 1.8225284814834595 = 1.1344488859176636 + 0.1 * 6.880795955657959
Epoch 170, val loss: 1.2538727521896362
Epoch 180, training loss: 1.7431120872497559 = 1.0566236972808838 + 0.1 * 6.864882946014404
Epoch 180, val loss: 1.191689372062683
Epoch 190, training loss: 1.6638281345367432 = 0.977813184261322 + 0.1 * 6.860148906707764
Epoch 190, val loss: 1.1286096572875977
Epoch 200, training loss: 1.5857901573181152 = 0.9021299481391907 + 0.1 * 6.836602210998535
Epoch 200, val loss: 1.067906141281128
Epoch 210, training loss: 1.5132555961608887 = 0.8312087059020996 + 0.1 * 6.820467948913574
Epoch 210, val loss: 1.0104007720947266
Epoch 220, training loss: 1.4476863145828247 = 0.7667557001113892 + 0.1 * 6.8093061447143555
Epoch 220, val loss: 0.9589107036590576
Epoch 230, training loss: 1.3895056247711182 = 0.7092958688735962 + 0.1 * 6.802098274230957
Epoch 230, val loss: 0.9144763946533203
Epoch 240, training loss: 1.3366587162017822 = 0.6575681567192078 + 0.1 * 6.790905952453613
Epoch 240, val loss: 0.8763655424118042
Epoch 250, training loss: 1.2868289947509766 = 0.6098417639732361 + 0.1 * 6.769871711730957
Epoch 250, val loss: 0.8431330919265747
Epoch 260, training loss: 1.2387187480926514 = 0.5635798573493958 + 0.1 * 6.751389026641846
Epoch 260, val loss: 0.8126682639122009
Epoch 270, training loss: 1.1928751468658447 = 0.5176360607147217 + 0.1 * 6.752389907836914
Epoch 270, val loss: 0.784050464630127
Epoch 280, training loss: 1.145750641822815 = 0.47230130434036255 + 0.1 * 6.734493255615234
Epoch 280, val loss: 0.757550835609436
Epoch 290, training loss: 1.1005651950836182 = 0.42802712321281433 + 0.1 * 6.725380897521973
Epoch 290, val loss: 0.7333161234855652
Epoch 300, training loss: 1.0578961372375488 = 0.38597172498703003 + 0.1 * 6.719244480133057
Epoch 300, val loss: 0.7124402523040771
Epoch 310, training loss: 1.0178160667419434 = 0.3468494713306427 + 0.1 * 6.709665775299072
Epoch 310, val loss: 0.6951844096183777
Epoch 320, training loss: 0.9832982420921326 = 0.31103962659835815 + 0.1 * 6.722586154937744
Epoch 320, val loss: 0.6815951466560364
Epoch 330, training loss: 0.9491392374038696 = 0.27898916602134705 + 0.1 * 6.701500415802002
Epoch 330, val loss: 0.6715396046638489
Epoch 340, training loss: 0.9204059839248657 = 0.2503424882888794 + 0.1 * 6.700634956359863
Epoch 340, val loss: 0.6645206809043884
Epoch 350, training loss: 0.8926043510437012 = 0.22490784525871277 + 0.1 * 6.676965236663818
Epoch 350, val loss: 0.6602371335029602
Epoch 360, training loss: 0.8699477910995483 = 0.20231765508651733 + 0.1 * 6.676301002502441
Epoch 360, val loss: 0.6581435799598694
Epoch 370, training loss: 0.8491569757461548 = 0.18232963979244232 + 0.1 * 6.668272972106934
Epoch 370, val loss: 0.657930314540863
Epoch 380, training loss: 0.8311307430267334 = 0.16464760899543762 + 0.1 * 6.664830684661865
Epoch 380, val loss: 0.6593592762947083
Epoch 390, training loss: 0.814147412776947 = 0.1490708738565445 + 0.1 * 6.650765419006348
Epoch 390, val loss: 0.6620802283287048
Epoch 400, training loss: 0.8003352284431458 = 0.13532300293445587 + 0.1 * 6.650122165679932
Epoch 400, val loss: 0.6658629775047302
Epoch 410, training loss: 0.7873092889785767 = 0.12315231561660767 + 0.1 * 6.6415696144104
Epoch 410, val loss: 0.6704236268997192
Epoch 420, training loss: 0.775950014591217 = 0.1123652458190918 + 0.1 * 6.635847568511963
Epoch 420, val loss: 0.6755865216255188
Epoch 430, training loss: 0.7664857506752014 = 0.10276472568511963 + 0.1 * 6.637210369110107
Epoch 430, val loss: 0.6812249422073364
Epoch 440, training loss: 0.7582805752754211 = 0.09419501572847366 + 0.1 * 6.64085578918457
Epoch 440, val loss: 0.687225878238678
Epoch 450, training loss: 0.7486670613288879 = 0.08654425293207169 + 0.1 * 6.621227741241455
Epoch 450, val loss: 0.6934689283370972
Epoch 460, training loss: 0.7421127557754517 = 0.07967923581600189 + 0.1 * 6.624334812164307
Epoch 460, val loss: 0.6998591423034668
Epoch 470, training loss: 0.7344998121261597 = 0.07350608706474304 + 0.1 * 6.60993766784668
Epoch 470, val loss: 0.7063933610916138
Epoch 480, training loss: 0.7326358556747437 = 0.06792619079351425 + 0.1 * 6.647096633911133
Epoch 480, val loss: 0.7130122184753418
Epoch 490, training loss: 0.7226234078407288 = 0.06291534006595612 + 0.1 * 6.597080707550049
Epoch 490, val loss: 0.7196301817893982
Epoch 500, training loss: 0.7177101373672485 = 0.058382630348205566 + 0.1 * 6.59327507019043
Epoch 500, val loss: 0.7262560725212097
Epoch 510, training loss: 0.7150565981864929 = 0.054265573620796204 + 0.1 * 6.60791015625
Epoch 510, val loss: 0.7328877449035645
Epoch 520, training loss: 0.709406852722168 = 0.050529371947050095 + 0.1 * 6.588774681091309
Epoch 520, val loss: 0.7395173907279968
Epoch 530, training loss: 0.7059404253959656 = 0.047122981399297714 + 0.1 * 6.588174819946289
Epoch 530, val loss: 0.7461279630661011
Epoch 540, training loss: 0.7045656442642212 = 0.04401630535721779 + 0.1 * 6.605493068695068
Epoch 540, val loss: 0.7527117729187012
Epoch 550, training loss: 0.6986757516860962 = 0.04119422286748886 + 0.1 * 6.574815273284912
Epoch 550, val loss: 0.7591648101806641
Epoch 560, training loss: 0.6962016820907593 = 0.03862011060118675 + 0.1 * 6.575815677642822
Epoch 560, val loss: 0.7655612826347351
Epoch 570, training loss: 0.6935907602310181 = 0.0362611822783947 + 0.1 * 6.573295593261719
Epoch 570, val loss: 0.7718710899353027
Epoch 580, training loss: 0.6904236674308777 = 0.034103188663721085 + 0.1 * 6.563204288482666
Epoch 580, val loss: 0.7781347632408142
Epoch 590, training loss: 0.6892882585525513 = 0.03211351856589317 + 0.1 * 6.571747779846191
Epoch 590, val loss: 0.7843650579452515
Epoch 600, training loss: 0.6865124702453613 = 0.030290452763438225 + 0.1 * 6.562220096588135
Epoch 600, val loss: 0.7904451489448547
Epoch 610, training loss: 0.6850973963737488 = 0.028616003692150116 + 0.1 * 6.564813613891602
Epoch 610, val loss: 0.7964636087417603
Epoch 620, training loss: 0.6824855804443359 = 0.027072438970208168 + 0.1 * 6.554131507873535
Epoch 620, val loss: 0.8023606538772583
Epoch 630, training loss: 0.6806455254554749 = 0.025648290291428566 + 0.1 * 6.549972057342529
Epoch 630, val loss: 0.8081925511360168
Epoch 640, training loss: 0.6793553233146667 = 0.02432870864868164 + 0.1 * 6.550266265869141
Epoch 640, val loss: 0.8139398694038391
Epoch 650, training loss: 0.6783223152160645 = 0.02310659922659397 + 0.1 * 6.552157402038574
Epoch 650, val loss: 0.8196050524711609
Epoch 660, training loss: 0.6759946942329407 = 0.021972496062517166 + 0.1 * 6.54022216796875
Epoch 660, val loss: 0.8251816630363464
Epoch 670, training loss: 0.6745386123657227 = 0.0209216196089983 + 0.1 * 6.536169528961182
Epoch 670, val loss: 0.8306732773780823
Epoch 680, training loss: 0.6737149953842163 = 0.019943872466683388 + 0.1 * 6.537710666656494
Epoch 680, val loss: 0.8360384702682495
Epoch 690, training loss: 0.6720257997512817 = 0.019035523757338524 + 0.1 * 6.529902458190918
Epoch 690, val loss: 0.8413317203521729
Epoch 700, training loss: 0.6732543706893921 = 0.018186191096901894 + 0.1 * 6.5506815910339355
Epoch 700, val loss: 0.8465442061424255
Epoch 710, training loss: 0.6699345707893372 = 0.017394913360476494 + 0.1 * 6.525396347045898
Epoch 710, val loss: 0.85164874792099
Epoch 720, training loss: 0.6690235137939453 = 0.016656838357448578 + 0.1 * 6.523666858673096
Epoch 720, val loss: 0.8566588759422302
Epoch 730, training loss: 0.6690354943275452 = 0.01596364565193653 + 0.1 * 6.5307183265686035
Epoch 730, val loss: 0.8615860939025879
Epoch 740, training loss: 0.6684155464172363 = 0.015315826050937176 + 0.1 * 6.530996799468994
Epoch 740, val loss: 0.8664254546165466
Epoch 750, training loss: 0.6664268970489502 = 0.014708026312291622 + 0.1 * 6.517188549041748
Epoch 750, val loss: 0.8712071776390076
Epoch 760, training loss: 0.6655992865562439 = 0.014137718826532364 + 0.1 * 6.514615058898926
Epoch 760, val loss: 0.8758746981620789
Epoch 770, training loss: 0.6650627851486206 = 0.013599409721791744 + 0.1 * 6.514633655548096
Epoch 770, val loss: 0.8804832100868225
Epoch 780, training loss: 0.6636158227920532 = 0.013094119727611542 + 0.1 * 6.505216598510742
Epoch 780, val loss: 0.8850134611129761
Epoch 790, training loss: 0.6631131768226624 = 0.01261722669005394 + 0.1 * 6.5049591064453125
Epoch 790, val loss: 0.889471709728241
Epoch 800, training loss: 0.6640033721923828 = 0.012166681699454784 + 0.1 * 6.518366813659668
Epoch 800, val loss: 0.8938911557197571
Epoch 810, training loss: 0.6614653468132019 = 0.011743578128516674 + 0.1 * 6.497217178344727
Epoch 810, val loss: 0.8981733918190002
Epoch 820, training loss: 0.6615155339241028 = 0.011342654936015606 + 0.1 * 6.501728534698486
Epoch 820, val loss: 0.9024631977081299
Epoch 830, training loss: 0.6610121726989746 = 0.010963018983602524 + 0.1 * 6.500491619110107
Epoch 830, val loss: 0.9066091179847717
Epoch 840, training loss: 0.660692572593689 = 0.010604334995150566 + 0.1 * 6.500882625579834
Epoch 840, val loss: 0.9107310771942139
Epoch 850, training loss: 0.6603759527206421 = 0.010263152420520782 + 0.1 * 6.50112771987915
Epoch 850, val loss: 0.9147889018058777
Epoch 860, training loss: 0.6589762568473816 = 0.009939411655068398 + 0.1 * 6.490368366241455
Epoch 860, val loss: 0.9187226295471191
Epoch 870, training loss: 0.6587302088737488 = 0.009631677530705929 + 0.1 * 6.490984916687012
Epoch 870, val loss: 0.9226344227790833
Epoch 880, training loss: 0.6584423184394836 = 0.00933984573930502 + 0.1 * 6.491024494171143
Epoch 880, val loss: 0.926437258720398
Epoch 890, training loss: 0.6583396792411804 = 0.009061690419912338 + 0.1 * 6.492779731750488
Epoch 890, val loss: 0.9302264451980591
Epoch 900, training loss: 0.657566249370575 = 0.008796615526080132 + 0.1 * 6.487696170806885
Epoch 900, val loss: 0.9339461922645569
Epoch 910, training loss: 0.6574826240539551 = 0.008544470183551311 + 0.1 * 6.489381790161133
Epoch 910, val loss: 0.9375847578048706
Epoch 920, training loss: 0.6568852066993713 = 0.008304470218718052 + 0.1 * 6.485807418823242
Epoch 920, val loss: 0.9411690831184387
Epoch 930, training loss: 0.6555507183074951 = 0.008075068704783916 + 0.1 * 6.474756240844727
Epoch 930, val loss: 0.9446729421615601
Epoch 940, training loss: 0.6573424339294434 = 0.007855554111301899 + 0.1 * 6.494868755340576
Epoch 940, val loss: 0.9481893181800842
Epoch 950, training loss: 0.6561287045478821 = 0.00764570152387023 + 0.1 * 6.484829425811768
Epoch 950, val loss: 0.9515830874443054
Epoch 960, training loss: 0.6548227071762085 = 0.0074455467984080315 + 0.1 * 6.473771095275879
Epoch 960, val loss: 0.9549227952957153
Epoch 970, training loss: 0.6549023985862732 = 0.007253589574247599 + 0.1 * 6.47648811340332
Epoch 970, val loss: 0.9582660794258118
Epoch 980, training loss: 0.6550350189208984 = 0.007069920655339956 + 0.1 * 6.479650497436523
Epoch 980, val loss: 0.9615057110786438
Epoch 990, training loss: 0.6548619866371155 = 0.006894017104059458 + 0.1 * 6.479679584503174
Epoch 990, val loss: 0.9647640585899353
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8186610437532947
The final CL Acc:0.78025, 0.00462, The final GNN Acc:0.82059, 0.00351
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13160])
remove edge: torch.Size([2, 7808])
updated graph: torch.Size([2, 10412])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.811229705810547 = 1.9515461921691895 + 0.1 * 8.59683609008789
Epoch 0, val loss: 1.9468908309936523
Epoch 10, training loss: 2.8004674911499023 = 1.9407925605773926 + 0.1 * 8.596750259399414
Epoch 10, val loss: 1.9364235401153564
Epoch 20, training loss: 2.7870285511016846 = 1.9274004697799683 + 0.1 * 8.596280097961426
Epoch 20, val loss: 1.9229923486709595
Epoch 30, training loss: 2.767925977706909 = 1.908664584159851 + 0.1 * 8.592613220214844
Epoch 30, val loss: 1.9037925004959106
Epoch 40, training loss: 2.7383110523223877 = 1.8814620971679688 + 0.1 * 8.568490028381348
Epoch 40, val loss: 1.8760473728179932
Epoch 50, training loss: 2.6883723735809326 = 1.844277024269104 + 0.1 * 8.440953254699707
Epoch 50, val loss: 1.839751124382019
Epoch 60, training loss: 2.6234960556030273 = 1.8020563125610352 + 0.1 * 8.214398384094238
Epoch 60, val loss: 1.8018033504486084
Epoch 70, training loss: 2.555453300476074 = 1.7611347436904907 + 0.1 * 7.9431867599487305
Epoch 70, val loss: 1.7662019729614258
Epoch 80, training loss: 2.479102611541748 = 1.718287706375122 + 0.1 * 7.608148574829102
Epoch 80, val loss: 1.7282896041870117
Epoch 90, training loss: 2.3996312618255615 = 1.6677064895629883 + 0.1 * 7.319247722625732
Epoch 90, val loss: 1.6834325790405273
Epoch 100, training loss: 2.322207450866699 = 1.602144479751587 + 0.1 * 7.200630187988281
Epoch 100, val loss: 1.6233371496200562
Epoch 110, training loss: 2.234355926513672 = 1.520557165145874 + 0.1 * 7.137986660003662
Epoch 110, val loss: 1.5494422912597656
Epoch 120, training loss: 2.1383421421051025 = 1.429275631904602 + 0.1 * 7.090664386749268
Epoch 120, val loss: 1.4711103439331055
Epoch 130, training loss: 2.0394837856292725 = 1.333858609199524 + 0.1 * 7.056252479553223
Epoch 130, val loss: 1.391250491142273
Epoch 140, training loss: 1.9397454261779785 = 1.2362382411956787 + 0.1 * 7.035070896148682
Epoch 140, val loss: 1.3112202882766724
Epoch 150, training loss: 1.8403043746948242 = 1.1383787393569946 + 0.1 * 7.019257068634033
Epoch 150, val loss: 1.2322109937667847
Epoch 160, training loss: 1.744088888168335 = 1.0433051586151123 + 0.1 * 7.00783634185791
Epoch 160, val loss: 1.1557003259658813
Epoch 170, training loss: 1.6539227962493896 = 0.9543883204460144 + 0.1 * 6.995345115661621
Epoch 170, val loss: 1.0851746797561646
Epoch 180, training loss: 1.5708576440811157 = 0.872893750667572 + 0.1 * 6.979639053344727
Epoch 180, val loss: 1.0208830833435059
Epoch 190, training loss: 1.4962670803070068 = 0.7994058132171631 + 0.1 * 6.9686126708984375
Epoch 190, val loss: 0.9634498953819275
Epoch 200, training loss: 1.4293174743652344 = 0.7344517111778259 + 0.1 * 6.948657989501953
Epoch 200, val loss: 0.9136530160903931
Epoch 210, training loss: 1.370544195175171 = 0.6771095395088196 + 0.1 * 6.934345722198486
Epoch 210, val loss: 0.8710696697235107
Epoch 220, training loss: 1.3185529708862305 = 0.6262663006782532 + 0.1 * 6.922867298126221
Epoch 220, val loss: 0.835273265838623
Epoch 230, training loss: 1.2705832719802856 = 0.5794726014137268 + 0.1 * 6.911106586456299
Epoch 230, val loss: 0.8040041923522949
Epoch 240, training loss: 1.2267792224884033 = 0.5350785851478577 + 0.1 * 6.917006492614746
Epoch 240, val loss: 0.7761613726615906
Epoch 250, training loss: 1.1820071935653687 = 0.492577463388443 + 0.1 * 6.8942975997924805
Epoch 250, val loss: 0.7514512538909912
Epoch 260, training loss: 1.1389167308807373 = 0.4509681165218353 + 0.1 * 6.879486083984375
Epoch 260, val loss: 0.7292121052742004
Epoch 270, training loss: 1.096781611442566 = 0.40988945960998535 + 0.1 * 6.868921279907227
Epoch 270, val loss: 0.7093353867530823
Epoch 280, training loss: 1.0562330484390259 = 0.36958202719688416 + 0.1 * 6.866509914398193
Epoch 280, val loss: 0.6916375160217285
Epoch 290, training loss: 1.017074704170227 = 0.3312663733959198 + 0.1 * 6.858083724975586
Epoch 290, val loss: 0.6763693690299988
Epoch 300, training loss: 0.9795405864715576 = 0.2953505218029022 + 0.1 * 6.841900825500488
Epoch 300, val loss: 0.6635006666183472
Epoch 310, training loss: 0.9452283382415771 = 0.26227322220802307 + 0.1 * 6.829550743103027
Epoch 310, val loss: 0.6533268094062805
Epoch 320, training loss: 0.9161351919174194 = 0.23244625329971313 + 0.1 * 6.836889266967773
Epoch 320, val loss: 0.6461241841316223
Epoch 330, training loss: 0.88719642162323 = 0.2061859369277954 + 0.1 * 6.810104846954346
Epoch 330, val loss: 0.6414865255355835
Epoch 340, training loss: 0.8631340861320496 = 0.18313880264759064 + 0.1 * 6.799952983856201
Epoch 340, val loss: 0.6392601132392883
Epoch 350, training loss: 0.8420032262802124 = 0.16301321983337402 + 0.1 * 6.789899826049805
Epoch 350, val loss: 0.6391754746437073
Epoch 360, training loss: 0.825181245803833 = 0.1455095112323761 + 0.1 * 6.796717643737793
Epoch 360, val loss: 0.6408066153526306
Epoch 370, training loss: 0.8075131177902222 = 0.13028213381767273 + 0.1 * 6.772309303283691
Epoch 370, val loss: 0.6439415216445923
Epoch 380, training loss: 0.7931942939758301 = 0.11694524437189102 + 0.1 * 6.762490272521973
Epoch 380, val loss: 0.6482165455818176
Epoch 390, training loss: 0.7822166085243225 = 0.10523597151041031 + 0.1 * 6.769805908203125
Epoch 390, val loss: 0.6535003781318665
Epoch 400, training loss: 0.7700523734092712 = 0.09499119222164154 + 0.1 * 6.750611305236816
Epoch 400, val loss: 0.6594525575637817
Epoch 410, training loss: 0.7595993280410767 = 0.08596885949373245 + 0.1 * 6.736304759979248
Epoch 410, val loss: 0.6660546660423279
Epoch 420, training loss: 0.7529382109642029 = 0.07802838087081909 + 0.1 * 6.749098300933838
Epoch 420, val loss: 0.6732326745986938
Epoch 430, training loss: 0.744190514087677 = 0.07106746733188629 + 0.1 * 6.73123025894165
Epoch 430, val loss: 0.680521547794342
Epoch 440, training loss: 0.7361864447593689 = 0.06490516662597656 + 0.1 * 6.712812423706055
Epoch 440, val loss: 0.6880989074707031
Epoch 450, training loss: 0.7301288843154907 = 0.05942150950431824 + 0.1 * 6.70707368850708
Epoch 450, val loss: 0.6959545016288757
Epoch 460, training loss: 0.726719856262207 = 0.054553236812353134 + 0.1 * 6.72166633605957
Epoch 460, val loss: 0.7039321064949036
Epoch 470, training loss: 0.719691276550293 = 0.0502321757376194 + 0.1 * 6.694591045379639
Epoch 470, val loss: 0.7117921113967896
Epoch 480, training loss: 0.7154878377914429 = 0.04636630415916443 + 0.1 * 6.691215515136719
Epoch 480, val loss: 0.7197296023368835
Epoch 490, training loss: 0.7109564542770386 = 0.042900655418634415 + 0.1 * 6.680557727813721
Epoch 490, val loss: 0.7277313470840454
Epoch 500, training loss: 0.7077481150627136 = 0.03978513926267624 + 0.1 * 6.679629802703857
Epoch 500, val loss: 0.7356334924697876
Epoch 510, training loss: 0.7066006660461426 = 0.0369747094810009 + 0.1 * 6.696259498596191
Epoch 510, val loss: 0.7434437870979309
Epoch 520, training loss: 0.7017381191253662 = 0.03444531932473183 + 0.1 * 6.6729278564453125
Epoch 520, val loss: 0.7511262893676758
Epoch 530, training loss: 0.6985931992530823 = 0.03215895593166351 + 0.1 * 6.664342403411865
Epoch 530, val loss: 0.7587039470672607
Epoch 540, training loss: 0.6966121196746826 = 0.03008478321135044 + 0.1 * 6.665273189544678
Epoch 540, val loss: 0.766129195690155
Epoch 550, training loss: 0.6938278675079346 = 0.02820180356502533 + 0.1 * 6.6562604904174805
Epoch 550, val loss: 0.7733824253082275
Epoch 560, training loss: 0.6925532817840576 = 0.026486393064260483 + 0.1 * 6.660668849945068
Epoch 560, val loss: 0.780508816242218
Epoch 570, training loss: 0.690139889717102 = 0.02492155134677887 + 0.1 * 6.652183532714844
Epoch 570, val loss: 0.7874956130981445
Epoch 580, training loss: 0.6899463534355164 = 0.02348960004746914 + 0.1 * 6.664567470550537
Epoch 580, val loss: 0.7942663431167603
Epoch 590, training loss: 0.6865257620811462 = 0.022183774039149284 + 0.1 * 6.6434197425842285
Epoch 590, val loss: 0.800869345664978
Epoch 600, training loss: 0.6842406988143921 = 0.02098342962563038 + 0.1 * 6.632572174072266
Epoch 600, val loss: 0.8072792291641235
Epoch 610, training loss: 0.6834659576416016 = 0.019876770675182343 + 0.1 * 6.635891914367676
Epoch 610, val loss: 0.8135684728622437
Epoch 620, training loss: 0.6818070411682129 = 0.018857523798942566 + 0.1 * 6.629494667053223
Epoch 620, val loss: 0.8197385668754578
Epoch 630, training loss: 0.6805596947669983 = 0.01791616901755333 + 0.1 * 6.626435279846191
Epoch 630, val loss: 0.8257452249526978
Epoch 640, training loss: 0.679192066192627 = 0.017049424350261688 + 0.1 * 6.621426105499268
Epoch 640, val loss: 0.8315092325210571
Epoch 650, training loss: 0.6776490211486816 = 0.016244620084762573 + 0.1 * 6.614044189453125
Epoch 650, val loss: 0.8371433019638062
Epoch 660, training loss: 0.6783687472343445 = 0.015496033243834972 + 0.1 * 6.628727436065674
Epoch 660, val loss: 0.8427330255508423
Epoch 670, training loss: 0.6755426526069641 = 0.014801288023591042 + 0.1 * 6.6074137687683105
Epoch 670, val loss: 0.8481210470199585
Epoch 680, training loss: 0.6754062175750732 = 0.014153946191072464 + 0.1 * 6.612522602081299
Epoch 680, val loss: 0.8533602952957153
Epoch 690, training loss: 0.6735799312591553 = 0.01355073880404234 + 0.1 * 6.600291728973389
Epoch 690, val loss: 0.8585016131401062
Epoch 700, training loss: 0.6722569465637207 = 0.012987649999558926 + 0.1 * 6.5926923751831055
Epoch 700, val loss: 0.8634459376335144
Epoch 710, training loss: 0.6722068190574646 = 0.012459814548492432 + 0.1 * 6.597469806671143
Epoch 710, val loss: 0.8683639764785767
Epoch 720, training loss: 0.6712429523468018 = 0.01196576189249754 + 0.1 * 6.592772006988525
Epoch 720, val loss: 0.8732059001922607
Epoch 730, training loss: 0.6706666946411133 = 0.011501784436404705 + 0.1 * 6.591649055480957
Epoch 730, val loss: 0.8778635263442993
Epoch 740, training loss: 0.6692337393760681 = 0.011066981591284275 + 0.1 * 6.581667423248291
Epoch 740, val loss: 0.8823927640914917
Epoch 750, training loss: 0.6708453893661499 = 0.010657178238034248 + 0.1 * 6.601881980895996
Epoch 750, val loss: 0.8868771195411682
Epoch 760, training loss: 0.6678266525268555 = 0.010272915475070477 + 0.1 * 6.575537204742432
Epoch 760, val loss: 0.8912746906280518
Epoch 770, training loss: 0.6690956354141235 = 0.009909894317388535 + 0.1 * 6.591857433319092
Epoch 770, val loss: 0.8954787850379944
Epoch 780, training loss: 0.6665960550308228 = 0.009567667730152607 + 0.1 * 6.57028341293335
Epoch 780, val loss: 0.8996127247810364
Epoch 790, training loss: 0.6656767129898071 = 0.009244866669178009 + 0.1 * 6.564318656921387
Epoch 790, val loss: 0.9037320017814636
Epoch 800, training loss: 0.665573000907898 = 0.008938044309616089 + 0.1 * 6.566349983215332
Epoch 800, val loss: 0.9077329039573669
Epoch 810, training loss: 0.6643742322921753 = 0.008648339658975601 + 0.1 * 6.5572590827941895
Epoch 810, val loss: 0.9116384983062744
Epoch 820, training loss: 0.6662976741790771 = 0.00837355013936758 + 0.1 * 6.5792412757873535
Epoch 820, val loss: 0.9154902100563049
Epoch 830, training loss: 0.6638983488082886 = 0.008113667368888855 + 0.1 * 6.557846546173096
Epoch 830, val loss: 0.9192777872085571
Epoch 840, training loss: 0.6635235548019409 = 0.007866096682846546 + 0.1 * 6.55657434463501
Epoch 840, val loss: 0.9229146838188171
Epoch 850, training loss: 0.663524329662323 = 0.0076311733573675156 + 0.1 * 6.558931350708008
Epoch 850, val loss: 0.9266226887702942
Epoch 860, training loss: 0.6620394587516785 = 0.007408089004456997 + 0.1 * 6.546313762664795
Epoch 860, val loss: 0.9301807880401611
Epoch 870, training loss: 0.6619459986686707 = 0.007195174694061279 + 0.1 * 6.547508239746094
Epoch 870, val loss: 0.9336861968040466
Epoch 880, training loss: 0.662630021572113 = 0.006992301903665066 + 0.1 * 6.556377410888672
Epoch 880, val loss: 0.9371128678321838
Epoch 890, training loss: 0.6605624556541443 = 0.006799574010074139 + 0.1 * 6.537628650665283
Epoch 890, val loss: 0.940529465675354
Epoch 900, training loss: 0.6607231497764587 = 0.006615099031478167 + 0.1 * 6.541080474853516
Epoch 900, val loss: 0.9438133239746094
Epoch 910, training loss: 0.6600562334060669 = 0.006438911892473698 + 0.1 * 6.536172866821289
Epoch 910, val loss: 0.9470825791358948
Epoch 920, training loss: 0.6593842506408691 = 0.006270642392337322 + 0.1 * 6.5311360359191895
Epoch 920, val loss: 0.9503307342529297
Epoch 930, training loss: 0.6586523056030273 = 0.006109954789280891 + 0.1 * 6.525423526763916
Epoch 930, val loss: 0.9534884691238403
Epoch 940, training loss: 0.6586463451385498 = 0.005955546163022518 + 0.1 * 6.526907920837402
Epoch 940, val loss: 0.9566041231155396
Epoch 950, training loss: 0.6594388484954834 = 0.0058085015043616295 + 0.1 * 6.536303520202637
Epoch 950, val loss: 0.9596349596977234
Epoch 960, training loss: 0.6577622890472412 = 0.005667476914823055 + 0.1 * 6.52094841003418
Epoch 960, val loss: 0.9626809358596802
Epoch 970, training loss: 0.6577019691467285 = 0.005532328505069017 + 0.1 * 6.521696090698242
Epoch 970, val loss: 0.9655812382698059
Epoch 980, training loss: 0.6580051183700562 = 0.005402558483183384 + 0.1 * 6.526025295257568
Epoch 980, val loss: 0.9685096740722656
Epoch 990, training loss: 0.6584756970405579 = 0.0052780695259571075 + 0.1 * 6.531976222991943
Epoch 990, val loss: 0.9713810682296753
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.802260398864746 = 1.942579984664917 + 0.1 * 8.596802711486816
Epoch 0, val loss: 1.93368661403656
Epoch 10, training loss: 2.7919459342956543 = 1.9322746992111206 + 0.1 * 8.596713066101074
Epoch 10, val loss: 1.9236063957214355
Epoch 20, training loss: 2.7794809341430664 = 1.919875144958496 + 0.1 * 8.59605884552002
Epoch 20, val loss: 1.9115134477615356
Epoch 30, training loss: 2.761798858642578 = 1.9027683734893799 + 0.1 * 8.590303421020508
Epoch 30, val loss: 1.8951466083526611
Epoch 40, training loss: 2.7325286865234375 = 1.8778280019760132 + 0.1 * 8.547005653381348
Epoch 40, val loss: 1.8720035552978516
Epoch 50, training loss: 2.6729133129119873 = 1.8441839218139648 + 0.1 * 8.287293434143066
Epoch 50, val loss: 1.8423913717269897
Epoch 60, training loss: 2.609602928161621 = 1.8069396018981934 + 0.1 * 8.026633262634277
Epoch 60, val loss: 1.8109368085861206
Epoch 70, training loss: 2.533438205718994 = 1.7710001468658447 + 0.1 * 7.624379634857178
Epoch 70, val loss: 1.7804642915725708
Epoch 80, training loss: 2.465668201446533 = 1.7349094152450562 + 0.1 * 7.307586669921875
Epoch 80, val loss: 1.7497998476028442
Epoch 90, training loss: 2.404146194458008 = 1.6890195608139038 + 0.1 * 7.1512651443481445
Epoch 90, val loss: 1.7081903219223022
Epoch 100, training loss: 2.3318347930908203 = 1.626768708229065 + 0.1 * 7.050660133361816
Epoch 100, val loss: 1.6510952711105347
Epoch 110, training loss: 2.2490525245666504 = 1.550403356552124 + 0.1 * 6.9864912033081055
Epoch 110, val loss: 1.5849004983901978
Epoch 120, training loss: 2.1620867252349854 = 1.467133641242981 + 0.1 * 6.949531555175781
Epoch 120, val loss: 1.513892650604248
Epoch 130, training loss: 2.077615737915039 = 1.3855772018432617 + 0.1 * 6.920384883880615
Epoch 130, val loss: 1.445663571357727
Epoch 140, training loss: 1.995715856552124 = 1.3059377670288086 + 0.1 * 6.897780418395996
Epoch 140, val loss: 1.3779292106628418
Epoch 150, training loss: 1.9152517318725586 = 1.226758360862732 + 0.1 * 6.884932994842529
Epoch 150, val loss: 1.3116328716278076
Epoch 160, training loss: 1.8351936340332031 = 1.1490626335144043 + 0.1 * 6.861310005187988
Epoch 160, val loss: 1.2487431764602661
Epoch 170, training loss: 1.756068229675293 = 1.071333408355713 + 0.1 * 6.847348690032959
Epoch 170, val loss: 1.187209129333496
Epoch 180, training loss: 1.6780141592025757 = 0.9940279126167297 + 0.1 * 6.83986234664917
Epoch 180, val loss: 1.12716543674469
Epoch 190, training loss: 1.5998919010162354 = 0.9184874892234802 + 0.1 * 6.8140435218811035
Epoch 190, val loss: 1.0687153339385986
Epoch 200, training loss: 1.5240815877914429 = 0.8441682457923889 + 0.1 * 6.79913330078125
Epoch 200, val loss: 1.0106178522109985
Epoch 210, training loss: 1.452239990234375 = 0.7727733850479126 + 0.1 * 6.794666290283203
Epoch 210, val loss: 0.9553691744804382
Epoch 220, training loss: 1.3862831592559814 = 0.707754373550415 + 0.1 * 6.785288333892822
Epoch 220, val loss: 0.9067689180374146
Epoch 230, training loss: 1.3255772590637207 = 0.6490927338600159 + 0.1 * 6.764845371246338
Epoch 230, val loss: 0.8651019334793091
Epoch 240, training loss: 1.27532160282135 = 0.5969701409339905 + 0.1 * 6.783514499664307
Epoch 240, val loss: 0.8317320942878723
Epoch 250, training loss: 1.2272427082061768 = 0.5518226623535156 + 0.1 * 6.754200458526611
Epoch 250, val loss: 0.806756317615509
Epoch 260, training loss: 1.1859910488128662 = 0.5117573738098145 + 0.1 * 6.742335796356201
Epoch 260, val loss: 0.7881907820701599
Epoch 270, training loss: 1.14816153049469 = 0.47520753741264343 + 0.1 * 6.72953987121582
Epoch 270, val loss: 0.7744654417037964
Epoch 280, training loss: 1.114880919456482 = 0.4413103759288788 + 0.1 * 6.735705375671387
Epoch 280, val loss: 0.7639244198799133
Epoch 290, training loss: 1.0814094543457031 = 0.40947964787483215 + 0.1 * 6.719297409057617
Epoch 290, val loss: 0.7557918429374695
Epoch 300, training loss: 1.0507886409759521 = 0.37905699014663696 + 0.1 * 6.717316150665283
Epoch 300, val loss: 0.7495580315589905
Epoch 310, training loss: 1.0212407112121582 = 0.35019147396087646 + 0.1 * 6.71049165725708
Epoch 310, val loss: 0.7452484369277954
Epoch 320, training loss: 0.9921057224273682 = 0.3227274417877197 + 0.1 * 6.693782806396484
Epoch 320, val loss: 0.7428011298179626
Epoch 330, training loss: 0.9652225971221924 = 0.29650309681892395 + 0.1 * 6.687195301055908
Epoch 330, val loss: 0.7422016263008118
Epoch 340, training loss: 0.9397383332252502 = 0.2715991139411926 + 0.1 * 6.681392192840576
Epoch 340, val loss: 0.7434384822845459
Epoch 350, training loss: 0.9181368947029114 = 0.2483530044555664 + 0.1 * 6.69783878326416
Epoch 350, val loss: 0.746489942073822
Epoch 360, training loss: 0.894375205039978 = 0.22701194882392883 + 0.1 * 6.673632621765137
Epoch 360, val loss: 0.7512505650520325
Epoch 370, training loss: 0.8740077018737793 = 0.20748572051525116 + 0.1 * 6.665219306945801
Epoch 370, val loss: 0.7575686573982239
Epoch 380, training loss: 0.8573824167251587 = 0.18969199061393738 + 0.1 * 6.676904678344727
Epoch 380, val loss: 0.7652401328086853
Epoch 390, training loss: 0.8390446305274963 = 0.17356961965560913 + 0.1 * 6.654749870300293
Epoch 390, val loss: 0.7739765644073486
Epoch 400, training loss: 0.8261233568191528 = 0.15897323191165924 + 0.1 * 6.671501159667969
Epoch 400, val loss: 0.7836359143257141
Epoch 410, training loss: 0.8104076385498047 = 0.14580073952674866 + 0.1 * 6.646068572998047
Epoch 410, val loss: 0.793919563293457
Epoch 420, training loss: 0.797577977180481 = 0.13385118544101715 + 0.1 * 6.63726806640625
Epoch 420, val loss: 0.8048322200775146
Epoch 430, training loss: 0.7887773513793945 = 0.12299180775880814 + 0.1 * 6.65785551071167
Epoch 430, val loss: 0.816220760345459
Epoch 440, training loss: 0.7761770486831665 = 0.11316321790218353 + 0.1 * 6.630137920379639
Epoch 440, val loss: 0.8278024792671204
Epoch 450, training loss: 0.7661469578742981 = 0.10424704849720001 + 0.1 * 6.618999004364014
Epoch 450, val loss: 0.8396795988082886
Epoch 460, training loss: 0.7599661350250244 = 0.09615521878004074 + 0.1 * 6.63810920715332
Epoch 460, val loss: 0.8516857028007507
Epoch 470, training loss: 0.7500130534172058 = 0.08883654326200485 + 0.1 * 6.611764907836914
Epoch 470, val loss: 0.8636301755905151
Epoch 480, training loss: 0.7460824251174927 = 0.08219343423843384 + 0.1 * 6.638889789581299
Epoch 480, val loss: 0.8755912184715271
Epoch 490, training loss: 0.7368337512016296 = 0.07617691904306412 + 0.1 * 6.606567859649658
Epoch 490, val loss: 0.8874185085296631
Epoch 500, training loss: 0.731522262096405 = 0.07070557773113251 + 0.1 * 6.608166694641113
Epoch 500, val loss: 0.8992137908935547
Epoch 510, training loss: 0.7257852554321289 = 0.06572594493627548 + 0.1 * 6.600593090057373
Epoch 510, val loss: 0.9108225107192993
Epoch 520, training loss: 0.7205989360809326 = 0.061191920191049576 + 0.1 * 6.594069957733154
Epoch 520, val loss: 0.9222970604896545
Epoch 530, training loss: 0.7181369066238403 = 0.05704990401864052 + 0.1 * 6.610869407653809
Epoch 530, val loss: 0.933609127998352
Epoch 540, training loss: 0.7118738889694214 = 0.053276222199201584 + 0.1 * 6.5859761238098145
Epoch 540, val loss: 0.944658100605011
Epoch 550, training loss: 0.709347665309906 = 0.04982399940490723 + 0.1 * 6.595236301422119
Epoch 550, val loss: 0.9555230140686035
Epoch 560, training loss: 0.7051133513450623 = 0.046669330447912216 + 0.1 * 6.584440231323242
Epoch 560, val loss: 0.9661329388618469
Epoch 570, training loss: 0.7009115815162659 = 0.04378122091293335 + 0.1 * 6.571303367614746
Epoch 570, val loss: 0.9765139818191528
Epoch 580, training loss: 0.698891282081604 = 0.041129425168037415 + 0.1 * 6.577618598937988
Epoch 580, val loss: 0.9867146611213684
Epoch 590, training loss: 0.696157693862915 = 0.03869239613413811 + 0.1 * 6.574652671813965
Epoch 590, val loss: 0.9966902136802673
Epoch 600, training loss: 0.6923556327819824 = 0.03644874691963196 + 0.1 * 6.559068202972412
Epoch 600, val loss: 1.006445288658142
Epoch 610, training loss: 0.6900997161865234 = 0.03438091278076172 + 0.1 * 6.557188034057617
Epoch 610, val loss: 1.0160009860992432
Epoch 620, training loss: 0.688872754573822 = 0.032475795596838 + 0.1 * 6.563969612121582
Epoch 620, val loss: 1.025277853012085
Epoch 630, training loss: 0.6865648031234741 = 0.030716855078935623 + 0.1 * 6.5584797859191895
Epoch 630, val loss: 1.0343469381332397
Epoch 640, training loss: 0.6844788193702698 = 0.02908874675631523 + 0.1 * 6.553900718688965
Epoch 640, val loss: 1.0432329177856445
Epoch 650, training loss: 0.6814131736755371 = 0.027580928057432175 + 0.1 * 6.538322448730469
Epoch 650, val loss: 1.0519006252288818
Epoch 660, training loss: 0.6804832220077515 = 0.026182958856225014 + 0.1 * 6.543002605438232
Epoch 660, val loss: 1.0603585243225098
Epoch 670, training loss: 0.6798998713493347 = 0.024883491918444633 + 0.1 * 6.550163745880127
Epoch 670, val loss: 1.0686566829681396
Epoch 680, training loss: 0.6794742941856384 = 0.02367568016052246 + 0.1 * 6.557985782623291
Epoch 680, val loss: 1.076725721359253
Epoch 690, training loss: 0.6758972406387329 = 0.022553548216819763 + 0.1 * 6.5334367752075195
Epoch 690, val loss: 1.0846086740493774
Epoch 700, training loss: 0.6751675009727478 = 0.0215067770332098 + 0.1 * 6.536607265472412
Epoch 700, val loss: 1.0923285484313965
Epoch 710, training loss: 0.6735430359840393 = 0.020529162138700485 + 0.1 * 6.530138969421387
Epoch 710, val loss: 1.099875807762146
Epoch 720, training loss: 0.671766459941864 = 0.019616713747382164 + 0.1 * 6.5214972496032715
Epoch 720, val loss: 1.1072450876235962
Epoch 730, training loss: 0.6725882291793823 = 0.018761388957500458 + 0.1 * 6.538268089294434
Epoch 730, val loss: 1.1144777536392212
Epoch 740, training loss: 0.6705321073532104 = 0.01796134002506733 + 0.1 * 6.525707721710205
Epoch 740, val loss: 1.1215147972106934
Epoch 750, training loss: 0.671989381313324 = 0.017211737111210823 + 0.1 * 6.547776222229004
Epoch 750, val loss: 1.1283762454986572
Epoch 760, training loss: 0.6688376665115356 = 0.0165083184838295 + 0.1 * 6.5232930183410645
Epoch 760, val loss: 1.1350669860839844
Epoch 770, training loss: 0.6674306988716125 = 0.015849890187382698 + 0.1 * 6.51580810546875
Epoch 770, val loss: 1.1415683031082153
Epoch 780, training loss: 0.6661774516105652 = 0.015229248441755772 + 0.1 * 6.509481906890869
Epoch 780, val loss: 1.1479811668395996
Epoch 790, training loss: 0.6660913825035095 = 0.014645449817180634 + 0.1 * 6.514459133148193
Epoch 790, val loss: 1.1542444229125977
Epoch 800, training loss: 0.6639502644538879 = 0.014095446094870567 + 0.1 * 6.49854850769043
Epoch 800, val loss: 1.1603814363479614
Epoch 810, training loss: 0.6648737192153931 = 0.013576003722846508 + 0.1 * 6.51297664642334
Epoch 810, val loss: 1.1664113998413086
Epoch 820, training loss: 0.6637956500053406 = 0.013084917329251766 + 0.1 * 6.507107734680176
Epoch 820, val loss: 1.1723121404647827
Epoch 830, training loss: 0.6625863909721375 = 0.012621383182704449 + 0.1 * 6.499649524688721
Epoch 830, val loss: 1.178054928779602
Epoch 840, training loss: 0.6611148715019226 = 0.012182475067675114 + 0.1 * 6.48932409286499
Epoch 840, val loss: 1.1836974620819092
Epoch 850, training loss: 0.6615766286849976 = 0.011767708696424961 + 0.1 * 6.49808931350708
Epoch 850, val loss: 1.18922758102417
Epoch 860, training loss: 0.6612806916236877 = 0.011374336667358875 + 0.1 * 6.499063491821289
Epoch 860, val loss: 1.1946654319763184
Epoch 870, training loss: 0.6599559187889099 = 0.011001565493643284 + 0.1 * 6.489543437957764
Epoch 870, val loss: 1.1999807357788086
Epoch 880, training loss: 0.6621300578117371 = 0.010647483170032501 + 0.1 * 6.514825820922852
Epoch 880, val loss: 1.2052159309387207
Epoch 890, training loss: 0.6590479612350464 = 0.010311384685337543 + 0.1 * 6.48736572265625
Epoch 890, val loss: 1.2103108167648315
Epoch 900, training loss: 0.6588658690452576 = 0.009992574341595173 + 0.1 * 6.488732814788818
Epoch 900, val loss: 1.2152929306030273
Epoch 910, training loss: 0.6572327613830566 = 0.009688680060207844 + 0.1 * 6.475440979003906
Epoch 910, val loss: 1.2201974391937256
Epoch 920, training loss: 0.658696174621582 = 0.00939970463514328 + 0.1 * 6.492964744567871
Epoch 920, val loss: 1.2250194549560547
Epoch 930, training loss: 0.6564140319824219 = 0.009123729541897774 + 0.1 * 6.472902774810791
Epoch 930, val loss: 1.2297381162643433
Epoch 940, training loss: 0.6565378904342651 = 0.008861240930855274 + 0.1 * 6.476766586303711
Epoch 940, val loss: 1.2343809604644775
Epoch 950, training loss: 0.6559005379676819 = 0.008609924465417862 + 0.1 * 6.47290563583374
Epoch 950, val loss: 1.2389644384384155
Epoch 960, training loss: 0.6551347374916077 = 0.008370046503841877 + 0.1 * 6.467646598815918
Epoch 960, val loss: 1.2434430122375488
Epoch 970, training loss: 0.655007541179657 = 0.008141042664647102 + 0.1 * 6.468664646148682
Epoch 970, val loss: 1.2478517293930054
Epoch 980, training loss: 0.6558110117912292 = 0.00792323425412178 + 0.1 * 6.478877544403076
Epoch 980, val loss: 1.252143144607544
Epoch 990, training loss: 0.6542117595672607 = 0.00771411182358861 + 0.1 * 6.4649763107299805
Epoch 990, val loss: 1.2563568353652954
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8460727464417502
=== training gcn model ===
Epoch 0, training loss: 2.816249370574951 = 1.956569790840149 + 0.1 * 8.596795082092285
Epoch 0, val loss: 1.9624812602996826
Epoch 10, training loss: 2.8059566020965576 = 1.9462924003601074 + 0.1 * 8.596641540527344
Epoch 10, val loss: 1.9525673389434814
Epoch 20, training loss: 2.7927894592285156 = 1.9332244396209717 + 0.1 * 8.595650672912598
Epoch 20, val loss: 1.939538836479187
Epoch 30, training loss: 2.7731213569641113 = 1.9143855571746826 + 0.1 * 8.587356567382812
Epoch 30, val loss: 1.920441746711731
Epoch 40, training loss: 2.7399773597717285 = 1.8861933946609497 + 0.1 * 8.53783893585205
Epoch 40, val loss: 1.8920754194259644
Epoch 50, training loss: 2.6778502464294434 = 1.8480433225631714 + 0.1 * 8.298069953918457
Epoch 50, val loss: 1.855485439300537
Epoch 60, training loss: 2.6079981327056885 = 1.8070564270019531 + 0.1 * 8.009417533874512
Epoch 60, val loss: 1.8186060190200806
Epoch 70, training loss: 2.5219647884368896 = 1.7706528902053833 + 0.1 * 7.513119220733643
Epoch 70, val loss: 1.7857893705368042
Epoch 80, training loss: 2.454601526260376 = 1.733444333076477 + 0.1 * 7.211572647094727
Epoch 80, val loss: 1.7509515285491943
Epoch 90, training loss: 2.3970847129821777 = 1.686234474182129 + 0.1 * 7.108502388000488
Epoch 90, val loss: 1.7065379619598389
Epoch 100, training loss: 2.328787088394165 = 1.6240413188934326 + 0.1 * 7.047457695007324
Epoch 100, val loss: 1.6495198011398315
Epoch 110, training loss: 2.247278928756714 = 1.5471320152282715 + 0.1 * 7.001469612121582
Epoch 110, val loss: 1.5820525884628296
Epoch 120, training loss: 2.1593222618103027 = 1.462281584739685 + 0.1 * 6.9704060554504395
Epoch 120, val loss: 1.5106475353240967
Epoch 130, training loss: 2.0704312324523926 = 1.3759845495224 + 0.1 * 6.944467067718506
Epoch 130, val loss: 1.4409821033477783
Epoch 140, training loss: 1.9805994033813477 = 1.2890193462371826 + 0.1 * 6.91580057144165
Epoch 140, val loss: 1.3718185424804688
Epoch 150, training loss: 1.891296148300171 = 1.201547622680664 + 0.1 * 6.897485256195068
Epoch 150, val loss: 1.3025437593460083
Epoch 160, training loss: 1.8023059368133545 = 1.1154102087020874 + 0.1 * 6.86895751953125
Epoch 160, val loss: 1.234850287437439
Epoch 170, training loss: 1.715580940246582 = 1.0309971570968628 + 0.1 * 6.8458380699157715
Epoch 170, val loss: 1.1683580875396729
Epoch 180, training loss: 1.6340945959091187 = 0.9511071443557739 + 0.1 * 6.829874515533447
Epoch 180, val loss: 1.1058727502822876
Epoch 190, training loss: 1.5580978393554688 = 0.8766260147094727 + 0.1 * 6.814718246459961
Epoch 190, val loss: 1.0472642183303833
Epoch 200, training loss: 1.4872682094573975 = 0.8069902658462524 + 0.1 * 6.802779674530029
Epoch 200, val loss: 0.9921064376831055
Epoch 210, training loss: 1.4219361543655396 = 0.7429457306861877 + 0.1 * 6.7899041175842285
Epoch 210, val loss: 0.9417136311531067
Epoch 220, training loss: 1.361642599105835 = 0.6839329600334167 + 0.1 * 6.777096271514893
Epoch 220, val loss: 0.8958237171173096
Epoch 230, training loss: 1.306617021560669 = 0.6298348903656006 + 0.1 * 6.767820358276367
Epoch 230, val loss: 0.8556044697761536
Epoch 240, training loss: 1.2552045583724976 = 0.5797142386436462 + 0.1 * 6.7549028396606445
Epoch 240, val loss: 0.8209443688392639
Epoch 250, training loss: 1.2068214416503906 = 0.5320217609405518 + 0.1 * 6.7479963302612305
Epoch 250, val loss: 0.7907111644744873
Epoch 260, training loss: 1.1604702472686768 = 0.48688805103302 + 0.1 * 6.735821723937988
Epoch 260, val loss: 0.7649984359741211
Epoch 270, training loss: 1.1167893409729004 = 0.4441607594490051 + 0.1 * 6.726285934448242
Epoch 270, val loss: 0.7433182597160339
Epoch 280, training loss: 1.0755161046981812 = 0.40396469831466675 + 0.1 * 6.715514183044434
Epoch 280, val loss: 0.7256941199302673
Epoch 290, training loss: 1.0385990142822266 = 0.3661800026893616 + 0.1 * 6.724189281463623
Epoch 290, val loss: 0.7114160656929016
Epoch 300, training loss: 1.0010967254638672 = 0.33108893036842346 + 0.1 * 6.700077533721924
Epoch 300, val loss: 0.7003713250160217
Epoch 310, training loss: 0.9694008827209473 = 0.29853567481040955 + 0.1 * 6.708652496337891
Epoch 310, val loss: 0.6920158267021179
Epoch 320, training loss: 0.9392340183258057 = 0.26878127455711365 + 0.1 * 6.704526901245117
Epoch 320, val loss: 0.6862087249755859
Epoch 330, training loss: 0.9100046157836914 = 0.24177463352680206 + 0.1 * 6.682299613952637
Epoch 330, val loss: 0.6825731992721558
Epoch 340, training loss: 0.8841378688812256 = 0.21717321872711182 + 0.1 * 6.669646263122559
Epoch 340, val loss: 0.6808649301528931
Epoch 350, training loss: 0.8623837828636169 = 0.19485007226467133 + 0.1 * 6.675336837768555
Epoch 350, val loss: 0.6810317039489746
Epoch 360, training loss: 0.8402656316757202 = 0.17487429082393646 + 0.1 * 6.6539130210876465
Epoch 360, val loss: 0.682928204536438
Epoch 370, training loss: 0.8223519921302795 = 0.15706944465637207 + 0.1 * 6.652825355529785
Epoch 370, val loss: 0.6862607598304749
Epoch 380, training loss: 0.8104742765426636 = 0.1412944197654724 + 0.1 * 6.691798686981201
Epoch 380, val loss: 0.690934956073761
Epoch 390, training loss: 0.7924743890762329 = 0.12749677896499634 + 0.1 * 6.649775981903076
Epoch 390, val loss: 0.6965737342834473
Epoch 400, training loss: 0.7778667211532593 = 0.1153230369091034 + 0.1 * 6.625436782836914
Epoch 400, val loss: 0.7030990719795227
Epoch 410, training loss: 0.7667638659477234 = 0.10453818738460541 + 0.1 * 6.622256278991699
Epoch 410, val loss: 0.7104607820510864
Epoch 420, training loss: 0.7581450939178467 = 0.09501418471336365 + 0.1 * 6.631309509277344
Epoch 420, val loss: 0.7184007167816162
Epoch 430, training loss: 0.7472891807556152 = 0.08662465214729309 + 0.1 * 6.606645107269287
Epoch 430, val loss: 0.7266803979873657
Epoch 440, training loss: 0.7404705882072449 = 0.07916789501905441 + 0.1 * 6.6130266189575195
Epoch 440, val loss: 0.7352926135063171
Epoch 450, training loss: 0.7345160841941833 = 0.07254849374294281 + 0.1 * 6.619675636291504
Epoch 450, val loss: 0.7440979480743408
Epoch 460, training loss: 0.7263698577880859 = 0.06666293740272522 + 0.1 * 6.597068786621094
Epoch 460, val loss: 0.7528844475746155
Epoch 470, training loss: 0.7197507619857788 = 0.061392053961753845 + 0.1 * 6.583586692810059
Epoch 470, val loss: 0.7617785930633545
Epoch 480, training loss: 0.7147799730300903 = 0.056658342480659485 + 0.1 * 6.581216335296631
Epoch 480, val loss: 0.7706334590911865
Epoch 490, training loss: 0.7103598117828369 = 0.05240373685956001 + 0.1 * 6.579560279846191
Epoch 490, val loss: 0.7794654965400696
Epoch 500, training loss: 0.7057725787162781 = 0.04858105629682541 + 0.1 * 6.571915149688721
Epoch 500, val loss: 0.7881311178207397
Epoch 510, training loss: 0.7036118507385254 = 0.045120153576135635 + 0.1 * 6.584916591644287
Epoch 510, val loss: 0.7966988682746887
Epoch 520, training loss: 0.6987544298171997 = 0.04199083894491196 + 0.1 * 6.567636013031006
Epoch 520, val loss: 0.8052157163619995
Epoch 530, training loss: 0.6953962445259094 = 0.03915970399975777 + 0.1 * 6.5623650550842285
Epoch 530, val loss: 0.8134838938713074
Epoch 540, training loss: 0.6912856101989746 = 0.03659317269921303 + 0.1 * 6.546924114227295
Epoch 540, val loss: 0.8216138482093811
Epoch 550, training loss: 0.689085841178894 = 0.03424961864948273 + 0.1 * 6.5483622550964355
Epoch 550, val loss: 0.829653799533844
Epoch 560, training loss: 0.6884401440620422 = 0.03211171180009842 + 0.1 * 6.563284397125244
Epoch 560, val loss: 0.8375735878944397
Epoch 570, training loss: 0.6841424703598022 = 0.030167996883392334 + 0.1 * 6.539744853973389
Epoch 570, val loss: 0.8453463315963745
Epoch 580, training loss: 0.6816182732582092 = 0.02838723361492157 + 0.1 * 6.5323100090026855
Epoch 580, val loss: 0.8529415130615234
Epoch 590, training loss: 0.680368185043335 = 0.026754267513751984 + 0.1 * 6.536139011383057
Epoch 590, val loss: 0.8604342341423035
Epoch 600, training loss: 0.6785944104194641 = 0.02525893598794937 + 0.1 * 6.53335428237915
Epoch 600, val loss: 0.8677952289581299
Epoch 610, training loss: 0.6768994927406311 = 0.02388380840420723 + 0.1 * 6.53015661239624
Epoch 610, val loss: 0.8749589920043945
Epoch 620, training loss: 0.6752558946609497 = 0.022616848349571228 + 0.1 * 6.526390075683594
Epoch 620, val loss: 0.8820139169692993
Epoch 630, training loss: 0.6730148196220398 = 0.021447397768497467 + 0.1 * 6.515674114227295
Epoch 630, val loss: 0.8889448046684265
Epoch 640, training loss: 0.6727489233016968 = 0.02036740630865097 + 0.1 * 6.523815155029297
Epoch 640, val loss: 0.8957023620605469
Epoch 650, training loss: 0.6705736517906189 = 0.019370900467038155 + 0.1 * 6.512027263641357
Epoch 650, val loss: 0.9023311138153076
Epoch 660, training loss: 0.6689598560333252 = 0.018445586785674095 + 0.1 * 6.5051422119140625
Epoch 660, val loss: 0.9088101387023926
Epoch 670, training loss: 0.6680591106414795 = 0.01758618652820587 + 0.1 * 6.504729270935059
Epoch 670, val loss: 0.915194034576416
Epoch 680, training loss: 0.6663973331451416 = 0.0167886633425951 + 0.1 * 6.496086597442627
Epoch 680, val loss: 0.9214193224906921
Epoch 690, training loss: 0.6674349308013916 = 0.016043957322835922 + 0.1 * 6.513909816741943
Epoch 690, val loss: 0.9274943470954895
Epoch 700, training loss: 0.6651986241340637 = 0.015351664274930954 + 0.1 * 6.498469352722168
Epoch 700, val loss: 0.9334526062011719
Epoch 710, training loss: 0.6646053194999695 = 0.0147054148837924 + 0.1 * 6.498998641967773
Epoch 710, val loss: 0.9393150806427002
Epoch 720, training loss: 0.6634112000465393 = 0.01410139724612236 + 0.1 * 6.49309778213501
Epoch 720, val loss: 0.9450304508209229
Epoch 730, training loss: 0.6625723838806152 = 0.013535773381590843 + 0.1 * 6.490365505218506
Epoch 730, val loss: 0.9506586194038391
Epoch 740, training loss: 0.6627821922302246 = 0.013003620319068432 + 0.1 * 6.4977850914001465
Epoch 740, val loss: 0.956095278263092
Epoch 750, training loss: 0.6611449718475342 = 0.012505668215453625 + 0.1 * 6.486392974853516
Epoch 750, val loss: 0.9615517854690552
Epoch 760, training loss: 0.6612679362297058 = 0.012037763372063637 + 0.1 * 6.4923014640808105
Epoch 760, val loss: 0.9668503403663635
Epoch 770, training loss: 0.6596575975418091 = 0.011597339995205402 + 0.1 * 6.480602264404297
Epoch 770, val loss: 0.9719774127006531
Epoch 780, training loss: 0.6600267291069031 = 0.011182157322764397 + 0.1 * 6.48844575881958
Epoch 780, val loss: 0.9770337343215942
Epoch 790, training loss: 0.6590492129325867 = 0.010790601372718811 + 0.1 * 6.48258638381958
Epoch 790, val loss: 0.9820082783699036
Epoch 800, training loss: 0.6574966907501221 = 0.010421439073979855 + 0.1 * 6.470752239227295
Epoch 800, val loss: 0.9869076013565063
Epoch 810, training loss: 0.6574726700782776 = 0.010072763077914715 + 0.1 * 6.4739990234375
Epoch 810, val loss: 0.9917055368423462
Epoch 820, training loss: 0.6575188636779785 = 0.009741153568029404 + 0.1 * 6.477777004241943
Epoch 820, val loss: 0.9963769912719727
Epoch 830, training loss: 0.6561551690101624 = 0.009427325800061226 + 0.1 * 6.467278480529785
Epoch 830, val loss: 1.0009888410568237
Epoch 840, training loss: 0.6568315625190735 = 0.009129414334893227 + 0.1 * 6.47702169418335
Epoch 840, val loss: 1.0055222511291504
Epoch 850, training loss: 0.6565784811973572 = 0.008847174234688282 + 0.1 * 6.477313041687012
Epoch 850, val loss: 1.0099984407424927
Epoch 860, training loss: 0.6545648574829102 = 0.00857970118522644 + 0.1 * 6.4598517417907715
Epoch 860, val loss: 1.0143460035324097
Epoch 870, training loss: 0.6542445421218872 = 0.008325248956680298 + 0.1 * 6.459193229675293
Epoch 870, val loss: 1.0186550617218018
Epoch 880, training loss: 0.6549316644668579 = 0.008082258515059948 + 0.1 * 6.468493938446045
Epoch 880, val loss: 1.0228288173675537
Epoch 890, training loss: 0.6538916230201721 = 0.00785077828913927 + 0.1 * 6.4604082107543945
Epoch 890, val loss: 1.0269373655319214
Epoch 900, training loss: 0.6535961031913757 = 0.007630349136888981 + 0.1 * 6.459657669067383
Epoch 900, val loss: 1.0310516357421875
Epoch 910, training loss: 0.6537843346595764 = 0.007420396897941828 + 0.1 * 6.463639259338379
Epoch 910, val loss: 1.0349873304367065
Epoch 920, training loss: 0.6527674794197083 = 0.0072202724404633045 + 0.1 * 6.455471992492676
Epoch 920, val loss: 1.0389608144760132
Epoch 930, training loss: 0.6517419815063477 = 0.007028401829302311 + 0.1 * 6.4471354484558105
Epoch 930, val loss: 1.042784333229065
Epoch 940, training loss: 0.6522720456123352 = 0.00684455456212163 + 0.1 * 6.454274654388428
Epoch 940, val loss: 1.0465590953826904
Epoch 950, training loss: 0.6517512798309326 = 0.006669359747320414 + 0.1 * 6.45081901550293
Epoch 950, val loss: 1.0503008365631104
Epoch 960, training loss: 0.6526681184768677 = 0.006501368712633848 + 0.1 * 6.461667060852051
Epoch 960, val loss: 1.0539990663528442
Epoch 970, training loss: 0.6520081162452698 = 0.006340343505144119 + 0.1 * 6.4566779136657715
Epoch 970, val loss: 1.0575529336929321
Epoch 980, training loss: 0.650618314743042 = 0.006186299491673708 + 0.1 * 6.444319725036621
Epoch 980, val loss: 1.0611358880996704
Epoch 990, training loss: 0.6506352424621582 = 0.006038464140146971 + 0.1 * 6.445967197418213
Epoch 990, val loss: 1.064614176750183
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.838165524512388
The final CL Acc:0.81852, 0.00605, The final GNN Acc:0.84150, 0.00334
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11522])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10460])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.792442798614502 = 1.9327642917633057 + 0.1 * 8.596783638000488
Epoch 0, val loss: 1.922088623046875
Epoch 10, training loss: 2.7821619510650635 = 1.9224987030029297 + 0.1 * 8.59663200378418
Epoch 10, val loss: 1.9114980697631836
Epoch 20, training loss: 2.7694878578186035 = 1.9099105596542358 + 0.1 * 8.595771789550781
Epoch 20, val loss: 1.8984384536743164
Epoch 30, training loss: 2.7512307167053223 = 1.8923532962799072 + 0.1 * 8.588774681091309
Epoch 30, val loss: 1.8804280757904053
Epoch 40, training loss: 2.7216129302978516 = 1.8671568632125854 + 0.1 * 8.544561386108398
Epoch 40, val loss: 1.8551340103149414
Epoch 50, training loss: 2.6689107418060303 = 1.835490107536316 + 0.1 * 8.334206581115723
Epoch 50, val loss: 1.8252787590026855
Epoch 60, training loss: 2.6159558296203613 = 1.8033736944198608 + 0.1 * 8.12582015991211
Epoch 60, val loss: 1.797829508781433
Epoch 70, training loss: 2.5462756156921387 = 1.7748022079467773 + 0.1 * 7.7147345542907715
Epoch 70, val loss: 1.7753640413284302
Epoch 80, training loss: 2.479193687438965 = 1.7440507411956787 + 0.1 * 7.351428031921387
Epoch 80, val loss: 1.7510955333709717
Epoch 90, training loss: 2.4194564819335938 = 1.7030919790267944 + 0.1 * 7.163646221160889
Epoch 90, val loss: 1.7177891731262207
Epoch 100, training loss: 2.3530120849609375 = 1.646737813949585 + 0.1 * 7.062742233276367
Epoch 100, val loss: 1.6705507040023804
Epoch 110, training loss: 2.2760396003723145 = 1.5774279832839966 + 0.1 * 6.986117362976074
Epoch 110, val loss: 1.6134748458862305
Epoch 120, training loss: 2.1946170330047607 = 1.5009304285049438 + 0.1 * 6.936866760253906
Epoch 120, val loss: 1.552813172340393
Epoch 130, training loss: 2.1139395236968994 = 1.4239987134933472 + 0.1 * 6.899408340454102
Epoch 130, val loss: 1.493803858757019
Epoch 140, training loss: 2.0358901023864746 = 1.3490631580352783 + 0.1 * 6.868269920349121
Epoch 140, val loss: 1.438102126121521
Epoch 150, training loss: 1.9608087539672852 = 1.275022268295288 + 0.1 * 6.857865333557129
Epoch 150, val loss: 1.3843564987182617
Epoch 160, training loss: 1.8860758543014526 = 1.2031883001327515 + 0.1 * 6.828875541687012
Epoch 160, val loss: 1.3336939811706543
Epoch 170, training loss: 1.8129757642745972 = 1.1323167085647583 + 0.1 * 6.806590557098389
Epoch 170, val loss: 1.2845382690429688
Epoch 180, training loss: 1.7413933277130127 = 1.0616246461868286 + 0.1 * 6.7976861000061035
Epoch 180, val loss: 1.2361352443695068
Epoch 190, training loss: 1.6705780029296875 = 0.9926649332046509 + 0.1 * 6.779130935668945
Epoch 190, val loss: 1.1880825757980347
Epoch 200, training loss: 1.6018550395965576 = 0.9252886176109314 + 0.1 * 6.765664577484131
Epoch 200, val loss: 1.1405783891677856
Epoch 210, training loss: 1.5356478691101074 = 0.8594845533370972 + 0.1 * 6.761633396148682
Epoch 210, val loss: 1.0935523509979248
Epoch 220, training loss: 1.4735100269317627 = 0.7966333627700806 + 0.1 * 6.768766403198242
Epoch 220, val loss: 1.048791527748108
Epoch 230, training loss: 1.4117155075073242 = 0.7385510802268982 + 0.1 * 6.731644153594971
Epoch 230, val loss: 1.0081456899642944
Epoch 240, training loss: 1.3566561937332153 = 0.6850335001945496 + 0.1 * 6.716227054595947
Epoch 240, val loss: 0.9718561172485352
Epoch 250, training loss: 1.3065025806427002 = 0.6361668705940247 + 0.1 * 6.703357696533203
Epoch 250, val loss: 0.9406481981277466
Epoch 260, training loss: 1.2619513273239136 = 0.59243243932724 + 0.1 * 6.695188522338867
Epoch 260, val loss: 0.9148745536804199
Epoch 270, training loss: 1.2222802639007568 = 0.5534154772758484 + 0.1 * 6.688648223876953
Epoch 270, val loss: 0.894321620464325
Epoch 280, training loss: 1.1867976188659668 = 0.5182492136955261 + 0.1 * 6.685483455657959
Epoch 280, val loss: 0.8781503438949585
Epoch 290, training loss: 1.153171420097351 = 0.4860442578792572 + 0.1 * 6.671271800994873
Epoch 290, val loss: 0.8656038641929626
Epoch 300, training loss: 1.121586561203003 = 0.4556143581867218 + 0.1 * 6.659721851348877
Epoch 300, val loss: 0.8558278679847717
Epoch 310, training loss: 1.0929652452468872 = 0.4263329803943634 + 0.1 * 6.666322708129883
Epoch 310, val loss: 0.8482757210731506
Epoch 320, training loss: 1.0628918409347534 = 0.3979395925998688 + 0.1 * 6.64952278137207
Epoch 320, val loss: 0.8427513241767883
Epoch 330, training loss: 1.0348021984100342 = 0.370317667722702 + 0.1 * 6.644845008850098
Epoch 330, val loss: 0.8390111923217773
Epoch 340, training loss: 1.0083469152450562 = 0.3438633680343628 + 0.1 * 6.644835472106934
Epoch 340, val loss: 0.8372344374656677
Epoch 350, training loss: 0.98268723487854 = 0.3191472589969635 + 0.1 * 6.635399341583252
Epoch 350, val loss: 0.8373927474021912
Epoch 360, training loss: 0.9588574767112732 = 0.2964102625846863 + 0.1 * 6.624472141265869
Epoch 360, val loss: 0.8395530581474304
Epoch 370, training loss: 0.9382357597351074 = 0.2755909264087677 + 0.1 * 6.626448631286621
Epoch 370, val loss: 0.8433115482330322
Epoch 380, training loss: 0.9178130030632019 = 0.256488561630249 + 0.1 * 6.613244533538818
Epoch 380, val loss: 0.8483337163925171
Epoch 390, training loss: 0.9004737138748169 = 0.2386510670185089 + 0.1 * 6.618226051330566
Epoch 390, val loss: 0.8540620803833008
Epoch 400, training loss: 0.8829838037490845 = 0.22168990969657898 + 0.1 * 6.612938404083252
Epoch 400, val loss: 0.8599935173988342
Epoch 410, training loss: 0.8666731119155884 = 0.20518629252910614 + 0.1 * 6.614867687225342
Epoch 410, val loss: 0.8659479022026062
Epoch 420, training loss: 0.8491567969322205 = 0.18901516497135162 + 0.1 * 6.601416110992432
Epoch 420, val loss: 0.8716574907302856
Epoch 430, training loss: 0.8324120044708252 = 0.1731586754322052 + 0.1 * 6.592532634735107
Epoch 430, val loss: 0.8772076368331909
Epoch 440, training loss: 0.8176875114440918 = 0.15778352320194244 + 0.1 * 6.5990400314331055
Epoch 440, val loss: 0.8828573822975159
Epoch 450, training loss: 0.8026903867721558 = 0.14322449266910553 + 0.1 * 6.594658851623535
Epoch 450, val loss: 0.8887723684310913
Epoch 460, training loss: 0.7879823446273804 = 0.12969054281711578 + 0.1 * 6.582918167114258
Epoch 460, val loss: 0.8953045010566711
Epoch 470, training loss: 0.7761865854263306 = 0.11728857457637787 + 0.1 * 6.588979721069336
Epoch 470, val loss: 0.9025660753250122
Epoch 480, training loss: 0.7649134993553162 = 0.10609791427850723 + 0.1 * 6.588155746459961
Epoch 480, val loss: 0.9108490347862244
Epoch 490, training loss: 0.7539252042770386 = 0.09612840414047241 + 0.1 * 6.577967643737793
Epoch 490, val loss: 0.9199216365814209
Epoch 500, training loss: 0.7437177896499634 = 0.08726699650287628 + 0.1 * 6.564507961273193
Epoch 500, val loss: 0.9298290014266968
Epoch 510, training loss: 0.736187219619751 = 0.07939223200082779 + 0.1 * 6.5679497718811035
Epoch 510, val loss: 0.9405134916305542
Epoch 520, training loss: 0.7287161350250244 = 0.07241808623075485 + 0.1 * 6.5629801750183105
Epoch 520, val loss: 0.9517295360565186
Epoch 530, training loss: 0.722586989402771 = 0.06625606119632721 + 0.1 * 6.563309192657471
Epoch 530, val loss: 0.9632887244224548
Epoch 540, training loss: 0.7163076400756836 = 0.060809630900621414 + 0.1 * 6.554980278015137
Epoch 540, val loss: 0.9751923680305481
Epoch 550, training loss: 0.7116483449935913 = 0.05597034841775894 + 0.1 * 6.556779861450195
Epoch 550, val loss: 0.9872198104858398
Epoch 560, training loss: 0.7065137028694153 = 0.05167160555720329 + 0.1 * 6.548420429229736
Epoch 560, val loss: 0.999323308467865
Epoch 570, training loss: 0.7028882503509521 = 0.04783757030963898 + 0.1 * 6.550507068634033
Epoch 570, val loss: 1.0113900899887085
Epoch 580, training loss: 0.6979510188102722 = 0.044409047812223434 + 0.1 * 6.535419940948486
Epoch 580, val loss: 1.0234642028808594
Epoch 590, training loss: 0.6962583065032959 = 0.04133124649524689 + 0.1 * 6.5492706298828125
Epoch 590, val loss: 1.0352623462677002
Epoch 600, training loss: 0.6918265223503113 = 0.03856430575251579 + 0.1 * 6.53262186050415
Epoch 600, val loss: 1.047182559967041
Epoch 610, training loss: 0.6895017027854919 = 0.03605816140770912 + 0.1 * 6.534434795379639
Epoch 610, val loss: 1.0587103366851807
Epoch 620, training loss: 0.6883679628372192 = 0.03378793224692345 + 0.1 * 6.54580020904541
Epoch 620, val loss: 1.070249080657959
Epoch 630, training loss: 0.6847944855690002 = 0.03172997757792473 + 0.1 * 6.53064489364624
Epoch 630, val loss: 1.0814241170883179
Epoch 640, training loss: 0.6823303699493408 = 0.029861070215702057 + 0.1 * 6.524692535400391
Epoch 640, val loss: 1.0925785303115845
Epoch 650, training loss: 0.679580569267273 = 0.02815338224172592 + 0.1 * 6.5142717361450195
Epoch 650, val loss: 1.1034414768218994
Epoch 660, training loss: 0.6800965666770935 = 0.026588326320052147 + 0.1 * 6.5350823402404785
Epoch 660, val loss: 1.1141000986099243
Epoch 670, training loss: 0.6769124269485474 = 0.025158921256661415 + 0.1 * 6.5175347328186035
Epoch 670, val loss: 1.1245564222335815
Epoch 680, training loss: 0.675233006477356 = 0.023844679817557335 + 0.1 * 6.513883590698242
Epoch 680, val loss: 1.1348233222961426
Epoch 690, training loss: 0.6733136773109436 = 0.022634686902165413 + 0.1 * 6.506789684295654
Epoch 690, val loss: 1.1448864936828613
Epoch 700, training loss: 0.672866702079773 = 0.021516088396310806 + 0.1 * 6.513505935668945
Epoch 700, val loss: 1.1545614004135132
Epoch 710, training loss: 0.670653223991394 = 0.020485861226916313 + 0.1 * 6.501673221588135
Epoch 710, val loss: 1.1642190217971802
Epoch 720, training loss: 0.6697641611099243 = 0.019529730081558228 + 0.1 * 6.502344131469727
Epoch 720, val loss: 1.173661470413208
Epoch 730, training loss: 0.6682429313659668 = 0.018641235306859016 + 0.1 * 6.496016979217529
Epoch 730, val loss: 1.1828144788742065
Epoch 740, training loss: 0.6694716215133667 = 0.0178152397274971 + 0.1 * 6.516563892364502
Epoch 740, val loss: 1.1918935775756836
Epoch 750, training loss: 0.6663490533828735 = 0.017046205699443817 + 0.1 * 6.493028163909912
Epoch 750, val loss: 1.2007832527160645
Epoch 760, training loss: 0.6657097339630127 = 0.01632797345519066 + 0.1 * 6.493817329406738
Epoch 760, val loss: 1.2095128297805786
Epoch 770, training loss: 0.6641823649406433 = 0.01565752550959587 + 0.1 * 6.48524808883667
Epoch 770, val loss: 1.2180252075195312
Epoch 780, training loss: 0.6658410429954529 = 0.015027536079287529 + 0.1 * 6.5081353187561035
Epoch 780, val loss: 1.2261683940887451
Epoch 790, training loss: 0.6633361577987671 = 0.014443298801779747 + 0.1 * 6.48892879486084
Epoch 790, val loss: 1.234430193901062
Epoch 800, training loss: 0.6620691418647766 = 0.013890808448195457 + 0.1 * 6.481783390045166
Epoch 800, val loss: 1.2423996925354004
Epoch 810, training loss: 0.6633894443511963 = 0.013372981920838356 + 0.1 * 6.50016450881958
Epoch 810, val loss: 1.2501716613769531
Epoch 820, training loss: 0.6606860160827637 = 0.012885821051895618 + 0.1 * 6.478002071380615
Epoch 820, val loss: 1.2578130960464478
Epoch 830, training loss: 0.6601147651672363 = 0.01242680475115776 + 0.1 * 6.476879596710205
Epoch 830, val loss: 1.2653639316558838
Epoch 840, training loss: 0.6600717306137085 = 0.011992852203547955 + 0.1 * 6.480788707733154
Epoch 840, val loss: 1.2726572751998901
Epoch 850, training loss: 0.6600867509841919 = 0.011583611369132996 + 0.1 * 6.4850311279296875
Epoch 850, val loss: 1.2798917293548584
Epoch 860, training loss: 0.6580779552459717 = 0.011197256855666637 + 0.1 * 6.468806743621826
Epoch 860, val loss: 1.2869240045547485
Epoch 870, training loss: 0.6580696105957031 = 0.010831487365067005 + 0.1 * 6.472381114959717
Epoch 870, val loss: 1.293889045715332
Epoch 880, training loss: 0.658470094203949 = 0.010484217666089535 + 0.1 * 6.4798583984375
Epoch 880, val loss: 1.3006130456924438
Epoch 890, training loss: 0.6582002639770508 = 0.010155506432056427 + 0.1 * 6.480447769165039
Epoch 890, val loss: 1.307270884513855
Epoch 900, training loss: 0.6565712094306946 = 0.009844089858233929 + 0.1 * 6.467271327972412
Epoch 900, val loss: 1.3138338327407837
Epoch 910, training loss: 0.658021092414856 = 0.00954672321677208 + 0.1 * 6.484744071960449
Epoch 910, val loss: 1.3202699422836304
Epoch 920, training loss: 0.6556993722915649 = 0.009264493361115456 + 0.1 * 6.464348793029785
Epoch 920, val loss: 1.326397180557251
Epoch 930, training loss: 0.655580461025238 = 0.008997656404972076 + 0.1 * 6.465827941894531
Epoch 930, val loss: 1.3327912092208862
Epoch 940, training loss: 0.6550219058990479 = 0.00874170009046793 + 0.1 * 6.462802410125732
Epoch 940, val loss: 1.3388278484344482
Epoch 950, training loss: 0.6539438962936401 = 0.008496688678860664 + 0.1 * 6.454471588134766
Epoch 950, val loss: 1.344695806503296
Epoch 960, training loss: 0.6541509032249451 = 0.008264546282589436 + 0.1 * 6.458863258361816
Epoch 960, val loss: 1.3507987260818481
Epoch 970, training loss: 0.6548418402671814 = 0.008041233755648136 + 0.1 * 6.468005657196045
Epoch 970, val loss: 1.3565731048583984
Epoch 980, training loss: 0.6525603532791138 = 0.007829032838344574 + 0.1 * 6.44731330871582
Epoch 980, val loss: 1.3622891902923584
Epoch 990, training loss: 0.6535582542419434 = 0.00762556679546833 + 0.1 * 6.45932674407959
Epoch 990, val loss: 1.3680063486099243
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 2.807023048400879 = 1.947340726852417 + 0.1 * 8.596822738647461
Epoch 0, val loss: 1.9508264064788818
Epoch 10, training loss: 2.7978293895721436 = 1.938153624534607 + 0.1 * 8.596756935119629
Epoch 10, val loss: 1.9412564039230347
Epoch 20, training loss: 2.7867798805236816 = 1.9271376132965088 + 0.1 * 8.596421241760254
Epoch 20, val loss: 1.9297313690185547
Epoch 30, training loss: 2.7713823318481445 = 1.9119731187820435 + 0.1 * 8.594093322753906
Epoch 30, val loss: 1.9138685464859009
Epoch 40, training loss: 2.7473223209381104 = 1.8897610902786255 + 0.1 * 8.575613021850586
Epoch 40, val loss: 1.8909024000167847
Epoch 50, training loss: 2.7027440071105957 = 1.8584911823272705 + 0.1 * 8.44252872467041
Epoch 50, val loss: 1.8597484827041626
Epoch 60, training loss: 2.629101276397705 = 1.8234038352966309 + 0.1 * 8.056974411010742
Epoch 60, val loss: 1.8272072076797485
Epoch 70, training loss: 2.55472993850708 = 1.7920615673065186 + 0.1 * 7.626683235168457
Epoch 70, val loss: 1.7989342212677002
Epoch 80, training loss: 2.4951133728027344 = 1.7619744539260864 + 0.1 * 7.331389427185059
Epoch 80, val loss: 1.7724149227142334
Epoch 90, training loss: 2.4431657791137695 = 1.7269647121429443 + 0.1 * 7.16201114654541
Epoch 90, val loss: 1.7408279180526733
Epoch 100, training loss: 2.3868932723999023 = 1.6786818504333496 + 0.1 * 7.082112789154053
Epoch 100, val loss: 1.695595622062683
Epoch 110, training loss: 2.3146543502807617 = 1.612596035003662 + 0.1 * 7.0205841064453125
Epoch 110, val loss: 1.636351227760315
Epoch 120, training loss: 2.225573778152466 = 1.527532935142517 + 0.1 * 6.980408191680908
Epoch 120, val loss: 1.5642355680465698
Epoch 130, training loss: 2.124472141265869 = 1.4296835660934448 + 0.1 * 6.9478864669799805
Epoch 130, val loss: 1.4824845790863037
Epoch 140, training loss: 2.0174245834350586 = 1.3252487182617188 + 0.1 * 6.92175817489624
Epoch 140, val loss: 1.3977192640304565
Epoch 150, training loss: 1.9101037979125977 = 1.2209635972976685 + 0.1 * 6.891402244567871
Epoch 150, val loss: 1.314802646636963
Epoch 160, training loss: 1.8067336082458496 = 1.1194416284561157 + 0.1 * 6.87291955947876
Epoch 160, val loss: 1.2363109588623047
Epoch 170, training loss: 1.7108376026153564 = 1.026308536529541 + 0.1 * 6.845290660858154
Epoch 170, val loss: 1.1653977632522583
Epoch 180, training loss: 1.6251851320266724 = 0.9413699507713318 + 0.1 * 6.838151454925537
Epoch 180, val loss: 1.1015419960021973
Epoch 190, training loss: 1.54734206199646 = 0.8671343922615051 + 0.1 * 6.802077293395996
Epoch 190, val loss: 1.0468811988830566
Epoch 200, training loss: 1.4810876846313477 = 0.8022792935371399 + 0.1 * 6.788083076477051
Epoch 200, val loss: 1.0007452964782715
Epoch 210, training loss: 1.4236160516738892 = 0.7466698288917542 + 0.1 * 6.7694621086120605
Epoch 210, val loss: 0.9635279774665833
Epoch 220, training loss: 1.3736227750778198 = 0.6981088519096375 + 0.1 * 6.755138874053955
Epoch 220, val loss: 0.9332705736160278
Epoch 230, training loss: 1.3290913105010986 = 0.6543546915054321 + 0.1 * 6.747366428375244
Epoch 230, val loss: 0.908242404460907
Epoch 240, training loss: 1.2880758047103882 = 0.6134911179542542 + 0.1 * 6.745846748352051
Epoch 240, val loss: 0.8863376379013062
Epoch 250, training loss: 1.2460627555847168 = 0.5739904046058655 + 0.1 * 6.7207231521606445
Epoch 250, val loss: 0.8662444353103638
Epoch 260, training loss: 1.205437421798706 = 0.5348227620124817 + 0.1 * 6.706146717071533
Epoch 260, val loss: 0.8475844264030457
Epoch 270, training loss: 1.1668423414230347 = 0.49587714672088623 + 0.1 * 6.709651947021484
Epoch 270, val loss: 0.8309091925621033
Epoch 280, training loss: 1.127685785293579 = 0.45795702934265137 + 0.1 * 6.697287082672119
Epoch 280, val loss: 0.8171831965446472
Epoch 290, training loss: 1.0928497314453125 = 0.421740859746933 + 0.1 * 6.71108865737915
Epoch 290, val loss: 0.8071076273918152
Epoch 300, training loss: 1.0568127632141113 = 0.3884449601173401 + 0.1 * 6.68367862701416
Epoch 300, val loss: 0.8009474873542786
Epoch 310, training loss: 1.025058388710022 = 0.35813748836517334 + 0.1 * 6.669209003448486
Epoch 310, val loss: 0.7986599206924438
Epoch 320, training loss: 1.0004757642745972 = 0.3306226432323456 + 0.1 * 6.698531150817871
Epoch 320, val loss: 0.7999423146247864
Epoch 330, training loss: 0.972043514251709 = 0.3056853711605072 + 0.1 * 6.663581848144531
Epoch 330, val loss: 0.803844690322876
Epoch 340, training loss: 0.9492566585540771 = 0.28278598189353943 + 0.1 * 6.664707183837891
Epoch 340, val loss: 0.8100664615631104
Epoch 350, training loss: 0.9263604879379272 = 0.2616640031337738 + 0.1 * 6.6469645500183105
Epoch 350, val loss: 0.8179948329925537
Epoch 360, training loss: 0.9063054323196411 = 0.24203482270240784 + 0.1 * 6.642706394195557
Epoch 360, val loss: 0.8274062871932983
Epoch 370, training loss: 0.8874647617340088 = 0.22377929091453552 + 0.1 * 6.636855125427246
Epoch 370, val loss: 0.8379899859428406
Epoch 380, training loss: 0.8697859644889832 = 0.2067672461271286 + 0.1 * 6.630187034606934
Epoch 380, val loss: 0.8495736718177795
Epoch 390, training loss: 0.8552442789077759 = 0.1909022331237793 + 0.1 * 6.643420219421387
Epoch 390, val loss: 0.8619815707206726
Epoch 400, training loss: 0.8384799361228943 = 0.1761821061372757 + 0.1 * 6.622978210449219
Epoch 400, val loss: 0.8749615550041199
Epoch 410, training loss: 0.8239774703979492 = 0.16246072947978973 + 0.1 * 6.615167140960693
Epoch 410, val loss: 0.8886337280273438
Epoch 420, training loss: 0.8120932579040527 = 0.14967185258865356 + 0.1 * 6.624213695526123
Epoch 420, val loss: 0.9028788805007935
Epoch 430, training loss: 0.8001363277435303 = 0.13782218098640442 + 0.1 * 6.623141765594482
Epoch 430, val loss: 0.9175092577934265
Epoch 440, training loss: 0.7879142761230469 = 0.12687331438064575 + 0.1 * 6.610409259796143
Epoch 440, val loss: 0.9325594902038574
Epoch 450, training loss: 0.7774239182472229 = 0.11675230413675308 + 0.1 * 6.606715679168701
Epoch 450, val loss: 0.9480312466621399
Epoch 460, training loss: 0.7676094174385071 = 0.10741329938173294 + 0.1 * 6.601961135864258
Epoch 460, val loss: 0.963809072971344
Epoch 470, training loss: 0.7587904930114746 = 0.09881333261728287 + 0.1 * 6.599771022796631
Epoch 470, val loss: 0.9799078106880188
Epoch 480, training loss: 0.7510207891464233 = 0.09092152863740921 + 0.1 * 6.600992679595947
Epoch 480, val loss: 0.9962643980979919
Epoch 490, training loss: 0.7423263788223267 = 0.08371022343635559 + 0.1 * 6.586161136627197
Epoch 490, val loss: 1.0127465724945068
Epoch 500, training loss: 0.735338568687439 = 0.07712619006633759 + 0.1 * 6.582123279571533
Epoch 500, val loss: 1.0293943881988525
Epoch 510, training loss: 0.7285739183425903 = 0.07112965732812881 + 0.1 * 6.574442386627197
Epoch 510, val loss: 1.0460631847381592
Epoch 520, training loss: 0.7232450246810913 = 0.06568092107772827 + 0.1 * 6.575640678405762
Epoch 520, val loss: 1.0627071857452393
Epoch 530, training loss: 0.7185863852500916 = 0.06073417514562607 + 0.1 * 6.578522205352783
Epoch 530, val loss: 1.079298973083496
Epoch 540, training loss: 0.7124475240707397 = 0.05625606328248978 + 0.1 * 6.561914443969727
Epoch 540, val loss: 1.0956761837005615
Epoch 550, training loss: 0.7084077000617981 = 0.052198901772499084 + 0.1 * 6.5620880126953125
Epoch 550, val loss: 1.1119197607040405
Epoch 560, training loss: 0.705858588218689 = 0.0485188327729702 + 0.1 * 6.573397636413574
Epoch 560, val loss: 1.1279462575912476
Epoch 570, training loss: 0.70311439037323 = 0.04518335685133934 + 0.1 * 6.579309940338135
Epoch 570, val loss: 1.1436504125595093
Epoch 580, training loss: 0.6974842548370361 = 0.0421614944934845 + 0.1 * 6.553226947784424
Epoch 580, val loss: 1.1590348482131958
Epoch 590, training loss: 0.694006085395813 = 0.039415497332811356 + 0.1 * 6.545906066894531
Epoch 590, val loss: 1.1741605997085571
Epoch 600, training loss: 0.6936025023460388 = 0.03691329061985016 + 0.1 * 6.566892147064209
Epoch 600, val loss: 1.1890109777450562
Epoch 610, training loss: 0.6891145706176758 = 0.03463491052389145 + 0.1 * 6.544796466827393
Epoch 610, val loss: 1.2034354209899902
Epoch 620, training loss: 0.6862432360649109 = 0.032555557787418365 + 0.1 * 6.536876201629639
Epoch 620, val loss: 1.2175979614257812
Epoch 630, training loss: 0.6869142651557922 = 0.030650941655039787 + 0.1 * 6.562633514404297
Epoch 630, val loss: 1.2314659357070923
Epoch 640, training loss: 0.6832685470581055 = 0.028911961242556572 + 0.1 * 6.54356575012207
Epoch 640, val loss: 1.2448649406433105
Epoch 650, training loss: 0.6801298260688782 = 0.027317162603139877 + 0.1 * 6.528126239776611
Epoch 650, val loss: 1.258017897605896
Epoch 660, training loss: 0.6787368655204773 = 0.025849347934126854 + 0.1 * 6.528875350952148
Epoch 660, val loss: 1.2708784341812134
Epoch 670, training loss: 0.6768712997436523 = 0.02449667453765869 + 0.1 * 6.523746013641357
Epoch 670, val loss: 1.2834440469741821
Epoch 680, training loss: 0.6780638694763184 = 0.023248955607414246 + 0.1 * 6.5481486320495605
Epoch 680, val loss: 1.2957035303115845
Epoch 690, training loss: 0.6740221977233887 = 0.022098209708929062 + 0.1 * 6.519239902496338
Epoch 690, val loss: 1.3076061010360718
Epoch 700, training loss: 0.6734066605567932 = 0.021033786237239838 + 0.1 * 6.523728847503662
Epoch 700, val loss: 1.3193111419677734
Epoch 710, training loss: 0.6712592244148254 = 0.020047640427947044 + 0.1 * 6.512115478515625
Epoch 710, val loss: 1.3306776285171509
Epoch 720, training loss: 0.6703197360038757 = 0.01913050375878811 + 0.1 * 6.511891841888428
Epoch 720, val loss: 1.3418314456939697
Epoch 730, training loss: 0.6690583229064941 = 0.018278436735272408 + 0.1 * 6.507798671722412
Epoch 730, val loss: 1.3526921272277832
Epoch 740, training loss: 0.6686118841171265 = 0.017484869807958603 + 0.1 * 6.511270523071289
Epoch 740, val loss: 1.363305687904358
Epoch 750, training loss: 0.6679068803787231 = 0.016745658591389656 + 0.1 * 6.5116119384765625
Epoch 750, val loss: 1.3736612796783447
Epoch 760, training loss: 0.6658957004547119 = 0.016054829582571983 + 0.1 * 6.498408794403076
Epoch 760, val loss: 1.3838096857070923
Epoch 770, training loss: 0.6654891967773438 = 0.015407579950988293 + 0.1 * 6.500816345214844
Epoch 770, val loss: 1.3937627077102661
Epoch 780, training loss: 0.664375364780426 = 0.01480092667043209 + 0.1 * 6.495744705200195
Epoch 780, val loss: 1.4034277200698853
Epoch 790, training loss: 0.6637942790985107 = 0.014233196154236794 + 0.1 * 6.495611190795898
Epoch 790, val loss: 1.4128503799438477
Epoch 800, training loss: 0.6636337041854858 = 0.013699624687433243 + 0.1 * 6.499340534210205
Epoch 800, val loss: 1.4221118688583374
Epoch 810, training loss: 0.6623315215110779 = 0.013199009001255035 + 0.1 * 6.4913249015808105
Epoch 810, val loss: 1.431128978729248
Epoch 820, training loss: 0.6614018082618713 = 0.012725995853543282 + 0.1 * 6.486757755279541
Epoch 820, val loss: 1.4400420188903809
Epoch 830, training loss: 0.6613814830780029 = 0.012279869057238102 + 0.1 * 6.491015911102295
Epoch 830, val loss: 1.4487462043762207
Epoch 840, training loss: 0.662404477596283 = 0.01185907144099474 + 0.1 * 6.505454063415527
Epoch 840, val loss: 1.457253098487854
Epoch 850, training loss: 0.6599622964859009 = 0.011461569927632809 + 0.1 * 6.485007286071777
Epoch 850, val loss: 1.4655879735946655
Epoch 860, training loss: 0.6600757241249084 = 0.01108533050864935 + 0.1 * 6.489903926849365
Epoch 860, val loss: 1.4738032817840576
Epoch 870, training loss: 0.6584998369216919 = 0.010728903114795685 + 0.1 * 6.4777092933654785
Epoch 870, val loss: 1.4818445444107056
Epoch 880, training loss: 0.658214807510376 = 0.010390901006758213 + 0.1 * 6.478239059448242
Epoch 880, val loss: 1.4897372722625732
Epoch 890, training loss: 0.6591653227806091 = 0.010070488788187504 + 0.1 * 6.49094820022583
Epoch 890, val loss: 1.4974619150161743
Epoch 900, training loss: 0.6570159792900085 = 0.009766320697963238 + 0.1 * 6.472496032714844
Epoch 900, val loss: 1.5049992799758911
Epoch 910, training loss: 0.656394898891449 = 0.009477794170379639 + 0.1 * 6.469171047210693
Epoch 910, val loss: 1.5124354362487793
Epoch 920, training loss: 0.6563736200332642 = 0.009202497079968452 + 0.1 * 6.471711158752441
Epoch 920, val loss: 1.5197837352752686
Epoch 930, training loss: 0.6564198732376099 = 0.008940163999795914 + 0.1 * 6.474796772003174
Epoch 930, val loss: 1.526978850364685
Epoch 940, training loss: 0.6564026474952698 = 0.008689917623996735 + 0.1 * 6.4771270751953125
Epoch 940, val loss: 1.5340217351913452
Epoch 950, training loss: 0.6554532051086426 = 0.008451379835605621 + 0.1 * 6.470017910003662
Epoch 950, val loss: 1.540845274925232
Epoch 960, training loss: 0.654606282711029 = 0.008224721066653728 + 0.1 * 6.463815689086914
Epoch 960, val loss: 1.5476160049438477
Epoch 970, training loss: 0.6542551517486572 = 0.00800743605941534 + 0.1 * 6.462477207183838
Epoch 970, val loss: 1.554342269897461
Epoch 980, training loss: 0.654638946056366 = 0.007799571845680475 + 0.1 * 6.468393802642822
Epoch 980, val loss: 1.5609407424926758
Epoch 990, training loss: 0.6538089513778687 = 0.007600640412420034 + 0.1 * 6.462082862854004
Epoch 990, val loss: 1.5674067735671997
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 2.8039329051971436 = 1.9442545175552368 + 0.1 * 8.596783638000488
Epoch 0, val loss: 1.9390276670455933
Epoch 10, training loss: 2.793860912322998 = 1.9341930150985718 + 0.1 * 8.596678733825684
Epoch 10, val loss: 1.9293701648712158
Epoch 20, training loss: 2.7813313007354736 = 1.921726942062378 + 0.1 * 8.596043586730957
Epoch 20, val loss: 1.9170564413070679
Epoch 30, training loss: 2.763439416885376 = 1.9043309688568115 + 0.1 * 8.591084480285645
Epoch 30, val loss: 1.8996624946594238
Epoch 40, training loss: 2.735267162322998 = 1.8792481422424316 + 0.1 * 8.560190200805664
Epoch 40, val loss: 1.874890923500061
Epoch 50, training loss: 2.685546636581421 = 1.8464621305465698 + 0.1 * 8.39084529876709
Epoch 50, val loss: 1.8443481922149658
Epoch 60, training loss: 2.6279702186584473 = 1.8122737407684326 + 0.1 * 8.156964302062988
Epoch 60, val loss: 1.8153830766677856
Epoch 70, training loss: 2.5531084537506104 = 1.7822673320770264 + 0.1 * 7.708411693572998
Epoch 70, val loss: 1.7917495965957642
Epoch 80, training loss: 2.483102798461914 = 1.752442479133606 + 0.1 * 7.306603908538818
Epoch 80, val loss: 1.7672016620635986
Epoch 90, training loss: 2.4283862113952637 = 1.713432788848877 + 0.1 * 7.149535179138184
Epoch 90, val loss: 1.7339509725570679
Epoch 100, training loss: 2.3659470081329346 = 1.65806245803833 + 0.1 * 7.078845977783203
Epoch 100, val loss: 1.6867952346801758
Epoch 110, training loss: 2.288501501083374 = 1.5870928764343262 + 0.1 * 7.0140862464904785
Epoch 110, val loss: 1.629270315170288
Epoch 120, training loss: 2.2039151191711426 = 1.5071767568588257 + 0.1 * 6.967383861541748
Epoch 120, val loss: 1.5662530660629272
Epoch 130, training loss: 2.120142936706543 = 1.4263992309570312 + 0.1 * 6.937437057495117
Epoch 130, val loss: 1.5050493478775024
Epoch 140, training loss: 2.0385875701904297 = 1.3473230600357056 + 0.1 * 6.912644386291504
Epoch 140, val loss: 1.4467538595199585
Epoch 150, training loss: 1.9573464393615723 = 1.2686187028884888 + 0.1 * 6.887278079986572
Epoch 150, val loss: 1.3908203840255737
Epoch 160, training loss: 1.8769311904907227 = 1.1897382736206055 + 0.1 * 6.87192964553833
Epoch 160, val loss: 1.3354219198226929
Epoch 170, training loss: 1.7981069087982178 = 1.112890601158142 + 0.1 * 6.8521623611450195
Epoch 170, val loss: 1.281977653503418
Epoch 180, training loss: 1.7216347455978394 = 1.0376203060150146 + 0.1 * 6.840144157409668
Epoch 180, val loss: 1.2303072214126587
Epoch 190, training loss: 1.6479170322418213 = 0.9655971527099609 + 0.1 * 6.823197841644287
Epoch 190, val loss: 1.1822247505187988
Epoch 200, training loss: 1.5769245624542236 = 0.8962598443031311 + 0.1 * 6.806646347045898
Epoch 200, val loss: 1.1374270915985107
Epoch 210, training loss: 1.5092741250991821 = 0.8295889496803284 + 0.1 * 6.796851634979248
Epoch 210, val loss: 1.0960254669189453
Epoch 220, training loss: 1.44429349899292 = 0.7663122415542603 + 0.1 * 6.779813289642334
Epoch 220, val loss: 1.058760166168213
Epoch 230, training loss: 1.3825891017913818 = 0.7064059972763062 + 0.1 * 6.761831283569336
Epoch 230, val loss: 1.0252327919006348
Epoch 240, training loss: 1.3271020650863647 = 0.6511517763137817 + 0.1 * 6.75950288772583
Epoch 240, val loss: 0.9967961311340332
Epoch 250, training loss: 1.2761399745941162 = 0.601529598236084 + 0.1 * 6.7461042404174805
Epoch 250, val loss: 0.9740297794342041
Epoch 260, training loss: 1.2299015522003174 = 0.556890606880188 + 0.1 * 6.730109214782715
Epoch 260, val loss: 0.9562462568283081
Epoch 270, training loss: 1.189131498336792 = 0.5166701078414917 + 0.1 * 6.724613666534424
Epoch 270, val loss: 0.9431823492050171
Epoch 280, training loss: 1.1509757041931152 = 0.480265736579895 + 0.1 * 6.707098960876465
Epoch 280, val loss: 0.9341952800750732
Epoch 290, training loss: 1.1177780628204346 = 0.4467422366142273 + 0.1 * 6.710357666015625
Epoch 290, val loss: 0.9286720752716064
Epoch 300, training loss: 1.0852153301239014 = 0.415824294090271 + 0.1 * 6.693911075592041
Epoch 300, val loss: 0.9264006614685059
Epoch 310, training loss: 1.0556373596191406 = 0.3870983421802521 + 0.1 * 6.685389518737793
Epoch 310, val loss: 0.9271188974380493
Epoch 320, training loss: 1.0281963348388672 = 0.3602973222732544 + 0.1 * 6.678989887237549
Epoch 320, val loss: 0.9306343793869019
Epoch 330, training loss: 1.0024268627166748 = 0.3353166878223419 + 0.1 * 6.6711015701293945
Epoch 330, val loss: 0.9367258548736572
Epoch 340, training loss: 0.9785871505737305 = 0.31197670102119446 + 0.1 * 6.666104793548584
Epoch 340, val loss: 0.945094108581543
Epoch 350, training loss: 0.9552417993545532 = 0.29018184542655945 + 0.1 * 6.650599479675293
Epoch 350, val loss: 0.9554013609886169
Epoch 360, training loss: 0.9361664652824402 = 0.2695963382720947 + 0.1 * 6.665700912475586
Epoch 360, val loss: 0.967384934425354
Epoch 370, training loss: 0.9142839908599854 = 0.2501613199710846 + 0.1 * 6.641226291656494
Epoch 370, val loss: 0.9805818796157837
Epoch 380, training loss: 0.8976179361343384 = 0.23175926506519318 + 0.1 * 6.658586502075195
Epoch 380, val loss: 0.994763195514679
Epoch 390, training loss: 0.8777605295181274 = 0.2144051045179367 + 0.1 * 6.633553981781006
Epoch 390, val loss: 1.0098779201507568
Epoch 400, training loss: 0.8611563444137573 = 0.19801589846611023 + 0.1 * 6.631404399871826
Epoch 400, val loss: 1.025863766670227
Epoch 410, training loss: 0.8466054797172546 = 0.18264912068843842 + 0.1 * 6.639563083648682
Epoch 410, val loss: 1.0426580905914307
Epoch 420, training loss: 0.8302914500236511 = 0.16836027801036835 + 0.1 * 6.619311332702637
Epoch 420, val loss: 1.0601036548614502
Epoch 430, training loss: 0.8164325952529907 = 0.1551056206226349 + 0.1 * 6.613269805908203
Epoch 430, val loss: 1.078403353691101
Epoch 440, training loss: 0.8037159442901611 = 0.1428731232881546 + 0.1 * 6.608428478240967
Epoch 440, val loss: 1.0973761081695557
Epoch 450, training loss: 0.7914206385612488 = 0.1316349357366562 + 0.1 * 6.597856521606445
Epoch 450, val loss: 1.1169850826263428
Epoch 460, training loss: 0.7813480496406555 = 0.1213163286447525 + 0.1 * 6.600317478179932
Epoch 460, val loss: 1.137171983718872
Epoch 470, training loss: 0.7719525694847107 = 0.11191223561763763 + 0.1 * 6.60040283203125
Epoch 470, val loss: 1.157702088356018
Epoch 480, training loss: 0.7619515061378479 = 0.10338065773248672 + 0.1 * 6.585708141326904
Epoch 480, val loss: 1.1785074472427368
Epoch 490, training loss: 0.7537109851837158 = 0.09561347961425781 + 0.1 * 6.58097505569458
Epoch 490, val loss: 1.1995691061019897
Epoch 500, training loss: 0.7466289401054382 = 0.08854945749044418 + 0.1 * 6.580794811248779
Epoch 500, val loss: 1.2207200527191162
Epoch 510, training loss: 0.7397313714027405 = 0.08214256912469864 + 0.1 * 6.575887680053711
Epoch 510, val loss: 1.2417923212051392
Epoch 520, training loss: 0.7341527342796326 = 0.07632208615541458 + 0.1 * 6.578306674957275
Epoch 520, val loss: 1.2627997398376465
Epoch 530, training loss: 0.7276600003242493 = 0.07102300971746445 + 0.1 * 6.566370010375977
Epoch 530, val loss: 1.283681035041809
Epoch 540, training loss: 0.7227495312690735 = 0.06619612127542496 + 0.1 * 6.565533638000488
Epoch 540, val loss: 1.3042562007904053
Epoch 550, training loss: 0.7179439067840576 = 0.0618058443069458 + 0.1 * 6.561380386352539
Epoch 550, val loss: 1.3245471715927124
Epoch 560, training loss: 0.7143051028251648 = 0.05779557302594185 + 0.1 * 6.565094947814941
Epoch 560, val loss: 1.3445045948028564
Epoch 570, training loss: 0.7096807956695557 = 0.054133981466293335 + 0.1 * 6.55546760559082
Epoch 570, val loss: 1.364109992980957
Epoch 580, training loss: 0.7054523229598999 = 0.050783585757017136 + 0.1 * 6.546687126159668
Epoch 580, val loss: 1.383401870727539
Epoch 590, training loss: 0.701829195022583 = 0.047708865255117416 + 0.1 * 6.541203022003174
Epoch 590, val loss: 1.4023884534835815
Epoch 600, training loss: 0.6994966268539429 = 0.044885434210300446 + 0.1 * 6.546111583709717
Epoch 600, val loss: 1.4209452867507935
Epoch 610, training loss: 0.6967265605926514 = 0.04229727014899254 + 0.1 * 6.54429292678833
Epoch 610, val loss: 1.43915855884552
Epoch 620, training loss: 0.6938546895980835 = 0.03991333022713661 + 0.1 * 6.5394134521484375
Epoch 620, val loss: 1.4570306539535522
Epoch 630, training loss: 0.6918124556541443 = 0.03771400451660156 + 0.1 * 6.540984630584717
Epoch 630, val loss: 1.4745780229568481
Epoch 640, training loss: 0.6894044876098633 = 0.03568776696920395 + 0.1 * 6.537167072296143
Epoch 640, val loss: 1.491645336151123
Epoch 650, training loss: 0.6869564056396484 = 0.03381923586130142 + 0.1 * 6.531371116638184
Epoch 650, val loss: 1.5083746910095215
Epoch 660, training loss: 0.6840542554855347 = 0.03208838030695915 + 0.1 * 6.51965856552124
Epoch 660, val loss: 1.5247917175292969
Epoch 670, training loss: 0.6834330558776855 = 0.030481835827231407 + 0.1 * 6.529512405395508
Epoch 670, val loss: 1.5408062934875488
Epoch 680, training loss: 0.6824558973312378 = 0.028991641476750374 + 0.1 * 6.534642696380615
Epoch 680, val loss: 1.5565162897109985
Epoch 690, training loss: 0.6791723370552063 = 0.027608271688222885 + 0.1 * 6.515640735626221
Epoch 690, val loss: 1.5718191862106323
Epoch 700, training loss: 0.6780717968940735 = 0.02631870098412037 + 0.1 * 6.51753044128418
Epoch 700, val loss: 1.5867937803268433
Epoch 710, training loss: 0.676612913608551 = 0.025117525830864906 + 0.1 * 6.514954090118408
Epoch 710, val loss: 1.6014013290405273
Epoch 720, training loss: 0.6757258176803589 = 0.023997345939278603 + 0.1 * 6.517284393310547
Epoch 720, val loss: 1.6157118082046509
Epoch 730, training loss: 0.6744130849838257 = 0.022950993850827217 + 0.1 * 6.514620780944824
Epoch 730, val loss: 1.6296898126602173
Epoch 740, training loss: 0.6729551553726196 = 0.021973783150315285 + 0.1 * 6.50981330871582
Epoch 740, val loss: 1.6433733701705933
Epoch 750, training loss: 0.6713541746139526 = 0.021058356389403343 + 0.1 * 6.502957820892334
Epoch 750, val loss: 1.6566861867904663
Epoch 760, training loss: 0.6713736057281494 = 0.020201031118631363 + 0.1 * 6.511725902557373
Epoch 760, val loss: 1.6697874069213867
Epoch 770, training loss: 0.6688002943992615 = 0.01939535327255726 + 0.1 * 6.494049549102783
Epoch 770, val loss: 1.682557463645935
Epoch 780, training loss: 0.667941153049469 = 0.018639056012034416 + 0.1 * 6.493021011352539
Epoch 780, val loss: 1.6950578689575195
Epoch 790, training loss: 0.6668325066566467 = 0.017926353961229324 + 0.1 * 6.48906135559082
Epoch 790, val loss: 1.7072123289108276
Epoch 800, training loss: 0.6673473715782166 = 0.017256489023566246 + 0.1 * 6.500908851623535
Epoch 800, val loss: 1.719140648841858
Epoch 810, training loss: 0.6652036905288696 = 0.016624856740236282 + 0.1 * 6.485788345336914
Epoch 810, val loss: 1.730843186378479
Epoch 820, training loss: 0.6644560098648071 = 0.01602902263402939 + 0.1 * 6.484270095825195
Epoch 820, val loss: 1.74223792552948
Epoch 830, training loss: 0.6644765138626099 = 0.015465292148292065 + 0.1 * 6.490111827850342
Epoch 830, val loss: 1.7534674406051636
Epoch 840, training loss: 0.6629832983016968 = 0.014932265505194664 + 0.1 * 6.480510234832764
Epoch 840, val loss: 1.7644028663635254
Epoch 850, training loss: 0.6624134182929993 = 0.014428164809942245 + 0.1 * 6.479852676391602
Epoch 850, val loss: 1.7752091884613037
Epoch 860, training loss: 0.6625908613204956 = 0.013949962332844734 + 0.1 * 6.486408710479736
Epoch 860, val loss: 1.785702109336853
Epoch 870, training loss: 0.6625268459320068 = 0.013496349565684795 + 0.1 * 6.490304946899414
Epoch 870, val loss: 1.7959927320480347
Epoch 880, training loss: 0.6602293252944946 = 0.013066545128822327 + 0.1 * 6.471627712249756
Epoch 880, val loss: 1.8061074018478394
Epoch 890, training loss: 0.6600755453109741 = 0.012658202089369297 + 0.1 * 6.474173545837402
Epoch 890, val loss: 1.816037893295288
Epoch 900, training loss: 0.6603890061378479 = 0.012269342318177223 + 0.1 * 6.481196880340576
Epoch 900, val loss: 1.825734257698059
Epoch 910, training loss: 0.6605460047721863 = 0.011899255216121674 + 0.1 * 6.486467361450195
Epoch 910, val loss: 1.835268259048462
Epoch 920, training loss: 0.6592175960540771 = 0.011547219939529896 + 0.1 * 6.476703643798828
Epoch 920, val loss: 1.8445875644683838
Epoch 930, training loss: 0.6574814319610596 = 0.01121219340711832 + 0.1 * 6.4626922607421875
Epoch 930, val loss: 1.8538485765457153
Epoch 940, training loss: 0.6584067344665527 = 0.010891749523580074 + 0.1 * 6.475150108337402
Epoch 940, val loss: 1.8628071546554565
Epoch 950, training loss: 0.6572623252868652 = 0.010585570707917213 + 0.1 * 6.46676778793335
Epoch 950, val loss: 1.8716599941253662
Epoch 960, training loss: 0.6587302684783936 = 0.010294152423739433 + 0.1 * 6.484361171722412
Epoch 960, val loss: 1.8802366256713867
Epoch 970, training loss: 0.6566886305809021 = 0.010016022250056267 + 0.1 * 6.466725826263428
Epoch 970, val loss: 1.888843059539795
Epoch 980, training loss: 0.6552361845970154 = 0.009749997407197952 + 0.1 * 6.454861640930176
Epoch 980, val loss: 1.897104024887085
Epoch 990, training loss: 0.6556019186973572 = 0.009495033882558346 + 0.1 * 6.461068630218506
Epoch 990, val loss: 1.9053428173065186
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8128624143384291
The final CL Acc:0.72222, 0.01210, The final GNN Acc:0.81409, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 7884])
updated graph: torch.Size([2, 10480])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.804220676422119 = 1.9445414543151855 + 0.1 * 8.596792221069336
Epoch 0, val loss: 1.946823000907898
Epoch 10, training loss: 2.794442653656006 = 1.9347752332687378 + 0.1 * 8.596673965454102
Epoch 10, val loss: 1.9366846084594727
Epoch 20, training loss: 2.782341718673706 = 1.9227474927902222 + 0.1 * 8.595942497253418
Epoch 20, val loss: 1.924132227897644
Epoch 30, training loss: 2.7647902965545654 = 1.9058219194412231 + 0.1 * 8.58968448638916
Epoch 30, val loss: 1.9062925577163696
Epoch 40, training loss: 2.735045909881592 = 1.8808057308197021 + 0.1 * 8.542401313781738
Epoch 40, val loss: 1.8798811435699463
Epoch 50, training loss: 2.6694681644439697 = 1.846724271774292 + 0.1 * 8.227437973022461
Epoch 50, val loss: 1.8452447652816772
Epoch 60, training loss: 2.593869209289551 = 1.8088595867156982 + 0.1 * 7.850095272064209
Epoch 60, val loss: 1.808403730392456
Epoch 70, training loss: 2.518641233444214 = 1.7712209224700928 + 0.1 * 7.474203109741211
Epoch 70, val loss: 1.7740062475204468
Epoch 80, training loss: 2.4584028720855713 = 1.7324906587600708 + 0.1 * 7.259121417999268
Epoch 80, val loss: 1.7407894134521484
Epoch 90, training loss: 2.4001665115356445 = 1.6857366561889648 + 0.1 * 7.144299030303955
Epoch 90, val loss: 1.6991761922836304
Epoch 100, training loss: 2.33162784576416 = 1.6243714094161987 + 0.1 * 7.072564125061035
Epoch 100, val loss: 1.6435565948486328
Epoch 110, training loss: 2.2470626831054688 = 1.5453506708145142 + 0.1 * 7.017119884490967
Epoch 110, val loss: 1.5751936435699463
Epoch 120, training loss: 2.147416591644287 = 1.449690341949463 + 0.1 * 6.977262020111084
Epoch 120, val loss: 1.4927116632461548
Epoch 130, training loss: 2.0379140377044678 = 1.3436661958694458 + 0.1 * 6.942477703094482
Epoch 130, val loss: 1.4032953977584839
Epoch 140, training loss: 1.9253413677215576 = 1.234113335609436 + 0.1 * 6.912281036376953
Epoch 140, val loss: 1.3124274015426636
Epoch 150, training loss: 1.8185265064239502 = 1.1294116973876953 + 0.1 * 6.891148567199707
Epoch 150, val loss: 1.2270478010177612
Epoch 160, training loss: 1.718651294708252 = 1.031802773475647 + 0.1 * 6.868484973907471
Epoch 160, val loss: 1.1486985683441162
Epoch 170, training loss: 1.627697229385376 = 0.9421616792678833 + 0.1 * 6.8553547859191895
Epoch 170, val loss: 1.0781488418579102
Epoch 180, training loss: 1.5449743270874023 = 0.8613359928131104 + 0.1 * 6.8363823890686035
Epoch 180, val loss: 1.015383243560791
Epoch 190, training loss: 1.4729697704315186 = 0.7894220948219299 + 0.1 * 6.835475921630859
Epoch 190, val loss: 0.9607312083244324
Epoch 200, training loss: 1.4082562923431396 = 0.7276701331138611 + 0.1 * 6.805860996246338
Epoch 200, val loss: 0.9157441854476929
Epoch 210, training loss: 1.3520166873931885 = 0.6733868718147278 + 0.1 * 6.786297798156738
Epoch 210, val loss: 0.878399670124054
Epoch 220, training loss: 1.3026642799377441 = 0.6245700716972351 + 0.1 * 6.780941963195801
Epoch 220, val loss: 0.8477805256843567
Epoch 230, training loss: 1.256338357925415 = 0.5805109143257141 + 0.1 * 6.758274078369141
Epoch 230, val loss: 0.823017954826355
Epoch 240, training loss: 1.2136510610580444 = 0.5393430590629578 + 0.1 * 6.743080139160156
Epoch 240, val loss: 0.8023517727851868
Epoch 250, training loss: 1.1735444068908691 = 0.5003483891487122 + 0.1 * 6.731959342956543
Epoch 250, val loss: 0.784825325012207
Epoch 260, training loss: 1.1358532905578613 = 0.4637327790260315 + 0.1 * 6.721205711364746
Epoch 260, val loss: 0.7700735330581665
Epoch 270, training loss: 1.1010689735412598 = 0.4294936954975128 + 0.1 * 6.715753078460693
Epoch 270, val loss: 0.7578276991844177
Epoch 280, training loss: 1.0704551935195923 = 0.39750581979751587 + 0.1 * 6.729493618011475
Epoch 280, val loss: 0.7480527758598328
Epoch 290, training loss: 1.0386841297149658 = 0.36816421151161194 + 0.1 * 6.705199241638184
Epoch 290, val loss: 0.7407211065292358
Epoch 300, training loss: 1.0107989311218262 = 0.34113001823425293 + 0.1 * 6.696688652038574
Epoch 300, val loss: 0.7356935739517212
Epoch 310, training loss: 0.985611081123352 = 0.31595879793167114 + 0.1 * 6.6965227127075195
Epoch 310, val loss: 0.7325181365013123
Epoch 320, training loss: 0.9610066413879395 = 0.29234299063682556 + 0.1 * 6.686635971069336
Epoch 320, val loss: 0.7308633923530579
Epoch 330, training loss: 0.9374906420707703 = 0.2698040008544922 + 0.1 * 6.67686653137207
Epoch 330, val loss: 0.7304115295410156
Epoch 340, training loss: 0.9158622622489929 = 0.2479429394006729 + 0.1 * 6.679193496704102
Epoch 340, val loss: 0.7308139801025391
Epoch 350, training loss: 0.8949373960494995 = 0.22662818431854248 + 0.1 * 6.68309211730957
Epoch 350, val loss: 0.7318245768547058
Epoch 360, training loss: 0.87254399061203 = 0.20590825378894806 + 0.1 * 6.666357517242432
Epoch 360, val loss: 0.7333728075027466
Epoch 370, training loss: 0.8521968126296997 = 0.18598471581935883 + 0.1 * 6.662120819091797
Epoch 370, val loss: 0.735698401927948
Epoch 380, training loss: 0.8329480290412903 = 0.16729986667633057 + 0.1 * 6.6564812660217285
Epoch 380, val loss: 0.738825261592865
Epoch 390, training loss: 0.8161516189575195 = 0.15021094679832458 + 0.1 * 6.6594061851501465
Epoch 390, val loss: 0.7428659796714783
Epoch 400, training loss: 0.7992842197418213 = 0.1348666250705719 + 0.1 * 6.6441755294799805
Epoch 400, val loss: 0.7476944327354431
Epoch 410, training loss: 0.7855662107467651 = 0.12114077806472778 + 0.1 * 6.644254207611084
Epoch 410, val loss: 0.7533503770828247
Epoch 420, training loss: 0.7722371220588684 = 0.10894990712404251 + 0.1 * 6.632872104644775
Epoch 420, val loss: 0.7597029805183411
Epoch 430, training loss: 0.7622223496437073 = 0.09816388785839081 + 0.1 * 6.640584468841553
Epoch 430, val loss: 0.766680121421814
Epoch 440, training loss: 0.7518372535705566 = 0.0886441022157669 + 0.1 * 6.631931781768799
Epoch 440, val loss: 0.7740271091461182
Epoch 450, training loss: 0.7421063184738159 = 0.08024735003709793 + 0.1 * 6.618589401245117
Epoch 450, val loss: 0.7817049622535706
Epoch 460, training loss: 0.7336578965187073 = 0.07283689826726913 + 0.1 * 6.60821008682251
Epoch 460, val loss: 0.789649248123169
Epoch 470, training loss: 0.730182945728302 = 0.0662764236330986 + 0.1 * 6.639064788818359
Epoch 470, val loss: 0.7977509498596191
Epoch 480, training loss: 0.7203933000564575 = 0.06047923117876053 + 0.1 * 6.599140644073486
Epoch 480, val loss: 0.8058173060417175
Epoch 490, training loss: 0.7146579027175903 = 0.05532238259911537 + 0.1 * 6.59335470199585
Epoch 490, val loss: 0.8139277696609497
Epoch 500, training loss: 0.710152268409729 = 0.050716105848550797 + 0.1 * 6.594361305236816
Epoch 500, val loss: 0.8219904899597168
Epoch 510, training loss: 0.7057347893714905 = 0.04659869894385338 + 0.1 * 6.591361045837402
Epoch 510, val loss: 0.8299981951713562
Epoch 520, training loss: 0.7010937929153442 = 0.042912423610687256 + 0.1 * 6.581813335418701
Epoch 520, val loss: 0.8379368185997009
Epoch 530, training loss: 0.6978632807731628 = 0.03961138799786568 + 0.1 * 6.582518577575684
Epoch 530, val loss: 0.8457112908363342
Epoch 540, training loss: 0.6939436793327332 = 0.03665373474359512 + 0.1 * 6.572898864746094
Epoch 540, val loss: 0.8533255457878113
Epoch 550, training loss: 0.6908686757087708 = 0.03399273008108139 + 0.1 * 6.568759441375732
Epoch 550, val loss: 0.8608242869377136
Epoch 560, training loss: 0.6883212924003601 = 0.03159676119685173 + 0.1 * 6.567245006561279
Epoch 560, val loss: 0.8681550621986389
Epoch 570, training loss: 0.6857414841651917 = 0.029437275603413582 + 0.1 * 6.563041687011719
Epoch 570, val loss: 0.8753197193145752
Epoch 580, training loss: 0.6824297904968262 = 0.027485178783535957 + 0.1 * 6.549446105957031
Epoch 580, val loss: 0.8823984861373901
Epoch 590, training loss: 0.6814450025558472 = 0.025713428854942322 + 0.1 * 6.557315349578857
Epoch 590, val loss: 0.8892645239830017
Epoch 600, training loss: 0.6789603233337402 = 0.02410944178700447 + 0.1 * 6.548509120941162
Epoch 600, val loss: 0.8959444761276245
Epoch 610, training loss: 0.6757427453994751 = 0.02265314944088459 + 0.1 * 6.530895709991455
Epoch 610, val loss: 0.9025342464447021
Epoch 620, training loss: 0.6778560876846313 = 0.021323369815945625 + 0.1 * 6.565327167510986
Epoch 620, val loss: 0.9089206457138062
Epoch 630, training loss: 0.6741747260093689 = 0.02011296898126602 + 0.1 * 6.5406174659729
Epoch 630, val loss: 0.9151090979576111
Epoch 640, training loss: 0.6715670228004456 = 0.01900501549243927 + 0.1 * 6.525619983673096
Epoch 640, val loss: 0.9212079644203186
Epoch 650, training loss: 0.6705448627471924 = 0.01798921450972557 + 0.1 * 6.525556564331055
Epoch 650, val loss: 0.9271093010902405
Epoch 660, training loss: 0.6698839664459229 = 0.01705631986260414 + 0.1 * 6.528275966644287
Epoch 660, val loss: 0.9328909516334534
Epoch 670, training loss: 0.6686034798622131 = 0.01619759015738964 + 0.1 * 6.524059295654297
Epoch 670, val loss: 0.9384777545928955
Epoch 680, training loss: 0.6669798493385315 = 0.015405929647386074 + 0.1 * 6.5157389640808105
Epoch 680, val loss: 0.9439770579338074
Epoch 690, training loss: 0.6664525270462036 = 0.014674358069896698 + 0.1 * 6.5177812576293945
Epoch 690, val loss: 0.94929438829422
Epoch 700, training loss: 0.6668489575386047 = 0.013997891917824745 + 0.1 * 6.528510570526123
Epoch 700, val loss: 0.9544774889945984
Epoch 710, training loss: 0.6644535064697266 = 0.01337333582341671 + 0.1 * 6.510801792144775
Epoch 710, val loss: 0.9595494270324707
Epoch 720, training loss: 0.6623860597610474 = 0.012791980057954788 + 0.1 * 6.495940685272217
Epoch 720, val loss: 0.9645220637321472
Epoch 730, training loss: 0.6625784635543823 = 0.012249293737113476 + 0.1 * 6.503291606903076
Epoch 730, val loss: 0.9693501591682434
Epoch 740, training loss: 0.661620020866394 = 0.01174375880509615 + 0.1 * 6.498762130737305
Epoch 740, val loss: 0.9740209579467773
Epoch 750, training loss: 0.6617745757102966 = 0.011272726580500603 + 0.1 * 6.50501823425293
Epoch 750, val loss: 0.9786428809165955
Epoch 760, training loss: 0.6600805521011353 = 0.010832512751221657 + 0.1 * 6.492480278015137
Epoch 760, val loss: 0.983161211013794
Epoch 770, training loss: 0.6592647433280945 = 0.010419574566185474 + 0.1 * 6.488451957702637
Epoch 770, val loss: 0.9875503182411194
Epoch 780, training loss: 0.6589381098747253 = 0.010031692683696747 + 0.1 * 6.4890642166137695
Epoch 780, val loss: 0.9918486475944519
Epoch 790, training loss: 0.6591355800628662 = 0.009667712263762951 + 0.1 * 6.494678497314453
Epoch 790, val loss: 0.9960123896598816
Epoch 800, training loss: 0.6573412418365479 = 0.009326099418103695 + 0.1 * 6.480151176452637
Epoch 800, val loss: 1.000114917755127
Epoch 810, training loss: 0.6577190160751343 = 0.009004437364637852 + 0.1 * 6.487145900726318
Epoch 810, val loss: 1.0041821002960205
Epoch 820, training loss: 0.6558034420013428 = 0.008700678125023842 + 0.1 * 6.471027374267578
Epoch 820, val loss: 1.008062720298767
Epoch 830, training loss: 0.6564041376113892 = 0.008413819596171379 + 0.1 * 6.479903221130371
Epoch 830, val loss: 1.0119404792785645
Epoch 840, training loss: 0.6559412479400635 = 0.008142183534801006 + 0.1 * 6.477990627288818
Epoch 840, val loss: 1.01570725440979
Epoch 850, training loss: 0.6553111672401428 = 0.007885819301009178 + 0.1 * 6.474253177642822
Epoch 850, val loss: 1.0193859338760376
Epoch 860, training loss: 0.6543838381767273 = 0.007643337361514568 + 0.1 * 6.467404842376709
Epoch 860, val loss: 1.0229917764663696
Epoch 870, training loss: 0.654270589351654 = 0.00741328252479434 + 0.1 * 6.468573093414307
Epoch 870, val loss: 1.0265644788742065
Epoch 880, training loss: 0.6552327275276184 = 0.007194614503532648 + 0.1 * 6.480380535125732
Epoch 880, val loss: 1.0300078392028809
Epoch 890, training loss: 0.6535202264785767 = 0.006986535154283047 + 0.1 * 6.465336322784424
Epoch 890, val loss: 1.033303141593933
Epoch 900, training loss: 0.6531847715377808 = 0.00678957998752594 + 0.1 * 6.46395206451416
Epoch 900, val loss: 1.0366556644439697
Epoch 910, training loss: 0.6521100997924805 = 0.006601711735129356 + 0.1 * 6.455083847045898
Epoch 910, val loss: 1.0398772954940796
Epoch 920, training loss: 0.652113676071167 = 0.006422331556677818 + 0.1 * 6.456913471221924
Epoch 920, val loss: 1.0430209636688232
Epoch 930, training loss: 0.6522643566131592 = 0.006251626648008823 + 0.1 * 6.460127353668213
Epoch 930, val loss: 1.0461022853851318
Epoch 940, training loss: 0.6514295339584351 = 0.0060887448489665985 + 0.1 * 6.4534077644348145
Epoch 940, val loss: 1.049195647239685
Epoch 950, training loss: 0.650407075881958 = 0.005932624917477369 + 0.1 * 6.44474458694458
Epoch 950, val loss: 1.0521382093429565
Epoch 960, training loss: 0.6507412195205688 = 0.005783984437584877 + 0.1 * 6.449572563171387
Epoch 960, val loss: 1.0550882816314697
Epoch 970, training loss: 0.6507936716079712 = 0.005641670897603035 + 0.1 * 6.45151948928833
Epoch 970, val loss: 1.0579665899276733
Epoch 980, training loss: 0.6511124968528748 = 0.005505574867129326 + 0.1 * 6.456068992614746
Epoch 980, val loss: 1.0607867240905762
Epoch 990, training loss: 0.649711549282074 = 0.005375342909246683 + 0.1 * 6.443362236022949
Epoch 990, val loss: 1.063604474067688
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 2.8001749515533447 = 1.9404973983764648 + 0.1 * 8.596776008605957
Epoch 0, val loss: 1.9379010200500488
Epoch 10, training loss: 2.7901694774627686 = 1.9305068254470825 + 0.1 * 8.596626281738281
Epoch 10, val loss: 1.9282658100128174
Epoch 20, training loss: 2.777519464492798 = 1.9179409742355347 + 0.1 * 8.595784187316895
Epoch 20, val loss: 1.9155964851379395
Epoch 30, training loss: 2.759012222290039 = 1.9000906944274902 + 0.1 * 8.589214324951172
Epoch 30, val loss: 1.89716637134552
Epoch 40, training loss: 2.728898048400879 = 1.873995065689087 + 0.1 * 8.549028396606445
Epoch 40, val loss: 1.870374321937561
Epoch 50, training loss: 2.6722259521484375 = 1.8393430709838867 + 0.1 * 8.328828811645508
Epoch 50, val loss: 1.8368029594421387
Epoch 60, training loss: 2.603994369506836 = 1.8002849817276 + 0.1 * 8.037095069885254
Epoch 60, val loss: 1.8010305166244507
Epoch 70, training loss: 2.5294837951660156 = 1.7596416473388672 + 0.1 * 7.698421001434326
Epoch 70, val loss: 1.7655773162841797
Epoch 80, training loss: 2.455962657928467 = 1.7151721715927124 + 0.1 * 7.4079060554504395
Epoch 80, val loss: 1.7280339002609253
Epoch 90, training loss: 2.387099266052246 = 1.6589338779449463 + 0.1 * 7.281652450561523
Epoch 90, val loss: 1.678528904914856
Epoch 100, training loss: 2.3033535480499268 = 1.5846788883209229 + 0.1 * 7.186747074127197
Epoch 100, val loss: 1.611952781677246
Epoch 110, training loss: 2.206184148788452 = 1.494634985923767 + 0.1 * 7.115492343902588
Epoch 110, val loss: 1.5350383520126343
Epoch 120, training loss: 2.10184383392334 = 1.3945250511169434 + 0.1 * 7.07318639755249
Epoch 120, val loss: 1.4509528875350952
Epoch 130, training loss: 1.9946720600128174 = 1.2910151481628418 + 0.1 * 7.036569595336914
Epoch 130, val loss: 1.366308569908142
Epoch 140, training loss: 1.8874480724334717 = 1.1880972385406494 + 0.1 * 6.993508815765381
Epoch 140, val loss: 1.2840824127197266
Epoch 150, training loss: 1.7842435836791992 = 1.0890617370605469 + 0.1 * 6.951817989349365
Epoch 150, val loss: 1.2065285444259644
Epoch 160, training loss: 1.6905109882354736 = 0.9983366131782532 + 0.1 * 6.921742916107178
Epoch 160, val loss: 1.1371819972991943
Epoch 170, training loss: 1.6042027473449707 = 0.9152439832687378 + 0.1 * 6.889586925506592
Epoch 170, val loss: 1.0739471912384033
Epoch 180, training loss: 1.5263656377792358 = 0.839958131313324 + 0.1 * 6.86407470703125
Epoch 180, val loss: 1.0166802406311035
Epoch 190, training loss: 1.456524133682251 = 0.7724398374557495 + 0.1 * 6.840843200683594
Epoch 190, val loss: 0.9655340313911438
Epoch 200, training loss: 1.3943967819213867 = 0.7121320962905884 + 0.1 * 6.822646141052246
Epoch 200, val loss: 0.9208834767341614
Epoch 210, training loss: 1.3390976190567017 = 0.6579843759536743 + 0.1 * 6.811132431030273
Epoch 210, val loss: 0.8824707865715027
Epoch 220, training loss: 1.2874491214752197 = 0.608977735042572 + 0.1 * 6.784714221954346
Epoch 220, val loss: 0.8502939939498901
Epoch 230, training loss: 1.2401046752929688 = 0.5634655356407166 + 0.1 * 6.766390800476074
Epoch 230, val loss: 0.823157548904419
Epoch 240, training loss: 1.1970932483673096 = 0.5212072134017944 + 0.1 * 6.758859634399414
Epoch 240, val loss: 0.8011282086372375
Epoch 250, training loss: 1.1571950912475586 = 0.48299363255500793 + 0.1 * 6.742014408111572
Epoch 250, val loss: 0.7842825651168823
Epoch 260, training loss: 1.1230523586273193 = 0.44823944568634033 + 0.1 * 6.748129844665527
Epoch 260, val loss: 0.7716039419174194
Epoch 270, training loss: 1.0898795127868652 = 0.4169829487800598 + 0.1 * 6.728964805603027
Epoch 270, val loss: 0.7629316449165344
Epoch 280, training loss: 1.060815453529358 = 0.3885912001132965 + 0.1 * 6.722242832183838
Epoch 280, val loss: 0.7580896019935608
Epoch 290, training loss: 1.0334287881851196 = 0.3626774847507477 + 0.1 * 6.707512855529785
Epoch 290, val loss: 0.7567046880722046
Epoch 300, training loss: 1.0086135864257812 = 0.33863013982772827 + 0.1 * 6.69983434677124
Epoch 300, val loss: 0.758051872253418
Epoch 310, training loss: 0.9855049848556519 = 0.3158782422542572 + 0.1 * 6.696267127990723
Epoch 310, val loss: 0.7615235447883606
Epoch 320, training loss: 0.9637004137039185 = 0.293987900018692 + 0.1 * 6.69712495803833
Epoch 320, val loss: 0.7664334177970886
Epoch 330, training loss: 0.940395712852478 = 0.2724303603172302 + 0.1 * 6.679653644561768
Epoch 330, val loss: 0.7724171876907349
Epoch 340, training loss: 0.9205760955810547 = 0.2507874071598053 + 0.1 * 6.6978864669799805
Epoch 340, val loss: 0.778986394405365
Epoch 350, training loss: 0.8970789909362793 = 0.2293035387992859 + 0.1 * 6.6777544021606445
Epoch 350, val loss: 0.7857305407524109
Epoch 360, training loss: 0.8752169013023376 = 0.20844703912734985 + 0.1 * 6.667698383331299
Epoch 360, val loss: 0.7928406596183777
Epoch 370, training loss: 0.8549838662147522 = 0.1888902634382248 + 0.1 * 6.660935878753662
Epoch 370, val loss: 0.8007254004478455
Epoch 380, training loss: 0.8373078107833862 = 0.1711992472410202 + 0.1 * 6.66108512878418
Epoch 380, val loss: 0.8095783591270447
Epoch 390, training loss: 0.8206297755241394 = 0.1555279642343521 + 0.1 * 6.651018142700195
Epoch 390, val loss: 0.8196463584899902
Epoch 400, training loss: 0.8061837553977966 = 0.14165432751178741 + 0.1 * 6.645294189453125
Epoch 400, val loss: 0.8308655619621277
Epoch 410, training loss: 0.7935855984687805 = 0.1293775588274002 + 0.1 * 6.642080307006836
Epoch 410, val loss: 0.8430505990982056
Epoch 420, training loss: 0.7820113897323608 = 0.11845535039901733 + 0.1 * 6.635560512542725
Epoch 420, val loss: 0.856057345867157
Epoch 430, training loss: 0.7729072570800781 = 0.10869909822940826 + 0.1 * 6.6420817375183105
Epoch 430, val loss: 0.8695966005325317
Epoch 440, training loss: 0.7628721594810486 = 0.09998402744531631 + 0.1 * 6.628880977630615
Epoch 440, val loss: 0.8835607171058655
Epoch 450, training loss: 0.7552562952041626 = 0.0921408012509346 + 0.1 * 6.631155014038086
Epoch 450, val loss: 0.8977857232093811
Epoch 460, training loss: 0.7472633719444275 = 0.08507824689149857 + 0.1 * 6.621851444244385
Epoch 460, val loss: 0.9118709564208984
Epoch 470, training loss: 0.7400404810905457 = 0.07868405431509018 + 0.1 * 6.613564491271973
Epoch 470, val loss: 0.9260737299919128
Epoch 480, training loss: 0.7334771156311035 = 0.07289351522922516 + 0.1 * 6.605835914611816
Epoch 480, val loss: 0.9401962161064148
Epoch 490, training loss: 0.7280937433242798 = 0.06764215230941772 + 0.1 * 6.60451602935791
Epoch 490, val loss: 0.9541242718696594
Epoch 500, training loss: 0.7228255271911621 = 0.06289215385913849 + 0.1 * 6.599333763122559
Epoch 500, val loss: 0.9678324460983276
Epoch 510, training loss: 0.7175790667533875 = 0.05858253687620163 + 0.1 * 6.589964866638184
Epoch 510, val loss: 0.9813835024833679
Epoch 520, training loss: 0.7129814624786377 = 0.05466265603899956 + 0.1 * 6.583188056945801
Epoch 520, val loss: 0.9947364926338196
Epoch 530, training loss: 0.7096951007843018 = 0.05108815059065819 + 0.1 * 6.586069583892822
Epoch 530, val loss: 1.0079967975616455
Epoch 540, training loss: 0.7088148593902588 = 0.047829750925302505 + 0.1 * 6.609850883483887
Epoch 540, val loss: 1.0209416151046753
Epoch 550, training loss: 0.7018596529960632 = 0.044865187257528305 + 0.1 * 6.569944381713867
Epoch 550, val loss: 1.0337059497833252
Epoch 560, training loss: 0.6989052295684814 = 0.042150288820266724 + 0.1 * 6.567549228668213
Epoch 560, val loss: 1.046164870262146
Epoch 570, training loss: 0.6960564255714417 = 0.03965819627046585 + 0.1 * 6.563982009887695
Epoch 570, val loss: 1.0586017370224
Epoch 580, training loss: 0.6935444474220276 = 0.037366144359111786 + 0.1 * 6.5617828369140625
Epoch 580, val loss: 1.0706034898757935
Epoch 590, training loss: 0.6916245222091675 = 0.035258516669273376 + 0.1 * 6.56365966796875
Epoch 590, val loss: 1.0824393033981323
Epoch 600, training loss: 0.6885180473327637 = 0.03331673517823219 + 0.1 * 6.552012920379639
Epoch 600, val loss: 1.0939887762069702
Epoch 610, training loss: 0.6858214139938354 = 0.031524624675512314 + 0.1 * 6.542967796325684
Epoch 610, val loss: 1.1053471565246582
Epoch 620, training loss: 0.6870198249816895 = 0.029864273965358734 + 0.1 * 6.5715556144714355
Epoch 620, val loss: 1.1164171695709229
Epoch 630, training loss: 0.68284672498703 = 0.02833031862974167 + 0.1 * 6.545164108276367
Epoch 630, val loss: 1.127289891242981
Epoch 640, training loss: 0.6803874969482422 = 0.026904506608843803 + 0.1 * 6.534829616546631
Epoch 640, val loss: 1.1379228830337524
Epoch 650, training loss: 0.6789188385009766 = 0.025580214336514473 + 0.1 * 6.53338623046875
Epoch 650, val loss: 1.1484266519546509
Epoch 660, training loss: 0.6783559322357178 = 0.024347618222236633 + 0.1 * 6.540082931518555
Epoch 660, val loss: 1.1584856510162354
Epoch 670, training loss: 0.675458550453186 = 0.023201629519462585 + 0.1 * 6.522569179534912
Epoch 670, val loss: 1.1685473918914795
Epoch 680, training loss: 0.6761966943740845 = 0.02213156409561634 + 0.1 * 6.540651321411133
Epoch 680, val loss: 1.178292989730835
Epoch 690, training loss: 0.6739243268966675 = 0.02113598771393299 + 0.1 * 6.527883052825928
Epoch 690, val loss: 1.1878691911697388
Epoch 700, training loss: 0.6741741895675659 = 0.020201928913593292 + 0.1 * 6.539722442626953
Epoch 700, val loss: 1.197195053100586
Epoch 710, training loss: 0.6714776754379272 = 0.0193316787481308 + 0.1 * 6.521459579467773
Epoch 710, val loss: 1.2062617540359497
Epoch 720, training loss: 0.6700982451438904 = 0.018515368923544884 + 0.1 * 6.515829086303711
Epoch 720, val loss: 1.2152413129806519
Epoch 730, training loss: 0.6684035658836365 = 0.01774977520108223 + 0.1 * 6.506537437438965
Epoch 730, val loss: 1.2240124940872192
Epoch 740, training loss: 0.6679714918136597 = 0.01703011803328991 + 0.1 * 6.509413719177246
Epoch 740, val loss: 1.2326159477233887
Epoch 750, training loss: 0.6670246720314026 = 0.01635552942752838 + 0.1 * 6.5066914558410645
Epoch 750, val loss: 1.2410541772842407
Epoch 760, training loss: 0.6654540300369263 = 0.01571912318468094 + 0.1 * 6.497348785400391
Epoch 760, val loss: 1.2492716312408447
Epoch 770, training loss: 0.6645227670669556 = 0.015120701864361763 + 0.1 * 6.494020938873291
Epoch 770, val loss: 1.257412075996399
Epoch 780, training loss: 0.665010392665863 = 0.014555556699633598 + 0.1 * 6.504548072814941
Epoch 780, val loss: 1.265297532081604
Epoch 790, training loss: 0.6635878682136536 = 0.0140231903642416 + 0.1 * 6.495646953582764
Epoch 790, val loss: 1.2730621099472046
Epoch 800, training loss: 0.6639231443405151 = 0.013520155102014542 + 0.1 * 6.504029273986816
Epoch 800, val loss: 1.2806987762451172
Epoch 810, training loss: 0.6621795296669006 = 0.013045881874859333 + 0.1 * 6.491336345672607
Epoch 810, val loss: 1.2880933284759521
Epoch 820, training loss: 0.6613563895225525 = 0.012596054002642632 + 0.1 * 6.487603187561035
Epoch 820, val loss: 1.2955195903778076
Epoch 830, training loss: 0.6615810990333557 = 0.01217035111039877 + 0.1 * 6.494107246398926
Epoch 830, val loss: 1.3026294708251953
Epoch 840, training loss: 0.6607466340065002 = 0.011766906827688217 + 0.1 * 6.489797115325928
Epoch 840, val loss: 1.3097138404846191
Epoch 850, training loss: 0.6599085330963135 = 0.01138401124626398 + 0.1 * 6.485245227813721
Epoch 850, val loss: 1.3165544271469116
Epoch 860, training loss: 0.658979594707489 = 0.011020668782293797 + 0.1 * 6.479589462280273
Epoch 860, val loss: 1.3233282566070557
Epoch 870, training loss: 0.6586204767227173 = 0.010675732046365738 + 0.1 * 6.479447364807129
Epoch 870, val loss: 1.3300219774246216
Epoch 880, training loss: 0.657597541809082 = 0.010347874835133553 + 0.1 * 6.472496032714844
Epoch 880, val loss: 1.3364843130111694
Epoch 890, training loss: 0.6572142243385315 = 0.010035517625510693 + 0.1 * 6.471786975860596
Epoch 890, val loss: 1.3428982496261597
Epoch 900, training loss: 0.6573283672332764 = 0.009737547487020493 + 0.1 * 6.475908279418945
Epoch 900, val loss: 1.3491437435150146
Epoch 910, training loss: 0.6570338010787964 = 0.009454280138015747 + 0.1 * 6.475794792175293
Epoch 910, val loss: 1.355284571647644
Epoch 920, training loss: 0.655663788318634 = 0.009183865040540695 + 0.1 * 6.464798927307129
Epoch 920, val loss: 1.361327886581421
Epoch 930, training loss: 0.6559925675392151 = 0.008926313370466232 + 0.1 * 6.4706621170043945
Epoch 930, val loss: 1.367310881614685
Epoch 940, training loss: 0.6551728844642639 = 0.008679565042257309 + 0.1 * 6.464932918548584
Epoch 940, val loss: 1.3730971813201904
Epoch 950, training loss: 0.6552690267562866 = 0.008443517610430717 + 0.1 * 6.468255043029785
Epoch 950, val loss: 1.3787611722946167
Epoch 960, training loss: 0.6550992727279663 = 0.008218744769692421 + 0.1 * 6.468804836273193
Epoch 960, val loss: 1.3843729496002197
Epoch 970, training loss: 0.65350741147995 = 0.008003877475857735 + 0.1 * 6.455035209655762
Epoch 970, val loss: 1.3898845911026
Epoch 980, training loss: 0.6535374522209167 = 0.00779783446341753 + 0.1 * 6.457396030426025
Epoch 980, val loss: 1.395430326461792
Epoch 990, training loss: 0.653958261013031 = 0.007600354962050915 + 0.1 * 6.463578701019287
Epoch 990, val loss: 1.4006987810134888
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.809807777404785 = 1.9501276016235352 + 0.1 * 8.5968017578125
Epoch 0, val loss: 1.953211784362793
Epoch 10, training loss: 2.8002657890319824 = 1.9405921697616577 + 0.1 * 8.596736907958984
Epoch 10, val loss: 1.9440960884094238
Epoch 20, training loss: 2.788936138153076 = 1.9292998313903809 + 0.1 * 8.596363067626953
Epoch 20, val loss: 1.9328030347824097
Epoch 30, training loss: 2.773202657699585 = 1.9138494729995728 + 0.1 * 8.593531608581543
Epoch 30, val loss: 1.9168879985809326
Epoch 40, training loss: 2.7484817504882812 = 1.891370415687561 + 0.1 * 8.571112632751465
Epoch 40, val loss: 1.89339280128479
Epoch 50, training loss: 2.7023074626922607 = 1.8594599962234497 + 0.1 * 8.428475379943848
Epoch 50, val loss: 1.8607580661773682
Epoch 60, training loss: 2.621669292449951 = 1.821122407913208 + 0.1 * 8.00546932220459
Epoch 60, val loss: 1.823503017425537
Epoch 70, training loss: 2.545259714126587 = 1.7822291851043701 + 0.1 * 7.63030481338501
Epoch 70, val loss: 1.785563588142395
Epoch 80, training loss: 2.480377674102783 = 1.741137146949768 + 0.1 * 7.392406463623047
Epoch 80, val loss: 1.747734785079956
Epoch 90, training loss: 2.4167580604553223 = 1.6922374963760376 + 0.1 * 7.245206356048584
Epoch 90, val loss: 1.7026171684265137
Epoch 100, training loss: 2.344466209411621 = 1.627602458000183 + 0.1 * 7.168636322021484
Epoch 100, val loss: 1.6410611867904663
Epoch 110, training loss: 2.255122423171997 = 1.545072317123413 + 0.1 * 7.100500106811523
Epoch 110, val loss: 1.5648720264434814
Epoch 120, training loss: 2.1533827781677246 = 1.4480501413345337 + 0.1 * 7.053326606750488
Epoch 120, val loss: 1.4788764715194702
Epoch 130, training loss: 2.0456957817077637 = 1.3443384170532227 + 0.1 * 7.0135722160339355
Epoch 130, val loss: 1.3900991678237915
Epoch 140, training loss: 1.938201904296875 = 1.2401617765426636 + 0.1 * 6.980401039123535
Epoch 140, val loss: 1.304611325263977
Epoch 150, training loss: 1.8346343040466309 = 1.1384775638580322 + 0.1 * 6.961566925048828
Epoch 150, val loss: 1.2248342037200928
Epoch 160, training loss: 1.7358105182647705 = 1.0424374341964722 + 0.1 * 6.933731555938721
Epoch 160, val loss: 1.1514679193496704
Epoch 170, training loss: 1.6424789428710938 = 0.9511617422103882 + 0.1 * 6.913172245025635
Epoch 170, val loss: 1.0823824405670166
Epoch 180, training loss: 1.5550568103790283 = 0.8659008741378784 + 0.1 * 6.891558647155762
Epoch 180, val loss: 1.0185933113098145
Epoch 190, training loss: 1.4750840663909912 = 0.7878450155258179 + 0.1 * 6.872389793395996
Epoch 190, val loss: 0.9615135192871094
Epoch 200, training loss: 1.4037843942642212 = 0.7180859446525574 + 0.1 * 6.856984615325928
Epoch 200, val loss: 0.9121066331863403
Epoch 210, training loss: 1.3413410186767578 = 0.6566155552864075 + 0.1 * 6.847253799438477
Epoch 210, val loss: 0.8708469867706299
Epoch 220, training loss: 1.2853915691375732 = 0.6022156476974487 + 0.1 * 6.831758499145508
Epoch 220, val loss: 0.8368511199951172
Epoch 230, training loss: 1.235944390296936 = 0.5536239147186279 + 0.1 * 6.823204517364502
Epoch 230, val loss: 0.8088613152503967
Epoch 240, training loss: 1.190981149673462 = 0.5095394849777222 + 0.1 * 6.814416885375977
Epoch 240, val loss: 0.7857463955879211
Epoch 250, training loss: 1.149345874786377 = 0.4688122272491455 + 0.1 * 6.805335521697998
Epoch 250, val loss: 0.7667691707611084
Epoch 260, training loss: 1.1104114055633545 = 0.43049588799476624 + 0.1 * 6.799155235290527
Epoch 260, val loss: 0.7509264945983887
Epoch 270, training loss: 1.073684573173523 = 0.3938710391521454 + 0.1 * 6.798134803771973
Epoch 270, val loss: 0.7380639910697937
Epoch 280, training loss: 1.037306308746338 = 0.35864368081092834 + 0.1 * 6.786625862121582
Epoch 280, val loss: 0.7275659441947937
Epoch 290, training loss: 1.0047979354858398 = 0.3247470557689667 + 0.1 * 6.800508975982666
Epoch 290, val loss: 0.7194021940231323
Epoch 300, training loss: 0.9706286787986755 = 0.292902410030365 + 0.1 * 6.7772626876831055
Epoch 300, val loss: 0.7136268019676208
Epoch 310, training loss: 0.9401533603668213 = 0.2633874714374542 + 0.1 * 6.767658710479736
Epoch 310, val loss: 0.7102187871932983
Epoch 320, training loss: 0.9142110347747803 = 0.23652683198451996 + 0.1 * 6.776841640472412
Epoch 320, val loss: 0.7091078162193298
Epoch 330, training loss: 0.8882077932357788 = 0.21255634725093842 + 0.1 * 6.756514072418213
Epoch 330, val loss: 0.7101629972457886
Epoch 340, training loss: 0.8660146594047546 = 0.19123812019824982 + 0.1 * 6.74776554107666
Epoch 340, val loss: 0.7129385471343994
Epoch 350, training loss: 0.8488261699676514 = 0.1723053902387619 + 0.1 * 6.765207767486572
Epoch 350, val loss: 0.717155396938324
Epoch 360, training loss: 0.8303225040435791 = 0.15568915009498596 + 0.1 * 6.746333122253418
Epoch 360, val loss: 0.7225380539894104
Epoch 370, training loss: 0.8145487308502197 = 0.14102956652641296 + 0.1 * 6.735191345214844
Epoch 370, val loss: 0.7288619875907898
Epoch 380, training loss: 0.8002941012382507 = 0.12805746495723724 + 0.1 * 6.7223663330078125
Epoch 380, val loss: 0.7358537316322327
Epoch 390, training loss: 0.7879141569137573 = 0.11652155220508575 + 0.1 * 6.713926315307617
Epoch 390, val loss: 0.7434217929840088
Epoch 400, training loss: 0.7761377096176147 = 0.10622236132621765 + 0.1 * 6.699153900146484
Epoch 400, val loss: 0.751519501209259
Epoch 410, training loss: 0.7673490047454834 = 0.09701517969369888 + 0.1 * 6.703338146209717
Epoch 410, val loss: 0.7600159049034119
Epoch 420, training loss: 0.7584623098373413 = 0.08876963704824448 + 0.1 * 6.696926593780518
Epoch 420, val loss: 0.768706738948822
Epoch 430, training loss: 0.7499732375144958 = 0.08141510933637619 + 0.1 * 6.685581207275391
Epoch 430, val loss: 0.7775467038154602
Epoch 440, training loss: 0.7416031956672668 = 0.07483009994029999 + 0.1 * 6.667730808258057
Epoch 440, val loss: 0.786396324634552
Epoch 450, training loss: 0.7351371049880981 = 0.06888207048177719 + 0.1 * 6.662550449371338
Epoch 450, val loss: 0.795336127281189
Epoch 460, training loss: 0.7309476137161255 = 0.06349107623100281 + 0.1 * 6.674565315246582
Epoch 460, val loss: 0.8043136596679688
Epoch 470, training loss: 0.7246609330177307 = 0.05863058567047119 + 0.1 * 6.660303115844727
Epoch 470, val loss: 0.813292384147644
Epoch 480, training loss: 0.7193760275840759 = 0.05424491688609123 + 0.1 * 6.651310920715332
Epoch 480, val loss: 0.8220015168190002
Epoch 490, training loss: 0.714546263217926 = 0.05027780681848526 + 0.1 * 6.642683982849121
Epoch 490, val loss: 0.830604076385498
Epoch 500, training loss: 0.7097342014312744 = 0.04666950926184654 + 0.1 * 6.6306471824646
Epoch 500, val loss: 0.8390762805938721
Epoch 510, training loss: 0.7074574828147888 = 0.043389540165662766 + 0.1 * 6.640678882598877
Epoch 510, val loss: 0.8474196791648865
Epoch 520, training loss: 0.7027329802513123 = 0.04041556641459465 + 0.1 * 6.623173713684082
Epoch 520, val loss: 0.8554971218109131
Epoch 530, training loss: 0.7002758383750916 = 0.03770850971341133 + 0.1 * 6.625672817230225
Epoch 530, val loss: 0.8633995652198792
Epoch 540, training loss: 0.6966668963432312 = 0.0352383628487587 + 0.1 * 6.614284992218018
Epoch 540, val loss: 0.8711345791816711
Epoch 550, training loss: 0.6932262182235718 = 0.03298300504684448 + 0.1 * 6.6024322509765625
Epoch 550, val loss: 0.8786860108375549
Epoch 560, training loss: 0.6913163065910339 = 0.030924079939723015 + 0.1 * 6.603922367095947
Epoch 560, val loss: 0.8860701322555542
Epoch 570, training loss: 0.6884585618972778 = 0.029040180146694183 + 0.1 * 6.594183921813965
Epoch 570, val loss: 0.8932095766067505
Epoch 580, training loss: 0.68744295835495 = 0.027314262464642525 + 0.1 * 6.601286888122559
Epoch 580, val loss: 0.9002245664596558
Epoch 590, training loss: 0.6855977773666382 = 0.025733884423971176 + 0.1 * 6.598638534545898
Epoch 590, val loss: 0.9070186018943787
Epoch 600, training loss: 0.6821040511131287 = 0.02428530715405941 + 0.1 * 6.578186988830566
Epoch 600, val loss: 0.9136767983436584
Epoch 610, training loss: 0.6816428899765015 = 0.022954223677515984 + 0.1 * 6.586886405944824
Epoch 610, val loss: 0.9201851487159729
Epoch 620, training loss: 0.6782950758934021 = 0.0217308197170496 + 0.1 * 6.565642356872559
Epoch 620, val loss: 0.9264575242996216
Epoch 630, training loss: 0.6775608658790588 = 0.02060088701546192 + 0.1 * 6.569599151611328
Epoch 630, val loss: 0.9326100945472717
Epoch 640, training loss: 0.6762182116508484 = 0.019556976854801178 + 0.1 * 6.566612243652344
Epoch 640, val loss: 0.9386014342308044
Epoch 650, training loss: 0.674616813659668 = 0.01859322190284729 + 0.1 * 6.560235977172852
Epoch 650, val loss: 0.9444534778594971
Epoch 660, training loss: 0.6734751462936401 = 0.017699608579277992 + 0.1 * 6.557755470275879
Epoch 660, val loss: 0.950157105922699
Epoch 670, training loss: 0.6723121404647827 = 0.016870221123099327 + 0.1 * 6.554419040679932
Epoch 670, val loss: 0.9557194709777832
Epoch 680, training loss: 0.6707004904747009 = 0.01609930954873562 + 0.1 * 6.546011447906494
Epoch 680, val loss: 0.9610907435417175
Epoch 690, training loss: 0.6700044274330139 = 0.01538314949721098 + 0.1 * 6.546212673187256
Epoch 690, val loss: 0.9664083123207092
Epoch 700, training loss: 0.6690496802330017 = 0.014716221950948238 + 0.1 * 6.543334484100342
Epoch 700, val loss: 0.9714968204498291
Epoch 710, training loss: 0.6676629781723022 = 0.014093791134655476 + 0.1 * 6.535691261291504
Epoch 710, val loss: 0.9765310883522034
Epoch 720, training loss: 0.6674543023109436 = 0.013512265868484974 + 0.1 * 6.5394206047058105
Epoch 720, val loss: 0.9814258813858032
Epoch 730, training loss: 0.6662405729293823 = 0.012967790476977825 + 0.1 * 6.5327277183532715
Epoch 730, val loss: 0.9862016439437866
Epoch 740, training loss: 0.6649889349937439 = 0.012457149103283882 + 0.1 * 6.525318145751953
Epoch 740, val loss: 0.9908750057220459
Epoch 750, training loss: 0.6638911962509155 = 0.011978457681834698 + 0.1 * 6.519127368927002
Epoch 750, val loss: 0.9954188466072083
Epoch 760, training loss: 0.6628394722938538 = 0.011527837254106998 + 0.1 * 6.513116359710693
Epoch 760, val loss: 0.9999001026153564
Epoch 770, training loss: 0.663759708404541 = 0.011104237288236618 + 0.1 * 6.526554584503174
Epoch 770, val loss: 1.0042744874954224
Epoch 780, training loss: 0.6628443002700806 = 0.010705821216106415 + 0.1 * 6.5213847160339355
Epoch 780, val loss: 1.0085328817367554
Epoch 790, training loss: 0.660797655582428 = 0.010330131277441978 + 0.1 * 6.504674911499023
Epoch 790, val loss: 1.012671947479248
Epoch 800, training loss: 0.6603662371635437 = 0.0099762799218297 + 0.1 * 6.503899574279785
Epoch 800, val loss: 1.0167622566223145
Epoch 810, training loss: 0.6609861850738525 = 0.00964147225022316 + 0.1 * 6.513447284698486
Epoch 810, val loss: 1.0207945108413696
Epoch 820, training loss: 0.6600151062011719 = 0.00932373572140932 + 0.1 * 6.506913661956787
Epoch 820, val loss: 1.024704098701477
Epoch 830, training loss: 0.6590318083763123 = 0.009023732505738735 + 0.1 * 6.500080585479736
Epoch 830, val loss: 1.0285528898239136
Epoch 840, training loss: 0.658480167388916 = 0.008738905191421509 + 0.1 * 6.49741268157959
Epoch 840, val loss: 1.0323020219802856
Epoch 850, training loss: 0.6571744680404663 = 0.00846883375197649 + 0.1 * 6.487056255340576
Epoch 850, val loss: 1.0360348224639893
Epoch 860, training loss: 0.658019483089447 = 0.008211887441575527 + 0.1 * 6.498075485229492
Epoch 860, val loss: 1.0396901369094849
Epoch 870, training loss: 0.6563302874565125 = 0.007967076264321804 + 0.1 * 6.4836320877075195
Epoch 870, val loss: 1.043241262435913
Epoch 880, training loss: 0.6565124988555908 = 0.007735265418887138 + 0.1 * 6.487772464752197
Epoch 880, val loss: 1.0467197895050049
Epoch 890, training loss: 0.6555911898612976 = 0.007513960357755423 + 0.1 * 6.480772495269775
Epoch 890, val loss: 1.050141453742981
Epoch 900, training loss: 0.657222330570221 = 0.007302878890186548 + 0.1 * 6.499194145202637
Epoch 900, val loss: 1.0534800291061401
Epoch 910, training loss: 0.6553424596786499 = 0.0071016401052474976 + 0.1 * 6.482408046722412
Epoch 910, val loss: 1.0567941665649414
Epoch 920, training loss: 0.6548103094100952 = 0.006909715943038464 + 0.1 * 6.479005813598633
Epoch 920, val loss: 1.0599809885025024
Epoch 930, training loss: 0.6540935635566711 = 0.006726120598614216 + 0.1 * 6.473674297332764
Epoch 930, val loss: 1.0631619691848755
Epoch 940, training loss: 0.653678297996521 = 0.006550255697220564 + 0.1 * 6.471280097961426
Epoch 940, val loss: 1.0663046836853027
Epoch 950, training loss: 0.653107225894928 = 0.006382755469530821 + 0.1 * 6.467244625091553
Epoch 950, val loss: 1.0693439245224
Epoch 960, training loss: 0.6535422801971436 = 0.0062217311933636665 + 0.1 * 6.47320556640625
Epoch 960, val loss: 1.072359561920166
Epoch 970, training loss: 0.6530688405036926 = 0.006067329552024603 + 0.1 * 6.470015048980713
Epoch 970, val loss: 1.075324296951294
Epoch 980, training loss: 0.6520167589187622 = 0.005919862072914839 + 0.1 * 6.460968494415283
Epoch 980, val loss: 1.0782204866409302
Epoch 990, training loss: 0.6518840193748474 = 0.005778410006314516 + 0.1 * 6.461055755615234
Epoch 990, val loss: 1.0810365676879883
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8418555614127571
The final CL Acc:0.79877, 0.01666, The final GNN Acc:0.83781, 0.00305
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10546])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.789548397064209 = 1.9298661947250366 + 0.1 * 8.596820831298828
Epoch 0, val loss: 1.935075044631958
Epoch 10, training loss: 2.780358076095581 = 1.9206868410110474 + 0.1 * 8.596713066101074
Epoch 10, val loss: 1.925665020942688
Epoch 20, training loss: 2.7686851024627686 = 1.9090700149536133 + 0.1 * 8.596150398254395
Epoch 20, val loss: 1.9137078523635864
Epoch 30, training loss: 2.7516391277313232 = 1.892445683479309 + 0.1 * 8.591935157775879
Epoch 30, val loss: 1.8967201709747314
Epoch 40, training loss: 2.724752187728882 = 1.8681955337524414 + 0.1 * 8.565567016601562
Epoch 40, val loss: 1.872376561164856
Epoch 50, training loss: 2.6787970066070557 = 1.8369859457015991 + 0.1 * 8.418109893798828
Epoch 50, val loss: 1.8428242206573486
Epoch 60, training loss: 2.619595527648926 = 1.8063946962356567 + 0.1 * 8.132007598876953
Epoch 60, val loss: 1.8160948753356934
Epoch 70, training loss: 2.5426440238952637 = 1.7805355787277222 + 0.1 * 7.6210856437683105
Epoch 70, val loss: 1.7925688028335571
Epoch 80, training loss: 2.4788434505462646 = 1.7518491744995117 + 0.1 * 7.269942283630371
Epoch 80, val loss: 1.7656980752944946
Epoch 90, training loss: 2.4305927753448486 = 1.712566614151001 + 0.1 * 7.180261611938477
Epoch 90, val loss: 1.7318141460418701
Epoch 100, training loss: 2.3705575466156006 = 1.6581552028656006 + 0.1 * 7.124023914337158
Epoch 100, val loss: 1.685659408569336
Epoch 110, training loss: 2.2953944206237793 = 1.588467001914978 + 0.1 * 7.069275379180908
Epoch 110, val loss: 1.6268737316131592
Epoch 120, training loss: 2.2075955867767334 = 1.5062751770019531 + 0.1 * 7.013203144073486
Epoch 120, val loss: 1.5599594116210938
Epoch 130, training loss: 2.114063024520874 = 1.4174542427062988 + 0.1 * 6.9660868644714355
Epoch 130, val loss: 1.4899418354034424
Epoch 140, training loss: 2.0229787826538086 = 1.328894853591919 + 0.1 * 6.9408392906188965
Epoch 140, val loss: 1.4212175607681274
Epoch 150, training loss: 1.9335335493087769 = 1.2413842678070068 + 0.1 * 6.921492576599121
Epoch 150, val loss: 1.3547756671905518
Epoch 160, training loss: 1.8447024822235107 = 1.1538301706314087 + 0.1 * 6.908722400665283
Epoch 160, val loss: 1.2899092435836792
Epoch 170, training loss: 1.756758451461792 = 1.0667849779129028 + 0.1 * 6.899734973907471
Epoch 170, val loss: 1.22713041305542
Epoch 180, training loss: 1.6678005456924438 = 0.9792921543121338 + 0.1 * 6.8850836753845215
Epoch 180, val loss: 1.1640088558197021
Epoch 190, training loss: 1.580924391746521 = 0.8926796317100525 + 0.1 * 6.882447242736816
Epoch 190, val loss: 1.1014739274978638
Epoch 200, training loss: 1.4981883764266968 = 0.8121530413627625 + 0.1 * 6.860352993011475
Epoch 200, val loss: 1.044221043586731
Epoch 210, training loss: 1.4247932434082031 = 0.7394481897354126 + 0.1 * 6.853450775146484
Epoch 210, val loss: 0.9939039349555969
Epoch 220, training loss: 1.360149621963501 = 0.6762377619743347 + 0.1 * 6.839118003845215
Epoch 220, val loss: 0.9529610872268677
Epoch 230, training loss: 1.3023128509521484 = 0.620372474193573 + 0.1 * 6.819403648376465
Epoch 230, val loss: 0.9199653267860413
Epoch 240, training loss: 1.2512016296386719 = 0.5701762437820435 + 0.1 * 6.810253143310547
Epoch 240, val loss: 0.8939374089241028
Epoch 250, training loss: 1.2031562328338623 = 0.5242990255355835 + 0.1 * 6.788572788238525
Epoch 250, val loss: 0.8739538192749023
Epoch 260, training loss: 1.160715937614441 = 0.48171496391296387 + 0.1 * 6.790009498596191
Epoch 260, val loss: 0.858872652053833
Epoch 270, training loss: 1.1186527013778687 = 0.4421282708644867 + 0.1 * 6.765244007110596
Epoch 270, val loss: 0.8484264612197876
Epoch 280, training loss: 1.081134557723999 = 0.40495097637176514 + 0.1 * 6.76183557510376
Epoch 280, val loss: 0.8418446183204651
Epoch 290, training loss: 1.0444469451904297 = 0.3704700469970703 + 0.1 * 6.739768981933594
Epoch 290, val loss: 0.8387674689292908
Epoch 300, training loss: 1.011303186416626 = 0.3386076092720032 + 0.1 * 6.726954936981201
Epoch 300, val loss: 0.8386470675468445
Epoch 310, training loss: 0.9815715551376343 = 0.3092666566371918 + 0.1 * 6.723048686981201
Epoch 310, val loss: 0.8411298990249634
Epoch 320, training loss: 0.9539101123809814 = 0.2822946012020111 + 0.1 * 6.716155052185059
Epoch 320, val loss: 0.845805823802948
Epoch 330, training loss: 0.9268949627876282 = 0.2577005624771118 + 0.1 * 6.691944122314453
Epoch 330, val loss: 0.8523639440536499
Epoch 340, training loss: 0.9034363627433777 = 0.23523582518100739 + 0.1 * 6.682005405426025
Epoch 340, val loss: 0.8604418635368347
Epoch 350, training loss: 0.8857350945472717 = 0.21466155350208282 + 0.1 * 6.710735321044922
Epoch 350, val loss: 0.8700212836265564
Epoch 360, training loss: 0.8629271388053894 = 0.1960306316614151 + 0.1 * 6.6689653396606445
Epoch 360, val loss: 0.8807408809661865
Epoch 370, training loss: 0.8452574014663696 = 0.17909972369670868 + 0.1 * 6.661576747894287
Epoch 370, val loss: 0.8923977613449097
Epoch 380, training loss: 0.8303673267364502 = 0.1637726128101349 + 0.1 * 6.665946960449219
Epoch 380, val loss: 0.9049603343009949
Epoch 390, training loss: 0.8153848648071289 = 0.1499101221561432 + 0.1 * 6.654747009277344
Epoch 390, val loss: 0.9182947278022766
Epoch 400, training loss: 0.8021815419197083 = 0.1373947709798813 + 0.1 * 6.647867679595947
Epoch 400, val loss: 0.9322282671928406
Epoch 410, training loss: 0.7921222448348999 = 0.12608182430267334 + 0.1 * 6.660404205322266
Epoch 410, val loss: 0.9466093182563782
Epoch 420, training loss: 0.7790995240211487 = 0.11590195447206497 + 0.1 * 6.6319756507873535
Epoch 420, val loss: 0.9614003300666809
Epoch 430, training loss: 0.7704721093177795 = 0.1066981628537178 + 0.1 * 6.637739181518555
Epoch 430, val loss: 0.9763927459716797
Epoch 440, training loss: 0.762164831161499 = 0.0983978807926178 + 0.1 * 6.637669086456299
Epoch 440, val loss: 0.9916449785232544
Epoch 450, training loss: 0.7524033784866333 = 0.09088564664125443 + 0.1 * 6.615177154541016
Epoch 450, val loss: 1.006980538368225
Epoch 460, training loss: 0.7451953291893005 = 0.08406209200620651 + 0.1 * 6.611332416534424
Epoch 460, val loss: 1.0224922895431519
Epoch 470, training loss: 0.738680362701416 = 0.07787235081195831 + 0.1 * 6.6080803871154785
Epoch 470, val loss: 1.0379074811935425
Epoch 480, training loss: 0.7326155304908752 = 0.0722614973783493 + 0.1 * 6.603539943695068
Epoch 480, val loss: 1.0533602237701416
Epoch 490, training loss: 0.7283709049224854 = 0.06715948134660721 + 0.1 * 6.612113952636719
Epoch 490, val loss: 1.0686562061309814
Epoch 500, training loss: 0.7232929468154907 = 0.06251765042543411 + 0.1 * 6.607752799987793
Epoch 500, val loss: 1.0838083028793335
Epoch 510, training loss: 0.7176986932754517 = 0.05829702690243721 + 0.1 * 6.5940165519714355
Epoch 510, val loss: 1.0987989902496338
Epoch 520, training loss: 0.7132782936096191 = 0.05444245785474777 + 0.1 * 6.588357925415039
Epoch 520, val loss: 1.1136014461517334
Epoch 530, training loss: 0.7109466791152954 = 0.050911951810121536 + 0.1 * 6.60034704208374
Epoch 530, val loss: 1.1282235383987427
Epoch 540, training loss: 0.7065517902374268 = 0.047688547521829605 + 0.1 * 6.588632106781006
Epoch 540, val loss: 1.142551302909851
Epoch 550, training loss: 0.7031020522117615 = 0.0447346493601799 + 0.1 * 6.58367395401001
Epoch 550, val loss: 1.1566224098205566
Epoch 560, training loss: 0.7004557847976685 = 0.04202871769666672 + 0.1 * 6.584270477294922
Epoch 560, val loss: 1.1703360080718994
Epoch 570, training loss: 0.6964666843414307 = 0.039553720504045486 + 0.1 * 6.569129467010498
Epoch 570, val loss: 1.183838129043579
Epoch 580, training loss: 0.6937308311462402 = 0.037270862609148026 + 0.1 * 6.564599990844727
Epoch 580, val loss: 1.1970605850219727
Epoch 590, training loss: 0.6915452480316162 = 0.035167887806892395 + 0.1 * 6.5637736320495605
Epoch 590, val loss: 1.2099336385726929
Epoch 600, training loss: 0.6887592077255249 = 0.0332331582903862 + 0.1 * 6.555260181427002
Epoch 600, val loss: 1.2226289510726929
Epoch 610, training loss: 0.6875940561294556 = 0.0314452089369297 + 0.1 * 6.561488151550293
Epoch 610, val loss: 1.2349236011505127
Epoch 620, training loss: 0.6864401698112488 = 0.029796335846185684 + 0.1 * 6.566437721252441
Epoch 620, val loss: 1.2470200061798096
Epoch 630, training loss: 0.6832490563392639 = 0.0282671507447958 + 0.1 * 6.549818992614746
Epoch 630, val loss: 1.258734107017517
Epoch 640, training loss: 0.6814513206481934 = 0.026855288073420525 + 0.1 * 6.545960426330566
Epoch 640, val loss: 1.2702702283859253
Epoch 650, training loss: 0.6807491779327393 = 0.025540059432387352 + 0.1 * 6.552091121673584
Epoch 650, val loss: 1.281514048576355
Epoch 660, training loss: 0.6792550086975098 = 0.024320565164089203 + 0.1 * 6.549344539642334
Epoch 660, val loss: 1.292411208152771
Epoch 670, training loss: 0.6775375008583069 = 0.023185396566987038 + 0.1 * 6.543520927429199
Epoch 670, val loss: 1.3031578063964844
Epoch 680, training loss: 0.6777044534683228 = 0.022126611322164536 + 0.1 * 6.555778503417969
Epoch 680, val loss: 1.3135970830917358
Epoch 690, training loss: 0.6750192642211914 = 0.02114068530499935 + 0.1 * 6.538785457611084
Epoch 690, val loss: 1.3237247467041016
Epoch 700, training loss: 0.6733179092407227 = 0.020218411460518837 + 0.1 * 6.530994415283203
Epoch 700, val loss: 1.3337186574935913
Epoch 710, training loss: 0.6740207076072693 = 0.01935514435172081 + 0.1 * 6.546655654907227
Epoch 710, val loss: 1.3434350490570068
Epoch 720, training loss: 0.6725353002548218 = 0.01854724809527397 + 0.1 * 6.539880275726318
Epoch 720, val loss: 1.3529056310653687
Epoch 730, training loss: 0.6707301139831543 = 0.017790252342820168 + 0.1 * 6.529398441314697
Epoch 730, val loss: 1.3621604442596436
Epoch 740, training loss: 0.6697636842727661 = 0.017081065103411674 + 0.1 * 6.526825904846191
Epoch 740, val loss: 1.371228575706482
Epoch 750, training loss: 0.6694117784500122 = 0.016412928700447083 + 0.1 * 6.5299882888793945
Epoch 750, val loss: 1.3800663948059082
Epoch 760, training loss: 0.6680471301078796 = 0.015785209834575653 + 0.1 * 6.522619247436523
Epoch 760, val loss: 1.3887189626693726
Epoch 770, training loss: 0.6674179434776306 = 0.015192784368991852 + 0.1 * 6.522251129150391
Epoch 770, val loss: 1.3971832990646362
Epoch 780, training loss: 0.6681057810783386 = 0.01463545672595501 + 0.1 * 6.534703254699707
Epoch 780, val loss: 1.4054443836212158
Epoch 790, training loss: 0.6664769649505615 = 0.01410875003784895 + 0.1 * 6.523681640625
Epoch 790, val loss: 1.4135653972625732
Epoch 800, training loss: 0.6646695733070374 = 0.013611169531941414 + 0.1 * 6.510583400726318
Epoch 800, val loss: 1.4213947057724
Epoch 810, training loss: 0.6643676161766052 = 0.013142330572009087 + 0.1 * 6.512252330780029
Epoch 810, val loss: 1.4292068481445312
Epoch 820, training loss: 0.6645686626434326 = 0.012695759534835815 + 0.1 * 6.518729209899902
Epoch 820, val loss: 1.4367650747299194
Epoch 830, training loss: 0.6628229022026062 = 0.012273951433598995 + 0.1 * 6.505489349365234
Epoch 830, val loss: 1.4441832304000854
Epoch 840, training loss: 0.6640667915344238 = 0.011874085292220116 + 0.1 * 6.5219268798828125
Epoch 840, val loss: 1.4515416622161865
Epoch 850, training loss: 0.6621320843696594 = 0.011494331993162632 + 0.1 * 6.506377220153809
Epoch 850, val loss: 1.4585931301116943
Epoch 860, training loss: 0.661055862903595 = 0.01113309245556593 + 0.1 * 6.499227523803711
Epoch 860, val loss: 1.465686559677124
Epoch 870, training loss: 0.6609770059585571 = 0.010789529420435429 + 0.1 * 6.501874923706055
Epoch 870, val loss: 1.472529411315918
Epoch 880, training loss: 0.6610723733901978 = 0.010462937876582146 + 0.1 * 6.506094455718994
Epoch 880, val loss: 1.4792547225952148
Epoch 890, training loss: 0.6600396037101746 = 0.010152824223041534 + 0.1 * 6.498867511749268
Epoch 890, val loss: 1.4859293699264526
Epoch 900, training loss: 0.6605254411697388 = 0.009855552576482296 + 0.1 * 6.5066986083984375
Epoch 900, val loss: 1.4924179315567017
Epoch 910, training loss: 0.6584509611129761 = 0.009573263116180897 + 0.1 * 6.488776683807373
Epoch 910, val loss: 1.4987432956695557
Epoch 920, training loss: 0.6579620242118835 = 0.009302742779254913 + 0.1 * 6.486592769622803
Epoch 920, val loss: 1.505010962486267
Epoch 930, training loss: 0.6595103144645691 = 0.009044645354151726 + 0.1 * 6.5046563148498535
Epoch 930, val loss: 1.5110889673233032
Epoch 940, training loss: 0.658004641532898 = 0.008798639290034771 + 0.1 * 6.492059707641602
Epoch 940, val loss: 1.517031192779541
Epoch 950, training loss: 0.6565690636634827 = 0.008564048446714878 + 0.1 * 6.480050086975098
Epoch 950, val loss: 1.5230069160461426
Epoch 960, training loss: 0.657619297504425 = 0.00833779014647007 + 0.1 * 6.492815017700195
Epoch 960, val loss: 1.5287858247756958
Epoch 970, training loss: 0.6567946672439575 = 0.008121445775032043 + 0.1 * 6.486732482910156
Epoch 970, val loss: 1.5344338417053223
Epoch 980, training loss: 0.6572689414024353 = 0.007914191111922264 + 0.1 * 6.493547439575195
Epoch 980, val loss: 1.5400625467300415
Epoch 990, training loss: 0.6553447842597961 = 0.007716209627687931 + 0.1 * 6.476285457611084
Epoch 990, val loss: 1.5455389022827148
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.8071482181549072 = 1.947463035583496 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.94265615940094
Epoch 10, training loss: 2.797102689743042 = 1.9374221563339233 + 0.1 * 8.59680461883545
Epoch 10, val loss: 1.933233380317688
Epoch 20, training loss: 2.7846317291259766 = 1.9249765872955322 + 0.1 * 8.596549987792969
Epoch 20, val loss: 1.9211829900741577
Epoch 30, training loss: 2.7667758464813232 = 1.9073076248168945 + 0.1 * 8.594681739807129
Epoch 30, val loss: 1.9039478302001953
Epoch 40, training loss: 2.739400625228882 = 1.8812923431396484 + 0.1 * 8.581083297729492
Epoch 40, val loss: 1.879037857055664
Epoch 50, training loss: 2.697451591491699 = 1.8458969593048096 + 0.1 * 8.515545845031738
Epoch 50, val loss: 1.8471890687942505
Epoch 60, training loss: 2.6404051780700684 = 1.8078076839447021 + 0.1 * 8.325973510742188
Epoch 60, val loss: 1.816152811050415
Epoch 70, training loss: 2.5940752029418945 = 1.773388385772705 + 0.1 * 8.206867218017578
Epoch 70, val loss: 1.7885618209838867
Epoch 80, training loss: 2.534057140350342 = 1.7330348491668701 + 0.1 * 8.010221481323242
Epoch 80, val loss: 1.7517837285995483
Epoch 90, training loss: 2.4577741622924805 = 1.6809519529342651 + 0.1 * 7.768222808837891
Epoch 90, val loss: 1.706748604774475
Epoch 100, training loss: 2.3695950508117676 = 1.6165459156036377 + 0.1 * 7.530490398406982
Epoch 100, val loss: 1.6554539203643799
Epoch 110, training loss: 2.284506320953369 = 1.540769338607788 + 0.1 * 7.4373698234558105
Epoch 110, val loss: 1.5902982950210571
Epoch 120, training loss: 2.195969343185425 = 1.4621987342834473 + 0.1 * 7.337705135345459
Epoch 120, val loss: 1.5276718139648438
Epoch 130, training loss: 2.1109304428100586 = 1.3875068426132202 + 0.1 * 7.234234809875488
Epoch 130, val loss: 1.4674744606018066
Epoch 140, training loss: 2.0314860343933105 = 1.317212462425232 + 0.1 * 7.142734527587891
Epoch 140, val loss: 1.4136351346969604
Epoch 150, training loss: 1.9570657014846802 = 1.2488341331481934 + 0.1 * 7.082315444946289
Epoch 150, val loss: 1.3613691329956055
Epoch 160, training loss: 1.8860217332839966 = 1.1816740036010742 + 0.1 * 7.0434770584106445
Epoch 160, val loss: 1.312290072441101
Epoch 170, training loss: 1.8153586387634277 = 1.113846778869629 + 0.1 * 7.015118598937988
Epoch 170, val loss: 1.2637354135513306
Epoch 180, training loss: 1.7445176839828491 = 1.044999599456787 + 0.1 * 6.995180606842041
Epoch 180, val loss: 1.214745283126831
Epoch 190, training loss: 1.6735421419143677 = 0.9763786196708679 + 0.1 * 6.971634864807129
Epoch 190, val loss: 1.16604483127594
Epoch 200, training loss: 1.6048835515975952 = 0.9099547266960144 + 0.1 * 6.9492878913879395
Epoch 200, val loss: 1.1199612617492676
Epoch 210, training loss: 1.5393052101135254 = 0.8465176224708557 + 0.1 * 6.9278764724731445
Epoch 210, val loss: 1.0769989490509033
Epoch 220, training loss: 1.4775605201721191 = 0.7864165902137756 + 0.1 * 6.911439895629883
Epoch 220, val loss: 1.038278579711914
Epoch 230, training loss: 1.418321132659912 = 0.7292725443840027 + 0.1 * 6.890486240386963
Epoch 230, val loss: 1.004017949104309
Epoch 240, training loss: 1.3618903160095215 = 0.674193263053894 + 0.1 * 6.876970291137695
Epoch 240, val loss: 0.9733753204345703
Epoch 250, training loss: 1.3075520992279053 = 0.621462881565094 + 0.1 * 6.860891819000244
Epoch 250, val loss: 0.9463684558868408
Epoch 260, training loss: 1.2565653324127197 = 0.5708411335945129 + 0.1 * 6.857241630554199
Epoch 260, val loss: 0.9228260517120361
Epoch 270, training loss: 1.2067066431045532 = 0.5231038331985474 + 0.1 * 6.836028099060059
Epoch 270, val loss: 0.9037667512893677
Epoch 280, training loss: 1.1614606380462646 = 0.47854647040367126 + 0.1 * 6.829141616821289
Epoch 280, val loss: 0.8897469639778137
Epoch 290, training loss: 1.1199166774749756 = 0.4376398026943207 + 0.1 * 6.822768688201904
Epoch 290, val loss: 0.8813508749008179
Epoch 300, training loss: 1.0819587707519531 = 0.40041685104370117 + 0.1 * 6.815418720245361
Epoch 300, val loss: 0.8782063722610474
Epoch 310, training loss: 1.047286033630371 = 0.36689022183418274 + 0.1 * 6.803957462310791
Epoch 310, val loss: 0.8798909187316895
Epoch 320, training loss: 1.01627516746521 = 0.3368094563484192 + 0.1 * 6.7946577072143555
Epoch 320, val loss: 0.8859248161315918
Epoch 330, training loss: 0.9896889328956604 = 0.30932343006134033 + 0.1 * 6.803654670715332
Epoch 330, val loss: 0.8953601717948914
Epoch 340, training loss: 0.9620392918586731 = 0.28391605615615845 + 0.1 * 6.7812323570251465
Epoch 340, val loss: 0.9073349237442017
Epoch 350, training loss: 0.9375190734863281 = 0.2600821554660797 + 0.1 * 6.774369239807129
Epoch 350, val loss: 0.9210962057113647
Epoch 360, training loss: 0.9151676893234253 = 0.2375616878271103 + 0.1 * 6.776059627532959
Epoch 360, val loss: 0.9365056157112122
Epoch 370, training loss: 0.8927666544914246 = 0.2163933664560318 + 0.1 * 6.76373291015625
Epoch 370, val loss: 0.9534343481063843
Epoch 380, training loss: 0.8720079064369202 = 0.19672459363937378 + 0.1 * 6.752832889556885
Epoch 380, val loss: 0.9717704653739929
Epoch 390, training loss: 0.8531633615493774 = 0.17868688702583313 + 0.1 * 6.74476432800293
Epoch 390, val loss: 0.9914823174476624
Epoch 400, training loss: 0.8392493724822998 = 0.16230055689811707 + 0.1 * 6.769488334655762
Epoch 400, val loss: 1.0123993158340454
Epoch 410, training loss: 0.8217531442642212 = 0.14760838449001312 + 0.1 * 6.741447448730469
Epoch 410, val loss: 1.0345181226730347
Epoch 420, training loss: 0.8072450160980225 = 0.13439352810382843 + 0.1 * 6.728514671325684
Epoch 420, val loss: 1.0575426816940308
Epoch 430, training loss: 0.7954948544502258 = 0.12251342087984085 + 0.1 * 6.729814529418945
Epoch 430, val loss: 1.0812169313430786
Epoch 440, training loss: 0.7834540009498596 = 0.1118626594543457 + 0.1 * 6.71591329574585
Epoch 440, val loss: 1.1054104566574097
Epoch 450, training loss: 0.774063766002655 = 0.10228727012872696 + 0.1 * 6.717764854431152
Epoch 450, val loss: 1.1301026344299316
Epoch 460, training loss: 0.7650291323661804 = 0.09370319545269012 + 0.1 * 6.713259220123291
Epoch 460, val loss: 1.154914379119873
Epoch 470, training loss: 0.7565726637840271 = 0.08600949496030807 + 0.1 * 6.705631256103516
Epoch 470, val loss: 1.1795933246612549
Epoch 480, training loss: 0.7491261959075928 = 0.0791020542383194 + 0.1 * 6.700241565704346
Epoch 480, val loss: 1.2043129205703735
Epoch 490, training loss: 0.7423892021179199 = 0.07290530949831009 + 0.1 * 6.694839000701904
Epoch 490, val loss: 1.2287418842315674
Epoch 500, training loss: 0.73639976978302 = 0.06734047085046768 + 0.1 * 6.6905927658081055
Epoch 500, val loss: 1.2528326511383057
Epoch 510, training loss: 0.7302362322807312 = 0.06233285740017891 + 0.1 * 6.6790337562561035
Epoch 510, val loss: 1.276536226272583
Epoch 520, training loss: 0.7253456711769104 = 0.05781645327806473 + 0.1 * 6.675292015075684
Epoch 520, val loss: 1.2998270988464355
Epoch 530, training loss: 0.7239499688148499 = 0.053729474544525146 + 0.1 * 6.702204704284668
Epoch 530, val loss: 1.3226326704025269
Epoch 540, training loss: 0.7173299789428711 = 0.05004856362938881 + 0.1 * 6.672813892364502
Epoch 540, val loss: 1.3450499773025513
Epoch 550, training loss: 0.7130022048950195 = 0.04671028256416321 + 0.1 * 6.662919044494629
Epoch 550, val loss: 1.3666857481002808
Epoch 560, training loss: 0.7099880576133728 = 0.04367600753903389 + 0.1 * 6.663120269775391
Epoch 560, val loss: 1.3879848718643188
Epoch 570, training loss: 0.706116795539856 = 0.04091427102684975 + 0.1 * 6.652024745941162
Epoch 570, val loss: 1.4088597297668457
Epoch 580, training loss: 0.7047479152679443 = 0.03839058429002762 + 0.1 * 6.663573265075684
Epoch 580, val loss: 1.4290069341659546
Epoch 590, training loss: 0.7017833590507507 = 0.03609113022685051 + 0.1 * 6.656921863555908
Epoch 590, val loss: 1.448865532875061
Epoch 600, training loss: 0.697807788848877 = 0.033990226686000824 + 0.1 * 6.6381754875183105
Epoch 600, val loss: 1.4679559469223022
Epoch 610, training loss: 0.6955538988113403 = 0.032063208520412445 + 0.1 * 6.634906768798828
Epoch 610, val loss: 1.4867829084396362
Epoch 620, training loss: 0.6948137283325195 = 0.030287085101008415 + 0.1 * 6.645266056060791
Epoch 620, val loss: 1.5050969123840332
Epoch 630, training loss: 0.6918692588806152 = 0.02865704894065857 + 0.1 * 6.632121562957764
Epoch 630, val loss: 1.5229294300079346
Epoch 640, training loss: 0.6897842288017273 = 0.0271564032882452 + 0.1 * 6.626277923583984
Epoch 640, val loss: 1.5401697158813477
Epoch 650, training loss: 0.6879866123199463 = 0.02577078714966774 + 0.1 * 6.622158050537109
Epoch 650, val loss: 1.5571832656860352
Epoch 660, training loss: 0.6866015791893005 = 0.024486538022756577 + 0.1 * 6.621150016784668
Epoch 660, val loss: 1.5737885236740112
Epoch 670, training loss: 0.6852110028266907 = 0.023296907544136047 + 0.1 * 6.619141101837158
Epoch 670, val loss: 1.5898617506027222
Epoch 680, training loss: 0.6829383373260498 = 0.022193241864442825 + 0.1 * 6.607450485229492
Epoch 680, val loss: 1.605696678161621
Epoch 690, training loss: 0.682852029800415 = 0.021165799349546432 + 0.1 * 6.6168622970581055
Epoch 690, val loss: 1.620989203453064
Epoch 700, training loss: 0.6813439130783081 = 0.020211104303598404 + 0.1 * 6.611328125
Epoch 700, val loss: 1.6359663009643555
Epoch 710, training loss: 0.679227888584137 = 0.01932273991405964 + 0.1 * 6.599050998687744
Epoch 710, val loss: 1.6505357027053833
Epoch 720, training loss: 0.6783112287521362 = 0.018490999937057495 + 0.1 * 6.598202705383301
Epoch 720, val loss: 1.6647752523422241
Epoch 730, training loss: 0.6775867938995361 = 0.017714302986860275 + 0.1 * 6.598724842071533
Epoch 730, val loss: 1.6785013675689697
Epoch 740, training loss: 0.6755538582801819 = 0.016989341005682945 + 0.1 * 6.585644721984863
Epoch 740, val loss: 1.6922260522842407
Epoch 750, training loss: 0.6765150427818298 = 0.01630757376551628 + 0.1 * 6.60207462310791
Epoch 750, val loss: 1.7053667306900024
Epoch 760, training loss: 0.6745259761810303 = 0.015669675543904305 + 0.1 * 6.588562488555908
Epoch 760, val loss: 1.718284010887146
Epoch 770, training loss: 0.6739228963851929 = 0.015069092623889446 + 0.1 * 6.58853816986084
Epoch 770, val loss: 1.7309057712554932
Epoch 780, training loss: 0.672614574432373 = 0.01450430043041706 + 0.1 * 6.58110237121582
Epoch 780, val loss: 1.7433645725250244
Epoch 790, training loss: 0.6713976263999939 = 0.013972314074635506 + 0.1 * 6.574252605438232
Epoch 790, val loss: 1.7552517652511597
Epoch 800, training loss: 0.6705964207649231 = 0.013471740297973156 + 0.1 * 6.571247100830078
Epoch 800, val loss: 1.767263412475586
Epoch 810, training loss: 0.6715329885482788 = 0.012997318990528584 + 0.1 * 6.58535623550415
Epoch 810, val loss: 1.77869713306427
Epoch 820, training loss: 0.6691414713859558 = 0.01255049742758274 + 0.1 * 6.5659098625183105
Epoch 820, val loss: 1.7902480363845825
Epoch 830, training loss: 0.6692864298820496 = 0.012126469984650612 + 0.1 * 6.571599960327148
Epoch 830, val loss: 1.8013426065444946
Epoch 840, training loss: 0.6695541143417358 = 0.011724983341991901 + 0.1 * 6.578290939331055
Epoch 840, val loss: 1.8122025728225708
Epoch 850, training loss: 0.6675416827201843 = 0.011345808394253254 + 0.1 * 6.5619587898254395
Epoch 850, val loss: 1.8230315446853638
Epoch 860, training loss: 0.6682642102241516 = 0.0109852384775877 + 0.1 * 6.572789192199707
Epoch 860, val loss: 1.8333276510238647
Epoch 870, training loss: 0.6664431691169739 = 0.010644065216183662 + 0.1 * 6.557990550994873
Epoch 870, val loss: 1.8437622785568237
Epoch 880, training loss: 0.6655274629592896 = 0.010319181717932224 + 0.1 * 6.5520830154418945
Epoch 880, val loss: 1.8538289070129395
Epoch 890, training loss: 0.6672366261482239 = 0.01000965479761362 + 0.1 * 6.572269916534424
Epoch 890, val loss: 1.8635395765304565
Epoch 900, training loss: 0.6653074026107788 = 0.00971639808267355 + 0.1 * 6.555910110473633
Epoch 900, val loss: 1.8734673261642456
Epoch 910, training loss: 0.6638970375061035 = 0.009436305612325668 + 0.1 * 6.544607162475586
Epoch 910, val loss: 1.8828712701797485
Epoch 920, training loss: 0.665620744228363 = 0.009169178083539009 + 0.1 * 6.564515590667725
Epoch 920, val loss: 1.8922622203826904
Epoch 930, training loss: 0.6641003489494324 = 0.008914758451282978 + 0.1 * 6.551855564117432
Epoch 930, val loss: 1.9014438390731812
Epoch 940, training loss: 0.6634549498558044 = 0.008671415038406849 + 0.1 * 6.547835350036621
Epoch 940, val loss: 1.9105474948883057
Epoch 950, training loss: 0.663483738899231 = 0.008438789285719395 + 0.1 * 6.550449371337891
Epoch 950, val loss: 1.9193720817565918
Epoch 960, training loss: 0.6649231314659119 = 0.008216345682740211 + 0.1 * 6.567068099975586
Epoch 960, val loss: 1.9280332326889038
Epoch 970, training loss: 0.6625792384147644 = 0.008004133589565754 + 0.1 * 6.545751094818115
Epoch 970, val loss: 1.9365290403366089
Epoch 980, training loss: 0.6609774827957153 = 0.0078012277372181416 + 0.1 * 6.531762599945068
Epoch 980, val loss: 1.9449998140335083
Epoch 990, training loss: 0.6613864302635193 = 0.007605947554111481 + 0.1 * 6.53780460357666
Epoch 990, val loss: 1.9531656503677368
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8212967843964154
=== training gcn model ===
Epoch 0, training loss: 2.8037216663360596 = 1.9440367221832275 + 0.1 * 8.596848487854004
Epoch 0, val loss: 1.9418506622314453
Epoch 10, training loss: 2.7941558361053467 = 1.934475302696228 + 0.1 * 8.59680461883545
Epoch 10, val loss: 1.9321975708007812
Epoch 20, training loss: 2.782599449157715 = 1.9229422807693481 + 0.1 * 8.596572875976562
Epoch 20, val loss: 1.9198271036148071
Epoch 30, training loss: 2.766387939453125 = 1.9069079160690308 + 0.1 * 8.594799041748047
Epoch 30, val loss: 1.9021507501602173
Epoch 40, training loss: 2.7414400577545166 = 1.8832883834838867 + 0.1 * 8.581517219543457
Epoch 40, val loss: 1.8759708404541016
Epoch 50, training loss: 2.7033257484436035 = 1.850797414779663 + 0.1 * 8.525283813476562
Epoch 50, val loss: 1.841476559638977
Epoch 60, training loss: 2.654205799102783 = 1.8158727884292603 + 0.1 * 8.383331298828125
Epoch 60, val loss: 1.8087728023529053
Epoch 70, training loss: 2.6117470264434814 = 1.7861557006835938 + 0.1 * 8.255912780761719
Epoch 70, val loss: 1.7855808734893799
Epoch 80, training loss: 2.531919002532959 = 1.7534148693084717 + 0.1 * 7.7850422859191895
Epoch 80, val loss: 1.7605687379837036
Epoch 90, training loss: 2.4490928649902344 = 1.712697982788086 + 0.1 * 7.363949775695801
Epoch 90, val loss: 1.725913643836975
Epoch 100, training loss: 2.380464792251587 = 1.6581493616104126 + 0.1 * 7.223154544830322
Epoch 100, val loss: 1.678062081336975
Epoch 110, training loss: 2.3011069297790527 = 1.5878117084503174 + 0.1 * 7.132950782775879
Epoch 110, val loss: 1.6202483177185059
Epoch 120, training loss: 2.211632251739502 = 1.504973292350769 + 0.1 * 7.06658935546875
Epoch 120, val loss: 1.554500937461853
Epoch 130, training loss: 2.1171419620513916 = 1.4144177436828613 + 0.1 * 7.0272417068481445
Epoch 130, val loss: 1.4824861288070679
Epoch 140, training loss: 2.019286632537842 = 1.3192836046218872 + 0.1 * 7.000030994415283
Epoch 140, val loss: 1.4103562831878662
Epoch 150, training loss: 1.9173498153686523 = 1.2200860977172852 + 0.1 * 6.972637176513672
Epoch 150, val loss: 1.3362737894058228
Epoch 160, training loss: 1.8134891986846924 = 1.1181490421295166 + 0.1 * 6.953400611877441
Epoch 160, val loss: 1.2615212202072144
Epoch 170, training loss: 1.713292121887207 = 1.019716501235962 + 0.1 * 6.935755729675293
Epoch 170, val loss: 1.1912940740585327
Epoch 180, training loss: 1.6201913356781006 = 0.9282709360122681 + 0.1 * 6.9192047119140625
Epoch 180, val loss: 1.1282333135604858
Epoch 190, training loss: 1.538440227508545 = 0.8472487330436707 + 0.1 * 6.911914348602295
Epoch 190, val loss: 1.0753021240234375
Epoch 200, training loss: 1.4672679901123047 = 0.7778316140174866 + 0.1 * 6.894362926483154
Epoch 200, val loss: 1.032999038696289
Epoch 210, training loss: 1.4064222574234009 = 0.7177984118461609 + 0.1 * 6.886238098144531
Epoch 210, val loss: 0.9991629123687744
Epoch 220, training loss: 1.3519586324691772 = 0.6654947996139526 + 0.1 * 6.864638328552246
Epoch 220, val loss: 0.9724946022033691
Epoch 230, training loss: 1.3039755821228027 = 0.6186357140541077 + 0.1 * 6.853399276733398
Epoch 230, val loss: 0.9514484405517578
Epoch 240, training loss: 1.260187029838562 = 0.5758675932884216 + 0.1 * 6.843194007873535
Epoch 240, val loss: 0.9349774718284607
Epoch 250, training loss: 1.2192332744598389 = 0.5360774993896484 + 0.1 * 6.831557750701904
Epoch 250, val loss: 0.922262966632843
Epoch 260, training loss: 1.1800867319107056 = 0.4985196888446808 + 0.1 * 6.815670490264893
Epoch 260, val loss: 0.9125102758407593
Epoch 270, training loss: 1.1430881023406982 = 0.46236181259155273 + 0.1 * 6.807262420654297
Epoch 270, val loss: 0.904890775680542
Epoch 280, training loss: 1.1079418659210205 = 0.42774778604507446 + 0.1 * 6.801941394805908
Epoch 280, val loss: 0.8997437357902527
Epoch 290, training loss: 1.0730395317077637 = 0.3947649598121643 + 0.1 * 6.782745838165283
Epoch 290, val loss: 0.8971554040908813
Epoch 300, training loss: 1.0422248840332031 = 0.3633241355419159 + 0.1 * 6.789007663726807
Epoch 300, val loss: 0.897199809551239
Epoch 310, training loss: 1.00992751121521 = 0.3335746228694916 + 0.1 * 6.763528347015381
Epoch 310, val loss: 0.8996484279632568
Epoch 320, training loss: 0.9806091785430908 = 0.30511966347694397 + 0.1 * 6.754894733428955
Epoch 320, val loss: 0.9040431976318359
Epoch 330, training loss: 0.9543805122375488 = 0.2779572308063507 + 0.1 * 6.764232158660889
Epoch 330, val loss: 0.9103212952613831
Epoch 340, training loss: 0.9274594783782959 = 0.25253868103027344 + 0.1 * 6.749207973480225
Epoch 340, val loss: 0.9182486534118652
Epoch 350, training loss: 0.9025875329971313 = 0.22892561554908752 + 0.1 * 6.736619472503662
Epoch 350, val loss: 0.9276753664016724
Epoch 360, training loss: 0.880950927734375 = 0.2072366178035736 + 0.1 * 6.737143039703369
Epoch 360, val loss: 0.9388052225112915
Epoch 370, training loss: 0.859836220741272 = 0.18758323788642883 + 0.1 * 6.722529888153076
Epoch 370, val loss: 0.9514231085777283
Epoch 380, training loss: 0.8417328596115112 = 0.16990876197814941 + 0.1 * 6.718240737915039
Epoch 380, val loss: 0.9653967022895813
Epoch 390, training loss: 0.8266611099243164 = 0.15413442254066467 + 0.1 * 6.725266933441162
Epoch 390, val loss: 0.9806696176528931
Epoch 400, training loss: 0.8103867173194885 = 0.14010851085186005 + 0.1 * 6.702781677246094
Epoch 400, val loss: 0.9971251487731934
Epoch 410, training loss: 0.7982355356216431 = 0.1275976151227951 + 0.1 * 6.706378936767578
Epoch 410, val loss: 1.0145426988601685
Epoch 420, training loss: 0.7869116067886353 = 0.11644681543111801 + 0.1 * 6.704648017883301
Epoch 420, val loss: 1.0327714681625366
Epoch 430, training loss: 0.7755010724067688 = 0.10650596767663956 + 0.1 * 6.689950466156006
Epoch 430, val loss: 1.0516197681427002
Epoch 440, training loss: 0.7667256593704224 = 0.09761635214090347 + 0.1 * 6.6910929679870605
Epoch 440, val loss: 1.0709164142608643
Epoch 450, training loss: 0.7581685781478882 = 0.08966702222824097 + 0.1 * 6.685015678405762
Epoch 450, val loss: 1.0904721021652222
Epoch 460, training loss: 0.751914381980896 = 0.08253881335258484 + 0.1 * 6.693756103515625
Epoch 460, val loss: 1.1101619005203247
Epoch 470, training loss: 0.7429149746894836 = 0.07614076882600784 + 0.1 * 6.6677422523498535
Epoch 470, val loss: 1.1300265789031982
Epoch 480, training loss: 0.7370613813400269 = 0.07037271559238434 + 0.1 * 6.666886806488037
Epoch 480, val loss: 1.14981210231781
Epoch 490, training loss: 0.7317094206809998 = 0.06516720354557037 + 0.1 * 6.665421962738037
Epoch 490, val loss: 1.1696199178695679
Epoch 500, training loss: 0.7262806296348572 = 0.06046897917985916 + 0.1 * 6.658116340637207
Epoch 500, val loss: 1.1891279220581055
Epoch 510, training loss: 0.7212008237838745 = 0.056217316538095474 + 0.1 * 6.649835109710693
Epoch 510, val loss: 1.208412528038025
Epoch 520, training loss: 0.7165179252624512 = 0.052361100912094116 + 0.1 * 6.641568183898926
Epoch 520, val loss: 1.2274380922317505
Epoch 530, training loss: 0.7128924131393433 = 0.048863526433706284 + 0.1 * 6.640288829803467
Epoch 530, val loss: 1.2461862564086914
Epoch 540, training loss: 0.710351288318634 = 0.045676056295633316 + 0.1 * 6.64675235748291
Epoch 540, val loss: 1.2645423412322998
Epoch 550, training loss: 0.7057633399963379 = 0.042768292129039764 + 0.1 * 6.629950523376465
Epoch 550, val loss: 1.2826364040374756
Epoch 560, training loss: 0.7038025856018066 = 0.04010732099413872 + 0.1 * 6.6369524002075195
Epoch 560, val loss: 1.3002420663833618
Epoch 570, training loss: 0.6999270915985107 = 0.03767424076795578 + 0.1 * 6.622528076171875
Epoch 570, val loss: 1.3175846338272095
Epoch 580, training loss: 0.7004539966583252 = 0.0354422926902771 + 0.1 * 6.650116920471191
Epoch 580, val loss: 1.3343430757522583
Epoch 590, training loss: 0.695896565914154 = 0.03339681774377823 + 0.1 * 6.624997138977051
Epoch 590, val loss: 1.350770115852356
Epoch 600, training loss: 0.6926411986351013 = 0.03151397034525871 + 0.1 * 6.611271858215332
Epoch 600, val loss: 1.366756796836853
Epoch 610, training loss: 0.691447377204895 = 0.029776303097605705 + 0.1 * 6.616710662841797
Epoch 610, val loss: 1.3823790550231934
Epoch 620, training loss: 0.6894751787185669 = 0.028174053877592087 + 0.1 * 6.613011360168457
Epoch 620, val loss: 1.3977789878845215
Epoch 630, training loss: 0.6874291896820068 = 0.02669243887066841 + 0.1 * 6.607367038726807
Epoch 630, val loss: 1.4125407934188843
Epoch 640, training loss: 0.6852989196777344 = 0.025323985144495964 + 0.1 * 6.5997490882873535
Epoch 640, val loss: 1.427179217338562
Epoch 650, training loss: 0.6839780807495117 = 0.024052588269114494 + 0.1 * 6.599255084991455
Epoch 650, val loss: 1.4413070678710938
Epoch 660, training loss: 0.6818954348564148 = 0.02287360094487667 + 0.1 * 6.590218544006348
Epoch 660, val loss: 1.4549994468688965
Epoch 670, training loss: 0.6804341077804565 = 0.02177908830344677 + 0.1 * 6.586550235748291
Epoch 670, val loss: 1.4685193300247192
Epoch 680, training loss: 0.6813661456108093 = 0.020758401602506638 + 0.1 * 6.606077194213867
Epoch 680, val loss: 1.4815000295639038
Epoch 690, training loss: 0.6778947114944458 = 0.019809624180197716 + 0.1 * 6.580851078033447
Epoch 690, val loss: 1.494293212890625
Epoch 700, training loss: 0.6765697002410889 = 0.018923316150903702 + 0.1 * 6.57646369934082
Epoch 700, val loss: 1.5066972970962524
Epoch 710, training loss: 0.6763513088226318 = 0.0180948618799448 + 0.1 * 6.582564830780029
Epoch 710, val loss: 1.5189040899276733
Epoch 720, training loss: 0.6752545237541199 = 0.017319653183221817 + 0.1 * 6.579348087310791
Epoch 720, val loss: 1.5306050777435303
Epoch 730, training loss: 0.6740120053291321 = 0.01659541018307209 + 0.1 * 6.5741658210754395
Epoch 730, val loss: 1.5422917604446411
Epoch 740, training loss: 0.6729317307472229 = 0.015915045514702797 + 0.1 * 6.57016658782959
Epoch 740, val loss: 1.5533949136734009
Epoch 750, training loss: 0.6722145676612854 = 0.01527781318873167 + 0.1 * 6.569367408752441
Epoch 750, val loss: 1.5643802881240845
Epoch 760, training loss: 0.6706709861755371 = 0.014679154381155968 + 0.1 * 6.559918403625488
Epoch 760, val loss: 1.5751787424087524
Epoch 770, training loss: 0.6711211204528809 = 0.01411531399935484 + 0.1 * 6.5700578689575195
Epoch 770, val loss: 1.5856060981750488
Epoch 780, training loss: 0.6689983606338501 = 0.01358491089195013 + 0.1 * 6.554134368896484
Epoch 780, val loss: 1.5959020853042603
Epoch 790, training loss: 0.6682140827178955 = 0.013084912672638893 + 0.1 * 6.5512919425964355
Epoch 790, val loss: 1.6059683561325073
Epoch 800, training loss: 0.6677541732788086 = 0.012612729333341122 + 0.1 * 6.551414489746094
Epoch 800, val loss: 1.6157103776931763
Epoch 810, training loss: 0.6681885719299316 = 0.012167184613645077 + 0.1 * 6.560214042663574
Epoch 810, val loss: 1.6253740787506104
Epoch 820, training loss: 0.6665399074554443 = 0.011746114119887352 + 0.1 * 6.547937870025635
Epoch 820, val loss: 1.6347798109054565
Epoch 830, training loss: 0.6653954386711121 = 0.011347160674631596 + 0.1 * 6.540482521057129
Epoch 830, val loss: 1.6439759731292725
Epoch 840, training loss: 0.6645403504371643 = 0.010968925431370735 + 0.1 * 6.535714149475098
Epoch 840, val loss: 1.6527965068817139
Epoch 850, training loss: 0.664659321308136 = 0.010611624456942081 + 0.1 * 6.5404767990112305
Epoch 850, val loss: 1.6617149114608765
Epoch 860, training loss: 0.6645189523696899 = 0.010272260755300522 + 0.1 * 6.542466640472412
Epoch 860, val loss: 1.6701996326446533
Epoch 870, training loss: 0.6637184619903564 = 0.009950228966772556 + 0.1 * 6.537682056427002
Epoch 870, val loss: 1.6787546873092651
Epoch 880, training loss: 0.6632111668586731 = 0.00964359287172556 + 0.1 * 6.535675525665283
Epoch 880, val loss: 1.686787724494934
Epoch 890, training loss: 0.6621403694152832 = 0.00935286469757557 + 0.1 * 6.527874946594238
Epoch 890, val loss: 1.695077657699585
Epoch 900, training loss: 0.661895215511322 = 0.009075384587049484 + 0.1 * 6.5281982421875
Epoch 900, val loss: 1.702935814857483
Epoch 910, training loss: 0.6647171378135681 = 0.008811148814857006 + 0.1 * 6.5590596199035645
Epoch 910, val loss: 1.7106764316558838
Epoch 920, training loss: 0.6614246964454651 = 0.008559697307646275 + 0.1 * 6.528649806976318
Epoch 920, val loss: 1.7182694673538208
Epoch 930, training loss: 0.6607306599617004 = 0.00831994041800499 + 0.1 * 6.524107456207275
Epoch 930, val loss: 1.725634217262268
Epoch 940, training loss: 0.6605970859527588 = 0.00809151865541935 + 0.1 * 6.525055885314941
Epoch 940, val loss: 1.7329820394515991
Epoch 950, training loss: 0.6588744521141052 = 0.00787325669080019 + 0.1 * 6.510012149810791
Epoch 950, val loss: 1.740156888961792
Epoch 960, training loss: 0.6596453785896301 = 0.00766441086307168 + 0.1 * 6.519809722900391
Epoch 960, val loss: 1.7471915483474731
Epoch 970, training loss: 0.6580417156219482 = 0.007464154157787561 + 0.1 * 6.505775451660156
Epoch 970, val loss: 1.7541486024856567
Epoch 980, training loss: 0.6586238741874695 = 0.007272368296980858 + 0.1 * 6.513514995574951
Epoch 980, val loss: 1.7608095407485962
Epoch 990, training loss: 0.6583697199821472 = 0.007088880520313978 + 0.1 * 6.512808322906494
Epoch 990, val loss: 1.7675459384918213
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8191881918819188
The final CL Acc:0.76420, 0.00630, The final GNN Acc:0.81954, 0.00131
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13204])
remove edge: torch.Size([2, 7858])
updated graph: torch.Size([2, 10506])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.7901058197021484 = 1.9304203987121582 + 0.1 * 8.596854209899902
Epoch 0, val loss: 1.9134584665298462
Epoch 10, training loss: 2.7805423736572266 = 1.9208645820617676 + 0.1 * 8.59677791595459
Epoch 10, val loss: 1.9045988321304321
Epoch 20, training loss: 2.7686872482299805 = 1.9090440273284912 + 0.1 * 8.596430778503418
Epoch 20, val loss: 1.893234133720398
Epoch 30, training loss: 2.7520053386688232 = 1.892635703086853 + 0.1 * 8.593696594238281
Epoch 30, val loss: 1.8772683143615723
Epoch 40, training loss: 2.725827932357788 = 1.8687469959259033 + 0.1 * 8.570809364318848
Epoch 40, val loss: 1.8542970418930054
Epoch 50, training loss: 2.679564952850342 = 1.8361026048660278 + 0.1 * 8.434624671936035
Epoch 50, val loss: 1.8247231245040894
Epoch 60, training loss: 2.615413188934326 = 1.7996479272842407 + 0.1 * 8.157651901245117
Epoch 60, val loss: 1.7944527864456177
Epoch 70, training loss: 2.531399965286255 = 1.764105200767517 + 0.1 * 7.672947883605957
Epoch 70, val loss: 1.765394687652588
Epoch 80, training loss: 2.4525156021118164 = 1.7254518270492554 + 0.1 * 7.270638942718506
Epoch 80, val loss: 1.7318429946899414
Epoch 90, training loss: 2.3902180194854736 = 1.675965428352356 + 0.1 * 7.142526626586914
Epoch 90, val loss: 1.68764328956604
Epoch 100, training loss: 2.3184478282928467 = 1.6104328632354736 + 0.1 * 7.080150127410889
Epoch 100, val loss: 1.630118727684021
Epoch 110, training loss: 2.233915328979492 = 1.5297163724899292 + 0.1 * 7.041989803314209
Epoch 110, val loss: 1.5614964962005615
Epoch 120, training loss: 2.1406185626983643 = 1.4388482570648193 + 0.1 * 7.017702102661133
Epoch 120, val loss: 1.4856599569320679
Epoch 130, training loss: 2.0425641536712646 = 1.3422706127166748 + 0.1 * 7.002934455871582
Epoch 130, val loss: 1.406320333480835
Epoch 140, training loss: 1.9408308267593384 = 1.2418605089187622 + 0.1 * 6.989703178405762
Epoch 140, val loss: 1.325567364692688
Epoch 150, training loss: 1.8386693000793457 = 1.140635371208191 + 0.1 * 6.9803385734558105
Epoch 150, val loss: 1.2450385093688965
Epoch 160, training loss: 1.7400226593017578 = 1.043664813041687 + 0.1 * 6.963579177856445
Epoch 160, val loss: 1.1702834367752075
Epoch 170, training loss: 1.6482152938842773 = 0.9529083967208862 + 0.1 * 6.953068733215332
Epoch 170, val loss: 1.101961612701416
Epoch 180, training loss: 1.5632174015045166 = 0.8700461387634277 + 0.1 * 6.931712627410889
Epoch 180, val loss: 1.0411511659622192
Epoch 190, training loss: 1.486931562423706 = 0.7948814630508423 + 0.1 * 6.920501708984375
Epoch 190, val loss: 0.9872733950614929
Epoch 200, training loss: 1.4190006256103516 = 0.7285301089286804 + 0.1 * 6.904705047607422
Epoch 200, val loss: 0.9414960741996765
Epoch 210, training loss: 1.358091950416565 = 0.6695288419723511 + 0.1 * 6.885631084442139
Epoch 210, val loss: 0.9028156399726868
Epoch 220, training loss: 1.3053035736083984 = 0.6171324253082275 + 0.1 * 6.881711959838867
Epoch 220, val loss: 0.8715264797210693
Epoch 230, training loss: 1.2570189237594604 = 0.570868194103241 + 0.1 * 6.861506938934326
Epoch 230, val loss: 0.8472996354103088
Epoch 240, training loss: 1.2148020267486572 = 0.5290265679359436 + 0.1 * 6.857755184173584
Epoch 240, val loss: 0.828652024269104
Epoch 250, training loss: 1.1757135391235352 = 0.4909982681274414 + 0.1 * 6.847153186798096
Epoch 250, val loss: 0.8144047856330872
Epoch 260, training loss: 1.1387031078338623 = 0.45548057556152344 + 0.1 * 6.832225799560547
Epoch 260, val loss: 0.8030860424041748
Epoch 270, training loss: 1.1036171913146973 = 0.4214590787887573 + 0.1 * 6.82158088684082
Epoch 270, val loss: 0.7939739227294922
Epoch 280, training loss: 1.0707639455795288 = 0.38864895701408386 + 0.1 * 6.821150302886963
Epoch 280, val loss: 0.7867546677589417
Epoch 290, training loss: 1.0388200283050537 = 0.35744887590408325 + 0.1 * 6.813710689544678
Epoch 290, val loss: 0.7813766598701477
Epoch 300, training loss: 1.0074338912963867 = 0.3276575803756714 + 0.1 * 6.797762870788574
Epoch 300, val loss: 0.7779155373573303
Epoch 310, training loss: 0.9782904386520386 = 0.29944127798080444 + 0.1 * 6.788491249084473
Epoch 310, val loss: 0.7764066457748413
Epoch 320, training loss: 0.9510596394538879 = 0.2728503942489624 + 0.1 * 6.782092094421387
Epoch 320, val loss: 0.7767494320869446
Epoch 330, training loss: 0.9260755777359009 = 0.24790635704994202 + 0.1 * 6.7816925048828125
Epoch 330, val loss: 0.7787625789642334
Epoch 340, training loss: 0.9013956785202026 = 0.22458837926387787 + 0.1 * 6.768072605133057
Epoch 340, val loss: 0.7821265459060669
Epoch 350, training loss: 0.8800842761993408 = 0.2028551697731018 + 0.1 * 6.7722907066345215
Epoch 350, val loss: 0.7868699431419373
Epoch 360, training loss: 0.8584033250808716 = 0.18290378153324127 + 0.1 * 6.754995346069336
Epoch 360, val loss: 0.7928375005722046
Epoch 370, training loss: 0.8395795226097107 = 0.16474121809005737 + 0.1 * 6.748383045196533
Epoch 370, val loss: 0.8000166416168213
Epoch 380, training loss: 0.8227688670158386 = 0.14841699600219727 + 0.1 * 6.743518829345703
Epoch 380, val loss: 0.8082925081253052
Epoch 390, training loss: 0.8071198463439941 = 0.13386723399162292 + 0.1 * 6.7325263023376465
Epoch 390, val loss: 0.8175876140594482
Epoch 400, training loss: 0.794111430644989 = 0.12097035348415375 + 0.1 * 6.731410503387451
Epoch 400, val loss: 0.8277585506439209
Epoch 410, training loss: 0.7812637090682983 = 0.10958241671323776 + 0.1 * 6.716812610626221
Epoch 410, val loss: 0.8385065793991089
Epoch 420, training loss: 0.7715171575546265 = 0.09950239211320877 + 0.1 * 6.720147609710693
Epoch 420, val loss: 0.849789023399353
Epoch 430, training loss: 0.7609333395957947 = 0.09058212488889694 + 0.1 * 6.703512191772461
Epoch 430, val loss: 0.8612777590751648
Epoch 440, training loss: 0.7540048956871033 = 0.0826682448387146 + 0.1 * 6.713366508483887
Epoch 440, val loss: 0.8729244470596313
Epoch 450, training loss: 0.7453986406326294 = 0.07565310597419739 + 0.1 * 6.697455406188965
Epoch 450, val loss: 0.8844631910324097
Epoch 460, training loss: 0.7379904985427856 = 0.06938862055540085 + 0.1 * 6.686018466949463
Epoch 460, val loss: 0.8960884213447571
Epoch 470, training loss: 0.7322893142700195 = 0.06379515677690506 + 0.1 * 6.68494176864624
Epoch 470, val loss: 0.9075864553451538
Epoch 480, training loss: 0.7262850999832153 = 0.05879988148808479 + 0.1 * 6.67485237121582
Epoch 480, val loss: 0.9188576340675354
Epoch 490, training loss: 0.7215905785560608 = 0.05431364104151726 + 0.1 * 6.672769069671631
Epoch 490, val loss: 0.9300411939620972
Epoch 500, training loss: 0.7173586487770081 = 0.05027556046843529 + 0.1 * 6.670830726623535
Epoch 500, val loss: 0.9410058856010437
Epoch 510, training loss: 0.7128972411155701 = 0.04663825035095215 + 0.1 * 6.6625895500183105
Epoch 510, val loss: 0.9517179131507874
Epoch 520, training loss: 0.710120677947998 = 0.04335685074329376 + 0.1 * 6.667638301849365
Epoch 520, val loss: 0.9621732234954834
Epoch 530, training loss: 0.7062304019927979 = 0.04039109870791435 + 0.1 * 6.658392906188965
Epoch 530, val loss: 0.9724836349487305
Epoch 540, training loss: 0.7029694318771362 = 0.03770337253808975 + 0.1 * 6.652660369873047
Epoch 540, val loss: 0.9824981689453125
Epoch 550, training loss: 0.6992478370666504 = 0.0352664478123188 + 0.1 * 6.639813423156738
Epoch 550, val loss: 0.9922980666160583
Epoch 560, training loss: 0.696781575679779 = 0.03305457904934883 + 0.1 * 6.637269496917725
Epoch 560, val loss: 1.0017762184143066
Epoch 570, training loss: 0.6942110061645508 = 0.031038912013173103 + 0.1 * 6.631720542907715
Epoch 570, val loss: 1.011127233505249
Epoch 580, training loss: 0.6921588182449341 = 0.029196668416261673 + 0.1 * 6.629621505737305
Epoch 580, val loss: 1.0202176570892334
Epoch 590, training loss: 0.6907357573509216 = 0.027510447427630424 + 0.1 * 6.6322526931762695
Epoch 590, val loss: 1.0290910005569458
Epoch 600, training loss: 0.6878361701965332 = 0.025967633351683617 + 0.1 * 6.618684768676758
Epoch 600, val loss: 1.0377134084701538
Epoch 610, training loss: 0.6864250898361206 = 0.024551913142204285 + 0.1 * 6.618731498718262
Epoch 610, val loss: 1.0461006164550781
Epoch 620, training loss: 0.6836903691291809 = 0.023250695317983627 + 0.1 * 6.604396820068359
Epoch 620, val loss: 1.0543187856674194
Epoch 630, training loss: 0.6817495226860046 = 0.02205023542046547 + 0.1 * 6.596992492675781
Epoch 630, val loss: 1.0623254776000977
Epoch 640, training loss: 0.6799612641334534 = 0.020942311733961105 + 0.1 * 6.590188980102539
Epoch 640, val loss: 1.0701408386230469
Epoch 650, training loss: 0.6801791787147522 = 0.01991499774158001 + 0.1 * 6.602641582489014
Epoch 650, val loss: 1.077851414680481
Epoch 660, training loss: 0.6798034310340881 = 0.018962005153298378 + 0.1 * 6.608414173126221
Epoch 660, val loss: 1.085384488105774
Epoch 670, training loss: 0.6770932078361511 = 0.018078988417983055 + 0.1 * 6.590142250061035
Epoch 670, val loss: 1.0927034616470337
Epoch 680, training loss: 0.6753222942352295 = 0.017257343977689743 + 0.1 * 6.580649375915527
Epoch 680, val loss: 1.0998626947402954
Epoch 690, training loss: 0.674898624420166 = 0.01649116538465023 + 0.1 * 6.5840744972229
Epoch 690, val loss: 1.1069684028625488
Epoch 700, training loss: 0.6731394529342651 = 0.01577608659863472 + 0.1 * 6.573633193969727
Epoch 700, val loss: 1.1138267517089844
Epoch 710, training loss: 0.6720240712165833 = 0.015110231004655361 + 0.1 * 6.569138050079346
Epoch 710, val loss: 1.1205081939697266
Epoch 720, training loss: 0.6720898747444153 = 0.014486543834209442 + 0.1 * 6.576033115386963
Epoch 720, val loss: 1.1271039247512817
Epoch 730, training loss: 0.6697474122047424 = 0.013903029263019562 + 0.1 * 6.558443546295166
Epoch 730, val loss: 1.1335657835006714
Epoch 740, training loss: 0.6712096929550171 = 0.013354744762182236 + 0.1 * 6.578549861907959
Epoch 740, val loss: 1.1398991346359253
Epoch 750, training loss: 0.6703475713729858 = 0.01283945795148611 + 0.1 * 6.5750813484191895
Epoch 750, val loss: 1.1460734605789185
Epoch 760, training loss: 0.6681626439094543 = 0.012356524355709553 + 0.1 * 6.558061122894287
Epoch 760, val loss: 1.1520841121673584
Epoch 770, training loss: 0.6680260300636292 = 0.011901337653398514 + 0.1 * 6.561246871948242
Epoch 770, val loss: 1.1580127477645874
Epoch 780, training loss: 0.6663162708282471 = 0.011472124606370926 + 0.1 * 6.548441410064697
Epoch 780, val loss: 1.1638339757919312
Epoch 790, training loss: 0.6664767861366272 = 0.01106629241257906 + 0.1 * 6.554104804992676
Epoch 790, val loss: 1.1695895195007324
Epoch 800, training loss: 0.6668917536735535 = 0.010683270171284676 + 0.1 * 6.562084197998047
Epoch 800, val loss: 1.175215721130371
Epoch 810, training loss: 0.6648808717727661 = 0.010321796871721745 + 0.1 * 6.545590877532959
Epoch 810, val loss: 1.1806796789169312
Epoch 820, training loss: 0.6651058197021484 = 0.00997934490442276 + 0.1 * 6.55126428604126
Epoch 820, val loss: 1.1861088275909424
Epoch 830, training loss: 0.6640610694885254 = 0.009654815308749676 + 0.1 * 6.54406213760376
Epoch 830, val loss: 1.1914116144180298
Epoch 840, training loss: 0.6647314429283142 = 0.009347202256321907 + 0.1 * 6.553842067718506
Epoch 840, val loss: 1.1966444253921509
Epoch 850, training loss: 0.6630423069000244 = 0.009055162779986858 + 0.1 * 6.539871692657471
Epoch 850, val loss: 1.201748013496399
Epoch 860, training loss: 0.6619912981987 = 0.008778133429586887 + 0.1 * 6.532131195068359
Epoch 860, val loss: 1.2067687511444092
Epoch 870, training loss: 0.6620934009552002 = 0.00851426087319851 + 0.1 * 6.535790920257568
Epoch 870, val loss: 1.2117382287979126
Epoch 880, training loss: 0.6610722541809082 = 0.008262784220278263 + 0.1 * 6.528094291687012
Epoch 880, val loss: 1.216634750366211
Epoch 890, training loss: 0.6620296239852905 = 0.00802311860024929 + 0.1 * 6.540064811706543
Epoch 890, val loss: 1.2214436531066895
Epoch 900, training loss: 0.6607983112335205 = 0.0077949389815330505 + 0.1 * 6.530034065246582
Epoch 900, val loss: 1.2261483669281006
Epoch 910, training loss: 0.6612781882286072 = 0.007577610667794943 + 0.1 * 6.53700590133667
Epoch 910, val loss: 1.230761170387268
Epoch 920, training loss: 0.6599234938621521 = 0.00736959045752883 + 0.1 * 6.525538921356201
Epoch 920, val loss: 1.235337257385254
Epoch 930, training loss: 0.6589252352714539 = 0.007171141915023327 + 0.1 * 6.517540454864502
Epoch 930, val loss: 1.2397745847702026
Epoch 940, training loss: 0.6595421433448792 = 0.0069816093891859055 + 0.1 * 6.525605201721191
Epoch 940, val loss: 1.2441680431365967
Epoch 950, training loss: 0.6587438583374023 = 0.006799811031669378 + 0.1 * 6.519440174102783
Epoch 950, val loss: 1.2485229969024658
Epoch 960, training loss: 0.6576350331306458 = 0.006625764537602663 + 0.1 * 6.510092735290527
Epoch 960, val loss: 1.2527989149093628
Epoch 970, training loss: 0.6591116189956665 = 0.006458905525505543 + 0.1 * 6.526527404785156
Epoch 970, val loss: 1.2570321559906006
Epoch 980, training loss: 0.6575651168823242 = 0.006299101747572422 + 0.1 * 6.512660026550293
Epoch 980, val loss: 1.261085867881775
Epoch 990, training loss: 0.6564974188804626 = 0.006146308500319719 + 0.1 * 6.50351095199585
Epoch 990, val loss: 1.2651177644729614
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.809229850769043 = 1.949552059173584 + 0.1 * 8.596778869628906
Epoch 0, val loss: 1.9542274475097656
Epoch 10, training loss: 2.7994601726531982 = 1.9397910833358765 + 0.1 * 8.596691131591797
Epoch 10, val loss: 1.9439830780029297
Epoch 20, training loss: 2.787626266479492 = 1.928015947341919 + 0.1 * 8.596101760864258
Epoch 20, val loss: 1.931549072265625
Epoch 30, training loss: 2.7704944610595703 = 1.9113117456436157 + 0.1 * 8.591827392578125
Epoch 30, val loss: 1.9139525890350342
Epoch 40, training loss: 2.7425718307495117 = 1.8863147497177124 + 0.1 * 8.562570571899414
Epoch 40, val loss: 1.887890338897705
Epoch 50, training loss: 2.6883833408355713 = 1.8511734008789062 + 0.1 * 8.372098922729492
Epoch 50, val loss: 1.8524898290634155
Epoch 60, training loss: 2.608839988708496 = 1.8116297721862793 + 0.1 * 7.972102165222168
Epoch 60, val loss: 1.8150500059127808
Epoch 70, training loss: 2.5275156497955322 = 1.7761818170547485 + 0.1 * 7.513338565826416
Epoch 70, val loss: 1.78279709815979
Epoch 80, training loss: 2.462588310241699 = 1.7396998405456543 + 0.1 * 7.228883266448975
Epoch 80, val loss: 1.7506170272827148
Epoch 90, training loss: 2.4064879417419434 = 1.6935429573059082 + 0.1 * 7.129449844360352
Epoch 90, val loss: 1.7097575664520264
Epoch 100, training loss: 2.3400959968566895 = 1.630646824836731 + 0.1 * 7.094491958618164
Epoch 100, val loss: 1.652925968170166
Epoch 110, training loss: 2.2559337615966797 = 1.5492591857910156 + 0.1 * 7.066746711730957
Epoch 110, val loss: 1.5812522172927856
Epoch 120, training loss: 2.1577987670898438 = 1.4542455673217773 + 0.1 * 7.0355305671691895
Epoch 120, val loss: 1.5001741647720337
Epoch 130, training loss: 2.0528323650360107 = 1.3523820638656616 + 0.1 * 7.00450325012207
Epoch 130, val loss: 1.4136662483215332
Epoch 140, training loss: 1.9452424049377441 = 1.247517704963684 + 0.1 * 6.977247714996338
Epoch 140, val loss: 1.3265626430511475
Epoch 150, training loss: 1.8378628492355347 = 1.142138123512268 + 0.1 * 6.957247257232666
Epoch 150, val loss: 1.2405352592468262
Epoch 160, training loss: 1.7346272468566895 = 1.0409024953842163 + 0.1 * 6.937246799468994
Epoch 160, val loss: 1.15977144241333
Epoch 170, training loss: 1.6372883319854736 = 0.9450861811637878 + 0.1 * 6.922021865844727
Epoch 170, val loss: 1.084294319152832
Epoch 180, training loss: 1.5479516983032227 = 0.8586851358413696 + 0.1 * 6.892665863037109
Epoch 180, val loss: 1.0171775817871094
Epoch 190, training loss: 1.470353364944458 = 0.7827205657958984 + 0.1 * 6.876328468322754
Epoch 190, val loss: 0.9593007564544678
Epoch 200, training loss: 1.4017503261566162 = 0.7179573178291321 + 0.1 * 6.8379292488098145
Epoch 200, val loss: 0.9113379120826721
Epoch 210, training loss: 1.3438074588775635 = 0.6620215773582458 + 0.1 * 6.817858695983887
Epoch 210, val loss: 0.8719165921211243
Epoch 220, training loss: 1.2924530506134033 = 0.6127843856811523 + 0.1 * 6.796687126159668
Epoch 220, val loss: 0.8398856520652771
Epoch 230, training loss: 1.2458827495574951 = 0.5673269629478455 + 0.1 * 6.785557270050049
Epoch 230, val loss: 0.8130421042442322
Epoch 240, training loss: 1.2022998332977295 = 0.5249777436256409 + 0.1 * 6.773221015930176
Epoch 240, val loss: 0.790907621383667
Epoch 250, training loss: 1.1603273153305054 = 0.48491692543029785 + 0.1 * 6.754103660583496
Epoch 250, val loss: 0.7726468443870544
Epoch 260, training loss: 1.1228617429733276 = 0.4466378390789032 + 0.1 * 6.762239456176758
Epoch 260, val loss: 0.7575871348381042
Epoch 270, training loss: 1.0838192701339722 = 0.4104008376598358 + 0.1 * 6.734184265136719
Epoch 270, val loss: 0.7452779412269592
Epoch 280, training loss: 1.0484685897827148 = 0.3759538531303406 + 0.1 * 6.725147247314453
Epoch 280, val loss: 0.7352529168128967
Epoch 290, training loss: 1.0153160095214844 = 0.34349045157432556 + 0.1 * 6.718255043029785
Epoch 290, val loss: 0.7273311614990234
Epoch 300, training loss: 0.9838314056396484 = 0.3130587339401245 + 0.1 * 6.70772647857666
Epoch 300, val loss: 0.7215983271598816
Epoch 310, training loss: 0.9551016688346863 = 0.28475409746170044 + 0.1 * 6.703475475311279
Epoch 310, val loss: 0.7181255221366882
Epoch 320, training loss: 0.928530752658844 = 0.25883203744888306 + 0.1 * 6.696987152099609
Epoch 320, val loss: 0.7169718146324158
Epoch 330, training loss: 0.9057196378707886 = 0.23520633578300476 + 0.1 * 6.705132961273193
Epoch 330, val loss: 0.7180639505386353
Epoch 340, training loss: 0.8822056651115417 = 0.21395134925842285 + 0.1 * 6.6825432777404785
Epoch 340, val loss: 0.7211362719535828
Epoch 350, training loss: 0.8625489473342896 = 0.1948055475950241 + 0.1 * 6.677433967590332
Epoch 350, val loss: 0.72611403465271
Epoch 360, training loss: 0.8471052050590515 = 0.1777079701423645 + 0.1 * 6.693972110748291
Epoch 360, val loss: 0.7325894236564636
Epoch 370, training loss: 0.8295665979385376 = 0.1624840497970581 + 0.1 * 6.670825481414795
Epoch 370, val loss: 0.7403396964073181
Epoch 380, training loss: 0.8142629265785217 = 0.14882762730121613 + 0.1 * 6.654353141784668
Epoch 380, val loss: 0.7493005394935608
Epoch 390, training loss: 0.8043617010116577 = 0.1365605741739273 + 0.1 * 6.678011417388916
Epoch 390, val loss: 0.7591560482978821
Epoch 400, training loss: 0.7907463312149048 = 0.12563057243824005 + 0.1 * 6.651157379150391
Epoch 400, val loss: 0.7695377469062805
Epoch 410, training loss: 0.7796883583068848 = 0.11581358313560486 + 0.1 * 6.638747692108154
Epoch 410, val loss: 0.7804120779037476
Epoch 420, training loss: 0.7702077627182007 = 0.10693153738975525 + 0.1 * 6.6327619552612305
Epoch 420, val loss: 0.7917104363441467
Epoch 430, training loss: 0.7653590440750122 = 0.0988721251487732 + 0.1 * 6.6648688316345215
Epoch 430, val loss: 0.8032937049865723
Epoch 440, training loss: 0.7541815042495728 = 0.09159266203641891 + 0.1 * 6.625888347625732
Epoch 440, val loss: 0.8149909377098083
Epoch 450, training loss: 0.7470412254333496 = 0.08496645092964172 + 0.1 * 6.6207475662231445
Epoch 450, val loss: 0.8268532156944275
Epoch 460, training loss: 0.7405851483345032 = 0.07892883569002151 + 0.1 * 6.616562843322754
Epoch 460, val loss: 0.8387594819068909
Epoch 470, training loss: 0.7339969873428345 = 0.07342389971017838 + 0.1 * 6.6057305335998535
Epoch 470, val loss: 0.8506664037704468
Epoch 480, training loss: 0.7289407849311829 = 0.06837405264377594 + 0.1 * 6.6056671142578125
Epoch 480, val loss: 0.8626317381858826
Epoch 490, training loss: 0.7242571115493774 = 0.06375022977590561 + 0.1 * 6.605069160461426
Epoch 490, val loss: 0.8744964599609375
Epoch 500, training loss: 0.7197778224945068 = 0.05952759459614754 + 0.1 * 6.602502346038818
Epoch 500, val loss: 0.8863089084625244
Epoch 510, training loss: 0.714674174785614 = 0.05564853176474571 + 0.1 * 6.590256690979004
Epoch 510, val loss: 0.8980522155761719
Epoch 520, training loss: 0.7122945785522461 = 0.05208619311451912 + 0.1 * 6.602083683013916
Epoch 520, val loss: 0.9097000360488892
Epoch 530, training loss: 0.7073581218719482 = 0.04881613701581955 + 0.1 * 6.585419654846191
Epoch 530, val loss: 0.9212324619293213
Epoch 540, training loss: 0.7039297223091125 = 0.045808300375938416 + 0.1 * 6.58121395111084
Epoch 540, val loss: 0.9326202273368835
Epoch 550, training loss: 0.7021644115447998 = 0.04304639250040054 + 0.1 * 6.591179847717285
Epoch 550, val loss: 0.9438343644142151
Epoch 560, training loss: 0.6978617310523987 = 0.04050492122769356 + 0.1 * 6.573567867279053
Epoch 560, val loss: 0.9549098014831543
Epoch 570, training loss: 0.6971656084060669 = 0.03815769776701927 + 0.1 * 6.590078830718994
Epoch 570, val loss: 0.9658475518226624
Epoch 580, training loss: 0.6941215395927429 = 0.035997603088617325 + 0.1 * 6.581239223480225
Epoch 580, val loss: 0.9765974283218384
Epoch 590, training loss: 0.693792462348938 = 0.03400405868887901 + 0.1 * 6.597884178161621
Epoch 590, val loss: 0.9870928525924683
Epoch 600, training loss: 0.687961220741272 = 0.032167524099349976 + 0.1 * 6.557936668395996
Epoch 600, val loss: 0.997385561466217
Epoch 610, training loss: 0.6863662004470825 = 0.03046613559126854 + 0.1 * 6.559000492095947
Epoch 610, val loss: 1.0074992179870605
Epoch 620, training loss: 0.6843717694282532 = 0.028887320309877396 + 0.1 * 6.554844379425049
Epoch 620, val loss: 1.0175224542617798
Epoch 630, training loss: 0.6832404732704163 = 0.02742045372724533 + 0.1 * 6.558199882507324
Epoch 630, val loss: 1.0273516178131104
Epoch 640, training loss: 0.681388795375824 = 0.026061324402689934 + 0.1 * 6.553274631500244
Epoch 640, val loss: 1.037024736404419
Epoch 650, training loss: 0.6793590188026428 = 0.02479458600282669 + 0.1 * 6.545644283294678
Epoch 650, val loss: 1.0464990139007568
Epoch 660, training loss: 0.6782169938087463 = 0.02361791580915451 + 0.1 * 6.545990467071533
Epoch 660, val loss: 1.0557435750961304
Epoch 670, training loss: 0.6772863864898682 = 0.02252560295164585 + 0.1 * 6.547607421875
Epoch 670, val loss: 1.0648458003997803
Epoch 680, training loss: 0.6764046549797058 = 0.02150450460612774 + 0.1 * 6.549001216888428
Epoch 680, val loss: 1.0738195180892944
Epoch 690, training loss: 0.6737291812896729 = 0.020551253110170364 + 0.1 * 6.5317792892456055
Epoch 690, val loss: 1.0825656652450562
Epoch 700, training loss: 0.6728574633598328 = 0.01966084912419319 + 0.1 * 6.531966209411621
Epoch 700, val loss: 1.0912123918533325
Epoch 710, training loss: 0.6727823615074158 = 0.018826259300112724 + 0.1 * 6.5395612716674805
Epoch 710, val loss: 1.099733591079712
Epoch 720, training loss: 0.6719018220901489 = 0.018043624237179756 + 0.1 * 6.538581848144531
Epoch 720, val loss: 1.1080347299575806
Epoch 730, training loss: 0.6693028211593628 = 0.01730881631374359 + 0.1 * 6.51993989944458
Epoch 730, val loss: 1.1162465810775757
Epoch 740, training loss: 0.6700373888015747 = 0.016617970541119576 + 0.1 * 6.534193992614746
Epoch 740, val loss: 1.1242907047271729
Epoch 750, training loss: 0.6696485877037048 = 0.015968676656484604 + 0.1 * 6.53679895401001
Epoch 750, val loss: 1.132132649421692
Epoch 760, training loss: 0.6677406430244446 = 0.015359455719590187 + 0.1 * 6.5238118171691895
Epoch 760, val loss: 1.1397873163223267
Epoch 770, training loss: 0.6665346026420593 = 0.01478531863540411 + 0.1 * 6.517492771148682
Epoch 770, val loss: 1.147374153137207
Epoch 780, training loss: 0.6672561764717102 = 0.014243541285395622 + 0.1 * 6.530126094818115
Epoch 780, val loss: 1.1547985076904297
Epoch 790, training loss: 0.6650386452674866 = 0.013732793740928173 + 0.1 * 6.513058662414551
Epoch 790, val loss: 1.1621215343475342
Epoch 800, training loss: 0.664276123046875 = 0.013248937204480171 + 0.1 * 6.510272026062012
Epoch 800, val loss: 1.1692917346954346
Epoch 810, training loss: 0.6635788679122925 = 0.012791993096470833 + 0.1 * 6.507868766784668
Epoch 810, val loss: 1.1763633489608765
Epoch 820, training loss: 0.665499210357666 = 0.012359094806015491 + 0.1 * 6.53140115737915
Epoch 820, val loss: 1.1832940578460693
Epoch 830, training loss: 0.6633008122444153 = 0.01194935105741024 + 0.1 * 6.513514518737793
Epoch 830, val loss: 1.1901156902313232
Epoch 840, training loss: 0.664065957069397 = 0.011560370214283466 + 0.1 * 6.525055885314941
Epoch 840, val loss: 1.196796178817749
Epoch 850, training loss: 0.6611419320106506 = 0.011191163212060928 + 0.1 * 6.499507427215576
Epoch 850, val loss: 1.2033226490020752
Epoch 860, training loss: 0.660068690776825 = 0.010840391740202904 + 0.1 * 6.492282390594482
Epoch 860, val loss: 1.209804654121399
Epoch 870, training loss: 0.663968026638031 = 0.010506706312298775 + 0.1 * 6.534613132476807
Epoch 870, val loss: 1.2161200046539307
Epoch 880, training loss: 0.6608076095581055 = 0.010190257802605629 + 0.1 * 6.506173610687256
Epoch 880, val loss: 1.2223176956176758
Epoch 890, training loss: 0.6591007709503174 = 0.009888635948300362 + 0.1 * 6.49212121963501
Epoch 890, val loss: 1.2284530401229858
Epoch 900, training loss: 0.6581532955169678 = 0.009600795805454254 + 0.1 * 6.485524654388428
Epoch 900, val loss: 1.2345216274261475
Epoch 910, training loss: 0.6600372195243835 = 0.009325725957751274 + 0.1 * 6.507114410400391
Epoch 910, val loss: 1.2405003309249878
Epoch 920, training loss: 0.6593338251113892 = 0.00906316377222538 + 0.1 * 6.502706527709961
Epoch 920, val loss: 1.2462830543518066
Epoch 930, training loss: 0.6574823260307312 = 0.008812732063233852 + 0.1 * 6.486695766448975
Epoch 930, val loss: 1.2520259618759155
Epoch 940, training loss: 0.6572933197021484 = 0.008573963306844234 + 0.1 * 6.487193584442139
Epoch 940, val loss: 1.2576920986175537
Epoch 950, training loss: 0.6568413376808167 = 0.008344397880136967 + 0.1 * 6.484969139099121
Epoch 950, val loss: 1.2632454633712769
Epoch 960, training loss: 0.656771183013916 = 0.008125536143779755 + 0.1 * 6.486456394195557
Epoch 960, val loss: 1.2686911821365356
Epoch 970, training loss: 0.6562855839729309 = 0.007915697060525417 + 0.1 * 6.483698844909668
Epoch 970, val loss: 1.2741209268569946
Epoch 980, training loss: 0.6554660201072693 = 0.007714581675827503 + 0.1 * 6.477514266967773
Epoch 980, val loss: 1.2793540954589844
Epoch 990, training loss: 0.6553966403007507 = 0.00752263842150569 + 0.1 * 6.4787397384643555
Epoch 990, val loss: 1.2845618724822998
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.7949233055114746 = 1.9352449178695679 + 0.1 * 8.596784591674805
Epoch 0, val loss: 1.9294723272323608
Epoch 10, training loss: 2.785217761993408 = 1.9255520105361938 + 0.1 * 8.596657752990723
Epoch 10, val loss: 1.9196736812591553
Epoch 20, training loss: 2.772956371307373 = 1.913372278213501 + 0.1 * 8.595840454101562
Epoch 20, val loss: 1.907362461090088
Epoch 30, training loss: 2.754727363586426 = 1.8958241939544678 + 0.1 * 8.589031219482422
Epoch 30, val loss: 1.8898907899856567
Epoch 40, training loss: 2.723247528076172 = 1.8695192337036133 + 0.1 * 8.53728199005127
Epoch 40, val loss: 1.8642774820327759
Epoch 50, training loss: 2.654639720916748 = 1.8343279361724854 + 0.1 * 8.203117370605469
Epoch 50, val loss: 1.8317229747772217
Epoch 60, training loss: 2.5799360275268555 = 1.797446370124817 + 0.1 * 7.824895858764648
Epoch 60, val loss: 1.799533724784851
Epoch 70, training loss: 2.5047342777252197 = 1.7619125843048096 + 0.1 * 7.42821741104126
Epoch 70, val loss: 1.7678006887435913
Epoch 80, training loss: 2.444453477859497 = 1.7212018966674805 + 0.1 * 7.232515335083008
Epoch 80, val loss: 1.7309319972991943
Epoch 90, training loss: 2.3778858184814453 = 1.6678119897842407 + 0.1 * 7.100738525390625
Epoch 90, val loss: 1.6822396516799927
Epoch 100, training loss: 2.298029661178589 = 1.5958181619644165 + 0.1 * 7.0221147537231445
Epoch 100, val loss: 1.6171443462371826
Epoch 110, training loss: 2.204202175140381 = 1.5070126056671143 + 0.1 * 6.971894264221191
Epoch 110, val loss: 1.5396344661712646
Epoch 120, training loss: 2.1026663780212402 = 1.4093210697174072 + 0.1 * 6.93345308303833
Epoch 120, val loss: 1.4568809270858765
Epoch 130, training loss: 2.002265453338623 = 1.3117458820343018 + 0.1 * 6.905195236206055
Epoch 130, val loss: 1.3783804178237915
Epoch 140, training loss: 1.9060289859771729 = 1.2179884910583496 + 0.1 * 6.880405426025391
Epoch 140, val loss: 1.3049795627593994
Epoch 150, training loss: 1.8163073062896729 = 1.1301087141036987 + 0.1 * 6.861985683441162
Epoch 150, val loss: 1.2387813329696655
Epoch 160, training loss: 1.7330117225646973 = 1.0486332178115845 + 0.1 * 6.843785762786865
Epoch 160, val loss: 1.1791967153549194
Epoch 170, training loss: 1.6562159061431885 = 0.9731071591377258 + 0.1 * 6.831087589263916
Epoch 170, val loss: 1.1252949237823486
Epoch 180, training loss: 1.5842431783676147 = 0.9026283621788025 + 0.1 * 6.816147804260254
Epoch 180, val loss: 1.0757828950881958
Epoch 190, training loss: 1.5181437730789185 = 0.8368281722068787 + 0.1 * 6.8131561279296875
Epoch 190, val loss: 1.0300085544586182
Epoch 200, training loss: 1.453993320465088 = 0.7747223377227783 + 0.1 * 6.792710304260254
Epoch 200, val loss: 0.9874258637428284
Epoch 210, training loss: 1.3929100036621094 = 0.7148841023445129 + 0.1 * 6.780258655548096
Epoch 210, val loss: 0.9471740126609802
Epoch 220, training loss: 1.3346045017242432 = 0.6575648188591003 + 0.1 * 6.770396709442139
Epoch 220, val loss: 0.9099920988082886
Epoch 230, training loss: 1.280505657196045 = 0.6042580008506775 + 0.1 * 6.762475967407227
Epoch 230, val loss: 0.8775705099105835
Epoch 240, training loss: 1.2299089431762695 = 0.5552541613578796 + 0.1 * 6.746548175811768
Epoch 240, val loss: 0.8504425883293152
Epoch 250, training loss: 1.1852350234985352 = 0.5099387168884277 + 0.1 * 6.752962589263916
Epoch 250, val loss: 0.8286003470420837
Epoch 260, training loss: 1.1416232585906982 = 0.4681708514690399 + 0.1 * 6.734524250030518
Epoch 260, val loss: 0.8115545511245728
Epoch 270, training loss: 1.100716233253479 = 0.42911309003829956 + 0.1 * 6.716031551361084
Epoch 270, val loss: 0.798373818397522
Epoch 280, training loss: 1.064377784729004 = 0.39224502444267273 + 0.1 * 6.721327781677246
Epoch 280, val loss: 0.7884181141853333
Epoch 290, training loss: 1.0281546115875244 = 0.3577656149864197 + 0.1 * 6.703890323638916
Epoch 290, val loss: 0.7813045978546143
Epoch 300, training loss: 0.9942876696586609 = 0.32537001371383667 + 0.1 * 6.689176559448242
Epoch 300, val loss: 0.7767232060432434
Epoch 310, training loss: 0.9642202854156494 = 0.2950498163700104 + 0.1 * 6.691704750061035
Epoch 310, val loss: 0.7744836211204529
Epoch 320, training loss: 0.9351457953453064 = 0.26717883348464966 + 0.1 * 6.679669380187988
Epoch 320, val loss: 0.774151623249054
Epoch 330, training loss: 0.9087194204330444 = 0.2417556345462799 + 0.1 * 6.669637680053711
Epoch 330, val loss: 0.7754473090171814
Epoch 340, training loss: 0.8842769861221313 = 0.21856354176998138 + 0.1 * 6.657134056091309
Epoch 340, val loss: 0.7782610058784485
Epoch 350, training loss: 0.8629467487335205 = 0.19755861163139343 + 0.1 * 6.653881072998047
Epoch 350, val loss: 0.7823816537857056
Epoch 360, training loss: 0.8436476588249207 = 0.1787392646074295 + 0.1 * 6.649084091186523
Epoch 360, val loss: 0.7876243591308594
Epoch 370, training loss: 0.8260983824729919 = 0.16186769306659698 + 0.1 * 6.642306804656982
Epoch 370, val loss: 0.7937994003295898
Epoch 380, training loss: 0.811710000038147 = 0.14679932594299316 + 0.1 * 6.649106502532959
Epoch 380, val loss: 0.8008121848106384
Epoch 390, training loss: 0.7957707047462463 = 0.13337285816669464 + 0.1 * 6.623978614807129
Epoch 390, val loss: 0.808545708656311
Epoch 400, training loss: 0.7830692529678345 = 0.12136194854974747 + 0.1 * 6.617072582244873
Epoch 400, val loss: 0.8170058131217957
Epoch 410, training loss: 0.7740079164505005 = 0.11060643196105957 + 0.1 * 6.63401460647583
Epoch 410, val loss: 0.8259837627410889
Epoch 420, training loss: 0.7617531418800354 = 0.10101485252380371 + 0.1 * 6.607382774353027
Epoch 420, val loss: 0.8352725505828857
Epoch 430, training loss: 0.7524532079696655 = 0.09243296831846237 + 0.1 * 6.6002020835876465
Epoch 430, val loss: 0.8448020815849304
Epoch 440, training loss: 0.743766188621521 = 0.0847308337688446 + 0.1 * 6.590353012084961
Epoch 440, val loss: 0.8545922040939331
Epoch 450, training loss: 0.7394348978996277 = 0.07778213918209076 + 0.1 * 6.616527557373047
Epoch 450, val loss: 0.864568293094635
Epoch 460, training loss: 0.7303153872489929 = 0.0715356096625328 + 0.1 * 6.587798118591309
Epoch 460, val loss: 0.8745834231376648
Epoch 470, training loss: 0.7241834402084351 = 0.06590129435062408 + 0.1 * 6.582821369171143
Epoch 470, val loss: 0.884617269039154
Epoch 480, training loss: 0.71860271692276 = 0.060795437544584274 + 0.1 * 6.578073024749756
Epoch 480, val loss: 0.894676148891449
Epoch 490, training loss: 0.714448094367981 = 0.05617724359035492 + 0.1 * 6.582708358764648
Epoch 490, val loss: 0.9046233892440796
Epoch 500, training loss: 0.7086300253868103 = 0.052010033279657364 + 0.1 * 6.56619930267334
Epoch 500, val loss: 0.914457380771637
Epoch 510, training loss: 0.7045498490333557 = 0.048244159668684006 + 0.1 * 6.563056468963623
Epoch 510, val loss: 0.924139678478241
Epoch 520, training loss: 0.7000597715377808 = 0.044832758605480194 + 0.1 * 6.55226993560791
Epoch 520, val loss: 0.9336208701133728
Epoch 530, training loss: 0.6969144344329834 = 0.04173945635557175 + 0.1 * 6.5517497062683105
Epoch 530, val loss: 0.942963182926178
Epoch 540, training loss: 0.694135308265686 = 0.038935400545597076 + 0.1 * 6.551998615264893
Epoch 540, val loss: 0.9520370960235596
Epoch 550, training loss: 0.690735936164856 = 0.036387138068675995 + 0.1 * 6.543488025665283
Epoch 550, val loss: 0.9609411358833313
Epoch 560, training loss: 0.6880044341087341 = 0.03406953811645508 + 0.1 * 6.539348602294922
Epoch 560, val loss: 0.9695934057235718
Epoch 570, training loss: 0.6857579946517944 = 0.03195899352431297 + 0.1 * 6.537990093231201
Epoch 570, val loss: 0.9779940247535706
Epoch 580, training loss: 0.6828335523605347 = 0.030030367895960808 + 0.1 * 6.528031826019287
Epoch 580, val loss: 0.9861920475959778
Epoch 590, training loss: 0.6814329624176025 = 0.028263740241527557 + 0.1 * 6.531692028045654
Epoch 590, val loss: 0.9941649436950684
Epoch 600, training loss: 0.6792933940887451 = 0.026647862046957016 + 0.1 * 6.526454925537109
Epoch 600, val loss: 1.0019197463989258
Epoch 610, training loss: 0.6773364543914795 = 0.02516305260360241 + 0.1 * 6.52173376083374
Epoch 610, val loss: 1.009446144104004
Epoch 620, training loss: 0.6759471893310547 = 0.02379491738975048 + 0.1 * 6.5215229988098145
Epoch 620, val loss: 1.0168015956878662
Epoch 630, training loss: 0.6737298369407654 = 0.02253444865345955 + 0.1 * 6.511953830718994
Epoch 630, val loss: 1.0239770412445068
Epoch 640, training loss: 0.6725456118583679 = 0.02137015014886856 + 0.1 * 6.511754512786865
Epoch 640, val loss: 1.030958890914917
Epoch 650, training loss: 0.6706042885780334 = 0.020294688642024994 + 0.1 * 6.503096103668213
Epoch 650, val loss: 1.037754774093628
Epoch 660, training loss: 0.671309232711792 = 0.01929793506860733 + 0.1 * 6.52011251449585
Epoch 660, val loss: 1.0443483591079712
Epoch 670, training loss: 0.6696395874023438 = 0.018375953659415245 + 0.1 * 6.512636661529541
Epoch 670, val loss: 1.0507880449295044
Epoch 680, training loss: 0.6678992509841919 = 0.017519202083349228 + 0.1 * 6.503800868988037
Epoch 680, val loss: 1.0570080280303955
Epoch 690, training loss: 0.6673835515975952 = 0.016720181331038475 + 0.1 * 6.506633758544922
Epoch 690, val loss: 1.0631197690963745
Epoch 700, training loss: 0.6657320261001587 = 0.01597541756927967 + 0.1 * 6.497566223144531
Epoch 700, val loss: 1.0690252780914307
Epoch 710, training loss: 0.6652773022651672 = 0.01528139691799879 + 0.1 * 6.4999589920043945
Epoch 710, val loss: 1.0747871398925781
Epoch 720, training loss: 0.6640976667404175 = 0.014634090475738049 + 0.1 * 6.494636058807373
Epoch 720, val loss: 1.0804201364517212
Epoch 730, training loss: 0.6627845168113708 = 0.014027261175215244 + 0.1 * 6.48757266998291
Epoch 730, val loss: 1.0859297513961792
Epoch 740, training loss: 0.6626107096672058 = 0.013457801192998886 + 0.1 * 6.49152946472168
Epoch 740, val loss: 1.0913325548171997
Epoch 750, training loss: 0.6618689894676208 = 0.012922743335366249 + 0.1 * 6.489462375640869
Epoch 750, val loss: 1.096614956855774
Epoch 760, training loss: 0.6615887880325317 = 0.01242134440690279 + 0.1 * 6.491673946380615
Epoch 760, val loss: 1.1017427444458008
Epoch 770, training loss: 0.6606178879737854 = 0.011950400657951832 + 0.1 * 6.486674785614014
Epoch 770, val loss: 1.1067781448364258
Epoch 780, training loss: 0.6595383286476135 = 0.011505909264087677 + 0.1 * 6.4803242683410645
Epoch 780, val loss: 1.1117037534713745
Epoch 790, training loss: 0.6593530178070068 = 0.011086884886026382 + 0.1 * 6.482661247253418
Epoch 790, val loss: 1.1165170669555664
Epoch 800, training loss: 0.6585714221000671 = 0.01069127768278122 + 0.1 * 6.478801250457764
Epoch 800, val loss: 1.121212363243103
Epoch 810, training loss: 0.6579849123954773 = 0.010319197550415993 + 0.1 * 6.476657390594482
Epoch 810, val loss: 1.1258131265640259
Epoch 820, training loss: 0.6577390432357788 = 0.009966586716473103 + 0.1 * 6.477724075317383
Epoch 820, val loss: 1.1303304433822632
Epoch 830, training loss: 0.656299352645874 = 0.009632973000407219 + 0.1 * 6.466663837432861
Epoch 830, val loss: 1.1347442865371704
Epoch 840, training loss: 0.6573158502578735 = 0.00931647140532732 + 0.1 * 6.47999382019043
Epoch 840, val loss: 1.1390936374664307
Epoch 850, training loss: 0.6557651162147522 = 0.009016594849526882 + 0.1 * 6.467485427856445
Epoch 850, val loss: 1.1433219909667969
Epoch 860, training loss: 0.6567147374153137 = 0.00873170793056488 + 0.1 * 6.479829788208008
Epoch 860, val loss: 1.1474865674972534
Epoch 870, training loss: 0.6550464630126953 = 0.008461310528218746 + 0.1 * 6.465851306915283
Epoch 870, val loss: 1.151506781578064
Epoch 880, training loss: 0.6544005870819092 = 0.008204394951462746 + 0.1 * 6.46196174621582
Epoch 880, val loss: 1.15549635887146
Epoch 890, training loss: 0.6548474431037903 = 0.00795970018953085 + 0.1 * 6.46887731552124
Epoch 890, val loss: 1.1593557596206665
Epoch 900, training loss: 0.6540693044662476 = 0.007726916112005711 + 0.1 * 6.463423728942871
Epoch 900, val loss: 1.1631722450256348
Epoch 910, training loss: 0.6542203426361084 = 0.007504992187023163 + 0.1 * 6.467153072357178
Epoch 910, val loss: 1.1668901443481445
Epoch 920, training loss: 0.6525466442108154 = 0.007293062750250101 + 0.1 * 6.452535629272461
Epoch 920, val loss: 1.1705456972122192
Epoch 930, training loss: 0.6527190804481506 = 0.007090644910931587 + 0.1 * 6.456284523010254
Epoch 930, val loss: 1.174117088317871
Epoch 940, training loss: 0.6527775526046753 = 0.006897271145135164 + 0.1 * 6.458802700042725
Epoch 940, val loss: 1.177647352218628
Epoch 950, training loss: 0.651595413684845 = 0.00671321339905262 + 0.1 * 6.448822021484375
Epoch 950, val loss: 1.1811007261276245
Epoch 960, training loss: 0.6518473029136658 = 0.006536835338920355 + 0.1 * 6.453104496002197
Epoch 960, val loss: 1.1845165491104126
Epoch 970, training loss: 0.6509910821914673 = 0.006367630325257778 + 0.1 * 6.446234703063965
Epoch 970, val loss: 1.187841773033142
Epoch 980, training loss: 0.6511033773422241 = 0.006205942947417498 + 0.1 * 6.448974609375
Epoch 980, val loss: 1.1911348104476929
Epoch 990, training loss: 0.6508620977401733 = 0.0060512651689350605 + 0.1 * 6.448108673095703
Epoch 990, val loss: 1.194370985031128
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8344754876120191
The final CL Acc:0.80617, 0.00873, The final GNN Acc:0.83500, 0.00043
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11554])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10452])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8188769817352295 = 1.9591926336288452 + 0.1 * 8.596843719482422
Epoch 0, val loss: 1.9577041864395142
Epoch 10, training loss: 2.8093011379241943 = 1.949622631072998 + 0.1 * 8.596784591674805
Epoch 10, val loss: 1.9486603736877441
Epoch 20, training loss: 2.797579765319824 = 1.9379346370697021 + 0.1 * 8.596451759338379
Epoch 20, val loss: 1.9372061491012573
Epoch 30, training loss: 2.7807769775390625 = 1.921416997909546 + 0.1 * 8.593598365783691
Epoch 30, val loss: 1.9206422567367554
Epoch 40, training loss: 2.7535452842712402 = 1.8965513706207275 + 0.1 * 8.569937705993652
Epoch 40, val loss: 1.895613193511963
Epoch 50, training loss: 2.705852508544922 = 1.8606486320495605 + 0.1 * 8.452038764953613
Epoch 50, val loss: 1.860641360282898
Epoch 60, training loss: 2.62684965133667 = 1.8189743757247925 + 0.1 * 8.078753471374512
Epoch 60, val loss: 1.8232115507125854
Epoch 70, training loss: 2.5720033645629883 = 1.784892201423645 + 0.1 * 7.8711113929748535
Epoch 70, val loss: 1.7951538562774658
Epoch 80, training loss: 2.5051817893981934 = 1.7548437118530273 + 0.1 * 7.503381252288818
Epoch 80, val loss: 1.7670320272445679
Epoch 90, training loss: 2.4423911571502686 = 1.7176417112350464 + 0.1 * 7.247494220733643
Epoch 90, val loss: 1.7305665016174316
Epoch 100, training loss: 2.379059076309204 = 1.668871521949768 + 0.1 * 7.101875305175781
Epoch 100, val loss: 1.684682846069336
Epoch 110, training loss: 2.3065261840820312 = 1.6041988134384155 + 0.1 * 7.0232744216918945
Epoch 110, val loss: 1.6264797449111938
Epoch 120, training loss: 2.222900152206421 = 1.5234854221343994 + 0.1 * 6.994146347045898
Epoch 120, val loss: 1.5552887916564941
Epoch 130, training loss: 2.12849497795105 = 1.4313966035842896 + 0.1 * 6.970983028411865
Epoch 130, val loss: 1.4761292934417725
Epoch 140, training loss: 2.0282695293426514 = 1.3335281610488892 + 0.1 * 6.947413921356201
Epoch 140, val loss: 1.3948785066604614
Epoch 150, training loss: 1.9262043237686157 = 1.2335331439971924 + 0.1 * 6.926711559295654
Epoch 150, val loss: 1.3131468296051025
Epoch 160, training loss: 1.82369065284729 = 1.13272225856781 + 0.1 * 6.909683704376221
Epoch 160, val loss: 1.232087254524231
Epoch 170, training loss: 1.7225358486175537 = 1.0330435037612915 + 0.1 * 6.894923686981201
Epoch 170, val loss: 1.1539102792739868
Epoch 180, training loss: 1.6272588968276978 = 0.9389852285385132 + 0.1 * 6.882736682891846
Epoch 180, val loss: 1.0829516649246216
Epoch 190, training loss: 1.5416896343231201 = 0.8544538021087646 + 0.1 * 6.872358798980713
Epoch 190, val loss: 1.02177894115448
Epoch 200, training loss: 1.464980959892273 = 0.7794310450553894 + 0.1 * 6.855498790740967
Epoch 200, val loss: 0.9698622822761536
Epoch 210, training loss: 1.3975470066070557 = 0.7133235931396484 + 0.1 * 6.842233180999756
Epoch 210, val loss: 0.9266852736473083
Epoch 220, training loss: 1.337541937828064 = 0.6549254655838013 + 0.1 * 6.826164722442627
Epoch 220, val loss: 0.8910338878631592
Epoch 230, training loss: 1.2841267585754395 = 0.6026579141616821 + 0.1 * 6.814688205718994
Epoch 230, val loss: 0.861477792263031
Epoch 240, training loss: 1.2361141443252563 = 0.5552744269371033 + 0.1 * 6.80839729309082
Epoch 240, val loss: 0.8366061449050903
Epoch 250, training loss: 1.1917200088500977 = 0.5112252235412598 + 0.1 * 6.8049468994140625
Epoch 250, val loss: 0.8154421448707581
Epoch 260, training loss: 1.1497187614440918 = 0.4698958396911621 + 0.1 * 6.798229217529297
Epoch 260, val loss: 0.7973150014877319
Epoch 270, training loss: 1.1097840070724487 = 0.4304109811782837 + 0.1 * 6.79373025894165
Epoch 270, val loss: 0.7816365957260132
Epoch 280, training loss: 1.0710176229476929 = 0.3919726014137268 + 0.1 * 6.790450096130371
Epoch 280, val loss: 0.7677544355392456
Epoch 290, training loss: 1.0337246656417847 = 0.3543534278869629 + 0.1 * 6.793712139129639
Epoch 290, val loss: 0.7552026510238647
Epoch 300, training loss: 0.9956296682357788 = 0.31763675808906555 + 0.1 * 6.779928684234619
Epoch 300, val loss: 0.7435299754142761
Epoch 310, training loss: 0.9595535397529602 = 0.2817331552505493 + 0.1 * 6.778203964233398
Epoch 310, val loss: 0.7330164313316345
Epoch 320, training loss: 0.9246691465377808 = 0.2472926825284958 + 0.1 * 6.773764610290527
Epoch 320, val loss: 0.7239773273468018
Epoch 330, training loss: 0.892693817615509 = 0.21540139615535736 + 0.1 * 6.772923946380615
Epoch 330, val loss: 0.7168479561805725
Epoch 340, training loss: 0.8630852103233337 = 0.18689022958278656 + 0.1 * 6.76194953918457
Epoch 340, val loss: 0.7122181057929993
Epoch 350, training loss: 0.8390337228775024 = 0.16201917827129364 + 0.1 * 6.770145416259766
Epoch 350, val loss: 0.7103112936019897
Epoch 360, training loss: 0.816027045249939 = 0.14097917079925537 + 0.1 * 6.750478744506836
Epoch 360, val loss: 0.7108331322669983
Epoch 370, training loss: 0.7974357008934021 = 0.12323988229036331 + 0.1 * 6.741958141326904
Epoch 370, val loss: 0.7140506505966187
Epoch 380, training loss: 0.7845322489738464 = 0.10829520970582962 + 0.1 * 6.7623701095581055
Epoch 380, val loss: 0.7192558646202087
Epoch 390, training loss: 0.7685118913650513 = 0.0958109200000763 + 0.1 * 6.727009296417236
Epoch 390, val loss: 0.7257091403007507
Epoch 400, training loss: 0.7567938566207886 = 0.08525136113166809 + 0.1 * 6.71542501449585
Epoch 400, val loss: 0.7336089015007019
Epoch 410, training loss: 0.7470652461051941 = 0.07626907527446747 + 0.1 * 6.707961559295654
Epoch 410, val loss: 0.7424271702766418
Epoch 420, training loss: 0.7396278381347656 = 0.06863927096128464 + 0.1 * 6.709885597229004
Epoch 420, val loss: 0.75125652551651
Epoch 430, training loss: 0.7309467196464539 = 0.06206633895635605 + 0.1 * 6.688803672790527
Epoch 430, val loss: 0.7608523964881897
Epoch 440, training loss: 0.727674663066864 = 0.056353840976953506 + 0.1 * 6.713207721710205
Epoch 440, val loss: 0.7706198692321777
Epoch 450, training loss: 0.7199613451957703 = 0.05143429711461067 + 0.1 * 6.685270309448242
Epoch 450, val loss: 0.7804074883460999
Epoch 460, training loss: 0.7145875692367554 = 0.047145914286375046 + 0.1 * 6.674416542053223
Epoch 460, val loss: 0.7902323007583618
Epoch 470, training loss: 0.7095281481742859 = 0.04336163029074669 + 0.1 * 6.661665439605713
Epoch 470, val loss: 0.7999663352966309
Epoch 480, training loss: 0.7063210010528564 = 0.04002013057470322 + 0.1 * 6.663008689880371
Epoch 480, val loss: 0.8100746273994446
Epoch 490, training loss: 0.7023296356201172 = 0.03707512468099594 + 0.1 * 6.65254545211792
Epoch 490, val loss: 0.8193524479866028
Epoch 500, training loss: 0.6988398432731628 = 0.03444870188832283 + 0.1 * 6.643911361694336
Epoch 500, val loss: 0.8290002346038818
Epoch 510, training loss: 0.6977839469909668 = 0.03209333121776581 + 0.1 * 6.6569061279296875
Epoch 510, val loss: 0.8383617997169495
Epoch 520, training loss: 0.6938430666923523 = 0.02998553402721882 + 0.1 * 6.638575077056885
Epoch 520, val loss: 0.8476136922836304
Epoch 530, training loss: 0.691148042678833 = 0.028089255094528198 + 0.1 * 6.630588054656982
Epoch 530, val loss: 0.8565603494644165
Epoch 540, training loss: 0.6893646121025085 = 0.02636965923011303 + 0.1 * 6.629949569702148
Epoch 540, val loss: 0.8655847907066345
Epoch 550, training loss: 0.686622679233551 = 0.02480786293745041 + 0.1 * 6.618147850036621
Epoch 550, val loss: 0.874374270439148
Epoch 560, training loss: 0.6862962245941162 = 0.023390060290694237 + 0.1 * 6.629061698913574
Epoch 560, val loss: 0.8828722834587097
Epoch 570, training loss: 0.6834243535995483 = 0.022099576890468597 + 0.1 * 6.613247871398926
Epoch 570, val loss: 0.8913065791130066
Epoch 580, training loss: 0.6819219589233398 = 0.020919693633913994 + 0.1 * 6.61002254486084
Epoch 580, val loss: 0.8993538022041321
Epoch 590, training loss: 0.6801860928535461 = 0.019831785932183266 + 0.1 * 6.603542804718018
Epoch 590, val loss: 0.9075201153755188
Epoch 600, training loss: 0.679020881652832 = 0.01883069984614849 + 0.1 * 6.601901531219482
Epoch 600, val loss: 0.9156303405761719
Epoch 610, training loss: 0.678435206413269 = 0.017912505194544792 + 0.1 * 6.605226993560791
Epoch 610, val loss: 0.9231804609298706
Epoch 620, training loss: 0.6761608719825745 = 0.017064262181520462 + 0.1 * 6.590965747833252
Epoch 620, val loss: 0.9307824373245239
Epoch 630, training loss: 0.676039457321167 = 0.01627667061984539 + 0.1 * 6.597628116607666
Epoch 630, val loss: 0.9381206631660461
Epoch 640, training loss: 0.6753422021865845 = 0.015546703711152077 + 0.1 * 6.597955226898193
Epoch 640, val loss: 0.945663332939148
Epoch 650, training loss: 0.6737554669380188 = 0.014868708327412605 + 0.1 * 6.5888671875
Epoch 650, val loss: 0.9525282979011536
Epoch 660, training loss: 0.6724259853363037 = 0.014236331917345524 + 0.1 * 6.5818963050842285
Epoch 660, val loss: 0.9597195386886597
Epoch 670, training loss: 0.6716533899307251 = 0.013645843602716923 + 0.1 * 6.580075740814209
Epoch 670, val loss: 0.9665740728378296
Epoch 680, training loss: 0.672410249710083 = 0.013095022179186344 + 0.1 * 6.593152046203613
Epoch 680, val loss: 0.9736253619194031
Epoch 690, training loss: 0.6695637106895447 = 0.012581413611769676 + 0.1 * 6.569822788238525
Epoch 690, val loss: 0.979901134967804
Epoch 700, training loss: 0.6719571948051453 = 0.01209823414683342 + 0.1 * 6.5985894203186035
Epoch 700, val loss: 0.9864444136619568
Epoch 710, training loss: 0.6689810156822205 = 0.011646111495792866 + 0.1 * 6.5733489990234375
Epoch 710, val loss: 0.9929623007774353
Epoch 720, training loss: 0.6673756837844849 = 0.01122161466628313 + 0.1 * 6.561540603637695
Epoch 720, val loss: 0.9989376664161682
Epoch 730, training loss: 0.6682071685791016 = 0.010820464231073856 + 0.1 * 6.573866844177246
Epoch 730, val loss: 1.0054789781570435
Epoch 740, training loss: 0.6666670441627502 = 0.010444793850183487 + 0.1 * 6.562222003936768
Epoch 740, val loss: 1.0113136768341064
Epoch 750, training loss: 0.6662074327468872 = 0.010089704766869545 + 0.1 * 6.5611772537231445
Epoch 750, val loss: 1.0170660018920898
Epoch 760, training loss: 0.6655147075653076 = 0.009753268212080002 + 0.1 * 6.557614326477051
Epoch 760, val loss: 1.0230733156204224
Epoch 770, training loss: 0.6641864776611328 = 0.009436210617423058 + 0.1 * 6.547502517700195
Epoch 770, val loss: 1.0289007425308228
Epoch 780, training loss: 0.6630456447601318 = 0.009136704728007317 + 0.1 * 6.539089679718018
Epoch 780, val loss: 1.0342962741851807
Epoch 790, training loss: 0.6648243069648743 = 0.008852113969624043 + 0.1 * 6.55972146987915
Epoch 790, val loss: 1.039951205253601
Epoch 800, training loss: 0.6619377136230469 = 0.00858214683830738 + 0.1 * 6.533555030822754
Epoch 800, val loss: 1.0455161333084106
Epoch 810, training loss: 0.6612684726715088 = 0.008327321149408817 + 0.1 * 6.529411315917969
Epoch 810, val loss: 1.050579309463501
Epoch 820, training loss: 0.6631856560707092 = 0.00808373000472784 + 0.1 * 6.551019191741943
Epoch 820, val loss: 1.0558972358703613
Epoch 830, training loss: 0.661339282989502 = 0.007852902635931969 + 0.1 * 6.534863471984863
Epoch 830, val loss: 1.0614689588546753
Epoch 840, training loss: 0.6603023409843445 = 0.007634666282683611 + 0.1 * 6.526677131652832
Epoch 840, val loss: 1.0661907196044922
Epoch 850, training loss: 0.6595748066902161 = 0.007425673305988312 + 0.1 * 6.521491527557373
Epoch 850, val loss: 1.0711010694503784
Epoch 860, training loss: 0.6584033370018005 = 0.007226010784506798 + 0.1 * 6.511773586273193
Epoch 860, val loss: 1.0761566162109375
Epoch 870, training loss: 0.6598837375640869 = 0.007035654969513416 + 0.1 * 6.528480529785156
Epoch 870, val loss: 1.0810160636901855
Epoch 880, training loss: 0.6587803363800049 = 0.006853513885289431 + 0.1 * 6.519268035888672
Epoch 880, val loss: 1.0859040021896362
Epoch 890, training loss: 0.6581396460533142 = 0.006679707672446966 + 0.1 * 6.514599323272705
Epoch 890, val loss: 1.090664267539978
Epoch 900, training loss: 0.6557568907737732 = 0.006514076143503189 + 0.1 * 6.492428302764893
Epoch 900, val loss: 1.0951682329177856
Epoch 910, training loss: 0.6574929356575012 = 0.006355660036206245 + 0.1 * 6.5113725662231445
Epoch 910, val loss: 1.0996583700180054
Epoch 920, training loss: 0.6566767692565918 = 0.006202647928148508 + 0.1 * 6.504741191864014
Epoch 920, val loss: 1.1043586730957031
Epoch 930, training loss: 0.6544433832168579 = 0.0060569592751562595 + 0.1 * 6.4838643074035645
Epoch 930, val loss: 1.1085995435714722
Epoch 940, training loss: 0.6553853750228882 = 0.005916765425354242 + 0.1 * 6.494685649871826
Epoch 940, val loss: 1.1129982471466064
Epoch 950, training loss: 0.6537354588508606 = 0.005781486630439758 + 0.1 * 6.47953987121582
Epoch 950, val loss: 1.1175150871276855
Epoch 960, training loss: 0.6544560194015503 = 0.005652626510709524 + 0.1 * 6.488034248352051
Epoch 960, val loss: 1.121600866317749
Epoch 970, training loss: 0.6524747014045715 = 0.005527858156710863 + 0.1 * 6.469468116760254
Epoch 970, val loss: 1.1257083415985107
Epoch 980, training loss: 0.6551367044448853 = 0.005407737568020821 + 0.1 * 6.497289657592773
Epoch 980, val loss: 1.1299464702606201
Epoch 990, training loss: 0.6531580686569214 = 0.005292223766446114 + 0.1 * 6.478658199310303
Epoch 990, val loss: 1.1340757608413696
Epoch 1000, training loss: 0.653910756111145 = 0.005181442946195602 + 0.1 * 6.487293243408203
Epoch 1000, val loss: 1.137993574142456
Epoch 1010, training loss: 0.6516084671020508 = 0.005074303597211838 + 0.1 * 6.465341091156006
Epoch 1010, val loss: 1.1418761014938354
Epoch 1020, training loss: 0.6517236828804016 = 0.004971053451299667 + 0.1 * 6.467526435852051
Epoch 1020, val loss: 1.1457264423370361
Epoch 1030, training loss: 0.6515777707099915 = 0.004871130455285311 + 0.1 * 6.467066287994385
Epoch 1030, val loss: 1.1496001482009888
Epoch 1040, training loss: 0.6509431600570679 = 0.004774224478751421 + 0.1 * 6.461689472198486
Epoch 1040, val loss: 1.15365731716156
Epoch 1050, training loss: 0.650894820690155 = 0.004681593272835016 + 0.1 * 6.462132453918457
Epoch 1050, val loss: 1.1572471857070923
Epoch 1060, training loss: 0.6503814458847046 = 0.0045918733812868595 + 0.1 * 6.457895278930664
Epoch 1060, val loss: 1.1606155633926392
Epoch 1070, training loss: 0.6512231826782227 = 0.004504857584834099 + 0.1 * 6.4671831130981445
Epoch 1070, val loss: 1.1644032001495361
Epoch 1080, training loss: 0.6493289470672607 = 0.004420258104801178 + 0.1 * 6.449086666107178
Epoch 1080, val loss: 1.1682538986206055
Epoch 1090, training loss: 0.6484369039535522 = 0.004339056555181742 + 0.1 * 6.440978527069092
Epoch 1090, val loss: 1.1716283559799194
Epoch 1100, training loss: 0.6516062021255493 = 0.004260348156094551 + 0.1 * 6.473458290100098
Epoch 1100, val loss: 1.1750984191894531
Epoch 1110, training loss: 0.6488434672355652 = 0.004183444660156965 + 0.1 * 6.446599960327148
Epoch 1110, val loss: 1.1788419485092163
Epoch 1120, training loss: 0.6497557163238525 = 0.004109859000891447 + 0.1 * 6.456458568572998
Epoch 1120, val loss: 1.1821637153625488
Epoch 1130, training loss: 0.6486345529556274 = 0.0040378752164542675 + 0.1 * 6.445966720581055
Epoch 1130, val loss: 1.1857296228408813
Epoch 1140, training loss: 0.6480035185813904 = 0.003968775272369385 + 0.1 * 6.440347194671631
Epoch 1140, val loss: 1.1888861656188965
Epoch 1150, training loss: 0.648717999458313 = 0.0039014220237731934 + 0.1 * 6.448165416717529
Epoch 1150, val loss: 1.1920933723449707
Epoch 1160, training loss: 0.6476627588272095 = 0.0038361623883247375 + 0.1 * 6.438265800476074
Epoch 1160, val loss: 1.1953215599060059
Epoch 1170, training loss: 0.6474135518074036 = 0.0037726033478975296 + 0.1 * 6.4364094734191895
Epoch 1170, val loss: 1.1985706090927124
Epoch 1180, training loss: 0.6485099196434021 = 0.0037108352407813072 + 0.1 * 6.447990894317627
Epoch 1180, val loss: 1.2018647193908691
Epoch 1190, training loss: 0.6464409828186035 = 0.003650775644928217 + 0.1 * 6.427901744842529
Epoch 1190, val loss: 1.20509934425354
Epoch 1200, training loss: 0.6460676193237305 = 0.003592786844819784 + 0.1 * 6.424748420715332
Epoch 1200, val loss: 1.2080250978469849
Epoch 1210, training loss: 0.6484983563423157 = 0.003536317264661193 + 0.1 * 6.449619770050049
Epoch 1210, val loss: 1.2110636234283447
Epoch 1220, training loss: 0.6467381715774536 = 0.003481112653389573 + 0.1 * 6.432570934295654
Epoch 1220, val loss: 1.2141857147216797
Epoch 1230, training loss: 0.6467950344085693 = 0.0034275902435183525 + 0.1 * 6.433674335479736
Epoch 1230, val loss: 1.2173081636428833
Epoch 1240, training loss: 0.6457858085632324 = 0.0033755775075405836 + 0.1 * 6.424101829528809
Epoch 1240, val loss: 1.220231533050537
Epoch 1250, training loss: 0.6473706960678101 = 0.0033248444087803364 + 0.1 * 6.440458297729492
Epoch 1250, val loss: 1.2231649160385132
Epoch 1260, training loss: 0.6456024646759033 = 0.003275608643889427 + 0.1 * 6.4232683181762695
Epoch 1260, val loss: 1.2260223627090454
Epoch 1270, training loss: 0.6460686326026917 = 0.003227683249861002 + 0.1 * 6.428409099578857
Epoch 1270, val loss: 1.2288275957107544
Epoch 1280, training loss: 0.6444399952888489 = 0.0031810493674129248 + 0.1 * 6.4125895500183105
Epoch 1280, val loss: 1.231651782989502
Epoch 1290, training loss: 0.6452877521514893 = 0.003135836683213711 + 0.1 * 6.421518802642822
Epoch 1290, val loss: 1.2342396974563599
Epoch 1300, training loss: 0.6439879536628723 = 0.0030914428643882275 + 0.1 * 6.408965110778809
Epoch 1300, val loss: 1.2370944023132324
Epoch 1310, training loss: 0.6460897326469421 = 0.0030483289156109095 + 0.1 * 6.430414199829102
Epoch 1310, val loss: 1.2398654222488403
Epoch 1320, training loss: 0.6438772082328796 = 0.0030062070582062006 + 0.1 * 6.40871000289917
Epoch 1320, val loss: 1.2426153421401978
Epoch 1330, training loss: 0.6448783874511719 = 0.002965243300423026 + 0.1 * 6.419131278991699
Epoch 1330, val loss: 1.2452634572982788
Epoch 1340, training loss: 0.643706738948822 = 0.002925150329247117 + 0.1 * 6.407815456390381
Epoch 1340, val loss: 1.2478641271591187
Epoch 1350, training loss: 0.6456458568572998 = 0.002886116737499833 + 0.1 * 6.4275970458984375
Epoch 1350, val loss: 1.2504442930221558
Epoch 1360, training loss: 0.6437699198722839 = 0.002848200034350157 + 0.1 * 6.40921688079834
Epoch 1360, val loss: 1.2530871629714966
Epoch 1370, training loss: 0.643795907497406 = 0.0028112030122429132 + 0.1 * 6.409847259521484
Epoch 1370, val loss: 1.255456805229187
Epoch 1380, training loss: 0.6424130201339722 = 0.002774890512228012 + 0.1 * 6.39638090133667
Epoch 1380, val loss: 1.2581329345703125
Epoch 1390, training loss: 0.6453272104263306 = 0.002739447634667158 + 0.1 * 6.425877571105957
Epoch 1390, val loss: 1.260624885559082
Epoch 1400, training loss: 0.6424016356468201 = 0.0027047747280448675 + 0.1 * 6.396968364715576
Epoch 1400, val loss: 1.2632997035980225
Epoch 1410, training loss: 0.6431211233139038 = 0.0026710983365774155 + 0.1 * 6.4045000076293945
Epoch 1410, val loss: 1.26558518409729
Epoch 1420, training loss: 0.6417660713195801 = 0.002638084813952446 + 0.1 * 6.391279697418213
Epoch 1420, val loss: 1.2679604291915894
Epoch 1430, training loss: 0.6446861028671265 = 0.0026058226358145475 + 0.1 * 6.420803070068359
Epoch 1430, val loss: 1.2705209255218506
Epoch 1440, training loss: 0.6420454978942871 = 0.0025740540586411953 + 0.1 * 6.39471435546875
Epoch 1440, val loss: 1.27308988571167
Epoch 1450, training loss: 0.6417401432991028 = 0.0025432833936065435 + 0.1 * 6.391968727111816
Epoch 1450, val loss: 1.275288701057434
Epoch 1460, training loss: 0.642823338508606 = 0.0025132091250270605 + 0.1 * 6.403100967407227
Epoch 1460, val loss: 1.277533769607544
Epoch 1470, training loss: 0.6436284184455872 = 0.002483608666807413 + 0.1 * 6.411448001861572
Epoch 1470, val loss: 1.2801172733306885
Epoch 1480, training loss: 0.6413552165031433 = 0.0024547132197767496 + 0.1 * 6.389005184173584
Epoch 1480, val loss: 1.2823833227157593
Epoch 1490, training loss: 0.6423590779304504 = 0.002426520921289921 + 0.1 * 6.399325847625732
Epoch 1490, val loss: 1.2846581935882568
Epoch 1500, training loss: 0.6411125063896179 = 0.0023988147731870413 + 0.1 * 6.387136936187744
Epoch 1500, val loss: 1.2869434356689453
Epoch 1510, training loss: 0.6415188908576965 = 0.002371665323153138 + 0.1 * 6.391472339630127
Epoch 1510, val loss: 1.2892744541168213
Epoch 1520, training loss: 0.6411889791488647 = 0.0023451736196875572 + 0.1 * 6.3884382247924805
Epoch 1520, val loss: 1.2916141748428345
Epoch 1530, training loss: 0.6405782699584961 = 0.002319063525646925 + 0.1 * 6.38259220123291
Epoch 1530, val loss: 1.2939231395721436
Epoch 1540, training loss: 0.6413539052009583 = 0.002293726895004511 + 0.1 * 6.390601634979248
Epoch 1540, val loss: 1.2960150241851807
Epoch 1550, training loss: 0.6409270763397217 = 0.0022688210010528564 + 0.1 * 6.386582851409912
Epoch 1550, val loss: 1.2981112003326416
Epoch 1560, training loss: 0.6409761905670166 = 0.002244312781840563 + 0.1 * 6.3873186111450195
Epoch 1560, val loss: 1.3004390001296997
Epoch 1570, training loss: 0.6407831907272339 = 0.00222042971290648 + 0.1 * 6.385627269744873
Epoch 1570, val loss: 1.3026584386825562
Epoch 1580, training loss: 0.6402722001075745 = 0.0021968907676637173 + 0.1 * 6.380753040313721
Epoch 1580, val loss: 1.3048020601272583
Epoch 1590, training loss: 0.6401748061180115 = 0.0021739385556429625 + 0.1 * 6.380008220672607
Epoch 1590, val loss: 1.3067744970321655
Epoch 1600, training loss: 0.6397016644477844 = 0.0021513563115149736 + 0.1 * 6.375503063201904
Epoch 1600, val loss: 1.3089455366134644
Epoch 1610, training loss: 0.6413583159446716 = 0.0021292115561664104 + 0.1 * 6.392291069030762
Epoch 1610, val loss: 1.3112066984176636
Epoch 1620, training loss: 0.639735758304596 = 0.002107391133904457 + 0.1 * 6.376283645629883
Epoch 1620, val loss: 1.3132264614105225
Epoch 1630, training loss: 0.6403675675392151 = 0.00208601588383317 + 0.1 * 6.382815361022949
Epoch 1630, val loss: 1.3154196739196777
Epoch 1640, training loss: 0.639766275882721 = 0.0020650506485253572 + 0.1 * 6.377012252807617
Epoch 1640, val loss: 1.317362904548645
Epoch 1650, training loss: 0.6406744122505188 = 0.0020445319823920727 + 0.1 * 6.386298656463623
Epoch 1650, val loss: 1.3193080425262451
Epoch 1660, training loss: 0.6386563777923584 = 0.002024291781708598 + 0.1 * 6.366320610046387
Epoch 1660, val loss: 1.3214243650436401
Epoch 1670, training loss: 0.6403323411941528 = 0.0020046180579811335 + 0.1 * 6.38327693939209
Epoch 1670, val loss: 1.3233250379562378
Epoch 1680, training loss: 0.6403868794441223 = 0.001985016046091914 + 0.1 * 6.384018421173096
Epoch 1680, val loss: 1.3253872394561768
Epoch 1690, training loss: 0.6392250657081604 = 0.0019659195095300674 + 0.1 * 6.372591018676758
Epoch 1690, val loss: 1.3273557424545288
Epoch 1700, training loss: 0.6400476694107056 = 0.001947176642715931 + 0.1 * 6.381004810333252
Epoch 1700, val loss: 1.329391598701477
Epoch 1710, training loss: 0.6383136510848999 = 0.0019286912865936756 + 0.1 * 6.363849639892578
Epoch 1710, val loss: 1.3311747312545776
Epoch 1720, training loss: 0.6396147012710571 = 0.0019106455147266388 + 0.1 * 6.377040863037109
Epoch 1720, val loss: 1.3330132961273193
Epoch 1730, training loss: 0.6390792727470398 = 0.0018928463105112314 + 0.1 * 6.371863842010498
Epoch 1730, val loss: 1.3350797891616821
Epoch 1740, training loss: 0.6400132775306702 = 0.0018753053154796362 + 0.1 * 6.381380081176758
Epoch 1740, val loss: 1.3369526863098145
Epoch 1750, training loss: 0.6384570598602295 = 0.0018580330070108175 + 0.1 * 6.365990161895752
Epoch 1750, val loss: 1.3389544486999512
Epoch 1760, training loss: 0.6392892003059387 = 0.0018411383498460054 + 0.1 * 6.374480724334717
Epoch 1760, val loss: 1.3407111167907715
Epoch 1770, training loss: 0.6379850506782532 = 0.001824503648094833 + 0.1 * 6.361605167388916
Epoch 1770, val loss: 1.3426371812820435
Epoch 1780, training loss: 0.6389952898025513 = 0.0018081654561683536 + 0.1 * 6.371870994567871
Epoch 1780, val loss: 1.3444305658340454
Epoch 1790, training loss: 0.6380299925804138 = 0.0017920234240591526 + 0.1 * 6.362379550933838
Epoch 1790, val loss: 1.3463377952575684
Epoch 1800, training loss: 0.6396450996398926 = 0.001776296878233552 + 0.1 * 6.378688335418701
Epoch 1800, val loss: 1.3480693101882935
Epoch 1810, training loss: 0.6379802823066711 = 0.0017606158507987857 + 0.1 * 6.362196445465088
Epoch 1810, val loss: 1.3501023054122925
Epoch 1820, training loss: 0.6383824348449707 = 0.0017454023472964764 + 0.1 * 6.36637020111084
Epoch 1820, val loss: 1.3516796827316284
Epoch 1830, training loss: 0.637367844581604 = 0.0017303373897448182 + 0.1 * 6.356375217437744
Epoch 1830, val loss: 1.3535537719726562
Epoch 1840, training loss: 0.6386317014694214 = 0.0017155694076791406 + 0.1 * 6.369161605834961
Epoch 1840, val loss: 1.3553510904312134
Epoch 1850, training loss: 0.6377792358398438 = 0.0017008483409881592 + 0.1 * 6.36078405380249
Epoch 1850, val loss: 1.3572041988372803
Epoch 1860, training loss: 0.6375381350517273 = 0.001686565694399178 + 0.1 * 6.35851526260376
Epoch 1860, val loss: 1.358961820602417
Epoch 1870, training loss: 0.6382216215133667 = 0.001672368380241096 + 0.1 * 6.365492343902588
Epoch 1870, val loss: 1.3607333898544312
Epoch 1880, training loss: 0.6373825073242188 = 0.0016584808472543955 + 0.1 * 6.357240200042725
Epoch 1880, val loss: 1.362565279006958
Epoch 1890, training loss: 0.63727867603302 = 0.0016449018148705363 + 0.1 * 6.356337547302246
Epoch 1890, val loss: 1.3640073537826538
Epoch 1900, training loss: 0.6368277072906494 = 0.0016314600361511111 + 0.1 * 6.351962566375732
Epoch 1900, val loss: 1.365757703781128
Epoch 1910, training loss: 0.6378782987594604 = 0.001618194510228932 + 0.1 * 6.362600803375244
Epoch 1910, val loss: 1.3675047159194946
Epoch 1920, training loss: 0.6373546123504639 = 0.0016051232814788818 + 0.1 * 6.357494354248047
Epoch 1920, val loss: 1.369312047958374
Epoch 1930, training loss: 0.6375342607498169 = 0.0015922033926472068 + 0.1 * 6.359420299530029
Epoch 1930, val loss: 1.3710986375808716
Epoch 1940, training loss: 0.6363828182220459 = 0.001579566509462893 + 0.1 * 6.348032474517822
Epoch 1940, val loss: 1.3727073669433594
Epoch 1950, training loss: 0.6371645331382751 = 0.0015672157751396298 + 0.1 * 6.355972766876221
Epoch 1950, val loss: 1.374109148979187
Epoch 1960, training loss: 0.6366323232650757 = 0.0015549095114693046 + 0.1 * 6.35077428817749
Epoch 1960, val loss: 1.375802755355835
Epoch 1970, training loss: 0.6374769806861877 = 0.0015428075566887856 + 0.1 * 6.359341621398926
Epoch 1970, val loss: 1.377577304840088
Epoch 1980, training loss: 0.6360300183296204 = 0.0015308528672903776 + 0.1 * 6.344991683959961
Epoch 1980, val loss: 1.3793476819992065
Epoch 1990, training loss: 0.6375111937522888 = 0.0015192001592367887 + 0.1 * 6.359920024871826
Epoch 1990, val loss: 1.3807644844055176
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 2.805251121520996 = 1.9455657005310059 + 0.1 * 8.596855163574219
Epoch 0, val loss: 1.9476341009140015
Epoch 10, training loss: 2.795448064804077 = 1.9357738494873047 + 0.1 * 8.596742630004883
Epoch 10, val loss: 1.9377479553222656
Epoch 20, training loss: 2.7830591201782227 = 1.9234697818756104 + 0.1 * 8.595891952514648
Epoch 20, val loss: 1.925252914428711
Epoch 30, training loss: 2.7646093368530273 = 1.9058016538619995 + 0.1 * 8.588077545166016
Epoch 30, val loss: 1.9075990915298462
Epoch 40, training loss: 2.733962297439575 = 1.8795808553695679 + 0.1 * 8.543814659118652
Epoch 40, val loss: 1.8825172185897827
Epoch 50, training loss: 2.676997661590576 = 1.8450933694839478 + 0.1 * 8.319043159484863
Epoch 50, val loss: 1.8519638776779175
Epoch 60, training loss: 2.6213040351867676 = 1.8094334602355957 + 0.1 * 8.118706703186035
Epoch 60, val loss: 1.82314133644104
Epoch 70, training loss: 2.58068585395813 = 1.7802975177764893 + 0.1 * 8.003883361816406
Epoch 70, val loss: 1.7982354164123535
Epoch 80, training loss: 2.5233218669891357 = 1.7477020025253296 + 0.1 * 7.756198406219482
Epoch 80, val loss: 1.7662937641143799
Epoch 90, training loss: 2.446054458618164 = 1.7073273658752441 + 0.1 * 7.387269496917725
Epoch 90, val loss: 1.7298290729522705
Epoch 100, training loss: 2.369291305541992 = 1.6540206670761108 + 0.1 * 7.152705669403076
Epoch 100, val loss: 1.686181902885437
Epoch 110, training loss: 2.287675619125366 = 1.584139347076416 + 0.1 * 7.0353617668151855
Epoch 110, val loss: 1.6273690462112427
Epoch 120, training loss: 2.201256513595581 = 1.5049614906311035 + 0.1 * 6.962950229644775
Epoch 120, val loss: 1.561882734298706
Epoch 130, training loss: 2.1181235313415527 = 1.4266185760498047 + 0.1 * 6.915048122406006
Epoch 130, val loss: 1.4991477727890015
Epoch 140, training loss: 2.0415139198303223 = 1.352489709854126 + 0.1 * 6.890242576599121
Epoch 140, val loss: 1.44266939163208
Epoch 150, training loss: 1.9678633213043213 = 1.2805811166763306 + 0.1 * 6.872822284698486
Epoch 150, val loss: 1.3880703449249268
Epoch 160, training loss: 1.894801139831543 = 1.208804726600647 + 0.1 * 6.859964847564697
Epoch 160, val loss: 1.3365310430526733
Epoch 170, training loss: 1.8237011432647705 = 1.1385443210601807 + 0.1 * 6.851568222045898
Epoch 170, val loss: 1.2889155149459839
Epoch 180, training loss: 1.7549827098846436 = 1.0709185600280762 + 0.1 * 6.840641498565674
Epoch 180, val loss: 1.2451884746551514
Epoch 190, training loss: 1.6886193752288818 = 1.0057333707809448 + 0.1 * 6.828859806060791
Epoch 190, val loss: 1.2041525840759277
Epoch 200, training loss: 1.6262000799179077 = 0.9437019228935242 + 0.1 * 6.824981689453125
Epoch 200, val loss: 1.1646920442581177
Epoch 210, training loss: 1.5667959451675415 = 0.8855962157249451 + 0.1 * 6.811996936798096
Epoch 210, val loss: 1.1270476579666138
Epoch 220, training loss: 1.50873863697052 = 0.8289892673492432 + 0.1 * 6.7974934577941895
Epoch 220, val loss: 1.0896553993225098
Epoch 230, training loss: 1.4510914087295532 = 0.7725419998168945 + 0.1 * 6.785493850708008
Epoch 230, val loss: 1.0519545078277588
Epoch 240, training loss: 1.3944907188415527 = 0.7167572975158691 + 0.1 * 6.777333736419678
Epoch 240, val loss: 1.0146231651306152
Epoch 250, training loss: 1.3383128643035889 = 0.6623979806900024 + 0.1 * 6.759149551391602
Epoch 250, val loss: 0.9787814617156982
Epoch 260, training loss: 1.2854588031768799 = 0.6102679371833801 + 0.1 * 6.751908302307129
Epoch 260, val loss: 0.9458907842636108
Epoch 270, training loss: 1.235743761062622 = 0.5620307922363281 + 0.1 * 6.737128734588623
Epoch 270, val loss: 0.9174956679344177
Epoch 280, training loss: 1.1913270950317383 = 0.5179412961006165 + 0.1 * 6.733857154846191
Epoch 280, val loss: 0.894088089466095
Epoch 290, training loss: 1.1495907306671143 = 0.47796863317489624 + 0.1 * 6.716221332550049
Epoch 290, val loss: 0.875395655632019
Epoch 300, training loss: 1.1111640930175781 = 0.4408985376358032 + 0.1 * 6.70265531539917
Epoch 300, val loss: 0.8602061867713928
Epoch 310, training loss: 1.075035572052002 = 0.4058520793914795 + 0.1 * 6.691834449768066
Epoch 310, val loss: 0.8475337028503418
Epoch 320, training loss: 1.0404030084609985 = 0.37196412682533264 + 0.1 * 6.684389114379883
Epoch 320, val loss: 0.8364702463150024
Epoch 330, training loss: 1.0061841011047363 = 0.3387661576271057 + 0.1 * 6.674180030822754
Epoch 330, val loss: 0.826416015625
Epoch 340, training loss: 0.9737686514854431 = 0.3063840866088867 + 0.1 * 6.673845291137695
Epoch 340, val loss: 0.8173794150352478
Epoch 350, training loss: 0.9419302940368652 = 0.275181382894516 + 0.1 * 6.667489051818848
Epoch 350, val loss: 0.809346079826355
Epoch 360, training loss: 0.9110903739929199 = 0.24550782144069672 + 0.1 * 6.655825138092041
Epoch 360, val loss: 0.8027032613754272
Epoch 370, training loss: 0.8846209645271301 = 0.2178005427122116 + 0.1 * 6.668203830718994
Epoch 370, val loss: 0.7980176210403442
Epoch 380, training loss: 0.8571161031723022 = 0.19265787303447723 + 0.1 * 6.6445817947387695
Epoch 380, val loss: 0.7954578399658203
Epoch 390, training loss: 0.8341736793518066 = 0.1702282875776291 + 0.1 * 6.639453411102295
Epoch 390, val loss: 0.7952815294265747
Epoch 400, training loss: 0.815412163734436 = 0.15052886307239532 + 0.1 * 6.648833274841309
Epoch 400, val loss: 0.7973388433456421
Epoch 410, training loss: 0.796619713306427 = 0.13351331651210785 + 0.1 * 6.631063938140869
Epoch 410, val loss: 0.801281750202179
Epoch 420, training loss: 0.781359076499939 = 0.11884622275829315 + 0.1 * 6.625128269195557
Epoch 420, val loss: 0.8069765567779541
Epoch 430, training loss: 0.7687882781028748 = 0.10617649555206299 + 0.1 * 6.626117706298828
Epoch 430, val loss: 0.8141763210296631
Epoch 440, training loss: 0.7581390142440796 = 0.0952468141913414 + 0.1 * 6.628921985626221
Epoch 440, val loss: 0.8225318789482117
Epoch 450, training loss: 0.7467110753059387 = 0.08576638996601105 + 0.1 * 6.609447002410889
Epoch 450, val loss: 0.8318052887916565
Epoch 460, training loss: 0.7382446527481079 = 0.07747794687747955 + 0.1 * 6.607666969299316
Epoch 460, val loss: 0.8419272899627686
Epoch 470, training loss: 0.7305989861488342 = 0.0702223852276802 + 0.1 * 6.603765964508057
Epoch 470, val loss: 0.8526767492294312
Epoch 480, training loss: 0.7236046195030212 = 0.06386251002550125 + 0.1 * 6.597421169281006
Epoch 480, val loss: 0.8637420535087585
Epoch 490, training loss: 0.7187178730964661 = 0.05825795605778694 + 0.1 * 6.604599475860596
Epoch 490, val loss: 0.8752173185348511
Epoch 500, training loss: 0.712225079536438 = 0.05331984907388687 + 0.1 * 6.589051723480225
Epoch 500, val loss: 0.886699914932251
Epoch 510, training loss: 0.7090420722961426 = 0.048945825546979904 + 0.1 * 6.600962162017822
Epoch 510, val loss: 0.8983884453773499
Epoch 520, training loss: 0.7033894658088684 = 0.045068178325891495 + 0.1 * 6.583212375640869
Epoch 520, val loss: 0.9099599123001099
Epoch 530, training loss: 0.698945164680481 = 0.04161439463496208 + 0.1 * 6.573307514190674
Epoch 530, val loss: 0.9215824604034424
Epoch 540, training loss: 0.6976398825645447 = 0.03852885589003563 + 0.1 * 6.5911102294921875
Epoch 540, val loss: 0.9332591891288757
Epoch 550, training loss: 0.693062424659729 = 0.03577491641044617 + 0.1 * 6.572875499725342
Epoch 550, val loss: 0.944589376449585
Epoch 560, training loss: 0.6914717555046082 = 0.03330404683947563 + 0.1 * 6.581676959991455
Epoch 560, val loss: 0.9559010863304138
Epoch 570, training loss: 0.687541127204895 = 0.03108607418835163 + 0.1 * 6.564550399780273
Epoch 570, val loss: 0.9669797420501709
Epoch 580, training loss: 0.6850165128707886 = 0.02908126451075077 + 0.1 * 6.559352397918701
Epoch 580, val loss: 0.9779437184333801
Epoch 590, training loss: 0.6827553510665894 = 0.027266548946499825 + 0.1 * 6.554887771606445
Epoch 590, val loss: 0.9887550473213196
Epoch 600, training loss: 0.681056559085846 = 0.025619152933359146 + 0.1 * 6.554373741149902
Epoch 600, val loss: 0.9993917942047119
Epoch 610, training loss: 0.6793715357780457 = 0.02412210963666439 + 0.1 * 6.552494049072266
Epoch 610, val loss: 1.009683609008789
Epoch 620, training loss: 0.6777569651603699 = 0.022757163271307945 + 0.1 * 6.549997806549072
Epoch 620, val loss: 1.0199698209762573
Epoch 630, training loss: 0.6755313873291016 = 0.0215104091912508 + 0.1 * 6.5402092933654785
Epoch 630, val loss: 1.0299094915390015
Epoch 640, training loss: 0.6742491722106934 = 0.020367447286844254 + 0.1 * 6.538816928863525
Epoch 640, val loss: 1.039680004119873
Epoch 650, training loss: 0.6732134222984314 = 0.019316615536808968 + 0.1 * 6.538968086242676
Epoch 650, val loss: 1.0492383241653442
Epoch 660, training loss: 0.6722743511199951 = 0.018349483609199524 + 0.1 * 6.539248466491699
Epoch 660, val loss: 1.0586363077163696
Epoch 670, training loss: 0.6705358624458313 = 0.01746007241308689 + 0.1 * 6.530757904052734
Epoch 670, val loss: 1.0676629543304443
Epoch 680, training loss: 0.6698833703994751 = 0.016634836792945862 + 0.1 * 6.532485485076904
Epoch 680, val loss: 1.0767027139663696
Epoch 690, training loss: 0.6681087613105774 = 0.01587073504924774 + 0.1 * 6.5223798751831055
Epoch 690, val loss: 1.0854074954986572
Epoch 700, training loss: 0.6681432723999023 = 0.015161078423261642 + 0.1 * 6.529821872711182
Epoch 700, val loss: 1.0940946340560913
Epoch 710, training loss: 0.6662657260894775 = 0.0145042072981596 + 0.1 * 6.51761531829834
Epoch 710, val loss: 1.1023073196411133
Epoch 720, training loss: 0.6660441160202026 = 0.013891147449612617 + 0.1 * 6.521529674530029
Epoch 720, val loss: 1.1105074882507324
Epoch 730, training loss: 0.6645815968513489 = 0.01331854797899723 + 0.1 * 6.512630462646484
Epoch 730, val loss: 1.1185640096664429
Epoch 740, training loss: 0.6638768315315247 = 0.012783720158040524 + 0.1 * 6.510931015014648
Epoch 740, val loss: 1.126332402229309
Epoch 750, training loss: 0.6642163991928101 = 0.01228293776512146 + 0.1 * 6.51933479309082
Epoch 750, val loss: 1.1341608762741089
Epoch 760, training loss: 0.6619743704795837 = 0.011815114878118038 + 0.1 * 6.50159215927124
Epoch 760, val loss: 1.1415162086486816
Epoch 770, training loss: 0.6620731949806213 = 0.011375097557902336 + 0.1 * 6.506981372833252
Epoch 770, val loss: 1.1488211154937744
Epoch 780, training loss: 0.6608251929283142 = 0.010960817337036133 + 0.1 * 6.498643398284912
Epoch 780, val loss: 1.1562087535858154
Epoch 790, training loss: 0.6602334976196289 = 0.010572615079581738 + 0.1 * 6.496608734130859
Epoch 790, val loss: 1.1630914211273193
Epoch 800, training loss: 0.6612454652786255 = 0.010204863734543324 + 0.1 * 6.510406017303467
Epoch 800, val loss: 1.1700750589370728
Epoch 810, training loss: 0.6592683792114258 = 0.00985822081565857 + 0.1 * 6.494101524353027
Epoch 810, val loss: 1.1768546104431152
Epoch 820, training loss: 0.6590467095375061 = 0.009530816227197647 + 0.1 * 6.495158672332764
Epoch 820, val loss: 1.1834806203842163
Epoch 830, training loss: 0.6589463949203491 = 0.009220299310982227 + 0.1 * 6.497260570526123
Epoch 830, val loss: 1.1900701522827148
Epoch 840, training loss: 0.6580349206924438 = 0.008926833048462868 + 0.1 * 6.4910807609558105
Epoch 840, val loss: 1.196567416191101
Epoch 850, training loss: 0.6574369072914124 = 0.008649399504065514 + 0.1 * 6.487874984741211
Epoch 850, val loss: 1.2026983499526978
Epoch 860, training loss: 0.6584111452102661 = 0.0083853704854846 + 0.1 * 6.50025749206543
Epoch 860, val loss: 1.2089147567749023
Epoch 870, training loss: 0.6566434502601624 = 0.008134952746331692 + 0.1 * 6.485084533691406
Epoch 870, val loss: 1.2150098085403442
Epoch 880, training loss: 0.6562290787696838 = 0.007897474803030491 + 0.1 * 6.483315467834473
Epoch 880, val loss: 1.2207815647125244
Epoch 890, training loss: 0.6575877666473389 = 0.007670606952160597 + 0.1 * 6.499171733856201
Epoch 890, val loss: 1.226731300354004
Epoch 900, training loss: 0.6559840440750122 = 0.0074548060074448586 + 0.1 * 6.485292434692383
Epoch 900, val loss: 1.2324661016464233
Epoch 910, training loss: 0.654699444770813 = 0.007250113412737846 + 0.1 * 6.474493026733398
Epoch 910, val loss: 1.2379637956619263
Epoch 920, training loss: 0.6547781229019165 = 0.007054035551846027 + 0.1 * 6.477240562438965
Epoch 920, val loss: 1.2435426712036133
Epoch 930, training loss: 0.6544130444526672 = 0.006866988725960255 + 0.1 * 6.475460529327393
Epoch 930, val loss: 1.2488991022109985
Epoch 940, training loss: 0.6544545292854309 = 0.006688473746180534 + 0.1 * 6.477660179138184
Epoch 940, val loss: 1.2542866468429565
Epoch 950, training loss: 0.6533442735671997 = 0.006517902947962284 + 0.1 * 6.468263626098633
Epoch 950, val loss: 1.2594397068023682
Epoch 960, training loss: 0.6551121473312378 = 0.006354551296681166 + 0.1 * 6.487575531005859
Epoch 960, val loss: 1.2645173072814941
Epoch 970, training loss: 0.6540740728378296 = 0.00619811937212944 + 0.1 * 6.478759765625
Epoch 970, val loss: 1.2696902751922607
Epoch 980, training loss: 0.6527388095855713 = 0.006048576440662146 + 0.1 * 6.466902256011963
Epoch 980, val loss: 1.2745946645736694
Epoch 990, training loss: 0.652439534664154 = 0.0059053050354123116 + 0.1 * 6.465342044830322
Epoch 990, val loss: 1.279404878616333
Epoch 1000, training loss: 0.6520777940750122 = 0.005768447183072567 + 0.1 * 6.4630937576293945
Epoch 1000, val loss: 1.2840907573699951
Epoch 1010, training loss: 0.6518660187721252 = 0.005636089481413364 + 0.1 * 6.46229887008667
Epoch 1010, val loss: 1.2889090776443481
Epoch 1020, training loss: 0.6508601903915405 = 0.005509525537490845 + 0.1 * 6.453506946563721
Epoch 1020, val loss: 1.2935057878494263
Epoch 1030, training loss: 0.65070641040802 = 0.005387557204812765 + 0.1 * 6.453188419342041
Epoch 1030, val loss: 1.2981451749801636
Epoch 1040, training loss: 0.6511080265045166 = 0.005270790308713913 + 0.1 * 6.458372592926025
Epoch 1040, val loss: 1.3026204109191895
Epoch 1050, training loss: 0.6504619717597961 = 0.005157912615686655 + 0.1 * 6.453040599822998
Epoch 1050, val loss: 1.3070659637451172
Epoch 1060, training loss: 0.6517977714538574 = 0.005049682687968016 + 0.1 * 6.467480659484863
Epoch 1060, val loss: 1.31143319606781
Epoch 1070, training loss: 0.6498452425003052 = 0.004945590626448393 + 0.1 * 6.448996543884277
Epoch 1070, val loss: 1.3157601356506348
Epoch 1080, training loss: 0.648982048034668 = 0.004845441784709692 + 0.1 * 6.441365718841553
Epoch 1080, val loss: 1.319854736328125
Epoch 1090, training loss: 0.6495116949081421 = 0.004748248960822821 + 0.1 * 6.447634696960449
Epoch 1090, val loss: 1.3240917921066284
Epoch 1100, training loss: 0.6492434740066528 = 0.004654771648347378 + 0.1 * 6.445887088775635
Epoch 1100, val loss: 1.328294038772583
Epoch 1110, training loss: 0.6493817567825317 = 0.004564416594803333 + 0.1 * 6.448173522949219
Epoch 1110, val loss: 1.332404375076294
Epoch 1120, training loss: 0.6481980681419373 = 0.004477504175156355 + 0.1 * 6.437205791473389
Epoch 1120, val loss: 1.336351752281189
Epoch 1130, training loss: 0.6498703360557556 = 0.004393401555716991 + 0.1 * 6.454769134521484
Epoch 1130, val loss: 1.3402929306030273
Epoch 1140, training loss: 0.647360622882843 = 0.004311712458729744 + 0.1 * 6.430488586425781
Epoch 1140, val loss: 1.3442955017089844
Epoch 1150, training loss: 0.6473783850669861 = 0.004233594052493572 + 0.1 * 6.431447982788086
Epoch 1150, val loss: 1.3479827642440796
Epoch 1160, training loss: 0.6490247249603271 = 0.00415757205337286 + 0.1 * 6.448671817779541
Epoch 1160, val loss: 1.351792812347412
Epoch 1170, training loss: 0.6473809480667114 = 0.004083571024239063 + 0.1 * 6.432973384857178
Epoch 1170, val loss: 1.3556989431381226
Epoch 1180, training loss: 0.6464183926582336 = 0.004012764431536198 + 0.1 * 6.424056053161621
Epoch 1180, val loss: 1.3592021465301514
Epoch 1190, training loss: 0.6473065614700317 = 0.003943748772144318 + 0.1 * 6.433628082275391
Epoch 1190, val loss: 1.3627992868423462
Epoch 1200, training loss: 0.6459226608276367 = 0.0038769778329879045 + 0.1 * 6.420456886291504
Epoch 1200, val loss: 1.3664344549179077
Epoch 1210, training loss: 0.6455849409103394 = 0.0038122455589473248 + 0.1 * 6.417726993560791
Epoch 1210, val loss: 1.3699266910552979
Epoch 1220, training loss: 0.6462841033935547 = 0.0037491153925657272 + 0.1 * 6.425349712371826
Epoch 1220, val loss: 1.3735946416854858
Epoch 1230, training loss: 0.6454998850822449 = 0.003688023192808032 + 0.1 * 6.418118476867676
Epoch 1230, val loss: 1.377099871635437
Epoch 1240, training loss: 0.6466591358184814 = 0.0036288511473685503 + 0.1 * 6.430302619934082
Epoch 1240, val loss: 1.3805617094039917
Epoch 1250, training loss: 0.6443525552749634 = 0.003571438603103161 + 0.1 * 6.407810688018799
Epoch 1250, val loss: 1.3838471174240112
Epoch 1260, training loss: 0.6449284553527832 = 0.0035160318948328495 + 0.1 * 6.414124488830566
Epoch 1260, val loss: 1.3870034217834473
Epoch 1270, training loss: 0.6441889405250549 = 0.003461764892563224 + 0.1 * 6.407271385192871
Epoch 1270, val loss: 1.3903427124023438
Epoch 1280, training loss: 0.6454736590385437 = 0.003409196622669697 + 0.1 * 6.420644283294678
Epoch 1280, val loss: 1.3935884237289429
Epoch 1290, training loss: 0.6442127227783203 = 0.0033577741123735905 + 0.1 * 6.4085493087768555
Epoch 1290, val loss: 1.3969171047210693
Epoch 1300, training loss: 0.6445280909538269 = 0.0033079597633332014 + 0.1 * 6.412201404571533
Epoch 1300, val loss: 1.4000964164733887
Epoch 1310, training loss: 0.6452382206916809 = 0.00325938337482512 + 0.1 * 6.419788360595703
Epoch 1310, val loss: 1.4033458232879639
Epoch 1320, training loss: 0.6439048647880554 = 0.003212238661944866 + 0.1 * 6.40692663192749
Epoch 1320, val loss: 1.4064228534698486
Epoch 1330, training loss: 0.6433733105659485 = 0.0031666026916354895 + 0.1 * 6.402066707611084
Epoch 1330, val loss: 1.4092239141464233
Epoch 1340, training loss: 0.6433407664299011 = 0.0031216610223054886 + 0.1 * 6.402190685272217
Epoch 1340, val loss: 1.412302851676941
Epoch 1350, training loss: 0.6429554224014282 = 0.003078189678490162 + 0.1 * 6.398772716522217
Epoch 1350, val loss: 1.4153099060058594
Epoch 1360, training loss: 0.6434102654457092 = 0.0030356592033058405 + 0.1 * 6.403745651245117
Epoch 1360, val loss: 1.4183026552200317
Epoch 1370, training loss: 0.6427707672119141 = 0.0029943783301860094 + 0.1 * 6.397763729095459
Epoch 1370, val loss: 1.4211113452911377
Epoch 1380, training loss: 0.6433982849121094 = 0.0029540809337049723 + 0.1 * 6.404442310333252
Epoch 1380, val loss: 1.4239861965179443
Epoch 1390, training loss: 0.6425925493240356 = 0.002914461772888899 + 0.1 * 6.396780490875244
Epoch 1390, val loss: 1.4269667863845825
Epoch 1400, training loss: 0.6423301100730896 = 0.0028760831337422132 + 0.1 * 6.394539833068848
Epoch 1400, val loss: 1.429720401763916
Epoch 1410, training loss: 0.6428103446960449 = 0.0028388125356286764 + 0.1 * 6.399715423583984
Epoch 1410, val loss: 1.4324253797531128
Epoch 1420, training loss: 0.6426143646240234 = 0.002801973605528474 + 0.1 * 6.3981242179870605
Epoch 1420, val loss: 1.4352569580078125
Epoch 1430, training loss: 0.6425426006317139 = 0.0027663528453558683 + 0.1 * 6.397762298583984
Epoch 1430, val loss: 1.4380086660385132
Epoch 1440, training loss: 0.6423187851905823 = 0.002731488086283207 + 0.1 * 6.395872592926025
Epoch 1440, val loss: 1.4406481981277466
Epoch 1450, training loss: 0.6414376497268677 = 0.0026973614003509283 + 0.1 * 6.3874030113220215
Epoch 1450, val loss: 1.4433612823486328
Epoch 1460, training loss: 0.6425410509109497 = 0.0026640857104212046 + 0.1 * 6.398769378662109
Epoch 1460, val loss: 1.4459660053253174
Epoch 1470, training loss: 0.6423215270042419 = 0.002631518756970763 + 0.1 * 6.396899700164795
Epoch 1470, val loss: 1.4486175775527954
Epoch 1480, training loss: 0.641906201839447 = 0.0025996784679591656 + 0.1 * 6.393065452575684
Epoch 1480, val loss: 1.4511816501617432
Epoch 1490, training loss: 0.6411848664283752 = 0.002568657975643873 + 0.1 * 6.386161804199219
Epoch 1490, val loss: 1.4537007808685303
Epoch 1500, training loss: 0.6421166062355042 = 0.002538269618526101 + 0.1 * 6.395783424377441
Epoch 1500, val loss: 1.4562170505523682
Epoch 1510, training loss: 0.6410003304481506 = 0.0025084984954446554 + 0.1 * 6.384918212890625
Epoch 1510, val loss: 1.458770513534546
Epoch 1520, training loss: 0.6406964659690857 = 0.002479353453963995 + 0.1 * 6.382170677185059
Epoch 1520, val loss: 1.4612905979156494
Epoch 1530, training loss: 0.6412311792373657 = 0.0024509653449058533 + 0.1 * 6.3878021240234375
Epoch 1530, val loss: 1.4637248516082764
Epoch 1540, training loss: 0.6404176354408264 = 0.0024229856207966805 + 0.1 * 6.379946231842041
Epoch 1540, val loss: 1.4661784172058105
Epoch 1550, training loss: 0.6409326195716858 = 0.00239594210870564 + 0.1 * 6.385366916656494
Epoch 1550, val loss: 1.4685122966766357
Epoch 1560, training loss: 0.6411918997764587 = 0.0023691460955888033 + 0.1 * 6.388227462768555
Epoch 1560, val loss: 1.470947027206421
Epoch 1570, training loss: 0.6413984894752502 = 0.0023430611472576857 + 0.1 * 6.390553951263428
Epoch 1570, val loss: 1.4733506441116333
Epoch 1580, training loss: 0.6396843194961548 = 0.002317430218681693 + 0.1 * 6.373668670654297
Epoch 1580, val loss: 1.4757450819015503
Epoch 1590, training loss: 0.6398111581802368 = 0.0022925422526896 + 0.1 * 6.375186443328857
Epoch 1590, val loss: 1.477906346321106
Epoch 1600, training loss: 0.639959454536438 = 0.002267889678478241 + 0.1 * 6.376915454864502
Epoch 1600, val loss: 1.4802242517471313
Epoch 1610, training loss: 0.6403001546859741 = 0.0022437446750700474 + 0.1 * 6.380563735961914
Epoch 1610, val loss: 1.4825514554977417
Epoch 1620, training loss: 0.6391221880912781 = 0.0022201689425855875 + 0.1 * 6.369019985198975
Epoch 1620, val loss: 1.4848144054412842
Epoch 1630, training loss: 0.6399710178375244 = 0.002197077265009284 + 0.1 * 6.377738952636719
Epoch 1630, val loss: 1.4869506359100342
Epoch 1640, training loss: 0.6401336789131165 = 0.002174277789890766 + 0.1 * 6.379594326019287
Epoch 1640, val loss: 1.4892467260360718
Epoch 1650, training loss: 0.6391615271568298 = 0.0021519004367291927 + 0.1 * 6.370096206665039
Epoch 1650, val loss: 1.491528868675232
Epoch 1660, training loss: 0.6400742530822754 = 0.0021301032975316048 + 0.1 * 6.379441261291504
Epoch 1660, val loss: 1.4937143325805664
Epoch 1670, training loss: 0.6396478414535522 = 0.0021084679756313562 + 0.1 * 6.375393867492676
Epoch 1670, val loss: 1.4960311651229858
Epoch 1680, training loss: 0.6389231085777283 = 0.002087618922814727 + 0.1 * 6.3683552742004395
Epoch 1680, val loss: 1.4980791807174683
Epoch 1690, training loss: 0.6393792033195496 = 0.002067119814455509 + 0.1 * 6.373120307922363
Epoch 1690, val loss: 1.4999815225601196
Epoch 1700, training loss: 0.6396053433418274 = 0.0020467396825551987 + 0.1 * 6.375585556030273
Epoch 1700, val loss: 1.502223253250122
Epoch 1710, training loss: 0.6392871141433716 = 0.002026882953941822 + 0.1 * 6.372602462768555
Epoch 1710, val loss: 1.5043281316757202
Epoch 1720, training loss: 0.6397303342819214 = 0.002007450209930539 + 0.1 * 6.377228736877441
Epoch 1720, val loss: 1.5063401460647583
Epoch 1730, training loss: 0.6387300491333008 = 0.0019881809130311012 + 0.1 * 6.3674187660217285
Epoch 1730, val loss: 1.5084513425827026
Epoch 1740, training loss: 0.6380168795585632 = 0.0019694827497005463 + 0.1 * 6.360474109649658
Epoch 1740, val loss: 1.5103800296783447
Epoch 1750, training loss: 0.6385555863380432 = 0.0019510803977027535 + 0.1 * 6.366044998168945
Epoch 1750, val loss: 1.5123425722122192
Epoch 1760, training loss: 0.6397297382354736 = 0.0019329381175339222 + 0.1 * 6.3779683113098145
Epoch 1760, val loss: 1.514356017112732
Epoch 1770, training loss: 0.6379891037940979 = 0.0019150061998516321 + 0.1 * 6.360741138458252
Epoch 1770, val loss: 1.5165053606033325
Epoch 1780, training loss: 0.6381716728210449 = 0.0018975483253598213 + 0.1 * 6.362740993499756
Epoch 1780, val loss: 1.5183982849121094
Epoch 1790, training loss: 0.6379142999649048 = 0.0018802837003022432 + 0.1 * 6.360340118408203
Epoch 1790, val loss: 1.5203747749328613
Epoch 1800, training loss: 0.6393460631370544 = 0.0018632836872711778 + 0.1 * 6.3748273849487305
Epoch 1800, val loss: 1.5224345922470093
Epoch 1810, training loss: 0.6380431056022644 = 0.001846655155532062 + 0.1 * 6.361964225769043
Epoch 1810, val loss: 1.5243878364562988
Epoch 1820, training loss: 0.6387057304382324 = 0.0018304776167497039 + 0.1 * 6.368752479553223
Epoch 1820, val loss: 1.5260729789733887
Epoch 1830, training loss: 0.6379768252372742 = 0.0018143984489142895 + 0.1 * 6.361623764038086
Epoch 1830, val loss: 1.5279065370559692
Epoch 1840, training loss: 0.6374740600585938 = 0.001798667130060494 + 0.1 * 6.356753826141357
Epoch 1840, val loss: 1.5297614336013794
Epoch 1850, training loss: 0.6385682225227356 = 0.0017832384910434484 + 0.1 * 6.367849349975586
Epoch 1850, val loss: 1.5316210985183716
Epoch 1860, training loss: 0.6375591158866882 = 0.0017678869189694524 + 0.1 * 6.357912063598633
Epoch 1860, val loss: 1.5335417985916138
Epoch 1870, training loss: 0.6380313038825989 = 0.0017529785400256515 + 0.1 * 6.362783432006836
Epoch 1870, val loss: 1.5353251695632935
Epoch 1880, training loss: 0.6369678974151611 = 0.0017382127698510885 + 0.1 * 6.352296829223633
Epoch 1880, val loss: 1.5371657609939575
Epoch 1890, training loss: 0.6377766728401184 = 0.001723717083223164 + 0.1 * 6.36052942276001
Epoch 1890, val loss: 1.5390077829360962
Epoch 1900, training loss: 0.6375048756599426 = 0.0017094755312427878 + 0.1 * 6.357954025268555
Epoch 1900, val loss: 1.5407603979110718
Epoch 1910, training loss: 0.6375309824943542 = 0.0016954729799181223 + 0.1 * 6.3583550453186035
Epoch 1910, val loss: 1.5425232648849487
Epoch 1920, training loss: 0.6378592252731323 = 0.0016816603019833565 + 0.1 * 6.3617753982543945
Epoch 1920, val loss: 1.5443581342697144
Epoch 1930, training loss: 0.637143075466156 = 0.0016680972184985876 + 0.1 * 6.35474967956543
Epoch 1930, val loss: 1.546118140220642
Epoch 1940, training loss: 0.6375264525413513 = 0.00165475660469383 + 0.1 * 6.35871696472168
Epoch 1940, val loss: 1.5478265285491943
Epoch 1950, training loss: 0.6374433636665344 = 0.001641643699258566 + 0.1 * 6.3580169677734375
Epoch 1950, val loss: 1.5495259761810303
Epoch 1960, training loss: 0.6361339092254639 = 0.0016286686295643449 + 0.1 * 6.345052242279053
Epoch 1960, val loss: 1.5512629747390747
Epoch 1970, training loss: 0.6374152302742004 = 0.001616049325093627 + 0.1 * 6.357991695404053
Epoch 1970, val loss: 1.5528508424758911
Epoch 1980, training loss: 0.6366076469421387 = 0.001603445503860712 + 0.1 * 6.35004186630249
Epoch 1980, val loss: 1.5546321868896484
Epoch 1990, training loss: 0.637207567691803 = 0.0015911508817225695 + 0.1 * 6.35616397857666
Epoch 1990, val loss: 1.556343913078308
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 2.8184289932250977 = 1.9587455987930298 + 0.1 * 8.596834182739258
Epoch 0, val loss: 1.9569661617279053
Epoch 10, training loss: 2.8079569339752197 = 1.9482814073562622 + 0.1 * 8.596755027770996
Epoch 10, val loss: 1.9471977949142456
Epoch 20, training loss: 2.795433521270752 = 1.9358137845993042 + 0.1 * 8.596198081970215
Epoch 20, val loss: 1.9351087808609009
Epoch 30, training loss: 2.777677059173584 = 1.9185975790023804 + 0.1 * 8.590795516967773
Epoch 30, val loss: 1.9179877042770386
Epoch 40, training loss: 2.748382806777954 = 1.8932863473892212 + 0.1 * 8.55096435546875
Epoch 40, val loss: 1.8927427530288696
Epoch 50, training loss: 2.6901426315307617 = 1.8586748838424683 + 0.1 * 8.314676284790039
Epoch 50, val loss: 1.8598428964614868
Epoch 60, training loss: 2.626160144805908 = 1.8203709125518799 + 0.1 * 8.057890892028809
Epoch 60, val loss: 1.8260688781738281
Epoch 70, training loss: 2.563960552215576 = 1.7891292572021484 + 0.1 * 7.748311519622803
Epoch 70, val loss: 1.7999128103256226
Epoch 80, training loss: 2.497119188308716 = 1.7612628936767578 + 0.1 * 7.358561992645264
Epoch 80, val loss: 1.7743089199066162
Epoch 90, training loss: 2.4396917819976807 = 1.7268316745758057 + 0.1 * 7.12860107421875
Epoch 90, val loss: 1.7417787313461304
Epoch 100, training loss: 2.3838858604431152 = 1.6806539297103882 + 0.1 * 7.032318592071533
Epoch 100, val loss: 1.7001018524169922
Epoch 110, training loss: 2.319059371948242 = 1.6188331842422485 + 0.1 * 7.002261638641357
Epoch 110, val loss: 1.6451003551483154
Epoch 120, training loss: 2.2399017810821533 = 1.5420187711715698 + 0.1 * 6.978829860687256
Epoch 120, val loss: 1.5778589248657227
Epoch 130, training loss: 2.1520376205444336 = 1.4562753438949585 + 0.1 * 6.957622528076172
Epoch 130, val loss: 1.5052810907363892
Epoch 140, training loss: 2.0606160163879395 = 1.3670381307601929 + 0.1 * 6.935779094696045
Epoch 140, val loss: 1.43168306350708
Epoch 150, training loss: 1.9670593738555908 = 1.2758731842041016 + 0.1 * 6.911861896514893
Epoch 150, val loss: 1.3587191104888916
Epoch 160, training loss: 1.8699142932891846 = 1.1812026500701904 + 0.1 * 6.8871169090271
Epoch 160, val loss: 1.2841169834136963
Epoch 170, training loss: 1.7728805541992188 = 1.0860925912857056 + 0.1 * 6.867879867553711
Epoch 170, val loss: 1.21201753616333
Epoch 180, training loss: 1.6779582500457764 = 0.9927735924720764 + 0.1 * 6.851847171783447
Epoch 180, val loss: 1.142431616783142
Epoch 190, training loss: 1.5866892337799072 = 0.902969479560852 + 0.1 * 6.837198257446289
Epoch 190, val loss: 1.0770313739776611
Epoch 200, training loss: 1.503016710281372 = 0.8203805685043335 + 0.1 * 6.826362133026123
Epoch 200, val loss: 1.018447995185852
Epoch 210, training loss: 1.4275224208831787 = 0.745994508266449 + 0.1 * 6.815279483795166
Epoch 210, val loss: 0.9679036736488342
Epoch 220, training loss: 1.3619993925094604 = 0.6793846487998962 + 0.1 * 6.826147079467773
Epoch 220, val loss: 0.9254043102264404
Epoch 230, training loss: 1.3008148670196533 = 0.620556652545929 + 0.1 * 6.802582740783691
Epoch 230, val loss: 0.8911089897155762
Epoch 240, training loss: 1.246783971786499 = 0.5674877166748047 + 0.1 * 6.792961597442627
Epoch 240, val loss: 0.8631920218467712
Epoch 250, training loss: 1.1983792781829834 = 0.5192245244979858 + 0.1 * 6.791547775268555
Epoch 250, val loss: 0.8407999277114868
Epoch 260, training loss: 1.1537781953811646 = 0.47549545764923096 + 0.1 * 6.782827377319336
Epoch 260, val loss: 0.8228728771209717
Epoch 270, training loss: 1.1129727363586426 = 0.4350273013114929 + 0.1 * 6.779454708099365
Epoch 270, val loss: 0.8087008595466614
Epoch 280, training loss: 1.074563980102539 = 0.39728617668151855 + 0.1 * 6.772778034210205
Epoch 280, val loss: 0.7977461814880371
Epoch 290, training loss: 1.038818597793579 = 0.3621472120285034 + 0.1 * 6.766714096069336
Epoch 290, val loss: 0.7894407510757446
Epoch 300, training loss: 1.0058865547180176 = 0.3295164108276367 + 0.1 * 6.763701915740967
Epoch 300, val loss: 0.7831025123596191
Epoch 310, training loss: 0.9741772413253784 = 0.2987283170223236 + 0.1 * 6.754488945007324
Epoch 310, val loss: 0.778541088104248
Epoch 320, training loss: 0.9469467401504517 = 0.26969650387763977 + 0.1 * 6.772502422332764
Epoch 320, val loss: 0.7755696177482605
Epoch 330, training loss: 0.9169815182685852 = 0.2424372285604477 + 0.1 * 6.745442867279053
Epoch 330, val loss: 0.7738135457038879
Epoch 340, training loss: 0.8897469639778137 = 0.2167162448167801 + 0.1 * 6.730307102203369
Epoch 340, val loss: 0.7733038663864136
Epoch 350, training loss: 0.8659584522247314 = 0.19286765158176422 + 0.1 * 6.730907917022705
Epoch 350, val loss: 0.7743135690689087
Epoch 360, training loss: 0.843058168888092 = 0.1713353395462036 + 0.1 * 6.717227935791016
Epoch 360, val loss: 0.7763952612876892
Epoch 370, training loss: 0.8234074711799622 = 0.15217840671539307 + 0.1 * 6.712290287017822
Epoch 370, val loss: 0.7798043489456177
Epoch 380, training loss: 0.8056206107139587 = 0.13541840016841888 + 0.1 * 6.702021598815918
Epoch 380, val loss: 0.7845268249511719
Epoch 390, training loss: 0.7900772094726562 = 0.12087886035442352 + 0.1 * 6.691983222961426
Epoch 390, val loss: 0.7902019023895264
Epoch 400, training loss: 0.777165412902832 = 0.10833308100700378 + 0.1 * 6.688323497772217
Epoch 400, val loss: 0.7969607710838318
Epoch 410, training loss: 0.7654313445091248 = 0.09751273691654205 + 0.1 * 6.67918586730957
Epoch 410, val loss: 0.8043670654296875
Epoch 420, training loss: 0.7574955821037292 = 0.08810023218393326 + 0.1 * 6.693953037261963
Epoch 420, val loss: 0.8126378655433655
Epoch 430, training loss: 0.7463017106056213 = 0.07994664460420609 + 0.1 * 6.663550853729248
Epoch 430, val loss: 0.8214345574378967
Epoch 440, training loss: 0.739693820476532 = 0.07282650470733643 + 0.1 * 6.668673038482666
Epoch 440, val loss: 0.8308671116828918
Epoch 450, training loss: 0.7316505312919617 = 0.06660044938325882 + 0.1 * 6.650500774383545
Epoch 450, val loss: 0.8403016328811646
Epoch 460, training loss: 0.7256959676742554 = 0.06110915169119835 + 0.1 * 6.645868301391602
Epoch 460, val loss: 0.8502644300460815
Epoch 470, training loss: 0.7197675108909607 = 0.056260108947753906 + 0.1 * 6.635074138641357
Epoch 470, val loss: 0.8602845668792725
Epoch 480, training loss: 0.7164814472198486 = 0.051964011043310165 + 0.1 * 6.645174503326416
Epoch 480, val loss: 0.8704178333282471
Epoch 490, training loss: 0.7096732258796692 = 0.04815826192498207 + 0.1 * 6.615149021148682
Epoch 490, val loss: 0.8803830742835999
Epoch 500, training loss: 0.706963300704956 = 0.04475674033164978 + 0.1 * 6.62206506729126
Epoch 500, val loss: 0.8905003666877747
Epoch 510, training loss: 0.7019782662391663 = 0.041713375598192215 + 0.1 * 6.602648735046387
Epoch 510, val loss: 0.9004742503166199
Epoch 520, training loss: 0.6992835998535156 = 0.03897314891219139 + 0.1 * 6.603104591369629
Epoch 520, val loss: 0.9104346036911011
Epoch 530, training loss: 0.6953323483467102 = 0.03650004044175148 + 0.1 * 6.58832311630249
Epoch 530, val loss: 0.9201710820198059
Epoch 540, training loss: 0.6946456432342529 = 0.034258030354976654 + 0.1 * 6.603876113891602
Epoch 540, val loss: 0.9299596548080444
Epoch 550, training loss: 0.6898743510246277 = 0.03222675621509552 + 0.1 * 6.576476097106934
Epoch 550, val loss: 0.9393660426139832
Epoch 560, training loss: 0.6913744211196899 = 0.030373547226190567 + 0.1 * 6.610008239746094
Epoch 560, val loss: 0.9488873481750488
Epoch 570, training loss: 0.6857156157493591 = 0.028683586046099663 + 0.1 * 6.570320129394531
Epoch 570, val loss: 0.9580674767494202
Epoch 580, training loss: 0.6843655705451965 = 0.027132347226142883 + 0.1 * 6.572332382202148
Epoch 580, val loss: 0.9671768546104431
Epoch 590, training loss: 0.6828793287277222 = 0.025707047432661057 + 0.1 * 6.571722507476807
Epoch 590, val loss: 0.9761969447135925
Epoch 600, training loss: 0.6800311207771301 = 0.024396583437919617 + 0.1 * 6.556344985961914
Epoch 600, val loss: 0.9849522113800049
Epoch 610, training loss: 0.6786491274833679 = 0.023183410987257957 + 0.1 * 6.554656982421875
Epoch 610, val loss: 0.9936590194702148
Epoch 620, training loss: 0.6771633625030518 = 0.022065216675400734 + 0.1 * 6.550981521606445
Epoch 620, val loss: 1.0022979974746704
Epoch 630, training loss: 0.6749563813209534 = 0.021027544513344765 + 0.1 * 6.53928804397583
Epoch 630, val loss: 1.0105760097503662
Epoch 640, training loss: 0.6753071546554565 = 0.02006646804511547 + 0.1 * 6.5524067878723145
Epoch 640, val loss: 1.0188755989074707
Epoch 650, training loss: 0.6720049977302551 = 0.019171331077814102 + 0.1 * 6.528336048126221
Epoch 650, val loss: 1.0269218683242798
Epoch 660, training loss: 0.6750413179397583 = 0.01833738014101982 + 0.1 * 6.567039489746094
Epoch 660, val loss: 1.0349057912826538
Epoch 670, training loss: 0.6697400808334351 = 0.017561933025717735 + 0.1 * 6.5217814445495605
Epoch 670, val loss: 1.0427109003067017
Epoch 680, training loss: 0.6690659523010254 = 0.016835546121001244 + 0.1 * 6.522303581237793
Epoch 680, val loss: 1.0503031015396118
Epoch 690, training loss: 0.667697012424469 = 0.016157058998942375 + 0.1 * 6.515399932861328
Epoch 690, val loss: 1.057884693145752
Epoch 700, training loss: 0.6675230860710144 = 0.015519378706812859 + 0.1 * 6.520036697387695
Epoch 700, val loss: 1.0651754140853882
Epoch 710, training loss: 0.6671479940414429 = 0.01492135226726532 + 0.1 * 6.522265911102295
Epoch 710, val loss: 1.0725171566009521
Epoch 720, training loss: 0.6653766632080078 = 0.014359853230416775 + 0.1 * 6.510168075561523
Epoch 720, val loss: 1.0796374082565308
Epoch 730, training loss: 0.6650488376617432 = 0.01383050624281168 + 0.1 * 6.51218318939209
Epoch 730, val loss: 1.0866215229034424
Epoch 740, training loss: 0.6638224124908447 = 0.013332227244973183 + 0.1 * 6.50490140914917
Epoch 740, val loss: 1.0936055183410645
Epoch 750, training loss: 0.6626995205879211 = 0.012863513082265854 + 0.1 * 6.498359680175781
Epoch 750, val loss: 1.1002973318099976
Epoch 760, training loss: 0.66221684217453 = 0.012420514598488808 + 0.1 * 6.497962951660156
Epoch 760, val loss: 1.1070054769515991
Epoch 770, training loss: 0.6619637608528137 = 0.012000967748463154 + 0.1 * 6.499627590179443
Epoch 770, val loss: 1.113639235496521
Epoch 780, training loss: 0.6607319712638855 = 0.01160527765750885 + 0.1 * 6.491267204284668
Epoch 780, val loss: 1.1199743747711182
Epoch 790, training loss: 0.6611403226852417 = 0.011229326017200947 + 0.1 * 6.499109745025635
Epoch 790, val loss: 1.1263511180877686
Epoch 800, training loss: 0.6594178676605225 = 0.010873517021536827 + 0.1 * 6.485443592071533
Epoch 800, val loss: 1.1326122283935547
Epoch 810, training loss: 0.6595145463943481 = 0.010534392669796944 + 0.1 * 6.489801406860352
Epoch 810, val loss: 1.1386774778366089
Epoch 820, training loss: 0.6591496467590332 = 0.010212509892880917 + 0.1 * 6.4893717765808105
Epoch 820, val loss: 1.1447780132293701
Epoch 830, training loss: 0.6581869721412659 = 0.009906868450343609 + 0.1 * 6.4828009605407715
Epoch 830, val loss: 1.1506816148757935
Epoch 840, training loss: 0.6572931408882141 = 0.009616040624678135 + 0.1 * 6.476771354675293
Epoch 840, val loss: 1.15656316280365
Epoch 850, training loss: 0.6561891436576843 = 0.009338568896055222 + 0.1 * 6.468505859375
Epoch 850, val loss: 1.1623114347457886
Epoch 860, training loss: 0.6574363112449646 = 0.009074080735445023 + 0.1 * 6.483622074127197
Epoch 860, val loss: 1.1679922342300415
Epoch 870, training loss: 0.6565618515014648 = 0.008821814320981503 + 0.1 * 6.477400302886963
Epoch 870, val loss: 1.1735928058624268
Epoch 880, training loss: 0.6564570665359497 = 0.008581290021538734 + 0.1 * 6.478757858276367
Epoch 880, val loss: 1.1790471076965332
Epoch 890, training loss: 0.6539219617843628 = 0.008351079188287258 + 0.1 * 6.4557085037231445
Epoch 890, val loss: 1.1844933032989502
Epoch 900, training loss: 0.6535168290138245 = 0.008130407892167568 + 0.1 * 6.453864097595215
Epoch 900, val loss: 1.1898235082626343
Epoch 910, training loss: 0.6558629274368286 = 0.00791923888027668 + 0.1 * 6.47943639755249
Epoch 910, val loss: 1.1951204538345337
Epoch 920, training loss: 0.6530555486679077 = 0.007718022912740707 + 0.1 * 6.453375339508057
Epoch 920, val loss: 1.200272560119629
Epoch 930, training loss: 0.6556425094604492 = 0.007524235639721155 + 0.1 * 6.48118257522583
Epoch 930, val loss: 1.2054142951965332
Epoch 940, training loss: 0.6526819467544556 = 0.0073397038504481316 + 0.1 * 6.4534220695495605
Epoch 940, val loss: 1.2103931903839111
Epoch 950, training loss: 0.6533616781234741 = 0.007162331137806177 + 0.1 * 6.461993217468262
Epoch 950, val loss: 1.2152631282806396
Epoch 960, training loss: 0.6510323882102966 = 0.006991639267653227 + 0.1 * 6.440407752990723
Epoch 960, val loss: 1.2201112508773804
Epoch 970, training loss: 0.6521435976028442 = 0.006826980970799923 + 0.1 * 6.4531660079956055
Epoch 970, val loss: 1.2248932123184204
Epoch 980, training loss: 0.6514260172843933 = 0.006669418420642614 + 0.1 * 6.447566032409668
Epoch 980, val loss: 1.2297414541244507
Epoch 990, training loss: 0.64995938539505 = 0.006517535075545311 + 0.1 * 6.434418678283691
Epoch 990, val loss: 1.2342826128005981
Epoch 1000, training loss: 0.653121292591095 = 0.006371803116053343 + 0.1 * 6.467494964599609
Epoch 1000, val loss: 1.2389254570007324
Epoch 1010, training loss: 0.6504672765731812 = 0.006230977829545736 + 0.1 * 6.442363262176514
Epoch 1010, val loss: 1.2435272932052612
Epoch 1020, training loss: 0.6497209668159485 = 0.006095998454838991 + 0.1 * 6.436249256134033
Epoch 1020, val loss: 1.2479156255722046
Epoch 1030, training loss: 0.6493390798568726 = 0.00596569012850523 + 0.1 * 6.4337334632873535
Epoch 1030, val loss: 1.2522789239883423
Epoch 1040, training loss: 0.6480109095573425 = 0.005839688703417778 + 0.1 * 6.4217119216918945
Epoch 1040, val loss: 1.2566405534744263
Epoch 1050, training loss: 0.649104654788971 = 0.005718486849218607 + 0.1 * 6.43386173248291
Epoch 1050, val loss: 1.2608362436294556
Epoch 1060, training loss: 0.6494012475013733 = 0.005601923447102308 + 0.1 * 6.437993049621582
Epoch 1060, val loss: 1.265113353729248
Epoch 1070, training loss: 0.6483162641525269 = 0.005489060189574957 + 0.1 * 6.428272247314453
Epoch 1070, val loss: 1.2691843509674072
Epoch 1080, training loss: 0.6476969718933105 = 0.005380467511713505 + 0.1 * 6.423165321350098
Epoch 1080, val loss: 1.273269772529602
Epoch 1090, training loss: 0.6488226056098938 = 0.005275142844766378 + 0.1 * 6.435474395751953
Epoch 1090, val loss: 1.2772672176361084
Epoch 1100, training loss: 0.6467932462692261 = 0.0051733688451349735 + 0.1 * 6.41619873046875
Epoch 1100, val loss: 1.2813018560409546
Epoch 1110, training loss: 0.6461644768714905 = 0.005074968561530113 + 0.1 * 6.410894870758057
Epoch 1110, val loss: 1.2851802110671997
Epoch 1120, training loss: 0.6470667719841003 = 0.004979358520358801 + 0.1 * 6.420873641967773
Epoch 1120, val loss: 1.2890715599060059
Epoch 1130, training loss: 0.6478903889656067 = 0.004887256305664778 + 0.1 * 6.430030822753906
Epoch 1130, val loss: 1.2929695844650269
Epoch 1140, training loss: 0.6457535028457642 = 0.0047978744842112064 + 0.1 * 6.409555912017822
Epoch 1140, val loss: 1.2967220544815063
Epoch 1150, training loss: 0.646389901638031 = 0.0047116512432694435 + 0.1 * 6.416782379150391
Epoch 1150, val loss: 1.300462007522583
Epoch 1160, training loss: 0.6452358961105347 = 0.004627774003893137 + 0.1 * 6.406081199645996
Epoch 1160, val loss: 1.3041635751724243
Epoch 1170, training loss: 0.646883487701416 = 0.004546461161226034 + 0.1 * 6.423369884490967
Epoch 1170, val loss: 1.3077479600906372
Epoch 1180, training loss: 0.6454059481620789 = 0.004467970225960016 + 0.1 * 6.409379959106445
Epoch 1180, val loss: 1.3113842010498047
Epoch 1190, training loss: 0.6451650857925415 = 0.004391368478536606 + 0.1 * 6.4077372550964355
Epoch 1190, val loss: 1.3148804903030396
Epoch 1200, training loss: 0.6459991931915283 = 0.0043173679150640965 + 0.1 * 6.416818141937256
Epoch 1200, val loss: 1.3184493780136108
Epoch 1210, training loss: 0.6453070044517517 = 0.004245366435497999 + 0.1 * 6.410616397857666
Epoch 1210, val loss: 1.3219739198684692
Epoch 1220, training loss: 0.6446150541305542 = 0.00417562248185277 + 0.1 * 6.404394149780273
Epoch 1220, val loss: 1.3253310918807983
Epoch 1230, training loss: 0.6442549824714661 = 0.004107813350856304 + 0.1 * 6.4014716148376465
Epoch 1230, val loss: 1.3287297487258911
Epoch 1240, training loss: 0.6441318392753601 = 0.004041697364300489 + 0.1 * 6.4009013175964355
Epoch 1240, val loss: 1.332105040550232
Epoch 1250, training loss: 0.6443079113960266 = 0.003977772779762745 + 0.1 * 6.403301239013672
Epoch 1250, val loss: 1.3354029655456543
Epoch 1260, training loss: 0.6445688605308533 = 0.003915301524102688 + 0.1 * 6.406535625457764
Epoch 1260, val loss: 1.338709831237793
Epoch 1270, training loss: 0.6439655423164368 = 0.003854519221931696 + 0.1 * 6.4011101722717285
Epoch 1270, val loss: 1.3419876098632812
Epoch 1280, training loss: 0.6446389555931091 = 0.003795405151322484 + 0.1 * 6.408435821533203
Epoch 1280, val loss: 1.3451616764068604
Epoch 1290, training loss: 0.6427220702171326 = 0.0037380652502179146 + 0.1 * 6.389840126037598
Epoch 1290, val loss: 1.3484216928482056
Epoch 1300, training loss: 0.64328533411026 = 0.0036822864785790443 + 0.1 * 6.396029949188232
Epoch 1300, val loss: 1.351477861404419
Epoch 1310, training loss: 0.644154965877533 = 0.0036278541665524244 + 0.1 * 6.405271053314209
Epoch 1310, val loss: 1.354599952697754
Epoch 1320, training loss: 0.6419015526771545 = 0.0035752272233366966 + 0.1 * 6.383263111114502
Epoch 1320, val loss: 1.35760498046875
Epoch 1330, training loss: 0.644414484500885 = 0.0035237029660493135 + 0.1 * 6.408907413482666
Epoch 1330, val loss: 1.3605728149414062
Epoch 1340, training loss: 0.6436722278594971 = 0.0034730201587080956 + 0.1 * 6.401991844177246
Epoch 1340, val loss: 1.3636245727539062
Epoch 1350, training loss: 0.6415271162986755 = 0.003424229798838496 + 0.1 * 6.381028652191162
Epoch 1350, val loss: 1.366600751876831
Epoch 1360, training loss: 0.6423630714416504 = 0.0033768077846616507 + 0.1 * 6.389863014221191
Epoch 1360, val loss: 1.369391679763794
Epoch 1370, training loss: 0.642055332660675 = 0.0033300931099802256 + 0.1 * 6.387251853942871
Epoch 1370, val loss: 1.372298002243042
Epoch 1380, training loss: 0.6428206562995911 = 0.003284364240244031 + 0.1 * 6.3953633308410645
Epoch 1380, val loss: 1.3751944303512573
Epoch 1390, training loss: 0.6435528993606567 = 0.0032403236255049706 + 0.1 * 6.403125286102295
Epoch 1390, val loss: 1.3780797719955444
Epoch 1400, training loss: 0.6425336003303528 = 0.003197066718712449 + 0.1 * 6.393365383148193
Epoch 1400, val loss: 1.3808366060256958
Epoch 1410, training loss: 0.6411711573600769 = 0.0031548687256872654 + 0.1 * 6.380162715911865
Epoch 1410, val loss: 1.3835982084274292
Epoch 1420, training loss: 0.641961932182312 = 0.003113693557679653 + 0.1 * 6.388482570648193
Epoch 1420, val loss: 1.386300802230835
Epoch 1430, training loss: 0.6407181620597839 = 0.003073320724070072 + 0.1 * 6.376448154449463
Epoch 1430, val loss: 1.3890352249145508
Epoch 1440, training loss: 0.6424587368965149 = 0.0030340109951794147 + 0.1 * 6.394247055053711
Epoch 1440, val loss: 1.3917293548583984
Epoch 1450, training loss: 0.6403678059577942 = 0.0029958030208945274 + 0.1 * 6.373720169067383
Epoch 1450, val loss: 1.39436674118042
Epoch 1460, training loss: 0.6410472989082336 = 0.002958253026008606 + 0.1 * 6.380890369415283
Epoch 1460, val loss: 1.3969758749008179
Epoch 1470, training loss: 0.6410498023033142 = 0.0029214713722467422 + 0.1 * 6.381283283233643
Epoch 1470, val loss: 1.3996418714523315
Epoch 1480, training loss: 0.6405549645423889 = 0.0028856240678578615 + 0.1 * 6.376693248748779
Epoch 1480, val loss: 1.402225136756897
Epoch 1490, training loss: 0.641374945640564 = 0.0028507935348898172 + 0.1 * 6.385241508483887
Epoch 1490, val loss: 1.4047967195510864
Epoch 1500, training loss: 0.6399677991867065 = 0.002816465450450778 + 0.1 * 6.371513366699219
Epoch 1500, val loss: 1.407328486442566
Epoch 1510, training loss: 0.6400100588798523 = 0.0027831050101667643 + 0.1 * 6.372269153594971
Epoch 1510, val loss: 1.4098293781280518
Epoch 1520, training loss: 0.6401875019073486 = 0.0027503096498548985 + 0.1 * 6.374371528625488
Epoch 1520, val loss: 1.4122804403305054
Epoch 1530, training loss: 0.6408312320709229 = 0.0027181822806596756 + 0.1 * 6.381130218505859
Epoch 1530, val loss: 1.4148119688034058
Epoch 1540, training loss: 0.6398394703865051 = 0.0026866060215979815 + 0.1 * 6.371528625488281
Epoch 1540, val loss: 1.4172853231430054
Epoch 1550, training loss: 0.6409016251564026 = 0.0026560756377875805 + 0.1 * 6.382454872131348
Epoch 1550, val loss: 1.4196842908859253
Epoch 1560, training loss: 0.6393777132034302 = 0.0026261149905622005 + 0.1 * 6.367516040802002
Epoch 1560, val loss: 1.4220269918441772
Epoch 1570, training loss: 0.6399127840995789 = 0.0025966805405914783 + 0.1 * 6.3731608390808105
Epoch 1570, val loss: 1.4244261980056763
Epoch 1580, training loss: 0.6389597654342651 = 0.002567716408520937 + 0.1 * 6.36392068862915
Epoch 1580, val loss: 1.4267961978912354
Epoch 1590, training loss: 0.6399001479148865 = 0.0025395378470420837 + 0.1 * 6.373606204986572
Epoch 1590, val loss: 1.429155707359314
Epoch 1600, training loss: 0.6396563649177551 = 0.0025118275079876184 + 0.1 * 6.371445655822754
Epoch 1600, val loss: 1.4314765930175781
Epoch 1610, training loss: 0.6390889883041382 = 0.0024847271852195263 + 0.1 * 6.366042137145996
Epoch 1610, val loss: 1.433807611465454
Epoch 1620, training loss: 0.6404060125350952 = 0.00245797005482018 + 0.1 * 6.379480361938477
Epoch 1620, val loss: 1.4360846281051636
Epoch 1630, training loss: 0.6378733515739441 = 0.002431934466585517 + 0.1 * 6.354413986206055
Epoch 1630, val loss: 1.438359260559082
Epoch 1640, training loss: 0.639801025390625 = 0.0024064553435891867 + 0.1 * 6.373945236206055
Epoch 1640, val loss: 1.4406007528305054
Epoch 1650, training loss: 0.6390908360481262 = 0.002381227444857359 + 0.1 * 6.367095947265625
Epoch 1650, val loss: 1.442818522453308
Epoch 1660, training loss: 0.6380627751350403 = 0.0023566794116050005 + 0.1 * 6.357060432434082
Epoch 1660, val loss: 1.4449959993362427
Epoch 1670, training loss: 0.6390162706375122 = 0.0023327141534537077 + 0.1 * 6.366835594177246
Epoch 1670, val loss: 1.447108507156372
Epoch 1680, training loss: 0.6379476189613342 = 0.0023088736925274134 + 0.1 * 6.356387615203857
Epoch 1680, val loss: 1.449346661567688
Epoch 1690, training loss: 0.6380903720855713 = 0.002285820199176669 + 0.1 * 6.3580451011657715
Epoch 1690, val loss: 1.4514528512954712
Epoch 1700, training loss: 0.6381588578224182 = 0.0022629685699939728 + 0.1 * 6.358958721160889
Epoch 1700, val loss: 1.4535691738128662
Epoch 1710, training loss: 0.6381627321243286 = 0.002240836387500167 + 0.1 * 6.359218597412109
Epoch 1710, val loss: 1.455620288848877
Epoch 1720, training loss: 0.6378308534622192 = 0.0022188948933035135 + 0.1 * 6.356119155883789
Epoch 1720, val loss: 1.4577258825302124
Epoch 1730, training loss: 0.638565182685852 = 0.0021973499096930027 + 0.1 * 6.363678455352783
Epoch 1730, val loss: 1.4597551822662354
Epoch 1740, training loss: 0.6376417279243469 = 0.0021763634867966175 + 0.1 * 6.354653835296631
Epoch 1740, val loss: 1.4618242979049683
Epoch 1750, training loss: 0.6376304030418396 = 0.0021557617001235485 + 0.1 * 6.354745864868164
Epoch 1750, val loss: 1.4638077020645142
Epoch 1760, training loss: 0.6370486617088318 = 0.0021353370975703 + 0.1 * 6.349133014678955
Epoch 1760, val loss: 1.4657819271087646
Epoch 1770, training loss: 0.6378942131996155 = 0.0021153679117560387 + 0.1 * 6.3577880859375
Epoch 1770, val loss: 1.4677988290786743
Epoch 1780, training loss: 0.6379209160804749 = 0.002095795702189207 + 0.1 * 6.358251094818115
Epoch 1780, val loss: 1.469809889793396
Epoch 1790, training loss: 0.6367467641830444 = 0.002076408825814724 + 0.1 * 6.34670352935791
Epoch 1790, val loss: 1.4717495441436768
Epoch 1800, training loss: 0.6374479532241821 = 0.0020574794616550207 + 0.1 * 6.3539042472839355
Epoch 1800, val loss: 1.4737188816070557
Epoch 1810, training loss: 0.6367636322975159 = 0.002038761507719755 + 0.1 * 6.347248554229736
Epoch 1810, val loss: 1.475645899772644
Epoch 1820, training loss: 0.6378585696220398 = 0.002020347397774458 + 0.1 * 6.358382225036621
Epoch 1820, val loss: 1.4775751829147339
Epoch 1830, training loss: 0.6371623277664185 = 0.0020025470294058323 + 0.1 * 6.351597785949707
Epoch 1830, val loss: 1.4794180393218994
Epoch 1840, training loss: 0.6377813816070557 = 0.0019846796058118343 + 0.1 * 6.357966899871826
Epoch 1840, val loss: 1.4813398122787476
Epoch 1850, training loss: 0.6373172402381897 = 0.0019674012437462807 + 0.1 * 6.3534979820251465
Epoch 1850, val loss: 1.4831783771514893
Epoch 1860, training loss: 0.636222243309021 = 0.0019501961069181561 + 0.1 * 6.342720031738281
Epoch 1860, val loss: 1.484995722770691
Epoch 1870, training loss: 0.6374191045761108 = 0.0019335136748850346 + 0.1 * 6.354855537414551
Epoch 1870, val loss: 1.486761450767517
Epoch 1880, training loss: 0.636252760887146 = 0.0019168466096743941 + 0.1 * 6.343358993530273
Epoch 1880, val loss: 1.4885573387145996
Epoch 1890, training loss: 0.6356372237205505 = 0.0019006358925253153 + 0.1 * 6.337366104125977
Epoch 1890, val loss: 1.4903208017349243
Epoch 1900, training loss: 0.637371301651001 = 0.0018844527658075094 + 0.1 * 6.354868412017822
Epoch 1900, val loss: 1.4921530485153198
Epoch 1910, training loss: 0.6355602741241455 = 0.0018687833799049258 + 0.1 * 6.336914539337158
Epoch 1910, val loss: 1.4939285516738892
Epoch 1920, training loss: 0.636602520942688 = 0.0018533781403675675 + 0.1 * 6.3474907875061035
Epoch 1920, val loss: 1.495625615119934
Epoch 1930, training loss: 0.6357394456863403 = 0.0018380796536803246 + 0.1 * 6.339013576507568
Epoch 1930, val loss: 1.4973888397216797
Epoch 1940, training loss: 0.6363570094108582 = 0.0018230474088340998 + 0.1 * 6.345339298248291
Epoch 1940, val loss: 1.4990822076797485
Epoch 1950, training loss: 0.6360889673233032 = 0.0018083509057760239 + 0.1 * 6.342806339263916
Epoch 1950, val loss: 1.5007784366607666
Epoch 1960, training loss: 0.6357808709144592 = 0.0017937278607860208 + 0.1 * 6.339870929718018
Epoch 1960, val loss: 1.5025036334991455
Epoch 1970, training loss: 0.6353933215141296 = 0.0017795739695429802 + 0.1 * 6.336137771606445
Epoch 1970, val loss: 1.5041016340255737
Epoch 1980, training loss: 0.636504054069519 = 0.0017654277617111802 + 0.1 * 6.347386360168457
Epoch 1980, val loss: 1.5057868957519531
Epoch 1990, training loss: 0.6353243589401245 = 0.0017515372019261122 + 0.1 * 6.335728168487549
Epoch 1990, val loss: 1.507531762123108
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.812335266209805
The final CL Acc:0.76667, 0.00302, The final GNN Acc:0.81409, 0.00131
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13250])
remove edge: torch.Size([2, 7810])
updated graph: torch.Size([2, 10504])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8323748111724854 = 1.9726896286010742 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.9671757221221924
Epoch 10, training loss: 2.8208818435668945 = 1.9612044095993042 + 0.1 * 8.59677505493164
Epoch 10, val loss: 1.9561220407485962
Epoch 20, training loss: 2.8068699836730957 = 1.947244644165039 + 0.1 * 8.59625244140625
Epoch 20, val loss: 1.942347526550293
Epoch 30, training loss: 2.7869677543640137 = 1.927851676940918 + 0.1 * 8.591161727905273
Epoch 30, val loss: 1.9228112697601318
Epoch 40, training loss: 2.7544777393341064 = 1.8991717100143433 + 0.1 * 8.553060531616211
Epoch 40, val loss: 1.8938555717468262
Epoch 50, training loss: 2.696814775466919 = 1.859365701675415 + 0.1 * 8.374490737915039
Epoch 50, val loss: 1.8554949760437012
Epoch 60, training loss: 2.6237740516662598 = 1.8154581785202026 + 0.1 * 8.083159446716309
Epoch 60, val loss: 1.8168904781341553
Epoch 70, training loss: 2.570791721343994 = 1.7770252227783203 + 0.1 * 7.937665939331055
Epoch 70, val loss: 1.786561131477356
Epoch 80, training loss: 2.503375291824341 = 1.7399168014526367 + 0.1 * 7.634584426879883
Epoch 80, val loss: 1.7552393674850464
Epoch 90, training loss: 2.4263384342193604 = 1.6947354078292847 + 0.1 * 7.316030979156494
Epoch 90, val loss: 1.714509129524231
Epoch 100, training loss: 2.348573684692383 = 1.6353962421417236 + 0.1 * 7.131773948669434
Epoch 100, val loss: 1.6611264944076538
Epoch 110, training loss: 2.263753890991211 = 1.560319423675537 + 0.1 * 7.03434419631958
Epoch 110, val loss: 1.5945333242416382
Epoch 120, training loss: 2.173844814300537 = 1.476681113243103 + 0.1 * 6.971637725830078
Epoch 120, val loss: 1.5231415033340454
Epoch 130, training loss: 2.086468458175659 = 1.3936002254486084 + 0.1 * 6.928681373596191
Epoch 130, val loss: 1.456808090209961
Epoch 140, training loss: 2.002962827682495 = 1.3135251998901367 + 0.1 * 6.894376754760742
Epoch 140, val loss: 1.3939416408538818
Epoch 150, training loss: 1.9200633764266968 = 1.233404517173767 + 0.1 * 6.866588592529297
Epoch 150, val loss: 1.3314249515533447
Epoch 160, training loss: 1.8380085229873657 = 1.1533087491989136 + 0.1 * 6.8469977378845215
Epoch 160, val loss: 1.2692362070083618
Epoch 170, training loss: 1.7595820426940918 = 1.0764859914779663 + 0.1 * 6.830960750579834
Epoch 170, val loss: 1.2105516195297241
Epoch 180, training loss: 1.6850543022155762 = 1.0035916566848755 + 0.1 * 6.814626693725586
Epoch 180, val loss: 1.1559677124023438
Epoch 190, training loss: 1.6147563457489014 = 0.9349637627601624 + 0.1 * 6.797924995422363
Epoch 190, val loss: 1.1051290035247803
Epoch 200, training loss: 1.5486594438552856 = 0.8701595664024353 + 0.1 * 6.784998893737793
Epoch 200, val loss: 1.0571703910827637
Epoch 210, training loss: 1.4856066703796387 = 0.8080095648765564 + 0.1 * 6.775970935821533
Epoch 210, val loss: 1.011602520942688
Epoch 220, training loss: 1.4236916303634644 = 0.7470883131027222 + 0.1 * 6.766033172607422
Epoch 220, val loss: 0.9669612050056458
Epoch 230, training loss: 1.3642747402191162 = 0.6881151795387268 + 0.1 * 6.761596202850342
Epoch 230, val loss: 0.9246014356613159
Epoch 240, training loss: 1.3068218231201172 = 0.6318811178207397 + 0.1 * 6.749406337738037
Epoch 240, val loss: 0.8849592208862305
Epoch 250, training loss: 1.253117322921753 = 0.5789318680763245 + 0.1 * 6.741854190826416
Epoch 250, val loss: 0.8493306040763855
Epoch 260, training loss: 1.2033940553665161 = 0.5299767851829529 + 0.1 * 6.734172344207764
Epoch 260, val loss: 0.8185998201370239
Epoch 270, training loss: 1.1574127674102783 = 0.4847894310951233 + 0.1 * 6.72623348236084
Epoch 270, val loss: 0.7927713394165039
Epoch 280, training loss: 1.115301251411438 = 0.4424745440483093 + 0.1 * 6.728266716003418
Epoch 280, val loss: 0.77099609375
Epoch 290, training loss: 1.0746424198150635 = 0.4027567505836487 + 0.1 * 6.7188568115234375
Epoch 290, val loss: 0.7526874542236328
Epoch 300, training loss: 1.0361210107803345 = 0.3650011420249939 + 0.1 * 6.711198329925537
Epoch 300, val loss: 0.7372403740882874
Epoch 310, training loss: 0.9993891716003418 = 0.32899054884910583 + 0.1 * 6.703986167907715
Epoch 310, val loss: 0.7245160341262817
Epoch 320, training loss: 0.9660220146179199 = 0.2949974536895752 + 0.1 * 6.710245609283447
Epoch 320, val loss: 0.7147825360298157
Epoch 330, training loss: 0.9333748817443848 = 0.26360538601875305 + 0.1 * 6.697695255279541
Epoch 330, val loss: 0.7081214785575867
Epoch 340, training loss: 0.9042590856552124 = 0.23482473194599152 + 0.1 * 6.694343566894531
Epoch 340, val loss: 0.7044194936752319
Epoch 350, training loss: 0.8775204420089722 = 0.20863452553749084 + 0.1 * 6.688858509063721
Epoch 350, val loss: 0.7034863233566284
Epoch 360, training loss: 0.8546818494796753 = 0.18503780663013458 + 0.1 * 6.69644021987915
Epoch 360, val loss: 0.7051948308944702
Epoch 370, training loss: 0.8328334093093872 = 0.1641118973493576 + 0.1 * 6.6872148513793945
Epoch 370, val loss: 0.7090256810188293
Epoch 380, training loss: 0.8135119676589966 = 0.14563672244548798 + 0.1 * 6.678752422332764
Epoch 380, val loss: 0.7148117423057556
Epoch 390, training loss: 0.7975195050239563 = 0.12937454879283905 + 0.1 * 6.6814494132995605
Epoch 390, val loss: 0.7222955822944641
Epoch 400, training loss: 0.782956063747406 = 0.11517813056707382 + 0.1 * 6.677779674530029
Epoch 400, val loss: 0.7308937311172485
Epoch 410, training loss: 0.7694549560546875 = 0.10278531908988953 + 0.1 * 6.666696548461914
Epoch 410, val loss: 0.740571916103363
Epoch 420, training loss: 0.7604179978370667 = 0.0919824168086052 + 0.1 * 6.684355735778809
Epoch 420, val loss: 0.7509693503379822
Epoch 430, training loss: 0.7489849328994751 = 0.08263003826141357 + 0.1 * 6.663548946380615
Epoch 430, val loss: 0.7615230679512024
Epoch 440, training loss: 0.7400112748146057 = 0.07446909695863724 + 0.1 * 6.655421733856201
Epoch 440, val loss: 0.7724285125732422
Epoch 450, training loss: 0.7325664162635803 = 0.06731704622507095 + 0.1 * 6.652493476867676
Epoch 450, val loss: 0.7833361029624939
Epoch 460, training loss: 0.7261341214179993 = 0.06104699522256851 + 0.1 * 6.6508708000183105
Epoch 460, val loss: 0.7942665815353394
Epoch 470, training loss: 0.7200599908828735 = 0.05555335059762001 + 0.1 * 6.645066261291504
Epoch 470, val loss: 0.8047134280204773
Epoch 480, training loss: 0.7166614532470703 = 0.050702422857284546 + 0.1 * 6.659590721130371
Epoch 480, val loss: 0.8151533007621765
Epoch 490, training loss: 0.7099522352218628 = 0.046434301882982254 + 0.1 * 6.63517951965332
Epoch 490, val loss: 0.8250627517700195
Epoch 500, training loss: 0.7051732540130615 = 0.04264740273356438 + 0.1 * 6.625257968902588
Epoch 500, val loss: 0.8348774313926697
Epoch 510, training loss: 0.7022759914398193 = 0.03927953168749809 + 0.1 * 6.629964828491211
Epoch 510, val loss: 0.8445150256156921
Epoch 520, training loss: 0.6980512738227844 = 0.03629111498594284 + 0.1 * 6.61760139465332
Epoch 520, val loss: 0.8536078929901123
Epoch 530, training loss: 0.695100724697113 = 0.03361569344997406 + 0.1 * 6.614850044250488
Epoch 530, val loss: 0.8626106381416321
Epoch 540, training loss: 0.6919296383857727 = 0.031216520816087723 + 0.1 * 6.607131004333496
Epoch 540, val loss: 0.8713933229446411
Epoch 550, training loss: 0.6889179944992065 = 0.029057463631033897 + 0.1 * 6.598605155944824
Epoch 550, val loss: 0.8799226880073547
Epoch 560, training loss: 0.68669593334198 = 0.027107395231723785 + 0.1 * 6.595884799957275
Epoch 560, val loss: 0.8883329033851624
Epoch 570, training loss: 0.6853285431861877 = 0.02534971758723259 + 0.1 * 6.599788188934326
Epoch 570, val loss: 0.8966314792633057
Epoch 580, training loss: 0.6821247339248657 = 0.02376507595181465 + 0.1 * 6.583596229553223
Epoch 580, val loss: 0.904343843460083
Epoch 590, training loss: 0.6830775737762451 = 0.02232341095805168 + 0.1 * 6.607541561126709
Epoch 590, val loss: 0.9121639728546143
Epoch 600, training loss: 0.679046630859375 = 0.021013770252466202 + 0.1 * 6.580328464508057
Epoch 600, val loss: 0.9196856021881104
Epoch 610, training loss: 0.6772274971008301 = 0.01981784589588642 + 0.1 * 6.5740966796875
Epoch 610, val loss: 0.9269728660583496
Epoch 620, training loss: 0.6774725914001465 = 0.01872165873646736 + 0.1 * 6.587509632110596
Epoch 620, val loss: 0.934177041053772
Epoch 630, training loss: 0.6763826608657837 = 0.017719360068440437 + 0.1 * 6.58663272857666
Epoch 630, val loss: 0.9411406517028809
Epoch 640, training loss: 0.6730506420135498 = 0.016800951212644577 + 0.1 * 6.562497138977051
Epoch 640, val loss: 0.9478116631507874
Epoch 650, training loss: 0.67318195104599 = 0.015954675152897835 + 0.1 * 6.572272777557373
Epoch 650, val loss: 0.9544309377670288
Epoch 660, training loss: 0.6712523102760315 = 0.015174370259046555 + 0.1 * 6.560779094696045
Epoch 660, val loss: 0.9607568979263306
Epoch 670, training loss: 0.6707073450088501 = 0.014452879317104816 + 0.1 * 6.562544345855713
Epoch 670, val loss: 0.9670612215995789
Epoch 680, training loss: 0.6704168319702148 = 0.013785332441329956 + 0.1 * 6.566315174102783
Epoch 680, val loss: 0.9731455445289612
Epoch 690, training loss: 0.6675730347633362 = 0.013168453238904476 + 0.1 * 6.544045925140381
Epoch 690, val loss: 0.9789456725120544
Epoch 700, training loss: 0.6677671670913696 = 0.012594284489750862 + 0.1 * 6.55172872543335
Epoch 700, val loss: 0.984629213809967
Epoch 710, training loss: 0.6666690111160278 = 0.012058920226991177 + 0.1 * 6.546101093292236
Epoch 710, val loss: 0.9902012348175049
Epoch 720, training loss: 0.6654556393623352 = 0.011560792103409767 + 0.1 * 6.5389485359191895
Epoch 720, val loss: 0.9956320524215698
Epoch 730, training loss: 0.6660748720169067 = 0.011095858179032803 + 0.1 * 6.549789905548096
Epoch 730, val loss: 1.0009701251983643
Epoch 740, training loss: 0.6642052531242371 = 0.010663997381925583 + 0.1 * 6.535412311553955
Epoch 740, val loss: 1.0060055255889893
Epoch 750, training loss: 0.6626153588294983 = 0.010259315371513367 + 0.1 * 6.523560047149658
Epoch 750, val loss: 1.0107619762420654
Epoch 760, training loss: 0.662469208240509 = 0.009877623058855534 + 0.1 * 6.525915622711182
Epoch 760, val loss: 1.0156925916671753
Epoch 770, training loss: 0.6612834930419922 = 0.009519311599433422 + 0.1 * 6.517641544342041
Epoch 770, val loss: 1.0203781127929688
Epoch 780, training loss: 0.6629263162612915 = 0.009182412177324295 + 0.1 * 6.537438869476318
Epoch 780, val loss: 1.0252567529678345
Epoch 790, training loss: 0.6597649455070496 = 0.00886712595820427 + 0.1 * 6.508977890014648
Epoch 790, val loss: 1.029541015625
Epoch 800, training loss: 0.6597819924354553 = 0.008569275960326195 + 0.1 * 6.512126922607422
Epoch 800, val loss: 1.0337339639663696
Epoch 810, training loss: 0.6587468385696411 = 0.008286097086966038 + 0.1 * 6.504607200622559
Epoch 810, val loss: 1.0382734537124634
Epoch 820, training loss: 0.6578983664512634 = 0.008019655011594296 + 0.1 * 6.4987874031066895
Epoch 820, val loss: 1.042450189590454
Epoch 830, training loss: 0.6580107808113098 = 0.007766752503812313 + 0.1 * 6.502440452575684
Epoch 830, val loss: 1.046566367149353
Epoch 840, training loss: 0.6598318815231323 = 0.007525845896452665 + 0.1 * 6.523060321807861
Epoch 840, val loss: 1.0508979558944702
Epoch 850, training loss: 0.656886637210846 = 0.007299566175788641 + 0.1 * 6.495870590209961
Epoch 850, val loss: 1.0548683404922485
Epoch 860, training loss: 0.655785858631134 = 0.007084704469889402 + 0.1 * 6.487010955810547
Epoch 860, val loss: 1.0584684610366821
Epoch 870, training loss: 0.6554509997367859 = 0.006878958083689213 + 0.1 * 6.485720157623291
Epoch 870, val loss: 1.0623340606689453
Epoch 880, training loss: 0.656666100025177 = 0.006682842504233122 + 0.1 * 6.499832630157471
Epoch 880, val loss: 1.0662497282028198
Epoch 890, training loss: 0.6543137431144714 = 0.00649681081995368 + 0.1 * 6.4781694412231445
Epoch 890, val loss: 1.0700563192367554
Epoch 900, training loss: 0.6544138789176941 = 0.006320217624306679 + 0.1 * 6.480936527252197
Epoch 900, val loss: 1.0734699964523315
Epoch 910, training loss: 0.655449390411377 = 0.006151167675852776 + 0.1 * 6.492982387542725
Epoch 910, val loss: 1.0769165754318237
Epoch 920, training loss: 0.6540051102638245 = 0.00598920276388526 + 0.1 * 6.480159282684326
Epoch 920, val loss: 1.080688238143921
Epoch 930, training loss: 0.6535791754722595 = 0.0058352467603981495 + 0.1 * 6.4774394035339355
Epoch 930, val loss: 1.0839775800704956
Epoch 940, training loss: 0.6533023118972778 = 0.005687414202839136 + 0.1 * 6.47614860534668
Epoch 940, val loss: 1.0874347686767578
Epoch 950, training loss: 0.6520002484321594 = 0.005546432454138994 + 0.1 * 6.464537620544434
Epoch 950, val loss: 1.090786337852478
Epoch 960, training loss: 0.6528851389884949 = 0.00541165005415678 + 0.1 * 6.474734783172607
Epoch 960, val loss: 1.0939117670059204
Epoch 970, training loss: 0.6512237191200256 = 0.005281925201416016 + 0.1 * 6.459417819976807
Epoch 970, val loss: 1.0971646308898926
Epoch 980, training loss: 0.6512694954872131 = 0.005157724022865295 + 0.1 * 6.461117744445801
Epoch 980, val loss: 1.100429654121399
Epoch 990, training loss: 0.650446891784668 = 0.00503904465585947 + 0.1 * 6.454078197479248
Epoch 990, val loss: 1.1034162044525146
Epoch 1000, training loss: 0.6521931290626526 = 0.004924808628857136 + 0.1 * 6.472682952880859
Epoch 1000, val loss: 1.1064163446426392
Epoch 1010, training loss: 0.65101557970047 = 0.004814666695892811 + 0.1 * 6.462008953094482
Epoch 1010, val loss: 1.1094902753829956
Epoch 1020, training loss: 0.651585578918457 = 0.004709309432655573 + 0.1 * 6.4687628746032715
Epoch 1020, val loss: 1.112478494644165
Epoch 1030, training loss: 0.6500053405761719 = 0.004608026705682278 + 0.1 * 6.453972816467285
Epoch 1030, val loss: 1.1152924299240112
Epoch 1040, training loss: 0.6496623158454895 = 0.00451029185205698 + 0.1 * 6.451519966125488
Epoch 1040, val loss: 1.11815345287323
Epoch 1050, training loss: 0.649370551109314 = 0.004416115581989288 + 0.1 * 6.449544429779053
Epoch 1050, val loss: 1.1210014820098877
Epoch 1060, training loss: 0.6512474417686462 = 0.004325381945818663 + 0.1 * 6.4692206382751465
Epoch 1060, val loss: 1.1237808465957642
Epoch 1070, training loss: 0.6481651663780212 = 0.004238142166286707 + 0.1 * 6.439270496368408
Epoch 1070, val loss: 1.1265727281570435
Epoch 1080, training loss: 0.6490670442581177 = 0.0041542863473296165 + 0.1 * 6.449127674102783
Epoch 1080, val loss: 1.1291126012802124
Epoch 1090, training loss: 0.6488135457038879 = 0.004072857089340687 + 0.1 * 6.447406768798828
Epoch 1090, val loss: 1.1317307949066162
Epoch 1100, training loss: 0.6474410891532898 = 0.0039942823350429535 + 0.1 * 6.434467792510986
Epoch 1100, val loss: 1.1344558000564575
Epoch 1110, training loss: 0.6482686996459961 = 0.003918693400919437 + 0.1 * 6.44350004196167
Epoch 1110, val loss: 1.136890172958374
Epoch 1120, training loss: 0.6470474600791931 = 0.0038452043663710356 + 0.1 * 6.4320220947265625
Epoch 1120, val loss: 1.1394988298416138
Epoch 1130, training loss: 0.647790789604187 = 0.003774250391870737 + 0.1 * 6.4401655197143555
Epoch 1130, val loss: 1.1421396732330322
Epoch 1140, training loss: 0.6475639939308167 = 0.003705924144014716 + 0.1 * 6.438580513000488
Epoch 1140, val loss: 1.144694447517395
Epoch 1150, training loss: 0.6454976201057434 = 0.0036398235242813826 + 0.1 * 6.418578147888184
Epoch 1150, val loss: 1.146980881690979
Epoch 1160, training loss: 0.6466508507728577 = 0.0035759119782596827 + 0.1 * 6.43074893951416
Epoch 1160, val loss: 1.1491566896438599
Epoch 1170, training loss: 0.6462915539741516 = 0.003513297066092491 + 0.1 * 6.4277825355529785
Epoch 1170, val loss: 1.1517692804336548
Epoch 1180, training loss: 0.6484906673431396 = 0.0034531736746430397 + 0.1 * 6.450374603271484
Epoch 1180, val loss: 1.1542209386825562
Epoch 1190, training loss: 0.6465765833854675 = 0.003394805360585451 + 0.1 * 6.431818008422852
Epoch 1190, val loss: 1.1566181182861328
Epoch 1200, training loss: 0.6449419260025024 = 0.0033384975977241993 + 0.1 * 6.41603422164917
Epoch 1200, val loss: 1.1587510108947754
Epoch 1210, training loss: 0.645576536655426 = 0.00328386714681983 + 0.1 * 6.422926425933838
Epoch 1210, val loss: 1.1608201265335083
Epoch 1220, training loss: 0.6448509693145752 = 0.0032305007334798574 + 0.1 * 6.416204452514648
Epoch 1220, val loss: 1.1630823612213135
Epoch 1230, training loss: 0.644580602645874 = 0.003178888000547886 + 0.1 * 6.4140167236328125
Epoch 1230, val loss: 1.1652222871780396
Epoch 1240, training loss: 0.6439782977104187 = 0.003128558164462447 + 0.1 * 6.408497333526611
Epoch 1240, val loss: 1.1674318313598633
Epoch 1250, training loss: 0.6452980637550354 = 0.003079683054238558 + 0.1 * 6.422183513641357
Epoch 1250, val loss: 1.1696058511734009
Epoch 1260, training loss: 0.6450256705284119 = 0.003032291540876031 + 0.1 * 6.419933795928955
Epoch 1260, val loss: 1.1718266010284424
Epoch 1270, training loss: 0.6439000964164734 = 0.0029864355456084013 + 0.1 * 6.4091362953186035
Epoch 1270, val loss: 1.173833966255188
Epoch 1280, training loss: 0.6445558071136475 = 0.0029416929464787245 + 0.1 * 6.416141510009766
Epoch 1280, val loss: 1.1758174896240234
Epoch 1290, training loss: 0.6432534456253052 = 0.002898403210565448 + 0.1 * 6.403550148010254
Epoch 1290, val loss: 1.1777433156967163
Epoch 1300, training loss: 0.6447173357009888 = 0.002856186591088772 + 0.1 * 6.4186110496521
Epoch 1300, val loss: 1.1796138286590576
Epoch 1310, training loss: 0.6433312296867371 = 0.00281483749859035 + 0.1 * 6.405163764953613
Epoch 1310, val loss: 1.1817346811294556
Epoch 1320, training loss: 0.6428747773170471 = 0.0027748614083975554 + 0.1 * 6.400999069213867
Epoch 1320, val loss: 1.183613896369934
Epoch 1330, training loss: 0.6447774767875671 = 0.0027356611099094152 + 0.1 * 6.420417785644531
Epoch 1330, val loss: 1.1855512857437134
Epoch 1340, training loss: 0.6433408260345459 = 0.0026975004002451897 + 0.1 * 6.40643310546875
Epoch 1340, val loss: 1.1876929998397827
Epoch 1350, training loss: 0.6426805853843689 = 0.002660745056346059 + 0.1 * 6.400198459625244
Epoch 1350, val loss: 1.1894538402557373
Epoch 1360, training loss: 0.6423685550689697 = 0.002624650252982974 + 0.1 * 6.397439002990723
Epoch 1360, val loss: 1.1911795139312744
Epoch 1370, training loss: 0.6419054865837097 = 0.0025895319413393736 + 0.1 * 6.39315938949585
Epoch 1370, val loss: 1.1928856372833252
Epoch 1380, training loss: 0.6417789459228516 = 0.0025552089791744947 + 0.1 * 6.392237663269043
Epoch 1380, val loss: 1.1946842670440674
Epoch 1390, training loss: 0.642460286617279 = 0.0025217863731086254 + 0.1 * 6.399384498596191
Epoch 1390, val loss: 1.1964153051376343
Epoch 1400, training loss: 0.6431636214256287 = 0.002488871803507209 + 0.1 * 6.406747341156006
Epoch 1400, val loss: 1.1983857154846191
Epoch 1410, training loss: 0.6414991021156311 = 0.0024570594541728497 + 0.1 * 6.390419960021973
Epoch 1410, val loss: 1.2001653909683228
Epoch 1420, training loss: 0.6423934102058411 = 0.0024260266218334436 + 0.1 * 6.3996734619140625
Epoch 1420, val loss: 1.2017079591751099
Epoch 1430, training loss: 0.6412180066108704 = 0.002395615680143237 + 0.1 * 6.388223648071289
Epoch 1430, val loss: 1.203384518623352
Epoch 1440, training loss: 0.6426604986190796 = 0.0023658345453441143 + 0.1 * 6.402946472167969
Epoch 1440, val loss: 1.2052130699157715
Epoch 1450, training loss: 0.6417301297187805 = 0.0023368014954030514 + 0.1 * 6.393933296203613
Epoch 1450, val loss: 1.2069889307022095
Epoch 1460, training loss: 0.6418808698654175 = 0.0023085547145456076 + 0.1 * 6.395723342895508
Epoch 1460, val loss: 1.208557367324829
Epoch 1470, training loss: 0.6405228972434998 = 0.0022808758076280355 + 0.1 * 6.382420063018799
Epoch 1470, val loss: 1.2100850343704224
Epoch 1480, training loss: 0.6408485770225525 = 0.002253748243674636 + 0.1 * 6.385948181152344
Epoch 1480, val loss: 1.2117102146148682
Epoch 1490, training loss: 0.6399344801902771 = 0.002227307762950659 + 0.1 * 6.377071380615234
Epoch 1490, val loss: 1.2133296728134155
Epoch 1500, training loss: 0.6411008238792419 = 0.002201333874836564 + 0.1 * 6.3889946937561035
Epoch 1500, val loss: 1.2148892879486084
Epoch 1510, training loss: 0.640661358833313 = 0.0021758703514933586 + 0.1 * 6.384854793548584
Epoch 1510, val loss: 1.2165371179580688
Epoch 1520, training loss: 0.6402446031570435 = 0.002151039196178317 + 0.1 * 6.3809356689453125
Epoch 1520, val loss: 1.218166470527649
Epoch 1530, training loss: 0.6393712162971497 = 0.002126815263181925 + 0.1 * 6.372443675994873
Epoch 1530, val loss: 1.2196537256240845
Epoch 1540, training loss: 0.6406064629554749 = 0.0021031470969319344 + 0.1 * 6.385033130645752
Epoch 1540, val loss: 1.2209985256195068
Epoch 1550, training loss: 0.640282392501831 = 0.002079758560284972 + 0.1 * 6.382025718688965
Epoch 1550, val loss: 1.2225779294967651
Epoch 1560, training loss: 0.6389366984367371 = 0.0020569583866745234 + 0.1 * 6.368797302246094
Epoch 1560, val loss: 1.224084734916687
Epoch 1570, training loss: 0.6394386291503906 = 0.002034567529335618 + 0.1 * 6.374040603637695
Epoch 1570, val loss: 1.225616693496704
Epoch 1580, training loss: 0.6386831402778625 = 0.0020127324387431145 + 0.1 * 6.366703987121582
Epoch 1580, val loss: 1.2271031141281128
Epoch 1590, training loss: 0.6396417617797852 = 0.001991336001083255 + 0.1 * 6.376504421234131
Epoch 1590, val loss: 1.2284868955612183
Epoch 1600, training loss: 0.6389570236206055 = 0.001970287412405014 + 0.1 * 6.369867324829102
Epoch 1600, val loss: 1.2299314737319946
Epoch 1610, training loss: 0.6406971216201782 = 0.0019497472094371915 + 0.1 * 6.3874735832214355
Epoch 1610, val loss: 1.2313798666000366
Epoch 1620, training loss: 0.6386106610298157 = 0.0019296742975711823 + 0.1 * 6.366809368133545
Epoch 1620, val loss: 1.2328044176101685
Epoch 1630, training loss: 0.6397521495819092 = 0.0019100091885775328 + 0.1 * 6.378421306610107
Epoch 1630, val loss: 1.2340574264526367
Epoch 1640, training loss: 0.6384409666061401 = 0.0018906055483967066 + 0.1 * 6.365503787994385
Epoch 1640, val loss: 1.2353795766830444
Epoch 1650, training loss: 0.6385968923568726 = 0.0018716594204306602 + 0.1 * 6.367252349853516
Epoch 1650, val loss: 1.2366745471954346
Epoch 1660, training loss: 0.6387652158737183 = 0.0018529504304751754 + 0.1 * 6.3691229820251465
Epoch 1660, val loss: 1.2381664514541626
Epoch 1670, training loss: 0.6382024884223938 = 0.0018347017467021942 + 0.1 * 6.363677978515625
Epoch 1670, val loss: 1.2395708560943604
Epoch 1680, training loss: 0.6388024091720581 = 0.0018168137175962329 + 0.1 * 6.369855880737305
Epoch 1680, val loss: 1.2407798767089844
Epoch 1690, training loss: 0.6378968954086304 = 0.0017991089262068272 + 0.1 * 6.360977649688721
Epoch 1690, val loss: 1.2421550750732422
Epoch 1700, training loss: 0.6389299035072327 = 0.001781906234100461 + 0.1 * 6.3714799880981445
Epoch 1700, val loss: 1.2434649467468262
Epoch 1710, training loss: 0.6367717385292053 = 0.0017649834044277668 + 0.1 * 6.350067138671875
Epoch 1710, val loss: 1.2446573972702026
Epoch 1720, training loss: 0.6390366554260254 = 0.0017483799019828439 + 0.1 * 6.37288236618042
Epoch 1720, val loss: 1.245802879333496
Epoch 1730, training loss: 0.6381375193595886 = 0.0017319503240287304 + 0.1 * 6.364055633544922
Epoch 1730, val loss: 1.2471896409988403
Epoch 1740, training loss: 0.637703001499176 = 0.001715927035547793 + 0.1 * 6.359870910644531
Epoch 1740, val loss: 1.2484194040298462
Epoch 1750, training loss: 0.6366664171218872 = 0.0017001022351905704 + 0.1 * 6.349662780761719
Epoch 1750, val loss: 1.2496942281723022
Epoch 1760, training loss: 0.637961745262146 = 0.001684648683294654 + 0.1 * 6.3627705574035645
Epoch 1760, val loss: 1.2508342266082764
Epoch 1770, training loss: 0.6368961334228516 = 0.0016693277284502983 + 0.1 * 6.352267742156982
Epoch 1770, val loss: 1.252181887626648
Epoch 1780, training loss: 0.6385800838470459 = 0.0016544278478249907 + 0.1 * 6.369256496429443
Epoch 1780, val loss: 1.2534255981445312
Epoch 1790, training loss: 0.6363431811332703 = 0.0016396952560171485 + 0.1 * 6.347034454345703
Epoch 1790, val loss: 1.2546082735061646
Epoch 1800, training loss: 0.6380662322044373 = 0.0016252694185823202 + 0.1 * 6.364409923553467
Epoch 1800, val loss: 1.2556096315383911
Epoch 1810, training loss: 0.6369295716285706 = 0.0016110477736219764 + 0.1 * 6.353185176849365
Epoch 1810, val loss: 1.256799578666687
Epoch 1820, training loss: 0.6369364261627197 = 0.0015970389358699322 + 0.1 * 6.3533935546875
Epoch 1820, val loss: 1.2580238580703735
Epoch 1830, training loss: 0.6364566087722778 = 0.0015832704957574606 + 0.1 * 6.348733425140381
Epoch 1830, val loss: 1.2592936754226685
Epoch 1840, training loss: 0.6372256875038147 = 0.0015698530478402972 + 0.1 * 6.356558322906494
Epoch 1840, val loss: 1.2603082656860352
Epoch 1850, training loss: 0.6353526711463928 = 0.0015565367648378015 + 0.1 * 6.337961196899414
Epoch 1850, val loss: 1.2612998485565186
Epoch 1860, training loss: 0.637717068195343 = 0.0015435043023899198 + 0.1 * 6.3617353439331055
Epoch 1860, val loss: 1.262270212173462
Epoch 1870, training loss: 0.6363856792449951 = 0.0015305100241675973 + 0.1 * 6.348551273345947
Epoch 1870, val loss: 1.263573169708252
Epoch 1880, training loss: 0.6361750364303589 = 0.0015179127221927047 + 0.1 * 6.346571445465088
Epoch 1880, val loss: 1.2646170854568481
Epoch 1890, training loss: 0.6362110376358032 = 0.001505427761003375 + 0.1 * 6.347055912017822
Epoch 1890, val loss: 1.2655978202819824
Epoch 1900, training loss: 0.6354271769523621 = 0.0014930808683857322 + 0.1 * 6.339340686798096
Epoch 1900, val loss: 1.2667059898376465
Epoch 1910, training loss: 0.6376601457595825 = 0.0014809761196374893 + 0.1 * 6.361791610717773
Epoch 1910, val loss: 1.2678121328353882
Epoch 1920, training loss: 0.6352362632751465 = 0.0014690671814605594 + 0.1 * 6.337671756744385
Epoch 1920, val loss: 1.269028902053833
Epoch 1930, training loss: 0.637102484703064 = 0.0014573867665603757 + 0.1 * 6.356451034545898
Epoch 1930, val loss: 1.2699968814849854
Epoch 1940, training loss: 0.634945809841156 = 0.001445842906832695 + 0.1 * 6.3349995613098145
Epoch 1940, val loss: 1.2710378170013428
Epoch 1950, training loss: 0.6361395716667175 = 0.001434587175026536 + 0.1 * 6.347049713134766
Epoch 1950, val loss: 1.2718610763549805
Epoch 1960, training loss: 0.6347665190696716 = 0.0014233696274459362 + 0.1 * 6.333431720733643
Epoch 1960, val loss: 1.2727464437484741
Epoch 1970, training loss: 0.6368011832237244 = 0.0014123343862593174 + 0.1 * 6.353888511657715
Epoch 1970, val loss: 1.273752212524414
Epoch 1980, training loss: 0.6346141695976257 = 0.0014014202170073986 + 0.1 * 6.332127571105957
Epoch 1980, val loss: 1.2748966217041016
Epoch 1990, training loss: 0.6366221904754639 = 0.0013907639076933265 + 0.1 * 6.352313995361328
Epoch 1990, val loss: 1.2757302522659302
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 2.8204903602600098 = 1.96080482006073 + 0.1 * 8.596854209899902
Epoch 0, val loss: 1.9621518850326538
Epoch 10, training loss: 2.8091511726379395 = 1.9494730234146118 + 0.1 * 8.596781730651855
Epoch 10, val loss: 1.9502265453338623
Epoch 20, training loss: 2.795172691345215 = 1.9355369806289673 + 0.1 * 8.596357345581055
Epoch 20, val loss: 1.9354424476623535
Epoch 30, training loss: 2.7755422592163086 = 1.9162579774856567 + 0.1 * 8.592844009399414
Epoch 30, val loss: 1.9149755239486694
Epoch 40, training loss: 2.7449045181274414 = 1.8884108066558838 + 0.1 * 8.564936637878418
Epoch 40, val loss: 1.8858200311660767
Epoch 50, training loss: 2.693993330001831 = 1.8508646488189697 + 0.1 * 8.431286811828613
Epoch 50, val loss: 1.8484102487564087
Epoch 60, training loss: 2.6161115169525146 = 1.8104417324066162 + 0.1 * 8.056696891784668
Epoch 60, val loss: 1.8118048906326294
Epoch 70, training loss: 2.566389322280884 = 1.7751364707946777 + 0.1 * 7.912527561187744
Epoch 70, val loss: 1.7826191186904907
Epoch 80, training loss: 2.5008127689361572 = 1.7369033098220825 + 0.1 * 7.639094352722168
Epoch 80, val loss: 1.750243067741394
Epoch 90, training loss: 2.4240918159484863 = 1.688410997390747 + 0.1 * 7.356808185577393
Epoch 90, val loss: 1.708221673965454
Epoch 100, training loss: 2.3448801040649414 = 1.625167727470398 + 0.1 * 7.19712495803833
Epoch 100, val loss: 1.6535155773162842
Epoch 110, training loss: 2.2574422359466553 = 1.5469796657562256 + 0.1 * 7.1046247482299805
Epoch 110, val loss: 1.587364912033081
Epoch 120, training loss: 2.168059825897217 = 1.4631708860397339 + 0.1 * 7.048888683319092
Epoch 120, val loss: 1.5195225477218628
Epoch 130, training loss: 2.0832743644714355 = 1.3817870616912842 + 0.1 * 7.014872074127197
Epoch 130, val loss: 1.4581245183944702
Epoch 140, training loss: 2.002013683319092 = 1.3027939796447754 + 0.1 * 6.992197513580322
Epoch 140, val loss: 1.4007138013839722
Epoch 150, training loss: 1.9193222522735596 = 1.22239089012146 + 0.1 * 6.969314098358154
Epoch 150, val loss: 1.342397689819336
Epoch 160, training loss: 1.8333144187927246 = 1.1392570734024048 + 0.1 * 6.940573692321777
Epoch 160, val loss: 1.28140127658844
Epoch 170, training loss: 1.7473725080490112 = 1.0559276342391968 + 0.1 * 6.9144487380981445
Epoch 170, val loss: 1.2205398082733154
Epoch 180, training loss: 1.6644725799560547 = 0.9747947454452515 + 0.1 * 6.896778583526611
Epoch 180, val loss: 1.161543369293213
Epoch 190, training loss: 1.5856823921203613 = 0.8968492150306702 + 0.1 * 6.888331413269043
Epoch 190, val loss: 1.1038572788238525
Epoch 200, training loss: 1.5113654136657715 = 0.8231858015060425 + 0.1 * 6.881795406341553
Epoch 200, val loss: 1.0486748218536377
Epoch 210, training loss: 1.4437925815582275 = 0.755837619304657 + 0.1 * 6.879549503326416
Epoch 210, val loss: 0.9984447360038757
Epoch 220, training loss: 1.3833839893341064 = 0.6962971687316895 + 0.1 * 6.870868682861328
Epoch 220, val loss: 0.9554179906845093
Epoch 230, training loss: 1.3301644325256348 = 0.6437765955924988 + 0.1 * 6.86387825012207
Epoch 230, val loss: 0.9193350672721863
Epoch 240, training loss: 1.2819595336914062 = 0.5964239239692688 + 0.1 * 6.855356216430664
Epoch 240, val loss: 0.8885341882705688
Epoch 250, training loss: 1.2363858222961426 = 0.5516493320465088 + 0.1 * 6.847364902496338
Epoch 250, val loss: 0.8604283928871155
Epoch 260, training loss: 1.1912562847137451 = 0.5073912143707275 + 0.1 * 6.838650703430176
Epoch 260, val loss: 0.833192765712738
Epoch 270, training loss: 1.1452540159225464 = 0.462618350982666 + 0.1 * 6.826356410980225
Epoch 270, val loss: 0.8064785599708557
Epoch 280, training loss: 1.100359320640564 = 0.41707292199134827 + 0.1 * 6.8328633308410645
Epoch 280, val loss: 0.7807192206382751
Epoch 290, training loss: 1.0525894165039062 = 0.3720332384109497 + 0.1 * 6.805561542510986
Epoch 290, val loss: 0.7568323612213135
Epoch 300, training loss: 1.0074849128723145 = 0.3282252252101898 + 0.1 * 6.792596340179443
Epoch 300, val loss: 0.7357041835784912
Epoch 310, training loss: 0.9645075798034668 = 0.28693774342536926 + 0.1 * 6.775698661804199
Epoch 310, val loss: 0.7181340456008911
Epoch 320, training loss: 0.9264047145843506 = 0.24962468445301056 + 0.1 * 6.767800331115723
Epoch 320, val loss: 0.7048182487487793
Epoch 330, training loss: 0.8928017020225525 = 0.2169719934463501 + 0.1 * 6.758296966552734
Epoch 330, val loss: 0.6956498622894287
Epoch 340, training loss: 0.8628412485122681 = 0.18859317898750305 + 0.1 * 6.742480278015137
Epoch 340, val loss: 0.6903548240661621
Epoch 350, training loss: 0.8372215628623962 = 0.16414044797420502 + 0.1 * 6.730810642242432
Epoch 350, val loss: 0.6888630986213684
Epoch 360, training loss: 0.8178763389587402 = 0.14329536259174347 + 0.1 * 6.745809555053711
Epoch 360, val loss: 0.6906589865684509
Epoch 370, training loss: 0.7970719933509827 = 0.12582534551620483 + 0.1 * 6.712466239929199
Epoch 370, val loss: 0.6945418119430542
Epoch 380, training loss: 0.7816128134727478 = 0.11101628839969635 + 0.1 * 6.705965042114258
Epoch 380, val loss: 0.7004507184028625
Epoch 390, training loss: 0.7684351801872253 = 0.09849237650632858 + 0.1 * 6.699427604675293
Epoch 390, val loss: 0.7077577114105225
Epoch 400, training loss: 0.7561910152435303 = 0.08782771229743958 + 0.1 * 6.6836323738098145
Epoch 400, val loss: 0.7160224914550781
Epoch 410, training loss: 0.7459909915924072 = 0.07866965234279633 + 0.1 * 6.673213005065918
Epoch 410, val loss: 0.7249462604522705
Epoch 420, training loss: 0.7389044165611267 = 0.07075882703065872 + 0.1 * 6.681456089019775
Epoch 420, val loss: 0.7344273924827576
Epoch 430, training loss: 0.7296061515808105 = 0.06395871192216873 + 0.1 * 6.6564741134643555
Epoch 430, val loss: 0.7441539764404297
Epoch 440, training loss: 0.7238256931304932 = 0.05805373936891556 + 0.1 * 6.657719612121582
Epoch 440, val loss: 0.7540245056152344
Epoch 450, training loss: 0.7176241874694824 = 0.052915167063474655 + 0.1 * 6.647089958190918
Epoch 450, val loss: 0.7639170289039612
Epoch 460, training loss: 0.7128782868385315 = 0.04841255396604538 + 0.1 * 6.644657135009766
Epoch 460, val loss: 0.773759663105011
Epoch 470, training loss: 0.7079864144325256 = 0.044451113790273666 + 0.1 * 6.635352611541748
Epoch 470, val loss: 0.7834490537643433
Epoch 480, training loss: 0.7040416598320007 = 0.040957119315862656 + 0.1 * 6.630845546722412
Epoch 480, val loss: 0.7930841445922852
Epoch 490, training loss: 0.6998302340507507 = 0.037864670157432556 + 0.1 * 6.619655609130859
Epoch 490, val loss: 0.8024073243141174
Epoch 500, training loss: 0.6972436904907227 = 0.035109300166368484 + 0.1 * 6.621344089508057
Epoch 500, val loss: 0.8116297721862793
Epoch 510, training loss: 0.6956472396850586 = 0.032649360597133636 + 0.1 * 6.629979133605957
Epoch 510, val loss: 0.8206426501274109
Epoch 520, training loss: 0.6915913224220276 = 0.030454516410827637 + 0.1 * 6.611368179321289
Epoch 520, val loss: 0.8293408751487732
Epoch 530, training loss: 0.6918044090270996 = 0.028478235006332397 + 0.1 * 6.633261203765869
Epoch 530, val loss: 0.8379128575325012
Epoch 540, training loss: 0.6861432194709778 = 0.026702184230089188 + 0.1 * 6.594410419464111
Epoch 540, val loss: 0.8461225032806396
Epoch 550, training loss: 0.6840940117835999 = 0.025089923292398453 + 0.1 * 6.590040683746338
Epoch 550, val loss: 0.8541645407676697
Epoch 560, training loss: 0.6828424334526062 = 0.023627279326319695 + 0.1 * 6.592151641845703
Epoch 560, val loss: 0.8620456457138062
Epoch 570, training loss: 0.6811782717704773 = 0.022293904796242714 + 0.1 * 6.58884334564209
Epoch 570, val loss: 0.8696606159210205
Epoch 580, training loss: 0.6787750124931335 = 0.021075822412967682 + 0.1 * 6.576991558074951
Epoch 580, val loss: 0.8771213293075562
Epoch 590, training loss: 0.6793964505195618 = 0.01995784044265747 + 0.1 * 6.594386100769043
Epoch 590, val loss: 0.8844327330589294
Epoch 600, training loss: 0.6760683059692383 = 0.018936127424240112 + 0.1 * 6.571321964263916
Epoch 600, val loss: 0.8914552330970764
Epoch 610, training loss: 0.6752331256866455 = 0.017998106777668 + 0.1 * 6.572350025177002
Epoch 610, val loss: 0.8982542157173157
Epoch 620, training loss: 0.6726737022399902 = 0.017133057117462158 + 0.1 * 6.55540657043457
Epoch 620, val loss: 0.9049595594406128
Epoch 630, training loss: 0.6711364388465881 = 0.01633155718445778 + 0.1 * 6.548048973083496
Epoch 630, val loss: 0.911460816860199
Epoch 640, training loss: 0.671604573726654 = 0.015586872585117817 + 0.1 * 6.560177326202393
Epoch 640, val loss: 0.9177977442741394
Epoch 650, training loss: 0.670582115650177 = 0.01489899680018425 + 0.1 * 6.556830883026123
Epoch 650, val loss: 0.9241130352020264
Epoch 660, training loss: 0.668919563293457 = 0.014259619638323784 + 0.1 * 6.546599388122559
Epoch 660, val loss: 0.93006831407547
Epoch 670, training loss: 0.6673856973648071 = 0.01366537157446146 + 0.1 * 6.537202835083008
Epoch 670, val loss: 0.9359817504882812
Epoch 680, training loss: 0.6663642525672913 = 0.013109461404383183 + 0.1 * 6.532547950744629
Epoch 680, val loss: 0.9416719079017639
Epoch 690, training loss: 0.665776789188385 = 0.012588750571012497 + 0.1 * 6.5318803787231445
Epoch 690, val loss: 0.9473370313644409
Epoch 700, training loss: 0.665316641330719 = 0.012101789005100727 + 0.1 * 6.532148361206055
Epoch 700, val loss: 0.9528765678405762
Epoch 710, training loss: 0.6641039252281189 = 0.01164502464234829 + 0.1 * 6.5245890617370605
Epoch 710, val loss: 0.958294689655304
Epoch 720, training loss: 0.6627505421638489 = 0.011217200197279453 + 0.1 * 6.515333652496338
Epoch 720, val loss: 0.9634851813316345
Epoch 730, training loss: 0.6618964672088623 = 0.010813883505761623 + 0.1 * 6.5108256340026855
Epoch 730, val loss: 0.9686971306800842
Epoch 740, training loss: 0.6624116897583008 = 0.01043426152318716 + 0.1 * 6.519773960113525
Epoch 740, val loss: 0.9736484289169312
Epoch 750, training loss: 0.6618581414222717 = 0.01007645670324564 + 0.1 * 6.517816543579102
Epoch 750, val loss: 0.9786561131477356
Epoch 760, training loss: 0.6600503325462341 = 0.009739628992974758 + 0.1 * 6.503107070922852
Epoch 760, val loss: 0.9834231734275818
Epoch 770, training loss: 0.6592583656311035 = 0.009420960210263729 + 0.1 * 6.498373985290527
Epoch 770, val loss: 0.9881041646003723
Epoch 780, training loss: 0.6595529913902283 = 0.009118812158703804 + 0.1 * 6.5043416023254395
Epoch 780, val loss: 0.9926705956459045
Epoch 790, training loss: 0.6582818031311035 = 0.00883310753852129 + 0.1 * 6.4944868087768555
Epoch 790, val loss: 0.9972366690635681
Epoch 800, training loss: 0.6574434638023376 = 0.008561993017792702 + 0.1 * 6.488814830780029
Epoch 800, val loss: 1.0017411708831787
Epoch 810, training loss: 0.6561237573623657 = 0.008305027149617672 + 0.1 * 6.478187561035156
Epoch 810, val loss: 1.0060545206069946
Epoch 820, training loss: 0.6568010449409485 = 0.008060154505074024 + 0.1 * 6.487408638000488
Epoch 820, val loss: 1.0103768110275269
Epoch 830, training loss: 0.656014621257782 = 0.007828145287930965 + 0.1 * 6.481864929199219
Epoch 830, val loss: 1.0145652294158936
Epoch 840, training loss: 0.6581769585609436 = 0.007607273291796446 + 0.1 * 6.505696773529053
Epoch 840, val loss: 1.018668293952942
Epoch 850, training loss: 0.654582142829895 = 0.007397141307592392 + 0.1 * 6.4718499183654785
Epoch 850, val loss: 1.0226995944976807
Epoch 860, training loss: 0.6561100482940674 = 0.007196241058409214 + 0.1 * 6.489138126373291
Epoch 860, val loss: 1.0265161991119385
Epoch 870, training loss: 0.6547762751579285 = 0.007004542741924524 + 0.1 * 6.47771692276001
Epoch 870, val loss: 1.0304900407791138
Epoch 880, training loss: 0.6543471813201904 = 0.006821972783654928 + 0.1 * 6.4752516746521
Epoch 880, val loss: 1.0342453718185425
Epoch 890, training loss: 0.6526312232017517 = 0.0066475155763328075 + 0.1 * 6.459836959838867
Epoch 890, val loss: 1.0379648208618164
Epoch 900, training loss: 0.6534410119056702 = 0.00648014759644866 + 0.1 * 6.469608306884766
Epoch 900, val loss: 1.0416096448898315
Epoch 910, training loss: 0.652484655380249 = 0.006320063956081867 + 0.1 * 6.461645603179932
Epoch 910, val loss: 1.0452734231948853
Epoch 920, training loss: 0.6523093581199646 = 0.0061667123809456825 + 0.1 * 6.461426258087158
Epoch 920, val loss: 1.0488126277923584
Epoch 930, training loss: 0.6531718969345093 = 0.00601952476426959 + 0.1 * 6.471523761749268
Epoch 930, val loss: 1.0523509979248047
Epoch 940, training loss: 0.6515509486198425 = 0.005878554657101631 + 0.1 * 6.456723690032959
Epoch 940, val loss: 1.0557656288146973
Epoch 950, training loss: 0.6513755917549133 = 0.005743132904171944 + 0.1 * 6.456324100494385
Epoch 950, val loss: 1.0591037273406982
Epoch 960, training loss: 0.649222731590271 = 0.005612976849079132 + 0.1 * 6.436097621917725
Epoch 960, val loss: 1.0625320672988892
Epoch 970, training loss: 0.6490098834037781 = 0.005488082300871611 + 0.1 * 6.43521785736084
Epoch 970, val loss: 1.065740704536438
Epoch 980, training loss: 0.6523970365524292 = 0.005367923993617296 + 0.1 * 6.470290660858154
Epoch 980, val loss: 1.0689420700073242
Epoch 990, training loss: 0.64883953332901 = 0.0052517796866595745 + 0.1 * 6.435877799987793
Epoch 990, val loss: 1.0721852779388428
Epoch 1000, training loss: 0.649939239025116 = 0.005140428896993399 + 0.1 * 6.447988033294678
Epoch 1000, val loss: 1.0752360820770264
Epoch 1010, training loss: 0.6486443877220154 = 0.005032768007367849 + 0.1 * 6.436115741729736
Epoch 1010, val loss: 1.0784096717834473
Epoch 1020, training loss: 0.6480122804641724 = 0.004929150454699993 + 0.1 * 6.430831432342529
Epoch 1020, val loss: 1.0814592838287354
Epoch 1030, training loss: 0.6500769853591919 = 0.004829117562621832 + 0.1 * 6.452478408813477
Epoch 1030, val loss: 1.0844428539276123
Epoch 1040, training loss: 0.6478322148323059 = 0.004732138942927122 + 0.1 * 6.431000709533691
Epoch 1040, val loss: 1.0874265432357788
Epoch 1050, training loss: 0.6481054425239563 = 0.0046389722265303135 + 0.1 * 6.434664726257324
Epoch 1050, val loss: 1.090369701385498
Epoch 1060, training loss: 0.6479653120040894 = 0.0045486860908567905 + 0.1 * 6.434166431427002
Epoch 1060, val loss: 1.0932878255844116
Epoch 1070, training loss: 0.6469312906265259 = 0.004461668431758881 + 0.1 * 6.424696445465088
Epoch 1070, val loss: 1.0961719751358032
Epoch 1080, training loss: 0.6466841697692871 = 0.004377636127173901 + 0.1 * 6.423064708709717
Epoch 1080, val loss: 1.098917007446289
Epoch 1090, training loss: 0.6478452086448669 = 0.00429608253762126 + 0.1 * 6.43549108505249
Epoch 1090, val loss: 1.1016925573349
Epoch 1100, training loss: 0.6453250050544739 = 0.004217186477035284 + 0.1 * 6.411077976226807
Epoch 1100, val loss: 1.104540228843689
Epoch 1110, training loss: 0.6461500525474548 = 0.004141098354011774 + 0.1 * 6.420089244842529
Epoch 1110, val loss: 1.107178807258606
Epoch 1120, training loss: 0.6462805867195129 = 0.004067097790539265 + 0.1 * 6.4221343994140625
Epoch 1120, val loss: 1.1098699569702148
Epoch 1130, training loss: 0.6456847190856934 = 0.003995693288743496 + 0.1 * 6.4168901443481445
Epoch 1130, val loss: 1.112481713294983
Epoch 1140, training loss: 0.6471207141876221 = 0.003926508128643036 + 0.1 * 6.431941986083984
Epoch 1140, val loss: 1.1150678396224976
Epoch 1150, training loss: 0.6439720392227173 = 0.0038592263590544462 + 0.1 * 6.401127815246582
Epoch 1150, val loss: 1.1176695823669434
Epoch 1160, training loss: 0.6455584764480591 = 0.0037941078189760447 + 0.1 * 6.4176435470581055
Epoch 1160, val loss: 1.1201618909835815
Epoch 1170, training loss: 0.6446911096572876 = 0.003730792086571455 + 0.1 * 6.409603118896484
Epoch 1170, val loss: 1.1226609945297241
Epoch 1180, training loss: 0.6443598866462708 = 0.0036692877765744925 + 0.1 * 6.4069061279296875
Epoch 1180, val loss: 1.1251246929168701
Epoch 1190, training loss: 0.6450440287590027 = 0.003609450999647379 + 0.1 * 6.4143452644348145
Epoch 1190, val loss: 1.1275979280471802
Epoch 1200, training loss: 0.6438647508621216 = 0.003551455680280924 + 0.1 * 6.403132438659668
Epoch 1200, val loss: 1.1300573348999023
Epoch 1210, training loss: 0.6444541215896606 = 0.0034950035624206066 + 0.1 * 6.409591197967529
Epoch 1210, val loss: 1.1324841976165771
Epoch 1220, training loss: 0.6436957120895386 = 0.0034404187463223934 + 0.1 * 6.402552604675293
Epoch 1220, val loss: 1.1348822116851807
Epoch 1230, training loss: 0.6454127430915833 = 0.003387352917343378 + 0.1 * 6.420254230499268
Epoch 1230, val loss: 1.137205958366394
Epoch 1240, training loss: 0.643021285533905 = 0.003335424233227968 + 0.1 * 6.3968586921691895
Epoch 1240, val loss: 1.139527678489685
Epoch 1250, training loss: 0.643767237663269 = 0.003284974256530404 + 0.1 * 6.40482234954834
Epoch 1250, val loss: 1.1418609619140625
Epoch 1260, training loss: 0.6428619623184204 = 0.0032359955366700888 + 0.1 * 6.396259307861328
Epoch 1260, val loss: 1.1442254781723022
Epoch 1270, training loss: 0.6430257558822632 = 0.0031881926115602255 + 0.1 * 6.398375511169434
Epoch 1270, val loss: 1.1465115547180176
Epoch 1280, training loss: 0.6422829031944275 = 0.003141906810924411 + 0.1 * 6.3914103507995605
Epoch 1280, val loss: 1.1487648487091064
Epoch 1290, training loss: 0.6427393555641174 = 0.0030967046041041613 + 0.1 * 6.396426677703857
Epoch 1290, val loss: 1.1509507894515991
Epoch 1300, training loss: 0.6416999697685242 = 0.0030524954199790955 + 0.1 * 6.386474609375
Epoch 1300, val loss: 1.153132677078247
Epoch 1310, training loss: 0.6435239911079407 = 0.0030094205867499113 + 0.1 * 6.405145645141602
Epoch 1310, val loss: 1.155321478843689
Epoch 1320, training loss: 0.6422647833824158 = 0.0029674461111426353 + 0.1 * 6.39297342300415
Epoch 1320, val loss: 1.1575183868408203
Epoch 1330, training loss: 0.6423777341842651 = 0.002926404355093837 + 0.1 * 6.394513130187988
Epoch 1330, val loss: 1.159703016281128
Epoch 1340, training loss: 0.6419363617897034 = 0.0028866883367300034 + 0.1 * 6.390496253967285
Epoch 1340, val loss: 1.1617965698242188
Epoch 1350, training loss: 0.641188383102417 = 0.0028479727916419506 + 0.1 * 6.383403778076172
Epoch 1350, val loss: 1.1638106107711792
Epoch 1360, training loss: 0.6425524950027466 = 0.002809955505654216 + 0.1 * 6.397425651550293
Epoch 1360, val loss: 1.1658803224563599
Epoch 1370, training loss: 0.6406998634338379 = 0.0027726416010409594 + 0.1 * 6.3792724609375
Epoch 1370, val loss: 1.167999267578125
Epoch 1380, training loss: 0.6430534720420837 = 0.0027365442365407944 + 0.1 * 6.403168678283691
Epoch 1380, val loss: 1.170026421546936
Epoch 1390, training loss: 0.6408334970474243 = 0.002701196586713195 + 0.1 * 6.381322383880615
Epoch 1390, val loss: 1.1720706224441528
Epoch 1400, training loss: 0.6415632963180542 = 0.0026668256614357233 + 0.1 * 6.388964653015137
Epoch 1400, val loss: 1.1740084886550903
Epoch 1410, training loss: 0.6425238847732544 = 0.002633033785969019 + 0.1 * 6.3989081382751465
Epoch 1410, val loss: 1.1760002374649048
Epoch 1420, training loss: 0.6407372951507568 = 0.0026000766083598137 + 0.1 * 6.381371974945068
Epoch 1420, val loss: 1.177922248840332
Epoch 1430, training loss: 0.640162467956543 = 0.002567829331383109 + 0.1 * 6.375946044921875
Epoch 1430, val loss: 1.1798627376556396
Epoch 1440, training loss: 0.6422098875045776 = 0.0025364502798765898 + 0.1 * 6.39673376083374
Epoch 1440, val loss: 1.1817570924758911
Epoch 1450, training loss: 0.6405986547470093 = 0.002505467040464282 + 0.1 * 6.380931854248047
Epoch 1450, val loss: 1.1837002038955688
Epoch 1460, training loss: 0.6401485204696655 = 0.00247556297108531 + 0.1 * 6.376729488372803
Epoch 1460, val loss: 1.1855806112289429
Epoch 1470, training loss: 0.6396886110305786 = 0.0024461287539452314 + 0.1 * 6.372425079345703
Epoch 1470, val loss: 1.1873723268508911
Epoch 1480, training loss: 0.641066312789917 = 0.0024172067642211914 + 0.1 * 6.386490821838379
Epoch 1480, val loss: 1.1892274618148804
Epoch 1490, training loss: 0.6402696371078491 = 0.002388980472460389 + 0.1 * 6.3788065910339355
Epoch 1490, val loss: 1.191077470779419
Epoch 1500, training loss: 0.6402789950370789 = 0.0023614338133484125 + 0.1 * 6.379175662994385
Epoch 1500, val loss: 1.1928973197937012
Epoch 1510, training loss: 0.6401374340057373 = 0.0023344364017248154 + 0.1 * 6.378030300140381
Epoch 1510, val loss: 1.1946698427200317
Epoch 1520, training loss: 0.6391914486885071 = 0.0023080145474523306 + 0.1 * 6.368834495544434
Epoch 1520, val loss: 1.1964404582977295
Epoch 1530, training loss: 0.6400452852249146 = 0.002282138215377927 + 0.1 * 6.377631187438965
Epoch 1530, val loss: 1.198180079460144
Epoch 1540, training loss: 0.639748215675354 = 0.002256732899695635 + 0.1 * 6.374914646148682
Epoch 1540, val loss: 1.1999640464782715
Epoch 1550, training loss: 0.6399399638175964 = 0.0022318533156067133 + 0.1 * 6.377081394195557
Epoch 1550, val loss: 1.2016648054122925
Epoch 1560, training loss: 0.6401615738868713 = 0.002207606565207243 + 0.1 * 6.379539966583252
Epoch 1560, val loss: 1.2034164667129517
Epoch 1570, training loss: 0.6387785077095032 = 0.002183692529797554 + 0.1 * 6.36594820022583
Epoch 1570, val loss: 1.205037236213684
Epoch 1580, training loss: 0.639418363571167 = 0.002160265576094389 + 0.1 * 6.3725810050964355
Epoch 1580, val loss: 1.2067211866378784
Epoch 1590, training loss: 0.6400818824768066 = 0.002137245610356331 + 0.1 * 6.379446506500244
Epoch 1590, val loss: 1.2083905935287476
Epoch 1600, training loss: 0.6380853652954102 = 0.0021146624349057674 + 0.1 * 6.359706878662109
Epoch 1600, val loss: 1.2100862264633179
Epoch 1610, training loss: 0.6392517685890198 = 0.0020926466677337885 + 0.1 * 6.371591091156006
Epoch 1610, val loss: 1.2116541862487793
Epoch 1620, training loss: 0.6395694017410278 = 0.0020710458047688007 + 0.1 * 6.374983787536621
Epoch 1620, val loss: 1.213314175605774
Epoch 1630, training loss: 0.6383680701255798 = 0.002049725968390703 + 0.1 * 6.36318302154541
Epoch 1630, val loss: 1.214903473854065
Epoch 1640, training loss: 0.6386025547981262 = 0.002028935356065631 + 0.1 * 6.36573600769043
Epoch 1640, val loss: 1.2164899110794067
Epoch 1650, training loss: 0.6397314667701721 = 0.002008444629609585 + 0.1 * 6.377230167388916
Epoch 1650, val loss: 1.2180523872375488
Epoch 1660, training loss: 0.6414873003959656 = 0.001988387433812022 + 0.1 * 6.394988536834717
Epoch 1660, val loss: 1.219684362411499
Epoch 1670, training loss: 0.6383311748504639 = 0.001968664349988103 + 0.1 * 6.363624572753906
Epoch 1670, val loss: 1.2212659120559692
Epoch 1680, training loss: 0.6392130851745605 = 0.0019494337029755116 + 0.1 * 6.372636318206787
Epoch 1680, val loss: 1.2227425575256348
Epoch 1690, training loss: 0.6379206776618958 = 0.001930520636960864 + 0.1 * 6.359901428222656
Epoch 1690, val loss: 1.2242679595947266
Epoch 1700, training loss: 0.6381826400756836 = 0.001911962521262467 + 0.1 * 6.362707138061523
Epoch 1700, val loss: 1.2257378101348877
Epoch 1710, training loss: 0.6391090154647827 = 0.0018936180276796222 + 0.1 * 6.3721537590026855
Epoch 1710, val loss: 1.2272292375564575
Epoch 1720, training loss: 0.6377250552177429 = 0.0018755957717075944 + 0.1 * 6.358494758605957
Epoch 1720, val loss: 1.2287315130233765
Epoch 1730, training loss: 0.6372509002685547 = 0.0018580814357846975 + 0.1 * 6.353928089141846
Epoch 1730, val loss: 1.2301840782165527
Epoch 1740, training loss: 0.6393240690231323 = 0.001840777462348342 + 0.1 * 6.374832630157471
Epoch 1740, val loss: 1.2316327095031738
Epoch 1750, training loss: 0.6377040147781372 = 0.0018237103940919042 + 0.1 * 6.358802795410156
Epoch 1750, val loss: 1.2331042289733887
Epoch 1760, training loss: 0.6382781863212585 = 0.001807006890885532 + 0.1 * 6.364711284637451
Epoch 1760, val loss: 1.2345070838928223
Epoch 1770, training loss: 0.6374958157539368 = 0.0017906043212860823 + 0.1 * 6.357052326202393
Epoch 1770, val loss: 1.235955834388733
Epoch 1780, training loss: 0.6375455260276794 = 0.00177448068279773 + 0.1 * 6.357710361480713
Epoch 1780, val loss: 1.2373688220977783
Epoch 1790, training loss: 0.6376806497573853 = 0.0017586154863238335 + 0.1 * 6.359220027923584
Epoch 1790, val loss: 1.2387409210205078
Epoch 1800, training loss: 0.6378886699676514 = 0.001743014552630484 + 0.1 * 6.361456394195557
Epoch 1800, val loss: 1.2401453256607056
Epoch 1810, training loss: 0.6374000310897827 = 0.0017276582075282931 + 0.1 * 6.356723785400391
Epoch 1810, val loss: 1.2415528297424316
Epoch 1820, training loss: 0.6379052996635437 = 0.0017124813748523593 + 0.1 * 6.3619279861450195
Epoch 1820, val loss: 1.2429436445236206
Epoch 1830, training loss: 0.6378965377807617 = 0.0016976926708593965 + 0.1 * 6.361988067626953
Epoch 1830, val loss: 1.2443162202835083
Epoch 1840, training loss: 0.6371069550514221 = 0.0016830904642120004 + 0.1 * 6.354238033294678
Epoch 1840, val loss: 1.24569571018219
Epoch 1850, training loss: 0.6373943090438843 = 0.001668846933171153 + 0.1 * 6.357254505157471
Epoch 1850, val loss: 1.2470518350601196
Epoch 1860, training loss: 0.6379188895225525 = 0.001654642983339727 + 0.1 * 6.362642765045166
Epoch 1860, val loss: 1.2483820915222168
Epoch 1870, training loss: 0.6372484564781189 = 0.0016408442752435803 + 0.1 * 6.356075763702393
Epoch 1870, val loss: 1.2497484683990479
Epoch 1880, training loss: 0.637039065361023 = 0.001627222984097898 + 0.1 * 6.354118347167969
Epoch 1880, val loss: 1.2510452270507812
Epoch 1890, training loss: 0.6361113786697388 = 0.0016138249775394797 + 0.1 * 6.34497594833374
Epoch 1890, val loss: 1.252279281616211
Epoch 1900, training loss: 0.6367759704589844 = 0.001600575982593 + 0.1 * 6.3517537117004395
Epoch 1900, val loss: 1.2536219358444214
Epoch 1910, training loss: 0.6359168291091919 = 0.0015875675017014146 + 0.1 * 6.343292713165283
Epoch 1910, val loss: 1.254887342453003
Epoch 1920, training loss: 0.637514591217041 = 0.0015748017467558384 + 0.1 * 6.359397888183594
Epoch 1920, val loss: 1.2561755180358887
Epoch 1930, training loss: 0.6380611658096313 = 0.0015622032806277275 + 0.1 * 6.364989757537842
Epoch 1930, val loss: 1.257442831993103
Epoch 1940, training loss: 0.6362720727920532 = 0.001549780834466219 + 0.1 * 6.347222805023193
Epoch 1940, val loss: 1.2587225437164307
Epoch 1950, training loss: 0.6373646259307861 = 0.0015376594383269548 + 0.1 * 6.358269214630127
Epoch 1950, val loss: 1.259950041770935
Epoch 1960, training loss: 0.6361437439918518 = 0.0015255928738042712 + 0.1 * 6.346181392669678
Epoch 1960, val loss: 1.261196255683899
Epoch 1970, training loss: 0.6359073519706726 = 0.0015138124581426382 + 0.1 * 6.343935012817383
Epoch 1970, val loss: 1.2623955011367798
Epoch 1980, training loss: 0.6365492939949036 = 0.0015021060826256871 + 0.1 * 6.3504719734191895
Epoch 1980, val loss: 1.2636350393295288
Epoch 1990, training loss: 0.6360064148902893 = 0.0014906511642038822 + 0.1 * 6.345157623291016
Epoch 1990, val loss: 1.2649047374725342
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 2.8178369998931885 = 1.9581546783447266 + 0.1 * 8.596822738647461
Epoch 0, val loss: 1.965357780456543
Epoch 10, training loss: 2.807018518447876 = 1.9473484754562378 + 0.1 * 8.596699714660645
Epoch 10, val loss: 1.9540965557098389
Epoch 20, training loss: 2.793729066848755 = 1.9341492652893066 + 0.1 * 8.59579849243164
Epoch 20, val loss: 1.9400804042816162
Epoch 30, training loss: 2.7744789123535156 = 1.9156625270843506 + 0.1 * 8.588163375854492
Epoch 30, val loss: 1.9203766584396362
Epoch 40, training loss: 2.74261212348938 = 1.8882719278335571 + 0.1 * 8.543401718139648
Epoch 40, val loss: 1.8915852308273315
Epoch 50, training loss: 2.6845340728759766 = 1.8506799936294556 + 0.1 * 8.338541984558105
Epoch 50, val loss: 1.8540743589401245
Epoch 60, training loss: 2.6102232933044434 = 1.8087644577026367 + 0.1 * 8.014589309692383
Epoch 60, val loss: 1.8153105974197388
Epoch 70, training loss: 2.551891326904297 = 1.772154450416565 + 0.1 * 7.797369480133057
Epoch 70, val loss: 1.783219337463379
Epoch 80, training loss: 2.486348867416382 = 1.7340067625045776 + 0.1 * 7.523421287536621
Epoch 80, val loss: 1.7484256029129028
Epoch 90, training loss: 2.417109489440918 = 1.6836243867874146 + 0.1 * 7.334849834442139
Epoch 90, val loss: 1.7029953002929688
Epoch 100, training loss: 2.336719036102295 = 1.6186245679855347 + 0.1 * 7.180944442749023
Epoch 100, val loss: 1.6465675830841064
Epoch 110, training loss: 2.245299816131592 = 1.5390033721923828 + 0.1 * 7.06296443939209
Epoch 110, val loss: 1.5783530473709106
Epoch 120, training loss: 2.1487982273101807 = 1.4496512413024902 + 0.1 * 6.991468906402588
Epoch 120, val loss: 1.5014137029647827
Epoch 130, training loss: 2.0510101318359375 = 1.3559824228286743 + 0.1 * 6.950276851654053
Epoch 130, val loss: 1.4241453409194946
Epoch 140, training loss: 1.9524776935577393 = 1.2609130144119263 + 0.1 * 6.915646076202393
Epoch 140, val loss: 1.346983551979065
Epoch 150, training loss: 1.8533138036727905 = 1.1646307706832886 + 0.1 * 6.8868303298950195
Epoch 150, val loss: 1.2695897817611694
Epoch 160, training loss: 1.7556450366973877 = 1.0691094398498535 + 0.1 * 6.865355491638184
Epoch 160, val loss: 1.194258451461792
Epoch 170, training loss: 1.6637158393859863 = 0.978720486164093 + 0.1 * 6.849954128265381
Epoch 170, val loss: 1.1234617233276367
Epoch 180, training loss: 1.578053593635559 = 0.8945900797843933 + 0.1 * 6.834634780883789
Epoch 180, val loss: 1.0578309297561646
Epoch 190, training loss: 1.4999563694000244 = 0.8181994557380676 + 0.1 * 6.817568778991699
Epoch 190, val loss: 0.9985814690589905
Epoch 200, training loss: 1.4298509359359741 = 0.7497497200965881 + 0.1 * 6.80101203918457
Epoch 200, val loss: 0.9458221793174744
Epoch 210, training loss: 1.36799955368042 = 0.6895602345466614 + 0.1 * 6.784392833709717
Epoch 210, val loss: 0.9011906981468201
Epoch 220, training loss: 1.3133671283721924 = 0.6363014578819275 + 0.1 * 6.770656108856201
Epoch 220, val loss: 0.8641900420188904
Epoch 230, training loss: 1.2632242441177368 = 0.5872963666915894 + 0.1 * 6.759278774261475
Epoch 230, val loss: 0.8329116702079773
Epoch 240, training loss: 1.2164815664291382 = 0.5412470698356628 + 0.1 * 6.752344608306885
Epoch 240, val loss: 0.8062028288841248
Epoch 250, training loss: 1.1703383922576904 = 0.49727705121040344 + 0.1 * 6.730613708496094
Epoch 250, val loss: 0.7829240560531616
Epoch 260, training loss: 1.127274990081787 = 0.4548557996749878 + 0.1 * 6.724192142486572
Epoch 260, val loss: 0.7627187371253967
Epoch 270, training loss: 1.0860586166381836 = 0.41435009241104126 + 0.1 * 6.717085838317871
Epoch 270, val loss: 0.745702862739563
Epoch 280, training loss: 1.0479159355163574 = 0.3763905167579651 + 0.1 * 6.715253829956055
Epoch 280, val loss: 0.7324851751327515
Epoch 290, training loss: 1.0121278762817383 = 0.34138211607933044 + 0.1 * 6.707457542419434
Epoch 290, val loss: 0.7229598164558411
Epoch 300, training loss: 0.979540228843689 = 0.30921629071235657 + 0.1 * 6.7032389640808105
Epoch 300, val loss: 0.7167309522628784
Epoch 310, training loss: 0.9498657584190369 = 0.27984941005706787 + 0.1 * 6.7001633644104
Epoch 310, val loss: 0.7132261395454407
Epoch 320, training loss: 0.922730565071106 = 0.253078430891037 + 0.1 * 6.696521759033203
Epoch 320, val loss: 0.7121219038963318
Epoch 330, training loss: 0.8986957669258118 = 0.22862927615642548 + 0.1 * 6.700664520263672
Epoch 330, val loss: 0.7131173610687256
Epoch 340, training loss: 0.8756074905395508 = 0.20639446377754211 + 0.1 * 6.692129611968994
Epoch 340, val loss: 0.7155970335006714
Epoch 350, training loss: 0.8553778529167175 = 0.1861707717180252 + 0.1 * 6.692070960998535
Epoch 350, val loss: 0.7194728255271912
Epoch 360, training loss: 0.8365973830223083 = 0.16786833107471466 + 0.1 * 6.687290668487549
Epoch 360, val loss: 0.7245535254478455
Epoch 370, training loss: 0.8194153904914856 = 0.15134017169475555 + 0.1 * 6.680751800537109
Epoch 370, val loss: 0.7306151390075684
Epoch 380, training loss: 0.8050417900085449 = 0.1364445686340332 + 0.1 * 6.685972213745117
Epoch 380, val loss: 0.7375170588493347
Epoch 390, training loss: 0.7914520502090454 = 0.12313812971115112 + 0.1 * 6.683139324188232
Epoch 390, val loss: 0.7452082633972168
Epoch 400, training loss: 0.7785329222679138 = 0.11124611645936966 + 0.1 * 6.672868251800537
Epoch 400, val loss: 0.7534400820732117
Epoch 410, training loss: 0.7692618370056152 = 0.10062365233898163 + 0.1 * 6.686381816864014
Epoch 410, val loss: 0.7621422410011292
Epoch 420, training loss: 0.758319616317749 = 0.09119042754173279 + 0.1 * 6.671292304992676
Epoch 420, val loss: 0.7710946798324585
Epoch 430, training loss: 0.7493171095848083 = 0.08277084678411484 + 0.1 * 6.665462493896484
Epoch 430, val loss: 0.7802744507789612
Epoch 440, training loss: 0.742885410785675 = 0.0752483382821083 + 0.1 * 6.676370143890381
Epoch 440, val loss: 0.7896971106529236
Epoch 450, training loss: 0.7344527244567871 = 0.06855791062116623 + 0.1 * 6.658947944641113
Epoch 450, val loss: 0.7992201447486877
Epoch 460, training loss: 0.7278712391853333 = 0.06257658451795578 + 0.1 * 6.652946472167969
Epoch 460, val loss: 0.8087078332901001
Epoch 470, training loss: 0.7239539623260498 = 0.057220280170440674 + 0.1 * 6.667336940765381
Epoch 470, val loss: 0.8181896209716797
Epoch 480, training loss: 0.7181263566017151 = 0.05246198922395706 + 0.1 * 6.656643867492676
Epoch 480, val loss: 0.8276104927062988
Epoch 490, training loss: 0.7126696705818176 = 0.04820891469717026 + 0.1 * 6.6446075439453125
Epoch 490, val loss: 0.8368088006973267
Epoch 500, training loss: 0.7083261609077454 = 0.04438817501068115 + 0.1 * 6.639379978179932
Epoch 500, val loss: 0.8458748459815979
Epoch 510, training loss: 0.7053111791610718 = 0.04095092788338661 + 0.1 * 6.64360237121582
Epoch 510, val loss: 0.8548359274864197
Epoch 520, training loss: 0.7041221857070923 = 0.037866175174713135 + 0.1 * 6.662559986114502
Epoch 520, val loss: 0.863728404045105
Epoch 530, training loss: 0.6979169249534607 = 0.03510409593582153 + 0.1 * 6.6281280517578125
Epoch 530, val loss: 0.8722760081291199
Epoch 540, training loss: 0.695548415184021 = 0.03261668235063553 + 0.1 * 6.629317283630371
Epoch 540, val loss: 0.8805557489395142
Epoch 550, training loss: 0.6922752261161804 = 0.03037215769290924 + 0.1 * 6.619030952453613
Epoch 550, val loss: 0.8887529373168945
Epoch 560, training loss: 0.6896493434906006 = 0.028342673555016518 + 0.1 * 6.613066673278809
Epoch 560, val loss: 0.8965904116630554
Epoch 570, training loss: 0.6877450346946716 = 0.026501428335905075 + 0.1 * 6.612435817718506
Epoch 570, val loss: 0.9043495655059814
Epoch 580, training loss: 0.6865193247795105 = 0.02483220398426056 + 0.1 * 6.616871356964111
Epoch 580, val loss: 0.9119053483009338
Epoch 590, training loss: 0.6835399866104126 = 0.02332274802029133 + 0.1 * 6.602172374725342
Epoch 590, val loss: 0.9192389249801636
Epoch 600, training loss: 0.6832767128944397 = 0.02194826304912567 + 0.1 * 6.613284111022949
Epoch 600, val loss: 0.9263060688972473
Epoch 610, training loss: 0.6801108717918396 = 0.020695246756076813 + 0.1 * 6.594155788421631
Epoch 610, val loss: 0.9331580996513367
Epoch 620, training loss: 0.67942214012146 = 0.019549541175365448 + 0.1 * 6.59872579574585
Epoch 620, val loss: 0.9398371577262878
Epoch 630, training loss: 0.6773170232772827 = 0.018503423780202866 + 0.1 * 6.588135719299316
Epoch 630, val loss: 0.9464392066001892
Epoch 640, training loss: 0.6754343509674072 = 0.017542732879519463 + 0.1 * 6.578916072845459
Epoch 640, val loss: 0.952692985534668
Epoch 650, training loss: 0.6734690070152283 = 0.01665879413485527 + 0.1 * 6.5681023597717285
Epoch 650, val loss: 0.9588410258293152
Epoch 660, training loss: 0.6725452542304993 = 0.015845881775021553 + 0.1 * 6.566993236541748
Epoch 660, val loss: 0.9647636413574219
Epoch 670, training loss: 0.6711003184318542 = 0.015092052519321442 + 0.1 * 6.56008243560791
Epoch 670, val loss: 0.9705567359924316
Epoch 680, training loss: 0.6701452732086182 = 0.014393944293260574 + 0.1 * 6.557513236999512
Epoch 680, val loss: 0.9762822985649109
Epoch 690, training loss: 0.6694098711013794 = 0.013749832287430763 + 0.1 * 6.556600093841553
Epoch 690, val loss: 0.9817237854003906
Epoch 700, training loss: 0.6692599058151245 = 0.013150990940630436 + 0.1 * 6.561089038848877
Epoch 700, val loss: 0.9870262742042542
Epoch 710, training loss: 0.6664997339248657 = 0.012593978084623814 + 0.1 * 6.53905725479126
Epoch 710, val loss: 0.9923573732376099
Epoch 720, training loss: 0.6659721732139587 = 0.01207392755895853 + 0.1 * 6.538982391357422
Epoch 720, val loss: 0.9974705576896667
Epoch 730, training loss: 0.6654659509658813 = 0.011587223969399929 + 0.1 * 6.538787364959717
Epoch 730, val loss: 1.0024484395980835
Epoch 740, training loss: 0.6645165085792542 = 0.0111311636865139 + 0.1 * 6.533853530883789
Epoch 740, val loss: 1.0073615312576294
Epoch 750, training loss: 0.6662238240242004 = 0.010703248903155327 + 0.1 * 6.555205345153809
Epoch 750, val loss: 1.0122578144073486
Epoch 760, training loss: 0.6634867191314697 = 0.010304564610123634 + 0.1 * 6.531821250915527
Epoch 760, val loss: 1.0169678926467896
Epoch 770, training loss: 0.6622574925422668 = 0.00993020087480545 + 0.1 * 6.52327299118042
Epoch 770, val loss: 1.0214191675186157
Epoch 780, training loss: 0.661742091178894 = 0.009577447548508644 + 0.1 * 6.521646499633789
Epoch 780, val loss: 1.0258300304412842
Epoch 790, training loss: 0.6598256230354309 = 0.009243860840797424 + 0.1 * 6.505817413330078
Epoch 790, val loss: 1.0302523374557495
Epoch 800, training loss: 0.6603715419769287 = 0.008929924108088017 + 0.1 * 6.514416217803955
Epoch 800, val loss: 1.0344488620758057
Epoch 810, training loss: 0.6591660380363464 = 0.008632546290755272 + 0.1 * 6.505334854125977
Epoch 810, val loss: 1.038712739944458
Epoch 820, training loss: 0.659241259098053 = 0.008352686651051044 + 0.1 * 6.508885383605957
Epoch 820, val loss: 1.042777180671692
Epoch 830, training loss: 0.6586081981658936 = 0.008086745627224445 + 0.1 * 6.505214214324951
Epoch 830, val loss: 1.0467337369918823
Epoch 840, training loss: 0.6579522490501404 = 0.00783455464988947 + 0.1 * 6.501176834106445
Epoch 840, val loss: 1.0506998300552368
Epoch 850, training loss: 0.6569485068321228 = 0.007595287170261145 + 0.1 * 6.493531703948975
Epoch 850, val loss: 1.05451500415802
Epoch 860, training loss: 0.6570062041282654 = 0.007368277292698622 + 0.1 * 6.496379375457764
Epoch 860, val loss: 1.0582283735275269
Epoch 870, training loss: 0.6573255658149719 = 0.007152045611292124 + 0.1 * 6.501735210418701
Epoch 870, val loss: 1.0619728565216064
Epoch 880, training loss: 0.6550066471099854 = 0.006946875713765621 + 0.1 * 6.480597496032715
Epoch 880, val loss: 1.0655441284179688
Epoch 890, training loss: 0.6574768424034119 = 0.00675152987241745 + 0.1 * 6.5072526931762695
Epoch 890, val loss: 1.0689647197723389
Epoch 900, training loss: 0.6554769277572632 = 0.006565173156559467 + 0.1 * 6.489117622375488
Epoch 900, val loss: 1.0724862813949585
Epoch 910, training loss: 0.6551868915557861 = 0.006387482862919569 + 0.1 * 6.487994194030762
Epoch 910, val loss: 1.0758652687072754
Epoch 920, training loss: 0.6532132029533386 = 0.006218257360160351 + 0.1 * 6.469949245452881
Epoch 920, val loss: 1.079136610031128
Epoch 930, training loss: 0.6539690494537354 = 0.0060560572892427444 + 0.1 * 6.479129791259766
Epoch 930, val loss: 1.0823851823806763
Epoch 940, training loss: 0.653269350528717 = 0.005900744814425707 + 0.1 * 6.473686218261719
Epoch 940, val loss: 1.08561110496521
Epoch 950, training loss: 0.6536300778388977 = 0.005752552766352892 + 0.1 * 6.4787750244140625
Epoch 950, val loss: 1.0887137651443481
Epoch 960, training loss: 0.6517296433448792 = 0.005610242486000061 + 0.1 * 6.461194038391113
Epoch 960, val loss: 1.091798186302185
Epoch 970, training loss: 0.6528982520103455 = 0.005474111530929804 + 0.1 * 6.474241256713867
Epoch 970, val loss: 1.094770073890686
Epoch 980, training loss: 0.6521425843238831 = 0.005343461409211159 + 0.1 * 6.467991352081299
Epoch 980, val loss: 1.0978195667266846
Epoch 990, training loss: 0.6504865884780884 = 0.005218613427132368 + 0.1 * 6.452679634094238
Epoch 990, val loss: 1.100661277770996
Epoch 1000, training loss: 0.6527043581008911 = 0.005098262336105108 + 0.1 * 6.476060390472412
Epoch 1000, val loss: 1.1035029888153076
Epoch 1010, training loss: 0.6515279412269592 = 0.004982568323612213 + 0.1 * 6.465453624725342
Epoch 1010, val loss: 1.1064289808273315
Epoch 1020, training loss: 0.6507357954978943 = 0.004871939774602652 + 0.1 * 6.4586381912231445
Epoch 1020, val loss: 1.1091009378433228
Epoch 1030, training loss: 0.6491106152534485 = 0.004765339661389589 + 0.1 * 6.443452835083008
Epoch 1030, val loss: 1.111822485923767
Epoch 1040, training loss: 0.6503674387931824 = 0.004662860184907913 + 0.1 * 6.457045555114746
Epoch 1040, val loss: 1.1144388914108276
Epoch 1050, training loss: 0.6493003964424133 = 0.004563924390822649 + 0.1 * 6.447364807128906
Epoch 1050, val loss: 1.117039680480957
Epoch 1060, training loss: 0.6501533389091492 = 0.0044684200547635555 + 0.1 * 6.456848621368408
Epoch 1060, val loss: 1.1197232007980347
Epoch 1070, training loss: 0.6486015915870667 = 0.004376945085823536 + 0.1 * 6.442246437072754
Epoch 1070, val loss: 1.122246503829956
Epoch 1080, training loss: 0.6500549912452698 = 0.004288795404136181 + 0.1 * 6.4576616287231445
Epoch 1080, val loss: 1.1246546506881714
Epoch 1090, training loss: 0.6475691199302673 = 0.004203390795737505 + 0.1 * 6.433657169342041
Epoch 1090, val loss: 1.1270865201950073
Epoch 1100, training loss: 0.6483863592147827 = 0.004121096339076757 + 0.1 * 6.442652225494385
Epoch 1100, val loss: 1.1294922828674316
Epoch 1110, training loss: 0.6480437517166138 = 0.004041624721139669 + 0.1 * 6.440021514892578
Epoch 1110, val loss: 1.1318771839141846
Epoch 1120, training loss: 0.6483852863311768 = 0.003964529372751713 + 0.1 * 6.444207191467285
Epoch 1120, val loss: 1.1342028379440308
Epoch 1130, training loss: 0.6472037434577942 = 0.003890323918312788 + 0.1 * 6.433134078979492
Epoch 1130, val loss: 1.136523962020874
Epoch 1140, training loss: 0.6464872360229492 = 0.0038183568976819515 + 0.1 * 6.4266886711120605
Epoch 1140, val loss: 1.138814926147461
Epoch 1150, training loss: 0.6474175453186035 = 0.0037490276154130697 + 0.1 * 6.436685085296631
Epoch 1150, val loss: 1.1409568786621094
Epoch 1160, training loss: 0.6462374925613403 = 0.0036817488726228476 + 0.1 * 6.425557613372803
Epoch 1160, val loss: 1.1431231498718262
Epoch 1170, training loss: 0.6456218957901001 = 0.0036165385972708464 + 0.1 * 6.420053958892822
Epoch 1170, val loss: 1.1453783512115479
Epoch 1180, training loss: 0.6456336975097656 = 0.003553664544597268 + 0.1 * 6.42080020904541
Epoch 1180, val loss: 1.147447943687439
Epoch 1190, training loss: 0.645578920841217 = 0.0034924419596791267 + 0.1 * 6.420865058898926
Epoch 1190, val loss: 1.149549126625061
Epoch 1200, training loss: 0.647081196308136 = 0.0034331372007727623 + 0.1 * 6.436480522155762
Epoch 1200, val loss: 1.1516730785369873
Epoch 1210, training loss: 0.6462637186050415 = 0.0033755635377019644 + 0.1 * 6.4288811683654785
Epoch 1210, val loss: 1.1537325382232666
Epoch 1220, training loss: 0.6453892588615417 = 0.0033202068880200386 + 0.1 * 6.420690059661865
Epoch 1220, val loss: 1.1557438373565674
Epoch 1230, training loss: 0.6449361443519592 = 0.003266063518822193 + 0.1 * 6.416700839996338
Epoch 1230, val loss: 1.1576460599899292
Epoch 1240, training loss: 0.6445668935775757 = 0.0032137513626366854 + 0.1 * 6.413531303405762
Epoch 1240, val loss: 1.1595674753189087
Epoch 1250, training loss: 0.6438310146331787 = 0.003162506502121687 + 0.1 * 6.406684875488281
Epoch 1250, val loss: 1.1615371704101562
Epoch 1260, training loss: 0.646183967590332 = 0.0031130402348935604 + 0.1 * 6.430709362030029
Epoch 1260, val loss: 1.1634316444396973
Epoch 1270, training loss: 0.6436533331871033 = 0.0030648810788989067 + 0.1 * 6.405884742736816
Epoch 1270, val loss: 1.1653809547424316
Epoch 1280, training loss: 0.643798291683197 = 0.00301817967556417 + 0.1 * 6.407800674438477
Epoch 1280, val loss: 1.1671617031097412
Epoch 1290, training loss: 0.6450982093811035 = 0.0029727276414632797 + 0.1 * 6.421254634857178
Epoch 1290, val loss: 1.16900634765625
Epoch 1300, training loss: 0.643548846244812 = 0.0029284153133630753 + 0.1 * 6.406203746795654
Epoch 1300, val loss: 1.1708729267120361
Epoch 1310, training loss: 0.6444764137268066 = 0.0028853926341980696 + 0.1 * 6.415910243988037
Epoch 1310, val loss: 1.172670602798462
Epoch 1320, training loss: 0.6423816084861755 = 0.002843475202098489 + 0.1 * 6.395380973815918
Epoch 1320, val loss: 1.1744681596755981
Epoch 1330, training loss: 0.6440475583076477 = 0.0028027447406202555 + 0.1 * 6.412448406219482
Epoch 1330, val loss: 1.1761846542358398
Epoch 1340, training loss: 0.6425895094871521 = 0.002762862015515566 + 0.1 * 6.398266315460205
Epoch 1340, val loss: 1.1779296398162842
Epoch 1350, training loss: 0.6435228586196899 = 0.002724276389926672 + 0.1 * 6.407985687255859
Epoch 1350, val loss: 1.1796842813491821
Epoch 1360, training loss: 0.6419790387153625 = 0.002686447696760297 + 0.1 * 6.39292573928833
Epoch 1360, val loss: 1.1813329458236694
Epoch 1370, training loss: 0.6419084668159485 = 0.002649670699611306 + 0.1 * 6.392588138580322
Epoch 1370, val loss: 1.1829919815063477
Epoch 1380, training loss: 0.6444960832595825 = 0.0026137768290936947 + 0.1 * 6.418822765350342
Epoch 1380, val loss: 1.1846786737442017
Epoch 1390, training loss: 0.6414647102355957 = 0.002578617073595524 + 0.1 * 6.388860702514648
Epoch 1390, val loss: 1.1863934993743896
Epoch 1400, training loss: 0.6419962644577026 = 0.002544632414355874 + 0.1 * 6.3945159912109375
Epoch 1400, val loss: 1.1879913806915283
Epoch 1410, training loss: 0.6414328217506409 = 0.002511397935450077 + 0.1 * 6.389214038848877
Epoch 1410, val loss: 1.1895920038223267
Epoch 1420, training loss: 0.6421850323677063 = 0.002478834241628647 + 0.1 * 6.397061824798584
Epoch 1420, val loss: 1.1912118196487427
Epoch 1430, training loss: 0.6409314870834351 = 0.002446995349600911 + 0.1 * 6.384844779968262
Epoch 1430, val loss: 1.1928691864013672
Epoch 1440, training loss: 0.6417350769042969 = 0.0024161154869943857 + 0.1 * 6.393189430236816
Epoch 1440, val loss: 1.1943265199661255
Epoch 1450, training loss: 0.6403223276138306 = 0.0023858200293034315 + 0.1 * 6.37936544418335
Epoch 1450, val loss: 1.195958137512207
Epoch 1460, training loss: 0.6413230895996094 = 0.0023563390132039785 + 0.1 * 6.38966703414917
Epoch 1460, val loss: 1.1974536180496216
Epoch 1470, training loss: 0.641072154045105 = 0.002327464986592531 + 0.1 * 6.387446880340576
Epoch 1470, val loss: 1.1989694833755493
Epoch 1480, training loss: 0.6408762335777283 = 0.002299289684742689 + 0.1 * 6.385769367218018
Epoch 1480, val loss: 1.2005102634429932
Epoch 1490, training loss: 0.6403592824935913 = 0.0022717704996466637 + 0.1 * 6.380875110626221
Epoch 1490, val loss: 1.201900839805603
Epoch 1500, training loss: 0.6405823826789856 = 0.0022448571398854256 + 0.1 * 6.38337516784668
Epoch 1500, val loss: 1.203392505645752
Epoch 1510, training loss: 0.6406971216201782 = 0.002218410838395357 + 0.1 * 6.384787082672119
Epoch 1510, val loss: 1.204835057258606
Epoch 1520, training loss: 0.6400677561759949 = 0.0021926010958850384 + 0.1 * 6.378751277923584
Epoch 1520, val loss: 1.2063566446304321
Epoch 1530, training loss: 0.6393988132476807 = 0.002167374361306429 + 0.1 * 6.372313976287842
Epoch 1530, val loss: 1.2077257633209229
Epoch 1540, training loss: 0.6394127607345581 = 0.002142647048458457 + 0.1 * 6.3727006912231445
Epoch 1540, val loss: 1.2092139720916748
Epoch 1550, training loss: 0.6405358910560608 = 0.002118490170687437 + 0.1 * 6.38417387008667
Epoch 1550, val loss: 1.2106364965438843
Epoch 1560, training loss: 0.6391006708145142 = 0.002094798954203725 + 0.1 * 6.370058536529541
Epoch 1560, val loss: 1.2121062278747559
Epoch 1570, training loss: 0.6398928165435791 = 0.002071705646812916 + 0.1 * 6.37821102142334
Epoch 1570, val loss: 1.2134090662002563
Epoch 1580, training loss: 0.6399773955345154 = 0.002049140864983201 + 0.1 * 6.379282474517822
Epoch 1580, val loss: 1.214759349822998
Epoch 1590, training loss: 0.6393803358078003 = 0.0020269211381673813 + 0.1 * 6.373534202575684
Epoch 1590, val loss: 1.2160884141921997
Epoch 1600, training loss: 0.6389203667640686 = 0.0020051307510584593 + 0.1 * 6.369152069091797
Epoch 1600, val loss: 1.217468500137329
Epoch 1610, training loss: 0.6390830874443054 = 0.0019838118460029364 + 0.1 * 6.370992660522461
Epoch 1610, val loss: 1.2187590599060059
Epoch 1620, training loss: 0.6394212245941162 = 0.0019629462622106075 + 0.1 * 6.374582290649414
Epoch 1620, val loss: 1.2200894355773926
Epoch 1630, training loss: 0.6375836730003357 = 0.00194248603656888 + 0.1 * 6.356411457061768
Epoch 1630, val loss: 1.2213118076324463
Epoch 1640, training loss: 0.6404068470001221 = 0.001922417082823813 + 0.1 * 6.3848443031311035
Epoch 1640, val loss: 1.2225534915924072
Epoch 1650, training loss: 0.6383955478668213 = 0.001902551855891943 + 0.1 * 6.364929676055908
Epoch 1650, val loss: 1.22406804561615
Epoch 1660, training loss: 0.6394857168197632 = 0.0018832688219845295 + 0.1 * 6.37602424621582
Epoch 1660, val loss: 1.2254161834716797
Epoch 1670, training loss: 0.6387020349502563 = 0.0018644456285983324 + 0.1 * 6.368375778198242
Epoch 1670, val loss: 1.2266253232955933
Epoch 1680, training loss: 0.6376973986625671 = 0.0018458487465977669 + 0.1 * 6.35851526260376
Epoch 1680, val loss: 1.227790355682373
Epoch 1690, training loss: 0.6390884518623352 = 0.0018276235787197948 + 0.1 * 6.372608184814453
Epoch 1690, val loss: 1.2290109395980835
Epoch 1700, training loss: 0.637454092502594 = 0.001809735898859799 + 0.1 * 6.356443881988525
Epoch 1700, val loss: 1.2303203344345093
Epoch 1710, training loss: 0.6408488154411316 = 0.0017922029364854097 + 0.1 * 6.390565872192383
Epoch 1710, val loss: 1.2315402030944824
Epoch 1720, training loss: 0.6375304460525513 = 0.001774951466359198 + 0.1 * 6.3575544357299805
Epoch 1720, val loss: 1.2328367233276367
Epoch 1730, training loss: 0.6376012563705444 = 0.0017580667044967413 + 0.1 * 6.358432292938232
Epoch 1730, val loss: 1.2339777946472168
Epoch 1740, training loss: 0.6391761302947998 = 0.00174154550768435 + 0.1 * 6.374345779418945
Epoch 1740, val loss: 1.235107421875
Epoch 1750, training loss: 0.6369539499282837 = 0.00172516074962914 + 0.1 * 6.352287769317627
Epoch 1750, val loss: 1.2363814115524292
Epoch 1760, training loss: 0.6369927525520325 = 0.001709219068288803 + 0.1 * 6.352835178375244
Epoch 1760, val loss: 1.2374935150146484
Epoch 1770, training loss: 0.6379495859146118 = 0.0016934862360358238 + 0.1 * 6.362560749053955
Epoch 1770, val loss: 1.2387008666992188
Epoch 1780, training loss: 0.6367162466049194 = 0.001678003929555416 + 0.1 * 6.350382328033447
Epoch 1780, val loss: 1.239941954612732
Epoch 1790, training loss: 0.6368725895881653 = 0.0016628431621938944 + 0.1 * 6.352097511291504
Epoch 1790, val loss: 1.2410303354263306
Epoch 1800, training loss: 0.6379504203796387 = 0.0016478202305734158 + 0.1 * 6.363025665283203
Epoch 1800, val loss: 1.24225652217865
Epoch 1810, training loss: 0.6375374794006348 = 0.001633177394978702 + 0.1 * 6.359043121337891
Epoch 1810, val loss: 1.2434735298156738
Epoch 1820, training loss: 0.6372634172439575 = 0.0016188498120754957 + 0.1 * 6.3564453125
Epoch 1820, val loss: 1.2445800304412842
Epoch 1830, training loss: 0.6364169120788574 = 0.001604638178832829 + 0.1 * 6.348122596740723
Epoch 1830, val loss: 1.245612621307373
Epoch 1840, training loss: 0.6366428136825562 = 0.0015906894113868475 + 0.1 * 6.350521087646484
Epoch 1840, val loss: 1.2467149496078491
Epoch 1850, training loss: 0.6369495987892151 = 0.001577037270180881 + 0.1 * 6.353725910186768
Epoch 1850, val loss: 1.2478277683258057
Epoch 1860, training loss: 0.636983335018158 = 0.001563484314829111 + 0.1 * 6.354198455810547
Epoch 1860, val loss: 1.2489831447601318
Epoch 1870, training loss: 0.6366321444511414 = 0.001550196437165141 + 0.1 * 6.3508195877075195
Epoch 1870, val loss: 1.2502083778381348
Epoch 1880, training loss: 0.6360934376716614 = 0.0015371384797617793 + 0.1 * 6.345562934875488
Epoch 1880, val loss: 1.2512130737304688
Epoch 1890, training loss: 0.6375625133514404 = 0.0015242646913975477 + 0.1 * 6.360382556915283
Epoch 1890, val loss: 1.2522212266921997
Epoch 1900, training loss: 0.6356047987937927 = 0.0015115366550162435 + 0.1 * 6.340932846069336
Epoch 1900, val loss: 1.2533788681030273
Epoch 1910, training loss: 0.6359235644340515 = 0.001499104779213667 + 0.1 * 6.344244480133057
Epoch 1910, val loss: 1.2544479370117188
Epoch 1920, training loss: 0.6367108225822449 = 0.0014868677826598287 + 0.1 * 6.35223913192749
Epoch 1920, val loss: 1.255510926246643
Epoch 1930, training loss: 0.6389789581298828 = 0.0014746778178960085 + 0.1 * 6.37504243850708
Epoch 1930, val loss: 1.2566298246383667
Epoch 1940, training loss: 0.6361475586891174 = 0.0014627972850576043 + 0.1 * 6.3468475341796875
Epoch 1940, val loss: 1.2578309774398804
Epoch 1950, training loss: 0.6357778906822205 = 0.0014511087210848927 + 0.1 * 6.343267440795898
Epoch 1950, val loss: 1.2588458061218262
Epoch 1960, training loss: 0.6356561779975891 = 0.0014395511243492365 + 0.1 * 6.342165946960449
Epoch 1960, val loss: 1.25984787940979
Epoch 1970, training loss: 0.6352781653404236 = 0.0014282662887126207 + 0.1 * 6.338499069213867
Epoch 1970, val loss: 1.2607697248458862
Epoch 1980, training loss: 0.6356788873672485 = 0.0014170784270390868 + 0.1 * 6.342617511749268
Epoch 1980, val loss: 1.2617610692977905
Epoch 1990, training loss: 0.6370041370391846 = 0.001406015013344586 + 0.1 * 6.35598087310791
Epoch 1990, val loss: 1.2628663778305054
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8328940432261466
The final CL Acc:0.81605, 0.01222, The final GNN Acc:0.83694, 0.00400
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10538])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8079276084899902 = 1.9482414722442627 + 0.1 * 8.5968599319458
Epoch 0, val loss: 1.9421331882476807
Epoch 10, training loss: 2.7982466220855713 = 1.938568353652954 + 0.1 * 8.596782684326172
Epoch 10, val loss: 1.9327211380004883
Epoch 20, training loss: 2.7864327430725098 = 1.9267939329147339 + 0.1 * 8.59638786315918
Epoch 20, val loss: 1.9211093187332153
Epoch 30, training loss: 2.7697689533233643 = 1.9104456901550293 + 0.1 * 8.593233108520508
Epoch 30, val loss: 1.9047973155975342
Epoch 40, training loss: 2.743635654449463 = 1.886528491973877 + 0.1 * 8.571070671081543
Epoch 40, val loss: 1.881134271621704
Epoch 50, training loss: 2.700558662414551 = 1.8534806966781616 + 0.1 * 8.470780372619629
Epoch 50, val loss: 1.8498142957687378
Epoch 60, training loss: 2.628446102142334 = 1.8167659044265747 + 0.1 * 8.116802215576172
Epoch 60, val loss: 1.818085789680481
Epoch 70, training loss: 2.5806121826171875 = 1.7843931913375854 + 0.1 * 7.962189197540283
Epoch 70, val loss: 1.7912958860397339
Epoch 80, training loss: 2.5175087451934814 = 1.7506072521209717 + 0.1 * 7.6690144538879395
Epoch 80, val loss: 1.7608217000961304
Epoch 90, training loss: 2.4479739665985107 = 1.7088819742202759 + 0.1 * 7.390920639038086
Epoch 90, val loss: 1.7224699258804321
Epoch 100, training loss: 2.37709379196167 = 1.6543246507644653 + 0.1 * 7.22769021987915
Epoch 100, val loss: 1.6717956066131592
Epoch 110, training loss: 2.2950217723846436 = 1.5847618579864502 + 0.1 * 7.102598667144775
Epoch 110, val loss: 1.6092320680618286
Epoch 120, training loss: 2.2029166221618652 = 1.501025915145874 + 0.1 * 7.018906593322754
Epoch 120, val loss: 1.537570834159851
Epoch 130, training loss: 2.106050491333008 = 1.4082741737365723 + 0.1 * 6.977764129638672
Epoch 130, val loss: 1.4598928689956665
Epoch 140, training loss: 2.007091522216797 = 1.3119134902954102 + 0.1 * 6.951780319213867
Epoch 140, val loss: 1.3805211782455444
Epoch 150, training loss: 1.908710241317749 = 1.2152509689331055 + 0.1 * 6.934591770172119
Epoch 150, val loss: 1.3027757406234741
Epoch 160, training loss: 1.8137929439544678 = 1.1215847730636597 + 0.1 * 6.922082424163818
Epoch 160, val loss: 1.2292664051055908
Epoch 170, training loss: 1.7262262105941772 = 1.0349160432815552 + 0.1 * 6.913101673126221
Epoch 170, val loss: 1.162819266319275
Epoch 180, training loss: 1.6473819017410278 = 0.9569442868232727 + 0.1 * 6.904376029968262
Epoch 180, val loss: 1.1043145656585693
Epoch 190, training loss: 1.5746378898620605 = 0.8850756883621216 + 0.1 * 6.895622253417969
Epoch 190, val loss: 1.0514644384384155
Epoch 200, training loss: 1.5052032470703125 = 0.8166601657867432 + 0.1 * 6.885430335998535
Epoch 200, val loss: 1.0019749402999878
Epoch 210, training loss: 1.437873125076294 = 0.7506186366081238 + 0.1 * 6.87254524230957
Epoch 210, val loss: 0.9552120566368103
Epoch 220, training loss: 1.3733878135681152 = 0.6873009204864502 + 0.1 * 6.860869407653809
Epoch 220, val loss: 0.9119021892547607
Epoch 230, training loss: 1.3123575448989868 = 0.6275659799575806 + 0.1 * 6.8479156494140625
Epoch 230, val loss: 0.8732268810272217
Epoch 240, training loss: 1.2555122375488281 = 0.5719524025917053 + 0.1 * 6.835598468780518
Epoch 240, val loss: 0.8400062322616577
Epoch 250, training loss: 1.2029860019683838 = 0.520461916923523 + 0.1 * 6.825241565704346
Epoch 250, val loss: 0.8123810291290283
Epoch 260, training loss: 1.1549103260040283 = 0.47328847646713257 + 0.1 * 6.816218376159668
Epoch 260, val loss: 0.790643036365509
Epoch 270, training loss: 1.1112775802612305 = 0.4304355978965759 + 0.1 * 6.808419227600098
Epoch 270, val loss: 0.7744944095611572
Epoch 280, training loss: 1.0720744132995605 = 0.39173322916030884 + 0.1 * 6.803411483764648
Epoch 280, val loss: 0.7633399963378906
Epoch 290, training loss: 1.0365890264511108 = 0.356929212808609 + 0.1 * 6.796597957611084
Epoch 290, val loss: 0.7562264204025269
Epoch 300, training loss: 1.0042414665222168 = 0.325326532125473 + 0.1 * 6.789149761199951
Epoch 300, val loss: 0.7523519396781921
Epoch 310, training loss: 0.9746402502059937 = 0.2964358627796173 + 0.1 * 6.78204345703125
Epoch 310, val loss: 0.7509509325027466
Epoch 320, training loss: 0.9474972486495972 = 0.2698194086551666 + 0.1 * 6.776778221130371
Epoch 320, val loss: 0.7510098814964294
Epoch 330, training loss: 0.9229950904846191 = 0.24495799839496613 + 0.1 * 6.780371189117432
Epoch 330, val loss: 0.7524276971817017
Epoch 340, training loss: 0.8982728719711304 = 0.2218170315027237 + 0.1 * 6.7645583152771
Epoch 340, val loss: 0.7545708417892456
Epoch 350, training loss: 0.875669538974762 = 0.2002158761024475 + 0.1 * 6.7545366287231445
Epoch 350, val loss: 0.75765460729599
Epoch 360, training loss: 0.8555835485458374 = 0.18018880486488342 + 0.1 * 6.753947734832764
Epoch 360, val loss: 0.7616504430770874
Epoch 370, training loss: 0.8357599377632141 = 0.1617976874113083 + 0.1 * 6.739622116088867
Epoch 370, val loss: 0.7663610577583313
Epoch 380, training loss: 0.8179272413253784 = 0.14495861530303955 + 0.1 * 6.729686260223389
Epoch 380, val loss: 0.7720175385475159
Epoch 390, training loss: 0.8020787835121155 = 0.12963511049747467 + 0.1 * 6.7244367599487305
Epoch 390, val loss: 0.7785573601722717
Epoch 400, training loss: 0.7878175377845764 = 0.11594237387180328 + 0.1 * 6.718751430511475
Epoch 400, val loss: 0.7859975695610046
Epoch 410, training loss: 0.7748518586158752 = 0.10378136485815048 + 0.1 * 6.710705280303955
Epoch 410, val loss: 0.7937280535697937
Epoch 420, training loss: 0.7634664177894592 = 0.0929492712020874 + 0.1 * 6.705171585083008
Epoch 420, val loss: 0.802265465259552
Epoch 430, training loss: 0.7535278797149658 = 0.08331671357154846 + 0.1 * 6.702111721038818
Epoch 430, val loss: 0.8114251494407654
Epoch 440, training loss: 0.7457295060157776 = 0.07485087960958481 + 0.1 * 6.708786487579346
Epoch 440, val loss: 0.8208940029144287
Epoch 450, training loss: 0.7365134954452515 = 0.06743903458118439 + 0.1 * 6.690744876861572
Epoch 450, val loss: 0.8305302858352661
Epoch 460, training loss: 0.7295501828193665 = 0.06092928349971771 + 0.1 * 6.686208724975586
Epoch 460, val loss: 0.8405743837356567
Epoch 470, training loss: 0.7234579920768738 = 0.05522618070244789 + 0.1 * 6.682317733764648
Epoch 470, val loss: 0.8508744239807129
Epoch 480, training loss: 0.7181410193443298 = 0.05022802576422691 + 0.1 * 6.6791300773620605
Epoch 480, val loss: 0.8609849214553833
Epoch 490, training loss: 0.7133259177207947 = 0.045829109847545624 + 0.1 * 6.6749677658081055
Epoch 490, val loss: 0.8713074922561646
Epoch 500, training loss: 0.7090542316436768 = 0.04194357618689537 + 0.1 * 6.671106815338135
Epoch 500, val loss: 0.881715714931488
Epoch 510, training loss: 0.7055200338363647 = 0.03851688280701637 + 0.1 * 6.670031547546387
Epoch 510, val loss: 0.8920999765396118
Epoch 520, training loss: 0.7010341286659241 = 0.0354907251894474 + 0.1 * 6.655433654785156
Epoch 520, val loss: 0.9021071195602417
Epoch 530, training loss: 0.7013633847236633 = 0.03279818966984749 + 0.1 * 6.685652256011963
Epoch 530, val loss: 0.9123538136482239
Epoch 540, training loss: 0.6957935690879822 = 0.030412519350647926 + 0.1 * 6.653810501098633
Epoch 540, val loss: 0.9220038056373596
Epoch 550, training loss: 0.692675769329071 = 0.028278212994337082 + 0.1 * 6.643975257873535
Epoch 550, val loss: 0.9316316843032837
Epoch 560, training loss: 0.6904287934303284 = 0.026355857029557228 + 0.1 * 6.640728950500488
Epoch 560, val loss: 0.9411795735359192
Epoch 570, training loss: 0.6886371374130249 = 0.02462640590965748 + 0.1 * 6.640107154846191
Epoch 570, val loss: 0.9503524303436279
Epoch 580, training loss: 0.6857315301895142 = 0.02306549623608589 + 0.1 * 6.626660346984863
Epoch 580, val loss: 0.9592852592468262
Epoch 590, training loss: 0.6837396025657654 = 0.021651294082403183 + 0.1 * 6.620882987976074
Epoch 590, val loss: 0.9682228565216064
Epoch 600, training loss: 0.682368814945221 = 0.020364785566926003 + 0.1 * 6.620040416717529
Epoch 600, val loss: 0.9767671823501587
Epoch 610, training loss: 0.6817854046821594 = 0.019197305664420128 + 0.1 * 6.625881195068359
Epoch 610, val loss: 0.9853153824806213
Epoch 620, training loss: 0.6784564256668091 = 0.0181349515914917 + 0.1 * 6.603214740753174
Epoch 620, val loss: 0.9930936694145203
Epoch 630, training loss: 0.6772149205207825 = 0.017159711569547653 + 0.1 * 6.600552082061768
Epoch 630, val loss: 1.0010863542556763
Epoch 640, training loss: 0.677016019821167 = 0.01626572012901306 + 0.1 * 6.607502460479736
Epoch 640, val loss: 1.0089645385742188
Epoch 650, training loss: 0.6745656132698059 = 0.015450065024197102 + 0.1 * 6.591155529022217
Epoch 650, val loss: 1.0161405801773071
Epoch 660, training loss: 0.6734966039657593 = 0.014697018079459667 + 0.1 * 6.587996006011963
Epoch 660, val loss: 1.0233927965164185
Epoch 670, training loss: 0.6720134019851685 = 0.01399940438568592 + 0.1 * 6.580140113830566
Epoch 670, val loss: 1.030688762664795
Epoch 680, training loss: 0.6708288192749023 = 0.01335182972252369 + 0.1 * 6.574769496917725
Epoch 680, val loss: 1.0375031232833862
Epoch 690, training loss: 0.6703698039054871 = 0.012750620022416115 + 0.1 * 6.5761919021606445
Epoch 690, val loss: 1.044665813446045
Epoch 700, training loss: 0.6695135235786438 = 0.012194210663437843 + 0.1 * 6.573193073272705
Epoch 700, val loss: 1.0509462356567383
Epoch 710, training loss: 0.6700026988983154 = 0.011677821166813374 + 0.1 * 6.583248615264893
Epoch 710, val loss: 1.0576926469802856
Epoch 720, training loss: 0.6675828099250793 = 0.011197574436664581 + 0.1 * 6.563852310180664
Epoch 720, val loss: 1.06370210647583
Epoch 730, training loss: 0.6660784482955933 = 0.010748118162155151 + 0.1 * 6.553303241729736
Epoch 730, val loss: 1.069853663444519
Epoch 740, training loss: 0.6654214262962341 = 0.010325430892407894 + 0.1 * 6.550960063934326
Epoch 740, val loss: 1.0759849548339844
Epoch 750, training loss: 0.6653401851654053 = 0.00992942601442337 + 0.1 * 6.554107666015625
Epoch 750, val loss: 1.0821877717971802
Epoch 760, training loss: 0.6641744375228882 = 0.009559603407979012 + 0.1 * 6.54614782333374
Epoch 760, val loss: 1.0877392292022705
Epoch 770, training loss: 0.6648939847946167 = 0.00921190157532692 + 0.1 * 6.556820869445801
Epoch 770, val loss: 1.0934319496154785
Epoch 780, training loss: 0.663632333278656 = 0.008884668350219727 + 0.1 * 6.547476291656494
Epoch 780, val loss: 1.0990663766860962
Epoch 790, training loss: 0.6630426645278931 = 0.008577150292694569 + 0.1 * 6.544654846191406
Epoch 790, val loss: 1.104312539100647
Epoch 800, training loss: 0.6616168022155762 = 0.008286754600703716 + 0.1 * 6.533300399780273
Epoch 800, val loss: 1.109639286994934
Epoch 810, training loss: 0.6612887382507324 = 0.008012385107576847 + 0.1 * 6.5327630043029785
Epoch 810, val loss: 1.1148546934127808
Epoch 820, training loss: 0.6597974300384521 = 0.007752543780952692 + 0.1 * 6.520448684692383
Epoch 820, val loss: 1.120025634765625
Epoch 830, training loss: 0.6593224406242371 = 0.007507079280912876 + 0.1 * 6.518153667449951
Epoch 830, val loss: 1.1249284744262695
Epoch 840, training loss: 0.6613484621047974 = 0.0072741275653243065 + 0.1 * 6.540742874145508
Epoch 840, val loss: 1.129873514175415
Epoch 850, training loss: 0.6595777273178101 = 0.007052957080304623 + 0.1 * 6.525247573852539
Epoch 850, val loss: 1.134782075881958
Epoch 860, training loss: 0.6591838598251343 = 0.006844444666057825 + 0.1 * 6.5233941078186035
Epoch 860, val loss: 1.1393241882324219
Epoch 870, training loss: 0.6573044657707214 = 0.0066452003084123135 + 0.1 * 6.506592750549316
Epoch 870, val loss: 1.1439447402954102
Epoch 880, training loss: 0.6577723026275635 = 0.006455930415540934 + 0.1 * 6.513163089752197
Epoch 880, val loss: 1.1483501195907593
Epoch 890, training loss: 0.6578781008720398 = 0.00627537677064538 + 0.1 * 6.516027450561523
Epoch 890, val loss: 1.1529796123504639
Epoch 900, training loss: 0.6563030481338501 = 0.0061040036380290985 + 0.1 * 6.50199031829834
Epoch 900, val loss: 1.1572391986846924
Epoch 910, training loss: 0.6566153168678284 = 0.005940553732216358 + 0.1 * 6.506747722625732
Epoch 910, val loss: 1.161363959312439
Epoch 920, training loss: 0.6553171277046204 = 0.005783641245216131 + 0.1 * 6.495335102081299
Epoch 920, val loss: 1.1657077074050903
Epoch 930, training loss: 0.6565388441085815 = 0.0056344810873270035 + 0.1 * 6.5090436935424805
Epoch 930, val loss: 1.1698054075241089
Epoch 940, training loss: 0.6549105644226074 = 0.005491785239428282 + 0.1 * 6.494187355041504
Epoch 940, val loss: 1.1737463474273682
Epoch 950, training loss: 0.6549257040023804 = 0.005355825647711754 + 0.1 * 6.49569845199585
Epoch 950, val loss: 1.1775596141815186
Epoch 960, training loss: 0.6551193594932556 = 0.005224999971687794 + 0.1 * 6.498943328857422
Epoch 960, val loss: 1.1814779043197632
Epoch 970, training loss: 0.6548445820808411 = 0.005099507514387369 + 0.1 * 6.497450351715088
Epoch 970, val loss: 1.1853761672973633
Epoch 980, training loss: 0.6543968319892883 = 0.004979663994163275 + 0.1 * 6.494171619415283
Epoch 980, val loss: 1.1890301704406738
Epoch 990, training loss: 0.6534751653671265 = 0.0048649911768734455 + 0.1 * 6.4861016273498535
Epoch 990, val loss: 1.1925978660583496
Epoch 1000, training loss: 0.6535642147064209 = 0.0047545102424919605 + 0.1 * 6.488097190856934
Epoch 1000, val loss: 1.19605553150177
Epoch 1010, training loss: 0.652642011642456 = 0.004648149479180574 + 0.1 * 6.479938507080078
Epoch 1010, val loss: 1.199815273284912
Epoch 1020, training loss: 0.6523019671440125 = 0.0045463345013558865 + 0.1 * 6.477556228637695
Epoch 1020, val loss: 1.203147053718567
Epoch 1030, training loss: 0.6529200673103333 = 0.004448378458619118 + 0.1 * 6.484716892242432
Epoch 1030, val loss: 1.2064876556396484
Epoch 1040, training loss: 0.6518725156784058 = 0.004354442469775677 + 0.1 * 6.475180625915527
Epoch 1040, val loss: 1.209841012954712
Epoch 1050, training loss: 0.6526122093200684 = 0.004264017101377249 + 0.1 * 6.4834818840026855
Epoch 1050, val loss: 1.2129716873168945
Epoch 1060, training loss: 0.6516924500465393 = 0.004176559392362833 + 0.1 * 6.47515869140625
Epoch 1060, val loss: 1.2162986993789673
Epoch 1070, training loss: 0.6517921686172485 = 0.004092791583389044 + 0.1 * 6.476993560791016
Epoch 1070, val loss: 1.2193901538848877
Epoch 1080, training loss: 0.650740385055542 = 0.004011849407106638 + 0.1 * 6.46728515625
Epoch 1080, val loss: 1.222424030303955
Epoch 1090, training loss: 0.6512817740440369 = 0.003933913540095091 + 0.1 * 6.473478317260742
Epoch 1090, val loss: 1.2254995107650757
Epoch 1100, training loss: 0.6503753662109375 = 0.0038581273984164 + 0.1 * 6.465172290802002
Epoch 1100, val loss: 1.2285834550857544
Epoch 1110, training loss: 0.6517302989959717 = 0.0037855589762330055 + 0.1 * 6.479447364807129
Epoch 1110, val loss: 1.2315582036972046
Epoch 1120, training loss: 0.6499133706092834 = 0.0037151528522372246 + 0.1 * 6.461981773376465
Epoch 1120, val loss: 1.234405279159546
Epoch 1130, training loss: 0.651378870010376 = 0.003647427074611187 + 0.1 * 6.477313995361328
Epoch 1130, val loss: 1.2371649742126465
Epoch 1140, training loss: 0.6513796448707581 = 0.0035817339085042477 + 0.1 * 6.477978706359863
Epoch 1140, val loss: 1.2401509284973145
Epoch 1150, training loss: 0.6490561366081238 = 0.003518554847687483 + 0.1 * 6.455375671386719
Epoch 1150, val loss: 1.242811918258667
Epoch 1160, training loss: 0.648531973361969 = 0.003457376966252923 + 0.1 * 6.450746059417725
Epoch 1160, val loss: 1.245459794998169
Epoch 1170, training loss: 0.6482360363006592 = 0.0033984205219894648 + 0.1 * 6.448376178741455
Epoch 1170, val loss: 1.2481043338775635
Epoch 1180, training loss: 0.649991512298584 = 0.00334098469465971 + 0.1 * 6.466505527496338
Epoch 1180, val loss: 1.250718593597412
Epoch 1190, training loss: 0.6483122706413269 = 0.003285272279754281 + 0.1 * 6.45026969909668
Epoch 1190, val loss: 1.2534008026123047
Epoch 1200, training loss: 0.648135244846344 = 0.003231435315683484 + 0.1 * 6.449038505554199
Epoch 1200, val loss: 1.255883812904358
Epoch 1210, training loss: 0.6474730372428894 = 0.0031789790373295546 + 0.1 * 6.442940711975098
Epoch 1210, val loss: 1.2584381103515625
Epoch 1220, training loss: 0.6480419635772705 = 0.003128139767795801 + 0.1 * 6.449138641357422
Epoch 1220, val loss: 1.261027216911316
Epoch 1230, training loss: 0.6467567086219788 = 0.0030791552271693945 + 0.1 * 6.436775207519531
Epoch 1230, val loss: 1.263466715812683
Epoch 1240, training loss: 0.6467781662940979 = 0.003031366039067507 + 0.1 * 6.437467575073242
Epoch 1240, val loss: 1.2657628059387207
Epoch 1250, training loss: 0.6470540165901184 = 0.002985102590173483 + 0.1 * 6.4406890869140625
Epoch 1250, val loss: 1.2680689096450806
Epoch 1260, training loss: 0.6460814476013184 = 0.002939816564321518 + 0.1 * 6.431416034698486
Epoch 1260, val loss: 1.2706243991851807
Epoch 1270, training loss: 0.6457467675209045 = 0.002896208083257079 + 0.1 * 6.428505897521973
Epoch 1270, val loss: 1.2728285789489746
Epoch 1280, training loss: 0.6484732031822205 = 0.002853753976523876 + 0.1 * 6.4561944007873535
Epoch 1280, val loss: 1.2750062942504883
Epoch 1290, training loss: 0.6457631587982178 = 0.002812274731695652 + 0.1 * 6.429508686065674
Epoch 1290, val loss: 1.2774008512496948
Epoch 1300, training loss: 0.6457418203353882 = 0.002772218780592084 + 0.1 * 6.429696083068848
Epoch 1300, val loss: 1.2795854806900024
Epoch 1310, training loss: 0.6456518173217773 = 0.0027330394368618727 + 0.1 * 6.429187774658203
Epoch 1310, val loss: 1.281791090965271
Epoch 1320, training loss: 0.64512699842453 = 0.0026950363535434008 + 0.1 * 6.424319744110107
Epoch 1320, val loss: 1.2838249206542969
Epoch 1330, training loss: 0.6456080675125122 = 0.0026578488759696484 + 0.1 * 6.429502010345459
Epoch 1330, val loss: 1.2860850095748901
Epoch 1340, training loss: 0.6445102691650391 = 0.002621626015752554 + 0.1 * 6.418886184692383
Epoch 1340, val loss: 1.2882933616638184
Epoch 1350, training loss: 0.6451916098594666 = 0.002586544258520007 + 0.1 * 6.426050662994385
Epoch 1350, val loss: 1.2903655767440796
Epoch 1360, training loss: 0.6470008492469788 = 0.002552156103774905 + 0.1 * 6.44448709487915
Epoch 1360, val loss: 1.2923343181610107
Epoch 1370, training loss: 0.6448842287063599 = 0.0025188224390149117 + 0.1 * 6.423653602600098
Epoch 1370, val loss: 1.294476866722107
Epoch 1380, training loss: 0.6434847712516785 = 0.0024862869177013636 + 0.1 * 6.409984588623047
Epoch 1380, val loss: 1.2963365316390991
Epoch 1390, training loss: 0.6446040868759155 = 0.0024545055348426104 + 0.1 * 6.42149543762207
Epoch 1390, val loss: 1.2983031272888184
Epoch 1400, training loss: 0.643925130367279 = 0.0024233837611973286 + 0.1 * 6.415017127990723
Epoch 1400, val loss: 1.3003630638122559
Epoch 1410, training loss: 0.6430842876434326 = 0.002393109258264303 + 0.1 * 6.406911373138428
Epoch 1410, val loss: 1.3023440837860107
Epoch 1420, training loss: 0.6435629725456238 = 0.0023634887766093016 + 0.1 * 6.411994457244873
Epoch 1420, val loss: 1.3042221069335938
Epoch 1430, training loss: 0.64298015832901 = 0.002334685530513525 + 0.1 * 6.406454563140869
Epoch 1430, val loss: 1.3061145544052124
Epoch 1440, training loss: 0.6429259777069092 = 0.0023064804263412952 + 0.1 * 6.406195163726807
Epoch 1440, val loss: 1.3079147338867188
Epoch 1450, training loss: 0.6425840258598328 = 0.002278820611536503 + 0.1 * 6.403051853179932
Epoch 1450, val loss: 1.3099287748336792
Epoch 1460, training loss: 0.6436291337013245 = 0.0022521463688462973 + 0.1 * 6.413769721984863
Epoch 1460, val loss: 1.3116085529327393
Epoch 1470, training loss: 0.644021213054657 = 0.002225705189630389 + 0.1 * 6.417954921722412
Epoch 1470, val loss: 1.3135865926742554
Epoch 1480, training loss: 0.6422259211540222 = 0.0022000998724251986 + 0.1 * 6.400257587432861
Epoch 1480, val loss: 1.3152878284454346
Epoch 1490, training loss: 0.6445891857147217 = 0.002175072440877557 + 0.1 * 6.424140930175781
Epoch 1490, val loss: 1.3170373439788818
Epoch 1500, training loss: 0.6424551010131836 = 0.0021504710894078016 + 0.1 * 6.403046131134033
Epoch 1500, val loss: 1.3187166452407837
Epoch 1510, training loss: 0.6419970989227295 = 0.0021264860406517982 + 0.1 * 6.39870548248291
Epoch 1510, val loss: 1.3204678297042847
Epoch 1520, training loss: 0.6415964365005493 = 0.002102889819070697 + 0.1 * 6.394935131072998
Epoch 1520, val loss: 1.322251558303833
Epoch 1530, training loss: 0.6420995593070984 = 0.0020799061749130487 + 0.1 * 6.400196552276611
Epoch 1530, val loss: 1.323797583580017
Epoch 1540, training loss: 0.6419751048088074 = 0.002057324629276991 + 0.1 * 6.3991780281066895
Epoch 1540, val loss: 1.325543761253357
Epoch 1550, training loss: 0.6418264508247375 = 0.00203521316871047 + 0.1 * 6.397912502288818
Epoch 1550, val loss: 1.327248215675354
Epoch 1560, training loss: 0.6416040062904358 = 0.0020135832019150257 + 0.1 * 6.395904541015625
Epoch 1560, val loss: 1.3288192749023438
Epoch 1570, training loss: 0.6418447494506836 = 0.0019923641812056303 + 0.1 * 6.398523807525635
Epoch 1570, val loss: 1.3305373191833496
Epoch 1580, training loss: 0.6408717036247253 = 0.0019716916140168905 + 0.1 * 6.389000415802002
Epoch 1580, val loss: 1.33206307888031
Epoch 1590, training loss: 0.6418679356575012 = 0.0019513571169227362 + 0.1 * 6.399165630340576
Epoch 1590, val loss: 1.3336609601974487
Epoch 1600, training loss: 0.6404697299003601 = 0.0019314484670758247 + 0.1 * 6.385382652282715
Epoch 1600, val loss: 1.3352073431015015
Epoch 1610, training loss: 0.6415060758590698 = 0.0019119390053674579 + 0.1 * 6.395941257476807
Epoch 1610, val loss: 1.3367364406585693
Epoch 1620, training loss: 0.641554057598114 = 0.0018928092904388905 + 0.1 * 6.396612644195557
Epoch 1620, val loss: 1.338221549987793
Epoch 1630, training loss: 0.6407932639122009 = 0.001873954781331122 + 0.1 * 6.389192581176758
Epoch 1630, val loss: 1.339927315711975
Epoch 1640, training loss: 0.6404827833175659 = 0.0018556209979578853 + 0.1 * 6.3862714767456055
Epoch 1640, val loss: 1.3413465023040771
Epoch 1650, training loss: 0.6403835415840149 = 0.001837570103816688 + 0.1 * 6.385459899902344
Epoch 1650, val loss: 1.342786192893982
Epoch 1660, training loss: 0.639962911605835 = 0.0018198603065684438 + 0.1 * 6.381430625915527
Epoch 1660, val loss: 1.344230055809021
Epoch 1670, training loss: 0.6406964659690857 = 0.001802446087822318 + 0.1 * 6.38893985748291
Epoch 1670, val loss: 1.3457268476486206
Epoch 1680, training loss: 0.6397247910499573 = 0.0017854046309366822 + 0.1 * 6.379393577575684
Epoch 1680, val loss: 1.3471343517303467
Epoch 1690, training loss: 0.6405962109565735 = 0.0017686409410089254 + 0.1 * 6.388275623321533
Epoch 1690, val loss: 1.3485783338546753
Epoch 1700, training loss: 0.6415804028511047 = 0.0017520923865959048 + 0.1 * 6.398283004760742
Epoch 1700, val loss: 1.3499714136123657
Epoch 1710, training loss: 0.6391730904579163 = 0.0017359624616801739 + 0.1 * 6.37437105178833
Epoch 1710, val loss: 1.351469874382019
Epoch 1720, training loss: 0.6407464742660522 = 0.001720173517242074 + 0.1 * 6.390262603759766
Epoch 1720, val loss: 1.3526923656463623
Epoch 1730, training loss: 0.6392790079116821 = 0.001704581780359149 + 0.1 * 6.375744342803955
Epoch 1730, val loss: 1.3539215326309204
Epoch 1740, training loss: 0.640556275844574 = 0.0016891813138499856 + 0.1 * 6.388670444488525
Epoch 1740, val loss: 1.355444073677063
Epoch 1750, training loss: 0.6395570039749146 = 0.0016742543084546924 + 0.1 * 6.3788275718688965
Epoch 1750, val loss: 1.3566925525665283
Epoch 1760, training loss: 0.6386280059814453 = 0.0016594247426837683 + 0.1 * 6.369685649871826
Epoch 1760, val loss: 1.3580663204193115
Epoch 1770, training loss: 0.6392791867256165 = 0.0016448997193947434 + 0.1 * 6.3763427734375
Epoch 1770, val loss: 1.359323263168335
Epoch 1780, training loss: 0.6406341791152954 = 0.0016306114848703146 + 0.1 * 6.390035629272461
Epoch 1780, val loss: 1.3606561422348022
Epoch 1790, training loss: 0.638740062713623 = 0.0016165104461833835 + 0.1 * 6.371234893798828
Epoch 1790, val loss: 1.362031102180481
Epoch 1800, training loss: 0.6392940878868103 = 0.0016028271056711674 + 0.1 * 6.376912593841553
Epoch 1800, val loss: 1.363256573677063
Epoch 1810, training loss: 0.6381599307060242 = 0.001589275198057294 + 0.1 * 6.365705966949463
Epoch 1810, val loss: 1.3643614053726196
Epoch 1820, training loss: 0.6395403742790222 = 0.0015759792877361178 + 0.1 * 6.379643440246582
Epoch 1820, val loss: 1.3656654357910156
Epoch 1830, training loss: 0.6382430791854858 = 0.001562839257530868 + 0.1 * 6.366802215576172
Epoch 1830, val loss: 1.3669557571411133
Epoch 1840, training loss: 0.6384967565536499 = 0.0015499595319852233 + 0.1 * 6.369467735290527
Epoch 1840, val loss: 1.3681474924087524
Epoch 1850, training loss: 0.6393422484397888 = 0.0015372808557003736 + 0.1 * 6.378049850463867
Epoch 1850, val loss: 1.369320034980774
Epoch 1860, training loss: 0.6382020115852356 = 0.0015248015988618135 + 0.1 * 6.366771697998047
Epoch 1860, val loss: 1.370516300201416
Epoch 1870, training loss: 0.6395014524459839 = 0.0015125370118767023 + 0.1 * 6.379889011383057
Epoch 1870, val loss: 1.3717029094696045
Epoch 1880, training loss: 0.638123631477356 = 0.0015003621811047196 + 0.1 * 6.366232395172119
Epoch 1880, val loss: 1.3730196952819824
Epoch 1890, training loss: 0.637826144695282 = 0.001488561276346445 + 0.1 * 6.363375663757324
Epoch 1890, val loss: 1.3741251230239868
Epoch 1900, training loss: 0.6383703351020813 = 0.0014768241671845317 + 0.1 * 6.3689351081848145
Epoch 1900, val loss: 1.3752082586288452
Epoch 1910, training loss: 0.6386046409606934 = 0.0014652475947514176 + 0.1 * 6.37139368057251
Epoch 1910, val loss: 1.376427412033081
Epoch 1920, training loss: 0.6371320486068726 = 0.0014538790564984083 + 0.1 * 6.356781482696533
Epoch 1920, val loss: 1.377562165260315
Epoch 1930, training loss: 0.6373755931854248 = 0.0014427134301513433 + 0.1 * 6.359328746795654
Epoch 1930, val loss: 1.378630518913269
Epoch 1940, training loss: 0.63785320520401 = 0.001431607874110341 + 0.1 * 6.364215850830078
Epoch 1940, val loss: 1.3799021244049072
Epoch 1950, training loss: 0.6375463604927063 = 0.0014208017382770777 + 0.1 * 6.361255645751953
Epoch 1950, val loss: 1.3810402154922485
Epoch 1960, training loss: 0.6381732225418091 = 0.0014101476408541203 + 0.1 * 6.367630958557129
Epoch 1960, val loss: 1.382020115852356
Epoch 1970, training loss: 0.638117790222168 = 0.0013996001798659563 + 0.1 * 6.367181777954102
Epoch 1970, val loss: 1.3831099271774292
Epoch 1980, training loss: 0.636858344078064 = 0.0013892068527638912 + 0.1 * 6.354691028594971
Epoch 1980, val loss: 1.384262204170227
Epoch 1990, training loss: 0.6381585001945496 = 0.0013789886143058538 + 0.1 * 6.367794513702393
Epoch 1990, val loss: 1.3853230476379395
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 2.799215793609619 = 1.939532995223999 + 0.1 * 8.59682846069336
Epoch 0, val loss: 1.9482401609420776
Epoch 10, training loss: 2.7887892723083496 = 1.929120421409607 + 0.1 * 8.596687316894531
Epoch 10, val loss: 1.9380457401275635
Epoch 20, training loss: 2.775562286376953 = 1.9159903526306152 + 0.1 * 8.595719337463379
Epoch 20, val loss: 1.9245221614837646
Epoch 30, training loss: 2.756232261657715 = 1.8974369764328003 + 0.1 * 8.587953567504883
Epoch 30, val loss: 1.9049420356750488
Epoch 40, training loss: 2.725675582885742 = 1.8709872961044312 + 0.1 * 8.546881675720215
Epoch 40, val loss: 1.8771134614944458
Epoch 50, training loss: 2.6751461029052734 = 1.8379542827606201 + 0.1 * 8.371918678283691
Epoch 50, val loss: 1.843734622001648
Epoch 60, training loss: 2.6113474369049072 = 1.8051879405975342 + 0.1 * 8.06159496307373
Epoch 60, val loss: 1.8121192455291748
Epoch 70, training loss: 2.56613826751709 = 1.775771975517273 + 0.1 * 7.903662204742432
Epoch 70, val loss: 1.784283995628357
Epoch 80, training loss: 2.501539468765259 = 1.7411752939224243 + 0.1 * 7.603642463684082
Epoch 80, val loss: 1.751680850982666
Epoch 90, training loss: 2.4294259548187256 = 1.6975886821746826 + 0.1 * 7.3183722496032715
Epoch 90, val loss: 1.7121983766555786
Epoch 100, training loss: 2.3565025329589844 = 1.6405714750289917 + 0.1 * 7.159309387207031
Epoch 100, val loss: 1.6610101461410522
Epoch 110, training loss: 2.2751541137695312 = 1.567179799079895 + 0.1 * 7.079743385314941
Epoch 110, val loss: 1.5957880020141602
Epoch 120, training loss: 2.182766914367676 = 1.4796606302261353 + 0.1 * 7.03106164932251
Epoch 120, val loss: 1.5191771984100342
Epoch 130, training loss: 2.084408760070801 = 1.3844324350357056 + 0.1 * 6.999762058258057
Epoch 130, val loss: 1.437562108039856
Epoch 140, training loss: 1.9836877584457397 = 1.2865185737609863 + 0.1 * 6.971691608428955
Epoch 140, val loss: 1.3556299209594727
Epoch 150, training loss: 1.8840153217315674 = 1.189861536026001 + 0.1 * 6.941537380218506
Epoch 150, val loss: 1.2771925926208496
Epoch 160, training loss: 1.7899787425994873 = 1.0986648797988892 + 0.1 * 6.913139343261719
Epoch 160, val loss: 1.2056820392608643
Epoch 170, training loss: 1.70601487159729 = 1.0163493156433105 + 0.1 * 6.8966546058654785
Epoch 170, val loss: 1.1431883573532104
Epoch 180, training loss: 1.6310076713562012 = 0.9424038529396057 + 0.1 * 6.886037826538086
Epoch 180, val loss: 1.0886896848678589
Epoch 190, training loss: 1.55999755859375 = 0.8724815249443054 + 0.1 * 6.875159740447998
Epoch 190, val loss: 1.0382053852081299
Epoch 200, training loss: 1.490433692932129 = 0.8039318919181824 + 0.1 * 6.865017890930176
Epoch 200, val loss: 0.9901995658874512
Epoch 210, training loss: 1.4217956066131592 = 0.7364358305931091 + 0.1 * 6.853597640991211
Epoch 210, val loss: 0.9450736045837402
Epoch 220, training loss: 1.3568122386932373 = 0.6723204255104065 + 0.1 * 6.8449177742004395
Epoch 220, val loss: 0.9053565263748169
Epoch 230, training loss: 1.296642780303955 = 0.6137846112251282 + 0.1 * 6.828582286834717
Epoch 230, val loss: 0.8727720975875854
Epoch 240, training loss: 1.2427103519439697 = 0.5611153841018677 + 0.1 * 6.815950393676758
Epoch 240, val loss: 0.847822904586792
Epoch 250, training loss: 1.1943541765213013 = 0.5144855380058289 + 0.1 * 6.7986860275268555
Epoch 250, val loss: 0.830209493637085
Epoch 260, training loss: 1.1514118909835815 = 0.4727002680301666 + 0.1 * 6.787116527557373
Epoch 260, val loss: 0.8181638121604919
Epoch 270, training loss: 1.1126576662063599 = 0.4343649446964264 + 0.1 * 6.782927513122559
Epoch 270, val loss: 0.8098379373550415
Epoch 280, training loss: 1.0757992267608643 = 0.398756742477417 + 0.1 * 6.770424842834473
Epoch 280, val loss: 0.8040382266044617
Epoch 290, training loss: 1.0414419174194336 = 0.3650868237018585 + 0.1 * 6.763550758361816
Epoch 290, val loss: 0.8004679083824158
Epoch 300, training loss: 1.0087027549743652 = 0.3329657018184662 + 0.1 * 6.757370948791504
Epoch 300, val loss: 0.7989283800125122
Epoch 310, training loss: 0.9783048629760742 = 0.30262118577957153 + 0.1 * 6.756836414337158
Epoch 310, val loss: 0.7992928624153137
Epoch 320, training loss: 0.9481039643287659 = 0.27413690090179443 + 0.1 * 6.739670276641846
Epoch 320, val loss: 0.8015785217285156
Epoch 330, training loss: 0.9219768643379211 = 0.24747000634670258 + 0.1 * 6.745068550109863
Epoch 330, val loss: 0.8055582642555237
Epoch 340, training loss: 0.895413875579834 = 0.22281737625598907 + 0.1 * 6.7259650230407715
Epoch 340, val loss: 0.810813844203949
Epoch 350, training loss: 0.8720482587814331 = 0.20009036362171173 + 0.1 * 6.719579219818115
Epoch 350, val loss: 0.8174698948860168
Epoch 360, training loss: 0.8506437540054321 = 0.17939844727516174 + 0.1 * 6.712453365325928
Epoch 360, val loss: 0.8250584602355957
Epoch 370, training loss: 0.832257866859436 = 0.16072601079940796 + 0.1 * 6.715318202972412
Epoch 370, val loss: 0.833586573600769
Epoch 380, training loss: 0.8140967488288879 = 0.14399538934230804 + 0.1 * 6.701013565063477
Epoch 380, val loss: 0.8426027894020081
Epoch 390, training loss: 0.798184871673584 = 0.1290632039308548 + 0.1 * 6.691216945648193
Epoch 390, val loss: 0.8523750305175781
Epoch 400, training loss: 0.7849087715148926 = 0.11582264304161072 + 0.1 * 6.690860748291016
Epoch 400, val loss: 0.8625600337982178
Epoch 410, training loss: 0.7717589735984802 = 0.10411398112773895 + 0.1 * 6.676449775695801
Epoch 410, val loss: 0.8731372952461243
Epoch 420, training loss: 0.7621561884880066 = 0.09376479685306549 + 0.1 * 6.683913707733154
Epoch 420, val loss: 0.8842610120773315
Epoch 430, training loss: 0.7516530156135559 = 0.08470426499843597 + 0.1 * 6.669487476348877
Epoch 430, val loss: 0.894847571849823
Epoch 440, training loss: 0.7423270344734192 = 0.07670348137617111 + 0.1 * 6.656235218048096
Epoch 440, val loss: 0.9058340787887573
Epoch 450, training loss: 0.734717845916748 = 0.06961894780397415 + 0.1 * 6.650989055633545
Epoch 450, val loss: 0.9169759750366211
Epoch 460, training loss: 0.7286716103553772 = 0.06333626806735992 + 0.1 * 6.653353214263916
Epoch 460, val loss: 0.9282879829406738
Epoch 470, training loss: 0.722990870475769 = 0.057825859636068344 + 0.1 * 6.6516499519348145
Epoch 470, val loss: 0.9393108487129211
Epoch 480, training loss: 0.717056155204773 = 0.05296972021460533 + 0.1 * 6.640864372253418
Epoch 480, val loss: 0.9501038789749146
Epoch 490, training loss: 0.7114970684051514 = 0.04865211993455887 + 0.1 * 6.628449440002441
Epoch 490, val loss: 0.9608397483825684
Epoch 500, training loss: 0.706993043422699 = 0.044793229550123215 + 0.1 * 6.621997833251953
Epoch 500, val loss: 0.9715330004692078
Epoch 510, training loss: 0.7045437693595886 = 0.04135097563266754 + 0.1 * 6.631927967071533
Epoch 510, val loss: 0.9823614954948425
Epoch 520, training loss: 0.7006134986877441 = 0.03828989341855049 + 0.1 * 6.623235702514648
Epoch 520, val loss: 0.9921543598175049
Epoch 530, training loss: 0.6975633502006531 = 0.035544488579034805 + 0.1 * 6.620188236236572
Epoch 530, val loss: 1.0023356676101685
Epoch 540, training loss: 0.6935636401176453 = 0.03307817131280899 + 0.1 * 6.604854106903076
Epoch 540, val loss: 1.0121961832046509
Epoch 550, training loss: 0.6937706470489502 = 0.030849270522594452 + 0.1 * 6.629213333129883
Epoch 550, val loss: 1.0221091508865356
Epoch 560, training loss: 0.6882995367050171 = 0.028843354433774948 + 0.1 * 6.59456205368042
Epoch 560, val loss: 1.0311849117279053
Epoch 570, training loss: 0.6859489679336548 = 0.02702230215072632 + 0.1 * 6.589266777038574
Epoch 570, val loss: 1.0405100584030151
Epoch 580, training loss: 0.6860941052436829 = 0.02536427043378353 + 0.1 * 6.607298374176025
Epoch 580, val loss: 1.0494053363800049
Epoch 590, training loss: 0.6830031871795654 = 0.023861559107899666 + 0.1 * 6.591415882110596
Epoch 590, val loss: 1.0583562850952148
Epoch 600, training loss: 0.680802047252655 = 0.022493060678243637 + 0.1 * 6.583089828491211
Epoch 600, val loss: 1.0666640996932983
Epoch 610, training loss: 0.6788683533668518 = 0.021239155903458595 + 0.1 * 6.576292037963867
Epoch 610, val loss: 1.0749844312667847
Epoch 620, training loss: 0.677894651889801 = 0.020086757838726044 + 0.1 * 6.578078746795654
Epoch 620, val loss: 1.0834522247314453
Epoch 630, training loss: 0.6759189963340759 = 0.019033631309866905 + 0.1 * 6.568853855133057
Epoch 630, val loss: 1.0909051895141602
Epoch 640, training loss: 0.6767815351486206 = 0.01806231215596199 + 0.1 * 6.587192535400391
Epoch 640, val loss: 1.0987048149108887
Epoch 650, training loss: 0.6734716892242432 = 0.017169872298836708 + 0.1 * 6.563017845153809
Epoch 650, val loss: 1.1062272787094116
Epoch 660, training loss: 0.6721822619438171 = 0.016343649476766586 + 0.1 * 6.558385848999023
Epoch 660, val loss: 1.1134735345840454
Epoch 670, training loss: 0.6709803938865662 = 0.015579942613840103 + 0.1 * 6.554004192352295
Epoch 670, val loss: 1.12064790725708
Epoch 680, training loss: 0.6704980731010437 = 0.014871950261294842 + 0.1 * 6.55626106262207
Epoch 680, val loss: 1.1273818016052246
Epoch 690, training loss: 0.6688635945320129 = 0.014214150607585907 + 0.1 * 6.546494007110596
Epoch 690, val loss: 1.1343703269958496
Epoch 700, training loss: 0.6683177351951599 = 0.013605399988591671 + 0.1 * 6.547122955322266
Epoch 700, val loss: 1.1404962539672852
Epoch 710, training loss: 0.6674870848655701 = 0.013035428710281849 + 0.1 * 6.544516563415527
Epoch 710, val loss: 1.1473355293273926
Epoch 720, training loss: 0.6648146510124207 = 0.01250606682151556 + 0.1 * 6.523085594177246
Epoch 720, val loss: 1.1530698537826538
Epoch 730, training loss: 0.6645999550819397 = 0.012008954770863056 + 0.1 * 6.525909900665283
Epoch 730, val loss: 1.159163475036621
Epoch 740, training loss: 0.6656634211540222 = 0.011542076244950294 + 0.1 * 6.541213035583496
Epoch 740, val loss: 1.165374755859375
Epoch 750, training loss: 0.6635562181472778 = 0.011105607263743877 + 0.1 * 6.524506092071533
Epoch 750, val loss: 1.1709790229797363
Epoch 760, training loss: 0.662183940410614 = 0.01069580763578415 + 0.1 * 6.514881134033203
Epoch 760, val loss: 1.1764858961105347
Epoch 770, training loss: 0.6635658740997314 = 0.01030956394970417 + 0.1 * 6.532562732696533
Epoch 770, val loss: 1.1819179058074951
Epoch 780, training loss: 0.6626315712928772 = 0.009946521371603012 + 0.1 * 6.52685022354126
Epoch 780, val loss: 1.1877440214157104
Epoch 790, training loss: 0.6606816053390503 = 0.009605363011360168 + 0.1 * 6.5107622146606445
Epoch 790, val loss: 1.1924551725387573
Epoch 800, training loss: 0.6615532636642456 = 0.009281578473746777 + 0.1 * 6.522716999053955
Epoch 800, val loss: 1.1978222131729126
Epoch 810, training loss: 0.6601378917694092 = 0.008976219221949577 + 0.1 * 6.5116167068481445
Epoch 810, val loss: 1.20308518409729
Epoch 820, training loss: 0.6588048934936523 = 0.008688932284712791 + 0.1 * 6.50115966796875
Epoch 820, val loss: 1.2075268030166626
Epoch 830, training loss: 0.6594585180282593 = 0.008414633572101593 + 0.1 * 6.510438919067383
Epoch 830, val loss: 1.212505578994751
Epoch 840, training loss: 0.6575499176979065 = 0.008155647665262222 + 0.1 * 6.493942737579346
Epoch 840, val loss: 1.217334270477295
Epoch 850, training loss: 0.6571086645126343 = 0.00791002344340086 + 0.1 * 6.491986274719238
Epoch 850, val loss: 1.2215988636016846
Epoch 860, training loss: 0.655976414680481 = 0.007675448432564735 + 0.1 * 6.483009338378906
Epoch 860, val loss: 1.226505160331726
Epoch 870, training loss: 0.6593613624572754 = 0.00745332520455122 + 0.1 * 6.51908016204834
Epoch 870, val loss: 1.231061577796936
Epoch 880, training loss: 0.6551797389984131 = 0.007243041414767504 + 0.1 * 6.479366779327393
Epoch 880, val loss: 1.2353445291519165
Epoch 890, training loss: 0.6564123034477234 = 0.007042958401143551 + 0.1 * 6.4936933517456055
Epoch 890, val loss: 1.239229679107666
Epoch 900, training loss: 0.6542611122131348 = 0.006851651705801487 + 0.1 * 6.474094390869141
Epoch 900, val loss: 1.2436643838882446
Epoch 910, training loss: 0.6551763415336609 = 0.006669173017144203 + 0.1 * 6.485071659088135
Epoch 910, val loss: 1.2477115392684937
Epoch 920, training loss: 0.6540154814720154 = 0.006494258996099234 + 0.1 * 6.475212097167969
Epoch 920, val loss: 1.2519466876983643
Epoch 930, training loss: 0.6537840962409973 = 0.006328490097075701 + 0.1 * 6.4745564460754395
Epoch 930, val loss: 1.255672812461853
Epoch 940, training loss: 0.653195321559906 = 0.006169413682073355 + 0.1 * 6.470258712768555
Epoch 940, val loss: 1.2593666315078735
Epoch 950, training loss: 0.6542814373970032 = 0.0060166059993207455 + 0.1 * 6.4826483726501465
Epoch 950, val loss: 1.2635687589645386
Epoch 960, training loss: 0.6516446471214294 = 0.005871312227100134 + 0.1 * 6.457733631134033
Epoch 960, val loss: 1.267478108406067
Epoch 970, training loss: 0.652030885219574 = 0.005732763558626175 + 0.1 * 6.462981224060059
Epoch 970, val loss: 1.2706815004348755
Epoch 980, training loss: 0.650550127029419 = 0.005599355790764093 + 0.1 * 6.449507236480713
Epoch 980, val loss: 1.274206280708313
Epoch 990, training loss: 0.6512853503227234 = 0.005470937117934227 + 0.1 * 6.458144187927246
Epoch 990, val loss: 1.27766752243042
Epoch 1000, training loss: 0.6521841287612915 = 0.005347374826669693 + 0.1 * 6.468367576599121
Epoch 1000, val loss: 1.281639814376831
Epoch 1010, training loss: 0.6504936814308167 = 0.005229337606579065 + 0.1 * 6.452642917633057
Epoch 1010, val loss: 1.2847429513931274
Epoch 1020, training loss: 0.6500822901725769 = 0.005115853156894445 + 0.1 * 6.449664115905762
Epoch 1020, val loss: 1.2883050441741943
Epoch 1030, training loss: 0.6503362655639648 = 0.005007417872548103 + 0.1 * 6.4532880783081055
Epoch 1030, val loss: 1.2912291288375854
Epoch 1040, training loss: 0.6495245695114136 = 0.004903028719127178 + 0.1 * 6.4462151527404785
Epoch 1040, val loss: 1.294400691986084
Epoch 1050, training loss: 0.6490831971168518 = 0.004802136681973934 + 0.1 * 6.442811012268066
Epoch 1050, val loss: 1.2975285053253174
Epoch 1060, training loss: 0.6487252116203308 = 0.004704621154814959 + 0.1 * 6.440206050872803
Epoch 1060, val loss: 1.300832986831665
Epoch 1070, training loss: 0.6490663886070251 = 0.004610535688698292 + 0.1 * 6.444558143615723
Epoch 1070, val loss: 1.3037692308425903
Epoch 1080, training loss: 0.6478793025016785 = 0.00452004000544548 + 0.1 * 6.433592319488525
Epoch 1080, val loss: 1.3067810535430908
Epoch 1090, training loss: 0.6484130024909973 = 0.004432174377143383 + 0.1 * 6.439807891845703
Epoch 1090, val loss: 1.3099339008331299
Epoch 1100, training loss: 0.648571789264679 = 0.004348003771156073 + 0.1 * 6.442237854003906
Epoch 1100, val loss: 1.3128165006637573
Epoch 1110, training loss: 0.6471995711326599 = 0.004266454838216305 + 0.1 * 6.429331302642822
Epoch 1110, val loss: 1.3155725002288818
Epoch 1120, training loss: 0.6483836770057678 = 0.004187806509435177 + 0.1 * 6.441958427429199
Epoch 1120, val loss: 1.318357229232788
Epoch 1130, training loss: 0.6466680765151978 = 0.0041114576160907745 + 0.1 * 6.42556619644165
Epoch 1130, val loss: 1.3212312459945679
Epoch 1140, training loss: 0.648364782333374 = 0.0040378812700510025 + 0.1 * 6.4432692527771
Epoch 1140, val loss: 1.3239657878875732
Epoch 1150, training loss: 0.6462574005126953 = 0.003966328222304583 + 0.1 * 6.422910690307617
Epoch 1150, val loss: 1.3267953395843506
Epoch 1160, training loss: 0.6460821032524109 = 0.0038971994072198868 + 0.1 * 6.421848773956299
Epoch 1160, val loss: 1.329573631286621
Epoch 1170, training loss: 0.6458529233932495 = 0.003830633359029889 + 0.1 * 6.420223236083984
Epoch 1170, val loss: 1.3319839239120483
Epoch 1180, training loss: 0.6459231972694397 = 0.0037660456728190184 + 0.1 * 6.421571254730225
Epoch 1180, val loss: 1.3343735933303833
Epoch 1190, training loss: 0.6461928486824036 = 0.0037028987426310778 + 0.1 * 6.424899578094482
Epoch 1190, val loss: 1.3372000455856323
Epoch 1200, training loss: 0.6447999477386475 = 0.003642129013314843 + 0.1 * 6.4115777015686035
Epoch 1200, val loss: 1.3397047519683838
Epoch 1210, training loss: 0.6447661519050598 = 0.0035832077264785767 + 0.1 * 6.411829471588135
Epoch 1210, val loss: 1.3420811891555786
Epoch 1220, training loss: 0.6440972089767456 = 0.003525331150740385 + 0.1 * 6.405718803405762
Epoch 1220, val loss: 1.3447989225387573
Epoch 1230, training loss: 0.6455827355384827 = 0.003469625487923622 + 0.1 * 6.421130657196045
Epoch 1230, val loss: 1.3470304012298584
Epoch 1240, training loss: 0.6450212001800537 = 0.003415453713387251 + 0.1 * 6.416057109832764
Epoch 1240, val loss: 1.3495208024978638
Epoch 1250, training loss: 0.6441147923469543 = 0.003362564370036125 + 0.1 * 6.407522201538086
Epoch 1250, val loss: 1.3520692586898804
Epoch 1260, training loss: 0.6446008086204529 = 0.0033113660756498575 + 0.1 * 6.412893772125244
Epoch 1260, val loss: 1.3543366193771362
Epoch 1270, training loss: 0.6427990198135376 = 0.003261675126850605 + 0.1 * 6.395373344421387
Epoch 1270, val loss: 1.3564015626907349
Epoch 1280, training loss: 0.6450244188308716 = 0.0032134170178323984 + 0.1 * 6.418109893798828
Epoch 1280, val loss: 1.3584964275360107
Epoch 1290, training loss: 0.643092930316925 = 0.003165860427543521 + 0.1 * 6.399271011352539
Epoch 1290, val loss: 1.3612182140350342
Epoch 1300, training loss: 0.644310712814331 = 0.0031202270183712244 + 0.1 * 6.411904335021973
Epoch 1300, val loss: 1.363311529159546
Epoch 1310, training loss: 0.6436220407485962 = 0.0030756574124097824 + 0.1 * 6.405463695526123
Epoch 1310, val loss: 1.3654135465621948
Epoch 1320, training loss: 0.6427100896835327 = 0.003032372100278735 + 0.1 * 6.396777153015137
Epoch 1320, val loss: 1.3674263954162598
Epoch 1330, training loss: 0.6423986554145813 = 0.002990211360156536 + 0.1 * 6.394084453582764
Epoch 1330, val loss: 1.3695037364959717
Epoch 1340, training loss: 0.644220769405365 = 0.0029488184954971075 + 0.1 * 6.412719249725342
Epoch 1340, val loss: 1.3717674016952515
Epoch 1350, training loss: 0.6414714455604553 = 0.00290857907384634 + 0.1 * 6.3856282234191895
Epoch 1350, val loss: 1.374125361442566
Epoch 1360, training loss: 0.6426934599876404 = 0.002869655843824148 + 0.1 * 6.398237705230713
Epoch 1360, val loss: 1.3759528398513794
Epoch 1370, training loss: 0.6436203718185425 = 0.0028312436770647764 + 0.1 * 6.407891273498535
Epoch 1370, val loss: 1.3781204223632812
Epoch 1380, training loss: 0.6430092453956604 = 0.002794090425595641 + 0.1 * 6.402151584625244
Epoch 1380, val loss: 1.3801759481430054
Epoch 1390, training loss: 0.6411013603210449 = 0.002757750917226076 + 0.1 * 6.38343620300293
Epoch 1390, val loss: 1.3820377588272095
Epoch 1400, training loss: 0.6422996520996094 = 0.0027224812656641006 + 0.1 * 6.395771503448486
Epoch 1400, val loss: 1.3838756084442139
Epoch 1410, training loss: 0.641791820526123 = 0.0026875955518335104 + 0.1 * 6.391042232513428
Epoch 1410, val loss: 1.3862690925598145
Epoch 1420, training loss: 0.6407504677772522 = 0.0026537864468991756 + 0.1 * 6.380966663360596
Epoch 1420, val loss: 1.3880535364151
Epoch 1430, training loss: 0.6407448649406433 = 0.0026208912022411823 + 0.1 * 6.381239891052246
Epoch 1430, val loss: 1.3898475170135498
Epoch 1440, training loss: 0.6423693299293518 = 0.0025884518399834633 + 0.1 * 6.39780855178833
Epoch 1440, val loss: 1.3920849561691284
Epoch 1450, training loss: 0.640648365020752 = 0.002556720981374383 + 0.1 * 6.380916118621826
Epoch 1450, val loss: 1.394081950187683
Epoch 1460, training loss: 0.6425216794013977 = 0.0025260646361857653 + 0.1 * 6.399956226348877
Epoch 1460, val loss: 1.3958035707473755
Epoch 1470, training loss: 0.6404409408569336 = 0.002495828550308943 + 0.1 * 6.379450798034668
Epoch 1470, val loss: 1.3978718519210815
Epoch 1480, training loss: 0.6405442357063293 = 0.002466530306264758 + 0.1 * 6.380776882171631
Epoch 1480, val loss: 1.3994522094726562
Epoch 1490, training loss: 0.63990718126297 = 0.0024376881774514914 + 0.1 * 6.37469482421875
Epoch 1490, val loss: 1.401340365409851
Epoch 1500, training loss: 0.6405448913574219 = 0.0024094905238598585 + 0.1 * 6.381353855133057
Epoch 1500, val loss: 1.4032113552093506
Epoch 1510, training loss: 0.6404094099998474 = 0.0023817664477974176 + 0.1 * 6.380276679992676
Epoch 1510, val loss: 1.4052904844284058
Epoch 1520, training loss: 0.6400544047355652 = 0.002354945056140423 + 0.1 * 6.376994609832764
Epoch 1520, val loss: 1.4069976806640625
Epoch 1530, training loss: 0.6398143172264099 = 0.002328647766262293 + 0.1 * 6.374856948852539
Epoch 1530, val loss: 1.4086815118789673
Epoch 1540, training loss: 0.640618085861206 = 0.002302862238138914 + 0.1 * 6.383152008056641
Epoch 1540, val loss: 1.4103951454162598
Epoch 1550, training loss: 0.6410733461380005 = 0.0022775849793106318 + 0.1 * 6.3879570960998535
Epoch 1550, val loss: 1.4123057126998901
Epoch 1560, training loss: 0.6388863325119019 = 0.0022528020199388266 + 0.1 * 6.366334915161133
Epoch 1560, val loss: 1.4141998291015625
Epoch 1570, training loss: 0.6399236917495728 = 0.0022287892643362284 + 0.1 * 6.376948833465576
Epoch 1570, val loss: 1.4157278537750244
Epoch 1580, training loss: 0.6391162872314453 = 0.002205121563747525 + 0.1 * 6.36911153793335
Epoch 1580, val loss: 1.4175078868865967
Epoch 1590, training loss: 0.640630841255188 = 0.0021817891392856836 + 0.1 * 6.384490489959717
Epoch 1590, val loss: 1.4194260835647583
Epoch 1600, training loss: 0.6387854814529419 = 0.002158874412998557 + 0.1 * 6.366265773773193
Epoch 1600, val loss: 1.4213593006134033
Epoch 1610, training loss: 0.6401474475860596 = 0.0021368677262216806 + 0.1 * 6.380105495452881
Epoch 1610, val loss: 1.422681212425232
Epoch 1620, training loss: 0.63832026720047 = 0.0021150519605726004 + 0.1 * 6.362051963806152
Epoch 1620, val loss: 1.4243049621582031
Epoch 1630, training loss: 0.6415774822235107 = 0.0020936985965818167 + 0.1 * 6.394837856292725
Epoch 1630, val loss: 1.4260883331298828
Epoch 1640, training loss: 0.6384887099266052 = 0.002072506584227085 + 0.1 * 6.364161968231201
Epoch 1640, val loss: 1.4278849363327026
Epoch 1650, training loss: 0.6382419466972351 = 0.002052068943157792 + 0.1 * 6.361898422241211
Epoch 1650, val loss: 1.4292951822280884
Epoch 1660, training loss: 0.6396466493606567 = 0.0020319551695138216 + 0.1 * 6.3761467933654785
Epoch 1660, val loss: 1.4307883977890015
Epoch 1670, training loss: 0.6382283568382263 = 0.0020120826084166765 + 0.1 * 6.3621625900268555
Epoch 1670, val loss: 1.4326213598251343
Epoch 1680, training loss: 0.6381674408912659 = 0.001992704113945365 + 0.1 * 6.3617472648620605
Epoch 1680, val loss: 1.4341663122177124
Epoch 1690, training loss: 0.638542652130127 = 0.001973615027964115 + 0.1 * 6.365690231323242
Epoch 1690, val loss: 1.4357532262802124
Epoch 1700, training loss: 0.6390921473503113 = 0.0019548945128917694 + 0.1 * 6.371372222900391
Epoch 1700, val loss: 1.4374206066131592
Epoch 1710, training loss: 0.6373914480209351 = 0.0019364594481885433 + 0.1 * 6.354549884796143
Epoch 1710, val loss: 1.4391840696334839
Epoch 1720, training loss: 0.6381118893623352 = 0.0019185501150786877 + 0.1 * 6.36193323135376
Epoch 1720, val loss: 1.4404923915863037
Epoch 1730, training loss: 0.6393185257911682 = 0.0019009148236364126 + 0.1 * 6.374176025390625
Epoch 1730, val loss: 1.4421378374099731
Epoch 1740, training loss: 0.6380971074104309 = 0.0018834411166608334 + 0.1 * 6.362136363983154
Epoch 1740, val loss: 1.4436283111572266
Epoch 1750, training loss: 0.637254536151886 = 0.0018665228271856904 + 0.1 * 6.353879928588867
Epoch 1750, val loss: 1.4450669288635254
Epoch 1760, training loss: 0.6385864019393921 = 0.0018498423742130399 + 0.1 * 6.36736536026001
Epoch 1760, val loss: 1.4466296434402466
Epoch 1770, training loss: 0.6371898055076599 = 0.001833182293921709 + 0.1 * 6.3535661697387695
Epoch 1770, val loss: 1.448310136795044
Epoch 1780, training loss: 0.6388736367225647 = 0.0018170453840866685 + 0.1 * 6.370565891265869
Epoch 1780, val loss: 1.4496662616729736
Epoch 1790, training loss: 0.6377183198928833 = 0.0018011750653386116 + 0.1 * 6.359171390533447
Epoch 1790, val loss: 1.4512279033660889
Epoch 1800, training loss: 0.6381696462631226 = 0.001785558182746172 + 0.1 * 6.363840579986572
Epoch 1800, val loss: 1.4526938199996948
Epoch 1810, training loss: 0.6368407607078552 = 0.0017702891491353512 + 0.1 * 6.350704193115234
Epoch 1810, val loss: 1.4540221691131592
Epoch 1820, training loss: 0.6372608542442322 = 0.0017553295474499464 + 0.1 * 6.35505485534668
Epoch 1820, val loss: 1.455309271812439
Epoch 1830, training loss: 0.6384669542312622 = 0.0017404283862560987 + 0.1 * 6.367264747619629
Epoch 1830, val loss: 1.4568740129470825
Epoch 1840, training loss: 0.6361041069030762 = 0.0017257588915526867 + 0.1 * 6.343783855438232
Epoch 1840, val loss: 1.4586387872695923
Epoch 1850, training loss: 0.6373312473297119 = 0.0017115614609792829 + 0.1 * 6.356196880340576
Epoch 1850, val loss: 1.4598755836486816
Epoch 1860, training loss: 0.636925995349884 = 0.0016973892925307155 + 0.1 * 6.352286338806152
Epoch 1860, val loss: 1.461169958114624
Epoch 1870, training loss: 0.6366060972213745 = 0.0016834234120324254 + 0.1 * 6.349226474761963
Epoch 1870, val loss: 1.462767243385315
Epoch 1880, training loss: 0.6386566162109375 = 0.0016697236569598317 + 0.1 * 6.369868755340576
Epoch 1880, val loss: 1.4641640186309814
Epoch 1890, training loss: 0.635854959487915 = 0.0016562271630391479 + 0.1 * 6.341987133026123
Epoch 1890, val loss: 1.465576410293579
Epoch 1900, training loss: 0.6376126408576965 = 0.0016431189142167568 + 0.1 * 6.359694957733154
Epoch 1900, val loss: 1.4667326211929321
Epoch 1910, training loss: 0.636443018913269 = 0.0016299561830237508 + 0.1 * 6.348130702972412
Epoch 1910, val loss: 1.468353271484375
Epoch 1920, training loss: 0.6360372304916382 = 0.0016170769231393933 + 0.1 * 6.344201564788818
Epoch 1920, val loss: 1.469618320465088
Epoch 1930, training loss: 0.6367873549461365 = 0.0016043430659919977 + 0.1 * 6.351830005645752
Epoch 1930, val loss: 1.471153974533081
Epoch 1940, training loss: 0.6354292035102844 = 0.0015918880235403776 + 0.1 * 6.338373184204102
Epoch 1940, val loss: 1.472502589225769
Epoch 1950, training loss: 0.6374647617340088 = 0.0015797408996149898 + 0.1 * 6.358850002288818
Epoch 1950, val loss: 1.473646640777588
Epoch 1960, training loss: 0.6363050937652588 = 0.001567529165185988 + 0.1 * 6.347375392913818
Epoch 1960, val loss: 1.475099802017212
Epoch 1970, training loss: 0.635441243648529 = 0.0015556467697024345 + 0.1 * 6.338856220245361
Epoch 1970, val loss: 1.47636079788208
Epoch 1980, training loss: 0.6373527646064758 = 0.001543997903354466 + 0.1 * 6.358087539672852
Epoch 1980, val loss: 1.477565050125122
Epoch 1990, training loss: 0.6354457139968872 = 0.001532332506030798 + 0.1 * 6.3391337394714355
Epoch 1990, val loss: 1.4792112112045288
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 2.8045215606689453 = 1.9448366165161133 + 0.1 * 8.596848487854004
Epoch 0, val loss: 1.9402621984481812
Epoch 10, training loss: 2.7950217723846436 = 1.9353415966033936 + 0.1 * 8.5968017578125
Epoch 10, val loss: 1.9310903549194336
Epoch 20, training loss: 2.78367280960083 = 1.9240187406539917 + 0.1 * 8.596540451049805
Epoch 20, val loss: 1.9196038246154785
Epoch 30, training loss: 2.7678866386413574 = 1.9084335565567017 + 0.1 * 8.594531059265137
Epoch 30, val loss: 1.9033840894699097
Epoch 40, training loss: 2.743318796157837 = 1.8855520486831665 + 0.1 * 8.577668190002441
Epoch 40, val loss: 1.8796100616455078
Epoch 50, training loss: 2.703002691268921 = 1.8531512022018433 + 0.1 * 8.498515129089355
Epoch 50, val loss: 1.8470009565353394
Epoch 60, training loss: 2.629974126815796 = 1.8161778450012207 + 0.1 * 8.137962341308594
Epoch 60, val loss: 1.812391757965088
Epoch 70, training loss: 2.573044538497925 = 1.7831664085388184 + 0.1 * 7.898780822753906
Epoch 70, val loss: 1.7844146490097046
Epoch 80, training loss: 2.5000622272491455 = 1.7505247592926025 + 0.1 * 7.495373725891113
Epoch 80, val loss: 1.7547227144241333
Epoch 90, training loss: 2.433194160461426 = 1.7088781595230103 + 0.1 * 7.243160247802734
Epoch 90, val loss: 1.7169219255447388
Epoch 100, training loss: 2.3707122802734375 = 1.6538902521133423 + 0.1 * 7.168220520019531
Epoch 100, val loss: 1.6670531034469604
Epoch 110, training loss: 2.294720411300659 = 1.5820926427841187 + 0.1 * 7.126277923583984
Epoch 110, val loss: 1.6018153429031372
Epoch 120, training loss: 2.202756404876709 = 1.492937684059143 + 0.1 * 7.09818696975708
Epoch 120, val loss: 1.5226150751113892
Epoch 130, training loss: 2.097344398498535 = 1.3900471925735474 + 0.1 * 7.072970867156982
Epoch 130, val loss: 1.4332631826400757
Epoch 140, training loss: 1.9836277961730957 = 1.278890609741211 + 0.1 * 7.047372341156006
Epoch 140, val loss: 1.3380961418151855
Epoch 150, training loss: 1.8683598041534424 = 1.1662838459014893 + 0.1 * 7.0207600593566895
Epoch 150, val loss: 1.244204044342041
Epoch 160, training loss: 1.7571406364440918 = 1.0578651428222656 + 0.1 * 6.9927544593811035
Epoch 160, val loss: 1.1561133861541748
Epoch 170, training loss: 1.6530303955078125 = 0.9568704962730408 + 0.1 * 6.961599349975586
Epoch 170, val loss: 1.0762290954589844
Epoch 180, training loss: 1.5593714714050293 = 0.8649241328239441 + 0.1 * 6.944472789764404
Epoch 180, val loss: 1.0059516429901123
Epoch 190, training loss: 1.477011799812317 = 0.7853620648384094 + 0.1 * 6.916497230529785
Epoch 190, val loss: 0.9481765031814575
Epoch 200, training loss: 1.4058723449707031 = 0.7157370448112488 + 0.1 * 6.901353359222412
Epoch 200, val loss: 0.9000641703605652
Epoch 210, training loss: 1.342059850692749 = 0.6534993648529053 + 0.1 * 6.885604381561279
Epoch 210, val loss: 0.8594616055488586
Epoch 220, training loss: 1.283263921737671 = 0.5964592695236206 + 0.1 * 6.868045806884766
Epoch 220, val loss: 0.8247449398040771
Epoch 230, training loss: 1.2286221981048584 = 0.5436217188835144 + 0.1 * 6.85000467300415
Epoch 230, val loss: 0.7953813672065735
Epoch 240, training loss: 1.178260087966919 = 0.49414151906967163 + 0.1 * 6.841186046600342
Epoch 240, val loss: 0.7711874842643738
Epoch 250, training loss: 1.1319483518600464 = 0.4494072496891022 + 0.1 * 6.825411319732666
Epoch 250, val loss: 0.7515419721603394
Epoch 260, training loss: 1.0885863304138184 = 0.40779584646224976 + 0.1 * 6.807904243469238
Epoch 260, val loss: 0.7356858253479004
Epoch 270, training loss: 1.048496961593628 = 0.36893755197525024 + 0.1 * 6.795594692230225
Epoch 270, val loss: 0.7227428555488586
Epoch 280, training loss: 1.0127403736114502 = 0.3328563868999481 + 0.1 * 6.798839569091797
Epoch 280, val loss: 0.7125918865203857
Epoch 290, training loss: 0.9778820276260376 = 0.2999068796634674 + 0.1 * 6.779751300811768
Epoch 290, val loss: 0.7046923637390137
Epoch 300, training loss: 0.9470295906066895 = 0.269677072763443 + 0.1 * 6.773525238037109
Epoch 300, val loss: 0.6992664337158203
Epoch 310, training loss: 0.919809103012085 = 0.24206379055976868 + 0.1 * 6.777453422546387
Epoch 310, val loss: 0.6960282325744629
Epoch 320, training loss: 0.8932414054870605 = 0.21719686686992645 + 0.1 * 6.7604451179504395
Epoch 320, val loss: 0.6944283843040466
Epoch 330, training loss: 0.870932936668396 = 0.19499316811561584 + 0.1 * 6.759397506713867
Epoch 330, val loss: 0.6947846412658691
Epoch 340, training loss: 0.8499910831451416 = 0.17535032331943512 + 0.1 * 6.746407508850098
Epoch 340, val loss: 0.6963082551956177
Epoch 350, training loss: 0.8314965963363647 = 0.1578598916530609 + 0.1 * 6.7363667488098145
Epoch 350, val loss: 0.6994108557701111
Epoch 360, training loss: 0.8149764537811279 = 0.14233383536338806 + 0.1 * 6.726426601409912
Epoch 360, val loss: 0.7038431763648987
Epoch 370, training loss: 0.8015025854110718 = 0.12862031161785126 + 0.1 * 6.728822708129883
Epoch 370, val loss: 0.7091145515441895
Epoch 380, training loss: 0.7870324850082397 = 0.11647436767816544 + 0.1 * 6.705580711364746
Epoch 380, val loss: 0.7153106927871704
Epoch 390, training loss: 0.776801347732544 = 0.1057458445429802 + 0.1 * 6.710554599761963
Epoch 390, val loss: 0.7223536968231201
Epoch 400, training loss: 0.7646167278289795 = 0.0962638407945633 + 0.1 * 6.683528900146484
Epoch 400, val loss: 0.7296268343925476
Epoch 410, training loss: 0.7552205324172974 = 0.08778809010982513 + 0.1 * 6.674324035644531
Epoch 410, val loss: 0.7376409769058228
Epoch 420, training loss: 0.7493691444396973 = 0.08024518191814423 + 0.1 * 6.691239356994629
Epoch 420, val loss: 0.7460528612136841
Epoch 430, training loss: 0.7399527430534363 = 0.07352788001298904 + 0.1 * 6.664248466491699
Epoch 430, val loss: 0.7543498873710632
Epoch 440, training loss: 0.7322887182235718 = 0.06748781353235245 + 0.1 * 6.648008823394775
Epoch 440, val loss: 0.7631792426109314
Epoch 450, training loss: 0.7277042865753174 = 0.06205637380480766 + 0.1 * 6.656479358673096
Epoch 450, val loss: 0.7721267342567444
Epoch 460, training loss: 0.7205759882926941 = 0.05718337744474411 + 0.1 * 6.633925914764404
Epoch 460, val loss: 0.7809748649597168
Epoch 470, training loss: 0.7163839936256409 = 0.05279216915369034 + 0.1 * 6.635918140411377
Epoch 470, val loss: 0.7900184988975525
Epoch 480, training loss: 0.7102963328361511 = 0.04884195700287819 + 0.1 * 6.614543437957764
Epoch 480, val loss: 0.7989217042922974
Epoch 490, training loss: 0.7073246836662292 = 0.04527117684483528 + 0.1 * 6.620535373687744
Epoch 490, val loss: 0.8078638911247253
Epoch 500, training loss: 0.7035434246063232 = 0.042049236595630646 + 0.1 * 6.614941596984863
Epoch 500, val loss: 0.8167015910148621
Epoch 510, training loss: 0.699558436870575 = 0.03913045674562454 + 0.1 * 6.6042799949646
Epoch 510, val loss: 0.8254658579826355
Epoch 520, training loss: 0.6979063153266907 = 0.03648853302001953 + 0.1 * 6.614177703857422
Epoch 520, val loss: 0.834258496761322
Epoch 530, training loss: 0.693205714225769 = 0.034103065729141235 + 0.1 * 6.591026306152344
Epoch 530, val loss: 0.8426413536071777
Epoch 540, training loss: 0.6909580230712891 = 0.031929079443216324 + 0.1 * 6.590289115905762
Epoch 540, val loss: 0.85103440284729
Epoch 550, training loss: 0.6889699101448059 = 0.029945747926831245 + 0.1 * 6.5902419090271
Epoch 550, val loss: 0.8594133257865906
Epoch 560, training loss: 0.686500072479248 = 0.028142567723989487 + 0.1 * 6.5835747718811035
Epoch 560, val loss: 0.867462694644928
Epoch 570, training loss: 0.6846816539764404 = 0.026492826640605927 + 0.1 * 6.581888198852539
Epoch 570, val loss: 0.8754034042358398
Epoch 580, training loss: 0.6845380067825317 = 0.024981124326586723 + 0.1 * 6.595568656921387
Epoch 580, val loss: 0.8832372426986694
Epoch 590, training loss: 0.6807260513305664 = 0.023597147315740585 + 0.1 * 6.5712890625
Epoch 590, val loss: 0.8908658623695374
Epoch 600, training loss: 0.6802994012832642 = 0.022324735298752785 + 0.1 * 6.579746246337891
Epoch 600, val loss: 0.898382306098938
Epoch 610, training loss: 0.6776496171951294 = 0.021156515926122665 + 0.1 * 6.564931392669678
Epoch 610, val loss: 0.9057035446166992
Epoch 620, training loss: 0.6761812567710876 = 0.020079826936125755 + 0.1 * 6.561014175415039
Epoch 620, val loss: 0.9128082394599915
Epoch 630, training loss: 0.6744104027748108 = 0.019085891544818878 + 0.1 * 6.5532450675964355
Epoch 630, val loss: 0.9198684096336365
Epoch 640, training loss: 0.675748348236084 = 0.01816675253212452 + 0.1 * 6.575815677642822
Epoch 640, val loss: 0.9267343878746033
Epoch 650, training loss: 0.6725925207138062 = 0.01731809414923191 + 0.1 * 6.552744388580322
Epoch 650, val loss: 0.9334338307380676
Epoch 660, training loss: 0.6707143783569336 = 0.016533011570572853 + 0.1 * 6.541813373565674
Epoch 660, val loss: 0.9398804903030396
Epoch 670, training loss: 0.6692572832107544 = 0.015799935907125473 + 0.1 * 6.534573554992676
Epoch 670, val loss: 0.9463367462158203
Epoch 680, training loss: 0.6700272560119629 = 0.015117031522095203 + 0.1 * 6.549101829528809
Epoch 680, val loss: 0.9526994228363037
Epoch 690, training loss: 0.667587399482727 = 0.014480250887572765 + 0.1 * 6.531071662902832
Epoch 690, val loss: 0.9588804841041565
Epoch 700, training loss: 0.6683556437492371 = 0.013886213302612305 + 0.1 * 6.544693946838379
Epoch 700, val loss: 0.9649134874343872
Epoch 710, training loss: 0.665239155292511 = 0.013331618160009384 + 0.1 * 6.519075393676758
Epoch 710, val loss: 0.970794677734375
Epoch 720, training loss: 0.6658132076263428 = 0.012810295447707176 + 0.1 * 6.530028820037842
Epoch 720, val loss: 0.976597785949707
Epoch 730, training loss: 0.6626013517379761 = 0.0123221380636096 + 0.1 * 6.5027923583984375
Epoch 730, val loss: 0.98233562707901
Epoch 740, training loss: 0.6639150977134705 = 0.01186368428170681 + 0.1 * 6.520514011383057
Epoch 740, val loss: 0.9879192113876343
Epoch 750, training loss: 0.6616321802139282 = 0.011432075873017311 + 0.1 * 6.5020012855529785
Epoch 750, val loss: 0.9934006333351135
Epoch 760, training loss: 0.6604113578796387 = 0.011026927269995213 + 0.1 * 6.493844032287598
Epoch 760, val loss: 0.9987820386886597
Epoch 770, training loss: 0.6620792746543884 = 0.010643260553479195 + 0.1 * 6.514359951019287
Epoch 770, val loss: 1.0040758848190308
Epoch 780, training loss: 0.6597248315811157 = 0.010282435454428196 + 0.1 * 6.494423866271973
Epoch 780, val loss: 1.0092862844467163
Epoch 790, training loss: 0.6630135774612427 = 0.009941726922988892 + 0.1 * 6.530718803405762
Epoch 790, val loss: 1.0143990516662598
Epoch 800, training loss: 0.6575281620025635 = 0.009618381969630718 + 0.1 * 6.479097366333008
Epoch 800, val loss: 1.0193806886672974
Epoch 810, training loss: 0.6571494936943054 = 0.009312525391578674 + 0.1 * 6.47836971282959
Epoch 810, val loss: 1.0243065357208252
Epoch 820, training loss: 0.656360924243927 = 0.009022885002195835 + 0.1 * 6.473380088806152
Epoch 820, val loss: 1.0291697978973389
Epoch 830, training loss: 0.6600397229194641 = 0.008747505024075508 + 0.1 * 6.5129218101501465
Epoch 830, val loss: 1.0339654684066772
Epoch 840, training loss: 0.6557857394218445 = 0.008486775681376457 + 0.1 * 6.472990036010742
Epoch 840, val loss: 1.038644790649414
Epoch 850, training loss: 0.6552923917770386 = 0.008238675072789192 + 0.1 * 6.470537185668945
Epoch 850, val loss: 1.043213963508606
Epoch 860, training loss: 0.6543720364570618 = 0.008002183400094509 + 0.1 * 6.463698387145996
Epoch 860, val loss: 1.0477405786514282
Epoch 870, training loss: 0.654062032699585 = 0.007776529993861914 + 0.1 * 6.462854385375977
Epoch 870, val loss: 1.0522468090057373
Epoch 880, training loss: 0.6526177525520325 = 0.007561149541288614 + 0.1 * 6.450565814971924
Epoch 880, val loss: 1.0566635131835938
Epoch 890, training loss: 0.6555706262588501 = 0.007355142384767532 + 0.1 * 6.482154369354248
Epoch 890, val loss: 1.0610426664352417
Epoch 900, training loss: 0.6535179615020752 = 0.0071595110930502415 + 0.1 * 6.4635844230651855
Epoch 900, val loss: 1.0653265714645386
Epoch 910, training loss: 0.6527466177940369 = 0.006971859838813543 + 0.1 * 6.457747936248779
Epoch 910, val loss: 1.0694962739944458
Epoch 920, training loss: 0.6516130566596985 = 0.006793323438614607 + 0.1 * 6.448197364807129
Epoch 920, val loss: 1.0735902786254883
Epoch 930, training loss: 0.6516153812408447 = 0.0066218324936926365 + 0.1 * 6.449935436248779
Epoch 930, val loss: 1.077631950378418
Epoch 940, training loss: 0.6505893468856812 = 0.006457759998738766 + 0.1 * 6.4413161277771
Epoch 940, val loss: 1.0816048383712769
Epoch 950, training loss: 0.6521071791648865 = 0.006301065906882286 + 0.1 * 6.458061218261719
Epoch 950, val loss: 1.0855534076690674
Epoch 960, training loss: 0.6496009826660156 = 0.006149382330477238 + 0.1 * 6.434515476226807
Epoch 960, val loss: 1.089443325996399
Epoch 970, training loss: 0.6503832340240479 = 0.006004146300256252 + 0.1 * 6.443790435791016
Epoch 970, val loss: 1.0932904481887817
Epoch 980, training loss: 0.6487939357757568 = 0.005865150596946478 + 0.1 * 6.429287910461426
Epoch 980, val loss: 1.0970799922943115
Epoch 990, training loss: 0.6535518169403076 = 0.00573158310726285 + 0.1 * 6.4782023429870605
Epoch 990, val loss: 1.1008093357086182
Epoch 1000, training loss: 0.6496807932853699 = 0.005603085272014141 + 0.1 * 6.44077730178833
Epoch 1000, val loss: 1.1044137477874756
Epoch 1010, training loss: 0.6486179828643799 = 0.005480025429278612 + 0.1 * 6.431379318237305
Epoch 1010, val loss: 1.1079862117767334
Epoch 1020, training loss: 0.6484856009483337 = 0.005361206829547882 + 0.1 * 6.431243896484375
Epoch 1020, val loss: 1.1115167140960693
Epoch 1030, training loss: 0.64871746301651 = 0.005246730521321297 + 0.1 * 6.434706687927246
Epoch 1030, val loss: 1.1150166988372803
Epoch 1040, training loss: 0.6477426886558533 = 0.005136455874890089 + 0.1 * 6.42606258392334
Epoch 1040, val loss: 1.1184957027435303
Epoch 1050, training loss: 0.6492968797683716 = 0.00502970302477479 + 0.1 * 6.442671775817871
Epoch 1050, val loss: 1.1219258308410645
Epoch 1060, training loss: 0.6482715010643005 = 0.00492726219817996 + 0.1 * 6.433442115783691
Epoch 1060, val loss: 1.1252778768539429
Epoch 1070, training loss: 0.6475799083709717 = 0.004828294739127159 + 0.1 * 6.427516460418701
Epoch 1070, val loss: 1.128570795059204
Epoch 1080, training loss: 0.6458442807197571 = 0.0047327191568911076 + 0.1 * 6.4111151695251465
Epoch 1080, val loss: 1.131844401359558
Epoch 1090, training loss: 0.6479235887527466 = 0.004640847910195589 + 0.1 * 6.432826995849609
Epoch 1090, val loss: 1.1350635290145874
Epoch 1100, training loss: 0.6466104388237 = 0.004550768993794918 + 0.1 * 6.420596599578857
Epoch 1100, val loss: 1.1382324695587158
Epoch 1110, training loss: 0.6465742588043213 = 0.00446442561224103 + 0.1 * 6.421098232269287
Epoch 1110, val loss: 1.1413981914520264
Epoch 1120, training loss: 0.6458900570869446 = 0.0043811616487801075 + 0.1 * 6.415088653564453
Epoch 1120, val loss: 1.14451003074646
Epoch 1130, training loss: 0.6448452472686768 = 0.0043006781488657 + 0.1 * 6.405445575714111
Epoch 1130, val loss: 1.1475902795791626
Epoch 1140, training loss: 0.6483914852142334 = 0.004222393501549959 + 0.1 * 6.441690921783447
Epoch 1140, val loss: 1.1505937576293945
Epoch 1150, training loss: 0.6452282071113586 = 0.004146370571106672 + 0.1 * 6.410818099975586
Epoch 1150, val loss: 1.1536074876785278
Epoch 1160, training loss: 0.6458780765533447 = 0.004073422402143478 + 0.1 * 6.418046474456787
Epoch 1160, val loss: 1.1565837860107422
Epoch 1170, training loss: 0.6445672512054443 = 0.004002181347459555 + 0.1 * 6.405651092529297
Epoch 1170, val loss: 1.1594840288162231
Epoch 1180, training loss: 0.6435521841049194 = 0.003933449741452932 + 0.1 * 6.3961873054504395
Epoch 1180, val loss: 1.1623764038085938
Epoch 1190, training loss: 0.6454083919525146 = 0.0038670585490763187 + 0.1 * 6.4154133796691895
Epoch 1190, val loss: 1.1652432680130005
Epoch 1200, training loss: 0.643201470375061 = 0.0038018953055143356 + 0.1 * 6.393995761871338
Epoch 1200, val loss: 1.1680611371994019
Epoch 1210, training loss: 0.6445430517196655 = 0.00373918772675097 + 0.1 * 6.40803861618042
Epoch 1210, val loss: 1.1708890199661255
Epoch 1220, training loss: 0.6443573832511902 = 0.0036784326657652855 + 0.1 * 6.406789302825928
Epoch 1220, val loss: 1.1736512184143066
Epoch 1230, training loss: 0.6449019908905029 = 0.0036189977545291185 + 0.1 * 6.412829875946045
Epoch 1230, val loss: 1.176436185836792
Epoch 1240, training loss: 0.6423874497413635 = 0.0035613819491118193 + 0.1 * 6.388260841369629
Epoch 1240, val loss: 1.1791211366653442
Epoch 1250, training loss: 0.6435597538948059 = 0.00350560387596488 + 0.1 * 6.40054178237915
Epoch 1250, val loss: 1.181779384613037
Epoch 1260, training loss: 0.6426268219947815 = 0.003451228840276599 + 0.1 * 6.3917555809021
Epoch 1260, val loss: 1.1844006776809692
Epoch 1270, training loss: 0.6435542106628418 = 0.0033985022455453873 + 0.1 * 6.401557445526123
Epoch 1270, val loss: 1.1870239973068237
Epoch 1280, training loss: 0.6417616009712219 = 0.003347290912643075 + 0.1 * 6.384142875671387
Epoch 1280, val loss: 1.1895726919174194
Epoch 1290, training loss: 0.6444920301437378 = 0.0032974928617477417 + 0.1 * 6.41194486618042
Epoch 1290, val loss: 1.1921448707580566
Epoch 1300, training loss: 0.6422114372253418 = 0.003248486202210188 + 0.1 * 6.389628887176514
Epoch 1300, val loss: 1.1946351528167725
Epoch 1310, training loss: 0.6423326134681702 = 0.0032013074960559607 + 0.1 * 6.391313076019287
Epoch 1310, val loss: 1.1971415281295776
Epoch 1320, training loss: 0.6409916281700134 = 0.0031552419532090425 + 0.1 * 6.378363609313965
Epoch 1320, val loss: 1.1995943784713745
Epoch 1330, training loss: 0.6429444551467896 = 0.0031102823559194803 + 0.1 * 6.398341655731201
Epoch 1330, val loss: 1.202045202255249
Epoch 1340, training loss: 0.6416036486625671 = 0.0030666389502584934 + 0.1 * 6.385369777679443
Epoch 1340, val loss: 1.204451322555542
Epoch 1350, training loss: 0.6407824158668518 = 0.0030238875187933445 + 0.1 * 6.377585411071777
Epoch 1350, val loss: 1.2068387269973755
Epoch 1360, training loss: 0.641371488571167 = 0.0029822371434420347 + 0.1 * 6.383892059326172
Epoch 1360, val loss: 1.2091988325119019
Epoch 1370, training loss: 0.640860378742218 = 0.0029416314791888 + 0.1 * 6.37918758392334
Epoch 1370, val loss: 1.2115199565887451
Epoch 1380, training loss: 0.641140341758728 = 0.0029023990500718355 + 0.1 * 6.382379531860352
Epoch 1380, val loss: 1.213849425315857
Epoch 1390, training loss: 0.6409998536109924 = 0.002863777568563819 + 0.1 * 6.3813605308532715
Epoch 1390, val loss: 1.2161271572113037
Epoch 1400, training loss: 0.6406476497650146 = 0.0028259928803890944 + 0.1 * 6.378216743469238
Epoch 1400, val loss: 1.2184135913848877
Epoch 1410, training loss: 0.6400322914123535 = 0.0027891064528375864 + 0.1 * 6.372432231903076
Epoch 1410, val loss: 1.2206562757492065
Epoch 1420, training loss: 0.639510452747345 = 0.0027533569373190403 + 0.1 * 6.367570877075195
Epoch 1420, val loss: 1.2229163646697998
Epoch 1430, training loss: 0.6406864523887634 = 0.002718489384278655 + 0.1 * 6.379679203033447
Epoch 1430, val loss: 1.2251310348510742
Epoch 1440, training loss: 0.641321063041687 = 0.002683961298316717 + 0.1 * 6.38637113571167
Epoch 1440, val loss: 1.2273198366165161
Epoch 1450, training loss: 0.6394213438034058 = 0.002650410169735551 + 0.1 * 6.367708683013916
Epoch 1450, val loss: 1.229486107826233
Epoch 1460, training loss: 0.6403771638870239 = 0.002617787104099989 + 0.1 * 6.377593517303467
Epoch 1460, val loss: 1.2316206693649292
Epoch 1470, training loss: 0.6394825577735901 = 0.0025858604349195957 + 0.1 * 6.368966579437256
Epoch 1470, val loss: 1.2337828874588013
Epoch 1480, training loss: 0.6402919888496399 = 0.00255446950905025 + 0.1 * 6.37737512588501
Epoch 1480, val loss: 1.235882043838501
Epoch 1490, training loss: 0.6392065286636353 = 0.0025238581001758575 + 0.1 * 6.36682653427124
Epoch 1490, val loss: 1.2380023002624512
Epoch 1500, training loss: 0.6399171948432922 = 0.0024939957074820995 + 0.1 * 6.374232292175293
Epoch 1500, val loss: 1.2400797605514526
Epoch 1510, training loss: 0.6387459635734558 = 0.0024647070094943047 + 0.1 * 6.362812519073486
Epoch 1510, val loss: 1.2421660423278809
Epoch 1520, training loss: 0.6416706442832947 = 0.002435874892398715 + 0.1 * 6.392347812652588
Epoch 1520, val loss: 1.244239330291748
Epoch 1530, training loss: 0.6392413973808289 = 0.002407886553555727 + 0.1 * 6.368334770202637
Epoch 1530, val loss: 1.246200680732727
Epoch 1540, training loss: 0.6390069127082825 = 0.0023804365191608667 + 0.1 * 6.366264820098877
Epoch 1540, val loss: 1.2482260465621948
Epoch 1550, training loss: 0.6380873918533325 = 0.002353413961827755 + 0.1 * 6.357339382171631
Epoch 1550, val loss: 1.2501968145370483
Epoch 1560, training loss: 0.6390266418457031 = 0.002327040070667863 + 0.1 * 6.3669962882995605
Epoch 1560, val loss: 1.2521533966064453
Epoch 1570, training loss: 0.637783944606781 = 0.0023012186866253614 + 0.1 * 6.354826927185059
Epoch 1570, val loss: 1.254097580909729
Epoch 1580, training loss: 0.6404423713684082 = 0.0022759425919502974 + 0.1 * 6.381664276123047
Epoch 1580, val loss: 1.2560529708862305
Epoch 1590, training loss: 0.6374740600585938 = 0.0022510236594825983 + 0.1 * 6.352230072021484
Epoch 1590, val loss: 1.2579352855682373
Epoch 1600, training loss: 0.637688934803009 = 0.002226723125204444 + 0.1 * 6.354621887207031
Epoch 1600, val loss: 1.2598637342453003
Epoch 1610, training loss: 0.6385554075241089 = 0.002202956937253475 + 0.1 * 6.363524913787842
Epoch 1610, val loss: 1.2617379426956177
Epoch 1620, training loss: 0.6379573345184326 = 0.002179467584937811 + 0.1 * 6.357778549194336
Epoch 1620, val loss: 1.2635966539382935
Epoch 1630, training loss: 0.6373076438903809 = 0.002156528877094388 + 0.1 * 6.351510524749756
Epoch 1630, val loss: 1.2653982639312744
Epoch 1640, training loss: 0.6382910013198853 = 0.0021341131068766117 + 0.1 * 6.361568927764893
Epoch 1640, val loss: 1.2672392129898071
Epoch 1650, training loss: 0.6370234489440918 = 0.0021119280718266964 + 0.1 * 6.349115371704102
Epoch 1650, val loss: 1.2690643072128296
Epoch 1660, training loss: 0.6383122205734253 = 0.002090359339490533 + 0.1 * 6.362218856811523
Epoch 1660, val loss: 1.270881175994873
Epoch 1670, training loss: 0.6381108164787292 = 0.0020690206438302994 + 0.1 * 6.360418319702148
Epoch 1670, val loss: 1.2726385593414307
Epoch 1680, training loss: 0.6365252733230591 = 0.002048100810497999 + 0.1 * 6.344771862030029
Epoch 1680, val loss: 1.2744237184524536
Epoch 1690, training loss: 0.6378898620605469 = 0.002027735812589526 + 0.1 * 6.358621120452881
Epoch 1690, val loss: 1.2761956453323364
Epoch 1700, training loss: 0.6366186141967773 = 0.0020077007357031107 + 0.1 * 6.346108913421631
Epoch 1700, val loss: 1.2779440879821777
Epoch 1710, training loss: 0.6379497051239014 = 0.0019879762548953295 + 0.1 * 6.359617233276367
Epoch 1710, val loss: 1.2796592712402344
Epoch 1720, training loss: 0.6363521218299866 = 0.0019685160368680954 + 0.1 * 6.343836307525635
Epoch 1720, val loss: 1.2813469171524048
Epoch 1730, training loss: 0.6389046311378479 = 0.0019496494205668569 + 0.1 * 6.36954927444458
Epoch 1730, val loss: 1.283056616783142
Epoch 1740, training loss: 0.6368287801742554 = 0.001930887345224619 + 0.1 * 6.3489789962768555
Epoch 1740, val loss: 1.2847490310668945
Epoch 1750, training loss: 0.6370697021484375 = 0.0019125493708997965 + 0.1 * 6.351571559906006
Epoch 1750, val loss: 1.286388874053955
Epoch 1760, training loss: 0.6375420093536377 = 0.0018944564508274198 + 0.1 * 6.356475353240967
Epoch 1760, val loss: 1.2880494594573975
Epoch 1770, training loss: 0.636040449142456 = 0.0018767843721434474 + 0.1 * 6.341636657714844
Epoch 1770, val loss: 1.289663553237915
Epoch 1780, training loss: 0.6362763047218323 = 0.0018595237052068114 + 0.1 * 6.344167709350586
Epoch 1780, val loss: 1.2913204431533813
Epoch 1790, training loss: 0.6361382603645325 = 0.0018424191512167454 + 0.1 * 6.342957973480225
Epoch 1790, val loss: 1.2929463386535645
Epoch 1800, training loss: 0.6368544697761536 = 0.0018255027243867517 + 0.1 * 6.350289344787598
Epoch 1800, val loss: 1.2945839166641235
Epoch 1810, training loss: 0.6360288858413696 = 0.001808931352570653 + 0.1 * 6.342199325561523
Epoch 1810, val loss: 1.2961859703063965
Epoch 1820, training loss: 0.6367951035499573 = 0.0017927800072357059 + 0.1 * 6.35002326965332
Epoch 1820, val loss: 1.2978018522262573
Epoch 1830, training loss: 0.6356858611106873 = 0.0017766926903277636 + 0.1 * 6.3390913009643555
Epoch 1830, val loss: 1.2993910312652588
Epoch 1840, training loss: 0.6361693739891052 = 0.0017610377399250865 + 0.1 * 6.344083309173584
Epoch 1840, val loss: 1.3009952306747437
Epoch 1850, training loss: 0.6355009078979492 = 0.001745578134432435 + 0.1 * 6.337553024291992
Epoch 1850, val loss: 1.3025636672973633
Epoch 1860, training loss: 0.6358035802841187 = 0.0017303891945630312 + 0.1 * 6.340731620788574
Epoch 1860, val loss: 1.3041415214538574
Epoch 1870, training loss: 0.6357002854347229 = 0.0017154235392808914 + 0.1 * 6.339848518371582
Epoch 1870, val loss: 1.305680274963379
Epoch 1880, training loss: 0.6362802386283875 = 0.0017007802380248904 + 0.1 * 6.345794677734375
Epoch 1880, val loss: 1.3072377443313599
Epoch 1890, training loss: 0.6360036134719849 = 0.0016862652264535427 + 0.1 * 6.343173027038574
Epoch 1890, val loss: 1.30874502658844
Epoch 1900, training loss: 0.6351172924041748 = 0.001672109356150031 + 0.1 * 6.334451675415039
Epoch 1900, val loss: 1.3102858066558838
Epoch 1910, training loss: 0.6375428438186646 = 0.0016580696683377028 + 0.1 * 6.358847618103027
Epoch 1910, val loss: 1.3118083477020264
Epoch 1920, training loss: 0.6349066495895386 = 0.0016443507047370076 + 0.1 * 6.332622528076172
Epoch 1920, val loss: 1.3132737874984741
Epoch 1930, training loss: 0.6360540986061096 = 0.0016309006605297327 + 0.1 * 6.344231605529785
Epoch 1930, val loss: 1.3148092031478882
Epoch 1940, training loss: 0.6357858180999756 = 0.0016174677293747663 + 0.1 * 6.341683387756348
Epoch 1940, val loss: 1.3163107633590698
Epoch 1950, training loss: 0.6343283653259277 = 0.0016043628565967083 + 0.1 * 6.327239990234375
Epoch 1950, val loss: 1.3177852630615234
Epoch 1960, training loss: 0.6355374455451965 = 0.001591552747413516 + 0.1 * 6.339458465576172
Epoch 1960, val loss: 1.3192894458770752
Epoch 1970, training loss: 0.6347337365150452 = 0.0015787150477990508 + 0.1 * 6.331550121307373
Epoch 1970, val loss: 1.320784568786621
Epoch 1980, training loss: 0.6348132491111755 = 0.0015661687357351184 + 0.1 * 6.332470893859863
Epoch 1980, val loss: 1.3222119808197021
Epoch 1990, training loss: 0.6354367733001709 = 0.0015538521111011505 + 0.1 * 6.338829517364502
Epoch 1990, val loss: 1.323699712753296
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8112809699525567
The final CL Acc:0.78642, 0.01944, The final GNN Acc:0.81023, 0.00114
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13212])
remove edge: torch.Size([2, 7906])
updated graph: torch.Size([2, 10562])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8156776428222656 = 1.9559919834136963 + 0.1 * 8.596856117248535
Epoch 0, val loss: 1.9613685607910156
Epoch 10, training loss: 2.8057267665863037 = 1.946051001548767 + 0.1 * 8.596756935119629
Epoch 10, val loss: 1.9516152143478394
Epoch 20, training loss: 2.793092966079712 = 1.9334757328033447 + 0.1 * 8.596171379089355
Epoch 20, val loss: 1.9388095140457153
Epoch 30, training loss: 2.7744054794311523 = 1.915330410003662 + 0.1 * 8.590749740600586
Epoch 30, val loss: 1.9200353622436523
Epoch 40, training loss: 2.74245548248291 = 1.887814998626709 + 0.1 * 8.546405792236328
Epoch 40, val loss: 1.891726016998291
Epoch 50, training loss: 2.6763558387756348 = 1.8493915796279907 + 0.1 * 8.269643783569336
Epoch 50, val loss: 1.8538811206817627
Epoch 60, training loss: 2.5965707302093506 = 1.8070197105407715 + 0.1 * 7.895509243011475
Epoch 60, val loss: 1.8142722845077515
Epoch 70, training loss: 2.5194592475891113 = 1.7697930335998535 + 0.1 * 7.496662139892578
Epoch 70, val loss: 1.780647873878479
Epoch 80, training loss: 2.4483087062835693 = 1.7333109378814697 + 0.1 * 7.149978160858154
Epoch 80, val loss: 1.7464722394943237
Epoch 90, training loss: 2.3848912715911865 = 1.6866861581802368 + 0.1 * 6.982050895690918
Epoch 90, val loss: 1.7037887573242188
Epoch 100, training loss: 2.3158445358276367 = 1.6242318153381348 + 0.1 * 6.916126728057861
Epoch 100, val loss: 1.648919701576233
Epoch 110, training loss: 2.2339987754821777 = 1.5448684692382812 + 0.1 * 6.891303062438965
Epoch 110, val loss: 1.5800162553787231
Epoch 120, training loss: 2.1427125930786133 = 1.4550379514694214 + 0.1 * 6.876745223999023
Epoch 120, val loss: 1.503847599029541
Epoch 130, training loss: 2.049541711807251 = 1.362730622291565 + 0.1 * 6.868110656738281
Epoch 130, val loss: 1.4273568391799927
Epoch 140, training loss: 1.9581561088562012 = 1.2717335224151611 + 0.1 * 6.8642258644104
Epoch 140, val loss: 1.3524399995803833
Epoch 150, training loss: 1.8679332733154297 = 1.1818643808364868 + 0.1 * 6.860688209533691
Epoch 150, val loss: 1.2790167331695557
Epoch 160, training loss: 1.7780630588531494 = 1.0924519300460815 + 0.1 * 6.856110572814941
Epoch 160, val loss: 1.2064590454101562
Epoch 170, training loss: 1.6888911724090576 = 1.0039174556732178 + 0.1 * 6.849736213684082
Epoch 170, val loss: 1.134533405303955
Epoch 180, training loss: 1.6023859977722168 = 0.9181926846504211 + 0.1 * 6.841933727264404
Epoch 180, val loss: 1.0654898881912231
Epoch 190, training loss: 1.5199896097183228 = 0.8368203639984131 + 0.1 * 6.831692218780518
Epoch 190, val loss: 1.0003423690795898
Epoch 200, training loss: 1.442988395690918 = 0.7611970901489258 + 0.1 * 6.81791353225708
Epoch 200, val loss: 0.9406517744064331
Epoch 210, training loss: 1.3738524913787842 = 0.6924237608909607 + 0.1 * 6.8142876625061035
Epoch 210, val loss: 0.888187825679779
Epoch 220, training loss: 1.3111910820007324 = 0.6318652033805847 + 0.1 * 6.7932586669921875
Epoch 220, val loss: 0.8447662591934204
Epoch 230, training loss: 1.256174921989441 = 0.5779337286949158 + 0.1 * 6.782411575317383
Epoch 230, val loss: 0.8093221783638
Epoch 240, training loss: 1.2066028118133545 = 0.5292052626609802 + 0.1 * 6.773975372314453
Epoch 240, val loss: 0.7805201411247253
Epoch 250, training loss: 1.161124587059021 = 0.48447102308273315 + 0.1 * 6.76653528213501
Epoch 250, val loss: 0.7567363381385803
Epoch 260, training loss: 1.1186715364456177 = 0.4426840841770172 + 0.1 * 6.759873867034912
Epoch 260, val loss: 0.7359830737113953
Epoch 270, training loss: 1.0774632692337036 = 0.40213343501091003 + 0.1 * 6.753298759460449
Epoch 270, val loss: 0.71672523021698
Epoch 280, training loss: 1.0372810363769531 = 0.361894428730011 + 0.1 * 6.753865718841553
Epoch 280, val loss: 0.6978794932365417
Epoch 290, training loss: 0.9958898425102234 = 0.32214653491973877 + 0.1 * 6.737432956695557
Epoch 290, val loss: 0.6794279217720032
Epoch 300, training loss: 0.9578295350074768 = 0.2835043668746948 + 0.1 * 6.743251323699951
Epoch 300, val loss: 0.661698043346405
Epoch 310, training loss: 0.9199914932250977 = 0.2475108653306961 + 0.1 * 6.724806308746338
Epoch 310, val loss: 0.6456036567687988
Epoch 320, training loss: 0.8870540857315063 = 0.21518269181251526 + 0.1 * 6.718713283538818
Epoch 320, val loss: 0.6321344971656799
Epoch 330, training loss: 0.8596059679985046 = 0.1873195767402649 + 0.1 * 6.722863674163818
Epoch 330, val loss: 0.6219057440757751
Epoch 340, training loss: 0.8343154191970825 = 0.1638713777065277 + 0.1 * 6.704440593719482
Epoch 340, val loss: 0.6149166822433472
Epoch 350, training loss: 0.8144433498382568 = 0.14411912858486176 + 0.1 * 6.70324182510376
Epoch 350, val loss: 0.610755205154419
Epoch 360, training loss: 0.7970478534698486 = 0.12748153507709503 + 0.1 * 6.6956634521484375
Epoch 360, val loss: 0.6089518666267395
Epoch 370, training loss: 0.7819081544876099 = 0.11332038044929504 + 0.1 * 6.685877799987793
Epoch 370, val loss: 0.6089727282524109
Epoch 380, training loss: 0.7690945863723755 = 0.10112746059894562 + 0.1 * 6.679671287536621
Epoch 380, val loss: 0.6104490160942078
Epoch 390, training loss: 0.7587829828262329 = 0.0905570536851883 + 0.1 * 6.6822590827941895
Epoch 390, val loss: 0.6131109595298767
Epoch 400, training loss: 0.7489680647850037 = 0.08142710477113724 + 0.1 * 6.67540979385376
Epoch 400, val loss: 0.6165117025375366
Epoch 410, training loss: 0.7401134967803955 = 0.0734693706035614 + 0.1 * 6.666440963745117
Epoch 410, val loss: 0.620559811592102
Epoch 420, training loss: 0.7321133613586426 = 0.06650323420763016 + 0.1 * 6.656101226806641
Epoch 420, val loss: 0.6251522898674011
Epoch 430, training loss: 0.7253566384315491 = 0.06040465086698532 + 0.1 * 6.649519920349121
Epoch 430, val loss: 0.629982054233551
Epoch 440, training loss: 0.7204375267028809 = 0.05502216890454292 + 0.1 * 6.654153347015381
Epoch 440, val loss: 0.6351795196533203
Epoch 450, training loss: 0.7140231132507324 = 0.05028790235519409 + 0.1 * 6.637351989746094
Epoch 450, val loss: 0.6405488848686218
Epoch 460, training loss: 0.7097048163414001 = 0.04610278084874153 + 0.1 * 6.636020660400391
Epoch 460, val loss: 0.646012008190155
Epoch 470, training loss: 0.7048783898353577 = 0.042407650500535965 + 0.1 * 6.624707221984863
Epoch 470, val loss: 0.6515361666679382
Epoch 480, training loss: 0.7006326913833618 = 0.03913344442844391 + 0.1 * 6.614992618560791
Epoch 480, val loss: 0.6569709777832031
Epoch 490, training loss: 0.6976714134216309 = 0.03621085733175278 + 0.1 * 6.61460542678833
Epoch 490, val loss: 0.6625468134880066
Epoch 500, training loss: 0.6941726803779602 = 0.03359735384583473 + 0.1 * 6.605752944946289
Epoch 500, val loss: 0.6680710911750793
Epoch 510, training loss: 0.692001223564148 = 0.03125830739736557 + 0.1 * 6.607429504394531
Epoch 510, val loss: 0.673599898815155
Epoch 520, training loss: 0.6884111762046814 = 0.02915980853140354 + 0.1 * 6.592513561248779
Epoch 520, val loss: 0.6789430975914001
Epoch 530, training loss: 0.685867190361023 = 0.027270527556538582 + 0.1 * 6.585966110229492
Epoch 530, val loss: 0.6842659115791321
Epoch 540, training loss: 0.684154212474823 = 0.02556733787059784 + 0.1 * 6.5858683586120605
Epoch 540, val loss: 0.6894835233688354
Epoch 550, training loss: 0.6815857887268066 = 0.024023788049817085 + 0.1 * 6.575620174407959
Epoch 550, val loss: 0.6945576667785645
Epoch 560, training loss: 0.6818196177482605 = 0.02261698618531227 + 0.1 * 6.592026233673096
Epoch 560, val loss: 0.6996201276779175
Epoch 570, training loss: 0.6789007782936096 = 0.02133931592106819 + 0.1 * 6.575614929199219
Epoch 570, val loss: 0.7045430541038513
Epoch 580, training loss: 0.6763856410980225 = 0.02017376571893692 + 0.1 * 6.5621185302734375
Epoch 580, val loss: 0.7093073129653931
Epoch 590, training loss: 0.6746689081192017 = 0.019101228564977646 + 0.1 * 6.5556769371032715
Epoch 590, val loss: 0.7140680551528931
Epoch 600, training loss: 0.6743951439857483 = 0.018114594742655754 + 0.1 * 6.562805652618408
Epoch 600, val loss: 0.7187842726707458
Epoch 610, training loss: 0.6744797825813293 = 0.017210230231285095 + 0.1 * 6.572695255279541
Epoch 610, val loss: 0.7233093976974487
Epoch 620, training loss: 0.6707769632339478 = 0.01638098433613777 + 0.1 * 6.543960094451904
Epoch 620, val loss: 0.7276704907417297
Epoch 630, training loss: 0.66921067237854 = 0.015613136813044548 + 0.1 * 6.535975456237793
Epoch 630, val loss: 0.7320058345794678
Epoch 640, training loss: 0.6690017580986023 = 0.014898590743541718 + 0.1 * 6.541031360626221
Epoch 640, val loss: 0.7362807989120483
Epoch 650, training loss: 0.6697989702224731 = 0.014236606657505035 + 0.1 * 6.555623531341553
Epoch 650, val loss: 0.7404745221138
Epoch 660, training loss: 0.666442334651947 = 0.013623295351862907 + 0.1 * 6.5281901359558105
Epoch 660, val loss: 0.7444995045661926
Epoch 670, training loss: 0.6653221249580383 = 0.013051524758338928 + 0.1 * 6.522706031799316
Epoch 670, val loss: 0.748408854007721
Epoch 680, training loss: 0.664473831653595 = 0.012515692040324211 + 0.1 * 6.5195817947387695
Epoch 680, val loss: 0.7523292899131775
Epoch 690, training loss: 0.6634384989738464 = 0.01201443001627922 + 0.1 * 6.514240264892578
Epoch 690, val loss: 0.7561450004577637
Epoch 700, training loss: 0.6622591018676758 = 0.011545456014573574 + 0.1 * 6.507136344909668
Epoch 700, val loss: 0.7598839998245239
Epoch 710, training loss: 0.661986231803894 = 0.011105012148618698 + 0.1 * 6.508811950683594
Epoch 710, val loss: 0.7635939121246338
Epoch 720, training loss: 0.6621343493461609 = 0.010691876523196697 + 0.1 * 6.514424800872803
Epoch 720, val loss: 0.7672181725502014
Epoch 730, training loss: 0.6611753106117249 = 0.0103036779910326 + 0.1 * 6.508716106414795
Epoch 730, val loss: 0.7707051038742065
Epoch 740, training loss: 0.6593068838119507 = 0.009937933646142483 + 0.1 * 6.49368953704834
Epoch 740, val loss: 0.7742406725883484
Epoch 750, training loss: 0.66035395860672 = 0.009593063034117222 + 0.1 * 6.507608890533447
Epoch 750, val loss: 0.7776495814323425
Epoch 760, training loss: 0.6591227650642395 = 0.009267288260161877 + 0.1 * 6.498554706573486
Epoch 760, val loss: 0.7810580730438232
Epoch 770, training loss: 0.6583735942840576 = 0.008960269391536713 + 0.1 * 6.494133472442627
Epoch 770, val loss: 0.7843528985977173
Epoch 780, training loss: 0.6576870083808899 = 0.008670282550156116 + 0.1 * 6.490167140960693
Epoch 780, val loss: 0.7875459790229797
Epoch 790, training loss: 0.6577088832855225 = 0.008394806645810604 + 0.1 * 6.493140697479248
Epoch 790, val loss: 0.7907440662384033
Epoch 800, training loss: 0.6562387943267822 = 0.008133173920214176 + 0.1 * 6.481056213378906
Epoch 800, val loss: 0.7939250469207764
Epoch 810, training loss: 0.6567925810813904 = 0.007884891703724861 + 0.1 * 6.489076614379883
Epoch 810, val loss: 0.7969826459884644
Epoch 820, training loss: 0.6554093360900879 = 0.0076495748944580555 + 0.1 * 6.477597236633301
Epoch 820, val loss: 0.8000032305717468
Epoch 830, training loss: 0.6574438810348511 = 0.007425442337989807 + 0.1 * 6.500184535980225
Epoch 830, val loss: 0.802975594997406
Epoch 840, training loss: 0.6553382277488708 = 0.007212770637124777 + 0.1 * 6.481254577636719
Epoch 840, val loss: 0.8058921098709106
Epoch 850, training loss: 0.655135989189148 = 0.007010188885033131 + 0.1 * 6.481258392333984
Epoch 850, val loss: 0.8087214231491089
Epoch 860, training loss: 0.654326856136322 = 0.0068167163990437984 + 0.1 * 6.475100994110107
Epoch 860, val loss: 0.8115969896316528
Epoch 870, training loss: 0.6529917120933533 = 0.006632511503994465 + 0.1 * 6.463592052459717
Epoch 870, val loss: 0.8142972588539124
Epoch 880, training loss: 0.6546643972396851 = 0.006456589791923761 + 0.1 * 6.482077598571777
Epoch 880, val loss: 0.8170464634895325
Epoch 890, training loss: 0.652831494808197 = 0.006288130767643452 + 0.1 * 6.465433597564697
Epoch 890, val loss: 0.81971275806427
Epoch 900, training loss: 0.652564525604248 = 0.006127465981990099 + 0.1 * 6.464370250701904
Epoch 900, val loss: 0.8223423957824707
Epoch 910, training loss: 0.651728630065918 = 0.00597413768991828 + 0.1 * 6.457544803619385
Epoch 910, val loss: 0.8249086737632751
Epoch 920, training loss: 0.6522102952003479 = 0.005826775915920734 + 0.1 * 6.4638352394104
Epoch 920, val loss: 0.8274650573730469
Epoch 930, training loss: 0.653870165348053 = 0.005685210693627596 + 0.1 * 6.481849193572998
Epoch 930, val loss: 0.8299440145492554
Epoch 940, training loss: 0.6501339673995972 = 0.0055500478483736515 + 0.1 * 6.4458394050598145
Epoch 940, val loss: 0.8324241042137146
Epoch 950, training loss: 0.6504314541816711 = 0.005420524626970291 + 0.1 * 6.450109481811523
Epoch 950, val loss: 0.8348178267478943
Epoch 960, training loss: 0.6498847603797913 = 0.005295951850712299 + 0.1 * 6.445888042449951
Epoch 960, val loss: 0.8372263312339783
Epoch 970, training loss: 0.649742603302002 = 0.005176276434212923 + 0.1 * 6.445662975311279
Epoch 970, val loss: 0.8395936489105225
Epoch 980, training loss: 0.6492231488227844 = 0.005061034113168716 + 0.1 * 6.441620826721191
Epoch 980, val loss: 0.8418827652931213
Epoch 990, training loss: 0.6483302116394043 = 0.004950371105223894 + 0.1 * 6.433798313140869
Epoch 990, val loss: 0.8441389799118042
Epoch 1000, training loss: 0.650834858417511 = 0.004843612667173147 + 0.1 * 6.459912300109863
Epoch 1000, val loss: 0.8464275002479553
Epoch 1010, training loss: 0.6485050916671753 = 0.0047408705577254295 + 0.1 * 6.4376420974731445
Epoch 1010, val loss: 0.8486441373825073
Epoch 1020, training loss: 0.6482731699943542 = 0.004642306827008724 + 0.1 * 6.43630838394165
Epoch 1020, val loss: 0.850822925567627
Epoch 1030, training loss: 0.6479613184928894 = 0.004547035787254572 + 0.1 * 6.434142589569092
Epoch 1030, val loss: 0.8530406951904297
Epoch 1040, training loss: 0.6504029035568237 = 0.004454816225916147 + 0.1 * 6.459481239318848
Epoch 1040, val loss: 0.8551572561264038
Epoch 1050, training loss: 0.6469703912734985 = 0.004365907050669193 + 0.1 * 6.426044464111328
Epoch 1050, val loss: 0.8572514653205872
Epoch 1060, training loss: 0.6469953656196594 = 0.004280006978660822 + 0.1 * 6.42715311050415
Epoch 1060, val loss: 0.8593116998672485
Epoch 1070, training loss: 0.6474782824516296 = 0.004196637775748968 + 0.1 * 6.432816028594971
Epoch 1070, val loss: 0.8614171147346497
Epoch 1080, training loss: 0.6450114846229553 = 0.004116517957299948 + 0.1 * 6.408949375152588
Epoch 1080, val loss: 0.8634545803070068
Epoch 1090, training loss: 0.6466555595397949 = 0.004038147162646055 + 0.1 * 6.426173686981201
Epoch 1090, val loss: 0.8654266595840454
Epoch 1100, training loss: 0.6463183164596558 = 0.003962683025747538 + 0.1 * 6.423555850982666
Epoch 1100, val loss: 0.8674641251564026
Epoch 1110, training loss: 0.6447417736053467 = 0.0038894307799637318 + 0.1 * 6.408523082733154
Epoch 1110, val loss: 0.8693984150886536
Epoch 1120, training loss: 0.6482242941856384 = 0.003818635828793049 + 0.1 * 6.444056510925293
Epoch 1120, val loss: 0.8713832497596741
Epoch 1130, training loss: 0.6448155641555786 = 0.0037502185441553593 + 0.1 * 6.410653591156006
Epoch 1130, val loss: 0.8733017444610596
Epoch 1140, training loss: 0.6453009843826294 = 0.0036840050015598536 + 0.1 * 6.4161696434021
Epoch 1140, val loss: 0.875201940536499
Epoch 1150, training loss: 0.6442995667457581 = 0.003619819413870573 + 0.1 * 6.406797885894775
Epoch 1150, val loss: 0.8770148754119873
Epoch 1160, training loss: 0.6441672444343567 = 0.0035573740024119616 + 0.1 * 6.406098365783691
Epoch 1160, val loss: 0.8788729906082153
Epoch 1170, training loss: 0.6436492204666138 = 0.0034970641136169434 + 0.1 * 6.4015212059021
Epoch 1170, val loss: 0.8807132244110107
Epoch 1180, training loss: 0.6441900134086609 = 0.0034384941682219505 + 0.1 * 6.407515048980713
Epoch 1180, val loss: 0.8825168013572693
Epoch 1190, training loss: 0.644041121006012 = 0.003381061600521207 + 0.1 * 6.406600475311279
Epoch 1190, val loss: 0.8843064904212952
Epoch 1200, training loss: 0.6450952291488647 = 0.003326835110783577 + 0.1 * 6.4176836013793945
Epoch 1200, val loss: 0.8861027956008911
Epoch 1210, training loss: 0.6423841714859009 = 0.003273377427831292 + 0.1 * 6.39110803604126
Epoch 1210, val loss: 0.8878371119499207
Epoch 1220, training loss: 0.643409013748169 = 0.0032199001871049404 + 0.1 * 6.401890754699707
Epoch 1220, val loss: 0.8895110487937927
Epoch 1230, training loss: 0.6426236033439636 = 0.0031700183171778917 + 0.1 * 6.394535541534424
Epoch 1230, val loss: 0.8912506699562073
Epoch 1240, training loss: 0.6433312296867371 = 0.00312074925750494 + 0.1 * 6.40210485458374
Epoch 1240, val loss: 0.8929576873779297
Epoch 1250, training loss: 0.642786979675293 = 0.00307225133292377 + 0.1 * 6.397147178649902
Epoch 1250, val loss: 0.8946108222007751
Epoch 1260, training loss: 0.6424354314804077 = 0.0030259781051427126 + 0.1 * 6.394094944000244
Epoch 1260, val loss: 0.8962885737419128
Epoch 1270, training loss: 0.6415613889694214 = 0.0029808448161929846 + 0.1 * 6.385805606842041
Epoch 1270, val loss: 0.8978877067565918
Epoch 1280, training loss: 0.6431993246078491 = 0.002936780918389559 + 0.1 * 6.40262508392334
Epoch 1280, val loss: 0.8994817137718201
Epoch 1290, training loss: 0.6426911354064941 = 0.0028936699964106083 + 0.1 * 6.397974491119385
Epoch 1290, val loss: 0.9010843634605408
Epoch 1300, training loss: 0.6413811445236206 = 0.0028521583881229162 + 0.1 * 6.385289669036865
Epoch 1300, val loss: 0.9026636481285095
Epoch 1310, training loss: 0.6410170793533325 = 0.0028113634325563908 + 0.1 * 6.382057189941406
Epoch 1310, val loss: 0.9041893482208252
Epoch 1320, training loss: 0.6404872536659241 = 0.0027719370555132627 + 0.1 * 6.377153396606445
Epoch 1320, val loss: 0.905752956867218
Epoch 1330, training loss: 0.6415517330169678 = 0.0027327516581863165 + 0.1 * 6.388189792633057
Epoch 1330, val loss: 0.9072698354721069
Epoch 1340, training loss: 0.6415551900863647 = 0.0026954039931297302 + 0.1 * 6.3885979652404785
Epoch 1340, val loss: 0.9088035225868225
Epoch 1350, training loss: 0.640593945980072 = 0.0026588106993585825 + 0.1 * 6.379351615905762
Epoch 1350, val loss: 0.9102941155433655
Epoch 1360, training loss: 0.6408932209014893 = 0.002623056061565876 + 0.1 * 6.382701396942139
Epoch 1360, val loss: 0.9117752909660339
Epoch 1370, training loss: 0.6400771141052246 = 0.0025881812907755375 + 0.1 * 6.374889373779297
Epoch 1370, val loss: 0.9132283926010132
Epoch 1380, training loss: 0.6407414674758911 = 0.0025541316717863083 + 0.1 * 6.38187313079834
Epoch 1380, val loss: 0.9146642684936523
Epoch 1390, training loss: 0.6396183967590332 = 0.002520943759009242 + 0.1 * 6.370974540710449
Epoch 1390, val loss: 0.9160608053207397
Epoch 1400, training loss: 0.6414137482643127 = 0.002488583093509078 + 0.1 * 6.389251708984375
Epoch 1400, val loss: 0.9174734354019165
Epoch 1410, training loss: 0.6393068432807922 = 0.002457043621689081 + 0.1 * 6.368497848510742
Epoch 1410, val loss: 0.9188825488090515
Epoch 1420, training loss: 0.6394233107566833 = 0.002426393097266555 + 0.1 * 6.369969367980957
Epoch 1420, val loss: 0.9202796220779419
Epoch 1430, training loss: 0.6396353244781494 = 0.002395629184320569 + 0.1 * 6.372396945953369
Epoch 1430, val loss: 0.9216929078102112
Epoch 1440, training loss: 0.6398217678070068 = 0.00236616306938231 + 0.1 * 6.374555587768555
Epoch 1440, val loss: 0.9231082797050476
Epoch 1450, training loss: 0.6397861838340759 = 0.0023377046454697847 + 0.1 * 6.374485015869141
Epoch 1450, val loss: 0.9244737029075623
Epoch 1460, training loss: 0.6393565535545349 = 0.0023094634525477886 + 0.1 * 6.370471000671387
Epoch 1460, val loss: 0.9258036017417908
Epoch 1470, training loss: 0.6390517950057983 = 0.0022818308789283037 + 0.1 * 6.36769962310791
Epoch 1470, val loss: 0.9271324872970581
Epoch 1480, training loss: 0.638678252696991 = 0.0022549594286829233 + 0.1 * 6.364233016967773
Epoch 1480, val loss: 0.9284540414810181
Epoch 1490, training loss: 0.6404902935028076 = 0.002228413475677371 + 0.1 * 6.382618427276611
Epoch 1490, val loss: 0.9298079013824463
Epoch 1500, training loss: 0.6394912600517273 = 0.0022027764935046434 + 0.1 * 6.372885227203369
Epoch 1500, val loss: 0.9311268329620361
Epoch 1510, training loss: 0.6377984285354614 = 0.0021776745561510324 + 0.1 * 6.356207370758057
Epoch 1510, val loss: 0.9324207901954651
Epoch 1520, training loss: 0.6398898363113403 = 0.002152995904907584 + 0.1 * 6.377368450164795
Epoch 1520, val loss: 0.933655321598053
Epoch 1530, training loss: 0.6386123299598694 = 0.002128653461113572 + 0.1 * 6.364836692810059
Epoch 1530, val loss: 0.9349740147590637
Epoch 1540, training loss: 0.6391130089759827 = 0.002105060964822769 + 0.1 * 6.370079517364502
Epoch 1540, val loss: 0.9362680912017822
Epoch 1550, training loss: 0.6384130120277405 = 0.0020820305217057467 + 0.1 * 6.363309383392334
Epoch 1550, val loss: 0.9375582933425903
Epoch 1560, training loss: 0.6375596523284912 = 0.0020594352390617132 + 0.1 * 6.355001926422119
Epoch 1560, val loss: 0.9388149976730347
Epoch 1570, training loss: 0.6386709809303284 = 0.0020370667334645987 + 0.1 * 6.366339206695557
Epoch 1570, val loss: 0.9400617480278015
Epoch 1580, training loss: 0.6378268003463745 = 0.002015083096921444 + 0.1 * 6.35811710357666
Epoch 1580, val loss: 0.9412944316864014
Epoch 1590, training loss: 0.639045000076294 = 0.0019940331112593412 + 0.1 * 6.370509624481201
Epoch 1590, val loss: 0.9425591230392456
Epoch 1600, training loss: 0.6378399133682251 = 0.001973110716789961 + 0.1 * 6.358667850494385
Epoch 1600, val loss: 0.9437517523765564
Epoch 1610, training loss: 0.63718581199646 = 0.001952510210685432 + 0.1 * 6.352332592010498
Epoch 1610, val loss: 0.944962739944458
Epoch 1620, training loss: 0.6373344659805298 = 0.0019323810702189803 + 0.1 * 6.354020595550537
Epoch 1620, val loss: 0.9461680054664612
Epoch 1630, training loss: 0.6379713416099548 = 0.0019126017577946186 + 0.1 * 6.360587120056152
Epoch 1630, val loss: 0.9474238753318787
Epoch 1640, training loss: 0.6385305523872375 = 0.001893303357064724 + 0.1 * 6.366372585296631
Epoch 1640, val loss: 0.9486330151557922
Epoch 1650, training loss: 0.6370418667793274 = 0.0018742328975349665 + 0.1 * 6.3516764640808105
Epoch 1650, val loss: 0.9498333930969238
Epoch 1660, training loss: 0.6375554800033569 = 0.0018558786250650883 + 0.1 * 6.356996059417725
Epoch 1660, val loss: 0.9509493112564087
Epoch 1670, training loss: 0.6365525722503662 = 0.0018375306390225887 + 0.1 * 6.347149848937988
Epoch 1670, val loss: 0.9521103501319885
Epoch 1680, training loss: 0.6379886865615845 = 0.0018196601886302233 + 0.1 * 6.361690044403076
Epoch 1680, val loss: 0.9532570242881775
Epoch 1690, training loss: 0.6366903185844421 = 0.0018020663410425186 + 0.1 * 6.34888219833374
Epoch 1690, val loss: 0.9543697834014893
Epoch 1700, training loss: 0.63718181848526 = 0.0017848049756139517 + 0.1 * 6.353970050811768
Epoch 1700, val loss: 0.9555319547653198
Epoch 1710, training loss: 0.6368765234947205 = 0.0017681398894637823 + 0.1 * 6.351084232330322
Epoch 1710, val loss: 0.9566391706466675
Epoch 1720, training loss: 0.6355640888214111 = 0.0017514940118417144 + 0.1 * 6.338126182556152
Epoch 1720, val loss: 0.9577409625053406
Epoch 1730, training loss: 0.6372807025909424 = 0.0017350922571495175 + 0.1 * 6.3554558753967285
Epoch 1730, val loss: 0.9588494300842285
Epoch 1740, training loss: 0.635799765586853 = 0.0017189661739394069 + 0.1 * 6.340807914733887
Epoch 1740, val loss: 0.9599965214729309
Epoch 1750, training loss: 0.6364679932594299 = 0.001703401911072433 + 0.1 * 6.347646236419678
Epoch 1750, val loss: 0.9610777497291565
Epoch 1760, training loss: 0.6363862156867981 = 0.0016880580224096775 + 0.1 * 6.346981525421143
Epoch 1760, val loss: 0.9622218608856201
Epoch 1770, training loss: 0.6354732513427734 = 0.00167280959431082 + 0.1 * 6.3380045890808105
Epoch 1770, val loss: 0.9632591009140015
Epoch 1780, training loss: 0.6362634301185608 = 0.0016578625654801726 + 0.1 * 6.346055507659912
Epoch 1780, val loss: 0.9643456339836121
Epoch 1790, training loss: 0.6370470523834229 = 0.001643397263251245 + 0.1 * 6.354036331176758
Epoch 1790, val loss: 0.9653968811035156
Epoch 1800, training loss: 0.6358031630516052 = 0.001628475496545434 + 0.1 * 6.341746807098389
Epoch 1800, val loss: 0.9664989113807678
Epoch 1810, training loss: 0.6354620456695557 = 0.0016146863345056772 + 0.1 * 6.338473320007324
Epoch 1810, val loss: 0.9675617218017578
Epoch 1820, training loss: 0.6360745429992676 = 0.0016005586367100477 + 0.1 * 6.3447394371032715
Epoch 1820, val loss: 0.9685960412025452
Epoch 1830, training loss: 0.6355035305023193 = 0.0015870511997491121 + 0.1 * 6.339164733886719
Epoch 1830, val loss: 0.9696431756019592
Epoch 1840, training loss: 0.6355171203613281 = 0.0015734819462522864 + 0.1 * 6.339436054229736
Epoch 1840, val loss: 0.9706945419311523
Epoch 1850, training loss: 0.6355946063995361 = 0.0015602409839630127 + 0.1 * 6.340343475341797
Epoch 1850, val loss: 0.97173011302948
Epoch 1860, training loss: 0.636012852191925 = 0.001547207124531269 + 0.1 * 6.344655990600586
Epoch 1860, val loss: 0.9727740287780762
Epoch 1870, training loss: 0.6348190903663635 = 0.001534368610009551 + 0.1 * 6.332847595214844
Epoch 1870, val loss: 0.9738243818283081
Epoch 1880, training loss: 0.6349273920059204 = 0.0015217678155750036 + 0.1 * 6.334056377410889
Epoch 1880, val loss: 0.9748181104660034
Epoch 1890, training loss: 0.6356637477874756 = 0.001509270048700273 + 0.1 * 6.341545104980469
Epoch 1890, val loss: 0.9758366346359253
Epoch 1900, training loss: 0.6359907984733582 = 0.001497075892984867 + 0.1 * 6.344936847686768
Epoch 1900, val loss: 0.9768611192703247
Epoch 1910, training loss: 0.6341093182563782 = 0.001485052634961903 + 0.1 * 6.326242446899414
Epoch 1910, val loss: 0.9778806567192078
Epoch 1920, training loss: 0.635683000087738 = 0.0014732026029378176 + 0.1 * 6.342098236083984
Epoch 1920, val loss: 0.9788783192634583
Epoch 1930, training loss: 0.6347330808639526 = 0.0014614596730098128 + 0.1 * 6.332716464996338
Epoch 1930, val loss: 0.9798848628997803
Epoch 1940, training loss: 0.6344900727272034 = 0.0014500985853374004 + 0.1 * 6.330399513244629
Epoch 1940, val loss: 0.98087078332901
Epoch 1950, training loss: 0.6349758505821228 = 0.0014387359842658043 + 0.1 * 6.335371017456055
Epoch 1950, val loss: 0.9818763732910156
Epoch 1960, training loss: 0.6344910860061646 = 0.0014276171568781137 + 0.1 * 6.330634593963623
Epoch 1960, val loss: 0.9828703999519348
Epoch 1970, training loss: 0.6357477307319641 = 0.0014165726024657488 + 0.1 * 6.343311309814453
Epoch 1970, val loss: 0.9838730692863464
Epoch 1980, training loss: 0.6346054673194885 = 0.0014058203669264913 + 0.1 * 6.331996440887451
Epoch 1980, val loss: 0.9848255515098572
Epoch 1990, training loss: 0.6343543529510498 = 0.0013951541623100638 + 0.1 * 6.329591751098633
Epoch 1990, val loss: 0.9858433604240417
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.8146233558654785 = 1.9549391269683838 + 0.1 * 8.596842765808105
Epoch 0, val loss: 1.9586482048034668
Epoch 10, training loss: 2.804744243621826 = 1.9450665712356567 + 0.1 * 8.596776008605957
Epoch 10, val loss: 1.9491019248962402
Epoch 20, training loss: 2.7932920455932617 = 1.9336587190628052 + 0.1 * 8.596334457397461
Epoch 20, val loss: 1.9373784065246582
Epoch 30, training loss: 2.7777771949768066 = 1.9185166358947754 + 0.1 * 8.592606544494629
Epoch 30, val loss: 1.9213415384292603
Epoch 40, training loss: 2.753358840942383 = 1.8967828750610352 + 0.1 * 8.565759658813477
Epoch 40, val loss: 1.8980463743209839
Epoch 50, training loss: 2.709038257598877 = 1.8655201196670532 + 0.1 * 8.435182571411133
Epoch 50, val loss: 1.8652044534683228
Epoch 60, training loss: 2.624931573867798 = 1.825857162475586 + 0.1 * 7.990743637084961
Epoch 60, val loss: 1.8253650665283203
Epoch 70, training loss: 2.560349941253662 = 1.7837154865264893 + 0.1 * 7.766343116760254
Epoch 70, val loss: 1.7855596542358398
Epoch 80, training loss: 2.4885008335113525 = 1.7434775829315186 + 0.1 * 7.450232028961182
Epoch 80, val loss: 1.748397946357727
Epoch 90, training loss: 2.4197847843170166 = 1.6985114812850952 + 0.1 * 7.212732315063477
Epoch 90, val loss: 1.7065775394439697
Epoch 100, training loss: 2.34869122505188 = 1.639809250831604 + 0.1 * 7.08881950378418
Epoch 100, val loss: 1.6524945497512817
Epoch 110, training loss: 2.2665083408355713 = 1.5634610652923584 + 0.1 * 7.030472278594971
Epoch 110, val loss: 1.5830398797988892
Epoch 120, training loss: 2.16845440864563 = 1.469153881072998 + 0.1 * 6.993005275726318
Epoch 120, val loss: 1.4991868734359741
Epoch 130, training loss: 2.0577750205993652 = 1.3611716032028198 + 0.1 * 6.966033458709717
Epoch 130, val loss: 1.4054131507873535
Epoch 140, training loss: 1.9409925937652588 = 1.2463593482971191 + 0.1 * 6.946332931518555
Epoch 140, val loss: 1.3087208271026611
Epoch 150, training loss: 1.8250188827514648 = 1.1317658424377441 + 0.1 * 6.932529926300049
Epoch 150, val loss: 1.2137621641159058
Epoch 160, training loss: 1.714656114578247 = 1.0225059986114502 + 0.1 * 6.921501159667969
Epoch 160, val loss: 1.1249889135360718
Epoch 170, training loss: 1.6131062507629395 = 0.9219674468040466 + 0.1 * 6.911387920379639
Epoch 170, val loss: 1.044729471206665
Epoch 180, training loss: 1.5229177474975586 = 0.8327770829200745 + 0.1 * 6.901407241821289
Epoch 180, val loss: 0.9752290844917297
Epoch 190, training loss: 1.4451117515563965 = 0.7559671998023987 + 0.1 * 6.891444683074951
Epoch 190, val loss: 0.9167542457580566
Epoch 200, training loss: 1.3774969577789307 = 0.689624547958374 + 0.1 * 6.878724575042725
Epoch 200, val loss: 0.8672632575035095
Epoch 210, training loss: 1.318117618560791 = 0.6313878893852234 + 0.1 * 6.867297172546387
Epoch 210, val loss: 0.8254450559616089
Epoch 220, training loss: 1.2643007040023804 = 0.5789430737495422 + 0.1 * 6.853576183319092
Epoch 220, val loss: 0.7899437546730042
Epoch 230, training loss: 1.2146666049957275 = 0.5303763151168823 + 0.1 * 6.842903137207031
Epoch 230, val loss: 0.7598577737808228
Epoch 240, training loss: 1.168127417564392 = 0.48476389050483704 + 0.1 * 6.833635330200195
Epoch 240, val loss: 0.7345192432403564
Epoch 250, training loss: 1.1238188743591309 = 0.44136911630630493 + 0.1 * 6.824497222900391
Epoch 250, val loss: 0.7133263349533081
Epoch 260, training loss: 1.0820667743682861 = 0.3999803364276886 + 0.1 * 6.820864677429199
Epoch 260, val loss: 0.6957685947418213
Epoch 270, training loss: 1.041990876197815 = 0.36059674620628357 + 0.1 * 6.813941478729248
Epoch 270, val loss: 0.6814554929733276
Epoch 280, training loss: 1.0038214921951294 = 0.32303181290626526 + 0.1 * 6.807897090911865
Epoch 280, val loss: 0.6697390079498291
Epoch 290, training loss: 0.9678764939308167 = 0.28746527433395386 + 0.1 * 6.804111957550049
Epoch 290, val loss: 0.6604512333869934
Epoch 300, training loss: 0.9345999360084534 = 0.2545265555381775 + 0.1 * 6.80073356628418
Epoch 300, val loss: 0.6536305546760559
Epoch 310, training loss: 0.9041259288787842 = 0.2246893048286438 + 0.1 * 6.794365882873535
Epoch 310, val loss: 0.6493405699729919
Epoch 320, training loss: 0.8771960139274597 = 0.1981787234544754 + 0.1 * 6.790173053741455
Epoch 320, val loss: 0.6476578116416931
Epoch 330, training loss: 0.8545743823051453 = 0.17504291236400604 + 0.1 * 6.795314311981201
Epoch 330, val loss: 0.6485376358032227
Epoch 340, training loss: 0.8335666656494141 = 0.15518563985824585 + 0.1 * 6.783810138702393
Epoch 340, val loss: 0.6515634655952454
Epoch 350, training loss: 0.8159187436103821 = 0.1381307691335678 + 0.1 * 6.77787971496582
Epoch 350, val loss: 0.6565141677856445
Epoch 360, training loss: 0.8012720942497253 = 0.1234421357512474 + 0.1 * 6.778299808502197
Epoch 360, val loss: 0.6630472540855408
Epoch 370, training loss: 0.7874591946601868 = 0.11080842465162277 + 0.1 * 6.766507625579834
Epoch 370, val loss: 0.6706739068031311
Epoch 380, training loss: 0.7759636044502258 = 0.09985505044460297 + 0.1 * 6.761085510253906
Epoch 380, val loss: 0.6793291568756104
Epoch 390, training loss: 0.7667529582977295 = 0.09032891690731049 + 0.1 * 6.764240264892578
Epoch 390, val loss: 0.6886419057846069
Epoch 400, training loss: 0.7571186423301697 = 0.0820307657122612 + 0.1 * 6.75087833404541
Epoch 400, val loss: 0.6984567046165466
Epoch 410, training loss: 0.7491354942321777 = 0.07473050802946091 + 0.1 * 6.744049549102783
Epoch 410, val loss: 0.708634078502655
Epoch 420, training loss: 0.7426151037216187 = 0.0682981088757515 + 0.1 * 6.743169784545898
Epoch 420, val loss: 0.7190342545509338
Epoch 430, training loss: 0.7358238697052002 = 0.06262695044279099 + 0.1 * 6.731968879699707
Epoch 430, val loss: 0.7295160889625549
Epoch 440, training loss: 0.7297437191009521 = 0.057579927146434784 + 0.1 * 6.721637725830078
Epoch 440, val loss: 0.7401102185249329
Epoch 450, training loss: 0.7255504131317139 = 0.0530688501894474 + 0.1 * 6.724815368652344
Epoch 450, val loss: 0.7506773471832275
Epoch 460, training loss: 0.7207471132278442 = 0.04906437173485756 + 0.1 * 6.716826915740967
Epoch 460, val loss: 0.7609445452690125
Epoch 470, training loss: 0.7158384323120117 = 0.04548339545726776 + 0.1 * 6.703549861907959
Epoch 470, val loss: 0.7712150812149048
Epoch 480, training loss: 0.7113906145095825 = 0.04225560277700424 + 0.1 * 6.691349983215332
Epoch 480, val loss: 0.7813396453857422
Epoch 490, training loss: 0.7083290219306946 = 0.03933441638946533 + 0.1 * 6.689946174621582
Epoch 490, val loss: 0.7913382053375244
Epoch 500, training loss: 0.7045380473136902 = 0.03670336678624153 + 0.1 * 6.678347110748291
Epoch 500, val loss: 0.8010753393173218
Epoch 510, training loss: 0.7019578218460083 = 0.03432419151067734 + 0.1 * 6.676336288452148
Epoch 510, val loss: 0.8107234835624695
Epoch 520, training loss: 0.6989724636077881 = 0.0321568101644516 + 0.1 * 6.668156147003174
Epoch 520, val loss: 0.8201868534088135
Epoch 530, training loss: 0.6963052749633789 = 0.030175788328051567 + 0.1 * 6.661294460296631
Epoch 530, val loss: 0.8294702172279358
Epoch 540, training loss: 0.6943181157112122 = 0.02836453728377819 + 0.1 * 6.659535884857178
Epoch 540, val loss: 0.8385428786277771
Epoch 550, training loss: 0.6919246912002563 = 0.026713481172919273 + 0.1 * 6.652112007141113
Epoch 550, val loss: 0.8473951816558838
Epoch 560, training loss: 0.6899644136428833 = 0.02519727125763893 + 0.1 * 6.647671222686768
Epoch 560, val loss: 0.8561224937438965
Epoch 570, training loss: 0.6900725960731506 = 0.02380220964550972 + 0.1 * 6.662703514099121
Epoch 570, val loss: 0.864628255367279
Epoch 580, training loss: 0.6860635280609131 = 0.0225248821079731 + 0.1 * 6.635385990142822
Epoch 580, val loss: 0.8728247284889221
Epoch 590, training loss: 0.6842460036277771 = 0.021347500383853912 + 0.1 * 6.6289849281311035
Epoch 590, val loss: 0.8810303807258606
Epoch 600, training loss: 0.6846606135368347 = 0.020259058102965355 + 0.1 * 6.644015312194824
Epoch 600, val loss: 0.8889346718788147
Epoch 610, training loss: 0.6819427013397217 = 0.01925424113869667 + 0.1 * 6.6268839836120605
Epoch 610, val loss: 0.8966119289398193
Epoch 620, training loss: 0.6804513931274414 = 0.01832342892885208 + 0.1 * 6.621279716491699
Epoch 620, val loss: 0.9042345285415649
Epoch 630, training loss: 0.6785628795623779 = 0.017459649592638016 + 0.1 * 6.611032485961914
Epoch 630, val loss: 0.9116519689559937
Epoch 640, training loss: 0.6774226427078247 = 0.01665489561855793 + 0.1 * 6.607676982879639
Epoch 640, val loss: 0.9189251661300659
Epoch 650, training loss: 0.6761870980262756 = 0.015902340412139893 + 0.1 * 6.602847576141357
Epoch 650, val loss: 0.9260213971138
Epoch 660, training loss: 0.6756137013435364 = 0.015200705267488956 + 0.1 * 6.604129791259766
Epoch 660, val loss: 0.9328571557998657
Epoch 670, training loss: 0.6739001870155334 = 0.014547010883688927 + 0.1 * 6.593532085418701
Epoch 670, val loss: 0.939536988735199
Epoch 680, training loss: 0.6731650829315186 = 0.013934190385043621 + 0.1 * 6.59230899810791
Epoch 680, val loss: 0.9462445974349976
Epoch 690, training loss: 0.6741018891334534 = 0.013359968550503254 + 0.1 * 6.607419013977051
Epoch 690, val loss: 0.9526872634887695
Epoch 700, training loss: 0.6715911030769348 = 0.012822814285755157 + 0.1 * 6.587682723999023
Epoch 700, val loss: 0.9588367342948914
Epoch 710, training loss: 0.6705813407897949 = 0.012318684719502926 + 0.1 * 6.5826263427734375
Epoch 710, val loss: 0.9650462865829468
Epoch 720, training loss: 0.6704126000404358 = 0.011844459921121597 + 0.1 * 6.585681438446045
Epoch 720, val loss: 0.9711037874221802
Epoch 730, training loss: 0.6691802144050598 = 0.011398705653846264 + 0.1 * 6.57781457901001
Epoch 730, val loss: 0.9768730998039246
Epoch 740, training loss: 0.6684863567352295 = 0.010978939011693 + 0.1 * 6.575074195861816
Epoch 740, val loss: 0.9827617406845093
Epoch 750, training loss: 0.6686851978302002 = 0.01058337651193142 + 0.1 * 6.581017971038818
Epoch 750, val loss: 0.9881536364555359
Epoch 760, training loss: 0.6667099595069885 = 0.010211397893726826 + 0.1 * 6.564985275268555
Epoch 760, val loss: 0.9936776757240295
Epoch 770, training loss: 0.6669942736625671 = 0.009859767742455006 + 0.1 * 6.571345329284668
Epoch 770, val loss: 0.9990806579589844
Epoch 780, training loss: 0.6652816534042358 = 0.009528325870633125 + 0.1 * 6.557533264160156
Epoch 780, val loss: 1.004266619682312
Epoch 790, training loss: 0.6649103164672852 = 0.009214557707309723 + 0.1 * 6.556957244873047
Epoch 790, val loss: 1.0094460248947144
Epoch 800, training loss: 0.6634476184844971 = 0.008917602710425854 + 0.1 * 6.545299530029297
Epoch 800, val loss: 1.0145796537399292
Epoch 810, training loss: 0.6638016104698181 = 0.008635922335088253 + 0.1 * 6.551656723022461
Epoch 810, val loss: 1.0194724798202515
Epoch 820, training loss: 0.6631754636764526 = 0.008369163610041142 + 0.1 * 6.548063278198242
Epoch 820, val loss: 1.0242689847946167
Epoch 830, training loss: 0.6619858741760254 = 0.008115863427519798 + 0.1 * 6.538699626922607
Epoch 830, val loss: 1.0290645360946655
Epoch 840, training loss: 0.6607154607772827 = 0.007875191979110241 + 0.1 * 6.528402328491211
Epoch 840, val loss: 1.033738136291504
Epoch 850, training loss: 0.6619274616241455 = 0.007646333426237106 + 0.1 * 6.542811393737793
Epoch 850, val loss: 1.0383453369140625
Epoch 860, training loss: 0.6598737835884094 = 0.007428375538438559 + 0.1 * 6.524454116821289
Epoch 860, val loss: 1.042646884918213
Epoch 870, training loss: 0.6587457656860352 = 0.007220929488539696 + 0.1 * 6.5152482986450195
Epoch 870, val loss: 1.0470954179763794
Epoch 880, training loss: 0.6618192791938782 = 0.00702294846996665 + 0.1 * 6.5479631423950195
Epoch 880, val loss: 1.0513948202133179
Epoch 890, training loss: 0.658097505569458 = 0.006834052968770266 + 0.1 * 6.512634754180908
Epoch 890, val loss: 1.055493950843811
Epoch 900, training loss: 0.6579172611236572 = 0.006653813179582357 + 0.1 * 6.512634754180908
Epoch 900, val loss: 1.059768557548523
Epoch 910, training loss: 0.6564738154411316 = 0.006481172516942024 + 0.1 * 6.4999260902404785
Epoch 910, val loss: 1.063720464706421
Epoch 920, training loss: 0.6563899517059326 = 0.006316271144896746 + 0.1 * 6.500737190246582
Epoch 920, val loss: 1.067788004875183
Epoch 930, training loss: 0.6562298536300659 = 0.006158262491226196 + 0.1 * 6.500716209411621
Epoch 930, val loss: 1.0717862844467163
Epoch 940, training loss: 0.6559348106384277 = 0.006006564013659954 + 0.1 * 6.4992828369140625
Epoch 940, val loss: 1.075632929801941
Epoch 950, training loss: 0.6555105447769165 = 0.0058609191328287125 + 0.1 * 6.496496200561523
Epoch 950, val loss: 1.0793462991714478
Epoch 960, training loss: 0.6555856466293335 = 0.005721297115087509 + 0.1 * 6.498643398284912
Epoch 960, val loss: 1.0830695629119873
Epoch 970, training loss: 0.6547794342041016 = 0.005587087944149971 + 0.1 * 6.4919233322143555
Epoch 970, val loss: 1.0866481065750122
Epoch 980, training loss: 0.6535007357597351 = 0.005458462052047253 + 0.1 * 6.480422496795654
Epoch 980, val loss: 1.0902599096298218
Epoch 990, training loss: 0.6539153456687927 = 0.005334681365638971 + 0.1 * 6.485806465148926
Epoch 990, val loss: 1.0937130451202393
Epoch 1000, training loss: 0.6528356671333313 = 0.005215745884925127 + 0.1 * 6.476198673248291
Epoch 1000, val loss: 1.097190499305725
Epoch 1010, training loss: 0.6529478430747986 = 0.005101346876472235 + 0.1 * 6.4784650802612305
Epoch 1010, val loss: 1.1005275249481201
Epoch 1020, training loss: 0.6530172824859619 = 0.004991492722183466 + 0.1 * 6.480257987976074
Epoch 1020, val loss: 1.103894591331482
Epoch 1030, training loss: 0.6532137393951416 = 0.004885450936853886 + 0.1 * 6.483283042907715
Epoch 1030, val loss: 1.107147216796875
Epoch 1040, training loss: 0.6518829464912415 = 0.0047834487631917 + 0.1 * 6.47099494934082
Epoch 1040, val loss: 1.1103686094284058
Epoch 1050, training loss: 0.6512912511825562 = 0.004685114603489637 + 0.1 * 6.466061592102051
Epoch 1050, val loss: 1.1136231422424316
Epoch 1060, training loss: 0.6504580974578857 = 0.004590000491589308 + 0.1 * 6.458681106567383
Epoch 1060, val loss: 1.1166183948516846
Epoch 1070, training loss: 0.6509326100349426 = 0.00449841795489192 + 0.1 * 6.464341640472412
Epoch 1070, val loss: 1.1197141408920288
Epoch 1080, training loss: 0.6497030258178711 = 0.00440990598872304 + 0.1 * 6.452930927276611
Epoch 1080, val loss: 1.1227298974990845
Epoch 1090, training loss: 0.6503938436508179 = 0.004324654117226601 + 0.1 * 6.460691452026367
Epoch 1090, val loss: 1.125812292098999
Epoch 1100, training loss: 0.6522847414016724 = 0.00424213195219636 + 0.1 * 6.480425834655762
Epoch 1100, val loss: 1.1287041902542114
Epoch 1110, training loss: 0.6494130492210388 = 0.004162132740020752 + 0.1 * 6.452508926391602
Epoch 1110, val loss: 1.131516933441162
Epoch 1120, training loss: 0.6494948863983154 = 0.004085146822035313 + 0.1 * 6.454097747802734
Epoch 1120, val loss: 1.1344215869903564
Epoch 1130, training loss: 0.6490736603736877 = 0.004010295029729605 + 0.1 * 6.4506330490112305
Epoch 1130, val loss: 1.137191891670227
Epoch 1140, training loss: 0.6476035714149475 = 0.003938036039471626 + 0.1 * 6.436655521392822
Epoch 1140, val loss: 1.1400015354156494
Epoch 1150, training loss: 0.6497613787651062 = 0.003868288593366742 + 0.1 * 6.458930969238281
Epoch 1150, val loss: 1.1428135633468628
Epoch 1160, training loss: 0.6482782959938049 = 0.0038001348730176687 + 0.1 * 6.444781303405762
Epoch 1160, val loss: 1.1454424858093262
Epoch 1170, training loss: 0.6481181383132935 = 0.003734442638233304 + 0.1 * 6.443836688995361
Epoch 1170, val loss: 1.1481199264526367
Epoch 1180, training loss: 0.6478406190872192 = 0.003670681733638048 + 0.1 * 6.441699028015137
Epoch 1180, val loss: 1.1507374048233032
Epoch 1190, training loss: 0.6469751596450806 = 0.003608854953199625 + 0.1 * 6.4336628913879395
Epoch 1190, val loss: 1.1533504724502563
Epoch 1200, training loss: 0.6465771794319153 = 0.003548976266756654 + 0.1 * 6.430281639099121
Epoch 1200, val loss: 1.1559628248214722
Epoch 1210, training loss: 0.6466132998466492 = 0.003490791190415621 + 0.1 * 6.431224822998047
Epoch 1210, val loss: 1.1585721969604492
Epoch 1220, training loss: 0.6467706561088562 = 0.003433998441323638 + 0.1 * 6.433366298675537
Epoch 1220, val loss: 1.1609909534454346
Epoch 1230, training loss: 0.645911693572998 = 0.0033789610024541616 + 0.1 * 6.425326824188232
Epoch 1230, val loss: 1.1634128093719482
Epoch 1240, training loss: 0.646590530872345 = 0.0033257256727665663 + 0.1 * 6.432648181915283
Epoch 1240, val loss: 1.1659183502197266
Epoch 1250, training loss: 0.6461238265037537 = 0.0032736845314502716 + 0.1 * 6.428501605987549
Epoch 1250, val loss: 1.1682968139648438
Epoch 1260, training loss: 0.6457692384719849 = 0.0032232722733169794 + 0.1 * 6.425459384918213
Epoch 1260, val loss: 1.1706098318099976
Epoch 1270, training loss: 0.6452217698097229 = 0.0031744299922138453 + 0.1 * 6.420473575592041
Epoch 1270, val loss: 1.1729824542999268
Epoch 1280, training loss: 0.6450843214988708 = 0.0031268589664250612 + 0.1 * 6.419574737548828
Epoch 1280, val loss: 1.1753283739089966
Epoch 1290, training loss: 0.6444490551948547 = 0.0030806181021034718 + 0.1 * 6.413684368133545
Epoch 1290, val loss: 1.1777054071426392
Epoch 1300, training loss: 0.6454795598983765 = 0.0030354843474924564 + 0.1 * 6.424440860748291
Epoch 1300, val loss: 1.1799167394638062
Epoch 1310, training loss: 0.6463444828987122 = 0.002991465153172612 + 0.1 * 6.433530330657959
Epoch 1310, val loss: 1.1821198463439941
Epoch 1320, training loss: 0.6436461806297302 = 0.0029485421255230904 + 0.1 * 6.406976699829102
Epoch 1320, val loss: 1.1842554807662964
Epoch 1330, training loss: 0.644841194152832 = 0.00290707778185606 + 0.1 * 6.41934061050415
Epoch 1330, val loss: 1.1865133047103882
Epoch 1340, training loss: 0.6453084945678711 = 0.002866550348699093 + 0.1 * 6.424418926239014
Epoch 1340, val loss: 1.188660979270935
Epoch 1350, training loss: 0.6429846882820129 = 0.002826953073963523 + 0.1 * 6.401577472686768
Epoch 1350, val loss: 1.1908035278320312
Epoch 1360, training loss: 0.6431118845939636 = 0.002788553247228265 + 0.1 * 6.403233528137207
Epoch 1360, val loss: 1.1929656267166138
Epoch 1370, training loss: 0.6447072625160217 = 0.002751030260697007 + 0.1 * 6.419561862945557
Epoch 1370, val loss: 1.1950669288635254
Epoch 1380, training loss: 0.6431547403335571 = 0.002714360598474741 + 0.1 * 6.4044036865234375
Epoch 1380, val loss: 1.1971015930175781
Epoch 1390, training loss: 0.6431163549423218 = 0.0026786383241415024 + 0.1 * 6.404376983642578
Epoch 1390, val loss: 1.1991491317749023
Epoch 1400, training loss: 0.6420432925224304 = 0.002643788466230035 + 0.1 * 6.39399528503418
Epoch 1400, val loss: 1.2011663913726807
Epoch 1410, training loss: 0.642846941947937 = 0.0026097288355231285 + 0.1 * 6.402371883392334
Epoch 1410, val loss: 1.2031195163726807
Epoch 1420, training loss: 0.6422628164291382 = 0.002576460363343358 + 0.1 * 6.3968634605407715
Epoch 1420, val loss: 1.2050154209136963
Epoch 1430, training loss: 0.64280766248703 = 0.0025440154131501913 + 0.1 * 6.402636528015137
Epoch 1430, val loss: 1.2069638967514038
Epoch 1440, training loss: 0.6428065299987793 = 0.0025122901424765587 + 0.1 * 6.402942180633545
Epoch 1440, val loss: 1.2088758945465088
Epoch 1450, training loss: 0.6418750882148743 = 0.002481411909684539 + 0.1 * 6.393936634063721
Epoch 1450, val loss: 1.2108129262924194
Epoch 1460, training loss: 0.6419810056686401 = 0.0024513115640729666 + 0.1 * 6.395297050476074
Epoch 1460, val loss: 1.2127716541290283
Epoch 1470, training loss: 0.6412255167961121 = 0.0024217762984335423 + 0.1 * 6.388037204742432
Epoch 1470, val loss: 1.2146040201187134
Epoch 1480, training loss: 0.6423950791358948 = 0.0023929185699671507 + 0.1 * 6.400021076202393
Epoch 1480, val loss: 1.2164249420166016
Epoch 1490, training loss: 0.641470193862915 = 0.0023646813351660967 + 0.1 * 6.391054630279541
Epoch 1490, val loss: 1.2181396484375
Epoch 1500, training loss: 0.6409316658973694 = 0.0023370871786028147 + 0.1 * 6.3859453201293945
Epoch 1500, val loss: 1.2199722528457642
Epoch 1510, training loss: 0.641089677810669 = 0.002310218522325158 + 0.1 * 6.387794494628906
Epoch 1510, val loss: 1.2218064069747925
Epoch 1520, training loss: 0.6404386162757874 = 0.0022837978322058916 + 0.1 * 6.381547927856445
Epoch 1520, val loss: 1.2235946655273438
Epoch 1530, training loss: 0.6411032676696777 = 0.0022580204531550407 + 0.1 * 6.388452529907227
Epoch 1530, val loss: 1.2252897024154663
Epoch 1540, training loss: 0.6402079463005066 = 0.00223277835175395 + 0.1 * 6.379751205444336
Epoch 1540, val loss: 1.2270524501800537
Epoch 1550, training loss: 0.6418997049331665 = 0.0022080708295106888 + 0.1 * 6.396916389465332
Epoch 1550, val loss: 1.2287222146987915
Epoch 1560, training loss: 0.6398234963417053 = 0.002183872275054455 + 0.1 * 6.376396179199219
Epoch 1560, val loss: 1.2304255962371826
Epoch 1570, training loss: 0.6399475932121277 = 0.0021602935157716274 + 0.1 * 6.377872467041016
Epoch 1570, val loss: 1.2321664094924927
Epoch 1580, training loss: 0.6400026082992554 = 0.0021371194161474705 + 0.1 * 6.378654479980469
Epoch 1580, val loss: 1.2337545156478882
Epoch 1590, training loss: 0.6390970945358276 = 0.002114445436745882 + 0.1 * 6.369825839996338
Epoch 1590, val loss: 1.2353729009628296
Epoch 1600, training loss: 0.6396604776382446 = 0.002092269016429782 + 0.1 * 6.375682353973389
Epoch 1600, val loss: 1.2370293140411377
Epoch 1610, training loss: 0.6402426958084106 = 0.002070503542199731 + 0.1 * 6.3817219734191895
Epoch 1610, val loss: 1.238681674003601
Epoch 1620, training loss: 0.6394670605659485 = 0.002049152972176671 + 0.1 * 6.374178886413574
Epoch 1620, val loss: 1.2402269840240479
Epoch 1630, training loss: 0.6389076113700867 = 0.002028282731771469 + 0.1 * 6.36879301071167
Epoch 1630, val loss: 1.241836667060852
Epoch 1640, training loss: 0.6401755213737488 = 0.002007806906476617 + 0.1 * 6.381677150726318
Epoch 1640, val loss: 1.2434148788452148
Epoch 1650, training loss: 0.6389950513839722 = 0.0019876305013895035 + 0.1 * 6.370074272155762
Epoch 1650, val loss: 1.2449769973754883
Epoch 1660, training loss: 0.6389433741569519 = 0.0019679302349686623 + 0.1 * 6.369754791259766
Epoch 1660, val loss: 1.246482253074646
Epoch 1670, training loss: 0.6390185356140137 = 0.0019485217053443193 + 0.1 * 6.370699882507324
Epoch 1670, val loss: 1.2479712963104248
Epoch 1680, training loss: 0.6392463445663452 = 0.0019295039819553494 + 0.1 * 6.373167991638184
Epoch 1680, val loss: 1.2494640350341797
Epoch 1690, training loss: 0.6383669972419739 = 0.0019109147833660245 + 0.1 * 6.364560604095459
Epoch 1690, val loss: 1.2509841918945312
Epoch 1700, training loss: 0.6390107274055481 = 0.0018925800686702132 + 0.1 * 6.371181488037109
Epoch 1700, val loss: 1.2524770498275757
Epoch 1710, training loss: 0.6384903192520142 = 0.001874591805972159 + 0.1 * 6.366157531738281
Epoch 1710, val loss: 1.2539671659469604
Epoch 1720, training loss: 0.6393296122550964 = 0.0018569843377918005 + 0.1 * 6.374726295471191
Epoch 1720, val loss: 1.2553809881210327
Epoch 1730, training loss: 0.6380538940429688 = 0.0018397020176053047 + 0.1 * 6.362142086029053
Epoch 1730, val loss: 1.2568289041519165
Epoch 1740, training loss: 0.6378348469734192 = 0.0018227858236059546 + 0.1 * 6.36012077331543
Epoch 1740, val loss: 1.2583205699920654
Epoch 1750, training loss: 0.6389721632003784 = 0.0018061816226691008 + 0.1 * 6.371659755706787
Epoch 1750, val loss: 1.2597447633743286
Epoch 1760, training loss: 0.6381582617759705 = 0.0017897805664688349 + 0.1 * 6.363684177398682
Epoch 1760, val loss: 1.2611509561538696
Epoch 1770, training loss: 0.638163149356842 = 0.0017736635636538267 + 0.1 * 6.363894939422607
Epoch 1770, val loss: 1.2625510692596436
Epoch 1780, training loss: 0.6379067301750183 = 0.0017578545957803726 + 0.1 * 6.3614888191223145
Epoch 1780, val loss: 1.2639095783233643
Epoch 1790, training loss: 0.6378657221794128 = 0.0017423274694010615 + 0.1 * 6.361234188079834
Epoch 1790, val loss: 1.265263557434082
Epoch 1800, training loss: 0.6380385160446167 = 0.0017270691459998488 + 0.1 * 6.363114356994629
Epoch 1800, val loss: 1.2666398286819458
Epoch 1810, training loss: 0.6372709274291992 = 0.0017120648408308625 + 0.1 * 6.355588436126709
Epoch 1810, val loss: 1.2680368423461914
Epoch 1820, training loss: 0.636682391166687 = 0.0016973245656117797 + 0.1 * 6.349850654602051
Epoch 1820, val loss: 1.2693673372268677
Epoch 1830, training loss: 0.6368794441223145 = 0.0016828518128022552 + 0.1 * 6.351965427398682
Epoch 1830, val loss: 1.2706990242004395
Epoch 1840, training loss: 0.6365853548049927 = 0.0016686638118699193 + 0.1 * 6.349166393280029
Epoch 1840, val loss: 1.2720348834991455
Epoch 1850, training loss: 0.6367526054382324 = 0.0016546028200536966 + 0.1 * 6.350979804992676
Epoch 1850, val loss: 1.2733253240585327
Epoch 1860, training loss: 0.637115478515625 = 0.0016408049268648028 + 0.1 * 6.3547468185424805
Epoch 1860, val loss: 1.2746121883392334
Epoch 1870, training loss: 0.6364691257476807 = 0.0016272107604891062 + 0.1 * 6.348419189453125
Epoch 1870, val loss: 1.2758190631866455
Epoch 1880, training loss: 0.6370741724967957 = 0.0016139090294018388 + 0.1 * 6.354602336883545
Epoch 1880, val loss: 1.277142882347107
Epoch 1890, training loss: 0.6370093822479248 = 0.0016007981030270457 + 0.1 * 6.354085445404053
Epoch 1890, val loss: 1.2783935070037842
Epoch 1900, training loss: 0.6365547180175781 = 0.0015878267586231232 + 0.1 * 6.349668979644775
Epoch 1900, val loss: 1.2796181440353394
Epoch 1910, training loss: 0.6361793279647827 = 0.0015751420287415385 + 0.1 * 6.346042156219482
Epoch 1910, val loss: 1.280871868133545
Epoch 1920, training loss: 0.636599063873291 = 0.0015625637024641037 + 0.1 * 6.350365161895752
Epoch 1920, val loss: 1.2821458578109741
Epoch 1930, training loss: 0.6354109644889832 = 0.0015501688467338681 + 0.1 * 6.338607311248779
Epoch 1930, val loss: 1.283313274383545
Epoch 1940, training loss: 0.6364026069641113 = 0.0015380290569737554 + 0.1 * 6.3486456871032715
Epoch 1940, val loss: 1.2845549583435059
Epoch 1950, training loss: 0.6351876854896545 = 0.001526002073660493 + 0.1 * 6.3366169929504395
Epoch 1950, val loss: 1.2857484817504883
Epoch 1960, training loss: 0.6362272500991821 = 0.00151425926014781 + 0.1 * 6.347129821777344
Epoch 1960, val loss: 1.286986231803894
Epoch 1970, training loss: 0.6366103291511536 = 0.0015025611501187086 + 0.1 * 6.351077556610107
Epoch 1970, val loss: 1.2881838083267212
Epoch 1980, training loss: 0.6369490623474121 = 0.0014910985482856631 + 0.1 * 6.354579925537109
Epoch 1980, val loss: 1.2892975807189941
Epoch 1990, training loss: 0.6352213621139526 = 0.0014797800686210394 + 0.1 * 6.3374152183532715
Epoch 1990, val loss: 1.2904331684112549
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.8204410076141357 = 1.9607572555541992 + 0.1 * 8.596837043762207
Epoch 0, val loss: 1.9630552530288696
Epoch 10, training loss: 2.810091018676758 = 1.9504159688949585 + 0.1 * 8.596750259399414
Epoch 10, val loss: 1.953120231628418
Epoch 20, training loss: 2.797517776489258 = 1.9379016160964966 + 0.1 * 8.596162796020508
Epoch 20, val loss: 1.9406155347824097
Epoch 30, training loss: 2.779707908630371 = 1.9206188917160034 + 0.1 * 8.590888977050781
Epoch 30, val loss: 1.9229826927185059
Epoch 40, training loss: 2.7504782676696777 = 1.8951380252838135 + 0.1 * 8.553401947021484
Epoch 40, val loss: 1.8970059156417847
Epoch 50, training loss: 2.6946675777435303 = 1.859411358833313 + 0.1 * 8.352561950683594
Epoch 50, val loss: 1.861541986465454
Epoch 60, training loss: 2.619887351989746 = 1.8174346685409546 + 0.1 * 8.024527549743652
Epoch 60, val loss: 1.8220640420913696
Epoch 70, training loss: 2.561147689819336 = 1.7778278589248657 + 0.1 * 7.833198547363281
Epoch 70, val loss: 1.7876708507537842
Epoch 80, training loss: 2.4962427616119385 = 1.7405391931533813 + 0.1 * 7.55703592300415
Epoch 80, val loss: 1.7545970678329468
Epoch 90, training loss: 2.422740936279297 = 1.6944022178649902 + 0.1 * 7.283386707305908
Epoch 90, val loss: 1.7133934497833252
Epoch 100, training loss: 2.343388319015503 = 1.6345120668411255 + 0.1 * 7.088763236999512
Epoch 100, val loss: 1.6613473892211914
Epoch 110, training loss: 2.2559139728546143 = 1.5570348501205444 + 0.1 * 6.988790512084961
Epoch 110, val loss: 1.5956761837005615
Epoch 120, training loss: 2.1590332984924316 = 1.4650202989578247 + 0.1 * 6.940131187438965
Epoch 120, val loss: 1.5175299644470215
Epoch 130, training loss: 2.0580036640167236 = 1.3666985034942627 + 0.1 * 6.913050651550293
Epoch 130, val loss: 1.4363133907318115
Epoch 140, training loss: 1.95755934715271 = 1.2678425312042236 + 0.1 * 6.897168159484863
Epoch 140, val loss: 1.355622410774231
Epoch 150, training loss: 1.860019326210022 = 1.1713166236877441 + 0.1 * 6.887026786804199
Epoch 150, val loss: 1.2780101299285889
Epoch 160, training loss: 1.7689435482025146 = 1.0806407928466797 + 0.1 * 6.883026599884033
Epoch 160, val loss: 1.2056787014007568
Epoch 170, training loss: 1.687652349472046 = 0.9999409914016724 + 0.1 * 6.877112865447998
Epoch 170, val loss: 1.1433281898498535
Epoch 180, training loss: 1.615311622619629 = 0.9281120300292969 + 0.1 * 6.8719964027404785
Epoch 180, val loss: 1.0893242359161377
Epoch 190, training loss: 1.5485174655914307 = 0.8616243004798889 + 0.1 * 6.868931293487549
Epoch 190, val loss: 1.040436863899231
Epoch 200, training loss: 1.4837594032287598 = 0.7971643805503845 + 0.1 * 6.865950584411621
Epoch 200, val loss: 0.9930059313774109
Epoch 210, training loss: 1.418684720993042 = 0.7325392365455627 + 0.1 * 6.861454963684082
Epoch 210, val loss: 0.9449948072433472
Epoch 220, training loss: 1.3537782430648804 = 0.6682091355323792 + 0.1 * 6.855690956115723
Epoch 220, val loss: 0.8973629474639893
Epoch 230, training loss: 1.2918968200683594 = 0.6069138050079346 + 0.1 * 6.849830627441406
Epoch 230, val loss: 0.8529460430145264
Epoch 240, training loss: 1.2349997758865356 = 0.5511776804924011 + 0.1 * 6.838220596313477
Epoch 240, val loss: 0.8149375915527344
Epoch 250, training loss: 1.18428635597229 = 0.5017499923706055 + 0.1 * 6.825362682342529
Epoch 250, val loss: 0.7841055989265442
Epoch 260, training loss: 1.1401618719100952 = 0.45830628275871277 + 0.1 * 6.8185553550720215
Epoch 260, val loss: 0.7603318095207214
Epoch 270, training loss: 1.0990440845489502 = 0.41951727867126465 + 0.1 * 6.795267105102539
Epoch 270, val loss: 0.7419561743736267
Epoch 280, training loss: 1.0610511302947998 = 0.3831387162208557 + 0.1 * 6.779123783111572
Epoch 280, val loss: 0.7267419099807739
Epoch 290, training loss: 1.0253854990005493 = 0.34830108284950256 + 0.1 * 6.770843982696533
Epoch 290, val loss: 0.7133018374443054
Epoch 300, training loss: 0.990247368812561 = 0.3148757517337799 + 0.1 * 6.753715515136719
Epoch 300, val loss: 0.7011458277702332
Epoch 310, training loss: 0.9562067985534668 = 0.2829309105873108 + 0.1 * 6.732758522033691
Epoch 310, val loss: 0.6901997923851013
Epoch 320, training loss: 0.9252539873123169 = 0.2528170049190521 + 0.1 * 6.724370002746582
Epoch 320, val loss: 0.6810016632080078
Epoch 330, training loss: 0.8969761729240417 = 0.22498203814029694 + 0.1 * 6.719941139221191
Epoch 330, val loss: 0.6737122535705566
Epoch 340, training loss: 0.8704125881195068 = 0.19966749846935272 + 0.1 * 6.7074503898620605
Epoch 340, val loss: 0.6685324311256409
Epoch 350, training loss: 0.8472226858139038 = 0.17698737978935242 + 0.1 * 6.702353000640869
Epoch 350, val loss: 0.6654457449913025
Epoch 360, training loss: 0.8293663263320923 = 0.15696018934249878 + 0.1 * 6.724061012268066
Epoch 360, val loss: 0.6644006371498108
Epoch 370, training loss: 0.8090314865112305 = 0.1395758092403412 + 0.1 * 6.694556713104248
Epoch 370, val loss: 0.6650516986846924
Epoch 380, training loss: 0.7926163673400879 = 0.12438809871673584 + 0.1 * 6.682282447814941
Epoch 380, val loss: 0.667327344417572
Epoch 390, training loss: 0.7802035808563232 = 0.11105230450630188 + 0.1 * 6.6915130615234375
Epoch 390, val loss: 0.6707203388214111
Epoch 400, training loss: 0.7666698098182678 = 0.09940589964389801 + 0.1 * 6.672638893127441
Epoch 400, val loss: 0.6751413941383362
Epoch 410, training loss: 0.75566166639328 = 0.08915521204471588 + 0.1 * 6.665064811706543
Epoch 410, val loss: 0.6803810596466064
Epoch 420, training loss: 0.7491239309310913 = 0.08009584248065948 + 0.1 * 6.690280914306641
Epoch 420, val loss: 0.6862358450889587
Epoch 430, training loss: 0.7380675077438354 = 0.07213052362203598 + 0.1 * 6.659369945526123
Epoch 430, val loss: 0.6924813985824585
Epoch 440, training loss: 0.7299348711967468 = 0.06509154289960861 + 0.1 * 6.648433208465576
Epoch 440, val loss: 0.699194073677063
Epoch 450, training loss: 0.7231382131576538 = 0.058863840997219086 + 0.1 * 6.642744064331055
Epoch 450, val loss: 0.7061833143234253
Epoch 460, training loss: 0.7179555892944336 = 0.05338365584611893 + 0.1 * 6.645719528198242
Epoch 460, val loss: 0.7133620381355286
Epoch 470, training loss: 0.7122145891189575 = 0.04858776554465294 + 0.1 * 6.636268615722656
Epoch 470, val loss: 0.7205422520637512
Epoch 480, training loss: 0.7074657678604126 = 0.04435630142688751 + 0.1 * 6.631094455718994
Epoch 480, val loss: 0.7278798818588257
Epoch 490, training loss: 0.7032060027122498 = 0.040634628385305405 + 0.1 * 6.625713348388672
Epoch 490, val loss: 0.7350776195526123
Epoch 500, training loss: 0.6985200643539429 = 0.037352055311203 + 0.1 * 6.611680030822754
Epoch 500, val loss: 0.7422108054161072
Epoch 510, training loss: 0.6954870223999023 = 0.03443510830402374 + 0.1 * 6.6105194091796875
Epoch 510, val loss: 0.7493014931678772
Epoch 520, training loss: 0.6921260356903076 = 0.03184356167912483 + 0.1 * 6.602824687957764
Epoch 520, val loss: 0.7563858032226562
Epoch 530, training loss: 0.6895231604576111 = 0.029535086825489998 + 0.1 * 6.599880218505859
Epoch 530, val loss: 0.7631938457489014
Epoch 540, training loss: 0.6860628128051758 = 0.027473803609609604 + 0.1 * 6.58588981628418
Epoch 540, val loss: 0.769985020160675
Epoch 550, training loss: 0.6838220357894897 = 0.02562602236866951 + 0.1 * 6.581960201263428
Epoch 550, val loss: 0.7765548825263977
Epoch 560, training loss: 0.6826677322387695 = 0.023959878832101822 + 0.1 * 6.58707857131958
Epoch 560, val loss: 0.7830170392990112
Epoch 570, training loss: 0.6797924637794495 = 0.022457517683506012 + 0.1 * 6.5733489990234375
Epoch 570, val loss: 0.7893731594085693
Epoch 580, training loss: 0.6780786514282227 = 0.02110038883984089 + 0.1 * 6.569782257080078
Epoch 580, val loss: 0.7954972982406616
Epoch 590, training loss: 0.676217794418335 = 0.01987290196120739 + 0.1 * 6.563448905944824
Epoch 590, val loss: 0.8014559149742126
Epoch 600, training loss: 0.6792559623718262 = 0.01875336281955242 + 0.1 * 6.605025768280029
Epoch 600, val loss: 0.807320773601532
Epoch 610, training loss: 0.6720948815345764 = 0.017733121290802956 + 0.1 * 6.5436177253723145
Epoch 610, val loss: 0.8129836320877075
Epoch 620, training loss: 0.6700699925422668 = 0.01679871417582035 + 0.1 * 6.532712459564209
Epoch 620, val loss: 0.8185476064682007
Epoch 630, training loss: 0.6700427532196045 = 0.01593616232275963 + 0.1 * 6.5410661697387695
Epoch 630, val loss: 0.8241323828697205
Epoch 640, training loss: 0.6693521738052368 = 0.015144966542720795 + 0.1 * 6.542072296142578
Epoch 640, val loss: 0.829302966594696
Epoch 650, training loss: 0.666172981262207 = 0.014417926780879498 + 0.1 * 6.517550468444824
Epoch 650, val loss: 0.8344172835350037
Epoch 660, training loss: 0.6669296622276306 = 0.013743819668889046 + 0.1 * 6.531858444213867
Epoch 660, val loss: 0.8395344614982605
Epoch 670, training loss: 0.664948582649231 = 0.013119155541062355 + 0.1 * 6.518294334411621
Epoch 670, val loss: 0.844434916973114
Epoch 680, training loss: 0.6650711894035339 = 0.012538457289338112 + 0.1 * 6.525327682495117
Epoch 680, val loss: 0.8492416143417358
Epoch 690, training loss: 0.6625953316688538 = 0.012000421062111855 + 0.1 * 6.505948543548584
Epoch 690, val loss: 0.8538819551467896
Epoch 700, training loss: 0.6650488972663879 = 0.011498760432004929 + 0.1 * 6.535501480102539
Epoch 700, val loss: 0.8584194183349609
Epoch 710, training loss: 0.6610226631164551 = 0.011032069101929665 + 0.1 * 6.499906063079834
Epoch 710, val loss: 0.8629176020622253
Epoch 720, training loss: 0.6601866483688354 = 0.010595367290079594 + 0.1 * 6.495912551879883
Epoch 720, val loss: 0.86725914478302
Epoch 730, training loss: 0.66166752576828 = 0.010184910148382187 + 0.1 * 6.51482629776001
Epoch 730, val loss: 0.8715345859527588
Epoch 740, training loss: 0.6593454480171204 = 0.009800877422094345 + 0.1 * 6.495445728302002
Epoch 740, val loss: 0.8755990862846375
Epoch 750, training loss: 0.6597000956535339 = 0.009440201334655285 + 0.1 * 6.502599239349365
Epoch 750, val loss: 0.8797003030776978
Epoch 760, training loss: 0.6572079062461853 = 0.009101026691496372 + 0.1 * 6.4810686111450195
Epoch 760, val loss: 0.883594810962677
Epoch 770, training loss: 0.6597309112548828 = 0.008781983517110348 + 0.1 * 6.509489059448242
Epoch 770, val loss: 0.8874123692512512
Epoch 780, training loss: 0.6558408141136169 = 0.008481839671730995 + 0.1 * 6.4735894203186035
Epoch 780, val loss: 0.8911257982254028
Epoch 790, training loss: 0.6555284261703491 = 0.008198722265660763 + 0.1 * 6.473296642303467
Epoch 790, val loss: 0.8948452472686768
Epoch 800, training loss: 0.6562030911445618 = 0.007929633371531963 + 0.1 * 6.482734680175781
Epoch 800, val loss: 0.8985068798065186
Epoch 810, training loss: 0.6553834080696106 = 0.007675554137676954 + 0.1 * 6.477078914642334
Epoch 810, val loss: 0.9019730687141418
Epoch 820, training loss: 0.6541635990142822 = 0.007435777690261602 + 0.1 * 6.467278003692627
Epoch 820, val loss: 0.905365526676178
Epoch 830, training loss: 0.6562731266021729 = 0.007207687944173813 + 0.1 * 6.490653991699219
Epoch 830, val loss: 0.908732533454895
Epoch 840, training loss: 0.6531643271446228 = 0.006991439498960972 + 0.1 * 6.461728572845459
Epoch 840, val loss: 0.9120750427246094
Epoch 850, training loss: 0.6564602851867676 = 0.006785855628550053 + 0.1 * 6.496744155883789
Epoch 850, val loss: 0.9153016805648804
Epoch 860, training loss: 0.6522669792175293 = 0.006591056007891893 + 0.1 * 6.456758975982666
Epoch 860, val loss: 0.9184010028839111
Epoch 870, training loss: 0.6510752439498901 = 0.0064057521522045135 + 0.1 * 6.446694374084473
Epoch 870, val loss: 0.9215016961097717
Epoch 880, training loss: 0.6531928181648254 = 0.006228576879948378 + 0.1 * 6.469642162322998
Epoch 880, val loss: 0.9245241284370422
Epoch 890, training loss: 0.6503289937973022 = 0.006060292944312096 + 0.1 * 6.442687034606934
Epoch 890, val loss: 0.927524745464325
Epoch 900, training loss: 0.6504836678504944 = 0.005899840034544468 + 0.1 * 6.44583797454834
Epoch 900, val loss: 0.9304908514022827
Epoch 910, training loss: 0.6498211622238159 = 0.005745753180235624 + 0.1 * 6.440753936767578
Epoch 910, val loss: 0.9333674311637878
Epoch 920, training loss: 0.6492064595222473 = 0.005598824471235275 + 0.1 * 6.4360761642456055
Epoch 920, val loss: 0.9361773729324341
Epoch 930, training loss: 0.6506273150444031 = 0.005458115600049496 + 0.1 * 6.451691627502441
Epoch 930, val loss: 0.9390102028846741
Epoch 940, training loss: 0.6492522358894348 = 0.005323208402842283 + 0.1 * 6.4392900466918945
Epoch 940, val loss: 0.9417350888252258
Epoch 950, training loss: 0.6479605436325073 = 0.005194594617933035 + 0.1 * 6.427659034729004
Epoch 950, val loss: 0.9444189071655273
Epoch 960, training loss: 0.6515710353851318 = 0.005070801824331284 + 0.1 * 6.465002059936523
Epoch 960, val loss: 0.9470630288124084
Epoch 970, training loss: 0.6477628946304321 = 0.004951818846166134 + 0.1 * 6.428110599517822
Epoch 970, val loss: 0.9496655464172363
Epoch 980, training loss: 0.6493953466415405 = 0.004838179796934128 + 0.1 * 6.4455718994140625
Epoch 980, val loss: 0.9522023797035217
Epoch 990, training loss: 0.6476720571517944 = 0.004728755913674831 + 0.1 * 6.429433345794678
Epoch 990, val loss: 0.954778254032135
Epoch 1000, training loss: 0.6472103595733643 = 0.004623886663466692 + 0.1 * 6.4258646965026855
Epoch 1000, val loss: 0.9572300910949707
Epoch 1010, training loss: 0.6458515524864197 = 0.004522914998233318 + 0.1 * 6.413286209106445
Epoch 1010, val loss: 0.9596900939941406
Epoch 1020, training loss: 0.648585319519043 = 0.004425318446010351 + 0.1 * 6.441599369049072
Epoch 1020, val loss: 0.9621288180351257
Epoch 1030, training loss: 0.6457597613334656 = 0.004331311676651239 + 0.1 * 6.414284706115723
Epoch 1030, val loss: 0.9645879864692688
Epoch 1040, training loss: 0.6482678651809692 = 0.004241150338202715 + 0.1 * 6.4402666091918945
Epoch 1040, val loss: 0.9669352173805237
Epoch 1050, training loss: 0.6458914279937744 = 0.004154128488153219 + 0.1 * 6.417372703552246
Epoch 1050, val loss: 0.9691690802574158
Epoch 1060, training loss: 0.6456037759780884 = 0.004070425406098366 + 0.1 * 6.4153337478637695
Epoch 1060, val loss: 0.9715098142623901
Epoch 1070, training loss: 0.6476054191589355 = 0.0039893039502203465 + 0.1 * 6.436161041259766
Epoch 1070, val loss: 0.9737241268157959
Epoch 1080, training loss: 0.6450607776641846 = 0.003911023028194904 + 0.1 * 6.411497592926025
Epoch 1080, val loss: 0.9760076403617859
Epoch 1090, training loss: 0.6449074149131775 = 0.0038356678560376167 + 0.1 * 6.410717010498047
Epoch 1090, val loss: 0.9781606197357178
Epoch 1100, training loss: 0.6454657316207886 = 0.0037624770775437355 + 0.1 * 6.417032241821289
Epoch 1100, val loss: 0.9803707599639893
Epoch 1110, training loss: 0.6436137557029724 = 0.0036918448749929667 + 0.1 * 6.399219036102295
Epoch 1110, val loss: 0.9824798703193665
Epoch 1120, training loss: 0.6454746127128601 = 0.003623413620516658 + 0.1 * 6.418511867523193
Epoch 1120, val loss: 0.9846490025520325
Epoch 1130, training loss: 0.6441096663475037 = 0.0035571109037846327 + 0.1 * 6.4055256843566895
Epoch 1130, val loss: 0.9867458939552307
Epoch 1140, training loss: 0.6445536017417908 = 0.0034932319540530443 + 0.1 * 6.4106035232543945
Epoch 1140, val loss: 0.9888345003128052
Epoch 1150, training loss: 0.6433497667312622 = 0.0034310577902942896 + 0.1 * 6.399186611175537
Epoch 1150, val loss: 0.9908429384231567
Epoch 1160, training loss: 0.6432749032974243 = 0.0033712086733430624 + 0.1 * 6.3990373611450195
Epoch 1160, val loss: 0.9928706288337708
Epoch 1170, training loss: 0.6429488658905029 = 0.0033129132352769375 + 0.1 * 6.396359443664551
Epoch 1170, val loss: 0.9948779344558716
Epoch 1180, training loss: 0.6421381235122681 = 0.003256400115787983 + 0.1 * 6.388817310333252
Epoch 1180, val loss: 0.9968874454498291
Epoch 1190, training loss: 0.643428385257721 = 0.0032015261240303516 + 0.1 * 6.402268409729004
Epoch 1190, val loss: 0.9988877773284912
Epoch 1200, training loss: 0.6421440243721008 = 0.003148179268464446 + 0.1 * 6.389958381652832
Epoch 1200, val loss: 1.0007890462875366
Epoch 1210, training loss: 0.6428102850914001 = 0.003096690634265542 + 0.1 * 6.3971357345581055
Epoch 1210, val loss: 1.0026880502700806
Epoch 1220, training loss: 0.6430187821388245 = 0.0030464224983006716 + 0.1 * 6.399723529815674
Epoch 1220, val loss: 1.0046308040618896
Epoch 1230, training loss: 0.6412493586540222 = 0.0029977853409945965 + 0.1 * 6.382515907287598
Epoch 1230, val loss: 1.006466031074524
Epoch 1240, training loss: 0.6423063278198242 = 0.0029504101257771254 + 0.1 * 6.393558979034424
Epoch 1240, val loss: 1.0083485841751099
Epoch 1250, training loss: 0.6431382894515991 = 0.0029044055845588446 + 0.1 * 6.40233850479126
Epoch 1250, val loss: 1.0101877450942993
Epoch 1260, training loss: 0.640855610370636 = 0.002859838306903839 + 0.1 * 6.379957675933838
Epoch 1260, val loss: 1.0120183229446411
Epoch 1270, training loss: 0.6425825357437134 = 0.0028164591640233994 + 0.1 * 6.397660732269287
Epoch 1270, val loss: 1.0138098001480103
Epoch 1280, training loss: 0.6407650113105774 = 0.002774304011836648 + 0.1 * 6.379907131195068
Epoch 1280, val loss: 1.015602469444275
Epoch 1290, training loss: 0.6403715014457703 = 0.002733194036409259 + 0.1 * 6.376383304595947
Epoch 1290, val loss: 1.017383337020874
Epoch 1300, training loss: 0.64229816198349 = 0.002692975802347064 + 0.1 * 6.396051406860352
Epoch 1300, val loss: 1.0191575288772583
Epoch 1310, training loss: 0.6406198143959045 = 0.0026540118269622326 + 0.1 * 6.379657745361328
Epoch 1310, val loss: 1.0208975076675415
Epoch 1320, training loss: 0.64220130443573 = 0.002616322599351406 + 0.1 * 6.395849704742432
Epoch 1320, val loss: 1.0225797891616821
Epoch 1330, training loss: 0.6394360661506653 = 0.0025794177781790495 + 0.1 * 6.368566513061523
Epoch 1330, val loss: 1.0243068933486938
Epoch 1340, training loss: 0.6404134035110474 = 0.0025435530114918947 + 0.1 * 6.378698348999023
Epoch 1340, val loss: 1.0260095596313477
Epoch 1350, training loss: 0.6394465565681458 = 0.002508407924324274 + 0.1 * 6.369381427764893
Epoch 1350, val loss: 1.027669906616211
Epoch 1360, training loss: 0.6397199630737305 = 0.0024741427041590214 + 0.1 * 6.372457981109619
Epoch 1360, val loss: 1.0293580293655396
Epoch 1370, training loss: 0.6397175192832947 = 0.0024409224279224873 + 0.1 * 6.37276554107666
Epoch 1370, val loss: 1.0310192108154297
Epoch 1380, training loss: 0.6397321820259094 = 0.0024083966854959726 + 0.1 * 6.3732380867004395
Epoch 1380, val loss: 1.0326045751571655
Epoch 1390, training loss: 0.6406726837158203 = 0.0023765945807099342 + 0.1 * 6.382960796356201
Epoch 1390, val loss: 1.0341951847076416
Epoch 1400, training loss: 0.6390959024429321 = 0.0023455710615962744 + 0.1 * 6.3675031661987305
Epoch 1400, val loss: 1.0358870029449463
Epoch 1410, training loss: 0.6398680806159973 = 0.0023153098300099373 + 0.1 * 6.375527858734131
Epoch 1410, val loss: 1.037473440170288
Epoch 1420, training loss: 0.6395947933197021 = 0.0022859845776110888 + 0.1 * 6.373088359832764
Epoch 1420, val loss: 1.0390146970748901
Epoch 1430, training loss: 0.6389201283454895 = 0.0022571133449673653 + 0.1 * 6.3666300773620605
Epoch 1430, val loss: 1.0405915975570679
Epoch 1440, training loss: 0.639016330242157 = 0.002228939440101385 + 0.1 * 6.3678741455078125
Epoch 1440, val loss: 1.042158842086792
Epoch 1450, training loss: 0.638081967830658 = 0.0022015548311173916 + 0.1 * 6.358803749084473
Epoch 1450, val loss: 1.0436811447143555
Epoch 1460, training loss: 0.6411429047584534 = 0.00217475020326674 + 0.1 * 6.389681339263916
Epoch 1460, val loss: 1.0451642274856567
Epoch 1470, training loss: 0.638366162776947 = 0.0021486179903149605 + 0.1 * 6.362175464630127
Epoch 1470, val loss: 1.046703577041626
Epoch 1480, training loss: 0.6380370855331421 = 0.002123055513948202 + 0.1 * 6.359139919281006
Epoch 1480, val loss: 1.0482017993927002
Epoch 1490, training loss: 0.6389060616493225 = 0.002097935415804386 + 0.1 * 6.368081092834473
Epoch 1490, val loss: 1.0496422052383423
Epoch 1500, training loss: 0.6387357115745544 = 0.002073521725833416 + 0.1 * 6.366621971130371
Epoch 1500, val loss: 1.051131010055542
Epoch 1510, training loss: 0.637617826461792 = 0.00204963400028646 + 0.1 * 6.355681896209717
Epoch 1510, val loss: 1.0525928735733032
Epoch 1520, training loss: 0.6388804912567139 = 0.002026099944487214 + 0.1 * 6.368544101715088
Epoch 1520, val loss: 1.0540825128555298
Epoch 1530, training loss: 0.6374788880348206 = 0.0020031710155308247 + 0.1 * 6.354756832122803
Epoch 1530, val loss: 1.0554981231689453
Epoch 1540, training loss: 0.6379392147064209 = 0.001980843720957637 + 0.1 * 6.359583377838135
Epoch 1540, val loss: 1.0569572448730469
Epoch 1550, training loss: 0.6389291286468506 = 0.0019587576389312744 + 0.1 * 6.369703769683838
Epoch 1550, val loss: 1.058325171470642
Epoch 1560, training loss: 0.6369273066520691 = 0.0019372195238247514 + 0.1 * 6.349900722503662
Epoch 1560, val loss: 1.0597678422927856
Epoch 1570, training loss: 0.6384778022766113 = 0.0019161711679771543 + 0.1 * 6.365616321563721
Epoch 1570, val loss: 1.0611584186553955
Epoch 1580, training loss: 0.6368601322174072 = 0.0018956064013764262 + 0.1 * 6.349645137786865
Epoch 1580, val loss: 1.0625070333480835
Epoch 1590, training loss: 0.6376561522483826 = 0.0018754831980913877 + 0.1 * 6.35780668258667
Epoch 1590, val loss: 1.063828945159912
Epoch 1600, training loss: 0.6383879780769348 = 0.0018555950373411179 + 0.1 * 6.365323543548584
Epoch 1600, val loss: 1.065259337425232
Epoch 1610, training loss: 0.6370077133178711 = 0.0018361874390393496 + 0.1 * 6.351715087890625
Epoch 1610, val loss: 1.066557765007019
Epoch 1620, training loss: 0.6377169489860535 = 0.0018172233831137419 + 0.1 * 6.358996868133545
Epoch 1620, val loss: 1.0679452419281006
Epoch 1630, training loss: 0.6372400522232056 = 0.0017985244048759341 + 0.1 * 6.354415416717529
Epoch 1630, val loss: 1.0692617893218994
Epoch 1640, training loss: 0.6368996500968933 = 0.0017802923684939742 + 0.1 * 6.351193904876709
Epoch 1640, val loss: 1.0706278085708618
Epoch 1650, training loss: 0.6369197964668274 = 0.001762394793331623 + 0.1 * 6.351573944091797
Epoch 1650, val loss: 1.0718963146209717
Epoch 1660, training loss: 0.6383149027824402 = 0.0017447331920266151 + 0.1 * 6.365701675415039
Epoch 1660, val loss: 1.0732454061508179
Epoch 1670, training loss: 0.6368454694747925 = 0.0017275870777666569 + 0.1 * 6.3511786460876465
Epoch 1670, val loss: 1.074459433555603
Epoch 1680, training loss: 0.6371304392814636 = 0.001710787764750421 + 0.1 * 6.354196548461914
Epoch 1680, val loss: 1.0757828950881958
Epoch 1690, training loss: 0.635707437992096 = 0.0016942131333053112 + 0.1 * 6.340132236480713
Epoch 1690, val loss: 1.0770376920700073
Epoch 1700, training loss: 0.6380177736282349 = 0.00167793408036232 + 0.1 * 6.363398551940918
Epoch 1700, val loss: 1.0782976150512695
Epoch 1710, training loss: 0.6356603503227234 = 0.001661996589973569 + 0.1 * 6.3399834632873535
Epoch 1710, val loss: 1.0795544385910034
Epoch 1720, training loss: 0.6369866728782654 = 0.00164636573754251 + 0.1 * 6.353403091430664
Epoch 1720, val loss: 1.080804467201233
Epoch 1730, training loss: 0.6371244788169861 = 0.0016308709746226668 + 0.1 * 6.354936122894287
Epoch 1730, val loss: 1.0820636749267578
Epoch 1740, training loss: 0.635880172252655 = 0.001615780172869563 + 0.1 * 6.342644214630127
Epoch 1740, val loss: 1.08328115940094
Epoch 1750, training loss: 0.6369978785514832 = 0.0016009759856387973 + 0.1 * 6.353968620300293
Epoch 1750, val loss: 1.0845122337341309
Epoch 1760, training loss: 0.636345386505127 = 0.0015864999732002616 + 0.1 * 6.347589015960693
Epoch 1760, val loss: 1.0856938362121582
Epoch 1770, training loss: 0.636122465133667 = 0.0015722047537565231 + 0.1 * 6.3455023765563965
Epoch 1770, val loss: 1.0868903398513794
Epoch 1780, training loss: 0.6357883214950562 = 0.001558117102831602 + 0.1 * 6.342301845550537
Epoch 1780, val loss: 1.0880857706069946
Epoch 1790, training loss: 0.6356561779975891 = 0.001544275088235736 + 0.1 * 6.341118812561035
Epoch 1790, val loss: 1.0892934799194336
Epoch 1800, training loss: 0.637030839920044 = 0.0015307263238355517 + 0.1 * 6.355001449584961
Epoch 1800, val loss: 1.0904501676559448
Epoch 1810, training loss: 0.6355132460594177 = 0.0015173149295151234 + 0.1 * 6.339959144592285
Epoch 1810, val loss: 1.0916812419891357
Epoch 1820, training loss: 0.6373569369316101 = 0.0015042255399748683 + 0.1 * 6.358526706695557
Epoch 1820, val loss: 1.0927973985671997
Epoch 1830, training loss: 0.6351079344749451 = 0.0014913809718564153 + 0.1 * 6.336165428161621
Epoch 1830, val loss: 1.0939950942993164
Epoch 1840, training loss: 0.6349790096282959 = 0.0014787178952246904 + 0.1 * 6.335002422332764
Epoch 1840, val loss: 1.095122218132019
Epoch 1850, training loss: 0.6360230445861816 = 0.0014661707682535052 + 0.1 * 6.345568656921387
Epoch 1850, val loss: 1.0962985754013062
Epoch 1860, training loss: 0.6357048153877258 = 0.0014538827817887068 + 0.1 * 6.3425092697143555
Epoch 1860, val loss: 1.0973963737487793
Epoch 1870, training loss: 0.6356175541877747 = 0.001441827742382884 + 0.1 * 6.341757297515869
Epoch 1870, val loss: 1.0985896587371826
Epoch 1880, training loss: 0.6342682242393494 = 0.0014299280010163784 + 0.1 * 6.328382968902588
Epoch 1880, val loss: 1.0997244119644165
Epoch 1890, training loss: 0.6363303065299988 = 0.0014181778533384204 + 0.1 * 6.34912109375
Epoch 1890, val loss: 1.1008154153823853
Epoch 1900, training loss: 0.6360437870025635 = 0.0014066242147237062 + 0.1 * 6.346371650695801
Epoch 1900, val loss: 1.1019164323806763
Epoch 1910, training loss: 0.6352110505104065 = 0.0013953158631920815 + 0.1 * 6.338156700134277
Epoch 1910, val loss: 1.103039026260376
Epoch 1920, training loss: 0.6352089047431946 = 0.001384193543344736 + 0.1 * 6.338246822357178
Epoch 1920, val loss: 1.1041749715805054
Epoch 1930, training loss: 0.6347233057022095 = 0.0013732485240325332 + 0.1 * 6.333500862121582
Epoch 1930, val loss: 1.1052446365356445
Epoch 1940, training loss: 0.6364114880561829 = 0.0013624564744532108 + 0.1 * 6.350490093231201
Epoch 1940, val loss: 1.1063097715377808
Epoch 1950, training loss: 0.6342760324478149 = 0.0013518895721063018 + 0.1 * 6.3292412757873535
Epoch 1950, val loss: 1.107373595237732
Epoch 1960, training loss: 0.6348128318786621 = 0.0013414504937827587 + 0.1 * 6.334713459014893
Epoch 1960, val loss: 1.1084513664245605
Epoch 1970, training loss: 0.6348643898963928 = 0.0013310754438862205 + 0.1 * 6.335332870483398
Epoch 1970, val loss: 1.1095647811889648
Epoch 1980, training loss: 0.6341043710708618 = 0.001320963609032333 + 0.1 * 6.327834129333496
Epoch 1980, val loss: 1.1106613874435425
Epoch 1990, training loss: 0.63490891456604 = 0.001310995896346867 + 0.1 * 6.335978984832764
Epoch 1990, val loss: 1.1116560697555542
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.8365840801265156
The final CL Acc:0.83457, 0.00462, The final GNN Acc:0.83764, 0.00188
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10604])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8031702041625977 = 1.9434837102890015 + 0.1 * 8.5968656539917
Epoch 0, val loss: 1.9433410167694092
Epoch 10, training loss: 2.793649673461914 = 1.9339712858200073 + 0.1 * 8.596782684326172
Epoch 10, val loss: 1.934294581413269
Epoch 20, training loss: 2.782254695892334 = 1.9226250648498535 + 0.1 * 8.596296310424805
Epoch 20, val loss: 1.9232383966445923
Epoch 30, training loss: 2.766078233718872 = 1.9069409370422363 + 0.1 * 8.5913724899292
Epoch 30, val loss: 1.9077587127685547
Epoch 40, training loss: 2.7382097244262695 = 1.8837908506393433 + 0.1 * 8.544187545776367
Epoch 40, val loss: 1.8850868940353394
Epoch 50, training loss: 2.6711854934692383 = 1.852508783340454 + 0.1 * 8.186765670776367
Epoch 50, val loss: 1.8558768033981323
Epoch 60, training loss: 2.6154680252075195 = 1.8199830055236816 + 0.1 * 7.954850196838379
Epoch 60, val loss: 1.8281201124191284
Epoch 70, training loss: 2.550355911254883 = 1.7930798530578613 + 0.1 * 7.572761535644531
Epoch 70, val loss: 1.8064360618591309
Epoch 80, training loss: 2.489558219909668 = 1.7672011852264404 + 0.1 * 7.223570823669434
Epoch 80, val loss: 1.7835737466812134
Epoch 90, training loss: 2.4414563179016113 = 1.7341433763504028 + 0.1 * 7.073128700256348
Epoch 90, val loss: 1.7550110816955566
Epoch 100, training loss: 2.3869526386260986 = 1.6896486282348633 + 0.1 * 6.973040580749512
Epoch 100, val loss: 1.719366431236267
Epoch 110, training loss: 2.321013927459717 = 1.6305203437805176 + 0.1 * 6.904935836791992
Epoch 110, val loss: 1.6728875637054443
Epoch 120, training loss: 2.2425708770751953 = 1.557127833366394 + 0.1 * 6.854430198669434
Epoch 120, val loss: 1.6147842407226562
Epoch 130, training loss: 2.155632972717285 = 1.4737240076065063 + 0.1 * 6.819089889526367
Epoch 130, val loss: 1.5500504970550537
Epoch 140, training loss: 2.068486452102661 = 1.3887380361557007 + 0.1 * 6.797484874725342
Epoch 140, val loss: 1.4861875772476196
Epoch 150, training loss: 1.9819183349609375 = 1.3038092851638794 + 0.1 * 6.781089782714844
Epoch 150, val loss: 1.4232391119003296
Epoch 160, training loss: 1.8938692808151245 = 1.2171485424041748 + 0.1 * 6.767207145690918
Epoch 160, val loss: 1.3608229160308838
Epoch 170, training loss: 1.8055953979492188 = 1.129396677017212 + 0.1 * 6.76198673248291
Epoch 170, val loss: 1.2984696626663208
Epoch 180, training loss: 1.7197144031524658 = 1.0450037717819214 + 0.1 * 6.747106075286865
Epoch 180, val loss: 1.2402215003967285
Epoch 190, training loss: 1.6380480527877808 = 0.9645243883132935 + 0.1 * 6.735236644744873
Epoch 190, val loss: 1.185723066329956
Epoch 200, training loss: 1.5608806610107422 = 0.8882512450218201 + 0.1 * 6.726294040679932
Epoch 200, val loss: 1.1354116201400757
Epoch 210, training loss: 1.4879846572875977 = 0.8163530826568604 + 0.1 * 6.716315746307373
Epoch 210, val loss: 1.0888571739196777
Epoch 220, training loss: 1.4178156852722168 = 0.7475771903991699 + 0.1 * 6.702383995056152
Epoch 220, val loss: 1.0448118448257446
Epoch 230, training loss: 1.3526771068572998 = 0.6833648681640625 + 0.1 * 6.693122863769531
Epoch 230, val loss: 1.0044097900390625
Epoch 240, training loss: 1.2933354377746582 = 0.6249523162841797 + 0.1 * 6.683830261230469
Epoch 240, val loss: 0.9692271947860718
Epoch 250, training loss: 1.240645170211792 = 0.5722318291664124 + 0.1 * 6.684133052825928
Epoch 250, val loss: 0.9392428398132324
Epoch 260, training loss: 1.1918859481811523 = 0.5251507759094238 + 0.1 * 6.667351245880127
Epoch 260, val loss: 0.9152858257293701
Epoch 270, training loss: 1.1482899188995361 = 0.4823234975337982 + 0.1 * 6.659664154052734
Epoch 270, val loss: 0.8961670398712158
Epoch 280, training loss: 1.1078171730041504 = 0.4428199529647827 + 0.1 * 6.649972915649414
Epoch 280, val loss: 0.8813806176185608
Epoch 290, training loss: 1.069053292274475 = 0.4053950011730194 + 0.1 * 6.636582851409912
Epoch 290, val loss: 0.8695768713951111
Epoch 300, training loss: 1.0321826934814453 = 0.36924242973327637 + 0.1 * 6.629402160644531
Epoch 300, val loss: 0.8601396679878235
Epoch 310, training loss: 0.9994300603866577 = 0.33412161469459534 + 0.1 * 6.6530842781066895
Epoch 310, val loss: 0.8524497151374817
Epoch 320, training loss: 0.9630328416824341 = 0.3006892204284668 + 0.1 * 6.623435974121094
Epoch 320, val loss: 0.8467680811882019
Epoch 330, training loss: 0.9319007396697998 = 0.2693637013435364 + 0.1 * 6.625370502471924
Epoch 330, val loss: 0.8429622650146484
Epoch 340, training loss: 0.9016143083572388 = 0.24062123894691467 + 0.1 * 6.609930038452148
Epoch 340, val loss: 0.8418058156967163
Epoch 350, training loss: 0.8745107054710388 = 0.214644655585289 + 0.1 * 6.598659992218018
Epoch 350, val loss: 0.8431656956672668
Epoch 360, training loss: 0.8536566495895386 = 0.1914771944284439 + 0.1 * 6.621794700622559
Epoch 360, val loss: 0.8471011519432068
Epoch 370, training loss: 0.8314708471298218 = 0.171230286359787 + 0.1 * 6.602406024932861
Epoch 370, val loss: 0.8532583117485046
Epoch 380, training loss: 0.811877965927124 = 0.153553307056427 + 0.1 * 6.583246231079102
Epoch 380, val loss: 0.8614384531974792
Epoch 390, training loss: 0.7982555031776428 = 0.13811802864074707 + 0.1 * 6.601374626159668
Epoch 390, val loss: 0.8713762760162354
Epoch 400, training loss: 0.782336413860321 = 0.12472955882549286 + 0.1 * 6.57606840133667
Epoch 400, val loss: 0.8826034665107727
Epoch 410, training loss: 0.7698475122451782 = 0.11303965747356415 + 0.1 * 6.56807804107666
Epoch 410, val loss: 0.8949596285820007
Epoch 420, training loss: 0.7603242993354797 = 0.10280559957027435 + 0.1 * 6.575186729431152
Epoch 420, val loss: 0.9080796241760254
Epoch 430, training loss: 0.7495242357254028 = 0.09380907565355301 + 0.1 * 6.5571513175964355
Epoch 430, val loss: 0.9218249320983887
Epoch 440, training loss: 0.742469072341919 = 0.08585889637470245 + 0.1 * 6.566101551055908
Epoch 440, val loss: 0.9358760118484497
Epoch 450, training loss: 0.7341866493225098 = 0.07882454246282578 + 0.1 * 6.553621292114258
Epoch 450, val loss: 0.9502528309822083
Epoch 460, training loss: 0.7266036868095398 = 0.0725414827466011 + 0.1 * 6.540622234344482
Epoch 460, val loss: 0.9647413492202759
Epoch 470, training loss: 0.7228477001190186 = 0.06690483540296555 + 0.1 * 6.559428691864014
Epoch 470, val loss: 0.9792852401733398
Epoch 480, training loss: 0.7154959440231323 = 0.06185479462146759 + 0.1 * 6.536411285400391
Epoch 480, val loss: 0.9938197135925293
Epoch 490, training loss: 0.7106105089187622 = 0.057291340082883835 + 0.1 * 6.533191680908203
Epoch 490, val loss: 1.0082892179489136
Epoch 500, training loss: 0.7072932720184326 = 0.05315661430358887 + 0.1 * 6.5413665771484375
Epoch 500, val loss: 1.0226467847824097
Epoch 510, training loss: 0.7016761898994446 = 0.04941614344716072 + 0.1 * 6.5226006507873535
Epoch 510, val loss: 1.0369006395339966
Epoch 520, training loss: 0.6977064609527588 = 0.04601136967539787 + 0.1 * 6.516950607299805
Epoch 520, val loss: 1.0509191751480103
Epoch 530, training loss: 0.6958885788917542 = 0.04290972650051117 + 0.1 * 6.529788494110107
Epoch 530, val loss: 1.0648008584976196
Epoch 540, training loss: 0.690985918045044 = 0.04008564352989197 + 0.1 * 6.509002208709717
Epoch 540, val loss: 1.0783700942993164
Epoch 550, training loss: 0.6883121132850647 = 0.03750938922166824 + 0.1 * 6.508027076721191
Epoch 550, val loss: 1.091834306716919
Epoch 560, training loss: 0.6853087544441223 = 0.03515571355819702 + 0.1 * 6.501530170440674
Epoch 560, val loss: 1.1049127578735352
Epoch 570, training loss: 0.6826927661895752 = 0.03300881385803223 + 0.1 * 6.49683952331543
Epoch 570, val loss: 1.1176860332489014
Epoch 580, training loss: 0.6803433299064636 = 0.031049704179167747 + 0.1 * 6.492936134338379
Epoch 580, val loss: 1.1302127838134766
Epoch 590, training loss: 0.6785139441490173 = 0.02925148792564869 + 0.1 * 6.492624282836914
Epoch 590, val loss: 1.142390251159668
Epoch 600, training loss: 0.6761090755462646 = 0.027598947286605835 + 0.1 * 6.485101699829102
Epoch 600, val loss: 1.154358148574829
Epoch 610, training loss: 0.6762189865112305 = 0.026079028844833374 + 0.1 * 6.501399040222168
Epoch 610, val loss: 1.165967345237732
Epoch 620, training loss: 0.6755642294883728 = 0.02469017542898655 + 0.1 * 6.508740425109863
Epoch 620, val loss: 1.1771308183670044
Epoch 630, training loss: 0.6714199185371399 = 0.02341340482234955 + 0.1 * 6.480064868927002
Epoch 630, val loss: 1.188129186630249
Epoch 640, training loss: 0.6717206835746765 = 0.022232195362448692 + 0.1 * 6.494884967803955
Epoch 640, val loss: 1.1987422704696655
Epoch 650, training loss: 0.6694599390029907 = 0.021143123507499695 + 0.1 * 6.48316764831543
Epoch 650, val loss: 1.2090606689453125
Epoch 660, training loss: 0.6675060987472534 = 0.02013484016060829 + 0.1 * 6.473711967468262
Epoch 660, val loss: 1.2192764282226562
Epoch 670, training loss: 0.6668302416801453 = 0.019198855385184288 + 0.1 * 6.476314067840576
Epoch 670, val loss: 1.2289965152740479
Epoch 680, training loss: 0.6657007336616516 = 0.018331488594412804 + 0.1 * 6.473692417144775
Epoch 680, val loss: 1.238576889038086
Epoch 690, training loss: 0.6641551852226257 = 0.017525557428598404 + 0.1 * 6.466296195983887
Epoch 690, val loss: 1.2479255199432373
Epoch 700, training loss: 0.6627168655395508 = 0.016772713512182236 + 0.1 * 6.459441661834717
Epoch 700, val loss: 1.2571039199829102
Epoch 710, training loss: 0.6625581979751587 = 0.016069361940026283 + 0.1 * 6.464888572692871
Epoch 710, val loss: 1.2659025192260742
Epoch 720, training loss: 0.6613426208496094 = 0.015413538552820683 + 0.1 * 6.459290981292725
Epoch 720, val loss: 1.2746549844741821
Epoch 730, training loss: 0.660216748714447 = 0.014798631891608238 + 0.1 * 6.454180717468262
Epoch 730, val loss: 1.283125638961792
Epoch 740, training loss: 0.660828173160553 = 0.014220788143575191 + 0.1 * 6.466073989868164
Epoch 740, val loss: 1.291386604309082
Epoch 750, training loss: 0.6589401364326477 = 0.013681289739906788 + 0.1 * 6.452588081359863
Epoch 750, val loss: 1.2994304895401
Epoch 760, training loss: 0.6584551334381104 = 0.013172856532037258 + 0.1 * 6.452822685241699
Epoch 760, val loss: 1.3074074983596802
Epoch 770, training loss: 0.6575159430503845 = 0.012695074081420898 + 0.1 * 6.448208808898926
Epoch 770, val loss: 1.315038800239563
Epoch 780, training loss: 0.6574117541313171 = 0.01224377192556858 + 0.1 * 6.451679706573486
Epoch 780, val loss: 1.3225843906402588
Epoch 790, training loss: 0.6564322113990784 = 0.011819908395409584 + 0.1 * 6.446123123168945
Epoch 790, val loss: 1.329921841621399
Epoch 800, training loss: 0.6555501222610474 = 0.011418864130973816 + 0.1 * 6.441312313079834
Epoch 800, val loss: 1.337204933166504
Epoch 810, training loss: 0.6552789807319641 = 0.011038952507078648 + 0.1 * 6.442399978637695
Epoch 810, val loss: 1.3442109823226929
Epoch 820, training loss: 0.6554586887359619 = 0.010679921135306358 + 0.1 * 6.447787761688232
Epoch 820, val loss: 1.351130723953247
Epoch 830, training loss: 0.656749427318573 = 0.010339196771383286 + 0.1 * 6.464102268218994
Epoch 830, val loss: 1.357935905456543
Epoch 840, training loss: 0.6534146666526794 = 0.010017650201916695 + 0.1 * 6.433969974517822
Epoch 840, val loss: 1.3644137382507324
Epoch 850, training loss: 0.6525198817253113 = 0.009711270220577717 + 0.1 * 6.428086280822754
Epoch 850, val loss: 1.3710179328918457
Epoch 860, training loss: 0.6541413068771362 = 0.009420235641300678 + 0.1 * 6.447210788726807
Epoch 860, val loss: 1.3773225545883179
Epoch 870, training loss: 0.6520999073982239 = 0.009143217466771603 + 0.1 * 6.429566383361816
Epoch 870, val loss: 1.3835043907165527
Epoch 880, training loss: 0.6524225473403931 = 0.008880061097443104 + 0.1 * 6.4354248046875
Epoch 880, val loss: 1.3897358179092407
Epoch 890, training loss: 0.6508417129516602 = 0.008628548122942448 + 0.1 * 6.422131538391113
Epoch 890, val loss: 1.3956718444824219
Epoch 900, training loss: 0.6517178416252136 = 0.008389356546103954 + 0.1 * 6.433284759521484
Epoch 900, val loss: 1.4015233516693115
Epoch 910, training loss: 0.6503239870071411 = 0.008160265162587166 + 0.1 * 6.421637058258057
Epoch 910, val loss: 1.40725576877594
Epoch 920, training loss: 0.6513753533363342 = 0.007942602038383484 + 0.1 * 6.434327125549316
Epoch 920, val loss: 1.4130016565322876
Epoch 930, training loss: 0.6497089266777039 = 0.007734287064522505 + 0.1 * 6.419746398925781
Epoch 930, val loss: 1.418484091758728
Epoch 940, training loss: 0.6495094299316406 = 0.007533842697739601 + 0.1 * 6.419755935668945
Epoch 940, val loss: 1.4240366220474243
Epoch 950, training loss: 0.6505375504493713 = 0.007343025878071785 + 0.1 * 6.431944847106934
Epoch 950, val loss: 1.4292762279510498
Epoch 960, training loss: 0.6486765742301941 = 0.007159953471273184 + 0.1 * 6.415165901184082
Epoch 960, val loss: 1.4344525337219238
Epoch 970, training loss: 0.6484943628311157 = 0.006985358893871307 + 0.1 * 6.4150896072387695
Epoch 970, val loss: 1.4397367238998413
Epoch 980, training loss: 0.6488608121871948 = 0.006816675420850515 + 0.1 * 6.420441150665283
Epoch 980, val loss: 1.4448353052139282
Epoch 990, training loss: 0.6481704711914062 = 0.006655698176473379 + 0.1 * 6.41514778137207
Epoch 990, val loss: 1.4496570825576782
Epoch 1000, training loss: 0.6474534869194031 = 0.006500401999801397 + 0.1 * 6.4095306396484375
Epoch 1000, val loss: 1.4545691013336182
Epoch 1010, training loss: 0.6476061344146729 = 0.006351742893457413 + 0.1 * 6.412543296813965
Epoch 1010, val loss: 1.459439754486084
Epoch 1020, training loss: 0.6471512317657471 = 0.0062082367949187756 + 0.1 * 6.409430027008057
Epoch 1020, val loss: 1.4641491174697876
Epoch 1030, training loss: 0.6464728713035583 = 0.006070110481232405 + 0.1 * 6.404026985168457
Epoch 1030, val loss: 1.468843936920166
Epoch 1040, training loss: 0.6481709480285645 = 0.005937679670751095 + 0.1 * 6.422332286834717
Epoch 1040, val loss: 1.4733914136886597
Epoch 1050, training loss: 0.6460865139961243 = 0.005810154601931572 + 0.1 * 6.402763366699219
Epoch 1050, val loss: 1.4777908325195312
Epoch 1060, training loss: 0.6464650630950928 = 0.005687173455953598 + 0.1 * 6.407778739929199
Epoch 1060, val loss: 1.4823002815246582
Epoch 1070, training loss: 0.6464037895202637 = 0.0055686188861727715 + 0.1 * 6.408351421356201
Epoch 1070, val loss: 1.4866514205932617
Epoch 1080, training loss: 0.6448413133621216 = 0.005454261787235737 + 0.1 * 6.3938703536987305
Epoch 1080, val loss: 1.4909547567367554
Epoch 1090, training loss: 0.6474979519844055 = 0.00534330727532506 + 0.1 * 6.42154598236084
Epoch 1090, val loss: 1.4952465295791626
Epoch 1100, training loss: 0.6458113789558411 = 0.00523723941296339 + 0.1 * 6.405741214752197
Epoch 1100, val loss: 1.4992408752441406
Epoch 1110, training loss: 0.6449589729309082 = 0.0051340083591639996 + 0.1 * 6.398249626159668
Epoch 1110, val loss: 1.5033961534500122
Epoch 1120, training loss: 0.6443354487419128 = 0.005035074427723885 + 0.1 * 6.393003463745117
Epoch 1120, val loss: 1.5074225664138794
Epoch 1130, training loss: 0.645214319229126 = 0.004938866477459669 + 0.1 * 6.402754783630371
Epoch 1130, val loss: 1.511417031288147
Epoch 1140, training loss: 0.6437435150146484 = 0.0048457300290465355 + 0.1 * 6.388977527618408
Epoch 1140, val loss: 1.5152610540390015
Epoch 1150, training loss: 0.6445672512054443 = 0.004755766596645117 + 0.1 * 6.398114204406738
Epoch 1150, val loss: 1.5190985202789307
Epoch 1160, training loss: 0.6436732411384583 = 0.004669007379561663 + 0.1 * 6.390042304992676
Epoch 1160, val loss: 1.522841215133667
Epoch 1170, training loss: 0.6434120535850525 = 0.004584906622767448 + 0.1 * 6.388271331787109
Epoch 1170, val loss: 1.526642918586731
Epoch 1180, training loss: 0.6441827416419983 = 0.004503149073570967 + 0.1 * 6.396795749664307
Epoch 1180, val loss: 1.530362844467163
Epoch 1190, training loss: 0.6443248987197876 = 0.004424287471920252 + 0.1 * 6.399006366729736
Epoch 1190, val loss: 1.5339735746383667
Epoch 1200, training loss: 0.6425108313560486 = 0.004347543232142925 + 0.1 * 6.381632328033447
Epoch 1200, val loss: 1.5374095439910889
Epoch 1210, training loss: 0.6431196331977844 = 0.004273359198123217 + 0.1 * 6.388462543487549
Epoch 1210, val loss: 1.541003704071045
Epoch 1220, training loss: 0.6430177688598633 = 0.0042016287334263325 + 0.1 * 6.3881611824035645
Epoch 1220, val loss: 1.544374942779541
Epoch 1230, training loss: 0.6426163911819458 = 0.004131757654249668 + 0.1 * 6.384846210479736
Epoch 1230, val loss: 1.5477689504623413
Epoch 1240, training loss: 0.6440396308898926 = 0.004063926171511412 + 0.1 * 6.399757385253906
Epoch 1240, val loss: 1.5511847734451294
Epoch 1250, training loss: 0.6419007778167725 = 0.003998139873147011 + 0.1 * 6.379025936126709
Epoch 1250, val loss: 1.5543848276138306
Epoch 1260, training loss: 0.64264315366745 = 0.003934261854737997 + 0.1 * 6.387088775634766
Epoch 1260, val loss: 1.5577294826507568
Epoch 1270, training loss: 0.6425854563713074 = 0.003872147761285305 + 0.1 * 6.38713264465332
Epoch 1270, val loss: 1.5608998537063599
Epoch 1280, training loss: 0.6415846347808838 = 0.0038116418290883303 + 0.1 * 6.377729892730713
Epoch 1280, val loss: 1.5640499591827393
Epoch 1290, training loss: 0.6415475606918335 = 0.003752894466742873 + 0.1 * 6.377946376800537
Epoch 1290, val loss: 1.5673099756240845
Epoch 1300, training loss: 0.6416345834732056 = 0.0036957645788788795 + 0.1 * 6.379387855529785
Epoch 1300, val loss: 1.5703606605529785
Epoch 1310, training loss: 0.640809953212738 = 0.0036401748657226562 + 0.1 * 6.371697902679443
Epoch 1310, val loss: 1.5733537673950195
Epoch 1320, training loss: 0.6428486108779907 = 0.0035863935481756926 + 0.1 * 6.392621994018555
Epoch 1320, val loss: 1.5764273405075073
Epoch 1330, training loss: 0.6403384208679199 = 0.003533922368660569 + 0.1 * 6.368044853210449
Epoch 1330, val loss: 1.579261064529419
Epoch 1340, training loss: 0.6406339406967163 = 0.0034825566690415144 + 0.1 * 6.371513366699219
Epoch 1340, val loss: 1.5822553634643555
Epoch 1350, training loss: 0.640660285949707 = 0.003432827303186059 + 0.1 * 6.372274398803711
Epoch 1350, val loss: 1.5852136611938477
Epoch 1360, training loss: 0.6402602195739746 = 0.0033843915443867445 + 0.1 * 6.368758201599121
Epoch 1360, val loss: 1.5880059003829956
Epoch 1370, training loss: 0.6406769156455994 = 0.0033371157478541136 + 0.1 * 6.3733978271484375
Epoch 1370, val loss: 1.5908422470092773
Epoch 1380, training loss: 0.6414726376533508 = 0.0032912709284573793 + 0.1 * 6.3818135261535645
Epoch 1380, val loss: 1.5936226844787598
Epoch 1390, training loss: 0.6405042409896851 = 0.003246124368160963 + 0.1 * 6.3725810050964355
Epoch 1390, val loss: 1.5963150262832642
Epoch 1400, training loss: 0.639031708240509 = 0.0032023668754845858 + 0.1 * 6.358293056488037
Epoch 1400, val loss: 1.5990670919418335
Epoch 1410, training loss: 0.6414394378662109 = 0.003159831976518035 + 0.1 * 6.382796287536621
Epoch 1410, val loss: 1.601866602897644
Epoch 1420, training loss: 0.6391478180885315 = 0.00311807612888515 + 0.1 * 6.360297203063965
Epoch 1420, val loss: 1.604419231414795
Epoch 1430, training loss: 0.640113115310669 = 0.003077550558373332 + 0.1 * 6.370355129241943
Epoch 1430, val loss: 1.6070585250854492
Epoch 1440, training loss: 0.6393632292747498 = 0.003037866670638323 + 0.1 * 6.363253593444824
Epoch 1440, val loss: 1.609637975692749
Epoch 1450, training loss: 0.6391624212265015 = 0.0029989329632371664 + 0.1 * 6.361634731292725
Epoch 1450, val loss: 1.6122124195098877
Epoch 1460, training loss: 0.6391789317131042 = 0.0029612400103360415 + 0.1 * 6.362176895141602
Epoch 1460, val loss: 1.6147637367248535
Epoch 1470, training loss: 0.6398913860321045 = 0.0029243838507682085 + 0.1 * 6.3696699142456055
Epoch 1470, val loss: 1.6172096729278564
Epoch 1480, training loss: 0.6385167241096497 = 0.0028883039485663176 + 0.1 * 6.356284141540527
Epoch 1480, val loss: 1.6197148561477661
Epoch 1490, training loss: 0.6393797397613525 = 0.0028530603740364313 + 0.1 * 6.365266799926758
Epoch 1490, val loss: 1.6222667694091797
Epoch 1500, training loss: 0.6380735039710999 = 0.0028186810668557882 + 0.1 * 6.352547645568848
Epoch 1500, val loss: 1.6246740818023682
Epoch 1510, training loss: 0.6392667889595032 = 0.0027849068865180016 + 0.1 * 6.364818572998047
Epoch 1510, val loss: 1.6269738674163818
Epoch 1520, training loss: 0.6383536458015442 = 0.0027519571594893932 + 0.1 * 6.356016635894775
Epoch 1520, val loss: 1.629255771636963
Epoch 1530, training loss: 0.6391038298606873 = 0.002719938289374113 + 0.1 * 6.3638386726379395
Epoch 1530, val loss: 1.631676197052002
Epoch 1540, training loss: 0.6376398801803589 = 0.0026883529499173164 + 0.1 * 6.349514961242676
Epoch 1540, val loss: 1.6339340209960938
Epoch 1550, training loss: 0.6387307047843933 = 0.002657645381987095 + 0.1 * 6.3607306480407715
Epoch 1550, val loss: 1.6361851692199707
Epoch 1560, training loss: 0.6378722786903381 = 0.0026274919509887695 + 0.1 * 6.352447986602783
Epoch 1560, val loss: 1.638420581817627
Epoch 1570, training loss: 0.6375443339347839 = 0.0025981252547353506 + 0.1 * 6.349462032318115
Epoch 1570, val loss: 1.6406841278076172
Epoch 1580, training loss: 0.6378239393234253 = 0.002569163218140602 + 0.1 * 6.352547645568848
Epoch 1580, val loss: 1.642927885055542
Epoch 1590, training loss: 0.637769341468811 = 0.002540999324992299 + 0.1 * 6.352283000946045
Epoch 1590, val loss: 1.6450071334838867
Epoch 1600, training loss: 0.6379144191741943 = 0.0025132475420832634 + 0.1 * 6.354011535644531
Epoch 1600, val loss: 1.6471714973449707
Epoch 1610, training loss: 0.6377919912338257 = 0.0024862713180482388 + 0.1 * 6.353056907653809
Epoch 1610, val loss: 1.6493762731552124
Epoch 1620, training loss: 0.6363791227340698 = 0.002459682757034898 + 0.1 * 6.339194297790527
Epoch 1620, val loss: 1.6514455080032349
Epoch 1630, training loss: 0.6387493014335632 = 0.0024337104987353086 + 0.1 * 6.363155841827393
Epoch 1630, val loss: 1.6535747051239014
Epoch 1640, training loss: 0.6364231705665588 = 0.0024082153104245663 + 0.1 * 6.340149402618408
Epoch 1640, val loss: 1.6555660963058472
Epoch 1650, training loss: 0.6370041370391846 = 0.0023832020815461874 + 0.1 * 6.346209526062012
Epoch 1650, val loss: 1.6577050685882568
Epoch 1660, training loss: 0.6375074982643127 = 0.002358808880671859 + 0.1 * 6.351487159729004
Epoch 1660, val loss: 1.6597111225128174
Epoch 1670, training loss: 0.6361733078956604 = 0.0023346450179815292 + 0.1 * 6.338386535644531
Epoch 1670, val loss: 1.6616476774215698
Epoch 1680, training loss: 0.6372522115707397 = 0.0023111868649721146 + 0.1 * 6.349410057067871
Epoch 1680, val loss: 1.6637771129608154
Epoch 1690, training loss: 0.6381159424781799 = 0.002288187388330698 + 0.1 * 6.358277797698975
Epoch 1690, val loss: 1.6656790971755981
Epoch 1700, training loss: 0.6371290683746338 = 0.002265448682010174 + 0.1 * 6.348636150360107
Epoch 1700, val loss: 1.667538046836853
Epoch 1710, training loss: 0.6373925805091858 = 0.002243421971797943 + 0.1 * 6.351491451263428
Epoch 1710, val loss: 1.6694773435592651
Epoch 1720, training loss: 0.6356353163719177 = 0.002221637638285756 + 0.1 * 6.334136486053467
Epoch 1720, val loss: 1.6714707612991333
Epoch 1730, training loss: 0.6368734836578369 = 0.0022002405021339655 + 0.1 * 6.346732139587402
Epoch 1730, val loss: 1.6734520196914673
Epoch 1740, training loss: 0.6363496780395508 = 0.002179225906729698 + 0.1 * 6.341704368591309
Epoch 1740, val loss: 1.675268530845642
Epoch 1750, training loss: 0.6373523473739624 = 0.0021586092188954353 + 0.1 * 6.351937294006348
Epoch 1750, val loss: 1.6771509647369385
Epoch 1760, training loss: 0.6359388828277588 = 0.0021384647116065025 + 0.1 * 6.3380045890808105
Epoch 1760, val loss: 1.6788897514343262
Epoch 1770, training loss: 0.6357718110084534 = 0.002118621952831745 + 0.1 * 6.336531639099121
Epoch 1770, val loss: 1.6807807683944702
Epoch 1780, training loss: 0.6361575126647949 = 0.002099116099998355 + 0.1 * 6.340583801269531
Epoch 1780, val loss: 1.6826943159103394
Epoch 1790, training loss: 0.6358808875083923 = 0.0020799802150577307 + 0.1 * 6.338008880615234
Epoch 1790, val loss: 1.6844336986541748
Epoch 1800, training loss: 0.6360857486724854 = 0.0020610406063497066 + 0.1 * 6.340246677398682
Epoch 1800, val loss: 1.6862057447433472
Epoch 1810, training loss: 0.6356947422027588 = 0.0020426863338798285 + 0.1 * 6.336520195007324
Epoch 1810, val loss: 1.6878949403762817
Epoch 1820, training loss: 0.6365115642547607 = 0.0020246244966983795 + 0.1 * 6.344869136810303
Epoch 1820, val loss: 1.6896488666534424
Epoch 1830, training loss: 0.6363723278045654 = 0.002006767550483346 + 0.1 * 6.343655586242676
Epoch 1830, val loss: 1.6913578510284424
Epoch 1840, training loss: 0.634986937046051 = 0.001989225158467889 + 0.1 * 6.329977035522461
Epoch 1840, val loss: 1.6930809020996094
Epoch 1850, training loss: 0.6362336874008179 = 0.001972096273675561 + 0.1 * 6.342616081237793
Epoch 1850, val loss: 1.694897174835205
Epoch 1860, training loss: 0.635232150554657 = 0.0019549874123185873 + 0.1 * 6.3327717781066895
Epoch 1860, val loss: 1.6965055465698242
Epoch 1870, training loss: 0.6356773376464844 = 0.001938451430760324 + 0.1 * 6.337388515472412
Epoch 1870, val loss: 1.6981494426727295
Epoch 1880, training loss: 0.63514643907547 = 0.0019221147522330284 + 0.1 * 6.332242965698242
Epoch 1880, val loss: 1.6998140811920166
Epoch 1890, training loss: 0.634936511516571 = 0.0019060635240748525 + 0.1 * 6.3303046226501465
Epoch 1890, val loss: 1.70147705078125
Epoch 1900, training loss: 0.635296106338501 = 0.0018901615403592587 + 0.1 * 6.334059238433838
Epoch 1900, val loss: 1.7031697034835815
Epoch 1910, training loss: 0.6363884806632996 = 0.0018746658461168408 + 0.1 * 6.345137596130371
Epoch 1910, val loss: 1.704776644706726
Epoch 1920, training loss: 0.6353609561920166 = 0.0018593260319903493 + 0.1 * 6.335016250610352
Epoch 1920, val loss: 1.70625638961792
Epoch 1930, training loss: 0.6346187591552734 = 0.0018443287117406726 + 0.1 * 6.327744007110596
Epoch 1930, val loss: 1.7079259157180786
Epoch 1940, training loss: 0.6351844072341919 = 0.0018295657355338335 + 0.1 * 6.333548069000244
Epoch 1940, val loss: 1.7095917463302612
Epoch 1950, training loss: 0.6342871785163879 = 0.0018149417592212558 + 0.1 * 6.324721813201904
Epoch 1950, val loss: 1.7111555337905884
Epoch 1960, training loss: 0.6350112557411194 = 0.001800506142899394 + 0.1 * 6.3321075439453125
Epoch 1960, val loss: 1.7127059698104858
Epoch 1970, training loss: 0.6349313259124756 = 0.0017864686669781804 + 0.1 * 6.331448554992676
Epoch 1970, val loss: 1.7142267227172852
Epoch 1980, training loss: 0.6353591680526733 = 0.001772620133124292 + 0.1 * 6.335865020751953
Epoch 1980, val loss: 1.7157443761825562
Epoch 1990, training loss: 0.6339677572250366 = 0.0017589067574590445 + 0.1 * 6.322088241577148
Epoch 1990, val loss: 1.7172781229019165
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 2.809654712677002 = 1.9499658346176147 + 0.1 * 8.596887588500977
Epoch 0, val loss: 1.9595884084701538
Epoch 10, training loss: 2.799572467803955 = 1.9398893117904663 + 0.1 * 8.596831321716309
Epoch 10, val loss: 1.9485507011413574
Epoch 20, training loss: 2.7869880199432373 = 1.9273442029953003 + 0.1 * 8.596437454223633
Epoch 20, val loss: 1.934585690498352
Epoch 30, training loss: 2.769010066986084 = 1.9097437858581543 + 0.1 * 8.592663764953613
Epoch 30, val loss: 1.9147828817367554
Epoch 40, training loss: 2.7408316135406494 = 1.8841911554336548 + 0.1 * 8.566405296325684
Epoch 40, val loss: 1.8860102891921997
Epoch 50, training loss: 2.6950302124023438 = 1.8498585224151611 + 0.1 * 8.451715469360352
Epoch 50, val loss: 1.8489760160446167
Epoch 60, training loss: 2.631671905517578 = 1.8131555318832397 + 0.1 * 8.185164451599121
Epoch 60, val loss: 1.813893437385559
Epoch 70, training loss: 2.5888280868530273 = 1.7815418243408203 + 0.1 * 8.072861671447754
Epoch 70, val loss: 1.7888751029968262
Epoch 80, training loss: 2.5251269340515137 = 1.7460360527038574 + 0.1 * 7.790909290313721
Epoch 80, val loss: 1.7620176076889038
Epoch 90, training loss: 2.4449000358581543 = 1.6994229555130005 + 0.1 * 7.454771995544434
Epoch 90, val loss: 1.7242001295089722
Epoch 100, training loss: 2.3670835494995117 = 1.636534571647644 + 0.1 * 7.305488586425781
Epoch 100, val loss: 1.670949935913086
Epoch 110, training loss: 2.2809882164001465 = 1.5565358400344849 + 0.1 * 7.244524002075195
Epoch 110, val loss: 1.6043634414672852
Epoch 120, training loss: 2.1831235885620117 = 1.4640809297561646 + 0.1 * 7.190426826477051
Epoch 120, val loss: 1.5306806564331055
Epoch 130, training loss: 2.0801823139190674 = 1.366417646408081 + 0.1 * 7.137646675109863
Epoch 130, val loss: 1.456160545349121
Epoch 140, training loss: 1.976219892501831 = 1.2681238651275635 + 0.1 * 7.080960750579834
Epoch 140, val loss: 1.3823155164718628
Epoch 150, training loss: 1.8755065202713013 = 1.171086311340332 + 0.1 * 7.044201850891113
Epoch 150, val loss: 1.3118774890899658
Epoch 160, training loss: 1.7815062999725342 = 1.0788328647613525 + 0.1 * 7.026734828948975
Epoch 160, val loss: 1.2478338479995728
Epoch 170, training loss: 1.6944762468338013 = 0.9934409260749817 + 0.1 * 7.010353088378906
Epoch 170, val loss: 1.1898208856582642
Epoch 180, training loss: 1.6131319999694824 = 0.9142972826957703 + 0.1 * 6.98834753036499
Epoch 180, val loss: 1.137096643447876
Epoch 190, training loss: 1.5380182266235352 = 0.8410932421684265 + 0.1 * 6.969250202178955
Epoch 190, val loss: 1.0898711681365967
Epoch 200, training loss: 1.4686864614486694 = 0.7740446925163269 + 0.1 * 6.946417331695557
Epoch 200, val loss: 1.0487674474716187
Epoch 210, training loss: 1.4067763090133667 = 0.7130408883094788 + 0.1 * 6.93735408782959
Epoch 210, val loss: 1.0140540599822998
Epoch 220, training loss: 1.3497281074523926 = 0.6586797833442688 + 0.1 * 6.910483360290527
Epoch 220, val loss: 0.9864895343780518
Epoch 230, training loss: 1.2992396354675293 = 0.6096450686454773 + 0.1 * 6.8959455490112305
Epoch 230, val loss: 0.9649851322174072
Epoch 240, training loss: 1.252943515777588 = 0.5650832653045654 + 0.1 * 6.878602981567383
Epoch 240, val loss: 0.9490106701850891
Epoch 250, training loss: 1.21022629737854 = 0.5237994194030762 + 0.1 * 6.864269256591797
Epoch 250, val loss: 0.9373526573181152
Epoch 260, training loss: 1.1709840297698975 = 0.48501378297805786 + 0.1 * 6.859701633453369
Epoch 260, val loss: 0.9293850660324097
Epoch 270, training loss: 1.1330904960632324 = 0.4485122859477997 + 0.1 * 6.845782279968262
Epoch 270, val loss: 0.9246973395347595
Epoch 280, training loss: 1.097069501876831 = 0.4139306843280792 + 0.1 * 6.831388473510742
Epoch 280, val loss: 0.9232619404792786
Epoch 290, training loss: 1.0637669563293457 = 0.381513386964798 + 0.1 * 6.822535037994385
Epoch 290, val loss: 0.9252858757972717
Epoch 300, training loss: 1.0335570573806763 = 0.3515864908695221 + 0.1 * 6.819705009460449
Epoch 300, val loss: 0.9306409358978271
Epoch 310, training loss: 1.0055962800979614 = 0.32441824674606323 + 0.1 * 6.811779975891113
Epoch 310, val loss: 0.9388190507888794
Epoch 320, training loss: 0.9798201322555542 = 0.2996814250946045 + 0.1 * 6.801386833190918
Epoch 320, val loss: 0.9490059018135071
Epoch 330, training loss: 0.9569950103759766 = 0.27677246928215027 + 0.1 * 6.802225112915039
Epoch 330, val loss: 0.9606181979179382
Epoch 340, training loss: 0.9346829652786255 = 0.25510066747665405 + 0.1 * 6.795823097229004
Epoch 340, val loss: 0.9733210802078247
Epoch 350, training loss: 0.9127466678619385 = 0.23403917253017426 + 0.1 * 6.787074565887451
Epoch 350, val loss: 0.9868369698524475
Epoch 360, training loss: 0.8924648761749268 = 0.21325336396694183 + 0.1 * 6.792114734649658
Epoch 360, val loss: 1.0010907649993896
Epoch 370, training loss: 0.8712043762207031 = 0.19295787811279297 + 0.1 * 6.782464981079102
Epoch 370, val loss: 1.0160778760910034
Epoch 380, training loss: 0.8510748147964478 = 0.17353780567646027 + 0.1 * 6.775370121002197
Epoch 380, val loss: 1.0318437814712524
Epoch 390, training loss: 0.8337778449058533 = 0.15549705922603607 + 0.1 * 6.78280782699585
Epoch 390, val loss: 1.0485590696334839
Epoch 400, training loss: 0.8161361813545227 = 0.13920283317565918 + 0.1 * 6.769333362579346
Epoch 400, val loss: 1.0661957263946533
Epoch 410, training loss: 0.801415741443634 = 0.12469498068094254 + 0.1 * 6.767207145690918
Epoch 410, val loss: 1.0848888158798218
Epoch 420, training loss: 0.788339376449585 = 0.11186013370752335 + 0.1 * 6.764791965484619
Epoch 420, val loss: 1.1045538187026978
Epoch 430, training loss: 0.7769083380699158 = 0.1005239337682724 + 0.1 * 6.763843536376953
Epoch 430, val loss: 1.1249992847442627
Epoch 440, training loss: 0.7664586305618286 = 0.09052184224128723 + 0.1 * 6.7593674659729
Epoch 440, val loss: 1.1461541652679443
Epoch 450, training loss: 0.7573394775390625 = 0.08169423788785934 + 0.1 * 6.7564520835876465
Epoch 450, val loss: 1.1677074432373047
Epoch 460, training loss: 0.7486585974693298 = 0.0739104226231575 + 0.1 * 6.747481822967529
Epoch 460, val loss: 1.1894241571426392
Epoch 470, training loss: 0.7436419725418091 = 0.06705150008201599 + 0.1 * 6.765904903411865
Epoch 470, val loss: 1.2110720872879028
Epoch 480, training loss: 0.7353719472885132 = 0.06104026734828949 + 0.1 * 6.743316650390625
Epoch 480, val loss: 1.232277274131775
Epoch 490, training loss: 0.7294467091560364 = 0.055754199624061584 + 0.1 * 6.73692512512207
Epoch 490, val loss: 1.2531521320343018
Epoch 500, training loss: 0.7239370942115784 = 0.05108277127146721 + 0.1 * 6.728542804718018
Epoch 500, val loss: 1.2737139463424683
Epoch 510, training loss: 0.7209190130233765 = 0.04693960025906563 + 0.1 * 6.73979377746582
Epoch 510, val loss: 1.2938060760498047
Epoch 520, training loss: 0.7154393196105957 = 0.04327874630689621 + 0.1 * 6.72160530090332
Epoch 520, val loss: 1.3133119344711304
Epoch 530, training loss: 0.7134667634963989 = 0.040022559463977814 + 0.1 * 6.734441757202148
Epoch 530, val loss: 1.33220374584198
Epoch 540, training loss: 0.7088534832000732 = 0.03712604567408562 + 0.1 * 6.717274188995361
Epoch 540, val loss: 1.3504540920257568
Epoch 550, training loss: 0.7048510313034058 = 0.034531887620687485 + 0.1 * 6.70319128036499
Epoch 550, val loss: 1.3681402206420898
Epoch 560, training loss: 0.7019152641296387 = 0.03219723328948021 + 0.1 * 6.697180271148682
Epoch 560, val loss: 1.3853857517242432
Epoch 570, training loss: 0.6997999548912048 = 0.03009258210659027 + 0.1 * 6.697073459625244
Epoch 570, val loss: 1.4020882844924927
Epoch 580, training loss: 0.696395993232727 = 0.02819148451089859 + 0.1 * 6.682044506072998
Epoch 580, val loss: 1.418341875076294
Epoch 590, training loss: 0.6948854923248291 = 0.02646540477871895 + 0.1 * 6.684200763702393
Epoch 590, val loss: 1.4341354370117188
Epoch 600, training loss: 0.6940310597419739 = 0.024901077151298523 + 0.1 * 6.6912994384765625
Epoch 600, val loss: 1.4493149518966675
Epoch 610, training loss: 0.6912619471549988 = 0.023479564115405083 + 0.1 * 6.677823543548584
Epoch 610, val loss: 1.4640682935714722
Epoch 620, training loss: 0.6882486343383789 = 0.02217963896691799 + 0.1 * 6.660689830780029
Epoch 620, val loss: 1.4784870147705078
Epoch 630, training loss: 0.6884445548057556 = 0.020985625684261322 + 0.1 * 6.67458963394165
Epoch 630, val loss: 1.492541790008545
Epoch 640, training loss: 0.6881189942359924 = 0.019888784736394882 + 0.1 * 6.682302474975586
Epoch 640, val loss: 1.5061028003692627
Epoch 650, training loss: 0.6841713786125183 = 0.018882350996136665 + 0.1 * 6.652890205383301
Epoch 650, val loss: 1.5192110538482666
Epoch 660, training loss: 0.6827644109725952 = 0.017954356968402863 + 0.1 * 6.64810037612915
Epoch 660, val loss: 1.53214430809021
Epoch 670, training loss: 0.6816906929016113 = 0.01709594391286373 + 0.1 * 6.645947456359863
Epoch 670, val loss: 1.5445785522460938
Epoch 680, training loss: 0.6803587079048157 = 0.01630183681845665 + 0.1 * 6.640568733215332
Epoch 680, val loss: 1.5568128824234009
Epoch 690, training loss: 0.6798892617225647 = 0.015563169494271278 + 0.1 * 6.643260478973389
Epoch 690, val loss: 1.5687164068222046
Epoch 700, training loss: 0.6784195303916931 = 0.014876116998493671 + 0.1 * 6.635434150695801
Epoch 700, val loss: 1.5803146362304688
Epoch 710, training loss: 0.6785774827003479 = 0.014237347058951855 + 0.1 * 6.643401145935059
Epoch 710, val loss: 1.5916937589645386
Epoch 720, training loss: 0.6768369078636169 = 0.01364086102694273 + 0.1 * 6.631959915161133
Epoch 720, val loss: 1.6027508974075317
Epoch 730, training loss: 0.6774465441703796 = 0.013083482161164284 + 0.1 * 6.6436309814453125
Epoch 730, val loss: 1.6135375499725342
Epoch 740, training loss: 0.6751514077186584 = 0.012563720345497131 + 0.1 * 6.6258769035339355
Epoch 740, val loss: 1.6241374015808105
Epoch 750, training loss: 0.6770020723342896 = 0.012075655162334442 + 0.1 * 6.649263858795166
Epoch 750, val loss: 1.6344646215438843
Epoch 760, training loss: 0.6737440228462219 = 0.011619208380579948 + 0.1 * 6.6212477684021
Epoch 760, val loss: 1.644448161125183
Epoch 770, training loss: 0.6735784411430359 = 0.011191393248736858 + 0.1 * 6.623870372772217
Epoch 770, val loss: 1.6541924476623535
Epoch 780, training loss: 0.6719741821289062 = 0.010790598578751087 + 0.1 * 6.611835956573486
Epoch 780, val loss: 1.6637965440750122
Epoch 790, training loss: 0.6714681386947632 = 0.010412690229713917 + 0.1 * 6.610554218292236
Epoch 790, val loss: 1.6732251644134521
Epoch 800, training loss: 0.6706230640411377 = 0.010056300088763237 + 0.1 * 6.605667591094971
Epoch 800, val loss: 1.6824685335159302
Epoch 810, training loss: 0.669714629650116 = 0.009719082154333591 + 0.1 * 6.5999555587768555
Epoch 810, val loss: 1.6914386749267578
Epoch 820, training loss: 0.6704722046852112 = 0.009401463903486729 + 0.1 * 6.610707759857178
Epoch 820, val loss: 1.700332522392273
Epoch 830, training loss: 0.6681034564971924 = 0.009099994786083698 + 0.1 * 6.590034008026123
Epoch 830, val loss: 1.7089027166366577
Epoch 840, training loss: 0.6686678528785706 = 0.008815126493573189 + 0.1 * 6.598527431488037
Epoch 840, val loss: 1.7174301147460938
Epoch 850, training loss: 0.6673356890678406 = 0.00854525063186884 + 0.1 * 6.58790397644043
Epoch 850, val loss: 1.7257142066955566
Epoch 860, training loss: 0.6669308543205261 = 0.008288943208754063 + 0.1 * 6.586419105529785
Epoch 860, val loss: 1.733741283416748
Epoch 870, training loss: 0.6663253307342529 = 0.008046301081776619 + 0.1 * 6.582789897918701
Epoch 870, val loss: 1.7417385578155518
Epoch 880, training loss: 0.6654972434043884 = 0.007814902812242508 + 0.1 * 6.576823711395264
Epoch 880, val loss: 1.7494127750396729
Epoch 890, training loss: 0.6666356921195984 = 0.007595379371196032 + 0.1 * 6.590403079986572
Epoch 890, val loss: 1.757061243057251
Epoch 900, training loss: 0.6642100214958191 = 0.007386143784970045 + 0.1 * 6.568238735198975
Epoch 900, val loss: 1.7645962238311768
Epoch 910, training loss: 0.6642981767654419 = 0.007186227943748236 + 0.1 * 6.57111930847168
Epoch 910, val loss: 1.77191960811615
Epoch 920, training loss: 0.6651480197906494 = 0.00699507724493742 + 0.1 * 6.581529140472412
Epoch 920, val loss: 1.7790844440460205
Epoch 930, training loss: 0.6633877158164978 = 0.006812893319875002 + 0.1 * 6.56574821472168
Epoch 930, val loss: 1.7861038446426392
Epoch 940, training loss: 0.6627703309059143 = 0.006638756953179836 + 0.1 * 6.561315536499023
Epoch 940, val loss: 1.7930790185928345
Epoch 950, training loss: 0.6622910499572754 = 0.006471933331340551 + 0.1 * 6.558191299438477
Epoch 950, val loss: 1.799808382987976
Epoch 960, training loss: 0.6619391441345215 = 0.006312897894531488 + 0.1 * 6.556262493133545
Epoch 960, val loss: 1.8064953088760376
Epoch 970, training loss: 0.6620541214942932 = 0.006159961223602295 + 0.1 * 6.55894136428833
Epoch 970, val loss: 1.813092589378357
Epoch 980, training loss: 0.6601756811141968 = 0.006013588979840279 + 0.1 * 6.54162073135376
Epoch 980, val loss: 1.819530725479126
Epoch 990, training loss: 0.6614936590194702 = 0.005873002577573061 + 0.1 * 6.556206226348877
Epoch 990, val loss: 1.8258464336395264
Epoch 1000, training loss: 0.6607908010482788 = 0.005738068372011185 + 0.1 * 6.550527095794678
Epoch 1000, val loss: 1.8320692777633667
Epoch 1010, training loss: 0.6590905785560608 = 0.005608527455478907 + 0.1 * 6.534820556640625
Epoch 1010, val loss: 1.8381881713867188
Epoch 1020, training loss: 0.6614428162574768 = 0.005484211258590221 + 0.1 * 6.559586048126221
Epoch 1020, val loss: 1.8443154096603394
Epoch 1030, training loss: 0.6590754389762878 = 0.005364164710044861 + 0.1 * 6.537112236022949
Epoch 1030, val loss: 1.8501018285751343
Epoch 1040, training loss: 0.6597906947135925 = 0.005248780827969313 + 0.1 * 6.545419216156006
Epoch 1040, val loss: 1.8558751344680786
Epoch 1050, training loss: 0.6572083234786987 = 0.005138269625604153 + 0.1 * 6.520700454711914
Epoch 1050, val loss: 1.8616005182266235
Epoch 1060, training loss: 0.6583310961723328 = 0.005032142624258995 + 0.1 * 6.532989501953125
Epoch 1060, val loss: 1.8673889636993408
Epoch 1070, training loss: 0.6568783521652222 = 0.004928868729621172 + 0.1 * 6.519494533538818
Epoch 1070, val loss: 1.8727670907974243
Epoch 1080, training loss: 0.6579365134239197 = 0.004829880781471729 + 0.1 * 6.531065940856934
Epoch 1080, val loss: 1.8782196044921875
Epoch 1090, training loss: 0.6562587022781372 = 0.004734562709927559 + 0.1 * 6.5152411460876465
Epoch 1090, val loss: 1.8836454153060913
Epoch 1100, training loss: 0.6561233401298523 = 0.004642308223992586 + 0.1 * 6.514810085296631
Epoch 1100, val loss: 1.888949990272522
Epoch 1110, training loss: 0.6580414175987244 = 0.004552910570055246 + 0.1 * 6.534884929656982
Epoch 1110, val loss: 1.894171118736267
Epoch 1120, training loss: 0.6551405191421509 = 0.004466724116355181 + 0.1 * 6.50673770904541
Epoch 1120, val loss: 1.8992034196853638
Epoch 1130, training loss: 0.6549270749092102 = 0.004383460618555546 + 0.1 * 6.505435943603516
Epoch 1130, val loss: 1.9043127298355103
Epoch 1140, training loss: 0.6549609303474426 = 0.004302889574319124 + 0.1 * 6.506579875946045
Epoch 1140, val loss: 1.9092170000076294
Epoch 1150, training loss: 0.6549744606018066 = 0.00422535790130496 + 0.1 * 6.507490634918213
Epoch 1150, val loss: 1.914088249206543
Epoch 1160, training loss: 0.6546237468719482 = 0.004150202497839928 + 0.1 * 6.504735469818115
Epoch 1160, val loss: 1.918986439704895
Epoch 1170, training loss: 0.6543456315994263 = 0.004077389370650053 + 0.1 * 6.502682209014893
Epoch 1170, val loss: 1.9238783121109009
Epoch 1180, training loss: 0.6537229418754578 = 0.004006459377706051 + 0.1 * 6.497164726257324
Epoch 1180, val loss: 1.9284335374832153
Epoch 1190, training loss: 0.6541866660118103 = 0.003938230220228434 + 0.1 * 6.50248384475708
Epoch 1190, val loss: 1.9331642389297485
Epoch 1200, training loss: 0.6537486910820007 = 0.003871921682730317 + 0.1 * 6.498767375946045
Epoch 1200, val loss: 1.9377583265304565
Epoch 1210, training loss: 0.6530008912086487 = 0.003807653905823827 + 0.1 * 6.491931915283203
Epoch 1210, val loss: 1.942185878753662
Epoch 1220, training loss: 0.6523860692977905 = 0.003745374735444784 + 0.1 * 6.4864068031311035
Epoch 1220, val loss: 1.9466696977615356
Epoch 1230, training loss: 0.6556410789489746 = 0.003684940515086055 + 0.1 * 6.519561290740967
Epoch 1230, val loss: 1.9511456489562988
Epoch 1240, training loss: 0.6534175276756287 = 0.0036263258662074804 + 0.1 * 6.4979119300842285
Epoch 1240, val loss: 1.9554393291473389
Epoch 1250, training loss: 0.6517617702484131 = 0.0035693461541086435 + 0.1 * 6.481924057006836
Epoch 1250, val loss: 1.9597914218902588
Epoch 1260, training loss: 0.6526257991790771 = 0.0035141387488693 + 0.1 * 6.491117000579834
Epoch 1260, val loss: 1.9640541076660156
Epoch 1270, training loss: 0.6506861448287964 = 0.003460400039330125 + 0.1 * 6.472257137298584
Epoch 1270, val loss: 1.9682508707046509
Epoch 1280, training loss: 0.6521795988082886 = 0.0034081065095961094 + 0.1 * 6.4877142906188965
Epoch 1280, val loss: 1.9725027084350586
Epoch 1290, training loss: 0.6501948237419128 = 0.0033571571111679077 + 0.1 * 6.468376159667969
Epoch 1290, val loss: 1.9765526056289673
Epoch 1300, training loss: 0.6528624296188354 = 0.003307643812149763 + 0.1 * 6.495547771453857
Epoch 1300, val loss: 1.9805833101272583
Epoch 1310, training loss: 0.6505933403968811 = 0.0032598618417978287 + 0.1 * 6.473334789276123
Epoch 1310, val loss: 1.9846605062484741
Epoch 1320, training loss: 0.6506629586219788 = 0.0032132044434547424 + 0.1 * 6.474497318267822
Epoch 1320, val loss: 1.988646388053894
Epoch 1330, training loss: 0.6494923233985901 = 0.0031677589286118746 + 0.1 * 6.463245391845703
Epoch 1330, val loss: 1.9925875663757324
Epoch 1340, training loss: 0.6520560383796692 = 0.0031234619673341513 + 0.1 * 6.489325523376465
Epoch 1340, val loss: 1.9965499639511108
Epoch 1350, training loss: 0.6492348909378052 = 0.003080292372033 + 0.1 * 6.461545944213867
Epoch 1350, val loss: 2.000384569168091
Epoch 1360, training loss: 0.6497972011566162 = 0.003038175404071808 + 0.1 * 6.46759033203125
Epoch 1360, val loss: 2.0041799545288086
Epoch 1370, training loss: 0.6505179405212402 = 0.002997065894305706 + 0.1 * 6.475208759307861
Epoch 1370, val loss: 2.0079457759857178
Epoch 1380, training loss: 0.6509058475494385 = 0.0029571496415883303 + 0.1 * 6.479486465454102
Epoch 1380, val loss: 2.011711835861206
Epoch 1390, training loss: 0.6488479971885681 = 0.0029181360732764006 + 0.1 * 6.459298610687256
Epoch 1390, val loss: 2.015387773513794
Epoch 1400, training loss: 0.6500512957572937 = 0.0028801767621189356 + 0.1 * 6.471711158752441
Epoch 1400, val loss: 2.0190677642822266
Epoch 1410, training loss: 0.649260938167572 = 0.002843055408447981 + 0.1 * 6.464178562164307
Epoch 1410, val loss: 2.0226364135742188
Epoch 1420, training loss: 0.6493988633155823 = 0.002806902164593339 + 0.1 * 6.465919494628906
Epoch 1420, val loss: 2.0262515544891357
Epoch 1430, training loss: 0.6480869650840759 = 0.0027716304175555706 + 0.1 * 6.453153610229492
Epoch 1430, val loss: 2.0297796726226807
Epoch 1440, training loss: 0.6471972465515137 = 0.0027370823081582785 + 0.1 * 6.444601058959961
Epoch 1440, val loss: 2.0332984924316406
Epoch 1450, training loss: 0.6489812135696411 = 0.0027035027742385864 + 0.1 * 6.462777137756348
Epoch 1450, val loss: 2.0368101596832275
Epoch 1460, training loss: 0.6469284892082214 = 0.002670452930033207 + 0.1 * 6.442580223083496
Epoch 1460, val loss: 2.0401370525360107
Epoch 1470, training loss: 0.6479144096374512 = 0.002638333709910512 + 0.1 * 6.452760696411133
Epoch 1470, val loss: 2.0435245037078857
Epoch 1480, training loss: 0.6479310393333435 = 0.002607000060379505 + 0.1 * 6.453240394592285
Epoch 1480, val loss: 2.0469233989715576
Epoch 1490, training loss: 0.6464574933052063 = 0.0025763739831745625 + 0.1 * 6.438811302185059
Epoch 1490, val loss: 2.0502662658691406
Epoch 1500, training loss: 0.6471635103225708 = 0.002546448726207018 + 0.1 * 6.446170806884766
Epoch 1500, val loss: 2.0536506175994873
Epoch 1510, training loss: 0.6473177671432495 = 0.002516958164051175 + 0.1 * 6.448007583618164
Epoch 1510, val loss: 2.0569143295288086
Epoch 1520, training loss: 0.6472640037536621 = 0.002488107653334737 + 0.1 * 6.447758674621582
Epoch 1520, val loss: 2.0601041316986084
Epoch 1530, training loss: 0.6467793583869934 = 0.002460016869008541 + 0.1 * 6.443193435668945
Epoch 1530, val loss: 2.063368797302246
Epoch 1540, training loss: 0.6456635594367981 = 0.0024324944242835045 + 0.1 * 6.432311058044434
Epoch 1540, val loss: 2.0665807723999023
Epoch 1550, training loss: 0.6474205851554871 = 0.0024055405519902706 + 0.1 * 6.450150489807129
Epoch 1550, val loss: 2.0697238445281982
Epoch 1560, training loss: 0.6472318172454834 = 0.0023792865686118603 + 0.1 * 6.4485249519348145
Epoch 1560, val loss: 2.0727925300598145
Epoch 1570, training loss: 0.6465056538581848 = 0.002353470306843519 + 0.1 * 6.441521644592285
Epoch 1570, val loss: 2.075906753540039
Epoch 1580, training loss: 0.6459590196609497 = 0.002328436588868499 + 0.1 * 6.436305522918701
Epoch 1580, val loss: 2.0790297985076904
Epoch 1590, training loss: 0.6459538340568542 = 0.0023037134669721127 + 0.1 * 6.4365010261535645
Epoch 1590, val loss: 2.0820963382720947
Epoch 1600, training loss: 0.6452512145042419 = 0.0022795861586928368 + 0.1 * 6.429716110229492
Epoch 1600, val loss: 2.0851614475250244
Epoch 1610, training loss: 0.6459106802940369 = 0.0022559415083378553 + 0.1 * 6.436547756195068
Epoch 1610, val loss: 2.088223457336426
Epoch 1620, training loss: 0.64837247133255 = 0.002232820261269808 + 0.1 * 6.461396217346191
Epoch 1620, val loss: 2.0912959575653076
Epoch 1630, training loss: 0.645217776298523 = 0.0022099027410149574 + 0.1 * 6.430078506469727
Epoch 1630, val loss: 2.0941102504730225
Epoch 1640, training loss: 0.644646406173706 = 0.002187705598771572 + 0.1 * 6.424586772918701
Epoch 1640, val loss: 2.0970823764801025
Epoch 1650, training loss: 0.6447408199310303 = 0.0021660425700247288 + 0.1 * 6.425747871398926
Epoch 1650, val loss: 2.1000919342041016
Epoch 1660, training loss: 0.6463760733604431 = 0.0021445562597364187 + 0.1 * 6.442315101623535
Epoch 1660, val loss: 2.10298228263855
Epoch 1670, training loss: 0.6443766951560974 = 0.0021236080210655928 + 0.1 * 6.422530651092529
Epoch 1670, val loss: 2.1057662963867188
Epoch 1680, training loss: 0.6443941593170166 = 0.0021030602511018515 + 0.1 * 6.422911167144775
Epoch 1680, val loss: 2.1086654663085938
Epoch 1690, training loss: 0.6458836197853088 = 0.002083056839182973 + 0.1 * 6.438005447387695
Epoch 1690, val loss: 2.1116011142730713
Epoch 1700, training loss: 0.6439800262451172 = 0.0020631100051105022 + 0.1 * 6.4191694259643555
Epoch 1700, val loss: 2.1143429279327393
Epoch 1710, training loss: 0.6437211036682129 = 0.0020437606144696474 + 0.1 * 6.416772842407227
Epoch 1710, val loss: 2.1172235012054443
Epoch 1720, training loss: 0.643987238407135 = 0.002024861751124263 + 0.1 * 6.419623374938965
Epoch 1720, val loss: 2.120143413543701
Epoch 1730, training loss: 0.6456754207611084 = 0.002006033668294549 + 0.1 * 6.4366936683654785
Epoch 1730, val loss: 2.122885227203369
Epoch 1740, training loss: 0.6439086198806763 = 0.0019877124577760696 + 0.1 * 6.419209003448486
Epoch 1740, val loss: 2.1255288124084473
Epoch 1750, training loss: 0.6439910531044006 = 0.0019697067327797413 + 0.1 * 6.420213222503662
Epoch 1750, val loss: 2.128368854522705
Epoch 1760, training loss: 0.6440936923027039 = 0.0019520646892488003 + 0.1 * 6.421416282653809
Epoch 1760, val loss: 2.1310791969299316
Epoch 1770, training loss: 0.643541157245636 = 0.001934677129611373 + 0.1 * 6.416064739227295
Epoch 1770, val loss: 2.133760690689087
Epoch 1780, training loss: 0.6425327658653259 = 0.0019175915513187647 + 0.1 * 6.406151294708252
Epoch 1780, val loss: 2.136462688446045
Epoch 1790, training loss: 0.6448649764060974 = 0.001900856732390821 + 0.1 * 6.429641246795654
Epoch 1790, val loss: 2.1391279697418213
Epoch 1800, training loss: 0.6444198489189148 = 0.0018842908320948482 + 0.1 * 6.425355434417725
Epoch 1800, val loss: 2.1417150497436523
Epoch 1810, training loss: 0.643245279788971 = 0.0018681098008528352 + 0.1 * 6.413771152496338
Epoch 1810, val loss: 2.1442840099334717
Epoch 1820, training loss: 0.6428632140159607 = 0.0018523222533985972 + 0.1 * 6.41010856628418
Epoch 1820, val loss: 2.1470024585723877
Epoch 1830, training loss: 0.6425728797912598 = 0.0018366584554314613 + 0.1 * 6.40736198425293
Epoch 1830, val loss: 2.149594783782959
Epoch 1840, training loss: 0.6437130570411682 = 0.0018212886061519384 + 0.1 * 6.418917655944824
Epoch 1840, val loss: 2.152162551879883
Epoch 1850, training loss: 0.6425831317901611 = 0.0018060739384964108 + 0.1 * 6.407770156860352
Epoch 1850, val loss: 2.154700994491577
Epoch 1860, training loss: 0.6434212923049927 = 0.0017912833718582988 + 0.1 * 6.416299819946289
Epoch 1860, val loss: 2.157299041748047
Epoch 1870, training loss: 0.6433783769607544 = 0.0017766185337677598 + 0.1 * 6.416017532348633
Epoch 1870, val loss: 2.1597976684570312
Epoch 1880, training loss: 0.6416120529174805 = 0.001762236119247973 + 0.1 * 6.398498058319092
Epoch 1880, val loss: 2.1622962951660156
Epoch 1890, training loss: 0.6433929800987244 = 0.001748163951560855 + 0.1 * 6.41644811630249
Epoch 1890, val loss: 2.1648740768432617
Epoch 1900, training loss: 0.6410918235778809 = 0.0017342129722237587 + 0.1 * 6.393576145172119
Epoch 1900, val loss: 2.167383909225464
Epoch 1910, training loss: 0.6434147953987122 = 0.0017205114709213376 + 0.1 * 6.416943073272705
Epoch 1910, val loss: 2.1699087619781494
Epoch 1920, training loss: 0.6433236002922058 = 0.0017070736503228545 + 0.1 * 6.416165351867676
Epoch 1920, val loss: 2.172412633895874
Epoch 1930, training loss: 0.6422904133796692 = 0.001693657715804875 + 0.1 * 6.4059672355651855
Epoch 1930, val loss: 2.174759864807129
Epoch 1940, training loss: 0.6431730389595032 = 0.0016806458588689566 + 0.1 * 6.414923667907715
Epoch 1940, val loss: 2.1772780418395996
Epoch 1950, training loss: 0.6417493224143982 = 0.0016677379608154297 + 0.1 * 6.400815963745117
Epoch 1950, val loss: 2.1796469688415527
Epoch 1960, training loss: 0.6421336531639099 = 0.0016551121370866895 + 0.1 * 6.40478515625
Epoch 1960, val loss: 2.182121515274048
Epoch 1970, training loss: 0.6408924460411072 = 0.0016425944631919265 + 0.1 * 6.39249849319458
Epoch 1970, val loss: 2.184551954269409
Epoch 1980, training loss: 0.6420227885246277 = 0.0016303210286423564 + 0.1 * 6.403924465179443
Epoch 1980, val loss: 2.186962366104126
Epoch 1990, training loss: 0.6409143805503845 = 0.0016181966057047248 + 0.1 * 6.392961502075195
Epoch 1990, val loss: 2.189333438873291
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 2.805143356323242 = 1.9454588890075684 + 0.1 * 8.596844673156738
Epoch 0, val loss: 1.9476991891860962
Epoch 10, training loss: 2.7952566146850586 = 1.9355814456939697 + 0.1 * 8.596750259399414
Epoch 10, val loss: 1.9384424686431885
Epoch 20, training loss: 2.7826879024505615 = 1.9230833053588867 + 0.1 * 8.596046447753906
Epoch 20, val loss: 1.9263988733291626
Epoch 30, training loss: 2.7641258239746094 = 1.9051926136016846 + 0.1 * 8.58933162689209
Epoch 30, val loss: 1.908956527709961
Epoch 40, training loss: 2.7331323623657227 = 1.87879478931427 + 0.1 * 8.543375968933105
Epoch 40, val loss: 1.8835927248001099
Epoch 50, training loss: 2.6739158630371094 = 1.8440486192703247 + 0.1 * 8.29867172241211
Epoch 50, val loss: 1.8522592782974243
Epoch 60, training loss: 2.616483211517334 = 1.8064419031143188 + 0.1 * 8.100414276123047
Epoch 60, val loss: 1.820528268814087
Epoch 70, training loss: 2.5660033226013184 = 1.7697532176971436 + 0.1 * 7.962501525878906
Epoch 70, val loss: 1.7894874811172485
Epoch 80, training loss: 2.4977896213531494 = 1.727207899093628 + 0.1 * 7.705817222595215
Epoch 80, val loss: 1.751828670501709
Epoch 90, training loss: 2.417117118835449 = 1.6740305423736572 + 0.1 * 7.43086576461792
Epoch 90, val loss: 1.7077716588974
Epoch 100, training loss: 2.3338465690612793 = 1.6070713996887207 + 0.1 * 7.267751693725586
Epoch 100, val loss: 1.6548864841461182
Epoch 110, training loss: 2.2452688217163086 = 1.5268090963363647 + 0.1 * 7.184597492218018
Epoch 110, val loss: 1.591030240058899
Epoch 120, training loss: 2.1534125804901123 = 1.4415333271026611 + 0.1 * 7.1187920570373535
Epoch 120, val loss: 1.5255762338638306
Epoch 130, training loss: 2.0643961429595947 = 1.35743248462677 + 0.1 * 7.06963586807251
Epoch 130, val loss: 1.4646704196929932
Epoch 140, training loss: 1.9785668849945068 = 1.2754065990447998 + 0.1 * 7.0316033363342285
Epoch 140, val loss: 1.4091441631317139
Epoch 150, training loss: 1.8960516452789307 = 1.1963849067687988 + 0.1 * 6.996667861938477
Epoch 150, val loss: 1.3591984510421753
Epoch 160, training loss: 1.8177831172943115 = 1.1213403940200806 + 0.1 * 6.964426517486572
Epoch 160, val loss: 1.3148707151412964
Epoch 170, training loss: 1.7437305450439453 = 1.0501400232315063 + 0.1 * 6.935904502868652
Epoch 170, val loss: 1.2737451791763306
Epoch 180, training loss: 1.6737087965011597 = 0.9826611876487732 + 0.1 * 6.910476207733154
Epoch 180, val loss: 1.2349363565444946
Epoch 190, training loss: 1.6066663265228271 = 0.9178617596626282 + 0.1 * 6.8880462646484375
Epoch 190, val loss: 1.1971993446350098
Epoch 200, training loss: 1.5417711734771729 = 0.854697048664093 + 0.1 * 6.8707404136657715
Epoch 200, val loss: 1.1600656509399414
Epoch 210, training loss: 1.4774620532989502 = 0.7914949655532837 + 0.1 * 6.859670639038086
Epoch 210, val loss: 1.122853398323059
Epoch 220, training loss: 1.41205632686615 = 0.7272257804870605 + 0.1 * 6.8483052253723145
Epoch 220, val loss: 1.085341215133667
Epoch 230, training loss: 1.3458764553070068 = 0.6616391539573669 + 0.1 * 6.842372417449951
Epoch 230, val loss: 1.0480068922042847
Epoch 240, training loss: 1.279637336730957 = 0.5962432622909546 + 0.1 * 6.833941459655762
Epoch 240, val loss: 1.0128382444381714
Epoch 250, training loss: 1.2161684036254883 = 0.532927930355072 + 0.1 * 6.832404613494873
Epoch 250, val loss: 0.982215166091919
Epoch 260, training loss: 1.156464695930481 = 0.4741445481777191 + 0.1 * 6.823201656341553
Epoch 260, val loss: 0.958321213722229
Epoch 270, training loss: 1.103714942932129 = 0.4214657247066498 + 0.1 * 6.8224921226501465
Epoch 270, val loss: 0.9420439600944519
Epoch 280, training loss: 1.0571272373199463 = 0.37554335594177246 + 0.1 * 6.8158392906188965
Epoch 280, val loss: 0.9330237507820129
Epoch 290, training loss: 1.0168070793151855 = 0.33578890562057495 + 0.1 * 6.810181140899658
Epoch 290, val loss: 0.9301081895828247
Epoch 300, training loss: 0.9819368720054626 = 0.301041841506958 + 0.1 * 6.808950424194336
Epoch 300, val loss: 0.9316666126251221
Epoch 310, training loss: 0.9503296613693237 = 0.2700633108615875 + 0.1 * 6.802663326263428
Epoch 310, val loss: 0.9364343881607056
Epoch 320, training loss: 0.9222617745399475 = 0.24189887940883636 + 0.1 * 6.803628444671631
Epoch 320, val loss: 0.9431694149971008
Epoch 330, training loss: 0.8954674005508423 = 0.2160874754190445 + 0.1 * 6.79379940032959
Epoch 330, val loss: 0.9512273669242859
Epoch 340, training loss: 0.8716904520988464 = 0.192558154463768 + 0.1 * 6.791322708129883
Epoch 340, val loss: 0.9603254795074463
Epoch 350, training loss: 0.8496620655059814 = 0.17140814661979675 + 0.1 * 6.782539367675781
Epoch 350, val loss: 0.9706231951713562
Epoch 360, training loss: 0.8300824165344238 = 0.15264025330543518 + 0.1 * 6.774421691894531
Epoch 360, val loss: 0.9824203848838806
Epoch 370, training loss: 0.81546950340271 = 0.13621363043785095 + 0.1 * 6.792558193206787
Epoch 370, val loss: 0.9957858324050903
Epoch 380, training loss: 0.7991939783096313 = 0.12206169962882996 + 0.1 * 6.771322727203369
Epoch 380, val loss: 1.0103449821472168
Epoch 390, training loss: 0.7863250374794006 = 0.10979937762022018 + 0.1 * 6.765256404876709
Epoch 390, val loss: 1.0259684324264526
Epoch 400, training loss: 0.7748408317565918 = 0.09916451573371887 + 0.1 * 6.756762981414795
Epoch 400, val loss: 1.0423388481140137
Epoch 410, training loss: 0.7641490697860718 = 0.08987320214509964 + 0.1 * 6.742758750915527
Epoch 410, val loss: 1.0592389106750488
Epoch 420, training loss: 0.7579982280731201 = 0.08169729262590408 + 0.1 * 6.763009548187256
Epoch 420, val loss: 1.0765430927276611
Epoch 430, training loss: 0.746978759765625 = 0.07454028725624084 + 0.1 * 6.724384307861328
Epoch 430, val loss: 1.093788981437683
Epoch 440, training loss: 0.7405969500541687 = 0.06819864362478256 + 0.1 * 6.723983287811279
Epoch 440, val loss: 1.1111198663711548
Epoch 450, training loss: 0.7344781756401062 = 0.0625714585185051 + 0.1 * 6.719067096710205
Epoch 450, val loss: 1.1284254789352417
Epoch 460, training loss: 0.7297341823577881 = 0.057551827281713486 + 0.1 * 6.721823692321777
Epoch 460, val loss: 1.1454932689666748
Epoch 470, training loss: 0.7221390604972839 = 0.053074486553668976 + 0.1 * 6.690645694732666
Epoch 470, val loss: 1.1623696088790894
Epoch 480, training loss: 0.7199867367744446 = 0.04904591664671898 + 0.1 * 6.709407806396484
Epoch 480, val loss: 1.179057002067566
Epoch 490, training loss: 0.7134966850280762 = 0.045433733612298965 + 0.1 * 6.680629253387451
Epoch 490, val loss: 1.1953622102737427
Epoch 500, training loss: 0.7114742398262024 = 0.042179666459560394 + 0.1 * 6.692945957183838
Epoch 500, val loss: 1.211438775062561
Epoch 510, training loss: 0.7068111300468445 = 0.03925727680325508 + 0.1 * 6.675538539886475
Epoch 510, val loss: 1.2271169424057007
Epoch 520, training loss: 0.7032326459884644 = 0.03661265969276428 + 0.1 * 6.666200160980225
Epoch 520, val loss: 1.2425330877304077
Epoch 530, training loss: 0.7002378702163696 = 0.034213609993457794 + 0.1 * 6.660242557525635
Epoch 530, val loss: 1.257468581199646
Epoch 540, training loss: 0.6980588436126709 = 0.03204275667667389 + 0.1 * 6.660161018371582
Epoch 540, val loss: 1.2720669507980347
Epoch 550, training loss: 0.695122241973877 = 0.030062325298786163 + 0.1 * 6.650599002838135
Epoch 550, val loss: 1.2863589525222778
Epoch 560, training loss: 0.6940882802009583 = 0.028250085189938545 + 0.1 * 6.658381938934326
Epoch 560, val loss: 1.3003159761428833
Epoch 570, training loss: 0.6922728419303894 = 0.026600360870361328 + 0.1 * 6.656724452972412
Epoch 570, val loss: 1.313960075378418
Epoch 580, training loss: 0.6903445720672607 = 0.025095250457525253 + 0.1 * 6.652493476867676
Epoch 580, val loss: 1.3272041082382202
Epoch 590, training loss: 0.6874678134918213 = 0.02371724508702755 + 0.1 * 6.637506008148193
Epoch 590, val loss: 1.340153694152832
Epoch 600, training loss: 0.6861525177955627 = 0.022448783740401268 + 0.1 * 6.63703727722168
Epoch 600, val loss: 1.3526759147644043
Epoch 610, training loss: 0.6846979856491089 = 0.02128518372774124 + 0.1 * 6.634128093719482
Epoch 610, val loss: 1.3648996353149414
Epoch 620, training loss: 0.6840012669563293 = 0.02021251618862152 + 0.1 * 6.637887001037598
Epoch 620, val loss: 1.3768941164016724
Epoch 630, training loss: 0.681716799736023 = 0.019221773371100426 + 0.1 * 6.624949932098389
Epoch 630, val loss: 1.3885184526443481
Epoch 640, training loss: 0.6809617280960083 = 0.018306391313672066 + 0.1 * 6.626553535461426
Epoch 640, val loss: 1.3998806476593018
Epoch 650, training loss: 0.6806348562240601 = 0.01745586097240448 + 0.1 * 6.631789684295654
Epoch 650, val loss: 1.41093909740448
Epoch 660, training loss: 0.678836464881897 = 0.01666707545518875 + 0.1 * 6.6216936111450195
Epoch 660, val loss: 1.421772837638855
Epoch 670, training loss: 0.6786677837371826 = 0.01593356765806675 + 0.1 * 6.627342224121094
Epoch 670, val loss: 1.432281732559204
Epoch 680, training loss: 0.6760290265083313 = 0.015253204852342606 + 0.1 * 6.607758045196533
Epoch 680, val loss: 1.4424340724945068
Epoch 690, training loss: 0.6763657927513123 = 0.014619473367929459 + 0.1 * 6.617463111877441
Epoch 690, val loss: 1.4524849653244019
Epoch 700, training loss: 0.6740381717681885 = 0.014027342200279236 + 0.1 * 6.6001081466674805
Epoch 700, val loss: 1.4621237516403198
Epoch 710, training loss: 0.6725743412971497 = 0.013472670689225197 + 0.1 * 6.5910162925720215
Epoch 710, val loss: 1.4716720581054688
Epoch 720, training loss: 0.6716028451919556 = 0.012952417135238647 + 0.1 * 6.5865044593811035
Epoch 720, val loss: 1.4808799028396606
Epoch 730, training loss: 0.6714771389961243 = 0.01246568001806736 + 0.1 * 6.590114593505859
Epoch 730, val loss: 1.4899933338165283
Epoch 740, training loss: 0.670870304107666 = 0.012007041834294796 + 0.1 * 6.588632106781006
Epoch 740, val loss: 1.4987584352493286
Epoch 750, training loss: 0.6691222190856934 = 0.01157792005687952 + 0.1 * 6.575443267822266
Epoch 750, val loss: 1.5074225664138794
Epoch 760, training loss: 0.6693893671035767 = 0.011172143742442131 + 0.1 * 6.582172393798828
Epoch 760, val loss: 1.5158826112747192
Epoch 770, training loss: 0.6679355502128601 = 0.010789611376821995 + 0.1 * 6.5714592933654785
Epoch 770, val loss: 1.5240819454193115
Epoch 780, training loss: 0.6669775247573853 = 0.010428853332996368 + 0.1 * 6.565486431121826
Epoch 780, val loss: 1.5322600603103638
Epoch 790, training loss: 0.6687925457954407 = 0.010086317546665668 + 0.1 * 6.587062358856201
Epoch 790, val loss: 1.5401182174682617
Epoch 800, training loss: 0.6664830446243286 = 0.009763846173882484 + 0.1 * 6.567192077636719
Epoch 800, val loss: 1.547876000404358
Epoch 810, training loss: 0.6681288480758667 = 0.009457853622734547 + 0.1 * 6.586709976196289
Epoch 810, val loss: 1.5555084943771362
Epoch 820, training loss: 0.6646312475204468 = 0.009167653508484364 + 0.1 * 6.554635524749756
Epoch 820, val loss: 1.562818169593811
Epoch 830, training loss: 0.6637092232704163 = 0.008893609046936035 + 0.1 * 6.548155784606934
Epoch 830, val loss: 1.570175290107727
Epoch 840, training loss: 0.6623926162719727 = 0.008630899712443352 + 0.1 * 6.537617206573486
Epoch 840, val loss: 1.5773226022720337
Epoch 850, training loss: 0.6616343855857849 = 0.008381419815123081 + 0.1 * 6.532529354095459
Epoch 850, val loss: 1.5842375755310059
Epoch 860, training loss: 0.660578191280365 = 0.008144492283463478 + 0.1 * 6.524336814880371
Epoch 860, val loss: 1.5911428928375244
Epoch 870, training loss: 0.6615554690361023 = 0.007917923852801323 + 0.1 * 6.536375045776367
Epoch 870, val loss: 1.597962737083435
Epoch 880, training loss: 0.6655011773109436 = 0.007701512426137924 + 0.1 * 6.577996253967285
Epoch 880, val loss: 1.604513168334961
Epoch 890, training loss: 0.6597885489463806 = 0.007496380712836981 + 0.1 * 6.522921562194824
Epoch 890, val loss: 1.6109243631362915
Epoch 900, training loss: 0.659229040145874 = 0.007300708442926407 + 0.1 * 6.519282817840576
Epoch 900, val loss: 1.617390751838684
Epoch 910, training loss: 0.6621875762939453 = 0.007112205494195223 + 0.1 * 6.550753593444824
Epoch 910, val loss: 1.6236540079116821
Epoch 920, training loss: 0.6588923931121826 = 0.006931821815669537 + 0.1 * 6.51960563659668
Epoch 920, val loss: 1.629744052886963
Epoch 930, training loss: 0.6596236228942871 = 0.00675999978557229 + 0.1 * 6.5286359786987305
Epoch 930, val loss: 1.6357711553573608
Epoch 940, training loss: 0.6576136946678162 = 0.006594877224415541 + 0.1 * 6.51018762588501
Epoch 940, val loss: 1.6417125463485718
Epoch 950, training loss: 0.6566354036331177 = 0.006436640862375498 + 0.1 * 6.501987457275391
Epoch 950, val loss: 1.6475242376327515
Epoch 960, training loss: 0.6565244793891907 = 0.006284535862505436 + 0.1 * 6.502399444580078
Epoch 960, val loss: 1.6532450914382935
Epoch 970, training loss: 0.6565820574760437 = 0.006138328928500414 + 0.1 * 6.50443696975708
Epoch 970, val loss: 1.6589179039001465
Epoch 980, training loss: 0.6556446552276611 = 0.0059978594072163105 + 0.1 * 6.4964680671691895
Epoch 980, val loss: 1.6644128561019897
Epoch 990, training loss: 0.6557530760765076 = 0.005863445345312357 + 0.1 * 6.498896598815918
Epoch 990, val loss: 1.6698379516601562
Epoch 1000, training loss: 0.6542243361473083 = 0.005734165199100971 + 0.1 * 6.4849019050598145
Epoch 1000, val loss: 1.6751714944839478
Epoch 1010, training loss: 0.6544963717460632 = 0.005609691143035889 + 0.1 * 6.488866806030273
Epoch 1010, val loss: 1.680465817451477
Epoch 1020, training loss: 0.6545127034187317 = 0.005489418748766184 + 0.1 * 6.490232944488525
Epoch 1020, val loss: 1.6856791973114014
Epoch 1030, training loss: 0.6537060737609863 = 0.005373767111450434 + 0.1 * 6.483323097229004
Epoch 1030, val loss: 1.6908122301101685
Epoch 1040, training loss: 0.6553006172180176 = 0.005261830985546112 + 0.1 * 6.500387668609619
Epoch 1040, val loss: 1.6958885192871094
Epoch 1050, training loss: 0.6542127132415771 = 0.005153766833245754 + 0.1 * 6.490589141845703
Epoch 1050, val loss: 1.7008509635925293
Epoch 1060, training loss: 0.6541393399238586 = 0.005050017964094877 + 0.1 * 6.490893363952637
Epoch 1060, val loss: 1.7057759761810303
Epoch 1070, training loss: 0.6517837047576904 = 0.004949444439262152 + 0.1 * 6.468342304229736
Epoch 1070, val loss: 1.710633397102356
Epoch 1080, training loss: 0.6522669196128845 = 0.004852416925132275 + 0.1 * 6.47414493560791
Epoch 1080, val loss: 1.7153714895248413
Epoch 1090, training loss: 0.6514024138450623 = 0.004758847411721945 + 0.1 * 6.46643590927124
Epoch 1090, val loss: 1.7201098203659058
Epoch 1100, training loss: 0.6511603593826294 = 0.004668067209422588 + 0.1 * 6.464922904968262
Epoch 1100, val loss: 1.724764108657837
Epoch 1110, training loss: 0.6529812216758728 = 0.004580238368362188 + 0.1 * 6.484009742736816
Epoch 1110, val loss: 1.7293527126312256
Epoch 1120, training loss: 0.6502386331558228 = 0.004495563916862011 + 0.1 * 6.457430839538574
Epoch 1120, val loss: 1.7338224649429321
Epoch 1130, training loss: 0.6522340178489685 = 0.0044136191718280315 + 0.1 * 6.478203773498535
Epoch 1130, val loss: 1.7383249998092651
Epoch 1140, training loss: 0.6502330303192139 = 0.004333706106990576 + 0.1 * 6.458992958068848
Epoch 1140, val loss: 1.7427284717559814
Epoch 1150, training loss: 0.650691032409668 = 0.004256696440279484 + 0.1 * 6.464343070983887
Epoch 1150, val loss: 1.747047781944275
Epoch 1160, training loss: 0.6497471928596497 = 0.004182071425020695 + 0.1 * 6.455650806427002
Epoch 1160, val loss: 1.7513179779052734
Epoch 1170, training loss: 0.6504220366477966 = 0.004109647125005722 + 0.1 * 6.4631242752075195
Epoch 1170, val loss: 1.7555797100067139
Epoch 1180, training loss: 0.6487511992454529 = 0.004039373714476824 + 0.1 * 6.447118282318115
Epoch 1180, val loss: 1.759737491607666
Epoch 1190, training loss: 0.6512632369995117 = 0.003971134778112173 + 0.1 * 6.472920894622803
Epoch 1190, val loss: 1.7638388872146606
Epoch 1200, training loss: 0.6483453512191772 = 0.0039049966726452112 + 0.1 * 6.444403648376465
Epoch 1200, val loss: 1.7678524255752563
Epoch 1210, training loss: 0.6502483487129211 = 0.00384103343822062 + 0.1 * 6.464073181152344
Epoch 1210, val loss: 1.7719011306762695
Epoch 1220, training loss: 0.64801025390625 = 0.0037785938475281 + 0.1 * 6.44231653213501
Epoch 1220, val loss: 1.7758400440216064
Epoch 1230, training loss: 0.6493147015571594 = 0.003717926098033786 + 0.1 * 6.455967903137207
Epoch 1230, val loss: 1.779787540435791
Epoch 1240, training loss: 0.6511338949203491 = 0.003658728674054146 + 0.1 * 6.474751949310303
Epoch 1240, val loss: 1.783614993095398
Epoch 1250, training loss: 0.6489589214324951 = 0.003602055599913001 + 0.1 * 6.453568935394287
Epoch 1250, val loss: 1.787384271621704
Epoch 1260, training loss: 0.6488672494888306 = 0.003546557854861021 + 0.1 * 6.453207015991211
Epoch 1260, val loss: 1.7911337614059448
Epoch 1270, training loss: 0.6467465758323669 = 0.003492404008284211 + 0.1 * 6.432541370391846
Epoch 1270, val loss: 1.7948803901672363
Epoch 1280, training loss: 0.6494888067245483 = 0.003439616411924362 + 0.1 * 6.46049165725708
Epoch 1280, val loss: 1.79863440990448
Epoch 1290, training loss: 0.6477736830711365 = 0.0033881778363138437 + 0.1 * 6.443855285644531
Epoch 1290, val loss: 1.8021975755691528
Epoch 1300, training loss: 0.648137092590332 = 0.0033383334521204233 + 0.1 * 6.4479875564575195
Epoch 1300, val loss: 1.8058152198791504
Epoch 1310, training loss: 0.6472409963607788 = 0.0032898650970309973 + 0.1 * 6.439511299133301
Epoch 1310, val loss: 1.8093163967132568
Epoch 1320, training loss: 0.6469177007675171 = 0.003242761129513383 + 0.1 * 6.43674898147583
Epoch 1320, val loss: 1.8128306865692139
Epoch 1330, training loss: 0.6477997303009033 = 0.003196714213117957 + 0.1 * 6.446030139923096
Epoch 1330, val loss: 1.8162823915481567
Epoch 1340, training loss: 0.6458170413970947 = 0.003151781391352415 + 0.1 * 6.426652431488037
Epoch 1340, val loss: 1.8197312355041504
Epoch 1350, training loss: 0.6481998562812805 = 0.0031078963074833155 + 0.1 * 6.450919151306152
Epoch 1350, val loss: 1.8231831789016724
Epoch 1360, training loss: 0.6459373831748962 = 0.0030651255510747433 + 0.1 * 6.428722381591797
Epoch 1360, val loss: 1.8265048265457153
Epoch 1370, training loss: 0.6483817100524902 = 0.0030236162710934877 + 0.1 * 6.453580856323242
Epoch 1370, val loss: 1.8298248052597046
Epoch 1380, training loss: 0.6457719802856445 = 0.0029829663690179586 + 0.1 * 6.427889823913574
Epoch 1380, val loss: 1.8330159187316895
Epoch 1390, training loss: 0.6455914974212646 = 0.002943576779216528 + 0.1 * 6.426479339599609
Epoch 1390, val loss: 1.8362860679626465
Epoch 1400, training loss: 0.6454165577888489 = 0.0029049257282167673 + 0.1 * 6.425116062164307
Epoch 1400, val loss: 1.8394815921783447
Epoch 1410, training loss: 0.646259069442749 = 0.0028673848137259483 + 0.1 * 6.4339165687561035
Epoch 1410, val loss: 1.8426415920257568
Epoch 1420, training loss: 0.6447803974151611 = 0.0028303838334977627 + 0.1 * 6.41949987411499
Epoch 1420, val loss: 1.84581458568573
Epoch 1430, training loss: 0.6457822918891907 = 0.0027943591121584177 + 0.1 * 6.429879188537598
Epoch 1430, val loss: 1.8488963842391968
Epoch 1440, training loss: 0.6457626223564148 = 0.0027592412661761045 + 0.1 * 6.430034160614014
Epoch 1440, val loss: 1.85194730758667
Epoch 1450, training loss: 0.644320547580719 = 0.002725012367591262 + 0.1 * 6.415955066680908
Epoch 1450, val loss: 1.8549937009811401
Epoch 1460, training loss: 0.6461178660392761 = 0.0026914223562926054 + 0.1 * 6.434264183044434
Epoch 1460, val loss: 1.8580787181854248
Epoch 1470, training loss: 0.6450557112693787 = 0.002658495446667075 + 0.1 * 6.423972129821777
Epoch 1470, val loss: 1.8609904050827026
Epoch 1480, training loss: 0.6449551582336426 = 0.00262646097689867 + 0.1 * 6.423286437988281
Epoch 1480, val loss: 1.8639264106750488
Epoch 1490, training loss: 0.6457999348640442 = 0.0025950272101908922 + 0.1 * 6.43204927444458
Epoch 1490, val loss: 1.8668365478515625
Epoch 1500, training loss: 0.6439448595046997 = 0.0025645187124609947 + 0.1 * 6.413803577423096
Epoch 1500, val loss: 1.8696873188018799
Epoch 1510, training loss: 0.6448622345924377 = 0.0025345361791551113 + 0.1 * 6.423276901245117
Epoch 1510, val loss: 1.8725188970565796
Epoch 1520, training loss: 0.6434881687164307 = 0.002505129436030984 + 0.1 * 6.409830093383789
Epoch 1520, val loss: 1.8753314018249512
Epoch 1530, training loss: 0.6451531648635864 = 0.0024763979017734528 + 0.1 * 6.426767826080322
Epoch 1530, val loss: 1.8781400918960571
Epoch 1540, training loss: 0.6441472172737122 = 0.002448241924867034 + 0.1 * 6.416989326477051
Epoch 1540, val loss: 1.8808412551879883
Epoch 1550, training loss: 0.6433864831924438 = 0.002420809818431735 + 0.1 * 6.409657001495361
Epoch 1550, val loss: 1.8835585117340088
Epoch 1560, training loss: 0.6439891457557678 = 0.002393837319687009 + 0.1 * 6.415952682495117
Epoch 1560, val loss: 1.8862675428390503
Epoch 1570, training loss: 0.6442524194717407 = 0.0023674245458096266 + 0.1 * 6.418849468231201
Epoch 1570, val loss: 1.8889389038085938
Epoch 1580, training loss: 0.6423907279968262 = 0.002341547515243292 + 0.1 * 6.400491237640381
Epoch 1580, val loss: 1.8916057348251343
Epoch 1590, training loss: 0.6450936198234558 = 0.002316158264875412 + 0.1 * 6.427774429321289
Epoch 1590, val loss: 1.8942842483520508
Epoch 1600, training loss: 0.642554521560669 = 0.0022912800777703524 + 0.1 * 6.402632713317871
Epoch 1600, val loss: 1.8968487977981567
Epoch 1610, training loss: 0.6442385315895081 = 0.0022668889723718166 + 0.1 * 6.4197163581848145
Epoch 1610, val loss: 1.8994213342666626
Epoch 1620, training loss: 0.6417285203933716 = 0.0022430885583162308 + 0.1 * 6.3948540687561035
Epoch 1620, val loss: 1.901936411857605
Epoch 1630, training loss: 0.6450219750404358 = 0.0022198145743459463 + 0.1 * 6.4280219078063965
Epoch 1630, val loss: 1.9044078588485718
Epoch 1640, training loss: 0.642909824848175 = 0.0021969403605908155 + 0.1 * 6.40712833404541
Epoch 1640, val loss: 1.9068517684936523
Epoch 1650, training loss: 0.6415042281150818 = 0.0021746396087110043 + 0.1 * 6.393296241760254
Epoch 1650, val loss: 1.9092944860458374
Epoch 1660, training loss: 0.6426900625228882 = 0.0021526156924664974 + 0.1 * 6.405374050140381
Epoch 1660, val loss: 1.9117815494537354
Epoch 1670, training loss: 0.641684353351593 = 0.002130867913365364 + 0.1 * 6.395534992218018
Epoch 1670, val loss: 1.9142125844955444
Epoch 1680, training loss: 0.6418135762214661 = 0.002109657973051071 + 0.1 * 6.397039413452148
Epoch 1680, val loss: 1.9166250228881836
Epoch 1690, training loss: 0.6428611874580383 = 0.0020887861028313637 + 0.1 * 6.407723903656006
Epoch 1690, val loss: 1.9189904928207397
Epoch 1700, training loss: 0.6423023343086243 = 0.0020684741903096437 + 0.1 * 6.40233850479126
Epoch 1700, val loss: 1.9213062524795532
Epoch 1710, training loss: 0.6409085392951965 = 0.002048423746600747 + 0.1 * 6.388600826263428
Epoch 1710, val loss: 1.9236732721328735
Epoch 1720, training loss: 0.6431826949119568 = 0.0020286976359784603 + 0.1 * 6.4115400314331055
Epoch 1720, val loss: 1.9260329008102417
Epoch 1730, training loss: 0.6420073509216309 = 0.0020094099454581738 + 0.1 * 6.399979591369629
Epoch 1730, val loss: 1.9283076524734497
Epoch 1740, training loss: 0.6417272686958313 = 0.0019904293585568666 + 0.1 * 6.39736795425415
Epoch 1740, val loss: 1.93057119846344
Epoch 1750, training loss: 0.641175389289856 = 0.001971973804756999 + 0.1 * 6.39203405380249
Epoch 1750, val loss: 1.9328172206878662
Epoch 1760, training loss: 0.6414608359336853 = 0.001953626051545143 + 0.1 * 6.395071983337402
Epoch 1760, val loss: 1.9350467920303345
Epoch 1770, training loss: 0.6411680579185486 = 0.0019357481505721807 + 0.1 * 6.3923234939575195
Epoch 1770, val loss: 1.9372323751449585
Epoch 1780, training loss: 0.6417360305786133 = 0.0019181100651621819 + 0.1 * 6.398179054260254
Epoch 1780, val loss: 1.9394481182098389
Epoch 1790, training loss: 0.6417532563209534 = 0.0019008320523425937 + 0.1 * 6.398523807525635
Epoch 1790, val loss: 1.941564917564392
Epoch 1800, training loss: 0.640577495098114 = 0.0018838238902390003 + 0.1 * 6.386936664581299
Epoch 1800, val loss: 1.9437172412872314
Epoch 1810, training loss: 0.6416423320770264 = 0.0018671215511858463 + 0.1 * 6.397752285003662
Epoch 1810, val loss: 1.9458550214767456
Epoch 1820, training loss: 0.6402615904808044 = 0.0018506832420825958 + 0.1 * 6.384109020233154
Epoch 1820, val loss: 1.9479811191558838
Epoch 1830, training loss: 0.6413078904151917 = 0.0018344528507441282 + 0.1 * 6.3947343826293945
Epoch 1830, val loss: 1.9500563144683838
Epoch 1840, training loss: 0.6402716040611267 = 0.0018186778761446476 + 0.1 * 6.384529113769531
Epoch 1840, val loss: 1.9520752429962158
Epoch 1850, training loss: 0.6413965225219727 = 0.0018030928913503885 + 0.1 * 6.395934104919434
Epoch 1850, val loss: 1.9541202783584595
Epoch 1860, training loss: 0.6397867798805237 = 0.0017876399215310812 + 0.1 * 6.379991054534912
Epoch 1860, val loss: 1.9562090635299683
Epoch 1870, training loss: 0.641592264175415 = 0.0017724856734275818 + 0.1 * 6.398198127746582
Epoch 1870, val loss: 1.9582479000091553
Epoch 1880, training loss: 0.6397895216941833 = 0.0017576507525518537 + 0.1 * 6.380319118499756
Epoch 1880, val loss: 1.9602148532867432
Epoch 1890, training loss: 0.6414273977279663 = 0.0017431409796699882 + 0.1 * 6.3968424797058105
Epoch 1890, val loss: 1.9621834754943848
Epoch 1900, training loss: 0.6406480073928833 = 0.001728739240206778 + 0.1 * 6.389193058013916
Epoch 1900, val loss: 1.9641284942626953
Epoch 1910, training loss: 0.6395480632781982 = 0.0017146774334833026 + 0.1 * 6.378333568572998
Epoch 1910, val loss: 1.9660841226577759
Epoch 1920, training loss: 0.6405991911888123 = 0.00170075090136379 + 0.1 * 6.388984203338623
Epoch 1920, val loss: 1.9680356979370117
Epoch 1930, training loss: 0.6409303545951843 = 0.0016870753606781363 + 0.1 * 6.392433166503906
Epoch 1930, val loss: 1.9699372053146362
Epoch 1940, training loss: 0.6398641467094421 = 0.0016736517427489161 + 0.1 * 6.3819050788879395
Epoch 1940, val loss: 1.9718551635742188
Epoch 1950, training loss: 0.6397585868835449 = 0.0016603880794718862 + 0.1 * 6.380981922149658
Epoch 1950, val loss: 1.9737238883972168
Epoch 1960, training loss: 0.6396186351776123 = 0.001647356664761901 + 0.1 * 6.3797125816345215
Epoch 1960, val loss: 1.9756240844726562
Epoch 1970, training loss: 0.6404445767402649 = 0.0016344883479177952 + 0.1 * 6.388101100921631
Epoch 1970, val loss: 1.9775010347366333
Epoch 1980, training loss: 0.6387599110603333 = 0.001621808740310371 + 0.1 * 6.3713812828063965
Epoch 1980, val loss: 1.9793277978897095
Epoch 1990, training loss: 0.6397925019264221 = 0.0016093298327177763 + 0.1 * 6.381831645965576
Epoch 1990, val loss: 1.9811636209487915
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8096995255666843
The final CL Acc:0.75309, 0.00972, The final GNN Acc:0.80935, 0.00050
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13196])
remove edge: torch.Size([2, 7712])
updated graph: torch.Size([2, 10352])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8025784492492676 = 1.942893385887146 + 0.1 * 8.596851348876953
Epoch 0, val loss: 1.9435765743255615
Epoch 10, training loss: 2.7923855781555176 = 1.9327129125595093 + 0.1 * 8.59672737121582
Epoch 10, val loss: 1.93330979347229
Epoch 20, training loss: 2.7796554565429688 = 1.9200764894485474 + 0.1 * 8.595789909362793
Epoch 20, val loss: 1.9203377962112427
Epoch 30, training loss: 2.761073350906372 = 1.9023137092590332 + 0.1 * 8.587596893310547
Epoch 30, val loss: 1.9020847082138062
Epoch 40, training loss: 2.730404853820801 = 1.8763295412063599 + 0.1 * 8.540753364562988
Epoch 40, val loss: 1.8758840560913086
Epoch 50, training loss: 2.67403507232666 = 1.8413946628570557 + 0.1 * 8.32640266418457
Epoch 50, val loss: 1.8425688743591309
Epoch 60, training loss: 2.604095458984375 = 1.8016701936721802 + 0.1 * 8.024251937866211
Epoch 60, val loss: 1.807310700416565
Epoch 70, training loss: 2.5466389656066895 = 1.7628166675567627 + 0.1 * 7.838223457336426
Epoch 70, val loss: 1.7744559049606323
Epoch 80, training loss: 2.469926118850708 = 1.718892216682434 + 0.1 * 7.510338306427002
Epoch 80, val loss: 1.7348921298980713
Epoch 90, training loss: 2.390697956085205 = 1.6617673635482788 + 0.1 * 7.289304733276367
Epoch 90, val loss: 1.682200312614441
Epoch 100, training loss: 2.30668044090271 = 1.586981177330017 + 0.1 * 7.196991920471191
Epoch 100, val loss: 1.6139999628067017
Epoch 110, training loss: 2.2077808380126953 = 1.4945180416107178 + 0.1 * 7.132627010345459
Epoch 110, val loss: 1.5320870876312256
Epoch 120, training loss: 2.0977554321289062 = 1.3904190063476562 + 0.1 * 7.073365211486816
Epoch 120, val loss: 1.4421162605285645
Epoch 130, training loss: 1.985598087310791 = 1.2822651863098145 + 0.1 * 7.033329010009766
Epoch 130, val loss: 1.3506630659103394
Epoch 140, training loss: 1.8767833709716797 = 1.1752263307571411 + 0.1 * 7.015570640563965
Epoch 140, val loss: 1.2627408504486084
Epoch 150, training loss: 1.7729523181915283 = 1.072736382484436 + 0.1 * 7.002159118652344
Epoch 150, val loss: 1.179200530052185
Epoch 160, training loss: 1.6765313148498535 = 0.9778414368629456 + 0.1 * 6.986897945404053
Epoch 160, val loss: 1.1037079095840454
Epoch 170, training loss: 1.5882220268249512 = 0.8917325735092163 + 0.1 * 6.9648942947387695
Epoch 170, val loss: 1.03670072555542
Epoch 180, training loss: 1.507655382156372 = 0.8137263059616089 + 0.1 * 6.939291477203369
Epoch 180, val loss: 0.977543830871582
Epoch 190, training loss: 1.4346046447753906 = 0.7435315251350403 + 0.1 * 6.910731792449951
Epoch 190, val loss: 0.9262366890907288
Epoch 200, training loss: 1.3680338859558105 = 0.6797928810119629 + 0.1 * 6.882410049438477
Epoch 200, val loss: 0.8820924758911133
Epoch 210, training loss: 1.3080542087554932 = 0.621719241142273 + 0.1 * 6.863348960876465
Epoch 210, val loss: 0.8450930118560791
Epoch 220, training loss: 1.2521531581878662 = 0.5682632923126221 + 0.1 * 6.838898658752441
Epoch 220, val loss: 0.8147087693214417
Epoch 230, training loss: 1.200242042541504 = 0.5180416703224182 + 0.1 * 6.82200288772583
Epoch 230, val loss: 0.789729654788971
Epoch 240, training loss: 1.1514074802398682 = 0.4708690643310547 + 0.1 * 6.805384159088135
Epoch 240, val loss: 0.7694815993309021
Epoch 250, training loss: 1.106006383895874 = 0.42641106247901917 + 0.1 * 6.795953273773193
Epoch 250, val loss: 0.7534793615341187
Epoch 260, training loss: 1.0639939308166504 = 0.385022908449173 + 0.1 * 6.78971004486084
Epoch 260, val loss: 0.7417922019958496
Epoch 270, training loss: 1.0249687433242798 = 0.3466718792915344 + 0.1 * 6.782968521118164
Epoch 270, val loss: 0.7342618703842163
Epoch 280, training loss: 0.9891300201416016 = 0.31130486726760864 + 0.1 * 6.7782511711120605
Epoch 280, val loss: 0.7307055592536926
Epoch 290, training loss: 0.9560067653656006 = 0.2787441313266754 + 0.1 * 6.772625923156738
Epoch 290, val loss: 0.7306076884269714
Epoch 300, training loss: 0.9255461692810059 = 0.24885745346546173 + 0.1 * 6.766887187957764
Epoch 300, val loss: 0.7337786555290222
Epoch 310, training loss: 0.8982826471328735 = 0.221695214509964 + 0.1 * 6.76587438583374
Epoch 310, val loss: 0.7398751378059387
Epoch 320, training loss: 0.8735014200210571 = 0.19739453494548798 + 0.1 * 6.761068820953369
Epoch 320, val loss: 0.7485293745994568
Epoch 330, training loss: 0.8513602018356323 = 0.17582984268665314 + 0.1 * 6.755303859710693
Epoch 330, val loss: 0.7592682242393494
Epoch 340, training loss: 0.8317317962646484 = 0.15678814053535461 + 0.1 * 6.749435901641846
Epoch 340, val loss: 0.7719099521636963
Epoch 350, training loss: 0.8147106170654297 = 0.14009767770767212 + 0.1 * 6.746129512786865
Epoch 350, val loss: 0.7857929468154907
Epoch 360, training loss: 0.7997795939445496 = 0.12548957765102386 + 0.1 * 6.7428998947143555
Epoch 360, val loss: 0.8007398247718811
Epoch 370, training loss: 0.786245584487915 = 0.11270502954721451 + 0.1 * 6.735404968261719
Epoch 370, val loss: 0.8164728283882141
Epoch 380, training loss: 0.7755200266838074 = 0.10154955089092255 + 0.1 * 6.739704608917236
Epoch 380, val loss: 0.832568883895874
Epoch 390, training loss: 0.7642054557800293 = 0.0918346643447876 + 0.1 * 6.723707675933838
Epoch 390, val loss: 0.8489128947257996
Epoch 400, training loss: 0.7548413872718811 = 0.08329571038484573 + 0.1 * 6.715456485748291
Epoch 400, val loss: 0.865464448928833
Epoch 410, training loss: 0.7474943995475769 = 0.07575863599777222 + 0.1 * 6.717357635498047
Epoch 410, val loss: 0.8820948004722595
Epoch 420, training loss: 0.7395042181015015 = 0.06914818286895752 + 0.1 * 6.7035603523254395
Epoch 420, val loss: 0.8983574509620667
Epoch 430, training loss: 0.7332755923271179 = 0.06331710517406464 + 0.1 * 6.699584484100342
Epoch 430, val loss: 0.9143663048744202
Epoch 440, training loss: 0.7287601828575134 = 0.05815412849187851 + 0.1 * 6.706060409545898
Epoch 440, val loss: 0.930009126663208
Epoch 450, training loss: 0.7225579023361206 = 0.05357877165079117 + 0.1 * 6.689791202545166
Epoch 450, val loss: 0.9452059268951416
Epoch 460, training loss: 0.7170116901397705 = 0.04947926476597786 + 0.1 * 6.675324440002441
Epoch 460, val loss: 0.9600979089736938
Epoch 470, training loss: 0.7126240134239197 = 0.04578615725040436 + 0.1 * 6.6683783531188965
Epoch 470, val loss: 0.9747006893157959
Epoch 480, training loss: 0.7103317379951477 = 0.04245910421013832 + 0.1 * 6.678725719451904
Epoch 480, val loss: 0.9888894557952881
Epoch 490, training loss: 0.7059416770935059 = 0.03947854042053223 + 0.1 * 6.664631366729736
Epoch 490, val loss: 1.0024880170822144
Epoch 500, training loss: 0.702079176902771 = 0.036777887493371964 + 0.1 * 6.653012752532959
Epoch 500, val loss: 1.0157556533813477
Epoch 510, training loss: 0.6988853216171265 = 0.03432183340191841 + 0.1 * 6.645634651184082
Epoch 510, val loss: 1.0287631750106812
Epoch 520, training loss: 0.6964945793151855 = 0.03208846598863602 + 0.1 * 6.644061088562012
Epoch 520, val loss: 1.041351556777954
Epoch 530, training loss: 0.6940984129905701 = 0.030062025412917137 + 0.1 * 6.640363693237305
Epoch 530, val loss: 1.0535187721252441
Epoch 540, training loss: 0.6912935972213745 = 0.028211280703544617 + 0.1 * 6.630823135375977
Epoch 540, val loss: 1.0654288530349731
Epoch 550, training loss: 0.6916522979736328 = 0.02651781216263771 + 0.1 * 6.6513447761535645
Epoch 550, val loss: 1.0769838094711304
Epoch 560, training loss: 0.6871351003646851 = 0.024975528940558434 + 0.1 * 6.62159538269043
Epoch 560, val loss: 1.0880650281906128
Epoch 570, training loss: 0.6850870847702026 = 0.023565014824271202 + 0.1 * 6.61522102355957
Epoch 570, val loss: 1.0988682508468628
Epoch 580, training loss: 0.6847020983695984 = 0.022266073152422905 + 0.1 * 6.624360084533691
Epoch 580, val loss: 1.1094112396240234
Epoch 590, training loss: 0.6821720004081726 = 0.021068580448627472 + 0.1 * 6.611033916473389
Epoch 590, val loss: 1.1196277141571045
Epoch 600, training loss: 0.6799468994140625 = 0.019966891035437584 + 0.1 * 6.599799633026123
Epoch 600, val loss: 1.129592776298523
Epoch 610, training loss: 0.6834629774093628 = 0.01894707977771759 + 0.1 * 6.645158767700195
Epoch 610, val loss: 1.1392639875411987
Epoch 620, training loss: 0.6789276003837585 = 0.01801670715212822 + 0.1 * 6.609108924865723
Epoch 620, val loss: 1.1485337018966675
Epoch 630, training loss: 0.6759918332099915 = 0.017150720581412315 + 0.1 * 6.588411331176758
Epoch 630, val loss: 1.157670259475708
Epoch 640, training loss: 0.6748432517051697 = 0.01634739153087139 + 0.1 * 6.584958076477051
Epoch 640, val loss: 1.1665959358215332
Epoch 650, training loss: 0.6743879914283752 = 0.015599282458424568 + 0.1 * 6.587886810302734
Epoch 650, val loss: 1.1752305030822754
Epoch 660, training loss: 0.6720994114875793 = 0.014906472526490688 + 0.1 * 6.571929454803467
Epoch 660, val loss: 1.1836624145507812
Epoch 670, training loss: 0.67108553647995 = 0.01425767969340086 + 0.1 * 6.5682783126831055
Epoch 670, val loss: 1.191925287246704
Epoch 680, training loss: 0.6706745624542236 = 0.01365227997303009 + 0.1 * 6.570222854614258
Epoch 680, val loss: 1.199989676475525
Epoch 690, training loss: 0.6709391474723816 = 0.013083724305033684 + 0.1 * 6.578554630279541
Epoch 690, val loss: 1.2078850269317627
Epoch 700, training loss: 0.669590950012207 = 0.012556108646094799 + 0.1 * 6.570348262786865
Epoch 700, val loss: 1.2154465913772583
Epoch 710, training loss: 0.6680149435997009 = 0.01206163503229618 + 0.1 * 6.559532642364502
Epoch 710, val loss: 1.2229171991348267
Epoch 720, training loss: 0.6672533750534058 = 0.011595931835472584 + 0.1 * 6.55657434463501
Epoch 720, val loss: 1.230277419090271
Epoch 730, training loss: 0.6678484082221985 = 0.011157154105603695 + 0.1 * 6.566912651062012
Epoch 730, val loss: 1.2374123334884644
Epoch 740, training loss: 0.6682385802268982 = 0.0107468506321311 + 0.1 * 6.574917316436768
Epoch 740, val loss: 1.2443630695343018
Epoch 750, training loss: 0.6662886142730713 = 0.010360738262534142 + 0.1 * 6.559278964996338
Epoch 750, val loss: 1.2510989904403687
Epoch 760, training loss: 0.6647467017173767 = 0.0099985022097826 + 0.1 * 6.547482013702393
Epoch 760, val loss: 1.2577372789382935
Epoch 770, training loss: 0.6633270978927612 = 0.009655189700424671 + 0.1 * 6.536718845367432
Epoch 770, val loss: 1.2642349004745483
Epoch 780, training loss: 0.664277970790863 = 0.009329037740826607 + 0.1 * 6.549489498138428
Epoch 780, val loss: 1.2705880403518677
Epoch 790, training loss: 0.663828432559967 = 0.009022101759910583 + 0.1 * 6.548062801361084
Epoch 790, val loss: 1.276832103729248
Epoch 800, training loss: 0.6638005375862122 = 0.008731076493859291 + 0.1 * 6.550694465637207
Epoch 800, val loss: 1.2828688621520996
Epoch 810, training loss: 0.66171795129776 = 0.008456659503281116 + 0.1 * 6.5326128005981445
Epoch 810, val loss: 1.288783311843872
Epoch 820, training loss: 0.6604884266853333 = 0.008196688257157803 + 0.1 * 6.522916793823242
Epoch 820, val loss: 1.2945762872695923
Epoch 830, training loss: 0.6616001129150391 = 0.007948537357151508 + 0.1 * 6.536515712738037
Epoch 830, val loss: 1.3002997636795044
Epoch 840, training loss: 0.6603316068649292 = 0.007712909020483494 + 0.1 * 6.526186943054199
Epoch 840, val loss: 1.3058559894561768
Epoch 850, training loss: 0.660504937171936 = 0.007491204421967268 + 0.1 * 6.530137062072754
Epoch 850, val loss: 1.311220645904541
Epoch 860, training loss: 0.6581728458404541 = 0.007279214449226856 + 0.1 * 6.508935928344727
Epoch 860, val loss: 1.3165518045425415
Epoch 870, training loss: 0.6585306525230408 = 0.007076307199895382 + 0.1 * 6.514543056488037
Epoch 870, val loss: 1.3217722177505493
Epoch 880, training loss: 0.6589636206626892 = 0.006883286405354738 + 0.1 * 6.520803451538086
Epoch 880, val loss: 1.3268682956695557
Epoch 890, training loss: 0.6571572422981262 = 0.006699992343783379 + 0.1 * 6.50457239151001
Epoch 890, val loss: 1.331910252571106
Epoch 900, training loss: 0.657189667224884 = 0.006524196360260248 + 0.1 * 6.506654262542725
Epoch 900, val loss: 1.3368281126022339
Epoch 910, training loss: 0.6577705144882202 = 0.00635560741648078 + 0.1 * 6.514149188995361
Epoch 910, val loss: 1.3416972160339355
Epoch 920, training loss: 0.6547425389289856 = 0.006195219699293375 + 0.1 * 6.485472679138184
Epoch 920, val loss: 1.3464341163635254
Epoch 930, training loss: 0.6550082564353943 = 0.006041666027158499 + 0.1 * 6.489665985107422
Epoch 930, val loss: 1.3510979413986206
Epoch 940, training loss: 0.6549206376075745 = 0.005894334986805916 + 0.1 * 6.490262508392334
Epoch 940, val loss: 1.3557217121124268
Epoch 950, training loss: 0.6555123329162598 = 0.005753121804445982 + 0.1 * 6.497592449188232
Epoch 950, val loss: 1.360211730003357
Epoch 960, training loss: 0.653724193572998 = 0.005617936607450247 + 0.1 * 6.481062889099121
Epoch 960, val loss: 1.3646589517593384
Epoch 970, training loss: 0.6527266502380371 = 0.0054877521470189095 + 0.1 * 6.472388744354248
Epoch 970, val loss: 1.3689870834350586
Epoch 980, training loss: 0.6532177329063416 = 0.005362595431506634 + 0.1 * 6.478551387786865
Epoch 980, val loss: 1.3733619451522827
Epoch 990, training loss: 0.6540838479995728 = 0.0052427081391215324 + 0.1 * 6.4884114265441895
Epoch 990, val loss: 1.3775440454483032
Epoch 1000, training loss: 0.6520465016365051 = 0.005127576645463705 + 0.1 * 6.469189643859863
Epoch 1000, val loss: 1.3816369771957397
Epoch 1010, training loss: 0.6539703011512756 = 0.005016393959522247 + 0.1 * 6.48953914642334
Epoch 1010, val loss: 1.3857052326202393
Epoch 1020, training loss: 0.6516344547271729 = 0.004909851588308811 + 0.1 * 6.467245578765869
Epoch 1020, val loss: 1.3897706270217896
Epoch 1030, training loss: 0.6499701142311096 = 0.004806711804121733 + 0.1 * 6.451633930206299
Epoch 1030, val loss: 1.393723487854004
Epoch 1040, training loss: 0.6502342820167542 = 0.0047071692533791065 + 0.1 * 6.455271244049072
Epoch 1040, val loss: 1.3976287841796875
Epoch 1050, training loss: 0.6500493884086609 = 0.004611335229128599 + 0.1 * 6.454380512237549
Epoch 1050, val loss: 1.401530146598816
Epoch 1060, training loss: 0.6495475172996521 = 0.004518607165664434 + 0.1 * 6.450288772583008
Epoch 1060, val loss: 1.405230164527893
Epoch 1070, training loss: 0.6516013145446777 = 0.004429508000612259 + 0.1 * 6.471717834472656
Epoch 1070, val loss: 1.4089831113815308
Epoch 1080, training loss: 0.6497604846954346 = 0.004343298729509115 + 0.1 * 6.454172134399414
Epoch 1080, val loss: 1.4126513004302979
Epoch 1090, training loss: 0.649188220500946 = 0.004260375630110502 + 0.1 * 6.449278354644775
Epoch 1090, val loss: 1.4163044691085815
Epoch 1100, training loss: 0.6480166912078857 = 0.004179305396974087 + 0.1 * 6.438374042510986
Epoch 1100, val loss: 1.419874668121338
Epoch 1110, training loss: 0.6498183012008667 = 0.004101309925317764 + 0.1 * 6.457169532775879
Epoch 1110, val loss: 1.4234404563903809
Epoch 1120, training loss: 0.6497536897659302 = 0.004025651142001152 + 0.1 * 6.45728063583374
Epoch 1120, val loss: 1.4268733263015747
Epoch 1130, training loss: 0.6475191116333008 = 0.003952717408537865 + 0.1 * 6.43566370010376
Epoch 1130, val loss: 1.43032705783844
Epoch 1140, training loss: 0.6474205851554871 = 0.0038819387555122375 + 0.1 * 6.435386657714844
Epoch 1140, val loss: 1.4337037801742554
Epoch 1150, training loss: 0.6493597626686096 = 0.0038133019115775824 + 0.1 * 6.4554643630981445
Epoch 1150, val loss: 1.4370375871658325
Epoch 1160, training loss: 0.6471565961837769 = 0.0037470886018127203 + 0.1 * 6.4340949058532715
Epoch 1160, val loss: 1.4403022527694702
Epoch 1170, training loss: 0.6483209133148193 = 0.0036828843876719475 + 0.1 * 6.446379661560059
Epoch 1170, val loss: 1.4435479640960693
Epoch 1180, training loss: 0.646239161491394 = 0.0036203539930284023 + 0.1 * 6.426187992095947
Epoch 1180, val loss: 1.446757435798645
Epoch 1190, training loss: 0.6484008431434631 = 0.003559697885066271 + 0.1 * 6.448410987854004
Epoch 1190, val loss: 1.4498944282531738
Epoch 1200, training loss: 0.6457675099372864 = 0.003501104423776269 + 0.1 * 6.422663688659668
Epoch 1200, val loss: 1.452955722808838
Epoch 1210, training loss: 0.6471443772315979 = 0.0034441878087818623 + 0.1 * 6.437001705169678
Epoch 1210, val loss: 1.4560400247573853
Epoch 1220, training loss: 0.6454389691352844 = 0.0033886327873915434 + 0.1 * 6.42050313949585
Epoch 1220, val loss: 1.459060549736023
Epoch 1230, training loss: 0.6451951265335083 = 0.003334863344207406 + 0.1 * 6.418602466583252
Epoch 1230, val loss: 1.4620405435562134
Epoch 1240, training loss: 0.6455162763595581 = 0.0032824124209582806 + 0.1 * 6.422338485717773
Epoch 1240, val loss: 1.4650402069091797
Epoch 1250, training loss: 0.6447759866714478 = 0.0032314255367964506 + 0.1 * 6.415445804595947
Epoch 1250, val loss: 1.4679079055786133
Epoch 1260, training loss: 0.6451358199119568 = 0.0031821667216718197 + 0.1 * 6.419536590576172
Epoch 1260, val loss: 1.470795750617981
Epoch 1270, training loss: 0.6451553106307983 = 0.0031342599540948868 + 0.1 * 6.420210361480713
Epoch 1270, val loss: 1.4736523628234863
Epoch 1280, training loss: 0.6447003483772278 = 0.0030874728690832853 + 0.1 * 6.416128635406494
Epoch 1280, val loss: 1.4764317274093628
Epoch 1290, training loss: 0.6446787118911743 = 0.0030419162940233946 + 0.1 * 6.416368007659912
Epoch 1290, val loss: 1.4792214632034302
Epoch 1300, training loss: 0.6441931128501892 = 0.00299763772636652 + 0.1 * 6.411954879760742
Epoch 1300, val loss: 1.481900930404663
Epoch 1310, training loss: 0.6443321108818054 = 0.002954634139314294 + 0.1 * 6.413774490356445
Epoch 1310, val loss: 1.4845986366271973
Epoch 1320, training loss: 0.6449340581893921 = 0.0029128773603588343 + 0.1 * 6.4202117919921875
Epoch 1320, val loss: 1.4872901439666748
Epoch 1330, training loss: 0.6444069743156433 = 0.002872020471841097 + 0.1 * 6.415349006652832
Epoch 1330, val loss: 1.4899476766586304
Epoch 1340, training loss: 0.6431213021278381 = 0.0028322930447757244 + 0.1 * 6.402890205383301
Epoch 1340, val loss: 1.4924945831298828
Epoch 1350, training loss: 0.6442708969116211 = 0.002793674124404788 + 0.1 * 6.414772033691406
Epoch 1350, val loss: 1.4949959516525269
Epoch 1360, training loss: 0.6430799961090088 = 0.0027560547459870577 + 0.1 * 6.4032392501831055
Epoch 1360, val loss: 1.4975786209106445
Epoch 1370, training loss: 0.6439154148101807 = 0.002719227457419038 + 0.1 * 6.411962032318115
Epoch 1370, val loss: 1.4999765157699585
Epoch 1380, training loss: 0.6439449787139893 = 0.0026837040204554796 + 0.1 * 6.412612438201904
Epoch 1380, val loss: 1.5024725198745728
Epoch 1390, training loss: 0.6420965790748596 = 0.0026487556751817465 + 0.1 * 6.394477844238281
Epoch 1390, val loss: 1.504883885383606
Epoch 1400, training loss: 0.6448021531105042 = 0.002614635741338134 + 0.1 * 6.421875476837158
Epoch 1400, val loss: 1.5072869062423706
Epoch 1410, training loss: 0.6425082683563232 = 0.0025811924133449793 + 0.1 * 6.399271011352539
Epoch 1410, val loss: 1.5096689462661743
Epoch 1420, training loss: 0.6427811980247498 = 0.0025487327948212624 + 0.1 * 6.402324199676514
Epoch 1420, val loss: 1.5119657516479492
Epoch 1430, training loss: 0.6427991986274719 = 0.002516999840736389 + 0.1 * 6.402822017669678
Epoch 1430, val loss: 1.5142916440963745
Epoch 1440, training loss: 0.642827033996582 = 0.0024859195109456778 + 0.1 * 6.403410911560059
Epoch 1440, val loss: 1.516530156135559
Epoch 1450, training loss: 0.6424883008003235 = 0.0024555749259889126 + 0.1 * 6.400327205657959
Epoch 1450, val loss: 1.5187933444976807
Epoch 1460, training loss: 0.6428000926971436 = 0.0024259649217128754 + 0.1 * 6.403741359710693
Epoch 1460, val loss: 1.5210081338882446
Epoch 1470, training loss: 0.6421464085578918 = 0.0023970287293195724 + 0.1 * 6.397493839263916
Epoch 1470, val loss: 1.5232316255569458
Epoch 1480, training loss: 0.643143355846405 = 0.002368621528148651 + 0.1 * 6.407747268676758
Epoch 1480, val loss: 1.5254106521606445
Epoch 1490, training loss: 0.6411647200584412 = 0.0023410944268107414 + 0.1 * 6.388236045837402
Epoch 1490, val loss: 1.5275814533233643
Epoch 1500, training loss: 0.6418670415878296 = 0.002314093289896846 + 0.1 * 6.395529270172119
Epoch 1500, val loss: 1.5296732187271118
Epoch 1510, training loss: 0.641642153263092 = 0.0022876854054629803 + 0.1 * 6.393544673919678
Epoch 1510, val loss: 1.5317938327789307
Epoch 1520, training loss: 0.6408597826957703 = 0.0022617557551711798 + 0.1 * 6.385980129241943
Epoch 1520, val loss: 1.5338712930679321
Epoch 1530, training loss: 0.6413173079490662 = 0.002236433792859316 + 0.1 * 6.390808582305908
Epoch 1530, val loss: 1.535937786102295
Epoch 1540, training loss: 0.6406776309013367 = 0.002211662707850337 + 0.1 * 6.384659290313721
Epoch 1540, val loss: 1.5379778146743774
Epoch 1550, training loss: 0.6417633891105652 = 0.002187238074839115 + 0.1 * 6.395761489868164
Epoch 1550, val loss: 1.5399682521820068
Epoch 1560, training loss: 0.6407497525215149 = 0.002163554076105356 + 0.1 * 6.385861873626709
Epoch 1560, val loss: 1.5419296026229858
Epoch 1570, training loss: 0.6404406428337097 = 0.0021402391139417887 + 0.1 * 6.383004188537598
Epoch 1570, val loss: 1.5439022779464722
Epoch 1580, training loss: 0.6406253576278687 = 0.0021174163557589054 + 0.1 * 6.385079383850098
Epoch 1580, val loss: 1.5458450317382812
Epoch 1590, training loss: 0.6415862441062927 = 0.002095132600516081 + 0.1 * 6.394911289215088
Epoch 1590, val loss: 1.5477508306503296
Epoch 1600, training loss: 0.6400632858276367 = 0.0020732684060931206 + 0.1 * 6.379899978637695
Epoch 1600, val loss: 1.5497008562088013
Epoch 1610, training loss: 0.6427261233329773 = 0.0020518351811915636 + 0.1 * 6.406743049621582
Epoch 1610, val loss: 1.5515371561050415
Epoch 1620, training loss: 0.6402343511581421 = 0.0020308520179241896 + 0.1 * 6.382034778594971
Epoch 1620, val loss: 1.553479552268982
Epoch 1630, training loss: 0.6402133703231812 = 0.002010256750509143 + 0.1 * 6.382030487060547
Epoch 1630, val loss: 1.5552722215652466
Epoch 1640, training loss: 0.6400043368339539 = 0.001990091986954212 + 0.1 * 6.3801422119140625
Epoch 1640, val loss: 1.5571540594100952
Epoch 1650, training loss: 0.640023946762085 = 0.001970444805920124 + 0.1 * 6.380534648895264
Epoch 1650, val loss: 1.558966040611267
Epoch 1660, training loss: 0.6394112706184387 = 0.0019509252160787582 + 0.1 * 6.374603271484375
Epoch 1660, val loss: 1.5607855319976807
Epoch 1670, training loss: 0.6413275003433228 = 0.001931903068907559 + 0.1 * 6.393955707550049
Epoch 1670, val loss: 1.562576174736023
Epoch 1680, training loss: 0.6387730240821838 = 0.0019131987355649471 + 0.1 * 6.368597984313965
Epoch 1680, val loss: 1.5643538236618042
Epoch 1690, training loss: 0.6404502987861633 = 0.0018948197830468416 + 0.1 * 6.385554313659668
Epoch 1690, val loss: 1.5660550594329834
Epoch 1700, training loss: 0.6399083733558655 = 0.0018769416492432356 + 0.1 * 6.380313873291016
Epoch 1700, val loss: 1.5678242444992065
Epoch 1710, training loss: 0.6400960683822632 = 0.0018592202104628086 + 0.1 * 6.382368087768555
Epoch 1710, val loss: 1.5694503784179688
Epoch 1720, training loss: 0.6385361552238464 = 0.0018419900443404913 + 0.1 * 6.366941928863525
Epoch 1720, val loss: 1.5711497068405151
Epoch 1730, training loss: 0.6381741166114807 = 0.0018249474233016372 + 0.1 * 6.363491535186768
Epoch 1730, val loss: 1.5727852582931519
Epoch 1740, training loss: 0.6380323171615601 = 0.0018082935130223632 + 0.1 * 6.362240314483643
Epoch 1740, val loss: 1.5744680166244507
Epoch 1750, training loss: 0.6396481990814209 = 0.0017918808152899146 + 0.1 * 6.378562927246094
Epoch 1750, val loss: 1.576075553894043
Epoch 1760, training loss: 0.639059841632843 = 0.001775870332494378 + 0.1 * 6.37283992767334
Epoch 1760, val loss: 1.5776561498641968
Epoch 1770, training loss: 0.6393347978591919 = 0.0017600740538910031 + 0.1 * 6.375747203826904
Epoch 1770, val loss: 1.5792561769485474
Epoch 1780, training loss: 0.6383230686187744 = 0.001744594075717032 + 0.1 * 6.365784645080566
Epoch 1780, val loss: 1.5807784795761108
Epoch 1790, training loss: 0.6381320953369141 = 0.001729384297505021 + 0.1 * 6.36402702331543
Epoch 1790, val loss: 1.5823785066604614
Epoch 1800, training loss: 0.6402077078819275 = 0.0017144031589850783 + 0.1 * 6.384932994842529
Epoch 1800, val loss: 1.5839293003082275
Epoch 1810, training loss: 0.6378936767578125 = 0.0016995416954159737 + 0.1 * 6.361940860748291
Epoch 1810, val loss: 1.5854477882385254
Epoch 1820, training loss: 0.6379206776618958 = 0.0016851559048518538 + 0.1 * 6.3623552322387695
Epoch 1820, val loss: 1.586959958076477
Epoch 1830, training loss: 0.6401500701904297 = 0.001670926809310913 + 0.1 * 6.384791374206543
Epoch 1830, val loss: 1.5884431600570679
Epoch 1840, training loss: 0.6374791264533997 = 0.0016570003936067224 + 0.1 * 6.358221054077148
Epoch 1840, val loss: 1.5899074077606201
Epoch 1850, training loss: 0.6370588541030884 = 0.0016432414995506406 + 0.1 * 6.354156017303467
Epoch 1850, val loss: 1.5913933515548706
Epoch 1860, training loss: 0.6389537453651428 = 0.0016296431422233582 + 0.1 * 6.373240947723389
Epoch 1860, val loss: 1.5928512811660767
Epoch 1870, training loss: 0.6373602747917175 = 0.001616352703422308 + 0.1 * 6.357439041137695
Epoch 1870, val loss: 1.5942872762680054
Epoch 1880, training loss: 0.6390459537506104 = 0.0016032066196203232 + 0.1 * 6.374427318572998
Epoch 1880, val loss: 1.5956796407699585
Epoch 1890, training loss: 0.6368953585624695 = 0.0015903851017355919 + 0.1 * 6.3530497550964355
Epoch 1890, val loss: 1.5971324443817139
Epoch 1900, training loss: 0.6394434571266174 = 0.0015776923391968012 + 0.1 * 6.378657817840576
Epoch 1900, val loss: 1.5984902381896973
Epoch 1910, training loss: 0.6374321579933167 = 0.0015652080764994025 + 0.1 * 6.358669281005859
Epoch 1910, val loss: 1.5999327898025513
Epoch 1920, training loss: 0.6386899352073669 = 0.0015529111260548234 + 0.1 * 6.3713698387146
Epoch 1920, val loss: 1.6012846231460571
Epoch 1930, training loss: 0.6374830603599548 = 0.001540794619359076 + 0.1 * 6.35942268371582
Epoch 1930, val loss: 1.60260009765625
Epoch 1940, training loss: 0.6373987793922424 = 0.0015288792783394456 + 0.1 * 6.358698844909668
Epoch 1940, val loss: 1.6039447784423828
Epoch 1950, training loss: 0.6378602981567383 = 0.0015171186532825232 + 0.1 * 6.363431930541992
Epoch 1950, val loss: 1.6053227186203003
Epoch 1960, training loss: 0.6363393664360046 = 0.0015055857365950942 + 0.1 * 6.348337173461914
Epoch 1960, val loss: 1.6066594123840332
Epoch 1970, training loss: 0.6382156610488892 = 0.0014941691188141704 + 0.1 * 6.367214679718018
Epoch 1970, val loss: 1.6079350709915161
Epoch 1980, training loss: 0.6366407871246338 = 0.0014829352730885148 + 0.1 * 6.351578235626221
Epoch 1980, val loss: 1.6092561483383179
Epoch 1990, training loss: 0.6381990313529968 = 0.0014719164464622736 + 0.1 * 6.3672709465026855
Epoch 1990, val loss: 1.6105211973190308
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 2.8137166500091553 = 1.9540313482284546 + 0.1 * 8.59685230255127
Epoch 0, val loss: 1.9585548639297485
Epoch 10, training loss: 2.80328106880188 = 1.9436030387878418 + 0.1 * 8.596780776977539
Epoch 10, val loss: 1.9477730989456177
Epoch 20, training loss: 2.790860652923584 = 1.9312398433685303 + 0.1 * 8.596206665039062
Epoch 20, val loss: 1.934747576713562
Epoch 30, training loss: 2.7732179164886475 = 1.9141730070114136 + 0.1 * 8.590448379516602
Epoch 30, val loss: 1.9168531894683838
Epoch 40, training loss: 2.7435150146484375 = 1.8890461921691895 + 0.1 * 8.544687271118164
Epoch 40, val loss: 1.8907667398452759
Epoch 50, training loss: 2.68211030960083 = 1.8540152311325073 + 0.1 * 8.280949592590332
Epoch 50, val loss: 1.855822205543518
Epoch 60, training loss: 2.616366147994995 = 1.8141982555389404 + 0.1 * 8.02167797088623
Epoch 60, val loss: 1.8188238143920898
Epoch 70, training loss: 2.555110454559326 = 1.7777249813079834 + 0.1 * 7.7738542556762695
Epoch 70, val loss: 1.7869375944137573
Epoch 80, training loss: 2.482053279876709 = 1.7412294149398804 + 0.1 * 7.408239841461182
Epoch 80, val loss: 1.7540944814682007
Epoch 90, training loss: 2.4083986282348633 = 1.6940224170684814 + 0.1 * 7.143760681152344
Epoch 90, val loss: 1.711045503616333
Epoch 100, training loss: 2.335149049758911 = 1.6315606832504272 + 0.1 * 7.035883903503418
Epoch 100, val loss: 1.6554800271987915
Epoch 110, training loss: 2.251188278198242 = 1.552128791809082 + 0.1 * 6.990593433380127
Epoch 110, val loss: 1.5876452922821045
Epoch 120, training loss: 2.1575722694396973 = 1.4609380960464478 + 0.1 * 6.966342449188232
Epoch 120, val loss: 1.5099976062774658
Epoch 130, training loss: 2.0578274726867676 = 1.3636221885681152 + 0.1 * 6.94205379486084
Epoch 130, val loss: 1.4287755489349365
Epoch 140, training loss: 1.9542498588562012 = 1.2625716924667358 + 0.1 * 6.916781425476074
Epoch 140, val loss: 1.3459832668304443
Epoch 150, training loss: 1.8492257595062256 = 1.1597446203231812 + 0.1 * 6.894811153411865
Epoch 150, val loss: 1.2650887966156006
Epoch 160, training loss: 1.747877836227417 = 1.0602198839187622 + 0.1 * 6.876579284667969
Epoch 160, val loss: 1.1881732940673828
Epoch 170, training loss: 1.6529432535171509 = 0.9665822982788086 + 0.1 * 6.863609313964844
Epoch 170, val loss: 1.1173819303512573
Epoch 180, training loss: 1.5672845840454102 = 0.8814657926559448 + 0.1 * 6.858187675476074
Epoch 180, val loss: 1.0544930696487427
Epoch 190, training loss: 1.4907243251800537 = 0.8064097166061401 + 0.1 * 6.843145370483398
Epoch 190, val loss: 1.0001367330551147
Epoch 200, training loss: 1.4233371019363403 = 0.7401573061943054 + 0.1 * 6.8317975997924805
Epoch 200, val loss: 0.9530096054077148
Epoch 210, training loss: 1.3645062446594238 = 0.6824943423271179 + 0.1 * 6.8201189041137695
Epoch 210, val loss: 0.9134862422943115
Epoch 220, training loss: 1.312148094177246 = 0.6315601468086243 + 0.1 * 6.805880069732666
Epoch 220, val loss: 0.8804675340652466
Epoch 230, training loss: 1.26707124710083 = 0.5855157375335693 + 0.1 * 6.815555095672607
Epoch 230, val loss: 0.8530189990997314
Epoch 240, training loss: 1.2223799228668213 = 0.5436986088752747 + 0.1 * 6.786813735961914
Epoch 240, val loss: 0.8303821086883545
Epoch 250, training loss: 1.1820300817489624 = 0.5042887926101685 + 0.1 * 6.7774128913879395
Epoch 250, val loss: 0.8107029795646667
Epoch 260, training loss: 1.1443564891815186 = 0.4667568802833557 + 0.1 * 6.775996208190918
Epoch 260, val loss: 0.7934521436691284
Epoch 270, training loss: 1.1074610948562622 = 0.4311031997203827 + 0.1 * 6.763579368591309
Epoch 270, val loss: 0.7786380052566528
Epoch 280, training loss: 1.0732828378677368 = 0.39766228199005127 + 0.1 * 6.7562055587768555
Epoch 280, val loss: 0.7667264938354492
Epoch 290, training loss: 1.0419096946716309 = 0.3671507239341736 + 0.1 * 6.747590065002441
Epoch 290, val loss: 0.7581415176391602
Epoch 300, training loss: 1.0135743618011475 = 0.33996424078941345 + 0.1 * 6.7361016273498535
Epoch 300, val loss: 0.7530335783958435
Epoch 310, training loss: 0.9916471242904663 = 0.31585875153541565 + 0.1 * 6.757883071899414
Epoch 310, val loss: 0.7509047985076904
Epoch 320, training loss: 0.9669747352600098 = 0.2943059504032135 + 0.1 * 6.726687908172607
Epoch 320, val loss: 0.7511318922042847
Epoch 330, training loss: 0.9458563327789307 = 0.2744612991809845 + 0.1 * 6.7139506340026855
Epoch 330, val loss: 0.7528296113014221
Epoch 340, training loss: 0.9258547425270081 = 0.2556570768356323 + 0.1 * 6.701976776123047
Epoch 340, val loss: 0.7552931904792786
Epoch 350, training loss: 0.9067620038986206 = 0.2371997982263565 + 0.1 * 6.695621967315674
Epoch 350, val loss: 0.7580088376998901
Epoch 360, training loss: 0.887325644493103 = 0.218582883477211 + 0.1 * 6.687427520751953
Epoch 360, val loss: 0.7602631449699402
Epoch 370, training loss: 0.8671417236328125 = 0.1997555047273636 + 0.1 * 6.673861980438232
Epoch 370, val loss: 0.762123703956604
Epoch 380, training loss: 0.8483628630638123 = 0.180954709649086 + 0.1 * 6.674081802368164
Epoch 380, val loss: 0.7636468410491943
Epoch 390, training loss: 0.830132246017456 = 0.16293202340602875 + 0.1 * 6.672001838684082
Epoch 390, val loss: 0.7650229930877686
Epoch 400, training loss: 0.8127284646034241 = 0.14633417129516602 + 0.1 * 6.663942813873291
Epoch 400, val loss: 0.7668320536613464
Epoch 410, training loss: 0.7964738011360168 = 0.1314491480588913 + 0.1 * 6.6502461433410645
Epoch 410, val loss: 0.7693548202514648
Epoch 420, training loss: 0.7835696935653687 = 0.11834979057312012 + 0.1 * 6.652198791503906
Epoch 420, val loss: 0.7728869915008545
Epoch 430, training loss: 0.7704099416732788 = 0.10694137960672379 + 0.1 * 6.634685516357422
Epoch 430, val loss: 0.7773721218109131
Epoch 440, training loss: 0.7617701292037964 = 0.09697951376438141 + 0.1 * 6.647906303405762
Epoch 440, val loss: 0.7827417850494385
Epoch 450, training loss: 0.751250147819519 = 0.08827532082796097 + 0.1 * 6.629748344421387
Epoch 450, val loss: 0.7889664173126221
Epoch 460, training loss: 0.7440388202667236 = 0.08063778281211853 + 0.1 * 6.634010314941406
Epoch 460, val loss: 0.7957973480224609
Epoch 470, training loss: 0.7350773811340332 = 0.07391153275966644 + 0.1 * 6.611658096313477
Epoch 470, val loss: 0.8031871914863586
Epoch 480, training loss: 0.7291929721832275 = 0.06793662160634995 + 0.1 * 6.612563610076904
Epoch 480, val loss: 0.8108608722686768
Epoch 490, training loss: 0.7232676148414612 = 0.06262405216693878 + 0.1 * 6.606435775756836
Epoch 490, val loss: 0.8187544941902161
Epoch 500, training loss: 0.7171311378479004 = 0.057880617678165436 + 0.1 * 6.592504978179932
Epoch 500, val loss: 0.8268555998802185
Epoch 510, training loss: 0.7163209319114685 = 0.053620655089616776 + 0.1 * 6.627002716064453
Epoch 510, val loss: 0.8349431157112122
Epoch 520, training loss: 0.7084656953811646 = 0.04980143532156944 + 0.1 * 6.586642742156982
Epoch 520, val loss: 0.8430788516998291
Epoch 530, training loss: 0.704468846321106 = 0.04634809121489525 + 0.1 * 6.581207275390625
Epoch 530, val loss: 0.8511720895767212
Epoch 540, training loss: 0.7010014057159424 = 0.043210919946432114 + 0.1 * 6.57790470123291
Epoch 540, val loss: 0.8592891097068787
Epoch 550, training loss: 0.7001266479492188 = 0.04036248102784157 + 0.1 * 6.597641468048096
Epoch 550, val loss: 0.8672921061515808
Epoch 560, training loss: 0.6938289999961853 = 0.03777511045336723 + 0.1 * 6.5605387687683105
Epoch 560, val loss: 0.8752799034118652
Epoch 570, training loss: 0.6931108236312866 = 0.03540976345539093 + 0.1 * 6.577010631561279
Epoch 570, val loss: 0.8830301761627197
Epoch 580, training loss: 0.6889188289642334 = 0.03325392305850983 + 0.1 * 6.556649208068848
Epoch 580, val loss: 0.8907940983772278
Epoch 590, training loss: 0.6873157024383545 = 0.031278904527425766 + 0.1 * 6.560368061065674
Epoch 590, val loss: 0.8983755707740784
Epoch 600, training loss: 0.6848235130310059 = 0.029467033222317696 + 0.1 * 6.553564548492432
Epoch 600, val loss: 0.9057860374450684
Epoch 610, training loss: 0.6816398501396179 = 0.027804778888821602 + 0.1 * 6.538350582122803
Epoch 610, val loss: 0.9130733013153076
Epoch 620, training loss: 0.6796011328697205 = 0.026274899020791054 + 0.1 * 6.533262252807617
Epoch 620, val loss: 0.9202539324760437
Epoch 630, training loss: 0.680275559425354 = 0.02486359514296055 + 0.1 * 6.55411958694458
Epoch 630, val loss: 0.927185595035553
Epoch 640, training loss: 0.6774522662162781 = 0.023562312126159668 + 0.1 * 6.5388994216918945
Epoch 640, val loss: 0.9340982437133789
Epoch 650, training loss: 0.6763007640838623 = 0.022359460592269897 + 0.1 * 6.539412975311279
Epoch 650, val loss: 0.9407674074172974
Epoch 660, training loss: 0.6738243103027344 = 0.021246761083602905 + 0.1 * 6.52577543258667
Epoch 660, val loss: 0.9474150538444519
Epoch 670, training loss: 0.6723147034645081 = 0.020212750881910324 + 0.1 * 6.521018981933594
Epoch 670, val loss: 0.9538156986236572
Epoch 680, training loss: 0.6716891527175903 = 0.019251206889748573 + 0.1 * 6.524379253387451
Epoch 680, val loss: 0.9601311683654785
Epoch 690, training loss: 0.6696677803993225 = 0.018358139321208 + 0.1 * 6.513096332550049
Epoch 690, val loss: 0.9663428664207458
Epoch 700, training loss: 0.6706990003585815 = 0.0175254475325346 + 0.1 * 6.531735420227051
Epoch 700, val loss: 0.9723522067070007
Epoch 710, training loss: 0.6676446199417114 = 0.016750536859035492 + 0.1 * 6.508941173553467
Epoch 710, val loss: 0.978291392326355
Epoch 720, training loss: 0.6657829284667969 = 0.016025710850954056 + 0.1 * 6.49757194519043
Epoch 720, val loss: 0.9841189980506897
Epoch 730, training loss: 0.6656901240348816 = 0.015347235836088657 + 0.1 * 6.5034284591674805
Epoch 730, val loss: 0.9897738099098206
Epoch 740, training loss: 0.6647215485572815 = 0.014712216332554817 + 0.1 * 6.50009298324585
Epoch 740, val loss: 0.9953633546829224
Epoch 750, training loss: 0.6631242632865906 = 0.014117342419922352 + 0.1 * 6.4900689125061035
Epoch 750, val loss: 1.0007662773132324
Epoch 760, training loss: 0.6629294753074646 = 0.013559035025537014 + 0.1 * 6.493704319000244
Epoch 760, val loss: 1.0061677694320679
Epoch 770, training loss: 0.6621997952461243 = 0.013033752329647541 + 0.1 * 6.491660118103027
Epoch 770, val loss: 1.011306643486023
Epoch 780, training loss: 0.6603615283966064 = 0.012540556490421295 + 0.1 * 6.478209495544434
Epoch 780, val loss: 1.016481876373291
Epoch 790, training loss: 0.6609182357788086 = 0.012075142934918404 + 0.1 * 6.488430976867676
Epoch 790, val loss: 1.0214948654174805
Epoch 800, training loss: 0.6595925688743591 = 0.011636284179985523 + 0.1 * 6.479562759399414
Epoch 800, val loss: 1.0264109373092651
Epoch 810, training loss: 0.6608356833457947 = 0.011222420260310173 + 0.1 * 6.4961323738098145
Epoch 810, val loss: 1.0312082767486572
Epoch 820, training loss: 0.6583890914916992 = 0.010832234285771847 + 0.1 * 6.4755682945251465
Epoch 820, val loss: 1.0359108448028564
Epoch 830, training loss: 0.6576823592185974 = 0.010463201440870762 + 0.1 * 6.47219181060791
Epoch 830, val loss: 1.0405354499816895
Epoch 840, training loss: 0.6590576171875 = 0.010113789699971676 + 0.1 * 6.489438533782959
Epoch 840, val loss: 1.044998288154602
Epoch 850, training loss: 0.6568845510482788 = 0.009783480316400528 + 0.1 * 6.471010208129883
Epoch 850, val loss: 1.049496054649353
Epoch 860, training loss: 0.6560249924659729 = 0.009469537064433098 + 0.1 * 6.465554714202881
Epoch 860, val loss: 1.0538344383239746
Epoch 870, training loss: 0.6569464206695557 = 0.009171358309686184 + 0.1 * 6.477750301361084
Epoch 870, val loss: 1.0581023693084717
Epoch 880, training loss: 0.6558972597122192 = 0.008888131938874722 + 0.1 * 6.470091342926025
Epoch 880, val loss: 1.0622644424438477
Epoch 890, training loss: 0.6558108329772949 = 0.008619078435003757 + 0.1 * 6.471917629241943
Epoch 890, val loss: 1.0663269758224487
Epoch 900, training loss: 0.6541976928710938 = 0.008363444358110428 + 0.1 * 6.458342552185059
Epoch 900, val loss: 1.070386528968811
Epoch 910, training loss: 0.6541814804077148 = 0.008119732141494751 + 0.1 * 6.4606170654296875
Epoch 910, val loss: 1.07432222366333
Epoch 920, training loss: 0.6547918319702148 = 0.007887119427323341 + 0.1 * 6.4690470695495605
Epoch 920, val loss: 1.0781413316726685
Epoch 930, training loss: 0.6528690457344055 = 0.00766583438962698 + 0.1 * 6.452032089233398
Epoch 930, val loss: 1.0819132328033447
Epoch 940, training loss: 0.6521894931793213 = 0.007454725913703442 + 0.1 * 6.447347164154053
Epoch 940, val loss: 1.0857200622558594
Epoch 950, training loss: 0.6522647738456726 = 0.007252460811287165 + 0.1 * 6.450122833251953
Epoch 950, val loss: 1.0893733501434326
Epoch 960, training loss: 0.652734100818634 = 0.007059518713504076 + 0.1 * 6.456745624542236
Epoch 960, val loss: 1.0928142070770264
Epoch 970, training loss: 0.6524105668067932 = 0.006875395309180021 + 0.1 * 6.455351829528809
Epoch 970, val loss: 1.0964246988296509
Epoch 980, training loss: 0.651123583316803 = 0.0066992309875786304 + 0.1 * 6.444243431091309
Epoch 980, val loss: 1.099976897239685
Epoch 990, training loss: 0.6518076658248901 = 0.006529920268803835 + 0.1 * 6.45277738571167
Epoch 990, val loss: 1.1032769680023193
Epoch 1000, training loss: 0.6520698666572571 = 0.006367890164256096 + 0.1 * 6.457019805908203
Epoch 1000, val loss: 1.1066229343414307
Epoch 1010, training loss: 0.6496689915657043 = 0.0062125264666974545 + 0.1 * 6.434564590454102
Epoch 1010, val loss: 1.109910249710083
Epoch 1020, training loss: 0.6497525572776794 = 0.00606348505243659 + 0.1 * 6.436891078948975
Epoch 1020, val loss: 1.1131787300109863
Epoch 1030, training loss: 0.6508910059928894 = 0.005920257419347763 + 0.1 * 6.449707508087158
Epoch 1030, val loss: 1.1163314580917358
Epoch 1040, training loss: 0.6503580808639526 = 0.0057828305289149284 + 0.1 * 6.445752143859863
Epoch 1040, val loss: 1.1194844245910645
Epoch 1050, training loss: 0.6490529179573059 = 0.005650582257658243 + 0.1 * 6.434023380279541
Epoch 1050, val loss: 1.1226450204849243
Epoch 1060, training loss: 0.6513587236404419 = 0.005523401312530041 + 0.1 * 6.458353042602539
Epoch 1060, val loss: 1.125674843788147
Epoch 1070, training loss: 0.6497912406921387 = 0.0054011838510632515 + 0.1 * 6.443900108337402
Epoch 1070, val loss: 1.128593921661377
Epoch 1080, training loss: 0.6496604084968567 = 0.0052840253338217735 + 0.1 * 6.443763732910156
Epoch 1080, val loss: 1.1315861940383911
Epoch 1090, training loss: 0.6486333608627319 = 0.005171132739633322 + 0.1 * 6.434622287750244
Epoch 1090, val loss: 1.1345303058624268
Epoch 1100, training loss: 0.6466200947761536 = 0.005062173120677471 + 0.1 * 6.415579319000244
Epoch 1100, val loss: 1.1373244524002075
Epoch 1110, training loss: 0.6490867137908936 = 0.004956996068358421 + 0.1 * 6.441296577453613
Epoch 1110, val loss: 1.1401697397232056
Epoch 1120, training loss: 0.6493978500366211 = 0.004855579696595669 + 0.1 * 6.445422649383545
Epoch 1120, val loss: 1.1428906917572021
Epoch 1130, training loss: 0.6475408673286438 = 0.004757911432534456 + 0.1 * 6.427829265594482
Epoch 1130, val loss: 1.1456049680709839
Epoch 1140, training loss: 0.6477146148681641 = 0.004663657862693071 + 0.1 * 6.430509567260742
Epoch 1140, val loss: 1.1483135223388672
Epoch 1150, training loss: 0.6467868089675903 = 0.0045725321397185326 + 0.1 * 6.42214298248291
Epoch 1150, val loss: 1.1509711742401123
Epoch 1160, training loss: 0.6474750638008118 = 0.004484372213482857 + 0.1 * 6.429906845092773
Epoch 1160, val loss: 1.1536036729812622
Epoch 1170, training loss: 0.6482856869697571 = 0.0043991790153086185 + 0.1 * 6.4388651847839355
Epoch 1170, val loss: 1.15608811378479
Epoch 1180, training loss: 0.6451876163482666 = 0.004317105282098055 + 0.1 * 6.40870475769043
Epoch 1180, val loss: 1.1586205959320068
Epoch 1190, training loss: 0.6451224088668823 = 0.004237801302224398 + 0.1 * 6.4088454246521
Epoch 1190, val loss: 1.1611665487289429
Epoch 1200, training loss: 0.6458455920219421 = 0.004160861950367689 + 0.1 * 6.416847229003906
Epoch 1200, val loss: 1.1635992527008057
Epoch 1210, training loss: 0.6450549960136414 = 0.004086181055754423 + 0.1 * 6.4096879959106445
Epoch 1210, val loss: 1.1659691333770752
Epoch 1220, training loss: 0.6471766829490662 = 0.004013903439044952 + 0.1 * 6.4316277503967285
Epoch 1220, val loss: 1.1683601140975952
Epoch 1230, training loss: 0.644804835319519 = 0.0039440724067389965 + 0.1 * 6.408607482910156
Epoch 1230, val loss: 1.1707017421722412
Epoch 1240, training loss: 0.6454797387123108 = 0.003876326372846961 + 0.1 * 6.41603422164917
Epoch 1240, val loss: 1.1730436086654663
Epoch 1250, training loss: 0.6444814801216125 = 0.003810707712545991 + 0.1 * 6.406707763671875
Epoch 1250, val loss: 1.1753607988357544
Epoch 1260, training loss: 0.6455171704292297 = 0.0037470185197889805 + 0.1 * 6.417701244354248
Epoch 1260, val loss: 1.1775717735290527
Epoch 1270, training loss: 0.6435191631317139 = 0.003685284173116088 + 0.1 * 6.398338794708252
Epoch 1270, val loss: 1.1798039674758911
Epoch 1280, training loss: 0.6443732976913452 = 0.0036252732388675213 + 0.1 * 6.407480239868164
Epoch 1280, val loss: 1.1820231676101685
Epoch 1290, training loss: 0.6455959677696228 = 0.003566994797438383 + 0.1 * 6.420289516448975
Epoch 1290, val loss: 1.1841418743133545
Epoch 1300, training loss: 0.6436153650283813 = 0.003510719398036599 + 0.1 * 6.401046276092529
Epoch 1300, val loss: 1.186295509338379
Epoch 1310, training loss: 0.6432096362113953 = 0.0034559278283268213 + 0.1 * 6.397536754608154
Epoch 1310, val loss: 1.1884634494781494
Epoch 1320, training loss: 0.642749547958374 = 0.0034025306813418865 + 0.1 * 6.393470287322998
Epoch 1320, val loss: 1.1905162334442139
Epoch 1330, training loss: 0.6440173387527466 = 0.003350709332153201 + 0.1 * 6.406665802001953
Epoch 1330, val loss: 1.1925950050354004
Epoch 1340, training loss: 0.6426536440849304 = 0.0033001902047544718 + 0.1 * 6.393534183502197
Epoch 1340, val loss: 1.1946547031402588
Epoch 1350, training loss: 0.6448587775230408 = 0.00325101800262928 + 0.1 * 6.416077613830566
Epoch 1350, val loss: 1.1966460943222046
Epoch 1360, training loss: 0.6421405673027039 = 0.0032033976167440414 + 0.1 * 6.389371395111084
Epoch 1360, val loss: 1.1986360549926758
Epoch 1370, training loss: 0.6441586017608643 = 0.003156935330480337 + 0.1 * 6.410016059875488
Epoch 1370, val loss: 1.2006628513336182
Epoch 1380, training loss: 0.6438740491867065 = 0.0031117326579988003 + 0.1 * 6.407623291015625
Epoch 1380, val loss: 1.202510118484497
Epoch 1390, training loss: 0.641364574432373 = 0.0030677681788802147 + 0.1 * 6.382967948913574
Epoch 1390, val loss: 1.2044925689697266
Epoch 1400, training loss: 0.6414934396743774 = 0.003024902194738388 + 0.1 * 6.384685516357422
Epoch 1400, val loss: 1.2064605951309204
Epoch 1410, training loss: 0.6423978805541992 = 0.002982889534905553 + 0.1 * 6.3941497802734375
Epoch 1410, val loss: 1.2082263231277466
Epoch 1420, training loss: 0.6415997743606567 = 0.0029422559309750795 + 0.1 * 6.386575222015381
Epoch 1420, val loss: 1.210047721862793
Epoch 1430, training loss: 0.6418437957763672 = 0.002902545966207981 + 0.1 * 6.389411926269531
Epoch 1430, val loss: 1.212000846862793
Epoch 1440, training loss: 0.6403365135192871 = 0.002863901201635599 + 0.1 * 6.374726295471191
Epoch 1440, val loss: 1.2137516736984253
Epoch 1450, training loss: 0.6425337195396423 = 0.0028261507395654917 + 0.1 * 6.397075653076172
Epoch 1450, val loss: 1.2155616283416748
Epoch 1460, training loss: 0.6401650905609131 = 0.0027893322985619307 + 0.1 * 6.373757839202881
Epoch 1460, val loss: 1.2172991037368774
Epoch 1470, training loss: 0.6420533657073975 = 0.0027534645050764084 + 0.1 * 6.392999172210693
Epoch 1470, val loss: 1.219130277633667
Epoch 1480, training loss: 0.6402601599693298 = 0.0027182153426110744 + 0.1 * 6.375419616699219
Epoch 1480, val loss: 1.220785140991211
Epoch 1490, training loss: 0.6415706276893616 = 0.0026839913334697485 + 0.1 * 6.388865947723389
Epoch 1490, val loss: 1.2225135564804077
Epoch 1500, training loss: 0.6406358480453491 = 0.0026504897978156805 + 0.1 * 6.379853248596191
Epoch 1500, val loss: 1.224190592765808
Epoch 1510, training loss: 0.6410470008850098 = 0.002617915626615286 + 0.1 * 6.38429069519043
Epoch 1510, val loss: 1.2258714437484741
Epoch 1520, training loss: 0.6406986117362976 = 0.0025860012974590063 + 0.1 * 6.3811259269714355
Epoch 1520, val loss: 1.2275563478469849
Epoch 1530, training loss: 0.6395445466041565 = 0.002554766833782196 + 0.1 * 6.369897842407227
Epoch 1530, val loss: 1.2291593551635742
Epoch 1540, training loss: 0.6401585936546326 = 0.00252430303953588 + 0.1 * 6.3763427734375
Epoch 1540, val loss: 1.2308140993118286
Epoch 1550, training loss: 0.6403785943984985 = 0.0024944967590272427 + 0.1 * 6.378840446472168
Epoch 1550, val loss: 1.2324059009552002
Epoch 1560, training loss: 0.6389785408973694 = 0.002465379424393177 + 0.1 * 6.365131855010986
Epoch 1560, val loss: 1.2340070009231567
Epoch 1570, training loss: 0.6400434374809265 = 0.0024368739686906338 + 0.1 * 6.376065731048584
Epoch 1570, val loss: 1.2356274127960205
Epoch 1580, training loss: 0.6389669179916382 = 0.0024089915677905083 + 0.1 * 6.365579128265381
Epoch 1580, val loss: 1.237170696258545
Epoch 1590, training loss: 0.6397498250007629 = 0.0023816353641450405 + 0.1 * 6.373682022094727
Epoch 1590, val loss: 1.2387648820877075
Epoch 1600, training loss: 0.6385987997055054 = 0.002354895230382681 + 0.1 * 6.362438678741455
Epoch 1600, val loss: 1.2402501106262207
Epoch 1610, training loss: 0.64048832654953 = 0.002328854752704501 + 0.1 * 6.381594181060791
Epoch 1610, val loss: 1.2417986392974854
Epoch 1620, training loss: 0.6392506957054138 = 0.0023032405879348516 + 0.1 * 6.369474411010742
Epoch 1620, val loss: 1.2433332204818726
Epoch 1630, training loss: 0.638709545135498 = 0.002278213622048497 + 0.1 * 6.364313125610352
Epoch 1630, val loss: 1.2448393106460571
Epoch 1640, training loss: 0.6387811899185181 = 0.002253621816635132 + 0.1 * 6.365275859832764
Epoch 1640, val loss: 1.246357798576355
Epoch 1650, training loss: 0.6400558948516846 = 0.0022295410744845867 + 0.1 * 6.378263473510742
Epoch 1650, val loss: 1.2477728128433228
Epoch 1660, training loss: 0.6373960375785828 = 0.0022060528863221407 + 0.1 * 6.351900100708008
Epoch 1660, val loss: 1.2492154836654663
Epoch 1670, training loss: 0.6390897035598755 = 0.0021830424666404724 + 0.1 * 6.3690667152404785
Epoch 1670, val loss: 1.2507182359695435
Epoch 1680, training loss: 0.6378705501556396 = 0.0021604362409561872 + 0.1 * 6.357100963592529
Epoch 1680, val loss: 1.2521533966064453
Epoch 1690, training loss: 0.6387585401535034 = 0.00213826191611588 + 0.1 * 6.3662028312683105
Epoch 1690, val loss: 1.2535632848739624
Epoch 1700, training loss: 0.637525200843811 = 0.002116504590958357 + 0.1 * 6.354086875915527
Epoch 1700, val loss: 1.2549693584442139
Epoch 1710, training loss: 0.6382627487182617 = 0.0020952674094587564 + 0.1 * 6.3616743087768555
Epoch 1710, val loss: 1.2563743591308594
Epoch 1720, training loss: 0.6382754445075989 = 0.0020744688808918 + 0.1 * 6.362009525299072
Epoch 1720, val loss: 1.2577701807022095
Epoch 1730, training loss: 0.6396346092224121 = 0.0020540442783385515 + 0.1 * 6.375805377960205
Epoch 1730, val loss: 1.2591511011123657
Epoch 1740, training loss: 0.6377305388450623 = 0.0020340452902019024 + 0.1 * 6.356964588165283
Epoch 1740, val loss: 1.2604343891143799
Epoch 1750, training loss: 0.6371114253997803 = 0.002014417899772525 + 0.1 * 6.350970268249512
Epoch 1750, val loss: 1.2618509531021118
Epoch 1760, training loss: 0.6381658315658569 = 0.0019951192662119865 + 0.1 * 6.361706733703613
Epoch 1760, val loss: 1.263204574584961
Epoch 1770, training loss: 0.6373700499534607 = 0.00197613425552845 + 0.1 * 6.353939056396484
Epoch 1770, val loss: 1.2644822597503662
Epoch 1780, training loss: 0.6377527713775635 = 0.001957494532689452 + 0.1 * 6.35795259475708
Epoch 1780, val loss: 1.2658214569091797
Epoch 1790, training loss: 0.6379824876785278 = 0.0019392297836020589 + 0.1 * 6.360432147979736
Epoch 1790, val loss: 1.2670999765396118
Epoch 1800, training loss: 0.6370093822479248 = 0.0019212597981095314 + 0.1 * 6.350881576538086
Epoch 1800, val loss: 1.2683881521224976
Epoch 1810, training loss: 0.6369269490242004 = 0.0019036573357880116 + 0.1 * 6.3502326011657715
Epoch 1810, val loss: 1.2696480751037598
Epoch 1820, training loss: 0.6379632949829102 = 0.001886399695649743 + 0.1 * 6.3607683181762695
Epoch 1820, val loss: 1.2709629535675049
Epoch 1830, training loss: 0.6361066102981567 = 0.0018693922320380807 + 0.1 * 6.342372417449951
Epoch 1830, val loss: 1.2721734046936035
Epoch 1840, training loss: 0.6374326944351196 = 0.001852690358646214 + 0.1 * 6.355799674987793
Epoch 1840, val loss: 1.2734582424163818
Epoch 1850, training loss: 0.6362246870994568 = 0.001836286624893546 + 0.1 * 6.343883991241455
Epoch 1850, val loss: 1.2746678590774536
Epoch 1860, training loss: 0.6373401284217834 = 0.0018201349303126335 + 0.1 * 6.355199813842773
Epoch 1860, val loss: 1.2758939266204834
Epoch 1870, training loss: 0.6374822854995728 = 0.001804335042834282 + 0.1 * 6.356779098510742
Epoch 1870, val loss: 1.27707040309906
Epoch 1880, training loss: 0.6374104022979736 = 0.0017887626308947802 + 0.1 * 6.3562164306640625
Epoch 1880, val loss: 1.2782907485961914
Epoch 1890, training loss: 0.6359675526618958 = 0.0017735681030899286 + 0.1 * 6.341939449310303
Epoch 1890, val loss: 1.2794461250305176
Epoch 1900, training loss: 0.6372261643409729 = 0.0017585812602192163 + 0.1 * 6.354675769805908
Epoch 1900, val loss: 1.2806423902511597
Epoch 1910, training loss: 0.6364590525627136 = 0.001743835979141295 + 0.1 * 6.347151756286621
Epoch 1910, val loss: 1.2818241119384766
Epoch 1920, training loss: 0.6353459358215332 = 0.0017292997799813747 + 0.1 * 6.336165904998779
Epoch 1920, val loss: 1.2829738855361938
Epoch 1930, training loss: 0.6379784345626831 = 0.0017150158528238535 + 0.1 * 6.362634181976318
Epoch 1930, val loss: 1.2841335535049438
Epoch 1940, training loss: 0.6358022689819336 = 0.0017009272705763578 + 0.1 * 6.341013431549072
Epoch 1940, val loss: 1.2852407693862915
Epoch 1950, training loss: 0.6374979615211487 = 0.001687145559117198 + 0.1 * 6.358107566833496
Epoch 1950, val loss: 1.2863640785217285
Epoch 1960, training loss: 0.6360812783241272 = 0.0016735611716285348 + 0.1 * 6.344077110290527
Epoch 1960, val loss: 1.2874985933303833
Epoch 1970, training loss: 0.6366307735443115 = 0.0016602216055616736 + 0.1 * 6.349705219268799
Epoch 1970, val loss: 1.2886236906051636
Epoch 1980, training loss: 0.6354255080223083 = 0.001647095661610365 + 0.1 * 6.3377838134765625
Epoch 1980, val loss: 1.2897019386291504
Epoch 1990, training loss: 0.6360539197921753 = 0.0016341782175004482 + 0.1 * 6.3441972732543945
Epoch 1990, val loss: 1.2907875776290894
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 2.804685115814209 = 1.9450020790100098 + 0.1 * 8.596831321716309
Epoch 0, val loss: 1.9495271444320679
Epoch 10, training loss: 2.795351266860962 = 1.9356743097305298 + 0.1 * 8.596769332885742
Epoch 10, val loss: 1.9399598836898804
Epoch 20, training loss: 2.7841744422912598 = 1.9245431423187256 + 0.1 * 8.5963134765625
Epoch 20, val loss: 1.9282665252685547
Epoch 30, training loss: 2.768007278442383 = 1.9087775945663452 + 0.1 * 8.592296600341797
Epoch 30, val loss: 1.911630630493164
Epoch 40, training loss: 2.7409963607788086 = 1.8848848342895508 + 0.1 * 8.561115264892578
Epoch 40, val loss: 1.8865129947662354
Epoch 50, training loss: 2.691296339035034 = 1.8505830764770508 + 0.1 * 8.407133102416992
Epoch 50, val loss: 1.8518447875976562
Epoch 60, training loss: 2.611792802810669 = 1.810242772102356 + 0.1 * 8.01550006866455
Epoch 60, val loss: 1.8139944076538086
Epoch 70, training loss: 2.551900625228882 = 1.7725715637207031 + 0.1 * 7.793289661407471
Epoch 70, val loss: 1.7812974452972412
Epoch 80, training loss: 2.4772346019744873 = 1.7341735363006592 + 0.1 * 7.4306111335754395
Epoch 80, val loss: 1.746616244316101
Epoch 90, training loss: 2.3985776901245117 = 1.6846415996551514 + 0.1 * 7.139361381530762
Epoch 90, val loss: 1.7009025812149048
Epoch 100, training loss: 2.3212943077087402 = 1.6185247898101807 + 0.1 * 7.0276947021484375
Epoch 100, val loss: 1.6405583620071411
Epoch 110, training loss: 2.2325589656829834 = 1.5331878662109375 + 0.1 * 6.993711471557617
Epoch 110, val loss: 1.5642255544662476
Epoch 120, training loss: 2.1302192211151123 = 1.4336498975753784 + 0.1 * 6.965693950653076
Epoch 120, val loss: 1.4774047136306763
Epoch 130, training loss: 2.021247148513794 = 1.3272700309753418 + 0.1 * 6.93977165222168
Epoch 130, val loss: 1.387943148612976
Epoch 140, training loss: 1.9077306985855103 = 1.2160422801971436 + 0.1 * 6.916883945465088
Epoch 140, val loss: 1.294993281364441
Epoch 150, training loss: 1.7914397716522217 = 1.1015921831130981 + 0.1 * 6.8984761238098145
Epoch 150, val loss: 1.2014521360397339
Epoch 160, training loss: 1.6780009269714355 = 0.9887925386428833 + 0.1 * 6.892084121704102
Epoch 160, val loss: 1.1104159355163574
Epoch 170, training loss: 1.5732169151306152 = 0.8853484392166138 + 0.1 * 6.878685474395752
Epoch 170, val loss: 1.0283533334732056
Epoch 180, training loss: 1.4808114767074585 = 0.7936957478523254 + 0.1 * 6.871157169342041
Epoch 180, val loss: 0.9571902751922607
Epoch 190, training loss: 1.4003514051437378 = 0.7143325209617615 + 0.1 * 6.8601884841918945
Epoch 190, val loss: 0.8980692625045776
Epoch 200, training loss: 1.3302340507507324 = 0.6454103589057922 + 0.1 * 6.84823751449585
Epoch 200, val loss: 0.8503367304801941
Epoch 210, training loss: 1.2673147916793823 = 0.5837332606315613 + 0.1 * 6.835814952850342
Epoch 210, val loss: 0.8113579154014587
Epoch 220, training loss: 1.2105047702789307 = 0.5276265740394592 + 0.1 * 6.8287811279296875
Epoch 220, val loss: 0.7791167497634888
Epoch 230, training loss: 1.157062292098999 = 0.47585251927375793 + 0.1 * 6.812098026275635
Epoch 230, val loss: 0.7518472075462341
Epoch 240, training loss: 1.1085641384124756 = 0.428116112947464 + 0.1 * 6.804479598999023
Epoch 240, val loss: 0.729039192199707
Epoch 250, training loss: 1.0647954940795898 = 0.38523298501968384 + 0.1 * 6.795625686645508
Epoch 250, val loss: 0.71104496717453
Epoch 260, training loss: 1.0257638692855835 = 0.34671109914779663 + 0.1 * 6.790527820587158
Epoch 260, val loss: 0.697656512260437
Epoch 270, training loss: 0.9906131625175476 = 0.31216275691986084 + 0.1 * 6.784503936767578
Epoch 270, val loss: 0.6885643601417542
Epoch 280, training loss: 0.9589798450469971 = 0.281074196100235 + 0.1 * 6.779056072235107
Epoch 280, val loss: 0.6830965876579285
Epoch 290, training loss: 0.9303578734397888 = 0.2529059052467346 + 0.1 * 6.774519443511963
Epoch 290, val loss: 0.6805356740951538
Epoch 300, training loss: 0.9072877764701843 = 0.22738675773143768 + 0.1 * 6.799009799957275
Epoch 300, val loss: 0.6804112792015076
Epoch 310, training loss: 0.8814399242401123 = 0.20447450876235962 + 0.1 * 6.769653797149658
Epoch 310, val loss: 0.6820059418678284
Epoch 320, training loss: 0.8601274490356445 = 0.18367977440357208 + 0.1 * 6.764476776123047
Epoch 320, val loss: 0.6852236390113831
Epoch 330, training loss: 0.8408080339431763 = 0.16483134031295776 + 0.1 * 6.759767055511475
Epoch 330, val loss: 0.6897217035293579
Epoch 340, training loss: 0.8232250213623047 = 0.14783383905887604 + 0.1 * 6.753911972045898
Epoch 340, val loss: 0.6954826712608337
Epoch 350, training loss: 0.8079171180725098 = 0.13258150219917297 + 0.1 * 6.753356456756592
Epoch 350, val loss: 0.7022883892059326
Epoch 360, training loss: 0.794470489025116 = 0.1190728172659874 + 0.1 * 6.753976821899414
Epoch 360, val loss: 0.7099369764328003
Epoch 370, training loss: 0.7813353538513184 = 0.1071348786354065 + 0.1 * 6.74200439453125
Epoch 370, val loss: 0.7183055877685547
Epoch 380, training loss: 0.7698373198509216 = 0.09654781967401505 + 0.1 * 6.7328948974609375
Epoch 380, val loss: 0.7272733449935913
Epoch 390, training loss: 0.7627289295196533 = 0.08717615902423859 + 0.1 * 6.755527973175049
Epoch 390, val loss: 0.7368488907814026
Epoch 400, training loss: 0.7517870664596558 = 0.07898659259080887 + 0.1 * 6.7280049324035645
Epoch 400, val loss: 0.7465777397155762
Epoch 410, training loss: 0.7432290315628052 = 0.07175294309854507 + 0.1 * 6.714760780334473
Epoch 410, val loss: 0.7566050291061401
Epoch 420, training loss: 0.7381505966186523 = 0.06533671170473099 + 0.1 * 6.7281389236450195
Epoch 420, val loss: 0.766904890537262
Epoch 430, training loss: 0.729852020740509 = 0.05967266857624054 + 0.1 * 6.701793670654297
Epoch 430, val loss: 0.7772635817527771
Epoch 440, training loss: 0.725077211856842 = 0.05464373156428337 + 0.1 * 6.704334259033203
Epoch 440, val loss: 0.7876954078674316
Epoch 450, training loss: 0.7198687791824341 = 0.05018032714724541 + 0.1 * 6.696884632110596
Epoch 450, val loss: 0.7981701493263245
Epoch 460, training loss: 0.7140393257141113 = 0.04620407149195671 + 0.1 * 6.678352355957031
Epoch 460, val loss: 0.8085768818855286
Epoch 470, training loss: 0.7110435366630554 = 0.04264826700091362 + 0.1 * 6.683952808380127
Epoch 470, val loss: 0.8190123438835144
Epoch 480, training loss: 0.7065283060073853 = 0.03947456553578377 + 0.1 * 6.67053747177124
Epoch 480, val loss: 0.8292232155799866
Epoch 490, training loss: 0.7027742862701416 = 0.036628708243370056 + 0.1 * 6.6614556312561035
Epoch 490, val loss: 0.8393555283546448
Epoch 500, training loss: 0.6998869776725769 = 0.03407042846083641 + 0.1 * 6.658165454864502
Epoch 500, val loss: 0.8492740392684937
Epoch 510, training loss: 0.6960477232933044 = 0.031767189502716064 + 0.1 * 6.642805099487305
Epoch 510, val loss: 0.8589958548545837
Epoch 520, training loss: 0.693122386932373 = 0.02968555875122547 + 0.1 * 6.634367942810059
Epoch 520, val loss: 0.8685340881347656
Epoch 530, training loss: 0.6939787864685059 = 0.02780231088399887 + 0.1 * 6.661764621734619
Epoch 530, val loss: 0.8778306841850281
Epoch 540, training loss: 0.6877366900444031 = 0.0261027030646801 + 0.1 * 6.616339683532715
Epoch 540, val loss: 0.8868114948272705
Epoch 550, training loss: 0.6864432096481323 = 0.024551821872591972 + 0.1 * 6.6189141273498535
Epoch 550, val loss: 0.8956968188285828
Epoch 560, training loss: 0.6834772229194641 = 0.023133160546422005 + 0.1 * 6.603440284729004
Epoch 560, val loss: 0.9044579863548279
Epoch 570, training loss: 0.6832531690597534 = 0.02182944118976593 + 0.1 * 6.614237308502197
Epoch 570, val loss: 0.9130235314369202
Epoch 580, training loss: 0.6803877353668213 = 0.020634451881051064 + 0.1 * 6.597532272338867
Epoch 580, val loss: 0.9213666915893555
Epoch 590, training loss: 0.6800902485847473 = 0.01953701488673687 + 0.1 * 6.605532169342041
Epoch 590, val loss: 0.9294814467430115
Epoch 600, training loss: 0.6775196194648743 = 0.018530763685703278 + 0.1 * 6.589888095855713
Epoch 600, val loss: 0.937390148639679
Epoch 610, training loss: 0.6759417057037354 = 0.017600975930690765 + 0.1 * 6.583407402038574
Epoch 610, val loss: 0.9450903534889221
Epoch 620, training loss: 0.6751160621643066 = 0.01674099825322628 + 0.1 * 6.5837507247924805
Epoch 620, val loss: 0.9526393413543701
Epoch 630, training loss: 0.6728712916374207 = 0.015945829451084137 + 0.1 * 6.569254398345947
Epoch 630, val loss: 0.9599194526672363
Epoch 640, training loss: 0.6713039875030518 = 0.015207604505121708 + 0.1 * 6.5609636306762695
Epoch 640, val loss: 0.9670388698577881
Epoch 650, training loss: 0.6724250912666321 = 0.014521736651659012 + 0.1 * 6.579033851623535
Epoch 650, val loss: 0.974075973033905
Epoch 660, training loss: 0.6696773767471313 = 0.013883979059755802 + 0.1 * 6.557933807373047
Epoch 660, val loss: 0.9807693958282471
Epoch 670, training loss: 0.6710491180419922 = 0.013291009701788425 + 0.1 * 6.57758092880249
Epoch 670, val loss: 0.9873939752578735
Epoch 680, training loss: 0.6675761938095093 = 0.012738224118947983 + 0.1 * 6.548379421234131
Epoch 680, val loss: 0.9938077330589294
Epoch 690, training loss: 0.6664658188819885 = 0.012220844626426697 + 0.1 * 6.542449474334717
Epoch 690, val loss: 1.00007164478302
Epoch 700, training loss: 0.6655206680297852 = 0.011735837906599045 + 0.1 * 6.537847995758057
Epoch 700, val loss: 1.0062230825424194
Epoch 710, training loss: 0.6646213531494141 = 0.011282243765890598 + 0.1 * 6.533390522003174
Epoch 710, val loss: 1.0121114253997803
Epoch 720, training loss: 0.6646832823753357 = 0.010856032371520996 + 0.1 * 6.538272380828857
Epoch 720, val loss: 1.0179626941680908
Epoch 730, training loss: 0.6636165976524353 = 0.01045556552708149 + 0.1 * 6.531610488891602
Epoch 730, val loss: 1.0236196517944336
Epoch 740, training loss: 0.6619200706481934 = 0.01007904764264822 + 0.1 * 6.5184102058410645
Epoch 740, val loss: 1.0291244983673096
Epoch 750, training loss: 0.662887692451477 = 0.009723363444209099 + 0.1 * 6.531642913818359
Epoch 750, val loss: 1.0345579385757446
Epoch 760, training loss: 0.6602556109428406 = 0.009388338774442673 + 0.1 * 6.508672714233398
Epoch 760, val loss: 1.0397815704345703
Epoch 770, training loss: 0.6592851281166077 = 0.009072783403098583 + 0.1 * 6.5021233558654785
Epoch 770, val loss: 1.0449714660644531
Epoch 780, training loss: 0.6589770913124084 = 0.00877256877720356 + 0.1 * 6.502045154571533
Epoch 780, val loss: 1.0500229597091675
Epoch 790, training loss: 0.6584881544113159 = 0.008489296771585941 + 0.1 * 6.499988555908203
Epoch 790, val loss: 1.0549691915512085
Epoch 800, training loss: 0.6595714092254639 = 0.008221113122999668 + 0.1 * 6.513503074645996
Epoch 800, val loss: 1.0598340034484863
Epoch 810, training loss: 0.6581114530563354 = 0.007967190816998482 + 0.1 * 6.5014424324035645
Epoch 810, val loss: 1.064574956893921
Epoch 820, training loss: 0.657717764377594 = 0.007726202253252268 + 0.1 * 6.499915599822998
Epoch 820, val loss: 1.069133996963501
Epoch 830, training loss: 0.6560014486312866 = 0.007497011683881283 + 0.1 * 6.485044002532959
Epoch 830, val loss: 1.0737338066101074
Epoch 840, training loss: 0.6562259793281555 = 0.007278874982148409 + 0.1 * 6.489470958709717
Epoch 840, val loss: 1.078160285949707
Epoch 850, training loss: 0.6552205681800842 = 0.007071125321090221 + 0.1 * 6.481494426727295
Epoch 850, val loss: 1.0825812816619873
Epoch 860, training loss: 0.6539717316627502 = 0.006872430443763733 + 0.1 * 6.4709930419921875
Epoch 860, val loss: 1.0868737697601318
Epoch 870, training loss: 0.6532328128814697 = 0.006683229468762875 + 0.1 * 6.465495586395264
Epoch 870, val loss: 1.091147541999817
Epoch 880, training loss: 0.6576496958732605 = 0.006502373144030571 + 0.1 * 6.511473178863525
Epoch 880, val loss: 1.0953054428100586
Epoch 890, training loss: 0.652675211429596 = 0.006329740397632122 + 0.1 * 6.463454723358154
Epoch 890, val loss: 1.099353313446045
Epoch 900, training loss: 0.6528400778770447 = 0.006165423896163702 + 0.1 * 6.4667463302612305
Epoch 900, val loss: 1.103354573249817
Epoch 910, training loss: 0.6519767045974731 = 0.006007857620716095 + 0.1 * 6.459688186645508
Epoch 910, val loss: 1.107283592224121
Epoch 920, training loss: 0.6511027812957764 = 0.005857233423739672 + 0.1 * 6.452455520629883
Epoch 920, val loss: 1.111112117767334
Epoch 930, training loss: 0.6533557176589966 = 0.005712768994271755 + 0.1 * 6.476429462432861
Epoch 930, val loss: 1.1149084568023682
Epoch 940, training loss: 0.6508641242980957 = 0.005574098788201809 + 0.1 * 6.452899932861328
Epoch 940, val loss: 1.1186379194259644
Epoch 950, training loss: 0.651515007019043 = 0.005441216751933098 + 0.1 * 6.460738182067871
Epoch 950, val loss: 1.1223199367523193
Epoch 960, training loss: 0.6504773497581482 = 0.005313548259437084 + 0.1 * 6.4516377449035645
Epoch 960, val loss: 1.1259287595748901
Epoch 970, training loss: 0.6497306227684021 = 0.005190753377974033 + 0.1 * 6.445398807525635
Epoch 970, val loss: 1.12947416305542
Epoch 980, training loss: 0.649639368057251 = 0.005073005333542824 + 0.1 * 6.4456634521484375
Epoch 980, val loss: 1.1329443454742432
Epoch 990, training loss: 0.6504018902778625 = 0.004959492944180965 + 0.1 * 6.454423904418945
Epoch 990, val loss: 1.136400580406189
Epoch 1000, training loss: 0.6501148343086243 = 0.004850275814533234 + 0.1 * 6.452645301818848
Epoch 1000, val loss: 1.1397604942321777
Epoch 1010, training loss: 0.6487008929252625 = 0.004745490849018097 + 0.1 * 6.439553737640381
Epoch 1010, val loss: 1.143066644668579
Epoch 1020, training loss: 0.6484680771827698 = 0.004644585773348808 + 0.1 * 6.438234806060791
Epoch 1020, val loss: 1.1463148593902588
Epoch 1030, training loss: 0.6494939923286438 = 0.004547321703284979 + 0.1 * 6.449466705322266
Epoch 1030, val loss: 1.1495153903961182
Epoch 1040, training loss: 0.6483927965164185 = 0.004453371744602919 + 0.1 * 6.439393997192383
Epoch 1040, val loss: 1.1526716947555542
Epoch 1050, training loss: 0.647064745426178 = 0.004362932406365871 + 0.1 * 6.427018165588379
Epoch 1050, val loss: 1.1557716131210327
Epoch 1060, training loss: 0.649692714214325 = 0.004275490529835224 + 0.1 * 6.454172134399414
Epoch 1060, val loss: 1.1588780879974365
Epoch 1070, training loss: 0.6469589471817017 = 0.004190851002931595 + 0.1 * 6.427680969238281
Epoch 1070, val loss: 1.1618826389312744
Epoch 1080, training loss: 0.6470249891281128 = 0.004109691362828016 + 0.1 * 6.429152965545654
Epoch 1080, val loss: 1.164820671081543
Epoch 1090, training loss: 0.6464347839355469 = 0.00403106864541769 + 0.1 * 6.424036979675293
Epoch 1090, val loss: 1.1677607297897339
Epoch 1100, training loss: 0.6463893055915833 = 0.003955348394811153 + 0.1 * 6.424339294433594
Epoch 1100, val loss: 1.1706002950668335
Epoch 1110, training loss: 0.6462043523788452 = 0.0038820032496005297 + 0.1 * 6.423223495483398
Epoch 1110, val loss: 1.17341947555542
Epoch 1120, training loss: 0.646630048751831 = 0.00381071912124753 + 0.1 * 6.42819356918335
Epoch 1120, val loss: 1.1762479543685913
Epoch 1130, training loss: 0.646511971950531 = 0.0037416305858641863 + 0.1 * 6.427703380584717
Epoch 1130, val loss: 1.1790460348129272
Epoch 1140, training loss: 0.6456649303436279 = 0.003674964653328061 + 0.1 * 6.4198994636535645
Epoch 1140, val loss: 1.1817691326141357
Epoch 1150, training loss: 0.6463414430618286 = 0.003610426327213645 + 0.1 * 6.427309989929199
Epoch 1150, val loss: 1.1844505071640015
Epoch 1160, training loss: 0.6447439789772034 = 0.003547810949385166 + 0.1 * 6.411961555480957
Epoch 1160, val loss: 1.187090277671814
Epoch 1170, training loss: 0.6466253995895386 = 0.0034870419185608625 + 0.1 * 6.43138313293457
Epoch 1170, val loss: 1.189695954322815
Epoch 1180, training loss: 0.6453831791877747 = 0.003428220748901367 + 0.1 * 6.419549465179443
Epoch 1180, val loss: 1.1923002004623413
Epoch 1190, training loss: 0.6470340490341187 = 0.0033711781725287437 + 0.1 * 6.436628818511963
Epoch 1190, val loss: 1.1948026418685913
Epoch 1200, training loss: 0.6445506811141968 = 0.003315950045362115 + 0.1 * 6.412347316741943
Epoch 1200, val loss: 1.1972708702087402
Epoch 1210, training loss: 0.6454042196273804 = 0.003262174781411886 + 0.1 * 6.421420574188232
Epoch 1210, val loss: 1.199723243713379
Epoch 1220, training loss: 0.6448469161987305 = 0.0032098894007503986 + 0.1 * 6.416370391845703
Epoch 1220, val loss: 1.2022051811218262
Epoch 1230, training loss: 0.6443936824798584 = 0.0031591258011758327 + 0.1 * 6.412345886230469
Epoch 1230, val loss: 1.2046087980270386
Epoch 1240, training loss: 0.6438332796096802 = 0.003109979210421443 + 0.1 * 6.407232761383057
Epoch 1240, val loss: 1.207005262374878
Epoch 1250, training loss: 0.6436468362808228 = 0.0030620719771832228 + 0.1 * 6.405847549438477
Epoch 1250, val loss: 1.2093473672866821
Epoch 1260, training loss: 0.6445233225822449 = 0.003015455324202776 + 0.1 * 6.415078639984131
Epoch 1260, val loss: 1.2116665840148926
Epoch 1270, training loss: 0.6445096135139465 = 0.002970245433971286 + 0.1 * 6.415393829345703
Epoch 1270, val loss: 1.2139686346054077
Epoch 1280, training loss: 0.6438619494438171 = 0.0029261219315230846 + 0.1 * 6.409358501434326
Epoch 1280, val loss: 1.216222882270813
Epoch 1290, training loss: 0.6435238122940063 = 0.0028833418618887663 + 0.1 * 6.406404972076416
Epoch 1290, val loss: 1.2184557914733887
Epoch 1300, training loss: 0.642889678478241 = 0.0028416942805051804 + 0.1 * 6.400479316711426
Epoch 1300, val loss: 1.2206782102584839
Epoch 1310, training loss: 0.6424354910850525 = 0.0028009451925754547 + 0.1 * 6.396345615386963
Epoch 1310, val loss: 1.2228773832321167
Epoch 1320, training loss: 0.6424797177314758 = 0.0027614692226052284 + 0.1 * 6.397182464599609
Epoch 1320, val loss: 1.2250547409057617
Epoch 1330, training loss: 0.6427909135818481 = 0.0027229860424995422 + 0.1 * 6.400679111480713
Epoch 1330, val loss: 1.2271941900253296
Epoch 1340, training loss: 0.6423230171203613 = 0.0026855028700083494 + 0.1 * 6.3963751792907715
Epoch 1340, val loss: 1.229306936264038
Epoch 1350, training loss: 0.6425967216491699 = 0.002648924244567752 + 0.1 * 6.399477481842041
Epoch 1350, val loss: 1.2313958406448364
Epoch 1360, training loss: 0.6425071358680725 = 0.002613169839605689 + 0.1 * 6.39893913269043
Epoch 1360, val loss: 1.2334656715393066
Epoch 1370, training loss: 0.6410932540893555 = 0.00257850787602365 + 0.1 * 6.385147571563721
Epoch 1370, val loss: 1.2355016469955444
Epoch 1380, training loss: 0.6427444219589233 = 0.0025445439387112856 + 0.1 * 6.401998996734619
Epoch 1380, val loss: 1.2375198602676392
Epoch 1390, training loss: 0.6411320567131042 = 0.0025114852469414473 + 0.1 * 6.386205673217773
Epoch 1390, val loss: 1.239531397819519
Epoch 1400, training loss: 0.6432891488075256 = 0.0024791781324893236 + 0.1 * 6.408099174499512
Epoch 1400, val loss: 1.2415168285369873
Epoch 1410, training loss: 0.6420333981513977 = 0.002447734819725156 + 0.1 * 6.395856857299805
Epoch 1410, val loss: 1.2435004711151123
Epoch 1420, training loss: 0.6414830088615417 = 0.002417073119431734 + 0.1 * 6.390659332275391
Epoch 1420, val loss: 1.2454421520233154
Epoch 1430, training loss: 0.6414627432823181 = 0.002387098968029022 + 0.1 * 6.390756130218506
Epoch 1430, val loss: 1.2473461627960205
Epoch 1440, training loss: 0.6410271525382996 = 0.0023577173706144094 + 0.1 * 6.386693954467773
Epoch 1440, val loss: 1.2492526769638062
Epoch 1450, training loss: 0.6409584283828735 = 0.002329102950170636 + 0.1 * 6.386292934417725
Epoch 1450, val loss: 1.2511643171310425
Epoch 1460, training loss: 0.6405585408210754 = 0.0023010927252471447 + 0.1 * 6.382574558258057
Epoch 1460, val loss: 1.2530536651611328
Epoch 1470, training loss: 0.6397318840026855 = 0.0022736273240298033 + 0.1 * 6.374582767486572
Epoch 1470, val loss: 1.2549045085906982
Epoch 1480, training loss: 0.6412893533706665 = 0.002246902324259281 + 0.1 * 6.3904242515563965
Epoch 1480, val loss: 1.2567224502563477
Epoch 1490, training loss: 0.6398948431015015 = 0.0022208578884601593 + 0.1 * 6.376739501953125
Epoch 1490, val loss: 1.2585209608078003
Epoch 1500, training loss: 0.6408469080924988 = 0.0021953319665044546 + 0.1 * 6.386515140533447
Epoch 1500, val loss: 1.2603102922439575
Epoch 1510, training loss: 0.6396350860595703 = 0.0021702887024730444 + 0.1 * 6.374648094177246
Epoch 1510, val loss: 1.2620885372161865
Epoch 1520, training loss: 0.6411572098731995 = 0.002145781647413969 + 0.1 * 6.3901143074035645
Epoch 1520, val loss: 1.2638517618179321
Epoch 1530, training loss: 0.6395851373672485 = 0.0021218026522547007 + 0.1 * 6.374633312225342
Epoch 1530, val loss: 1.2656086683273315
Epoch 1540, training loss: 0.6390724182128906 = 0.0020983691792935133 + 0.1 * 6.3697404861450195
Epoch 1540, val loss: 1.2673192024230957
Epoch 1550, training loss: 0.6419863104820251 = 0.0020753785502165556 + 0.1 * 6.399109363555908
Epoch 1550, val loss: 1.2690492868423462
Epoch 1560, training loss: 0.6390848755836487 = 0.0020528349559754133 + 0.1 * 6.3703203201293945
Epoch 1560, val loss: 1.2707277536392212
Epoch 1570, training loss: 0.6404415965080261 = 0.002030875999480486 + 0.1 * 6.3841071128845215
Epoch 1570, val loss: 1.2723983526229858
Epoch 1580, training loss: 0.6397843956947327 = 0.0020093827042728662 + 0.1 * 6.377749919891357
Epoch 1580, val loss: 1.27406907081604
Epoch 1590, training loss: 0.6402111053466797 = 0.0019884370267391205 + 0.1 * 6.382226943969727
Epoch 1590, val loss: 1.2756460905075073
Epoch 1600, training loss: 0.6382577419281006 = 0.001967743970453739 + 0.1 * 6.3628997802734375
Epoch 1600, val loss: 1.2772867679595947
Epoch 1610, training loss: 0.6400932669639587 = 0.0019475137814879417 + 0.1 * 6.381457328796387
Epoch 1610, val loss: 1.2789210081100464
Epoch 1620, training loss: 0.6380645036697388 = 0.0019275621743872762 + 0.1 * 6.361369609832764
Epoch 1620, val loss: 1.280547022819519
Epoch 1630, training loss: 0.6385526657104492 = 0.0019081749487668276 + 0.1 * 6.3664445877075195
Epoch 1630, val loss: 1.282109022140503
Epoch 1640, training loss: 0.6387054920196533 = 0.0018890331266447902 + 0.1 * 6.3681640625
Epoch 1640, val loss: 1.2836804389953613
Epoch 1650, training loss: 0.6385154128074646 = 0.0018701738445088267 + 0.1 * 6.366452217102051
Epoch 1650, val loss: 1.285260558128357
Epoch 1660, training loss: 0.6389656066894531 = 0.0018518517026677728 + 0.1 * 6.371137619018555
Epoch 1660, val loss: 1.2868263721466064
Epoch 1670, training loss: 0.6392586827278137 = 0.0018337744986638427 + 0.1 * 6.374248504638672
Epoch 1670, val loss: 1.2883191108703613
Epoch 1680, training loss: 0.6381622552871704 = 0.001815980882383883 + 0.1 * 6.363462924957275
Epoch 1680, val loss: 1.289889931678772
Epoch 1690, training loss: 0.638444185256958 = 0.0017986972816288471 + 0.1 * 6.366454601287842
Epoch 1690, val loss: 1.291372299194336
Epoch 1700, training loss: 0.6379036903381348 = 0.0017816722393035889 + 0.1 * 6.361220359802246
Epoch 1700, val loss: 1.292864441871643
Epoch 1710, training loss: 0.6376760005950928 = 0.0017649315996095538 + 0.1 * 6.359110355377197
Epoch 1710, val loss: 1.2943220138549805
Epoch 1720, training loss: 0.6378511786460876 = 0.0017484149429947138 + 0.1 * 6.361027240753174
Epoch 1720, val loss: 1.2958240509033203
Epoch 1730, training loss: 0.6382912397384644 = 0.0017322693020105362 + 0.1 * 6.365589141845703
Epoch 1730, val loss: 1.2972646951675415
Epoch 1740, training loss: 0.6384599208831787 = 0.0017164088785648346 + 0.1 * 6.367434978485107
Epoch 1740, val loss: 1.2987207174301147
Epoch 1750, training loss: 0.637747049331665 = 0.0017008864087983966 + 0.1 * 6.360461711883545
Epoch 1750, val loss: 1.30010187625885
Epoch 1760, training loss: 0.6373569369316101 = 0.0016856194706633687 + 0.1 * 6.35671329498291
Epoch 1760, val loss: 1.3015469312667847
Epoch 1770, training loss: 0.6376897692680359 = 0.0016706029418855906 + 0.1 * 6.360191345214844
Epoch 1770, val loss: 1.3029316663742065
Epoch 1780, training loss: 0.6381415724754333 = 0.0016557928174734116 + 0.1 * 6.3648576736450195
Epoch 1780, val loss: 1.3043407201766968
Epoch 1790, training loss: 0.636993408203125 = 0.0016412105178460479 + 0.1 * 6.353521823883057
Epoch 1790, val loss: 1.3057388067245483
Epoch 1800, training loss: 0.6394650936126709 = 0.0016269558109343052 + 0.1 * 6.378381252288818
Epoch 1800, val loss: 1.3071136474609375
Epoch 1810, training loss: 0.6373083591461182 = 0.0016129838768392801 + 0.1 * 6.3569536209106445
Epoch 1810, val loss: 1.3084542751312256
Epoch 1820, training loss: 0.6374128460884094 = 0.0015992234693840146 + 0.1 * 6.35813570022583
Epoch 1820, val loss: 1.3097913265228271
Epoch 1830, training loss: 0.6371827721595764 = 0.001585592282935977 + 0.1 * 6.355971813201904
Epoch 1830, val loss: 1.311143159866333
Epoch 1840, training loss: 0.6372402310371399 = 0.0015722174430266023 + 0.1 * 6.356680393218994
Epoch 1840, val loss: 1.3124666213989258
Epoch 1850, training loss: 0.6372325420379639 = 0.0015591176925227046 + 0.1 * 6.356733798980713
Epoch 1850, val loss: 1.3138030767440796
Epoch 1860, training loss: 0.6378079056739807 = 0.0015462626470252872 + 0.1 * 6.362616062164307
Epoch 1860, val loss: 1.315106987953186
Epoch 1870, training loss: 0.6365878582000732 = 0.0015335527714341879 + 0.1 * 6.3505425453186035
Epoch 1870, val loss: 1.3163710832595825
Epoch 1880, training loss: 0.6362008452415466 = 0.0015210655983537436 + 0.1 * 6.346797943115234
Epoch 1880, val loss: 1.3176454305648804
Epoch 1890, training loss: 0.6370987892150879 = 0.0015087634092196822 + 0.1 * 6.355899810791016
Epoch 1890, val loss: 1.3189657926559448
Epoch 1900, training loss: 0.6372082233428955 = 0.001496591023169458 + 0.1 * 6.357115745544434
Epoch 1900, val loss: 1.3202413320541382
Epoch 1910, training loss: 0.6356226205825806 = 0.0014846432022750378 + 0.1 * 6.341379642486572
Epoch 1910, val loss: 1.3215577602386475
Epoch 1920, training loss: 0.6361733078956604 = 0.0014729584800079465 + 0.1 * 6.34700345993042
Epoch 1920, val loss: 1.322767734527588
Epoch 1930, training loss: 0.6380164623260498 = 0.0014614202082157135 + 0.1 * 6.3655500411987305
Epoch 1930, val loss: 1.3239994049072266
Epoch 1940, training loss: 0.63570636510849 = 0.0014500656398013234 + 0.1 * 6.342562675476074
Epoch 1940, val loss: 1.3252553939819336
Epoch 1950, training loss: 0.6359069347381592 = 0.001438923180103302 + 0.1 * 6.344680309295654
Epoch 1950, val loss: 1.3264864683151245
Epoch 1960, training loss: 0.637119710445404 = 0.0014279495226219296 + 0.1 * 6.356917381286621
Epoch 1960, val loss: 1.3276770114898682
Epoch 1970, training loss: 0.636114776134491 = 0.0014170226640999317 + 0.1 * 6.346977233886719
Epoch 1970, val loss: 1.3289302587509155
Epoch 1980, training loss: 0.6364732980728149 = 0.0014063770649954677 + 0.1 * 6.3506693840026855
Epoch 1980, val loss: 1.3301115036010742
Epoch 1990, training loss: 0.6359407305717468 = 0.001395834144204855 + 0.1 * 6.345448970794678
Epoch 1990, val loss: 1.3313003778457642
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8408012651555088
The final CL Acc:0.79753, 0.02270, The final GNN Acc:0.83781, 0.00237
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10558])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7923507690429688 = 1.9326657056808472 + 0.1 * 8.596850395202637
Epoch 0, val loss: 1.9314312934875488
Epoch 10, training loss: 2.783114433288574 = 1.923436164855957 + 0.1 * 8.596781730651855
Epoch 10, val loss: 1.9215947389602661
Epoch 20, training loss: 2.772052526473999 = 1.9124103784561157 + 0.1 * 8.596421241760254
Epoch 20, val loss: 1.9096064567565918
Epoch 30, training loss: 2.756795883178711 = 1.897444725036621 + 0.1 * 8.593511581420898
Epoch 30, val loss: 1.8933225870132446
Epoch 40, training loss: 2.733034133911133 = 1.8760133981704712 + 0.1 * 8.570207595825195
Epoch 40, val loss: 1.8704280853271484
Epoch 50, training loss: 2.6917500495910645 = 1.8469011783599854 + 0.1 * 8.44848918914795
Epoch 50, val loss: 1.8412535190582275
Epoch 60, training loss: 2.625962018966675 = 1.8146570920944214 + 0.1 * 8.113048553466797
Epoch 60, val loss: 1.8127822875976562
Epoch 70, training loss: 2.5696144104003906 = 1.7839226722717285 + 0.1 * 7.8569183349609375
Epoch 70, val loss: 1.7886501550674438
Epoch 80, training loss: 2.4938712120056152 = 1.752082347869873 + 0.1 * 7.4178876876831055
Epoch 80, val loss: 1.7630068063735962
Epoch 90, training loss: 2.4269962310791016 = 1.7132095098495483 + 0.1 * 7.137868404388428
Epoch 90, val loss: 1.7294464111328125
Epoch 100, training loss: 2.366929292678833 = 1.6615512371063232 + 0.1 * 7.053780555725098
Epoch 100, val loss: 1.6844089031219482
Epoch 110, training loss: 2.2934494018554688 = 1.5928025245666504 + 0.1 * 7.006469249725342
Epoch 110, val loss: 1.6261776685714722
Epoch 120, training loss: 2.2061142921447754 = 1.5082218647003174 + 0.1 * 6.9789228439331055
Epoch 120, val loss: 1.557002305984497
Epoch 130, training loss: 2.1087160110473633 = 1.4125854969024658 + 0.1 * 6.961304664611816
Epoch 130, val loss: 1.4805903434753418
Epoch 140, training loss: 2.0041799545288086 = 1.3090022802352905 + 0.1 * 6.951777458190918
Epoch 140, val loss: 1.3997106552124023
Epoch 150, training loss: 1.8931574821472168 = 1.1985615491867065 + 0.1 * 6.945959091186523
Epoch 150, val loss: 1.315743088722229
Epoch 160, training loss: 1.7784521579742432 = 1.0844142436981201 + 0.1 * 6.940379619598389
Epoch 160, val loss: 1.2299531698226929
Epoch 170, training loss: 1.6671557426452637 = 0.9735812544822693 + 0.1 * 6.935744285583496
Epoch 170, val loss: 1.1481292247772217
Epoch 180, training loss: 1.5640158653259277 = 0.8715258836746216 + 0.1 * 6.924900531768799
Epoch 180, val loss: 1.0742560625076294
Epoch 190, training loss: 1.4719998836517334 = 0.7807672023773193 + 0.1 * 6.912326335906982
Epoch 190, val loss: 1.010668396949768
Epoch 200, training loss: 1.39335036277771 = 0.7019725441932678 + 0.1 * 6.9137773513793945
Epoch 200, val loss: 0.9581881165504456
Epoch 210, training loss: 1.323635220527649 = 0.6351024508476257 + 0.1 * 6.885327339172363
Epoch 210, val loss: 0.91761714220047
Epoch 220, training loss: 1.2631956338882446 = 0.5763500928878784 + 0.1 * 6.868455410003662
Epoch 220, val loss: 0.8851458430290222
Epoch 230, training loss: 1.2089259624481201 = 0.5232849717140198 + 0.1 * 6.856409072875977
Epoch 230, val loss: 0.8591302037239075
Epoch 240, training loss: 1.1596745252609253 = 0.474575012922287 + 0.1 * 6.85099458694458
Epoch 240, val loss: 0.8378061056137085
Epoch 250, training loss: 1.1138842105865479 = 0.42994225025177 + 0.1 * 6.839418888092041
Epoch 250, val loss: 0.8204679489135742
Epoch 260, training loss: 1.0719393491744995 = 0.3887121379375458 + 0.1 * 6.832272052764893
Epoch 260, val loss: 0.8064031600952148
Epoch 270, training loss: 1.0329704284667969 = 0.35049939155578613 + 0.1 * 6.824710369110107
Epoch 270, val loss: 0.7956733107566833
Epoch 280, training loss: 0.9973793029785156 = 0.3152734637260437 + 0.1 * 6.82105827331543
Epoch 280, val loss: 0.7880838513374329
Epoch 290, training loss: 0.964316725730896 = 0.2831355631351471 + 0.1 * 6.811811923980713
Epoch 290, val loss: 0.7834768891334534
Epoch 300, training loss: 0.9338523149490356 = 0.2535015940666199 + 0.1 * 6.803507328033447
Epoch 300, val loss: 0.7813377976417542
Epoch 310, training loss: 0.9064153432846069 = 0.22615134716033936 + 0.1 * 6.802639961242676
Epoch 310, val loss: 0.7814049124717712
Epoch 320, training loss: 0.8798250555992126 = 0.20118838548660278 + 0.1 * 6.7863664627075195
Epoch 320, val loss: 0.7835010886192322
Epoch 330, training loss: 0.8564793467521667 = 0.17845825850963593 + 0.1 * 6.780210494995117
Epoch 330, val loss: 0.7872039675712585
Epoch 340, training loss: 0.8353310823440552 = 0.15807683765888214 + 0.1 * 6.772542476654053
Epoch 340, val loss: 0.7924275398254395
Epoch 350, training loss: 0.8167692422866821 = 0.14003770053386688 + 0.1 * 6.76731538772583
Epoch 350, val loss: 0.7990501523017883
Epoch 360, training loss: 0.8005731701850891 = 0.12419303506612778 + 0.1 * 6.763801574707031
Epoch 360, val loss: 0.8067886233329773
Epoch 370, training loss: 0.7850856184959412 = 0.11039374768733978 + 0.1 * 6.746918678283691
Epoch 370, val loss: 0.8154603838920593
Epoch 380, training loss: 0.7734697461128235 = 0.09837627410888672 + 0.1 * 6.750934600830078
Epoch 380, val loss: 0.824910044670105
Epoch 390, training loss: 0.7619602084159851 = 0.087958924472332 + 0.1 * 6.7400126457214355
Epoch 390, val loss: 0.8349176049232483
Epoch 400, training loss: 0.752267599105835 = 0.07887179404497147 + 0.1 * 6.733957767486572
Epoch 400, val loss: 0.8452795743942261
Epoch 410, training loss: 0.7451003789901733 = 0.07094624638557434 + 0.1 * 6.741540908813477
Epoch 410, val loss: 0.8559604287147522
Epoch 420, training loss: 0.736213207244873 = 0.06406135112047195 + 0.1 * 6.721518516540527
Epoch 420, val loss: 0.8667928576469421
Epoch 430, training loss: 0.7303392291069031 = 0.05803767964243889 + 0.1 * 6.723015308380127
Epoch 430, val loss: 0.877640426158905
Epoch 440, training loss: 0.7243943214416504 = 0.052779268473386765 + 0.1 * 6.716150283813477
Epoch 440, val loss: 0.888496994972229
Epoch 450, training loss: 0.7184838056564331 = 0.04816262423992157 + 0.1 * 6.703211307525635
Epoch 450, val loss: 0.8991615772247314
Epoch 460, training loss: 0.7150084972381592 = 0.04409578815102577 + 0.1 * 6.709126949310303
Epoch 460, val loss: 0.9097470045089722
Epoch 470, training loss: 0.7100199460983276 = 0.04051667079329491 + 0.1 * 6.695032596588135
Epoch 470, val loss: 0.9201499819755554
Epoch 480, training loss: 0.7073572874069214 = 0.037345774471759796 + 0.1 * 6.700115203857422
Epoch 480, val loss: 0.9302018284797668
Epoch 490, training loss: 0.702811598777771 = 0.034532010555267334 + 0.1 * 6.682796001434326
Epoch 490, val loss: 0.9401724934577942
Epoch 500, training loss: 0.700387716293335 = 0.03202040120959282 + 0.1 * 6.683672904968262
Epoch 500, val loss: 0.9498170018196106
Epoch 510, training loss: 0.697270929813385 = 0.029780792072415352 + 0.1 * 6.674901008605957
Epoch 510, val loss: 0.9592249989509583
Epoch 520, training loss: 0.6963909864425659 = 0.027770813554525375 + 0.1 * 6.686201572418213
Epoch 520, val loss: 0.9684120416641235
Epoch 530, training loss: 0.6927711963653564 = 0.025965822860598564 + 0.1 * 6.66805362701416
Epoch 530, val loss: 0.9773467779159546
Epoch 540, training loss: 0.6906574964523315 = 0.02433847449719906 + 0.1 * 6.663189888000488
Epoch 540, val loss: 0.9859187602996826
Epoch 550, training loss: 0.6886529922485352 = 0.022863879799842834 + 0.1 * 6.657891273498535
Epoch 550, val loss: 0.9944412708282471
Epoch 560, training loss: 0.6868816018104553 = 0.021524257957935333 + 0.1 * 6.653573513031006
Epoch 560, val loss: 1.0025720596313477
Epoch 570, training loss: 0.685025155544281 = 0.020305845886468887 + 0.1 * 6.647192478179932
Epoch 570, val loss: 1.0106022357940674
Epoch 580, training loss: 0.6832928657531738 = 0.01919473148882389 + 0.1 * 6.640981197357178
Epoch 580, val loss: 1.0182644128799438
Epoch 590, training loss: 0.6815066933631897 = 0.018180029466748238 + 0.1 * 6.633266925811768
Epoch 590, val loss: 1.0257930755615234
Epoch 600, training loss: 0.6814225316047668 = 0.017245572060346603 + 0.1 * 6.6417694091796875
Epoch 600, val loss: 1.0330140590667725
Epoch 610, training loss: 0.6785373687744141 = 0.016386890783905983 + 0.1 * 6.621504783630371
Epoch 610, val loss: 1.0401325225830078
Epoch 620, training loss: 0.679538369178772 = 0.015597564168274403 + 0.1 * 6.639407634735107
Epoch 620, val loss: 1.0470305681228638
Epoch 630, training loss: 0.6761833429336548 = 0.014868061058223248 + 0.1 * 6.613152503967285
Epoch 630, val loss: 1.0537521839141846
Epoch 640, training loss: 0.6774094700813293 = 0.014191528782248497 + 0.1 * 6.632179260253906
Epoch 640, val loss: 1.0602965354919434
Epoch 650, training loss: 0.6737555861473083 = 0.01356454472988844 + 0.1 * 6.601910591125488
Epoch 650, val loss: 1.0666340589523315
Epoch 660, training loss: 0.6744176745414734 = 0.012980340048670769 + 0.1 * 6.614373207092285
Epoch 660, val loss: 1.0728826522827148
Epoch 670, training loss: 0.6713887453079224 = 0.012436365708708763 + 0.1 * 6.5895233154296875
Epoch 670, val loss: 1.0789381265640259
Epoch 680, training loss: 0.6718425154685974 = 0.011929052881896496 + 0.1 * 6.59913444519043
Epoch 680, val loss: 1.084934949874878
Epoch 690, training loss: 0.6710266470909119 = 0.011453895829617977 + 0.1 * 6.595727443695068
Epoch 690, val loss: 1.0907138586044312
Epoch 700, training loss: 0.6693651080131531 = 0.011009271256625652 + 0.1 * 6.583558082580566
Epoch 700, val loss: 1.0963178873062134
Epoch 710, training loss: 0.6715888381004333 = 0.01059223897755146 + 0.1 * 6.609965801239014
Epoch 710, val loss: 1.1019197702407837
Epoch 720, training loss: 0.6677161455154419 = 0.010202349163591862 + 0.1 * 6.575137615203857
Epoch 720, val loss: 1.10723078250885
Epoch 730, training loss: 0.6660277247428894 = 0.009836900979280472 + 0.1 * 6.56190824508667
Epoch 730, val loss: 1.112558126449585
Epoch 740, training loss: 0.6689510345458984 = 0.0094903027638793 + 0.1 * 6.594606876373291
Epoch 740, val loss: 1.1177034378051758
Epoch 750, training loss: 0.6652435660362244 = 0.009165736846625805 + 0.1 * 6.56077766418457
Epoch 750, val loss: 1.1226897239685059
Epoch 760, training loss: 0.667488157749176 = 0.008859019726514816 + 0.1 * 6.5862908363342285
Epoch 760, val loss: 1.1277124881744385
Epoch 770, training loss: 0.6641414165496826 = 0.00856880471110344 + 0.1 * 6.555726051330566
Epoch 770, val loss: 1.1324126720428467
Epoch 780, training loss: 0.6640247106552124 = 0.008296402171254158 + 0.1 * 6.557282447814941
Epoch 780, val loss: 1.1371405124664307
Epoch 790, training loss: 0.6623290777206421 = 0.008036816492676735 + 0.1 * 6.5429229736328125
Epoch 790, val loss: 1.1417006254196167
Epoch 800, training loss: 0.6642889976501465 = 0.007791030686348677 + 0.1 * 6.564979553222656
Epoch 800, val loss: 1.1462196111679077
Epoch 810, training loss: 0.6608671545982361 = 0.007558112032711506 + 0.1 * 6.533090591430664
Epoch 810, val loss: 1.150554895401001
Epoch 820, training loss: 0.6617296934127808 = 0.007338044233620167 + 0.1 * 6.54391622543335
Epoch 820, val loss: 1.1549679040908813
Epoch 830, training loss: 0.6601570248603821 = 0.0071273911744356155 + 0.1 * 6.530296325683594
Epoch 830, val loss: 1.1591328382492065
Epoch 840, training loss: 0.6595274209976196 = 0.006927989423274994 + 0.1 * 6.525994300842285
Epoch 840, val loss: 1.1632567644119263
Epoch 850, training loss: 0.6583837866783142 = 0.006737348157912493 + 0.1 * 6.5164642333984375
Epoch 850, val loss: 1.1673524379730225
Epoch 860, training loss: 0.6605048775672913 = 0.006555819883942604 + 0.1 * 6.539490699768066
Epoch 860, val loss: 1.1712331771850586
Epoch 870, training loss: 0.6592808365821838 = 0.0063832420855760574 + 0.1 * 6.528975963592529
Epoch 870, val loss: 1.175108551979065
Epoch 880, training loss: 0.6568154692649841 = 0.006218113470822573 + 0.1 * 6.5059733390808105
Epoch 880, val loss: 1.1789569854736328
Epoch 890, training loss: 0.6571231484413147 = 0.006059993989765644 + 0.1 * 6.510631561279297
Epoch 890, val loss: 1.1826775074005127
Epoch 900, training loss: 0.6560350656509399 = 0.00590867456048727 + 0.1 * 6.501263618469238
Epoch 900, val loss: 1.1863658428192139
Epoch 910, training loss: 0.6575893759727478 = 0.0057640690356493 + 0.1 * 6.518252849578857
Epoch 910, val loss: 1.1899263858795166
Epoch 920, training loss: 0.6560600399971008 = 0.005625631660223007 + 0.1 * 6.504344463348389
Epoch 920, val loss: 1.1934645175933838
Epoch 930, training loss: 0.6551818251609802 = 0.005492953583598137 + 0.1 * 6.496889114379883
Epoch 930, val loss: 1.1969250440597534
Epoch 940, training loss: 0.6540775895118713 = 0.005365905351936817 + 0.1 * 6.487116813659668
Epoch 940, val loss: 1.2003788948059082
Epoch 950, training loss: 0.6542009115219116 = 0.005243012215942144 + 0.1 * 6.489579200744629
Epoch 950, val loss: 1.2036830186843872
Epoch 960, training loss: 0.6553848385810852 = 0.005126084666699171 + 0.1 * 6.502587795257568
Epoch 960, val loss: 1.2069065570831299
Epoch 970, training loss: 0.6529425382614136 = 0.00501339603215456 + 0.1 * 6.4792914390563965
Epoch 970, val loss: 1.2101221084594727
Epoch 980, training loss: 0.6528412103652954 = 0.004905319772660732 + 0.1 * 6.479358673095703
Epoch 980, val loss: 1.213327169418335
Epoch 990, training loss: 0.6529732346534729 = 0.004800674505531788 + 0.1 * 6.481725215911865
Epoch 990, val loss: 1.2164028882980347
Epoch 1000, training loss: 0.6522159576416016 = 0.0047004795633256435 + 0.1 * 6.475154399871826
Epoch 1000, val loss: 1.2194706201553345
Epoch 1010, training loss: 0.6521371603012085 = 0.004603262525051832 + 0.1 * 6.475338935852051
Epoch 1010, val loss: 1.2224674224853516
Epoch 1020, training loss: 0.6510839462280273 = 0.004510853439569473 + 0.1 * 6.465730667114258
Epoch 1020, val loss: 1.2254847288131714
Epoch 1030, training loss: 0.6506024599075317 = 0.004421195946633816 + 0.1 * 6.461812973022461
Epoch 1030, val loss: 1.2284643650054932
Epoch 1040, training loss: 0.6520435214042664 = 0.0043340325355529785 + 0.1 * 6.477094650268555
Epoch 1040, val loss: 1.2312989234924316
Epoch 1050, training loss: 0.6521008610725403 = 0.004250307101756334 + 0.1 * 6.478505611419678
Epoch 1050, val loss: 1.2341278791427612
Epoch 1060, training loss: 0.6508234739303589 = 0.004168794024735689 + 0.1 * 6.466546535491943
Epoch 1060, val loss: 1.2369170188903809
Epoch 1070, training loss: 0.6520634889602661 = 0.004090951755642891 + 0.1 * 6.479724884033203
Epoch 1070, val loss: 1.2397409677505493
Epoch 1080, training loss: 0.649221658706665 = 0.004014905076473951 + 0.1 * 6.4520673751831055
Epoch 1080, val loss: 1.242413878440857
Epoch 1090, training loss: 0.6503444314002991 = 0.003942207433283329 + 0.1 * 6.464022159576416
Epoch 1090, val loss: 1.2451139688491821
Epoch 1100, training loss: 0.6488218903541565 = 0.003871062770485878 + 0.1 * 6.449507713317871
Epoch 1100, val loss: 1.2477835416793823
Epoch 1110, training loss: 0.6499878764152527 = 0.0038026825059205294 + 0.1 * 6.461852073669434
Epoch 1110, val loss: 1.2503596544265747
Epoch 1120, training loss: 0.6486382484436035 = 0.0037362880539149046 + 0.1 * 6.449019432067871
Epoch 1120, val loss: 1.252877116203308
Epoch 1130, training loss: 0.651285707950592 = 0.003672303631901741 + 0.1 * 6.476133823394775
Epoch 1130, val loss: 1.2554497718811035
Epoch 1140, training loss: 0.6478353142738342 = 0.0036097292322665453 + 0.1 * 6.442255973815918
Epoch 1140, val loss: 1.2579476833343506
Epoch 1150, training loss: 0.6482556462287903 = 0.00354946730658412 + 0.1 * 6.447061538696289
Epoch 1150, val loss: 1.2604361772537231
Epoch 1160, training loss: 0.6488396525382996 = 0.003490377916023135 + 0.1 * 6.453492641448975
Epoch 1160, val loss: 1.2628676891326904
Epoch 1170, training loss: 0.6493045091629028 = 0.003434379119426012 + 0.1 * 6.4587016105651855
Epoch 1170, val loss: 1.2652117013931274
Epoch 1180, training loss: 0.6467810273170471 = 0.003378785215318203 + 0.1 * 6.434021949768066
Epoch 1180, val loss: 1.2676794528961182
Epoch 1190, training loss: 0.6477659344673157 = 0.0033255591988563538 + 0.1 * 6.444403648376465
Epoch 1190, val loss: 1.2700386047363281
Epoch 1200, training loss: 0.6461716890335083 = 0.00327318231575191 + 0.1 * 6.428984642028809
Epoch 1200, val loss: 1.272331714630127
Epoch 1210, training loss: 0.6490137577056885 = 0.003222645027562976 + 0.1 * 6.457911014556885
Epoch 1210, val loss: 1.2745853662490845
Epoch 1220, training loss: 0.6463455557823181 = 0.0031735925003886223 + 0.1 * 6.431719779968262
Epoch 1220, val loss: 1.2768959999084473
Epoch 1230, training loss: 0.6461461186408997 = 0.0031259628012776375 + 0.1 * 6.430201530456543
Epoch 1230, val loss: 1.2792023420333862
Epoch 1240, training loss: 0.6460078358650208 = 0.0030792749021202326 + 0.1 * 6.429285049438477
Epoch 1240, val loss: 1.281456470489502
Epoch 1250, training loss: 0.6468058824539185 = 0.00303418911062181 + 0.1 * 6.437716960906982
Epoch 1250, val loss: 1.2836191654205322
Epoch 1260, training loss: 0.6452344059944153 = 0.0029898639768362045 + 0.1 * 6.422444820404053
Epoch 1260, val loss: 1.285750150680542
Epoch 1270, training loss: 0.6457636952400208 = 0.002947139786556363 + 0.1 * 6.428165435791016
Epoch 1270, val loss: 1.2879234552383423
Epoch 1280, training loss: 0.6451181769371033 = 0.002905441215261817 + 0.1 * 6.4221272468566895
Epoch 1280, val loss: 1.290102243423462
Epoch 1290, training loss: 0.6451323628425598 = 0.0028644793201237917 + 0.1 * 6.422678470611572
Epoch 1290, val loss: 1.2921676635742188
Epoch 1300, training loss: 0.6455294489860535 = 0.0028249742463231087 + 0.1 * 6.427044868469238
Epoch 1300, val loss: 1.2942183017730713
Epoch 1310, training loss: 0.6449392437934875 = 0.002786363707855344 + 0.1 * 6.4215288162231445
Epoch 1310, val loss: 1.296295166015625
Epoch 1320, training loss: 0.6449089050292969 = 0.0027487839106470346 + 0.1 * 6.421600818634033
Epoch 1320, val loss: 1.2983320951461792
Epoch 1330, training loss: 0.6436639428138733 = 0.002712082350626588 + 0.1 * 6.409518718719482
Epoch 1330, val loss: 1.300338864326477
Epoch 1340, training loss: 0.6450437903404236 = 0.0026761479675769806 + 0.1 * 6.423676490783691
Epoch 1340, val loss: 1.3023496866226196
Epoch 1350, training loss: 0.6449259519577026 = 0.002641344675794244 + 0.1 * 6.42284631729126
Epoch 1350, val loss: 1.3042658567428589
Epoch 1360, training loss: 0.6448081135749817 = 0.0026070233434438705 + 0.1 * 6.42201042175293
Epoch 1360, val loss: 1.3062013387680054
Epoch 1370, training loss: 0.6441956758499146 = 0.0025738973636180162 + 0.1 * 6.41621732711792
Epoch 1370, val loss: 1.3081341981887817
Epoch 1380, training loss: 0.6434735059738159 = 0.0025414524134248495 + 0.1 * 6.409320831298828
Epoch 1380, val loss: 1.310058832168579
Epoch 1390, training loss: 0.6435487866401672 = 0.002509979996830225 + 0.1 * 6.410388469696045
Epoch 1390, val loss: 1.31196928024292
Epoch 1400, training loss: 0.6438268423080444 = 0.0024788263253867626 + 0.1 * 6.413479804992676
Epoch 1400, val loss: 1.3138025999069214
Epoch 1410, training loss: 0.643513023853302 = 0.0024485080502927303 + 0.1 * 6.410645008087158
Epoch 1410, val loss: 1.3156046867370605
Epoch 1420, training loss: 0.6427898406982422 = 0.002419183263555169 + 0.1 * 6.4037065505981445
Epoch 1420, val loss: 1.3174591064453125
Epoch 1430, training loss: 0.6428534984588623 = 0.00239018676802516 + 0.1 * 6.404633045196533
Epoch 1430, val loss: 1.319261908531189
Epoch 1440, training loss: 0.6429432034492493 = 0.0023621725849807262 + 0.1 * 6.4058098793029785
Epoch 1440, val loss: 1.321029543876648
Epoch 1450, training loss: 0.6433600187301636 = 0.002334089018404484 + 0.1 * 6.410258769989014
Epoch 1450, val loss: 1.322841763496399
Epoch 1460, training loss: 0.6427796483039856 = 0.002307288581505418 + 0.1 * 6.404723644256592
Epoch 1460, val loss: 1.324576735496521
Epoch 1470, training loss: 0.642235517501831 = 0.002280802233144641 + 0.1 * 6.3995466232299805
Epoch 1470, val loss: 1.3263500928878784
Epoch 1480, training loss: 0.6429398059844971 = 0.0022549668792635202 + 0.1 * 6.406848430633545
Epoch 1480, val loss: 1.3280706405639648
Epoch 1490, training loss: 0.641781747341156 = 0.0022296884562820196 + 0.1 * 6.3955206871032715
Epoch 1490, val loss: 1.3298068046569824
Epoch 1500, training loss: 0.6423236727714539 = 0.0022044945508241653 + 0.1 * 6.401191711425781
Epoch 1500, val loss: 1.331476092338562
Epoch 1510, training loss: 0.6419643759727478 = 0.002180620562285185 + 0.1 * 6.397837162017822
Epoch 1510, val loss: 1.3331137895584106
Epoch 1520, training loss: 0.6436291337013245 = 0.0021568445954471827 + 0.1 * 6.414722919464111
Epoch 1520, val loss: 1.3348852396011353
Epoch 1530, training loss: 0.6420140266418457 = 0.0021334837656468153 + 0.1 * 6.398805141448975
Epoch 1530, val loss: 1.3364927768707275
Epoch 1540, training loss: 0.6419087052345276 = 0.002110851928591728 + 0.1 * 6.397978782653809
Epoch 1540, val loss: 1.3381621837615967
Epoch 1550, training loss: 0.6419012546539307 = 0.002088334411382675 + 0.1 * 6.398129463195801
Epoch 1550, val loss: 1.339767336845398
Epoch 1560, training loss: 0.6411893367767334 = 0.002066649030894041 + 0.1 * 6.391226291656494
Epoch 1560, val loss: 1.3413708209991455
Epoch 1570, training loss: 0.642326295375824 = 0.0020449336152523756 + 0.1 * 6.402813911437988
Epoch 1570, val loss: 1.3429739475250244
Epoch 1580, training loss: 0.6407329440116882 = 0.0020241914317011833 + 0.1 * 6.387087821960449
Epoch 1580, val loss: 1.3445699214935303
Epoch 1590, training loss: 0.6422104239463806 = 0.002003431785851717 + 0.1 * 6.402070045471191
Epoch 1590, val loss: 1.3461503982543945
Epoch 1600, training loss: 0.64090895652771 = 0.0019833585247397423 + 0.1 * 6.389256000518799
Epoch 1600, val loss: 1.347704529762268
Epoch 1610, training loss: 0.6423047780990601 = 0.0019635576754808426 + 0.1 * 6.403412342071533
Epoch 1610, val loss: 1.349289059638977
Epoch 1620, training loss: 0.6408513188362122 = 0.001944031217135489 + 0.1 * 6.389072895050049
Epoch 1620, val loss: 1.3508137464523315
Epoch 1630, training loss: 0.6417064070701599 = 0.0019250744953751564 + 0.1 * 6.397813320159912
Epoch 1630, val loss: 1.3523350954055786
Epoch 1640, training loss: 0.6402674317359924 = 0.0019062525825574994 + 0.1 * 6.383611679077148
Epoch 1640, val loss: 1.3538647890090942
Epoch 1650, training loss: 0.6397207975387573 = 0.0018879774725064635 + 0.1 * 6.3783278465271
Epoch 1650, val loss: 1.3553816080093384
Epoch 1660, training loss: 0.6412883996963501 = 0.0018697670893743634 + 0.1 * 6.394186496734619
Epoch 1660, val loss: 1.3569117784500122
Epoch 1670, training loss: 0.6399728655815125 = 0.0018520205048844218 + 0.1 * 6.381208419799805
Epoch 1670, val loss: 1.3583241701126099
Epoch 1680, training loss: 0.6410194635391235 = 0.0018346031429246068 + 0.1 * 6.391848087310791
Epoch 1680, val loss: 1.3598116636276245
Epoch 1690, training loss: 0.6393439173698425 = 0.0018174872966483235 + 0.1 * 6.375264644622803
Epoch 1690, val loss: 1.3613226413726807
Epoch 1700, training loss: 0.6406863331794739 = 0.001800759113393724 + 0.1 * 6.388855457305908
Epoch 1700, val loss: 1.3628010749816895
Epoch 1710, training loss: 0.6410548686981201 = 0.0017840804066509008 + 0.1 * 6.392707824707031
Epoch 1710, val loss: 1.36419677734375
Epoch 1720, training loss: 0.6397707462310791 = 0.001768038491718471 + 0.1 * 6.380026817321777
Epoch 1720, val loss: 1.3656411170959473
Epoch 1730, training loss: 0.6392572522163391 = 0.0017519963439553976 + 0.1 * 6.375052452087402
Epoch 1730, val loss: 1.3670843839645386
Epoch 1740, training loss: 0.6406434774398804 = 0.0017364381346851587 + 0.1 * 6.3890700340271
Epoch 1740, val loss: 1.3685060739517212
Epoch 1750, training loss: 0.6389076709747314 = 0.001720838830806315 + 0.1 * 6.371868133544922
Epoch 1750, val loss: 1.3699287176132202
Epoch 1760, training loss: 0.6395921111106873 = 0.001705953385680914 + 0.1 * 6.378861427307129
Epoch 1760, val loss: 1.3713293075561523
Epoch 1770, training loss: 0.6407747268676758 = 0.001691031618975103 + 0.1 * 6.390836715698242
Epoch 1770, val loss: 1.3727364540100098
Epoch 1780, training loss: 0.6392034888267517 = 0.001676454790867865 + 0.1 * 6.375270366668701
Epoch 1780, val loss: 1.3740606307983398
Epoch 1790, training loss: 0.6394741535186768 = 0.001662111608311534 + 0.1 * 6.378119945526123
Epoch 1790, val loss: 1.375454068183899
Epoch 1800, training loss: 0.6387923955917358 = 0.0016479414189234376 + 0.1 * 6.371444225311279
Epoch 1800, val loss: 1.376814365386963
Epoch 1810, training loss: 0.6395195722579956 = 0.001634117099456489 + 0.1 * 6.378854274749756
Epoch 1810, val loss: 1.3781505823135376
Epoch 1820, training loss: 0.639552652835846 = 0.0016203384147956967 + 0.1 * 6.3793230056762695
Epoch 1820, val loss: 1.3795219659805298
Epoch 1830, training loss: 0.6393451690673828 = 0.0016070426208898425 + 0.1 * 6.377380847930908
Epoch 1830, val loss: 1.3808485269546509
Epoch 1840, training loss: 0.6390787959098816 = 0.0015936463605612516 + 0.1 * 6.374851226806641
Epoch 1840, val loss: 1.3822060823440552
Epoch 1850, training loss: 0.6386180520057678 = 0.0015806982992216945 + 0.1 * 6.370373249053955
Epoch 1850, val loss: 1.3835335969924927
Epoch 1860, training loss: 0.6390373110771179 = 0.0015677998308092356 + 0.1 * 6.374695301055908
Epoch 1860, val loss: 1.3848687410354614
Epoch 1870, training loss: 0.6390470266342163 = 0.001555283204652369 + 0.1 * 6.374917507171631
Epoch 1870, val loss: 1.3861355781555176
Epoch 1880, training loss: 0.6387624144554138 = 0.0015428261831402779 + 0.1 * 6.372195720672607
Epoch 1880, val loss: 1.3874835968017578
Epoch 1890, training loss: 0.6385303735733032 = 0.0015305974520742893 + 0.1 * 6.369997501373291
Epoch 1890, val loss: 1.388764500617981
Epoch 1900, training loss: 0.6386852860450745 = 0.0015185973607003689 + 0.1 * 6.371666431427002
Epoch 1900, val loss: 1.3900645971298218
Epoch 1910, training loss: 0.6386545300483704 = 0.0015067833010107279 + 0.1 * 6.371477127075195
Epoch 1910, val loss: 1.3913425207138062
Epoch 1920, training loss: 0.6380816698074341 = 0.0014951553894206882 + 0.1 * 6.3658647537231445
Epoch 1920, val loss: 1.3926461935043335
Epoch 1930, training loss: 0.6375572085380554 = 0.0014836534392088652 + 0.1 * 6.360734939575195
Epoch 1930, val loss: 1.3939129114151
Epoch 1940, training loss: 0.6394034028053284 = 0.0014723464846611023 + 0.1 * 6.379310131072998
Epoch 1940, val loss: 1.395165205001831
Epoch 1950, training loss: 0.6380103826522827 = 0.0014612232334911823 + 0.1 * 6.36549186706543
Epoch 1950, val loss: 1.3964135646820068
Epoch 1960, training loss: 0.6393089294433594 = 0.0014501691330224276 + 0.1 * 6.378587245941162
Epoch 1960, val loss: 1.397642970085144
Epoch 1970, training loss: 0.6396697163581848 = 0.0014395189937204123 + 0.1 * 6.3823018074035645
Epoch 1970, val loss: 1.3989123106002808
Epoch 1980, training loss: 0.6375882029533386 = 0.0014287398662418127 + 0.1 * 6.3615946769714355
Epoch 1980, val loss: 1.4001188278198242
Epoch 1990, training loss: 0.6382429599761963 = 0.0014183551538735628 + 0.1 * 6.368246078491211
Epoch 1990, val loss: 1.401383399963379
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8244596731681603
=== training gcn model ===
Epoch 0, training loss: 2.792637348175049 = 1.9329525232315063 + 0.1 * 8.596847534179688
Epoch 0, val loss: 1.9302575588226318
Epoch 10, training loss: 2.783276081085205 = 1.9235963821411133 + 0.1 * 8.596796035766602
Epoch 10, val loss: 1.9207627773284912
Epoch 20, training loss: 2.771976947784424 = 1.9123282432556152 + 0.1 * 8.596487998962402
Epoch 20, val loss: 1.9093526601791382
Epoch 30, training loss: 2.7563297748565674 = 1.8969085216522217 + 0.1 * 8.594212532043457
Epoch 30, val loss: 1.8940186500549316
Epoch 40, training loss: 2.731888771057129 = 1.8744012117385864 + 0.1 * 8.574874877929688
Epoch 40, val loss: 1.8723162412643433
Epoch 50, training loss: 2.6885690689086914 = 1.843207597732544 + 0.1 * 8.453615188598633
Epoch 50, val loss: 1.8439451456069946
Epoch 60, training loss: 2.619558811187744 = 1.8086155652999878 + 0.1 * 8.109432220458984
Epoch 60, val loss: 1.815165400505066
Epoch 70, training loss: 2.562049388885498 = 1.776153802871704 + 0.1 * 7.858956336975098
Epoch 70, val loss: 1.788305401802063
Epoch 80, training loss: 2.4877572059631348 = 1.7397724390029907 + 0.1 * 7.479848384857178
Epoch 80, val loss: 1.7559735774993896
Epoch 90, training loss: 2.415935516357422 = 1.695468544960022 + 0.1 * 7.204668998718262
Epoch 90, val loss: 1.7182095050811768
Epoch 100, training loss: 2.3467674255371094 = 1.6357365846633911 + 0.1 * 7.110309600830078
Epoch 100, val loss: 1.667349934577942
Epoch 110, training loss: 2.264979839324951 = 1.5592771768569946 + 0.1 * 7.05702543258667
Epoch 110, val loss: 1.6020386219024658
Epoch 120, training loss: 2.1732542514801025 = 1.4714571237564087 + 0.1 * 7.017971992492676
Epoch 120, val loss: 1.5309163331985474
Epoch 130, training loss: 2.0759825706481934 = 1.3778685331344604 + 0.1 * 6.981140613555908
Epoch 130, val loss: 1.4560781717300415
Epoch 140, training loss: 1.9771792888641357 = 1.2821122407913208 + 0.1 * 6.950671195983887
Epoch 140, val loss: 1.3807450532913208
Epoch 150, training loss: 1.880042552947998 = 1.1872620582580566 + 0.1 * 6.927804946899414
Epoch 150, val loss: 1.308637022972107
Epoch 160, training loss: 1.7899203300476074 = 1.0983891487121582 + 0.1 * 6.915311336517334
Epoch 160, val loss: 1.2433305978775024
Epoch 170, training loss: 1.7053157091140747 = 1.0154743194580078 + 0.1 * 6.89841365814209
Epoch 170, val loss: 1.184320092201233
Epoch 180, training loss: 1.6263554096221924 = 0.937921404838562 + 0.1 * 6.884339332580566
Epoch 180, val loss: 1.130326747894287
Epoch 190, training loss: 1.552673578262329 = 0.8653493523597717 + 0.1 * 6.873242378234863
Epoch 190, val loss: 1.0808308124542236
Epoch 200, training loss: 1.4838554859161377 = 0.7975410223007202 + 0.1 * 6.863145351409912
Epoch 200, val loss: 1.0357763767242432
Epoch 210, training loss: 1.4182496070861816 = 0.7330697774887085 + 0.1 * 6.851798057556152
Epoch 210, val loss: 0.994366466999054
Epoch 220, training loss: 1.3568031787872314 = 0.672627866268158 + 0.1 * 6.8417534828186035
Epoch 220, val loss: 0.957575261592865
Epoch 230, training loss: 1.3011059761047363 = 0.6177647113800049 + 0.1 * 6.833413124084473
Epoch 230, val loss: 0.9272636771202087
Epoch 240, training loss: 1.2505760192871094 = 0.5683280229568481 + 0.1 * 6.822480201721191
Epoch 240, val loss: 0.9034450054168701
Epoch 250, training loss: 1.2059201002120972 = 0.5241304636001587 + 0.1 * 6.817896366119385
Epoch 250, val loss: 0.8857256174087524
Epoch 260, training loss: 1.165503740310669 = 0.48469623923301697 + 0.1 * 6.808074951171875
Epoch 260, val loss: 0.8733081817626953
Epoch 270, training loss: 1.12809419631958 = 0.44841837882995605 + 0.1 * 6.79675817489624
Epoch 270, val loss: 0.8647915124893188
Epoch 280, training loss: 1.0930969715118408 = 0.4142347574234009 + 0.1 * 6.7886223793029785
Epoch 280, val loss: 0.8589689135551453
Epoch 290, training loss: 1.059029221534729 = 0.3810790479183197 + 0.1 * 6.779501438140869
Epoch 290, val loss: 0.8549137115478516
Epoch 300, training loss: 1.026620864868164 = 0.3482983112335205 + 0.1 * 6.783224582672119
Epoch 300, val loss: 0.8522338271141052
Epoch 310, training loss: 0.9935963153839111 = 0.31638315320014954 + 0.1 * 6.772130966186523
Epoch 310, val loss: 0.8508144617080688
Epoch 320, training loss: 0.9617866277694702 = 0.285700261592865 + 0.1 * 6.760863304138184
Epoch 320, val loss: 0.8506919145584106
Epoch 330, training loss: 0.9328591823577881 = 0.2567645013332367 + 0.1 * 6.760946750640869
Epoch 330, val loss: 0.8526548743247986
Epoch 340, training loss: 0.9052826166152954 = 0.23036286234855652 + 0.1 * 6.749197006225586
Epoch 340, val loss: 0.8570986986160278
Epoch 350, training loss: 0.8809071779251099 = 0.20660941302776337 + 0.1 * 6.742977619171143
Epoch 350, val loss: 0.8639563918113708
Epoch 360, training loss: 0.8590492010116577 = 0.18536871671676636 + 0.1 * 6.736804962158203
Epoch 360, val loss: 0.8733015656471252
Epoch 370, training loss: 0.8400118947029114 = 0.1664818376302719 + 0.1 * 6.735300064086914
Epoch 370, val loss: 0.8847070932388306
Epoch 380, training loss: 0.8222271203994751 = 0.14980889856815338 + 0.1 * 6.72418212890625
Epoch 380, val loss: 0.8979086875915527
Epoch 390, training loss: 0.8079873919487 = 0.13503354787826538 + 0.1 * 6.729538440704346
Epoch 390, val loss: 0.9124412536621094
Epoch 400, training loss: 0.7940735220909119 = 0.12196610867977142 + 0.1 * 6.721073627471924
Epoch 400, val loss: 0.928036093711853
Epoch 410, training loss: 0.781305193901062 = 0.11035600304603577 + 0.1 * 6.709492206573486
Epoch 410, val loss: 0.9443555474281311
Epoch 420, training loss: 0.7702481150627136 = 0.0999932810664177 + 0.1 * 6.702548027038574
Epoch 420, val loss: 0.9613426923751831
Epoch 430, training loss: 0.7608005404472351 = 0.09075671434402466 + 0.1 * 6.700438022613525
Epoch 430, val loss: 0.9786750078201294
Epoch 440, training loss: 0.7514010071754456 = 0.08252585679292679 + 0.1 * 6.688751220703125
Epoch 440, val loss: 0.9962176084518433
Epoch 450, training loss: 0.7448142766952515 = 0.07514828443527222 + 0.1 * 6.696659564971924
Epoch 450, val loss: 1.0139451026916504
Epoch 460, training loss: 0.7370966076850891 = 0.06856411695480347 + 0.1 * 6.685324668884277
Epoch 460, val loss: 1.0315381288528442
Epoch 470, training loss: 0.7303049564361572 = 0.06266865134239197 + 0.1 * 6.67636251449585
Epoch 470, val loss: 1.0489948987960815
Epoch 480, training loss: 0.7250661253929138 = 0.05737050995230675 + 0.1 * 6.6769561767578125
Epoch 480, val loss: 1.0664068460464478
Epoch 490, training loss: 0.7194207310676575 = 0.05262980982661247 + 0.1 * 6.667908668518066
Epoch 490, val loss: 1.0834649801254272
Epoch 500, training loss: 0.7146908640861511 = 0.04838477447628975 + 0.1 * 6.663060665130615
Epoch 500, val loss: 1.100063681602478
Epoch 510, training loss: 0.709370493888855 = 0.04458348825573921 + 0.1 * 6.64786958694458
Epoch 510, val loss: 1.1162235736846924
Epoch 520, training loss: 0.7064241170883179 = 0.041175875812768936 + 0.1 * 6.652482032775879
Epoch 520, val loss: 1.1317954063415527
Epoch 530, training loss: 0.7023262977600098 = 0.038122303783893585 + 0.1 * 6.642039775848389
Epoch 530, val loss: 1.1471067667007446
Epoch 540, training loss: 0.6989627480506897 = 0.03538273274898529 + 0.1 * 6.635799884796143
Epoch 540, val loss: 1.1615335941314697
Epoch 550, training loss: 0.6954447031021118 = 0.03293005749583244 + 0.1 * 6.625146389007568
Epoch 550, val loss: 1.1757566928863525
Epoch 560, training loss: 0.6933171153068542 = 0.030717631801962852 + 0.1 * 6.625994682312012
Epoch 560, val loss: 1.189258337020874
Epoch 570, training loss: 0.6899752020835876 = 0.028716733679175377 + 0.1 * 6.612584590911865
Epoch 570, val loss: 1.2024123668670654
Epoch 580, training loss: 0.6895015835762024 = 0.026902763172984123 + 0.1 * 6.625988483428955
Epoch 580, val loss: 1.2149996757507324
Epoch 590, training loss: 0.6872236728668213 = 0.025263013318181038 + 0.1 * 6.6196064949035645
Epoch 590, val loss: 1.2273417711257935
Epoch 600, training loss: 0.6848303079605103 = 0.02376951463520527 + 0.1 * 6.610608100891113
Epoch 600, val loss: 1.2390954494476318
Epoch 610, training loss: 0.6839930415153503 = 0.022405151277780533 + 0.1 * 6.615879058837891
Epoch 610, val loss: 1.2506234645843506
Epoch 620, training loss: 0.6804237961769104 = 0.02116345427930355 + 0.1 * 6.5926032066345215
Epoch 620, val loss: 1.2616719007492065
Epoch 630, training loss: 0.6790012121200562 = 0.020026251673698425 + 0.1 * 6.589749336242676
Epoch 630, val loss: 1.2724653482437134
Epoch 640, training loss: 0.678397536277771 = 0.018980013206601143 + 0.1 * 6.594174861907959
Epoch 640, val loss: 1.2828863859176636
Epoch 650, training loss: 0.6763723492622375 = 0.018018905073404312 + 0.1 * 6.583534240722656
Epoch 650, val loss: 1.2927777767181396
Epoch 660, training loss: 0.6747022867202759 = 0.017137208953499794 + 0.1 * 6.575650215148926
Epoch 660, val loss: 1.3026752471923828
Epoch 670, training loss: 0.6740325093269348 = 0.016320785507559776 + 0.1 * 6.577117443084717
Epoch 670, val loss: 1.3120687007904053
Epoch 680, training loss: 0.6730144619941711 = 0.015564857050776482 + 0.1 * 6.574495792388916
Epoch 680, val loss: 1.3212264776229858
Epoch 690, training loss: 0.6717966794967651 = 0.014864674769341946 + 0.1 * 6.569319725036621
Epoch 690, val loss: 1.3301491737365723
Epoch 700, training loss: 0.6710679531097412 = 0.01421399600803852 + 0.1 * 6.568539619445801
Epoch 700, val loss: 1.3388923406600952
Epoch 710, training loss: 0.6694446206092834 = 0.013608057051897049 + 0.1 * 6.558365821838379
Epoch 710, val loss: 1.3472232818603516
Epoch 720, training loss: 0.6688849925994873 = 0.01304344180971384 + 0.1 * 6.558415412902832
Epoch 720, val loss: 1.3553993701934814
Epoch 730, training loss: 0.6685776114463806 = 0.012517381459474564 + 0.1 * 6.560602188110352
Epoch 730, val loss: 1.3634487390518188
Epoch 740, training loss: 0.6673773527145386 = 0.012024913914501667 + 0.1 * 6.553524017333984
Epoch 740, val loss: 1.3710359334945679
Epoch 750, training loss: 0.6659252047538757 = 0.011564738117158413 + 0.1 * 6.543604850769043
Epoch 750, val loss: 1.3787709474563599
Epoch 760, training loss: 0.6644440293312073 = 0.011132098734378815 + 0.1 * 6.533118724822998
Epoch 760, val loss: 1.3859682083129883
Epoch 770, training loss: 0.6631894707679749 = 0.010726428590714931 + 0.1 * 6.524630069732666
Epoch 770, val loss: 1.3932654857635498
Epoch 780, training loss: 0.665233850479126 = 0.010343793779611588 + 0.1 * 6.548900604248047
Epoch 780, val loss: 1.4002996683120728
Epoch 790, training loss: 0.6627237200737 = 0.009983321651816368 + 0.1 * 6.527404308319092
Epoch 790, val loss: 1.4071123600006104
Epoch 800, training loss: 0.6628057360649109 = 0.009644679725170135 + 0.1 * 6.531610488891602
Epoch 800, val loss: 1.4137325286865234
Epoch 810, training loss: 0.6614166498184204 = 0.009325630962848663 + 0.1 * 6.520910263061523
Epoch 810, val loss: 1.420290231704712
Epoch 820, training loss: 0.6617758870124817 = 0.009023299440741539 + 0.1 * 6.527525424957275
Epoch 820, val loss: 1.4266258478164673
Epoch 830, training loss: 0.6595622897148132 = 0.00873668771237135 + 0.1 * 6.508255958557129
Epoch 830, val loss: 1.4328327178955078
Epoch 840, training loss: 0.6596921682357788 = 0.008465339429676533 + 0.1 * 6.51226806640625
Epoch 840, val loss: 1.438940405845642
Epoch 850, training loss: 0.6588127017021179 = 0.008207748644053936 + 0.1 * 6.506049633026123
Epoch 850, val loss: 1.4449234008789062
Epoch 860, training loss: 0.6591317057609558 = 0.007963287644088268 + 0.1 * 6.511683940887451
Epoch 860, val loss: 1.4506937265396118
Epoch 870, training loss: 0.6582224369049072 = 0.007730813696980476 + 0.1 * 6.504915714263916
Epoch 870, val loss: 1.456483006477356
Epoch 880, training loss: 0.6577501893043518 = 0.007509415037930012 + 0.1 * 6.502407550811768
Epoch 880, val loss: 1.462099313735962
Epoch 890, training loss: 0.6562010049819946 = 0.007298274431377649 + 0.1 * 6.48902702331543
Epoch 890, val loss: 1.4675294160842896
Epoch 900, training loss: 0.6572259664535522 = 0.007097034715116024 + 0.1 * 6.501289367675781
Epoch 900, val loss: 1.4728463888168335
Epoch 910, training loss: 0.6554271578788757 = 0.006905786227434874 + 0.1 * 6.485213756561279
Epoch 910, val loss: 1.4781442880630493
Epoch 920, training loss: 0.6558329463005066 = 0.006723134312778711 + 0.1 * 6.491098403930664
Epoch 920, val loss: 1.4833792448043823
Epoch 930, training loss: 0.6552708745002747 = 0.006547798868268728 + 0.1 * 6.4872307777404785
Epoch 930, val loss: 1.4883027076721191
Epoch 940, training loss: 0.6549164652824402 = 0.006380556151270866 + 0.1 * 6.485359191894531
Epoch 940, val loss: 1.493257999420166
Epoch 950, training loss: 0.6541656255722046 = 0.006220596376806498 + 0.1 * 6.479450225830078
Epoch 950, val loss: 1.4980981349945068
Epoch 960, training loss: 0.65373694896698 = 0.006067000329494476 + 0.1 * 6.476699352264404
Epoch 960, val loss: 1.502962350845337
Epoch 970, training loss: 0.6538743376731873 = 0.005919471383094788 + 0.1 * 6.479548931121826
Epoch 970, val loss: 1.5074634552001953
Epoch 980, training loss: 0.6536555290222168 = 0.005778494756668806 + 0.1 * 6.4787702560424805
Epoch 980, val loss: 1.5120893716812134
Epoch 990, training loss: 0.652718722820282 = 0.005643179174512625 + 0.1 * 6.470755100250244
Epoch 990, val loss: 1.5165516138076782
Epoch 1000, training loss: 0.653453528881073 = 0.0055135213769972324 + 0.1 * 6.479399681091309
Epoch 1000, val loss: 1.5210578441619873
Epoch 1010, training loss: 0.6516862511634827 = 0.005388422403484583 + 0.1 * 6.462977886199951
Epoch 1010, val loss: 1.5253183841705322
Epoch 1020, training loss: 0.6526761651039124 = 0.005268211010843515 + 0.1 * 6.474079132080078
Epoch 1020, val loss: 1.5295370817184448
Epoch 1030, training loss: 0.652225911617279 = 0.005153190344572067 + 0.1 * 6.470727443695068
Epoch 1030, val loss: 1.5337882041931152
Epoch 1040, training loss: 0.6517314314842224 = 0.005042032804340124 + 0.1 * 6.466894149780273
Epoch 1040, val loss: 1.537870168685913
Epoch 1050, training loss: 0.6503574848175049 = 0.0049352278001606464 + 0.1 * 6.454222679138184
Epoch 1050, val loss: 1.5419082641601562
Epoch 1060, training loss: 0.6511936187744141 = 0.004832125268876553 + 0.1 * 6.463614463806152
Epoch 1060, val loss: 1.5459933280944824
Epoch 1070, training loss: 0.6520540118217468 = 0.004732418339699507 + 0.1 * 6.473215579986572
Epoch 1070, val loss: 1.5497959852218628
Epoch 1080, training loss: 0.6500629186630249 = 0.004636426456272602 + 0.1 * 6.454265117645264
Epoch 1080, val loss: 1.5536531209945679
Epoch 1090, training loss: 0.6502270102500916 = 0.0045439619570970535 + 0.1 * 6.4568305015563965
Epoch 1090, val loss: 1.5573508739471436
Epoch 1100, training loss: 0.6488803029060364 = 0.004454867448657751 + 0.1 * 6.444254398345947
Epoch 1100, val loss: 1.5611326694488525
Epoch 1110, training loss: 0.6508159637451172 = 0.004368533380329609 + 0.1 * 6.464474201202393
Epoch 1110, val loss: 1.5647945404052734
Epoch 1120, training loss: 0.6497796177864075 = 0.0042854053899645805 + 0.1 * 6.454942226409912
Epoch 1120, val loss: 1.5683836936950684
Epoch 1130, training loss: 0.6496990919113159 = 0.004204719793051481 + 0.1 * 6.454943656921387
Epoch 1130, val loss: 1.57188081741333
Epoch 1140, training loss: 0.6477896571159363 = 0.004127024672925472 + 0.1 * 6.436625957489014
Epoch 1140, val loss: 1.5753943920135498
Epoch 1150, training loss: 0.6498299241065979 = 0.004051818046718836 + 0.1 * 6.4577813148498535
Epoch 1150, val loss: 1.578851342201233
Epoch 1160, training loss: 0.649575412273407 = 0.0039787087589502335 + 0.1 * 6.455966472625732
Epoch 1160, val loss: 1.5821679830551147
Epoch 1170, training loss: 0.6476457715034485 = 0.003908178303390741 + 0.1 * 6.437376022338867
Epoch 1170, val loss: 1.58543062210083
Epoch 1180, training loss: 0.6483166813850403 = 0.0038398418109863997 + 0.1 * 6.44476842880249
Epoch 1180, val loss: 1.5888423919677734
Epoch 1190, training loss: 0.6474241018295288 = 0.003773494390770793 + 0.1 * 6.4365057945251465
Epoch 1190, val loss: 1.5920816659927368
Epoch 1200, training loss: 0.6487181186676025 = 0.0037089677061885595 + 0.1 * 6.450091361999512
Epoch 1200, val loss: 1.5951385498046875
Epoch 1210, training loss: 0.6477206945419312 = 0.003646719502285123 + 0.1 * 6.440739631652832
Epoch 1210, val loss: 1.5981959104537964
Epoch 1220, training loss: 0.6466891765594482 = 0.0035863379016518593 + 0.1 * 6.431028366088867
Epoch 1220, val loss: 1.60133957862854
Epoch 1230, training loss: 0.6473972797393799 = 0.00352780194953084 + 0.1 * 6.438694477081299
Epoch 1230, val loss: 1.604429841041565
Epoch 1240, training loss: 0.646670937538147 = 0.003470603609457612 + 0.1 * 6.432003498077393
Epoch 1240, val loss: 1.6072511672973633
Epoch 1250, training loss: 0.6473281979560852 = 0.003415490500628948 + 0.1 * 6.439126968383789
Epoch 1250, val loss: 1.6102063655853271
Epoch 1260, training loss: 0.6467481255531311 = 0.0033617133740335703 + 0.1 * 6.433864116668701
Epoch 1260, val loss: 1.6131412982940674
Epoch 1270, training loss: 0.6458534002304077 = 0.0033094584941864014 + 0.1 * 6.425439357757568
Epoch 1270, val loss: 1.6160072088241577
Epoch 1280, training loss: 0.6454094052314758 = 0.0032585642766207457 + 0.1 * 6.421507835388184
Epoch 1280, val loss: 1.618678331375122
Epoch 1290, training loss: 0.6454154253005981 = 0.003209315240383148 + 0.1 * 6.422061443328857
Epoch 1290, val loss: 1.6215046644210815
Epoch 1300, training loss: 0.6469707489013672 = 0.0031613281462341547 + 0.1 * 6.438094139099121
Epoch 1300, val loss: 1.6242831945419312
Epoch 1310, training loss: 0.6454817056655884 = 0.0031147110275924206 + 0.1 * 6.423669815063477
Epoch 1310, val loss: 1.626891016960144
Epoch 1320, training loss: 0.6471405625343323 = 0.003069389844313264 + 0.1 * 6.440711498260498
Epoch 1320, val loss: 1.6295950412750244
Epoch 1330, training loss: 0.6450161933898926 = 0.003025130834430456 + 0.1 * 6.419910430908203
Epoch 1330, val loss: 1.632171392440796
Epoch 1340, training loss: 0.6451570391654968 = 0.0029822259675711393 + 0.1 * 6.421748161315918
Epoch 1340, val loss: 1.6348165273666382
Epoch 1350, training loss: 0.6465599536895752 = 0.002940242877230048 + 0.1 * 6.436196804046631
Epoch 1350, val loss: 1.6373777389526367
Epoch 1360, training loss: 0.6448284983634949 = 0.0028993168380111456 + 0.1 * 6.419291973114014
Epoch 1360, val loss: 1.6397271156311035
Epoch 1370, training loss: 0.6448866724967957 = 0.0028597344644367695 + 0.1 * 6.42026948928833
Epoch 1370, val loss: 1.6423394680023193
Epoch 1380, training loss: 0.6441661715507507 = 0.0028208130970597267 + 0.1 * 6.413453578948975
Epoch 1380, val loss: 1.6448696851730347
Epoch 1390, training loss: 0.6447873711585999 = 0.002782889176160097 + 0.1 * 6.420044898986816
Epoch 1390, val loss: 1.6471538543701172
Epoch 1400, training loss: 0.6445498466491699 = 0.0027459554839879274 + 0.1 * 6.418039321899414
Epoch 1400, val loss: 1.6496102809906006
Epoch 1410, training loss: 0.6435522437095642 = 0.0027098674327135086 + 0.1 * 6.408423900604248
Epoch 1410, val loss: 1.6519733667373657
Epoch 1420, training loss: 0.6456956267356873 = 0.0026748005766421556 + 0.1 * 6.430208206176758
Epoch 1420, val loss: 1.6543434858322144
Epoch 1430, training loss: 0.6438977122306824 = 0.0026402610819786787 + 0.1 * 6.412574291229248
Epoch 1430, val loss: 1.6564662456512451
Epoch 1440, training loss: 0.6439570188522339 = 0.0026069958694279194 + 0.1 * 6.4135003089904785
Epoch 1440, val loss: 1.658821940422058
Epoch 1450, training loss: 0.6441512107849121 = 0.002574254758656025 + 0.1 * 6.415769577026367
Epoch 1450, val loss: 1.6610534191131592
Epoch 1460, training loss: 0.6425658464431763 = 0.0025423867627978325 + 0.1 * 6.400234699249268
Epoch 1460, val loss: 1.6632705926895142
Epoch 1470, training loss: 0.6442315578460693 = 0.002511303871870041 + 0.1 * 6.417202472686768
Epoch 1470, val loss: 1.6655583381652832
Epoch 1480, training loss: 0.6448327302932739 = 0.0024805343709886074 + 0.1 * 6.423521995544434
Epoch 1480, val loss: 1.6675148010253906
Epoch 1490, training loss: 0.6435410380363464 = 0.0024508298374712467 + 0.1 * 6.41090202331543
Epoch 1490, val loss: 1.669611930847168
Epoch 1500, training loss: 0.6427538394927979 = 0.0024218044709414244 + 0.1 * 6.4033203125
Epoch 1500, val loss: 1.671849250793457
Epoch 1510, training loss: 0.6440510749816895 = 0.0023932955227792263 + 0.1 * 6.416577339172363
Epoch 1510, val loss: 1.6739035844802856
Epoch 1520, training loss: 0.6428614258766174 = 0.0023653816897422075 + 0.1 * 6.4049601554870605
Epoch 1520, val loss: 1.6758956909179688
Epoch 1530, training loss: 0.6425919532775879 = 0.002338114660233259 + 0.1 * 6.402538299560547
Epoch 1530, val loss: 1.6778910160064697
Epoch 1540, training loss: 0.6421298980712891 = 0.0023114029318094254 + 0.1 * 6.398184776306152
Epoch 1540, val loss: 1.6798783540725708
Epoch 1550, training loss: 0.6433523893356323 = 0.0022854029666632414 + 0.1 * 6.410669803619385
Epoch 1550, val loss: 1.6819151639938354
Epoch 1560, training loss: 0.6427419185638428 = 0.002259841887280345 + 0.1 * 6.404820442199707
Epoch 1560, val loss: 1.6838490962982178
Epoch 1570, training loss: 0.6420865654945374 = 0.0022347846534103155 + 0.1 * 6.398518085479736
Epoch 1570, val loss: 1.6857157945632935
Epoch 1580, training loss: 0.6422430872917175 = 0.002210331615060568 + 0.1 * 6.400327682495117
Epoch 1580, val loss: 1.687686562538147
Epoch 1590, training loss: 0.6415975093841553 = 0.0021863607689738274 + 0.1 * 6.394111156463623
Epoch 1590, val loss: 1.6895596981048584
Epoch 1600, training loss: 0.6427882313728333 = 0.002162911230698228 + 0.1 * 6.406252861022949
Epoch 1600, val loss: 1.691424012184143
Epoch 1610, training loss: 0.641856849193573 = 0.002139867516234517 + 0.1 * 6.397169589996338
Epoch 1610, val loss: 1.6932823657989502
Epoch 1620, training loss: 0.6424925923347473 = 0.00211742683313787 + 0.1 * 6.403751373291016
Epoch 1620, val loss: 1.6950584650039673
Epoch 1630, training loss: 0.6410389542579651 = 0.0020954383071511984 + 0.1 * 6.389434814453125
Epoch 1630, val loss: 1.6968557834625244
Epoch 1640, training loss: 0.6419376134872437 = 0.0020738402381539345 + 0.1 * 6.398637771606445
Epoch 1640, val loss: 1.6986333131790161
Epoch 1650, training loss: 0.6409378051757812 = 0.0020525932777673006 + 0.1 * 6.388852119445801
Epoch 1650, val loss: 1.700302004814148
Epoch 1660, training loss: 0.6423859596252441 = 0.0020319498144090176 + 0.1 * 6.403540134429932
Epoch 1660, val loss: 1.7021595239639282
Epoch 1670, training loss: 0.6407820582389832 = 0.002011464908719063 + 0.1 * 6.3877058029174805
Epoch 1670, val loss: 1.7038017511367798
Epoch 1680, training loss: 0.6423562169075012 = 0.0019914680160582066 + 0.1 * 6.403647422790527
Epoch 1680, val loss: 1.7055761814117432
Epoch 1690, training loss: 0.6415570974349976 = 0.0019718154799193144 + 0.1 * 6.395852565765381
Epoch 1690, val loss: 1.7071588039398193
Epoch 1700, training loss: 0.6430990099906921 = 0.0019526430405676365 + 0.1 * 6.411463737487793
Epoch 1700, val loss: 1.7089039087295532
Epoch 1710, training loss: 0.6409687995910645 = 0.0019337236881256104 + 0.1 * 6.390350818634033
Epoch 1710, val loss: 1.7104322910308838
Epoch 1720, training loss: 0.6414692401885986 = 0.0019152909517288208 + 0.1 * 6.395539283752441
Epoch 1720, val loss: 1.712203025817871
Epoch 1730, training loss: 0.6402575373649597 = 0.0018971358658745885 + 0.1 * 6.383603572845459
Epoch 1730, val loss: 1.7137850522994995
Epoch 1740, training loss: 0.6417151093482971 = 0.0018793618073686957 + 0.1 * 6.398357391357422
Epoch 1740, val loss: 1.7154542207717896
Epoch 1750, training loss: 0.640469491481781 = 0.00186174176633358 + 0.1 * 6.386077404022217
Epoch 1750, val loss: 1.716946005821228
Epoch 1760, training loss: 0.641348659992218 = 0.001844561193138361 + 0.1 * 6.395040512084961
Epoch 1760, val loss: 1.7185330390930176
Epoch 1770, training loss: 0.6396996378898621 = 0.0018276411574333906 + 0.1 * 6.378719806671143
Epoch 1770, val loss: 1.7200748920440674
Epoch 1780, training loss: 0.6406790614128113 = 0.0018110853852704167 + 0.1 * 6.388679504394531
Epoch 1780, val loss: 1.7217066287994385
Epoch 1790, training loss: 0.640899658203125 = 0.001794846961274743 + 0.1 * 6.391047477722168
Epoch 1790, val loss: 1.7231969833374023
Epoch 1800, training loss: 0.6404285430908203 = 0.001778723904863 + 0.1 * 6.386497974395752
Epoch 1800, val loss: 1.7246661186218262
Epoch 1810, training loss: 0.6405636668205261 = 0.0017630689544603229 + 0.1 * 6.388006210327148
Epoch 1810, val loss: 1.7261337041854858
Epoch 1820, training loss: 0.6392666697502136 = 0.0017475701170042157 + 0.1 * 6.3751912117004395
Epoch 1820, val loss: 1.7275186777114868
Epoch 1830, training loss: 0.6401904225349426 = 0.0017325003864243627 + 0.1 * 6.384578704833984
Epoch 1830, val loss: 1.7290538549423218
Epoch 1840, training loss: 0.6396498084068298 = 0.0017175436951220036 + 0.1 * 6.379322052001953
Epoch 1840, val loss: 1.7304831743240356
Epoch 1850, training loss: 0.6395462155342102 = 0.0017029040027409792 + 0.1 * 6.378432750701904
Epoch 1850, val loss: 1.7319093942642212
Epoch 1860, training loss: 0.6408617496490479 = 0.0016885034274309874 + 0.1 * 6.391732692718506
Epoch 1860, val loss: 1.7332714796066284
Epoch 1870, training loss: 0.6408154368400574 = 0.0016743459273129702 + 0.1 * 6.391410827636719
Epoch 1870, val loss: 1.7345845699310303
Epoch 1880, training loss: 0.6391153931617737 = 0.0016604115953668952 + 0.1 * 6.374549865722656
Epoch 1880, val loss: 1.7360022068023682
Epoch 1890, training loss: 0.6401382684707642 = 0.0016467183595523238 + 0.1 * 6.384915351867676
Epoch 1890, val loss: 1.737421989440918
Epoch 1900, training loss: 0.6389877796173096 = 0.0016332415398210287 + 0.1 * 6.3735456466674805
Epoch 1900, val loss: 1.7386943101882935
Epoch 1910, training loss: 0.6403582692146301 = 0.0016199484234675765 + 0.1 * 6.387382984161377
Epoch 1910, val loss: 1.7400133609771729
Epoch 1920, training loss: 0.6384322047233582 = 0.001606905716471374 + 0.1 * 6.368253231048584
Epoch 1920, val loss: 1.7413328886032104
Epoch 1930, training loss: 0.6395847201347351 = 0.0015940996818244457 + 0.1 * 6.379905700683594
Epoch 1930, val loss: 1.74263596534729
Epoch 1940, training loss: 0.6387260556221008 = 0.001581472111865878 + 0.1 * 6.371445655822754
Epoch 1940, val loss: 1.7438908815383911
Epoch 1950, training loss: 0.6391379833221436 = 0.0015690511791035533 + 0.1 * 6.375689506530762
Epoch 1950, val loss: 1.7452436685562134
Epoch 1960, training loss: 0.6392362713813782 = 0.0015567680820822716 + 0.1 * 6.376794815063477
Epoch 1960, val loss: 1.7464874982833862
Epoch 1970, training loss: 0.6384808421134949 = 0.0015447161858901381 + 0.1 * 6.369361400604248
Epoch 1970, val loss: 1.7475601434707642
Epoch 1980, training loss: 0.638685405254364 = 0.0015329321613535285 + 0.1 * 6.371524810791016
Epoch 1980, val loss: 1.7488648891448975
Epoch 1990, training loss: 0.6388242840766907 = 0.001521331607364118 + 0.1 * 6.3730292320251465
Epoch 1990, val loss: 1.7502199411392212
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8255139694254086
=== training gcn model ===
Epoch 0, training loss: 2.7918407917022705 = 1.932160496711731 + 0.1 * 8.596803665161133
Epoch 0, val loss: 1.9420939683914185
Epoch 10, training loss: 2.7826344966888428 = 1.9229614734649658 + 0.1 * 8.59673023223877
Epoch 10, val loss: 1.93313467502594
Epoch 20, training loss: 2.771404504776001 = 1.9117826223373413 + 0.1 * 8.59621810913086
Epoch 20, val loss: 1.9218107461929321
Epoch 30, training loss: 2.7551817893981934 = 1.8960347175598145 + 0.1 * 8.591471672058105
Epoch 30, val loss: 1.9054946899414062
Epoch 40, training loss: 2.7285377979278564 = 1.8729314804077148 + 0.1 * 8.556062698364258
Epoch 40, val loss: 1.8817782402038574
Epoch 50, training loss: 2.6803781986236572 = 1.8420166969299316 + 0.1 * 8.383615493774414
Epoch 50, val loss: 1.8514631986618042
Epoch 60, training loss: 2.6226985454559326 = 1.8088034391403198 + 0.1 * 8.13895034790039
Epoch 60, val loss: 1.8207826614379883
Epoch 70, training loss: 2.5728085041046143 = 1.7774826288223267 + 0.1 * 7.953258037567139
Epoch 70, val loss: 1.7916926145553589
Epoch 80, training loss: 2.5026967525482178 = 1.7417011260986328 + 0.1 * 7.609955310821533
Epoch 80, val loss: 1.7571287155151367
Epoch 90, training loss: 2.4285287857055664 = 1.6966803073883057 + 0.1 * 7.318484783172607
Epoch 90, val loss: 1.716384768486023
Epoch 100, training loss: 2.3533241748809814 = 1.636549949645996 + 0.1 * 7.1677422523498535
Epoch 100, val loss: 1.6623868942260742
Epoch 110, training loss: 2.2641022205352783 = 1.5575138330459595 + 0.1 * 7.065884113311768
Epoch 110, val loss: 1.5919350385665894
Epoch 120, training loss: 2.1638994216918945 = 1.4647670984268188 + 0.1 * 6.991322040557861
Epoch 120, val loss: 1.5136258602142334
Epoch 130, training loss: 2.057830333709717 = 1.3632404804229736 + 0.1 * 6.945898056030273
Epoch 130, val loss: 1.4312779903411865
Epoch 140, training loss: 1.9511024951934814 = 1.2596346139907837 + 0.1 * 6.914679050445557
Epoch 140, val loss: 1.3497296571731567
Epoch 150, training loss: 1.8492405414581299 = 1.160362720489502 + 0.1 * 6.888777732849121
Epoch 150, val loss: 1.2755155563354492
Epoch 160, training loss: 1.7563836574554443 = 1.0697389841079712 + 0.1 * 6.866446495056152
Epoch 160, val loss: 1.2108030319213867
Epoch 170, training loss: 1.6731470823287964 = 0.9883182644844055 + 0.1 * 6.848288059234619
Epoch 170, val loss: 1.1555185317993164
Epoch 180, training loss: 1.598250389099121 = 0.9140956401824951 + 0.1 * 6.841547012329102
Epoch 180, val loss: 1.1070195436477661
Epoch 190, training loss: 1.5288100242614746 = 0.8456602096557617 + 0.1 * 6.831498622894287
Epoch 190, val loss: 1.0640407800674438
Epoch 200, training loss: 1.4632086753845215 = 0.7809791564941406 + 0.1 * 6.822294235229492
Epoch 200, val loss: 1.0248010158538818
Epoch 210, training loss: 1.4019927978515625 = 0.719523012638092 + 0.1 * 6.824697494506836
Epoch 210, val loss: 0.9890507459640503
Epoch 220, training loss: 1.3431220054626465 = 0.6621147394180298 + 0.1 * 6.810072898864746
Epoch 220, val loss: 0.9578481316566467
Epoch 230, training loss: 1.2898205518722534 = 0.6089966297149658 + 0.1 * 6.808238983154297
Epoch 230, val loss: 0.9315577745437622
Epoch 240, training loss: 1.240370273590088 = 0.560688853263855 + 0.1 * 6.796814441680908
Epoch 240, val loss: 0.9107159376144409
Epoch 250, training loss: 1.1952431201934814 = 0.5166506171226501 + 0.1 * 6.785924911499023
Epoch 250, val loss: 0.8948767185211182
Epoch 260, training loss: 1.1550712585449219 = 0.47642192244529724 + 0.1 * 6.786493301391602
Epoch 260, val loss: 0.8835163116455078
Epoch 270, training loss: 1.1165179014205933 = 0.43930697441101074 + 0.1 * 6.772109031677246
Epoch 270, val loss: 0.8756204843521118
Epoch 280, training loss: 1.0823100805282593 = 0.4043307602405548 + 0.1 * 6.779792785644531
Epoch 280, val loss: 0.8703809976577759
Epoch 290, training loss: 1.0471141338348389 = 0.3713216483592987 + 0.1 * 6.757925033569336
Epoch 290, val loss: 0.8671055436134338
Epoch 300, training loss: 1.0148674249649048 = 0.3399215042591095 + 0.1 * 6.7494587898254395
Epoch 300, val loss: 0.8654958605766296
Epoch 310, training loss: 0.984322726726532 = 0.31020885705947876 + 0.1 * 6.741138458251953
Epoch 310, val loss: 0.8653388619422913
Epoch 320, training loss: 0.956015408039093 = 0.28237754106521606 + 0.1 * 6.7363786697387695
Epoch 320, val loss: 0.8666041493415833
Epoch 330, training loss: 0.9297634363174438 = 0.25633829832077026 + 0.1 * 6.734251022338867
Epoch 330, val loss: 0.8694397807121277
Epoch 340, training loss: 0.905182421207428 = 0.23213732242584229 + 0.1 * 6.7304511070251465
Epoch 340, val loss: 0.8738206624984741
Epoch 350, training loss: 0.8821007013320923 = 0.20978398621082306 + 0.1 * 6.7231669425964355
Epoch 350, val loss: 0.8795245885848999
Epoch 360, training loss: 0.8614243865013123 = 0.18920589983463287 + 0.1 * 6.722184658050537
Epoch 360, val loss: 0.8866195678710938
Epoch 370, training loss: 0.8420165181159973 = 0.17038919031620026 + 0.1 * 6.716273307800293
Epoch 370, val loss: 0.8947691321372986
Epoch 380, training loss: 0.825607419013977 = 0.15322987735271454 + 0.1 * 6.723775386810303
Epoch 380, val loss: 0.9039775729179382
Epoch 390, training loss: 0.8084393739700317 = 0.1377321034669876 + 0.1 * 6.707072734832764
Epoch 390, val loss: 0.9139384031295776
Epoch 400, training loss: 0.7951663136482239 = 0.12374819815158844 + 0.1 * 6.714180946350098
Epoch 400, val loss: 0.9245713949203491
Epoch 410, training loss: 0.7807139754295349 = 0.11123151332139969 + 0.1 * 6.694824695587158
Epoch 410, val loss: 0.9356393814086914
Epoch 420, training loss: 0.7687326073646545 = 0.10002485662698746 + 0.1 * 6.687077045440674
Epoch 420, val loss: 0.947241485118866
Epoch 430, training loss: 0.758490800857544 = 0.09004412591457367 + 0.1 * 6.684466361999512
Epoch 430, val loss: 0.959149181842804
Epoch 440, training loss: 0.7485865950584412 = 0.08118618279695511 + 0.1 * 6.674004077911377
Epoch 440, val loss: 0.9712443947792053
Epoch 450, training loss: 0.7421415448188782 = 0.07331862300634384 + 0.1 * 6.688229560852051
Epoch 450, val loss: 0.983523428440094
Epoch 460, training loss: 0.7339680194854736 = 0.0663875862956047 + 0.1 * 6.675804138183594
Epoch 460, val loss: 0.9956533908843994
Epoch 470, training loss: 0.7261647582054138 = 0.06027134135365486 + 0.1 * 6.658934116363525
Epoch 470, val loss: 1.0076581239700317
Epoch 480, training loss: 0.7202663421630859 = 0.054847754538059235 + 0.1 * 6.654185771942139
Epoch 480, val loss: 1.0196709632873535
Epoch 490, training loss: 0.7145933508872986 = 0.05004286766052246 + 0.1 * 6.645504951477051
Epoch 490, val loss: 1.0315062999725342
Epoch 500, training loss: 0.7101563811302185 = 0.04578305780887604 + 0.1 * 6.643733501434326
Epoch 500, val loss: 1.0431342124938965
Epoch 510, training loss: 0.7077582478523254 = 0.04200662672519684 + 0.1 * 6.6575164794921875
Epoch 510, val loss: 1.0545644760131836
Epoch 520, training loss: 0.7010400891304016 = 0.03866520896553993 + 0.1 * 6.623748302459717
Epoch 520, val loss: 1.065621256828308
Epoch 530, training loss: 0.6984024047851562 = 0.035690587013959885 + 0.1 * 6.62711763381958
Epoch 530, val loss: 1.076444387435913
Epoch 540, training loss: 0.6958418488502502 = 0.033047016710042953 + 0.1 * 6.62794828414917
Epoch 540, val loss: 1.0869958400726318
Epoch 550, training loss: 0.691615104675293 = 0.030690088868141174 + 0.1 * 6.609250068664551
Epoch 550, val loss: 1.0971202850341797
Epoch 560, training loss: 0.6896926164627075 = 0.02857566438615322 + 0.1 * 6.611169338226318
Epoch 560, val loss: 1.1071078777313232
Epoch 570, training loss: 0.6873998641967773 = 0.026675015687942505 + 0.1 * 6.607248783111572
Epoch 570, val loss: 1.1167885065078735
Epoch 580, training loss: 0.6850069165229797 = 0.024968953803181648 + 0.1 * 6.60037899017334
Epoch 580, val loss: 1.1261125802993774
Epoch 590, training loss: 0.6828956007957458 = 0.023432333022356033 + 0.1 * 6.594632625579834
Epoch 590, val loss: 1.1351604461669922
Epoch 600, training loss: 0.6807271838188171 = 0.02203928865492344 + 0.1 * 6.586878776550293
Epoch 600, val loss: 1.1440393924713135
Epoch 610, training loss: 0.681788980960846 = 0.020771680399775505 + 0.1 * 6.610172748565674
Epoch 610, val loss: 1.1527063846588135
Epoch 620, training loss: 0.6779788732528687 = 0.019620807841420174 + 0.1 * 6.583580493927002
Epoch 620, val loss: 1.1609727144241333
Epoch 630, training loss: 0.676014244556427 = 0.018570266664028168 + 0.1 * 6.574440002441406
Epoch 630, val loss: 1.1691179275512695
Epoch 640, training loss: 0.6756935119628906 = 0.01760605350136757 + 0.1 * 6.580874443054199
Epoch 640, val loss: 1.1771132946014404
Epoch 650, training loss: 0.6749999523162842 = 0.01672009751200676 + 0.1 * 6.582798480987549
Epoch 650, val loss: 1.1848732233047485
Epoch 660, training loss: 0.6720359921455383 = 0.01590571179986 + 0.1 * 6.561302661895752
Epoch 660, val loss: 1.1924021244049072
Epoch 670, training loss: 0.6709250807762146 = 0.015154015272855759 + 0.1 * 6.557710647583008
Epoch 670, val loss: 1.1997644901275635
Epoch 680, training loss: 0.6714101433753967 = 0.014456910081207752 + 0.1 * 6.5695319175720215
Epoch 680, val loss: 1.2070039510726929
Epoch 690, training loss: 0.6686918139457703 = 0.01381372008472681 + 0.1 * 6.548780918121338
Epoch 690, val loss: 1.2139184474945068
Epoch 700, training loss: 0.6676245331764221 = 0.013214901089668274 + 0.1 * 6.54409646987915
Epoch 700, val loss: 1.2208062410354614
Epoch 710, training loss: 0.6672741770744324 = 0.012656236998736858 + 0.1 * 6.54617977142334
Epoch 710, val loss: 1.2276043891906738
Epoch 720, training loss: 0.6661429405212402 = 0.012135744094848633 + 0.1 * 6.540071964263916
Epoch 720, val loss: 1.2342398166656494
Epoch 730, training loss: 0.6659966111183167 = 0.01164922397583723 + 0.1 * 6.543473720550537
Epoch 730, val loss: 1.2407374382019043
Epoch 740, training loss: 0.664591908454895 = 0.011194713413715363 + 0.1 * 6.533971786499023
Epoch 740, val loss: 1.247009038925171
Epoch 750, training loss: 0.664659857749939 = 0.010769189335405827 + 0.1 * 6.538906574249268
Epoch 750, val loss: 1.2531834840774536
Epoch 760, training loss: 0.6630682945251465 = 0.010371239855885506 + 0.1 * 6.526969909667969
Epoch 760, val loss: 1.2592543363571167
Epoch 770, training loss: 0.6651700139045715 = 0.009996708482503891 + 0.1 * 6.551733016967773
Epoch 770, val loss: 1.265169620513916
Epoch 780, training loss: 0.6618286967277527 = 0.009645199403166771 + 0.1 * 6.521834850311279
Epoch 780, val loss: 1.270995020866394
Epoch 790, training loss: 0.661142110824585 = 0.009313641116023064 + 0.1 * 6.518284797668457
Epoch 790, val loss: 1.27669358253479
Epoch 800, training loss: 0.660770058631897 = 0.009001060388982296 + 0.1 * 6.5176897048950195
Epoch 800, val loss: 1.2822575569152832
Epoch 810, training loss: 0.6603268384933472 = 0.008706055581569672 + 0.1 * 6.516207695007324
Epoch 810, val loss: 1.2876701354980469
Epoch 820, training loss: 0.66053307056427 = 0.008427515625953674 + 0.1 * 6.521055698394775
Epoch 820, val loss: 1.2930301427841187
Epoch 830, training loss: 0.6582121849060059 = 0.0081637566909194 + 0.1 * 6.500484466552734
Epoch 830, val loss: 1.2982501983642578
Epoch 840, training loss: 0.6594220399856567 = 0.007913658395409584 + 0.1 * 6.5150837898254395
Epoch 840, val loss: 1.3033802509307861
Epoch 850, training loss: 0.6575424671173096 = 0.007676373235881329 + 0.1 * 6.498661041259766
Epoch 850, val loss: 1.3083970546722412
Epoch 860, training loss: 0.6582171320915222 = 0.007450893521308899 + 0.1 * 6.507662296295166
Epoch 860, val loss: 1.3133461475372314
Epoch 870, training loss: 0.6565455794334412 = 0.007236975245177746 + 0.1 * 6.493085861206055
Epoch 870, val loss: 1.3181781768798828
Epoch 880, training loss: 0.6571108102798462 = 0.007033631671220064 + 0.1 * 6.500771522521973
Epoch 880, val loss: 1.3228601217269897
Epoch 890, training loss: 0.6565911769866943 = 0.006840159185230732 + 0.1 * 6.497509956359863
Epoch 890, val loss: 1.3274617195129395
Epoch 900, training loss: 0.6562556624412537 = 0.006655706092715263 + 0.1 * 6.495999813079834
Epoch 900, val loss: 1.332038402557373
Epoch 910, training loss: 0.6549197435379028 = 0.006480003707110882 + 0.1 * 6.4843974113464355
Epoch 910, val loss: 1.3364907503128052
Epoch 920, training loss: 0.6549432277679443 = 0.006311357021331787 + 0.1 * 6.486318588256836
Epoch 920, val loss: 1.3409141302108765
Epoch 930, training loss: 0.6535781621932983 = 0.006150381173938513 + 0.1 * 6.474277973175049
Epoch 930, val loss: 1.3452235460281372
Epoch 940, training loss: 0.6546791791915894 = 0.005996468476951122 + 0.1 * 6.4868268966674805
Epoch 940, val loss: 1.3494828939437866
Epoch 950, training loss: 0.6534364819526672 = 0.005849477369338274 + 0.1 * 6.475870132446289
Epoch 950, val loss: 1.3535740375518799
Epoch 960, training loss: 0.6535267233848572 = 0.005708741024136543 + 0.1 * 6.478179454803467
Epoch 960, val loss: 1.3576241731643677
Epoch 970, training loss: 0.652520477771759 = 0.00557409692555666 + 0.1 * 6.46946382522583
Epoch 970, val loss: 1.361642837524414
Epoch 980, training loss: 0.6528284549713135 = 0.00544428126886487 + 0.1 * 6.473841667175293
Epoch 980, val loss: 1.3656213283538818
Epoch 990, training loss: 0.6532427072525024 = 0.005319677293300629 + 0.1 * 6.4792304039001465
Epoch 990, val loss: 1.369536280632019
Epoch 1000, training loss: 0.6514058709144592 = 0.005200242158025503 + 0.1 * 6.4620561599731445
Epoch 1000, val loss: 1.37332022190094
Epoch 1010, training loss: 0.6533012390136719 = 0.005085594952106476 + 0.1 * 6.482156276702881
Epoch 1010, val loss: 1.3770272731781006
Epoch 1020, training loss: 0.6508941054344177 = 0.0049750832840800285 + 0.1 * 6.459190368652344
Epoch 1020, val loss: 1.3807262182235718
Epoch 1030, training loss: 0.652452290058136 = 0.004869068041443825 + 0.1 * 6.475832462310791
Epoch 1030, val loss: 1.3842955827713013
Epoch 1040, training loss: 0.6505104303359985 = 0.00476688239723444 + 0.1 * 6.457435131072998
Epoch 1040, val loss: 1.3877969980239868
Epoch 1050, training loss: 0.6510841250419617 = 0.004668711218982935 + 0.1 * 6.464154243469238
Epoch 1050, val loss: 1.3912657499313354
Epoch 1060, training loss: 0.650982141494751 = 0.004573834594339132 + 0.1 * 6.464083194732666
Epoch 1060, val loss: 1.3946863412857056
Epoch 1070, training loss: 0.6498762965202332 = 0.00448227534070611 + 0.1 * 6.453939914703369
Epoch 1070, val loss: 1.398076057434082
Epoch 1080, training loss: 0.6494008898735046 = 0.004394040443003178 + 0.1 * 6.450068473815918
Epoch 1080, val loss: 1.4013839960098267
Epoch 1090, training loss: 0.6493818759918213 = 0.004308846779167652 + 0.1 * 6.450730323791504
Epoch 1090, val loss: 1.4046566486358643
Epoch 1100, training loss: 0.6489319801330566 = 0.004226482938975096 + 0.1 * 6.447054386138916
Epoch 1100, val loss: 1.4078736305236816
Epoch 1110, training loss: 0.6492035388946533 = 0.004146933555603027 + 0.1 * 6.450565814971924
Epoch 1110, val loss: 1.4110597372055054
Epoch 1120, training loss: 0.6487293839454651 = 0.004070316441357136 + 0.1 * 6.446590423583984
Epoch 1120, val loss: 1.4141100645065308
Epoch 1130, training loss: 0.6495178937911987 = 0.003995956853032112 + 0.1 * 6.455219268798828
Epoch 1130, val loss: 1.4171888828277588
Epoch 1140, training loss: 0.6478986740112305 = 0.003924187272787094 + 0.1 * 6.439744472503662
Epoch 1140, val loss: 1.4202229976654053
Epoch 1150, training loss: 0.6477034091949463 = 0.003854480106383562 + 0.1 * 6.4384894371032715
Epoch 1150, val loss: 1.4232017993927002
Epoch 1160, training loss: 0.6479778289794922 = 0.003787015099078417 + 0.1 * 6.44190788269043
Epoch 1160, val loss: 1.4261813163757324
Epoch 1170, training loss: 0.6487438678741455 = 0.0037215782795101404 + 0.1 * 6.450222492218018
Epoch 1170, val loss: 1.4290673732757568
Epoch 1180, training loss: 0.6467535495758057 = 0.0036579426378011703 + 0.1 * 6.43095588684082
Epoch 1180, val loss: 1.431922197341919
Epoch 1190, training loss: 0.651153028011322 = 0.0035965777933597565 + 0.1 * 6.475564002990723
Epoch 1190, val loss: 1.4347115755081177
Epoch 1200, training loss: 0.6480545997619629 = 0.003537347074598074 + 0.1 * 6.445172309875488
Epoch 1200, val loss: 1.4374216794967651
Epoch 1210, training loss: 0.6463507413864136 = 0.003479749197140336 + 0.1 * 6.428709506988525
Epoch 1210, val loss: 1.4401121139526367
Epoch 1220, training loss: 0.6472449898719788 = 0.003423933871090412 + 0.1 * 6.438210487365723
Epoch 1220, val loss: 1.4427814483642578
Epoch 1230, training loss: 0.6458713412284851 = 0.003369461977854371 + 0.1 * 6.425018787384033
Epoch 1230, val loss: 1.4454375505447388
Epoch 1240, training loss: 0.6466338634490967 = 0.0033165118657052517 + 0.1 * 6.433173179626465
Epoch 1240, val loss: 1.4480537176132202
Epoch 1250, training loss: 0.6456893682479858 = 0.003265166189521551 + 0.1 * 6.42424201965332
Epoch 1250, val loss: 1.4506373405456543
Epoch 1260, training loss: 0.6466821432113647 = 0.003215322270989418 + 0.1 * 6.434668064117432
Epoch 1260, val loss: 1.4531896114349365
Epoch 1270, training loss: 0.6462017297744751 = 0.003166663460433483 + 0.1 * 6.430350303649902
Epoch 1270, val loss: 1.4557005167007446
Epoch 1280, training loss: 0.6464861631393433 = 0.003119402565062046 + 0.1 * 6.433667182922363
Epoch 1280, val loss: 1.4582127332687378
Epoch 1290, training loss: 0.6453697681427002 = 0.0030735419131815434 + 0.1 * 6.422962665557861
Epoch 1290, val loss: 1.4606736898422241
Epoch 1300, training loss: 0.6454851627349854 = 0.003028873121365905 + 0.1 * 6.424562931060791
Epoch 1300, val loss: 1.4630486965179443
Epoch 1310, training loss: 0.6453913450241089 = 0.002985424129292369 + 0.1 * 6.42405891418457
Epoch 1310, val loss: 1.4654312133789062
Epoch 1320, training loss: 0.6449645757675171 = 0.0029430631548166275 + 0.1 * 6.420215129852295
Epoch 1320, val loss: 1.4677579402923584
Epoch 1330, training loss: 0.6453971862792969 = 0.0029020102228969336 + 0.1 * 6.424951553344727
Epoch 1330, val loss: 1.4700227975845337
Epoch 1340, training loss: 0.6450917720794678 = 0.0028618795331567526 + 0.1 * 6.422298908233643
Epoch 1340, val loss: 1.4723143577575684
Epoch 1350, training loss: 0.6460173726081848 = 0.0028227383736521006 + 0.1 * 6.43194580078125
Epoch 1350, val loss: 1.474579930305481
Epoch 1360, training loss: 0.644611656665802 = 0.0027847604360431433 + 0.1 * 6.41826868057251
Epoch 1360, val loss: 1.4767954349517822
Epoch 1370, training loss: 0.6436697840690613 = 0.002747645601630211 + 0.1 * 6.409221172332764
Epoch 1370, val loss: 1.4789535999298096
Epoch 1380, training loss: 0.6451575756072998 = 0.0027113899122923613 + 0.1 * 6.424461364746094
Epoch 1380, val loss: 1.4811288118362427
Epoch 1390, training loss: 0.6437907218933105 = 0.0026759232860058546 + 0.1 * 6.411147594451904
Epoch 1390, val loss: 1.4833022356033325
Epoch 1400, training loss: 0.6443638205528259 = 0.002641458297148347 + 0.1 * 6.4172234535217285
Epoch 1400, val loss: 1.4853767156600952
Epoch 1410, training loss: 0.6428713798522949 = 0.0026078743394464254 + 0.1 * 6.402635097503662
Epoch 1410, val loss: 1.4874632358551025
Epoch 1420, training loss: 0.6436883807182312 = 0.0025750871282070875 + 0.1 * 6.4111328125
Epoch 1420, val loss: 1.4895412921905518
Epoch 1430, training loss: 0.644155740737915 = 0.0025430391542613506 + 0.1 * 6.416126728057861
Epoch 1430, val loss: 1.4915448427200317
Epoch 1440, training loss: 0.6445409655570984 = 0.0025118349585682154 + 0.1 * 6.420290946960449
Epoch 1440, val loss: 1.4935648441314697
Epoch 1450, training loss: 0.6431318521499634 = 0.0024811378680169582 + 0.1 * 6.406506538391113
Epoch 1450, val loss: 1.4955711364746094
Epoch 1460, training loss: 0.6435567140579224 = 0.0024513599928468466 + 0.1 * 6.41105318069458
Epoch 1460, val loss: 1.4975342750549316
Epoch 1470, training loss: 0.6429919004440308 = 0.002422032877802849 + 0.1 * 6.405698776245117
Epoch 1470, val loss: 1.4994606971740723
Epoch 1480, training loss: 0.6443502902984619 = 0.0023935679346323013 + 0.1 * 6.419567108154297
Epoch 1480, val loss: 1.5013811588287354
Epoch 1490, training loss: 0.6426708102226257 = 0.002365467604249716 + 0.1 * 6.403053283691406
Epoch 1490, val loss: 1.5032566785812378
Epoch 1500, training loss: 0.6435898542404175 = 0.002338321879506111 + 0.1 * 6.412515163421631
Epoch 1500, val loss: 1.5051391124725342
Epoch 1510, training loss: 0.6424534320831299 = 0.002311627846211195 + 0.1 * 6.4014177322387695
Epoch 1510, val loss: 1.5069217681884766
Epoch 1520, training loss: 0.6441188454627991 = 0.0022855668794363737 + 0.1 * 6.418333053588867
Epoch 1520, val loss: 1.5087761878967285
Epoch 1530, training loss: 0.6420013904571533 = 0.002259930595755577 + 0.1 * 6.397414684295654
Epoch 1530, val loss: 1.510567307472229
Epoch 1540, training loss: 0.6430338025093079 = 0.002235008869320154 + 0.1 * 6.40798807144165
Epoch 1540, val loss: 1.5123764276504517
Epoch 1550, training loss: 0.6416809558868408 = 0.002210517879575491 + 0.1 * 6.394704341888428
Epoch 1550, val loss: 1.5141514539718628
Epoch 1560, training loss: 0.6424277424812317 = 0.0021865617018193007 + 0.1 * 6.402411937713623
Epoch 1560, val loss: 1.5158909559249878
Epoch 1570, training loss: 0.6428748965263367 = 0.002163063734769821 + 0.1 * 6.407118320465088
Epoch 1570, val loss: 1.517638921737671
Epoch 1580, training loss: 0.6430105566978455 = 0.002140012104064226 + 0.1 * 6.408705711364746
Epoch 1580, val loss: 1.5193390846252441
Epoch 1590, training loss: 0.6415582299232483 = 0.0021176193840801716 + 0.1 * 6.394405841827393
Epoch 1590, val loss: 1.5210063457489014
Epoch 1600, training loss: 0.6425352692604065 = 0.002095600822940469 + 0.1 * 6.404397010803223
Epoch 1600, val loss: 1.5226426124572754
Epoch 1610, training loss: 0.6410959959030151 = 0.0020740290638059378 + 0.1 * 6.390219688415527
Epoch 1610, val loss: 1.5242916345596313
Epoch 1620, training loss: 0.6419084072113037 = 0.0020528538152575493 + 0.1 * 6.398555755615234
Epoch 1620, val loss: 1.5259608030319214
Epoch 1630, training loss: 0.6415533423423767 = 0.002032106975093484 + 0.1 * 6.395212173461914
Epoch 1630, val loss: 1.527612566947937
Epoch 1640, training loss: 0.6415173411369324 = 0.002011706819757819 + 0.1 * 6.395056247711182
Epoch 1640, val loss: 1.529181718826294
Epoch 1650, training loss: 0.6416081786155701 = 0.0019917648751288652 + 0.1 * 6.3961639404296875
Epoch 1650, val loss: 1.5308113098144531
Epoch 1660, training loss: 0.6409066915512085 = 0.0019721996504813433 + 0.1 * 6.389344692230225
Epoch 1660, val loss: 1.5323375463485718
Epoch 1670, training loss: 0.6406513452529907 = 0.0019530795980244875 + 0.1 * 6.386982440948486
Epoch 1670, val loss: 1.5338667631149292
Epoch 1680, training loss: 0.6417576670646667 = 0.001934316591359675 + 0.1 * 6.398233890533447
Epoch 1680, val loss: 1.5354228019714355
Epoch 1690, training loss: 0.6402782797813416 = 0.001915810746140778 + 0.1 * 6.38362455368042
Epoch 1690, val loss: 1.5369871854782104
Epoch 1700, training loss: 0.6411149501800537 = 0.001897624577395618 + 0.1 * 6.392172813415527
Epoch 1700, val loss: 1.5385146141052246
Epoch 1710, training loss: 0.6400728225708008 = 0.0018799457466229796 + 0.1 * 6.38192892074585
Epoch 1710, val loss: 1.5399894714355469
Epoch 1720, training loss: 0.6416055560112 = 0.0018625454977154732 + 0.1 * 6.397430419921875
Epoch 1720, val loss: 1.5414488315582275
Epoch 1730, training loss: 0.6403844952583313 = 0.001845326041802764 + 0.1 * 6.385391712188721
Epoch 1730, val loss: 1.542958378791809
Epoch 1740, training loss: 0.6411868333816528 = 0.0018285474507138133 + 0.1 * 6.393582820892334
Epoch 1740, val loss: 1.5444614887237549
Epoch 1750, training loss: 0.6401342749595642 = 0.0018119440646842122 + 0.1 * 6.383223533630371
Epoch 1750, val loss: 1.545930027961731
Epoch 1760, training loss: 0.639444887638092 = 0.0017956884112209082 + 0.1 * 6.376491546630859
Epoch 1760, val loss: 1.5474159717559814
Epoch 1770, training loss: 0.6416195631027222 = 0.0017797454493120313 + 0.1 * 6.398397922515869
Epoch 1770, val loss: 1.5488406419754028
Epoch 1780, training loss: 0.6391851902008057 = 0.0017640978330746293 + 0.1 * 6.374210834503174
Epoch 1780, val loss: 1.5501881837844849
Epoch 1790, training loss: 0.6403818130493164 = 0.0017487540608271956 + 0.1 * 6.386330604553223
Epoch 1790, val loss: 1.5515841245651245
Epoch 1800, training loss: 0.6397601962089539 = 0.0017335828160867095 + 0.1 * 6.380265712738037
Epoch 1800, val loss: 1.5530314445495605
Epoch 1810, training loss: 0.6406802535057068 = 0.001718757557682693 + 0.1 * 6.389615058898926
Epoch 1810, val loss: 1.5543780326843262
Epoch 1820, training loss: 0.6391794681549072 = 0.001704158028587699 + 0.1 * 6.374752998352051
Epoch 1820, val loss: 1.5557056665420532
Epoch 1830, training loss: 0.6419627070426941 = 0.001689823460765183 + 0.1 * 6.40272855758667
Epoch 1830, val loss: 1.5570590496063232
Epoch 1840, training loss: 0.6391993761062622 = 0.001675682608038187 + 0.1 * 6.375236988067627
Epoch 1840, val loss: 1.5584419965744019
Epoch 1850, training loss: 0.6406347155570984 = 0.0016617529327049851 + 0.1 * 6.389729022979736
Epoch 1850, val loss: 1.5597803592681885
Epoch 1860, training loss: 0.6390188336372375 = 0.001648223609663546 + 0.1 * 6.373705863952637
Epoch 1860, val loss: 1.5610352754592896
Epoch 1870, training loss: 0.6395772099494934 = 0.0016348722856491804 + 0.1 * 6.379423141479492
Epoch 1870, val loss: 1.562337040901184
Epoch 1880, training loss: 0.6383726596832275 = 0.0016216434305533767 + 0.1 * 6.367509841918945
Epoch 1880, val loss: 1.5636318922042847
Epoch 1890, training loss: 0.6398882865905762 = 0.0016086434479802847 + 0.1 * 6.382796287536621
Epoch 1890, val loss: 1.5649194717407227
Epoch 1900, training loss: 0.6390975713729858 = 0.0015959113370627165 + 0.1 * 6.375016689300537
Epoch 1900, val loss: 1.566224217414856
Epoch 1910, training loss: 0.639974057674408 = 0.0015832716599106789 + 0.1 * 6.383907794952393
Epoch 1910, val loss: 1.5674620866775513
Epoch 1920, training loss: 0.6385489702224731 = 0.0015708935679867864 + 0.1 * 6.369781017303467
Epoch 1920, val loss: 1.5687717199325562
Epoch 1930, training loss: 0.6405578255653381 = 0.0015586733352392912 + 0.1 * 6.389991760253906
Epoch 1930, val loss: 1.5699840784072876
Epoch 1940, training loss: 0.638252317905426 = 0.0015467220218852162 + 0.1 * 6.367055416107178
Epoch 1940, val loss: 1.5712157487869263
Epoch 1950, training loss: 0.6388240456581116 = 0.0015349320601671934 + 0.1 * 6.372890949249268
Epoch 1950, val loss: 1.5724120140075684
Epoch 1960, training loss: 0.638735294342041 = 0.001523345708847046 + 0.1 * 6.372119426727295
Epoch 1960, val loss: 1.5736604928970337
Epoch 1970, training loss: 0.6378955841064453 = 0.0015118233859539032 + 0.1 * 6.363837718963623
Epoch 1970, val loss: 1.5748826265335083
Epoch 1980, training loss: 0.6387609243392944 = 0.0015005323803052306 + 0.1 * 6.372603893280029
Epoch 1980, val loss: 1.5761157274246216
Epoch 1990, training loss: 0.6386644840240479 = 0.0014893878251314163 + 0.1 * 6.371750831604004
Epoch 1990, val loss: 1.5773727893829346
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8249868212967845
The final CL Acc:0.74074, 0.00800, The final GNN Acc:0.82499, 0.00043
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13258])
remove edge: torch.Size([2, 7752])
updated graph: torch.Size([2, 10454])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8054749965667725 = 1.945789098739624 + 0.1 * 8.596858024597168
Epoch 0, val loss: 1.9469441175460815
Epoch 10, training loss: 2.7951860427856445 = 1.9355089664459229 + 0.1 * 8.596769332885742
Epoch 10, val loss: 1.9362571239471436
Epoch 20, training loss: 2.7821760177612305 = 1.9225521087646484 + 0.1 * 8.596238136291504
Epoch 20, val loss: 1.9227267503738403
Epoch 30, training loss: 2.7633609771728516 = 1.9042558670043945 + 0.1 * 8.59105110168457
Epoch 30, val loss: 1.9035427570343018
Epoch 40, training loss: 2.732499361038208 = 1.8771758079528809 + 0.1 * 8.55323600769043
Epoch 40, val loss: 1.875298023223877
Epoch 50, training loss: 2.679215908050537 = 1.8405265808105469 + 0.1 * 8.386893272399902
Epoch 50, val loss: 1.8388924598693848
Epoch 60, training loss: 2.61523175239563 = 1.8013827800750732 + 0.1 * 8.13848876953125
Epoch 60, val loss: 1.8033905029296875
Epoch 70, training loss: 2.564047336578369 = 1.7658008337020874 + 0.1 * 7.982466220855713
Epoch 70, val loss: 1.7730566263198853
Epoch 80, training loss: 2.4899282455444336 = 1.7238479852676392 + 0.1 * 7.66080379486084
Epoch 80, val loss: 1.7353589534759521
Epoch 90, training loss: 2.405987501144409 = 1.6714791059494019 + 0.1 * 7.345084190368652
Epoch 90, val loss: 1.6885273456573486
Epoch 100, training loss: 2.3215394020080566 = 1.6054667234420776 + 0.1 * 7.160727500915527
Epoch 100, val loss: 1.63042151927948
Epoch 110, training loss: 2.233577013015747 = 1.5268805027008057 + 0.1 * 7.066964149475098
Epoch 110, val loss: 1.5607777833938599
Epoch 120, training loss: 2.1449480056762695 = 1.4434183835983276 + 0.1 * 7.0152974128723145
Epoch 120, val loss: 1.4903541803359985
Epoch 130, training loss: 2.05879282951355 = 1.3610188961029053 + 0.1 * 6.977738857269287
Epoch 130, val loss: 1.425482988357544
Epoch 140, training loss: 1.973478078842163 = 1.2784335613250732 + 0.1 * 6.95044469833374
Epoch 140, val loss: 1.3614084720611572
Epoch 150, training loss: 1.885840892791748 = 1.1925294399261475 + 0.1 * 6.933114528656006
Epoch 150, val loss: 1.2953474521636963
Epoch 160, training loss: 1.794281005859375 = 1.1022433042526245 + 0.1 * 6.920376777648926
Epoch 160, val loss: 1.225562572479248
Epoch 170, training loss: 1.7021998167037964 = 1.011055588722229 + 0.1 * 6.911442279815674
Epoch 170, val loss: 1.1558282375335693
Epoch 180, training loss: 1.6122524738311768 = 0.9219831824302673 + 0.1 * 6.902692794799805
Epoch 180, val loss: 1.087929368019104
Epoch 190, training loss: 1.5258595943450928 = 0.8367359638214111 + 0.1 * 6.891236305236816
Epoch 190, val loss: 1.0232044458389282
Epoch 200, training loss: 1.4460499286651611 = 0.7579404711723328 + 0.1 * 6.881094932556152
Epoch 200, val loss: 0.963811457157135
Epoch 210, training loss: 1.3739709854125977 = 0.6872897148132324 + 0.1 * 6.8668131828308105
Epoch 210, val loss: 0.9114398956298828
Epoch 220, training loss: 1.311702013015747 = 0.6254382133483887 + 0.1 * 6.862638473510742
Epoch 220, val loss: 0.8677018284797668
Epoch 230, training loss: 1.2575299739837646 = 0.5726214647293091 + 0.1 * 6.849084377288818
Epoch 230, val loss: 0.8335762023925781
Epoch 240, training loss: 1.2109973430633545 = 0.5271738171577454 + 0.1 * 6.838235378265381
Epoch 240, val loss: 0.807572603225708
Epoch 250, training loss: 1.1707875728607178 = 0.4875716269016266 + 0.1 * 6.832159519195557
Epoch 250, val loss: 0.7883489727973938
Epoch 260, training loss: 1.1353670358657837 = 0.4524211585521698 + 0.1 * 6.829458236694336
Epoch 260, val loss: 0.774411678314209
Epoch 270, training loss: 1.1034107208251953 = 0.420613169670105 + 0.1 * 6.827976226806641
Epoch 270, val loss: 0.7642205357551575
Epoch 280, training loss: 1.072936773300171 = 0.39088961482048035 + 0.1 * 6.82047176361084
Epoch 280, val loss: 0.755966305732727
Epoch 290, training loss: 1.0436989068984985 = 0.3620540499687195 + 0.1 * 6.81644868850708
Epoch 290, val loss: 0.7484249472618103
Epoch 300, training loss: 1.0151408910751343 = 0.33328482508659363 + 0.1 * 6.818560600280762
Epoch 300, val loss: 0.7407737374305725
Epoch 310, training loss: 0.985256552696228 = 0.3042570650577545 + 0.1 * 6.809994220733643
Epoch 310, val loss: 0.7329121828079224
Epoch 320, training loss: 0.955700695514679 = 0.27485382556915283 + 0.1 * 6.808468341827393
Epoch 320, val loss: 0.7247472405433655
Epoch 330, training loss: 0.9263820648193359 = 0.24560809135437012 + 0.1 * 6.807739734649658
Epoch 330, val loss: 0.7167762517929077
Epoch 340, training loss: 0.8977961540222168 = 0.21749372780323029 + 0.1 * 6.8030242919921875
Epoch 340, val loss: 0.7094308137893677
Epoch 350, training loss: 0.8713757395744324 = 0.19139958918094635 + 0.1 * 6.7997612953186035
Epoch 350, val loss: 0.7032739520072937
Epoch 360, training loss: 0.8484306335449219 = 0.16808684170246124 + 0.1 * 6.803438186645508
Epoch 360, val loss: 0.6989707946777344
Epoch 370, training loss: 0.827383816242218 = 0.14784981310367584 + 0.1 * 6.795340061187744
Epoch 370, val loss: 0.6964960098266602
Epoch 380, training loss: 0.8100534677505493 = 0.1304650753736496 + 0.1 * 6.795883655548096
Epoch 380, val loss: 0.6958826780319214
Epoch 390, training loss: 0.7939419746398926 = 0.11561214923858643 + 0.1 * 6.783298015594482
Epoch 390, val loss: 0.6968992352485657
Epoch 400, training loss: 0.7828292846679688 = 0.10283668339252472 + 0.1 * 6.799925804138184
Epoch 400, val loss: 0.699335515499115
Epoch 410, training loss: 0.7696791887283325 = 0.0918569266796112 + 0.1 * 6.77822208404541
Epoch 410, val loss: 0.7029327750205994
Epoch 420, training loss: 0.7590087056159973 = 0.08231405913829803 + 0.1 * 6.766946315765381
Epoch 420, val loss: 0.7076004147529602
Epoch 430, training loss: 0.750429093837738 = 0.07397029548883438 + 0.1 * 6.764587879180908
Epoch 430, val loss: 0.713153600692749
Epoch 440, training loss: 0.7445077300071716 = 0.06673882156610489 + 0.1 * 6.777688503265381
Epoch 440, val loss: 0.7192144989967346
Epoch 450, training loss: 0.7352763414382935 = 0.06043966859579086 + 0.1 * 6.748366832733154
Epoch 450, val loss: 0.7258434295654297
Epoch 460, training loss: 0.7287532091140747 = 0.054907456040382385 + 0.1 * 6.738457679748535
Epoch 460, val loss: 0.7329379320144653
Epoch 470, training loss: 0.7230554819107056 = 0.05002318322658539 + 0.1 * 6.73032283782959
Epoch 470, val loss: 0.7404420375823975
Epoch 480, training loss: 0.7185425758361816 = 0.045722596347332 + 0.1 * 6.728199481964111
Epoch 480, val loss: 0.7482110857963562
Epoch 490, training loss: 0.7136405110359192 = 0.04193885251879692 + 0.1 * 6.717016696929932
Epoch 490, val loss: 0.7559529542922974
Epoch 500, training loss: 0.7097028493881226 = 0.03859151527285576 + 0.1 * 6.711113452911377
Epoch 500, val loss: 0.7637759447097778
Epoch 510, training loss: 0.7053951621055603 = 0.03562154620885849 + 0.1 * 6.6977362632751465
Epoch 510, val loss: 0.7715138792991638
Epoch 520, training loss: 0.7025787830352783 = 0.032969530671834946 + 0.1 * 6.696092128753662
Epoch 520, val loss: 0.7792021036148071
Epoch 530, training loss: 0.7018705010414124 = 0.03059687651693821 + 0.1 * 6.712736129760742
Epoch 530, val loss: 0.786801815032959
Epoch 540, training loss: 0.6969040632247925 = 0.028489986434578896 + 0.1 * 6.684140205383301
Epoch 540, val loss: 0.7941702008247375
Epoch 550, training loss: 0.6936430931091309 = 0.02659820206463337 + 0.1 * 6.6704487800598145
Epoch 550, val loss: 0.801459014415741
Epoch 560, training loss: 0.6924757957458496 = 0.02488573081791401 + 0.1 * 6.675900936126709
Epoch 560, val loss: 0.8086084723472595
Epoch 570, training loss: 0.6892917156219482 = 0.02334148809313774 + 0.1 * 6.6595025062561035
Epoch 570, val loss: 0.8156323432922363
Epoch 580, training loss: 0.6870502829551697 = 0.021940000355243683 + 0.1 * 6.6511030197143555
Epoch 580, val loss: 0.8224565386772156
Epoch 590, training loss: 0.6857665777206421 = 0.020667459815740585 + 0.1 * 6.650991439819336
Epoch 590, val loss: 0.8290632963180542
Epoch 600, training loss: 0.6829423904418945 = 0.019510654732584953 + 0.1 * 6.634317398071289
Epoch 600, val loss: 0.835496187210083
Epoch 610, training loss: 0.6809439063072205 = 0.01845025084912777 + 0.1 * 6.624936580657959
Epoch 610, val loss: 0.8418478965759277
Epoch 620, training loss: 0.6810700297355652 = 0.01747550442814827 + 0.1 * 6.6359453201293945
Epoch 620, val loss: 0.8480318188667297
Epoch 630, training loss: 0.6791699528694153 = 0.016586730256676674 + 0.1 * 6.6258320808410645
Epoch 630, val loss: 0.8540372252464294
Epoch 640, training loss: 0.6776002049446106 = 0.01577221229672432 + 0.1 * 6.618279457092285
Epoch 640, val loss: 0.8599275946617126
Epoch 650, training loss: 0.6755437254905701 = 0.015016390010714531 + 0.1 * 6.605273246765137
Epoch 650, val loss: 0.8657312989234924
Epoch 660, training loss: 0.6764442324638367 = 0.01431484054774046 + 0.1 * 6.621294021606445
Epoch 660, val loss: 0.8714206218719482
Epoch 670, training loss: 0.6739827990531921 = 0.013665687292814255 + 0.1 * 6.603171348571777
Epoch 670, val loss: 0.8769813179969788
Epoch 680, training loss: 0.6731497049331665 = 0.013063272461295128 + 0.1 * 6.600864410400391
Epoch 680, val loss: 0.8824106454849243
Epoch 690, training loss: 0.6721785068511963 = 0.01250416599214077 + 0.1 * 6.596743106842041
Epoch 690, val loss: 0.8877072334289551
Epoch 700, training loss: 0.6706764698028564 = 0.011982494033873081 + 0.1 * 6.586939811706543
Epoch 700, val loss: 0.8929396271705627
Epoch 710, training loss: 0.6710997819900513 = 0.01149526983499527 + 0.1 * 6.596045017242432
Epoch 710, val loss: 0.8980271220207214
Epoch 720, training loss: 0.6696420907974243 = 0.011040872894227505 + 0.1 * 6.586012363433838
Epoch 720, val loss: 0.9029995799064636
Epoch 730, training loss: 0.6678522229194641 = 0.010614840313792229 + 0.1 * 6.572373867034912
Epoch 730, val loss: 0.9078997373580933
Epoch 740, training loss: 0.6680437922477722 = 0.010214080102741718 + 0.1 * 6.578296661376953
Epoch 740, val loss: 0.9127097725868225
Epoch 750, training loss: 0.6671982407569885 = 0.009837659075856209 + 0.1 * 6.573605537414551
Epoch 750, val loss: 0.9173506498336792
Epoch 760, training loss: 0.6672731637954712 = 0.009485510177910328 + 0.1 * 6.577876567840576
Epoch 760, val loss: 0.9219659566879272
Epoch 770, training loss: 0.6653357744216919 = 0.009154021739959717 + 0.1 * 6.561817646026611
Epoch 770, val loss: 0.9264217019081116
Epoch 780, training loss: 0.6651826500892639 = 0.008841395378112793 + 0.1 * 6.563412189483643
Epoch 780, val loss: 0.9307935833930969
Epoch 790, training loss: 0.6645470857620239 = 0.008545720018446445 + 0.1 * 6.560013771057129
Epoch 790, val loss: 0.93510502576828
Epoch 800, training loss: 0.6639348268508911 = 0.008266383782029152 + 0.1 * 6.556684494018555
Epoch 800, val loss: 0.9393326044082642
Epoch 810, training loss: 0.6650230288505554 = 0.008001212030649185 + 0.1 * 6.570218086242676
Epoch 810, val loss: 0.9433989524841309
Epoch 820, training loss: 0.6629515290260315 = 0.007751225959509611 + 0.1 * 6.552002906799316
Epoch 820, val loss: 0.9474553465843201
Epoch 830, training loss: 0.6634507179260254 = 0.007513876538723707 + 0.1 * 6.559368133544922
Epoch 830, val loss: 0.9514026641845703
Epoch 840, training loss: 0.6615643501281738 = 0.007288757711648941 + 0.1 * 6.542756080627441
Epoch 840, val loss: 0.955279529094696
Epoch 850, training loss: 0.6619167923927307 = 0.007075048517435789 + 0.1 * 6.548417091369629
Epoch 850, val loss: 0.9590768218040466
Epoch 860, training loss: 0.6615049242973328 = 0.006871200632303953 + 0.1 * 6.546336650848389
Epoch 860, val loss: 0.9628386497497559
Epoch 870, training loss: 0.6609856486320496 = 0.0066770995035767555 + 0.1 * 6.54308557510376
Epoch 870, val loss: 0.9664971232414246
Epoch 880, training loss: 0.6612194180488586 = 0.006492051295936108 + 0.1 * 6.5472731590271
Epoch 880, val loss: 0.970059871673584
Epoch 890, training loss: 0.6606932282447815 = 0.0063161058351397514 + 0.1 * 6.543770790100098
Epoch 890, val loss: 0.9735899567604065
Epoch 900, training loss: 0.6592944860458374 = 0.006148602347820997 + 0.1 * 6.531458854675293
Epoch 900, val loss: 0.977073609828949
Epoch 910, training loss: 0.6607978343963623 = 0.0059881554916501045 + 0.1 * 6.548096656799316
Epoch 910, val loss: 0.9805078506469727
Epoch 920, training loss: 0.6598401665687561 = 0.0058341724798083305 + 0.1 * 6.540060043334961
Epoch 920, val loss: 0.9838123321533203
Epoch 930, training loss: 0.659751832485199 = 0.005688041914254427 + 0.1 * 6.540637493133545
Epoch 930, val loss: 0.9871334433555603
Epoch 940, training loss: 0.6584041118621826 = 0.005548024550080299 + 0.1 * 6.528561115264893
Epoch 940, val loss: 0.9903640151023865
Epoch 950, training loss: 0.6588333249092102 = 0.005413881503045559 + 0.1 * 6.534194469451904
Epoch 950, val loss: 0.9935518503189087
Epoch 960, training loss: 0.658045768737793 = 0.005284879822283983 + 0.1 * 6.527608394622803
Epoch 960, val loss: 0.9967041015625
Epoch 970, training loss: 0.6590670347213745 = 0.005161443259567022 + 0.1 * 6.539055347442627
Epoch 970, val loss: 0.9997895359992981
Epoch 980, training loss: 0.6575420498847961 = 0.005042894743382931 + 0.1 * 6.524991035461426
Epoch 980, val loss: 1.0028386116027832
Epoch 990, training loss: 0.6569953560829163 = 0.004929432645440102 + 0.1 * 6.52065896987915
Epoch 990, val loss: 1.0058584213256836
Epoch 1000, training loss: 0.6565245389938354 = 0.00482019130140543 + 0.1 * 6.517043590545654
Epoch 1000, val loss: 1.0087641477584839
Epoch 1010, training loss: 0.6564714908599854 = 0.00471561960875988 + 0.1 * 6.517558574676514
Epoch 1010, val loss: 1.0117043256759644
Epoch 1020, training loss: 0.6569255590438843 = 0.0046147191897034645 + 0.1 * 6.52310848236084
Epoch 1020, val loss: 1.0145386457443237
Epoch 1030, training loss: 0.6571668386459351 = 0.004517420660704374 + 0.1 * 6.526494026184082
Epoch 1030, val loss: 1.017317295074463
Epoch 1040, training loss: 0.6548359394073486 = 0.004424280487000942 + 0.1 * 6.504116535186768
Epoch 1040, val loss: 1.0200810432434082
Epoch 1050, training loss: 0.6553805470466614 = 0.004334175959229469 + 0.1 * 6.510463714599609
Epoch 1050, val loss: 1.0228044986724854
Epoch 1060, training loss: 0.6551154851913452 = 0.004247382748872042 + 0.1 * 6.508680820465088
Epoch 1060, val loss: 1.0254665613174438
Epoch 1070, training loss: 0.6553708910942078 = 0.004163528326898813 + 0.1 * 6.512073993682861
Epoch 1070, val loss: 1.0281003713607788
Epoch 1080, training loss: 0.6549994349479675 = 0.004082909785211086 + 0.1 * 6.509165287017822
Epoch 1080, val loss: 1.0306907892227173
Epoch 1090, training loss: 0.654337465763092 = 0.0040048654191195965 + 0.1 * 6.503326416015625
Epoch 1090, val loss: 1.0332354307174683
Epoch 1100, training loss: 0.6541475057601929 = 0.0039299605414271355 + 0.1 * 6.502175331115723
Epoch 1100, val loss: 1.0357939004898071
Epoch 1110, training loss: 0.6532964110374451 = 0.003856911789625883 + 0.1 * 6.4943952560424805
Epoch 1110, val loss: 1.0382550954818726
Epoch 1120, training loss: 0.6555291414260864 = 0.003786698216572404 + 0.1 * 6.517424583435059
Epoch 1120, val loss: 1.0407085418701172
Epoch 1130, training loss: 0.6523433327674866 = 0.0037186916451901197 + 0.1 * 6.486246585845947
Epoch 1130, val loss: 1.0430673360824585
Epoch 1140, training loss: 0.6529961824417114 = 0.003653089515864849 + 0.1 * 6.4934306144714355
Epoch 1140, val loss: 1.0454868078231812
Epoch 1150, training loss: 0.6520425081253052 = 0.0035894596949219704 + 0.1 * 6.484530448913574
Epoch 1150, val loss: 1.0478038787841797
Epoch 1160, training loss: 0.6522175073623657 = 0.003527715103700757 + 0.1 * 6.486897945404053
Epoch 1160, val loss: 1.0500630140304565
Epoch 1170, training loss: 0.6513267159461975 = 0.0034681374672800303 + 0.1 * 6.478585243225098
Epoch 1170, val loss: 1.0523426532745361
Epoch 1180, training loss: 0.6516577005386353 = 0.0034102906938642263 + 0.1 * 6.482474327087402
Epoch 1180, val loss: 1.0545910596847534
Epoch 1190, training loss: 0.6506665945053101 = 0.0033539633732289076 + 0.1 * 6.473126411437988
Epoch 1190, val loss: 1.05677330493927
Epoch 1200, training loss: 0.6498165726661682 = 0.0032993387430906296 + 0.1 * 6.465172290802002
Epoch 1200, val loss: 1.0589206218719482
Epoch 1210, training loss: 0.6500772833824158 = 0.0032464764080941677 + 0.1 * 6.468307971954346
Epoch 1210, val loss: 1.0611050128936768
Epoch 1220, training loss: 0.6497883200645447 = 0.003195039229467511 + 0.1 * 6.465932846069336
Epoch 1220, val loss: 1.0631630420684814
Epoch 1230, training loss: 0.6501144766807556 = 0.003145297057926655 + 0.1 * 6.469691753387451
Epoch 1230, val loss: 1.065242052078247
Epoch 1240, training loss: 0.6495960354804993 = 0.003096803557127714 + 0.1 * 6.464992046356201
Epoch 1240, val loss: 1.0673032999038696
Epoch 1250, training loss: 0.6501574516296387 = 0.0030497650150209665 + 0.1 * 6.471076488494873
Epoch 1250, val loss: 1.0693410634994507
Epoch 1260, training loss: 0.6499982476234436 = 0.0030041621066629887 + 0.1 * 6.469941139221191
Epoch 1260, val loss: 1.0713433027267456
Epoch 1270, training loss: 0.6483007669448853 = 0.002959393197670579 + 0.1 * 6.453413963317871
Epoch 1270, val loss: 1.0733489990234375
Epoch 1280, training loss: 0.6513491272926331 = 0.0029159130062907934 + 0.1 * 6.484332084655762
Epoch 1280, val loss: 1.0752702951431274
Epoch 1290, training loss: 0.6488258838653564 = 0.0028736658859997988 + 0.1 * 6.459522247314453
Epoch 1290, val loss: 1.077187180519104
Epoch 1300, training loss: 0.648508608341217 = 0.0028325410094112158 + 0.1 * 6.456760883331299
Epoch 1300, val loss: 1.079114317893982
Epoch 1310, training loss: 0.6478089094161987 = 0.002792451763525605 + 0.1 * 6.450164318084717
Epoch 1310, val loss: 1.0810006856918335
Epoch 1320, training loss: 0.6481890678405762 = 0.002753558335825801 + 0.1 * 6.454354763031006
Epoch 1320, val loss: 1.0828920602798462
Epoch 1330, training loss: 0.6479843258857727 = 0.0027154278941452503 + 0.1 * 6.452689170837402
Epoch 1330, val loss: 1.084733247756958
Epoch 1340, training loss: 0.6469606757164001 = 0.002678334480151534 + 0.1 * 6.44282341003418
Epoch 1340, val loss: 1.0865685939788818
Epoch 1350, training loss: 0.6484758853912354 = 0.0026421949733048677 + 0.1 * 6.458336353302002
Epoch 1350, val loss: 1.0884095430374146
Epoch 1360, training loss: 0.6467946767807007 = 0.002606876427307725 + 0.1 * 6.441877841949463
Epoch 1360, val loss: 1.0901801586151123
Epoch 1370, training loss: 0.6494120955467224 = 0.0025722861755639315 + 0.1 * 6.468398094177246
Epoch 1370, val loss: 1.0919448137283325
Epoch 1380, training loss: 0.6471402645111084 = 0.0025389171205461025 + 0.1 * 6.446013450622559
Epoch 1380, val loss: 1.0936857461929321
Epoch 1390, training loss: 0.6479023694992065 = 0.002506169956177473 + 0.1 * 6.453961372375488
Epoch 1390, val loss: 1.0954186916351318
Epoch 1400, training loss: 0.6479108333587646 = 0.002474122680723667 + 0.1 * 6.454366683959961
Epoch 1400, val loss: 1.0971351861953735
Epoch 1410, training loss: 0.6454890370368958 = 0.0024430297780781984 + 0.1 * 6.430459976196289
Epoch 1410, val loss: 1.0988637208938599
Epoch 1420, training loss: 0.6473991274833679 = 0.002412558766081929 + 0.1 * 6.449865341186523
Epoch 1420, val loss: 1.1005752086639404
Epoch 1430, training loss: 0.64653080701828 = 0.002382532460615039 + 0.1 * 6.4414825439453125
Epoch 1430, val loss: 1.102210283279419
Epoch 1440, training loss: 0.649088442325592 = 0.0023533788044005632 + 0.1 * 6.467350482940674
Epoch 1440, val loss: 1.1038376092910767
Epoch 1450, training loss: 0.6464590430259705 = 0.0023250842932611704 + 0.1 * 6.44133996963501
Epoch 1450, val loss: 1.105456829071045
Epoch 1460, training loss: 0.6462430357933044 = 0.0022973990999162197 + 0.1 * 6.439455986022949
Epoch 1460, val loss: 1.1071174144744873
Epoch 1470, training loss: 0.6469259858131409 = 0.0022700834088027477 + 0.1 * 6.446558475494385
Epoch 1470, val loss: 1.1086816787719727
Epoch 1480, training loss: 0.6454108953475952 = 0.002243530936539173 + 0.1 * 6.431673526763916
Epoch 1480, val loss: 1.1102567911148071
Epoch 1490, training loss: 0.6443794965744019 = 0.00221752910874784 + 0.1 * 6.421619415283203
Epoch 1490, val loss: 1.1118693351745605
Epoch 1500, training loss: 0.6449369788169861 = 0.0021919873543083668 + 0.1 * 6.427450180053711
Epoch 1500, val loss: 1.1134235858917236
Epoch 1510, training loss: 0.6444767713546753 = 0.002167115919291973 + 0.1 * 6.423096656799316
Epoch 1510, val loss: 1.1149488687515259
Epoch 1520, training loss: 0.6459259390830994 = 0.002142593264579773 + 0.1 * 6.437833309173584
Epoch 1520, val loss: 1.1164443492889404
Epoch 1530, training loss: 0.6439425945281982 = 0.0021189292892813683 + 0.1 * 6.418236255645752
Epoch 1530, val loss: 1.1179553270339966
Epoch 1540, training loss: 0.6440277695655823 = 0.0020956064108759165 + 0.1 * 6.419321537017822
Epoch 1540, val loss: 1.1195040941238403
Epoch 1550, training loss: 0.6450637578964233 = 0.002072755014523864 + 0.1 * 6.429910182952881
Epoch 1550, val loss: 1.1210232973098755
Epoch 1560, training loss: 0.6432110071182251 = 0.0020502700936049223 + 0.1 * 6.411607265472412
Epoch 1560, val loss: 1.1224853992462158
Epoch 1570, training loss: 0.6448972225189209 = 0.002028294838964939 + 0.1 * 6.428689002990723
Epoch 1570, val loss: 1.1239677667617798
Epoch 1580, training loss: 0.6434729695320129 = 0.0020067542791366577 + 0.1 * 6.414661884307861
Epoch 1580, val loss: 1.1254462003707886
Epoch 1590, training loss: 0.6437295079231262 = 0.001985766924917698 + 0.1 * 6.4174370765686035
Epoch 1590, val loss: 1.1269211769104004
Epoch 1600, training loss: 0.6435004472732544 = 0.0019649856258183718 + 0.1 * 6.4153547286987305
Epoch 1600, val loss: 1.128326654434204
Epoch 1610, training loss: 0.6433552503585815 = 0.0019447426311671734 + 0.1 * 6.414105415344238
Epoch 1610, val loss: 1.1297718286514282
Epoch 1620, training loss: 0.6444879770278931 = 0.001924856100231409 + 0.1 * 6.425631046295166
Epoch 1620, val loss: 1.1312137842178345
Epoch 1630, training loss: 0.6434469819068909 = 0.0019053425639867783 + 0.1 * 6.415416717529297
Epoch 1630, val loss: 1.1326100826263428
Epoch 1640, training loss: 0.6454248428344727 = 0.0018862133147194982 + 0.1 * 6.435386657714844
Epoch 1640, val loss: 1.1340328454971313
Epoch 1650, training loss: 0.6436898708343506 = 0.001867583836428821 + 0.1 * 6.418222427368164
Epoch 1650, val loss: 1.1354118585586548
Epoch 1660, training loss: 0.6425617337226868 = 0.0018491483060643077 + 0.1 * 6.407125473022461
Epoch 1660, val loss: 1.136804461479187
Epoch 1670, training loss: 0.6425257921218872 = 0.0018310161540284753 + 0.1 * 6.406948089599609
Epoch 1670, val loss: 1.1381956338882446
Epoch 1680, training loss: 0.643269419670105 = 0.001813317183405161 + 0.1 * 6.4145612716674805
Epoch 1680, val loss: 1.1395649909973145
Epoch 1690, training loss: 0.6428048014640808 = 0.0017959015676751733 + 0.1 * 6.410089015960693
Epoch 1690, val loss: 1.1409248113632202
Epoch 1700, training loss: 0.6435238718986511 = 0.0017788029508665204 + 0.1 * 6.417450904846191
Epoch 1700, val loss: 1.1422728300094604
Epoch 1710, training loss: 0.6425206065177917 = 0.001762060564942658 + 0.1 * 6.407585620880127
Epoch 1710, val loss: 1.1435878276824951
Epoch 1720, training loss: 0.6425914764404297 = 0.0017455358756706119 + 0.1 * 6.408459186553955
Epoch 1720, val loss: 1.144927740097046
Epoch 1730, training loss: 0.642484724521637 = 0.0017293683486059308 + 0.1 * 6.407553672790527
Epoch 1730, val loss: 1.146243929862976
Epoch 1740, training loss: 0.6418017148971558 = 0.0017134372610598803 + 0.1 * 6.400882244110107
Epoch 1740, val loss: 1.1475541591644287
Epoch 1750, training loss: 0.6422991156578064 = 0.0016976948827505112 + 0.1 * 6.4060139656066895
Epoch 1750, val loss: 1.1488150358200073
Epoch 1760, training loss: 0.6416764259338379 = 0.0016823606565594673 + 0.1 * 6.399940490722656
Epoch 1760, val loss: 1.1500780582427979
Epoch 1770, training loss: 0.6422857046127319 = 0.0016673144418746233 + 0.1 * 6.40618371963501
Epoch 1770, val loss: 1.1513763666152954
Epoch 1780, training loss: 0.6423414945602417 = 0.0016524347010999918 + 0.1 * 6.406890869140625
Epoch 1780, val loss: 1.152632474899292
Epoch 1790, training loss: 0.6424008011817932 = 0.0016379000153392553 + 0.1 * 6.407629013061523
Epoch 1790, val loss: 1.1539180278778076
Epoch 1800, training loss: 0.6407962441444397 = 0.0016236063092947006 + 0.1 * 6.391726016998291
Epoch 1800, val loss: 1.1551647186279297
Epoch 1810, training loss: 0.6426684260368347 = 0.0016095370519906282 + 0.1 * 6.41058874130249
Epoch 1810, val loss: 1.1564480066299438
Epoch 1820, training loss: 0.640593945980072 = 0.0015955620910972357 + 0.1 * 6.389983654022217
Epoch 1820, val loss: 1.1576666831970215
Epoch 1830, training loss: 0.6434009075164795 = 0.0015819682739675045 + 0.1 * 6.418189525604248
Epoch 1830, val loss: 1.1588879823684692
Epoch 1840, training loss: 0.6408112645149231 = 0.0015686077531427145 + 0.1 * 6.392426490783691
Epoch 1840, val loss: 1.160089135169983
Epoch 1850, training loss: 0.6409861445426941 = 0.0015554785495623946 + 0.1 * 6.394306182861328
Epoch 1850, val loss: 1.1613436937332153
Epoch 1860, training loss: 0.6424715518951416 = 0.001542538870126009 + 0.1 * 6.409289836883545
Epoch 1860, val loss: 1.1625574827194214
Epoch 1870, training loss: 0.6400730609893799 = 0.001529756118543446 + 0.1 * 6.385432720184326
Epoch 1870, val loss: 1.1637588739395142
Epoch 1880, training loss: 0.6428272128105164 = 0.0015172691782936454 + 0.1 * 6.41309928894043
Epoch 1880, val loss: 1.1649694442749023
Epoch 1890, training loss: 0.6415254473686218 = 0.00150481762830168 + 0.1 * 6.400206089019775
Epoch 1890, val loss: 1.166142225265503
Epoch 1900, training loss: 0.6405221819877625 = 0.0014927205629646778 + 0.1 * 6.390294551849365
Epoch 1900, val loss: 1.1673176288604736
Epoch 1910, training loss: 0.6405224800109863 = 0.0014807723928242922 + 0.1 * 6.390417098999023
Epoch 1910, val loss: 1.1685162782669067
Epoch 1920, training loss: 0.6417402625083923 = 0.001468945061787963 + 0.1 * 6.402712821960449
Epoch 1920, val loss: 1.1697115898132324
Epoch 1930, training loss: 0.6400374174118042 = 0.0014573881635442376 + 0.1 * 6.385799884796143
Epoch 1930, val loss: 1.1709058284759521
Epoch 1940, training loss: 0.6411418318748474 = 0.0014459596714004874 + 0.1 * 6.396958827972412
Epoch 1940, val loss: 1.172056794166565
Epoch 1950, training loss: 0.6400686502456665 = 0.0014347616815939546 + 0.1 * 6.38633918762207
Epoch 1950, val loss: 1.1732022762298584
Epoch 1960, training loss: 0.6420313715934753 = 0.001423801644705236 + 0.1 * 6.406075954437256
Epoch 1960, val loss: 1.1743948459625244
Epoch 1970, training loss: 0.6405022740364075 = 0.0014127993490546942 + 0.1 * 6.390894412994385
Epoch 1970, val loss: 1.1754989624023438
Epoch 1980, training loss: 0.6399065852165222 = 0.0014021564275026321 + 0.1 * 6.385044097900391
Epoch 1980, val loss: 1.1766822338104248
Epoch 1990, training loss: 0.6400279402732849 = 0.0013915370218455791 + 0.1 * 6.386363983154297
Epoch 1990, val loss: 1.1778082847595215
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 2.799266815185547 = 1.9395825862884521 + 0.1 * 8.596842765808105
Epoch 0, val loss: 1.9415111541748047
Epoch 10, training loss: 2.789858102798462 = 1.9301828145980835 + 0.1 * 8.596752166748047
Epoch 10, val loss: 1.9322758913040161
Epoch 20, training loss: 2.778306245803833 = 1.9186968803405762 + 0.1 * 8.596094131469727
Epoch 20, val loss: 1.9206095933914185
Epoch 30, training loss: 2.7615976333618164 = 1.9026440382003784 + 0.1 * 8.589536666870117
Epoch 30, val loss: 1.9041130542755127
Epoch 40, training loss: 2.7328577041625977 = 1.8789069652557373 + 0.1 * 8.539507865905762
Epoch 40, val loss: 1.879880428314209
Epoch 50, training loss: 2.6716959476470947 = 1.8461095094680786 + 0.1 * 8.255865097045898
Epoch 50, val loss: 1.8473625183105469
Epoch 60, training loss: 2.6136791706085205 = 1.8081609010696411 + 0.1 * 8.055182456970215
Epoch 60, val loss: 1.8119206428527832
Epoch 70, training loss: 2.5449252128601074 = 1.769978642463684 + 0.1 * 7.749464511871338
Epoch 70, val loss: 1.7774028778076172
Epoch 80, training loss: 2.471879005432129 = 1.7290748357772827 + 0.1 * 7.428042888641357
Epoch 80, val loss: 1.7398630380630493
Epoch 90, training loss: 2.3993356227874756 = 1.6770354509353638 + 0.1 * 7.223001003265381
Epoch 90, val loss: 1.693607211112976
Epoch 100, training loss: 2.321481227874756 = 1.6071444749832153 + 0.1 * 7.143368244171143
Epoch 100, val loss: 1.630785584449768
Epoch 110, training loss: 2.2257938385009766 = 1.5201432704925537 + 0.1 * 7.056506156921387
Epoch 110, val loss: 1.5529507398605347
Epoch 120, training loss: 2.1192989349365234 = 1.4223264455795288 + 0.1 * 6.969725608825684
Epoch 120, val loss: 1.4677926301956177
Epoch 130, training loss: 2.011256217956543 = 1.3200327157974243 + 0.1 * 6.912236213684082
Epoch 130, val loss: 1.3796921968460083
Epoch 140, training loss: 1.9067988395690918 = 1.2188259363174438 + 0.1 * 6.879729747772217
Epoch 140, val loss: 1.293616533279419
Epoch 150, training loss: 1.803481936454773 = 1.1180559396743774 + 0.1 * 6.854259967803955
Epoch 150, val loss: 1.2096030712127686
Epoch 160, training loss: 1.7026991844177246 = 1.0194380283355713 + 0.1 * 6.832611083984375
Epoch 160, val loss: 1.1292637586593628
Epoch 170, training loss: 1.608149528503418 = 0.926737368106842 + 0.1 * 6.814121723175049
Epoch 170, val loss: 1.0554410219192505
Epoch 180, training loss: 1.520704746246338 = 0.8409169316291809 + 0.1 * 6.797878742218018
Epoch 180, val loss: 0.9878933429718018
Epoch 190, training loss: 1.4442861080169678 = 0.7648537158966064 + 0.1 * 6.794322967529297
Epoch 190, val loss: 0.9300118088722229
Epoch 200, training loss: 1.377225637435913 = 0.6999168992042542 + 0.1 * 6.773087024688721
Epoch 200, val loss: 0.8831982016563416
Epoch 210, training loss: 1.319916009902954 = 0.6441273093223572 + 0.1 * 6.757887363433838
Epoch 210, val loss: 0.846283495426178
Epoch 220, training loss: 1.2704269886016846 = 0.5954281687736511 + 0.1 * 6.749988555908203
Epoch 220, val loss: 0.8179198503494263
Epoch 230, training loss: 1.2273147106170654 = 0.5517971515655518 + 0.1 * 6.755175590515137
Epoch 230, val loss: 0.7963821291923523
Epoch 240, training loss: 1.1856613159179688 = 0.5123393535614014 + 0.1 * 6.733219623565674
Epoch 240, val loss: 0.7803113460540771
Epoch 250, training loss: 1.1484410762786865 = 0.475667268037796 + 0.1 * 6.7277374267578125
Epoch 250, val loss: 0.7681016325950623
Epoch 260, training loss: 1.1128994226455688 = 0.4409254491329193 + 0.1 * 6.71973991394043
Epoch 260, val loss: 0.7586044669151306
Epoch 270, training loss: 1.0793821811676025 = 0.4078027904033661 + 0.1 * 6.715794086456299
Epoch 270, val loss: 0.7513214349746704
Epoch 280, training loss: 1.0485894680023193 = 0.3761581778526306 + 0.1 * 6.7243123054504395
Epoch 280, val loss: 0.7461093664169312
Epoch 290, training loss: 1.0170233249664307 = 0.3462626039981842 + 0.1 * 6.707606792449951
Epoch 290, val loss: 0.7428855299949646
Epoch 300, training loss: 0.9889172911643982 = 0.3179969787597656 + 0.1 * 6.709202766418457
Epoch 300, val loss: 0.7414860129356384
Epoch 310, training loss: 0.9616596102714539 = 0.29147523641586304 + 0.1 * 6.701843738555908
Epoch 310, val loss: 0.7417678833007812
Epoch 320, training loss: 0.9362452626228333 = 0.26657021045684814 + 0.1 * 6.696750640869141
Epoch 320, val loss: 0.7435874938964844
Epoch 330, training loss: 0.9121599793434143 = 0.2432461380958557 + 0.1 * 6.689138412475586
Epoch 330, val loss: 0.7470224499702454
Epoch 340, training loss: 0.8902027606964111 = 0.22165857255458832 + 0.1 * 6.685441493988037
Epoch 340, val loss: 0.7520968914031982
Epoch 350, training loss: 0.8701874017715454 = 0.20194096863269806 + 0.1 * 6.682464122772217
Epoch 350, val loss: 0.7586876749992371
Epoch 360, training loss: 0.8534926772117615 = 0.18399351835250854 + 0.1 * 6.694991588592529
Epoch 360, val loss: 0.7668619751930237
Epoch 370, training loss: 0.83509361743927 = 0.16786155104637146 + 0.1 * 6.672320365905762
Epoch 370, val loss: 0.7762728333473206
Epoch 380, training loss: 0.8200756311416626 = 0.1533389389514923 + 0.1 * 6.6673665046691895
Epoch 380, val loss: 0.7869571447372437
Epoch 390, training loss: 0.8086860775947571 = 0.14024986326694489 + 0.1 * 6.684361934661865
Epoch 390, val loss: 0.7987694144248962
Epoch 400, training loss: 0.7949690818786621 = 0.1285242736339569 + 0.1 * 6.664448261260986
Epoch 400, val loss: 0.8114027976989746
Epoch 410, training loss: 0.7833119630813599 = 0.11795780807733536 + 0.1 * 6.653541088104248
Epoch 410, val loss: 0.8248274922370911
Epoch 420, training loss: 0.7737182974815369 = 0.1084137111902237 + 0.1 * 6.653046131134033
Epoch 420, val loss: 0.8388230800628662
Epoch 430, training loss: 0.7646785974502563 = 0.09983321279287338 + 0.1 * 6.648454189300537
Epoch 430, val loss: 0.8531185388565063
Epoch 440, training loss: 0.7562614679336548 = 0.09212180972099304 + 0.1 * 6.6413960456848145
Epoch 440, val loss: 0.8674880266189575
Epoch 450, training loss: 0.7489545345306396 = 0.08516275882720947 + 0.1 * 6.637917518615723
Epoch 450, val loss: 0.8820832371711731
Epoch 460, training loss: 0.741638720035553 = 0.07884005457162857 + 0.1 * 6.627986431121826
Epoch 460, val loss: 0.8967379927635193
Epoch 470, training loss: 0.7386085987091064 = 0.07308569550514221 + 0.1 * 6.655229091644287
Epoch 470, val loss: 0.9114629626274109
Epoch 480, training loss: 0.7302222847938538 = 0.06787669658660889 + 0.1 * 6.62345552444458
Epoch 480, val loss: 0.9260381460189819
Epoch 490, training loss: 0.7246371507644653 = 0.06314059346914291 + 0.1 * 6.614965915679932
Epoch 490, val loss: 0.940490186214447
Epoch 500, training loss: 0.7206352949142456 = 0.05881376191973686 + 0.1 * 6.618215084075928
Epoch 500, val loss: 0.9548224806785583
Epoch 510, training loss: 0.716621458530426 = 0.05487025901675224 + 0.1 * 6.617511749267578
Epoch 510, val loss: 0.9689823985099792
Epoch 520, training loss: 0.7119364738464355 = 0.05126627907156944 + 0.1 * 6.606701850891113
Epoch 520, val loss: 0.9829188585281372
Epoch 530, training loss: 0.7090865969657898 = 0.047975536435842514 + 0.1 * 6.611110687255859
Epoch 530, val loss: 0.9966047406196594
Epoch 540, training loss: 0.7045333385467529 = 0.044970106333494186 + 0.1 * 6.595632076263428
Epoch 540, val loss: 1.0099856853485107
Epoch 550, training loss: 0.7015126943588257 = 0.04221384599804878 + 0.1 * 6.592988014221191
Epoch 550, val loss: 1.0231696367263794
Epoch 560, training loss: 0.6982635855674744 = 0.039681751281023026 + 0.1 * 6.585818290710449
Epoch 560, val loss: 1.0361711978912354
Epoch 570, training loss: 0.6962924599647522 = 0.037353772670030594 + 0.1 * 6.589386940002441
Epoch 570, val loss: 1.0488513708114624
Epoch 580, training loss: 0.6932552456855774 = 0.035215847194194794 + 0.1 * 6.5803937911987305
Epoch 580, val loss: 1.061263918876648
Epoch 590, training loss: 0.6902675628662109 = 0.033243197947740555 + 0.1 * 6.570243835449219
Epoch 590, val loss: 1.0734094381332397
Epoch 600, training loss: 0.691443681716919 = 0.031423721462488174 + 0.1 * 6.600199222564697
Epoch 600, val loss: 1.0853103399276733
Epoch 610, training loss: 0.6865027546882629 = 0.029745282605290413 + 0.1 * 6.567574501037598
Epoch 610, val loss: 1.0968608856201172
Epoch 620, training loss: 0.6843256950378418 = 0.02819231152534485 + 0.1 * 6.561334133148193
Epoch 620, val loss: 1.108264684677124
Epoch 630, training loss: 0.6825062036514282 = 0.026752429082989693 + 0.1 * 6.55753755569458
Epoch 630, val loss: 1.1193978786468506
Epoch 640, training loss: 0.6815280318260193 = 0.02541830576956272 + 0.1 * 6.561097621917725
Epoch 640, val loss: 1.1303437948226929
Epoch 650, training loss: 0.6795150637626648 = 0.02417929470539093 + 0.1 * 6.5533576011657715
Epoch 650, val loss: 1.1409770250320435
Epoch 660, training loss: 0.6781626343727112 = 0.023025793954730034 + 0.1 * 6.551368236541748
Epoch 660, val loss: 1.1515142917633057
Epoch 670, training loss: 0.676729142665863 = 0.021951572969555855 + 0.1 * 6.547775745391846
Epoch 670, val loss: 1.161676049232483
Epoch 680, training loss: 0.6765905618667603 = 0.020952504128217697 + 0.1 * 6.556380271911621
Epoch 680, val loss: 1.1716283559799194
Epoch 690, training loss: 0.6740013957023621 = 0.020021378993988037 + 0.1 * 6.53980016708374
Epoch 690, val loss: 1.1814590692520142
Epoch 700, training loss: 0.6723363995552063 = 0.019149642437696457 + 0.1 * 6.531867027282715
Epoch 700, val loss: 1.1910004615783691
Epoch 710, training loss: 0.6707841753959656 = 0.018334275111556053 + 0.1 * 6.52449893951416
Epoch 710, val loss: 1.2003663778305054
Epoch 720, training loss: 0.6697366237640381 = 0.01756962388753891 + 0.1 * 6.521670341491699
Epoch 720, val loss: 1.2095646858215332
Epoch 730, training loss: 0.6693634390830994 = 0.016853533685207367 + 0.1 * 6.525099277496338
Epoch 730, val loss: 1.218544602394104
Epoch 740, training loss: 0.6682956218719482 = 0.016180455684661865 + 0.1 * 6.521151542663574
Epoch 740, val loss: 1.2273708581924438
Epoch 750, training loss: 0.6685720086097717 = 0.015549254603683949 + 0.1 * 6.5302276611328125
Epoch 750, val loss: 1.2359991073608398
Epoch 760, training loss: 0.6660289168357849 = 0.014954258687794209 + 0.1 * 6.510746479034424
Epoch 760, val loss: 1.2444323301315308
Epoch 770, training loss: 0.6648988127708435 = 0.014394023455679417 + 0.1 * 6.50504732131958
Epoch 770, val loss: 1.252816915512085
Epoch 780, training loss: 0.6639114022254944 = 0.013864648528397083 + 0.1 * 6.500467300415039
Epoch 780, val loss: 1.2609128952026367
Epoch 790, training loss: 0.6638344526290894 = 0.013365940190851688 + 0.1 * 6.504684925079346
Epoch 790, val loss: 1.268896460533142
Epoch 800, training loss: 0.6624847650527954 = 0.012895602732896805 + 0.1 * 6.495891571044922
Epoch 800, val loss: 1.2766777276992798
Epoch 810, training loss: 0.6624032855033875 = 0.012450095266103745 + 0.1 * 6.4995317459106445
Epoch 810, val loss: 1.2843319177627563
Epoch 820, training loss: 0.6608145236968994 = 0.01202850416302681 + 0.1 * 6.487859725952148
Epoch 820, val loss: 1.2918461561203003
Epoch 830, training loss: 0.6611075401306152 = 0.011629149317741394 + 0.1 * 6.494783878326416
Epoch 830, val loss: 1.2992826700210571
Epoch 840, training loss: 0.6596051454544067 = 0.011249734088778496 + 0.1 * 6.483553886413574
Epoch 840, val loss: 1.306496500968933
Epoch 850, training loss: 0.6601301431655884 = 0.010889965109527111 + 0.1 * 6.492402076721191
Epoch 850, val loss: 1.313623309135437
Epoch 860, training loss: 0.6590489149093628 = 0.010548312216997147 + 0.1 * 6.485005855560303
Epoch 860, val loss: 1.320518970489502
Epoch 870, training loss: 0.6588691473007202 = 0.010224606841802597 + 0.1 * 6.486445426940918
Epoch 870, val loss: 1.3273781538009644
Epoch 880, training loss: 0.6572975516319275 = 0.009916059672832489 + 0.1 * 6.473814964294434
Epoch 880, val loss: 1.3341031074523926
Epoch 890, training loss: 0.6576125025749207 = 0.009621878154575825 + 0.1 * 6.4799065589904785
Epoch 890, val loss: 1.340742826461792
Epoch 900, training loss: 0.6583002209663391 = 0.009340916760265827 + 0.1 * 6.489592552185059
Epoch 900, val loss: 1.3471494913101196
Epoch 910, training loss: 0.657183825969696 = 0.009073860943317413 + 0.1 * 6.481099605560303
Epoch 910, val loss: 1.3534899950027466
Epoch 920, training loss: 0.6556909084320068 = 0.008819074369966984 + 0.1 * 6.4687180519104
Epoch 920, val loss: 1.3597108125686646
Epoch 930, training loss: 0.6550308465957642 = 0.008575744926929474 + 0.1 * 6.464550971984863
Epoch 930, val loss: 1.3658463954925537
Epoch 940, training loss: 0.6544098258018494 = 0.008343043737113476 + 0.1 * 6.460667610168457
Epoch 940, val loss: 1.3718584775924683
Epoch 950, training loss: 0.6548269987106323 = 0.00812110211700201 + 0.1 * 6.467059135437012
Epoch 950, val loss: 1.3777531385421753
Epoch 960, training loss: 0.6549827456474304 = 0.007908153347671032 + 0.1 * 6.47074556350708
Epoch 960, val loss: 1.38362717628479
Epoch 970, training loss: 0.6567758321762085 = 0.007704058662056923 + 0.1 * 6.49071741104126
Epoch 970, val loss: 1.3892545700073242
Epoch 980, training loss: 0.6541048288345337 = 0.007509257178753614 + 0.1 * 6.46595573425293
Epoch 980, val loss: 1.3948180675506592
Epoch 990, training loss: 0.6525685787200928 = 0.007322310470044613 + 0.1 * 6.452462673187256
Epoch 990, val loss: 1.400341510772705
Epoch 1000, training loss: 0.654004693031311 = 0.0071427361108362675 + 0.1 * 6.468619346618652
Epoch 1000, val loss: 1.4058201313018799
Epoch 1010, training loss: 0.6526835560798645 = 0.006970662157982588 + 0.1 * 6.457129001617432
Epoch 1010, val loss: 1.4111912250518799
Epoch 1020, training loss: 0.6523749232292175 = 0.006805417127907276 + 0.1 * 6.455695152282715
Epoch 1020, val loss: 1.4164011478424072
Epoch 1030, training loss: 0.6515428423881531 = 0.006646852474659681 + 0.1 * 6.448959827423096
Epoch 1030, val loss: 1.4215846061706543
Epoch 1040, training loss: 0.6516543626785278 = 0.0064945220947265625 + 0.1 * 6.451598167419434
Epoch 1040, val loss: 1.4266879558563232
Epoch 1050, training loss: 0.6509323120117188 = 0.006347590126097202 + 0.1 * 6.445847034454346
Epoch 1050, val loss: 1.43169367313385
Epoch 1060, training loss: 0.650043249130249 = 0.0062065706588327885 + 0.1 * 6.438366889953613
Epoch 1060, val loss: 1.4366997480392456
Epoch 1070, training loss: 0.6502341032028198 = 0.006070394534617662 + 0.1 * 6.44163703918457
Epoch 1070, val loss: 1.4415947198867798
Epoch 1080, training loss: 0.6500489711761475 = 0.005939291324466467 + 0.1 * 6.441096782684326
Epoch 1080, val loss: 1.4463629722595215
Epoch 1090, training loss: 0.6491608023643494 = 0.005812798626720905 + 0.1 * 6.4334797859191895
Epoch 1090, val loss: 1.4510875940322876
Epoch 1100, training loss: 0.6497372388839722 = 0.005691205151379108 + 0.1 * 6.440459728240967
Epoch 1100, val loss: 1.4557316303253174
Epoch 1110, training loss: 0.6488006114959717 = 0.0055741616524755955 + 0.1 * 6.43226432800293
Epoch 1110, val loss: 1.4603315591812134
Epoch 1120, training loss: 0.6498526334762573 = 0.0054610841907560825 + 0.1 * 6.443915367126465
Epoch 1120, val loss: 1.4649052619934082
Epoch 1130, training loss: 0.6481368541717529 = 0.005351420491933823 + 0.1 * 6.427854061126709
Epoch 1130, val loss: 1.469287633895874
Epoch 1140, training loss: 0.6488227248191833 = 0.005245784763246775 + 0.1 * 6.435769081115723
Epoch 1140, val loss: 1.4737142324447632
Epoch 1150, training loss: 0.6478919386863708 = 0.0051436168141663074 + 0.1 * 6.427483081817627
Epoch 1150, val loss: 1.478033423423767
Epoch 1160, training loss: 0.6474337577819824 = 0.0050447904504835606 + 0.1 * 6.42388916015625
Epoch 1160, val loss: 1.4823040962219238
Epoch 1170, training loss: 0.6472756862640381 = 0.0049492777325212955 + 0.1 * 6.423264026641846
Epoch 1170, val loss: 1.4865498542785645
Epoch 1180, training loss: 0.6470820903778076 = 0.004856804385781288 + 0.1 * 6.422252655029297
Epoch 1180, val loss: 1.4907617568969727
Epoch 1190, training loss: 0.647443950176239 = 0.004767421167343855 + 0.1 * 6.426764965057373
Epoch 1190, val loss: 1.4949190616607666
Epoch 1200, training loss: 0.6479653716087341 = 0.004680574871599674 + 0.1 * 6.43284797668457
Epoch 1200, val loss: 1.4988837242126465
Epoch 1210, training loss: 0.646211564540863 = 0.004596595652401447 + 0.1 * 6.416149616241455
Epoch 1210, val loss: 1.5028657913208008
Epoch 1220, training loss: 0.6464121341705322 = 0.004515496082603931 + 0.1 * 6.418966293334961
Epoch 1220, val loss: 1.5068475008010864
Epoch 1230, training loss: 0.647363007068634 = 0.004436712246388197 + 0.1 * 6.429262638092041
Epoch 1230, val loss: 1.5107307434082031
Epoch 1240, training loss: 0.6458696126937866 = 0.004360179882496595 + 0.1 * 6.415093898773193
Epoch 1240, val loss: 1.5145518779754639
Epoch 1250, training loss: 0.6459635496139526 = 0.0042860540561378 + 0.1 * 6.416774749755859
Epoch 1250, val loss: 1.5183550119400024
Epoch 1260, training loss: 0.6457072496414185 = 0.004213995765894651 + 0.1 * 6.4149322509765625
Epoch 1260, val loss: 1.5221314430236816
Epoch 1270, training loss: 0.6446026563644409 = 0.004143939819186926 + 0.1 * 6.4045867919921875
Epoch 1270, val loss: 1.5258182287216187
Epoch 1280, training loss: 0.645579993724823 = 0.004076091106981039 + 0.1 * 6.4150390625
Epoch 1280, val loss: 1.5295456647872925
Epoch 1290, training loss: 0.6456058025360107 = 0.004009724128991365 + 0.1 * 6.415960788726807
Epoch 1290, val loss: 1.5330963134765625
Epoch 1300, training loss: 0.6439215540885925 = 0.003945622593164444 + 0.1 * 6.399758815765381
Epoch 1300, val loss: 1.536677598953247
Epoch 1310, training loss: 0.645128071308136 = 0.00388333760201931 + 0.1 * 6.412446975708008
Epoch 1310, val loss: 1.5402302742004395
Epoch 1320, training loss: 0.6444137096405029 = 0.003822617931291461 + 0.1 * 6.405910968780518
Epoch 1320, val loss: 1.5437082052230835
Epoch 1330, training loss: 0.6434106230735779 = 0.003763457527384162 + 0.1 * 6.396471977233887
Epoch 1330, val loss: 1.547108769416809
Epoch 1340, training loss: 0.6440063714981079 = 0.0037062494084239006 + 0.1 * 6.403000831604004
Epoch 1340, val loss: 1.5505311489105225
Epoch 1350, training loss: 0.6431888937950134 = 0.003650162136182189 + 0.1 * 6.395386695861816
Epoch 1350, val loss: 1.553882360458374
Epoch 1360, training loss: 0.644473135471344 = 0.0035961102694272995 + 0.1 * 6.4087700843811035
Epoch 1360, val loss: 1.5572733879089355
Epoch 1370, training loss: 0.6448308825492859 = 0.0035428935661911964 + 0.1 * 6.412879467010498
Epoch 1370, val loss: 1.5604777336120605
Epoch 1380, training loss: 0.6425581574440002 = 0.0034912663977593184 + 0.1 * 6.390668869018555
Epoch 1380, val loss: 1.5637353658676147
Epoch 1390, training loss: 0.6443036198616028 = 0.00344092957675457 + 0.1 * 6.408627033233643
Epoch 1390, val loss: 1.56693434715271
Epoch 1400, training loss: 0.6436141133308411 = 0.003391972044482827 + 0.1 * 6.402221202850342
Epoch 1400, val loss: 1.5701050758361816
Epoch 1410, training loss: 0.6425154209136963 = 0.003344121389091015 + 0.1 * 6.391712665557861
Epoch 1410, val loss: 1.5732022523880005
Epoch 1420, training loss: 0.6443725824356079 = 0.0032976591028273106 + 0.1 * 6.4107489585876465
Epoch 1420, val loss: 1.5763044357299805
Epoch 1430, training loss: 0.641838788986206 = 0.0032521646935492754 + 0.1 * 6.385866165161133
Epoch 1430, val loss: 1.5793558359146118
Epoch 1440, training loss: 0.6425127983093262 = 0.003207977395504713 + 0.1 * 6.393048286437988
Epoch 1440, val loss: 1.582406759262085
Epoch 1450, training loss: 0.642986536026001 = 0.0031648154836148024 + 0.1 * 6.39821720123291
Epoch 1450, val loss: 1.5854018926620483
Epoch 1460, training loss: 0.6417080760002136 = 0.003122547175735235 + 0.1 * 6.385854721069336
Epoch 1460, val loss: 1.5883299112319946
Epoch 1470, training loss: 0.6437748074531555 = 0.0030815491918474436 + 0.1 * 6.406932353973389
Epoch 1470, val loss: 1.5912359952926636
Epoch 1480, training loss: 0.6411941051483154 = 0.0030412666965276003 + 0.1 * 6.381528377532959
Epoch 1480, val loss: 1.594139575958252
Epoch 1490, training loss: 0.6429573893547058 = 0.003002161392942071 + 0.1 * 6.399552345275879
Epoch 1490, val loss: 1.5969918966293335
Epoch 1500, training loss: 0.6416225433349609 = 0.002963840961456299 + 0.1 * 6.386586666107178
Epoch 1500, val loss: 1.5998058319091797
Epoch 1510, training loss: 0.6420273184776306 = 0.002926609246060252 + 0.1 * 6.391006946563721
Epoch 1510, val loss: 1.6026438474655151
Epoch 1520, training loss: 0.6413416266441345 = 0.0028900366742163897 + 0.1 * 6.384515762329102
Epoch 1520, val loss: 1.6054095029830933
Epoch 1530, training loss: 0.6417715549468994 = 0.0028544452507048845 + 0.1 * 6.3891706466674805
Epoch 1530, val loss: 1.6081790924072266
Epoch 1540, training loss: 0.6418572068214417 = 0.0028195157647132874 + 0.1 * 6.390376567840576
Epoch 1540, val loss: 1.610903263092041
Epoch 1550, training loss: 0.6408527493476868 = 0.0027852654457092285 + 0.1 * 6.380674839019775
Epoch 1550, val loss: 1.6135441064834595
Epoch 1560, training loss: 0.6409515738487244 = 0.002751845633611083 + 0.1 * 6.381997585296631
Epoch 1560, val loss: 1.616184949874878
Epoch 1570, training loss: 0.6406639218330383 = 0.0027193212881684303 + 0.1 * 6.379445552825928
Epoch 1570, val loss: 1.6188164949417114
Epoch 1580, training loss: 0.640648603439331 = 0.0026874698232859373 + 0.1 * 6.379611492156982
Epoch 1580, val loss: 1.6214640140533447
Epoch 1590, training loss: 0.641766369342804 = 0.002656158758327365 + 0.1 * 6.391101837158203
Epoch 1590, val loss: 1.6240532398223877
Epoch 1600, training loss: 0.6409791707992554 = 0.0026255492120981216 + 0.1 * 6.383536338806152
Epoch 1600, val loss: 1.6265729665756226
Epoch 1610, training loss: 0.640073835849762 = 0.002595488913357258 + 0.1 * 6.374783515930176
Epoch 1610, val loss: 1.629040241241455
Epoch 1620, training loss: 0.6404722929000854 = 0.0025662502739578485 + 0.1 * 6.3790602684021
Epoch 1620, val loss: 1.6315255165100098
Epoch 1630, training loss: 0.6397958993911743 = 0.0025376672856509686 + 0.1 * 6.37258243560791
Epoch 1630, val loss: 1.6340128183364868
Epoch 1640, training loss: 0.641679048538208 = 0.0025095068849623203 + 0.1 * 6.391695022583008
Epoch 1640, val loss: 1.636414647102356
Epoch 1650, training loss: 0.6392863392829895 = 0.002482072915881872 + 0.1 * 6.368042945861816
Epoch 1650, val loss: 1.6388620138168335
Epoch 1660, training loss: 0.6395238041877747 = 0.0024552273098379374 + 0.1 * 6.370685577392578
Epoch 1660, val loss: 1.6412432193756104
Epoch 1670, training loss: 0.6411454081535339 = 0.0024287758860737085 + 0.1 * 6.3871660232543945
Epoch 1670, val loss: 1.6435545682907104
Epoch 1680, training loss: 0.6393303275108337 = 0.002402883255854249 + 0.1 * 6.369274139404297
Epoch 1680, val loss: 1.645899772644043
Epoch 1690, training loss: 0.6391269564628601 = 0.0023776490706950426 + 0.1 * 6.367493152618408
Epoch 1690, val loss: 1.648244023323059
Epoch 1700, training loss: 0.6392688751220703 = 0.0023527590092271566 + 0.1 * 6.3691606521606445
Epoch 1700, val loss: 1.6505557298660278
Epoch 1710, training loss: 0.6410470008850098 = 0.0023283774498850107 + 0.1 * 6.387186050415039
Epoch 1710, val loss: 1.6528549194335938
Epoch 1720, training loss: 0.6402459144592285 = 0.002304310444742441 + 0.1 * 6.379415988922119
Epoch 1720, val loss: 1.6550121307373047
Epoch 1730, training loss: 0.6391735672950745 = 0.00228085252456367 + 0.1 * 6.368927001953125
Epoch 1730, val loss: 1.6572428941726685
Epoch 1740, training loss: 0.6403791308403015 = 0.002257829299196601 + 0.1 * 6.381213188171387
Epoch 1740, val loss: 1.6594046354293823
Epoch 1750, training loss: 0.6390128135681152 = 0.002235373016446829 + 0.1 * 6.367774486541748
Epoch 1750, val loss: 1.6616164445877075
Epoch 1760, training loss: 0.6383699178695679 = 0.002213212428614497 + 0.1 * 6.361566543579102
Epoch 1760, val loss: 1.6637595891952515
Epoch 1770, training loss: 0.6400147080421448 = 0.0021915663965046406 + 0.1 * 6.378231525421143
Epoch 1770, val loss: 1.6659313440322876
Epoch 1780, training loss: 0.6381700038909912 = 0.002170168561860919 + 0.1 * 6.3599982261657715
Epoch 1780, val loss: 1.6679933071136475
Epoch 1790, training loss: 0.6390448212623596 = 0.002149289008229971 + 0.1 * 6.368955135345459
Epoch 1790, val loss: 1.6700999736785889
Epoch 1800, training loss: 0.638681948184967 = 0.002128683729097247 + 0.1 * 6.365532875061035
Epoch 1800, val loss: 1.6722018718719482
Epoch 1810, training loss: 0.6381449699401855 = 0.0021084174513816833 + 0.1 * 6.360365867614746
Epoch 1810, val loss: 1.6742267608642578
Epoch 1820, training loss: 0.6388756036758423 = 0.0020885991398245096 + 0.1 * 6.367869853973389
Epoch 1820, val loss: 1.6762471199035645
Epoch 1830, training loss: 0.6379756331443787 = 0.002069048583507538 + 0.1 * 6.359065532684326
Epoch 1830, val loss: 1.6782835721969604
Epoch 1840, training loss: 0.6391056180000305 = 0.002049898263067007 + 0.1 * 6.370556831359863
Epoch 1840, val loss: 1.6802830696105957
Epoch 1850, training loss: 0.6377450823783875 = 0.0020311763510107994 + 0.1 * 6.3571391105651855
Epoch 1850, val loss: 1.682286262512207
Epoch 1860, training loss: 0.6388640403747559 = 0.002012732205912471 + 0.1 * 6.368513107299805
Epoch 1860, val loss: 1.684261679649353
Epoch 1870, training loss: 0.6381995677947998 = 0.0019945173989981413 + 0.1 * 6.362050533294678
Epoch 1870, val loss: 1.6861997842788696
Epoch 1880, training loss: 0.637618899345398 = 0.0019767119083553553 + 0.1 * 6.35642147064209
Epoch 1880, val loss: 1.6881375312805176
Epoch 1890, training loss: 0.6384633183479309 = 0.001959216548129916 + 0.1 * 6.3650407791137695
Epoch 1890, val loss: 1.6900838613510132
Epoch 1900, training loss: 0.6371945142745972 = 0.0019420874305069447 + 0.1 * 6.352524280548096
Epoch 1900, val loss: 1.6919981241226196
Epoch 1910, training loss: 0.6374905705451965 = 0.0019251226913183928 + 0.1 * 6.355654716491699
Epoch 1910, val loss: 1.693909764289856
Epoch 1920, training loss: 0.6375992894172668 = 0.0019084261730313301 + 0.1 * 6.356908321380615
Epoch 1920, val loss: 1.6957826614379883
Epoch 1930, training loss: 0.6381292343139648 = 0.0018919920548796654 + 0.1 * 6.362372398376465
Epoch 1930, val loss: 1.6976234912872314
Epoch 1940, training loss: 0.6386544704437256 = 0.0018757735379040241 + 0.1 * 6.367786884307861
Epoch 1940, val loss: 1.6994740962982178
Epoch 1950, training loss: 0.6382109522819519 = 0.0018598821479827166 + 0.1 * 6.363510608673096
Epoch 1950, val loss: 1.7013152837753296
Epoch 1960, training loss: 0.637241780757904 = 0.001844280632212758 + 0.1 * 6.35397481918335
Epoch 1960, val loss: 1.7031992673873901
Epoch 1970, training loss: 0.6372599005699158 = 0.0018288753926753998 + 0.1 * 6.354310512542725
Epoch 1970, val loss: 1.7049970626831055
Epoch 1980, training loss: 0.6374439597129822 = 0.001813697163015604 + 0.1 * 6.356302261352539
Epoch 1980, val loss: 1.706785798072815
Epoch 1990, training loss: 0.6362268328666687 = 0.0017987674800679088 + 0.1 * 6.34428071975708
Epoch 1990, val loss: 1.7085801362991333
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 2.8095226287841797 = 1.949838399887085 + 0.1 * 8.596840858459473
Epoch 0, val loss: 1.9542617797851562
Epoch 10, training loss: 2.798375368118286 = 1.9387022256851196 + 0.1 * 8.596732139587402
Epoch 10, val loss: 1.94231116771698
Epoch 20, training loss: 2.7844343185424805 = 1.9248435497283936 + 0.1 * 8.595908164978027
Epoch 20, val loss: 1.9273104667663574
Epoch 30, training loss: 2.764346122741699 = 1.9054683446884155 + 0.1 * 8.588776588439941
Epoch 30, val loss: 1.9064276218414307
Epoch 40, training loss: 2.7326083183288574 = 1.8776774406433105 + 0.1 * 8.549307823181152
Epoch 40, val loss: 1.8770220279693604
Epoch 50, training loss: 2.6816420555114746 = 1.8417295217514038 + 0.1 * 8.399125099182129
Epoch 50, val loss: 1.8413090705871582
Epoch 60, training loss: 2.6201705932617188 = 1.8046002388000488 + 0.1 * 8.155704498291016
Epoch 60, val loss: 1.8082221746444702
Epoch 70, training loss: 2.5725345611572266 = 1.7718172073364258 + 0.1 * 8.007173538208008
Epoch 70, val loss: 1.7816015481948853
Epoch 80, training loss: 2.501811981201172 = 1.7343124151229858 + 0.1 * 7.674994945526123
Epoch 80, val loss: 1.7491130828857422
Epoch 90, training loss: 2.4201531410217285 = 1.6870856285095215 + 0.1 * 7.33067512512207
Epoch 90, val loss: 1.706534743309021
Epoch 100, training loss: 2.341212272644043 = 1.6256399154663086 + 0.1 * 7.155723571777344
Epoch 100, val loss: 1.6522585153579712
Epoch 110, training loss: 2.255080461502075 = 1.5474376678466797 + 0.1 * 7.076426982879639
Epoch 110, val loss: 1.5856159925460815
Epoch 120, training loss: 2.1595375537872314 = 1.4568657875061035 + 0.1 * 7.026717662811279
Epoch 120, val loss: 1.5104867219924927
Epoch 130, training loss: 2.061342716217041 = 1.3621011972427368 + 0.1 * 6.992414474487305
Epoch 130, val loss: 1.4339021444320679
Epoch 140, training loss: 1.9640977382659912 = 1.267457127571106 + 0.1 * 6.966405868530273
Epoch 140, val loss: 1.3587449789047241
Epoch 150, training loss: 1.8685016632080078 = 1.173484206199646 + 0.1 * 6.950174808502197
Epoch 150, val loss: 1.2845467329025269
Epoch 160, training loss: 1.775752305984497 = 1.0816211700439453 + 0.1 * 6.941310882568359
Epoch 160, val loss: 1.2121076583862305
Epoch 170, training loss: 1.6882712841033936 = 0.9945358037948608 + 0.1 * 6.9373555183410645
Epoch 170, val loss: 1.1444271802902222
Epoch 180, training loss: 1.607234001159668 = 0.9139207005500793 + 0.1 * 6.933133602142334
Epoch 180, val loss: 1.082657814025879
Epoch 190, training loss: 1.5319585800170898 = 0.8391629457473755 + 0.1 * 6.927956581115723
Epoch 190, val loss: 1.0258268117904663
Epoch 200, training loss: 1.4627262353897095 = 0.7705716490745544 + 0.1 * 6.92154598236084
Epoch 200, val loss: 0.9746023416519165
Epoch 210, training loss: 1.4004710912704468 = 0.708758533000946 + 0.1 * 6.917125701904297
Epoch 210, val loss: 0.9300736784934998
Epoch 220, training loss: 1.3445639610290527 = 0.6541019678115845 + 0.1 * 6.90462064743042
Epoch 220, val loss: 0.8931854367256165
Epoch 230, training loss: 1.2941439151763916 = 0.6048515439033508 + 0.1 * 6.892922878265381
Epoch 230, val loss: 0.8629685640335083
Epoch 240, training loss: 1.2472336292266846 = 0.5592483878135681 + 0.1 * 6.879852294921875
Epoch 240, val loss: 0.8379822969436646
Epoch 250, training loss: 1.2033610343933105 = 0.5165831446647644 + 0.1 * 6.867778778076172
Epoch 250, val loss: 0.8173121213912964
Epoch 260, training loss: 1.1611160039901733 = 0.4763173460960388 + 0.1 * 6.847986221313477
Epoch 260, val loss: 0.8000661730766296
Epoch 270, training loss: 1.1207480430603027 = 0.4375263452529907 + 0.1 * 6.832217693328857
Epoch 270, val loss: 0.7853641510009766
Epoch 280, training loss: 1.0823874473571777 = 0.4001038372516632 + 0.1 * 6.822835445404053
Epoch 280, val loss: 0.7730949521064758
Epoch 290, training loss: 1.0452077388763428 = 0.36411991715431213 + 0.1 * 6.810877799987793
Epoch 290, val loss: 0.763388454914093
Epoch 300, training loss: 1.009360432624817 = 0.3293727934360504 + 0.1 * 6.799876689910889
Epoch 300, val loss: 0.7562926411628723
Epoch 310, training loss: 0.9760985374450684 = 0.29600340127944946 + 0.1 * 6.8009514808654785
Epoch 310, val loss: 0.7519077658653259
Epoch 320, training loss: 0.9434549808502197 = 0.26446422934532166 + 0.1 * 6.789907455444336
Epoch 320, val loss: 0.7500188946723938
Epoch 330, training loss: 0.9132291078567505 = 0.23495334386825562 + 0.1 * 6.78275728225708
Epoch 330, val loss: 0.7506091594696045
Epoch 340, training loss: 0.8854478597640991 = 0.20763543248176575 + 0.1 * 6.77812385559082
Epoch 340, val loss: 0.7538662552833557
Epoch 350, training loss: 0.8600523471832275 = 0.1829739362001419 + 0.1 * 6.7707839012146
Epoch 350, val loss: 0.7596951127052307
Epoch 360, training loss: 0.8372423052787781 = 0.16118137538433075 + 0.1 * 6.760609149932861
Epoch 360, val loss: 0.768204927444458
Epoch 370, training loss: 0.8176271915435791 = 0.14231184124946594 + 0.1 * 6.753153324127197
Epoch 370, val loss: 0.778994619846344
Epoch 380, training loss: 0.8008078932762146 = 0.12617157399654388 + 0.1 * 6.746363162994385
Epoch 380, val loss: 0.7916218638420105
Epoch 390, training loss: 0.7870482206344604 = 0.11245878785848618 + 0.1 * 6.745894432067871
Epoch 390, val loss: 0.8056175112724304
Epoch 400, training loss: 0.7737647294998169 = 0.10080628842115402 + 0.1 * 6.729584217071533
Epoch 400, val loss: 0.8204329609870911
Epoch 410, training loss: 0.7637978196144104 = 0.09080100059509277 + 0.1 * 6.729968070983887
Epoch 410, val loss: 0.83589768409729
Epoch 420, training loss: 0.7544305920600891 = 0.08220424503087997 + 0.1 * 6.722263813018799
Epoch 420, val loss: 0.8515782952308655
Epoch 430, training loss: 0.7459172606468201 = 0.07475675642490387 + 0.1 * 6.711605072021484
Epoch 430, val loss: 0.8673475384712219
Epoch 440, training loss: 0.7410597801208496 = 0.06824576109647751 + 0.1 * 6.728139877319336
Epoch 440, val loss: 0.8831568956375122
Epoch 450, training loss: 0.7326474785804749 = 0.06255938112735748 + 0.1 * 6.700881004333496
Epoch 450, val loss: 0.8987612128257751
Epoch 460, training loss: 0.727087676525116 = 0.057541605085134506 + 0.1 * 6.695460796356201
Epoch 460, val loss: 0.9141474962234497
Epoch 470, training loss: 0.7224093079566956 = 0.053086843341588974 + 0.1 * 6.693224906921387
Epoch 470, val loss: 0.929348349571228
Epoch 480, training loss: 0.7177748680114746 = 0.04911546781659126 + 0.1 * 6.686594009399414
Epoch 480, val loss: 0.9442715048789978
Epoch 490, training loss: 0.7135645151138306 = 0.04555323347449303 + 0.1 * 6.680112838745117
Epoch 490, val loss: 0.95899498462677
Epoch 500, training loss: 0.7097100019454956 = 0.04235094040632248 + 0.1 * 6.673590660095215
Epoch 500, val loss: 0.973429799079895
Epoch 510, training loss: 0.7070406675338745 = 0.03947119787335396 + 0.1 * 6.675694465637207
Epoch 510, val loss: 0.9874589443206787
Epoch 520, training loss: 0.7032524347305298 = 0.036868516355752945 + 0.1 * 6.663838863372803
Epoch 520, val loss: 1.0011894702911377
Epoch 530, training loss: 0.7010478377342224 = 0.03450705483555794 + 0.1 * 6.665407657623291
Epoch 530, val loss: 1.0146147012710571
Epoch 540, training loss: 0.6976499557495117 = 0.032361894845962524 + 0.1 * 6.6528801918029785
Epoch 540, val loss: 1.0276843309402466
Epoch 550, training loss: 0.6972891688346863 = 0.03040299564599991 + 0.1 * 6.668861389160156
Epoch 550, val loss: 1.0405055284500122
Epoch 560, training loss: 0.6937043070793152 = 0.028615403920412064 + 0.1 * 6.650888919830322
Epoch 560, val loss: 1.0529931783676147
Epoch 570, training loss: 0.6934263110160828 = 0.026973584666848183 + 0.1 * 6.66452693939209
Epoch 570, val loss: 1.0651675462722778
Epoch 580, training loss: 0.6900632977485657 = 0.025465209037065506 + 0.1 * 6.6459808349609375
Epoch 580, val loss: 1.0771536827087402
Epoch 590, training loss: 0.6886641979217529 = 0.02407371625304222 + 0.1 * 6.645904541015625
Epoch 590, val loss: 1.0888333320617676
Epoch 600, training loss: 0.6863442063331604 = 0.022790558636188507 + 0.1 * 6.6355366706848145
Epoch 600, val loss: 1.100324034690857
Epoch 610, training loss: 0.6855843663215637 = 0.02160598896443844 + 0.1 * 6.63978385925293
Epoch 610, val loss: 1.1114732027053833
Epoch 620, training loss: 0.68331378698349 = 0.020512333139777184 + 0.1 * 6.628014087677002
Epoch 620, val loss: 1.1223671436309814
Epoch 630, training loss: 0.6846277713775635 = 0.019496500492095947 + 0.1 * 6.651312351226807
Epoch 630, val loss: 1.1330358982086182
Epoch 640, training loss: 0.6792240738868713 = 0.018557528033852577 + 0.1 * 6.606665134429932
Epoch 640, val loss: 1.1434522867202759
Epoch 650, training loss: 0.6800702810287476 = 0.017684193328022957 + 0.1 * 6.6238603591918945
Epoch 650, val loss: 1.1536098718643188
Epoch 660, training loss: 0.6786004304885864 = 0.016875317320227623 + 0.1 * 6.617250919342041
Epoch 660, val loss: 1.163487195968628
Epoch 670, training loss: 0.6768801808357239 = 0.016123568639159203 + 0.1 * 6.607565879821777
Epoch 670, val loss: 1.1730544567108154
Epoch 680, training loss: 0.6749460697174072 = 0.015422272495925426 + 0.1 * 6.595237731933594
Epoch 680, val loss: 1.1824167966842651
Epoch 690, training loss: 0.6734166741371155 = 0.01476652454584837 + 0.1 * 6.586501598358154
Epoch 690, val loss: 1.1916135549545288
Epoch 700, training loss: 0.6726052165031433 = 0.014152279123663902 + 0.1 * 6.584528923034668
Epoch 700, val loss: 1.2005828619003296
Epoch 710, training loss: 0.6723871827125549 = 0.013579798862338066 + 0.1 * 6.58807373046875
Epoch 710, val loss: 1.2093307971954346
Epoch 720, training loss: 0.6722056865692139 = 0.013041680678725243 + 0.1 * 6.591639995574951
Epoch 720, val loss: 1.2178574800491333
Epoch 730, training loss: 0.6701086163520813 = 0.012540201656520367 + 0.1 * 6.57568359375
Epoch 730, val loss: 1.2261581420898438
Epoch 740, training loss: 0.6701532602310181 = 0.01206898596137762 + 0.1 * 6.5808424949646
Epoch 740, val loss: 1.2341866493225098
Epoch 750, training loss: 0.6686584949493408 = 0.011626088060438633 + 0.1 * 6.570323944091797
Epoch 750, val loss: 1.242056965827942
Epoch 760, training loss: 0.6667232513427734 = 0.01120789721608162 + 0.1 * 6.5551533699035645
Epoch 760, val loss: 1.2497930526733398
Epoch 770, training loss: 0.6659047603607178 = 0.01081222016364336 + 0.1 * 6.550925254821777
Epoch 770, val loss: 1.257365107536316
Epoch 780, training loss: 0.6662224531173706 = 0.010439298115670681 + 0.1 * 6.557831764221191
Epoch 780, val loss: 1.2648046016693115
Epoch 790, training loss: 0.6652161478996277 = 0.01008724607527256 + 0.1 * 6.551289081573486
Epoch 790, val loss: 1.2720059156417847
Epoch 800, training loss: 0.6641681790351868 = 0.009754552505910397 + 0.1 * 6.5441365242004395
Epoch 800, val loss: 1.2790929079055786
Epoch 810, training loss: 0.6641690135002136 = 0.009438790380954742 + 0.1 * 6.547301769256592
Epoch 810, val loss: 1.28602933883667
Epoch 820, training loss: 0.6632782816886902 = 0.009138572961091995 + 0.1 * 6.5413970947265625
Epoch 820, val loss: 1.2928444147109985
Epoch 830, training loss: 0.6615691781044006 = 0.008855142630636692 + 0.1 * 6.527140140533447
Epoch 830, val loss: 1.2995268106460571
Epoch 840, training loss: 0.66302889585495 = 0.008585343137383461 + 0.1 * 6.544435501098633
Epoch 840, val loss: 1.3060249090194702
Epoch 850, training loss: 0.6615568399429321 = 0.00832886528223753 + 0.1 * 6.532279968261719
Epoch 850, val loss: 1.3123681545257568
Epoch 860, training loss: 0.6620414853096008 = 0.008085194043815136 + 0.1 * 6.539562702178955
Epoch 860, val loss: 1.3186345100402832
Epoch 870, training loss: 0.6607454419136047 = 0.007852728478610516 + 0.1 * 6.528926849365234
Epoch 870, val loss: 1.3246769905090332
Epoch 880, training loss: 0.6588237285614014 = 0.007631929591298103 + 0.1 * 6.511917591094971
Epoch 880, val loss: 1.3306738138198853
Epoch 890, training loss: 0.6589412689208984 = 0.007420496549457312 + 0.1 * 6.515207767486572
Epoch 890, val loss: 1.3364616632461548
Epoch 900, training loss: 0.6577091813087463 = 0.007218973245471716 + 0.1 * 6.504902362823486
Epoch 900, val loss: 1.34217369556427
Epoch 910, training loss: 0.6616722345352173 = 0.007026186212897301 + 0.1 * 6.546460151672363
Epoch 910, val loss: 1.34782075881958
Epoch 920, training loss: 0.6576707363128662 = 0.006841919384896755 + 0.1 * 6.508288383483887
Epoch 920, val loss: 1.3532824516296387
Epoch 930, training loss: 0.6566784977912903 = 0.00666580256074667 + 0.1 * 6.500126838684082
Epoch 930, val loss: 1.358704924583435
Epoch 940, training loss: 0.656292200088501 = 0.006496717222034931 + 0.1 * 6.497954368591309
Epoch 940, val loss: 1.363966941833496
Epoch 950, training loss: 0.656018853187561 = 0.006335444748401642 + 0.1 * 6.496833801269531
Epoch 950, val loss: 1.369158387184143
Epoch 960, training loss: 0.654909074306488 = 0.006180785596370697 + 0.1 * 6.487282752990723
Epoch 960, val loss: 1.3742148876190186
Epoch 970, training loss: 0.6557839512825012 = 0.006032075732946396 + 0.1 * 6.497519016265869
Epoch 970, val loss: 1.3791632652282715
Epoch 980, training loss: 0.6557824015617371 = 0.005889734253287315 + 0.1 * 6.498926639556885
Epoch 980, val loss: 1.384067416191101
Epoch 990, training loss: 0.6540168523788452 = 0.005752834025770426 + 0.1 * 6.482639789581299
Epoch 990, val loss: 1.3888798952102661
Epoch 1000, training loss: 0.6537836194038391 = 0.005621337331831455 + 0.1 * 6.481622695922852
Epoch 1000, val loss: 1.3935952186584473
Epoch 1010, training loss: 0.6533294916152954 = 0.005494897719472647 + 0.1 * 6.47834587097168
Epoch 1010, val loss: 1.398179531097412
Epoch 1020, training loss: 0.652557373046875 = 0.005373193416744471 + 0.1 * 6.471841812133789
Epoch 1020, val loss: 1.402719259262085
Epoch 1030, training loss: 0.6522864103317261 = 0.005255425814539194 + 0.1 * 6.470309734344482
Epoch 1030, val loss: 1.4071565866470337
Epoch 1040, training loss: 0.6519324779510498 = 0.005142975598573685 + 0.1 * 6.467894554138184
Epoch 1040, val loss: 1.4115952253341675
Epoch 1050, training loss: 0.6516475081443787 = 0.005034090019762516 + 0.1 * 6.466134071350098
Epoch 1050, val loss: 1.4158989191055298
Epoch 1060, training loss: 0.6519705057144165 = 0.004928768612444401 + 0.1 * 6.470417499542236
Epoch 1060, val loss: 1.4201184511184692
Epoch 1070, training loss: 0.6506865620613098 = 0.004827919881790876 + 0.1 * 6.4585862159729
Epoch 1070, val loss: 1.4242932796478271
Epoch 1080, training loss: 0.6526881456375122 = 0.004730566870421171 + 0.1 * 6.4795756340026855
Epoch 1080, val loss: 1.4283915758132935
Epoch 1090, training loss: 0.6506081819534302 = 0.004635872319340706 + 0.1 * 6.459722995758057
Epoch 1090, val loss: 1.4323787689208984
Epoch 1100, training loss: 0.6509082317352295 = 0.0045450287871062756 + 0.1 * 6.463632106781006
Epoch 1100, val loss: 1.436396837234497
Epoch 1110, training loss: 0.6492763757705688 = 0.004456670489162207 + 0.1 * 6.4481964111328125
Epoch 1110, val loss: 1.4403218030929565
Epoch 1120, training loss: 0.6501100659370422 = 0.004371307790279388 + 0.1 * 6.457387447357178
Epoch 1120, val loss: 1.4442265033721924
Epoch 1130, training loss: 0.6500505208969116 = 0.004289115313440561 + 0.1 * 6.457614421844482
Epoch 1130, val loss: 1.4480226039886475
Epoch 1140, training loss: 0.6490654945373535 = 0.004209582693874836 + 0.1 * 6.448559284210205
Epoch 1140, val loss: 1.4518150091171265
Epoch 1150, training loss: 0.6484707593917847 = 0.004131951369345188 + 0.1 * 6.443387985229492
Epoch 1150, val loss: 1.455491542816162
Epoch 1160, training loss: 0.6483880281448364 = 0.004057277925312519 + 0.1 * 6.443307399749756
Epoch 1160, val loss: 1.4591681957244873
Epoch 1170, training loss: 0.648909330368042 = 0.003984768874943256 + 0.1 * 6.449245929718018
Epoch 1170, val loss: 1.4628124237060547
Epoch 1180, training loss: 0.6487059593200684 = 0.003914072643965483 + 0.1 * 6.447918891906738
Epoch 1180, val loss: 1.466371774673462
Epoch 1190, training loss: 0.6490602493286133 = 0.003846116131171584 + 0.1 * 6.452141284942627
Epoch 1190, val loss: 1.469907283782959
Epoch 1200, training loss: 0.6469026207923889 = 0.003780137514695525 + 0.1 * 6.431224822998047
Epoch 1200, val loss: 1.4733699560165405
Epoch 1210, training loss: 0.6492018103599548 = 0.0037160126958042383 + 0.1 * 6.45485782623291
Epoch 1210, val loss: 1.4767827987670898
Epoch 1220, training loss: 0.6474337577819824 = 0.0036534792743623257 + 0.1 * 6.437802791595459
Epoch 1220, val loss: 1.4801452159881592
Epoch 1230, training loss: 0.6481388211250305 = 0.0035931498277932405 + 0.1 * 6.445456504821777
Epoch 1230, val loss: 1.4834953546524048
Epoch 1240, training loss: 0.6465675830841064 = 0.00353450421243906 + 0.1 * 6.430330753326416
Epoch 1240, val loss: 1.4868062734603882
Epoch 1250, training loss: 0.6469495296478271 = 0.0034774986561387777 + 0.1 * 6.434720039367676
Epoch 1250, val loss: 1.490049123764038
Epoch 1260, training loss: 0.6471842527389526 = 0.0034219904337078333 + 0.1 * 6.437623023986816
Epoch 1260, val loss: 1.493265986442566
Epoch 1270, training loss: 0.6455763578414917 = 0.003368235658854246 + 0.1 * 6.422080993652344
Epoch 1270, val loss: 1.496430516242981
Epoch 1280, training loss: 0.648178219795227 = 0.003315812675282359 + 0.1 * 6.448624134063721
Epoch 1280, val loss: 1.4995687007904053
Epoch 1290, training loss: 0.6461796164512634 = 0.0032646977342665195 + 0.1 * 6.4291486740112305
Epoch 1290, val loss: 1.502699613571167
Epoch 1300, training loss: 0.6465036273002625 = 0.0032149872276932 + 0.1 * 6.432886123657227
Epoch 1300, val loss: 1.5057672262191772
Epoch 1310, training loss: 0.6463431119918823 = 0.003166760317981243 + 0.1 * 6.431763648986816
Epoch 1310, val loss: 1.508815050125122
Epoch 1320, training loss: 0.6449065208435059 = 0.0031195704359561205 + 0.1 * 6.4178690910339355
Epoch 1320, val loss: 1.5117851495742798
Epoch 1330, training loss: 0.6456610560417175 = 0.003073891391977668 + 0.1 * 6.425871849060059
Epoch 1330, val loss: 1.5147541761398315
Epoch 1340, training loss: 0.6447879672050476 = 0.0030291357543319464 + 0.1 * 6.417588710784912
Epoch 1340, val loss: 1.5176931619644165
Epoch 1350, training loss: 0.6451035141944885 = 0.002985593630000949 + 0.1 * 6.421178817749023
Epoch 1350, val loss: 1.520596981048584
Epoch 1360, training loss: 0.6449995636940002 = 0.00294318119995296 + 0.1 * 6.4205641746521
Epoch 1360, val loss: 1.5234581232070923
Epoch 1370, training loss: 0.6444187164306641 = 0.002902072621509433 + 0.1 * 6.415165901184082
Epoch 1370, val loss: 1.5262848138809204
Epoch 1380, training loss: 0.6454420685768127 = 0.0028619254007935524 + 0.1 * 6.4258012771606445
Epoch 1380, val loss: 1.5290766954421997
Epoch 1390, training loss: 0.6457388401031494 = 0.0028225486166775227 + 0.1 * 6.429162502288818
Epoch 1390, val loss: 1.5318036079406738
Epoch 1400, training loss: 0.6433130502700806 = 0.002784407464787364 + 0.1 * 6.4052863121032715
Epoch 1400, val loss: 1.534566879272461
Epoch 1410, training loss: 0.6464619040489197 = 0.0027471655048429966 + 0.1 * 6.43714714050293
Epoch 1410, val loss: 1.5373109579086304
Epoch 1420, training loss: 0.6446320414543152 = 0.00271045695990324 + 0.1 * 6.419215679168701
Epoch 1420, val loss: 1.5399519205093384
Epoch 1430, training loss: 0.6450279951095581 = 0.0026748983655124903 + 0.1 * 6.4235310554504395
Epoch 1430, val loss: 1.542638897895813
Epoch 1440, training loss: 0.6438117027282715 = 0.002640146529302001 + 0.1 * 6.411715507507324
Epoch 1440, val loss: 1.5452656745910645
Epoch 1450, training loss: 0.6467755436897278 = 0.0026061951648443937 + 0.1 * 6.441693305969238
Epoch 1450, val loss: 1.5478440523147583
Epoch 1460, training loss: 0.6438791155815125 = 0.002573134144768119 + 0.1 * 6.413059711456299
Epoch 1460, val loss: 1.5504119396209717
Epoch 1470, training loss: 0.6429380774497986 = 0.002540813060477376 + 0.1 * 6.403972148895264
Epoch 1470, val loss: 1.5529316663742065
Epoch 1480, training loss: 0.6436935663223267 = 0.002509147860109806 + 0.1 * 6.411843776702881
Epoch 1480, val loss: 1.5554542541503906
Epoch 1490, training loss: 0.6427385807037354 = 0.00247827242128551 + 0.1 * 6.402602672576904
Epoch 1490, val loss: 1.5579276084899902
Epoch 1500, training loss: 0.64328533411026 = 0.002447956707328558 + 0.1 * 6.4083733558654785
Epoch 1500, val loss: 1.560407280921936
Epoch 1510, training loss: 0.6427199244499207 = 0.0024183345958590508 + 0.1 * 6.403015613555908
Epoch 1510, val loss: 1.5628492832183838
Epoch 1520, training loss: 0.6440455317497253 = 0.0023894256446510553 + 0.1 * 6.416560649871826
Epoch 1520, val loss: 1.5653132200241089
Epoch 1530, training loss: 0.6434023380279541 = 0.0023611944634467363 + 0.1 * 6.410411357879639
Epoch 1530, val loss: 1.5676761865615845
Epoch 1540, training loss: 0.6419140100479126 = 0.0023336296435445547 + 0.1 * 6.395803451538086
Epoch 1540, val loss: 1.5700701475143433
Epoch 1550, training loss: 0.6442299485206604 = 0.002306574722751975 + 0.1 * 6.419233798980713
Epoch 1550, val loss: 1.5723973512649536
Epoch 1560, training loss: 0.6418375372886658 = 0.0022799719590693712 + 0.1 * 6.395575523376465
Epoch 1560, val loss: 1.5746817588806152
Epoch 1570, training loss: 0.6427043080329895 = 0.002254169201478362 + 0.1 * 6.404500961303711
Epoch 1570, val loss: 1.5769981145858765
Epoch 1580, training loss: 0.6422234177589417 = 0.0022287776228040457 + 0.1 * 6.399946212768555
Epoch 1580, val loss: 1.5792584419250488
Epoch 1590, training loss: 0.6419261693954468 = 0.0022038831375539303 + 0.1 * 6.397222995758057
Epoch 1590, val loss: 1.581504225730896
Epoch 1600, training loss: 0.6432495713233948 = 0.0021795372013002634 + 0.1 * 6.410699844360352
Epoch 1600, val loss: 1.5837217569351196
Epoch 1610, training loss: 0.6414813995361328 = 0.0021558008156716824 + 0.1 * 6.393256187438965
Epoch 1610, val loss: 1.585952877998352
Epoch 1620, training loss: 0.6421278715133667 = 0.002132505876943469 + 0.1 * 6.399953365325928
Epoch 1620, val loss: 1.588110327720642
Epoch 1630, training loss: 0.6412467956542969 = 0.0021095790434628725 + 0.1 * 6.391372203826904
Epoch 1630, val loss: 1.5902458429336548
Epoch 1640, training loss: 0.6426191926002502 = 0.002087176777422428 + 0.1 * 6.405320167541504
Epoch 1640, val loss: 1.5923717021942139
Epoch 1650, training loss: 0.6418389678001404 = 0.002065244596451521 + 0.1 * 6.3977370262146
Epoch 1650, val loss: 1.5945333242416382
Epoch 1660, training loss: 0.6416354179382324 = 0.0020437755156308413 + 0.1 * 6.395915985107422
Epoch 1660, val loss: 1.5966333150863647
Epoch 1670, training loss: 0.6401358842849731 = 0.0020226892083883286 + 0.1 * 6.381131649017334
Epoch 1670, val loss: 1.59870445728302
Epoch 1680, training loss: 0.6416072845458984 = 0.0020020906813442707 + 0.1 * 6.396051406860352
Epoch 1680, val loss: 1.6007461547851562
Epoch 1690, training loss: 0.6418479084968567 = 0.0019817573484033346 + 0.1 * 6.3986616134643555
Epoch 1690, val loss: 1.6027907133102417
Epoch 1700, training loss: 0.6402600407600403 = 0.001961864298209548 + 0.1 * 6.382981300354004
Epoch 1700, val loss: 1.6048345565795898
Epoch 1710, training loss: 0.641826868057251 = 0.0019423963967710733 + 0.1 * 6.3988447189331055
Epoch 1710, val loss: 1.606835126876831
Epoch 1720, training loss: 0.6404350399971008 = 0.0019232375780120492 + 0.1 * 6.385117530822754
Epoch 1720, val loss: 1.6088311672210693
Epoch 1730, training loss: 0.6404925584793091 = 0.0019044700311496854 + 0.1 * 6.385880470275879
Epoch 1730, val loss: 1.6108115911483765
Epoch 1740, training loss: 0.640906035900116 = 0.0018858796684071422 + 0.1 * 6.390201568603516
Epoch 1740, val loss: 1.6127891540527344
Epoch 1750, training loss: 0.6405245661735535 = 0.00186784821562469 + 0.1 * 6.386567115783691
Epoch 1750, val loss: 1.6147090196609497
Epoch 1760, training loss: 0.6409363150596619 = 0.00185010873246938 + 0.1 * 6.390861511230469
Epoch 1760, val loss: 1.616637110710144
Epoch 1770, training loss: 0.6398062109947205 = 0.0018326742574572563 + 0.1 * 6.379735469818115
Epoch 1770, val loss: 1.6185276508331299
Epoch 1780, training loss: 0.64032381772995 = 0.0018155741272494197 + 0.1 * 6.385082244873047
Epoch 1780, val loss: 1.6203840970993042
Epoch 1790, training loss: 0.639162540435791 = 0.0017987190512940288 + 0.1 * 6.373638153076172
Epoch 1790, val loss: 1.622255563735962
Epoch 1800, training loss: 0.6398789286613464 = 0.00178227957803756 + 0.1 * 6.3809661865234375
Epoch 1800, val loss: 1.6240921020507812
Epoch 1810, training loss: 0.6394177079200745 = 0.001766037312336266 + 0.1 * 6.376516819000244
Epoch 1810, val loss: 1.6259199380874634
Epoch 1820, training loss: 0.6410061717033386 = 0.0017501120455563068 + 0.1 * 6.3925604820251465
Epoch 1820, val loss: 1.6277607679367065
Epoch 1830, training loss: 0.6391832232475281 = 0.0017345647793263197 + 0.1 * 6.374486446380615
Epoch 1830, val loss: 1.6295167207717896
Epoch 1840, training loss: 0.6402183771133423 = 0.001719185383990407 + 0.1 * 6.3849921226501465
Epoch 1840, val loss: 1.631274700164795
Epoch 1850, training loss: 0.6389127373695374 = 0.0017040739767253399 + 0.1 * 6.372086048126221
Epoch 1850, val loss: 1.6330177783966064
Epoch 1860, training loss: 0.6399462223052979 = 0.0016892199637368321 + 0.1 * 6.382570266723633
Epoch 1860, val loss: 1.6347832679748535
Epoch 1870, training loss: 0.6386038064956665 = 0.0016746069304645061 + 0.1 * 6.36929178237915
Epoch 1870, val loss: 1.6365163326263428
Epoch 1880, training loss: 0.6428008079528809 = 0.0016602632822468877 + 0.1 * 6.411405086517334
Epoch 1880, val loss: 1.6382282972335815
Epoch 1890, training loss: 0.6393853425979614 = 0.0016462019411846995 + 0.1 * 6.3773908615112305
Epoch 1890, val loss: 1.6399919986724854
Epoch 1900, training loss: 0.6383442878723145 = 0.0016324486350640655 + 0.1 * 6.3671183586120605
Epoch 1900, val loss: 1.6416743993759155
Epoch 1910, training loss: 0.6386311650276184 = 0.001618861104361713 + 0.1 * 6.370122909545898
Epoch 1910, val loss: 1.6432852745056152
Epoch 1920, training loss: 0.6405477523803711 = 0.00160537613555789 + 0.1 * 6.389423370361328
Epoch 1920, val loss: 1.6449627876281738
Epoch 1930, training loss: 0.6390136480331421 = 0.0015922713791951537 + 0.1 * 6.374213695526123
Epoch 1930, val loss: 1.6466072797775269
Epoch 1940, training loss: 0.639983057975769 = 0.0015793655766174197 + 0.1 * 6.384037017822266
Epoch 1940, val loss: 1.648208737373352
Epoch 1950, training loss: 0.6381377577781677 = 0.0015666013350710273 + 0.1 * 6.365711688995361
Epoch 1950, val loss: 1.649858832359314
Epoch 1960, training loss: 0.6410127282142639 = 0.0015540779568254948 + 0.1 * 6.394586563110352
Epoch 1960, val loss: 1.6514424085617065
Epoch 1970, training loss: 0.6385984420776367 = 0.0015416862443089485 + 0.1 * 6.370567321777344
Epoch 1970, val loss: 1.6530156135559082
Epoch 1980, training loss: 0.6386920809745789 = 0.0015295591438189149 + 0.1 * 6.371624946594238
Epoch 1980, val loss: 1.65461003780365
Epoch 1990, training loss: 0.6383585333824158 = 0.0015175997978076339 + 0.1 * 6.368409156799316
Epoch 1990, val loss: 1.6561765670776367
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8376383763837639
The final CL Acc:0.80123, 0.01222, The final GNN Acc:0.83975, 0.00375
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10500])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.811513900756836 = 1.9518343210220337 + 0.1 * 8.596796035766602
Epoch 0, val loss: 1.945704460144043
Epoch 10, training loss: 2.801168918609619 = 1.941502332687378 + 0.1 * 8.596665382385254
Epoch 10, val loss: 1.9358421564102173
Epoch 20, training loss: 2.788466215133667 = 1.9289000034332275 + 0.1 * 8.595661163330078
Epoch 20, val loss: 1.923445224761963
Epoch 30, training loss: 2.7702512741088867 = 1.9116030931472778 + 0.1 * 8.586481094360352
Epoch 30, val loss: 1.9060028791427612
Epoch 40, training loss: 2.740326404571533 = 1.8866848945617676 + 0.1 * 8.53641414642334
Epoch 40, val loss: 1.8809653520584106
Epoch 50, training loss: 2.6848738193511963 = 1.8539714813232422 + 0.1 * 8.309022903442383
Epoch 50, val loss: 1.8497579097747803
Epoch 60, training loss: 2.6236982345581055 = 1.8173421621322632 + 0.1 * 8.063560485839844
Epoch 60, val loss: 1.817609190940857
Epoch 70, training loss: 2.557201385498047 = 1.7848119735717773 + 0.1 * 7.723893642425537
Epoch 70, val loss: 1.7910131216049194
Epoch 80, training loss: 2.4917564392089844 = 1.7531991004943848 + 0.1 * 7.3855719566345215
Epoch 80, val loss: 1.764336109161377
Epoch 90, training loss: 2.4368531703948975 = 1.7135826349258423 + 0.1 * 7.2327046394348145
Epoch 90, val loss: 1.729172945022583
Epoch 100, training loss: 2.374678373336792 = 1.6609680652618408 + 0.1 * 7.137103080749512
Epoch 100, val loss: 1.6828863620758057
Epoch 110, training loss: 2.3001720905303955 = 1.5920583009719849 + 0.1 * 7.081137657165527
Epoch 110, val loss: 1.6247613430023193
Epoch 120, training loss: 2.2137770652770996 = 1.5090198516845703 + 0.1 * 7.047573089599609
Epoch 120, val loss: 1.5568007230758667
Epoch 130, training loss: 2.121304512023926 = 1.419634461402893 + 0.1 * 7.016700744628906
Epoch 130, val loss: 1.4852629899978638
Epoch 140, training loss: 2.0267043113708496 = 1.3280783891677856 + 0.1 * 6.986259460449219
Epoch 140, val loss: 1.4131793975830078
Epoch 150, training loss: 1.9299229383468628 = 1.2341771125793457 + 0.1 * 6.957458019256592
Epoch 150, val loss: 1.3404487371444702
Epoch 160, training loss: 1.8350598812103271 = 1.1407828330993652 + 0.1 * 6.942770004272461
Epoch 160, val loss: 1.2699016332626343
Epoch 170, training loss: 1.7440009117126465 = 1.0512269735336304 + 0.1 * 6.927740097045898
Epoch 170, val loss: 1.2032979726791382
Epoch 180, training loss: 1.6578936576843262 = 0.9663550853729248 + 0.1 * 6.9153852462768555
Epoch 180, val loss: 1.141731858253479
Epoch 190, training loss: 1.5772926807403564 = 0.8867846131324768 + 0.1 * 6.9050798416137695
Epoch 190, val loss: 1.0845847129821777
Epoch 200, training loss: 1.5028531551361084 = 0.8137097954750061 + 0.1 * 6.891434192657471
Epoch 200, val loss: 1.0333657264709473
Epoch 210, training loss: 1.435825228691101 = 0.7482429146766663 + 0.1 * 6.875823020935059
Epoch 210, val loss: 0.9884952306747437
Epoch 220, training loss: 1.3761613368988037 = 0.6904669404029846 + 0.1 * 6.8569440841674805
Epoch 220, val loss: 0.9506158828735352
Epoch 230, training loss: 1.3236725330352783 = 0.6394837498664856 + 0.1 * 6.841888427734375
Epoch 230, val loss: 0.9195420742034912
Epoch 240, training loss: 1.276404619216919 = 0.5942288637161255 + 0.1 * 6.821758270263672
Epoch 240, val loss: 0.8943286538124084
Epoch 250, training loss: 1.2333035469055176 = 0.5524381399154663 + 0.1 * 6.808654308319092
Epoch 250, val loss: 0.8733401894569397
Epoch 260, training loss: 1.1931695938110352 = 0.5132156610488892 + 0.1 * 6.799539089202881
Epoch 260, val loss: 0.8559200763702393
Epoch 270, training loss: 1.154188632965088 = 0.47612273693084717 + 0.1 * 6.78065824508667
Epoch 270, val loss: 0.8417060971260071
Epoch 280, training loss: 1.1174880266189575 = 0.4405071437358856 + 0.1 * 6.769808292388916
Epoch 280, val loss: 0.8301902413368225
Epoch 290, training loss: 1.0823959112167358 = 0.4063732624053955 + 0.1 * 6.760226249694824
Epoch 290, val loss: 0.8207772374153137
Epoch 300, training loss: 1.0480824708938599 = 0.3734980523586273 + 0.1 * 6.745843887329102
Epoch 300, val loss: 0.8128806352615356
Epoch 310, training loss: 1.0158801078796387 = 0.3419380784034729 + 0.1 * 6.739419460296631
Epoch 310, val loss: 0.806344211101532
Epoch 320, training loss: 0.9838352799415588 = 0.3118555545806885 + 0.1 * 6.719797134399414
Epoch 320, val loss: 0.8011536002159119
Epoch 330, training loss: 0.9566537141799927 = 0.2834603190422058 + 0.1 * 6.73193359375
Epoch 330, val loss: 0.7973586916923523
Epoch 340, training loss: 0.9285451173782349 = 0.257211297750473 + 0.1 * 6.713338375091553
Epoch 340, val loss: 0.7947086095809937
Epoch 350, training loss: 0.9023290872573853 = 0.2329694777727127 + 0.1 * 6.693595886230469
Epoch 350, val loss: 0.7933775186538696
Epoch 360, training loss: 0.8795036673545837 = 0.21068285405635834 + 0.1 * 6.688207626342773
Epoch 360, val loss: 0.7934235334396362
Epoch 370, training loss: 0.857848048210144 = 0.190317302942276 + 0.1 * 6.675307273864746
Epoch 370, val loss: 0.7945693731307983
Epoch 380, training loss: 0.8405808210372925 = 0.1717696636915207 + 0.1 * 6.688111305236816
Epoch 380, val loss: 0.797058641910553
Epoch 390, training loss: 0.8212110996246338 = 0.15503457188606262 + 0.1 * 6.661764621734619
Epoch 390, val loss: 0.8003997802734375
Epoch 400, training loss: 0.8049279451370239 = 0.13987067341804504 + 0.1 * 6.650572299957275
Epoch 400, val loss: 0.8048575520515442
Epoch 410, training loss: 0.7909924983978271 = 0.12616941332817078 + 0.1 * 6.64823055267334
Epoch 410, val loss: 0.8102788925170898
Epoch 420, training loss: 0.7778698205947876 = 0.11387496441602707 + 0.1 * 6.63994836807251
Epoch 420, val loss: 0.816412091255188
Epoch 430, training loss: 0.7665892243385315 = 0.10285400599241257 + 0.1 * 6.637351989746094
Epoch 430, val loss: 0.8232913017272949
Epoch 440, training loss: 0.7550419569015503 = 0.09300445020198822 + 0.1 * 6.62037467956543
Epoch 440, val loss: 0.8308150172233582
Epoch 450, training loss: 0.7458333969116211 = 0.08420141041278839 + 0.1 * 6.61631965637207
Epoch 450, val loss: 0.8388546705245972
Epoch 460, training loss: 0.737934410572052 = 0.07635168731212616 + 0.1 * 6.6158270835876465
Epoch 460, val loss: 0.8473569750785828
Epoch 470, training loss: 0.7294285297393799 = 0.0693608820438385 + 0.1 * 6.600676536560059
Epoch 470, val loss: 0.8561692237854004
Epoch 480, training loss: 0.7247829437255859 = 0.0631299689412117 + 0.1 * 6.61652946472168
Epoch 480, val loss: 0.8654271960258484
Epoch 490, training loss: 0.7166876196861267 = 0.05760082229971886 + 0.1 * 6.59086799621582
Epoch 490, val loss: 0.8746105432510376
Epoch 500, training loss: 0.7112979292869568 = 0.052667420357465744 + 0.1 * 6.586304664611816
Epoch 500, val loss: 0.8842757344245911
Epoch 510, training loss: 0.7077932357788086 = 0.04827768728137016 + 0.1 * 6.595155715942383
Epoch 510, val loss: 0.893961489200592
Epoch 520, training loss: 0.7017369866371155 = 0.044381480664014816 + 0.1 * 6.573554515838623
Epoch 520, val loss: 0.9033612012863159
Epoch 530, training loss: 0.6977591514587402 = 0.04089825600385666 + 0.1 * 6.56860876083374
Epoch 530, val loss: 0.9131544232368469
Epoch 540, training loss: 0.694352924823761 = 0.03778252378106117 + 0.1 * 6.565703392028809
Epoch 540, val loss: 0.9227756261825562
Epoch 550, training loss: 0.6905569434165955 = 0.03499731048941612 + 0.1 * 6.555596351623535
Epoch 550, val loss: 0.9322622418403625
Epoch 560, training loss: 0.6885387897491455 = 0.03250095620751381 + 0.1 * 6.560378074645996
Epoch 560, val loss: 0.9418306946754456
Epoch 570, training loss: 0.6865386962890625 = 0.0302653256803751 + 0.1 * 6.5627336502075195
Epoch 570, val loss: 0.9510056376457214
Epoch 580, training loss: 0.6830896735191345 = 0.028259217739105225 + 0.1 * 6.548304557800293
Epoch 580, val loss: 0.9600842595100403
Epoch 590, training loss: 0.6804729700088501 = 0.026449132710695267 + 0.1 * 6.540237903594971
Epoch 590, val loss: 0.9691442251205444
Epoch 600, training loss: 0.6807485222816467 = 0.024810949340462685 + 0.1 * 6.559375286102295
Epoch 600, val loss: 0.9780215620994568
Epoch 610, training loss: 0.6765411496162415 = 0.023331062868237495 + 0.1 * 6.532101154327393
Epoch 610, val loss: 0.9867526292800903
Epoch 620, training loss: 0.6754269003868103 = 0.021987317129969597 + 0.1 * 6.534395694732666
Epoch 620, val loss: 0.9952293038368225
Epoch 630, training loss: 0.674235999584198 = 0.02076255902647972 + 0.1 * 6.53473424911499
Epoch 630, val loss: 1.0037286281585693
Epoch 640, training loss: 0.6725233197212219 = 0.01964789628982544 + 0.1 * 6.528754234313965
Epoch 640, val loss: 1.0117393732070923
Epoch 650, training loss: 0.6702439785003662 = 0.018625959753990173 + 0.1 * 6.516180038452148
Epoch 650, val loss: 1.0199158191680908
Epoch 660, training loss: 0.6696923971176147 = 0.017688922584056854 + 0.1 * 6.520034313201904
Epoch 660, val loss: 1.027877926826477
Epoch 670, training loss: 0.6676214337348938 = 0.016827993094921112 + 0.1 * 6.507934093475342
Epoch 670, val loss: 1.0355021953582764
Epoch 680, training loss: 0.6680898070335388 = 0.016033515334129333 + 0.1 * 6.520562648773193
Epoch 680, val loss: 1.0431482791900635
Epoch 690, training loss: 0.6666220426559448 = 0.015301155857741833 + 0.1 * 6.513208866119385
Epoch 690, val loss: 1.0505374670028687
Epoch 700, training loss: 0.6655473113059998 = 0.01462294440716505 + 0.1 * 6.509243488311768
Epoch 700, val loss: 1.0577927827835083
Epoch 710, training loss: 0.6640035510063171 = 0.013993609696626663 + 0.1 * 6.500099182128906
Epoch 710, val loss: 1.0648705959320068
Epoch 720, training loss: 0.6627478003501892 = 0.013408619910478592 + 0.1 * 6.493391990661621
Epoch 720, val loss: 1.0718481540679932
Epoch 730, training loss: 0.6631855964660645 = 0.01286176685243845 + 0.1 * 6.503238201141357
Epoch 730, val loss: 1.0788493156433105
Epoch 740, training loss: 0.6620344519615173 = 0.012353428639471531 + 0.1 * 6.496810436248779
Epoch 740, val loss: 1.085444450378418
Epoch 750, training loss: 0.6606765389442444 = 0.011878379620611668 + 0.1 * 6.487981796264648
Epoch 750, val loss: 1.0920088291168213
Epoch 760, training loss: 0.6601596474647522 = 0.011433219537138939 + 0.1 * 6.487264156341553
Epoch 760, val loss: 1.0984240770339966
Epoch 770, training loss: 0.6600020527839661 = 0.011015782132744789 + 0.1 * 6.48986291885376
Epoch 770, val loss: 1.1047803163528442
Epoch 780, training loss: 0.6604613661766052 = 0.010623255744576454 + 0.1 * 6.498380661010742
Epoch 780, val loss: 1.111012578010559
Epoch 790, training loss: 0.6585574150085449 = 0.010255839675664902 + 0.1 * 6.483016014099121
Epoch 790, val loss: 1.11693274974823
Epoch 800, training loss: 0.6567103266716003 = 0.009908999316394329 + 0.1 * 6.468013286590576
Epoch 800, val loss: 1.1228389739990234
Epoch 810, training loss: 0.6586989164352417 = 0.00958090741187334 + 0.1 * 6.491179943084717
Epoch 810, val loss: 1.128785252571106
Epoch 820, training loss: 0.6595202088356018 = 0.009270659647881985 + 0.1 * 6.502495288848877
Epoch 820, val loss: 1.1346510648727417
Epoch 830, training loss: 0.6562889814376831 = 0.008978978730738163 + 0.1 * 6.473100185394287
Epoch 830, val loss: 1.1400928497314453
Epoch 840, training loss: 0.6562728881835938 = 0.008703178726136684 + 0.1 * 6.475697040557861
Epoch 840, val loss: 1.1455299854278564
Epoch 850, training loss: 0.6543429493904114 = 0.008440470322966576 + 0.1 * 6.459024429321289
Epoch 850, val loss: 1.1510282754898071
Epoch 860, training loss: 0.6567416787147522 = 0.008191552013158798 + 0.1 * 6.485501289367676
Epoch 860, val loss: 1.1562848091125488
Epoch 870, training loss: 0.6542642712593079 = 0.007955404929816723 + 0.1 * 6.463088035583496
Epoch 870, val loss: 1.1615180969238281
Epoch 880, training loss: 0.6537455916404724 = 0.007731023244559765 + 0.1 * 6.460145473480225
Epoch 880, val loss: 1.1665247678756714
Epoch 890, training loss: 0.6531476974487305 = 0.007516553159803152 + 0.1 * 6.456311225891113
Epoch 890, val loss: 1.171631932258606
Epoch 900, training loss: 0.6542825698852539 = 0.007312326692044735 + 0.1 * 6.46970272064209
Epoch 900, val loss: 1.1766935586929321
Epoch 910, training loss: 0.6527398228645325 = 0.0071177249774336815 + 0.1 * 6.456220626831055
Epoch 910, val loss: 1.181425929069519
Epoch 920, training loss: 0.6524880528450012 = 0.006932512391358614 + 0.1 * 6.455555438995361
Epoch 920, val loss: 1.1860723495483398
Epoch 930, training loss: 0.6535346508026123 = 0.006754464469850063 + 0.1 * 6.467802047729492
Epoch 930, val loss: 1.1908416748046875
Epoch 940, training loss: 0.6517448425292969 = 0.006584546994417906 + 0.1 * 6.451602935791016
Epoch 940, val loss: 1.19563889503479
Epoch 950, training loss: 0.650985836982727 = 0.006422225385904312 + 0.1 * 6.44563627243042
Epoch 950, val loss: 1.2000298500061035
Epoch 960, training loss: 0.6504342555999756 = 0.006266552489250898 + 0.1 * 6.441677093505859
Epoch 960, val loss: 1.2045687437057495
Epoch 970, training loss: 0.6515005230903625 = 0.006117475219070911 + 0.1 * 6.453830242156982
Epoch 970, val loss: 1.2089858055114746
Epoch 980, training loss: 0.6507675051689148 = 0.005974281579256058 + 0.1 * 6.44793176651001
Epoch 980, val loss: 1.213331699371338
Epoch 990, training loss: 0.6493954658508301 = 0.005837210454046726 + 0.1 * 6.435582637786865
Epoch 990, val loss: 1.2175571918487549
Epoch 1000, training loss: 0.6497976779937744 = 0.005705684423446655 + 0.1 * 6.440919876098633
Epoch 1000, val loss: 1.2216132879257202
Epoch 1010, training loss: 0.6491801738739014 = 0.005578901618719101 + 0.1 * 6.4360127449035645
Epoch 1010, val loss: 1.225854516029358
Epoch 1020, training loss: 0.650278627872467 = 0.005457068327814341 + 0.1 * 6.448215484619141
Epoch 1020, val loss: 1.2299610376358032
Epoch 1030, training loss: 0.6483616828918457 = 0.005339880008250475 + 0.1 * 6.43021821975708
Epoch 1030, val loss: 1.2339773178100586
Epoch 1040, training loss: 0.6495394110679626 = 0.005227685905992985 + 0.1 * 6.443117141723633
Epoch 1040, val loss: 1.2378178834915161
Epoch 1050, training loss: 0.6489756107330322 = 0.005118913017213345 + 0.1 * 6.438567161560059
Epoch 1050, val loss: 1.241757869720459
Epoch 1060, training loss: 0.6489266753196716 = 0.005014139227569103 + 0.1 * 6.4391255378723145
Epoch 1060, val loss: 1.245681881904602
Epoch 1070, training loss: 0.6483180522918701 = 0.004913486540317535 + 0.1 * 6.434045314788818
Epoch 1070, val loss: 1.2493860721588135
Epoch 1080, training loss: 0.6476805806159973 = 0.004816655535250902 + 0.1 * 6.4286394119262695
Epoch 1080, val loss: 1.2529550790786743
Epoch 1090, training loss: 0.6474649310112 = 0.004722845274955034 + 0.1 * 6.4274210929870605
Epoch 1090, val loss: 1.2565876245498657
Epoch 1100, training loss: 0.6468881964683533 = 0.004631977528333664 + 0.1 * 6.4225616455078125
Epoch 1100, val loss: 1.2603956460952759
Epoch 1110, training loss: 0.647733747959137 = 0.0045443433336913586 + 0.1 * 6.431894302368164
Epoch 1110, val loss: 1.2639144659042358
Epoch 1120, training loss: 0.6481605172157288 = 0.004459582734853029 + 0.1 * 6.437009334564209
Epoch 1120, val loss: 1.267460823059082
Epoch 1130, training loss: 0.6465020179748535 = 0.004377851262688637 + 0.1 * 6.421241760253906
Epoch 1130, val loss: 1.2708007097244263
Epoch 1140, training loss: 0.6464983224868774 = 0.004298779647797346 + 0.1 * 6.421995162963867
Epoch 1140, val loss: 1.2740920782089233
Epoch 1150, training loss: 0.6456093788146973 = 0.004221749026328325 + 0.1 * 6.413876056671143
Epoch 1150, val loss: 1.277572751045227
Epoch 1160, training loss: 0.64885413646698 = 0.004147252067923546 + 0.1 * 6.44706916809082
Epoch 1160, val loss: 1.2809704542160034
Epoch 1170, training loss: 0.6462298035621643 = 0.004075052682310343 + 0.1 * 6.4215474128723145
Epoch 1170, val loss: 1.2843941450119019
Epoch 1180, training loss: 0.6450023651123047 = 0.004005501978099346 + 0.1 * 6.409968376159668
Epoch 1180, val loss: 1.2873733043670654
Epoch 1190, training loss: 0.6456711292266846 = 0.003937830217182636 + 0.1 * 6.417332649230957
Epoch 1190, val loss: 1.2905164957046509
Epoch 1200, training loss: 0.6460913419723511 = 0.0038721042219549417 + 0.1 * 6.422192096710205
Epoch 1200, val loss: 1.293772578239441
Epoch 1210, training loss: 0.6453861594200134 = 0.003808054607361555 + 0.1 * 6.415780544281006
Epoch 1210, val loss: 1.2969611883163452
Epoch 1220, training loss: 0.6460665464401245 = 0.003746556816622615 + 0.1 * 6.423199653625488
Epoch 1220, val loss: 1.2998549938201904
Epoch 1230, training loss: 0.6440454125404358 = 0.003686574986204505 + 0.1 * 6.40358829498291
Epoch 1230, val loss: 1.3028098344802856
Epoch 1240, training loss: 0.64656662940979 = 0.0036282003857195377 + 0.1 * 6.429384231567383
Epoch 1240, val loss: 1.3058805465698242
Epoch 1250, training loss: 0.6445454359054565 = 0.003571497043594718 + 0.1 * 6.409739017486572
Epoch 1250, val loss: 1.3089680671691895
Epoch 1260, training loss: 0.6448373794555664 = 0.0035166041925549507 + 0.1 * 6.413207530975342
Epoch 1260, val loss: 1.3117561340332031
Epoch 1270, training loss: 0.6440932750701904 = 0.003463094588369131 + 0.1 * 6.406301975250244
Epoch 1270, val loss: 1.314582347869873
Epoch 1280, training loss: 0.6441934704780579 = 0.003411106998100877 + 0.1 * 6.40782356262207
Epoch 1280, val loss: 1.3174861669540405
Epoch 1290, training loss: 0.6447284817695618 = 0.0033603396732360125 + 0.1 * 6.413681507110596
Epoch 1290, val loss: 1.3203104734420776
Epoch 1300, training loss: 0.6433761715888977 = 0.003310746280476451 + 0.1 * 6.400654315948486
Epoch 1300, val loss: 1.3232505321502686
Epoch 1310, training loss: 0.6435933709144592 = 0.0032628439366817474 + 0.1 * 6.403305530548096
Epoch 1310, val loss: 1.325879454612732
Epoch 1320, training loss: 0.6439664959907532 = 0.003216239158064127 + 0.1 * 6.407502174377441
Epoch 1320, val loss: 1.3285084962844849
Epoch 1330, training loss: 0.6443005204200745 = 0.003170618787407875 + 0.1 * 6.411298751831055
Epoch 1330, val loss: 1.3312623500823975
Epoch 1340, training loss: 0.6425593495368958 = 0.0031260098330676556 + 0.1 * 6.394333362579346
Epoch 1340, val loss: 1.3340340852737427
Epoch 1350, training loss: 0.64349764585495 = 0.0030828339513391256 + 0.1 * 6.404148101806641
Epoch 1350, val loss: 1.3365709781646729
Epoch 1360, training loss: 0.6420774459838867 = 0.0030406222213059664 + 0.1 * 6.3903679847717285
Epoch 1360, val loss: 1.3391987085342407
Epoch 1370, training loss: 0.6431249976158142 = 0.0029995481017977 + 0.1 * 6.401254653930664
Epoch 1370, val loss: 1.3416489362716675
Epoch 1380, training loss: 0.6440990567207336 = 0.0029594276566058397 + 0.1 * 6.411396026611328
Epoch 1380, val loss: 1.3441824913024902
Epoch 1390, training loss: 0.6418461203575134 = 0.0029200627468526363 + 0.1 * 6.389260292053223
Epoch 1390, val loss: 1.3468137979507446
Epoch 1400, training loss: 0.6435837149620056 = 0.0028820151928812265 + 0.1 * 6.407016754150391
Epoch 1400, val loss: 1.3492642641067505
Epoch 1410, training loss: 0.6427978873252869 = 0.00284454133361578 + 0.1 * 6.399533748626709
Epoch 1410, val loss: 1.3517154455184937
Epoch 1420, training loss: 0.642554759979248 = 0.0028082425706088543 + 0.1 * 6.397465229034424
Epoch 1420, val loss: 1.3541020154953003
Epoch 1430, training loss: 0.6415557265281677 = 0.002772571984678507 + 0.1 * 6.387831687927246
Epoch 1430, val loss: 1.356512188911438
Epoch 1440, training loss: 0.6417985558509827 = 0.002737907925620675 + 0.1 * 6.390606880187988
Epoch 1440, val loss: 1.3588911294937134
Epoch 1450, training loss: 0.6413533687591553 = 0.002703956561163068 + 0.1 * 6.386493682861328
Epoch 1450, val loss: 1.3612196445465088
Epoch 1460, training loss: 0.6421282291412354 = 0.002670807996764779 + 0.1 * 6.39457368850708
Epoch 1460, val loss: 1.3634676933288574
Epoch 1470, training loss: 0.6416391730308533 = 0.002638262463733554 + 0.1 * 6.390008926391602
Epoch 1470, val loss: 1.3659749031066895
Epoch 1480, training loss: 0.6417301893234253 = 0.0026065893471240997 + 0.1 * 6.391236305236816
Epoch 1480, val loss: 1.3681650161743164
Epoch 1490, training loss: 0.6406447887420654 = 0.002575706923380494 + 0.1 * 6.380690574645996
Epoch 1490, val loss: 1.3703432083129883
Epoch 1500, training loss: 0.6418434381484985 = 0.002545539988204837 + 0.1 * 6.392979145050049
Epoch 1500, val loss: 1.3724900484085083
Epoch 1510, training loss: 0.6407129168510437 = 0.0025157907512038946 + 0.1 * 6.38197135925293
Epoch 1510, val loss: 1.3747525215148926
Epoch 1520, training loss: 0.6417340636253357 = 0.002486743964254856 + 0.1 * 6.392472743988037
Epoch 1520, val loss: 1.3771001100540161
Epoch 1530, training loss: 0.6410972476005554 = 0.0024584063794463873 + 0.1 * 6.386388301849365
Epoch 1530, val loss: 1.3792370557785034
Epoch 1540, training loss: 0.6422725319862366 = 0.0024307488929480314 + 0.1 * 6.398417949676514
Epoch 1540, val loss: 1.381324291229248
Epoch 1550, training loss: 0.6396854519844055 = 0.0024036860559135675 + 0.1 * 6.372817516326904
Epoch 1550, val loss: 1.3833619356155396
Epoch 1560, training loss: 0.6398084163665771 = 0.002377202967181802 + 0.1 * 6.374311923980713
Epoch 1560, val loss: 1.385420560836792
Epoch 1570, training loss: 0.6416021585464478 = 0.002351237228140235 + 0.1 * 6.3925089836120605
Epoch 1570, val loss: 1.387494444847107
Epoch 1580, training loss: 0.6394031643867493 = 0.0023256142158061266 + 0.1 * 6.3707756996154785
Epoch 1580, val loss: 1.3896982669830322
Epoch 1590, training loss: 0.6412590146064758 = 0.0023007369600236416 + 0.1 * 6.389582633972168
Epoch 1590, val loss: 1.3916651010513306
Epoch 1600, training loss: 0.6394070982933044 = 0.0022762261796742678 + 0.1 * 6.371308326721191
Epoch 1600, val loss: 1.3937472105026245
Epoch 1610, training loss: 0.6398513317108154 = 0.002252419712021947 + 0.1 * 6.3759894371032715
Epoch 1610, val loss: 1.395603895187378
Epoch 1620, training loss: 0.640774667263031 = 0.0022289855405688286 + 0.1 * 6.3854570388793945
Epoch 1620, val loss: 1.3975251913070679
Epoch 1630, training loss: 0.6394299268722534 = 0.0022058298345655203 + 0.1 * 6.3722405433654785
Epoch 1630, val loss: 1.3996304273605347
Epoch 1640, training loss: 0.6402800679206848 = 0.0021833006758242846 + 0.1 * 6.380967617034912
Epoch 1640, val loss: 1.4016519784927368
Epoch 1650, training loss: 0.6387375593185425 = 0.0021612378768622875 + 0.1 * 6.365762710571289
Epoch 1650, val loss: 1.4034706354141235
Epoch 1660, training loss: 0.6398624777793884 = 0.0021395832300186157 + 0.1 * 6.377228736877441
Epoch 1660, val loss: 1.4052592515945435
Epoch 1670, training loss: 0.6389797925949097 = 0.0021181832998991013 + 0.1 * 6.368616104125977
Epoch 1670, val loss: 1.407256841659546
Epoch 1680, training loss: 0.6393510103225708 = 0.0020972704514861107 + 0.1 * 6.372537612915039
Epoch 1680, val loss: 1.4093058109283447
Epoch 1690, training loss: 0.6392709612846375 = 0.0020768146496266127 + 0.1 * 6.371941566467285
Epoch 1690, val loss: 1.411033272743225
Epoch 1700, training loss: 0.6386005878448486 = 0.002056653378531337 + 0.1 * 6.365438938140869
Epoch 1700, val loss: 1.4129911661148071
Epoch 1710, training loss: 0.6392929553985596 = 0.002037053694948554 + 0.1 * 6.372559070587158
Epoch 1710, val loss: 1.4146878719329834
Epoch 1720, training loss: 0.6388762593269348 = 0.0020177175756543875 + 0.1 * 6.368585109710693
Epoch 1720, val loss: 1.4164477586746216
Epoch 1730, training loss: 0.638877272605896 = 0.001998822670429945 + 0.1 * 6.368784427642822
Epoch 1730, val loss: 1.4180606603622437
Epoch 1740, training loss: 0.637855589389801 = 0.001980095636099577 + 0.1 * 6.358755111694336
Epoch 1740, val loss: 1.419819951057434
Epoch 1750, training loss: 0.6392573714256287 = 0.001961756031960249 + 0.1 * 6.372955799102783
Epoch 1750, val loss: 1.4216035604476929
Epoch 1760, training loss: 0.6389985084533691 = 0.0019436388975009322 + 0.1 * 6.370548248291016
Epoch 1760, val loss: 1.4234490394592285
Epoch 1770, training loss: 0.6387494206428528 = 0.001925902790389955 + 0.1 * 6.368235111236572
Epoch 1770, val loss: 1.4252426624298096
Epoch 1780, training loss: 0.6382160186767578 = 0.001908438978716731 + 0.1 * 6.3630757331848145
Epoch 1780, val loss: 1.4269235134124756
Epoch 1790, training loss: 0.6383993029594421 = 0.0018912923987954855 + 0.1 * 6.365079879760742
Epoch 1790, val loss: 1.4285989999771118
Epoch 1800, training loss: 0.6381364464759827 = 0.0018744029803201556 + 0.1 * 6.3626203536987305
Epoch 1800, val loss: 1.4303075075149536
Epoch 1810, training loss: 0.6386895179748535 = 0.0018578340532258153 + 0.1 * 6.368316650390625
Epoch 1810, val loss: 1.4320001602172852
Epoch 1820, training loss: 0.6374514698982239 = 0.0018415364902466536 + 0.1 * 6.3560991287231445
Epoch 1820, val loss: 1.433685541152954
Epoch 1830, training loss: 0.6381134986877441 = 0.0018255444010719657 + 0.1 * 6.362879276275635
Epoch 1830, val loss: 1.435324788093567
Epoch 1840, training loss: 0.6374732255935669 = 0.0018097787396982312 + 0.1 * 6.356634140014648
Epoch 1840, val loss: 1.436879277229309
Epoch 1850, training loss: 0.6376262903213501 = 0.0017942801350727677 + 0.1 * 6.358320236206055
Epoch 1850, val loss: 1.438523530960083
Epoch 1860, training loss: 0.6385899782180786 = 0.0017789946869015694 + 0.1 * 6.368109703063965
Epoch 1860, val loss: 1.4402676820755005
Epoch 1870, training loss: 0.6384220123291016 = 0.0017638688441365957 + 0.1 * 6.366581439971924
Epoch 1870, val loss: 1.4419564008712769
Epoch 1880, training loss: 0.6375941038131714 = 0.001749170827679336 + 0.1 * 6.358449459075928
Epoch 1880, val loss: 1.4435521364212036
Epoch 1890, training loss: 0.6375096440315247 = 0.0017347275279462337 + 0.1 * 6.357748985290527
Epoch 1890, val loss: 1.4448871612548828
Epoch 1900, training loss: 0.6370981335639954 = 0.0017205013427883387 + 0.1 * 6.353776454925537
Epoch 1900, val loss: 1.4463986158370972
Epoch 1910, training loss: 0.6373503804206848 = 0.001706439070403576 + 0.1 * 6.356439590454102
Epoch 1910, val loss: 1.447967529296875
Epoch 1920, training loss: 0.6382964849472046 = 0.0016925269737839699 + 0.1 * 6.366039276123047
Epoch 1920, val loss: 1.449631690979004
Epoch 1930, training loss: 0.6371358633041382 = 0.0016788856592029333 + 0.1 * 6.354569911956787
Epoch 1930, val loss: 1.4513022899627686
Epoch 1940, training loss: 0.6369617581367493 = 0.0016655002254992723 + 0.1 * 6.352962970733643
Epoch 1940, val loss: 1.452734351158142
Epoch 1950, training loss: 0.6367337703704834 = 0.0016522700898349285 + 0.1 * 6.350815296173096
Epoch 1950, val loss: 1.4541804790496826
Epoch 1960, training loss: 0.6373128294944763 = 0.0016393178375437856 + 0.1 * 6.3567352294921875
Epoch 1960, val loss: 1.455641269683838
Epoch 1970, training loss: 0.6377466320991516 = 0.0016264342702925205 + 0.1 * 6.361201763153076
Epoch 1970, val loss: 1.4572560787200928
Epoch 1980, training loss: 0.6367974281311035 = 0.0016137610655277967 + 0.1 * 6.351836681365967
Epoch 1980, val loss: 1.4588193893432617
Epoch 1990, training loss: 0.6364579796791077 = 0.0016013117274269462 + 0.1 * 6.34856653213501
Epoch 1990, val loss: 1.460330605506897
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 2.8023335933685303 = 1.9426509141921997 + 0.1 * 8.596826553344727
Epoch 0, val loss: 1.941827654838562
Epoch 10, training loss: 2.792203187942505 = 1.9325298070907593 + 0.1 * 8.596734046936035
Epoch 10, val loss: 1.9310486316680908
Epoch 20, training loss: 2.7795393466949463 = 1.9199289083480835 + 0.1 * 8.596104621887207
Epoch 20, val loss: 1.91726553440094
Epoch 30, training loss: 2.7612268924713135 = 1.902169108390808 + 0.1 * 8.590578079223633
Epoch 30, val loss: 1.8977580070495605
Epoch 40, training loss: 2.7314047813415527 = 1.8762656450271606 + 0.1 * 8.5513916015625
Epoch 40, val loss: 1.8698893785476685
Epoch 50, training loss: 2.676295042037964 = 1.842309832572937 + 0.1 * 8.339852333068848
Epoch 50, val loss: 1.8357428312301636
Epoch 60, training loss: 2.6191558837890625 = 1.8075031042099 + 0.1 * 8.116528511047363
Epoch 60, val loss: 1.8042634725570679
Epoch 70, training loss: 2.5560455322265625 = 1.777359127998352 + 0.1 * 7.786865234375
Epoch 70, val loss: 1.7796064615249634
Epoch 80, training loss: 2.4815402030944824 = 1.7444257736206055 + 0.1 * 7.371142864227295
Epoch 80, val loss: 1.7517786026000977
Epoch 90, training loss: 2.4210193157196045 = 1.7021502256393433 + 0.1 * 7.188690662384033
Epoch 90, val loss: 1.7156099081039429
Epoch 100, training loss: 2.3529438972473145 = 1.6435725688934326 + 0.1 * 7.093712329864502
Epoch 100, val loss: 1.6657578945159912
Epoch 110, training loss: 2.269108295440674 = 1.5675235986709595 + 0.1 * 7.015845775604248
Epoch 110, val loss: 1.6017183065414429
Epoch 120, training loss: 2.1758084297180176 = 1.4791604280471802 + 0.1 * 6.966480255126953
Epoch 120, val loss: 1.5284878015518188
Epoch 130, training loss: 2.079514265060425 = 1.385250210762024 + 0.1 * 6.942640781402588
Epoch 130, val loss: 1.4524060487747192
Epoch 140, training loss: 1.9824612140655518 = 1.2891175746917725 + 0.1 * 6.933436393737793
Epoch 140, val loss: 1.378204107284546
Epoch 150, training loss: 1.883598804473877 = 1.1908475160598755 + 0.1 * 6.927513599395752
Epoch 150, val loss: 1.3051139116287231
Epoch 160, training loss: 1.7851524353027344 = 1.0926108360290527 + 0.1 * 6.925415992736816
Epoch 160, val loss: 1.2337121963500977
Epoch 170, training loss: 1.6887224912643433 = 0.9967635869979858 + 0.1 * 6.919589042663574
Epoch 170, val loss: 1.1643012762069702
Epoch 180, training loss: 1.5970439910888672 = 0.9057919979095459 + 0.1 * 6.912519454956055
Epoch 180, val loss: 1.0985020399093628
Epoch 190, training loss: 1.5145208835601807 = 0.8240191340446472 + 0.1 * 6.905017375946045
Epoch 190, val loss: 1.0403755903244019
Epoch 200, training loss: 1.4439444541931152 = 0.7547502517700195 + 0.1 * 6.891941070556641
Epoch 200, val loss: 0.993187665939331
Epoch 210, training loss: 1.3851230144500732 = 0.6973009705543518 + 0.1 * 6.878221035003662
Epoch 210, val loss: 0.9569434523582458
Epoch 220, training loss: 1.3354756832122803 = 0.6491072177886963 + 0.1 * 6.863683700561523
Epoch 220, val loss: 0.9296040534973145
Epoch 230, training loss: 1.2909661531448364 = 0.6066883206367493 + 0.1 * 6.842778205871582
Epoch 230, val loss: 0.9078351259231567
Epoch 240, training loss: 1.2500603199005127 = 0.5673356056213379 + 0.1 * 6.827246189117432
Epoch 240, val loss: 0.888798177242279
Epoch 250, training loss: 1.2100963592529297 = 0.5288686752319336 + 0.1 * 6.812276840209961
Epoch 250, val loss: 0.8705077171325684
Epoch 260, training loss: 1.1698498725891113 = 0.4898231625556946 + 0.1 * 6.800266742706299
Epoch 260, val loss: 0.8520252704620361
Epoch 270, training loss: 1.129835844039917 = 0.44973859190940857 + 0.1 * 6.800972938537598
Epoch 270, val loss: 0.8337720036506653
Epoch 280, training loss: 1.0879391431808472 = 0.40927571058273315 + 0.1 * 6.78663444519043
Epoch 280, val loss: 0.8163489699363708
Epoch 290, training loss: 1.0462265014648438 = 0.3687931001186371 + 0.1 * 6.77433443069458
Epoch 290, val loss: 0.8009881377220154
Epoch 300, training loss: 1.0055969953536987 = 0.3290654718875885 + 0.1 * 6.76531457901001
Epoch 300, val loss: 0.7882872819900513
Epoch 310, training loss: 0.9672386050224304 = 0.29131996631622314 + 0.1 * 6.759186267852783
Epoch 310, val loss: 0.7790805101394653
Epoch 320, training loss: 0.9319103360176086 = 0.2567440867424011 + 0.1 * 6.751662254333496
Epoch 320, val loss: 0.7737423777580261
Epoch 330, training loss: 0.9006392955780029 = 0.22575250267982483 + 0.1 * 6.748867511749268
Epoch 330, val loss: 0.7724027037620544
Epoch 340, training loss: 0.8726480007171631 = 0.19867606461048126 + 0.1 * 6.739719390869141
Epoch 340, val loss: 0.7744606137275696
Epoch 350, training loss: 0.8473998308181763 = 0.17509739100933075 + 0.1 * 6.723023891448975
Epoch 350, val loss: 0.7799026370048523
Epoch 360, training loss: 0.826369047164917 = 0.1546545922756195 + 0.1 * 6.71714448928833
Epoch 360, val loss: 0.7881195545196533
Epoch 370, training loss: 0.8099711537361145 = 0.13710834085941315 + 0.1 * 6.728628158569336
Epoch 370, val loss: 0.7985213994979858
Epoch 380, training loss: 0.7923343181610107 = 0.12208817899227142 + 0.1 * 6.702461242675781
Epoch 380, val loss: 0.8104599118232727
Epoch 390, training loss: 0.7786070704460144 = 0.10913094133138657 + 0.1 * 6.694761276245117
Epoch 390, val loss: 0.8235864639282227
Epoch 400, training loss: 0.7670530080795288 = 0.0979059636592865 + 0.1 * 6.691470146179199
Epoch 400, val loss: 0.83772212266922
Epoch 410, training loss: 0.757830798625946 = 0.08821244537830353 + 0.1 * 6.696183681488037
Epoch 410, val loss: 0.8523492217063904
Epoch 420, training loss: 0.7476394176483154 = 0.0798402801156044 + 0.1 * 6.677990913391113
Epoch 420, val loss: 0.8670953512191772
Epoch 430, training loss: 0.7399552464485168 = 0.07256407290697098 + 0.1 * 6.6739115715026855
Epoch 430, val loss: 0.8815776109695435
Epoch 440, training loss: 0.732750415802002 = 0.06616933643817902 + 0.1 * 6.665810585021973
Epoch 440, val loss: 0.8962933421134949
Epoch 450, training loss: 0.7265951633453369 = 0.06051236763596535 + 0.1 * 6.660828113555908
Epoch 450, val loss: 0.9111127853393555
Epoch 460, training loss: 0.7219083905220032 = 0.05550660938024521 + 0.1 * 6.664018154144287
Epoch 460, val loss: 0.9254728555679321
Epoch 470, training loss: 0.7160965204238892 = 0.05107732489705086 + 0.1 * 6.6501922607421875
Epoch 470, val loss: 0.9396283030509949
Epoch 480, training loss: 0.7114323973655701 = 0.04713258892297745 + 0.1 * 6.642997741699219
Epoch 480, val loss: 0.9535227417945862
Epoch 490, training loss: 0.7081241011619568 = 0.04360221326351166 + 0.1 * 6.645218372344971
Epoch 490, val loss: 0.967147946357727
Epoch 500, training loss: 0.7049352526664734 = 0.04044793173670769 + 0.1 * 6.644873142242432
Epoch 500, val loss: 0.9803454279899597
Epoch 510, training loss: 0.700729250907898 = 0.037621092051267624 + 0.1 * 6.631081581115723
Epoch 510, val loss: 0.9933314323425293
Epoch 520, training loss: 0.6978458166122437 = 0.035071007907390594 + 0.1 * 6.627748012542725
Epoch 520, val loss: 1.005814790725708
Epoch 530, training loss: 0.6950191855430603 = 0.03277076408267021 + 0.1 * 6.62248420715332
Epoch 530, val loss: 1.018136739730835
Epoch 540, training loss: 0.6929455399513245 = 0.030691754072904587 + 0.1 * 6.622538089752197
Epoch 540, val loss: 1.0300663709640503
Epoch 550, training loss: 0.6906191110610962 = 0.028808552771806717 + 0.1 * 6.618105411529541
Epoch 550, val loss: 1.0415138006210327
Epoch 560, training loss: 0.6871281266212463 = 0.027097048237919807 + 0.1 * 6.600310802459717
Epoch 560, val loss: 1.0528583526611328
Epoch 570, training loss: 0.6849263310432434 = 0.025536371394991875 + 0.1 * 6.593899726867676
Epoch 570, val loss: 1.0638599395751953
Epoch 580, training loss: 0.6836190223693848 = 0.024112045764923096 + 0.1 * 6.595069885253906
Epoch 580, val loss: 1.0745770931243896
Epoch 590, training loss: 0.682743489742279 = 0.022810015827417374 + 0.1 * 6.599334239959717
Epoch 590, val loss: 1.0849645137786865
Epoch 600, training loss: 0.6792155504226685 = 0.02161613665521145 + 0.1 * 6.575993537902832
Epoch 600, val loss: 1.0949190855026245
Epoch 610, training loss: 0.6782615780830383 = 0.02051585726439953 + 0.1 * 6.577456951141357
Epoch 610, val loss: 1.1049176454544067
Epoch 620, training loss: 0.6781121492385864 = 0.019503990188241005 + 0.1 * 6.586081504821777
Epoch 620, val loss: 1.1144603490829468
Epoch 630, training loss: 0.6757771968841553 = 0.0185705553740263 + 0.1 * 6.572066307067871
Epoch 630, val loss: 1.1239486932754517
Epoch 640, training loss: 0.6744093298912048 = 0.017708729952573776 + 0.1 * 6.5670061111450195
Epoch 640, val loss: 1.132780909538269
Epoch 650, training loss: 0.6719342470169067 = 0.016908986493945122 + 0.1 * 6.550252437591553
Epoch 650, val loss: 1.1417547464370728
Epoch 660, training loss: 0.6729375123977661 = 0.01616528630256653 + 0.1 * 6.567722320556641
Epoch 660, val loss: 1.1505411863327026
Epoch 670, training loss: 0.6703042387962341 = 0.015472345054149628 + 0.1 * 6.548318862915039
Epoch 670, val loss: 1.1589765548706055
Epoch 680, training loss: 0.6691665053367615 = 0.014825993217527866 + 0.1 * 6.543405055999756
Epoch 680, val loss: 1.1674131155014038
Epoch 690, training loss: 0.6701464653015137 = 0.014220758341252804 + 0.1 * 6.5592570304870605
Epoch 690, val loss: 1.1757371425628662
Epoch 700, training loss: 0.6679420471191406 = 0.013655547983944416 + 0.1 * 6.542864799499512
Epoch 700, val loss: 1.1836233139038086
Epoch 710, training loss: 0.6663298010826111 = 0.013128025457262993 + 0.1 * 6.532017230987549
Epoch 710, val loss: 1.1912685632705688
Epoch 720, training loss: 0.6654128432273865 = 0.012632245197892189 + 0.1 * 6.527805805206299
Epoch 720, val loss: 1.1990392208099365
Epoch 730, training loss: 0.6653927564620972 = 0.012164982967078686 + 0.1 * 6.5322771072387695
Epoch 730, val loss: 1.2065938711166382
Epoch 740, training loss: 0.6632646322250366 = 0.011726809665560722 + 0.1 * 6.515377998352051
Epoch 740, val loss: 1.2139853239059448
Epoch 750, training loss: 0.6632331013679504 = 0.011315166018903255 + 0.1 * 6.519179344177246
Epoch 750, val loss: 1.2209776639938354
Epoch 760, training loss: 0.6624307632446289 = 0.010925646871328354 + 0.1 * 6.515050888061523
Epoch 760, val loss: 1.228055477142334
Epoch 770, training loss: 0.6619179844856262 = 0.010557051748037338 + 0.1 * 6.513608932495117
Epoch 770, val loss: 1.2352523803710938
Epoch 780, training loss: 0.6621320247650146 = 0.010210278443992138 + 0.1 * 6.519217491149902
Epoch 780, val loss: 1.2417455911636353
Epoch 790, training loss: 0.6603721380233765 = 0.009881534613668919 + 0.1 * 6.504905700683594
Epoch 790, val loss: 1.2483898401260376
Epoch 800, training loss: 0.6596735119819641 = 0.009570952504873276 + 0.1 * 6.501025676727295
Epoch 800, val loss: 1.2547972202301025
Epoch 810, training loss: 0.6607131361961365 = 0.009276151657104492 + 0.1 * 6.514369964599609
Epoch 810, val loss: 1.2612472772598267
Epoch 820, training loss: 0.658183217048645 = 0.00899621844291687 + 0.1 * 6.491869926452637
Epoch 820, val loss: 1.2675490379333496
Epoch 830, training loss: 0.6580153703689575 = 0.008730045519769192 + 0.1 * 6.492853164672852
Epoch 830, val loss: 1.273667812347412
Epoch 840, training loss: 0.658253014087677 = 0.0084766186773777 + 0.1 * 6.4977641105651855
Epoch 840, val loss: 1.2797622680664062
Epoch 850, training loss: 0.658286988735199 = 0.008235560730099678 + 0.1 * 6.500514030456543
Epoch 850, val loss: 1.2855963706970215
Epoch 860, training loss: 0.6567028164863586 = 0.008006495423614979 + 0.1 * 6.486962795257568
Epoch 860, val loss: 1.2915244102478027
Epoch 870, training loss: 0.6556854844093323 = 0.007788132876157761 + 0.1 * 6.478973388671875
Epoch 870, val loss: 1.2970716953277588
Epoch 880, training loss: 0.6572243571281433 = 0.007579046301543713 + 0.1 * 6.496453285217285
Epoch 880, val loss: 1.30294668674469
Epoch 890, training loss: 0.6546868085861206 = 0.007379881571978331 + 0.1 * 6.473069190979004
Epoch 890, val loss: 1.3083025217056274
Epoch 900, training loss: 0.6560090184211731 = 0.0071900165639817715 + 0.1 * 6.488190174102783
Epoch 900, val loss: 1.3136357069015503
Epoch 910, training loss: 0.6536232233047485 = 0.007007749751210213 + 0.1 * 6.466154098510742
Epoch 910, val loss: 1.3190017938613892
Epoch 920, training loss: 0.6545578837394714 = 0.006833228282630444 + 0.1 * 6.477246284484863
Epoch 920, val loss: 1.324424386024475
Epoch 930, training loss: 0.6538856029510498 = 0.006666216067969799 + 0.1 * 6.472193717956543
Epoch 930, val loss: 1.3294661045074463
Epoch 940, training loss: 0.6530913710594177 = 0.0065061915665864944 + 0.1 * 6.465851306915283
Epoch 940, val loss: 1.3346319198608398
Epoch 950, training loss: 0.6523337364196777 = 0.006352949887514114 + 0.1 * 6.459807872772217
Epoch 950, val loss: 1.3394341468811035
Epoch 960, training loss: 0.6517655253410339 = 0.006205619778484106 + 0.1 * 6.455598831176758
Epoch 960, val loss: 1.344367504119873
Epoch 970, training loss: 0.6517220139503479 = 0.006063394248485565 + 0.1 * 6.456585884094238
Epoch 970, val loss: 1.3494750261306763
Epoch 980, training loss: 0.6543684005737305 = 0.005927859805524349 + 0.1 * 6.484405040740967
Epoch 980, val loss: 1.354218602180481
Epoch 990, training loss: 0.6517294049263 = 0.005797012243419886 + 0.1 * 6.459323406219482
Epoch 990, val loss: 1.3587242364883423
Epoch 1000, training loss: 0.6511286497116089 = 0.005671944934874773 + 0.1 * 6.4545674324035645
Epoch 1000, val loss: 1.363267421722412
Epoch 1010, training loss: 0.6501651406288147 = 0.0055507756769657135 + 0.1 * 6.44614315032959
Epoch 1010, val loss: 1.3677377700805664
Epoch 1020, training loss: 0.6499438285827637 = 0.00543420622125268 + 0.1 * 6.445096015930176
Epoch 1020, val loss: 1.372281551361084
Epoch 1030, training loss: 0.6506834030151367 = 0.005321431439369917 + 0.1 * 6.453619480133057
Epoch 1030, val loss: 1.3768370151519775
Epoch 1040, training loss: 0.6492287516593933 = 0.005212686490267515 + 0.1 * 6.440160751342773
Epoch 1040, val loss: 1.3811603784561157
Epoch 1050, training loss: 0.6491702795028687 = 0.005108027718961239 + 0.1 * 6.440622806549072
Epoch 1050, val loss: 1.3855054378509521
Epoch 1060, training loss: 0.6488701701164246 = 0.005007073748856783 + 0.1 * 6.4386305809021
Epoch 1060, val loss: 1.3896564245224
Epoch 1070, training loss: 0.6512855291366577 = 0.004909038078039885 + 0.1 * 6.463764667510986
Epoch 1070, val loss: 1.393945574760437
Epoch 1080, training loss: 0.6482822895050049 = 0.0048145633190870285 + 0.1 * 6.4346771240234375
Epoch 1080, val loss: 1.3980330228805542
Epoch 1090, training loss: 0.6474123001098633 = 0.004723258316516876 + 0.1 * 6.4268903732299805
Epoch 1090, val loss: 1.402034878730774
Epoch 1100, training loss: 0.64844810962677 = 0.004634887911379337 + 0.1 * 6.438131809234619
Epoch 1100, val loss: 1.40594482421875
Epoch 1110, training loss: 0.6473717093467712 = 0.004549112636595964 + 0.1 * 6.428225994110107
Epoch 1110, val loss: 1.4101547002792358
Epoch 1120, training loss: 0.6468231081962585 = 0.004466434940695763 + 0.1 * 6.423566818237305
Epoch 1120, val loss: 1.413931965827942
Epoch 1130, training loss: 0.6485775709152222 = 0.00438610790297389 + 0.1 * 6.441914081573486
Epoch 1130, val loss: 1.4178504943847656
Epoch 1140, training loss: 0.6470237970352173 = 0.004308403003960848 + 0.1 * 6.427153587341309
Epoch 1140, val loss: 1.4216498136520386
Epoch 1150, training loss: 0.6469067335128784 = 0.00423309626057744 + 0.1 * 6.426735877990723
Epoch 1150, val loss: 1.425358533859253
Epoch 1160, training loss: 0.6475468873977661 = 0.004159924108535051 + 0.1 * 6.433869361877441
Epoch 1160, val loss: 1.4291642904281616
Epoch 1170, training loss: 0.6451585292816162 = 0.004089232534170151 + 0.1 * 6.4106926918029785
Epoch 1170, val loss: 1.432774305343628
Epoch 1180, training loss: 0.6461729407310486 = 0.004020657856017351 + 0.1 * 6.421523094177246
Epoch 1180, val loss: 1.4363651275634766
Epoch 1190, training loss: 0.6450395584106445 = 0.003953910898417234 + 0.1 * 6.410856246948242
Epoch 1190, val loss: 1.4398683309555054
Epoch 1200, training loss: 0.6454099416732788 = 0.0038890161085873842 + 0.1 * 6.4152092933654785
Epoch 1200, val loss: 1.4434336423873901
Epoch 1210, training loss: 0.6460075974464417 = 0.003825817024335265 + 0.1 * 6.421817302703857
Epoch 1210, val loss: 1.4471206665039062
Epoch 1220, training loss: 0.6444222927093506 = 0.0037644393742084503 + 0.1 * 6.406578540802002
Epoch 1220, val loss: 1.4505723714828491
Epoch 1230, training loss: 0.6463121175765991 = 0.003704989794641733 + 0.1 * 6.4260711669921875
Epoch 1230, val loss: 1.4537972211837769
Epoch 1240, training loss: 0.6443964838981628 = 0.0036470177583396435 + 0.1 * 6.40749454498291
Epoch 1240, val loss: 1.4573942422866821
Epoch 1250, training loss: 0.6439249515533447 = 0.0035909407306462526 + 0.1 * 6.403339862823486
Epoch 1250, val loss: 1.4605897665023804
Epoch 1260, training loss: 0.6456276774406433 = 0.0035363254137337208 + 0.1 * 6.420913219451904
Epoch 1260, val loss: 1.4637563228607178
Epoch 1270, training loss: 0.6442252397537231 = 0.0034829997457563877 + 0.1 * 6.4074225425720215
Epoch 1270, val loss: 1.4672056436538696
Epoch 1280, training loss: 0.643947184085846 = 0.0034312158823013306 + 0.1 * 6.405159950256348
Epoch 1280, val loss: 1.4702928066253662
Epoch 1290, training loss: 0.6433836817741394 = 0.003380832029506564 + 0.1 * 6.400028228759766
Epoch 1290, val loss: 1.473374843597412
Epoch 1300, training loss: 0.6441304087638855 = 0.003331411862745881 + 0.1 * 6.407989501953125
Epoch 1300, val loss: 1.4767793416976929
Epoch 1310, training loss: 0.6430981159210205 = 0.003283503232523799 + 0.1 * 6.398146152496338
Epoch 1310, val loss: 1.4798420667648315
Epoch 1320, training loss: 0.6432367563247681 = 0.0032367603853344917 + 0.1 * 6.399999618530273
Epoch 1320, val loss: 1.4827717542648315
Epoch 1330, training loss: 0.6423444747924805 = 0.003191030351445079 + 0.1 * 6.391533851623535
Epoch 1330, val loss: 1.4859286546707153
Epoch 1340, training loss: 0.644293487071991 = 0.0031466586515307426 + 0.1 * 6.411468029022217
Epoch 1340, val loss: 1.4889897108078003
Epoch 1350, training loss: 0.6424692273139954 = 0.0031031726393848658 + 0.1 * 6.393660068511963
Epoch 1350, val loss: 1.4919872283935547
Epoch 1360, training loss: 0.6439635157585144 = 0.003060897346585989 + 0.1 * 6.409026145935059
Epoch 1360, val loss: 1.4949451684951782
Epoch 1370, training loss: 0.6424947381019592 = 0.003019666764885187 + 0.1 * 6.394750595092773
Epoch 1370, val loss: 1.4977874755859375
Epoch 1380, training loss: 0.6418054103851318 = 0.002979482291266322 + 0.1 * 6.388259410858154
Epoch 1380, val loss: 1.5006039142608643
Epoch 1390, training loss: 0.6436000466346741 = 0.002940103877335787 + 0.1 * 6.406599044799805
Epoch 1390, val loss: 1.5035569667816162
Epoch 1400, training loss: 0.6415471434593201 = 0.0029015601612627506 + 0.1 * 6.386455535888672
Epoch 1400, val loss: 1.5064244270324707
Epoch 1410, training loss: 0.6417430639266968 = 0.0028640609234571457 + 0.1 * 6.388790130615234
Epoch 1410, val loss: 1.5091874599456787
Epoch 1420, training loss: 0.6428257822990417 = 0.0028274268843233585 + 0.1 * 6.399983882904053
Epoch 1420, val loss: 1.5119749307632446
Epoch 1430, training loss: 0.6416423320770264 = 0.002791464561596513 + 0.1 * 6.3885087966918945
Epoch 1430, val loss: 1.5147360563278198
Epoch 1440, training loss: 0.6409648060798645 = 0.0027565236669033766 + 0.1 * 6.382082462310791
Epoch 1440, val loss: 1.5174636840820312
Epoch 1450, training loss: 0.6418325901031494 = 0.002722327131778002 + 0.1 * 6.391102313995361
Epoch 1450, val loss: 1.5201420783996582
Epoch 1460, training loss: 0.6412407755851746 = 0.002688763430342078 + 0.1 * 6.385519981384277
Epoch 1460, val loss: 1.5229097604751587
Epoch 1470, training loss: 0.6411899924278259 = 0.002656099619343877 + 0.1 * 6.38533878326416
Epoch 1470, val loss: 1.5255696773529053
Epoch 1480, training loss: 0.6411491632461548 = 0.002624134300276637 + 0.1 * 6.385250091552734
Epoch 1480, val loss: 1.5280832052230835
Epoch 1490, training loss: 0.6403407454490662 = 0.002592888195067644 + 0.1 * 6.37747859954834
Epoch 1490, val loss: 1.530619740486145
Epoch 1500, training loss: 0.6406866312026978 = 0.002562213921919465 + 0.1 * 6.381243705749512
Epoch 1500, val loss: 1.5332282781600952
Epoch 1510, training loss: 0.641465425491333 = 0.002532216953113675 + 0.1 * 6.389331817626953
Epoch 1510, val loss: 1.5358092784881592
Epoch 1520, training loss: 0.6410495638847351 = 0.002502840245142579 + 0.1 * 6.385467052459717
Epoch 1520, val loss: 1.538500189781189
Epoch 1530, training loss: 0.6398236751556396 = 0.0024741291999816895 + 0.1 * 6.373495578765869
Epoch 1530, val loss: 1.5408369302749634
Epoch 1540, training loss: 0.6411169767379761 = 0.0024459536653012037 + 0.1 * 6.386710166931152
Epoch 1540, val loss: 1.5432881116867065
Epoch 1550, training loss: 0.6401627659797668 = 0.0024183776695281267 + 0.1 * 6.377443313598633
Epoch 1550, val loss: 1.5458251237869263
Epoch 1560, training loss: 0.6400106549263 = 0.0023914938792586327 + 0.1 * 6.376191139221191
Epoch 1560, val loss: 1.5482369661331177
Epoch 1570, training loss: 0.6400022506713867 = 0.0023650284856557846 + 0.1 * 6.37637186050415
Epoch 1570, val loss: 1.5505930185317993
Epoch 1580, training loss: 0.6393821239471436 = 0.0023392029106616974 + 0.1 * 6.370429515838623
Epoch 1580, val loss: 1.5529611110687256
Epoch 1590, training loss: 0.6393246054649353 = 0.0023137200623750687 + 0.1 * 6.370108604431152
Epoch 1590, val loss: 1.5553923845291138
Epoch 1600, training loss: 0.6403365731239319 = 0.0022887347731739283 + 0.1 * 6.3804779052734375
Epoch 1600, val loss: 1.557768702507019
Epoch 1610, training loss: 0.6402197480201721 = 0.0022642845287919044 + 0.1 * 6.379554748535156
Epoch 1610, val loss: 1.560205340385437
Epoch 1620, training loss: 0.6390673518180847 = 0.0022404631599783897 + 0.1 * 6.3682684898376465
Epoch 1620, val loss: 1.5624533891677856
Epoch 1630, training loss: 0.6405152082443237 = 0.0022169495932757854 + 0.1 * 6.3829827308654785
Epoch 1630, val loss: 1.564687728881836
Epoch 1640, training loss: 0.6387573480606079 = 0.0021939396392554045 + 0.1 * 6.365633964538574
Epoch 1640, val loss: 1.5669749975204468
Epoch 1650, training loss: 0.6398483514785767 = 0.002171451458707452 + 0.1 * 6.376768589019775
Epoch 1650, val loss: 1.5691763162612915
Epoch 1660, training loss: 0.6386888027191162 = 0.0021493094973266125 + 0.1 * 6.3653950691223145
Epoch 1660, val loss: 1.5713696479797363
Epoch 1670, training loss: 0.6397337317466736 = 0.00212758663110435 + 0.1 * 6.37606143951416
Epoch 1670, val loss: 1.5736521482467651
Epoch 1680, training loss: 0.638390839099884 = 0.0021062905434519053 + 0.1 * 6.362845420837402
Epoch 1680, val loss: 1.5758168697357178
Epoch 1690, training loss: 0.6393298506736755 = 0.002085375366732478 + 0.1 * 6.3724446296691895
Epoch 1690, val loss: 1.577948808670044
Epoch 1700, training loss: 0.63864666223526 = 0.0020648008212447166 + 0.1 * 6.365818500518799
Epoch 1700, val loss: 1.58025324344635
Epoch 1710, training loss: 0.6392242312431335 = 0.0020446612033993006 + 0.1 * 6.371795177459717
Epoch 1710, val loss: 1.5822923183441162
Epoch 1720, training loss: 0.6378433108329773 = 0.002024933695793152 + 0.1 * 6.35818338394165
Epoch 1720, val loss: 1.5844475030899048
Epoch 1730, training loss: 0.6408581733703613 = 0.0020056175999343395 + 0.1 * 6.388525485992432
Epoch 1730, val loss: 1.5865752696990967
Epoch 1740, training loss: 0.6379744410514832 = 0.001986496616154909 + 0.1 * 6.359879016876221
Epoch 1740, val loss: 1.5886023044586182
Epoch 1750, training loss: 0.6385998129844666 = 0.0019678946118801832 + 0.1 * 6.366319179534912
Epoch 1750, val loss: 1.5905874967575073
Epoch 1760, training loss: 0.6387527585029602 = 0.0019495711894705892 + 0.1 * 6.368031978607178
Epoch 1760, val loss: 1.5926220417022705
Epoch 1770, training loss: 0.6373135447502136 = 0.0019314580131322145 + 0.1 * 6.35382080078125
Epoch 1770, val loss: 1.5947740077972412
Epoch 1780, training loss: 0.6381776332855225 = 0.0019137681229040027 + 0.1 * 6.362638473510742
Epoch 1780, val loss: 1.5967364311218262
Epoch 1790, training loss: 0.6379882097244263 = 0.0018963374895974994 + 0.1 * 6.360918998718262
Epoch 1790, val loss: 1.5987763404846191
Epoch 1800, training loss: 0.6373714804649353 = 0.0018791360780596733 + 0.1 * 6.354923248291016
Epoch 1800, val loss: 1.6007732152938843
Epoch 1810, training loss: 0.6390355825424194 = 0.0018622186034917831 + 0.1 * 6.37173318862915
Epoch 1810, val loss: 1.6027685403823853
Epoch 1820, training loss: 0.6371852159500122 = 0.001845580292865634 + 0.1 * 6.353395938873291
Epoch 1820, val loss: 1.6048012971878052
Epoch 1830, training loss: 0.639797568321228 = 0.0018292733002454042 + 0.1 * 6.379683017730713
Epoch 1830, val loss: 1.6067606210708618
Epoch 1840, training loss: 0.6376065015792847 = 0.0018132112454622984 + 0.1 * 6.3579325675964355
Epoch 1840, val loss: 1.6086442470550537
Epoch 1850, training loss: 0.6368404626846313 = 0.0017974403453990817 + 0.1 * 6.350430011749268
Epoch 1850, val loss: 1.6104687452316284
Epoch 1860, training loss: 0.6373459696769714 = 0.0017820551292970777 + 0.1 * 6.3556389808654785
Epoch 1860, val loss: 1.6122219562530518
Epoch 1870, training loss: 0.6372737884521484 = 0.0017667895881459117 + 0.1 * 6.355069637298584
Epoch 1870, val loss: 1.614214301109314
Epoch 1880, training loss: 0.637184202671051 = 0.0017517483793199062 + 0.1 * 6.354324817657471
Epoch 1880, val loss: 1.6161226034164429
Epoch 1890, training loss: 0.6371159553527832 = 0.001736916252411902 + 0.1 * 6.353789806365967
Epoch 1890, val loss: 1.6179693937301636
Epoch 1900, training loss: 0.6369667053222656 = 0.0017223430331796408 + 0.1 * 6.352443695068359
Epoch 1900, val loss: 1.6198465824127197
Epoch 1910, training loss: 0.6380241513252258 = 0.0017080861143767834 + 0.1 * 6.363160610198975
Epoch 1910, val loss: 1.6215763092041016
Epoch 1920, training loss: 0.636202335357666 = 0.001693908590823412 + 0.1 * 6.345084190368652
Epoch 1920, val loss: 1.6233824491500854
Epoch 1930, training loss: 0.636880099773407 = 0.0016800963785499334 + 0.1 * 6.3520002365112305
Epoch 1930, val loss: 1.6250629425048828
Epoch 1940, training loss: 0.6363445520401001 = 0.0016664615832269192 + 0.1 * 6.346780300140381
Epoch 1940, val loss: 1.626844882965088
Epoch 1950, training loss: 0.6370911002159119 = 0.0016529448330402374 + 0.1 * 6.354381084442139
Epoch 1950, val loss: 1.6287518739700317
Epoch 1960, training loss: 0.6371867060661316 = 0.0016397623112425208 + 0.1 * 6.355469226837158
Epoch 1960, val loss: 1.6305317878723145
Epoch 1970, training loss: 0.6364933252334595 = 0.0016267007449641824 + 0.1 * 6.348665714263916
Epoch 1970, val loss: 1.6322135925292969
Epoch 1980, training loss: 0.6373004913330078 = 0.0016139326617121696 + 0.1 * 6.356865882873535
Epoch 1980, val loss: 1.6337924003601074
Epoch 1990, training loss: 0.6363930106163025 = 0.001601280178874731 + 0.1 * 6.347917079925537
Epoch 1990, val loss: 1.635574221611023
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.823405376910912
=== training gcn model ===
Epoch 0, training loss: 2.8099477291107178 = 1.9502668380737305 + 0.1 * 8.596809387207031
Epoch 0, val loss: 1.937983751296997
Epoch 10, training loss: 2.7990198135375977 = 1.9393563270568848 + 0.1 * 8.596633911132812
Epoch 10, val loss: 1.9279242753982544
Epoch 20, training loss: 2.785299301147461 = 1.925755262374878 + 0.1 * 8.595439910888672
Epoch 20, val loss: 1.9150108098983765
Epoch 30, training loss: 2.7652370929718018 = 1.9067561626434326 + 0.1 * 8.584809303283691
Epoch 30, val loss: 1.8966622352600098
Epoch 40, training loss: 2.731400489807129 = 1.8794196844100952 + 0.1 * 8.519806861877441
Epoch 40, val loss: 1.8704044818878174
Epoch 50, training loss: 2.665116310119629 = 1.845015048980713 + 0.1 * 8.20101261138916
Epoch 50, val loss: 1.8391493558883667
Epoch 60, training loss: 2.6129908561706543 = 1.8093329668045044 + 0.1 * 8.036579132080078
Epoch 60, val loss: 1.8083760738372803
Epoch 70, training loss: 2.543809413909912 = 1.7790684700012207 + 0.1 * 7.647408962249756
Epoch 70, val loss: 1.7833155393600464
Epoch 80, training loss: 2.4715747833251953 = 1.7495381832122803 + 0.1 * 7.22036600112915
Epoch 80, val loss: 1.7573654651641846
Epoch 90, training loss: 2.417320489883423 = 1.7122114896774292 + 0.1 * 7.051090717315674
Epoch 90, val loss: 1.7241355180740356
Epoch 100, training loss: 2.3593878746032715 = 1.6612333059310913 + 0.1 * 6.981545925140381
Epoch 100, val loss: 1.6790778636932373
Epoch 110, training loss: 2.2889246940612793 = 1.5947260856628418 + 0.1 * 6.941986560821533
Epoch 110, val loss: 1.6227977275848389
Epoch 120, training loss: 2.2088775634765625 = 1.516780138015747 + 0.1 * 6.920973300933838
Epoch 120, val loss: 1.5598465204238892
Epoch 130, training loss: 2.1252999305725098 = 1.435038685798645 + 0.1 * 6.902613162994385
Epoch 130, val loss: 1.4950103759765625
Epoch 140, training loss: 2.0413358211517334 = 1.3527940511703491 + 0.1 * 6.8854169845581055
Epoch 140, val loss: 1.4317758083343506
Epoch 150, training loss: 1.95656156539917 = 1.2695215940475464 + 0.1 * 6.870400428771973
Epoch 150, val loss: 1.368836760520935
Epoch 160, training loss: 1.870661735534668 = 1.1862156391143799 + 0.1 * 6.844460487365723
Epoch 160, val loss: 1.3061959743499756
Epoch 170, training loss: 1.7865551710128784 = 1.1041865348815918 + 0.1 * 6.823686122894287
Epoch 170, val loss: 1.245658278465271
Epoch 180, training loss: 1.7060152292251587 = 1.026053547859192 + 0.1 * 6.799616813659668
Epoch 180, val loss: 1.1897120475769043
Epoch 190, training loss: 1.6304574012756348 = 0.9515461921691895 + 0.1 * 6.789111614227295
Epoch 190, val loss: 1.1381174325942993
Epoch 200, training loss: 1.557973027229309 = 0.8812679648399353 + 0.1 * 6.767050266265869
Epoch 200, val loss: 1.090943694114685
Epoch 210, training loss: 1.4889647960662842 = 0.8134809136390686 + 0.1 * 6.754837989807129
Epoch 210, val loss: 1.0459508895874023
Epoch 220, training loss: 1.4238238334655762 = 0.748906135559082 + 0.1 * 6.749176025390625
Epoch 220, val loss: 1.0032144784927368
Epoch 230, training loss: 1.3617980480194092 = 0.6881285905838013 + 0.1 * 6.736695289611816
Epoch 230, val loss: 0.9636820554733276
Epoch 240, training loss: 1.304126262664795 = 0.6312093138694763 + 0.1 * 6.729168891906738
Epoch 240, val loss: 0.9284312725067139
Epoch 250, training loss: 1.250641107559204 = 0.5784134864807129 + 0.1 * 6.722275257110596
Epoch 250, val loss: 0.8984413146972656
Epoch 260, training loss: 1.200624704360962 = 0.5290036201477051 + 0.1 * 6.716211318969727
Epoch 260, val loss: 0.8734685778617859
Epoch 270, training loss: 1.153568148612976 = 0.4829854667186737 + 0.1 * 6.705826759338379
Epoch 270, val loss: 0.8530429601669312
Epoch 280, training loss: 1.1097627878189087 = 0.4401170015335083 + 0.1 * 6.696457862854004
Epoch 280, val loss: 0.8363961577415466
Epoch 290, training loss: 1.0704460144042969 = 0.4002314805984497 + 0.1 * 6.702144622802734
Epoch 290, val loss: 0.8231279850006104
Epoch 300, training loss: 1.0322721004486084 = 0.36374497413635254 + 0.1 * 6.685271263122559
Epoch 300, val loss: 0.813054621219635
Epoch 310, training loss: 0.9979790449142456 = 0.3301864564418793 + 0.1 * 6.677926063537598
Epoch 310, val loss: 0.8060857057571411
Epoch 320, training loss: 0.966796875 = 0.29950010776519775 + 0.1 * 6.672967433929443
Epoch 320, val loss: 0.8018175363540649
Epoch 330, training loss: 0.938184380531311 = 0.2714713513851166 + 0.1 * 6.667129993438721
Epoch 330, val loss: 0.7998718023300171
Epoch 340, training loss: 0.9113659858703613 = 0.2458389699459076 + 0.1 * 6.655270576477051
Epoch 340, val loss: 0.7999284267425537
Epoch 350, training loss: 0.8870384097099304 = 0.22229315340518951 + 0.1 * 6.647452354431152
Epoch 350, val loss: 0.801738440990448
Epoch 360, training loss: 0.864464282989502 = 0.20064932107925415 + 0.1 * 6.638149738311768
Epoch 360, val loss: 0.8050389289855957
Epoch 370, training loss: 0.8451347351074219 = 0.18087896704673767 + 0.1 * 6.642557621002197
Epoch 370, val loss: 0.8095834255218506
Epoch 380, training loss: 0.8258370161056519 = 0.16301147639751434 + 0.1 * 6.628255367279053
Epoch 380, val loss: 0.8153005242347717
Epoch 390, training loss: 0.8082611560821533 = 0.1468842774629593 + 0.1 * 6.613768577575684
Epoch 390, val loss: 0.8219444751739502
Epoch 400, training loss: 0.7941919565200806 = 0.13234886527061462 + 0.1 * 6.6184306144714355
Epoch 400, val loss: 0.8295522332191467
Epoch 410, training loss: 0.7803823947906494 = 0.11939531564712524 + 0.1 * 6.609870910644531
Epoch 410, val loss: 0.8378940224647522
Epoch 420, training loss: 0.7671366333961487 = 0.1078861877322197 + 0.1 * 6.592504501342773
Epoch 420, val loss: 0.8468689322471619
Epoch 430, training loss: 0.7573961615562439 = 0.09765130281448364 + 0.1 * 6.597448348999023
Epoch 430, val loss: 0.8563616275787354
Epoch 440, training loss: 0.7465401887893677 = 0.08858043700456619 + 0.1 * 6.579597473144531
Epoch 440, val loss: 0.8662658333778381
Epoch 450, training loss: 0.738264799118042 = 0.08053286373615265 + 0.1 * 6.577319145202637
Epoch 450, val loss: 0.8764185309410095
Epoch 460, training loss: 0.7302051782608032 = 0.07340344786643982 + 0.1 * 6.568017482757568
Epoch 460, val loss: 0.8867492079734802
Epoch 470, training loss: 0.7276921272277832 = 0.06706316024065018 + 0.1 * 6.606289386749268
Epoch 470, val loss: 0.8971941471099854
Epoch 480, training loss: 0.7191237211227417 = 0.061474405229091644 + 0.1 * 6.576493263244629
Epoch 480, val loss: 0.907492458820343
Epoch 490, training loss: 0.7114804983139038 = 0.056511394679546356 + 0.1 * 6.5496907234191895
Epoch 490, val loss: 0.9178131818771362
Epoch 500, training loss: 0.7066363096237183 = 0.052066653966903687 + 0.1 * 6.545696258544922
Epoch 500, val loss: 0.9280253052711487
Epoch 510, training loss: 0.7023319005966187 = 0.04807034507393837 + 0.1 * 6.5426154136657715
Epoch 510, val loss: 0.9382463693618774
Epoch 520, training loss: 0.7002664804458618 = 0.044484540820121765 + 0.1 * 6.55781888961792
Epoch 520, val loss: 0.9482264518737793
Epoch 530, training loss: 0.6951931118965149 = 0.04127490147948265 + 0.1 * 6.539182186126709
Epoch 530, val loss: 0.9580002427101135
Epoch 540, training loss: 0.6936612129211426 = 0.03837798535823822 + 0.1 * 6.552832126617432
Epoch 540, val loss: 0.9676175713539124
Epoch 550, training loss: 0.6898782253265381 = 0.0357699878513813 + 0.1 * 6.54108190536499
Epoch 550, val loss: 0.9768844246864319
Epoch 560, training loss: 0.6855916976928711 = 0.03341248631477356 + 0.1 * 6.521791458129883
Epoch 560, val loss: 0.9860524535179138
Epoch 570, training loss: 0.6833252310752869 = 0.031269364058971405 + 0.1 * 6.5205583572387695
Epoch 570, val loss: 0.9949780702590942
Epoch 580, training loss: 0.6806356906890869 = 0.029320964589715004 + 0.1 * 6.513147354125977
Epoch 580, val loss: 1.0037254095077515
Epoch 590, training loss: 0.6800613403320312 = 0.027544792741537094 + 0.1 * 6.525165557861328
Epoch 590, val loss: 1.0122473239898682
Epoch 600, training loss: 0.6772477626800537 = 0.025928644463419914 + 0.1 * 6.513191223144531
Epoch 600, val loss: 1.0205246210098267
Epoch 610, training loss: 0.6757822632789612 = 0.02444923296570778 + 0.1 * 6.513329982757568
Epoch 610, val loss: 1.0286242961883545
Epoch 620, training loss: 0.6742729544639587 = 0.023094933480024338 + 0.1 * 6.51177978515625
Epoch 620, val loss: 1.0364433526992798
Epoch 630, training loss: 0.6715270280838013 = 0.02185674011707306 + 0.1 * 6.496703147888184
Epoch 630, val loss: 1.0441280603408813
Epoch 640, training loss: 0.6710855960845947 = 0.020715774968266487 + 0.1 * 6.503698348999023
Epoch 640, val loss: 1.051643967628479
Epoch 650, training loss: 0.6686941981315613 = 0.019666604697704315 + 0.1 * 6.490275859832764
Epoch 650, val loss: 1.0588736534118652
Epoch 660, training loss: 0.6676954030990601 = 0.018698491156101227 + 0.1 * 6.489968776702881
Epoch 660, val loss: 1.0660171508789062
Epoch 670, training loss: 0.6677765250205994 = 0.017802948132157326 + 0.1 * 6.4997358322143555
Epoch 670, val loss: 1.0729289054870605
Epoch 680, training loss: 0.666192352771759 = 0.016973577439785004 + 0.1 * 6.492187976837158
Epoch 680, val loss: 1.0796843767166138
Epoch 690, training loss: 0.6639748215675354 = 0.01620548777282238 + 0.1 * 6.4776930809021
Epoch 690, val loss: 1.0862683057785034
Epoch 700, training loss: 0.6635689735412598 = 0.015489619225263596 + 0.1 * 6.480792999267578
Epoch 700, val loss: 1.092730164527893
Epoch 710, training loss: 0.6643251180648804 = 0.01482268888503313 + 0.1 * 6.49502420425415
Epoch 710, val loss: 1.0990604162216187
Epoch 720, training loss: 0.6616377234458923 = 0.014201437123119831 + 0.1 * 6.474362373352051
Epoch 720, val loss: 1.1051651239395142
Epoch 730, training loss: 0.6625243425369263 = 0.013621972873806953 + 0.1 * 6.489023208618164
Epoch 730, val loss: 1.111150860786438
Epoch 740, training loss: 0.6601302027702332 = 0.013081545941531658 + 0.1 * 6.470486640930176
Epoch 740, val loss: 1.1169582605361938
Epoch 750, training loss: 0.6588314175605774 = 0.012574668042361736 + 0.1 * 6.462567329406738
Epoch 750, val loss: 1.1227638721466064
Epoch 760, training loss: 0.6574440598487854 = 0.01209767535328865 + 0.1 * 6.453464031219482
Epoch 760, val loss: 1.128406047821045
Epoch 770, training loss: 0.6586229205131531 = 0.01164842490106821 + 0.1 * 6.46974515914917
Epoch 770, val loss: 1.1338753700256348
Epoch 780, training loss: 0.6561068892478943 = 0.011227455921471119 + 0.1 * 6.448794364929199
Epoch 780, val loss: 1.1392133235931396
Epoch 790, training loss: 0.6558735370635986 = 0.010831166990101337 + 0.1 * 6.450423240661621
Epoch 790, val loss: 1.144565224647522
Epoch 800, training loss: 0.6567131280899048 = 0.010455762967467308 + 0.1 * 6.462573051452637
Epoch 800, val loss: 1.1496484279632568
Epoch 810, training loss: 0.6548556089401245 = 0.01010312419384718 + 0.1 * 6.4475250244140625
Epoch 810, val loss: 1.1546814441680908
Epoch 820, training loss: 0.6566326022148132 = 0.009769441559910774 + 0.1 * 6.468631744384766
Epoch 820, val loss: 1.1596782207489014
Epoch 830, training loss: 0.6549160480499268 = 0.00945253949612379 + 0.1 * 6.454635143280029
Epoch 830, val loss: 1.164477825164795
Epoch 840, training loss: 0.6543402075767517 = 0.009153389371931553 + 0.1 * 6.451868534088135
Epoch 840, val loss: 1.1692590713500977
Epoch 850, training loss: 0.6524451971054077 = 0.008869368582963943 + 0.1 * 6.435758590698242
Epoch 850, val loss: 1.1739346981048584
Epoch 860, training loss: 0.652378261089325 = 0.008599692955613136 + 0.1 * 6.437785625457764
Epoch 860, val loss: 1.1785695552825928
Epoch 870, training loss: 0.6522461175918579 = 0.008342931047081947 + 0.1 * 6.439031600952148
Epoch 870, val loss: 1.1831148862838745
Epoch 880, training loss: 0.6507188677787781 = 0.008098176680505276 + 0.1 * 6.426206588745117
Epoch 880, val loss: 1.187532663345337
Epoch 890, training loss: 0.6518639922142029 = 0.007865925319492817 + 0.1 * 6.439980983734131
Epoch 890, val loss: 1.1919254064559937
Epoch 900, training loss: 0.6518740653991699 = 0.007643986959010363 + 0.1 * 6.442300319671631
Epoch 900, val loss: 1.1961175203323364
Epoch 910, training loss: 0.6504104137420654 = 0.007433760911226273 + 0.1 * 6.4297661781311035
Epoch 910, val loss: 1.2003053426742554
Epoch 920, training loss: 0.6501289010047913 = 0.007232738193124533 + 0.1 * 6.428961753845215
Epoch 920, val loss: 1.2044212818145752
Epoch 930, training loss: 0.6494752168655396 = 0.007040440104901791 + 0.1 * 6.424347400665283
Epoch 930, val loss: 1.208491325378418
Epoch 940, training loss: 0.6491965651512146 = 0.006856338586658239 + 0.1 * 6.423402309417725
Epoch 940, val loss: 1.212512731552124
Epoch 950, training loss: 0.6496666073799133 = 0.006679947953671217 + 0.1 * 6.429866313934326
Epoch 950, val loss: 1.2164732217788696
Epoch 960, training loss: 0.6489729285240173 = 0.006511107552796602 + 0.1 * 6.424618244171143
Epoch 960, val loss: 1.2202702760696411
Epoch 970, training loss: 0.6482310891151428 = 0.00634967815130949 + 0.1 * 6.418813705444336
Epoch 970, val loss: 1.224125623703003
Epoch 980, training loss: 0.6482985615730286 = 0.00619448022916913 + 0.1 * 6.4210405349731445
Epoch 980, val loss: 1.2277963161468506
Epoch 990, training loss: 0.6476473808288574 = 0.006045819725841284 + 0.1 * 6.416015625
Epoch 990, val loss: 1.2314573526382446
Epoch 1000, training loss: 0.6496657133102417 = 0.005903292912989855 + 0.1 * 6.437623977661133
Epoch 1000, val loss: 1.2350651025772095
Epoch 1010, training loss: 0.6472111940383911 = 0.0057658301666378975 + 0.1 * 6.414453506469727
Epoch 1010, val loss: 1.2385213375091553
Epoch 1020, training loss: 0.6473205089569092 = 0.005634589120745659 + 0.1 * 6.416859149932861
Epoch 1020, val loss: 1.2420578002929688
Epoch 1030, training loss: 0.6466371417045593 = 0.005507687106728554 + 0.1 * 6.411294460296631
Epoch 1030, val loss: 1.2454123497009277
Epoch 1040, training loss: 0.6458008885383606 = 0.005386215168982744 + 0.1 * 6.404146671295166
Epoch 1040, val loss: 1.248829960823059
Epoch 1050, training loss: 0.6470112800598145 = 0.0052689299918711185 + 0.1 * 6.417423248291016
Epoch 1050, val loss: 1.2521518468856812
Epoch 1060, training loss: 0.6462251543998718 = 0.005155767779797316 + 0.1 * 6.410694122314453
Epoch 1060, val loss: 1.2554136514663696
Epoch 1070, training loss: 0.6462268233299255 = 0.005047056823968887 + 0.1 * 6.411797523498535
Epoch 1070, val loss: 1.2586164474487305
Epoch 1080, training loss: 0.6449831128120422 = 0.004942296072840691 + 0.1 * 6.400407791137695
Epoch 1080, val loss: 1.26177978515625
Epoch 1090, training loss: 0.6448530554771423 = 0.00484125642105937 + 0.1 * 6.400117874145508
Epoch 1090, val loss: 1.2649779319763184
Epoch 1100, training loss: 0.6457278728485107 = 0.004743337631225586 + 0.1 * 6.409845352172852
Epoch 1100, val loss: 1.2680772542953491
Epoch 1110, training loss: 0.6447335481643677 = 0.004649107810109854 + 0.1 * 6.400844097137451
Epoch 1110, val loss: 1.2711354494094849
Epoch 1120, training loss: 0.6440010070800781 = 0.004557852167636156 + 0.1 * 6.3944315910339355
Epoch 1120, val loss: 1.274133563041687
Epoch 1130, training loss: 0.6448440551757812 = 0.004469944164156914 + 0.1 * 6.403741359710693
Epoch 1130, val loss: 1.2770934104919434
Epoch 1140, training loss: 0.6438290476799011 = 0.0043847584165632725 + 0.1 * 6.394442558288574
Epoch 1140, val loss: 1.2800477743148804
Epoch 1150, training loss: 0.6459237337112427 = 0.004302469547837973 + 0.1 * 6.416213035583496
Epoch 1150, val loss: 1.2829686403274536
Epoch 1160, training loss: 0.6435664892196655 = 0.004222839139401913 + 0.1 * 6.393436431884766
Epoch 1160, val loss: 1.2857120037078857
Epoch 1170, training loss: 0.6445886492729187 = 0.004145980346947908 + 0.1 * 6.404426574707031
Epoch 1170, val loss: 1.2885375022888184
Epoch 1180, training loss: 0.6427792310714722 = 0.004071412608027458 + 0.1 * 6.387077808380127
Epoch 1180, val loss: 1.2912986278533936
Epoch 1190, training loss: 0.6437886953353882 = 0.003999256528913975 + 0.1 * 6.397894382476807
Epoch 1190, val loss: 1.294063687324524
Epoch 1200, training loss: 0.6427617073059082 = 0.003929142374545336 + 0.1 * 6.3883256912231445
Epoch 1200, val loss: 1.2967618703842163
Epoch 1210, training loss: 0.6425637006759644 = 0.0038613390643149614 + 0.1 * 6.387022972106934
Epoch 1210, val loss: 1.2994675636291504
Epoch 1220, training loss: 0.6435865759849548 = 0.003795317141339183 + 0.1 * 6.397912502288818
Epoch 1220, val loss: 1.3020739555358887
Epoch 1230, training loss: 0.6428812742233276 = 0.003731170203536749 + 0.1 * 6.391500473022461
Epoch 1230, val loss: 1.3046663999557495
Epoch 1240, training loss: 0.6436904072761536 = 0.0036690952256321907 + 0.1 * 6.400213241577148
Epoch 1240, val loss: 1.307206630706787
Epoch 1250, training loss: 0.642436146736145 = 0.003609118517488241 + 0.1 * 6.388269901275635
Epoch 1250, val loss: 1.3097368478775024
Epoch 1260, training loss: 0.6427428126335144 = 0.0035507939755916595 + 0.1 * 6.39192008972168
Epoch 1260, val loss: 1.3123010396957397
Epoch 1270, training loss: 0.6409313678741455 = 0.0034939064644277096 + 0.1 * 6.3743743896484375
Epoch 1270, val loss: 1.3147603273391724
Epoch 1280, training loss: 0.6425548195838928 = 0.003438846208155155 + 0.1 * 6.391160011291504
Epoch 1280, val loss: 1.3172709941864014
Epoch 1290, training loss: 0.6420478224754333 = 0.0033848939929157495 + 0.1 * 6.386629104614258
Epoch 1290, val loss: 1.3196461200714111
Epoch 1300, training loss: 0.6405947208404541 = 0.003332747146487236 + 0.1 * 6.37261962890625
Epoch 1300, val loss: 1.3219996690750122
Epoch 1310, training loss: 0.6431620121002197 = 0.00328205362893641 + 0.1 * 6.398799419403076
Epoch 1310, val loss: 1.3243680000305176
Epoch 1320, training loss: 0.6413432359695435 = 0.0032324327621608973 + 0.1 * 6.381108283996582
Epoch 1320, val loss: 1.3266291618347168
Epoch 1330, training loss: 0.6409900188446045 = 0.003184783738106489 + 0.1 * 6.378052234649658
Epoch 1330, val loss: 1.328922986984253
Epoch 1340, training loss: 0.6404110789299011 = 0.003138073952868581 + 0.1 * 6.372730255126953
Epoch 1340, val loss: 1.331266164779663
Epoch 1350, training loss: 0.6430147886276245 = 0.003092612838372588 + 0.1 * 6.399221897125244
Epoch 1350, val loss: 1.3334925174713135
Epoch 1360, training loss: 0.640927791595459 = 0.0030481095891445875 + 0.1 * 6.378796577453613
Epoch 1360, val loss: 1.335616111755371
Epoch 1370, training loss: 0.640170693397522 = 0.00300521869212389 + 0.1 * 6.371654510498047
Epoch 1370, val loss: 1.3378452062606812
Epoch 1380, training loss: 0.6408165693283081 = 0.0029631746001541615 + 0.1 * 6.378533840179443
Epoch 1380, val loss: 1.3400492668151855
Epoch 1390, training loss: 0.6397340893745422 = 0.00292229768820107 + 0.1 * 6.368117809295654
Epoch 1390, val loss: 1.342231035232544
Epoch 1400, training loss: 0.6406168937683105 = 0.0028822864405810833 + 0.1 * 6.377346038818359
Epoch 1400, val loss: 1.3443924188613892
Epoch 1410, training loss: 0.6393209099769592 = 0.002843226073309779 + 0.1 * 6.364777088165283
Epoch 1410, val loss: 1.346506118774414
Epoch 1420, training loss: 0.6402320861816406 = 0.002805221825838089 + 0.1 * 6.374268531799316
Epoch 1420, val loss: 1.3486242294311523
Epoch 1430, training loss: 0.6405140161514282 = 0.0027677591424435377 + 0.1 * 6.377462387084961
Epoch 1430, val loss: 1.3506367206573486
Epoch 1440, training loss: 0.6394850611686707 = 0.002731609856709838 + 0.1 * 6.367534160614014
Epoch 1440, val loss: 1.3526853322982788
Epoch 1450, training loss: 0.641044557094574 = 0.0026963711716234684 + 0.1 * 6.383481502532959
Epoch 1450, val loss: 1.3547650575637817
Epoch 1460, training loss: 0.6391633749008179 = 0.0026615611277520657 + 0.1 * 6.365017890930176
Epoch 1460, val loss: 1.356618881225586
Epoch 1470, training loss: 0.6392313838005066 = 0.002628093818202615 + 0.1 * 6.366032600402832
Epoch 1470, val loss: 1.3585761785507202
Epoch 1480, training loss: 0.6390687227249146 = 0.002595390658825636 + 0.1 * 6.364733695983887
Epoch 1480, val loss: 1.3605791330337524
Epoch 1490, training loss: 0.6391085982322693 = 0.0025632684119045734 + 0.1 * 6.365452766418457
Epoch 1490, val loss: 1.3625407218933105
Epoch 1500, training loss: 0.6388112902641296 = 0.002531794598326087 + 0.1 * 6.362794876098633
Epoch 1500, val loss: 1.3644765615463257
Epoch 1510, training loss: 0.63862544298172 = 0.002501009264960885 + 0.1 * 6.361244201660156
Epoch 1510, val loss: 1.3664084672927856
Epoch 1520, training loss: 0.6405677795410156 = 0.0024707738775759935 + 0.1 * 6.380969524383545
Epoch 1520, val loss: 1.3682295083999634
Epoch 1530, training loss: 0.6383185982704163 = 0.0024413010105490685 + 0.1 * 6.358773231506348
Epoch 1530, val loss: 1.3700299263000488
Epoch 1540, training loss: 0.6395134925842285 = 0.002412666566669941 + 0.1 * 6.371007919311523
Epoch 1540, val loss: 1.37191903591156
Epoch 1550, training loss: 0.6377914547920227 = 0.0023845157120376825 + 0.1 * 6.354069232940674
Epoch 1550, val loss: 1.3737181425094604
Epoch 1560, training loss: 0.6396382451057434 = 0.0023570505436509848 + 0.1 * 6.372811794281006
Epoch 1560, val loss: 1.3755955696105957
Epoch 1570, training loss: 0.6382617950439453 = 0.0023297700099647045 + 0.1 * 6.359320163726807
Epoch 1570, val loss: 1.3772821426391602
Epoch 1580, training loss: 0.6382748484611511 = 0.0023035602644085884 + 0.1 * 6.359713077545166
Epoch 1580, val loss: 1.3790532350540161
Epoch 1590, training loss: 0.6381401419639587 = 0.0022777309641242027 + 0.1 * 6.35862398147583
Epoch 1590, val loss: 1.3808106184005737
Epoch 1600, training loss: 0.6384005546569824 = 0.0022524716332554817 + 0.1 * 6.361480712890625
Epoch 1600, val loss: 1.3825658559799194
Epoch 1610, training loss: 0.6381574869155884 = 0.002227668184787035 + 0.1 * 6.359298229217529
Epoch 1610, val loss: 1.3843213319778442
Epoch 1620, training loss: 0.637370765209198 = 0.0022033771965652704 + 0.1 * 6.351673603057861
Epoch 1620, val loss: 1.3860059976577759
Epoch 1630, training loss: 0.6379324793815613 = 0.0021796454675495625 + 0.1 * 6.3575286865234375
Epoch 1630, val loss: 1.3877429962158203
Epoch 1640, training loss: 0.6388007998466492 = 0.0021562017500400543 + 0.1 * 6.366446018218994
Epoch 1640, val loss: 1.3894293308258057
Epoch 1650, training loss: 0.637351930141449 = 0.0021333065815269947 + 0.1 * 6.35218620300293
Epoch 1650, val loss: 1.3910489082336426
Epoch 1660, training loss: 0.6382840871810913 = 0.002110905246809125 + 0.1 * 6.361732006072998
Epoch 1660, val loss: 1.3927052021026611
Epoch 1670, training loss: 0.6380447745323181 = 0.0020889639854431152 + 0.1 * 6.35955810546875
Epoch 1670, val loss: 1.394368290901184
Epoch 1680, training loss: 0.6375725865364075 = 0.0020674236584454775 + 0.1 * 6.355051517486572
Epoch 1680, val loss: 1.3959553241729736
Epoch 1690, training loss: 0.6371168494224548 = 0.0020463368855416775 + 0.1 * 6.350704669952393
Epoch 1690, val loss: 1.3976210355758667
Epoch 1700, training loss: 0.6371533274650574 = 0.0020255991257727146 + 0.1 * 6.351276874542236
Epoch 1700, val loss: 1.3991612195968628
Epoch 1710, training loss: 0.6368663311004639 = 0.0020052320323884487 + 0.1 * 6.348610877990723
Epoch 1710, val loss: 1.4007424116134644
Epoch 1720, training loss: 0.6376485228538513 = 0.0019852546975016594 + 0.1 * 6.356632709503174
Epoch 1720, val loss: 1.4023188352584839
Epoch 1730, training loss: 0.637367844581604 = 0.001965775154531002 + 0.1 * 6.354020595550537
Epoch 1730, val loss: 1.4038809537887573
Epoch 1740, training loss: 0.6373363137245178 = 0.0019465828081592917 + 0.1 * 6.353897571563721
Epoch 1740, val loss: 1.4053266048431396
Epoch 1750, training loss: 0.6362247467041016 = 0.0019279202679172158 + 0.1 * 6.342967987060547
Epoch 1750, val loss: 1.4068946838378906
Epoch 1760, training loss: 0.6371920108795166 = 0.0019095062743872404 + 0.1 * 6.352825164794922
Epoch 1760, val loss: 1.408427357673645
Epoch 1770, training loss: 0.6366043090820312 = 0.0018913038074970245 + 0.1 * 6.347129821777344
Epoch 1770, val loss: 1.4099397659301758
Epoch 1780, training loss: 0.6367608904838562 = 0.0018734901677817106 + 0.1 * 6.348874092102051
Epoch 1780, val loss: 1.411401391029358
Epoch 1790, training loss: 0.6372842788696289 = 0.0018559899181127548 + 0.1 * 6.354282855987549
Epoch 1790, val loss: 1.4128565788269043
Epoch 1800, training loss: 0.636121928691864 = 0.0018387865275144577 + 0.1 * 6.342831134796143
Epoch 1800, val loss: 1.414300560951233
Epoch 1810, training loss: 0.6364912986755371 = 0.0018220007186755538 + 0.1 * 6.34669303894043
Epoch 1810, val loss: 1.4157845973968506
Epoch 1820, training loss: 0.6360076069831848 = 0.0018054497195407748 + 0.1 * 6.342021942138672
Epoch 1820, val loss: 1.4172073602676392
Epoch 1830, training loss: 0.6372520327568054 = 0.0017892743926495314 + 0.1 * 6.35462760925293
Epoch 1830, val loss: 1.4186887741088867
Epoch 1840, training loss: 0.636569082736969 = 0.0017731902189552784 + 0.1 * 6.347959041595459
Epoch 1840, val loss: 1.4200330972671509
Epoch 1850, training loss: 0.6363856792449951 = 0.0017574912635609508 + 0.1 * 6.3462815284729
Epoch 1850, val loss: 1.4214255809783936
Epoch 1860, training loss: 0.6356882452964783 = 0.0017421151278540492 + 0.1 * 6.339460849761963
Epoch 1860, val loss: 1.4228861331939697
Epoch 1870, training loss: 0.6357786059379578 = 0.0017269500531256199 + 0.1 * 6.340516567230225
Epoch 1870, val loss: 1.4243037700653076
Epoch 1880, training loss: 0.6367862820625305 = 0.0017119980184361339 + 0.1 * 6.350742340087891
Epoch 1880, val loss: 1.425685167312622
Epoch 1890, training loss: 0.635869026184082 = 0.0016972882440313697 + 0.1 * 6.34171724319458
Epoch 1890, val loss: 1.4270302057266235
Epoch 1900, training loss: 0.635752260684967 = 0.001682895701378584 + 0.1 * 6.340693473815918
Epoch 1900, val loss: 1.428420901298523
Epoch 1910, training loss: 0.6365267634391785 = 0.0016687135212123394 + 0.1 * 6.348580837249756
Epoch 1910, val loss: 1.4297982454299927
Epoch 1920, training loss: 0.6363183259963989 = 0.001654691412113607 + 0.1 * 6.3466362953186035
Epoch 1920, val loss: 1.4311542510986328
Epoch 1930, training loss: 0.6356269121170044 = 0.0016410391544923186 + 0.1 * 6.339858531951904
Epoch 1930, val loss: 1.432449460029602
Epoch 1940, training loss: 0.6351629495620728 = 0.001627421355806291 + 0.1 * 6.335354804992676
Epoch 1940, val loss: 1.4337884187698364
Epoch 1950, training loss: 0.6351785659790039 = 0.0016141679370775819 + 0.1 * 6.335643768310547
Epoch 1950, val loss: 1.4351481199264526
Epoch 1960, training loss: 0.6356330513954163 = 0.0016011014813557267 + 0.1 * 6.340319633483887
Epoch 1960, val loss: 1.4364852905273438
Epoch 1970, training loss: 0.6359105110168457 = 0.001588186016306281 + 0.1 * 6.343223571777344
Epoch 1970, val loss: 1.4377778768539429
Epoch 1980, training loss: 0.6349904537200928 = 0.001575422938913107 + 0.1 * 6.334150314331055
Epoch 1980, val loss: 1.439059853553772
Epoch 1990, training loss: 0.6353782415390015 = 0.001563011552207172 + 0.1 * 6.3381524085998535
Epoch 1990, val loss: 1.440392017364502
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8239325250395362
The final CL Acc:0.76790, 0.01062, The final GNN Acc:0.82305, 0.00090
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13232])
remove edge: torch.Size([2, 8000])
updated graph: torch.Size([2, 10676])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.790997266769409 = 1.9313188791275024 + 0.1 * 8.596783638000488
Epoch 0, val loss: 1.9333559274673462
Epoch 10, training loss: 2.781759023666382 = 1.922093391418457 + 0.1 * 8.596656799316406
Epoch 10, val loss: 1.9238150119781494
Epoch 20, training loss: 2.7706072330474854 = 1.9110335111618042 + 0.1 * 8.595736503601074
Epoch 20, val loss: 1.9123716354370117
Epoch 30, training loss: 2.754512310028076 = 1.8957719802856445 + 0.1 * 8.587404251098633
Epoch 30, val loss: 1.8968019485473633
Epoch 40, training loss: 2.7262775897979736 = 1.8735241889953613 + 0.1 * 8.527533531188965
Epoch 40, val loss: 1.8746740818023682
Epoch 50, training loss: 2.663865089416504 = 1.8435002565383911 + 0.1 * 8.203649520874023
Epoch 50, val loss: 1.846215009689331
Epoch 60, training loss: 2.6031582355499268 = 1.8087295293807983 + 0.1 * 7.944287300109863
Epoch 60, val loss: 1.8151003122329712
Epoch 70, training loss: 2.5352258682250977 = 1.7742464542388916 + 0.1 * 7.609793663024902
Epoch 70, val loss: 1.7847397327423096
Epoch 80, training loss: 2.4659903049468994 = 1.7392139434814453 + 0.1 * 7.267762660980225
Epoch 80, val loss: 1.7535370588302612
Epoch 90, training loss: 2.401426076889038 = 1.6941406726837158 + 0.1 * 7.072854042053223
Epoch 90, val loss: 1.7128400802612305
Epoch 100, training loss: 2.325833797454834 = 1.631880760192871 + 0.1 * 6.939530849456787
Epoch 100, val loss: 1.6576564311981201
Epoch 110, training loss: 2.239236831665039 = 1.553406000137329 + 0.1 * 6.858308792114258
Epoch 110, val loss: 1.5913037061691284
Epoch 120, training loss: 2.1470561027526855 = 1.46477472782135 + 0.1 * 6.822813510894775
Epoch 120, val loss: 1.5168371200561523
Epoch 130, training loss: 2.0532126426696777 = 1.3726410865783691 + 0.1 * 6.805716037750244
Epoch 130, val loss: 1.4404983520507812
Epoch 140, training loss: 1.9600907564163208 = 1.2806835174560547 + 0.1 * 6.794072151184082
Epoch 140, val loss: 1.3654561042785645
Epoch 150, training loss: 1.8657914400100708 = 1.1874252557754517 + 0.1 * 6.783661842346191
Epoch 150, val loss: 1.2904163599014282
Epoch 160, training loss: 1.7716975212097168 = 1.0938377380371094 + 0.1 * 6.778597354888916
Epoch 160, val loss: 1.2164568901062012
Epoch 170, training loss: 1.6788753271102905 = 1.0022519826889038 + 0.1 * 6.766233444213867
Epoch 170, val loss: 1.1456283330917358
Epoch 180, training loss: 1.59035325050354 = 0.9136351943016052 + 0.1 * 6.767180442810059
Epoch 180, val loss: 1.0787581205368042
Epoch 190, training loss: 1.5068464279174805 = 0.8316675424575806 + 0.1 * 6.751789093017578
Epoch 190, val loss: 1.018078088760376
Epoch 200, training loss: 1.4307628870010376 = 0.7568829655647278 + 0.1 * 6.738799095153809
Epoch 200, val loss: 0.963837206363678
Epoch 210, training loss: 1.364417314529419 = 0.6900991201400757 + 0.1 * 6.74318265914917
Epoch 210, val loss: 0.9172329902648926
Epoch 220, training loss: 1.3038842678070068 = 0.6320725083351135 + 0.1 * 6.718116760253906
Epoch 220, val loss: 0.8795071840286255
Epoch 230, training loss: 1.252089262008667 = 0.5809206962585449 + 0.1 * 6.7116851806640625
Epoch 230, val loss: 0.8492463231086731
Epoch 240, training loss: 1.205864667892456 = 0.5355343818664551 + 0.1 * 6.703302383422852
Epoch 240, val loss: 0.8256505131721497
Epoch 250, training loss: 1.1638996601104736 = 0.49458298087120056 + 0.1 * 6.693166732788086
Epoch 250, val loss: 0.8073975443840027
Epoch 260, training loss: 1.1251606941223145 = 0.4567485749721527 + 0.1 * 6.6841206550598145
Epoch 260, val loss: 0.7934377193450928
Epoch 270, training loss: 1.0897858142852783 = 0.42143043875694275 + 0.1 * 6.683553218841553
Epoch 270, val loss: 0.7829802632331848
Epoch 280, training loss: 1.0552170276641846 = 0.3883543014526367 + 0.1 * 6.668626308441162
Epoch 280, val loss: 0.7754642367362976
Epoch 290, training loss: 1.0231226682662964 = 0.35703781247138977 + 0.1 * 6.660848617553711
Epoch 290, val loss: 0.7704327702522278
Epoch 300, training loss: 0.9937058687210083 = 0.3275851309299469 + 0.1 * 6.66120719909668
Epoch 300, val loss: 0.7677232623100281
Epoch 310, training loss: 0.964763879776001 = 0.3001123368740082 + 0.1 * 6.646515846252441
Epoch 310, val loss: 0.7671751379966736
Epoch 320, training loss: 0.9394835233688354 = 0.27463269233703613 + 0.1 * 6.648508071899414
Epoch 320, val loss: 0.7686875462532043
Epoch 330, training loss: 0.9146488904953003 = 0.25118890404701233 + 0.1 * 6.634599685668945
Epoch 330, val loss: 0.7721063494682312
Epoch 340, training loss: 0.8923382759094238 = 0.2295508086681366 + 0.1 * 6.627874851226807
Epoch 340, val loss: 0.7774685621261597
Epoch 350, training loss: 0.8716811537742615 = 0.20978903770446777 + 0.1 * 6.618921279907227
Epoch 350, val loss: 0.7845751643180847
Epoch 360, training loss: 0.8532431721687317 = 0.1917704939842224 + 0.1 * 6.614726543426514
Epoch 360, val loss: 0.793290913105011
Epoch 370, training loss: 0.836392343044281 = 0.1754225641489029 + 0.1 * 6.6096978187561035
Epoch 370, val loss: 0.8034888505935669
Epoch 380, training loss: 0.8212134838104248 = 0.16067782044410706 + 0.1 * 6.605356216430664
Epoch 380, val loss: 0.8148903250694275
Epoch 390, training loss: 0.8083744049072266 = 0.14730753004550934 + 0.1 * 6.610668659210205
Epoch 390, val loss: 0.827464759349823
Epoch 400, training loss: 0.7959104180335999 = 0.13522732257843018 + 0.1 * 6.606831073760986
Epoch 400, val loss: 0.8408681154251099
Epoch 410, training loss: 0.7834175825119019 = 0.1243145540356636 + 0.1 * 6.591030120849609
Epoch 410, val loss: 0.8550170063972473
Epoch 420, training loss: 0.7729431986808777 = 0.11441081017255783 + 0.1 * 6.585323810577393
Epoch 420, val loss: 0.8697634935379028
Epoch 430, training loss: 0.7633590698242188 = 0.10539683699607849 + 0.1 * 6.579622268676758
Epoch 430, val loss: 0.885001003742218
Epoch 440, training loss: 0.7549898028373718 = 0.09720539301633835 + 0.1 * 6.577844142913818
Epoch 440, val loss: 0.9005774259567261
Epoch 450, training loss: 0.7471326589584351 = 0.08981078118085861 + 0.1 * 6.573218822479248
Epoch 450, val loss: 0.9161859154701233
Epoch 460, training loss: 0.7401486039161682 = 0.08307822048664093 + 0.1 * 6.570703983306885
Epoch 460, val loss: 0.9320161938667297
Epoch 470, training loss: 0.7343499064445496 = 0.07695531845092773 + 0.1 * 6.573945999145508
Epoch 470, val loss: 0.9478954076766968
Epoch 480, training loss: 0.7285248637199402 = 0.07139913737773895 + 0.1 * 6.5712571144104
Epoch 480, val loss: 0.9636563658714294
Epoch 490, training loss: 0.7224966883659363 = 0.06633944064378738 + 0.1 * 6.561572551727295
Epoch 490, val loss: 0.9793065786361694
Epoch 500, training loss: 0.7171506881713867 = 0.06172681599855423 + 0.1 * 6.554238796234131
Epoch 500, val loss: 0.9948285222053528
Epoch 510, training loss: 0.7129097580909729 = 0.057506855577230453 + 0.1 * 6.5540289878845215
Epoch 510, val loss: 1.0102386474609375
Epoch 520, training loss: 0.7097028493881226 = 0.05365293473005295 + 0.1 * 6.56049919128418
Epoch 520, val loss: 1.0254372358322144
Epoch 530, training loss: 0.7057768106460571 = 0.05015687644481659 + 0.1 * 6.556199073791504
Epoch 530, val loss: 1.0402487516403198
Epoch 540, training loss: 0.7017741799354553 = 0.046985235065221786 + 0.1 * 6.547889709472656
Epoch 540, val loss: 1.0545028448104858
Epoch 550, training loss: 0.6976625323295593 = 0.04407636076211929 + 0.1 * 6.535861492156982
Epoch 550, val loss: 1.0685454607009888
Epoch 560, training loss: 0.6946142315864563 = 0.04140292480587959 + 0.1 * 6.5321125984191895
Epoch 560, val loss: 1.0824031829833984
Epoch 570, training loss: 0.6915954351425171 = 0.03895008936524391 + 0.1 * 6.526453495025635
Epoch 570, val loss: 1.0958786010742188
Epoch 580, training loss: 0.6903421878814697 = 0.0367010273039341 + 0.1 * 6.536411285400391
Epoch 580, val loss: 1.109068512916565
Epoch 590, training loss: 0.6871342062950134 = 0.03463314101099968 + 0.1 * 6.525010585784912
Epoch 590, val loss: 1.1218559741973877
Epoch 600, training loss: 0.6847216486930847 = 0.03272769972681999 + 0.1 * 6.519939422607422
Epoch 600, val loss: 1.1344070434570312
Epoch 610, training loss: 0.6824146509170532 = 0.03096904791891575 + 0.1 * 6.514456272125244
Epoch 610, val loss: 1.1466169357299805
Epoch 620, training loss: 0.6808689832687378 = 0.029342595487833023 + 0.1 * 6.51526403427124
Epoch 620, val loss: 1.1585290431976318
Epoch 630, training loss: 0.6794286966323853 = 0.027835510671138763 + 0.1 * 6.515932083129883
Epoch 630, val loss: 1.1702048778533936
Epoch 640, training loss: 0.6803569197654724 = 0.026441803202033043 + 0.1 * 6.539151191711426
Epoch 640, val loss: 1.181594729423523
Epoch 650, training loss: 0.6752490401268005 = 0.025152185931801796 + 0.1 * 6.500967979431152
Epoch 650, val loss: 1.1925809383392334
Epoch 660, training loss: 0.6730948686599731 = 0.023950688540935516 + 0.1 * 6.49144172668457
Epoch 660, val loss: 1.2034131288528442
Epoch 670, training loss: 0.674927830696106 = 0.022832246497273445 + 0.1 * 6.520955562591553
Epoch 670, val loss: 1.213983178138733
Epoch 680, training loss: 0.6715652346611023 = 0.02178988978266716 + 0.1 * 6.497753143310547
Epoch 680, val loss: 1.2242662906646729
Epoch 690, training loss: 0.670163094997406 = 0.020819075405597687 + 0.1 * 6.4934401512146
Epoch 690, val loss: 1.234311580657959
Epoch 700, training loss: 0.6689234972000122 = 0.019912313669919968 + 0.1 * 6.490111827850342
Epoch 700, val loss: 1.2441620826721191
Epoch 710, training loss: 0.6682000756263733 = 0.019062526524066925 + 0.1 * 6.49137544631958
Epoch 710, val loss: 1.2537554502487183
Epoch 720, training loss: 0.6659927368164062 = 0.01826636493206024 + 0.1 * 6.477263450622559
Epoch 720, val loss: 1.2631561756134033
Epoch 730, training loss: 0.6661337614059448 = 0.0175206009298563 + 0.1 * 6.48613166809082
Epoch 730, val loss: 1.2723621129989624
Epoch 740, training loss: 0.6638820171356201 = 0.016820097342133522 + 0.1 * 6.470619201660156
Epoch 740, val loss: 1.281360387802124
Epoch 750, training loss: 0.6653911471366882 = 0.016161873936653137 + 0.1 * 6.492292881011963
Epoch 750, val loss: 1.2901211977005005
Epoch 760, training loss: 0.6624161601066589 = 0.015544637106359005 + 0.1 * 6.468715190887451
Epoch 760, val loss: 1.2987364530563354
Epoch 770, training loss: 0.6609563231468201 = 0.014962298795580864 + 0.1 * 6.459939956665039
Epoch 770, val loss: 1.3071585893630981
Epoch 780, training loss: 0.6613327860832214 = 0.014412058517336845 + 0.1 * 6.469207286834717
Epoch 780, val loss: 1.31544029712677
Epoch 790, training loss: 0.6608380675315857 = 0.013893677853047848 + 0.1 * 6.4694437980651855
Epoch 790, val loss: 1.3235005140304565
Epoch 800, training loss: 0.6595061421394348 = 0.013404506258666515 + 0.1 * 6.461016654968262
Epoch 800, val loss: 1.3313931226730347
Epoch 810, training loss: 0.6582460403442383 = 0.01294147688895464 + 0.1 * 6.45304536819458
Epoch 810, val loss: 1.3391185998916626
Epoch 820, training loss: 0.6596217751502991 = 0.012503041885793209 + 0.1 * 6.471187591552734
Epoch 820, val loss: 1.346709132194519
Epoch 830, training loss: 0.6570189595222473 = 0.012088365852832794 + 0.1 * 6.449306011199951
Epoch 830, val loss: 1.3541362285614014
Epoch 840, training loss: 0.6565113663673401 = 0.011695151217281818 + 0.1 * 6.448162078857422
Epoch 840, val loss: 1.361466884613037
Epoch 850, training loss: 0.655136227607727 = 0.011320940218865871 + 0.1 * 6.43815279006958
Epoch 850, val loss: 1.3685991764068604
Epoch 860, training loss: 0.6557583808898926 = 0.010966232977807522 + 0.1 * 6.447921276092529
Epoch 860, val loss: 1.3756532669067383
Epoch 870, training loss: 0.6540699601173401 = 0.010627929121255875 + 0.1 * 6.434420108795166
Epoch 870, val loss: 1.382591724395752
Epoch 880, training loss: 0.6555302143096924 = 0.010305586270987988 + 0.1 * 6.452246189117432
Epoch 880, val loss: 1.3893640041351318
Epoch 890, training loss: 0.6538533568382263 = 0.010000153444707394 + 0.1 * 6.438531875610352
Epoch 890, val loss: 1.3959888219833374
Epoch 900, training loss: 0.6538422107696533 = 0.009710143320262432 + 0.1 * 6.441320896148682
Epoch 900, val loss: 1.4025391340255737
Epoch 910, training loss: 0.6525035500526428 = 0.009431952610611916 + 0.1 * 6.430716037750244
Epoch 910, val loss: 1.4089323282241821
Epoch 920, training loss: 0.6529401540756226 = 0.009166382253170013 + 0.1 * 6.437737941741943
Epoch 920, val loss: 1.4152709245681763
Epoch 930, training loss: 0.6534417867660522 = 0.008912432007491589 + 0.1 * 6.445293426513672
Epoch 930, val loss: 1.421487808227539
Epoch 940, training loss: 0.6522286534309387 = 0.00867044273763895 + 0.1 * 6.435582160949707
Epoch 940, val loss: 1.4275877475738525
Epoch 950, training loss: 0.6517806649208069 = 0.008439312689006329 + 0.1 * 6.433413505554199
Epoch 950, val loss: 1.433655858039856
Epoch 960, training loss: 0.6503574848175049 = 0.00821760855615139 + 0.1 * 6.421398639678955
Epoch 960, val loss: 1.4395314455032349
Epoch 970, training loss: 0.6501818299293518 = 0.008005416020751 + 0.1 * 6.421763896942139
Epoch 970, val loss: 1.445405125617981
Epoch 980, training loss: 0.6502322554588318 = 0.007800984662026167 + 0.1 * 6.424312591552734
Epoch 980, val loss: 1.4511326551437378
Epoch 990, training loss: 0.6493667364120483 = 0.007606822997331619 + 0.1 * 6.417599201202393
Epoch 990, val loss: 1.4568164348602295
Epoch 1000, training loss: 0.6500436067581177 = 0.00741950049996376 + 0.1 * 6.42624044418335
Epoch 1000, val loss: 1.4623976945877075
Epoch 1010, training loss: 0.6489927768707275 = 0.007239627186208963 + 0.1 * 6.417531490325928
Epoch 1010, val loss: 1.4678117036819458
Epoch 1020, training loss: 0.6494116187095642 = 0.007067316211760044 + 0.1 * 6.423442840576172
Epoch 1020, val loss: 1.4732482433319092
Epoch 1030, training loss: 0.6507887244224548 = 0.006901969201862812 + 0.1 * 6.438867568969727
Epoch 1030, val loss: 1.4785171747207642
Epoch 1040, training loss: 0.6483948230743408 = 0.006743273232132196 + 0.1 * 6.416515827178955
Epoch 1040, val loss: 1.4837092161178589
Epoch 1050, training loss: 0.6472766399383545 = 0.006590791046619415 + 0.1 * 6.406858444213867
Epoch 1050, val loss: 1.4888811111450195
Epoch 1060, training loss: 0.6465588212013245 = 0.0064434390515089035 + 0.1 * 6.401153564453125
Epoch 1060, val loss: 1.4939720630645752
Epoch 1070, training loss: 0.6491937041282654 = 0.006301155313849449 + 0.1 * 6.428925037384033
Epoch 1070, val loss: 1.4989783763885498
Epoch 1080, training loss: 0.6470723152160645 = 0.006163889542222023 + 0.1 * 6.409084320068359
Epoch 1080, val loss: 1.5038517713546753
Epoch 1090, training loss: 0.646617591381073 = 0.006032250821590424 + 0.1 * 6.405853748321533
Epoch 1090, val loss: 1.5087738037109375
Epoch 1100, training loss: 0.6456511616706848 = 0.0059055183082818985 + 0.1 * 6.397456645965576
Epoch 1100, val loss: 1.5135835409164429
Epoch 1110, training loss: 0.6485544443130493 = 0.005782692693173885 + 0.1 * 6.427717685699463
Epoch 1110, val loss: 1.5183234214782715
Epoch 1120, training loss: 0.6466161608695984 = 0.0056641316041350365 + 0.1 * 6.409520149230957
Epoch 1120, val loss: 1.5229289531707764
Epoch 1130, training loss: 0.6469758152961731 = 0.005549953319132328 + 0.1 * 6.4142584800720215
Epoch 1130, val loss: 1.5275431871414185
Epoch 1140, training loss: 0.644862949848175 = 0.005439827684313059 + 0.1 * 6.39423131942749
Epoch 1140, val loss: 1.5320810079574585
Epoch 1150, training loss: 0.6452118158340454 = 0.005333288107067347 + 0.1 * 6.39878511428833
Epoch 1150, val loss: 1.5365792512893677
Epoch 1160, training loss: 0.6444457173347473 = 0.0052299099043011665 + 0.1 * 6.392158031463623
Epoch 1160, val loss: 1.5409553050994873
Epoch 1170, training loss: 0.6454763412475586 = 0.0051304614171385765 + 0.1 * 6.403458595275879
Epoch 1170, val loss: 1.5453381538391113
Epoch 1180, training loss: 0.6449762582778931 = 0.005033645313233137 + 0.1 * 6.399425983428955
Epoch 1180, val loss: 1.549600601196289
Epoch 1190, training loss: 0.6443887948989868 = 0.004940428771078587 + 0.1 * 6.39448356628418
Epoch 1190, val loss: 1.5537796020507812
Epoch 1200, training loss: 0.6441605687141418 = 0.004849650897085667 + 0.1 * 6.39310884475708
Epoch 1200, val loss: 1.5579402446746826
Epoch 1210, training loss: 0.6435500979423523 = 0.004762211814522743 + 0.1 * 6.38787841796875
Epoch 1210, val loss: 1.5620700120925903
Epoch 1220, training loss: 0.6438876986503601 = 0.004677020478993654 + 0.1 * 6.392107009887695
Epoch 1220, val loss: 1.5661414861679077
Epoch 1230, training loss: 0.6440428495407104 = 0.004594564903527498 + 0.1 * 6.394482612609863
Epoch 1230, val loss: 1.5701007843017578
Epoch 1240, training loss: 0.6427295207977295 = 0.004515140783041716 + 0.1 * 6.382143974304199
Epoch 1240, val loss: 1.5741068124771118
Epoch 1250, training loss: 0.6434833407402039 = 0.004437816329300404 + 0.1 * 6.39045524597168
Epoch 1250, val loss: 1.5780366659164429
Epoch 1260, training loss: 0.6440710425376892 = 0.004362339153885841 + 0.1 * 6.397087097167969
Epoch 1260, val loss: 1.5818594694137573
Epoch 1270, training loss: 0.6426644325256348 = 0.004289356525987387 + 0.1 * 6.383750915527344
Epoch 1270, val loss: 1.585677146911621
Epoch 1280, training loss: 0.6421931385993958 = 0.004218735732138157 + 0.1 * 6.379743576049805
Epoch 1280, val loss: 1.589506983757019
Epoch 1290, training loss: 0.6420757174491882 = 0.004149827640503645 + 0.1 * 6.379258632659912
Epoch 1290, val loss: 1.5932121276855469
Epoch 1300, training loss: 0.6419261693954468 = 0.004083041567355394 + 0.1 * 6.37843132019043
Epoch 1300, val loss: 1.5968841314315796
Epoch 1310, training loss: 0.6420292854309082 = 0.004017701838165522 + 0.1 * 6.380115509033203
Epoch 1310, val loss: 1.600518822669983
Epoch 1320, training loss: 0.6418836712837219 = 0.003954582381993532 + 0.1 * 6.379290580749512
Epoch 1320, val loss: 1.6041011810302734
Epoch 1330, training loss: 0.6415343284606934 = 0.003893147222697735 + 0.1 * 6.3764119148254395
Epoch 1330, val loss: 1.6076399087905884
Epoch 1340, training loss: 0.6417587995529175 = 0.0038335041608661413 + 0.1 * 6.3792524337768555
Epoch 1340, val loss: 1.6111184358596802
Epoch 1350, training loss: 0.6411942839622498 = 0.003775475313887 + 0.1 * 6.37418794631958
Epoch 1350, val loss: 1.6145650148391724
Epoch 1360, training loss: 0.6418213248252869 = 0.0037188129499554634 + 0.1 * 6.381025314331055
Epoch 1360, val loss: 1.6179414987564087
Epoch 1370, training loss: 0.6406900882720947 = 0.003663520095869899 + 0.1 * 6.370265483856201
Epoch 1370, val loss: 1.6212594509124756
Epoch 1380, training loss: 0.6408592462539673 = 0.0036100908182561398 + 0.1 * 6.372491359710693
Epoch 1380, val loss: 1.6246055364608765
Epoch 1390, training loss: 0.6406921744346619 = 0.0035577593371272087 + 0.1 * 6.371344089508057
Epoch 1390, val loss: 1.6278992891311646
Epoch 1400, training loss: 0.6411524415016174 = 0.0035066022537648678 + 0.1 * 6.376458168029785
Epoch 1400, val loss: 1.6310925483703613
Epoch 1410, training loss: 0.6400955319404602 = 0.0034569259732961655 + 0.1 * 6.366385459899902
Epoch 1410, val loss: 1.634286880493164
Epoch 1420, training loss: 0.6402102708816528 = 0.0034084294456988573 + 0.1 * 6.36801815032959
Epoch 1420, val loss: 1.6374691724777222
Epoch 1430, training loss: 0.6400498747825623 = 0.003361052367836237 + 0.1 * 6.366888046264648
Epoch 1430, val loss: 1.6405534744262695
Epoch 1440, training loss: 0.6411046981811523 = 0.0033149225637316704 + 0.1 * 6.3778977394104
Epoch 1440, val loss: 1.643643856048584
Epoch 1450, training loss: 0.6391147375106812 = 0.003270103130489588 + 0.1 * 6.35844612121582
Epoch 1450, val loss: 1.6467164754867554
Epoch 1460, training loss: 0.6399463415145874 = 0.0032263752073049545 + 0.1 * 6.367199420928955
Epoch 1460, val loss: 1.6497527360916138
Epoch 1470, training loss: 0.6405699849128723 = 0.003183386055752635 + 0.1 * 6.373866081237793
Epoch 1470, val loss: 1.6527011394500732
Epoch 1480, training loss: 0.639873743057251 = 0.003141556866466999 + 0.1 * 6.367321491241455
Epoch 1480, val loss: 1.6556450128555298
Epoch 1490, training loss: 0.6387303471565247 = 0.003100800793617964 + 0.1 * 6.356295108795166
Epoch 1490, val loss: 1.6585735082626343
Epoch 1500, training loss: 0.6391435861587524 = 0.0030611110851168633 + 0.1 * 6.3608245849609375
Epoch 1500, val loss: 1.6615089178085327
Epoch 1510, training loss: 0.6408538222312927 = 0.003022185293957591 + 0.1 * 6.3783159255981445
Epoch 1510, val loss: 1.6643985509872437
Epoch 1520, training loss: 0.6392084956169128 = 0.002983818296343088 + 0.1 * 6.362246990203857
Epoch 1520, val loss: 1.6670963764190674
Epoch 1530, training loss: 0.6390312314033508 = 0.0029467639978975058 + 0.1 * 6.360844612121582
Epoch 1530, val loss: 1.6699259281158447
Epoch 1540, training loss: 0.6388872861862183 = 0.002910641487687826 + 0.1 * 6.359766483306885
Epoch 1540, val loss: 1.6727168560028076
Epoch 1550, training loss: 0.6391183733940125 = 0.002875079633668065 + 0.1 * 6.362432956695557
Epoch 1550, val loss: 1.675382375717163
Epoch 1560, training loss: 0.6400414109230042 = 0.0028403971809893847 + 0.1 * 6.372009754180908
Epoch 1560, val loss: 1.6780476570129395
Epoch 1570, training loss: 0.6385050415992737 = 0.0028065238147974014 + 0.1 * 6.356984615325928
Epoch 1570, val loss: 1.6807482242584229
Epoch 1580, training loss: 0.6383855938911438 = 0.002773497486487031 + 0.1 * 6.356120586395264
Epoch 1580, val loss: 1.6834276914596558
Epoch 1590, training loss: 0.6385583877563477 = 0.00274095986969769 + 0.1 * 6.358173847198486
Epoch 1590, val loss: 1.6860016584396362
Epoch 1600, training loss: 0.6379316449165344 = 0.0027091039810329676 + 0.1 * 6.352225303649902
Epoch 1600, val loss: 1.6885827779769897
Epoch 1610, training loss: 0.6403830051422119 = 0.0026779731269925833 + 0.1 * 6.377050399780273
Epoch 1610, val loss: 1.6911436319351196
Epoch 1620, training loss: 0.6379320621490479 = 0.0026475845370441675 + 0.1 * 6.352844715118408
Epoch 1620, val loss: 1.6936830282211304
Epoch 1630, training loss: 0.6387806534767151 = 0.002617943799123168 + 0.1 * 6.361626625061035
Epoch 1630, val loss: 1.6962140798568726
Epoch 1640, training loss: 0.6375152468681335 = 0.002588744042441249 + 0.1 * 6.349264621734619
Epoch 1640, val loss: 1.6986790895462036
Epoch 1650, training loss: 0.6385269165039062 = 0.0025601142551749945 + 0.1 * 6.359667778015137
Epoch 1650, val loss: 1.7010809183120728
Epoch 1660, training loss: 0.6380181312561035 = 0.002532137790694833 + 0.1 * 6.354859828948975
Epoch 1660, val loss: 1.703495979309082
Epoch 1670, training loss: 0.6380578875541687 = 0.0025046614464372396 + 0.1 * 6.355532169342041
Epoch 1670, val loss: 1.7058964967727661
Epoch 1680, training loss: 0.6381374001502991 = 0.0024779015220701694 + 0.1 * 6.356595039367676
Epoch 1680, val loss: 1.7082630395889282
Epoch 1690, training loss: 0.6367124319076538 = 0.0024516743142157793 + 0.1 * 6.342607498168945
Epoch 1690, val loss: 1.710606575012207
Epoch 1700, training loss: 0.6373715400695801 = 0.002425957005470991 + 0.1 * 6.349455833435059
Epoch 1700, val loss: 1.7129297256469727
Epoch 1710, training loss: 0.637086033821106 = 0.0024004727602005005 + 0.1 * 6.346855640411377
Epoch 1710, val loss: 1.7151912450790405
Epoch 1720, training loss: 0.6373340487480164 = 0.0023756118025630713 + 0.1 * 6.349584102630615
Epoch 1720, val loss: 1.71743905544281
Epoch 1730, training loss: 0.6369661688804626 = 0.0023512626066803932 + 0.1 * 6.346148490905762
Epoch 1730, val loss: 1.7197003364562988
Epoch 1740, training loss: 0.6380234956741333 = 0.0023273807018995285 + 0.1 * 6.356960773468018
Epoch 1740, val loss: 1.7219078540802002
Epoch 1750, training loss: 0.6371756196022034 = 0.0023039747029542923 + 0.1 * 6.3487162590026855
Epoch 1750, val loss: 1.7240709066390991
Epoch 1760, training loss: 0.636829137802124 = 0.002281036227941513 + 0.1 * 6.345480918884277
Epoch 1760, val loss: 1.7262414693832397
Epoch 1770, training loss: 0.6369355320930481 = 0.0022585121914744377 + 0.1 * 6.3467698097229
Epoch 1770, val loss: 1.7283828258514404
Epoch 1780, training loss: 0.6364834904670715 = 0.0022364624310284853 + 0.1 * 6.342470169067383
Epoch 1780, val loss: 1.7305305004119873
Epoch 1790, training loss: 0.6371285915374756 = 0.0022147062700241804 + 0.1 * 6.349138259887695
Epoch 1790, val loss: 1.732597827911377
Epoch 1800, training loss: 0.6363463997840881 = 0.0021933685056865215 + 0.1 * 6.341529846191406
Epoch 1800, val loss: 1.734654188156128
Epoch 1810, training loss: 0.6370998620986938 = 0.002172484528273344 + 0.1 * 6.349273681640625
Epoch 1810, val loss: 1.7367359399795532
Epoch 1820, training loss: 0.6361218094825745 = 0.0021519125439226627 + 0.1 * 6.339698791503906
Epoch 1820, val loss: 1.7387727499008179
Epoch 1830, training loss: 0.6374696493148804 = 0.002131720306351781 + 0.1 * 6.353379249572754
Epoch 1830, val loss: 1.7407810688018799
Epoch 1840, training loss: 0.6355826258659363 = 0.002111923648044467 + 0.1 * 6.334706783294678
Epoch 1840, val loss: 1.7427585124969482
Epoch 1850, training loss: 0.6366984248161316 = 0.002092494396492839 + 0.1 * 6.346059322357178
Epoch 1850, val loss: 1.7447123527526855
Epoch 1860, training loss: 0.6363123059272766 = 0.0020733452402055264 + 0.1 * 6.3423895835876465
Epoch 1860, val loss: 1.7466328144073486
Epoch 1870, training loss: 0.6367274522781372 = 0.002054566517472267 + 0.1 * 6.346728801727295
Epoch 1870, val loss: 1.7485158443450928
Epoch 1880, training loss: 0.6356916427612305 = 0.0020361815113574266 + 0.1 * 6.336554527282715
Epoch 1880, val loss: 1.7504700422286987
Epoch 1890, training loss: 0.637010931968689 = 0.002018161816522479 + 0.1 * 6.34992790222168
Epoch 1890, val loss: 1.7523759603500366
Epoch 1900, training loss: 0.6354725956916809 = 0.0020002585370093584 + 0.1 * 6.334722995758057
Epoch 1900, val loss: 1.7541921138763428
Epoch 1910, training loss: 0.635084331035614 = 0.001982754794880748 + 0.1 * 6.331015586853027
Epoch 1910, val loss: 1.7560464143753052
Epoch 1920, training loss: 0.6362998485565186 = 0.0019655711948871613 + 0.1 * 6.343342304229736
Epoch 1920, val loss: 1.7579272985458374
Epoch 1930, training loss: 0.6358011364936829 = 0.0019484516233205795 + 0.1 * 6.338526725769043
Epoch 1930, val loss: 1.7597019672393799
Epoch 1940, training loss: 0.6373395323753357 = 0.0019317794358357787 + 0.1 * 6.354077339172363
Epoch 1940, val loss: 1.761513113975525
Epoch 1950, training loss: 0.635210394859314 = 0.001915326458401978 + 0.1 * 6.332950592041016
Epoch 1950, val loss: 1.7633254528045654
Epoch 1960, training loss: 0.6356818675994873 = 0.0018992910627275705 + 0.1 * 6.337825775146484
Epoch 1960, val loss: 1.765152096748352
Epoch 1970, training loss: 0.6362304091453552 = 0.0018834038637578487 + 0.1 * 6.343470096588135
Epoch 1970, val loss: 1.7668633460998535
Epoch 1980, training loss: 0.6347997784614563 = 0.0018676795298233628 + 0.1 * 6.329320907592773
Epoch 1980, val loss: 1.7685521841049194
Epoch 1990, training loss: 0.6358413696289062 = 0.0018523450708016753 + 0.1 * 6.339890003204346
Epoch 1990, val loss: 1.7703222036361694
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.793809413909912 = 1.9341249465942383 + 0.1 * 8.596843719482422
Epoch 0, val loss: 1.9273157119750977
Epoch 10, training loss: 2.783555030822754 = 1.923880696296692 + 0.1 * 8.5967435836792
Epoch 10, val loss: 1.91764235496521
Epoch 20, training loss: 2.7707200050354004 = 1.9111037254333496 + 0.1 * 8.596161842346191
Epoch 20, val loss: 1.9051796197891235
Epoch 30, training loss: 2.752413034439087 = 1.8932873010635376 + 0.1 * 8.591257095336914
Epoch 30, val loss: 1.8874785900115967
Epoch 40, training loss: 2.7232110500335693 = 1.8674921989440918 + 0.1 * 8.557188987731934
Epoch 40, val loss: 1.861847996711731
Epoch 50, training loss: 2.669027328491211 = 1.8330061435699463 + 0.1 * 8.360212326049805
Epoch 50, val loss: 1.8290808200836182
Epoch 60, training loss: 2.5981767177581787 = 1.7952831983566284 + 0.1 * 8.028934478759766
Epoch 60, val loss: 1.7952568531036377
Epoch 70, training loss: 2.5320122241973877 = 1.7567166090011597 + 0.1 * 7.752955436706543
Epoch 70, val loss: 1.7614370584487915
Epoch 80, training loss: 2.4490761756896973 = 1.7132463455200195 + 0.1 * 7.3582987785339355
Epoch 80, val loss: 1.7222306728363037
Epoch 90, training loss: 2.3712077140808105 = 1.6591598987579346 + 0.1 * 7.120479106903076
Epoch 90, val loss: 1.6726062297821045
Epoch 100, training loss: 2.290165424346924 = 1.587752342224121 + 0.1 * 7.0241312980651855
Epoch 100, val loss: 1.6069846153259277
Epoch 110, training loss: 2.1973273754119873 = 1.5006722211837769 + 0.1 * 6.966550827026367
Epoch 110, val loss: 1.5294169187545776
Epoch 120, training loss: 2.09761643409729 = 1.4050366878509521 + 0.1 * 6.925797462463379
Epoch 120, val loss: 1.4475656747817993
Epoch 130, training loss: 1.9962844848632812 = 1.3066633939743042 + 0.1 * 6.896210193634033
Epoch 130, val loss: 1.3650699853897095
Epoch 140, training loss: 1.8956636190414429 = 1.2080157995224 + 0.1 * 6.87647819519043
Epoch 140, val loss: 1.284208059310913
Epoch 150, training loss: 1.798525333404541 = 1.1123383045196533 + 0.1 * 6.861870288848877
Epoch 150, val loss: 1.2071831226348877
Epoch 160, training loss: 1.7077431678771973 = 1.022629737854004 + 0.1 * 6.851133823394775
Epoch 160, val loss: 1.1367887258529663
Epoch 170, training loss: 1.6245644092559814 = 0.9402303695678711 + 0.1 * 6.8433403968811035
Epoch 170, val loss: 1.0737792253494263
Epoch 180, training loss: 1.5471827983856201 = 0.8633837699890137 + 0.1 * 6.837989807128906
Epoch 180, val loss: 1.0163475275039673
Epoch 190, training loss: 1.4730653762817383 = 0.7897833585739136 + 0.1 * 6.832819938659668
Epoch 190, val loss: 0.9617202281951904
Epoch 200, training loss: 1.4016387462615967 = 0.7187842726707458 + 0.1 * 6.828544616699219
Epoch 200, val loss: 0.909315824508667
Epoch 210, training loss: 1.333056926727295 = 0.6505630016326904 + 0.1 * 6.824939250946045
Epoch 210, val loss: 0.8596296310424805
Epoch 220, training loss: 1.267021894454956 = 0.5848087072372437 + 0.1 * 6.822132587432861
Epoch 220, val loss: 0.8126475214958191
Epoch 230, training loss: 1.203714370727539 = 0.5218014717102051 + 0.1 * 6.819128036499023
Epoch 230, val loss: 0.7695251107215881
Epoch 240, training loss: 1.143437385559082 = 0.46176594495773315 + 0.1 * 6.816714763641357
Epoch 240, val loss: 0.7310243248939514
Epoch 250, training loss: 1.0866281986236572 = 0.4051278829574585 + 0.1 * 6.81500244140625
Epoch 250, val loss: 0.6978135704994202
Epoch 260, training loss: 1.0342140197753906 = 0.35304442048072815 + 0.1 * 6.811695575714111
Epoch 260, val loss: 0.6703954935073853
Epoch 270, training loss: 0.9872319102287292 = 0.30630385875701904 + 0.1 * 6.8092803955078125
Epoch 270, val loss: 0.6485045552253723
Epoch 280, training loss: 0.9459713697433472 = 0.26548120379447937 + 0.1 * 6.804902076721191
Epoch 280, val loss: 0.6318002939224243
Epoch 290, training loss: 0.910331666469574 = 0.23050324618816376 + 0.1 * 6.79828405380249
Epoch 290, val loss: 0.6198831796646118
Epoch 300, training loss: 0.8798174858093262 = 0.20073291659355164 + 0.1 * 6.7908453941345215
Epoch 300, val loss: 0.6119116544723511
Epoch 310, training loss: 0.8542938828468323 = 0.1754962056875229 + 0.1 * 6.7879767417907715
Epoch 310, val loss: 0.6073567867279053
Epoch 320, training loss: 0.8323640823364258 = 0.15422561764717102 + 0.1 * 6.781383991241455
Epoch 320, val loss: 0.6054571270942688
Epoch 330, training loss: 0.8133124113082886 = 0.1360747069120407 + 0.1 * 6.772376537322998
Epoch 330, val loss: 0.605866014957428
Epoch 340, training loss: 0.7971648573875427 = 0.12049087136983871 + 0.1 * 6.766739845275879
Epoch 340, val loss: 0.6082005500793457
Epoch 350, training loss: 0.7837780714035034 = 0.10718461126089096 + 0.1 * 6.765934944152832
Epoch 350, val loss: 0.6120651960372925
Epoch 360, training loss: 0.7711679935455322 = 0.09578351676464081 + 0.1 * 6.753844738006592
Epoch 360, val loss: 0.6172634959220886
Epoch 370, training loss: 0.7602484226226807 = 0.08589525520801544 + 0.1 * 6.743531703948975
Epoch 370, val loss: 0.6234631538391113
Epoch 380, training loss: 0.7508262395858765 = 0.07729698717594147 + 0.1 * 6.735292434692383
Epoch 380, val loss: 0.6304661631584167
Epoch 390, training loss: 0.7427080869674683 = 0.06982368975877762 + 0.1 * 6.728844165802002
Epoch 390, val loss: 0.6380242109298706
Epoch 400, training loss: 0.7357591986656189 = 0.06327037513256073 + 0.1 * 6.724887847900391
Epoch 400, val loss: 0.6461151242256165
Epoch 410, training loss: 0.7294813990592957 = 0.05749917030334473 + 0.1 * 6.719822406768799
Epoch 410, val loss: 0.6545001268386841
Epoch 420, training loss: 0.723975419998169 = 0.052407123148441315 + 0.1 * 6.715682506561279
Epoch 420, val loss: 0.6631401777267456
Epoch 430, training loss: 0.7196897268295288 = 0.047927968204021454 + 0.1 * 6.717617511749268
Epoch 430, val loss: 0.67178875207901
Epoch 440, training loss: 0.7145299315452576 = 0.043968938291072845 + 0.1 * 6.705610275268555
Epoch 440, val loss: 0.6804413199424744
Epoch 450, training loss: 0.710218071937561 = 0.04045257344841957 + 0.1 * 6.697654724121094
Epoch 450, val loss: 0.6890990138053894
Epoch 460, training loss: 0.7066603899002075 = 0.037320319563150406 + 0.1 * 6.693400859832764
Epoch 460, val loss: 0.6976965069770813
Epoch 470, training loss: 0.703875720500946 = 0.03451903164386749 + 0.1 * 6.693566799163818
Epoch 470, val loss: 0.7062057852745056
Epoch 480, training loss: 0.7006340026855469 = 0.0320110097527504 + 0.1 * 6.686229705810547
Epoch 480, val loss: 0.7146089673042297
Epoch 490, training loss: 0.6979266405105591 = 0.02976073883473873 + 0.1 * 6.68165922164917
Epoch 490, val loss: 0.7228264212608337
Epoch 500, training loss: 0.6967424154281616 = 0.027734246104955673 + 0.1 * 6.69008207321167
Epoch 500, val loss: 0.7309373021125793
Epoch 510, training loss: 0.6935343146324158 = 0.02591053955256939 + 0.1 * 6.6762375831604
Epoch 510, val loss: 0.7388532161712646
Epoch 520, training loss: 0.6909880638122559 = 0.024259863421320915 + 0.1 * 6.6672821044921875
Epoch 520, val loss: 0.746614396572113
Epoch 530, training loss: 0.6898442506790161 = 0.022763684391975403 + 0.1 * 6.670805931091309
Epoch 530, val loss: 0.7541497349739075
Epoch 540, training loss: 0.687110185623169 = 0.02141025848686695 + 0.1 * 6.656998634338379
Epoch 540, val loss: 0.7615338563919067
Epoch 550, training loss: 0.685606062412262 = 0.020173661410808563 + 0.1 * 6.654324054718018
Epoch 550, val loss: 0.7687864899635315
Epoch 560, training loss: 0.6838005185127258 = 0.019043590873479843 + 0.1 * 6.647569179534912
Epoch 560, val loss: 0.7757798433303833
Epoch 570, training loss: 0.6826931238174438 = 0.018010718747973442 + 0.1 * 6.646823883056641
Epoch 570, val loss: 0.7826780676841736
Epoch 580, training loss: 0.6819707155227661 = 0.017061909660696983 + 0.1 * 6.649087905883789
Epoch 580, val loss: 0.7893280386924744
Epoch 590, training loss: 0.6799489259719849 = 0.016192659735679626 + 0.1 * 6.637562274932861
Epoch 590, val loss: 0.7958466410636902
Epoch 600, training loss: 0.6804690957069397 = 0.015390514396131039 + 0.1 * 6.65078592300415
Epoch 600, val loss: 0.8021917343139648
Epoch 610, training loss: 0.6774641871452332 = 0.014651920646429062 + 0.1 * 6.628122806549072
Epoch 610, val loss: 0.808362603187561
Epoch 620, training loss: 0.6768651604652405 = 0.013967880047857761 + 0.1 * 6.62897253036499
Epoch 620, val loss: 0.8144300580024719
Epoch 630, training loss: 0.6745548248291016 = 0.013335413299500942 + 0.1 * 6.612194061279297
Epoch 630, val loss: 0.8202817440032959
Epoch 640, training loss: 0.6730942726135254 = 0.012748925015330315 + 0.1 * 6.603453636169434
Epoch 640, val loss: 0.8260545134544373
Epoch 650, training loss: 0.6724444031715393 = 0.012201632373034954 + 0.1 * 6.602427959442139
Epoch 650, val loss: 0.8316752910614014
Epoch 660, training loss: 0.6719949841499329 = 0.011691953055560589 + 0.1 * 6.603030681610107
Epoch 660, val loss: 0.8371183276176453
Epoch 670, training loss: 0.6705242395401001 = 0.011217999272048473 + 0.1 * 6.593062400817871
Epoch 670, val loss: 0.8424567580223083
Epoch 680, training loss: 0.6697658896446228 = 0.010774033144116402 + 0.1 * 6.589918613433838
Epoch 680, val loss: 0.847646951675415
Epoch 690, training loss: 0.669029712677002 = 0.010358171537518501 + 0.1 * 6.5867156982421875
Epoch 690, val loss: 0.8526977300643921
Epoch 700, training loss: 0.6678242683410645 = 0.009969222359359264 + 0.1 * 6.578550338745117
Epoch 700, val loss: 0.8576520085334778
Epoch 710, training loss: 0.6666317582130432 = 0.009603049606084824 + 0.1 * 6.570287227630615
Epoch 710, val loss: 0.862492561340332
Epoch 720, training loss: 0.6658011674880981 = 0.009258979931473732 + 0.1 * 6.5654215812683105
Epoch 720, val loss: 0.8671847581863403
Epoch 730, training loss: 0.66542649269104 = 0.008935246616601944 + 0.1 * 6.56491231918335
Epoch 730, val loss: 0.8718156814575195
Epoch 740, training loss: 0.6658559441566467 = 0.008628967218101025 + 0.1 * 6.572269916534424
Epoch 740, val loss: 0.8763354420661926
Epoch 750, training loss: 0.6634681820869446 = 0.008340875618159771 + 0.1 * 6.551272869110107
Epoch 750, val loss: 0.880708634853363
Epoch 760, training loss: 0.6625890135765076 = 0.00806895550340414 + 0.1 * 6.545200347900391
Epoch 760, val loss: 0.8850367665290833
Epoch 770, training loss: 0.6631847023963928 = 0.007810650859028101 + 0.1 * 6.553740501403809
Epoch 770, val loss: 0.8892519474029541
Epoch 780, training loss: 0.6610772013664246 = 0.007567178457975388 + 0.1 * 6.53510046005249
Epoch 780, val loss: 0.8933690190315247
Epoch 790, training loss: 0.6623994708061218 = 0.00733608053997159 + 0.1 * 6.550633907318115
Epoch 790, val loss: 0.8974425792694092
Epoch 800, training loss: 0.660041332244873 = 0.00711654219776392 + 0.1 * 6.529247760772705
Epoch 800, val loss: 0.9014431238174438
Epoch 810, training loss: 0.6589594483375549 = 0.006907833740115166 + 0.1 * 6.520516395568848
Epoch 810, val loss: 0.9053907990455627
Epoch 820, training loss: 0.6594230532646179 = 0.006708870176225901 + 0.1 * 6.527141571044922
Epoch 820, val loss: 0.9092570543289185
Epoch 830, training loss: 0.6590078473091125 = 0.006520271301269531 + 0.1 * 6.524875640869141
Epoch 830, val loss: 0.9130344390869141
Epoch 840, training loss: 0.658568799495697 = 0.0063409944996237755 + 0.1 * 6.522278308868408
Epoch 840, val loss: 0.9167848229408264
Epoch 850, training loss: 0.6573700308799744 = 0.006169528234750032 + 0.1 * 6.512004375457764
Epoch 850, val loss: 0.9204394817352295
Epoch 860, training loss: 0.6574015021324158 = 0.006006010342389345 + 0.1 * 6.5139546394348145
Epoch 860, val loss: 0.9240508675575256
Epoch 870, training loss: 0.657526433467865 = 0.005849333014339209 + 0.1 * 6.516770362854004
Epoch 870, val loss: 0.9275591373443604
Epoch 880, training loss: 0.6556089520454407 = 0.005700025241822004 + 0.1 * 6.499088764190674
Epoch 880, val loss: 0.9310570359230042
Epoch 890, training loss: 0.6549971699714661 = 0.005556977353990078 + 0.1 * 6.494401454925537
Epoch 890, val loss: 0.9344645738601685
Epoch 900, training loss: 0.6563754081726074 = 0.0054197898134589195 + 0.1 * 6.509556293487549
Epoch 900, val loss: 0.9378299713134766
Epoch 910, training loss: 0.6551485061645508 = 0.005288622807711363 + 0.1 * 6.498599052429199
Epoch 910, val loss: 0.9411242604255676
Epoch 920, training loss: 0.6546416878700256 = 0.005163335241377354 + 0.1 * 6.494783878326416
Epoch 920, val loss: 0.9443766474723816
Epoch 930, training loss: 0.6538324952125549 = 0.005042918957769871 + 0.1 * 6.487895488739014
Epoch 930, val loss: 0.9475623965263367
Epoch 940, training loss: 0.6528329849243164 = 0.004927211906760931 + 0.1 * 6.479057312011719
Epoch 940, val loss: 0.9506721496582031
Epoch 950, training loss: 0.6527748703956604 = 0.0048162974417209625 + 0.1 * 6.479585647583008
Epoch 950, val loss: 0.9537510871887207
Epoch 960, training loss: 0.6536704301834106 = 0.004709582310169935 + 0.1 * 6.489608287811279
Epoch 960, val loss: 0.9568189978599548
Epoch 970, training loss: 0.6527612805366516 = 0.004606653470546007 + 0.1 * 6.481545925140381
Epoch 970, val loss: 0.959780752658844
Epoch 980, training loss: 0.6520923972129822 = 0.0045081269927322865 + 0.1 * 6.475842475891113
Epoch 980, val loss: 0.962699830532074
Epoch 990, training loss: 0.651244044303894 = 0.004413198679685593 + 0.1 * 6.468308448791504
Epoch 990, val loss: 0.9655898213386536
Epoch 1000, training loss: 0.6536456346511841 = 0.00432127108797431 + 0.1 * 6.493243217468262
Epoch 1000, val loss: 0.9684244990348816
Epoch 1010, training loss: 0.6514739394187927 = 0.004232652019709349 + 0.1 * 6.472413063049316
Epoch 1010, val loss: 0.9711923003196716
Epoch 1020, training loss: 0.6507261991500854 = 0.004147688392549753 + 0.1 * 6.465785026550293
Epoch 1020, val loss: 0.9739162921905518
Epoch 1030, training loss: 0.6503452658653259 = 0.004066166467964649 + 0.1 * 6.4627909660339355
Epoch 1030, val loss: 0.976595401763916
Epoch 1040, training loss: 0.6506748795509338 = 0.003987110685557127 + 0.1 * 6.466877460479736
Epoch 1040, val loss: 0.9792323112487793
Epoch 1050, training loss: 0.6495428085327148 = 0.003910419065505266 + 0.1 * 6.456323623657227
Epoch 1050, val loss: 0.9818543791770935
Epoch 1060, training loss: 0.6489807367324829 = 0.0038365551736205816 + 0.1 * 6.451441287994385
Epoch 1060, val loss: 0.984406054019928
Epoch 1070, training loss: 0.6496813297271729 = 0.0037649809382855892 + 0.1 * 6.459163188934326
Epoch 1070, val loss: 0.9869626760482788
Epoch 1080, training loss: 0.6490905284881592 = 0.0036957741249352694 + 0.1 * 6.453947067260742
Epoch 1080, val loss: 0.9894338846206665
Epoch 1090, training loss: 0.6493021249771118 = 0.0036291310098022223 + 0.1 * 6.456729412078857
Epoch 1090, val loss: 0.9919123649597168
Epoch 1100, training loss: 0.6483586430549622 = 0.0035642171278595924 + 0.1 * 6.447944164276123
Epoch 1100, val loss: 0.9943171739578247
Epoch 1110, training loss: 0.6485131978988647 = 0.0035014860332012177 + 0.1 * 6.450117111206055
Epoch 1110, val loss: 0.9966776371002197
Epoch 1120, training loss: 0.6484337449073792 = 0.0034410529769957066 + 0.1 * 6.449926853179932
Epoch 1120, val loss: 0.9990527033805847
Epoch 1130, training loss: 0.6472036838531494 = 0.0033823344856500626 + 0.1 * 6.438213348388672
Epoch 1130, val loss: 1.001361608505249
Epoch 1140, training loss: 0.6487223505973816 = 0.003325355937704444 + 0.1 * 6.453969955444336
Epoch 1140, val loss: 1.003633975982666
Epoch 1150, training loss: 0.6469029784202576 = 0.0032698980066925287 + 0.1 * 6.436330795288086
Epoch 1150, val loss: 1.005867838859558
Epoch 1160, training loss: 0.6462963819503784 = 0.003216424025595188 + 0.1 * 6.43079948425293
Epoch 1160, val loss: 1.0080845355987549
Epoch 1170, training loss: 0.6469528079032898 = 0.0031642892863601446 + 0.1 * 6.43788480758667
Epoch 1170, val loss: 1.0102728605270386
Epoch 1180, training loss: 0.6465435028076172 = 0.0031137587502598763 + 0.1 * 6.434297561645508
Epoch 1180, val loss: 1.0123746395111084
Epoch 1190, training loss: 0.6475043296813965 = 0.0030649008695036173 + 0.1 * 6.444393634796143
Epoch 1190, val loss: 1.0144797563552856
Epoch 1200, training loss: 0.6469904184341431 = 0.0030172739643603563 + 0.1 * 6.439731597900391
Epoch 1200, val loss: 1.016587257385254
Epoch 1210, training loss: 0.6460013389587402 = 0.002970990026369691 + 0.1 * 6.43030309677124
Epoch 1210, val loss: 1.0185965299606323
Epoch 1220, training loss: 0.6465532183647156 = 0.002926274435594678 + 0.1 * 6.436269283294678
Epoch 1220, val loss: 1.020647406578064
Epoch 1230, training loss: 0.644816517829895 = 0.0028822757303714752 + 0.1 * 6.419342041015625
Epoch 1230, val loss: 1.0226218700408936
Epoch 1240, training loss: 0.6466546654701233 = 0.002839815802872181 + 0.1 * 6.438148498535156
Epoch 1240, val loss: 1.0245970487594604
Epoch 1250, training loss: 0.6447227001190186 = 0.002798287197947502 + 0.1 * 6.419244289398193
Epoch 1250, val loss: 1.0264967679977417
Epoch 1260, training loss: 0.6449978947639465 = 0.002758151153102517 + 0.1 * 6.422397613525391
Epoch 1260, val loss: 1.0284112691879272
Epoch 1270, training loss: 0.6441494226455688 = 0.0027191550470888615 + 0.1 * 6.414302349090576
Epoch 1270, val loss: 1.0302972793579102
Epoch 1280, training loss: 0.6447657346725464 = 0.0026807019021362066 + 0.1 * 6.420849800109863
Epoch 1280, val loss: 1.03213632106781
Epoch 1290, training loss: 0.6439140439033508 = 0.0026436089538037777 + 0.1 * 6.4127044677734375
Epoch 1290, val loss: 1.0339401960372925
Epoch 1300, training loss: 0.6451008915901184 = 0.002607431262731552 + 0.1 * 6.4249348640441895
Epoch 1300, val loss: 1.0357617139816284
Epoch 1310, training loss: 0.644670844078064 = 0.0025719564873725176 + 0.1 * 6.420989036560059
Epoch 1310, val loss: 1.0375314950942993
Epoch 1320, training loss: 0.643670916557312 = 0.0025375885888934135 + 0.1 * 6.411333084106445
Epoch 1320, val loss: 1.0393081903457642
Epoch 1330, training loss: 0.6431487202644348 = 0.0025039620231837034 + 0.1 * 6.406446933746338
Epoch 1330, val loss: 1.0410466194152832
Epoch 1340, training loss: 0.6434221267700195 = 0.002471265383064747 + 0.1 * 6.409508228302002
Epoch 1340, val loss: 1.0428060293197632
Epoch 1350, training loss: 0.6441073417663574 = 0.002439159667119384 + 0.1 * 6.41668176651001
Epoch 1350, val loss: 1.044509768486023
Epoch 1360, training loss: 0.6433330774307251 = 0.0024079803843051195 + 0.1 * 6.409250736236572
Epoch 1360, val loss: 1.0461786985397339
Epoch 1370, training loss: 0.6427484154701233 = 0.0023777512833476067 + 0.1 * 6.4037065505981445
Epoch 1370, val loss: 1.0478594303131104
Epoch 1380, training loss: 0.643276572227478 = 0.002348091686144471 + 0.1 * 6.409285068511963
Epoch 1380, val loss: 1.0495059490203857
Epoch 1390, training loss: 0.6429396867752075 = 0.002319015795364976 + 0.1 * 6.4062066078186035
Epoch 1390, val loss: 1.0511295795440674
Epoch 1400, training loss: 0.6423594355583191 = 0.002290582051500678 + 0.1 * 6.400688171386719
Epoch 1400, val loss: 1.0527342557907104
Epoch 1410, training loss: 0.6431294679641724 = 0.002263037022203207 + 0.1 * 6.408664703369141
Epoch 1410, val loss: 1.0543529987335205
Epoch 1420, training loss: 0.6423940062522888 = 0.002235731342807412 + 0.1 * 6.401582717895508
Epoch 1420, val loss: 1.055944561958313
Epoch 1430, training loss: 0.6434891223907471 = 0.002209289465099573 + 0.1 * 6.412797927856445
Epoch 1430, val loss: 1.0575079917907715
Epoch 1440, training loss: 0.6423081159591675 = 0.002183453645557165 + 0.1 * 6.401246547698975
Epoch 1440, val loss: 1.0590876340866089
Epoch 1450, training loss: 0.6425009369850159 = 0.0021582196932286024 + 0.1 * 6.4034271240234375
Epoch 1450, val loss: 1.0606404542922974
Epoch 1460, training loss: 0.6416493058204651 = 0.0021334609482437372 + 0.1 * 6.395157814025879
Epoch 1460, val loss: 1.0621649026870728
Epoch 1470, training loss: 0.641680896282196 = 0.002109273336827755 + 0.1 * 6.395716190338135
Epoch 1470, val loss: 1.0636757612228394
Epoch 1480, training loss: 0.6425066590309143 = 0.0020855646580457687 + 0.1 * 6.404211044311523
Epoch 1480, val loss: 1.0651507377624512
Epoch 1490, training loss: 0.6404649019241333 = 0.0020625628530979156 + 0.1 * 6.384023189544678
Epoch 1490, val loss: 1.066650152206421
Epoch 1500, training loss: 0.6409534215927124 = 0.0020400250796228647 + 0.1 * 6.389133930206299
Epoch 1500, val loss: 1.068137288093567
Epoch 1510, training loss: 0.6420466899871826 = 0.002017717808485031 + 0.1 * 6.400289535522461
Epoch 1510, val loss: 1.0695909261703491
Epoch 1520, training loss: 0.6419245600700378 = 0.0019960342906415462 + 0.1 * 6.399285316467285
Epoch 1520, val loss: 1.0710338354110718
Epoch 1530, training loss: 0.6419252753257751 = 0.001974669983610511 + 0.1 * 6.399506092071533
Epoch 1530, val loss: 1.0724353790283203
Epoch 1540, training loss: 0.640434205532074 = 0.0019539697095751762 + 0.1 * 6.384802341461182
Epoch 1540, val loss: 1.0738606452941895
Epoch 1550, training loss: 0.6404685974121094 = 0.0019336454570293427 + 0.1 * 6.385349750518799
Epoch 1550, val loss: 1.0752537250518799
Epoch 1560, training loss: 0.6407608985900879 = 0.0019136244663968682 + 0.1 * 6.388472557067871
Epoch 1560, val loss: 1.076658844947815
Epoch 1570, training loss: 0.639649510383606 = 0.0018939328147098422 + 0.1 * 6.377555847167969
Epoch 1570, val loss: 1.0780341625213623
Epoch 1580, training loss: 0.641095757484436 = 0.001874774694442749 + 0.1 * 6.392209529876709
Epoch 1580, val loss: 1.0794204473495483
Epoch 1590, training loss: 0.6398433446884155 = 0.0018558958545327187 + 0.1 * 6.379874229431152
Epoch 1590, val loss: 1.0807520151138306
Epoch 1600, training loss: 0.6398597359657288 = 0.0018375387880951166 + 0.1 * 6.380221843719482
Epoch 1600, val loss: 1.0821175575256348
Epoch 1610, training loss: 0.6409775614738464 = 0.0018194197909906507 + 0.1 * 6.391581058502197
Epoch 1610, val loss: 1.0834788084030151
Epoch 1620, training loss: 0.6393543481826782 = 0.001801541424356401 + 0.1 * 6.375527858734131
Epoch 1620, val loss: 1.084777593612671
Epoch 1630, training loss: 0.6396061778068542 = 0.0017841843655332923 + 0.1 * 6.3782196044921875
Epoch 1630, val loss: 1.0860986709594727
Epoch 1640, training loss: 0.6391481757164001 = 0.001767067238688469 + 0.1 * 6.3738112449646
Epoch 1640, val loss: 1.0874024629592896
Epoch 1650, training loss: 0.6398636698722839 = 0.001750339288264513 + 0.1 * 6.381133079528809
Epoch 1650, val loss: 1.0886950492858887
Epoch 1660, training loss: 0.6387239694595337 = 0.0017338149482384324 + 0.1 * 6.369901180267334
Epoch 1660, val loss: 1.0899499654769897
Epoch 1670, training loss: 0.6387816667556763 = 0.0017177086556330323 + 0.1 * 6.370639801025391
Epoch 1670, val loss: 1.0912281274795532
Epoch 1680, training loss: 0.6386098861694336 = 0.001701976521871984 + 0.1 * 6.369079113006592
Epoch 1680, val loss: 1.0925052165985107
Epoch 1690, training loss: 0.6383703351020813 = 0.0016862263437360525 + 0.1 * 6.3668413162231445
Epoch 1690, val loss: 1.0937273502349854
Epoch 1700, training loss: 0.6384367346763611 = 0.0016709385672584176 + 0.1 * 6.367657661437988
Epoch 1700, val loss: 1.094963550567627
Epoch 1710, training loss: 0.6383858323097229 = 0.0016559252981096506 + 0.1 * 6.3672990798950195
Epoch 1710, val loss: 1.0961891412734985
Epoch 1720, training loss: 0.6384953260421753 = 0.0016412589466199279 + 0.1 * 6.368540287017822
Epoch 1720, val loss: 1.0974172353744507
Epoch 1730, training loss: 0.638736367225647 = 0.0016268447507172823 + 0.1 * 6.371095180511475
Epoch 1730, val loss: 1.0986322164535522
Epoch 1740, training loss: 0.6374268531799316 = 0.0016124977264553308 + 0.1 * 6.3581438064575195
Epoch 1740, val loss: 1.0998188257217407
Epoch 1750, training loss: 0.6390515565872192 = 0.001598534407094121 + 0.1 * 6.374529838562012
Epoch 1750, val loss: 1.101008653640747
Epoch 1760, training loss: 0.6386791467666626 = 0.001584674697369337 + 0.1 * 6.370944976806641
Epoch 1760, val loss: 1.102195143699646
Epoch 1770, training loss: 0.6385734677314758 = 0.0015711394371464849 + 0.1 * 6.370023250579834
Epoch 1770, val loss: 1.103338599205017
Epoch 1780, training loss: 0.6381838917732239 = 0.0015578179154545069 + 0.1 * 6.366260528564453
Epoch 1780, val loss: 1.104516625404358
Epoch 1790, training loss: 0.6373178362846375 = 0.0015447144396603107 + 0.1 * 6.357731342315674
Epoch 1790, val loss: 1.1056442260742188
Epoch 1800, training loss: 0.6398807764053345 = 0.0015318074729293585 + 0.1 * 6.383489608764648
Epoch 1800, val loss: 1.1067860126495361
Epoch 1810, training loss: 0.6372420191764832 = 0.0015190665144473314 + 0.1 * 6.357229232788086
Epoch 1810, val loss: 1.1079189777374268
Epoch 1820, training loss: 0.6372644901275635 = 0.0015066262567415833 + 0.1 * 6.357578754425049
Epoch 1820, val loss: 1.1090410947799683
Epoch 1830, training loss: 0.6381868720054626 = 0.0014943170826882124 + 0.1 * 6.3669257164001465
Epoch 1830, val loss: 1.1101579666137695
Epoch 1840, training loss: 0.6377177238464355 = 0.0014821812510490417 + 0.1 * 6.3623552322387695
Epoch 1840, val loss: 1.1112536191940308
Epoch 1850, training loss: 0.6375827789306641 = 0.0014702986227348447 + 0.1 * 6.361124515533447
Epoch 1850, val loss: 1.1123428344726562
Epoch 1860, training loss: 0.6369297504425049 = 0.001458614831790328 + 0.1 * 6.354711532592773
Epoch 1860, val loss: 1.1134283542633057
Epoch 1870, training loss: 0.6373594403266907 = 0.001447054324671626 + 0.1 * 6.359123229980469
Epoch 1870, val loss: 1.1145117282867432
Epoch 1880, training loss: 0.6366412043571472 = 0.0014356571482494473 + 0.1 * 6.352055549621582
Epoch 1880, val loss: 1.1155580282211304
Epoch 1890, training loss: 0.6373079419136047 = 0.0014244444901123643 + 0.1 * 6.358835220336914
Epoch 1890, val loss: 1.1166218519210815
Epoch 1900, training loss: 0.636787474155426 = 0.0014134336961433291 + 0.1 * 6.353740692138672
Epoch 1900, val loss: 1.1176671981811523
Epoch 1910, training loss: 0.6369428634643555 = 0.0014025557320564985 + 0.1 * 6.355402946472168
Epoch 1910, val loss: 1.1187068223953247
Epoch 1920, training loss: 0.6367343664169312 = 0.0013918669428676367 + 0.1 * 6.353424549102783
Epoch 1920, val loss: 1.1197199821472168
Epoch 1930, training loss: 0.6362091302871704 = 0.0013812902616336942 + 0.1 * 6.348278522491455
Epoch 1930, val loss: 1.1207231283187866
Epoch 1940, training loss: 0.6369946599006653 = 0.0013709372142329812 + 0.1 * 6.356236934661865
Epoch 1940, val loss: 1.1217149496078491
Epoch 1950, training loss: 0.6363797187805176 = 0.0013606379507109523 + 0.1 * 6.35019063949585
Epoch 1950, val loss: 1.1226933002471924
Epoch 1960, training loss: 0.6367161273956299 = 0.0013505805982276797 + 0.1 * 6.3536553382873535
Epoch 1960, val loss: 1.1236748695373535
Epoch 1970, training loss: 0.6368903517723083 = 0.0013405987992882729 + 0.1 * 6.355496883392334
Epoch 1970, val loss: 1.1246354579925537
Epoch 1980, training loss: 0.6362302899360657 = 0.0013308193301782012 + 0.1 * 6.348994731903076
Epoch 1980, val loss: 1.1255837678909302
Epoch 1990, training loss: 0.636648416519165 = 0.00132115394808352 + 0.1 * 6.353272438049316
Epoch 1990, val loss: 1.1265348196029663
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 2.7957286834716797 = 1.9360462427139282 + 0.1 * 8.59682559967041
Epoch 0, val loss: 1.9444937705993652
Epoch 10, training loss: 2.786008834838867 = 1.9263352155685425 + 0.1 * 8.596735954284668
Epoch 10, val loss: 1.9344356060028076
Epoch 20, training loss: 2.774106025695801 = 1.9144941568374634 + 0.1 * 8.59611988067627
Epoch 20, val loss: 1.9220538139343262
Epoch 30, training loss: 2.757006883621216 = 1.8979977369308472 + 0.1 * 8.590091705322266
Epoch 30, val loss: 1.9049692153930664
Epoch 40, training loss: 2.728389024734497 = 1.8739290237426758 + 0.1 * 8.544600486755371
Epoch 40, val loss: 1.8805944919586182
Epoch 50, training loss: 2.6684341430664062 = 1.8412795066833496 + 0.1 * 8.271547317504883
Epoch 50, val loss: 1.848775863647461
Epoch 60, training loss: 2.5983102321624756 = 1.8038344383239746 + 0.1 * 7.944758415222168
Epoch 60, val loss: 1.8136576414108276
Epoch 70, training loss: 2.526911735534668 = 1.765217900276184 + 0.1 * 7.616937160491943
Epoch 70, val loss: 1.7774714231491089
Epoch 80, training loss: 2.459150791168213 = 1.723543643951416 + 0.1 * 7.356070041656494
Epoch 80, val loss: 1.7386691570281982
Epoch 90, training loss: 2.3868160247802734 = 1.67067551612854 + 0.1 * 7.161405086517334
Epoch 90, val loss: 1.6900475025177002
Epoch 100, training loss: 2.3066020011901855 = 1.6000158786773682 + 0.1 * 7.065860748291016
Epoch 100, val loss: 1.625179409980774
Epoch 110, training loss: 2.2117435932159424 = 1.512332797050476 + 0.1 * 6.9941086769104
Epoch 110, val loss: 1.5480915307998657
Epoch 120, training loss: 2.1084213256835938 = 1.413672924041748 + 0.1 * 6.947483062744141
Epoch 120, val loss: 1.4633845090866089
Epoch 130, training loss: 2.003612518310547 = 1.3121038675308228 + 0.1 * 6.91508674621582
Epoch 130, val loss: 1.3794641494750977
Epoch 140, training loss: 1.901869297027588 = 1.213096261024475 + 0.1 * 6.887729644775391
Epoch 140, val loss: 1.2989909648895264
Epoch 150, training loss: 1.8073841333389282 = 1.1205192804336548 + 0.1 * 6.868648529052734
Epoch 150, val loss: 1.2263514995574951
Epoch 160, training loss: 1.717106580734253 = 1.032292127609253 + 0.1 * 6.848145008087158
Epoch 160, val loss: 1.1573195457458496
Epoch 170, training loss: 1.6319384574890137 = 0.9486821889877319 + 0.1 * 6.832562446594238
Epoch 170, val loss: 1.0919049978256226
Epoch 180, training loss: 1.552344799041748 = 0.8702672123908997 + 0.1 * 6.820775508880615
Epoch 180, val loss: 1.0300182104110718
Epoch 190, training loss: 1.4803574085235596 = 0.7987987995147705 + 0.1 * 6.815585136413574
Epoch 190, val loss: 0.973574697971344
Epoch 200, training loss: 1.4161689281463623 = 0.7358902096748352 + 0.1 * 6.802787780761719
Epoch 200, val loss: 0.9249406456947327
Epoch 210, training loss: 1.3602967262268066 = 0.6809379458427429 + 0.1 * 6.7935872077941895
Epoch 210, val loss: 0.8845009803771973
Epoch 220, training loss: 1.3146603107452393 = 0.6326106786727905 + 0.1 * 6.820496082305908
Epoch 220, val loss: 0.851758599281311
Epoch 230, training loss: 1.2685890197753906 = 0.5901360511779785 + 0.1 * 6.784530162811279
Epoch 230, val loss: 0.8261598944664001
Epoch 240, training loss: 1.227949619293213 = 0.5511108040809631 + 0.1 * 6.768388748168945
Epoch 240, val loss: 0.8054081201553345
Epoch 250, training loss: 1.1904335021972656 = 0.5144515037536621 + 0.1 * 6.759820461273193
Epoch 250, val loss: 0.7885026931762695
Epoch 260, training loss: 1.1553412675857544 = 0.47981563210487366 + 0.1 * 6.755256175994873
Epoch 260, val loss: 0.7751052975654602
Epoch 270, training loss: 1.121496319770813 = 0.4470195472240448 + 0.1 * 6.744767189025879
Epoch 270, val loss: 0.7649508714675903
Epoch 280, training loss: 1.0895159244537354 = 0.41549596190452576 + 0.1 * 6.740199565887451
Epoch 280, val loss: 0.7574185132980347
Epoch 290, training loss: 1.058092713356018 = 0.3850155770778656 + 0.1 * 6.730771541595459
Epoch 290, val loss: 0.7519174218177795
Epoch 300, training loss: 1.0273699760437012 = 0.3552762269973755 + 0.1 * 6.720938205718994
Epoch 300, val loss: 0.7479684948921204
Epoch 310, training loss: 0.9976148009300232 = 0.3260486125946045 + 0.1 * 6.715661525726318
Epoch 310, val loss: 0.7453574538230896
Epoch 320, training loss: 0.9679309725761414 = 0.29739540815353394 + 0.1 * 6.705355644226074
Epoch 320, val loss: 0.7438755035400391
Epoch 330, training loss: 0.94188392162323 = 0.2693873345851898 + 0.1 * 6.724966049194336
Epoch 330, val loss: 0.74330073595047
Epoch 340, training loss: 0.9124588370323181 = 0.2425246387720108 + 0.1 * 6.699341773986816
Epoch 340, val loss: 0.7434420585632324
Epoch 350, training loss: 0.8861231803894043 = 0.21700236201286316 + 0.1 * 6.6912078857421875
Epoch 350, val loss: 0.7442595362663269
Epoch 360, training loss: 0.861274242401123 = 0.19320327043533325 + 0.1 * 6.680709362030029
Epoch 360, val loss: 0.745910108089447
Epoch 370, training loss: 0.8391005396842957 = 0.17142343521118164 + 0.1 * 6.6767706871032715
Epoch 370, val loss: 0.7483484148979187
Epoch 380, training loss: 0.819312572479248 = 0.15179207921028137 + 0.1 * 6.675204277038574
Epoch 380, val loss: 0.7518557906150818
Epoch 390, training loss: 0.8007094860076904 = 0.13443249464035034 + 0.1 * 6.662769794464111
Epoch 390, val loss: 0.7561857104301453
Epoch 400, training loss: 0.7850315570831299 = 0.11922093480825424 + 0.1 * 6.658106327056885
Epoch 400, val loss: 0.761454164981842
Epoch 410, training loss: 0.7708074450492859 = 0.10594119876623154 + 0.1 * 6.648662567138672
Epoch 410, val loss: 0.7676967978477478
Epoch 420, training loss: 0.75908362865448 = 0.09440013766288757 + 0.1 * 6.646834850311279
Epoch 420, val loss: 0.7747868299484253
Epoch 430, training loss: 0.7486079931259155 = 0.08441772311925888 + 0.1 * 6.641902923583984
Epoch 430, val loss: 0.7825013995170593
Epoch 440, training loss: 0.7389497756958008 = 0.0757683664560318 + 0.1 * 6.631814002990723
Epoch 440, val loss: 0.7908292412757874
Epoch 450, training loss: 0.7329555153846741 = 0.06825243681669235 + 0.1 * 6.647030830383301
Epoch 450, val loss: 0.7996311783790588
Epoch 460, training loss: 0.7239779233932495 = 0.06174105033278465 + 0.1 * 6.622368812561035
Epoch 460, val loss: 0.8085895776748657
Epoch 470, training loss: 0.718866765499115 = 0.05605801194906235 + 0.1 * 6.628087520599365
Epoch 470, val loss: 0.817840576171875
Epoch 480, training loss: 0.7123603820800781 = 0.0510995090007782 + 0.1 * 6.612608909606934
Epoch 480, val loss: 0.8270640969276428
Epoch 490, training loss: 0.7068734169006348 = 0.0467427559196949 + 0.1 * 6.601306915283203
Epoch 490, val loss: 0.8362715840339661
Epoch 500, training loss: 0.7027708888053894 = 0.04289156198501587 + 0.1 * 6.598793029785156
Epoch 500, val loss: 0.8454346060752869
Epoch 510, training loss: 0.698989748954773 = 0.039483509957790375 + 0.1 * 6.595062732696533
Epoch 510, val loss: 0.8544469475746155
Epoch 520, training loss: 0.6952324509620667 = 0.03646383807063103 + 0.1 * 6.587685585021973
Epoch 520, val loss: 0.8632111549377441
Epoch 530, training loss: 0.6918613314628601 = 0.03377888351678848 + 0.1 * 6.58082389831543
Epoch 530, val loss: 0.8717677593231201
Epoch 540, training loss: 0.6887477040290833 = 0.03137040510773659 + 0.1 * 6.57377290725708
Epoch 540, val loss: 0.8801622986793518
Epoch 550, training loss: 0.6892311573028564 = 0.029203157871961594 + 0.1 * 6.600279808044434
Epoch 550, val loss: 0.8883540630340576
Epoch 560, training loss: 0.6851120591163635 = 0.027256634086370468 + 0.1 * 6.578554630279541
Epoch 560, val loss: 0.8962529301643372
Epoch 570, training loss: 0.6815661191940308 = 0.025498125702142715 + 0.1 * 6.5606794357299805
Epoch 570, val loss: 0.9039815664291382
Epoch 580, training loss: 0.6806749105453491 = 0.02390090376138687 + 0.1 * 6.567739486694336
Epoch 580, val loss: 0.9115726947784424
Epoch 590, training loss: 0.6792300343513489 = 0.02245485410094261 + 0.1 * 6.567751884460449
Epoch 590, val loss: 0.9188408255577087
Epoch 600, training loss: 0.676564633846283 = 0.02114458940923214 + 0.1 * 6.554200649261475
Epoch 600, val loss: 0.9259351491928101
Epoch 610, training loss: 0.6746244430541992 = 0.019948173314332962 + 0.1 * 6.546762466430664
Epoch 610, val loss: 0.9328463077545166
Epoch 620, training loss: 0.6737481355667114 = 0.01885303482413292 + 0.1 * 6.548951148986816
Epoch 620, val loss: 0.9396106004714966
Epoch 630, training loss: 0.671853244304657 = 0.017846060916781425 + 0.1 * 6.540071964263916
Epoch 630, val loss: 0.9461751580238342
Epoch 640, training loss: 0.6719051003456116 = 0.016921455040574074 + 0.1 * 6.549836158752441
Epoch 640, val loss: 0.9525589346885681
Epoch 650, training loss: 0.6695340275764465 = 0.016073239967226982 + 0.1 * 6.534607887268066
Epoch 650, val loss: 0.9587974548339844
Epoch 660, training loss: 0.6678949594497681 = 0.01528975274413824 + 0.1 * 6.526052474975586
Epoch 660, val loss: 0.9649006128311157
Epoch 670, training loss: 0.6708851456642151 = 0.014564392156898975 + 0.1 * 6.563207626342773
Epoch 670, val loss: 0.9708260893821716
Epoch 680, training loss: 0.6662312150001526 = 0.013895577751100063 + 0.1 * 6.5233564376831055
Epoch 680, val loss: 0.976564884185791
Epoch 690, training loss: 0.6664958000183105 = 0.01327576395124197 + 0.1 * 6.532200336456299
Epoch 690, val loss: 0.9822187423706055
Epoch 700, training loss: 0.6641210317611694 = 0.012699935585260391 + 0.1 * 6.5142107009887695
Epoch 700, val loss: 0.9876946210861206
Epoch 710, training loss: 0.6634237170219421 = 0.012163640931248665 + 0.1 * 6.512600898742676
Epoch 710, val loss: 0.9930741786956787
Epoch 720, training loss: 0.6643240451812744 = 0.011662651784718037 + 0.1 * 6.526613712310791
Epoch 720, val loss: 0.9983007311820984
Epoch 730, training loss: 0.6615157127380371 = 0.011196168139576912 + 0.1 * 6.503194808959961
Epoch 730, val loss: 1.0033636093139648
Epoch 740, training loss: 0.6602838635444641 = 0.01076040044426918 + 0.1 * 6.495234489440918
Epoch 740, val loss: 1.0083383321762085
Epoch 750, training loss: 0.6609678864479065 = 0.010351269505918026 + 0.1 * 6.506166458129883
Epoch 750, val loss: 1.013190746307373
Epoch 760, training loss: 0.6610700488090515 = 0.009967954829335213 + 0.1 * 6.511020660400391
Epoch 760, val loss: 1.0179007053375244
Epoch 770, training loss: 0.6583530306816101 = 0.009609991684556007 + 0.1 * 6.487430095672607
Epoch 770, val loss: 1.0225443840026855
Epoch 780, training loss: 0.6585155129432678 = 0.00927244033664465 + 0.1 * 6.492430210113525
Epoch 780, val loss: 1.0270203351974487
Epoch 790, training loss: 0.6566947102546692 = 0.008953575044870377 + 0.1 * 6.477411270141602
Epoch 790, val loss: 1.0314034223556519
Epoch 800, training loss: 0.6570590734481812 = 0.008652598597109318 + 0.1 * 6.48406457901001
Epoch 800, val loss: 1.035720705986023
Epoch 810, training loss: 0.6569795608520508 = 0.008368468843400478 + 0.1 * 6.486110687255859
Epoch 810, val loss: 1.039916753768921
Epoch 820, training loss: 0.6561235189437866 = 0.008099989965558052 + 0.1 * 6.4802350997924805
Epoch 820, val loss: 1.044026494026184
Epoch 830, training loss: 0.6541137099266052 = 0.007846705615520477 + 0.1 * 6.462669849395752
Epoch 830, val loss: 1.0480509996414185
Epoch 840, training loss: 0.655205249786377 = 0.007606524042785168 + 0.1 * 6.475987434387207
Epoch 840, val loss: 1.0519887208938599
Epoch 850, training loss: 0.6534081101417542 = 0.007377792615443468 + 0.1 * 6.460302829742432
Epoch 850, val loss: 1.0557827949523926
Epoch 860, training loss: 0.6534029245376587 = 0.007161209359765053 + 0.1 * 6.462417125701904
Epoch 860, val loss: 1.0595409870147705
Epoch 870, training loss: 0.6532338261604309 = 0.00695526460185647 + 0.1 * 6.462785720825195
Epoch 870, val loss: 1.063225507736206
Epoch 880, training loss: 0.653956949710846 = 0.006759390234947205 + 0.1 * 6.471975326538086
Epoch 880, val loss: 1.0667957067489624
Epoch 890, training loss: 0.6513658165931702 = 0.006573200225830078 + 0.1 * 6.447926044464111
Epoch 890, val loss: 1.0702857971191406
Epoch 900, training loss: 0.6511134505271912 = 0.006396015640348196 + 0.1 * 6.447174549102783
Epoch 900, val loss: 1.0737782716751099
Epoch 910, training loss: 0.6529104113578796 = 0.006226439028978348 + 0.1 * 6.466839790344238
Epoch 910, val loss: 1.0771484375
Epoch 920, training loss: 0.6511937975883484 = 0.006064345128834248 + 0.1 * 6.451294422149658
Epoch 920, val loss: 1.080428957939148
Epoch 930, training loss: 0.6513664126396179 = 0.0059097823686897755 + 0.1 * 6.45456600189209
Epoch 930, val loss: 1.0836834907531738
Epoch 940, training loss: 0.6496591567993164 = 0.00576234795153141 + 0.1 * 6.438968181610107
Epoch 940, val loss: 1.0868602991104126
Epoch 950, training loss: 0.6503912210464478 = 0.005621273536235094 + 0.1 * 6.447699546813965
Epoch 950, val loss: 1.0899516344070435
Epoch 960, training loss: 0.6498261094093323 = 0.005486136302351952 + 0.1 * 6.443399429321289
Epoch 960, val loss: 1.0930018424987793
Epoch 970, training loss: 0.6493914127349854 = 0.005356455687433481 + 0.1 * 6.440349578857422
Epoch 970, val loss: 1.0960030555725098
Epoch 980, training loss: 0.6484911441802979 = 0.005232066381722689 + 0.1 * 6.432590484619141
Epoch 980, val loss: 1.0989494323730469
Epoch 990, training loss: 0.6508070230484009 = 0.00511248828843236 + 0.1 * 6.456945419311523
Epoch 990, val loss: 1.1018239259719849
Epoch 1000, training loss: 0.6483885645866394 = 0.004998325370252132 + 0.1 * 6.433902263641357
Epoch 1000, val loss: 1.1046348810195923
Epoch 1010, training loss: 0.647320568561554 = 0.004888908006250858 + 0.1 * 6.424316883087158
Epoch 1010, val loss: 1.107461929321289
Epoch 1020, training loss: 0.6473807692527771 = 0.004783138632774353 + 0.1 * 6.425975799560547
Epoch 1020, val loss: 1.1102203130722046
Epoch 1030, training loss: 0.6474418640136719 = 0.004680817946791649 + 0.1 * 6.427610397338867
Epoch 1030, val loss: 1.1128509044647217
Epoch 1040, training loss: 0.6482324004173279 = 0.004582572262734175 + 0.1 * 6.436498165130615
Epoch 1040, val loss: 1.1154570579528809
Epoch 1050, training loss: 0.6463286280632019 = 0.004488498903810978 + 0.1 * 6.418400764465332
Epoch 1050, val loss: 1.1180806159973145
Epoch 1060, training loss: 0.6459012627601624 = 0.004397637210786343 + 0.1 * 6.415036201477051
Epoch 1060, val loss: 1.1206375360488892
Epoch 1070, training loss: 0.6466230154037476 = 0.004309738986194134 + 0.1 * 6.42313289642334
Epoch 1070, val loss: 1.1231449842453003
Epoch 1080, training loss: 0.6463624835014343 = 0.004224753938615322 + 0.1 * 6.421377182006836
Epoch 1080, val loss: 1.1255544424057007
Epoch 1090, training loss: 0.6468931436538696 = 0.00414304481819272 + 0.1 * 6.4275007247924805
Epoch 1090, val loss: 1.1279792785644531
Epoch 1100, training loss: 0.645837664604187 = 0.004064452834427357 + 0.1 * 6.417732238769531
Epoch 1100, val loss: 1.1303489208221436
Epoch 1110, training loss: 0.645555853843689 = 0.003988331183791161 + 0.1 * 6.415675163269043
Epoch 1110, val loss: 1.1326675415039062
Epoch 1120, training loss: 0.6446239948272705 = 0.003914837259799242 + 0.1 * 6.4070916175842285
Epoch 1120, val loss: 1.1349587440490723
Epoch 1130, training loss: 0.6450178027153015 = 0.003843690035864711 + 0.1 * 6.411741256713867
Epoch 1130, val loss: 1.1372294425964355
Epoch 1140, training loss: 0.6464092135429382 = 0.0037745279259979725 + 0.1 * 6.426346778869629
Epoch 1140, val loss: 1.1394269466400146
Epoch 1150, training loss: 0.6448596715927124 = 0.003707911353558302 + 0.1 * 6.41151762008667
Epoch 1150, val loss: 1.1415958404541016
Epoch 1160, training loss: 0.6437707543373108 = 0.0036434088833630085 + 0.1 * 6.401273250579834
Epoch 1160, val loss: 1.1437841653823853
Epoch 1170, training loss: 0.644088089466095 = 0.0035808365792036057 + 0.1 * 6.4050726890563965
Epoch 1170, val loss: 1.1459224224090576
Epoch 1180, training loss: 0.6430814266204834 = 0.0035199765115976334 + 0.1 * 6.3956146240234375
Epoch 1180, val loss: 1.1480016708374023
Epoch 1190, training loss: 0.6451029777526855 = 0.0034610084258019924 + 0.1 * 6.416419506072998
Epoch 1190, val loss: 1.1500890254974365
Epoch 1200, training loss: 0.6429571509361267 = 0.0034038089215755463 + 0.1 * 6.395533561706543
Epoch 1200, val loss: 1.152077555656433
Epoch 1210, training loss: 0.6436710953712463 = 0.0033486245665699244 + 0.1 * 6.403224468231201
Epoch 1210, val loss: 1.1541218757629395
Epoch 1220, training loss: 0.6428098678588867 = 0.0032949992455542088 + 0.1 * 6.395148277282715
Epoch 1220, val loss: 1.1560981273651123
Epoch 1230, training loss: 0.6425788402557373 = 0.0032430249266326427 + 0.1 * 6.393357753753662
Epoch 1230, val loss: 1.1580734252929688
Epoch 1240, training loss: 0.6430268883705139 = 0.003192279953509569 + 0.1 * 6.398345947265625
Epoch 1240, val loss: 1.1599878072738647
Epoch 1250, training loss: 0.6433296203613281 = 0.003142997156828642 + 0.1 * 6.4018659591674805
Epoch 1250, val loss: 1.1618685722351074
Epoch 1260, training loss: 0.6424852013587952 = 0.0030952179804444313 + 0.1 * 6.393899917602539
Epoch 1260, val loss: 1.163743495941162
Epoch 1270, training loss: 0.6419217586517334 = 0.003048686310648918 + 0.1 * 6.388731002807617
Epoch 1270, val loss: 1.1656068563461304
Epoch 1280, training loss: 0.6421366930007935 = 0.0030032629147171974 + 0.1 * 6.391334533691406
Epoch 1280, val loss: 1.1674234867095947
Epoch 1290, training loss: 0.6434847116470337 = 0.0029591452330350876 + 0.1 * 6.4052557945251465
Epoch 1290, val loss: 1.1691951751708984
Epoch 1300, training loss: 0.6422868967056274 = 0.002916247583925724 + 0.1 * 6.39370584487915
Epoch 1300, val loss: 1.1709544658660889
Epoch 1310, training loss: 0.6415009498596191 = 0.002874645870178938 + 0.1 * 6.386262893676758
Epoch 1310, val loss: 1.1727056503295898
Epoch 1320, training loss: 0.6407656073570251 = 0.0028341624420136213 + 0.1 * 6.379314422607422
Epoch 1320, val loss: 1.174453616142273
Epoch 1330, training loss: 0.6427925229072571 = 0.002794664353132248 + 0.1 * 6.399978160858154
Epoch 1330, val loss: 1.17616868019104
Epoch 1340, training loss: 0.6414892077445984 = 0.0027561690658330917 + 0.1 * 6.387330055236816
Epoch 1340, val loss: 1.1778335571289062
Epoch 1350, training loss: 0.6412388682365417 = 0.0027186512015759945 + 0.1 * 6.385202407836914
Epoch 1350, val loss: 1.1794991493225098
Epoch 1360, training loss: 0.6406694054603577 = 0.0026821072679013014 + 0.1 * 6.379873275756836
Epoch 1360, val loss: 1.1811665296554565
Epoch 1370, training loss: 0.6404625773429871 = 0.0026464026886969805 + 0.1 * 6.378161907196045
Epoch 1370, val loss: 1.1827630996704102
Epoch 1380, training loss: 0.640609860420227 = 0.002611639443784952 + 0.1 * 6.379981994628906
Epoch 1380, val loss: 1.1843785047531128
Epoch 1390, training loss: 0.6406013369560242 = 0.0025777306873351336 + 0.1 * 6.3802361488342285
Epoch 1390, val loss: 1.1859581470489502
Epoch 1400, training loss: 0.641143262386322 = 0.002544546965509653 + 0.1 * 6.385987281799316
Epoch 1400, val loss: 1.1875063180923462
Epoch 1410, training loss: 0.640308141708374 = 0.002512306673452258 + 0.1 * 6.377957820892334
Epoch 1410, val loss: 1.1890380382537842
Epoch 1420, training loss: 0.6401002407073975 = 0.0024807481095194817 + 0.1 * 6.376194953918457
Epoch 1420, val loss: 1.190565586090088
Epoch 1430, training loss: 0.6422839164733887 = 0.002449984662234783 + 0.1 * 6.39833927154541
Epoch 1430, val loss: 1.192095398902893
Epoch 1440, training loss: 0.6401585340499878 = 0.002419837052002549 + 0.1 * 6.377387046813965
Epoch 1440, val loss: 1.1935615539550781
Epoch 1450, training loss: 0.6396577954292297 = 0.002390520879998803 + 0.1 * 6.372673034667969
Epoch 1450, val loss: 1.1950715780258179
Epoch 1460, training loss: 0.6397011876106262 = 0.002361710648983717 + 0.1 * 6.37339448928833
Epoch 1460, val loss: 1.1965222358703613
Epoch 1470, training loss: 0.641258716583252 = 0.002333705546334386 + 0.1 * 6.3892502784729
Epoch 1470, val loss: 1.1979748010635376
Epoch 1480, training loss: 0.63987797498703 = 0.002306270645931363 + 0.1 * 6.375716686248779
Epoch 1480, val loss: 1.1993473768234253
Epoch 1490, training loss: 0.6394916772842407 = 0.00227959081530571 + 0.1 * 6.3721208572387695
Epoch 1490, val loss: 1.2007832527160645
Epoch 1500, training loss: 0.6398163437843323 = 0.002253398997709155 + 0.1 * 6.37562894821167
Epoch 1500, val loss: 1.2021756172180176
Epoch 1510, training loss: 0.63951176404953 = 0.002227838383987546 + 0.1 * 6.372838973999023
Epoch 1510, val loss: 1.2035356760025024
Epoch 1520, training loss: 0.6389185786247253 = 0.0022027723025530577 + 0.1 * 6.367157936096191
Epoch 1520, val loss: 1.2048888206481934
Epoch 1530, training loss: 0.6388940215110779 = 0.002178207505494356 + 0.1 * 6.367157936096191
Epoch 1530, val loss: 1.2062536478042603
Epoch 1540, training loss: 0.6385611295700073 = 0.002154143527150154 + 0.1 * 6.36406946182251
Epoch 1540, val loss: 1.2075728178024292
Epoch 1550, training loss: 0.6393617391586304 = 0.0021306544076651335 + 0.1 * 6.372311115264893
Epoch 1550, val loss: 1.2089120149612427
Epoch 1560, training loss: 0.6386961936950684 = 0.0021075813565403223 + 0.1 * 6.365886211395264
Epoch 1560, val loss: 1.2101777791976929
Epoch 1570, training loss: 0.6391201615333557 = 0.002085105050355196 + 0.1 * 6.3703508377075195
Epoch 1570, val loss: 1.2114677429199219
Epoch 1580, training loss: 0.6389650702476501 = 0.002063083229586482 + 0.1 * 6.369019508361816
Epoch 1580, val loss: 1.2127734422683716
Epoch 1590, training loss: 0.638264536857605 = 0.0020415550097823143 + 0.1 * 6.362229347229004
Epoch 1590, val loss: 1.2140322923660278
Epoch 1600, training loss: 0.6376948952674866 = 0.002020464977249503 + 0.1 * 6.356743812561035
Epoch 1600, val loss: 1.2153156995773315
Epoch 1610, training loss: 0.6390440464019775 = 0.001999709289520979 + 0.1 * 6.370442867279053
Epoch 1610, val loss: 1.2165685892105103
Epoch 1620, training loss: 0.638107419013977 = 0.001979375956580043 + 0.1 * 6.36128044128418
Epoch 1620, val loss: 1.2177602052688599
Epoch 1630, training loss: 0.6379263401031494 = 0.0019593758042901754 + 0.1 * 6.3596696853637695
Epoch 1630, val loss: 1.2189512252807617
Epoch 1640, training loss: 0.6379446387290955 = 0.0019399668090045452 + 0.1 * 6.360046863555908
Epoch 1640, val loss: 1.2201557159423828
Epoch 1650, training loss: 0.6380347609519958 = 0.0019208589801564813 + 0.1 * 6.361138820648193
Epoch 1650, val loss: 1.221382975578308
Epoch 1660, training loss: 0.638027012348175 = 0.0019020701292902231 + 0.1 * 6.3612494468688965
Epoch 1660, val loss: 1.2225549221038818
Epoch 1670, training loss: 0.6384047269821167 = 0.0018836521776393056 + 0.1 * 6.365211009979248
Epoch 1670, val loss: 1.223662257194519
Epoch 1680, training loss: 0.6375362873077393 = 0.001865694997832179 + 0.1 * 6.356705665588379
Epoch 1680, val loss: 1.224799394607544
Epoch 1690, training loss: 0.6386513710021973 = 0.0018480017315596342 + 0.1 * 6.3680338859558105
Epoch 1690, val loss: 1.2259442806243896
Epoch 1700, training loss: 0.6370155811309814 = 0.001830710913054645 + 0.1 * 6.351848602294922
Epoch 1700, val loss: 1.2270668745040894
Epoch 1710, training loss: 0.637738823890686 = 0.00181371730286628 + 0.1 * 6.359251022338867
Epoch 1710, val loss: 1.2281948328018188
Epoch 1720, training loss: 0.6377142071723938 = 0.0017969015752896667 + 0.1 * 6.359172821044922
Epoch 1720, val loss: 1.229274034500122
Epoch 1730, training loss: 0.6378929018974304 = 0.0017804402159526944 + 0.1 * 6.361124515533447
Epoch 1730, val loss: 1.2303136587142944
Epoch 1740, training loss: 0.6369603276252747 = 0.001764334156177938 + 0.1 * 6.351959705352783
Epoch 1740, val loss: 1.2314180135726929
Epoch 1750, training loss: 0.6372295022010803 = 0.0017484647687524557 + 0.1 * 6.35481071472168
Epoch 1750, val loss: 1.2324979305267334
Epoch 1760, training loss: 0.6372817754745483 = 0.0017328959656879306 + 0.1 * 6.3554887771606445
Epoch 1760, val loss: 1.2335753440856934
Epoch 1770, training loss: 0.6366051435470581 = 0.001717624138109386 + 0.1 * 6.348875045776367
Epoch 1770, val loss: 1.2346404790878296
Epoch 1780, training loss: 0.6367351412773132 = 0.001702560461126268 + 0.1 * 6.350326061248779
Epoch 1780, val loss: 1.2356868982315063
Epoch 1790, training loss: 0.6365825533866882 = 0.0016878318274393678 + 0.1 * 6.348947048187256
Epoch 1790, val loss: 1.2367310523986816
Epoch 1800, training loss: 0.6373698711395264 = 0.0016732719959691167 + 0.1 * 6.356966018676758
Epoch 1800, val loss: 1.2377508878707886
Epoch 1810, training loss: 0.6368008255958557 = 0.001659034751355648 + 0.1 * 6.351417541503906
Epoch 1810, val loss: 1.238761305809021
Epoch 1820, training loss: 0.6374314427375793 = 0.0016450309194624424 + 0.1 * 6.357863903045654
Epoch 1820, val loss: 1.2397990226745605
Epoch 1830, training loss: 0.6368430256843567 = 0.0016312302323058248 + 0.1 * 6.352117538452148
Epoch 1830, val loss: 1.2407891750335693
Epoch 1840, training loss: 0.636275589466095 = 0.0016177319921553135 + 0.1 * 6.346578598022461
Epoch 1840, val loss: 1.241790533065796
Epoch 1850, training loss: 0.6364296078681946 = 0.0016044327057898045 + 0.1 * 6.348251819610596
Epoch 1850, val loss: 1.2428146600723267
Epoch 1860, training loss: 0.6370840668678284 = 0.0015913256211206317 + 0.1 * 6.3549275398254395
Epoch 1860, val loss: 1.2437974214553833
Epoch 1870, training loss: 0.635759174823761 = 0.0015784258721396327 + 0.1 * 6.341806888580322
Epoch 1870, val loss: 1.2447525262832642
Epoch 1880, training loss: 0.6355319619178772 = 0.0015657627955079079 + 0.1 * 6.339662075042725
Epoch 1880, val loss: 1.2457408905029297
Epoch 1890, training loss: 0.636637806892395 = 0.0015532566467300057 + 0.1 * 6.3508453369140625
Epoch 1890, val loss: 1.246736764907837
Epoch 1900, training loss: 0.6360945105552673 = 0.0015409021871164441 + 0.1 * 6.345536231994629
Epoch 1900, val loss: 1.2476364374160767
Epoch 1910, training loss: 0.6356971859931946 = 0.001528861583210528 + 0.1 * 6.3416829109191895
Epoch 1910, val loss: 1.2485843896865845
Epoch 1920, training loss: 0.6362913846969604 = 0.0015169819816946983 + 0.1 * 6.347743988037109
Epoch 1920, val loss: 1.2495362758636475
Epoch 1930, training loss: 0.6362332105636597 = 0.001505299936980009 + 0.1 * 6.347279071807861
Epoch 1930, val loss: 1.2504695653915405
Epoch 1940, training loss: 0.6365160942077637 = 0.0014937674859538674 + 0.1 * 6.350223541259766
Epoch 1940, val loss: 1.2513782978057861
Epoch 1950, training loss: 0.6357199549674988 = 0.0014824639074504375 + 0.1 * 6.342374324798584
Epoch 1950, val loss: 1.252301573753357
Epoch 1960, training loss: 0.6352071762084961 = 0.001471308059990406 + 0.1 * 6.3373589515686035
Epoch 1960, val loss: 1.2532320022583008
Epoch 1970, training loss: 0.6352406740188599 = 0.0014602774754166603 + 0.1 * 6.337803840637207
Epoch 1970, val loss: 1.2541570663452148
Epoch 1980, training loss: 0.6353464126586914 = 0.0014493991620838642 + 0.1 * 6.338970184326172
Epoch 1980, val loss: 1.2550688982009888
Epoch 1990, training loss: 0.635100245475769 = 0.001438673003576696 + 0.1 * 6.336615562438965
Epoch 1990, val loss: 1.2559455633163452
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8339483394833949
The final CL Acc:0.80864, 0.01772, The final GNN Acc:0.83764, 0.00269
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10552])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8126308917999268 = 1.9529465436935425 + 0.1 * 8.596842765808105
Epoch 0, val loss: 1.9486411809921265
Epoch 10, training loss: 2.801987886428833 = 1.9423165321350098 + 0.1 * 8.59671401977539
Epoch 10, val loss: 1.938415765762329
Epoch 20, training loss: 2.788547992706299 = 1.9289686679840088 + 0.1 * 8.595791816711426
Epoch 20, val loss: 1.925013780593872
Epoch 30, training loss: 2.768789768218994 = 1.9100457429885864 + 0.1 * 8.58743953704834
Epoch 30, val loss: 1.9056100845336914
Epoch 40, training loss: 2.7360172271728516 = 1.8821436166763306 + 0.1 * 8.538737297058105
Epoch 40, val loss: 1.8771517276763916
Epoch 50, training loss: 2.6760332584381104 = 1.8456400632858276 + 0.1 * 8.30393123626709
Epoch 50, val loss: 1.8419352769851685
Epoch 60, training loss: 2.625119209289551 = 1.8081756830215454 + 0.1 * 8.169435501098633
Epoch 60, val loss: 1.8091814517974854
Epoch 70, training loss: 2.5799560546875 = 1.7775919437408447 + 0.1 * 8.023640632629395
Epoch 70, val loss: 1.7839971780776978
Epoch 80, training loss: 2.5143673419952393 = 1.7433253526687622 + 0.1 * 7.710419654846191
Epoch 80, val loss: 1.7528899908065796
Epoch 90, training loss: 2.4437062740325928 = 1.6999889612197876 + 0.1 * 7.4371724128723145
Epoch 90, val loss: 1.7150294780731201
Epoch 100, training loss: 2.35770320892334 = 1.643010139465332 + 0.1 * 7.146931171417236
Epoch 100, val loss: 1.6671816110610962
Epoch 110, training loss: 2.272324562072754 = 1.569427490234375 + 0.1 * 7.028970718383789
Epoch 110, val loss: 1.6026643514633179
Epoch 120, training loss: 2.1833019256591797 = 1.4857534170150757 + 0.1 * 6.975485324859619
Epoch 120, val loss: 1.5323408842086792
Epoch 130, training loss: 2.095801830291748 = 1.4010964632034302 + 0.1 * 6.947054862976074
Epoch 130, val loss: 1.4647737741470337
Epoch 140, training loss: 2.011532783508301 = 1.3186423778533936 + 0.1 * 6.928904056549072
Epoch 140, val loss: 1.4032129049301147
Epoch 150, training loss: 1.931086540222168 = 1.239263892173767 + 0.1 * 6.918226718902588
Epoch 150, val loss: 1.3454943895339966
Epoch 160, training loss: 1.8539535999298096 = 1.1636712551116943 + 0.1 * 6.9028239250183105
Epoch 160, val loss: 1.2924262285232544
Epoch 170, training loss: 1.7807669639587402 = 1.0918383598327637 + 0.1 * 6.889286041259766
Epoch 170, val loss: 1.2435338497161865
Epoch 180, training loss: 1.7111232280731201 = 1.0237317085266113 + 0.1 * 6.873915195465088
Epoch 180, val loss: 1.197557806968689
Epoch 190, training loss: 1.6456552743911743 = 0.9591143131256104 + 0.1 * 6.8654093742370605
Epoch 190, val loss: 1.1546379327774048
Epoch 200, training loss: 1.5826572179794312 = 0.8978735208511353 + 0.1 * 6.847836971282959
Epoch 200, val loss: 1.114195466041565
Epoch 210, training loss: 1.5217955112457275 = 0.8385812640190125 + 0.1 * 6.832142353057861
Epoch 210, val loss: 1.0755159854888916
Epoch 220, training loss: 1.4630107879638672 = 0.7811762094497681 + 0.1 * 6.81834602355957
Epoch 220, val loss: 1.0385034084320068
Epoch 230, training loss: 1.406281590461731 = 0.7257540822029114 + 0.1 * 6.805274963378906
Epoch 230, val loss: 1.0031682252883911
Epoch 240, training loss: 1.3518673181533813 = 0.672459065914154 + 0.1 * 6.794082164764404
Epoch 240, val loss: 0.9699015021324158
Epoch 250, training loss: 1.3010510206222534 = 0.6226949095726013 + 0.1 * 6.783560752868652
Epoch 250, val loss: 0.939757764339447
Epoch 260, training loss: 1.2547879219055176 = 0.5775838494300842 + 0.1 * 6.772040843963623
Epoch 260, val loss: 0.9140254855155945
Epoch 270, training loss: 1.2153759002685547 = 0.5371320843696594 + 0.1 * 6.782437801361084
Epoch 270, val loss: 0.893243670463562
Epoch 280, training loss: 1.176680564880371 = 0.501433789730072 + 0.1 * 6.752468109130859
Epoch 280, val loss: 0.8773849606513977
Epoch 290, training loss: 1.1429810523986816 = 0.4691406488418579 + 0.1 * 6.738403797149658
Epoch 290, val loss: 0.8654594421386719
Epoch 300, training loss: 1.1117342710494995 = 0.43894264101982117 + 0.1 * 6.727916717529297
Epoch 300, val loss: 0.8564581871032715
Epoch 310, training loss: 1.0819799900054932 = 0.4098743200302124 + 0.1 * 6.7210564613342285
Epoch 310, val loss: 0.8493017554283142
Epoch 320, training loss: 1.0541527271270752 = 0.3813944458961487 + 0.1 * 6.727583408355713
Epoch 320, val loss: 0.8433753848075867
Epoch 330, training loss: 1.0243632793426514 = 0.3535032272338867 + 0.1 * 6.708600044250488
Epoch 330, val loss: 0.8383594751358032
Epoch 340, training loss: 0.9961128234863281 = 0.3262959122657776 + 0.1 * 6.698169231414795
Epoch 340, val loss: 0.8342127203941345
Epoch 350, training loss: 0.9710297584533691 = 0.3001587688922882 + 0.1 * 6.708710193634033
Epoch 350, val loss: 0.8308552503585815
Epoch 360, training loss: 0.9446085095405579 = 0.2755013704299927 + 0.1 * 6.691071510314941
Epoch 360, val loss: 0.8284586071968079
Epoch 370, training loss: 0.9204827547073364 = 0.2521800398826599 + 0.1 * 6.6830267906188965
Epoch 370, val loss: 0.8269663453102112
Epoch 380, training loss: 0.8987656831741333 = 0.2300187200307846 + 0.1 * 6.687469482421875
Epoch 380, val loss: 0.826439380645752
Epoch 390, training loss: 0.8768051862716675 = 0.2089938223361969 + 0.1 * 6.6781134605407715
Epoch 390, val loss: 0.8268404006958008
Epoch 400, training loss: 0.8574087023735046 = 0.18925762176513672 + 0.1 * 6.6815104484558105
Epoch 400, val loss: 0.8280170559883118
Epoch 410, training loss: 0.8380417823791504 = 0.17094425857067108 + 0.1 * 6.6709747314453125
Epoch 410, val loss: 0.8302424550056458
Epoch 420, training loss: 0.8230098485946655 = 0.1541392058134079 + 0.1 * 6.688705921173096
Epoch 420, val loss: 0.8332582712173462
Epoch 430, training loss: 0.805321991443634 = 0.13898169994354248 + 0.1 * 6.663402557373047
Epoch 430, val loss: 0.8372542262077332
Epoch 440, training loss: 0.790846049785614 = 0.12537334859371185 + 0.1 * 6.654726505279541
Epoch 440, val loss: 0.8421151041984558
Epoch 450, training loss: 0.7794222831726074 = 0.1132248193025589 + 0.1 * 6.661974906921387
Epoch 450, val loss: 0.847790539264679
Epoch 460, training loss: 0.7693576812744141 = 0.10242971032857895 + 0.1 * 6.669280052185059
Epoch 460, val loss: 0.8540750741958618
Epoch 470, training loss: 0.7578384280204773 = 0.092882439494133 + 0.1 * 6.649559497833252
Epoch 470, val loss: 0.8608487248420715
Epoch 480, training loss: 0.7486164569854736 = 0.08439020812511444 + 0.1 * 6.642261981964111
Epoch 480, val loss: 0.8680620193481445
Epoch 490, training loss: 0.7414692044258118 = 0.07683976739645004 + 0.1 * 6.646294593811035
Epoch 490, val loss: 0.8755883574485779
Epoch 500, training loss: 0.735245406627655 = 0.07013748586177826 + 0.1 * 6.651078701019287
Epoch 500, val loss: 0.8832962512969971
Epoch 510, training loss: 0.7268333435058594 = 0.06417234241962433 + 0.1 * 6.626609802246094
Epoch 510, val loss: 0.891124963760376
Epoch 520, training loss: 0.7206080555915833 = 0.05882759392261505 + 0.1 * 6.617804527282715
Epoch 520, val loss: 0.8990892171859741
Epoch 530, training loss: 0.7172313928604126 = 0.05403454601764679 + 0.1 * 6.6319684982299805
Epoch 530, val loss: 0.9070882797241211
Epoch 540, training loss: 0.7113341689109802 = 0.04974912852048874 + 0.1 * 6.61584997177124
Epoch 540, val loss: 0.9150804877281189
Epoch 550, training loss: 0.7068459987640381 = 0.04590553790330887 + 0.1 * 6.609404563903809
Epoch 550, val loss: 0.9230005145072937
Epoch 560, training loss: 0.7026400566101074 = 0.042444758117198944 + 0.1 * 6.60195255279541
Epoch 560, val loss: 0.9309450387954712
Epoch 570, training loss: 0.6994247436523438 = 0.03932914137840271 + 0.1 * 6.600956439971924
Epoch 570, val loss: 0.9388277530670166
Epoch 580, training loss: 0.6969477534294128 = 0.03652549535036087 + 0.1 * 6.604222297668457
Epoch 580, val loss: 0.9465876817703247
Epoch 590, training loss: 0.6932153105735779 = 0.03399985283613205 + 0.1 * 6.592154502868652
Epoch 590, val loss: 0.9542524814605713
Epoch 600, training loss: 0.6895381212234497 = 0.031713925302028656 + 0.1 * 6.57824182510376
Epoch 600, val loss: 0.9617601037025452
Epoch 610, training loss: 0.6888130307197571 = 0.0296400748193264 + 0.1 * 6.591729640960693
Epoch 610, val loss: 0.9692164659500122
Epoch 620, training loss: 0.6865971088409424 = 0.027760764583945274 + 0.1 * 6.5883636474609375
Epoch 620, val loss: 0.9765246510505676
Epoch 630, training loss: 0.6830324530601501 = 0.026057211682200432 + 0.1 * 6.5697526931762695
Epoch 630, val loss: 0.9836722016334534
Epoch 640, training loss: 0.6817725896835327 = 0.024505486711859703 + 0.1 * 6.572670936584473
Epoch 640, val loss: 0.9906984567642212
Epoch 650, training loss: 0.6796031594276428 = 0.023090440779924393 + 0.1 * 6.565127372741699
Epoch 650, val loss: 0.9975976943969727
Epoch 660, training loss: 0.6777003407478333 = 0.02179659530520439 + 0.1 * 6.559037685394287
Epoch 660, val loss: 1.004351258277893
Epoch 670, training loss: 0.6760644316673279 = 0.020610280334949493 + 0.1 * 6.55454158782959
Epoch 670, val loss: 1.01094388961792
Epoch 680, training loss: 0.674699068069458 = 0.019520610570907593 + 0.1 * 6.551784992218018
Epoch 680, val loss: 1.017422080039978
Epoch 690, training loss: 0.6746702194213867 = 0.01851772703230381 + 0.1 * 6.561524868011475
Epoch 690, val loss: 1.023797631263733
Epoch 700, training loss: 0.6724410057067871 = 0.017595797777175903 + 0.1 * 6.5484514236450195
Epoch 700, val loss: 1.029915452003479
Epoch 710, training loss: 0.6714199185371399 = 0.016746267676353455 + 0.1 * 6.546736240386963
Epoch 710, val loss: 1.0358933210372925
Epoch 720, training loss: 0.6702618598937988 = 0.015960335731506348 + 0.1 * 6.543015003204346
Epoch 720, val loss: 1.0417360067367554
Epoch 730, training loss: 0.6687629222869873 = 0.015231451950967312 + 0.1 * 6.535314559936523
Epoch 730, val loss: 1.0474963188171387
Epoch 740, training loss: 0.6678915619850159 = 0.014554066583514214 + 0.1 * 6.533374786376953
Epoch 740, val loss: 1.053125262260437
Epoch 750, training loss: 0.6681095361709595 = 0.01392421219497919 + 0.1 * 6.541852951049805
Epoch 750, val loss: 1.0586224794387817
Epoch 760, training loss: 0.6659411191940308 = 0.013339182361960411 + 0.1 * 6.52601957321167
Epoch 760, val loss: 1.0639828443527222
Epoch 770, training loss: 0.6645867824554443 = 0.012792153283953667 + 0.1 * 6.517945766448975
Epoch 770, val loss: 1.0692377090454102
Epoch 780, training loss: 0.6645978093147278 = 0.012280559167265892 + 0.1 * 6.523171901702881
Epoch 780, val loss: 1.0743780136108398
Epoch 790, training loss: 0.6636324524879456 = 0.011802050285041332 + 0.1 * 6.518303394317627
Epoch 790, val loss: 1.079415202140808
Epoch 800, training loss: 0.6625182628631592 = 0.011352401226758957 + 0.1 * 6.511658191680908
Epoch 800, val loss: 1.0843545198440552
Epoch 810, training loss: 0.6616231799125671 = 0.010930794291198254 + 0.1 * 6.506923675537109
Epoch 810, val loss: 1.0891697406768799
Epoch 820, training loss: 0.6612051129341125 = 0.010535741224884987 + 0.1 * 6.5066938400268555
Epoch 820, val loss: 1.0938451290130615
Epoch 830, training loss: 0.6605702042579651 = 0.010165140964090824 + 0.1 * 6.504050254821777
Epoch 830, val loss: 1.0983946323394775
Epoch 840, training loss: 0.6594932079315186 = 0.00981514248996973 + 0.1 * 6.4967803955078125
Epoch 840, val loss: 1.1029233932495117
Epoch 850, training loss: 0.6591934561729431 = 0.009483568370342255 + 0.1 * 6.497098922729492
Epoch 850, val loss: 1.1073436737060547
Epoch 860, training loss: 0.6594486832618713 = 0.009169602766633034 + 0.1 * 6.502790927886963
Epoch 860, val loss: 1.1116747856140137
Epoch 870, training loss: 0.6586536765098572 = 0.008873934857547283 + 0.1 * 6.497797012329102
Epoch 870, val loss: 1.115938663482666
Epoch 880, training loss: 0.6587250232696533 = 0.008594009093940258 + 0.1 * 6.501309871673584
Epoch 880, val loss: 1.1200859546661377
Epoch 890, training loss: 0.6568804383277893 = 0.008329289965331554 + 0.1 * 6.485511779785156
Epoch 890, val loss: 1.1241766214370728
Epoch 900, training loss: 0.6568254232406616 = 0.008076999336481094 + 0.1 * 6.487483978271484
Epoch 900, val loss: 1.12819242477417
Epoch 910, training loss: 0.6584315299987793 = 0.007837563753128052 + 0.1 * 6.505939960479736
Epoch 910, val loss: 1.132143497467041
Epoch 920, training loss: 0.6556745171546936 = 0.007610349915921688 + 0.1 * 6.480641841888428
Epoch 920, val loss: 1.135963797569275
Epoch 930, training loss: 0.6553563475608826 = 0.007394364103674889 + 0.1 * 6.47961950302124
Epoch 930, val loss: 1.1397759914398193
Epoch 940, training loss: 0.655169665813446 = 0.007188043091446161 + 0.1 * 6.479816436767578
Epoch 940, val loss: 1.1435011625289917
Epoch 950, training loss: 0.6549014449119568 = 0.006991875357925892 + 0.1 * 6.479095935821533
Epoch 950, val loss: 1.1471772193908691
Epoch 960, training loss: 0.6536109447479248 = 0.006804491858929396 + 0.1 * 6.468064785003662
Epoch 960, val loss: 1.1507643461227417
Epoch 970, training loss: 0.6547089219093323 = 0.006625416222959757 + 0.1 * 6.4808349609375
Epoch 970, val loss: 1.154334306716919
Epoch 980, training loss: 0.6539391875267029 = 0.006454503629356623 + 0.1 * 6.474846363067627
Epoch 980, val loss: 1.1578010320663452
Epoch 990, training loss: 0.6524534821510315 = 0.006291046738624573 + 0.1 * 6.461624622344971
Epoch 990, val loss: 1.1612067222595215
Epoch 1000, training loss: 0.6527491211891174 = 0.006134622264653444 + 0.1 * 6.466144561767578
Epoch 1000, val loss: 1.164589524269104
Epoch 1010, training loss: 0.65262371301651 = 0.005984521470963955 + 0.1 * 6.466391563415527
Epoch 1010, val loss: 1.1678972244262695
Epoch 1020, training loss: 0.6526332497596741 = 0.005840967874974012 + 0.1 * 6.467922687530518
Epoch 1020, val loss: 1.1711463928222656
Epoch 1030, training loss: 0.651593804359436 = 0.005703416187316179 + 0.1 * 6.458904266357422
Epoch 1030, val loss: 1.1743228435516357
Epoch 1040, training loss: 0.6517655849456787 = 0.0055712806060910225 + 0.1 * 6.461942672729492
Epoch 1040, val loss: 1.1774717569351196
Epoch 1050, training loss: 0.6527363061904907 = 0.005444267764687538 + 0.1 * 6.472919940948486
Epoch 1050, val loss: 1.1805773973464966
Epoch 1060, training loss: 0.6513285636901855 = 0.0053224870935082436 + 0.1 * 6.460060119628906
Epoch 1060, val loss: 1.1836228370666504
Epoch 1070, training loss: 0.6499177813529968 = 0.005205419845879078 + 0.1 * 6.4471235275268555
Epoch 1070, val loss: 1.1865900754928589
Epoch 1080, training loss: 0.6502506732940674 = 0.005092608276754618 + 0.1 * 6.45158052444458
Epoch 1080, val loss: 1.1895620822906494
Epoch 1090, training loss: 0.6509411931037903 = 0.004983908962458372 + 0.1 * 6.459572792053223
Epoch 1090, val loss: 1.1924645900726318
Epoch 1100, training loss: 0.6495031118392944 = 0.004879646468907595 + 0.1 * 6.446234703063965
Epoch 1100, val loss: 1.1953177452087402
Epoch 1110, training loss: 0.650470495223999 = 0.004779180511832237 + 0.1 * 6.456913471221924
Epoch 1110, val loss: 1.198160171508789
Epoch 1120, training loss: 0.6494479179382324 = 0.004682119470089674 + 0.1 * 6.447658061981201
Epoch 1120, val loss: 1.2009387016296387
Epoch 1130, training loss: 0.6512172818183899 = 0.004588380455970764 + 0.1 * 6.466289043426514
Epoch 1130, val loss: 1.2036850452423096
Epoch 1140, training loss: 0.6487096548080444 = 0.004498184192925692 + 0.1 * 6.442114353179932
Epoch 1140, val loss: 1.206373691558838
Epoch 1150, training loss: 0.6482146978378296 = 0.004411062225699425 + 0.1 * 6.4380364418029785
Epoch 1150, val loss: 1.2090566158294678
Epoch 1160, training loss: 0.6494418382644653 = 0.004326696507632732 + 0.1 * 6.451151371002197
Epoch 1160, val loss: 1.2116916179656982
Epoch 1170, training loss: 0.6481303572654724 = 0.004245337098836899 + 0.1 * 6.438849925994873
Epoch 1170, val loss: 1.2142518758773804
Epoch 1180, training loss: 0.6480855345726013 = 0.004166784696280956 + 0.1 * 6.439187526702881
Epoch 1180, val loss: 1.216794490814209
Epoch 1190, training loss: 0.6473771929740906 = 0.004090857692062855 + 0.1 * 6.432863235473633
Epoch 1190, val loss: 1.219336748123169
Epoch 1200, training loss: 0.6481426358222961 = 0.004017193336039782 + 0.1 * 6.441254615783691
Epoch 1200, val loss: 1.2218314409255981
Epoch 1210, training loss: 0.6495370268821716 = 0.003945793956518173 + 0.1 * 6.4559125900268555
Epoch 1210, val loss: 1.2242763042449951
Epoch 1220, training loss: 0.6470943093299866 = 0.00387717317789793 + 0.1 * 6.43217134475708
Epoch 1220, val loss: 1.2266769409179688
Epoch 1230, training loss: 0.6470491290092468 = 0.0038104639388620853 + 0.1 * 6.43238639831543
Epoch 1230, val loss: 1.229053258895874
Epoch 1240, training loss: 0.6473783850669861 = 0.003745659487321973 + 0.1 * 6.43632698059082
Epoch 1240, val loss: 1.2314167022705078
Epoch 1250, training loss: 0.647335946559906 = 0.0036828264128416777 + 0.1 * 6.436530590057373
Epoch 1250, val loss: 1.233742594718933
Epoch 1260, training loss: 0.6468727588653564 = 0.0036220198962837458 + 0.1 * 6.432507514953613
Epoch 1260, val loss: 1.236040711402893
Epoch 1270, training loss: 0.6466037631034851 = 0.003563106060028076 + 0.1 * 6.43040657043457
Epoch 1270, val loss: 1.2382930517196655
Epoch 1280, training loss: 0.6476205587387085 = 0.0035058087669312954 + 0.1 * 6.441147327423096
Epoch 1280, val loss: 1.240525722503662
Epoch 1290, training loss: 0.6451664566993713 = 0.003450320567935705 + 0.1 * 6.417160987854004
Epoch 1290, val loss: 1.2427343130111694
Epoch 1300, training loss: 0.6466571688652039 = 0.003396411892026663 + 0.1 * 6.432607650756836
Epoch 1300, val loss: 1.244938850402832
Epoch 1310, training loss: 0.6450363397598267 = 0.0033438806422054768 + 0.1 * 6.416924476623535
Epoch 1310, val loss: 1.247118353843689
Epoch 1320, training loss: 0.6476885080337524 = 0.0032926423009485006 + 0.1 * 6.443958759307861
Epoch 1320, val loss: 1.2492759227752686
Epoch 1330, training loss: 0.6460669040679932 = 0.0032432067673653364 + 0.1 * 6.428236961364746
Epoch 1330, val loss: 1.2514036893844604
Epoch 1340, training loss: 0.645806610584259 = 0.00319519336335361 + 0.1 * 6.426114559173584
Epoch 1340, val loss: 1.2534464597702026
Epoch 1350, training loss: 0.6452893614768982 = 0.0031485329382121563 + 0.1 * 6.421408176422119
Epoch 1350, val loss: 1.2555369138717651
Epoch 1360, training loss: 0.6459119319915771 = 0.00310300150886178 + 0.1 * 6.428089141845703
Epoch 1360, val loss: 1.2575942277908325
Epoch 1370, training loss: 0.643976628780365 = 0.0030586840584874153 + 0.1 * 6.4091796875
Epoch 1370, val loss: 1.2596299648284912
Epoch 1380, training loss: 0.6453372240066528 = 0.0030154550913721323 + 0.1 * 6.423217296600342
Epoch 1380, val loss: 1.2616568803787231
Epoch 1390, training loss: 0.6443775296211243 = 0.0029733062256127596 + 0.1 * 6.414041996002197
Epoch 1390, val loss: 1.2636609077453613
Epoch 1400, training loss: 0.64534592628479 = 0.0029322803020477295 + 0.1 * 6.424136638641357
Epoch 1400, val loss: 1.2656372785568237
Epoch 1410, training loss: 0.6441287398338318 = 0.00289254286326468 + 0.1 * 6.4123616218566895
Epoch 1410, val loss: 1.267540454864502
Epoch 1420, training loss: 0.6442912817001343 = 0.0028537774924188852 + 0.1 * 6.414375305175781
Epoch 1420, val loss: 1.2694716453552246
Epoch 1430, training loss: 0.6432439088821411 = 0.0028158421628177166 + 0.1 * 6.404280662536621
Epoch 1430, val loss: 1.2713731527328491
Epoch 1440, training loss: 0.6435793042182922 = 0.0027788630686700344 + 0.1 * 6.408003807067871
Epoch 1440, val loss: 1.2732688188552856
Epoch 1450, training loss: 0.6441622972488403 = 0.0027427291497588158 + 0.1 * 6.414196014404297
Epoch 1450, val loss: 1.275138258934021
Epoch 1460, training loss: 0.6439194083213806 = 0.0027077416889369488 + 0.1 * 6.412116527557373
Epoch 1460, val loss: 1.2769733667373657
Epoch 1470, training loss: 0.6427281498908997 = 0.002673554001376033 + 0.1 * 6.400545597076416
Epoch 1470, val loss: 1.2787531614303589
Epoch 1480, training loss: 0.6440152525901794 = 0.0026400976348668337 + 0.1 * 6.413751602172852
Epoch 1480, val loss: 1.2806028127670288
Epoch 1490, training loss: 0.6430609822273254 = 0.002607298083603382 + 0.1 * 6.404536724090576
Epoch 1490, val loss: 1.282402753829956
Epoch 1500, training loss: 0.6423727869987488 = 0.002575363963842392 + 0.1 * 6.397974491119385
Epoch 1500, val loss: 1.2841899394989014
Epoch 1510, training loss: 0.6438574194908142 = 0.0025441350881010294 + 0.1 * 6.413132190704346
Epoch 1510, val loss: 1.2859420776367188
Epoch 1520, training loss: 0.642311155796051 = 0.0025136827025562525 + 0.1 * 6.397974491119385
Epoch 1520, val loss: 1.2876701354980469
Epoch 1530, training loss: 0.6434098482131958 = 0.00248396466486156 + 0.1 * 6.409258842468262
Epoch 1530, val loss: 1.2894048690795898
Epoch 1540, training loss: 0.6422117352485657 = 0.0024548324290663004 + 0.1 * 6.397569179534912
Epoch 1540, val loss: 1.2911244630813599
Epoch 1550, training loss: 0.6422291994094849 = 0.0024264100939035416 + 0.1 * 6.3980278968811035
Epoch 1550, val loss: 1.2927991151809692
Epoch 1560, training loss: 0.6420964598655701 = 0.0023986001033335924 + 0.1 * 6.396978378295898
Epoch 1560, val loss: 1.2944499254226685
Epoch 1570, training loss: 0.6418828368186951 = 0.0023714376147836447 + 0.1 * 6.395113945007324
Epoch 1570, val loss: 1.296095848083496
Epoch 1580, training loss: 0.6416797041893005 = 0.0023447151761502028 + 0.1 * 6.393349647521973
Epoch 1580, val loss: 1.2977603673934937
Epoch 1590, training loss: 0.6430961489677429 = 0.0023185766767710447 + 0.1 * 6.40777587890625
Epoch 1590, val loss: 1.2993727922439575
Epoch 1600, training loss: 0.6422824859619141 = 0.0022931534331291914 + 0.1 * 6.399892807006836
Epoch 1600, val loss: 1.3009165525436401
Epoch 1610, training loss: 0.6414580941200256 = 0.0022683909628540277 + 0.1 * 6.391897201538086
Epoch 1610, val loss: 1.3024576902389526
Epoch 1620, training loss: 0.6416251063346863 = 0.002243984956294298 + 0.1 * 6.393811225891113
Epoch 1620, val loss: 1.3040584325790405
Epoch 1630, training loss: 0.6410983800888062 = 0.0022200054954737425 + 0.1 * 6.3887834548950195
Epoch 1630, val loss: 1.305625081062317
Epoch 1640, training loss: 0.6414863467216492 = 0.002196572721004486 + 0.1 * 6.392898082733154
Epoch 1640, val loss: 1.3071948289871216
Epoch 1650, training loss: 0.6411339640617371 = 0.0021735280752182007 + 0.1 * 6.389604091644287
Epoch 1650, val loss: 1.3086950778961182
Epoch 1660, training loss: 0.6419562697410583 = 0.002151052700355649 + 0.1 * 6.398051738739014
Epoch 1660, val loss: 1.3102138042449951
Epoch 1670, training loss: 0.6415626406669617 = 0.00212903437204659 + 0.1 * 6.394336223602295
Epoch 1670, val loss: 1.3116679191589355
Epoch 1680, training loss: 0.6416879892349243 = 0.0021075496915727854 + 0.1 * 6.395803928375244
Epoch 1680, val loss: 1.3131364583969116
Epoch 1690, training loss: 0.6404860615730286 = 0.0020864729303866625 + 0.1 * 6.38399600982666
Epoch 1690, val loss: 1.3146034479141235
Epoch 1700, training loss: 0.6401699185371399 = 0.0020657521672546864 + 0.1 * 6.381041049957275
Epoch 1700, val loss: 1.316071629524231
Epoch 1710, training loss: 0.6409755945205688 = 0.0020453170873224735 + 0.1 * 6.3893022537231445
Epoch 1710, val loss: 1.3175455331802368
Epoch 1720, training loss: 0.6403968334197998 = 0.0020252959802746773 + 0.1 * 6.3837151527404785
Epoch 1720, val loss: 1.3189654350280762
Epoch 1730, training loss: 0.6413649320602417 = 0.002005769871175289 + 0.1 * 6.39359188079834
Epoch 1730, val loss: 1.3204100131988525
Epoch 1740, training loss: 0.6399083733558655 = 0.001986611867323518 + 0.1 * 6.379217147827148
Epoch 1740, val loss: 1.3217896223068237
Epoch 1750, training loss: 0.6406697034835815 = 0.0019678643438965082 + 0.1 * 6.38701868057251
Epoch 1750, val loss: 1.3231942653656006
Epoch 1760, training loss: 0.6395477056503296 = 0.001949405879713595 + 0.1 * 6.375982761383057
Epoch 1760, val loss: 1.3245534896850586
Epoch 1770, training loss: 0.6406890153884888 = 0.0019313193624839187 + 0.1 * 6.387577056884766
Epoch 1770, val loss: 1.3259344100952148
Epoch 1780, training loss: 0.639874279499054 = 0.0019135719630867243 + 0.1 * 6.379607200622559
Epoch 1780, val loss: 1.3273158073425293
Epoch 1790, training loss: 0.6412425637245178 = 0.0018960312008857727 + 0.1 * 6.393465042114258
Epoch 1790, val loss: 1.3286763429641724
Epoch 1800, training loss: 0.6388018727302551 = 0.0018788805464282632 + 0.1 * 6.369229793548584
Epoch 1800, val loss: 1.3300204277038574
Epoch 1810, training loss: 0.6405472755432129 = 0.0018619915936142206 + 0.1 * 6.386852741241455
Epoch 1810, val loss: 1.3313742876052856
Epoch 1820, training loss: 0.640177309513092 = 0.0018454021774232388 + 0.1 * 6.383318901062012
Epoch 1820, val loss: 1.3327006101608276
Epoch 1830, training loss: 0.6386145949363708 = 0.0018292074091732502 + 0.1 * 6.36785364151001
Epoch 1830, val loss: 1.334010124206543
Epoch 1840, training loss: 0.641241729259491 = 0.0018131820252165198 + 0.1 * 6.394285202026367
Epoch 1840, val loss: 1.335327386856079
Epoch 1850, training loss: 0.6388342976570129 = 0.0017974951770156622 + 0.1 * 6.370368003845215
Epoch 1850, val loss: 1.33663010597229
Epoch 1860, training loss: 0.6398650407791138 = 0.0017821211367845535 + 0.1 * 6.380828857421875
Epoch 1860, val loss: 1.337911605834961
Epoch 1870, training loss: 0.6381526589393616 = 0.0017669734079390764 + 0.1 * 6.363856792449951
Epoch 1870, val loss: 1.339181661605835
Epoch 1880, training loss: 0.6395276188850403 = 0.0017520560650154948 + 0.1 * 6.377755165100098
Epoch 1880, val loss: 1.3404637575149536
Epoch 1890, training loss: 0.6391813158988953 = 0.0017373928567394614 + 0.1 * 6.374438762664795
Epoch 1890, val loss: 1.341745376586914
Epoch 1900, training loss: 0.6387641429901123 = 0.0017229834338650107 + 0.1 * 6.370411396026611
Epoch 1900, val loss: 1.3430196046829224
Epoch 1910, training loss: 0.6398143768310547 = 0.0017087954329326749 + 0.1 * 6.3810553550720215
Epoch 1910, val loss: 1.3442589044570923
Epoch 1920, training loss: 0.6401771903038025 = 0.0016948519041761756 + 0.1 * 6.384823322296143
Epoch 1920, val loss: 1.345472812652588
Epoch 1930, training loss: 0.6382675170898438 = 0.0016812627436593175 + 0.1 * 6.3658623695373535
Epoch 1930, val loss: 1.3466846942901611
Epoch 1940, training loss: 0.6385005712509155 = 0.0016678479732945561 + 0.1 * 6.3683271408081055
Epoch 1940, val loss: 1.3479331731796265
Epoch 1950, training loss: 0.6385847330093384 = 0.0016545015387237072 + 0.1 * 6.369302272796631
Epoch 1950, val loss: 1.3491864204406738
Epoch 1960, training loss: 0.6387128829956055 = 0.001641355105675757 + 0.1 * 6.370715141296387
Epoch 1960, val loss: 1.3504148721694946
Epoch 1970, training loss: 0.6381454467773438 = 0.0016284772427752614 + 0.1 * 6.365170001983643
Epoch 1970, val loss: 1.351644515991211
Epoch 1980, training loss: 0.6387800574302673 = 0.0016157333739101887 + 0.1 * 6.37164306640625
Epoch 1980, val loss: 1.3528735637664795
Epoch 1990, training loss: 0.6395745873451233 = 0.0016032023122534156 + 0.1 * 6.379714012145996
Epoch 1990, val loss: 1.3540465831756592
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 2.794426441192627 = 1.9347416162490845 + 0.1 * 8.596847534179688
Epoch 0, val loss: 1.9302858114242554
Epoch 10, training loss: 2.785304307937622 = 1.9256250858306885 + 0.1 * 8.596792221069336
Epoch 10, val loss: 1.92185378074646
Epoch 20, training loss: 2.7743968963623047 = 1.9147590398788452 + 0.1 * 8.5963773727417
Epoch 20, val loss: 1.9114960432052612
Epoch 30, training loss: 2.759038209915161 = 1.8997974395751953 + 0.1 * 8.5924072265625
Epoch 30, val loss: 1.8970192670822144
Epoch 40, training loss: 2.7337489128112793 = 1.877882957458496 + 0.1 * 8.558659553527832
Epoch 40, val loss: 1.8758403062820435
Epoch 50, training loss: 2.685777425765991 = 1.8474652767181396 + 0.1 * 8.383121490478516
Epoch 50, val loss: 1.8474456071853638
Epoch 60, training loss: 2.624173879623413 = 1.8124220371246338 + 0.1 * 8.117517471313477
Epoch 60, val loss: 1.816487193107605
Epoch 70, training loss: 2.5624613761901855 = 1.7793030738830566 + 0.1 * 7.8315839767456055
Epoch 70, val loss: 1.7879269123077393
Epoch 80, training loss: 2.483980894088745 = 1.7454913854599 + 0.1 * 7.384895324707031
Epoch 80, val loss: 1.7562648057937622
Epoch 90, training loss: 2.4204154014587402 = 1.703149437904358 + 0.1 * 7.172659397125244
Epoch 90, val loss: 1.7166193723678589
Epoch 100, training loss: 2.3542861938476562 = 1.6451759338378906 + 0.1 * 7.0911030769348145
Epoch 100, val loss: 1.6653540134429932
Epoch 110, training loss: 2.2744927406311035 = 1.5695894956588745 + 0.1 * 7.0490312576293945
Epoch 110, val loss: 1.6007150411605835
Epoch 120, training loss: 2.1825389862060547 = 1.4798978567123413 + 0.1 * 7.026411056518555
Epoch 120, val loss: 1.5244336128234863
Epoch 130, training loss: 2.0834097862243652 = 1.382728934288025 + 0.1 * 7.006808280944824
Epoch 130, val loss: 1.4440025091171265
Epoch 140, training loss: 1.9815819263458252 = 1.2825995683670044 + 0.1 * 6.989823341369629
Epoch 140, val loss: 1.3643327951431274
Epoch 150, training loss: 1.877803087234497 = 1.180997371673584 + 0.1 * 6.9680562019348145
Epoch 150, val loss: 1.2856155633926392
Epoch 160, training loss: 1.7755053043365479 = 1.0804569721221924 + 0.1 * 6.950482368469238
Epoch 160, val loss: 1.2107105255126953
Epoch 170, training loss: 1.678396463394165 = 0.9866268634796143 + 0.1 * 6.917695045471191
Epoch 170, val loss: 1.1444276571273804
Epoch 180, training loss: 1.5886476039886475 = 0.8989591002464294 + 0.1 * 6.896885395050049
Epoch 180, val loss: 1.0851377248764038
Epoch 190, training loss: 1.5058056116104126 = 0.817808985710144 + 0.1 * 6.8799662590026855
Epoch 190, val loss: 1.032902479171753
Epoch 200, training loss: 1.4298760890960693 = 0.7432361245155334 + 0.1 * 6.866399765014648
Epoch 200, val loss: 0.9871705174446106
Epoch 210, training loss: 1.3612606525421143 = 0.674574077129364 + 0.1 * 6.866865634918213
Epoch 210, val loss: 0.9472718834877014
Epoch 220, training loss: 1.2966411113739014 = 0.6122297644615173 + 0.1 * 6.844113349914551
Epoch 220, val loss: 0.9132388234138489
Epoch 230, training loss: 1.2380712032318115 = 0.5549574494361877 + 0.1 * 6.8311381340026855
Epoch 230, val loss: 0.8838788866996765
Epoch 240, training loss: 1.1854534149169922 = 0.5023493766784668 + 0.1 * 6.8310394287109375
Epoch 240, val loss: 0.8593897223472595
Epoch 250, training loss: 1.1352136135101318 = 0.4539257884025574 + 0.1 * 6.812877655029297
Epoch 250, val loss: 0.8395814299583435
Epoch 260, training loss: 1.0893852710723877 = 0.4087032079696655 + 0.1 * 6.806821346282959
Epoch 260, val loss: 0.8237406015396118
Epoch 270, training loss: 1.0458970069885254 = 0.3664763867855072 + 0.1 * 6.794206142425537
Epoch 270, val loss: 0.8113583326339722
Epoch 280, training loss: 1.005995750427246 = 0.327017217874527 + 0.1 * 6.789784908294678
Epoch 280, val loss: 0.8020026087760925
Epoch 290, training loss: 0.9687288999557495 = 0.2908448278903961 + 0.1 * 6.778840065002441
Epoch 290, val loss: 0.7954087853431702
Epoch 300, training loss: 0.9343141317367554 = 0.2581462860107422 + 0.1 * 6.761678218841553
Epoch 300, val loss: 0.7912553548812866
Epoch 310, training loss: 0.9039959907531738 = 0.2289760261774063 + 0.1 * 6.750199794769287
Epoch 310, val loss: 0.7895819544792175
Epoch 320, training loss: 0.8776204586029053 = 0.20335564017295837 + 0.1 * 6.742647647857666
Epoch 320, val loss: 0.7901725172996521
Epoch 330, training loss: 0.8548698425292969 = 0.18101446330547333 + 0.1 * 6.738553524017334
Epoch 330, val loss: 0.7928053736686707
Epoch 340, training loss: 0.8343534469604492 = 0.1617317795753479 + 0.1 * 6.726216793060303
Epoch 340, val loss: 0.7972505688667297
Epoch 350, training loss: 0.8170592188835144 = 0.14510978758335114 + 0.1 * 6.719494342803955
Epoch 350, val loss: 0.8030602931976318
Epoch 360, training loss: 0.8022919297218323 = 0.1306404024362564 + 0.1 * 6.71651554107666
Epoch 360, val loss: 0.8102046847343445
Epoch 370, training loss: 0.7891963124275208 = 0.11801022291183472 + 0.1 * 6.711860656738281
Epoch 370, val loss: 0.8184308409690857
Epoch 380, training loss: 0.7765690088272095 = 0.10695654898881912 + 0.1 * 6.69612455368042
Epoch 380, val loss: 0.8273797631263733
Epoch 390, training loss: 0.7658995389938354 = 0.09722156822681427 + 0.1 * 6.686779975891113
Epoch 390, val loss: 0.8370013236999512
Epoch 400, training loss: 0.7576082944869995 = 0.08859055489301682 + 0.1 * 6.690176963806152
Epoch 400, val loss: 0.8472293019294739
Epoch 410, training loss: 0.7483206987380981 = 0.08094874024391174 + 0.1 * 6.673719882965088
Epoch 410, val loss: 0.8577106595039368
Epoch 420, training loss: 0.7407272458076477 = 0.07415052503347397 + 0.1 * 6.665767192840576
Epoch 420, val loss: 0.8684282898902893
Epoch 430, training loss: 0.7338553667068481 = 0.06805814057588577 + 0.1 * 6.65797233581543
Epoch 430, val loss: 0.8793736100196838
Epoch 440, training loss: 0.7292038202285767 = 0.06260324269533157 + 0.1 * 6.666005611419678
Epoch 440, val loss: 0.8903777599334717
Epoch 450, training loss: 0.7224975824356079 = 0.057726554572582245 + 0.1 * 6.64771032333374
Epoch 450, val loss: 0.9013495445251465
Epoch 460, training loss: 0.7175121903419495 = 0.053333599120378494 + 0.1 * 6.641786098480225
Epoch 460, val loss: 0.9123433232307434
Epoch 470, training loss: 0.7139629125595093 = 0.04936972260475159 + 0.1 * 6.645931720733643
Epoch 470, val loss: 0.923244833946228
Epoch 480, training loss: 0.7095465660095215 = 0.045809902250766754 + 0.1 * 6.63736629486084
Epoch 480, val loss: 0.9339991807937622
Epoch 490, training loss: 0.705237865447998 = 0.04258851706981659 + 0.1 * 6.626492977142334
Epoch 490, val loss: 0.9446254968643188
Epoch 500, training loss: 0.7022383213043213 = 0.03966983035206795 + 0.1 * 6.625685214996338
Epoch 500, val loss: 0.9551210999488831
Epoch 510, training loss: 0.699260950088501 = 0.03702864423394203 + 0.1 * 6.6223225593566895
Epoch 510, val loss: 0.9653910398483276
Epoch 520, training loss: 0.6966521739959717 = 0.0346270427107811 + 0.1 * 6.620251178741455
Epoch 520, val loss: 0.9755476117134094
Epoch 530, training loss: 0.6934463977813721 = 0.03244229778647423 + 0.1 * 6.61004114151001
Epoch 530, val loss: 0.9854733943939209
Epoch 540, training loss: 0.6911759376525879 = 0.030446354299783707 + 0.1 * 6.607295513153076
Epoch 540, val loss: 0.9952092170715332
Epoch 550, training loss: 0.688775897026062 = 0.028624488040804863 + 0.1 * 6.601513862609863
Epoch 550, val loss: 1.0047413110733032
Epoch 560, training loss: 0.6867567300796509 = 0.026960104703903198 + 0.1 * 6.597965717315674
Epoch 560, val loss: 1.0140571594238281
Epoch 570, training loss: 0.6860728859901428 = 0.025432396680116653 + 0.1 * 6.606404781341553
Epoch 570, val loss: 1.0231425762176514
Epoch 580, training loss: 0.682917594909668 = 0.024031003937125206 + 0.1 * 6.588866233825684
Epoch 580, val loss: 1.0320212841033936
Epoch 590, training loss: 0.6808669567108154 = 0.022742491215467453 + 0.1 * 6.581244468688965
Epoch 590, val loss: 1.0406506061553955
Epoch 600, training loss: 0.6799240708351135 = 0.02155921421945095 + 0.1 * 6.583648204803467
Epoch 600, val loss: 1.0490494966506958
Epoch 610, training loss: 0.6781659126281738 = 0.020462969318032265 + 0.1 * 6.577029228210449
Epoch 610, val loss: 1.0572381019592285
Epoch 620, training loss: 0.6771689653396606 = 0.019448962062597275 + 0.1 * 6.577200412750244
Epoch 620, val loss: 1.0652636289596558
Epoch 630, training loss: 0.6751725077629089 = 0.01851389929652214 + 0.1 * 6.566585540771484
Epoch 630, val loss: 1.0730564594268799
Epoch 640, training loss: 0.674786388874054 = 0.01764395646750927 + 0.1 * 6.57142448425293
Epoch 640, val loss: 1.0806677341461182
Epoch 650, training loss: 0.6735657453536987 = 0.016833500936627388 + 0.1 * 6.567322254180908
Epoch 650, val loss: 1.0880894660949707
Epoch 660, training loss: 0.6718311905860901 = 0.01608096808195114 + 0.1 * 6.557502269744873
Epoch 660, val loss: 1.0953178405761719
Epoch 670, training loss: 0.6711991429328918 = 0.015377878211438656 + 0.1 * 6.5582122802734375
Epoch 670, val loss: 1.102368950843811
Epoch 680, training loss: 0.671367883682251 = 0.014722184278070927 + 0.1 * 6.566457271575928
Epoch 680, val loss: 1.1092684268951416
Epoch 690, training loss: 0.6692871451377869 = 0.01411033421754837 + 0.1 * 6.551767826080322
Epoch 690, val loss: 1.1159483194351196
Epoch 700, training loss: 0.6675189733505249 = 0.013538439758121967 + 0.1 * 6.5398054122924805
Epoch 700, val loss: 1.1224491596221924
Epoch 710, training loss: 0.6680481433868408 = 0.013000329956412315 + 0.1 * 6.550477981567383
Epoch 710, val loss: 1.1288564205169678
Epoch 720, training loss: 0.6674628853797913 = 0.012495642527937889 + 0.1 * 6.549672603607178
Epoch 720, val loss: 1.1350805759429932
Epoch 730, training loss: 0.6655675172805786 = 0.01202364731580019 + 0.1 * 6.5354390144348145
Epoch 730, val loss: 1.1411638259887695
Epoch 740, training loss: 0.6649365425109863 = 0.01157992985099554 + 0.1 * 6.533565998077393
Epoch 740, val loss: 1.1470826864242554
Epoch 750, training loss: 0.6645680069923401 = 0.011160235852003098 + 0.1 * 6.534077167510986
Epoch 750, val loss: 1.1528643369674683
Epoch 760, training loss: 0.6634638905525208 = 0.010764101520180702 + 0.1 * 6.5269975662231445
Epoch 760, val loss: 1.1585861444473267
Epoch 770, training loss: 0.6636382937431335 = 0.010389802977442741 + 0.1 * 6.532484531402588
Epoch 770, val loss: 1.1641461849212646
Epoch 780, training loss: 0.6644869446754456 = 0.010036914609372616 + 0.1 * 6.544500350952148
Epoch 780, val loss: 1.169599175453186
Epoch 790, training loss: 0.6619762182235718 = 0.009704779833555222 + 0.1 * 6.522714138031006
Epoch 790, val loss: 1.1748826503753662
Epoch 800, training loss: 0.6608988642692566 = 0.009389295242726803 + 0.1 * 6.5150957107543945
Epoch 800, val loss: 1.1800798177719116
Epoch 810, training loss: 0.6609705090522766 = 0.0090892119333148 + 0.1 * 6.518812656402588
Epoch 810, val loss: 1.185142993927002
Epoch 820, training loss: 0.6599947810173035 = 0.00880457554012537 + 0.1 * 6.51190185546875
Epoch 820, val loss: 1.1901401281356812
Epoch 830, training loss: 0.658886194229126 = 0.008535078726708889 + 0.1 * 6.50351095199585
Epoch 830, val loss: 1.1950212717056274
Epoch 840, training loss: 0.6588257551193237 = 0.008279480040073395 + 0.1 * 6.505462646484375
Epoch 840, val loss: 1.1998029947280884
Epoch 850, training loss: 0.6581922769546509 = 0.008034942671656609 + 0.1 * 6.501573085784912
Epoch 850, val loss: 1.2044557332992554
Epoch 860, training loss: 0.6583521366119385 = 0.007802316453307867 + 0.1 * 6.505497932434082
Epoch 860, val loss: 1.2090483903884888
Epoch 870, training loss: 0.6579392552375793 = 0.007581864017993212 + 0.1 * 6.503573417663574
Epoch 870, val loss: 1.21353280544281
Epoch 880, training loss: 0.6576407551765442 = 0.007370966952294111 + 0.1 * 6.502697944641113
Epoch 880, val loss: 1.2179210186004639
Epoch 890, training loss: 0.6568264365196228 = 0.007170296739786863 + 0.1 * 6.496561050415039
Epoch 890, val loss: 1.2222157716751099
Epoch 900, training loss: 0.6570152044296265 = 0.00697786221280694 + 0.1 * 6.500373363494873
Epoch 900, val loss: 1.226455807685852
Epoch 910, training loss: 0.6554049253463745 = 0.006793969310820103 + 0.1 * 6.486109256744385
Epoch 910, val loss: 1.2305983304977417
