{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Cora', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--proj_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=160,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=0,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default=0)\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0)\n",
    "parser.add_argument('--num_hidden', type=int, default=0)\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=0)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=0)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_feature_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_feature_rate_2', type=float, default=0)\n",
    "parser.add_argument('--tau', type=float, default=0)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=0)\n",
    "parser.add_argument('--cl_weight_decay', type=str, default=0)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/project-robust-contrastive/Robust-Contrastive-Learning/run_preliminary.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-01-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_preliminary.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39massert\u001b[39;00m args\u001b[39m.\u001b[39mgpu_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m8\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-01-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_preliminary.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# torch.cuda.set_device(args.gpu_id)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bszw494-01-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_preliminary.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m config \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(args\u001b[39m.\u001b[39;49mconfig), Loader\u001b[39m=\u001b[39;49mSafeLoader)[args\u001b[39m.\u001b[39mdataset]\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/yaml/__init__.py:79\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(stream, Loader):\n\u001b[1;32m     75\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     loader \u001b[39m=\u001b[39m Loader(stream)\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         \u001b[39mreturn\u001b[39;00m loader\u001b[39m.\u001b[39mget_single_data()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/yaml/loader.py:34\u001b[0m, in \u001b[0;36mSafeLoader.__init__\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, stream):\n\u001b[0;32m---> 34\u001b[0m     Reader\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, stream)\n\u001b[1;32m     35\u001b[0m     Scanner\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m     36\u001b[0m     Parser\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/yaml/reader.py:85\u001b[0m, in \u001b[0;36mReader.__init__\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meof \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_buffer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetermine_encoding()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/yaml/reader.py:124\u001b[0m, in \u001b[0;36mReader.determine_encoding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetermine_encoding\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    123\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meof \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_buffer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_buffer) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_raw()\n\u001b[1;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_buffer, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m    126\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_buffer\u001b[39m.\u001b[39mstartswith(codecs\u001b[39m.\u001b[39mBOM_UTF16_LE):\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/yaml/reader.py:178\u001b[0m, in \u001b[0;36mReader.update_raw\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_raw\u001b[39m(\u001b[39mself\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39m4096\u001b[39m):\n\u001b[0;32m--> 178\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mread(size)\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_buffer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_buffer \u001b[39m=\u001b[39m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import Planetoid, CitationFull\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from model import Encoder, Model, drop_feature\n",
    "from eval import label_classification\n",
    "\n",
    "from construct_graph import construct_noisy_graph,construct_augmentation, construct_augmentation_1\n",
    "\n",
    "def train(model: Model, x, edge_index):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    edge_index_1 = dropout_adj(edge_index, p=drop_edge_rate_1)[0]\n",
    "    edge_index_2 = dropout_adj(edge_index, p=drop_edge_rate_2)[0]\n",
    "    x_1 = drop_feature(x, drop_feature_rate_1)\n",
    "    x_2 = drop_feature(x, drop_feature_rate_2)\n",
    "\n",
    "    z1 = model(x_1, edge_index_1)\n",
    "    z2 = model(x_2, edge_index_2)\n",
    "\n",
    "    loss = model.loss(z1, z2, batch_size=0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "def train_1(model: Model, optimizer, x, edge_index,edge_weights = None,seen_node_idx = None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    edge_index_1,x_1,edge_index_2,x_2 = construct_augmentation_1(x, edge_index, None)\n",
    "    # print(edge_index_1,edge_index_2)\n",
    "    # edge_index_1 = dropout_adj(edge_index, p=drop_edge_rate_1)[0]\n",
    "    # edge_index_2 = dropout_adj(edge_index, p=drop_edge_rate_2)[0]\n",
    "    # x_1 = drop_feature(x, drop_feature_rate_1)\n",
    "    # x_2 = drop_feature(x, drop_feature_rate_2)\n",
    "\n",
    "    z1 = model(x_1, edge_index_1)\n",
    "    z2 = model(x_2, edge_index_2)\n",
    "    if(seen_node_idx!=None):\n",
    "        loss = model.loss(z1[seen_node_idx], z2[seen_node_idx], batch_size=0)\n",
    "    else:\n",
    "        loss = model.loss(z1, z2, batch_size=0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def test(model: Model, x, edge_index, y, idx_train, idx_test, final=False):\n",
    "    model.eval()\n",
    "    z = model(x, edge_index)\n",
    "\n",
    "    results = label_classification(z, y, idx_train, idx_test)\n",
    "    return results['F1Mi']['mean'],results['F1Ma']['mean']\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type=str, default='Cora')\n",
    "# parser.add_argument('--gpu_id', type=int, default=0)\n",
    "# parser.add_argument('--config', type=str, default='config.yaml')\n",
    "# args = parser.parse_known_args()[0]\n",
    "\n",
    "assert args.gpu_id in range(0, 8)\n",
    "# torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "\n",
    "# torch.manual_seed(config['seed'])\n",
    "# random.seed(config['seed'])\n",
    "learning_rate = config['learning_rate']\n",
    "num_hidden = config['num_hidden']\n",
    "num_proj_hidden = config['num_proj_hidden']\n",
    "activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[config['activation']]\n",
    "base_model = ({'GCNConv': GCNConv})[config['base_model']]\n",
    "num_layers = config['num_layers']\n",
    "\n",
    "drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "drop_feature_rate_1 = config['drop_feature_rate_1']\n",
    "drop_feature_rate_2 = config['drop_feature_rate_2']\n",
    "tau = config['tau']\n",
    "num_epochs = config['num_epochs']\n",
    "weight_decay = config['weight_decay']\n",
    "\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.3,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n",
    "# gdc(A,1,1)\n",
    "# import copy \n",
    "# from model import UnifyModel\n",
    "# from models.construct import model_construct\n",
    "# data = data.to(device)\n",
    "# config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "# num_epochs = config['num_epochs']\n",
    "# learning_rate = config['learning_rate']\n",
    "# weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# args.cont_batch_size = config['cont_batch_size']\n",
    "# args.cont_weight = config['cont_weight']\n",
    "# args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# # args.add_edge_rate_1 = 0\n",
    "# # args.add_edge_rate_2 = 0\n",
    "# # args.drop_edge_rate_1 = 0.3\n",
    "# # args.drop_edge_rate_2 = 0.5\n",
    "# # args.drop_feat_rate_1 = 0.4\n",
    "# # args.drop_feat_rate_2 = 0.4\n",
    "# num_class = int(data.y.max()+1)\n",
    "\n",
    "# noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "# noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data)\n",
    "# diff_noisy_data = diff_dataset.data\n",
    "# # diff_dataset.processed_paths\n",
    "# diff_noisy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Structure Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transductive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "'''Transductive'''\n",
    "import copy \n",
    "encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "gnn_model = model_construct(args,'GCN',data,device)\n",
    "\n",
    "model_origin = copy.deepcopy(model)\n",
    "encoder_origin = copy.deepcopy(encoder)\n",
    "optimizer_origin = copy.deepcopy(optimizer)\n",
    "gnn_model_origin = copy.deepcopy(gnn_model)\n",
    "\n",
    "# seeds = [args.seed]\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    model = copy.deepcopy(model_origin)\n",
    "    encoder = copy.deepcopy(encoder_origin)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, data.x, data.edge_index)\n",
    "\n",
    "        now = t()\n",
    "        if(epoch%10 == 0):\n",
    "            print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "                    f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, data.x, data.edge_index, data.y, idx_train, idx_overall_test, final=True)\n",
    "    final_cl_acc.append(f1mi)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model = copy.deepcopy(gnn_model_origin)\n",
    "    gnn_model.fit(data.x, data.edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_overall_test)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"=== Noisy graph ===\")\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "for seed in seeds:\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "    model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, noisy_data.x, noisy_data.edge_index)\n",
    "\n",
    "        now = t()\n",
    "        if(epoch%10 == 0):\n",
    "            print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "                    f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)\n",
    "    final_cl_acc_noisy.append(f1mi)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))\n",
    "# print(\"=== Nosi graph: random noise ===\")\n",
    "# for seed in seeds:\n",
    "#     np.random.seed(seed)\n",
    "#     # torch.manual_seed(seed)\n",
    "#     # torch.cuda.manual_seed(seed)\n",
    "#     encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "#                         base_model=base_model, k=num_layers).to(device)\n",
    "#     model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#     start = t()\n",
    "#     prev = start\n",
    "#     for epoch in range(1, num_epochs + 1):\n",
    "#         # loss = train_(model, data.x, data.edge_index)\n",
    "#         loss = train_1(model, noisy_data.x, noisy_data.edge_index)\n",
    "\n",
    "#         now = t()\n",
    "#         # if(epoch%10 == 0):\n",
    "#     \n",
    "#     #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "#         #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "#         prev = now\n",
    "\n",
    "#     print(\"=== Test ===\")\n",
    "#     test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inductive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transductive'''\n",
    "import copy \n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "\n",
    "encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "gnn_model = model_construct(args,'GCN',data,device)\n",
    "\n",
    "model_origin = copy.deepcopy(model)\n",
    "encoder_origin = copy.deepcopy(encoder)\n",
    "optimizer_origin = copy.deepcopy(optimizer)\n",
    "gnn_model_origin = copy.deepcopy(gnn_model)\n",
    "\n",
    "# seeds = [args.seed]\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    model = copy.deepcopy(model_origin)\n",
    "    encoder = copy.deepcopy(encoder_origin)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, data.x, train_edge_index, None, seen_node_idx)\n",
    "\n",
    "        now = t()\n",
    "        # if(epoch%10 == 0):\n",
    "        #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "        #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, data.x, data.edge_index, data.y, idx_train, idx_clean_test, final=True)\n",
    "    final_cl_acc.append(f1mi)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model = copy.deepcopy(gnn_model_origin)\n",
    "    gnn_model.fit(data.x, train_edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_clean_test)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"=== Noisy graph ===\")\n",
    "noisy_train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(noisy_data.test_mask),noisy_data.edge_index,relabel_nodes=False)\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "for seed in seeds:\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "    model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, noisy_data.x, noisy_train_edge_index, None, seen_node_idx)\n",
    "\n",
    "        now = t()\n",
    "        # if(epoch%10 == 0):\n",
    "        #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "        #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)\n",
    "    final_cl_acc_noisy.append(f1mi)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_train_edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))\n",
    "# print(\"=== Nosi graph: random noise ===\")\n",
    "# for seed in seeds:\n",
    "#     np.random.seed(seed)\n",
    "#     # torch.manual_seed(seed)\n",
    "#     # torch.cuda.manual_seed(seed)\n",
    "#     encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "#                         base_model=base_model, k=num_layers).to(device)\n",
    "#     model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#     start = t()\n",
    "#     prev = start\n",
    "#     for epoch in range(1, num_epochs + 1):\n",
    "#         # loss = train_(model, data.x, data.edge_index)\n",
    "#         loss = train_1(model, noisy_data.x, noisy_data.edge_index)\n",
    "\n",
    "#         now = t()\n",
    "#         # if(epoch%10 == 0):\n",
    "#     \n",
    "#     #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "#         #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "#         prev = now\n",
    "\n",
    "#     print(\"=== Test ===\")\n",
    "#     test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Contrastive learning to backdoor in Contrastive learning'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # learn contrastive node representation\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "    contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(contrastive_model, optimizer, poison_x, poison_edge_index, poison_edge_weights, seen_node_idx)\n",
    "\n",
    "        now = t()\n",
    "        # if(epoch%10 == 0):\n",
    "        #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "        #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    contrastive_model.eval()\n",
    "    cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    # test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "                nhid=args.hidden,\\\n",
    "                nclass= int(data.y.max()+1),\\\n",
    "                dropout=args.dropout,\\\n",
    "                lr=args.train_lr,\\\n",
    "                weight_decay=args.weight_decay,\\\n",
    "                device=device).to(device) \n",
    "    test_model.fit(cont_poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(cont_poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(cont_poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(cont_induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    elif(args.evaluate_mode == 'overall'):\n",
    "        # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        # test_model = test_model.to(device)\n",
    "        output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Backdoor attack to GNN classifier'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, poison_x, poison_edge_index, poison_edge_weights)\n",
    "\n",
    "    #     now = t()\n",
    "    #     if(epoch%10 == 0):\n",
    "    #         print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #                 f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "num_epochs = config['num_epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.10,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "print(\"=== Noisy graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device).to(device)\n",
    "    # model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc_noisy.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "num_epochs = config['num_epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=None).to(device)\n",
    "    model.fit(args, data.x, data.edge_index,data.edge_weight,data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(data.x, data.edge_index,data.edge_weight,data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',data,device)\n",
    "    gnn_model.fit(data.x, data.edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy+DIffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "num_epochs = config['num_epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "\n",
    "diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "print(\"=== Noisy graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    # model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device).to(device)\n",
    "    model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    # model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc_noisy.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN+CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Contrastive learning to backdoor in Contrastive learning'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "\n",
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "args.homo_loss_weight=config['homo_loss_weight']\n",
    "args.homo_boost_thrd = config['homo_boost_thrd']\n",
    "args.trojan_epochs = config['trojan_epochs']\n",
    "args.selection_method = config['selection_method']\n",
    "args.vs_number=config['vs_number']\n",
    "args.num_epochs = config['num_epochs']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "data = data.to(device)\n",
    "# learning_rate = 0.0002\n",
    "weight_decay = config['weight_decay']\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                         base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=None).to(device)\n",
    "    # contrastive_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,idx_train,idx_val=idx_val,train_iters=1000,cont_iters=num_epochs,seen_node_idx=seen_node_idx)\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, optimizer, poison_x, poison_edge_index, poison_edge_weights, seen_node_idx)\n",
    "\n",
    "    #     now = t()\n",
    "    #     # if(epoch%10 == 0):\n",
    "    #     #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #     #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    # test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=None).to(device)\n",
    "    # test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=seen_node_idx)\n",
    "    test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    output = test_model.clf_head(test_embds)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    # test_model.fit(cont_poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    # output = test_model(cont_poison_x,poison_edge_index,poison_edge_weights)\n",
    "    # train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    # print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    # torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "                output = test_model.clf_head(test_embeds)\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                # output = test_model(cont_induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    elif(args.evaluate_mode == 'overall'):\n",
    "        # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "        output = test_model.clf_head(test_embeds)\n",
    "        # test_model = test_model.to(device)\n",
    "        # output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Backdoor attack to GNN classifier'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "args.homo_loss_weight=config['homo_loss_weight']\n",
    "args.vs_number=config['vs_number']\n",
    "args.trojan_epochs = config['trojan_epochs']\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "args.seed = config['seed']\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, poison_x, poison_edge_index, poison_edge_weights)\n",
    "\n",
    "    #     now = t()\n",
    "    #     if(epoch%10 == 0):\n",
    "    #         print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #                 f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4260eba67904b42d68a3963bc583366103d86fb6c89846e20de6072b78e7707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
