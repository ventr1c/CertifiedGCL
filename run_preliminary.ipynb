{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2452713/3161803554.py:6: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config='config.yaml', cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=3, dis_weight=1, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=160, vs_ratio=0, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Cora', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--proj_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=160,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=0,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default=0)\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0)\n",
    "parser.add_argument('--num_hidden', type=int, default=0)\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=0)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=0)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_feature_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_feature_rate_2', type=float, default=0)\n",
    "parser.add_argument('--tau', type=float, default=0)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=0)\n",
    "parser.add_argument('--cl_weight_decay', type=str, default=0)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 13770])\n",
      "remove edge: torch.Size([2, 7470])\n",
      "updated graph: torch.Size([2, 10684])\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import Planetoid, CitationFull\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from model import Encoder, Model, drop_feature\n",
    "from eval import label_classification\n",
    "\n",
    "from construct_graph import construct_noisy_graph,construct_augmentation, construct_augmentation_1\n",
    "\n",
    "def train(model: Model, x, edge_index):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    edge_index_1 = dropout_adj(edge_index, p=drop_edge_rate_1)[0]\n",
    "    edge_index_2 = dropout_adj(edge_index, p=drop_edge_rate_2)[0]\n",
    "    x_1 = drop_feature(x, drop_feature_rate_1)\n",
    "    x_2 = drop_feature(x, drop_feature_rate_2)\n",
    "\n",
    "    z1 = model(x_1, edge_index_1)\n",
    "    z2 = model(x_2, edge_index_2)\n",
    "\n",
    "    loss = model.loss(z1, z2, batch_size=0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "def train_1(model: Model, optimizer, x, edge_index,edge_weights = None,seen_node_idx = None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    edge_index_1,x_1,edge_index_2,x_2 = construct_augmentation_1(x, edge_index, None)\n",
    "    # print(edge_index_1,edge_index_2)\n",
    "    # edge_index_1 = dropout_adj(edge_index, p=drop_edge_rate_1)[0]\n",
    "    # edge_index_2 = dropout_adj(edge_index, p=drop_edge_rate_2)[0]\n",
    "    # x_1 = drop_feature(x, drop_feature_rate_1)\n",
    "    # x_2 = drop_feature(x, drop_feature_rate_2)\n",
    "\n",
    "    z1 = model(x_1, edge_index_1)\n",
    "    z2 = model(x_2, edge_index_2)\n",
    "    if(seen_node_idx!=None):\n",
    "        loss = model.loss(z1[seen_node_idx], z2[seen_node_idx], batch_size=0)\n",
    "    else:\n",
    "        loss = model.loss(z1, z2, batch_size=0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def test(model: Model, x, edge_index, y, idx_train, idx_test, final=False):\n",
    "    model.eval()\n",
    "    z = model(x, edge_index)\n",
    "\n",
    "    results = label_classification(z, y, idx_train, idx_test)\n",
    "    return results['F1Mi']['mean'],results['F1Ma']['mean']\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type=str, default='Cora')\n",
    "# parser.add_argument('--gpu_id', type=int, default=0)\n",
    "# parser.add_argument('--config', type=str, default='config.yaml')\n",
    "# args = parser.parse_known_args()[0]\n",
    "\n",
    "assert args.gpu_id in range(0, 8)\n",
    "# torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "\n",
    "# torch.manual_seed(config['seed'])\n",
    "# random.seed(config['seed'])\n",
    "learning_rate = config['learning_rate']\n",
    "num_hidden = config['num_hidden']\n",
    "num_proj_hidden = config['num_proj_hidden']\n",
    "activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[config['activation']]\n",
    "base_model = ({'GCNConv': GCNConv})[config['base_model']]\n",
    "num_layers = config['num_layers']\n",
    "\n",
    "drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "drop_feature_rate_1 = config['drop_feature_rate_1']\n",
    "drop_feature_rate_2 = config['drop_feature_rate_2']\n",
    "tau = config['tau']\n",
    "num_epochs = config['num_epochs']\n",
    "weight_decay = config['weight_decay']\n",
    "\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.3,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n",
    "# gdc(A,1,1)\n",
    "# import copy \n",
    "# from model import UnifyModel\n",
    "# from models.construct import model_construct\n",
    "# data = data.to(device)\n",
    "# config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "# num_epochs = config['num_epochs']\n",
    "# learning_rate = config['learning_rate']\n",
    "# weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# args.cont_batch_size = config['cont_batch_size']\n",
    "# args.cont_weight = config['cont_weight']\n",
    "# args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# # args.add_edge_rate_1 = 0\n",
    "# # args.add_edge_rate_2 = 0\n",
    "# # args.drop_edge_rate_1 = 0.3\n",
    "# # args.drop_edge_rate_2 = 0.5\n",
    "# # args.drop_feat_rate_1 = 0.4\n",
    "# # args.drop_feat_rate_2 = 0.4\n",
    "# num_class = int(data.y.max()+1)\n",
    "\n",
    "# noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "# noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data)\n",
    "# diff_noisy_data = diff_dataset.data\n",
    "# # diff_dataset.processed_paths\n",
    "# diff_noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 40819], edge_attr=[40819], y=[2708], train_mask=[2708], test_mask=[2708], val_mask=[2708])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_noisy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Structure Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transductive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw graph ===\n",
      "Learn node representations via contrastive learning\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'add_edge_rate_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m prev \u001b[39m=\u001b[39m start\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m# loss = train_(model, data.x, data.edge_index)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_1(model, optimizer, data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     now \u001b[39m=\u001b[39m t()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mif\u001b[39;00m(epoch\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain_1\u001b[0;34m(model, optimizer, x, edge_index, edge_weights, seen_node_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m edge_index_1,x_1,edge_index_2,x_2 \u001b[39m=\u001b[39m construct_augmentation_1(x, edge_index, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# print(edge_index_1,edge_index_2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# edge_index_1 = dropout_adj(edge_index, p=drop_edge_rate_1)[0]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# edge_index_2 = dropout_adj(edge_index, p=drop_edge_rate_2)[0]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# x_1 = drop_feature(x, drop_feature_rate_1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# x_2 = drop_feature(x, drop_feature_rate_2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m z1 \u001b[39m=\u001b[39m model(x_1, edge_index_1)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/construct_graph.py:60\u001b[0m, in \u001b[0;36mconstruct_augmentation_1\u001b[0;34m(args, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstruct_augmentation_1\u001b[39m(args, x, edge_index, edge_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m     \u001b[39m# graph 1:\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     noisy_edge_index,added_edges\u001b[39m=\u001b[39madd_random_edge(edge_index,force_undirected\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,p\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39;49madd_edge_rate_1)\n\u001b[1;32m     61\u001b[0m     noisy_edge_index,noisy_edge_weight\u001b[39m=\u001b[39mdropout_adj(edge_index,edge_weight,force_undirected\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,p\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mdrop_edge_rate_1)\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(added_edges)\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'add_edge_rate_1'"
     ]
    }
   ],
   "source": [
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "'''Transductive'''\n",
    "import copy \n",
    "encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "gnn_model = model_construct(args,'GCN',data,device)\n",
    "\n",
    "model_origin = copy.deepcopy(model)\n",
    "encoder_origin = copy.deepcopy(encoder)\n",
    "optimizer_origin = copy.deepcopy(optimizer)\n",
    "gnn_model_origin = copy.deepcopy(gnn_model)\n",
    "\n",
    "# seeds = [args.seed]\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    model = copy.deepcopy(model_origin)\n",
    "    encoder = copy.deepcopy(encoder_origin)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, data.x, data.edge_index)\n",
    "\n",
    "        now = t()\n",
    "        if(epoch%10 == 0):\n",
    "            print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "                    f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, data.x, data.edge_index, data.y, idx_train, idx_overall_test, final=True)\n",
    "    final_cl_acc.append(f1mi)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model = copy.deepcopy(gnn_model_origin)\n",
    "    gnn_model.fit(data.x, data.edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_overall_test)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"=== Noisy graph ===\")\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "for seed in seeds:\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "    model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, noisy_data.x, noisy_data.edge_index)\n",
    "\n",
    "        now = t()\n",
    "        if(epoch%10 == 0):\n",
    "            print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "                    f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)\n",
    "    final_cl_acc_noisy.append(f1mi)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))\n",
    "# print(\"=== Nosi graph: random noise ===\")\n",
    "# for seed in seeds:\n",
    "#     np.random.seed(seed)\n",
    "#     # torch.manual_seed(seed)\n",
    "#     # torch.cuda.manual_seed(seed)\n",
    "#     encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "#                         base_model=base_model, k=num_layers).to(device)\n",
    "#     model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#     start = t()\n",
    "#     prev = start\n",
    "#     for epoch in range(1, num_epochs + 1):\n",
    "#         # loss = train_(model, data.x, data.edge_index)\n",
    "#         loss = train_1(model, noisy_data.x, noisy_data.edge_index)\n",
    "\n",
    "#         now = t()\n",
    "#         # if(epoch%10 == 0):\n",
    "#     \n",
    "#     #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "#         #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "#         prev = now\n",
    "\n",
    "#     print(\"=== Test ===\")\n",
    "#     test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inductive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw graph ===\n",
      "Learn node representations via contrastive learning\n",
      "(E) | label_classification: F1Mi=0.8089+-0.0000, F1Ma=0.7931+-0.0000\n",
      "accuracy of clean model on clean test nodes: 0.8163\n",
      "Learn node representations via contrastive learning\n",
      "(E) | label_classification: F1Mi=0.8131+-0.0000, F1Ma=0.8030+-0.0000\n",
      "accuracy of clean model on clean test nodes: 0.8215\n",
      "Learn node representations via contrastive learning\n",
      "(E) | label_classification: F1Mi=0.8194+-0.0000, F1Ma=0.8077+-0.0000\n",
      "accuracy of clean model on clean test nodes: 0.8194\n",
      "=== Noisy graph ===\n",
      "Learn node representations via contrastive learning\n",
      "(E) | label_classification: F1Mi=0.7677+-0.0000, F1Ma=0.7487+-0.0000\n",
      "accuracy of clean model on clean test nodes: 0.7825\n",
      "Learn node representations via contrastive learning\n",
      "(E) | label_classification: F1Mi=0.7508+-0.0000, F1Ma=0.7294+-0.0000\n",
      "accuracy of clean model on clean test nodes: 0.7846\n",
      "Learn node representations via contrastive learning\n",
      "(E) | label_classification: F1Mi=0.7645+-0.0000, F1Ma=0.7470+-0.0000\n",
      "accuracy of clean model on clean test nodes: 0.7793\n",
      "The final CL Acc:0.81380, 0.00434, The final GNN Acc:0.81908, 0.00217\n",
      "The final CL Acc:0.76100, 0.00733, The final GNN Acc:0.78212, 0.00217\n"
     ]
    }
   ],
   "source": [
    "'''Transductive'''\n",
    "import copy \n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "\n",
    "encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "gnn_model = model_construct(args,'GCN',data,device)\n",
    "\n",
    "model_origin = copy.deepcopy(model)\n",
    "encoder_origin = copy.deepcopy(encoder)\n",
    "optimizer_origin = copy.deepcopy(optimizer)\n",
    "gnn_model_origin = copy.deepcopy(gnn_model)\n",
    "\n",
    "# seeds = [args.seed]\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    model = copy.deepcopy(model_origin)\n",
    "    encoder = copy.deepcopy(encoder_origin)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, data.x, train_edge_index, None, seen_node_idx)\n",
    "\n",
    "        now = t()\n",
    "        # if(epoch%10 == 0):\n",
    "        #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "        #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, data.x, data.edge_index, data.y, idx_train, idx_clean_test, final=True)\n",
    "    final_cl_acc.append(f1mi)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model = copy.deepcopy(gnn_model_origin)\n",
    "    gnn_model.fit(data.x, train_edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_clean_test)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"=== Noisy graph ===\")\n",
    "noisy_train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(noisy_data.test_mask),noisy_data.edge_index,relabel_nodes=False)\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "for seed in seeds:\n",
    "    print(\"Learn node representations via contrastive learning\")\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "    model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = copy.deepcopy(optimizer_origin)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(model, optimizer, noisy_data.x, noisy_train_edge_index, None, seen_node_idx)\n",
    "\n",
    "        now = t()\n",
    "        # if(epoch%10 == 0):\n",
    "        #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "        #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    # print(\"=== Test ===\")\n",
    "    f1mi,f1ma = test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)\n",
    "    final_cl_acc_noisy.append(f1mi)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_train_edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "    print(\"accuracy of clean model on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))\n",
    "# print(\"=== Nosi graph: random noise ===\")\n",
    "# for seed in seeds:\n",
    "#     np.random.seed(seed)\n",
    "#     # torch.manual_seed(seed)\n",
    "#     # torch.cuda.manual_seed(seed)\n",
    "#     encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "#                         base_model=base_model, k=num_layers).to(device)\n",
    "#     model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#     start = t()\n",
    "#     prev = start\n",
    "#     for epoch in range(1, num_epochs + 1):\n",
    "#         # loss = train_(model, data.x, data.edge_index)\n",
    "#         loss = train_1(model, noisy_data.x, noisy_data.edge_index)\n",
    "\n",
    "#         now = t()\n",
    "#         # if(epoch%10 == 0):\n",
    "#     \n",
    "#     #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "#         #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "#         prev = now\n",
    "\n",
    "#     print(\"=== Test ===\")\n",
    "#     test(model, noisy_data.x, noisy_data.edge_index, noisy_data.y, idx_train, idx_clean_test, final=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:160\n",
      "raw graph: torch.Size([2, 9104])\n",
      "add edge: torch.Size([2, 10002])\n",
      "remove edge: torch.Size([2, 8128])\n",
      "updated graph: torch.Size([2, 9026])\n",
      "./selected_nodes/Citeseer/Overall/seed265/nodes.txt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 158.00 MiB (GPU 3; 47.54 GiB total capacity; 45.93 GiB already allocated; 93.69 MiB free; 46.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# train trigger generator \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m model \u001b[39m=\u001b[39m Backdoor(args,device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(data\u001b[39m.\u001b[39;49mx, train_edge_index, \u001b[39mNone\u001b[39;49;00m, data\u001b[39m.\u001b[39;49my, idx_train,idx_attach, unlabeled_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m poison_x, poison_edge_index, poison_edge_weights, poison_labels \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_poisoned()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m bkd_tn_nodes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([idx_train,idx_attach])\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/models/GTA.py:221\u001b[0m, in \u001b[0;36mBackdoor.fit\u001b[0;34m(self, features, edge_index, edge_weight, labels, idx_train, idx_attach, idx_unlabeled)\u001b[0m\n\u001b[1;32m    219\u001b[0m         loss_inner\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    220\u001b[0m         optimizer_shadow\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 221\u001b[0m         optimizer_trigger\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrojan\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/optim/adam.py:142\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m,), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mdevice) \\\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(p, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format)\n\u001b[1;32m    143\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    144\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 3; 47.54 GiB total capacity; 45.93 GiB already allocated; 93.69 MiB free; 46.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "''' Contrastive learning to backdoor in Contrastive learning'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # learn contrastive node representation\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                        base_model=base_model, k=num_layers).to(device)\n",
    "    contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # loss = train_(model, data.x, data.edge_index)\n",
    "        loss = train_1(contrastive_model, optimizer, poison_x, poison_edge_index, poison_edge_weights, seen_node_idx)\n",
    "\n",
    "        now = t()\n",
    "        # if(epoch%10 == 0):\n",
    "        #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "        #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    contrastive_model.eval()\n",
    "    cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    # test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "                nhid=args.hidden,\\\n",
    "                nclass= int(data.y.max()+1),\\\n",
    "                dropout=args.dropout,\\\n",
    "                lr=args.train_lr,\\\n",
    "                weight_decay=args.weight_decay,\\\n",
    "                device=device).to(device) \n",
    "    test_model.fit(cont_poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(cont_poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(cont_poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(cont_induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    elif(args.evaluate_mode == 'overall'):\n",
    "        # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        # test_model = test_model.to(device)\n",
    "        output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:40\n",
      "raw graph: torch.Size([2, 88648])\n",
      "add edge: torch.Size([2, 97572])\n",
      "remove edge: torch.Size([2, 79578])\n",
      "updated graph: torch.Size([2, 88502])\n",
      "Length of training set: 3943\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.088500738143921\n",
      "acc_val: 0.3927\n",
      "Epoch 10, training loss: 0.904198944568634\n",
      "acc_val: 0.6555\n",
      "Epoch 20, training loss: 0.5371429920196533\n",
      "acc_val: 0.7590\n",
      "Epoch 30, training loss: 0.40202251076698303\n",
      "acc_val: 0.8595\n",
      "Epoch 40, training loss: 0.32376089692115784\n",
      "acc_val: 0.8716\n",
      "Epoch 50, training loss: 0.28753167390823364\n",
      "acc_val: 0.8696\n",
      "Epoch 60, training loss: 0.2911297678947449\n",
      "acc_val: 0.8721\n",
      "Epoch 70, training loss: 0.260020911693573\n",
      "acc_val: 0.8732\n",
      "Epoch 80, training loss: 0.2535090446472168\n",
      "acc_val: 0.8757\n",
      "Epoch 90, training loss: 0.2482851892709732\n",
      "acc_val: 0.8747\n",
      "Epoch 100, training loss: 0.2385844886302948\n",
      "acc_val: 0.8752\n",
      "Epoch 110, training loss: 0.2319324016571045\n",
      "acc_val: 0.8727\n",
      "Epoch 120, training loss: 0.22981302440166473\n",
      "acc_val: 0.8721\n",
      "Epoch 130, training loss: 0.21877065300941467\n",
      "acc_val: 0.8767\n",
      "Epoch 140, training loss: 0.239915132522583\n",
      "acc_val: 0.8640\n",
      "Epoch 150, training loss: 0.21610097587108612\n",
      "acc_val: 0.8701\n",
      "Epoch 160, training loss: 0.2206849604845047\n",
      "acc_val: 0.8681\n",
      "Epoch 170, training loss: 0.20094820857048035\n",
      "acc_val: 0.8701\n",
      "Epoch 180, training loss: 0.20884069800376892\n",
      "acc_val: 0.8727\n",
      "Epoch 190, training loss: 0.20178070664405823\n",
      "acc_val: 0.8762\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 3.6251s\n",
      "Encoder CA on clean test nodes: 0.8346\n",
      "[1 1 2 ... 0 2 0]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8503\n",
      "Overall ASR: 0.8443\n",
      "Flip ASR: 0.8063/1554 nodes\n",
      "Length of training set: 3943\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.1127082109451294\n",
      "acc_val: 0.3927\n",
      "Epoch 10, training loss: 0.9186080098152161\n",
      "acc_val: 0.6413\n",
      "Epoch 20, training loss: 0.5641453862190247\n",
      "acc_val: 0.8153\n",
      "Epoch 30, training loss: 0.4246295392513275\n",
      "acc_val: 0.8513\n",
      "Epoch 40, training loss: 0.35979729890823364\n",
      "acc_val: 0.8671\n",
      "Epoch 50, training loss: 0.3142615854740143\n",
      "acc_val: 0.8671\n",
      "Epoch 60, training loss: 0.2885335087776184\n",
      "acc_val: 0.8721\n",
      "Epoch 70, training loss: 0.26939496397972107\n",
      "acc_val: 0.8772\n",
      "Epoch 80, training loss: 0.256050705909729\n",
      "acc_val: 0.8732\n",
      "Epoch 90, training loss: 0.25859686732292175\n",
      "acc_val: 0.8716\n",
      "Epoch 100, training loss: 0.2417396605014801\n",
      "acc_val: 0.8737\n",
      "Epoch 110, training loss: 0.23783838748931885\n",
      "acc_val: 0.8777\n",
      "Epoch 120, training loss: 0.23951023817062378\n",
      "acc_val: 0.8691\n",
      "Epoch 130, training loss: 0.22805170714855194\n",
      "acc_val: 0.8671\n",
      "Epoch 140, training loss: 0.2315577268600464\n",
      "acc_val: 0.8732\n",
      "Epoch 150, training loss: 0.2315201610326767\n",
      "acc_val: 0.8737\n",
      "Epoch 160, training loss: 0.22380363941192627\n",
      "acc_val: 0.8696\n",
      "Epoch 170, training loss: 0.2119821012020111\n",
      "acc_val: 0.8752\n",
      "Epoch 180, training loss: 0.20953208208084106\n",
      "acc_val: 0.8747\n",
      "Epoch 190, training loss: 0.2026713341474533\n",
      "acc_val: 0.8706\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 3.6721s\n",
      "Encoder CA on clean test nodes: 0.8245\n",
      "[0 0 1 ... 2 1 2]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8529\n",
      "Overall ASR: 0.8621\n",
      "Flip ASR: 0.8282/1554 nodes\n",
      "Length of training set: 3943\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.0943326950073242\n",
      "acc_val: 0.3927\n",
      "Epoch 10, training loss: 0.9545613527297974\n",
      "acc_val: 0.6540\n",
      "Epoch 20, training loss: 0.6092671155929565\n",
      "acc_val: 0.6946\n",
      "Epoch 30, training loss: 0.448663592338562\n",
      "acc_val: 0.8498\n",
      "Epoch 40, training loss: 0.3665342628955841\n",
      "acc_val: 0.8650\n",
      "Epoch 50, training loss: 0.3174534738063812\n",
      "acc_val: 0.8681\n",
      "Epoch 60, training loss: 0.2983534634113312\n",
      "acc_val: 0.8696\n",
      "Epoch 70, training loss: 0.27918708324432373\n",
      "acc_val: 0.8716\n",
      "Epoch 80, training loss: 0.2561819553375244\n",
      "acc_val: 0.8716\n",
      "Epoch 90, training loss: 0.25173330307006836\n",
      "acc_val: 0.8727\n",
      "Epoch 100, training loss: 0.24277564883232117\n",
      "acc_val: 0.8721\n",
      "Epoch 110, training loss: 0.24242721498012543\n",
      "acc_val: 0.8767\n",
      "Epoch 120, training loss: 0.23365148901939392\n",
      "acc_val: 0.8696\n",
      "Epoch 130, training loss: 0.22601470351219177\n",
      "acc_val: 0.8727\n",
      "Epoch 140, training loss: 0.22607795894145966\n",
      "acc_val: 0.8737\n",
      "Epoch 150, training loss: 0.21283762156963348\n",
      "acc_val: 0.8727\n",
      "Epoch 160, training loss: 0.21172067523002625\n",
      "acc_val: 0.8721\n",
      "Epoch 170, training loss: 0.20302805304527283\n",
      "acc_val: 0.8732\n",
      "Epoch 180, training loss: 0.20445340871810913\n",
      "acc_val: 0.8706\n",
      "Epoch 190, training loss: 0.2055342048406601\n",
      "acc_val: 0.8737\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 3.6025s\n",
      "Encoder CA on clean test nodes: 0.8305\n",
      "[1 1 0 ... 2 0 2]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8519\n",
      "Overall ASR: 0.8220\n",
      "Flip ASR: 0.7786/1554 nodes\n",
      "The final ASR:0.84280, 0.01639, Accuracy:0.85168, 0.00104\n"
     ]
    }
   ],
   "source": [
    "'''Backdoor attack to GNN classifier'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, poison_x, poison_edge_index, poison_edge_weights)\n",
    "\n",
    "    #     now = t()\n",
    "    #     if(epoch%10 == 0):\n",
    "    #         print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #                 f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 11578])\n",
      "remove edge: torch.Size([2, 9464])\n",
      "updated graph: torch.Size([2, 10486])\n",
      "=== Noisy graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 27.730697631835938 = 1.940068006515503 + 3 * 8.596877098083496\n",
      "Epoch 0, val loss: 1.9428575038909912\n",
      "Epoch 10, training loss: 27.726070404052734 = 1.9355969429016113 + 3 * 8.596824645996094\n",
      "Epoch 10, val loss: 1.9381632804870605\n",
      "Epoch 20, training loss: 27.720748901367188 = 1.9307069778442383 + 3 * 8.596680641174316\n",
      "Epoch 20, val loss: 1.9330360889434814\n",
      "Epoch 30, training loss: 27.713720321655273 = 1.9249321222305298 + 3 * 8.59626293182373\n",
      "Epoch 30, val loss: 1.9269921779632568\n",
      "Epoch 40, training loss: 27.70213508605957 = 1.917741298675537 + 3 * 8.59479808807373\n",
      "Epoch 40, val loss: 1.9194406270980835\n",
      "Epoch 50, training loss: 27.675506591796875 = 1.9085081815719604 + 3 * 8.58899974822998\n",
      "Epoch 50, val loss: 1.90967857837677\n",
      "Epoch 60, training loss: 27.596033096313477 = 1.8970259428024292 + 3 * 8.566335678100586\n",
      "Epoch 60, val loss: 1.8976662158966064\n",
      "Epoch 70, training loss: 27.299022674560547 = 1.8848713636398315 + 3 * 8.471384048461914\n",
      "Epoch 70, val loss: 1.8853344917297363\n",
      "Epoch 80, training loss: 26.27604103088379 = 1.873010516166687 + 3 * 8.134343147277832\n",
      "Epoch 80, val loss: 1.8735132217407227\n",
      "Epoch 90, training loss: 25.146697998046875 = 1.8657217025756836 + 3 * 7.760324954986572\n",
      "Epoch 90, val loss: 1.8670216798782349\n",
      "Epoch 100, training loss: 24.225234985351562 = 1.8615446090698242 + 3 * 7.454563140869141\n",
      "Epoch 100, val loss: 1.8630200624465942\n",
      "Epoch 110, training loss: 23.707958221435547 = 1.8571287393569946 + 3 * 7.283609867095947\n",
      "Epoch 110, val loss: 1.8584859371185303\n",
      "Epoch 120, training loss: 23.360538482666016 = 1.8517731428146362 + 3 * 7.169588565826416\n",
      "Epoch 120, val loss: 1.853218913078308\n",
      "Epoch 130, training loss: 23.10113525390625 = 1.8458325862884521 + 3 * 7.0851006507873535\n",
      "Epoch 130, val loss: 1.8475730419158936\n",
      "Epoch 140, training loss: 22.904327392578125 = 1.8399335145950317 + 3 * 7.021464824676514\n",
      "Epoch 140, val loss: 1.8419831991195679\n",
      "Epoch 150, training loss: 22.745237350463867 = 1.8342660665512085 + 3 * 6.97032356262207\n",
      "Epoch 150, val loss: 1.8365942239761353\n",
      "Epoch 160, training loss: 22.61323356628418 = 1.8287734985351562 + 3 * 6.9281535148620605\n",
      "Epoch 160, val loss: 1.831363320350647\n",
      "Epoch 170, training loss: 22.49862289428711 = 1.823378562927246 + 3 * 6.891747951507568\n",
      "Epoch 170, val loss: 1.8262678384780884\n",
      "Epoch 180, training loss: 22.40007781982422 = 1.8179783821105957 + 3 * 6.860699653625488\n",
      "Epoch 180, val loss: 1.8212611675262451\n",
      "Epoch 190, training loss: 22.311079025268555 = 1.8126755952835083 + 3 * 6.832801342010498\n",
      "Epoch 190, val loss: 1.81633722782135\n",
      "Epoch 200, training loss: 22.237361907958984 = 1.8074573278427124 + 3 * 6.8099684715271\n",
      "Epoch 200, val loss: 1.8115930557250977\n",
      "Epoch 210, training loss: 22.163135528564453 = 1.8023401498794556 + 3 * 6.786931991577148\n",
      "Epoch 210, val loss: 1.8068898916244507\n",
      "Epoch 220, training loss: 22.096044540405273 = 1.7972737550735474 + 3 * 6.766256809234619\n",
      "Epoch 220, val loss: 1.802294373512268\n",
      "Epoch 230, training loss: 22.03485679626465 = 1.7922680377960205 + 3 * 6.747529029846191\n",
      "Epoch 230, val loss: 1.7977797985076904\n",
      "Epoch 240, training loss: 21.978132247924805 = 1.787267804145813 + 3 * 6.730288505554199\n",
      "Epoch 240, val loss: 1.793312430381775\n",
      "Epoch 250, training loss: 21.930171966552734 = 1.7822309732437134 + 3 * 6.715980529785156\n",
      "Epoch 250, val loss: 1.7888507843017578\n",
      "Epoch 260, training loss: 21.88186264038086 = 1.777153491973877 + 3 * 6.7015700340271\n",
      "Epoch 260, val loss: 1.784369707107544\n",
      "Epoch 270, training loss: 21.83247947692871 = 1.7719628810882568 + 3 * 6.686838626861572\n",
      "Epoch 270, val loss: 1.7798610925674438\n",
      "Epoch 280, training loss: 21.78713607788086 = 1.7666720151901245 + 3 * 6.673488140106201\n",
      "Epoch 280, val loss: 1.7752931118011475\n",
      "Epoch 290, training loss: 21.763071060180664 = 1.7612029314041138 + 3 * 6.6672892570495605\n",
      "Epoch 290, val loss: 1.7706339359283447\n",
      "Epoch 300, training loss: 21.7067813873291 = 1.7555493116378784 + 3 * 6.6504106521606445\n",
      "Epoch 300, val loss: 1.765823483467102\n",
      "Epoch 310, training loss: 21.66950225830078 = 1.749677300453186 + 3 * 6.639941692352295\n",
      "Epoch 310, val loss: 1.760877013206482\n",
      "Epoch 320, training loss: 21.632680892944336 = 1.7435548305511475 + 3 * 6.629708766937256\n",
      "Epoch 320, val loss: 1.7557724714279175\n",
      "Epoch 330, training loss: 21.598711013793945 = 1.7371474504470825 + 3 * 6.620521068572998\n",
      "Epoch 330, val loss: 1.7504667043685913\n",
      "Epoch 340, training loss: 21.56709861755371 = 1.7304283380508423 + 3 * 6.6122236251831055\n",
      "Epoch 340, val loss: 1.7449350357055664\n",
      "Epoch 350, training loss: 21.530893325805664 = 1.7233821153640747 + 3 * 6.602503776550293\n",
      "Epoch 350, val loss: 1.7391737699508667\n",
      "Epoch 360, training loss: 21.497785568237305 = 1.7159727811813354 + 3 * 6.593937397003174\n",
      "Epoch 360, val loss: 1.7331544160842896\n",
      "Epoch 370, training loss: 21.46928596496582 = 1.7081822156906128 + 3 * 6.587035179138184\n",
      "Epoch 370, val loss: 1.7268341779708862\n",
      "Epoch 380, training loss: 21.43606948852539 = 1.6999744176864624 + 3 * 6.57869815826416\n",
      "Epoch 380, val loss: 1.7202248573303223\n",
      "Epoch 390, training loss: 21.406335830688477 = 1.6913554668426514 + 3 * 6.57166051864624\n",
      "Epoch 390, val loss: 1.7132689952850342\n",
      "Epoch 400, training loss: 21.376319885253906 = 1.6823289394378662 + 3 * 6.564663410186768\n",
      "Epoch 400, val loss: 1.7060145139694214\n",
      "Epoch 410, training loss: 21.34604835510254 = 1.6728465557098389 + 3 * 6.55773401260376\n",
      "Epoch 410, val loss: 1.6983911991119385\n",
      "Epoch 420, training loss: 21.317304611206055 = 1.6628659963607788 + 3 * 6.551479339599609\n",
      "Epoch 420, val loss: 1.6903854608535767\n",
      "Epoch 430, training loss: 21.294496536254883 = 1.6523759365081787 + 3 * 6.5473737716674805\n",
      "Epoch 430, val loss: 1.6819440126419067\n",
      "Epoch 440, training loss: 21.260971069335938 = 1.6413826942443848 + 3 * 6.539863109588623\n",
      "Epoch 440, val loss: 1.6731748580932617\n",
      "Epoch 450, training loss: 21.232324600219727 = 1.6299155950546265 + 3 * 6.534136772155762\n",
      "Epoch 450, val loss: 1.6640311479568481\n",
      "Epoch 460, training loss: 21.205459594726562 = 1.617940902709961 + 3 * 6.529172897338867\n",
      "Epoch 460, val loss: 1.6544605493545532\n",
      "Epoch 470, training loss: 21.177139282226562 = 1.6054373979568481 + 3 * 6.523900508880615\n",
      "Epoch 470, val loss: 1.6445043087005615\n",
      "Epoch 480, training loss: 21.147977828979492 = 1.592408537864685 + 3 * 6.5185227394104\n",
      "Epoch 480, val loss: 1.6341149806976318\n",
      "Epoch 490, training loss: 21.12202262878418 = 1.578870177268982 + 3 * 6.5143842697143555\n",
      "Epoch 490, val loss: 1.6233505010604858\n",
      "Epoch 500, training loss: 21.091506958007812 = 1.5648375749588013 + 3 * 6.508889675140381\n",
      "Epoch 500, val loss: 1.6121448278427124\n",
      "Epoch 510, training loss: 21.064189910888672 = 1.5503276586532593 + 3 * 6.5046210289001465\n",
      "Epoch 510, val loss: 1.6006287336349487\n",
      "Epoch 520, training loss: 21.035751342773438 = 1.5353691577911377 + 3 * 6.50012731552124\n",
      "Epoch 520, val loss: 1.5887093544006348\n",
      "Epoch 530, training loss: 21.017974853515625 = 1.51995849609375 + 3 * 6.4993391036987305\n",
      "Epoch 530, val loss: 1.5764554738998413\n",
      "Epoch 540, training loss: 20.98070526123047 = 1.5041195154190063 + 3 * 6.492195129394531\n",
      "Epoch 540, val loss: 1.563889980316162\n",
      "Epoch 550, training loss: 20.954599380493164 = 1.4879307746887207 + 3 * 6.488889694213867\n",
      "Epoch 550, val loss: 1.5510369539260864\n",
      "Epoch 560, training loss: 20.925046920776367 = 1.4714140892028809 + 3 * 6.484543800354004\n",
      "Epoch 560, val loss: 1.5379517078399658\n",
      "Epoch 570, training loss: 20.89706802368164 = 1.4546178579330444 + 3 * 6.48081636428833\n",
      "Epoch 570, val loss: 1.524700403213501\n",
      "Epoch 580, training loss: 20.875511169433594 = 1.437576413154602 + 3 * 6.479311943054199\n",
      "Epoch 580, val loss: 1.5112571716308594\n",
      "Epoch 590, training loss: 20.84444808959961 = 1.4202818870544434 + 3 * 6.474721908569336\n",
      "Epoch 590, val loss: 1.497718095779419\n",
      "Epoch 600, training loss: 20.81356430053711 = 1.4028774499893188 + 3 * 6.470229148864746\n",
      "Epoch 600, val loss: 1.4840855598449707\n",
      "Epoch 610, training loss: 20.786087036132812 = 1.385356068611145 + 3 * 6.466909885406494\n",
      "Epoch 610, val loss: 1.4703876972198486\n",
      "Epoch 620, training loss: 20.775196075439453 = 1.3677337169647217 + 3 * 6.469154357910156\n",
      "Epoch 620, val loss: 1.4566580057144165\n",
      "Epoch 630, training loss: 20.73259735107422 = 1.35004460811615 + 3 * 6.460850715637207\n",
      "Epoch 630, val loss: 1.4429715871810913\n",
      "Epoch 640, training loss: 20.7119197845459 = 1.3324103355407715 + 3 * 6.459836006164551\n",
      "Epoch 640, val loss: 1.4294244050979614\n",
      "Epoch 650, training loss: 20.681076049804688 = 1.314861536026001 + 3 * 6.455404758453369\n",
      "Epoch 650, val loss: 1.4158451557159424\n",
      "Epoch 660, training loss: 20.654943466186523 = 1.297395944595337 + 3 * 6.452515602111816\n",
      "Epoch 660, val loss: 1.4025259017944336\n",
      "Epoch 670, training loss: 20.62844467163086 = 1.280049204826355 + 3 * 6.449465274810791\n",
      "Epoch 670, val loss: 1.389312744140625\n",
      "Epoch 680, training loss: 20.604583740234375 = 1.262818694114685 + 3 * 6.447254657745361\n",
      "Epoch 680, val loss: 1.3762762546539307\n",
      "Epoch 690, training loss: 20.584945678710938 = 1.2456936836242676 + 3 * 6.446417331695557\n",
      "Epoch 690, val loss: 1.3634233474731445\n",
      "Epoch 700, training loss: 20.559118270874023 = 1.2288042306900024 + 3 * 6.443438529968262\n",
      "Epoch 700, val loss: 1.3507566452026367\n",
      "Epoch 710, training loss: 20.53089714050293 = 1.212114930152893 + 3 * 6.439594268798828\n",
      "Epoch 710, val loss: 1.3383337259292603\n",
      "Epoch 720, training loss: 20.515972137451172 = 1.1956398487091064 + 3 * 6.440110206604004\n",
      "Epoch 720, val loss: 1.3262056112289429\n",
      "Epoch 730, training loss: 20.492414474487305 = 1.179406762123108 + 3 * 6.437668800354004\n",
      "Epoch 730, val loss: 1.3142054080963135\n",
      "Epoch 740, training loss: 20.460479736328125 = 1.1634169816970825 + 3 * 6.432353973388672\n",
      "Epoch 740, val loss: 1.3026155233383179\n",
      "Epoch 750, training loss: 20.43901824951172 = 1.1476876735687256 + 3 * 6.430443286895752\n",
      "Epoch 750, val loss: 1.2912582159042358\n",
      "Epoch 760, training loss: 20.424680709838867 = 1.1321570873260498 + 3 * 6.430841445922852\n",
      "Epoch 760, val loss: 1.280087947845459\n",
      "Epoch 770, training loss: 20.398025512695312 = 1.1167997121810913 + 3 * 6.427074909210205\n",
      "Epoch 770, val loss: 1.26921808719635\n",
      "Epoch 780, training loss: 20.374282836914062 = 1.1016658544540405 + 3 * 6.424205780029297\n",
      "Epoch 780, val loss: 1.2584987878799438\n",
      "Epoch 790, training loss: 20.353567123413086 = 1.0867221355438232 + 3 * 6.422281742095947\n",
      "Epoch 790, val loss: 1.248084545135498\n",
      "Epoch 800, training loss: 20.332609176635742 = 1.071950078010559 + 3 * 6.420219421386719\n",
      "Epoch 800, val loss: 1.2378075122833252\n",
      "Epoch 810, training loss: 20.31566047668457 = 1.057350993156433 + 3 * 6.419436931610107\n",
      "Epoch 810, val loss: 1.2277708053588867\n",
      "Epoch 820, training loss: 20.29405403137207 = 1.0429879426956177 + 3 * 6.417022228240967\n",
      "Epoch 820, val loss: 1.2179510593414307\n",
      "Epoch 830, training loss: 20.271570205688477 = 1.0287786722183228 + 3 * 6.41426420211792\n",
      "Epoch 830, val loss: 1.208351969718933\n",
      "Epoch 840, training loss: 20.251108169555664 = 1.0147457122802734 + 3 * 6.412120819091797\n",
      "Epoch 840, val loss: 1.198913812637329\n",
      "Epoch 850, training loss: 20.242019653320312 = 1.0008349418640137 + 3 * 6.413727760314941\n",
      "Epoch 850, val loss: 1.1896421909332275\n",
      "Epoch 860, training loss: 20.217357635498047 = 0.9870104789733887 + 3 * 6.410115718841553\n",
      "Epoch 860, val loss: 1.1805412769317627\n",
      "Epoch 870, training loss: 20.196796417236328 = 0.9733734130859375 + 3 * 6.40780782699585\n",
      "Epoch 870, val loss: 1.171614170074463\n",
      "Epoch 880, training loss: 20.177947998046875 = 0.959841787815094 + 3 * 6.40603494644165\n",
      "Epoch 880, val loss: 1.1628180742263794\n",
      "Epoch 890, training loss: 20.160274505615234 = 0.9464385509490967 + 3 * 6.4046125411987305\n",
      "Epoch 890, val loss: 1.1542224884033203\n",
      "Epoch 900, training loss: 20.139507293701172 = 0.9331285953521729 + 3 * 6.402126312255859\n",
      "Epoch 900, val loss: 1.1457010507583618\n",
      "Epoch 910, training loss: 20.127336502075195 = 0.9199475049972534 + 3 * 6.402463436126709\n",
      "Epoch 910, val loss: 1.1373376846313477\n",
      "Epoch 920, training loss: 20.109148025512695 = 0.9068238139152527 + 3 * 6.400774955749512\n",
      "Epoch 920, val loss: 1.1291053295135498\n",
      "Epoch 930, training loss: 20.093198776245117 = 0.8938831686973572 + 3 * 6.3997721672058105\n",
      "Epoch 930, val loss: 1.1209877729415894\n",
      "Epoch 940, training loss: 20.06863021850586 = 0.8810411095619202 + 3 * 6.395863056182861\n",
      "Epoch 940, val loss: 1.113099217414856\n",
      "Epoch 950, training loss: 20.050933837890625 = 0.8683669567108154 + 3 * 6.394189357757568\n",
      "Epoch 950, val loss: 1.1053547859191895\n",
      "Epoch 960, training loss: 20.034486770629883 = 0.8557937741279602 + 3 * 6.392898082733154\n",
      "Epoch 960, val loss: 1.0976979732513428\n",
      "Epoch 970, training loss: 20.028331756591797 = 0.8433119654655457 + 3 * 6.395007133483887\n",
      "Epoch 970, val loss: 1.0902072191238403\n",
      "Epoch 980, training loss: 20.005226135253906 = 0.830966055393219 + 3 * 6.391419887542725\n",
      "Epoch 980, val loss: 1.0828173160552979\n",
      "Epoch 990, training loss: 20.00050163269043 = 0.8187518119812012 + 3 * 6.393916606903076\n",
      "Epoch 990, val loss: 1.075616717338562\n",
      "Epoch 1000, training loss: 19.971872329711914 = 0.8067421913146973 + 3 * 6.388376712799072\n",
      "Epoch 1000, val loss: 1.0685640573501587\n",
      "Epoch 1010, training loss: 19.952346801757812 = 0.7948835492134094 + 3 * 6.385821342468262\n",
      "Epoch 1010, val loss: 1.0617260932922363\n",
      "Epoch 1020, training loss: 19.935916900634766 = 0.7831825613975525 + 3 * 6.384244918823242\n",
      "Epoch 1020, val loss: 1.055041790008545\n",
      "Epoch 1030, training loss: 19.93038558959961 = 0.7716111540794373 + 3 * 6.386258602142334\n",
      "Epoch 1030, val loss: 1.0484592914581299\n",
      "Epoch 1040, training loss: 19.912630081176758 = 0.7601473331451416 + 3 * 6.384160995483398\n",
      "Epoch 1040, val loss: 1.0420411825180054\n",
      "Epoch 1050, training loss: 19.890745162963867 = 0.7488835453987122 + 3 * 6.380620002746582\n",
      "Epoch 1050, val loss: 1.0357942581176758\n",
      "Epoch 1060, training loss: 19.87569808959961 = 0.737788200378418 + 3 * 6.379303455352783\n",
      "Epoch 1060, val loss: 1.029721736907959\n",
      "Epoch 1070, training loss: 19.87873649597168 = 0.7268575429916382 + 3 * 6.3839592933654785\n",
      "Epoch 1070, val loss: 1.0237550735473633\n",
      "Epoch 1080, training loss: 19.8577938079834 = 0.7160211205482483 + 3 * 6.380590915679932\n",
      "Epoch 1080, val loss: 1.0180281400680542\n",
      "Epoch 1090, training loss: 19.833189010620117 = 0.7054323554039001 + 3 * 6.375918865203857\n",
      "Epoch 1090, val loss: 1.0124369859695435\n",
      "Epoch 1100, training loss: 19.81859016418457 = 0.6950145363807678 + 3 * 6.37452507019043\n",
      "Epoch 1100, val loss: 1.0070611238479614\n",
      "Epoch 1110, training loss: 19.808629989624023 = 0.6847330331802368 + 3 * 6.374632835388184\n",
      "Epoch 1110, val loss: 1.0018479824066162\n",
      "Epoch 1120, training loss: 19.793041229248047 = 0.6746025085449219 + 3 * 6.3728132247924805\n",
      "Epoch 1120, val loss: 0.9967471361160278\n",
      "Epoch 1130, training loss: 19.778156280517578 = 0.6646299362182617 + 3 * 6.371175765991211\n",
      "Epoch 1130, val loss: 0.9918333888053894\n",
      "Epoch 1140, training loss: 19.767318725585938 = 0.6548117995262146 + 3 * 6.370835304260254\n",
      "Epoch 1140, val loss: 0.987101674079895\n",
      "Epoch 1150, training loss: 19.753761291503906 = 0.6451529264450073 + 3 * 6.36953592300415\n",
      "Epoch 1150, val loss: 0.9825404286384583\n",
      "Epoch 1160, training loss: 19.741317749023438 = 0.6356500387191772 + 3 * 6.368555545806885\n",
      "Epoch 1160, val loss: 0.9780844449996948\n",
      "Epoch 1170, training loss: 19.728296279907227 = 0.6262727975845337 + 3 * 6.3673415184021\n",
      "Epoch 1170, val loss: 0.9738718867301941\n",
      "Epoch 1180, training loss: 19.71670913696289 = 0.6170445680618286 + 3 * 6.3665547370910645\n",
      "Epoch 1180, val loss: 0.9697287082672119\n",
      "Epoch 1190, training loss: 19.706144332885742 = 0.6079319715499878 + 3 * 6.366070747375488\n",
      "Epoch 1190, val loss: 0.9657782912254333\n",
      "Epoch 1200, training loss: 19.6922550201416 = 0.5989585518836975 + 3 * 6.364432334899902\n",
      "Epoch 1200, val loss: 0.9619777202606201\n",
      "Epoch 1210, training loss: 19.699600219726562 = 0.5901050567626953 + 3 * 6.369831562042236\n",
      "Epoch 1210, val loss: 0.9582964777946472\n",
      "Epoch 1220, training loss: 19.670578002929688 = 0.581367015838623 + 3 * 6.363070487976074\n",
      "Epoch 1220, val loss: 0.954671323299408\n",
      "Epoch 1230, training loss: 19.65682601928711 = 0.5727739334106445 + 3 * 6.361351013183594\n",
      "Epoch 1230, val loss: 0.9512325525283813\n",
      "Epoch 1240, training loss: 19.65010643005371 = 0.5643056035041809 + 3 * 6.361933708190918\n",
      "Epoch 1240, val loss: 0.9480293989181519\n",
      "Epoch 1250, training loss: 19.63255500793457 = 0.5559453368186951 + 3 * 6.358870029449463\n",
      "Epoch 1250, val loss: 0.9448160529136658\n",
      "Epoch 1260, training loss: 19.630718231201172 = 0.5476965308189392 + 3 * 6.361007213592529\n",
      "Epoch 1260, val loss: 0.9417076110839844\n",
      "Epoch 1270, training loss: 19.61217498779297 = 0.5395478010177612 + 3 * 6.357542991638184\n",
      "Epoch 1270, val loss: 0.9388090968132019\n",
      "Epoch 1280, training loss: 19.60238265991211 = 0.5315147638320923 + 3 * 6.3569560050964355\n",
      "Epoch 1280, val loss: 0.9359681606292725\n",
      "Epoch 1290, training loss: 19.592897415161133 = 0.5235686898231506 + 3 * 6.356442451477051\n",
      "Epoch 1290, val loss: 0.9332553148269653\n",
      "Epoch 1300, training loss: 19.57817840576172 = 0.5157006978988647 + 3 * 6.354158878326416\n",
      "Epoch 1300, val loss: 0.9306451082229614\n",
      "Epoch 1310, training loss: 19.570322036743164 = 0.507935106754303 + 3 * 6.354129314422607\n",
      "Epoch 1310, val loss: 0.9281665086746216\n",
      "Epoch 1320, training loss: 19.574281692504883 = 0.5002164244651794 + 3 * 6.3580217361450195\n",
      "Epoch 1320, val loss: 0.925667941570282\n",
      "Epoch 1330, training loss: 19.55631446838379 = 0.49263036251068115 + 3 * 6.3545613288879395\n",
      "Epoch 1330, val loss: 0.9232898950576782\n",
      "Epoch 1340, training loss: 19.53790283203125 = 0.4851071834564209 + 3 * 6.3509321212768555\n",
      "Epoch 1340, val loss: 0.9210968613624573\n",
      "Epoch 1350, training loss: 19.526077270507812 = 0.4776778221130371 + 3 * 6.349466323852539\n",
      "Epoch 1350, val loss: 0.9189389944076538\n",
      "Epoch 1360, training loss: 19.517417907714844 = 0.4703022241592407 + 3 * 6.349038600921631\n",
      "Epoch 1360, val loss: 0.9168790578842163\n",
      "Epoch 1370, training loss: 19.52724838256836 = 0.46298548579216003 + 3 * 6.3547539710998535\n",
      "Epoch 1370, val loss: 0.9148103594779968\n",
      "Epoch 1380, training loss: 19.502782821655273 = 0.45575061440467834 + 3 * 6.349010467529297\n",
      "Epoch 1380, val loss: 0.9128534197807312\n",
      "Epoch 1390, training loss: 19.490997314453125 = 0.4485774636268616 + 3 * 6.34747314453125\n",
      "Epoch 1390, val loss: 0.9110323786735535\n",
      "Epoch 1400, training loss: 19.478519439697266 = 0.4414980411529541 + 3 * 6.34567403793335\n",
      "Epoch 1400, val loss: 0.9092631936073303\n",
      "Epoch 1410, training loss: 19.47226905822754 = 0.43447649478912354 + 3 * 6.345931053161621\n",
      "Epoch 1410, val loss: 0.9075585603713989\n",
      "Epoch 1420, training loss: 19.46567726135254 = 0.427507609128952 + 3 * 6.346056938171387\n",
      "Epoch 1420, val loss: 0.9059299826622009\n",
      "Epoch 1430, training loss: 19.461280822753906 = 0.4206061363220215 + 3 * 6.346891403198242\n",
      "Epoch 1430, val loss: 0.9043276309967041\n",
      "Epoch 1440, training loss: 19.44402503967285 = 0.4137432277202606 + 3 * 6.343426704406738\n",
      "Epoch 1440, val loss: 0.9028097987174988\n",
      "Epoch 1450, training loss: 19.433225631713867 = 0.4069693982601166 + 3 * 6.342085361480713\n",
      "Epoch 1450, val loss: 0.9013445377349854\n",
      "Epoch 1460, training loss: 19.426599502563477 = 0.40023261308670044 + 3 * 6.342122554779053\n",
      "Epoch 1460, val loss: 0.8999404311180115\n",
      "Epoch 1470, training loss: 19.42605972290039 = 0.3935585916042328 + 3 * 6.3441667556762695\n",
      "Epoch 1470, val loss: 0.8985307216644287\n",
      "Epoch 1480, training loss: 19.41586685180664 = 0.3869124948978424 + 3 * 6.342984676361084\n",
      "Epoch 1480, val loss: 0.897343099117279\n",
      "Epoch 1490, training loss: 19.399160385131836 = 0.38036656379699707 + 3 * 6.339597702026367\n",
      "Epoch 1490, val loss: 0.8960574269294739\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7555555555555555\n",
      "0.8091723774380601\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 27.746055603027344 = 1.9554526805877686 + 3 * 8.596867561340332\n",
      "Epoch 0, val loss: 1.951629638671875\n",
      "Epoch 10, training loss: 27.740577697753906 = 1.9501543045043945 + 3 * 8.596807479858398\n",
      "Epoch 10, val loss: 1.946304202079773\n",
      "Epoch 20, training loss: 27.734426498413086 = 1.9445184469223022 + 3 * 8.596635818481445\n",
      "Epoch 20, val loss: 1.9405444860458374\n",
      "Epoch 30, training loss: 27.726247787475586 = 1.9379442930221558 + 3 * 8.596100807189941\n",
      "Epoch 30, val loss: 1.9337496757507324\n",
      "Epoch 40, training loss: 27.712501525878906 = 1.929872989654541 + 3 * 8.594209671020508\n",
      "Epoch 40, val loss: 1.92535400390625\n",
      "Epoch 50, training loss: 27.680267333984375 = 1.9198185205459595 + 3 * 8.58681583404541\n",
      "Epoch 50, val loss: 1.9149326086044312\n",
      "Epoch 60, training loss: 27.5751895904541 = 1.9078571796417236 + 3 * 8.555777549743652\n",
      "Epoch 60, val loss: 1.902647852897644\n",
      "Epoch 70, training loss: 27.106191635131836 = 1.8951587677001953 + 3 * 8.403677940368652\n",
      "Epoch 70, val loss: 1.8898470401763916\n",
      "Epoch 80, training loss: 26.089427947998047 = 1.8822901248931885 + 3 * 8.069046020507812\n",
      "Epoch 80, val loss: 1.877423882484436\n",
      "Epoch 90, training loss: 25.54845428466797 = 1.8741999864578247 + 3 * 7.891417980194092\n",
      "Epoch 90, val loss: 1.8698879480361938\n",
      "Epoch 100, training loss: 24.71776580810547 = 1.8681387901306152 + 3 * 7.616542339324951\n",
      "Epoch 100, val loss: 1.864140510559082\n",
      "Epoch 110, training loss: 24.058399200439453 = 1.8619967699050903 + 3 * 7.398800373077393\n",
      "Epoch 110, val loss: 1.858054280281067\n",
      "Epoch 120, training loss: 23.69244384765625 = 1.8552618026733398 + 3 * 7.279060363769531\n",
      "Epoch 120, val loss: 1.8515965938568115\n",
      "Epoch 130, training loss: 23.372844696044922 = 1.8484439849853516 + 3 * 7.174800395965576\n",
      "Epoch 130, val loss: 1.8453234434127808\n",
      "Epoch 140, training loss: 23.128095626831055 = 1.8420506715774536 + 3 * 7.095348358154297\n",
      "Epoch 140, val loss: 1.8395668268203735\n",
      "Epoch 150, training loss: 22.936561584472656 = 1.8358566761016846 + 3 * 7.033567905426025\n",
      "Epoch 150, val loss: 1.834020972251892\n",
      "Epoch 160, training loss: 22.785354614257812 = 1.8298259973526 + 3 * 6.985176086425781\n",
      "Epoch 160, val loss: 1.8286210298538208\n",
      "Epoch 170, training loss: 22.65871238708496 = 1.8239315748214722 + 3 * 6.944927215576172\n",
      "Epoch 170, val loss: 1.823347568511963\n",
      "Epoch 180, training loss: 22.548328399658203 = 1.818240761756897 + 3 * 6.91002893447876\n",
      "Epoch 180, val loss: 1.8182915449142456\n",
      "Epoch 190, training loss: 22.450475692749023 = 1.8127567768096924 + 3 * 6.879239559173584\n",
      "Epoch 190, val loss: 1.8134416341781616\n",
      "Epoch 200, training loss: 22.36176300048828 = 1.8074835538864136 + 3 * 6.851426601409912\n",
      "Epoch 200, val loss: 1.8088220357894897\n",
      "Epoch 210, training loss: 22.287715911865234 = 1.8023858070373535 + 3 * 6.82844352722168\n",
      "Epoch 210, val loss: 1.8043928146362305\n",
      "Epoch 220, training loss: 22.209699630737305 = 1.7974640130996704 + 3 * 6.804078578948975\n",
      "Epoch 220, val loss: 1.800101399421692\n",
      "Epoch 230, training loss: 22.143491744995117 = 1.792631983757019 + 3 * 6.7836198806762695\n",
      "Epoch 230, val loss: 1.795914888381958\n",
      "Epoch 240, training loss: 22.08604621887207 = 1.7878594398498535 + 3 * 6.7660627365112305\n",
      "Epoch 240, val loss: 1.7918022871017456\n",
      "Epoch 250, training loss: 22.02572250366211 = 1.7830920219421387 + 3 * 6.7475433349609375\n",
      "Epoch 250, val loss: 1.7877120971679688\n",
      "Epoch 260, training loss: 21.971435546875 = 1.7782859802246094 + 3 * 6.73105001449585\n",
      "Epoch 260, val loss: 1.7836060523986816\n",
      "Epoch 270, training loss: 21.922025680541992 = 1.7733913660049438 + 3 * 6.716211318969727\n",
      "Epoch 270, val loss: 1.7794504165649414\n",
      "Epoch 280, training loss: 21.875755310058594 = 1.7683873176574707 + 3 * 6.702456474304199\n",
      "Epoch 280, val loss: 1.7752104997634888\n",
      "Epoch 290, training loss: 21.833023071289062 = 1.763229489326477 + 3 * 6.689931392669678\n",
      "Epoch 290, val loss: 1.770859718322754\n",
      "Epoch 300, training loss: 21.788532257080078 = 1.7578904628753662 + 3 * 6.676880359649658\n",
      "Epoch 300, val loss: 1.766364336013794\n",
      "Epoch 310, training loss: 21.749929428100586 = 1.7523332834243774 + 3 * 6.665865421295166\n",
      "Epoch 310, val loss: 1.7617013454437256\n",
      "Epoch 320, training loss: 21.71118927001953 = 1.7465544939041138 + 3 * 6.65487813949585\n",
      "Epoch 320, val loss: 1.7568416595458984\n",
      "Epoch 330, training loss: 21.6732234954834 = 1.7405072450637817 + 3 * 6.644238471984863\n",
      "Epoch 330, val loss: 1.7517732381820679\n",
      "Epoch 340, training loss: 21.64017677307129 = 1.734179139137268 + 3 * 6.6353325843811035\n",
      "Epoch 340, val loss: 1.7464860677719116\n",
      "Epoch 350, training loss: 21.6053524017334 = 1.7275441884994507 + 3 * 6.625936031341553\n",
      "Epoch 350, val loss: 1.7409286499023438\n",
      "Epoch 360, training loss: 21.569080352783203 = 1.7205946445465088 + 3 * 6.616161823272705\n",
      "Epoch 360, val loss: 1.7351247072219849\n",
      "Epoch 370, training loss: 21.53616714477539 = 1.7132865190505981 + 3 * 6.607626438140869\n",
      "Epoch 370, val loss: 1.7290321588516235\n",
      "Epoch 380, training loss: 21.511049270629883 = 1.7056142091751099 + 3 * 6.60181188583374\n",
      "Epoch 380, val loss: 1.7226115465164185\n",
      "Epoch 390, training loss: 21.474367141723633 = 1.6975791454315186 + 3 * 6.5922627449035645\n",
      "Epoch 390, val loss: 1.7159136533737183\n",
      "Epoch 400, training loss: 21.444358825683594 = 1.6891822814941406 + 3 * 6.585058689117432\n",
      "Epoch 400, val loss: 1.7089070081710815\n",
      "Epoch 410, training loss: 21.42253303527832 = 1.6803889274597168 + 3 * 6.580714702606201\n",
      "Epoch 410, val loss: 1.701572299003601\n",
      "Epoch 420, training loss: 21.387914657592773 = 1.6711788177490234 + 3 * 6.572245121002197\n",
      "Epoch 420, val loss: 1.6938469409942627\n",
      "Epoch 430, training loss: 21.3570556640625 = 1.6615514755249023 + 3 * 6.565168380737305\n",
      "Epoch 430, val loss: 1.6857998371124268\n",
      "Epoch 440, training loss: 21.32776641845703 = 1.6514735221862793 + 3 * 6.5587639808654785\n",
      "Epoch 440, val loss: 1.6773353815078735\n",
      "Epoch 450, training loss: 21.308198928833008 = 1.6409480571746826 + 3 * 6.555750370025635\n",
      "Epoch 450, val loss: 1.6685044765472412\n",
      "Epoch 460, training loss: 21.27245330810547 = 1.6299854516983032 + 3 * 6.547489166259766\n",
      "Epoch 460, val loss: 1.6593159437179565\n",
      "Epoch 470, training loss: 21.243127822875977 = 1.618567705154419 + 3 * 6.541519641876221\n",
      "Epoch 470, val loss: 1.6497310400009155\n",
      "Epoch 480, training loss: 21.21686363220215 = 1.6066912412643433 + 3 * 6.536724090576172\n",
      "Epoch 480, val loss: 1.6397790908813477\n",
      "Epoch 490, training loss: 21.1875057220459 = 1.5943657159805298 + 3 * 6.531046390533447\n",
      "Epoch 490, val loss: 1.6295111179351807\n",
      "Epoch 500, training loss: 21.16352653503418 = 1.5816073417663574 + 3 * 6.527306079864502\n",
      "Epoch 500, val loss: 1.6188212633132935\n",
      "Epoch 510, training loss: 21.13299560546875 = 1.568426489830017 + 3 * 6.5215229988098145\n",
      "Epoch 510, val loss: 1.6079133749008179\n",
      "Epoch 520, training loss: 21.10514259338379 = 1.5548095703125 + 3 * 6.516777515411377\n",
      "Epoch 520, val loss: 1.5966304540634155\n",
      "Epoch 530, training loss: 21.094165802001953 = 1.5407633781433105 + 3 * 6.517800331115723\n",
      "Epoch 530, val loss: 1.5850907564163208\n",
      "Epoch 540, training loss: 21.051828384399414 = 1.5264402627944946 + 3 * 6.508462905883789\n",
      "Epoch 540, val loss: 1.5732767581939697\n",
      "Epoch 550, training loss: 21.023855209350586 = 1.5118623971939087 + 3 * 6.503997802734375\n",
      "Epoch 550, val loss: 1.5613493919372559\n",
      "Epoch 560, training loss: 20.99616241455078 = 1.4970057010650635 + 3 * 6.49971866607666\n",
      "Epoch 560, val loss: 1.549252986907959\n",
      "Epoch 570, training loss: 20.986841201782227 = 1.481905460357666 + 3 * 6.501645565032959\n",
      "Epoch 570, val loss: 1.5370392799377441\n",
      "Epoch 580, training loss: 20.94342041015625 = 1.4666632413864136 + 3 * 6.492252349853516\n",
      "Epoch 580, val loss: 1.5248664617538452\n",
      "Epoch 590, training loss: 20.918603897094727 = 1.451306700706482 + 3 * 6.489099025726318\n",
      "Epoch 590, val loss: 1.512704610824585\n",
      "Epoch 600, training loss: 20.895198822021484 = 1.435818076133728 + 3 * 6.4864606857299805\n",
      "Epoch 600, val loss: 1.5005193948745728\n",
      "Epoch 610, training loss: 20.866525650024414 = 1.4201996326446533 + 3 * 6.4821085929870605\n",
      "Epoch 610, val loss: 1.4882980585098267\n",
      "Epoch 620, training loss: 20.8414363861084 = 1.4045321941375732 + 3 * 6.478968620300293\n",
      "Epoch 620, val loss: 1.4762412309646606\n",
      "Epoch 630, training loss: 20.81377410888672 = 1.3888128995895386 + 3 * 6.474987030029297\n",
      "Epoch 630, val loss: 1.4642997980117798\n",
      "Epoch 640, training loss: 20.789684295654297 = 1.373036503791809 + 3 * 6.47221565246582\n",
      "Epoch 640, val loss: 1.4524140357971191\n",
      "Epoch 650, training loss: 20.763643264770508 = 1.357239842414856 + 3 * 6.4688005447387695\n",
      "Epoch 650, val loss: 1.4406490325927734\n",
      "Epoch 660, training loss: 20.740163803100586 = 1.3414711952209473 + 3 * 6.466231346130371\n",
      "Epoch 660, val loss: 1.4290716648101807\n",
      "Epoch 670, training loss: 20.71274757385254 = 1.3257137537002563 + 3 * 6.462344646453857\n",
      "Epoch 670, val loss: 1.4176064729690552\n",
      "Epoch 680, training loss: 20.70781707763672 = 1.3099712133407593 + 3 * 6.465949058532715\n",
      "Epoch 680, val loss: 1.4062526226043701\n",
      "Epoch 690, training loss: 20.666893005371094 = 1.2942237854003906 + 3 * 6.457556247711182\n",
      "Epoch 690, val loss: 1.3950473070144653\n",
      "Epoch 700, training loss: 20.64944839477539 = 1.2785474061965942 + 3 * 6.456966876983643\n",
      "Epoch 700, val loss: 1.3839513063430786\n",
      "Epoch 710, training loss: 20.619638442993164 = 1.2629127502441406 + 3 * 6.452241897583008\n",
      "Epoch 710, val loss: 1.373051643371582\n",
      "Epoch 720, training loss: 20.594070434570312 = 1.2473102807998657 + 3 * 6.448920249938965\n",
      "Epoch 720, val loss: 1.3622139692306519\n",
      "Epoch 730, training loss: 20.569917678833008 = 1.2317122220993042 + 3 * 6.446068286895752\n",
      "Epoch 730, val loss: 1.3515222072601318\n",
      "Epoch 740, training loss: 20.54909896850586 = 1.2160990238189697 + 3 * 6.444333076477051\n",
      "Epoch 740, val loss: 1.3409050703048706\n",
      "Epoch 750, training loss: 20.531997680664062 = 1.200474500656128 + 3 * 6.443840980529785\n",
      "Epoch 750, val loss: 1.330298662185669\n",
      "Epoch 760, training loss: 20.506114959716797 = 1.1848762035369873 + 3 * 6.440413475036621\n",
      "Epoch 760, val loss: 1.3197909593582153\n",
      "Epoch 770, training loss: 20.481674194335938 = 1.1693612337112427 + 3 * 6.437438011169434\n",
      "Epoch 770, val loss: 1.3093504905700684\n",
      "Epoch 780, training loss: 20.458038330078125 = 1.1539233922958374 + 3 * 6.4347052574157715\n",
      "Epoch 780, val loss: 1.2991547584533691\n",
      "Epoch 790, training loss: 20.436012268066406 = 1.138484001159668 + 3 * 6.432508945465088\n",
      "Epoch 790, val loss: 1.2888916730880737\n",
      "Epoch 800, training loss: 20.423221588134766 = 1.1230851411819458 + 3 * 6.43337869644165\n",
      "Epoch 800, val loss: 1.2787400484085083\n",
      "Epoch 810, training loss: 20.39360809326172 = 1.1077240705490112 + 3 * 6.428627967834473\n",
      "Epoch 810, val loss: 1.2687299251556396\n",
      "Epoch 820, training loss: 20.372337341308594 = 1.0924619436264038 + 3 * 6.426624774932861\n",
      "Epoch 820, val loss: 1.2587878704071045\n",
      "Epoch 830, training loss: 20.357925415039062 = 1.0772844552993774 + 3 * 6.426880359649658\n",
      "Epoch 830, val loss: 1.2489514350891113\n",
      "Epoch 840, training loss: 20.331523895263672 = 1.0622113943099976 + 3 * 6.4231038093566895\n",
      "Epoch 840, val loss: 1.239327311515808\n",
      "Epoch 850, training loss: 20.321378707885742 = 1.04721999168396 + 3 * 6.424719333648682\n",
      "Epoch 850, val loss: 1.2297559976577759\n",
      "Epoch 860, training loss: 20.293087005615234 = 1.0323212146759033 + 3 * 6.420255184173584\n",
      "Epoch 860, val loss: 1.220300316810608\n",
      "Epoch 870, training loss: 20.26800537109375 = 1.0176078081130981 + 3 * 6.4167985916137695\n",
      "Epoch 870, val loss: 1.2110674381256104\n",
      "Epoch 880, training loss: 20.248783111572266 = 1.0030015707015991 + 3 * 6.415260314941406\n",
      "Epoch 880, val loss: 1.2020069360733032\n",
      "Epoch 890, training loss: 20.235042572021484 = 0.9885058999061584 + 3 * 6.4155120849609375\n",
      "Epoch 890, val loss: 1.1930148601531982\n",
      "Epoch 900, training loss: 20.208465576171875 = 0.9741418361663818 + 3 * 6.411441326141357\n",
      "Epoch 900, val loss: 1.1842339038848877\n",
      "Epoch 910, training loss: 20.192480087280273 = 0.9599692821502686 + 3 * 6.410837173461914\n",
      "Epoch 910, val loss: 1.1756318807601929\n",
      "Epoch 920, training loss: 20.178895950317383 = 0.9460123181343079 + 3 * 6.410961151123047\n",
      "Epoch 920, val loss: 1.1673401594161987\n",
      "Epoch 930, training loss: 20.159210205078125 = 0.9321946501731873 + 3 * 6.409005165100098\n",
      "Epoch 930, val loss: 1.1591439247131348\n",
      "Epoch 940, training loss: 20.136930465698242 = 0.9186344146728516 + 3 * 6.406098365783691\n",
      "Epoch 940, val loss: 1.1511949300765991\n",
      "Epoch 950, training loss: 20.115772247314453 = 0.9052585959434509 + 3 * 6.403504848480225\n",
      "Epoch 950, val loss: 1.1435160636901855\n",
      "Epoch 960, training loss: 20.097095489501953 = 0.892105758190155 + 3 * 6.401663303375244\n",
      "Epoch 960, val loss: 1.136042833328247\n",
      "Epoch 970, training loss: 20.09139633178711 = 0.879159688949585 + 3 * 6.404078960418701\n",
      "Epoch 970, val loss: 1.1288301944732666\n",
      "Epoch 980, training loss: 20.074636459350586 = 0.8664141893386841 + 3 * 6.402740478515625\n",
      "Epoch 980, val loss: 1.1218180656433105\n",
      "Epoch 990, training loss: 20.043916702270508 = 0.8539485931396484 + 3 * 6.396656036376953\n",
      "Epoch 990, val loss: 1.115065336227417\n",
      "Epoch 1000, training loss: 20.028974533081055 = 0.8417453765869141 + 3 * 6.395742893218994\n",
      "Epoch 1000, val loss: 1.108646035194397\n",
      "Epoch 1010, training loss: 20.016006469726562 = 0.8297582268714905 + 3 * 6.395416259765625\n",
      "Epoch 1010, val loss: 1.1024551391601562\n",
      "Epoch 1020, training loss: 20.005563735961914 = 0.8179746270179749 + 3 * 6.395863056182861\n",
      "Epoch 1020, val loss: 1.0964572429656982\n",
      "Epoch 1030, training loss: 19.985567092895508 = 0.8064165711402893 + 3 * 6.393049716949463\n",
      "Epoch 1030, val loss: 1.0906648635864258\n",
      "Epoch 1040, training loss: 19.964567184448242 = 0.7951261401176453 + 3 * 6.389813423156738\n",
      "Epoch 1040, val loss: 1.0852469205856323\n",
      "Epoch 1050, training loss: 19.94915771484375 = 0.7840356826782227 + 3 * 6.388373851776123\n",
      "Epoch 1050, val loss: 1.0800116062164307\n",
      "Epoch 1060, training loss: 19.945499420166016 = 0.7731377482414246 + 3 * 6.390787124633789\n",
      "Epoch 1060, val loss: 1.0749760866165161\n",
      "Epoch 1070, training loss: 19.922861099243164 = 0.762444794178009 + 3 * 6.386805057525635\n",
      "Epoch 1070, val loss: 1.0702661275863647\n",
      "Epoch 1080, training loss: 19.908374786376953 = 0.7519368529319763 + 3 * 6.38547945022583\n",
      "Epoch 1080, val loss: 1.0656448602676392\n",
      "Epoch 1090, training loss: 19.894441604614258 = 0.7416377663612366 + 3 * 6.384267807006836\n",
      "Epoch 1090, val loss: 1.0613714456558228\n",
      "Epoch 1100, training loss: 19.88173484802246 = 0.7315196394920349 + 3 * 6.383404731750488\n",
      "Epoch 1100, val loss: 1.0572127103805542\n",
      "Epoch 1110, training loss: 19.866334915161133 = 0.7215530872344971 + 3 * 6.381593704223633\n",
      "Epoch 1110, val loss: 1.053263545036316\n",
      "Epoch 1120, training loss: 19.85331153869629 = 0.7117564678192139 + 3 * 6.380518913269043\n",
      "Epoch 1120, val loss: 1.0495154857635498\n",
      "Epoch 1130, training loss: 19.83930206298828 = 0.7021178603172302 + 3 * 6.379061222076416\n",
      "Epoch 1130, val loss: 1.0459212064743042\n",
      "Epoch 1140, training loss: 19.832901000976562 = 0.6926277279853821 + 3 * 6.380091667175293\n",
      "Epoch 1140, val loss: 1.0425434112548828\n",
      "Epoch 1150, training loss: 19.82753562927246 = 0.683309018611908 + 3 * 6.38140869140625\n",
      "Epoch 1150, val loss: 1.0392913818359375\n",
      "Epoch 1160, training loss: 19.804363250732422 = 0.6741490960121155 + 3 * 6.37673807144165\n",
      "Epoch 1160, val loss: 1.0362882614135742\n",
      "Epoch 1170, training loss: 19.786697387695312 = 0.6651409268379211 + 3 * 6.373852252960205\n",
      "Epoch 1170, val loss: 1.033481240272522\n",
      "Epoch 1180, training loss: 19.77469253540039 = 0.6562541127204895 + 3 * 6.3728132247924805\n",
      "Epoch 1180, val loss: 1.0307081937789917\n",
      "Epoch 1190, training loss: 19.774492263793945 = 0.6474851965904236 + 3 * 6.375669002532959\n",
      "Epoch 1190, val loss: 1.0281753540039062\n",
      "Epoch 1200, training loss: 19.7532901763916 = 0.6388042569160461 + 3 * 6.371495246887207\n",
      "Epoch 1200, val loss: 1.0256942510604858\n",
      "Epoch 1210, training loss: 19.740947723388672 = 0.6302485466003418 + 3 * 6.370232582092285\n",
      "Epoch 1210, val loss: 1.023358941078186\n",
      "Epoch 1220, training loss: 19.7313232421875 = 0.6218114495277405 + 3 * 6.369837760925293\n",
      "Epoch 1220, val loss: 1.0212234258651733\n",
      "Epoch 1230, training loss: 19.71605682373047 = 0.6134337782859802 + 3 * 6.3675408363342285\n",
      "Epoch 1230, val loss: 1.0191115140914917\n",
      "Epoch 1240, training loss: 19.70556640625 = 0.6051511764526367 + 3 * 6.366805076599121\n",
      "Epoch 1240, val loss: 1.0172115564346313\n",
      "Epoch 1250, training loss: 19.699682235717773 = 0.5969339609146118 + 3 * 6.36758279800415\n",
      "Epoch 1250, val loss: 1.015330195426941\n",
      "Epoch 1260, training loss: 19.687959671020508 = 0.5887972116470337 + 3 * 6.366387367248535\n",
      "Epoch 1260, val loss: 1.013524055480957\n",
      "Epoch 1270, training loss: 19.6889591217041 = 0.5807467699050903 + 3 * 6.369403839111328\n",
      "Epoch 1270, val loss: 1.0119223594665527\n",
      "Epoch 1280, training loss: 19.66411018371582 = 0.5727986097335815 + 3 * 6.363770961761475\n",
      "Epoch 1280, val loss: 1.010351538658142\n",
      "Epoch 1290, training loss: 19.653106689453125 = 0.5649350881576538 + 3 * 6.3627238273620605\n",
      "Epoch 1290, val loss: 1.0088658332824707\n",
      "Epoch 1300, training loss: 19.638843536376953 = 0.5571833848953247 + 3 * 6.36055326461792\n",
      "Epoch 1300, val loss: 1.0075665712356567\n",
      "Epoch 1310, training loss: 19.627933502197266 = 0.5494796633720398 + 3 * 6.359484672546387\n",
      "Epoch 1310, val loss: 1.006300687789917\n",
      "Epoch 1320, training loss: 19.63927459716797 = 0.541850745677948 + 3 * 6.36580753326416\n",
      "Epoch 1320, val loss: 1.005081295967102\n",
      "Epoch 1330, training loss: 19.616558074951172 = 0.5342513918876648 + 3 * 6.3607683181762695\n",
      "Epoch 1330, val loss: 1.0039814710617065\n",
      "Epoch 1340, training loss: 19.598955154418945 = 0.5267809629440308 + 3 * 6.357391357421875\n",
      "Epoch 1340, val loss: 1.0029460191726685\n",
      "Epoch 1350, training loss: 19.587709426879883 = 0.5193626880645752 + 3 * 6.356115341186523\n",
      "Epoch 1350, val loss: 1.0020438432693481\n",
      "Epoch 1360, training loss: 19.590059280395508 = 0.5120089650154114 + 3 * 6.359350204467773\n",
      "Epoch 1360, val loss: 1.0011743307113647\n",
      "Epoch 1370, training loss: 19.57297706604004 = 0.5047203302383423 + 3 * 6.356085777282715\n",
      "Epoch 1370, val loss: 1.0004103183746338\n",
      "Epoch 1380, training loss: 19.57810401916504 = 0.49753010272979736 + 3 * 6.360191345214844\n",
      "Epoch 1380, val loss: 0.9997372031211853\n",
      "Epoch 1390, training loss: 19.55353546142578 = 0.49038273096084595 + 3 * 6.354384422302246\n",
      "Epoch 1390, val loss: 0.9990478754043579\n",
      "Epoch 1400, training loss: 19.53857421875 = 0.4833614230155945 + 3 * 6.3517374992370605\n",
      "Epoch 1400, val loss: 0.9985939264297485\n",
      "Epoch 1410, training loss: 19.53004264831543 = 0.4764057397842407 + 3 * 6.351212024688721\n",
      "Epoch 1410, val loss: 0.9981650114059448\n",
      "Epoch 1420, training loss: 19.537927627563477 = 0.469522088766098 + 3 * 6.35613489151001\n",
      "Epoch 1420, val loss: 0.9978222846984863\n",
      "Epoch 1430, training loss: 19.51718521118164 = 0.4626862704753876 + 3 * 6.351499557495117\n",
      "Epoch 1430, val loss: 0.9974444508552551\n",
      "Epoch 1440, training loss: 19.503042221069336 = 0.4559844434261322 + 3 * 6.3490190505981445\n",
      "Epoch 1440, val loss: 0.9972947835922241\n",
      "Epoch 1450, training loss: 19.4943904876709 = 0.44934362173080444 + 3 * 6.348349094390869\n",
      "Epoch 1450, val loss: 0.9971125721931458\n",
      "Epoch 1460, training loss: 19.5042724609375 = 0.4428039789199829 + 3 * 6.353822708129883\n",
      "Epoch 1460, val loss: 0.9970781207084656\n",
      "Epoch 1470, training loss: 19.480545043945312 = 0.4362793266773224 + 3 * 6.348088264465332\n",
      "Epoch 1470, val loss: 0.9969597458839417\n",
      "Epoch 1480, training loss: 19.466930389404297 = 0.42990002036094666 + 3 * 6.345676898956299\n",
      "Epoch 1480, val loss: 0.9970766305923462\n",
      "Epoch 1490, training loss: 19.460105895996094 = 0.423571914434433 + 3 * 6.345510959625244\n",
      "Epoch 1490, val loss: 0.9971160888671875\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.6925925925925926\n",
      "0.8096995255666843\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 27.73525047302246 = 1.944612741470337 + 3 * 8.596879005432129\n",
      "Epoch 0, val loss: 1.9533734321594238\n",
      "Epoch 10, training loss: 27.729881286621094 = 1.9394025802612305 + 3 * 8.59682559967041\n",
      "Epoch 10, val loss: 1.9481137990951538\n",
      "Epoch 20, training loss: 27.723674774169922 = 1.9336748123168945 + 3 * 8.59666633605957\n",
      "Epoch 20, val loss: 1.942305326461792\n",
      "Epoch 30, training loss: 27.7152042388916 = 1.9268207550048828 + 3 * 8.5961275100708\n",
      "Epoch 30, val loss: 1.935398817062378\n",
      "Epoch 40, training loss: 27.70012855529785 = 1.918206810951233 + 3 * 8.593974113464355\n",
      "Epoch 40, val loss: 1.9266831874847412\n",
      "Epoch 50, training loss: 27.664152145385742 = 1.907164454460144 + 3 * 8.585662841796875\n",
      "Epoch 50, val loss: 1.9155097007751465\n",
      "Epoch 60, training loss: 27.566844940185547 = 1.8939547538757324 + 3 * 8.557629585266113\n",
      "Epoch 60, val loss: 1.9024015665054321\n",
      "Epoch 70, training loss: 27.275623321533203 = 1.8810462951660156 + 3 * 8.464859008789062\n",
      "Epoch 70, val loss: 1.8899800777435303\n",
      "Epoch 80, training loss: 26.70742416381836 = 1.869449496269226 + 3 * 8.279324531555176\n",
      "Epoch 80, val loss: 1.8791955709457397\n",
      "Epoch 90, training loss: 25.691465377807617 = 1.8626421689987183 + 3 * 7.942940711975098\n",
      "Epoch 90, val loss: 1.8730168342590332\n",
      "Epoch 100, training loss: 24.859920501708984 = 1.8596020936965942 + 3 * 7.666772842407227\n",
      "Epoch 100, val loss: 1.8700761795043945\n",
      "Epoch 110, training loss: 24.122905731201172 = 1.8562936782836914 + 3 * 7.422203540802002\n",
      "Epoch 110, val loss: 1.8666504621505737\n",
      "Epoch 120, training loss: 23.634363174438477 = 1.8503412008285522 + 3 * 7.261340618133545\n",
      "Epoch 120, val loss: 1.8609117269515991\n",
      "Epoch 130, training loss: 23.286767959594727 = 1.8439362049102783 + 3 * 7.147610187530518\n",
      "Epoch 130, val loss: 1.8549727201461792\n",
      "Epoch 140, training loss: 23.041606903076172 = 1.8378336429595947 + 3 * 7.067924499511719\n",
      "Epoch 140, val loss: 1.8491735458374023\n",
      "Epoch 150, training loss: 22.855247497558594 = 1.8317036628723145 + 3 * 7.007847785949707\n",
      "Epoch 150, val loss: 1.8432193994522095\n",
      "Epoch 160, training loss: 22.701202392578125 = 1.8254919052124023 + 3 * 6.958569526672363\n",
      "Epoch 160, val loss: 1.8373019695281982\n",
      "Epoch 170, training loss: 22.580167770385742 = 1.81936776638031 + 3 * 6.920266628265381\n",
      "Epoch 170, val loss: 1.8314303159713745\n",
      "Epoch 180, training loss: 22.464324951171875 = 1.8132774829864502 + 3 * 6.8836822509765625\n",
      "Epoch 180, val loss: 1.825641393661499\n",
      "Epoch 190, training loss: 22.365856170654297 = 1.8072668313980103 + 3 * 6.852863311767578\n",
      "Epoch 190, val loss: 1.81986403465271\n",
      "Epoch 200, training loss: 22.278512954711914 = 1.8012982606887817 + 3 * 6.825738430023193\n",
      "Epoch 200, val loss: 1.8141511678695679\n",
      "Epoch 210, training loss: 22.2020263671875 = 1.795354962348938 + 3 * 6.8022236824035645\n",
      "Epoch 210, val loss: 1.808452844619751\n",
      "Epoch 220, training loss: 22.131214141845703 = 1.7894177436828613 + 3 * 6.7805986404418945\n",
      "Epoch 220, val loss: 1.8027266263961792\n",
      "Epoch 230, training loss: 22.061538696289062 = 1.783423900604248 + 3 * 6.759371757507324\n",
      "Epoch 230, val loss: 1.796995759010315\n",
      "Epoch 240, training loss: 22.015485763549805 = 1.77731454372406 + 3 * 6.746057033538818\n",
      "Epoch 240, val loss: 1.7912226915359497\n",
      "Epoch 250, training loss: 21.94834327697754 = 1.7711125612258911 + 3 * 6.725743770599365\n",
      "Epoch 250, val loss: 1.7853302955627441\n",
      "Epoch 260, training loss: 21.89153480529785 = 1.7647582292556763 + 3 * 6.708925247192383\n",
      "Epoch 260, val loss: 1.7793413400650024\n",
      "Epoch 270, training loss: 21.841812133789062 = 1.7582072019577026 + 3 * 6.694534778594971\n",
      "Epoch 270, val loss: 1.773226261138916\n",
      "Epoch 280, training loss: 21.79635238647461 = 1.751414179801941 + 3 * 6.681646347045898\n",
      "Epoch 280, val loss: 1.766940951347351\n",
      "Epoch 290, training loss: 21.752887725830078 = 1.7443512678146362 + 3 * 6.6695122718811035\n",
      "Epoch 290, val loss: 1.760461449623108\n",
      "Epoch 300, training loss: 21.71084976196289 = 1.736979603767395 + 3 * 6.657956123352051\n",
      "Epoch 300, val loss: 1.7537860870361328\n",
      "Epoch 310, training loss: 21.668272018432617 = 1.7293143272399902 + 3 * 6.6463189125061035\n",
      "Epoch 310, val loss: 1.7468806505203247\n",
      "Epoch 320, training loss: 21.628406524658203 = 1.721305251121521 + 3 * 6.635700225830078\n",
      "Epoch 320, val loss: 1.7397093772888184\n",
      "Epoch 330, training loss: 21.5906982421875 = 1.712910771369934 + 3 * 6.62592887878418\n",
      "Epoch 330, val loss: 1.7322548627853394\n",
      "Epoch 340, training loss: 21.569664001464844 = 1.7041064500808716 + 3 * 6.621852397918701\n",
      "Epoch 340, val loss: 1.7244702577590942\n",
      "Epoch 350, training loss: 21.520793914794922 = 1.6948614120483398 + 3 * 6.608643531799316\n",
      "Epoch 350, val loss: 1.7163901329040527\n",
      "Epoch 360, training loss: 21.48459243774414 = 1.6851892471313477 + 3 * 6.5998005867004395\n",
      "Epoch 360, val loss: 1.7079834938049316\n",
      "Epoch 370, training loss: 21.455888748168945 = 1.6750677824020386 + 3 * 6.593606948852539\n",
      "Epoch 370, val loss: 1.6992281675338745\n",
      "Epoch 380, training loss: 21.419286727905273 = 1.6645032167434692 + 3 * 6.584927558898926\n",
      "Epoch 380, val loss: 1.6901342868804932\n",
      "Epoch 390, training loss: 21.383342742919922 = 1.6534589529037476 + 3 * 6.576627731323242\n",
      "Epoch 390, val loss: 1.680672287940979\n",
      "Epoch 400, training loss: 21.352426528930664 = 1.6419392824172974 + 3 * 6.570162296295166\n",
      "Epoch 400, val loss: 1.6708471775054932\n",
      "Epoch 410, training loss: 21.317768096923828 = 1.6300017833709717 + 3 * 6.562589168548584\n",
      "Epoch 410, val loss: 1.6607177257537842\n",
      "Epoch 420, training loss: 21.285598754882812 = 1.6176214218139648 + 3 * 6.555992603302002\n",
      "Epoch 420, val loss: 1.6502549648284912\n",
      "Epoch 430, training loss: 21.26390838623047 = 1.6047879457473755 + 3 * 6.553040027618408\n",
      "Epoch 430, val loss: 1.6394661664962769\n",
      "Epoch 440, training loss: 21.22733497619629 = 1.5915132761001587 + 3 * 6.545273780822754\n",
      "Epoch 440, val loss: 1.6283318996429443\n",
      "Epoch 450, training loss: 21.191688537597656 = 1.5778506994247437 + 3 * 6.537945747375488\n",
      "Epoch 450, val loss: 1.6169291734695435\n",
      "Epoch 460, training loss: 21.16238784790039 = 1.5637906789779663 + 3 * 6.532865524291992\n",
      "Epoch 460, val loss: 1.6052159070968628\n",
      "Epoch 470, training loss: 21.13463592529297 = 1.549372673034668 + 3 * 6.528420925140381\n",
      "Epoch 470, val loss: 1.5932549238204956\n",
      "Epoch 480, training loss: 21.102951049804688 = 1.5346790552139282 + 3 * 6.522757053375244\n",
      "Epoch 480, val loss: 1.5811294317245483\n",
      "Epoch 490, training loss: 21.071815490722656 = 1.5197008848190308 + 3 * 6.517371654510498\n",
      "Epoch 490, val loss: 1.5688499212265015\n",
      "Epoch 500, training loss: 21.042423248291016 = 1.504467248916626 + 3 * 6.5126519203186035\n",
      "Epoch 500, val loss: 1.556388258934021\n",
      "Epoch 510, training loss: 21.012495040893555 = 1.489017367362976 + 3 * 6.50782585144043\n",
      "Epoch 510, val loss: 1.5437816381454468\n",
      "Epoch 520, training loss: 20.987018585205078 = 1.4734143018722534 + 3 * 6.50453519821167\n",
      "Epoch 520, val loss: 1.5311657190322876\n",
      "Epoch 530, training loss: 20.954355239868164 = 1.4577096700668335 + 3 * 6.498881816864014\n",
      "Epoch 530, val loss: 1.5185242891311646\n",
      "Epoch 540, training loss: 20.93801498413086 = 1.4419476985931396 + 3 * 6.498688697814941\n",
      "Epoch 540, val loss: 1.5058629512786865\n",
      "Epoch 550, training loss: 20.898794174194336 = 1.4261234998703003 + 3 * 6.4908905029296875\n",
      "Epoch 550, val loss: 1.4932657480239868\n",
      "Epoch 560, training loss: 20.87187957763672 = 1.4103196859359741 + 3 * 6.487186431884766\n",
      "Epoch 560, val loss: 1.4807591438293457\n",
      "Epoch 570, training loss: 20.847286224365234 = 1.3945398330688477 + 3 * 6.484248638153076\n",
      "Epoch 570, val loss: 1.4683345556259155\n",
      "Epoch 580, training loss: 20.823745727539062 = 1.378760576248169 + 3 * 6.481661319732666\n",
      "Epoch 580, val loss: 1.455980896949768\n",
      "Epoch 590, training loss: 20.794965744018555 = 1.3631083965301514 + 3 * 6.477285861968994\n",
      "Epoch 590, val loss: 1.4436874389648438\n",
      "Epoch 600, training loss: 20.763587951660156 = 1.3474888801574707 + 3 * 6.4720330238342285\n",
      "Epoch 600, val loss: 1.4316333532333374\n",
      "Epoch 610, training loss: 20.743846893310547 = 1.3319768905639648 + 3 * 6.47062349319458\n",
      "Epoch 610, val loss: 1.4196510314941406\n",
      "Epoch 620, training loss: 20.721023559570312 = 1.3165655136108398 + 3 * 6.4681525230407715\n",
      "Epoch 620, val loss: 1.4078470468521118\n",
      "Epoch 630, training loss: 20.691314697265625 = 1.3012752532958984 + 3 * 6.463346481323242\n",
      "Epoch 630, val loss: 1.396125316619873\n",
      "Epoch 640, training loss: 20.6641902923584 = 1.2861114740371704 + 3 * 6.459359169006348\n",
      "Epoch 640, val loss: 1.38461172580719\n",
      "Epoch 650, training loss: 20.642736434936523 = 1.2710481882095337 + 3 * 6.4572296142578125\n",
      "Epoch 650, val loss: 1.3732333183288574\n",
      "Epoch 660, training loss: 20.619529724121094 = 1.2560704946517944 + 3 * 6.45448637008667\n",
      "Epoch 660, val loss: 1.3619385957717896\n",
      "Epoch 670, training loss: 20.602394104003906 = 1.2412025928497314 + 3 * 6.453730583190918\n",
      "Epoch 670, val loss: 1.350745677947998\n",
      "Epoch 680, training loss: 20.5723819732666 = 1.2263516187667847 + 3 * 6.448677062988281\n",
      "Epoch 680, val loss: 1.339677333831787\n",
      "Epoch 690, training loss: 20.5484676361084 = 1.211656093597412 + 3 * 6.445603847503662\n",
      "Epoch 690, val loss: 1.3287099599838257\n",
      "Epoch 700, training loss: 20.527034759521484 = 1.1969671249389648 + 3 * 6.443355560302734\n",
      "Epoch 700, val loss: 1.3177672624588013\n",
      "Epoch 710, training loss: 20.504194259643555 = 1.182397484779358 + 3 * 6.440598487854004\n",
      "Epoch 710, val loss: 1.3069047927856445\n",
      "Epoch 720, training loss: 20.481794357299805 = 1.1678920984268188 + 3 * 6.437967300415039\n",
      "Epoch 720, val loss: 1.2961015701293945\n",
      "Epoch 730, training loss: 20.457395553588867 = 1.1534615755081177 + 3 * 6.43464469909668\n",
      "Epoch 730, val loss: 1.2854400873184204\n",
      "Epoch 740, training loss: 20.436187744140625 = 1.1390901803970337 + 3 * 6.432365894317627\n",
      "Epoch 740, val loss: 1.2747982740402222\n",
      "Epoch 750, training loss: 20.430500030517578 = 1.1247459650039673 + 3 * 6.435251235961914\n",
      "Epoch 750, val loss: 1.2642371654510498\n",
      "Epoch 760, training loss: 20.395278930664062 = 1.1105133295059204 + 3 * 6.428255081176758\n",
      "Epoch 760, val loss: 1.2536792755126953\n",
      "Epoch 770, training loss: 20.372411727905273 = 1.0963664054870605 + 3 * 6.425348281860352\n",
      "Epoch 770, val loss: 1.243293285369873\n",
      "Epoch 780, training loss: 20.35102081298828 = 1.0823458433151245 + 3 * 6.422891616821289\n",
      "Epoch 780, val loss: 1.2329617738723755\n",
      "Epoch 790, training loss: 20.349578857421875 = 1.0683794021606445 + 3 * 6.427066326141357\n",
      "Epoch 790, val loss: 1.2227360010147095\n",
      "Epoch 800, training loss: 20.31793975830078 = 1.0544521808624268 + 3 * 6.421162128448486\n",
      "Epoch 800, val loss: 1.2125314474105835\n",
      "Epoch 810, training loss: 20.2896728515625 = 1.040665864944458 + 3 * 6.416335582733154\n",
      "Epoch 810, val loss: 1.2024766206741333\n",
      "Epoch 820, training loss: 20.268857955932617 = 1.0269666910171509 + 3 * 6.413963794708252\n",
      "Epoch 820, val loss: 1.1924850940704346\n",
      "Epoch 830, training loss: 20.250436782836914 = 1.013353943824768 + 3 * 6.412361145019531\n",
      "Epoch 830, val loss: 1.1826013326644897\n",
      "Epoch 840, training loss: 20.237455368041992 = 0.9998413920402527 + 3 * 6.412537574768066\n",
      "Epoch 840, val loss: 1.1728332042694092\n",
      "Epoch 850, training loss: 20.215232849121094 = 0.9864519834518433 + 3 * 6.40959358215332\n",
      "Epoch 850, val loss: 1.1631855964660645\n",
      "Epoch 860, training loss: 20.197311401367188 = 0.9732600450515747 + 3 * 6.408016681671143\n",
      "Epoch 860, val loss: 1.1536082029342651\n",
      "Epoch 870, training loss: 20.178512573242188 = 0.960170328617096 + 3 * 6.406113624572754\n",
      "Epoch 870, val loss: 1.1443208456039429\n",
      "Epoch 880, training loss: 20.156143188476562 = 0.9472852349281311 + 3 * 6.402953147888184\n",
      "Epoch 880, val loss: 1.1351354122161865\n",
      "Epoch 890, training loss: 20.137975692749023 = 0.9345228672027588 + 3 * 6.401150703430176\n",
      "Epoch 890, val loss: 1.1261401176452637\n",
      "Epoch 900, training loss: 20.137832641601562 = 0.9219018220901489 + 3 * 6.40531063079834\n",
      "Epoch 900, val loss: 1.1172704696655273\n",
      "Epoch 910, training loss: 20.114519119262695 = 0.9094021320343018 + 3 * 6.401705265045166\n",
      "Epoch 910, val loss: 1.1085560321807861\n",
      "Epoch 920, training loss: 20.086803436279297 = 0.8971178531646729 + 3 * 6.396562099456787\n",
      "Epoch 920, val loss: 1.100115418434143\n",
      "Epoch 930, training loss: 20.069360733032227 = 0.8850106000900269 + 3 * 6.3947834968566895\n",
      "Epoch 930, val loss: 1.0918219089508057\n",
      "Epoch 940, training loss: 20.05731201171875 = 0.8730565905570984 + 3 * 6.394752025604248\n",
      "Epoch 940, val loss: 1.0837913751602173\n",
      "Epoch 950, training loss: 20.041187286376953 = 0.8612459897994995 + 3 * 6.393313884735107\n",
      "Epoch 950, val loss: 1.0758116245269775\n",
      "Epoch 960, training loss: 20.021255493164062 = 0.8496559858322144 + 3 * 6.390533447265625\n",
      "Epoch 960, val loss: 1.0682158470153809\n",
      "Epoch 970, training loss: 20.004165649414062 = 0.8382478952407837 + 3 * 6.388639450073242\n",
      "Epoch 970, val loss: 1.060725212097168\n",
      "Epoch 980, training loss: 19.98848533630371 = 0.8269816040992737 + 3 * 6.387167453765869\n",
      "Epoch 980, val loss: 1.0535520315170288\n",
      "Epoch 990, training loss: 19.993968963623047 = 0.8158966898918152 + 3 * 6.392690658569336\n",
      "Epoch 990, val loss: 1.0465232133865356\n",
      "Epoch 1000, training loss: 19.95688247680664 = 0.804914116859436 + 3 * 6.383989334106445\n",
      "Epoch 1000, val loss: 1.0397465229034424\n",
      "Epoch 1010, training loss: 19.943180084228516 = 0.7941386103630066 + 3 * 6.38301420211792\n",
      "Epoch 1010, val loss: 1.0332401990890503\n",
      "Epoch 1020, training loss: 19.927295684814453 = 0.783534824848175 + 3 * 6.381253242492676\n",
      "Epoch 1020, val loss: 1.026858925819397\n",
      "Epoch 1030, training loss: 19.927001953125 = 0.7730292081832886 + 3 * 6.384657859802246\n",
      "Epoch 1030, val loss: 1.0207602977752686\n",
      "Epoch 1040, training loss: 19.918174743652344 = 0.7627381682395935 + 3 * 6.385145664215088\n",
      "Epoch 1040, val loss: 1.0147128105163574\n",
      "Epoch 1050, training loss: 19.884571075439453 = 0.7525184154510498 + 3 * 6.3773512840271\n",
      "Epoch 1050, val loss: 1.0090157985687256\n",
      "Epoch 1060, training loss: 19.87101173400879 = 0.7424647808074951 + 3 * 6.376182556152344\n",
      "Epoch 1060, val loss: 1.0035113096237183\n",
      "Epoch 1070, training loss: 19.855127334594727 = 0.7325536012649536 + 3 * 6.3741912841796875\n",
      "Epoch 1070, val loss: 0.9981060028076172\n",
      "Epoch 1080, training loss: 19.848350524902344 = 0.7227003574371338 + 3 * 6.375216960906982\n",
      "Epoch 1080, val loss: 0.9929226636886597\n",
      "Epoch 1090, training loss: 19.830665588378906 = 0.7129401564598083 + 3 * 6.372575283050537\n",
      "Epoch 1090, val loss: 0.987861692905426\n",
      "Epoch 1100, training loss: 19.81830406188965 = 0.7033042311668396 + 3 * 6.371666431427002\n",
      "Epoch 1100, val loss: 0.9829903841018677\n",
      "Epoch 1110, training loss: 19.805753707885742 = 0.6937667727470398 + 3 * 6.370662212371826\n",
      "Epoch 1110, val loss: 0.9783380627632141\n",
      "Epoch 1120, training loss: 19.798738479614258 = 0.6843332052230835 + 3 * 6.3714680671691895\n",
      "Epoch 1120, val loss: 0.9738196730613708\n",
      "Epoch 1130, training loss: 19.783002853393555 = 0.6749826073646545 + 3 * 6.369340419769287\n",
      "Epoch 1130, val loss: 0.9693560600280762\n",
      "Epoch 1140, training loss: 19.770158767700195 = 0.6657300591468811 + 3 * 6.368143081665039\n",
      "Epoch 1140, val loss: 0.9651827216148376\n",
      "Epoch 1150, training loss: 19.752885818481445 = 0.6565854549407959 + 3 * 6.365433216094971\n",
      "Epoch 1150, val loss: 0.9611320495605469\n",
      "Epoch 1160, training loss: 19.745134353637695 = 0.6475281715393066 + 3 * 6.365869045257568\n",
      "Epoch 1160, val loss: 0.9572103023529053\n",
      "Epoch 1170, training loss: 19.736003875732422 = 0.6385054588317871 + 3 * 6.365832805633545\n",
      "Epoch 1170, val loss: 0.9534046053886414\n",
      "Epoch 1180, training loss: 19.714492797851562 = 0.6296045780181885 + 3 * 6.361629486083984\n",
      "Epoch 1180, val loss: 0.9496904015541077\n",
      "Epoch 1190, training loss: 19.70292091369629 = 0.6207675337791443 + 3 * 6.3607177734375\n",
      "Epoch 1190, val loss: 0.9462419152259827\n",
      "Epoch 1200, training loss: 19.690292358398438 = 0.6120211482048035 + 3 * 6.359424114227295\n",
      "Epoch 1200, val loss: 0.9428608417510986\n",
      "Epoch 1210, training loss: 19.6824951171875 = 0.6033331751823425 + 3 * 6.3597211837768555\n",
      "Epoch 1210, val loss: 0.9396260976791382\n",
      "Epoch 1220, training loss: 19.67169761657715 = 0.5946981310844421 + 3 * 6.358999729156494\n",
      "Epoch 1220, val loss: 0.9365134835243225\n",
      "Epoch 1230, training loss: 19.660213470458984 = 0.586132824420929 + 3 * 6.358027458190918\n",
      "Epoch 1230, val loss: 0.9335083365440369\n",
      "Epoch 1240, training loss: 19.65166664123535 = 0.5776644349098206 + 3 * 6.358000755310059\n",
      "Epoch 1240, val loss: 0.9306454062461853\n",
      "Epoch 1250, training loss: 19.637584686279297 = 0.5692362785339355 + 3 * 6.35611629486084\n",
      "Epoch 1250, val loss: 0.9279611706733704\n",
      "Epoch 1260, training loss: 19.62255859375 = 0.5609142184257507 + 3 * 6.353881359100342\n",
      "Epoch 1260, val loss: 0.9253000020980835\n",
      "Epoch 1270, training loss: 19.6329345703125 = 0.5526338815689087 + 3 * 6.360100269317627\n",
      "Epoch 1270, val loss: 0.9227409362792969\n",
      "Epoch 1280, training loss: 19.603321075439453 = 0.5443691611289978 + 3 * 6.3529839515686035\n",
      "Epoch 1280, val loss: 0.9204230308532715\n",
      "Epoch 1290, training loss: 19.5892276763916 = 0.5362290740013123 + 3 * 6.35099983215332\n",
      "Epoch 1290, val loss: 0.9181219339370728\n",
      "Epoch 1300, training loss: 19.579561233520508 = 0.5281395316123962 + 3 * 6.350473880767822\n",
      "Epoch 1300, val loss: 0.9159529209136963\n",
      "Epoch 1310, training loss: 19.585357666015625 = 0.5200965404510498 + 3 * 6.3550872802734375\n",
      "Epoch 1310, val loss: 0.9139217138290405\n",
      "Epoch 1320, training loss: 19.564350128173828 = 0.5121212601661682 + 3 * 6.350742816925049\n",
      "Epoch 1320, val loss: 0.9119848608970642\n",
      "Epoch 1330, training loss: 19.550798416137695 = 0.5041720271110535 + 3 * 6.348875522613525\n",
      "Epoch 1330, val loss: 0.9100824594497681\n",
      "Epoch 1340, training loss: 19.53693962097168 = 0.4963168501853943 + 3 * 6.346874237060547\n",
      "Epoch 1340, val loss: 0.9084120988845825\n",
      "Epoch 1350, training loss: 19.528566360473633 = 0.4885008633136749 + 3 * 6.346688270568848\n",
      "Epoch 1350, val loss: 0.906788170337677\n",
      "Epoch 1360, training loss: 19.532072067260742 = 0.4807451069355011 + 3 * 6.350442409515381\n",
      "Epoch 1360, val loss: 0.9052830934524536\n",
      "Epoch 1370, training loss: 19.5076904296875 = 0.47299981117248535 + 3 * 6.3448967933654785\n",
      "Epoch 1370, val loss: 0.903853178024292\n",
      "Epoch 1380, training loss: 19.49827003479004 = 0.4653545022010803 + 3 * 6.344305038452148\n",
      "Epoch 1380, val loss: 0.90260249376297\n",
      "Epoch 1390, training loss: 19.484474182128906 = 0.45779433846473694 + 3 * 6.342226505279541\n",
      "Epoch 1390, val loss: 0.901440441608429\n",
      "Epoch 1400, training loss: 19.477901458740234 = 0.45026400685310364 + 3 * 6.342545509338379\n",
      "Epoch 1400, val loss: 0.9003769755363464\n",
      "Epoch 1410, training loss: 19.473573684692383 = 0.44278308749198914 + 3 * 6.343596935272217\n",
      "Epoch 1410, val loss: 0.8994030356407166\n",
      "Epoch 1420, training loss: 19.463773727416992 = 0.4353577196598053 + 3 * 6.342804908752441\n",
      "Epoch 1420, val loss: 0.8985598683357239\n",
      "Epoch 1430, training loss: 19.447917938232422 = 0.42799416184425354 + 3 * 6.339974880218506\n",
      "Epoch 1430, val loss: 0.8977997303009033\n",
      "Epoch 1440, training loss: 19.43821907043457 = 0.4206937253475189 + 3 * 6.339175224304199\n",
      "Epoch 1440, val loss: 0.8971829414367676\n",
      "Epoch 1450, training loss: 19.427274703979492 = 0.413457989692688 + 3 * 6.3379387855529785\n",
      "Epoch 1450, val loss: 0.8966638445854187\n",
      "Epoch 1460, training loss: 19.426345825195312 = 0.4062858819961548 + 3 * 6.340019702911377\n",
      "Epoch 1460, val loss: 0.8962277173995972\n",
      "Epoch 1470, training loss: 19.41044044494629 = 0.39911919832229614 + 3 * 6.337107181549072\n",
      "Epoch 1470, val loss: 0.8958917260169983\n",
      "Epoch 1480, training loss: 19.405723571777344 = 0.3920494019985199 + 3 * 6.337891101837158\n",
      "Epoch 1480, val loss: 0.8956364393234253\n",
      "Epoch 1490, training loss: 19.391809463500977 = 0.3850230574607849 + 3 * 6.335595607757568\n",
      "Epoch 1490, val loss: 0.895606279373169\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7000000000000001\n",
      "0.8181338956246705\n",
      "The final CL Acc:0.71605, 0.02810, The final GNN Acc:0.81234, 0.00411\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "num_epochs = config['num_epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.10,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "print(\"=== Noisy graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device).to(device)\n",
    "    # model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc_noisy.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 13252])\n",
      "remove edge: torch.Size([2, 7990])\n",
      "updated graph: torch.Size([2, 10686])\n",
      "=== Raw graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 23.430858612060547 = 1.9388624429702759 + 2.5 * 8.59679889678955\n",
      "Epoch 0, val loss: 1.9236836433410645\n",
      "Epoch 10, training loss: 23.385498046875 = 1.9084854125976562 + 2.5 * 8.590805053710938\n",
      "Epoch 10, val loss: 1.893352746963501\n",
      "Epoch 20, training loss: 22.959487915039062 = 1.863033652305603 + 2.5 * 8.438581466674805\n",
      "Epoch 20, val loss: 1.849848985671997\n",
      "Epoch 30, training loss: 20.82345199584961 = 1.8153865337371826 + 2.5 * 7.603226184844971\n",
      "Epoch 30, val loss: 1.8069489002227783\n",
      "Epoch 40, training loss: 19.640308380126953 = 1.7822760343551636 + 2.5 * 7.143212795257568\n",
      "Epoch 40, val loss: 1.7775217294692993\n",
      "Epoch 50, training loss: 19.259761810302734 = 1.7454637289047241 + 2.5 * 7.005719184875488\n",
      "Epoch 50, val loss: 1.743575930595398\n",
      "Epoch 60, training loss: 18.935094833374023 = 1.7061216831207275 + 2.5 * 6.891589164733887\n",
      "Epoch 60, val loss: 1.708975911140442\n",
      "Epoch 70, training loss: 18.642539978027344 = 1.6622390747070312 + 2.5 * 6.792120456695557\n",
      "Epoch 70, val loss: 1.6702226400375366\n",
      "Epoch 80, training loss: 18.442214965820312 = 1.604502558708191 + 2.5 * 6.735085487365723\n",
      "Epoch 80, val loss: 1.6201756000518799\n",
      "Epoch 90, training loss: 18.249135971069336 = 1.5310461521148682 + 2.5 * 6.6872358322143555\n",
      "Epoch 90, val loss: 1.5574363470077515\n",
      "Epoch 100, training loss: 18.079435348510742 = 1.4434223175048828 + 2.5 * 6.654405117034912\n",
      "Epoch 100, val loss: 1.4849026203155518\n",
      "Epoch 110, training loss: 17.910898208618164 = 1.346468448638916 + 2.5 * 6.625772476196289\n",
      "Epoch 110, val loss: 1.406823992729187\n",
      "Epoch 120, training loss: 17.731599807739258 = 1.2454620599746704 + 2.5 * 6.594455242156982\n",
      "Epoch 120, val loss: 1.3274874687194824\n",
      "Epoch 130, training loss: 17.571533203125 = 1.1419119834899902 + 2.5 * 6.571848392486572\n",
      "Epoch 130, val loss: 1.2483913898468018\n",
      "Epoch 140, training loss: 17.425565719604492 = 1.040508508682251 + 2.5 * 6.554023265838623\n",
      "Epoch 140, val loss: 1.1725832223892212\n",
      "Epoch 150, training loss: 17.28864097595215 = 0.9458433389663696 + 2.5 * 6.537119388580322\n",
      "Epoch 150, val loss: 1.1029448509216309\n",
      "Epoch 160, training loss: 17.16058349609375 = 0.8592987060546875 + 2.5 * 6.520514011383057\n",
      "Epoch 160, val loss: 1.0408815145492554\n",
      "Epoch 170, training loss: 17.06789779663086 = 0.7806535959243774 + 2.5 * 6.51489782333374\n",
      "Epoch 170, val loss: 0.9861695170402527\n",
      "Epoch 180, training loss: 16.946044921875 = 0.7111112475395203 + 2.5 * 6.493973255157471\n",
      "Epoch 180, val loss: 0.9391416311264038\n",
      "Epoch 190, training loss: 16.846435546875 = 0.6486624479293823 + 2.5 * 6.479109287261963\n",
      "Epoch 190, val loss: 0.8998505473136902\n",
      "Epoch 200, training loss: 16.774883270263672 = 0.5909350514411926 + 2.5 * 6.473579406738281\n",
      "Epoch 200, val loss: 0.8659616708755493\n",
      "Epoch 210, training loss: 16.69818878173828 = 0.5371837019920349 + 2.5 * 6.464402198791504\n",
      "Epoch 210, val loss: 0.837329089641571\n",
      "Epoch 220, training loss: 16.610504150390625 = 0.4866594076156616 + 2.5 * 6.44953727722168\n",
      "Epoch 220, val loss: 0.8127946853637695\n",
      "Epoch 230, training loss: 16.543127059936523 = 0.4387872517108917 + 2.5 * 6.441735744476318\n",
      "Epoch 230, val loss: 0.7920498847961426\n",
      "Epoch 240, training loss: 16.52412986755371 = 0.3937948942184448 + 2.5 * 6.452133655548096\n",
      "Epoch 240, val loss: 0.7751438021659851\n",
      "Epoch 250, training loss: 16.43781280517578 = 0.35283362865448 + 2.5 * 6.433992385864258\n",
      "Epoch 250, val loss: 0.7624003291130066\n",
      "Epoch 260, training loss: 16.36641502380371 = 0.315843790769577 + 2.5 * 6.420228958129883\n",
      "Epoch 260, val loss: 0.7540168762207031\n",
      "Epoch 270, training loss: 16.368194580078125 = 0.2823914587497711 + 2.5 * 6.434321880340576\n",
      "Epoch 270, val loss: 0.7492688298225403\n",
      "Epoch 280, training loss: 16.27960968017578 = 0.2522915005683899 + 2.5 * 6.410927772521973\n",
      "Epoch 280, val loss: 0.7474857568740845\n",
      "Epoch 290, training loss: 16.23628044128418 = 0.2250855714082718 + 2.5 * 6.404477596282959\n",
      "Epoch 290, val loss: 0.7483150959014893\n",
      "Epoch 300, training loss: 16.24012565612793 = 0.20034964382648468 + 2.5 * 6.415910720825195\n",
      "Epoch 300, val loss: 0.7514487504959106\n",
      "Epoch 310, training loss: 16.1729736328125 = 0.17801159620285034 + 2.5 * 6.397984981536865\n",
      "Epoch 310, val loss: 0.7562332153320312\n",
      "Epoch 320, training loss: 16.13570213317871 = 0.15785758197307587 + 2.5 * 6.391137599945068\n",
      "Epoch 320, val loss: 0.7626820206642151\n",
      "Epoch 330, training loss: 16.137773513793945 = 0.139793261885643 + 2.5 * 6.3991923332214355\n",
      "Epoch 330, val loss: 0.7709304094314575\n",
      "Epoch 340, training loss: 16.08325958251953 = 0.12389485538005829 + 2.5 * 6.3837456703186035\n",
      "Epoch 340, val loss: 0.7802265286445618\n",
      "Epoch 350, training loss: 16.056015014648438 = 0.10999318212270737 + 2.5 * 6.378408432006836\n",
      "Epoch 350, val loss: 0.790794849395752\n",
      "Epoch 360, training loss: 16.03949737548828 = 0.09791172295808792 + 2.5 * 6.376634120941162\n",
      "Epoch 360, val loss: 0.80269455909729\n",
      "Epoch 370, training loss: 16.022220611572266 = 0.08749134838581085 + 2.5 * 6.373891830444336\n",
      "Epoch 370, val loss: 0.8148934841156006\n",
      "Epoch 380, training loss: 16.000022888183594 = 0.07855010032653809 + 2.5 * 6.368588924407959\n",
      "Epoch 380, val loss: 0.8273048996925354\n",
      "Epoch 390, training loss: 15.98143196105957 = 0.07081107795238495 + 2.5 * 6.364248275756836\n",
      "Epoch 390, val loss: 0.8406214118003845\n",
      "Epoch 400, training loss: 16.022363662719727 = 0.06410419940948486 + 2.5 * 6.383303642272949\n",
      "Epoch 400, val loss: 0.8539013862609863\n",
      "Epoch 410, training loss: 15.966987609863281 = 0.05825986713171005 + 2.5 * 6.363491058349609\n",
      "Epoch 410, val loss: 0.8667073845863342\n",
      "Epoch 420, training loss: 15.946282386779785 = 0.05320116505026817 + 2.5 * 6.357232570648193\n",
      "Epoch 420, val loss: 0.8799151182174683\n",
      "Epoch 430, training loss: 15.948019027709961 = 0.04876163601875305 + 2.5 * 6.359703063964844\n",
      "Epoch 430, val loss: 0.8930439352989197\n",
      "Epoch 440, training loss: 15.927740097045898 = 0.04486856609582901 + 2.5 * 6.353148460388184\n",
      "Epoch 440, val loss: 0.905427098274231\n",
      "Epoch 450, training loss: 15.906651496887207 = 0.04141511768102646 + 2.5 * 6.346094608306885\n",
      "Epoch 450, val loss: 0.9179502725601196\n",
      "Epoch 460, training loss: 15.912043571472168 = 0.038349542766809464 + 2.5 * 6.349477767944336\n",
      "Epoch 460, val loss: 0.9303879737854004\n",
      "Epoch 470, training loss: 15.93548583984375 = 0.035630762577056885 + 2.5 * 6.3599419593811035\n",
      "Epoch 470, val loss: 0.9421707987785339\n",
      "Epoch 480, training loss: 15.890434265136719 = 0.033193375915288925 + 2.5 * 6.342896461486816\n",
      "Epoch 480, val loss: 0.9530841112136841\n",
      "Epoch 490, training loss: 15.873515129089355 = 0.031021326780319214 + 2.5 * 6.3369975090026855\n",
      "Epoch 490, val loss: 0.9645825624465942\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7851851851851852\n",
      "0.8376383763837639\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 23.43621063232422 = 1.944177508354187 + 2.5 * 8.596813201904297\n",
      "Epoch 0, val loss: 1.9418244361877441\n",
      "Epoch 10, training loss: 23.389007568359375 = 1.9136841297149658 + 2.5 * 8.590128898620605\n",
      "Epoch 10, val loss: 1.9095796346664429\n",
      "Epoch 20, training loss: 22.79500961303711 = 1.8688621520996094 + 2.5 * 8.370458602905273\n",
      "Epoch 20, val loss: 1.8652647733688354\n",
      "Epoch 30, training loss: 20.684370040893555 = 1.8254534006118774 + 2.5 * 7.543567180633545\n",
      "Epoch 30, val loss: 1.8248416185379028\n",
      "Epoch 40, training loss: 19.48661994934082 = 1.793092131614685 + 2.5 * 7.077410697937012\n",
      "Epoch 40, val loss: 1.79435133934021\n",
      "Epoch 50, training loss: 19.097923278808594 = 1.76188325881958 + 2.5 * 6.934415817260742\n",
      "Epoch 50, val loss: 1.7637590169906616\n",
      "Epoch 60, training loss: 18.742206573486328 = 1.722243309020996 + 2.5 * 6.807984828948975\n",
      "Epoch 60, val loss: 1.725561499595642\n",
      "Epoch 70, training loss: 18.489826202392578 = 1.680099368095398 + 2.5 * 6.723890781402588\n",
      "Epoch 70, val loss: 1.6850847005844116\n",
      "Epoch 80, training loss: 18.264446258544922 = 1.6303073167800903 + 2.5 * 6.653655529022217\n",
      "Epoch 80, val loss: 1.6389634609222412\n",
      "Epoch 90, training loss: 18.086923599243164 = 1.5654194355010986 + 2.5 * 6.6086015701293945\n",
      "Epoch 90, val loss: 1.5816655158996582\n",
      "Epoch 100, training loss: 17.922279357910156 = 1.4855083227157593 + 2.5 * 6.574708938598633\n",
      "Epoch 100, val loss: 1.511574625968933\n",
      "Epoch 110, training loss: 17.784183502197266 = 1.3928147554397583 + 2.5 * 6.55654764175415\n",
      "Epoch 110, val loss: 1.4319764375686646\n",
      "Epoch 120, training loss: 17.611083984375 = 1.2904154062271118 + 2.5 * 6.5282673835754395\n",
      "Epoch 120, val loss: 1.3476903438568115\n",
      "Epoch 130, training loss: 17.465534210205078 = 1.1842035055160522 + 2.5 * 6.5125322341918945\n",
      "Epoch 130, val loss: 1.262532353401184\n",
      "Epoch 140, training loss: 17.32266616821289 = 1.0814216136932373 + 2.5 * 6.496498107910156\n",
      "Epoch 140, val loss: 1.181748390197754\n",
      "Epoch 150, training loss: 17.198091506958008 = 0.9861932992935181 + 2.5 * 6.484759330749512\n",
      "Epoch 150, val loss: 1.1083030700683594\n",
      "Epoch 160, training loss: 17.085712432861328 = 0.8997659683227539 + 2.5 * 6.47437858581543\n",
      "Epoch 160, val loss: 1.0421779155731201\n",
      "Epoch 170, training loss: 16.976783752441406 = 0.8221335411071777 + 2.5 * 6.461859703063965\n",
      "Epoch 170, val loss: 0.9835904836654663\n",
      "Epoch 180, training loss: 16.88701629638672 = 0.7522167563438416 + 2.5 * 6.453920364379883\n",
      "Epoch 180, val loss: 0.9321972131729126\n",
      "Epoch 190, training loss: 16.79405403137207 = 0.6891881227493286 + 2.5 * 6.441946506500244\n",
      "Epoch 190, val loss: 0.8878791928291321\n",
      "Epoch 200, training loss: 16.709474563598633 = 0.6308340430259705 + 2.5 * 6.431456089019775\n",
      "Epoch 200, val loss: 0.8498920798301697\n",
      "Epoch 210, training loss: 16.647239685058594 = 0.575973629951477 + 2.5 * 6.428506851196289\n",
      "Epoch 210, val loss: 0.8173888325691223\n",
      "Epoch 220, training loss: 16.570362091064453 = 0.5241318345069885 + 2.5 * 6.418492317199707\n",
      "Epoch 220, val loss: 0.7896608114242554\n",
      "Epoch 230, training loss: 16.505231857299805 = 0.4749623239040375 + 2.5 * 6.412107944488525\n",
      "Epoch 230, val loss: 0.7659236192703247\n",
      "Epoch 240, training loss: 16.444467544555664 = 0.42866945266723633 + 2.5 * 6.4063191413879395\n",
      "Epoch 240, val loss: 0.7459783554077148\n",
      "Epoch 250, training loss: 16.39652442932129 = 0.38537096977233887 + 2.5 * 6.404461860656738\n",
      "Epoch 250, val loss: 0.729590892791748\n",
      "Epoch 260, training loss: 16.341205596923828 = 0.3452422022819519 + 2.5 * 6.398385524749756\n",
      "Epoch 260, val loss: 0.716364324092865\n",
      "Epoch 270, training loss: 16.280569076538086 = 0.3084801733493805 + 2.5 * 6.388835430145264\n",
      "Epoch 270, val loss: 0.7065611481666565\n",
      "Epoch 280, training loss: 16.23649024963379 = 0.2750456631183624 + 2.5 * 6.384577751159668\n",
      "Epoch 280, val loss: 0.6996843218803406\n",
      "Epoch 290, training loss: 16.19062042236328 = 0.2447614073753357 + 2.5 * 6.37834358215332\n",
      "Epoch 290, val loss: 0.6957727670669556\n",
      "Epoch 300, training loss: 16.17706298828125 = 0.217497780919075 + 2.5 * 6.38382625579834\n",
      "Epoch 300, val loss: 0.6944649815559387\n",
      "Epoch 310, training loss: 16.131467819213867 = 0.19321833550930023 + 2.5 * 6.37529993057251\n",
      "Epoch 310, val loss: 0.6949008107185364\n",
      "Epoch 320, training loss: 16.08555030822754 = 0.17192845046520233 + 2.5 * 6.365448951721191\n",
      "Epoch 320, val loss: 0.6974214315414429\n",
      "Epoch 330, training loss: 16.07475471496582 = 0.15308761596679688 + 2.5 * 6.368666648864746\n",
      "Epoch 330, val loss: 0.7019382119178772\n",
      "Epoch 340, training loss: 16.04468536376953 = 0.13661912083625793 + 2.5 * 6.363226413726807\n",
      "Epoch 340, val loss: 0.7073301672935486\n",
      "Epoch 350, training loss: 16.015117645263672 = 0.12217380851507187 + 2.5 * 6.357177734375\n",
      "Epoch 350, val loss: 0.7136743664741516\n",
      "Epoch 360, training loss: 16.012845993041992 = 0.10951070487499237 + 2.5 * 6.361333847045898\n",
      "Epoch 360, val loss: 0.7213291525840759\n",
      "Epoch 370, training loss: 15.971749305725098 = 0.09842944145202637 + 2.5 * 6.34932804107666\n",
      "Epoch 370, val loss: 0.7296227812767029\n",
      "Epoch 380, training loss: 15.969987869262695 = 0.08873534947633743 + 2.5 * 6.352500915527344\n",
      "Epoch 380, val loss: 0.7383913993835449\n",
      "Epoch 390, training loss: 15.940079689025879 = 0.08019769936800003 + 2.5 * 6.3439531326293945\n",
      "Epoch 390, val loss: 0.7473896145820618\n",
      "Epoch 400, training loss: 15.922829627990723 = 0.07270930707454681 + 2.5 * 6.340048313140869\n",
      "Epoch 400, val loss: 0.7568041682243347\n",
      "Epoch 410, training loss: 15.941012382507324 = 0.0661163404583931 + 2.5 * 6.349958419799805\n",
      "Epoch 410, val loss: 0.7665483355522156\n",
      "Epoch 420, training loss: 15.914766311645508 = 0.06028194725513458 + 2.5 * 6.341794013977051\n",
      "Epoch 420, val loss: 0.7758156061172485\n",
      "Epoch 430, training loss: 15.883566856384277 = 0.055155687034130096 + 2.5 * 6.331364631652832\n",
      "Epoch 430, val loss: 0.7853529453277588\n",
      "Epoch 440, training loss: 15.8822660446167 = 0.050652872771024704 + 2.5 * 6.332645416259766\n",
      "Epoch 440, val loss: 0.7951763868331909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(dataset\u001b[39m.\u001b[39mnum_features, num_hidden, activation,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m                         base_model\u001b[39m=\u001b[39mbase_model, k\u001b[39m=\u001b[39mnum_layers)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m model \u001b[39m=\u001b[39m UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(args, data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index,data\u001b[39m.\u001b[39;49medge_weight,data\u001b[39m.\u001b[39;49my,idx_train,idx_val\u001b[39m=\u001b[39;49midx_val,train_iters\u001b[39m=\u001b[39;49mnum_epochs,cont_iters\u001b[39m=\u001b[39;49mnum_epochs,seen_node_idx\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m acc_cl \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index,data\u001b[39m.\u001b[39medge_weight,data\u001b[39m.\u001b[39my,idx_clean_test)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:231\u001b[0m, in \u001b[0;36mUnifyModel.fit\u001b[0;34m(self, args, x, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, cont_iters, seen_node_idx)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:310\u001b[0m, in \u001b[0;36mUnifyModel._train_with_val\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, training loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m + \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m * \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i, loss\u001b[39m.\u001b[39mitem(),clf_loss\u001b[39m.\u001b[39mitem(),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcont_weight,cont_loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m--> 310\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    311\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    314\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "num_epochs = config['num_epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=None).to(device)\n",
    "    model.fit(args, data.x, data.edge_index,data.edge_weight,data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(data.x, data.edge_index,data.edge_weight,data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',data,device)\n",
    "    gnn_model.fit(data.x, data.edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy+DIffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 13190])\n",
      "remove edge: torch.Size([2, 7998])\n",
      "updated graph: torch.Size([2, 10632])\n",
      "=== Noisy graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.6275024414062 = 1.9418861865997314 + 100 * 8.596856117248535\n",
      "Epoch 0, val loss: 1.9364506006240845\n",
      "Epoch 10, training loss: 848.9478759765625 = 1.897546648979187 + 100 * 8.470503807067871\n",
      "Epoch 10, val loss: 1.8890513181686401\n",
      "Epoch 20, training loss: 760.0466918945312 = 1.8456153869628906 + 100 * 7.582010746002197\n",
      "Epoch 20, val loss: 1.8392390012741089\n",
      "Epoch 30, training loss: 731.3434448242188 = 1.8136775493621826 + 100 * 7.295298099517822\n",
      "Epoch 30, val loss: 1.808591365814209\n",
      "Epoch 40, training loss: 714.7428588867188 = 1.788662314414978 + 100 * 7.129542350769043\n",
      "Epoch 40, val loss: 1.7784889936447144\n",
      "Epoch 50, training loss: 705.1812744140625 = 1.7627829313278198 + 100 * 7.034185409545898\n",
      "Epoch 50, val loss: 1.748166561126709\n",
      "Epoch 60, training loss: 695.6590576171875 = 1.7388758659362793 + 100 * 6.939201354980469\n",
      "Epoch 60, val loss: 1.7244641780853271\n",
      "Epoch 70, training loss: 689.8345336914062 = 1.7147310972213745 + 100 * 6.881198406219482\n",
      "Epoch 70, val loss: 1.6988224983215332\n",
      "Epoch 80, training loss: 684.240478515625 = 1.684454321861267 + 100 * 6.825560092926025\n",
      "Epoch 80, val loss: 1.669772744178772\n",
      "Epoch 90, training loss: 679.1021728515625 = 1.6641491651535034 + 100 * 6.774380207061768\n",
      "Epoch 90, val loss: 1.647947072982788\n",
      "Epoch 100, training loss: 676.1454467773438 = 1.6357783079147339 + 100 * 6.745096683502197\n",
      "Epoch 100, val loss: 1.6176695823669434\n",
      "Epoch 110, training loss: 671.8375244140625 = 1.60782790184021 + 100 * 6.702296733856201\n",
      "Epoch 110, val loss: 1.5917643308639526\n",
      "Epoch 120, training loss: 668.9812622070312 = 1.5809794664382935 + 100 * 6.674002647399902\n",
      "Epoch 120, val loss: 1.5651280879974365\n",
      "Epoch 130, training loss: 667.2160034179688 = 1.5507876873016357 + 100 * 6.656652450561523\n",
      "Epoch 130, val loss: 1.5334396362304688\n",
      "Epoch 140, training loss: 664.4934692382812 = 1.520186185836792 + 100 * 6.629732608795166\n",
      "Epoch 140, val loss: 1.5075812339782715\n",
      "Epoch 150, training loss: 664.570556640625 = 1.4938722848892212 + 100 * 6.63076639175415\n",
      "Epoch 150, val loss: 1.4794654846191406\n",
      "Epoch 160, training loss: 664.0630493164062 = 1.4620964527130127 + 100 * 6.626009464263916\n",
      "Epoch 160, val loss: 1.4516395330429077\n",
      "Epoch 170, training loss: 660.37841796875 = 1.431633710861206 + 100 * 6.589468002319336\n",
      "Epoch 170, val loss: 1.4242453575134277\n",
      "Epoch 180, training loss: 658.3257446289062 = 1.4055275917053223 + 100 * 6.569202423095703\n",
      "Epoch 180, val loss: 1.4011321067810059\n",
      "Epoch 190, training loss: 659.1428833007812 = 1.3766684532165527 + 100 * 6.577662467956543\n",
      "Epoch 190, val loss: 1.3742740154266357\n",
      "Epoch 200, training loss: 656.3142700195312 = 1.3466519117355347 + 100 * 6.549675941467285\n",
      "Epoch 200, val loss: 1.3487579822540283\n",
      "Epoch 210, training loss: 657.0576171875 = 1.3211767673492432 + 100 * 6.557364463806152\n",
      "Epoch 210, val loss: 1.3240406513214111\n",
      "Epoch 220, training loss: 654.1902465820312 = 1.2902557849884033 + 100 * 6.5289998054504395\n",
      "Epoch 220, val loss: 1.2974364757537842\n",
      "Epoch 230, training loss: 654.40478515625 = 1.2598240375518799 + 100 * 6.531449794769287\n",
      "Epoch 230, val loss: 1.2699055671691895\n",
      "Epoch 240, training loss: 653.74853515625 = 1.2298442125320435 + 100 * 6.525186538696289\n",
      "Epoch 240, val loss: 1.2427815198898315\n",
      "Epoch 250, training loss: 652.3123779296875 = 1.2035788297653198 + 100 * 6.5110883712768555\n",
      "Epoch 250, val loss: 1.220241904258728\n",
      "Epoch 260, training loss: 651.0294189453125 = 1.1788502931594849 + 100 * 6.498505592346191\n",
      "Epoch 260, val loss: 1.2020516395568848\n",
      "Epoch 270, training loss: 652.3500366210938 = 1.1488637924194336 + 100 * 6.512011528015137\n",
      "Epoch 270, val loss: 1.1731982231140137\n",
      "Epoch 280, training loss: 650.5430908203125 = 1.1179497241973877 + 100 * 6.494251728057861\n",
      "Epoch 280, val loss: 1.1497611999511719\n",
      "Epoch 290, training loss: 648.824951171875 = 1.095263123512268 + 100 * 6.477296829223633\n",
      "Epoch 290, val loss: 1.13498055934906\n",
      "Epoch 300, training loss: 648.9354858398438 = 1.0665699243545532 + 100 * 6.478688716888428\n",
      "Epoch 300, val loss: 1.110095500946045\n",
      "Epoch 310, training loss: 648.1292724609375 = 1.0417900085449219 + 100 * 6.470874786376953\n",
      "Epoch 310, val loss: 1.0901564359664917\n",
      "Epoch 320, training loss: 647.8590698242188 = 1.0199153423309326 + 100 * 6.4683918952941895\n",
      "Epoch 320, val loss: 1.073639988899231\n",
      "Epoch 330, training loss: 647.169189453125 = 0.9966698884963989 + 100 * 6.46172571182251\n",
      "Epoch 330, val loss: 1.0614732503890991\n",
      "Epoch 340, training loss: 647.5020751953125 = 0.9698240160942078 + 100 * 6.465322494506836\n",
      "Epoch 340, val loss: 1.039226770401001\n",
      "Epoch 350, training loss: 646.164306640625 = 0.9476423263549805 + 100 * 6.45216703414917\n",
      "Epoch 350, val loss: 1.0236902236938477\n",
      "Epoch 360, training loss: 645.9779052734375 = 0.9277147650718689 + 100 * 6.450501918792725\n",
      "Epoch 360, val loss: 1.01019287109375\n",
      "Epoch 370, training loss: 645.12548828125 = 0.9034939408302307 + 100 * 6.4422197341918945\n",
      "Epoch 370, val loss: 0.9942207336425781\n",
      "Epoch 380, training loss: 645.4757080078125 = 0.8838961124420166 + 100 * 6.445918083190918\n",
      "Epoch 380, val loss: 0.9798300266265869\n",
      "Epoch 390, training loss: 644.0909423828125 = 0.8609434962272644 + 100 * 6.432299613952637\n",
      "Epoch 390, val loss: 0.9657952189445496\n",
      "Epoch 400, training loss: 643.480224609375 = 0.8441293835639954 + 100 * 6.426361083984375\n",
      "Epoch 400, val loss: 0.9587718844413757\n",
      "Epoch 410, training loss: 643.7907104492188 = 0.82208251953125 + 100 * 6.429686546325684\n",
      "Epoch 410, val loss: 0.9428423643112183\n",
      "Epoch 420, training loss: 643.2448120117188 = 0.8023104667663574 + 100 * 6.42442512512207\n",
      "Epoch 420, val loss: 0.9308084845542908\n",
      "Epoch 430, training loss: 642.1791381835938 = 0.7845478057861328 + 100 * 6.413946151733398\n",
      "Epoch 430, val loss: 0.9229530096054077\n",
      "Epoch 440, training loss: 642.4922485351562 = 0.7641425132751465 + 100 * 6.417281150817871\n",
      "Epoch 440, val loss: 0.906264066696167\n",
      "Epoch 450, training loss: 641.5244750976562 = 0.7467882633209229 + 100 * 6.407777309417725\n",
      "Epoch 450, val loss: 0.8996762633323669\n",
      "Epoch 460, training loss: 642.0057373046875 = 0.7286452651023865 + 100 * 6.412771224975586\n",
      "Epoch 460, val loss: 0.8880949020385742\n",
      "Epoch 470, training loss: 640.7015380859375 = 0.7132714986801147 + 100 * 6.399882793426514\n",
      "Epoch 470, val loss: 0.8825547099113464\n",
      "Epoch 480, training loss: 641.8826293945312 = 0.6920902729034424 + 100 * 6.411905288696289\n",
      "Epoch 480, val loss: 0.8664342164993286\n",
      "Epoch 490, training loss: 640.7890625 = 0.6781553030014038 + 100 * 6.401108741760254\n",
      "Epoch 490, val loss: 0.8615360856056213\n",
      "Epoch 500, training loss: 639.535888671875 = 0.6671088337898254 + 100 * 6.388687610626221\n",
      "Epoch 500, val loss: 0.8584086298942566\n",
      "Epoch 510, training loss: 642.8735961914062 = 0.6566919088363647 + 100 * 6.422169208526611\n",
      "Epoch 510, val loss: 0.8551090955734253\n",
      "Epoch 520, training loss: 640.529541015625 = 0.6373589038848877 + 100 * 6.398921966552734\n",
      "Epoch 520, val loss: 0.8423970341682434\n",
      "Epoch 530, training loss: 639.1076049804688 = 0.6236010193824768 + 100 * 6.38484001159668\n",
      "Epoch 530, val loss: 0.8370981812477112\n",
      "Epoch 540, training loss: 639.101318359375 = 0.6133402585983276 + 100 * 6.3848795890808105\n",
      "Epoch 540, val loss: 0.8347387313842773\n",
      "Epoch 550, training loss: 639.0292358398438 = 0.6001339554786682 + 100 * 6.38429069519043\n",
      "Epoch 550, val loss: 0.8289783000946045\n",
      "Epoch 560, training loss: 638.2081298828125 = 0.5888285040855408 + 100 * 6.376193523406982\n",
      "Epoch 560, val loss: 0.8239976167678833\n",
      "Epoch 570, training loss: 638.2418212890625 = 0.5759431719779968 + 100 * 6.376658916473389\n",
      "Epoch 570, val loss: 0.8154981732368469\n",
      "Epoch 580, training loss: 638.7869262695312 = 0.5652849674224854 + 100 * 6.382216453552246\n",
      "Epoch 580, val loss: 0.8119034171104431\n",
      "Epoch 590, training loss: 637.5098266601562 = 0.5559126734733582 + 100 * 6.369539260864258\n",
      "Epoch 590, val loss: 0.8091481924057007\n",
      "Epoch 600, training loss: 638.3580932617188 = 0.547991156578064 + 100 * 6.378101348876953\n",
      "Epoch 600, val loss: 0.80866938829422\n",
      "Epoch 610, training loss: 638.9732666015625 = 0.5349536538124084 + 100 * 6.384382724761963\n",
      "Epoch 610, val loss: 0.8014382719993591\n",
      "Epoch 620, training loss: 637.1691284179688 = 0.5242087841033936 + 100 * 6.366448879241943\n",
      "Epoch 620, val loss: 0.7954917550086975\n",
      "Epoch 630, training loss: 636.5751342773438 = 0.5170226097106934 + 100 * 6.360580921173096\n",
      "Epoch 630, val loss: 0.7955236434936523\n",
      "Epoch 640, training loss: 638.4197387695312 = 0.5073521137237549 + 100 * 6.379124164581299\n",
      "Epoch 640, val loss: 0.7931056618690491\n",
      "Epoch 650, training loss: 637.3732299804688 = 0.4977502226829529 + 100 * 6.368754863739014\n",
      "Epoch 650, val loss: 0.7899861931800842\n",
      "Epoch 660, training loss: 636.3311767578125 = 0.48774102330207825 + 100 * 6.358434200286865\n",
      "Epoch 660, val loss: 0.7861679196357727\n",
      "Epoch 670, training loss: 636.1935424804688 = 0.4818361699581146 + 100 * 6.357117176055908\n",
      "Epoch 670, val loss: 0.7851928472518921\n",
      "Epoch 680, training loss: 638.751708984375 = 0.4730844795703888 + 100 * 6.382786273956299\n",
      "Epoch 680, val loss: 0.7810469269752502\n",
      "Epoch 690, training loss: 636.0681762695312 = 0.4638517200946808 + 100 * 6.356042861938477\n",
      "Epoch 690, val loss: 0.7761590480804443\n",
      "Epoch 700, training loss: 635.4922485351562 = 0.4570977985858917 + 100 * 6.350351333618164\n",
      "Epoch 700, val loss: 0.7752387523651123\n",
      "Epoch 710, training loss: 637.3612060546875 = 0.44948601722717285 + 100 * 6.369117259979248\n",
      "Epoch 710, val loss: 0.7762894034385681\n",
      "Epoch 720, training loss: 636.4683837890625 = 0.4405999183654785 + 100 * 6.3602776527404785\n",
      "Epoch 720, val loss: 0.7731257081031799\n",
      "Epoch 730, training loss: 636.1910400390625 = 0.4333285987377167 + 100 * 6.357576847076416\n",
      "Epoch 730, val loss: 0.7670995593070984\n",
      "Epoch 740, training loss: 635.349609375 = 0.4271891117095947 + 100 * 6.349224090576172\n",
      "Epoch 740, val loss: 0.7692822217941284\n",
      "Epoch 750, training loss: 637.2493896484375 = 0.4221670925617218 + 100 * 6.368272304534912\n",
      "Epoch 750, val loss: 0.76853346824646\n",
      "Epoch 760, training loss: 635.0310668945312 = 0.4118765592575073 + 100 * 6.346191883087158\n",
      "Epoch 760, val loss: 0.7615153193473816\n",
      "Epoch 770, training loss: 634.7171630859375 = 0.40661272406578064 + 100 * 6.343105316162109\n",
      "Epoch 770, val loss: 0.7611885070800781\n",
      "Epoch 780, training loss: 636.0640869140625 = 0.39951953291893005 + 100 * 6.356645584106445\n",
      "Epoch 780, val loss: 0.7616478800773621\n",
      "Epoch 790, training loss: 635.7239379882812 = 0.39179760217666626 + 100 * 6.353321552276611\n",
      "Epoch 790, val loss: 0.7556948065757751\n",
      "Epoch 800, training loss: 634.9149780273438 = 0.38673698902130127 + 100 * 6.345282554626465\n",
      "Epoch 800, val loss: 0.7569147348403931\n",
      "Epoch 810, training loss: 634.399658203125 = 0.38243526220321655 + 100 * 6.340171813964844\n",
      "Epoch 810, val loss: 0.7558572292327881\n",
      "Epoch 820, training loss: 635.6427612304688 = 0.37807902693748474 + 100 * 6.352647304534912\n",
      "Epoch 820, val loss: 0.7553092241287231\n",
      "Epoch 830, training loss: 635.2112426757812 = 0.3697269856929779 + 100 * 6.348414897918701\n",
      "Epoch 830, val loss: 0.7506685853004456\n",
      "Epoch 840, training loss: 634.3243408203125 = 0.36522743105888367 + 100 * 6.339591026306152\n",
      "Epoch 840, val loss: 0.7512746453285217\n",
      "Epoch 850, training loss: 634.6577758789062 = 0.36010831594467163 + 100 * 6.3429765701293945\n",
      "Epoch 850, val loss: 0.7489684224128723\n",
      "Epoch 860, training loss: 634.5314331054688 = 0.35523080825805664 + 100 * 6.341762065887451\n",
      "Epoch 860, val loss: 0.7495896220207214\n",
      "Epoch 870, training loss: 633.6841430664062 = 0.349960595369339 + 100 * 6.333341598510742\n",
      "Epoch 870, val loss: 0.7487584352493286\n",
      "Epoch 880, training loss: 634.0548095703125 = 0.34479689598083496 + 100 * 6.337100505828857\n",
      "Epoch 880, val loss: 0.7495181560516357\n",
      "Epoch 890, training loss: 634.3980102539062 = 0.3411153554916382 + 100 * 6.340569019317627\n",
      "Epoch 890, val loss: 0.7491872310638428\n",
      "Epoch 900, training loss: 634.15478515625 = 0.33575403690338135 + 100 * 6.33819055557251\n",
      "Epoch 900, val loss: 0.7451780438423157\n",
      "Epoch 910, training loss: 633.1964111328125 = 0.3302505910396576 + 100 * 6.3286614418029785\n",
      "Epoch 910, val loss: 0.7416749000549316\n",
      "Epoch 920, training loss: 633.3172607421875 = 0.3278762698173523 + 100 * 6.329893589019775\n",
      "Epoch 920, val loss: 0.746479332447052\n",
      "Epoch 930, training loss: 634.2876586914062 = 0.32306134700775146 + 100 * 6.339645862579346\n",
      "Epoch 930, val loss: 0.7378126978874207\n",
      "Epoch 940, training loss: 633.3906860351562 = 0.31855520606040955 + 100 * 6.330721378326416\n",
      "Epoch 940, val loss: 0.7392882704734802\n",
      "Epoch 950, training loss: 633.6802368164062 = 0.31265634298324585 + 100 * 6.333675384521484\n",
      "Epoch 950, val loss: 0.7382371425628662\n",
      "Epoch 960, training loss: 633.6351928710938 = 0.30989915132522583 + 100 * 6.333252906799316\n",
      "Epoch 960, val loss: 0.737333357334137\n",
      "Epoch 970, training loss: 632.4682006835938 = 0.30583420395851135 + 100 * 6.321623802185059\n",
      "Epoch 970, val loss: 0.7388951182365417\n",
      "Epoch 980, training loss: 632.314208984375 = 0.30333685874938965 + 100 * 6.320108890533447\n",
      "Epoch 980, val loss: 0.7379735708236694\n",
      "Epoch 990, training loss: 633.2953491210938 = 0.2996322214603424 + 100 * 6.329957008361816\n",
      "Epoch 990, val loss: 0.7363893985748291\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7444444444444445\n",
      "0.7843964153927254\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.61572265625 = 1.9340059757232666 + 100 * 8.596817016601562\n",
      "Epoch 0, val loss: 1.9331543445587158\n",
      "Epoch 10, training loss: 829.1777954101562 = 1.8923553228378296 + 100 * 8.272854804992676\n",
      "Epoch 10, val loss: 1.8897552490234375\n",
      "Epoch 20, training loss: 749.75 = 1.8455866575241089 + 100 * 7.479043960571289\n",
      "Epoch 20, val loss: 1.8453446626663208\n",
      "Epoch 30, training loss: 725.14306640625 = 1.8157151937484741 + 100 * 7.233273029327393\n",
      "Epoch 30, val loss: 1.8112410306930542\n",
      "Epoch 40, training loss: 711.1557006835938 = 1.7896735668182373 + 100 * 7.093660354614258\n",
      "Epoch 40, val loss: 1.7766329050064087\n",
      "Epoch 50, training loss: 700.4847412109375 = 1.7638299465179443 + 100 * 6.987208843231201\n",
      "Epoch 50, val loss: 1.7453099489212036\n",
      "Epoch 60, training loss: 691.3570556640625 = 1.7472503185272217 + 100 * 6.8960981369018555\n",
      "Epoch 60, val loss: 1.7283118963241577\n",
      "Epoch 70, training loss: 685.134521484375 = 1.7256333827972412 + 100 * 6.8340888023376465\n",
      "Epoch 70, val loss: 1.7046388387680054\n",
      "Epoch 80, training loss: 679.1522216796875 = 1.7006292343139648 + 100 * 6.7745161056518555\n",
      "Epoch 80, val loss: 1.6782724857330322\n",
      "Epoch 90, training loss: 675.21875 = 1.676274299621582 + 100 * 6.735424995422363\n",
      "Epoch 90, val loss: 1.6530909538269043\n",
      "Epoch 100, training loss: 672.903076171875 = 1.6485207080841064 + 100 * 6.712545871734619\n",
      "Epoch 100, val loss: 1.6257619857788086\n",
      "Epoch 110, training loss: 668.7114868164062 = 1.624391794204712 + 100 * 6.670870780944824\n",
      "Epoch 110, val loss: 1.6027559041976929\n",
      "Epoch 120, training loss: 666.9005126953125 = 1.5988067388534546 + 100 * 6.653017044067383\n",
      "Epoch 120, val loss: 1.5767797231674194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m model \u001b[39m=\u001b[39m UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay, device\u001b[39m=\u001b[39mdevice,data1\u001b[39m=\u001b[39mnoisy_data,data2\u001b[39m=\u001b[39mdiff_noisy_data)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit_1(args, noisy_data\u001b[39m.\u001b[39;49mx, noisy_data\u001b[39m.\u001b[39;49medge_index,noisy_data\u001b[39m.\u001b[39;49medge_weight,noisy_data\u001b[39m.\u001b[39;49my,idx_train,idx_val\u001b[39m=\u001b[39;49midx_val,train_iters\u001b[39m=\u001b[39;49mnum_epochs,cont_iters\u001b[39m=\u001b[39;49mnum_epochs,seen_node_idx\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m acc_cl \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest(noisy_data\u001b[39m.\u001b[39mx, noisy_data\u001b[39m.\u001b[39medge_index,noisy_data\u001b[39m.\u001b[39medge_weight,noisy_data\u001b[39m.\u001b[39my,idx_clean_test)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:247\u001b[0m, in \u001b[0;36mUnifyModel.fit_1\u001b[0;34m(self, args, x, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, cont_iters, seen_node_idx)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[39m# self._train_with_val(self.labels, idx_train, idx_val, train_iters)\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:377\u001b[0m, in \u001b[0;36mUnifyModel._train_with_val_2\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    373\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 377\u001b[0m cont_embds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_index,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_weight)\n\u001b[1;32m    378\u001b[0m clf_loss_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclf_loss(cont_embds,labels,idx_val)\n\u001b[1;32m    379\u001b[0m \u001b[39m# loss_val = clf_loss_val + self.cont_weight * cont_loss\u001b[39;00m\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:155\u001b[0m, in \u001b[0;36mUnifyModel.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    154\u001b[0m             edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, edge_index, edge_weights)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:46\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk):\n\u001b[0;32m---> 46\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv[i](x, edge_index))\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:175\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    173\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    176\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[1;32m    177\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m    179\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:60\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     56\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m     57\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 60\u001b[0m     edge_index, tmp_edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[1;32m     61\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m     62\u001b[0m     \u001b[39massert\u001b[39;00m tmp_edge_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     edge_weight \u001b[39m=\u001b[39m tmp_edge_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/utils/loop.py:224\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo valid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfill_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m     inv_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mmask\n\u001b[0;32m--> 224\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[1;32m    226\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    228\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "num_epochs = config['num_epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "\n",
    "diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "print(\"=== Noisy graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    # model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device).to(device)\n",
    "    model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    # model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc_noisy.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN+CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:80\n",
      "./selected_nodes/Pubmed/Overall/seed265/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 32.18242645263672 = 1.1051568984985352 + 3 * 10.359089851379395\n",
      "Epoch 0, val loss: 1.1028962135314941\n",
      "Epoch 10, training loss: 31.999353408813477 = 1.0672951936721802 + 3 * 10.310686111450195\n",
      "Epoch 10, val loss: 1.061737298965454\n",
      "Epoch 20, training loss: 30.302324295043945 = 1.0193684101104736 + 3 * 9.760985374450684\n",
      "Epoch 20, val loss: 1.0125272274017334\n",
      "Epoch 30, training loss: 29.326562881469727 = 0.9534475803375244 + 3 * 9.4577054977417\n",
      "Epoch 30, val loss: 0.9465145468711853\n",
      "Epoch 40, training loss: 29.144182205200195 = 0.8632984161376953 + 3 * 9.426960945129395\n",
      "Epoch 40, val loss: 0.8570841550827026\n",
      "Epoch 50, training loss: 28.9471492767334 = 0.7454797625541687 + 3 * 9.400556564331055\n",
      "Epoch 50, val loss: 0.7456722259521484\n",
      "Epoch 60, training loss: 28.668258666992188 = 0.6258248090744019 + 3 * 9.347477912902832\n",
      "Epoch 60, val loss: 0.6361944079399109\n",
      "Epoch 70, training loss: 28.344999313354492 = 0.518261194229126 + 3 * 9.275579452514648\n",
      "Epoch 70, val loss: 0.5414265990257263\n",
      "Epoch 80, training loss: 28.161968231201172 = 0.43747106194496155 + 3 * 9.241498947143555\n",
      "Epoch 80, val loss: 0.475567102432251\n",
      "Epoch 90, training loss: 28.072803497314453 = 0.38862332701683044 + 3 * 9.228059768676758\n",
      "Epoch 90, val loss: 0.44119203090667725\n",
      "Epoch 100, training loss: 28.003673553466797 = 0.3598519265651703 + 3 * 9.214607238769531\n",
      "Epoch 100, val loss: 0.424256831407547\n",
      "Epoch 110, training loss: 27.949234008789062 = 0.33997005224227905 + 3 * 9.20308780670166\n",
      "Epoch 110, val loss: 0.41429153084754944\n",
      "Epoch 120, training loss: 27.933443069458008 = 0.32420071959495544 + 3 * 9.203080177307129\n",
      "Epoch 120, val loss: 0.4071075916290283\n",
      "Epoch 130, training loss: 27.887771606445312 = 0.31164103746414185 + 3 * 9.19204330444336\n",
      "Epoch 130, val loss: 0.40260908007621765\n",
      "Epoch 140, training loss: 27.85540008544922 = 0.3009180724620819 + 3 * 9.184826850891113\n",
      "Epoch 140, val loss: 0.3996649980545044\n",
      "Epoch 150, training loss: 27.82279396057129 = 0.291324257850647 + 3 * 9.177156448364258\n",
      "Epoch 150, val loss: 0.3972914218902588\n",
      "Epoch 160, training loss: 27.78581428527832 = 0.28242993354797363 + 3 * 9.167794227600098\n",
      "Epoch 160, val loss: 0.39594438672065735\n",
      "Epoch 170, training loss: 27.761075973510742 = 0.27444615960121155 + 3 * 9.162209510803223\n",
      "Epoch 170, val loss: 0.3962668478488922\n",
      "Epoch 180, training loss: 27.742660522460938 = 0.2672615945339203 + 3 * 9.158466339111328\n",
      "Epoch 180, val loss: 0.3966449201107025\n",
      "Epoch 190, training loss: 27.722490310668945 = 0.2604388892650604 + 3 * 9.154017448425293\n",
      "Epoch 190, val loss: 0.3959454298019409\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8473\n",
      "Overall ASR: 0.7150\n",
      "Flip ASR: 0.6448/1554 nodes\n",
      "./selected_nodes/Pubmed/Overall/seed125/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 32.16830062866211 = 1.0912407636642456 + 3 * 10.359020233154297\n",
      "Epoch 0, val loss: 1.0878543853759766\n",
      "Epoch 10, training loss: 31.903640747070312 = 1.0645138025283813 + 3 * 10.279708862304688\n",
      "Epoch 10, val loss: 1.0600996017456055\n",
      "Epoch 20, training loss: 29.89044761657715 = 1.0335246324539185 + 3 * 9.618973731994629\n",
      "Epoch 20, val loss: 1.0269728899002075\n",
      "Epoch 30, training loss: 29.2506160736084 = 0.9670185446739197 + 3 * 9.427865982055664\n",
      "Epoch 30, val loss: 0.9605767726898193\n",
      "Epoch 40, training loss: 29.08148956298828 = 0.8730457425117493 + 3 * 9.402814865112305\n",
      "Epoch 40, val loss: 0.8691599369049072\n",
      "Epoch 50, training loss: 28.82680892944336 = 0.7649738788604736 + 3 * 9.353944778442383\n",
      "Epoch 50, val loss: 0.7656345367431641\n",
      "Epoch 60, training loss: 28.576345443725586 = 0.6418277621269226 + 3 * 9.311505317687988\n",
      "Epoch 60, val loss: 0.647817075252533\n",
      "Epoch 70, training loss: 28.28866958618164 = 0.5174333453178406 + 3 * 9.257079124450684\n",
      "Epoch 70, val loss: 0.5369020104408264\n",
      "Epoch 80, training loss: 28.105419158935547 = 0.43511220812797546 + 3 * 9.223435401916504\n",
      "Epoch 80, val loss: 0.4710129201412201\n",
      "Epoch 90, training loss: 27.993255615234375 = 0.3899606168270111 + 3 * 9.201098442077637\n",
      "Epoch 90, val loss: 0.44206976890563965\n",
      "Epoch 100, training loss: 27.94057846069336 = 0.36175841093063354 + 3 * 9.192939758300781\n",
      "Epoch 100, val loss: 0.4282899498939514\n",
      "Epoch 110, training loss: 27.89201545715332 = 0.3407426178455353 + 3 * 9.183757781982422\n",
      "Epoch 110, val loss: 0.4195663332939148\n",
      "Epoch 120, training loss: 27.84457015991211 = 0.3240402042865753 + 3 * 9.173510551452637\n",
      "Epoch 120, val loss: 0.41288119554519653\n",
      "Epoch 130, training loss: 27.824583053588867 = 0.3108881413936615 + 3 * 9.171231269836426\n",
      "Epoch 130, val loss: 0.4086180329322815\n",
      "Epoch 140, training loss: 27.76441764831543 = 0.30042824149131775 + 3 * 9.1546630859375\n",
      "Epoch 140, val loss: 0.40592867136001587\n",
      "Epoch 150, training loss: 27.729389190673828 = 0.2912631034851074 + 3 * 9.146041870117188\n",
      "Epoch 150, val loss: 0.40426233410835266\n",
      "Epoch 160, training loss: 27.72370719909668 = 0.28290292620658875 + 3 * 9.146934509277344\n",
      "Epoch 160, val loss: 0.40297067165374756\n",
      "Epoch 170, training loss: 27.700855255126953 = 0.27559277415275574 + 3 * 9.141754150390625\n",
      "Epoch 170, val loss: 0.4026649594306946\n",
      "Epoch 180, training loss: 27.677738189697266 = 0.26874855160713196 + 3 * 9.136329650878906\n",
      "Epoch 180, val loss: 0.4022245705127716\n",
      "Epoch 190, training loss: 27.658706665039062 = 0.26211515069007874 + 3 * 9.132197380065918\n",
      "Epoch 190, val loss: 0.4021083414554596\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8392\n",
      "Overall ASR: 0.7901\n",
      "Flip ASR: 0.7375/1554 nodes\n",
      "./selected_nodes/Pubmed/Overall/seed996/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 32.167388916015625 = 1.0903472900390625 + 3 * 10.359014511108398\n",
      "Epoch 0, val loss: 1.0876606702804565\n",
      "Epoch 10, training loss: 31.80080795288086 = 1.0631873607635498 + 3 * 10.24587345123291\n",
      "Epoch 10, val loss: 1.0590697526931763\n",
      "Epoch 20, training loss: 29.834352493286133 = 1.031775951385498 + 3 * 9.600858688354492\n",
      "Epoch 20, val loss: 1.0251414775848389\n",
      "Epoch 30, training loss: 29.25065040588379 = 0.9713391065597534 + 3 * 9.426437377929688\n",
      "Epoch 30, val loss: 0.9642313122749329\n",
      "Epoch 40, training loss: 29.05682373046875 = 0.877650260925293 + 3 * 9.393057823181152\n",
      "Epoch 40, val loss: 0.8739688396453857\n",
      "Epoch 50, training loss: 28.703163146972656 = 0.7696259021759033 + 3 * 9.311179161071777\n",
      "Epoch 50, val loss: 0.7705264687538147\n",
      "Epoch 60, training loss: 28.424022674560547 = 0.6427233219146729 + 3 * 9.260433197021484\n",
      "Epoch 60, val loss: 0.6493310332298279\n",
      "Epoch 70, training loss: 28.211217880249023 = 0.5121635794639587 + 3 * 9.233017921447754\n",
      "Epoch 70, val loss: 0.5291482210159302\n",
      "Epoch 80, training loss: 28.080461502075195 = 0.4252015948295593 + 3 * 9.218420028686523\n",
      "Epoch 80, val loss: 0.4587113559246063\n",
      "Epoch 90, training loss: 27.996076583862305 = 0.3791777789592743 + 3 * 9.205633163452148\n",
      "Epoch 90, val loss: 0.42630618810653687\n",
      "Epoch 100, training loss: 27.960187911987305 = 0.3531135022640228 + 3 * 9.20235824584961\n",
      "Epoch 100, val loss: 0.41040053963661194\n",
      "Epoch 110, training loss: 27.906421661376953 = 0.335080623626709 + 3 * 9.190446853637695\n",
      "Epoch 110, val loss: 0.4001295268535614\n",
      "Epoch 120, training loss: 27.86421012878418 = 0.32089412212371826 + 3 * 9.181105613708496\n",
      "Epoch 120, val loss: 0.39319780468940735\n",
      "Epoch 130, training loss: 27.816566467285156 = 0.30903762578964233 + 3 * 9.16917610168457\n",
      "Epoch 130, val loss: 0.38871628046035767\n",
      "Epoch 140, training loss: 27.77755355834961 = 0.2988588213920593 + 3 * 9.159564971923828\n",
      "Epoch 140, val loss: 0.3857477903366089\n",
      "Epoch 150, training loss: 27.743440628051758 = 0.2897914946079254 + 3 * 9.151216506958008\n",
      "Epoch 150, val loss: 0.3838469386100769\n",
      "Epoch 160, training loss: 27.72982406616211 = 0.28135213255882263 + 3 * 9.149490356445312\n",
      "Epoch 160, val loss: 0.38319697976112366\n",
      "Epoch 170, training loss: 27.7026424407959 = 0.27372413873672485 + 3 * 9.142972946166992\n",
      "Epoch 170, val loss: 0.3832148611545563\n",
      "Epoch 180, training loss: 27.684070587158203 = 0.26668789982795715 + 3 * 9.139127731323242\n",
      "Epoch 180, val loss: 0.3839960992336273\n",
      "Epoch 190, training loss: 27.67112922668457 = 0.26008328795433044 + 3 * 9.137015342712402\n",
      "Epoch 190, val loss: 0.3850516676902771\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8453\n",
      "Overall ASR: 0.7601\n",
      "Flip ASR: 0.7008/1554 nodes\n",
      "The final ASR:0.75507, 0.03085, Accuracy:0.84390, 0.00345\n"
     ]
    }
   ],
   "source": [
    "''' Contrastive learning to backdoor in Contrastive learning'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "\n",
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "args.homo_loss_weight=config['homo_loss_weight']\n",
    "args.homo_boost_thrd = config['homo_boost_thrd']\n",
    "args.trojan_epochs = config['trojan_epochs']\n",
    "args.selection_method = config['selection_method']\n",
    "args.vs_number=config['vs_number']\n",
    "args.num_epochs = config['num_epochs']\n",
    "args.seed = config['seed']\n",
    "args.cont_batch_size = config['cont_batch_size']\n",
    "args.cont_weight = config['cont_weight']\n",
    "args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "data = data.to(device)\n",
    "# learning_rate = 0.0002\n",
    "weight_decay = config['weight_decay']\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                         base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=None).to(device)\n",
    "    # contrastive_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,idx_train,idx_val=idx_val,train_iters=1000,cont_iters=num_epochs,seen_node_idx=seen_node_idx)\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, optimizer, poison_x, poison_edge_index, poison_edge_weights, seen_node_idx)\n",
    "\n",
    "    #     now = t()\n",
    "    #     # if(epoch%10 == 0):\n",
    "    #     #     print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #     #             f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    # test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "                            base_model=base_model, k=num_layers).to(device)\n",
    "    test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=None).to(device)\n",
    "    # test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=seen_node_idx)\n",
    "    test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    output = test_model.clf_head(test_embds)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    # test_model.fit(cont_poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    # output = test_model(cont_poison_x,poison_edge_index,poison_edge_weights)\n",
    "    # train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    # print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    # torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "                output = test_model.clf_head(test_embeds)\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                # output = test_model(cont_induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    elif(args.evaluate_mode == 'overall'):\n",
    "        # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "        output = test_model.clf_head(test_embeds)\n",
    "        # test_model = test_model.to(device)\n",
    "        # output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:80\n",
      "raw graph: torch.Size([2, 88406])\n",
      "add edge: torch.Size([2, 97208])\n",
      "remove edge: torch.Size([2, 79626])\n",
      "updated graph: torch.Size([2, 88428])\n",
      "./selected_nodes/Pubmed/Overall/seed265/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8524\n",
      "Overall ASR: 0.8134\n",
      "Flip ASR: 0.7683/1554 nodes\n",
      "./selected_nodes/Pubmed/Overall/seed125/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8503\n",
      "Overall ASR: 0.8448\n",
      "Flip ASR: 0.8069/1554 nodes\n",
      "./selected_nodes/Pubmed/Overall/seed996/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8534\n",
      "Overall ASR: 0.8626\n",
      "Flip ASR: 0.8288/1554 nodes\n",
      "The final ASR:0.84026, 0.02034, Accuracy:0.85202, 0.00127\n"
     ]
    }
   ],
   "source": [
    "'''Backdoor attack to GNN classifier'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "args.homo_loss_weight=config['homo_loss_weight']\n",
    "args.vs_number=config['vs_number']\n",
    "args.trojan_epochs = config['trojan_epochs']\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "args.seed = config['seed']\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, poison_x, poison_edge_index, poison_edge_weights)\n",
    "\n",
    "    #     now = t()\n",
    "    #     if(epoch%10 == 0):\n",
    "    #         print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #                 f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch120': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ab847dfc59cee10fa08e4e9fed31787c275fa5742f67664facc345e6fad65e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
